Phrase Pair Rescoring with Term Weightings forStatistical Machine TranslationBing Zhao   Stephan Vogel   Alex WaibelCarnegie Mellon University5000 Forbes AvenuePittsburgh, PA, 15213, USA{bzhao, vogel+, ahw}@cs.cmu.eduAbstractWe propose to score phrase translationpairs for statistical machine translationusing term weight based models.
Thesemodels employ tf.idf to encode theweights of content and non-content wordsin phrase translation pairs.
The transla-tion probability is then modeled by simi-larity functions defined in a vector space.Two similarity functions are compared.Using these models in a statistical ma-chine translation task shows significantimprovements.1 IntroductionWords can be classified as content and func-tional words.
Content words like verbs andproper nouns are more informative than functionwords like "to'' and "the''.
In machine translation,intuitively, the informative content words shouldbe emphasized more for better adequacy of thetranslation quality.
However, the standard statis-tical translation approach does not take accounthow informative and thereby, how important aword is, in its translation model.
One reason isthe difficulty to measure how informative a wordis.
Another problem is to integrate it naturallyinto the existing statistical machine translationframework, which typically is built on wordalignment models, like the well-known IBMalignment models (Brown et al1993).In recent years there has been a strong ten-dency to incorporate phrasal translation into sta-tistical machine translation.
It directly translatesan n-gram from the source language into an m-gram in the target language.
The advantages areobvious:  It has built-in local context modeling,and provides reliable local word reordering.
Ithas multi-word translations, and models a word?sconditional fertility given a local context.
It cap-tures idiomatic phrase translations and can beeasily enriched with bilingual dictionaries.
Inaddition, it can compensate for the segmentationerrors made during preprocessing, i.e.
word seg-mentation errors of Chinese.
The advantage ofusing phrase-based translation in a statisticalframework has been shown in many studies suchas (Koehn et al 2003; Vogel et al 2003; Zens etal.
2002; Marcu and Wong, 2002).
However, thephrase translation pairs are typically extractedfrom a parallel corpus based on the Viterbi align-ment of some word alignment models.
The leadsto the question what probability should be as-signed to those phrase translations.
Differentapproaches have been suggested as using relativefrequencies (Zens et al 2002), calculate prob-abilities based on a statistical word-to-word dic-tionary (Vogel et al 2003) or use a linearinterpolation of these scores (Koehn et al 2003).In this paper we investigate a different ap-proach with takes the information content ofwords better into account.
Term weighting basedvector models are proposed to encode the transla-tion quality.
The advantage is that term weights,such as tf.idf, are useful to model the informa-tiveness of words.
Highly informative contentwords usually have high tf.idf scores.
In informa-tion retrieval this has been successfully applied tocapture the relevance of a document to a query,by representing both query and documents asterm weight vectors and use for example thecosine distance to calculate the similarity be-tween query vector and document vector.
Theidea now is to consider the source phrase as a?query?, and the different target phrases ex-tracted from the bilingual corpus as translationcandidates as a relevant ?documents?.
The co-sine distance is then a natural choice to model thetranslation probability.Our approach is to apply term weightingschemes to transform source and target phrasesinto term vectors.
Usually content words in bothsource and target languages will be emphasizedby large term weights.
Thus, good phrase trans-lation pairs will share similar contours, or, to ex-press it in a different way, will be close to eachother in the term weight vector space.
A similar-ity function is then defined to approximate trans-lation probability in the vector space.The paper is structured as follows:  in Section2, our phrase-based statistical machine translationsystem is introduced; in Section 3, a phrase trans-lation score function based on word translationprobabilities is explained, as this will be used as abaseline system;  in Section 4, a vector modelbased on tf.idf is proposed together with twosimilarity functions;  in Section 5, length regu-larization and smoothing schemes are explainedbriefly;  in Section 6, the translation experimentsare presented; and Section 7 concludes with adiscussion.2 Phrase-based Machine TranslationIn this section, the phrase-based machine transla-tion system used in the experiments is brieflydescribed: the phrase based translation modelsand the decoding algorithm, which allows forlocal word reordering.2.1 Translation ModelThe phrase-based statistical translation systemsuse not only word-to-word translation, extractedfrom bilingual data, but also phrase-to phrasetranslations.
.
Different types of extraction ap-proaches have been described in the literature:syntax-based, word-alignment-based, and genu-ine phrase alignment models.
The syntax-basedapproach has the advantage to model the gram-mar structures using models of more or lessstructural richness, such as the syntax-basedalignment model in (Yamada and Knight, 2001)or the Bilingual Bracketing in (Wu, 1997).
Popu-lar word-alignment-based approaches usuallyrely on initial word alignments from the IBM andHMM alignment models (Och and Ney, 2000),from which the phrase pairs are then extracted.
(Marcu and Wong 2002) and (Zhang et al 2003)do not rely on word alignment but model directlythe phrase alignment.Because all statistical machine translation sys-tems search for a globally optimal translationusing the language and translation model, a trans-lation probability has to be assigned to eachphrase translation pair.
This score should bemeaningful in that better translations have ahigher probability assigned to them, and balancedwith respect to word translations.
Bad phrasetranslations should not win over better word forword translations, only because they are phrases.Our focus here is not phrase extraction, buthow to estimate a reasonable probability (orscore) to better represent the translation qualityof the extracted phrase pairs.
One major problemis that most phrase pairs are seen only severaltimes, even in a very large corpus.
A reliable andeffective estimation approach is explained in sec-tion 3, and the proposed models are introduced insection 4.In our system, a collection of phrase transla-tions is called a transducer.
Different phrase ex-traction methods result in different transducers.A manual dictionary can be added to the systemas just another transducer.
Typically, one sourcephrase is aligned with several candidate targetphrases, with a score attached to each candidaterepresenting the translation quality.2.2 Decoding AlgorithmGiven a set of transducers as the translationmodel (i.e.
phrase translation pairs together withthe scores of their translation quality), decodingis divided into several steps.The first step is to build a lattice by applyingthe transducers to the input source sentence.
Westart from a lattice, which has as its only path thesource sentence.
Then for each word or sequenceof words in the source sentence for which wehave an entry in the transducer new edges aregenerated and inserted into the lattice, spanningover the source phrase.
One new edge is createdfor each translation candidate, and the translationscore is assigned to this edge.
The resulting lat-tice has then all the information available fromthe translation model.The second step is search for a best paththrough this lattice, but not only based on thetranslation model scores but applying also thelanguage model.
We start with an initial specialsentence begin hypothesis at the first node in thelattice.
Hypotheses are then expanded over theedges, applying the language model to the partialtranslations attached to the edges.
The followingalgorithm summarizes the decoding processwhen not considering word reordering:Current node n, previous node n?
; edge eLanguage model state L, L?Hypothesis h, h?Foreach node n in the latticeForeach incoming edge e in nphrase = word sequence at en?
= FromNode(e)foreach L in n?foreach h with LMstate LLMcost = 0.0foreach word w in phraseLMcost += -log p(w|L)L?
= NewState(L,w)L  = L?endCost= LMcost+TMcost(e)TotalCost=TotalCost(h)+Costh?
= (L,e,h,TotalCost)store h?in Hypotheses(n,L)The updated hypothesis h?
at the current nodestores the pointer to the previous hypothesis andthe edge (labeled with the target phrase) overwhich it was expanded.
Thus, at the final step,one can trace back to get the path associated withthe minimum cost, i.e.
the best hypothesis.Other operators such as local word reorderingare incorporated into this dynamic programmingsearch (Vogel, 2003).3 Phrase Pair Translation ProbabilityAs stated in the previous section, one of themajor problems is how to assign a reasonableprobability for the extracted phrase pair to repre-sent the translation quality.Most of the phrase pairs are seen only once ortwice in the training data.
This is especially truefor longer phrases.
Therefore, phrase pair co-occurrence counts collected from the trainingcorpus are not reliable and have little discrimina-tive power.
In (Vogel et al 2003) a different es-timation approach was proposed.
Similar as inthe IBM models, it is assumed that each sourceword si in the source phrase ),,( 21 Issss Lv =  isaligned to every target word tj in the target phrase),,( 21 Jtttt Lv =  with probability )|Pr( ij st .
Thetotal phrase translation probability is then calcu-lated according to the following generativemodel:?
?= ==IiJjji tsts1 1))|Pr(()|Pr(vv  (1)This is essentially the lexical probability ascalculated in the IBM1 alignment model, withoutconsidering position alignment probabilities.Any statistical translation can be used in (1) tocalculate the phrase translation probability.However, in our experiment we typically see nowsignificant difference in translation results whenusing lexicons trained from different alignmentmodels.Also Equation (1) was confirmed to be robustand effective in parallel sentence mining from avery large and noisy comparable corpus (Zhaoand Vogel, 2002).Equation (1) does not explicitly discriminatecontent words from non-content words.
As non-content words such as high frequency functionalwords tend to occur in nearly every parallel sen-tence pair, they co-occur with most of the sourcewords in the vocabulary with non-trivial transla-tion probabilities.
This noise propagates via (1)into the phrase translations probabilities, increas-ing the chance that non-optimal phrase transla-tion candidates get high probabilities and bettertranslations are often not in the top ranks.We propose a vector model to better distin-guish between content words and non-contentwords with the goal to emphasize content wordsin the translation.
This model will be used torescore the phrase translation pairs, and to get anormalized score representing the translationprobability.4 Vector Model for Phrase TranslationProbabilityTerm weighting models such as tf.idf are ap-plied successfully in information retrieval.
Theduality of term frequency (tf) and inverse docu-ment frequency (idf), document space and collec-tion space respectively, can smoothly predict theprobability of terms being informative (Roelleke,2003).
Naturally, tf.idf is suitable to model con-tent words as these words in general have largetf.idf weights.4.1 Phrase Pair as Bag-of-WordsOur translation model: (transducer, as definedin 2.1), is a collection of phrase translation pairstogether with scores representing the translationquality.
Each phrase translation pair, which canbe represented as a triple },{ pts vv?
, is now con-verted into a ?Bag-of-Words?
D consisting of acollection of both source and target words ap-pearing in the phrase pair, as shown in (2):},,,,,{},{ 2121 JI tttsssDpts LLvv =??
(2)Given each phrase pair as one document, thewhole transducer is a collection of such docu-ments.
We can calculate tf.idf for each is  and jt ,and represent source and target phrases by vec-tors of svv  and tvv   as in Equation (3):},,,{21 Isssswwwv Lv =},,,{21 Jttttwwwv Lv =  (3)whereisw andjtw are tf.idf for is or jt respectively.This vector representation can be justified byword co-occurrence considerations.
As thephrase translation pairs are extracted from paral-lel sentences, the source words is  and targetwords jt  in the source and target phrases mustco-occur in the training data.
The co-occurringwords should share similar term frequency anddocument frequency statistics.
Therefore, thevectors  svv and tvv  have similar term weight con-tours corresponding to the co-occurring wordpairs.
So the vector representations of a phrasetranslation pair can reflect the translation quality.In addition, the content words and non-contentwords are modeled explicitly by using termweights.
An over-simplified example would bethat a rare word in the source language usuallytranslates into a rare word in the target language.4.2 Term Weighting SchemesGiven the transducer, it is straightforward tocalculate term weights for source and targetwords.
There are several versions of tf.idf.
Thesmooth ones are preferred, because phrase trans-lation pairs are rare events collected from train-ing data.The idf model selected is as in Equation (4):)5.05.0log( ++?=dfdfNidf  (4)where N is the total number of documents in thetransducer, i.e.
the total number of translationpairs, and df is the document frequency, i.e.
inhow many phrase pairs a given word occurs.
Theconstant of 0.5 acts as smoothing.Because most of the phrases are short, such as2 to 8 words, the term frequency in the bag ofwords representation is usually 1, and some times2.
This, in general, does not bring much dis-crimination in representing translation quality.The following version of tf is chosen, so thatlonger target phrases with more words than aver-age will be slightly down-weighted:)(/)(5.15.0'vavglenvlentftftf vv?++=  (5)where tf is the term frequency, )(vlen v  is thelength in words of the phrase vv , and )(vavglen v  isthe average length of source or target phrase cal-culated from the transducer.
Again, the values of0.5 and 1.5 are constants used in IR tasks actingas smoothing.Thus after a transducer is extracted from a par-allel corpus, tf and df are counted from the collec-tion of the ?bag-of-words'' phrase alignmentrepresentations.
For each word in the phrase pairtranslation its tf.idf weight is assigned and thesource and target phrase are transformed intovectors as shown in Equation (3).
These vectorsreserve the translation quality information andalso model the content and non-content words bythe term weighting model of tf.idf.4.3 Vector Space AlignmentGiven the vector representations in Equation(3), a similarity between the two vectors can notdirectly be calculated.
The dimensions I and Jare not guaranteed to be the same.
The goal is totransform the source vector into a vector havingthe same dimensions as the target vector, i.e.
tomap the source vector into the space of the targetvector, so that a similarity distance can be calcu-lated.
Using the same reasoning as used to moti-vate Equation (1), it is assumed that every sourceword is  contributes some probability mass toeach target word jt .
That is to say, given a termweight for jt , all source term weights are alignedto it with some probability.
So we can calculatea transformed vector from the source vectors bycalculating weights jtaw  using a translation lexi-con )|Pr( st  as in Equation (6):?=?=Iisijta ij wstw1)|Pr(  (6)Now the target vector and the mapped vectoravv  have the same dimensions as shown in (7):},,,{ 21 Jtatataa wwwv Lv =},,,{21 Jttttwwwv Lv =  (7)4.4 Similarity FunctionsAs explained in section 4.1, intuitively, if svand tv  is a good translation pair, then the corre-sponding vectors of avv  and tvv  should be similarto each other in the vector space.Cosine distanceThe standard cosine distance is defined as theinner product of the two vectors avv  and tvv  nor-malized by their norms.
Based on Equation (6),it is easy to derive the similarity as follows:)()()|()|(11),(),(12121 11 11cos???
??
?
?=== == ======JjtaJjtJjIisijtJjIisijtttaJjttattattattattajjijijjjwsqrtwsqrtwstPwwstPwvvwwvvvvvvvvd(8)where I and J are the length of the source andtarget phrases;isw  andjtw  are term weights forsource word and target words;  jtaw  is the trans-formed weight mapped from all source words tothe target dimension at word jt .BM25 distanceTREC tests show that bm25 (Robertson andWalker, 1997) is one of the best-known distanceschemes.
This distance metric is given in Equa-tion (9).
The constants of 31 ,, kbk are set to be 1, 1and 1000 respectively.
)()1()()1(331125 jjjjtataJj ttbm wkwkwKwkwd ++++=?=)5.0/()5.0( ++?==jjj tttdfdfNidfw))(/)1((1 lavgJbkK +?=(9)where avg(l) is the average target phrase lengthin words given the same source phrase.Our experiments confirmed the bm25 distanceis slightly better than the cosine distance, thoughthe difference is not really significant.
One ad-vantage of bm25 distance is that the set of freeparameters 31 ,, kbk can be tuned to get better per-formance e.g.
via n-fold cross validation.4.5 Integrated Translation ScoreOur goal is to rescore the phrase translationpairs by using additional evidence of the transla-tion quality in the vector space.The vector based scores (8) & (9) provide adistinct view of the translation quality in the vec-tor space.
Equation (1) provides a evidence ofthe translation quality based on the word align-ment probability, and can be assumed to be dif-ferent from the evidences in vector space.
Thus,a natural way of integrating them together is ageometric interpolation shown in (10) or equiva-lently a linear interpolation in the log domain.
)|(Pr),( 1int tsstdd vecvvvv ??
?
?=  (10)where ),( stdvecvv is the score from the cosine orbm25 vector distance, normalized within [0, 1],like a probability.0.1),( =?tvec stdvvvThe parameter ?
can be tuned using held-outdata.
In our cross validation experiments 5.0=?gave the best performance in most cases.
There-fore, Equation (10) can be simplified into:)|Pr(),(int tsstdd vecvvvv ?=  (11)The phrase translation score functions in (1)and (11) are non-symmetric.
This is because thestatistical lexicon Pr(s|t) is non-symmetric.
Onecan easily re-write all the distances by usingPr(t|s).
But in our experiments this reverse di-rection of using Pr(t|s) gives trivially difference.So in all the experimental results reported in thispaper, the distances defined in (1) and (11) areused.5 Length RegularizationPhrase pair extraction does not work perfectlyand sometimes a short source phrase is aligned toa long target phrase or vice versa.
Length regu-larization can be applied to penalize too long ortoo short candidate translations.
Similar to thesentence alignment work in (Gale and Church,1991), the phrase length ratio is assumed to be aGaussian distribution as given in Equation (12):)))(/)((5.0exp(),( 22??????
sltlstlvvvv  (12)where l(t) is the target sentence  length.
Mean ?and variance ?
can be estimated using a parallelcorpus using a Maximum Likelihood criteria.The regularized score is the product of (11) and(12).6 ExperimentsExperiments were carried out on the so-calledlarge data track Chinese-English TIDES transla-tion task, using the June 2002 test data.
Thetraining data used to train the statistical lexiconand to extract the phrase translation pairs wasselected from a 120 million word parallel corpusin such a way as to cover the phrases in test sen-tences.
The restricted training corpus containedthen approximately 10 million words..  A trigrammodel was built on 20 million words of generalnewswire text, using the SRILM toolkit (Stolcke,2002).
Decoding was carried out as described insection 2.2.
The test data consists of 878 Chinesesentences or 24,337 words after word segmenta-tion.
There are four human translations per Chi-nese sentence as references.
Both NIST scoreand Bleu score (in percentage) are reported foradequacy and fluency aspects of the translationquality.6.1 TransducersFour transducers were used in our experi-ments: LDC, BiBr, HMM, and ISA.LDC was built from the LDC Chinese-Englishdictionary in two steps: first, morphologicalvariations are created.
For nouns and nounphrases plural forms and entries with definite andindefinite determiners were generated.
For verbsadditional word forms with -s -ed and -ing weregenerated, and the infinitive form with 'to'.
Sec-ond, a large monolingual English corpus wasused to filter out the new word forms.
If they didnot appear in the corpus, the new entries were notadded to the transducer (Vogel, 2004).BiBr extracts sub-tree mappings from Bilin-gual Bracketing alignments (Wu, 1997);  HMMextracts partial path mappings from the Viterbipath in the Hidden Markov Model alignments(Vogel et.
al., 1996).
ISA is an integrated seg-mentation and alignment for phrases (Zhang et.al,2003), which is an extension of (Marcu andWong, 2002).LDC BiBr HMM ISA)(KN  425K 137K 349K 263K)/( srctgt llavg  1.80 1.11 1.09 1.20Table-1 statistics of transducersTable-1 shows some statistics of the fourtransducers extracted for the translation task.
Nis the total number of phrase pairs in the trans-ducer.
LDC is the largest one having 425K en-tries, as the other transducers are restricted to?useful?
entries, i.e.
those translation pairs wherethe source phrase matches a sequence of words inone of the test sentence.
Notice that the LDCdictionary has a large number of long transla-tions, leading to a high source to target lengthratio.6.2 Cosine vs BM25The normalized cosine and bm25 distances de-fined in (8) and (9) respectively, are plugged into(11) to calculate the translation probabilities.Initial experiments are reported on the LDCtransducer, which gives already a good transla-tion, and therefore allows for fast and yet mean-ingful experimentation.Four baselines (Uniform, Base-m1, Base-m4,and Base-m4S) are presented in Table-2.NIST BleuUniform 6.69 13.82Base-m1 7.08 14.84Base-m4 7.04 14.91Base-m4S 6.91 14.44cosine 7.17 15.30bm25 7.19 15.51bm25-len 7.21 15.64Table-2 Comparisons of different score functionsIn the first uniform probabilities are assignedto each phrase pair in the transducer.
The secondone (Base-m1) is using Equation (1) with a statis-tical lexicon trained using IBM Model-1, andBase-m4 is using the lexicon from IBM Model-4.Base-m4S is using IBM Model-4, but we skipped194 high frequency English stop words in thecalculation of Equation (1).Table-2 shows that the translation score de-fined by Equation (1) is much better than a uni-form model, as expected.
Base-m4 is slightlyworse than Base-m1.on NIST score, but slightlybetter using the Bleu metric.
Both differencesare not statistically significant.
The result forBase-m4S shows that skipping English stopwords in Equation (1) gives a disadvantage.
Onereason is that skipping ignores too much non-trivial statistics from parallel corpus especiallyfor short phrases.
These high frequency wordsactually account already for more than 40% ofthe tokens in the corpus.Using the vector model, both with the cosinecosd  and the bm25 25bmd  distance, is significantlybetter than Base-m1 and Base-m4 models, whichconfirms our intuition of the vector model as anadditional useful evidence for translation quality.The length regularization (12) helps only slightlyfor LDC.
Since bm25?s parameters could betuned for potentially better performance, we se-lected bm25 with length regularization as themodel tested in further experiments.A full-loaded system is tested using theLM020 with and without word-reordering in de-coding.
The results are presented in Table-3.Table-3 shows consistent improvements on allconfigurations: the individual transducers, com-binations of transducers, and different decodersettings of word-reordering.
Because each phrasepair is treated as a ?bag-of-words?, the grammarstructure is not well represented in the vectormodel.
Thus our model is more tuned towardsthe adequacy aspect, corresponding to NISTscore improvement.Because the transducers of BiBr, HMM, andISA are extracted from the same training data,they have significant overlaps with each other.This is why we observe only small improvementswhen adding more transducers.The final NIST score of the full system is 8.24,and the Bleu score is 22.37.
This corresponds to3.1% and 11.8% relative improvements over thebaseline.
These improvements are statisticallysignificant according to a previous study (Zhanget.al., 2004), which shows that a 2% improve-ment in NIST score and a 5% improvement inBleu score is significant for our translation sys-tem on the June 2002 test data.6.3 Mean Reciprocal RankTo further investigate the effects of the rescor-ing function in (11), Mean Reciprocal Rank(MRR) experiments were carried out.
MRR for alabeled set is the mean of the reciprocal rank ofthe individual phrase pair, at which the best can-didate translation is found (Kantor and Voorhees,1996).Totally 9,641 phrase pairs were selected con-taining 216 distinct source phrases.
Each sourcephrase was labeled with its best translation can-didate without ambiguity.
The rank of the la-beled candidate is calculated according totranslation scores.
The results are shown in Ta-ble-4.baseline cosine bm25MRR 0.40 0.58 0.75Table-4 Mean Reciprocal RankThe rescore functions improve the MRR from0.40 to 0.58 using cosine distance, and to 0.75using bm25.
This confirms our intuitions thatgood translation candidates move up in the rankafter the rescoring.Decoder settings without word reordering with word reorderingbaseline bm25 baseline bm25 Scores (%) NIST Bleu NIST Bleu NIST Bleu NIST BleuLDC 7.08 14.84 7.21 15.64 7.13 15.10 7.26 15.98LDC+ISA 7.73 19.60 7.99 19.58 7.86 20.80 8.13 20.93LDC+ISA+HMM 7.86 19.08 8.14 20.70 7.95 19.84 8.19 21.60LDC+ISA+HMM+BiBr 7.87 19.23 8.14 21.48 7.99 20.01 8.24 22.37Table-3 Translation using bm25 rescore function with different decoder settings7 Conclusion and DiscussionIn this work, we proposed a way of using termweight based models in a vector space as addi-tional evidences for translation quality, and inte-grated the model into an existing phrase-basedstatistical machine translation system.
The mod-el shows significant improvements when using itto score a manual dictionary as well as when us-ing different phrase transducers or a combinationof all available translation information.
Addi-tional experiments also confirmed the effective-ness of the proposed model in terms of ofimproved Mean Reciprocal Rank of good transla-tions.Our future work is to explore alternatives suchas the reranking work in (Collins, 2002) and in-clude more knowledge such as syntax informa-tion in rescoring the phrase translation pairs.ReferencesA.
Stolcke.
2002.
SRILM -- An Extensible LanguageModeling Toolkit.
In 2002 Proc.
Intl.
Conf.
onSpoken Language Processing, Denver.Peter F. Brown, Stephen A. Della Pietra, VincentJ.Della Pietra, and Robert L. Mercer.
1993.
TheMathematics of Statistical Machine Translation:Parameter Estimation, Computational Linguistics,vol.
19, no.
2, pp.
263?311.Michael Collins.
2000.
Discriminative Reranking forNatural Language Parsing.
Proc.
17th InternationalConf.
on Machine Learning.
pp.
175-182.William A. Gale and Kenneth W. Church.
1991.
AProgram for Aligning Sentences in Bilingual Cor-pora.
In Computational Linguistics, vol.19 pp.
75-102.Paul B. Kantor, Ellen Voorhees.
1996.
Report on theTREC-5 Confusion Track.
The Fifth Text RetrievalConference.Philipp Koehn, Franz Josef Och, and Daniel Marcu.2003.
Statistical Phrase-Based Translation.
Pro-ceedings of HLT-NAACL.
Edmonton, Canada.Daniel Marcu and William Wong.
2002.
A Phrase-Based, Joint Probability Model for Statistical Ma-chine Translation.
Proceedings of EMNLP-2002,Philadelphia, PA.Franz Josef Och and Hermann Ney.
2000.
ImprovedStatistical Alignment Models.
Proceedings of ACL-00, pp.
440-447, Hongkong, China.Thomas Roelleke.
2003.
A Frequency-based and aPoisson-based Definition of the Probability of Be-ing Informative.
Proceedings of the 26th annual in-ternational ACM SIGIR.
pp.
227-234.S.E.
Robertson, and S. Walker.
1997.
On relevanceweights with little relevance information.
In 1997Proceedings of ACM SIGIR.
pp.
16-24.Dekai Wu.
1997.
Stochastic inversion transductiongrammars and bilingual parsing of parallel corpora,Computational Linguistics 23(3): pp.377-404.K.
Yamada and K. Knight.
2001.
A Syntax-BasedStatistical Translation Model.
Proceedings of the39th Annual Meeting of the Association for Compu-tational Linguistics.
pp.
523-529.
Toulouse, France.Richard Zens, Franz Josef Och and Hermann Ney.2002.
Phrase-Based Statistical Machine Transla-tion.
Proceedings of the 25th Annual German Con-ference on AI: Advances in Artificial Intelligence.pp.
18-22.Stephan Vogel, Hermann Ney, Christian Tillmann.1996.
HMM-based word alignment in statisticaltranslation.
In: COLING '96: The 16th Int.
Conf.
onComputational Linguistics, Copenhagen, Denmark(1996) pp.
836-841.Stephan Vogel, Ying Zhang, Fei Huang, Alicia Trib-ble, Ashish Venogupal, Bing Zhao, Alex Waibel.2003.
The CMU Statistical Translation System,Proceedings of MT-Summit IX.
New Orleans, LA.Stephan Vogel.
2003.
SMT decoder dissected: wordreordering, In 2003 Proceedings of Natural Lan-guage Processing and Knowledge Engineering,(NLP-KE'03) Beijing, China.Stephan Vogel.
2004.
Augmenting Manual Dictionar-ies for Statistical Machine Translation Systems, In2003 Proceedings of LREC, Lisbon, Portugal.
pp.1593-1596.Ying Zhang, Stephan Vogel, Alex Waibel.
2004.
In-terpreting BLEU/NIST Scores : How much Im-provement Do We Need to Have a Better System?In 2004 Proceedings of LREC, Lisbon, Portugal.pp.
2051-2054.Ying Zhang, Stephan Vogel, Alex Waibel.
2003.
"In-tegrated Phrase Segmentation and Alignment Algo-rithm for Statistical Machine Translation," in theProceedings of NLP-KE'03, Beijing, China.Bing Zhao, Stephan Vogel.
2002.
Adaptative ParallelSentences Mining from web bilingual news collec-tion.
In 2002 IEEE International Conference onData Mining, Maebashi City, Japan.
