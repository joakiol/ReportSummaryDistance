Proceedings of the 43rd Annual Meeting of the ACL, pages 91?98,Ann Arbor, June 2005. c?2005 Association for Computational LinguisticsOnline Large-Margin Training of Dependency ParsersRyan McDonald Koby Crammer Fernando PereiraDepartment of Computer and Information ScienceUniversity of PennsylvaniaPhiladelphia, PA{ryantm,crammer,pereira}@cis.upenn.eduAbstractWe present an effective training al-gorithm for linearly-scored dependencyparsers that implements online large-margin multi-class training (Crammer andSinger, 2003; Crammer et al, 2003) ontop of efficient parsing techniques for de-pendency trees (Eisner, 1996).
The trainedparsers achieve a competitive dependencyaccuracy for both English and Czech withno language specific enhancements.1 IntroductionResearch on training parsers from annotated datahas for the most part focused on models and train-ing algorithms for phrase structure parsing.
Thebest phrase-structure parsing models represent gen-eratively the joint probability P (x,y) of sentencex having the structure y (Collins, 1999; Charniak,2000).
Generative parsing models are very conve-nient because training consists of computing proba-bility estimates from counts of parsing events in thetraining set.
However, generative models make com-plicated and poorly justified independence assump-tions and estimations, so we might expect better per-formance from discriminatively trained models, ashas been shown for other tasks like document classi-fication (Joachims, 2002) and shallow parsing (Shaand Pereira, 2003).
Ratnaparkhi?s conditional max-imum entropy model (Ratnaparkhi, 1999), trainedto maximize conditional likelihood P (y|x) of thetraining data, performed nearly as well as generativemodels of the same vintage even though it scoresparsing decisions in isolation and thus may sufferfrom the label bias problem (Lafferty et al, 2001).Discriminatively trained parsers that score entiretrees for a given sentence have only recently beeninvestigated (Riezler et al, 2002; Clark and Curran,2004; Collins and Roark, 2004; Taskar et al, 2004).The most likely reason for this is that discrimina-tive training requires repeatedly reparsing the train-ing corpus with the current model to determine theparameter updates that will improve the training cri-terion.
The reparsing cost is already quite highfor simple context-free models with O(n3) parsingcomplexity, but it becomes prohibitive for lexical-ized grammars with O(n5) parsing complexity.Dependency trees are an alternative syntactic rep-resentation with a long history (Hudson, 1984).
De-pendency trees capture important aspects of func-tional relationships between words and have beenshown to be useful in many applications includ-ing relation extraction (Culotta and Sorensen, 2004),paraphrase acquisition (Shinyama et al, 2002) andmachine translation (Ding and Palmer, 2005).
Yet,they can be parsed in O(n3) time (Eisner, 1996).Therefore, dependency parsing is a potential ?sweetspot?
that deserves investigation.
We focus here onprojective dependency trees in which a word is theparent of all of its arguments, and dependencies arenon-crossing with respect to word order (see Fig-ure 1).
However, there are cases where crossingdependencies may occur, as is the case for Czech(Hajic?, 1998).
Edges in a dependency tree may betyped (for instance to indicate grammatical func-tion).
Though we focus on the simpler non-typed91root John hit the ball with the batFigure 1: An example dependency tree.case, all algorithms are easily extendible to typedstructures.The following work on dependency parsing ismost relevant to our research.
Eisner (1996) gavea generative model with a cubic parsing algorithmbased on an edge factorization of trees.
Yamada andMatsumoto (2003) trained support vector machines(SVM) to make parsing decisions in a shift-reducedependency parser.
As in Ratnaparkhi?s parser, theclassifiers are trained on individual decisions ratherthan on the overall quality of the parse.
Nivre andScholz (2004) developed a history-based learningmodel.
Their parser uses a hybrid bottom-up/top-down linear-time heuristic parser and the ability tolabel edges with semantic types.
The accuracy oftheir parser is lower than that of Yamada and Mat-sumoto (2003).We present a new approach to training depen-dency parsers, based on the online large-marginlearning algorithms of Crammer and Singer (2003)and Crammer et al (2003).
Unlike the SVMparser of Yamada and Matsumoto (2003) and Ratna-parkhi?s parser, our parsers are trained to maximizethe accuracy of the overall tree.Our approach is related to those of Collins andRoark (2004) and Taskar et al (2004) for phrasestructure parsing.
Collins and Roark (2004) pre-sented a linear parsing model trained with an aver-aged perceptron algorithm.
However, to use parsefeatures with sufficient history, their parsing algo-rithm must prune heuristically most of the possibleparses.
Taskar et al (2004) formulate the parsingproblem in the large-margin structured classificationsetting (Taskar et al, 2003), but are limited to pars-ing sentences of 15 words or less due to computationtime.
Though these approaches represent good firststeps towards discriminatively-trained parsers, theyhave not yet been able to display the benefits of dis-criminative training that have been seen in named-entity extraction and shallow parsing.Besides simplicity, our method is efficient and ac-curate, as we demonstrate experimentally on Englishand Czech treebank data.2 System Description2.1 Definitions and BackgroundIn what follows, the generic sentence is denoted byx (possibly subscripted); the ith word of x is de-noted by xi.
The generic dependency tree is denotedby y.
If y is a dependency tree for sentence x, wewrite (i, j) ?
y to indicate that there is a directededge from word xi to word xj in the tree, that is, xiis the parent of xj .
T = {(xt,yt)}Tt=1 denotes thetraining data.We follow the edge based factorization method ofEisner (1996) and define the score of a dependencytree as the sum of the score of all edges in the tree,s(x,y) =?
(i,j)?ys(i, j) =?
(i,j)?yw ?
f(i, j)where f(i, j) is a high-dimensional binary featurerepresentation of the edge from xi to xj .
For exam-ple, in the dependency tree of Figure 1, the followingfeature would have a value of 1:f(i, j) ={1 if xi=?hit?
and xj=?ball?0 otherwise.In general, any real-valued feature may be used, butwe use binary features for simplicity.
The featureweights in the weight vector w are the parametersthat will be learned during training.
Our training al-gorithms are iterative.
We denote by w(i) the weightvector after the ith training iteration.Finally we define dt(x) as the set of possi-ble dependency trees for the input sentence x andbestk(x; w) as the set of k dependency trees in dt(x)that are given the highest scores by weight vector w,with ties resolved by an arbitrary but fixed rule.Three basic questions must be answered for mod-els of this form: how to find the dependency tree ywith highest score for sentence x; how to learn anappropriate weight vector w from the training data;and finally, what feature representation f(i, j) shouldbe used.
The following sections address each ofthese questions.2.2 Parsing AlgorithmGiven a feature representation for edges and aweight vector w, we seek the dependency tree or92h1 h1 h2 h2?s h1 h1 r r+1 h2 h2 th1h1 h2 h2?s h1 h1 h2 h2 th1h1s h1 h1 tFigure 2: O(n3) algorithm of Eisner (1996), needs to keep 3 indices at any given stage.trees that maximize the score function, s(x,y).
Theprimary difficulty is that for a given sentence oflength n there are exponentially many possible de-pendency trees.
Using a slightly modified version ofa lexicalized CKY chart parsing algorithm, it is pos-sible to generate and represent these sentences in aforest that is O(n5) in size and takes O(n5) time tocreate.Eisner (1996) made the observation that if thehead of each chart item is on the left or right periph-ery, then it is possible to parse in O(n3).
The idea isto parse the left and right dependents of a word inde-pendently and combine them at a later stage.
This re-moves the need for the additional head indices of theO(n5) algorithm and requires only two additionalbinary variables that specify the direction of the item(either gathering left dependents or gathering rightdependents) and whether an item is complete (avail-able to gather more dependents).
Figure 2 showsthe algorithm schematically.
As with normal CKYparsing, larger elements are created bottom-up frompairs of smaller elements.Eisner showed that his algorithm is sufficient forboth searching the space of dependency parses and,with slight modification, finding the highest scoringtree y for a given sentence x under the edge fac-torization assumption.
Eisner and Satta (1999) givea cubic algorithm for lexicalized phrase structures.However, it only works for a limited class of lan-guages in which tree spines are regular.
Further-more, there is a large grammar constant, which istypically in the thousands for treebank parsers.2.3 Online LearningFigure 3 gives pseudo-code for the generic onlinelearning setting.
A single training instance is con-sidered on each iteration, and parameters updatedby applying an algorithm-specific update rule to theinstance under consideration.
The algorithm in Fig-ure 3 returns an averaged weight vector: an auxil-iary weight vector v is maintained that accumulatesTraining data: T = {(xt, yt)}Tt=11.
w0 = 0; v = 0; i = 02. for n : 1..N3.
for t : 1..T4.
w(i+1) = update w(i) according to instance (xt, yt)5. v = v + w(i+1)6. i = i + 17. w = v/(N ?
T )Figure 3: Generic online learning algorithm.the values of w after each iteration, and the returnedweight vector is the average of all the weight vec-tors throughout training.
Averaging has been shownto help reduce overfitting (Collins, 2002).2.3.1 MIRACrammer and Singer (2001) developed a naturalmethod for large-margin multi-class classification,which was later extended by Taskar et al (2003) tostructured classification:min ?w?s.t.
s(x,y) ?
s(x,y?)
?
L(y,y?)?
(x,y) ?
T , y?
?
dt(x)where L(y,y?)
is a real-valued loss for the tree y?relative to the correct tree y.
We define the loss ofa dependency tree as the number of words that havethe incorrect parent.
Thus, the largest loss a depen-dency tree can have is the length of the sentence.Informally, this update looks to create a marginbetween the correct dependency tree and each incor-rect dependency tree at least as large as the loss ofthe incorrect tree.
The more errors a tree has, thefarther away its score will be from the score of thecorrect tree.
In order to avoid a blow-up in the normof the weight vector we minimize it subject to con-straints that enforce the desired margin between thecorrect and incorrect trees1.1The constraints may be unsatisfiable, in which case we canrelax them with slack variables as in SVM training.93The Margin Infused Relaxed Algorithm(MIRA) (Crammer and Singer, 2003; Cram-mer et al, 2003) employs this optimization directlywithin the online framework.
On each update,MIRA attempts to keep the norm of the change tothe parameter vector as small as possible, subject tocorrectly classifying the instance under considera-tion with a margin at least as large as the loss of theincorrect classifications.
This can be formalized bysubstituting the following update into line 4 of thegeneric online algorithm,min?
?w(i+1) ?
w(i)??s.t.
s(xt,yt) ?
s(xt,y?)
?
L(yt,y?)?y?
?
dt(xt)(1)This is a standard quadratic programming prob-lem that can be easily solved using Hildreth?s al-gorithm (Censor and Zenios, 1997).
Crammer andSinger (2003) and Crammer et al (2003) providean analysis of both the online generalization errorand convergence properties of MIRA.
In equation(1), s(x,y) is calculated with respect to the weightvector after optimization, w(i+1).To apply MIRA to dependency parsing, we cansimply see parsing as a multi-class classificationproblem in which each dependency tree is one ofmany possible classes for a sentence.
However, thatinterpretation fails computationally because a gen-eral sentence has exponentially many possible de-pendency trees and thus exponentially many marginconstraints.To circumvent this problem we make the assump-tion that the constraints that matter for large marginoptimization are those involving the incorrect treesy?
with the highest scores s(x,y?).
The resultingoptimization made by MIRA (see Figure 3, line 4)would then be:min?
?w(i+1) ?
w(i)??s.t.
s(xt,yt) ?
s(xt,y?)
?
L(yt,y?)?y?
?
bestk(xt; w(i))reducing the number of constraints to the constant k.We tested various values of k on a development dataset and found that small values of k are sufficient toachieve close to best performance, justifying our as-sumption.
In fact, as k grew we began to observe aslight degradation of performance, indicating someoverfitting to the training data.
All the experimentspresented here use k = 5.
The Eisner (1996) algo-rithm can be modified to find the k-best trees whileonly adding an additional O(k log k) factor to theruntime (Huang and Chiang, 2005).A more common approach is to factor the struc-ture of the output space to yield a polynomial set oflocal constraints (Taskar et al, 2003; Taskar et al,2004).
One such factorization for dependency treesismin?
?w(i+1) ?
w(i)??s.t.
s(l, j) ?
s(k, j) ?
1?
(l, j) ?
yt, (k, j) /?
ytIt is trivial to show that if these O(n2) constraintsare satisfied, then so are those in (1).
We imple-mented this model, but found that the required train-ing time was much larger than the k-best formu-lation and typically did not improve performance.Furthermore, the k-best formulation is more flexi-ble with respect to the loss function since it does notassume the loss function can be factored into a sumof terms for each dependency.2.4 Feature SetFinally, we need a suitable feature representationf(i, j) for each dependency.
The basic features inour model are outlined in Table 1a and b.
All fea-tures are conjoined with the direction of attachmentas well as the distance between the two words beingattached.
These features represent a system of back-off from very specific features over words and part-of-speech tags to less sparse features over just part-of-speech tags.
These features are added for both theentire words as well as the 5-gram prefix if the wordis longer than 5 characters.Using just features over the parent-child nodepairs in the tree was not enough for high accuracy,because all attachment decisions were made outsideof the context in which the words occurred.
To solvethis problem, we added two other types of features,which can be seen in Table 1c.
Features of the firsttype look at words that occur between a child andits parent.
These features take the form of a POStrigram: the POS of the parent, of the child, and ofa word in between, for all words linearly betweenthe parent and the child.
This feature was particu-larly helpful for nouns identifying their parent, since94a)Basic Uni-gram Featuresp-word, p-posp-wordp-posc-word, c-posc-wordc-posb)Basic Big-ram Featuresp-word, p-pos, c-word, c-posp-pos, c-word, c-posp-word, c-word, c-posp-word, p-pos, c-posp-word, p-pos, c-wordp-word, c-wordp-pos, c-posc)In Between POS Featuresp-pos, b-pos, c-posSurrounding Word POS Featuresp-pos, p-pos+1, c-pos-1, c-posp-pos-1, p-pos, c-pos-1, c-posp-pos, p-pos+1, c-pos, c-pos+1p-pos-1, p-pos, c-pos, c-pos+1Table 1: Features used by system.
p-word: word of parent node in dependency tree.
c-word: word of childnode.
p-pos: POS of parent node.
c-pos: POS of child node.
p-pos+1: POS to the right of parent in sentence.p-pos-1: POS to the left of parent.
c-pos+1: POS to the right of child.
c-pos-1: POS to the left of child.b-pos: POS of a word in between parent and child nodes.it would typically rule out situations when a nounattached to another noun with a verb in between,which is a very uncommon phenomenon.The second type of feature provides the local con-text of the attachment, that is, the words before andafter the parent-child pair.
This feature took the formof a POS 4-gram: The POS of the parent, child,word before/after parent and word before/after child.The system also used back-off features to various tri-grams where one of the local context POS tags wasremoved.
Adding these two features resulted in alarge improvement in performance and brought thesystem to state-of-the-art accuracy.2.5 System SummaryBesides performance (see Section 3), the approachto dependency parsing we described has severalother advantages.
The system is very general andcontains no language specific enhancements.
In fact,the results we report for English and Czech use iden-tical features, though are obviously trained on differ-ent data.
The online learning algorithms themselvesare intuitive and easy to implement.The efficient O(n3) parsing algorithm of Eisnerallows the system to search the entire space of de-pendency trees while parsing thousands of sentencesin a few minutes, which is crucial for discriminativetraining.
We compare the speed of our model to astandard lexicalized phrase structure parser in Sec-tion 3.1 and show a significant improvement in pars-ing times on the testing data.The major limiting factor of the system is its re-striction to features over single dependency attach-ments.
Often, when determining the next depen-dent for a word, it would be useful to know previ-ous attachment decisions and incorporate these intothe features.
It is fairly straightforward to modifythe parsing algorithm to store previous attachments.However, any modification would result in an as-ymptotic increase in parsing complexity.3 ExperimentsWe tested our methods experimentally on the Eng-lish Penn Treebank (Marcus et al, 1993) and on theCzech Prague Dependency Treebank (Hajic?, 1998).All experiments were run on a dual 64-bit AMDOpteron 2.4GHz processor.To create dependency structures from the PennTreebank, we used the extraction rules of Yamadaand Matsumoto (2003), which are an approximationto the lexicalization rules of Collins (1999).
We splitthe data into three parts: sections 02-21 for train-ing, section 22 for development and section 23 forevaluation.
Currently the system has 6, 998, 447 fea-tures.
Each instance only uses a tiny fraction of thesefeatures making sparse vector calculations possible.Our system assumes POS tags as input and uses thetagger of Ratnaparkhi (1996) to provide tags for thedevelopment and evaluation sets.Table 2 shows the performance of the systemsthat were compared.
Y&M2003 is the SVM-shift-reduce parsing model of Yamada and Matsumoto(2003), N&S2004 is the memory-based learner ofNivre and Scholz (2004) and MIRA is the the sys-tem we have described.
We also implemented an av-eraged perceptron system (Collins, 2002) (anotheronline learning algorithm) for comparison.
This ta-ble compares only pure dependency parsers that do95English CzechAccuracy Root Complete Accuracy Root CompleteY&M2003 90.3 91.6 38.4 - - -N&S2004 87.3 84.3 30.4 - - -Avg.
Perceptron 90.6 94.0 36.5 82.9 88.0 30.3MIRA 90.9 94.2 37.5 83.3 88.6 31.3Table 2: Dependency parsing results for English and Czech.
Accuracy is the number of words that correctlyidentified their parent in the tree.
Root is the number of trees in which the root word was correctly identified.For Czech this is f-measure since a sentence may have multiple roots.
Complete is the number of sentencesfor which the entire dependency tree was correct.not exploit phrase structure.
We ensured that thegold standard dependencies of all systems comparedwere identical.Table 2 shows that the model described here per-forms as well or better than previous comparablesystems, including that of Yamada and Matsumoto(2003).
Their method has the potential advantagethat SVM batch training takes into account all ofthe constraints from all training instances in the op-timization, whereas online training only considersconstraints from one instance at a time.
However,they are fundamentally limited by their approximatesearch algorithm.
In contrast, our system searchesthe entire space of dependency trees and most likelybenefits greatly from this.
This difference is am-plified when looking at the percentage of trees thatcorrectly identify the root word.
The models thatsearch the entire space will not suffer from bad ap-proximations made early in the search and thus aremore likely to identify the correct root, whereas theapproximate algorithms are prone to error propaga-tion, which culminates with attachment decisions atthe top of the tree.
When comparing the two onlinelearning models, it can be seen that MIRA outper-forms the averaged perceptron method.
This differ-ence is statistically significant, p < 0.005 (McNe-mar test on head selection accuracy).In our Czech experiments, we used the depen-dency trees annotated in the Prague Treebank, andthe predefined training, development and evaluationsections of this data.
The number of sentences inthis data set is nearly twice that of the English tree-bank, leading to a very large number of features ?13, 450, 672.
But again, each instance uses just ahandful of these features.
For POS tags we used theautomatically generated tags in the data set.
Thoughwe made no language specific model changes, wedid need to make some data specific changes.
In par-ticular, we used the method of Collins et al (1999) tosimplify part-of-speech tags since the rich tags usedby Czech would have led to a large but rarely seenset of POS features.The model based on MIRA also performs well onCzech, again slightly outperforming averaged per-ceptron.
Unfortunately, we do not know of any otherparsing systems tested on the same data set.
TheCzech parser of Collins et al (1999) was run on adifferent data set and most other dependency parsersare evaluated using English.
Learning a model fromthe Czech training data is somewhat problematicsince it contains some crossing dependencies whichcannot be parsed by the Eisner algorithm.
One trickis to rearrange the words in the training set so thatall trees are nested.
This at least allows the train-ing algorithm to obtain reasonably low error on thetraining set.
We found that this did improve perfor-mance slightly to 83.6% accuracy.3.1 Lexicalized Phrase Structure ParsersIt is well known that dependency trees extractedfrom lexicalized phrase structure parsers (Collins,1999; Charniak, 2000) typically are more accuratethan those produced by pure dependency parsers(Yamada and Matsumoto, 2003).
We comparedour system to the Bikel re-implementation of theCollins parser (Bikel, 2004; Collins, 1999) trainedwith the same head rules of our system.
There aretwo ways to extract dependencies from lexicalizedphrase structure.
The first is to use the automaticallygenerated dependencies that are explicit in the lex-icalization of the trees, we call this system Collins-auto.
The second is to take just the phrase structureoutput of the parser and run the automatic head rulesover it to extract the dependencies, we call this sys-96EnglishAccuracy Root Complete Complexity TimeCollins-auto 88.2 92.3 36.1 O(n5) 98m 21sCollins-rules 91.4 95.1 42.6 O(n5) 98m 21sMIRA-Normal 90.9 94.2 37.5 O(n3) 5m 52sMIRA-Collins 92.2 95.8 42.9 O(n5) 105m 08sTable 3: Results comparing our system to those based on the Collins parser.
Complexity represents thecomputational complexity of each parser and Time the CPU time to parse sec.
23 of the Penn Treebank.tem Collins-rules.
Table 3 shows the results compar-ing our system, MIRA-Normal, to the Collins parserfor English.
All systems are implemented in Javaand run on the same machine.Interestingly, the dependencies that are automati-cally produced by the Collins parser are worse thanthose extracted statically using the head rules.
Ar-guably, this displays the artificialness of English de-pendency parsing using dependencies automaticallyextracted from treebank phrase-structure trees.
Oursystem falls in-between, better than the automati-cally generated dependency trees and worse than thehead-rule extracted trees.Since the dependencies returned from our systemare better than those actually learnt by the Collinsparser, one could argue that our model is actu-ally learning to parse dependencies more accurately.However, phrase structure parsers are built to max-imize the accuracy of the phrase structure and uselexicalization as just an additional source of infor-mation.
Thus it is not too surprising that the de-pendencies output by the Collins parser are not asaccurate as our system, which is trained and built tomaximize accuracy on dependency trees.
In com-plexity and run-time, our system is a huge improve-ment over the Collins parser.The final system in Table 3 takes the output ofCollins-rules and adds a feature to MIRA-Normalthat indicates for given edge, whether the Collinsparser believed this dependency actually exists, wecall this system MIRA-Collins.
This is a well knowndiscriminative training trick ?
using the sugges-tions of a generative system to influence decisions.This system can essentially be considered a correc-tor of the Collins parser and represents a significantimprovement over it.
However, there is an addedcomplexity with such a model as it requires the out-put of the O(n5) Collins parser.k=1 k=2 k=5 k=10 k=20Accuracy 90.73 90.82 90.88 90.92 90.91Train Time 183m 235m 627m 1372m 2491mTable 4: Evaluation of k-best MIRA approximation.3.2 k-best MIRA ApproximationOne question that can be asked is how justifiable isthe k-best MIRA approximation.
Table 4 indicatesthe accuracy on testing and the time it took to trainmodels with k = 1, 2, 5, 10, 20 for the English dataset.
Even though the parsing algorithm is propor-tional to O(k log k), empirically, the training timesscale linearly with k. Peak performance is achievedvery early with a slight degradation around k=20.The most likely reason for this phenomenon is thatthe model is overfitting by ensuring that even un-likely trees are separated from the correct tree pro-portional to their loss.4 SummaryWe described a successful new method for trainingdependency parsers.
We use simple linear parsingmodels trained with margin-sensitive online trainingalgorithms, achieving state-of-the-art performancewith relatively modest training times and no needfor pruning heuristics.
We evaluated the system onboth English and Czech data to display state-of-the-art performance without any language specific en-hancements.
Furthermore, the model can be aug-mented to include features over lexicalized phrasestructure parsing decisions to increase dependencyaccuracy over those parsers.We plan on extending our parser in two ways.First, we would add labels to dependencies to rep-resent grammatical roles.
Those labels are very im-portant for using parser output in tasks like infor-mation extraction or machine translation.
Second,97we are looking at model extensions to allow non-projective dependencies, which occur in languagessuch as Czech, German and Dutch.Acknowledgments: We thank Jan Hajic?
for an-swering queries on the Prague treebank, and JoakimNivre for providing the Yamada and Matsumoto(2003) head rules for English that allowed for a di-rect comparison with our systems.
This work wassupported by NSF ITR grants 0205456, 0205448,and 0428193.ReferencesD.M.
Bikel.
2004.
Intricacies of Collins parsing model.Computational Linguistics.Y.
Censor and S.A. Zenios.
1997.
Parallel optimization :theory, algorithms, and applications.
Oxford Univer-sity Press.E.
Charniak.
2000.
A maximum-entropy-inspired parser.In Proc.
NAACL.S.
Clark and J.R. Curran.
2004.
Parsing the WSJ usingCCG and log-linear models.
In Proc.
ACL.M.
Collins and B. Roark.
2004.
Incremental parsing withthe perceptron algorithm.
In Proc.
ACL.M.
Collins, J.
Hajic?, L. Ramshaw, and C. Tillmann.
1999.A statistical parser for Czech.
In Proc.
ACL.M.
Collins.
1999.
Head-Driven Statistical Models forNatural Language Parsing.
Ph.D. thesis, Universityof Pennsylvania.M.
Collins.
2002.
Discriminative training methods forhidden Markov models: Theory and experiments withperceptron algorithms.
In Proc.
EMNLP.K.
Crammer and Y.
Singer.
2001.
On the algorithmicimplementation of multiclass kernel based vector ma-chines.
JMLR.K.
Crammer and Y.
Singer.
2003.
Ultraconservative on-line algorithms for multiclass problems.
JMLR.K.
Crammer, O. Dekel, S. Shalev-Shwartz, and Y. Singer.2003.
Online passive aggressive algorithms.
In Proc.NIPS.A.
Culotta and J. Sorensen.
2004.
Dependency tree ker-nels for relation extraction.
In Proc.
ACL.Y.
Ding and M. Palmer.
2005.
Machine translation usingprobabilistic synchronous dependency insertion gram-mars.
In Proc.
ACL.J.
Eisner and G. Satta.
1999.
Efficient parsing for bilexi-cal context-free grammars and head-automaton gram-mars.
In Proc.
ACL.J.
Eisner.
1996.
Three new probabilistic models for de-pendency parsing: An exploration.
In Proc.
COLING.J.
Hajic?.
1998.
Building a syntactically annotated cor-pus: The Prague dependency treebank.
Issues of Va-lency and Meaning.L.
Huang and D. Chiang.
2005.
Better k-best parsing.Technical Report MS-CIS-05-08, University of Penn-sylvania.Richard Hudson.
1984.
Word Grammar.
Blackwell.T.
Joachims.
2002.
Learning to Classify Text using Sup-port Vector Machines.
Kluwer.J.
Lafferty, A. McCallum, and F. Pereira.
2001.
Con-ditional random fields: Probabilistic models for seg-menting and labeling sequence data.
In Proc.
ICML.M.
Marcus, B. Santorini, and M. Marcinkiewicz.
1993.Building a large annotated corpus of english: the penntreebank.
Computational Linguistics.J.
Nivre and M. Scholz.
2004.
Deterministic dependencyparsing of english text.
In Proc.
COLING.A.
Ratnaparkhi.
1996.
A maximum entropy model forpart-of-speech tagging.
In Proc.
EMNLP.A.
Ratnaparkhi.
1999.
Learning to parse naturallanguage with maximum entropy models.
MachineLearning.S.
Riezler, T. King, R. Kaplan, R. Crouch, J. Maxwell,and M. Johnson.
2002.
Parsing the Wall Street Journalusing a lexical-functional grammar and discriminativeestimation techniques.
In Proc.
ACL.F.
Sha and F. Pereira.
2003.
Shallow parsing with condi-tional random fields.
In Proc.
HLT-NAACL.Y.
Shinyama, S. Sekine, K. Sudo, and R. Grishman.2002.
Automatic paraphrase acquisition from news ar-ticles.
In Proc.
HLT.B.
Taskar, C. Guestrin, and D. Koller.
2003.
Max-marginMarkov networks.
In Proc.
NIPS.B.
Taskar, D. Klein, M. Collins, D. Koller, and C. Man-ning.
2004.
Max-margin parsing.
In Proc.
EMNLP.H.
Yamada and Y. Matsumoto.
2003.
Statistical depen-dency analysis with support vector machines.
In Proc.IWPT.98
