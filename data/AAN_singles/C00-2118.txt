Automat ic  Lexical Acquisit ion Based on Statistical Distributions*Suzanne StevensonDepar l ;ment of Coml)uter ScienceUldvcrsity of Toronto6 King's College l(oa,d'.l.
'oronto, ON Ca, lla, d~t M5S 311115suzanne~cs, toronto, eduAbstractWe a, ui;omatically cla,ssi(y verbs into lexica,1 se-mantic classes, 1)ased on distributions of indices-tots of verb a.lterna.tions, extra.cCed froln a veryla.rge a.nnota.ted corpus.
We ~ddress a. prol)lemwhich is pa.rticuhtrly difficult 1)eca.use the verl)classes, a.lthough sema.ntica.lly different, show sim-ila.r surface syntactic 1)eha.vior, Five gra.m,na.tica.1fea.l;tlres ~u:e su\[\[icient to reduce error i:ate by moretha.n 50% over cha.nc(,: we a.chieve almost 70%a.ceura.cy in a 1;ask whose baseline perl'ornmn(:e is34%, and whose exl)ert-I)ased Ul)l)er bound we ('aJ-culated a.t 86.5%.
We conclude l;ha.1; corl)us-drivenexl;racl;ion of gramma.1;ical \['eaJ;ures i a. promisinglnethodology for line-grained verb classilica.tion.1 I n t roduct ion\])eta.ileal hfforma.tion a.I)out verbs is critical to a.broad ra.nge of NI,I ) and l i t  1;asks, yet; ils mauus.1 (lel;('rmina.tion for la.rge numl)ers o\[' verl)s isdifficult aml resource intensive.
I{.esea.rch o,I tima.ul,()matic, a(-quisil;ion o\[' verb-I)ased k,owl(~(Ig(,has succeded in gleaning sylH.a.
('l;ic l)rol)erties o\['verl)s such as sul)ca.tegoriza.tion frames from el>line resources (I}rent, \]9!
)3; lh'iscoe a.nd C,a.rroll,1997; \])err, 1997; Ma.nning, \]993), ll,ecently,researchers have investiga?ed statistica.l corpus-ba.sed methods for lexica.l sema.ntic classitica.tionfrom synta.ctic prol)erties of verl) usage (Aone a.ndMcKee, \]996; l,a.pa.ta, and Brew, \]999; Schulte imWa.lde, :1998; Stevenson a.nd Merle.
1999; Steve.n-son et a l., \] 999; McCarthy, 2000).C, orl)us-based al)pro~mhes to lexica.l sema.nticclassitic~tion in pa,rticular ha.ve dra.wn on Levin'shypothesis (I,evin, 1993) that verbs can be classi-lied according to the dia.thesis aJterna.tions (a.lter-nations in the syntactic expressions o\[" a.rguments)ill which they f)articil)a.te l'or exa.mple, whether a.
* This research was partly sponsored 1)y US NSI" grants#9702331 and #9818322, Swiss NSI" Mlowshlp 8210-d65(;9, Information Sciences (.Ollll(il of Hurters Universityand IllCS, U. of Peimsylwmia.
'l'his research was con-ducted wldle the tirst author was at l lutgers University.Pao la  Mer leLAT\ ] , -  \])cpa.rtment of l,inguisCicsUnivcrsil;y of ( leneva2 rue de C'~mdolle121\] Gent;re d -.
SuissemerZo?let t res ,  un?ge, chverb occurs in the dative/prepositiona.l phrase al-terna.tion in l';nglish.
One diagnostic for dis.thesisa.lternations i the sulx'a.tegorization aJternativesof a. verb.
ltowew~,r, some classes exhibit the samesubca.tegoriza.tkm possibilities but differ in theira.rgument structures, i.e.
tim content el' the the-real;it roles assigned to the arguments of l;he verb.rFhis gyl)e of situation consl;itutes a. pa.rticula.rlydifficult case R)r corpus-based classification meth-ods.In this paper, we apply corpus-based lexica.la.cquisition methodology 1;o distinguish classes ofverbs which allow the same subca.tegoriza?ions,but differ in tlteJna.tic roles.
We first a ssu me tha.tone ca.n a.ut;oma.tiea.lly restrict l;he choice o\[' ('lassesto those that; paxl;icil)a.1;e in the relewu~t subcate-gorizations (c\['.
(l,a.p~ta.
and Brew, \]999)).
OurprOl)OSa.1 is lhen to use st.a.tistics ovel: di~Ll;hesisa lterna.nl,s as a, wa.y to \['urther distinguish thoseverl)s wldch allow 1;he same sul)ca.tegoriza.tions;achievi,g fine-grained cla.ssifica.tion within thatS('l,.
()UI' work \['O(tllSeS oll determining tile 1)esl se-ma.nl, ic class for a verl) lgpc - the set of usages o1"a. verl) across a. document or corpus rather t;ha.nfl)r a single verb I, okc n in {~ single local context.In this way, we c~u/ exploit the broad beha.vior o1'the verb a.cross 1;he corl)uS to determine its mostlikely class overall.We investiga.te the proposed a.l)l)rOaCh in an in-del)th case study o\[' the three major classes of o1>tiona.lly inlra,nsitive w, rl)s in English: ullergative,unaccusa,tive, and ol)ject-drop.
More specifically,according to l,evin's classificaJ;ion (l,evin, 1993),the unerga.tives are ma.nner of motion verbs, suchas jump and march; the una.ccusa.tives are verl)sof cha.nge of state,, such a.s open and explode; theobject-drop verbs a.re unexpressed object a.lCerna.-Lion verl)s, such as played a.nd painted.
Theseclasses a.ll supl)ort 1)oth tr~msitive and intra.nsi-1,ire sul)cal,egoriza.tions, I)ut a.re distinguished bythe pal;tern of thema.tic role assignments i,o sub-jecC a.nd object position.
We a.utomatica.lly cla.s-si(y these verbs on the basis of sta.tistical a,p-815proxilnations to syntactic indicators of the under-lying argunlent structures, using numerical fea-tures collected from a large syntactically anno-tated (tagged or parsed) corpus.
We apply ma-chine learning techniques to determine whetherthe fi'equency distribntions of the features, in-dividually or in combination, support automaticclassification of the verbs.
To preview our re-sults, we demonstrate that combining only fivenumerical indicators is sufficient o reduce the er-ro r  rate in this classification task by more than50% over chance.
Specifically, we achieve ahnost7(1% accuracy in a task whose baseline (chance)per\[brmance is 34%, and whose expert-based up-per bound is calculated at 86.5%.
We concludethat a distribution-based method for lexical se-mantic verb classification is a promising avenue ofresearch.2 The  Argument  S t ruc turesOur approach rests on tile hypothesis that, even incases where verb classes cannot be distinguishedby subcategorizations, the frequency distributionsof syntactic indicators can hold clues to the under-lying thematic role differences.
We start here thenwith a description of the subca.tegorizations andthematic role assignments for each of l.he threeverb classes under investigation.As optionally intransitive verbs, each ofthe three classes participates in the transi-tive/intransitive Mternation:Uuergative(la) The horse raced past the barn.
(1 b) The jockey raced the horse past tile barn.Unaccnsative(2a) The butter melted in the pan.
(2b) The cook melted the butter in the pan.Object-drop(3a) The boy washed the hall.
(3b) The boy washed.Unergatives are intransitive action verbs, as in (1),whose transitive form can be the causative coun-terpart of the intransitive form.
In the causativeuse, the semantic argument hat appears as thesubject of the intransitive, as in (la), surfacesas the object of the transitive, as in (lb) (Ilaleand Keyser, 1993).
Unaccusatives are intransitivechange of state verbs, as in (2a); the transitivecounterpart for these verbs exhibits the causativealternation, as in (2b).
Object-drop verbs, as in(3), have a non-causative transitive/intransitivealternation, in which the object is simply optional.Subj ofClasses TransUnergative Causal AgentUnaccusative Causal AgentObject-drop AgentObj of Subj of'rPrans IntransAgent Agent'?heme rI'hemeTheme AgentTable 1: Summary of Thematic Alternations.Each class is distinguished by the content of tilethematic roles assigned by tile verb.
For object-drop verbs, tile subject is all Agent and the op-tional object is a Theme, yielding tile thematicassignments (Agent, Tlmme) and (Agent) for thetransitive and intransitive alternants respectively.Unergatives and uuaccusatives differ \['1"o111 object-drop verbs in participating in the causative alter-nation, and also differ from each other in their corethematic argument.
In an intransitive unerga-live, the subject is an Agent, and in an intran-sitive unaccusative, the subject is a Theme.
Inthe causative transitive form of each, this core se-mantic argument is expressed as the direct object,with the addition of a Causal Agent (the causer ofthe action) as subject in bol;h cases.
The thematicroles assigned, and their mapping to syntactic po-sition, are summarized in Ta.ble 1.3 The  features  for  C lass i f i ca t ionThe key to any automatic lassification task is todetermine a set of' useful fea.tures for discriminat-ing the itenls to be classitied.
In what follows, werefer to the cohnnns of Table 1 to explain \]tow weexpect the thematic distinctions to yield distri-butional features whose frequencies discriminateamong the classes ~t hand.Considering column one of Table 1, onlyunergative and unaccusa.tive rbs assign ~ CausalAgent to the subject of the transitive.
We hy-1)othesize that the causative construction is lin-guistically more complex than the simple argu-ment optionality of object-drop verbs (Stevensonand Merlo, 1.997).
We expect then that object-drop verbs will be more fi:equent in the transi-tive than the other two classes.
Furthernmre, theobject of an unergative verb receives the Agentrole (see the second column of Table 1.
), a linguis-tically marked transitive construction (Stevensonand Merlo, 1997).
We therefore xpect unerga-tives to be quite rare in the transitive, leading toa three-way distinction in transitive usage amongthe three classes.Second, due to the causative alternation of816'l'able 2: The l.i'ea.Cures and 'l'heir F;xpected 13ehaviorTrmlsitiviLy Unaccusativcs and unergativcs have ~, causative transit ive, hence lower transit ive use.
Fur-l;hc'rlnorc, unerga.tivcs ha.re a.n agent.ire object, hence very low transit ive use.Pa.ssivc Voice Passive implies transit ive use, hence correlated with transit ive feature.VBN Tag Passive implies past pa.rt;iciple use (VBN), hence correlated with transit ive (and passive).Causat iv i ty  () l ) ject-drop verbs do not have a. causal agent, hence low "ca.usative" use.
Unergatives arerare in the transit ive, hence low cmlsative use.An imacy  Unaccusatives have a Theme subject in the intransit ive, hence lower use of animal, esubjects.unergatives and nnaccusatives, the l, hematic roleof the subjec~ of the intransitiw,~ is identical tothat of the objecl of the transitiw;, as shown incolumns two and three of Table 1.
C, iven theidentity of thematic role mal)ped to subject andobject positions, we expect to observe the sa.menoun occurring at times a.s subject of the verb,and at other times as object of the verb.
In con-t ras t ,  for object-tirol) verbs, Cite thenm.tic role o\['the sul)ject o17 the intransitive is identical to l;ha{,of the sul)ject of the transitive, not the object ofthe transitive.
Thus, we expect that it will be lesscommon for the same noun to occur in subject andobject position of the same object-drop verb.
Wehypothesize that this pattern of thematic role as-signments will be retlected in difl'erential amountof u~'~age across the classes of the same nouns assubjects and ol)jects for a given verb.
Further-more, since the causative is a transitive use, a.ndthe 1,ra.nsitive use of u nerga.gives i oxpocl;ed to berare., this overlap o(' subjects and ob.iects shouldprimarily distinguish unaccusatives (predicted tohave high overlap of subjects and objects) fromthe other two classes.Finally, considering columns one and three ofTal)le 1, we note that unergative and objecl;-dropverbs assign all agentive role to their subject inboth the transitive and intra.nsitive, while unac-cusatives assign an agentive role to their subjectonly in the tr~msil, ive.
Under the assutnpL ion thatthe intransitive use of' unaccusatives i  not rare, 1we then expect thai, unaccusatives will occur lessoften overall with an agentive subject than theother two verb classes.
On the flu:ther assump-tion that Agents tend to be animate entities moreso than Themes, we expect that unaccusativeswill occur less freqnently with an animate subjectcompared to unergative and object-drop verbs.Note the importance of our use of frequency dis-tributions: the claim is not that only Agents can~This assumpl, ion is based on the linguistic conlplexityof the causative, and borne out in our corpus analysis.be animate, but rather that nouns that receive anAgent role will more often be animate than nounsthat receive a Theme ,'ole.The above interactions between thematic rolesand the syntactic expressions of arguments thuslead to three features whose distrit)utional proper-ties appear promising for distinguishing the verbclasses: transitivity, causativity, and animaey ofsubject.
We also investigate two additionM syn-tactic l'ea.l, ures, the passive voice and tile past pa.r-ticiple POS tag (VI3N).
These features are relatedto the transitive/intransitive Mternal;ion, since apassive use implies a transitive use of the verb,and the nse of passive in turn implies the use ofthe past participle.
Our hyl)ol;hesis is that thesefive features will exhibit distributional differencesin the observed usages of the verbs, which can beused for classifica.tion.
The features and their ex-pected relevance are summarized in '13ble 2.4 Da~a Col lec t ion  and  Ana lys i sWe chose a set of 20 verbs from each of threeclasses.
The complete list of verbs is reported inAppendix A.
Recall that our goal is to achieve afine-grained classification of verbs that exhibit thesame subcategorization frames; thus, tile verbswere chosen because they do not generally showmassive del)artures from the intended verb sense(and usage) in the corpus.
2 In order to simplifytile counting procedure, we included only tile reg-ular ("-ed") simple past/past participle form oftile verb, assuming that this would approximatethe distribution of tile features across all forms ofthe verb.
Finally, as far as we were able giventhe preceding constraints, we selected verbs thatcould occur in the transitive and in the passive.We counted the occurrences of each verb tokenin a transitive or intransitive use (3'RANS), ill a2~l~hough note that there are only 19 unaccusatives be-cause ripped was excluded fl'om the analysis as it occurredmostly in a very different use (ripped off) in the corpusfrom the intended diange of state usage.817passive or active use (PASS), in a past participleor simple past use (VBN), in a causative or non-causative use (tAgS), and with an animate subjector not (ANIM), as described below.
The first threecounts  (TRANS, I'ASS~ VBN) were performed onthe LDC's 65-million word tagged ACL/DCI  cor-pus (Brown, and Wall Street Journal 1987-1989).The last two counts (CAUS and ANIM) were per-formed on a 29-million word parsed corpus (\gallStreet Journal 1988, provided by Michael Collins(Collins, 1997)).
The features were counted asfollows:TaANS: The closest noun following a verb wasconsidered a potential object.
A verb immedi-ately \[bllowed by a potential object was countedas transitive, otherwise as intransitive.pass: A token tagged VBD (the tag for simplepast) was counted as active.
A token tagged VBN(the tag for past participle) was counted as activeif the closest preceding auxiliary was have, and aspassive if the closest preceding auxiliary was be.VBN: The counts tbr VBN/VBI )  were based onthe POS label in the tagged corl)us.Each of the above counts was normalized overall occurrences of tim "-ed" form of the verb, yield-ing a single relative fi:equency measure \['or eachverb for that feature.tags :  For each verl) token, the subject and ob-ject (it' there was one) were extracted from theparsed corpus, and the proportion of overlap be-tween subject and object nouns across all tokensof a verb was calculated.ANIM: To approximate animacy without refer-ence to a resource external to the corpus (suchas WordNet), we count pronouns (other than it)in subject position (cf.
(Aone and McKee, 1996)).The aSSUlnption is that the words I, we, you, .~'tze,he, and theft most often refer to animate entities.We automatically extracted all subject/verb tu-ples, and computed the ratio of occurrences ofpronoun subjects to all subjects for each verb.The aggregate means by class resulting from thecounts above are shown in Table 3.
The distri-butions of each feature are indeed roughly as ex-pected according to the description in Section 3.Unergatives how a very low relative fi'equencyof the TRANS feature, followed by unaccusatives,then object-drop verbs.
Unaccusative verbs showa high frequency of the CAUS feature and a lowfrequency of the ANIM feature compared to theother classes.
Although expected to be a redun-dant indicator of transitivity, pass and VBN doTa.ble 3: Aggregated Relative Frequency Data \['ortile Five Features.
E = unergatives, A = unac-cusatives, O = object-drol)s.ClassEAON MEAN I~ELATIVE FREQUENCYTR, ANS PASS VBN CAUS ANIM20 0.23 0.07 0.21 0.00 0.2519 0.40 0.33 0.65 0.12 0.0720 0.62 0.31 0.65 0.04 0.15not distinguish t)etween unaccusative and object-drop verbs, indicating that their distributions aresensitive to factors we have not yet investigated, a5 Exper iments  in C lass i f icat ionThe frequency distributions of our  features yielda vector for each verb that represents the relativefrequency wdues for the verb on eacln dimension:\[verb, TRANS, PASS, VBN~ CAUS, ANIM, class\]Example: \[opened, .69, .09, .21, .16, .36, unaec\]\?e use the resulting 59 vectors to train an au-tomatic classifier to determine, given a verb thatexhibits transitive~intransitive sttbcategorizationframes, which of the three major lexical semanticclasses of English optionally intransitive verbs itbelongs to.
Note that the baseline (chance) per-Ibrmance in this task is 33.9%, since there are 59vectors and 3 possible classes, with the most coin-men class having 20 verbs.We used the C5.0 machine learning system(tnttp://www.rulequest.com), a newer version ofC4.5 (Quinlan, 1992), which generates decisiontrees and corresponding rule sets from a trainingset of known classifications.
We found little to nodifference in performance between the trees andrule sets, and report only the rule set results.
\?ereport here on experiments using a single hold-out training and testing methodology.
In this ap-proach, we hold out a single verb vector as thetest case, and train the system on the remaining58 cases.
We then test the resulting classifier ontile single hold-out case, recording tile assignedclass for that verb.
This is then repeated for eachof the 59 verbs.
This technique has the benefitof yielding both an overall accuracy rate (whenthe results are averaged across all 59 trials), aswell as providing tile data necessary tbr determin-ing accuracy for each verb class (because we havethe classification of each verb when it is the testcase).
This allows us to evaluate tile contributionaThese observations have been confirmed by t-test.s be-tween feature values for each pair of classes.818%d)le <1:: Percent Accuracy of Verb Clas-sifica,l,ion Task Using \],'eatures in Combina-tion.
T=Tl lANS;  \])=PASS; \ /=VBN;  C=CAUS;An=ANIM.
E=unergatives, A=unaccusatives,O =:ol)ject-dropsPercent Accuracy by ClassAll l E I a I 0 \],'ca.I,1llJes1.
'.I'P VCAn 69.5 85.02.
P V C An 64:.4: 80.03.
TV  C An 71.2 80.04:.
51' P C An 61.0 65.05.
TPVAn 62.7 70.06.
51'PV C 61.0 80.063.2dT.d73.768.463.242.160.065.060.050.055.060.0of individual feal:ures with respect to their effecton the perfornlance of individual classes.We performed experiments on the \['ull sel, of fea-tures, as well a.s each subsel, of fea.l,ures wil,h a. sin-gle f~ture  remow;d, as reported in Table d. Con-sider l;he first column in the ta.ble.
The first lineshows that the overall ~ccuracy for all live featuresis 69.5%, a reduction in tile error ra.te of morethan 50% above the baseline.
The removal o\[" thePASS lea.lure appears to improve performance (row3 of Ta.ble 4).
However, it should be noted thatthis increase in performance results h:oln a singleadditionaJ verb being classified correctly.
The re-ma.ining rows show thal no feal,ure is superflousor hm'mfld as l,he removal of any I'ealure has a.
58% negative elfect on l)erR)rmance.
Coral)arable.accuracies have been demonsl;rated vsing a morethorough cross-validation methodology a.nd usingreel;hods that are, in principle, better a,t takingadva.nl,age of correlated lea,lures (Stevenson andMerle, 1999; Stevenson el.
al., 1999).q'he single hold-out prol,ocol provides new data,fbr analysing the performalme on individual verbsand classes.
The class-by-class accuracies a.reshown in the remaining columns of Ta.ble 4.
\?ecan see clearly thal, using all five features, l,heunergatives are classified with much greater ac-curacy (85%) than l,he UlmCCUsatives and object-drop verbs (63.2% and 60.0% respectively), asshown in the first row.
The rema.ining rows showthat this l)al,tern generally holds \['or l,he subsel,sof features as well, with tire excel)lion of line d.\?hile ful,ure work on our verb classificaliontask will need lo focus on deterlnining featuresthal bel,ter discriminate unaccusative a.nd object-drop verbs, we can ah:eady exclude an explana-tion of the resull,s based simply on l,he wwbs' ortile classes' frequency.
Unergatives have tile low-est average (log) frequency (1.3), but are the bestclassified, while unaccusatives and object-dropsare comparable (a.verage log fi'equency = 2).
If wegroup verbs by frequency, the proportion of errorsto lhe total number of verbs remains fairly simi-lar (freq 1:7  errors/23 verbs; fi:eq.
2 :6  errors/24verbs; freq.
3 :4  errors/10 verbs).
The only verbof frequency 0 is correctly classified, while lhe onlyone with log frequency 4 is not .
In sum, we donot find that more frequent classes or verbs aremore accurately classitied.lmlmrtantly, the experiments also enable us tosee whether the fealures indeed contribute to dis-criminating the classes in the manner predicted inSeclion 3.
The single hold-out results allow us todo 1;his, by comparing the individual class labelsassigned using the full sol, of five features (TIIANS,PASS, VBN, CAUS, ANIM) to the class labels as-signed using each size four subset of features.
Thiscomparison indicates 1;he changes in class labelsl,hat we can a.l,tribul,e to l,he added feature in goingfi'om a size four subset to the full set of features.
(The individual class labels supporling our a.naly-sis below a.re a.vailable from the authors.)
\?e con-cent;rate on tile three main features: CAUS, ANIM,TRANS.
\?e filial thai, the behaviour of lhese fea-l,ures generaJly does conform to our predicl;ions.We expected that TRANS would help make a. three-way distinction among the verb classes.
Whileunergatives are ah:eady accurately classified with-Ollt TRANS, inspection of lhe change in class la.-bels reveals that the addition of TRANS tO tire sel;improves performance on unaccusatives by help-ing to distinguish 1;hem from object-drol)s, llow-ever, in this case, we also observe a loss in pre-cision of unerga.lives, ince some object-drops arenow classitied a.s unergatives.
Moreover, we ex-pected CAUS and ANIM tO be parl,icularly helpfidin identi\['ying unaccus~l,ives, and this is also borneout in our analysis of individual la.bels.
We notethat the increased accuracy from CAUS is primar-ily due to bel,ter disl,inguishing unergatives fromunaccusatives, and l,he increased accura.cy fromAN1M is primarily due go better distinguishing un-accusatives from objecl,-drops.
\?e conclude tha.tthe feal,ures we have devised are successful in clas-siting optionally 1,ra.nsil;ive verbs because they ca.p-lure predicted ifl'erences in underlying argumentst ruct l r re .
44 Matters are more cmnplex with the other two featuresand we arc still interpreting tile results.
Our prediction819Table 5: Pair-wise Agreement (Calculated t)3' theKappa Statistics) of Three Experts (El, E2, h;3)Compared to a Gold Standard (Levin) and to theClassifier (Prog).
Numbers in parentheses are per-centage of verbs on which judges agree.PltOG F.I.
E2 E3El .36 (59)E2 .50 (68) .59 (75)E3 .49 (66) .53 (70) .66 (77)LEWN .54 (69.5) .56 (71.)
.80 (86.5) .74 (83)6 Compar i son  to Exper tsIn order to evaluate the performance of the al-gorithm in practice, we need to compare it to theaccuracy of classification performed by an expert,which gives a realistic upper bound for the task.In (Merle and Stevenson, 2000) we report the re-suits of an experiment that measures experts per-tbrmance and agreement on a classification taskvery similar: to the program we have describedhere.
The results summarised in Table 5 illus-trate the performance of the progra, m. On theone hand, the algorithm does not perform at ex-pert level, as indicated by the fact that, for all ex-perts, the lowest agreement score is with the pro-gram.
On the other: hand, the accuracy achievedby the program of 69.5% is only 1.5% less thanone of the human experts in comparison to tiregold standard.
In fact, if we take the best per-formance achieved by an expert in this task86.5%--as the maximum achievable accuracy inclassification, our algorithm then reduces the er-ror rate over; chance by approximately 68%, a veryrespectable result.7 D iscuss ionThe work here contributes both to general andtechnical issues in automatic lexical acquisition.Firstly, our results confirm the primary role ofargument structure in verb classification.
Our ex-perimental focus is particularly clear in this re-gard because we deM with verbs that are ~Illilli-was that VBN and PASS would behave similarly to TRANS.In fact, PASS is at best unhelpful in classification.
VBNdoes appear to make the expected I.hree-way distinction.The change ill class labels shows that the improvement inperformance with VBN results from better distinguishingunergatives fi'om object-drops, and object-drops from un-accusatives.
The latter is surprising, since analysis of thedata found that the VnN feature values are statistically in-distinc~ for the object-drop and unaccusative classes as awhole.mal pairs" with respect o argument structure.
13yclassif~ying verbs that show the same subcatego-rizations into different classes, we are able to elim-inate one of the confounds in classification workcreated by the fact that subcategorization a d ar-gument structure :M'e largely co-variant.
We caninfer that the accuracy in our classification is dueto argument structure information, a.s subcatego-rization is the same for: all verbs, confirming thatthe con, tent of thematic roles is crucial for clas-sification.
Secondly, our results further supportthe assumption that thematic differences such asthese are apparent not only in differences in sub-categorization frames, but also in differences in(;heir frequencies.
We thus join the many recentresults that all seem to converge in SUl)portingthe view that the relation between lexical syntaxand semantics can be usefully exploited (Aone andMcKee, 1996; l)orr, 1997; Dorr and Jones, 1996;Lapata and Brew, 1999; Schulte im Walde, 1998;Siegel, 1998), especially in a statistical franmwork.Finally, we observe that this information is de-tectable in a corpus and can be learned automat-ically.
Thus we view corpora, especially if an-notated wil;h currently available tools, a.s usefulrepositories of implicit grammars.Technically, our N)proach extends existingcorpus-based learning techniques 1;o a more com-plex lea.ruing problem, in severaJ dimensions.
Ourstatistical apl)roach , which does not require ex-plicit negative xamples, extends ai)l)roaehes thatencode l~evin's alternations directly, as symbolicproperties of a verb (Dorr et al, 1995; l)orr andJones, 1996; l)orr, 1997).
We also extend workusing surface indicators to approximate underly-ing properties.
(Oishi and Matsumoto, 1997) usecase marking particles to approximate graimnat-ical functions, such as subject and object.
Weimprove on this approach by learning argumentstructure properties, which, unlike grammaticalfunctions, are not marked lnorphologically.
Oth-ers have tackled the problem of lexical semanticclassification, as we have, but using only snbeate-gorization frequencies as input data (Lapata andBrew, 1.999; Sehulte im Walde, 1998).
By con-trast, we explicitly address the definition of fea-tures that can tap directly into thematic role dif-ferences that are not reflected in "subcategoriza-tion distinctions.
Finally, when learning of the-matic role assignment has been the explicit goal,the text has been semantically annotated (Web-ster and Marcus, 1989), or external semantic re-820sources ha.re I)een consulted (Ache and McI(ee,19!)6).
We extend these results by showing thatthem;~tic informa,tion can 1)e inducexl from corpusCOtllltS.The exl)er imental  results show that  our methodis l)owerful, and suited to Cite classitica.l;i(m of lex-ica.1 items.
However, we have not yet addressedthe problem of verbs that  can h;~ve mult iple clas-sifications.
We think tha.t many eases of am-1)iguous classification of verb types can 1)e ad-dressed with the notion of intersective sets in-t roduced by (Da.ng et al, 71998).
This is an im-t)ortant concept tha,t l)rOl)OSes tha,t "i'egula, r" a.m-biguity in classifica.tion -i.e., sets of v(;rbs thatha.ve the same mult i -way classitications a~ccordingto (l,evin, 1993) can be captured with a. liner-grained notion of lexical semant ic  classes.
I~x-tending our work to exploit this idea.
requiresonly to define the classes a.pl)ropriately; the ba-sic a.1)t)roac\]l will remain the same.
When we turnto consider ambiguity,  we must a.lso address thel)roblem l;ha.t individual insta.nces of verl)s maycome from diffel:ent classes.
In future research wet)lan to extend our method to the ('\]a.ssificagion fa.mbiguous tokens, by exper iment ing with a. func-t ics  that  combines severaJ sources of information:a bias tbr the verb type (using the cross-corpussta.l;istics we collect), as well as \[~a.tures o\[" the us-age of the insta.nce being classiiiod (cf.
(l,apa.taa,n<l I~rew, t999; Siegel, 199,q)).Re ferencesCllinatsu Aolm and Dot@as Mcl(ec.
1990.
Acquiringpredicate-argument mapping information in multilingualtexts.
In Branimir l\]oguraev and James l)ustetjovsky, ed-itors, Cou)us \])rocessinq.\[or l, cxieal Acq~tisition, pages191-202.
MIq' Press.Michac'\] lh'ent.
1993. l"rom grammar to lexicon: UnSUl)er-vised learning o\[ lexical syntax.
Compatational Linguis-tics, 1912):243 262.
'lk:d Briscoe and ./ohn Cm'roll.
1997.
Automatic extractionof subcategorization from corpora.
In IS"ocs of the I"~HhANLP Co,@rence, pages 356-363.Michael John Collins.
1997.
Three generative, lexicallsedmodels for statistical pro'sing.
In lS"ocs of ACL '97, pages16-23.
Madrid, Spain.Hot Trang Dang, Karin Kipper, Mm'tha l)almer, and.loseph l{osenzweig.
1998.
Investigating regular senseexl, ensions 1oased on intersective 1,evin classes.
\[nProcs of COI, ING-ACL '98, pages 293 299, Montreal,Canada.BOltnie 1)orr attd l)(n,g Jones.
1996.
Role of word sensedisambiguatlon i  lexical acquisition: lhedieting seman-tics from syntactic ues.
In 15"oc.
of UOL1NG'96, pages3;22 327, Col)enhage,l, )enmark.Boluiie l)orr, Joe t~larlliall, and Amy VVeinberg.
1995.\]~l:Oln syntactic encodillgs to thematic ro|es: l}uildinglexical entries for interlingual MT.
Journal o\[ Machine!l'ran.
?lation, 9(3):71- 100.\]\]onnie l)orr.
1997.
I,m-ge-scale dictionary constructionfor foreign language tutoring and inter\]ingual machine.translation.
Machine Translation, 12:1 55.K('.n l\[ale and Jay \](eyser.
1993.
On argument structureand the lexical representation f syntactic relations.
InI(.
\]tale and .\].
l(eyser, editors, The View fl'om lJuilding20, pages 53-110.
MIT Press.Maria Lapata and Chris Brew.
11999.
Using subcategoriza-tion t,o resolve verb class ambiguity.
In Frocs of Joint,5'IGDAT Con\[erence on Empirical Mett, ods in NaturalLangaage, College Park, M\]).Beth Lcvin.
1993.
English Verb Classes and Alternations.University of Chicago Press, Chicago, I\],.Christopher 1).
Manning.
:1993.
Automatic acquisition ofa large subcategorlzation dictionary l>om corpora.
Inl)rocs of A UL'93, pages 235 -242.
Ohio State University.Diana McCarthy.
2000.
Using semantic l)referenee Lo idcn-tit'y verb participation in role switchitlg alternations.
In15"oes of NAA C1,-2000, Seattle, Washington.l)aola Merlo and Sllzantle Stevenson.
2000:-F, stablishingthe upper-bound and inter-judge agremcnt in a verbclassification task.
In l)rocs of LI~EC-2000, pages 16591G64.
Athens, Greece.Akira Oishi and Yuji Matsumoto.
1997. l)etecting the or-ganization of semantic subclasses of Japanese verbs.
In-ternational Journal of Corpus Linguistics, 2(1):65 89.,/.
\]toss Quinlan.
1992.
(?4.5 : l~rograms .for Maehi,zeLearning.
Morgan l(aufmamt, San Mateo, CA.Sabine Sehulte im Walde.
1998.
Automatic semanl.ic las-sification of verbs according to their alternation be-haviour.
AIMS l\]eport 4(3), IMS, UniversitSt Stuttgart.I",eic V. Siegel.
1998.
Linguistic Indicators .for L(mouaffcUndcrslandin 9 l)h.I), thesis, I)ept.
of Comput(w Sci-ence, (Johm|l)ia University.Suzalllle SteveHsoll alld l)aola Merlo.
1997. l~exica\] st.ru(:-turc and \])ro(:essing complexity.
Lanuagc and (7o\[pzilivc\])rocc:,~se.,% 12(1-2):3.
'19 399.Suzanne Stevenson and \])aola Mer\]o.
1999.
Verb classifi-cation using distributions of gt'ammatical features.
Inl)rocs of 1?,4 CL'99.
Bergen, Norway.Suzmme Stevenson, l)aola Merlo, Natalia Kariaeva, andl(amin Whitehouse.
1999.
Supervised learning of lexicalsemantic verb classes using fl'equency distributions.
\[nI)rocs of Si.qLex '99, College Park, Maryland.Mort Webster and Mirth Mm'cus.
1989.
Automatic ac-quisition of the lexical semanl, ics of verbs fl'om sentenceframes.
In Procs of A CL'89, pages 177-184, Vancouver,Canada.Append ix  AUnergatives: floated, galloped, glided, hiked, hopped, hur-ried, jogged , jumped, leaped, marched, paraded, raced,rushcd, seootcd, scurricd, skipped, tiptoed, t~vttcd, va,dtcd,wandered.
Unaccusativcs: boiled, changcd, cleared,collapsed, cooled, cracked, dissolved, divided, exploded,flooded, .folded, fractured, hardcned, melted, opcncd, sim-mcred, solidified, stabilized, widened.
Object-drops: bofrowed, eallcd, earvcd, clca~cd, danced, inheritcd, kiekcd,knittcd, oraniscd, packcd, paintcd, playcd, reaped, rcnted,skclehe.d, studied, swallowed, typed, washcd, ycllcd.821
