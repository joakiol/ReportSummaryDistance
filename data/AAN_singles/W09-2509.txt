Proceedings of the 2009 Workshop on Applied Textual Inference, ACL-IJCNLP 2009, pages 61?69,Suntec, Singapore, 6 August 2009. c?2009 ACL and AFNLPAutomating Model Building in c-raterJana Z. SukkariehEducational Testing ServiceRosedale Road, Princeton, NJ 08541jsukkarieh@ets.orgSvetlana StoyanchevStony Brook UniversityStony Brook, NY, 11794svetastenchikova@gmail.comAbstractc-rater is Educational Testing Service?stechnology for the content scoring of shortstudent responses.
A major step in the scor-ing process is Model Building where vari-ants of model answers are generated thatcorrespond to the rubric for each item or testquestion.
Until recently, Model Buildingwas knowledge-engineered (KE) and hencelabor and time intensive.
In this paper, wedescribe our approach to automating ModelBuilding in c-rater.
We show that c-raterachieves comparable accuracy on automati-cally built and KE models.1 Introductionc-rater (Leacock and Chodorow, 2003) is Edu-cational Testing Service?s (ETS) technologyfor the automatic content scoring of short free-text student answers, ranging in length from afew words to approximately 100 words.
Whileother content scoring systems [e.g., Intelligent.Essay Assessor (Foltz, Laham and Landauer,2003), SEAR (Christie, 1999), IntelliMetric(Vantage Learning Tech, 2000)] take a holis-tic 1  approach, c-rater takes an analytical ap-proach to scoring content.
The item rubricsspecify content in terms of main points or con-cepts required to appear in a student?s correctanswer.
An example of a test question or itemfollows:1 Holistic means an overall score is given for a student?sanswer as opposed to scores for individual components ofa student?s answer.Item 1 (Full credit: 2 points)Stimulus: A Reading passagePrompt:In the space below, write thequestion that Alice was mostlikely trying to answer whenshe performed Step B.Concepts or main/key points:C :1  How does rain forma-tion occur in winter?C : 2 How is rain formed?C : 3 How do temperatureand altitude contributeto the formation ofrain?Scoring rules:2 points for C11 for C2 (only if C1 is not present)1 for C3 (only if C1 and C2 are not present)Otherwise 0We view c-rater's task as a textual entailment(TE) problem.
We use TE here to mean eithera paraphrase or an inference (up to the contextof the item or test question).
c-rater's task isreduced to a TE problem in the following way:Given a concept, C, (e.g., ?body increasesits temperature?)
and a student answer, A,(e.g., either ?the body raises temperature,?
?the body responded.
His temperature was37?
and now it is 38?,?
or ?Max has a fe-ver?)
and the context of the item, the goalis to check whether C is an inference orparaphrase of A (in other words, A impliesC and A is true).There are four main steps in c-rater.
The firstone is Model Building (MB), where a set ofmodel answers are generated (either manuallyor automatically).
Second, c-rater automati-cally processes model answers and students?answers using a set of natural language proc-essing (NLP) tools and extracts the linguisticfeatures.
Third, the matching algorithmGoldmap uses the linguistic features culmi-nated from both MB and NLP to automaticallydetermine whether a student?s response entailsthe expected concepts.
Finally, c-rater applies61the scoring rules to produce a score and feed-back that justifies the score to the student.Until recently, MB was knowledge-engineered(KE).
The KE approach for one item required,on average, 12 hours of time and labor.
Thispaper describes our approach to automatic MB.We show that c-rater achieves comparable ac-curacy on automatically- and manually-builtmodels.
Section 2 outlines others?
work in thisdomain and emphasizes the contribution of thispaper.
Section 3 outlines c-rater.
In Section 4,we describe how MB works.
Section 5 ex-plains how we automate the process.
Prior tothe conclusion, we report the evaluation of thiswork.2 Automatic Content Scoring:Others?
WorkA few systems that deal with both short an-swers and analytic-based content exist.
Thetask, in general, is reduced to comparing a stu-dent?s answer to a model answer.
Recent workby Mohler and Mihalcea (2009) at the Univer-sity of North Texas uses unsupervised methodsin text-to-text semantic similarity comparingunseen students?
answers to one correct an-swer.
Previous work, including c-rater, usedsupervised techniques to compare unseen stu-dents?
answers to the space of potentially ?allpossible correct answers?
specified in the ru-bric of the item at hand.
The techniques variedfrom information extraction with knowledge-engineered patterns representing the modelanswers [Automark at Intelligent AssessmentTechnologies (Mitchell, 2002), the Oxford-UCLES system (Sukkarieh, et.
al., 2003) at theUniversity of Oxford] to data mining tech-niques using very shallow linguistic features[e.g., Sukkarieh and Pulman (2005) and Car-melTC at Carnegie Mellon University (Rose,et al 2003)].
Data mining techniques provednot to be very transparent when digging upjustifications for scores.c-rater?s model building process is similar togenerating patterns but the patterns in c-raterare written in English instead of a formal lan-guage.
The aim of the process is to produce anon-trivial space of possible correct answersguided by a subset of the students?
answers.The motivation is that the best place to look forvariations and refinements for the rubric is thestudents?
answers.
This is what test developersdo before piloting a large-scale exam.
From anNLP point of view, the idea is that generatingthis space will make scoring an unseen answereasier than just having one correct answer.However, similar to what other systems re-ported, generating manually-engineered pat-terns is very costly.
In Sukkarieh et al (2004)there was an attempt to generate patternsautomatically but the results reported were notcomparable to those using manually-generatedpatterns.
This paper presents improvements onprevious supervised approaches by automatingthe process of model-answer building usingwell-known NLP methods and resources whileyielding comparable results to knowledge-engineered methods.3 c-rater, in BriefIn c-rater, manual MB has its own graphicalinterface, Alchemist.
MB uses the NLP toolsand Goldmap (which reside in the c-raterEngine).
On the other hand, Goldmap dependson the model generated.
The c-rater Engineperforms NLP on input text and concept rec-ognition or TE between the input text and eachconcept (see Figure 1).
First, a student answeris processed for spelling corrections in an at-tempt to decrease the noise for subsequentNLP tools.
In the next stage, parts-of-speechtagging and parsing are performed (theOpenNLP parser is usedhttp://opennlp.sourceforge.net).
In the thirdstage, a parse tree is passed through a featureextractor.
Manually-generated rules extractfeatures from the parse tree.
The result is a flatstructure representing phrases, predicates, andrelationships between predicates and entities.Each phrase is annotated with a label indicat-ing whether it is independent or dependent.Each entity is annotated with a syntactic andsemantic role.
In the pronoun resolutionstage, pronouns are resolved to either an entityin the student?s answer or the question.
Finally,a morphology analyzer reduces words to theirlemmas.2 The culmination of the above toolsresults in a set of linguistic features used by thematching algorithm, Goldmap.
In addition tothe item-independent linguistic features col-lected by the NLP tools, Goldmap uses item-dependent features specified in MB to decidewhether a student?s answer, A, and a model2 We do not go into detail, assuming that the reader isfamiliar with the described NLP techniques.62answer match, i.e.
that concept C representedin the model answer, is entailed by A.Figure 1. c-rater Engine4 KE Model BuildingA dataset of student answers for an item is splitinto development (DEV), cross-validation(XVAL), and blind (BLIND) datasets.
DEV isused to build the model, XVAL is used to vali-date it and BLIND is used to evaluate it.
Alldatasets are double-scored holistically by hu-man raters and the scoring process takes anaverage 3 hours per item for a dataset ofroughly 200 answers.For each concept Ci in item X, a model builderuses DEV to create a set of Model Sentences(MSij) that s/he believes entails concept Ci inthe context of the item.
S/he is required towrite MSij in complete sentences.
For eachmodel sentence MSij,, the model builder selectsthe Required Lexicon (RLijk), a set of the mostessential lexical entities required to appear in astudent?s answer.
Then, for each RLijk, themodel builder selects a set of Similar Lexicon(SLijkt), guided by the list of words automati-cally extracted from a dependency-based the-saurus (cs.ualberta.ca/~lindek/downloads.htm).The process is exemplified in Figure 2.
Pre-sented with the concept, ?What causes rain toform in winter time?,?
a model builder writesmodel sentences like ?Why does rain fall inthe winter?,?
highlights or selects lexical itemsthat s/he believes are the required tokens(e.g., ?why,?
?rain,?
?fall,?
?in,?
?winter?
)and writes a list of similar lexical entities foreach required token if needed (e.g., {descend,go~down, ?}
are similar to words like?fall?
).3Figure 2.
KE Model BuildingThe model for each item X is comprised of thescoring rules, the collections of model sen-tences MSij, associated lexical entities RLijk,and corresponding similar lexicon SLijkt.
Eachmodel answer is written in terms of MSijwhere:MSij entails Ci for i=1,?, N, and N is thenumber of concepts specified for item X.For each concept Ci, Goldmap checkswhether answer A entails Ci, by check-ing whether A entails one of the modelsentences MSij, given the additional fea-tures RLijk and corresponding SLijkt.In practice, model building works as follows.The model builder, guided by the DEV datasetand holistic scores, starts with writing a fewmodel sentences and selects correspondingrequired (RLijk) and similar (SLijkt) lexicon.S/he then uses the c-rater engine to automati-cally evaluate the model using the DEV data-set, i.e., using the model produced up to thatpoint.
Goldmap is used to detect if any answersin the DEV dataset contain any of the modelsentences and scores are assigned for each an-swer.
If the scoring agreement between c-raterand each of the two human raters (in terms of akappa statistic) is much lower than that be-tween the two human raters, then the model isjudged unsuitable and the process continuesiteratively until kappa statistics on the DEVdataset are satisfactory, i.e., c-rater?s agree-ment with human raters is as high as the kappabetween human raters.
Once kappa statistics onDEV are satisfactory, the model builder uses3 We use lexicon, lexical entities, words, terms and to-kens interchangeably meaning either uni- or bi-grams.63c-rater to evaluate the model on the XVALdataset automatically.
Again, until the scoringagreement between c-rater and human raterson XVAL dataset is satisfactory, the modelbuilder iteratively changes the model.
Unlikethe DEV dataset, the XVAL dataset is neverseen by a model builder.
The logic here is thatover-fitting DEV is a concern, making it hardor impossible to generalize beyond this set.Hence, the results on XVAL can help preventover-fitting and ideally would predict resultsover unseen data.Note that a model builder can introduce whatwe call a negative concept Ci-1 for a concept Ciand adjust the scoring rules accordingly.
Whenthis happens, a model builder writes modelsentences MSi-1j  entailing Ci-1 , and selects re-quired words RLi-1jk and corresponding similarwords SLi-1jkt  in the same way for any other(positive) concept.On average, MB takes 12 hours of manualwork per item (plus 2 hours, on average, for anoptional model review by someone other thanthe model builder).
This process is time con-suming and error-prone despite utilizing auser-friendly interface like Alchemist.
In addi-tion, the satisfaction criterion while building amodel is subjective to the model builder.5 Automated Model BuildingThe process of writing model sentences de-scribed above involves: 1) finding the parts ofstudents?
answers containing the concept foreach expected concept, 2) abstracting over?similar?
parts, and 3) representing the abstrac-tion in one (or more) model sentence(s).
Theprocess, as mentioned earlier, is similar towriting rules for information extraction, buthere one writes them in English sentences andnot in a formal language.
In practice, there isno mechanism in Alchemist to cluster ?simi-lar?
parts and MB, in this aspect, is not per-formed in any systematic manner.
Hence, weintroduce what we call concept-based scoring?
used instead of the holistic human scoring.
Inconcept-based scoring, human raters annotatestudents?
responses for each concept C, andhighlight the part of the answer that entails C.In Sukkarieh and Blackmore (2009), we de-scribe concept-based scoring in detail and howthis helps in the KE-MB approach.
In this pa-per, we extend the approach by showing howconcept-based scores used in the automatedapproach reduce the time needed for MB sub-stantially while yielding comparable results.Concept-based scoring is done manually.
Onaverage, it takes around 3.5 hours per item fora dataset of roughly 200 answers.The MB process is reduced to:1.
Concept-based scoring2.
Automatically selecting required lexicon3.
Automatically selecting similar lexiconWhile holistic scoring takes on average 3 hoursfor a dataset of 200 answers, concept-basedscoring takes 3.5 hours for the same set.
How-ever, automated MB takes 0 hours of humanintervention?a substantial reduction over the12 hours required for manual MB.5.1   Concept-based ScoringWe have developed a concept-based scoringinterface (CBS) that can be customized foreach item [due to lack of space we do not in-clude an illustration].
The CBS interface dis-plays a student?s answer to an item and all ofthe concepts corresponding to that item.
Theterms {Absent, Present, Negated} are what wecall analytic or concept-based scores.
UsingCBS, the human scorer clicks Present when aconcept is present and Negated when a conceptis negated or refuted (the default is Absent).This is done for each concept.
The humanscorer also highlights the part of a student?sanswer that entails the concept in the contextof the item.
We call a quote corresponding toconcept C ?Positive Evidence?
or ?NegativeEvidence?
for Present and Negated, respec-tively.
For example, assume a student answerfor Item 1 is ?Her research tells us a lot aboutrain and hail; in particular, the impact thattemperature variations have on altitude con-tribute to the formation of rain.?
ForConcept C3, the human rater highlights thePositive Evidence, ?the impact that tempera-ture variations have on altitude contribute tothe formation of rain.?
Parts of answers corre-sponding to one piece of Evidence (positive ornegative) do not need to be in the same sen-tence and could be scattered over a few lines.Similar to the KE approach, we split thedouble-concept-based scored dataset into DEVand XVAL sets.
However, the splitting is done64according to the presence (or absence) of aconcept.
We use stratified sampling (Tucker,1998) trying to uniformly split data such thateach concept is represented in the DEV as wellas the XVAL datasets.
As mentioned earlier,the KE approach can include negative con-cepts; currently we do not use Negative Evi-dence automatically.
In the remainder of thispaper, Evidence is taken to mean the collectionof Positive Evidence.5.2 Automatically Selecting ModelSentencesMotivationDuring manual MB with Alchemist, a modelbuilder is guided by the complete set of stu-dents?
answers in the DEV dataset, includingholistic scores.
Concept-based scoring allows amodel builder, if we were to continue the man-ual MB, to be guided by concept-based scoresand students?
answers highlighted with theEvidence that corresponds to each conceptwhen writing model sentences as shown,where MSij entails Ci and Eir entails Ci.Concept Ci Evidence Eir MSijC1 E11 MS11E1s1 MS1t1C2 E21 MS21E2s2 MS2t2Cn ?
?Further, students may misspell, write ungram-matically, or use incomplete sentences.
Hence,Evidence may contain spelling and grammati-cal errors.
Evidence may also be in the form ofincomplete sentences.
Although human modelbuilders generating sentences with Alchemistare asked to write complete MSij,, there is noreason why MSij, needs to be in the form ofcomplete sentences.
The NLP tools in thec-rater engine can cope with a reasonableamount of misspelled words as well as un-grammatical and/or incomplete sentences.We observe the following:1.
Concepts are seen as a set of model sen-tences that are subsumed by the list ofmodel sentences built by humans2.
Evidence is seen as a list of model?sentences?
that nearly subsume the set gener-ated by humans (i.e., the intersection is notempty)ApproachIn the automatic approach, we select the Evi-dence highlighted in the DEV dataset as MSijs.We either choose the intersection of Evidence(i.e., where both human raters agree) or theunion (i.e., highlighted by either human) asentailing a concept.5.3 Automatically Selecting RequiredLexiconMotivationRequired lexicon for an item includes the mostessential lexicon for this item.
In the KE ap-proach, the required lexicon is selected by themodel builder, who makes a judgment about it.In Alchemist, a model builder is presentedwith a tokenized model sentence and s/heclicks on a token to select it as a required lexi-cal entity.We have observed that selecting required lexi-con RLijk involves ignoring or removing noise,such as stop-words (e.g., ?a,?
?the,?
?to,?
etc.
),from the presented model sentence.
For exam-ple, a model builder may select the words,?how,?
?rain,?
?formation,?
and ?winter?
inthe model sentence ?How does rain formationoccur in the winter??
and ignore the rest.
Inaddition, there might be words other than stop-words that can be ignored.
For example, if amodel builder writes, ?It may help Alice andscientists to know how rain formation occursin the winter?
?
the tokens ?scientists?
and?Alice?
are not stop-words and can be ignored.ApproachWe evaluate five methods of automaticallyselecting the required lexicon:1.
Consider all tokens in MSij2.
Consider all tokens in MSij without stop-words3.
Consider all heads of NPs and VPs (nounsand verbs)4.
Consider all heads of all various syntacticroles including adjectives and adverbs5.
Consider the lexicon with the highest mu-tual information measures, with all lexicaltokens in model sentences correspondingto the same concept65The first method does not need any elabora-tion.
In the following, we briefly elaborate oneach of the other methods.5.3.1 All Words Without Stop LexiconIn addition to the list of stop-words provided inVan Rijsbergen?s book (Rijsbergen, 2004) andthe ones we extracted from WordNet 2.0http://wordnet.princeton.edu/(except for ?zero,?
?minus,?
?plus,?
and ?op-posite?
), we have developed a list of approxi-mately 2,000 stop-words based on students?data.
This includes various interjections andcommon short message service (SMS) abbre-viations that are found in students?
data (seeTable 1 for examples).1.
Umm 2.
Aka 3.
Coz4.
Viz.
5. e.g.
6.
Hmm7.
Phew 8.
Aha 9.
Wow10.
Ta 11.Yippee 12.
NTHING13.
Dont know 14.
Nada 15.
Guess16.
Yoink 17.
RUOK 18.
SPKTable 1.
Student-driven stop-words5.3.2 Head Words of Noun and VerbPhrasesThe feature extractor in c-rater, mentioned inSection 2, labels the various noun and verbphrases with a corresponding syntactic or se-mantic role using in-house developed rules.We extract the heads of these by simply con-sidering the rightmost lexical entity with anexpected POS tag, i.e., for noun phrases welook for the rightmost nominal lexical entity,for verb phrases we look for the rightmostverbs.5.3.3 Head Words of all PhrasesWe consider all phrases or syntactic roles, i.e.,not only noun and verb phrases but also adjec-tive and adverb phrases.5.3.4 Words with Highest MutualInformationThe mutual information (MI) method measuresthe mutual dependence of two variables.
MI innatural language tasks has been used for in-formation retrieval (Manning et.
al., 2008) andfor feature selection in classification tasks(Stoyanchev and Stent, 2009).Here, MI selects words that are indicative ofthe correct answer while filtering out the wordsthat are also frequent in incorrect answers.
Ouralgorithm selects a lexical term if it has highmutual dependence with a correct concept orEvidence in students?
answers.
For each termmentioned in a students?
answer we computemutual information measure (I):where N11 is the number of student answerswith the term co-occurring with a correct con-cept or Evidence, N01 is the number of studentanswers with a correct concept but without theterm, N10 is the number of student answerswith the term but without a correct concept,N00 is the number of student answers with nei-ther the term nor a correct concept, N1.
is thetotal number of student answers with the term,N.1 is the total number of utterances with a cor-rect concept, and N is the total number of ut-terances.
The MI method selects the terms orwords predictive of both presence and absenceof a concept.
In this task we are interested infinding the terms that indicate presence of acorrect concept.
We ignore the words that aremore likely to occur without the concept (thewords for which N11< N10).
In this study, afterlooking at the list of words produced, we sim-ply selected the top 40 words with the highestmutual information measure.5.4 Automatically Selecting SimilarLexiconMotivationIn the KE approach, once a model builder se-lects a required word, a screen on Alchemistlists similar words extracted automaticallyfrom Dekang Lin?s dependency-based thesau-rus.
The model builder can also use other re-sources like Roget?s thesaurus(http://gutenberg.org/etext/22) and WordNet3.0 (http://wordnet.princeton.edu/).
The modelbuilder can also write her/his own words thats/he believes are similar to the required word.ApproachOther than choosing no similar lexicon to arequired word W, automatically selecting simi-66lar lexicon consists of the following experi-ments:1.
All words similar to W in Dekang Lin?sgenerated list2.
Direct synonyms for W or its lemma fromWordNet 3.0 (excluding compounds).Compounds are excluded because we no-ticed many irrelevant compounds thatcould not replace uni-grams in our data.3.
All similar words for W or its lemma fromWordNet 3.0, i.e., direct synonyms, relatedwords and hypernyms (excluding com-pounds).
Hypernyms of W are restricted toa maximum of 2 levels up from WTo summarize, for each concept in the KE ap-proach, a model builder writes a set of ModelSentences, manually selects Required Lexiconand Similar Lexicon for each required word.
Inthe automated approach, all of the above isselected automatically.
Table 2 summarizes themethods or experiments.
We refer to a methodor experiment in the order of selection of RLijkand SLijkt; e.g., we denote the method where allwords were required and similar lexicon cho-sen from WordNet Direct synonyms by AWD.HSVocWA denotes the method where heads ofNPs and VPs with similar words from Word-Net All, i.e., direct, related, and hypernyms areselected.
A method name preceded by I or Urefers to Evidence Intersection or Union, re-spectively.
For each item, there are 40 experi-ments/methods performed with Evidence asmodel sentences.ModelSentences Required Lexicon Similar LexiconConcepts(C)All words (A) None chosen (N)EvidenceIntersection(I)All words with no stop-words (S)Lin all (L)EvidenceUnion (U)Heads of NPs and VPs(HSvoc)WordNet directsynonyms (WD)Heads of all phrases (HA) WordNet alsimilar words(WA)Highest Mutual informa-tion measure (M)Table 2.
Parameters and ?Values?
of ModelBuildingBefore presenting the evaluation results, wemake a note about spelling correction.
c-raterhas its own automatic spelling corrector.
Here,we only outline how spelling correction relatesto a model.
In the KE approach, model sen-tences are assumed to not having spelling er-rors.
We use the model sentences, the stimulus(if it exists), and the prompt of the item foradditional guidance to select the correctly-spelled word from a list of potential correctly-spelled words designated by the spelling cor-rector.
On the other hand, the Evidence can bemisspelled.
Consequently, when the Evidenceis considered for model sentences, the spellingcorrector first performs spelling correction onthe Evidence, using stimulus, concepts, andprompts as guides.
The students?
answers arethen corrected, as in the KE approach.6 EvaluationThe study involves 12 test items developed atETS for grades 7 and 8.
There are seven Read-ing Comprehension items, denoted R1-R7 andfive Mathematics items, denoted M1-M5.Score points for the items range from 0 to 3and the number of concepts ranges from 2 to 7.The answers for these items were collected inschools in Maine, USA.
The number of an-swers collected for each item ranges from 190-264.
Answers were concept-based scored bytwo human raters (H1, H2).
We split the dou-ble-scored students?
answers available intoDEV (90-100 answers), XVAL (40-50) andBLIND (60-114).
Training data refer to DEVtogether with XVAL datasets.
Results are re-ported in terms of un-weighted kappa, repre-senting scoring agreement with humans on theBLIND dataset.
H1/2 refers to the agreementbetween the two humans, c-H1/2 denotes theaverage of kappa values between c-rater andeach human (c-H1 and c-H2).
Table 3 reportsthe best kappa over the 40 experiments onBLIND (Auto I or U).
The baseline (Auto C)uses concepts as model sentences.Item#Training(Blind) H1/2 ManualAutoCAutoI or Uc-H1/2 c-H1/2 c-H1/2R1 150  (114) 1.0    0.94   0.51 0.97R2 150  (113) 0.76    0.69   0.28 0.76R3 150  (107) 0.96    0.87   0.18 0.88R4 150    (66) 0.77    0.71   0.46 0.75R5 130    (60) 0.71    0.58   0.22 0.61R6 130    (61) 0.71    0.73   0.23 0.77R7 130    (61) 0.87    0.55   0.42 0.42M1 130    (67) 0.71      0.6   0.0 0.66M2 130    (67) 0.8     0.71   0.54 0.67M3 130    (67) 0.86    0.76   0.0 0.79M4 130    (67) 0.87    0.82   0.13 0.82M5 130    (67) 0.77    0.63   0.29 0.65Table 3.
Best on BLIND over all experiments67The accuracy using the automated approachwith Evidence as model sentences is compara-ble to that of the KE approach (noted in thecolumn labeled, ?Manual?)
with a 0.1 maxi-mum difference in un-weighted kappa statis-tics.
The first methods (in terms of runningorder) yielding the best results for the items (inorder of appearance in Table 3) are ISWD,ISW, ISN, IMN, IHSVocN, UHALA, ISN,UHSVocN, SLA, ISN, IHAN and IHS-VocWA.
The methods yielding the best results(regardless of running order) for all items us-ing the Evidence were:IHAN U/IHAWD IHAWAU/IHALA U/IHSvocN IHSvocWAUHSvocLA UHSvocWA UHSvocWDU/ISLA U/ISN U/ISWAU/ISWD U/IAWA IMNIMWDThis approach was only evaluated on a smallnumber of items.
We expect that some meth-ods will outperform others through additionalevaluation.In an operational setting (i.e., not a researchenvironment), we must choose a model beforewe score the BLIND data.
Hence, a votingstrategy over all the experiments has to be de-vised based on the results on DEV and XVAL.Following our original logic, i.e., using XVALto avoid over-fitting and predicting the resultsof BLIND, we implemented a simple votingstrategy.
We considered c-H1/2 on XVAL foreach experiment.
We found the maximum overall the c-H1/2 for all experiments.
The modelcorresponding to the maximum was consideredthe model for the item and used to score theBLIND data.
When there was a tie, the firstmethod to yield the maximum W chosen.Table 4 shows the results on BLIND using thevoting strategy.
The results are comparable tothose of the manual approach except for R7which has 7 concepts, the highest number ofconcepts among all items.
The results alsoshow that the voting strategy did not select the?best?
model or experiment.
We notice thatsome methods were better in detecting whetheran answer entailed a concept C than detectingwhether it entailed anotherconcept D, specified for the same item.
Thisimplies that the voting strategy will have to bea function that not only considers the overallkappa agreement (i.e., holistic scores), butconcept-based agreement (i.e., using concept-based scores).
Next, we noticed that for R7,XVAL did not predict the results on BLIND.This was mainly due to the inability to applystratified sampling with such a small samplesize when there are 7 concepts involved.
Fur-ther, we may need to take advantage of thetraining data differently, e.g.
an n-fold cross-validation approach.
Finally, when there is atie, factors other than running order should beconsidered.Item#Training(Blind) H1/2 ManualAuto(C)Auto(I or U)c-H1/2 c-H1/2 c-H1/2R1 150  (114) 1.0    0.94   0.51 0.88R2 150  (113) 0.76   0.69   0.18 0.61R3 150  (107) 0.96   0.87   0.18 0.86R4 150    (66) 0.77   0.71   0.38 0.67R5 130    (60) 0.71   0.58   0.17 0.51R6 130    (61) 0.71   0.73   0.13 0.73R7 130    (61) 0.87   0.55   0.39 0.16M1 130     67) 0.71    0.6    0.0 0.65M2 130     67) 0.8    0.71   0.54 0.58M3 130     67) 0.86   0.76   0.0 0.79M4 130     67) 0.87   0.82   0.13 0.68M5 130     67) 0.77   0.63   0.26 0.49Table 4.
Voting Strategy results on BLINDIn all of the above experiments, the Evidencewas corrected using the c-rater?s automaticspelling corrector using the stimulus (in case ofReading), the concepts, and the prompts toguide the selection of the correctly-spelledwords.7 ConclusionAnalytic-based content scoring is an applica-tion of textual entailment.
The complexity ofthe problem increases due to the noise in stu-dent data, the context of an item, and differentsubject areas.
In this paper, we have shownthat building a c-rater scoring model for anitem can be reduced from 12 to 0 hours of hu-man intervention with comparable scoring per-formance.
This is a significant improvement onresearch to date using supervised techniques.In addition, as far as we know, no one otherthan Calvo et al (2005) made any comparisonsbetween a manually-built ?thesaurus?
(e.g.WordNet) and an automatically-generated?thesaurus?
(e.g.
Dekang Lin?s database) in anNLP task or application prior to our work.
Ournext step is to evaluate (and refine) the ap-proach on a larger set of items.
Further im-provements will include using Negative Evi-dence, automating concept-based scoring, in-vestigating a context-sensitive selection ofsimilar words using the students?
answers andexperimenting with various voting strategies.Finally, we need to compare the results re-ported using unsupervised techniques on thesame items and datasets if possible.68AcknowledgmentsSpecial thanks to Michael Flor, Rene Lawless,Sarah Ohls and Waverely VanWinkle.ReferencesCalvo H., Gelbukh A., and Kilgarriff A.
(2005).Distributional thesaurus vs. WordNet: A com-parison of backoff techniques for unsupervisedPP attachment.
In CICLing.Christie, J.R. (1999).
Automated essay marking forboth content and style.
In Proceedings of the 3rdInternational Computer Assisted AssessmentConference.
Loughborough University.Loughborough, Uk.Foltz, P.W.
and Laham, D. and Landauer, T.K.
(2003) Automated essay scoring.
Applications toEducational technology.
http://www-psych.nmsu.edu/%7Epfoltz/reprints/Edmedia99.htmlLeacock, C. and Chodorow, M. (2003) C-rater:Automated Scoring of Short-Answer Questions.Computers and Humanities.
pp.
389-405Manning C. D., Raghavan P., and Sch?utze H.(2008).
Introduction to Information Retrieval.Cambridge University Press.Mitchell, T. and Russel, T. and Broomhead, P. andAldrige, N. (2002) Towards robust computerisedmarking of free-text responses.
Proceedings ofthe 6th International Computer Assisted As-sessment Conference.Mohler M. and Mihalcea R (2009).
Text-to-textSemantic Similarity for Automatic Short AnswerGrading.
Proceedings of the European Chapterof the Association for Computational Linguis-tics, Athens, Greece, March 2009.Ros?, C. P. and Roque, A. and Bhembe, D. andVanLehn, K.. (2003) A hybrid text classificationapproach for analysis of student essays.
Proceed-ings of the HLT-NAACL 03 Workshop on Edu-cational Applications of NLP.Stoyanchev S. and Stent A.
(2009).
Predicting Con-cept Types in User Corrections in Dialog.
Pro-ceedings of EACL Workshop on the SemanticRepresentation of Spoken Language.
Athens,Greece.Sukkarieh, J.
Z., and Blackmore, J.
(2009).
c-rater:Automatic Content Scoring for Short Con-structed Responses.
Proceedings of the 22nd In-ternational Conference for the Florida ArtificialIntelligence Research Society, Florida, USA.Sukkarieh, J.Z.
and Stephen G. Pulman (2005).Information Extraction and Machine Learning:Auto-marking short free-text responses for Sci-ence questions.
Proceedings of the 12th Interna-tional conference on Artificial Intelligence inEducation, Amsterdam, The Netherlands.Sukkarieh, J.Z.
Pulman S. G. and Raikes, N.(2004).
Auto-marking 2: An update on theUCLES-Oxford University research into usingcomputational linguistics to score short, free textresponses.
Proceedings of the AIEA, Philadel-phia, USA.Sukkarieh, J.
Z. and Pulman, S. G. and Raikes, N.(2003) Auto-marking: using computational lin-guistics to score short, free text responses.Proceedings of international association ofeducational assessment.
Manchester, UK.Tucker H. G. (1998) Mathematical Methods inSample Surveys.
Series on multivariate analysisVol.
3.
University of California, Irvine.Van Rijsbergen C. J.
( 2004) The Geometry of In-formation Retrieval.
Cambridge UniversityPress.
The Edinburgh Building, Cambridge,CB2 2RU, UK.Vantage.
(2000) A study of expert scoring and In-telliMetric scoring accuracy for dimensionalscoring of grade 11 student writing responses.Technical report RB-397, Vantage LearningTech.69
