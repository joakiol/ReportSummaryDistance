Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 275?285,Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational LinguisticsInducing Domain-specific Semantic Class Taggers from (Almost) NothingRuihong Huang and Ellen RiloffSchool of ComputingUniversity of UtahSalt Lake City, UT 84112{huangrh,riloff}@cs.utah.eduAbstractThis research explores the idea of inducingdomain-specific semantic class taggers us-ing only a domain-specific text collectionand seed words.
The learning process be-gins by inducing a classifier that only hasaccess to contextual features, forcing it togeneralize beyond the seeds.
The contex-tual classifier then labels new instances,to expand and diversify the training set.Next, a cross-category bootstrapping pro-cess simultaneously trains a suite of clas-sifiers for multiple semantic classes.
Thepositive instances for one class are used asnegative instances for the others in an it-erative bootstrapping cycle.
We also ex-plore a one-semantic-class-per-discourseheuristic, and use the classifiers to dynam-ically create semantic features.
We eval-uate our approach by inducing six seman-tic taggers from a collection of veterinarymedicine message board posts.1 IntroductionThe goal of our research is to create semantic classtaggers that can assign a semantic class label to ev-ery noun phrase in a sentence.
For example, con-sider the sentence: ?The lab mix was diagnosedwith parvo and given abx?.
A semantic taggershould identify the ?the lab mix?
as an ANIMAL,?parvo?
as a DISEASE, and ?abx?
(antibiotics)as a DRUG.
Accurate semantic tagging could bebeneficial for many NLP tasks, including coref-erence resolution and word sense disambiguation,and many NLP applications, such as event extrac-tion systems and question answering technology.Semantic class tagging has been the subject ofprevious research, primarily under the guises ofnamed entity recognition (NER) and mention de-tection.
Named entity recognizers perform se-mantic tagging on proper name noun phrases, andsometimes temporal and numeric expressions aswell.
The mention detection task was introducedin recent ACE evaluations (e.g., (ACE, 2007;ACE, 2008)) and requires systems to identify allnoun phrases (proper names, nominals, and pro-nouns) that correspond to 5-7 semantic classes.Despite widespread interest in semantic tag-ging, nearly all semantic taggers for comprehen-sive NP tagging still rely on supervised learn-ing, which requires annotated data for training.A few annotated corpora exist, but they are rela-tively small and most were developed for broad-coverage NLP.
Many domains, however, are re-plete with specialized terminology and jargon thatcannot be adequately handled by general-purposesystems.
Domains such as biology, medicine, andlaw are teeming with specialized vocabulary thatnecessitates training on domain-specific corpora.Our research explores the idea of inducingdomain-specific semantic taggers using a smallset of seed words as the only form of human su-pervision.
Given an (unannotated) collection ofdomain-specific text, we automatically generatetraining instances by labelling every instance of aseed word with its designated semantic class.
Wethen train a classifier to do semantic tagging usingthese seed-based annotations, using bootstrappingto iteratively improve performance.On the surface, this approach appears to be acontradiction.
The classifier must learn how to as-sign different semantic tags to different instancesof the same word based on context (e.g., ?lab?may refer to an animal in one context but a labora-tory in another).
And yet, we plan to train the clas-sifier using stand-alone seed words, making the as-sumption that every instance of the seed belongs tothe same semantic class.
We resolve this apparentcontradiction by using semantically unambiguousseeds and by introducing an initial context-onlytraining phase before bootstrapping begins.
First,we train a strictly contextual classifier that only275has access to contextual features and cannot seethe seed.
Then we apply the classifier to the corpusto automatically label new instances, and combinethese new instances with the seed-based instances.This process expands and diversifies the trainingset to fuel subsequent bootstrapping.Another challenge is that we want to use a smallset of seeds to minimize the amount of human ef-fort, and then use bootstrapping to fully exploitthe domain-specific corpus.
Iterative self-training,however, often has difficulty sustaining momen-tum or it succumbs to semantic drift (Komachiet al, 2008; McIntosh and Curran, 2009).
Toaddress these issues, we simultaneously inducea suite of classifiers for multiple semantic cat-egories, using the positive instances of one se-mantic category as negative instances for the oth-ers.
As bootstrapping progresses, the classifiersgradually improve themselves, and each other,over many iterations.
We also explore a one-semantic-class-per-discourse (OSCPD) heuristicthat infuses the learning process with fresh train-ing instances, which may be substantially differ-ent from the ones seen previously, and we use thelabels produced by the classifiers to dynamicallycreate semantic features.We evaluate our approach by creating six se-mantic taggers using a collection of message boardposts in the domain of veterinary medicine.
Ourresults show this approach produces high-qualitysemantic taggers after a sustained bootstrappingcycle that maintains good precision while steadilyincreasing recall over many iterations.2 Related WorkSemantic class tagging is most closely related tonamed entity recognition (NER), mention detec-tion, and semantic lexicon induction.
NER sys-tems (e.g., (Bikel et al, 1997; Collins and Singer,1999; Cucerzan and Yarowsky, 1999; Fleischmanand Hovy, 2002) identify proper named entities,such as people, organizations, and locations.
Sev-eral bootstrapping methods for NER have beenpreviously developed (e.g., (Collins and Singer,1999; Niu et al, 2003)).
NER systems, how-ever, do not identify nominal NP instances (e.g.,?a software manufacturer?
or ?the beach?
), or han-dle semantic classes that are not associated withproper named entities (e.g., symptoms).1 ACE1Some NER systems also handle specialized constructssuch as dates and monetary amounts.mention detection systems (e.g., see (ACE, 2005;ACE, 2007; ACE, 2008)) require tagging of NPsthat correspond to 5-7 general semantic classes.These systems are typically trained with super-vised learning using annotated corpora, althoughtechniques have been developed to use resourcesfor one language to train systems for different lan-guages (e.g., (Zitouni and Florian, 2009)).Another line of relevant work is semantic classinduction (e.g., (Riloff and Shepherd, 1997; Roarkand Charniak, 1998; Thelen and Riloff, 2002; Ng,2007; McIntosh and Curran, 2009), where the goalis to induce a stand-alone dictionary of words withsemantic class labels.
These techniques are of-ten designed to learn specialized terminology fromunannotated domain-specific texts via bootstrap-ping.
Our work, however, focuses on classifica-tion of NP instances in context, so the same phrasemay be assigned to different semantic classes indifferent contexts.
Consequently, our classifiercan also assign semantic class labels to pronouns.There has also been work on extracting seman-tically related terms or category members fromthe Web (e.g., (Pas?ca, 2004; Etzioni et al, 2005;Kozareva et al, 2008; Carlson et al, 2009)).
Thesetechniques harvest broad-coverage semantic infor-mation from the Web using patterns and statistics,typically for the purpose of knowledge acquisi-tion.
Importantly, our goal is to classify instancesin context, rather than generate lists of terms.
Inaddition, the goal of our research is to learn spe-cialized terms and jargon that may not be commonon the Web, as well as domain-specific usages thatmay differ from the norm (e.g., ?mix?
and ?lab?are usually ANIMALS in our domain).The idea of simulataneously learning multiplesemantic categories to prevent semantic drift hasbeen explored for other tasks, such as semanticlexicon induction (Thelen and Riloff, 2002; McIn-tosh and Curran, 2009) and pattern learning (Yan-garber, 2003).
Our bootstrapping model can beviewed as a form of self-training (e.g., (Ng andCardie, 2003; Mihalcea, 2004; McClosky et al,2006)), and cross-category training is similar inspirit to co-training (e.g., (Blum and Mitchell,1998; Collins and Singer, 1999; Riloff and Jones,1999; Mueller et al, 2002; Phillips and Riloff,2002)).
But, importantly, our classifiers all use thesame feature set so they do not represent indepen-dent views of the data.
They do, however, offerslightly different perspectives because each is at-276tempting to recognize a different semantic class.3 Bootstrapping an Instance-basedSemantic Class Tagger from Seeds3.1 MotivationOur goal is to create a bootstrapping model thatcan rapidly create semantic class taggers usingjust a small set of seed words and an unanno-tated domain-specific corpus.
Our motivationcomes from specialized domains that cannot beadequately handled by general-purpose NLP sys-tems.
As an example of such a domain, we havebeen working with a collection of message boardposts in the field of veterinary medicine.
Given adocument, we want a semantic class tagger to labelevery NP with a semantic category, for example:[A 14yo doxy]ANIMAL owned by[a reputable breeder]HUMAN is be-ing treated for [IBD]DISEASE with[pred]DRUG.When we began working with these texts, wewere immediately confronted by a dizzying arrayof non-standard words and word uses.
In additionto formal veterinary vocabulary (e.g., animal dis-eases), veterinarians often use informal, shorthandterms when posting on-line.
For example, theyfrequently refer to breeds using ?nicknames?
orshortened terms (e.g., gshep for German shepherd,doxy for dachsund, bxr for boxer, labx for labradorcross).
Often, they refer to animals based solely ontheir physical characteristics, for example ?a dlh?
(domestic long hair), ?a m/n?
(male, neutered), or?a 2yo?
(2 year old).
This phenomenon occurswith other semantic categories as well, such asdrugs and medical tests (e.g., pred for prednisone,and rads for radiographs).Nearly all semantic class taggers are trained us-ing supervised learning with manually annotateddata.
However, annotated data is rarely availablefor specialized domains, and it is expensive to ob-tain because domain experts must do the annota-tion work.
So we set out to create a bootstrappingmodel that can rapidly create domain-specific se-mantic taggers using just a few seed words and adomain-specific text collection.Our bootstrapping model consists of two dis-tinct phases.
First, we train strictly contextualclassifiers from the seed annotations.
We then ap-ply the classifiers to the unlabeled data to gener-ate new annotated instances that are added to thetraining set.
Second, we employ a cross-categorybootstrapping process that simultaneously trainsa suite of classifiers for multiple semantic cate-gories, using the positive instances for one se-mantic class as negative instances for the oth-ers.
This cross-category training process givesthe learner sustained momentum over many boot-strapping iterations.
Finally, we explore two ad-ditional enhancements: (1) a one-semantic-class-per-discourse heuristic to automatically generatenew training instances, and (2) dynamically cre-ated semantic features produced by the classifiersthemselves.
In the following sections, we explaineach of these steps in detail.3.2 Phase 1: Inducing a Contextual ClassifierThe main challenge that we faced was how to trainan instance-based classifier using seed words asthe only form of human supervision.
First, the usermust provide a small set of seed words that arerelatively unambiguous (e.g., ?dog?
will nearlyalways refer to an animal in our domain).
Buteven so, training a traditional classifier from seed-based instances would likely produce a classifierthat learns to recognize the seeds but is unable toclassify new examples.
We needed to force theclassifier to generalize beyond the seed words.Our solution was to introduce an initial train-ing step that induces a strictly contextual classifier.First, we generate training instances by automati-cally labeling each instance of a seed word withits designated semantic class.
However, when wecreate feature vectors for the classifier, the seedsthemselves are hidden and only contextual fea-tures are used to represent each training instance.By essentially ?masking?
the seed words so theclassifier can only see the contexts around them,we force the classifier to generalize.We create a suite of strictly contextual classi-fiers, one for each semantic category.
Each classi-fier makes a binary decision as to whether a nounphrase belongs to its semantic category.
We usethe seed words for category Ck to generate posi-tive training instances for the Ck classifier, and theseed words for all other categories to generate thenegative training instances for Ck.We use an in-house sentence segmenter and NPchunker to identify the base NPs in each sentenceand create feature vectors that represent each con-stituent in the sentence as either an NP or an in-dividual word.
For each seed word, the feature277vector captures a context window of 3 constituents(word or NP) to its left and 3 constituents (wordor NP) to its right.
Each constituent is representedwith a lexical feature: for NPs, we use its headnoun; for individual words, we use the word itself.The seed word, however, is discarded so that theclassifier is essentially blind-folded and cannot seethe seed that produced the training instance.
Wealso create a feature for every modifier that pre-cedes the head noun in the target NP, except forarticles which are discarded.
As an example, con-sider the following sentence:Fluffy was diagnosed with FELV after ablood test showed that he tested positive.Suppose that ?FELV?
is a seed for the DISEASEcategory and ?test?
is a seed for the TEST cate-gory.
Two training instances would be created,with feature vectors that look like this, where Mrepresents a modifier inside the target NP:was?3 diagnosed?2 with?1 after1 test2showed3 ?
DISEASEwith?3 FELV?2 after?1 bloodM showed1that2 he3 ?
TESTThe contextual classifiers are then applied to thecorpus to automatically label new instances.
Weuse a confidence score to label only the instancesthat the classifiers are most certain about.
We com-pute a confidence score for instance i with respectto semantic class Ck by considering both the scoreof the Ck classifier as well as the scores of thecompeting classifiers.
Intuitively, we have confi-dence in labeling an instance as category Ck if theCk classifier gave it a positive score, and its scoreis much higher than the score of any other classi-fier.
We use the following scoring function:Confidence(i,Ck) =score(i,Ck) - max(?j 6=k score(i,Cj ))We employ support vector machines (SVMs)(Joachims, 1999) with a linear kernel as our classi-fiers, using the SVMlin software (Keerthi and De-Coste, 2005).
We use the value produced by thedecision function (essentially the distance fromthe hyperplane) as the score for a classifier.
Wespecify a threshold ?cf and only assign a semantictag Ck to an instance i if Confidence(i,Ck) ?
?cf .All instances that pass the confidence thresh-old are labeled and added to the training set.This process greatly enhances the diversity ofthe training data.
In this initial learning step,the strictly contextual classifiers substantially in-crease the number of training instances for eachsemantic category, producing a more diverse mixof seed-generated instances and context-generatedinstances.3.3 Phase 2: Cross-Category BootstrappingThe next phase of the learning process is an iter-ative bootstrapping procedure.
The key challengewas to design a bootstrapping model that wouldnot succumb to semantic drift and would have sus-tained momentum to continue learning over manyiterations.Figure 1 shows the design of our cross-categorybootstrapping model.2 We simultaneously train asuite of binary classifiers, one for each semanticcategory, C1 .
.
.
Cn.
After each training cycle,all of the classifiers are applied to the remainingunlabeled instances and each classifier labels the(positive) instances that it is most confident about(i.e., the instances that it classifies with a confi-dence score ?
?cf ).
The set of instances positivelylabeled by classifier Ck are shown as C+k in Figure1.
All of the new instances produced by classifierCk are then added to the set of positive traininginstances for Ck and to the set of negative traininginstances for all of the other classifiers.One potential problem with this scheme is thatsome categories are more prolific than others, pluswe are collecting negative instances from a setof competing classifiers.
Consequently, this ap-proach can produce highly imbalanced trainingsets.
Therefore we enforced a 3:1 ratio of nega-tives to positives by randomly selecting a subsetof the possible negative instances.
We discuss thisissue further in Section 4.4.labeledC+1C2C+2 C+nCnunlabeledseedsC1i=1C+_( )C+1(+) (+)C+2 i=2C+_( ) _( )i=nC+C+n(+)Figure 1: Cross-Category Bootstrapping2For simplicity, this picture does not depict the initial con-textual training step, but that can be viewed as the first itera-tion in this general framework.278Cross-category training has two advantagesover independent self-training.
First, as oth-ers have shown for pattern learning and lexiconinduction (Thelen and Riloff, 2002; Yangarber,2003; McIntosh and Curran, 2009), simultane-ously training classifiers for multiple categoriesreduces semantic drift because each classifier isdeterred from encroaching on another one?s terri-tory (i.e., claiming the instances from a compet-ing class as its own).
Second, similar in spirit toco-training3 , this approach allows each classifierto obtain new training instances from an outsidesource that has a slightly different perspective.While independent self-training can quickly runout of steam, cross-category training supplies eachclassifier with a constant stream of new (negative)instances produced by competing classifiers.
InSection 4, we will show that cross-category boot-strapping performs substantially better than an in-dependent self-training model, where each classi-fier is bootstrapped separately.The feature set for these classifiers is exactly thesame as described in Section 3.2, except that weadd a new lexical feature that represents the headnoun of the target NP (i.e., the NP that needs to betagged).
This allows the classifiers to consider thelocal context as well as the target word itself whenmaking decisions.3.4 One Semantic Class Per DiscourseWe also explored the idea of using a one semanticclass per discourse (OSCPD) heuristic to gener-ate additional training instances during bootstrap-ping.
Inspired by Yarowsky?s one sense per dis-course heuristic for word sense disambiguation(Yarowsky, 1995), we make the assumption thatmultiple instances of a word in the same discoursewill nearly always correspond to the same seman-tic class.
Since our data set consists of messageboard posts organized as threads, we consider allposts in the same thread to be a single discourse.After each training step, we apply the classi-fiers to the unlabeled data to label some new in-stances.
For each newly labeled instance, the OS-CPD heuristic collects all instances with the samehead noun in the same discourse (thread) and uni-laterally labels them with the same semantic class.This heuristic serves as meta-knowledge to labelinstances that (potentially) occur in very different3But technically this is not co-training because our featuresets are all the same.contexts, thereby infusing the bootstrapping pro-cess with ?fresh?
training examples.In early experiments, we found that OSCPD canbe aggressive, pulling in many new instances.
Ifthe classifier labels a word incorrectly, however,then the OSCPD heuristic will compound the er-ror and mislabel even more instances incorrectly.Therefore we only apply this heuristic to instancesthat are labeled with extremely high confidence(?cf ?
2.5) and that pass a global sanity check,gsc(w) ?
0.2, which ensures that a relatively highproportion of labeled instances with the same headnoun have been assigned to the same semanticclass.
Specifically, gsc(w) = 0.1?wl/cwl +0.9?wu/cwuwhere wl and wu are the # of labeled and unla-beled instances, respectively, wl/c is the # of in-stances labeled as c, and wu/c is the # of unlabeledinstances that receive a positive confidence scorefor c when given to the classifier.
The intuitionbehind the second term is that most instances areinitially unlabeled and we want to make sure thatmany of the unlabeled instances are likely to be-long to the same semantic class (even though theclassifier isn?t ready to commit to them yet).3.5 Dynamic Semantic FeaturesFor many NLP tasks, classifiers use semantic fea-tures to represent the semantic class of words.These features are typically obtained from exter-nal resources such as Wordnet (Miller, 1990).
Ourbootstrapping model incrementally trains seman-tic class taggers, so we explored the idea of usingthe labels assigned by the classifiers to create en-hanced feature vectors by dynamically adding se-mantic features.
This process allows later stagesof bootstrapping to directly benefit from earlierstages.
For example, consider the sentence:He started the doxy on Vetsulin today.If ?Vetsulin?
was labeled as a DRUG in a previ-ous bootstrapping iteration, then the feature vectorrepresenting the context around ?doxy?
can be en-hanced to include an additional semantic featureidentifying Vetsulin as a DRUG, which would looklike this:He?2 started?1 on1 V etsulin2 DRUG2 today3Intuitively, the semantic features should help theclassifier identify more general contextual pat-terns, such as ?started <X> on DRUG?.
To createsemantic features, we use the semantic tags that279have been assigned to the current set of labeled in-stances.
When a feature vector is created for a tar-get NP, we check every noun instance in its contextwindow to see if it has been assigned a semantictag, and if so, then we add a semantic feature.
Inthe early stages of bootstrapping, however, rela-tively few nouns will be assigned semantic tags,so these features are often missing.3.6 Thresholds and Stopping CriterionWhen new instances are automatically labeledduring bootstrapping, it is critically important thatmost of the labels are correct or performancerapidly deteriorates.
This suggests that we shouldonly label instances in which the classifier hashigh confidence.
On the other hand, a high thresh-old often yields few new instances, which cancause the bootstrapping process to sputter and halt.To balance these competing demands, we useda sliding threshold that begins conservatively butgradually loosens the reins as bootstrapping pro-gresses.
Initially, we set ?cf = 2.0, which onlylabels instances that the classifier is highly confi-dent about.
When fewer than min new instancescan be labeled, we automatically decrease ?cf by0.2, allowing another batch of new instances to belabeled, albeit with slightly less confidence.
Wecontinue decreasing the threshold, as needed, un-til ?cf < 1.0, when we end the bootstrappingprocess.
In Section 4, we show that this slidingthreshold outperforms fixed threshold values.4 Evaluation4.1 DataOur data set consists of message board posts fromthe Veterinary Information Network (VIN), whichis a web site (www.vin.com) for professionals inveterinary medicine.
Among other things, VINhosts forums where veterinarians engage in dis-cussions about medical issues, cases in their prac-tices, etc.
Over half of the small animal veterinar-ians in the U.S. and Canada use VIN.
Analysis ofveterinary data could not only improve pet healthcare, but also provide early warning signs of in-fectious disease outbreaks, emerging zoonotic dis-eases, exposures to environmental toxins, and con-tamination in the food chain.We obtained over 15,000 VIN message boardthreads representing three topics: cardiology, en-docrinology, and feline internal medicine.
We didbasic cleaning, removing html tags and tokeniz-ing numbers.
For training, we used 4,629 threads,consisting of 25,944 individual posts.
We devel-oped classifiers to identify six semantic categories:ANIMAL, DISEASE/SYMPTOM4 , DRUG, HUMAN,TEST, and OTHER.The message board posts contain an abundanceof veterinary terminology and jargon, so two do-main experts5 from VIN created a test set (answerkey) for our evaluation.
We defined annotationguidelines6 for each semantic category and con-ducted an inter-annotator agreement study to mea-sure the consistency of the two domain experts on30 message board posts, which contained 1,473noun phrases.
The annotators achieved a relativelyhigh ?
score of .80.
Each annotator then labeled anadditional 35 documents, which gave us a test setcontaining 100 manually annotated message boardposts.
The table below shows the distribution ofsemantic classes in the test set.Animal Dis/Sym Drug Test Human Other612 900 369 404 818 1723To select seed words, we used the procedureproposed by Roark and Charniak (1998), rankingall of the head nouns in the training corpus by fre-quency and manually selecting the first 10 nounsthat unambiguously belong to each category.7 Thisprocess is fast, relatively objective, and guaranteedto yield high-frequency terms, which is importantfor bootstrapping.
We used the Stanford part-of-speech tagger (Toutanova et al, 2003) to identifynouns, and our own simple rule-based NP chunker.4.2 BaselinesTo assess the difficulty of our data set and task,we evaluated several baselines.
The first baselinesearches for each head noun in WordNet and la-bels the noun as category Ck if it has a hypernymsynset corresponding to that category.
We manu-ally identified the WordNet synsets that, to the bestof our ability, seem to most closely correspond4We used a single category for diseases and symptomsbecause our domain experts had difficulty distinguishing be-tween them.
A veterinary consultant explained that the sameterm (e.g., diabetes) may be considered a symptom in onecontext if it is secondary to another condition (e.g., pancre-atitis) and a disease in a different context if it is the primarydiagnosis.5One annotator is a veterinarian and the other is a veteri-nary technician.6The annotators were also allowed to label an NP asPOS Error if it was clearly misparsed.
These cases were notused in the evaluation.7We used 20 seeds for DIS/SYM because we merged twocategories and for OTHER because it is a broad catch-all class.280Method Animal Dis/Sym Drug Test Human Other AvgBASELINESWordNet 32/80/46 21/81/34 25/35/29 NA 62/66/64 NA 35/66/45.8Seeds 38/100/55 14/99/25 21/97/35 29/94/45 80/99/88 18/93/30 37/98/53.1Supervised 67/94/78 20/88/33 24/96/39 34/90/49 79/99/88 31/91/46 45/94/60.7Ind.
Self-Train I.13 61/84/71 39/80/52 53/77/62 55/70/61 81/96/88 30/82/44 58/81/67.4CROSS-CATEGORY BOOTSTRAPPED CLASSIFIERSContextual I.1 59/77/67 33/84/47 42/80/55 49/77/59 82/93/87 33/80/47 53/82/64.3XCategory I.45 86/71/78 57/82/67 70/78/74 73/65/69 85/92/89 46/82/59 75/78/76.1XCat+OSCPD I.40 86/69/77 59/81/68 72/70/71 72/69/71 86/92/89 50/81/62 75/76/75.6XCat+OSCPD+SF I.39 86/70/77 60/81/69 69/81/75 73/69/71 86/91/89 50/81/62 75/78/76.6Table 1: Experimental results, reported as Recall/Precision/F scoreto each semantic class.
We do not report Word-Net results for TEST because there did not seembe an appropriate synset, or for the OTHER cate-gory because that is a catch-all class.
The first rowof Table 1 shows the results, which are reportedas Recall/Precision/F score8.
The WordNet base-line yields low recall (21-32%) for every categoryexcept HUMAN, which confirms that many veteri-nary terms are not present in WordNet.
The sur-prisingly low precision for some categories is dueto atypical word uses (e.g., patient, boy, and girlare HUMAN in WordNet but nearly always ANI-MALS in our domain), and overgeneralities (e.g.,WordNet lists calcium as a DRUG).The second baseline simply labels every in-stance of a seed with its designated semantic class.All non-seed instances remain unlabeled.
As ex-pected, the seeds produce high precision but lowrecall.
The exception is HUMAN, where 80% ofthe instances match a seed word, undoubtedly be-cause five of the ten HUMAN seeds are 1st and 2ndperson pronouns, which are extremely common.A third baseline trains semantic classifiers usingsupervised learning by performing 10-fold cross-validation on the test set.
The feature set andclassifier settings are exactly the same as withour bootstrapped classifiers.9 Supervised learningachieves good precision but low recall for all cate-gories except ANIMAL and HUMAN.
In the nextsection, we present the experimental results forour bootstrapped classifiers.4.3 Results for Bootstrapped ClassifiersThe bottom section of Table 1 displays the resultsfor our bootstrapped classifiers.
The ContextualI.1 row shows results after just the first iteration,8We use an F(1) score, where recall and precision areequally weighted.9For all of our classifiers, supervised and bootstrapped,we label all instances of the seed words first and then applythe classifiers to the unlabeled (non-seed) instances.which trains only the strictly contextual classi-fiers.
The average F score improved from 53.1 forthe seeds alone to 64.3 with the contextual classi-fiers.
The next row, XCategory I.45, shows theresults after cross-category bootstrapping, whichended after 45 iterations.
(We indicate the num-ber of iterations until bootstrapping ended usingthe notation I.#.)
With cross-category bootstrap-ping, the average F score increased from 64.3 to76.1.
A closer inspection reveals that all of the se-mantic categories except HUMAN achieved largerecall gains.
And importantly, these recall gainswere obtained with relatively little loss of preci-sion, with the exception of TEST.Next, we measured the impact of the one-semantic-class-per-discourse heuristic, shown asXCat+OSCPD I.40.
From Table 1, it appears thatOSCPD produced mixed results: recall increasedby 1-4 points for DIS/SYM, DRUG, HUMAN, andOTHER, but precision was inconsistent, improv-ing by +4 for TEST but dropping by -8 for DRUG.However, this single snapshot in time does not tellthe full story.
Figure 2 shows the performanceof the classifiers during the course of bootstrap-ping.
The OSCPD heuristic produced a steeperlearning curve, and consistently improved perfor-mance until the last few iterations when its perfor-mance dipped.
This is probably due to the fact thatnoise gradually increases during bootstrapping, soincorrect labels are more likely and OSCPD willcompound any mistakes by the classifier.
A goodfuture strategy might be to use the OSCPD heuris-tic only during the early stages of bootstrappingwhen the classifier?s decisions are most reliable.We also evaluated the effect of dynamically cre-ated semantic features.
When added to the ba-sic XCategory system, they had almost no ef-fect.
We suspect this is because the semantic fea-tures are sparse during most of the bootstrappingprocess.
However, the semantic features did im-2810 5 10 15 20 25 30 35 40 456466687072747678Fmeasure(%)# of iterationsindependent self?trainingcross?category bootstrapping+OSCPD+OSCPD+SemFeatFigure 2: Average F scores after each iterationprove performance when coupled with the OSCPDheuristic, presumably because the OSCPD heuris-tic aggressively labels more instances in the earlierstages of bootstrapping, increasing the prevalenceof semantic class tags.
The XCat+OSCPD+SFI.39 row in Table 1 shows that the semantic fea-tures coupled with OSCPD dramatically increasedthe precision for DRUG, yielding the best overall Fscore of 76.6.We conducted one additional experiment to as-sess the benefits of cross-category bootstrapping.We created an analogous suite of classifiers usingself-training, where each classifier independentlylabels the instances that it is most confident about,adds them only to its own training set, and thenretrains itself.
The Ind.
Self-Train I.13 row inTable 1 shows that these classifiers achieved only58% recall (compared to 75% for XCategory) andan average F score of 67.4 (compared to 76.1 forXCategory).
One reason for the disparity is thatthe self-training model ended after just 13 boot-strapping cycles (I.13), given the same thresholdvalues.
To see if we could push it further, we low-ered the confidence threshold to 0 and it continuedlearning through 35 iterations.
Even so, its finalscore was 65% recall with 79% precision, which isstill well below the 75% recall with 78% precisionproduced by the XCategory model.
These resultssupport our claim that cross-category bootstrap-ping is more effective than independently self-trained models.Figure 3 tracks the recall and precision scoresof the XCat+OSCPD+SF system as bootstrap-ping progresses.
This graph shows the sustainedmomentum of cross-category bootstrapping: re-0 5 10 15 20 25 30 35 405055606570758085# of iterationsPrecisionRecallFigure 3: Recall and Precision scores duringcross-category bootstrappingcall steadily improves while precision stays con-sistently high with only a slight dropoff at the end.4.4 AnalysisTo assess the impact of corpus size, we generateda learning curve with randomly selected subsetsof the training texts.
Figure 4 shows the average Fscore of our best system using 116 ,18 ,14 ,12 ,34 , andall of the data.
With just 116 th of the training set,the system has about 1,600 message board poststo use for training, which yields a similar F score(roughly 61%) as the supervised baseline that used100 manually annotated posts via 10-fold cross-validation.
So with 16 times more text, seed-basedbootstrapping achieves roughly the same results assupervised learning.
This result reflects the naturaltrade-off between supervised learning and seed-based bootstrapping.
Supervised learning exploitsmanually annotated data, but must make do witha relatively small amount of training text becausemanual annotations are expensive.
In contrast,seed-based bootstrapping exploits a small numberof human-provided seeds, but needs a larger set of(unannotated) texts for training because the seedsproduce relatively sparse annotations of the texts.An additional advantage of seed-based boot-strapping methods is that they can easily exploitunlimited amounts of training text.
For many do-mains, large text collections are readily available.Figure 4 shows a steady improvement in perfor-mance as the amount of training text grows.
Over-all, the F score improves from roughly 61% tonearly 77% simply by giving the system access tomore unannotated text during bootstrapping.We also evaluated the effectiveness of our slid-ing confidence threshold (Section 3.6).
The ta-ble below shows the results using fixed thresholds2820 1/16 1/8 1/4 1/2 3/4 1020406065707580ration of dataFmeasure(%)Figure 4: Learning Curveof 1.0, 1.5, 2.0, as well as the sliding threshold(which begins at 2.0 and ends at 1.0 decreasing by0.2 when the number of newly labeled instancesfalls below 3000 (i.e., < 500 per category, on av-erage).
This table depicts the expected trade-offbetween recall and precision for the fixed thresh-olds, with higher thresholds producing higher pre-cision but lower recall.
The sliding threshold pro-duces the best F score, achieving the best balanceof high recall and precision.
?cf R/P/F1.0 71/77/74.11.5 69/81/74.72.0 65/82/72.4Sliding 75/78/76.6As mentioned in Section 3.3, we used 3 timesas many negative instances as positive instancesfor every semantic category during bootstrap-ping.
This ratio was based on early experimentswhere we needed to limit the number of neg-ative instances per category because the cross-category framework naturally produces an ex-tremely skewed negative/positive training set.
Werevisited this issue to empirically assess the impactof the negative/positive ratio on performance.
Thetable below shows recall, precision, and F scoreresults when we vary the ratio from 1:1 to 5:1.
A1:1 ratio seems to be too conservative, improvingprecision a bit but lowering recall.
However thedifference in performance between the other ra-tios is small.
Our conclusion is that a 1:1 ratio istoo restrictive but, in general, the cross-categorybootstrapping process is relatively insensitive tothe specific negative/positive ratio used.
Our ob-servation from preliminary experiments, however,is that the negative/positive ratio does need to becontrolled, or else the dominant categories over-whelm the less frequent categories with negativeinstances.Neg:Pos R/P/F1:1 72/79/75.22:1 74/78/76.13:1 75/78/76.64:1 75/77/76.05:1 76/77/76.4Finally, we examined performance on genderedpronouns (he/she/him/her), which can refer to ei-ther animals or people in the veterinary domain.84% (220/261) of the gendered pronouns were an-notated as ANIMAL in the test set.
Our classi-fier achieved 95% recall (209/220) and 90% preci-sion (209/232) for ANIMAL and 15% recall (6/41)and 100% precision (6/6) for HUMAN.
So whileit failed to recognize most of the (relatively few)gendered pronouns that refer to a person, it washighly effective at identifying the ANIMAL refer-ences and it was always correct when it did assigna HUMAN tag to a pronoun.5 ConclusionsWe presented a novel technique for inducingdomain-specific semantic class taggers from ahandful of seed words and an unannotated textcollection.
Our results showed that the inducedtaggers achieve good performance on six seman-tic categories associated with the domain of vet-erinary medicine.
Our technique allows seman-tic class taggers to be rapidly created for special-ized domains with minimal human effort.
In futurework, we plan to investigate whether these seman-tic taggers can be used to improve other tasks.AcknowledgmentsWe are very grateful to the people at the VeterinaryInformation Network for providing us access totheir resources.
Special thanks to Paul Pion, DVMand Nicky Mastin, DVM for making their dataavailable to us, and to Sherri Lofing and BeckyLundgren, DVM for their time and expertise increating the gold standard annotations.
This re-search was supported in part by Department ofHomeland Security Grant N0014-07-1-0152 andAir Force Contract FA8750-09-C-0172 under theDARPA Machine Reading Program.ReferencesACE.
2005.
NIST ACE evaluation website.
Inhttp://www.nist.gov/speech/tests/ace/2005.283ACE.
2007.
NIST ACE evaluation website.
Inhttp://www.nist.gov/speech/tests/ace/2007.ACE.
2008.
NIST ACE evaluation website.
Inhttp://www.nist.gov/speech/tests/ace/2008.Daniel M. Bikel, Scott Miller, Richard Schwartz,and Ralph Weischedel.
1997.
Nymble: a high-performance learning name-finder.
In Proceedingsof ANLP-97, pages 194?201.A.
Blum and T. Mitchell.
1998.
Combining Labeledand Unlabeled Data with Co-Training.
In Proceed-ings of the 11th Annual Conference on Computa-tional Learning Theory (COLT-98).Andrew Carlson, Justin Betteridge, Estevam R. Hr-uschka Jr., and Tom M. Mitchell.
2009.
Couplingsemi-supervised learning of categories and relations.In HLT-NAACL 2009 Workshop on Semi-SupervisedLearning for NLP.M.
Collins and Y.
Singer.
1999.
Unsupervised Mod-els for Named Entity Classification.
In Proceedingsof the Joint SIGDAT Conference on Empirical Meth-ods in Natural Language Processing and Very LargeCorpora (EMNLP/VLC-99).S.
Cucerzan and D. Yarowsky.
1999.
Language In-dependent Named Entity Recognition CombiningMorphologi cal and Contextual Evidence.
In Pro-ceedings of the Joint SIGDAT Conference on Empir-ical Methods in Natural Language Processing andVery Large Corpora (EMNLP/VLC-99).O.
Etzioni, M. Cafarella, D. Downey, A. Popescu,T.
Shaked, S. Soderland, D. Weld, and A. Yates.2005.
Unsupervised named-entity extraction fromthe web: an experimental study.
Artificial Intelli-gence, 165(1):91?134, June.M.B.
Fleischman and E.H. Hovy.
2002.
Fine grainedclassification of named entities.
In Proceedings ofthe COLING conference, August.T.
Joachims.
1999.
Making Large-Scale SupportVector Machine Learning Practical.
In A. SmolaB.
Scho?lkopf, C. Burges, editor, Advances in Ker-nel Methods: Support Vector Machines.
MIT Press,Cambridge, MA.S.
Keerthi and D. DeCoste.
2005.
A Modified FiniteNewton Method for Fast Solution of Large ScaleLinear SVMs.
Journal of Machine Learning Re-search.Mamoru Komachi, Taku Kudo, Masashi Shimbo, andYuji Matsumoto.
2008.
Graph-based analysis ofsemantic drift in espresso-like bootstrapping algo-rithms.
In Proceedings of the 2008 Conference onEmpirical Methods in Natural Language Process-ing.Z.
Kozareva, E. Riloff, and E. Hovy.
2008.
SemanticClass Learning from the Web with Hyponym PatternLinkage Graphs.
In Proceedings of the 46th AnnualMeeting of the Association for Computational Lin-guistics: Human Language Technologies (ACL-08).D.
McClosky, E. Charniak, and M Johnson.
2006.
Ef-fective self-training for parsing.
In HLT-NAACL-2006.T.
McIntosh and J. Curran.
2009.
Reducing SemanticDrift with Bagging and Distributional Similarity.
InProceedings of the 47th Annual Meeting of the As-sociation for Computational Linguistics.R.
Mihalcea.
2004.
Co-training and Self-training forWord Sense Disambiguation.
In CoNLL-2004.G.
Miller.
1990.
Wordnet: An On-line LexicalDatabase.
International Journal of Lexicography,3(4).C.
Mueller, S. Rapp, and M. Strube.
2002.
Applyingco-training to reference resolution.
In Proceedingsof the 40th Annual Meeting of the Association forComputational Linguistics.V.
Ng and C. Cardie.
2003.
Weakly supervised naturallanguage learning without redundant views.
In HLT-NAACL-2003.V.
Ng.
2007.
Semantic Class Induction and Corefer-ence Resolution.
In Proceedings of the 45th AnnualMeeting of the Association for Computational Lin-guistics.Cheng Niu, Wei Li, Jihong Ding, and Rohini K. Sri-hari.
2003.
A bootstrapping approach to namedentity classification using successive learners.
InProceedings of the 41st Annual Meeting on Associa-tion for Computational Linguistics (ACL-03), pages335?342.M.
Pas?ca.
2004.
Acquisition of categorized namedentities for web search.
In Proc.
of the ThirteenthACM International Conference on Information andKnowledge Management, pages 137?145.W.
Phillips and E. Riloff.
2002.
Exploiting StrongSyntactic Heuristics and Co-Training to Learn Se-mantic Lexicons.
In Proceedings of the 2002 Con-ference on Empirical Methods in Natural LanguageProcessing, pages 125?132.E.
Riloff and R. Jones.
1999.
Learning Dictionar-ies for Information Extraction by Multi-Level Boot-strapping.
In Proceedings of the Sixteenth NationalConference on Artificial Intelligence.E.
Riloff and J. Shepherd.
1997.
A Corpus-Based Ap-proach for Building Semantic Lexicons.
In Proceed-ings of the Second Conference on Empirical Meth-ods in Natural Language Processing, pages 117?124.B.
Roark and E. Charniak.
1998.
Noun-phrase Co-occurrence Statistics for Semi-automatic SemanticLexicon Construction.
In Proceedings of the 36thAnnual Meeting of the Association for Computa-tional Linguistics, pages 1110?1116.284M.
Thelen and E. Riloff.
2002.
A BootstrappingMethod for Learning Semantic Lexicons Using Ex-traction Pa ttern Contexts.
In Proceedings of the2002 Conference on Empirical Methods in NaturalLanguage Processing, pages 214?221.K.
Toutanova, D. Klein, C. Manning, and Y. Singer.2003.
Feature-Rich Part-of-Speech Tagging witha Cyclic Dependency Network.
In Proceedings ofHLT-NAACL 2003.R.
Yangarber.
2003.
Counter-training in the discoveryof semantic patterns.
In Proceedings of the 41th An-nual Meeting of the Association for ComputationalLinguistics.D.
Yarowsky.
1995.
Unsupervised Word Sense Dis-ambiguation Rivaling Supervised Methods.
In Pro-ceedings of the 33rd Annual Meeting of the Associa-tion for Computational Linguistics.Imed Zitouni and Radu Florian.
2009.
Cross-languageinformation propagation for arabic mention detec-tion.
ACM Transactions on Asian Language Infor-mation Processing (TALIP), 8(4):1?21.285
