Proceedings of ACL-08: HLT, pages 290?298,Columbus, Ohio, USA, June 2008. c?2008 Association for Computational LinguisticsWhen Specialists and Generalists Work Together: Overcoming DomainDependence in Sentiment TaggingAlina AndreevskaiaConcordia UniversityMontreal, Quebecandreev@cs.concordia.caSabine BerglerConcordia UniversityMontreal, Canadabergler@cs.concordia.caAbstractThis study presents a novel approach to theproblem of system portability across differ-ent domains: a sentiment annotation systemthat integrates a corpus-based classifier trainedon a small set of annotated in-domain dataand a lexicon-based system trained on Word-Net.
The paper explores the challenges of sys-tem portability across domains and text gen-res (movie reviews, news, blogs, and productreviews), highlights the factors affecting sys-tem performance on out-of-domain and small-set in-domain data, and presents a new sys-tem consisting of the ensemble of two classi-fiers with precision-based vote weighting, thatprovides significant gains in accuracy and re-call over the corpus-based classifier and thelexicon-based system taken individually.1 IntroductionOne of the emerging directions in NLP is the de-velopment of machine learning methods that per-form well not only on the domain on which theywere trained, but also on other domains, for whichtraining data is not available or is not sufficient toensure adequate machine learning.
Many applica-tions require reliable processing of heterogeneouscorpora, such as the World Wide Web, where thediversity of genres and domains present in the Inter-net limits the feasibility of in-domain training.
Inthis paper, sentiment annotation is defined as theassignment of positive, negative or neutral senti-ment values to texts, sentences, and other linguisticunits.
Recent experiments assessing system porta-bility across different domains, conducted by Aueand Gamon (2005), demonstrated that sentiment an-notation classifiers trained in one domain do not per-form well on other domains.
A number of methodshas been proposed in order to overcome this systemportability limitation by using out-of-domain data,unlabelled in-domain corpora or a combination ofin-domain and out-of-domain examples (Aue andGamon, 2005; Bai et al, 2005; Drezde et al, 2007;Tan et al, 2007).In this paper, we present a novel approach to theproblem of system portability across different do-mains by developing a sentiment annotation sys-tem that integrates a corpus-based classifier witha lexicon-based system trained on WordNet.
Byadopting this approach, we sought to develop asystem that relies on both general and domain-specific knowledge, as humans do when analyzinga text.
The information contained in lexicographi-cal sources, such as WordNet, reflects a lay person?sgeneral knowledge about the world, while domain-specific knowledge can be acquired through classi-fier training on a small set of in-domain data.The first part of this paper reviews the extant lit-erature on domain adaptation in sentiment analy-sis and highlights promising directions for research.The second part establishes a baseline for systemevaluation by drawing comparisons of system per-formance across four different domains/genres -movie reviews, news, blogs, and product reviews.The final, third part of the paper presents our sys-tem, composed of an ensemble of two classifiers ?one trained on WordNet glosses and synsets and theother trained on a small in-domain training set.2902 Domain Adaptation in SentimentResearchMost text-level sentiment classifiers use standardmachine learning techniques to learn and select fea-tures from labeled corpora.
Such approaches workwell in situations where large labeled corpora areavailable for training and validation (e.g., movie re-views), but they do not perform well when trainingdata is scarce or when it comes from a different do-main (Aue and Gamon, 2005; Read, 2005), topic(Read, 2005) or time period (Read, 2005).
There aretwo alternatives to supervised machine learning thatcan be used to get around this problem: on the onehand, general lists of sentiment clues/features can beacquired from domain-independent sources such asdictionaries or the Internet, on the other hand, unsu-pervised and weakly-supervised approaches can beused to take advantage of a small number of anno-tated in-domain examples and/or of unlabelled in-domain data.The first approach, using general word lists au-tomatically acquired from the Internet or from dic-tionaries, outperforms corpus-based classifiers whensuch classifiers use out-of-domain training data orwhen the training corpus is not sufficiently large toaccumulate the necessary feature frequency infor-mation.
But such general word lists were shown toperform worse than statistical models built on suf-ficiently large in-domain training sets of movie re-views (Pang et al, 2002).
On other domains, suchas product reviews, the performance of systems thatuse general word lists is comparable to the perfor-mance of supervised machine learning approaches(Gamon and Aue, 2005).The recognition of major performance deficien-cies of supervised machine learning methods withinsufficient or out-of-domain training brought aboutan increased interest in unsupervised and weakly-supervised approaches to feature learning.
For in-stance, Aue and Gamon (2005) proposed trainingon a samll number of labeled examples and largequantities of unlabelled in-domain data.
This sys-tem performed well even when compared to sys-tems trained on a large set of in-domain examples:on feedback messages from a web survey on knowl-edge bases, Aue and Gamon report 73.86% accu-racy using unlabelled data compared to 77.34% forin-domain and 72.39% for the best out-of-domaintraining on a large training set.Drezde et al (2007) applied structural corre-spondence learning (Drezde et al, 2007) to the taskof domain adaptation for sentiment classification ofproduct reviews.
They showed that, depending onthe domain, a small number (e.g., 50) of labeledexamples allows to adapt the model learned on an-other corpus to a new domain.
However, they notethat the success of such adaptation and the num-ber of necessary in-domain examples depends onthe similarity between the original domain and thenew one.
Similarly, Tan et al (2007) suggested tocombine out-of-domain labeled examples with unla-belled ones from the target domain in order to solvethe domain-transfer problem.
They applied an out-of-domain-trained SVM classifier to label examplesfrom the target domain and then retrained the classi-fier using these new examples.
In order to maximizethe utility of the examples from the target domain,these examples were selected using Similarity Rank-ing and Relative Similarity Ranking algorithms (Tanet al, 2007).
Depending on the similarity betweendomains, this method brought up to 15% gain com-pared to the baseline SVM.Overall, the development of semi-supervised ap-proaches to sentiment tagging is a promising direc-tion of the research in this area but so far, basedon reported results, the performance of such meth-ods is inferior to the supervised approaches with in-domain training and to the methods that use generalword lists.
It also strongly depends on the similaritybetween the domains as has been shown by (Drezdeet al, 2007; Tan et al, 2007).3 Factors Affecting System PerformanceThe comparison of system performance across dif-ferent domains involves a number of factors that cansignificantly affect system performance ?
from train-ing set size to level of analysis (sentence or entiredocument), document domain/genre and many otherfactors.
In this section we present a series of experi-ments conducted to assess the effects of different ex-ternal factors (i.e., factors unrelated to the merits ofthe system itself) on system performance in order toestablish the baseline for performance comparisonsacross different domains/genres.2913.1 Level of AnalysisResearch on sentiment annotation is usually con-ducted at the text (Aue and Gamon, 2005; Pang etal., 2002; Pang and Lee, 2004; Riloff et al, 2006;Turney, 2002; Turney and Littman, 2003) or at thesentence levels (Gamon and Aue, 2005; Hu and Liu,2004; Kim and Hovy, 2005; Riloff et al, 2006).
Itshould be noted that each of these levels presents dif-ferent challenges for sentiment annotation.
For ex-ample, it has been observed that texts often containmultiple opinions on different topics (Turney, 2002;Wiebe et al, 2001), which makes assignment of theoverall sentiment to the whole document problem-atic.
On the other hand, each individual sentencecontains a limited number of sentiment clues, whichoften negatively affects the accuracy and recall ifthat single sentiment clue encountered in the sen-tence was not learned by the system.Since the comparison of sentiment annotationsystem performance on texts and on sentenceshas not been attempted to date, we also soughtto close this gap in the literature by conductingthe first set of our comparative experiments ondata sets of 2,002 movie review texts and 10,662movie review snippets (5331 with positive and5331 with negative sentiment) provided by Bo Pang(http://www.cs.cornell.edu/People/pabo/movie-review-data/).3.2 Domain EffectsThe second set of our experiments explores systemperformance on different domains at sentence level.For this we used four different data sets of sentencesannotated with sentiment tags:?
A set of movie review snippets (further: movie)from (Pang and Lee, 2005).
This dataset of10,662 snippets was collected automaticallyfrom www.rottentomatoes.com website.
Allsentences in reviews marked ?rotten?
were con-sidered negative and snippets from ?fresh?
re-views were deemed positive.
In order to makethe results obtained on this dataset comparableto other domains, a randomly selected subset of1066 snippets was used in the experiments.?
A balanced corpus of 800 manually annotatedsentences extracted from 83 newspaper texts(further, news).
The full set of sentenceswas annotated by one judge.
200 sentencesfrom this corpus (100 positive and 100 neg-ative) were also randomly selected from thecorpus for an inter-annotator agreement studyand were manually annotated by two indepen-dent annotators.
The pairwise agreement be-tween annotators was calculated as the percentof same tags divided by the number of sen-tences with this tag in the gold standard.
Thepair-wise agreement between the three anno-tators ranged from 92.5 to 95.9% (?=0.74 and0.75 respectively) on positive vs. negative tags.?
A set of sentences taken from personalweblogs (further, blogs) posted on Live-Journal (http://www.livejournal.com) and onhttp://www.cyberjournalist.com.
This corpusis composed of 800 sentences (400 sentenceswith positive and 400 sentences with negativesentiment).
In order to establish the inter-annotator agreement, two independent judgeswere asked to annotate 200 sentences from thiscorpus.
The agreement between the two an-notators on positive vs. negative tags reached99% (?=0.97).?
A set of 1200 product review (PR) sentencesextracted from the annotated corpus madeavailable by Bing Liu (Hu and Liu, 2004)(http://www.cs.uic.edu/ liub/FBS/FBS.html).The data set sizes are summarized in Table 1.Movies News Blogs PRText level 2002 texts n/a n/a n/aSentence level 10662 800 800 1200snippets sent.
sent.
sent.Table 1: Datasets3.3 Establishing a Baseline for a Corpus-basedSystem (CBS)Supervised statistical methods have been very suc-cessful in sentiment tagging of texts: on movie re-view texts they reach accuracies of 85-90% (Aueand Gamon, 2005; Pang and Lee, 2004).
Thesemethods perform particularly well when a large vol-ume of labeled data from the same domain as the292test set is available for training (Aue and Gamon,2005).
For this reason, most of the research on senti-ment tagging using statistical classifiers was limitedto product and movie reviews, where review authorsusually indicate their sentiment in a form of a stan-dardized score that accompanies the texts of their re-views.The lack of sufficient data for training appears tobe the main reason for the virtual absence of exper-iments with statistical classifiers in sentiment tag-ging at the sentence level.
To our knowledge, theonly work that describes the application of statis-tical classifiers (SVM) to sentence-level sentimentclassification is (Gamon and Aue, 2005)1.
The av-erage performance of the system on ternary clas-sification (positive, negative, and neutral) was be-tween 0.50 and 0.52 for both average precision andrecall.
The results reported by (Riloff et al, 2006)for binary classification of sentences in a relateddomain of subjectivity tagging (i.e., the separationof sentiment-laden from neutral sentences) suggestthat statistical classifiers can perform well on thistask: the authors have reached 74.9% accuracy onthe MPQA corpus (Riloff et al, 2006).In order to explore the performance of dif-ferent approaches in sentiment annotation at thetext and sentence levels, we used a basic Na?
?veBayes classifier.
It has been shown that bothNa?
?ve Bayes and SVMs perform with similar ac-curacy on different sentiment tagging tasks (Pangand Lee, 2004).
These observations were con-firmed with our own experiments with SVMs andNa?
?ve Bayes (Table 3).
We used the Weka pack-age (http://www.cs.waikato.ac.nz/ml/weka/) withdefault settings.In the sections that follow, we describe a setof comparative experiments with SVMs and Na?
?veBayes classifiers (1) on texts and sentences and (2)on four different domains (movie reviews, news,blogs, and product reviews).
System runs with un-igrams, bigrams, and trigrams as features and withdifferent training set sizes are presented.1Recently, a similar task has been addressed by the AffectiveText Task at SemEval-1 where even shorter units ?
headlines?
were classified into positive, negative and neutral categoriesusing a variety of techniques (Strapparava and Mihalcea, 2007).4 Experiments4.1 System Performance on Texts vs. SentencesThe experiments comparing in-domain trained sys-tem performance on texts vs. sentences were con-ducted on 2,002 movie review texts and on 10,662movie review snippets.
The results with 10-foldcross-validation are reported in Table 22.Trained on Texts Trained on Sent.Tested on Tested on Tested on Tested onTexts Sent.
Texts Sent.1gram 81.1 69.0 66.8 77.42gram 83.7 68.6 71.2 73.93gram 82.5 64.1 70.0 65.4Table 2: Accuracy of Na?
?ve Bayes on movie reviews.Consistent with findings in the literature (Cui etal., 2006; Dave et al, 2003; Gamon and Aue, 2005),on the large corpus of movie review texts, the in-domain-trained system based solely on unigramshad lower accuracy than the similar system trainedon bigrams.
But the trigrams fared slightly worsethan bigrams.
On sentences, however, we have ob-served an inverse pattern: unigrams performed bet-ter than bigrams and trigrams.
These results high-light a special property of sentence-level annota-tion: greater sensitivity to sparseness of the model:On texts, classifier error on one particular sentimentmarker is often compensated by a number of cor-rectly identified other sentiment clues.
Since sen-tences usually contain a much smaller number ofsentiment clues than texts, sentence-level annota-tion more readily yields errors when a single sen-timent clue is incorrectly identified or missed bythe system.
Due to lower frequency of higher-ordern-grams (as opposed to unigrams), higher-order n-gram language models are more sparse, which in-creases the probability of missing a particular sen-timent marker in a sentence (Table 33).
Very large2All results are statistically significant at ?
= 0.01 with twoexceptions: the difference between trigrams and bigrams for thesystem trained and tested on texts is statistically significant atalpha=0.1 and for the system trained on sentences and tested ontexts is not statistically significant at ?
= 0.01.3The results for movie reviews are lower than those reportedin Table 2 since the dataset is 10 times smaller, which resultsin less accurate classification.
The statistical significance of the293training sets are required to overcome this higher n-gram sparseness in sentence-level annotation.Dataset Movie News Blogs PRsDataset size 1066 800 800 1200unigramsSVM 68.5 61.5 63.85 76.9NB 60.2 59.5 60.5 74.25nb features 5410 4544 3615 2832bigramsSVM 59.9 63.2 61.5 75.9NB 57.0 58.4 59.5 67.8nb features 16286 14633 15182 12951trigramsSVM 54.3 55.4 52.7 64.4NB 53.3 57.0 56.0 69.7nb features 20837 18738 19847 19132Table 3: Accuracy of unigram, bigram and trigram mod-els across domains.4.2 System Performance on Different DomainsIn the second set of experiments we sought to com-pare system results on sentences using in-domainand out-of-domain training.
Table 4 shows that in-domain training, as expected, consistently yields su-perior accuracy than out-of-domain training acrossall four datasets: movie reviews (Movies), news,blogs, and product reviews (PRs).
The numbers forin-domain trained runs are highlighted in bold.Test DataTraining Data Movies News Blogs PRsMovies 68.5 55.2 53.2 60.7News 55.0 61.5 56.25 57.4Blogs 53.7 49.9 63.85 58.8PRs 55.8 55.9 56.25 76.9Table 4: Accuracy of SVM with unigram modelresults depends on the genre and size of the n-gram: on prod-uct reviews, all results are statistically significant at ?
= 0.025level; on movie reviews, the difference between Nav?e Bayesand SVM is statistically significant at ?
= 0.01 but the signif-icance diminishes as the size of the n-gram increases; on news,only bi-grams produce a statistically significant (?
= 0.01) dif-ference between the two machine learning methods, while onblogs the difference between SVMs and Nav?e Bayes is mostpronounced when unigrams are used (?
= 0.025).It is interesting to note that on sentences, regard-less of the domain used in system training and re-gardless of the domain used in system testing, un-igrams tend to perform better than higher-order n-grams.
This observation suggests that, given theconstraints on the size of the available training sets,unigram-based systems may be better suited forsentence-level sentiment annotation.5 Lexicon-Based ApproachThe search for a base-learner that can produce great-est synergies with a classifier trained on small-setin-domain data has turned our attention to lexicon-based systems.
Since the benefits from combiningclassifiers that always make similar decisions is min-imal, the two (or more) base-learners should com-plement each other (Alpaydin, 2004).
Since a sys-tem based on a fairly different learning approachis more likely to produce a different decision un-der a given set of circumstances, the diversity ofapproaches integrated in the ensemble of classifierswas expected to have a beneficial effect on the over-all system performance.A lexicon-based approach capitalizes on thefact that dictionaries, such as WordNet (Fell-baum, 1998), contain a comprehensive and domain-independent set of sentiment clues that exist ingeneral English.
A system trained on such gen-eral data, therefore, should be less sensitive to do-main changes.
This robustness, however is expectedto come at some cost, since some domain-specificsentiment clues may not be covered in the dictio-nary.
Our hypothesis was, therefore, that a lexicon-based system will perform worse than an in-domaintrained classifier but possibly better than a classifiertrained on out-of domain data.One of the limitations of general lexicons anddictionaries, such as WordNet (Fellbaum, 1998), astraining sets for sentiment tagging systems is thatthey contain only definitions of individual wordsand, hence, only unigrams could be effectivelylearned from dictionary entries.
Since the struc-ture of WordNet glosses is fairly different fromthat of other types of corpora, we developed a sys-tem that used the list of human-annotated adjec-tives from (Hatzivassiloglou and McKeown, 1997)as a seed list and then learned additional unigrams294from WordNet synsets and glosses with up to 88%accuracy, when evaluated against General Inquirer(Stone et al, 1966) (GI) on the intersection of ourautomatically acquired list with GI.
In order to ex-pand the list coverage for our experiments at the textand sentence levels, we then augmented the list byadding to it all the words annotated with ?Positiv?or ?Negativ?
tags in GI, that were not picked up bythe system.
The resulting list of features contained11,000 unigrams with the degree of membership inthe category of positive or negative sentiment as-signed to each of them.In order to assign the membership score to eachword, we did 58 system runs on unique non-intersecting seed lists drawn from manually anno-tated list of positive and negative adjectives from(Hatzivassiloglou and McKeown, 1997).
The 58runs were then collapsed into a single set of 7,813unique words.
For each word we computed a scoreby subtracting the total number of runs assigningthis word a negative sentiment from the total of theruns that consider it positive.
The resulting measure,termed Net Overlap Score (NOS), reflected the num-ber of ties linking a given word with other sentiment-laden words in WordNet, and hence, could be usedas a measure of the words?
centrality in the fuzzycategory of sentiment.
The NOSs were then normal-ized into the interval from -1 to +1 using a sigmoidfuzzy membership function (Zadeh, 1975)4.
Onlywords with fuzzy membership degree not equal tozero were retained in the list.
The resulting listcontained 10,809 sentiment-bearing words of differ-ent parts of speech.
The sentiment determination atthe sentence and text level was then done by sum-ming up the scores of all identified positive unigrams(NOS>0) and all negative unigrams (NOS<0) (An-dreevskaia and Bergler, 2006).5.1 Establishing a Baseline for theLexicon-Based System (LBS)The baseline performance of the Lexicon-BasedSystem (LBS) described above is presented in Ta-ble 5, along with the performance results of the in-domain- and out-of-domain-trained SVM classifier.Table 5 confirms the predicted pattern: theLBS performs with lower accuracy than in-domain-4With coefficients: ?=1, ?=15.Movies News Blogs PRsLBS 57.5 62.3 63.3 59.3SVM in-dom.
68.5 61.5 63.85 76.9SVM out-of-dom.
55.8 55.9 56.25 60.7Table 5: System accuracy on best runs on sentencestrained corpus-based classifiers, and with similaror better accuracy than the corpus-based classifierstrained on out-of-domain data.
Thus, the lexicon-based approach is characterized by a bounded butstable performance when the system is ported acrossdomains.
These performance characteristics ofcorpus-based and lexicon-based approaches promptfurther investigation into the possibility to combinethe portability of dictionary-trained systems with theaccuracy of in-domain trained systems.6 Integrating the Corpus-based andDictionary-based ApproachesThe strategy of integration of two or more sys-tems in a single ensemble of classifiers has beenactively used on different tasks within NLP.
In sen-timent tagging and related areas, Aue and Gamon(2005) demonstrated that combining classifiers canbe a valuable tool in domain adaptation for senti-ment analysis.
In the ensemble of classifiers, theyused a combination of nine SVM-based classifiersdeployed to learn unigrams, bigrams, and trigramson three different domains, while the fourth domainwas used as an evaluation set.
Using then an SVMmeta-classifier trained on a small number of targetdomain examples to combine the nine base clas-sifiers, they obtained a statistically significant im-provement on out-of-domain texts from book re-views, knowledge-base feedback, and product sup-port services survey data.
No improvement occurredon movie reviews.Pang and Lee (2004) applied two different clas-sifiers to perform sentiment annotation in two se-quential steps: the first classifier separated subjec-tive (sentiment-laden) texts from objective (neutral)ones and then they used the second classifier to clas-sify the subjective texts into positive and negative.Das and Chen (2004) used five classifiers to deter-mine market sentiment on Yahoo!
postings.
Simplemajority vote was applied to make decisions within295the ensemble of classifiers and achieved accuracy of62% on ternary in-domain classification.In this study we describe a system that attempts tocombine the portability of a dictionary-trained sys-tem (LBS) with the accuracy of an in-domain trainedcorpus-based system (CBS).
The selection of thesetwo classifiers for this system, thus, was theory-based.
The section that follows describes the classi-fier integration and presents the performance resultsof the system consisting of an ensemble CBS andLBS classifier and a precision-based vote weightingprocedure.6.1 The Classifier Integration Procedure andSystem EvaluationThe comparative analysis of the corpus-based andlexicon-based systems described above revealed thatthe errors produced by CBS and LBS were to agreat extent complementary (i.e., where one classi-fier makes an error, the other tends to give the cor-rect answer).
This provided further justification tothe integration of corpus-based and lexicon-basedapproaches in a single system.Table 6 below illustrates the complementarity ofthe performance CBS and LBS classifiers on thepositive and negative categories.
In this experiment,the corpus-based classifier was trained on 400 an-notated product review sentences5.
The two systemswere then evaluated on a test set of another 400 prod-uct review sentences.
The results reported in Table 6are statistically significant at ?
= 0.01.CBS LBSPrecision positives 89.3% 69.3%Precision negatives 55.5% 81.5%Pos/Neg Precision 58.0% 72.1%Table 6: Base-learners?
precision and recall on productreviews on test data.Table 6 shows that the corpus-based system has avery good precision on those sentences that it classi-fies as positive but makes a lot of errors on those sen-tences that it deems negative.
At the same time, thelexicon-based system has low precision on positives5The small training set explains relatively low overall per-formance of the CBS system.and high precision on negatives6.
Such complemen-tary distribution of errors produced by the two sys-tems was observed on different data sets from differ-ent domains, which suggests that the observed dis-tribution pattern reflects the properties of each ofthe classifiers, rather than the specifics of the do-main/genre.In order to take advantage of the observed com-plementarity of the two systems, the following pro-cedure was used.
First, a small set of in-domaindata was used to train the CBS system.
Then bothCBS and LBS systems were run separately on thesame training set, and for each classifier, the preci-sion measures were calculated separately for thosesentences that the classifier considered positive andthose it considered negative.
The chance-level per-formance (50%) was then subtracted from the pre-cision figures to ensure that the final weights reflectby how much the classifier?s precision exceeds thechance level.
The resulting chance-adjusted preci-sion numbers of the two classifiers were then nor-malized, so that the weights of CBS and LBS clas-sifiers sum up to 100% on positive and to 100% onnegative sentences.
These weights were then usedto adjust the contribution of each classifier to the de-cision of the ensemble system.
The choice of theweight applied to the classifier decision, thus, varieddepending on whether the classifier scored a givensentence as positive or as negative.
The resultingsystem was then tested on a separate test set of sen-tences7.
The small-set training and evaluation exper-iments with the system were performed on differentdomains using 3-fold validation.The experiments conducted with the Ensemblesystem were designed to explore system perfor-mance under conditions of limited availability of an-notated data for classifier training.
For this reason,the numbers reported for the corpus-based classifierdo not reflect the full potential of machine learn-ing approaches when sufficient in-domain trainingdata is available.
Table 7 presents the results ofthese experiments by domain/genre.
The results6These results are consistent with an observation in(Kennedy and Inkpen, 2006), where a lexicon-based systemperformed with a better precision on negative than on positivetexts.7The size of the test set varied in different experiments dueto the availability of annotated data for a particular domain.296are statistically significant at ?
= 0.01, except theruns on movie reviews where the difference betweenthe LBS and Ensemble classifiers was significant at?
= 0.05.LBS CBS EnsembleNews Acc 67.8 53.2 73.3F 0.82 0.71 0.85Movies Acc 54.5 53.5 62.1F 0.73 0.72 0.77Blogs Acc 61.2 51.1 70.9F 0.78 0.69 0.83PRs Acc 59.5 58.9 78.0F 0.77 0.75 0.88Average Acc 60.7 54.2 71.1F 0.77 0.72 0.83Table 7: Performance of the ensemble classifierTable 7 shows that the combination of two classi-fiers into an ensemble using the weighting techniquedescribed above leads to consistent improvement insystem performance across all domains/genres.
Inthe ensemble system, the average gain in accuracyacross the four domains was 16.9% relative to CBSand 10.3% relative to LBS.
Moreover, the gain inaccuracy and precision was not offset by decreasesin recall: the net gain in recall was 7.4% relative toCBS and 13.5% vs. LBS.
The ensemble system onaverage reached 99.1% recall.
The F-measure hasincreased from 0.77 and 0.72 for LBS and CBS clas-sifiers respectively to 0.83 for the whole ensemblesystem.7 DiscussionThe development of domain-independent sentimentdetermination systems poses a substantial challengefor researchers in NLP and artificial intelligence.The results presented in this study suggest that theintegration of two fairly different classifier learningapproaches in a single ensemble of classifiers canyield substantial gains in system performance on allmeasures.
The most substantial gains occurred inrecall, accuracy, and F-measure.This study permits to highlight a set of factorsthat enable substantial performance gains with theensemble of classifiers approach.
Such gains aremost likely when (1) the errors made by the clas-sifiers are complementary, i.e., where one classifiermakes an error, the other tends to give the correctanswer, (2) the classifier errors are not fully randomand occur more often in a certain segment (or cate-gory) of classifier results, and (3) there is a way fora system to identify that low-precision segment andreduce the weights of that classifier?s results on thatsegment accordingly.
The two classifiers used in thisstudy ?
corpus-based and lexicon-based ?
providedan interesting illustration of potential performancegains associated with these three conditions.
Theuse of precision of classifier results on the positivesand negatives proved to be an effective technique forclassifier vote weighting within the ensemble.8 ConclusionThis study contributes to the research on sentimenttagging, domain adaptation, and the development ofensembles of classifiers (1) by proposing a novel ap-proach for sentiment determination at sentence leveland delineating the conditions under which great-est synergies among combined classifiers can beachieved, (2) by describing a precision-based tech-nique for assigning differential weights to classifierresults on different categories identified by the clas-sifier (i.e., categories of positive vs. negative sen-tences), and (3) by proposing a new method for sen-timent annotation in situations where the annotatedin-domain data is scarce and insufficient to ensureadequate performance of the corpus-based classifier,which still remains the preferred choice when largevolumes of annotated data are available for systemtraining.Among the most promising directions for futureresearch in the direction laid out in this paper is thedeployment of more advanced classifiers and fea-ture selection techniques that can further enhancethe performance of the ensemble of classifiers.
Theprecision-based vote weighting technique may proveto be effective also in situations, where more thantwo classifiers are integrated into a single system.We expect that these more advanced ensemble-of-classifiers systems would inherit the benefits of mul-tiple complementary approaches to sentiment anno-tation and will be able to achieve better and morestable accuracy on in-domain, as well as on out-of-domain data.297ReferencesEthem Alpaydin.
2004.
Introduction to Machine Learn-ing.
The MIT Press, Cambridge, MA.Alina Andreevskaia and Sabine Bergler.
2006.
MiningWordNet for a fuzzy sentiment: Sentiment tag extrac-tion from WordNet glosses.
In Proceedings the 11thConference of the European Chapter of the Associa-tion for Computational Linguistics, Trento, IT.Anthony Aue and Michael Gamon.
2005.
Customizingsentiment classifiers to new domains: a case study.
InProccedings of the International Conference on RecentAdvances in Natural Language Processing, Borovets,BG.Xue Bai, Rema Padman, and Edoardo Airoldi.
2005.
Onlearning parsimonious models for extracting consumeropinions.
In Proceedings of the 38th Annual HawaiiInternational Conference on System Sciences, Wash-ington, DC.Hang Cui, Vibhu Mittal, and Mayur Datar.
2006.
Com-parative experiments on sentiment classification foronline product reviews.
In Proceedings of the 21stInternational Conference on Artificial Intelligence,Boston, MA.Kushal Dave, Steve Lawrence, and David M. Pennock.2003.
Mining the Peanut gallery: opinion extractionand semantic classification of product reviews.
In Pro-ceedings of WWW03, Budapest, HU.Mark Drezde, John Blitzer, and Fernando Pereira.
2007.Biographies, Bollywood, Boom-boxes and Blenders:Domain Adaptation for Sentiment Classification.
InProceedings of the 45th Annual Meeting of the Associ-ation for Computational Linguistics, Prague, CZ.Christiane Fellbaum, editor.
1998.
WordNet: An Elec-tronic Lexical Database.
MIT Press, Cambridge, MA.Michael Gamon and Anthony Aue.
2005.
Automaticidentification of sentiment vocabulary: exploiting lowassociation with known sentiment terms.
In Proceed-ings of the ACL-05 Workshop on Feature Engineeringfor Machine Learning in Natural Language Process-ing, Ann Arbor, US.Vasileios Hatzivassiloglou and Kathleen B. McKeown.1997.
Predicting the Semantic Orientation of Adjec-tives.
In Proceedings of the the 40th Annual Meetingof the Association of Computational Linguistics.Minqing Hu and Bing Liu.
2004.
Mining and summariz-ing customer reviews.
In KDD-04, pages 168?177.Alistair Kennedy and Diana Inkpen.
2006.
Senti-ment Classification of Movie Reviews Using Con-textual Valence Shifters.
Computational Intelligence,22(2):110?125.Soo-Min Kim and Eduard Hovy.
2005.
Automatic detec-tion of opinion bearing words and sentences.
In Pro-ceedings of the Second International Joint Conferenceon Natural Language Processing, Companion Volume,Jeju Island, KR.Bo Pang and Lilian Lee.
2004.
A sentiment education:Sentiment analysis using subjectivity summarizationbased on minimum cuts.
In Proceedings of the 42ndMeeting of the Association for Computational Linguis-tics.Bo Pang and Lillian Lee.
2005.
Seeing stars: Exploitingclass relationships for sentiment categorization withrespect to rating scales.
In Proceedings of the 43ndMeeting of the Association for Computational Linguis-tics, Ann Arbor, US.Bo Pang, Lilian Lee, and Shrivakumar Vaithyanathan.2002.
Thumbs up?
Sentiment classification using ma-chine learning techniques.
In Conference on Empiri-cal Methods in Natural Language Processing.Jonathon Read.
2005.
Using emoticons to reduce depen-dency in machine learning techniques for sentimentclassification.
In Proceedings of the ACL-2005 Stu-dent Research Workshop, Ann Arbor, MI.Ellen Riloff, Siddharth Patwardhan, and Janyce Wiebe.2006.
Feature subsumption for opinion analysis.
InProceedings of the Conference on Empirical Methodsin Natural Language Processing, Sydney, AUS.P.J.
Stone, D.C. Dumphy, M.S.
Smith, and D.M.
Ogilvie.1966.
The General Inquirer: a computer approach tocontent analysis.
M.I.T.
studies in comparative poli-tics.
M.I.T.
Press, Cambridge, MA.Carlo Strapparava and Rada Mihalcea.
2007.
SemEval-2007 Task 14: Affective Text.
In Proceedings of the4th International Workshop on Semantic Evaluations,Prague, CZ.Songbo Tan, Gaowei Wu, Huifeng Tang, and ZueqiCheng.
2007.
A Novel Scheme for Domain-transferProblem in the context of Sentiment Analysis.
In Pro-ceedings of CIKM 2007.Peter Turney and Michael Littman.
2003.
Measuringpraise and criticism: inference of semantic orientationfrom association.
ACM Transactions on InformationSystems (TOIS), 21:315?346.Peter Turney.
2002.
Thumbs up or thumbs down?
Se-mantic orientation applied to unsupervised classifica-tion of reviews.
In Proceedings of the 40th AnnualMeeting of the Association of Computational Linguis-tics.Janyce Wiebe, Rebecca Bruce, Matthew Bell, MelanieMartin, and Theresa Wilson.
2001.
A corpus study ofEvaluative and Speculative Language.
In Proceedingsof the 2nd ACL SIGDial Workshop on Discourse andDialogue, Aalberg, DK.Lotfy A. Zadeh.
1975.
Calculus of Fuzzy Restrictions.In L.A. Zadeh et al, editor, Fuzzy Sets and their Ap-plications to cognitive and decision processes, pages1?40.
Academic Press Inc., New-York.298
