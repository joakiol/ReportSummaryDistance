Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 460?469,Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational LinguisticsPrefix Probabilityfor Probabilistic Synchronous Context-Free GrammarsMark-Jan NederhofSchool of Computer ScienceUniversity of St AndrewsNorth Haugh, St Andrews, FifeKY16 9SXUnited Kingdommarkjan.nederhof@googlemail.comGiorgio SattaDept.
of Information EngineeringUniversity of Paduavia Gradenigo, 6/AI-35131 PadovaItalysatta@dei.unipd.itAbstractWe present a method for the computation ofprefix probabilities for synchronous context-free grammars.
Our framework is fairly gen-eral and relies on the combination of a sim-ple, novel grammar transformation and stan-dard techniques to bring grammars into nor-mal forms.1 IntroductionWithin the area of statistical machine translation,there has been a growing interest in so-called syntax-based translation models, that is, models that de-fine mappings between languages through hierar-chical sentence structures.
Several such statisticalmodels that have been investigated in the literatureare based on synchronous rewriting or tree transduc-tion.
Probabilistic synchronous context-free gram-mars (PSCFGs) are one among the most popular ex-amples of such models.
PSCFGs subsume severalsyntax-based statistical translation models, as for in-stance the stochastic inversion transduction gram-mars of Wu (1997), the statistical model used by theHiero system of Chiang (2007), and systems whichextract rules from parsed text, as in Galley et al(2004).Despite the widespread usage of models related toPSCFGs, our theoretical understanding of this classis quite limited.
In contrast to the closely relatedclass of probabilistic context-free grammars, a syn-tax model for which several interesting mathemati-cal and statistical properties have been investigated,as for instance by Chi (1999), many theoretical prob-lems are still unsolved for the class of PSCFGs.This paper considers a parsing problem that iswell understood for probabilistic context-free gram-mars but that has never been investigated in the con-text of PSCFGs, viz.
the computation of prefix prob-abilities.
In the case of a probabilistic context-freegrammar, this problem is defined as follows.
Weare asked to compute the probability that a sentencegenerated by our model starts with a prefix string vgiven as input.
This quantity is defined as the (pos-sibly infinite) sum of the probabilities of all stringsof the form vw, for any string w over the alphabetof the model.
This problem has been studied byJelinek and Lafferty (1991) and by Stolcke (1995).Prefix probabilities can be used to compute probabil-ity distributions for the next word or part-of-speech.This has applications in incremental processing oftext or speech from left to right; see again (Jelinekand Lafferty, 1991).
Prefix probabilities can also beexploited in speech understanding systems to scorepartial hypotheses in beam search (Corazza et al,1991).This paper investigates the problem of computingprefix probabilities for PSCFGs.
In this context, apair of strings v1 and v2 is given as input, and we areasked to compute the probability that any string inthe source language starting with prefix v1 is trans-lated into any string in the target language startingwith prefix v2.
This probability is more preciselydefined as the sum of the probabilities of translationpairs of the form [v1w1, v2w2], for any strings w1and w2.A special case of prefix probability for PSCFGsis the right prefix probability.
This is defined as theprobability that some (complete) input string w inthe source language is translated into a string in thetarget language starting with an input prefix v.460Prefix probabilities and right prefix probabilitiesfor PSCFGs can be exploited to compute probabil-ity distributions for the next word or part-of-speechin left-to-right incremental translation, essentially inthe same way as described by Jelinek and Lafferty(1991) for probabilistic context-free grammars, asdiscussed later in this paper.Our solution to the problem of computing prefixprobabilities is formulated in quite different termsfrom the solutions by Jelinek and Lafferty (1991)and by Stolcke (1995) for probabilistic context-freegrammars.
In this paper we reduce the computationof prefix probabilities for PSCFGs to the computa-tion of inside probabilities under the same model.Computation of inside probabilities for PSCFGs isa well-known problem that can be solved using off-the-shelf algorithms that extend basic parsing algo-rithms.
Our reduction is a novel grammar trans-formation, and the proof of correctness proceedsby fairly conventional techniques from formal lan-guage theory, relying on the correctness of standardmethods for the computation of inside probabilitiesfor PSCFG.
This contrasts with the techniques pro-posed by Jelinek and Lafferty (1991) and by Stolcke(1995), which are extensions of parsing algorithmsfor probabilistic context-free grammars, and requireconsiderably more involved proofs of correctness.Our method for computing the prefix probabili-ties for PSCFGs runs in exponential time, since thatis the running time of existing methods for comput-ing the inside probabilities for PSCFGs.
It is un-likely this can be improved, because the recogni-tion problem for PSCFG is NP-complete, as estab-lished by Satta and Peserico (2005), and there is astraightforward reduction from the recognition prob-lem for PSCFGs to the problem of computing theprefix probabilities for PSCFGs.2 DefinitionsIn this section we introduce basic definitions re-lated to synchronous context-free grammars andtheir probabilistic extension; our notation followsSatta and Peserico (2005).Let N and ?
be sets of nonterminal and terminalsymbols, respectively.
In what follows we need torepresent bijections between the occurrences of non-terminals in two strings over N ??.
This is realizedby annotating nonterminals with indices from an in-finite set.
We define I(N) = {A t | A ?
N, t ?N} and VI = I(N) ?
?.
For a string ?
?
V ?I , wewrite index(?)
to denote the set of all indices thatappear in symbols in ?.Two strings ?1, ?2 ?
V ?I are synchronous if eachindex from N occurs at most once in ?1 and at mostonce in ?2, and index(?1) = index(?2).
Therefore?1, ?2 have the general form:?1 = u10At111 u11At212 u12 ?
?
?u1r?1Atr1r u1r?2 = u20Atpi(1)21 u21Atpi(2)22 u22 ?
?
?u2r?1Atpi(r)2r u2rwhere r ?
0, u1i, u2i ?
?
?, Ati1i , Atpi(i)2i ?
I(N),ti 6= tj for i 6= j, and pi is a permutation of the set{1, .
.
.
, r}.A synchronous context-free grammar (SCFG)is a tuple G = (N,?,P, S), where N and ?
are fi-nite, disjoint sets of nonterminal and terminal sym-bols, respectively, S ?
N is the start symbol andP is a finite set of synchronous rules.
Each syn-chronous rule has the form s : [A1 ?
?1, A2 ?
?2], where A1, A2 ?
N and where ?1, ?2 ?
V ?I aresynchronous strings.
The symbol s is the label ofthe rule, and each rule is uniquely identified by itslabel.
For technical reasons, we allow the existenceof multiple rules that are identical apart from theirlabels.
We refer to A1 ?
?1 and A2 ?
?2, respec-tively, as the left and right components of rule s.Example 1 The following synchronous rules im-plicitly define a SCFG:s1 : [S ?
A1 B 2 , S ?
B 2 A 1 ]s2 : [A ?
aA1 b, A ?
bA 1 a]s3 : [A ?
ab, A ?
ba]s4 : [B ?
cB1 d, B ?
dB 1 c]s5 : [B ?
cd, B ?
dc] 2In each step of the derivation process of a SCFGG, two nonterminals with the same index in a pair ofsynchronous strings are rewritten by a synchronousrule.
This is done in such a way that the result is oncemore a pair of synchronous strings.
An auxiliarynotion is that of reindexing, which is an injectivefunction f fromN toN.
We extend f to VI by lettingf(A t ) = A f(t) for A t ?
I(N) and f(a) = afor a ?
?.
We also extend f to strings in V ?I by461letting f(?)
= ?
and f(X?)
= f(X)f(?
), for eachX ?
VI and ?
?
V ?I .Let ?1, ?2 be synchronous strings in V ?I .
The de-rive relation [?1, ?2] ?G [?1, ?2] holds wheneverthere exist an index t in index(?1) = index(?2), asynchronous rule s : [A1 ?
?1, A2 ?
?2] in Pand some reindexing f such that:(i) index(f(?1)) ?
(index(?1) \ {t}) = ?
;(ii) ?1 = ?
?1At1 ??
?1 , ?2 = ?
?2At2 ??
?2 ; and(iii) ?1 = ??1f(?1)??
?1 , ?2 = ??2f(?2)??
?2 .We also write [?1, ?2] ?sG [?1, ?2] to explicitlyindicate that the derive relation holds through rule s.Note that ?1, ?2 above are guaranteed to be syn-chronous strings, because ?1 and ?2 are syn-chronous strings and because of (i) above.
Notealso that, for a given pair [?1, ?2] of synchronousstrings, an index t and a rule s, there may be in-finitely many choices of reindexing f such that theabove constraints are satisfied.
In this paper we willnot further specify the choice of f .We say the pair [A1, A2] of nonterminals is linked(in G) if there is a rule of the form s : [A1 ?
?1, A2 ?
?2].
The set of linked nonterminal pairsis denoted by N [2].A derivation is a sequence ?
= s1s2 ?
?
?
sd of syn-chronous rules si ?
P with d ?
0 (?
= ?
ford = 0) such that [?1i?1, ?2i?1] ?siG [?1i, ?2i] forevery i with 1 ?
i ?
d and synchronous strings[?1i, ?2i] with 0 ?
i ?
d. Throughout this paper,we always implicitly assume some canonical formfor derivations in G, by demanding for instance thateach step rewrites a pair of nonterminal occurrencesof which the first is leftmost in the left component.When we want to focus on the specific synchronousstrings being derived, we also write derivations inthe form [?10, ?20] ?
?G [?1d, ?2d], and we write[?10, ?20] ?
?G [?1d, ?2d] when ?
is not furtherspecified.
The translation generated by a SCFG Gis defined as:T (G) = {[w1, w2] | [S1 , S 1 ] ?
?G [w1, w2],w1, w2 ?
??
}For w1, w2 ?
?
?, we write D(G, [w1, w2]) to de-note the set of all (canonical) derivations ?
such that[S 1 , S 1 ] ?
?G [w1, w2].Analogously to standard terminology for context-free grammars, we call a SCFG reduced if ev-ery rule occurs in at least one derivation ?
?D(G, [w1, w2]), for some w1, w2 ?
??.
We as-sume without loss of generality that the start sym-bol S does not occur in the right-hand side of eithercomponent of any rule.Example 2 Consider the SCFG G from example 1.The following is a canonical derivation in G, since itis always the leftmost nonterminal occurrence in theleft component that is involved in a derivation step:[S 1 , S 1 ] ?G [A1 B 2 , B 2 A 1 ]?G [aA3 bB 2 , B 2 bA 3 a]?G [aaA4 bbB 2 , B 2 bbA 4 aa]?G [aaabbbB2 , B 2 bbbaaa]?G [aaabbbcB5 d, dB 5 cbbbaaa]?G [aaabbbccdd, ddccbbbaaa]It is not difficult to see that the generated translationis T (G) = {[apbpcqdq, dqcqbpap] | p, q ?
1}.
2The size of a synchronous rule s : [A1 ?
?1,A2 ?
?2], is defined as |s| = |A1?1A2?2|.
Thesize of G is defined as |G| =?s?P |s|.A probabilistic SCFG (PSCFG) is a pair G =(G, pG) where G = (N,?,P, S) is a SCFG and pGis a function from P to real numbers in [0, 1].
Wesay that G is proper if for each pair [A1, A2] ?
N [2]we have:?s:[A1?
?1, A2?
?2]pG(s) = 1Intuitively, properness ensures that where a pairof nonterminals in two synchronous strings can berewritten, there is a probability distribution over theapplicable rules.For a (canonical) derivation ?
= s1s2 ?
?
?
sd, wedefine pG(?)
=?di=1 pG(si).
For w1, w2 ?
?
?,we also define:pG([w1, w2]) =???D(G,[w1,w2])pG(?)
(1)We say a PSCFG is consistent if pG defines a prob-ability distribution over the translation, or formally:?w1,w2pG([w1, w2]) = 1462If the grammar is reduced, proper and consistent,then also:?w1,w2??
?, ?
?P ?s.t.
[A 11 , A12 ]?
?G[w1, w2]pG(?)
= 1for every pair [A1, A2] ?
N [2].
The proof is identi-cal to that of the corresponding fact for probabilisticcontext-free grammars.3 Effective PSCFG parsingIf w = a1 ?
?
?
an then the expression w[i, j], with0 ?
i ?
j ?
n, denotes the substring ai+1 ?
?
?
aj (ifi = j then w[i, j] = ?).
In this section, we assumethe input is the pair [w1, w2] of terminal strings.The task of a recognizer for SCFG G is to decidewhether [w1, w2] ?
T (G).We present a general algorithm for solving theabove problem in terms of the specification of a de-duction system, following Shieber et al (1995).
Theitems that are constructed by the system have theform [m1, A1,m?1; m2, A2,m?2], where [A1, A2] ?N [2] and where m1, m?1, m2, m?2 are non-negativeintegers such that 0 ?
m1 ?
m?1 ?
|w1| and0 ?
m2 ?
m?2 ?
|w2|.
Such an item can be de-rived by the deduction system if and only if:[A 11 , A12 ] ?
?G [w1[m1,m?1], w2[m2,m?2]]The deduction system has one inference rule,shown in figure 1.
One of its side conditions hasa synchronous rule in P of the form:s : [A1 ?
u10At111 u11 ?
?
?u1r?1Atr1r u1r,A2 ?
u20Atpi(1)21 u21 ?
?
?u2r?1Atpi(r)2r u2r] (2)Observe that, in the right-hand side of the two rulecomponents above, nonterminals A1i and A2pi?1(i),1 ?
i ?
r, have both the same index.
More pre-cisely, A1i has index ti and A2pi?1(i) has index ti?with i?
= pi(pi?1(i)) = i.
Thus the nonterminals ineach antecedent item in figure 1 form a linked pair.We now turn to a computational analysis of theabove algorithm.
In the inference rule in figure 1there are 2(r + 1) variables that can be bound topositions in w1, and as many that can be bound topositions in w2.
However, the side conditions implym?ij = mij + |uij |, for i ?
{1, 2} and 0 ?
j ?
r,and therefore the number of free variables is onlyr + 1 for each component.
By standard complex-ity analysis of deduction systems, for example fol-lowing McAllester (2002), the time complexity ofa straightforward implementation of the recogni-tion algorithm is O(|P | ?
|w1|rmax+1 ?
|w2|rmax+1),where rmax is the maximum number of right-handside nonterminals in either component of a syn-chronous rule.
The algorithm therefore runs in ex-ponential time, when the grammar G is consideredas part of the input.
Such computational behaviorseems unavoidable, since the recognition problemfor SCFG is NP-complete, as reported by Satta andPeserico (2005).
See also Gildea and Stefankovic(2007) and Hopkins and Langmead (2010) for fur-ther analysis of the upper bound above.The recognition algorithm above can easily beturned into a parsing algorithm by letting an imple-mentation keep track of which items were derivedfrom which other items, as instantiations of the con-sequent and the antecedents, respectively, of the in-ference rule in figure 1.A probabilistic parsing algorithm that computespG([w1, w2]), defined in (1), can also be obtainedfrom the recognition algorithm above, by associat-ing each item with a probability.
To explain the ba-sic idea, let us first assume that each item can beinferred in finitely many ways by the inference rulein figure 1.
Each instantiation of the inference ruleshould be associated with a term that is computedby multiplying the probability of the involved rules and the product of all probabilities previously as-sociated with the instantiations of the antecedents.The probability associated with an item is thencomputed as the sum of each term resulting fromsome instantiation of an inference rule deriving thatitem.
This is a generalization to PSCFG of the in-side algorithm defined for probabilistic context-freegrammars (Manning and Schu?tze, 1999), and wecan show that the probability associated with item[0, S, |w1| ; 0, S, |w2|] provides the desired valuepG([w1, w2]).
We refer to the procedure sketchedabove as the inside algorithm for PSCFGs.However, this simple procedure fails if there arecyclic dependencies, whereby the derivation of anitem involves a proper subderivation of the sameitem.
Cyclic dependencies can be excluded if it can463[m?10, A11,m11; m?2pi?1(1)?1, A2pi?1(1),m2pi?1(1)]...[m?1r?1, A1r,m1r; m?2pi?1(r)?1, A2pi?1(r),m2pi?1(r)][m10, A1,m?1r; m20, A2,m?2r]??????????????
?s:[A1 ?
u10At111 u11 ?
?
?u1r?1Atr1r u1r,A2 ?
u20Atpi(1)21 u21 ?
?
?u2r?1Atpi(r)2r u2r] ?
P,w1[m10,m?10] = u10,...w1[m1r,m?1r] = u1r,w2[m20,m?20] = u20,...w2[m2r,m?2r] = u2rFigure 1: SCFG recognition, by a deduction system consisting of a single inference rule.be guaranteed that, in figure 1, m?1r ?m10 is greaterthan m1j ?
m?1j?1 for each j (1 ?
j ?
r), orm?2r ?
m20 is greater than m2j ?
m?2j?1 for eachj (1 ?
j ?
r).Consider again a synchronous rule s of the formin (2).
We say s is an epsilon rule if r = 0 andu10 = u20 = .
We say s is a unit rule if r = 1and u10 = u11 = u20 = u21 = .
Similarly tocontext-free grammars, absence of epsilon rules andunit rules guarantees that there are no cyclic depen-dencies between items and in this case the inside al-gorithm correctly computes pG([w1, w2]).Epsilon rules can be eliminated from PSCFGsby a grammar transformation that is very similarto the transformation eliminating epsilon rules froma probabilistic context-free grammar (Abney et al,1999).
This is sketched in what follows.
We firstcompute the set of all nullable linked pairs of non-terminals of the underlying SCFG, that is, the set ofall [A1, A2] ?
N [2] such that [A11 , A12 ] ?
?G [?, ?
].This can be done in linear time O(|G|) using essen-tially the same algorithm that identifies nullable non-terminals in a context-free grammar, as presented forinstance by Sippu and Soisalon-Soininen (1988).Next, we identify all occurrences of nullable pairs[A1, A2] in the right-hand side components of a rules, such that A1 and A2 have the same index.
Forevery possible choice of a subset U of these occur-rences, we add to our grammar a new rule sU con-structed by omitting all of the nullable occurrencesin U .
The probability of sU is computed as the prob-ability of s multiplied by terms of the form:??
s.t.
[A 11 ,A12 ]?
?G[?, ?]pG(?)
(3)for every pair [A1, A2] in U .
After adding these extrarules, which in effect circumvents the use of epsilon-generating subderivations, we can safely remove allepsilon rules, with the only exception of a possiblerule of the form [S ?
, S ?
].
The translation andthe associated probability distribution in the result-ing grammar will be the same as those in the sourcegrammar.One problem with the above construction is thatwe have to create new synchronous rules sU for eachpossible choice of subset U .
In the worst case, thismay result in an exponential blow-up of the sourcegrammar.
In the case of context-free grammars, thisis usually circumvented by casting the rules in bi-nary form prior to epsilon rule elimination.
How-ever, this is not possible in our case, since SCFGsdo not allow normal forms with a constant boundon the length of the right-hand side of each compo-nent.
This follows from a result due to Aho and Ull-man (1969) for a formalism called syntax directedtranslation schemata, which is a syntactic variant ofSCFGs.An additional complication with our constructionis that finding any of the values in (3) may involvesolving a system of non-linear equations, similarlyto the case of probabilistic context-free grammars;see again Abney et al (1999), and Stolcke (1995).Approximate solution of such systems might takeexponential time, as pointed out by Kiefer et al(2007).Notwithstanding the worst cases mentionedabove, there is a special case that can be easily dealtwith.
Assume that, for each nullable pair [A1, A2] inG we have that [A 11 , A12 ] ?
?G [w1, w2] does nothold for any w1 and w2 with w1 6= ?
or w2 6= ?.Then each of the values in (3) is guaranteed to be 1,and furthermore we can remove the instances of thenullable pairs in the source rule s all at the sametime.
This means that the overall construction of464elimination of nullable rules from G can be imple-mented in linear time |G|.
It is this special case thatwe will encounter in section 4.After elimination of epsilon rules, one can elimi-nate unit rules.
We define Cunit([A1, A2], [B1, B2])as the sum of the probabilities of all derivations de-riving [B1, B2] from [A1, A2] with arbitrary indices,or more precisely:??
?P ?
s.t.
?t?N,[A 11 , A12 ]?
?G[Bt1 , Bt2 ]pG(?
)Note that [A1, A2] may be equal to [B1, B2] and ?may be ?, in which case Cunit([A1, A2], [B1, B2]) isat least 1, but it may be larger if there are unit rules.Therefore Cunit([A1, A2], [B1, B2]) should not beseen as a probability.Consider a pair [A1, A2] ?
N [2] and let al unitrules with left-hand sides A1 and A2 be:s1 : [A1, A2] ?
[At111 , At121 ]...sm : [A1, A2] ?
[Atm1m , Atm2m ]The values ofCunit(?, ?)
are related by the following:Cunit([A1, A2], [B1, B2]) =?
([A1, A2] = [B1, B2]) +?ipG(si) ?
Cunit([A1i, A2i], [B1, B2])where ?
([A1, A2] = [B1, B2]) is defined to be 1 if[A1, A2] = [B1, B2] and 0 otherwise.
This forms asystem of linear equations in the unknown variablesCunit(?, ?).
Such a system can be solved in polyno-mial time in the number of variables, for exampleusing Gaussian elimination.The elimination of unit rules starts with addinga rule s?
: [A1 ?
?1, A2 ?
?2] for each non-unit rule s : [B1 ?
?1, B2 ?
?2] and pair[A1, A2] such that Cunit([A1, A2], [B1, B2]) > 0.We assign to the new rule s?
the probability pG(s) ?Cunit([A1, A2], [B1, B2]).
The unit rules can nowbe removed from the grammar.
Again, in the re-sulting grammar the translation and the associatedprobability distribution will be the same as those inthe source grammar.
The new grammar has sizeO(|G|2), where G is the input grammar.
The timecomplexity is dominated by the computation of thesolution of the linear system of equations.
This com-putation takes cubic time in the number of variables.The number of variables in this case is O(|G|2),which makes the running time O(|G|6).4 Prefix probabilitiesThe joint prefix probability pprefixG ([v1, v2]) of apair [v1, v2] of terminal strings is the sum of theprobabilities of all pairs of strings that have v1 andv2, respectively, as their prefixes.
Formally:pprefixG ([v1, v2]) =?w1,w2??
?pG([v1w1, v2w2])At first sight, it is not clear this quantity can be ef-fectively computed, as it involves a sum over in-finitely many choices of w1 and w2.
However, anal-ogously to the case of context-free prefix probabili-ties (Jelinek and Lafferty, 1991), we can isolate twoparts in the computation.
One part involves infinitesums, which are independent of the input strings v1and v2, and can be precomputed by solving a sys-tem of linear equations.
The second part does relyon v1 and v2, and involves the actual evaluation ofpprefixG ([v1, v2]).
This second part can be realizedeffectively, on the basis of the precomputed valuesfrom the first part.In order to keep the presentation simple, andto allow for simple proofs of correctness, wesolve the problem in a modular fashion.
First,we present a transformation from a PSCFGG = (G, pG), with G = (N,?,P, S), to aPSCFG Gprefix = (Gprefix, pGprefix), with Gprefix =(Nprefix, ?, Pprefix, S?).
The latter grammar derivesall possible pairs [v1, v2] such that [v1w1, v2w2] canbe derived from G, for some w1 and w2.
Moreover,pGprefix([v1, v2]) = pprefixG ([v1, v2]), as will be veri-fied later.Computing pGprefix([v1, v2]) directly using ageneric probabilistic parsing algorithm for PSCFGsis difficult, due to the presence of epsilon rules andunit rules.
The next step will be to transform Gprefixinto a third grammar G?prefix by eliminating epsilonrules and unit rules from the underlying SCFG,and preserving the probability distribution over pairsof strings.
Using G?prefix one can then effectively465apply generic probabilistic parsing algorithms forPSCFGs, such as the inside algorithm discussed insection 3, in order to compute the desired prefixprobabilities for the source PSCFG G.For each nonterminal A in the source SCFG G,the grammar Gprefix contains three nonterminals,namely A itself, A?
and A?.
The meaning of A re-mains unchanged, whereas A?
is intended to gen-erate a string that is a suffix of a known prefix v1 orv2.
Nonterminals A?
generate only the empty string,and are used to simulate the generation by G of in-fixes of the unknown suffix w1 or w2.
The two left-hand sides of a synchronous rule in Gprefix can con-tain different combinations of nonterminals of theforms A, A?, or A?.
The start symbol of Gprefix isS?.
The structure of the rules from the source gram-mar is largely retained, except that some terminalsymbols are omitted in order to obtain the intendedinterpretation of A?
and A?.In more detail, let us consider a synchronous rules : [A1 ?
?1, A2 ?
?2] from the source gram-mar, where for i ?
{1, 2} we have:?i = ui0Ati1i1 ui1 ?
?
?uir?1Atirir uirThe transformed grammar then contains a largenumber of rules, each of which is of the form s?
:[B1 ?
?1, B2 ?
?2], where Bi ?
?i is ofone of three forms, namely Ai ?
?i, A?i ?
?
?ior A?i ?
?
?i , where ?
?i and ?
?i are explained below.The choices for i = 1 and for i = 2 are independent,so that we can have 3 ?
3 = 9 kinds of synchronousrules, to be further subdivided in what follows.
Aunique label s?
is produced for each new rule, andthe probability of each new rule equals that of s.The right-hand side ?
?i is constructed by omittingall terminals and propagating downwards the ?
su-perscript, resulting in:?
?i = A?
ti1i1 ?
?
?A?
tirirIt is more difficult to define ?
?i .
In fact, there canbe a number of choices for ?
?i and, for each choice,the transformed grammar contains an instance of thesynchronous rule s?
: [B1 ?
?1, B2 ?
?2] as de-fined above.
The reason why different choices needto be considered is because the boundary betweenthe known prefix vi and the unknown suffix wi canoccur at different positions, either within a terminalstring uij or else further down in a subderivation in-volving Aij .
In the first case, we have for some j(0 ?
j ?
r):?
?i = ui0Ati1i1 ui1Ati2i2 ?
?
?uij?1Atijij u?ijA?
tij+1ij+1 A?
tij+2ij+2 ?
?
?A?
tirirwhere u?ij is a choice of a prefix of uij .
In words,the known prefix ends after u?ij and, thereafter, nomore terminals are generated.
We demand that u?ijmust not be the empty string, unless Ai = S andj = 0.
The reason for this restriction is that we wantto avoid an overlap with the second case.
In thissecond case, we have for some j (1 ?
j ?
r):?
?i = ui0Ati1i1 ui1Ati2i2 ?
?
?uij?1A?
tijij A?
tij+1ij+1 A?
tij+2ij+2 ?
?
?A?
tirirHere the known prefix of the input ends within a sub-derivation involving Aij , and further to the right nomore terminals are generated.Example 3 Consider the synchronous rule s :[A ?
aB 1 bc C 2 d,D ?
ef E 2 F 1 ].
The firstcomponent of a synchronous rule derived from thiscan be one of the following eight:A?
?
B?
1 C?
2A?
?
aB?
1 C?
2A?
?
aB?
1 C?
2A?
?
aB 1 b C?
2A?
?
aB 1 bc C?
2A?
?
aB 1 bc C?
2A?
?
aB 1 bc C 2 dA ?
aB 1 bc C 2 dThe second component can be one of the followingsix:D?
?
E?
2 F ?
1D?
?
eE?
2 F ?
1D?
?
ef E?
2 F ?
1D?
?
ef E?
2 F ?
1D?
?
ef E 2 F ?
1D ?
ef E 2 F 1466In total, the transformed grammar will contain 8 ?6 = 48 synchronous rules derived from s. 2For each synchronous rule s, the above gram-mar transformation produces O(|s|) left rule com-ponents and as many right rule components.
Thismeans the number of new synchronous rules isO(|s|2), and the size of each such rule is O(|s|).
Ifwe sum O(|s|3) for every rule s we obtain a timeand space complexity of O(|G|3).We now investigate formal properties of ourgrammar transformation, in order to relate it to pre-fix probabilities.
We define the relation ` between Pand Pprefix such that s ` s?
if and only if s?
was ob-tained from s by the transformation described above.This is extended in a natural way to derivations, suchthat s1 ?
?
?
sd ` s?1 ?
?
?
s?d?
if and only if d = d?
andsi ` s?i for each i (1 ?
i ?
d).The formal relation between G and Gprefix is re-vealed by the following two lemmas.Lemma 1 For each v1, v2, w1, w2 ?
??
and?
?
P ?
such that [S, S] ?
?G [v1w1, v2w2], thereis a unique ??
?
P ?prefix such that [S?, S?]
??
?Gprefix[v1, v2] and ?
` ??.
2Lemma 2 For each v1, v2 ?
??
and derivation??
?
P ?prefix such that [S?, S?]
??
?Gprefix[v1, v2],there is a unique ?
?
P ?
and unique w1, w2 ?
?
?such that [S, S] ?
?G [v1w1, v2w2] and ?
` ??.
2The only non-trivial issue in the proof of Lemma 1is the uniqueness of ??.
This follows from the obser-vation that the length of v1 in v1w1 uniquely deter-mines how occurrences of left components of rulesin P found in ?
are mapped to occurrences of leftcomponents of rules in Pprefix found in ??.
The sameapplies to the length of v2 in v2w2 and the right com-ponents.Lemma 2 is easy to prove as the structure of thetransformation ensures that the terminals that are inrules from P but not in the corresponding rules fromPprefix occur at the end of a string v1 (and v2) to formthe longer string v1w1 (and v2w2, respectively).The transformation also ensures that s ` s?
im-plies pG(s) = pGprefix(s?).
Therefore ?
` ??
impliespG(?)
= pGprefix(??).
By this and Lemmas 1 and 2we may conclude:Theorem 1 pGprefix([v1, v2]) = pprefixG ([v1, v2]).
2Because of the introduction of rules with left-handsides of the formA?
in both the left and right compo-nents of synchronous rules, it is not straightforwardto do effective probabilistic parsing with the gram-mar Gprefix.
We can however apply the transforma-tions from section 3 to eliminate epsilon rules andthereafter eliminate unit rules, in a way that leavesthe derived string pairs and their probabilities un-changed.The simplest case is when the source grammar Gis reduced, proper and consistent, and has no epsilonrules.
The only nullable pairs of nonterminals inGprefix will then be of the form [A?1, A?2].
Considersuch a pair [A?1, A?2].
Because of reduction, proper-ness and consistency of G we have:?w1,w2??
?, ?
?P ?
s.t.
[A 11 , A12 ]?
?G[w1, w2]pG(?)
= 1Because of the structure of the grammar transforma-tion by which Gprefix was obtained from G, we alsohave:??
?P ?
s.t.[A?
11 , A?
12 ]?
?Gprefix[?, ?]pGprefix(?)
= 1Therefore pairs of occurrences of A?1 and A?2 withthe same index in synchronous rules of Gprefixcan be systematically removed without affecting theprobability of the resulting rule, as outlined in sec-tion 3.
Thereafter, unit rules can be removed to allowparsing by the inside algorithm for PSCFGs.Following the computational analyses for all ofthe constructions presented in section 3, and for thegrammar transformation discussed in this section,we can conclude that the running time of the pro-posed algorithm for the computation of prefix prob-abilities is dominated by the running time of the in-side algorithm, which in the worst case is exponen-tial in |G|.
This result is not unexpected, as alreadypointed out in the introduction, since the recogni-tion problem for PSCFGs is NP-complete, as estab-lished by Satta and Peserico (2005), and there is astraightforward reduction from the recognition prob-lem for PSCFGs to the problem of computing theprefix probabilities for PSCFGs.467One should add that, in real world machine trans-lation applications, it has been observed that recog-nition (and computation of inside probabilities) forSCFGs can typically be carried out in low-degreepolynomial time, and the worst cases mentionedabove are not observed with real data.
Further dis-cussion on this issue is due to Zhang et al (2006).5 DiscussionWe have shown that the computation of joint prefixprobabilities for PSCFGs can be reduced to the com-putation of inside probabilities for the same model.Our reduction relies on a novel grammar transfor-mation, followed by elimination of epsilon rules andunit rules.Next to the joint prefix probability, we can alsoconsider the right prefix probability, which is de-fined by:pr?prefixG ([v1, v2]) =?wpG([v1, v2w])In words, the entire left string is given, along with aprefix of the right string, and the task is to sum theprobabilities of all string pairs for different suffixesfollowing the given right prefix.
This can be com-puted as a special case of the joint prefix probability.Concretely, one can extend the input and the gram-mar by introducing an end-of-sentence marker $.Let G?
be the underlying SCFG grammar after theextension.
Then:pr?prefixG ([v1, v2]) = pprefixG?
([v1$, v2])Prefix probabilities and right prefix probabilitiesfor PSCFGs can be exploited to compute probabilitydistributions for the next word or part-of-speech inleft-to-right incremental translation of speech, or al-ternatively as a predictive tool in applications of in-teractive machine translation, of the kind describedby Foster et al (2002).
We provide some technicaldetails here, generalizing to PSCFGs the approachby Jelinek and Lafferty (1991).Let G = (G, pG) be a PSCFG, with ?
the alpha-bet of terminal symbols.
We are interested in theprobability that the next terminal in the target trans-lation is a ?
?, after having processed a prefix v1 ofthe source sentence and having produced a prefix v2of the target translation.
This can be computed as:pr?wordG (a | [v1, v2]) =pprefixG ([v1, v2a])pprefixG ([v1, v2])Two considerations are relevant when applyingthe above formula in practice.
First, the computa-tion of pprefixG ([v1, v2a]) need not be computed fromscratch if pprefixG ([v1, v2]) has been computed al-ready.
Because of the tabular nature of the inside al-gorithm, one can extend the table for pprefixG ([v1, v2])by adding new entries to obtain the table forpprefixG ([v1, v2a]).
The same holds for the compu-tation of pprefixG ([v1b, v2]).Secondly, the computation of pprefixG ([v1, v2a]) forall possible a ?
?
may be impractical.
However,one may also compute the probability that the nextpart-of-speech in the target translation isA.
This canbe realised by adding a rule s?
: [B ?
b, A ?
cA]for each rule s : [B ?
b, A ?
a] from the sourcegrammar, where A is a nonterminal representing apart-of-speech and cA is a (pre-)terminal specific toA.
The probability of s?
is the same as that of s. IfG?
is the underlying SCFG after adding such rules,then the required value is pprefixG?
([v1, v2 cA]).One variant of the definitions presented in this pa-per is the notion of infix probability, which is use-ful in island-driven speech translation.
Here we areinterested in the probability that any string in thesource language with infix v1 is translated into anystring in the target language with infix v2.
However,just as infix probabilities are difficult to computefor probabilistic context-free grammars (Corazza etal., 1991; Nederhof and Satta, 2008) so (joint) infixprobabilities are difficult to compute for PSCFGs.The problem lies in the possibility that a given in-fix may occur more than once in a string in the lan-guage.
The computation of infix probabilities canbe reduced to that of solving non-linear systems ofequations, which can be approximated using for in-stance Newton?s algorithm.
However, such a systemof equations is built from the input strings, which en-tails that the computational effort of solving the sys-tem primarily affects parse time rather than parser-generation time.468ReferencesS.
Abney, D. McAllester, and F. Pereira.
1999.
Relatingprobabilistic grammars and automata.
In 37th AnnualMeeting of the Association for Computational Linguis-tics, Proceedings of the Conference, pages 542?549,Maryland, USA, June.A.V.
Aho and J.D.
Ullman.
1969.
Syntax directed trans-lations and the pushdown assembler.
Journal of Com-puter and System Sciences, 3:37?56.Z.
Chi.
1999.
Statistical properties of probabilisticcontext-free grammars.
Computational Linguistics,25(1):131?160.D.
Chiang.
2007.
Hierarchical phrase-based translation.Computational Linguistics, 33(2):201?228.A.
Corazza, R. De Mori, R. Gretter, and G. Satta.1991.
Computation of probabilities for an island-driven parser.
IEEE Transactions on Pattern Analysisand Machine Intelligence, 13(9):936?950.G.
Foster, P. Langlais, and G. Lapalme.
2002.
User-friendly text prediction for translators.
In Confer-ence on Empirical Methods in Natural Language Pro-cessing, pages 148?155, University of Pennsylvania,Philadelphia, PA, USA, July.M.
Galley, M. Hopkins, K. Knight, and D. Marcu.
2004.What?s in a translation rule?
In HLT-NAACL 2004,Proceedings of the Main Conference, Boston, Mas-sachusetts, USA, May.D.
Gildea and D. Stefankovic.
2007.
Worst-case syn-chronous grammar rules.
In Human Language Tech-nologies 2007: The Conference of the North AmericanChapter of the Association for Computational Linguis-tics, Proceedings of the Main Conference, pages 147?154, Rochester, New York, USA, April.M.
Hopkins and G. Langmead.
2010.
SCFG decod-ing without binarization.
In Conference on EmpiricalMethods in Natural Language Processing, Proceed-ings of the Conference, pages 646?655, October.F.
Jelinek and J.D.
Lafferty.
1991.
Computation of theprobability of initial substring generation by stochas-tic context-free grammars.
Computational Linguistics,17(3):315?323.S.
Kiefer, M. Luttenberger, and J. Esparza.
2007.
On theconvergence of Newton?s method for monotone sys-tems of polynomial equations.
In Proceedings of the39th ACM Symposium on Theory of Computing, pages217?266.C.D.
Manning and H. Schu?tze.
1999.
Foundations ofStatistical Natural Language Processing.
MIT Press.D.
McAllester.
2002.
On the complexity analysis ofstatic analyses.
Journal of the ACM, 49(4):512?537.M.-J.
Nederhof and G. Satta.
2008.
Computing parti-tion functions of PCFGs.
Research on Language andComputation, 6(2):139?162.G.
Satta and E. Peserico.
2005.
Some computationalcomplexity results for synchronous context-free gram-mars.
In Human Language Technology Conferenceand Conference on Empirical Methods in Natural Lan-guage Processing, pages 803?810.S.M.
Shieber, Y. Schabes, and F.C.N.
Pereira.
1995.Principles and implementation of deductive parsing.Journal of Logic Programming, 24:3?36.S.
Sippu and E. Soisalon-Soininen.
1988.
ParsingTheory, Vol.
I: Languages and Parsing, volume 15of EATCS Monographs on Theoretical Computer Sci-ence.
Springer-Verlag.A.
Stolcke.
1995.
An efficient probabilistic context-freeparsing algorithm that computes prefix probabilities.Computational Linguistics, 21(2):167?201.D.
Wu.
1997.
Stochastic inversion transduction gram-mars and bilingual parsing of parallel corpora.
Com-putational Linguistics, 23(3):377?404.Hao Zhang, Liang Huang, Daniel Gildea, and KevinKnight.
2006.
Synchronous binarization for machinetranslation.
In Proceedings of the Human LanguageTechnology Conference of the NAACL, Main Confer-ence, pages 256?263, New York, USA, June.469
