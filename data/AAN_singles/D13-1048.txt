Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 501?512,Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational LinguisticsAnchor Graph:Global Reordering Contexts for Statistical Machine TranslationHendra Setiawan ?IBM Research1101 Kitchawan RoadNY 10598, USABowen ZhouIBM Research1101 Kitchawan RoadNY 10598, USABing Xiang ?Thomson Reuters3 Times SquareNY 10036, USAAbstractReordering poses one of the greatest chal-lenges in Statistical Machine Translation re-search as the key contextual information maywell be beyond the confine of translation units.We present the ?Anchor Graph?
(AG) modelwhere we use a graph structure to modelglobal contextual information that is crucialfor reordering.
The key ingredient of our AGmodel is the edges that capture the relation-ship between the reordering around a set ofselected translation units, which we refer to asanchors.
As the edges link anchors that mayspan multiple translation units at decodingtime, our AG model effectively encodes globalcontextual information that is previously ab-sent.
We integrate our proposed model into astate-of-the-art translation system and demon-strate the efficacy of our proposal in a large-scale Chinese-to-English translation task.1 IntroductionReordering remains one of the greatest challengesin Statistical Machine Translation (SMT) research asthe key contextual information may span across mul-tiple translation units.1 Unfortunately, previous ap-proaches fall short in capturing such cross-unit con-textual information that could be critical in reorder-ing.
For example, state-of-the-art translation mod-els, such as Hiero (Chiang, 2005) or Moses (Koehnet al 2007), are good at capturing local reorderingwithin the confine of a translation unit, but their for-mulation is approximately a simple unigram model?
This work was done when the authors were with IBM.1We define translation units as phrases in phrase-based SMTor as translation rules in syntax-based SMT.over derivation (a sequence of the application oftranslation units) with some aid from target languagemodels.
Moving to a higher order formulation (sayto a bigram model) is highly impractical for severalreasons: 1) it has to deal with a severe sparsity issueas the size of the unigram model is already huge;and 2) it has to deal with a spurious ambiguity issuewhich allows multiple derivations of a sentence pairto have radically different model scores.In this paper, we develop ?Anchor Graph?
(AG)where we use a graph structure to capture globalcontexts that are crucial for translation.
To circum-vent the sparsity issue, we design our model to relyonly on contexts from a set of selected translationunits, particularly those that appear frequently withimportant reordering patterns.
We refer to the unitsin this special set as anchors where they act as ver-tices in the graph.
To address the spurious ambigu-ity issue, we insist on computing the model score forevery anchors in the derivation, including those thatappear inside larger translation units, as such our AGmodel gives the same score to the derivations thatshare the same reordering pattern.In AG model, the actual reordering is modeledby the edges, or more specifically, by the edges?
la-bels where different reordering around the anchorswould correspond to a different label.
As detailedlater, we consider two distinct set of labels, namelydominance and precedence, reflecting the two domi-nant views about reordering in literature, i.e.
the firstone that views reordering as a linear operation overa sequence and the second one that views reorderingas a recursive operation over nodes in a tree struc-ture The former is prevalent in phrase-based con-text, while the latter in hierarchical phrase-based and501syntax-based context.
More concretely, the domi-nance looks at the anchors?
relative positions in thetranslated sentence, while the precedence looks atthe anchors?
relative positions in a latent structure,induced via a novel synchronous grammar: Anchor-centric, Lexicalized Synchronous Grammar.From these two sets of labels, we develop twoprobabilistic models, namely the dominance and theorientation models.
As the edges of AG link pairsof anchors that may appear in multiple translationunits, our AG models are able to capture high or-der contextual information that is previously absent.Furthermore, the parameters of these models are es-timated in an unsupervised manner without linguis-tic supervision.
More importantly, our experimentalresults demonstrate the efficacy of our proposed AG-based models, which we integrate into a state-of-the-art syntax-based translation system, in a large scaleChinese-to-English translation task.
We would liketo emphasize that although we use a syntax-basedtranslation system in our experiments, in principle,our approach is applicable to other translation mod-els as it is agnostic to the translation units.2 Anchor Graph ModelFormally, an AG consists of {A,L} where A is aset of vertices that correspond to anchors, while Lis a set of labeled edges that link a pair of anchors.In principle, our AG model is part of a transla-tion model that focuses on the reordering within thesource sentence F and its translation E. Thus, westart by first introducing A into a translation model(either word-based, phrase-based or syntax-basedmodel) followed by L. Given an F , A is essentiallya subset of non-overlapping (word or phrase) unitsthat make up F .
As the information related to A isnot observed, we introduce A as a latent variable.Let P (E,?
|F ) be a translation model where ?corresponds to the alignments between units in Fand E. 2 We introduce A into a translation model,2Alignment (?)
represents an existing latent variable.
De-pending on the translation units, it can be defined at differentlevel, i.e.
word, phrase or hierarchical phrase.
As during trans-lation, we are interested in the anchors that appear inside largertranslation units, we set ?
at word level, which information canbe induced for (hierarchical) phrase units by either keeping theword alignment from the training data inside the units or infer-ring it via lexical translation probability.
We use the former.as follow:P (E,?
|F ) =?
?A?P (E,?,A?|F ) (1)P (E,?,A?|F ) = P (E,?
|A?, F )P (A?)
(2)As there can be many possible subsets of F andsumming over all possibleA is intractable, we makethe following approximation for P (A?)
such that weonly need to consider one particular A?
: P (A?)
=?(A?
= A?)
which returns 1 only for A?, otherwise0.
The exact definition of the heuristic will be de-scribed in Section 7, but in short, we equateA?
withunits that appear frequently with important reorder-ing patterns in training data.Given an A?, we then introduce the edges of AG(L) into the equation as follow:P (E,?
|A?, F ) = P (E,?,L|A?, F ) (3)Note that L is also a latent variable but its values arederived deterministically from (F,E,?)
and A?,thus no extra summation is present in Eq.
3.Then, we further simplify Eq.
3 by factorizing itwith respect to each individual edges, as follow:P (E,?,L|A?, F ) ??
?am,an?A?m<nP (Lm,n|am, an) (4)where Lm,n ?
L corresponds to the label of an edgethat links am and an.In principle, Lm,n can take any arbitrary value.For addressing the reordering challenge, it shouldideally correspond to some aspect of the reorderingaround am and an, for example, how the reorder-ing around am affects the reordering around an.
Asmentioned earlier, we choose to associate Lm,n withthe dominance and the precedence relations betweenam and an, where the former looks at the relative po-sitions of the two anchors when they are projectedinto a latent tree structure, while the latter looks attheir relative positions when they are projected intothe target sentence.
We illustrate the two in Fig.
1.Furthermore, we assume that dominance andprecedence are independent and develop one modelfor each, resulting in the dominance and the orien-tation models, which we describe in Section 3 and 4respectively.
To make the model more compact, we502introduce an additional parameterO that restricts themaximum order of AG as follows:?O?o=1|A?|+o?1?i=0Po(Li?o,i|ai?o, ai) (5)Thus, we only consider edges that link two anchorsthat are at most O?
1 anchors apart.
For O = 1, theAG model only considers relations between neigh-boring anchors.
Following the standard practice inthe n-gram language modeling, we append O num-ber of pseudo anchors at the beginning and at the endof F , which represent the sentence delimiter mark-ers.
We do so in a monotone order.Figure 1: The illustration of the dominance and the prece-dence relations.
The former looks at the anchors?
pro-jection on a derivation structure.
The latter looks at theanchors?
projection on the translated sentence.3 Dominance ModelThis section describes our dominance model wherewe equate Lm,n in Eq.
4 with dom(am, an) that ex-presses to the dominance relation between am andan in a latent tree structure.
Due to reordering, an-chors can only appear in specific nodes.
We firstdescribe a novel formalism of Anchor-centric, Lexi-calized Synchronous Grammar (AL-SG), used to in-duce the tree structure and then discuss the proba-bilistic formulation of the model.
Just to be clear,we introduce AL-SG mainly to facilitate the compu-tation of dom(am, an).
The actual translation modelat decoding time remains either phrase-based, hier-archical phrase-based or syntax-based model.3.1 Anchor-centric, Lexicalized SynchronousGrammarGiven (F,E,?)
and A, Anchor-centric, Lexical-ized Synchronous Grammar (AL-SG) produces atree structure where the nodes are decorated withanchors-related information.
As the name alludes,the core of AL-SG is anchor-centric constituents(ACC), which corresponds to nodes, composed frommerging anchors with by either their left, their rightneighboring constituents or both.More concretely, first of all, we consider a spanon the source sentence F to be a constituent if it isconsistent with the alignment (?).
Second of all, wecan construct a larger constituent by merging smallerconstituents given that the larger constituent is alsoconsistent with the alignment.
These two constraintsare similar to the heuristic applied to extract hierar-chical phrases (Chiang, 2005).Then, specific to AL-SG, we consider an anchor ato lexicalize a constituent c, if: a) we can compose cfrom at most three smaller constituents: cL, a and cRwhere a is the anchor while cL,cR are the (possiblyempty) constituents immediately to the left and tothe right of a; and b) we can create smaller anchors-centric constituents from concatenating a with cLand a with cR.
If a can lexicalize c, then the nodeassociated with c would be marked with a.
In com-puting dom(am, an), we look at the constituents thatcover both anchors and check whether the anchorscan lexicalized any of such constituents.Now, we will describe AL-SG in a formal way.For simplicity, we use a simple grammar, called In-version Transduction Grammar (ITG) (Wu, 1997),although in practice, we handle a more powerfulsynchronous grammar.
Hence, we proceed to de-scribe Anchor-centric, Lexicalized ITG (AL-ITG).An AL-ITG is a quadruple {?,A,V,R} where:?
?
= {(f/e)} is a set of terminal symbols,which represents all possible units defined over(F,E,?)
where each pair corresponds to a linkin ?.
We define ?
at the most fine-grainedlevel (i.e.
word-level), as we insist on comput-ing model score for each anchors even if theyappear inside larger units.?
A ?
?
is a set of anchors, which is a subset ofthe terminal symbols.?
V = {{P,X, Y } ?
{A, ?}}
is a set of (possi-bly lexicalized) nonterminal symbols.
P rep-resents the terminal symbols (?
); while X andY correspond to the spans that are created frommerging two adjacent constituents.
On the tar-503Figure 2: An illustration of an aligned Chinese-English sentence pair with one possible AL-ITG derivation obtainedby applying the grammar in a left-to-right fashion.
Circles represent alignment points.
Black circle represents theanchor; boxes represent the anchor?s neighbors.
In the derivation tree, the anchors are represented by their positionand in bold.
For succinctness, we omit the preterminal rules in the tree.get side, for X , the order of the two childrenfollows the source order, while for Y , the or-der follows the inverse.
Nonterminal symbolscan be lexicalized with zero or more than oneanchor.
We represent a lexicalized constituentas a nonterminal symbol followed by a bracketwhich contains the lexicalizing anchors, e.g.P (H) where H is the anchors lexicalizing P .?
R is a set of production rules which can be clas-sified into the following categories:?
Preterminal rules.
We propagate the sym-bol if it corresponds to an anchor.P (H = f/e)?
f/e, if f/e ?
A?P (H = ?)?
f/e, otherwise?
Monotone production rules, which reorderthe children in monotone order, denotedby square brackets (?[?,?]?
).X(H1 ?H2)?
[P (H1)P (H2)]X(H1 ?H2)?
[X(H1)P (H2)]X(H1 ?H2)?
[X(H1)X(H2)]X(H1)?
[X(H1)Y (H2)]X(H2)?
[Y (H1)P (H2)]X(H2)?
[Y (H1)X(H2)]X(?)?
[Y (H1)Y (H2)]?
Inverse production rules, which reorderthe children in the inverse order, denotedby angle brackets (???,???
).Y (H1 ?H2)?
?P (H1)P (H2)?Y (H1 ?H2)?
?Y (H1)P (H2)?Y (H1 ?H2)?
?Y (H1)Y (H2)?Y (H1)?
?Y (H1)X(H2)?Y (H2)?
?X(H1)P (H2)?Y (H2)?
?X(H1)Y (H2)?Y (?)?
?X(H1)X(H2)?Like ITG, AL-ITG only permits two kind of re-ordering operations, namely monotone and inverse.To accommodate the lexicalization, we first assigna unique nonterminal symbol for each, i.e.
X formonotone reordering and Y for inverse reordering.Then, we lexicalize Xs and Y s with anchors as longas they satisfy the constraint that the child shares thesame label as the parent.
This constraint guaranteesthat the constituents are valid ACCs.
It also enablesthe anchors to lexicalize long constituents, althoughthe terminal symbols are defined at word-level.Fig.
2 illustrates an example Chinese-to-Englishtranslation with a AL-ITG derivation when thegrammar is applied in a left-to-right fashion.
Admit-tedly, AL-ITG (or more generally AL-SG) is suscep-tible to spurious ambiguity as it produces multiplederivation trees for a given (F,E,?).
Fortunately,the value of dom(am, an) is identical for all deriva-tions, since the computation of dom(am, an) relies504only on whether am and an can lexicalize at leastone constituent that covers both anchors.
Hence,we only need to look at one derivation to computedom(am, an).
Generalizing AL-ITG to a more pow-erful formalism is trivial; we just need to forbid thepropagation for non-binarizeable production rules.3.2 Probabilistic ModelWe read-off the dominance relations dom(am, an)from D obtained from the application of AL-SG to(F,E,?).
As lexicalization is a bottom-up process,for reading-off dom(am, an), it is sufficient to lookat the lowest common ancestor (LCA) of both an-chors; if the anchors cannot lexicalize the LCA, theywon?t be able to lexicalize the constituents largerthan LCA.
To be more concrete, let?s consider theDin Fig.
2.
In that D, the LCA of am = yu3/with10and an = de7/that7 is Y5(7).
Then, we check theanchors that can lexicalize the LCA.
Let V (H) bethe LCA, then dom(am, an) ?
(LH) , if am ?
H ?
an 6?
H(RH) , if am 6?
H ?
an ?
H(BL) , if am ?
H ?
an ?
H(BD) , if am 6?
H ?
an 6?
HThe value refers to cases where am and an canlexicalize V (H) and it is useful to model spansthat share a simple, uniform reordering, i.e.
all-monotone or all-inverse, while the value refers tothe cases where am and an cannot lexicalize V (H)and it is useful to model spans that involve in a com-plex reordering.
Meanwhile, the and refer to caseswhere only one anchor can lexicalize V (H), i.e.
amand an respectively.
These values are useful formodeling cases where the surroundings of the twoanchors exhibit different kind of reordering pattern.With such definition, the edge labels L in Fig.
2are indicated in Table 1.
Note that in Table 1, wedon?t specify the relations involving pseudo anchors,although they are crucial.The final probabilistic formulation of the domi-nance model is as follows:?O?o=1|A|+o?1?i=0Pdomo(dom(ai?o, ai)|ai?o, ai) (6)As shown, we allocate a separate model Pdomo foreach separate order (o) where each Pdomo will con-HHHHHnm1 2 3 4 51 = (shi2/is2) - - - - -2 = (yu3/with10) LH - - - -3 = (you5/have8) LH BD - - -4 = (de7/that7) LH RH RH - -5 = (zhi10/of4) LH RH RH BL -Table 1: The dominance relations between pairs of an-chors according to the derivation in Fig.
2.tribute as one additional feature in the log-linearmodel of the translation model.
In allocating a sep-arate model for each o, we conjecture that differentpair of anchors contributes differently depending onhow far the two anchors are.4 Orientation ModelIn this section, we introduce the orientation model(ori) where we equate Lm,n with the precedence re-lations between a pair of anchors.
Instead of directlymodeling the precedence between the two anchors,we approximate it by modeling the precedence ofeach anchor with its neighboring constituents.
For-mally, we approximate P (Lm,n|am, an) asPoriR(ori(am,MR(am))|am)?PoriL(ori(an,ML(an))|an) (7)where MR(am) is the largest constituent to the rightof the first anchor am, ML(an) the largest con-stituent to the left of the second anchor an, and ori()a function that maps the anchor and the neighboringconstituent to a particular orientation.Plugging Eq.
7 into Eq.
5 results in the followingapproximation of P (?|A):C.|A|?1?i=0{PoriL(ori(ai,ML(ai))|ai)?PoriR(ori(ai,MR(ai))|ai)}O (8)where C is a constant term related to the pseudo an-chors and O is the maximum order of the AG.
Inpractice, we can safely ignore both C and O as theyare constant for a given AG.
As shown, the orienta-tion model is simplified into a model that looks at thereordering of the anchors?
neighboring constituents.The exact definition of ML and MR will bediscussed in Section 5.
Their orientation, i.e.505oriL(CL, a) and oriR(CR, a) respectively, may takeone of the following four values: (MA), (RA), (MG)and (RG).
The first clause (monotone, reverse) in-dicates whether the target order follows the sourceorder; the second (adjacent, gap) indicates whetherthe anchor and its neighboring constituent are adja-cent or separated by an intervening when projected.5 Parameter EstimationFor each (F,E,?
), the training starts with the iden-tification of the regions in the source sentences asanchors (A).
For our Chinese-English experiments,we use a simple heuristic that equates anchors (A?
)with constituents whose corresponding word classbelongs to function words-related classes, bearinga close resemblance to (Setiawan et al 2007).
Intotal, we consider 21 part-of-speech tags; some ofwhich are as follows: VC (copula), DEG, DEG,DER, DEV (de-related), PU (punctuation), AD (ad-jectives) and P (prepositions).5.1 Extracting Events from (F,E,?
)The parameter estimation first involves extractingtwo statistics from (F,E,?
), namely dom(am, an)for the dominance model as well as ori(a,ML(a))and ori(a,MR(a)) for the orientation model.
In-stead of developing a separate algorithm for each,we describe a unified way to extract these statisticsvia the largest neighboring constituents of the an-chors, i.e.
ML(a) and MR(a).
This approach en-ables the dominance model to share the same resid-ual state information as the orientation model.3Let am be an anchor and MR(am) be its largestneighboring constituent to the right.
Let an bean anchor to the left of am and ML(an) be an?slargest neighboring constituent to the left.
Ac-cording to AL-SG, we say that am dominates anif ori(am,MR(am)) ?
{MA,RA} and an ?MR(am).
By the same token, we say that an dom-inates am if ori(an,ML(an)) ?
{MA,RA} andam ?
ML(an).
The constraints on the orientationreflect the fact that in AL-SG, anchors can only bepropagated through monotone or inverse productionrules, which correspond to the MA and RA respec-tively.
The fact that we are looking at the largest3The analogy in an n-gram language model is the first n?1words of the hypothesis that have incomplete history.neighboring constituents guarantees that if the otheranchor is outside that constituent, then that other an-chor is never dominated.More formally, given an aligned sentence pair?
= (F,E,?
), let ?(?)
be all possible con-stituents that can be extracted from ?
:4{(f j2j1/ei2i1) :?
(j, i) ??
: ((j1?
j?
j2) ?
(ii?
i?
i2))?(?(j1?
j?
j2) ?
?(ii?
i?
i2))Then, let the anchors A be a subset of ?(?
).Given A ?
?(?
), let a = (f j2j1/ei2i1) ?
A be a par-ticular anchor.
And, let CL(a) ?
?(?)
be a?s leftneighbors and let CR(a) ?
?(?)
be a?s right neigh-bors, iff:?CL = (fj4j3/ei4i3) ?
CL(a) : j4 + 1 = j1?CR = (fj6j5/ei6i5) ?
CR(a) : j2 + 1 = j5Then, let ML(a) ?
CL(a) and MR(a) ?
CR(a) bethe largest left and right neighbors according to:ML(a) = arg max(fj4j3/ei4i3)?CL(a)(j4 ?
j3)MR(a) = arg max(fj6j5/ei6i5)?CR(a)(j6 ?
j5)Let ML = (fj4j3/ei4i3) and MR = (fj6j5/ei6i5).We then proceed to extract oriL(a,ML(a)) andoriR(a,MR(a)) respectively as follows:?
MA, if (i4 +1) = i1 for oriL or if (i2 +1) = i5for oriR?
RA, if (i2 + 1) = i3 for oriL or if (i6 + 1) = i1for oriR?
MG, if (i4 +1) < i1 for oriL or if (i2 +1) < i5for oriR?
RG, if (i2 + 1) < i3 for oriL or if (i6 + 1) < i1for oriR.Then, we proceed to extract dom(am, an).
Giventwo anchors am, an where m < n, we define the4We represent a constituent as a source and target phrasepair (f j2j1/ei2i1) where the subscript and the superscript indicatethe starting and the ending indices as such f j2j1 denotes a sourcephrase that spans from j1 to j2.506dominance relation between am and an viaMR(am)and ML(an).
Let am = (fj2j1/ei2i1), MR(am) =(f j4j3/ei4i3), an = (fj6j5/ei6i5) and ML(an) = (fj8j7/ei8i7).Then, ldom(am, an) is true only if (j4 ?
j6)and oriR(am,MR(am)) ?
{MA,RA}.
Simi-larly, rdom(am, an) is true only if (j7 ?
j1) andoriL(an,ML(an)) ?
{MA,RA}.Hence, dom(am, an) is as follows:?
LH, if ldom(am, an) ?
?rdom(am, an)?
RH, if ?ldom(am, an) ?
rdom(am, an)?
BL, if ldom(am, an) ?
rdom(am, an)?
BD, if ?ldom(am, an) ?
?rdom(am, an)5.2 Parameterization and TrainingAfter extracting events, we are now ready to trainthe models.
To estimate them, we train a discrimi-native classifier for each model and use the normal-ized posteriors at decoding time as additional featurescores in SMT?s log-linear framework.At a high level, we use a rich set of binary fea-tures ranging from lexical to part-of-speech (POS)and to syntactic features.
Additionally, we augmentthe feature set with compound features, e.g.
a con-junction of the source word of the left anchor and thesource word of the right anchor.
Although they in-crease the number of features significantly, we foundthat they are empirically beneficial.Suppose a = (f j2j1 /ei2i1), ML(a) = (fj4j3 /ei4i3) andMR(a) = (fj6j5 /ei6i5), then based on the context?slocation, the elementary features employed in ourclassifiers can be categorized into:?
anchor-related: (the actual word of f j2j1 ),(part-of-speech (POS) tag of ), (?s parent in theparse tree), (ei2i1?s actual target word).?
surrounding: (the previous word / f j1?1j1?1 ), (thenext word / f j2+1j2+1 ), (?s POS tag), (?s POS tag),(?s parent), (?s parent).?
non-local: (the previous anchor?s source word), (the next anchor?s source word), (?s POS tag),(?s POS tag).There is a separate set of elementary features for amand an and we come up with manual combination toconstruct compound features.In training the models, we manually come up witharound 30-50 types of features, which consists of acombination of elementary and compound features.Due to space constraints, we will describe the ac-tual features that we use and the classification per-formance of our models elsewhere.
In total, wegenerate around one hundred millions binary fea-tures from our training data that contains six millionsentence pairs.
To reduce the number of features,we employ the L1-regularization in training to en-force sparse solutions, using the off-the-shelf LIB-LINEAR toolkit (Fan et al 2008).
After training,the number of features in our classifiers decreases tobelow 1 million features for each classifier.6 DecodingAs mentioned earlier, we wish to avoid the spuri-ous ambiguity issue where different derivations haveradically different scores although they lead to thesame reordering.
This section describes our decod-ing algorithm that avoids spurious ambiguity issueby incrementally constructing MLs and MRs thusallowing the computation of the models over partialhypotheses.In our experiments, we integrate our dominancemodel as well as our orientation model into a syntax-based SMT system that uses SCFG formalism.
In-tegrating the models into syntax-based SMT sys-tems is non-trivial, especially since the anchors of-ten reside within translation rules and the modeldoesn?t always decompose naturally with the hy-pothesis structure.
To facilitate that, we need tofirst induce the necessary alignment for all transla-tion units in the hypothesis.To describe the algorithm, let us consider a cheat-ing exercise where we have to translate the Chinesesentence in Fig.
2 with the following set of hierar-chical phrases:Xa?
?Aozhou1shi2X1,Australia1 is2X1?Xb?
?yu3 Beihan4X1, X1with3 North4 Korea?Xc?
?you5bangjiao6, have5dipl.6 rels.?Xd?
?X1 de7shaoshu8 guojia9 zhi10 yi11,one11of10the few8 countries9 that7X1?As a case in point, let us consider D = Xa ?
Xb?
Xd ?
Xc, which will lead to the correct English507Target string (w/ source index) Symbol(s) read Op.
Stack(s)(1) Xc have5 dipl.6 rels.
[5][6] S,S,R Xc:[5-6](2) Xd one11 of10 few8 countries9 [11][10] S,S,R [10-11]that7 Xc(3) [8][9] S,S,R,R [8-11](4) [7] S [8-11][7](5) Xc:[5,6] S Xd:[8-11][7][5,6](6) Xb Xd with3 North4 Korea Xd:[8-11][7][5,6] S [8-11][7][5,6](7) [3][4] S,S,R,R Xb:[8-11][7][3-6](8) Xa Australia1 is2 Xb [1][2] S,S,R [1-2](9) Xb:[8-11][7][3,6] S,A Xa:[1-2][8-11][7][3,6]Table 2: The application of the shift-reduce parsing algorithm, which corresponds to the following derivation D =Xa ?
Xb ?
Xd ?
Xc.
Anchor is in bold.
In column Op., S, R and A refer to shift, reduce and accept operationrespectively.translation as in Fig.
2.
Note that the translationrules contain internal word alignment, which we as-sume to have been previously inferred.The algorithm bears a close resemblance to theshift-reduce algorithm found in phrase-based decod-ing (Galley and Manning, 2008; Feng et al 2010;Cherry et al 2012).
A stack is used to accumulate(partial) information about a, ML and MR for eacha ?
A in the derivation.
This algorithm takes an in-put stream and applies either the shift or the reduceoperations starting from the beginning until the endof the stream.
The shift operation advances the inputstream by one symbol and push the symbol into thestack; while the reduce operation applies some ruleto the top-most elements of the stack.
The algorithmterminates at the end of the input stream where theresulting stack will be propagated to the parent forthe later stage of decoding.
In our case, the inputstream is the target string of the rule and the symbolis the corresponding source index of the elements ofthe target string.
The reduction rule looks at two in-dices and merge them if they are adjacent (i.e.
hasno intervening phrase).
We forbid the applicationof the reduction rule to anchors.
Table 2 shows theexecution trace of the algorithm for the derivationdescribed earlier.
For conciseness, we assume thatthere is only one anchor and that is de7/that7.As shown, the algorithm starts with an emptystack.
It then projects the source index to the corre-sponding target word and then enumerates the targetstring in a left to right fashion.
If it finds a targetword with a source index, it applies the shift oper-ation, pushing the index to the stack.
Unless thesymbol corresponds to an anchor, it tries to applythe reduce operation.
Line (4) indicates the specialtreatment to the anchor.
If the symbol being readis a nonterminal, then we push the entire stack thatcorresponds to that nonterminal.
For example, whenthe algorithm reads Xd at line (6), it pushes the en-tire stack from line (5).As MLs and MRs are being incremen-tally constructed, we can immediately com-pute Pdomo(dom(am, an)|am, an) as soonas a partial derivation covers both amand an.
For example, we can computePdom1(dom(you5/have8, de7/that7) = ),Pdom1(dom(de7/that7, zhi10/of4) = ) andPdom2(dom(you5/have8, zhi10/of4) = ) atpartial hypothesis Xd ?
Xc which corresponds to aconstituent spanning from 5-11.7 ExperimentsOur baseline systems is a state-of-the-art string-to-dependency system (Shen et al 2008).
The sys-tem is trained on 10 million parallel sentences thatare available to the Phase 1 of the DARPA BOLTChinese-English MT task.
The training corpora in-clude a mixed genre of newswire, weblog, broad-cast news, broadcast conversation, discussion fo-rums and comes from various sources such as LDC,HK Law, HK Hansard and UN data.In total, our baseline model employs more than50 features, including from our proposed dominanceand orientation models.
In addition to the standard508Modelnewswire weblog newswire+weblogBLEU TER Comb BLEU TER Comb BLEU TER Comb(a) (b) (c) (d) (e) (f) (g) (h) (i)(1) S2D 37.63 53.17 7.77 27.60 57.19 14.77 33.39 54.97 10.79(2) +dom1 38.12 52.31 7.10 27.56 56.58 14.51 33.64 54.24 10.30(3) +dom2 38.31 52.28 6.99 27.66 56.57 14.45 33.78 54.20 10.21(4) +dom3 38.31 52.52 7.10 28.24 56.56 14.16 34.02 54.33 10.15(5) +dom4 38.54 52.22 6.84 28.38 56.55 14.08 34.20 54.16 9.98(6) +dom5 38.17 52.57 7.20 28.67 56.27 13.80 34.16 54.27 10.05(7) +dom6 38.17 52.52 7.18 28.64 56.22 13.79 34.10 54.18 10.04(8) +ori 38.52 52.43 6.96 28.26 56.54 14.14 34.15 54.27 10.06(9) +ori+dom1 38.87 52.05 6.59 28.01 56.48 14.23 34.26 54.03 9.89(10) +ori+dom2 38.96 51.87 6.45 27.98 56.23 14.12 34.29 53.82 9.77(11) +ori+dom3 39.19 51.77 6.29 28.19 56.15 13.98 34.52 53.73 9.61(12) +ori+dom4 39.34 51.77 6.21 28.41 56.17 13.88 34.60 53.69 9.54(13) +ori+dom5 39.31 51.67 6.18 28.62 56.09 13.74 34.76 53.65 9.45Table 3: The NIST MT08 results on newswire (nw), weblog (wb) and combined genres.
S2D is the baseline string-to-dependency system (line 1).
Lines 2-7 shows the results of the dominance model with O = 1 ?
6.
Line 8 showsresult on adding ori to the baseline.
Lines 9-13 shows the results of the orientation complemented with the dominancemodel with varying O.
The best BLEU, TER and Comb on each genre of the first set are in italic while those of thesecond set are in bold.
For BLEU, higher scores are better, while for TER and Comb, lower scores are better.features such as translation probabilities, we incor-porate features that are found useful for developinga state-of-the-art baseline, such as the provenancefeatures (Chiang et al 2011).
We use a 6-gramlanguage model, which was trained on 10 billionEnglish words from multiple corpora, including theEnglish side of our parallel corpus plus other cor-pora such as Gigaword (LDC2011T07) and GoogleNews.
We also train a class-based language model(Chen, 2009) on two million English sentences se-lected from the parallel corpus.
As for our string-to-dependency system, we train 3-gram models for leftand right dependencies and unigram for head usingthe target side of the parallel corpus.
To train ourmodels, we select a set of 5 million sentence pairs.For the tuning and development sets, we setaside 1275 and 1239 sentences selected fromLDC2010E30 corpus.
We tune the feature weightswith PRO (Hopkins and May, 2011) to minimize(TER-BLEU)/2 metric.
As for the blind test set,we report the performance on the NIST MT08 eval-uation set, which consists of 691 sentences fromnewswire and 666 sentences from weblog.
We pickthe weights that produce the highest development setscores to decode the test set.We perform two sets of experiments.
The first setlooks at the contribution of the dominance modelwith varying values of o.
The second one looks atthe combination of the dominance model and theorientation model.
Table 3 summarizes the experi-mental results on NIST MT08 sets, categorized bygenres.
We report the results on newswire genre incolumns a-c, those on weblog genre in column d-f,and those on mixed genre in column g-i.
The perfor-mance of our baseline string-to-dependency syntax-based SMT is shown in the first line.Lines 2-7 in Table 3 show the results of our firstset of experiments, starting from the result of dom1,which looks at only at pairs of adjacent anchors, tothe result of dom6, which looks at pairs of anchorsthat are at most 5 anchors away.
As shown in line2, our dominance model provides a nice improve-ment of around 0.5 point over the baseline even if itonly looks at restricted context.
Increasing the or-der of our dominance model provides an additionalgain.
However, the gain is more pronounced in theweblog genre (up to around 1 BLEU point) than inthe newswire genre.
We conjecture that this may bethe artifact of our tune set, which comes from theweblog genre.
We stop at dom6 because we observe509that the weight of the feature score that correspondsto the maximum order (o = 6) has a negative sign,which often indicates a high correlation between thenew features and existing ones.Lines 8-13 in Table 3 shows the results of our sec-ond set of experiments.
Line 8 shows the result ofadding the orientation model (ori) to the baselinesystem.
As shown, integrating ori shows a signifi-cant gain.
On top of which, we then integrate dom1to dom5.
We see a very encouraging result as addingthe dominance model increases the performance fur-ther, consistently over different value of o.
This sug-gests that the dominance model is complementaryto the orientation model.
Our best result providesmore than 1 BP improvement and 1 TER reductionconsistently over different genres.
We see this resultas confirming our intuition that the global contextualinformation provided by our AG model can signifi-cantly improve the performance of SMT even in astate-of-the-art system.8 Related WorkOur work intersects with existing work in many dif-ferent respects.
In this section, we mainly focus onwork related to introducing higher-order contextualinformation to reordering model.In providing global contextual information, ourwork is related to a large amount of literature.
Toname a few, Zens and Ney (2006) improves the lexi-calized reordering model of Tillman (2004) by in-corporating part-of-speech information.
Chang etal.
(2009) incorporates contexts from syntactic parsetree.
Bach et al(2009) exploits the dependency in-formation and Xiong et al(2012) uses the predicate-argument structure.Vaswani et al(2011) introduces rule markovmodels for a forest-to-string model in which thenumber of possible derivations is restricted.
Morerecently, Durrani et al(2013) and Zhang et al(2013) cast reordering process as a Markov process.Similar to these models, our proposed model alsoprovide context dependencies to the application oftranslation rules, however, as they focus on mini-mal translation units (MTU) where we focus on aselected set of translation units.
(Banchs et al 2005)introduces a bigram model for monotone phrase-based system, but their definition of translation unitsis suitable only for language pairs with limited re-ordering, such as translating Spanish to English.In equating anchors with the function word class,our work is closely related to the function word-centered model of Setiawan et al(2007), especiallythe orientation model.
Our dominance model isclosely related to the reordering model of Setiawanet al(2009), except that they only look at pair of ad-jacent anchors, forming a chain structure instead ofa graph like in our dominance model.
Furthermore,we provide a discriminative treatment to the modelto include a richer set of features including syntac-tic features.
This work can be seen as modeling theidentity of the neighboring of the anchors, similar to(Setiawan et al 2013).
However, instead of lookingat the words at the borders, we look at whether theneighboring constituents contain other anchors.9 ConclusionWe propose the ?Anchor Graph?
(AG) model to en-code global contextual information.
A selected setof translation units, which we call anchors, servesas the vertices of AG.
And as the edges, we modeltwo types of relations, namely the dominance andthe precedence relations, where the former looks atthe positions of the anchors in the derivation struc-ture, while the latter looks at the positions of theanchors in the surface structure, resulting into twoprobabilistic models over edge labels.
As the mod-els look at the pairs of anchors that go beyond multi-ple translation units, our AG model provides globalcontextual information.Our AG model embodies (admittedly crudely)some basic principles of sentence organization,namely categorization (in categorizing units into an-chors and non-anchors), linear order (in modelingthe precedence of anchors) and constituency struc-ture (in modeling the dominance between anchors).We are encouraged by the facts that we learn theseprinciples in an unsupervised way and that we canachieve a significant improvement over a strongbaseline in a large-scale Chinese-to-English trans-lation task.
In the future, we hope to continue thisline of research, perhaps by learning to identify an-chors automatically from training data or by usingour models to induce derivations directly from un-aligned sentence pair.510AcknowledgementsWe would like to acknowledge the support ofDARPA under Grant HR0011-12-C-0015 for fund-ing part of this work.
The views, opinions, and/orfindings contained in this article/presentation arethose of the author/presenter and should not be inter-preted as representing the official views or policies,either expressed or implied, of the DARPA.ReferencesNguyen Bach, Qin Gao, and Stephan Vogel.
2009.Source-side dependency tree reordering models withsubtree movements and constraints.
In Proceedings ofthe Twelfth Machine Translation Summit (MTSummit-XII), Ottawa, Canada, August.
International Associa-tion for Machine Translation.Rafael E. Banchs, Josep M. Crego, Adria` de Gispert, Pa-trik Lambert, and Jose?
B. Marin?o.
2005.
Statisti-cal machine translation of Euparl data by using bilin-gual n-grams.
In Proceedings of the ACL Workshopon Building and Using Parallel Texts, pages 133?136,Ann Arbor, Michigan, June.
Association for Compu-tational Linguistics.Pi-Chuan Chang, Huihsin Tseng, Dan Jurafsky, andChristopher D. Manning.
2009.
Discriminative re-ordering with Chinese grammatical relations features.In Proceedings of the Third Workshop on Syntax andStructure in Statistical Translation (SSST-3) at NAACLHLT 2009, pages 51?59, Boulder, Colorado, June.
As-sociation for Computational Linguistics.Stanley Chen.
2009.
Shrinking exponential languagemodels.
In Proceedings of Human Language Tech-nologies: The 2009 Annual Conference of the NorthAmerican Chapter of the Association for Computa-tional Linguistics, pages 468?476, Boulder, Colorado,June.
Association for Computational Linguistics.Colin Cherry, Robert C. Moore, and Chris Quirk.
2012.On hierarchical re-ordering and permutation parsingfor phrase-based decoding.
In Proceedings of theSeventh Workshop on Statistical Machine Translation,pages 200?209, Montre?al, Canada, June.
Associationfor Computational Linguistics.David Chiang, Steve DeNeefe, and Michael Pust.
2011.Two easy improvements to lexical weighting.
In Pro-ceedings of the 49th Annual Meeting of the Associa-tion for Computational Linguistics: Human LanguageTechnologies, pages 455?460, Portland, Oregon, USA,June.
Association for Computational Linguistics.David Chiang.
2005.
A hierarchical phrase-based modelfor statistical machine translation.
In Proceedings ofthe 43rd Annual Meeting of the Association for Com-putational Linguistics (ACL?05), pages 263?270, AnnArbor, Michigan, June.
Association for ComputationalLinguistics.Nadir Durrani, Alexander Fraser, and Helmut Schmid.2013.
Model with minimal translation units, but de-code with phrases.
In Proceedings of the 2013 Con-ference of the North American Chapter of the Associa-tion for Computational Linguistics: Human LanguageTechnologies, pages 1?11, Atlanta, Georgia, June.
As-sociation for Computational Linguistics.Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-RuiWang, and Chih-Jen Lin.
2008.
LIBLINEAR: A li-brary for large linear classification.
Journal of Ma-chine Learning Research, 9:1871?1874.Yang Feng, Haitao Mi, Yang Liu, and Qun Liu.
2010.
Anefficient shift-reduce decoding algorithm for phrased-based machine translation.
In Coling 2010: Posters,pages 285?293, Beijing, China, August.
Coling 2010Organizing Committee.Michel Galley and Christopher D. Manning.
2008.
Asimple and effective hierarchical phrase reorderingmodel.
In Proceedings of the 2008 Conference onEmpirical Methods in Natural Language Processing,pages 848?856, Honolulu, Hawaii, October.
Associa-tion for Computational Linguistics.Mark Hopkins and Jonathan May.
2011.
Tuning as rank-ing.
In Proceedings of the 2011 Conference on Empir-ical Methods in Natural Language Processing, pages1352?1362, Edinburgh, Scotland, UK., July.
Associa-tion for Computational Linguistics.Philipp Koehn, Hieu Hoang, Alexandra Birch, ChrisCallison-Burch, Marcello Federico, Nicola Bertoldi,Brooke Cowan, Wade Shen, Christine Moran, RichardZens, Chris Dyer, Ondrej Bojar, Alexandra Con-stantin, and Evan Herbst.
2007.
Moses: Open sourcetoolkit for statistical machine translation, June.Hendra Setiawan, Min-Yen Kan, and Haizhou Li.
2007.Ordering phrases with function words.
In Proceed-ings of the 45th Annual Meeting of the Associationof Computational Linguistics, pages 712?719, Prague,Czech Republic, June.
Association for ComputationalLinguistics.Hendra Setiawan, Min Yen Kan, Haizhou Li, and PhilipResnik.
2009.
Topological ordering of function wordsin hierarchical phrase-based translation.
In Proceed-ings of the Joint Conference of the 47th Annual Meet-ing of the ACL and the 4th International Joint Confer-ence on Natural Language Processing of the AFNLP,pages 324?332, Suntec, Singapore, August.
Associa-tion for Computational Linguistics.Hendra Setiawan, Bowen Zhou, Bing Xiang, and LibinShen.
2013.
Two-neighbor orientation model withcross-boundary global contexts.
In Proceedings of the51st Annual Meeting of the Association for Compu-tational Linguistics (Volume 1: Long Papers), pages5111264?1274, Sofia, Bulgaria, August.
Association forComputational Linguistics.Libin Shen, Jinxi Xu, and Ralph Weischedel.
2008.
Anew string-to-dependency machine translation algo-rithm with a target dependency language model.
InProceedings of ACL-08: HLT, pages 577?585, Colum-bus, Ohio, June.
Association for Computational Lin-guistics.Christoph Tillman.
2004.
A unigram orientation modelfor statistical machine translation.
In HLT-NAACL2004: Short Papers, pages 101?104, Boston, Mas-sachusetts, USA, May 2 - May 7.
Association forComputational Linguistics.Ashish Vaswani, Haitao Mi, Liang Huang, and DavidChiang.
2011.
Rule markov models for fast tree-to-string translation.
In Proceedings of the 49th AnnualMeeting of the Association for Computational Linguis-tics: Human Language Technologies, pages 856?864,Portland, Oregon, USA, June.
Association for Compu-tational Linguistics.Dekai Wu.
1997.
Stochastic inversion transductiongrammars and bilingual parsing of parallel corpora.Computational Linguistics, 23(3):377?404, Sep.Deyi Xiong, Min Zhang, and Haizhou Li.
2012.
Model-ing the translation of predicate-argument structure forsmt.
In Proceedings of the 50th Annual Meeting of theAssociation for Computational Linguistics (Volume 1:Long Papers), pages 902?911, Jeju Island, Korea, July.Association for Computational Linguistics.Richard Zens and Hermann Ney.
2006.
Discrimina-tive reordering models for statistical machine trans-lation.
In Human Language Technology Conferenceof the North American Chapter of the Association forComputational Linguistics (HLT-NAACL): Proceed-ings of the Workshop on Statistical Machine Transla-tion, pages 55?63, New York City, NY, June.
Associa-tion for Computational Linguistics.Hui Zhang, Kristina Toutanova, Chris Quirk, and Jian-feng Gao.
2013.
Beyond left-to-right: Multiple de-composition structures for smt.
In Proceedings of the2013 Conference of the North American Chapter of theAssociation for Computational Linguistics: HumanLanguage Technologies, pages 12?21, Atlanta, Geor-gia, June.
Association for Computational Linguistics.512
