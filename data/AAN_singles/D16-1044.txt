Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 457?468,Austin, Texas, November 1-5, 2016. c?2016 Association for Computational LinguisticsMultimodal Compact Bilinear Poolingfor Visual Question Answering and Visual GroundingAkira Fukui*1,2 Dong Huk Park*1 Daylen Yang*1Anna Rohrbach*1,3 Trevor Darrell1 Marcus Rohrbach11UC Berkeley EECS, CA, United States2Sony Corp., Tokyo, Japan3Max Planck Institute for Informatics, Saarbru?cken, GermanyAbstractModeling textual or visual information withvector representations trained from large lan-guage or visual datasets has been successfullyexplored in recent years.
However, tasks suchas visual question answering require combin-ing these vector representations with each other.Approaches to multimodal pooling includeelement-wise product or sum, as well as con-catenation of the visual and textual represen-tations.
We hypothesize that these methodsare not as expressive as an outer product ofthe visual and textual vectors.
As the outerproduct is typically infeasible due to its highdimensionality, we instead propose utilizingMultimodal Compact Bilinear pooling (MCB)to efficiently and expressively combine multi-modal features.
We extensively evaluate MCBon the visual question answering and ground-ing tasks.
We consistently show the benefit ofMCB over ablations without MCB.
For visualquestion answering, we present an architec-ture which uses MCB twice, once for predict-ing attention over spatial features and againto combine the attended representation withthe question representation.
This model out-performs the state-of-the-art on the Visual7Wdataset and the VQA challenge.1 IntroductionRepresentation learning for text and images has beenextensively studied in recent years.
Recurrent neuralnetworks (RNNs) are often used to represent sen-tences or phrases (Sutskever et al, 2014; Kiros et al,* indicates equal contributionCNNWE, LSTMWhat are allthe peopledoing??
?FFT FFT -1Convolution?FFT?flying kites?ClassifierMultimodalCompactBilinearCount SketchSigned SqrtL2 NormFigure 1: Multimodal Compact Bilinear Pooling forvisual question answering.2015), and convolutional neural networks (CNNs)have shown to work best to represent images (Don-ahue et al, 2013; He et al, 2015).
For tasks such asvisual question answering (VQA) and visual ground-ing, most approaches require joining the represen-tation of both modalities.
For combining the twovector representations (multimodal pooling), currentapproaches in VQA or grounding rely on concatenat-ing vectors or applying element-wise sum or product.While this generates a joint representation, it mightnot be expressive enough to fully capture the complexassociations between the two different modalities.In this paper, we propose to rely on MultimodalCompact Bilinear pooling (MCB) to get a joint repre-sentation.
Bilinear pooling computes the outer prod-uct between two vectors, which allows, in contrastto element-wise product, a multiplicative interactionbetween all elements of both vectors.
Bilinear pool-ing models (Tenenbaum and Freeman, 2000) haverecently been shown to be beneficial for fine-grainedclassification for vision only tasks (Lin et al, 2015).However, given their high dimensionality (n2), bi-linear pooling has so far not been widely used.
In457this paper, we adopt the idea from Gao et al (2016)which shows how to efficiently compress bilinearpooling for a single modality.
In this work, we dis-cuss and extensively evaluate the extension to themultimodal case for text and visual modalities.
Asshown in Figure 1, Multimodal Compact Bilinearpooling (MCB) is approximated by randomly pro-jecting the image and text representations to a higherdimensional space (using Count Sketch (Charikaret al, 2002)) and then convolving both vectors effi-ciently by using element-wise product in Fast FourierTransform (FFT) space.
We use MCB to predict an-swers for the VQA task and locations for the visualgrounding task.
For open-ended question answering,we present an architecture for VQA which uses MCBtwice, once to predict spatial attention and the secondtime to predict the answer.
For multiple-choice ques-tion answering we introduce a third MCB to relate theencoded answer to the question-image space.
Addi-tionally, we discuss the benefit of attention maps andadditional training data for the VQA task.
To sum-marize, MCB is evaluated on two tasks, four datasets,and with a diverse set of ablations and comparisonsto the state-of-the-art.2 Related WorkMultimodal pooling.
Current approaches to mul-timodal pooling involve element-wise operations orvector concatenation.
In the visual question answer-ing domain, a number of models have been proposed.Simpler models such as iBOWIMG baseline (Zhouet al, 2015) use concatenation and fully connectedlayers to combine the image and question modali-ties.
Stacked Attention Networks (Yang et al, 2015)and Spatial Memory Networks (Xu et al, 2015) useLSTMs or extract soft-attention on the image fea-tures, but ultimately use element-wise product orelement-wise sum to merge modalities.
D-NMN (An-dreas et al, 2016a) introduced REINFORCE to dy-namically create a network and use element-wiseproduct to join attentions and element-wise sum pre-dict answers.
Dynamic Memory Networks (DMN)(Xiong et al, 2016) pool the image and questionwith element-wise product and sum, attending to partof the image and question with an Episodic Mem-ory Module (Kumar et al, 2016).
DPPnet (Noh etal., 2015) creates a Parameter Prediction Networkwhich learns to predict the parameters of the secondto last visual recognition layer dynamically from thequestion.
Similar to this work, DPPnet alows mul-tiplicative interactions between the visual and ques-tion encodings.
Lu et al (2016) recently proposeda model that extracts multiple co-attentions on theimage and question and combines the co-attentionsin a hierarchical manner using element-wise sum,concatenation, and fully connected layers.For the visual grounding task, Rohrbach et al(2016) propose an approach where the languagephrase embedding is concatenated with the visualfeatures in order to predict the attention weights overmultiple bounding box proposals.
Similarly, Hu etal.
(2016a) concatenate phrase embeddings with vi-sual features at different spatial locations to obtain asegmentation.Bilinear pooling.
Bilinear pooling has been ap-plied to the fine-grained visual recognition task.
Linet al (2015) use two CNNs to extract features froman image and combine the resulting vectors using anouter product, which is fully connected to an outputlayer.
Gao et al (2016) address the space and timecomplexity of bilinear features by viewing the bilin-ear transformation as a polynomial kernel.
Pham andPagh (2013) describe a method to approximate thepolynomial kernel using Count Sketches and convo-lutions.Joint multimodal embeddings.
In order to modelsimilarities between two modalities, many priorworks have learned joint multimodal spaces, or em-beddings.
Some of such embeddings are basedon Canonical Correlation Analysis (Hardoon et al,2004) e.g.
(Gong et al, 2014; Klein et al, 2015;Plummer et al, 2015), linear models with rankingloss (Frome et al, 2013; Karpathy and Fei-Fei, 2015;Socher et al, 2014; Weston et al, 2011) or non-lineardeep learning models (Kiros et al, 2014; Mao et al,2015; Ngiam et al, 2011).
Our multimodal com-pact bilinear pooling can be seen as a complementaryoperation that allows us to capture different interac-tions between two modalities more expressively thane.g.
concatenation.
Consequently, many embeddinglearning approaches could benefit from incorporatingsuch interactions.4580-xn1...0-x10 0x2-q20...q400 qn2q9q1q2...qn2x1x2...xn1Visual VectorTextual Vector?
?Count Sketch of Visual VectorCount Sketch of Textual VectorFFTFFTFFT-1ConvolutionFigure 2: Multimodal Compact Bilinear Pooling(MCB)3 Multimodal Compact Bilinear Poolingfor Visual and Textual EmbeddingsFor the task of visual question answering (VQA) orvisual grounding, we have to predict the most likelyanswer or location a?
for a given image x and questionor phrase q.
This can be formulated asa?
= argmaxa?Ap(a|x,q; ?)
(1)with parameters ?
and the set of answers or loca-tions A.
For an image embedding x = ?
(x) (i.e.
aCNN) and question embedding q = ?
(q) (i.e.
anLSTM), we are interested in getting a good joint rep-resentation by pooling both representations.
With amultimodal pooling ?
(x, q) that encodes the relation-ship between x and q well, it becomes easier to learna classifier for Equation (1).In this section, we first discuss our multimodalpooling ?
for combining representations from differ-ent modalities into a single representation (Sec.
3.1)and then detail our architectures for VQA (Sec.
3.2)and visual grounding (Sec.
3.3), further explaininghow we predict a?
with the given image representation?
and text representation ?.3.1 Multimodal Compact BilinearPooling (MCB)Bilinear models (Tenenbaum and Freeman, 2000)take the outer product of two vectors x ?
Rn1 andq ?
Rn2 and learn a model W (here linear), i.e.z = W [x?
q], where ?
denotes the outer product(xqT ) and [ ] denotes linearizing the matrix in a vec-tor.
As discussed in the introduction, bilinear poolingis interesting because it allows all elements of bothvectors to interact with each other in a multiplicativeAlgorithm 1 Multimodal Compact Bilinear1: input: v1 ?
Rn1 , v2 ?
Rn22: output: ?
(v1, v2) ?
Rd3: procedure MCB(v1, v2, n1, n2, d)4: for k ?
1 .
.
.
2 do5: if hk, sk not initialized then6: for i?
1 .
.
.
nk do7: sample hk[i] from {1, .
.
.
, d}8: sample sk[i] from {?1, 1}9: v?k = ?
(vk, hk, sk, nk)10: ?
= FFT?1(FFT(v?1) FFT(v?2))11: return ?12: procedure ?
(v, h, s, n)13: y = [0, .
.
.
, 0]14: for i?
1 .
.
.
n do15: y[h[i]] = y[h[i]] + s[i] ?
v[i]16: return yway.
However, the high dimensional representation(i.e.
when n1 and n2 are large) leads to an infeasiblenumber of parameters to learn in W .
For example,we use n1 = n2 = 2048 and z ?
R3000 for VQA.W thus would have 12.5 billion parameters, whichleads to very high memory consumption and highcomputation times.We thus need a method that projects the outer prod-uct to a lower dimensional space and also avoidscomputing the outer product directly.
As suggestedby Gao et al (2016) for a single modality, we relyon the Count Sketch projection function ?
(Charikaret al, 2002), which projects a vector v ?
Rn toy ?
Rd.
We initialize two vectors s ?
{?1, 1}n andh ?
{1, ..., d}n: s contains either 1 or ?1 for eachindex, and h maps each index i in the input v to anindex j in the output y.
Both s and h are initializedrandomly from a uniform distribution and remainconstant for future invocations of count sketch.
y isinitialized as a zero vector.
For every element v[i] itsdestination index j = h[i] is looked up using h, ands[i] ?
v[i] is added to y[j].
See lines 1-9 and 12-16 inAlgorithm 1.This allows us to project the outer product to alower dimensional space, which reduces the numberof parameters in W .
To avoid computing the outerproduct explicitly, Pham and Pagh (2013) showedthat the count sketch of the outer product of twovectors can be expressed as convolution of both countsketches: ?
(x ?
q, h, s) = ?
(x, h, s) ?
?
(q, h, s),4591 x 14 x 14512 x 14 x 14CNN(ResNet152) 16k x14x142048x14x142048x14x14Conv, ReluConv?Carrot?16k300020482048WE, LSTMSoftmaxWeighted SumTile2048What is thewoman feedingthe giraffe?Multimodal Compact BilinearMultimodal Compact BilinearFCSoftmaxFigure 3: Our architecture for VQA: Multimodal Compact Bilinear (MCB) with Attention.
Conv impliesconvolutional layers and FC implies fully connected layers.
For details see Sec.
3.2.where ?
is the convolution operator.
Additionally, theconvolution theorem states that convolution in thetime domain is equivalent to element-wise productin the frequency domain.
The convolution x?
?
q?
canbe rewritten as FFT?1(FFT(x?)
FFT(q?
)), whererefers to element-wise product.
These ideas aresummarized in Figure 2 and formalized in Algorithm1, which is based on the Tensor Sketch algorithm ofPham and Pagh (2013).
We invoke the algorithm withv1 = x and v2 = q.
We note that this easily extendsand remains efficient for more than two multi-modalinputs as the combination happens as element-wiseproduct.3.2 Architectures for VQAIn VQA, the input to the model is an image and aquestion, and the goal is to answer the question.
Ourmodel extracts representations for the image and thequestion, pools the vectors using MCB, and arrivesat the answer by treating the problem as a multi-classclassification problem with 3,000 possible classes.We extract image features using a 152-layer Resid-ual Network (He et al, 2015) that is pretrained onImageNet data (Deng et al, 2009).
Images are re-sized to 448?448, and we use the output of the layer(?pool5?)
before the 1000-way classifier.
We thenperform L2 normalization on the 2048-D vector.Input questions are first tokenized into words, andthe words are one-hot encoded and passed througha learned embedding layer.
The tanh nonlinearityis used after the embedding.
The embedding layeris followed by a 2-layer LSTM with 1024 units ineach layer.
The outputs of each LSTM layer areconcatenated to form a 2048-D vector.The two vectors are then passed through MCB.The MCB is followed by an element-wise signedsquare-root and L2 normalization.
After MCB pool-ing, a fully connected layer connects the resulting16,000-D multimodal representation to the 3,000 topanswers.Attention.
To incorporate spatial information, weuse soft attention on our MCB pooling method.
Ex-plored by (Xu et al, 2015) for image captioning andby (Xu and Saenko, 2016) and (Yang et al, 2015)for VQA, the soft attention mechanism can be easilyintegrated in our model.For each spatial grid location in the visual rep-resentation (i.e.
last convolutional layer of ResNet[res5c], last convolutional layer of VGG [conv5]),we use MCB pooling to merge the slice of the visualfeature with the language representation.
As depictedin Figure 3, after the pooling we use two convolu-tional layers to predict the attention weight for eachgrid location.
We apply softmax to produce a nor-malized soft attention map.
We then take a weightedsum of the spatial vectors using the attention map tocreate the attended visual representation.
We also ex-periment with generating multiple attention maps toallow the model to make multiple ?glimpses?
whichare concatenated before being merged with the lan-guage representation through another MCB poolingfor prediction.
Predicting attention maps with MCBpooling allows the model to effectively learn how toattend to salient locations based on both the visualand language representations.Answer Encoding.
For VQA with multiplechoices, we can additionally embed the answers.
We460Q : ?What do you see??
(Ground Truth : a3)a1 : ?A courtyard with flowers?a2 : ?A restaurant kitchen?a3 : ?A family with a stroller, tables for dining?a4 : ?People waiting on a train?a1a2a3a4Attention  I MCB Qa2 encodeda3 encodeda4 encodedConv a3MultimodalCompactBilinear a1 encoded WELSTMTile ReluConvSoftmaxFigure 4: Our architecture for VQA: MCB with At-tention and Answer Encodingbase our approach on the proposed MCB with atten-tion.
As can be seen from Figure 4, to deal withmultiple variable-length answer choices, each choiceis encoded using a word embedding and LSTM lay-ers whose weights are shared across the candidates.In addition to using MCB with attention, we use anadditional MCB pooling to merge the encoded an-swer choices with the multimodal representation ofthe original pipeline.
The resulting embedding isprojected to a classification vector with a dimensionequal to the number of answers.3.3 Architecture for Visual GroundingWe base our grounding approach on the fully-supervised version of GroundeR (Rohrbach et al,2016).
The overview of our model is shown in Fig-ure 5.
The input to the model is a query naturallanguage phrase and an image along with multipleproposal bounding boxes.
The goal is to predict abounding box which corresponds to the query phrase.We replace the concatenation of the visual representa-tion and the encoded phrase in GroundeR with MCBto combine both modalities.
In contrast to Rohrbachet al (2016), we include a linear embedding of thevisual representation and L2 normalization of both in-put modalities, instead of batch normalization (Ioffeand Szegedy, 2015), which we found to be beneficialwhen using MCB for the grounding task.4 Evaluation on Visual QuestionAnsweringWe evaluate the benefit of MCB with a diverse set ofablations on two visual question answering datasets.Q: ?Person in blue checkered shirt?b1b2b3b4QConvb4MultimodalCompactBilinearTileReluConvSoftmaxCNNCNNCNNCNNb3b2b1b3WELSTML2 normConv L2 normConv L2 normConv L2 normConv L2 normFigure 5: Our Architecture for Grounding with MCB(Sec.
3.3)4.1 DatasetsThe Visual Question Answering (VQA) real-imagedataset (Antol et al, 2015) consists of approximately200,000 MSCOCO images (Lin et al, 2014), with3 questions per image and 10 answers per question.There are 3 data splits: train (80K images), validation(40K images), and test (80K images).
Additionally,there is a 25% subset of test named test-dev.
Ac-curacies for ablation experiments in this paper arereported on the test-dev data split.
We use the VQAtool provided by Antol et al (2015) for evaluation.We conducted most of our experiments on the open-ended real-image task.
In Table 4, we also report ourmultiple-choice real-image scores.The Visual Genome dataset (Krishna et al,2016) uses 108,249 images from the intersection ofYFCC100M (Thomee et al, 2015) and MSCOCO.For each image, an average of 17 question-answerpairs are collected.
There are 1.7 million QA pairsof the 6W question types (what, where, when, who,why, and how).
Compared to the VQA dataset, Vi-sual Genome represents a more balanced distribu-tion of the 6W question types.
Moreover, the aver-age question and answer lengths for Visual Genomeare larger than the VQA dataset.
To leverage theVisual Genome dataset as additional training data,we remove all the unnecessary words such as ?a?,?the?, and ?it is?
from the answers to decrease thelength of the answers and extract QA pairs whoseanswers are single-worded.
The extracted data is fil-tered again based on the answer vocabulary spacecreated from the VQA dataset, leaving us with addi-tional 1M image-QA triplets.The Visual7W dataset (Zhu et al, 2016) is a partof the Visual Genome.
Visual7W adds a 7th whichquestion category to accommodate visual answers,461Method AccuracyElement-wise Sum 56.50Concatenation 57.49Concatenation + FC 58.40Concatenation + FC + FC 57.10Element-wise Product 58.57Element-wise Product + FC 56.44Element-wise Product + FC + FC 57.88MCB (2048?
2048?
16K) 59.83Full Bilinear (128?
128?
16K) 58.46MCB (128?
128?
4K) 58.69Element-wise Product with VGG-19 55.97MCB (d = 16K) with VGG-19 57.05Concatenation + FC with Attention 58.36MCB (d = 16K) with Attention 62.50Table 1: Comparison of multimodal pooling methods.Models are trained on the VQA train split and testedon test-dev.but we only evaluate the models on the Telling taskwhich involves 6W questions.
The natural languageanswers in Visual7W are in a multiple-choice formatand each question comes with four answer candidates,with only one being the correct answer.
Visual7Wis composed of 47,300 images from MSCOCO andthere are a total of 139,868 QA pairs.4.2 Experimental SetupWe use the Adam solver with  = 0.0007, ?1 = 0.9,?2 = 0.999.
We use dropout after the LSTM layersand in fully connected layers.
For the experiments inTable 1 and 2, we train on the VQA train split, vali-date on the VQA validation split, and report resultson the VQA test-dev split.
We use early stopping: ifthe validation score does not improve for 50,000 iter-ations, we stop training and evaluate the best iterationon test-dev.For the Visual7W task, we use the same hyperpa-rameters and training settings as in the VQA exper-iments.
We use the splits from (Zhu et al, 2016) totrain, validate, and test our models.
We also computeaccuracies on this data using their evaluation code.For VQA multiple choice, we train the open-endedmodels and take the argmax over the multiple choiceCompact Bilinear d Accuracy1024 58.382048 58.804096 59.428192 59.6916000 59.8332000 59.71Table 2: Accuracies for different values of d, thedimension of the compact bilinear feature.
Modelsare trained on the VQA train split and tested on test-dev.
Details in Sec.
4.3.Method What Where When Who Why How AvgZhu et al 51.5 57.0 75.0 59.5 55.5 49.8 54.3Concat+Att.
47.8 56.9 74.1 62.3 52.7 51.2 52.8MCB+Att.
60.3 70.4 79.5 69.2 58.2 51.1 62.2Table 3: Multiple-choice QA tasks accuracy (%) onVisual7W test set.answers at test time.
For Visual7W, we use the an-swer encoding as described in Sec.
3.2.4.3 Ablation ResultsWe compare the performance of non-bilinear andbilinear pooling methods in Table 1.
We see thatMCB pooling outperforms all non-bilinear poolingmethods, such as eltwise sum, concatenation, andeltwise product.One could argue that the compact bilinear methodsimply has more parameters than the non-bilinearpooling methods, which contributes to its perfor-mance.
We compensated for this by stacking fullyconnected layers (with 4096 units per layer, ReLUactivation, and dropout) after the non-bilinear pool-ing methods to increase their number of parameters.However, even with similar parameter budgets, non-bilinear methods could not achieve the same accuracyas the MCB method.
For example, the ?Concatena-tion + FC + FC?
pooling method has approximately40962 + 40962 + 4096 ?
3000 ?
46 million pa-rameters, which matches the 48 million parametersavailable in MCB with d = 16000.
However, the per-formance of the ?Concatenation + FC + FC?
methodis only 57.10% compared to MCB?s 59.83%.Section 2 in Table 1 also shows that compact bi-462Test-dev Test-standardOpen Ended MC Open Ended MCY/N No.
Other All All Y/N No.
Other All AllMCB 81.2 35.1 49.3 60.8 65.4 - - - - -MCB + Genome 81.7 36.6 51.5 62.3 66.4 - - - - -MCB + Att.
82.2 37.7 54.8 64.2 68.6 - - - - -MCB + Att.
+ GloVe 82.5 37.6 55.6 64.7 69.1 - - - - -MCB + Att.
+ Genome 81.7 38.2 57.0 65.1 69.5 - - - - -MCB + Att.
+ GloVe + Genome 82.3 37.2 57.4 65.4 69.9 - - - - -Ensemble of 7 Att.
models 83.4 39.8 58.5 66.7 70.2 83.2 39.5 58.0 66.5 70.1Naver Labs (challenge 2nd) 83.5 39.8 54.8 64.9 69.4 83.3 38.7 54.6 64.8 69.3HieCoAtt (Lu et al, 2016) 79.7 38.7 51.7 61.8 65.8 - - - 62.1 66.1DMN+ (Xiong et al, 2016) 80.5 36.8 48.3 60.3 - - - - 60.4 -FDA (Ilievski et al, 2016) 81.1 36.2 45.8 59.2 - - - - 59.5 -D-NMN (Andreas et al, 2016a) 81.1 38.6 45.5 59.4 - - - - 59.4 -AMA (Wu et al, 2016) 81.0 38.4 45.2 59.2 - 81.1 37.1 45.8 59.4 -SAN (Yang et al, 2015) 79.3 36.6 46.1 58.7 - - - - 58.9 -NMN (Andreas et al, 2016b) 81.2 38.0 44.0 58.6 - 81.2 37.7 44.0 58.7 -AYN (Malinowski et al, 2016) 78.4 36.4 46.3 58.4 - 78.2 36.3 46.3 58.4 -SMem (Xu and Saenko, 2016) 80.9 37.3 43.1 58.0 - 80.9 37.5 43.5 58.2 -VQA team (Antol et al, 2015) 80.5 36.8 43.1 57.8 62.7 80.6 36.5 43.7 58.2 63.1DPPnet (Noh et al, 2015) 80.7 37.2 41.7 57.2 - 80.3 36.9 42.2 57.4 -iBOWIMG (Zhou et al, 2015) 76.5 35.0 42.6 55.7 - 76.8 35.0 42.6 55.9 62.0Table 4: Open-ended and multiple-choice (MC) results on VQA test set (trained on train+val set) comparedwith state-of-the-art: accuracy in %.
See Sec.
4.4.linear pooling has no impact on accuracy comparedto full bilinear pooling.
Section 3 in Table 1 demon-strates that the MCB brings improvements regardlessof the image CNN used.
We primarily use ResNet-152 in this paper, but MCB also improves perfor-mance if VGG-19 is used.
Section 4 in Table 1 showsthat our soft attention model works best with MCBpooling.
In fact, attending to the Concatenation + FClayer has the same performance as not using attentionat all, while attending to the MCB layer improvesperformance by 2.67 points.Table 2 compares different values of d, the outputdimensionality of the multimodal compact bilinearfeature.
Approximating the bilinear feature with a16,000-D vector yields the highest accuracy.We also evaluated models with multiple atten-tion maps or channels.
One attenion map achieves64.67%, two 65.08% and four 64.24% accuracy(trained on train+val).
Visual inspection of the gen-erated attention maps reveals that an ensembling orsmoothing effect occurs when using multiple maps.Table 3 presents results for the Visual7W multiple-choice QA task.
The MCB with attention model out-performs the previous state-of-the-art by 7.9 pointsoverall and performs better in almost every category.4.4 Comparison to State-of-the-ArtTable 4 compares our approach with the state-of-the-art on VQA test set.
Our best single model usesMCB pooling with two attention maps.
Additionally,we augment our training data with images and QApairs from the Visual Genome dataset.
We also con-catenate the learned word embedding with pretrainedGloVe vectors (Pennington et al, 2014).Each model in our ensemble of 7 models usesMCB with attention.
Some of the models weretrained with data from Visual Genome, and somewere trained with two attention maps.
This ensem-463Method Accuracy, %Plummer et al (2015) 27.42Hu et al (2016b) 27.80Plummer et al (2016)1 43.84Wang et al (2016) 43.89Rohrbach et al (2016) 47.81Concatenation 46.50Element-wise Product 47.41Element-wise Product + Conv 47.86MCB 48.69Table 5: Grounding accuracy on Flickr30k Entitiesdataset.Method Accuracy, %Hu et al (2016b) 17.93Rohrbach et al (2016) 26.93Concatenation 25.48Element-wise Product 27.80Element-wise Product + Conv 27.98MCB 28.91Table 6: Grounding accuracy on ReferItGamedataset.ble is 1.8 points above the next best approach on theVQA open-ended task and 0.8 points above the nextbest approach on the multiple-choice task (on Test-dev).
Even without ensembles, our ?MCB + Genome+ Att.
+ GloVe?
model still outperforms the nextbest result by 0.5 points, with an accuracy of 65.4%versus 64.9% on the open-ended task (on Test-dev).5 Evaluation on Visual Grounding5.1 DatasetsWe evaluate our visual grounding approach on twodatasets.
The first is Flickr30k Entities (Plummeret al, 2015) which consists of 31K images fromFlickr30k dataset (Hodosh et al, 2014) with 244Kphrases localized with bounding boxes.
We followthe experimental setup of Rohrbach et al (2016),e.g.
we use the same Selective Search (Uijlings et1Plummer et al (2016) achieve higher accuracy of 50.89%when taking into account box size and color.
We believe ourapproach would also benefit from such additional features.al., 2013) object proposals and the Fast R-CNN (Gir-shick, 2015) fine-tuned VGG16 features (Simonyanand Zisserman, 2014).
The second dataset is Refer-ItGame (Kazemzadeh et al, 2014), which contains20K images from IAPR TC-12 dataset (Grubinger etal., 2006) with segmented regions from SAIAPR-12dataset (Escalante et al, 2010) and 120K associatednatural language referring expressions.
For Refer-ItGame we follow the experimental setup of Hu etal.
(2016b) and rely on their ground-truth bound-ing boxes extracted around the segmentation masks.We use the Edge Box (Zitnick and Dolla?r, 2014) ob-ject proposals and visual features (VGG16 combinedwith the spatial features, which encode bounding boxrelative position) from Hu et al (2016b).5.2 Experimental SetupIn all experiments we use Adam solver (Kingma andBa, 2014) with learning rate  = 0.0001.
The embed-ding size is 500 both for visual and language embed-dings.
We use d = 2048 in the MCB pooling, whichwe found to work best for the visual grounding task.The accuracy is measured as percentage of queryphrases which have been localized correctly.
Thephrase is localized correctly if the predicted bound-ing box overlaps with the ground-truth bounding boxby more than 50% intersection over union (IOU).5.3 ResultsTables 5 and 6 summarize our results in the visualgrounding task.
We present multiple ablations of ourproposed architecture.
First, we replace the MCBwith simple concatenation of the embedded visualfeature and the embedded phrase, resulting in 46.5%on the Flickr30k Entities and 25.48% on the Refer-ItGame datasets.
The results can be improved byreplacing the concatenation with the element-wiseproduct of both embedded features (47.41% and27.80%).
We can further slightly increase the per-formance by introducing additional 2048-D convo-lution after the element-wise product (47.86% and27.98%).
However, even with fewer parameters, ourMCB pooling significantly improves over this base-line on both datasets, reaching state-of-the-art accu-racy of 48.69% on Flickr30k Entities and 28.91%on ReferItGame dataset.
Figure 6 (bottom) showsexamples of improved phrase localization.464What vegetable is the dogchewing on?MCB: carrotGT: carrotWhat kind of dog is this?MCB: huskyGT: huskyWhat kind of flooring doesthe room have?MCB: carpetGT: carpetWhat color is the trafficlight?MCB: greenGT: greenIs this an urban area?MCB: yesGT: yesWhere are the buildings?MCB: in backgroundGT: on leftMCB Eltwise Product + Conv MCB Eltwise Product + ConvA tattooed woman with a green dress and yellow back-pack holding a water bottle is walking across the street.A dog distracts his owner from working at her computer.Figure 6: Top: predicted answers and attention maps from MCB model on VQA images.
Bottom: predictedgrounding from MCB model (left) and Eltwise Product + Conv model (right) on Flickr30k Entities images.6 ConclusionWe propose the Multimodal Compact Bilinear Pool-ing (MCB) to combine visual and text representa-tions.
For visual question answering, our architecturewith attention and multiple MCBs gives significantimprovements on two VQA datasets compared tostate-of-the-art.
In the visual grounding task, in-troducing MCB pooling leads to improved phraselocalization accuracy, indicating better interactionbetween query phrase representations and visual rep-resentations of proposal bounding boxes.
The codeto replicate our experiments is available at https://github.com/akirafukui/vqa-mcb.AcknowledgmentsWe would like to thank Yang Gao and Oscar Beijbomfor helpful discussions about Compact Bilinear Pool-ing.
This work was supported by DARPA, AFRL,DoD MURI award N000141110688, NSF awardsIIS-1427425 and IIS-1212798, and the Berkeley Ar-tificial Intelligence Research (BAIR) Lab.465References[Andreas et al2016a] Jacob Andreas, Marcus Rohrbach,Trevor Darrell, and Dan Klein.
2016a.
Learning tocompose neural networks for question answering.
InProceedings of the Conference of the North AmericanChapter of the Association for Computational Linguis-tics (NAACL).
[Andreas et al2016b] Jacob Andreas, Marcus Rohrbach,Trevor Darrell, and Dan Klein.
2016b.
Neural modulenetworks.
In Proceedings of the IEEE Conference onComputer Vision and Pattern Recognition (CVPR).
[Antol et al2015] Stanislaw Antol, Aishwarya Agrawal,Jiasen Lu, Margaret Mitchell, Dhruv Batra, C LawrenceZitnick, and Devi Parikh.
2015.
Vqa: Visual questionanswering.
In Proceedings of the IEEE InternationalConference on Computer Vision (ICCV).
[Charikar et al2002] Moses Charikar, Kevin Chen, andMartin Farach-Colton.
2002.
Finding frequent itemsin data streams.
In Automata, languages and program-ming, pages 693?703.
Springer.
[Deng et al2009] J. Deng, W. Dong, R. Socher, L.-J.
Li,K.
Li, and L. Fei-Fei.
2009.
ImageNet: A Large-Scale Hierarchical Image Database.
In Proceedings ofthe IEEE Conference on Computer Vision and PatternRecognition (CVPR).
[Donahue et al2013] Jeff Donahue, Yangqing Jia, OriolVinyals, Judy Hoffman, Ning Zhang, Eric Tzeng, andTrevor Darrell.
2013.
Decaf: A deep convolutionalactivation feature for generic visual recognition.
InProceedings of the International Conference on Ma-chine Learning (ICML).
[Escalante et al2010] Hugo Jair Escalante, Carlos AHerna?ndez, Jesus A Gonzalez, Aurelio Lo?pez-Lo?pez,Manuel Montes, Eduardo F Morales, L Enrique Sucar,Luis Villasen?or, and Michael Grubinger.
2010.
Thesegmented and annotated iapr tc-12 benchmark.
Com-puter Vision and Image Understanding, 114(4):419?428.
[Frome et al2013] Andrea Frome, Greg S Corrado, JonShlens, Samy Bengio, Jeff Dean, Tomas Mikolov, et al2013.
Devise: A deep visual-semantic embeddingmodel.
In Advances in Neural Information Process-ing Systems (NIPS).
[Gao et al2016] Yang Gao, Oscar Beijbom, Ning Zhang,and Trevor Darrell.
2016.
Compact bilinear pooling.In Proceedings of the IEEE Conference on ComputerVision and Pattern Recognition (CVPR).
[Girshick2015] Ross Girshick.
2015.
Fast R-CNN.
InProceedings of the IEEE International Conference onComputer Vision (ICCV).
[Gong et al2014] Yunchao Gong, Liwei Wang, Micah Ho-dosh, Julia Hockenmaier, and Svetlana Lazebnik.
2014.Improving image-sentence embeddings using largeweakly annotated photo collections.
In Proceedings ofthe European Conference on Computer Vision (ECCV).
[Grubinger et al2006] Michael Grubinger, Paul Clough,Henning Mu?ller, and Thomas Deselaers.
2006.
Theiapr tc-12 benchmark: A new evaluation resource forvisual information systems.
In International WorkshopOntoImage, volume 5, page 10.
[Hardoon et al2004] David R Hardoon, Sandor Szedmak,and John Shawe-Taylor.
2004.
Canonical correlationanalysis: An overview with application to learningmethods.
Neural computation, 16(12):2639?2664.
[He et al2015] Kaiming He, Xiangyu Zhang, ShaoqingRen, and Jian Sun.
2015.
Deep residual learning forimage recognition.
In Proceedings of the IEEE Con-ference on Computer Vision and Pattern Recognition(CVPR).
[Hodosh et al2014] Peter Hodosh, Alice Young, MicahLai, and Julia Hockenmaier.
2014.
From image de-scriptions to visual denotations: New similarity met-rics for semantic inference over event descriptions.
InTransactions of the Association for Computational Lin-guistics (TACL).
[Hu et al2016a] Ronghang Hu, Marcus Rohrbach, andTrevor Darrell.
2016a.
Segmentation from naturallanguage expressions.
In Proceedings of the EuropeanConference on Computer Vision (ECCV).
[Hu et al2016b] Ronghang Hu, Huazhe Xu, MarcusRohrbach, Jiashi Feng, Kate Saenko, and Trevor Dar-rell.
2016b.
Natural language object retrieval.
In Pro-ceedings of the IEEE Conference on Computer Visionand Pattern Recognition (CVPR).
[Ilievski et al2016] Ilija Ilievski, Shuicheng Yan, and Ji-ashi Feng.
2016.
A focused dynamic attention modelfor visual question answering.
arXiv:1604.01485.
[Ioffe and Szegedy2015] Sergey Ioffe and ChristianSzegedy.
2015.
Batch normalization: Acceleratingdeep network training by reducing internal covariateshift.
In Proceedings of the International Conferenceon Machine Learning (ICML).
[Karpathy and Fei-Fei2015] Andrej Karpathy and Li Fei-Fei.
2015.
Deep visual-semantic alignments for gener-ating image descriptions.
In Proceedings of the IEEEConference on Computer Vision and Pattern Recogni-tion (CVPR).
[Kazemzadeh et al2014] Sahar Kazemzadeh, Vicente Or-donez, Mark Matten, and Tamara L. Berg.
2014.Referit game: Referring to objects in photographs ofnatural scenes.
In Proceedings of the Conference onEmpirical Methods in Natural Language Processing(EMNLP).
[Kingma and Ba2014] Diederik Kingma and Jimmy Ba.2014.
Adam: A method for stochastic optimization.
InProceedings of the International Conference on Learn-ing Representations (ICLR).466[Kiros et al2014] Ryan Kiros, Ruslan Salakhutdinov, andRich Zemel.
2014.
Multimodal neural language mod-els.
In Proceedings of the International Conference onMachine Learning (ICML), pages 595?603.
[Kiros et al2015] Ryan Kiros, Yukun Zhu, RuslanSalakhutdinov, Richard S Zemel, Antonio Torralba,Raquel Urtasun, and Sanja Fidler.
2015.
Skip-thoughtvectors.
In Advances in Neural Information ProcessingSystems (NIPS).
[Klein et al2015] Benjamin Klein, Guy Lev, Gil Sadeh,and Lior Wolf.
2015.
Fisher vectors derived fromhybrid gaussian-laplacian mixture models for imageannotation.
In Proceedings of the IEEE Conference onComputer Vision and Pattern Recognition (CVPR).
[Krishna et al2016] Ranjay Krishna, Yuke Zhu, OliverGroth, Justin Johnson, Kenji Hata, Joshua Kravitz,Stephanie Chen, Yannis Kalantidis, Li-Jia Li, David AShamma, Michael Bernstein, and Li Fei-Fei.
2016.Visual genome: Connecting language and vi-sion using crowdsourced dense image annotations.arXiv:1602.07332.
[Kumar et al2016] Ankit Kumar, Ozan Irsoy, Jonathan Su,James Bradbury, Robert English, Brian Pierce, PeterOndruska, Ishaan Gulrajani, and Richard Socher.
2016.Ask me anything: Dynamic memory networks for natu-ral language processing.
In Proceedings of the Interna-tional Conference on Machine Learning (ICML).
[Lin et al2014] Tsung-Yi Lin, Michael Maire, Serge Be-longie, James Hays, Pietro Perona, Deva Ramanan,Piotr Dolla?r, and C Lawrence Zitnick.
2014.
Microsoftcoco: Common objects in context.
In Proceedings ofthe European Conference on Computer Vision (ECCV).
[Lin et al2015] Tsung-Yu Lin, Aruni RoyChowdhury, andSubhransu Maji.
2015.
Bilinear cnn models for fine-grained visual recognition.
In Proceedings of the IEEEInternational Conference on Computer Vision (ICCV).
[Lu et al2016] Jiasen Lu, Jianwei Yang, Dhruv Batra, andDevi Parikh.
2016.
Hierarchical Co-Attention for Vi-sual Question Answering.
In Advances in Neural Infor-mation Processing Systems (NIPS).
[Malinowski et al2016] Mateusz Malinowski, MarcusRohrbach, and Mario Fritz.
2016.
Ask Your Neu-rons: A Deep Learning Approach to Visual QuestionAnswering.
arXiv: 1605.02697.
[Mao et al2015] Junhua Mao, Wei Xu, Yi Yang, JiangWang, Zhiheng Huang, and Alan Yuille.
2015.
Deepcaptioning with multimodal recurrent neural networks(m-rnn).
In Proceedings of the International Confer-ence on Learning Representations (ICLR).
[Ngiam et al2011] Jiquan Ngiam, Aditya Khosla, MingyuKim, Juhan Nam, Honglak Lee, and Andrew Y Ng.2011.
Multimodal deep learning.
In Proceedings ofthe International Conference on Machine Learning(ICML), pages 689?696.
[Noh et al2015] Hyeonwoo Noh, Paul Hongsuck Seo, andBohyung Han.
2015.
Image question answering usingconvolutional neural network with dynamic parameterprediction.
In Proceedings of the IEEE Conference onComputer Vision and Pattern Recognition (CVPR).
[Pennington et al2014] Jeffrey Pennington, RichardSocher, and Christopher D. Manning.
2014.
Glove:Global vectors for word representation.
In Proceedingsof the Conference on Empirical Methods in NaturalLanguage Processing (EMNLP).
[Pham and Pagh2013] Ninh Pham and Rasmus Pagh.2013.
Fast and scalable polynomial kernels via ex-plicit feature maps.
In Proceedings of the 19th ACMSIGKDD International Conference on Knowledge Dis-covery and Data Mining, KDD ?13, pages 239?247,New York, NY, USA.
ACM.
[Plummer et al2015] Bryan Plummer, Liwei Wang, ChrisCervantes, Juan Caicedo, Julia Hockenmaier, and Svet-lana Lazebnik.
2015.
Flickr30k entities: Collectingregion-to-phrase correspondences for richer image-to-sentence models.
In Proceedings of the IEEE Interna-tional Conference on Computer Vision (ICCV).
[Plummer et al2016] Bryan Plummer, Liwei Wang, ChrisCervantes, Juan Caicedo, Julia Hockenmaier, and Svet-lana Lazebnik.
2016.
Flickr30k entities: Collectingregion-to-phrase correspondences for richer image-to-sentence models.
arXiv:1505.04870v3.
[Rohrbach et al2016] Anna Rohrbach, Marcus Rohrbach,Ronghang Hu, Trevor Darrell, and Bernt Schiele.
2016.Grounding of textual phrases in images by reconstruc-tion.
In Proceedings of the European Conference onComputer Vision (ECCV).
[Simonyan and Zisserman2014] Karen Simonyan and An-drew Zisserman.
2014.
Very deep convolutional net-works for large-scale image recognition.
In Proceed-ings of the International Conference on Learning Rep-resentations (ICLR).
[Socher et al2014] Richard Socher, Andrej Karpathy,Quoc V Le, Christopher D Manning, and Andrew Y Ng.2014.
Grounded compositional semantics for findingand describing images with sentences.
Transactions ofthe Association for Computational Linguistics, 2:207?218.
[Sutskever et al2014] Ilya Sutskever, Oriol Vinyals, andQuoc V. V Le.
2014.
Sequence to sequence learningwith neural networks.
In Advances in Neural Informa-tion Processing Systems (NIPS).
[Tenenbaum and Freeman2000] Joshua B Tenenbaum andWilliam T Freeman.
2000.
Separating style and contentwith bilinear models.
Neural computation, 12(6):1247?1283.
[Thomee et al2015] Bart Thomee, David A. Shamma,Gerald Friedland, Benjamin Elizalde, Karl Ni, Dou-glas Poland, Damian Borth, and Li-Jia Li.
2015.
The467new data and new challenges in multimedia research.CoRR, abs/1503.01817.
[Uijlings et al2013] Jasper RR Uijlings, Koen EA van deSande, Theo Gevers, and Arnold WM Smeulders.
2013.Selective search for object recognition.
InternationalJournal of Computer Vision (IJCV), 104(2).
[Wang et al2016] Liwei Wang, Yin Li, and SvetlanaLazebnik.
2016.
Learning deep structure-preservingimage-text embeddings.
In Proceedings of the IEEEConference on Computer Vision and Pattern Recogni-tion (CVPR).
[Weston et al2011] Jason Weston, Samy Bengio, andNicolas Usunier.
2011.
Wsabie: Scaling up to largevocabulary image annotation.
In Proceedings of the In-ternational Joint Conference on Artificial Intelligence(IJCAI).
[Wu et al2016] Qi Wu, Peng Wang, Chunhua Shen, An-ton van den Hengel, and Anthony Dick.
2016.
AskMe Anything: Free-form Visual Question AnsweringBased on Knowledge from External Sources.
In Proc.IEEE Conf.
Computer Vision Pattern Recognition.
[Xiong et al2016] Caiming Xiong, Stephen Merity, andRichard Socher.
2016.
Dynamic memory networks forvisual and textual question answering.
In Proceedingsof the International Conference on Machine Learning(ICML).
[Xu and Saenko2016] Huijuan Xu and Kate Saenko.
2016.Ask, attend and answer: Exploring question-guided spa-tial attention for visual question answering.
In Proceed-ings of the European Conference on Computer Vision(ECCV).
[Xu et al2015] Kelvin Xu, Jimmy Ba, Ryan Kiros, AaronCourville, Ruslan Salakhutdinov, Richard Zemel, andYoshua Bengio.
2015.
Show, attend and tell: Neuralimage caption generation with visual attention.
Pro-ceedings of the International Conference on MachineLearning (ICML).
[Yang et al2015] Zichao Yang, Xiaodong He, JianfengGao, Li Deng, and Alex Smola.
2015.
Stackedattention networks for image question answering.arXiv:1511.02274.
[Zhou et al2015] Bolei Zhou, Yuandong Tian, Sainba-yar Sukhbaatar, Arthur Szlam, and Rob Fergus.2015.
Simple baseline for visual question answering.arXiv:1512.02167.
[Zhu et al2016] Yuke Zhu, Oliver Groth, Michael Bern-stein, and Li Fei-Fei.
2016.
Visual7W: GroundedQuestion Answering in Images.
In Proceedings ofthe IEEE Conference on Computer Vision and PatternRecognition (CVPR).
[Zitnick and Dolla?r2014] C Lawrence Zitnick and PiotrDolla?r.
2014.
Edge boxes: Locating object propos-als from edges.
In Proceedings of the European Con-ference on Computer Vision (ECCV), pages 391?405.Springer.468
