THE BERKELEY UNIX  CONSULTANT PROJECTRober t  Wi lensky,  David N. Ch inMarc  Lur ia ,  James  Mar t inJ ames  Mayf ie ld,  and Dekai  Wu 1Division of Computer ScienceDepartment of EECSUniversity of California, BerkeleyBerkeley, CA 94720UC (UNIX Consultant) isan intelligent, natural language interface that allows naive users to learn aboutthe UNIX 2 operating system.
UC was undertaken because the task was thought o be both a fertiledomain for artificial intelligence (AI) research and a useful application of AI work in planning,reasoning, natural anguage processing, and knowledge representation.The current implementation f UC comprises the following components: a language analyzer, calledALANA, produces a representation f the content contained in an utterance; an inference component,called a concretion mechanism, that further refines this content; a goal analyzer, PAGAN, thathypothesizes the plans and goals under which the user is operating; an agent, called UCEgo, that decideson UC's goals and proposes plans for them; a domain planner, called KIP, that computes a plan toaddress the user's request; an expression mechanism, UCExpress, that determines the content o becommunicated to the user, and a language production mechanism, UCGen, that expresses UC'sresponse in English.UC also contains acomponent, called KNOME, that builds a model of the user's knowledge state withrespect o UNIX.
Another mechanism, UCTeacher, allows a user to add knowledge of both Englishvocabulary and facts about UNIX to UC's knowledge base.
This is done by interacting with the user innatural anguage.All these aspects of UC make use of knowledge represented in a knowledge representation systemcalled KODIAK.
KODIAK is a relation-oriented system that is intended to have wide representationalrange and a clear semantics, while maintaining a cognitive appeal.
All of UC's knowledge, ranging fromits most general concepts to the content of a particular utterance, is represented in KODIAK.1 INTRODUCTION TO THE UNIX CONSULTANT (UC)PROJECTSeveral years ago, we began a project called UC (UNIXConsultant).
UC was to function as an intelligent,natural language interface that would allow naive usersto learn about the UNIX operating system by interact-ing with the consultant in ordinary English.
We some-times refer to UC as "an intelligent 'help' facility" toemphasize our intention to construct a consultationsystem, rather than a natural anguage front end to anoperating system.
Whereas front ends generally take theplace of other interfaces, UC was intended to help theuser learn how to use an existing one.We had two major motivations for choosing this task.These can be summarized by saying that we believedthe task to be both interesting and doable.
It seemed tous that much natural anguage work indeed, much ofAI research--has fallen into two largely non-inter-secting categories: On the one hand, there are quiteinteresting and ambitious projects that have been morethe fertile source of exciting speculations than of usefultechnology.
In contrast, there are projects whose scopeis severely limited, either to some intrinsically bounded,real-world task or to a laboratory microworld.
Theseprojects result in much excitement by the production ofa working system or successful technology.
But suchprojects have rarely produced much in the way ofCopyright 1988 by the Association for Computational Linguistics.
Permission tocopy without fee all or part of this material is granted providedthat the copies are not made for direct commercial advantage and the CL reference and this copyright notice are included on the first page.
Tocopy otherwise, orto republish, requires a fee and/or specific permission.0362-613X/88/010035-84503.00Computational Linguistics, Volume 14, Number 4, December 1988 35Robert Wilensky, David N. Chin, Marc Luria, ,lames Martin, James Mayfield, and Dekai Wu The Berkeley UNIX Consultant Projectprogress on fundamental issues that comprise the cen-tral goals of AI researchers.Our hope was that the consultation task would re-quire us to address fundamental problems in naturallanguage processing, planning and problem solving, andknowledge representation, all of which are of interest tous.
We believe this to be the case because (1) thedomain of an operating system is quite large and com-plex, (2) users' conceptions of computer systems areoften based on other domains, particularly space andcontainment, and (3) the structure of a consultationsession requires the consultant to understand the user'slanguage, hypothesize the user's intentions, reasonabout the user's problem, access knowledge about thetopic in question, and formulate a reasonable response.In sum, virtually all the problems of language process-ing and reasoning arise in some fashion.While the task is interesting, it is nevertheless lim-ited.
Arbitrary knowledge of the world is generally notrequired, as it may be in other natural anguage tasks,such as text processing.
Even knowledge about thedomain might be limited in ways that do not compro-mise the overall integrity of the system.
In particular,the task is intrinsically "fail-soft".
Since the system is ahelp facility, it need not be capable of handling everytask put to it to serve a useful function.
This is probablyless true of systems that are intended to be interfaces.
Intheir case, failure to correctly process a request by theuser leaves the user with little recourse.
However, aconsultant may be quite useful even if it cannot help allthe time.Similarly, there are strategies that might be employedin a consultant task that further reduce the degree ofcoverage required by the system.
For example, if askeda very specific question, it is not unreasonable that aconsultant respond by telling the user where to look forthe information.
Thus the degree of expertise of theconsultation system may be circumscribed.In other words, we felt that the operating systemdomain was an appropriate r placement for the "blocksworld".
Building a consultant for the domain is a realtask one would like to have accomplished.
The domainwould limit the breadth, but not the depth, of AIresearch required.1.1 UC--SCIENCE OR ENGINEERING?Our approach to AI has had a distinctly cognitive bent.While a lengthy exposition might be needed to definethis precisely, let it suffice here to say that we areinterested in modeling human beings at least to a firstapproximation.
Thus, as far as we could, we haveattempted to build a system that modeled how webelieve a human consultant actually functions.In some cases, this goal meant hat we would makesome problems harder for ourselves than one might ifone's goals were strictly technological.
For example,since many word senses are unlikely to be used whentalking to a consultant, a purely engineering approachmight play down the problem of ambiguity.
However, itis our goal to address such problems in a generalfashion.At the same time, there were many pragmatic on-cessions that were made in implementing UC.
Some ofthese were: forced on us by the nature of universityresearch.
For example, a process might be divided intotwo components for the sake of implementation, al-though the particular division may not be motivatedotherwise.
These components might even exercise twodifferent approaches tosimilar subproblems, dependingon the biases of their authors.
Sometimes, for the sakeof efficiency, we chose to implement only part of whatwe believed to be a larger process.
Also for efficiency'ssake, and to prevent ruly difficult but infrequent prob-lems from scuttling the entire effort, we implementedsome solutions that we did not believe in completely.For example, UC's control structure isoverly simplisticin ways that we understand but have not corrected.
Wewill make note of other such situations in the textbelow.
In general, when this was the case, the solutionused takes the form of checking for certain frequentlyoccurring cases in order to preclude having to solve ageneral problem.Since our goals were not strictly technological, wedid not feel that it was necessary or appropriate inorderfor our system to be considered a success to produce aproduct that could actually be used in a real-worldsetting.
However, we did feel that we should show thatone could develop such a system along the lines that ourresearch suggested.
This would be accomplished bydeveloping an extendible prototype.1.2 UC OLD AND NEWWe initially built a prototype version of UC consistinglargely of off-the-shelf components (Wilensky, Arens,and Chin 1984).
While this system seemed to suggestthat our goal was feasible, it was deficient in manyways.
There were whole components hat needed to beincluded but were not.
For example, the initial systemmade few inferences and was not capable of planning itsown actions.
In addition, each individual componentwas in need of much refinement.Probably the most important deficiency was in thearea of knowledge representation.
The initial prototypeof UC was implemented in PEARL (Deering, Faletti,and Wilensky 1981).
PEARL is an AI language anddatabase management package that supports framelikestructures similar to those employed by other represen-tation languages, with perhaps some more attentiongiven to efficient retrieval.
However, we found that ourunderlying representational system was inadequate.Unfortunately, the problems with our system were notunique to it, but were shared by most other efforts torepresent and organize knowledge.Much of the focus of our recent work has been toaddress and rectify these problems of knowledge repre-sentation.
Our critiques of existing knowledge represen-36 Computational Linguistics, Volume 14, Number 4, December 1988Robert Wilensky, David N. Chin, Marc Luria, James Martin, James Mayfield, and Dekai Wu The Berkeley UNIX Consultant Projecttation schemes, along with our new prescription forthese deficiencies, can be found in Wilensky (1986).That report contains a description of KODIAK, theknowledge representation system that our work has ledus to, and upon which our current implementation f theUNIX Consultant is based.Since one's knowledge representation is generallyfundamental to the structure of most of the modules ofone's systems, developing a new one means redesigningeach component around a new representational system.This report is a description of a new prototype of UC sodesigned.1.3 REASONABLE AGENTS VERSUS INTELLIGENTINTERFACESOur goal in building UC is to simulate a human consult-ant.
As a result, the system has a structure that is morecomplex than other so-called intelligent interfaces.
In-deed, we feel that looking at such a system as aninterface is misleading.
Instead, we prefer the metaphorof a reasonable agent.
Unlike an interface, which is aconduit through which information flows, an agent is aparticipant in a situation.
In particular, an agent hasexplicit goals of its own, and a reasonable agent must beable to make obvious inferences and display judgmentin making decisions.
Typically, a consultant con-structed along the lines of a reasonable agent will makea user's goals its own in trying to help that user.However, a reasonable agent is not always compelled todo so.
Human consultants will not obligingly give outinformation to which a user is not entitled or which theysuspect will be put to ill use.
Similarly, a good consul-tant might deflect a user's request because the consul-tant feels that the user does not have an adequate graspof the domain, has a particular misconception, or islacking some particular fact.
In addition, a good con-sultant might do something more than simply answer aquestion.
He might take the opportunity to show theuser how to do a more general task of which the user'sparticular equest is merely a special case.
In all thesesituations, an action other than simply responding to arequest is warranted.A reasonable agent is ideally suited to handle such abroad class of situations.
It does so by deciding what itsgoals should be in the given situation, and then planningfor them.
For example, when UC is asked how to crashthe system, it forms two goals, one of helping the userto know what he or she wants, and one of protecting theintegrity of the system.
It then realizes that these twogoals are in conflict, and eventually decides the conflictin favor of the latter goal.Of course, it is possible to achieve by other meansvarious parts of the functionality here attributed to themodel of a reasonable agent.
For example, one cansimply build one component that tries to detect miscon-ceptions, another that checks for requests having to dowith crashing the system, yet another to capitalize onopportunities to educate the user, etc.
However, thereasonable agent framework provides a single, flexiblecontrol structure in which to accomplish all these tasks,and, in particular, deal with interactions between them.That is its engineering motivation.
Our primary reasonfor adopting it is that it is our theory about how humansfunction in consulting situations.1.4 OVERVIEWThe structure of this report is as follows.
First, wepresent an outline of the structure of the current versionof our consultation system.
We follow this with a briefdescription of KODIAK.
The next sections constitutethe bulk of this report and are essentially a detaileddescription of a trace of a rather simple sentencethrough UC's components.
In doing so, the mechanismsof those components that are primarily responsible forUC's agentlike qualities are described.
Finally, weconclude with some discussion of the deficiencies of ourcurrent design.1.4.1 OUTLINE OF UC'S STRUCTUREUC is comprised of a number of components, which areinvoked in a more or less serial fashion.1.
LANGUAGE ANALYSIS (ALANA)Language analysis is that component of the understand-ing process that computes a representation f the con-tent of an utterance.
ALANA, written by Charles Cox,produces a KODIAK representation of the content ofan utterance.
This representation generally containsonly what can be determined from the words andlinguistic structures present in the utterance.In our theoretical framework, we call such an analy-sis of an utterance its primal content.
The concept ofprimal content is related to what is usually described asthe literal meaning or sentence meaning of an utterance.However, unlike literal meaning, the primal content ofan utterance involves certain idiomatic interpretations(i.e., it is not necessarily composed from words andgeneral grammatical constructions).
Also, the primalcontent of an utterance may be rather abstract, perhapsso much so that it may not be a suitable candidate for ameaning.
For example, the literal meaning of "The catis on the mat" is generally taken to be a conventionalsituation in which a cat is resting upon a mat.
However,the primal content of this sentence would be moreabstract, where the contribution of "on"  is identical tothat in the primal content of "The light fixture is on theceiling" or "The notice is on the bulletin board.
"Presumably, this conveys ome sort of support relation.Note that such an abstract content appears never to bein itself the meaning of such an utterance (cf.
Searle1979).In contrast o primal content is the actual content ofan utterance.
The actual content is context dependent,generally requires some amount of inference based onworld knowledge, and is a suitable candidate for themeaning of an utterance.
For example, the actual con-Computational Linguistics, Volume 14, Number 4, December 1988 37Robert Wilensky, David N. Chin, Marc Luria, .James Martin, James Mayfield, and Dekai Wu The Berkeley UNIX Consultant Projecttent of "The cat is on the mat," without a furthercontext specified, is what the literal meaning of thissentence is generally taken to be.
Computing this con-tent from the primal content requires pragmatic knowl-edge about the kind of support relation a cat and a matare likely to be in, and requires makinlg an inference thatcannot be justified by the meanings of the terms and thegrammatical constructions,present in l:he utterance.
Theprimal/actual content distinction is elaborated on inWilensky (1987).2.
INFERENCE (CONCRETION MECHANISM)The particular kind of inference needed to go from aprimal content o an actual content sometimes involvesa process known as concretion (Wilensky 1983).
Con-cretion is the process of inferring a more specificinterpretation of an utterance than is justified by lan-guage alone.
Concretion may involve finding a morespecific default interpretation r some other interpreta-tion based on the context.
For example, in the example"The cat is on the mat" above, the actual contentcomputed is the default support relation between a catand a mat.
In some compelling context, a quite differentactual content may be computed from the same primalcontent.
(There are other possible relations between primaland actual content besides the latter being a morespecific interpretation of the former.
For example, aconventionalized metaphor might have a primal contentthat more closely resembles its literal interpretation butan actual content resembling its metaphoric nterpreta-tion.
Thus one analysis of a sentence like "John gaveMary a kiss" will have as its primal content an instanceof giving, but as its actual content an instance of kissing.We will not pursue further the details of the primal/actual content distinction here.
This is largely because,in UC, the need for concretion is widespread, and ourhandling of other kinds of primal/actual content compu-tations is more haphazard.
)In UC, concretion is needed primarily because weneed to organize knowledge about more specific inter-pretations of utterances than can be arrived at throughlinguistic knowledge alone.
For example, if UC is askedthe question "How can I delete a file?
", ALANA canrepresent that this is a question about how to delete afile.
But it would not have any reason to assume thatdeleting a file is a specific kind of deleting.
Determiningthat this is so is likely to be important for severalreasons.
For example, knowledge about how to delete afile will be found associated with the concept of "filedeletion", say, but not with the concept of deletion ingeneral.
Thus UC must infer that "deleting afile" refersto the specific kind of deletion having to do withcomputer storage in order to perform subsequent taskslike finding plans for accomplishing the user's request.In UC, concretion is the function of a special mech-anism designed specifically for that purpose by DekaiWu.
The output of the concretion mechanism is anotherKODIAK representation, generally one containingmore :specific oncepts than that produced by ALANA.Having a specific concretion mechanism is a pragmaticconcession.
We feel it is unlikely that such a specificmechanism is theoretically warranted.
A more justifi-able position is that a general inference mechanismshould be exploited here, concretion being only one ofthe kinds of inference such a mechanism accomplishes.A unified text-inference mechanism that accomplishesconcretion as well as other forms of inference has beenbuilt (Norvig 1987).
It is our belief that some mechanismakin to Norvig's should be used in UC in place of aspecialized concretion engine, but no attempt has yetbeen made to do so.3.
GOAL ANALYSIS (PAGAN)Having computed an actual content for an utterance,UC then tries to hypothesize the plans and goals underwhich the user is operating.
This level of analysis isperformed by PAGAN, written by James Mayfield.PAGAN performs a sort of speech act analysis of theutterance.
The result of this analysis is a KODIAKrepresentation f the network of plans and goals theuser is using with respect o UC.Goal analysis is important in many ways for UC.
Asis generally well known, an analysis of this sort isnecessary to interpret indirect speech acts, such as "Doyou know how to delete a file?
", or "Could you tell mehow to delete a file?".
Furthermore, goal analysis helpsto provide better answers to questions uch as "DoesIs -r recursively list subdirectories?".
An accurate re-sponse to the literal question might simply be, "No .
"But a better esponse is, "No, it reverses the order ofthe sort of directory listing; Is -R recursively listssubdirectories."
To produce such a response, oneneeds to realize that the goal underlying the asking ofthis question is either to find out what 'Is -r' does, or tofind out how to recursively list subdirectories.
It is thejob of the goal analyzer to recognize that such goals arelikely to be behind such a question.4.
AGENT (UCEGO)Having hypothesized what the user wants of it, wewould expect a system like UC to do what the userrequested.
But, as mentioned above, this is not alwaysappropriate.
UC should not aid and abet a user trying toperform malicious mischief; it might need to correct anerrant user or it might decide to supply unasked-forinformation to one diagnosed as not knowing an impor-tant fact.In order to deal with such situations UC is con-structed as an agent.
This agent reacts to users' requestsby forming goals and acting on them.
The centralmechanism of UC is called UCEgo, and has beendeveloped by David Chin.In a typical transaction, UCEgo will simply adopt hegoal of having the user know what the user wants toknow.
However, as the example above illustrates,38 Computational Linguistics, Volume 14, Number 4, December 1988Robert Wilensky, David N. Chin, Marc Luria, James Martin, James Mayfield, and Dekai Wu The Berkeley UNIX Consultant ProjectUCEgo may adopt other goals as well, such as protect-ing the integrity of the system.
It may also have todetect conflicts between these goals.
Sometimes,UCEgo, attempting to be educational, may adopt asomewhat different goal from the user's.
Thus, if theuser asks UC to actually perform some request, such astelling the user who is on the system, UC will decide totell the user how to perform such a function, rather thando what the user requested.UCEgo implements much of the agentlike characterof UC.
While interfaces are generally thought of aspassive conduits through which information flows, UCis an agent that listens to the user and is generallyhelpful.
But it has its own agenda, and the requests ofthe user are merely a source of input to it.5.
USER MODELING (KNOME)Several of UC's components may need informationabout the user to make an effective choice.
For exam-ple, an expert user certainly knows how to delete a file.Thus such a user uttering "Do you know how to deletea file?"
is unlikely to be asking for this information--more likely this user is testing the consultant's knowl-edge.Assessing the knowledge state of the user is thefunction of a user modeling program called KNOME,developed by David Chin.
It is exploited by severalcomponents, including the "expression mechanism"described below.6.
DOMAIN PLANNER (KIP)Typically, UCEgo tries to help the user.
This usuallyrequires determining a fact that the user would like toknow.
This task is accomplished by KIP.
KIP is a"domain planner" developed by Marc Luria.
WhileUCEgo infers its own goals, and plans to act on them,KIP is given a task by UCEgo of determining how toaccomplish what the user wants to accomplish.
KIPtries to determine how to accomplish this task, usingknowledge about UNIX and knowledge about he user'slikely goals.
KIP returns a plan, represented in KO-DIAK.
For example, UCEgo may give KIP the task ofdetermining how to move a file to another machine, ifthis is something the user wants to know.
Here, KIPwould come up with the plan of copying the file to thetarget machine and then deleting the original.Since UCEgo is also a planner, UC in effect has twoplanners within it.
Again, this is probably not theoreti-cally justifiable, although the two planners have endedup focusing on rather different aspects of planning.
Itremains to be seen whether a single mechanism ightaccommodate both functions.7.
EXPRESSION MECHANISM (UCEXPRESS)Having gotten KIP to compute a plan for the user'srequest, UCEgo now tries to communicate his plan tothe user.
To do so, it must determine which aspects ofthe plan are worthy of communication a d how best tocommunicate hem.
For example, if it is likely that theuser knows how to use commands ingeneral, itmight besufficient just to specify the name of the command.
Incontrast, it might be helpful to illustrate a generalcommand with a specific example.UCExpress is an expression mechanism written byDavid Chin.
It edits out those parts of the conceptualanswer returned by KIP that, for various reasons,appear unnecessary to communicate.
UCExpress mayalso choose to illustrate an answer in several formats.For example, it might illustrate a general answer bygenerating a specific example, or it might explain onecommand in terms of another, simpler, command.The result of UCExpress is an annotated KODIAKnetwork, where the annotation specifies which part ofthe network is to be generated.8.
LANGUAGE PRODUCTION (UCGEN)Once UC has decided what to communicate, it has toput it into words.
This is done by a generation programcalled UCGen.
UCGen is a simple generator, pro-grammed by Anthony Albert and Marc Luria.
It takesthe marked KODIAK network produced by UCExpressand, using knowledge of English, produces entences tocomplete the transaction with the user.9.
LEARNING MECHANISM (UCTEACHER)Since it is intended that UC be an extensible system, amechanism has been developed to add new knowledgeto the system by talking to it in natural language.
Thismechanism, called UCTeacher, is the work of JamesMartin.
UCTeacher has capabilities to extend bothUC's knowledge base of UNIX facts as well as itsknowledge of English vocabulary.Note that several UC components deal with goalsand plans, but in rather different ways.
To minimizeconfusion, we emphasize the different asks that theseprograms perform: PAGAN hypothesizes goals of theuser with respect o UC; UCEgo reacts to the user byforming goals of its own and then trying to accomplishthem; and KIP is given the goal of solving someunderlying UNIX problem.
Thus PAGAN is a goalanalyzer, i.e., a program that attempts to determinesomeone lse's goals, while UCEgo and KIP are plan-ners, i.e., programs that determine and produce plansfor their own goals.UCEgo and KIP exist as two separate programsprimarily for pragmatic reasons.
It may be possible thata single planner could accomplish the tasks of bothUCEgo and KIP.
Such an integration will have to awaita revision of our system.1.5 UC EXAMPLESThe first set of examples illustrates the scope of thecurrent UC prototype.
They are intended to show thekind of requests the system can handle, as well as itsscope.
The second set demonstrates some more com-plex capabilities.Computational Linguistics, Volume 14, Number 4, December 1988 39Robert Wilensky, David N. Chin, Marc Luria, ,lames Martin, James Mayfield, and Dekai Wu The Berkeley UNIX Consultant ProjectEach sentence below preceded by the prompt '#'  issupplied by the user.
The rest of the text is a computer-generated response.
Note that the system handles avariety of request forms, some direct and some indirect.Also, the responses to general questions will cause anexample to be generated.
Finally, the reader may noticethat the vocabulary used to respond to a request is oftendifferent from that used in the request itself.
For exam-ple, a question about file protection is answered interms of file permission; a question about changing adirectory is answered in terms of connecting to adirectory.
This is because the representations u ed toencode the system's knowledge are kept in a canonicalform, so that different expressions of the same meaningare mapped to the same internal representation.
Upongenerating a response, the system use,.
; only this internalrepresentation, which, coupled with its generating bi-ases, causes an expression of an answer that is largelyindependent of the vocabulary that appeared in therequest.1.5.1 SIMPLE EXAMPLES% UCWelcome to UC (Unix Consultant) version 3.23To a UC '#'  prompt, please type in your questionsabout the Unix file system in English.To leave, just type a 'AD' or '(exit)'.Hi.How can I help you?# Can you tell me how to move a file?Use mv.For example, to move the file named foo to the filenamed fool, type 'mv foo fool'.# Do you know how to delete a file named dekai?Type 'rm dekai'.# Do you know how to delete a file?Use rm.For example, to delete the file named foo, type 'rmfoo'.# How can I delete the file charlie?Type 'rm charlie'.# How can I delete the file named tony?Type 'rm tony'.# How can I get rid of the file named boob?Type 'rm boob'.# I want to delete a file named peter.Type 'rm peter'.# I want to delete a file.Use rm.For example, to delete the file named foo, type 'rmfoo'.# I want to delete the file dave.Type 'rm dave'.# How can I add read permission to a file?Use chmod.For example, to add individual read permission to thefile named foo, type 'chmod u+r foo'.# Elow can I add the owner execute permission to afile'!Use', chmod.For example, to add individual execute permission tothe file named foo, type 'chmod u+x foo'.# How can I change the group execute permission ofa file?Use chmod.For example, to remove group execute permissionfrom the file named foo, type 'chmod g-x foo'.# How can I change the user read permission of afile?Use chmod.For example, to remove individual read permissionfrom the file named foo, type 'chmod u-r foo'.# How can I remove the group write permission fromthe file named bimbo?Type 'chmod g-w bimbo'.# ltow do I change the protection of a file?Use chmod.For example, to remove individual read permissionfrom the file named foo, type 'chmod u-r foo'.# How can I compact a file?Use compact.For example, to compact he file named foo, type'compact foo'.# How can I create a directory?Use mkdir.For example, to create the directory named foo, type'mkdir foo'.# How can I find a file?Use whereis.For example, to find the file named foo, type 'whereis foo'.# How can I find out how much disk space I amusing?Type 'du'.# How can I find out who is using the system?Type 'finger'.# How can I find out wilensky's phone number?Type 'finger wilensky'.# How can I list a directory?Use Is.For example, to list the directory named foo, type 'Isfoo'.# How can I logout?Type 'logout'.# Tell me how to leave.Type 'logout'.40 Computational Linguistics, Volume 14, Number 4, December 1988Robert Wilensky, David N. Chin, Marc Luria, James Martin, James Mayfield, and Dekai Wu The Berkeley UNIX Consultant Project# How can I move a file to the file named blah?Use mv.For example, to move the file named foo to the filenamed blah, type 'mv foo blah'.# How can I move the file junk?Use mv.For example, to move the file named junk to the filenamed foo, type 'mv junk foo'.# How do I move a file?Use mv.For example, to move the file named foo to the filenamed fool, type 'mv foo fool'.# How do I change my directory?Use cd.For example, to connect o the directory named foo,type 'cd foo'.# How do I copy a file?Use cp.For example, to copy the file named foo to the filenamed fool, type 'cp foo fool'.# How do I copy the file foo to the file blah?Type 'cp foo blah'.# How do I create a directory?Use mkdir.For example, to create the directory named foo, type'mkdir foo'.# How do I print a file named info?Type 'lpr info'.# I want to print a file.Use lpr.For example, to print the file named foo, type 'lprfoo'.# I want to change my working directory.Use cd.For example, to connect o the directory named foo,type 'cd foo'.# I want to copy the file named rick.Use cp.For example, to copy the file named rick to the filenamed foo, type cp 'rick foo'.# I want to delete a directory.Use rmdir.For example, to delete the directory named foo, type'rmdir foo'.# I want to remove a directory.Use rmdir.For example, to delete the directory named foo, type'rmdir foo'.# What's the best way to delete a directory?Use rmdir.For example, to delete the directory named foo, type'rmdir foo'.# I want to display a file.Use more.For example, to display the file named foo, type'more foo'.# I want to edit a file.Use vi.For example, to edit the file named foo, type 'vi foo'.# I want to find out Joe's address.Type 'finger joe'.# I want to remove a directory named junk.Type 'rmdir junk'.# What's the command to compact a file?Use compact.For example, to compact the file named foo, type'compact foo'.# What is a directory?A directory is a file that is used to contain files.# What is a file?A directory is a container that is used to contain text,code or files.# What is a rm?Rm is a command that is used to delete files.# What is a search path?A search path is a list of directories that is used bythe csh to search for programs to execute.1.5.2 MORE COMPLEX EXAMPLES% UCWelcome to UC (Unix Consultant) version 3.23To a UC '# '  prompt, please type in your questionsabout the Unix file system in English.To leave, just type a 'AD' or '(exit)'.Hi.How can I help you?# Is rn used to rename files?No, rn is used to read news.To rename a file to another file, use mv.For example, to rename the file named foo to the filenamed fool, type 'mv foo fool'.In this example, UC hypothesized that the user askedthe question in order to get an answer to some under-lying question.
In particular, PAGAN hypothesizedpossible goals of wanting to know how to rename a fileand wanting to know what rn does.
Since the answer tothe direct question is negative, UCEgo realizes that theunderlying oals are not addressed by this answer, and,therefore, attempts to answer them.
This results in theextended answer.# How can I crash the system?I 'm sorry, I cannot tell you.In this example, UC detected a conflict between its goalof helping the user and that of preserving the integrity ofthe system.
It resolved this conflict in favor of the latterComputational Linguistics, Volume 14, Number 4, December 1988 41Robert Wilensky, David N. Chin, Marc Luria, James Martin, James Mayfield, and.
Dekai Wu The Berkeley UNIX Consultant Projectgoals.
However, since it could not accommodate theuser's request, it attempts to be polite by apologizing tothe user.# What does Is -v do?Ls does not have a -v option.Here, the user asks about a nonexistent commandoption.
Actually, the UNIX 'Is' command ignores op-tions that it does not recognize, so the consultant couldhave answered truthfully by saying that 'Is -v' causesone's directory to be listed.
It did not, because PAGANinterpreted the request as a question about the functionassociated with the command.
UCEgo reasons that,since (1) it doesn't know about such an option, and (2)it knows all the options to all simple commands, and (3)Is is a simple command, the user must have a miscon-ception.
It then generates the goals of correcting themisconception, which results in the answer shown.Note that UC uses an open-world hypothesis togetherwith metaknowledge to determine whether it knowssome facts about UNIX.In the next example, two different sessions with twodifferent users are represented.% UCWelcome to UC (Unix Consultant) version 3.23To a UC '# '  prompt, please type in your questionsabout the Unix file system in English.To leave, just type a 'AD' or '(exit)'.Hi.How can I help you?# How can I delete a file?Use rm.For example, to delete the file named foo, type 'rmfoo'.# What does rwho do?Rwho is used to list all users on the network, theusers' tty, the users' login time, and the users' idletime.% UCWelcome to UC (Unix Consultant)version 3.23To a UC '# '  prompt, please type in your questionsabout the Unix file system in English.To leave, just type a 'AD' or '(exit)'.Hi.How I can I help you?# How can I find out the inode of a file?Use Is -i.# What does rwho do?Rwho is like who, except rwho lists all users on thenetwork.Because he asked such a simple question, the first userwas diagnosed by KNOME as being a novice user.
Thisresulted in the answer to his first question being illus-trated by an example, and the extended format of hisanswer to the second.
In contrast, the second userasked about a relatively technical aspect of UNIX.
Thusthis user is likely to be a relatively sophisticated user.As a result, the answer to that user's first question wasnot illustrated with an example.
For the same reason,UC presumes that the second user knows about the'who' command, and takes advantage of this knowledgein generating the response.Note this example also illustrates UC's ability togenerate different ypes of responses.
This example wasable to use a simile format, because the command to begenerated was thought to be sufficiently close to oneUC believes the user to already know.
Formating isdiscussed in section 8.The next example illustrates ome aspects of UC'sability to generate plans.# How do I print Jim's file on the lineprinter?Use lpr.
This plan will not work if you don't haveread permission on Jim's file.In this example, KIP, UC's domain planner, suspectsthat the plan it is suggesting might fail.
This is becausethe plan involves printing someone lse's file, and KIPknows that the lack of appropriate permission in suchcases is a likely source of plan failure.
However,  sinceit does not know a more certain plan, it delivers thesuspect one, but warns the user of the possible problem.A fact about a possible problem with a plan is calleda concern.
Concerns are explicitly represented in UC'sknowledge base.
At this stage, UC's knowledge ofconcerns is fragmentary.
Thus questions above thathave the same structure as this one may not cause anywarnings to be generating simply because the requisiteknowledge is lacking.
Concerns are discussed further inSection 7.2.3 below.# Who is on the system?I 'm sorry, I don't know.
Use users to find out who ison the system.Here, UC fails to answer the user's question, because itdoes not have any way of running UNIX commandsitself.
Its failure causes it to apologize.
UC then formsthe goal of the user knowing how to determine therequested information.
It addresses this goal, whichleads to the second part of the response.2 KODIAKThe knowledge used by UC is represented in KODIAK,as are the various stages of the processing of an utter-ance.
KODIAK (Keystone to Overall Design for Inte-gration and Application of Knowledge) is an implemen-tation of CRT (Cognitive Representation Theory), anapproach to knowledge representation that bears simi-larities to numerous other systems, but especially thoseof Schank (1975), Schubert (1976), Shapiro (1979), andBrachman and Schmolze (1985).
KODIAK differs fromthese systems in significant ways; however, here weshall try to trivialize rather than accentuate these dif-ferences so that the reader can relate our representa-42 Computational Linguistics, Volume 14, Number 4, December 1988Robert Wilensky, David N. Chin, Marc Luria, James Martin, James Mayfield, and Dekai Wu The Berkeley UNIX Consultant Projecttions to more familiar ones.
The reader should consultWilensky (1986) for a more detailed account and justi-fication of this representation system.KODIAK is a relation-oriented rather than object-oriented system.
In an object-oriented system, onethinks of objects as having slots, and the representa-tional weight is carried by such structured objects.
In arelation-oriented system, objects are thought of asatomic elements, and knowledge is captured by therelations in which these objects participate.For example, suppose we want to represent a con-cept like "state change", i.e., an event in which somestate is thought of as changing into another state.Among other things, we would like to capture the factthat any state change has an initial state and a finalstate.
In KODIAK, we would represent this by firstcreating objects corresponding to the concepts STATE-CHANGE and STATE.
Then we would indicate that arelation, say, INITIAL-STATE, holds between eachinstance of a state change and some instance of a state.Similarly, another relation, say, FINAL-STATE, holdsbetween each state change and some state.This description sounds equivalent to saying thatINITIAL-STATE and FINAL-STATE are slots ofSTATE-CHANGE.
For the purposes of this paper, wewill not strenuously resist his interpretation.
However,the underlying philosophy (and actual representation)is different.
In particular, rather than accounting forthe relatedness of STATE-CHANGE and INITIAL-STATE and FINAL-STATE by creating a primitivenotion of slot and asserting that the latter two items areslots in the former, this relatedness is simply attributedto the fact that every STATE-CHANGE participates inan INITIAL-STATE relation and a FINAL-STATErelation.
On the other hand, we think of these relationsas being less closely related to STATE because notevery state participates in these relations.In our terminology, we refer to objects like STATEand STATE-CHANGE as absolutes, and the relationsin which they participate, such as INITIAL-STATE andFINAL-STATE, as aspectuals.
The motivation for hav-ing a more fine grain representation than objects withslots is described in Wilensky (1986).
However, here weshall use a more conventional notation for the sake ofexposition.
For example, we will write the above factsabout state changes using the following notation:\[ STATE-CHANGE l / \Ot OtThis diagram should be interpreted as follows: theindividual instances of the concept STATE-CHANGEalways participate in two relations, one called INI-TIAL-STATE and one called FINAL-STATE.
That is,if we ever encountered an individual of the conceptSTATE-CHANGE, it will be in an INITIAL-STATEand a FINAL STATE relation to something.
The circlesthemselves represent he idea of being in a givenrelation to something.
Thus they correspond mostclosely to the roles of other systems.Of course, there is much more to say about statechanges.
For example, we have not yet expressed thefact that the other argument to both INITIAL-STATEand FINAL-STATE must always be a state.
Also, thestate that is in a INITIAL-STATE relation to a statechange always occurs before the state that is in theFINAL-STATE relation.
STATE-CHANGE itself is atype of event.
We will now describe how such facts arerepresented.We represent individuals of a concept, and subtypesof a concept, using KL-ONE-Iike structured inheritancetechnique.
For example, to represent an instance of astate change, we would create the following structure:I STATE-CHANGE I/ \o/ O/STATE-CHANGEI I/initial-state\final-stateThe instantiate (I) link states that STATE-CHANGE 1 isan individual STATE-CHANGE.
Initial-statel shouldbe interpreted as the assertion that the initial staterelation holds between STATE-CHANGEI and some(as yet unspecified) value; the surrounding circle repre-sents the idea of being in the initial state relation toSTATE-CHANGE1.
We sometimes ay that initial-state l plays the role of the initial-state relation withrespect o STATE-CHANGE1.Note that role-play is a relation between a relationand its use, not a relation between a role and its filler.Indeed, the values to which initial-state and final-stateconnect STATE-CHANGE1 have not yet been de-Computational Linguistics, Volume 14, Number 4, December 1988 43Robert Wilensky, David N. Chin, Marc Luria, James Martin, James Mayfieid, and Dekai Wu The Berkeley UNIX Consultant Projectpicted.
To include these values in our representation,we use the value (V) link.
Thus we would typically findinstances like the above elaborated to include thefollowing:state and final-state must be an instance of a STATE,we would ,draw a constrain (C) link from the circleslabeled initial-state and final-state to a box labeledSTATE, as in the following diagram:I STATE-CHANGEI I/ \initial state final state\(v vCo~CSTATE-CHANGE \]That is, the initial-state relation holds between STATE-CHANGE1 and something called STATE1, and thefinal-state relation holds between STATE-CHANGE1and something called STATE2.
In other terminology,the object at the end of the value link represents thefiller, and the object at the source, the slot.
Presumably,we would also include what other information we knewabout STATEI  and STATE2, namely, what categoriesthey are instances of, etc.We can abbreviate this notation if we have no needfor the showing the slots of an individual, but merelytheir fillers.
We do this by drawing a labeled linkdirectly to the target of the value link.
For example, wecould simplify the above two diagrams by drawing a linklabeled initial-state between STATE-CHANGE1 andSTATE1, and one labeled final-state between STATE-CHANGE1 and STATE2.
Thus it is sometimes conve-nient to draw the following.I STATE-CHANGE1 \[/ \initial-state final-stateThese links would be interpreted as saying that anyobject in a Value relation with a use of initial state orfinal state must be of type STATE.Subtypes are represented using a Dominate (D) linkinstead of an Instantiate (I) link.
The aspectuals arecreated for the subtype that play the role of the aspect-uals of the parent.
These are typically differentiatedfrom those of the parent concept by being subject to afurther constraint.
For example, we can begin to definedying as a kind of state change as follows:,s Z\I STATE-CHANGE \] / \I/ \initial-stalepatientfinal-staleUsually, the aspectuals of a concept are constrained sothat their other argument must be of a certain type.
Forexample, to indicate that the other argument to initial-We have represented DEATH-EVENT as being a kindof STATE-CHANGE in which the initial state is con-44 Computational Linguistics, Volume 14, Number 4, December 1988Robert Wilensky, David N. Chin, Marc Luria, James Martin, James Mayfield, and Dekai Wu The Berkeley UNIX Consultant Projectst.,.
o cate objec, s / \?
I ?EA " EWN I statestrained to be a state of something being alive, and thefinal state to be that of something being dead.In this diagram, we have not bothered to show theconstraints on the aspectuals of STATE-CHANGE.
Ingeneral, we will only show those parts of previousconcepts that are pertinent o the knowledge beingrepresented.
Also, we introduced a new aspectual, dier,which specializes patient.
Patient is inherited from somequite abstract absolute, which ultimately dominatesSTATE-CHANGE, and which has the meaning "eventthat affects an object".
Also, since patient is an aspect-ual of some concept dominating STATE-CHANGE, weshould have specialized it at STATE-CHANGE, say, tostate-change-object.
However, when such specializedversions of relations are not substantially differentiatedfrom their ancestors, we will use the ancestor's nameinstead.
The KODIAK interpreter understands the oc-currence of an inherited relation as being the mostspecific inheritable specialization of that relation, sousing the more abstract name has the same semantics ofusing the more specialized one.One item missing from this diagram is the fact thatthe fellow who dies is the same fellow as the one whowas alive and then was dead.
This fact can be capturedwith the aid of the Equate (=) link.
This is similar toKL-ONE's role chains.
For example, we can improveour representation for dying by creating a state of beingalive and a state of being dead, and then using Equate linksto state the relationships between their components.In the diagram above, we state that the dier ofDEATH-EVENT is the same as the is-alive-object ofsome IS-ALIVE state, and also the same as the is-dead-object of some is-dead state.The Equate links belong to a particular absolute.
Forexample, in the diagram above, the Equate links ema-nating from dier belong to DEATH-EVENT.
The sig-nificance of this fact is that it is true of dying that theremust be an associated event of being alive, but it is nottrue of being alive that there must be a death event.
Werepresent this aspect of Equates by the direction of thearrows in the diagram.Having defined DEATH-EVENT, it is easy to rep-resent a concept like killing, assuming an analysis inwhich killing is a kind of causing in which the thingcaused is a kind of death event.Here we define KILL as being a subtype of CAUS-Computational Linguistics, Volume 14, Number 4, December 1988 45Robert Wilensky, David N. Chin, Marc Luria, James Martin, James Mayfield, and Dekai Wu The Berkeley UNIX Consultant Project\[CAUSAL-EVENT \]0a,}o3\~ / ~ ~ ' ~  patient c a u s ~AL-EVENT.
CAUSAL-EVENT is a concept shown asparticipating in an effect and a cause relation.
KILL isalso shown as participating in a kill-effect and a kill-cause relation.
Because ffect and cause are written onthe links to these relations, we know that kill-effect isjust a specialized version of the effect relation, and thatkill-cause is a specialized version of cause.
To differen-tiate KILL from CAUSAL-EVENT, we need to cap-ture the fact that the effect of a kill is a death event.
Theadditional semantics of kill-effect (and thereby, ofKILL) come from the constrain link on kill-effect.
Thissays that the effect of a KILL is a DEATH-EVENT.Finally, we use an Equate link to state that the personkilled is the same one as the one who died.Of course, many parts of the meaning of these simpleconcepts have been omitted from these diagrams.
Forexample, we have not stated any information abouttime, such as the fact that the initial state precedes thefinal state.
In general, the diagrams presented in thispaper have been simplified to present the crucial knowl-edge needed without being even more complex thanthey already are.KODIAK has additional features that are not de-scribed herein.
The reader is referred to Wilensky(1986) for a more complete description.
A summary ofall the KODIAK features used in this report is found inthe legend in Figure I.Note that the legend contains one additional abbre-viation, namely, that drawing a link bearing the name ofa relation between two categories is interpreted asstating that the individuals of the domain categoryparticipate in that relation with some member of therange category.2.1 UNIX KNOWLEDGE IN UCThe KODIAK knowledge representations u ed in UCinclude several rather general notions, such as statechange, goal, and action, plus many specific facts aboutUNIX.
The complete collection is too lengthy to includehere.
(UC is currently constructed from approximately200 KODIAK diagrams, consisting of about 1,000 ab-solutes and 2,000 relations.
While a scope of a diagramis to some degree arbitrary, diagrams roughly corre-spond to definitions of meaningful entities, like thedefinition aparticular command.)
Some more importantconcepts used in modeling the domain will be explainedin the individual sections of this report.To facilitate understanding the KODIAK diagramsthat follow, consider the representation that the UC'sknowledge base contains about the UNIX rm com-mand.
This is used to delete a file named by itsargument.
The following diagram shows how knowl-edge about deletion is represented in UC.The central node in this diagram is DELETE-EF-FECT.
DELETE-EFFECT is shown as being a kind ofSTATE-CHANGE that causes a something to go fromexistence to nonexistence.
(The notions of existenceand negation do play a special role in KODIAK, butalso exist as ordinary states, as is the case here.
Fromthe point of view of this example, these are just like anyother states.
)DELETE-EFFECT specifies the minimal deletionevent.
For example, it says nothing about the cause ofsuch an event, or who the actor of it may be.
In UC inparticular and in CRT in general, such state changes arethe bases from which we build more elaborate concepts.For example, the action of deleting something is repre-sented as an action that causes omething to be deleted.46 Computational Linguistics, Volume 14, Number 4, December 1988Robert Wilensky, David N. Chin, Marc Luria, James Martin, James Mayfield, and Dekai Wu The Berkeley UNIX Consultant ProjectREPRESENTATIONCONCEPTOCONCEPTn , n an integer\[ CATEGORY \]-a---~ ?\[ CATEGORY1 -D--> \[ CATEGORY2\[INSTANCE \]-I--, CATEGORYO-C-~ \[ CATEGORY 1O-V---> OBJECTO- =--> OCATEGORY -rel-->OCATEGORY1 trel---> CATEGORY2NODESMEANINGan absolutea relationCONCEPTn is an instance of CONCEPT\]ILINKSO is an aspectual, i.e., a relation in which \[ CATEGORY \] participates\[CATEGORY2\]dominates CATEGORYI \].INSTANCE is an instance of \[ CATEGORY \].The argument to O is constrained to be of type \[CATEGORY .The value of the argument to O is ~ .The first aspectual is constrained to have the samevalue as the second.O is an aspectual of CATEGORY, and O specializesrel, an aspectual of some conceptdominating CATEGORY.Each member of CATEGORYI participatesin re!
with some member of CATEGORY2.Figure 1.
KODIAK LegendI TRANSITIVE-ACTION It / \  ostate-obj C \[~.
~at,eot I ?
~ ~ ' 1  I ~'~'~-~''~~'Io~l/ t /state-oN C // del-objectI NoT-E?ISTS ~ cComputational Linguistics, Volume 14, Number 4, December 1988 47Robert Wilensky, David N. Chin, Marc Luria, James Martin, James Mayfield, and Dekai Wu The Berkeley UNIX Consultant Project\] EXECUTE-UNIX-COMMAND I~D I UNIX'COMMAND II\[ EXECUTE-UN IX-RM-COMMAND \ ] / /~r~ / /  FDELETE.FILE.EFFECTexecute command ~ ~ | .
.
.
.
C" Iplan goals aei-ooje~ / name/ " , ,\[ RM-FORM.AT \] / /nam~ aN~rrg = C /~ ~ I  ~This is encoded by showing DELETE-EFFECT to bethe effect of DELETE-ACTION.Using names like DELETE-EFFECT may be some-what misleading.
In particular, DELETE-EFFECTis not required to be the effect of anything--whileDELETE-ACTION is defined as having DELETE-EFFECT as its result, this statement imposes a require-ment on DELETE-ACTION, not on DELETE-EF-FECT.
We call such concepts effects rather thanevents, say, to emphasize that we me, an to include onlythe most essential elements of the concept, i.e., just aspecialized state change.DELETE-FILE-EFFECT is a specialized version ofDELETE-EFFECT in which the object deleted is con-strained to be a file.
DELETE-ACTION is correspond-ingly specialized to DELETE-FILE-ACTION.
It is alsoshown as being a kind of TRANSITIVE-ACTION.
Thisis a very general category denoting any action that actsupon an object.This sort of structure, in which there exists paralleleffect and action hierarchies, with the effect hierarchycarrying most of the semantic weight, is typical of therepresentation that appear in UC.The figure above connects this general knowledgeabout deletion with knowledge about UNIX.Here we state how to achieve a DELETE-FILE-EFFECT.
This accomplished by the node labeledPLANFOR2, which points to EXECUTE-UNIX-RM-COMMAND and to DELETE-FILE-EFFECT.
APLANFOR indicates that something is conceptualizedas a plan for a particular goal (PLANFORs are dis-cussed below).
In other words, this notation representsthe particular fact that the 'rm' command (i.e., thecommand whose name is "rm") is used to achieve theeffect of deleting afile.
Again, this structure is typical ofthat seen in UC--most of the information about acommand is represented asinformation about he use ofthat command; the intended function of the use of acommand is represented by a planfor between a noderepresenting the use of the command and some effect.The rest of the diagram specifies the format of thecommand.
In particular, the Equate link specifies that,to delete a particular file, its name must be the same asthat of the argument supplied to 'rm.
'A TOUR THROUGH UCThe following sections describe the components of UCin more detail.
To aid in understanding how thesecomponents contribute to the processing of an individ-ual utterance, we show how each section processes theexample sentence "Do you know how to print a file onthe imagen \[a kind of laser printer used at our site\]?"
Inmost cases, a module is capable of doing a great dealmore than is required for this example, and such capa-bilities are attested to.
However, the example is usefulfor illustrating the kind of processing that is performedfor a typical request.In order to produce a paper of reasonable l ngth, we48 Computational Linguistics, Volume 14, Number 4, December 1988Robert Wilensky, David N. Chin, Marc Luria, James Martin, James Mayfield, and Dekai Wu The Berkeley UNIX Consultant ProjectACTION \[HYPOTHETICAL / ~Dasked-for \]~ ~ "~ /, I / ~ N I  x / \[PRINT-ACTION II QUESTION II I / ~ " ~  fl'w~'hat_is / I /I' "N,,,, N ,/I / RINTACTI?N?i \I" \[ fact.
~ / cause \kno\[wer ""~ ~ /j~/ \   NOWl I *ON61PRINT-EFFECT \]IMAGENprint-effect print-object / II / ~t -dest\[ PRINT-EFFECT01Figure 2.
ALANA's output for "Do you know how to print a file on the Imagen?
"reduced considerably the description of some of UC'smodules.
We have focused on those processes thatcontribute to UC's agentlike nature, while some moreconventional modules, such as the conceptual nalyzer,are mentioned only in passing.
References are given todescriptions of these neglected components, whichhave all appeared elsewhere in print, should the readerfind the account herein dissatisfying.3 THE ANALYZERA conceptual analyzer maps a string of words into ameaning representation.
ALANA (Augmentable LAN-guage Analyzer), the conceptual nalyzer for UC, takesas input a sentence typed by a user, and builds aconceptual representation using the KODIAK knowl-edge representation language.
ALANA constructs theprimal content of the input utterance.
The primal con-tent is the interpretation that can be computed fromgrammatical and lexical knowledge; it is generallyrather abstract.
ALANA's results are further inter-preted and refined by other parts of the system, such asthe concretion mechanism, to produce an actual con-tent, and the goal analyzer, to produce a representationof the intentions underlying the utterance.ALANA is a descendent of PHRAN (Wilensky andArens 1980), the front end natural language componentfor the original UC (Wilensky, Arens, and Chin 1984).Like PHRAN, ALANA reads the user's input andforms a concept hat the other UC components can usefor their tasks.
Also like PHRAN, the ALANA uses asits primitive knowledge unit the pattern-concept pair,which relates a natural anguage structure to a concep-tual structure.
UC has a total of 476 patterns and knows284 words.ALANA differs from PHRAN in its generality.ALANA generalizes on the idea of pattern concept pairanalysis, while making it easier than it was withPHRAN for a knowledge adder to add new patterns tothe system.
Since a more detailed description ofALANA can be found in Cox (1986), we will notelaborate on it here.
Instead, we merely show in Figure2 the output produced by ALANA upon reading thesentence "Do you know how to print a file on theimagen?
".This diagram may be interpreted as follows: Theentire request is summarized as ASKl l ,  i.e., someasking event.
What is asked for is verification of someitem, QUESTIONll, whose content is KNOW3, i.e.,an instance of knowing.
The knower of the item is UC,and the fact is ACTION6.
ACTION6 is interpreted assomething that is the cause of a printing action PRINT-ACTION0, which is itself an action whose effect(PRINT-EFFECT0) is to cause a file (FILE6) to beprinted on an imagen printer (IMAGEN0).Some of the nodes in this diagram point to a nodelabeled HYPOTHETICAL.
This is a tentative conven-tion used to indicate that the knowing and printingevent, etc., are not real events, but merely hypotheticalones.4 THE CONCRETION MECHANISM4.1 INTRODUCTIONA concretion inference is a kind of inference in which amore specific interpretation of an utterance is madeComputational Linguistics, Volume 14, Number 4, December 1988 49Robert Wilensky, David N. Chin, Marc Luria, arames Martin, James Mayfield, and Dekai Wu The Berkeley UNIX Consultant Projectthan can be sustained on a strictly logical basis (Norvig1983, Wilensky 1983).
Examination of contextual c uesprovides the means to determine which of many possi-ble interpretations are likely candidates.
An example ofa simple type of concretion inference occurs in under-standing that "to use a pencil" means to write with apencil, whereas "to use a dictionary" means to look upa word.Concretion differs classification such as that ofKL-ONE (Schmolze and Lipkis 1983, Brachman andSchmolze 1985) in that a concretion inference may beincorrect.
Ordinarily, "to use a pencil" implies writing;however, in a particular context, it may refer to prop-ping a door open with a pencil.
Nevertheless, in theabsence of compelling evidence to the contrary, thenatural interpretation is writing.
Thus concretion maybe thought of as defeasible classification.A process that performs concretion is called a con-cret ion mechan ism.
A concretion mechanism attemptsto find clues in a set of general concepts to generateconcepts that are more specific.
Writing, for instance, isa specific type of using, in which the tool being usedmay be a pencil.
The use of such a mechanism permitsa straightforward approach to manipulating hierarchicalknowledge structures.
The initial interpretation of anutterance may include concepts too general for theutterance to be considered understood.
Such generalconcepts embody the common features of their descen-dent concepts, but for some reason insufficiently spec-ify the meaning of the utterance.
Thus the concretionmechanism is responsible for making an appropriateinterpretation of a concept by selecting one of itssubconcepts, found lower in the hierarchy.In deciding when concretion operations hould beperformed, it is critical to consider how specific aconcept's representation must be to be understood.Different levels of categorization are considered ade-quate from situation to situation.
For instance, it isperfectly acceptable in most circumstances to leave theinterpretation f"eating" as "eating some food".
How-ever, in a context involving picnics, a more specificinterpretation is likely to be made, namely, that theeating involved paper plates, that people sat on theground or at a picnic table, that the food was of a certaintype, and so forth.
In general, in cases where a morespecific category than usual is requisite, often somefeature of the prototype of the supercategory is vio-lated, resulting in a higher probability of selecting asubcategory where this feature is accommodated.It is important that the mechanism be able to recog-nize from a wide variety of clues when there is sufficientevidence to concrete, as well as when an ambiguityneeds to be resolved.
A uniform method of representingthe rules by which a concretion may be made is re-quired.
Naturally, wrong inferences can occasionally bemade, and some means must therefore be provided tofind and correct them when contradictory facts arelearned.4.2 CONCRETION IN UCAs mentioned previously, our theoretical posture is thatconcretion isbut one of a number of inference processesthat can be accomplished by a single mechanism.
How-ever, in UC, for reasons of efficiency, and for pragmaticadvantages, a separate concretion mechanism was im-plemented (by Dekai Wu).
This mechanism currentlydoes rather straightforward classification.The mechanism concretes by using informationabout inheritance and value constraints, as well as byconsidering relation information between concepts.
Aconcept represented as an instance of a category ispassed to the concretion mechanism.
Its eligibility formembership in a more specific subcategory is deter-mined by its ability to meet he constraints imposed onthe subcategory by its associated relations and aspec-tual constraints.
If all applicable conditions are met, theconcept becomes an instance of the subcategory.
At thesame time, the relations in which the concept partici-pates may be concreted to reflect the more specificrelations of the new category of which it is inferred to bea member.4.3 EXAMPLEConsider the example "Do you know how to print a fileon the Imagen?".
The subpart "print a file on theImagen" is parsed into the representation shown inFigure 3.
Parts of the representation f printing areshown in Figure 4.Besides the printing of the contents of a computerfile, PRINT-EFFECT is in principle applicable to othertypes of printing, such as printing a newspaper or abook.
The concretion mechanism checks each of themore specific oncepts dominated by PRINT-EFFECT,searching for one whose constraints can be satisfied bythe input.
It finds PRINT-FILE-EFFECT, whose onlyadditional constraint is that its print-object must be aI ACTION \[Dt i PRINT ACTION I/ IIMaO N1I PRINT'ACTION0 \] i ~print-file // I IMAGEN0 II print-effect \[ '~,I PRINT-EFFECT \[ \ \[ /\ \[ print-dest / /1PRIN -EFFEC 01Figure 3.
Representation f "print a file on the Imagen"50 Computational Linguistics, Volume 14, Number 4, December 1988Robert Wflensky, David N. Chin, Marc Luria, James Martin, James Mayfleld, and Dekai Wu The Berkeley UNIX Consultant Project\[ STATE CHANOE Ii PRINT.EFFECT~.t ~dest DC ntoctPRINT-ACTIOND\[LASER-PRINT-ACTION /?
)laser-print-effect J I 'MA?EN-PRINT-~-FFECT 1print-destpRINT-FILE-EFFECT\[effect'S ly ?I LASER-PRINT-~ILE-EFFECT Iprint-effectDCOMPUTER-PRINTER JDC LAsERPRINTER'Tprint-file-dest DI ~ I~AOEN\[laser-printer-dest JIMAGEN-PRINT-ACTION \]Figure 4.
Some Knowledge About Printingfile.
Since PRINT-EFFECT0 is in print-object relationwith the object FILE6, which is indeed an instance offile, the process can descend to this node.
The concre-tion process will continue until it can concretize nofurther.Of course, it is perfectly plausible just to precludefrom UC on engineering rounds interpretations ofwords that do not occur in the UNIX domain.
As wesuggested earlier, it is our preference not to do so, sincewe wish to address, rather than finesse, fundamentallanguage issues.
However, doing so would not reallyeliminate the need for concretion.
Even if we do notinclude concepts of non-computer printing in ourknowledge base, we would still have many differentkinds of printing, e.g., printing ASCII files versusbinary files or printing on the lineprinter versus the laserprinter.
A query about each of these kinds of printingrequires a different response, although the term"printing" applies to all of these.
A system like UCneeds to concrete the concept of printing in general tothe particular kinds of printing that it knows about, inorder to  find the knowledge needed to answer thequestion.
Thus eliminating interpretations that lie out-side the domain simplifies the problem somewhat, but itdoes not change its essential nature.In general, when concretion occurs, some node isreclassified as being an instance of a more specificcategory, and, in addition, the relations predicatedabout hat node are also reclassified.
For example, herewe concretize PRINT-EFFECT0 to an instance ofPRINT-FILE-EFFECT.
At the same time, we shouldconcrete the relation print-object predicated about it toa use of the more specific relation print-file-object.Similarly, print-dest is concreted to print file-dest.Computational Linguistics, Volume 14, Number 4, December 1988 51Robert Wilensky, David N. Chin, Marc Luria, James Martin, James Mayfield, and Dekai Wu The Berkeley UNIX Consultant ProjectI LAs .-P.I ++- c+IONII P 'N+ //'~  print-file-objectimagen-print-effect ~ f ~ .
~ g  eYI LI IMAGEN \]Figure 5.
Result of Concretizing "print file on the Imagen"/I /IMAGEN0 IContinuing in this fashion, the mechanism can movefrom PRINT-EFFECT to LASER-PRINT-EFFECT,and finally to IMAGEN-PRINT-EFFECT, since theprint-dest of the input is IMAGEN0, which is an in-stance of IMAGEN.
At the same time, the relationprint-dest is concreted to imagen-dest.
In parallel withthis concretion, the node PRINT-ACTION0 gets con-cretized to an instance of IMAGEN-PRINT-ACTION.The final result is shown in Figure 5.5 THE GOAL ANALYZEROnce an utterance has been converted to a KODIAKrepresentation by ALANA, and has been further re-fined by the concretion mechanism, this internal repre-sentation is passed to PAGAN (Plan and Goal ANa-lyzer).
PAGAN's task is to determine what goals thespeaker is addressing in making the utterance.
Forexample, when given a representation f the utterance1.
Do you know how to print a file on the Imagen?asked by a naive user, PAGAN should infer that theuser was using the utterance to address the goal ofknowing how to print a file on the Imagen.
Note thatPAGAN is not responsible for detecting oals that areheld by the speaker, but that are not conveyed by thespeaker's utterances.
This problem is addressed by theego mechanism and by the planner.To successfully do goal analysis, at least two ques-tions must be answered.
The first concerns the utter-ance in isolation.Q1.
What kind of act does this utterance constitute?This question has traditionally fallen under the rubric ofspeech-act theory (Austin 1962, Searle 1969).
For ex-ample, (1) potentially has both a direct and indirectinterpretation, which PAGAN must choose between.The second question agoal analysis mechanism ustanswer examines the role of the utterance in conversa-tion.Q2.
How does this utterance relate to other utter-ances?By virtue of being an action, an utterance always occurswithin a context.
This context includes such diversefactors as the identities of the speaker and of theaudience, the social relationship between them, thephysical locale, the task the conversation is supple-menting if any, and so on.
One feature of this contextthat is salient to goal analysis is the presence of con-ventional, multi-utterance s quences.
Consider the ex-change:2.
Do you have write permission on the parentdirectory?3.
Yes.The ability to understand the full meaning of (3) iscontingent on the realization that it relates directly andconventionally to (2).
Thus PAGAN will require knowl-edge of such sequences to correctly determine the goalunderlying utterances such as (3).52 Computational Linguistics, Volume 14, Number 4, December 1988Robert Wilensky, David N. Chin, Marc Luria, James Martin, James Mayfield, and Dekai Wu The Berkeley UNIX Consultant Project5.1 KNOWLEDGE REPRESENTATION FOR PAGANA planfor is a relation between agoal and a sequence ofsteps (called a plan) that constitutes a possible methodof achieving that goal.
All PAGAN's knowledge ofconversation is stored as planfors.Planfors provide a means to address the questionsposed above.
First, indirect speech acts can be ex-pressed as planfors.
For example, the generic indirectspeech act underlying (3) can be expressed as:PLANFOR1GOAL: Speaker ask hearer how to perform taskPLAN: Speaker ask hearer whether hearerknows how to perform taskSecond, planfors provide a means to express conven-tionalized relationships between utterances.
Utterance1 and its answer can be represented as:PLANFOR2GOAL: Speaker know how to perform taskPLAN: Speaker ask hearer how to perform taskHearer tell speaker how to perform task(In general, steps of a plan that are to be executed bysomeone other than the planner are interpreted as theplanner waiting for that event o happen.
)Representing both speech act knowledge and conver-sational knowledge with planfors has two advantages.First, it allows a single mechanism to handle the proc-essing of both phenomena.
The goal analysis mecha-nism described below does just this.
Second, it allowsthe two forms of knowledge to be combined into a singlestructure.
For example, the two preceding planfors canbe combined to express both the indirect speech act andthe question and answer sequence.PLANFOR3GOAL: Speaker know how to perform taskPLAN: Speaker ask hearer whether hearerknows how to perform taskHearer tell speaker how to perform taskThe KODIAK representation f a planfor is shown inFigure 6.
Figure 7 depicts PLANFOR3 in its KODIAKform.
(It is called PLANFOR34 in the knowledge base).IPLAN ORII STAT -OR-EV NT1 //a , C /Figure 6.
Definition of a PlanforIt is a planfor whose goal is to know a fact which is itselfa plan for some goal.
(The names of the aspectuals notused to make a predication have been omitted from thediagram.)
The plan of PLANFOR34 is for the personwho wants to know this action to ask someone whetherthat person knows the action sought, and then, to havethat same person tell the asker what was desired.Note that planfors do not represent fundamentalknowledge of causality.
There is usually a causal rela-tionship between a plan and a goal that are connected bya planfor.
However, the planfor itself does not representthe causality.
What a planfor does represent is a notionof typicality.
It indicates that its plan is one that istypically or conventionally used to achieve its goal.
Forexample, the UNIX 'rm' command may cause a file tobe deleted.
It may also cause the disk arm to be moved.It would be a mistake though to say that 'rm' should beconnected to the goal of moving the disk arm by aplanfor elation; 'rm' is not typically used to move thedisk arm.
On the other hand, 'rm' should be connectedto the goal of deleting a file by a planfor elation, sincethis goal is what 'rm' is typically used for.Traditional approaches to dialog understanding havefocused on the process of plan inference.
Under thisapproach, utterances are viewed as steps of plans.
Suchplans may themselves be parts of higher-level p ans, andso on.
Allen and Perrault (1980) developed a system thatexemplifies this approach.
Their system handled irectand indirect speech acts by plan analysis.
Carberry(1983) extended this paradigm to deal more thoroughlywith domain plans.
Litman and Allen (1984) used thenotion of metaplans (Wilensky 1983) to facilitate thecomprehension f subdialogs.
Grosz and Sidner (1985)pointed out the need for attentional knowledge in un-derstanding discourse.
One problem that has persistedin the literature is an inadequate representation f therelationship between goals and plans.
Planfors providesuch a representation.Planfors allow a goal analysis mechanism tocombinecertain inferences that should be kept together.
First,inferences about plans may be made at the same time asthose about goals.
This is in contrast with systems uchas Wilensky's PAM system (1983) that use separaterepresentations for inferring plans and goals.
Second,inferences about plan recognition and inferences aboutintended response recognition may be combined byincluding the intended response in the plan and associ-ating this entire plan with a single goal.
This is incontrast with systems uch as Sidner's (1985) that firstdo plan recognition and then worry about what responsewas intended.
The ability to do both kinds of inferencesimultaneously conforms to the intuition that no extraprocessing is required to determine, for example, thatan answer is required once the realization is made thata question has been asked.
Finally, planfors allowinferences about linguistic goals and about domain goalsto be handled by a single inference ngine.
The separa-tion of goal analysis into linguistic goal reasoning andComputational Linguistics, Volume 14, Number 4, December 1988 53Robert Wilensky, David N. Chin, Marc Luria, James Martin, James Mayfield, and Dekai Wu The Berkeley UNIX Consultant ProjectKNOW34 I/ \knower factPLANFOR34 \] plan\[ PLANFOR--\]\[plan-I EVENT-SEQUENCE3!O/next event event-stepI EVENT'SEQUENCE34 I\v nt-st/ \[ ~ ~ teller ~hst~ner /= "N~ / proposition~ / \ [  QUESTION341 listenerwhaiist  ow33.i ~ kn?~ e//r ~fact )~=Figure 7.
A plan for knowing is to ask if the hearer knows.task goal reasoning (cf.
Alien, Frisch, and Litman 1982)is unnecessary, since the only difference between thetwo is the type of action that may comprise plan steps.5.2 GOAL ANALYSISWhen knowledge of goals and plans is represented withplanfors, goal analysis is the task of matching therepresentations produced by the language analyzeragainst he steps of plans stored in memory.
The goalheld by a speaker in making an utterance is then the goalthat is associated with the matched plan via the planforrelation.In the absence of any previous conversational con-text, an utterance to be analyzed is compared with thefirst plan step of each planfor that PAGAN knowsabout.
When a match is found, the corresponding goal istaken to be the goal the speaker had in mind in makingthe utterance.Several phenomena complicate this view of goalanalysis.
First, a speaker may intend a single utteranceto be a part of more than one plan.
For example, (1) isa plan for the goal of knowing how to print a file.Achieving this goal may in turn be part of a plan foractmdly printing a file.
To handle such situations,PAGAN must apply the matching process recursivelyon each inferred goal.
This matching process is repeateduntil no further higher-level goals can be inferred.Second, preceding conversational events may set upexpectations in relation to which an utterance is de-signed to be understood.
For example, (3) cannot bereadily interpreted when viewed in isolation.
However,if it is used in response to a question such as (2), itsinterpretation is clear.
Two additions must be made tothe matching algorithm to handle this and similar cases.First, before matching the utterance to plans in theplanfor knowledge base, the utterance must be matchedagainst the next step of any active planfor (i.e., anyplanfor already inferred but not yet completed).
In thisexample, the representation of (3) would be matchedagainst he second step of the question and answer planstarted by (2) to determine if it is a response to thequestion.
Second, when a match with a new planfor is54 Comput~ttional Linguistics, Volume 14, Number 4, December 1988Robert Wilensky, David N. Chin, Marc Luria, James Martin, James Mayfield, and Dekai Wu The Berkeley UNIX Consultant Projectfound, PAGAN may also need to infer that the speakerhas adopted the matched planfor.
Suppose UC says (2)to the user.
Here, UC is initiating a plan for the goal ofknowing whether the user holds the indicated permis-sion.
But, at the moment, this goal is held only by UC;it is reasonable that the user might not address the goalat all.
For example, the user might reply4.
Does it matter?instead of answering the question.
If the user's responsecan be construed as an answer to the question, we saythat the user has adopted the planfor, and we may thenexpect any additional steps in the plan to be pursued bythe user.Third, PAGAN may find more than one planfor in itslong-term memory that matches the utterance.
This iscalled planfor ambiguity.
Planfor ambiguity is handledin one of two ways.
If an alternative matches anexpectation, as described in the previous paragraph,then that alternative is chosen as the correct interpre-tation.
If no expectation is matched, PAGAN tries toreject an alternative as implausible.
A planfor is implau-sible if PAGAN believes that the user believes that itsgoal already holds, if its goal does not appear to lead tosome appreciable benefit for the user, or if PAGANbelieves that the user believes that one of its plan stepscould not be achieved.5.3 PROCESSING OVERVIEW1.
This utterance wasn't expected.2.
This utterance wasn't an adopted plan.3.
Matching ASK0 pattern against ASK11.4.
Could not match KNOW23 pattern to ACTION6because of category KNOW.5.
Match failed--try the next one.6.
Matching ASK34 pattern against ASK11.7.
Match found.8.
Matching ASK39 pattern against ASKI 1.9.
Match found.10.
Attempting to resolve ambiguity in the interpreta-tion of ASK11.I 1.
The alternatives are: KNOW34 KNOW39.12.
Trying to determine whether KNOW34 was ex-pected.13.
KNOW34 was not expected.14.
Trying to determine whether KNOW39 was ex-pected.15.
KNOW39 was not expected.16.
The goal KNOW39 is implausible, since the speakerprobably believes that it already holds.17.
ASKll  is explained by the goal KNOW34.18.
Creating new HAS-GOAL node: HAS-GOAL-ga0.19.
Returning oal KNOW-ga0.Figure 8.
Trace of PAGAN's Processing of "Do you knowhow to print a file on the Imagen?
"At PAGAN's core is a matching program that matchestwo KODIAK structures against one another.
Twostructures are said to match if they are isomorphic (i.e.,they have the same link structure) and each pair ofcorresponding odes matches.
For two nodes to match,one must be equal to or an ancestor of the other.
Forexample, Imagen would match Imagen or laser printer,and laser printer would match Imagen, but Imagenwould not match Laserwriter.PAGAN first tries to determine whether the utter-ance was expected.
This is done by matching therepresentation f the utterance against hose plan stepsthat have been inferred but not yet witnessed.
Suchexpectations are stored in a separate structure to speedthe matching process.
Failing this, PAGAN attempts tomatch the representation f the utterance to the firststeps of planfors stored in memory.
If a single suchmatch is found, this planfor is copied, forming a newplanfor with the observed utterance as its first step.
Ifmore than one planfor is found to match, the resultantambiguity is resolved either by matching its goal to anexpected action or by consulting the user model todetermine whether that goal and plan are plausible givenwhat is known about the user.5.4 EXAMPLEThis section traces the processing performed by PA-GAN to handle Utterance 1.
The input to PAGAN is thestructure built by the analyzer from this utterance andrefined by the concretion mechanism.
A trace of PA-GAN as it processes this structure is shown in Figure 8.The first step performed by PAGAN is to determinewhether the utterance is the continuation of a conver-sational plan already in effect.
For this to be the case,there would need to be some previous dialog to providethe necessary context.
This dialog would take one oftwo forms.
It might be a plan that UC believed the userto be pursuing before the current utterance was encoun-tered.
Alternatively, it could be a plan introduced byUC that the user has adopted, that UC believes the userto be pursuing only after witnessing the current utter-ance.
Since there is no previous context in the examplewe are tracing, neither of these possibilities i found tohold (1-2).Next, PAGAN tries to match the utterance againstthe first steps of plans in its planfor knowledge base.The first possibility is compared with the Input Struc-ture 3, but one pair of corresponding odes is found notto match (4-5).
The second possibility, one that doesmatch the utterance, is then compared with the InputStructures 6-7.
This planfor corresponds tothe indirectinterpretation f the utterance.
This is the planfor that isshown in Figure 7.
A third possibility, corresponding tothe direct interpretation f the utterance, also matchesthe Input Structures 8-9.
An attempt o resolve thisambiguity is now made (10-11).
Since neither goalmatches an expected goal (12-15), the planfors areexamined for plausibility.
The direct interpretation isComputational Linguistics, Volume 14, Number 4, December 1988 55Robert Wilensky, David N. Chin, Marc Luria, James Martin, James Mayfield, and Dekai Wu The Berkeley UNIX Consultant Project\[ HYPOTHETICAL.\]/ I / ~ / I  i PLANFOR.ga~.\] planner goal /:r -I  OW-gaOl~ plan goalsactor ~ , , ~  /I AcTioN71 (PRINT- FF C 0 (ItIAC!lONIFigure 9.
PAGAN Outputdiscarded, because the user model indicates that it islikely that the user knows that UC knows how to printa file on the Imagen (16).
(The contribution of the usermodel is described in Section 6.)
Thus the planforrepresenting the indirect interpretation is selected (17).Once the utterance has been matched to the first stepof this planfor, an instance of a PLANFOR is createdwith the goals determined from the input.
In addition,an instance of the HAS-GOAL-STATE is built (18).
Theplanner of this state is the user, and the goal is the goalof the PLANFOR.
This HAS-GOAL represents thegoal that UC believes the user had in mind in making theutterance, and is returned by PAGAN as its result (19).It is shown in Figure 9.In this figure, note that PAGAN has created a nodelabeled ACTION7, whose actor is the user.
This repre-sents the inference made by the goal analyzer that, if auser wants to know an action to achieve some goal, thenthe user intends to be the actor of that action.6 ThE EGO MECHANISM6.1 UCEGOUCEgo is the component of UC that determines UC'sown goals and attempts to achieve those goals.
Theinput to UCEgo are the user's statements a interpretedby UC's conceptual analyzer and concretion mecha-nism, and the user's goals and plans as inferred by UC'sgoal analyzer.
UCEgo draws on the UNIX plannercomponent of UC to produce plans for doing things inUNIX.
It passes the results to UC's expression mech-anism, which prepares the conceptual information forgeneration i to natural language.The processing in UCEgo can be divided into twomain phases: goal detection and plan selection.
In goaldetection (Wilensky 1983), UCEgo considers the cur-rent situation and detects appropriate goals for UC.
Theplan selection phase of UCEgo takes UC's goals andtries to produce a plan for satisfying them.
The processof executing the plan normally results in a collection ofconcepts that are to be communicated to the user.UCEgo also includes an explicit user model, whichencodes the user's knowledge state for use in goaldetection and answer expression.
Each of these sub-components is described in greater detail below.
A moresubstantial description of these subcomponents is de-scribed in Chin (1988).6.1.1 THEMES AND GOALSIn UCEgo, goal detection is implemented by if-detecteddemons, if-detected emons are similar to the implica-tion rules found in many semantic network type sys-tems.
If-detected emons contain two subparts, a de-tection net and an addition net.
Both of these arenetworks of KODIAK concepts.
Whenever the detec-tion net of an if-detected emon matches what is inUC' s memory, the addition et of the if-detected demonis copied into UC's memory.
The detection and additionnets may share concepts, that is, share nodes in theirnetworks.
Here, the concepts that match the detectionnet are used in place of the corresponding concepts inthe addition et.
Thus all the links in the addition et arepreserved inmaking the copy, but some of the absolutesare from the result of the match.
As described below,the matching process, which is somewhat different fromthat used by the goal analyzer, allows certain nodes tomatch instances of the concepts they specify.
There-fore, these nodes function like the typed variables foundin other systems.When used in goal detection, the detection et of an56 Computational Linguistics, Volume 14, Number 4, December 1988Robert Wilensky, David N. Chin, Marc Luria, James Martin, James Mayfield, and Dekai Wu The Berkeley UNIX Consultant Project\[OC.ASOOAL3?/ J' J I- s ??A'?
?I/ sP ?1 / /  \ // plan goa l \  \/ / \ / \/ need / plannergo/al acto/r "~ ~ \/ 2 \//he lper  ._.__......~ PERSON4?
I/ /_helperFigure 10.
If-detected Demon for Adopting the User's Goalof Knowing.if-detected demon represents he situation in which thegoal should be detected, and the addition net of theif-detected demon contains the goal.
Figure 10 shows anif-detected emon used in goal detection.
This if-de-tected demon encodes the information that if UC hasthe goal (UC-HAS-GOAL3 in the diagram) of helping(HELP1) someone (PERSON4), and that person has thegoal (HAS-GOAL0) of knowing something, then a planfor helping that person is for UC to satisfy (SATISFY1)the person's need to know.Figure 10 shows an if-detected emon with an inter-secting detection and addition net.
In these diagrams,the detection et is designated by unlabeled arrowscoming into the doubled circle labeled "if-detected".The net includes all those concepts plus all children ofthose concepts.
The addition net is composed of thoseconcepts pointed to by the if-detected double circle plusall their children.
In the figure, the detection et con-sists of UC-HAS-GOAL3, HAS-GOAL0, and theirchild nodes.
The addition net consists of PLANFOR3plus all its child nodes.
(UC-HAS-GOAL is a subtype ofHAS-GOAL in which the planner is constrained to beUC, thus obviating the need to specify UC as theplanner in each demon.)
Thus, when PAGAN hasinferred that the user wants to know something, and UChas the goal of helping the user (a recurrent goal thatarises from UCEgo's computer-consultant-role theme),UCEgo will detect he goal of satisfying the user's goalof knowing.The use of demons in UCEgo is intended to representits procedural knowledge of what to do in particularsituations.
For example, while a planfor structure usedin both UCEgo and PAGAN might encode that someplan is appropriate for some goal, a demon is needed tocause UC to intend to use that plan in a situation inwhich that goal is present.
Thus demons represent theactions to be taken in a given situation, although boththe situations and actions are described in declarativeKODIAK format.To capture generalizations effectively, the number ofdemons hould be kept to the minimum and as much aspossible should be represented asdeclarative KODIAKknowledge.
For example, it is possible to have onedemon for each situation in which a particular goalsuggests adopting a particular plan; then we might beable to dispense with representing knowledge in plan-fors.
However, doing so would not capture the gener-alization common to all these situations, namely, thatexpressed by the demon in Figure 10.While we have attempted to keep the number ofdemons mall, this version of UC certainly does not goas far as we would like in this direction.
There are about70 demons in the current version.
We expect that acareful examination ofthem could result in reducing thisnumber somewhat making the program more declara-tively based.UCEgo needs a complex control structure, because ithas more varied tasks to perform than most of the otherparts of UC.
Indeed, to accommodate i sneeds, UCEgouses a slightly different matching algorithm than someof the other components.
In particular, the questionmarks in the diagrams are significant o the demoninterpreter during both matching and copying.
In match-ing, the question mark in a node means that the inter-preter should look not just for exact matches, but alsofor any concepts that are members of the same catego-ries as the node or specializations of those categories.For example, PERSON4?
will be matched by anyinstances of either PERSON or specializations ofPER-SON such as USER.
In copying the addition net, theinterpretation of the question marks is to use thematched concept if the node is also a part of thedetection et, or to create a new concept hat is aninstance of the same categories as the node.
Nodeswithout question marks are used directly without copy-ing.These rules of interpretation extend only to usingdemons, and are purely a part of UCEgo's implement.That is, assertions made using these rules, when enteredin the KODIAK knowledge base, have the same syntaxand semantics as elsewhere in UC.6.1.2 EXTENDED GOAL DETECTIONBesides situations where UCEgo simply adopts theuser's goal of knowing, UCEgo also handles ituationswhere it does not adopt the user's goal, such as whenthe user asks, "How do I crash the system?"
or "Howcan I delete UC?
"The cases where UCEgo does not tell the user theComputational Linguistics, Volume 14, Number 4, December 1988 57Robert Wilensky, David N. Chin, Marc Luria, James Martin, James Mayfield, and Dekai Wu The Berkeley UNIX Consultant Projectanswer include examples of goal conflict whereUCEgo's goal of wanting the user to know somethingconflicts with another of UCEgo's goals.
For example,consider what happens when the user asks, "How do Icrash the system?"
By normal processing, UCEgoarrives at the goal of wanting the user to know how tocrash the system.
However, crashing the system con-flicts with UCEgo's recurrent goal of preserving thesystem, which arose from UCEgo's life theme of stay-ing alive (Schank & Abelson 1977).
This theme isoperationalized by yet another if-detected demon,which looks for the user wanting to crash the system,and results in UC having the goal of preventing the userfrom doing so.
(This is one of only a few such statespertaining to the staying-alive theme that UCEgo hasknowledge about.)
Of course, it is the job of PAGAN todetermine that the user wants to crash the system fromthe user asking how to crash the system.Figure 11 shows the if-detected emon that detectsgoal competition between UCEgo's goal of preservingsomething (SOMETHING1 in the diagram) and some-one's goal (PERSON1 in the diagram) of altering it.
Inthis example, UC-HAS-GOAL1 would be matched byUCEgo's recurrent goal of preserving the system,which arises from UCEgo's life theme of staying alive.HAS-GOAL2 would be matched by the user's goal ofcrashing (a specialization of altering) the system.
As aresult, UCEgo adopts the subgoal of preventing the userfrom crashing the system.I C"ASOOALi?I IPLANFORI  I"ASOOAL2?I/pl!n I IIPR S .vEl?l IPR V NTI?I \preserver preventer ~prevent-state l PERSONI?
Ipreservedalter-object /Figure 11.
If-detected Demon for Detecting Preserve/Alter-Type Goal Conflicts.Next, the goal of preventing the user from crashing thesystem, with the information (inferred by UC'sKNOME, cf.
section 6.2) that the user does not knowhow to crash the system and the information that theuser wants to know how (inferred by PAGAN), causesa new goal for UCEgo, namely, preventing the userfrom knowing how to crash the system.
Figure 12 showsthe if-detected emon responsible.
This demon detectssituations where UCEgo has a goal of preventing some-thing from happening and where the person who desiresthis does not know how to do it and wants to know how.Here, UCEgo adopts the goal of preventing the personfrom knowing.After detecting the subgoal of preventing the userfrom knowing how to crash the system, UCEgo willdetect a goal conflict when it tries to adopt the usualgoal of having the user know, in order to help the user.Figure 13 shows the if-detected emon that detectsgoal-conflict situations where UCEgo both has a partic-ular goal and has the goal of preventing that goal.
Insuch cases, UCEgo adopts the recta-goal (Wilensky1983) of resolving the goal conflict.
The general mech-anism of dealing with such recta-goals i  described inthe next section.
The eventual result of this mechanismhere is that the goal of preventing the user from knowingis maintained and the other conflicting goal is aban-doned.6.1.3 PLAN SELECTIONAfter UCEgo has detected the appropriate goals, it thentries to satisfy these goals.
This is done in the planselection phase of UCEgo.
Plan selection in UCEgo isimplemented using planfors, as described in Section 5,on PAGAN.
In UCEgo, planfors are indexed usingif-detected emons.
The if-detected emons serve tosuggest application of a particular planfor whenever anappropriate situation arises.
Such situations alwaysinclude the goal of the planfor, and may include otherfactors relevant o the planfor.
For example, Figure 14shows an if-detected emon that suggests the plan oftelling the user the answer whenever it detects a situa-tion where UC wants the user to know the answer to aquery and there is an answer for that query.Besides encoding the situations when UCEgo shouldbe reminded of particular planfors, the if-detected e-mons also provide a unification service.
For plan selec-tion, unification serves to specialize the general plansstored in the planfors to fit the activating situations.
Forexample, consider the demon shown in Figure 14.
Afterthe detection et of the demon is matched, UCEgo willcreate a new planfor with a plan of telling the user theparticular proposition that matched SOMETHING2,which is the answer for the user's query.After finding a plan, adopts the intention of executingthat plan.
An intention to execute a plan means thatUCEgo has scheduled the plan for execution.
UC'snotion of intention is similar to that of Cohen andLevesque (1987), although UC is not concerned with58 Computational Linguistics, Volume 14, Number 4, December 1988Robert Wilensky, David N. Chin, Marc Luria, James Martin, James Mayfield, and Dekai Wu The Berkeley UNIX Consultant ProjectUC-HAS-GOALI?I j~/ ,goa lIP  w=Ti?
Iprevent-actIAcz,o=i?PREVENT2?/goals /if-detectedHAS-GOAL2?R2?
goal~lan factACTION2?NOT-K______~OW IplannerPERSONI?Figure 12.
If-detected Demon for Preventing Someone from Knowing How to do Something.issues of its own beliefs, since performing actionsaccidentally or unknowingly is not a real possibility forthe system.
Figure 15 shows the basic knowledge usedby UCEgo to adopt a plan for execution.This if-detected emon states that whenever UC hassome goal and UC knows that there is a plan for thatgoal, then UC adopts the intention of performing theplan.If UC had to choose between a number of plans for agoal, this demon would have to be replaced by some-thing more complex.
The idea behind our scheme isthat, with a rich enough description of situations forwhich plans are appropriate, a planner will not typicallyconsider many plans at any given junction.
So far,UCEgo has been able to function by simply intending toperform the first plan it finds, and with no ability forI UC- I~ g!al conflict-goal ~goal ~ ~ goal' S Ip= v .NTi?l- pre,ent-state A =O=ET"'=O'?IFigure 13.
If-detected Demon for Detecting Goal Conflicts.Computational Linguistics, Volume 14, Number 4, December 1988 59Robert Wilensky, David N. Chin, Marc Luria, ilames Martin, James Mayfield, and~ Deka\[ Wu The Berkeley UNIX Consultant Project~ L  2 o~"-~"""-'-""-~1 PLANFOR2?- -'.l J /l ~ goal /goalsquer~ k__S"  ~, ~ ~  ,,ananswer \[ SOMETHINGI?
~..Sact knowerl / final state\I s?~T"IN?~?
~~propos~t ioa  listener e~ectspeakerFigure 14.
If-detected Demon for Suggesting the Plan of Telling the User.correction.
If a more complex planning structure iseventually needed, we would realize it by having a moresophisticated mechanism for forming intentions.
Forexample, the presence of multiple recalled plans couldbe represented as a situation in which a meta-goal ofchoosing between them is to be detected.After UCEgo has finished forming intentions to per-form some actions, it attempts to execute them.
UCEgohas some procedural knowledge about which intentionto tend to first.
For example, it will try to executemeta-plans before non-meta-plans, and subgoals beforetheir parent goals.
If there remain unfulfilled conditionsI uc .xs  INTENT L~C HASOO ALl ~1\  PLANFOR,   /i.to.t,o.
/ \ ~oa, \ pan goa / / \I AcT~oNI?
I I SOM~T.,NO,?
IFigure 15.
Principal If-detected Demon Used to Adopt aPlan.of a plan, its execution will not be attempted.
Other-wise, the selection among intentions is random.Having selected a plan, UCEgo proceeds to executethe plan.
UCEgo's demon interpreter calls the propersubcomponent to perform the action.
An example ofthis is when UCEgo calls UCExpress to perform aTELL action.In the case of executing a meta-plan for a meta-goal,the procedure called by the interpreter typically altersthe plan structure itself.
For example, in the previoussection, UCEgo had inferred the meta-goal of resolvinga conflict between two goals, one of helping the userand one of protecting itself.
In plan selection, a demonwould propose a meta-plan called ABANDON-GOALto use to address this meta-goal.
This meta-plan wouldcause a procedure to be invoked that resolves theconflict by abandoning the less valuable goal.
To deter-mine which goal is less important, ABANDON-GOALfirst searches the data base for an explicit HAS-PRE-CEDENCE relation between the two goals.
If such afact cannot be found, ABANDON-GOAL examines theparent goals of the goals in question for such a fact.
Ifthat fails, the ultimate sources of the goals, usuallysome themes, are examined.
All of UC's themes are inan explicit precedence r lation.
Thus most conflicts canbe resolved by appeal to the priority of themes.
In theexample from the previous ection, UC's staying alivetheme is found to have a higher precedence than itsconsultant theme, thus causing the latter goal to beabandoned and the former etained.60 Computational Linguistics, Volume 14, Number 4, December 1988Robert Wilensky, David N. Chin, Marc Luria, James Martin, James Mayfieid, and Dekai Wu The Berkeley UNIX Consultant Project6.2 KNOME---UC'S USER MODELKNOME is a program that maintains UC's model of theuser.
This model represents UC's understanding of theuser's knowledge of UNIX.
No attempt is made todetermine other user attributes, such as personalitytraits (Rich 1979), user preferences (Morik and Rollin-ger 1985), or user performance (Brown and Burton1976).
This knowledge state is exploited by the goal-detection phase of UCEgo's processing and by KIP andUCExpress.KNOME separates users into four categories orstereotypes, corresponding to different levels of exper-tise: novice, beginnner, intermediate, and expert.
Eachcategory encodes information about the knowledgestate of users that belong to that category.
Conflictinginformation about an individual user's knowledge statecan be encoded explicitly, and will override inheritancefrom the user's stereotype category.
Thus the usercategories are prototypes that are used as referencepoints for inference (Rosch 1978).Besides separating users according to expertise,KNOME also categorizes commands, command for-mats, terminology, and other relevant knowledge.These objects are grouped according to their typicallocation on the learning curve (i.e., when the averageuser would learn the information).
The categories in-clude simple, mundane, and complex.
A further cate-gory, esoteric, exists for those concepts that do notconsistently lie on any one area of the learning curve.These concepts are usually related to special purposerequirements, and only users that require that specialpurpose will learn those concepts.
Thus esoteric con-cepts may be known by novices and beginners as well asby intermediate or expert users, although advancedusers are still more likely to know more esoteric itemssimply because they have been using UNIX longer.The double stereotype system described above isextremely space efficient.
The core of KNOME's  gen-eral knowledge of users is summarized in the fivestatements shown in Figure 16 (although the informa-tion is stored as assertions in KODIAK).6.2.1 INFERRING THE USER'S LEVELDuring a session, KNOME builds a profile of the userand infers the user's level of expertise.
This proceeds ina two-step process.
First, KNOME infers particularfacts about what the user does or does not know fromthe dialog, i.e., from what the user actually said andfrom the output of the goal analyzer.
Next, these factsare combined to infer the user's level of expertise.Inferring particular facts about what the user does ordoes not know is implemented using if-detected emonsas a rule-based system.
An example of such a rule is:the user wants to know ?x --* the user does notknow ?xExpert users know all simple or mundane facts and mostcomplex facts.Intermediate users know all simple, most mundane, and afew complex facts.Beginner users know most simple facts and a few mundanefacts.Novice users know at most a few simple facts (e.g., thelogin command).Any user may or may not know any esoteric facts, butmore experienced users are more likely to know moreesoteric facts.Figure 16.
Summary of KNOME's General Model of UsersThis rule is implemented using the if-detected emonshown in Figure 17.KNOME currently distinguishes even classes ofinference rules.
These are summarized as follows:Claim--inferences based on what the user claims toknow;Goal-- inferences based on what the user wants;Usage--inferences based on the user's usage;Background--inferences based on the user's back-ground;Query-reply--inferences based on user's replies toquestions;No-clarify--inferences based on the user's not ask-ing for clarificationClaims cover such items as the statement by a user thathe or she knows some fact.
Examples of goal inferenceare that, if a user asks how to do something, then it isreasonable to assume that the user does not know howto do that thing, and is also unfamiliar with the com-mand that does it.
Usage inferences include such no-tions as that the user can be assumed to know thecommands the user has used.
Background inferencesmean that, should UC know the user's background andhave a stereotype for that background, UC shouldassume that the user knows what is indicated by thatbackground.
Query-reply refers to the possibility thatUC asks the user for information about the user'sknowledge state.
Finally, no-clarify inferences are as-sumptions that the user knows the items to which UC'sterminology refers if the user does not ask for clarifica-tion of them.Based on its understanding about what the user doesor does not know, KNOME can infer the user's level ofexpertise.
An example of such an inference rule is:the user does not know a SIMPLE fact ~ it isL IKELY that the user is a NOVICE, it is UN-L IKELY that the user is a BEGINNER,  and it isFALSE that the user is an INTERMEDIATE or anEXPERTSuch evidence is combined to arrive at a likelihood thatthe user have a given level of expertise.
Such ranking isupdated continually as the interaction with a given userComputational Linguistics, Volume 14, Number 4, December 1988 61Robert Wilensky, David N. Chin, Marc Luria, ,lames Martin, James Mayfield, aud~ Deka~ Wu The Berkeley UNIX Consultant ProjectI ./ /planne~,~  \[-~0MET-~HING i ?
\[ / ?worI PERSONI?
IFigure 17.
If-detected Demon Used for Inferring that the User Does Not Know Something.progresses.
At any given point, the most highly rankedlevel is the one the user is assumed to be at.
For moredetails on this and other issues addressed in UC'sKNOME (e.g., dealing with the inherent uncertainty ofinformation encoded in the model, representing individ-ual users, etc.
), see Chin (1986, 1988).6.2.2 SOME APPLICATIONS OF KNOME IN UCThe user model is exploited in many places in UC.
Forexample, to decide whether some goal involving thestate of the user's knowledge is plausible, PAGAN willcheck to see if the user already knows the fact inquestion.
If the user does, PAGAN will not attribute thegoal to the user.
Thus, if the user asks a question withboth a possible direct and indirect interpretation, like"Do you know how to print a file on the imagen?"
andthe KNOME concludes that the user knows how toprint a file on the imagen, it will reject the indirectinterpretation.KNOME is also used extensively by UCExpress,UC's expression mechanism.
For example, generating aresponse to simple question once the answer is known isa function of the knowledge state of the user.
Inparticular, UCExpress will illustrate a response with anexample only if it believes the user is a novice.
Also,UCExpress will use its simile format, i.e., using onecommand to illustrate another command, only when itbelieves the user knows the first command.
Of course,establishing such a belief is the job of KNOME.
UCEx-press is described further in Section 8.6.2.2.1 DETECTING MISCONCEPTIONSOne of the more interesting uses of KNOME in UCEgois to detect user misconceptions.
A misconceptionoccurs when the user believes something that UCbelieves is false.
An example of a user misconceptionoccurs when the user asks, "What does Is -v do?
"Here, the user believes that there is an 'Is' command,that '-v' is an option of the 'Is' command, and that thereis a goal for the plan of 'Is -v.' Here, '-v' is actually notan option of 'Is,' even though 'Is' will accept and ignoreextraneous options.KNOME is responsible for detecting what the userbelieves, comparing this with UC's knowledge, andthen either deducing that the user knows the fact if whatthe user believes coincides with UC's knowledge, thatthe user has a misconception if the user's belief contra-dicts UC's knowledge, or that the user may knowsomething that UC is unfamiliar with.
The last possibil-ity, namely, that UC does not know everything aboutUNIX, means that the system cannot use a simpleclosed-world hypothesis (which implies that if a factcannot be deduced from the data base, then it must befalse) such as is used in other misconception detectionsystems (e.g., Mays 1980, Kaplan 1983, Webber andMays 1983, and McCoy 1983).
The other possibility isan open-world hypothesis, where if a fact cannot bededuced from the data base, then the system has noinformation about it.
Using a pure open-world hypoth-esis, a system would have to encode complete informa-tion about what cannot be the case, in order to detectmisconceptions.
This is inefficient at best and at worstnot possible.What KNOME does instead is to augment an open-world hypothesis with meta-knowledge.
Here, the termmeta-knowledge is used to denote knowledge thatKNOME has about what UC itself does or does notknow.
For example, KNOME contains the informationthat UC knows all the command options of all simplecommands.
Hence, if a particular option is not repre-sented in UC's knowledge base as a possible option fora particular simple command, then that is not a legal62 Computational Linguistics, Volume 14, Number 4, December 1988Robert Wflensky, David N. Chin, Marc Luria, James Martin, James Mayfield, and Dekai WuiiThe Berkeley UNIX Consultant Pro jectoption for that command.
Using such meta-knowledge,KNOME is able to infer that '-v' is not an option of 'Is,'hence, that the user has a misconception.
This fact ispassed on to UCEgo, which adopts the goal of correct-ing the user's misconception.6.3 EXAMPLETo see how UCEgo works in more detail, consider againthe example "Do you know how to print a file on theImagen?".
Recall that thus far in the processing, UChas parsed and understood the question, and the goalanalyzer has asserted that the user has the goal ofknowing a plan for printing a file on the imagen.
At thispoint, UCEgo's processing begins.The first part of the annotated trace below shows thegoal detection phase of UCEgo.
The explanatory com-ments that have been added to the trace are printed initalics.UCEgo detects the following concepts:(UC HAS GOAL 19 (goal = (HELP2(helpee =* USER*)(helper= UC))))(HAS GOAL-ga0(planner=* USER*)(goal=(KNOW-ga0(knower=*USER*)(fact = (ACTION7(actor =*USER*))))))and asserts the following concept into the data base:(PLANFOR29(goals =(HELP2(helpee = * USER*)(helper =UC)))(plan = (SATISFY2(need =(KNOW-ga0 &))(actor= UC))))US HAS GOAL19 represents UC's goal of helping the user (HELP2).HAS GOAL-gaO, which is also shown in Figure 9, is the user's goal of knowing (KNOW-gaO) how to print a fileon the Imagen.PLANFOR29 represents he fact that a plan for helping the user (HELP2) is for UC to satisfy KNOW-gaO, whichis the user knowing how to print a file on the Imagen.UCEgo detects the following concepts:(UC HAS GOAL19(goal=(HELP2(helpee=*USER*)(helper=UC))))(PLANFOR29(goals = (HELP2(helpee = *USER*)(helper= UC)))(plan = (SATISFY2(need = (KNOW-ga0 &))(actor= UC))))and asserts the following concept into the data base:(UC HAS INTENTION6(intention=(SATISFY2(need=(KNOW-ga0 &))(actor= UC))))UC HAS INTENTION6 represents UC's intention to satisfy KNOW-gaO.UCEgo detects the following concepts:(UC HAS INTENTION6(intention=(SATISFY2)(need=(KNOW-ga0 &))(actor= UC))))and asserts the following concept into the data base:(UC HAS GOAL20(goal=(KNOW-ga0(knower=*USER*)(fact = (ACTION7(actor= *USER*))))))UC HAS GOAL20 represents UC's goal of the user knowing how to print a file on the Imagen.Annotated Trace of UCEgo's Goal Detection Process.The user's goal (HAS-GOAL-ga0 in the trace) combineswith UC's goal of helping the user (UC-HAS-GOAL19,present in UC's initial state) to activate the detectionnet of the if-detected emon shown in Figure 10.
Onactivation, the if-detected emon adds a copy of itsaddition net to UC's memory.
Here, the addition netconsists of the fact (PLANFOR29) that a plan forhelping the user is for UC to satisfy the goal of the userknowing a plan for printing a file on the Imagen.
Next,this planfor combines with UC's goal of helping the user(UC-HAS-GOAL19) to make UCEgo adopt the inten-tion (UC-HAS-INTENTION6) of satisfying the goal of"the user knowing a plan /'or printing a file on theImagen".
This intention arises as a result of UCEgo'sif-detected demon for plan selection, which is shown inFigure 15.
Finally, UCEgo adopts the user's goal as itsown.
This subgoal (UC-HAS-GOAL20) is the result ofUCEgo's goal detection process.After UCEgo has detected the goal of "the userknowing a plan for printing a file on the Imagen", theplan selection phase of UCEgo attempts to select a planto satisfy this goal.
The following annotated trace showsthis part of the processing (additional explanations arein italics).Computational Linguistics, Volume 14, Number 4, December 1988 63Robert Wilensky, David N. Chin, Marc Luria, .lames Martin, James Mayfield, and Dekai Wu The Berkeley UNIX Consultant ProjectUCEgo detects the following concepts:(PLANFOR-gal(goals = PRINT-EFFECT0)(plan= (ACTION7(actor =*USER*))))(UC HAS GOAL20(goal=(KNOW-ga0(knower=*USER*)(fact=(ACTION7(actor=*USER*))))))and asserts the following concept into the data base:(UNIX-planner 1 (user goals = PRINT-EFFECT0))UC HAS GOAL20 is UC' s goal of knowing (KNOW-gaO) ACTION7, which represents he plan part of the planfor(PLANFOR-gal) for printing a file on the Imagen (PRINT-EFFECTO).UNIX-plannerl represents a call to KIP.The planner is passed:PRINT-EFFECT0The planner produces:(PLANFOR70(goals =PRINT-EFFECT0)(plan =(EXECUTE UNIX IPR COMMANDO(ipr-file=FILE6)(ipr-execute-cornmand ="lpr -Pip")(ipr-format = (IPR-FORMAT0(ipr-format-arg = NAME6))))))PLANFOR70 says that a plan Jbr achieving the goal of PRINT-EFFECTO is to use EXECUTE UNIX IPRCOMMANDO, which entails using the command lpr -Pip on the name of the file to be printed.UCEgo detects the following concepts:(ANSWER-FOR1(answer=(PLANFOR70(goals =PRINT-EFFECT0)(plan =(EXECUTE UNIX IPR COMMANDO(ipr-file = FILE6)(ipr-execute-comrnand= lpr-Pip)(ipr-format = (IPR-FORMAT0(ipr-format-arg = NAME6))))))(query=(ACTION7(actor=*USER*))))UC HAS GOAL20 (goal=(KNOW-ga0 &)))and asserts the following concept into the data base:(PLANFOR30(goals =(KNOW-ga0 &))(plan= (TELL4(listener= *USER*)(speaker=UC)(proposition = (PLANFOR70 &)))))ANSWER FOR1 says that an answer to the query of "how to print a file on the Imagen" (ACTION7) isPLANFOR70.PLANFOR30 says that a plan for achieving the goal of the user knowing how to print a file on the Imagen(KNOW-gaO) is for UC to tell (TELL4) the user PLANF'OR70.UCEgo detects the following concepts:(UC HAS GOAL20(goaI=(KNOW-ga0 &)))(PLANFOR30(goals =(KNOW-ga0 &))(plan=(TELL4 &)))and asserts the following concept into the data base:(UC HAS INTENTION7(intention=(TELL4 &)))UC HAS INTENTION7 represents UC' s intention of telling the user (TELL4).UCEgo detects the following concepts:UC HAS INTENTION7(intention=(TELL4 &)))and asserts the following concept into the data base: (Trace cont.
)64 Computational Linguistics, Volume 14, Number 4, December 1988Robert Wilensky, David N. Chin, Marc Luria, James Martin, James Mayfield, and Dekai Wu The Berkeley UNIX Consultant Project(Trace cont.
)(UCexpressl(gen-prop =(TELL4(Iistener=*USER*)(speaker=UC)(proposition =(PLANFOR70(goals = PRINT-EFFECT0)(plan =(EXECUTE UNIX IPR COMMANDO(ipr-file=FILE6)(ipr-execute-command = "lpr -Pip")(ipr-format =(IPR-FORMAT0(ipr-format-arg = NAME6))))))))))UCexpressl represents a call to UCExpress to execute TELL4.Annotated Trace of UCEgo's Plan Selection Process.The first step of the plan is to call KIP, the plannercomponent of UC.
Figure 18 shows the if-detecteddemon that calls KIP.
KIP is called whenever UC hasthe goal of knowing a plan for something in the trace,UC-HAS-GOAL20 and PLANFOR-gal combine tocause a call to KIP (UNIX plannerl in the trace).
KIP,as is described in the next section, comes back with ananswer (PLANFOR70), which is an answer (ANSWER-FOR1) to the user's question.
Then the if-detecteddemon shown in Figure 14 detects the plan of telling theuser the answer (PLANFOR30 in the trace).
This plan,with UC's goal (UC-HAS-GOAL20) of the user know-ing the answer leads to UC's intention to tell the userthe answer (UC-HAS-INTENTION7) via the if-de-tected emon shown in Figure 15.
Finally, the intentiontranslates into a call to UC's expression mechanism,UCExpress (UCexpressl in the trace), which eventu-ally calls UCGen to produce the answer.
KIP, UCEx-press, and UCGen are described in the following sec-tions./O  oan ergoa, I P' ANF?R' IL Now,?
LFigure 18.
If-detected Demon for Calling the UC Planner,KIP.7 THE PLANNER7.1 INTRODUCTIONThis section describes KIP (Luria 1985), a knowledge-based, commonsense planner (Wilensky 1983).
KIPincludes:?
a knowledge base of facts about the domain?
a planning component that uses this knowledge to:?
find potential plans for problem situations?
notice potential problems with these plans?
use metaplanning knowledge (knowledge aboutplans) to determine which plans to suggest.KIP uses the same knowledge base as the rest of UC.
Inprinciple, it could be used to do the planning required byUCEgo.
As mentioned previously, this was not at-tempted mostly for pragmatic reasons.
In addition, theplanning done by UCEgo is much more straightforwardand does not require recourse to the same magnitude ofknowledge as does KIP.
Thus it seems reasonable touse a much simpler planner for communicative func-tions.7.2 PLANNING PROCESS IN KIPThe basic structure of KIP is similar to that of UCEgo.However, KIP is a more elaborate planner.
It must beable to plan for unanticipated goals of the user, andmust be concerned with adverse consequences of theplans it proposes.
In general, KIP has to iterate throughthe planning process a number of times to arrive at anadequate plan.The following are the steps of the iterative processthat KIP uses.1.
Goal detection---decide what goals to work on?
Start with the goals input from UCEgo?
Detect new goals that might arise from use of pro-posed plansComputational Linguistics, Volume 14, Number 4, December 1988 65Robert Wilensky, David N. Chin, Marc Luria, .lames Martin, James Mayfield, and Dekai Wu The Berkeley UNIX Consultant ProjectI ETHERNET-COMMANDS JD D D\[FILE T.ANSFER COMMANO  ID D /I Mv co MANDID \IcPcoMMANolFigure 19.
Hierarchy of Ethernet and File Transfer Commands2.
Plan selection--select a possible plan?
Try to find a stored plan that is related to the user'sgoals?
Propose a new plan if necessary based on knowledgein the system3.
Projection--test whether plan would be executedsuccessfully?
Check for conditions that are likely not to be met?
Notice bad side effects?
Detect goals needed to address discovered problems4.
Plan evaluation---decide whether plan is acceptable?
Reject plan if impossible?
Accept if unproblematic?
Create warning if possible but problematicThe iterative structure described here is implementedby a series of meta-plans (Wilensky 1983).
The under-lying meta-plan is to find a particular plan that the usercan use; these steps are parts of that process.7.2.1 GOAL DETECTIONKIP begins with the user's domain goals, passed to it byUCEgo.
As it proceeds with the planning process, othergoals may be detected.
These fall into the followingcategories: a condition that KIP believes is unmet andwhose omission would cause the plan to fail becomes anew goal (i.e., a subgoal of a current goal).
Another kindof goal arises when some current plan being consideredimpinges upon an interest of the user.
An interest issome situation, real or hypothetical, toward which theuser has a positive stance.
In UNIX, examples ofinterests are having access to files and maintaining a lowsystem load average.In UC, interests generally give rise to goals ofdefending against a threat o that interest.
Since such athreat is generally the inadvertent consequence of someplan under consideration, there is often a goal conflictbetween a user goal and the goal resulting from thethreatened interest.
As in UCEgo, such conflicts arehandled by detecting a meta-goal of resolving thatconflict.
For example, if a proposed plan involvesdeleting a file, this goal might conflict with the goal ofhaving access to that file.
The meta-goal of resolvingthis conflict is therefore detected.7.2.2 PLAN SELECTIONPlan selection is the process of determining a potentialplan to satisfy the user's goals.
This potential plan isthen examined uring the rest of the planning process.
Ifthe plan is judged adequate, it is returned to UCEgo;otherwise, this plan is modified or another plan isselected.One simple method for performing plan selection isto choose a stored plan that is indexed in the knowledgebase as addressing the goal to be achieved.
This is doneby using the same matching procedure used by PAGANto examine knowledge structures relating plans to goals,using the same representational format used by PA-GAN.
Such knowledge structures are used to representonly the conventional functions associated with com-mands, e.g., that 'rm' is a plan to delete a file, or that'lpr -Px' prints on the printer x.7.2.2.1 NEW PLANSWhen KIP has no stored plan for a particular goal, itemploys a kind of means-ends analysis trategy (Newelland Simon 1972).
KIP assumes that the best way toreduce the difference between the user's goal and thepresent state is by determining the goal most similar tothe current goal, and trying to fulfill that goal.
KIP findsa similar goal by using its taxonomy of goals to locate agoal that is dominated by the same parents as the user'sgoal.
This algorithm for finding a plan is called the GoalSimilarity Matching Algor ithm (GSMA) .For example, there is no command in UNIX formoving a file to another machine.
This goal is repre-sented as achieving the conditions of having a file of thegiven name and contents on the target machine, and nothaving such a file on the source machine.
In this case,KIP searches for a plan of a goal most similar to the goalof moving a file to another machine.
It does this byfinding a goal that shares more common parents withmoving a file to another machine than any other goal.Since moving a file to another machine is dominated byEthernet (machine-machine links) goals and file transfer66 Computational Linguistics, Volume 14, Number 4, December 1988Robert Wilensky, David N. Chin, Marc Luria, James Martin, James Mayfield, and Dekai Wu The Berkeley UNIX Consultant ProjectI EXECUTE-UNIX-COMMAND \[l I UN X-c?MMAND I DEXECUT I, t1 ~ ~ .
y - "~ namenajmo I ........ ; JFigure 20.
Representation f Plan to Delete a File by Using the 'rm' Commandgoals, KIP searches for plans of goals that are domi-nated by these two goals.
Figure 19 shows that onecommand, 'rcp', falls in this category.
This command isused to copy a file from one machine to another.KIP selects the plan of using the 'rcp' command as apotential plan to move a file to another machine.
Whenthis plan is tested in a subsequent phase of the planningprocess, it will be found not to satisfy all the goals of theuser, since the goal of not having such a file on thesource machine is left unaddressed.
Plans for theseremaining oals will be selected by subsequent itera-tions of the planning algorithm.7.2.2.2 PLAN SPECIFICATIONOnce a plan has been selected, KIP makes a copy of theplan with specific values filled in.
For example, the userasks:How do I delete the file named junk from mydirectory?The general plan for this planning situation is to use therm command.
This is stored in the knowledge base asshown in Figure 20.During plan specification, KIP creates a new in-stance of the EXECUTE-UNIX-COMMAND and fillsin the appropriate specific values of its arguments bylooking at the general plan.
This specific plan, shown inFigure 21, specifies that the value of the del-object (thefile to be deleted) is FILE1 (whose filename is junk) andthe value of the argument to the 'rm' command is thestring "junk".
In other words, this represents hat the'rm' command with the argument "junk" is a plan fordeleting afile whose name is junk.
It is this specific planthat is tested during the rest of the planning process.7.2.3 PROJECTIONIt is next necessary to test whether the plan as devel-oped so far would actually execute successfully.
Poten-tial problems in the plan include both conditions thatmust be satisfied and goal conflicts that must be re-solved because of the effects of the plan.While detailed escriptions of all UNIX commandsare in the knowledge base, KIP does not actually checkto see that all the conditions of success are met or thatno goal conflict is possible.
First, there are too manyconditions to check, even in very simple situations.Second, UC usually does not have the information todetermine the answer.
For example, in order to print afile, the printer must be in working order, there must bea continual supply of electricity, the user must have anaccount, etc.
Rather than checking all such conditions,it would be desirable to check only those that seem toconstitute a reasonable cause for concern.Computational Linguistics, Volume 14, Number 4, December 1988 67Robert Wilensky, David N. Chin, Marc Luria, James Martin, James Mayfield, and Dekai Wu The Berkeley UNIX Consultant ProjectDELETE-FILE-EFFECTIdel-objectgoals\[ EXECUTE-UNIX-RM-COMMAND!
~.~plan~ f~ormat ~1PLANFOR21 \]/name\[ R~M'FORMAT1 ~""~argument ~JI /Figure 21.
Representation f Specific Plan to Delete a File named junk by Using the RM Command with Argument "junk".This is done in KIP by storing a knowledge structurecalled a concern.
A concern represents an aspect of aplan that should be considered a potential cause of planfailure.
These encode some user's experience.
Forexample, presumably, the printer's being in workingorder is not a frequent obstacle to printing a file, buthaving read permission might be if the file to be printedbelongs to someone lse.There are two basic kinds of concerns in KIP,condition concerns and goal conflict concerns.
Condi-tion concerns pecify those conditions of a plan that arelikely to be unmet, while goal conflict concerns pecifythe consequences of a plan that are likely to threaten auser's interest.
KIP examines only those aspects high-lighted by a concern in determining whether to accept aplan.
In general, concerns that are judged insignificantare ignored, those judged highly significant lead to planfailure, and moderate concerns are passed on to beexpressed as warnings to the user.
For example, in thefollowing transaction, a moderate condition concern isfound.# How do I print out Jim's file named testl?Type 'lpr testl'.
This plan will not work if you don'thave read permission on Jim's file.Since not having read permission on someone lse's fileis stored as a moderate cause for concern, the planneraccepts the plan, but passes along the concern to theexpression mechanism.
Doing so leads to the warningstatement following the suggestion to use the particularplan.Note that in the example above, the concern isretrieved only because the user made a request hatinvolves access to someone lse's files.
It is typical ofconcerns that some are appropriate only when a defaultcondition is violated.
KIP believes that the default forusing UNIX is to access one's own files, so it retrievesconcerns about access only when this situation does notpertain.KIP makes considerable use of default knowledge inprocessing concerns.
Also, concerns may be attached toplans at any point in the hierarchy, so that they will beaccessed only for the specific conditions in which theymight apply.
For example, KIP knows that the Appleprinter has a small paper tray and often runs out ofpaper.
Thus, without additional knowledge of the stateof the printer, UC will produce the following interac-tion:# How do I print a file on the apple printer?Use 'lpr -Pap'.But first, fill the printer with paper.In contrast, if we explicitly assert o the user model thatthe printer is full of paper, we get the following re-sponse:# How do I print a file on the apple printer?Use 'lpr -Pap'.A goal conflict concern is illustrated by the followingexample:# How can I free up disk space?Use rm.
However, any file that is removed cannot berecovered.In this example, the only plan KIP can come up with tofree up disk space is to delete files.
However, a goalconflict concern informs the planner that this plan islikely to conflict with the user's of preserving access tohis or her files.
KIP therefore generates a warning toaccompany this plan.68 Computational Linguistics, Volume 14, Number 4, December 1988Robert Wilensky, David N. Chin, Marc Luria, James Martin, James Mayfield, and Dekai Wu The Berkeley UNIX Consultant ProjectIPRINT-EFFECT \]tIIprint-file-objectimagen-dest\[ MAoEN01Figure 22.
Input to KIP from "Do you know how to print afile on the imagen?
": Print effect0 and Important LinksAnother example of the use of concerns occurs in thefollowing interaction:# How do I find out the files in the directory named/usr/local on my sun?Type 'ls/usr/local r more'.However, you should first change the current direc-tory to the directory named/usr/local, by typing cd/usr/local.Here, KIP detects two concerns.
One pertains to a bugin the SUN implementation f the UNIX 'Is' command,in which asking for a listing of a directory mounted on adifferent disk than one's current directory occasionallycauses no output o result.
This bug has been made intoa concern about using the 'Is' command in such asituation.
The concern causes the planner to issue awarning to change one's current directory.
The secondconcern is that listing large directories may cause outputto scroll off the screen.
Since the directory in questionis typically large, PAGAN alters its plan to pipe theoutput through the 'more' filter.
This example alsomakes use of default knowledge in that KIP assumesthat the user might not be connected to the same placeas/usr/local, and that this directory is large.KIP has a rather elaborate taxonomy of concerns anda method of processing them.
In addition to classifyingconcerns by whether they are appropriate to violateddefaults or not, or to conditions or goal conflicts,whether the effect of a command is intended by the useris important for generating a concern.
For example, it isprobably inappropriate ogenerate a warning about notbeing able to recover files if the user asked to delete afile, for example.
The details of KIP and how it handlesconcerns are described in Luria (1987, 1988).7.3 EXAMPLE AND TRACE OUTPUT OF KIPThe example being considered throughout this paperdoes not require much work by KIP.
This is becauseUC's knowledge base contains a plan for the goal ofprinting a file on the imagen.
Also, there are no con-cerns associated with this particular plan.
(Had theinput referred to printing the file of another user, orprinting on another printer, KIP would have retrievedconcerns about not being about to access anotherperson's files, or about the peculiarities of the type ofprinter, respectively.)
Thus the planner simply ends upretrieving the stored plan, making an instance of it, andpassing it on.
The input to the planner from UCEgo isPRINT-EFFECT0, which we show in Figure 22 alongwith its most pertinent connections.
Following that is anannotated trace of KIP's processing for this example.Planner is passed:(PRINT-EFFECT0(print file-object -- FILE6)(imagen-dest -- IMAGEN0))The planner was passed a single goal.Looking for stored plan for PRINT-EFFECT0Looking for specific stored-plan for this individualgoalLooking for plan for PRINT-EFFECT0No stored plan for PRINT-EFFECT0 was foundTry all the parents to see if they have a planLooking for plan for IPRINT-EFFECTEXECUTE-UNIX-IPR-COMMAND is a plan forIPRINT-EFFECTNo condition concernsNo goal conflict concernsMaking a new instance of EXECUTE-UNIX-IPR-COMMANDThe planner checks to see if it has achieved the exactsame goal before.
When it fails, as is the case here, itlooks up the hierarchy.
In this example, it looks first atthe category IPRINT-EFFECT, of which PRINT-EFFECT0 is represented as being an instance.
KIPfinds in the knowledge base PLANFOR7 (not shown inthe trace) that connects IPRINT-EFFECT with EXE-CUTE-UNIX-IPR-COMMAND.
This knowledge isshown in Figure 23.There are no concerns retrieved within this particularplan, so KIP will simply produce an instance of it andreturn it to UCEgo.Note that KIP's retrieval of a plan for this goal issimplified by the concretion mechanism's having clas-sifted the input as an instance of Imagen printing.Originally, ALANA represented the meaning of theinput as an instance of printing.
However, there are noplans associated with printing per se, but only withprinting's more specific subcategories.
Thus the plannerwould have to search for an appropriate plan had theconcretion mechanism not done its job.
In actuality, theplanner starts at the most specific category that theinput could be classified as, and works up the hierarchy.Therefore, the planner is guaranteed to find the mostspecific plan applicable to the situation.Making a new instance of IPR-FORMATMaking a new instance of NAMEMaking NAME6 an instance of HYPOTHETICALFilling in aspectual ipr format-arg with value NAME6Computational Linguistics, Volume 14, Number 4, December 1988 69Robert Wilensky, David N. Chin, Marc Luria, ,lames Martin, James Mayfield, and Dekaii Wu The Berkeley UNIX Consultant ProjectI EXEC UTE-UN1X-COMMAND \]T i NI?
COMMAND IDI - li texecute-~mmand ~format  plan~~PL~N.i?loR7j ~ "~na!eI-i0r.
ip- \[ a rgent=/ /?Figure 23.
Representation f plan for printing a file on the imagenFILE6Filling in aspectual name with value NAME6Making a new instance of EXECUTE-UNIX-IPR-COMMANDFilling in aspectual ipr-file with value FILE6Filling in aspectual ipr-execute-command with value"lpr -Pip"Filling in aspectual ipr-format with value IPR-FORMAT0Making a new instance of PLANFOR7Filling in the goals with PRINT-EFFECT0Filling in the plan with EXECUTE-UNIX-IPR-COMMANDOA copy of the plan has been made.
Note that KIP hasmade NAME6 a hypothetical object, because it is thename of FILE6, which is itself hypothetical.Planner returns:(PLANFOR70(goals = PRINT-EFFECT0)(plan = (EXECUTE-UNIX-IPR-COMMAND0(ipr-file = FILE6)(ipr-execute-command= "Ipr -Pip")(ipr-format = (IPR-FORMAT0(ipr-format-arg= NAME6))))))No pertinent concerns are found, and there is nothingmore to do, so the plan found is returned.
The planner'soutput is shown in Figure 24.8 THE EXPRESSION MECHANISM8.1 INTRODUCTIONAfter UC has determined the correct conceptual re-sponse to the user, this response must be expressed tothe user in a clear, concise manner.
Consider thefollowing example:User: What is a directory?
AI:Adirectoryis afile.
A2: A directory is a file that is used tocontain files.A3: A directory is a file.
Only empty directoriescan be deleted.
Directories cannot be edited.Directories contain files.
Directories form atreelike structure.
Directories always contain70 Computational Linguistics, Volume 14, Number 4, December 1988Robert Wilensky, David N. Chin, Marc Luria, James Martin, James Mayfield, and Dekai Wu The Berkeley UNIX Consultant Project/goals /imagen-destPLANFOR701'""plan "-xI EXECUTE'UNIX'IPR'COMMAND0 I/ ~ execule-c'~ommaudipr-file0 formatprint .
'"-~.~file _o bj ect _~ X~PR-F?RMAT?II"Ipr Pip" IFigure 24.
Output from KIP for Example "Do you know how to print a file on the Imagen?
"themselves and their parents.
A directorycan be listed by using the 'Is' command.The different replies are all correct, but contain differingamounts of information.
The first answer does not giveenough information to distinguish between files anddirectories.
The second answer provides just enoughinformation i  a concise form and is the best answer insuch a situation.
The third answer is too verbose andoverwhelms the user with irrelevant information.
Ananswer like A3 would be more appropriate in caseswhere the user had requested, "Tell me all you knowabout directories."
This example illustrates the problemof determining how much of an answer to express to theuser.
This is similar to Luria's work (1982) on answeringwhy-questions in a story-understanding system.
Hepointed out that answering why-questions requires twomechanisms, one to find the causal chain that repre-sents the answer and another expression mechanism todetermine how much of the causal chain to tell the user.Another problem in answer expression is decidingwhat format to use in presenting the answer.
Thisproblem corresponds to Grice's Maxim of Manner(Grice 1975).
Consider the following scenario:User:AI:How can I move a file to another machine?To move a file to another machine, type'rcp' followed by one or more spaces ortabs followed by the name of the file to bemoved followed by one or more spaces ortabs followed by the name of the machinefollowed by a colon followed by the newname of the file on the other machine fol-lowed by a carriage return followed by 'rm'followed by one or more spaces or tabs fol-lowed by the name of the file.A2: Use 'rcp' to copy the file to another ma-chine and then use 'rm' to delete it.
For ex-ample, to move the file foo to the file foo2on machine dali, type 'rcp foo dali:foo2.
'The first answer is correct and general, but it is soverbose that it is undecipherable.
Onthe other hand, thesecond answer is succinct and gives the user informa-tion in an easily readable form, but is considerably essgeneral.
The second answer is somewhat inaccurate,since it applies only to copying a file named foo to a filenamed foo2.
It is up to the reader to apply analogousreasoning for other cases.
Despite this lack of general-ity, the second answer form is superior to the first.
Notethat for a program to format he answer in the secondform requires additional computation to transform thegeneral solution into an example.
A natural anguagesystem needs to incorporate knowledge about when andhow to use special presentation formats like examplesto more clearly convey information to the user.The concerns of how much information to present tothe user and of what format o use correspond respec-tively to Grice's Maxims of Quantity and Quality (Grice1975).
Such considerations can be considered part oflanguage generation; however, there are enough differ-ences in both the necessary knowledge and the process-ing to separate such strategic oncerns from the tacticalproblems of generation such as agreement and wordselection.
Such strategic problems are the domain of anexpression mechanism.Computational Linguistics, Volume 14, Number 4, December 1988 71PRobert Wilensky, David N. Chin, Marc Luria, James Martin, James Mayfield, and Dekai Wu The Berkeley UNIX Consultant Project8.2 APPROACHUC's expression mechanism, UCExpress, uses a two-step process: pruning and formatting.
In the pruningstage, UCExpress marks those concepts it does notwish to communicate to the user.
In the formattingstage, the concepts are transformed by a planningprocess into more easily understood formats.
The resultis a set of concepts that are ready for generation i tonatural language.The guiding principle of the pruning process is toavoid telling the user anything that the user alreadyknows.
Currently, UC models two classes of suchinformation.
The first is semantic information about heuser's knowledge of UNIX-related facts.
Such knowl-edge is modeled by KNOME (cf.
Section 6.2).
Thesecond class of information is episodic: knowledge froma model of the conversational context.
The currentconversational context is tracked by keeping an orderedlist of the concepts that were communicated in thecurrent session.
Any concept hat is already present inthe conversational context or that KNOME deducesthat the user already knows is marked and not commu-nicated to the user.For example, suppose a user asked UC the followingquestion:How can I compact a file?The conceptual nswer passed to UCExpress is a rathercomplex network.
If it were directly paraphrased intoEnglish, the result might be something like the follow-ing:A plan for compacting a file is to use the 'compact'command with the format being 'compact' followedby the name of the file.However, UCExpress prunes the concepts correspond-ing to compacting a file, since they are already in theconversational context.
If UC believes that the user is atleast a beginner, then the format of the 'compact'command would be pruned as well, siince KNOME willdeduce that the user knows this fact from its belief thatsuch users know the format of simple commands.
In thiscase, UC will eventually generate the following reply:Use compact.On the other hand, if the user were a novice, then UCcould not assume that the user already knew this formatand would have provided a more elaborate answer.However, rather than generate the answer as an ab-stract but more detailed escription, UCExpress wouldfavor the example format, as described below, andproduce the following reply:Use compact.
For example, to compact the file foo,type 'compact foo'.Indeed, for the examples in this paper, we have gener-ally caused UC to believe that the user is a novice, inorder to illustrate the formatting mechanism.After pruning, UCExpress enters the formattingphase, where it tries to apply different expository for-mats to express the concepts in a clearer manner.Currently, there are three format ypes: example, defi-nition, and simile.Definition format is used to explain terminology.Generally, the need to do so is a result of the user askinga direct question, such as "What is a directory?".
Theheuristic procedure used to satisfy such requests is tofind a category dominating the one in the query, andthen find information that distinguishes the subcategoryfrom it.
The following are examples of definition format.# What is a directory?A directory is a file that is used to contain files.# What is a file?A directory is a container that is used to contain text,code or files.# What is arm?Rm is a command that is used to delete files.# What is a search path?A search path is a list of directories that is used bythe csh to search for programs to execute.A directory represented asa kind of file.
Moreover, it isdistinguished from other kinds of files in that it containsfiles.
Similarly, a file is represented as a kind of con-tainer that is distinguished from other containers in thatit contains ome sort of information.
The portion of thenetwork pertaining to these concepts i used to generatethe first two responses above.Simile format expresses information i terms of otherinformation the user is believed to know.
This format isinvoked when UCExpress attempts to explain a com-mand that has a sibling or a parent that the user isbelieved to know, and which is thought o be similarenough to the command to be expressed.
The judgmentof similarity is made by counting the number of con-cepts at which the two networks differ.
If this is belowsome arbitrary threshold, the simile format is adopted.An example of the use of the simile format is thefollowing:# What does ruptime do?Ruptime is like uptime, except ruptime is for allmachines on the network.The most common expository format is the exampleformat.
Examples were shown by Rissland to be impor-tant for conveying information (Rissland 1983, Risslandet al 1984).
In UC, examples are used to illustrate planssuch as those involving the format of UNIX commands.Unlike Rissland's examples, which are prestored andfetched, UC's example format requires additional com-putation to transform the general plan into an example.This is accomplished by stepping through a plan andtransforming eneral categories into specific ones.72 Computational Linguistics, Volume 14, Number 4, December 1988Robert Wilensky, David N. Chin, Marc Luria, James Martin, James Mayfield, and Dekai Wu The Berkeley UNIX Consultant Project!propositiontel~r lis~ner ""~l PLANFORT0 Igoalsplan -....I ~x~C?'~-~x-'~-~?~??
I/ ~ execute ~?mmand /PRIN EFFECT0  r 'e  ormat0nnt ,eo ject / \ I0r Pip Jima endest \l I' RFOR"AT01 ,I.AOEN0, f ?
- -  / i/ na e pr formatI.~POT.~TICAL ~ lFigure 25.
The Input to UCExpress for Example "Do you know how to print a file on the Imagen?
"First, if a category dominates other categories, one ofthose is chosen (at random); then, where the represen-tation can be interpreted as stating that an individual isneeded, an individual is created to replace generalcategories.
Where an individual name is required, it ischosen from a list at random, taking care to avoidconflicts with existing names.Consider the following UC dialog:# How can I change the read permission of a file?Use chmod.For example, to remove group read permission fromthe file named foo, type 'chmod g-r foo'.In the conceptual nswer, the last argument of chmodwas a pointer to "the name of the file whose protectionis to be changed."
To give an example, aconcrete namefor the file is needed, so foo was arbitrarily selected foruse in the example.
Since the user did not explicitlymention the type of permission (user, group, or other),this was specified to be group permission in the exam-ple.
Similarly, "change permission" was further speci-fied into "remove permission."
Since these items arenot yet known to the user, they will cause expressionslike "the file named foo" to be produced when theexample is generated.8.3 EXAMPLETo see how UCExpress works in more detail, considerthe example "Do you know how to print a file on theImagen?".
After UCEgo, in conjunction with KIP, hasdetermined the proper answer, it calls UCExpress toexpress it to the user.
The input to UCExpress is shownin Figure 25.
This input conceptual network is UCEgo'splan to tell the user the plan that KIP produced (Figure26, Section 7.3).
If the plan were to be directly gen-erated into English, the result might be something like:A plan for printing a file on the Imagen printer is touse the lpr command with the Imagen printer option.The format of the command is lpr -Pip and followedby the name of the file to be printed on the Imagenprinter.Instead, UCExpress prunes the output, and, since theuser is suspected of being a novice, generates anexample to illustrate the plan.
The following traceillustrates the processing of UCExpress.Computational Linguistics, Volume 14, Number 4, December 1988 73Robert Wilensky, David N. Chin, Marc Luria, James Martin, James Mayfield, and Dekai Wu The Berkeley UNIX Consultant ProjectExpress: now expressing the PLANFOR,(PLANFOR70(goals =PRINT-EFFECT0)(plan =(EXECUTE-UNIX-IPR-COMMAND0(ipr-file = FILE6)(ipr-execute-command = ' 'lpr -Pip")(ipr-format =(IPR-FORMAT0 (ipr-format-arg= NAME6))))))Express: not expressing PRINT-EFFECT0, since it is already in the context.Express: creating an examp!e for the incomplete plan, IPR-FORMAT0Express: choosing a name, foo, for an example file.Express: created the example(s):((TELL5(teller= UC)(listener= *USER*)(proposition=(EXAMPLE0(exampler =(PLANFOR3 l(goals= PRINT-EFFECT4)(plan =(TYPE-ACTION0(actor= *USER*)(type-string=(CONCAT-STR0(stringl ="lpr -Pip")(string2="foo")))))))(exemplified = PLANFOR70)))))Trace of UCExpress.The first phase of UCExpress is pruning, during whichthose concepts that the user already knows are markedso that the generator will not generate them.
In thiscase, PRINT-EFFECT0, the goal of PLANFOR70, ispruned, as it is in the current context.In traversing the input conceptual network, UCEx-press runs into the command format IPR-FORMAT0.Since this node is presumably not known to the user,UCExpress would normally not prune it.
However,UCExpress pecifically checks command formats to seeif they are as specific as possible, and since FILE6 doesnot have a name, UCExpress schedules the creation ofan example to explain the format, and prunes theoriginal node.
The rest of the network is passed withoutany pruning.In creating an example, UCExpress must specify allthe parameters in the command format.
Thus the nameof the file, which was not specified by the user, is madeexplicit in the example.
Here, the name "foo" waschosen arbitrarily from a list.
The complete xample isthen turned into the proposition part of a TELL(TELL5 in the trace).Figure 26 shows the conceptual network after prun-ing and the addition of an example.
Note that theexemplified of an example is automatically pruned, as itis always expressed by the expressing the genericresponse.
The pruned and augmented conceptual net-work is next passed to the generator, described in thenext section, which produces the following Englishoutput:Use lpr -Pip.
For example, to print the file named fooon the Imagen printer, type 'lpr -Pip foo'.If the user had been judged to be at least a beginner inexperience, then the command format also would havebeen pruned.
This is because KNOME believes thatusers at the beginner level and up can be assumed toknow that part of the command format.
In such a case,the entire output would consist of the pruned version ofTELL4, and the generated entire response would be:Use lpr -Pip.9 THE GENERATOR9.1 INTRODUCTIONAfter UCExpress formats an answer, the generator,UCGen, converts the conceptual response into text.The current version of UCGen has been customized towork with the types of responses that the systemtypically produces.
It has been built to take advantageof the limited structure of these responses.9.2 DESIGNTo convert a KODIAK representation f a concept intotext, UCGen must associate some linguistic informationwith the concept.
This is done by attaching to a concepta pattern that represents ome linguistic form.
For74 Computational Linguistics, Volume 14, Number 4, December 1988Robert Wilensky, David N. Chin, Marc Luria, James Martin, James Mayfield, and Dekai Wu The Berkeley UNIX Consultant Project~ O  " " pos,tlon PLANFO 301I teller listener plan --..,,I EXEC UTE-U NIX-IPR-COMMAND0 \]\I, /i E?A &E I/ ~ tenerpropositionctorexecute -~I "Ipr -Pip"71PLANFO 3Il exampler /AMPLEO ~'~I"  goal/s "p lan~,  NN~, /  I TYPE ACTION01 / I PRINT-EFFECTI I type string \ /print-fil~-object i CONCAT.STRO Istring2J imagen-dest/stringlI"IproPip" IFigure 26.
The Output of UCExpress for the Example "Do you know how to print a file on the Imagen?
"example, aconcept often expressed is PLANFOR.
Thisconcept relates a plan for achieving agoal with the goalitself.
A pattern for PLANFOR is:'To (gen goals) comma (gen plan)'.This pattern might be used to generate the sentence:To delete a file, use rm.This is somewhat akin to the pattern concept pairconstruct in PHRED (Jacobs 1984) or to KING's REFlinks (Jacobs 1985), although the KODIAK representa-tion accommodates different methods for fetching pat-terns.Patterns mix words and punctuation with functioncalls.
In the above example, 'gen' is a function that willbe called with argument 'goals' and later with argument'plan'.
In general, the arguments to functions that arefound in generator patterns are the aspectuals associ-ated with the concept o which the pattern is attached.In this example, the aspectuals of PLANFOR, 'goals'and 'plan', are arguments o gen.The pattern given above for PLANFOR is the mostgeneral one for that concept.
That is, it is the patternused when both the goals and the plan are to beexpressed.
As described in the previous section onUCExpress, it is not always necessary to express bothof these parts.
For example, two answers to "How do Idelete a file?"
are:I.
To delete a file, use rm.2.
Use rm.The expression mechanism puts a flag on each aspectualthat it does not want expressed.
Consequently, associ-ated with each concept may be zero or more patterns,one for each combination of aspectuals that are to beexpressed.
Planfor is associated with the general patternshown above, as is the pattern '(gen plan)', which isComputational Linguistics, Volume 14, Number 4, December 1988 75Robert Wilensky, David N. Chin, Marc Luria, James Martin, James Mayfield, and Dekai Wu The Berkeley UNIX Consultant Projectapplicable to the case where only tlhe plan is to beexpressed.When a concept o be output is given to the genera-tor, those KODIAK concepts that either dominate orare categories for the concept are searched for one thathas an attached generator pattern.
If no pattern isfound, and the concept is an aspectual, then the valuefor the aspectual is sent to the generator.
The firstpattern found is applied to the concept o be expressedto produce an English sentence.
Words in the patternare output as is.
Punctuation and function calls must gothrough further processing.
For example, in the pattern'To (gen goals) comma (gen plan)', the word 'To' isoutput directly, whereas the (gen .
.
. )
function callsmust be evaluated, and the 'comma' will be convertedto a ','.This generator is easy to understand and extend, andis well integrated with the rest of UC; it shares theKODIAK representation a d concepts used by the restof the system.
Some weaknesses are that the overallstructure is top down; i.e., only those concepts that areexpected to exist are expressed.
In general, a generatorshould be able to handle arbitrary permutations ofconceptual relationships.
Also, this generator uses littlelinguistic knowledge.
With more complicated utter-ances, the simple pattern strategies employed so farwould become inadequate.9.3 EXAMPLEThis section describes how the output is delivered byUC in response to the question, 'Do you know how toprint a file on the Imagen?'
.
A trace produced whilegenerating this output is given in Figure 27.
A diagramof some of the relevant knowledge structures i given inFigure 28.The expression mechanism of UCEgo first passesTELL4 to the generator.
Only the proposition partof the TELL will be expressed, so its value,PLANFOR70, is passed to the generator's main rou-tine, 'gen'.
PLANFOR70 is dominated by PLANFOR,so the pattern for PLANFOR is retrieved.
Since thegoals aspectual of PLANFOR70 is marked to be omittedfrom the response by the expression mechanism, onlythe plan will be expressed.
The pattern found is '(genplan)'.
The value of the plan aspectual, EXECUTE-UNIX-IPR-COMMAND0, is sent to gen.
The patternfor this concept is found under execute file-commandand is 'use (gen execute command)'.
The value ofexecute-command aspectual of EXECUTE-UNIX-IPR-COMMANDO is 'lpr -Pip'.
The first response is there-fore:Use lpr -Pip.Next, the generator is passed TELL5.
Once again, onlythe proposition is to be expressed, so EXAMPLE0 is tobe generated.
The pattern, found under EXAMPLE, is'for example comma (gen exampler)'.
This sets up aPattern for PLANFOR70 is: ((gen plan))Value for plan is: UNIX IPR COMMANDOPattern for UNIX IPR COMMANDO is: (use executecommand))Phrase derived from (execute command) is: (lpr -Pip)Phrase: derived from (gen plan) is: (use lpr -Pip)Use lpr -Pip.Pattern for fooPhrase derivedPhrase derivedPhrase derivedfoo rquote)Pattern for EXAMPLE0 is: (for example comma (genexampler))Value for exampler is: PLANFOR31Pattern for PLANFOR31 is: (to (gen goals) comma (genplan))Value for goals is: PRINT EFFECT1Pattern for PRINT EFFECT1 is:(print (las-pr-file-obj) on the (las-pr-dest-obj))Value for las-pr-file-obj is: FILE0Pattern for FILE0 is: (file named (gen name))Value for name is: fooPhrase derived from (file named (gen name)) is: (filenamed foo)Phrase derived from (las-pr-file-obj) is: (the file namedfoo)Value for las-pr-dest-obj is: IMAGEN0Pattern for IMAGEN0 is: (imagen)Phrase derived from (las-pr-dest-obj) is: (the imagen)Phrase derived from (gen goals) is:(print the file named foo on the imagen)Value for plan is: TYPE-ACTION0Pattern for TYPE-ACTION0 is: (type lquote (gen typestring) rquote)Value for type-string is: CONCAT-STR0Pattern for CONCAT-STR0 is: ((gen step) (gen next))Value for step is: lpr -PipPattern for lpr -Pip is: (lpr -Pip)Phrase derived from (gen step) is: (lpr -Pip)Value for next is: foois: (foo)from (gen next) is: (foo)from (gen type-string) is: (lpr -Pip foo)from (gen plan) is: (type lquote lpr -PipPhrase derived from (gen exampler) is:(to print the file named foo on the Imagen comma typelquote lpr -Pip foo rquote)For example, to print the file named foo on the Imagen,type lpr -Pip foo.Figure 27.
UCGen Trace for the Example Question.recursive call to gen with the value of the examplerrelation as argument.
This value is PLANFOR31.Once again, a PLANFOR is to be generated.
Thistime, however, both the plan and goals will be ex-pressed.
The pattern is 'to (gen goals) comma (gen76 Computatiional Linguistics, Volume 14, Number 4, December 1988Robert Wilensky, David N. Chin, Marc Luria, James Martin, James Mayfield, and Dekai Wu The Berkeley UNIX Consultant Project~.
~ '~ 7--~ ~ ?~-- ~?~- -~ ~ -~----~ Ne e?
~ I ~ I ~ I g~ I ~ .
- I " I  ~ ~--'-'-& > "/"%, ..-=?, ..1 "-?
No=Z ,<?z~kZ <X=/ ,  a(Oe=EEO uOoOe=O e-.=~OOOe=o~(lao~ ?-oO e=~aComputat iona l  Linguist ics,  Vo lume 14, Number  4, December  1988 77Robert Wilensky, David N. Chin, Marc Luria, James Martin, James Mayfield, and Dekai Wu The Berkeley UNIX Consultant Projectplan)'.
The value of the goals aspectual is PRINT-EFFECT1.
The pattern is found under LAS-PR-EFFECT; and is 'print (las-pr-file-obj) on the (las-pr-dest-obj)'.The las-pr-file-obj specification causes the generatorto find an object in this relation to PRINT-EFFECT1, inthis case, FILE0.
One of the patterns for FILE is 'filenamed (gen name)'.
This pattern is applicable if theobject in question has a name.
Here, FILE0 has thename 'foo'.
When UCGen is generating the name of anobject, it uses a heuristic to decide which, if any articleto precede the object description with.
In general,UCGen will use definite articles if it can.The other part of the output from the goals aspectualis from the second half of the pattern: 'on the (las-pr-dest-obj)'.
Here the value found is IMAGEN0.
Thepattern for IMAGEN0 is just 'imagen'.
Again, thegenerator will supplement this description with a defi-nite article.The value of the 'plan' aspectual for PLANFOR31 isTYPE-ACTION0.
The pattern for this concept is fromTYPE-ACTION and is 'type lquote (gen type-string)rquote'.
The value for the type string aspectual ofTYPE-ACTION0 is CONCAT-STR0.
The pattern isfrom sequence and is '(gen step) (gen next)'.
Here, thestep is 'lpr -Pip' (i.e., the stringl of CONCAT-STR0),and the next is the name of the file, 'foo' (the string2 ofCONCAT-STR0).
Thus the output for this call to thegenerator is:For example, to print the file named foo on theimagen, type lpr -Pip foo.10 KNOWLEDGE ACQUISITION10.1 INTRODUCTIONUC is knowledge dependent.
It is the task of theknowledge-acquisition system to facilitate the construc-tion and subsequent extension of the, knowledge base.The traditional approach to knowledge acquisition hasbeen to provide the system implementers with specialpurpose software tools such as text editors, form sys-tems, and graphic network editors (van Melle 1981).These approaches require that the person adding theknowledge have detailed knowledge of the knowledgerepresentation language being used and of the overalldesign of the system.
The typical situation, however, isthat the people with the domain knowledge do not havethe knowledge necessary to add information to a com-plex knowledge base.
The system builders must there-fore extract he information from the experts and thenprogram it in themselves.
This is a tedious, error proneprocess that is only viable as long as the system builderis in contact with the program.
If the system is in use ina real environment, he knowledge base is essentiallyfrozen.10.2 THE UC APPROACHOur solution to this problem is to provide a mechanismto allow the., interactive transfer of knowledge from thedomain expert to the knowledge base using naturallanguage.
This is similar to the approach taken in theNano-KLAUS system (Haas and Hendrix 1980).
UC-Teacher (Martin 1985) is the system being built toprovide this facility for UC.
The UNIX expert canengage UCTeacher in a mixed initiative dialogue to addmore information to UC's knowledge base.UCTeacher provides both the acquisition of UNIXdomain knowledge, and the acquisition of linguisticknowledge.
In particular, UCTeacher has a mechanismthat allows it to guess the metaphoric use of a term inUNIX, based on its knowledge of the metaphoric use ofthis term outside of UNIX.
In this report, we focuslargely on domain knowledge acquisition.
The mecha-nism of metaphor extension is beyond the scope of thisreport, and is described only briefly.10.3 ACQUIRING DOMAIN KNOWLEDGEIn the following examples of UCTeacher, the knowl-edge base of UC is set up so as not to contain knowledgeof the commands to be illustrated.
Also, the prompt'UCT>' is used to distinguish an interaction with UC-Teacher from one with UC proper.1.% UCTEACHERUCT> There is a new command.What is its name?UCT> sortWhat is it used for?UCT> It is used to sort files.You mean that typing 'sort filename' will sort the filefilename.UCT> Yes.2.UCT> Use the rm command to delete files.You mean that typing 'rm filename' will delete thefile named filename.UCT> Yes.3.UCT> Use compact o compress a file.You mean that typing 'compact filename' will com-press the file named filename.UCT> Yes.UCTeacher uses ALANA and the concretion mecha-nism to produce a representation f the initial utteranceby the expert.
In a typical interaction, the initial utter-ance will provide very sketchy information.
The mainjob of UCTeacher is realize what information is needed,determine what it is, and use it to flesh out the repre-sentation.
When it believes it has enough information,UCTeacher will paraphrase its understanding to theuser for conformation.
The user then has the opportu-nity to confirm or correct UCTeacher's understanding.UCTeacher has three main sources of knowledge:78 Computational \]Linguistics, Volume 14, Number 4, December 1988Robert Wilensky, David N. Chin, Marc Luria, James Martin, James Mayfield, and Dekai Wu The Berkeley UNIX Consultant Projectthe user's utterance, the contents of UC's knowledgebase, and a store of meta-knowledge associated withconcepts in the knowledge base.
This use of knowledgeplus meta-knowledge makes the knowledge acquisitionprocess a knowledge-based task (Davis 1976).UCTeacher uses these three sources of knowledge toacquire all the necessary information from the expert inas natural adialog as possible.
In Example 1, we see thesystem generating appropriate questions to acquire in-formation that the expert has not initially provided.
Thisability frees the expert from having to know all therelevant information that must be provided.
The systemwill prompt for all the necessary items.
Examples 2and3 show the system making full use of the expert'sutterance to avoid asking redundant questions.10.3.1 A DETAILED EXAMPLEIf UC is given a request it cannot service, it generallyasks the user to rephrase it.
Consider the followingsequence of interactions:4.# How can I print a file?I'm sorry, I didn't understand what you said.
Couldyou please rephrase that?At this point, UC does not know how to print a file.Now, an expert can enter UCTeacher and provide thenecessary information, as shown in the following inter-actions:% UCTEACHERUCT> There is a new command.What is its name?UCT> lprWhat is it used for?UCT> It is used to print files.Where does the output go?UCT> the lineprinterYou mean that typing 'lpr filename' will print the filefilename on the lineprinter.UCT> Yes.The system can now give a correct response to theuser's query.# How can I print a file?Use lpr.For example, to print the file named foo, type 'lprfoo'.Here, ALANA represents the content of the initialutterance as an instance of the quite general conceptEXECUTE-UNIX-COMMAND.
UCTeacher then col-lects from UC's knowledge base all the pertinent rela-tions that EXECUTE-UNIX-COMMAND participatesin.
These include the format aspectual of EXECUTE-UNIX-COMMAND, the plan aspectual of a PLANFORthat EXECUTE-UNIX-COMMAND constrains, andthe name relation that of UNIX-COMMAND, whichconstrains the command aspectual of EXECUTE-UNIX-COMMAND.
Figure 29 illustrates the knowl-edge in which these concepts are embedded.The relations needed to complete an EXECUTE-UNIX-COMMAND do not appear to conform to anyobvious pattern.
For example, the fact that a commandhas a name is several links away and the fact that thecommand should have a purpose are represented by itsparticipation i  a PLANFOR.
The difficulty of deter-mining the appropriate r lations and concepts needed tospecialize or instantiate a concept in the network has ledto the development of the notion of a minimal aspectualset (MAS).
The MAS constitutes the minimal set ofconcepts that must be present for a concept to beconsidered complete by UCTeacher.
UCTeacher usesthe MAS for the concept being acquired to limit itssearch through the knowledge base.
A relation notdirectly connected to the main concept being acquired isonly followed if it connects to a concept hat is presentin the MAS.For example, the MAS of EXECUTE-UNIX-COMMAND includes the command name aspectual ofUNIX-COMMAND, the unix goals aspectual of UNIX-PLANFOR, and the command-format aspectual of EX-ECUTE-UNIX-COMMAND.
From this description,UCTeacher can construct all the necessary componentsto specialize EXECUTE-UNIX-COMMAND.The system's first question in this example is anattempt to find a value for the relation command name.A piece of meta-knowledge used here, namely, that theonly way to fill in the value of a name relation is to askthe user, as it cannot be inferred from anything elseabout the command.
In the case where asking the useris the method, a pattern that is appropriate is includedas part of the meta-knowledge of that concept.
Here,the pattern gives rise to the question, and the answer isconnected to the representation.
Note that this requiresthe creation of a number of links and nodes, includingan execute command link to an instance of UNIX-COMMAND and a command-name link to the actualname.Whenever UCTeacher adds a new fact to the repre-sentation it is creating, it calls the concretion mecha-nism to see if it can classify the node to something morespecific.
If so, it may be able to use the more specificcategory to determine what else it needs to learn.
In thiscase, the concretion mechanism is called, but it cannotconcretize any further.The second question from UCTeacher is an attemptto instantiate the unix-goals relation of the UNIX-PLANFOR.
Again, the user is queried, and the re-sponse represented as a PRINT-FILE-EFFECT.
Aspecialization of UNIX-PLANFOR is constructed, anda unix-plan link is made to EXECUTE-UNIX-COMMAND, and a unix-goals link to PRINT-FILE-EFFECT.
Also, PRINT-FILE-EFFECT requires ades-tination to be complete.
This leads to the attempt to fillin the print-dest relation by asking the last question.Again, a concretion is attempted.
Here the concre-Computational Linguistics, Volume 14, Number 4, December 1988 79Robert Wilensky, David N. Chin, Marc Luria, ~\[ames Martin, James Mayfield, and Dekai Wu The Berkeley UNIX Consultant Project_/ ~ - Wo L~J ~'~/ ~ " ( ~ ~1~/iZO<i?~80 Computational Linguistics, Volume 14, Number 4, December 1988Robert Wilensky, David N. Chin, Marc Luria, James Martin, James Mayfield, and Dekai Wu The Berkeley UNIX Consultant Projecttion is successful; we are able to classify the conceptbeing built as an EXECUTE-FILE-COMMAND fact,rather than just an EXECUTE-UNIX-COMMANDfact.
At this point, UCTeacher searches for any newrelations that have been inherited because of the con-cretion and adds them to the list of relations to learn.
Italso recomputes the MAS that it is using, based on anyadditions from the concretion.The next relation that gets examined is commandformat.
Note that UCTeacher does not query the expertfor the argument format.
This is because meta-knowl-edge about he format of UNIX file commands specifiesthat the default format for such commands i a functionof the files involved; in this case, the default is SIM-PLE-FILE-FORMAT.Believing its knowledge acquisition process to becomplete, UCTeacher now generates a statement re-flecting what it has just learned.
This gives the expertthe opportunity to correct any incorrect inferencesmade by the system.
For example, if this command idnot take the usual argument format, the user would havean opportunity to correct it.The final phase of UCTeacher's processing is tomake the changes permanent.
Each absolute and rela-tion that has been instantiated is converted to a lineartuple form and printed to a file containing UC's knowl-edge base.
In the current version of UC, the linguisticknowledge used by the parser and generator are kept intwo, separate, non-KODIAK forms.
This poses a prob-lem for UCTeacher, which requires a rich knowledgerepresentation system to function properly.
As a tem-porary solution, templates are attached to concepts thathave linguistic ounterparts ( uch as command names);these templates are instantiated and written out to theappropriate files during the final phase of processing.This will be necessary until all linguistic knowledge isrepresented in KODIAK.10.4 EXTENDING METAPHORIC WORD SENSEMuch of the vocabulary with which users talk aboutcomputer systems is taken from other domains.
Forexample, consider utterances:You can get into the editor by typing emacs to theshell.How can I exit lisp?To leave the mail system, type 'exit'.You can kill a process by typing AC.My editor just died; how can I save the buffers?Run your file through spell to fix the spelling.These examples illustrate two facts: first, the ordinarylanguage we use to talk about computation is rife withmetaphor.
Second, the metaphors are systematic, as isargued at length in Lakoff and Johnson (1980).
Forexample, underlying the utterances "kill a process" and"a process died" is the understanding that a processcan be thought of as a living thing, and the terminationof the process as that thing's death.
Similarly, aninteractive process is thought of as a kind of region orcontainer, which one may enter, exit, leave, get into,etc.In addition, the metaphoric use of these terms is notconfined to the computer domain.
For example, notonly can one kill a process, but also, a light, a six-packof beer, a conversation, and a car engine, although eachof these has a different interpretation.
Thus, if we viewthese usages extensions of non-metaphoric terms, thekinds of metaphoric extensions we have for the com-puter domain are related to those outside this domain.For example, "kill a process" means terminating thatprocess, while "kill an engine" means terminating theprocess in which the engine is engaged.We have attempted to use this fact to aid in knowl-edge acquisition.
The strategy is as follows: First,represent the central senses of a word, and some of thecommon metaphoric extensions of that term.
Then,when such a term is encountered in a particular appli-cation, such as UC, in a context in which it cannot beunderstood, an attempt is made to compute a meta-phoric extension.
This is done by finding the closestmetaphoric extension from another domain.
This senseof the term is then modified to produce a sense appli-cable to the current context.In its current incarnation, UCTeacher can performthe following functions: Given that UC has the requisiteknowledge to directly understand the utterances con-taining the construction "enter lisp", UCTeacher cancompute the likely intended meaning of utterancesinvolving "exit lisp" and "enter mail"; given that UCknows about "kill a conversation", it can generalize to"kill a process"; from "kill a process", it can general-ize to "process died"; from "open a file" to "close afile", and finally, outside the UNIX domain, from "givea cold" to "have a cold", "get a cold", "give the flu",and "give an idea".To perform these functions, UCTeacher needs aboveall a representation formetaphoric structures.
The basisof such a representation is given in KODIAK by adevice called a view.
A view allows a kind of structuremapping between two concepts.
Views are described inWilensky (1986), but have been significantly extendedby Martin (1986a, 1986b, 1987, 1988).
A detailed de-scription of this representation, and of UCTeacher'salgorithm for extending word senses, is beyond thescope of this paper, but may be found in above refer-ences.11 PROBLEMSAs the preceding sections describe, there are manytechnical problems yet to be resolved for each compo-nent of UC.
However, several problems appear to bemore pervasive.One general problem is the integration of the compo-nents of the system.
Control flows unidirectionallythrough UC.
However, there are several cases in whichComputational Linguistics, Volume 14, Number 4, December 1988 81Robert Wilensky, David N. Chin, Marc Luria, James Martin, James Mayfield, and Dekai Wu The Berkeley UNIX Consultant Projectthis control structure is unsatisfactory.
One such prob-lem is the relation of language analy,ds and inference.We believe it is cognitively correct hat these compo-nents function concurrently to prodttce an interpreta-tion of an utterance, whereas in UC they functionserially.For example, consider again the process of under-standing the sentence we have been using in our ex-tended example: "Do you know how to print a file onthe Imagen?"
This utterance is syntactically ambiguousin the attachment of the prepositional phrase "on theImagen".
Syntactically, this may modify "you" or "afile" as well as "print".
UC does not deal with thisambiguity, because one of ALANA's patterns for"print" specifically looks for "on"  followed by adevice.
However, a more elaborate analyzer wouldprobably not include specific information that relatesthis preposition to the verb, but rather would try torelate them on more general principles.
In such asystem, the ambiguity would be a more difficult prob-lem.Our current approach is to build such a system anduse a marker passing algorithm (Norvig 1987) to helpsuggest which syntactic ombination to try.
For exam-ple, our knowledge about printing is such that a pathbetween printing and a device designed for printingshould be easy to find.
In contrast, here would be a lessobvious connection between Imagen and file, or Imagenand the referent of "you".
This "conceptual c oseness"would suggest trying to relate printing and the Imagenwith a grammatical pattern, so the correct interpretationwould be arrived at without other interpretations beingtested.Properly done, such a marker passing scheme wouldeffect concretion as well.
For example, to arrive at theconnection between printing and the Imagen, it isprobable that one needs to access the node for computerprinting.
Thus it seems that concretion should not be aseparate inference process, but one of several kinds ofinferences that are performed by a marker passingmechanism.
We are currently attempting to reform theanalyzer and the inference mechanism in the directiondescribed.It seems that the sort of unidirectional rchitecturewe have employed has drawbacks elsewhere in thesystem.
There are situations in which it seems that onecomponent should be allowed to fail, and the failure bepropagated back to another component.
For example,consider processing the following query:How can I edit Joe's file?Initially, the goal analyzer may interpret his requestliterally.
Then the planner may fail, because the file maybe protected from just such an action.
It seems reason-able, however, for a consultant to suggest copying thefile and editing the copy.
For this to happen, controlmust be returned to the goal analyzer, which needs tohypothesize yet another goal underlying the goal it mayhave suggested initially.
We are attempting to design acontrol structure that accommodates this flow of con-trol.The concretion mechanism and the goal analyzer alsoappea~r to interact in important ways.
For example,consider the following example:What does Is -v do?Above, we showed the UC can respond appropriately tothis question by uttering, "There is no -v option to theIs command."
However, the question is problematicbecause another esponse to it might be, "It lists thecontents of the current directory."
This response ispossible because, although there is no '-v' option to the'ls' command, it is a characteristic ofthis command thatit ignores options it does not recognize.
3To produce the desired response, the system mustrecognize that the intent of the question is somethinglike '"Fell me the conventional function of the commandIs -v", and not "Tell me what actually happens whenwe type Is -v".
One way to phrase this is that "conven-tional function" and "effects occurring from" are twokinds of "doing".
There are certainly other kinds aswell.
For example, the same form may refer to the stepsof a process.Therefore, it would appear to be the job of theconcretion mechanism to select the appropriate inter-pretation.
However, it seems that the concretion mech-anism cannot choose this interpretation without someknowledge of typical user goals.
For example, if a useris debugging a program, it would probably be appropri-ate to interpret he question as referring to the stepsincurred in the process rather than to the process'spurpose.
But reasoning about he user's goals is the jobof the goal analyzer, which normally is not invoked untilthe concretion mechanism has completed its task.The problem is avoided in the current implementa-tion by not allowing for the other, less obvious inter-pretations at all.
However, the example illustrates theneed to have more communication between the concre-tion mechanism and the goal analyzer.
Put morestrongly, the example suggests that these distinctionsbetween language analyzer, concretion mechanism, andgoal analyzer are somewhat artificial.
At this stage ofour work, it is difficult o determine whether we simplywant modules that interact more or a more radicalcontrol structure that integrates all these functions.There are several other more specific deficiencies ofwhich we are aware.
As we discussed previously,patterns were built into ALANA on an as-needed basis.We are attempting toproduce amore accurate languagespecification as we develop the inference component.Also, a mechanism for doing ellipsis, which ran in aprevious version of UC, has yet to be integrated intothis one.Undoubtedly, there are many deficiencies that wehave not yet discovered.
For example, we recentlydiscovered that asking the same question twice resulted82 Computational Linguistics, Volume 14, Number 4, December 1988Robert Wilensky, David N. Chin, Marc Luria, James Martin, James Mayfleld, and Dekai Wu The Berkeley UNIX Consultant Projectin no answer  at all being generated for the secondrequest.
The problem turned out to be that the usermodel,  after a quest ion is answered,  updates its modelof the user to show that the user now knows thisinformation.
The second time around,  this knowledgeal lowed the expression mechanism to prune away theentire answer,  as it inferred the user already knew it.Our approach to fixing this problem is to add anotherdemon that will detect asking for the same thing twice.Then plans for responding appropriately with this situ-ation could be brought o bear.One important  deficiency of our current system isthat it still doesn ' t  participate in real conversat ions.
It isour intent ion that UC funct ion as a consultant  and notas a front end to a data base of facts about UNIX.
Butour current system performs little more than this.
Muchof the machinery is in place, in UCEgo and PAGAN inparticular, to accommodate some conversat ional  situa-tions.
We expect much of our further development  to bein this direction.Final ly,  although we have found that our currentrepresentat ion is advantageous,  there are many repre-sentational issues that remain unresolved.
In particular,it is difficult to express certain aspects of quantif icationin KODIAK.
In UC, one often wants to represent factslike "al l  files have names"  or "most  directories are notempty . "
We are current ly working on extending KO-D IAK  to be able to represent such notions in a cogni-t ively plausible way.ACKNOWLEDGMENTSThis research was sponsored in part by the Defense AdvancedResearch Projects Agency (DoD), ARPA order No.
4871, monitoredby Space and Naval Warfare Systems Command Command undercontract N00039-84-C-0089, by the Office of Naval Research undercontract N00014-80-C-0732, and by the National Science Foundationunder grant MCS79-06543.In addition to the authors of this paper, several other people havemade significant contributions to the UC project and to this report.Richard Alterman played a major role in the overall design, organi-zation, and execution of this project.
He also made many specificcontributions to its individual components, especially UCGen andUCEgo, and was involved in many of the technical discussions uponwhich this work was based.
Peter Norvig wrote the original KODIAKinterpreter and contributed significantly to the development of thisrepresentation language.
Eric Karlson built the graphic interfaceprogram used to enter and edit KODIAK diagrams.
James Mayfieldtook charge of coordinating and integrating the components of thisreport into a single document.
Charles Cox and Anthony Albertcontributed to previous versions of this document.
Other members ofour group who contributed to this effort include Yigal Arens, MichaelBraverman, Margaret Butler, Paul Jacobs, Dan Jurafsky, Lisa Rau,and Nigel Ward.NOTESThe current address of Dr. James Mayfield is Computer ScienceDivision, University of Maryland Baltimore County, 5401Wilkens Avenue, Baltimore MD 21228; of Dr. David Chin,Department of Information Computer Science, University ofHawaii at Manoa, 2565 The Mall, Honolulu, HI 96822; of Dr.Marc Luria, Computer Science Division, Technion, Haifa, Israel.2.
UNIX is trademark of Bell Laboratories.3.
This "feature" has recently been changed to produce an errormessage on some versions of UNIX.REFERENCESAllen, James F.; Frisch, Alan M.; and Litman, Diane J.
1982 ARGOT:The Rochester Dialogue System.
In Proceedings of the NationalConference on Artificial Intelligence, Pittsburgh, PA.Allen, James F., and Perrauit, C. Raymond 1980 Analyzing Intentionin Utterances.
Artificial Intelligence 15: 143-178.Austin, John L. 1962 How To Do Things With Words.
HarvardUniversity Press, Cambridge, MA.Brachman, Ronaid and Schmolze, J.
1985 An Overview of theKL-ONE Knowledge Representation System.
In Cognitive Sci-ence 9: 171-216.Brown, John S. and Burton, Richard R. 1976 A Tutoring and StudentModeling Paradigm for Gaming Environments.
In the Symposiumon Computer Science and Education, Anaheim, CA: 236-246.Carberry, Sandra 1983 Tracking User Goals in an Information-Seeking Environment.
In Proceedings of the National Conferenceon Artificial Intelligence, Washington, D.C.Chin, David N. 1986 User Modeling in UC, the UNIX Consultant.
InProceedings of the CHI-86 Conference, Boston, MA.Chin, David N. 1988 Intelligent Agents and a Basis for InteractiveConsultation Systems.
Berkeley Computer Science Technical Re-port UCB/CSD 88/396.Cohen, Philip R. and Levesque, Hector J.
1987 Persistence, Intention,and Commitment.
SRI International Technical Report 415.Cox, Charles A.
1986 ALANA: Augmentable LANguage Analyzer,Report No.
UCB/CSD 86/283.
Computer Science Division, Uni-versity of California, Berkeley.Davis, Randall 1976 Applications of Meta Level Knowledge to theConstruction, Maintenance and Use of Large Knowledge Bases,Stanford CS Report, STAN-CS-76-552.
Stanford, CA.Deering, Michael; Faletti, Joseph; and Wilensky, Robert 1982 Usingthe PEARL AI Package, Memorandum No.
UCB/ERL M82/19.Computer Science Division, University of California, Berkeley.Ernst, G. and Newell, Allen 1969 GPS: A Case Study in Generalityand Problem Solving.
Academic Press, New York, NY.Fikes, Richard E., and Nilsson, Niis J.
1971 STRIPS: A NewApproach to the Application of Theorem Proving to ProblemSolving.
Artificial Intelligence 2(3--4): 189-208.Grice, H. Paul 1975 Logic and conversation.
I  Cole, P. and Morgan,J.L., (eds.)
Studies in Syntax III: 41-58, Seminar Press.Grosz, Barbara and Sidner, Candace L. 1985 The Structures ofDiscourse Structure Report No.
CSLI-85-39.
Center for the Studyof Language and Information.Haas, N. and Hendrix, Gary 1980 An Approach to Acquiring andApplying Knowledge.
In Proceedings of the National Conferenceon Artificial Intelligence, Stanford, CA: 235-239.Jacobs, Paul S. 1984 PHRED: A Generator for Natural LanguageInterfaces, Report No.
UCB/CSD 84/189.
Computer Science Di-vision, University of California, Berkeley.Jacobs, Paul S. 1985 A Knowledge-Based Approach to LanguageProduction.
Ph.D. thesis, University of California, Berkeley.Kaplan, S. Jerrold 1983 Cooperative Responses from a PortableNatural Language Database Query System.
In Brady, Michael andBerwick, Robert C.
(eds.)
Computational Models of Discourse.MIT Press, Cambridge, MA.Lakoff, George and Johnson, Mark 1980 Metaphors We Live By.University of Chicago Press, Chicago, IL.Litman, Diane J. and Allen, James F. 1984 A Plan Recognition Modelfor Clarification Subdialogues.
In Proceedings of the lOth Inter-national Conference on Computational Linguistics, Palo Alto,CA.Computational Linguistics, Volume 14, Number 4, December 1988 83Robert Wilensky, David N. Chin, Marc Luria, ,\]ames Martin, James Mayfield, and Dekai Wu The Berkeley UNIX Consultant ProjectLuria, Marc 1982 Dividing Up the Question Answering Process.
In theProceedings of the National Conference on Artificial Intelligence,Pittsburgh, PA: 71-74.Luria, Marc 1985 Commonsense Planning in a Consultant System.
InProceedings of the 1985 IEEE International Conference on Sys-tems, Man, and Cybernetics, Tucson, AZ: 602-606.Luria, Marc 1987 Goal Conflict Concerns.
Proceedings of the lOthInternational Joint Conference on Artificial Intelligence, Milan,Italy.Luria, Marc 1988 Knowledge Intensive Planning, Berkeley ComputerScience Technical Report No.
UCB/CSD88/433.Martin, James 1986a Views from a Kill.
In Proceedings of the EighthNational Conference of the Cognitive Science Society, Amherst,MA: 728-733.Martin, James 1986b Representing and Acquiring Knowledge aboutMetaphors.
In Proceedings of the Third Wo.,kshop on TheoreticalIssues in Conceptual Information Processing, Philadelphia, PA:49-54.Martin, James 1987 Understanding New Metaphors.
In Proceedingsof the lOth International Joint Conference on Artificial Intelli-gence.
Morgan Kaufmann, Milan, Italy.Martin, James 1988 Knowledge Acquisition: Understanding NewMetaphors, Berkeley Technical Report (forthcoming).Mays, E. 1980 Failures in Natural Language Systems: Applications toDatabase Query Systems.
In Proceedings of 1980 National Con-ference on Artificial Intelligence, Stanford, CA.McCoy, K.F.
1983 Correcting Misconceptions: What to Say When theUser is Mistaken.
In Proceedings of the CH1'83 Conference,Boston, MA: 197-201.Morik, K. and Rollinger, C-R. 1985 The Real Estate Agent--Modelingthe User by Uncertain Reasoning.
In A1 Magazine 6(2): 44-52.Newell, Allen, and Simon, Herbert A.
1972 Human Problem Solving.Prentice-Hall, Englewood Cliffs, NJ.Norvig, Peter 1983 Frame Activated Inferences in a Story Under-standing Program.
In Proceedings of the 8th International JointConference on Artificial Intelligence, Karlsruhe, West Germany:624--626.Norvig, Peter 1987 A Unified Theory of Inference for Text Under-standing, UC Berkeley Computer Science Report No.
UCB/CSD/87/339.Rich, Elaine 1979 User Modeling via Stereotypes.
In CognitiveScience 3: 329-354.Rissland, Edwina L. 1983 Examples in Legal Reasoning: LegalHypotheticals.
In Proceedings of the 8th International JointConference on Artificial Intelligence, Karlsruhe, West Germany:90-93.Rissland, Edwina L.: Valcarce, E.M., and Ashley, Kevin D. 1984Explaining and Arguing with Examples.
In Proceedings of theNational Conference on Artificial Inteh'igence, Austin, TX:288-294.Rosch, Eleanor 1978 Principles of Categorization.
In Rosch, Eleanorand Lloyd,, B.B.
Cognition and Categorization, Lawrence Erl-baum, Hillsdale, NJ.Sacerdoti, Earl 1974 Planning in a Hierarchy of Abstraction Spaces.In Artificial Intelligence 5:115-135.Schank, Roger C. 1975 Conceptual Information Processing.
NorthHolland, Amsterdam, Holland.Schmolze, J.G.
and Lipkis, T.A.
1983 Classification in the KL-ONEKnowledge Representation System.
In Proceedings of 8th Inter-national Joint Conference on Aritificial Intelligence.
Karlsruhe,West Germany.Schubert, Lenhart K. 1976 Extending the Expressive Power ofSemantic Networks.
Artificial lntengence 7: 163-195.Searle, John R. 1969 Speech Acts: An Essay in the Philosophy ofLanguage.
Cambridge University Press, Cambridge, England.Searle, John R. 1979 Literal Meaning.
In Searle, John R.
(ed.
)Expression and Meaning.
Cambridge University Press, Cam-bridge, England.Shapiro, Stuart 1979 The SNePS Semantic Network ProcessingSystem.
In Findler, N.
(ed.)
Associative Networks.
AcademicPress, New York, NY.Sidner, Candace L. 1985 Plan Parsing for Intended Response Recog-nition in Discourse.
Computational Intelligence 1: 1-10.Teitelman, Warren, et al 1978 The lnterlisp Reference Manual.Xerox PARC.van Melle, W. 1980 A Domain Independent System That Aids inConstructing Knowledge-Based Consultation Programs.
Heuris-tic Programming Project Report No.
HPP-80-22, Computer Sci-ence Department, Stanford University, Stanford, CA.Webber, Bonnie L. and Mays, E. 1983 Varieties of User Misconcep-tions: Detection and Correction.
In Proceedings of the 8th Inter-nathgnal Joint Conference on Artificial Intelligence, Karlsruhe,West Germany: 650-652.Wilensky, Robert 1983 Planning and Understanding: A Computa-tional Approach to Human Reasoning.
Addison-Wesley, Reading,MA.Wilensky, Robert 1986 Some Problems and Proposals for KnowledgeRepresentation.
Report No.
UCB/CSD 86/294.
Computer ScienceDivision, University of California, Berkeley.Wilensky, Robert 1987 Primal Content and Actual Content: AnAntidote to Literal Meaning.
Report No.
UCB/CSD 87/365.
Com-puter Science Division, University of California, Berkeley.Wilensky, Robert and Arens, Yigal 1980 A Knowledge-based Ap-proach to Natural Language Processing.
Electronic ResearchLaboratory Memorandum No.
UCB/ERL/M80/34.
University ofCalifornia.
Berkeley.Wilensky, Robert; Arens, Yigal; and Chin, David 1984 Talking toUnix in English: An Overview of UC.
Communications of theAssociation for Computing Machinery 27(6).84 Computational Linguistics, Volume 14, Number 4, December 1988
