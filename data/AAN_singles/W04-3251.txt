Instance-Based Question Answering:A Data-Driven ApproachLucian Vlad LitaCarnegie Mellon Universityllita@cs.cmu.eduJaime CarbonellCarnegie Mellon Universityjgc@cs.cmu.eduAbstractAnticipating the availability of large question-answer datasets, we propose a principled, data-driven Instance-Based approach to Question An-swering.
Most question answering systems incor-porate three major steps: classify questions accord-ing to answer types, formulate queries for documentretrieval, and extract actual answers.
Under our ap-proach, strategies for answering new questions aredirectly learned from training data.
We learn mod-els of answer type, query content, and answer ex-traction from clusters of similar questions.
We viewthe answer type as a distribution, rather than a classin an ontology.
In addition to query expansion, welearn general content features from training data anduse them to enhance the queries.
Finally, we treatanswer extraction as a binary classification problemin which text snippets are labeled as correct or in-correct answers.
We present a basic implementationof these concepts that achieves a good performanceon TREC test data.1 IntroductionEver since Question Answering (QA) emerged asan active research field, the community has slowlydiversified question types, increased question com-plexity, and refined evaluation metrics - as reflectedby the TREC QA track (Voorhees, 2003).
Startingfrom successful pipeline architectures (Moldovan etal., 2000; Hovy et al, 2000), QA systems have re-sponded to changes in the nature of the QA task byincorporating knowledge resources (Hermjakob etal., 2000; Hovy et al, 2002), handling additionaltypes of questions, employing complex reasoningmechanisms (Moldovan et al, 2003; Nyberg et al,2003), tapping into external data sources such asthe Web, encyclopedias, databases (Dumais et al,2002; Xu et al, 2003), and merging multiple agentsand strategies into meta-systems (Chu-Carroll et al,2003; Burger et al, 2002).In recent years, learning components have startedto permeate Question Answering (Clarke et al,2003; Ravichandran et al, 2003; Echihabi andMarcu, 2003).
Although the field is still domi-nated by knowledge-intensive approaches, compo-nents such as question classification, answer extrac-tion, and answer verification are beginning to be ad-dressed through statistical methods.
At the sametime, research efforts in data acquisition promise todeliver increasingly larger question-answer datasets(Girju et al, 2003; Fleischman et al, 2003).
More-over, Question Answering is expanding to differentlanguages (Magnini et al, 2003) and domains otherthan news stories (Zweigenbaum, 2003).
Thesetrends suggest the need for principled, statisti-cally based, easily re-trainable, language indepen-dent QA systems that take full advantage of largeamounts of training data.We propose an instance-based, data-driven ap-proach to Question Answering.
Instead of classify-ing questions according to limited, predefined on-tologies, we allow training data to shape the strate-gies for answering new questions.
Answer mod-els, query content models, and extraction models arealso learned directly from training data.
We presenta basic implementation of these concepts and eval-uate the performance.2 MotivationMost existing Question Answering systems classifynew questions according to static ontologies.
Theseontologies incorporate human knowledge about theexpected answer (e.g.
date, location, person), an-swer type granularity (e.g.
date, year, century), andvery often semantic information about the questiontype (e.g.
birth date, discovery date, death date).While effective to some degree, these ontologiesare still very small, and inconsistent.
Considerablemanual effort is invested into building and maintain-ing accurate ontologies even though answer typesare arguably not always disjoint and hierarchical innature (e.g.
?Where is the corpus callosum??
ex-pects an answer that is both location and body part).The most significant drawback is that ontologiesare not standard among systems, making individualcomponent evaluation very difficult and re-trainingfor new domains time-consuming.2.1 Answer ModelingThe task of determining the answer type of a ques-tion is usually considered a hard 1 decision prob-lem: questions are classified according to an an-swer ontology.
The classification (location, per-son?s name, etc) is usually made in the beginningof the QA process and all subsequent efforts arefocused on finding answers of that particular type.Several existing QA systems implement feedbackloops (Harabagiu et al, 2000) or full-fledged plan-ning (Nyberg et al, 2003) to allow for potential an-swer type re-classification.However, most questions can have multiple an-swer types as well as specific answer type distribu-tions.
The following questions can accommodateanswers of types: full date, year, and decade.Question AnswerWhen did Glen lift off in Friendship7?
Feb. 20, 1962When did Glen join NASA?
1959When did Glen have long hair?
the fiftiesHowever, it can be argued that date is the mostlikely answer type to be observed for the first ques-tion, year the most likely type for the second ques-tion, and decade most likely for the third ques-tion.
In fact, although the three questions can beanswered by various temporal expressions, the dis-tributions over these expressions are quite different.Existing answer models do not usually account forthese distributions, even though there is a clear po-tential for better answer extraction and more refinedanswer scoring.2.2 Document RetrievalWhen faced with a new question, QA systems usu-ally generate few, carefully expanded queries whichproduce ranked lists of documents.
The retrievalstep, which is very critical in the QA process,does not take full advantage of context information.However, similar questions with known answers doshare context information in the form of lexical andstructural features present in relevant documents.For example all questions of the type ?When wasX born??
find their answers in documents whichoften contain words such as ?native?
or ?record?,phrases such as ?gave birth to X?, and sometimeseven specific parse trees.Most IR research in Question Answering is fo-cused on improving query expansion and structur-1the answer is classified into a single class instead of gener-ating a probability distribution over answersing queries in order to take advantage of specificdocument pre-processing.
In addition to automaticquery expansion for QA (Yang et al, 2003), queriesare optimized to take advantage of expansion re-sources and document sources.
Very often, theseoptimizations are performed offline, based on thetype of question being asked.Several QA systems associate this type of infor-mation with question ontologies: upon observingquestions of a certain type, specific lexical featuresare sought in the retrieved documents.
These fea-tures are not always automatically learned in orderto be used in query generation.
Moreover, systemsare highly dependent on specific ontologies and be-come harder to re-train.2.3 Answer ExtractionGiven a set of relevant documents, the answer ex-traction step consists of identifying snippets of textor exact phrases that answer the question.
Manualapproaches to answer extraction have been mod-erately successful in the news domain.
Regularexpressions, rule and pattern-based extraction areamong the most efficient techniques for informationextraction.
However, because of the difficulty in ex-tending them to additional types of questions, learn-ing methods are becoming more prevalent.Current systems (Ravichandran et al, 2003) al-ready employ traditional information extraction andmachine learning for extracting answers from rel-evant documents.
Boundary detection techniques,finite state transducers, and text passage classifica-tion are a few methods that are usually applied tothis task.The drawback shared by most statistical answerextractors is their reliance on predefined ontologies.They are often tailored to expected answer types andrequire type-specific resources.
Gazetteers, ency-clopedias, and other resources are used to generatetype specific features.3 Related WorkCurrent efforts in data acquisition for Question An-swering are becoming more and more common.
(Girju et al, 2003) propose a supervised algorithmfor part-whole relations extraction.
(Fleischman etal., 2003) also propose a supervised algorithm thatuses part of speech patterns and a large corpus to ex-tract semantic relations for Who-is type questions.Such efforts promise to provide large and densedatasets required by instance based approaches.Several statistical approaches have proven to besuccessful in answer extraction.
The statisticalagent presented in (Chu-Carroll et al, 2003) usesTestQuestion: WhendidJohnGlenstartworkingatNASA?WhendidJayLenogetajobattheNBC?WhendidColumbusarriveathisdestination??Whendid<NNP+><VB>?at?
?Whendid?SonybeginitsV AIOcampaign?Whendid?T omRidgeinitiatetheterr oralertsystem??Whendid<NNP+><SYNSETto_initiate>??WhendidBeethovendie?WhendidMuhammadlive?
?Whendid<NNP><VB>?WhendidtheRaiderswintheirlastgame?WhendidEMNLP celebrateits5 th anniversary??Whendid<NNP+>?
?Whendiddinosaurswalktheearth?Whendidpeoplediscoverfir e??Whendid<NN><VB><NP>?Whendid<NP>?
?Figure 1: Neighboring questions are clustered according to features they share.maximum entropy and models answer correctnessby introducing a hidden variable representing theexpected answer type.
Large corpora such as theWeb can be mined for simple patterns (Ravichan-dran et al, 2003) corresponding to individual ques-tion types.
These patterns are then applied to testquestions in order to extract answers.
Other meth-ods rely solely on answer redundancy (Dumais etal., 2002): high performance retrieval engines andlarge corpora contribute to the fact that the most re-dundant entity is very often the correct answer.Predictive annotation (Prager et al, 1999) is oneof the techniques that bring together corpus process-ing and smarter queries.
Twenty classes of objectsare identified and annotated in the corpus, and cor-responding labels are used to enhance IR queries.Along the same lines, (Agichtein et al, 2001) pro-pose a method for learning query transformationsin order to improve answer retrieval.
The methodinvolves learning phrase features for question clas-sification.
(Wen and Zhang, 2003) address the prob-lem of query clustering based on semantic similar-ity and analyze several applications such as queryre-formulation and index-term selection.4 An Instance-Based ApproachThis paper presents a data driven, instance-basedapproach for Question Answering.
We adopt theview that strategies required in answering new ques-tions can be directly learned from similar train-ing examples (question-answer pairs).
Considera multi-dimensional space, determined by featuresextracted from training data.
Each training questionis represented as a data point in this space.
Featurescan range from lexical n-grams to parse trees ele-ments, depending on available processing.Each test question is also projected onto the fea-ture space.
Its neighborhood consists of traininginstances that share a number of features with thenew data point.
Intuitively, each neighbor is similarin some fashion to the new question.
The obviousnext step would be to learn from the entire neigh-borhood - similar to KNN classification.
However,due to the sparsity of the data and because differentgroups of neighbors capture different aspects of thetest question, we choose to cluster the neighborhoodinstead.
Inside the neighborhood, we build individ-ual clusters based on internal similarity.
Figure 1shows an example of neighborhood clustering.
No-tice that clusters may also have different granularity- i.e.
can share more or less features with the newquestion.Cluster1ModelsAnswerSet 1Cluster2ModelsAnswerSet 2Cluster3ModelsAnswerSet 3ClusterkModelsAnswerSet kNeighborhoodCluster2Cluster3ClusterkNewQuestionNET aggingPOSParsingCluster1Figure 2: The new question is projected onto the multi-dimensional feature space.
A set of neighborhood clus-ters are identified and a model is dynamically built foreach of them.
Each model is applied to the test questionin order to produce its own set of candidate answers.By clustering the neighborhood, we set the stagefor supervised methods, provided the clusters aresufficiently dense.
The goal is to learn models thatexplain individual clusters.
A model explains thedata if it successfully answers questions from itscorresponding cluster.
For each cluster, a mod-els is constructed and tailored to the local data.Models generating high confidence answers are ap-plied to the new question to produce answer candi-dates (Figure 2) Since the test question belongs tomultiple clusters, it benefits from different answer-seeking strategies and different granularities.Answering clusters of similar questions involvesseveral steps: learning the distribution of theexpected answer type, learning the structure andcontent of queries, and learning how to extract theanswer.
Although present in most systems, thesesteps are often static, manually defined, or based onlimited resources (section 2).
This paper proposes aset of trainable, cluster-specific models:1. the Answer Model Ai learns the cluster-specificdistribution of answer types.2.
the Query Content Model Ui is trained to enhancethe keyword-based queries with cluster-specificcontent conducive to better document retrieval.This model is orthogonal to query expansion.3.
the Extraction Model Ei is dynamically builtfor answer candidate extraction, by classifyingsnippets of text whether they contain a correctanswer or not.AnswerModelQueryContentModelExtractionModelClusterModelsTrainingSamples(Q, A)Figure 3: Three cluster-specific components are learnedin order to better retrieve relevant documents, model theexpected answer, and then extract it from raw text.
Localquestion-answer pairs (Q,A) are used as training data.These models are derived directly from clusterdata and collectively define a focused strategy forfinding answers to similar questions (Figure 3).4.1 The Answer ModelLearning cluster-specific answer type distributionsis useful not only in terms of identifying answersin running text but also in answer ranking.
A prob-abilistic approach has the advantage of postponinganswer type decisions from early in the QA processuntil answer extraction or answer ranking.
It alsohas the advantage of allowing training data to shapethe expected structure of answers.The answer modeling task consists of learningspecific answer type distributions for each cluster ofquestions.
Provided enough data, simple techniquessuch as constructing finite state machines or learn-ing regular expressions are sufficient.
The principlecan also be applied to current answer ontologies byreplacing the hard classification with a distributionover answer types.For high-density clusters, the problem of learn-ing the expected answer type is reduced to learn-ing possible answer types and performing a reliablefrequency count.
However, very often clusters aresparse (e.g.
are based on rare features) and a morereliable method is required.
k-nearest training datapoints Q1..Qk can be used in order to estimate theprobability that the test question q will observe ananswer type ?j :P (?j , q) = ?
?k?i=0P (?j |Qi) ?
?
(q,Qi) (1)where P (?j , Qi) is the probability of observingan answer of type ?j when asking question Qi.?
(q,Qi) represents a distance function between qand Qi, and ?
is a normalizing factor over the setof all viable answer types in the neighborhood of q.4.2 The Query Content ModelCurrent Question Answering systems use IR in astraight-forward fashion.
Query terms are extractedand then expanded using statistical and semanticsimilarity measures.
Documents are retrieved andthe top K are further processed.
This approach de-scribes the traditional IR task and does not take ad-vantage of specific constraints, requirements, andrich context available in the QA process.The data-driven framework we propose takes ad-vantage of knowledge available at retrieval timeand incorporates it to create better cluster-specificqueries.
In addition to query expansion, the goal isto learn content features: n-grams and paraphrases(Hermjakob et al, 2002) which yield better querieswhen added to simple keyword-based queries.
TheQuery Content Model is a cluster-specific collec-tion of content features that generate the best docu-ment set (Table 1).Cluster: When did X start working for Y?Simple Queries Query Content ModelX, Y ?X joined Y in?X, Y start working ?X started working for Y?X, Y ?start working?
?X was hired by Y?...
?Y hired X?X, Y ?job interview?...Table 1: Queries based only on X and Y questionterms may not be appropriate if the two entities sharea long history.
A focused, cluster-specific content modelis likely to generate more precise queries.For training, simple keyword-based queries arerun through a retrieval engine in order to producea set of potentially relevant documents.
Features(n-grams and paraphrases) are extracted and scoredbased on their co-occurrence with the correct an-swer.
More specifically, consider a positive class:documents which contain the correct answer, and anegative class: documents which do not contain theanswer.
We compute the average mutual informa-tion I(C;Fi) between a class of a document, andthe absence or presence of a feature fi in the doc-ument (McCallum and Nigam, 1998).
We let C bethe class variable and Fi the feature variable:I(C;Fi) = H(C) ?
H(C|Fi)=?c?C?fi?0,1P (c, fi) logP (c, fi)P (c)P (fi)where H(C) is the entropy of the class variable andH(C|Fi) is the entropy of the class variable condi-tioned on the feature variable.
Features that best dis-criminate passages containing correct answers fromthose that do not, are selected as potential candi-dates for enhancing keyword-based queries.For each question-answer pair, we generate can-didate queries by individually adding selected fea-tures (e.g.
table 1) to the expanded word-basedquery.
The resulting candidate queries are subse-quently run through a retrieval engine and scoredbased on the number of passages containing cor-rect answers (precision).
The content features foundin the top u candidate queries are included in theQuery Content Model.The Content Model is cluster specific and not in-stance specific.
It does not replace traditional queryexpansion - both methods can be applied simulta-neously to the test questions: specific keywords arethe basis for traditional query expansion and clus-ters of similar questions are the basis for learningadditional content conducive to better document re-trieval.
Through the Query Content Model we al-low shared context to play a more significant role inquery generation.4.3 The Extraction ModelDuring training, documents are retrieved for eachquestion cluster and a set of one-sentence passagescontaining a minimum number of query terms isselected.
The passages are then transformed intofeature vectors to be used for classification.
Thefeatures consist of n-grams, paraphrases, distancesbetween keywords and potential answers, simplestatistics such as document and sentence length, partof speech features such as required verbs etc.
Moreextensive sets of features can be found in informa-tion extraction literature (Bikel et al, 1999).Under our data-driven approach, answer extrac-tion consists of deciding the correctness of candi-date passages.
The task is to build a model thataccepts snippets of text and decides whether theycontain a correct answer.A classifier is trained for each question cluster.When new question instances arrive, the alreadytrained cluster-specific models are applied to new,relevant text snippets in order to test for correctness.We will refer to the resulting classifier scores as an-swer confidence scores.5 ExperimentsWe present a basic implementation of the instance-based approach.
The resulting QA system is fullyautomatically trained, without human intervention.Instance-based approaches are known to requirelarge, dense training datasets which are currentlyunder development.
Although still sparse, thesubset of all temporal questions from the TREC9-12 (Voorhees, 2003) datasets is relatively densecompared to the rest of the question space.
Thismakes it a good candidate for evaluating ourinstance-based QA approach until larger and denserdatasets become available.
It is also broad enoughto include different question structures and varyingdegrees of difficulty and complexity such as:?
?When did Beethoven die???
?How long is a quarter in an NBA game???
?What year did General Montgomery lead the Alliesto a victory over the Axis troops in North Africa?
?The 296 temporal questions and their correspond-ing answer patterns provided by NIST were usedin our experiments.
The questions were processedwith a part of speech tagger (Brill, 1994) and aparser (Collins, 1999).The questions were clustered using template-style frames that incorporate lexical items, parserlabels, and surface form flags (Figure 1).
Considerthe following question and several of its corre-sponding frames:?When did Beethoven die?
?when did <NNP> diewhen did <NNP> <VB>when did <NNP> <Q>when did <NP> <Q>when did <Q>where <NNP>,<NP>,<VB>,<Q> denote:proper noun, noun phrase, verb, and generic ques-tion term sequence, respectively.
Initially, framesare generated exhaustively for each question.
Eachframe that applies to more than three questions isthen selected to represent a specific cluster.One hundred documents were retrievedfor each query through the Google API(www.google.com/api).
Documents containingthe full question, question number, references toTREC, NIST, AQUAINT, Question Answering andother similar problematic content were filtered out.When building the Query Content Modelkeyword-based queries were initially formulatedand expanded.
From the retrieved documents a setof content features (n-grams and paraphrases) wereselected through average mutual information.
Thefeatures were added to the simple queries and anew set of documents was retrieved.
The enhancedqueries were scored and the corresponding top 10 n-grams/paraphrases were included in the Query Con-tent Model.
The maximum n-gram and paraphrasesize for these features was set to 6 words.The Extraction Model uses a support vector ma-chine (SVM) classifier (Joachims, 2002) with a lin-ear kernel.
The task of the classifier is to decide iftext snippets contain a correct answer.
The SVMwas trained on features extracted from one-sentencepassages containing at least one keyword from theoriginal question.
The features consist of: distancebetween keywords and potential answers, keyworddensity in a passage, simple statistics such as doc-ument and sentence length, query type, lexical n-grams (up to 6-grams), and paraphrases.We performed experiments using leave-one-outcross validation.
The system was trained and testedwithout any question filtering or manual input.
Eachcluster produced an answer set with correspond-ing scores.
Top 5 answers for each instance wereconsidered by a mean reciprocal rank (MRR) met-ric over all N questions: MRRN =?Ni=01ranki ,where ranki refers to the first correct occurrence inthe top 5 answers for question i.
While not the fo-cus of this paper, answer clustering algorithms arelikely to further improve performance.6 ResultsThe most important step in our instance-based ap-proach is identifying clusters of questions.
Figure4 shows the question distribution in terms of num-ber of clusters.
For example: 30 questions belongto exactly 3 clusters.
The number of clusters cor-responding to a question can be seen as a measureof how common the question is - the more clustersa question has, the more likely it is to have a denseneighborhood.The resulting MRR is 0.447 and 61.5% ques-tions have correct answers among the first five pro-posed answers.
This translates into results consis-tently above the sixth highest score at each TREC9-12.
Our results were compared directly to the topperforming systems?
results on the same temporal2 3 4 5 6 7 8 9 larger01020304050607080Question Distribution With Number of Clusters# clusters#questions(avg)Figure 4: Question distribution - each bar shows thenumber of questions that belong to exactly c clusters.1 2 3 4 5 6 7 801020304050607080Cluster Contribution to Top 10 Answers# clusters#questionsFigure 5: Number of clusters that contribute with cor-rect answers to the final answer set - only the top 10 an-swers were considered for each question.question test set.Figure 5 shows the degree to which clusters pro-duce correct answers to test questions.
Very often,more than one cluster contributes to the final answerset, which suggests that there is a benefit in cluster-ing the neighborhood according to different similar-ity features and granularity.It is not surprising that cluster size is not cor-related with performance (Figure 6).
The overallstrategy learned from the cluster ?When did <NP>die??
corresponds to an MRR of 0.79, while thestrategy learned from cluster ?How <Q>??
corre-sponds to an MRR of 0.13.
Even if the two clustersgenerate strategies with radically different perfor-mance, they have the same size - 10 questions arecovered by each cluster.Figure 7 shows that performance is correlatedwith answer confidence scores.
The higher the con-fidence threshold the higher the precision (MRR)of the predicted answers.
When small, unstableclusters are ignored, the predicted MRR improvesconsiderably.
Small clusters tend to produce unsta-0 20 40 60 80 100 12000.10.20.30.40.50.60.70.80.91Performance And Cluster Sizecluster sizeMRRFigure 6: Since training data is not uniformly distributedin the feature space, cluster size is not well correlatedwith performance.
A specific cardinality may represent asmall and dense part cluster, or a large and sparse cluster.0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.90.20.30.40.50.60.70.80.91Performance And Confidence Thresholdsconfidence thresholdMRRCardinality 2+Cardinality 3+Cardinality 4+Cardinality 5+Figure 7: MRR of predicted answers varies with answerconfidence thresholds.
There is a tradeoff between confi-dence threshold and MRR .
The curves represent differ-ent thresholds for minimum cluster size.ble strategies and have extremely low performance.Often times structurally different but semanticallyequivalent clusters have a higher cardinality andmuch better performance.
For example, the cluster?What year did <NP> die??
has cardinality 2 anda corresponding MRR of zero.
However, as seenpreviously, the cluster ?When did <NP> die??
hascardinality 10 and a corresponding MRR of 0.79.Table 2 presents an intuitive cluster and the top n-grams and paraphrases with most information con-tent.
Each feature has also a corresponding averagemutual information score.
These particular contentfeatures are intuitive and highly indicative of a cor-rect answer.
However, in sparse clusters, the con-tent features have less information content and aremore vague.
For example, the very sparse cluster?When was <Q>??
yields content features such as?April?, ?May?, ?in the spring of?, ?back in?
whichonly suggest broad temporal expressions.Cluster: When did <QTERM> die?N-grams Paraphrases0.81 his death in 0.80 <Q> died in0.78 died on 0.78 <Q> died0.77 died in 0.68 <Q> died on0.75 death in 0.58 <Q> died at0.73 of death 0.38 <Q> , who died0.69 to his death 0.38 <Q> dies0.66 died 0.38 <Q> died at the age of0.63 , born on 0.38 <Q> , born0.63 date of death 0.35 <Q> ?s death onTable 2: Query Content Model: learning n-grams andparaphrases for class ?When did <NP> die?
?, where<Q> refers to a phrase in the original question.7 ConclusionsThis paper presents an principled, statisticallybased, instance-based approach to Question An-swering.
Strategies and models required for answer-ing new questions are directly learned from trainingdata.
Since training requires very little human ef-fort, relevant context, high information query con-tent, and extraction are constantly improved withthe addition of more question-answer pairs.Training data is a critical resource for this ap-proach - clusters with very few data points are notlikely to generate accurate models.
However, re-search efforts involving data acquisition are promis-ing to deliver larger datasets in the near future andsolve this problem.
We present an implementationof the instance-based QA approach and we eval-uate it on temporal questions.
The dataset is ofreasonable size and complexity, and is sufficientlydense for applying instance-based methods.
We per-formed leave-one-out cross validation experimentsand obtained an overall mean reciprocal rank of0.447.
61.5% of questions obtained correct answersamong the top five which is equivalent to a score inthe top six TREC systems on the same test set.The experiments show that strategies derivedfrom very small clusters are noisy and unstable.When larger clusters are involved, answer confi-dence becomes correlated with higher predictiveperformance.
Moreover, when ignoring sparse data,answering strategies tend to be more stable.
Thissupports the need for more training data as means toimprove the overall performance of the data driven,instance based approach to question answering.8 Current & Future WorkData is the single most important resource forinstance-based approaches.
Currently we are ex-ploring large-scale data acquisition methods thatcan provide the necessary training data density formost question types, as well as the use of triviaquestions in the training process.Our data-driven approach to Question Answeringhas the advantage of incorporating learning com-ponents.
It is very easy to train and makes use ofvery few resources.
This property suggests that lit-tle effort is required to re-train the system for dif-ferent domains as well as other languages.
We planto apply instance-based QA to European languagesand test this hypothesis using training data acquiredthrough unsupervised means.More effort is required in order to better integratethe cluster-specific models.
Strategy overlap analy-sis and refinement of local optimization criteria hasthe potential to improve overall performance undertime constraints.ReferencesE.
Agichtein, S. Lawrence, and L. Gravano.
2001.Learning search engine specific query transfor-mations for question answering.
WWW.D.
Bikel, R. Schwartz, and R. Weischedel.
1999.An algorithm that learns what?s in a name.
Ma-chine Learning.E.
Brill.
1994.
Some advances in rule-based part ofspeech tagging.
AAAI.J.
Burger, L. Ferro, W. Greiff, J. Henderson,M.
Light, and S. Mardis.
2002.
Mitre?s qandaat trec-11.
TREC.J.
Chu-Carroll, K. Czuba, J. Prager, and A. Itty-cheriah.
2003.
In question answering, two headsare better than one.
HLT-NAACL.C.
Clarke, G. Cormack, G. Kemkes, M. Laszlo,T.
Lynam, E. Terra, and P. Tilker.
2003.
Statis-tical selection of exact answers.
TREC.M.
Collins.
1999.
Head-driven statistical modelsfor natural language parsing.
Ph.D. Disertation.S.
Dumais, M. Banko, E. Brill, J. Lin, and A. Ng.2002.
Web question answering: Is more alwaysbetter?
SIGIR.A.
Echihabi and D. Marcu.
2003.
A noisy channelapproach to question answering.
ACL.M.
Fleischman, E. Hovy, and A. Echihabi.
2003.Offline strategies for online question answering:Answering questions before they are asked.
ACL.R.
Girju, D. Moldovan, and A. Badulescu.
2003.Learning semantic constraints for the automaticdiscovery of part-whole relations.
HLT-NAACL.S.
Harabagiu, D. Moldovan, M. Pasca, R. Mihalcea,R.
Bunescu, R. Girju, V. Rus, and P. Morarescu.2000.
Falcon: Boosting knowledge for answerengines.
TREC.U.
Hermjakob, E. Hovy, and C. Lin.
2000.Knowledge-based question answering.
TREC.Ulf Hermjakob, Abdessamad Echihabi, and DanielMarcu.
2002.
Natural language based reformu-lation resource and web exploitation for questionanswering.
TREC.E.
Hovy, L. Gerber, U. Hermjakob, M. Junk, andC.Y.
Lin.
2000.
Question answering in webclo-pedia.
TREC.E.
Hovy, U. Hermjakob, C. Lin, and D. Ravichan-dran.
2002.
Using knowledge to facilitate factoidanswer pinpointing.
COLING.T.
Joachims.
2002.
Learning to classify text usingsupport vector machines.
Disertation.B.
Magnini, S. Romagnoli, A. Vallin, J. Herrera,A.
Penas, V. Peiado, F. Verdejo, and M. de Rijke.2003.
The multiple language question answeringtrack at clef 2003.
CLEF.A.
McCallum and K. Nigam.
1998.
A comparisonof event models for naive bayes text classifica-tion.
AAAI, Workshop on Learning for Text Cate-gorization.D.
Moldovan, S. Harabagiu, M. Pasca, R. Mihalcea,R.
Girju, R. Goodrum, and V. Rus.
2000.
Thestructure and performance of an open-domainquestion answering system.
ACL.D.
Moldovan, D. Clark, S. Harabagiu, and S. Maio-rano.
2003.
Cogex: A logic prover for questionanswering.
ACL.E.
Nyberg, T. Mitamura, J. Callan, J. Carbonell,R.
Frederking, K. Collins-Thompson, L. Hiyaku-moto, Y. Huang, C. Huttenhower, S. Judy, J. Ko,A.
Kupsc, L.V.
Lita, V. Pedro, D. Svoboda, andB.
Vand Durme.
2003.
A multi strategy approachwith dynamic planning.
TREC.J.
Prager, D. Radev, E. Brown, A. Coden, andV.
Samn.
1999.
The use of predictive annotationfor question answering in trec8.
TREC.D.
Ravichandran, A. Ittycheriah, and S. Roukos.2003.
Automatic derivation of surface text pat-terns for a maximum entropy based question an-swering system.
HLT-NAACL.E.M.
Voorhees.
2003.
Overview of the trec 2003question answering track.
TREC.J.R.
Wen and H.J.
Zhang.
2003.
Query clusteringin the web context.
IR and Clustering.J.
Xu, A. Licuanan, and R. Weischedel.
2003.
Trec2003 qa at bbn: Answering definitional ques-tions.
TREC.H.
Yang, T.S.
Chua, S. Wang, and C.K.
Koh.
2003.Structured use of external knowledge for event-based open domain question answering.
SIGIR.P.
Zweigenbaum.
2003.
Question answering inbiomedicine.
EACL.
