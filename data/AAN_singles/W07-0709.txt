Proceedings of the Second Workshop on Statistical Machine Translation, pages 64?71,Prague, June 2007. c?2007 Association for Computational Linguistics1Meta-Structure Transformation Modelfor Statistical Machine TranslationJiadong Sun, Tiejun, Zhao and Huashen LiangMOE-MS Key Lab of National Language Processing and speechHarbin Institute of TechnologyNo.
92, West Da-zhi Street ,Harbin Heilongjiang ,150001 ,Chinajiadongsun@hit.edu.cn{tjzhao, hsliang }@mtlab.hit.edu.cnAbstractWe propose a novel syntax-based modelfor statistical machine translation in whichmeta-structure (MS) and meta-structure se-quence (SMS) of a parse tree are defined.In this framework, a parse tree is decom-posed into SMS to deal with the structuredivergence and the alignment can be recon-structed at different levels of recombinationof MS (RM).
RM pairs extracted can per-form the mapping between the sub-structures across languages.
As a result,we have got not only the translation for thetarget language, but an SMS of its parsetree at the same time.
Experiments withBLEU metric show that the model signifi-cantly outperforms Pharaoh, a state-art-the-art phrase-based system.1 IntroductionThe statistical approach has been widely used inmachine translation, which use the noisy-channel-based model.
A joint probability model, proposedby Marcu and Wong (2002), is a kind of phrase-based one.
Och and Ney (2004) gave a frameworkof alignment templates for this kind of models.
Allof the phrase-based models outperformed theword-based models, by automatically learningword and phrase equivalents from bilingual corpusand reordering at the phrase level.
But it has beenfound that phrases longer than three words havelittle improvement in the performance (Koehn,2003).
Above the phrase level, these models have asimple distortion model that reorders phrases inde-pendently, without consideration of their contentsand syntactic information.In recent years, applying different statisticallearning methods to structured data has attractedvarious researchers.
Syntax-based MT approachesbegan with Wu (1997), who introduced the Inver-sion Transduction Grammars.
Utilizing syntacticstructure as the channel input was introduced intoMT by Yamada (2001).
Syntax-based models havebeen presented in different grammar formalisms.The model based on Head-transducer was pre-sented by Alshawi (2000).
Daniel Gildea (2003)dealt with the problem of the parse tree isomor-phism with a cloning operation to either tree-to-string or tree-to-tree alignment models.
Ding andPalmer (2005) introduced a version of probabilisticextension of Synchronous Dependency InsertionGrammars (SDIG) to deal with the pervasivestructure divergence.
All these approaches don?tmodel the translation process, but formalize amodel that generates two languages at the sametime, which can be considered as some kind of treetransducers.
Graehl and Knight (2004) describedthe use of tree transducers for natural languageprocessing and addressed the training problems forthis kind of transducers.In this paper, we define a model based on theMS decomposition of the parse trees for statisticalmachine translation, which can capture structuralvariations and has a proven generation capacity.During the translation process of our model, theparse tree of the source language is decomposedinto different levels of MS and then transformedinto the ones of the target language in the form ofRM.
The source language can be reordered accord-ing to the structure transformation.
At last, the tar-get translation string is generated in the scopes ofRM.
In the framework of this model,642Figure 1: MS and the SMS and RM for a given parser treethe RM transformation can be regarded as produc-tion rules and be extracted automatically from thebilingual corpus.
The overall translation probabil-ity is thus decomposed.In the rest of this paper, we first give thedefinitions for MS, SMS, RM and thedecomposition of the parse tree in section 2.1, wegive a detailed description of our model in section2.2, section 3 describes the training details andsection 4 describes the decoding algorithms, andthen the experiment (section 5) proves that ourmodel can outperform the baseline model,pharaoh, under the same condition.2 The model2.1 MS for a parse treeA source language sentence (s1 s2 s3 s4 s5 s6),and its parse tree S-P, are given in Figure 1.Wealso give the translation of the sentence, which isillustrated as (t1 t2 t3).Its parse tree is T-P.Definition 1MS of a parse treeWe call a sub-tree a MS of a parse tree, if it sat-isfies the following constraints:1.
An MS should be a sub-tree of a parse tree2.
Its direct sons of the leaf nodes in the sub-tree are the words or punctuations of the sen-tenceFor example, each of the sub-trees in the right-hand of Figure 1 is an MS for the parse tree of S-P.The sub-tree of [I [G, D, H]] of S-P is not an MS,because the direct sons of the leaf nodes, G, D, H,are not words in the sentence of (s1 s2 s3 s4 s5s6).Definition 2 SMS and RMA sequence of MS is called a meta-structuresequence (SMS) of a parse tree if and only if,1.
Its elements are MS of the parse tree2.
The parse tree can be reconstructed with theelements in the same order as in the sequence.It is denoted as SMS [T(S)].1 Two examplesfor the concept of SMS can be found in Figure1.RM(recombination of MS) is a sub-sequenceof SMS.
We can express an SMS as differ-ent )]([1 STRMk .The parse tree of S-P in Figure1is decomposed into SMS and expressed in theframework of RM.
The two RM, ][21 PSRM ?
,are used to express its parse tree in Figure1.It isnoted that there is structure divergence betweenthe two parse trees in Figure1.
The correspondingnode of Node I in the tree S-P cannot be found inthe tree T-P.
But under the conception of RM, thestructure alignments can be achieved at the levelof RM, which is illustrated in Figure2.Figure2.The RM alignments for S-P and T-P1 T[S] denotes the parse tree of a given sentencef and e denote the foreign and target sentences653In Figure2, both of the parse trees are decom-posed and reconstructed in the forms of RM.
Thealignments based on RM are illustrated at thesame time.2.2 Description of the modelIn the framework of Statistical machine transla-tion, the task is to find the sentence e for the givenforeign language f, which can be described in thefollowing formulation.
)}|(max{arg~fePee=                     (1)To make the model have the ability to modelthe structure transformation, some hidden vari-ables are introduced into the probability equation.To make the equations simple to read, we takesome denotations different from the above defini-tions.
SMS[T(S)] is denoted as SM[T(S)].The first variable is the SM[T(S)], we inducethe equation as follows??=))(()|)]([,()|(fTSMffTSMePfeP))],([|()|)]([()]([ffTSMePffTSMPfTSM?=?2??=)](SM[))],([|)](SM[())],([|(eTffTSMeTePffTSMeP??
?=)](SM[))],([|)](SM[(eTffTSMeTP))],([)],(SM[|( ffTSMeTeP                  (3)In order to simplify this model we have two as-sumptions:An assumption is that the generation of SMS [T(e)] is only related with SMS[T(f)]:))],([|)]([( ffTSMeTSMP)])([|)]([( fTSMeTSMP?
(4)Here we do all segmentations for any SMSof [T (f)] to get different )]([1 fTRMk .?
?==kiiifRMTfTRMeTRMPfTSMeTSMP1)]()])([|)]([()])([|)]([((5)The use of RM is to decompose bi-lingualparse trees and get the alignments in differenthierarchical levels of the structure.Now we have another assumption that all)|)]([( ffTSMP should have the same prob-ability?
.
A simplified form for this model isderived:=)|( feP??
?
?
))(( ))((fTSM eTSM))],([)],([|()])([|)]([()]([ 1ffTRMeTRMePfTRMeTRMPiifTRMkiii??
?=(6), Where ))],([)],([|( ffTRMeTRMeP ii can be re-garded as a lexical transformation process, whichwill be further decomposed.In order to model the direct translation processbetter by extending the feature functions, the di-rect translation probability is obtained in theframework of maximum entropy model:( )( )( )?
?
?===)]([)],([, 11)],([)],([,exp[)],([)],([,exp[|fTSMeTSMeMm mmMm mmffTSMeTSMeffTSMeTSMefePhh??
(7)We can achieve the translation according tothe function below:( ){ }?
== Mm mm ffTSMeTSMee h1 )],([)],([,exp[maxarg~ ?
(8)A detailed list of the feature functions for themodel and some explanations are given as below:z Just as the derivation in the model, we takeinto consideration of the structure trans-formation when selecting the features.
TheMS are combined in the forms of RM andtransformed as a whole structure.
( ) ?==kiii fTRMeTRMPfeh11)])([|)]([(log,(9)( ) ?==kiii eTRMfTRMPfeh12)])([|)]([(log,(10)z Features to model lexical transformationprocesses, and its inverted version, wherethe symbol L (RMi [T(S)]) denotes the664words belonging to this sub-structure in thesentence.
In Figure1, L (RM1) denotes thewords, s1 s2 s3, in the source language.This part of transformation happens in thescope of each RM, which means that allthe words in any RM can be transformedinto the target language words just in theway of phrase-based model, serving as an-other reordering factor at a different level:( ) ?==kiii fTRMLeTRMLPfeh13)])([(|)]))([((log,(11)( ) ?==kiii eTRMLfTRMLPfeh14)])([(|)]))([((log,(12)z We define a 3-gram model for the RM ofthe target language, which is called a struc-ture model according to the function of itin this model.
( ) )])([)],([|)]([(log, 1215eTRMeTRMeTRMPfe iikiih ?
?=?=(13)This feature can model the recombination ofthe parse structure of the target sentences.
Forexample in Figure3, ),|( BBAACCP  is used to de-scribe the probability of the RM sequence, (AA,BB) should be followed by RM (CC) in  thetranslation process.
This function can ensurethat a more reasonable sub-tree can be generatedfor the target language.
That would be explainedfurther in section 3.Figure3.
The 3-gram structure modelz The 3-gram language model is also used( ) )(log,6ePfeh =(14)The phrase-based model (Koehn, 2003) is aspecial case of this framework, if we take thewhole structure of the parse tree as the only MS ofthe parse tree of the sentence, and set some specialfeature weights to zero.From the description above, we know theframework of this model.
When transformed totarget languages, the source language is reorderedat the RM level first.
In this process, only theknowledge of the structure is taken intoconsideration.
It is obvious that a lot of sentencesin the source language can have the same RM.
Sothis model has better generative ability.
At thesame time, RM is a subsequence of SMS, whichconsists of different hierarchical MS.
So RM is astructure, which can model the structure mappingacross the sub-tree structure.
By decomposing thesource parse tree, the isomorphic between theparse trees can be obtained, at the level of RM.When reordering at the RM level, this modeljust takes an RM as a symbol, and it can perform along distance reordering job according to theknowledge of RM alignments.3 TrainingFor training the model, a parallel tree corpus isneeded.
The methods and details are described asfollows:3.1 Decomposition of the parse treeTo reduce the amount of MS used in decodingand training, we take some constrains for the MS.?1?.The height of the sub-tree shouldn?t begreater than a fixed value?
;?
2?.
???
)()(heightNnodesLeafNGiven a parse tree, we get the initial SMS insuch a top -down and left- to ?right way.Any node is deleted if the sub-tree can?t satisfythe constrains (1), (2).Figure3.
Decomposition of a parse tree67RMS for Ch-Parse Tree  RMS for EN-Parse Tree Pro for transformationAP[AP[AP[a-a]-usde]-m] NPB [DT-JJ-NN-PUNC.]
0.000155497AP[AP[AP[r-a]-usde]-m] NPB[PDT-DT-JJ-NN] 0.0151515AP[AP[BMP[m-q]-a]-usde] wj ADVP [RB-RB-PUNC.]
0.00344828AP[AP[BMP[m-q]-a]-usde] wj DT CD JJ NNS PUNC 0.0833333AP[AP[BMP[m-q]-a]-usde] wj DT JJ NN NNS PUNC.
0.015625Table 1 some examples of the RM transformationRM1            RM2 RM3 P(RM3|RM1,RM2)IN  NP-A[NPB[PRP-NN] IN 0.2479237NPB NP-A[NPB[PRP-NN] VBZ 0.2479235IN NP-A[NPB[PRP-NN] MD 0.6458637<s> NP-A[NPB[PRP-NN] VBD 0.904308Table 2 Examples for the 3-gram structure model of RMGenerate all of the SMS by deleting a node inany Ms to generate new SMS, applying the sameoperation to any SMS3.2 Parallel SMS and Estimation of the pa-rameters for RM transformationsWe can get bi-lingual SMS by recombining allthe possible SMS obtained from the parallelparse trees.
nm ?
Parallel SMS can be obtainedif m is the number of SMS for a parse tree in thesource language, n for the target one.The alignments of the parallel MS and extrac-tion can be performed in such a simple way.Given the parallel tree corpus, we first get thealignments based on the level of words, for whichwe used GIZA++ in both of the directions.
Ac-cording to the knowledge of the word alignments,we derived the alignments of leave nodes of thegiven parse trees, which are the direct root nodesof the words.
Then all the knowledge of the wordsis discarded for the RM extraction.
The next stepfor the extraction of the RM is based on the popu-lar phrase-extraction algorithm of the phrase-based statistical machine translation model.
Thepresent alignment and phrase extraction methodscan be applied to the extraction of the MS and RM[T(S)].
),(),()|(EiFiRMEIFiFiEI RMRMCountRMRMCountRMRMPEi?=),( BAountC is the expected number of times Ais aligned with B in the training corpus.Table1shows some parameters for this part in the model.Training n-gram model for the monolingualstructure model is based on the English RM ofeach parse tree, selected from the parallel tree cor-pus.
The 3-gram structure model is defined as fol-lows:=??
)])([)],([|)]([( 12 eTRMeTRMeTRMP iiI),,(),,(1212jIIjIIIRMRMRMCountRMRMRMCount?????
),,( CBAountC  is the times of the situation, inwhich the RM is consecutive sub-trees of theparse trees in the training set.
Some 3-gram pa-rameters in the training task are given in Table2.We didn?t meet with the serious data sparsenessproblem in this part of work, because most of theMS structures have occurred enough times forparameters estimation.
But we still set somefixed value for the unseen parameters in thetraining set.4 DecodingA beam search algorithm is applied to thismodel for decoding, which is based on the frameof the beam search for phrase-based statisticalmachine translation (Koehn et al 03).Here the process of the hypothesis generation ispresented.
Given a sentence and its parse tree, allthe possible candidate RM are collected, whichcan cover a part of the parse tree at the bottom.With the candidates, the hypotheses can beformed and extended.For example, all the parse tree?s leaf nodes of aChinese sentence in Figure4, are covered by [r],[ pron ] and  VP[vg-BNP[pron-n]] in the order ofchoosing candidate RM{ (1),  (2), (3)}.686Figure4.
Process of translation based on RM),( VBDWRBr                                              (1)??
?how did),( PRPpron                                                   (2)??you]])[[]],[[(NNDTNPBVBVPnpronBNPvgVP????(3)??
??
???
find the informationBefore the next expansion of a hypothesis, thewords in the scope of the present RM are trans-lated into the target language and the correspond-ing )]([ eTRM i  is generated.
For example, when),( VBDWRBr , is used to expand the hypothe-sis , the words in the sub-tree are translated intothe target language, ??
?how did.We also need to calculate the cost for the hy-potheses according to the parameters in the modelto perform the beam search.
The task for the beamsearch is to find the hypothesis with the least cost.When the expansion of a hypothesis comes to thefinal state, the target language is generated.
All ofthe leave nodes of the parse tree for the sourcelanguage are covered.
The parser for the targetlanguage isn?t used for decoding.
But a targetSMS is generated during the process of decodingto achieve better reordering performance.5 ExperimentsThe experiment was conducted for the task ofChinese-to-English translation.
A corpus, whichconsists of 602,701 sentence pairs, was used asthe training set.
We took CLDC 863 test set as ourtest set (http://www.chineseldc.org/resourse.asp),which consists of 467 sentences with an averagelength of 14.287 Chinese words and 4 references.To evaluate the result of the translation, the BLEUmetric (Papineni et al 2002) was used.5.1 The baselineSystem used for comparison was Pharaoh(Koehn et al, 2003; Koehn, 2004), which uses abeam search algorithm for decoding.
In its model,it takes the following features: language model,phrase translation probability in the two directions,distortion model, word penalty and phrase penalty,all of which can be achieved with the trainingtoolkits distributed by Koehn.
The training set anddevelopment set mentioned above were used toperform the training task and to tune the featureweights by the minimum error training algorithm.All the other settings were the same as the defaultones.
SRI Language Modeling Toolkit was usedto train a 3-gram language model.
After training,164 MB  language model were obtained.5.2 Our modelAll the common features shared with Pharaohwere trained with the same toolkits and the samecorpus.
Besides those features, we need to trainthe structure transformation model and the mono-lingual structure model for our model.
First,10,000 sentence pairs were selected to achieve the697BLEU-n n-gram precisions System4 1 2 3 4 5 6 7 8Pharaoh 0.2053 0.6449 0.4270 0.2919 0.2053 0.1480 0.1061 0.0752 0.0534Ms  sys-tem0.2232 0.6917 0.4605 0.3160 0.2232 0.1615 0.1163 0.0826 0.0587Table3.
Comparison of Pharaoh and our systemFeaturesSystem Plm(e) P(RT) P( IRT ) Pw( f|e ) Pw( e|f ) Word Phr Ph(RM)Pharaoh 0.151 ---- ------ 0.08 0.14 -0.29 0.26 -----MS sys-tem0.157 0.16 0.23 0.06 0.11 -0.20 0.22 0.36Table4.Feature weights obtained by minimum error rate training on development settraining set for this part of task.
The Collins parserand a Chinese parser of our own lab were used.After processing this corpus, we get a parallel treecorpus.
SRI Language Modeling Toolkits wereused again to train this part of parameters.
In thisexperiment, we set 3=?
,and 5.1=?
.
149MB)]([ sTRMS  pairs and a 25 MB 3-gram mono-lingual structure model were obtained.6.
Conclusion and Future workA framework for statistical machine translationis created in this paper.
The results of the experi-ments show that this model gives better perform-ance, compared with the baseline system.This model can incorporate the syntactic infor-mation into the process of translation and modelthe sub-structure projections across the parallelparse trees.The advantage of this frame work lies in thatthe reordering operations can be performed at thedifferent levels according to the hierarchical RMof the parse tree.But we should notice that some independent as-sumptions were made in the decomposition of theparse tree.
In the future, a proper method shouldbe introduced into this model to achieve the mostpossible decomposition of the parse tree.
In fact,we can incorporate some other feature functionsinto the model to model the structure transforma-tion more effectively.AcknowledgementThanks to the reviewers for their reviews andcomments  on improving our presentation of thispaper.ReferencesA.P.Dempster,N.M.Laird, and D.B.Rubin1977.Maximum likelihood from imcomplete datavia the EM algorithm.
Journal of the Royal Statisti-cal Society, 39(Ser B):1-38.Christoph Tillman.
A projection extension algorithmfor statistical machine translation.
Proceedings ofthe Conference on Empirical Methods in NaturalLanguage Processing, Sapporo, Japan, June 30-July4, 2003, 1-8.Daniel Gildea.2003.Loosely tree based alignment formachine translation.
In Proceedings of ACL-03Daniel Marcu, William Wong.
A phrase-based, jointprobability model for statistical machine translation.Proceedings of the Conference on Empirical Meth-ods in Natural Language Processing, Philadelphia,PA, USA, July 11-13, 2002, 133-139.Dekai Wu.
1997.
Stochastic inversion transductiongrammars and bilingual parsing of parallel corpora.Computational Linguistics, 23(3):3-403.F.Casacuberta, E. Vidal: Machine Translation withInferred Stochastic Finite-state Transducers.
Com-putational Linguistics, Vol.
30, No.
2, pp.
205-225,June 2004Franz J. Och, C. Tillmann, Hermann Ney.
Improvedalignment models for statistical machine translation.Proceedings of the Joint SIGDAT Conference onEmpirical Methods in Natural Language Processingand Very Large Corpora (EMNLP), College Park,MD, USA, June 21-22, 1999, 20-28.Franz J. Och, Hermann Ney.2002 Discriminative train-ing and maximum entropy models.
In Proceedings ofACL-00, pages 440-447, Hong Kong, Octorber.Hiyan Alshawi, Srinvas Bangalore, and Shona Douglas.2000.
Learning dependency translation models as708?collections of finite state head transducers Compu-tational Linguistics, 26(1):45-60.Ilya D. Melamed.
Automatic evaluation and uniformfilter cascades for inducing n-best translation lexi-cons.
Proceedings of the Third Workshop on VeryLarge Corpora, Boston, USA, July 30, 1995, 197-211.Jonathan Graehl Kevin Knight  Training Tree Trans-ducers  In Proceedings of NAACL-HLT 2004,pages 105-112.Kenji Yamada  and  Kevin Knight 2001.
A Syntax-based statistical translation model.
In Proceedingsof the 39th Annual Meeting of the association forcomputational Linguists(ACL  01), Toulouse,France, July 6-11Michael John Collins.
1999.
Head-driven statisticalModels for Natural Language Parsing.
Ph.D. the-sis,University of Pennsyvania,Philadelphia.P.
Koehn, Franz Josef Och, Daniel Marcu.
Statisticalphrase-based translation.
Proceedings of the Con-ference on Human Language Technology, Edmon-ton, Canada, May 27-June 1, 2003, 127-133.P.
Koehn: Pharaoh: a Beam Search Decoder forPhrase-based Statistical Machine Translation Mod-els .
Meeting of the American Association for ma-chine translation(AMTA), Washington DC, pp.
115-124 Sep./Oct.
2004Peter F. Brown ,Stephen A. Della Pietra,VincentJ.Della Pietra, and Robert Merrcer.1993.
Themathematics of statistical machine transla-tion:Parameter estimation.Computational Linguis-tics,19(2).
:263-311.Quirk, Chris, Arul Menezes, and Colin Cherry.
De-pendency Tree Translation.
Microsoft ResearchTechnical Report: MSR-TR-2004-113.Regina Barzilay and Lillian Lee.
2003.
Learning toparaphrase: An supervised approach using multiple-sequence alignment.
In Proceedings ofHLT/NAACLS.
Nie    en , H. Ney: Statistical Machine Translationwith Scarce Resources using Morpho-syntacticInformation.
Computational Linguistics, Vol.
30 No.2, pp.
181-204, June 20Yuan Ding and Martha Palmer.
2005.
Machine transla-tion using probabilistic synchronous dependency in-sert grammars.
In Proceedings of 43rd AnnualMeeting of the NAACL-HLT2004, pages 273-280..71
