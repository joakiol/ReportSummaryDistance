Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 1606?1615,Denver, Colorado, May 31 ?
June 5, 2015.c?2015 Association for Computational LinguisticsRetrofitting Word Vectors to Semantic LexiconsManaal Faruqui Jesse Dodge Sujay K. JauharChris Dyer Eduard Hovy Noah A. SmithLanguage Technologies InstituteCarnegie Mellon UniversityPittsburgh, PA, 15213, USA{mfaruqui,jessed,sjauhar,cdyer,ehovy,nasmith}@cs.cmu.eduAbstractVector space word representations are learnedfrom distributional information of words inlarge corpora.
Although such statistics aresemantically informative, they disregard thevaluable information that is contained in se-mantic lexicons such as WordNet, FrameNet,and the Paraphrase Database.
This paperproposes a method for refining vector spacerepresentations using relational informationfrom semantic lexicons by encouraging linkedwords to have similar vector representations,and it makes no assumptions about how the in-put vectors were constructed.
Evaluated on abattery of standard lexical semantic evaluationtasks in several languages, we obtain substan-tial improvements starting with a variety ofword vector models.
Our refinement methodoutperforms prior techniques for incorporat-ing semantic lexicons into word vector train-ing algorithms.1 IntroductionData-driven learning of word vectors that capturelexico-semantic information is a technique of cen-tral importance in NLP.
These word vectors canin turn be used for identifying semantically relatedword pairs (Turney, 2006; Agirre et al, 2009) oras features in downstream text processing applica-tions (Turian et al, 2010; Guo et al, 2014).
A vari-ety of approaches for constructing vector space em-beddings of vocabularies are in use, notably includ-ing taking low rank approximations of cooccurrencestatistics (Deerwester et al, 1990) and using internalrepresentations from neural network models of wordsequences (Collobert and Weston, 2008).Because of their value as lexical semantic repre-sentations, there has been much research on improv-ing the quality of vectors.
Semantic lexicons, whichprovide type-level information about the semanticsof words, typically by identifying synonymy, hyper-nymy, hyponymy, and paraphrase relations shouldbe a valuable resource for improving the quality ofword vectors that are trained solely on unlabeledcorpora.
Examples of such resources include Word-Net (Miller, 1995), FrameNet (Baker et al, 1998)and the Paraphrase Database (Ganitkevitch et al,2013).Recent work has shown that by either changingthe objective of the word vector training algorithmin neural language models (Yu and Dredze, 2014;Xu et al, 2014; Bian et al, 2014; Fried and Duh,2014) or by relation-specific augmentation of thecooccurence matrix in spectral word vector modelsto incorporate semantic knowledge (Yih et al, 2012;Chang et al, 2013), the quality of word vectors canbe improved.
However, these methods are limited toparticular methods for constructing vectors.The contribution of this paper is a graph-basedlearning technique for using lexical relational re-sources to obtain higher quality semantic vectors,which we call ?retrofitting.?
In contrast to previ-ous work, retrofitting is applied as a post-processingstep by running belief propagation on a graph con-structed from lexicon-derived relational informationto update word vectors (?2).
This allows retrofittingto be used on pre-trained word vectors obtainedusing any vector training model.
Intuitively, ourmethod encourages the new vectors to be (i) simi-lar to the vectors of related word types and (ii) simi-lar to their purely distributional representations.
Theretrofitting process is fast, taking about 5 seconds fora graph of 100,000 words and vector length 300, andits runtime is independent of the original word vec-tor training model.1606Figure 1: Word graph with edges between related wordsshowing the observed (grey) and the inferred (white)word vector representations.Experimentally, we show that our method workswell with different state-of-the-art word vector mod-els, using different kinds of semantic lexicons andgives substantial improvements on a variety ofbenchmarks, while beating the current state-of-the-art approaches for incorporating semantic informa-tion in vector training and trivially extends to mul-tiple languages.
We show that retrofitting givesconsistent improvement in performance on evalua-tion benchmarks with different word vector lengthsand show a qualitative visualization of the effect ofretrofitting on word vector quality.
The retrofittingtool is available at: https://github.com/mfaruqui/retrofitting.2 Retrofitting with Semantic LexiconsLet V = {w1, .
.
.
, wn} be a vocabulary, i.e, the setof word types, and ?
be an ontology that encodes se-mantic relations between words in V .
We represent?
as an undirected graph (V,E) with one vertex foreach word type and edges (wi, wj) ?
E ?
V ?
Vindicating a semantic relationship of interest.
Theserelations differ for different semantic lexicons andare described later (?4).The matrix?Q will be the collection of vector rep-resentations q?i?
Rd, for each wi?
V , learnedusing a standard data-driven technique, where d isthe length of the word vectors.
Our objective isto learn the matrix Q = (q1, .
.
.
, qn) such that thecolumns are both close (under a distance metric) totheir counterparts in?Q and to adjacent vertices in ?.Figure 1 shows a small word graph with such edgeconnections; white nodes are labeled with theQ vec-tors to be retrofitted (and correspond to V?
); shadednodes are labeled with the corresponding vectors in?Q, which are observed.
The graph can be interpretedas a Markov random field (Kindermann and Snell,1980).The distance between a pair of vectors is definedto be the Euclidean distance.
Since we want theinferred word vector to be close to the observedvalue q?iand close to its neighbors qj,?j such that(i, j) ?
E, the objective to be minimized becomes:?
(Q) =n?i=1???i?qi?
q?i?2+?(i,j)?E?ij?qi?
qj?2?
?where ?
and ?
values control the relative strengthsof associations (more details in ?6.1).In this case, we first train the word vectors inde-pendent of the information in the semantic lexiconsand then retrofit them.
?
is convex in Q and its so-lution can be found by solving a system of linearequations.
To do so, we use an efficient iterativeupdating method (Bengio et al, 2006; Subramanyaet al, 2010; Das and Petrov, 2011; Das and Smith,2011).
The vectors in Q are initialized to be equalto the vectors in?Q.
We take the first derivative of ?with respect to one qivector, and by equating it tozero arrive at the following online update:qi=?j:(i,j)?E?ijqj+ ?iq?i?j:(i,j)?E?ij+ ?i(1)In practice, running this procedure for 10 iterationsconverges to changes in Euclidean distance of ad-jacent vertices of less than 10?2.
The retrofittingapproach described above is modular; it can be ap-plied to word vector representations obtained fromany model as the updates in Eq.
1 are agnostic to theoriginal vector training model objective.Semantic Lexicons during Learning.
Our pro-posed approach is reminiscent of recent work onimproving word vectors using lexical resources (Yuand Dredze, 2014; Bian et al, 2014; Xu et al, 2014)which alters the learning objective of the originalvector training model with a prior (or a regularizer)that encourages semantically related vectors (in ?
)to be close together, except that our technique is ap-plied as a second stage of learning.
We describe the1607prior approach here since it will serve as a baseline.Here semantic lexicons play the role of a prior on Qwhich we define as follows:p(Q) ?
exp????n?i=1?j:(i,j)?E?ij?qi?
qj?2??
(2)Here, ?
is a hyperparameter that controls thestrength of the prior.
As in the retrofitting objec-tive, this prior on the word vector parameters forceswords connected in the lexicon to have close vec-tor representations as did ?
(Q) (with the role of?Qbeing played by cross entropy of the empirical dis-tribution).This prior can be incorporated during learn-ing through maximum a posteriori (MAP) estima-tion.
Since there is no closed form solution ofthe estimate, we consider two iterative procedures.In the first, we use the sum of gradients of thelog-likelihood (given by the extant vector learningmodel) and the log-prior (from Eq.
2), with respectto Q for learning.
Since computing the gradient ofEq.
2 has linear runtime in the vocabulary size n, weuse lazy updates (Carpenter, 2008) for every k wordsduring training.
We call this the lazy method ofMAP.
The second technique applies stochastic gra-dient ascent to the log-likelihood, and after every kwords applies the update in Eq.
1.
We call this theperiodic method.
We later experimentally comparethese methods against retrofitting (?6.2).3 Word Vector RepresentationsWe now describe the various publicly available pre-trained English word vectors on which we will testthe applicability of the retrofitting model.
Thesevectors have been chosen to have a balanced mixbetween large and small amounts of unlabeled textas well as between neural and spectral methods oftraining word vectors.Glove Vectors.
Global vectors for word represen-tations (Pennington et al, 2014) are trained on ag-gregated global word-word co-occurrence statisticsfrom a corpus, and the resulting representationsshow interesting linear substructures of the wordvector space.
These vectors were trained on 6 bil-lion words from Wikipedia and English GigawordLexicon Words EdgesPPDB 102,902 374,555WordNetsyn148,730 304,856WordNetall148,730 934,705FrameNet 10,822 417,456Table 1: Approximate size of the graphs obtained fromdifferent lexicons.and are of length 300.1Skip-Gram Vectors (SG).
The word2vectool (Mikolov et al, 2013a) is fast and currently inwide use.
In this model, each word?s Huffman codeis used as an input to a log-linear classifier witha continuous projection layer and words within agiven context window are predicted.
The availablevectors are trained on 100 billion words of Googlenews dataset and are of length 300.2Global Context Vectors (GC).
These vectors arelearned using a recursive neural network that incor-porates both local and global (document-level) con-text features (Huang et al, 2012).
These vectorswere trained on the first 1 billion words of EnglishWikipedia and are of length 50.3Multilingual Vectors (Multi).
Faruqui and Dyer(2014) learned vectors by first performing SVD ontext in different languages, then applying canonicalcorrelation analysis (CCA) on pairs of vectors forwords that align in parallel corpora.
The monolin-gual vectors were trained on WMT-2011 news cor-pus for English, French, German and Spanish.
Weuse the Enligsh word vectors projected in the com-mon English?German space.
The monolingual En-glish WMT corpus had 360 million words and thetrained vectors are of length 512.44 Semantic LexiconsWe use three different semantic lexicons to evaluatetheir utility in improving the word vectors.
We in-clude both manually and automatically created lexi-cons.
Table 1 shows the size of the graphs obtained1http://www-nlp.stanford.edu/projects/glove/2https://code.google.com/p/word2vec3http://nlp.stanford.edu/?socherr/ACL2012_wordVectorsTextFile.zip4http://cs.cmu.edu/?mfaruqui/soft.html1608from these lexicons.PPDB.
The paraphrase database (Ganitkevitch etal., 2013) is a semantic lexicon containing more than220 million paraphrase pairs of English.5Of these, 8million are lexical (single word to single word) para-phrases.
The key intuition behind the acquisition ofits lexical paraphrases is that two words in one lan-guage that align, in parallel text, to the same word ina different language, should be synonymous.
For ex-ample, if the words jailed and imprisoned are trans-lated as the same word in another language, it maybe reasonable to assume they have the same mean-ing.
In our experiments, we instantiate an edge inE for each lexical paraphrase in PPDB.
The lexicalparaphrase dataset comes in different sizes rangingfrom S to XXXL, in decreasing order of paraphras-ing confidence and increasing order of size.
Wechose XL for our experiments.
We want to givehigher edge weights (?i) connecting the retrofittedword vectors (q) to the purely distributional wordvectors (q?)
than to edges connecting the retrofittedvectors to each other (?ij), so all ?iare set to 1 and?ijto be degree(i)?1(with i being the node the up-date is being applied to).6WordNet.
WordNet (Miller, 1995) is a largehuman-constructed semantic lexicon of Englishwords.
It groups English words into sets of syn-onyms called synsets, provides short, general defini-tions, and records the various semantic relations be-tween synsets.
This database is structured in a graphparticularly suitable for our task because it explicitlyrelates concepts with semantically aligned relationssuch as hypernyms and hyponyms.
For example, theword dog is a synonym of canine, a hypernym ofpuppy and a hyponym of animal.
We perform twodifferent experiments with WordNet: (1) connectinga word only to synonyms, and (2) connecting a wordto synonyms, hypernyms and hyponyms.
We referto these two graphs as WNsynand WNall, respec-tively.
In both settings, all ?iare set to 1 and ?ijtobe degree(i)?1.5http://www.cis.upenn.edu/?ccb/ppdb6In principle, these hyperparameters can be tuned to opti-mize performance on a particular task, which we leave for fu-ture work.FrameNet.
FrameNet (Baker et al, 1998; Fill-more et al, 2003) is a rich linguistic resourcecontaining information about lexical and predicate-argument semantics in English.
Frames can be re-alized on the surface by many different word types,which suggests that the word types evoking the sameframe should be semantically related.
For exam-ple, the frame Cause change of position on a scaleis associated with push, raise, and growth (amongmany others).
In our use of FrameNet, two wordsthat group together with any frame are given an edgein E. We refer to this graph as FN.
All ?iare set to1 and ?ijto be degree(i)?1.5 Evaluation BenchmarksWe evaluate the quality of our word vector represen-tations on tasks that test how well they capture bothsemantic and syntactic aspects of the representationsalong with an extrinsic sentiment analysis task.Word Similarity.
We evaluate our word represen-tations on a variety of different benchmarks thathave been widely used to measure word similarity.The first one is the WS-353 dataset (Finkelstein etal., 2001) containing 353 pairs of English words thathave been assigned similarity ratings by humans.The second benchmark is the RG-65 (Rubensteinand Goodenough, 1965) dataset that contain 65 pairsof nouns.
Since the commonly used word similar-ity datasets contain a small number of word pairswe also use the MEN dataset (Bruni et al, 2012) of3,000 word pairs sampled from words that occur atleast 700 times in a large web corpus.
We calculatecosine similarity between the vectors of two wordsforming a test item, and report Spearman?s rank cor-relation coefficient (Myers and Well, 1995) betweenthe rankings produced by our model against the hu-man rankings.Syntactic Relations (SYN-REL).
Mikolov et al(2013b) present a syntactic relation dataset com-posed of analogous word pairs.
It contains pairsof tuples of word relations that follow a commonsyntactic relation.
For example, given walking andwalked, the words are differently inflected forms ofthe same verb.
There are nine different kinds of rela-tions and overall there are 10,675 syntactic pairs ofword tuples.
The task is to find a word d that best1609fits the following relationship: ?a is to b as c is to d,?given a, b, and c. We use the vector offset method(Mikolov et al, 2013a; Levy and Goldberg, 2014),computing q = qa?
qb+ qcand returning the vectorfrom Q which has the highest cosine similarity to q.Synonym Selection (TOEFL).
The TOEFL syn-onym selection task is to select the semanticallyclosest word to a target from a list of four candi-dates (Landauer and Dumais, 1997).
The datasetcontains 80 such questions.
An example is ?rug?
{sofa, ottoman, carpet, hallway}?, with carpet be-ing the most synonym-like candidate to the target.Sentiment Analysis (SA).
Socher et al (2013)created a treebank containing sentences annotatedwith fine-grained sentiment labels on phrases andsentences from movie review excerpts.
The coarse-grained treebank of positive and negative classeshas been split into training, development, and testdatasets containing 6,920, 872, and 1,821 sentences,respectively.
We train an `2-regularized logistic re-gression classifier on the average of the word vectorsof a given sentence to predict the coarse-grained sen-timent tag at the sentence level, and report the test-set accuracy of the classifier.6 ExperimentsWe first show experiments measuring improvementsfrom the retrofitting method (?6.1), followed bycomparisons to using lexicons during MAP learn-ing (?6.2) and other published methods (?6.3).
Wethen test how well retrofitting generalizes to otherlanguages (?6.4).6.1 RetrofittingWe use Eq.
1 to retrofit word vectors (?3) usinggraphs derived from semantic lexicons (?4).Results.
Table 2 shows the absolute changes inperformance on different tasks (as columns) withdifferent semantic lexicons (as rows).
All of the lexi-cons offer high improvements on the word similaritytasks (the first three columns).
On the TOEFL task,we observe large improvements of the order of 10absolute points in accuracy for all lexicons exceptfor FrameNet.
FrameNet?s performance is weaker,in some cases leading to worse performance (e.g.,with Glove and SG vectors).
For the extrinsic senti-ment analysis task, we observe improvements usingall the lexicons and gain 1.4% (absolute) in accuracyfor the Multi vectors over the baseline.
This increaseis statistically significant (p < 0.01, McNemar).We observe improvements over Glove and SGvectors, which were trained on billions of tokens onall tasks except for SYN-REL.
For stronger base-lines (Glove and Multi) we observe smaller im-provements as compared to lower baseline scores(SG and GC).
We believe that FrameNet does notperform as well as the other lexicons because itsframes group words based on very abstract concepts;often words with seemingly distantly related mean-ings (e.g., push and growth) can evoke the sameframe.
Interestingly, we almost never improve onthe SYN-REL task, especially with higher baselines,this can be attributed to the fact that SYN-REL is in-herently a syntactic task and during retrofitting weare incorporating additional semantic information inthe vectors.
In summary, we find that PPDB givesthe best improvement maximum number of timesaggreagted over different vetor types, closely fol-lowed by WNall, and retrofitting gives gains acrosstasks and vectors.
An ensemble lexicon, in whichthe graph is the union of the WNalland PPDBlexicons, on average performed slightly worse thanPPDB; we omit those results here for brevity.6.2 Semantic Lexicons during LearningTo incorporate lexicon information during training,and compare its performance against retrofitting,we train log-bilinear (LBL) vectors (Mnih and Teh,2012).
These vectors are trained to optimize thelog-likelihood of a language model which predictsa word token w?s vector given the set of words in itscontext (h), also represented as vectors:p(w | h;Q) ?
exp(?i?hq>iqj+ bj)(3)We optimize the above likelihood combined with theprior defined in Eq.
2 using the lazy and periodictechniques described in ?2.
Since it is costly to com-pute the partition function over the whole vocab-ulary, we use noise constrastive estimation (NCE)to estimate the parameters of the model (Mnih andTeh, 2012) using AdaGrad (Duchi et al, 2010) witha learning rate of 0.05.1610Lexicon MEN-3k RG-65 WS-353 TOEFL SYN-REL SAGlove 73.7 76.7 60.5 89.7 67.0 79.6+PPDB 1.4 2.9 ?1.2 5.1 ?0.4 1.6+WNsyn0.0 2.7 0.5 5.1 ?12.4 0.7+WNall2.2 7.5 0.7 2.6 ?8.4 0.5+FN ?3.6 ?1.0 ?5.3 2.6 ?7.0 0.0SG 67.8 72.8 65.6 85.3 73.9 81.2+PPDB 5.4 3.5 4.4 10.7 ?2.3 0.9+WNsyn0.7 3.9 0.0 9.3 ?13.6 0.7+WNall2.5 5.0 1.9 9.3 ?10.7 ?0.3+FN ?3.2 2.6 ?4.9 1.3 ?7.3 0.5GC 31.3 62.8 62.3 60.8 10.9 67.8+PPDB 7.0 6.1 2.0 13.1 5.3 1.1+WNsyn3.6 6.4 0.6 7.3 ?1.7 0.0+WNall6.7 10.2 2.3 4.4 ?0.6 0.2+FN 1.8 4.0 0.0 4.4 ?0.6 0.2Multi 75.8 75.5 68.1 84.0 45.5 81.0+PPDB 3.8 4.0 6.0 12.0 4.3 0.6+WNsyn1.2 0.2 2.2 6.6 ?12.3 1.4+WNall2.9 8.5 4.3 6.6 ?10.6 1.4+FN 1.8 4.0 0.0 4.4 ?0.6 0.2Table 2: Absolute performance changes with retrofitting.
Spearman?s correlation (3 left columns) and accuracy (3right columns) on different tasks.
Higher scores are always better.
Bold indicates greatest improvement for a vectortype.Method k, ?
MEN-3k RG-65 WS-353 TOEFL SYN-REL SALBL (Baseline) k =?, ?
= 0 58.0 42.7 53.6 66.7 31.5 72.5LBL + Lazy?
= 1 ?0.4 4.2 0.6 ?0.1 0.6 1.2?
= 0.1 0.7 8.1 0.4 ?1.4 0.7 0.8?
= 0.01 0.7 9.5 1.7 2.6 1.9 0.4LBL + Periodick = 100M 3.8 18.4 3.6 12.0 4.8 1.3k = 50M 3.4 19.5 4.4 18.6 0.6 1.9k = 25M 0.5 18.1 2.7 21.3 ?3.7 0.8LBL + Retrofitting ?
5.7 15.6 5.5 18.6 14.7 0.9Table 3: Absolute performance changes for including PPDB information while training LBL vectors.
Spearman?scorrelation (3 left columns) and accuracy (3 right columns) on different tasks.
Bold indicates greatest improvement.We train vectors of length 100 on the WMT-2011news corpus, which contains 360 million words,and use PPDB as the semantic lexicon as it per-formed reasonably well in the retrofitting experi-ments (?6.1).
For the lazy method we update withrespect to the prior every k = 100,000 words7and test for different values of prior strength ?
?
{1, 0.1, 0.01}.
For the periodic method, we up-date the word vectors using Eq.
1 every k ?
{25, 50, 100} million words.7k = 10,000 or 50,000 yielded similar results.Results.
See Table 3.
For lazy, ?
= 0.01 performsbest, but the method is in most cases not highly sen-sitive to ?
?s value.
For periodic, which overall leadsto greater improvements over the baseline than lazy,k = 50M performs best, although all other valuesof k also outperform the the baseline.
Retrofitting,which can be applied to any word vectors, regardlessof how they are trained, is competitive and some-times better.1611Corpus Vector Training MEN-3k RG-65 WS-353 TOEFL SYN-REL SAWMT-11CBOW 55.2 44.8 54.7 73.3 40.8 74.1Yu and Dredze (2014) 50.1 47.1 53.7 61.3 29.9 71.5CBOW + Retrofitting 60.5 57.7 58.4 81.3 52.5 75.7WikipediaSG 76.1 66.7 68.6 72.0 40.3 73.1Xu et al (2014) ?
?
68.3 ?
44.4 ?SG + Retrofitting 65.7 73.9 67.5 86.0 49.9 74.6Table 4: Comparison of retrofitting for semantic enrichment against Yu and Dredze (2014), Xu et al (2014).
Spear-man?s correlation (3 left columns) and accuracy (3 right columns) on different tasks.6.3 Comparisons to Prior WorkTwo previous models (Yu and Dredze, 2014; Xuet al, 2014) have shown that the quality of wordvectors obtained using word2vec tool can be im-proved by using semantic knowledge from lexicons.Both these models use constraints among words asa regularization term on the training objective dur-ing training, and their methods can only be appliedfor improving the quality of SG and CBOW vectorsproduced by the word2vec tool.
We compared thequality of our vectors against each of these.Yu and Dredze (2014).
We train word vectors us-ing their joint model training code8while using ex-actly the same training settings as specified in theirbest model: CBOW, vector length 100 and PPDB forenrichment.
The results are shown in the top half ofTable 4 where our model consistently outperformsthe baseline and their model.Xu et al (2014).
This model extracts categori-cal and relational knowledge among words fromFreebase9and uses it as a constraint while train-ing.
Unfortunately, neither their word embeddingsnor model training code is publicly available, sowe train the SG model by using exactly the samesettings as described in their system (vector length300) and on the same corpus: monolingual EnglishWikipedia text.10We compare the performance ofour retrofitting vectors on the SYN-REL and WS-353 task against the best model11reported in theirpaper.
As shown in the lower half of Table 4, ourmodel outperforms their model by an absolute 5.5points absolute on the SYN-REL task, but a slightly8https://github.com/Gorov/JointRCM9https://www.freebase.com10http://mattmahoney.net/dc/enwik9.zip11Their best model is named ?RC-NET?
in their paper.inferior score on the WS-353 task.6.4 Multilingual EvaluationWe tested our method on three additional languages:German, French, and Spanish.
We used the Univer-sal WordNet (de Melo and Weikum, 2009), an au-tomatically constructed multilingual lexical knowl-edge base based on WordNet.12It contains wordsconnected via different lexical relations to otherwords both within and across languages.
We con-struct separate graphs for different languages (i.e.,only linking words to other words in the same lan-guage) and apply retrofitting to each.
Since notmany word similarity evaluation benchmarks areavailable for languages other than English, we testedour baseline and improved vectors on one bench-mark per language.We used RG-65 (Gurevych, 2005), RG-65(Joubarne and Inkpen, 2011) and MC-30 (Hassanand Mihalcea, 2009) for German, French and Span-ish, respectively.13We trained SG vectors for eachlanguage of length 300 on a corpus of 1 billion to-kens, each extracted from Wikipedia, and evaluatethem on word similarity on the benchmarks beforeand after retrofitting.
Table 5 shows that we obtainhigh improvements which strongly indicates that ourmethod generalizes across these languages.7 Further AnalysisRetrofitting vs. vector length.
With more di-mensions, word vectors might be able to cap-ture higher orders of semantic information andretrofitting might be less helpful.
We train SG vec-12http://www.mpi-inf.mpg.de/yago-naga/uwn13These benchmarks were created by translating the corre-sponding English benchmarks word by word manually.1612Figure 3: Two-dimensional PCA projections of 100-dimensional SG vector pairs holding the ?adjective to adverb?relation, before (left) and after (right) retrofitting.Language Task SG Retrofitted SGGerman RG-65 53.4 60.3French RG-65 46.7 60.6Spanish MC-30 54.0 59.1Table 5: Spearman?s correlation for word similarity eval-uation using the using original and retrofitted SG vectors.0 200 400 600 800 1000Vector length56586062646668707274Spearman'scorrelationSG + RetrofittingSGFigure 2: Spearman?s correlation on the MEN word sim-ilarity task, before and after retrofitting.tors on 1 billion English tokens for vector lengthsranging from 50 to 1,000 and evaluate on the MENword similarity task.
We retrofit these vectors toPPDB (?4) and evaluate those on the same task.
Fig-ure 2 shows consistent improvement in vector qual-ity across different vector lengths.Visualization.
We randomly select eight wordpairs that have the ?adjective to adverb?
relationfrom the SYN-REL task (?5).
We then take a two-dimensional PCA projection of the 100-dimensionalSG word vectors and plot them inR2.
In Figure 3 weplot these projections before (left) and after (right)retrofitting.
It can be seen that in the first case thedirection of the analogy vectors is not consistent, butafter retrofitting all the analogy vectors are alignedin the same direction.8 Related WorkThe use of lexical semantic information in trainingword vectors has been limited.
Recently, word sim-ilarity knowledge (Yu and Dredze, 2014; Fried andDuh, 2014) and word relational knowledge (Xu etal., 2014; Bian et al, 2014) have been used to im-prove the word2vec embeddings in a joint train-ing model similar to our regularization approach.In latent semantic analysis, the word cooccurrencematrix can be constructed to incorporate relationalinformation like antonym specific polarity induc-tion (Yih et al, 2012) and multi-relational latent se-mantic analysis (Chang et al, 2013).The approach we propose is conceptually similarto previous work that uses graph structures to prop-agate information among semantic concepts (Zhu,2005; Culp and Michailidis, 2008).
Graph-basedbelief propagation has also been used to inducePOS tags (Subramanya et al, 2010; Das and Petrov,2011) and semantic frame associations (Das andSmith, 2011).
In those efforts, labels for unknownwords were inferred using a method similar toours.
Broadly, graph-based semi-supervised learn-ing (Zhu, 2005; Talukdar and Pereira, 2010) hasbeen applied to machine translation (Alexandrescu1613and Kirchhoff, 2009), unsupervised semantic roleinduction (Lang and Lapata, 2011), semantic docu-ment modeling (Schuhmacher and Ponzetto, 2014),language generation (Krahmer et al, 2003) and sen-timent analysis (Goldberg and Zhu, 2006).9 ConclusionWe have proposed a simple and effective methodnamed retrofitting to improve word vectors usingword relation knowledge found in semantic lex-icons.
Retrofitting is used as a post-processingstep to improve vector quality and is more modu-lar than other approaches that use semantic informa-tion while training.
It can be applied to vectors ob-tained from any word vector training method.
Ourexperiments explored the method?s performanceacross tasks, semantic lexicons, and languages andshowed that it outperforms existing alternatives.The retrofitting tool is available at: https://github.com/mfaruqui/retrofitting.AcknowledgementsThis research was supported in part by the NationalScience Foundation under grants IIS-1143703, IIS-1147810, and IIS-1251131; by IARPA via De-partment of Interior National Business Center(DoI/NBC) contract number D12PC00337; and byDARPA under grant FA87501220342.
Part of thecomputational work was carried out on resourcesprovided by the Pittsburgh Supercomputing Center.The U.S. Government is authorized to reproduceand distribute reprints for Governmental purposesnotwithstanding any copyright annotation thereon.Disclaimer: the views and conclusions containedherein are those of the authors and should not beinterpreted as necessarily representing the officialpolicies or endorsements, either expressed or im-plied, of IARPA, DoI/NBC, DARPA, or the U.S.Government.ReferencesEneko Agirre, Enrique Alfonseca, Keith Hall, JanaKravalova, Marius Pas?ca, and Aitor Soroa.
2009.A study on similarity and relatedness using distribu-tional and wordnet-based approaches.
In Proceedingsof NAACL.Andrei Alexandrescu and Katrin Kirchhoff.
2009.Graph-based learning for statistical machine transla-tion.
In Proceedings of NAACL.Collin F. Baker, Charles J. Fillmore, and John B. Lowe.1998.
The berkeley framenet project.
In Proceedingsof ACL.Yoshua Bengio, Olivier Delalleau, and Nicolas Le Roux.2006.
Label propagation and quadratic criterion.
InSemi-Supervised Learning.Jiang Bian, Bin Gao, and Tie-Yan Liu.
2014.Knowledge-powered deep learning for word embed-ding.
In Machine Learning and Knowledge Discoveryin Databases.Elia Bruni, Gemma Boleda, Marco Baroni, and Nam-Khanh Tran.
2012.
Distributional semantics in tech-nicolor.
In Proceedings of ACL.Bob Carpenter.
2008.
Lazy sparse stochastic gradient de-scent for regularized multinomial logistic regression.Technical Report Alias-i Inc.Kai-Wei Chang, Wen-tau Yih, and Christopher Meek.2013.
Multi-relational latent semantic analysis.
InProceedings of EMNLP.Ronan Collobert and Jason Weston.
2008.
A unified ar-chitecture for natural language processing: deep neu-ral networks with multitask learning.
In Proceedingsof ICML.Mark Culp and George Michailidis.
2008.
Graph-basedsemisupervised learning.
IEEE Transactions on Pat-tern Analysis and Machine Intelligence.Dipanjan Das and Slav Petrov.
2011.
Unsupervised part-of-speech tagging with bilingual graph-based projec-tions.
In Proc.
of ACL.Dipanjan Das and Noah A. Smith.
2011.
Semi-supervised frame-semantic parsing for unknown pred-icates.
In Proc.
of ACL.Gerard de Melo and Gerhard Weikum.
2009.
Towardsa universal wordnet by learning from combined evi-dence.
In Proceedings of CIKM.S.
C. Deerwester, S. T. Dumais, T. K. Landauer, G. W.Furnas, and R. A. Harshman.
1990.
Indexing by latentsemantic analysis.
Journal of the American Society forInformation Science.John Duchi, Elad Hazan, and Yoram Singer.
2010.Adaptive subgradient methods for online learningand stochastic optimization.
Technical ReportUCB/EECS-2010-24, Mar.Manaal Faruqui and Chris Dyer.
2014.
Improving vectorspace word representations using multilingual correla-tion.
In Proceedings of EACL.Charles Fillmore, Christopher Johnson, and MiriamPetruck.
2003. International Journal of Lexicogra-phy.1614Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias,Ehud Rivlin, Zach Solan, Gadi Wolfman, and EytanRuppin.
2001.
Placing search in context: the conceptrevisited.
In Proceedings of WWW.Daniel Fried and Kevin Duh.
2014.
Incorporating bothdistributional and relational semantics in word repre-sentations.
arXiv preprint arXiv:1412.4369.Juri Ganitkevitch, Benjamin Van Durme, and ChrisCallison-Burch.
2013.
PPDB: The paraphrasedatabase.
In Proceedings of NAACL.Andrew B. Goldberg and Xiaojin Zhu.
2006.
See-ing stars when there aren?t many stars: Graph-basedsemi-supervised learning for sentiment categorization.TextGraphs-1.Jiang Guo, Wanxiang Che, Haifeng Wang, and Ting Liu.2014.
Revisiting embedding features for simple semi-supervised learning.
In Proceedings of EMNLP.Iryna Gurevych.
2005.
Using the structure of a concep-tual network in computing semantic relatedness.
InProceedings of IJCNLP.Samer Hassan and Rada Mihalcea.
2009.
Cross-lingualsemantic relatedness using encyclopedic knowledge.In Proc.
of EMNLP.Eric H Huang, Richard Socher, Christopher D Manning,and Andrew Y Ng.
2012.
Improving word representa-tions via global context and multiple word prototypes.In Proceedings of ACL.Colette Joubarne and Diana Inkpen.
2011.
Compari-son of semantic similarity for different languages us-ing the google n-gram corpus and second- order co-occurrence measures.
In Proceedings of CAAI.Ross Kindermann and J. L. Snell.
1980.
Markov RandomFields and Their Applications.
AMS.Emiel Krahmer, Sebastian van Erk, and Andr?e Verleg.2003.
Graph-based generation of referring expres-sions.
Comput.
Linguist.Thomas K Landauer and Susan T. Dumais.
1997.
A so-lution to plato?s problem: The latent semantic analysistheory of acquisition, induction, and representation ofknowledge.
Psychological review.Joel Lang and Mirella Lapata.
2011.
Unsupervised se-mantic role induction with graph partitioning.
In Pro-ceedings of EMNLP.Omer Levy and Yoav Goldberg.
2014.
Linguistic regu-larities in sparse and explicit word representations.
InProceedings of CoNLL.Tomas Mikolov, Kai Chen, Greg Corrado, and JeffreyDean.
2013a.
Efficient estimation of word representa-tions in vector space.
arXiv preprint arXiv:1301.3781.Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig.2013b.
Linguistic regularities in continuous spaceword representations.
In Proceedings of NAACL.George A Miller.
1995.
Wordnet: a lexical database forenglish.
Communications of the ACM.Andriy Mnih and Yee Whye Teh.
2012.
A fast and sim-ple algorithm for training neural probabilistic languagemodels.
In Proceedings of ICML.Jerome L. Myers and Arnold D. Well.
1995.
ResearchDesign & Statistical Analysis.
Routledge.Jeffrey Pennington, Richard Socher, and Christopher D.Manning.
2014.
Glove: Global vectors for word rep-resentation.
In Proceedings of EMNLP.Herbert Rubenstein and John B. Goodenough.
1965.Contextual correlates of synonymy.
Commun.
ACM,8(10):627?633, October.Michael Schuhmacher and Simone Paolo Ponzetto.2014.
Knowledge-based graph document modeling.In Proceedings of WSDM.Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang,Christopher D. Manning, Andrew Y. Ng, and Christo-pher Potts.
2013.
Recursive deep models for semanticcompositionality over a sentiment treebank.
In Pro-ceedings of EMNLP.Amarnag Subramanya, Slav Petrov, and FernandoPereira.
2010.
Efficient graph-based semi-supervisedlearning of structured tagging models.
In Proceedingsof EMNLP.Partha Pratim Talukdar and Fernando Pereira.
2010.Experiments in graph-based semi-supervised learningmethods for class-instance acquisition.
In Proceedingsof ACL.Joseph Turian, Lev Ratinov, and Yoshua Bengio.
2010.Word representations: a simple and general method forsemi-supervised learning.
In Proc.
of ACL.Peter D. Turney.
2006.
Similarity of semantic relations.Comput.
Linguist., 32(3):379?416, September.Chang Xu, Yalong Bai, Jiang Bian, Bin Gao, Gang Wang,Xiaoguang Liu, and Tie-Yan Liu.
2014.
Rc-net: Ageneral framework for incorporating knowledge intoword representations.
In Proceedings of CIKM.Wen-tau Yih, Geoffrey Zweig, and John C. Platt.
2012.Polarity inducing latent semantic analysis.
In Pro-ceedings of EMNLP.Mo Yu and Mark Dredze.
2014.
Improving lexical em-beddings with semantic knowledge.
In ACL.Xiaojin Zhu.
2005.
Semi-supervised Learningwith Graphs.
Ph.D. thesis, Pittsburgh, PA, USA.AAI3179046.1615
