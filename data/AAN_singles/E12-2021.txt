Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 102?107,Avignon, France, April 23 - 27 2012. c?2012 Association for Computational LinguisticsBRAT: a Web-based Tool for NLP-Assisted Text AnnotationPontus Stenetorp1?
Sampo Pyysalo2,3?
Goran Topic?1Tomoko Ohta1,2,3 Sophia Ananiadou2,3 and Jun?ichi Tsujii41Department of Computer Science, The University of Tokyo, Tokyo, Japan2School of Computer Science, University of Manchester, Manchester, UK3National Centre for Text Mining, University of Manchester, Manchester, UK4Microsoft Research Asia, Beijing, People?s Republic of China{pontus,smp,goran,okap}@is.s.u-tokyo.ac.jpsophia.ananiadou@manchester.ac.ukjtsujii@microsoft.comAbstractWe introduce the brat rapid annotation tool(BRAT), an intuitive web-based tool for textannotation supported by Natural LanguageProcessing (NLP) technology.
BRAT hasbeen developed for rich structured annota-tion for a variety of NLP tasks and aimsto support manual curation efforts and in-crease annotator productivity using NLPtechniques.
We discuss several case stud-ies of real-world annotation projects usingpre-release versions of BRAT and presentan evaluation of annotation assisted by se-mantic class disambiguation on a multi-category entity mention annotation task,showing a 15% decrease in total annota-tion time.
BRAT is available under an open-source license from: http://brat.nlplab.org1 IntroductionManually-curated gold standard annotations area prerequisite for the evaluation and training ofstate-of-the-art tools for most Natural LanguageProcessing (NLP) tasks.
However, annotation isalso one of the most time-consuming and finan-cially costly components of many NLP researchefforts, and can place heavy demands on humanannotators for maintaining annotation quality andconsistency.
Yet, modern annotation tools aregenerally technically oriented and many offer lit-tle support to users beyond the minimum requiredfunctionality.
We believe that intuitive and user-friendly interfaces as well as the judicious appli-cation of NLP technology to support, not sup-plant, human judgements can help maintain thequality of annotations, make annotation more ac-cessible to non-technical users such as subject?These authors contributed equally to this workFigure 1: Visualisation examples.
Top: named en-tity recognition, middle: dependency syntax, bot-tom: verb frames.domain experts, and improve annotation produc-tivity, thus reducing both the human and finan-cial cost of annotation.
The tool presented inthis work, BRAT, represents our attempt to realisethese possibilities.2 Features2.1 High-quality Annotation VisualisationBRAT is based on our previously released open-source STAV text annotation visualiser (Stene-torp et al 2011b), which was designed to helpusers gain an understanding of complex annota-tions involving a large number of different se-mantic types, dense, partially overlapping text an-notations, and non-projective sets of connectionsbetween annotations.
Both tools share a vectorgraphics-based visualisation component, whichprovide scalable detail and rendering.
BRAT in-tegrates PDF and EPS image format export func-tionality to support use in e.g.
figures in publica-tions (Figure 1).102Figure 2: Screenshot of the main BRAT user-interface, showing a connection being made between theannotations for ?moving?
and ?Citibank?.2.2 Intuitive Annotation InterfaceWe extended the capabilities of STAV by imple-menting support for annotation editing.
This wasdone by adding functionality for recognising stan-dard user interface gestures familiar from text ed-itors, presentation software, and many other tools.In BRAT, a span of text is marked for annotationsimply by selecting it with the mouse by ?drag-ging?
or by double-clicking on a word.
Similarly,annotations are linked by clicking with the mouseon one annotation and dragging a connection tothe other (Figure 2).BRAT is browser-based and built entirely usingstandard web technologies.
It thus offers a fa-miliar environment to annotators, and it is pos-sible to start using BRAT simply by pointing astandards-compliant modern browser to an instal-lation.
There is thus no need to install or dis-tribute any additional annotation software or touse browser plug-ins.
The use of web standardsalso makes it possible for BRAT to uniquely iden-tify any annotation using Uniform Resource Iden-tifiers (URIs), which enables linking to individualannotations for discussions in e-mail, documentsand on web pages, facilitating easy communica-tion regarding annotations.2.3 Versatile Annotation SupportBRAT is fully configurable and can be set up tosupport most text annotation tasks.
The most ba-sic annotation primitive identifies a text span andassigns it a type (or tag or label), marking for e.g.POS-tagged tokens, chunks or entity mentions(Figure 1 top).
These base annotations can beconnected by binary relations ?
either directed orundirected ?
which can be configured for e.g.
sim-ple relation extraction, or verb frame annotation(Figure 1 middle and bottom).
n-ary associationsof annotations are also supported, allowing the an-notation of event structures such as those targetedin the MUC (Sundheim, 1996), ACE (Doddingtonet al 2004), and BioNLP (Kim et al 2011) In-formation Extraction (IE) tasks (Figure 2).
Addi-tional aspects of annotations can be marked usingattributes, binary or multi-valued flags that canbe added to other annotations.
Finally, annotatorscan attach free-form text notes to any annotation.In addition to information extraction tasks,these annotation primitives allow BRAT to beconfigured for use in various other tasks, suchas chunking (Abney, 1991), Semantic Role La-beling (Gildea and Jurafsky, 2002; Carrerasand Ma`rquez, 2005), and dependency annotation(Nivre, 2003) (See Figure 1 for examples).
Fur-ther, both the BRAT client and server implementfull support for the Unicode standard, which al-low the tool to support the annotation of text us-ing e.g.
Chinese or Devana?gar??
characters.
BRATis distributed with examples from over 20 cor-pora for a variety of tasks, involving texts in sevendifferent languages and including examples fromcorpora such as those introduced for the CoNLLshared tasks on language-independent named en-tity recognition (Tjong Kim Sang and De Meul-der, 2003) and multilingual dependency parsing(Buchholz and Marsi, 2006).BRAT also implements a fully configurable sys-tem for checking detailed constraints on anno-tation semantics, for example specifying that aTRANSFER event must take exactly one of eachof GIVER, RECIPIENT and BENEFICIARY argu-ments, each of which must have one of the typesPERSON, ORGANIZATION or GEO-POLITICALENTITY, as well as a MONEY argument of type103Figure 3: Incomplete TRANSFER event indicatedto the annotatorMONEY, and may optionally take a PLACE argu-ment of type LOCATION (LDC, 2005).
Constraintchecking is fully integrated into the annotation in-terface and feedback is immediate, with clear vi-sual effects marking incomplete or erroneous an-notations (Figure 3).2.4 NLP Technology IntegrationBRAT supports two standard approaches for inte-grating the results of fully automatic annotationtools into an annotation workflow: bulk anno-tation imports can be performed by format con-version tools distributed with BRAT for manystandard formats (such as in-line and column-formatted BIO), and tools that provide standardweb service interfaces can be configured to be in-voked from the user interface.However, human judgements cannot be re-placed or based on a completely automatic analy-sis without some risk of introducing bias and re-ducing annotation quality.
To address this issue,we have been studying ways to augment the an-notation process with input from statistical andmachine learning methods to support the annota-tion process while still involving human annotatorjudgement for each annotation.As a specific realisation based on this approach,we have integrated a recently introduced ma-chine learning-based semantic class disambigua-tion system capable of offering multiple outputswith probability estimates that was shown to beable to reduce ambiguity on average by over 75%while retaining the correct class in on average99% of cases over six corpora (Stenetorp et al2011a).
Section 4 presents an evaluation of thecontribution of this component to annotator pro-ductivity.2.5 Corpus Search FunctionalityBRAT implements a comprehensive set of searchfunctions, allowing users to search document col-Figure 4: The BRAT search dialoglections for text span annotations, relations, eventstructures, or simply text, with a rich set of searchoptions definable using a simple point-and-clickinterface (Figure 4).
Additionally, search resultscan optionally be displayed using keyword-in-context concordancing and sorted for browsingusing any aspect of the matched annotation (e.g.type, text, or context).3 ImplementationBRAT is implemented using a client-server ar-chitecture with communication over HTTP usingJavaScript Object Notation (JSON).
The server isa RESTful web service (Fielding, 2000) and thetool can easily be extended or adapted to switchout the server or client.
The client user interface isimplemented using XHTML and Scalable VectorGraphics (SVG), with interactivity implementedusing JavaScript with the jQuery library.
Theclient communicates with the server using Asyn-chronous JavaScript and XML (AJAX), whichpermits asynchronous messaging.BRAT uses a stateless server back-end imple-mented in Python and supports both the CommonGateway Interface (CGI) and FastCGI protocols,the latter allowing response times far below the100 ms boundary for a ?smooth?
user experiencewithout noticeable delay (Card et al 1983).
Forserver side annotation storage BRAT uses an easy-to-process file-based stand-off format that can beconverted from or into other formats; there is noneed to perform database import or export to in-terface with the data storage.
The BRAT server in-104Figure 5: Example annotation from the BioNLP Shared Task 2011 Epigenetics and Post-translationalModifications event extraction task.stallation requires only a CGI-capable web serverand the set-up supports any number of annotatorswho access the server using their browsers, on anyoperating system, without separate installation.Client-server communication is managed sothat all user edit operations are immediately sentto the server, which consolidates them with thestored data.
There is no separate ?save?
operationand thus a minimal risk of data loss, and as theauthoritative version of all annotations is alwaysmaintained by the server, there is no chance ofconflicting annotations being made which wouldneed to be merged to produce an authoritative ver-sion.
The BRAT client-server architecture alsomakes real-time collaboration possible: multipleannotators can work on a single document simul-taneously, seeing each others edits as they appearin a document.4 Case Studies4.1 Annotation ProjectsBRAT has been used throughout its developmentduring 2011 in the annotation of six different cor-pora by four research groups in efforts that havein total involved the creation of well-over 50,000annotations in thousands of documents compris-ing hundreds of thousands of words.These projects include structured event an-notation for the domain of cancer biology,Japanese verb frame annotation, and gene-mutation-phenotype relation annotation.
Oneprominent effort making use of BRAT is theBioNLP Shared Task 2011,1 in which the tool wasused in the annotation of the EPI and ID maintask corpora (Pyysalo et al 2012).
These twoinformation extraction tasks involved the annota-tion of entities, relations and events in the epige-netics and infectious diseases subdomains of biol-ogy.
Figure 5 shows an illustration of shared taskannotations.Many other annotation efforts using BRAT arestill ongoing.
We refer the reader to the BRAT1http://2011.bionlp-st.orgMode Total Type SelectionNormal 45:28 13:49Rapid 39:24 (-6:04) 09:35 (-4:14)Table 1: Total annotation time, portion spent se-lecting annotation type, and absolute improve-ment for rapid mode.website2 for further details on current and past an-notation projects using BRAT.4.2 Automatic Annotation SupportTo estimate the contribution of the semantic classdisambiguation component to annotation produc-tivity, we performed a small-scale experiment in-volving an entity and process mention taggingtask.
The annotation targets were of 54 dis-tinct mention types (19 physical entity and 35event/process types) marked using the simpletyped-span representation.
To reduce confound-ing effects from annotator productivity differ-ences and learning during the task, annotation wasperformed by a single experienced annotator witha Ph.D. in biology in a closely related area whowas previously familiar with the annotation task.The experiment was performed on publicationabstracts from the biomolecular science subdo-main of glucose metabolism in cancer.
The textswere drawn from a pool of 1,750 initial candi-dates using stratified sampling to select pairs of10-document sets with similar overall statisticalproperties.3 Four pairs of 10 documents (80 in to-tal) were annotated in the experiment, with 10 ineach pair annotated with automatic support and 10without, in alternating sequence to prevent learn-ing effects from favouring either approach.The results of this experiment are summarizedin Table 1 and Figure 6.
In total 1,546 annotationswere created in normal mode and 1,541 annota-2http://brat.nlplab.org3Document word count and expected annotation count,were estimated from the output of NERsuite, a freely avail-able CRF-based NER tagger: http://nersuite.nlplab.org105050010001500200025003000Normal Mode Rapid ModeTime(seconds)Figure 6: Allocation of annotation time.
GREENsignifies time spent on selecting annotation typeand BLUE the remaining annotation time.tions in rapid mode; the sets are thus highly com-parable.
We observe a 15.4% reduction in totalannotation time, and, as expected, this is almostexclusively due to a reduction in the time the an-notator spent selecting the type to assign to eachspan, which is reduced by 30.7%; annotation timeis otherwise stable across the annotation modes(Figure 6).
The reduction in the time spent in se-lecting the span is explained by the limiting of thenumber of candidate types exposed to the annota-tor, which were decreased from the original 54 toan average of 2.88 by the semantic class disam-biguation component (Stenetorp et al 2011a).Although further research is needed to establishthe benefits of this approach in various annotationtasks, we view the results of this initial experi-ment as promising regarding the potential of ourapproach to using machine learning to support an-notation efforts.5 Related Work and ConclusionsWe have introduced BRAT, an intuitive and user-friendly web-based annotation tool that aims toenhance annotator productivity by closely inte-grating NLP technology into the annotation pro-cess.
BRAT has been and is being used for severalongoing annotation efforts at a number of aca-demic institutions and has so far been used forthe creation of well-over 50,000 annotations.
Wepresented an experiment demonstrating that inte-grated machine learning technology can reducethe time for type selection by over 30% and over-all annotation time by 15% for a multi-type entitymention annotation task.The design and implementation of BRAT wasinformed by experience from several annotationtasks and research efforts spanning more thana decade.
A variety of previously introducedannotation tools and approaches also served toguide our design decisions, including the fast an-notation mode of Knowtator (Ogren, 2006), thesearch capabilities of the XConc tool (Kim et al2008), and the design of web-based systems suchas MyMiner (Salgado et al 2010), and GATETeamware (Cunningham et al 2011).
Using ma-chine learning to accelerate annotation by sup-porting human judgements is well documented inthe literature for tasks such as entity annotation(Tsuruoka et al 2008) and translation (Mart?
?nez-Go?mez et al 2011), efforts which served as in-spiration for our own approach.BRAT, along with conversion tools and exten-sive documentation, is freely available under theopen-source MIT license from its homepage athttp://brat.nlplab.orgAcknowledgementsThe authors would like to thank early adopters ofBRAT who have provided us with extensive feed-back and feature suggestions.
This work was sup-ported by Grant-in-Aid for Specially PromotedResearch (MEXT, Japan), the UK Biotechnologyand Biological Sciences Research Council (BB-SRC) under project Automated Biological EventExtraction from the Literature for Drug Discov-ery (reference number: BB/G013160/1), and theRoyal Swedish Academy of Sciences.106ReferencesSteven Abney.
1991.
Parsing by chunks.
Principle-based parsing, 44:257?278.Sabine Buchholz and Erwin Marsi.
2006.
CoNLL-X shared task on multilingual dependency parsing.In Proceedings of the Tenth Conference on Com-putational Natural Language Learning (CoNLL-X),pages 149?164.Stuart K. Card, Thomas P. Moran, and Allen Newell.1983.
The psychology of human-computer interac-tion.
Lawrence Erlbaum Associates, Hillsdale, NewJersey.Xavier Carreras and Llu?
?s Ma`rquez.
2005.
Introduc-tion to the CoNLL-2005 shared task: Semantic RoleLabeling.
In Proceedings of the 9th Conference onNatural Language Learning, pages 152?164.
Asso-ciation for Computational Linguistics.Hamish Cunningham, Diana Maynard, KalinaBontcheva, Valentin Tablan, Niraj Aswani, IanRoberts, Genevieve Gorrell, Adam Funk, AngusRoberts, Danica Damljanovic, Thomas Heitz,Mark A. Greenwood, Horacio Saggion, JohannPetrak, Yaoyong Li, and Wim Peters.
2011.
TextProcessing with GATE (Version 6).George Doddington, Alexis Mitchell, Mark Przybocki,Lance Ramshaw, Stephanie Strassel, and RalphWeischedel.
2004.
The Automatic Content Extrac-tion (ACE) program: Tasks, data, and evaluation.
InProceedings of the 4th International Conference onLanguage Resources and Evaluation, pages 837?840.Roy Fielding.
2000.
REpresentational State Trans-fer (REST).
Architectural Styles and the Designof Network-based Software Architectures.
Univer-sity of California, Irvine, page 120.Daniel Gildea and Daniel Jurafsky.
2002.
Automaticlabeling of semantic roles.
Computational Linguis-tics, 28(3):245?288.Jin-Dong Kim, Tomoko Ohta, and Jun?ichi Tsujii.2008.
Corpus annotation for mining biomedi-cal events from literature.
BMC Bioinformatics,9(1):10.Jin-Dong Kim, Sampo Pyysalo, Tomoko Ohta, RobertBossy, Ngan Nguyen, and Jun?ichi Tsujii.
2011.Overview of BioNLP Shared Task 2011.
In Pro-ceedings of BioNLP Shared Task 2011 Workshop,pages 1?6, Portland, Oregon, USA, June.
Associa-tion for Computational Linguistics.LDC.
2005.
ACE (Automatic Content Extraction) En-glish Annotation Guidelines for Events.
Technicalreport, Linguistic Data Consortium.Pascual Mart?
?nez-Go?mez, Germa?n Sanchis-Trilles,and Francisco Casacuberta.
2011.
Online learn-ing via dynamic reranking for computer assistedtranslation.
In Alexander Gelbukh, editor, Compu-tational Linguistics and Intelligent Text Processing,volume 6609 of Lecture Notes in Computer Science,pages 93?105.
Springer Berlin / Heidelberg.Joakim Nivre.
2003.
An Efficient Algorithm for Pro-jective Dependency Parsing.
In Proceedings of the8th International Workshop on Parsing Technolo-gies, pages 149?160.Philip V. Ogren.
2006.
Knowtator: A prote?ge?
plug-infor annotated corpus construction.
In Proceedingsof the Conference of the North American Chapter ofthe Association for Computational Linguistics: Hu-man Language Technologies, Companion Volume:Demonstrations, pages 273?275, New York City,USA, June.
Association for Computational Linguis-tics.Sampo Pyysalo, Tomoko Ohta, Rafal Rak, Dan Sul-livan, Chunhong Mao, Chunxia Wang, Bruno So-bral, Junichi Tsujii, and Sophia Ananiadou.
2012.Overview of the ID, EPI and REL tasks of BioNLPShared Task 2011.
BMC Bioinformatics, 13(suppl.8):S2.David Salgado, Martin Krallinger, Marc Depaule,Elodie Drula, and Ashish V Tendulkar.
2010.Myminer system description.
In Proceedings of theThird BioCreative Challenge Evaluation Workshop2010, pages 157?158.Pontus Stenetorp, Sampo Pyysalo, Sophia Ananiadou,and Jun?ichi Tsujii.
2011a.
Almost total recall: Se-mantic category disambiguation using large lexicalresources and approximate string matching.
In Pro-ceedings of the Fourth International Symposium onLanguages in Biology and Medicine.Pontus Stenetorp, Goran Topic?, Sampo Pyysalo,Tomoko Ohta, Jin-Dong Kim, and Jun?ichi Tsujii.2011b.
BioNLP Shared Task 2011: Supporting Re-sources.
In Proceedings of BioNLP Shared Task2011 Workshop, pages 112?120, Portland, Oregon,USA, June.
Association for Computational Linguis-tics.Beth M. Sundheim.
1996.
Overview of results ofthe MUC-6 evaluation.
In Proceedings of the SixthMessage Understanding Conference, pages 423?442.
Association for Computational Linguistics.Erik F. Tjong Kim Sang and Fien De Meulder.2003.
Introduction to the CoNLL-2003 sharedtask: Language-independent named entity recogni-tion.
In Proceedings of the Seventh Conference onNatural Language Learning at HLT-NAACL 2003,pages 142?147.Yoshimasa Tsuruoka, Jun?ichi Tsujii, and Sophia Ana-niadou.
2008.
Accelerating the annotation ofsparse named entities by dynamic sentence selec-tion.
BMC Bioinformatics, 9(Suppl 11):S8.107
