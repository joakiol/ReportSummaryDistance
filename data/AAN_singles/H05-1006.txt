Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural LanguageProcessing (HLT/EMNLP), pages 41?48, Vancouver, October 2005. c?2005 Association for Computational LinguisticsError Detection Using Linguistic FeaturesYongmei ShiDepartment of Computer Science andElectrical EngineeringUniversity of Maryland Baltimore CountyBaltimore, MD 21250yshi1@umbc.eduLina ZhouInformation Systems DepartmentUniversity of Maryland Baltimore CountyBaltimore, MD 21250zhoul@umbc.eduAbstractRecognition errors hinder the prolifera-tion of speech recognition (SR) systems.Based on the observation that recogni-tion errors may result in ungrammaticalsentences, especially in dictation appli-cation where an acceptable level of ac-curacy of generated documents is indis-pensable, we propose to incorporate twokinds of linguistic features into error de-tection: lexical features of words, and syn-tactic features from a robust lexicalizedparser.
Transformation-based learning ischosen to predict recognition errors by in-tegrating word confidence scores with lin-guistic features.
The experimental resultson a dictation data corpus show that lin-guistic features alone are not as useful asword confidence scores in detecting er-rors.
However, linguistic features providecomplementary information when com-bined with word confidence scores, whichcollectively reduce the classification errorrate by 12.30% and improve the F measureby 53.62%.1 IntroductionThe proliferation of speech recognition (SR) sys-tems is hampered by the ever-presence of recogni-tion errors and the significant amount of effort in-volved in error correction.
A user study (Sears et al,2001) showed that users spent one-third of their timefinding and locating errors and another one-third ofthe time correcting errors in a hand-free dictationtask.
Successfully detecting SR errors can speed upthe entire process of error correction.
Therefore, wefocus on error detection in this study.A common approach to detecting SR errors is an-notating confidence at the word level.
The major-ity of confidence annotation methods are based onfeature combination, which follows two steps: (i)extract useful features characteristics of the correct-ness of words either from the inner components ofan SR system (SR-dependent features) or from therecognition output (SR-independent features); and(ii) develop a binary classifier to separate words intotwo groups: correct recognitions and errors.Various features extracted from different compo-nents of an SR system, such as the acoustic model,the language model, and the decoder, have beenproven useful to detecting recognition errors (Chase,1997; Pao et al, 1998; San-Segundo et al, 2001).Nonetheless, merely using these features is inade-quate, because the information conveyed by thesefeatures has already been considered when SR sys-tems generate the output.
A common observation isthat the combination of SR-dependent features canonly marginally improve the performance achievedby using only the best single feature (Zhang andRudnicky, 2001; Sarikaya et al, 2003).
Hence in-formation sources beyond the SR system are desiredin error detection.High-level linguistic knowledge is a good candi-date for additional information sources.
It can beextracted from the SR output via natural languageprocessing, which compensates for the lack of high-41level linguistic knowledge in a typical SR system.A user study (Brill et al, 1998) showed that hu-mans can utilize linguistic knowledge at various lev-els to improve the SR output by selecting the bestutterance hypotheses from N-best lists.
Linguisticfeatures from syntactic, semantic, and dialogue dis-course analyses have proven their values in error de-tection in domain specific spoken dialogue systems,e.g.
(Rayner et al, 1994; Carpenter et al, 2001;Sarikaya et al, 2003).
However, few studies have in-vestigated the merit of linguistic knowledge for errordetection in dictation, a domain-independent appli-cation.Transformation-based learning (TBL) is a rule-based learning method.
It has been used in errorcorrection (Mangu and Padmanabhan, 2001) and er-ror detection (Skantze and Edlund, 2004).
The ruleslearned by TBL show good interpretability as wellas good performance.
Although statistical learningmethods have been widely used in confidence an-notation (Carpenter et al, 2001; Pao et al, 1998;Chase, 1997), their results are difficult to interpret.Therefore, we select TBL to derive error patternsfrom the SR output in this study.The rest of the paper is organized as follows.
InSection 2, we review the extant work on utilizing lin-guistic features in error detection.
In Section 3, weintroduce linguistic features used in this study.
InSection 4, we describe transformation-based learn-ing and define the transformations, followed withreporting the experimental results in Section 5.
Fi-nally, we summarize the findings of this study andsuggest directions for further research in Section 6.2 Related WorkWhen the output of an SR system is processed, theentire utterance is available and thus utterance-levelcontextual information can be utilized.
Featuresgenerated from high-level language processing suchas syntactic and semantic analyses may complementthe low-level language knowledge (usually n-gram)used in the SR systems.Most of the previous work on utilizing linguis-tic features in error detection focused on utterance-level confidence measures.
Most of features wereextracted from the output of syntactic or semanticparsers, including full/robust/no parse, number ofwords parsed, gap number, slot number, grammarrule used, and so on (Rayner et al, 1994; Pao etal., 1998; Carpenter et al, 2001; San-Segundo et al,2001).
Some discourse-level features were also em-ployed in spoken dialogue systems such as numberof turns, and dialog state (Carpenter et al, 2001).Several studies incorporated linguistic featuresinto word-level confidence measures.
Zhang andRudnicky (2001) selected two features, i.e., pars-ing mode and slot backoff mode, extracted from theparsing result of Phoenix, a semantic parser.
Theabove two features were combined with several SR-dependent features using SVM, which achieved a7.6% relative classification error rate reduction overSR-dependent features on the data from CMU Com-municator system.Sarikaya et al (2003) explored two sets of seman-tic features: one set from a statistical classer/parser,and the other set from a maximum entropy basedsemantic-structured language model.
When com-bined with the posterior probability using the deci-sion tree, both sets achieved about 13-14% absoluteimprovement on correct acceptance at 5% false ac-ceptance over the baseline posterior probability onthe data from IBM Communicator system.Skantze and Edlund (2004) focused on lexicalfeatures (e.g., part-of-speech, syllables, and con-tent words) and dialogue discourse features (e.g.,previous dialogue act, and mentioned word), butdid not consider parser-based features.
They em-ployed transformation-based learning and instance-based learning as classifiers.
When combined withconfidence scores, the linguistic features achieved7.8% absolute improvement in classification accu-racy over confidence scores on one of their dialoguecorpora.It is shown from the related work that linguis-tic features have merit in judging the correctnessof words and/or utterances.
However, such featureshave only been discussed in the context of conver-sational dialogue in specific domains such as ATIS(Rayner et al, 1994), JUPITER (Pao et al, 1998),and Communicator (Carpenter et al, 2001; San-Segundo et al, 2001; Zhang and Rudnicky, 2001;Sarikaya et al, 2003).In an early study, we investigated the usefulnessof linguistic features in detecting word errors in dic-tation recognition (Zhou et al, 2005).
The linguis-42tic features were extracted from the parsing resultof the link grammar.
The combination of linguis-tic features with various confidence score based fea-tures using SVM can improve F measure for errordetection from 42.2% to 55.3%, and classificationaccuracy from 80.91% to 83.53%.
However, parser-based features used were limited to the number oflinks that a word has.3 Linguistic FeaturesFor each output word, two sets of linguistic featuresare extracted: lexical features and syntactic features.3.1 Lexical FeaturesFor each word w, the following lexical features areextracted:?
word: w itself?
pos: part-of-speech tag from Brill?s tagger(Brill, 1995)?
syllables: number of syllables in w, estimatedbased on the distribution patterns of vowels andconsonants?
position: the position of w in the sentence: be-ginning, end, and middle3.2 Syntactic FeaturesSpeech recognition errors may result in ungrammat-ical sentences under the assumption that the speakerfollows grammar rules while speaking.
Such an as-sumption holds true especially for dictation appli-cation because the general purpose of dictation isto create understandable documents for communi-cation.Syntactic parsers are considered as the closest ap-proximation to this intuition since there is still a lackof semantic parsers for the general domain.
More-over, robust parsers are preferred so that an errorin a recognized sentence does not lead to failure inparsing the entire sentence.
Furthermore, lexicalizedparsers are desired to support error detection at theword level.
As a result, we select Link Grammar1 togenerate syntactic features.1Available via http://www.link.cs.cmu.edu/link/3.2.1 Link GrammarLink Grammar is a context-free lexicalized gram-mar without explicit constituents (Sleator and Tem-perley, 1993).
In link grammar, rules are expressedas link requirements associated with words.
A linkrequirement is a set of disjuncts, each of which rep-resents a possible usage of the word.
A sequence ofwords belongs to the grammar if the result linkage isa planar, connected graph in which at most one linkis between each word pair and no cross link exists.Link grammar supports robust parsing by incorpo-rating null links (Grinberg et al, 1995).3.2.2 Features from Link GrammarWe hypothesize that a word without any link ina linkage of the sentence is a good indicator ofthe occurrence of errors.
Either the word itselfor words around it are likely to be erroneous.
Ithas been shown that null links can successfully ig-nore false starts and connect grammatical phrases inungrammatical utterances, which are randomly se-lected from the Switchboard corpus (Grinberg et al,1995).A word with links may still be an error, andits correctness may affect the correctness of wordslinked to it, especially those words connected withthe shortest links that indicate the closest connec-tions.Accordingly, for each word w, the following fea-tures are extracted from the parsing result:?
haslink: whether w has left links, right links, orno link?
llinkto/rlinkto: the word to which w links viathe shortest left/right linkAn example of parsing results is illustrated in Fig-ure 1.
Links are represented with dotted lines whichare annotated with labels (e.g., Wd, Xp) represent-ing link types.
In Figure 1, word ?since?
has nolink, and word ?around?
has one left link and oneright link.
The word that has the shortest left link to?world?
is ?the?.43LEFT-WALL [since] people.p will.v come.v from around the world.n .Wd Sp I MVp FM DsJsXpFigure 1: An Example of Parsing Results of Link Grammar4 Error Detection based onTransformation-Based Learning4.1 Transformation-Based LearningTransformation-Based Learning is a rule-based ap-proach, in which rules are automatically learnedfrom the data corpus.
It has been successfully usedin many natural language applications such as part-of-speech tagging (Brill, 1995).
Three prerequisitesfor using TBL are: an initial state annotator, a set ofpossible transformations, and an objective functionfor choosing the best transformations.Before learning, the initial state annotator adds la-bels to the training data.
The learning goes throughthe following steps iteratively until no improvementcan be achieved: (i) try each possible transformationon the training data, (ii) score each transformationwith the objective function and choose the one withthe highest score, and (iii) apply the selected trans-formation to update the training data and append itto the learned transformation list.4.2 Error Detection Based on TBLPre-defined transformation templates are the rulesallowed to be used, which play a vital role in TBL.The transformation templates are defined in the fol-lowing format:Change the word label of a word w from X to Y , ifcondition C is satisfiedwhere, X and Y take binary values: 1 (correctrecognition) and -1 (error).
Each condition C is theconjunction of sub-conditions in form of f op v,where f represents a feature, v is a possible cate-gorical value of f , and op is the possible operationssuch as <, > and =.In addition to the linguistic features introduced inSection 3, two other features are used:?
word confidence score (CS): an SR dependentfeature generated by an SR system.?
word label (label): the target of the transfor-mation rules.
Using it as a feature enables thepropagation of the effect of preceding rules.As shown in Table 1, conditions are classified intothree categories based on the incrementally enlargedcontext from which features are extracted: wordalone, local context, and sentence context.
The threecategories are further split into seven groups accord-ing to the features they used.?
L: the correctness of w depends solely on itself.Conditions only include lexical features of w.?
Local: the correctness of w depends not onlyon itself but also on its surrounding words.Conditions incorporate lexical features of sur-rounding words as well as those of w. Fur-thermore, word labels of surrounding words arealso employed as a feature to capture the effectof the correctness of surrounding words of w.?
Long: the scope of conditions for the correct-ness of w is expanded to include syntactic fea-tures.
Syntactic features of w and its surround-ing words as well as the features in Local areincorporated into conditions.
In addition, thelexical features and word labels of words thathave the shortest links to w are also incorpo-rated.?
CS: the group in which conditions only includeconfidence scores of w.?
LCS, CSLocal, CSLong: these three groupsare generated by combining the features fromL, Local, and Long with the confidence scoresof w as an additional feature respectively.lrHaslink and llinkLabel are combinations ofbasic features.
lrHaslink represents whether thepreceding word and the following word have links,44Category Group ExampleWord CS cs(wi) < ciAlone L position(wi) = ti & syllables(wi) = siLCS cs(wi) < ci & pos(wi) = piLocal Local position(wi) = ti & label(wi?1) = li?1 & word(wi) = diContext CSLocal cs(wi) < ci & position(wi) = ti & label(wi?1) = li?1 & label(wi+1) =li+1Sentence Long position(wi) = ti & lrHaslink(wi) = hi & haslink(wi) = hliContext CSLong cs(wi) < ci & position(wi) = ti & llinkLabel(wi) = lli & pos(wi) = piTable 1: Condition Categories and Examplesand llinkLabel represents the label of the word towhich w has the shortest left link.
ci, ti, si, pi, li, di,hi, hli, and lli are possible values of the correspond-ing features.The initial state annotator initializes all the wordsas correct words.
A Prolog based TBL tool, ?-TBL (Lager, 1999) 2 is used in this study.
Classi-fication accuracy is adopted as the objective func-tion.
For each transformation, its positive effect(PE) is the number of words whose labels are cor-rectly updated by applying it, and its negative ef-fect (NE) is the number of words wrongly updated.Two cut-off thresholds are used to select transfor-mations with strong positive effects: net positive ef-fect (PE ?
NE), and the ratio of positive effect(PE/(PE +NE)).5 Experimental Results and DiscussionExperiments were conducted at several levels.
Start-ing with transformation rules with word alone con-ditions, additional rules with local context and sen-tence context conditions were incorporated incre-mentally by enlarging the scope of the context.
Assuch, the results help us not only identify the ad-ditional contribution of each condition group to thetask of error detection but also reveal the importanceof enriching contextual information to error detec-tion.5.1 Data CorpusThe data corpus was collected from a user studyon a composition dictation task (Feng et al, 2003).A total of 12 participants were native speakers and2Available via http://www.ling.gu.se/?lager/mutbl.htmlnone of them used their voice for professional pur-poses.
Participants spoke to IBM ViaVoice (Millen-nium edition), which contains a general vocabularyof 64,000 words.
The dictation task was completedin a quiet lab environment with high quality micro-phones.During the study, participants were given one pre-designed topic and instructed to compose a docu-ment of around 400 words on that topic.
Beforestarting the dictation, they completed enrollments tobuild personal profiles and received training on fin-ishing the task with a different topic.
They wereasked to make corrections only after they finishedcomposing a certain length of text.
The data cor-pus consists of the recognition output of their dicta-tions excluding corrections.
Word recognition errorswere first marked by the participants themselves andthen validated by researchers via cross-referencingthe recorded audios.
The data corpus contains 4,804words.5.2 Evaluation MetricsTo evaluate the overall performance of the error de-tection, classification error rate (CER) (Equation 1),commonly used metric to evaluate classifiers, isused.
CER is the percentage of words that arewrongly classified.CER = # of wrongly classified wordstotal# of words (1)The baseline CER is derived by assuming all thewords are correct, and it has the value as the ratio ofthe total number of insertion and substitution errorsto the total number of output words.Precision (PRE) and recall (REC) on errors areused to measure the performance of identifying er-45rors.
PRE is the percentage of words classified as er-rors that are in fact recognition errors.
REC denotesthe proportion of actual recognition errors that arecategorized as errors by the classifier.
In addition,F measure (Equation 2), a single-valued metric re-flecting the trade-off between PRE and REC, is alsoused.
The baselines of PRE, REC, and F for errorare zeros, for all of the output words are assumedcorrect.F = 2 ?
PRE ?RECPRE +REC (2)5.3 Results3-fold cross-validation was used to test the system.When dividing the data corpus, sentence is treatedas an atomic unit.
The 3-fold cross-validation wasrun 9 times, and the average performance is reportedin Table 2.
The labels of rule combinations are de-fined by the connections of several symbols definedin Section 4.2.
For each rule combination, the typesof rules can be included are decided by all the possi-ble combinations of those symbols which are in Ta-ble 1.
For example, L-CS-Local-Long includes ruleswith conditions L, CS, Local, Long, LCS, CSLocaland CSLong.The threshold of net positive effect is set to 5 toensure that enough evidence has been observed, andthat of the ratio of the positive effect is set to 0.5 toensure that selected transformations have the posi-tive effects.For the combinations without CS, L-Local-Longachieves the best performance in terms of both CERand F measure.
A relative improvement of 4.85% isachieved over the baseline CER, which is relativelysmall.
One possible explanation concerns the largevocabulary size in the data set.
Although the par-ticipants were asked to compose the documents onthe same topic, the word usage was greatly diversi-fied.
An analysis of the data corpus shows that thevocabulary size is 993.Despite its best performance in linguistic featuregroups, L-Local-Long produces worse performancethan CS in both CER and F measure.
Therefore, lin-guistic features by themselves are not as useful asconfidence scores.When linguistic features are combined withCS, they provide additional improvement.
L-CSachieves a 4.58% relative improvement on CER anda 31.37% relative improvement on F measure overCS.
L-CS-Local only achieves marginal improve-ment on CER and a 7.54% relative improvement onF measure over L-CS.The best performance is generated by L-CS-Local-Long.
In particular, it boosts CER by a rel-ative improvement of 12.30% over CS and a relativeimprovement of 7.02% over L-CS-Local.
In addi-tion, it improves F measure by 53.62% and 8.74%in comparison with CS and L-CS-Local respectively.Therefore, enlarging the scope of context can lead toimproved performance on error detection.It is revealed from Table 2 that the improvementon F measure is due to the improvement on re-call without hurting the precision.
After combininglinguistic features with CS, L-CS and L-CS-Local-Long achieve 43.77% and 75.57% relative improve-ments on recall over CS separately.
Hence, thelinguistic features can improve the system?s abilityin finding more errors.
Additionally, L-CS-Local-Long achieves a 7.32% relative improvement on pre-cision over CS.The average numbers of learned rules are shownin Table 2.
With the increased number of possibleused pre-defined rules, the number of learned rulesincreases moderately.
L-CS-Local-Long and L-CS-Local have the largest number of rules, 14, which israther a small set of rules.
As discussed above, theserules are straightforward and easy to understand.Figure 2 shows CERs when the learned rules areincrementally applied in one run for L-CS-Local-Long.
Three lines represent each of the three foldsseparately, and the number of learned rules differsamong folds.101112131415161718190 1 2 3 4 5 6 7 8 9 10 11 12 13 14Number of RulesCER(%)fold1fold2fold3Figure 2: Relations of CERs with Number of Rules46Combination Mean Std.
Mean Mean Mean Mean #CER (%) Dev PRE (%) REC (%) F (%) of rulesBaseline 15.66 0.06 - - - -L 15.55 0.11 61.85 2.04 3.88 3L-Local 15.58 0.14 60.88 2.19 4.17 4L-Local-Long 14.90 0.10 61.67 13.83 22.37 8CS 14.64 0.09 61.03 21.98 31.50 1L-CS 13.97 0.15 61.48 31.60 41.38 8L-CS-Local 13.81 0.18 61.28 35.52 44.50 14L-CS-Local-Long 12.84 0.21 65.50 38.59 48.39 14Table 2: Performance of Transformation Rule CombinationsAfter the first several rules are applied, CERs dropsignificantly.
Then the changes in CERs becomemarginal as additional rules are applied.
The fold1 and 3 reach the lowest CER after the last rule isapplied, and fold 2 reaches the lowest CERs in themiddle.
Thus, the top ranked rules are mostly useful.One advantage of TBL is that the learning resultcan be easily interpreted.
The following is the topsix rules learned in fold 3 in Figure 2.Mark a word as an error, if :?
its confidence score is less than 0; it is in themiddle of a sentence; and it is a null-link word.?
its confidence score is less than -5; it is in themiddle of a sentence; and it has links to preced-ing words.?
its confidence score is less than 0; it is the firstword of a sentence; and it is a null-link word.?
its confidence score is less than 2; it is in themiddle of a sentence; it has 1 syllable; and theword following it also has 1 syllable and is anerror.?
its confidence score is less than -1; and both itspreceding and following words are errors.Mark a word as a correct word, if :?
its confidence score is greater than -1; and bothits preceding and following words are correctwords.All of the above six rules include word confidencescore as a feature.
Rule 1 and rule 3 suggest thatnull-link words are good indicators of errors, whichconfirms our hypothesis.
Rule 2 shows that a wordwith low confidence score may also be an error evenif it is part of the linkage of the sentence.
Rule 4shows continuous short words are possible errors.Rule 5 indicates that a word with low confidencescore may be an error if its surrounding words are er-rors.
Rule 6 is a rule to compensate for the wronglylabeled words by previous rules.6 Conclusion and Future WorksWe introduced an error detection method based onfeature combinations.
Transformation-based learn-ing was used as the classifier to combine linguisticfeatures with word confidence scores.
Two kindsof linguistic features were selected: lexical fea-tures extracted from words themselves, and syntac-tic features from the parsing result of link grammar.Transformation templates were defined by varyingscope of the context.
Experimental results on a dic-tation corpus showed that although linguistic fea-tures alone were not as useful as word confidencescores to error detection, they provided complemen-tary information when combined with word confi-dence score.
Moreover, the performance of error de-tection was improved incrementally as the scope ofcontext was enlarged, and the best performance wasachieved when sentence context was considered.
Inparticular, enlarging the context modeled by linguis-tic features improved the capability of error detec-tion by finding more errors without deteriorating andeven improving the precision.The proposed method has been tested using a dic-tation corpus on a topic related to office environ-47ment.
We are working on evaluating the methodon spontaneous dictation utterances from the CSR-IIcorpus, and other monologue corpora such as Broad-cast News.
The method can be extended by incorpo-rating lexical semantic features from the semanticanalysis of recognition output to detect semantic er-rors that are likely overlooked by syntactic analysis.AcknowledgementThis work is supported by the National ScienceFoundation under Grant# 0328391.
Any opinions,findings and conclusions or recommendations ex-pressed in this material are those of the authors anddo not necessarily reflect the views of the NationalScience Foundation (NSF).ReferencesEric Brill, Radu Florian, John C. Henderson, and LidiaMangu.
1998.
Beyond n-grams: Can linguistic so-phistication improve language modeling?
In Proceed-ings of COLING/ACL, pages 186?190.Eric Brill.
1995.
Transformation-based error-drivenlearning and natural language processing: A casestudy in part of speech tagging.
Computational Lin-guistics, 21(4):543?565.Paul Carpenter, Chun Jin, Daniel Wilson, Rong Zhang,Dan Bohus, and Alex Rudnicky.
2001.
Is this conver-sation on track?
In Proceedings of Eurospeech, pages2121?2124.Lin L. Chase.
1997.
Error-Responsive Feedback Mecha-nisms for Speech Recognizers.
Ph.D. thesis, School ofComputer Science, CMU, April.Jinjuan Feng, Andrew Sears, and Clare-Marie Karat.2003.
A longitudinal investigation of hands-freespeech based navigation during dictation.
Technicalreport, UMBC.Dennis Grinberg, John Lafferty, and Daniel Sleator.1995.
A robust parsing algorithm for link grammars.Technical Report CMU-CS-95-125, Carnegie MellonUniversity.Torbjo?rn Lager.
1999.
The ?-tbl system: Logic program-ming tools for transformation-based learning.
In Pro-ceedings of the third international workshop on com-putational natural language learning.Lidia Mangu and Mukund Padmanabhan.
2001.
Errorcorrective mechanisms for speech recognition.
In Pro-ceedings of ICASSP, volume 1, pages 29?32.Christine Pao, Philipp Schmid, and James Glass.
1998.Confidence scoring for speech understanding systems.In Proceedings of ICSLP, pages 815?818.Manny Rayner, David Carter, Vassilios Digalakis, andPatti Price.
1994.
Combining knowledge sources toreorder n-best speech hypothesis lists.
In Proceedingsof the ARPA Workshop on Human Language Technol-ogy, pages 212?217.Rube?n San-Segundo, Bryan Pellom, Kadri Hacioglu, andWayne Ward.
2001.
Confidence measures for spo-ken dialogue systems.
In Proceedings of ICASSP, vol-ume 1, pages 393?396.Ruhi Sarikaya, Yuqing Gao, and Michael Picheny.
2003.Word level confidence measurement using semanticfeatures.
In Proceedings of ICASSP, volume 1, pages604?607.Andrew Sears, Clare-Marie Karat, Kwesi Oseitutu, Az-far S. Karimullah, and Jinjuan Feng.
2001.
Productiv-ity, satisfaction, and interaction strategies of individ-uals with spinal cord injuries and traditional users in-teracting with speech recognition software.
UniversalAccess in the Information Society, 1(1):4?15, June.Gabriel Skantze and Jens Edlund.
2004.
Early error de-tection on word level.
In Proceedings of Robust.Daniel Sleator and Davy Temperley.
1993.
Parsing eng-lish with a link grammar.
In Proceedings of the thirdinternational workshop on parsing technologies.Rong Zhang and Alexander I. Rudnicky.
2001.
Wordlevel confidence annotation using combinations of fea-tures.
In Proceedings of Eurospeech, pages 2105?2108.Lina Zhou, Yongmei Shi, Jinjuan Feng, and AndrewSears.
2005.
Data mining for detecting errors in dicta-tion speech recognition.
IEEE Transactions on Speechand Audio Processing, Special Issues on Data Miningof Speech, Audio and Dialog, 13(5), September.48
