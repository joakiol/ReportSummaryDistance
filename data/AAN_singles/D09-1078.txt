Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 746?755,Singapore, 6-7 August 2009. c?2009 ACL and AFNLPLess is More: Significance-Based N-gram Selectionfor Smaller, Better Language ModelsRobert C. Moore Chris QuirkMicrosoft ResearchRedmond, WA 98052, USA{bobmoore,chrisq}@microsoft.comAbstractThe recent availability of large corporafor training N-gram language models hasshown the utility of models of higher or-der than just trigrams.
In this paper, weinvestigate methods to control the increasein model size resulting from applying stan-dard methods at higher orders.
We in-troduce significance-based N-gram selec-tion, which not only reduces model size,but also improves perplexity for severalsmoothing methods, including Katz back-off and absolute discounting.
We alsoshow that, when combined with a newsmoothing method and a novel variant ofweighted-difference pruning, our selectionmethod performs better in the trade-off be-tween model size and perplexity than thebest pruning method we found for modi-fied Kneser-Ney smoothing.1 IntroductionStatistical language models are potentially usefulfor any language technology task that producesnatural-language text as a final (or intermediate)output.
In particular, they are extensively used inspeech recognition and machine translation.
De-spite the criticism that they ignore the structure ofnatural language, simple N-gram models, whichestimate the probability of each word in a textstring based on the N?1 preceding words, remainthe most widely-used type of model.Until the late 1990s, N-gram language modelsof order higher than trigrams were seldom used.This was due, at least in part, to the fact theamounts of training data available did not producesignificantly better results from higher-order mod-els.
Since that time, however, increasingly largeamounts of language model training data have be-come available ranging from approximately onebillion words (the Gigaword corpora from theLinguistic Data Consortium) to trillions of words(Brants et al, 2007).
With these larger resources,the use of language models based on 5-grams to7-grams is becoming increasingly common.The problem we address here is that, even whenrelatively modest amounts of training data areused, high-order N-gram language models esti-mated by standard techniques can be impracticallylarge.
Hence, we investigate ways of buildinghigh-order N-gram language models without dra-matically increasing model size.
This is, of course,the same goal behind much previous work on lan-guage model pruning, including that of Seymoreand Rosenfeld (1996), Stolcke (1998), and Good-man and Gao (2000).
We take a novel approach,however, which we refer to as significance-basedN-gram selection.
We reject a higher-order esti-mate of the probability of a particular word in aparticular context whenever the distribution of ob-servations for the higher-order estimate providesno evidence that the higher-order estimate is bet-ter than our backoff estimate.Perhaps our most surprising result is thatsignificance-based N-gram selection not only re-duces language model size, but it also improvesperplexity when applied to a number of widely-used smoothing methods, including Katz backoffand several variants of absolute discounting.1 Incontrast, experiments applying previous pruningmethods to Katz backoff (Seymore and Rosen-feld, 1996; Stolcke, 1998) and absolute discount-ing (Goodman and Gao, 2000) always found thelowest perplexity model to be the unpruned model.We tested significance-based selection on onlyone smoothing method without obtaining im-proved perplexity: modified Kneser-Ney (KN)1For most of the standard smoothing methods mentionedhere, we refer the reader to the excellent comparative studyof smoothing methods by Chen and Goodman (1998).
Refer-ences to the original sources may be found there.746smoothing (Chen and Goodman, 1998).
Thisis unfortunate, because modified KN smoothinggenerally seems to have the lowest perplexity ofany known smoothing method for N-gram lan-guage models; in our tests it had a lower perplex-ity than any of the other models, with or with-out significance-based N-gram selection.
How-ever, when we compared modified KN smooth-ing to our best results applying N-gram selectionto other smoothing methods for multiple N-gramorders, two of our models outperformed modifiedKN in terms of perplexity for a given model size.Of course, the trade-off between perplexity andmodel size for modified KN can also be im-proved by pruning.
So, in a final set of ex-periments we found the best combinations wecould for pruned modified KN models, and we didthe same for our best model using significance-based selection.
The best pruning method forthe latter turned out to be a novel modifica-tion of weighted-difference pruning (Seymore andRosenfeld, 1996) that was especially convenientto compute given our method for performingsignificance-based N-gram selection.
The final re-sult is that our best model using significance-basedselection and modified weighted difference prun-ing always had a better size/perplexity trade-offthan pruned modified KN, with up to about 8%perplexity reduction for a given model size.2 Significance-Based N-gram SelectionThe idea of using a statistical test to decidewhether to use a higher- or lower-order estimate ofan N-gram probablity is not new.
It was perhapsfirst proposed by Ron, et al (1996), who suggestedusing a threshold on relative entropy (Kullback-Liebler divergence) as an appropriate test to de-cide whether to extend the context used to predictthe next token in a sequence.
Stolcke (1998) usedthe same metric in his work on language modelpruning, and he also pointed out that weighted dif-ference pruning is, in fact, an approximation ofrelative entropy pruning.
However, while relativeentropy pruning is based on a statistical test, it isnot a significance test.
The difference in probabil-ity represented by a certain relative entropy valuecan be statistically significant when measured ona large corpus, but not significant when measuredon a small corpus.The primary test we use to choose betweenhigher- or lower-order estimates of an N-gramprobablity is inspired by an insight of Jedynak andKhudanpur (2005).
They note that, given a setof y observations of a multinomial distribution,the observed counts will have the highest proba-bilty of any possible set of y observations for themaximum likelihood estimate (MLE) model de-rived from the relative frequencies of those obser-vations.
In general, however, the MLE model willnot be the only model for which this set of obser-vations is the most probable set of y observations.Jedynak and Khudanpur call the set of such mod-els the maximum likelihood set (MLS) for the ob-servations.Jedynak and Khudanpur argue that the obser-vations alone do not support choosing the MLEover other members of the MLS.
The MLE mayassign the observations a higher probability thanother members of the MLS, but that may be anaccident of what outcomes are possible given thenumber of observations.
If we flip a coin 9 timesand get 5 heads, is there any reason to believe thatthe probability of heads is closer to the MLE 5/9than it is to 5/10?
No, because 5/9 is as close aswe can come to 5/10, given 9 observations.We apply this insight to the problem of N-gram selection as follows: For each word wnin a context w1...wn?1with a backoff estimatefor the probability of that word in that context?
p(wn|w2...wn?1),2 we do not include an explicitestimate of p(wn|w1...wn?1) in our model, if thebackoff estimate is within the MLS of the countsfor w1...wnand w1...wn?1.This requires finding the MLS of a set of obser-vations only for binomial distributions (rather thanthe general multinomial distributions studied byJedynak and Khudanpur), which has a very sim-ple solution:MLS(x, y) ={p???
?xy + 1?
p ?x+ 1y + 1}where x is the count for w1...wn, y is the count forw1...wn?1, and p is a possible backoff probabiltyestimate for p(wn|w1...wn?1).
In this case, theMLS is the set of binomial distributions that havex successes as their mode given y trials, which iswell-known to be specified by this formula.We describe this method as ?significance-based?
because we can consider our criterion asa significance test in which we take the backoff2p(wn|w2...wn?1) being the next lower-order estimate,and ?
being the backoff weight for the context w1...wn?1.747probability estimate as the null hypothesis for theestimate in the higher-order model, and we set therejection threshold to the lowest possible value;we reject the null hypothesis (the backoff probabil-ity) if there are any outcomes for the given numberof trials that are more likely, according to the nullhypothesis, than the one we observed.We make a few refinements to this basic idea.First, we never add an explicit higher-order esti-mate to our model, if the next lower-order estimateis not explicitly stored in the model.
This enablesus to keep only the next lower-order model avail-able while performing N-gram selection.Next, we observe that in some cases the higher-order estimate for p(wn|w1...wn?1) may not fallwithin the MLS for the observed counts, due tosmoothing.
In this case, we prefer the backoffprobability estimate if it lies within the MLS or be-tween the smoothed higher-order estimate and theMLS.
Otherwise, we would reject the backoff es-timate for being outside the MLS, only to replaceit with a higher-order estimate even farther outsidethe MLS.Finally, we note that the backoff probability es-timate for an N-gram not observed in the train-ing data sometimes falls outside the correspondingMLS, which in the 0-count case simplifies toMLS(0, y) ={p???
?0 ?
p ?1y + 1}When this happens, we include an explicit higher-order estimate p(wn|w1...wn?1) = 1/(y + 1),which is the upper limit of the MLS.
This is similarto Rosenfeld and Huang?s (1993) ?confidence in-terval capping?
method for reducing unreasonablyhigh backoff estimates for unobserved N-grams.In order to apply this treatment of 0-count N-grams, we sort the explicitly-stored N-grams foreach backoff context by decreasing probability.For each higher-order context, to find the 0-countN-grams subject to the 1/(y + 1) limit, we tra-verse the sorted list of explicitly-stored N-gramsfor its backoff context.
When we encounter an N-gram whose extension to the higher-order contextwas not observed in the training data, we give itan explicit probability of 1/(y+1), if its weightedbackoff probability is greater than that.
We stopthe traversal as soon as we encounter an N-gramfor the backoff context that has a weighted backoffprobability less than or equal to 1/(y+1), which inpractice means we actually examine only a smallnumber of backoff probabilities for each context.3 Finding Backoff Weights by IterativeSearchThe approach described above is very attractivefrom a theoretical perspective, but it has one prac-tical complication.
To decide which N-grams foreach context to explicitly include in the higher-order model, we need to know the backoff weightfor the context, but we cannot compute the backoffweight until we know exactly which higher-orderN-grams are included in the model.We address this problem by iteratively solvingfor a backoff weight that yields a normalized prob-ability distribution.
For each context, we guessan initial value for the backoff weight and keeptrack of the sum of the probabilites resulting fromapplying our N-gram selection method with thatbackoff weight.
If the sum is greater than 1.0, bymore than a convergence threshold, we reduce theestimated backoff weight and iterate.
If the sumis less than 1.0, by more than the threshold, weincrease the estimated weight and iterate.It is easy to see that, for all standard smooth-ing methods, the function from backoff weightsto probability sums is piece-wise linear.
Withina region where no decision changes about whichN-grams to include in the model, the probabilitysum is a linear function of the backoff weight.
Atvalues of the backoff weight where the set of se-lected N-grams changes, the function can be dis-continous.
With a little more effort, one can seethat the linear segments overlap with respect to theprobability sum in such a way that there will al-ways be one or more values of the backoff weightthat make the probability sum equal 1.0, with onespecific exception.The exception arises because of the capping ofbackoff probabilites for unobserved N-grams.
Itis possible for there to be a context for whichall observed N-grams are included in the higher-order model, the probabilities for all unobservedN-grams are either capped at 1/(y + 1) or effec-tively 0 due to arithmetic underflow, and the prob-ability sum is less than 1.0.
For some smoothingmethods, the probability sum cannot be increasedin this situation by increasing the backoff weight.We check for this situation, and if it arises, weincrease the cap on the 0-count probability justenough to make the probability sum equal 1.0.That exception aside, we iteratively find back-off weights as follows: For an initial estimateof the backoff weight for a context, we compute748what the backoff weight would be for the basesmoothing method without N-gram selection.
Ifthat value is less than 1.0, we use it as our ini-tial estimate, otherwise we use 1.0, which annec-dotally seems to produce better models than ini-tial estimates greater than 1.0, in situations wherethere are multiple solutions.
If the first iteration ofN-gram selection produces a probability sum lessthan 1.0, we repeatedly double the estimated back-off weight until we obtain a sum greater than orequal to 1.0, or we encounter the special situationpreviously described.
If the initial probability sumis greater than 1.0, we repeatedly halve the esti-mated backoff weight until we obtain a sum lessthan or equal to 1.0.Once we have values for the backoff weight thatproduce probability sums on both sides of 1.0, wehave a solution bracketed, and we can use standardnumerical search techniques to find that solution.At every subsequent iteration, we try a value forthe backoff weight between the largest value wehave tried that produces a sum less than 1.0 andthe smallest value we have tried that produces asum greater than 1.0.
We stop when the differencebetween these values of the backoff weight is lessthan a convergence threshold.We use a combination of simple techniques tochoose the next value of the backoff weight to try.The primary technique we use is called the ?falseposition method?, which basically solves the lin-ear equation defined by the two current bracketingvalues and corresponding probability sums.
Theadvantage of this method is that, if our bracket-ing points lie on the same linear segment of ourfunction, we obtain a solution in one step.
Thedisadvantage of the method is that it sometimesapproaches the solution by a long sequence of tinysteps from the same side.We try to detect the latter situation by keepingtrack of the number of consecutive iterations thatmake a step in the same direction.
If this num-ber reaches 10, we take the next step by the bi-section method, which simply tries the value ofthe backoff weight halfway between our two cur-rent bracketing values.
In practice, this combinedsearch method works very well, taking an averageof less than four iterations per backoff weight.4 Modified Weighted-Difference PruningWhile the N-gram selection method describedabove considerably reduces the number of para-meters in a high-order language model, we maywish to reduce language model size even more.The concept of significance-based N-gram selec-tion to produce smaller models could be extendedby relaxing our criterion for using backoff distrib-utions in place of explicit higher-order probabilityestimates, but true significance tests at more re-laxed thresholds that are accurate for small countsare expensive to compute; so we resort to moreconventional language model pruning methods.In our experiments, we tried four methods foradditional pruning: simple count cutoffs, relativeentropy pruning (REP) (Stolcke, 1998), and twomodified versions of Seymore and Rosenfeld?s(1996) weighted-difference pruning (WDP).
In thenotation we have been using, Seymore and Rosen-feld?s WDP criterion for using a backoff estimate,in place of an explicit higher-order estimate, is thatthe quantityK?
(log(p(wn|w1...wn?1))?log(?up(wn|w2...wn?1)))be less than a pruning threshold, where K isthe Good-Turing-discounted training set count forw1...wn, and ?uis the backoff weight for the un-pruned model.The first of our modified version of WDP usesthe following quantity instead:p(w1...wn)??????log(p(wn|w1...wn?1))?log(?pp(wn|w2...wn?1))????
?where p(w1...wn) is an estimate of the probabilityof w1...wnand ?pis the backoff weight for thepruned model.We make three modifications to WDP in thisformula.
First, we follow a suggestion of Stol-cke (1998) by replacing the discounted trainingset count K of w1...wnwith an estimate the jointprobability of w1...wn, computed by chaining theexplicit probability estimates, according to ourmodel, for all N-gram lengths up to n.The second modification to WDP is that we usethe absolute value of the difference of the log prob-abilities.
By using the signed difference of the logprobabilities, Seymore and Rosenfeld will alwaysprune a higher-order probability estimate if it isless than the backoff estimate.
But the backoff es-timate may well be too high.
Using the absolutevalue of the difference avoids this problem.749p(wn|w1.
.
.
wn?1) =???????????
?w1...wn?1C(w1...wn)?Dn,C(w1...wn)C(w1...wn?1)+ ?w1...wn?1p(wn|w2.
.
.
wn?1)if C(w1.
.
.
wn) > 0?w1...wn?1p(wn|w2.
.
.
wn?1) if C(w1.
.
.
wn) = 0?w1...wn?1= ?|{w?|C(w1...wn?1w?
)>0}|C(w1...wn?1)?w1...wn?1= 1?
?w1...wn?1Figure 1: New language model smoothing methodThe final modification is that we compute thedifference in log probability with respect to thebackoff weight for the pruned model rather thanthe unpruned model, which we are able to do byperforming the pruning inside our iterative searchfor the value of the backoff weight.
We do thisbecause, if the backoff weight is changed signifi-cantly by pruning, backoff estimates that meet thepruning criterion with the old backoff weight mayno longer meet the criterion with the new back-off weight, and vice versa.
Since the new backoffweight is the one that will be used in the prunedmodel, that seems to be the one that should be usedto make pruning decisions.Our second variant of modified WDP is like thefirst, but it estimates p(w1...wn) simply by divid-ing Seymore and Rosenfeld?s discounted N-gramcount K by the total number of highest-order N-grams in the training corpus.
This is equivalent tosmoothing only the highest-order conditional N-gram model in estimating p(w1...wn), estimatingall the lower-order probabilities in the chain by thecorresponding MLE model.
We refer to this jointprobability estimate as ?partially-smoothed?, andthe one suggested by Stolcke as ?fully-smoothed?.5 EvaluationWe carried out three sets of evaluations to testthe new techniques described above.
First wecompared the perplexity of full models and mod-els reduced by significance-based N-gram selec-tion for seven language model smoothing meth-ods.
For the best three results in that comparison,we looked at the trade-off between perplexity andmodel size over a range of N-gram orders.
Finally,we tried various pruning methods to further reducemodel size, and then compared the best result weobtained using previous techniques with the bestresult we obtained using our new techniques.5.1 Data and Base Smoothing MethodsFor training, parameter optimzation, and test datawe used English text from the WMT-06 Europarlcorpus (Koehn and Monz, 2006).
We trained onthe designated 1,003,349 sentences (27,493,499words) of English language model training data,and used 2000 sentences each for testing and pa-rameter optimization, from the English half of theEnglish-French dev and devtest data sets.We conducted our experiments on seven lan-guage model smoothing methods.
Five of theseare well-known: (1) interpolated absolute dis-counting with one discount per N-gram length, es-timated according to the formula derived by Neyet al (1994); (2) Katz backoff with Good-Turingdiscounts for N-grams occurring 5 times or less;(3) backoff absolute discounting with Ney et alformula discounts; (4) backoff absolute discount-ing with one discount used for all N-gram lengths,optimized on held-out data; (5) modified interpo-lated Kneser-Ney smoothing with three discountsper N-gram length, estimated according to the for-mulas suggested by Chen and Goodman (1998).We also experimented with two variants of anew smoothing method that we have recently de-veloped.
Full details of the new method are givenelsewhere (Moore and Quirk, 2009), but since it isnot well-known, we summarize the method here.Smoothed N-gram probabilities are defined by theformulas shown in Figure 1, for all n such thatN ?
n ?
2,3 where N is the greatest N-gramlength used in the model.
The novelty of thismodel is that, while it is an interpolated model, theinterpolation weights ?
for the lower-order model3For n = 2, we take the expression p(wn|w2.
.
.
wn?1)to denote a unigram probability estimate p(w2).750base select percentMethod PP PP change1 interp-AD-fix 62.6 61.6 -1.62 Katz backoff 59.8 56.1 -7.93 backoff-AD-fix 59.9 54.3 -9.34 backoff-AD-opt 58.8 54.4 -7.55 KN-mod-fix 51.6 54.6 +5.86 new-fix 56.1 52.1 -7.17 new-opt 53.7 52.0 -3.3Table 1: Perplexity results for N-gram selectionare not constrained to match the backoff weights?
for the lower-order model.
This allows the in-terpolation weights to be set independently of thediscounts D, with the backoff weights being ad-justed to normalize the resulting distributions.The motivation for this is to let the D para-meters correct for potential overestimation of theprobabilities for observed N-grams, while the ?parameter (which determines the ?
and ?
interpo-lation parameters) somewhat independently cor-rects for quantization errors caused by the fact thatonly certain probabilities can be derived from in-teger observed counts, even after discounting.
?
isinterpretable as the estimated mean quantizationerror for each distinct count for a given context.We tested two variants of the new method, (6)one in which the D parameters and the ?
parameterare set by fixed criteria, and (7) one in which a sin-gle value for all D parameters and the value of the?
parameter are optimized on held-out data.
Forthe fixed value of ?, we assume that, since the dis-tance between possible N-gram counts, after dis-counting, is approximately 1.0, their mean quan-tization error would be approximately 0.5.
Forthe fixed discount parameters, we use three valuesfor each N-gram length: D1for N-grams whosecount is 1, D2for N-grams whose count is 2, andD3for N-grams whose count is 3 or more.
Weset these values to be the discounts for 1-counts,2-counts, and 3-counts estimated by the Good-Turing method.
This yields the formulaDr= r ?
(r + 1)Nr+1Nr,for 1 ?
r ?
3, where Nris the number of distinctN-grams of the length in question occuring r timesin the training set.In all experiments, the unigram languagemodel is an un-smoothed, closed-vocabulary MLEmodel.
We use this unigram model, because thereis no simple, principled way of assigning prob-abilities to individual out-of-vocabulary (OOV)words.
The only principled solution to this prob-lem that we are aware of is to use a character-based model, but this seems overly complicatedfor something that is orthogonal to the main pointsof this study, and of minor practical importance.Since we make no provision for OOV words in themodels, OOV words are also omitted from all per-plexity measurements.
Thus, the perplexity num-bers are systematically lower than they would beif OOVs were taken into account, but they are allcomparable in this regard.5.2 Results for Significance-Based N-gramSelectionTable 1 shows the minimum perplexity (with re-spect to N-gram order) of language models up to7-grams for each of the seven smoothing methodsdiscussed above, with and without significance-based N-gram selection.
N-gram selection im-proved the perplexity of all models, except formodified KN.
The lowest overall perplexity re-mains that of the base modified KN method, butwith N-gram selection, the two variants of the newsmoothing method come very close to it.If we cared only about perplexity, that would bethe end of the story, but we also care about lan-guage model size.
The results in Table 1 were ob-tained on models estimated using just the countsneeded to cover the parameter optimization andtest sets; so to accurately measure model size, wetrained full language models using base modifedKN, and the two variants of the new method withN-gram selection.
The resulting sizes of the mod-els represented in backoff form (in terms of totalnumber of probability and backoff parameters) areshown in Figure 2 as function of N-gram length,from trigrams up to 7-grams for KN and up to10-grams for the two new models.
We see thatbeyond 4-grams the model sizes diverge dramati-cally, with the new models incorporating N-gramselection leveling off, but the modified KN model(or any standard model) continuing to grow in size,apparently linearly in the N-gram order.In Figure 3, we show the relationship betweenperplexity and model size for the same threemodels, varying N-gram order.
We see that be-tween about 20 million and 45 million parameters,both of the new models incorporating significance-7510204060801001201401601803 4 5 6 7 8 9 10Millions of modelparametersN-gram lengthNo N-gramselectionnew-opt +N-gram selectionnew-fix +N-gram selectionFigure 2: Model size vs. N-gram length51525354555657585960610 10 20 30 40 50 60 70 80 90 100 110 120 130 140 150 160PerplexityMillions of Model ParametersKN -mod -fixnew-opt +N-gram selectionnew-fix +N-gram selectionFigure 3: Perplexity vs. model sizebased N-gram selection seem to outperform mod-ified KN, and that the best of the three is, in fact,the new model with fixed parameter values.5.3 Results for Additional PruningWe further tested modified KN smoothing, and ournew smoothing method with fixed parameter val-ues and significance-based N-gram selection, withadditional pruning.
We compared several pruningmethods on trigram models: count cutoffs, REP,4and our two modified versions of WDP.Figure 4 shows the resulting combinations ofperplexity and model size for REP and modifiedWDP at various pruning thresholds, and for countcutoffs of 1, 2, and 3 for both bigrams and trigrams(n > 1) and for trigrams only (n > 2), applied to4Thanks to Asela Gunawardana for the use of his REPtool.our new smoothing method with fixed parametervalues, together with significance-based N-gramselection.
Overall, modified WDP with fully-smoothed joint probability estimates performs thebest.
It is has lower perplexity than count cut-offs at all model sizes tested, and is about equalto REP at very severe pruning levels and superiorto REP with less pruning.
Modified WDP withfully-smoothed joint probabilities is about equalto modified WDP with partially-smoothed jointprobabilities at the highest and lowest pruning lev-els tested, but superior in between.Figure 4 also shows the result of applyingmodified WDP with fully-smoothed joint prob-abilities to our new smoothing method with-out significance-based N-gram selection, to testwhether the former subsumes the gains from thelatter.
We see that modified WDP does not render75260616263646566676869707172730 1 2 3 4 5 6 7 8 9 10PerplexityMillions of Model Parameterscount cutoffs for n > 1count cutoffs for n > 2relative entropy pruningmodifed WDP w/fully -smoothed joint probs,wo/N -gram selectionmodified WDP w/partially -smoothed joint probsmodified WDP w/fully -smoothed joint probsFigure 4: Pruning methods for new smoothing technique with N-gram selection606162636465666768697071720 1 2 3 4 5 6 7 8 9 10 11PerplexityMillions of Model Parametersmodified WDP w/fully -smoothed joint probsrelative entropy pruningmodified WDP w/partially -smoothed joint probscount cutoffs for n > 1count cutoffs for n > 2Figure 5: Pruning methods for modified KN smoothingN-gram selection redundant except at very severepruning levels, much like REP.Figure 5 shows the results of applying thesame four pruning methods to KN smoothing.Count cutoffs clearly perform the best with KNsmoothing.
It is interesting to note, however,that?contrary to the results for our new smooth-ing method?with KN smoothing, modified WDPwith partially-smoothed joint probabilities is sig-nificantly better than either REP or modified WDPwith fully-smoothed joint probabilities.
We be-lieve this is due to the fact that the latter two meth-ods both estimate the joint probabilities by chain-ing the lower-order conditional probabilities fromthe fully-smoothed model, which in the case ofKN smoothing are designed specifically to coverN-grams that have not been observed, and are poorestimates for the probabilities of lower-order N-grams that do occcur in the training data.Finally, we compared the new smoothingmethod with N-gram selection and modified WDPwith fully-smoothed joint probabilities againstmodified KN smoothing with count cutoffs, us-ing combinations of pruning parameter values andN-gram order that yielded the best size/perplexitytrade-offs.
The results are shown in Figure 6.
Atall model sizes within the range of these experi-ments, the new method with significance-based N-gram selection and modified WDP had lower per-plexity than modifed KN with count cutoffs?upto about 8% lower at greater pruning levels.This experiment also suggests that thesize/perplexity trade-off is easier to optimizefor our new combination of smoothing, N-gramselection, and modified WDP, than for KNsmoothing with count cut-offs.
Table 2 shows the75352535455565758596061626364656667686970710 1 2 3 4 5 6 7 8 9 10 11 12 13PerplexityMillions of Model ParametersKN -mod -fix withcount cutoffsnew-fix with N -gramselection andmodified WDPFigure 6: Comparison of two best pruned language modelsPP N CC n >69.9 3 4 164.7 4 4 162.1 4 3 159.0 4 2 156.5 4 2 254.4 4 1 253.6 5 1 253.4 6 1 253.3 7 1 2Table 2: Optimal pruning parameters for KN-mod-fix with count cutoffsperplexity (PP), maximum N-gram length (N),count cutoff (CC), and N-gram lengths to whichthe count cutoffs are applied (n >) for the pointson the curve for pruned KN in Figure 6.
Althoughsome tendencies are discernable, it seems clearthat a significant part of the space of combinationsof N, CC, and ?n >?
parameter values must besearched to find the best points for trading offperplexity against model size.
Table 3 showsmaximum N-gram length and pruning thresholdvalues for the points on the corresponding curvefor our new approach.
Here the situation is muchsimpler.
The best trade-off points are found byvarying the pruning threshold, and includingin the model all N-grams that pass the pruningthreshold, regardless of N-gram length.6 ConclusionsWe have shown that significance-based N-gramselection can simultaneously reduce both modelPP N threshold67.2 10 10?6.562.7 10 10?6.7559.3 10 10?7.056.4 10 10?7.2554.6 10 10?7.553.7 10 10?7.7553.2 10 10?8.0Table 3: Optimal pruning parameters for new-fixwith N-gram selection and modified WDPsize and perplexity when applied to a number oflanguage model smoothing methods, including thewidely-used Katz backoff and absolute discount-ing methods.
We are not aware of any other tech-nique that does this.
We also found that, whencombined with a new smoothing method and anovel variant of weighted difference pruning, ourN-gram selection method outperformed modifiedKneser-Ney smoothing?using the best form ofpruning we found for that approach?with respectto the trade-off between model size and modelquality.As our next steps, first, we need to verify thatthe results obtained on a moderate-sized train-ing corpus are repeatable on much larger corpora.Second, we plan to extend this work to incorpo-rate language model size reduction by word clus-tering, which has been shown by Goodman andGao (2000) to produce additional gains when com-bined with previous methods of language modelpruning.754ReferencesBrants, Thorsten, Ashok C. Popat, Peng Xu, FranzJ.
Och, and Jeffrey Dean.
2007.
Large languagemodels in machine translation.
In Proceedingsof EMNLP 2007, 858?867.Chen, Stanley F., and Joshua Goodman.
1998.An empirical study of smoothing techniques forlanguage modeling.
Technical Report TR-10-98, Harvard University.Goodman, Joshua, and Jianfeng Gao.
2000.
Lan-guage model size reduction by pruning andclustering.
In Proceedings of ICSLP 2000, 110?113.Jedynak, Bruno M., and Sanjeev Khudanpur.2005.
Maximum likelihood set for estimating aprobability mass function.
Neural Computation17, 1?23.Koehn, Philipp, and Christof Monz.
2006.
Manualand automatic evaluation of machine translationbetween European languages.
In Proceedingsof WMT 2006, 102?121.Moore, Robert C., and Chris Quirk.
2009.
Im-proved smoothing for N-gram language mod-els based on ordinary counts.
In Proceedingsof ACL-IJCNLP 2009.Ney, Hermann, Ute Essen, and Reinhard Kneser.1994.
On structuring probabilistic dependen-cies in stochastic language modelling.
Com-puter Speech and Language, 8, 1?38.Ron, Dana, Yoram Singer, and Naftali Tishby.1996.
The power of amnesia: learning proba-bilistic automata with variable memory length.Machine Learning, 25, 117?149.Rosenfeld, Ronald, and Xuedong Huang.
1993.Improvements in stochastic language modeling.In Proceedings of HLT 1993, 107?111.Seymore, Kristie, and Ronald Rosenfeld.
1996.Scalable Trigram Backoff Language Models.
InProceedings of ICSLP 1996.
232?235.Stolcke, Andreas.
1998.
Entropy-based pruningof backoff language models.
In Proceedings,DARPA News Transcription and UnderstandingWorkshop 1998, 270?274.755
