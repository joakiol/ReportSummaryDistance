2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 731?741,Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational LinguisticsExploring Semi-Supervised Coreference Resolution of Medical Conceptsusing Semantic and Temporal FeaturesPreethi Raghavan?, Eric Fosler-Lussier?, and Albert M.
Lai?
?Department of Computer Science and Engineering?Department of Biomedical InformaticsThe Ohio State University, Columbus, Ohio, USA{raghavap, fosler}@cse.ohio-state.edu, albert.lai@osumc.eduAbstractWe investigate the task of medical conceptcoreference resolution in clinical text usingtwo semi-supervised methods, co-training andmulti-view learning with posterior regulariza-tion.
By extracting semantic and temporalfeatures of medical concepts found in clinicaltext, we create conditionally independent dataviews; co-training MaxEnt classifiers on thisdata works almost as well as supervised learn-ing for the task of pairwise coreference resolu-tion of medical concepts.
We also train Max-Ent models with expectation constraints, usingposterior regularization, and find that poste-rior regularization performs comparably to orslightly better than co-training.
We describethe process of semantic and temporal featureextraction and demonstrate our methods on acorpus of case reports from the New EnglandJournal of Medicine and a corpus of patientnarratives obtained from The Ohio State Uni-versity Wexner Medical Center.1 IntroductionThe clinical community creates and uses a varietyof semi-structured and unstructured electronic tex-tual documents that include medical reports suchas admission notes, progress notes, pathology re-ports, radiology reports and hospital discharge sum-maries.
The documents, collectively termed clini-cal narratives, account for various medical condi-tions, procedures, diagnoses and assessments in apatient?s medical history.
Researchers have inves-tigated ways in which clinical text can be automati-cally processed for enabling access to relevant infor-mation for physicians and health researchers (Embiand Payne, 2009).
One application is to support pa-tient recruitment into clinical trials (research studiesthat try to answer scientific questions to find bet-ter ways to prevent, diagnose, or treat a disease)by matching patient characteristics against eligibil-ity criteria (Raghavan and Lai, 2010).
While therehas been significant efforts to move to structureddata collection, clinical narratives remain a criticaldata source for these tasks.Extracting structured information from unstruc-tured clinical text using natural language processing(NLP) is complicated by the distinct clinical report-ing sub-language characterized by incomplete sen-tences and domain specific abbreviations (Friedmanet al, 2002).
The large number of clinical narra-tives generated per patient, over the years, alongwith redundant information within and across narra-tives, further adds to the complexity of using infor-mation structured using NLP.
There is a tendency tocopy and edit parts of an old clinical narrative when-ever a new one is created, thus leading to redundantinformation in clinical narratives of a patient.
Fur-thermore, since different types of clinical narrativesare created for different purposes, certain narrativesmay summarize information from various other, attimes older, clinical narratives.
All of this makes thetask of automatically processing unstructured clin-ical narratives significantly difficult.
However, theability to resolve medical concept coreferences helpsdeal with redundant information within and acrossclinical narratives and thus produce a unique list ofmedical concepts in the patient?s clinical history.We investigate the task of resolving references to731the same medical concept in the clinical narrativesof a patient using supervised and semi-supervisedmethods.
Our main contributions are as follows:1.
Since manual coreference annotation of patientnarratives is a slow and expensive process and pub-licly available datasets are difficult to acquire, westudy the application of semi-supervised methods,co-training and using expectation constraints withposterior regularization, to medical concept coref-erence resolution (MCCR).2.
We work with the hypothesis that if two medicalconcepts have the same meaning and have occurredat the same time, there is a very high probability thatthey corefer.
Based on this hypothesis, we explainextraction of semantic and temporal feature sets thatare effectively used for MCCR.3.
We propose a method to associate medical con-cepts with time durations centered around admissionand discharge dates of the patient using CRFs.4.
With the help of corpora created from the NewEngland Journal of Medicine (NEJM) and actual pa-tient narratives obtained from the medical center, wedemonstrate that the semi-supervised methods per-form comparably with supervised learning for pair-wise MCCR using a MaxEnt classifier.2 Related WorkFree-text reports form a significant portion of theinformation content in a patient?s medical record.There is great need for tools that can structure theinformation in clinical text for use in various stud-ies studies such as clinical trials, quality assess-ment of healthcare delivery in institutions, and pub-lic health research.
Researchers have been investi-gating ways in which clinical free-text can be struc-tured to transform the information content in a clin-ical narrative into a representation suitable for com-putational analysis (Ananiadou et al, 2004).
Medi-cal NLP systems like Mayo?s cTakes (Savova et al,2010), IBM?s MedKAT,1 and MedLEE (Chiang etal., 2010), have components specifically trained ordesigned for the clinical domain, to support taskssuch as named entity recognition.
Previous at-tempts at learning temporal relations between med-ical events in clinical text include work by Jung et1https://cabig-kc.nci.nih.gov/Vocab/KC/index.php/OHNLPal.
(2011) and Zhou et al (2006).
Gaizauskas etal.
(2006) learn the temporal relations before, after,is included between events from a corpus of clinicaltext much like the event-event relation tlink learn-ing in Timebank (Pustejovsky et al, 2003).
A com-prehensive survey of temporal reasoning in medi-cal data is provided by Zhou and Hripcsak (2007).Chapman et al (2011) discuss barriers to NLP de-velopment in the clinical domain.Coreference resolution is a well-studied prob-lem in computational linguistics (Ng, 2010; Raghu-nathan et al, 2010).
Supervised machine learn-ing algorithms have been previously used for nounphrase coreference resolution with fairly good re-sults (Soon et al, 2001; Raghunathan et al, 2010).Recently, the i2b2 challenge2 on coreference reso-lution examined coreference resolution in clinicaldata.
The problem addressed in our paper is simi-lar to the task described in the i2b2 challenge.3 Be-sides the i2b2 challenge, there has not been signifi-cant work in MCCR.
This may be due to various pri-vacy concerns and the efforts required to anonymizeand annotate massive amounts of patient narratives.Zheng et al (2011) review heuristic-based, super-vised and unsupervised methods for coreference res-olution in the context of the clinical domain.
He(2007) studied coreference resolution in dischargesummaries, treating coreference resolution as a bi-nary classification problem and investigated criticalfeatures for coreference resolution for entities thatfall into five medical semantic categories commonlyappearing in discharge summaries.
However, we fo-cus on feature extraction to determine the similaritybetween medical concepts, both in terms of meaningand time of occurrence, for resolving coreferenceswithin and across all types of clinical narratives.A disadvantage of supervised machine learningapproaches is the need for an unknown amount ofannotated training data for optimal performance.Researchers then began to experiment with weaklysupervised machine learning algorithms such as co-training (Blum and Mitchell, 1998).
Muller et al(2002) investigate the practical applicability of co-training for the task of building a classifier for coref-erence resolution and observed that the results were2https://www.i2b2.org/NLP/Coreference/3https://www.i2b2.org/NLP/Coreference/assets/CoreferenceGuidelines.pdf732mostly negative for their dataset.Ganchev et al (2010) propose a posterior regular-ization framework for weakly supervised learning toderive a multi-view learning algorithm.
Multi-viewmethods typically begin by assuming that each viewalone can yield a good predictor.
Under this as-sumption, we can regularize the models from eachview by constraining the amount by which we per-mit them to disagree on unlabeled instances.
In theproposed approach, they train a model for each view,and use constraints that the models should agree onthe label distribution.We investigate the applicability of these two weaklysupervised methods to the task of MCCR using se-mantic and temporal views.
Savova et al (2011) dis-cuss the creation of a corpus for coreference resolu-tion in the clinical narrative.
We annotate a corpus ofclinical narratives to tag medical concepts, temporalrelations, and coreference information.
We use thiscorpus as a gold standard to evaluate the proposedapproach to resolving coreferences between medicalconcepts in clinical text.To summarize, we study the problem of intra andcross-narrative coreference resolution on longitudi-nal patient data using relatedness between medicalconcepts in terms of semantics and time.
Further,we importantly demonstrate that this task gives usreasonable results even when modeled as a semi-supervised problem.
Creating annotated clinical cor-pora is tedious, time consuming, and costly, as itrequires experts with medical domain knowledge.Thus, the ability to train semi-supervised modelswith limited labeled data for MCCR would be oftremendous value.3 Problem DescriptionCoreference resolution in clinical text refers to theproblem of identifying all medical concepts that re-fer to the same medical concept.
Medical con-cepts are medical entities, events or states associ-ated with the patient?s medical condition and health-care.
These include medical conditions, drugs ad-ministered, diseases, procedures and lab tests as wellas normal health situations like pregnancy affectingthe patient?s health.
The task of MCCR is similar tonoun phrase coreference resolution.
However, med-ical concepts are not restricted to noun phrases.
Forinstance, the actions cauterize and cauterization areboth considered medical concepts.To make the task of identifying medical conceptsfrom clinical text more deterministic, any contigu-ous group of words that have a direct or close matchin the Unified Medical Language System (UMLS)Metathesaurus4 is considered a medical concept.The UMLS includes a large Metathesaurus of con-cepts and terms from many biomedical vocabular-ies and a lexicon which contains syntactic, morpho-logical, and orthographic information for biomedi-cal and common words in the English language.Problem Formulation.
Consider a corpus of clini-cal narratives, where multiple clinical narratives areassociated with each patient.
If Pi, i ?
{1, 2, ..., n}where n is the number of patients in corpus, thenfor each Pi, we have a set of associated clinical nar-ratives.
Each clinical narrative in turn has a set ofmedical concepts.
Thus, each Pi has a set of associ-ated medical concepts, M = {M1,M2,M3, ..} thatoccur within each clinical narrative as well as acrossclinical narratives for that Pi.
We study the problemof MCCR of all medical concepts in M for each Pi.4 Semantic and Temporal FeaturesWe extract features based on semantic and tempo-ral relatedness for each pair of medical concepts.Semantic relatedness measures closeness betweenmedical concepts in terms of their meaning.
This isquantified by measuring distance between medicalevents in the UMLS Metathesaurus graph structure(Xiang et al, 2011).
Temporal relatedness measuresthe closeness between medical concepts in terms ofwhen they occurred.
This is achieved by first, learn-ing to assign every medical concept to a time-bin,and then using the time-bin as a feature for learn-ing to resolve coreferences.
Extracting semantic andtemporal features helps identify conditionally inde-pendent views of the data for co-training classifiers.As previously noted by Nigam and Ghani (2000), itis hard to identify conditionally independent viewsfor real-data problems.
However, we believe thereare no natural dependencies between the semanticand temporal feature sets.
While semantic featureshelp identify synonymous medical concepts, thatalone may not guarantee coreference.
Medical con-4https://uts.nlm.nih.gov/home.html733ClinicalTextSemantic FeatureExtractionTemporal FeatureExtraction usingCRFsCo-trainPosteriorRegularizationCoreferencedecisionsSection 4 Section 5Medical ConceptCoreference Resolution(MCCR)ORFigure 1: MCCR pipeline: Extract semantic and tempo-ral features from clinical text to train MaxEnt classifiersfor medical concept coreference resolution using 1) Co-training or 2) Posterior Regularizationcepts that are similar in meaning, but dissimilar interms of their time of occurrence, most probably donot corefer.
Similarly, medical concepts that occurduring the same time duration but are dissimilar interms of meaning, most probably do not corefer.Semantic Relatedness.
We leverage the UMLSto derive a semantic relatedness score between med-ical concepts.
The UMLS codifies concepts foundin various medical vocabularies (e.g., ICD5 andSNOMED-CT6) and includes relationships betweenvarious concepts.
The medical concepts and theirrelationships are modeled in a graph structure.
Weuse the k-Neighborhood decentralization method(kDLS) (Xiang et al, 2011) to index and transi-tively traverse associated relations between conceptunique identifiers (CUIs) in the UMLS graph.
TheUMLS uses semantic relations to mark the avail-able links between two concepts.
Around 2,404,937CUIs and 15,333,246 links between them are seen inthe full UMLS graph structure.
The kDLS methodis shown to outperform both breadth-first and depth-first search in terms of speed and various othermeasures in finding important information, such asreachability, distance, and a summary of paths, be-tween two concepts in the UMLS graph structure.The relation between two concepts Mj (denoted byx) and Mk (denoted by y) is measured as follows.R(x, y) =?p?D(x,y)1?length(p)?1+?q?D(y,x)1?length(q)?1where D(x, y) is the set of paths from x to y andD(y, x) is the set of paths from y to x obtained us-5http://www.cdc.gov/nchs/icd.htm6http://www.ihtsdo.org/snomed-ct/ing the kDLS method, excluding paths with lengthequal to 1.
In order to make the measurement be-tween a medical concepts unbiased against the avail-able links in the UMLS that directly connect them,the paths with length being 1 between them are notcounted.
Each path?s contribution to the relationscore R(x, y) is determined by its length and ?.
?
isvaried between 1 to 50; if ?
is set to 1, then all pathscontribute equally to R irrespective of their lengths.When ?
increases, more weight will be placed onthe short paths as opposed to the long paths.
Xianget al (2011) observe several fold enrichment valueswhen ?
is varied between 5 and 15.Besides traversing the UMLS graph structure us-ing the kDLS method to obtain a similarity scorebetween medical concepts, we also measure similar-ity between medical concepts by taking into accountthe surrounding context.
We do so by measuringthe KL-divergence between the sentences to whichthe medical concepts belong.
In order to avoid thepossibility of an empty set when calculating the in-tersection of the probability distributions, we use asmoothing method that makes the probability distri-butions sum to 1 (Brigitte, 2003).Another important semantic feature is the type ofrelation between the medical concepts.
This featureis calculated by first computing the stemmed wordoverlap between the medical concepts and derivingfeatures based on exact and partial matches betweenthe word stems of the medical concepts.
If there isno exact or partial match between the concepts, wequery the UMLS to check if the stem of one of themedical concepts occurs in the UMLS definition oratoms of the other medical event.
An atom is thesmallest unit of naming within the UMLS.
A med-ical concept in UMLS represents a single meaningand contains all atoms in the UMLS that express thatmeaning in any way, whether formal or casual, ver-bose or abbreviated.
All of the atoms within a con-cept are synonymous.Besides the described features, we also includethe UMLS semantic category of each medical con-cept and the WordNet7 similarity score between sen-tences containing the medical concept.Temporal Relatedness.
Clinical text is fre-quently characterized by temporal expressions co-7http://wordnet.princeton.edu/734occurring with medical concepts (Zhou and Hripc-sak, 2007).
For instance, two days ago, fever started4 days before rash, July 10th, 2010 etc.
The abil-ity to associate medical concepts with temporal ex-pressions helps order medical concepts and deter-mine potential temporal overlap between them.
Thisin turn could be a powerful discriminatory featurein MCCR.
Consider the medical concept chest painthat occurs multiple times in a clinical narrative.
Ifthese mentions of chest pain have occurred at thesame time, there is a possibility that they all refer tothe same instance of the medical concept chest pain.Instead of relying on implicit temporal referencesthat may or may be evident from the clinical nar-rative, we focus on temporal expressions that arefound in most clinical narratives.
We do so by lever-aging structural properties of clinical narratives suchas section information and explicit temporal infor-mation such as admission and discharge dates, tolearn to assign medical concepts to time periods werefer to as time-bins.We now proceed to explain the process of assign-ing medical concepts to time-bins using CRFs.
Clin-ical narratives are usually formatted with a struc-tured header with information that includes the pa-tient admission and discharge date.
Clinical narra-tives are also typically divided into sections.
Sec-tions represent a logical, and at times, temporalgrouping of information in the narrative.
Sectionssuch as ?history of present illness,?
?physical ex-amination,?
?review of systems,?
?impression,?
and?assessment plan?
tend to occur in a certain orderwithin each clinical narrative.
Thus, section tran-sitions may indicate a temporal pattern for medicalconcepts across those sections.
For example, ?pastmedical history?
(before admission), followed by?findings on admission?
(on admission), followedby ?physical examination?
(after admission).
Sec-tions of certain types may also exhibit certain tem-poral patterns.
A ?history of present illness?
sec-tion may start with diseases and diagnoses 30 yearsago and then proceed to talk about them in the con-text of a medical condition that happened few yearsago and finally describe the patient?s condition onadmission.
Given the temporal patterns within sec-tions and at section transitions, it works well to treatthe list of medical concepts from each clinical nar-rative as a sequence (considering them in narrativeorder) and learning to label them with a correspond-ing time-bin.
We define the following sequence oftime-bins centered around admission and discharge,{way before admission, before admission, on admis-sion, after admission, after discharge}.We model the problem of assigning medical con-cepts to time-bins as a sequence labeling task usinga CRF where we predict labels from the set {way be-fore admission, before admission, on admission, af-ter admission, after discharge} as a sequence Y pre-dicted from the detected medical concepts X .
CRFsuse two types of features in classification, state fea-tures and transition features.
State features con-sider relating the label y (time-bin) of a single ver-tex (medical concept) to features corresponding to amedical concept x, and are given by,S(x, y, i) =?j ?jsj(y, x, i)Transition features consider the mutual depen-dence of labels yi?1 and yi (dependence between thetime-bins of the current and previous medical eventin the sequence) and are given by,T (x, y, i) =?k ?ktk(yi?1, yi, x, i)Above, sj is a state feature function, and ?j is itsassociated weight and tk is a transition function, and?k is its associated weight.
In contrast to the statefunction, the transition function takes as input thecurrent label as well as the previous label, in additionto the data.Example state features include indicator featuresbased on verbs patterns in the same sentence as thatof the medical concept, last verb before the medicalconcept, and type of clinical narrative.
We also in-clude position of medical event in the narrative aswell as within each section, the temporal expres-sions and dates co-occurring with the medical con-cept as features and the difference between thesedates and the admission date on each clinical nar-rative.
Example transition features include sectiontransitions based on the sections under which themedical concept occurs, UMLS relatedness scorebetween the previous and current medical concept,difference in verb patterns between the previous andcurrent medical concept, difference in dates (if any)between the dates co-occurring with the previousand current medical concept.In order to enable feature extraction for this learn-ing task, we use the following heuristic-based al-735gorithm to automatically identify sections and asso-ciate medical concepts with them.1.
Extract lines that are all upper-case, and longerthan a word, from all narratives in corpus.
Theymostly correspond to section titles.2.
Derive the stem of each word in the title using aPorter stemming algorithm8 and sort stemmedtitles by frequency.
If two or more words inthe title overlap, they are considered the same.This gives us a candidate set of section titles.3.
When parsing a clinical narrative, and encoun-tering a stemmed ngram matching a section ti-tle from the frequent list, all subsequent sen-tences are associated with that section until anew section title is encountered.
If an exactmatch is not found, we allow partially match-ing ngrams to be considered as section titles.Along with the time-bin that are learned using theprocess described above, dates and temporal expres-sions extracted from the annotations in our corpusare also used as temporal features.
The list of fea-tures extracted for the task of MCCR include thefollowing:1.
Verb pattern in the sentence in which the med-ical concept occurs.2.
Last verb before the medical concept in thesame sentence.3.
Type of clinical narrative.4.
Section under which the medical concept ismentioned.5.
Position of the medical concept.6.
Dates that fall in the same sentence as the med-ical concept.7.
Difference between admission date and the datein the same sentence as the clinical narrative.8.
The learned time-bin of each medical concept.We also derive features based on the overlap-ping in time-bins for the medical concept pairand the nature of time-bin (past, present, fu-ture).9.
Difference in verb patterns in the sentences ofthe medical concept pair.10.
Difference in dates between the medical con-cept pair.8http://tartarus.org/martin/PorterStemmer/11.
UMLS relatedness score between the medicalconcept pair and all the UMLS related andother features described previously in the se-mantic relatedness section.When applying CRFs to the problem of assigningmedical concepts to time-bins, an observation se-quence is medical concepts in the order in whichthey appear in a clinical narrative, and the state se-quence is the corresponding label sequence of timebins.
Thus, given a sequence of concepts in narrativeorder {M1,M2,M3, ..}, we learn a correspondinglabel sequence of time-bins {way before admission,before admission, on admission, after admission, af-ter discharge}.
The learned label sequence is nowused as part of the temporal feature set in co-trainingand posterior regularization for MCCR.5 Weakly Supervised Learning5.1 Co-trainingWe co-train two MaxEnt classifiers, one each on thesemantic features fs and temporal features ft of thedata, to classify pairs of medical concepts as core-fer or no-corefer in a semi-supervised fashion.
Weuse the co-training algorithm proposed by Blum andMitchell (1998).The assumption here is that each feature set containssufficient information to train a model for classifica-tion of medical concepts.
Consider the concept pair,{renal inflammation, posterior uveitis} that core-fer.
The semantic view for this concept pair maynot strongly indicate coreference.
The ?UMLS rela-tion type?
feature indicates that the two concepts arenot similar in meaning.
However, both concepts aremapped to the same time-bin after admission.
Thus,the time-bin along with features extracted based onexplicit temporal expressions co-occurring with themedical concepts indicate a coreference between thepair of medical concepts.
Similarly, the semanticview is confident about confident about the corefer-ence of certain medical concept pairs which do notoccur in the same time-bin.
The classifiers trainedon each view complement each other in the learn-ing process.
Thus, we can leverage the predictionsmade by each classifier on the unlabeled dataset toaugment the training data of both classifiers.The co-training algorithm is shown in Table 1.
Weset a threshold for an unlabeled sample to be added736Function coTrainRepeat till all unlabeled data is labeled.1.
Train classifier c1 on tf s to obtain model m12.
Train classifier c2 on tf t to obtain model m23.
Use m1 to classify a subset of unlabeled dataand update the training data as,tf s.subset = {usubset1, predicted label}iff classifier confidence > 1/number of labels4.
Use m2 to classify a subset of unlabeled dataand update the training data as,tf t.subset = {usubset2, predicted label}iff classifier confidence > 1/number of labels5.
tf s = tf s + tf t.subset +{usubset1, predicted label}6. tf t = tf t + tf s.subset +{usubset2, predicted label}Table 1: Co-training algorithm for the binary pairwiseclassification task of MCCR (Blum and Mitchell, 1998).c = classifier, u = unlabeled data.usubset1, usubset2 = subsets of unlabeled data.usubset1 and usubset2 are mutually exclusive.F = {fs, ft} is the features space divided into condition-ally independent semantic and temporal feature sets.tf s = {fs,l} training data consisting of semantic featuresof a medical concept pair along with class label.tf t = {ft,l} training data consisting of temporal featuresof a medical concept pair along with class label.into the labeled pool.
An unlabeled sample is la-beled in a particular iteration, if classifier confidence> 1/number of labels.
In the next iteration, ran-domly pick a subset of unlabeled samples and labelall samples in this subset.
This could include sam-ples that have already been labeled in previous iter-ations.
A label is assigned in a subsequent iterationif: the sample was previously labeled OR if classi-fier confidence > threshold.
The parameters in thisalgorithm are the number of iterations, the pool sizeof examples selected from the unlabeled set in eachiteration and the number of labeled examples addedat each iteration to the labeled data pool.
Similar toBlum and Mitchell (1998), we update the pool sizeby 2p+ 2n in each iteration, where p is the numberof medical pairs that corefer and n is the number ofmedical concept pairs that do not corefer.5.2 MaxEnt with Posterior RegularizationThe next semi-supervised learning method appliedto MCCR is MaxEnt with posterior regularizationusing expectation constraints (Ganchev et al, 2010).This method incorporates prior knowledge directlyon the output variables during learning.
The priorknowledge is expressed as inequalities on the ex-pected value under the posterior distribution of user-defined constraint features.
Thus, posterior regular-ization incorporates side-information into unsuper-vised estimation in the form of constraints on themodel?s posteriors.
It is similar to the EM algorithmduring learning, but it solves a problem similar toMaximum Entropy inside the E-Step to enforce theconstraints.Posterior regularization is used to derive a multi-view learning algorithm while specifying constraintsthat the models should agree on the label distri-bution.
We train MaxEnt models based on twoviews of the data, semantic and temporal.
Thismethod starts by considering the setting of completeagreement where there is a common desired out-put for the two models and each of the two viewsis sufficiently rich to predict labels accurately.
Thesearch is restricted to model pairs p1, p2 that sat-isfy p1(y|x) ?
p2(y|x), where p1 and p2 each de-fine a distribution over labels.
The product dis-tribution p1(y1)p2(y2) is considered and constraintfeatures are defined such that the proposal distri-bution q(y1, y2) will have the same marginal fory1 and y2.
There is one constraint feature definedfor each label y given by, ?y(y1, y2) = ?
(y1 =y)?
(y2 = y), where ?(.)
is the 0-1 indicator func-tion.
The constraint set Q = q : Eq[?]
= 0 re-quires that the marginals over the two output vari-ables are identical q(y1) = q(y2).
An agreementbetween two models is defined as agree(p1, p2) =argmin KL(q(y1, y2)||p1(y1)p2(y2)) | Eq [?]
= 0.In the semantic feature set, we convert the follow-ing feature (described in Section 4) into expectationconstraints.
The type of relation between the pairof medical concepts, is derived from matching theword stems and querying the UMLS definition andatoms of the medical concepts.
Based on the relationbetween the medical concepts (i.e., partial match,complete match, UMLS definition match, UMLSatom match, and no match), we indicate the prob-ability of label distribution coref and no-coref.
Ifthe relation turns out to be no match, there is a highprobability that the medical concepts do not corefer.In the temporal feature set, we convert the featuresbased on time-bins of the medical concepts in thepair into expectation constraints.737Class(time-bin) Precision Recallafter discharge 96.05 62.53before admission 94.02 92.44on admission 33.25 75.16way before admission 50.42 66.72after admission 93.62 99.14Table 2: Sequence tagging of medical concepts withtime-bins using CRFs.6 Experimental Setup6.1 Corpus AnnotationAnnotation of clinical text is a time consuming andcostly process.
Many annotation efforts have usedphysicians to annotate the data.
Instead, we use an-notators that are students or recently graduated stu-dents from diverse clinical backgrounds with vary-ing levels of clinical experience.
In spite of this di-versity, the annotation agreement across our team ofannotators is high; all annotators agreed on 89.5% ofthe events and our overall inter-annotator Cohen?skappa statistic (Conger, 1980) for medical eventswas 0.865.
The annotators mark medical concepts,coereference chains and temporal expressions in theclinical narratives and the NEJM case reports.
Theyalso map each medical concept to a UMLS CUI.6.2 Feature ExtractionThe first step involves extraction of semantic andtemporal features for the annotated medical con-cepts, as described in Section 4 from both corpora.The semantic relatedness scores are computed us-ing the kDLS (Xiang et al, 2011) method to calcu-late the relationship between concepts in the UMLSwith value of ?
set to 7.
The type of relation be-tween medical concepts is derived by matching wordstems in each medical concept using the Lucene9implementation of the Porter stemming algorithm.We query the latest release (UMLS 2011AB) of theUMLS Metathesaurus for finding a match betweenmedical concept and the UMLS definition or UMLSatoms.
The WordNet similarity score is computedusing Java API for WordNet Searching (JAWS).10Explicit temporal expressions annotated in thecorpora are included in our temporal feature set.Medical concepts in the NEJM are mostly de-scribed temporally relative to the patient?s admis-9http://lucene.apache.org/10http://lyle.smu.edu/?tspell/jaws/Class NEJM Clinical NarrativesPrecision Recall Precision Recallcoref 79.24 94.53 74.81 88.33no-coref 86.71 90.62 83.92 94.86Table 3: Supervised learning for MCCR.sion.
Temporal expressions like ?2 years before ad-mission?
and ?3 weeks before admission?
are com-mon.
Hence, we use a heuristic-based algorithmto associate medical concepts with explicit tempo-ral expressions in the NEJM corpus.
The algo-rithm parses case reports and identifies the tempo-ral expressions anchored to admission.
All medi-cal concepts following such a temporal expressionare anchored to it until a new temporal expressionis encountered.
Over 88% of the medical concept-temporal expression associations done with the al-gorithm above is accurate when compared againstthe NEJM gold standard.As described in Section 4, we apply sequence tag-ging using a CRF to assign medical concepts in clin-ical narratives to time-bins.
We use the implementa-tion of CRF in Mallet,11 trained by Limited-MemoryBFGS for our experiments.
We use the StanfordPOS tagger12 to identify verbs and derive verb pat-terns.
The dataset for the task of assigning medi-cal concepts to time-bins consisted of 1613 medicalconcepts.
We used a 60-40 train-test split to train aCRF using a sequence of medical concepts and ob-served an overall accuracy of 92%.
The precisionand recall values for each time-bin class is indicatedin Table 2.
The percentage of medical concepts thatfall under ?way before admission?
and ?on admis-sion?
are less than 5%, affecting the learning accu-racy of those classes.
When modeled as a multi-class classification task using MaxEnt, we achievearound 86% accuracy.7 MCCR Results and DiscussionWe perform the following experiments for pairwiseMCCR: 1) Supervised learning with a MaxEnt clas-sifier, using the combined semantic and temporalfeature set, 2) Co-training two MaxEnt models, 3)Training MaxEnt models with using posterior regu-larization.11http://mallet.cs.umass.edu/12http://nlp.stanford.edu/software/tagger.shtml738Class NEJM Clinical NarrativesCo-train Precision Recall Precision Recallcoref 70.32 82.54 69.26 87.31no-coref 82.54 84.85 71.15 89.44PR Precision Recall Precision Recallcoref 76.63 90.41 74.81 84.25no-coref 80.35 89.21 78.93 87.46Table 4: Co-training and posterior regularization (PR) forMCCR using semantic and temporal feature sets.We use the MaxEnt classifier available in Malletfor 1) and 2) and the the Mallet implementation ofMaxEnt models with posterior regularization for 3).The NEJM corpus has 722 medical concepts,12576 candidate pairs of medical concepts includ-ing 137 pairs that corefer.
We include all 12576pairs in our experiments.
The clinical narrative cor-pus has 1613 medical concepts.
The candidate pairsand coreference chains for each patient is as follows.Patient 1 has 241001 candidate pairs, 29 corefer-ence chains.
Patient 2 has 149604 candidate pairs,9 coreference chains.
Patient 3 has 6,446,521 can-didate pairs, 20 coreference chains.
From all thecandidate pairs in the clinical narrative corpus, 1025pairs corefer.
We randomly sample the no-coref in-stances to restrict the corpus size to 1 million candi-date pairs of medical concepts.The results for all 3 experiments for both corporais shown in Tables 3, 4.
We also train-test a super-vised MaxEnt classifier on a 60-40 split of the en-tire corpus.
This gives us a precision of 74.81% and88.33% recall (coref) for the binary classificationtask of pairwise MCCR in the clinical narratives cor-pus.
In the both the semi-supervised experiments,we use an initial labeled pool size of 30 where 12medical concept pairs that corefer (p) and 18 that donot corefer (n).
The growth size is each iteration ofco-training is 2p+2n.
At each iteration, confidentlylabeled examples are added to the training set fromthe previous iteration.
The co-training algorithmis run until all unlabeled instances become labeled.The parameters in the posterior regularization im-plementation include the regularization penalty foreach step and the number of iterations.
We use thedefault values (maxIterations=100, pGaussianPrior-Variance=0.1, qGaussianPriorVariance=1000) sug-gested on the Mallet toolkit page (Bellare et al,2009).
Co-training two MaxEnt models based onindependent semantic and temporal views of thedata results in 69.26% precision and 87.31% recall(coref), whereas training MaxEnt models with ex-pectation constraints gives us 74.81% precision and84.25% recall (coref), on the corpus of clinical nar-ratives.Posterior regularization does better than co-trainingand the performance of both the semi-supervisedmethods is comparable to if not as good as the super-vised classifier trained on a 60-40 split of the corpus.Thus, our results indicate that the use of semanticand temporal features is effective for MCCR in clin-ical text.
It is clear from the co-training and poste-rior regularization results that treating MCCR as asemi-supervised problem works.8 ConclusionsWe investigated the task of MCCR in clinical text us-ing supervised and semi-supervised learning meth-ods.
We create annotated corpora of clinical textwith case reports from the NEJM and narratives ob-tained from The Ohio State University Wexner Med-ical Center.
We work with the hypothesis that de-termining semantic and temporal similarity betweenmedical concepts helps resolve coreferences.
Inorder to test this hypothesis, we describe the pro-cess of semantic and temporal feature extractionfrom clinical text.
We demonstrate the effective-ness of the extracted features in a supervised binaryclassification task for MCCR with MaxEnt classi-fiers (using the combined feature set) as well as us-ing semi-supervised methods of co-training MaxEntclassifiers and training MaxEnt models using pos-terior regularization (using two independent viewsof the data - semantic view and temporal view).Thus, we show that MCRR can be performed usingsemi-supervised learning with semantic and tempo-ral views of the data.AcknowledgmentsThe project described was supported by theNational Center for Research Resources,Grant UL1RR025755, KL2RR025754, andTL1RR025753, and is now at the NationalCenter for Advancing Translational Sciences,Grant 8KL2TR000112-05, 8UL1TR000090-05,8TL1TR000091-05.
The content is solely the re-sponsibility of the authors and does not necessarilyrepresent the official views of the NIH.739ReferencesSophia Ananiadou, Carol Freidman, and Jun?
?chi Tsu-jii.
2004.
Introduction: named entity recognition inbiomedicine.
J. of Biomedical Informatics, pages 393?395.Kedar Bellare, Gregory Druck, and Andrew McCallum.2009.
Alternating projections for learning with expec-tation constraints.
In Proceedings of the Twenty-FifthConference on Uncertainty in Artificial Intelligence,UAI ?09, pages 43?50.Avrim Blum and Tom M. Mitchell.
1998.
Combin-ing labeled and unlabeled data with co-training.
InCOLT?98, pages 92?100.Bigi Brigitte.
2003.
Using Kullback-Leibler distance fortext categorization.
In Proceedings of the 25th Euro-pean conference on IR research, ECIR?03, pages 305?319.Wendy W Chapman, Prakash M Nadkarni, LynetteHirschman, Guergana K Savova Leonard W D?Avolio,and Ozlem Uzuner.
2011.
Overcoming barriers toNLP for clinical text: the role of shared tasks and theneed for additional creative solutions.
In JAMIA.Jung-Hsien Chiang, Jou-Wei Lin, and Chen-Wei Yang.2010.
Automated evaluation of electronic dischargenotes to assess quality of care for cardiovascular dis-eases using Medical Language Extraction and Encod-ing System (MedLEE).
JAMIA, pages 245?252.A.J.
Conger.
1980.
Integration and generalization ofkappas for multiple raters.
In Psychological BulletinVol 88(2), pages 322?328.Peter J Embi and Philip Payne.
2009.
Clinical researchinformatics: challenges, opportunities and definitionfor an emerging domain.
Journal of the AmericanMedical Informatics Association, 16(3):316?327.Carol Friedman, Pauline Kra, and Andrey Rzhetsky.2002.
Two biomedical sublanguages: a descriptionbased on the theories of Zellig Harris.
Journal ofBiomedical Informatics, 35(4):222?235.Rob Gaizauskas, Henk Harkema, Mark Hepple, and An-drea Setzer.
2006.
Task-oriented extraction of tem-poral information: The case of clinical narratives.In Proceedings of the Thirteenth International Sym-posium on Temporal Representation and Reasoning,TIME ?06, pages 188?195.Kuzman Ganchev, Joo Graa, Jennifer Gillenwater, andBen Taskar.
2010.
Posterior regularization for struc-tured latent variable models.
Journal of MachineLearning Research, pages 2001?2049.Tian Ye He.
2007.
Coreference Resolution on Entitiesand Events for Hospital Discharge Summaries.
EECS,Cambridge, MA, MIT.
M.Eng.Hyuckchul Jung, James Allen, Nate Blaylock, Willde Beaumont, Lucian Galescu, and Mary Swift.
2011.Building timelines from narrative clinical records: ini-tial results based-on deep natural language under-standing.
In Proceedings of BioNLP 2011 Workshop,BioNLP ?11, pages 146?154.Christoph Muller, Stefan Rapp, and Michael Strube.2002.
Applying co-training to reference resolution.
InACL, pages 352?359.Vincent Ng.
2010.
Supervised noun phrase coreferenceresearch: The first fifteen years.
In Proceedings of theACL, pages 1396?1411.Kamal Nigam and Rayid Ghani.
2000.
Analyzingthe effectiveness and applicability of co-training.
InCIKM?00, pages 86?93.James Pustejovsky, Jos M. Castao, Robert Ingria, RoserSauri, Robert J. Gaizauskas, Andrea Setzer, GrahamKatz, and Dragomir R. Radev.
2003.
Timeml: Robustspecification of event and temporal expressions in text.In New Directions in Question Answering?03, pages28?34.Preethi Raghavan and Albert M. Lai.
2010.
Leveragingnatural language processing of clinical narratives forphenotype modeling.
In PIKM?10, pages 57?66.Karthik Raghunathan, Heeyoung Lee, Sudarshan Ran-garajan, Nathanael Chambers, Mihai Surdeanu, DanJurafsky, and Christopher Manning.
2010.
A multi-pass sieve for coreference resolution.
In Proceedingsof the 2010 Conference on Empirical Methods in Nat-ural Language Processing, EMNLP ?10, pages 492?501, Stroudsburg, PA, USA.
Association for Compu-tational Linguistics.Guergana K. Savova, James J. Masanz, Philip V.Ogren, Jiaping Zheng, Sunghwan Sohn, Karin Kip-per Schuler, and Christopher G. Chute.
2010.
Mayoclinical text analysis and knowledge extraction sys-tem (cTAKES): architecture, component evaluationand applications.
JAMIA, pages 507?513.Guergana K. Savova, Wendy Webber Chapman, JiapingZheng, and Rebecca S. Crowley.
2011.
Anaphoricrelations in the clinical narrative: corpus creation.JAMIA, 18(4):459?465.Wee Meng Soon, Hwee Tou Ng, and Chung Yong Lim.2001.
A machine learning approach to coreferenceresolution of noun phrases.
Computational Linguis-tics, pages 521?544.Yang Xiang, Kewei Lu, Stephen L James, Tara B Bor-lawsky, Kun Huang, and Philip R O Payne.
2011. k-neighborhood decentralization: A comprehensive so-lution to index the UMLS for scale knowledge discov-ery.
In Journal of Biomedical Informatics.Jiaping Zheng, Wendy Webber Chapman, Rebecca S.Crowley, and Guergana K. Savova.
2011.
Coreferenceresolution: A review of general methodologies and ap-plications in the clinical domain.
Journal of Biomedi-cal Informatics, 44(6):1113?1122.740Li Zhou and George Hripcsak.
2007.
Temporal rea-soning with medical data - a review with emphasison medical natural language processing.
Journal ofBiomedical Informatics, pages 183?202.Li Zhou, Genevieve B. Melton, Simon Parsons, andGeorge Hripcsak.
2006.
A temporal constraint struc-ture for extracting temporal information from clinicalnarrative.
Journal of Biomedical Informatics, pages424?439.741
