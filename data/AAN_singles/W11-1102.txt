Proceedings of the TextGraphs-6 Workshop, pages 10?14,Portland, Oregon, USA, 19-24 June 2011. c?2011 Association for Computational LinguisticsNonparametric Bayesian Word Sense InductionXuchen Yao1 and Benjamin Van Durme1,21Department of Computer Science2Human Language Technology Center of ExcellenceJohns Hopkins UniversityAbstractWe propose the use of a nonparametric Bayesianmodel, the Hierarchical Dirichlet Process (HDP),for the task of Word Sense Induction.
Results areshown through comparison against Latent Dirich-let Allocation (LDA), a parametric Bayesian modelemployed by Brody and Lapata (2009) for this task.We find that the two models achieve similar levelsof induction quality, while the HDP confers the ad-vantage of automatically inducing a variable num-ber of senses per word, as compared to manuallyfixing the number of senses a priori, as in LDA.This flexibility allows for the model to adapt toterms with greater or lesser polysemy, when ev-idenced by corpus distributional statistics.
Whentrained on out-of-domain data, experimental resultsconfirm the model?s ability to make use of a re-stricted set of topically coherent induced senses,when then applied in a restricted domain.1 IntroductionWord Sense Induction (WSI) is the task of automat-ically discovering latent senses for each word type,across a collection of that word?s tokens situated incontext.
WSI differs from Word Sense Disambigua-tion (WSD) in that the task does not assume accessto some prespecified sense inventory.
This amountsto a clustering task: instances of a word are parti-tioned into the same bin based on whether a sys-tem deems them to have the same underlying mean-ing.
A large body of related work can be foundin (Schu?tze, 1998; Pantel and Lin, 2002; Dorowand Widdows, 2003; Purandare and Pedersen, 2004;Bordag, 2006; Niu et al, 2007; Pedersen, 2007;Brody and Lapata, 2009; Li et al, 2010; Klapaftisand Manandhar, 2010).Brody and Lapata (2009) (B&L herein) showedthat the parametric Bayesian model, Latent Dirich-let Allocation (LDA), could be successfully em-ployed for this task, as compared to previous re-sults published for the WSI component of SemEval-20071 (Agirre and Soroa, 2007).
A deficiency of theLDA model for WSI is that the number of sensesneeds to be manually specified a priori, either sepa-rately for each word type, or (as done by B&L) somefixed value that is shared globally across all types.Nonparametric methods instead have the flexibil-ity of automatically deciding the number of sensecluters (Vlachos et al, 2009; Reisinger and Mooney,2010).
In this work we first independently verifythe results of B&L, and then tackle the limitationon fixing the number of senses through the use ofthe Hierarchical Dirichlet Process (HDP) (Teh et al,2006), a nonparametric Bayesian model.
We showthis approach leads to results of similar quality asLDA, when using a bag-of-words context model, inaddition to allowing for variability in the number ofsenses across different words and domains.
Whentrained on a restricted domain corpus for whichmanually labeled sense data was present, we verifythat the model may be tuned to posit a similar num-ber of senses as determined by human judges.
Whentrained on a broader domain collection, we show thatthe number of induced senses increase, in line withthe intuition that a wider set of genres should leadto a greater diversity in underlying meanings.
Auto-matically inducing the proper number of senses hasgreat practical implications, especially in areas thatrequire word sense disambiguation.
For instance, in-ducing more senses for bank helps to tell differ-1Klapaftis and Manandhar (2010) and Brody and Lapata(2009) reported the best scores so far on this dataset.10ent word senses apart for naturally more ambigu-ous words, and inducing less senses for job helpsto prevent assigning too fined-grained senses in casethe same words in two similar contexts are mistak-enly regarded as carrying different senses.2 Bayesian Word Sense Inductionwm,nsm ,n??m????k??
n?[1,Nm]m?
[1,M ]k?
[1,K ]Figure 1: Latent Dirichlet Allocation (LDA) for WSI.As in prior work including B&L, we rely onthe intuition that the senses of words are hinted atby their contextual information (Yarowsky, 1992).From the perspective of a generative process, neigh-boring words of a target are generated by the target?sunderlying sense.2Both LDA and HDP define graphical models thatgenerate collections of discrete data.
The sense ofa target word is first drawn from a distribution andthen the context of this word is generated accordingto that distribution.
But while LDA assumes a fixed,finite set of distributions, the HDP draws from aninfinite set of distributions generated by a DirichletProcess.
This section details the distinction.Figure 1 shows the LDA model for word senseinduction.
The conventional notion of document isreplaced by a pseudo-document, consisting of everyword in an Nm-word window centered on the targetitem.
wm,n is the n-th token of the m-th pseudo-document for target word w. sm,n is the correspond-ing sense for wm,n.
Suppose there are K senses forthe target word w, then the distribution over a con-text word wm,n is:2For instance, given the word bank with a sense riverbank, it is more likely that the neighboring words are river,lake and water than finance, money and loan.wm,nsm ,nk m?
1n??,?
Nm?m?
?,?
M?k 1K?Figure 2: Hierarchical Dirichlet Process (HDP) for WSI.p(wm,n) =K?k=1p(wm,n | sm,n = k)p(sm,n = k).Let the word distribution given a sense bep(wm,n | sm,n = k) = ~?k, which is a vector oflength V (vocabulary size) that is generated froma Dirichlet distribution: ~?k ?
Dir(~?).
Let thesense distribution given a document be p(sm,n | d =m) = ~?m, which is a vector of length K that is gen-erated from a Dirichlet distribution: ~?m ?
Dir(~?
).The generative story for the data under LDA is then:For k ?
(1, ...,K) senses:Sample mixture component: ~?k ?
Dir(~?
).For m ?
(1, ...,M) pseudo-documents:Sample topic components ~?m ?
Dir(~?
).For n ?
(1, ..., Nm) words in pseudo-document m:Sample sense index sm,n ?Mult(~?m).Sample word wm,n ?Mult(~?sm,n).The sense distribution over a word is capturedas K mixture components.
In the HDP however,we assume the number of active components is un-known, and should be inferred from the data.
Foreach pseudo-document, the sense component sm,nfor word wm,n has a nonparametric prior Gm.
Gmis nonparametric in the sense that for every newpseudo-document m, a new Gm is sampled from abase distribution G0.
As the corpus grows, there are11more and more Gm?s.
However, the mixture com-ponent sm,n, drawn from Gm, can be shared amongpseudo-documents.
Thus the number of senses donot simply multiply out as m grows.
Both G0 andGm?s are distributed according to a Dirichlet Process(DP) (Ferguson, 1973).
The generative story is:Select base distribution G0 ?
DP (?,H) whichprovides an unlimited inventory of senses.For m ?
(1, ...,M) pseudo-documents:Draw Gm ?
DP (?0, G0).For n ?
(1, ..., Nm) words in pseudo-document m:Sample sm,n ?
Gm.Sample wm,n ?Mult(~?sm,n).Hyperparameters ?
and ?0 are the concentrationparameters of the DP, controlling the variability ofthe distributionsG0 andGm.
In a Chinese restaurantfranchise metaphor of the HDP, multiple restaurants(documents) share a set of dishes (senses).
Then?
controls the variability of the global sense distri-bution and ?0 controls the variability of each cus-tomer?s (word) choice of dishes (senses).33 Experiment SettingModel B&L experimented with variations to theLDA model that allowed for generating multiple lay-ers of features, such as smaller (5w) and larger (10w)bag-of-word contexts, and syntactic features.
Theadditional complexity beyond the standard modelled to only tenuous performance gains.
NormalLDA, when trained on pseudo-documents built from10 words of surrounding context, performed onlyslightly below their best reported results.4 Espe-cially as our goal here was to investigate the sense-specification problem, rather than eking out furtherimprovements in the base WSI evaluation measure,we chose to compare a standard LDA model to HDP,both strictly using a 10 word context.5Test Data Following B&L, we perform WSI onnouns.
The evaluation data comes from the WSItask of SemEval-2007 (Agirre and Soroa, 2007).
Itis derived from the Wall Street Journal portion of3Gibbs sampling (Geman and Geman, 1990) can be appliedfor inference.
Specifically, Teh et al (2006) describes the pos-terior sampling in the Chinese restaurant franchise.4F-score of 86.9% (10w), as compared to 87.3% (10w+5w).5We relied on implementations of LDA and HDP respec-tively from MALLET (McCallum, 2002), and Wang (2010).the Penn TreeBank (Marcus et al, 1994) and con-tains 15,852 instances of excerpts on 35 nouns.
Allthe nouns are hand-annotated with their OntoNotessenses (Hovy et al, 2006), with an average of 3.9senses per word.Evaluation Method WSI is an unsupervised taskthat results in sense clusters with no explicit map-ping to manually annotated sense data.
To derivesuch a mapping, we follow the supervised evalua-tion strategy of Agirre and Soroa (2007).
Anno-tated senses from SemEval-2007 are partitioned intoa standard mapping set (72%), a dev set (14%) and atest set (14%).
After an WSI system has tagged theelements in the mapping set with their ?cluster IDs?,then a cluster to sense derivation is constructed bysimply assigning to each cluster the manual senselabel that has the highest in-cluster frequency.
Oncesuch a mapping has been established, then resultson the dev or test set are reported based on treatingcluster assignment as a WSD operation.Training Data As out-of-domain source, we ex-tracted 930K instances of the 35 nouns from theBritish National Corpus (BNC) (Clear, 1993).
Asin-domain source we extracted another 930K in-stances from WSJ in years 87/88/90/94.
All pseudo-documents use the ?10 contextual window.4 EvaluationWe trained the LDA and HDP models on the WSJand BNC datasets separately.
In their experimentswith LDA, B&L iteratively tried 3 up to 9 senses,and then reported the number that led to best re-sults in evaluation (4 senses for WSJ, 8 for BNC).We repeated this approach for LDA, with hyper-parameters ?
= 0.02 and ?
= 0.1.
For the HDPmodel, we tuned hyper-parameters on the SemEval-2007 dev set.6 See Table 1 for results, averaged over5 runs of LDA and 3 runs of HDP.We report several findings based on this experi-ment.
First, for the LDA models trained on WSJand BNC, our F1 measures are 0.8% lower thanreported by B&L.7 Second, based on our own ex-periment, the HDP model performance is slightlybetter than that of LDA when training with BNC.6Final parameters: H = 0.1, ?0 ?
Gamma(0.1, 0.028),?
?
Gamma(1, 0.1).7We consider this acceptable experimental deviation, giventhe minor variation in respective training data.12WSJ BNCLDA-4s* 86.9 LDA-8s* 84.6LDA-4s 86.1 LDA-8s 83.8HDP 86.7 HDP 85.74Table 1: F-measure when training with WSJ (in-domain) andBNC (out-of-domain).
Results with * are taken from B&L.
4or 8 senses were used per word.
4: statistically significantagainst LDA-8s by paired permutation test with p < 0.001.The standard baseline, always picking the most frequent senseobserved in training, scores 80.9.WSJ BNCTrain Test Train TestLDA 4.0 3.9 8.0 7.4HDP 5.8 3.9 9.4 4.6Table 2: The average number of senses the LDA and HDPmodels output when training with WSJ/BNC and testing onSemEval-2007, which has 3.9 senses per word on average.Third, the HDP model appears to better adapt to datain other domains.
When switching the training setfrom WSJ (in-domain) to BNC (out-of-domain), we,along with B&L, found a 2.3% drop with LDA mod-els.
However, with the HDP model, there is only a1% drop in F1.
Moreover, even trained on out-of-domain data, HDP can still better infer the numberof senses from the test data, which is illustrated next.Table 2 shows the number of senses induced fromeach dataset.
When training on WSJ and test onSemEval-2007, HDP induced the correct number ofsenses (3.9 on average) from test, while LDA didthis by assuming 4 senses from the training data.When there is a domain mismatch between train-ing (BNC) and test (SemEval-2007, which comesfrom the 1989 WSJ), the LDA model preferred farmore than the annotated number of senses (7.4 vs.3.9), largely due to the fact that it assumed 8 sensesduring training.
However, even though the HDPmodel induced more senses (9.4) when training onthe broader coverage BNC set, it still inferred amuch reduced average of 4.6 senses on test.The BNC, being a balanced corpus, covers morediverse genres than the WSJ: we would expect it tolead to a more inclusive model of word sense.
Fig-ure 3 illustrates this comparison through the differ-ence between sense numbers.
For the 35 human-annotated nouns, HDP induced the number of sensesmostly within an error of ?2, whereas LDA tendedto prefer 3 ?
6 more senses than recognized by an--4 -3 -2 -1 0 1 2 3 4 5 6024681012 HDPLDA# induced senses - # annotated sensesfrequenc yFigure 3: The difference between induced number of sensesand annotated senses.
The training set is BNC.
The test setis SemEval-2007, containing 35 nouns with 3.9 senses.
LDAinduced 7.4 senses and HDP induced 4.6 senses on average.WSJ BNCLDA-5.8s 86.0 LDA-9.4s 82.7LDA-3.9s 85.3 LDA-3.9s 81.4HDP-5.8s 86.7 HDP-9.4s 85.74Table 3: F1 measure when training LDA with three other set-tings: 5.8s, 9.4s and 3.9s.
4: statistically significant againstboth LDA-9.4s and LDA-3.9s (for BNC) by paired permutationtest with p < 0.001.notators (on average the HDP model was off by 1.6senses, as compared to 3.6 by LDA).
Finally, theF1 performance of HDP is 1.9% better than LDA(85.7% vs. 83.8%).We further evaluated the LDA model by trainingseparately for each of the 35 nouns, first setting asthe number of topics the amount induced by HDP(on average, 5.8/9.4 senses for WSJ/BNC), then us-ing the number of senses as used by the human anno-tators in SemEval-2007 (an average of 3.8).
As seenin Table 3, in each of these cases HDP remained thesuperior model.5 ConclusionWe proposed the use of a nonparametric Bayesianmodel (HDP) for word sense induction and com-pared it with the parametric model by Brody andLapata (2009), based on LDA.
The HDP model con-fers the advantage of automatically identifying thenumber of senses, besides having equivalent (or bet-ter) performance than the LDA model, verified us-ing the SemEval-2007 dataset.
Future work includeslarge scale sense induction over a larger vocabulary,in tasks such as Paraphrase Acquisition.13ReferencesEneko Agirre and Aitor Soroa.
2007.
Semeval-2007 Task 02:Evaluating Word Sense Induction And Discrimination Sys-tems.
In Proceedings of the 4th International Workshop onSemantic Evaluations, SemEval ?07, pages 7?12.Stefan Bordag.
2006.
Word Sense Induction: Triplet-BasedClustering And Automatic Evaluation.
In Proceedings of the11th EACL, pages 137?144.Samuel Brody and Mirella Lapata.
2009.
Bayesian Word SenseInduction.
In EACL ?09: Proceedings of the 12th Confer-ence of the European Chapter of the Association for Compu-tational Linguistics, pages 103?111.Jeremy H. Clear, 1993.
The British national corpus, pages 163?187.
MIT Press, Cambridge, MA, USA.Beate Dorow and Dominic Widdows.
2003.
DiscoveringCorpus-Specific Word Senses.
In Proceedings of the tenthconference on European chapter of the Association for Com-putational Linguistics, EACL ?03, pages 79?82.T.
S. Ferguson.
1973.
A Bayesian Analysis of Some Nonpara-metric Problems.
The Annals of Statistics, 1(2):209?230.S.
Geman and D. Geman, 1990.
Stochastic Relaxation, GibbsDistributions, And The Bayesian Restoration Of Images,pages 452?472.Eduard Hovy, Mitchell Marcus, Martha Palmer, LanceRamshaw, and Ralph Weischedel.
2006.
OntoNotes: the90% solution.
In Proceedings of the Human LanguageTechnology Conference of the NAACL, Companion Volume:Short Papers, NAACL-Short ?06, pages 57?60.Ioannis Klapaftis and Suresh Manandhar.
2010.
Word SenseInduction & Disambiguation Using Hierarchical RandomGraphs.
In Proceedings of the 2010 Conference on Empir-ical Methods in Natural Language Processing, pages 745?755, October.Linlin Li, Benjamin Roth, and Caroline Sporleder.
2010.
TopicModels For Word Sense Disambiguation And Token-BasedIdiom Detection.
In Proceedings of the 48th Annual Meetingof the Association for Computational Linguistics, ACL ?10,pages 1138?1147.Mitchell P. Marcus, Beatrice Santorini, and Mary A.Marcinkiewicz.
1994.
Building a Large Annotated Corpusof English: The Penn Treebank.
Computational Linguistics,19(2):313?330.Andrew Kachites McCallum.
2002.
MALLET: A MachineLearning for Language Toolkit.Zheng-Yu Niu, Dong-Hong Ji, and Chew-Lim Tan.
2007.
I2R:Three Systems For Word Sense Discrimination, ChineseWord Sense Disambiguation, And English Word Sense Dis-ambiguation.
In Proceedings of the 4th International Work-shop on Semantic Evaluations, SemEval ?07, pages 177?182.Patrick Pantel and Dekang Lin.
2002.
Discovering WordSenses From Text.
In Proceedings of the eighth ACMSIGKDD international conference on Knowledge discoveryand data mining, KDD ?02, pages 613?619.Ted Pedersen.
2007.
UMND2: SenseClusters applied to thesense induction task of Senseval-4.
In Proceedings of the 4thInternational Workshop on Semantic Evaluations, SemEval?07, pages 394?397.Amruta Purandare and Ted Pedersen.
2004.
Word Sense Dis-crimination by Clustering Contexts in Vector and SimilaritySpaces.
In Proceedings of CoNLL-2004, pages 41?48.Joseph Reisinger and Raymond J. Mooney.
2010.
A MixtureModel with Sharing for Lexical Semantics.
In Proceedingsof the Conference on Empirical Methods in Natural Lan-guage Processing (EMNLP-2010), pages 1173?1182, MIT,Massachusetts, USA.Hinrich Schu?tze.
1998.
Automatic Word Sense Discrimination.Comput.
Linguist., 24:97?123, March.Y.
W. Teh, M. I. Jordan, M. J. Beal, and D. M. Blei.
2006.Hierarchical Dirichlet Processes.
Journal of the AmericanStatistical Association, 101(476):1566?1581.Andreas Vlachos, Anna Korhonen, and Zoubin Ghahramani.2009.
Unsupervised and constrained Dirichlet process mix-ture models for verb clustering.
In Proceedings of the Work-shop on Geometrical Models of Natural Language Seman-tics, GEMS ?09, pages 74?82.Chong Wang.
2010.
An implementation of hierarchical dirich-let process (HDP) with split-merge operations.David Yarowsky.
1992.
Word-Sense Disambiguation UsingStatistical Models Of Roget?s Categories Trained On LargeCorpora.
In Proceedings of the 14th conference on Compu-tational linguistics, pages 454?460.14
