Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1619?1629,Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational LinguisticsAid is Out There:Looking for Help from Tweets during a Large Scale DisasterIstva?n Varga?
Motoki Sano?
Kentaro Torisawa?
Chikara Hashimoto?Kiyonori Ohtake?
Takao Kawai?
Jong-Hoon Oh?
Stijn De Saeger?
?Information Analysis Laboratory,National Institute of Information and Communications Technology (NICT), Japan{istvan, msano, torisawa, ch, kiyonori.ohtake, rovellia, stijn}@nict.go.jp?Knowledge Discovery Research Laboratories, NEC Corporation, Japant-kawai@bx.jp.nec.comAbstractThe 2011 Great East Japan Earthquakecaused a wide range of problems, and ascountermeasures, many aid activities werecarried out.
Many of these problems andaid activities were reported via Twitter.However, most problem reports and corre-sponding aid messages were not success-fully exchanged between victims and lo-cal governments or humanitarian organi-zations, overwhelmed by the vast amountof information.
As a result, victims couldnot receive necessary aid and humanitar-ian organizations wasted resources on re-dundant efforts.
In this paper, we proposea method for discovering matches betweenproblem reports and aid messages.
Oursystem contributes to problem-solving ina large scale disaster situation by facilitat-ing communication between victims andhumanitarian organizations.1 IntroductionThe 2011 Great East Japan Earthquake in March11, 2011 killed 15,883 people and destroyed over260,000 households (National Police Agency ofJapan, 2013).
Accustomed way of living suddenlybecame unmanageable and people found them-selves in extreme conditions for months.Just after the disaster, many people used Twitterfor posting problem reports and aid messages asit functioned while most communication channelssuffered disruptions (Winn, 2011; Acar and Mu-raki, 2011; Sano et al, 2012).
Examples of suchproblem reports and aid messages, translated fromJapanese tweets, are given below (P1, A1).P1 My friend said infant formula is sold out.
Ifsomebody knows shops in Sendai-city wherethey still have it in stock, please let us know.A1 At Jusco supermarket in Sendai, you can stillbuy water and infant formula.If A1 would have been forwarded to the senderof P1, it could have helped since it would helpthe ?friend?
to obtain infant formula.
But in re-ality, the majority of such reports/messages, es-pecially unforeseen ones went unnoticed amongstthe mass of information (Ohtake et al, 2013).
Inaddition, there were cases where many humani-tarian organizations responded to the same prob-lems and wasted precious resources.
For instance,many volunteers responded to problems whichwere heavily reported by public media, leadingto oversupply (Saijo, 2012).
Such waste of re-sources could have been avoided if the organiza-tions would have successfully shared the aid mes-sages for the same problems.Such observations motivated this work.
We de-veloped methods for recognizing problem reportsand aid messages in tweets and finding propermatches between them.
By browsing the discov-ered matches, victims can be assisted to over-come their problems, and humanitarian organiza-tions can avoid redundant relief efforts.
We defineproblem reports, aid messages and their successfulmatches as follows.Problem report: A tweet that informs about thepossibility or emergence of a problem that re-quires a treatment or countermeasure.Aid message: A tweet that (1) informs about sit-uations or actions that can be a remedy or so-lution for a problem, or (2) informs that theproblem is solved or is about to be solved.Problem-aid tweet match: A tweet pair is aproblem-aid tweet match (1) if the aid mes-sage informs how to overcome the problem,(2) if the aid message informs about the set-1619tlement of the problem, or (3) if the aid mes-sage provides information which contributesto the settlement of the problem.In this work we excluded direct requests, such as?Send us food!
?, from problem reports.
This is be-cause it is relatively easy to recognize such directrequests by checking mood types (i.e., imperative)and their behavior is quite different from prob-lem reports like ?People in Sendai are starving?.Problem reports in this work do not directly statewhich actions are required, only implying the ne-cessity of a countermeasure through claiming theexistence of problems.An underlying assumption of our method is thatwe can find a noun-predicate dependency relationthat works as an indicator of problems and aids inproblem reports and aid messages, which we referto as problem nucleus and aid nucleus.1 An exam-ple of problem nucleus is ?infant formula is soldout?
in P1, and that of aid nucleus is ?
(can) buyinfant formula?
in A1.
Many problem-aid tweetmatches can be recognized through problem andaid nuclei pairs.We also assume that if the problem and aid nu-clei match, they share the same noun.
Then, thesemantics of predicates in the nuclei is the mainfactor that decides whether the nuclei constitutea match.
We introduce a semantic classificationof predicates according to the framework of ex-citation polarities proposed in Hashimoto et al(2012).
Our hypothesis is that excitation polaritiesalong with trouble expressions can characterizeproblem reports, aid messages and their matches.We developed a supervised method encoding suchinformation into its features.An evident alternative to this approach is to usesentiment analysis (Mandel et al, 2012; Tsagkali-dou et al, 2011) assuming that problem reportsshould include something ?bad?
while aid mes-sages describe something ?good?.
However, wewill show that this does not work well in our exper-iments.
We think this is due to mismatch betweenthe concepts of problem/aid and sentiment polar-ity.
Note that previous work on ?demand?
recogni-tion also found similar tendencies (Kanayama andNasukawa, 2008).Another issue in this task is, of course, thecontext surrounding problem/aid nuclei.
The fol-1We found that out of 500 random tweets only 4.5% ofproblem reports and 9.1% of aid messages did not containany problem report/aid message nuclei.lowing (imaginary) tweets exemplify the problemscaused by contexts.FP1 I do not believe infant formula is sold outin Sendai.FA1 At Jusco supermarket in Iwaki, you can stillbuy infant formula.The problem nuclei of FP1 and P1 are the samebut FP1 is not a problem report because of the ex-pression ?I do not believe?.
The aid nuclei of FA1and A1 are the same but FA1 does not constitutea proper match with P1 because FA1 and P1 re-fer to different cities, ?Iwaki?
and ?Sendai?.
Inthis work, the problems concerning the modalityand other semantic modifications to problem/aidnuclei by context are dealt with by the introduc-tion of features representing the text surroundingthe nuclei in machine learning.
As for the loca-tion problem, we apply a location recognizer to alltweets and restrict the matching candidates to thetweet pairs referring to the same location.2 Approach!"#$%"&"'()*&+*,*&-.(.+*,/+/0$0,/0,)-+$*#.(,'+!
"#$%&'("&!#")("&*#+,-.&"(12001.+ 12001+/0$0,/0,)-+#0&*3",+$#"4&0!+#0$"#1+$#"4&0!+,5)&05.+/-0('&11/+&("&*#+,-.&"(*(/+!0..*'0++*(/+,5)&05.+$#"4&0!+#0$"#1+$#"4&0!+,5)&05.+ *(/+!0..*'0+*(/+,5)&05.+!"#$%&'2/-0()3&&)('/)*4($#"4&0!+*,/+*(/+,"5,+*#0+1%0+.*!06+.*!0+'0"'#*$%()*&+&")*3",+&")*3",+#0)"',(70#+!
"#$%&'2/-0('/)*4("&*#+,-.&"(Figure 1: Problem-aid matching system overview.We developed machine learning based systemsto recognize problem reports, aid messages andproblem-aid tweet matches.
Figure 1 illustratesthe whole system.
First, location names in tweetsare identified by matching tweets against our loca-tion dictionary, described in Section 3.
Then, eachtweet is paired with each dependency relation inthe tweet, which is a candidate of problem/aid nu-clei and given to the problem report and aid mes-sage recognizers.
A tweet-nucleus-candidate pairjudged as problem report is combined with anothertweet-nucleus-candidate pair recognized as an aidmessage if the two nuclei share the same noun andthe tweets share the same location name, and givento the problem-aid match recognizer.1620In the following, problem and aid nuclei aredenoted by a noun-template pair.
A template iscomposed of a predicate and its argument posi-tion.
For instance, ?water supply stopped?
in P2is a problem nucleus, ?water supply recovered?
inA2 is an aid nucleus and they are denoted by thenoun-template pairs ?water supply, X stopped?
and?water supply, X recovered?.P2 In Sendai city, water supply stopped.A2 In Sendai city, water supply recovered.Roughly speaking, we regard the tasks of prob-lem report recognition and aid message recogni-tion as the tasks of finding proper problem/aidnuclei in tweets and our method performs thesetasks based on the semantic properties of nounsand templates in problem/aid nucleus candidatesand their surrounding contexts.The basic intuition behind this approach canbe explained using excitation polarity proposed inHashimoto et al (2012).
Excitation polarity differ-entiates templates into ?excitatory?
or ?inhibitory?with regard to the main function or effect of en-tities referred to by their argument noun.
Whileexcitatory templates (e.g., cause X, buy X, suf-fer from X) entail that the main function or ef-fect is activated or enhanced, inhibitory templates(e.g., ruin X, prevent X, X runs out) entail thatthe main function or effect is deactivated or sup-pressed.
The templates that do not fit into theabove categorization are classified as ?neutral?.We observed that problem reports in generalincluded either of (A) a dependency relation be-tween a noun referring to some trouble and anexcitatory template or (B) a dependency rela-tion between a noun not referring to any troubleand an inhibitory template.
Examples of (A) in-clude ?carbon monoxide poisoning, suffer fromX?, ?false rumor, spread X?.
They refer to eventsthat activate troubles.
On the other hand, (B) isexemplified by ?school, X is collapsed?, ?battery,X runs out?, which imply that some non-troubleobjects such as resources, appliances and facilitiesare dysfunctional.
We assume that if we can findsuch dependency relations in tweets, the tweets arelikely to be problem reports.Contrary, a tweet is more likely to be an aidmessage when it includes either (C) a dependencyrelation between a noun referring to some troubleand an inhibitory template or (D) a dependency re-lation between a noun not referring to any trou-trouble non-troubleexcitatory (A) problem nucleus (D) aid nucleusinhibitory (C) aid nucleus (B) problem nucleusTable 1: Problem/aid-excitation matrix.ble and an excitatory template.
Examples of (C)are ?flu, X was eradicated (in some shelter)?
and?debris, remove X?.
They represent the dysfunc-tion of troubles and can mean the solution or thesettlement of troubles.
On the other hand, exam-ples of (D) include ?school, X re-build?
and ?babyformula, buy X?.
They entail that some resourcesfunction properly or become available.
These for-mulations are summarized in Table 1.As an interesting consequence of such a viewon problem/aid nucleus, we can say the followingregarding problem-aid tweet matchings: when aproblem nucleus and an aid nucleus are an ade-quate match, the excitation polarities of their tem-plates are opposite.
Consider the following tweets.P3 Some people were going back to Iwaki, but thewater system has not come back yet.
It?s ter-rible that bath is unusable.A3 We open the bath for the public, located onthe 2F of Iwaki Kuhon temple.
If you?re stay-ing at a relief shelter and would like to take abath, you can use it.
?Bath is unusable?
in P3 is a problem nucleuswhile ?open the bath?
in A3 is an aid nucleus.Since the problem reported in P3 can be solvedwith A3, they are a successful match.
The in-hibitory template ?X is unusable?
indicates thatthe function of ?bath?, a non-trouble expression,is suppressed.
The excitatory template ?open X?indicates that the function of ?bath?
is activated.The same holds when we consider the noun re-ferring to troubles like ?flu?.
The polarity of thetemplate in a problem nucleus should be excita-tory like ?flu is raging?
while that of an aid nucleusshould be inhibitory like ?flu, X was eradicated?.These examples keep the constraint that the prob-lem and aid nucleus should have opposite polari-ties when they constitute a match.Note that the formulations of problem report,aid message and their matches or the excitationmatrix (Table 1) were not presented to our anno-tators and our test/training data may contain datathat contradict with the formulations.
These for-mulations constitute the hypothesis to be validatedin this work.1621An important point to be stressed here is thatthere are problem-aid tweet matches that do notfit into our formulations.
For instance, we as-sume that the problem nucleus and aid nucleus ina proper match share the same noun.
However,tweet pairs such as ?There are many injured peoplein Sendai city?
and ?We are sending ambulancesto Sendai?
can constitute a proper match, but thereis no proper problem-aid nuclei pair that share thesame noun in these tweets.
(We can find the de-pendency relations sharing ?Sendai?
but they donot express anything about the contents of prob-lem and aid.)
The point is that the tweet pairs canbe judged because people know ambulances canbe a countermeasure to injured people as worldknowledge.
Introducing such world knowledge isbeyond the scope of this current study.Also, we exclude direct requests from problemreports.
As mentioned in the introduction, identi-fying direct requests is relatively easy, hence weexcluded them from our target.3 Problem Report and Aid MessageRecognizersWe recognize problem reports and aid messages ingiven tweets using a supervised classifier, SVMswith linear kernel, which worked best in our pre-liminary experiments.
The feature set given tothe SVMs are summarized in the top part of Ta-ble 2.
Note that we used a common featureset for both the problem report recognizer andaid message recognizer and that it is categorizedinto several types: features concerning troubleexpressions (TR), excitation polarity (EX), theircombination (TREX1) and word sentiment polar-ity (WSP), features expressing morphological andsyntactic structures of nuclei and their context sur-rounding problem/aid nuclei (MSA), features con-cerning semantic word classes (SWC) appearingin nuclei and their context, request phrases, suchas ?Please help us?, appearing in tweets (REQ),and geographical locations in tweets recognizedby our location recognizer (GL).
MSA is used toexpress the modality of nuclei and other contex-tual information surrounding nuclei.
REQ was in-troduced based on our observation that if there aresome requests in tweets, problem nuclei tend toappear as justification for the requests.We also attempted to represent nucleus templateIDs, noun IDs and their combinations directly inour feature set to capture typical templates fre-TR Whether the nucleus noun is a trouble/non-trouble expression.EX1 The excitation polarity and the value of the excitation score of thenucleus template.TREX1 All possible combinations of trouble/non-trouble of TR and exci-tation polarities of EX1.WSP1 Whether the nucleus noun is positive/negative/not in theWord Sen-timent Polarity (WSP) dictionary.WSP2 Whether the nucleus template is positive/negative/not in the WSPdictionary.WSP3 Whether the nucleus template is followed by a positive/negativeword within the tweet.MSA1 Morpheme n-grams, syntactic dependency n-grams in the tweetand morpheme n-grams before and after the nucleus template.
(1 ?
n ?
3)MSA2 Character n-grams of the nucleus template to capture conjugationand modality variations.
(1 ?
n ?
3)MSA3 Morpheme and part-of-speech n-grams within the bunsetsu con-taining the nucleus template to capture conjugation and modalityvariations.
(1 ?
n ?
3) (A bunsetsu is a syntactic constituentcomposed of a content word and several function words, the small-est unit of syntactic analysis in Japanese.
)MSA4 The part-of-speech of the nucleus template?s head to capturemodality variations outside the nucleus template?s bunsetsu.MSA5 The number of bunsetsu between the nucleus noun and the nucleustemplate.
We found that a long distance between the noun and thetemplate suggests parsing errors.MSA6 Re-occurrence of the nucleus noun?s postpositional particle be-tween the nucleus noun and the nucleus template.
We foundthat the re-occurrence of the same postpositional particle withina clause suggests parsing errors.SWC1 The semantic class n-grams in the tweet.SWC2 The semantic class(es) of the nucleus noun.REQ Presence of a request phrase in the tweet, identified from within426 manually collected request phrases.GL Geographical locations in the tweet identified using our locationrecognizer.
Existence/non-existence of locations in tweets are alsoencoded.EX2 Whether the problem and aid nucleus templates have the same oropposite excitation polarities.EX3 Product of the values of the excitation scores for the problem andthe aid nucleus template.TREX2 All possible combinations of trouble/non-trouble of TR, excitationpolarity EX1 of the problem nucleus template and excitation po-larity EX1 of the aid nucleus template.SIM1 Common semantic word classes of the problem report and aid mes-sage.SIM2 Whether there are common nouns modifying the common nucleusnoun or not in the problem report and aid message.SIM3 Whether the words in the same word class modify the commonnucleus noun or not in the problem report and aid message.SIM4 The semantic similarity score between the problem nucleus tem-plate and the aid nucleus template.CTP Whether the problem nucleus template and the aid nucleus tem-plate are in contradiction relation dictionary or not.SSR1 Problem report recognizer?s SVM score of problem nucleus tem-plate.SSR2 Problem report recognizer?s SVM score of aid nucleus template.SSR3 Aid message recognizer?s SVM score of the problem nucleus tem-plate.SSR4 Aid message recognizer?s SVM score of the aid nucleus template.Table 2: Features used with the problem re-port recognizer and the aid message recognizer(above); additional features used in training theproblem-aid match recognizer (below).quently appearing in problem and aid nuclei, butsince there was no improvement we omit them.The other feature types need some non-trivialdictionaries.
In the following, we explain how wecreated the dictionaries for each feature type alongwith the motivation behind their introduction.Trouble Expressions (TR) As mentioned previ-ously, trouble expressions work as good evidencefor recognizing problem reports and aid messages.The TR feature indicates whether the noun in theproblem/aid nucleus candidate is a trouble ex-1622pression or not.
For this purpose, we createda list of trouble expressions following the semi-supervised procedure presented in De Saeger et al(2008).
After manual validation of the list, we ob-tained 20,249 expressions referring to some trou-bles, such as ?tsunami?
and ?flu?.
The value of theTR feature is determined by checking whether thenucleus noun is contained in the list.Excitation Polarities (EX) The excitation po-larities are also important in recognizing problemreports and aid messages as mentioned before.
Forconstructing the dictionary for excitation polaritiesof templates, we applied the bootstrapping proce-dure in Hashimoto et al (2012) to 600 millionWebpages.
Hashimoto?s method provides the value ofthe excitation score in [?1, 1] for each templateindicating the polarities and their strength.
Posi-tive value indicates excitatory, negative value in-hibitory and small absolute value neutral.
Aftermanual checking of the results by the majorityvote of three human annotators (other than the au-thors), we limited the templates to the ones thathave score values consistent with the majority voteof the annotators, obtaining a dictionary consistingof 7,848 excitatory, 836 inhibitory and 7,230 neu-tral templates.
The Fleiss?
(1971) kappa-score was0.48 (moderate agreement).
We used the excita-tion score values as feature values.
Excitation hasalready been used in many works, such as causal-ity and contradiction extraction (Hashimoto et al,2012) or Why-QA (Oh et al, 2013).Word Sentiment Polarity (WSP) As we sug-gested before, full-fledged sentiment analysis torecognize the expressions, including clauses andphrases, that refer to something good or bad wasnot effective in our task.
However, the sentimentpolarity, assigned to single words turned out tobe effective.
To identify the sentiment polarityof words, we employed the word sentiment polar-ity dictionary used with a sentiment analysis toolfor Japanese, the Opinion Extraction Tool soft-ware2, which is an implementation of Nakagawaet al (2010).
The dictionary includes 9,030 posi-tive and 27,951 negative words.
Note that we usedthe Opinion Extraction Tool in the experiments tocheck the effectiveness of the full-fledged senti-ment analysis in this task.Semantic Word Class (SWC) We assume thatnouns in the same semantic class behave simi-2Provided at the ALAGIN Forum (http://www.alagin.jp/).larly in crisis situations.
For example, if ?infec-tion?
appears in a problem report, the tweets in-cluding ?pulmonary embolism?
are also likely tobe problem reports.
Semantic word class featuresare used to capture such tendencies.
We appliedan EM-style word clustering algorithm in Kazamaand Torisawa (2008) to 600 millionWeb pages andclustered 1 million nouns into 500 classes.
Thisalgorithm has been used in many works, such asrelation extraction (De Saeger et al, 2011) andWhy-QA (Oh et al, 2012), and can generate vari-ous kinds of semantically clean word classes, suchas foods, disease names, and natural disasters.
Weused the word classes in tweets as features.3Geographical Locations (GL) Our locationrecognizer matches tweets against our loca-tion dictionary.
Location names and theirexistence/non-existence in tweets constitute evi-dence, thus we encoded such information into ourfeatures.
The location dictionary was created fromthe Japan Post code data4 and Wikipedia, contain-ing 2.7 million location names including cities,schools and other facilities (Kazama et al, 2013).4 Problem-Aid Match RecognizerAfter problem report and aid message recogni-tion, the positive outputs of the respective classi-fiers are used as input in this step.
The problem-aid match recognizer classifies an aid message-nucleus pair together with the problem report-nucleus pair employing SVMs with linear ker-nel, which performed best in this task again.
Theproblem-aid match recognizer uses all the featuresused in the problem report recognizer and the aidmessage recognizer along with additional featuresregarding: excitation polarity (EX) and troubleexpressions (TR), distributional similarity (SIM),contradiction (CTP) and SVM-scores of the prob-lem report and aid message recognizers (SSR).Here also we attempted to capture typical or fre-quent matches of nuclei using template and nounIDs and their combinations, but we did not observeany improvement so we omit them from the fea-ture set.
The bottom part of Table 2 summarizesthe additional feature set, some of which are de-scribed below in more detail.3There is a slight complication here.
For each noun n, EMclustering estimates a probability distribution P (n|c?)
for nand semantic class c?.
From this distribution we obtaineddiscrete semantic word classes by assigning each noun n tosemantic class c = argmaxc?
p(c?|n).4http://www.post.japanpost.jp/zipcode/download.html1623As for TR and EX, our intuition is that if a prob-lem nucleus and an aid nucleus are an adequatematch, their excitation polarities are opposite, asdescribed in Section 2.
We encode whether the ex-citation polarities of nuclei templates are the sameor not in our features.
Also, the excitation polar-ities of problem and aid nuclei and TR are com-bined (TREX1, TREX2) so that the classifier canknow whether the nuclei follow the constraint foradequate matches described in Section 2.As for SIM, if an aid message matches a prob-lem report, besides the common nucleus noun, it isreasonable to assume that certain contexts are se-mantically similar.
We capture this characteristicin three ways.
SIM1 looks for common semanticword classes in the problem report and aid mes-sage.
SIM2 and SIM3 target the modifiers of thecommon nucleus noun if they exist.We also observed that if an aid message matchesa problem report, the problem nucleus templateand aid nucleus template are often distributionallysimilar.
A typical example is ?X is sold out?
and?buy X?.
SIM4 captures this tendency.
As the dis-tributional similarity between templates, we useda Bayesian distributional similarity measure pro-posed by Kazama et al (2010).5CTP indicates whether the problem and aid nu-clei are in contradiction relation or not.
This fea-ture was implemented based on the observationthat when problem and aid nuclei are in contradic-tion relation, they are often proper matches (e.g.,?blackout, ?X starts??
and ?blackout, ?X ends??
).CTP indicates whether nucleus pairs are in theone million contradiction phrase pairs6 automat-ically obtained by applying a method proposed byHashimoto et al (2012) to 600 million Web pages.5 ExperimentsWe evaluated our problem report recognizer andproblem-aid match recognizer.
For the sake ofspace, we give only the performance figures of theaid message recognizer at the end of Section 5.1.We collected tweets posted during and afterthe 2011 Great East Japan Earthquake, betweenMarch 10 and April 4, 2011.
After applyingkeyword-based filtering with a list of over 3005The original similarity was defined over noun pairs and itwas estimated from dependency relations.
Obtaining similar-ity between template pairs, not noun pairs, is straightforwardgiven the same dependency relations.
We used 600 millionWeb pages for this similarity estimation.6The precision of the pairs was reported as around 70%.disaster related keywords, we obtained 55 milliontweets.
After dependency parsing7, we used themin our evaluation.5.1 Problem Report RecognitionFirstly, we evaluated our problem report recog-nizer.
Particularly, we assessed the effect of ex-citation polarities and trouble expressions in twosettings.
The first is against a naturally distributedgold standard data.
The second targets problemreports with problem nuclei unseen in the trainingdata.In both experiments we observed that the per-formance drops when excitation polarities andtrouble expressions are removed from the featureset.
The performance drop was larger in the sec-ond experiment which suggests that the excitationpolarities and trouble expressions are more effec-tive against unseen problem reports.Training and test data for problem report recog-nition consist of tweet-nucleus candidate pairsrandomly sampled from our 55 million tweet data.The training data (R) and test data (T ) consist of13,000 and 1,000 pairs, respectively, manually la-beled by three annotators (other than the authors)as problem or other.
Final judgment was made bymajority vote.
The Fleiss?
kappa score for train-ing and test data for annotation judgement is 0.74(substantial agreement).Our problem report recognizer and its variantsare listed in Table 3.
Table 4 shows the evalua-tion results.
The proposed method achieved about44% recall and nearly 80% precision, outperform-ing all other systems in terms of precision, F-scoreand average precision8.
The improvement in pre-cision when using TR&EX is statistically signif-icant (p < 0.05).9 Note that F-measure droppedPROPOSED: Our proposed method with all features used.PROPOSED-*: The proposed method without the feature set de-noted by ?*?.
Here EX and TR denote all excitation po-larity and trouble expression related features, respectively,including their combinations (TREX1).PROPOSED+OET: The proposed method incorporating theclassification results of problem nucleus candidates by theOpinion Extraction Tool as additional binary features.RULE-BASED: The method that regards only nuclei satisfyingthe constraint in Table 1 as problem nuclei.Table 3: Evaluated problem report recognizers.7http://nlp.ist.i.kyoto-u.ac.jp/EN/index.php?KNP8We calculate average precision using the formula: aP =?nk=1(Prec(k)?rel(k))n , where Prec(k) is the precision atcut-off k and rel(k) is an indicator function equaling 1 ifthe item at rank k is relevant, zero otherwise.9Throughout this paper we performed two-tailed test of1624Recognition system R (%) P (%) F (%) aP (%)PROPOSED 44.26 79.41 56.83 71.82PROPOSED-TR&EX 45.08 74.83 56.26 69.67PROPOSED-EX 44.67 74.66 55.89 69.90PROPOSED-TR 43.85 74.31 55.15 69.44PROPOSED-MSA 28.69 70.71 40.81 57.74PROPOSED-SWC 43.42 75.97 55.25 70.61PROPOSED-WSP 43.14 77.83 55.50 70.45PROPOSED-REQ 42.64 76.16 55.50 54.67PROPOSED-GL 44.14 78.34 55.50 56.46PROPOSED+OET 44.24 79.41 56.82 71.81RULE-BASED 30.32 67.96 41.93 n/aTable 4: Recall (R), precision (P), F-score (F) andaverage precision (aP) of the problem report rec-ognizers.whenever each type of feature was removed, im-plying that each type of feature is effective in thistask.
Especially note the performance drop if weremove excitation polarities (EX), trouble expres-sion (TR) and both excitation and trouble expres-sion features (TR&EX), confirming that they arecrucial in recognizing problem reports with highaccuracy.
Also note that the performance of PRO-POSED+OET was actually slightly worse than thatof the proposed method.
This suggests that full-fledged sentiment analysis is not effective at leastin this setting.
The rule-based method achievedrelatively high precision despite of the low recall,demonstrating the importance of problem and aidnuclei formulations described in Section 1.The second experiment assessed the efficiencyof our problem report recognizer against unseenproblem nuclei under the condition that every tem-plate in nuclei has excitation polarity.
We sampledthe training and test data so that the problem nu-cleus nouns and templates in the training and testdata are disjoint.
First we created a subset of thetest data by selecting the samples which had nu-clei with excitation templates.
We call this sub-set T ?.
Next, we removed samples from trainingdata R if either of their problem nouns or tem-plates appeared in the nuclei of T ?.
The result-ing new training data (called R?)
and test data (T ?
)consist of 6,484 and 407 tweet-nucleus candidatepairs, respectively.
We trained our problem reportrecognizer using R?
and tested its performance us-ing T ?.
Figure 2 shows the precision-recall curvesobtained by changing the threshold on the SVMscores.
The effectiveness of excitation polaritiesand trouble expressions was more evident in thissetting.
The PROPOSED?s performance was ac-tually better in this setting (almost 50% recall atpopulation proportion (Ott and Longnecker, 2010) usingSVM-threshold=0.0.20.30.40.50.60.70.80.910  0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9  1PrecisionRecallPROPOSED-TRPROPOSED-EX PROPOSED-TR&EXPROPOSEDFigure 2: Precision-recall curves of problem re-port recognizers against unseen problem nuclei.more that 80% precision), than the previous set-ting, showing that excitation templates and troubleexpressions are crucial in achieving high perfor-mance especially for unseen problem nuclei.
Thesame was confirmed when we removed excitationpolarity and trouble expression related features,with performance dropping by 7.43 points in termsof average precision.
The improvement in pre-cision when using TR&EX is statistically signif-icant (p < 0.01).
This implies, assuming that wehave a wide-coverage dictionary of templates withexcitation polarities, that excitation polarities areimportant in dealing with unexpected problems indisaster situations.We also evaluated the aid-message recognizer,using tweet-nucleus pairs in R and T as train-ing and test data and the annotation scheme wasalso the same.
The average Fleiss?
kappa scorewas 0.55 (moderate agreement).
Our recognizerachieved 53.82% recall and 65.67% precision andshowed similar tendencies with the problem reportrecognizer, with the excitation polarities and trou-ble expressions contributing to higher accuracy.We can conclude that excitation polarities andtrouble expressions are important in identifyingproblem reports and aid messages during disastersituations.5.2 Problem-Aid MatchingNext, we evaluated the performance of theproblem-aid match recognizer.
We applied ourproblem report recognizer and aid message recog-nizer to all 55 million tweets and combined thetweet-nucleus pairs judged as problem reports andaid messages, respectively, to create the trainingand test data.The training data consists of two parts (M1 andM2).
M1 includes many variations of the aidmessages for each problem report, while M2 en-1625sures diversity in nouns and templates in problemnuclei.
For M1, we randomly picked up problemreports from the output of the problem report rec-ognizer and to each we attached up to 30 randomlypicked, distinct aid messages that have the samenucleus noun.
Building M2 follows the construc-tion method of M1, except that: (1) we used upto 30 distinct problem nuclei for each noun; (2)for each problem report we attached only one ran-domly picked aid message.In creating the test data T2, we followed theconstruction method used for M2 to assess theperformance of our proposal with a large varietyof problems.
M1, M2 and T2 consist of 3,000,6,000 and 1,000 samples, respectively.
The an-notation was done by majority vote of three hu-man annotators (other than the authors), the aver-age Fleiss?
kappa-score for training and test datawas 0.63 (substantial agreement).We trained the problem-aid match recognizersof Table 5 with M1 and M2.
The evaluationresults performed on T2 are shown in Table 6.We can observe that, among the nuclei relatedfeatures, the trouble expression (TR) and excita-tion polarity (EX) features and their combination(TR&EX) contribute most to the performance, al-though the contribution of nuclei related featuresis less in comparison to the problem report and aidmessage recognition.
The improvement in preci-sion when using TR&EX is marginally significant(p = 0.056).
Instead, morphological and syntacticanalysis (MSA) and semantic word class (SWC)features greatly improved performance.As the final experiments, we evaluated top-ranking matches of our problem-aid match recog-nizer, where the recognizer classified all the possi-ble combinations of tweet-nuclei pairs taken from55 million tweets.
In addition, we assessed the ef-fectiveness of excitation polarities and trouble ex-pressions by comparing all positive matches pro-duced by our full problem-aid match recognizer(PROPOSED) and those produced by the problem-aid match recognizer (PROPOSED-TR&EX) thatPROPOSED: Our proposed method with all features used.PROPOSED-*: The proposed method without the feature set de-noted by ?*?.
Here also EX and TR denote all excitationpolarity and trouble expression related features, respec-tively, including their combinations (TREX1 and TREX2).RULE-BASED: The method that judges only problem-aid nucleicombinations with opposite excitation polarities as propermatches.Table 5: Evaluated problem-aid match recogniz-ers.Matching system R (%) P (%) F (%) aP (%)PROPOSED 30.67 70.42 42.92 55.16PROPOSED-TR&EX 28.83 67.14 40.33 53.99PROPOSED-EX 31.29 67.11 42.68 54.19PROPOSED-TR 30.56 69.33 42.42 54.85PROPOSED-MSA 13.50 53.66 21.57 44.52PROPOSED-SWC 26.99 67.69 38.59 52.23PROPOSED-WSP 30.61 69.51 42.50 54.81PROPOSED-CTP 30.06 70.00 42.05 54.94PROPOSED-SIM 29.95 70.11 41.97 54.98PROPOSED-REQ 30.58 70.25 42.61 54.67PROPOSED-GL 30.61 70.31 42.65 55.02PROPOSED-SSR 30.67 69.44 42.72 54.91RULE-BASED 15.33 17.36 16.28 n/aTable 6: Recall (R), precision (P), F-score (F) andaverage precision (aP) of the problem-aid matchrecognizers.did not use excitation polarities and trouble ex-pressions in its feature set.
Note that PROPOSED-TR&EX was fed by the problem report and aidmessage recognizers that didn?t use excitation po-larities and trouble expressions.
For both systems?training data we used R for the problem reportand aid message recognizers; M1 and M2 for theproblem-aid matching recognizers.PROPOSED and PROPOSED-TR&EX output 15.2million and 13.4 million positive matches, cover-ing 1,691 and 1,442 nucleus nouns, respectively.Table 7 shows match samples identified with PRO-POSED.
We observed that the output of each sys-tem was dominated by just a handful of frequentnucleus nouns, such as ?water?
or ?gasoline?.
Wepreferred to assess the performance of our systemagainst a large variation of problem-aid nuclei,thus we restricted the number of matches to 10for each noun10.
After this restriction the numberof matches found by PROPOSED and PROPOSED-00.20.40.60.810  1000  2000  3000  4000  5000  6000  7000  8000  9000PrecisionRankPROPOSED (unseen)PROPOSED-TR&EX (unseen)PROPOSED (all)PROPOSED-TR&EX (all)Figure 3: Problem-aid match recognition perfor-mance for ?all?
and ?unseen?
problem reports.10Note that this setting is a pessimistic estimation of oursystem?s overall performance, since according to our obser-vations problem reports with very frequent nucleus nouns hadproper matches with a higher accuracy than problem reportswith less frequent nucleus nouns.1626Problem report: ???????????????????????????????????????????????????????????????????????
(Starting from the 17th, the Iwaki Joban Hospital, the IwakiUrology Clinique, the Takebayashi Sadakichi Memorial Clin-ique and the Izumi Central Clinique have all suspended dial-ysis sessions.
Patients are advised to urgently make contact.
)Aid message: ?????????????????????????????????????????????
(Restart of dialysis sessions: short dialysis sessions areavailable at the Iwaki Urology Clinique between 9 AM and4 PM.
)Problem report: ?????????????????????????????????????????????????????????????????????????
(Please spread this message.
According to my father inSendai, there are more and more people whose phones ranout of battery.
We need phone chargers!
)Aid message: ??????????????????????????????
([Please spread] At the City Hall of Wakabayashi-ku, Sendai,you can recharge your phone battery.
)Table 7: Examples from the output of the proposedmethod in the ?all?
setting.
Problem report and aidmessage nuclei are boldfaced in the English trans-lations.TR&EX was 8,484 and 7,363, respectively.The performance of PROPOSED andPROPOSED-TR&EX were assessed in twosettings: ?all?
and ?unseen?.
For ?all?, we selected400 problem-aid matches from the outputs of therespective systems after applying the 10-matchrestriction.
For ?unseen?, first we removed thesamples from the systems?
outputs if either thenucleus noun or template pair appear in the nucleiof the problem-aid match recognizers?
trainingdata.
Next we applied the same sampling processas with ?all?.
Three annotators (other than theauthors) manually labeled the sample sets, finaljudgment being made by majority vote.
TheFleiss?
kappa score for all test data was 0.73(substantial agreement).Figure 3 shows the systems?
precision curves,drawn from the samples whose X-axis positionsrepresent the ranks according to SVM scores.
Inboth scenarios we can confirm that excitation po-larity and trouble expression related features con-tribute to this task.
In the ?all?
setting in termsof average precision calculated over the top 7,200matches, PROPOSED?s 62.36% is 10.48 pointshigher than that of PROPOSED-TR&EX.
For un-seen problem/aid nuclei PROPOSED method?s av-erage precision of 58.57% calculated at the top3,800 matches is 5.47 points higher than that ofPROPOSED-TR&EX at the same data point.
Theimprovement in precision when using TR&EX isstatistically significant in both settings (p < 0.01).6 Related WorkTwitter has been observed as a platform for situ-ational awareness during various crisis situations(Starbird et al, 2010; Vieweg et al, 2010), as sen-sors for an earthquake reporting system (Sakaki etal., 2010; Okazaki and Matsuo, 2010) or to de-tect epidemics (Aramaki et al, 2011).
BesidesTwitter, blogs or forums have also been the tar-get of community response analysis (Qu et al,2009; Torrey et al, 2007).
Similar to our workare the ones of Neubig et al (2011) and Ishino etal.
(2012), who tackle specific problems that occurduring disasters (i.e., safety information and trans-portation information, respectively); and Munro(2011), who extracted ?actionable messages?
(re-quests and aids, indiscriminately), matching beingperformed manually.
Our work differs from (Neu-big et al, 2011) and (Ishino et al, 2012) in that wedo not restrict the range of problem reports, and asopposed to (Munro, 2011), matching is automatic.Systems such as that of Seki (2011)11 or Munro(2013)12 are successful examples of crisis crowd-sourcing, but these require extensive human inter-vention to coordinate useful information.Another category of related work relevant to ourtask is troubleshooting.
Baldwin et al (2007) andRaghavan et al (2010) use discussion forums tosolve technical problems using supervised learn-ing methods, but these approaches presume thatthe solution of a specific problem is within thesame thread.
In our work we do not employ struc-tural characteristics of tweets as restrictions (e.g.,a problem report and its aid message need to be inthe same tweet chain).7 ConclusionsIn this paper, we proposed a method to dis-cover matches between problem reports and aidmessages from tweets in large-scale disasters.Through a series of experiments, we demonstratedthat the performance of the problem-aid match-ing can be improved with the usage of seman-tic orientation of excitation polarities, proposed in(Hashimoto et al, 2012), and trouble expressions.We are planning to deploy our system and re-lease model files of the classifiers to assist reliefefforts in future crisis scenarios.11http://www.sinsai.info/12http://www.mission4636.org/1627ReferencesAdam Acar and Yuya Muraki.
2011.
Twitter for cri-sis communication: Lessons learned from Japan?stsunami disaster.
International Journal of WebBased Communities, 7(3):392?402.Eiji Aramaki, Sachiko Maskawa, and Mizuki Morita.2011.
Twitter catches the flu: Detecting influenzaepidemics using Twitter.
In Proceedings of the Con-ference on Empirical Methods in Natural LanguageProcessing (EMNLP 2011), pages 1568?1576.Timothy Baldwin, David Martinez, and Richard B.Penman.
2007.
Automatic thread classification forLinux user forum information access.
In Proceed-ings of the 12th Australasian Document ComputingSymposium (ADCS 2007), pages 72?79.Stijn De Saeger, Kentaro Torisawa, and Jun?ichiKazama.
2008.
Looking for trouble.
In Proceed-ings of the 22nd International Conference on Com-putational Linguistics (COLING 2008), pages 185?192.Stijn De Saeger, Kentaro Torisawa, Masaaki Tsuchida,Jun?ichi Kazama, Chikara Hashimoto, Ichiro Ya-mada, Jong-Hoon Oh, Istva?n Varga, and Yulan Yan.2011.
Relation acquisition using word classes andpartial patterns.
In Proceedings of the Conferenceon Empirical Methods in Natural Language Pro-cessing (EMNLP 2011), pages 825?835.Joseph L. Fleiss.
1971.
Measuring nominal scaleagreement among many raters.
Psychological Bul-letin, 5:378?382.Chikara Hashimoto, Kentaro Torisawa, StijnDe Saeger, Jong-Hoon Oh, and Jun?ichi Kazama.2012.
Excitatory or inhibitory: A new semanticorientation extracts contradiction and causality fromthe web.
In Proceedings of the 2012 Joint Confer-ence on Empirical Methods in Natural LanguageProcessing and Computational Natural LanguageLearning (EMNLP-CoNLL 2012), pages 619?630.Aya Ishino, Shuhei Odawara, Hidetsugu Nanba, andToshiyuki Takezawa.
2012.
Extracting transporta-tion information and traffic problems from tweetsduring a disaster: Where do you evacuate to?
InProceedings of the Second International Conferenceon Advances in Information Mining and Manage-ment (IMMM 2012), pages 91?96.Hiroshi Kanayama and Tetsuya Nasukawa.
2008.
Tex-tual demand analysis: Detection of users?
wants andneeds from opinions.
In Proceedings of the 22nd In-ternational Conference on Computational Linguis-tics (COLING 2008), pages 409?416.Jun?ichi Kazama and Kentaro Torisawa.
2008.
Induc-ing gazetteers for named entity recognition by large-scale clustering of dependency relations.
In Pro-ceedings of the 46th Annual Meeting of the Associ-ation for Computational Linguistics: Human Lan-guage Technologies (ACL-08: HLT), pages 407?415.Jun?ichi Kazama, Stijn De Saeger, Kow Kuroda,Masaki Murata, and Kentaro Torisawa.
2010.
ABayesian method for robust estimation of distribu-tional similarities.
In Proceedings of the 48th An-nual Meeting of the Association for ComputationalLinguistics (ACL 2010), pages 247?256.Jun?ichi Kazama, Stijn De Saeger, Kentaro Torisawa,Jun Goto, and Istva?n Varga.
2013.
Saigaiji jouhoue no shitsumon outo shisutemu no tekiyou no koko-romi.
(An attempt for applying question-answeringsystem on disaster related information).
In Pro-ceeding of the Nineteenth Annual Meeting of TheAssociation for Natural Language Processing.
(inJapanese).Benjamin Mandel, Aron Culotta, John Boulahanis,Danielle Stark, Bonnie Lewis, and Jeremy Rodrigue.2012.
A demographic analysis of online sentimentduring Hurricane Irene.
In Proceedings of the Sec-ond Workshop on Language Analysis in Social Me-dia (LASM 2012), pages 27?36.Robert Munro.
2011.
Subword and spatiotempo-ral models for identifying actionable information inHaitian Kreyol.
In Proceedings of the FifteenthConference on Computational Natural LanguageLearning (CoNLL-2011), pages 68?77.Robert Munro.
2013.
Crowdsourcing and thecrisis-affected community.
Information Retrieval,16(2):210?266.Tetsuji Nakagawa, Kentaro Inui, and Sadao Kurohashi.2010.
Dependency tree-based sentiment classifica-tion using CRFs with hidden variables.
In HumanLanguage Technologies: The 2010 Annual Confer-ence of the North American Chapter of the Associ-ation for Computational Linguistics (NAACL HLT2010), pages 786?794.National Police Agency of Japan.
2013.
Damage sit-uation and public countermeasures associated with2011 Tohoku district ?
off the Pacific Ocean Earth-quake.
http://www.npa.go.jp/archive/keibi/biki/higaijokyo_e.pdf.
(accessedon 30 April, 2013).Graham Neubig, Yuichiroh Matsubayashi, MasatoHagiwara, and Koji Murakami.
2011.
Safety infor-mation mining?
what can NLP do in a disaster?.In Proceedings of the 5th International Joint Con-ference on Natural Language Processing (IJCNLP2011), pages 965?973.Jong-Hoon Oh, Kentaro Torisawa, Chikara Hashimoto,Takuya Kawada, Stijn De Saeger, Jun?ichi Kazama,and Yiou Wang.
2012.
Why question answeringusing sentiment analysis and word classes.
In Pro-ceedings of the 2012 Joint Conference on EmpiricalMethods in Natural Language Processing and Com-putational Natural Language Learning (EMNLP-CoNLL 2012), pages 368?378.1628Jong-Hoon Oh, Kentaro Torisawa, Chikara Hashimoto,Motoki Sano, Stijn De Saeger, and Kiyonori Ohtake.2013.
Why-question answering using intra- andinter-sentential causal relations.
In Proceedings ofthe 51st Annual Meeting of the Association for Com-putational Linguistics (ACL 2013).Kiyonori Ohtake, Kentaro Torisawa, Jun Goto, andStijn De Saeger.
2013.
Saigaiji ni okeru hi-saisha to kyuuen kyuujosha kan no souhoko komyu-nikeeshon.
(Bi-directional communication betweenvictims and rescures during a crisis).
In Proceedingof the Nineteenth Annual Meeting of The Associationfor Natural Language Processing.
(in Japanese).Makoto Okazaki and Yutaka Matsuo.
2010.
Seman-tic Twitter: Analyzing tweets for real-time eventnotification.
In Proceedings of the 2008/2009 in-ternational conference on Social software: Re-cent trends and developments in social software(BlogTalk 2008), pages 63?74.R.
Lyman Ott and Michael T. Longnecker, 2010.
AnIntroduction to Statistical Methods and Data Analy-sis, chapter 10.2.
Brooks Cole, 6th edition.Yan Qu, Philip Fei Wu, and Xiaoqing Wang.
2009.Online community response to major disaster: Astudy of Tianya forum in the 2008 Sichuan Earth-quake.
In 42st Hawaii International InternationalConference on Systems Science (HICSS-42), pages1?11.Preethi Raghavan, Rose Catherine, Shajith Ikbal,Nanda Kambhatla, and Debapriyo Majumdar.
2010.Extracting problem and resolution information fromonline discussion forums.
In Proceedings of the16th International Conference on Management ofData (COMAD 2010).Takeo Saijo.
2012.
Hito-o tasukeru sungoi shikumi.
(Astunning system that saves people).
Diamond Inc.(in Japanese).Takeshi Sakaki, Makoto Okazaki, and Yutaka Matsuo.2010.
Earthquake shakes Twitter users: Real-timeevent detection by social sensors.
In Proceedingsof the 19th International Conference on World WideWeb (WWW 2010), pages 851?860.Motoki Sano, Istva?n Varga, Jun?ichi Kazama, and Ken-taro Torisawa.
2012.
Requests in tweets dur-ing a crisis: A systemic functional analysis oftweets on the Great East Japan Earthquake andthe Fukushima Daiichi nuclear disaster.
In Pa-pers from the 39th International Systemic Func-tional Congress (ISFC39), pages 135?140.Haruyuki Seki.
2011.
Higashi-nihon daishinsai fukkoushien platform sinsai.info no naritachi to kongo nokadai.
(The organizational structure of sinsai.inforestoration support platform for the 2011 Great EastJapan Earthquake and future challenges).
Journal ofdigital practices, 2(4):237?241.
(in Japanese).Kate Starbird, Leysia Palen, Amanda L. Hughes, andSarah Vieweg.
2010.
Chatter on the red: Whathazards threat reveals about the social life of mi-croblogged information.
In Proceedings of The2010 ACM Conference on Computer Supported Co-operative Work (CSCW 2010), pages 241?250.Cristen Torrey, Moira Burke, Matthew L. Lee,Anind K. Dey, Susan R. Fussell, and Sara B. Kiesler.2007.
Connected giving: Ordinary people coordi-nating disaster relief on the Internet.
In Proceedingsof the 40th Annual Hawaii International Conferenceon System Sciences (HICSS-40), pages 179?188.Katerina Tsagkalidou, Vassiliki Koutsonikola, AthenaVakali, and Konstantinos Kafetsios.
2011.
Emo-tional aware clustering on micro-blogging sources.In Proceedings of the 4th international conferenceon Affective computing and intelligent interaction(ACII 2011), pages 387?396.Sarah Vieweg, Amanda L. Hughes, Kate Starbird, andLeysia Palen.
2010.
Microblogging during two nat-ural hazards events: What Twitter may contributeto situational awareness.
In Proceedings of theSIGCHI Conference on Human Factors in Comput-ing Systems (CHI 2010), pages 1079?1088.Patrick Winn.
2011.
Japan tsunami disaster: As Japanscrambles, Twitter reigns.
GlobalPost, 18 March.1629
