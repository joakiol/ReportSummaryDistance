Mixed-Initiative Development ofLanguage Processing SystemsDavid Day, John Aberdeen, Lynette Hirschman,Robyn Kozierok, Patricia Robinson and Marc VilainAdvanced Information Systems CenterThe MITRE Corporation202 Burlington RoadBedford, Massachusetts 01730 U.S.A.{ day,aberdeen,lynette } @mitre.org{robyn,parann,mbv } @mitre.orgAbstractHistorically, tailoring language processing systems tospecific domains and languages for which they were notoriginally built has required a great deal of effort.Recent advances in corpus-based manual and automatictraining methods have shown promise in reducing thetime and cost of this porting process.
Thesedevelopments have focused even greater attention on thebottleneck of acquiring reliable, manually taggedtraining data.
This paper describes a new set ofintegrated tools, collectively called the AlembicWorkbench, that uses a mixed-initiative approach to"bootstrapping" the manual tagging process, with thegoal of reducing the overhead associated with corpusdevelopment.
Initial empirical studies using theAlembic Workbench to annotate "named entities"demonstrates that this approach can approximatelydouble the production rate.
As an ~ benefit, thecombined efforts of machine and user produce domain-specific annotation rules that can be used to annotatesimilar texts automatically through the Alembic NLPsystem.
The ultimate goal of this project is to enableend users to generate a practical domain-specificinformation extraction system within a single session.1.
In t roduct ionIn the absence of complete and deep text understanding,implementing information extraction systems remains adelicate balance between general theories of languageprocessing and domain-specific heuristics.
Recentdevelopments in the area of corpus-based languageprocessing systems indicate that the successfulapplication of any system to a new task depends to avery large extent on the careful and frequent evaluationof the evolving system against training and test corpora.This has focused increased attention on the importanceof obtaining reliable training corpora.
Unfortunately,acquiring such data has usually been a labor-intensiveand time-consuming exercise.The goal of the Alembic Workbench is to dramaticallyaccelerate the process by which language processingsystems are tailored to perform new tasks.
Thephilosophy motivating our work is to maximally reuseand re-apply every kernel of knowledge available at eachstep of the tailoring process.
In particular, our approachapplies a bootstrapping procedure to the development ofthe training corpus itself.
By re-investing theknowledge available in the earliest raining data to pre-tag subsequent un-tagged ata, the Alembic Workbenchcan tralasform the process of manual tagging to onedominated by manual review.
In the limit, if the pre-tagging process performs well enough, it becomes thedomain-specific automatic tagging procedure itself, andcan be applied to those new documents from whichinformation is to be extracted.As we and others in the information extraction arenahave noticed, the quality of text processing heuristics isinfluenced critically not only by the power of one'slinguistic theory, but also by the ability to evaluatethose theories quickly and reliably.
Therefore, buildingnew information extraction systems requires anintegrated environment that supports: (1) thedevelopment of a domain-specific annotated corpus; (2)the multi-faceted analysis of that corpus; (3) the abilityto quickly generate hypotheses as to how to extract ortag information in that corpus; and (4) the ability toquickly evaluate and analyze the performance of thosehypotheses.
The Alembic Workbench is our attempt tobuild such an environment.As the Message Understanding Conferences move intotheir tenth year, we have seen a growing recognition ofthe value of balanced evaluations against acommon testcorpus.
What is unique in our approach is to integratesystem development with the corpus annotation processitself.
The early indications are that at the very leastthis integration can significantly increase theproductivity of the corpus annotator.
We believe thatthe benefits will flow in the other direction as well, andthat a concomitant increase in system performance willfollow as one applies the same mixed-initiativedevelopment environment to the problem of domain-specific tailoring of the language processing system.348Figure 1.
Screen dump of a typical Alembic Workbench session.2.
Alembic Workbench: A brief descriptionThe Alembic Workbench provides a graphical userinterface by which texts can be annotated using themouse and user-defined key bindings.
The Workbenchmouse interface is engineered specifically to minimizehand motion.
This allows text markup to proceed veryquickly.
Once a text has been marked up, the user'sannotations are highlighted in colors specified by theuser.
A "mouse line" at the bottom of the text windowprovides further visual feedback indicating all of theannotations associated with the location under themouse cursor, including document structure markup, ifavailable.
An example screen image from a typicalsession with the Workbench is shown above.Our focus in building the Alembic Workbench is toprovide a natural but powerful environment forannotating texts in the service of developing naturallanguage processing systems.
To this end we haveincorporated a growing number of analysis and reportingfeatures.
The current set of utilities includes:349?
A string-matching mechanism that canautomatically replicate new markup to identicalinstances elsewhere in the document.?
A rule language for constructing task-specificphrase tagging and/or p e-tagging rule sets.?
A tool that generates phrase-based KWlC ("key-word in context") reports to help the user identifycommon patterns in the markup.?
A procedure that generates word lists based on theirfrequency.
This tool also measures the degree towhich a word occurs in different markup contexts.?
A visualization component for viewing inter-annotator (or key/answer) agreement.?
A scorer that allows arbitrary SGML markup to beselected for scoring.?
A full-featured interface to the multi-stagearchitecture of the Alembic text processing system.?
An interface to Alembic's phrase-rule learningsystem for generating new application-specific rulesets.?
The Alembic Workbench also provides pecializedinterfaces for supporting more complex, linkedmarkup such as that needed for coreference.
Anotherinterface is geared towards capturing arbitrary n-aryrelations between tagged elements in a text (thesehave been called "Scenario Templates" in MUC).More details about the implementation of theWorkbench are provided in Section 7.The development of the Alembic Workbench environ-ment came about as a result of MYrRE's efforts atrefining and modifying our natural anguage processingsystem, Alembic \[1,7\], to new tasks: the MessageUnderstanding Conferences (MUC5 and MUC6), and theTIPSTER Multi-lingual Entity Task (MET1).
(See \[6\]for an overview and history of MUC6 and the ''NamedEntity Task".)
The Alembic text processing systemapplies Eric Brill's notion of ru/e sequences \[2,3\] atalmost every one of its processing stages, from part-of-speech tagging to phrase tagging, and even to someportions of semantic interpretation a d inference.While its name indicates its lineage, we do not view theAlembic Workbench as wetkkxt o the Alembic textprocessing system alone.
We intend to provide a well-documented API in the near future for external utilitiesto be incorporated smoothly into the corpus/systemdevelopment environment.
We envision two classes ofexternal utilities: tagging utilities and analysis utilities.By integrating other tagging modules (includingcomplete NLP systems), we hope those systems can bemore efficiently customized when the cycle of analysis,hypothesis generation and testing is tightened into awell-integrated loop.
The current version of the toolsupports viewing, annotating and analyzing documentsin 7-bit, 8-bit and 2-byte character sets.
Currentsupport includes the Latin-1 languages, Japanese (JIS),Chinese (GB1232), Russian, Greek and Thai.3.
Increasing manual annotation productivitythrough pre-taggingA motivating idea in the design of the AlembicWorkbench is to apply any available information asearly and as often as possible to reduce the burden ofmanual tagging.
In addition to careful interface designand support for user-customization, a core mechanismfor enhancing this process is through pre-tagging.The generation of reliably tagged text corpora requiresthat a human annotator ead and certify all of theannotations applied to a document.
This is especiallytrue if the annotations are to be used for subsequentmanual or automatic training procedures.
However,much of the drudgery of this process can be removed ifthe most obvious and/or oft-repeated expressions can betagged prior to the annotator's efforts.
One way ofdoing this is to apply tags to any and all strings in adocument that match a given string.
This is the natureof the "auto-tagging ~' facility built-in to the Workbenchinterface.
For example, in annotating journalisticdocument collections with "Named Entity" tags, onemight want to simply pre-tag every occurrence of"President Clinton" with Person.. ~ Of course, theseactions should be taken with some care, since mis-tagging entities throughout a document might actuallylead to an increase in effort required to accurately fix orremove tags in the document.A more powerful approach is to allow patterns, or rules,to form the basis for this pre-tagging.
The Alembicphrase-rule interpreter provides the basis for developingrule-based pre-tagging heuristics in the Workbench.
Inthe current version of the Workbench, the user is free tocompose these "phraser" rules and group them intospecialized rule sets.
Figure 2 shows an examplesequence of rules that could be composed for pre-tagginga corpus with Person tags.
The Brill control regimeinterprets these rules strictly sequentially: rule n isapplied wherever in the text it can be; it is thendiscarded and rule n+l is consulted.
There is nounconstrained forward chaining using a "soup" of rulesas in a standard production (or rule-based) system.
TheAlembic "phraser" rule interpreter has been applied totagging named entities, sentence chunks, simple entityrelations ("template element" in the parlance of MUC6),and other varieties of phrases.
(def-phraser-rule:anchor :lexeme:conditions (:left-1 :lex (=Mr."
=Ms."
"Dr." ...)):actions (:create-phrase :person))(def-phraser-rule:conditions (:phrase :phrase-label :person)(:right-1 :pos :NNP):actions (:expand :right-I))Figure 2.
An example Alembic role sequence that (1)produces Person phrases around any word immediately tothe fight of a title and/or honorific, and then (2) grows theextent of the phrase to the fight one lexeme, if that word i sa proper noun.4.
Mixed-initiative text annotationIn addition to allowing users to define pre-tagging rules,we have developed a learning procedure that can be usedto induce these rules from small training corpora.Operationally, an annotator starts by generating a smallinitial corpus and then invokes the learner to derive a setof pre-tagging rules.
These rules can then be applied tonew, unseen texts to pre-tag them.
Figure 3 illustratesthis bootstrapping cycle.i The Named Entity task from MUC6 consists of addingtags to indicate expressions of type Person, Location,Organization, Date, Time and Money, see \[6\].350The earlier we can extract heuristic rules on the basis ofmanually tagged ata, the earlier the user can be relievedfrom some portion of the chore of physically markingup the text--the user will need to edit and/or add only afraction of the total phrases in a given document.
Inour experience of applying the Alembic phrase rulelearner to named-entity and similar problems, our error-reduction learning method requires only modest amountsof training data.
(We present performance details inSection 6.
)Unprocessed materialIflcB$Training)Testing corporaFigure 3.
The Alembic Workbench seeks to involvethe user in a corpus development cycle, making use ofpre-tagging facilities, analysis facilities, and theautomatic generation of pre-tagging rule sets throughmachine learning.As the human annotator continues generating reliabletraining data, she may, at convenient intervals, re-invoke the learning process.
As the amount of trainingdata increases, the performance of the learned rules tendsto increase, and so the amount of labor saved in pre-tagging subsequent training data is further increased.The bootstrapping effect tends to increase over time.For the "named entity" task in MUC6 approximately25,000 words were provided as annotated training databy the conference organizers ("formal training" and"dryrun" data sets).
Prior to developing the AlembicWorkbench, we were able to use this amount of data inAlembic to generate a system performing at 85.2 P&Ron unseen test data.
2 Based on the tagging rates wehave measured thus far using the Workbench, it wouldtake somewhere between 1.5 to 2.5 hours to tag these25,000 words of data.There is a limit on how much one can reduce the time-requirements for generating reliable training data--thisis the rate required by a human domain expert tocarefully read and edit a perfectly pre-annotated trainingcorpus.
Training data cannot be generated without this2 P&R (or F-measure) is a weighted combination of recalland precision.human investment.
3 Indeed, in situations where thequality of the data is particularly important (as it is in,say a multi-system evaluation such as MUC), it istypical that multiple reviews of the same corpus isperformed by various annotators, especially given theknown ambiguity of any annotation task definition.5.
Manual refinement of automaticallyderived pre-tagging heuristicsIn the previous section we presented our approach tomixed-initiative corpus development and taggingheuristics without assuming any sophistication on thepart of the human user beyond a clear understanding ofthe information extraction task being addressed.Usually, however, even a lay end-user is likely to havea number of intuitions about how the un-annotated datacould be pre-tagged to reduce the burden of manualtagging.
Hand-coded rules can be applied in concertwith the machine-derived rules mentioned earlier.
Oneway this can be done is by invoking the rule learningsubsequent to the application of the hand-cxxted pre-tagging rules.
On the other hand, if the user notices aconsistent mistake being made by the machine-learnedrules early in the bootstrapping process, the user canaugment the machine-derived rule sequence withmanually composed rules.
In fact, every rule composedby the learning procedure is completely inspectable bythe user, and so some users may want to modifyindividual machine-derived rules, perhaps to expand theirgenerality beyond the particular data available in theemerging corpus.This is another way, then, that the Alembic Workbenchenvironment enables and encourages the mixed, orcooperative, application of human and machine skills tothe combined task of developing a domain-specificcorpus and set of extraction heuristics.Of course, composing rules is somewhat akin toprogramming, and not all users will be inclined, orwell-equipped, tobecome involved in this process.
Oneimpediment to end-users composing their own rules isthe particular syntax of Alembic's phraser ules, so weanticipate xploring other, simpler rule languages thatwill encourage end-user participation.
Another approachthat we are interested in exploring involves supportingmore indirect feedback or directives from the user thatare rooted more closely to examples in the data.3 This is not to say that high-quality machine-tagged datacannot be generated faster than this, and that these datamay indeed be helpful in the learning procedure of someother systems.
But all such data will remain suspect as faras being considered part of an annotated training corpusuntil inspected by a human, given the vagaries of genre andstyle that can easily foil the most sophisticated systems.351Similarities and differences between manualand automatic rule formationThe automatic rule-learuing procedure uses a generate-and-test approach to learn a sequence of rules.
A set ofrule schemata, defining a set of possible rule instancesdetermines the rule space that the learning procedureexplores.
The learner uses indexing based on the actualdata present in the corpus to help it explore the rulespace efficiently.
The learning process is initiated byderiving and applying an initial labeling function basedon the differences between an un-annotated version and acorrectly annotated version of the corpus.
Then, duringeach learning cycle, the learner tries out applicable ruleinstances and selects the rule that most improves thescore when applied to the corpus.
The score isdetermined by evaluating the corpus as currentlyannotated against he correctly annotated version, usingsome evaluation function (generally precision, recall orF-measure).
The corpus annotation is updated byapplying the chosen rule, and the learning cycle repeats.This cycle is continued until a stopping criterion isre, ached, which is usually defined as the point whereperformance improvement falls below a threshold, orceases.
Other alternatives include setting a strict limiton the number of rules, and testing the performanceimprovement of a rule on a corpus distinct from thetraining set.Of course, there are two important advantages that ahuman expert might have over the machine algorithm:linguistic intuition and world knowledge.
Rules thatinclude references toa single lexeme can be expanded tomore general applicability by the human expert who isable to predict alternatives that lie outside the currentcorpus available to the machine.
By supportingmultiple ways in which rules can be hypothesized,refined and tested, the strengths of both sources ofknowledge can be brought to bear.6.
Experimental ResultsWe are still in the early stages of evaluating theperformance ofthe Alembic Workbench along a numberof different dimensions.
However, the results from earlyexperiments are encouraging.
Figure 4 compares theproductivity rates using different corpus developmentutilities.
These are indicated by the four categories onthe X-axis: (1) using SGML-mode in emacs (by anexpert user); (2) using the Workbench interface and"auto-tag" string-matching utility only; (3) using theWorkbench following the application of learned taggingrules derived from 5 short documents--approximately1,500 words, and (4) using the Workbench followingthe application of learned tagging rules again, but thistime with the learned rules having trained on 100documents (approximately 48,000 words), instead ofonly five documents.As can be seen in these experiments, there is a clearincrease in the productivity as a function of both theuser interface (second column) and the application ofpre-tagging rules (third and fourth columns).
The largestep in performance between columns three and fourindicate that repeated invocation of the learning processduring the intermediate stages of the corpusdevelopment cycle will likely result in acceleration ofthe annotation rate.
(As it happens, these results areprobably underestimating the pre-tagging productivity.The reason for this is that the version of the Workbenchused was not yet able to incorporate date and timeannotations generated by a separate pre-processing step;this date and time tagger performs at an extremely highlevel of precision for this genre---in the high ninetiesP&R.)
These initial experiments involved a singleexpert annotator on a single tagging task (MUC6 namedentity).
The annotator was very familiar with thetagging task.28026O240160140120IO0Productivity Gains.,iCorpus Development Tools UsedI I I  Words/Minute ?
Tags/Minute IFigure 4.
Two measures of corpus annotationproductivity using the Alembic Workbench.
The X-axisindicates what kind of corpus-development utilities wereused: (1) SGMl.,-mode of emacs text editor; (2) Workbench(AWB) manual interface only, (3) AWB rule-learningbootstrap method with 5-document training set; (4) AWBrule-learning bootstrap method with 100-documenttrainin\[ set.
See discussion in text.To place this in the perspective of the human annotator,after only about 15 minutes of named entity tagging,having annotated some 1,500 words of text withapproximately 150 phrases, the phrase rule learner canderive heuristic rules that produce a pre-taggingperformance rate (P&R) of between 50 and 60 percent.Of course, this performance is far short of what isneeded for a practical extraction system, but it alreadyconstitutes a major source for labor savings, since50 to 60 percent of the annotations that need to bemoused (or clicked) in are already there.
Since theprecision at this early stage is only around 60 percent,there will be extra phrases that need (1) to be removed,(2) their assigned category changed (from, say,352organization to person), or (3) their boundaries adjusted.It turns out that for the first two of these kinds ofprecision errors, the manual corrections are extremelyquick to perform.
(Boundaries are not really difficult tomodify, but the time required is approximately the sameas inserting a tag from scratch.)
In addition, makingthese corrections removes both a precision and a recallerror at the same time.
Therefore, it turns out that evenat this very early stage, the modest pre-taggingperformance gained from applying the learningprocedure provides measurable performanceimprovement.In order to obtain more detailed results on the effect ofpre-tagging corpora, we conducted another experiment inwhich we made direct use of the iterative automaticgeneration of rules from a growing manually-taggedcorpus.
Using the same skilled annotator, weinlroduced a completely new corpus for which named-entity tagging happened to be needed within ourcompany.
We randomly divided approximately 50documents of varying sizes into five groups.
The wordcounts for these five groups were: Groupl: 19,300;Group2: 13,800; Group3: 6,3130; Group4: 15,800;Group5: 8,000; for a total of 63,000 words.
Aftermanually tagging the first group, we invoked the rulelearning procedure.
Applying the learning procedure oneach training set required two to three hours of elapsedtime on a Sun Sparc Ultra.
The new tagging rules werethen applied to the next ten documents prior to beingmanually tagged/edited.
This enlarged corpus was thenused to derive a new rule set to be applied to the nextgroup of documents, and so on.
A summarization ofthe results are presented in Figure 5.Clearly, more experiments are called for we plan toconduct hese across different annotators, task types, andlanguages, to better evaluate productivity, quality andother aspects of the annotation process.It is extremely difficult to control many of the featuresthat influence the annotation process, such as theintrinsic complexity of the topic in a particulardocument, he variation in tag-density (tags per word)that may occur, the user's own training effect as thestructure and content of documents become morefamiliar, office distractions, etc.
In order to gain abetter understanding of the underlying taggingperformance of the rule learner, and so separate outsome of these human factors issues, we ran anautomated experiment in which different random subsetsof sentences were used to train rule sets, which werethen evaluated on a static test corpus.
The resultsshown in Figure 6 give some indication of the abilityof the rule-sequence l arning procedure to glean usefulgeneralizations from meager amounts of training data.Performance of Learned Rule Set as aFunction of Training Set S ize; 80  T ~ ,,,-c 60 -tt,  f ,~ l l l l ' J  ,~t  - -.
.
, , ;  r F'measurelm 30 ~,  ~-- .
.
.
.
Precision | .s 20O 1 0 ~Training Set Size (Named Ent i t les )30Productivity by Group20151050 I p l1 2 3 4GroupI Tag /Minute  IFigure 5.
Tagging productivity gains with theincremental pplication of automatically acquired rule sets.The first observation we make is that there is a clear andobvious direction of improvement--by the time 30documents have been tagged, the annotation rate onGroup 4 has increased considerably.
It is important tonote, however, that there is still noise in the curve.
Inaddition, the granularity is perhaps till too coarse tomeasure the incremental influences of pre-tagging rules.Figure 6.
Performance of learned miss on independenttest set of 662 sentences.Average Performance of Learned Rules asa Function of Training Set Size80 ?= 70~"  ~ - -  - -  - * ,co4?
?-=;4o J- E.3o ~-'~ 2010 , , :  : : :  : : : :  ' : : :  : : : : : : : :  : : :  :Training EntitlesFigure 7.
Averaged F-measure performance figures.One clear effect of increasing training set size is areduction in the sensitivity of the learning procedure toparticular training sets.
We hypothesize that this effectis partly indicative of the generalization behavior onwhich the learning procedure is based, which amplifies353the effects of choosing more or less representativetraining sentences by chance.
Since the learningprocess is not merely memorizing phrases, butgenerating contextual rules to try to predict phrase typesand extents, the rules are very sensitive to extremelysmall selections of training sentences.
Figure 7 showsthe F-measure performance smoothed by averagingneighboring data points, to get a clearer picture of thegeneral tendency.We should note that the Alembic Workbench, havingbeen developed only recently in our laboratory, was notavailable to us in the course of our effort to apply theAlembic system to the MUC6 and MET tasks.Therefore we have not been able to measure itsinfluence in preparing for a particular new textprocessing task.
We intend to use the system to preparefor future evaluations (including MUC7 and MET2) andto carefully evaluate the Alembic Workbench as anenvironment for the mixed-initiative development ofinformation extraction systems in multiple languages.7.
ImplementationThe Alembic Workbench interface has been written inTci/Tk.
Some of the analysis and reporting utilities(available from within the interface as well as Unixcommand-line utilities) are written in Perl, C or Lisp.The separate Alembic NLP system consists of C pre-processing taggers (for dates, word and sentencetokenizafion and part-of-speech assignments) and a Lispimage that incorporates the rest of Alembic: the phrase-rule interpreter, the phrase rule learner, and a number ofdiscourse-level inference mechanisms described in \[8\].This code currently runs on Sun workstations runningSun OS 4.1.3 and Solaris 2.4 (Sun OS 5.4) and greater;we have begun porting the system to WindowsNT/Windows 95.
We anticipate providing an API forintegrating other NLP systems in the near future.The Workbench reads and saves its work in the form ofSGML-encoded files, though the original document neednot contain any SGML mark-up at all.
These files meparsed with the help of an SGML normalizer.
4 Duringthe course of the annotation process the Workbenchuses a "Parallel Tag File" (PTF) format, whichseparates out the embedded annotations from the sourcetext, and organizes user-defined sets of annotationswithin distinct "tag files."
While these files aregenerally hidden from the user, they provide a basis forthe combination and separation of document annotations("tagsets") without needing to modify or otherwisedisturb the base document.
This allows the user to view4 In cases where documents use some of the more complexaspects of SGML, the user supplies a Document TypeDescription (DTD) file for use in normalization.
For simpleSGML documents, or documents with no original SGMLmarkup at all, no DTD needs to be specified.only Named Entity tags, or only tokenization tags, orany desired subset of tagsets.
Thus, the Workbench iswritten to be TIPSTER-compliant, though it is notitself a document manager as envisioned by thatarchitecture (see \[5\]).
We anticipate integrating theWorkbench with other TIPSTER compliant modulesand document managers via the exchange of SGML-formatted ocuments.
The Parallel Tag File (PTF)format used by the Workbench provides another meansby which a translator could be written.8.
Future WorkBroadly defined, there are two distinct types of userswho we imagine will find the Workbench useful: NLPresearchers and information extraction system end-users.While our dominant focus so far has been on supportingthe language research community, it is important oremember that new domains for language processinggenerally, and information extraction in particular, willhave their own domain experts, and we want the textannotation aspects of the tool to be quite usable by awide population.
In this vein we would like to enablevirtually any user to be able to compose new patterns(rules) for performing pre-tagging on the data.
Whilethe current rule language has a simple syntax, as well asan extremely simple control regimen, we do notimagine all users will want to engage directly in anexploration for pre-tagging rules.
A goal for our futureresearch is to explore new methods for incorporatingend-user feedback to the learning procedure.
Thisfeedback might include modifying a very simplifiedform of a single rule for greater generality byintegrating thesauri to construct word-list suggestions.We also would like to give users immediate feedback asto how a single rule applies (correctly and incorrectly)to many different phrases in the corpus.In this paper we have concentrated on the named entitytask as a generic ase of corpus annotation.
Of course,there are many different ways in which corpora re beingannotated for many different tasks.
Some of the specificextensions to the user interface that we have alreadybegun building include part-of-speech tagging (and"dense" markup more generally), and full parse syntactictagging (where we believe reliable training data can beobtained much more quickly than heretofore).
In theseand other instances the tagging process can beaccelerated by applying partial knowledge early on,transforming the task once again into that of editing andcorrecting.
Most of these tagging tasks would beimproved by making use of methods that preferentiallyselect ambiguous data for manual annotation--forexample, as described in \[4\].There are a number of psychological nd human factorsissues that arise when one considers how the pre-annotated data in a mixed-initiative system may affect354the human editing or post-processing.
If the pre-tagging process has a relatively high recall, then wehypothesize that the human will tend increasingly totrust he pre-annotations, and thereby forget o read thetexts carefully to discover any phrases that escapedbeing annotated.
A similar effect seems possible forrelatively high precision systems, though properinterface design (to highlight the type assigned to aparticular phrase) should be able to mitigate thesetendencies.
A more subtle interaction is "theory creep,"where the heuristics induced by the machine learningcomponent begin to be adopted by the human annotator,due, in many cases, to the intrinsic ambiguity ofdefining annotation tasks in the first place.
In all ofthese cases the most reliable method for detecting thesehuman/machine interactions i probably to use somerepresentative sub-population of the corpus documentsto measure and analyze the inter-annotator agreementbetween human annotators who have and who have notbeen exposed to the machine derived heuristics forassigning annotations.9.
Conc lus ionsOn the basis of observing our own and others'experiences in building and porting natural anguagesystems for new domains, we have come to appreciatethe pivotal role played in continuous evaluationthroughout he system development cycle.
Butevaluation rests on an oracle, and for text processing,that oracle is the training and test corpora for aparticular task.
This has led us to develop a tailoringenvironment which focuses all of the availableknowledge on accelerating the corpus developmentprocess.
The very same learning procedure that is usedto bootstrap the manual tagging process leadseventually to the derivation of tagging heuristics thatcan be applied in the operational setting to unseendocuments.
Rules derived manually, automatically, andthrough a combination of efforts have been appliedsuccessfully in a variety of languages, includingEnglish, Spanish, Portuguese, Japanese and Chinese.The tailoring environment, known as the AlembicWorkbench, has been built and used within ourorganization, and we are making it available to otherorganizations involved in the development of languageprocessing systems and/or annotated corpora.
Initialexperiments indicate an significant improvement in therate at which annotated corpora can be generated usingthe Alembic Workbench methodology.
Earlier workhas shown that with the training dat~ obtained in thecourse of only a couple of hours of text annotation, aninformation extraction system can be induced purelyautomatically that achieves a very competitive l vel ofperformance.References\[1\] John Aberdeen, John Burger, David Day, LynetteHirschman, David Palmer, Palricia Robinson, and MarcVilain.
1996.
The Alembic system as used in MET.
InProceedings of the TIPSTER 24 Month Workshop,May.\[2\] Eric Brill.
1992.
A simple rule-based part of speechtagger.
In Proceedings of the Third Conference onApplied Natural Language Processing, Trento.\[3\] Eric Brill.
1993.
A Corpus-Based Approach toLanguage Learning.
Ph.D. thesis, University ofPennsylvania, Philadelphia, Penn.\[4\] Scan P. Engelson and Ido Dagan.
1996.
Minimizingmanual annotation cost in supervised training fromcorpora.
Computation and Linguistic E-Print Service(cmp-lg/9606030), June.\[5\] Ralph Grishman.
1995.
TIPSTER phase IIarchitecture design.
Worm Wide Web document.URL=http:llcs.nyu.edtdcslfacultylgfishman/tipster.html\[6\] Ralph Grishman and Beth Sundheim.
1996.Message Understanding Conference----6: A BriefHistory.
In International Conference on ComputationalLinguistics, Copenhagen, Denmark, August.
TheInternational Committee on Computational Linguistics.\[7\] Marc Vilain and David Day.
1996.
Finite-stateparsing by rule sequences.
In International Conferenceon Computational Linguistics, Copenhagen, Denmark,August.
The International Committee onComputational Linguistics.\[8\] Marc Vilain.
1993.
Validation of terminologicalinference in an information extraction task.
InProceedings of the ARPA Workshop on HumanLanguage Technology, Plainsboro, New Jersey.355
