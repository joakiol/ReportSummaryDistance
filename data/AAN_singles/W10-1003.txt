Proceedings of the NAACL HLT 2010 Fifth Workshop on Innovative Use of NLP for Building Educational Applications, pages 19?27,Los Angeles, California, June 2010. c?2010 Association for Computational LinguisticsAutoLearn?s authoring tool: a piece of cake for teachersMart?
Quixal1, Susanne Preu?3, David Garc?a-Narbona2, Jose R. Boullosa21 Voice and Language Group,2 Advanced Development GroupBarcelona Media Centre d?Innovaci?Diagonal, 177, E-08018 Barcelona, Spain{marti.quixal,david.garcian,beto.boullosa}@barcelonamedia.org3 GFAIMartin-Luther-Str.
14Saarbr?cken, Germanysusannep@iai.uni-sb.deAbstractThis paper1 presents AutoLearn?s authoringtool: AutoTutor, a software solution that en-ables teachers (content creators) to developlanguage learning activities including auto-matic feedback generation without the need ofbeing a programmer.
The software has beendesigned and implemented on the basis ofprocessing pipelines developed in previouswork.
A group of teachers has been trained touse the technology and the accompanyingmethodology, and has used materials createdby them in their courses in real instruction set-tings, which served as an initial evaluation.The paper is structured in four sections: Sec-tion 1 introduces and contextualizes the re-search work.
Section 2 describes the solution,its architecture and its components, and spe-cifically the way the NLP resources are cre-ated automatically with teacher input.
Section3 describes and analyses a case study usingthe tool to create and test a language learningactivity.
Finally Section 4 concludes with re-marks on the work done and connections torelated work, and with future work.1 IntroductionOver the past four decades there have been severalhundreds of CALL (Computer-Aided LanguageLearning) projects, often linked to CALL practice(Levy 1997), and within the last twenty years aconsiderable number of them focused on the use of1Research funded by the Lifelong Learning Programme 2007-2013 (AUTOLEARN, 2007-3625/001-001).NLP in the context of CALL (Amaral and Meur-ers, in preparation).
Despite this, there is an appall-ing absence of parser-based CALL in realinstruction settings, which has been partially at-tributed to a certain negligence of the pedagogicalneeds (Amaral and Meurers, in preparation).
Incontrast, projects and systems that were pedagogi-cally informed succeeded, yielded and are yieldinginteresting results, and are evolving for over a dec-ade now (Nagata 2002; Nagata 2009; Heift 2001;Heift 2003; Heift 2005; Amaral and Meurers, inpreparation).
According to Amaral and Meurerssuccessful projects were able to restrict learnerproduction in terms of NLP complexity by limitingthe scope of the learning activities to language-oriented (as opposed to communicative-oriented)or translation exercises, or by providing feedbackon formal aspects of language in content orientedactivities, always under pedagogical considerations?focus on form.Our proposal is a step forward in this directionin two ways: a) it allows for feedback generationfocusing both on formal and content (communica-tive-oriented) aspects of language learning activi-ties, and b) it provides teachers with a tool and amethodology ?both evolving?
for them to gainautonomy in the creation of parser-based CALLactivities ?which by the way has a long tradition inCALL (Levy 1997, chap.
2).
The goal is to shapelanguage technologies to the needs of the teachers,and truly ready-to-hand.1.1 Related work and research contextThe extent to which pedagogues appreciate andrequire autonomy in the design and creation ofCALL activities can be traced in the historical19overview offered by (Levy 1997, 16, 17, 19, 23and 38).
Moreover, parallel research shows that theintegration of CALL in the learning context iscritical to ensure the success of whatever materialsare offered to learners (Levy 1997, 200-203; Polis-ca 2006).AutoTutor goes beyond tools such as Hot Pota-toes, eXelearning or JClic2 in that it offers the pos-sibility of authoring NLP-based CALL activities.
Itis also more ambitious than other authoring toolsdeveloped for the creation of activities in intelli-gent tutoring systems.
Chen and Tokuda (2003)and R?sener (2009) present authoring tools fortranslation exercises, where expected learner inputis much more controlled (by the sentence in thesource language).Heift and Toole (2002) present Tutor Assistant,which enables to create activities such as build-a-sentence, drag-and-drop and fill-in-the-blank.
Animportant difference between AutoTutor and TutorAssistant is that the latter is a bit more restrictive interms of the linguistic objects that can be used.
Italso presents a lighter complexity in the modellingof the underlying correction modules.
However,the system underlying Tutor Assistant provideswith more complex student adaptation functional-ities (Heift 2003) and would be complementary interms of overall system functionalities.2http://hotpot.uvic.ca/, http://sourceforge.net/apps/trac/exe/wiki,http://clic.xtec.cat/es/jclic/index.htm.2 AutoTutor: AutoLearn?s authoringsoftwareAutoTutor is a web-based software solution toassist non-NLP experts in the creation of languagelearning activities using NLP-intensive processingtechniques.
The process includes a simplifiedspecification of the means to automatically createthe resources used to analyse learner input for eachexercise.
The goal is to use computational devicesto analyse learner production and to be able to gobeyond ?yes-or-no?
answers providing additionalfeedback focused both on form and content.This research work is framed within theAutoLearn project, a follow up of the ALLES pro-ject (Schmidt et al, 2004, Quixal et al, 2006).AutoLearn?s aim was to exploit in a larger scale asubset of the technologies developed in ALLES inreal instruction settings.
Estrada et al (2009) de-scribe how, in AutoLearn?s first evaluation phase,the topics of the activities were not attractiveenough for learners and how learner activity de-creased within the same learning unit across exer-cises.
Both observations ?together with what it hasbeen shown with respect to the integration of inde-pendent language learning, see above ?
impelledus to develop AutoTutor, which allows teachers tocreate their own learning units.As reflected in Figure 1, AutoTutor consistsprimarily of two pieces of software: AutoTutorActivity Creation Kit (ATACK) and AutoTutorActivity Player (ATAP).
ATACK, an authoringFigure 1.
AutoTutor software architecture.20tool, provides teachers with the ability to createparser-based CALL exercises and define the corre-sponding exercise specifications for the generationof automated feedback.
ATAP allows teachers toinsert, track and manage those exercises in Moodle(http://moodle.org), giving learners the possibilityto visualize and answer them.
Both ATACK andATAP share a common infrastructure of NLP ser-vices which provides the basic methods for gener-ating, storing and using NLP tools.
Access to thosemethods is made through XML-RPC calls.2.1 AutoTutor Activity Creation KitATACK is divided in two components: a GUI thatallows content creators to enter the text, questionsand instructions to be presented to learners in orderto elicit answers from them; and an NLP resourcecreation module that automatically generates theresources that will be used for the automated feed-back.
Through the GUI, teachers are also able todefine a set of expected correct answers for eachquestion, and, optionally, specific customizedfeedback and sample answers.To encode linguistic and conceptual variation inthe expected answers, teachers are required to turnthem into linguistic patterns using blocks.
Blocksrepresent abstract concepts, and contain the con-crete chunks linked to those concepts.
Within ablock one can define alternative linguistic struc-tures representing the same concept.
By combiningand ordering blocks, teachers can define the se-quences of text that correspond to the expectedcorrect answers ?i.e., they can provide the seedsfor answer modelling.Modelling answersGiven an exercise where learners are requiredto answer the question ?From an architecture pointof view, what makes Hagia Sophia in Istanbul sofamous according to its Wikipedia entry?
?, thefollowing answers would be accepted:1.
{The Hagia Sophia/The old mosque} isfamous for its massive dome.2.
The reputation of {the Hagia Sophia/theold mosque} is due to its massive dome.To model these possible answers, one woulduse four blocks (see Figure 2) corresponding toWHO (Hagia Sophia), WHAT (Famousness), andWHY (Dome), and complementary linguistic ex-pressions such as ?is due to?.
Thus, the possiblecorrect block sequences would be (indices corre-sponding to Figure 2):a) B1 B2.A B4b) B2.B B1 B3 B4Block B1 is an example of interchangeable al-ternatives (the Hagia Sophia or the old mosque),which do not require any further condition to ap-ply.
In contrast, block B2 is an instance of a syn-tactic variation of the concept.
Famousness can beexpressed through an adjective or through a verb(in our example), but each of the choices requires adifferent sentence structure.Alternative texts in a block with no variants (asin B1) exploit the paradigmatic properties of lan-guage, while alternative texts in a block with twovariants as in B2 account for its syntagmatic prop-erties, reflected in the block sequences.
Interest-ingly, this sort of splitting of a sentence into blocksis information-driven and simplifies the linguisticexpertise needed for the exercise specifications.2.2 Automatic generation of exercise-specificNLP-resourcesFigure 3 shows how the teacher?s input is con-verted into NLP-components.
Predefined systemcomponents present plain borders, and the result-ing ones present hyphenised borders.
The figurealso reflects the need for answer and error model-ling resources.NLP resource generation processB2 (FAMOUSNESS)B:the reputation ofA:is famous forB1 (SOPHIA)the Hagia Sophiathe old mosqueB3 (DUE)is due toB4 (CAUSE)its massive domeFigure 2 Blocks as specified in AutoTutor GUI.21The generation of the NLP resources is possiblethrough the processing of the teacher?s input withthree modules:  the morphological analysis moduleperforms a lexicon lookup and determines un-known words that are entered into the exercise-specific lexicon; the disambiguation of base formmodule, disambiguates base forms, e.g.
?better?
isdisambiguated between verb and adjective depend-ing on the context in preparation of customizedfeedback.The last and most important module in the ar-chitecture is the match settings component, whichdetermines the linguistic features and structures tobe used by the content matching and the exercise-specific error checking modules (see Figure 4).Using relaxation techniques, the parsing of learnerinput is flexible enough to recognize structuresincluding incorrect word forms and incorrect,missing or additional items such as determiners,prepositions or digits, or even longish chunks oftext with no correspondence the specified answers.The match settings component contains rules thatlater on trigger the input for the exercise-specificerror checking.The match settings component consists ofKURD rules (Carl et al 1998).
Thus it can bemodified and extended by a computational linguistany time without the need of a programmer.Once the exercise?s questions and expected an-swers have been defined, ATACK allows for thegeneration of the NLP resources needed for theautomatic correction of that exercise.
The right-hand side of Figure 3 shows which the generatedresources are:?
An exercise-specific lexicon to handle un-known words?
A content matching module based on theKURD formalism to define several lin-guistically-motivated layers with differentlevels of relaxation (using word, lemma,and grammatical features) for determiningthe matching between the learner input andthe expected answers?
A customized feedback module for teacher-defined exercise-specific feedback?
An exercise-specific error checking mod-ule for context-dependent errors linked tolanguage aspects in the expected answers?
A general content evaluation componentthat checks whether the analysis performedby the content matching module conformsto the specified block orders2.3 AutoTutor Activity Player (ATAP)With ATAP learners have access to the contentsenhanced with automatic tutoring previously cre-ated by teachers.
ATAP consists of a) a client GUIfor learners, integrated in Moodle, to answer exer-cises and track their own activity; b) a client GUIfor teachers, also integrated in Moodle, used tomanage and track learning resources and learnerTeacher input (GUI)ERROR MODELANSWER MODELMorph.analysisMorph.analysisCustomizedfeedbackMatchsettingsGeneral contentevaluationContent mat-chingDisam.
ofbase formExercise-specific lexiconExercise-specificerror checkingBlocks (wordchunks)Teacher definederror modellingBlock orderFigure 3.
Processing schema and components of the customizable NLP resources of ATACK22activity; and c) a backend module, integrated intothe AutoTutor NLP Services Infrastructure, re-sponsible for parsing the learner?s input and gener-ating feedback messages.Figure 4 describes the two steps involved in theNLP-based feedback generation: the NLP compo-nents created through ATACK ?in hyphenisedrectangles?
are combined with general built-inNLP-based correction modules.2.4 The feedback generation softwareFeedback is provided to learners in two steps,which is reflected in Figure 4 by the two parts, theupper and lower part, called General Checking andExercise Specific Checking respectively.
The for-mer consists in the application of standard spelland grammar checkers.
The latter consists in theapplication of the NLP resources automaticallygenerated with the teacher?s input.Content matching moduleThe text chunks (blocks) that the teacher has en-tered into ATACK?s GUI are converted intoKURD rules.
KURD provides with sophisticatedlinguistically-oriented matching and action opera-tors.
These operators are used to model (predict-able) learner text.
The content matching module isdesigned to be able to parse learners input withdifferent degrees of correctness combining bothrelaxation techniques and mal-rules.
For instance,it detects the presence of both correct and incorrectword forms, but it also detects incorrect wordsbelonging to a range of closed or open wordclasses ?mainly prepositions, determiners, modalverbs and digits?
which can be used to issue a cor-responding linguistically motivated error messageslike ?Preposition wrong in this context?, in a con-text where the preposition is determined by therelevant communicative situation.Error types that are more complex to handle intechnical terms involve mismatches between theamount of expected elements and the actualamount of informational elements in the learner?sanswer.
Such mismatches arise on the grammaticallevel if a composite verb form is used instead of asimple one, or when items such as determiners orcommas are missing or redundant.
The system alsoaccounts for additional modifiers and other wordsinterspersed in the learner?s answer.The matching strategy uses underspecifiedempty slots to fit in textual material in between thecorrect linguistic structures.
Missing words arehandled by a layer of matching in which certainelements, mainly grammatical function words suchas determiners or auxiliary verbs, are optional.Incorrect word choice in open and closed wordclasses is handled by matching on more abstractlinguistic features instead of lexeme features.The interaction between KURD-based linguis-tically-driven triggers in the content matchingmodule and the rules in the exercise-specific errorchecking (see below) module allows for specificmal-rule based error correction.Customized feedbackTeachers can create specific error messages forsimple linguistic patterns (containing errors orsearching for missing items) ranging from one ortwo word structures to more complex word-basedlinguistic structures.
Technically, error patterns areMorph.analysisSpellcheckingGrammarcheckingLexicon Exercise-specific lexiconCustomizedfeedbackExercise-specificerror checkingGeneral contentevaluationContentmatchingEXERCISE-SPECIFIC CHECKING (TWO)GENERAL CHECKING (ONE)Figure 4.
Processing schema of the NLP resources to generate automatic feedback.23implemented as KURD rules linked to a specificerror message.
These rules have preference overthe rules applied by any other later module.Exercise-specific error checkingTeachers do not encode all the exercise-specificerrors themselves because a set of KURD rules forthe detection of prototypical errors is encoded ?thismodule uses the triggers set by the content match-ing component.
Exercise-specific linguistic errorshandled in this module have in common that theyresult in sentences that are likely to be wrong ei-ther from a formal (but context-dependent) point ofview or from an informational point of view.General content evaluationSince the contents are specified by the blocks cre-ated by teachers, the evaluation has a final step inwhich the system checks whether the learner?sanswer contains all the necessary information thatbelongs to a valid block sequence.This module checks for correct order in infor-mation blocks, for blending structures (mixtures oftwo possible correct structures), missing informa-tion and extra words (which do not always implyan error).
The messages generated with this com-ponent pertain to the level of completeness andadequacy of the answer in terms of content.3 Usage and evaluationAutoTutor has been used by a group of sevencontent creators ?university and school teachers?for a period of three months.
They developed over20 activities for learning units on topics such asbusiness and finance, sustainable production andconsumption, and new technologies.
Those activi-ties contain listening and reading comprehensionactivities, short-text writing activities, enablingtasks on composition writing aspects, etc.
whoseanswers must be expressed in relatively free an-swers consisting of one sentence.
In November2009, these activities were used in real instructionsettings with approximately 600 learners of Eng-lish and German.
Furthermore, an evaluation ofboth teacher and learner satisfaction and systemperformance was carried out.We briefly describe the process of creating thematerials by one of the (secondary school) teachersparticipating in the content creation process andevaluate the results of system performance in oneactivity created by this same teacher.3.1 Content creation: training and practiceTo start the process teachers received a 4-hourtraining course (in two sessions) where they weretaught how to plan, pedagogically speaking, alearning sequence including activities to be cor-rected using automatically generated feedback.
Werequired them to develop autonomous learningunits if possible.
And we invited them to get holdof any available technology or platform functional-ity to implement their ideas (and partially offeredsupport to them too), convinced that technologyhad to be a means rather than a goal in itself.
Thecourse also included an overview of NLP tech-niques and a specific course on the mechanics ofATACK (the authoring tool) and ATAP (the activ-ity management and deployment tool).During this training we learned that most teach-ers do not plan how activities will be assessed: thatis, they often do not think of the concrete answersto the possible questions they will pose to learners.They do not need to, since they have all the knowl-edge required to correct learner production anyplace, any time in their heads (the learner, the ac-tivity and the expert model) no matter if the learnerproduction is written or oral.
This is crucial since itrequires a change in normal working routine.After the initial training they created learningmaterials.
During creation we interacted with themto make sure that they were not designing activitieswhose answers were simply impossible to model.For instance, the secondary school teacher whoprepared the activity on sustainable production andconsumption provided us with a listening compre-hension activity including questions such as:1) Which is your attitude concerning respon-sible consumption?
How do you deal withrecycling?
Do you think yours is an eco-logical home?
Are you doing your best toreduce your ecological footprint?
Make alist with 10 things you could do at home toreduce, reuse o recycle waste at home.All these things were asked in one sole instruc-tion, to be answered in one sole text area.
We thentalked to the teacher and argued with her the kindsof things that could be modelled using simple one-sentence answers.
We ended up reducing the inputprovided to learners to perform the activity to one24video (initially a text and a video) and promptinglearners with the following three questions:1) Explain in your words what the ecologicalfootprint is.2) What should be the role of retailers accord-ing to Timo M?kel?
?3) Why should producers and service provid-ers use the Ecolabel?Similar interventions were done in other activi-ties created by other content creators.
But some ofthem were able to create activities which could beused almost straightforwardly.3.2 System evaluationThe materials created by teachers were thenused in their courses.
In the setting that we analyselearners of English as a second language wereCatalan and Spanish native speakers between 15and 17 years old that attended a regular first yearof Batxillerat (first course for those preparing toenter university studies).
They had all been learn-ing English for more than five years, and accordingto their teacher their CEF level was between A2and B1.
They were all digital literates and they allused the computer on a weekly basis for their stud-ies or leisure (80% daily).We analyse briefly the results obtained for twoof the questions in one of the activities created bythe school teacher who authored the learning uniton sustainable production and consumption,namely questions 1) and 2) above.
This learningunit was offered to a group of 25 learners.Overall system performanceTable 1 reflects the number of attempts performedby learners trying to answer the two questionsevaluated here: correct, partially correct and incor-rect answers are almost equally distributed (around30% each) and non-evaluated answers are roughly10%.
In non-evaluated answers we include basi-cally answers where learners made a bad use of thesystem (e.g., answers in a language other than theone learned) or answers which were exactly thesame as the previous one for two attempts in a row,which can interpreted in several ways (misunder-standing of the feedback, usability problems withthe interface, problems with pop-up windows, etc.
)that fall out of the scope of the current analysis.Table 2 and Table 3 show the number of mes-sages issued by the system for correct, partiallycorrect and incorrect answers for each of the twoquestions analyzed.
The tables distinguish betweenForm Messages and Content Messages, and RealForm Errors and Real Content Errors ?a crucialdistinction given our claim that using AutoTutormore open questions could be tackled.3QST CORR.
PART.
INCORR.
INV.
TOT1ST 36 23 12 2 732ND 14 29 36 21 100ALL 50 (29%) 52(30%) 48(28%) 23(13%) 173Table 1.
Correct, partially correct and incorrect answers.Table 2 and Table 3 show that the contrast be-tween issued feedback messages (most commonlyerror messages, but sometimes rather pieces ofadvice or suggestions) and real problems found inthe answers is generally balanced in formal prob-lems (31:15, 8:7 and 41:39 for Table 2; and 6:8,29:18, and 20:21 for Table 3) independently of thecorrectness of the answer.On the contrary, the contrast between issuedmessages and content problems is much more un-balanced in correct and partially correct answers(139:71 and 84:42 for Table 2; and 45:20 and110:57 for Table 3) and more balanced for incor-rect answers (30:18 for Table 2; and 93:77 forTable 3).MESSAGES REAL ERRORSForm Cont Form ContCORRECT ANSWERS 31 139 15 71PARTIALLY CORRECT 8 84 7 42INCORRECT ANSWERS 41 30 39 18TOTAL ANSWERS 80 253 61 131Table 2.
Messages issued vs. real errors for question 1in the answers produced by learners.MESSAGES REAL ERRORSForm Cont Form ContCORRECT ANSWERS 6 45 8 20PARTIALLY CORRECT 29 110 18 57INCORRECT ANSWERS 20 93 21 77TOTAL ANSWERS 55 248 47 154Table 3.
Messages issued vs. real errors for question 2in the answers produced by learners.This indicates that generally speaking the sys-tem behaved more confidently in the detection offormal errors than in the detection of content er-rors.3A proper evaluation would require manual correction of theactivities by a number of teachers and the correspondingevaluation process.25System feedback analysisTo analyze the system?s feedback we looked intothe answers and the feedback proposed by the sys-tem and annotated each answer with one or moreof the tags corresponding to a possible cause ofmisbehaviour.
The possible causes and its absolutefrequency are listed in Table 4.The less frequent ones are bad use of the systemon the learner side, bad guidance (misleading thelearner to an improper answer or to a more com-plex way of getting to it), connection failure, andmessage drawing attention on form when the errorwas on content.MISBEHAVIOUR QUESTION 1 QUESTION 2CONN-FAIL 1 0BAD-USE 1 1FRM-INSTOF-CONT 2 1BAD-GUIDE 4 2OOV 11 13WRNG-DIAG 11 20FRM-STRICT 33 20ARTIF-SEP 0 61SPECS-POOR 1 62Table 4.
Frequent sources of system errors.The most frequent causes of system misbehav-iour are out-of-vocabulary words, wrong diagno-ses, and corrections too restrictive with respect toform.Two interesting causes of misbehaviour and infact the most frequent ones were artificial separa-tion and poor specifications.
The former refers tothe system dividing answer parts into smaller parts(and therefore generation of a larger number ofissued messages).
For instance in a sentence like(as an answer to question 2)The retailers need to make sure that whateverthey label or they put in shelf is understandableto consumers.4the system would generate six different feedbackmessages informing that some words were notexpected (even if correct) and some were found butnot in the expected location or form.In this same sentence above we find examplesof too poor specifications, where, for instance, itwas not foreseen that retailers was used in theanswer.
These two kinds of errors reflect the flawsof the current system: artificial separation reflects alack of generalization capacity of the underlying4One of the expected possible answers was ?They need tomake sure that whatever they label and whatever they put inthe shelves is understood by consumers?.parser, and poor specifications reflect the incom-pleteness of the information provided by noviceusers, teachers acting as material designers.4 Concluding remarksThis paper describes software that providesnon-NLP experts with a means to utilize and cus-tomize NLP-intensive resources using an authoringtool for language instruction activities.
Its usabilityand usefulness have been tested in real instructionsettings and are currently being evaluated and ana-lyzed.
Initial analyses show that the technologyand methodology proposed allow teachers to createcontents including automatic generation feedbackwithout the need of being neither a programmernor an NLP expert.Moreover, system performance shows a reason-able confidence in error detection given the imma-turity of the tool and of its users ?followingShneiderman and Plaisant?s terminology (2006).There is room for improvement in the way to re-duce false positives related with poor specifica-tions.
It is quite some work for exercise designersto foresee a reasonable range of linguistic alterna-tives for each answer.
One could further supportthem in the design of materials with added func-tionalities ?using strategies such as shallow seman-tic parsing, as in (Bailey and Meurers, 2008), oradding functionalities on the user interface thatallow teachers to easily feed exercise models orspecific feedback messages using learner answers.The architecture presented allows for portabilityinto other languages (English and German alreadyavailable), with a relative simplicity provided thatthe lexicon for the language exists and containsbasic morpho-syntactic information.
Moreover,having developed it as a Moodle extension makesit available to a wide community of teachers andlearners.
The modularity of ATACK and ATAPmakes them easy to integrate in other LearningManagement Systems.In the longer term we plan to improve AutoTu-tor?s configurability so that its behaviour can bedefined following pedagogical criteria.
One of theaspects to be improved is that a computationallinguist is needed to add new global error types tobe handled or new linguistic phenomena to be con-sidered in terms of block order.
If such a system isused by wider audiences, then statistically driventechniques might be employed gradually, probably26in combination with symbolic techniques ?theusage of the tool will provide with invaluablelearner corpora.
In the meantime AutoTutor pro-vides with a means to have automatic correctionand feedback generation for those areas and textgenres where corpus or native speaker text isscarce, and experiments show it could be realisti-cally used in real instruction settings.AcknowledgmentsWe want to thank the secondary school teacherswho enthusiastically volunteered in the creationand usage of AutoLearn materials: Eli Garrabou(Fundaci?
Llor), M?nica Castanyer, Montse Pada-reda (Fundaci?
GEM) and Anna Campillo (EscolaSant Gervasi).
We also want to thank their learn-ers, who took the time and made the effort to gothrough them.
We also thank two anonymous re-viewers for their useful comments.ReferencesAmaral, Luiz A., and Detmar Meurers.
On Using Intel-ligent Computer-Assisted Language Learning inReal-Life Foreign Language Teaching and Learning(Submitted).Bailey, Stacey and Detmar Meurers (2008) Diagnosingmeaning errors in short answers to reading compre-hension question.
In Proceedings of the Third ACLWorkshop on Innovative Use of NLP for BuildingEducational Applications, pages 107?115, Columbus,Ohio, USA, June 2008.Carl, Michael, and Antje Schmidt-Wigger (1998).
Shal-low Post Morphological Processing with KURD.
InProceedings of NeMLaP'98, Sydney.Chen, Liang and Naoyuki Tokuda (2003) A New Tem-plate-Template-enhanced ICALL System for a Sec-ond Language Composition Course.
CALICOJournal, Vol.
20, No.
3: May 2003.Estrada, M., R. Navarro-Prieto, M. Quixal (2009) Com-bined evaluation of a virtual learning environment:use of qualitative methods and log interpretation toevaluate a computer mediated language course.
InProceedings of International Conference on Educa-tion and New Learning Technologies, EDULEARN09.
Barcelona (Spain), 6th-8th July, 2009.Heift, Trude.
2001.
Intelligent Language Tutoring Sys-tems for Grammar Practice.
Zeitschrift f?r Interkul-turellen Fremdsprachenunterricht 6, no.
2.http://www.ualberta.ca/~german/ejournal/ heift2.htm.???.
2003.
Multiple learner errors and meaningfulfeedback: A challenge for ICALL systems.
CALICOJournal 20, no.
3: 533-548.???.
2005.
Corrective Feedback and Learner Uptakein CALL.
ReCALL Journal 17, no.
1: 32-46.Heift, Trude, and Mathias Schulze.
2007.
Errors andIntelligence in Computer-Assisted Language Learn-ing: Parsers and Pedagogues.
New York: Routledge.Levy, Michael.
1997.
Computer-Assisted LanguageLearning.
Context and Conceptualization.
Oxford:Oxford University Press.Nagata, Noriko.
2002.
BANZAI: An Application ofNatural Language Processingto Web basedLanguage Learning.
CALICO Journal 19, no.
3: 583-599.???.
2009.
Robo-Sensei?s NLP-Based Error Detec-tion and Feedback Generation.
CALICO Journal 26,no.
3: 562-579.Polisca, Elena.
2006.
Facilitating the Learning Process:An Evaluation of the Use and Benefits of a VirtualLearning Environment (VLE)-enhanced IndependentLanguage-learning Program (ILLP).
CALICO Jour-nal 23, no.3: 499-51.Quixal, M., T. Badia, B. Boullosa, L. D?az, and A. Rug-gia.
(2006).
Strategies for the Generation of Indi-vidualised Feedback in Distance Language Learning.In Proceedings of the Workshop on Language-Enabled Technology and Development and Evalua-tion of Robust Spoken Dialogue Systems of ECAI2006.
Riva del Garda, Italy, Sept. 2006.R?sener, C.: ?A linguistic intelligent system for tech-nology enhanced learning in vocational training ?
theILLU project?.
In Cress, U.; Dimitrova, V.; Specht,M.
(Eds.
): Learning in the Synergy of Multiple Dis-ciplines.
4th European Conference on TechnologyEnhanced Learning, EC-TEL 2009 Nice, France,Sept.
29 ?
Oct. 2, 2009.
Lecture Notes in ComputerScience.
Programming and Software Engineering,Vol.
5794, 2009, XVIII, p. 813, Springer, Berlin.Schmidt, P., S. Garnier, M. Sharwood, T. Badia, L.D?az, M. Quixal, A. Ruggia, A. S. Valderrabanos, A.J.
Cruz, E. Torrejon, C. Rico, J. Jimenez.
(2004)ALLES: Integrating NLP in ICALL Applications.
InProceedings of Fourth International Conference onLanguage Resources and Evaluation.
Lisbon, vol.
VIp.
1888-1891.
ISBN: 2-9517408-1-6.Shneiderman, B. and C. Plaisant.
(2006) Strategies forevaluating information visualization tools: multi-dimensional in-depth long-term case studies.
BELIV?06: Proceedings of the 2006 AVI workshop on Be-yond time and errors: novel evaluation methods forinformation visualization, May 2006.Toole, J.
& Heift, T. (2002).
The Tutor Assistant: AnAuthoring System for a Web-based Intelligent Lan-guage Tutor.
Computer Assisted Language Learning,15(4), 373-86.27
