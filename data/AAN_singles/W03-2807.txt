Adaptation of the F-measure to Cluster Based Lexicon QualityEvaluationAngelo DalliNLP Research GroupDepartment of Computer ScienceUniversity of Sheffielda.dalli@dcs.shef.ac.ukAbstractAn external lexicon quality measurecalled the L-measure is derived from theF-measure (Rijsbergen, 1979; Larsen andAone, 1999).
The typically small samplesizes available for minority languages andthe evaluation of Semitic language lexi-cons are two main factors considered.Large-scale evaluation results for theMaltilex Corpus are presented (Rosner etal., 1999).1 IntroductionComputational Lexicons form a fundamentalcomponent of any NLP system.
Unfortunately,good quality lexicons are hard to create andmaintain.
The labour intensive process of lexiconcreation is further compounded when minoritylanguages are concerned.
Inevitably, computa-tional lexicons for minor languages tend to bequite small when compared to computationallexicons available for more common languagessuch as English.The Maltilex Corpus is used in this paper toevaluate a cluster based lexicon quality measureadapted from the F-measure.
The Maltilex Cor-pus is the first large-scale computational lexiconfor Maltese (Rosner et al, 1999).
The choice ofMaltese as the evaluation language presentedsome additional problems due to the Semiticmorphology and grammar of Maltese (Mifsud,1995).
An innovative approach to lexicon crea-tion using an automated technique called theLexicon Structuring Technique (LST) was usedto create an initial computational lexicon from awordlist (Dalli, 2002a).
LST decreased theamount of work that is normally required to cre-ate a lexicon from scratch by adapting a numberof clustering, alignment, and approximate match-ing techniques to produce a set of clusters con-taining related wordforms.
Lexicon clusters arethus analogous to lemmas in more traditionallexicons.This approach has many advantages for a lan-guage having a Semitic morphology and gram-mar due to the large number of wordforms thatcan be derived for a single lemma.
Instead ofprocessing every wordform individually, thewhole cluster can be treated as a single entity,reducing processing requirements significantly.The close relationship of this lexicon defini-tion and standard clustering systems (with lem-mas corresponding to clusters), enabled the re-use of cluster quality evaluation measures to thetask of lexicon quality evaluation.
There are twomain ways of evaluating cluster quality which aresummarised in (Steinbach et al, 1999 pg.
6) asfollows:?
Internal Quality Measure ?
Clusters arecompared without reference to externalknowledge against some predefined set ofdesirable qualities.?
External Quality Measure ?
Clusters arecompared to known external classes.Internal quality measures are not always desir-able, since their very existence implies that betterquality can be achieved by applying an internalquality measure in conjunction with some opti-misation technique.
An internal quality measurefor cluster-based lexicons was not available ei-ther.The two main external quality measures appli-cable lexicon quality evaluation tasks are entropy(Shannon, 1948) and the F-measure (vanRijsbergen, 1979; Larsen and Aone, 1999).Entropy based quality measures assert that thebest entropy that can be obtained is when eachcluster contains the optimal number of members.In our context this corresponds to having clusters(corresponding to lemmas) that contain exactlyall the wordforms associated with that cluster.The class distribution of the data is calculated byconsidering the probability of every member be-longing to some class.
The entropy of every clus-ter j is calculated using the standard entropyformula( )( )E j p pij iji= ?
?log  where pijde-notes the probability that a member of cluster jbelongs to class i.
The total entropy is then calcu-lated as( )Enn E jjjm*= ?=?11where njis thesize of cluster j, m the number of clusters, and nthe total number of data points.The F-measure treats every cluster as a queryand every class as the desired result set for aquery.
The recall and precision values for eachgiven class are then calculated using informationretrieval concepts.
The F-measure of cluster j andclass i is given by( )( ) ( )( ) ( )F i jr i j p i jr i j p i j,, ,, ,=?
?+2where r denotes recall and p the precision.
Recallis defined as  ( )r i jnniji, =  and precision is de-fined as( )p i jnnijj, =where nijis the number ofclass i members in cluster j, while njand niarethe sizes of cluster j and class i respectively.
Theoverall F-measure for the entire data set of size nis given by( )[ ]FnnF i jii*max ,=?.2 Lexicon Quality MeasureComputational lexicons have an additional do-main-specific external quality measure availablein the form of existing non-computational lan-guage dictionaries.
Dictionaries can be used tocompare the results generated by the automatedsystem against those produced by human experts.Generally it can be assumed that reputableprinted dictionaries are of a very high quality andthus provide a gold standard for comparison.
Forsome languages, especially minority languages,the only available quality data would be inprinted dictionary form.
Unfortunately most non-computational dictionaries are not amenable toautomated analysis techniques since the processof re-inputting and re-structuring data into acomputational dictionary format is generally solabour intensive that it becomes too expensive.Additionally, since every cluster and classcorrespond to a lemma, the number of classes tobe considered is expected to number in the thou-sands.
This would make a straightforward appli-cation of the F-measure an overly long process.A modified statistical sampling technique basedon the F-measure that gives results that are ap-proximately as good as the full application of theF-measure and that caters for the particular nu-ances of lexicon quality evaluation is thusneeded.The L-measure is such a new measure basedon the F-measure that attempts to measure thequality of a given lexicon in relation to other ex-isting lexicons that are possibly non-computational lexicons (i.e.
human compiledlanguage dictionaries), taking into considerationthat a full population analysis may not be practi-cal under most circumstances.2.1 Lexicon Extraction from DictionariesThe L-Measure works by comparing two lexi-cons, one derived from a gold standard represen-tation in the form of human compiled dictionariesand the other being a computational lexiconwhose quality is being assessed.
In order to avoidconfusion, formal definitions of the terms dic-tionary, lexicon and wordlists are now presented.A dictionary D is formally modeled as a se-quence <t1.. th> of tuples of the form (l, def)where l denotes a lemma (i.e.
a dictionary head-word in a more traditional sense) and def is a 5-tuple (m, r, c, i, o) with m containing morpho-logical information that enables members of thelemma to be inferred or generated, r a set of rela-tions to other lemmas, c a description of the dif-ferent contexts where the lemma may benormally used, i containing meta-informationabout lemma l itself, and o an object containingadditional information (such as etymology, ex-amples of common use, etc.)
Since multiple en-tries of the same headword may be present in Dthe sequence is not injective, i.e.
the sequencecan contain duplicate elements.The main two differences between a dictionaryand a lexicon are that different types of informa-tion are stored about every lemma in the defcomponent, and secondly, that a lexicon has aninjective sequence of tuples (i.e.
a sequence thatdoes not have duplicates and where the exact or-der is important) while a dictionary does not(since a dictionary does not need to force aheadword to have one unique entry, especially inthe case of printed dictionaries that often have thesame headword appearing in multiple top-levelentries).A dictionary D can be thus transformed into alexicon L, denoted by L = lex(D), by filtering thetuple sequence <t1.. th> making up D to includeonly the l components of every tuple.
The filteredsequence is then transformed into an injectivesequence of unique lemmas <l1.. lu>, satisfyingthe requirements for a lexicon.
Appropriate trans-formations have to be defined to transform thedef component from dictionary to lexicon format.The sequence of lemmas is then expanded to acanonical wordlist W. A canonical wordlist W isa sequence <w1.. wu> of sets of strings generatedfrom a lexicon L, denoted by W = can(L), by list-ing all possible instances of every lemma in thelexicon (i.e.
all possible wordforms of a particu-lar lemma), in effect creating a full form lexicon.The canonical wordlist W thus has u sets ofstrings corresponding to u lemmas in the lexicon.The particular lemma used to generate a word-form w is obtained by the operator lem(w).
Thesequence of lemmas used to generate W is de-noted as lemmas(W).
The union of two wordlistsW1?
W2is defined to be the union of all sets ofstrings in both wordlists,i.e.jijiyxWWWyWx ?=????
?2121,provided that lem(xi) = lem(yj) ?
lem(xj) ?lemmas(W2) ?
lem(yj) ?
lemmas(W1) holds.This definition ensures maximum coverage ofthe resulting canonical wordlist.
An empty or nullcanonical wordlist results if no pair of stringsobey the previously stated condition while theunion of a wordlist with a null wordlist is theoriginal wordlist itself.Similarly the intersection of two wordlists W1?
W2is defined to be the union of all sets ofstrings in both wordlists that have correspondinglemmas appearing in both wordlists, i.e.?
?
?
?
?
= ?x W y W W W x yi j i j1 2 1 2,provided that lem(xi) = lem(yj) holds.Note that this definition is concerned mainlywith the lemmas and their associated wordformsthemselves.
Since lexicons are not just a list oflemmas and wordforms, other linguistic annota-tions will have to be evaluated using other tech-niques appropriate to the particular linguisticannotations added to the lemma entries.2.2 L-Measure DefinitionGiven a lexicon L and a set of dictionaries D ={D1.. Dk} transform the set of dictionaries D intoa set of lexicons L' = {L1.. Lk} using the lextransformation on every dictionary, thus( )UkiDlexL1'= .
Define W as the canonical word-list obtained from L, W = can(L) and W' as thecanonical wordlist obtained from L',( )UkiLcanW1'=  under canonical wordlist union.Define Y to be the canonical wordlist of wordscommon to both W and W', Y = W ?
W'.
Thesample size S used for the L-measure is definedas ?.|lemmas(Y)| where ?
is some value in therange (0..1) that controls the random sample size.Typically ?
should be set to somewhere between0.01 and 0.1.
It is expected that the sample sizewill be large enough to assume that the sample isrepresentative of the whole population.The L-measure of a lemma j in lemmas(W) andlemma i in lemmas(Y) is given by( )( ) ( )( ) ( )L i jr i j p i jr i j p i j,, ,, ,=?
?+2where r denotes re-call and p is the precision.
Recall is defined as( )r i jnniji, =  and precision is defined as( )p i jnnijj, =where nijis the number of lemma imembers in lemma j, while njand niare the sizesof lemma j and lemma i respectively.
The overallL-measure for the entire sample of size n is givenby( )[ ]LnnL i jii*max ,=?.
L*is always in therange [0..1] and is proportional to the lexiconquality, with an L*score of 1 representing a per-fect quality lexicon with respect to the lexiconbeing used as a standard.Y is used instead of W' since lexical word cov-erage is largely determined by the quality of thecorpus used to create the lexicon.
While this kindof analysis might be useful in determining thecoverage of a lexicon the L-measure is orientedtowards measuring quality rather than quantity,independently of the corpus that was used to cre-ate the lexicon.3 ResultsThe L-measure has been used to measure thequality of the Maltilex Computational Lexicon inrelation to existing paper based dictionaries.
Themost comprehensive dictionary of Maltese wasused to produce L', the comparison standard lexi-con (Aquilina, 1987-1990).
The capability of theL-measure to work with a statistical sample madea manual analysis of results possible without hav-ing L' in digital form.The value for the sample size S was deter-mined through a parameter ?
that was set to 0.01,meaning that 1% of all lemmas in the MaltilexComputational Lexicon were covered by the sta-tistical sample.
Since around 63,000 lemmas ex-ist in the combined lexicon the sample size S wasdetermined to be 630.
The set of 630 lemmaschosen at random from the Maltilex Corpus con-tained a total of 5,887 wordforms taken from thecombined lexicon.The precision and recall for the samples werecalculated individually to obtain the individual L-measure for a range of lemmas.
A fully workedout example of the calculation of the L-measurefor the lemma missier (father) is given.
Lem-mas in the Maltilex Computational Lexicon arealigned automatically using a technique adoptedfrom bioinformatics and hence the presentationof the wordforms in their aligned format (Dalli,2000b; Gusfield, 1997).The lemma missier (the Maltese word for fa-ther with the cluster showing different forms likemy father, your father, etc.)
taken from theMaltilex Computational Lexicon, which repre-sents lemma i, contains seven members as dis-played below:m i s s ie r _ _ _ _ _ _  _ _ _m i s s ie r e k _ _ _ _  _ _ _m i s s ie r _ _ _ n _ a  _ _ _m i s s ie r _ k o m _ _  _ _ _m i s s i  r i _ _ _ j ie t n am i s s ie r i _ _ _ _ _  _ _ _m i s s ie r _ h o m _ _  _ _ _The lemma missier, taken from Aquilina?s Dic-tionary, which represents lemma j, can be used togenerate the following ten members as displayedbelow:m i s s ie r _ _ _ _ _ _  _ _ _m i s s ie r e k _ _ _ _  _ _ _m i s s ie r _ _ _ n _ a  _ _ _m i s s ie r _ k o m _ _  _ _ _m i s s i  r i _ _ _ j ie t n am i s s ie r i _ _ _ _ _  _ _ _m i s s ie r a _ _ _ _ _  _ _ _m i s s ie r _ _ u _ _ _  _ _ _m i s s ie r _ h o m _ _  _ _ _m i s s i  r i _ _ _ j ie t _ _For this example, njand niare thus equal to 10and 7 respectively.
Recall and precision valuesare calculated as ( ) 177', ==missiermissierr( ) 7.0107', ==missiermissierp respectively.The L-measure for the lemma missier is( ) 8235.07.14.17.017.012', ==+?
?=missiermissierLThe overall L-measure for the entire sample of5,887 wordforms is given by( )[ ]?=iijiLnL ,max5887*.
The contribution ofthe lemma missier to the final L*score is thusgiven by 8235.058877= 0.000979226.
A highprecision floating point library was used to repre-sent the individual contribution values since theseare generally very small.
Figures 1 and 2 showthe precision and recall curves for the wholesample respectively.00.20.40.60.811.21 32 63 94 125 156 187 218 249 280 311 342 373 404 435 466 497 528 559 590 621LemmasFigure 1 Precision00.20.40.60.811.21 25 49 73 97 121 145 169 193 217 241 265 289 313 337 361 385 409 433 457 481 505 529 553 577 601 625LemmasFigure 2 Recall00.20.40.60.811.21 32 63 94 125 156 187 218 249 280 311 342 373 404 435 466 497 528 559 590 621LemmasFigure 3 Precision and Recall TrendsFigure 3 shows moving average trendlines forprecision and recall (precision is shown in a boldline on top, recall is the fainter line underneath).The average precision was 0.91748 and the aver-age rate of recall was 0.661359.00.20.40.60.811.21 55 109 163 217 271 325 379 433 487 541 595Figure 4 Individual L-Measure Values00.20.40.60.811.21 55 109 163 217 271 325 379 433 487 541 595Figure 5 Individual L-Measure Values TrendFigure 4 shows the individual L-measure val-ues for the sample.
The values displayed in Fig-ure 4 are those used to calculate the final L*value.
Figure 5 shows the moving average trend-line for the individual L-measure values.The average individual L-measure was0.707256882 while the average individualcontribution of a lemma to the L*value was0.000748924.
The variance in the L-measure in-dividual values was 0.065504369.The correlation between the L-measure andprecision was 0.163665769 while the correlationbetween the L-measure and recall was0.922214452.The overall L*score for the Maltilex Computa-tional Lexicon was 0.4718.
This score is quiteintuitive when the various problems in the exist-ing Maltese corpus used to create the Computa-tional Lexicon are considered.
This score meansthat the number of wordforms that are stored orthat can be generated by the current lexiconneeds to be expanded by around 53% in order tomatch the quality of the lexicon underlyingAquilina?s dictionary (Aquilina, 1987-1990).4 ConclusionThe L-measure is a useful evaluation metric thatcan be used to measure the quality of a computa-tional lexicon based on clustering concepts.
Thesmall data sample required by L-measure to givemeaningful results makes it a practical measureto use in a variety of situations where massiveamounts of data might not be available.
Thismakes L-measure ideal for use in the evaluationof Language Resources for minority languagesand also for quick benchmark studies that evalu-ate the quality of a computational lexicon as it isbeing created.Compared with the F-measure, the L-measurewill give highly similar results using less data.Naturally the validity of the L-measure resultsdepends on the choice of the ?
value, which inturn determines the sample size.The lemma/cluster based approach of the L-measure is suitable for the evaluation of Semiticlanguage lexicons that often prove problematic toevaluation techniques based on English or Ro-mance languages.The L-measure also has potential future appli-cations in the comparison and evaluation of dif-ferent lexicons.
The individual L-measure scorescan also be used to identify areas of similaritiesand differences between different lexiconsquickly.The L-measure can also be adapted to otherareas of Computational Linguistics as long as theconcept of a cluster and some means of determin-ing its precision and recall exist.
Minimalchanges are needed to adapt the L-measure toother domains making future adaptations likely.AcknowledgmentThis work has been made possible with the col-laboration of the Maltilex Project at the Univer-sity of Malta.ReferencesAngelo Dalli.
2002a.
Computational Lexicon for Mal-tese.
M.Sc.
Dissertation.
Department of ComputerScience and AI, University of Malta, Malta.Angelo Dalli.
2002b.
Biologically Inspired LexiconStructuring Technique.
HLT2002, San Diego, Cali-fornia.Bjorner Larsen and Chinatsu Aone.
1999.
Fast andEffective Text Mining Using Linear-time Docu-ment Clustering.
KDD-99, San Diego, California.C.
Van Rijsbergen.
1979.
Information Retrieval, 2nded.
Butterworth, London.Claude E. Shannon.
1948.
A mathematical theory ofcommunication.
Bell System Technical Journal 27:379-423, 623-656.Dan Gusfield.
1997.
Algorithms on Strings, Trees andSequences.
Cambridge University Press, Cam-bridge, UK.Joseph Aquilina.
1987-1990.
Maltese-English Dic-tionary.
Midsea Books, 2 Volumes, Valletta, Malta.Manwel Mifsud.
1995.
Loan verbs in Maltese a de-scriptive and comparative study.
Studies in Semiticlanguages and linguistics, Brill, Leiden.Michael Rosner et.
al.
1999.
Linguistic and Computa-tional Aspects of Maltilex.
ATLAS Symposium, Tu-nis.Michael Steinbach, George Karypis, and VipinKumar.
1999.
A comparison of document cluster-ing techniques, University of Minnesota, TechnicalReport 00-034.
