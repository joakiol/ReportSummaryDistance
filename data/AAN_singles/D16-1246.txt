Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2263?2270,Austin, Texas, November 1-5, 2016. c?2016 Association for Computational LinguisticsA Stacking Gated Neural Architecturefor Implicit Discourse Relation ClassificationLianhui Qin1,2, Zhisong Zhang1,2, Hai Zhao1,2,?1Department of Computer Science and Engineering,Shanghai Jiao Tong University, Shanghai, 200240, China2Key Laboratory of Shanghai Education Commission for Intelligent Interactionand Cognitive Engineering, Shanghai Jiao Tong University, Shanghai, 200240, China{qinlianhui, zzs2011}@sjtu.edu.cn,zhaohai@cs.sjtu.edu.cnAbstractDiscourse parsing is considered as one of themost challenging natural language processing(NLP) tasks.
Implicit discourse relation clas-sification is the bottleneck for discourse pars-ing.
Without the guide of explicit discourseconnectives, the relation of sentence pairs arevery hard to be inferred.
This paper proposesa stacking neural network model to solve theclassification problem in which a convolu-tional neural network (CNN) is utilized forsentence modeling and a collaborative gatedneural network (CGNN) is proposed for fea-ture transformation.
Our evaluation and com-parisons show that the proposed model outper-forms previous state-of-the-art systems.1 IntroductionAs a fundamental task in natural language process-ing (NLP), discourse parsing entails the discovery ofthe latent relational structure in multi-sentence levelanalysis.
It is also central to many practical taskssuch as question answering (Liakata et al, 2013;Jansen et al, 2014), machine translation (Meyerand Popescu-Belis, 2012; Meyer and Webber, 2013)and automatic summarization (Murray et al, 2006;?Corresponding author.
This paper was partially supportedby Cai Yuanpei Program (CSC No.
201304490199 and No.201304490171), National Natural Science Foundation of China(No.
61170114, No.
61672343 and No.
61272248), NationalBasic Research Program of China (No.
2013CB329401), Ma-jor Basic Research Program of Shanghai Science and Tech-nology Committee (No.
15JC1400103), Art and Science In-terdisciplinary Funds of Shanghai Jiao Tong University (No.14JCRZ04), and Key Project of National Society Science Foun-dation of China (No.
15-ZDA041).Yoshida et al, 2014).
Discourse parsing is also theshared task of CoNLL 2015 and 2016 (Xue et al,2015; Xue et al, 2016), and many previous worksprevious on this task (Qin et al, 2016b; Li et al,2016; Chen et al, 2015; Wang and Lan, 2016).
Ina discourse parser, implicit relation recognition hasbeen the bottleneck due to lack of explicit connec-tives (like ?because?
or ?and?)
that can be strongindicators for the senses between adjacent clauses(Qin et al, 2016b; Pitler et al, 2009; Lin et al,2014).
This work therefore focuses on implicit re-lation recognition that infers the senses of the dis-course relations within adjacent sentence pairs.Most previous works on PDTB implicit relationrecognition only focus on one-versus-others binaryclassification problems of the top level four classes(Pitler et al, 2009; Zhou et al, 2010; Park andCardie, 2012; Biran and McKeown, 2013; Ruther-ford and Xue, 2014; Braud and Denis, 2015).
Tra-ditional classification methods directly rely on fea-ture engineering, based on bag-of-words, produc-tion rules, and some linguistically-informed fea-tures (Zhou et al, 2010; Rutherford and Xue,2014).
However, discourse relations root in seman-tics, which may be hard to recover from surfacelevel feature, thus these methods did not report sat-isfactory performance.
Recently, neural network(NN) models have shown competitive or even bet-ter results than traditional linear models with hand-crafted sparse features (Wang et al, 2016b; Zhanget al, 2016a; Jia and Zhao, 2014).
They have beenproved to be effective for many tasks (Qin et al,2016a; Wang et al, 2016a; Zhang et al, 2016b;Wang et al, 2015; Wang et al, 2014; Cai and2263Zhao, 2016), also including discourse parsing.
Jiand Eisenstein (2015) adopt recursive neural net-work and incorporate with entity-augmented dis-tributed semantics.
Zhang et al (2015) explore ashallow convolutional neural network and achievecompetitive performance.
Although simple neuralnetwork has been shown effective, the result has notbeen quite satisfactory which suggests that there isstill space for improving.The concerned task could be straightforwardlyformalized as a sentence-pair classification problem,which needs inferring senses solely based on the twoarguments without cues of connectives.
Two prob-lems should be carefully handled in this task: how tomodel sentences and how to capture the interactionsbetween the two arguments.
The former could beaddressed by Convolutional Neural Network (CNN)which has been proved effective for sentence mod-eling (Kalchbrenner et al, 2014; Kim, 2014), whilethe latter is the key problem, which might need deepsemantic analysis for the interaction of two argu-ments.
To solve the latter problem, we proposecollaborative gated neural network (CGNN) whichis partially inspired by Highway Network whosegate mechanism achieves success (Srivastava et al,2015).
Our method will be evaluated on the bench-mark dataset against state-of-the-art methods.The rest of the paper is organized as follows: Sec-tion 2 briefly describes our model, introducing thestacking architecture of CNN and CGNN, Section 3shows the experiments and analysis, and Section 4concludes this paper.2 MethodThe architecture of the model, as shown in Figure 1,is straightforward.
It can be divided into three parts:1) CNN for modeling arguments; 2) CGNN unit forfeature transformation; 3) a conventional softmaxlayer for the final classification.
CNN is used toobtain the vector representations for the sentences,CGNN further captures and transforms the featuresfor the final classification.2.1 Convolutional Neural NetworkAs CNN has been broadly adopted for model-ing sentences, we will explain it in brevity.
Fortwo arguments, typical sentence modeling processSoftmaxCGNNCNNConvolution + PoolingConcatenateConvolution + PoolingParameter sharingArg1 Arg2vchgi?goFigure 1: Model Architecturewill be applied: sentence embedding (includingembeddings for words and part-of-speech (POS)tags) through projection layer, convolution opera-tions (with multiple groups of filters) through theconvolution layer, obtaining the sentence representa-tion through one-max-pooling.
The two argumentswill get their sentence vectors independently with-out any interfering, and the convolution operationwill be the same by sharing parameters.
The fi-nal argument-pair representation will be the vectorv which is concatenated from two sentence vec-tors and this vector will be used as the input of theCGNN unit.2264COMP.
CONT.
EXP.
TEMP.
AVGCNN Only 39.07 54.73 65.94 30.19 47.48CNN+MLP 37.81 56.30 69.44 32.29 48.96CNN+LSTM 39.15 53.44 68.85 29.79 47.81CNN+Highway 37.72 56.35 68.94 30.56 48.39CNN+CGNN 41.55 57.32 71.50 35.43 51.45Table 1: F1 scores (%) with different models.2.2 Collaborative Gated Neural NetworkFor implicit sense classification, the key is how toeffectively capture the interactions between the twoarguments.
The interactions could be word pairs,phrase pairs or even the latent meaning of the twofull arguments.
Pitler et al (2009) has shown thatword pair features are helpful.
To model these in-teractions, we have to make a full use of the sen-tence vectors obtained from CNN.
However, com-mon neural hidden layers might be insufficient todeal with the challenge.
We need to seek more pow-erful neural models, i.e., gated neural network.In recent years, gated mechanism has gained pop-ularity in neural models.
Although it is first in-troduced in the cells of recurrent neural networks,like Long-Short Term Memory (LSTM) (Hochreiterand Schmidhuber, 1997) and Gated Recurrent Unit(GRU) (Chung et al, 2014), traditional feed-forwardneural models such as the Highway Network couldalso benefit from it (Srivastava et al, 2015).
Theexisting studies show that the gated mechanism inhighway network serves not only a means for eas-ier training, but also a tool to route information in atrained network.Motivated by the idea of highway network,we propose a collaborative gated neural network(CGNN) for this task.
The architecture of CGNNis illustrated in Figure 1, and it contains a sequenceof transformations.
First, the inner-cell c?
is obtainedthrough linear transformation and non-linear activa-tion on the input v, and this process is exactly theoperation of an ordinary neural layer.c?
= tanh(Wc ?
v + bc)Meanwhile, the two gates gi and go are calculatedindependently because they are only influenced bythe original input through different parameters:gi = ?
(Wi ?
v + bi)go = ?
(Wo ?
v + bo)where the ?
denotes sigmoid function which guar-antees the values in the gates are in [0,1].
Two gatedoperations are applied sequentially, where a gatedoperation indicates the element-wise multiplicationof an inner-cell and a gate.
Between the two gatedoperations, a non-linear activation operation is ap-plied.
The procedure could be formulated as fol-lows:c = c?
gih = tanh(c) gowhere  denotes element-wise multiplication, c isthe second inner-cell and h is the output of CGNNunit.Although the two gates are generated indepen-dently, they will work collaboratively because theycontrol the information flow of the inner-cells se-quentially which resembles logical AND operationin a probabilistic version.
In fact, the transforma-tions after c?
will concern only element-wise oper-ations which might give finer controls for each di-mension, and the information can only flow on thedimensions where both gates are ?open?.
This pro-cedure will help select the most crucial features.The gates in this model are mainly used for rout-ing information from sentence-pairs vectors.
Whenthere is only one gate in our network, the modelworks similar to the highway network (Srivastava etal., 2015).2.3 Output and TrainingAfter the transformation of the CGNN unit, thetransformed vector h will be sent to a conventionalsoftmax for classification.2265The training object J will be the cross-entropy er-ror E with L2 regularization:E(y?, y) = ?l?jyj ?
log(Pr(y?j))J(?)
= 1mm?kE(y?
(k), y(k)) + ?2 ??
?2where yj is the gold label and y?j is the predicted one.We adopt the diagonal variant of AdaGrad (Duchi etal., 2011) for the optimization process.3 Experiments3.1 SettingAs for the benchmark dataset, Penn Discourse Tree-bank (PDTB) (Prasad et al, 2008) corpus1 is usedfor evaluation.
In the PDTB, each discourse relationis annotated between two argument spans.To be consistent with the setups of prior works,we formulate the implicit relation classification taskas four one-versus-other binary classification prob-lems only using the four top level classes: COM-PARISON (COMP.
), CONTINGENCY (CONT.
), EX-PANSION (EXP.)
and TEMPORAL (TEMP.).
Whiledifferent works include different relations of varyingspecificities, all of them include these four core rela-tions (Pitler et al, 2009).
Following dataset splittingconvention of the previous works, we use sections2-20 for training, sections 21-22 for testing and sec-tions 0-1 for development set.
The proposed modelis possible to be extended for multi-class classifica-tion of discourse parsing, but for the comparisonswith most of previous works, we will follow themand focus on the binary classification problems.For other hyper-parameters of the model andtraining process, we fix the lengths of both the in-put arguments to be 80, and apply truncating orzero-padding when necessary.
The dimensions forword embeddings and POS embeddings are respec-tively 300 and 50, and the embedding layer adoptsa dropout of 0.2.
The word embeddings are initial-ized with pre-trained word vectors using word2vec 2(Mikolov et al, 2013) and other parameters are ran-domly initialized including POS embeddings.
We1http://www.seas.upenn.edu/?pdtb/2http://www.code.google.com/p/word2vecset the starting learning rate to 0.001.
For CNNmodel, we utilize three groups of filters with win-dow widths of (2, 2, 2) and their filter numbers areall set to 1024.
The hyper-parameters are the samefor all models and we do not tune them individually.3.2 Model AnalysisFor transformation of sentence vectors, a sim-ple Multilayer Perceptron (MLP) layer could be astraightforward choice, while more complex neu-ral modules, such as LSTM and highway network,could also be considered.
Our model utilizes aCGNN unit with refined gated mechanism for thetransformation.
Will the proposed CGNN reallybring about further performance improvement?
Wenow answer this question empirically.As shown in Table 1, CNN model usually per-forms well on its own.
Utilizing an MLP layer ora Highway layer could improve the accuracies onCONTINGENCY, EXPANSION, TEMPORARY exceptfor COMPARISON.
Though the primary motivationof Highway is to ease gradient-based training ofhighly deep networks through utilizing gated units,it works merely as an ordinary MLP in the proposedmodel, which explains the reason that it performslike MLP.
Despite one of four classes, COMPAR-ISON, not receiving performance improvement, in-troducing a non-linear transformation layer lets theclassification benefit as a whole.
?CNN+LSTM?
de-notes the method of using LSTM to read the convo-lution sequence (without pooling operation), and iteven does not perform better than MLP.The CGNN achieves the best performance on allclasses including COMPARISON.
It gains 3.97%imrovement on average F1 score using CNN onlymodel.
We assume that CGNN is well-suited towork with CNN, adaptively transforming and com-bining local features detected by the individual fil-ters.3.3 ResultsWe show the main results in Tables 2 and 3.
Themetrics include precision (P), recall (R), accuracy(Acc) and F1 score.
Since not all of these metricsare reported in previous work, the comparisons arecorrespondingly in Table 2 and 3.
Some previouswork merges Entrel with Expansion, which is alsoexplored in our study and noted as EXP.+.2266COMP.
CONT.
EXP.+ TEMP.
AVG.F1 Acc F1 Acc F1 Acc F1 Acc F1 AccPitler et al (2009) 21.96 56.59 47.13 67.30 76.42 63.62 16.76 63.49 40.57 62.75Zhou et al (2010) 31.79 58.22 47.16 48.96 70.11 54.54 20.30 55.48 40.32 54.30P&C (2012) 31.32 74.66 49.82 72.09 79.22 69.14 26.57 79.32 46.73 73.80M&B (2013) 25.40 63.36 46.94 68.09 75.87 62.84 20.23 68.35 42.11 65.66J& (2015) 35.93 70.27 52.78 76.95 80.02 69.80 27.63 87.11 49.09 76.03B&D(2015) 36.36 - 55.76 - 61.76 - 29.30 - 45.80 -Chen et al (2016) 40.17 - 54.76 - 80.62 - 31.32 - 51.72 -Current 41.55 71.22 57.32 73.80 80.96 68.44 35.43 84.32 53.82 74.45Table 2: Comparisons of F1 scores (%) (symbol + means EXP.
with Entrel).P R F1COMP.R&Xue (2014) 27.34 72.41 39.70Zhang et al(2015) 22.00 67.76 33.22Current 29.48 70.39 41.55CONT.R&Xue (2014) 44.52 69.96 54.42Zhang et al(2015) 39.80 75.29 52.04Current 50.69 65.95 57.32EXP.R&Xue (2014) 59.59 85.50 70.23Zhang et al(2015) 56.29 91.11 69.59Current 60.81 86.76 71.50TEMP.R&Xue (2014) 18.52 63.64 28.69Zhang et al(2015) 20.22 62.35 30.54Current 26.63 52.94 35.43AVG.R&Xue (2014) 37.49 72.88 48.26Zhang et al(2015) 34.58 74.13 46.35Current 41.90 69.01 51.45Table 3: Comparisons of F1 scores (%) (EXP.
without Entrel).We compare with best-performed or competitivemodels including both traditional linear methodsand recent neural methods.
For traditional meth-ods: Pitler et al (2009) use several linguistically in-formed features, including polarity tags, Levin verbclasses, length of verb phrases, modality, context,and lexical features; Zhou et al (2010) improve theperformance through predicting connective words asfeatures; Park and Cardie (2012) propose a locally-optimal feature set and further identify factors forfeature extraction that can have a major impact per-formance, including stemming and lexicon look-up;Biran and McKeown (2013) collect word pairs fromarguments of explicit examples to help the learning;Rutherford and Xue (2014) employ Brown clusterpair and coreference patterns for performance en-hancement.
Several neural methods have also beenincluded for comparison: Zhang et al (2015) pro-pose a simplified neural network which has onlythree different pooling operations (max, min, aver-age); Ji and Eisenstein (2015) compute distributedsemantics representation by composition up the syn-tactic parse tree through recursive neural network;Braud and Denis (2015) consider shallow lexicalfeatures and word embeddings.
Chen et al (2016)replace the original words by word embeddings toovercome the data sparsity problem and they alsoutilize gated relevance network to capture the se-mantic interaction between word pairs.
The gatednetwork is different from ours but also works well.Our model achieves F-measure improvementsof 1.85% on COMPARISON, 1.56% on CONTIN-GENCY, 1.27% on EXPANSION, 0.94% on EXPAN-SION+, 4.89% on TEMPORAL, against the state-of-the-art of each class.
We improve by 4.73% on av-erage F1 score when not including ENTREL in EX-PANSION as reported in Table 2 and 3.19% on aver-age F1 score otherwise as reported in Table 3.
Theresults show that our model achieves the best per-formance and especially makes the most remarkableprogress on TEMPORAL.4 ConclusionIn this paper, we propose a stacking gated neural ar-chitecture for implicit discourse relation classifica-tion.
Our model includes convolution and collabo-rative gated neural network.
The analysis and ex-periments show that CNN performs well on its ownand combining CGNN provides further gains.
Ourevaluation on PTDB shows that the proposed modeloutperforms previous state-of-the-art systems.2267ReferencesOr Biran and Kathleen McKeown.
2013.
Aggregatedword pair features for implicit discourse relation dis-ambiguation.
In Proceedings of the 51st Annual Meet-ing of the Association for Computational Linguistics(ACL), pages 69?73, Sofia, Bulgaria, August.Chloe?
Braud and Pascal Denis.
2015.
Comparing wordrepresentations for implicit discourse relation clas-sification.
In Proceedings of the 2015 Conferenceon Empirical Methods in Natural Language Process-ing (EMNLP), pages 2201?2211, Lisbon, Portugal,September.Deng Cai and Hai Zhao.
2016.
Neural Word Segmenta-tion Learning for Chinese.
In Proceedings of the 54thAnnual Meeting of the Association for ComputationalLinguistics (ACL), pages 409?420, Berlin, Germany,August.Changge Chen, Peilu Wang, and Hai Zhao.
2015.
Shal-low discourse parsing using constituent parsing tree.In Proceedings of the CoNLL-15 shared task, pages37?41, Beijing, China, July.Jifan Chen, Qi Zhang, Pengfei Liu, Xipeng Qiu, and Xu-anjing Huang.
2016.
Implicit discourse relation detec-tion via a deep architecture with gated relevance net-work.
In Proceedings of the 54th Annual Meeting ofthe Association for Computational Linguistics (ACL),pages 1726?1735, Berlin, Germany, August.Junyoung Chung, Caglar Gulcehre, KyungHyun Cho,and Yoshua Bengio.
2014.
Empirical evaluation ofgated recurrent neural networks on sequence model-ing.
arXiv preprint arXiv:1412.3555.John Duchi, Elad Hazan, and Yoram Singer.
2011.Adaptive subgradient methods for online learning andstochastic optimization.
The Journal of MachineLearning Research, 12:2121?2159.Sepp Hochreiter and Ju?rgen Schmidhuber.
1997.
LongShort-Term Memory.
Neural computation, 9(8):1735?1780.Peter Jansen, Mihai Surdeanu, and Peter Clark.
2014.Discourse complements lexical semantics for non-factoid answer reranking.
In Proceedings of the 52ndAnnual Meeting of the Association for ComputationalLinguistics (ACL), pages 977?986, Baltimore, Mary-land, June.Yangfeng Ji and Jacob Eisenstein.
2015.
One vectoris not enough: Entity-augmented distributed seman-tics for discourse relations.
Transactions of the Asso-ciation for Computational Linguistics (TACL), 3:329?344.Zhongye Jia and Hai Zhao.
2014.
A Joint Graph Modelfor Pinyin-to-Chinese Conversion with Typo Correc-tion.
In Proceedings of the 52nd Annual Meeting ofthe Association for Computational Linguistics (ACL),pages 1512?1523, Baltimore, Maryland, June.Nal Kalchbrenner, Edward Grefenstette, and Phil Blun-som.
2014.
A convolutional neural network for mod-elling sentences.
In Proceedings of the 52nd AnnualMeeting of the Association for Computational Lin-guistics (ACL), pages 655?665, Baltimore, Maryland,June.Yoon Kim.
2014.
Convolutional neural networks for sen-tence classification.
In Proceedings of the 2014 Con-ference on Empirical Methods in Natural LanguageProcessing (EMNLP), pages 1746?1751, Doha, Qatar,October.Zhongyi Li, Hai Zhao, Chenxi Pang, Lili Wang, andHuan Wang.
2016.
A constituent syntactic parse treebased discourse parser.
In Proceedings of the CoNLL-16 shared task, pages 60?64, Berlin, Germany, Au-gust.Maria Liakata, Simon Dobnik, Shyamasree Saha, ColinBatchelor, and Dietrich Rebholz-Schuhmann.
2013.A discourse-driven content model for summarisingscientific articles evaluated in a complex question an-swering task.
In Proceedings of the 2013 Conferenceon Empirical Methods in Natural Language Process-ing (EMNLP), pages 747?757, Seattle, Washington,USA, October.Ziheng Lin, Hwee Tou Ng, and Min-Yen Kan. 2014.
Apdtb-styled end-to-end discourse parser.
Natural Lan-guage Engineering, 20(02):151?184.Thomas Meyer and Andrei Popescu-Belis.
2012.
Us-ing sense-labeled discourse connectives for statisti-cal machine translation.
In Proceedings of the JointWorkshop on Exploiting Synergies between Informa-tion Retrieval and Machine Translation (ESIRMT) andHybrid Approaches to Machine Translation (HyTra),pages 129?138, Avignon, France, April.Thomas Meyer and Bonnie Webber.
2013.
Implicitationof discourse connectives in (machine) translation.
InProceedings of the Workshop on Discourse in MachineTranslation, pages 19?26, Sofia, Bulgaria, August.Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-rado, and Jeff Dean.
2013.
Distributed representa-tions of words and phrases and their compositionality.In Advances in neural information processing systems(NIPS), pages 3111?3119, South Lake Tahoe, Nevada,US, December.Gabriel Murray, Steve Renals, Jean Carletta, and JohannaMoore.
2006.
Incorporating speaker and discoursefeatures into speech summarization.
In Proceedingsof the main conference on Human Language Technol-ogy Conference of the North American Chapter of theAssociation of Computational Linguistics (NAACL),pages 367?374, New York City, USA, June.2268Joonsuk Park and Claire Cardie.
2012.
Improving im-plicit discourse relation recognition through feature setoptimization.
In Proceedings of the 13th Annual Meet-ing of the Special Interest Group on Discourse and Di-alogue (SIGDIAL), pages 108?112, Seoul, South Ko-rea, July.Emily Pitler, Annie Louis, and Ani Nenkova.
2009.
Au-tomatic sense prediction for implicit discourse rela-tions in text.
In Proceedings of the Joint Conferenceof the 47th Annual Meeting of he Association for Com-putational Linguistics and the 4th International JointConference on Natural Language Processing (ACL-IJCNLP), pages 683?691, Suntec, Singapore, August.Rashmi Prasad, Nikhil Dinesh, Alan Lee, Eleni Milt-sakaki, Livio Robaldo, Aravind K Joshi, and Bon-nie L Webber.
2008.
The Penn Discourse TreeBank2.0.
In Proceedings of the Sixth conference on Inter-national Language Resources and Evaluation (LREC-2008), pages 2961?2968, Marrakech, Morocco, May.Lianhui Qin, Zhisong Zhang, and Hai Zhao.
2016a.Implicit discourse relation recognition with context-aware character-enhanced embeddings.
In the 26th In-ternational Conference on Computational Linguistics(COLING), Osaka, Japan, December.Lianhui Qin, Zhisong Zhang, and Hai Zhao.
2016b.Shallow discourse parsing using convolutional neuralnetwork.
In Proceedings of the CoNLL-16 shared task,pages 70?77, Berlin, Germany, August.Attapol Rutherford and Nianwen Xue.
2014.
Discov-ering implicit discourse relations through brown clus-ter pair representation and coreference patterns.
InProceedings of the 14th Conference of the EuropeanChapter of the Association for Computational Linguis-tics (EACL), pages 645?654, Gothenburg, Sweden,April.Rupesh Kumar Srivastava, Klaus Greff, and Ju?rgenSchmidhuber.
2015.
Highway networks.
arXivpreprint arXiv:1505.00387.Jianxiang Wang and Man Lan.
2016.
Two End-to-endShallow Discourse Parsers for English and Chinesein CoNLL-2016 Shared Task.
In Proceedings of theCoNLL-16 shared task, pages 33?40, Berlin, Germany,August.Rui Wang, Hai Zhao, Bao-Liang Lu, Masao Utiyama, andEiichiro Sumita.
2014.
Neural network based bilin-gual language model growing for statistical machinetranslation.
In Proceedings of the 2014 Conference onEmpirical Methods in Natural Language Processing(EMNLP), pages 189?195, Doha, Qatar, October.Peilu Wang, Yao Qian, Frank K Soong, Lei He, and HaiZhao.
2015.
Word embedding for recurrent neuralnetwork based TTS synthesis.
In 2015 IEEE Inter-national Conference on Acoustics, Speech and Sig-nal Processing (ICASSP), pages 4879?4883, Brisbane,Australia.Peilu Wang, Yao Qian, Frank K. Soong, Lei He, and HaiZhao.
2016a.
Learning distributed word represen-tations for bidirectional LSTM recurrent neural net-work.
In Proceedings of the 2016 Conference of theNorth American Chapter of the Association for Com-putational Linguistics: Human Language Technolo-gies (NAACL-HLT), pages 527?533, San Diego, Cal-ifornia, June.Rui Wang, Masao Utiyama, Isao Goto, Eiichiro Sumita,Hai Zhao, and Bao-Liang Lu.
2016b.
Convertingcontinuous-space language models into n-gram lan-guage models with efficient bilingual pruning for sta-tistical machine translation.
ACM Transactions onAsian and Low-Resource Language Information Pro-cessing, 15(3):11.Nianwen Xue, Hwee Tou Ng, Sameer Pradhan, RashmiPrasad, Christopher Bryant, and Attapol Rutherford.2015.
The CoNLL-2015 Shared Task on ShallowDiscourse Parsing.
In Proceedings of the CoNLL-15shared task, pages 1?16, Beijing, China, July.Nianwen Xue, Hwee Tou Ng, Sameer Pradhan, AttapolRutherford, Bonnie Webber, Chuan Wang, and Hong-min Wang.
2016.
CoNLL 2016 Shared Task on Multi-lingual Shallow Discourse Parsing.
In Proceedings ofthe CoNLL-16 shared task, pages 1?19, Berlin, Ger-many, August.Yasuhisa Yoshida, Jun Suzuki, Tsutomu Hirao, andMasaaki Nagata.
2014.
Dependency-based discourseparser for single-document summarization.
In Pro-ceedings of the 2014 Conference on Empirical Meth-ods in Natural Language Processing (EMNLP), pages1834?1839, Doha, Qatar, October.Biao Zhang, Jinsong Su, Deyi Xiong, Yaojie Lu, HongDuan, and Junfeng Yao.
2015.
Shallow convolu-tional neural network for implicit discourse relationrecognition.
In Proceedings of the 2015 Conferenceon Empirical Methods in Natural Language Process-ing (EMNLP), pages 2230?2235, Lisbon, Portugal,September.Jingyi Zhang, Masao Utiyama, Eiichro Sumita, Hai Zhao,Graham Neubig, and Satoshi Nakamura.
2016a.Learning local word reorderings for hierarchicalphrase-based statistical machine translation.
MachineTranslation, pages 1?18.Zhisong Zhang, Hai Zhao, and Lianhui Qin.
2016b.Probabilistic graph-based dependency parsing withconvolutional neural network.
In Proceedings of the54th Annual Meeting of the Association for Compu-tational Linguistics (ACL), pages 1382?1392, Berlin,Germany, August.Zhi-Min Zhou, Yu Xu, Zheng-Yu Niu, Man Lan, Jian Su,and Chew Lim Tan.
2010.
Predicting discourse con-2269nectives for implicit discourse relation recognition.
InProceedings of the 23rd International Conference onComputational Linguistics (COLING), pages 1507?1514, Beijing, China, August.2270
