Semiring ParsingJ oshua  Goodman*Microsoft ResearchWe synthesize work on parsing algorithms, deductive parsing, and the theory of algebra ppliedto formal languages into a general system for describing parsers.
Each parser performs abstractcomputations u ing the operations of a semiring.
The system allows a single, simple representationto be used for describing parsers that compute recognition, derivation forests, Viterbi, n-best,inside values, and other values, simply by substituting the operations of different semirings.
Wealso show how to use the same representation, i terpreted ifferently, to compute outside values.The system can be used to describe a wide variety of parsers, including Earley's algorithm, treeadjoining rammar parsing, Graham Harrison Ruzzo parsing, and prefix value computation.1.
IntroductionFor a given grammar and string, there are many interesting quantities we can compute.We can determine whether the string is generated by the grammar; we can enumerateall of the derivations of the string; if the grammar is probabilistic, we can compute theinside and outside probabilities of components of the string.
Traditionally, a differentparser description has been needed to compute ach of these values.
For some parsers,such as CKY parsers, all of these algorithms (except for the outside parser) stronglyresemble ach other.
For other parsers, such as Earley parsers, the algorithms forcomputing each value are somewhat different, and a fair amount of work can berequired to construct each one.
We present a formalism for describing parsers uchthat a single simple description can be used to generate parsers that compute all ofthese quantities and others.
This will be especially useful for finding parsers for outsidevalues, and for parsers that can handle general grammars, like Earley-style parsers.Although our description format is not limited to context-free grammars (CFGs),we will begin by considering parsers for this common formalism.
The input string willbe denoted wlw2... Wn.
We will refer to the complete string as the sentence.
A CFG Gis a 4-tuple (N, ~, R, S) where N is the set of nonterminals including the start symbolS, ~ is the set of terminal symbols, and R is the set of rules, each of the form A --* afor A c N and a E (N U ~)*.
We will use the symbol ~ for immediate derivation andfor its reflexive, transitive closure.We will illustrate the similarity of parsers for computing different values usingthe CKY algorithm as an example.
We can write this algorithm in its iterative formas shown in Figure 1.
Here, we explicitly construct a Boolean chart, chart\[1..n, 1..IN I,1..n + 1\].
Element chart\[i,A,j\] contains TRUE if and only if A G wi.. .
wj-1.
The algo-rithm consists of a first set of loops to handle the singleton productions, a second set ofloops to handle the binary productions, and a return of the start symbol's chart entry.Next, we consider probabilistic grammars, in which we associate a probabilitywith every rule, P(A --* a).
These probabilities can be used to associate a probability* One Microsoft Way, Redmond, WA 98052.
E-mail: joshuago@microsoft.comQ 1999 Association for Computational LinguisticsComputational Linguistics Volume 25, Number 4boolean chart\[1..n, 1..IN I, 1..n+1\] := FALSE;for s := 1 to n/* start position */for each rule A -+ ws c Rchart\[s, A, s+l \ ]  := TRUE;for l := 2 to n/*  length, shortest o longest */for s := 1 to n-l+1/*startposit ion */for t := 1 to / - 1/* split length */for each rule A -+ BC ?
R/* extra TRUE for expository purposes */chart\[s, A, s.l.l\] := chart\[s, A, s+l\] V(chart\[s, B, s + t\] A chart\[s ?t, C, s + l\] A TRUE);return chart\[l, S, n+ 1\];Figure 1CKY recognition algorithm.float chart\[1..n, 1..IN\[, 1..n?1\] := 0;for s := I to n/* start position */for each rule A --+ ws E Rchart\[s, A, s+l \ ]  := P(A --+ ws);for / := 2 to n/*  length, shortest o longest */for s := I to n- l+  l /* start position */for t := 1 to 1 - 1/* split length */for each rule A -+ BC c Rchart\[s, A, s+l\] := chart\[s, A, s+l\] +(chart\[s, B, s+t\] x chart\[s+t, C, s+l\] x P(A -+ BC));return chart\[l, S, n+ 1\];Figure 2CKY inside algorithm.with a particular derivation, equal to the product of the rule probabilities used in thederivation, or to associate a probability with a set of derivations, A ~ wi.
?
?
wj-1 equalto the sum of the probabilities of the individual derivations.
We call this latter prob-ability the inside probability of i,A,j.
We can rewrite the CKY algorithm to computethe inside probabilities, as shown in Figure 2 (Baker 1979; Lari and Young 1990).Notice how similar the inside algorithm is to the recognition algorithm: essentially,all that has been done is to substitute + for V, x for A, and P(A ~ ws) and P(A ~ BC)for TRUE.
For many parsing algorithms, this, or a similarly simple modification, is allthat is needed to create a probabilistic version of the algorithm.
On the other hand, asimple substitution is not always sufficient.
To give a trivial example, if in the CKYrecognition algorithm we had writtenchart\[s,A,s?l\] := chart\[s,A,s?l\] V chart\[s,B,s?t\] A chart\[s+t,C,s?l\];instead of the less naturalchart\[s, A, s?l\] := chart\[s,A, s l,l\] V chart\[s, B, s+t\] A chart\[s+t, C, s-t-l\] A TRUE;larger changes would be necessary to create the inside algorithm.Besides recognition, four other quantities are commonly computed by parsingalgorithms: derivation forests, Viterbi scores, number of parses, and outside proba-bilities.
The first quantity, a derivation forest, is a data structure that allows one to574Goodman Semiring Parsingefficiently compute the set of legal derivations of the input string.
The derivation for-est is typically found by modifying the recognition algorithm to keep track of "backpointers" for each cell of how it was produced.
The second quantity often computedis the Viterbi score, the probability of the most probable derivation of the sentence.This can typically be computed by substituting x for A and max for V. Less commonlycomputed is the total number of parses of the sentence, which, like the inside values,can be computed using multiplication and addition; unlike for the inside values, theprobabilities of the rules are not multiplied into the scores.
There is one last commonlycomputed quantity, the outside probabilities, which we will describe later, in Section 4.One of the key points of this paper is that all five of these commonly com-puted quantities can be described as elements of complete semirings (Kuich 1997).The relationship between grammars and semirings was discovered by Chomsky andSchiitzenberger (1963), and for parsing with the CKY algorithm, dates back to Teit-elbaum (1973).
A complete semiring is a set of values over which a multiplicativeoperator and a commutative additive operator have been defined, and for which infi-nite summations are defined.
For parsing algorithms satisfying certain conditions, themultiplicative and additive operations of any complete semiring can be used in placeof A and V, and correct values will be returned.
We will give a simple normal formfor describing parsers, then precisely define complete semirings, and the conditionsfor correctness.We now describe our normal form for parsers, which is very similar to that usedby Shieber, Schabes, and Pereira (1995) and by Sikkel (1993).
This work can be thoughtof as a generalization from their work in the Boolean semiring to semirings in general.In most parsers, there is at least one chart of some form.
In our normal form, wewill use a corresponding, equivalent concept, items.
Rather than, for instance, a chartelement chart\[i,A,j\], we will use an item \[i,A,j\].
Furthermore, rather than use explicit,procedural descriptions, uch aschart\[s,A,s+l\] := chart\[s,A,s+l\] V chart\[s,B,s+t\] A chart\[s+t,C,s+l\] A TRUEwe will use inference rules such asR(A ~ BC) \[i,B,k\] \[k,C,j\]\[i,A,j\]The meaning of an inference rule is that if the top line is all true, then we can concludethe bottom line.
For instance, this example inference rule can be read as saying that ifA ~ BC and B G wi .
.
.
Wk-1 and C ~ wk.
.
.
wj-1, then A G w l .
.
.
Wj_l.The general form for an inference rule will beA1 " .
AkBwhere if the conditions A1 ... Ak are all true, then we infer that B is also true.
The Aican be either items, or (in an extension of the usual convention for inference rules)rules, such as R(A ~ BC).
We write R(A ---* BC) rather than A --~ BC to indicate thatwe could be interested in a value associated with the rule, such as the probability ofthe rule if we were computing inside probabilities.
If an Ai is in the form R(...), wecall it a rule.
All of the Ai must be rules or items; when we wish to refer to both rulesand items, we use the word terms.We now give an example of an item-based description, and its semantics.
Figure 3gives a description of a CKY-style parser.
For this example, we will use the inside575Computational Linguistics Volume 25, Number 4Item form:\[i, A, j\]Goal:\[1, S, n + 1\]Rules:R(A -+ wi){i,A,i+l\]R(A --+ BC) \[i, B, k\] \[k, C, j\]\[i, A, j\]Figure 3Item-based description of a CKY parser.UnaryBinarysemiring, whose additive operator is addition and whose multiplicative operator ismultiplication.
We use the input string xxx to the following grammar:S ~ XX 1.0X --* XX 0.2X --* x 0.8(1)Our first step is to use the unary rule,R(A wi)\[i,A,i+l\]The effect of the unary rule will exactly parallel the first set of loops in the CKY insidealgorithm.
We will instantiate the free variables of the unary rule in every possibleway.
For instance, we instantiate the free variable i with the value 1, and the freevariable A with the nonterminal X.
Since wl = x, the instantiated rule is thenR(x x)\[1,X,2\]Because the value of the top line of the instantiated unary rule, R(X ---, x), has value0.8, we deduce that the bottom line, \[1,X, 2\], has value 0.8.
We instantiate the rule intwo other ways, and compute the following chart values:\[1,X,2\] = 0.8\[2,X,3\] = 0.8\[3,X,4\] = 0.8Next, we will use the binary rule,R(A --* BC) \[i, B, k\] \[k, C,j\]\[i,A,j\]The effect of the binary rule will parallel the second set of loops for the CKY insidealgorithm.
Consider the instantiation i = 1, k -- 2, j = 3, A -- X, B = X, C -- X,R(X ~ XX) \[1, X, 2\] \[2, X, 3\]\[1,X,3\]576Goodman Semiring ParsingWe use the multiplicative operator of the semiring of interest o multiply together thevalues of the top line, deducing that \[1, X, 3\] = 0.2 x 0.8 x 0.8 = 0.128.
Similarly,\[1,X,3\] = 0.128\[2,X,4\] = 0.128\[1,S,3\] -- 0.64\[2,S,4\] = 0.64There are two more ways to instantiate the conditions of the binary rule:R(S --~ XX)  \[1, X, 2\] \[2, X,4\]\[1, S, 4\]R(S --+ XX)  \[1,X,3\] \[3, X,4\]\[1, S, 4\]The first has the value 1 x 0.8 x 0.128 = 0.1024, and the second also has the value0.1024.
When there is more than one way to derive a value for an item, we use theadditive operator of the semiring to sum them up.
Thus, \[1, S, 4\] -- 0.2048.
Since \[1, S, 4\]is the goal item for the CKY parser, we know that the inside value for xxx is 0.2048.The goal item exactly parallels the return statement of the CKY inside algorithm.1.1 Earley ParsingMany parsers are much more complicated than the CKY parser, and we will need toexpand our notation a bit to describe them.
Earley's algorithm (Earley 1970) exhibitsmost of the complexities we wish to discuss.
Earley's algorithm is often described asa bottom-up arser with top-down filtering.
In a probabilistic framework, the bottom-up sections compute probabilities, while the top-down filtering nonprobabilisticallyremoves items that cannot be derived.
To capture these differences, we expand ournotation for deduction rules, to the following:a l " "ak  C1.
.
.C jBC1 ""  Cj are side conditions, interpreted nonprobabilistically, while A1 .-- Ak are mainconditions with values in whichever semiring we are using.
1 While the values of allmain conditions are multiplied together to yield the value for the item under the line,the side conditions are interpreted in a Boolean manner: if all of them are nonzero,the rule can be used, but if any of them are zero, it cannot be.
Other than for checkingwhether they are zero or nonzero, their values are ignored.Figure 4 gives an item-based escription of Earley's parser.
We assume the additionof a distinguished nonterminal S' with a single rule S' --+ S. An item of the form\[i,A --, c~ ,J fl, j\] asserts that A ~ aft G w i .
.
.
wj- l f l .1 The side conditions may depend on any purely local informat ion-- the values of A1 .
.
.
Ak, B, orC1 ... Cj, as well as constant global functions, such as R(X) ~6 sin(Y) (assuming here X and Y arevariables in the A, B, C).
The side conditions usual ly cannot depend on any contextual information,such as the grandfather of A1, which would not be well defined, since there might  be many  derivationsof A1.
Of course, one could encode the grandfather of A1 as a variable in the item A1, and then have adependency on that variable.
This would  guarantee that the context was unique and well defined.577Computational Linguistics Volume 25, Number 4I tem form:\ [ i ,A -~ a .
fl, j\]Goal:\[1,s' ~ S. ,n+l\]Rules:\[1, S' -~ ?
S, 1\]\[i, A -~ a ?
wj f l ,  j\]\ [ i ,A -~ awj  ?
fl, j+ l \ ]R(B  --+ "7) \[i, A ~ a ?
Bfl ,  j\]\[j,B ~ -'7,j\]\[i, A --+ a ?
Bf l ,  k l \[k, B ~ "7 ?
,  j\]\[i, A -+ aB  ?
fl, j\]Figure 4Item-based description of Earley parser.InitializationScanningPredictionCompletionThe prediction rule includes a side condition, making it a good example.
Therule is:R (B~'7)  \ [ i ,A~a.
Bfl, j\] ~,--~ 7_~ .
~,j\]Through the prediction rule, Earley's algorithm guarantees that an item of the form~', B -+ ?
'7,j\] can only be produced if S ~ Wl .
.
.
w j_ lB6  for some 6; this top-downfiltering leads to significantly more efficient parsing for some grammars than the CKYalgorithm.
The prediction rule combines ide and main conditions.
The side condi-tion, \[ i ,A --+ ce ?
Bfl,j\], provides the top-down filtering, ensuring that only items thatmight be used later by the completion rule can be predicted, while the main con-dition, R(B --+ "7), provides the probability of the relevant rule.
The side conditionis interpreted in a Boolean fashion, while the main condition's actual probability isused.Unlike the CKY algorithm, Earley's algorithm can handle grammars with ep-silon (e), unary, and n-ary branching rules.
In some cases, this can significantly com-plicate parsing.
For instance, given unary rules A --+ B and B --+ A, a cycle ex-ists.
This kind of cycle may allow an infinite number of different derivations, re-quiring an infinite summation to compute the inside probabilities.
The ability ofitem-based parsers to handle these infinite loops with relative ease is a majorattraction.1.2 Overv iewThis paper will simplify the development of new parsers in three important ways.First, it will simplify specification of parsers: the item-based escription is simplerthan a procedural description.
Second, it will make it easier to generalize parsersacross tasks: a single item-based escription can be used to compute values for avariety of applications, simply by changing semirings.
This will be especially ad-vantageous for parsers that can handle loops resulting from rules like A --+ A andcomputations resulting from ?
productions, both of which typically lead to infinitestuns.
In these cases, the procedure for computing an infinite sum differs from semi-578Goodman Semiring Parsingring to semiring, and the fact that we can specify that a parser computes an in-finite sum separately from its method of computing that sum will be very help-ful.
The third use of these techniques is for computing outside probabilities, val-ues related to the inside probabilities that we will define later.
Unlike the otherquantities we wish to compute, outside probabilities cannot be computed by sim-ply substituting a different semiring into either an iterative or item-based escrip-tion.
Instead, we will show how to compute the outside probabilities using a mod-ified interpreter of the same item-based escription used for computing the othervalues.In the next section, we describe the basics of semiring parsing.
In Section 3, wederive formulas for computing most of the values in semiring parsers, except out-side values, and then in Section 4, show how to compute outside values as well.
InSection 5, we give an algorithm for interpreting an item-based escription, followedin Section 6 by examples of using semiring parsers to solve a variety of problems.Section 7 discusses previous work, and Section 8 concludes the paper.2.
Semiring ParsingIn this section we first describe the inputs to a semiring parser: a semiring, an item-based description, and a grammar.
Next, we give the conditions under which a semi-ring parser gives correct results.
At the end of this section we discuss three especiallycomplicated and interesting semirings.2.1 SemiringIn this subsection, we define and discuss semirings (see Kuich \[1997\] for an intro-duction).
A semiring has two operations,  and ?, that intuitively have most (butnot necessarily all) of the properties of the conventional + and x operations on thepositive integers.
In particular, we require the following properties: ?
is associativeand commutative; ?
is associative and distributes over G. If @ is commutative, wewill say that the semiring is commutative.
We assume an additive identity element,which we write as 0, and a multiplicative identity element, which we write as 1.
Bothaddition and multiplication can be defined over finite sets of elements; if the set isempty, then the value is the respective identity element, 0 or 1.
We also assume thatx @ 0 = 0 ?
x = 0 for all x.
In other words, a semiring is just like a ring, except hat theadditive operator need not have an inverse.
We will write /A, ?, ?, 0,1 / to indicate asemiring over the set A with additive operator ?, multiplicative operator @, additiveidentity 0, and multiplicative identity 1.For parsers with loops, i.e., those in which an item can be used to derive itself,we will also require that sums of an infinite number of elements be well defined.
Inparticular, we will require that the semirings be complete (Kuich 1997, 611).
This meansthat sums of an infinite number of elements hould be associative and commutative,just like finite sums, and that multiplication should distribute over infinite sums, justas it does over finite ones.
All of the semirings we will deal with in this paper arecomplete.
2All of the semirings we discuss here are also w-continuous.
Intuitively, this meansthat if any partial sum of an infinite sequence is less than or equal to some value,2 Completeness i  a somewhat s ronger condition than we really need; we could, instead, require thatlimits be appropriately defined for those infinite sums that occur while parsing, but this weakercondition is more complicated todescribe precisely.579Computational Linguistics Volume 25, Number 4booleaninsideViterbicountingderivation forestViterbi-derivationViterbi-n-bestFigure 5({TRUE, FALSE }, V, A, FALSE, TRUE)+, x, o, 1>(II~, max, x, O, 1)( I~ ,  +, ?,0, 1){0}>(l~ 1 x 2E, max, x, (0,0>, (1, {(>}>>Vii Vit({topn(X)IX E 2~ x~}, max, x ,  0,Vit-n{0,{<>}>}>Semirings used: {A, @, ?, 0,1/.recognitionstring probabilityprob.
of best derivationnumber of derivationsset of derivationsbest derivationbest n derivationsthen the infinite sum is also less than or equal to that value.
3This important propertymakes it easy to compute, or at least approximate, infinite sums.There will be several especially useful semirings in this paper, which are definedin Figure 5.
We will write P~ to indicate the set of real numbers from a to b inclusive,with similar notation for the natural numbers, N. We will write E to indicate theset of all derivations in some canonical form, and 2 n to indicate the set of all setsof derivations in canonical form.
There are three derivation semirings: the derivationforest semiring, the Viterbi-derivation semiring, and the Viterbi-n-best semiring.
Theoperators used in the derivation semirings (., max, x, max, and x ) will be describedVit Vit Vit-n Vit-nlater, in Section 2.5.The inside semiring includes all nonnegative r al numbers, to be closed underaddition, and includes infinity to be closed under infinite sums, while the Viterbisemiring contains only numbers up to 1, since under max this still leads to closure.The three derivation forest semirings can be used to find especially important val-ues: the derivation forest semiring computes all derivations of a sentence; the Viterbi-derivation semiring computes the most probable derivation; and the Viterbi-n-bestsemiring computes the n most probable derivations.
A derivation is simply a listof rules from the grammar.
From a derivation, a parse tree can be derived, so thederivation forest semiring is analogous to conventional parse forests.
Unlike the othersemirings, all three of these semirings are noncommutative.
The additive operationof these semirings is essentially union or maximum, while the multiplicative oper-ation is essentially concatenation.
These semirings are described in more detail inSection 2.5.2.2 Item-based DescriptionA semiring parser equires an item-based description of the parsing algorithm, in theform given earlier.
So far, we have skipped one important detail of semiring parsing.
Ina simple recognition system, as used in deduction systems, all that matters is whetheran item can be deduced or not.
Thus, in these simple systems, the order of processingitems is relatively unimportant, as long as some simple constraints are met.
On theother hand, for a semiring such as the inside semiring, there are important orderingconstraints: we cannot compute the inside value of an item until the inside values of3 To be more precise, all semirings we discuss here are naturally ordered, meaning that we can define apartial ordering, _U, such that x _U y if and only if there exists z such that x @ z ---- y.
We call a naturallyordered complete semiring w-continuous (Kuich 1997, 612) if for any sequence Xl, x2 .
.
.
.
and for anyconstant y, if for all n, (~o<_i<_n xi U_ y, then (~ i  xi U_ y.580Goodman Semiring Parsingall of its children have been computed.Thus, we need to impose an ordering on the items, in such a way that no itemprecedes any item on which it depends.
We will assign each item x to a "bucket"B, writing bucket(x) = B and saying that item x is associated with B.
We order thebuckets in such a way that if item y depends on item x, then bucket(x) <_ bucket(y).
Forsome pairs of items, it may be that both depend, directly or indirectly, on each other;we associate these items with special "looping" buckets, whose values may requireinfinite sums to compute.
We will also call a bucket looping if an item associated withit depends on itself.One way to achieve a bucketing with the required ordering constraints (suggestedby Fernando Pereira) is to create a graph of the dependencies, with a node for eachitem, and an edge from each item x to each item b that depends on it.
We thenseparate the graph into its strongly connected components (maximal sets of nodes allreachable from each other), and perform a topological sort.
Items forming singletonstrongly connected components are associated with their own buckets; items formingnonsingleton strongly connected components are associated with the same loopingbucket.
See also Section 5.Later, when we discuss algorithms for interpreting an item-based escription, wewill need another concept.
Of all the items associated with a bucket B, we will beable to find derivations for only a subset.
If we can derive an item x associated withbucket B, we write x E B, and say that item x is in bucket B.
For example, the goalitem of a parser will almost always be associated with the last bucket; if the sentenceis grammatical, the goal item will be in the last bucket, and if it is not grammatical, itwill not be.It will be useful to assume that there is a single, variable-free goal item, and thatthis goal item does not occur as a condition for any rules.
We could always add a\[old-goal\]new goal item ~oal\] and a rule ~oal\] where \[old-goal\] is the goal in the originaldescription.2.3 The GrammarA semiring parser also requires a grammar as input.
We will need a list of rules in thegrammar, and a function, R(rule), that gives the value for each rule in the grammar.This latter function will be semiring-specific.
For instance, for computing the insideand Viterbi probabilities, the value of a grammar ule is just the conditional probabilityof that rule, or 0 if it is not in the grammar.
For the Boolean semiring, the value isTRUE if the rule is in the grammar, FALSE otherwise.
R(rule) replaces the set of rulesR of a conventional grammar description; a rule is in the grammar if R(rule) ~ O.2.4 Conditions for Correct ProcessingWe will say that a semiring parser works correctly if, for any grammar, input, andsemiring, the value of the input according to the grammar equals the value of the inputusing the parser.
In this subsection, we will define the value of an input accordingto the grammar, define the value of an input using the parser, and give a sufficientcondition for a semiring parser to work correctly.
From this point onwards, unless wespecifically mention otherwise, we will assume that some fixed semiring, item-baseddescription, and grammar have been given, without specifically mentioning whichones.2.4.1 Value According to Grammar.
Consider a derivation E, consisting of grammarrules el, e2 .
.
.
.
.
era.
We define the value of the derivation according to the grammar to581Computational Linguistics Volume 25, Number 4be simply the product (in the semiring) of the values of the rules used in E:mVG(E) : @ R(ei)i:1Then we can define the value of a sentence that can be derived using grammar deriva-tions E 1, E 2 .
.
.
.
.
E k to be:kv~ = (D v~(EJ)j=1where k is potentially infinite.
In other words, the value of the sentence according tothe grammar is the sum of the values of all derivations.
We will assume that in eachgrammar formalism there is some way to define derivations uniquely; for instance, inCFGs, one way would be using left-most derivations.
For simplicity, we will simplyrefer to derivations, rather than, for example, left-most derivations, since we are neverinterested in nonunique derivations.A short example will help clarify.
We consider the following grammar:s ~ AA a(S-+AA)A --+ AA a(A-+AA)A --+ a R(A-+a)(2)and the input string aaa.
There are two grammar derivations, the first of which~S- -+AA , ,A - -+AA , , --A---+a .
.A---+a --A---+a is ~ => Am ~ AAA ~ aAA ~ aaA ~ aaa, which has value R(S --+ AA)  ?
R(A  --+AA)  ?
R(A  --+ a) ?
R(A  --+ a) ?
R(A  --+ a).
Notice that the rules in the value arethe same rules in the same order as in the derivation.
The other grammar deriva-~S- -*AA- -  ~A- -*a  ~A- -+AA __ ~A- -*a  __A--*a tion is ~ ~ .4.4 ~ aA => aAA ~ aaA => aaa, which has value R(S --+ AA)  ?
R(A --+a) ?
R(A  -+ AA)  ?
R(A  --+ a) ?
R(A  ---* a).
The value of the sentence is the sum of thevalues of the two derivations,\[R(s --+ AA) ?
R(A -+ AA) 0 a(A --+ a) ?
R(A --+ ~) ?
R(A --+ a)\] ?\[a(S --+ AA) O R(A --+ a) ?
R(A --+ AA) ?
R(A -* a) ?
R(A --+ ~)\]2.4.2 I tem Derivat ions.
Next, we define item derivations, i.e., derivations using theitem-based escription of the parser.
We define item derivation in such a way thatfor a correct parser description, there is exactly one item derivation for each grammarderivation.
The value of a sentence using the parser is the sum of the value of allitem derivations of the goal item.
Just as with grammar derivations, individual itemderivations are finite, but there may be infinitely many item or grammar derivationsof a sentence.We say that ~ Cl.. .
cj is an instantiation of deduction rule A1 .B.
Ak C1.
.
.
Cjwhenever the first expression is a variable-free instance of the second; that is, the firstexpression is the result of consistently substituting constant terms for each variable inthe second.
Now, we can define an i tem derivation tree.
Intuitively, an item derivation582Goodman Semiring ParsingsS--AA----A-AA------A-.-a - - - -A.
-+a --A.--~a =:~ A  :=~ AA ::~ aAA =~ aaA =:~ aaaGrammar  DerivationR(S --+ AA)R(A ~ ~ - ~  a)a)Grammar  Derivation Tree\[1, S, 4\]R( S ,4\]- - + ~ A ~  R ( A --~ a)R(A ,3\]I I_ R(A--+a) _R(A~a)I tem Derivation \ ] teeR(S -~ AA) ?
R(A ~ AA) ?
R(A --+ a) ?
R(A ~ a) ?
R(A -+ a)Derivation ValueFigure 6Grammar derivation, grammar derivation tree, item derivation tree, and derivation value.tree for x just gives a way of deducing x from the grammar  ules.
We define anitem derivation tree recursively.
The base case is rules of the grammar:  (r / is an itemderivation tree, where r is a rule of the grammar.
Also, if Dal .
.
.
.
.
Da k, Dcl .
.
.
.
.
Dcj arederivation trees headed by al... ak, Cl... Cj respectively, and if ~c l .
.
.
cj is theinstantiation of a deduction rule, then (b: D~ 1 .
.
.
.
.
D~k/ is also a derivation tree.
Noticethat the De1 ?
?.
Dq do not occur in this tree: they are side conditions, and although theirexistence is required to prove that cl ?
.. cj could be derived, they do not contribute tothe value of the tree.
We will write a l .
.
.
ak b to indicate that there is an item deri-vation tree of the form (b: Da, .
.
.
.
.
Dakl.
As mentioned in Section 2.2, we will writex E B if bucket(x) =B and there is an item derivation tree for x.We can continue the example of parsing aaa, now using the item-based CKY parserof Figure 3.
There are two item derivation trees for the goal item; in Figure 6, we givethe first as an example, displaying it as a tree, rather than with angle bracket notation,for simplicity.Notice that an item derivation is a tree, not a directed graph.
Thus, an item sub-derivation could occur multiple times in a given item derivation.
This means that583Computational Linguistics Volume 25, Number 4we can have a one-to-one correspondence between item derivations and grammarderivations; loops in the grammar  lead to an infinite number  of g rammar  derivations,and an infinite number  of corresponding item derivations.A grammar  including rules such asS --, AAAA --+ BA ~ aB --* AB --,would allow derivations uch as S ~ AAA ~ BAA ~ AA ~ BA ~ A ~ B ~ e.We would include the exact same item derivation showing A ~ B ~ ~ three times.Similarly, for a derivation such as A ~ B ~ A ~ B ~ A =~ a, we would have acorresponding item derivation tree that included multiple uses of the A --* B andB --* A rules.2.4.3 Value of I tem Derivation.
The value of an item derivation D, V(D), is the productof the value of its rules, R(r), in the same order that they appear  in the item derivationtree.
Since rules occur only in the leaves of item derivation trees, the order is preciselydetermined.
For an item derivation tree D with rule values dl, d2 .
.
.
.
.
dj as its leaves,JV(D) = @ R(di)i=1(3)Alternatively, we can write this equation recursively as\[R(D) if D is a ruleV(D) = I@~--1 V(Di) if D = (b: D1, .
.
.
,  Dk} (4)Continuing our example, the value of the item derivation tree of Figure 6 isR(s AA) ?
R(A a) ?
R(A AA) ?
R(A a) ?
R(A a)the same as the value of the first g rammar  derivation.Let inner(x) represent the set of all item derivation trees headed by an item x. Thenthe value of x is the sum of all the values of all i tem derivation trees headed by x.Formally,V(x)= V(D)DEinner(x)The value of a sentence is just the value of the goal item, V(goal).2.4.4 I so -va lued  Der ivat ions .
In certain cases, a particular g rammar  derivation and aparticular item derivation will have the same value for any semiring and any rule valuefunction R. In this case, we say that the two derivations are iso-valued.
In particular, ifand only if the same rules occur in the same order in both derivations, then their valueswill always be the same, and they are iso-valued.
In Figure 6, the grammar  derivationand item derivation meet this condition.
In some cases, a g rammar  derivation and an584Goodman Semiring Parsingitem derivation will have the same value for any commutative s miring and any rulevalue function.
In this case, we say that the derivations are commutatively iso-valued.Finishing our example, the value of the goal item given our example sentence isjust the sum of the values of the two item-based erivations,\[R(S ---* AA) @ R(A --~ AA) @ R(A --~ a) @ R(A ~ a) @ R(A ---* a)\] @\[R(S ~ AA) ?
R(A ~ a) ?
R(A - .
AA) ?
R(A ~ a) ?
R(A ~ a) lThis value is the same as the value of the sentence according to the grammar.2.4.5 Conditions for Correctness.
We can now specify the conditions for an item-baseddescription to be correct.Theorem 1Given an item-based escription I, if for every grammar G, there exists a one-to-onecorrespondence b tween the item derivations using I and the grammar derivations,and the corresponding derivations are iso-valued, then for every complete semiring,the value of a given input wl ... wn is the same according to the grammar as the valueof the goal item.
(If the semiring is commutative, then the corresponding derivationsneed only be commutatively iso-valued.
)ProofThe proof is very simple; essentially, each term in each sum occurs in the other.
Byhypothesis, for a given input, there are grammar derivations E1 .
.
.
Ek (for 0 < k < o0)and corresponding item derivation trees D1 .. ?
Dk of the goal item.
Since correspondingitems are iso-valued, for all i, V(Ei) ~- V(Di).
(If the semiring is commutative, thensince the items are commutatively iso-valued, it is still the case that for all i, V(Ei) --V(Di).)
Now, since the value of the string according to the grammar is just (~i V(Ei) =(~i V(Di), and the value of the goal item is E)i V(Di), the value of the string accordingto the grammar equals the value of the goal item.
\[\]There is one additional condition for an item-based escription to be usable inpractice, which is that there be only a finite number of derivable items for a giveninput sentence; there may, however, be an infinite number of derivations of any item.2.5 The Derivation SemiringsAll of the semirings we use should be familiar, except for the derivation semirings,which we now describe.
These semirings, unlike the other semirings described inFigure 5, are not commutative under their multiplicative operator, concatenation.In many parsers, it is conventional to compute parse forests: compact represen-tations of the set of trees consistent with the input.
We will use a related concept,derivation forests, a compact representation f the set of derivations consistent withthe input, which corresponds to the parse forest for CFGs, but is easily extended toother formalisms.Often, we will not be interested in the set of all derivations, but only in the mostprobable derivation.
The Viterbi-derivation semiring computes this value.
Alterna-tively, we might want the n best derivations, which would be useful if the output ofthe parser were passed to another stage, such as semantic disambiguation; this valueis computed by the Viterbi-n-best derivation semiring.Notice that each of the derivation semirings can also be used to create trans-ducers.
That is, we simply associate strings rather than grammar ules with each585Computational Linguistics Volume 25, Number 4rule value.
Instead of grammar rule concatenation, we perform string concatena-tion.
The derivation semiring then corresponds to nondeterministic transductions;the Viterbi semiring corresponds to a weighted or probabilistic transducer; and theViterbi-n-best semiring could be used to get n-best lists from probabilistic transduc-ers.2.5.1 Derivation Forest.
The derivation forest semiring consists of sets of derivations,where a derivation is a list of rules of the grammar.
4 Sets containing one rule, such as{ (X --* YZ)} for a CFG, constitute the primitive elements of the semiring.
The additiveoperator kJ produces a union of derivations, and the multiplicative operator- producesthe concatenation, one derivation concatenated with the next.
The concatenation op-eration (.)
is defined on both derivations and sets of derivations; when applied to aset of derivations, it produces the set of pairwise concatenations.
The additive identityis simply the empty set, 0: union with the empty set is an identity operation.
Themultiplicative identity is the set containing the empty derivation, {0}: concatenationwith the empty derivation is an identity operation.
Derivations need not be complete.For instance, for CFGs, {(X --* YZ, Y ~ y)} is a valid element, as is {(Y --* y, X ~ x)}.In fact, {(X ~ A, B --* b)} is a valid element, although it could not occur in a validgrammar derivation, or in a correctly functioning parser.
An example of concatenationof sets is {(A ~ a),(B ~ b)}.
{(C ~ c),(D ~ d)} = {(A ~ a,C -+ c),(A --* a,Da), (B b, C c), (B b, D - .
a)}.Potentially, derivation forests are sets of infinitely many items.
However, it is stillpossible to store them using finite-sized representations.
Elsewhere (Goodman 1998),we show how to implement derivation forests efficiently, using pointers, in a manneranalogous to the typical implementation of parse forests, and also similar to the workof Billot and Lang (1989).
Using these techniques, both union and concatenation canbe implemented in constant time, and even infinite unions will be reasonably efficient.2.5.2 Viterbi-derivation Semiring.
The Viterbi-derivation semiring computes the mostprobable derivation of the sentence, given a probabilistic grammar.
Elements of thissemiring are a pair, a real number v and a derivation forest E, i.e., the set of derivationswith score v. We define max, the additive operator, asVit(v,E) if v > wmax((v ,E) , (w,D))= (w,D) i fv<wVit ( V , E kJ D) if v = wIn typical practical Viterbi parsers, when two derivations have the same value, one ofthe derivations is arbitrarily chosen.
In practice, this is usually a fine solution, and onethat could be used in a real-world implementation of the ideas in this paper, but froma theoretical viewpoint, the arbitrary choice destroys the associative property of theadditive operator, max.
To preserve associativity, we keep derivation forests of all ele-ments that tie for beret.The definition for max is only defined for two elements.
Since the operator isVitassociative, it is clear how to define max for any finite number of elements, but we alsoVitneed infinite summations to be defined.
We use the supremum, sup: the supremumof a set is the smallest value at least as large as all elements of the set; that is, it is a4 This semiring is equivalent to one well known to mathematicians, the polynomials overnoncommuting variables.586Goodman Semiring Parsingmaximum that is defined in the infinite case.
We can now define max for the case ofvitinfinite sums.
LetW ~- sup  V(v,E>6XD = {EI<w,E> E X}Then max X = (w, D/.
D is potentially empty, but this causes us no problems invittheory, and will not occur in practice.
We define x asvit(v, E I vXit(w, D> = (v x w, E. D>where E ?
D represents the concatenation of the two derivation forests.2.5.3 Viterbi-n-best Semiring.
The last kind of derivation semiring is the Viterbi-n-best semiring, which is used for constructing n-best lists.
Intuitively, the value of astring using this semiring will be the n most likely derivations of that string (unlessthere are fewer than n total derivations.)
In practice, this is actually how a Viterbi-n-bestsemiring would typically be implemented.
From a theoretical viewpoint, however, thisimplementation is inadequate, since we must also define infinite stuns and be sure thatthe distributive property holds.
Elsewhere (Goodman 1998), we give a mathematicallyprecise definition of the semiring that handles these cases.3.
Efficient Computation of Item ValuesRecall that the value of an item x is just V(x) = (~Deinner(x)V(D), the sum of thevalues of all derivation trees headed by x.
This definition may require summing overexponentially many or even infinitely many terms.
In this section, we give relativelyefficient formulas for computing the values of items.
There are three cases that mustbe handled.
First is the base case, when x is a rule.
In this case, inner(x) is trivially{(x/}, the set containing the single derivation tree x.
Thus, V(x) = (~Dcinner(x) V(D) =(~DC{<x)} V(D) = V((x>) = R(x)The second and third cases occur when x is an item.
Recall that each item is asso-ciated with a bucket, and that the buckets are ordered.
Each item x is either associatedwith a nonlooping bucket, in which case its value depends only on the values of itemsin earlier buckets; or with a looping bucket, in which case its value depends poten-tially on the values of other items in the same bucket.
In the case when the item isassociated with a nonlooping bucket, if we compute items in the same order as theirbuckets, we can assume that the values of items al .
.
.
ak contributing to the value ofitem b are known.
We give a formula for computing the value of item b that dependsonly on the values of items in earlier buckets.For the final case, in which x is associated with a looping bucket, infinite loopsmay occur, when the value of two items in the same bucket are mutual ly dependent,or an item depends on its own value.
These infinite loops may require computationof infinite sums.
Still, we can express these infinite sums in a relatively simple form,allowing them to be efficiently computed or approximated.587Computational Linguistics Volume 25, Number 43.1 Item Value FormulaTheorem 2If an item x is not in a looping bucket, thenkV(x) ---- (~ (~ V(ai)i :1al... ak s.t.
al.x.
al~(5)ProofLet us expand our notion of inner to include deduction rules: i nner (~)  is the setof all derivation trees of the form (b: (a l .
.
.
/ (a2.
.
.
/ - .
.
(ak...11" For any item derivationtree that is not a simple rule, there is some al...ak, b such that D E i nner (~) .Thus, for any item x,v(x) = (~ v(D)DE inner( x )= (~ (~ V(D) (6)al...al?
s.t.
al'~c, ak DEinner(aI"x" ak)Consider item derivation trees Dal ... Dak headed by items al .. .
ak such that ~g~.Recall that (x: Da, .
.
.
.
, Dakl is the item derivation tree formed by combining each ofthese trees into a full tree, and notice that U (x: Dal,..., Dakl = inner (~) .Da I ff inner( al ) .....Da k ff inner (ak )(9 v(o) = (9D6inner(al "~c" ak) Da I 6 inner(aJ  .....Da k 6inner(ak)Therefore= GDa 16inner(al ).....Da k ff inner( ak )ki=1 Dai Cinner(ai)k: (9  V(,, i)i=1Substituting this back into Equation 6, we getkV(K)= (~ (~V(a,)i=1al... ak s.t.
al.x.
aiv(Ix: Da, .
.
.
.
,Dak))k(~V(Dai)i=1completing the proof.
\[\]588Goodman Semiring ParsingNow, we address the case in which x is an item in a looping bucket.
This caserequires computation of an infinite sum.
We will write out this infinite sum, and discusshow to compute it exactly in all cases, except for one, where we approximate it.Consider the derivable items xl .
.
.
Xm in some looping bucket B.
If we build upderivation trees incrementally, when we begin processing bucket B, only those treeswith no items from bucket B will be available, what we will call zeroth generationderivation trees.
We can put these zeroth generation trees together to form first gener-ation trees, headed by elements in B.
We can combine these first generation trees witheach other and with zeroth generation trees to form second generation trees, and soon.
Formally, we define the generation of a derivation tree headed by x in bucket Bto be the largest number of items in B we can encounter on a path from the root to aleaf.Consider the set of all trees of generation at most g headed by x.
Call this setinner<_~(x, B).
We can define the Kg generation value of an item x in bucket B, V<_~(x, B):V<_g(x,B) = (~ V(D)D 6 inner<g (x,B)Intuitively, as g increases, for x E B, inner<~(x, B) becomes closer and closer toinner(x).
That is, the finite sum of values in the former approaches the infinite sum ofvalues in the latter.
For w-continuous emirings (which includes all of the semiringsconsidered in this paper), an infinite sum is equal to the supremum of the partial sums(Kuich 1997, 613).
Thus,V(x) = (~ V(D) = sup V<g(x, B)OC inner( x,B ) gIt will be easier to compute the supremum if we find a simple formula for V<_g(x, B).Notice that for items x E B, there will be no generation 0 derivations, o V_<0(x, B) =0.
Thus, generation 0 makes a trivial base for a recursive formula.
Now, we can considerthe general case:Theorem 3For x an item in a looping bucket B, and for g ~ 1,V<g(x,B)i=1 \[ V<_g-l(ai, B)al... ak s.t.
al'x" akif ai ~ Bif ai E B (7)The proof parallels that of Theorem 2 (Goodman 1998).3.2 Solving the Infinite SummationA formula for V<_g(x, B) is useful, but what we really need is specific techniques forcomputing the supremum, V(x) = supg V<<_g(x, B).
For all w-continuous emirings, thesupremum of iteratively approximating the value of a set of polynomial equations, aswe are essentially doing in Equation 7, is equal to the smallest solution to the equations(Kuich 1997, 622).
In particular, consider the equations:k ~V(ai) if ai ~ BV<_oo(x,B) = 0 ~ \[ V<_oo(ai, B) if a i C B (8)i:1al... ak s.t.
al"x" at;589Computational Linguistics Volume 25, Number 4where V<~(x, B) can be thought of as indicating \[B\[ different variables, one for eachitem x in the looping bucket B.
Equation 7 represents he iterative approximation ofEquation 8, and therefore the smallest solution to Equation 8represents he supremumof Equation 7.One fact will be useful for several semirings: whenever the values of all itemsx E B at generation g + 1 are the same as the values of all items in the precedinggeneration, g they will be the same at all succeeding enerations, as well.
Thus, thevalue at generation g will be the value of the supremum.
Elsewhere (Goodman 1998),we give a trivial proof of this fact.Now, we can consider various semiring-specific algorithms for computing thesupremum.
Most of these algorithms are well known, and we have simply extendedthem from specific parsers (described in Section 7) to the general case, or from onesemiring to another.Notice in this section the wide variety of different algorithms, one for each semi-ring, and some of them fairly complicated.
In a conventional system, these algorithmsare interweaved with the parsing algorithm, conflating computation of infinite sumswith parsing.
The result is algorithms that are both harder to understand, and lessportable to other semirings.We first examine the simplest case, the Boolean semiring.
Notice that whenevera particular item has value TRUE at generation g, it must also have value TRUEat generation g+ 1, since if the item can be derived in at most g generations thenit can certainly be derived in at most g + 1 generations.
Thus, since the numberof TRUE valued items is nondecreasing, and is at most IB\[, eventually the valuesof all items must not change from one generation to the next.
Therefore, for theBoolean semiring, a simple algorithm suffices: keep computing successive genera-tions, until no change is detected in some generation; the result is the supremum.We can perform this computation efficiently if we keep track of items that changevalue in generation g and only examine items that depend on them in generationg+l .
This algorithm is then similar to the algorithm of Shieber, Schabes, and Pereira(1993).For the counting semiring, the Viterbi semiring, and the derivation forest semi-ring, we need the concept of a derivation subgraph.
In Section 2.2 we consideredthe strongly connected components of the dependency graph, consisting of items thatfor some sentence could possibly depend on each other, and we put these possiblyinterdependent items together in looping buckets.
For a given sentence and gram-mar, not all items will have derivations.
We will find the subgraph of the dependencygraph of items with derivations, and compute the strongly connected components ofthis subgraph.
The strongly connected components of this subgraph correspond toloops that actually occur given the sentence and the grammar, as opposed to loopsthat might occur for some sentence and grammar, given the parser alone.
We call thissubgraph the derivation subgraph, and we will say that items in a strongly connectedcomponent of the derivation subgraph are part of a loop.Now, we can discuss the counting semiring (integers under + and x).
In thecounting semiring, for each item, there are three cases: the item can be in a loop;the item can depend (directly or indirectly) on an item in a loop; or the item doesnot depend on loops.
If the item is in a loop or depends on a loop, its value is in-finite.
If the item does not depend on a loop in the current bucket, then its valuebecomes fixed after some generation.
We can now give the algorithm: first, com-pute successive generations until the set of items in B does not change from onegeneration to the next.
Next, compute the derivation subgraph, and its strongly con-nected components.
Items in a strongly connected component (a loop) have an infi-590Goodman Semiring Parsingnite number of derivations, and thus an infinite value.
Compute items that dependdirectly or indirectly on items in loops: these items also have infinite value.
Anyother items can only be derived in finitely many ways using items in the currentbucket, so compute successive generations until the values of these items do notchange.The method for solving the infinite summation for the derivation forest semiringdepends on the implementation of derivation forests.
Essentially, that representationwill use pointers to efficiently represent derivation forests.
Pointers, in various forms,allow one to efficiently represent infinite circular references, either directly (Goodman1999), or indirectly (Goodman 1998).
Roughly, the algorithm we will use is to computethe derivation subgraph, and then create pointers analogous to the directed edges inthe derivation subgraph, including pointers in loops whenever there is a loop in thederivation subgraph (corresponding to an infinite number of derivations).
Details aregiven elsewhere (Goodman 1998).
As in the finite case, this representation is equivalentto that of Billot and Lang (1989).For the Viterbi semiring, the algorithm is analogous to the Boolean case.
Deriva-tions using loops in these semirings will always have values no greater than deriva-tions not using loops, since the value with the loop will be the same as some valuewithout the loop, multiplied by some set of rule probabilities that are at most 1.
Sincethe additive operation is max, these lower (or at most equal) looping derivations do notchange the value of an item.
Therefore, we can simply compute successive generationsuntil values fail to change from one iteration to the next.Now, consider implementations of the Viterbi-derivation semiring in practice,in which we keep only a representative derivation, rather than the whole deriva-tion forest.
In this case, loops do not change values, and we use the same algo-rithm as for the Viterbi semiring.
In an implementation of the Viterbi-n-best semi-ring, in practice, loops can change values, but at most n times, so the same algo-rithm used for the Viterbi semiring still works.
Elsewhere (Goodman 1998), we de-scribe theoretically correct implementations for both the Viterbi-derivation and Viterbi-n-best semirings that keep all values in the event of ties, preserving addition'sassociativity.The last semiring we consider is the inside semiring.
This semiring is the mostdifficult.
There are two cases of interest, one of which we can solve exactly, and theother of which requires approximations.
In many cases involving looping buckets, allalx deduction rules will be of the form ~- ,  where al and b are items in the looping bucket,and x is either a rule, or an item in a previously computed bucket.
This case corre-sponds to the items used for deducing singleton productions, such as those Earley'salgorithm uses for rules of the form A --* B and B --+ A.
In this case, Equation 8 formsa set of linear equations that can be solved by matrix inversion.
In the more generalcase, as is likely to happen with epsilon rules, we get a set of nonlinear equations, andmust solve them by approximation techniques, such as simply computing successivegenerations for many iterations.
5 Stolcke (1993) provides an excellent discussion ofthese cases, including a discussion of sparse matrix inversion, useful for speeding upsome computations.5 Note that even in the case where we can only use approximation techniques, this algorithm isrelatively efficient.
By assumption, i  this case, there is at least one deduction rule with two items inthe current generation; thus, the number of deduction trees over which we are summing rowsexponentially with the number of generations: a linear amount of computation yields the sum of thevalues of exponentially many trees.591Computational Linguistics Volume 25, Number 4goalDerivation of \[goal\]Figure 7Outside algorithm.goalOuter tree of \[b\]4.
Reverse ValuesThe previous section showed how to compute several of the most commonly usedvalues for parsers, including Boolean, inside, Viterbi, counting, and derivation forestvalues, among others.
Noticeably absent from the list are the outside probabilities,which we define below.
In general, computing outside probabilities is significantlymore complicated than computing inside probabilities.In this section, we show how to compute outside probabilities from the sameitem-based escriptions used for computing inside values.
Outside probabilities havemany uses, including for reestimating rammar probabilities (Baker 1979), for im-proving parser performance on some criteria (Goodman 1996b), for speeding parsingin some formalisms, such as data-oriented parsing (Goodman 1996a), and for goodthresholding algorithms (Goodman 1997).We will show that by substituting other semirings, we can get values analogousto the outside probabilities for any commutative semiring; elsewhere (Goodman 1998)we have shown that we can get similar values for many noncommutative s miringsas well.
We will refer to these analogous quantities as reverse values.
For instance,the quantity analogous to the outside value for the Viterbi semiring will be calledthe reverse Viterbi value.
Notice that the inside semiring values of a hidden Markovmodel (HMM) correspond to the forward values of HMMs, and the reverse insidevalues of an HMM correspond to the backwards values.Compare the outside algorithm (Baker 1979; Lari and Young 1990), given in Fig-ure 7, to the inside algorithm of Figure 2.
Notice that while the inside and recognitionalgorithms are very similar, the outside algorithm is quite a bit different.
In particular,while the inside and recognition algorithms looped over items from shortest to longest,the outside algorithm loops over items in the reverse order, from longest o shortest.Also, compare the inside algorithm's main loop formula to the outside algorithm'smain loop formula.
While there is clearly a relationship between the two equations,the exact pattern of the relationship is not obvious.
Notice that the outside formula isabout twice as complicated as the inside formula.
This doubled complexity is typicalof outside formulas, and partially explains why the item-based escription format is souseful: descriptions for the simpler inside values can be developed with relative ease,and then automatically used to compute the twice-as-complicated outside values.
66 Jumping ahead a bit, compare Equation 13 for reverse values to Equation 5 for forward values.
Let k bethe number of terms above the line.
Notice that the reverse values equation sums over k times as manyterms as the forward values equation.
Parsers where all rules have k = 1 terms above the line can only592Goodman Semiring ParsinggoalDerivation of \[goal\]Figure 8goalOuter tree of \[b\]Item derivation tree of \[goal\] and outer tree of \[b\].For a context-free grammar, using the CKY parser of Figure 3, recall that the insideprobability for an item \[i, A, j\] is P(A -~ wi... wj-1).
The outside probability for the sameitem is P(S G wl .
.
.
Wi_ lAWj .
, .
Wn).
Thus ,  the outside probability has the property thatwhen multiplied by the inside probability, it gives the probability that the start symbolgenerates the sentence using the given item, P(S G Wl .
.
,  w i_dAwj .
.
.
Wn G Wl .
.
.
Wn).This probability equals the sum of the probabilities of all derivations using the givenitem.
Formally, letting P(D) represent the probability of a particular derivation, andC(D, \[i, X,j\]) represent the number of occurrences of item \[i, X,j\] in derivation D (whichfor some parsers could be more than one if X were part of a loop),inside(i, X,j) x outside(i, X,j) = Z P(D) C(D, \[i, X,j\])D a derivationThe reverse values in general have an analogous meaning.
Let C(D, x) representthe number of occurrences (the count) of item x in item derivation tree D. Then, foran item x, the reverse value Z(x) should have the propertyV(x) ?
Z(x) = V(D)C(D, x) (9)D a derivationNotice that we have multiplied an element of the semiring, V(D), by an integer, C(D, x).This multiplication is meant o indicate repeated addition, using the additive operatorof the semiring.
Thus, for instance, in the Viterbi semiring, multiplying by a countother than 0 has no effect, since x ?
x = max(x, x) = x, while in the inside semiring,it corresponds to actual multiplication.
This value represents the sum of the values ofall derivation trees that the item x occurs in; if an item x occurs more than once in aderivation tree D, then the value of D is counted more than once.To formally define the reverse value of an item x, we must first define the outertrees outer(x).
Consider an item derivation tree of the goal item, containing one ormore instances of item x.
Remove one of these instances of x, and its children too,leaving a gap in its place.
This tree is an outer tree of x.
Figure 8 shows an itemderivation tree of the goal item, including a subderivation of an item b, derived fromterms al .
.
.
.
, ak.
It also shows an outer tree of b, with b and its children removed; thespot b was removed from is labeled (b).parse regular grammars, and tend to be less useful.
Thus, in most parsers of interest, k > 1, and thecomplexity of (at least some) outside quations, when the sum is written out, is at least doubled.593Computational Linguistics Volume 25, Number 4For an outer tree D E outer(x), we define its value, Z(D), to be the product of thevalues of all rules in D, (~rCD R(r).
Then, the reverse value of an item can be formallydefined asZ(x)= 0 Z(D) (10)DEouter( x)That is, the reverse value of x is the sum of the values of each outer tree of x.Now, we show that this definition of reverse values has the property described byEquation 9.
7Theorem 4V(x) ?
z(x) = E) v(D)C(D, x)D a der ivat ionProofFirst, observe thatV(x) ?Z(x)= ( E\]~ V(I)) ?
0 Z(O)= (~ ~ V(I)?Z(O) (11)\lEinner(x) Ocouter(x) IEinner(x) OEouter(x)Next, we argue that this last expression equals the expression on the right-hand sideof Equation 9, (~D V(D)C(D,x).
For an item x, any outer part of an item derivationtree for x can be combined with any inner part to form a complete item derivationtree.
That is, any O E outer(x) and any I E inner(x) can be combined to form an itemderivation tree D containing x, and any item derivation tree D containing x can bedecomposed into such outer and inner trees.
Thus, the list of all combinations of outerand inner trees corresponds exactly to the list of all item derivation trees containingx.
In fact, for an item derivation tree D containing C(D, x) instances of x, there areC(D, x) ways to form D from combinations of outer and inner trees.
Also, notice thatfor D combined from O and IV(I) ?
Z(O) = (~R(r )  ?
(~R(r )  = (~R(r )  = V(D)rEI rEO rEDThus ,{~ (~ V(I) ?
Z(O) = (~ V(D)C(D,x)IEinner(x) OEouter(x) DCombining Equation 11 with Equation 12, we see that(12)V(x) o z(x) = 0 V(D)C(O, x)D a derivationcompleting the proof.
\[\]7 We note that satisfying Equation 9 is a useful but not sufficient condition for using reverse insidevalues for grammar eestimation.
While this definition will typically provide the necessary values forthe E step of an E-M algorithm, additional work will typically be required to prove this fact; Equation9 should be useful in such a proof.594Goodman Semiring ParsingThere is a simple, recursive formula for efficiently computing reverse values.
Recallthat the basic equation for computing forward values not involved in loops waskV(x) ---- 0 (~ V(ai)i:1 al .
.
.
ak s.t.
al "x" akAt this point, for conciseness, we introduce a nonstandard notation.
We will soonbe using many sequences of the form 1, 2 .
.
.
.
.
j - 2, j - 1, j + 1, j + 2 .
.
.
.
.
k -  1, k. We denotesuch sequences by 1, ._4, k. By extension, we will also write f(1), zL,f(k) to indicate asequence of the form f(1),f(2) .
.
.
.
.
f ( j -  2 ) , f ( j -  1),f(j + 1),f(j + 2) .
.
.
.
.
f (k -  1),f(k).Now, we can give a simple formula for computing reverse values Z(x) not involvedin loops:Theorem 5For items x E B where B is nonlooping,z(x) =  Z(b) ?
@i=l,-t,k j,a,.., ak,b s.t.
~-~ A x=?V(ai) (13)unless x is the goal item, in which case Z(x) = 1, the multiplicative identity of thesemiring.ProofThe simple case is when x is the goal item.
Since an outer tree of the goal item is aderivation of the goal item, with the goal item and its children removed, and since weassumed in Section 2.2 that the goal item can only appear in the root of a derivationtree, the outer trees of the goal item are all empty.
Thus,Z(goal) = (~ Z(D) = Z({(/}) = ~)  R(r) = 1D6outer(goal) r6 { (I }As mentioned in Section 2.1, the value of the empty product is the multiplicativeidentity.Now, we consider the general case.
We need to expand our concept of outer toinclude deduction rules, where outer(\]', ~-~)  is an item derivation tree of the goalitem with one subtree removed, a subtree headed by aj whose parent is b and whosesiblings are headed by al, .-(, ak.
Notice that for every outer tree D C outer(x), there isexactly one j, al .
.
.
.
.
ak, and b such that x = aj and D E outer(\], ~ ) :  this correspondsto the deduction rule used at the spot in the tree where the subtree headed by x wasdeleted.
Figure 9 illustrates the idea of putting together an outer tree of b with innertrees for al, .J., ak to form an outer tree of x ---- aj.
Using this observation,Z(x) = G z(o)Dcouter(x)= (~ (~ Z(D) (14)al'~'akA x=aj Df f_outer( j ,~)  j,al.., ak,b s.t.595Computational Linguistics Volume 25, Number 4/ / / /a  1 a j-1Figure 9goal(aj)Combining an outer tree with inner trees to form an outer tree.Now, consider all of the outer trees outer ( j ,~) .
For each item derivation treeDal C inner(a1), ._4, Dak E inner(ak) and for each outer tree Db E outer(b), there will beone outer tree in the set outer(j, ff~--~)o Similarly, each tree in outer(j, al.
"b" ak) can bedecomposed into an outer tree in outer(b) and derivation trees for al, ._4, ak.
Then,z(D)= (~ Z(Db) ~ V(Da,)@ .
:J ~V(Dak )Db C outer( b ),Da 1 Einner(al ),Z!,Da k 6inner(ak )= (DbEOut~er(b) Z(Db)) @ (Dalcger(al)g(Dal)) @'~t@ (D~kEinngr(ak) g(Dak))= Z(b) @ W(al)@ --j ?V(ak)= Z(b)?
(~ V(ai) (15)i=l,Zt,kSubstituting equation 15 into equation 14, we conclude thatZ(x) = (9  Z(b) ?
@ V(a,)i=l,-!,k j,al.., ak,b s.t.
?t~ A x=ajcompleting the general case.Computing the reverse values for loops is somewhat more complicated, and as inthe forward case, requires an infinite sum, and the use of the concept of generation.596Goodman Semiring ParsingWe define the generation g of an outer tree D of item x in bucket B to be the numberof items in bucket B on the path between the root and the removal point, inclusive.We can then let Z<_g(x, B) represent the sum of the values of all trees headed by xof generation at most g. In the base case, Z_<0(x, B) = 0.
For ~;-continuous semirings,Z<_g(x, B) approaches Z(x) as g approaches c~.
We can give a recursive quation forZ<_~(x, B)as follows, using a proof similar to that of Theorem 5 (Goodman 1998):Theorem 6For items x E B and g > 1,( (~ ~ fZ<_~q(b,B) i fbEB \ Z<_g(x,B) = (~ V(ai)!
?
~Z(b) if b ~ B (16)j,al.., ak,b s.t.
~-~ A x=aj \i=l,Z!,k /5.
Semiring Parser ExecutionExecuting a semiring parser is fairly simple.
There is, however, one issue that mustbe dealt with before we can actually begin parsing.
A semiring parser computes thevalues of items in the order of the buckets they fall into.
Thus, before we can beginparsing, we need to know which items fall into which buckets, and the ordering ofthose buckets.
There are three approaches todetermining the buckets and ordering thatwe will discuss in this section.
The first approach is a simple, brute-force numerationof all items, derivable or not, followed by a topological sort.
This approach will havesuboptimal time and space complexity for some item-based escriptions.
The secondapproach is to use an agenda parser in the Boolean semiring to determine the derivableitems and their dependencies, and to then perform a topological sort.
This approachhas optimal time complexity, but typically suboptimal space complexity.
The finalapproach is to use bucketing code specific to the item-based interpreter.
This achievesoptimal performance for additional programming effort.The simplest way to determine the bucketing is to simply enumerate all possibleitems for the given item-based escription, grammar, and input sentence.
Then, wecompute the strongly connected components and a partial ordering; both steps can bedone in time proportional to the number of items plus the number of dependencies(Cormen, Leiserson, and Rivest 1990, Chap.
23).
For some parsers, this technique hasoptimal time complexity, although poor space complexity.
In particular, for the CKYalgorithm, the time complexity is optimal, but since it requires computing and storingall possible O(n 3) dependencies between the items, it takes significantly more spacethan the O(n 2) space required in the best implementation.
In general, the brute-forcetechnique raises the space complexity to be the same as the time complexity.
Further-more, for some algorithms, such as Earley's algorithm, there could be a significant timecomplexity added as well.
In particular, Earley's algorithm may not need to examineall possible items.
For certain grammars, Earley's algorithm examines only a linearnumber of items and a linear number of dependencies, even though there are O(n 2)possible items, and O(n 3) possible dependencies.
Thus the brute-force approach wouldrequire O(n 3) time and space instead of O(n) time and space, for these grammars.The next approach to finding the bucketing solves the time complexity problem.In this approach, we first parse in the Boolean semiring, using the agenda parser de-scribed by Shieber, Schabes, and Pereira (1995), and then we perform a topologicalsort.
The techniques that Shieber, Schabes, and Pereira use work well for the Booleansemiring, where items only have value TRUE or FALSE, but cannot be used directly for597Computational Linguistics Volume 25, Number 4for current := first bucket o last bucketif current is a looping bucket/* replace with semiring-specific code */for x E currentv\[x, 0\] = 0;for g :-- 1 to oofor each x E current, al ... ak s.t.k {V\[ai\]V\[x,g\] := V\[x,g\] ?
~i=1 V\[ai, g - 1\]for each x E currentv\[x\]  :=  v\[x,elsefor each x E current, al ... ak s.t.v\[x\] := V\[x\] ?
v\[ai\]; i=1return V\[goal\];Figure 10Forward semiring parser interpreter.ai ~ currentai E currentother semirings.
For other semirings, we need to make sure that the values of items arenot computed until after the values of all items they depend on are computed.
How-ever, we can use the algorithm of Shieber, Schabes, and Pereira to compute all of theitems that are derivable, and to store all of the dependencies between the items.
Thenwe perform a topological sort on the items.
The time complexity of both the agendaparser and the topological sort will be proportional to the number of dependencies,which will be proportional to the optimal time complexity.
Unfortunately, we still havethe space complexity problem, since again, the space used will be proportional to thenumber of dependencies, rather than to the number of items.The third approach to bucketing is to create algorithm-specific bucketing code;this results in parsers with both optimal time and optimal space complexity.
For in-stance, in a CKY-style parser, we can simply create one bucket for each length, andplace each item into the bucket for its length.
For some algorithms, such as Ear-ley's algorithm, special-purpose code for bucketing might have to be combined withcode to make sure all and only derivable items are considered (using triggering tech-niques described by Shieber, Schabes, and Pereira) in order to achieve optimal perfor-mance.Once we have the bucketing, the parsing step is fairly simple.
The basic algorithmappears in Figure 10.
We simply loop over each item in each bucket.
There are twotypes of buckets: looping buckets, and nonlooping buckets.
If the current bucket isa looping bucket, we compute the infinite sum needed to determine the bucket'svalues; in a working system, we substitute semiring-specific code for this section, asdescribed in Section 3.2.
If the bucket is not a looping bucket, we simply computeall of the possible instantiations that could contribute to the values of items in thatbucket.
Finally, we return the value of the goal item.The reverse semiring parser interpreter is very similar to the forward semiringparser interpreter.
The differences are that in the reverse semiring parser interpreter,we traverse the buckets in reverse order, and we use the formulas for the reversevalues, rather than the forward values.
Elsewhere (Goodman 1998), we give a simpleinductive proof to show that both interpreters compute the correct values.598Goodman Semiring ParsingThere are two other implementation issues.
First, for some parsers, it will be pos-sible to discard some items.
That is, some items serve the role of temporary variables,and can be discarded after they are no longer needed, especially if only the forwardvalues are going to be computed.
Also, some items do not depend on the input string,but only on the rule value function of the grammar.
The values of these items can beprecomputed.6.
ExamplesIn this section, we survey other results that are described in more detail elsewhere(Goodman 1998), including examples of formalisms that can be parsed using item-based descriptions, and other uses for the technique of semiring parsing.6.1 Finite State Automata and Hidden Markov ModelsNondeterministic f nite-state automata (NFAs) and HMMs turn out to be examples ofthe same underlying formalism, whose values are simply computed in different semi-rings.
Other semirings lead to other interesting values.
For HMMs, notice that the for-ward values are simply the forward inside values; the backward values are the reversevalues of the inside semiring; and Viterbi values are the forward values of the Viterbisemiring.
For NFAs, we can use the Boolean semiring to determine whether a string isin the language of an NFA; we can use the counting semiring to determine how manystate sequences there are in the NFA for a given string; and we can use the derivationforest semiring to get a compact representation f all state sequences in an NFA for aninput string.
A single item-based escription can be used to find all of these values.6.2 Prefix ValuesFor language modeling, it may be useful to compute the prefix probability of a string.That is, given a string wl .
.
.
Wn, we may wish to know the total probability of allsentences beginning with that string,P(S ~ w l .
.
.
wnv l .
.
,  v~)k>O,vl,...,Vkwhere Vl ... Vk represent words that could possibly follow wl ... wn.
Jelinek and Lafferty(1991) and Stolcke (1993) both give algorithms for computing these prefix probabilities.Elsewhere (Goodman 1998), we show how to produce an item-based escription of aprefix parser.
There are two main advantages tousing an item-based description: easeof derivation, and reusability.First, the conventional derivations are somewhat complex, requiring a fair amountof inside-semiring-specific mathematics.
In contrast, using item-based descriptions, weonly need to derive a parser that has the property that there is one item derivation foreach (complete) grammar derivation that would produce the prefix.
The value of anyprefix given the parser will then automatically be the sum of all grammar derivationsthat include that prefix.The other advantage is that the same description can be used to compute manyvalues, not just the prefix probability.
For instance, we can use this description with theViterbi-derivation semiring to find the most likely derivation that includes this prefix.With this most likely derivation, we could begin interpretation f a sentence ven be-fore the sentence was finished being spoken to a speech recognition system.
We couldeven use the Viterbi-n-best emiring to find the n most likely derivations that includethis prefix, if we wanted to take into account ambiguities present in parses of the prefix.599Computational Linguistics Volume 25, Number 46.3 Beyond Context-FreeThere has been quite a bit of previous work on the intersection of formal languagetheory and algebra, as described by Kuich (1997), among others.
This previous workhas made heavy use of the fact that there is a strong correspondence b tween alge-braic equations in certain noncommutative s mirings, and CFGs.
This correspondencehas made it possible to manipulate algebraic systems, rather than grammar systems,simplifying many operations.On the other hand, there is an inherent limit to such an approach, namely a limitto context-free systems.
It is then perhaps lightly surprising that we can avoid theselimitations, and create item-based escriptions of parsers for weakly context-sensitivegrammars, such as tree adjoining grammars (TAGs).
We avoid the limitations of pre-vious approaches using two techniques.
One technique is to compute derivation trees,rather than parse trees, for TAGs.
Computing derivation trees for TAGs is significantlyeasier than computing parse trees, since the derivation trees are context-free.
The othertrick we use is to create a set of equations for each grammar and string length ratherthan creating a set of equations for each grammar, as earlier formulations did.
Becausethe number of equations grows with the string length with our technique, we can rec-ognize strings in weakly context-sensitive languages.
Goodman (1998) gives a furtherexplication of this subject, including an item-based description for a simple TAG parser.6.4 Tomita ParsingOur goal in this section has been to show that item-based escriptions can be usedto simply describe almost all parsers of interest.
One parsing algorithm that wouldseem particularly difficult to describe is Tomita's graph-structured-stack LR parsingalgorithm.
This algorithm at first glance bears little resemblance to other parsing al-gorithms.
Despite this lack of similarity, Sikkel (1993) gives an item-based escriptionfor a Tomita-style parser for the Boolean semiring, which is also more efficient hanTomita's algorithm.
Sikkel's parser can be easily converted to our format, where it canbe used for w-continuous semirings in general.6.5 Graham Harrison Ruzzo (GHR) ParsingGraham, Harrison, and Ruzzo (1980) describe a parser similar to Earley's, but withseveral speedups that lead to significant improvements.
Essentially, there are threeimprovements in the GHR parser.
First, epsilon productions are precomputed; second,unary productions are precomputed; and, finally, completion is separated into twosteps, allowing better dynamic programming.Goodman (1998) gives a full item-based escription of a GHR parser.
The forwardvalues of many of the items in our parser related to unary and epsilon productionscan be computed off-line, once per grammar, which is an idea due to Stolcke (1993).Since reverse values require entire strings, the reverse values of these items cannotbe computed until the input string is known.
Because we use a single item-baseddescription for precomputed items and nonprecomputed items, and for forward andreverse values, this combination of off-line and on-line computation is easily andcompactly specified.6.6 Grammar TransformationsWe can apply the same techniques to grammar transformations that we have so farapplied to parsing.
Consider a grammar transformation, such as the Chomsky normalform (CNF) grammar transformation, which takes a grammar with epsilon, unary,and n-ary branching productions, and converts it into one in which all productionsare of the form A --* BC  or A --* a.
For any sentence Wl... Wn its value under the600Goodman Semiring Parsingoriginal grammar in the Boolean semiring (TRUE if the sentence can be generated bythe grammar, FALSE otherwise) is the same as its value under a transformed gram-mar.
Therefore, we say that this grammar transformation is value preserving underthe Boolean semiring.
We can generalize this concept of value preserving to othersemirings.Elsewhere (Goodman 1998), we show that using essentially the same item-baseddescriptions we have used for parsing, we can specify grammar transformations.
Theconcept of value preserving rammar transformation is already known in the inter-section of formal language theory and algebra (Kuich 1997; Kuich and Salomaa 1986;Teitelbaum 1973).
Our contribution is to show that these value preserving transforma-tions can be written as simple item-based escriptions, allowing the same computa-tional machinery to be used for grammar transformations a is used for parsing, and tosome extent showing the relationship between certain grammar transformations andcertain parsers, such as that of Graham, Harrison, and Ruzzo (1980).
This uniformmethod of specifying rammar transformations is similar to, but clearer than, similartechniques used with covering grammars (Nijholt 1980; Leermakers 1989).7.
Previous Work7.1 Historical WorkThe previous work in this area is extensive, including work in deductive parsing,work in statistical parsing, and work in the combination of formal language theoryand algebra.
This paper can be thought of as synthetic, combining the work in all threeareas, although in the course of synthesis, several general formulas have been found,most notably the general formula for reverse values.
A comprehensive examination ofall three areas is beyond the scope of this paper, but we can touch on a few significantareas of each.First, there is the work in deductive parsing.
This work in some sense dates backto Earley (1970), in which the use of items in parsers is introduced.
More recent work(Pereira and Warren 1983; Pereira and Shieber 1987) demonstrates how to use deduc-tion engines for parsing.
Finally, both Shieber, Schabes, and Pereira (1995) and Sikkel(1993) have shown how to specify parsers in a simple, interpretable, item-based format.This format is roughly the format we have used here, although there are differencesdue to the fact that their work was strictly in the Boolean semiring.Work in statistical parsing has also greatly influenced this work.
We can trace thiswork back to research in HMMs by Baum and his colleagues (Baum and Eagon 1967;Baum 1972).
In particular, the work of Baum developed the concept of backward prob-abilities (in the inside semiring), as well as many of the techniques for computing inthe inside semiring.
Viterbi (1967) developed corresponding algorithms for computingin the Viterbi semiring.
Baker (1979) extended the work of Baum and his colleagues toPCFGs, including to computation of the outside values (or reverse inside values in ourterminology).
Baker's work is described by Lari and Young (1990).
Baker's work wasonly for PCFGs in CNF, avoiding the need to compute infinite summations.
Jelinekand Lafferty (1991) showed how to compute some of the infinite summations in theinside semiring, those needed to compute the prefix probabilities of PCFGs in CNF.Stolcke (1993) showed how to use the same techniques to compute inside probabili-ties for Earley parsing, dealing with the difficult problems of unary transitions, andthe more difficult problems of epsilon transitions.
He thus solved all of the importantproblems encountered in using an item-based parser to compute the inside and out-side values (forward and reverse inside values); he also showed how to compute theforward Viterbi values.601Computational Linguistics Volume 25, Number 4The final area of work is in formal language theory and algebra.
Although it is notwidely known, there has been quite a bit of work showing how to use formal powerseries to elegantly derive results in formal language theory, dating back to Chomskyand Sch~itzenberger (1963).
The major classic results can be derived in this frame-work, but with the added benefit that they apply to all commutative w-continuoussemirings.
The most accessible introduction to this literature we have found is byKuich (1997).
There are also books by Salomaa and Soittola (1978) and Kuich andSalomaa (1986).One piece of work deserves pecial mention.
Teitelbaum (1973) showed that anysemiring could be used in the CKY algorithm, laying the foundation for much of thework that followed.In summary, this paper synthesizes work from several different related fields, in-cluding deductive parsing, statistical parsing, and formal language theory; we emulateand expand on the earlier synthesis of Teitelbaum.
The synthesis here is powerful: bygeneralizing and integrating many results, we make the computation of a much widervariety of values possible.7.2 Recent  S imi lar  WorkThere has also been recent similar work by Tendeau (1997b, 1997a).
Tendeau (1997b)gives an Earley-like algorithm that can be adapted to work with complete semiringssatisfying certain conditions.
Unlike our version of Earley's algorithm, Tendeau's ver-sion requires time O(n L+I) where L is the length of the longest right-hand side, asopposed to O(n 3) for the classic version, and for our description.
While one could splitright-hand sides of rules to make them binary branching, speeding Tendeau's versionup, this would then change values in the derivation semirings.
Tendeau (1997b, 1997a)introduces a parse forest semiring, similar to our derivation forest semiring, in thatit encodes a parse forest succinctly.
To implement this semiring, Tendeau's version ofrule value functions take as their input not only a nonterminal, but also the span that itcovers; this is somewhat less elegant han our version.
Tendeau (1997a) gives a genericdescription for dynamic programming algorithms.
His description is very similar toour item-based escriptions, except hat it does not include side conditions.
Thus, al-gorithms uch as Earley's algorithm cannot be described in Tendeau's formalism in away that captures their efficiency.There are some similarities between our work and the work of Koller, McAllester,and Pfeffer (1997), who create a general formalism for handling stochastic programsthat makes it easy to compute inside and outside probabilities.
While their formalismis more general than item-based escriptions, in that it is a good way to express anystochastic program, it is also less compact than ours for expressing most dynamic pro-gramming algorithms.
Our formalism also has advantages for approximating infinitesums, which we can do efficiently, and in some cases exactly.
It would be interestingto try to extend item-based escriptions to capture some of the formalisms coveredby Koller, McAllester, and Pfeffer, including Bayes' nets.8.
Conc lus ionIn this paper, we have given a simple item-based escription format that can be usedto describe a very wide variety of parsers.
These parsers include the CKY algorithm,Earley's algorithm, prefix probability computation, a TAG parsing algorithm, Graham,Harrison, Ruzzo (GHR) parsing, and HMM computations.
We have shown that this de-scription format makes it easy to find parsers that compute values in any w-continuoussemiring.
The same description can be used to find reverse values in commutative w-602Goodman Semiring Parsingcontinuous emirings, and in many noncommutative ones as well.
This descriptionformat can also be used to describe grammar transformations, including transfor-mations to CNF and GNF, which preserve values in any commutative w-continuoussemiring.While theoretical in nature, this paper is of some practical value.
There are threereasons the results of this paper would be used in practice: first, these techniques makecomputation of the outside values simple and mechanical; second, these techniquesmake it easy to show that a parser will work in any w-continuous semiring; and third,these techniques i olate computation of infinite sums in a given semiring from theparser specification process.Perhaps the most useful application of these results is in finding formulas foroutside values.
For parsers uch as CKY parsers, finding outside formulas is not par-ticularly burdensome, but for complicated parsers uch as TAG parsers, GHR parsers,and others, it can require a fair amount of thought o find these equations throughconventional reasoning.
With these techniques, the formulas can be found in a simplemechanical way.The second advantage comes from clarifying the conditions under which a parsercan be converted from computing values in the Boolean semiring (a recognizer) tocomputing values in any w-continuous semiring.
We should note that because in theBoolean semiring, infinite summations can be computed trivially and because repeat-edly adding a term does not change results, it is not uncommon for parsers that workin the Boolean semiring to require significant modification for other semirings.
Forparsers like CKY parsers, verifying that the parser will work in any semiring is triv-ial, but for other parsers the conditions are more complex.
With the techniques inthis paper, all that is necessary is to show that there is a one-to-one correspondencebetween item derivations and grammar derivations.
Once that has been shown, anyw-continuous semiring can be used.The third use of this paper is to separate the computation of infinite sums fromthe main parsing process.
Infinite sums can come from several different phenomena,such as loops from productions of the form A --* A; productions involving ~; andleft recursion.
In traditional procedural specifications, the solution to these difficultproblems is intermixed with the parser specification, and makes the parser specific tosemirings using the same techniques for solving the summations.It is important to notice that the algorithms for solving these infinite summationsvary fairly widely, depending on the semiring.
On the one hand, Boolean infinitesummations are nearly trivial to compute.
For other semirings, such as the countingsemiring, or derivation forest semiring, more complicated computations are required,including the detection of loops.
Finally, for the inside semiring, in most cases onlyapproximate chniques can be used, although in some cases, matrix inversion can beused.
Thus, the actual parsing algorithm, if specified procedurally, can vary quite abit depending on the semiring.On the other hand, using our techniques makes infinite sums easier to deal within two ways.
First, these difficult problems are separated out, relegated conceptu-ally to the parser interpreter, where they can be ignored by the constructor of theparsing algorithm.
Second, because they are separated out, they can be solved once,rather than again and again.
Both of these advantages make it significantly easier toconstruct parsers.
Even in the case where, for efficiency, loops are precomputed off-line, as in GHR parsing, the same item-based representation a d interpreter can beused.In summary, the techniques of this paper will make it easier to compute outsidevalues, easier to construct parsers that work for any w-continuous semiring, and easier603Computational Linguistics Volume 25, Number 4to compute infinite sums in those semirings.
In 1973, Teitelbaum wrote:We have pointed out the relevance of the theory of algebraic powerseries in noncommuting variables in order to minimize further piece-meal rediscovery (page 199).Many of the techniques needed to parse in specific semirings continue to be redis-covered, and outside formulas are derived without observation of the basic formulasgiven here.
We hope this paper will bring about Teitelbaum's wish.AcknowledgmentsI would like to thank Stan Chen, BarbaraGrosz, Luke Hunsberger, Fernando Pereira,and Stuart Shieber, for helpful commentsand discussions, as well as the anonymousreviewers for their comments on earlierdrafts.
This work was funded in part by theNational Science Foundation through GrantIRI-9350192, Grant IRI-9712068, and an NSFGraduate Student Fellowship.ReferencesBaker, James K. 1979.
Trainable grammarsfor speech recognition.
In Proceedings oftheSpring Conference ofthe Acoustical Society ofAmerica, pages 547-550, Boston, MA, June.Baum, Leonard E. 1972.
An inequality andassociated maximization technique instatistical estimation of probabilisticfunctions of a Markov process.Inequalities, 3:1-8.Baum, Leonard E. and J.
A. Eagon.
1967.
Aninequality with application to statisticalestimation for probabilistic functions ofMarkov processes and to a model forecology.
Bulletin of the AmericanMathematicians Society, 73:360-363.Billot, Sylvie and Bernard Lang.
1989.
Thestructure of shared forests in ambiguousparsing.
In Proceedings ofthe 27th AnnualMeeting, pages 143-151, Vancouver.Association for ComputationalLinguistics.Chomsky, Noam and Marcel-PaulSch(itzenberger.
1963.
The algebraictheory of context-free languages.
InP.
Braffort and D. Hirschberg, editors,Computer Programming and Formal Systems.North-Holland, pages 118-161.Cormen, Thomas H., Charles E. Leiserson,and Ronald L. Rivest.
1990.
Introduction toAlgorithms.
MIT Press, Cambridge, MA.Earley, Jay.
1970.
An efficient context-freeparsing algorithm.
Communications of theACM, 13:94-102.Goodman, Joshua.
1996a.
Efficientalgorithms for parsing the DOP model.
InProceedings ofthe Conference on EmpiricalMethods in Natural Language Processing,pages 143-152, May.
Available ascmp-lg/9604008.Goodman, Joshua.
1996b.
Parsingalgorithms and metrics.
In Proceedings ofthe 34th Annual Meeting, pages 177-183,Santa Cruz, CA, June.
Association forComputational Linguistics.
Available ascmp-lg/9605036.Goodman, Joshua.
1997.
Globalthresholding and multiple-pass parsing.In Proceedings ofthe Second Conference onEmpirical Methods in Natural LanguageProcessing, pages 11-25.Goodman, Joshua.
1998.
Parsing Inside-Out.Ph.D.
thesis, Harvard University.Available as cmp-lg/9805007 and fromhttp://www.eecs.harvard.edu/~goodman/thesis.ps.Goodman, Joshua.
1999.
Semiring parsing.Computational Linguistics, 25(4):573-605.Graham, Susan L., Michael A. Harrison, andWalter L. Ruzzo.
1980.
An improvedcontext-free r cognizer.
ACM Transactionson Programming Languages and Systems,2(3):415-462, July.Jelinek, Frederick and John D. Lafferty.
1991.Computation of the probability of initialsubstring eneration by stochasticcontext-free grammars.
ComputationalLinguistics, pages 315-323.Koller, Daphne, David McAllester, and AviPfeffer.
1997.
Effective bayesian inferencefor stochastic programs.
In Proceedings ofthe 14th National Conference on Arti~cialIntelligence, pages 740-747, Providence, RI,August.Kuich, Werner.
1997.
Semirings and formalpower series: Their relevance to formallanguages and automata.
In GrzegorzRozenberg and Arto Salomaa, editors,Handbook of Formal Languages.Springer-Verlag, Berlin, pages 609-677.Kuich, Werner and Arto Salomaa.
1986.Semirings, Automata, Languages.
Number 5of EATCS Monographs on TheoreticalComputer Science.
Springer-Verlag,Berlin, Germany.604Goodman Semiring ParsingLari, K. and S. J.
Young.
1990.
Theestimation of stochastic context-freegrammars using the inside-outsidealgorithm.
Computer Speech and Language,4:35-56.Leermakers, Ren~.
1989.
How to cover agrammar.
In Proceedings ofthe 27th AnnualMeeting, pages 135-142, Vancouver.Association for ComputationalLinguistics.Nijholt, Anton.
1980.
Context-Free Grammars:Covers, Normal Forms, and Parsing.Number 93 of Lecture Notes in ComputerScience.
Springer-Verlag, Berlin, Germany.Pereira, Fernando and Stuart Shieber.
1987.Prolog and Natural Language Analysis.Number 10 of CSU Lecture Notes.
Centerfor the Study of Language andInformation, Stanford, CA.Pereira, Fernando and David Warren.
1983.Parsing as deduction.
In Proceedings ofthe21st Annual Meeting, pages 137-44,Cambridge, MA.
Association forComputational Linguistics.Salomaa, Arto and Matti Soittola.
1978.Automata-Theoretic Aspects of Formal PowerSeries.
Springer-Verlag, Berlin, Germany.Shieber, Stuart, Yves Schabes, and FernandoPereira.
1995.
Principles andimplementation f deductive parsing.Journal of Logic Programming, 24(1-2):3-36.Sikkel, Klaas.
1993.
Parsing Schemata.
Ph.D.thesis, University of Twente, Enschede,The Netherlands.Stolcke, Andreas.
1993.
An efficientprobabilistic context-free parsingalgorithm that computes prefixprobabilities.
Technical Report TR-93-065,International Computer Science Institute,Berkeley, CA.
Available ascmp-lg/9411029.Teitelbaum, Ray.
1973.
Context-free erroranalysis by evaluation of algebraic powerseries.
In Proceedings ofthe Fifth AnnualACM Symposium on Theory of Computing,pages 196-199, Austin, TX.Tendeau, Fr~d4ric.
1997a.
Computingabstract decorations of parse forests usingdynamic programming and algebraicpower series.
Theoretical Computer Science.To appear.Tendeau, Fr~d4ric.
1997b.
An Earleyalgorithm for generic attribute augmentedgrammars and applications.
In Proceedingsof the International Workshop on ParsingTechnologies 1997, pages 199-209.Viterbi, Andrew J.
1967.
Error bounds forconvolutional codes and anasymptotically optimum decodingalgorithm.
IEEE Transactions on InformationTheory, IT-13:260-267.605
