Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 448?455,Prague, Czech Republic, June 2007. c?2007 Association for Computational LinguisticsClustering Clauses for High-Level Relation Detection: AnInformation-theoretic ApproachSamuel BrodySchool of InformaticsUniversity of Edinburghs.brody@sms.ed.ac.ukAbstractRecently, there has been a rise of in-terest in unsupervised detection of high-level semantic relations involving com-plex units, such as phrases and wholesentences.
Typically such approaches arefaced with two main obstacles: datasparseness and correctly generalizingfrom the examples.
In this work, wedescribe the Clustered Clause represen-tation, which utilizes information-basedclustering and inter-sentence dependen-cies to create a simplified and generalizedrepresentation of the grammatical clause.We implement an algorithm which usesthis representation to detect a predefinedset of high-level relations, and demon-strate our model?s effectiveness in over-coming both the problems mentioned.1 IntroductionThe semantic relationship between words, andthe extraction of meaning from syntactic datahas been one of the main points of research inthe field of computational linguistics (see Sec-tion 5 and references therein).
Until recently,the focus has remained largely either at the sin-gle word or sentence level (for instance: depen-dency extraction, word-to-word semantic simi-larity from syntax, etc.)
or on relations betweenunits at a very high context level such as theentire paragraph or document (e.g.
categorizingdocuments by topic).Recently there have been several attempts todefine frameworks for detecting and studying in-teractions at an intermediate context level, andinvolving whole clauses or sentences.
Daganet al (2005) have emphasized the importanceof detecting textual-entailment/implication be-tween two sentences, and its place as a key com-ponent in many real-world applications, such asInformation Retrieval and Question Answering.When designing such a framework, one isfaced with several obstacles.
As we approachhigher levels of complexity, the problem of defin-ing the basic units we study (e.g.
words, sen-tences etc.)
and the increasing amount of in-teractions make the task very difficult.
In addi-tion, the data sparseness problem becomes moreacute as the data units become more complexand have an increasing number of possible val-ues, despite the fact that many of these valueshave similar or identical meaning.In this paper we demonstrate an approachto solving the complexity and data sparse-ness problems in the task of detecting rela-tions between sentences or clauses.
We presentthe Clustered Clause structure, which utilizesinformation-based clustering and dependencieswithin the sentence to create a simplified andgeneralized representation of the grammaticalclause and is designed to overcome both theseproblems.The clustering method we employ is an inte-gral part of the model.
We evaluate our clustersagainst semantic similarity measures defined onthe human-annotated WordNet structure (Fell-baum, 1998).
The results of these comparisonsshow that our cluster members are very similarsemantically.
We also define a high-level rela-tion detection task involving relations betweenclauses, evaluate our results, and demonstrate448the effectiveness of using our model in this task.This work extends selected parts of Brody(2005), where further details can be found.2 Model ConstructionWhen designing our framework, we must ad-dress the complexity and sparseness problemsencountered when dealing with whole sentences.Our solution to these issues combines two ele-ments.
First, to reduce complexity, we simplifya grammatical clause to its primary components- the subject, verb and object.
Secondly, to pro-vide a generalization framework which will en-able us to overcome data-sparseness, we clustereach part of the clause using data from withinthe clause itself.
By combining the simplifiedclause structure and the clustering we produceour Clustered Clause model - a triplet of clustersrepresenting a generalized clause.The Simplified Clause: In order to extractclauses from the text, we use Lin?s parser MINI-PAR (Lin, 1994).
The output of the parser isa dependency tree of each sentence, also con-taining lemmatized versions of the componentwords.
We extract the verb, subject and objectof every clause (including subordinate clauses),and use this triplet of values, the simplifiedclause, in place of the original complete clause.As seen in Figure 1, these components make upthe top (root) triangle of the clause parse tree.We also use the lemmatized form of the wordsprovided by the parser, to further reduce com-plexity.Figure 1: The parse tree for the sentence ?Johnfound a solution to the problem?.
The subject-verb-object triplet is marked with a border.Clustering Clause Components: For ourmodel, we cluster the data to provide both gen-eralization, by using a cluster to represent amore generalized concept shared by its compo-nent words, and a form of dimensionality reduc-tion, by using fewer units (clusters) to representa much larger amount of words.We chose to use the Sequential InformationBottleneck algorithm (Slonim et al, 2002) forour clustering tasks.
The information Bottle-neck principle views the clustering task as anoptimization problem, where the clustering algo-rithm attempts to group together values of onevariable while retaining as much information aspossible regarding the values of another (target)variable.
There is a trade-off between the com-pactness of the clustering and the amount of re-tained information.
This algorithm (and othersbased on the IB principle) is especially suited foruse with graphical models or dependency struc-tures, since the distance measure it employs inthe clustering is defined solely by the depen-dency relation between two variables, and there-fore required no external parameters.
The val-ues of one variable are clustered using their co-occurrence distribution with regard to the valuesof the second (target) variable in the dependencyrelation.
As an example, consider the followingsubject-verb co-occurrence matrix:S \ V tell scratch drinkdog 0 4 5John 4 0 9cat 0 6 3man 6 1 2The value in cell (i, j) indicates the numberof times the noun i occurred as the subject ofthe verb j.
Calculating the Mutual Informationbetween the subjects variable (S) and verbs vari-able (V) in this table, we get MI(S, V ) = 0.52bits.
Suppose we wish to cluster the subjectnouns into two clusters while preserving thehighest Mutual Information with regard to theverbs.
The following co-occurrence matrix is theoptimal clustering, and retains a M.I.
value of0.4 bits (77% of original):Clustered S \ V tell scratch drink{dog,cat} 0 10 8{John,man} 10 1 11Notice that although the values in the drinkcolumn are higher than in others, and we may be449tempted to cluster together dog and John basedon this column, the informativeness of this verbis smaller - if we know the verb is tell we can besure the noun is not dog or cat, whereas if weknow it is drink, we can only say it is slightlymore probable that the noun is John or dog.Our dependency structure consists of threevariables: subject, verb, and object, and we takeadvantage of the subject-verb and verb-objectdependencies in our clustering.
The clusteringwas performed on each variable separately, ina two phase procedure (see Figure 2).
In thefirst stage, we clustered the subject variable into200 clusters1, using the subject-verb dependency(i.e.
the verb variable was the target).
The samewas done with the object variable, using theverb-object dependency.
In the second phase, wewish to cluster the verb values with regard toboth the subject and object variables.
We couldnot use all pairs of subjects and objects values asthe target variable in this task, since too manysuch combinations exist.
Instead, we used a vari-able composed of all the pairs of subject and ob-ject clusters as the target for the verb clustering.In this fashion we produced 100 verb clusters.Figure 2: The two clustering phases.
Arrows rep-resent dependencies between the variables whichare used in the clustering.Combining the Model Elements: Havingobtained our three clustered variables, our orig-inal simplified clause triplet can now be usedto produce the Clustered Clause model.
Thismodel represents a clause in the data by a tripletof cluster indexes, one cluster index for eachclustered variable.
In order to map a clause in1The chosen numbers of clusters are such that eachthe resulting clustered variables preserved approximatelyhalf of the co-occurrence mutual information that existedbetween the original (unclustered) variable and its target.the text to its corresponding clustered clause,it is first parsed and lemmatized to obtain thesubject, verb and object values, as describedabove, and then assigned to the clustered clausein which the subject cluster index is that ofthe cluster containing the subject word of theclause, and the same for the verb and objectwords.
For example, the sentence ?The terroristthrew the grenade?
would be converted to thetriplet (terrorist, throw, grenade) and assignedto the clustered clause composed of the threeclusters to which these words belong.
Othertriplets assigned to this clustered clause mightinclude (fundamentalist, throw, bomb) or (mil-itant, toss, explosive).
Applying this procedureto the entire text corpus results in a distilla-tion of the text into a series of clustered clausescontaining the essential information about theactions described in the text.Technical Specifications: For this work wechose to use the entire Reuters Corpus (En-glish, release 2000), containing 800,000 newsarticles collected uniformly from 20/8/1996 to19/8/1997.
Before clustering, several prepro-cessing steps were taken.
We had a very largeamount of word values for each of the Sub-ject (85,563), Verb (4,593) and Object (74,842)grammatical categories.
Many of the words wereinfrequent proper nouns or rare verbs and wereof little interest in the pattern recognition task.We therefore removed the less frequent words- those appearing in their category less thanone hundred times.
We also cleaned our databy removing all words that were one letter inlength, other than the word ?I?.
These weremostly initials in names of people or compa-nies, which were uninformative without the sur-rounding context.
This processing step broughtus to the final count of 2,874,763 clause triplets(75.8% of the original number), containing 3,153distinct subjects, 1,716 distinct verbs, and 3,312distinct objects.
These values were clustered asdescribed above.
The clusters were used to con-vert the simplified clauses into clustered clauses.3 Evaluating Cluster QualityExamples of some of the resulting clusters areprovided in Table 1.
When manually examin-450?Technical Developements?
(SubjectCluster 160): treatment, drug, method, tactic,version, technology, software, design, device, vaccine,ending, tool, mechanism, technique, instrument,therapy, concept, model?Ideals/Virtues?
(Object Cluster 14):sovereignty, dominance, logic, validity, legitimacy,freedom, discipline, viability, referendum, wisdom,innocence, credential, integrity, independence?Emphasis Verbs?
(Verb Cluster 92): im-ply, signify, highlight, mirror, exacerbate, mark, sig-nal, underscore, compound, precipitate, mask, illus-trate, herald, reinforce, suggest, underline, aggra-vate, reflect, demonstrate, spell, indicate, deepen?Plans?
(Object Cluster 33): journey, ar-rangement, trip, effort, attempt, revolution, pull-out, handover, sweep, preparation, filing, start, play,repatriation, redeployment, landing, visit, push,transition, processTable 1: Example clusters (labeled manually).ing the clusters, we noticed the ?fine-tuning?of some of the clusters.
For instance, we hada cluster of countries involved in military con-flicts, and another for other countries; a clusterfor winning game scores, and another for ties;etc.
The fact that the algorithm separated theseclusters indicates that the distinction betweenthem is important with regard to the interac-tions within the clause.
For instance, in the firstexample, the context in which countries from thefirst cluster appear is very different from that in-volving countries in the second cluster.The effect of the dependencies we use is alsostrongly felt.
Many clusters can be described bylabels such as ?things that are thrown?
(rock,flower, bottle, grenade and others), or ?verbsdescribing attacks?
(spearhead, foil, intensify,mount, repulse and others).
While such crite-ria may not be the first choice of someone whois asked to cluster verbs or nouns, they repre-sent unifying themes which are very appropri-ate to pattern detection tasks, in which we wishto detect connections between actions describedin the clauses.
For instance, we would like todetect the relation between throwing and mil-itary/police action (much of the throwing de-scribed in the news reports fits this relation).
Inorder to do this, we must have clusters whichunite the words relevant to those actions.
Othercriteria for clustering would most likely not besuitable, since they would probably not put egg,bottle and rock in the same category.
In this re-spect, our clustering method provides a moreeffective modeling of the domain knowledge.3.1 Evaluation via Semantic ResourceSince the success of our pattern detection taskdepends to a large extent on the quality of ourclusters, we performed an experiment designedto evaluate semantic similarity between mem-bers of our clusters.
For this purpose we madeuse of the WordNet Similarity package (Peder-sen et al, 2004).
This package contains manysimilarity measures, and we selected three ofthem (Resnik (1995), Leacock and Chodorow(1997), Hirst and St-Onge (1997)), which makeuse of different aspects of WordNet (hierarchyand graph structure).
We measured the averagepairwise similarity between any two words ap-pearing in the same cluster.
We then performedthe same calculation on a random grouping ofthe words, and compared the two scores.
The re-sults (Fig.
3) show that our clustering, based onco-occurrence statistics and dependencies withinthe sentence, correlates with a purely semanticsimilarity as represented by the WordNet struc-ture, and cannot be attributed to chance.Figure 3: Inter-cluster similarity (average pair-wise similarity between cluster members) in ourclustering (light) and a random one (dark).
Ran-dom clustering was performed 10 times.
Aver-age values are shown with error bars to indicatestandard deviation.
Only Hirst & St-Onge mea-sure verb similarity.4 Relation Detection TaskMotivation: In order to demonstrate the useof our model, we chose a relation detection task.The workshop on entailment mentioned in theintroduction was mainly focused on detectingwhether or not an entailment relation exists be-tween two texts.
In this work we present a com-451plementary approach - a method designed to au-tomatically detect relations between portions oftext and generate a knowledge base of the de-tected relations in a generalized form.
As statedby (Dagan et al, 2005), such relations are im-portant for IR applications.
In addition, the pat-terns we employ are likely to be useful in otherlinguistic tasks involving whole clauses, such asparaphrase acquisition.Pattern Definition: For our relation detec-tion task, we searched for instances of prede-fined patterns indicating a relation between twoclustered clauses.
We restricted the search toclause pairs which co-occur within a distance often clauses2 from each other.
In addition to thedistance restriction, we required an anchor : anoun that appears in both clauses, to furtherstrengthen the relation between them.
Noun an-chors establish the fact that the two compo-nent actions described by the pattern involve thesame entities, implying a direct connection be-tween them.
The use of verb anchors was alsotested, but found to be less helpful in detect-ing significant patterns, since in most cases itsimply found verbs which tend to repeat them-selves frequently in a context.
The method wedescribe assumes that statistically significant co-occurrences indicate a relationship between theclauses, but does not attempt to determine thetype of relation.Significance Calculation: The patterns de-tected by the system were scored using the sta-tistical p-value measure.
This value representsthe probability of detecting a certain numberof occurrences of a given pattern in the dataunder the independence assumption, i.e.
assum-ing there is no connection between the twohalves of the pattern.
If the system has detectedk instances of a certain pattern, we calculatethe probability of encountering this number ofinstances under the independence assumption.The smaller the probability, the higher the sig-nificance.
We consider patterns with a chanceprobability lower than 5% to be significant.We assume a Gaussian-like distribution of oc-2Our experiments showed that increasing the distancebeyond this point did not result in significant increase inthe number of detected patterns.currence probability for each pattern3.
In or-der to estimate the mean and standard devia-tion values, we created 100 simulated sequencesof triplets (representing clustered clauses) whichwere independently distributed and varied onlyin their overall probability of occurrence.
Wethen estimated the mean and standard devia-tion for any pair of clauses in the actual datausing the simulated sequences.
(X,V C36, OC7) ?10 (X,V C57, OC85)storm, lash, province ... storm, cross, Cubaquake, shake, city ... quake, hit, Iranearthquake, jolt, city ... earthquake, hit, Iran(X,V C40, OC165) ?10 (X,V C52, OC152)police, arrest, leader ... police, search, mosquepolice, detain, leader ... police, search, mosquepolice, arrest, member ... police, raid, enclave(SC39, V C21, X) ?10 (X, beat 4, OC155)sun, report, earnings ... earnings,beat,expectationxerox, report, earnings ... earnings, beat, forecastmicrosoft,release,result ... result, beat, forecast(X,V C57, OC7) ?10 (X, cause 4, OC153)storm, hit, coast ... storm, cause, damagecyclone, near, coast ... cyclone, cause, damageearthquake,hit,northwest ... earthquake,cause,damagequake , hit, northwest ... quake, cause, casualtyearthquake,hit,city ... earthquake,cause,damageTable 2: Example Patterns4.1 Pattern Detection ResultsIn Table 2 we present several examples ofhigh ranking (i.e.
significance) patterns withdifferent anchorings detected by our method.The detected patterns are represented usingthe notation of the form (SCi, V Cj , X) ?n(X,V Ci?
, OCj?).
X indicates the anchoringword.
In the example notation, the anchoringword is the object of the first clause and thesubject of the second (O-S for short).
n indicatesthe maximal distance between the two clauses.The terms SC, V C or OC with a subscriptedindex represent the cluster containing the sub-ject, verb or object (respectively) of the appro-priate clause.
For instance, in the first examplein Table 2, V C36 indicates verb cluster no.
36,containing the verbs lash, shake and jolt, amongothers.3Based on Gwadera et al (2003), dealing with a sim-ilar, though simpler, case.4In two of the patterns, instead of a cluster for theverb, we have a single word - beat or cause.
This is theresult of an automatic post-processing stage intended toprevent over-generalization.
If all the instances of the pat-452Anchoring Number ofSystem Patterns FoundSubject-Subject 428Object-Object 291Subject-Object 180Object-Subject 178Table 3: Numbers of patterns found (p < 5%)Table 3 lists the number of patterns found,for each anchoring system.
The different anchor-ing systems produce quantitatively different re-sults.
Anchoring between the same categoriesproduces more patterns than between the samenoun in different grammatical roles.
This is ex-pected, since many nouns can only play a certainpart in the clause (for instance, many verbs can-not have an inanimate entity as their subject).The number of instances of patterns we foundfor the anchored template might be consideredlow, and it is likely that some patterns weremissed simply because their occurrence proba-bility was very low and not enough instances ofthe pattern occurred in the text.
In Section 4 westated that in this task, we were more interestedin precision than in recall.
In order to detect awider range of patterns, a less restricted defini-tion of the patterns, or a different significanceindicator, should be used (see Sec.
6).Human Evaluation: In order to better de-termine the quality of patterns detected by oursystem, and confirm that the statistical signif-icance testing is consistent with human judg-ment, we performed an evaluation experimentwith the help of 22 human judges.
We presentedeach of the judges with 60 example groups, 15for each type of anchoring.
Each example groupcontained three clause pairs conforming to theanchoring relation.
The clauses were presentedin a normalized form consisting only of a sub-ject, object and verb converted to past tense,with the addition of necessary determiners andprepositions.
For example, the triplet (police,detain, leader) was converted to ?The police de-tained the leader?.
In half the cases (randomlytern in the text contained the same word in a certain po-sition (in these examples - the verb position in the secondclause), this word was placed in that position in the gen-eralized pattern, rather than the cluster it belonged to.Since we have no evidence for the fact that other wordsin the cluster can fit that position, using the cluster in-dicator would be over-generalizing.selected), these clause pairs were actual exam-ples (instances) of a pattern detected by our sys-tem (instances group), such as those appearingin Table 2.
In the other half, we listed threeclause pairs, each of which conformed to theanchoring specification listed in Section 4, butwhich were randomly sampled from the data,and so had no connection to one another (base-line group).
We asked the judges to rate on ascale of 1-5 whether they thought the clausepairs were a good set of examples of a commonrelation linking the first clause in each pair tothe second one.Instances Instances Baseline BaselineScore StdDev Score StdDevAll 3.5461 0.4780 2.6341 0.4244O-S 3.9266 0.6058 2.8761 0.5096O-O 3.4938 0.5144 2.7464 0.6205S-O 3.4746 0.7340 2.5758 0.6314S-S 3.2398 0.4892 2.3584 0.5645Table 4: Results for human evaluationTable 4 reports the overall average scores forbaseline and instances groups, and for each ofthe four anchoring types individually.
The scoreswere averaged over all examples and all judges.An ANOVA showed the difference in scores be-tween the baseline and instance groups to besignificant (p < 0.001) in all four cases.Achievement of Model Goals: We em-ployed clustering in our model to overcome data-sparseness.
The importance of this decision wasevident in our results.
For example, the secondpattern shown in Table 2 appeared only fourtimes in the text.
In these instances, verb cluster40 was represented twice by the verb arrest andtwice by detain.
Two appearances are within thestatistical deviation of all but the rarest words,and would not have been detected as significantwithout the clustering effect.
This means thepattern would have been overlooked, despite thestrongly intuitive connection it represents.
Thesystem detected several such patterns.The other reason for clustering was general-ization.
Even in cases where patterns involvingsingle words could have been detected, it wouldhave been impossible to unify similar patternsinto generalized ones.
In addition, when encoun-tering a new clause which differs slightly from453the ones we recognized in the original data, therewould be no way to recognize it and draw the ap-propriate conclusions.
For example, there wouldbe no way to relate the sentence ?The typhoonapproached the coast?
to the fourth example pat-tern, and the connection with the resulting dam-age would not be recognized.5 Comparison with Previous WorkThe relationship between textual features andsemantics and the use of syntax as an indica-tor of semantics has been widespread.
Followingthe idea proposed in Harris?
Distributional Hy-pothesis (Harris, 1985), that words occurring insimilar contexts are semantically similar, manyworks have used different definitions of contextto identify various types of semantic similarity.Hindle (1990) uses a mutual-information basedmetric derived from the distribution of subject,verb and object in a large corpus to classifynouns.
Pereira et al (1993) cluster nouns ac-cording to their distribution as direct objectsof verbs, using information-theoretic tools (thepredecessors of the tools we use in this work).They suggest that information theoretic mea-sures can also measure semantic relatedness.These works focus only on relatedness of indi-vidual words and do not describe how the au-tomatic estimation of semantic similarity canbe useful in real-world tasks.
In our work wedemonstrate that using clusters as generalizedword units helps overcome the sparseness andgeneralization problems typically encounteredwhen attempting to extract high-level patternsfrom text, as required for many applications.The DIRT system (Lin and Pantel, 2001)deals with inference rules, and employs the no-tion of paths between two nouns in a sentence?sparse tree.
The system extracts such path struc-tures from text, and provides a similarity mea-sure between two such paths by comparing thewords which fill the same slots in the two paths.After extracting the paths, the system findsgroups of similar paths.
This approach bearsseveral similarities to the ideas described in thispaper, since our structure can be seen as aspecific path in the parse tree (probably themost basic one, see Fig.
1).
In our setup, sim-ilar clauses are clustered together in the sameClustered-Clause, which could be compared toclustering DIRT?s paths using its similarity mea-sure.
Despite these similarities, there are severalimportant differences between the two systems.Our method uses only the relationships insidethe path or clause in the clustering procedure,so the similarity is based on the structure it-self.
Furthermore, Lin and Pantel did not createpath clusters or generalized paths, so that whiletheir method allowed them to compare phrasesfor similarity, there is no convenient way to iden-tify high level contextual relationships betweentwo nearby sentences.
This is one of the signifi-cant advantages that clustering has over similar-ity measures - it allows a group of similar objectsto be represented by a single unit.There have been several attempts to formu-late and detect relationships at a higher contextlevel.
The VerbOcean project (Chklovski andPantel, 2004) deals with relations between verbs.It presents an automatically acquired networkof such relations, similar to the WordNet frame-work.
Though the patterns used to acquire therelations are usually parts of a single sentence,the relationships themselves can also be usedto describe connections between different sen-tences, especially the enablement and happens-before relations.
Since verbs are the central partof the clause, VerbOcean can be viewed as de-tecting relations between clauses as whole units,as well as those between individual words.
Asa solution to the data sparseness problem, webqueries are used.
Torisawa (2006) addresses asimilar problem, but focuses on temporal re-lations, and makes use of the phenomena ofJapanese coordinate sentences.
Neither of theseapproaches attempt to create generalized rela-tions or group verbs into clusters, though inthe case of VerbOcean this could presumablybe done using the similarity and strength valueswhich are defined and detected by the system.6 Future WorkThe clustered clause model presents many di-rections for further research.
It may be produc-tive to extend the model further, and includeother parts of the sentence, such as adjectives454and adverbs.
Clustering nouns by the adjectivesthat describe them may provide a more intu-itive grouping.
The addition of further elementsto the structure may also allow the detection ofnew kinds of relations.The news-oriented domain of the corpus weused strongly influenced our results.
If we wereinterested in more mundane relations, involvingday-to-day actions of individuals, a literary cor-pus would probably be more suitable.In defining our pattern template, several ele-ments were tailored specifically to our task.
Welimited ourselves to a very restricted set of pat-terns in order to better demonstrate the effec-tiveness of our model.
For a more general knowl-edge acquisition task, several of these restric-tions may be relaxed, allowing a much largerset of relations to be detected.
For example, aless strict significance filter, such as the supportand confidence measures commonly used in datamining, may be preferable.
These can be set todifferent thresholds, according to the user?s pref-erence.In our current work, in order to prevent over-generalization, we employed a single step post-processing algorithm which detected the incor-rect use of an entire cluster in place of a singleword (see footnote for Table 2).
This methodallows only two levels of generalization - sin-gle words and whole clusters.
A more appro-priate way to handle generalization would beto use a hierarchical clustering algorithm.
TheAgglomerative Information Bottleneck (Slonimand Tishby, 1999) is an example of such an al-gorithm, and could be employed for this task.Use of a hierarchical method would result inseveral levels of clusters, representing differentlevels of generalization.
It would be relativelyeasy to modify our procedure to reduce general-ization to the level indicated by the pattern ex-amples in the text, producing a more accuratedescription of the patterns detected.AcknowledgmentsThe author acknowledges the support of EPSRC grantEP/C538447/1.
The author would like to thank NaftaliTishby and Mirella Lapata for their supervision and as-sistance on large portions of the work presented here.
Iwould also like to thank the anonymous reviewers andmy friends and colleagues for their helpful comments.ReferencesBrody, Samuel.
2005.
Cluster-Based Pattern Recognitionin Natural Language Text .
Master?s thesis, HebrewUniversity, Jerusalem, Israel.Chklovski, T. and P. Pantel.
2004.
Verbocean: Miningthe web for fine-grained semantic verb relations.
InProc.
of EMNLP .
pages 33?40.Dagan, I., O. Glickman, and B. Magnini.
2005.
Thepascal recognising textual entailment challenge.
InProceedings of the PASCAL Challenges Workshop onRecognising Textual Entailment .Fellbaum, Christiane, editor.
1998.
WordNet: An Elec-tronic Database.
MIT Press, Cambridge, MA.Gwadera, R., M. Atallah, and W. Szpankowski.
2003.Reliable detection of episodes in event sequences.
InICDM .Harris, Z.
1985.
Distributional structure.
Katz, J.
J.(ed.)
The Philosophy of Linguistics pages 26?47.Hindle, Donald.
1990.
Noun classification from predicate-argument structures.
In Meeting of the ACL.
pages268?275.Hirst, G. and D. St-Onge.
1997.
Lexical chains as repre-sentation of context for the detection and correctionof malapropisms.
In WordNet: An Electronic LexicalDatabase, ed., Christiane Fellbaum.
MIT Press.Leacock, C. and M. Chodorow.
1997.
Combining localcontext and wordnet similarity for word sense identi-fication.
In WordNet: An Electronic Lexical Database,ed., Christiane Fellbaum.
MIT Press.Lin, Dekang.
1994.
Principar - an efficient, broad-coverage, principle-based parser.
In COLING.
pages482?488.Lin, Dekang and Patrick Pantel.
2001.
DIRT - discoveryof inference rules from text.
In Knowledge Discoveryand Data Mining .
pages 323?328.Pedersen, T., S. Patwardhan, and J. Michelizzi.
2004.Wordnet::similarity - measuring the relatedness of con-cepts.
In Proc.
of AAAI-04 .Pereira, F., N. Tishby, and L. Lee.
1993.
Distributionalclustering of english words.
In Meeting of the Associ-ation for Computational Linguistics.
pages 183?190.Resnik, Philip.
1995.
Using information content to eval-uate semantic similarity in a taxonomy.
In IJCAI .pages 448?453.Slonim, N., N. Friedman, and N. Tishby.
2002.
Unsu-pervised document classification using sequential in-formation maximization.
In Proc.
of SIGIR?02 .Slonim, N. and N. Tishby.
1999.
Agglomerative informa-tion bottleneck.
In Proc.
of NIPS-12 .Torisawa, Kentaro.
2006.
Acquiring inference rules withtemporal constraints by using japanese coordinatedsentences and noun-verb co-occurrences.
In Proceed-ings of NAACL.
pages 57?64.455
