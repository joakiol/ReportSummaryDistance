Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,pages 677?687, Dublin, Ireland, August 23-29 2014.Joint Opinion Relation Detection Using One-Class Deep Neural NetworkLiheng Xu, Kang Liu and Jun ZhaoNational Laboratory of Pattern RecognitionInstitute of Automation, Chinese Academy of Sciences, Beijing, 100190, China{lhxu, kliu, jzhao}@nlpr.ia.ac.cnAbstractDetecting opinion relation is a crucial step for fine-gained opinion summarization.
A valid opin-ion relation has three requirements: a correct opinion word, a correct opinion target and thelinking relation between them.
Previous works prone to only verifying two of these requirementsfor opinion extraction, while leave the other requirement unverified.
This could inevitably intro-duce noise terms.
To tackle this problem, this paper proposes a joint approach, where all threerequirements are simultaneously verified by a deep neural network in a classification scenario.Some seeds are provided as positive labeled data for the classifier.
However, negative labeleddata are hard to acquire for this task.
We consequently introduce one-class classification problemand develop a One-Class Deep Neural Network.
Experimental results show that the proposedjoint approach significantly outperforms state-of-the-art weakly supervised methods.1 IntroductionOpinion summarization aims to extract and summarize customers?
opinions from reviews on products orservices (Hu and Liu, 2004; Cardie et al., 2004).
With the rapid expansion of e-commerce, the number ofonline reviews is growing at a high speed, which makes it impractical for customers to read throughoutlarge amounts of reviews to choose better products.
Therefore, it is imperative to automatically gener-ate opinion summarization to help customers make more informed purchase decisions, where detectingopinion relation is a crucial step for opinion summarization.Before going further, we first introduce some notions.
An opinion relation, is a triple o = (s, t, r),where three factors are involved: s is an opinion word which refers to those words indicating sentimentpolarities; t is an opinion target, which can be any entity or aspect of an entity about which an opinion hasbeen expressed; r refers to the linking relation between s and t. As in Example 1, s={clear}, t={sceen},and there is a linking relation between the two words because clear is used to modify screen.Example 1.
This mp3 has a clear screen.For a valid opinion relation, there are three requirements corresponding to the three factors: (i) theopinion word indicates sentiment polarity; (ii) the opinion target is related to current domain; (iii) theopinion word modifies the opinion target.
Previous weakly supervised methods often expand a seed setand identify opinion relation either by co-occurrence statistics (Hu and Liu, 2004; Hai et al., 2012) orsyntactic dependencies (Popescu and Etzioni, 2005; Qiu et al., 2009) following the assumption below.Assumption 1.
Terms that are likely to have linking relation with the seed terms are believed to beopinion words or opinion targets.For example, if one has an opinion word seed clear (which satisfies requirement i), and one finds thatit modifies the word screen in Example 1 (which satisfies requirement iii).
Then one infers that screenis an opinion target according to Assumption 1 (whether screen is correct is not checked).
However, inExample 2(a), we can see that good is an opinion word and it modifies thing, but thing is not related toThis work is licenced under a Creative Commons Attribution 4.0 International License.
Page numbers and proceedings footerare added by the organizers.
License details: http://creativecommons.org/licenses/by/4.0/677mp3 domain.
If one follows Assumption 1, thing will be mistaken as an opinion target.
Similarly, inExample 2(b), if one uses mp3 to extract another as an opinion word, he may get an objective word.Example 2.
(a) This mp3 has many good things.
(b) Just another mp3 I bought.The reason for the errors above is that Assumption 1 only verifies two requirements for an opinionrelation.
Unfortunately, this issue occurs frequently in online reviews.
As a result, previous methodsoften suffer from these noise terms.
To produce more precise opinion summary, it is argued that we shallfollow a more restricted assumption as follows.Assumption 2.
The three requirements: the opinion word, the opinion target and the linking relationbetween them, shall be all verified during opinion relation detection.To make accordance with Assumption 2, this paper proposes a novel joint opinion relation detectionmethod, where opinion words, opinion targets and linking relations are simultaneously considered in aclassification scenario.
Following previous works, we provide a small set of seeds (i.e.
opinion wordsor targets) for supervision, which are regarded as positive labeled examples for classification.
However,negative labeled examples (i.e.
noise terms) are hard to acquire, because we do not know which termis not an opinion word or target.
This leads to One-Class Classification (OCC) problem (Moya et al.,1993).
The key to OCC is semantic similarity measuring between terms, and Deep Neural Network(DNN) with word embeddings is a powerful tool for handling this problem.
We consequently inte-grate DNN into a OCC classifier and develop a One-Class Deep Neural Network (OCDNN).
Concretely,opinion words/targets/relations are first represented by embedding vectors and then jointly classified.Experimental results show that the proposed joint method which follows Assumption 2 significantlyoutperforms state-of-the-art weakly supervised methods which are based on Assumption 1.2 Related WorkIn opinion relation detection task, previous works often used co-occurrence statistics or syntax informa-tion to identify opinion relations.
For co-occurrence statistical methods, Hu and Liu (2004) proposed apioneer research for opinion summarization based on association rules.
Popescu and Etzioni (2005) de-fined some syntactic patterns and used Pointwise Mutual Information (PMI) to extract product features.Hai et al.
(2012) proposed an opinion feature mining method which employed Likelihood Ratio Tests(LRT) (Dunning, 1993) as the co-occurrence statistical measure.
For syntax-based approaches, Riloffand Wiebe (2003) performed syntactic pattern learning while extracting subjective expressions.
Zhuanget al.
(2006) used various syntactic templates from an annotated movie corpus and applied them to su-pervised movie feature extraction.
Kobayashi et al.
(2007) identified opinion relations by searching foruseful syntactic contextual clues.
Qiu et al.
(2009) proposed a bootstrapping framework called DoublePropagation which introduced eight heuristic syntactic rules to detect opinion relations.However, none of the above methods could verify opinion words/targets/relations simultaneously dur-ing opinion relation detection.
To perform joint extraction, various models had been proposed, most ofwhich employed classification or sequence labeling models, such as HMM (Jin and Ho, 2009), SVM(Wu et al., 2009) and CRFs (Breck et al., 2007; Jakob and Gurevych, 2010; Li et al., 2010).
Besides, op-timal models such as Integer Linear Programming (ILP) were also employed to perform joint inferencefor opinion extraction (Choi et al., 2006; Yang and Cardie, 2013).Joint methods had been shown to achieve better performance than pipeline approaches.
Nevertheless,most existing joint models rely on full supervision, which have the difficulty of obtaining annotatedtraining data in practical applications.
Also, supervised models that are trained on one domain often failto give satisfactory results when shifted to another domain.
Our method does not require annotated data.3 The Proposed MethodTo detect opinion relations, previous methods often leverage some seed terms, such as opinion wordseeds (Hu and Liu, 2004; Baccianella et al., 2010) and opinion target seeds (Jijkoun et al., 2010; Haiet al., 2012).
These seeds can be used as positive labeled examples to train a classifier.
However, it ishard to get negative labeled examples for this task.
Because opinion words or targets are often domain678dependent and words that do not bear any sentiment polarity in one domain may be used to expressopinion in another domain.
It is also very hard to specify in what case there is no linking relationbetween two words.To deal with this problem, we employ one-class classification, and develop a One-Class Deep NeuralNetwork (OCDNN) for opinion relation detection.
The architecture of OCDNN is shown in Figure 1,which consists of two levels.
The lower level learns feature representations unsupervisedly for opinionwords/targets/relations, where the left component uses word embedding learning to represent opinionwords/targets, and the right component maps linking relations to embedding vectors by a recursive au-toencoder.
Then the upper level uses the learnt features to perform one-class classification.Figure 1: The architecture of OCDNN.Figure 2: An example of recursive autoencoder.3.1 Opinion Seed GenerationTo obtain training data for OCDNN, we shall first get some seed terms as follows.Opinion Word Seeds.
We manually pick 186 domain independent opinion words from SentiWordNet(Baccianella et al., 2010) as the opinion word seed set SS.Opinion Target Seeds.
Likelihood Ratio Tests (LRT) (Dunning, 1993) used in (Hai et al., 2012) isemployed to generate opinion target seeds.
LRT aims to measure how greatly two terms Tiand Tjareassociated with each other by sentence-level corpus statistics which is defined as follows,LRT = 2[logL(p1, k1, n1) + logL(p2, k2, n2)?
logL(p, k1, n1)?
logL(p, k2, n2)] (1)where k1= tf(Ti, Tj), k2= tf(Ti,?Tj), k3= tf(?Ti, Tj), k4= tf(?Ti,?Tj), tf(?)
denotes term frequency;L(p, k, n) = pk(1 ?
p)n?k, n1= k1+ k3, n2= k2+ k4, p1= k1/n1, p2= k2/n2and p =(k1+ k2)/(n1+ n2).
We measure LRT between a domain name (e.g.
mp3, hotel, etc.)
and all opiniontarget candidates.
Then N terms with highest LRT scores are added into the opinion target seed set TS.Linking Relation Seeds.
Linking relation can be naturally captured by syntactic dependency, becauseit directly models the modification relation between opinion word and opinion target.
We employ anautomatic syntactic opinion pattern learning method called Sentiment Graph Walking (Xu et al., 2013)and get 12 opinion patterns with highest confidence as the linking relation seed set RS.After seed generation, every opinion relation so= (ss, st, sr) in review corpus that satisfies ss?
SS,st?
TS and sr?
RS is taken as a positive labeled training instance.3.2 Opinion Relation Candidate GenerationThe opinion term candidate set is denoted by C = {SC, TC}, where SC/TC represents opinionword/target candidate.
Following previous works (Hu and Liu, 2004; Popescu and Etzioni, 2005; Qiuet al., 2009), we take adjectives or verbs as opinion word candidates, and take nouns or noun phrases asopinion target candidates.
A statistic-based method in Zhu et al.
(2009) is used to detect noun phrases.An opinion relation candidate is denoted by co= (cs, ct, cr), where cs?
SC, ct?
TC, and cris apotential linking relation.
To get cr, we first get dependency tree of a sentence using Stanford Parser (de679Marneffe et al., 2006).
Then, the shortest dependency path between a csand a ctis taken as a cr.
Toavoid introducing too many noise candidates, we constrain that there are at most four terms in a cr.3.3 Word Representation by Word Embedding LearningWord embedding, a.k.a word representation, is a mathematical object associated with each word, whichis often used in a vector form, where each dimension?s value corresponds to a feature and might evenhave a semantic or grammatical interpretation (Turian et al., 2010).
By word embedding learning, wordsare embedded into a hyperspace, where two words that are more semantically similar to each other arelocated closer.
This characteristic is precisely what we want, because the key to one-class classificationis semantic similarity measuring (illustrated in Section 3.5).For word representation, we use a matrix LT ?
Rn?|Vw|, where i-th column represents the embeddingvector for term ti, n is the size of embedding vector and Vwis the vocabulary of LT .
Therefore, wecan denote tiby a binary vector bi?
R|Vw|and get its embedding vector by xi= LTbi.
The trainingcriterion for word embeddings is,??
= argmin?
?c?C?v?Vwmax{0, 1?
s?
(c) + s?
(v)} (2)where ?
is the parameters of neural network used for training.
See Collobert et al.
(2011) for the detailedimplementation.3.4 Linking Relation Representation by Using Recursive AutoencoderThe goal of this section is to represent the linking relation between an opinion word and an opinion targetby a n-element vector as we do during word representation.
Specifically, we combine embedding vectorsof words in a linking relation by a recursive autoencoder (Socher et al., 2011) according to syntacticdependency structure.
In this way, linking relations are no longer limited to the initial seeds duringclassification, because linking relations that are similar to the seed relations will have similar vectorrepresentations.Figure 2 shows a linking relation representation process by an example: too loud to listen to the player.First, we get its dependency path between the opinion word cs:loud and the opinion target ct:player.Then csand ctare replaced by wildcards [SC] and [TC] because they are not concerned in the linkingrelation.
The dash line box in Figure 2 shows a standard autoencoder, which is a three-layer neuralnetwork, where the number of nodes in input layer is equal to that of output layer.
It takes two n-elementvectors as input and compresses semantics of the two vectors into one n-element vector in hidden layerby,y = f(W(dep)[x1;x2] + b), W(dep)=12[I1; I2; Ib] +  (3)where [x1;x2] is the concatenation of the two input vectors and f is the sigmoid function; W(dep)is aparameter matrix that is chosen according to the dependency relation between x1and x2(In the case ofy1, W(dep)= W(xcomp)), which is initialized by Ii, where Iiis a n ?
n unit matrix, Ibis a n-elementnull vector, and  is sampled from a uniform distribution U [?0.001, 0.001] (Socher et al., 2013).
ThenW(dep)are updated during training.
The training criterion of autoencoder is to minimize Euclideandistance between the original input and its output,Erae= ||[x1;x2]?
[x?1;x?2]||2(4)where [x?1;x?2] = W(out)y and W(out)is initialized by W(dep)T.We always start the combination process from [SC] and it is repeated along the dependency path.
Forexample, the result vector y1of the first combination is used as the input vector when computing y2.Finally, the linking relation is represented by a n-element vector (the green vector in Figure 2).6803.5 One-Class Classification for Opinion Relation DetectionWe represent an opinion relation candidate co= (cs, ct, cr) by a vector vo= [vs; vt; vr], which isa concatenation of the opinion word embedding vs, the opinion target embedding vtand the linkingrelation embedding vr.
Then vois feed to the upper level autoencoder in Figure 1.To perform one-class classification, the number of nodes in the hidden layer of the upper level autoen-coder is constrained to be smaller than that of the input layer.
By using such a ?bottleneck?
networkstructure, characteristics of the input are first compressed into the hidden layer and then reconstructedby the output layer (Japkowicz et al., 1995).
Concretely, characteristics of positive labeled opinion rela-tions are first compressed into the hidden layer, and then the autoencoder should be able to adequatelyreconstruct positive instances in the output layer, but should fail to reconstruct negative instances whichpresent different characteristics from positive instances.
Therefore, the detection of opinion relation isequivalent to assessing how well a candidate is reconstructed by the autoencoder.
As the input vectorvoconsists of representations for opinion words/targets/relations, characteristics of the three factors arejointly compressed by one hidden layer.
Either false opinion word/target/relation will lead to failure ofreconstruction.
Consequently, our approach follows Assumption 2.For opinion relation detection, candidates with reconstruction error scores that are smaller than athreshold ?
are classified as positive.
Determining the exact value of ?
is very difficult.
Inspired by otherone-class approaches (Liu et al., 2002; Manevitz and Yousef, 2007), we introduce some negative opinionterms to help to estimate ?.1Although negative instances are hard to acquire, Xu et al.
(2013) show thata set of general nouns (such as thing, one, etc., we denote them by GN ) seldom appear to be opiniontargets.
One the other hand, we create a 50-opinion-word validation set SV from SentiWordNet.To estimate ?, we first introduce a positive proportion (pp) score,pp(t) = tf+(t)/tf(t), t ?
PE, PE = {co|Er(co) < ?}
(5)where PE denotes the opinion relations that are classified as positive, Er(?)
is the reconstruction errorof OCDNN and tf+(?)
is the frequency of term in PE.
Then an error function E?is minimized, whichbalances between the proportion of non-target terms (GN ) in PE (which shall be as small as possible)and the proportion of opinion words in validation set (SV ) in PE (which shall be as large as possible).E?=?t?GN?PE[pp(t)?
0]2+?s?SV ?PE[pp(s)?
1]2(6)3.6 Opinion Target ExpansionWe apply bootstrapping to iteratively expand opinion target seeds.
It is because the vocabulary of seedset is limited, which cannot fully represent the distribution of opinion targets.
So we expand opiniontarget seeds in a self-training manner to alleviate this issue.
After training OCDNN, all opinion relationcandidates are classified, and opinion targets are ranked in descent order by,s(t) = log tf(t)?
pp(t).
(7)Then, top M candidates are added into the target seed set TS for the next training iteration.4 Experiments4.1 Datasets and Evaluation MetricsDatasets.
Three real world datasets are selected for evaluation.
The first one is called Customer ReviewDataset (CRD)2which contains reviews on five products (denoted by D1 to D5).
The second is a bench-mark dataset (Wang et al., 2011) on MP3 and Hotel3.
The last one is crawled from www.amazon.com,which involves Mattress and Phone.
Two annotating criteria are applied.1This is not in contradiction with OCC problem, because these negative examples are NOT used during training.2http://www.cs.uic.edu/ liub/FBS/sentiment-analysis.html3http://timan.cs.uiuc.edu/downloads.html681Annotation 1 is used to evaluate opinion words/targets extraction.
Firstly, 10,000 sentences are ran-domly selected from reviews and all possible terms are extracted along with their contexts.
Then, anno-tators are required to judge whether each term is an opinion word or an opinion target.Annotation 2 is used to evaluate intra-sentence opinion relation detection.
Annotators are required tocarefully read through each sentence and find out every opinion relation, which consists of an opinionword, an opinion target, as well as the linking relation between them.
The annotation is very labor-intensive, so only 5,000 sentences are annotated for MP3 and Hotel.Two annotators were required to annotate following the criteria above.
When conflicts happened, athird annotator would make the final judgment.
Note that Annotation 1 and Annotation 2 were annotatedby two different groups.
Detailed information of the annotated datasets are shown in Table 1.
Further-more, the kappa values between Annotation 1 and Annotation 2 are 0.88 for opinion words and 0.84 foropinion targets, showing highly substantial agreement.Domain #OW #OT Kappa OW Kappa OTHotel 434 1,015 0.72 0.67MP3 559 1,158 0.69 0.65Mattress 366 523 0.67 0.62Phone 391 862 0.68 0.64(a) Annotation 1Domain #LR #OW #OT Kappa LRHotel 2,196 317 735 0.62MP3 2,328 342 791 0.61(b) Annotation 2Table 1: The detailed information of Annotations.
OW/OT/LR stands for opinion words/opinion tar-gets/linking relations.
The Kappa-values are calculated by using exact matching metric for Annotation 1and overlap matching metric for Annotation 2.Evaluation Metrics.
We perform evaluation in terms of Precision(P), Recall(R) and F-measure(F)according to exact and overlap matching metrics (Wiebe et al., 2005).
The exact metric is used toevaluate opinion word/target extraction, which requires exact string match.
And the overlap metric isused to evaluate opinion relation detection, where an extracted opinion relation is regarded as correctwhen both the opinion word and the opinion target in it overlap with the gold standard.4Evaluation Settings.
Four state-of-the-art weakly supervised approaches are selected as competi-tors.
Two are co-occurrence statistical methods and two are syntax-based methods, all of which followAssumption 1.AdjRule extracts opinion words/targets by using adjacency rules (Hu and Liu, 2004).LRTBOOT is a bootstrapping algorithm which employs Likelihood Ratio Tests (Dunning, 1993) asthe co-occurrence statistical measure (Hai et al., 2012).DP denotes the Double Propagation algorithm (Qiu et al., 2009).DP-HITS is an enhanced version of DP proposed by Zhang et al.
(2010), which ranks terms bys(t) = log tf(t)?
importance(t) (8)where importance(t) is estimated by the HITS algorithm (Kleinberg, 1999).OCDNN is the proposed method.
The target seed size N = 40, the opinion targets expanded in eachiteration M = 20, and the max bootstrapping iteration number is X = 10.
The representation learningin lower level of OCDNN is trained on the whole corpus, while the test data are the same for all settings.All results of OCDNN are taken by average performance over five runs with randomized parameters.4.2 OCDNN vs. the State-of-the-artWe compare OCDNN with state-of-the-art methods for opinion words/targets extraction.
In OCDNN,Eq.
7 is used to rank opinion words/targets.
The results on CRD and the four domains are shown inTable 2 and Table 3.
DP-HITS does not extract opinion words so their results for opinion words are nottaken into account.4Determining the exact boundaries of opinion terms is hard even for human (Wiebe et al., 2005), so we use this relaxation.682Opinion TargetsMethodD1 D2 D3 D4 D5Avg.P R F P R F P R F P R F P R F FAdjRule 0.75 0.82 0.78 0.71 0.79 0.75 0.72 0.76 0.74 0.69 0.82 0.75 0.74 0.80 0.77 0.76DP 0.87 0.81 0.84 0.90 0.81 0.85 0.90 0.86 0.88 0.81 0.84 0.82 0.92 0.86 0.89 0.86DP-HITS 0.83 0.84 0.83 0.86 0.85 0.85 0.86 0.88 0.87 0.80 0.85 0.82 0.86 0.86 0.86 0.85LRTBOOT 0.77 0.87 0.82 0.74 0.90 0.81 0.79 0.89 0.84 0.72 0.88 0.79 0.74 0.88 0.80 0.81OCDNN 0.83 0.82 0.82 0.86 0.85 0.85 0.86 0.87 0.86 0.78 0.84 0.81 0.89 0.85 0.87 0.84Opinion WordsAdjRule 0.57 0.75 0.65 0.51 0.76 0.61 0.57 0.73 0.64 0.54 0.62 0.58 0.62 0.67 0.64 0.62DP 0.64 0.73 0.68 0.57 0.79 0.66 0.65 0.70 0.67 0.61 0.65 0.63 0.70 0.68 0.69 0.67LRTBOOT 0.60 0.79 0.68 0.52 0.82 0.64 0.60 0.76 0.67 0.56 0.70 0.62 0.66 0.71 0.68 0.66OCDNN 0.64 0.77 0.70 0.63 0.79 0.70 0.66 0.73 0.69 0.68 0.70 0.69 0.70 0.69 0.69 0.70Table 2: Results of opinion terms extraction on Customer Review Dataset.Opinion TargetsMethodMP3 Hotel Mattress PhoneAvg.P R F P R F P R F P R F FAdjRule 0.53 0.55 0.54 0.55 0.57 0.56 0.50 0.60 0.55 0.52 0.51 0.51 0.54DP 0.66 0.57 0.61 0.66 0.60 0.63 0.55 0.60 0.57 0.60 0.53 0.56 0.59DP-HITS 0.65 0.62 0.63 0.64 0.66 0.65 0.55 0.67 0.60 0.62 0.64 0.63 0.63LRTBOOT 0.60 0.77 0.67 0.59 0.78 0.67 0.55 0.78 0.65 0.57 0.76 0.65 0.66OCDNN 0.70 0.68 0.69 0.71 0.70 0.70 0.63 0.69 0.66 0.69 0.68 0.68 0.68Opinion WordsAdjRule 0.48 0.65 0.55 0.51 0.68 0.58 0.51 0.68 0.58 0.48 0.61 0.54 0.56DP 0.58 0.62 0.60 0.60 0.66 0.63 0.54 0.68 0.60 0.55 0.59 0.57 0.60LRTBOOT 0.52 0.69 0.59 0.54 0.74 0.62 0.51 0.73 0.60 0.50 0.68 0.58 0.60OCDNN 0.68 0.65 0.66 0.70 0.68 0.69 0.59 0.70 0.64 0.63 0.59 0.61 0.65Table 3: Results of opinion terms extraction on the four domains.From Table 2, we can see that our method outperforms co-occurrence-based methods AdjRule andLRTBOOT, but achieves comparable or a little worse results than syntax-based methods DP and DP-HITS.
This is because CRD is quite small, which only contains several hundred sentences for each prod-uct review set.
In this case, methods based on careful-designed syntax rules have superiority over thosebased on statistics (Liu et al., 2013).
For results on larger datasets shown in Table 3, our method out-performs all of the competitors.
Comparing OCDNN with DP-HITS, the two approaches use similarterm ranking metrics (Eq.
7 and Eq.
8), but OCDNN significantly outperforms DP-HITS.
Therefore, thepositive proportion score estimated by OCDNN is more effective than the importance score in DP-HITS.Comparing OCDNN with LRTBOOT, we find that LRTBOOT achieves better recall but lower precision.This is because LRTBOOT follows Assumption 1 during bootstrapping, which suffers a lot from errorpropagation, while our joint classification approach effectively alleviates this issue.
We will discuss theimpact of error propagation in detail later.4.3 Assumption 1 vs.
Assumption 2This section evaluates intra-sentence opinion relation detection, which is more useful for practical appli-cations.
It also reflects the impacts of Assumption 1 and Assumption 2.
The results are shown in Table4 and Table 5, where OCDNN significantly outperforms all competitors.
The average improvement ofF-measure over the best competitor is 6% on CRD and 9% on Hotel and MP3.As Assumption 1 only verifies two of the requirements in an opinion relation, it would inevitablyintroduce noise terms during extraction.
For syntax-based method DP, it extracts many false opinionrelations such as good thing and nice one (where thing and one are false opinion targets) or objectiveexpressions like another mp3 and every mp3 (which contain false opinion words another and every).
Forco-occurrence statistical methods AdjRule and LRTBOOT, it is very hard to deal with ambiguous linkingrelations.
For example, in phrase this mp3 is very good except the size, co-occurrence statistical methodscould hardly tell which opinion target does good modify (mp3 or size).
Our method follows Assumption683MethodD1 D2 D3 D4 D5Avg.P R F P R F P R F P R F P R F FAdjRule 0.51 0.66 0.58 0.53 0.63 0.58 0.50 0.61 0.55 0.48 0.60 0.53 0.50 0.61 0.55 0.56DP 0.66 0.63 0.64 0.68 0.60 0.64 0.69 0.62 0.65 0.66 0.57 0.61 0.67 0.60 0.63 0.64LRTBOOT 0.53 0.70 0.60 0.57 0.72 0.64 0.55 0.69 0.61 0.52 0.70 0.60 0.55 0.68 0.61 0.61OCDNN 0.76 0.66 0.71 0.74 0.67 0.70 0.77 0.67 0.72 0.70 0.65 0.67 0.77 0.66 0.71 0.70Table 4: Results of opinion relation detection on Customer Review Dataset.MethodMP3 HotelAvg.P R F P R F FAdjRule 0.49 0.55 0.52 0.45 0.53 0.49 0.50DP 0.63 0.51 0.56 0.59 0.50 0.54 0.55LRTBOOT 0.54 0.63 0.58 0.50 0.60 0.55 0.56OCDNN 0.73 0.60 0.66 0.70 0.59 0.64 0.65Table 5: Results of opinion relation detection on the two domains.2, which verifies all three requirements for opinion word/target/relation in an opinion relation, so theabove errors are greatly reduced.
Therefore, Assumption 2 is more reasonable than Assumption 1.4.4 The Effect of Joint ClassificationWe evaluate the three bootstrapping methods (DP, LRTBOOT and OCDNN) for opinion target expansion.The precision of each iteration is shown in Figure 3.
We can see that DP and LRTBOOT gradually sufferfrom error propagation and the precision drops quickly along with the number of iteration increases.
ForOCDNN, although error propagation is inevitable, the precision curve retains at a high level.
Therefore,the joint approach produces more precise results.For more detailed analysis, we give a variation of the proposed method named 3NN, which uses3 individual autoencoders to classify opinion words/targets/relations separately.
An opinion relationcandidate is classified as positive only when the three factors are all classified as positive.
Then opinionrelations are ranked by the sum of reconstruction scores of the three factors.
In the results of opinionrelation detection, when the recall is fixed at 0.6, the precisions of 3NN are 0.67 for MP3 and 0.65for Hotel, while the precisions of OCDNN are 0.73 for MP3 and 0.70 for Hotel.
Therefore, OCDNNachieves much better performance than 3NN.An example may explain the reason of why 3NN gets worse performance.
In our experiment on Hotel,a false opinion relation happy day is misclassified as positive by 3NN.
It is because the word day hasa small reconstruction score in 3NN.
At the same time, happy is a correct opinion word, so the wholeexpression happy day also has a small reconstruction score and then be misclassified.
In contrast, thereconstruction score of happy day from OCDNN is quite large so the phrase is dropped.
The reasonis that the joint approach captures the semantic of a whole phrase rather than its single components.Therefore, it is more reasonable.1 2 3 4 5 6 7 8 9 10.5.6.7.8.91.0OCDNNDPLRTBOOT(a) MP31 2 3 4 5 6 7 8 9 10.5.6.7.8.91.0OCDNNDPLRTBOOT(b) Hotel1 2 3 4 5 6 7 8 9 10.5.6.7.8.91.0OCDNNDPLRTBOOT(c) Mattress1 2 3 4 5 6 7 8 9 10.5.6.7.8.91.0OCDNNDPLRTBOOT(d) PhoneFigure 3: Precision (y-axis) of opinion target seed expansion at each bootstrapping iteration (x-axis).6845 Conclusion and Future WorkThis paper proposes One-Class Deep Neural Network for joint opinion relation detection in one-classclassification scenario, where opinion words/targets/relations are simultaneously verified during classifi-cation.
Experimental results show the proposed method significantly outperforms state-of-the-art weaklysupervised methods that only verify two factors in an opinion relation.In future work, we plan to adapt our method and make it be capable of capturing implicit opinionrelations.AcknowledgementThis work was sponsored by the National Natural Science Foundation of China (No.
61202329 and No.61333018) and CCF-Tencent Open Research Fund.ReferencesStefano Baccianella, Andrea Esuli, and Fabrizio Sebastiani.
2010.
Sentiwordnet 3.0: An enhanced lexical re-source for sentiment analysis and opinion mining.
Seventh conference on International Language Resourcesand Evaluation, pages 2200?2204.Eric Breck, Yejin Choi, and Claire Cardie.
2007.
Identifying expressions of opinion in context.
In Proceedingsof the 20th international joint conference on Artifical intelligence, IJCAI?07, pages 2683?2688, San Francisco,CA, USA.
Morgan Kaufmann Publishers Inc.Claire Cardie, Janyce Wiebe, Theresa Wilson, and Diane Litman.
2004.
Low-level annotations and summaryrepresentations of opinions for multi-perspective question answering.Yejin Choi, Eric Breck, and Claire Cardie.
2006.
Joint extraction of entities and relations for opinion recognition.In Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing, EMNLP ?06,pages 431?439, Stroudsburg, PA, USA.
Association for Computational Linguistics.Ronan Collobert, Jason Weston, L?eon Bottou, Michael Karlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011.Natural language processing (almost) from scratch.
Journal of Machine Learning Research, 12:2493?2537,November.Marie-Catherine de Marneffe, Bill MacCartney, and Christopher D. Manning.
2006.
Generating typed dependencyparses from phrase structure parses.
In Proceedings of the IEEE / ACL 2006 Workshop on Spoken LanguageTechnology.
The Stanford Natural Language Processing Group.Ted Dunning.
1993.
Accurate methods for the statistics of surprise and coincidence.
Comput.
Linguist., 19(1):61?74, March.Zhen Hai, Kuiyu Chang, and Gao Cong.
2012.
One seed to find them all: mining opinion features via association.In Proceedings of the 21st ACM international conference on Information and knowledge management, CIKM?12, pages 255?264, New York, NY, USA.
ACM.Minqing Hu and Bing Liu.
2004.
Mining and summarizing customer reviews.
In Proceedings of the tenth ACMSIGKDD international conference on Knowledge discovery and data mining, pages 168?177.Niklas Jakob and Iryna Gurevych.
2010.
Extracting opinion targets in a single- and cross-domain setting withconditional random fields.
In Proceedings of the 2010 Conference on Empirical Methods in Natural LanguageProcessing, EMNLP ?10, pages 1035?1045, Stroudsburg, PA, USA.
Association for Computational Linguistics.Nathalie Japkowicz, Catherine Myers, and Mark Gluck.
1995.
A novelty detection approach to classification.
InProceedings of the 14th international joint conference on Artificial intelligence - Volume 1, IJCAI?95, pages518?523, San Francisco, CA, USA.
Morgan Kaufmann Publishers Inc.Valentin Jijkoun, Maarten de Rijke, and Wouter Weerkamp.
2010.
Generating focused topic-specific sentimentlexicons.
In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, ACL?10, pages 585?594, Stroudsburg, PA, USA.
Association for Computational Linguistics.Wei Jin and Hung Hay Ho.
2009.
A novel lexicalized hmm-based learning framework for web opinion mining.
InProceedings of the 26th Annual International Conference on Machine Learning, ICML ?09, pages 465?472.685Jon M. Kleinberg.
1999.
Authoritative sources in a hyperlinked environment.
J. ACM, 46(5):604?632, September.Nozomi Kobayashi, Kentaro Inui, and Yuji Matsumoto.
2007.
Extracting aspect-evaluation and aspect-of re-lations in opinion mining.
In Proceedings of the 2007 Joint Conference on Empirical Methods in NaturalLanguage Processing and Computational Natural Language Learning (EMNLP-CoNLL), pages 1065?1074,Prague, Czech Republic, June.
Association for Computational Linguistics.Fangtao Li, Chao Han, Minlie Huang, Xiaoyan Zhu, Ying-Ju Xia, Shu Zhang, and Hao Yu.
2010.
Structure-awarereview mining and summarization.
In Proceedings of the 23rd International Conference on ComputationalLinguistics, COLING ?10, pages 653?661, Stroudsburg, PA, USA.
Association for Computational Linguistics.Bing Liu, Wee Sun Lee, Philip S. Yu, and Xiaoli Li.
2002.
Partially supervised classification of text documents.In Proceedings of the Nineteenth International Conference on Machine Learning, ICML ?02, pages 387?394,San Francisco, CA, USA.
Morgan Kaufmann Publishers Inc.Kang Liu, Liheng Xu, and Jun Zhao.
2013.
Syntactic patterns versus word alignment: Extracting opinion tar-gets from online reviews.
In Proceedings of the 51st Annual Meeting of the Association for ComputationalLinguistics (Volume 1: Long Papers), pages 1754?1763, August.Larry Manevitz and Malik Yousef.
2007.
One-class document classification via neural networks.
Neurocomput-ing, 70(7C9):1466?1481.Mary M. Moya, Mark W. Koch, and Larry D. Hostetler.
1993.
One-class classifier networks for target recognitionapplications.
In Proceedings world congress on neural networks, pages 797?801.Ana-Maria Popescu and Oren Etzioni.
2005.
Extracting product features and opinions from reviews.
In Proceed-ings of the conference on Human Language Technology and Empirical Methods in Natural Language Process-ing, HLT ?05, pages 339?346.Guang Qiu, Bing Liu, Jiajun Bu, and Chun Chen.
2009.
Expanding domain sentiment lexicon through doublepropagation.
In Proceedings of the 21st international jont conference on Artifical intelligence, IJCAI?09, pages1199?1204.Ellen Riloff and Janyce Wiebe.
2003.
Learning extraction patterns for subjective expressions.
In Proceedingsof the 2003 conference on Empirical methods in natural language processing, EMNLP ?03, pages 105?112,Stroudsburg, PA, USA.
Association for Computational Linguistics.Richard Socher, Jeffrey Pennington, Eric H. Huang, Andrew Y. Ng, and Christopher D. Manning.
2011.
Semi-supervised recursive autoencoders for predicting sentiment distributions.
In Proceedings of the Conferenceon Empirical Methods in Natural Language Processing, EMNLP ?11, pages 151?161, Stroudsburg, PA, USA.Association for Computational Linguistics.Richard Socher, John Bauer, Christopher D. Manning, and Ng Andrew Y.
2013.
Parsing with compositionalvector grammars.
In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics(Volume 1: Long Papers), pages 455?465, Sofia, Bulgaria, August.
Association for Computational Linguistics.Joseph Turian, Lev Ratinov, and Yoshua Bengio.
2010.
Word representations: a simple and general method forsemi-supervised learning.
In Proceedings of the 48th Annual Meeting of the Association for ComputationalLinguistics, ACL ?10, pages 384?394, Stroudsburg, PA, USA.
Association for Computational Linguistics.Hongning Wang, Yue Lu, and ChengXiang Zhai.
2011.
Latent aspect rating analysis without aspect keywordsupervision.
In Proceedings of the 17th ACM SIGKDD international conference on Knowledge discovery anddata mining, pages 618?626.Janyce Wiebe, Theresa Wilson, and Claire Cardie.
2005.
Annotating expressions of opinions and emotions inlanguage.
Language Resources and Evaluation, 39(2-3):165?210.Yuanbin Wu, Qi Zhang, Xuanjing Huang, and Lide Wu.
2009.
Phrase dependency parsing for opinion mining.In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing: Volume 3 -Volume 3, EMNLP ?09, pages 1533?1541, Stroudsburg, PA, USA.
Association for Computational Linguistics.Liheng Xu, Kang Liu, Siwei Lai, Yubo Chen, and Jun Zhao.
2013.
Mining opinion words and opinion targetsin a two-stage framework.
In Proceedings of the 51st Annual Meeting of the Association for ComputationalLinguistics (Volume 1: Long Papers), pages 1764?1773, Sofia, Bulgaria, August.
Association for ComputationalLinguistics.686Bishan Yang and Claire Cardie.
2013.
Joint inference for fine-grained opinion extraction.
In Proceedings ofthe 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages1640?1649, Sofia, Bulgaria, August.
Association for Computational Linguistics.Lei Zhang, Bing Liu, Suk Hwan Lim, and Eamonn O?Brien-Strain.
2010.
Extracting and ranking product featuresin opinion documents.
In Proceedings of the 23rd International Conference on Computational Linguistics:Posters, COLING ?10, pages 1462?1470, Stroudsburg, PA, USA.
Association for Computational Linguistics.Li Zhuang, Feng Jing, and Xiao-Yan Zhu.
2006.
Movie review mining and summarization.
In Proceedings of the15th ACM international conference on Information and knowledge management, CIKM ?06, pages 43?50, NewYork, NY, USA.
ACM.687
