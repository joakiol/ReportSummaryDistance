Proceedings of the 2nd Workshop on Computational Approaches to Subjectivity and Sentiment Analysis, ACL-HLT 2011, pages 168?174,24 June, 2011, Portland, Oregon, USA c?2011 Association for Computational LinguisticsTowards a Unified Approach for Opinion Question Answering andSummarizationElena Lloret and Alexandra Balahur and Manuel Palomar and Andre?s MontoyoDepartment of Software and Computing SystemsUniversity of AlicanteAlicante 03690, Spain{elloret,abalahur, mpalomar, montoyo}@dlsi.ua.esAbstractThe aim of this paper is to present an ap-proach to tackle the task of opinion questionanswering and text summarization.
Follow-ing the guidelines TAC 2008 Opinion Sum-marization Pilot task, we propose new meth-ods for each of the major components of theprocess.
In particular, for the informationretrieval, opinion mining and summarizationstages.
The performance obtained improveswith respect to the state of the art by approxi-mately 12.50%, thus concluding that the sug-gested approaches for these three componentsare adequate.1 IntroductionSince the birth of the Social Web, users play a cru-cial role in the content appearing on the Internet.With this type of content increasing at an exponen-tial rate, the field of Opinion Mining (OM) becomesessential for analyzing and classifying the sentimentfound in texts.Nevertheless, real-world applications of OM of-ten require more than an opinion mining component.On the one hand, an application should allow a userto query about opinions in natural language.
There-fore, Question Answering (QA) techniques must beapplied in order to determine the information re-quired by the user and subsequently retrieve andanalyze it.
On the other hand, opinion mining of-fers mechanisms to automatically detect and classifysentiments in texts, overcoming the issue given bythe high volume of such information present on theInternet.
However, in many cases, even the result ofthe opinion processing by an automatic system stillcontains large quantities of information, which arestill difficult to deal with manually.
For example,for questions such as ?Why do people like GeorgeClooney??
we can find thousands of answers on theWeb.
Therefore, finding the relevant opinions ex-pressed on George Clooney, classifying them andfiltering only the positive opinions is not helpfulenough for the user.
He/she will still have to siftthrough thousands of texts snippets, containing rele-vant, but also much redundant information.
For that,we need to use Text Summarization (TS) techniques.TS provides a condensed version of one or severaldocuments (i.e., a summary) which can be used as asubstitute of the original ones (Spa?rck Jones, 2007).In this paper, we will concentrate on proposing ad-equate solutions to tackle the issue of opinion ques-tion answering and summarization.
Specifically, wewill propose methods to improve the task of ques-tion answering and summarization over opinionateddata, as defined in the TAC 2008 ?Opinion Sum-marization pilot?1.
Given the performance improve-ments obtained, we conclude that the approaches weproposed for these three components are adequate.2 Related WorkResearch focused on building factoid QA systemshas a long tradition, however, it is only recently thatstudies have started to focus on the creation and de-velopment of opinion QA systems.
Example of thiscan be (Stoyanov et al, 2004) who took advantage ofopinion summarization to support Multi-PerspectiveQA system, aiming at extracting opinion-orientedinformation of a question.
(Yu and Hatzivassiloglou,2003) separated opinions from facts and summa-rized them as answer to opinion questions.
Apartfrom these studies, specialized competitions for sys-tems dealing with opinion retrieval and QA havebeen organized in the past few years.
The TAC2008 Opinion Summarization Pilot track proposeda mixed setting of factoid and opinion questions.1http://www.nist.gov/tac/2008/summarization/168It is interesting to note that most of the participat-ing systems only adapted their factual QA systemsto overcome the newly introduced difficulties re-lated to opinion mining and polarity classification.Other relevant competition focused on the treatmentof subjective data is the NTCIR MOAT (Multilin-gual Opinion Analysis Test Collection).
The ap-proaches taken by the participants in this task are rel-evant to the process of opinion retrieval, which is thefirst step performed by an opinion mining questionanswering system.
For example, (Taras Zabibalov,2008) used an almost unsupervised approach ap-plied to two of the sub-tasks: opinionated sentenceand topic relevance detection.
(Qu et al, 2008) ap-plied a sequential tagging approach at the token leveland used the learned token labels in the sentencelevel classification task and their formal run submis-sion was is trained on MPQA (Wiebe et al, 2005).3 Text Analysis ConferencesIn 2008, the Opinion Summarization Pilot task atthe Text Analysis Conferences2 (TAC) consisted ingenerating summaries from blogs, according to spe-cific opinion questions provided by the TAC orga-nizers.
Given a set of blogs from the Blog06 col-lection3 and a list of questions, participants had toproduce a summary that answered these questions.The questions generally required determining opin-ion expressed on a target, each of which dealt with asingle topic (e.g.
George Clooney).
Additionally, aset of text snippets were also provided, which con-tained the answers to the questions.
Table 1 depictsan example of target, question, and optional snippet.Target: George ClooneyQuestions: Why do people like George Clooney?Why do people dislike George Clooney?Snippets: 1050 BLOG06-20060209-006-0013539097he?s a great actor.Table 1: Example of target, question, and snippet.Following the results obtained in the evaluationat TAC 2008 (Balahur et al, 2008), we proposean opinion question answering and summarization(OQA&S) approach, which is described in detail inthe following sections.2www.nist.gov/tac/3http://ir.dcs.gla.ac.uk/test collections/access to data.html4 An Opinion Question Answering andSummarization ApproachIn order to improve the results of the OQA&S sys-tem presented at TAC, we propose new methods foreach of the major components of the system: infor-mation retrieval, opinion mining and text summa-rization.4.1 Opinion Question Answering andSummarization Components?
Information RetrievalJAVA Information Retrieval system (JIRS) isa IR system especially suited for QA tasks(Go?mez, 2007).
Its purpose is to find frag-ments of text (passages) with more probabil-ity of containing the answer to a user questionmade in natural language instead of finding rel-evant documents for a query.
To that end, JIRSuses the own question structure and tries tofind an equal or similar expression in the docu-ments.
The more similar the structure betweenthe question and the passage is, the higher thepassage relevance.JIRS is able to find question structures in alarge document collection quickly and effi-ciently using different n-gram models.
Subse-quently, each passage is assessed depending onthe extracted n-grams, the weight of these n-grams, and the relative distance between them.Finally, it is worth noting that the number ofpassages in JIRS is configurable, and in thisresearch we are going to experiment with pas-sages of length 1 and 3.?
Opinion MiningThe first step we took in our approach wasto determine the opinionated sentences, as-sign each of them a polarity (positive or neg-ative) and a numerical value corresponding tothe polarity strength (the higher the negativescore, the more negative the sentence and viceversa).
In our first approximation (OMaprox1),we employed a simple, yet efficient method,presented in Balahur et al (Balahur et al,2009).
As lexicons for affect detection, weused WordNet Affect (Strapparava and Vali-tutti, 2004), SentiWordNet (Esuli and Sebas-169tiani, 2006), and MicroWNOp (Cerini et al,2007).
Each of the resources we employedwere mapped to four categories, which weregiven different scores: positive (1), negative(-1), high positive (4) and high negative (-4).First, the score of each of the blog posts wascomputed as the sum of the values of the wordsthat were identified.
Subsequently, we per-formed sentence splitting4 and classified thesentences we thus obtained according to theirpolarity, by adding the individual scores of theaffective words identified.In the second approach (OMaprox2), we firstfilter out the sentences that are associated tothe topic discussed, using LSA.
Further on, wescore the sentences identified as relating to thetopic of the blog post, in the same manner asin the previous approach.
The aim of this ap-proach is to select for further processing onlythe sentences which contain opinions on thepost topic.
In order to filter these sentencesin, we first create a small corpus of blog postson each of the topics included in our collec-tion5.
For each of the corpora obtained, weapply LSA, using the Infomap NLP Software6.Subsequently, we compute the 100 most asso-ciated words with two of the terms that are mostassociated with each of the topics and the 100most associated words with the topic word.
Theapproach was proven to be successful in (Bal-ahur et al, 2010).?
Text SummarizationThe text summarization approach used in thispaper was presented in (Lloret and Palomar,2009).
In order to generate a summary, thesuggested approach first carries out a basic pre-processing stage comprising HTML parsing,sentence segmentation, tokenization, and stem-ming.
Once the input document or documentshave been pre-processed, a relevance detectionstage, which is the core part of the approach, isapplied.
The objective of this step is to identify4http://alias-i.com/lingpipe/5These small corpora (30 posts for each of the top-ics) are gathered using the search on topic words onhttp://www.blogniscient.com/ and crawling the resulting pages.6http://infomap-nlp.sourceforge.net/potential relevant sentences in the document bymeans of three techniques: textual entailment,term frequency and the code quantity principle(Givo?n, 1990).
Then, each potential relevantsentence is given a score which is computedon the basis of the aforementioned techniques.Finally, all sentences are ordered accordingto their scores, and the highest ranked ones(which mean those sentences contain more im-portant information) are selected and extractedup to the desired length, thus building the fi-nal summary.
It is worth stressing upon the factthat in an attempt to maintain the coherence ofthe original documents, sentences are shown inthe same order they appear in the original doc-uments.4.2 Experimental FrameworkThe objective of this section is to describe the corpusused and the experiments performed with the dataprovided in TAC 2008 Opinion Summarization Pi-lot7 task.
The approaches analyzed comprise:?
OQA&S: The three components explainedin the previous section (information retrieval,opinion mining and summarization) werebound together in order to produce summariesthat include the answer to opinionated ques-tions.
First, the most relevant passages oflength 1 and 3 are retrieved by the IR module,as in the aforementioned approach, and thenthe subjective information is found and classi-fied within them using the OM approaches de-scribed in the previous section.
Further on, weincorporate the TS module, to select and ex-tract the most relevant opinionated facts fromthe pool of subjective information identifiedby the OM module.
We generate opinion-oriented summaries of compression rates rang-ing from 10% to 50%.
In the end, four dif-ferent approaches result from the integrationof the three components: IRp1-OMaprox1-TS; IRp1-OMaprox2-TS; IRp3-OMaprox1-TS; and IRp3-OMaprox2-TS.Moreover, apart from these approaches, two base-lines were also defined.
On the one hand, we sug-7http://www.nist.gov/tac/data/past-blog06/2008/OpSummQA08.html#OpSumm170gest a baseline using the list of snippets provided bythe TAC organization (QA-snippets).
This baselineproduces a summary by joining all the answers in thesnippets that related to the same topic On the otherhand, we took as a second baseline the approachfrom our participation in TAC 2008 (DLSIUAES),without not taking into account any information re-trieval or question answering system to retrieve thefragments of information which may be relevant tothe query.
In contrast, this was performed by com-puting the cosine similarity8 between each sentencein the blog and the query.
After all the potential rel-evant sentences for the query were identified, theywere classified in terms of subjectivity and polarity,and the most relevant ones were selected for the finalsummary.4.3 Evaluation MethodologySince we used the corpus provided at the OpinionSummarization Pilot task, and we followed simi-lar guidelines, we should evaluate our OQA&S ap-proach in the same way as participant systems wereassessed.
However, the evaluation methodologyproposed differs slightly from the one carried outin the competition.
The reason why we took suchdecision was due to the fact that the evaluation car-ried out in TAC had some limitations, and thereforewas not suitable for our purposes.
In this manner,our evaluation is also based on the gold-standardnuggets provided by TAC, but in addition we pro-posed an extended version of them, by adding otherpieces of information that are also relevant to thetopics.In this section, all the issues concerning the eval-uation are explained.
These comprise the originalevaluation method used in the Opinion Summariza-tion Pilot task at TAC (Section 4.3.1) , its draw-backs (Section 4.3.2), and the extended version forthe evaluation method we propose (Section 4.3.3).Further on, the results obtained together with a widediscussion, as well as its comparison with the base-lines and the TAC participants is provided in Section4.4.4.3.1 Nugget-based Evaluation at TACWithin the Opinion Summarization Pilot task,each summary was evaluated according to its con-8http://www.d.umn.edu/ tpederse/text-similarity.htmltent using the Pyramid method (Nenkova et al,2007).
A list of nuggets was provided and the asses-sors used such list of nuggets to count the numberof nuggets a summary contained.
Depending on thenumber of nuggets the summary included and theimportance of each one given by their weight, thevalues for recall, precision and F-measure were ob-tained.
An example of several nuggets correspond-ing to different topics can be seen in Table 2, wherethe weight for each one is also shown in brackets.Topic Nugget (weight)Carmax CARMAX prices are firm, the price isthe price (0.9)Jiffy Lube They should have torque wrenches (0.2)Talk show hosts Funny (0.78)Table 2: Example of evaluation nuggets and associatedweights.4.3.2 Limitations of the Nugget EvaluationThe evaluation method suggested at TAC requiresa lot of human effort when it comes to identifythe relevant fragments of information (nuggets) andcompute how many of them a summary contains, re-sulting in a very costly and time-consuming task.This is a general problem associated to the evalua-tion of summaries, which makes the task of summa-rization evaluation especially hard and difficult.But, apart from this, when an exhaustive exam-ination of the nuggets used in TAC is done, someother problems arised which are worth mentioning.The average number of nuggets for each topic is27, and this would mean, that longer summarieswill be highly penalized, because it will containmore useless information according to the nuggets.After analyzing in detail all the provided nuggets,we mainly classified the possible problems into sixgroups, which are:1.
Some of the nuggets were expressed differentlyfrom how they appeared in the original blogs.Since most of the summarization systems are ex-tractive, this fact forced that humans had to evaluatethe summaries, otherwise it would be very difficultto account for the presence of such nugget in thesummary, if they are not using the same vocabularyas the original blogs.2.
Some nuggets for the same topic express the171same idea, despite not being identical.
In thesecases, we are counting a single piece of informa-tion in the summary twice, if the idea that nuggetsexpressed is included.3.
Moreover, the meaning of one nugget can be de-duced from another?s, which is also related to theproblem stated before.4.
Some of the nuggets are not very clear in mean-ing (e.g.
?hot?, ?fun?).
This would mean that asummary might include such terms in a differentcontext, thus, obtaining incorrectly that it is reve-lant when might be out of context.5.
A sentence in the original blog can be covered byseveral nuggets.
For instance, both nuggets ?it isan honest book?
and ?it is a great book?
correspondto the same sentence ?It was such a great book-honest and hard to read (content not language dif-ficulty)?.
In this case, it is not clear how to proceedwith the evaluation; whether to count both nuggetsor just one of them.6.
Some information which is also relevant for thetopic is not present in any nugget.
For instance:?I go to Starbucks because they generally provideme better service?.
Although it is relevant with re-spect to the topic and it appears in a number of sum-maries, it would be not counted because it has notbeen chosen as a nugget.4.3.3 Extended Nugget-based EvaluationSince we are interested in testing a wide range ofapproaches involving IR, OM and TS, sticking to therules to the original TAC evaluation would mean thata lot of time as well as human effort will be required,as well as not accounting for important informationthat summaries may contain in addition to the oneexpressed by the nuggets.
Therefore, taking as a ba-sis the nuggets provided at TAC, we set out a modi-fied version of them.The underlying idea behind this is to create an ex-tended set of nuggets that serve as a reference forassessing the content of the summaries.
In this man-ner, we will map each original nugget with the set ofsentences in the original blogs that are most similarto it, thus generating a gold-standard summary foreach topic.
For creating this extended gold-standardnuggets we compute the cosine similarity9 between9The cosine similarity was computed using Pedersen?severy nugget and all the sentences in the blog relatedto the same topic.
We empirically established a sim-ilarity threshold of 0.5, meaning that if a sentencewas equal or above such similarity value, it will beconsidered also relevant.
One main disadvantage ofsuch a lower threshold value is that we can considerrelevant sentences that share the same vocabularybut in fact they are not relevant to the summary.
Inorder to avoid this, once we had identified all themost similar sentences to each nugget, we carriedout a manual analysis to discard cases like this.
Hav-ing created the extended set of nuggets, we groupedall of them pertaining to the same topic, and consid-ered it a gold-standard summary.
Now, the averagenumber of nuggets per topic is 53, which we haveincreased by twice the number of original nuggetsprovided at TAC.Further on, our summaries are compared againstthis new gold-standard using ROUGE (Lin, 2004).This tool computes the number of different kindsof overlap n-grams between an automatic summaryand a human-made summary.
For our evaluation,we compute ROUGE-1 (unigrams), ROUGE-2 (bi-grams), ROUGE-SU4 (it measures the overlap ofskip-bigrams between a candidate summary and aset of reference summaries with a maximum skipdistance of 4), and ROUGE-L (Longest CommonSubsequence between two texts).
The results anddiscussion are next provided.4.4 Results and DiscussionThis section contains the results obtained for ourOQA&S approach and all the sub-approaches tested.IRpN refers to the length of the passage employedin the information retrieval approach, whereasOMaproxN indicates the approach used for the opin-ion mining component.
Firstly, we show and ana-lyze the results of our different approaches, and thenwe compared the best performing one with the base-lines and the average Opinion Summarization Pilottask participants results in TAC.Table 3 shows the precision (Pre), recall (Rec) andF-measure results of ROUGE-1 (R-1) for all the ap-proaches we experimented with.Generally speaking, the results obtained showbetter figures for precision than for recall, and there-Text Similarity Package: http://www.d.umn.edu/ tpederse/text-similarity.html172Approach Summary lengthName R-1 10% 20% 30% 40% 50%Pre 24.29 26.17 29.73 30.82 32.54IRp1 Rec 14.45 18.58 22.32 23.63 26.32-OMaprox1-TS F?=1 16.53 20.65 24.58 25.75 28.12Pre 24.29 26.17 29.73 30.82 32.54IRp1 Rec 16.90 20.02 23.36 24.15 26.77-OMaprox2-TS F?=1 19.45 22.13 25.36 25.94 28.40Pre 27.27 30.18 30.91 30.05 30.19IRp3 Rec 20.56 24.76 28.25 31.67 34.47-OMaprox1-TS F?=1 22.65 26.23 27.98 29.18 29.74Pre 30.16 32.11 32.35 32.41 32.11IRp3 Rec 20.64 24.03 27.25 29.78 32.68-OMaprox2-TS F?=1 23.28 25.64 27.42 28.44 29.21Table 3: Results of our OQA&S approachesApproach Performance (ROUGE)Name % R-1 R-2 R-L R-SU4Pre 32.11 7.34 29.00 11.37IRp3-OMaprox2 Rec 32.68 8.31 33.24 12.76-TS (50%) F?=1 29.21 7.22 28.60 11.13Pre 17.97 8.76 17.65 9.98QA-snippets Rec 71.24 31.30 70.10 37.44F?=1 24.73 11.58 24.29 13.45Pre 20.54 7.00 19.46 9.29DLSIUAES Rec 57.66 18.98 54.61 25.77F?=1 27.04 9.10 25.59 12.22Pre 23.74 8.35 22.72 10.81Average TAC Rec 56.65 19.37 54.56 25.40participants F?=1 27.45 9.64 26.33 12.46Pre 20.42 6.06 19.55 8.62Average TAC Rec 56.45 17.3 54.40 24.11participants?
F?=1 24.31 7.25 23.31 10.29Table 4: Comparison with other systemsfore the F-measure value, which combines both val-ues, will be affected.
Good precision values meansthat the information our approaches select is the cor-rect one, despite not including all the relevant infor-mation.Our best performing approach in general is theone which uses a length passage of 3 and, as faras OM is concerned, when topic-sentiment analy-sis is carried out (IRp3-OMaprox2-TS).
This showsthat the approach dealing with topic-sentiment anal-ysis in opinion mining is more suitable than the onewhich does not consider topic relevance.
Taking alook at some individual results, we next try to eluci-date the reasons why our approach performs betterat some approaches and not so good at others.
Con-cerning the IR module, it is important to mentionthat a passage length of 1 always obtains poorer re-sults that when it is increased to 3, meaning that thelonger the passage, the better.Regarding the best summary length, we observedthat in general terms, the more content we allowfor the summary, the better.
In other words, com-pression rates of 50% get higher results than 20%or 10%.
However, there are cases in which shortersummaries (10% and 20%) obtains better resultsthan longer ones (e.g.
IRp3-OMaprox2-TS vs. IRp3-OMaprox1-TS).Although the results theirselves are not very high(around 30%), they are in line with the state-of-the-art, as can be seen in Table 4, where our best per-forming approach is compared with respect to otherapproaches.Although the compression rate which obtains bestresults is not very high (50%), indeed the final sum-maries have an average length of 2,333 non-whitespace characters.
This is really low compared to thelength that TAC organization allowed for the Opin-ion Summarization Pilot task, which was 7,000 non-white space characters per question, and most ofthe times there were two questions for each topic.Whereas the results of TAC participants are muchbetter for the recall value than ours, if we take a lookat the precision, our approach outperforms them ac-cording to this value in all of the cases.
The longera summary is, the more chances it has to contain in-formation related to the topic.
However, not all thisinformation may be relevant, as it is shown in theresults for the precision values, which decrease con-siderably compared to the recall ones.
In contrast,due to the fact that our approach is missing somerelevant information because we use a rather shortpassage length (3 sentences), we do not obtain suchhigh values for the recall, but we obtain good preci-sion results, which indicate that the information thatwe keep is important.Moreover, comparing those results with the onesobtained by our approach, it is worth mentioningthat IRp3-OMaprox2-TS outperforms the F-measurevalue for all the ROUGE metrics with respect to Av-erage TAC participants?.
More in detail, when theROUGE scores are averaged, IRp3-OMaprox2-TSimproves by 12.50% the Average TAC participants?for the F-measure value.1735 Conclusion and Future WorkIn this paper, we tackled the process of OQA&S.In particular, we analyzed specific methods withineach component of this process, i.e., informationretrieval, opinion mining and text summarization.These components are crucial in this task, since ourfinal goal was to provide users with the correct infor-mation containing the answer of a question.
How-ever, contrary to most research work in question an-swering, we focus on opinionated questions ratherthan factual, increasing the difficulty of the task.Our analysis comprises different configurationsand approaches: i) varying the length for retrievingthe passages of the documents in the retrieval infor-mation stage; ii) studying a method that take intoconsideration topic-sentiment analysis for detectingand classifying opinions in the retrieved passagesand comparing it to another that does not; and iii)generating summaries of different compression rates(10% to 50%).
The results obtained showed thatthe proposed methods are appropriate to tackle theOQA&S task, improving state of the art approachesby 12.50% approximately.In the future, we plan to continue investigatingsuitable approaches for each of the proposed com-ponents.
Our final goal is to build an integrated andcomplete approach.AcknowledgmentsThis research work has been funded by the Spanish Gov-ernment through the research program FPI (BES-2007-16268) associated to the project TEXT-MESS (TIN2006-1526-C06-01).
Moreover, it has been also partiallyfunded by projects TEXT-MESS 2.0 (TIN2009-13391-C04), and PROMETEO (PROMETEO/2009/199) fromthe Spanish and the Valencian Government, respectively.ReferencesA.
Balahur, E. Lloret, O. Ferra?ndez, A. Montoyo,M.
Palomar, and R. Mun?oz.
2008.
The DLSIUAESteam?s participation in the tac 2008 tracks.
In Pro-ceedings of the Text Analysis Conference.Alexandra Balahur, Ralf Steinberger, Erik van der Goot,Bruno Pouliquen, and Mijai Kabadjov.
2009.
Opinionmining from newspaper quotations.
In Proceedings ofthe Workshop on Intelligent Analysis and Processingof Web News Content.A.
Balahur, M. Kabadjov, and J. Steinberger.
2010.Exploiting higher-level semantic information for theopinion-oriented summarization of blogs.
In Proceed-ings of CICLing?2010.S.
Cerini, V. Compagnoni, A. Demontis, M. Formentelli,and G. Gandini.
2007.
Micro-WNOp: A gold stan-dard for the evaluation of automatically compiled lex-ical resources for opinion mining.
In Language re-sources and linguistic theory: Typology, second lan-guage acquisition, English linguistics.A.
Esuli and F. Sebastiani.
2006.
SentiWordNet: A pub-licly available resource for opinion mining.
In Pro-ceedings of LREC.Talmy Givo?n, 1990.
Syntax: A functional-typological in-troduction, II.
John Benjamins.Jose?
M. Go?mez.
2007.
Recuperacio?n de Pasajes Multil-ingu?e para la Bu?squeda de Respuestas.
Ph.D. thesis.Chin-Yew Lin.
2004.
ROUGE: a Package for AutomaticEvaluation of Summaries.
In Proceedings of ACL TextSummarization Workshop, pages 74?81.Elena Lloret and Manuel Palomar.
2009.
A gradual com-bination of features for building automatic summarisa-tion systems.
In Proceedings of TSD, pages 16?23.Ani Nenkova, Rebecca Passonneau, and Kathleen McK-eown.
2007.
The pyramid method: Incorporating hu-man content selection variation in summarization eval-uation.
ACM Transactions on Speech and LanguageProcessing, 4(2):4.Lizhen Qu, Cigdem Toprak, Niklas jakob, and irynaGurevych.
2008.
Sentence level subjectivity and sen-timent analysis experiments in ntcir-7 moat challenge.In Proceedings of NTCIR-7 Workshop meeting.Karen Spa?rck Jones.
2007.
Automatic summarising: TheState of the Art.
Information Processing & Manage-ment, 43(6):1449?1481.V.
Stoyanov, C. Cardie, D. Litman, and J. Wiebe.
2004.Evaluating an opinion annotation scheme using a newmulti-perspective question and answer corpus.
InAAAI Spring Symposium on Exploring Attitude and Af-fect in Text: Theories and Applications.C.
Strapparava and A. Valitutti.
2004.
WordNet-Affect:an affective extension of wordnet.
In Proceedingsof the 4th International Conference on Language Re-sources and Evaluation, pages 1083?1086.John Carroll Taras Zabibalov.
2008.
Almost-unsupervised cross-language opinion analysis at ntcis-7.
In Proceedings of NTCIR-7 Workshop meeting.J.
Wiebe, T. Wilson, and C. Cardie.
2005.
Annotatingexpressions of opinions and emotions in language.
InLanguage Resources and Evaluation, volume 39.D.
Yu and V. Hatzivassiloglou.
2003.
Towards answer-ing opinion questions: Separating facts from opinionsand identifying the polarity of opinion sentences.
InProceedings of EMNLP.174
