Proceedings of NAACL-HLT 2013, pages 52?62,Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational LinguisticsNamed Entity Recognition with Bilingual ConstraintsWanxiang Che?
Mengqiu Wang?
Christopher D. Manning?
Ting Liu??
{car, tliu}@ir.hit.edu.cnSchool of Computer Science and TechnologyHarbin Institute of TechnologyHarbin, China, 150001?
{mengqiu, manning}@stanford.eduComputer Science DepartmentStanford UniversityStanford, CA, 94305AbstractDifferent languages contain complementarycues about entities, which can be used to im-prove Named Entity Recognition (NER) sys-tems.
We propose a method that formu-lates the problem of exploring such signals onunannotated bilingual text as a simple Inte-ger Linear Program, which encourages entitytags to agree via bilingual constraints.
Bilin-gual NER experiments on the large OntoNotes4.0 Chinese-English corpus show that the pro-posed method can improve strong baselinesfor both Chinese and English.
In particular,Chinese performance improves by over 5%absolute F1 score.
We can then annotate alarge amount of bilingual text (80k sentencepairs) using our method, and add it as up-training data to the original monolingual NERtraining corpus.
The Chinese model retrainedon this new combined dataset outperforms thestrong baseline by over 3% F1 score.1 IntroductionNamed Entity Recognition (NER) is an importanttask for many applications, such as information ex-traction and machine translation.
State-of-the-art su-pervised NER methods require large amounts of an-notated data, which are difficult and expensive toproduce manually, especially for resource-poor lan-guages.A promising approach for improving NER per-formance without annotating more data is to exploitunannotated bilingual text (bitext), which are rela-tively easy to obtain for many language pairs, bor-rowing from the resources made available by statis-tical machine translation research.1 Different lan-guages contain complementary cues about entities.For example, in Figure 1, the word ??
(Ben)?
iscommon in Chinese but rarely appears as a trans-lated foreign name.
However, its aligned word onthe English side (?Ben?)
provides a strong clue thatthis is a person name.
Judicious use of this type ofbilingual cues can help to recognize errors a mono-lingual tagger would make, allowing us to producemore accurately tagged bitext.
Each side of thetagged bitext can then be used to expand the orig-inal monolingual training dataset, which may leadto higher accuracy in the monolingual taggers.Previous work such as Li et al(2012) and Kimet al(2012) demonstrated that bilingual corpus an-notated with NER labels can be used to improvemonolingual tagger performance.
But a major draw-back of their approaches are the need for manualannotation efforts to create such corpora.
To avoidthis requirement, Burkett et al(2010) suggested a?multi-view?
learning scheme based on re-ranking.Noisy output of a ?strong?
tagger is used as trainingdata to learn parameters of a log-linear re-rankingmodel with additional bilingual features, simulatedby a ?weak?
tagger.
The learned parameters are thenreused with the ?strong?
tagger to re-rank its ownoutputs for unseen inputs.
Designing good ?weak?taggers so that they complement the ?view?
of bilin-gual features in the log-linear re-ranker is crucial tothe success of this algorithm.
Unfortunately there isno principled way of designing such ?weak?
taggers.In this paper, we would like to explore a conceptu-ally much simpler idea that can also take advantage1opus.lingfil.uu.se52TheO chairmanO ofO theB?ORG FederalI?ORG ReserveI?ORG isO BenB?PER BernankeI?PER??
?B?ORG ?
?O ?O ?B?PER ??
?I?PERFigure 1: Example of NER labels between two word-aligned bilingual parallel sentences.of the large amount of unannotated bitext, withoutcomplicated machinery.
More specifically, we in-troduce a joint inference method that formulates thebilingual NER tagging problem as an Integer LinearProgram (ILP) and solves it during decoding.
Wepropose a set of intuitive and effective bilingual con-straints that encourage NER results to agree acrossthe two languages.Experimental results on the OntoNotes 4.0 namedentity annotated Chinese-English parallel corpusshow that the proposed method can improve thestrong Chinese NER baseline by over 5% F1 scoreand also give small improvements over the Englishbaseline.
Moreover, by adding the automaticallytagged data to the original NER training corpusand retraining the monolingual model using an up-training regimen (Petrov et al 2010), we can im-prove the monolingual Chinese NER performanceby over 3% F1 score.2 Constraint-based Monolingual NERNER is a sequence labeling task where we assigna named entity tag to each word in an input sen-tence.
One commonly used tagging scheme is theBIO scheme.
The tag B-X (Begin) represents thefirst word of a named entity of type X, for example,PER (Person) or LOC (Location).
The tag I-X (In-side) indicates that a word is part of an entity but notfirst word.
The tag O (Outside) is used for all non-entity words.2 See Figure 1 for an example taggedsentence.Conditional Random Fields (CRF) (Lafferty et al2001) is a state-of-the-art sequence labeling modelwidely used in NER.
A first-order linear-chain CRF2While the performance of NER is measured at the entitylevel (not the tag level).defines the following conditional probability:PCRF (y|x) =1Z(x)?iMi(yi, yi?1|x) (1)where x and y are the input and output sequences,respectively, Z(x) is the partition function, and Miis the clique potential for edge clique i. Decodingin CRF involves finding the most likely output se-quence that maximizes this objective, and is com-monly done by the Viterbi algorithm.Roth and Yih (2005) proposed an ILP inferencealgorithm, which can capture more task-specific andglobal constraints than the vanilla Viterbi algorithm.Our work is inspired by Roth and Yih (2005).
Butinstead of directly solving the shortest-path problemin the ILP formulation, we re-define the conditionalprobability as:PMAR(y|x) =?iP (yi|x) (2)where P (yi|x) is the marginal probability given byan underlying CRF model computed using forward-backward inference.
Since the early HMM litera-ture, it has been well known that using the marginaldistributions at each position works well, as opposedto Viterbi MAP sequence labeling (Me?rialdo, 1994).Our experimental results also supports this claim, aswe will show in Section 6.
Our objective is to findan optimal NER tag sequence:y?
= argmaxyPMAR(y|x)= argmaxy?ilogP (yi|x) (3)Then an ILP can be used to solve the inferenceproblem as classification problem with constraints.53The objective function is:max|x|?i=1?y?Yzyi logPyi (4)where Y is the set of all possible named entity tags.P yi = P (yi = y|x) is the CRF marginal probabil-ity that the ith word is tagged with y, and zyi is anindicator that equals 1 iff the ith word is tagged y;otherwise, zyi is 0.If no constraints are identified, then Eq.
(4)achieves maximum when all zyi are assigned to 1,which violates the condition that each word shouldonly be assigned a single entity tag.
We can expressthis with constraints:?i :?y?Yzyi = 1 (5)After adding the constraints, the probability of thesequence is maximized when each word is assignedthe tag with highest probability.
However, some in-valid results may still exist.
For example a tag Omay be wrongly followed by a tag I-X, although anamed entity cannot start with I-X.
Therefore, wecan add the following constraints:?i,?X : zB-Xi?1 + zI-Xi?1 ?
zI-Xi ?
0 (6)which specifies that when the ith word is tagged withI-X (zI-Xi = 1), then the previous word can only betagged with B-X or I-X (zB-Xi?1 + zI-Xi?1 ?
1).3 NER with Bilingual ConstraintsThis section demonstrates how to jointly performNER for two languages with bilingual constraints.We assume sentences have been aligned into pairs,and the word alignment between each pair of sen-tences is also given.3.1 Hard Bilingual ConstraintsWe first introduce the simplest hard constraints, i.e.,each word alignment pair should have the samenamed entity tag.
For example, in Figure 1, theChinese word ?????
was aligned with the En-glish words ?the?, ?Federal?
and ?Reserve?.
There-fore, they have the same named entity tags ORG.33The prefix B- and I- are ignored.Similarly, ???
and ?Ben?
as well as ?????
and?Bernanke?
were all tagged with the tag PER.The objective function for bilingual NER can beexpressed as follows:max|xc|?i=1?y?Yzyi logPyi +|xe|?j=1?y?Yzyj logPyj (7)where P yi and Pyj are the probabilities of the ith Chi-nese word and jth English word to be tagged with y,respectively.
xc and xe are respectively the Chineseand English sentences.Similar to monolingual constrained NER (Sec-tion 2), monolingual constraints are added for eachlanguage as shown in Eqs.
(8) and (9):?i :?y?Yzyi = 1;?j :?y?Yzyj = 1 (8)?i,?X : zB-Xi + zI-Xi ?
zB-Xi+1 ?
0 (9)?j,?X : zB-Xj + zI-Xj ?
zB-Xj+1 ?
0Bilingual constraints are added in Eq.
(10):?
(i, j) ?
A,?X : zB-Xi + zI-Xi = zB-Xj + zI-Xj (10)where A = {(i, j)} is the word alignment pair set,i.e., the ith Chinese word and the jth English wordwere aligned together.
Chinese word i is tagged witha named entity type X (zB-Xi + zI-Xi = 1), iff Englishword j is tagged with X (zB-Xj +zI-Xj = 1).
Therefore,these hard bilingual constraints guarantee that whentwo words are aligned, they are tagged with the samenamed entity tag.However, in practice, aligned word pairs do notalways have the same tag because of the differencein annotation standards across different languages.For example, in Figure 2(a), the Chinese word ?????
is a location.
However, it is aligned to the words,?development?
and ?zone?, which are not named en-tities in English.
Word alignment error is another se-rious problem that can cause violation of hard con-straints.
In Figure 2(b), the English word ?Agency?is wrongly aligned with the Chinese word ??
(re-port)?.
Thus, these two words cannot be assignedwith the same tag.To address these two problems, we present a prob-abilistic model for bilingual NER which can lead to54ThisO developmentO zoneO isO locatedO inO .
.
.?
?O ??
?B?LOC ?O ?O .
.
.
(a) Inconsistent named entity standardsXinhuaB?ORG NewsI?ORG AgencyI?ORG FebruaryO 16thO??
?B?ORG ?
?B?LOC ??
?O ?O(b) Word alignment errorFigure 2: Errors of hard bilingual constraints method.an optimization problem with two soft bilingual con-straints:1) allow word-aligned pairs to have differentnamed entity tags; 2) consider word alignment prob-abilities to reduce the influence of wrong word align-ments.3.2 Soft Constraints with Tag UncertaintyThe new probabilistic model for bilingual NER is:P (yc,ye|xc,xe, A) =P (yc,ye,xc,xe, A)P (xc,xe, A)= P (yc,xc,xe, A)P (xc,xe, A)?
P (ye,xc,xe, A)P (xc,xe, A)?
P (yc,ye,xc,xe, A)P (xc,xe, A)P (yc,xc,xe, A)P (ye,xc,xe, A)(11)?
P (yc|xc)P (ye|xe)P (yc,ye|A)P (yc|A)P (ye|A)(12)where yc and ye respectively denotes Chinese andEnglish named entity output sequences.
A is the setof word alignment pairs.If we assume that named entity tag assignments inChinese is only dependent on the observed Chinesesentence, then we can drop the A and xe term in thefirst factor of Eq.
(11), and arrive at the first factor ofEq.
(12); similarly we can use the same assumptionto derive the second factor in Eq.
(12) for English;alternatively, if we assume the named entity tag as-signments are only dependent on the cross-lingualword associations via word alignment, then we candrop xc and xe terms in the third factor of Eq.
(11)and arrive at the third factor of Eq.
(12).
These fac-tors represent the two major sources of informationin the model: monolingual surface observation, andcross-lingual word associations.The first two factors of Eq.
(12) can be furtherdecomposed into the product of probabilities of allwords in each language sentence like Eq.
(2).Assuming that the tags are independent betweendifferent word alignment pairs, then the last factorof Eq.
(12) can be decomposed into:P (yc,ye|A)P (yc|A)P (ye|A)=?a?AP (ycayea)P (yca)P (yea)=?a?A?ycyea (13)where yca and yea respectively denotes Chinese andEnglish named entity tags in a word alignment paira.
?ycye = P (ycye)P (yc)P (ye) is the pointwise mutual infor-mation (PMI) score between a Chinese named en-tity tag yc and an English named entity tag ye.
Ifyc = ye, then the score will be high; otherwise thescore will be low.
A number of methods for calculat-ing the scores are provided at the end of this section.We use ILP to maximize Eq.
(12).
The new ob-jective function is expressed as follow:max|xc|?i=1?y?Yzyi logPyi +|xe|?j=1?y?Yzyj logPyj+?a?A?yc?Y?ye?Yzycyea log ?ycyea (14)where zycyea is an indicator that equals 1 iff the Chi-nese and English named entity tags are yc and yerespectively, given a word alignment pair a; other-wise, zycyea is 0.Monolingual constraints such as Eqs.
(8) and (9)need to be added.
In addition, one and only one pos-sible named entity tag pair exists for a word align-ment pair.
This condition can be expressed as thefollowing constraints:?a ?
A :?yc?Y?ye?Yzycyea = 1 (15)When the tag pair of a word alignment pair is de-termined, the corresponding monolingual named en-55tity tags can also be identified.
This rule can be ex-pressed by the following constraints:?a = (i, j) ?
A : zycyea ?
zyci , zycyea ?
zyej (16)Thus, if zycyea = 1, then zyci and zyej must be bothequal to 1.
Here, the ith Chinese word and the jthEnglish word are aligned together.In contrast to hard bilingual constraints, inconsis-tent named entity tags for an aligned word pair areallowed in soft bilingual constraints, but are givenlower ?ycye scores.To calculate the ?ycye score, an annotated bilin-gual NER corpus is consulted.
We count from allword alignment pairs the number of times yc and yeoccur together (C(ycye)) and separately (C(yc) andC(ye)).
Afterwards, ?ycye is calculated with maxi-mum likelihood estimation as follows:?ycye = P (ycye)P (yc)P (ye)= N ?
C(ycye)C(yc)C(ye)(17)where N is the total number of word alignmentpairs.However, in this paper, we assume that no namedentity annotated bilingual corpus is available.
Thus,the above method is only used as Oracle.
A real-istic method for calculating the ?ycye score requiresthe use of two initial monolingual NER models, suchas baseline CRF, to predict named entity tags foreach language on an unannotated bitext.
We countfrom this automatically tagged corpus the statisticsmentioned above.
This method is henceforth re-ferred to as Auto.A simpler approach is to manually set the valueof ?ycye : if yc = ye then we assign a larger valueto ?ycye ; else we assign an ad-hoc smaller value.
Infact, if we set ?ycye = 1 iff yc = ye; otherwise,?ycye = 0, then the soft constraints backs off to hardconstraints.
We refer to this set of soft constraints asSoft-tag.3.3 Constraints with Alignment UncertaintySo far, we assumed that a word alignment set A isknown.
In practice, only the word alignment proba-bility Pa for each word pair a is provided.
We canset a threshold ?
for Pa to tune the set A: a ?
Aiff Pa ?
?.
This condition can be regarded as akind of hard word alignment.
However, the follow-ing problem exists: the smaller the ?, the noisier theword alignments are; the larger the ?, the more pos-sible word alignments are lost.
To ameliorate thisproblem, we introduce another set of soft bilingualconstraints.We can re-express Eq.
(13) as follows:?a?A?ycyea =?a?A(?ycyea )Ia (18)where A is the set of all word pairs between twolanguages.
Ia = 1 iff Pa ?
?
; otherwise, Ia = 0.We can then replace the hard indicator Ia withthe word alignment probability Pa, Eq.
(14) is thentransformed into the following equation:max|Wc|?i?y?Yzyi logPyi +|We|?j?y?Yzyj logPyj+?a?A?yc?Y?ye?Yzycyea Pa log ?ycyea (19)We name the set of constraints aboveSoft-align, which has the same constraintsas Soft-tag, i.e., Eqs.
(8), (9), (15) and (16).4 Experimental SetupWe conduct experiments on the latest OntoNotes4.0 corpus (LDC2011T03).
OntoNotes is a large,manually annotated corpus that contains various textgenres and annotations, such as part-of-speech tags,named entity labels, syntactic parse trees, predicate-argument structures and co-references (Hovy et al2006).
Aside from English, this corpus also con-tains several Chinese and Arabic corpora.
Some ofthese corpora contain bilingual parallel documents.We used the Chinese-English parallel corpus withnamed entity labels as our development and testdata.
This corpus includes about 400 document pairs(chtb 0001-0325, ectb 1001-1078).
We used odd-numbered documents as development data and even-numbered documents as test data.
We used all otherportions of the named entity annotated corpus astraining data for the monolingual systems.
Therewere a total of?660 Chinese documents (?16k sen-tences) and ?1,400 English documents (?39k sen-tences).
OntoNotes annotates 18 named entity types,such as person, location, date and money.
In thispaper, we selected the four most common namedentity types, i.e., PER (Person), LOC (Location),56Chinese NER Templates00: 1 (class bias param)01: wi+k,?1 ?
k ?
102: wi+k?1 ?
wi+k, 0 ?
k ?
103: shape(wi+k),?4 ?
k ?
404: prefix(wi, k), 1 ?
k ?
405: prefix(wi?1, k), 1 ?
k ?
406: suffix(wi, k), 1 ?
k ?
407: suffix(wi?1, k), 1 ?
k ?
408: radical(wi, k), 1 ?
k ?
len(wi)Unigram Featuresyi?
00 ?
08Bigram Featuresyi?1 ?
yi?
00 ?
08Table 1: Basic features of Chinese NER.ORG (Organization) and GPE (Geo-Political Enti-ties), and discarded the others.Since the bilingual corpus is only aligned at thedocument level, we performed sentence alignmentusing the Champollion Tool Kit (CTK).4 After re-moving sentences with no aligned sentence, a totalof 8,249 sentence pairs were retained.We used the BerkeleyAligner,5 to produceword alignments over the sentence-aligned datasets.BerkeleyAligner also gives posterior probabilitiesPa for each aligned word pair.We used the CRF-based Stanford NER tagger (us-ing Viterbi decoding) as our baseline monolingualNER tool.6 English features were taken from Finkelet al(2005).
Table 1 lists the basic features ofChinese NER, where ?
means string concatenationand yi is the named entity tag of the ith word wi.Moreover, shape(wi) is the shape of wi, such asdate and number.
prefix/suffix(wi, k) denotes thek-characters prefix/suffix of wi.
radical(wi, k) de-notes the radical of the kth Chinese character of wi.7len(wi) is the number of Chinese characters in wi.To make the baseline CRF taggers stronger, weadded word clustering features to improve gener-alization over unseen data for both Chinese andEnglish.
Word clustering features have been suc-cessfully used in several English tasks, including4champollion.sourceforge.net5code.google.com/p/berkeleyaligner6nlp.stanford.edu/software/CRF-NER.shtml,which has included our English and Chinese NER implementations.7The radical of a Chinese character can be found at: www.unicode.org/charts/unihan.htmlNER (Miller et al 2004) and dependency pars-ing (Koo et al 2008).
To our knowledge, this workis the first use of word clustering features for Chi-nese NER.
A C++ implementation of the Brownword clustering algorithms (Brown et al 1992) wasused to obtain the word clusters (Liang, 2005).8Raw text was obtained from the fifth edition of Chi-nese Gigaword (LDC2011T13).
One million para-graphs from Xinhua news section were randomlyselected, and the Stanford Word Segmenter withLDC standard was applied to segment Chinese textinto words.9 About 46 million words were obtainedwhich were clustered into 1,000 word classes.5 Threshold TuningDuring development, we tuned the word alignmentprobability thresholds to find the best value.
Figure 3shows the performance curves.When the word alignment probability threshold ?is set to 0.9, the hard bilingual constraints performwell for both Chinese and English.
But as the thresh-olds value gets smaller, and more noisy word align-ments are introduced, we see the hard bilingual con-straints method starts to perform badly.In Soft-tag setting, where inconsistent tag as-signments within aligned word pairs are allowed butpenalized, different languages have different optimalthreshold values.
For example, Chinese has an opti-mal threshold of 0.7, whereas English has 0.2.
Thus,the optimal thresholds for different languages needto be selected with care when Soft-tag is appliedin practice.Soft-align eliminates the need for carefultuning of word alignment thresholds, and thereforecan be more easily used in practice.
Experimen-tal results of Soft-align confirms our hypothe-sis ?
the performance of both Chinese and EnglishNER systems improves with decreasing threshold.However, we can still improve efficiency by set-ting a low threshold to prune away very unlikelyword alignments.
We set the threshold to 0.1 forSoft-align to increase speed, and we observedvery minimal performance lost when doing so.We also found that automatically estimated bilin-gual tag PMI scores (Auto) gave comparable results8github.com/percyliang/brown-cluster9nlp.stanford.edu/software/segmenter.shtml570.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9threshold of word alignment probability5560657075performance (F1)HardSoft-label (Oracle)Soft-label (Auto)Soft-align (Oracle)Soft-align (Auto)(a) Chinese0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9threshold of word alignment probability6065707580performance (F1)HardSoft-label (Oracle)Soft-label (Auto)Soft-align (Oracle)Soft-align (Auto)(b) EnglishFigure 3: Performance curves of different bilingual constraints methods on development set.to Oracle.
Therefore this technique is effectivefor computing the PMI scores, avoiding the need ofmanually annotating named entity bilingual corpus.6 Bilingual NER ResultsThe main results on Chinese and English test setswith the optimal word alignment threshold for eachmethod are shown in Table 2.The CRF-based Chinese NER with and withoutword clustering features are compared here.
Theword clustering features significantly (p < 0.01) im-proved the performance of Chinese NER, 10 givingus a strong Chinese NER baseline.11 The effective-ness of word clustering for English NER has beenproved in previous work.The performance of ILP with only monolingualconstraints is quite comparable with the CRF re-sults, especially on English.
The greater ILP perfor-mance on English is probably due to more accuratemarginal probabilities estimated by the English CRFmodel.The ILP model with hard bilingual constraintsgives a slight performance improvement on Chi-nese, but affects performance negatively on English.Once we introduced tagging uncertainties into theSoft-tag bilingual constraints, we see a very sig-10We use paired bootstrap resampling significance test (Efronand Tibshirani, 1993).11To the best of our knowledge, there was no performancereport of state-of-the-art NER results on the latest OntoNotesdataset.nificant (p < 0.01) performance boost on Chinese.This method also improves the recall on English,with a smaller decrease in precision.
Overall, it im-proves English F1 score by about 0.4%, which is un-fortunately not statistically significant.Compared with Soft-tag, the finalSoft-align method can further improveperformance on both Chinese and English.
This islikely to be because: 1) Soft-align includesmore word alignment pairs, thereby improvingrecall; and 2) uses probabilities to cut wrongword alignments, thereby improving precision.
Inparticular, compared with the strong CRF baseline,the gain on Chinese side is almost 5.5% in absoluteF1 score.Decoding/inferenc efficiency of different methodsare shown in the last column of Table 2.12 Com-pared with Viterbi decoding in CRF, monolingualILP decoding is about 2.3 times slower.
BilingualILP decoding, with either hard or soft constraints, issignificantly slower than the monolingual methods.The reason is that the number of monolingual ILPconstraints doubles, and there are additionally manymore bilingual constraints.
The difference in speedbetween the Soft-tag and Soft-align meth-ods is attributed to the difference in number of wordalignment pairs.Since each sentence pair can be decoded indepen-12CPU: Intel Xeon E5-2660 2.20GHz.
And the speed cal-culation of ILP inference methods exclude the time needed toobtain marginal probabilities from the CRF models.58Chinese English SpeedP R F1 P R F1 #sent/sCRF (No Cluster) 74.74 56.17 64.13 ?
?
?
?CRF (Word Cluster) 76.90 63.32 69.45 82.95 76.67 79.68 317.3Monolingual ILP 76.20 63.06 69.01 82.88 76.68 79.66 138.0Hard 74.38 65.78 69.82 82.66 75.36 78.84 21.1Soft-tag (Auto) 77.37 71.14 74.13 81.36 78.74 80.03 5.9Soft-align (Auto) 77.71 72.51 75.02 81.94 78.35 80.10 1.5Table 2: Results on bilingual parallel test set.dently, parallelization the decoding process can re-sult in significant speedup.7 Semi-supervised NER ResultsThe above results show the usefulness of our methodin a bilingual setting, where we are presented withsentence aligned data, and are tagging both lan-guages at the same time.
To have a greater impacton general monolingual NER systems, we employa semi-supervised learning setting.
First, we tag alarge amount of unannotated bitext with our bilin-gual constraint-based NER tagger.
Then we mix theautomatically tagged results with the original mono-lingual Chinese training data to train a new model.Our bitext is derived from the Chinese-Englishpart of the Foreign Broadcast Information Servicecorpus (FBIS, LDC2003E14).
The best perform-ing bilingual model Soft-align with threshold?
= 0.1 was used under the same experimental set-ting as described in Section 4Method #sent P R F1CRF ?16k 76.90 63.32 69.45Semi10k 77.60 66.51 71.6220k 77.28 67.26 71.9240k 77.40 67.81 72.2980k 77.44 68.64 72.77Table 3: Semi-supervised results on Chinese test set.Table 3 shows that the performance of the semi-supervised method improves with more additionaldata.
We simply appended these data to the orig-inal training data.
We also have done the experi-ments to down weight the additional training databy duplicating the original training data.
Therewas some slight improvements, but not very signif-icant.
Finally, when we add 80k sentences, the F1score is improved by 3.32%, which is significantly(p < 0.01) better than the baseline, and most of thecontribution comes from recall improvement.Before the end of experimental section, let ussummarize the usage of different kinds of data re-sources used in our experiments, as shown in Ta-ble 4, where  and ?
denote whether the corre-sponding resources are required.
In the bilingualcase, during training, only the monolingual namedentity annotated data (NE-mono) is necessary totrain a monolingual NER tagger.
During the test,unannotated bitext (Bitext) is required by the wordaligner and our bilingual NER tagger.
Named entityannotated bitext (NE-bitext) is used to evaluate ourbilingual model.
In the semi-supervised case, be-sides the original NE-mono data, the Bitext is usedas input to our bilingual NER tagger to product ad-ditional training data.
To evaluate the final NERmodel, only NE-mono is needed.NE-mono Bitext NE-bitextBilingualtrain  ?
?test ?
 Semitrain   ?test  ?
?Table 4: Summarization of the data resource usage8 Related WorkPrevious work explored the use of bilingual corporato improve existing monolingual analyzers.
Huanget al(2009) proposed methods to improve parsingperformance using bilingual parallel corpus.
Li etal.
(2012) jointly labeled bilingual named entitieswith a cyclic CRF model, where approximate in-ference was done using loopy belief propagation.These methods require manually annotated bilingual59corpora, which are expensive to construct, and hardto obtain.
Kim et al(2012) proposed a method oflabeling bilingual corpora with named entity labelsautomatically based on Wikipedia.
However, thismethod is restricted to topics covered by Wikipedia.Similar to our work, Burkett et al(2010) also as-sumed that annotated bilingual corpora are scarce.Beyond the difference discussed in Section 1, theirre-ranking strategy may lose the correct named en-tity results if they are not included in the top-N out-puts.
Furthermore, we consider the word alignmentprobabilities in our method which can reduce the in-fluence of word alignment errors.
Finally, we testour method on a large standard publicly availablecorpus (8,249 sentences), while they used a muchsmaller (200 sentences) manually annotated bilin-gual NER corpus for results validation.In addition to bilingual corpora, bilingual dictio-naries are also useful resources.
Huang and Vo-gel (2002) and Chen et al(2010) proposed ap-proaches for extracting bilingual named entity pairsfrom unannotated bitext, in which verification isbased on bilingual named entity dictionaries.
How-ever, large-scale bilingual named entity dictionariesare difficult to obtain for most language pairs.Yarowsky and Ngai (2001) proposed a projectionmethod that transforms high-quality analysis resultsof one language, such as English, into other lan-guages on the basis of word alignment.
Das andPetrov (2011) applied the above idea to part-of-speech tagging with a more complex model.
Fu et al(2011) projected English named entities onto Chi-nese by carefully designed heuristic rules.
Althoughthis type of method does not require manually an-notated bilingual corpora or dictionaries, errors insource language results, wrong word alignments andinconsistencies between the languages limit applica-tion of this method.Constraint-based monolingual methods by usingILP have been successfully applied to many naturallanguage processing tasks, such as Semantic RoleLabeling (Punyakanok et al 2004), DependencyParsing (Martins et al 2009) and Textual Entail-ment (Berant et al 2011).
Zhuang and Zong (2010)proposed a joint inference method for bilingual se-mantic role labeling with ILP.
However, their ap-proach requires training an alignment model with amanually annotated corpus.9 ConclusionsWe proposed a novel ILP based inference algorithmwith bilingual constraints for NER.
This methodcan jointly infer bilingual named entities withoutusing any annotated bilingual corpus.
We in-vestigate various bilingual constraints: hard andsoft constraints.
Out empirical study on large-scale OntoNotes Chinese-English parallel NER datashowed that Soft-align method, which allowsinconsistent named entity tags between two alignedwords and considers word alignment probabilities,can significantly improve over the performance ofa strong Chinese NER baseline.
Our work is thefirst to evaluate performance on a large-scale stan-dard dataset.
Finally, we can also improve mono-lingual Chinese NER performance significantly, bycombining the original monolingual training datawith new data obtained from bitext tagged by ourmethod.
The final ILP-based bilingual NER tag-ger with soft constraints is publicly available at:github.com/carfly/bi_ilpFuture work could apply the bilingual constraint-based method to other tasks, such as part-of-speechtagging and relation extraction.AcknowledgmentsThe authors would like to thank Rob Voigt and thethree anonymous reviewers for their valuable com-ments and suggestions.
We gratefully acknowledgethe support of the National Natural Science Foun-dation of China (NSFC) via grant 61133012, theNational ?863?
Project via grant 2011AA01A207and 2012AA011102, the Ministry of Education Re-search of Social Sciences Youth funded projectsvia grant 12YJCZH304, the Defense Advanced Re-search Projects Agency (DARPA) Machine Read-ing Program under Air Force Research Laboratory(AFRL) prime contract no.
FA8750-09-C-0181 andthe support of the DARPA Broad Operational Lan-guage Translation (BOLT) program through IBM.Any opinions, findings, and conclusion or recom-mendations expressed in this material are those ofthe authors and do not necessarily reflect the view ofthe DARPA, AFRL, or the US government.60ReferencesJonathan Berant, Ido Dagan, and Jacob Goldberger.2011.
Global learning of typed entailment rules.
InProceedings of the 49th Annual Meeting of the Asso-ciation for Computational Linguistics: Human Lan-guage Technologies, pages 610?619, Portland, Ore-gon, USA, June.
Association for Computational Lin-guistics.Peter F. Brown, Peter V. deSouza, Robert L. Mercer, Vin-cent J. Della Pietra, and Jenifer C. Lai.
1992.
Class-based n-gram models of natural language.
Comput.Linguist., 18(4):467?479, December.David Burkett, Slav Petrov, John Blitzer, and Dan Klein.2010.
Learning better monolingual models with unan-notated bilingual text.
In Proceedings of the Four-teenth Conference on Computational Natural Lan-guage Learning, pages 46?54, Uppsala, Sweden, July.Association for Computational Linguistics.Yufeng Chen, Chengqing Zong, and Keh-Yih Su.
2010.On jointly recognizing and aligning bilingual namedentities.
In Proceedings of the 48th Annual Meet-ing of the Association for Computational Linguistics,pages 631?639, Uppsala, Sweden, July.
Associationfor Computational Linguistics.Dipanjan Das and Slav Petrov.
2011.
Unsupervisedpart-of-speech tagging with bilingual graph-based pro-jections.
In Proceedings of the 49th Annual Meet-ing of the Association for Computational Linguistics:Human Language Technologies, pages 600?609, Port-land, Oregon, USA, June.
Association for Computa-tional Linguistics.B.
Efron and R. J. Tibshirani.
1993.
An Introduction tothe Bootstrap.
Chapman & Hall, New York.Jenny Rose Finkel, Trond Grenager, and ChristopherManning.
2005.
Incorporating non-local informationinto information extraction systems by gibbs sampling.In Proceedings of the 43rd Annual Meeting of theAssociation for Computational Linguistics (ACL?05),pages 363?370, Ann Arbor, Michigan, June.
Associa-tion for Computational Linguistics.Ruiji Fu, Bing Qin, and Ting Liu.
2011.
Generatingchinese named entity data from a parallel corpus.
InProceedings of 5th International Joint Conference onNatural Language Processing, pages 264?272, ChiangMai, Thailand, November.
Asian Federation of NaturalLanguage Processing.Eduard Hovy, Mitchell Marcus, Martha Palmer, LanceRamshaw, and Ralph Weischedel.
2006.
Ontonotes:the 90% solution.
In Proceedings of the Human Lan-guage Technology Conference of the NAACL, Com-panion Volume: Short Papers, NAACL-Short ?06,pages 57?60, Stroudsburg, PA, USA.
Association forComputational Linguistics.Fei Huang and Stephan Vogel.
2002.
Improved namedentity translation and bilingual named entity extrac-tion.
In Proceedings of the 4th IEEE InternationalConference on Multimodal Interfaces, ICMI 2002,Washington, DC, USA.
IEEE Computer Society.Liang Huang, Wenbin Jiang, and Qun Liu.
2009.Bilingually-constrained (monolingual) shift-reduceparsing.
In Proceedings of the 2009 Conference onEmpirical Methods in Natural Language Processing,pages 1222?1231, Singapore, August.
Association forComputational Linguistics.Sungchul Kim, Kristina Toutanova, and Hwanjo Yu.2012.
Multilingual named entity recognition usingparallel data and metadata from wikipedia.
In Pro-ceedings of the 50th Annual Meeting of the Associa-tion for Computational Linguistics (Volume 1: LongPapers), pages 694?702, Jeju Island, Korea, July.
As-sociation for Computational Linguistics.Terry Koo, Xavier Carreras, and Michael Collins.
2008.Simple semi-supervised dependency parsing.
In Pro-ceedings of ACL-08: HLT, pages 595?603, Columbus,Ohio, June.
Association for Computational Linguis-tics.Shankar Kumar.
2005.
Minimum bayes-risk techniquesin automatic speech recognition and statistical ma-chine translation.
Ph.D. thesis, Baltimore, MD, USA.AAI3155633.John D. Lafferty, Andrew McCallum, and Fernando C. N.Pereira.
2001.
Conditional random fields: Proba-bilistic models for segmenting and labeling sequencedata.
In Proceedings of the Eighteenth InternationalConference on Machine Learning, ICML ?01, pages282?289, San Francisco, CA, USA.
Morgan Kauf-mann Publishers Inc.Qi Li, Haibo Li, Heng Ji, Wen Wang, Jing Zheng, and FeiHuang.
2012.
Joint bilingual name tagging for paral-lel corpora.
In Proceedings of the 21st ACM Inter-national Conference on Information and KnowledgeManagement (CIKM 2012), Honolulu, Hawaii, Octo-ber.Percy Liang.
2005.
Semi-supervised learning for naturallanguage.
Master?s thesis, MIT.Andre Martins, Noah Smith, and Eric Xing.
2009.
Con-cise integer linear programming formulations for de-pendency parsing.
In Proceedings of the Joint Con-ference of the 47th Annual Meeting of the ACL andthe 4th International Joint Conference on Natural Lan-guage Processing of the AFNLP, pages 342?350, Sun-tec, Singapore, August.
Association for ComputationalLinguistics.Bernard Me?rialdo.
1994.
Tagging english text with aprobabilistic model.
Comput.
Linguist., 20(2):155?171.61Scott Miller, Jethran Guinness, and Alex Zamanian.2004.
Name tagging with word clusters and dis-criminative training.
In Daniel Marcu Susan Dumaisand Salim Roukos, editors, HLT-NAACL 2004: MainProceedings, pages 337?342, Boston, Massachusetts,USA, May 2 - May 7.
Association for ComputationalLinguistics.Slav Petrov, Pi-Chuan Chang, Michael Ringgaard, andHiyan Alshawi.
2010.
Uptraining for accurate deter-ministic question parsing.
In Proceedings of the 2010Conference on Empirical Methods in Natural Lan-guage Processing, pages 705?713, Cambridge, MA,October.
Association for Computational Linguistics.Vasin Punyakanok, Dan Roth, Wen-tau Yih, and Dav Zi-mak.
2004.
Semantic role labeling via integer lin-ear programming inference.
In Proceedings of Coling2004, pages 1346?1352, Geneva, Switzerland, Aug23?Aug 27.
COLING.Dan Roth and Wen-tau Yih.
2005.
Integer linear pro-gramming inference for conditional random fields.
InProceedings of the 22nd international conference onMachine learning, ICML ?05, pages 736?743, NewYork, NY, USA.
ACM.David Yarowsky and Grace Ngai.
2001.
Inducing mul-tilingual POS taggers and NP bracketers via robustprojection across aligned corpora.
In Proceedings ofthe second meeting of the North American Chapter ofthe Association for Computational Linguistics on Lan-guage technologies, NAACL ?01, pages 1?8, Strouds-burg, PA, USA.
Association for Computational Lin-guistics.Tao Zhuang and Chengqing Zong.
2010.
Joint inferencefor bilingual semantic role labeling.
In Proceedings ofthe 2010 Conference on Empirical Methods in Natu-ral Language Processing, pages 304?314, Cambridge,MA, October.
Association for Computational Linguis-tics.62
