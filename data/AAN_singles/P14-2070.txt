Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 427?433,Baltimore, Maryland, USA, June 23-25 2014.c?2014 Association for Computational LinguisticsDepecheMood:a Lexicon for Emotion Analysis from Crowd-Annotated NewsJacopo StaianoUniversity of TrentoTrento - Italystaiano@disi.unitn.itMarco GueriniTrento RISETrento - Italymarco.guerini@trentorise.euAbstractWhile many lexica annotated with wordspolarity are available for sentiment anal-ysis, very few tackle the harder task ofemotion analysis and are usually quitelimited in coverage.
In this paper, wepresent a novel approach for extracting?
in a totally automated way ?
a high-coverage and high-precision lexicon ofroughly 37 thousand terms annotated withemotion scores, called DepecheMood.Our approach exploits in an original way?crowd-sourced?
affective annotation im-plicitly provided by readers of news ar-ticles from rappler.com.
By provid-ing new state-of-the-art performances inunsupervised settings for regression andclassification tasks, even using a na?
?ve ap-proach, our experiments show the benefi-cial impact of harvesting social media datafor affective lexicon building.1 IntroductionSentiment analysis has proved useful in several ap-plication scenarios, for instance in buzz monitor-ing ?
the marketing technique for keeping trackof consumer responses to services and products ?where identifying positive and negative customerexperiences helps to assess product and service de-mand, tackle crisis management, etc.On the other hand, the use of finer-grained mod-els, accounting for the role of individual emotions,is still in its infancy.
The simple division in ?pos-itive?
vs. ?negative?
comments may not suffice, asin these examples: ?I?m so miserable, I droppedmy IPhone in the water and now it?s not workinganymore?
(SADNESS) vs. ?I am very upset, my newIPhone keeps not working!?
(ANGER).
While bothtexts express a negative sentiment, the latter, con-nected to anger, is more relevant for buzz monitor-ing.
Thus, emotion analysis represents a naturalevolution of sentiment analysis.Many approaches to sentiment analysis makeuse of lexical resources ?
i.e.
lists of positive andnegative words ?
often deployed as baselines or asfeatures for other methods, usually machine learn-ing based (Liu and Zhang, 2012).
In these lexica,words are associated with their prior polarity, i.e.whether such word out of context evokes some-thing positive or something negative.
For exam-ple, wonderful has a positive connotation ?
priorpolarity ?
while horrible has a negative one.The quest for a high precision and high cov-erage lexicon, where words are associated witheither sentiment or emotion scores, has severalreasons.
First, it is fundamental for tasks suchas affective modification of existing texts, wherewords?
polarity together with their score are nec-essary for creating multiple graded variations ofthe original text (Inkpen et al, 2006; Guerini etal., 2008; Whitehead and Cavedon, 2010).Second, considering word order makes a differ-ence in sentiment analysis.
This calls for a role ofcompositionality, where the score of a sentence iscomputed by composing the scores of the wordsup in the syntactic tree.
Works worth mention-ing in this connection are: Socher et al (2013),which uses recursive neural networks to learncompositional rules for sentiment analysis, and(Neviarouskaya et al, 2009; Neviarouskaya et al,2011) which exploit hand-coded rules to composethe emotions expressed by words in a sentence.
Inthis respect, compositional approaches represent anew promising trend, since all other approaches,either using semantic similarity or Bag-of-Words(BOW) based machine-learning, cannot handle,for example, cases of texts with same wordingbut different words order: ?The dangerous killerescaped one month ago, but recently he was ar-rested?
(RELIEF, HAPPYNESS) vs. ?The danger-ous killer was arrested one month ago, but re-427cently he escaped?
(FEAR).
The work in (Wangand Manning, 2012) partially accounts for thisproblem and argues that using word bigram fea-tures allows improving over BOW based meth-ods, where words are taken as features in isola-tion.
This way it is possible to capture simplecompositional phenomena like polarity reversingin ?killing cancer?.Finally, tasks such as copywriting, where evoca-tive names are a key element to a successful prod-uct (Ozbal and Strapparava, 2012; Ozbal et al,2012) require exhaustive lists of emotion relatedwords.
In such cases no context is given and thebrand name alone, with its perceived prior polar-ity, is responsible for stating the area of compe-tition and evoking semantic associations.
For ex-ample Mitsubishi changed the name of one of itsSUVs for the Spanish market, since the originalname Pajero had a very negative prior polarity, asit means ?wanker?
in Spanish (Piller, 2003).
Evok-ing emotions is also fundamental for a successfulname: consider names of a perfume like Obses-sion, or technological products like MacBook air.In this work, we aim at automatically producinga high coverage and high precision emotion lex-icon using distributional semantics, with numer-ical scores associated with each emotion, like ithas already been done for sentiment analysis.
Tothis end, we take advantage in an original way ofmassive crowd-sourced affective annotations as-sociated with news articles, obtained by crawl-ing the rappler.com social news network.
Wealso evaluate our lexicon by integrating it in unsu-pervised classification and regression settings foremotion recognition.
Results indicate that the useof our resource, even if automatically acquired, ishighly beneficial in affective text recognition.2 Related WorkWithin the broad field of sentiment analysis, wehereby provide a short review of research effortsput towards building sentiment and emotion lex-ica, regardless of the approach in which such listsare then used (machine learning, rule based ordeep learning).
A general overview can be foundin (Pang and Lee, 2008; Liu and Zhang, 2012;Wilson et al, 2004; Paltoglou et al, 2010).Sentiment Lexica.
In recent years there hasbeen an increasing focus on producing lists ofwords (lexica) with prior polarities, to be used insentiment analysis.
When building such lists, atrade-off between coverage of the resource and itsprecision is to be found.One of the most well-known resources is Senti-WordNet (SWN) (Esuli and Sebastiani, 2006; Bac-cianella et al, 2010), in which each entry is as-sociated with the numerical scores Pos(s) andNeg(s), ranging from 0 to 1.
These scores ?automatically assigned starting from a bunch ofseed terms ?
represent the positive and negativevalence (or posterior polarity) of each entry, thattakes the form lemma#pos#sense-number.Starting from SWN, several prior polarities forwords (SWN-prior), in the form lemma#PoS,can be computed (e.g.
considering only the first-sense, averaging on all the senses, etc.).
These ap-proaches, detailed in (Guerini et al, 2013), pro-duce a list of 155k words, where the lower pre-cision given by the automatic scoring of SWN iscompensated by the high coverage.Another widely used resource is ANEW(Bradley and Lang, 1999), providing valencescores for 1k words, which were manually as-signed by several annotators.
This resource hasa low coverage, but the precision is maximized.Similarly, the SO-CAL entries (Taboada et al,2011) were manually tagged by a small num-ber of annotators with a multi-class label (fromvery negative to very positive).
Theseratings were further validated through crowd-sourcing, ending up with a list of roughly 4kwords.
More recently, a resource that repli-cated ANEW annotation approach using crowd-sourcing, was released (Warriner et al, 2013), pro-viding sentiment scores for 14k words.
Interest-ingly, this resource annotates the most frequentwords in English, so, even if lexicon coverage isstill far lower than SWN-prior, it grants a high cov-erage, with human precision, of language use.Finally, the General Inquirer lexicon (Stoneet al, 1966) provides a binary classifica-tion (positive/negative) of 4k sentiment-bearing words, while the resource in (Wilson et al,2005) expands the General Inquirer to 6k words.Emotion Lexica.
Compared to sentimentlexica, far less emotion lexica have been pro-duced, and all have lower coverage.
One of themost used resources is WordNetAffect (Strappa-rava and Valitutti, 2004) which contains manu-ally assigned affective labels to WordNet synsets(ANGER, JOY, FEAR, etc.).
It currently provides900 annotated synsets and 1.6k words in the form428AFRAID AMUSED ANGRY ANNOYED DONT CARE HAPPY INSPIRED SADdoc 10002 0.75 0.00 0.00 0.00 0.00 0.00 0.25 0.00doc 10003 0.00 0.50 0.00 0.16 0.17 0.17 0.00 0.00doc 10004 0.52 0.02 0.03 0.02 0.02 0.06 0.02 0.31doc 10011 0.40 0.00 0.00 0.20 0.00 0.20 0.20 0.00doc 10028 0.00 0.30 0.08 0.00 0.00 0.23 0.31 0.08Table 1: An excerpt of the Document-by-Emotion Matrix - MDElemma#PoS#sense, corresponding to roughly1 thousand lemma#PoS.AffectNet, part of the SenticNet project (Cam-bria and Hussain, 2012), contains 10k words (outof 23k entries) taken from ConceptNet and alignedwithWordNetAffect.
This resource extendsWord-NetAffect labels to concepts like ?have breakfast?.Fuzzy Affect Lexicon (Subasic and Huettner, 2001)contains roughly 4k lemma#PoS manually an-notated by one linguist using 80 emotion labels.EmoLex (Mohammad and Turney, 2013) containsalmost 10k lemmas annotated with an intensity la-bel for each emotion using Mechanical Turk.
Fi-nally Affect database is an extension of SentiFul(Neviarouskaya et al, 2007) and contains 2.5Kwords in the form lemma#PoS.
The latter is theonly lexicon providing words annotated also withemotion scores rather than only with labels.3 Dataset CollectionTo build our emotion lexicon we harvested all thenews articles from rappler.com, as of June3rd 2013: the final dataset consists of 13.5 Mwords over 25.3 K documents, with an averageof 530 words per document.
For each document,along with the text we also harvested the informa-tion displayed by Rappler?s Mood Meter, a smallinterface offering the readers the opportunity toclick on the emotion that a given Rappler storymade them feel.
The idea behind the Mood Me-ter is actually ?getting people to crowdsource themood for the day?1, and returning the percentageof votes for each emotion label for a given story.This way, hundreds of thousands votes have beencollected since the launch of the service.
In ournovel approach to ?crowdsourcing?, as comparedto other NLP tasks that rely on tools like Ama-zon?s Mechanical Turk (Snow et al, 2008), thesubjects are aware of the ?implicit annotation task?but they are not paid.
From this data, we built adocument-by-emotion matrixMDE, providing thevoting percentages for each document in the eight1http://nie.mn/QuD17Zaffective dimensions available in Rappler.
An ex-cerpt is provided in Table 1.The idea of using documents annotated withemotions is not new (Strapparava and Mihalcea,2008; Mishne, 2005; Bellegarda, 2010), but theseworks had the limitation of providing a singleemotion label per document, rather than a score foreach emotion, and, moreover, the annotation wasperformed by the author of the document alone.Table 2 reports the average percentage of votesfor each emotion on the whole corpus: HAPPI-NESS has a far higher percentage of votes (at leastthree times).
There are several possible explana-tions, out of the scope of the present paper, for thisbias: (i) it is due to cultural characteristics of theaudience (ii) the bias is in the dataset itself, beingformed mainly by ?positive?
news; (iii) it is a psy-chological phenomenon due to the fact that peo-ple tend to express more positive moods on socialnetworks (Quercia et al, 2011; Vittengl and Holt,1998; De Choudhury et al, 2012).
In any case, thepredominance of happy mood has been found inother datasets, for instance LiveJournal.composts (Strapparava and Mihalcea, 2008).
In thefollowing section we will discuss how we handledthis problem.EMOTION Votes?EMOTION Votes?AFRAID 0.04 DONT CARE 0.05AMUSED 0.10 HAPPY 0.32ANGRY 0.10 INSPIRED 0.10ANNOYED 0.06 SAD 0.11Table 2: Average percentages of votes.4 Emotion Lexicon CreationAs a next step we built a word-by-emotion matrixstarting from MDEusing an approach based oncompositional semantics.
To do so, we first lem-matized and PoS tagged all the documents (wherePoS can be adj., nouns, verbs, adv.)
and keptonly those lemma#PoS present also in Word-Net, similar to SWN-prior and WordNetAffect re-sources, to which we want to align.
We then com-puted the term-by-document matrices using raw429Word AFRAID AMUSED ANGRY ANNOYED DONT CARE HAPPY INSPIRED SADawe#n 0.08 0.12 0.04 0.11 0.07 0.15 0.38 0.05comical#a 0.02 0.51 0.04 0.05 0.12 0.17 0.03 0.06crime#n 0.11 0.10 0.23 0.15 0.07 0.09 0.09 0.15criminal#a 0.12 0.10 0.25 0.14 0.10 0.11 0.07 0.11dead#a 0.17 0.07 0.17 0.07 0.07 0.05 0.05 0.35funny#a 0.04 0.29 0.04 0.11 0.16 0.13 0.15 0.08future#n 0.09 0.12 0.09 0.12 0.13 0.13 0.21 0.10game#n 0.06 0.15 0.06 0.08 0.15 0.23 0.15 0.12kill#v 0.23 0.06 0.21 0.07 0.05 0.06 0.05 0.27rapist#n 0.02 0.07 0.46 0.07 0.08 0.16 0.03 0.12sad#a 0.06 0.12 0.09 0.14 0.13 0.07 0.15 0.24warning#n 0.44 0.06 0.09 0.09 0.06 0.06 0.04 0.16Table 3: An excerpt of the Word-by-Emotion Matrix (MWE) using normalized frequencies (nf ).
Emo-tions weighting more than 20% in a word are highlighted for readability purposes.frequencies, normalized frequencies, and tf-idf(MWD,f, MWD,nfand MWD,tfidfrespectively),so to test which of the three weights is better.
Af-ter that, we applied matrix multiplication betweenthe document-by-emotion and word-by-documentmatrices (MDE?
MWD) to obtain a (raw) word-by-emotion matrix MWE.
This method allows usto ?merge?
words with emotions by summing theproducts of the weight of a word with the weightof the emotions in each document.Finally, we transformed MWEby first apply-ing normalization column-wise (so to eliminatethe over representation for happiness as discussedin Section 3) and then scaling the data row-wise soto sum up to one.
An excerpt of the final MatrixMWEis presented in Table 3, and it can be in-terpreted as a list of words with scores that repre-sent how much weight a given word has in the af-fective dimensions we consider.
So, for example,awe#n has a predominant weight in INSPIRED(0.38), comical#a has a predominant weight inAMUSED (0.51), while kill#v has a predomi-nant weight in AFRAID, ANGRY and SAD (0.23,0.21 and 0.27 respectively).
This matrix, that wecall DepecheMood2, represents our emotion lex-icon, it contains 37k entries and is freely availablefor research purposes at http://git.io/MqyoIg.5 ExperimentsTo evaluate the performance we can obtain withour lexicon, we use the public dataset provided forthe SemEval 2007 task on ?Affective Text?
(Strap-parava and Mihalcea, 2007).
The task was focusedon emotion recognition in one thousand newsheadlines, both in regression and classificationsettings.
Headlines typically consist of a few2In French, ?depeche?
means dispatch/news.words and are often written with the intention to?provoke?
emotions so to attract the readers?
atten-tion.
An example of headline from the dataset isthe following: ?Iraq car bombings kill 22 People,wound more than 60?.
For the regression taskthe values provided are: <anger (0.32),disgust (0.27), fear (0.84), joy(0.0), sadness (0.95), surprise(0.20)> while for the classification task thelabels provided are {FEAR, SADNESS}.This dataset is of interest to us since the ?com-positional?
problem is less prominent given thesimplified syntax of news headlines, containing,for example, fewer adverbs (like negations or in-tensifiers) than normal sentences (Turchi et al,2012).
Furthermore, this is to our knowledge theonly dataset available providing numerical scoresfor emotions.
Finally, this dataset was meant forunsupervised approaches (just a small trial samplewas provided), so to avoid simple text categoriza-tion approaches.As the affective dimensions present in the testset ?
based on the six basic emotions model (Ek-man and Friesen, 1971) ?
do not exactly matchwith the ones provided by Rappler?s Mood Meter,we first define a mapping between the two whenpossible, see Table 4.
Then, we proceed to trans-form the test headlines to the lemma#PoS format.SemEval Rappler SemEval RapplerFEAR AFRAID SURPRISE INSPIREDANGER ANGRY - ANNOYEDJOY HAPPY - AMUSEDSADNESS SAD - DON?T CARETable 4: Mapping of Rappler labels on Se-meval2007.
In bold, cases of suboptimal mapping.Only one test headline contained exclusivelywords not present in DepecheMood, further indi-430cating the high-coverage nature of our resource.
InTable 5 we report the coverage of some Sentimentand Emotion Lexica of different sizes on the samedataset.
Similar to Warriner et al (2013), we ob-serve that even if the number of entries of our lex-icon is far lower than SWN-prior approaches, thefact that we extracted and annotated words fromdocuments grants a high coverage of language use.SentimentLexicaANEW 1k entries 0.10Warriner et.
al 13k entries 0.51SWN-prior 155k entries 0.67EmotionLexicaWNAffect 1k entries 0.12DepecheMood 37k entries 0.64Table 5: Statistics on words coverage per headline.Since our primary goal is to assess the quality ofDepecheMood we first focus on the regressiontask.
We do so by using a very na?
?ve approach,similar to ?WordNetAffect presence?
discussed in(Strapparava and Mihalcea, 2008): for each head-line, we simply compute a value, for any affectivedimension, by averaging the corresponding affec-tive scores ?obtained from DepecheMood- of alllemma#PoS present in the headline.In Table 6 we report the results obtained usingthe three versions of our resource (Pearson corre-lation), along with the best performance on eachemotion of other systems3(bestse); the last col-umn contains the upper bound of inter-annotatoragreement.
For all the 5 emotions we improveover the best performing systems (DISGUST hasno alignment with our labels and was discarded).Interestingly, even using a sub-optimal align-ment for SURPRISE we still manage to outper-form other systems.
Considering the na?
?ve ap-proach we used, we can reasonably conclude thatthe quality and coverage of our resource are thereason of such results, and that adopting morecomplex approaches (i.e.
compositionality) canpossibly further improve performances in text-based emotion recognition.As a final test, we evaluate our resource in theclassification task.
The na?
?ve approach used inthis case consists in mapping the average of thescores of all words in the headline to a binary de-cision with fixed threshold at 0.5 for each emotion(after min-max normalization on all test headlines3Systems participating in the ?Affective Text?
task plus theapproaches in (Strapparava and Mihalcea, 2008).
Other su-pervised approaches in the classification task (Mohammad,2012; Bellegarda, 2010; Chaffar and Inkpen, 2011), report-ing only overall performances, are not considered.DepecheMood bestseupperf nf tfidfFEAR 0.56 0.54 0.53 0.45 0.64ANGER 0.36 0.38 0.36 0.32 0.50SURPRISE* 0.25 0.21 0.24 0.16 0.36JOY 0.39 0.40 0.39 0.26 0.60SADNESS 0.48 0.47 0.46 0.41 0.68Table 6: Regression results ?
Pearson?s correlationscores).
In Table 7 we report the results (F1 mea-sure) of our approach along with the best perfor-mance of other systems on each emotion (bestse),as in the previous case.
For 3 emotions out of5 we improve over the best performing systems,for one emotion we obtain the same results, andfor one emotion we do not outperform other sys-tems.
In this case the difference in performancesamong the various ways of representing the word-by-document matrix is more prominent: normal-ized frequencies (nf ) provide the best results.DepecheMood bestsef nf tfidfFEAR 0.25 0.32 0.31 0.23ANGER 0.00 0.00 0.00 0.17SURPRISE* 0.13 0.16 0.09 0.15JOY 0.22 0.30 0.32 0.32SADNESS 0.36 0.40 0.38 0.30Table 7: Classification results ?
F1 measures6 ConclusionsWe presented DepecheMood, an emotion lexi-con built in a novel and totally automated wayby harvesting crowd-sourced affective annota-tion from a social news network.
Our experi-mental results indicate high-coverage and high-precision of the lexicon, showing significant im-provements over state-of-the-art unsupervised ap-proaches even when using the resource with veryna?
?ve classification and regression strategies.
Webelieve that the wealth of information provided bysocial media can be harnessed to build models andresources for emotion recognition from text, goinga step beyond sentiment analysis.
Our future workwill include testing Singular Value Decomposi-tion on the word-by-document matrices, allowingto propagate emotions values for a document tosimilar words non present in the document itself,and the study of perceived mood effects on viral-ity indices and readers engagement by exploitingtweets, likes, reshares and comments.This work has been partially supported by the TrentoRISE PerTe project.431ReferencesS.
Baccianella, A. Esuli, and F. Sebastiani.
2010.
Sen-tiWordNet 3.0: An enhanced lexical resource forsentiment analysis and opinion mining.
In Proceed-ings of the Conference on International LanguageResources and Evaluation (LREC), pages 2200?2204, Valletta, Malta.J.
R. Bellegarda.
2010.
Emotion analysis using latentaffective folding and embedding.
In Proceedings ofthe NAACL HLT 2010 workshop on computationalapproaches to analysis and generation of emotion intext, pages 1?9.
Association for Computational Lin-guistics.M.
Bradley and P. Lang.
1999.
Affective norms forenglish words (ANEW): Instruction manual and af-fective ratings.
Technical Report C-1, University ofFlorida.E.
Cambria and A. Hussain.
2012.
Sentic computing.Springer.S.
Chaffar and D. Inkpen.
2011.
Using a hetero-geneous dataset for emotion analysis in text.
InAdvances in Artificial Intelligence, pages 62?67.Springer.M.
De Choudhury, S. Counts, and M. Gamon.
2012.Not all moods are created equal!
exploring humanemotional states in social media.
In Proceedings ofthe International Conference on Weblogs and SocialMedia (ICWSM).P.
Ekman and W. V. Friesen.
1971.
Constants acrosscultures in the face and emotion.
Journal of Person-ality and Social Psychology, 17:124?129.A.
Esuli and F. Sebastiani.
2006.
SentiWordNet: Apublicly available lexical resource for opinion min-ing.
In Proceedings of the Conference on Interna-tional Language Resources and Evaluation (LREC),pages 417?422, Genova, IT.M.
Guerini, O.
Stock, and C. Strapparava.
2008.Valentino: A tool for valence shifting of natural lan-guage texts.
In Proceedings of the Conference onInternational Language Resources and Evaluation(LREC), Marrakech, Morocco.M.
Guerini, L. Gatti, and M. Turchi.
2013.
Senti-ment analysis: How to derive prior polarities fromsentiwordnet.
In Proceedings of the Conference onEmpirical Methods in Natural Language Processing(EMNLP), pages 1259?1269.D.
Z. Inkpen, O. Feiguina, and G. Hirst.
2006.
Gener-ating more-positive and more-negative text.
InCom-puting Attitude and Affect in Text: Theory and Appli-cations, pages 187?198.
Springer.B.
Liu and L. Zhang.
2012.
A survey of opinion min-ing and sentiment analysis.
Mining Text Data, pages415?463.G.
Mishne.
2005.
Experiments with mood classifica-tion in blog posts.
In Proceedings of ACM SIGIR2005 Workshop on Stylistic Analysis of Text for In-formation Access, volume 19.S.
M. Mohammad and P. D. Turney.
2013.
Crowd-sourcing a word?emotion association lexicon.
Com-putational Intelligence, 29(3):436?465.S.
M. Mohammad.
2012.
# Emotional tweets.
In Pro-ceedings of the First Joint Conference on Lexicaland Computational Semantics (*Sem), pages 246?255.
Association for Computational Linguistics.A.
Neviarouskaya, H. Prendinger, and M. Ishizuka.2007.
Textual affect sensing for sociable andexpressive online communication.
In A. Paiva,R.
Prada, and R. Picard, editors, Affective Com-puting and Intelligent Interaction, volume 4738 ofLecture Notes in Computer Science, pages 218?229.Springer Berlin Heidelberg.A.
Neviarouskaya, H. Prendinger, and M. Ishizuka.2009.
Compositionality principle in recognition offine-grained emotions from text.
In Proceedings ofthe International Conference on Weblogs and SocialMedia (ICWSM).A.
Neviarouskaya, H. Prendinger, and M. Ishizuka.2011.
Affect analysis model: novel rule-based ap-proach to affect sensing from text.
Natural Lan-guage Engineering, 17(1):95.G.
Ozbal and C. Strapparava.
2012.
A computationalapproach to the automation of creative naming.
Pro-ceedings of the 50th Annual Meeting of the Associa-tion for Computational Linguistics (ACL).G.
Ozbal, C. Strapparava, and M. Guerini.
2012.Brand pitt: A corpus to explore the art of naming.In Proceedings of the Conference on InternationalLanguage Resources and Evaluation (LREC).G.
Paltoglou, M. Thelwall, and K. Buckley.
2010.
On-line textual communications annotated with gradesof emotion strength.
In Proceedings of the 3rd In-ternational Workshop of Emotion: Corpora for re-search on Emotion and Affect, pages 25?31.B.
Pang and L. Lee.
2008.
Opinion mining and senti-ment analysis.
Foundations and Trends in Informa-tion Retrieval, 2(1-2):1?135.I.
Piller.
2003.
10. advertising as a site of lan-guage contact.
Annual Review of Applied Linguis-tics, 23:170?183.D.
Quercia, J. Ellis, L. Capra, and J. Crowcroft.
2011.In the mood for being influential on twitter.
Pro-ceedings of IEEE SocialCom?11.R.
Snow, B. O?Connor, D. Jurafsky, and A. Ng.
2008.Cheap and fast?but is it good?
: evaluating non-expert annotations for natural language tasks.
InProceedings of the Conference on Empirical Meth-ods in Natural Language Processing (EMNLP),pages 254?263.432R.
Socher, A. Perelygin, J. Y. Wu, J. Chuang, C. D.Manning, A. Y. Ng, and C. Potts.
2013.
Recur-sive deep models for semantic compositionality overa sentiment treebank.
In Proceedings of the Con-ference on Empirical Methods in Natural LanguageProcessing (EMNLP), pages 1631?1642.P.
Stone, D. Dunphy, and M. Smith.
1966.
The Gen-eral Inquirer: A Computer Approach to ContentAnalysis.
MIT press.C.
Strapparava and R. Mihalcea.
2007.
Semeval-2007 task 14: Affective text.
In Proceedings ofthe 4th International Workshop on Semantic Evalu-ations, pages 70?74.
Association for ComputationalLinguistics.C.
Strapparava and R. Mihalcea.
2008.
Learning toidentify emotions in text.
In Proceedings of the2008 ACM symposium on Applied computing, pages1556?1560.
ACM.C.
Strapparava and A. Valitutti.
2004.
WordNet-Affect: an affective extension of WordNet.
In Pro-ceedings of the Conference on International Lan-guage Resources and Evaluation (LREC), pages1083 ?
1086, Lisbon, May.P.
Subasic and A. Huettner.
2001.
Affect analysis oftext using fuzzy semantic typing.
Fuzzy Systems,IEEE Transactions on, 9(4):483?496.M.
Taboada, J. Brooke, M. Tofiloski, K. Voll, andM.
Stede.
2011.
Lexicon-based methods forsentiment analysis.
Computational linguistics,37(2):267?307.M.
Turchi, M. Atkinson, A. Wilcox, B. Crawley,S.
Bucci, R. Steinberger, and E. Van der Goot.
2012.Onts: optima news translation system.
In Proceed-ings of the Demonstrations at the 13th Conference ofthe European Chapter of the Association for Com-putational Linguistics, pages 25?30.
Association forComputational Linguistics.J.
R. Vittengl and C. S. Holt.
1998.
A time-series diarystudy of mood and social interaction.
Motivationand Emotion, 22(3):255?275.S.
Wang and C. Manning.
2012.
Baselines and bi-grams: Simple, good sentiment and topic classifica-tion.
Proceedings of the 50th Annual Meeting of theAssociation for Computational Linguistics (ACL).A.
B. Warriner, V. Kuperman, and M. Brysbaert.2013.
Norms of valence, arousal, and dominance for13,915 english lemmas.
Behavior research methods,45(4):1191?1207.S.
Whitehead and L. Cavedon.
2010.
Generatingshifting sentiment for a conversational agent.
InProceedings of the NAACL HLT 2010 Workshop onComputational Approaches to Analysis and Genera-tion of Emotion in Text, pages 89?97, Los Angeles,CA, June.
Association for Computational Linguis-tics.T.
Wilson, J. Wiebe, and R. Hwa.
2004.
Just how madare you?
finding strong and weak opinion clauses.In Proceedings of AAAI, pages 761?769.T.
Wilson, J. Wiebe, and P. Hoffmann.
2005.
Recog-nizing contextual polarity in phrase-level sentimentanalysis.
In Proceedings of the conference on Hu-man Language Technology and Empirical Methodsin Natural Language Processing, pages 347?354.433
