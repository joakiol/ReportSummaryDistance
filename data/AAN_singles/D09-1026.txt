Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 248?256,Singapore, 6-7 August 2009.c?2009 ACL and AFNLPLabeled LDA: A supervised topic model for credit attribution inmulti-labeled corporaDaniel Ramage, David Hall, Ramesh Nallapati and Christopher D. ManningComputer Science DepartmentStanford University{dramage,dlwh,nmramesh,manning}@cs.stanford.eduAbstractA significant portion of the world?s textis tagged by readers on social bookmark-ing websites.
Credit attribution is an in-herent problem in these corpora becausemost pages have multiple tags, but the tagsdo not always apply with equal specificityacross the whole document.
Solving thecredit attribution problem requires associ-ating each word in a document with themost appropriate tags and vice versa.
Thispaper introduces Labeled LDA, a topicmodel that constrains Latent Dirichlet Al-location by defining a one-to-one corre-spondence between LDA?s latent topicsand user tags.
This allows Labeled LDA todirectly learn word-tag correspondences.We demonstrate Labeled LDA?s improvedexpressiveness over traditional LDA withvisualizations of a corpus of tagged webpages from del.icio.us.
Labeled LDA out-performs SVMs by more than 3 to 1 whenextracting tag-specific document snippets.As a multi-label text classifier, our modelis competitive with a discriminative base-line on a variety of datasets.1 IntroductionFrom news sources such as Reuters to moderncommunity web portals like del.icio.us, a signif-icant proportion of the world?s textual data is la-beled with multiple human-provided tags.
Thesecollections reflect the fact that documents are oftenabout more than one thing?for example, a newsstory about a highway transportation bill mightnaturally be filed under both transportation andpolitics, with neither category acting as a clearsubset of the other.
Similarly, a single web pagein del.icio.us might well be annotated with tags asdiverse as arts, physics, alaska, and beauty.However, not all tags apply with equal speci-ficity across the whole document, opening up newopportunities for information retrieval and cor-pus analysis on tagged corpora.
For instance,users who browse for documents with a particu-lar tag might prefer to see summaries that focuson the portion of the document most relevant tothe tag, a task we call tag-specific snippet extrac-tion.
And when a user browses to a particulardocument, a tag-augmented user interface mightprovide overview visualization cues highlightingwhich portions of the document are more or lessrelevant to the tag, helping the user quickly accessthe information they seek.One simple approach to these challenges canbe found in models that explicitly address thecredit attribution problem by associating individ-ual words in a document with their most appropri-ate labels.
For instance, in our news story aboutthe transportation bill, if the model knew that theword ?highway?
went with transportation and thatthe word ?politicians?
went with politics, morerelevant passages could be extracted for either la-bel.
We seek an approach that can automaticallylearn the posterior distribution of each word in adocument conditioned on the document?s label set.One promising approach to the credit attributionproblem lies in the machinery of Latent Dirich-let Allocation (LDA) (Blei et al, 2003), a recentmodel that has gained popularity among theoreti-cians and practitioners alike as a tool for automaticcorpus summarization and visualization.
LDA isa completely unsupervised algorithm that modelseach document as a mixture of topics.
The modelgenerates automatic summaries of topics in termsof a discrete probability distribution over wordsfor each topic, and further infers per-documentdiscrete distributions over topics.
Most impor-tantly, LDA makes the explicit assumption thateach word is generated from one underlying topic.Although LDA is expressive enough to model248multiple topics per document, it is not appropriatefor multi-labeled corpora because, as an unsuper-vised model, it offers no obvious way of incorpo-rating a supervised label set into its learning proce-dure.
In particular, LDA often learns some topicsthat are hard to interpret, and the model providesno tools for tuning the generated topics to suit anend-use application, even when time and resourcesexist to provide some document labels.Several modifications of LDA to incorporatesupervision have been proposed in the literature.Two such models, Supervised LDA (Blei andMcAuliffe, 2007) and DiscLDA (Lacoste-Julienet al, 2008) are inappropriate for multiply labeledcorpora because they limit a document to being as-sociated with only a single label.
Supervised LDAposits that a label is generated from each docu-ment?s empirical topic mixture distribution.
Dis-cLDA associates a single categorical label variablewith each document and associates a topic mixturewith each label.
A third model, MM-LDA (Ram-age et al, 2009), is not constrained to one labelper document because it models each document asa bag of words with a bag of labels, with topics foreach observation drawn from a shared topic dis-tribution.
But, like the other models, MM-LDA?slearned topics do not correspond directly with thelabel set.
Consequently, these models fall short asa solution to the credit attribution problem.
Be-cause labels have meaning to the people that as-signed them, a simple solution to the credit attri-bution problem is to assign a document?s words toits labels rather than to a latent and possibly lessinterpretable semantic space.This paper presents Labeled LDA (L-LDA), agenerative model for multiply labeled corpora thatmarries the multi-label supervision common tomodern text datasets with the word-assignmentambiguity resolution of the LDA family of mod-els.
In contrast to standard LDA and its existingsupervised variants, our model associates each la-bel with one topic in direct correspondence.
In thefollowing section, L-LDA is shown to be a natu-ral extension of both LDA (by incorporating su-pervision) and Multinomial Naive Bayes (by in-corporating a mixture model).
We demonstratethat L-LDA can go a long way toward solving thecredit attribution problem in multiply labeled doc-uments with improved interpretability over LDA(Section 4).
We show that L-LDA?s credit attribu-tion ability enables it to greatly outperform sup-D????NzwwK?
?Figure 1: Graphical model of Labeled LDA: un-like standard LDA, both the label set ?
as well asthe topic prior ?
influence the topic mixture ?.port vector machines on a tag-driven snippet ex-traction task on web pages from del.icio.us (Sec-tion 6).
And despite its generative semantics,we show that Labeled LDA is competitive witha strong baseline discriminative classifier on twomulti-label text classification tasks (Section 7).2 Labeled LDALabeled LDA is a probabilistic graphical modelthat describes a process for generating a labeleddocument collection.
Like Latent Dirichlet Allo-cation, Labeled LDA models each document as amixture of underlying topics and generates eachword from one topic.
Unlike LDA, L-LDA in-corporates supervision by simply constraining thetopic model to use only those topics that corre-spond to a document?s (observed) label set.
Themodel description that follows assumes the readeris familiar with the basic LDA model (Blei et al,2003).Let each document d be represented by a tu-ple consisting of a list of word indices w(d)=(w1, .
.
.
, wNd) and a list of binary topic pres-ence/absence indicators ?
(d)= (l1, .
.
.
, lK)where eachwi?
{1, .
.
.
, V } and each lk?
{0, 1}.Here Ndis the document length, V is the vocabu-lary size and K the total number of unique labelsin the corpus.We set the number of topics in Labeled LDA tobe the number of unique labels K in the corpus.The generative process for the algorithm is foundin Table 1.
Steps 1 and 2?drawing the multi-nomial topic distributions over vocabulary ?kforeach topic k, from a Dirichlet prior ?
?remainthe same as for traditional LDA (see (Blei et al,2003), page 4).
The traditional LDA model thendraws a multinomial mixture distribution ?
(d)overallK topics, for each document d, from a Dirichletprior ?.
However, we would like to restrict ?
(d)tobe defined only over the topics that correspond to2491 For each topic k ?
{1, .
.
.
,K}:2 Generate ?k= (?k,1, .
.
.
, ?k,V)T?
Dir(?|?
)3 For each document d:4 For each topic k ?
{1, .
.
.
,K}5 Generate ?(d)k?
{0, 1} ?
Bernoulli(?|?k)6 Generate ?
(d)= L(d)?
?7 Generate ?
(d)= (?l1, .
.
.
, ?lMd)T?
Dir(?|?
(d))8 For each i in {1, .
.
.
, Nd}:9 Generate zi?
{?
(d)1, .
.
.
, ?
(d)Md} ?
Mult(?|?
(d))10 Generate wi?
{1, .
.
.
, V } ?
Mult(?|?zi)Table 1: Generative process for Labeled LDA:?kis a vector consisting of the parameters of themultinomial distribution corresponding to the kthtopic, ?
are the parameters of the Dirichlet topicprior and ?
are the parameters of the word prior,while ?kis the label prior for topic k. For themeaning of the projection matrix L(d), please re-fer to Eq 1.its labels ?(d).
Since the word-topic assignmentszi(see step 9 in Table 1) are drawn from this dis-tribution, this restriction ensures that all the topicassignments are limited to the document?s labels.Towards this objective, we first generate thedocument?s labels ?
(d)using a Bernoulli coin tossfor each topic k, with a labeling prior probability?k, as shown in step 5.
Next, we define the vectorof document?s labels to be ?
(d)= {k|?
(d)k= 1}.This allows us to define a document-specific la-bel projection matrix L(d)of size Md?
K foreach document d, where Md= |?
(d)|, as fol-lows: For each row i ?
{1, .
.
.
,Md} and columnj ?
{1, .
.
.
,K} :L(d)ij={1 if ?
(d)i= j0 otherwise.
(1)In other words, the ithrow of L(d)has an entry of1 in column j if and only if the ithdocument label?
(d)iis equal to the topic j, and zero otherwise.As the name indicates, we use the L(d)matrix toproject the parameter vector of the Dirichlet topicprior ?
= (?1, .
.
.
, ?K)Tto a lower dimensionalvector ?
(d)as follows:?
(d)= L(d)??
= (??
(d)1, .
.
.
, ??
(d)Md)T(2)Clearly, the dimensions of the projected vectorcorrespond to the topics represented by the labelsof the document.
For example, suppose K = 4and that a document d has labels given by ?
(d)={0, 1, 1, 0}which implies ?
(d)= {2, 3}, then L(d)would be:(0 1 0 00 0 1 0).Then, ?
(d)is drawn from a Dirichlet distributionwith parameters ?
(d)= L(d)?
?
= (?2, ?3)T(i.e., with the Dirichlet restricted to the topics 2and 3).This fulfills our requirement that the docu-ment?s topics are restricted to its own labels.
Theprojection step constitutes the deterministic step6 in Table 1.
The remaining part of the modelfrom steps 7 through 10 are the same as for reg-ular LDA.The dependency of ?
on both ?
and ?
is in-dicated by directed edges from ?
and ?
to ?
inthe plate notation in Figure 1.
This is the only ad-ditional dependency we introduce in LDA?s repre-sentation (please compare with Figure 1 in (Blei etal., 2003)).2.1 Learning and inferenceIn most applications discussed in this paper, wewill assume that the documents are multiplytagged with human labels, both at learning and in-ference time.When the labels ?
(d)of the document are ob-served, the labeling prior ?
is d-separated fromthe rest of the model given ?(d).
Hence the modelis same as traditional LDA, except the constraintthat the topic prior ?
(d)is now restricted to theset of labeled topics ?(d).
Therefore, we can usecollapsed Gibbs sampling (Griffiths and Steyvers,2004) for training where the sampling probabilityfor a topic for position i in a document d in La-beled LDA is given by:P (zi= j|z?i) ?nwi?i,j+ ?win(?
)?i,j+ ?T1?n(d)?i,j+ ?jn(d)?i,?+ ?T1(3)where nwi?i,jis the count of word wiin topic j, thatdoes not include the current assignment zi, a miss-ing subscript or superscript (e.g.
n(?
)?i,j)) indicatesa summation over that dimension, and 1 is a vectorof 1?s of appropriate dimension.Although the equation above looks exactly thesame as that of LDA, we have an important dis-tinction in that, the target topic j is restricted tobelong to the set of labels, i.e., j ?
?
(d).Once the topic multinomials ?
are learned fromthe training set, one can perform inference on anynew labeled test document using Gibbs sampling250restricted to its tags, to determine its per-word la-bel assignments z.
In addition, one can also com-pute its posterior distribution ?
over topics by ap-propriately normalizing the topic assignments z.It should now be apparent to the reader howthe new model addresses some of the problems inmulti-labeled corpora that we highlighted in Sec-tion 1.
For example, since there is a one-to-onecorrespondence between the labels and topics, themodel can display automatic topical summariesfor each label k in terms of the topic-specific dis-tribution ?k.
Similarly, since the model assigns alabel zito each word wiin the document d au-tomatically, we can now extract portions of thedocument relevant to each label k (it would be allwords wi?
w(d)such that zi= k).
In addition,we can use the topic distribution ?
(d)to rank theuser specified labels in the order of their relevanceto the document, thereby also eliminating spuriousones if necessary.Finally, we note that other less restrictive vari-ants of the proposed L-LDA model are possible.For example, one could consider a version thatallows topics that do not correspond to the labelset of a given document with a small probability,or one that allows a common background topic inall documents.
We did implement these variantsin our preliminary experiments, but they did notyield better performance than L-LDA in the taskswe considered.
Hence we do not report them inthis paper.2.2 Relationship to Naive BayesThe derivation of the algorithm so far has fo-cused on its relationship to LDA.
However, La-beled LDA can also be seen as an extension ofthe event model of a traditional Multinomial NaiveBayes classifier (McCallum and Nigam, 1998) bythe introduction of a mixture model.
In this sec-tion, we develop the analogy as another way tounderstand L-LDA from a supervised perspective.Consider the case where no document in thecollection is assigned two or more labels.
Nowfor a particular document d with label ld, LabeledLDA draws each word?s topic variable zifrom amultinomial constrained to the document?s labelset, i.e.
zi= ldfor each word position i in the doc-ument.
During learning, the Gibbs sampler willassign each zito ldwhile incrementing ?ld(wi),effectively counting the occurences of each wordtype in documents labeled with ld.
Thus in thesingly labeled document case, the probability ofeach document under Labeled LDA is equal tothe probability of the document under the Multi-nomial Naive Bayes event model trained on thosesame document instances.
Unlike the Multino-mial Naive Bayes classifier, Labeled LDA doesnot encode a decision boundary for unlabeled doc-uments by comparing P (w(d)|ld) to P (w(d)|?ld),although we discuss using Labeled LDA for multi-label classification in Section 7.Labeled LDA?s similarity to Naive Bayes endswith the introduction of a second label to any doc-ument.
In a traditional one-versus-rest Multino-mial Naive Bayes model, a separate classifier foreach label would be trained on all documents withthat label, so each word can contribute a countof 1 to every observed label?s word distribution.By contrast, Labeled LDA assumes that each doc-ument is a mixture of underlying topics, so thecount mass of single word instance must instead bedistributed over the document?s observed labels.3 Credit attribution within taggeddocumentsSocial bookmarking websites contain millions oftags describing many of the web?s most popu-lar and useful pages.
However, not all tags areuniformly appropriate at all places within a doc-ument.
In the sections that follow, we examinemechanisms by which Labeled LDA?s credit as-signment mechanism can be utilized to help sup-port browsing and summarizing tagged documentcollections.To create a consistent dataset for experimentingwith our model, we selected 20 tags of mediumto high frequency from a collection of documentsdataset crawled from del.icio.us, a popular so-cial bookmarking website (Heymann et al, 2008).From that larger dataset, we selected uniformly atrandom four thousand documents that containedat least one of the 20 tags, and then filtered eachdocument?s tag set by removing tags not presentin our tag set.
After filtering, the resulting cor-pus averaged 781 non-stop words per document,with each document having 4 distinct tags on aver-age.
In contrast to many existing text datasets, ourtagged corpus is highly multiply labeled: almost90% of of the documents have more than one tag.
(For comparison, less than one third of the newsdocuments in the popular RCV1-v2 collection ofnewswire are multiply labeled).
We will refer to251this collection of data as the del.icio.us tag dataset.4 Topic VisualizationA first question we ask of Labeled LDA is how itstopics compare with those learned by traditionalLDA on the same collection of documents.
We ranour implementations of Labeled LDA and LDAon the del.icio.us corpus described above.
Bothare based on the standard collapsed Gibbs sam-pler, with the constraints for Labeled LDA imple-mented as in Section 2.web search site blog css content google list page posted great work comments read nice post great april blog march june wordpressbook image pdf review library posted read copyright books titlewebbookssciencecomputerreligionjavacultureworks water map human life work science time world years sleepwindows file version linux comp-uter free system software maccomment god jesus people gospel bible reply lord religion writtenapplications spring open web java pattern eclipse development ajaxpeople day link posted time com-ments back music jane permalinknews information service web on-line project site free search homeweb images design content java css website articles page learningjun quote pro views added check anonymous card core power ghzlife written jesus words made man called mark john person fact name8house light radio media photo-graphy news music travel covergame review street public art health food city history science131943212Tag (Labeled LDA) (LDA) Topic IDFigure 2: Comparison of some of the 20 topicslearned on del.icio.us by Labeled LDA (left) andtraditional LDA (right), with representative wordsfor each topic shown in the boxes.
Labeled LDA?stopics are named by their associated tag.
Arrowsfrom right-to-left show the mapping of LDA topicsto the closest Labeled LDA topic by cosine simi-larity.
Tags not shown are: design, education, en-glish, grammar, history, internet, language, phi-losophy, politics, programming, reference, style,writing.Figure 2 shows the top words associated with20 topics learned by Labeled LDA and 20 topicslearned by unsupervised LDA on the del.icio.usdocument collection.
Labeled LDA?s topics aredirectly named with the tag that corresponds toeach topic, an improvement over standard prac-tice of inferring the topic name by inspection (Meiet al, 2007).
The topics learned by the unsu-pervised variant were matched to a Labeled LDAtopic highest cosine similarity.The topics selected are representative: com-pared to Labeled LDA, unmodified LDA allocatesmany topics for describing the largest parts of theThe Elements of Style , William Strunk , Jr.Asserting that one must first know the rules to break them, thisclassic reference book is a must-have for any student andconscientious writer.
Intended for use in which the practice ofcomposition is combined with the study of literature, it gives inbrief space the principal requirements of plain English style andconcentratesattention on the rules of usage and principles ofcomposition most commonly violated.Figure 3: Example document with importantwords annotated with four of the page?s tags aslearned by Labeled LDA.
Red (single underline)is style, green (dashed underline) grammar, blue(double underline) reference, and black (jaggedunderline) education.corpus and under-represents tags that are less un-common: of the 20 topics learned, LDA learnedmultiple topics mapping to each of five tags (web,culture, and computer, reference, and politics, allof which were common in the dataset) and learnedno topics that aligned with six tags (books, english,science, history, grammar, java, and philosophy,which were rarer).5 Tagged document visualizationIn addition to providing automatic summaries ofthe words best associated with each tag in the cor-pus, Labeled LDA?s credit attribution mechanismcan be used to augment the view of a single doc-ument with rich contextual information about thedocument?s tags.Figure 3 shows one web document from the col-lection, a page describing a guide to writing En-glish prose.
The 10 most common tags for thatdocument are writing, reference, english, gram-mar, style, language, books, book, strunk, and ed-ucation, the first eight of which were included inour set of 20 tags.
In the figure, each word that hashigh posterior probability from one tag has beenannotated with that tag.
The red words come fromthe style tag, green from the grammar tag, bluefrom the reference tag, and black from the educa-tion tag.
In this case, the model does very well atassigning individual words to the tags that, subjec-tively, seem to strongly imply the presence of thattag on this page.
A more polished rendering couldadd subtle visual cues about which parts of a pageare most appropriate for a particular set of tags.252booksL-LDA this classic reference book is a must-have for anystudent and conscientious writer.
Intended forSVM the rules of usage and principles of compositionmost commonly violated.
Search: CONTENTS Bibli-ographiclanguageL-LDA the beginning of a sentence must refer to the gram-matical subject 8.
Divide words atSVM combined with the study of literature, it gives in briefspace the principal requirements ofgrammarL-LDA requirements of plain English style and concen-trates attention on the rules of usage and principles ofSVM them, this classic reference book is a must-have forany student and conscientious writer.Figure 4: Representative snippets extracted byL-LDA and tag-specific SVMs for the web pageshown in Figure 3.6 Snippet ExtractionAnother natural application of Labeled LDA?scredit assignment mechanism is as a means of se-lecting snippets of a document that best describeits contents from the perspective of a particulartag.
Consider again the document in Figure 3.
In-tuitively, if this document were shown to a userinterested in the tag grammar, the most appropri-ate snippet of words might prefer to contain thephrase ?rules of usage,?
whereas a user interestedin the term style might prefer the title ?Elementsof Style.
?To quantitatively evaluate Labeled LDA?s per-formance at this task, we constructed a set of 29recently tagged documents from del.icio.us thatwere labeled with two or more tags from the 20 tagsubset, resulting in a total of 149 (document,tag)pairs.
For each pair, we extracted a 15-word win-dow with the highest tag-specific score from thedocument.
Two systems were used to score eachwindow: Labeled LDA and a collection of one-vs-rest SVMs trained for each tag in the system.L-LDA scored each window as the expected prob-ability that the tag had generated each word.
ForSVMs, each window was taken as its own doc-ument and scored using the tag-specific SVM?sun-thresholded scoring function, taking the win-dow with the most positive score.
While a com-plete solution to the tag-specific snippet extractionModel Best Snippet UnanimousL-LDA 72 / 149 24 / 51SVM 21 / 149 2 / 51Table 2: Human judgments of tag-specific snippetquality as extracted by L-LDA and SVM.
The cen-ter column is the number of document-tag pairs forwhich a system?s snippet was judged superior.
Theright column is the number of snippets for whichall three annotators were in complete agreement(numerator) in the subset of document scored byall three annotators (denominator).problem might be more informed by better lin-guistic features (such as phrase boundaries), thisexperimental setup suffices to evaluate both kindsof models for their ability to appropriately assignwords to underlying labels.Figure 3 shows some example snippets outputby our system for this document.
Note that whileSVMs did manage to select snippets that werevaguely on topic, Labeled LDA?s outputs are gen-erally of superior subjective quality.
To quantifythis intuition, three human annotators rated eachpair of snippets.
The outputs were randomly la-beled as ?System A?
or ?System B,?
and the anno-tators were asked to judge which system generateda better tag-specific document subset.
The judgeswere also allowed to select neither system if therewas no clear winner.
The results are summarizedin Table 2.L-LDA was judged superior by a wide margin:of the 149 judgments, L-LDA?s output was se-lected as preferable in 72 cases, whereas SVM?swas selected in only 21.
The difference betweenthese scores was highly significant (p < .001) bythe sign test.
To quantify the reliability of the judg-ments, 51 of the 149 document-tag pairs were la-beled by all three annotators.
In this group, thejudgments were in substantial agreement,1withFleiss?
Kappa at .63.Further analysis of the triply-annotated sub-set yields further evidence of L-LDA?s advantageover SVM?s: 33 of the 51 were tag-page pairswhere L-LDA?s output was picked by at least oneannotator as a better snippet (although L-LDAmight not have been picked by the other annota-tors).
And of those, 24 were unanimous in that1Of the 15 judgments that were in contention, only twoconflicted on which system was superior (L-LDA versusSVM); the remaining disagreements were about whether ornot one of the systems was a clear winner.253all three judges selected L-LDA?s output.
By con-trast, only 10 of the 51 were tag-page pairs whereSVMs?
output was picked by at least one anno-tator, and of those, only 2 were selected unani-mously.7 Multilabeled Text ClassificationIn the preceding section we demonstrated how La-beled LDA?s credit attribution mechanism enabledeffective modeling within documents.
In this sec-tion, we consider whether L-LDA can be adaptedas an effective multi-label classifier for documentsas a whole.
To answer that question, we applieda modified variant of L-LDA to a multi-label doc-ument classification problem: given a training setconsisting of documents with multiple labels, pre-dict the set of labels appropriate for each docu-ment in a test set.Multi-label classification is a well researchedproblem.
Many modern approaches incorporatelabel correlations (e.g., Kazawa et al (2004), Jiet al (2008)).
Others, like our algorithm arebased on mixture models (such as Ueda and Saito(2003)).
However, we are aware of no methodsthat trade off label-specific word distributions withdocument-specific label distributions in quite thesame way.In Section 2, we discussed learning and infer-ence when labels are observed.
In the task of mul-tilabel classification, labels are available at train-ing time, so the learning part remains the same asdiscussed before.
However, inferring the best setof labels for an unlabeled document at test time ismore complex: it involves assessing all label as-signments and returning the assignment that hasthe highest posterior probability.
However, thisis not straight-forward, since there are 2Kpossi-ble label assignments.
To make matters worse, thesupport of ?(?
(d)) is different for different labelassignments.
Although we are in the process ofdeveloping an efficient sampling algorithm for thisinference, for the purposes of this paper we makethe simplifying assumption that the model reducesto standard LDA at inference, where the documentis free to sample from any of the K topics.
Thisis a reasonable assumption because allowing themodel to explore the whole topic space for eachdocument is similar to exploring all possible labelassignments.
The document?s most likely labelscan then be inferred by suitably thresholding itsposterior probability over topics.As a baseline, we use a set of multiple one-vs-rest SVM classifiers which is a popular and ex-tremely competitive baseline used by most previ-ous papers (see (Kazawa et al, 2004; Ueda andSaito, 2003) for instance).
We scored each modelbased on Micro-F1 and Macro-F1 as our evalua-tion measures (Lewis et al, 2004).
While the for-mer allows larger classes to dominate its results,the latter assigns an equal weight to all classes,providing us complementary information.7.1 YahooWe ran experiments on a corpus from the Yahoodirectory, modeling our experimental conditionson the ones described in (Ji et al, 2008).2Weconsidered documents drawn from 8 top level cat-egories in the Yahoo directory, where each doc-ument can be placed in any number of subcate-gories.
The results were mixed, with SVMs aheadon one measure: Labeled LDA beat SVMs on fiveout of eight datasets on MacroF1, but didn?t winon any datasets on MicroF1.
Results are presentedin Table 3.Because only a processed form of the docu-ments was released, the Yahoo dataset does notlend itself well to error analysis.
However, only33% of the documents in each top-level categorywere applied to more than one sub-category, so thecredit assignment machinery of L-LDA was un-used for the majority of documents.
We there-fore ran an artificial second set of experimentsconsidering only those documents that had beengiven more than one label in the training data.
Onthese documents, the results were again mixed, butLabeled LDA comes out ahead.
For MacroF1,L-LDA beat SVMs on four datasets, SVMs beatL-LDA on one dataset, and three were a statisticaltie.3On MicroF1, L-LDA did much better than onthe larger subset, outperforming on four datasetswith the other four a statistical tie.It is worth noting that the Yahoo datasets areskewed by construction to contain many docu-ments with highly overlapping content: becauseeach collection is within the same super-class suchas ?Arts?, ?Business?, etc., each sub-categories?2We did not carefully tune per-class thresholds of each ofthe one vs. rest classifiers in each model, but instead tunedonly one threshold for all classifiers in each model via cross-validation on the Arts subsets.
As such, our numbers were onan average 3-4% less than those reported in (Ji et al, 2008),but the methods were comparably tuned.3The difference between means of multiple runs were notsignificantly different by two-tailed paired t-test.254Dataset %MacroF1 %MicroF1L-LDA SVM L-LDA SVMArts 30.70(1.62) 23.23 (0.67) 39.81(1.85) 48.42 (0.45)Business 30.81(0.75) 22.82 (1.60) 67.00(1.29) 72.15 (0.62)Computers 27.55(1.98) 18.29 (1.53) 48.95(0.76) 61.97 (0.54)Education 33.78(1.70) 36.03 (1.30) 41.19(1.48) 59.45 (0.56)Entertainment 39.42(1.38) 43.22 (0.49) 47.71(0.61) 62.89 (0.50)Health 45.36(2.00) 47.86 (1.72) 58.13(0.43) 72.21 (0.26)Recreation 37.63(1.00) 33.77 (1.17) 43.71(0.31) 59.15 (0.71)Society 27.32(1.24) 23.89 (0.74) 42.98(0.28) 52.29 (0.67)Table 3: Averaged performance across ten runs of multi-label text classification for predicting subsetsof the named Yahoo directory categories.
Numbers in parentheses are standard deviations across runs.L-LDA outperforms SVMs on 5 subsets with MacroF1, but on no subsets with MicroF1.vocabularies will naturally overlap a great deal.L-LDA?s credit attribution mechanism is most ef-fective at partitioning semantically distinct wordsinto their respective label vocabularies, so we ex-pect that Labeled-LDA?s performance as a textclassifier would improve on collections with moresemantically diverse labels.7.2 Tagged Web PagesWe also applied our method to text classificationon the del.icio.us dataset, where the documents arenaturally multiply labeled (more than 89%) andwhere the tags are less inherently similar than inthe Yahoo subcategories.
Therefore we expect La-beled LDA to do better credit assignment on thissubset and consequently to show improved perfor-mance as a classifier, and indeed this is the case.We evaluated L-LDA and multiple one-vs-restSVMs on 4000 documents with the 20 tag sub-set described in Section 3.
L-LDA and multipleone-vs-rest SVMs were trained on the first 80% ofdocuments and evaluated on the remaining 20%,with results averaged across 10 random permuta-tions of the dataset.
The results are shown in Ta-ble 4.
We tuned the SVMs?
shared cost parameterC(= 10.0) and selected raw term frequency overtf-idf weighting based on 4-fold cross-validationon 3,000 documents drawn from an independentpermutation of the data.
For L-LDA, we tuned theshared parameters of threshold and proportional-ity constants in word and topic priors.
L-LDA andSVM have very similar performance on MacroF1,while L-LDA substantially outperforms on Mi-croF1.
In both cases, L-LDA?s improvement isstatistically significantly by a 2-tailed paired t-testat 95% confidence.Model %MacroF1 %MicroF1L-LDA 39.85 (.989) 52.12 (.434)SVM 39.00 (.423) 39.33 (.574)Table 4: Mean performance across ten runs ofmulti-label text classification for predicting 20tags on del.icio.us data.
L-LDA outperformsSVMs significantly on both metrics by a 2-tailed,paired t-test at 95% confidence.8 DiscussionOne of the main advantages of L-LDA on mul-tiply labeled documents comes from the model?sdocument-specific topic mixture ?.
By explicitlymodeling the importance of each label in the doc-ument, Labeled LDA can effective perform somecontextual word sense disambiguation, which sug-gests why L-LDA can outperform SVMs on thedel.icio.us dataset.As a concrete example, consider the excerptof text from the del.icio.us dataset in Figure 5.The document itself has several tags, includingdesign and programming.
Initially, many of thelikelihood probabilities p(w|label) for the (con-tent) words in this excerpt are higher for the labelprogramming than design, including ?content?,?client?, ?CMS?
and even ?designed?, while de-sign has higher likelihoods for just ?website?
and?happy?.
However, after performing inference onthis document using L-LDA, the inferred docu-ment probability for design (p(design)) is muchhigher than it is for programming.
In fact, thehigher probability for the tag more than makes upthe difference in the likelihood for all the wordsexcept ?CMS?
(Content Management System), so255The website is designed, CMS works, content has been added and the client is happy.The website is designed, CMS works, content has been added and the client is happy.Before InferenceAfter InferenceFigure 5: The effect of tag mixture proportions for credit assignment in a web document.
Blue (singleunderline) words are generated from the design tag; red (dashed underline) from the programming tag.By themselves, most words used here have a higher probability in programming than in design.
Butbecause the document as a whole is more about design than programming(incorporating words not shownhere), inferring the document?s topic-mixture ?
enables L-LDA to correctly re-assign most words.that L-LDA correctly infers that most of the wordsin this passage have more to do with design thanprogramming.9 ConclusionThis paper has introduced Labeled LDA, a novelmodel of multi-labeled corpora that directly ad-dresses the credit assignment problem.
The newmodel improves upon LDA for labeled corporaby gracefully incorporating user supervision in theform of a one-to-one mapping between topics andlabels.
We demonstrate the model?s effectivenesson tasks related to credit attribution within docu-ments, including document visualizations and tag-specific snippet extraction.
An approximation toLabeled LDA is also shown to be competitive witha strong baseline (multiple one vs-rest SVMs) formulti-label classification.Because Labeled LDA is a graphical modelin the LDA family, it enables a range of natu-ral extensions for future investigation.
For exam-ple, the current model does not capture correla-tions between labels, but such correlations mightbe introduced by composing Labeled LDA withnewer state of the art topic models like the Cor-related Topic Model (Blei and Lafferty, 2006) orthe Pachinko Allocation Model (Li and McCal-lum, 2006).
And with improved inference for un-supervised ?, Labeled LDA lends itself naturallyto modeling semi-supervised corpora where labelsare observed for only some documents.AcknowledgmentsThis project was supported in part by the Presi-dent of Stanford University through the IRiSS Ini-tiatives Assessment project.ReferencesD.
M. Blei and J. Lafferty.
2006.
Correlated TopicModels.
NIPS, 18:147.D.
Blei and J McAuliffe.
2007.
Supervised TopicModels.
In NIPS, volume 21.D.
M. Blei, A.Y.
Ng, and M.I.
Jordan.
2003.
LatentDirichlet alocation.
JMLR.T.
L. Griffiths and M. Steyvers.
2004.
Finding scien-tific topics.
PNAS, 1:5228?35.P.
Heymann, G. Koutrika, and H. Garcia-Molina.
2008.Can social bookmarking improve web search.
InWSDM.S.
Ji, L. Tang, S. Yu, and J. Ye.
2008.
Extractingshared subspace for multi-label classification.
InKDD, pages 381?389, New York, NY, USA.
ACM.H.
Kazawa, H. Taira T. Izumitani, and E. Maeda.
2004.Maximal margin labeling for multi-topic text catego-rization.
In NIPS.S.
Lacoste-Julien, F. Sha, and M. I. Jordan.
2008.
Dis-cLDA: Discriminative learning for dimensionalityreduction and classification.
In NIPS, volume 22.D.
D. Lewis, Y. Yang, T. G. Rose, G. Dietterich, F. Li,and F. Li.
2004.
RCV1: A new benchmark collec-tion for text categorization research.
JMLR, 5:361?397.Wei Li and Andrew McCallum.
2006.
Pachinko allo-cation: Dag-structured mixture models of topic cor-relations.
In International conference on Machinelearning, pages 577?584.A.
McCallum and K. Nigam.
1998.
A comparison ofevent models for naive bayes text classification.
InAAAI-98 workshop on learning for text categoriza-tion, volume 7.Q.
Mei, X. Shen, and C Zhai.
2007.
Automatic label-ing of multinomial topic models.
In KDD.D.
Ramage, P. Heymann, C. D. Manning, andH.
Garcia-Molina.
2009.
Clustering the tagged web.In WSDM.N.
Ueda and K. Saito.
2003.
Parametric mixture mod-els for multi-labeled text includes models that can beseen to fit within a dimensionality reduction frame-work.
In NIPS.256
