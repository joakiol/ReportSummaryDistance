Data-Oriented Methods for Grapheme-to-Phoneme ConversionAntal van den Bosch and Walter DaelemansITK (Institute for Language Technology and AI)Tilburg UniversityP.O.
Box 90153, NL-5000 LE TilburgTel: +31 13 663070Email: antalb~kub.nl, walter@kub.nlAbst ractIt is traditionally assumed that varioussources of linguistic knowledge and their in-teraction should be formalised in order tobe able to convert words into their phone-mic representations with reasonable accu-racy.
We show that using supervised learn-ing techniques, based on a corpus of tran-scribed words, the same and even betterperformance can be achieved, without ex-plicit modeling of linguistic knowledge.In this paper we present wo instances ofthis approach.
A first model implements avariant of instance-based learning, in whicha weighed similarity metric and a databaseof prototypical exemplars are used to pre-dict new mappings.
In the second model,grapheme-to-phoneme mappings are lookedup in a compressed text-to-speech lexicon(table lookup) enriched with default map-pings.
We compare performance and accu-racy of these approaches to a connectionist(backpropagation) approach and to the lin-guistic knowledge-based approach.1 In t roduct ionGrapheme-to-phoneme conversion is a central taskin any text-to-speech (reading aloud) system.
Givenan alphabet of spelling symbols (graphemes) andan alphabet of phonetic symbols, a mapping shouldbe achieved transliterating strings of graphemes intostrings of phonetic symbols.
It is well known thatthis mapping is difficult because in general, not allgraphemes are realised in the phonetic transcription,and the same grapheme may correspond to differentphonetic symbols, depending on context.It is traditionally assumed that various sources oflinguistic knowledge and their interaction should beformalised in order to be able to convert words intotheir phonemic representations with reasonable accu-racy.
Although different researchers propose differ-ent knowledge structures, consensus seems to be thatat least morphological and phonotactic knowledgeshould be incorporated in order to be able to findmorphological nd syllable structure.
These struc-tures are deemed necessary to define the proper do-mains for phonological nd phonetic rules.
As a typi-cal architecture for grapheme-to-phoneme conversionin Dutch, consider the modules in \[Daelemans, 1988\]shown in Figure 1.
It contains most of the traditionaldatastructures and processing components proposedby computational linguists.A problem with this approach is that the knowl-edge needed is highly language-dependent a d re-quires a significant amount of linguistic engineer-ing.
We argue that using data-oriented learning tech-niques on a corpus of transcribed words (information.which is readily available in many machine-readabledictionaries), the same and even better performancecan be achieved, without explicit modeling of linguis-tic knowledge.
The advantages of such an approachare that the technique is reusable for different setsof data (e.g.
different languages or sublanguages),and that it is automatic (no explicit linguistic engi-neering is needed to handcraft he rules and knowl-edge structures necessary for implementing the tar-get mapping).In this paper we present two instances of thisapproach in the domain of Grapheme-to-Phonemeconversion.
A first model implements a variant ofinstance-based learning, in which a similarity metric(weighed by using a metric based on information en-tropy) and a database of prototypieal exemplars are45l _I s.  I Syllabification ~ ,/ ~.
~ Assig~Pho~)~ogieal/~ /\[ M orph ,logical ~ "~1 V~ Trans~ve 't Lexic~l If ,~ I I Syntactic ~ DataBase J IPhoneti?Aniysis \] "~ " \[ Into~TextStress IAssignment \]onetic Rules & \[l ?niti?n \]SpeechFigure 1: Modules in GRAFON, a linguisticknowledge-based grapheme-to-phonerae conversionsystem.used to predict new mappings.
In a second model,grapheme-to-phoneme mappings are looked up in acompressed text-to-speech lexicon (table lookup) en-riched with default mappings.
The most surprisingresult of our research is that the simplest method(based on tables and defaults) yields the best gener-alisation results, suggesting that previous knowledge-based approaches to the problem were overkill.
Forthe case of Dutch, we make a comparison of perfor-mance and accuracy of these approaches to a connec-tionist (backpropagation) approach and to a state-of-the-art linguistic knowledge-based approach.
Toprove reusability of the method, we show how ourapproach can also be used for automatically gener-ating French and English phonemisation modules.2 Data -Or iented  Text - to -speechConvers ionThe algorithms we have applied in our research aresimilarity-based and data-oriented.
The phonemisa-tion problem is interpreted as a classification task.Given a target grapheme and its context, the cor-responding phoneme should be predicted.
The al-gorithms we used to learn this task are supervisedand data-intensive in the sense that a large num-ber of examples i  provided of input representationswith their correct category (in this case a phonetictranscription).
Within asupervised, similarity-basedapproach, the degree in which abstractions are ex-tracted from the examples may be different, as maybe the time when abstractions are created: dur-ing training in aggressive abstraction, during perfor-mance in lazy learning.
For grapheme-to-phonemeconversion, we claim a data-intensive, lazy learningapproach is appropriate to capture the intricate in-teractions between regularities, subregularities, andexceptions that characterise the domain.2.1 Training and Test Set Encod ingTraining and test set were randomly selected froma Dutch text-to-speech vocabulary data base.
Fromthe 70,000 word dataset, 20,000 were randomly se-lected and randomly divided into 18,500 trainingwords and 1,500 test words.
In both sets, eachgraphemic word is accompanied by its pronuncia-tion in the form of a string of phonemes.
In caseswhere phonemes correspond to grapheme clusters(i.e.
there is an alignment problem of graphemestrings with their corresponding phoneme strings), asis the case in, e.g., <schoenen> (shoes)/sXuno/, onegrapheme of that cluster is algorithmically mappedto the phoneme, and the remaining raphemes aremapped to phonetic nulls, represented by hyphens.In the example of <schoenen>, this phonetic null in-sertion results in the following alignment:EfTo provide a learning system with sufficient informa-tion about the phonemisation task, context informa-tion must be added.
In the models described below,this is done by using graphemic windows (compare\[Sejnowski and l~senberg, 1987\]), i.e.
fixed-lengthparts of words in which one grapheme is mappedto a phoneme; the other graphemes erve as con-text.
For example, using a window with one leftcontext grapheme and two right context graphemes(from here on written as '1-1-2'), the application ofthis window on the word < boek> (book), pronouncedas/buk/ ,  would result in the four pattern-categorypairs of Table 1.Pattern Leftnr.
context12 g3 o4 eFocuspositionboekRight Targetcontext phonemeo e be k uk _- _ kTable 1: Example of the application of the 1-1-~ en-coding on the word < boek > (book).
Underscoresrepresent spaces, a hyphen represents a phonetic null.This approach implies that dependencies stretch-ing longer than the length of the graphemic windowcannot be learned.462.2 Instance-Based LearningAs an example of a lazy learning approach, we exper-imented with Instance-Based Learning (IBL, \[Aha etal., 1991\]).
IBL is a framework and methodologyfor incremental supervised machine learning.
Thedistinguishing feature of IBL is the fact that no ex-plicit abstractions are constructed on the basis of thetraining examples during the training phase.
A se-lection of the training items themselves i used toclassify new inputs.
IBL shares with Memory-BasedReasoning (MBR, \[Stanfill and Waltz, 1986\]) andCase-Based Reasoning (CBR, \[Riesbeck and Schank,1989\]) the hypothesis that much of intelligent be-haviour is based on the immediate use of storedepisodes of earlier experience rather than on the useof explicitly constructed abstractions extracted fromthis experience (e.g.
in the form of rules or decisiontrees).
In the present context of learning linguis-tic mappings, the hypothesis would be that much oflanguage behaviour is based on this type of memory-based processing rather than on rule-based process-ing.
In linguistics, a similar emphasis on analogy tostored examples instead of explicit but inaccessiblerules, is present in the work of a.o.
\[Derwing andSkousen, 1989\].
IBL is inspired to some extent onpsychological research on exemplar-based categori-sation (as opposed to classical and probabilistic at-egorisation, \[Smith and Medin, 1981\]).
Finally, as faras algorithms are concerned, IBL finds its inspirationin statistical pattern recognition, especially the richresearch tradition on the nearest-neighbour decisionrule (see e.g.
\[Devijver and Kittler, 1982\], for anoverview).2.2.1 Basic Algor i thm and ExtensionsThe main datastructure in our version of IBL isthe exemplar, amemory structure representing abouteach pattern the following information: (i) Its distri-bution over the different categories (training patternsmay be ambiguous between different categories, sothe memory structure should keep information abouthow many times each category was assigned to a par-ticular pattern).
(ii) Its category.
This is simply thecategory with highest frequency in the distribution ofa pattern, or a random selection to break a tie.
(iii)Other bookkeeping information (performance data,frequency of pattern in training set, etc.)Training.
For each training pattern, it is checkedwhether an exemplar for it is already present in mem-ory.
If this is the case, the frequency of its categoryis incremented in the distribution field of the corre-sponding memory structure.
If the new training itemhas not yet been stored in memory, a new memorystructure is created.
In learning linguistic mappings(a noisy domain), learning in IBL often is helpedby forgetting poorly performing or unrepresentativetraining items.
In this research a simple techniquewas used to prune memory: each new training itemis first classified using the memory structures alreadypresent.
If it is categorised correctly, it is skipped.We have experimented also with more elaborate stor-age saving techniques (based on prototypicality andperformance oftraining patterns), but the results arepreliminary and will not be reported here.Testing.
If the test pattern is present in memory,the category with the highest frequency associatedwith it is used.
If it is not in memory, all memoryitems are sorted according to the similarity of theirpattern to the test pattern.
The (most frequent)category of the highest ranking exemplar is then pre-dicted as category of the test pattern.
When using aEuclidean distance metric (geometrical distance be-tween two patterns in pattern space), all features areinterpreted as being equally important.
But this isof course not necessarily the case.
We extended thebasic IBL algorithm proposed by \[Aha et al, 1991\]with a technique for assigning a different importanceto different features.
Our approach to the problem ofweighing the relative importance of features is basedon the concept of Information Gain (IG), also usedin learning inductive decision trees, \[Quinlan, 1986\],and first introduced (as far as we know) in IBL in\[Daelemans and Van den Bosch, 1992\] in the con-text of a syllable segmentation task.
The idea is tointerpret he training set as an information sourcecapable of generating a number of messages (the dif-ferent categories) with a certain probability.
The in-formation entropy of such an information source canbe compared in turn for each feature to the averageinformation entropy of the information source whenthe value of that feature is known.
The differenceis the IG value for that feature.
The (normalised)IG value is used as a weight for that feature duringsimilarity matching.
Figure 2 shows the pattern ofinformation-gain values for the different positions inthe 2-1-3 grapheme window.
Unsurprisingly, the tar-get grapheme is most informative, and context fea-tures become less informative the further they areremoved from the target.
We also found that rightcontext is more informative than left context (com-pare \[Weijters, 1991\]).2.3 Table Lookup with DefaultsOur table lookup model can be seen as a link betweenstraightforward lexical lookup and similarity-basedreasoning.
Lexical lookup of word-pronunciationpairs has various disadvantages, an important onebeing that this approach only works for the wordsthat are stored in the lexicon and not for new words.Without the possibility of manipulating raphemicstrings smaller than whole words, there is no waythat lexical ookup can provide generalisations onthebasis of which new words can be transliterated.The table lookup model presented here takes asits training set a text-to-speech lexicon, but solvesthe problems of lacking generalisation power and efli-47I6O5O40302O101111001-2 lalgel-1 Ilroel Illroel+| IIIrOet+2 targot+3Foat~Figure 2: Information gain value for each position inthe 2-1-3 grapheme window.ciency by compressing it into a text-to-speech lookuptable.
The main strategy behind the model is to dy-namically determine which left and right contexts areminimally sufficient o be able to map a graphemeto the correct phoneme with absolute certainty 1.The context needed to disambiguate a grapheme-to-phoneme mapping can be of very different width.Extreme xamples in Dutch are on the one hand thec-cedille, present in a small number of loan words(e.g., <re,u:>), always pronounced as/s/regardlessof left or right context, and on the other hand the< e>, which can map to various phonemes (e.g.,/o/,/?
/ , /e / )  in various contexts.
For example, the dis-ambiguation ofthe pronunciation fthe final < e> inwords ending with <-ster> (either star or female pro-fession suffix) sometimes involves taking into accountlarge left contexts, as in the examples <venster>(window) and <diensler> (servant), in which the fi-nal <e> is pronounced /0/, versus <morgenster>(morning star), in which the final < e> is pronounced/E/.
To disambiguate b tween these three cases, itis necessary to go back five positions in these wordsto find the first grapheme which the words do nothave in common.Table Construct ion.
The algorithm starts bysearching for all unambiguous one-to-one grapheme-phoneme mappings, and storing these mappings(patterns) in the lookup table, more specifically inthe 0-1-0 subtable.
The few unambiguous 0-1-0 pat-terns in our training set include the < f> - / s /casementioned earlier.
The next step of the algorithmis to extend the width of the graphemic window by1 Here, absolute certMnty of a grapheme-phoneme cor-respondence does only express the fact that that cor-respondence is unambiguous in the training set of themodel.one character.
We chose to start by extending thewindow on the right (i.e., a 0-1-1 window), because,as also reflected earlier in the Information Gain met-ric used in the IBL model, right context appears tocontain slightly more valuable information than theequivalent left context 2 .
The algorithm then searchesfor all certain 0-1-1 patterns to store in the 0-1-1subtable.
Compression is achieved because xten-sions of unambiguous patterns in the 0-1-0 subtabledo not have to be stored in the 0-1-1 subtable.
Thisprocedure of extending the window and storing allcertain patterns that have not been stored earlier isthen repeated (extending 0-1-1 to 1-1-1, then to 1-1-2, etc.
), and stops when the whole training corpusis compressed in the lookup table, and all grapheme-phoneme mappings in the corpus are supplied withsufficient left and right contexts.
The model eval-uated below is calculated up to the 5-1-5 window.At that point, the lookup table covers 99.5% of allgrapheme-phoneme mappings in the training set.
Asa measure of the amount of compression, in numberof bytes, the size of the set of linked tables (includingthe default able discussed below) is 5.8% of the sizeof the part of the lexicon used as training set 3.Figure 3 displays the magnitudes ofthe subtables.It can clearly be seen that most ambiguity is resolvedwith relatively small contexts.
The majority of theambiguity in the training set is already resolved atthe 2-1-2 subtable, after which further extension ofwindow width gradually decreases the number ofstored patterns (i.e., resolved ambiguities).Retrieval.
The pronunciation of a word can be re-trieved by taking each grapheme of that word sepa-rately, and searching in the lookup table for a match-ing graphemic pattern.
First, the grapheme is lookedup in the 0-1-0 subtable.
If it does not match withany graphemic pattern stored in that table, the sin-gle grapheme pattern is extended to a 0-1-1 pattern.This procedure is then repeated until a matchingpattern with a minimal context is found, returninga 'certain' grapheme-phoneme apping.
After allgraphemes have been processed this way, the phone-mic mappings are concatenated to form the pronun-ciation of the word.An example of retrieving the pronunciation of aword by table lookup is given in Table 2.
As thisexample illustrates, the contexts needed for disam-biguating between output categories are generallyvery small.2The fact that Information Gain reflects this asym-metry, led us to a new, more generic and domain-independent, conceptualisation a d implementation ofthe Table Lookup method, in which context features axeordered according to their information gain, and patternsaxe stored in a single trie instead of in separate tables.3With considerable further compression when using atrie representation.48X 1000 patterns1086420?
............................................................"'" !/" / / / / / / / / / / /0-1,,fl 0-1-1 1.1-1 1-1-J 2-1-2 IB*14 ~-1,.,1 ~1-14 4-I,,4 4-1-6 $.I,4Jtr,=tzup tnt~l lFigure 3: Table magnitudes of lookup subtables.leftcontextaanbiedinfocusgraphemeright targetcontext phonemea anbb mi bed idii dngg_ rjTable 2: Example of the correct retrieval of the pro-nunciation of< aanbieding > (offer).
Each row con-tains an unambiguous pattern with minimal contextfound by the lookup algorithm.
Underscores repre-sent spaces.In case of unseen test words that contain graphemepatterns not present in the training set, the lookupalgorithm will not be able to retrieve that specificmapping?
This problem is handled in our model byadding to the lookup table a second table which con-tains all occurring raphemic patterns in the trainingset of a fixed window width (1-1-1), coupled withtheir most frequently occurring (default) phonemicmapping.
Whenever lookup table retrieval fails anda match can be found between the test pattern anda 1-1-1 default pattern, this default able provides a'best guess' which in many cases till turns out to becorrect?
To cover for those particular cases where nomatching can be found between the test pattern andthe 1-1-1 default patterns, a small third default ableis added to the model, containing for each graphemeits most frequently occurring phonemic mapping re-gardless of context (0-1-0), returning a 'final guess'.It is important to see that generalisation i  thisapproach arises from two different mechanisms: (i)the fact that spellings of different words contain iden-tical grapheme substrings, and (ii) the default ableswhich reflect probabilities of mappings in the train-ing set.
More sophisticated reasoning methods canbe used instead of the default able: at present we areinvestigating the consequences of substituting case-based reasoning such as implemented in IBL for thepresent default ables.3 Compar i son  to  A l te rnat iveApproachesTo get a better insight into the performance ofourdata-oriented generalisation methods, we comparedit to the performance of a 'classical' data-orientedmethod (backpropagation learning in feed-forwardnets), and to the linguistic knowledge-based ap-proach.3.1 An Alternat ive Data-Or ientedApproach: Connect ionismIn IBL, explicit use is made of similarity-based rea-soning.
A similarity metric is used to compare items,and the items most similar to a test item are taken asa basis for making a decision about the category ofthe latter.
Backpropagation learning in feedforwardconnectionist networks (BP), too, uses similarity (oranalogy), but more implicitly.
Again, an input pat-tern activates an output pattern which is similar tothe activation pattern of those items that are sim-ilar to the new item.
Complexity is added by thefact that an intermediate hidden layer of units "re-defines" similarity by extracting features from theactivation patterns of the input layer.
(In our ver-sion of IBL, the information gain metric is used toachieve a similar result).Automatic learning of grapheme-phoneme conver-sion of English (NETtalk, \[Sejnowski and Rosenberg,1987\]) has been acclaimed as a success tory for BP.The approach was replicated for Dutch in NetSpraak(\[Weijters and Hoppenbrouwers, 1990l).
It is there-"fore appropriate to compare our two data-orientedmodels to BP.
The main reason for experimentingwith BP for learning complex linguistic mappingsis that BP networks are alleged to be able to ex-tract generalisations and sub-generalisations fromtheir training data, as well as store exceptions tothese generalisations.
However, there are limitationsto BP network capabilities.
BP learning is not guar-anteed to converge to optimal performance (i.e.
itcan end up in local minima).
A consequence of thisis that Mthough amulti-layered network may be ablein principle to represent the solution to any mappingproblem, this property is not of much help becausethe designer of such a network is confronted witha large search space of variable network parameters(e.g., size of the hidden layer, learning rate, num-49ber of training cycles, connection weight initialisa-tion) which may affect learning and performance ofthe network considerably, but which cannot be deter-mined by rule.
Experimenters can therefore almostnever be sure that their results are optimal.We trained BP networks on grapheme-phonemeconversion using a training set which was restricted,for computational reasons, to all unique 2-1-3 map-pings present in the original training set, removingfrequency information, but also removing a large por-tion of redundancy inherent in the original trainingset.
The same test set was used as with IBL andthe table lookup model.
Graphemes were encodedby assigning each letter a randomly generated 6-bit string; phonemes were encoded locally, i.e., eachphoneme was represented by a 46-bit string with onlyone phoneme-specific bit 'on'.
Systematic simula-tions were used to approach optimal hidden layersize (60 units), learning rate (0.05) and momentum(0.4).
With these parameter settings, 5 networkswere randomly initialised (weights were assigned ran-dom values between -0.5 and 0.5) and trained for100 epochs 4.
The performance r sult listed belowwas calculated by averaging the performance scoresof these 5 networks on the test set.3.1.1 ResultsThe performance scores on randomly selected, un-seen test words show a surprising best score of thetable lookup model.
Both IBL and the table performbetter than BP.
Similar esults were obtained for dif-ferent training and test sets, except hat for smalltraining sets, table lookup performance dropped be-low that of IBL, but was still higher than that of BP.Model'BP 91.3Generalisation Accuracy on PhonemesIBL 93.4TABLE 95.13.2 The L ingu is t i c  Knowledge-BasedApproachThe traditional linguistic knowledge-based approachof grapheme-phoneme conversion has produced var-ious examples of combined rule-based and lexicon-based models, e.g.
MITALK (\[Allen et al, 1987\])for English, GRAFON-D (\[Daelemans, 1988\]), andMORPA-CUM-MORPHON (\[Nunn and Van Heuven,1993\]) for Dutch.
The developers of all of these mod-els shared the assumption that the presence of lin-guistic (phonotactic, morphological) knowledge ises-sential for a grapheme-phoneme model to perform ata reasonably high level.4All connect ion is t  s imulat ions were run on P laNet  5.6,a network simulator written by Yoshiro Miyata (ChukyoU., Japan).In MORPA-CUM-MORPHON (\[Nunn and VanHeuven, 1993\]), a state-of-the-art system for Dutch,grapheme-phoneme conversion is done in two steps.First, MORPA (\[Heemskerk and Van Heuven, 1993\])decomposes a word into a list of morphemes.
Thesemorphemes are looked up in a lexicon.
Each mor-pheme is associated with its category and a phone-mic transcription.
The phonemic transcriptions ofthe consecutive morphemes are concatenated to forman underlying phonemic representation f the word.MORPHON then applies a number of phonologicalrules to this underlying representation, deriving thesurface pronunciation of the word.In \[Nunn and Van Heuven, 1993\], test data isreferred to with which they evaluated their sys-tem.
We applied the table-lookup method to thesame test data in order to make a comparison pos-sible.
The test file consists of 1,971 words from var-ious sources: newspaper text, compounds, and low-frequency words.
High-frequency words, acronyms,and proper names were removed from the originaldata set.
As we did not try to solve the stress as-signment problem in this experiment, we had to re-comI)ute the error given by \[Nunn and Van Heuven,1993J such that stress assignment or syllable struc-ture errors were not taken into account s .The table lookup model was reconstructed onthe basis of the complete 70,000 word-pronunciationpairs in our Dutch corpus, resulting in a model con-taining 48,000 patterns (including default ables).3.2.1 ResultsWhen comparing the phonemisation accuracy ofthe linguistic knowledge-based approach in MORPA-CUM-MORPHON to the results on the same data bythe table method, we see that the table scores signif-icantly higher.ModelTABLEMORPA-CUM-MORPHONGeneralisation Accuracyon Words89.585.3In the knowledge-based approach, errors of mor-phological analysis (spurious ambiguity or no anal-ysis) account for a considerable amount of incorrectphoneme output (even after removal by \[Nunn andVan Heuven, 1993\] of proper names and other diffi-cult cases from the test set).
A new data-orientedversion of MORPA (\[Heemskerk, 1993\]) assigns a pri-ority ordering to the set of morphological decom-Sin a different set of experiments, we successfully ap-plied the IBL approach and two other data-oriented algo-rithms, analogical modeling and backprop, to the stressassignment problem (see \[Gillis et al, 1992\], \[Daelemanset al, 1993\], but we have not yet tried to combine thetwo tasks.50positions, based on a probabilistic grammar derivedfrom a corpus of examples of correct decompositions.This new approach raises the overall performanceof MORPA-CUM-MORPHON tO 88.7%, which remainsslightly worse than the table method.On the basis of an analysis of confusion matri-ces (misclassifications per grapheme), we find thatthe same types of errors are made by both systems,mainly on vowels (especially on the transcription ofgrapheme <e>), but less frequently by the tablemethod.
E.g.
an intended /~/ was assigned cate-gory/~/  112 times by MOttPA-CUM-MORPHON, andonly 23 times by the table method.
Another dif-ference is that while confusions by the table methodare symmetric, onfusions in MORPA-CUM-MORPHONseem to be directed (e.g.
an intended /~/ i s  oftenmisclassified as/E/ ,  but almost never the other wayround).4 D iscuss ion4.1 Related WorkAs mentioned earlier, Instance-Based Learning is aform of case-based reasoning: a set of exemplars(cases) and a similarity metric are used to make de-cisions about unseen cases.
Earlier work on the ap-plication of Memory-Based Reasoning (\[Stanfill andWaltz, 1986; Stanfill, 1987\]) (another form of case-based reasoning) to the phonemisation problem usingthe NetTalk data (MBRTalk), showed a better per-formance than NetTalk itself (\[Sejnowski and Rosen-berg, 1987\]), however at the cost of an expensive,domain-dependent computational measure of dissim-ilarity that seems to be computationally feasible onlywhen working on a massive parallel computer like theConnection Machine.
The Information Gain metricwe use in our version of IBL is domain-independentand can be efficiently computed.
Another case-basedsystem (or rather a hybrid combination ofcase-basedreasoning and relaxation in a localist interactive ac-tivation etwork) is PRO (\[Lehnert, 1987\]).
The re-ported performance of this system is not very con-vincing, however, neither is the need for a combina-tion of connectionist and case-based techniques ap-parent.
Dietterich and Bakiri (\[1991\]) systematicallycompared the performance of ID3 (\[Quinlan, 1986\])and BP on the NetTalk data.
Their conclusion isthat BP consistently outperforms ID3 because theformer captures tatistical information that the lat-ter does not.
However, they demonstrate hat ID3can be extended to capture this statistical informa-tion.
Dietterich and Bakiri suggest that there is stillsubstantial room for improvement in learning meth-ods for text-to-speech mapping, and it is indeed thecase that our approach significantly outperforms BP.The application of compression techniques like ourtable method to the phonemisation problem has notyet been reported on as such in the literature.
In\[Golding and Rosenbloom, 1991\], the interaction ofrule-based reasoning and case-based reasoning inthe task of pronouncing surnames is studied.
Itis claimed that a hybrid approach is preferable, inwhich the output of the rules is used unless a com-pelling analogy exist in the case-base.
If a compellinganalogy is found, it overrides the rule output.
In thisapproach, the (hand-crafted) rules are interpreted asimplementing the defaults, and the cases the pocketsof exceptions.
Our table-method works along a differ-ent dimension: both default mappings and pocketsof exceptions are represented in both the table andthe default mapping (which as we suggested earlier,we have replaced in the current version by case-basedreasoning).
Certain mappings are present in the ta-ble (which can be interpreted as an automatically ac-quired rule set), and uncertain cases are handled bythe case-base (or default mapping).
Future researchshould make clearer how these two uses of case-basedreasoning are related and whether the strengths ofboth can be combined.4.2 Multi-lingual Grapheme-to-PhonemeConversionTo test the reusability of our algorithms tographeme-to-phoneme conversion in different lan-guages, we applied the table lookup approach to En-glish and French data.
For English, we used thebenchmark NetTalk corpus as used in \[Sejnowski andRosenberg, 1987\].
For French, we extracted word-pronunciation pairs from Brulex, a lexical data basefor French (\[Content et ai., 1990\]).
Roughly similarmodels resulted, proving reusability of technique.In order to be able to compare these models tothe Dutch model described in this paper, we selectedfor each language a comparable data base containing20,000 word-pronunciation pairs, and divided thesedata sets in test sets of 1,500 pairs and trainingsets of 18,500 pairs.
Figure 4 displays the result-ing lookup tables, including the Dutch lookup tabledisplayed earlier in Figure 3.Given the constant lookup table construction pro-cedure and the similar data sets, two interesting dif-ferences between the three models emerge.
Firstly,as can be seen clearly from Figure 4, there are sig-nificant differences between the magnitudes of themodels.
After expansion to 5-1-5 width, the Frenchmodel contains 18,000 patterns, the Dutch 27,000and the English 35,000, reflecting differences in deep-ness of orthography between the three languages.Secondly, as the labels on the 2-1-3 subtable barsin Figure 4 indicate, the performance accuracy ofthe English model ags behind that of the Dutch andFrench model.
Final performance accuracy on thetest set, with the inclusion of default ables, is 98.2%for the French model, 97.0% for the Dutch modeland 90.1% for the English model, again reflecting asignificant difference as regards deepness of orthog-51x 1000 paRems19108i6420 0-14 8-1-1 1-1-1 1-1-| ~l J /  2-1-t $-1-$ $-1-4 4,14 4-14 $-1-45BB EnglIsh I\[\] Omch\[\] FrenchFigure 4: Table magnitudes of subtables of English,Dutch and French models.
For each language, perfor-mance accuracy up to the 2-1-3 subtable is displayedon the 2-1-3 subtable bar.raphy between English on the one hand and Dutchand French on the other hand.5 Conc lus ionIn computational linguistics, one of the common-sense beliefs is that the performance of a system solv-ing a linguistic problem improves with the amountof linguistic knowledge and sophistication i corpo-rated into it.
We have shown that at least for onelinguistic task, this is not the case.
The linguisticallyleast informed method (compression fa training setinto a table, complemented with a rudimentary formof probabilistic reasoning) performed better on un-seen input than a linguistically sophisticated, state-of-the-art knowledge-based system.
We have reasonto believe that this also applies to other linguisticcategorisation problems where superficial input fea-tures and local context solve most ambiguities (wehave positive results on stress assignment, \[Gillis etal., 1992; Daehmans et al, 1993\], and part of speechtagging).The data-oriented algorithms described are sim-ple and domain-independent, and introduce a newkind of reusability into computational linguistics:reusability of the acquisition method (on differentdata sets) rather than reusability of (hand-coded)knowledge in different applications or formalisms.The former type of reusability seems to be easier toachieve than the latter.AcknowledgementsThanks to Henk Kempff (ITK), Alain Content(ULB), and Terrence Sejnowski (UCSD) for mak-ing available for research purposes the Dutch, Frenchand English data we used.
We are also gratefulto Josee Heemskerk, Anneke Nunn, Gert Durieux,Steven Gillis, Ton Weijters and participants of theCLIN'92-meeting in Tilburg for comments and ideas.References\[Aha et al, 1991\] D. Aha, D. Kibler and M. Albert.Instance-Based Learning Algorithms.
MachineLearning, 6, 37-66, 1991.\[Allen et al, 1987\] J. Allen, S. Hunnicutt, andD.
Klatt.
From text to speech: the MITaik system.Cambridge, UK: Cambridge University Press.\[Content et al, 1990\] A.
Content, P. Mousty, andM.
l~deau.
Brulex: une base de donn~es lexi-tales informatis6e pour le francais 6crit et purl6.L'Ann~e Psychologique, 90, 551-566, 1990.\[Daelemans, 1988\] W. Daelemans.
GRAFON: AGrapheme-to-phoneme Conversion System forDutch.
In Proceedings Twelfth International Con-ference on Computational Linguistics (COLING-88), Budapest, 133-138, 1988.\[Daelemans and Van den Bosch, 1992\]W. Daelemans and A. van den Bosch.
Generaliza-tion performance of backpropagation learning on asyllabification task.
In M. Drossaers and A.
Nijholt(Eds.
), Proceedings of the 3rd Twente Workshopon Language Technology.
Enschede: UniversiteitTwente, 27-37, 1992.\[Daelemans et al, 1993\]W. Daelemans, A. van den Bosch, S. Gillis andG.
Durieux.
A data-driven approach to stress ac-quisition.
In Proceedings of the ECML workshopon ML techniques for Te~t Analysis, Vienna, 1993.\[Derwing and Skousen, 1989\] B. L. Derwing andIt.
Skousen.
Real time morphology: symbolic rulesor analogical networks.
Berkeley Linguistic Soci-ety, 15: 48-62, 1989.\[DevijverandKittler, 1982\] P. A. Devijver andJ.
Kittler.
Pattern Recognition.
A Statistical Ap-proach.
London: Prentice-Hall, 1982.\[Dietterich and Bakiri, 1991\] T. G. Dietterich andG.
Bakiri.
Error-correcting output codes: a gen-eral method for improving multiclass inductivelearning programs.
Proceedings AAAI-91, MenloPark, CA, 572-577, 1991.\[Gillis et at., 1992\] S. Gillis, G. Durieux, W. Daele-roans and A. van den Bosch.
Exploring artificiallearning algorithms: learning to stress Dutch sim-plex words.
Antwerp Papers in Linguistics, 71,1992.\[Golding and Rosenbloom, 1991\] A. R. Golding andP.
S. l~osenbloom.
Improving rule-based sys-tems through Case-Based Reasoning.
ProceedingsAAAI-91, Menlo Park, CA, 22-27, 1991.52\[Heemskerk and Van Heuven, 1993\]J. Heemskerk and V. J. van Heuven.
MORPA, alexicon-based MORphological PArser.
In V.J.
vanHeuven and L.C.W.
Pols (Eds.
), Analysis and syn-thesis of speech; strategic research towards high-quality text-to-speech generation.
Berlin: Moutonde Gruyter, 1993.\[Heemskerk, 1993\] J. Heemskerk.
A probabilisticcontext-free grammar for disambiguation in mor-phological parsing.
In Proceedings EACL-93,Utrecht, 1993.\[Lehnert, 1987\] W. Lehnert.
Case-based problemsolving with a large knowledge base of learnedcases.
In Proceedings AAAI-87, Seattle, WA, 1987.\[Nunn and Van Heuven, 1993\] A. Nunnand V. J. van Heuven.
MORPHON, lexicon-based text-to-phoneme conversion and phonolog-ical rules.
In V.J.
van Heuven and L.C.W.
Pols(Eds.
), Analysis and synthesis of speech; strategicresearch towards high-quality text-to-speech gener-ation.
Berlin: Mouton de Gruyter, 1993.\[Quinlan, 1986\] J. R. Quinlan.
Induction of DecisionTrees.
Machine Learning, 1, 81-106, 1986.\[Riesbeck and Schank, 1989\] C. K. Riesbeck andR.
S. Schank.
Inside case based reasoning.
Hills-dale, NJ: Lawrence Earlbaum Assoc., 1989.\[Sejnowski and Rosenberg, 1987\] T. J. Sejnowskiand C. R. Rosenberg.
Parallel networks that learnto pronounce English text.
Complex Systems, 1,145-168, 1987.\[Smith and Media, 1981\] E. E. Smith andD.
L. Medin Categories and concepts.
Cambridge,MA: Harvard University Press, 1981.\[Stanfill and Waltz, 1986\]C. W. Stanfill and D. Waltz.
Toward memory-based reasoning.
Communications of the ACM,29:12, 1213-1228, 1986.\[Stanfill, 1987\] C. W. Stanfill.
Memory-based rea-soning applied to English pronunciation.
Proceed-ings AAAI-87, Seattle, WA, 577-581, 1987.\[Weijters and Hoppenbrouwers, 1990\] A. Wei-jters and G. Hoppenbrouwers.
NetSpraak: eenneuraal netwerk voor grafeem-foneem-omzetting.Tabu, 20:1, 1-25, 1990.\[Weijters, 1991\] A. Weijters.
Analyse van het pa-troonherkennend vermogen van NETtalk.
In J.Treur (Ed.
), NAIC'#I Proceedings, 249-260, 1991.53
