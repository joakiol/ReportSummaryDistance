Proceedings of ACL-IJCNLP 2015 System Demonstrations, pages 13?18,Beijing, China, July 26-31, 2015.c?2015 ACL and AFNLPIn-tool Learning for Selective Manual Annotation in Large CorporaErik-L?an Do Dinh?, Richard Eckart de Castilho?, Iryna Gurevych??
?Ubiquitous Knowledge Processing Lab (UKP-TUDA)Department of Computer Science, Technische Universit?at Darmstadt?Ubiquitous Knowledge Processing Lab (UKP-DIPF)German Institute for Educational Research and Educational Informationhttp://www.ukp.tu-darmstadt.deAbstractWe present a novel approach to the selec-tive annotation of large corpora throughthe use of machine learning.
Linguis-tic search engines used to locate potentialinstances of an infrequent phenomenondo not support ranking the search re-sults.
This favors the use of high-precisionqueries that return only a few results overbroader queries that have a higher recall.Our approach introduces a classifier usedto rank the search results and thus help-ing the annotator focus on those resultswith the highest potential of being an in-stance of the phenomenon in question,even in low-precision queries.
The clas-sifier is trained in an in-tool fashion, ex-cept for preprocessing relying only on themanual annotations done by the users inthe querying tool itself.
To implementthis approach, we build upon CSniper1, aweb-based multi-user search and annota-tion tool.1 IntroductionWith the rapidly growing body of digitally avail-able language data, it becomes possible to investi-gate phenomena of the language system that man-ifest themselves infrequently in corpus data, e.g.non-canonical constructions.
To pinpoint occur-rences of such phenomena and to annotate themrequires a new kind of annotation tool, since man-ual, sequential annotation is not feasible anymorefor large amounts of texts.An annotation-by-query approach to identifysuch phenomena in large corpora is implemented1https://dkpro.github.io/dkpro-csniperin the recently published open-source tool CSniper(Eckart de Castilho et al., 2012).To enable a selective manual annotation pro-cess, a linguistic search engine is used, allowingthe creation of queries which single out potentialinstances of the phenomenon in question.
Thosepotential instances are then displayed to the user,who annotates each one as being an instance ofthe phenomenon or not.
This process of search-ing and annotating can be performed by multipleusers concurrently; the annotations are stored foreach user separately.
In a subsequent evaluationstep, a user can review the annotations of all users,e.g.
to discard a query if it yields unsatisfying re-sults.
Finally, the annotations of multiple users canbe merged into a gold standard.QueryAnnotateRankEvaluateAggregatereviewassessmentsrefinequeryFigure 1: Annotation-by-query workflow extendedwith a ranking step.This approach relieves the annotator from hav-ing to read through the corpus from the beginningto the end to look for instances of a phenomenon.However, the search may yield many results thatmay superficially appear to be an instance of thedesired phenomenon, but due to ambiguities ordue to a broadly defined query only a small sub-set may be actual instances.
This still leaves theannotator with the tedious task of clicking throughthe search results to mark the true instances.13To reduce the time and effort required, wepresent an extension of the annotation-by-queryapproach (Figure 1) that introduces a ranking ofthe query results (Section 2) by means of machinelearning; we order the results by confidence ofthe used classifier.
To obtain a model for theclassifier, we employ an in-tool learning approach,where we learn from the annotations that are madeby users in the tool itself.
This makes our rankingapproach useful for highly specific tasks, since nopre-trained models are needed.Finally we demonstrate the viability of our con-cept by the example task of finding non-canonicalconstructions in Section 3.2 Ranking linguistic query resultsOur approach employs machine learning to facili-tate ?
but not to completely replace ?
the man-ual annotation of query results.
A query expressesthe intention of the user to find a specific linguis-tic phenomenon (information need).
An infor-mation retrieval search engine would provide theuser with a list of results that are ranked accord-ing to their relevance, fulfilling the informationneed.
However, linguistic search engines such asCQP (Evert and Hardie, 2011) ?
which is usedby CSniper ?
are basically pattern-matching en-gines, operating on different lexical or morpho-syntactic features like part-of-speech (POS) tagsand lemmata and do not have a concept of rele-vance.
Thus, if the query provided by the userovergeneralizes, relevant results are hidden amongmany irrelevant ones, ultimately failing to satisfythe user?s information need.To tackle this problem, we use the annotationsalready created by users on the search results totrain a classifier.
Unannotated query results arethen fed to the classifier whose output values arethen used as relevance ratings by which the resultsare ranked.
The classifier producing the rankingcan be invoked by the user at any time; it can beconfigured in certain characteristics, e.g.
the an-notations of which users should be used as train-ing data, or how many of the selected users haveto agree on an annotation for it to be included.2.1 Workflow and ranking process inCSniperCurrently, we train the classifier on features de-rived from the constituency parse tree, whichmakes it useful for tasks such as locating sen-tences containing infrequent ambiguous grammat-ical constructions (cf.
Section 3).
Since parsingthe query results is too time-intensive to be doneduring runtime, we parsed the corpora in advanceand stored the parse trees in a database.
To trainthe classifier, we employed SVM-light-tk (Mos-chitti, 2006; Joachims, 1999), a support vector ma-chine implementation which uses a tree kernel tointegrate all sub-trees of the parse tree as features.Consider the following typical scenario incor-porating the ranking: A user constructs a querybased on various features, such as POS tags orlemmata, which are used to search for matchingsentences, e.g.?It?
[lemma=?be?]
[pos=?AT0?]?[pos=?NN.*?
]2The result is a list of sentences presented in akeywords-in-context (KWIC) view, along with anannotation field (Figure 2).Then the user starts to annotate these sentencesas Correct or Wrong, depending whether theytruly represent instances of the phenomenon inquestion.
Clicking on the Rank results button(Figure 2) invokes our ranking process: The SVM-light-tk classifier is trained using the parse trees ofthe sentences which the user previously annotated.The resulting model is then used to classify the re-maining sentences in the query results.
We rankthe sentences according to the output value of thedecision function of the classifier (which we in-terpret as a relevance/confidence rating) and tran-siently label a sentence as either (Correct) (outputvalue > 0) or (Wrong) (output value ?
0).
The re-sults in the KWIC view are then reordered accord-ing to the rank, showing the highest-ranking resultfirst.
Repeatedly annotating those highest-rankingresults and re-ranking allows for quickly annotat-ing instances of the phenomenon, while also im-proving the classifier accuracy at the same time.2.2 Find modeAfter annotating instances based on simple queriesand ML-supported ranked queries, we consideredthe natural next step to be searching automaticallyfor the phenomenon in question utilizing machinelearning, using arbitrary sentences from the cor-pus as input for the classifier instead of only theresults returned by a query.
Such an automaticsearch could address two concerns: 1) it removes2It-cleft example query: ?It?, followed by a form of ?tobe?, an optional determiner and a common noun.14Figure 2: A screenshot showing the results table after the ranking process, with sentences sorted byconfidence of the classifier (Score).
The results are shown in a keywords-in-context (KWIC) view, sepa-rating left context, query match and right context (within a range of one sentence).
Clicking on (Correct)changes the label to Correct.the need for the user to design new queries, al-lowing users less experienced in the query lan-guage to annotate more effectively side-by-sidewith advanced users; 2) it could optimally gener-alize over all the queries that users have alreadymade and potentially locate instances that had notbeen found by individual high-precision queries.To support this, we implemented the Findmode, to locate instances of a phenomenon whileabstracting from the queries.
In this mode, theSVM is first trained from all previously (manu-ally) labeled instances for a given phenomenon,without taking the queries into account that wereused to find those instances.
Then the corpus ispartitioned into smaller parts containing a prede-fined amount of sentences (we used 500).
Oneof these partitions is chosen at random, and thesentences therein are ranked using the SVM.
Thisstep is repeated, until a previously defined num-ber of sentences have been classified as Correct.Those sentences are then shown to the user, whonow can either confirm a sentence as containingthe phenomenon or label it Wrong otherwise.2.3 Related workExisting annotation tools include automationfunctionality for annotation tasks, ranging fromrule-based tagging to more complex, machine-learning-based approaches.Such functionalities can be found in the anno-tation software WordFreak (Morton and LaCivita,2003), where a plug-in architecture allows for avariety of different taggers and classifiers to be in-tegrated, for example part-of-speech taggers or co-reference resolution engines.
Those require pre-trained models, which limits the applicability ofthe automation capabilities of WordFreak to tasksfor which such models are actually available.
Inaddition to assigning annotations a single label,WordFreak allows plugins to rank labels for eachannotation based on the confidence of the usedclassifier.
Note that this is different to our rank-ing approach, where we instead perform a rankingof the search results which shall be annotated.Another tool incorporating machine learning isWebAnno (Yimam et al., 2014), which imple-ments features such as custom labels and anno-tation types.
In addition, WebAnno supports au-tomatic annotation similar to our approach, alsoemploying machine learning to build models fromthe data annotated by users.
Those models are thenused to annotate the remainder of the documents.To accomplish this, WebAnno uses a split-paneview, showing automatic suggestions in one paneand manually entered annotations in another.
Theuser can accept a suggested annotation, which istransferred to the manual pane.
Lacking the searchcapability, WebAnno lists automatic annotationsin the running corpus text, which makes it unsuitedfor selective annotation in large corpora.
The ap-proach that we implemented on top of CSniper in-stead ranks the search results for a given query byconfidence of the classifier.Yet another form of in-tool learning is activelearning, as is implemented, e.g., in Dualist (Set-tles, 2011).
In an active learning scenario the sys-tem aims to efficiently train an accurate classifier(i.e.
with as little training data as possible) andthus repeatedly asks the user to annotate instancesfrom which it can learn the most.
Such an ap-proach can work well for reducing the amount oftraining data needed to produce a model whichachieves high accuracy, as has been ?
amongstothers ?
shown by Hachey et al.
(2005).
How-ever, they also learn in their experiments that thosehighly informative instances are often harder toannotate and increase required time and effort ofannotators.
Our approach is different from active15learning as our goal is not to improve the trainingefficiency of the classifier but rather to allow theuser to interactively find and label as many trueinstances of a phenomenon as possible in a largecorpus.
Thus, the items presented to the user arenot determined by the expected information gainfor the classifier but rather by the confidence of theclassifier, presenting the user with those instancesfirst which are most likely to be occurrences of thephenomenon in question.3 Case study: Finding non-canonicalconstructionsWe demonstrate our novel approach on the task oflocating non-canonical constructions (NCC) andconduct an intrinsic evaluation of the accuracyof the system augmented with machine learningoutput on the data annotated by expert linguists.The linguists annotated sentences for occurrencesof certain NCC subtypes: information-packagingconstructions (Huddleston and Pullum, 2002, pp.1365ff.
), which present information in a differ-ent way from their canonical counterparts withoutchanging truth conditions; specifically It-clefts (?Itwas Peter who made lunch.?
), NP-preposing (?Atreasure, he was searching.?
), and PP-inversion(?To his left lay the forest.?)
clauses.For our experiments, we used the British Na-tional Corpus (2007), comprising 100 millionwords in multiple domains3.
Constituency pars-ing was conducted using the factored variant ofthe Stanford Parser (Klein and Manning, 2003),incorporated into a UIMA pipeline using DKProCore (Eckart de Castilho and Gurevych, 2014).As a baseline we use queries representing theexperts?
intuition about the realization of theNCCs in terms of POS tags and lemmata.
Weshow that our system improves the precision of thequery results even with little training data.
Alsowe present run times for our ranking system un-der real-world conditions for different training setsizes.
Further, we compare Krippendorff?s ?
co-efficient as an inter-annotator agreement measureamong only annotators to the ?
which treats oursystem as one additional annotator.We conducted the experiments based on themanually assigned labels of up to five annota-tors.
If a sentence has been annotated by multiple3CSniper and the used SVM implementation are languageindependent, which allowed us to also run additional prelim-inary tests using German data.users, we use the label that has been assigned bythe majority; in case of a tie, we ignore the sen-tence.
These so created gold standard annotationswere used in an iterative cross-validation setting:for each query and the corresponding annotatedsentences we ran nine cross-validation configura-tions, ranging from a 10/90 split between trainingand testing data to a 90/10 split, to investigate thereliability of the classifier as well as its ability toachieve usable results with little training data.For It-clefts, we observe that elaborate queriesalready have a high precision, on which the SVMimproves only marginally.
The query?It?
/VCC[] [pos=?NP0?
]+ /RC[]4(it17)already yields a precision of 0.9598, which doesnot increase using our method (using 10% as train-ing data, comparing the precision for the remain-ing 90%).
However, while broader queries yieldlower precision, the gain by using the SVM be-comes significant (Table 1), as exemplified by theprecision improvement from 0.4919 to 0.7782 forthe following It-cleft query, even at a 10/90 split.?It?
/VCC[] /NP[] /RC[]5(it2)For other inspected types of NCC, even elaboratequeries yield a low baseline precision, which ourapproach can improve significantly.
This effectcan be observed for example in the following NP-preposing query, where precision can be improvedfrom 0.3946 to 0.5871.[pos=?N.*?
]{1,2} [pos=?PNP?
& word!=?I?][pos=?V.*?
]6(np55)We conducted a cursory, ?real-world?
test re-garding the speed of the ranking system.7Trainingthe SVM on differently sized subsets of the 449sentences returned by a test query, we measuredthe time from clicking the Rank results button untilthe process was complete and the GUI had updatedto reorder the sentences (i.e.
including databasequeries, training, classifying, GUI update).
Theprocess times averaged over five ?runs?
for eachtraining set size (20%, 50% and 90%) amount to 5seconds, 7 seconds, and 14 seconds respectively.This leaves us with the preliminary impressionthat our system is fast enough for small to medium4?It?, verb clause, one or more proper nouns, relativeclause.
VCC, NC, and RC are macros we defined in CQP,see Table 2.5?It?, verb clause, noun phrase, relative clause.6One to two nouns, personal pronoun other than ?I?, verb.7System configuration: Intel i5 2,4 GHz, 2GB RAM, SSD3GB/s, Linux in a VM16it2 it17 it33 np34 np55 np76 pp42 pp99 pp103Baseline 0.4919 0.9598 0.7076 0.4715 0.3946 0.4985 0.7893 0.4349 0.2365SVM, 10/90 0.7782 0.9598 0.7572 0.5744 0.5871 0.5274 0.8152 0.8357 0.8469SVM, 50/50 0.8517 0.9608 0.8954 0.6410 0.6872 0.6193 0.8657 0.8769 0.8720SVM, 90/10 0.8634 0.9646 0.9261 0.6822 0.7723 0.6806 0.8865 0.8820 0.8796Table 1: Precision for various NCC queries (Baseline) and for using the SVM with 10%, 50% and 90%training data.sized training sets; as the last result suggests, forlarger sets it would be desirable for our system tobe faster overall.
One way to achieve this is topre-compute the feature vectors used in the train-ing phase once ?
this could be done at the sametime with the parsing of the sentences, i.e.
at thesetup time of the system.Krippendorff?s ?, an inter-annotator agreement(IAA) measure which usually assumes values be-tween 0 (no reliable agreement) and 1 (perfectagreement), amounts to 0.8207 averaged over allmanually created It-cleft annotations.
If we inter-pret the SVM as an additional annotator (?svm),the IAA drops to 0.5903.
At first glance thisseems quite low, however upon closer inspectionthis can be explained by an overfitting of the clas-sifier.
This effect occurs for the already precisebaseline queries, where in some cases less than5% of the query results were labeled as Wrong.The same holds for NP-preposing (?
: 0.6574,?svm: 0.3835) and PP-inversion (?
: 0.9410,?svm: 0.6964).
We interpret this as the classifierbeing successful in helping the annotators after abrief training phase identifying additional occur-rences of particular variants of a phenomenon ascovered by the queries, but not easily generalizingto variants substantially different from those cov-ered by the queries.4 ConclusionWith automatic ranking we introduced an exten-sion to the annotation-by-query workflow whichfacilitates manual, selective annotation of largecorpora.
We explained the benefits of in-toollearning to this task and our extension of an open-source tool to incorporate this functionality.
Fi-nally, we showed the applicability of the conceptand its implementation to the task of finding non-canonical constructions.For future work, we plan to speed up the learn-ing process (e.g.
by saving feature vectors insteadof re-calculating them), and also add the abilityfor users to configure the features used to train theclassifier, e.g.
incorporating lemmata or namedentities instead of only using the parse tree.
In-tegrating such configuration options in an easilyunderstandable and user-friendly fashion may notbe trivial but can help to generalize the approachto support additional kinds of sentence level anno-tation.AcknowledgementsWe would like to thank Pia Gerhard, SabineBartsch, Gert Webelhuth, and Janina Rado for an-notating and testing.
Furthermore we would liketo thank Janina Rado for creating the CQP macrosused in the tests.This work has been supported by the Ger-man Federal Ministry of Education and Re-search (BMBF) under the promotional reference01UG1416B (CEDIFOR), by the German Insti-tute for Educational Research (DIPF) as part ofthe graduate program ?Knowledge Discovery inScientific Literature?
(KDSL), and by the Volk-swagen Foundation as part of the Lichtenberg-Professorship Program under grant No.
I/82806.ReferencesRichard Eckart de Castilho and Iryna Gurevych.
2014.A broad-coverage collection of portable NLP com-ponents for building shareable analysis pipelines.In Proceedings of the Workshop on OIAF4HLT atCOLING 2014, pages 1?11.Richard Eckart de Castilho, Iryna Gurevych, andSabine Bartsch.
2012.
CSniper: Annotation-by-query for Non-canonical Constructions in LargeCorpora.
In Proceedings of ACL 2012, SystemDemonstrations, pages 85?90, Stroudsburg, PA,USA.
ACL.Stefan Evert and Andrew Hardie.
2011.
Twenty-firstcentury corpus workbench: Updating a query archi-tecture for the new millennium.
In Proceedings ofCL2011, Birmingham, UK.17Shortcut ExpansionVCC ([pos=?VBB?
| pos=?VBD?
| pos=?VBZ?
]* [lemma=?be?])
|([pos=?V.*?
]* [pos=?VBG?
| pos=?VBI?
| pos=?VBN?
]* [lemma=?be?
])NP [pos=?AT0?]?
[]?
[pos=?AJ.*?
]* [pos=?N.*?
]RC ([pos=?DTQ?
| pos=?PNQ?
| pos=?CJT?]
/VCF[] []?)
|([pos=?CJT?]?
/NP[] /VCF[] []?)
|([pos=?PR.*?
]* [pos=?.Q?]
/NP[] /VCF[] []?
)VCF [pos=?V.?B?
| pos=?V.?D?
| pos=?V.?Z?
| pos=?VM0?]
[pos=?V.*?
]*Table 2: CQP macro expansions for self-defined macros.
BNC uses the CLAWS5 tagset for POS tags(http://www.natcorp.ox.ac.uk/docs/c5spec.html).Ben Hachey, Beatrice Alex, and Markus Becker.
2005.Investigating the Effects of Selective Sampling onthe Annotation Task.
In Proceedings of CoNLL2005, pages 144?151, Stroudsburg, PA, USA.
ACL.Rodney D. Huddleston and Geoffrey K. Pullum.
2002.The Cambridge Grammar of the English Language.Cambridge University Press.Thorsten Joachims.
1999.
Making large-scale sup-port vector machine learning practical.
In BernhardSch?olkopf, Christopher J. C. Burges, and Alexan-der J. Smola, editors, Advances in Kernel Methods,pages 169?184.
MIT Press, Cambridge, MA, USA.Dan Klein and Christopher D. Manning.
2003.
Accu-rate Unlexicalized Parsing.
In Proceedings of ACL2003, pages 423?430, Stroudsburg, PA, USA.
ACL.Thomas Morton and Jeremy LaCivita.
2003.WordFreak: An open tool for linguistic annota-tion.
In Proceedings of NAACL HLT 2003, NAACL-Demonstrations, pages 17?18, Stroudsburg, PA,USA.
ACL.Alessandro Moschitti.
2006.
Making Tree KernelsPractical for Natural Language Learning.
In Pro-ceedings of EACL 2006, pages 113?120, Trento,Italy.Burr Settles.
2011.
Closing the Loop: Fast, Inter-active Semi-supervised Annotation with Queries onFeatures and Instances.
In Proceedings of EMNLP2011, pages 1467?1478, Stroudsburg, PA, USA.ACL.The British National Corpus, version 3 (BNC XMLEdition).
2007.
Distributed by Oxford UniversityComputing Services on behalf of the BNC Consor-tium.
URL: http://www.natcorp.ox.ac.uk/.Seid Muhie Yimam, Richard Eckart de Castilho, IrynaGurevych, and Chris Biemann.
2014.
AutomaticAnnotation Suggestions and Custom AnnotationLayers in WebAnno.
In Proceedings of ACL 2014,System Demonstrations, pages 91?96, Stroudsburg,PA, USA.
ACL.18
