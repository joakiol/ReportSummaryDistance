Proceedings of the 6th Workshop on Statistical Machine Translation, pages 309?315,Edinburgh, Scotland, UK, July 30?31, 2011. c?2011 Association for Computational LinguisticsLIMSI @ WMT11Alexandre AllauzenHe?le`ne Bonneau-MaynardHai-Son LeAure?lien MaxGuillaume WisniewskiFranc?ois YvonUniv.
Paris-Sud and LIMSI-CNRSB.P.
133, 91403 Orsay cedex, FranceGilles AddaJosep M. CregoAdrien LardilleuxThomas LavergneArtem SokolovLIMSI-CNRSB.P.
133, 91403 Orsay cedex, FranceAbstractThis paper describes LIMSI?s submissions tothe Sixth Workshop on Statistical MachineTranslation.
We report results for the French-English and German-English shared transla-tion tasks in both directions.
Our systemsuse n-code, an open source Statistical Ma-chine Translation system based on bilingualn-grams.
For the French-English task, we fo-cussed on finding efficient ways to take ad-vantage of the large and heterogeneous train-ing parallel data.
In particular, using a sim-ple filtering strategy helped to improve bothprocessing time and translation quality.
Totranslate from English to French and Ger-man, we also investigated the use of theSOUL language model in Machine Trans-lation and showed significant improvementswith a 10-gram SOUL model.
We also brieflyreport experiments with several alternatives tothe standard n-best MERT procedure, leadingto a significant speed-up.1 IntroductionThis paper describes LIMSI?s submissions to theSixth Workshop on Statistical Machine Translation,where LIMSI participated in the French-English andGerman-English tasks in both directions.
For thisevaluation, we used n-code, our in-house StatisticalMachine Translation (SMT) system which is open-source and based on bilingual n-grams.This paper is organized as follows.
Section 2 pro-vides an overview of n-code, while the data pre-processing and filtering steps are described in Sec-tion 3.
Given the large amount of parallel data avail-able, we proposed a method to filter the French-English GigaWord corpus (Section 3.2).
As in ourprevious participations, data cleaning and filteringconstitute a non-negligible part of our work.
Thisincludes detecting and discarding sentences in otherlanguages; removing sentences which are also in-cluded in the provided development sets, as well asparts that are repeated (for the monolingual newsdata, this can reduce the amount of data by a fac-tor 3 or 4, depending on the language and the year);normalizing the character set (non-utf8 characterswhich are aberrant in context, or in the case of theGigaWord corpus, a lot of non-printable and thus in-visible control characters such as EOT (end of trans-mission)1).For target language modeling (Section 4), a stan-dard back-off n-gram model is estimated and tunedas described in Section 4.1.
Moreover, we also in-troduce in Section 4.2 the use of the SOUL lan-guage model (LM) (Le et al, 2011) in SMT.
Basedon neural networks, the SOUL LM can handle anarbitrary large vocabulary and a high order marko-vian assumption (up to 10-gram in this work).
Fi-nally, experimental results are reported in Section 5both in terms of BLEU scores and translation editrates (TER) measured on the provided newstest2010dataset.2 System OverviewOur in-house n-code SMT system implements thebilingual n-gram approach to Statistical MachineTranslation (Casacuberta and Vidal, 2004).
Given a1This kind of characters was used for Teletype up to the sev-enties or early eighties.309source sentence sJ1, a translation hypothesis t?I1 is de-fined as the sentence which maximizes a linear com-bination of feature functions:t?I1 = argmaxtI1{M?m=1?mhm(sJ1, tI1)}(1)where sJ1 and tI1 respectively denote the source andthe target sentences, and ?m is the weight associatedwith the feature function hm.
The translation fea-ture is the log-score of the translation model basedon bilingual units called tuples.
The probability as-signed to a sentence pair by the translation model isestimated by using the n-gram assumption:p(sJ1, tI1) =K?k=1p((s, t)k|(s, t)k?1 .
.
.
(s, t)k?n+1)where s refers to a source symbol (t for target) and(s, t)k to the kth tuple of the given bilingual sentencepair.
It is worth noticing that, since both languagesare linked up in tuples, the context information pro-vided by this translation model is bilingual.
In ad-dition to the translation model, eleven feature func-tions are combined: a target-language model (seeSection 4 for details); four lexicon models; two lex-icalized reordering models (Tillmann, 2004) aim-ing at predicting the orientation of the next transla-tion unit; a ?weak?
distance-based distortion model;and finally a word-bonus model and a tuple-bonusmodel which compensate for the system preferencefor short translations.
The four lexicon models aresimilar to the ones used in a standard phrase-basedsystem: two scores correspond to the relative fre-quencies of the tuples and two lexical weights areestimated from the automatically generated wordalignments.
The weights associated to feature func-tions are optimally combined using a discriminativetraining framework (Och, 2003) (Minimum ErrorRate Training (MERT), see details in Section 5.4),using the provided newstest2009 data as develop-ment set.2.1 TrainingOur translation model is estimated over a trainingcorpus composed of tuple sequences using classi-cal smoothing techniques.
Tuples are extracted froma word-aligned corpus (using MGIZA++2 with de-fault settings) in such a way that a unique segmenta-tion of the bilingual corpus is achieved, allowing toestimate the n-gram model.
Figure 1 presents a sim-ple example illustrating the unique tuple segmenta-tion for a given word-aligned pair of sentences (top).Figure 1: Tuple extraction from a sentence pair.The resulting sequence of tuples (1) is further re-fined to avoid NULL words in the source side of thetuples (2).
Once the whole bilingual training data issegmented into tuples, n-gram language model prob-abilities can be estimated.
In this example, note thatthe English source words perfect and translationshave been reordered in the final tuple segmentation,while the French target words are kept in their orig-inal order.2.2 InferenceDuring decoding, source sentences are encodedin the form of word lattices containing the mostpromising reordering hypotheses, so as to reproducethe word order modifications introduced during thetuple extraction process.
Hence, at decoding time,only those encoded reordering hypotheses are trans-lated.
Reordering hypotheses are introduced usinga set of reordering rules automatically learned fromthe word alignments.In the previous example, the rule [perfect transla-tions ; translations perfect] produces the swap ofthe English words that is observed for the Frenchand English pair.
Typically, part-of-speech (POS)information is used to increase the generalizationpower of such rules.
Hence, rewriting rules are builtusing POS rather than surface word forms.
Refer2http://geek.kyloo.net/software310to (Crego and Marin?o, 2007) for details on tuple ex-traction and reordering rules.3 Data Pre-processing and SelectionWe used all the available parallel data allowed inthe constrained task to compute the word align-ments, except for the French-English tasks wherethe United Nation corpus was not used to train ourtranslation models.
To train the target languagemodels, we also used all provided data and mono-lingual corpora released by the LDC for Frenchand English.
Moreover, all parallel corpora werePOS-tagged with the TreeTagger (Schmid, 1994).For German, the fine-grained POS information usedfor pre-processing was computed by the RFTag-ger (Schmid and Laws, 2008).3.1 TokenizationWe took advantage of our in-house text process-ing tools for the tokenization and detokenizationsteps (De?chelotte et al, 2008).
Previous experi-ments have demonstrated that better normalizationtools provide better BLEU scores (Papineni et al,2002).
Thus all systems are built in ?true-case.
?As German is morphologically more complexthan English, the default policy which consists intreating each word form independently is plaguedwith data sparsity, which poses a number of diffi-culties both at training and decoding time.
Thus,to translate from German to English, the Germanside was normalized using a specific pre-processingscheme (described in (Allauzen et al, 2010)), whichaims at reducing the lexical redundancy and splittingcomplex compounds.Using the same pre-processing scheme to trans-late from English to German would require to post-process the output to undo the pre-processing.
As inour last year?s experiments (Allauzen et al, 2010),this pre-processing step could be achieved with atwo-step decoding.
However, by stacking two de-coding steps, we may stack errors as well.
Thus, forthis direction, we used the German tokenizer pro-vided by the organizers.3.2 Filtering the GigaWord CorpusThe available parallel data for English-French in-cludes a large Web corpus, referred to as the Giga-Word parallel corpus.
This corpus is very noisy, andcontains large portions that are not useful for trans-lating news text.
The first filter aimed at detectingforeign languages based on perplexity and lexicalcoverage.
Then, to select a subset of parallel sen-tences, trigram LMs were trained for both Frenchand English languages on a subset of the availableNews data: the French (resp.
English) LM was usedto rank the French (resp.
English) side of the cor-pus, and only those sentences with perplexity abovea given threshold were selected.
Finally, the two se-lected sets were intersected.
In the following exper-iments, the threshold was set to the median or upperquartile value of the perplexity.
Therefore, half (or75%) of this corpus was discarded.4 Target Language ModelingNeural networks, working on top of conventionaln-gram models, have been introduced in (Bengioet al, 2003; Schwenk, 2007) as a potential meansto improve conventional n-gram language models(LMs).
However, probably the major bottleneckwith standard NNLMs is the computation of poste-rior probabilities in the output layer.
This layer mustcontain one unit for each vocabulary word.
Such adesign makes handling of large vocabularies, con-sisting of hundreds thousand words, infeasible dueto a prohibitive growth in computation time.
Whilerecent work proposed to estimate the n-gram dis-tributions only for the most frequent words (short-list) (Schwenk, 2007), we explored the use of theSOUL (Structured OUtput Layer Neural Network)language model for SMT in order to handle vocabu-laries of arbitrary sizes.Moreover, in our setting, increasing the order ofstandard n-gram LM did not show any significantimprovement.
This is mainly due to the data spar-sity issue and to the drastic increase in the number ofparameters that need to be estimated.
With NNLMhowever, the increase in context length at the inputlayer results in only a linear growth in complexityin the worst case (Schwenk, 2007).
Thus, traininglonger-context neural network models is still feasi-ble, and was found to be very effective in our system.3114.1 Standard n-gram Back-off LanguageModelsTo train our language models, we assumed that thetest set consisted in a selection of news texts dat-ing from the end of 2010 to the beginning of 2011.This assumption was based on what was done forthe 2010 evaluation.
Thus, for each language, webuilt a development corpus in order to optimize thevocabulary and the target language model.Development set and vocabulary In order tocover different periods, two development sets wereused.
The first one is newstest2008.
This corpus istwo years older than the targeted time period; there-fore, a second development corpus named dev2010-2011 was collected by randomly sampling bunchesof 5 consecutive sentences from the provided newsdata of 2010 and 2011.To estimate such large LMs, a vocabularywas first defined for each language by includingall tokens observed in the Europarl and News-Commentary corpora.
For French and English, thisvocabulary was then expanded with all words thatoccur more than 5 times in the French-English Gi-gaWord corpus, and with the most frequent propernames taken from the monolingual news data of2010 and 2011.
As for German, since the amountof training data was smaller, the vocabulary was ex-panded with the most frequent words observed in themonolingual news data of 2010 and 2011.
This pro-cedure resulted in a vocabulary containing around500k words in each language.Language model training All the training data al-lowed in the constrained task were divided into sev-eral sets based on dates or genres (resp.
9 and 7sets for English and French).
On each set, a stan-dard 4-gram LM was estimated from the 500k wordsvocabulary using absolute discounting interpolatedwith lower order models (Kneser and Ney, 1995;Chen and Goodman, 1998).All LMs except the one trained on the news cor-pora from 2010-2011 were first linearly interpolated.The associated coefficients were estimated so as tominimize the perplexity evaluated on dev2010-2011.The resulting LM and the 2010-2011 LM were fi-naly interpolated with newstest2008 as developmentdata.
This procedure aims to avoid overestimatingthe weight associated to the 2010-2011 LM.4.2 The SOUL ModelWe give here a brief overview of the SOUL LM;refer to (Le et al, 2011) for the complete trainingprocedure.
Following the classical work on dis-tributed word representation (Brown et al, 1992),we assume that the output vocabulary is structuredby a clustering tree, where each word belongs toonly one class and its associated sub-classes.
If widenotes the i-th word in a sentence, the sequencec1:D(wi) = c1, .
.
.
,cD encodes the path for the wordwi in the clustering tree, with D the depth of the tree,cd(wi) a class or sub-class assigned to wi, and cD(wi)the leaf associated with wi (the word itself).
Then-gram probability of wi given its history h can thenbe estimated as follows using the chain rule:P(wi|h) = P(c1(wi)|h)D?d=2P(cd(wi)|h,c1:d?1)Figure 2 represents the architecture of the NNLMto estimate this distribution, for a tree of depthD = 3.
The SOUL architecture is the same as forthe standard model up to the output layer.
Themain difference lies in the output structure which in-volves several layers with a softmax activation func-tion.
The first softmax layer (class layer) estimatesthe class probability P(c1(wi)|h), while other out-put sub-class layers estimate the sub-class proba-bilities P(cd(wi)|h,c1:d?1).
Finally, the word layersestimate the word probabilities P(cD(wi)|h,c1:D?1).Words in the short-list are a special case since eachof them represents its own class without any sub-classes (D = 1 in this case).5 Experimental ResultsThe experimental results are reported in terms ofBLEU and translation edit rate (TER) using thenewstest2010 corpus as evaluation set.
These auto-matic metrics are computed using the scripts pro-vided by the NIST after a detokenization step.5.1 English-FrenchCompared with last year evaluation, the amount ofavailable parallel data has drastically increased withabout 33M of sentence pairs.
It is worth noticing312wi-1wi-2wi-3RRRWihshared context spaceinput layerhidden layer:tanh activationword layerssub-classlayers}short listFigure 2: Architecture of the Structured Output LayerNeural Network language model.that the provided corpora are not homogeneous, nei-ther in terms of genre nor in terms of topics.
Never-theless, the most salient difference is the noise car-ried by the GigaWord and the United Nation cor-pora.
The former is an automatically collected cor-pus drawn from different websites, and while someparts are indeed relevant to translate news texts, us-ing the whole GigaWord corpus seems to be harm-ful.
The latter (United Nation) is obviously morehomogeneous, but clearly out of domain.
As an il-lustration, discarding the United Nation corpus im-proves performance slightly.Table 1 summarizes some of our attempts at deal-ing with such a large amount of parallel data.
Asstated above, translation models are trained withthe news-commentary, Europarl, and GigaWord cor-pora.
For this last data set, results show the reward ofsentence pair selection as described in Section 3.2.Indeed, filtering out 75% of the corpus yields toa significant BLEU improvement when translatingfrom English to French and of 1 point in the otherdirection (line upper quartile in Table 1).
More-over, a larger selection (50% in the median line) stillincreases the overall performance.
This shows theroom left for improvement by a more accurate dataselection process such as a well optimized thresh-old in our approach, or a more sophisticated filteringstrategy (see for example (Foster et al, 2010)).Another issue when using such a large amountSystem en2fr fr2enBLEU TER BLEU TERAll 27.4 56.6 26.8 55.0Upper quartile 27.8 56.3 28.4 53.8Median 28.1 56.0 28.6 53.5Table 1: English-French translation results in terms ofBLEU score and TER estimated on newstest2010 withthe NIST script.
All means that the translation model istrained on news-commentary, Europarl, and the wholeGigaWord.
The rows upper quartile and median corre-spond to the use of a filtered version of the GigaWord.of data is the mismatch between the target vocab-ulary derived from the translation model and that ofthe LM.
The translation model may generate wordswhich are unknown to the LM, and their probabili-ties could be overestimated.
To avoid this behaviour,the probability of unknown words for the target LMis penalized during the decoding step.5.2 English-GermanFor this translation task, we compare the impact oftwo different POS-taggers to process the Germanpart of the parallel data.
The results are reportedin Table 2.
Results show that to translate from En-glish to German, the use of a fine-grained POS infor-mation (RFTagger) leads to a slight improvement,whereas it harms the source reordering model in theother direction.
It is worth noticing that to translatefrom German to English, the RFTagger is alwaysused during the data pre-processing step, while a dif-ferent POS tagger may be involved for the sourcereordering model training.System en2de de2enBLEU TER BLEU TERRFTagger 22.8 60.1 16.3 66.0TreeTagger 23.1 59.4 16.2 66.0Table 2: Translation results in terms of BLEU scoreand translation edit rate (TER) estimated on newstest2010with the NIST scoring script.5.3 The SOUL ModelAs mentioned in Section 4.2, the order of a con-tinuous n-gram model such as the SOUL LM canbe raised without a prohibitive increase in complex-ity.
We summarize in Table 3 our experiments with313SOUL LMs of orders 4, 6, and 10.
The SOUL LMis introduced in the SMT pipeline by rescoring then-best list generated by the decoder, and the asso-ciated weight is tuned with MERT.
We observe forthe English-French task: a BLEU improvement of0.3, as well as a similar trend in TER, when intro-ducing a 4-gram SOUL LM; an additional BLEUimprovement of 0.3 when increasing the order from4 to 6; and a less important gain with the 10-gramSOUL LM.
In the end, the use of a 10-gram SOULLM achieves a 0.7 BLEU improvement and a TERdecrease of 0.8.
The results on the English-Germantask show the same trend with a 0.5 BLEU pointimprovement.SOUL LM en2fr en2deBLEU TER BLEU TERwithout 28.1 56.0 16.3 66.04-gram 28.4 55.5 16.5 64.96-gram 28.7 55.3 16.7 64.910-gram 28.8 55.2 16.8 64.6Table 3: Translation results from English to French andEnglish to German measured on newstest2010 using a100-best rescoring with SOUL LMs of different orders.5.4 Optimization IssuesAlong with MIRA (Margin Infused Relaxed Al-gorithm) (Watanabe et al, 2007), MERT is themost widely used algorithm for system optimiza-tion.
However, standard MERT procedure is knownto suffer from instability of results and very slowtraining cycle with approximate estimates of one de-coding cycle for each training parameter.
For thisyear?s evaluation, we experimented with several al-ternatives to the standard n-best MERT procedure,namely, MERT on word lattices (Macherey et al,2008) and two differentiable variants to the BLEUobjective function optimized during the MERT cy-cle.
We have recast the former in terms of a spe-cific semiring and implemented it using a general-purpose finite state automata framework (Sokolovand Yvon, 2011).
The last two approaches, hereafterreferred to as ZHN and BBN, replace the BLEUobjective function, with the usual BLEU score onexpected n-gram counts (Rosti et al, 2010) andwith an expected BLEU score for normal n-gramcounts (Zens et al, 2007), respectively.
All expecta-tions (of the n-gram counts in the first case and theBLEU score in the second) are taken over all hy-potheses from n-best lists for each source sentence.Experiments with the alternative optimizationmethods achieved virtually the same performance interms of BLEU score, but 2 to 4 times faster.
Neitherapproach, however, showed any consistent and sig-nificant improvement for the majority of setups tried(with the exception of the BBN approach, that hadalmost always improved over n-best MERT, but forthe sole French to English translation direction).
Ad-ditional experiments with 9 complementary transla-tion models as additional features were performedwith lattice-MERT, but neither showed any substan-tial improvement.
In the view of these rather incon-clusive experiments, we chose to stick to the classi-cal MERT for the submitted results.6 ConclusionIn this paper, we described our submissions toWMT?11 in the French-English and German-English shared translation tasks, in both directions.For this year?s participation, we only used n-code,our open source Statistical Machine Translation sys-tem based on bilingual n-grams.
Our contributionsare threefold.
First, we have shown that n-grambased systems can achieve state-of-the-art perfor-mance on large scale tasks in terms of automaticmetrics such as BLEU.
Then, as already shown byseveral sites in the past evaluations, there is a signifi-cant reward for using data selection algorithms whendealing with large heterogeneous data sources suchas the GigaWord.
Finally, the use of a large vocab-ulary continuous space language model such as theSOUL model has enabled to achieve significant andconsistent improvements.
For the upcoming evalua-tion(s), we would like to suggest that the importantwork of data cleaning and pre-processing could beshared among all the participants instead of beingdone independently several times by each site.
Re-ducing these differences could indeed help improvethe reliability of SMT systems evaluation.AcknowledgmentThis work was achieved as part of the Quaero Pro-gramme, funded by OSEO, French State agency forinnovation.314ReferencesAlexandre Allauzen, Josep M. Crego, I?lknur Durgar El-Kahlout, and Francois Yvon.
2010.
LIMSI?s statis-tical translation systems for WMT?10.
In Proc.
of theJoint Workshop on Statistical Machine Translation andMetricsMATR, pages 54?59, Uppsala, Sweden.Yoshua Bengio, Re?jean Ducharme, Pascal Vincent, andChristian Janvin.
2003.
A neural probabilistic lan-guage model.
JMLR, 3:1137?1155.P.F.
Brown, P.V.
de Souza, R.L.
Mercer, V.J.
Della Pietra,and J.C. Lai.
1992.
Class-based n-gram models of nat-ural language.
Computational Linguistics, 18(4):467?479.Francesco Casacuberta and Enrique Vidal.
2004.
Ma-chine translation with inferred stochastic finite-statetransducers.
Computational Linguistics, 30(3):205?225.Stanley F. Chen and Joshua T. Goodman.
1998.
Anempirical study of smoothing techniques for languagemodeling.
Technical Report TR-10-98, Computer Sci-ence Group, Harvard University.Josep Maria Crego and Jose?
Bernardo Marin?o.
2007.
Im-proving statistical MT by coupling reordering and de-coding.
Machine Translation, 20(3):199?215.Daniel De?chelotte, Gilles Adda, Alexandre Allauzen,Olivier Galibert, Jean-Luc Gauvain, He?le`ne Mey-nard, and Franc?ois Yvon.
2008.
LIMSI?s statisti-cal translation systems for WMT?08.
In Proc.
of theNAACL-HTL Statistical Machine Translation Work-shop, Columbus, Ohio.George Foster, Cyril Goutte, and Roland Kuhn.
2010.Discriminative instance weighting for domain adapta-tion in statistical machine translation.
In Proceedingsof the 2010 Conference on Empirical Methods in Natu-ral Language Processing, pages 451?459, Cambridge,MA, October.Reinhard Kneser and Herman Ney.
1995.
Improvedbacking-off for m-gram language modeling.
In Pro-ceedings of the International Conference on Acoustics,Speech, and Signal Processing, ICASSP?95, pages181?184, Detroit, MI.Hai-Son Le, Ilya Oparin, Alexandre Allauzen, Jean-LucGauvain, and Franc?ois Yvon.
2011.
Structured outputlayer neural network language model.
In IEEE Inter-national Conference on Acoustics, Speech and SignalProcessing (ICASSP 2011), Prague (Czech Republic),22-27 May.Wolfgang Macherey, Franz Josef Och, Ignacio Thayer,and Jakob Uszkoreit.
2008.
Lattice-based minimumerror rate training for statistical machine translation.In Proc.
of the Conf.
on EMNLP, pages 725?734.Franz Josef Och.
2003.
Minimum error rate training instatistical machine translation.
In ACL ?03: Proc.
ofthe 41st Annual Meeting on Association for Computa-tional Linguistics, pages 160?167.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
BLEU: a method for automatic eval-uation of machine translation.
In ACL ?02: Proc.
ofthe 40th Annual Meeting on Association for Compu-tational Linguistics, pages 311?318.
Association forComputational Linguistics.Antti-Veikko I. Rosti, Bing Zhang, Spyros Matsoukas,and Richard Schwartz.
2010.
BBN system descriptionfor wmt10 system combination task.
In Proceedings ofthe Joint Fifth Workshop on Statistical Machine Trans-lation and MetricsMATR, WMT ?10, pages 321?326,Stroudsburg, PA, USA.
Association for ComputationalLinguistics.Helmut Schmid and Florian Laws.
2008.
Estimationof conditional probabilities with decision trees and anapplication to fine-grained POS tagging.
In Proceed-ings of the 22nd International Conference on Com-putational Linguistics (Coling 2008), pages 777?784,Manchester, UK, August.Helmut Schmid.
1994.
Probabilistic part-of-speech tag-ging using decision trees.
In Proc.
of InternationalConference on New Methods in Language Processing,pages 44?49, Manchester, UK.Holger Schwenk.
2007.
Continuous space languagemodels.
Computer, Speech & Language, 21(3):492?518.Artem Sokolov and Franc?ois Yvon.
2011.
Minimum er-ror rate training semiring.
In Proceedings of the 15thAnnual Conference of the European Association forMachine Translation, EAMT?2011, May.Christoph Tillmann.
2004.
A unigram orientation modelfor statistical machine translation.
In Proceedings ofHLT-NAACL 2004, pages 101?104.
Association forComputational Linguistics.Taro Watanabe, Jun Suzuki, Hajime Tsukada, and HidekiIsozaki.
2007.
Online large-margin training for sta-tistical machine translation.
In Proceedings of the2007 Joint Conference on Empirical Methods in Nat-ural Language Processing and Computational Natu-ral Language Learning (EMNLP-CoNLL), pages 764?773, Prague, Czech Republic.Richard Zens, Sasa Hasan, and Hermann Ney.
2007.A systematic comparison of training criteria for sta-tistical machine translation.
In Proceedings of the2007 Joint Conference on Empirical Methods in Nat-ural Language Processing and Computational Natu-ral Language Learning (EMNLP-CoNLL), pages 524?532.315
