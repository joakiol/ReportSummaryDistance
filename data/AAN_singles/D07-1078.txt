Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and ComputationalNatural Language Learning, pp.
746?754, Prague, June 2007. c?2007 Association for Computational LinguisticsBinarizing Syntax Trees to ImproveSyntax-Based Machine Translation AccuracyWei Wang and Kevin Knight and Daniel MarcuLanguage Weaver, Inc.4640 Admiralty Way, Suite 1210Marina del Rey, CA, 90292{wwang,kknight,dmarcu}@languageweaver.comAbstractWe show that phrase structures in Penn Tree-bank style parses are not optimal for syntax-based machine translation.
We exploit a se-ries of binarization methods to restructurethe Penn Treebank style trees such that syn-tactified phrases smaller than Penn Treebankconstituents can be acquired and exploited intranslation.
We find that by employing theEM algorithm for determining the binariza-tion of a parse tree among a set of alternativebinarizations gives us the best translation re-sult.1 IntroductionSyntax-based translation models (Eisner, 2003; Gal-ley et al, 2006; Marcu et al, 2006) are usually builtdirectly from Penn Treebank (PTB) (Marcus et al,1993) style parse trees by composing treebank gram-mar rules.
As a result, often no substructures corre-sponding to partial PTB constituents are extracted toform translation rules.Syntax translation models acquired by composingtreebank grammar rules assume that long rewritesare not decomposable into smaller steps.
This ef-fectively restricts the generalization power of the in-duced model.
For example, suppose we have anxRs (Knight and Graehl, 2004) rule R1 in Figure 1that translates the Chinese phrase RUSSIA MINISTERVIKTOR-CHERNOMYRDIN into an English NPB treefragment yielding an English phrase.
Also supposethat we want to translate a Chinese phraseVIKTOR-CHERNOMYRDIN AND HIS COLLEAGUEinto English.
What we desire is that if we haveanother rule R2 as shown in Figure 1, we couldsomehow compose it with R1 to obtain the desir-able translation.
We unfortunately cannot do thisbecause R1 and R2 are not further decomposableand their substructures cannot be re-used.
The re-quirement that all translation rules have exactly oneroot node does not enable us to use the translation ofVIKTOR-CHERNOMYRDIN in any other contexts thanthose seen in the training corpus.A solution to overcome this problem is to right-binarize the left-hand side (LHS) (or the English-side) tree of R1 such that we can decomposeR1 into R3 and R4 by factoring NNP(viktor)NNP(chernomyrdin) out as R4 according to theword alignments; and left-binarize the LHS of R2 byintroducing a new tree node that collapses the twoNNP?s, so as to generalize this rule, getting rule R5and rule R6.
We also need to consistently syntact-ify the root labels of R4 and the new frontier labelof R6 such that these two rules can be composed.Since labeling is not a concern of this paper, we sim-ply label new nodes with X-bar where X here is theparent label.
With all these in place, we now cantranslate the foreign sentence by composing R6 andR4 in Figure 1.Binarizing the syntax trees for syntax-based ma-chine translation is similar in spirit to generalizingparsing models via markovization (Collins, 1997;Charniak, 2000).
But in translation modeling, it isunclear how to effectively markovize the translationrules, especially when the rules are complex likethose proposed by Galley et al (2006).In this paper, we explore the generalization abil-ity of simple binarization methods like left-, right-,and head-binarization, and also their combinations.Simple binarization methods binarize syntax treesin a consistent fashion (left-, right-, or head-) and746NPBJJ         NNP      NNP     NNPrussia    minister    viktor   chernomyrdinRUSSIA   MINISTER      V?CNPB?NNP         NNP      AND  HIS   COLLEAGUEV?C AND  HIS COLLEAGUENPBJJrussiaRUSSIAministerMINISTER NNP   NNPviktor  chernomyrdinV?CNPBcolleagueand    his     colleagueCOLLEAGUEPRB$hisHISCCandANDNNP      NNPx0:NNP  x1:NNP V?C AND  HIS COLLEAGUENNSx0:NNP   x1:NNP   CC   PRB$   NNSR1 R2NPBNPBR3NPBNPBR4 R5R6R6R4NPBFigure 1: Generalizing translation rules by binarizing trees.thus cannot guarantee that all the substructures canbe factored out.
For example, right binarization onthe LHS of R1 makes available R4, but misses R6on R2.
We then introduce a parallel restructuringmethod, that is, one can binarize both to the left andright at the same time, resulting in a binarization for-est.
We employ the EM (Dempster et al, 1977) algo-rithm to learn the binarization bias for each tree nodefrom the parallel alternatives.
The EM-binarizationyields best translation performance.The rest of the paper is organized as follows.Section 2 describes related research.
Section 3 de-fines the concepts necessary for describing the bina-rizations methods.
Section 4 describes the tree bina-rization methods in details.
Section 5 describes theforest-based rule extraction algorithm, and section 6explains how we restructure the trees using the EMalgorithm.
The last two sections are for experimentsand conclusions.2 Related ResearchSeveral researchers (Melamed et al, 2004; Zhanget al, 2006) have already proposed methods for bi-narizing synchronous grammars in the context ofmachine translation.
Grammar binarization usuallymaintains an equivalence to the original grammarsuch that binarized grammars generate the same lan-guage and assign the same probability to each stringas the original grammar does.
Grammar binarizationis often employed to make the grammar fit in a CKYparser.
In our work, we are focused on binarizationof parse trees.
Tree binarization generalizes the re-sulting grammar and changes its probability distri-bution.
In tree binarization, synchronous grammarsbuilt from restructured (binarized) training trees stillcontain non-binary, multi-level rules and thus stillrequire the binarization transformation so as to beemployed by a CKY parser.The translation model we are using in this paperbelongs to the xRs formalism (Knight and Graehl,2004), which has been proved successful for ma-chine translation in (Galley et al, 2004; Galley etal., 2006; Marcu et al, 2006).3 ConceptsWe focus on tree-to-string (in noisy-channel modelsense) translation models.
Translation models ofthis type are typically trained on tuples of a source-language sentence f, a target language (e.g., English)parse tree pi that yields e and translates from f, andthe word alignments a between e and f. Such a tupleis called an alignment graph in (Galley et al, 2004).The graph (1) in Figure 2 is such an alignment graph.747(1) unbinarized treeNPBviktor chernomyrdinVIKTOR?CHERNOMYRDINNNP1    NNP2   NNP3   NNP4*(2) left-binarization (3) right-/head-binarizationNPBNPBNNP1 NNP2 NNP3viktorNNP?4chernomyrdinNPBNNP1 NPB?NNP2 NNP3viktorNNP4?chernomyrdin(4) left-binarization (5) right-binarization (6) left-binarization (7) right-/head-binarizationNPBNPBNPBNNP1 NNP2NNP3viktorNNP4?chernomyrdin- -NPBNNP1 NPB?NNP2 NPB?NNP3viktorNNP4?chernomyrdinFigure 2: Left, right, and head binarizations.
Heads are marked with ??s.
New nonterminals introduced by binarization aredenoted by X-bars.A tree node in pi is admissible if the f string cov-ered by the node is contiguous but not empty, andif the f string does not align to any e string that isnot covered by pi.
An xRs rule can be extracted onlyfrom an admissible tree node, so that we do not haveto deal with dis-contiguous f spans in decoding (orsynchronous parsing).
For example, in tree (2) inFigure 2, node NPB is not admissible because thef string that the node covers also aligns to NNP4,which is not covered by the NPB.
Node NPB in tree(3), on the other hand, is admissible.A set of sibling tree nodes is called factorizableif we can form an admissible new node dominatingthem.
For example, in tree (1) of Figure 2, siblingnodes NNP2 NNP3 and NNP4 are factorizable be-cause we can factorize them out and form a newnode NPB, resulting in tree (3).
Sibling tree nodesNNP1 NNP2 and NNP3 are not factorizable.
In syn-chronous parse trees, not all sibling nodes are fac-torizable, thus not all sub-phrases can be acquiredand syntactified.
The main purpose of our paper isto restructure parse trees by factorization such thatsyntactified sub-phrases can be employed in transla-tion.4 Binarizing Syntax TreesWe are going to binarize a tree node n that domi-nates r children n1, ..., nr.
Restructuring will beperformed by introducing new tree nodes to domi-nate a subset of the children nodes.
To avoid over-generalization, we allow ourselves to form only onenew node at a time.
For example, in Figure 2, wecan binarize tree (1) into tree (2), but we are notallowed to form two new nodes, one dominatingNNP1 NNP2 and the other dominating NNP3 NNP4.Since labeling is not the concern of this paper, we re-label the newly formed nodes as n.4.1 Simple binarization methodsThe left binarization of node n (i.e., the NPB intree (1) of Figure 2) factorizes the leftmost r ?
1children by forming a new node n (i.e., NPB intree (2)) to dominate them, leaving the last childnr untouched; and then makes the new node n theleft child of n. The method then recursively left-binarizes the newly formed node n until two leavesare reached.
In Figure 2, we left-binarize tree (1)into (2) and then into (4).The right binarization of node n factorizes therightmost r ?
1 children by forming a new node n(i.e., NPB in tree (3)) to dominate them, leaving the748first child n1 untouched; and then makes the newnode n the right child of n. The method then recur-sively right-binarizes the newly formed node n. InFigure 2, we right-binarize tree (1) into (3) and theninto (7).The head binarization of node n left-binarizesn if the head is the first child; otherwise, right-binarizes n. We prefer right-binarization to left-binarization when both are applicable under the headrestriction because our initial motivation was to gen-eralize the NPB-rooted translation rules.
As we willshow in the experiments, binarization of other typesof phrases contribute to the translation accuracy im-provement as well.Any of these simple binarization methods is easyto implement, but is incapable of giving us all thefactorizable sub-phrases.
Binarizing all the way tothe left, for example, from tree (1) to tree (2) and totree (4) in Figure 2, does not enable us to acquire asubstructure that yields NNP3 NNP4 and their trans-lational equivalences.
To obtain more factorizablesub-phrases, we need to parallel-binarize in both di-rections.4.2 Parallel binarizationSimple binarizations transform a parse tree into an-other single parse tree.
Parallel binarization willtransform a parse tree into a binarization forest,desirably packed to enable dynamic programmingwhen extracting translation rules from it.Borrowing terms from parsing semirings (Good-man, 1999), a packed forest is composed of addi-tive forest nodes (?-nodes) and multiplicative forestnodes (?-nodes).
In the binarization forest, a ?-node corresponds to a tree node in the unbinarizedtree; and this ?-node composes several ?-nodes,forming a one-level substructure that is observed inthe unbinarized tree.
A ?-node corresponds to al-ternative ways of binarizing the same tree node inthe unbinarized tree and it contains one or more ?-nodes.
The same ?-node can appear in more thanone place in the packed forest, enabling sharing.Figure 3 shows a packed forest obtained by pack-ing trees (4) and (7) in Figure 2 via the followingparallel binarization algorithm.To parallel-binarize a tree node n that has childrenn1, ..., nr , we employ the following steps:?1(NPB)?2(NPB)?3(NPB)?4(NPB)?5(NPB)?6(NPB)?7(NNP1) ?8(NNP2)?9(NNP3)?10(NNP4)?11(NPB)?7(NNP1) ?12(NPB)?13(NPB)?8(NNP2) ?14(NPB)?15(NPB)?9(NNP3) ?10(NNP4)Figure 3: Packed forest obtained by packing trees (4) and (7)in Figure 2?
We recursively parallel-binarize children nodesn1, ..., nr, producing binarization ?-nodes?
(n1), ..., ?
(nr), respectively.?
We right-binarize n, if any contiguous1 subsetof children n2, ..., nr is factorizable, by intro-ducing an intermediate tree node labeled as n.We recursively parallel-binarize n to generatea binarization forest node ?(n).
We form amultiplicative forest node ?R as the parent of?
(n1) and ?(n).?
We left-binarize n if any contiguous subsetof n1, ..., nr?1 is factorizable and if this sub-set contains n1.
Similar to the above right-binarization, we introduce an intermediate treenode labeled as n, recursively parallel-binarizen to generate a binarization forest node ?
(n),form a multiplicative forest node ?L as the par-ent of ?
(n) and ?(n1).?
We form an additive node ?
(n) as the parentof the two already formed multiplicative nodes?L and ?R.The (left and right) binarization conditions con-sider any subset to enable the factorization of smallconstituents.
For example, in tree (1) of Figure 2,although NNP1 NNP2 NNP3 of NPB are not factor-izable, the subset NNP1 NNP2 is factorizable.
Thebinarization from tree (1) to tree (2) serves as a re-laying step for us to factorize NNP1 NNP2 in tree(4).
The left-binarization condition is stricter than1We factorize only subsets that cover contiguous spans toavoid introducing dis-contiguous constituents for practical pur-pose.
In principle, the algorithm works fine without this bina-rization condition.749the right-binarization condition to avoid spurious bi-narization; i.e., to avoid the same subconstituent be-ing reached via both binarizations.
We could trans-form tree (1) directly into tree (4) without bother-ing to generate tree (3).
However, skipping tree (3)will create us difficulty in applying the EM algo-rithm to choose a better binarization for each treenode, since tree (4) can neither be classified as leftbinarization nor as right binarization of the originaltree (1) ?
it is the result of the composition of twoleft-binarizations.In parallel binarization, nodes are not always bi-narizable in both directions.
For example, we do notneed to right-binarize tree (2) because NNP2 NNP3are not factorizable, and thus cannot be used to formsub-phrases.
It is still possible to right-binarize tree(2) without affecting the correctness of the parallelbinarization algorithm, but that will spuriously in-crease the branching factor of the search for the ruleextraction, because we will have to expand more treenodes.A restricted version of parallel binarization is theheaded parallel binarization, where both the left andthe right binarization must respect the head propaga-tion property at the same time.A nice property of parallel binarization is thatfor any factorizable substructure in the unbinarizedtree, we can always find a corresponding admissi-ble ?-node in the parallel-binarized packed forest.A leftmost substructure like the lowest NPB-subtreein tree (4) of Figure 2 can be made factorizableby several successive left binarizations, resulting in?5(NPB)-node in the packed forest in Figure 3.
Asubstructure in the middle can be factorized by thecomposition of several left- and right-binarizations.Therefore, after a tree is parallel-binarized, to makethe sub-phrases available to the MT system, all weneed to do is to extract rules from the admissiblenodes in the packed forest.
Rules that can be ex-tracted from the original unrestructured tree can beextracted from the packed forest as well.Parallel binarization results in parse forests.
Thustranslation rules need to be extracted from trainingdata consisting of (e-forest, f, a)-tuples.5 Extracting translation rules from(e-forest, f, a)-tuplesThe algorithm to extract rules from (e-forest, f, a)-tuples is a natural generalization of the (e-parse, f,a)-based rule extraction algorithm in (Galley et al,2006).
The input to the forest-based algorithm is a(e-forest, f, a)-triple.
The output of the algorithm isa derivation forest (Galley et al, 2006) composed ofxRs rules.
The algorithm recursively traverses the e-forest top-down and extracts rules only at admissibleforest nodes.The following procedure transforms the packed e-forest in Figure 3 into a packed synchronous deriva-tion in Figure 4.Condition 1: Suppose we reach an additivee-forest node, e.g.
?1(NPB) in Figure 3.
Foreach of ?1(NPB)?s children, e-forest nodes?2(NPB) and ?11(NPB), we go to condi-tion 2 to recursively extract rules on thesetwo e-forest nodes, generating multiplicativederivation forest nodes, i.e., ?
(NPB(NPB :x0 NNP3(viktor) NNP4(chernomyrdin)4) ?x0 V-C) and ?
(NPB(NNP1 NPB(NNP2 : x0 NPB :x1)) ?
x0 x1 x2) in Figure 4.
We make thesenew ?
nodes children of ?
(NPB) in the derivationforest.Condition 2: Suppose we reach a multiplicativeparse forest node, i.e., ?11(NPB) in Figure 3.
Weextract rules rooted at it using the procedure in(Galley et al, 2006), forming multiplicative deriva-tion forest nodes, i.e., ?
(NPB(NNP1 NPB(NNP2 :x0 NPB : x1)) ?
x0 x1 x2) We then goto condition 1 to form the derivation forest onthe additive frontier e-forest nodes of the newlyextracted rules, generating additive derivation for-est nodes, i.e., ?
(NNP1), ?
(NNP2) and ?
(NPB).We make these ?
nodes the children of node?
(NPB(NNP1 NPB(NNP2 : x0 NPB : x1)) ?x0 x1 x2) in the derivation forest.This algorithm is a natural extension of the extrac-tion algorithm in (Galley et al, 2006) in the sensethat we have an extra condition (1) to relay rule ex-traction on additive e-forest nodes.It is worthwhile to eliminate the spuriously am-biguous rules that are introduced by the parallel bi-750?(NPB)?
(NPB(NPB : x0 NNP(viktor) NNP(chernomyrdin)) ?
x0 V-C)?(NPB)?
(NPB(NNP : x0 NNP : x1 ?
x0 x1))?
(NPB(NNP : x0 NPB(NNP : x1 NPB : x2)) ?
x0 x1 x2)?
(NNP) ?
(NNP) ?(NPB)?
(NPB(NNP(viktor) NNP(chernomyrdin)) ?
V-C)Figure 4: Derivation forest.narization.
For example, we may extract the follow-ing two rules:- A(A(B:x0 C:x1)D:x2) ?
x1 x0 x2- A(B:x0 A(C:x1 D:x2)) ?
x1 x0 x2These two rules, however, are not really distinct.They both converge to the following rules if wedelete the auxiliary nodes A.- A(B:x0 C:x1 D:x2) ?
x1 x0 x2The forest-base rule extraction algorithm pro-duces much larger grammars than the tree-basedone, making it difficult to scale to very large trainingdata.
From a 50M-word Chinese-to-English parallelcorpus, we can extract more than 300 million trans-lation rules, while the tree-based rule extraction al-gorithm gives approximately 100 million.
However,the restructured trees from the simple binarizationmethods are not guaranteed to give the best trees forsyntax-based machine translation.
What we desire isa binarization method that still produces single parsetrees, but is able to mix left binarization and rightbinarization in the same tree.
In the following, weshall use the EM algorithm to learn the desirable bi-narization on the forest of binarization alternativesproposed by the parallel binarization algorithm.6 Learning how to binarize via the EMalgorithmThe basic idea of applying the EM algorithm tochoose a restructuring is as follows.
We perform aset {?}
of binarization operations on a parse tree ?
.Each binarization ?
is the sequence of binarizationson the necessary (i.e., factorizable) nodes in ?
in pre-order.
Each binarization ?
results in a restructuredtree ??
.
We extract rules from (??
, f, a), generating atranslation model consisting of parameters (i.e., rulee?parse(Galley et al, 2006)composed rule extraction12parallel binarization e?forestforest?based rule extractionof minimal rulesf,asynchronous derivation forestsEM34viterbi derivationsproject e?parsemodelsyntax translationFigure 5: Using the EM algorithm to choose restructuring.probabilities) ?.
Our aim is to obtain the binarization??
that gives the best likelihood of the restructuredtraining data consisting of (??
, f , a)-tuples.
That is??
= arg max?p(?
?, f ,a|??)
(1)In practice, we cannot enumerate all the exponen-tial number of binarized trees for a given e-parse.We therefore use the packed forest to store all thebinarizations that operate on an e-parse in a com-pact way, and then use the inside-outside algorithm(Lari and Young, 1990; Knight and Graehl, 2004)for model estimation.The probability p(?
?, f ,a) of a (??
, f, a)-tupleis what the basic syntax-based translation model isconcerned with.
It can be further computed by ag-gregating the rule probabilities p(r) in each deriva-tion ?
in the set of all derivations ?
(Galley et al,2004; Marcu et al, 2006).
That isp(?
?, f ,a) =?????r?
?p(r) (2)Since it has been well-known that applying EMwith tree fragments of different sizes causes over-fitting (Johnson, 1998), and since it is also knownthat syntax MT models with larger composed rulesin the mix significantly outperform rules that min-imally explain the training data (minimal rules) in751translation accuracy (Galley et al, 2006), we decom-pose p(?b, f ,a) using minimal rules during runningof the EM algorithm, but, after the EM restructuringis finished, we build the final translation model usingcomposed rules for evaluation.Figure 5 is the actual pipeline that we use forEM binarization.
We first generate a packed e-forestvia parallel binarization.
We then extract minimaltranslation rules from the (e-forest, f, a)-tuples, pro-ducing synchronous derivation forests.
We run theinside-outside algorithm on the derivation forestsuntil convergence.
We obtain the Viterbi derivationsand project the English parses from the derivations.Finally, we extract composed rules using Galley etal.
(2006)?s (e-tree, f, a)-based rule extraction algo-rithm.
This procedure corresponds to the path 13?42in the pipeline.7 ExperimentsWe carried out a series of experiments to comparethe performance of different binarization methodsin terms of BLEU on Chinese-to-English translationtasks.7.1 Experimental setupOur bitext consists of 16M words, all in themainland-news domain.
Our development set is a925-line subset of the 993-line NIST02 evaluationset.
We removed long sentences from the NIST02evaluation set to speed up discriminative training.The test set is the full 919-line NIST03 evaluationset.We used a bottom-up, CKY-style decoder thatworks with binary xRs rules obtained via a syn-chronous binarization procedure (Zhang et al,2006).
The decoder prunes hypotheses using strate-gies described in (Chiang, 2007).The parse trees on the English side of the bitextswere generated using a parser (Soricut, 2004) imple-menting the Collins parsing models (Collins, 1997).We used the EM procedure described in (Knightand Graehl, 2004) to perform the inside-outside al-gorithm on synchronous derivation forests and togenerate the Viterbi derivation forest.We used the rule extractor described in (Galley etal., 2006) to extract rules from (e-parse, f, a)-tuples,but we made an important modification: new nodesintroduced by binarization will not be counted whencomputing the rule size limit unless they appear asthe rule roots.
The motivation is that binarizationdeepens the parses and increases the number of treenodes.
In (Galley et al, 2006), a composed ruleis extracted only if the number of internal nodes itcontains does not exceed a limit (i.e., 4), similarto the phrase length limit in phrase-based systems.This means that rules extracted from the restructuredtrees will be smaller than those from the unrestruc-tured trees, if the X nodes are deleted.
As shown in(Galley et al, 2006), smaller rules lose context, andthus give lower translation performance.
Ignoring Xnodes when computing the rule sizes preserves theunstructured rules in the resulting translation modeland adds substructures as bonuses.7.2 Experiment resultsTable 1 shows the BLEU scores of mixed-cased anddetokenized translations of different systems.
Wesee that all the binarization methods improve thebaseline system that does not apply any binarizationalgorithm.
The EM-binarization performs the bestamong all the restructuring methods, leading to 1.0BLEU point improvement.
We also computed thebootstrap p-values (Riezler and Maxwell, 2005) forthe pairwise BLEU comparison between the base-line system and any of the system trained from bina-rized trees.
The significance test shows that the EMbinarization result is statistically significant betterthan the baseline system (p > 0.005), even thoughthe baseline is already quite strong.
To our bestknowledge, 37.94 is the highest BLEU score on thistest set to date.Also as shown in Table 1, the grammars trainedfrom the binarized training trees are almost twotimes of the grammar size with no binarization.
Theextra rules are substructures factored out by these bi-narization methods.How many more substructures (or translationrules) can be acquired is partially determined byhow many more admissible nodes each binariza-tion method can factorize, since rules are extractableonly from admissible tree nodes.
According toTable 1, binarization methods significantly increasethe number of admissible nodes in the training trees.The EM binarization makes available the largest752EXPERIMENT NIST03-BLEU # RULES # ADMISSIBLE NODES IN TRAININGno-bin 36.94 63.4M 7,995,569left binarization 37.47 (p = 0.047) 114.0M 10,463,148right binarization 37.49 (p = 0.044) 113.0M 10,413,194head binarization 37.54 (p = 0.086) 113.8M 10,534,339EM binarization 37.94 (p = 0.0047) 115.6M 10,658,859Table 1: Translation performance, grammar size and # admissible nodes versus binarization algorithms.
BLEU scores are formixed-cased and detokenized translations, as we usually do for NIST MT evaluations.nonterminal left-binarization right-binarizationNP 96.97% 3.03%NP-C 97.49% 2.51%NPB 0.25% 99.75%VP 93.90% 6.10%PP 83.75% 16.25%ADJP 87.83% 12.17%ADVP 82.74% 17.26%S 85.91% 14.09%S-C 18.88% 81.12%SBAR 96.69% 3.31%QP 86.40% 13.60%PRN 85.18% 14.82%WHNP 97.93% 2.07%NX 100% 0SINV 87.78% 12.22%PRT 100% 0SQ 93.53% 6.47%CONJP 18.08% 81.92%Table 2: Binarization bias learned by EM.number of admissible nodes, and thus results in themost rules.The EM binarization factorizes more admissiblenodes because it mixes both left and right binariza-tions in the same tree.
We computed the binarizationbiases learned by the EM algorithm for each nonter-minal from the binarization forest of headed-parallelbinarizations of the training trees, getting the statis-tics in Table 2.
Of course, the binarization biaschosen by left-/right-binarization methods would be100% deterministic.
One noticeable message fromTable 2 is that most of the categories are actually bi-ased toward left-binarization, although our motivat-ing example in our introduction section is for NPB,which needed right binarization.
The main reasonmight be that the head sub-constituents of most cat-egories tend to be on the left, but according to theperformance comparison between head binarizationand EM binarization, head binarization does not suf-fice because we still need to choose the binarizationbetween left and right if they both are head binariza-tions.8 ConclusionsIn this paper, we not only studied the impact ofsimple tree binarization algorithms on the perfor-mance of end-to-end syntax-based MT, but also pro-posed binarization methods that mix more than onesimple binarization in the binarization of the sameparse tree.
Binarizing a tree node whether to the leftor to the right was learned by employing the EMalgorithm on a set of alternative binarizations andby choosing the Viterbi one.
The EM binarizationmethod is informed by word alignments such thatunnecessary new tree nodes will not be ?blindly?
in-troduced.To our best knowledge, our research is the firstwork that aims to generalize a syntax-based trans-lation model by restructuring and achieves signifi-cant improvement on a strong baseline.
Our workdiffers from traditional work on binarization of syn-chronous grammars in that we are not concernedwith the equivalence of the binarized grammar to theoriginal grammar, but intend to generalize the orig-inal grammar via restructuring of the training parsetrees to improve translation performance.AcknowledgmentsThe authors would like to thank David Chiang,Bryant Huang, and the anonymous reviewers fortheir valuable feedbacks.ReferencesE.
Charniak.
2000.
A maximum-entropy-inspired parser.In Proceedings of the Human Language TechnologyConference of the North American Chapter of the As-sociation for Computational Linguistics, Seattle, May.David Chiang.
2007.
Hierarchical phrase-based transla-tion.
Computational Linguistics, 33(2).Michael Collins.
1997.
Three generative, lexicalizedmodels for statistical parsing.
In Proceedings of the75335th Annual Meeting of the Association for Computa-tional Linguistics (ACL), pages 16?23, Madrid, Spain,July.A.
P. Dempster, N. M. Laird, and D. B. Rubin.
1977.Maximum likelihood from incomplete data via the EMalgorithm.
Journal of the Royal Statistical Society,39(1):1?38.Jason Eisner.
2003.
Learning non-isomorphic tree map-pings for machine translation.
In Proceedings of the40th Annual Meeting of the Association for Compu-tational Linguistics (ACL), pages 205?208, Sapporo,July.M.
Galley, M. Hopkins, K. Knight, and D. Marcu.
2004.What?s in a Translation Rule?
In Proceedings ofthe Human Language Technology Conference and theNorth American Association for Computational Lin-guistics (HLT-NAACL), Boston, Massachusetts.M.
Galley, J. Graehl, K. Knight, D. Marcu, S. DeNeefe,W.
Wang, and I. Thayer.
2006.
Scalable Inference andTraining of Context-Rich Syntactic Models.
In Pro-ceedings of the 44th Annual Meeting of the Associationfor Computational Linguistics (ACL).Joshua Goodman.
1999.
Semiring parsing.
Computa-tional Linguistics, 25(4):573?605.M.
Johnson.
1998.
The DOP estimation method isbiased and inconsistent.
Computational Linguistics,28(1):71?76.K.
Knight and J. Graehl.
2004.
Training Tree Transduc-ers.
In Proceedings of NAACL-HLT.K.
Lari and S. Young.
1990.
The estimation of stochasticcontext-free grammars using the inside-outside algo-rithm.
Computer Speech and Language, pages 35?56.Daniel Marcu, Wei Wang, Abdessamad Echihabi, andKevin Knight.
2006.
SPMT: Statistical machinetranslation with syntactified target language phraases.In Proceedings of EMNLP-2006, pp.
44-52, Sydney,Australia.M.
Marcus, B. Santorini, and M. Marcinkiewicz.1993.
Building a large annotated corpus of En-glish: The Penn Treebank.
Computational Linguistics,19(2):313?330.I.
Dan Melamed, Giorgio Satta, and Benjamin Welling-ton.
2004.
Generalized multitext grammars.
In Pro-ceedings of the 42nd Annual Meeting of the Associa-tion for Computational Linguistics (ACL), Barcelona,Spain.Stefan Riezler and John T. Maxwell.
2005.
On somepitfalls in automatic evaluation and significance test-ing for MT.
In Proc.
ACL Workshop on Intrinsic andExtrinsic Evaluation Measures for MT and/or Summa-rization.Radu Soricut.
2004.
A reimplementation of Collins?sparsing models.
Technical report, Information Sci-ences Institute, Department of Computer Science Uni-versity of Southern California.Hao Zhang, Liang Huang, Daniel Gildea, and KevinKnight.
2006.
Synchronous binarization for machinetranslation.
In Proceedings of the HLT-NAACL.754
