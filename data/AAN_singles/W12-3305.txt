Proceedings of the 2012 Student Research Workshop, pages 25?30,Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational LinguisticsEvaluating Unsupervised Ensembles when applied to Word Sense InductionKeith Stevens1,21University of California Los Angeles; Los Angeles , California, USA2Lawrence Livermore National Lab; Livermore, California, USA?kstevens@cs.ucla.eduAbstractEnsembles combine knowledge from distinctmachine learning approaches into a generalflexible system.
While supervised ensemblesfrequently show great benefit, unsupervisedensembles prove to be more challenging.
Wepropose evaluating various unsupervised en-sembles when applied to the unsupervised taskof Word Sense Induction with a framework forcombining diverse feature spaces and cluster-ing algorithms.
We evaluate our system us-ing standard shared tasks and also introducenew automated semantic evaluations and su-pervised baselines, both of which highlight thecurrent limitations of existing Word Sense In-duction evaluations.1 IntroductionMachine learning problems often benefit from manydiffering solutions using ensembles (Dietterich,2000) and supervised Natural Language Processingtasks have been no exception.
However, use of un-supervised ensembles in NLP tasks has not yet beenrigorously evaluated.
Brody et al (2006) first con-sidered unsupervised ensembles by combining fourstate of the art Word Sense Disambiguation systemsusing a simple voting scheme with much success.Later, Brody and Lapata (2009) combined differentfeature sets using a probabilistic Word Sense Induc-tion model and found that only some combinationsproduced an improved system.
These early and lim-ited evaluations show both the promise and draw-back of combining different unsupervised models:?This work was performed under the auspices of the U.S.Department of Energy by Lawrence Livermore National Lab-oratory under Contract DE-AC52-07NA27344 (LLNL-CONF-530791).particular combinations provide a benefit but select-ing these combinations is non-trivial.We propose applying a new and more gen-eral framework for combining unsupervised systemsknown as Ensemble Clustering to unsupervised NLPsystems and focus on the fully unsupervised taskof Word Sense Induction.
Ensemble Clustering cancombine together multiple and diverse clustering al-gorithms or feature spaces and has been shown tonoticeably improve clustering accuracy for both textbased datasets and other datasets (Monti et al, 2003;Strehl et al, 2002).
Since Word Sense Induction isfundamentally a clustering problem, with many vari-ations, it serves well as a NLP case study for Ensem-ble Clustering.The task of Word Sense Induction extends theproblem of Word Sense Disambiguation by simplyassuming that a model must first learn and define asense inventory before disambiguating multi-sensewords.
This induction step frees the disambiguationprocess from any fixed sense inventory and can in-stead flexibly define senses based on observed pat-terns within a dataset (Pedersen, 2006).
However,this induction step has proven to be greatly chal-lenging, in the most recent shared tasks, inductionsystems either appear to perform poorly or fail tooutperform the simple Most Frequent Sense baseline(Agirre and Soroa, 2007a; Manandhar et al, 2010).In this work, we propose applying EnsembleClustering as a general framework for combiningnot only different feature spaces but also a variety ofdifferent clustering algorithms.
Within this frame-work we will explore which types of models shouldbe combined and how to best combine them.
In ad-dition, we propose two new evaluations: (1) new se-mantic coherence measures that evaluate the seman-25Figure 1: The Ensemble Clustering model: individ-ual clustering algorithms partition perturbations ofthe dataset and all partitions are combined via a con-sensus function to create a final solution, pi?.tic quality and uniqueness of induced word senseswithout referring to an external sense inventory (2)and a new set of baseline systems based on super-vised learning algorithms.
With the new evaluationsand a framework for combining general inductionmodels, we intend to find not only improved modelsbut a better understanding of how to improve laterinduction models.2 Consensus ClusteringEnsemble Clustering presents a new method forcombining together arbitrary clustering algorithmswithout any supervision (Monti et al, 2003; Strehlet al, 2002).
The method adapts simple boostingand voting approaches from supervised ensemblesto merge together diverse clustering partitions intoa single consensus solution.
Ensemble Clusteringforms a single consensus partition by processing adata set in two steps: (1) create a diverse set of en-sembles that each partition some perturbation of thefull dataset and (2) find the median partition that bestagrees with each ensemble?s partition.
Figure 1 vi-sually displays these two steps.Variation in these two steps accounts for the widevariety of Ensemble Clustering approaches.
Eachensemble can be created from either a large col-lection of distinct clustering algorithms or througha boosting approach where the same algorithm istrained on variations of the dataset.
Finding the me-dian partition turns out to be an NP-Complete prob-lem under most settings (Goder and Filkov, 2008)and thus must be approximated with one of sev-eral heuristics.
We consider several well tested ap-proaches to both steps.Formally, we define Ensemble Clustering tooperate over a dataset of N elements: D ={d1, .
.
.
, dN}.
Ensemble Clustering then creates Hensembles that each partition a perturbation Dh of Dto create H partitions, ?
= {pi1, ?
?
?
, piH}.
The con-sensus algorithm then approximates the best consen-sus partition pi?
that satisfies:argminpi??pih?
?d(pih, pi?)
(1)according to some distance metric d(pii, pij) betweentwo partitions.
We use the symmetric difference dis-tance as d(pii, pij).
Let Pi be the set of co-clusterdata points in pii.
The distance metric is then definedto bed(pi1, pi2) = |P1 \ P2|+ |P2 \ P1|2.1 Forming EnsemblesEnsemble clustering can combine together overlap-ping decisions from many different clustering algo-rithms or it can similarly boost the performance of asingle algorithm by using different parameters.
Weconsider two simple formulations of ensemble cre-ation: Homogeneous Ensembles and HeterogeneousEnsembles.
We secondly consider approaches forcombining the two creation methods.Homogeneous Ensembles partition randomlysampled subsets of the data points from D withoutreplacement.
By sampling without replacement,each ensemble will likely see different representa-tions of each cluster and can specialize its partitionthe around observed subset.
Furthermore, eachensemble will observe less noise and can betterdefine each true cluster (Monti et al, 2003).
Wenote that since each ensemble only observes anincomplete subset of D, some clusters may not berepresented at all in some partitions.Heterogeneous Ensembles create diverse parti-tions by simply using complete partitions over Dfrom different clustering algorithms, either due todifferent parameters or due to completely differentclustering models (Strehl et al, 2002).26Combined Heterogeneous and Homogeneous En-sembles can be created by creating many homo-geneous variations of each distinct clustering algo-rithm within a heterogeneous ensemble.
In thisframework, each single method can be boosted bysubsampling the data in order to observe the trueclusters and then combined with other algorithmsusing differing cluttering criteria.2.2 Combining data partitionsGiven the set of partitions, ?
= {pii, ?
?
?
, pih}, theconsensus algorithm must find a final partition, pi?that best minimizes Equation 1.
We find an approx-imation to pi?
using the following algorithms.Agglomerative Clustering first creates a consen-sus matrix, M that records the aggregate decisionsmade by each partition.
Formally, M records thefraction of partitions that observed two data pointand assigned them to the same cluster:M(i, j) =?hk=1 1{di, dj ?
pick}?hk=1 1{di, dj ?
pik}Where di refers to element i, pick refers to cluster c inpartition pik, and 1{?}
is the indicator function.
Theconsensus partition, pi?
is then the result of creat-ing C partitions with Agglomerative Clustering us-ing the Average Link criterion and M as the simi-larity between each data point (Monti et al, 2003).Best of K simply sets pi?
as the partition pih ?
?that minimizes Equation 1 (Goder and Filkov, 2008).Best One Element Move begins with an initialconsensus partition p?i?
and repeatedly changes theassignment of a single data point such that Equa-tion 1 is minimized and repeats until no move canbe found.
We initialize this with Best of K.Filtered Stochastic Best One Element Movealso begins with an initial consensus partition p?i?
andrepeatedly finds the best one element move, but doesnot compare against every partition in ?
for each it-eration.
It instead maintains a history of move costsand updates that history with a stochastically se-lected partition from ?
for each move iteration andends after some fixed number of iterations (Zheng etal., 2011).Figure 2: The general Word Sense Induction Model:models extract distributional data from contexts andinduce senses by clustering the extracted informa-tion.
Models then use representations of each senseto disambiguate new contexts.3 Word Sense Induction ModelsWord Sense Induction models define word senses interms of the distributional hypothesis, whereby themeaning of a word can be defined by the surround-ing context (Haris, 1985).
Rather than form a singlerepresentation for any word, induction models repre-sent the distinct contexts surrounding a multi-senseword and find commonalities between the observedcontexts by clustering.
These similar contexts thendefine a particular word sense and can be used tolater recognize later instances of the sense, Figure 2.Models can be roughly categorized based on theircontext model and their clustering algorithm intotwo categories: feature vector methods and graphmethods.
Feature vector methods simply transformeach context into a feature vector that records con-textual information and then cluster with any algo-rithm that can partition individual data points.
Graphmethods build a large distributional graph that mod-els lexical features from all contexts and then parti-tions the graph using a graph-based clustering algo-rithm.
In both cases, models disambiguate new usesof a word by finding the sense with the most featuresin common with the new context.3.1 Context ModelsContext models follow the distributional hypothesisby encoding various lexical and syntactic featuresthat frequently occur with a multi-sense word.
Eachcontext model records different levels of informa-tion, and in different formats, but are limited to fea-27tures available from syntactic parsing.
Below wesummarize our context models which are based onprevious induction systems:Word Co-occurence (WoC) acts as the core fea-ture vector method and has been at the core of nearlyall systems that model distributional semantics (Ped-ersen, 2006).
The WoC model represents each con-text simply as the words within ?W words fromthe multi-sense word.
Each co-occurring word isweighted by the number of times it occurs withinthe window.Parts of Speech (PoS) extends the WoC modelby appending each lexical feature with its part ofspeech.
This provides a simple disambiguationof each feature so that words with multiple partsof speech are not conflated into the same feature.
(Brody et al, 2006).Dependency Relations (DR) restrains word co-occurrence to words that are reachable from themulti-sense word via a syntactic parse composedof dependency relationships limited by some length(Pado?
and Lapata, 2007).
We treat each reachableword and the last relation in the path as a feature(Van de Cruys and Apidianaki, 2011).Second Order Co-occurrence (SndOrd) providesa rough compositional approach to representing sen-tences that utilizes word co-occurrence and partiallysolves the data sparsity problem observed with theWoC model.
The SndOrd model first builds a largedistributional vector for each word in a corpus andthen forms context vectors by adding the distribu-tional vector for each co-occurring context word(Pedersen, 2006).Graph models encode rich amounts of linguisticinformation for all contexts as a large distributionalgraph.
Each co-occurring context word is assigneda node in the graph and edges are formed betweenany words that co-occur in the same context.
Thegraph is refined by comparing nodes and edges to alarge representative corpus and dropping some oc-currences (Klapaftis and Manandhar, 2010).Latent Factor Models projects co-occurrence in-formation into a latent feature space that ties to-gether relationships between otherwise distinct fea-tures.
We consider three latent models: the SingularValue Decomposition (SVD) (Schu?tze, 1998), Non-negative Matrix Factorization (NMF) (Van de Cruysand Apidianaki, 2011), and Latent Dirichlet Alloca-tion (Brody and Lapata, 2009).
We note that SVDand NMF operate as a second step over any featurevector model whereas LDA is a standalone model.3.2 Clustering AlgorithmsDistributional clustering serves as the main toolfor detecting distinct word senses.
Each algorithmmakes unique assumptions about the distribution ofthe dataset and should thus serve well as diversemodels, as needed by supervised ensembles (Diet-terich, 2000).
While many WSI models automat-ically estimate the number of clusters for a word,we initially simplify our evaluation by assuming thenumber of clusters is known a priori and instead fo-cus on the distinct underlying clustering algorithms.Below we briefly summarize each base algorithm:K-Means operates over feature vectors and iter-atively refines clusters by associating each contextvector with its most representative centroid and thenreformulating the centroid (Pedersen and Kulkarni,2006).Hierarchical Agglomerative Clustering can beapplied to both feature vectors and collocationgraphs.
In both cases, each sentences or collocationvertex is placed in their own clusters and then thetwo most similar clusters are merged together into anew cluster (Schu?tze, 1998).Spectral Clustering separates an associativitymatrix by finding the cut with the lowest conduc-tance.
We consider two forms of spectral clustering:EigenCluster (Cheng et al, 2006), a method origi-nally designed to cluster snippets for search resultsinto semantically related categories, and GSpec (Nget al, 2001), a method that directly clusters a collo-cation graph.Random Graph Walks performs a series of ran-dom walks through a collocation graph in order todiscover nodes that serve as central discriminativepoints in the graph and tightly connected compo-nents in the graph.
We consider Chinese Whispers(Klapaftis and Manandhar, 2010) and a hub selec-tion algorithm (Agirre and Soroa, 2007b).284 Proposed EvaluationWe first propose evaluating ensemble configurationsof Word Sense Induction models using the standardshared tasks from SemEval-1 (Agirre and Soroa,2007a) and SemEval-2 (Manandhar et al, 2010).We then propose comparing these results, and pastSemEval results, to supervised baselines as a gaugeof how well the algorithms do compared to more in-formed models.
We then finally propose an intrin-sic evaluation that rates the semantic interpretabilityand uniqueness of each induced sense.Evaluating Ensemble Configurations must bedone to determine which variation of EnsembleClustering best applies to the Word Sense Inductiontasks.
Preliminary research has shown that Homoge-neous ensemble combined with the HAC consensusfunction typically improve base models while com-bining heterogeneous induction models greatly re-duces performance.
We thus propose various sets ofensembles to evaluate whether or not certain contextmodels or clustering algorithms can be effectivelycombined:1. mixing different feature vector models with the sameclustering algorithm,2.
mixing different clustering algorithms using the samecontext model,3.
mixing feature vector context models and graph contextmodels using matching clustering algorithms,4.
mixing all possible models,5.
and improving each heterogeneous algorithm by firstboosting them with homogeneous ensembles.SemEval Shared Tasks provide a shared corpusand evaluations for comparing different WSI Mod-els.
Both shared tasks from SemEval provide a cor-pus of training data for 100 multi-sense words andthen compare the induced sense labels generated fora set of test contexts with human annotated senseusing a fixed sense inventory.
The task provides twoevaluations: an unsupervised evaluation that treatseach set of induced senses as a clustering solutionand measures accuracy with simple metrics such asthe Paired F-Score, V-Measure, and Adjusted Mu-tual Information; and a supervised evaluation thatbuilds a simple supervised word sense disambigua-tion system using the sense labels (Agirre and Soroa,2007a; Manandhar et al, 2010).Supervised Baselines should set an upper limiton the performance we can expect from most unsu-pervised algorithms, as has been observed in otherNLP tasks.
We train these baselines by using featurevector models in combination with the SemEval-1dataset1.
We propose several standard supervisedmachine learning algorithms as different baselines:Naive Bayes, Logistic Regression, Decision Trees,Support Vector Machines, and various ensembles ofeach such as simple Bagged Ensembles.Semantic Coherence evaluations balance theshared task evaluations by functioning without asense inventory.
Any evaluation against an existinginventory cannot accurately measure newly detectedsenses, overlapping senses, or different sense gran-ularities.
Therefore, our proposed sense coherencemeasures focus on the semantic quality of a sense,adapted from topic coherence measures (Newmanet al, 2010; Mimno et al, 2011).
These evaluatethe degree to which features in an induced sense de-scribe the meaning of the word sense, where highlyrelated features constitute a more coherent sense andunrelated features indicate an incoherent sense.
Fur-thermore, we adapt the coherence metric to evaluatethe amount of semantic overlap between any two in-duced senses.5 Concluding RemarksThis research will better establish the benefit ofEnsemble Clustering when applied to unsuper-vised Natural Language Processing tasks that cen-ter around clustering by examining which featurespaces and algorithms can be effectively combinedalong with different different ensemble configura-tions.
Furthermore, this work will create new base-lines that evaluate the inherent challenge of WordSense Induction and new automated and knowledgelean measurements that better evaluate new or over-lapping senses learned by induction systems.
All ofthe work will be provided as part of a flexible opensource framework that can later be applied to newcontext models and clustering algorithms.1We cannot use graph context models as they do not modelcontexts individually, nor can we use the SemEval-2 dataset be-cause the training set lacks sense labels needed for training su-pervised systems29ReferencesEneko Agirre and Aitor Soroa.
2007a.
Semeval-2007task 02: evaluating word sense induction and discrim-ination systems.
In Proceedings of the 4th Interna-tional Workshop on Semantic Evaluations, SemEval?07, pages 7?12, Stroudsburg, PA, USA.
Associationfor Computational Linguistics.Eneko Agirre and Aitor Soroa.
2007b.
Ubc-as: agraph based unsupervised system for induction andclassification.
In Proceedings of the 4th Interna-tional Workshop on Semantic Evaluations, SemEval?07, pages 346?349, Stroudsburg, PA, USA.
Associ-ation for Computational Linguistics.Samuel Brody and Mirella Lapata.
2009.
Bayesian wordsense induction.
In Proceedings of the 12th Con-ference of the European Chapter of the Associationfor Computational Linguistics, EACL ?09, pages 103?111, Stroudsburg, PA, USA.
Association for Compu-tational Linguistics.Samuel Brody, Roberto Navigli, and Mirella Lapata.2006.
Ensemble methods for unsupervised wsd.
InProceedings of the 21st International Conference onComputational Linguistics and 44th Annual Meet-ing of the Association for Computational Linguistics,pages 97?104, Sydney, Australia, July.
Association forComputational Linguistics.David Cheng, Ravi Kannan, Santosh Vempala, and GrantWang.
2006.
A divide-and-merge methodology forclustering.
ACM Trans.
Database Syst., 31:1499?1525, December.Thomas G. Dietterich.
2000.
Ensemble methods in ma-chine learning.
In Proceedings of the First Interna-tional Workshop on Multiple Classifier Systems, MCS?00, pages 1?15, London, UK.
Springer-Verlag.Andrey Goder and Valdimir Filkov, 2008.
ConsensusClustering Algorithms: Comparison and Refinement.,pages 109?117.Zellig Haris, 1985.
Distributional Structure, pages 26?47.
Oxford University Press.Ioannis P. Klapaftis and Suresh Manandhar.
2010.
Wordsense induction & disambiguation using hierarchicalrandom graphs.
In Proceedings of the 2010 Confer-ence on Empirical Methods in Natural Language Pro-cessing, EMNLP ?10, pages 745?755, Stroudsburg,PA, USA.
Association for Computational Linguistics.Suresh Manandhar, Ioannis Klapaftis, Dmitriy Dligach,and Sameer Pradhan.
2010.
Semeval-2010 task 14:Word sense induction & disambiguation.
In Proceed-ings of the 5th International Workshop on SemanticEvaluation, pages 63?68, Uppsala, Sweden, July.
As-sociation for Computational Linguistics.David Mimno, Hanna Wallach, Edmund Talley, MiriamLeenders, and Andrew McCallum.
2011.
Optimizingsemantic coherence in topic models.
In Proceedings ofthe 2011 Conference on Emperical Methods in Natu-ral Language Processing, pages 262?272, Edinburgh,Scotland, UK.
Association of Computational Linguis-tics.Stefano Monti, Pablo Tamayo, Jill Mesirov, and ToddGolub.
2003.
Consensus clustering ?
a resampling-based method for class discovery and visualization ofgene expression microarray data.
Machine Learning,52:91?118, July.David Newman, Youn Noh, Edmund Talley, SarvnazKarimi, and Timothy Baldwin.
2010.
Evaluating topicmodels for digital libraries.
In Proceedings of the 10thannual joint conference on Digital libraries, JCDL?10, pages 215?224, New York, NY, USA.
ACM.A.
Ng, M. Jordan, and Y. Weiss.
2001.
On Spectral Clus-tering: Analysis and an algorithm.
In T. Dietterich,S.
Becker, and Z. Ghahramani, editors, Advances inNeural Information Processing Systems, pages 849?856.
MIT Press.Sebastian Pado?
and Mirella Lapata.
2007.
Dependency-Based Construction of Semantic Space Models.
Com-putational Linguistics, 33(2):161?199.Ted Pedersen and Anagha Kulkarni.
2006.
Automaticcluster stopping with criterion functions and the gapstatistic.
In Proceedings of the 2006 Conference of theNorth American Chapter of the Association for Com-putational Linguistics on Human Language Technol-ogy: companion volume: demonstrations, NAACL-Demonstrations ?06, pages 276?279, Stroudsburg, PA,USA.
Association for Computational Linguistics.Ted Pedersen.
2006.
Unsupervised corpus-based meth-ods for WSD.
In Word Sense Disambiguation: Algo-rithms and Applications, pages 133?166.
Springer.Hinrich Schu?tze.
1998.
Automatic word sense discrimi-nation.
Comput.
Linguist., 24:97?123, March.Alexander Strehl, Joydeep Ghosh, and Claire Cardie.2002.
Cluster ensembles - a knowledge reuse frame-work for combining multiple partitions.
Journal ofMachine Learning Research, 3:583?617.Tim Van de Cruys and Marianna Apidianaki.
2011.
La-tent semantic word sense induction and disambigua-tion.
In Proceedings of the 49th Annual Meeting ofthe Association for Computational Linguistics: Hu-man Language Technologies - Volume 1, HLT ?11,pages 1476?1485, Stroudsburg, PA, USA.
Associationfor Computational Linguistics.Haipeng Zheng, S.R.
Kulkarni, and V.H.
Poor.
2011.Consensus clustering: The filtered stochastic best-one-element-move algorithm.
In Information Sciencesand Systems (CISS), 2011 45th Annual Conference on,pages 1 ?6, march.30
