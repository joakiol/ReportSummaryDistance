Multilingual Text Entry using Automatic Language DetectionYo Ehara and Kumiko Tanaka-IshiiGraduate School of Information Science and Technology, University of Tokyo13F Akihabara Daibiru, 1-18-13 SotoKanda Chiyoda-ku, Tokyo, Japanehara@r.dl.itc.u-tokyo.ac.jp kumiko@i.u-tokyo.ac.jpAbstractComputer users increasingly need to pro-duce text written in multiple languages.However, typical computer interfaces re-quire the user to change the text entry soft-ware each time a different language is used.This is cumbersome, especially when lan-guage changes are frequent.To solve this problem, we propose TypeAny,a novel front-end interface that detects thelanguage of the user?s key entry and au-tomatically dispatches the input to the ap-propriate text entry system.
Unlike previ-ously reported methods, TypeAny can han-dle more than two languages, and can easilysupport any new language even if the avail-able corpus is small.When evaluating this method, we obtainedlanguage detection accuracy of 96.7% whenan appropriate language had to be chosenfrom among three languages.
The number ofcontrol actions needed to switch languageswas decreased over 93% when using Ty-peAny rather than a conventional method.1 IntroductionGlobalization has increased the need to producemultilingual text ?
i.e., text written in more thanone language ?
for many users.
When producinga text in a language other than English, a user hasto use text entry software corresponding to the otherlanguage which will transform the user?s key strokesequences into text of the desired language.
Suchsoftware is usually called an input method engine(IME) and is available for each widely used lan-guage.
When producing a multilingual text on atypical computer interface, though, the user has toswitch IMEs every time the language changes in amultilingual text.
The control actions to choose anappropriate IME are cumbersome, especially whenthe language changes frequently within the text.To solve this problem, we propose a front-end in-terface called TypeAny.
This interface detects thelanguage that the user is using to enter text and dy-namically switches IMEs.
Our system is situatedbetween the user key entry and various IMEs.
Ty-peAny largely frees the user from the need to exe-cute control actions when switching languages.The production of multilingual text involves threekinds of key entry action:?
actions to enter text?
actions to control an IME1?
actions to switch IMEsRegarding the first and second types, substantialwork has been done in the UI and NLP domain,as summarized in (MacKenzie and Tanaka-Ishii,2007).
There has especially been much work re-garding Chinese and Japanese because in these lan-guages the number of actions of the second type isclosely related to the accuracy of conversion fromRomanized transcription to characters in each ofthese languages, and this directly reflects the capa-bility of the language model used.1When using predictive methods such as completion, orkana-kanji conversion in Japanese, the user has to indicate tothe IME when it should predict and which proposed candidateto choose.441In contrast, this paper addresses the question ofhow to decrease the need for the third type of action.From the text entry viewpoint, this question has re-ceived much less attention than the need to reducethe number of actions of the second type.
As far aswe know, this issue has only been directly addressedby Chen et al (2000), who proposed integrating En-glish entry into a Chinese input system rather thanimplementing multilingual input.Reports on ways to detect a change in the lan-guage used are more abundant.
(Murthy and Ku-mar, 2006) studied the language identification prob-lem based on small samples in several Indian lan-guages when machine learning techniques are used.Although they report a high accuracy for the methodthey developed, their system handles switches be-tween two Indian languages only.
In contrast, Ty-peAny can handle any number of languages mixedwithin a text.
(Alex, 2005) addresses a related task, called for-eign inclusion detection (FID).
The task is to findforeign (i.e., English) inclusions, such as foreignnoun compounds, within monolingual (i.e., Ger-man) texts.
Alex reported that the use of FID tobuild a polyglot TTS synthesizer was also consid-ered (Pfister and Romsdorfer, 2003), (Marcadet etal., 2005).
Recently, Alex used FID to improve pars-ing accuracy (Alex et al, 2007).
While FID relieson large corpora and lexicons, our model requiresonly small corpora since it incorporates the transi-tion probabilities of language switching.
Also, whileFID is specific to alphabetic languages, we made ourmethod language-independent by taking into consid-eration the inclusion problem at the key entry level.In the following, we introduce the design of Ty-peAny, explain its underlying model, and report onour evaluation of its effectiveness.2 Design of TypeAnyFigure 1 shows an example of text written in En-glish, Japanese and Russian.
The strings shown be-tween the lines indicate the Roman transcription ofJapanese and Russian words.With a conventional computer interface, enteringthe text shown in Figure 1 would require at least sixcontrol actions since there are six switches betweenlanguages: from English to Japanese and back, andFigure 1: Example of Multilingual Text in English,Japanese and RussianFigure 2: System Structuretwice from English to Russian and back.
Note thatsuch IME switches are also required even when thetext consists only of European languages.
Each Eu-ropean language has its own font and diacritic sys-tem, which are realized by using IMEs.TypeAny solves the problem of changing IME.
Itis situated between the user?s key entry and variousIMEs as shown in the system architecture diagramof Figure 2.
The user?s key entry sequence is inputto our client software.
The client sends the sequenceto the server which has the language identifier mod-ule.
This module detects the language of the keysequence and then sends the key sequence to the ap-propriate IME2.
The selected IME then converts thekey entries into text of the detected language.In our study, IMEs for European languages arebuilt using simple transliteration: e.g., ?[?
typed inan English keyboard is transliterated into ?u??
of Ger-man.
In contrast, the IMEs for Japanese and Chinese2Precisely speaking, TypeAny detects keyboard layouts (i.e.,Qwerty, Dvorak, Azerty, etc.)
as well as the languages used.442Figure 3: Entry Flowrequire a more complicated system because in theselanguages there are several candidate transcriptionsfor a key sequence.
Fortunately, several existingsoftware resources can be used for this.
We use An-thy3 as the IME for Japanese.
As for the IME forChinese, we used a simple word-based pinyin-hanziconversion system.TypeAny restarts the language detection everytime a certain delimiter appears in the user?s key se-quence.
By using delimiters, the system can avoidresorting to a combinatorial search to find the bor-der between languages.
Such delimiters naturallyoccur in natural language texts.
For example, inthe case of European languages, blank spaces areused to delimit words and it is unlikely that two lan-guages will be mixed within one word.
In languagessuch as Chinese and Japanese, blank spaces are typ-ically used to indicate that the entry software shouldperform conversion, thus guaranteeing that the se-quence between two delimiters will consist of onlyone language4.
Therefore, assuming that a text frag-ment between two delimiters is written in just onelanguage is natural for users.
A text fragment be-tween two delimiters is called a token in TypeAny.An example of the TypeAny procedure to enter3http://anthy.sourceforge.jp/4Note that a token can consist of a sequence longer than aword, since many types of conversion software allow the con-version of multiple words at one time.the text from Figure 15 is shown in Figure 3.
Ineach step in Figure 3, the text is entered in the firstline, where the token that the user is entering is high-lighted.
The language estimated from the token isshown in the locale window shown below (called theStatus Window).
Each step proceeds as follows.
(a) The initial state.
(b) The user first wants to type a token ?Some?
inEnglish.
When ?Som?
is typed, the systemidentifies that the entry is in English.
The userconfirms this language by looking at the localewindow.
(c) The user finishes entering the token ?Some?
andwhen the user enters a blank space, the token?Some?
is confirmed as English text.
TypeAnyrestarts detection for the language of the nexttoken.
The tokens up to and including ?offer?are entered similarly to ?Some?.
(d) The user types in a token ?ikura?
in Japanese.The moment ?iku?
is typed, ?iku?
is identifiedas Japanese, as is confirmed by the user throughthe locale window.
(e) When the user finishes entering the token?ikura?
and types in a blank space, the se-quence is sent to a Japanese IME to be con-verted into ?ikura?, so that a Japanese text frag-ment is obtained.
(f) Through conventional kana-kanji conversion,5This case assumes use of the Qwerty keyboard.443Figure 4: When Detection Failsthe user can select the appropriate conversionof ?ikura?
among from candidates shown in theLookup Window and the token is confirmed.TypeAny begins detecting the language of thenext token.
The tokens between ?or?
and ?Rus-sian?
are successfully identified as English in away similar to procedures (b) and (c).
(g) The key entry ?brhf?
is the key sequence for theRussian token whose English transliteration is?ikra?.
(h) Since ?brhf?
is identified as Russian, ?brhf?
isconverted into Russian characters.
(i) The following word ?Caviar?
is detected as En-glish, as in (b) and (c).As seen in this example, the user does not need totake any action to switch IMEs to enter tokens ofdifferent languages.Two types of detection failure occur in TypeAny:Failure A: the language should switch, but the newlanguage is incorrectly selected.Failure B: the language should not switch, but Ty-peAny misjudges that it should.While conventional methods require a control ac-tion every time the language switches, TypeAny re-quires a control action only to correct such a failure.Therefore, Failure A never increases the numberof control actions compared to that of conventionalmethods.
On the other hand, Failure B errors area concern as such failures might increase the num-ber of control actions to beyond the number requiredby a conventional method.
Thus, the effectivenessof introducing TypeAny depends on a trade-off be-tween fewer control actions at language switchingpoints and potentially more control actions due toFailure B errors.
Our evaluation in ?4.2 shows thatthe increase in the number of actions due to FailureB errors is insignificant.In the event of failures, the user can see that thereis a problem by watching the locale window andthen easily correct the language by pressing the TABkey.
For example, while ?in?
was correctly judgedfor our example, suppose it is incorrectly detected asJapanese as shown in Figure 4(a).
In this case, theuser can manually correct the locale by pressing theTAB key once.
The locale is then changed from Fig-ure 4(a) (where ?in?
is identified as Japanese), to (b)where ?in?
is identified as English.Note that the language of some tokens will be am-biguous.
For example, the word ?sushi?
can be bothEnglish and Japanese because ?sushi?
has almost be-come an English word: many loan words share thisambiguity.
Another case is when diacritic marks areconsidered: for example, the word ?fur?
is usuallyan English word, but some German users may wishto use this word as ?fu?r?
without diacritic marks.Such a habit is widely seen among users of Euro-pean languages.
Some of this sort of ambiguity isdisambiguated by considering the context and by on-line learning, which is incorporated in the detectionmodel as explained next.3 Language Detection3.1 Language Detection ModelWe modeled the language detection as a hiddenMarkov model (HMM) process whose states corre-spond to languages and whose outputs correspond totokens from a language.Here, the goal is to estimate the languages l?m1 bymaximizing P(lm1 , tm1 ), where l ?
L denotes a lan-guage in L, a set of languages, and t denotes a to-ken6.
By applying a hidden Markov model, the max-imization of P(lm1 , tm1 ) is done as shown in Equa-tion (1).l?m1 = argmaxlm1 ?LP(lm1 , tm1 )= argmaxlm1 ?LP(tm1 |lm1 )P(lm1 )?
argmaxlm1 ?L( m?i=1P (ti|li))( m?i=1P (li|li?1i?k))(1)In the last transformation of Equation (1), itis assumed that P(tm1 |lm1 ) ?
?mi=1 P (ti|li) andP(li|li?11 ) ?
P (li|li?1i?k) for the first and the secondterms, respectively.
In Equation (1), the first term6Let tvu = (tu, tu+1, .
.
.
, tv) be an ordered list consisting ofv ?
u+ 1 elements for v ?
u.444corresponds to the output probabilities and the sec-ond term corresponds to the transition probabilities.In a usual HMM process, a system finds the lan-guage sequence (i.e., state sequence) lm1 that maxi-mizes Equation (1) by typically using a Viterbi algo-rithm.
In our case, too, the system can estimate thelanguage sequence for a sequence of tokens.
How-ever, as discussed earlier, since it is unlikely that auser enters a token consisting of multiple languages,our system is designed only to estimate the languageof the latest token lm, supposing that the languagesof the previous lm?11 are correct.In the following two sections, the estimation ofeach term is explained.3.2 Output ProbabilitiesThe output probabilities P (ti|li) indicate the proba-bilities of tokens in a monolingual corpus, and theirmodeling has been substantially investigated in NLP.Note that the estimation of P (ti|li) requiresmonolingual corpora.
If the corpora are large,P (ti|li) is estimated from the token frequencies.However, because large corpora are not alwaysavailable, especially for minor languages, P (ti|li)is estimated using key entry sequence probabilitiesbased on n-grams (with maximum n being nmax) asfollows:P (ti|li) = P (c|ti|1 |li) =|ti|?r=1P (cr|cr?1r?nmax+1, li)(2)In Equation (2), ti = c|ti|1 and |ti| is the length of tiwith respect to the key entry sequence.
For exam-ple, in the case of ti=?ikura?, |ti| = 5 and c1=?i?,c2=?k?, c3=?u?
and |ti|=5.
Here, each probabilityP (cr|cr?1r?nmax+1, li) needs to be smoothed.Values of P (ti|li) are estimated from monolin-gual corpora.
If the corpora are large, P (ti|li) isestimated from the token frequencies.
However, be-cause large corpora are not always available, espe-cially for minor languages, P (ti|li) is estimated us-ing smoothed character-based n-grams.
Predictionby Partial Matching, or PPM is adopted for this task,since it naturally incorporates online learning and itis effective in various NLP tasks as reported in (Tea-han et al, 2000) and (Tanaka-Ishii, 2006).
PPMuses cr?nmax1 as a corpus for training.
PPM is de-signed to predict the next cr by estimating the nmax-gram probability P (cr|cr?1r?nmax+1) using backing-off techniques with regard to the current context.Precisely, the probability is estimated as a weightedsum of different (n + 1)-gram probabilities up to afixed nmax-gram as follows:P (cr|cr?1r?nmax+1) =nmax?1?n=?1wnpn(cr) (3)The weights wn are determined through escapeprobabilities.
Depending on how the escape prob-abilities are calculated, there are several PPM vari-ants, which are named PPMA, PPMB, PPMC, andso on.
PPMC, the one that we have used, is alsoknown as Witten-Bell smoothing in the NLP field(Manning and Schuetze, 1999).
The escape proba-bilities are defined as follows.wn = (1?
en)ncont?n?=n+1en?
(?1 ?
n < ncont)(4)wncont = (1?
en)Here, ncont is defined as the maximum n that satis-fies Xn 6= 0.
Let Xn be the number of cr?1r?n, xn bethe number of crr?n and qn be the number of differ-ent keycodes followed by cr?1r?n found in cr?n?11 .Using these notations, pn(cr) is defined aspn(cr) = xnXn (5)In PPMC, the escape probabilities are calculated asen = qnXn + qn (6)For further details, see (Bell et al, 1990).3.3 Language Transition ProbabilitiesOnly a small corpus is typically available to esti-mate P (lm|lm?1m?kmax+1), where kmax is the longestk-gram in the language sequence to be considered.Thus, the transition probability is estimated on-line,making use of language that will be corrected inter-actively by the user.
For this on-line learning, weadopted PPM as well as the output probabilities.Note that a large kmax may reduce accuracy,which is intuitively explained as follows.
Whilethere is typically a high probability that the subse-quent language will be the same as the current lan-guage, it is unlikely that any language sequence willhave long regular patterns.
Therefore, kmax shouldbe fixed according to this consideration.4450204060801002  3  4  5  6  7  8Accuracy(%)Number of languagesPPMMLbaselineFigure 5: Detection Accuracy Test175808590951002  3  4  5  6  7  8Accuracy(%)Number of languagesPPMMLbaselineFigure 6: Detection Accuracy Test24 EvaluationWe evaluated TypeAny with respect to two mea-sures: language detection capability when using arti-ficially generated multilingual corpora, and the num-ber of required control actions when using actualmultilingual corpora.4.1 Language Detection AccuracyThe ideal experiment would be to use actual multi-lingual corpora for many language sets.
However, itis still difficult to collect a large amount of multilin-gual corpora with adequate quality for the test dataof languages.Therefore, we measured the language detectionaccuracies using artificially generated multilingualcorpora by mixing monolingual corpora for everycombination varying from two to eight languages.First, the following monolingual corpora werecollected: editions of the Mainichi newspaper in2004 for Japanese, the Peking University corpusfor Chinese, and the Leipzig corpora (Biemann etal., 2007) for English, French, German, Estonian,Finnish and Turkish.
The text of each of these cor-pora was transformed into a sequence of key entries.Two test sets, Test1 and Test2, were generated byusing different mixture rates.
In Test1, languageswere mixed uniformly and randomly, whereas inTest2 a major language accounted for 90% of thetext and the remaining 10% included different lan-guages chosen uniformly and randomly.
Test2 ismore realistic since a document is usually composedin one major language.The output and language transition probabilitieswere estimated and smoothed using PPMC as de-scribed in ?3.
Since part of the target of the exper-iment was to clarify the relation between learningsize and accuracy, the output probabilities and tran-sition probabilities were not trained on-line whilethe text was entered using PPMC, thus accuracy wasmeasured by fixing the language model at this initialstate.
We used nmax = 5 for the output probabilityand kmax = 1 for the transition probability since thedistribution of languages in the corpus was uniformhere as we generated it uniformly.
(See formula (4)in ?3.2).A 10-fold cross validation was applied to the gen-erated corpora.
Each generated corpus was 111Kbytes in size, consisting of a disjoint 100-Kbytetraining part and an 11-Kbyte testing part.
The out-put probabilities were trained using the 100-Kbytetraining part.
The language transition probabilitieswere trained using about 2000 tokens.The results for Test1 and Test2 are shown in Fig-ure 5 and Figure 6, respectively.
The horizontalaxis shows the number of languages and the ver-tical axis shows the detection accuracy.
There arethree lines: PPM indicates that the transition proba-bilities were trained by PPM; ML indicates that notransition probability was used and the language wasdetected using only output probabilities (maximumlikelihood only); Baseline is the accuracy when themost frequent language is always selected.446As shown in Figure 5 (Test1), when the mix-ture was uniform, the PPM performance was slightlylower but very close to that of ML.
This was becausePPM would be theoretically equivalent to ML withinfinite learning of language transition probabilities,since languages were uniformly distributed in Test1.These results show that our PPM for transition prob-abilities learns this uniformity in Test1.As shown in Figure 6, PPM clearly outperformedML in Test2.
This was because ML has no wayto learn the transition probability, which was biasedwith the major language being used 90% of the time.This shows that the introduction of language transi-tion probabilities accounts for higher performance.Interestingly, ML falls below the baseline case whenmore than three languages were used in Test2, a sit-uation that has rarely been considered in previousstudies.
This suggests that language detection usingonly ML requires large corpora for learning to selectone appropriate language, and that this requirementcan be alleviated by using PPM.Another finding is that the detection accuracy de-pends on the language set.
For example, the accu-racy for language sets consisting of both French andEnglish tended to be lower than for other languagesets due to the spelling closeness between these twolanguages.
For example, the accuracy for test dataconsisting of 90% English, 5% French and 5% Ger-man was 94.4%.
This is not surprising since the de-tection was made only within a token (which cor-responds to a word in European languages): natu-rally there were many words whose language wasambiguous within the test set.
In contrast, high ac-curacies were obtained for test sets consisting of lan-guages more different in their nature.
We obtained97.5% accuracy for test data consisting of 90% En-glish, 5% Finnish and 5% Turkish; this accuracy washigher than the average for all test sets.4.2 Number of Control ActionsThe second evaluation was done to compare thenumber of control actions needed to switch lan-guages with TypeAny and with a conventionalmethod.
As mentioned in ?1, three types of key-board actions are used when entering text.
Our work7E.
: English, J.: Japanese and C.: Chinese.8http://en.wikitravel.org/9http://en.wikipedia.org/Table 1: Articles Used in the Decrease Testarticle Article 1 Article 2Foreign tokens 286 55Total tokens 1725 5100Inclusion ratio 16.6% 1.1%languages E., J. E., J., C.7content Introductionof Japanesephrases fortravelingAbout tofu(bean curd)Source Wikitravel 8 Wikipedia 9Table 2: Required Number of Control ActionsArticle 1 Article 2Conventional 572 110Number of switches (100%) (100%)Ours Failure A 2.8% 3.6%Failure B 1.6% 2.7%Total Failures 4.4% 6.3%Decrease 95.6% 93.6%only concerns the control action to switch language,though, and the comparison in this section focuseson this type of action.This evaluation was done using two samples ofactual multilingual text collected from the Web.
Thefeatures of these samples are shown in the top blockof Table 1.
In both cases, the major language wasEnglish.For each of these articles, the number of con-trol actions required with the conventional methodand with TypeAny was measured.
The conventionalmethod requires a control action every time the lan-guage switches.
For TypeAny, control actions are re-quired only to correct language detection failures.
Inboth cases, the action required to switch languagesor correct the language was counted as one action.For the language model, the output probabilitieswere first trained using the 100-Kbyte monolingualcorpora collected for the previous evaluation.
Thetransition probabilities were not trained beforehand;i.e., the system initially regarded the languages to beuniformly distributed.
Since this experiment was in-tended to simulate a realistic case, both output and447transition probabilities were trained on-line usingPPMC while the text was entered.
Here, both nmaxand kmax were set at 5.The results are shown in Table 2.
First, some de-tection errors occurred for Article 2 because ?tofu?was detected as Japanese at the beginning of entry,even though it was used as an English word in theoriginal text.
As noted at the end of ?2, such loanwords can cause errors.
However, since our systemuses PPM and learns on-line, our system learned that?tofu?
had to be English, and such detection errorsoccurred only at the beginning of the text.Consequently, there was a substantial decrease inthe number of necessary control actions with Ty-peAny, over 93%, for both articles.
An especiallylarge decrease was observed for Article 2, eventhough the text was almost all in English (98.9%).There was only a small increase in the incidence rateof Failure B for Article 2, so the total decrease in thenumber of required actions was still large, puttingto rest the concern discussed in ?2.
These resultsdemonstrate the effectiveness of our approach.5 ConclusionTypeAny is a novel multilingual text input interfacein which the languages used for entries are detectedautomatically.
We modeled the language detectionas an HMM process whose transition probabilitiesare estimated by on-line learning through the PPMmethod.This system achieved language detection accu-racy of 96.7% in an evaluation where it had tochoose the appropriate language from among threelanguages with the major language accounting for90% of the sample.
In addition, the number of con-trol actions required to switch IMEs was decreasedby over 93%.
These results show the promise of oursystem and suggest that it will work well under real-istic circumstances.An interesting objection might be raised to theconclusions of this study: some users might findit difficult to watch the locale window all the timeand prefer the conventional method despite havingto work with a large number of key types.
We plan toexamine and clarify the cognitive load of such usersin our future work.ReferencesB.
Alex, A. Dubey, and F. Keller.
2007.
Using foreign in-clusion detection to improve parsing performance.
InProceedings of EMNLP-CoNLL, Prague, Czech, June.B.
Alex.
2005.
An unsupervised system for identifyingEnglish inclusions in German text.
In Proceedings ofthe ACL Student Research Workshop, pages 133?138,Ann Arbor, Michigan, June.
Association for Compu-tational Linguistics.T.
C. Bell, J. G. Clear, and I. H. Witten.
1990.
Text Com-pression.
Prentice-Hall, New Jersey.C.
Biemann, G. Heyer, U. Quasthoff, and M. Richter.2007.
The Leipzig corpora collection - monolingualcorpora of standard size.
In Proceedings of CorpusLinguistics, Birmingham, United Kingdom, July.Z.
Chen and K. Lee.
2000.
A new statistical approachto Chinese input.
In The 38th Annual Meeting of theAssociation for Computer Linguistics, pages 241?247,Hong Kong, October.I.
S. MacKenzie and K. Tanaka-Ishii.
2007.
Text EntrySystems ?Mobility, Accessibility, Universality?.
Mor-gan Kaufmann.C.
D. Manning and H. Schuetze.
1999.
Foundations ofStatistical Natural Language Processing.
MIT Press.J.-C. Marcadet, V. Fischer, and C. Waast-Richard.
2005.A transformation-based learning approach to languageidentification for mixed-lingual text-to-speech synthe-sis.
In Interspeech 2005 - ICSLP, pages 2249?2252,Lisbon, Portugal.K.
N. Murthy and G. B. Kumar.
2006.
Language identi-fication from small text samples.
Journal of Quantita-tive Linguistics, 13:57?80.B.
Pfister and H. Romsdorfer.
2003.
Mixed-lingual anal-ysis for polyglot TTS synthesis.
In Eurospeech, pages2037?2040, Geneva, Switzerland.K.
Tanaka-Ishii.
2006.
Word-based text entry techniquesusing adaptive language models.
Journal of NaturalLanguage Engineering, 13(1):51?74.W.
J. Teahan, Y. Wen, R. MacNab, and I. H. Witten.2000.
A compression-based algorithm for Chineseword segmentation.
In Computational Linguistics,volume 26, pages 375?393.448
