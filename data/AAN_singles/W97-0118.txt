' iThe Effects of Corpus Size and Homogeneity on Language Model QualityTony G. Rose ltgr@cre.canon.co.ukCanon Research Centre Europe Ltd.Surrey Research Park, Guildford, Surrey GU2 5YF UKNicholas J. Haddock, Roger C.F.
Tucker{njh, rcft} @hplb.hpl.hp.comHewlett-Packard LaboratoriesStoke Gifford, Bristol BS12 6QZ UKAbstractGeneric speech recognition systems typically use language models that are trained to cope with a broadvariety of input.
However, many recognition applications are more constrained, often to a specific topicor domain.
In cases such as these, a knowledge of the particular topic can be used to advantage.
Thisreport describes the development of a number of techniques for augmenting domain-specific languagemodels with data from a more general source.Two investigations are discussed.
The first concerns the problem of acquiring a suitable sample of thedomain-specific language data from which to train the models.
The issue here is essentially one ofquality, since it is shown that not all domain-specific corpora are equal.
Moreover, they can displaysignificantly different characteristics that affect he quality of any language models built therefrom.
Thesecharacteristics are defined using a number of statistical measures, and their significance for languagemodelling is discussed.The second investigation concerns the empirical development and evaluation of a set of language modelsfor the task of email speech-u>-text dictation.
The issue here is essentially one of quantity, since it isshown that effective language models can be built from very modestly sized corpora, providing thetraining data matches the target appfication.
Evaluations how that a language model trained on only 2million words can perform better than one trained on a corpus of over 100 times that size.1.
IntroductionThe development of robust speech recognition technology offers great potential for the design ofimproved interfaces to a wide range of applications.
The current project concerns the development of onesuch application: the speech-to-text dictation of email messages.
The work makes use of the Abbotrecogniser, which is a eonnectionist/HMM continuous peech recognition system developed by theConnectionist Speech Group at Cambridge University.
It is designed to recognise British English andAmerican English, clearly spoken in a quiet acoustic environment (I-Ioehberg et al, 1994).The Abbot system is available with a vocabulary of 20,000 words, which means that anything spokenoutside this vocabulary cannot be recognised (and therefore will be recognised as another word or stringof words).
The vocabulary and grammar 0-aM) were optimised for the task of reading from a NorthAmerican Business newspaper, in this case the Wall Street Journal.
Some 227 million words of trainingtext were used in building this LM and it is widely used throughout the speech community.
However,despite the size of the original training corpus, this LM was dearly not designed for the specific task ofemail dictation, so its performance is likely to be sub-optimal.
However, a new vocabulary and LM canThis work was completed at HP Labs during aprevious appoimmcnt funded by the Royal Academy ofEngineering.IIIIIII!!!
!i!II.II178easily be created and then substituted for the one supplied.
The LMs described in this paper were all'back-off trigram' LMs (Katz; 1987), built using the CMU SLM toolkit (Rosenfeld, 1994).2.
Corpus Acquisition2.1 The form of email messagesIn order to build a LM for the task of email dictation, it is necessary to acquire a corpus of suitable mailtraining data.
However, behind this ostensibly simple objective lie several subtle challenges.
"Email", as ageneral term, describes a great variety of types of communication.
These types are perhaps best illustratedby considering the range of functions that email messages typically provide.
For example, email can beused as a medium for:?
a formal face-to-face meeting;?
a casual face-to-face chat;?
a broadcast (e.g.
"Tannoy") message;?
requesting information;?
replacing an office memo;?
replacing aphone call, etc.Clearly, the purpose of each communication can be very different, and the language used will reflect his.Furthermore, apart from the issue of domain (i.e.
subject matter), the type of language used will also varyaccording to the role of the participants.
For example, when requesting advice from a mailing list onewould tend to be more formal and polite than when requesting the same advice from a friend or colleague.Consequently, it would appear that email messages vary almost as much as spontaneous, spoken dialogue.If this is indeed so, then the prospects for building effective language models for email may appearsomewhat limited.
Clearly, in order to move forward, it is necessary to define some limits.The first of these concerns the quantity.
What is a reasonable size for an email corpus?
There are fewprecedents for this so the question was answered empirically, by finding a compromise between the needto acquire sufficient raining data and the need to complete the acquisition phase within a reasonablespace of time.
However, the time taken to reach a certain quantity depends very much on the source of thedata, which forms the second limit: from where should the email be acquired?
A range of possibilitiesexists, e.g.
the Intemet (i.e.
bulletin boards, mailing lists, email archives) or specific individuals (i.e.previously saved messages, day-by-day output).Evidently, emaiI acquired from the Intemet exhibits a wide range of authorship, function and subjectmatter.
In addition, downloading large quantities of text from such sources without he authors' consentmay involve certain copyright issues.
Clearly, the limits of the source are more easily defined if the emailis restricted to the output of a group of specific individuals.
However, unless the group is very large,acquiring just 1 million words from their day-by-day output would be too slow to enable the acquisitionto be completed within a reasonable space of time.
Therefore, individuals with a large collection ofpreviously saved messages were identified as more suitable candidates.
Furthermore, a restriction that allmembers of this group must be employees of HPLB (Hewlett-Packard Labs, Bristol) placed a furtherconstraint on the source.
To ensure controlled authorship, only the outgoing messages of these individualswere collected.2.2 The content of email  messagesThe "content" of an email message is not an easy concept o define.
Evidently, the body contains muchimportant data, but what about the other elements, e.g.
headers, signatures, quoted sections, etc.
- what179roles do they play?
In the case of headers, a cursory analysis revealed that they could safely be discardedsince few contained any useful information.
However, other email components are not quite so easilycategorised, e.g.?
quoted (included) messages: these are usually referred to by the message content, but are often theproduct of a different author;?
emafl 'signatures': these are often quite verbose, but rarely contribute anything to the message content;?
samples of Postscript/Latex: these were problematic, since people would often quote verbatim largepassages to illustrate a point that did indeed contribute to the content of the message.
However, tobuild models from data that included such heavily marked-up text is questionable.Since the above items were all rendered as ASCII strings, their surface form could fairly reliably bepredicted and they were therefore removed from the corpus using suitably designed "filters".
However,there is a further number of email attachments that are not composed of predictable ASCII strings.
Theseinclude items such as shafted or uuencoded files and word processor/DTP output.
Evidently, such itemsneed to be removed, but finding small fragments of such diverse data in a corpus of several million wordsis a non-trivial problem.Alternatively, rather than trying to filter out "noise" from the email "signal", it is possible to adopt theconverse approach, and try to identify those lines that constitute genuine English within the overall emaildata, which may then be retained as the true training data.
It is possible to achieve this using variousheuristics, e.g.
"retain those lines that contain at least 90% English words".
However, this approachassumes there exists some predefmed vocabulary, which is somewhat tautological since the vocabulary isone of the things we seek to define in the first place.2.3 The emall  collection processA programme of email data collection took place over a period of 2-3 weeks, following the principlesdescribed above.
This resulted in the acquisition of some 4 million words of email data.
The "donors"were asked to provide both previously saved messages and intermittent day-by-day output.
This wasnecessary since (by their very nature) saved messages tend to possess ome sort of significant content,and were therefore often of above average length.
In contrast, much day-by-day email correspondenceuses an informal dialogue that is heavily context dependent, and therefore may be no more than a singlebrief phrase or sentence.
This was then filtered in the manner described above.
The final output was acorpus of emall data of 1,962,280 words (49% of the original size).
This was then partitioned (95% : 5%)into training and test data.3.
Corpus  adaptat ionIt is theoretically possible to build a LM using the tiniest of corpora.
On balance, however, the 2 millionwords of email training data look somewhat inadequate compared to the 227 million words used for theWSJ LM.
The problem is that the coverage of the n-grams is likely to be sparse and any LM so built willbe degenerate since it does not reliably predict he characteristics of the source.
To illustrate, consider thedistribution of unigram frequencies: a mere 14,137 word types (19%) in the email corpus havefrequencies of 6 or greater.
Therefore, to acquire a vocabulary of just 20k words without usingfrequencies of 5 or less clearly requires a training corpus larger than 2 million words.It would be highly desirable therefore if a method could be devised whereby information from a largecorpus could be combined with a smaller sample of the domain-specific training data to create an optimallanguage model.
One such approach involves augmenting a base model built from a larger, more generalcorpus with information from a small sample of the domain-specific language.
There is evidence tosuggest that this method can improve recognition performance ( .g.
Ruduicky, 1995, Vergyri, 1.995).
AnIII!i11I1!
I|iIII,II180alternative approach is to use a suitable similarity metric to acquire further "email-like" training data fromthe larger corpus (henceforth referred to as the "background corpus"), and then build a new languagemodel from the combined text.
This approach offers interesting possibilities regarding the development ofa general methodology for corpus adaptation, by attempting to "grow" a suitable corpus of training datafor any domain using only a small sample as a "seed".
A number of ways to implement this techniquehave been developed.
Broadly speaking, they fall into two categories: "top-down" methods and "bottom-up" methods.3.1 The top-down approachAt its simplest, this approach involves a combination of manual inspection and regular expressionsearching to identify those parts of the background corpus that contain suitable material.
It relies on agood classification scheme and reliable organisation of the background corpus.
The British NationalCorpus (BNC) is a suitable xample, since it contains 100 million words of modem English, both spokenand written, sampled from the widest range of materials.
It is annotated with part-of-speech odes, andSGML-encoded according to the Text Encoding Initiative's Guidelines (Bumard, 1995).
It is thereforepossible to use the SGML tags to identify suitable texts.
For example, extracting 10 million words of textfor a domain such as World Affairs is trivially easy, since domain information is encoded in the header ofeach individual file (of which there are over 4,000).Since much HP email concerns the computing business, and the BNC classifies computing as a branch ofApplied Science, it would appear that the 10 million words from Applied Science section of the BNC mayprove sufficiently similar.
Likewise, the 10 million words classified as Commerce and Finance may alsoprove suitable.
The effect of such an addition would be to increase the size of the training corpus from 2million words to 22 million, which constitutes an increase of 1100%.However, methods uch as this cannot be justified by subjective judgement and anecdotal evidence.
Whatis required is an objective measure that reliably identifies which of the domains in the BNC is mostsimilar to HP email.
There may be many standard statistical techniques for measuring the degree ofsimilarity of two data sets, but not all are suitable for the task of comparing corpora (Clmreh et al, 1991).For example, some assume a normal distribution, which is clearly inappropriate for textual data.
What isneeded therefore is a test that makes few assumptions about he distributions of the underlying data, butprovides adirectly usable measure of similarity.
One such test is the rank correlation, using Spearman's S.The assumptions behind rank correlation are few.
It measures the degree of monotonic associationbetween two rankable variables.
The distribution of r as normal (mean 0, variance I/(N-1), assumingindependence) is asymptotic for large enough samples, and does not make any assumptions aboutnormality.
This test was therefore applied to the word frequency lists of each of the domains in the BNCand the email corpus, to identify which corpora were most similar.
The correlation with the BNC as awhole was also measured.
All calculations were based on Spearman's S, where D 2 denotes the sum of thesquares of the differences between the ranks of each pair of word types, and N the number of rankedpairs:6D 2r= 1N 3 - NIt is known that a sublanguage corpus can have very different characteristics to a general corpus (Biber,1993), yet it is not obvious how the position on this scale of a given corpus can be assessed.Consequently, it is necessary to determine the homogeneity of acorpus prior to performing any similaritymeasures, ince it is not clear what a measure of similarity would mean if a homogeneous corpus wasbeing compared with a heterogeneous one (Kilgarriff, 1996).
A homogeneity est was therefore performedon the corpus of each domain.
This was calculated using the following algorithm:181?
j?
!t1.
For  each domain corpus, do (times 10) I I2.1 Divide the corpus into two halves, by randomly placing 5k-word chunks in one of  twosubco~o~ l2.2 Produce a word frequency list (wfl) for each subcorpus; i2.3 Calculate the rank correlation between the two subcorpora; Ill3.
Calculate the mean and standard eviation o f  r.Ar As Bt Cf Im Le Np Ss Un : Wa BNC EmailAr 0.7890.001As 0.2.55.
0.75872127 : 0.001Bt 0.408 0.23869932.
73839Cf 0.340 : 0.35170648 70452Im 0.404 0.15967604 73786Le 0.459 0.28467013 71281Np 0.122 0.31575822.
71596Ss 0.409 0.34268263 70405Un 0.273 0.16174i 10 767.54Wa 0.423 0.28068005" 71298BNC 0.611 0.49763522 66732Email 0.012 0.09381648 " 797450.5810.0020.291729700.340!70807' 0.8870.0010.310 0.407 .
0.82471768 67615 0.0010.150 0.061 0.14576151 ?
76628 749520.422 0.278 0.32569756 70475 697960.174 ?
0.244 0.35777136 .
74635 723730.395 0.311 0.39570189 69838 682960.505679200.7300.0010.215728000.337704070.161753390.470680230.229754230.432684040.54166189?
0.055808840.578 0.60563755 ?
63439-0.026 -0.062 -0.003: 82897 83083 ~ 819080.6620.0020.200 0.81274053 0.0010.032 0.22679904 751610.130 0.46975399 669210.307 0.60971615 63779-0.032 0.03582719 811430.4860.0020.284739810.38172033-0.06684338 "0.8650.0010.65362450-0.012820150.687 -0.001 ?0.073 ;800850.3620.002I!
!1Tab le  1.
S imi lar i ty  and homogenei  W of BNC domains  and  emai lTable 1 shows both sets of  results.
The homogeneity values are across the diagonal, with mean andstandard eviation shown in each cell.
The other cells show the rank correlation (r) and the value of  N."BNC" refers to the complete corpus.
The subdomains are labelled as follows:As: Applied ScienceAr: ArtsBe: Beliefs & thoughtCf: Commerce & Finance182il|I!\Ira: ImaginativeLe: LeisureNp: Natural & pure scienceSs: Social scienceUn: UnclassifiedWa: World affairs.BNC: the whole of the BNCEmail: the 2 million word email corpusFor large samples uch as these the rank correlation coefficient has a normal distribution with mean 0 andvariance 1/(n-l) where n is the number of common words.
Although the significance of the correlation isnot in doubt, the differences are highly significant oo.
The difference between two rank correlationcoefficients will be normally distributed with mean 0.
The maximum possible value for the standarddeviation is (l/~(nl-1))+(l/~(n2-1)) where nl,n2 are the two common vocabulary sizes.
Any differencegreater than about 0.03 is therefore significant, and there are many pairs for which this is true.
It istherefore possible to rank the rank correlations, and hence the BNC domains.Evidently, the strongest correlation with the email corpus is from As (Applied Science).
Interestingly, thisfigure is higher than that between email and the whole BNC.
The second highest domain correlation iswith Cf (Commerce & Finance).
This agrees with intuitions based on a manual inspection of the contentsof the email corpus.
The table also shows a polarity of the BNC - the "arts" domains at one pole,attracting each other (e.g.
Ar:Bt = 0.408) but repelling the sciences (e.g.
Ira:As = 0.159).
Similarly, thesciences attract each other (e.g.
Np:As = 0.315).
In the middle are domains uch as World Affairs, SocialSciences & Commerce & Finance that correlate with both poles to varying degrees.
Moreover, the Ernailco~us really stands out on its own, having a very poor correlation with the others (in many eases it isnegative).
This suggests that even if the most strongly correlated omains are chosen, it is difficult tojustify augmenting the email corpus with texts selected from the BNC using this method.
Table 1 alsoshows the results of the homogeneity ests.
Email is by far the most heterogeneous, more so even than the"Unclassified" section of the BNC (!)
This brings into question the results of the similarity calculations inwhich the email corpus was involved, and mitigates further against he strategy of augmenting the emailcorpus with texts selected using the top.down method.These results also provide insight into the relationship between homogeneity and language model quality.A common measure of LM quality is perplexity (PP), which can be thought of as a measure of the"branching factor" (i.e.
the average size of the set of words between which a speech recogniser mustchoose) when transcribing a single word of the spoken text.
PP thus measures the recognition difficulty ofthe text relative to the given LM, and is measured by applying the model to a sample of test data.Consequently, a LM derived from a heterogeneous corpus should have a higher perplexity than anequivalent one derived from a more homogenous corpus.
However, homogeneity is defined here as ameasure of unigram distributions, whereas perplexity is usually calculated using n-grams (where n isusually <=3), so it is not clear to what extent he two measures would be related.3.2 The bottom-up approachThe top-down approach assumes that the BNC classification system is perfect, in that each text classifiedas belonging to a certain domain really belongs in that domain.
However, this is ultimately a subjectivejudgement, and frequently more than one classification is possible or even preferable (Lewis, 1992).Moreover, it is often the case that texts from the same medium are more similar to each other than textsfrom the same domain (e.g.
a journal paper on computing may be more similar to a journal paper ongeology than an item from a popular computing magazine, because the "content" features are lost amongthe much more salient "'genre" features).
Besides, no classification system is 100% reliable, so techniques183that are based on them will inherit this uncertainty.
Furthermore, domains such as Applied Science arevery coarse-grained: they contain many more types of material than just those of computing.
Even if suchcorpora are subdivided to a further level of classification they still suffer the same problem, albeit at afiner level of detail.An alternative strategy is to work in a "bottom-up" direction.
In this approach, a similarity metric is usedto fmd and extract related material from the background corpus, regardless of the top-down classification.This method may not be as structured as the previous approach, but it is more robust in that it involves nomanual intervention and does not rely on correct organisation or SGML tagging of the backgroundcorpus.
Moreover, it will not "miss" material that is classified under an unexpected omain or mediumbut is otherwise suitable.
The success of this approach depends on the use of a reliable similarity metric(even more so than the top-down approach, since it is now being applied to each of the 4,000+ files in theBNC rather than the 10 domain-based collections).
Using this statistic to find texts that are similar toemail in the BNC could be achieved using the following algorithm:1.
Create a wfl for the email corpus.2.
For each individual text in the BNC, do:2.1 Create the wfl for the BNC text2.2 Create a contingency table from the 2 wfls (ignoring function words)2.3 Calculate the number of common words N and the rank correlation r3.
Store the filename, fltle, N and r in RESULTS4.
Output the RESULTS sorted on the value for r.Although the rank correlation may be applied to the wfls regardless of their content, it was foundempirically that performance improved if function words were excluded from the contingency table.
Astop list of 241 function words was therefore applied in Step 2.2 of the above algorithm.
The algorithmwas run on the entire BNC (i.e.
each of its 4,000+ files).
The output was a list of the files sortedaccording to the value of r. The top and bottom 10 texts on this fist are as follows:/BNCIi.
O/HIHAIHAC/BNC/I.O/J/JO/JOVIBNC/i.
OICICTICTXIBNCII.01FIFTIFT8/BNC/I.0/G/G01G00/BNC/I .0/ClCBICBU/BNC/i .0/CICBICBX/BNC/i .0/K/KRIKRG/BNCIi .0/H/HRIHRD/BNC/i .0/EIEEIEEB/BNC/i.O/F/FUIFUS/BNC/i.
OIJIJ51J5B/BNC/i.O/HIH41H4P/BNC/i.O/GIGYIGY5/BNC/i .0/KIKPIKPS/BNC/i.O/G/G51G54/BNC/I.OIH/HKIHKMIBNCIi .0/F/FDIFDE/BNC/i .0/G/G5/G5AIBNC/I.0/ J / J J I J J JARTICLES FROM PRACTICAL PC NOV 9$1&NDASH;FEELECTRONIC INFORMATION RESOURCES AND THE HIWHAT PERSONAL COMPUTERWHAT PERSONAL COMPUTER: THE ULTIMATE GUIDEMISC~rrI.ANEOUS ARTICLES ABOUT DESK-TOP PUBLIACCOUNTANCYACCOUNTANCYIDE/~ IN ACTION PROGRAMMES (03) -- AN ELECTMOLTIMEDIA IN THE 1990SPEOPLE IN ORGANISATIONSRESULTS OF PRSTATECTOMY SURVEY -- AN ELECTRECOVER BIO-DEGRADABLE HOUSEHOLD CLEANING PRMEDICAL CONSULTATIONS -- AN ELECTRONIC TRANMEDICAL CONSULTATIONS -- AN ELECTRONIC TRANSPOKEN MATERIALFROMRESPOND~T PAMELA2 --MEDICAL CONSULTATIONS -- AN ELECTRONIC TRANROCKWELL/THE BETTER ALTERNATIVE TO THE FLATTHE WEEKLY LAW REPORTS 1992 VOLUME 3&LSQB;PAUCTION ROOMS -- AN ELECTRONIC TRANSCRIPTIOBRISTOL UNIVERSITY -- AN ELECTRONIC TRANSCR7558 0.6065393 0.5924337 0.5814513 0.5775228 0.5726053 0.5675902 0.5573231 0.5563914 0.5543199 0.553239 0.096248 0.088137 0.08790 0.07821 0.06317 0.052215 0.050196 0.045186 0.00018 -0.259Each line shows the filename, the title of the text, the number of common words and the value for r. Atfirst glance, the results appear to be intuitively satisfying.
Of the top ten texts, six have titles that areclearly related to computing, including all of the top five.
The remaining four could arguably be classifiedas Commerce & Finance (which was identified as the second most similar domain to email).
However, asuitable tide is no guarantee of suitable contents.
As far as can reasonably be expected, the tidesconstitute a fair and accurate reflection of the contents of each text.
Of course, the whole point of this184"1approach is to develop techniques that do not rely on ambiguous manual annotations uch as title ordomain, so the presence of suitable floes is merely an initial indication of success.One way of evaluating this result is to go through the list and calculate the mean rank of the 61"Computergram International" texts, which are typical of the sort of texts this technique should identify asbeing similar to the email corpus.
If the technique is working perfectly, the mean rank should be 31.
If itis completely random, the mean rank would be 2062.
It transpires that the mean rank is 959.85 (std dev =524.44).
Clearly, this result is better than chance, but far from significant.
One of the main reasons forthis was a tendency to sometimes give high scores to texts that were actually too short to constitutereliable sainples (the BNC attempts to maintain a standard sample size but this is not always possible).
Alogical modification was therefore to ignore those texts for which the number of common words wasbelow a certain threshold.
A number of threshoIds were investigated, and the optimum value (determinedempirically) was around 1,370 words.
However, even with this modification, the mean rank remained ashigh as 818.41 (std dev = 407.81).
It is possible to reduce this value still further, but only bycompromising the overall recall value (i.e.
genuine texts are eliminated along with the "noise").However, there is a more fundamental limitation to the above methodology.
The rank correlation statisticcompares differences in rank, ignoring absolute value (which can be significant).
To illustrate, consider acase where the word "of' is ranked 3 in one corpus and 6 is another.
This is a very important difference.Conversely, ff "banana" is ranked 10,000 in one corpus and 100,000 in another, this is a very insignificantdifference.
But the difference of ranks for "of" = 3, for "banana" = 90,000.
Clearly this technique ismissing something important.
Consequently, it was decided to investigate an alternative measure: theLoglikellhood Ratio Statistic.The Logllkelihood Ratio, G 2, is a mathematically well-grounded and accurate method for calculating how"surprising" an event is (Dunning, 1993).
This is true even when the event has only occurred once (as isoften the case with linguistic phenomena).
It is an effective measure for the determination of domain-specific terms (e.g.
Daille, 1995) and can be also used as a measure of corpus similarity.
In the casewhere two corpora are being compared, it is possible to calculate the G 2 statistic either for single words(using a ~ contingency table) or for a vocabulary of N words (an N><2 table).
The analysis of the 4,000+BNC fries was therefore repeated using the Loglikelihood (instead of rank correlation) as the similaritymeasure.
This produced the following top and bottom 10 texts:/BNC/I.O/H/H4/H4L/BNC/i.0/G/G5/G54/BNC/I.O/H/H5/H58/BNC/1.0/F/F7/F78/BNC/1.0/H/H5/H5B/BNC/1.0/G/G4/G4Y/BNC/1.0/K/KN/KNU/BNC/1.0/J/ J J / J J J/BNC/1.0/A/A9/AgB/BNC/i.O/H/HK/HKC.
, , o .
.
.IBNCIi.
OICICBICBGIBNCIi.
OIKIK51K5DIBNCII .
0 IHIHUIHU4IBNCI1.
O IHIHWIIHWSIBNCIi .
O IHIHU IHU2IBNCI1.
O IHIHU IHU3IBNCIi.
OIHIHHIHHXIBNCII.OIKIK91K97IBNCII.OICICRICRMIBNClI.OIHIHHIHHVMEDICAL CONSULTATIONS -- AN ELECTRONIC TRANSCRIPTION 23226MEDICAL CONSULTATIONS -- AN ELECTRONIC TRANSCRIPTION 23226MEDICAL CONSULTATIONS -- AN ELECTRONIC TRANSCRIPTION 23228STAFF MEETING -- AN ELECTRONIC TRANSCRIPTION 23226MEDICAL CONSULTATIONS -- AN ELECTRONIC TRANSCRIPTION 23230MEDICAL CONSULTATIONS -- AN ELECTRONIC TRANSCRIPTION 23231SPOKEN MATERIAL FROM RESPONDENT 716 -- AN ELECTRONIC 23227BRISTOL UNIVERSITY -- AN ELECTRONIC TRANSCRIPTION 23231GUARDIAN, ELECTRONIC EDITION OF 19891210; APPSCI MAT 23232FREEMANS 23234TODAY 32658&LSQB;UNCATALOGUED TEXT SAWLDA&RSQB; 34572GUT$1$1$2JOURNAL OF GASTROENTEROLOGYAND HEPATOLOGY 29557GUTSI$1$2JOT/RNAL OF GASTROENTEROLOGY AND HEPATOLOGY 29531GUTSi$1$2JOURNALOF GASTROENTEROLOGY ANDHEPATOLOGY 29356GUT$1$1$2JOURNAL OF GA~TROENTEROLOGYAND HEPATOLOGY 29500HANSARD PROCEEDINGS 19951&NDASH;92 SESSION 30854LIVERPOOL ECHO Si$21DAILY POST$1$1$2NOVEMBER 1992 -- 38865NATURE 36867SELECTION FROM HANSARD 19951&NDASH;1992 30389108.897244.251318.442358.087385.826419.826425.806526.359532.956547.160351001.583352223.071367084.662367801.892379109.473382579.770383471.711402754.606463853.336469043.781Each fine shows the filename, the title of the text, the length of the contingency table and the value for G 2.These are sorted in ascending order since comparing two identical documents would produce a G 2 of zero.185!A brief inspection of the titles of the documents atthe top of the list would indicate that the metric has notproduced an improvemenL Moreover, it transpires that the mean rank of the CI texts is now 1171.98, withstd dev = 178.54.
However, as before, the number of common words is very small for some of the texts.Therefore, the filter was applied to ignore eases where there were fewer than 1,370 words in eornmon.This produced a mean rank of I25.15 (std.
dev.
= 75.62), which is significantly lower than that producedby the rank correlation (mean rank = 818.41, std.
dev.
= 75.621).So despite the absence of apparently suitable candidates in the top 10, the overall accuracy of thistechnique (measured by the mean rank of the 61 CI texts) is higher.
The G 2 statistic appears to be moresuitable for this type of data since it uses the actual frequency values for the words in the wfls, rather thanjust their ranks.
Other independent sources indicate that the G 2 produces results that appear to correspondreasonably well with human judgement (Dallle, 1995).However, both the rank correlation and Loglikelihood Ratio both make use only of unigrarn information.Clearly, much of the information that humans use to measure textual similarity is found not (solely) in theindividual word frequencies (unigrarns), but rather in the way they combine (n-grams).
The logical nextstep is therefore to compare word bigrams (or trigrarns) instead of just unigrarn data.
A variation on thiswould be to compare texts using the Loglikelihood applied to bigrams that are not necessarily adjacent,i.e.
counting occurrences of wordl and word2 within a limiting distance of each other.
Indeed, suchmethods have been previously used for actually building the LMs themselves, and have been successfullyapplied to both speech (Rose & Lee, 1994) and handwriting data (Rose & Evett, 1995).
Counting wordswithin a limited window would be smoother than using strict bigrarns and eousequently less affected bythe problems caused by sparse data (which are inevitable when small, individual text files are compared).Another interesting possibility is to use the LM itself as the similarity metric.
From an informationtheoretic point of view, entropy is a measure of a eorpus's homogeneity, and the cross-entropy betweentwo corpora is a measure of their similarity (Charniak, 1993).
After all, when a LM is applied to a testtext to produce aperplexity score, this value is a measure of the cross-entropy which reflects how well theLM predicts the words in the text.
So if a LM is trained on text that is very similar to the test text, then itshould predict he test data well and the perplexity should be low.
Conversely, ff the test text is verydifferent from the training text, then the perplexity will be high.
The perplexity score can therefore beused to measure textual similarity.
Moreover, it has the advantage doing so by considering (typically)uuigram, bigrarn and trigram data.
Indeed, this method has already been successfully used within thedevelopment of a similarity-based Interact search agent, and preliminary findings indicate that perplexityis indeed an effective corpus similarity measure (Rose & Wyard, 1997).However, the use of such an approach isnot entirely beyond question.
Firstly, the LM is being used as therepresentation f a training text against which similarity is to be judged, and yet it is, by definition, under-trained and therefore degenerate.
Secondly, the method by which similarity is measured should ideally beindependent to the method by which success is evaluated.
To use perplexity both as a similarity metricand an evaluation metric implies a certain amount of circular reasoning.
However, the use of suchiterafive techniques i not totally without precedent within the LM eornmunity.
Several research groupshave reported the successful improvement of LMs using techniques that iteratively tune the LMparameters u ing new samples of training data (e.g.
Jelinek, 1990).
So, this approach may transpire to beI1IIsufficient!y well principled to merit further investigation, i111 4.
Language model qualityA LM is built by collecting trigram, bigram & uuigram data from a training corpus.
However, it is notalways desirable to store all of this data.
Thresholds can be set such that some of the lower frequency n-grams are discarded.
For example, a trigram cut-off of 5 implies that all the trigrams with frequencies of 5or fewer in the training data are not used in building the model.
Setting lower thresholds allows the model186to focus on more frequent events, and produces a proportionately smaller model.
The LMs described inthis paper were built using the CMU SLM toolkit (Rosenfeld, 1994) which facilitated the construction ofa variety of LMs representing a range of different settings for each of the pertinent parameters.The first of these was the Email LM.
This was constructed using a vocabulary of 20,000 words that wasderived irectly from the ernail training data.
The bigram and trigram cutoffs were both set to zero.
Thesecond LM was buik from the whole of the BNC, using the same vocabulary as the Email LM (in order toensure consistency).
So although their n-grams had been based on general English rather than Email, theirvocabulary was derived from the Email data.
For comparison therefore, a third BNC LM was built, usinga vocabulary derived irectly from the BNC (rather than email).
This allowed the comparative evaluationof the contribution of vocabulary vs. n-grams to the LM effectiveness (measured using both perplexityand word error rate).
Due to memory constraints it was not possible to build the BNC models with cut-offs lower than 2-2.
The fourth LM investigated was the 20k WSJ LM that is available from the Abbot ftpsite at Cambridge University.The standard measure by which LMs are assessed is by calculating their perplexity using a sample of testdata.
This process is usually performed off-line, i.e.
independently of the speech reeogniser for which themodels are intended.
For the models described above, testing was performed using the CMU toolkit, byapplying each LM to a sample of 10,000 words from the transcriptions of a database of video mailmessages, developed by Cambridge University as part of their "Video Mail Retrieval using Voice"project (Jones et al, 1994).
Evidently, this data is not actually spoken email, but its domain and genre arenevertheless closely related to email.
Unfortunately, it was not possible to calculate the PP of the WSJLM due to the absence of a readily available version in the correct format.A second evaluation method is to integrate the I,M with the speech reeogniser and test the combinedsystem using recorded speech data.
The models can be interchanged between trials, allowing comparativeevaluation by measuring the word error rate (WER) produced by each model.
More precisely, the errorrates are measured using two standard metrics, percentage correct and accuracy:.~r% Correct = ~ x 100%N(H- X)2.
Accuracy = x 100%Nwhere: H is the number of correct transcriptions (words in the utterance that are found in thetranscription), D is the number of deletions (words in the utterance that are missing from thetranscription), S is the number of substitutions (words in the utterance that are replaced by an incorrectword in the transcription), and I is the number of insertions (extra words in the transcription).
Accuracy ismore critical than %correct in that it directly penalises insertions.
Deletions & substitutions reduce thevalue of H, since H = N - (D+S).As mentioned above, the VMR database is a collection of speech data with transcriptions (of which thelatter were used in the above evaluation).
The speech part contains audio files for 15 speakers, of which10 were used in the current investigation.
The Abbot recogniser was run using each combination of the 10speakers' data files (as input) and each of the four LMs: email, BNC with email vocabulary, BNC and theWSI LM.
The output ranscriptions were assessed for %correct and accuracy using the HResults program,which is part of HTK - the Hidden Markov Model Toolkit (Young & Woodland, 1993).Table 2 shows the results of this investigation.
The results for %correct and accuracy show the combinedeffect of the recogniser and LM.
The contribution of the LM depends on its vocabulary and perplexity.
As187the LM changes, it produces different behaviour in the combined system and therefore different ypes oferrors (e.g.
insertions, deletions & substitutions).
The net effect is that the email LM produces the highest%correct and also the highest accuracy.
It is around 5% better (on both measures) than the WSJ LM.
Thisis significant, considering the tiny corpus from which it was derived (2 million vs. 227 million in the caseof WSJ).
In between these two extremes are the two BNC LMs - the one with the email vocabularyperforms lightly better (-0.5%) than the one with the BNC vocabulary.%Correct Accurac~rSpeaker 1 speaker 6 "%Correct \[Accuracyemail 42.32 33.23 email 52.17 42.88BNC 41.47 32.02 BNC/email 49.53 39.76BNC/email 41.42 31.06 BNC 48.97 39.11WSJ 37.84 28.50 WSJ  46.36 36.65Speaker 2 Speaker 724.5S BNC/email 37.14 email 65.05email 36.77 25.03 BNC/email 64.49 53.93BNC 36.19 24.40 I BNC 64.23 53.83WSJ 32.90 22.11 WSJ 60.97Speaker 8Speaker 339A450.1543.44 email 44.42 email 154.70BNC/email 44.42 37.86 BNC/email I 51.40 38.74BNC 43.34 36.74 BNC 50.99 37.6439.72 33.90 WSJSpeaker 947.93 35.55 WSJ59.82 50.04 email 70.08 62.7559.28 49.50 BNC/email 69.86 62.0358.37 48.28 BNC 68.85 61.14Speaker 4emailWSJ 66.67 58.37Speaker 10email 65.91 56.14BNC/emailBNCWSJ 56.99 46.68Speaker 5BNC 76.52 71.99BNC/email 75.94 70.84 BNC/email 65.83 55.38WSJ 73.15 68.43 BNC 65.12 54.6771.90 66.31  WSJ 61.58 51.13 emailOVERALL %Correctemail 54.92BNC/email 54.04 44.24 241.70BNC 53.40 43.71 227.54WSJ 50.42 40.93 N/AAccuracy Perplexity45.85 261.58Table 2.
%Correct, accuracy, and perplexity of the language modelsThe result for the PP testing is highly revealing.
As described earlier, a corpus of low homogeneity shouldproduce a LM of higher PP than a corpus of high homogeneity.
This is indeed shown to be the case, sincethe PP for email is 261.58 (homogeneity = 0.362), whereas the PP for the BNC is 227.54 (homogeneity =0.687).
These PP values are calculated using the 10K test data sample from the transcriptions of the VMRproject.
The higher PP value for email would tend to indicate that this is the poorer LM.
However, it isclear that when used on the real spoken data, the email LM provides the lowest error rates.
Initialexplanations for this centred on the vocabulary, since a higher incidence of out-of-vocabulary (OOV)words can produce a lower PP but a higher WER.
However, the email LM performs better (by 0.88% ,188IIIIk ~correct) than the BNC/email LM even though both share the same vocabulary.
Two explanations for thisare possible.
Firstly, there may be n-grams in the email corpus that are simply not found in the BNC (eventhough the BNC is 50 times larger).
Secondly, the email LM may be better because it "wastes" lessprobability mass on n-grams that never actually occur in the test data.
This implies that quality, notquantity, is a major factor in training effective LMs.
Further PP testing, possibly using the completetranscriptions of the VMR data is necessary toclarify this issue.Evidently, the choice of vocabulary also makes an important contribution.
The BNC LM with the emailvocabulary performs better (by 0.64% correct) than the BNC LM with the BNC vocabulary, so clearly theemail vocabulary provides better coverage of the test data.
In fact, it is possible to directly compare theOOV rates with the performances shown above: the BNC LM with the ernail vocabulary has an OOV rateof 1.16% on the VMR data, and a %correct of 54.04.
By contrast, he BNC LM with the BNC vocabularyhas an OOV rate of 1.69% and a %correct of 53.40.
These figures uggest that an increase in OOV rate of0.56% leads to a reduction in %correct of 0.64%, or, in other words, a 1% increase in OOV rate producesa reduction in %correct of around 1.14%.
Interestingly, this figure correlates extremely well with theresults of a similar experiment performed by Rosenfeld (1995), who found that a 1% increase in the OOVrate can lead to a 1.2% increase in the word error ate.5.
ConclusionsThe analysis of the corpora has provided several revealing insights.
Firstly, it is necessary to determinethe homogeneity of a corpus prior to performing any similarity measures, ince it is not clear what ameasure of similarity would mean if a homogeneous corpus was being compared with a heterogeneousone.
A methodology for calculating homogeneity has been described and the accuracy and usefulness ofthis is further described in Kilgarriff (1997).Clearly, the ernail corpus is highly heterogeneous.
This means it is particularly prone to "burstiness" andunpredictability, which affects all levels of n-grams (including unlgrams).
This may be due in part to theparticular training corpus used, but it is more likely to be inherent to the medium, since email can fulfil somany communicative functions.
It therefore xhibits a level of diversity surpassed perhaps only byspontaneous speech.
Investigation of the spoken part of the BNC is therefore suggested as an area forfurther work.To a certain extent, the apparent heterogeneity of the emall undermines the results of any similaritymeasures applied to this corpus.
Nevertheless, the extent o which the email is unlike all the other BNCdomains is quite apparent and therefore mitigates any unprincipled approaches to corpus augmentationusing crude, top-down techniques that involve complete domains taken from the BNC.
Consequently, thebest way to acquire more ernail data appears to be either: (a) to instigate a further collection initiative, (b)to use more sophisticated bottom.up methods, or (c) to use self-organising adaptation techniques (e.g.Clarkson and Robinson, 1997).
The similarity metric used in (b) must be chosen carefully.
Although theLoglikelihood and rank correlation metrics both produce results that can look intuitively plausible, thismerely underlines the need for an objective, thorough evaluation method.
Loglikefihood appears to be themore principled of the two measures, and it is suggested that this offers the greater potential.The results of the language modelling exercise provide clear evidence that it is possible to build effectiveLMs from small corpora.
The email LM outperformed the other L_Ms on real spoken data (albeit takenfrom a technical, "ernaiMike" domain) for eight of the ten speakers.
This is significant, considering theother LMs were trained on corpora that were several times larger.
This effect can be mainly attributed tothe source of the n-grams and the extent.to which the larger LMs "waste" probability mass on n-gramsthat never actually occur in the test data.
Other researchers also have investigated methods for adaptinglarge, general LMs using data from a small domain corpus and have found merit in simply building a189smaller LM directly from the domain corpus.
For example, Ueberla (1997) observed that theimprovements gained by using adaptation techniques compared to simply "starting from scratch" on thedomain data become quite small when several tens of thousands of words of domain data are available.
(Since the email corpus is almost 2 million words it clearly meets this criterion.)
It is interesting tonotealso that his threshold isseen to vary according to the level of similarity between the adaptation (domain-specific) corpus and the background (general) corpus.It is also possible to adapt LMs dynamically, using cache-based methods (e.g.
Kuhn & de Mori 1990) andevidence suggests that his may prove the more effective approach (Matsunaga et al, 1992).
It is clear thaternail is highly heterogeneous and therefore inherently unpredictable.
Attempting to model this by staticmeans can thus produce only limited success.
By contrast, adynamic LM would adapt o the current inputand update its probabilities accordingly.
However, dynamic LMs still need a set of static, baselineprobabifities, sothe email LM may present the best starting point for this.6.
ReferencesBiber, D. (1993) "Using register-diversified corpora for general anguage studies", ComputationalLinguistics, 19, No.
2.Brown, P., Cocke, J., Della Pietra, S., Della Pietra, V., Jelinek, F., Lafferty, J., Mercer, R., P. (1989) "Astatistical pproach to machine translation", Technical Report, IBM Research Division.Burnard, L. (1995) Users Reference Guide for the British National Corpus, Oxford University ComputingServices.Chamiak, E. (1993) "Statistical Language Learning", M1T Press, Cambridge, Mass.Church, K., Hanks, P., Hindle, D. and Gale, W. (1991) "Using statistics in lexical analysis", in Zernik, U.(Ed.)
"Lexical Acquisition: Using On-Line Resources to Build a Lexicon", LEA, NJ.Clarkson, P.R.
& Robinson, AJ.
(1997) "Language model adaptation using mixtures and an exponentiallydecaying cache", Proceedings ofICASSP, Munich, Germany.Daille, B.
(1995) "Combined approach for terminology extraction", Technical Report 5, UCREL,Lancaster University.Dunning, E. (1993) "Accurate methods for the statistics of surprise and coincidence", ComputationalLinguistics, 19, No.
1.Hochberg, M., Robinson T. & Renals S. (1994) "Large vocabulary continuous speech recognition using ahybrid connectionist HMM system", Prec.
of ICSLP, pp.
1499-1502.Jelinek, F. (1990) "Self-organized language modeling for speech recognition", in Waibel and Lee (Eds.
),Readings in Speech Recognition, Morgan Kaufmann, San Mateo, CA.Jones, G., Foote, J., Sparck Jones, K. & Young, S. (1994) "Video mail retrieval using voice", TechnicalReport 335, Cambndge University Computer Laboratory.Katz, S. (1987) "Estimation of probabilities from sparse data", lEE Transactions on Acoustics, Speech &Signal Processing, vol.
ASSP-35.190Kilgarriff, A.
(1996) "Which words are particularly characteristic of a text?"
in Rose & Evett (Eds.
)Language Engineering for Document Analysis & Recognition, AISB Workshop roceedings.Kilgurriff, A (1997) "Using word frequency lists to measure corpus homogeneity and similarity betweencorpora", Proceedings of the Fifth Workshop on Very Large Corpora, Hong Kong.Kuhn, R. & De Mori, R. (1990) "A cache-based natural language model for speech recognition" IEEETrans.
on PAMI, 12(6), pp.
570--583.Lewis, D. (1992) "Text representation for intelligent text retrieval: a classification-oriented vi w", in P.Jaeobs ''Text-based Intelligent Systems", LEA Publishers, Hfllsdale, NJ.Matsunaga, S., Yamada, T. & Shikano, K. (1992) "Task adaptation i  stochastic language models forcontinuous speech recognition", Proc.
ICASSP Vol.
1, pp.
165-168.Rose, T.G.
& Evett, L. (1995) "The use of context in cursive script recognition", Machine Vision andApplications", Springer International.Rose, T.G.
& Lee, M (1994) "Language modelling for large vocabulary speech recognition", Proc.
IOAMeeting on LVSR, Cambridge, England.Rose, T.G.
& Wyard, PJ.
(1997) "A similarity-based agent for Intemet searching", Proceedings ofRIAO'97 - Computer-assisted Searching on the Interact, Montreal, Canada.Rosenfeld, R. (1994) "The CMU Statistical Language Modeling Toolkit and its use in the 1994 ARPACSR Evaluation, Proceedings of the Spoken Language Technology Workshop 1995, Austin ('IX).Rosenfeld, R. (1995) "Opdrnizing lexical and n-gram coverage via judicious use of linguistic data", Proc.Eurospeeeh 95.Rudnicky, A.
(1995) "Language modeling with limited domain data", Proceedings of the ARPAWorkshop on Spoken Language Technology, Morgan Kaufmann, San Mateo, pp.
66-69.Ueberla, J.P. (1997) "Domain adaptation with clustered language models", Proceedings of ICASSP,Munich, Germany.Vergyri, D. (1995) Unpublished web page http://www.elsp.jhu.edu/-dverg/bleaching.hUnlYoung S. & Woodland P. (1993) "HTK: Hidden Markov Model Toolkit V1.5 User Manual", CambridgeUniversity Engineering Dept.
and Entropic Research Labs Ltd.191
