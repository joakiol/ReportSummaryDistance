Incrementality in Syntactic Processing: Computational Modelsand Experimental EvidencePatrick STURTHuman Communication Research CentreDepartment of PsychologyUniversity of Glasgow58 Hillhead StreetGlasgow, SCOTLANDpatrick@psy.gla.ac.ukAbstractIt is a well-known intuition that human sentenceunderstanding works in an incremental fashion,with a seemingly constant update of the inter-pretation through the left-to-right processing ofa string.
Such intuitions are backed up by ex-perimental evidence dating from at least as farback as Marslen-Wilson (1973), showing thatunder many circumstances, interpretations areindeed updated very quickly.From a parsing point of view it is interestingto consider the structure-building processes thatmight underlie incremental interpretation?what kinds of partial structures are built dur-ing sentence processing, and with what time-course?In this talk I will give an overview of the state-of-the-art of experimental psycholinguistic re-search, paying particular attention to the time-course of structure-building.
The discussion willfocus on a new line of research (some as yet un-published) in which syntactic phenomena suchas binding relations (e.g., Sturt, 2003) and un-bounded dependencies (e.g., Aoshima, Phillips,& Weinberg, in press) are exploited to make avery direct test of the availability of syntacticstructure over time.The experimental research will be viewedfrom the perspective of a space of computa-tional models, which make different predictionsabout time-course of structure building.
Onedimension in this space is represented by theparsing algorithm used: For example, withinthe framework of Generalized Left Corner Pars-ing (Demers, 1977), algorithms can be char-acterized in terms of the point at which acontext-free rule is recognized, in relation to therecognition-point of the symbols on its right-hand side.
Another relevant dimension is repre-sented by the type of grammar formalism thatis assumed.
For example, with bottom-up pars-ing algorithms, the degree to which structure-building is delayed in right-branching structuresdepends heavily on whether we employ a tra-ditional phrase-structure formalism with rigidconstituency, or a cateogorial formalism withflexible constituency (e.g., Steedman, 2000).I will argue that the evidence is incompatiblewith models which predict systematic delays inthe construction of syntactic structure.
In par-ticular, I will argue against both head-drivenstrategies (e.g., Mulders, 2002), and purelybottom-up parsing strategies, even when flex-ible constituency is employed.
Instead, I willargue that to capture the data in the most par-simonious way, we should turn our attention tothose models in which a fully connected syn-tactic structure is maintained throughout theprocessing of a string.ReferencesAoshima, S., Phillips, C., & Weinberg, A.
(inpress).
Processing filler-gap dependenciesin a head-final language.
To appear inJournal of Memory and Language.Demers, A. J.
(1977).
Generalized left cor-ner parsing.
In Proceedings of the 4thacm sigact-sigplan symposium on princi-ples of programming languages (pp.
170?182).
ACM Press.Marslen-Wilson, W. (1973).
Linguistic struc-ture and speech shadowing at very shortlatencies.
Nature, 244, 522?533.Mulders, I.
(2002).
Transparent parsing: Head-driven processing of verb-final structures.Utrecht: LOT.Steedman, M. (2000).
The syntactic process.Cambridge, MA: MIT press.Sturt, P. (2003).
The time-course of the appli-cation of binding constraints in referenceresolution.
Journal of Memory and Lan-guage, 48 (3), 542?562.
