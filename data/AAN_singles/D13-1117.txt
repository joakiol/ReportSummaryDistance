Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1170?1179,Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational LinguisticsFeature Noising for Log-linear Structured PredictionSida I.
Wang?, Mengqiu Wang?, Stefan Wager?,Percy Liang, Christopher D. ManningDepartment of Computer Science, ?Department of StatisticsStanford University, Stanford, CA 94305, USA{sidaw, mengqiu, pliang, manning}@cs.stanford.eduswager@stanford.eduAbstractNLP models have many and sparse features,and regularization is key for balancing modeloverfitting versus underfitting.
A recently re-popularized form of regularization is to gen-erate fake training data by repeatedly addingnoise to real data.
We reinterpret this noisingas an explicit regularizer, and approximate itwith a second-order formula that can be usedduring training without actually generatingfake data.
We show how to apply this methodto structured prediction using multinomial lo-gistic regression and linear-chain CRFs.
Wetackle the key challenge of developing a dy-namic program to compute the gradient of theregularizer efficiently.
The regularizer is asum over inputs, so we can estimate it moreaccurately via a semi-supervised or transduc-tive extension.
Applied to text classificationand NER, our method provides a >1% abso-lute performance gain over use of standard L2regularization.1 IntroductionNLP models often have millions of mainly sparselyattested features.
As a result, balancing overfittingversus underfitting through good weight regulariza-tion remains a key issue for achieving optimal per-formance.
Traditionally, L2 or L1 regularization isemployed, but these simple types of regularizationpenalize all features in a uniform way without tak-ing into account the properties of the actual model.An alternative approach to regularization is togenerate fake training data by adding random noiseto the input features of the original training data.
In-tuitively, this can be thought of as simulating miss-?Both authors contributed equally to the papering features, whether due to typos or use of a pre-viously unseen synonym.
The effectiveness of thistechnique is well-known in machine learning (Abu-Mostafa, 1990; Burges and Scho?lkopf, 1997; Simardet al 2000; Rifai et al 2011a; van der Maatenet al 2013), but working directly with many cor-rupted copies of a dataset can be computationallyprohibitive.
Fortunately, feature noising ideas oftenlead to tractable deterministic objectives that can beoptimized directly.
Sometimes, training with cor-rupted features reduces to a special form of reg-ularization (Matsuoka, 1992; Bishop, 1995; Rifaiet al 2011b; Wager et al 2013).
For example,Bishop (1995) showed that training with featuresthat have been corrupted with additive Gaussiannoise is equivalent to a form of L2 regularization inthe low noise limit.
In other cases it is possible todevelop a new objective function by marginalizingover the artificial noise (Wang and Manning, 2013;van der Maaten et al 2013).The central contribution of this paper is to showhow to efficiently simulate training with artificiallynoised features in the context of log-linear struc-tured prediction, without actually having to gener-ate noised data.
We focus on dropout noise (Hintonet al 2012), a recently popularized form of artifi-cial feature noise where a random subset of featuresis omitted independently for each training example.Dropout and its variants have been shown to out-perform L2 regularization on various tasks (Hintonet al 2012; Wang and Manning, 2013; Wan et al2013).
Dropout is is similar in spirit to feature bag-ging in the deliberate removal of features, but per-forms the removal in a preset way rather than ran-domly (Bryll et al 2003; Sutton et al 2005; Smithet al 2005).1170Our approach is based on a second-order approx-imation to feature noising developed among othersby Bishop (1995) and Wager et al(2013), which al-lows us to convert dropout noise into a form of adap-tive regularization.
This method is suitable for struc-tured prediction in log-linear models where secondderivatives are computable.
In particular, it can beused for multiclass classification with maximum en-tropy models (a.k.a., softmax or multinomial logis-tic regression) and for the sequence models that areubiquitous in NLP, via linear chain Conditional Ran-dom Fields (CRFs).For linear chain CRFs, we additionally show howwe can use a noising scheme that takes advantageof the clique structure so that the resulting noisingregularizer can be computed in terms of the pair-wise marginals.
A simple forward-backward-typedynamic program can then be used to compute thegradient tractably.
For ease of implementation andscalability to semi-supervised learning, we also out-line an even faster approximation to the regularizer.The general approach also works in other cliquestructures in addition to the linear chain when theclique marginals can be computed efficiently.Finally, we extend feature noising for structuredprediction to a transductive or semi-supervised set-ting.
The regularizer induced by feature noisingis label-independent for log-linear models, and sowe can use unlabeled data to learn a better regu-larizer.
NLP sequence labeling tasks are especiallywell suited to a semi-supervised approach, as inputfeatures are numerous but sparse, and labeled datais expensive to obtain but unlabeled data is abundant(Li and McCallum, 2005; Jiao et al 2006).Wager et al(2013) showed that semi-superviseddropout training for logistic regression captures asimilar intuition to techniques such as entropy regu-larization (Grandvalet and Bengio, 2005) and trans-ductive SVMs (Joachims, 1999), which encourageconfident predictions on the unlabeled data.
Semi-supervised dropout has the advantage of only us-ing the predicted label probabilities on the unlabeleddata to modulate an L2 regularizer, rather than re-quiring more heavy-handed modeling of the unla-beled data as in entropy regularization or expecta-tion regularization (Mann and McCallum, 2007).In experimental results, we show that simulatedfeature noising gives more than a 1% absolute boostyt yt+1yt?1f (yt, xt )f (yt?1, yt ) f (yt, yt+1)yt yt+1yt?1f (yt, xt )f (yt?1, yt ) f (yt, yt+1)Figure 1: An illustration of dropout feature noisingin linear-chain CRFs with only transition featuresand node features.
The green squares are node fea-tures f(yt, xt), and the orange squares are edge fea-tures f(yt?1, yt).
Conceptually, given a training ex-ample, we sample some features to ignore (generatefake data) and make a parameter update.
Our goal isto train with a roughly equivalent objective, withoutactually sampling.in performance over L2 regularization, on both textclassification and an NER sequence labeling task.2 Feature Noising Log-linear ModelsConsider the standard structured prediction problemof mapping some input x ?
X (e.g., a sentence)to an output y ?
Y (e.g., a tag sequence).
Letf(y, x) ?
Rd be the feature vector, ?
?
Rd be theweight vector, and s = (s1, .
.
.
, s|Y|) be a vector ofscores for each output, with sy = f(y, x) ?
?.
Nowdefine a log-linear model:p(y | x; ?)
= exp{sy ?A(s)}, (1)where A(s) = log?y exp{sy} is the log-partitionfunction.
Given an example (x,y), parameter esti-mation corresponds to choosing ?
to maximize p(y |x; ?
).The key idea behind feature noising is to artifi-cially corrupt the feature vector f(y, x) randomly1171into some f?
(y, x) and then maximize the averagelog-likelihood of y given these corrupted features?the motivation is to choose predictors ?
that are ro-bust to noise (missing words for example).
Let s?,p?
(y | x; ?)
be the randomly perturbed versions cor-responding to f?
(y, x).
We will also assume thefeature noising preserves the mean: E[f?
(y, x)] =f(y, x), so that E[s?]
= s. This can always be doneby scaling the noised features as described in the listof noising schemes.It is useful to view feature noising as a form ofregularization.
Since feature noising preserves themean, the feature noising objective can be written asthe sum of the original log-likelihood plus the dif-ference in log-normalization constants:E[log p?
(y | x; ?)]
= E[s?y ?A(s?)]
(2)= log p(y | x; ?
)?R(?, x), (3)R(?, x)def= E[A(s?)]?A(s).
(4)Since A(?)
is convex, R(?, x) is always positive byJensen?s inequality and can therefore be interpretedas a regularizer.
Note that R(?, x) is in general non-convex.Computing the regularizer (4) requires summingover all possible noised feature vectors, which canimply exponential effort in the number of features.This is intractable even for flat classification.
Fol-lowing Bishop (1995) and Wager et al(2013), weapproximate R(?, x) using a second-order Taylorexpansion, which will allow us to work with onlymeans and covariances of the noised features.
Wetake a quadratic approximation of the log-partitionfunction A(?)
of the noised score vector s?
aroundthe the unnoised score vector s:A(s?)
u A(s) +?A(s)>(s??
s) (5)+12(s??
s)>?2A(s)(s??
s).Plugging (5) into (4), we obtain a new regularizerRq(?, x), which we will use as an approximation toR(?, x):Rq(?, x) =12E[(s??
s)>?2A(s)(s??
s)] (6)=12tr(?2A(s) Cov(s?)).
(7)This expression still has two sources of potential in-tractability, a sum over an exponential number ofnoised score vectors s?
and a sum over the |Y| com-ponents of s?.Multiclass classification If we assume that thecomponents of s?
are independent, then Cov(s?)
?R|Y|?|Y| is diagonal, and we haveRq(?, x) =12?y?Y?y(1?
?y)Var[s?y], (8)where the mean ?ydef= p?
(y | x) is the model prob-ability, the variance ?y(1?
?y) measures model un-certainty, andVar[s?y] = ?>Cov[f?
(y, x)]?
(9)measures the uncertainty caused by feature noising.1The regularizerRq(?, x) involves the product of twovariance terms, the first is non-convex in ?
and thesecond is quadratic in ?.
Note that to reduce the reg-ularization, we will favor models that (i) predict con-fidently and (ii) have stable scores in the presence offeature noise.For multiclass classification, we can explicitlysum over each y ?
Y to compute the regularizer,but this will be intractable for structured prediction.To specialize to multiclass classification for themoment, let us assume that we have a separateweight vector for each output y applied to the samefeature vector g(x); that is, the score sy = ?y ?
g(x).Further, assume that the components of the noisedfeature vector g?
(x) are independent.
Then we cansimplify (9) to the following:Var[s?y] =?jVar[gj(x)]?2yj .
(10)Noising schemes We now give some examples ofpossible noise schemes for generating f?
(y, x) giventhe original features f(y, x).
This distribution af-fects the regularization through the variance termVar[s?y].?
Additive Gaussian:f?
(y, x) = f(y, x) + ?, where ?
?N (0, ?2Id?d).1Here, we are using the fact that first and second derivativesof the log-partition function are the mean and variance.1172In this case, the contribution to the regularizerfrom noising is Var[s?y] =?j ?2?2yj .?
Dropout:f?
(y, x) = f(y, x)  z, where  takes the el-ementwise product of two vectors.
Here, z isa vector with independent components whichhas zi = 0 with probability ?, zi = 11??
withprobability 1 ?
?.
In this case, Var[s?y] =?jgj(x)2?1??
?2yj .?
Multiplicative Gaussian:f?
(y, x) = f(y, x)  (1 + ?
), where?
?
N (0, ?2Id?d).
Here, Var[s?y] =?j gj(x)2?2?2yj .
Note that under our second-order approximation Rq(?, x), the multiplica-tive Gaussian and dropout schemes are equiva-lent, but they differ under the original regular-izer R(?, x).2.1 Semi-supervised learningA key observation (Wager et al 2013) is thatthe noising regularizer R (8), while involving asum over examples, is independent of the outputy.
This suggests estimating R using unlabeleddata.
Specifically, if we have n labeled examplesD = {x1, x2, .
.
.
, xn} and m unlabeled examplesDunlabeled = {u1, u2, .
.
.
, un}, then we can define aregularizer that is a linear combination the regular-izer estimated on both datasets, with ?
tuning thetradeoff between the two:R?
(?,D,Dunlabeled) (11)def=nn+ ?m( n?i=1R(?, xi) + ?m?i=1R(?, ui)).3 Feature Noising in Linear-Chain CRFsSo far, we have developed a regularizer that worksfor all log-linear models, but?in its current form?is only practical for multiclass classification.
Wenow exploit the decomposable structure in CRFs todefine a new noising scheme which does not requireus to explicitly sum over all possible outputs y ?
Y .The key idea will be to noise each local feature vec-tor (which implicitly affects many y) rather thannoise each y independently.Assume that the output y = (y1, .
.
.
, yT ) is a se-quence of T tags.
In linear chain CRFs, the featurevector f decomposes into a sum of local feature vec-tors gt:f(y, x) =T?t=1gt(yt?1, yt, x), (12)where gt(a, b, x) is defined on a pair of consecutivetags a, b for positions t?
1 and t.Rather than working with a score sy for eachy ?
Y , we define a collection of local scoress = {sa,b,t}, for each tag pair (a, b) and posi-tion t = 1, .
.
.
, T .
We consider noising schemeswhich independently set g?t(a, b, x) for each a, b, t.Let s?
= {s?a,b,t} be the corresponding collection ofnoised scores.We can write the log-partition function of theselocal scores as follows:A(s) = log?y?Yexp{T?t=1syt?1,yt,t}.
(13)The first derivative yields the edge marginals underthe model, ?a,b,t = p?
(yt?1 = a, yt = b | x), andthe diagonal elements of the Hessian ?2A(s) yieldthe marginal variances.Now, following (7) and (8), we obtain the follow-ing regularizer:Rq(?, x) =12?a,b,t?a,b,t(1?
?a,b,t)Var[s?a,b,t],(14)where ?a,b,t(1?
?a,b,t) measures model uncertaintyabout edge marginals, and Var[s?a,b,t] is simply theuncertainty due to noising.
Again, minimizing theregularizer means making confident predictions andhaving stable scores under feature noise.Computing partial derivatives So far, we havedefined the regularizer Rq(?, x) based on featurenoising.
In order to minimize Rq(?, x), we need totake its derivative.First, note that log?a,b,t is the difference of a re-stricted log-partition function and the log-partitionfunction.
So again by properties of its first deriva-tive, we have:?
log?a,b,t = Ep?
(y|x,yt?1=a,yt=b)[f(y, x)] (15)?
Ep?
(y|x)[f(y, x)].1173Using the fact that ?
?a,b,t = ?a,b,t?
log?a,b,t andthe fact that Var[s?a,b,t] is a quadratic function in ?,we can simply apply the product rule to derive thefinal gradient?Rq(?, x).3.1 A Dynamic Program for the ConditionalExpectationA naive computation of the gradient ?Rq(?, x) re-quires a full forward-backward pass to computeEp?
(y|yt?1=a,yt=b,x)[f(y, x)] for each tag pair (a, b)and position t, resulting in a O(K4T 2) time algo-rithm.In this section, we reduce the running time toO(K2T ) using a more intricate dynamic program.By the Markov property of the CRF, y1:t?2 only de-pends on (yt?1, yt) through yt?1 and yt+1:T onlydepends on (yt?1, yt) through yt.First, it will be convenient to define the partialsum of the local feature vector from positions i toj as follows:Gi:j =j?t=igt(yt?1, yt, x).
(16)Consider the task of computing the feature expecta-tion Ep?
(y|yt?1=a,yt=b)[f(y, x)] for a fixed (a, b, t).We can expand this quantity into?y:yt?1=a,yt=bp?(y?
(t?1:t) | yt?1 = a, yt = b)G1:T .Conditioning on yt?1, yt decomposes the sum intothree pieces:?y:yt?1=a,yt=b[gt(yt?1 = a, yt = b, x) + Fat +Bbt ],whereF at =?y1:t?2p?
(y1:t?2 | yt?1 = a)G1:t?1, (17)Bbt =?yt+1:Tp?
(yt+1:T | yt = b)Gt+1:T , (18)are the expected feature vectors summed over theprefix and suffix of the tag sequence, respectively.Note that F at and Bbt are analogous to the forwardand backward messages of standard CRF inference,with the exception that they are vectors rather thanscalars.We can compute these messages recursively in thestandard way.
The forward recurrence isF at =?bp?
(yt?2 = b | yt?1 = a)[gt(yt?2 = b, yt?1 = a, x) + Fbt?1],and a similar recurrence holds for the backward mes-sages Bbt .Running the resulting dynamic program takesO(K2Tq) time and requires O(KTq) storage,where K is the number of tags, T is the sequencelength and q is the number of active features.
Notethat this is the same order of dependence as normalCRF training, but there is an additional dependenceon the number of active features q, which makestraining slower.4 Fast Gradient ComputationsIn this section, we provide two ways to further im-prove the efficiency of the gradient calculation basedon ignoring long-range interactions and based on ex-ploiting feature sparsity.4.1 Exploiting Feature Sparsity andCo-occurrenceIn each forward-backward pass over a training ex-ample, we need to compute the conditional ex-pectations for all features active in that example.Naively applying the dynamic program in Section 3is O(K2T ) for each active feature.
The total com-plexity has to factor in the number of active fea-tures, q.
Although q only scales linearly with sen-tence length, in practice this number could get largepretty quickly.
For example, in the NER tagging ex-periments (cf.
Section 5), the average number ofactive features per token is about 20, which meansq ' 20T ; this term quickly dominates the compu-tational costs.
Fortunately, in sequence tagging andother NLP tasks, the majority of features are sparseand they often co-occur.
That is, some of the ac-tive features would fire and only fire at the same lo-cations in a given sequence.
This happens when aparticular token triggers multiple rare features.We observe that all indicator features that onlyfired once at position t have the same conditional ex-pectations (and model expectations).
As a result, wecan collapse such a group of features into a single1174feature as a preprocessing step to avoid computingidentical expectations for each of the features.
Do-ing so on the same NER tagging experiments cutsdown q/T from 20 to less than 5, and gives us a 4times speed up at no loss of accuracy.
The exactsame trick is applicable to the general CRF gradientcomputation as well and gives similar speedup.4.2 Short-range interactionsIt is also possible to speed up the method by re-sorting to approximate gradients.
In our case, thedynamic program from Section 3 together with thetrick described above ran in a manageable amountof time.
The techniques developed here, however,could prove to be useful on larger tasks.Let us rewrite the quantity we want to computeslightly differently (again, for all a, b, t):T?i=1Ep?
(y|x,yt?1=a,yt=b)[gi(yi?1, yi, x)].
(19)The intuition is that conditioned on yt?1, yt, theterms gi(yi?1, yi, x) where i is far from t will beclose to Ep?
(y|x)[gi(yi?1, yi, x)].This motivates replacing the former with the latterwhenever |i?
k| ?
r where r is some window size.This approximation results in an expression whichonly has to consider the sum of the local feature vec-tors from i?r to i+r, which is captured byGi?r:i+r:Ep?
(y|yt?1=a,yt=b,x)[f(y, x)]?
Ep?
(y|x)[f(y, x)]?
Ep?
(y|yt?1=a,yt=b,x)[Gt?r:t+r] (20)?
Ep?
(y|x)[Gt?r:t+r].We can further approximate this last expression byletting r = 0, obtaining:gt(a, b, x)?
Ep?
(y|x)[gt(yt?1, yt, x)].
(21)The second expectation can be computed from theedge marginals.The accuracy of this approximation hinges on thelack of long range dependencies.
Equation (21)shows the case of r = 0; this takes almost no addi-tional effort to compute.
However, for some of ourexperiments, we observed a 20% difference with thereal derivative.
For r > 0, the computational savingsare more limited, but the bounded-window methodis easier to implement.Dataset q d K Ntrain NtestCoNLL 20 437906 5 204567 46666SANCL 5 679959 12 761738 8240520news 81 62061 20 15935 3993RCV14 76 29992 4 9625/2 9625/2R21578 47 18933 65 5946 2347TDT2 130 36771 30 9394/2 9394/2Table 1: Description of datasets.
q: average numberof non-zero features per example, d: total numberof features, K: number of classes to predict, Ntrain:number of training examples, Ntest: number of testexamples.5 ExperimentsWe show experimental results on the CoNLL-2003Named Entity Recognition (NER) task, the SANCLPart-of-speech (POS) tagging task, and several doc-ument classification tasks.2 The datasets used aredescribed in Table 1.
We used standard splits when-ever available; otherwise we split the data at ran-dom into a test set and a train set of equal sizes(RCV14, TDT2).
CoNLL has a development setof size 51578, which we used to tune regulariza-tion parameters.
The SANCL test set is divided into3 genres, namely answers, newsgroups, andreviews, each of which has a corresponding de-velopment set.35.1 Multiclass ClassificationWe begin by testing our regularizer in the simplecase of classification where Y = {1, 2, .
.
.
,K} forK classes.
We examine the performance of the nois-ing regularizer in both the fully supervised setting aswell as the transductive learning setting.In the transductive learning setting, the learneris allowed to inspect the test features at train time(without the labels).
We used the method describedin Section 2.1 for transductive dropout.2The document classification data are availableat http://www.csie.ntu.edu.tw/?cjlin/libsvmtools/datasets and http://www.cad.zju.edu.cn/home/dengcai/Data/TextData.html3The SANCL dataset has two additional genres?emails andweblogs?that we did not use, as we did not have access todevelopment sets for these genres.1175Dataset K None L2 Drop +TestCoNLL 5 78.03 80.12 80.90 81.6620news 20 81.44 82.19 83.37 84.71RCV14 4 95.76 95.90 96.03 96.11R21578 65 92.24 92.24 92.24 92.58TDT2 30 97.74 97.91 98.00 98.12Table 2: Classification performance and transduc-tive learning results on some standard datasets.None: use no regularization, Drop: quadratic ap-proximation to the dropout noise (8), +Test: also usethe test set to estimate the noising regularizer (11).5.1.1 Semi-supervised Learning with FeatureNoisingIn the transductive setting, we used test data(without labels) to learn a better regularizer.
As analternative, we could also use unlabeled data in placeof the test data to accomplish a similar goal; thisleads to a semi-supervised setting.To test the semi-supervised idea, we use the samedatasets as above.
We split each dataset evenly into3 thirds that we use as a training set, a test set and anunlabeled dataset.
Results are given in Table 3.In most cases, our semi-supervised accuracies arelower than the transductive accuracies given in Table2; this is normal in our setup, because we used lesslabeled data to train the semi-supervised classifierthan the transductive one.45.1.2 The Second-Order ApproximationThe results reported above all rely on the ap-proximate dropout regularizer (8) that is based on asecond-order Taylor expansion.
To test the validityof this approximation we compare it to the Gaussianmethod developed by Wang and Manning (2013) ona two-class classification task.We use the 20-newsgroups alt.atheism vssoc.religion.christian classification task;results are shown in Figure 2.
There are 1427 exam-4The CoNNL results look somewhat surprising, as the semi-supervised results are better than the transductive ones.
Thereason for this is that the original CoNLL test set came from adifferent distributions than the training set, and this made thetask more difficult.
Meanwhile, in our semi-supervised experi-ment, the test and train sets are drawn from the same distribu-tion and so our semi-supervised task is actually easier than theoriginal one.Dataset K L2 Drop +UnlabeledCoNLL 5 91.46 91.81 92.0220news 20 76.55 79.07 80.47RCV14 4 94.76 94.79 95.16R21578 65 90.67 91.24 90.30TDT2 30 97.34 97.54 97.89Table 3: Semisupervised learning results on somestandard datasets.
A third (33%) of the full datasetwas used for training, a third for testing, and the restas unlabeled.10?6 10?4 10?2 100 1020.780.80.820.840.860.880.9L2 regularization strength (?
)AccuracyL2 onlyL2+Gaussian dropoutL2+Quadratic dropoutFigure 2: Effect of ?
in ???
?22 on the testset perfor-mance.
Plotted is the test set accuracy with logis-tic regression as a function of ?
for the L2 regular-izer, Gaussian dropout (Wang and Manning, 2013)+ additional L2, and quadratic dropout (8) + L2 de-scribed in this paper.
The default noising regularizeris quite good, and additional L2 does not help.
No-tice that no choice of ?
in L2 can help us combatoverfitting as effectively as (8) without underfitting.ples with 22178 features, split evenly and randomlyinto a training set and a test set.Over a broad range of ?
values, we find thatdropout plus L2 regularization performs far betterthan using just L2 regularization for any value of?.
We see that Gaussian dropout appears to per-form slightly better than the quadratic approxima-tion discussed in this paper.
However, our quadraticapproximation extends easily to the multiclass caseand to structured prediction in general, while Gaus-sian dropout does not.
Thus, it appears that our ap-proximation presents a reasonable trade-off between1176computational efficiency and prediction accuracy.5.2 CRF ExperimentsWe evaluate the quadratic dropout regularizer inlinear-chain CRFs on two sequence tagging tasks:the CoNLL 2003 NER shared task (Tjong Kim Sangand De Meulder, 2003) and the SANCL 2012 POStagging task (Petrov and McDonald, 2012) .The standard CoNLL-2003 English shared taskbenchmark dataset (Tjong Kim Sang and De Meul-der, 2003) is a collection of documents fromReuters newswire articles, annotated with four en-tity types: Person, Location, Organization, andMiscellaneous.
We predicted the label sequenceY = {LOC, MISC, ORG, PER, O}T without con-sidering the BIO tags.For training the CRF model, we used a compre-hensive set of features from Finkel et al(2005) thatgives state-of-the-art results on this task.
A totalnumber of 437906 features were generated on theCoNLL-2003 training dataset.
The most importantfeatures are:?
The word, word shape, and letter n-grams (up to6gram) at current position?
The prediction, word, and word shape of the pre-vious and next position?
Previous word shape in conjunction with currentword shape?
Disjunctive word set of the previous and next 4positions?
Capitalization pattern in a 3 word window?
Previous two words in conjunction with the wordshape of the previous word?
The current word matched against a list of nametitles (e.g., Mr., Mrs.)The F?=1 results are summarized in Table 4.
Weobtain a 1.6% and 1.1% absolute gain on the testand dev set, respectively.
Detailed results are bro-ken down by precision and recall for each tag and areshown in Table 6.
These improvements are signifi-cant at the 0.1% level according to the paired boot-strap resampling method of 2000 iterations (Efronand Tibshirani, 1993).For the SANCL (Petrov and McDonald, 2012)POS tagging task, we used the same CRF frameworkwith a much simpler set of features?
word unigrams: w?1, w0, w1?
word bigram: (w?1, w0) and (w0, w1)F?=1 None L2 DropDev 89.40 90.73 91.86Test 84.67 85.82 87.42Table 4: CoNLL summary of results.
None: no reg-ularization, Drop: quadratic dropout regularization(14) described in this paper.F?=1 None L2 DropnewsgroupsDev 91.34 91.34 91.47Test 91.44 91.44 91.81reviewsDev 91.97 91.95 92.10Test 90.70 90.67 91.07answersDev 90.78 90.79 90.70Test 91.00 90.99 91.09Table 5: SANCL POS tagging F?=1 scores for the 3official evaluation sets.We obtained a small but consistent improvementusing the quadratic dropout regularizer in (14) overthe L2-regularized CRFs baseline.Although the difference on SANCL is small,the performance differences on the test sets ofreviews and newsgroups are statistically sig-nificant at the 0.1% level.
This is also interestingbecause here is a situation where the features are ex-tremely sparse, L2 regularization gave no improve-ment, and where regularization overall matters less.6 ConclusionWe have presented a new regularizer for learninglog-linear models such as multiclass logistic regres-sion and conditional random fields.
This regularizeris based on a second-order approximation of fea-ture noising schemes, and attempts to favor mod-els that predict confidently and are robust to noisein the data.
In order to apply our method to CRFs,we tackle the key challenge of dealing with featurecorrelations that arise in the structured predictionsetting in several ways.
In addition, we show thatthe regularizer can be applied naturally in the semi-supervised setting.
Finally, we applied our methodto a range of different datasets and demonstrate con-sistent gains over standard L2 regularization.
Inves-1177Precision Recall F?=1LOC 91.47% 91.12% 91.29MISC 88.77% 81.07% 84.75ORG 85.22% 84.08% 84.65PER 92.12% 93.97% 93.04Overall 89.84% 88.97% 89.40(a) CoNLL dev.
set with no regularizationPrecision Recall F?=192.05% 92.84% 92.4490.51% 83.52% 86.8788.35% 85.23% 86.7693.12% 94.19% 93.6591.36% 90.11% 90.73(b) CoNLL dev.
set with L2 reg-ularizationPrecision Recall F?=193.59% 92.69% 93.1493.99% 81.47% 87.2892.48% 84.61% 88.3794.81% 95.11% 94.9693.85% 89.96% 91.86(c) CoNLL dev.
set with dropoutregularizationTag Precision Recall F?=1LOC 87.33% 84.47% 85.87MISC 78.93% 77.12% 78.02ORG 78.70% 79.49% 79.09PER 88.82% 93.11% 90.92Overall 84.28% 85.06% 84.67(d) CoNLL test set with no regularizationPrecision Recall F?=187.96% 86.13% 87.0377.53% 79.30% 78.4181.30% 80.49% 80.8990.30% 93.33% 91.7985.57% 86.08% 85.82(e) CoNLL test set with L2 reg-ularizationPrecision Recall F?=186.26% 87.74% 86.9981.52% 77.34% 79.3788.29% 81.89% 84.9792.15% 92.68% 92.4188.40% 86.45% 87.42(f) CoNLL test set with dropoutregularizationTable 6: CoNLL NER results broken down by tags and by precision, recall, and F?=1.
Top: developmentset, bottom: test set performance.tigating how to better optimize this non-convex reg-ularizer online and convincingly scale it to the semi-supervised setting seem to be promising future di-rections.AcknowledgementsThe authors would like to thank the anonymous re-viewers for their comments.
We gratefully acknowl-edge the support of the Defense Advanced ResearchProjects Agency (DARPA) Broad Operational Lan-guage Translation (BOLT) program through IBM.Any opinions, findings, and conclusions or recom-mendations expressed in this material are those ofthe author(s) and do not necessarily reflect the viewof the DARPA, or the US government.
S. Wager issupported by a BC and EJ Eaves SGF Fellowship.ReferencesYaser S. Abu-Mostafa.
1990.
Learning from hints inneural networks.
Journal of Complexity, 6(2):192?198.Chris M. Bishop.
1995.
Training with noise is equiva-lent to Tikhonov regularization.
Neural computation,7(1):108?116.Robert Bryll, Ricardo Gutierrez-Osuna, and FrancisQuek.
2003.
Attribute bagging: improving accuracyof classifier ensembles by using random feature sub-sets.
Pattern recognition, 36(6):1291?1302.Chris J.C. Burges and Bernhard Scho?lkopf.
1997.
Im-proving the accuracy and speed of support vector ma-chines.
In Advances in Neural Information ProcessingSystems, pages 375?381.Brad Efron and Robert Tibshirani.
1993.
An Introductionto the Bootstrap.
Chapman & Hall, New York.Jenny Rose Finkel, Trond Grenager, and ChristopherManning.
2005.
Incorporating non-local informa-tion into information extraction systems by Gibbs sam-pling.
In Proceedings of the 43rd annual meeting ofthe Association for Computational Linguistics, pages363?370.Yves Grandvalet and Yoshua Bengio.
2005.
Entropyregularization.
In Semi-Supervised Learning, UnitedKingdom.
Springer.Geoffrey E. Hinton, Nitish Srivastava, Alex Krizhevsky,Ilya Sutskever, and Ruslan R. Salakhutdinov.2012.
Improving neural networks by preventingco-adaptation of feature detectors.
arXiv preprintarXiv:1207.0580.Feng Jiao, Shaojun Wang, Chi-Hoon Lee, RussellGreiner, and Dale Schuurmans.
2006.
Semi-supervised conditional random fields for improved se-quence segmentation and labeling.
In Proceedings ofthe 44th annual meeting of the Association for Com-putational Linguistics, ACL-44, pages 209?216.Thorsten Joachims.
1999.
Transductive inference for1178text classification using support vector machines.
InProceedings of the International Conference on Ma-chine Learning, pages 200?209.Wei Li and Andrew McCallum.
2005.
Semi-supervisedsequence modeling with syntactic topic models.
InProceedings of the 20th national conference on Arti-ficial Intelligence - Volume 2, AAAI?05, pages 813?818.Gideon S. Mann and Andrew McCallum.
2007.
Sim-ple, robust, scalable semi-supervised learning via ex-pectation regularization.
In Proceedings of the Inter-national Conference on Machine Learning.Kiyotoshi Matsuoka.
1992.
Noise injection into inputsin back-propagation learning.
Systems, Man and Cy-bernetics, IEEE Transactions on, 22(3):436?440.Slav Petrov and Ryan McDonald.
2012.
Overview of the2012 shared task on parsing the web.
Notes of the FirstWorkshop on Syntactic Analysis of Non-CanonicalLanguage (SANCL).Salah Rifai, Yann Dauphin, Pascal Vincent, Yoshua Ben-gio, and Xavier Muller.
2011a.
The manifold tangentclassifier.
Advances in Neural Information ProcessingSystems, 24:2294?2302.Salah Rifai, Xavier Glorot, Yoshua Bengio, and PascalVincent.
2011b.
Adding noise to the input of a modeltrained with a regularized objective.
arXiv preprintarXiv:1104.3250.Patrice Y. Simard, Yann A.
Le Cun, John S. Denker, andBernard Victorri.
2000.
Transformation invariance inpattern recognition: Tangent distance and propagation.International Journal of Imaging Systems and Tech-nology, 11(3):181?197.Andrew Smith, Trevor Cohn, and Miles Osborne.
2005.Logarithmic opinion pools for conditional randomfields.
In Proceedings of the 43rd Annual Meeting onAssociation for Computational Linguistics, pages 18?25.
Association for Computational Linguistics.Charles Sutton, Michael Sindelar, and Andrew McCal-lum.
2005.
Feature bagging: Preventing weight un-dertraining in structured discriminative learning.
Cen-ter for Intelligent Information Retrieval, U. of Mas-sachusetts.Erik F. Tjong Kim Sang and Fien De Meulder.
2003.Introduction to the conll-2003 shared task: language-independent named entity recognition.
In Proceedingsof the seventh conference on Natural language learn-ing at HLT-NAACL 2003 - Volume 4, CONLL ?03,pages 142?147.Laurens van der Maaten, Minmin Chen, Stephen Tyree,and Kilian Q. Weinberger.
2013.
Learning withmarginalized corrupted features.
In Proceedings of theInternational Conference on Machine Learning.Stefan Wager, Sida Wang, and Percy Liang.
2013.Dropout training as adaptive regularization.
arXivpreprint:1307.1493.Li Wan, Matthew Zeiler, Sixin Zhang, Yann LeCun, andRob Fergus.
2013.
Regularization of neural networksusing dropconnect.
In Proceedings of the Interna-tional Conference on Machine learning.Sida Wang and Christopher D. Manning.
2013.
Fastdropout training.
In Proceedings of the InternationalConference on Machine Learning.1179
