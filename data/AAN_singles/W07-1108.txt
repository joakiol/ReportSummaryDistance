Proceedings of the Workshop on A Broader Perspective on Multiword Expressions, pages 57?64,Prague, June 2007. c?2007 Association for Computational LinguisticsCo-occurrence Contexts for Noun Compound InterpretationDiarmuid O?
Se?aghdhaComputer LaboratoryUniversity of Cambridge15 JJ Thomson AvenueCambridge CB3 0FDUnited Kingdomdo242@cl.cam.ac.ukAnn CopestakeComputer LaboratoryUniversity of Cambridge15 JJ Thomson AvenueCambridge CB3 0FDUnited Kingdomaac10@cl.cam.ac.ukAbstractContextual information extracted from cor-pora is frequently used to model seman-tic similarity.
We discuss distinct classesof context types and compare their effec-tiveness for compound noun interpretation.Contexts corresponding to word-word sim-ilarity perform better than contexts corre-sponding to relation similarity, even whenrelational co-occurrences are extracted froma much larger corpus.
Combining word-similarity and relation-similarity kernels fur-ther improves SVM classification perfor-mance.1 IntroductionThe compound interpretation task is frequently castas the problem of classifying an unseen compoundnoun with one of a closed set of relation categories.These categories may consist of lexical paraphrases,such as the prepositions of Lauer (1995), or deepersemantic relations, such as the relations of Girju etal.
(2005) and those used here.
The challenge lies inthe fact that by their very nature compounds do notgive any surface realisation to the relation that holdsbetween their constituents.
To identify the differ-ence between bread knife and steel knife it is not suf-ficient to assign correct word-senses to bread, steeland knife; it is also necessary to reason about howthe entities referred to interact in the world.
A com-mon assumption in data-driven approaches to theproblem is that compounds with semantically sim-ilar constituents will encode similar relations.
If ahearer knows that a fish knife is a knife used to eatfish, he/she might conclude that the novel compoundpigeon fork is a fork used to eat pigeon given thatpigeon is similar to fish and knife is similar to fork.A second useful intuition is that word pairs whichco-occur in similar contexts are likely to enter intosimilar relations.In this paper, we apply these insights to identifydifferent kinds of contextual information that cap-ture different kinds of similarity and compare theirapplicability using medium- to large-sized corpora.In keeping with most other research on the prob-lem,1 we take a supervised learning approach tocompound interpretation.2 Defining Contexts for CompoundInterpretationWhen extracting corpus information to interpret acompound such as bread knife, there are a numberof context types that might plausibly be of interest:1.
The contexts in which instances of the com-pound type appear (type similarity); e.g., allsentences in the corpus that contain the com-pound bread knife.2.
The contexts in which instances of each con-stituent appear (word similarity); e.g., all sen-tences containing the word bread or the wordknife.3.
The contexts in which both constituents appeartogether (relation similarity); e.g., all sentencescontaining both bread and knife.4.
The context in which the particular compoundtoken was found (token similarity).1Such as Girju et al (2005), Girju (2006), Turney (2006).Lapata and Keller?s (2004) unsupervised approach is a notableexception.57A simple but effective method for exploiting thesecontexts is to count features that co-occur with thetarget items in those contexts.
Co-occurrence maybe defined in terms of proximity in the text, lexi-cal patterns, or syntactic patterns in a parse graph.We can parameterise our notion of context further,for example by enforcing a constraint that the co-occurrence correspond to a particular type of gram-matical relation or that co-occurrence features be-long to a particular word class.2Research in NLP frequently makes use of one ormore of these similarity types.
For example, Culottaand Sorensen (2004) combine word similarity andrelation similarity for relation extraction; Gliozzo etal.
(2005) combine word similarity and token simi-larity for word sense disambiguation.
Turney (2006)discusses word similarity (which he calls ?attribu-tional similarity?)
and relation similarity, but fo-cusses on the latter and does not perform a compar-ative study of the kind presented here.The experiments described here investigate type,word and relation similarity.
However, token simi-larity clearly has a role to play in the interpretationtask, as a given compound type can have a differ-ent meaning in different contexts ?
for example, aschool book can be a book used in school, a bookbelonging to a school or a book about a school.
Asour data have been annotated in context, we intendto model this dynamic in future work.3 Experimental Setup3.1 DataWe used the dataset of 1443 compounds whosedevelopment is described in O?
Se?aghdha (2007).These compounds have been annotated in their sen-tential contexts using the six deep semantic rela-tions listed in Table 1.
On the basis of a dual-annotator study, O?
Se?aghdha reports agreement of66.2% (??
= 0.62) on a more general task of an-notating a noisy corpus and estimated agreement of73.6% (??
= 0.68) on annotating the six relationsused here.
These figures are superior to previouslyreported results on annotating compounds extractedfrom corpora.
Always choosing the most frequentclass (IN) would give accuracy of 21.34%, and we2A flexible framework for this kind of context definition ispresented by Pado?
and Lapata (2003).Relation Distribution ExampleBE 191 (13.24%) steel knife, elm treeHAVE 199 (13.79%) street name, car doorIN 308 (21.34%) forest hut, lunch timeINST 266 (18.43%) rice cooker, bread knifeACTOR 236 (16.35%) honey bee, bus driverABOUT 243 (16.84%) fairy tale, history bookTable 1: The 6 relation classes and their distributionin the datasetuse this as a baseline for our experiments.3.2 CorpusThe written section of the British National Corpus,3consisting of around 90 million words, was used inall our experiments.
This corpus is not large com-pared to other corpora used in NLP, but it has beenmanually compiled with a view to a balance of genreand should be more representative of the language ingeneral than corpora containing only newswire text.Furthermore, the compound dataset was also ex-tracted from the BNC and information derived fromit will arguably describe the data items more accu-rately than information from other sources.
How-ever, this information may be very sparse given thecorpus?
size.
For comparison we also use a 187million word subset of the English Gigaword Cor-pus (Graff, 2003) to derive relational informationin Section 6.
This subset consists of every para-graph in the Gigaword Corpus belonging to articlestagged as ?story?
and containing both constituents ofa compound in the dataset, whether or not they arecompounded there.
Both corpora were lemmatised,tagged and parsed with RASP (Briscoe et al, 2006).3.3 Learning AlgorithmIn all our experiments we use a one-against-all im-plementation of the Support Vector Machine.4 Ex-cept for the work described in Section 6.2 we usedthe linear kernel K(x, y) = x ?y to compute similar-ity between vector representations of the data items.The linear kernel consistently achieved superior per-formance to the more flexible Gaussian kernel ina range tests, presumably due to the sensitivity of3http://www.natcorp.ox.ac.uk/4The software used was LIBSVM (Chang and Lin, 2001).58the Gaussian kernel to its parameter settings.5 One-against-all classification (training one classifier perclass) performed better than one-against-one (train-ing one classifier for each pair of classes).
We es-timate test accuracy by 5-fold cross-validation andwithin each fold we perform further 5-fold cross-validation on the training set to optimise the singleSVM parameter C. An advantage of the linear kernelis that learning is very efficient.
The optimisation,training and testing steps for each fold take from lessthan a minute on a single processor for the sparsestfeature vectors to a few hours for the most dense, andthe folds can easily be distributed across machines.4 Word SimilarityO?
Se?aghdha (2007) investigates the effectiveness ofword-level co-occurrences for compound interpre-tation, and the results presented in this section aretaken from that paper.
Co-occurrences were identi-fied in the BNC for each compound constituent inthe dataset, using the following context definitions:win5, win10: Each word within a window of 5 or10 words on either side of the item is a feature.Rbasic, Rmod, Rverb, Rconj: These feature setsuse the grammatical relation output of theRASP parser run over the written BNC.
TheRbasic feature set conflates information about25 grammatical relations; Rmod counts onlyprepositional, nominal and adjectival nounmodification; Rverb counts only relationsamong subjects, objects and verbs; Rconjcounts only conjunctions of nouns.The feature vector for each target constituent countsits co-occurrences with the 10,000 words that mostfrequently appear in the co-occurrence relations ofinterest over the entire corpus.
A feature vector foreach compound was created by appending the vec-tors for its modifier and head, and these compoundvectors were used for SVM learning.
To model as-pects of co-occurrence association that might be ob-scured by raw frequency, the log-likelihood ratio G2(Dunning, 1993) was also used to transform the fea-ture space.5Keerthi and Lin (2003) prove that the Gaussian kernel willalways do as well as or better than the linear kernel for binaryclassification.
For multiclass classification we use multiple bi-Raw G2Accuracy Macro Accuracy Macrow5 52.60% 51.07% 51.35% 49.93%w10 51.84% 50.32% 50.10% 48.60%Rbasic 51.28% 49.92% 51.83% 50.26%Rmod 51.35% 50.06% 48.51% 47.03%Rverb 48.79% 47.13% 48.58% 47.07%Rconj 54.12% 52.44% 54.95% 53.42%Table 2: Classification results for word similarityMicro- and macro-averaged performance figuresare given in Table 2.
The micro-averaged figureis calculated as the overall proportion of items thatwere classified correctly, whereas the macro-averageis calculated as the average of the accuracy on eachclass and thus balances out any skew in the classdistribution.
In all cases macro-accuracy is lowerthan micro-accuracy; this is due to much better per-formance on the relations IN, INST, ACTOR andABOUT than on BE and HAVE.
This may be be-cause those two relations are slightly rarer and henceprovide less training data, or it may reflect a dif-ference in the suitability of co-occurrence data fortheir classification.
It is interesting that features de-rived only from conjunctions give the best perfor-mance; these features are the most sparse but ap-pear to be of high quality.
The information con-tained in conjunctions is conceptually very close tothe WordNet-derived information frequently used inword-similarity based approaches to compound se-mantics, and the performance of these features is notfar off the 56.76% accuracy (54.6% macro-average)reported for WordNet-based classification for thesame dataset by O?
Se?aghdha (2007).5 Type SimilarityType similarity is measured by identifying co-occurrences with each instance of the compoundtype in the corpus.
In effect, we are treating com-pounds as single words and calculating their wordsimilarity with each other.
The same feature extrac-tion methods were used as in the previous section.Classification results are given in Table 3.This method performs very poorly.
Sparsity is un-doubtedly a factor: 513 of the 1,443 compounds oc-nary classifiers with a shared set of parameters which may notbe optimal for any single classifier.59Accuracy Macrowin5 28.62% 27.71%win10 30.01% 28.69%Rbasic 29.31% 28.22%Rmod 26.54% 25.30%Rverb 25.02% 23.96%Rconj 24.60% 24.48%Table 3: Classification results for type similaritycur 5 times or fewer in the BNC and 186 occur justonce.
The sparser feature sets (Rmod, Rverb andRconj) are all outperformed by the more dense ones.However, there is also a conceptual problem withtype similarity, in that the context of a compoundmay contain information about the referent of thecompound but is less likely to contain informationabout the implicit semantic relation.
For example,the following compounds all encode different mean-ings but are likely to appear in similar contexts:?
John cut the bread with the kitchen knife.?
John cut the bread with the steel knife.?
John cut the bread with the bread knife.6 Relation Similarity6.1 Vector Space KernelsThe intuition underlying the use of relation similar-ity is that while the relation between the constituentsof a compound may not be made explicit in the con-text of that compound, it may be described in othercontexts where both constituents appear.
For ex-ample, sentences containing both bread and knifemay contain information about the typical interac-tions between their referents.
To extract feature vec-tors for each constituent pair, we took the maximalcontext unit to be each sentence in which both con-stituents appear, and experimented with a range ofrefinements to that context definition.
The result-ing definitions are given below in order of intuitiverichness, from measures based on word-counting tomeasures making use of the structure of the sen-tence?s dependency parse graph.allwords All words in the sentence are co-occurrence features.
This context may be pa-rameterised by specifying a limit on the win-dow size to the left of the leftmost constituentand to the right of the rightmost constituent i.e.,the words between the two constituents are al-ways counted.midwords All words between the constituents arecounted.allGRs All words in the sentence entering into agrammatical relation (with any other word) arecounted.
This context may be parameterised byspecifying a limit on the length of the shortestpath in the dependency graph from either of thetarget constituents to the feature word.shortest path All words on the shortest depen-dency path between the two constituents arefeatures.
If there is no such path, no featuresare extracted.path triples The shortest dependency path is de-composed into a set of triples and these triplesare used as features.
Each triple consists of anode on the shortest path (the triple?s centrenode) and two edges connecting that node withother nodes in the parse graph (not necessarilynodes on the path).
To generate further triplefeatures, one or both of the off-centre nodes isreplaced by part(s) of speech.
For example, theRASP dependency parse of The knife cut thefresh bread is:(|ncsubj| |cut:3_VVD| |knife:2_NN1| _)(|dobj| |cut:3_VVD| |bread:6_NN1|)(|det| |bread:6_NN1| |the:4_AT|)(|ncmod| _ |bread:6_NN1| |fresh:5_JJ|)(|det| |knife:2_NN1| |The:1_AT|)The derived set of features includes the triples{the:A:det?knife:N?cut:V:ncsubj,A:det?knife:N?cut:V:ncsubj,the:A:det?knife:N?V:ncsubj,A:det?knife:N?V:ncsubj,knife:N:ncsubj?cut:V?bread:N:dobj,N:ncsubj?cut:V?bread:N:dobj,knife:N:ncsubj?cut:V?N:dobj,N:ncsubj?cut:V?N:dobj,.
.
.}(The?
and?
arrows indicate the direction ofthe head-modifier dependency)600 100 200 300 400 50030354045505560ThresholdAccuracyaw5tripFigure 1: Effect of BNC frequency on test item ac-curacy for the allwords5 and triples contextsTable 4 presents results for these contexts; inthe case of parameterisable contexts the best-performing parameter setting is presented.
We arecurrently unable to present results for the path-basedcontexts using the Gigaword corpus.
It is clearfrom the accuracy figures that we have not matchedthe performance of the word similarity approach.The best-performing single context definition is all-words with a window parameter of 5, which yieldsaccuracy of 38.74% (36.78% macro-average).
Wecan combine the contributions of two contexts bygenerating a new kernel that is the sum of the lin-ear kernels for the individual contexts;6 the sum ofallwords5 and triples achieves the best performancewith 42.34% (40.20% macro-average).It might be expected that the richer context def-initions provide sparser but more precise informa-tion, and that their relative performance might im-prove when only frequently observed word pairs areto be classified.
However, thresholding inclusionin the test set on corpus frequency belies that ex-pectation; as the threshold increases and the test-6The summed kernel function value for a pair of items issimply the sum of the two kernel functions?
values for the pair,i.e.
:Ksum(x, y) = K1(?1(x), ?1(y)) +K2(?2(x), ?2(y))where ?1, ?2 are the context representations used by the twokernels.
A detailed study of kernel combination is presented byJoachims et al (2001).0 100 200 300 400 500050010001500ThresholdSize GWBNCFigure 2: Effect of corpus frequency on dataset sizefor the BNC and Gigaword-derived corpusing data contains only more frequent pairs, all con-texts show improved performance but the effect isstrongest for the allwords and midwords contexts.Figure 1 shows threshold-accuracy curves for tworepresentative contexts (the macro-accuracy curvesare similar).For all frequency thresholds above 6, the numberof noun pairs with above-threshold corpus frequencyis greater for the Gigaword corpus than for the BNC,and this effect is amplified with increasing threshold(see Figure 2).
However, this difference in sparsitydoes not always induce an improvement in perfor-mance, but nor does the difference in corpus typeconsistently favour the BNC.BNC GigawordAccuracy Macro Accuracy Macroaw 35.97% 33.39% 34.58% 32.62%aw5 38.74% 36.78% 37.28% 35.25%mw 32.29% 30.38% 36.24% 34.25%agr 35.34% 33.40% 35.34% 33.34%agr2 36.73% 34.81% 37.28% 35.59%sp 33.54% 31.51%trip 35.62% 34.39%aw5+ 42.34% 40.20%tripTable 4: Classification results for relation similarity616.2 String KernelsThe classification techniques described in the pre-vious subsection represent the relational context foreach word pair as a co-occurrence vector in an in-ner product space and compute the similarity be-tween two pairs as a function of their vector repre-sentations.
A different kind of similarity measure isprovided by string kernels, which count the num-ber of subsequences shared by two strings.
Thisclass of kernel function implicitly calculates an in-ner product in a feature space indexed by all pos-sible subsequences (possibly restricted by length orcontiguity), but the feature vectors are not explic-itly represented.
This approach affords our notion ofcontext an increase in richness (features can be se-quences of length ?
1) without incurring the com-putational cost of the exponential growth in the di-mension of our feature space.
A particularly flexiblestring kernel is the gap-weighted kernel described byLodhi et al (2002), which allows the subsequencesto be non-contiguous but penalises the contributionof each subsequence to the kernel value according tothe number of items occurring between the start andend of the subsequence, including those that do notbelong to the subsequence (the ?gaps?
).The kernel is defined as follows.
Let s and tbe two strings of words belonging to a vocabulary?.
A subsequence u of s is defined by a sequenceof indices i = (i1, .
.
.
, i|u|) such that 1 ?
i1 <.
.
.
< i|u| ?
|s|, where s is the length of s. Letl(i) = i|u|?
i1 +1 be the length of the subsequencein s. For example, if s is the string ?cut the breadwith the knife?
and u is the subsequence ?cut with?indexed by i then l(i) = 4. ?
is a decay parameterbetween 0 and 1.
The gap-weighted kernel value forsubsequences of length n of strings s and t is givenbyKSn(s, t) =?u?
?n?i,j:s[i]=u=t[j]?l(i)+l(j)Directly computing this function would be in-tractable, as the sum is over all |?|n possible sub-sequences of length n; however, Lodhi et al (2002)present an efficient dynamic programming algo-rithm that can evaluate the kernel in O(n|s||t|) time.Those authors?
application of string kernels to textcategorisation counts sequences of characters, but itis generally more suitable for NLP applications touse sequences of words (Cancedda et al, 2003).This kernel calculates a similarity score for a pairof strings, but for context-based compound classi-fication we are interested in the similarity betweentwo sets of strings.
We therefore define a contextkernel, which sums the kernel scores for each pairof strings from the two context sets C1, C2 and nor-malises them by the number of pairs contributing tothe sum:KCn(C1, C2) =1|C1||C2|?s?C1,t?C2KSn(s, t)That this is a valid kernel (i.e., defines an inner prod-uct in some induced vector space) can be proven us-ing the definition of the derived subsets kernel inShawe-Taylor and Cristianini (2004, p. 317).
In ourexperiments we further normalise the kernel to en-sure that KCn(C1, C2) = 1 if and only if C1 = C2.To generate the context set for a given word pair,we extract a string from every sentence in the BNCwhere the pair of words occurs no more than eightwords apart.
On the hypothesis that the contextbetween the target words was most important andto avoid the computational cost incurred by longstrings, we only use this middle context.
To facilitategeneralisations over subsequences, the compoundhead is replaced by a marker HEAD and the modifieris replaced by a marker MOD.
Word pairs for whichno context strings were extracted (i.e., pairs whichonly occur as compounds in the corpus) are repre-sented by a dummy string that matches no other.
Thevalue of ?
is set to 0.5 as in Cancedda et al (2003).Table 5 presents results for the context kernels withsubsequence lengths 1,2,3 as well as the kernel sumof these three kernels.
These kernels perform betterthan the relational vector space kernels, with the ex-ception of the summed allwords5 + triples kernel.7 Combining ContextsWe can use the method of kernel summation to com-bine information from different context types.
If ourintuition is correct that type and relation similarityprovide different ?views?
of the same semantic rela-tion, we would expect their combination to give bet-ter results than either taken alone.
This is also sug-gested by the observation that the different context62Accuracy Macron = 1 15.94% 19.88%n = 2 39.09% 37.23%n = 3 39.29% 39.29%?1,2,3 40.61% 38.53%Table 5: Classification results for gap-weightedstring kernels with subsequence lengths 1,2,3 andthe kernel sum of these kernelsAccuracy MacroRconj-G2 + aw5 54.95% 53.50%Rconj-G2 + triples 56.20% 54.54%Rconj-G2 + aw5 + triples 55.86% 54.13%Rconj-G2 + KC2 56.48% 54.89%Rconj-G2 + KC?
56.55% 54.96%Table 6: Classification results for context combina-tionstypes favour different relations: the summed stringkernel is the best at identifying IN relations (70.45%precision, 46.67% recall), but Rconj-G2 is best atidentifying all others.
This intuition is confirmed byour experiments, the results of which appear in Ta-ble 6.
The best performance of 56.55% accuracy(54.96% macro-average) is attained by the com-bination of the G2-transformed Rconj word simi-larity kernel and the summed string kernel KC?
.We note that this result, using only information ex-tracted from the BNC, compares favourably with the56.76% accuracy (54.60% macro-average) resultsdescribed by O?
Se?aghdha (2007) for a WordNet-based method.
The combination of Rconj-G2 andtriples is also competitive, demonstrating that a lessflexible learning algorithm (the linear kernel) canperform well if it has access to a richer source ofinformation (dependency paths).8 Comparison with Prior WorkPrevious work on compound semantics has tendedto concentrate on either word or relation similarity.Approaches based on word similarity generally useinformation extracted from WordNet.
For example,Girju et al (2005) train SVM classifiers on hyper-nymy features for each constituent.
Their best re-ported accuracy with an equivalent level of supervi-sion to our work is 54.2%; they then improve perfor-mance by adding a significant amount of manually-annotated semantic information to the data, as doesGirju (2006) in a multilingual context.
It is difficultto make any conclusive comparison with these re-sults due to fundamental differences in datasets andclassification schemes.Approaches based on relational similarity of-ten use relative frequencies of fixed lexical se-quences estimated from massive corpora.
Lap-ata and Keller (2004) use Web counts for phrasesNoun P Noun where P belongs to a predefined setof prepositions.
This unsupervised approach givesstate-of-the-art results on the assignment of prepo-sitional paraphrases, but cannot be applied to deepsemantic relations which cannot be directly identi-fied in text.
Turney and Littman (2005) search forphrases Noun R Noun where R is one of 64 ?join-ing words?.
Turney (2006) presents a more flexibleframework in which automatically identified n-gramfeatures replace fixed unigrams and additional wordpairs are generated by considering synonyms, butthis method still requires a Web-magnitude corpusand a very large amount of computational time andstorage space.
The latter paper reports accuracy of58.0% (55.9% macro-average), which remains thehighest reported figure for corpus-based approachesand demonstrates that relational similarity can per-form well given sufficient resources.We are not aware of previous work that comparesthe effectiveness of different classes of context forcompound interpretation, nor of work that investi-gates the utility of different corpora.
We have alsodescribed the first application of string kernels to thecompound task, though gap-weighted kernels havebeen used successfully for related tasks such as wordsense disambiguation (Gliozzo et al, 2005) and re-lation extraction (Bunescu and Mooney, 2005).9 Conclusion and Future WorkWe have defined four kinds of co-occurrence con-texts for compound interpretation and demonstratedthat word similarity outperforms a range of relationcontexts using information derived from the BritishNational Corpus.
Our experiments with the EnglishGigaword Corpus indicate that more data is not al-ways better, and that large newswire corpora maynot be ideally suited to general relation-based tasks.63On the other hand it might be expected to be veryuseful for disambiguating relations more typical ofnews stories (such as tax cut, rail strike).Future research directions include developingmore sophisticated context kernels.
Cancedda etal.
(2003) present a number of potentially useful re-finements of the gap-weighted string kernel, includ-ing ?soft matching?
and differential values of ?
fordifferent words or word classes.
We intend to com-bine the benefits of string kernels with the linguis-tic richness of syntactic parses by computing subse-quence kernels on dependency paths.
We have alsobegun to experiment with the tree kernels of Mos-chitti (2006), but are not yet in a position to reportresults.
As mentioned in Section 2, we also intendto investigate the potential contribution of the sen-tential contexts that contain the compound tokens tobe classified (token similarity).While the BNC has many desirable properties,it may also be fruitful to investigate the utility ofa large encyclopaedic corpus such as Wikipedia,which may be more explicit in its description of re-lations between real-world entities than typical textcorpora.
Wikipedia has shown promise as a re-source for measuring word similarity (Strube andPonzetto, 2006) and relation similarity (Suchanek etal.
(2006)).ReferencesTed Briscoe, John Carroll, and Rebecca Watson.
2006.The second release of the RASP system.
In Proceed-ings of the ACL-06 Interactive Presentation Sessions.Razvan C. Bunescu and Raymond J. Mooney.
2005.Subsequence kernels for relation extraction.
In Pro-ceedings of the 19th Conference on Neural Informa-tion Processing Systems.Nicola Cancedda, Eric Gaussier, Cyril Goutte, and Jean-Michel Renders.
2003.
Word-sequence kernels.
Jour-nal of Machine Learning Research, 3:1059?1082.Chih-Chung Chang and Chih-Jen Lin, 2001.
LIB-SVM: a library for support vector machines.
Soft-ware available at http://www.csie.ntu.edu.tw/?cjlin/libsvm.Aron Culotta and Jeffrey Sorensen.
2004.
Dependencytree kernels for relation extraction.
In Proceedings ofACL-04.Ted Dunning.
1993.
Accurate methods for the statisticsof surprise and coincidence.
Computational Linguis-tics, 19(1):61?74.Roxana Girju, Dan Moldovan, Marta Tatu, and DanielAntohe.
2005.
On the semantics of noun compounds.Computer Speech and Language, 19(4):479?496.Roxana Girju.
2006.
Out-of-context noun phrase seman-tic interpretation with cross-linguistic evidence.
InProceedings of CIKM-06.Alfio Gliozzo, Claudio Giuliano, and Carlo Strapparava.2005.
Domain kernels for word sense disambiguation.In Proceedings of ACL-05.David Graff, 2003.
English Gigaword.
Linguistic DataConsortium, Philadelphia.Thorsten Joachims, Nello Cristianini, and John Shawe-Taylor.
2001.
Composite kernels for hypertext cate-gorisation.
In Proceedings of ICML-01.S.
Sathiya Keerthi and Chih-Jen Lin.
2003.
Asymptoticbehaviors of support vector machines with Gaussiankernel.
Neural Computation, 15:1667?1689.Mirella Lapata and Frank Keller.
2004.
The Web as abaseline: Evaluating the performance of unsupervisedWeb-based models for a range of NLP tasks.
In Pro-ceedings of HLT-NAACL-04.Mark Lauer.
1995.
Designing Statistical LanguageLearners: Experiments on Compound Nouns.
Ph.D.thesis, Macquarie University.Huma Lodhi, Craig Saunders, John Shawe-Taylor, NelloCristianini, and Chris Watkins.
2002.
Text classifica-tion using string kernels.
Journal of Machine LearningResearch, 2:419?444.Alessandro Moschitti.
2006.
Efficient convolution ker-nels for dependency and constituent syntactic trees.
InProceedings of ECML-06.Sebastian Pado?
and Mirella Lapata.
2003.
Constructingsemantic space models from parsed corpora.
In Pro-ceedings of ACL-03.Diarmuid O?
Se?aghdha.
2007.
Annotating and learn-ing compound noun semantics.
In Proceedings of theACL-07 Student Research Workshop.John Shawe-Taylor and Nello Cristianini.
2004.
KernelMethods for Pattern Analysis.
Cambridge UniversityPress, Cambridge.Michael Strube and Simone Paolo Ponzetto.
2006.WikiRelate!
computing semantic relatedness usingWikipedia.
In Proceedings of AAAI-06.Fabian M. Suchanek, Georgiana Ifrim, and GerhardWeikum.
2006.
LEILA: Learning to extract infor-mation by linguistic analysis.
In Proceedings of theACL-06 Workshop on Ontology Learning and Popula-tion.Peter D. Turney and Michael L. Littman.
2005.
Corpus-based learning of analogies and semantic relations.Machine Learning, 60(1?3):251?278.Peter D. Turney.
2006.
Similarity of semantic relations.Computational Linguistics, 32(3):379?416.64
