NAACL-HLT 2012 Workshop on Future directions and needs in the Spoken Dialog Community: Tools and Data, pages 5?6,Montre?al, Canada, June 7, 2012. c?2012 Association for Computational LinguisticsPosition Paper: Towards Standardized Metrics and Toolsfor Spoken and Multimodal Dialog System EvaluationSebastian M?ller, Klaus-Peter Engelbrecht,Florian Kretzschmar, Stefan Schmidt, Benjamin WeissQuality and Usability Lab, Telekom Innovation Laboratories, TU BerlinErnst-Reuter-Platz 710587 Berlin, Germanysebastian.moeller@telekom.deAbstractWe argue that standardized metrics and auto-matic evaluation tools are necessary forspeeding up knowledge generation and devel-opment processes for dialog systems.1 IntroductionThe Spoken Dialogue Challenge launched byCMU (Black et al, 2011) provides a common plat-form for dialog researchers in order to test the per-formance of their systems and components againstthe state-of-the-art.
Still, evaluations are individualundertakings in most areas, as common metricsand procedures which would be applicable for arange of systems are sparse.
In the following, it isargued that significant progress can be made ifthree prerequisites are available:?
Common metrics for quantifying user and sys-tem interaction behavior and perceived quality?
Reliable models for predicting user judgmentson the basis of automatically-extracted or an-notated interaction metrics?
Methods for realistically simulating user be-havior in response to dialog systemsThe state-of-the-art and necessary research in thesethree areas is outlined in the following paragraphs.The Spoken Dialogue Challenge can contribute tovalidating such metrics and models.2 Common MetricsWhereas early assessment and evaluation cycleswere based on ad-hoc selected metrics, approacheshave been made to come up with a standard set ofmetrics for quantifying interactions between usersand systems which would make evaluation exer-cises comparable.
The International Telecommuni-cation Union (ITU-T) has standardized two sets ofmetrics: ITU-T Suppl.
24 to P-Series (2005) forspoken dialog systems, and ITU-T Suppl.
25 to P-Series Rec.
(2011) for multimodal dialog systems.These metrics describe system performance (e.g.
interms of error rates) and user/system interactionbehavior (e.g.
in terms of meta-communicationacts, durations) in a quantitative way, and can thusserve as an input to the models discussed below.Input is welcome to stabilize these metrics, so thatthey are of more use to researchers and system de-velopers.
The proper conjunction between suchmetrics and standardized annotation schemes (e.g.,Bunt et al, 2010) will strengthen the establishmentand spreading of a specific set of metrics.When it comes to user-perceived quality, Honeand Graham (2000) have made a first attempt tocome up with a validated questionnaire (SASSI),which, however, lacks a scale to assess speech out-put quality.
The approach has been put forward inITU-T Rec.
P.851 (2003) by including speech out-put and dialog managing capabilities.
A frameworkstructure was preferred over a fixed (and validated)questionnaire, in order to more flexibly address theneeds of researchers and developers.
This approachstill needs to be extended towards multimodal sys-tems, where modality appropriateness, preferenceand perceived performance have to be considered.ITU-T welcomes contributions on this topic.5For practical usage, it is desirable to have evalu-ation methods which provide diagnostic value tothe system developer, so that the sources of misbe-havior can be identified.
The diagnosis can bebased on perceptual dimensions (effectiveness,efficiency, mental effort, etc.)
or on technical char-acteristics (error rates, vocabulary coverage, etc.
)or both.
Approaches in this direction are welcomeand would significantly increase the usefulness ofevaluation exercises for the system developers.3 User-perceived Quality PredictionThe first approach to predict user judgments on thebasis of interaction metrics is the well-knownPARADISE model (Walker et al, 1997).
The mainchallenge to date is the low generalizability of suchmodels.
The reason is that many of the underlyinginput parameters are interdependent, and that asimple linear combination does not account formore complex relationships (e.g.
there might be anoptimum length for a dialog, which cannot be easi-ly described by a purely linear model).However, other algorithms such as non-linearregression, classification trees or Markov models,have not shown a significantly improved perfor-mance (M?ller et al, 2008; Engelbrecht, 2011).The latter are however adequate to describe theevolution of user opinion during the dialog, andthus might have principled advantages over modelswhich use aggregated interaction performance met-rics as an input.4 User Behavior SimulationDuring system development, it would be useful toanticipate how users would interact with a dialogsystem.
Reflected to the system developer, suchanticipations help to identify usability problemsalready before real users interact with the system.Whereas user behavior simulation has frequentlybeen used for training statistical dialog managers,only few approaches are documented which applythem to system evaluation.
Early approaches main-ly selected possible utterances from a set of col-lected data.
The MeMo workbench (Engelbrecht,2011) tried to combine statistical selection of prob-able interaction paths with the knowledge of usa-bility experts about what typically influences userbehavior.
Such knowledge can also be generatedby a conversational analysis and categorization(Schmidt et al, 2010).A different approach has been followed in theSpeechEval project (M?ller et al, 2009) wherestatistical dialog managers have been trained on alarge diverse dataset to generate utterances on aconceptual level.
The system is then amended withASR and TTS to allow for a speech-based black-box interaction with telephone-based dialog sys-tems.
Combined with diagnostic quality predictionmodels, such tools can support system developersto evaluate different dialog strategies early in thedesign cycle and at low costs, and thus avoid dis-satisfied users.
The approach still has to be extend-ed towards multimodal dialog systems.ReferencesAlan W Black et al Spoken Dialog Challenge 2010:Comparison of Live and Control Test Results, Proc.SIGDIAL2011, Portland, OR.H.
Bunt, et al: Towards an ISO standard for dialogueact annotation.
Proc.
LREC 2010, 19-21.K.-P. Engelbrecht.
2011.
Estimating Spoken DialogSystem Quality with User Models, Doctoral Disserta-tion, TU Berlin, to appear with Springer, Berlin.K.S.
Hone, R. Graham.
2000.
Towards a Tool for Sub-jective Assessment of Speech System Interfaces(SASSI), Natural Language Eng., 6(3-4):287-303.ITU-T Rec.
P.851.
2003.
Subjective Quality Evaluationof Telephone Services Based on Spoken DialogueSystems, Int.
Telecomm.
Union, Gerneva.ITU-T Suppl.
24 to P-Series Rec.
2005.
ParametersDescribing the Interaction with Spoken DialogueSystems, Int.
Telecomm.
Union, Geneva.ITU-T Suppl.
25 to P-Series Rec.
2011.
ParametersDescribing the Interaction with Multimodal DialogueSystems, Int.
Telecomm.
Union, Geneva.S.
M?ller, K.-P. Engelbrecht, R. Schleicher.
2008.
Pre-dicting the Quality and Usability of Spoken DialogueServices, Speech Communication 50:730-744.S.
M?ller, R. Schleicher, D. Butenkov, K.-P. Engel-brecht, F. G?dde, T. Scheffler, R. Roller, N.Reithinger.
2009.
Usability Engineering for SpokenDialogue Systems Via Statistical User Models, in:First IWSDS 2009, Kloster Irsee.M.A.
Walker, D.J.
Litman, C.A.
Kamm, A. Abella.1997.
PARADISE: A Framework for EvaluatingSpoken Dialogue Agents, Proc.
ACL/EACL 35thMeeting, Madrid, 271-280.S.
Schmidt, J. Stubbe, M. T?ppel, S. M?ller.
2010.
Au-tomatic Usability Evaluation for Spoken Dialog Sys-tems Based on Rules Identified by a SociotechnicalApproach, in: Proc.
PQS 2010, Bautzen.6
