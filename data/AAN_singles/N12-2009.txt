Proceedings of the NAACL HLT 2012 Student Research Workshop, pages 48?53,Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational LinguisticsDeep Unsupervised Feature Learning for Natural Language ProcessingStephan GouwsMIH Media Lab, Stellenbosch UniversityStellenbosch, South Africastephan@ml.sun.ac.zaAbstractStatistical natural language processing (NLP) buildsmodels of language based on statistical features ex-tracted from the input text.
We investigate deeplearning methods for unsupervised feature learningfor NLP tasks.
Recent results indicate that featureslearned using deep learning methods are not a sil-ver bullet and do not always lead to improved re-sults.
In this work we hypothesise that this is theresult of a disjoint training protocol which resultsin mismatched word representations and classifiers.We also hypothesise that modelling long-range de-pendencies in the input and (separately) in the out-put layers would further improve performance.
Wesuggest methods for overcoming these limitations,which will form part of our final thesis work.1 IntroductionNatural language processing (NLP) can be seen as build-ing models h : X ?
Y for mapping an input encodingx ?
X representing a natural language (NL) fragment, toan output encoding y ?
Y representing some construct orformalism used in the particular NLP task of interest, e.g.part-of-speech (POS) tags, begin-, inside-, outside (BIO)tags for information extraction, semantic role labels, etc.Since the 90s, the predominant approach has been sta-tistical NLP, where one models the problem as learning apredictive function h for mapping from h : X ?
Y usingmachine learning techniques.
Machine learning consistsof a hypothesis function which learns this mapping basedon latent or explicit features extracted from the input data.In this framework, h is usually trained in a supervisedsetting from labelled training pairs (xi, yi) ?
X ?
Y .Additionally, the discriminant function h typically oper-ates on a transformed representation of the data to a com-mon feature space encoded as a feature vector ?
(x), andthen learns a mapping from feature space to the outputspace, h : ?
(x) ?
y.
In supervised learning, the idea~x ?
X the cat sits on the mat?
(~x) ?
(x1) ?
(x2) ?
(x3) ?
(x4) ?
(x5) ?
(x6)~y ?
Y B-NP I-NP B-VP O B-NP I-NPNE Tags [the cat] NP [sits] VP [on] O [the mat] NPTable 1: Example NLP syntactic chunking task for the sentence?the cat sits on the mat?.
X represents the words in the inputspace, Y represents labels in the output space.
?
(~x) is a featurerepresentation for the input text ~x and the bottom row representsthe output named entity tags in a more standard form.is generally that features represent strong discriminatingcharacteristics of the problem gained through manual en-gineering and domain-specific insight.As a concrete example, consider the task of syntacticchunking, also called ?shallow parsing?, (Gildea and Ju-rafsky, 2002): Given an input string, e.g.
?the cat sits on the mat?,the chunking problem consists of labelling segments of asentence with syntactic constituents such as noun or verbphrases (NPs or VPs).
Each word is assigned one uniquetag often encoded using the BIO encoding1.
We repre-sent the input text as a vector of words xi ?
~x, and eachword?s corresponding label is represented by yi ?
~y (seeTable 1).
Given a feature generating function ?
(xi) anda set of labelled training pairs (xi, yi) ?
X ?
Y , the taskthen reduces to learning a suitable mapping h : ?
(X ) ?Y .Most previous works have focused on manually en-gineered features and simpler, linear models, includ-ing ?shallow?
model architectures, like the percep-tron (Rosenblatt, 1957), linear SVM (Cortes and Vap-nik, 1995) and linear-chain conditional random fields(CRFs) (Lafferty, 2001).
However, a shallow learning ar-chitecture is only as good as its input features.
Due to thecomplex nature of NL, deeper architectures may be re-1E.g.
B-NP means ?begin NP?, I-NP means ?inside NP?, and Omeans other/outside.48quired to learn data representations which contain the ap-propriate level of information for the task at hand.
Prior to2006, it was computationally infeasible to perform infer-ence in hierarchical (?deep?
), non-linear models such asmulti-layer perceptrons with more than one hidden layer.However, Hinton (2006) proposed an efficient, layer-wisegreedy method for learning the model parameters in thesearchitectures, which spurred a renewed interest in deeplearning research.Still, creating annotated training data is labour-intensive and costly, and manually designing and extract-ing discriminating features from the data to be used inthe learning process is a costly procedure requiring sig-nificant levels of domain expertise.
Over the last twodecades, the growth of available unlabeled data x ?
Xand the ubiquity of scalable computing power has shiftedresearch focus to unsupervised approaches for automat-ically learning appropriate feature representations ?
(x)from large collections of unlabeled text.Several methods have been proposed for unsuper-vised feature learning, including simple k-means cluster-ing (Lloyd, 1982), Brown clustering (Brown et al, 1992),mutual information (Shannon and Weaver, 1962), princi-pal components analysis (PCA) (Jolliffe, 2002), and in-dependent component analysis (ICA) (Hyva?rinen et al,2001).However, natural language has complex mappingsfrom text to meaning, arguably involving higher-ordercorrelations between words which these simpler meth-ods struggle to model adequately.
Advances in the ?deeplearning?
community allow us to perform efficient unsu-pervised feature learning in highly complex and high-dimensional input feature spaces, making it an attrac-tive method for learning features in e.g.
vision or lan-guage (Bengio, 2009).The standard deep learning approach is to learnlower-dimensional embeddings from the raw high-dimensional2 input space X to lower dimensional (e.g.50-dimensional) feature spaces in an unsupervised man-ner, via repeated, layer-wise, non-linear transformationof the input features, e.g.y?
= f (k)(?
?
?
f (2)(f (1)(~x)) ?
?
?
),where f (i)(x) is some non-linear function (typicallytanh) for which the parameters are learned by back prop-agating error gradients.
This configuration is referred toas a ?deep?
architecture with k layers (see Figure 1 for anexample).For feature generation, we present a trained networkwith a new vector ~x representing the input data on its2E.g.
a ?one-hot?
50,000-dimensional vector of input words, with a?1?
indicating the presence of the word at that index, and a ?0?
every-where else.Figure 1: Example of a deep model.
The input vector x is trans-formed into the hidden representation, here denoted as h1, usingan affine transformation W and a non-linearity.
Each subse-quent hidden layer hk takes as input the output of its precedinglayer h(k?1) (Bengio, 2009).input layer.
After performing one iteration of forward-propagation through the network, we can then view theactivation values in the hidden layers as dense, so-called?distributed representations?
(features) of the input data.These features can in turn be passed to an output clas-sifier layer to produce some tagging task of interest.
Re-cent work in deep learning show state-of-the-art results inpart-of-speech parsing, chunking and named-entity tag-ging (Collobert, 2011), however performance in morecomplex NLP tasks like entity and event disambiguationand semantic role labelling are still trailing behind.In this work we focus specifically on extending currentstate of the art deep neural models to improve their per-formance on these more difficult tasks.
In the followingsection we briefly review and discuss the merits and lim-itations of three of the current state of the art deep learn-ing models for NLP.
We then identify our primary re-search questions and introduce our proposed future workroadmap.2 Current State-of-the-Art andLimitationsMost work builds on the idea of a neural probabilisticlanguage model (NPLM) where words are representedby learned real-valued embeddings, and a neural networkcombines word embeddings to predict the most likelynext word.
The first successful NPLM was introducedby Bengio et al in 2003 (Bengio et al, 2003).49Historically, training and testing these models wereslow, scaling linearly in the vocabulary size.
However,several recent approaches have been proposed whichovercome these limitations (Morin and Bengio, 2005),including the work by Collobert and Weston (2008) andMnih and Hinton (2009) discussed next.2.1 Collobert & Weston (2008)Collobert and Weston (2008) present a discriminative,non-probabilistic, non-linear neural language model thatcan be scaled to train over billions of words since eachtraining iteration only computes a loss gradient over asmall stochastic sample of the training data.All K-dimensional word embeddings are initially setto a random state.
During each training iteration, an n-gram is read from the training data and each word ismapped to its respective embedding.
All embeddings arethen concatenated to form a nK-length positive trainingvector.
A corrupted n-gram is also created by replac-ing the n?th (last) word by some word uniformly chosenfrom the vocabulary.
The training criterion is that the net-work must predict positive training vectors with a scoreat least some margin higher than the score predicted forcorrupted n-grams.
Model parameters are trained simul-taneously with word embeddings via gradient descent.2.2 The Hierarchical Log Bilinear (HLBL) ModelMnih and Hinton (2007) proposed a simple probabilis-tic linear neural language model called the log bilin-ear (LBL) model.
For an n-gram context window, theLBL model concatenates the first (n?
1)K-dimensionalword embeddings and then learns a linear mapping fromR(n?1)K to RK for predicting the embedding of the nthword.
For predicting the next word, the model outputsa probability distribution over the entire vocabulary bycomputing the dot product between the predicted embed-ding and the embedding for each word in the vocabularyin an output softmax layer.
This softmax computation islinear in the length of the vocabulary for each prediction,and is therefore the performance bottleneck.In follow-up work, Mnih and Hinton (2009) speed uptraining and testing time by extending the LBL model topredict the next word by hierarchically decomposing thesearch through the vocabulary by traversing a binary treeconstructed over the vocabulary.
This speeds up train-ing and testing exponentially, but initially reduces modelperformance as the construction of the binary partition-ing has a strong effect on the model?s performance.
Theyintroduce a method for bootstrapping the construction ofthe tree by initially using a random binary tree to learnword embeddings, and then rebuilding the tree based ona clustering of the learned embeddings.
Their final resultsare superior to the standard LBL in both model perplexityand model testing time.2.3 Recursive Neural Networks (RNNs)Socher (2010; 2011) introduces a recursive neural net-work (RNN) framework for parsing natural language.Previous approaches dealt with variable-length sentencesby eitheri using a window approach (shifting a window of nwords over the input, processing each fixed-size win-dow at a time), orii by using a convolutional layer where each word isconvolved with its neighbours within some sentence-or window-boundary.RNNs operate by recursively applying the same neuralnetwork to segments of its input, thereby allowing RNNsto naturally operate on variable-length inputs.
Each pairof neighbouring words is scored by the network to reflecthow likely these two words are considered to form part ofa phrase.
Each such operation takes two K-dimensionalword vectors and outputs another K-dimensional vectorand a score.
Socher (2010) proposes several strategies(ranging from local and greedy to global and optimal) forchoosing which pairs of words to collapse into a new K-dimensional vector representing the phrase comprised ofthe two words.
By viewing these collapsing operationsas branch merging decisions, one can construct a binaryparse tree over the words in a bottom-up fashion.2.4 Discussion of LimitationsNeural language models are appealing since they canmore easily deal with missing data (unknown word com-binations) due to their inherent continuous-space repre-sentation, whereas n-gram language models (Manning etal., 1999) need to employ (sometimes ad hoc) methodsfor smoothing unseen and hence zero probability wordcombinations.The original NPLM performs well in terms of modelperplexity on held-out data; however, its training and test-ing time is very slow.
Furthermore, it provides no supportfor handling multiple word senses, the property that anyword can have more than one meaning, since each wordis assigned an embedding based on its literal string repre-sentation (i.e.
from a lookup table).The Collobert & Weston model still provides no mech-anism for handling word senses, but improves on theNPLM by adding several non-linear layers which in-crease its modelling capacity, and a convolutional layerfor modelling longer range dependencies between words.Recursive neural nets (RNNs) directly address the prob-lem of longer-range dependencies by allowing neighbourwords to be combined into their phrasal equivalents in abottom-up process.The LBL model, despite its very simple linear struc-ture, provides very good performance in terms of model50perplexity, but shares the problem of slow training andtesting times and the inability to handle word senses ordependencies between words (outside its n-gram con-text).In the HLBL model, Mnih and Hinton address the slowtesting performance of the LBL model by using a hierar-chical search tree over the vocabulary to exponentiallyspeed up testing time, analogous to the concept of class-based language models (Brown et al, 1992).
The HLBLmodel can also handle multiple word senses, but in theirevaluation they show that in practice the model learnsmultiple senses (codes) for infrequently observed wordsinstead of words with more than one meaning (Mnih andHinton, 2009).
The performance is strongly dependenton the initialisation of the tree, for which they present aniterative but non-optimal bootstrap-and-train procedure.Despite being non-optimal, it is shown to outperform thestandard LBL model in terms of perplexity.3 Mismatched Word Representations andClassifiersThe deep learning ideal is to train deep, non-linear mod-els over large collections of unlabeled data, and then usethese models to automatically extract information-rich,higher-level features3 to integrate into standard NLP orimage processing systems as added features to improveperformance.
However, several recent papers report sur-prising and seemingly contradicting results for this ideal.In the most direct comparison for NLP, Turian (2010)compares features extracted using Brown clustering (ahierarchical clustering technique for clustering wordsbased on their observed co-occurrence patterns), the hi-erarchical log-bilinear (HLBL) embeddings (Mnih andHinton, 2007) and Collobert and Weston (C+W) em-beddings (Collobert and Weston, 2008), by integratingthese as additional features in standard supervised condi-tional random field (CRF) classification systems for NLP.Somewhat surprisingly, they find that using the morecomplex C+W and HLBL features do not improve signif-icantly over Brown features.
Indeed, under several con-ditions the Brown features give the best results.These results are important for several reasons (wehighlight these results in Table 2).
The goal was to im-prove classification performance in structured predictiontasks in natural language by integrating features learnedin a deep, unsupervised approach within a standard lin-ear classification framework.
Yet these complex, deepmethods are outperformed by simpler unsupervised fea-ture extraction methods.3?Higher-level?
features simply mean combining simpler featuresextracted from a text to produce conceptually more abstract indicators,e.g.
combining word-indicators for ?attack?, ?soldier?, etc.
to form anindicator for WAR, even though ?war?
is not mentioned anywhere in thetext.System Dev Test MUC7Baseline 90.03 84.39 67.48HLBL 100-dim 92.00 88.13 75.25C&W 50-dim 92.27 87.93 75.74Brown, 1000 clusters 92.32 88.52 78.84C&W 200-dim 92.46 87.96 75.51Table 2: Final NER F1 results reported by Turian (2010).In a sense, these seem to be negative results for theutility of deep learning in NLP.
However, in this work weargue that these seemingly anomalous results stem froma mismatch between the feature learning function and theclassifier that was used in the classification (and henceevaluation) process.We consider the learning problem h : X ?
Y todecompose into h = h?(?
(X )), where ?
is the featurelearning function and h?
is a standard supervised classi-fier.
?
reads input from X and outputs encodings in fea-ture space ?(x).
h reads input in feature space ?
(x) andoutputs encodings in the output label space Y .Note that this easily extends to deep featurelearning models by simply replacing ?
(X ) with?(k)(?
?
??(2)(?
(1)(X )) ?
?
?
), for a k-layer architecture,where the first layer reads input inX and each subsequentlayer reads the output of the previous layer.Within this view of the deep learning process, we cansee that unsupervised feature learning does not happen inisolation.
Instead, the learned features only make sensewithin some learning framework, since the output of thefeature learning function ?
(and each deep layer ?
(k?1))maps to a region in feature code space which becomesin turn the input to the output classifier h?
(or subsequentlayer ?
(k)) .
We therefore argue that in a semi-supervisedor unsupervised classification problem, the feature learn-ing function ?
should be strongly dependent on the clas-sifier h?
that interprets those features, and vice versa.This notion ties in with the standard deep-learningtraining protocol of unsupervised pre-training followedby joint supervised fine-tuning (Hinton et al, 2006) of thetop classification layer and the deeper feature extractionlayers.
We conjecture that jointly training a deep featureextraction model with a linear output classifier leads tobetter linearly separable feature vectors ?
(x) than train-ing both independently.
Note that this is in contrast tohow Turian (2010) integrated the unsupervised featuresinto existing NLP systems via disjoint training.4 Proposed Work and Research QuestionsFor simpler sequence tagging tasks such as part-of-speech tagging and noun phrase chunking, the state-of-the-art models introduced in Section 2 perform ade-quately.
However, in order to make use of the increased51modelling capacity of deep neural models, and to suc-cessfully model more complex semantic tasks such asanaphora resolution and semantic role labelling, we hy-pothesise that the model needs to avoid modelling purelylocal lexical semantics and needs to efficiently handlemultiple word senses and long-range dependencies be-tween input words (or phrases) and output labels.
Wepropose to overcome the limitations of previous modelswith regard to these design goals, by focusing on the fol-lowing key areas:Input language representation: Neural models relyon vector representations of their input (as opposed todiscrete representations as in, for instance, HMMs).
InNLP, sentences are therefore encoded as real-valued em-bedding vectors.
These vectors are learned in either atask-specific setting (as in the C+W model) or as part ofa language model (as in the LBL model), where the goalis to predict the next word given the learned representa-tions of the previous words.
In order to maximise theinformation available to the model, we need to provideinformation-rich representations to the model.
Currentapproaches represent each word in a sentence using a dis-tinct word vector based on its literal string representation.However, as noted earlier, in NL the same words can havedifferent senses based on the context in which it appears(polysemy).
We propose to extend the hierarchical log-bilinear (HLBL) language model (see Section 2.2) in twoimportant ways.
We choose the HLBL model for its sim-plicity and good performance compared to more complexmodels.Firstly, we propose to replace the iterative bootstrap-and-train process for learning the hierarchical tree struc-ture over the vocabulary with a modified self-balancingbinary tree.
The tree rebalances itself from an initialrandom tree to leave most frequently accessed wordsnear the root (for shorter codes and faster access times),while moving words between clusters to maximise over-all model perplexity.Secondly, we propose to add a word sense disambigua-tion layer capable of modelling long-range dependenciesbetween input words.
For this layer we will compare amodified RNN layer to a convolutional layer.
The modi-fied RNN will embed each focus word with its nmost dis-criminative neighbour words (in a sentence context win-dow) into a new K-dimensional, sense-disambiguatedembedding vector for the focus word.
We will evaluateand optimise the final model?s learned representations byevaluating language model perplexity on held out data.Model architecture and internal representation:Deep models derive their modelling power from their hi-erarchical structure.
Each layer transforms the outputrepresentation of its previous layer, allowing the modelto learn more general and abstract feature combinationsin the higher layers which are relevant for the currenttask.
The representations on the hidden layers serveas transformed feature representations of the input datafor the output classifier.
Enforcing sparsity on the hid-den layers has been shown to produce stronger featuresfor certain tasks in vision (Coates et al, 2010).
Ad-ditionally, individual nodes might be highly correlated,which can also reduce the performance of certain clas-sifiers which make strong independence assumptions (forinstance naive Bayes).
We propose to study the effect thatenforcing sparsity in the learned feature representationshas on task performance in NLP.
Additionally, we pro-pose to evaluate the effect that an even stronger trainingobjective ?
one that encourages statistical independencebetween hidden nodes by learning factorial code repre-sentations (Hochreiter and Schmidhuber, 1999) ?
has onmodel performance.Modelling structure in the output space: Tasksin NLP mostly involve predicting labels which exhibithighly regular structure.
For instance, in part-of-speechtagging, two determiners have a very low likelihood offollowing directly on one another, e.g.
?the the?.
In or-der to successfully model this phenomenon, a model musttake into account previous (and potentially future) predic-tions when making the current prediction, e.g.
as in hid-den Markov models and conditional random fields.
Wepropose to include sequential dependencies in the outputlabels and to compare this with including a convolutionallayer below the output layer, for predicting output labelsin complex NLP tasks such as coreference resolution andevent structure detection.5 ConclusionDeep learning methods offer an attractive unsupervisedapproach for extracting higher-level features from largequantities of text data to be used for NLP tasks.
Howevercurrent attempts at integrating these features into existingNLP systems do not produce the desired performance im-provements.
We conjecture that this is due to a mismatchbetween the learned word representations and the classi-fiers used as a result of disjoint training schemes, and ourthesis roadmap suggests three key areas for overcomingthese limitations.ReferencesY.
Bengio, R. Ducharme, P. Vincent, and C. Janvin.
2003.
Aneural probabilistic language model.
Journal of MachineLearning Research, 3:1137?1155, March.Y.
Bengio.
2009.
Learning deep architectures for ai.
Founda-tions and Trends R?
in Machine Learning, 2(1):1?127.P.F.
Brown, P.V.
Desouza, R.L.
Mercer, V.J.D.
Pietra, and J.C.Lai.
1992.
Class-based n-gram models of natural language.Computational linguistics, 18(4):467?479.52A.
Coates, H. Lee, and A.Y.
Ng.
2010.
An analysis of single-layer networks in unsupervised feature learning.
Ann Arbor,1001:48109.R.
Collobert and J. Weston.
2008.
A unified architecturefor natural language processing: Deep neural networks withmultitask learning.
In Proceedings of the 25th InternationalConference on Machine Learning, pages 160?167.
ACM.R.
Collobert.
2011.
Deep learning for efficient discrimina-tive parsing.
In International Conference on Artificial In-telligence and Statistics (AISTATS).C.
Cortes and V. Vapnik.
1995.
Support-vector networks.
Ma-chine learning, 20(3):273?297.D.
Gildea and D. Jurafsky.
2002.
Automatic labeling of seman-tic roles.
Computational Linguistics, 28(3):245?288.G.E.
Hinton, S. Osindero, and Y.W.
Teh.
2006.
A fast learn-ing algorithm for deep belief nets.
Neural computation,18(7):1527?1554.S.
Hochreiter and J. Schmidhuber.
1999.
Feature extractionthrough lococode.
Neural Computation, 11(3):679?714.A.
Hyva?rinen, J. Karhunen, and E. Oja.
2001.
Independentcomponent analysis, volume 26.
Wiley Interscience.I.T.
Jolliffe.
2002.
Principal component analysis, volume 2.Wiley Online Library.J.
Lafferty.
2001.
Conditional random fields: Probabilisticmodels for segmenting and labeling sequence data.
In Pro-ceedings of ICML, 2001.S.
Lloyd.
1982.
Least squares quantization in pcm.
IEEETransactions on Information Theory, 28(2):129?137.C.D.
Manning, H. Schu?tze, and MITCogNet.
1999.
Founda-tions of statistical natural language processing, volume 999.MIT Press.A.
Mnih and G. Hinton.
2007.
Three new graphical models forstatistical language modelling.
In Proceedings of the 24thinternational conference on Machine learning, pages 641?648.
ACM.A.
Mnih and G.E.
Hinton.
2009.
A scalable hierarchical dis-tributed language model.
Advances in neural informationprocessing systems, 21:1081?1088.F.
Morin and Y. Bengio.
2005.
Hierarchical probabilistic neuralnetwork language model.
In AISTATS05, pages 246?252.F.
Rosenblatt.
1957.
The Perceptron, a Perceiving and Recog-nizing Automaton Project Para.
Cornell Aeronautical Labo-ratory.C.E.
Shannon and W. Weaver.
1962.
The mathematical theoryof communication, volume 19.
University of Illinois PressUrbana.R.
Socher, C.D.
Manning, and A.Y.
Ng.
2010.
Learning contin-uous phrase representations and syntactic parsing with recur-sive neural networks.
In Proceedings of the NIPS-2010 DeepLearning and Unsupervised Feature Learning Workshop.R.
Socher, C.C.
Lin, A.Y.
Ng, and C.D.
Manning.
2011.
Pars-ing natural scenes and natural language with recursive neuralnetworks.
In Proceedings of the 26th International Confer-ence on Machine Learning (ICML), volume 2, page 7.J.
Turian, L. Ratinov, and Y. Bengio.
2010.
Word represen-tations: A simple and general method for semi-supervisedlearning.
In Proceedings of the 48th Annual Meeting of theAssociation for Computational Linguistics, pages 384?394.Association for Computational Linguistics.53
