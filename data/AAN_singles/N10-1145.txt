Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 1011?1019,Los Angeles, California, June 2010. c?2010 Association for Computational LinguisticsTree Edit Models for Recognizing Textual Entailments, Paraphrases,and Answers to QuestionsMichael Heilman Noah A. SmithLanguage Technologies InstituteCarnegie Mellon UniversityPittsburgh, PA 15213, USA{mheilman,nasmith}@cs.cmu.eduAbstractWe describe tree edit models for representingsequences of tree transformations involvingcomplex reordering phenomena and demon-strate that they offer a simple, intuitive, andeffective method for modeling pairs of seman-tically related sentences.
To efficiently extractsequences of edits, we employ a tree kernelas a heuristic in a greedy search routine.
Wedescribe a logistic regression model that uses33 syntactic features of edit sequences to clas-sify the sentence pairs.
The approach leads tocompetitive performance in recognizing tex-tual entailment, paraphrase identification, andanswer selection for question answering.1 IntroductionMany NLP tasks involve modeling relations be-tween pairs of sentences or short texts in the samelanguage.
Examples include recognizing textual en-tailment, paraphrase identification, and question an-swering.
Generic approaches are, of course, desir-able; we believe such approaches are also feasiblebecause these tasks exhibit some similar semanticrelationships between sentences.A popular method for such tasks is Tree Edit Dis-tance (TED), which models sentence pairs by find-ing a low or minimal cost sequence of editing oper-ations to transform a tree representation of one sen-tence (e.g., a dependency or phrase structure parsetree) into a tree for the other.
Unlike grammar-based models and shallow-feature discriminative ap-proaches, TED provides an intuitive story for treepairs where one tree is derived from the other by asequence of simple transformations.The available operations in standard TED are thefollowing: insertion of a node, relabeling (i.e., re-naming) of a node, and deletion (i.e., removal) of anode.
While the restriction to these three operationspermits efficient dynamic programming solutionsfor finding a minimum-cost edit sequence (Klein,1989; Zhang and Shasha, 1989), certain interestingand prevalent phenomena involving reordering andmovement cannot be elegantly captured.
For exam-ple, consider the following sentence pair, which isa simplified version of a true entailment (i.e., thepremise entails the hypothesis) in the developmentdata for the RTE-3 task.Premise: Pierce built the home for his daughter offRossville Blvd, as he lives nearby.Hypothesis: Pierce lives near Rossville Blvd.In a plausible dependency tree representation ofthe premise, live and Rossville Blvd would be in sep-arate subtrees under built.
In the hypothesis tree,however, the corresponding nodes would be in agrandparent-child relationship as part of the samephrase, lives near Rossville Blvd.
In general, onewould expect that short transformation sequences toprovide good evidence of true entailments.
How-ever, to account for the grandparent-child relation-ship in the hypothesis, TED would produce a fairlylong sequence, relabeling nearby to be near, delet-ing the two nodes for Rossville Blvd, and then re-inserting those nodes under near.We describe a tree edit approach that allows formore effective modeling of such complex reorderingphenomena.
Our approach can find a shorter andmore intuitive edit sequence, relabeling nearby to benear, and then moving the whole subtree RossvilleBlvd to be a child of near, as shown in Figure 1.A model should also be able to consider character-istics of the tree edit sequence other than its overalllength (e.g., how many proper nouns were deleted).Using a classifier with a small number of syntactic1011Pierce lives near Rossville Blvd.Pierce built the home for his daughter off Rossville Blvd, as he lives nearby.Pierce built the home for his daughter off Rossville Blvd, as he lives near.built the home for his daughter off, as Pierce he lives near Rossville Blvd.Pierce built the home for his daughter off, as he lives near Rossville Blvd.Piercie lvsinaRoBd.buvti hmcfPiinPg,,yRcybuvti hmcfPiinRBRb!"#RPiercie is$i%sieifi eir&%sieifi rls uiP$iR",Figure 1: A tree edit sequence transforming a premise to an entailed hypothesis.
Dependency types and parts of speechare omitted for clarity.features, our approach allows us to learn?from la-beled examples?how different types of edits shouldaffect the model?s decisions (e.g., about whether twosentences are paraphrases).The structure of this paper is as follows.
?2 in-troduces our model and describes the edit opera-tions that were implemented for our experiments.
?3 details the search-based procedure for extractingedit sequences for pairs of sentences.
?4 describesthe classifier for sentence pairs based on features oftheir corresponding edit sequences.
?5 describes andpresents the results of experiments involving recog-nizing textual entailment (Giampiccolo et al, 2007),paraphrase identification (Dolan et al, 2004), and ananswer selection task for question answering (Wanget al, 2007).
?6 addresses related work, and ?7 pro-vides concluding remarks.2 Extended Tree Edit SequencesThis section defines a tree edit sequence and de-scribes the operations used in our experiments.We begin with some conventions.
We use depen-dency trees as the structure upon which the tree ed-its will operate.
The child nodes for a given parentare represented in a head-outward fashion such thatthe left and right children are separate lists, with theleft- and right-most elements as the last members oftheir respective lists, as in most generative depen-dency models (Eisner, 1996).
Each node consists ofa lemmatized word token as its main label (hereafter,lemma), a part of speech tag (POS), and a syntacticrelation label for the edge to its parent.
We assumethe root node has a special dummy edge label ROOT.Let Tc be a ?current tree?
that is being trans-formed and let Tt be a ?target tree?
into which Tcwill ultimately be transformed.
Let T (i) be a nodewith an index i into the tree T , where the indices arearbitrary (e.g., they could be word positions).2.1 DefinitionWe define a tree edit sequence to be a series of editoperations that transform a source tree (the initialTc) into a target tree Tt.1 While TED permits onlyinsert, relabel, and delete operations, edit sequencesmay contain more complex operations, such as mov-ing entire subtrees and re-ordering child nodes.2.2 Implemented OperationsFor our experiments, we used the types of edit op-erations listed in Table 1.2 The first six operationsare straightforward extensions of the insert, rela-bel and delete operations allowed in TED.
The finalthree operations, MOVE-SUBTREE, NEW-ROOT,and MOVE-SIBLING, enable succinct edit se-quences for complex transformations.
For a givencurrent tree, there may be many instantiations ofeach operation (e.g., DELETE-LEAF could be in-voked to delete any of a number of leaf nodes).
Notethat any tree can be transformed into any other sim-ply by deleting all nodes from the one tree and in-serting all the nodes in the other.
However, our setof tree edit operations permits more concise and in-tuitive edit sequences.3 Searching for Tree Edit SequencesTo model sentence pairs effectively, we seek a shortsequence of tree edits that transforms one tree intoanother.
The space of possible edit sequences, aswith TED and many other methods involving trees,1Such a sequence is sometimes called a ?script?
for TED.2We leave for future work the exploration of other opera-tions (e.g., swapping parent and child nodes).1012Operation Arguments DescriptionINSERT-CHILD node index j, new lemma l, POS p,edge label e, side s ?
{left , right}Insert a node with lemma l, POS p, and edge label e as thelast child (i.e., farthest from parent) on side s of T (j).INSERT-PARENT non-root node index j, new lemma l,new POS p, edge label e,side s ?
{left , right}Create a node with lemma l, POS p, and edge label e. MakeT (j) a child of the new node on side s. Insert the new nodeas a child of the former parent of T (j) in the same position.DELETE-LEAF leaf node index j Remove the leaf node T (j).DELETE-&-MERGE node index j(s.t.
T (j) has exactly 1 child)Remove T (j).
Insert its child as a child of T (j)?s formerparent in the same position.RELABEL-NODE node index j, new lemma l, new POS p Set the lemma of T (j) to be l and its POS to be p.RELABEL-EDGE node index j, new edge label e Set the edge label of T (j) to be e.MOVE-SUBTREE node index j, node index k(s.t.
T (k) is not a descendant of T (j)),side s ?
{left , right}Move T (j) to be the last child on the s side of T (k).NEW-ROOT non-root node index j,side s ?
{left , right}Make T (j) the new root node of the tree.
Insert the formerroot as the last child on the s side of T (j).MOVE-SIBLING non-root node index j,side s ?
{left , right},position r ?
{first , last}Move T (j) to be the r child on the s side of its parent.Table 1: Possible operations in our extended tree edit implementation.
All are described as operations to tree T .is exponentially large in the size of the trees.
How-ever, while dynamic programming solutions existfor TED (Klein, 1989; Zhang and Shasha, 1989),it is unlikely that such efficient algorithms are avail-able for our problem because of the lack of localityrestrictions on edit operations.33.1 Algorithm for Extracting SequencesRather than dynamic programming, we use greedybest-first search (Pearl, 1984) to efficiently find sen-sible (if not minimal) edit sequences.
The distin-guishing characteristic of greedy best-first search isthat its function for evaluating search states is sim-ply a heuristic function that estimates the remainingcost, rather than a heuristic function plus the costso far (e.g., number of edits), as in other types ofsearch.Here, the initial search state is the source tree, thecurrent state is Tc, and the goal state is Tt.
The func-tion for generating the successors for a given statereturns returns trees for all possible specifications ofoperations on Tc (?2.2), subject to the minimal con-straints to be described in ?3.3.
The enumerationorder of the edits in the search procedure (i.e., theorder in which states are explored) follows the or-der of their presentation in Table 1.
In preliminary3Gildea (2003) proposes a dynamic programming algorithmfor a related tree alignment problem, but it is still exponential inthe maximum number of children for a node.experiments, varying this order had no effect on theextracted transformations.3.2 Tree Kernel HeuristicIn our greedy search approach, the evaluation func-tion?s value for a state depends only on the heuristicfunction?s estimate of how different the current treeat that state is from the target tree.
Using this func-tion, at each step, the search routine chooses the nextstate (i.e., edit) so as to minimize the difference be-tween the current and target trees.We use a tree kernel to define the heuristic func-tion.
A kernel is a special kind of symmetric func-tion from a pair of objects to a real number.
Itcan be interpreted as the inner product of those ob-jects represented in some real-valued feature space(Scho?lkopf and Smola, 2001).
A tree kernel, as pro-posed by Collins and Duffy (2001), is a convolutionkernel4 whose input is a pair of trees and whose out-put is a positive number indicating the similarity ofthe sets of all their subtrees.The dimensionality of the feature vector associ-ated with a tree kernel is thus unbounded in general,and larger trees generally lead to larger kernel val-ues.
Direct use as a search heuristic would lead tothe exploration of states for larger and larger trees,even ones larger than the target tree.
Thus, as in4Haussler (1999) provides a proof, which can be extendedfor our kernel, that tree kernels are valid kernel functions.1013Equation 1, the search heuristic H ?normalizes?
thekernel K of the current tree Tc and target tree Ttto unit range by dividing by the geometric mean ofthe kernels comparing the individual trees to them-selves.5 Also, the normalized value is subtractedfrom 1 so as to make it a difference rather than asimilarity.
The search routine will thus reach thegoal state when the heuristic reaches 0, indicatingthat the current and target trees are identical.H(Tc) = 1?K(Tc, Tt)?K(Tc, Tc)?K(Tt, Tt)(1)Kernels are most commonly used in the efficientconstruction of margin-based classifiers in the im-plied representation space (e.g., Zelenko et al,2003).
Here, however, the kernel helps to find arepresentation (i.e., an edit sequence) for subsequentmodeling steps.We are effectively mapping the source, current,and target trees to points on the surface of a high-dimensional unit sphere associated with the normal-ized kernel.
In this geometric interpretation, thesearch heuristic in Equation 1 leads the search al-gorithm to explore reachable trees along the surfaceof this sphere, always choosing the one whose an-gle with the target tree is smallest, until the angle is0.
The path on the sphere corresponds to an edit se-quence, from which we will derive edit features in?4 for classification.Our kernel is based on the partial tree kernel(PTK) proposed by Moschitti (2006).
It considersmatches between ordered subsequences of childrenin addition to the full sequences of children as inCollins and Duffy (2001).
This permits a very fine-grained measure of tree pair similarity.
Importantly,if two nodes differ only by the presence or positionof a single child, they will still lead to a large ker-nel function value.
We also sum over the similaritiesbetween all pairs of nodes, similar to (Collins andDuffy, 2001).Since the PTK considers non-contiguous subse-quences, it is very computationally expensive.
Wetherefore restrict our kernel to consider only con-tiguous subsequences, as in the contiguous tree ker-nel (CTK) (Zelenko et al, 2003).5This normalized function is also guaranteed to be a kernelfunction (Scho?lkopf and Smola, 2001).To define our kernel, we begin with a similarityfunction for pairs of nodes n1 and n2 that dependson their lemmas, POS tags, edge labels, and sideswith respect to their parents:6s(n1, n2) =?
(l(n1), l(n2))??f?{l,e,p,s}?
(f(n1), f(n2)) (2)where ?
returns 1 if its arguments are equivalent, 0otherwise.
l, e, p, and s are used here as functionsto select the lemma, edge label, POS, and side ofa node.
Equation 2 encodes the linguistic intuitionthat the primary indicator of node similarity shouldbe a lexical match between lemmas.
If the lem-mas match, then edge labels, POS, and the locations(sides) relative to their parents are also considered.The kernel is defined recursively (starting fromthe roots), where ni is a node in the set of nodesNTi in tree Ti:K(T1, T2) =?n1?{NT1}?n2?{NT2}?
(n1, n2) (3)?
(n1, n2) = ?
(?2s(n1, n2) + (4)?J1,J2,|J1|=|J2|l(J1)?i=1?
(cn1 [J1i], cn2 [J2i])?
?J1 = ?J11, J12, J13, .
.
.?
is an index sequence as-sociated with any contiguous ordered sequence ofchildren cn1 of node n1 (likewise for J2).
J1i andJ2i point to the ith children in the two sequences.| ?
| returns the length of a sequence.The kernel includes two decay factors: ?
for thelength of child subsequences, as in Zelenko et al(2003) and Moschitti (2006); and ?
for the height ofthe subtree, as in Collins and Duffy (2001) and Mos-chitti (2006).
We set both to 0.25 in our experimentsto encourage the search to consider edits leading tosmaller matches (e.g., of individual parent-child de-pendencies) before larger ones.76The side of a node relative to its parent in a dependency treeis important: two parent nodes with the same children shouldnot be considered exact matches if children are on differentsides (e.g., defeated the insurgents and the insurgents defeated).7From experiments with the paraphrase training set (?5.2),performance does not appear sensitive to the decay parameters.Settings of 0.1, 0.2, 0.3, and 0.4 led to 10-fold cross-validation1014The main difference between our kernel and theCTK is that we sum over all pairs of subtrees(Equation 3).
In contrast, the CTK only consid-ers only one pair of subtrees.
When the CTKis applied to relation extraction by Culotta andSorensen (2004), each subtree is the smallest com-mon subtree that includes the entities between whicha relation may exist (e.g., the subtree for Texas-based energy company Exxon Mobil when extract-ing ORGANIZATION-LOCATION relations).3.3 Constraints on the Search SpaceFor computational efficiency, we impose the follow-ing three constraints to simplify the search space.Note that the first two simply prune away obviouslyunhelpful search states.1.
For INSERT-CHILD, INSERT-PARENT, andRELABEL-NODE edits, the lemma and POS ofthe node to insert must occur in the target tree.Also, the pair consisting of the lemma for thenode to insert and the lemma for its prospectiveparent must not appear more times in the result-ing tree than in the target tree.2.
For MOVE-SUBTREE edits, the pair consistingof the lemma for the node to move and thelemma for its prospective parent must exist in thetarget tree.3.
For INSERT-CHILD and INSERT-PARENTedits, the edge labels attaching the newly in-serted nodes to their parents are always the mostfrequent edge label for the given POS.8 Furtheredits can modify these edge labels.3.4 Search Error and FailureThe search does not always find optimal edit se-quences, but most sequences seem reasonable uponinspection.
However, for some cases, the searchdoes not find a sequence in a reasonable numberof iterations.
We therefore set an upper limit ofmaxIters = 200 on the number of iterations.9 Inaccuracy values that were not significantly different from eachother.
However, we did observe that increased search failure(?3.4) resulted from settings above 0.5.8Edge label frequencies for each POS were computed fromthe training data for the MST parser (McDonald et al, 2005).9maxIters = 400 for the textual entailment experiments toaccount for multi-sentence premises.
For all tasks, extractingsequences took about 5 seconds on average per sentence pairwith 1 GB of RAM on a 3.0 GHz machine.practice, this constraint is enforced a small fractionof the time (e.g., less than 0.1% of the time for theanswer selection training data).
If no goal state isfound after maxIters iterations, a special unknownsequence feature is recorded.4 Classification of SequencesGiven a training set of labeled sentence pairs, af-ter extracting edit sequences, we train a logisticregression (LR) classification model (Hastie et al,2001) on the labels and features of the extracted se-quences.10 We optimize with a variant of Newton?smethod (le Cessie and van Houwelingen, 1997).The tree edit models use a set of 33 features ofedit sequences to classify sentence pairs.
We usedthe training data for the paraphrase task (?5.2) to de-velop this set.
All features are integer-valued, andmost are counts of different types of edits.
Five arecounts of the nodes in the source tree that were notedited directly by any operations (though their an-cestors or descendants may have been).
Table 2 de-scribes the features in detail.5 ExperimentsExperiments were conducted to evaluate tree editmodels for three tasks: recognizing textual entail-ment (Giampiccolo et al, 2007), paraphrase identi-fication (Dolan et al, 2004), and an answer selec-tion task (Wang et al, 2007) for question answering(Voorhees, 2004).
The feature set and first tree editmodel were developed for paraphrase, and then ap-plied to the other tasks with very few modifications(all explained below) and no further tuning.115.1 Recognizing Textual EntailmentA tree edit model was trained for recognizing tex-tual entailment (RTE).
Here, an instance consists of10In cross-validation experiments with the training data, wefound that unregularized LR outperformed SVMs (Vapnik,1995) and `2-regularized LR, perhaps due to the small numberof features in our models.11All datasets were POS-tagged using Ratnaparkhi?s (1996)tagger and parsed for dependencies using the MST Parser(McDonald et al, 2005).
Features were computed fromPOS and edge label information in the dependency parses.The WordNet API (Miller et al, 1990) was used forlemmatization only.
An appendix with further experimen-tal details is available at http://www.ark.cs.cmu.edu/mheilman/tree-edit-appendix/.1015Feature DescriptiontotalEdits # of edits in the sequence.XEdits #s of X edits (where X isone of the nine edit types inTable 1).relabelSamePOS,relabelSameLemma,relablePronoun,relabelProper,relabelNum#s of RELABEL-NODE editsthat: preserve POS, preservelemmas, convert betweennouns and pronouns, changeproper nouns, change numericvalues by more than 5% (toallow rounding), respectively.insertVorN,insertProper#s of INSERT-CHILD orINSERT-PARENT edits that:insert nouns or verbs, insertproper nouns, respectively.removeVorN,removeProper,removeSubj,removeObj,removeVC,removeRoot#s of REMOVE-LEAF orREMOVE-&-MERGE editsthat: remove nouns or verbs,remove proper nouns, removenodes with subject edge la-bels, remove nodes with objectedge labels, remove nodeswith verb complement edgelabels, remove nodes withroot edge labels (which mayoccur after NEW-ROOT edits),respectively.relabelEdgeSubj,relabeledgeObj,relabelEdgeVC,relabelEdgeRoot#s of RELABEL-EDGE editsthat: change to or from subjectedge labels, change to or fromobject edge labels, change toor from verb complement edgelabels, change to or from rootedge labels, respectively.uneditedNodes,uneditedNum,uneditedVerbs,uneditedNouns,uneditedProper#s of unedited nodes: in total,that are numeric values, thatare verbs, that are nouns, thatare proper nouns, respectively.unknownSeq 1 if no edit sequence wasfound and 0 otherwise (?3.4).Table 2: Tree edit sequence classification features.a ?premise,?
which is a sentence or paragraph abouta particular topic or event, and a ?hypothesis,?
whichis a single, usually short, sentence that may or maynot follow from the premise.
The task is to de-cide whether or not the hypothesis is entailed by thepremise (Giampiccolo et al, 2007).Tree edit sequences were extracted in one direc-tion, from premise to hypothesis.12 Since premises12It is counter-intuitive to model adding information throughextensive insertions, for both entailment and answer selection.System Acc.
% Prec.
% Rec.
%Harmeling, 2007 59.5 - -de Marneffe et al, 2006 60.5 61.8 60.2M&M, 2007 (NL) 59.4 70.1 36.1M&M, 2007 (Hybrid) 64.3 65.5 63.9Tree Edit Model 62.8 61.9 71.2Table 3: Results for recognizing textual entailments.
Pre-cision and recall values are for the true entailment class.Results for de Marneffe et al (2006) were reported byMacCartney and Manning (2008).
Harmeling (2007)only reported accuracy.may consist of multiple sentences, we attach sen-tences as children of dummy root nodes, for boththe premise and hypothesis.
The model was trainedon the development set (i.e., training data) for RTE-3 along with all the data from the RTE-1 and RTE-2tasks.
It was then evaluated on the RTE-3 test set.We report precision and recall for true entailments,and overall accuracy (i.e., percentage correct).We compare to four systems that use syntactic de-pendencies and lexical semantic information.13 DeMarneffe et al (2006) described an RTE systemthat finds word alignments and then classifies sen-tence pairs based on those alignments.
MacCart-ney and Manning (2008) used an inference pro-cedure based on Natural Logic, leading to a rela-tively high-precision, low-recall system.
MacCart-ney and Manning (2008) also tested a hybrid of thenatural logic system and the complementary systemof de Marneffe et al (2006) to improve coverage.Harmeling (2007) took an approach similar to oursinvolving classification based on transformation se-quences, but with less general operations and a morecomplex, heuristic procedure for finding sequences.Table 3 presents RTE results, showing that thetree edit model performs competitively.
While itdoes not outperform state-of-the-art RTE systems,the tree edit model is simpler and less tailored to thistask than many other RTE systems based on similarlinguistic information.13The top-performing RTE systems often involve significantmanual engineering for the RTE task.
Also, many employ tech-niques that make them not very comparable to our approach(e.g., theorem proving).
We also note that Kouylekov andMagnini (2005) report 55% accuracy for RTE-2 using TED.
SeeGiampiccolo et al (2007) for more RTE-3 results.1016System Acc.
% Prec.
% Rec.
%Wan et al, 2006 75.6 77 90D&S, 2009 (QG) 73.9 74.9 91.3D&S, 2009 (PoE) 76.1 79.6 86.0Tree Edit Model 73.2 75.7 87.8Table 4: Paraphrase identification results, with precisionand recall measures for true (positive) paraphrases.
Wanet al (2006) report precision and recall values with onlytwo significant digits.System MAP MRRPunyakanok et al, 2004 0.3814 0.4462+WN 0.4189 0.4939Cui et al, 2005 0.4350 0.5569+WN 0.4271 0.5259Wang et al, 2007 0.4828 0.5571+WN 0.6029 0.6852Tree Edit Model 0.6091 0.6917Table 5: Results for the task of answer selection for ques-tion answering.
+WN denotes use of WordNet features.5.2 Paraphrase IdentificationA tree edit model was trained and tested for para-phrase identification using the the Microsoft Re-search Paraphrase Corpus (Dolan et al, 2004).
Thetask is to identify whether two sentences convey es-sentially the same meaning.The standard training set was used to train the treeedit classification model to distinguish between trueand false paraphrases.
Since there is no predefineddirection for paraphrase pairs, we extracted two se-quences for each pair (one in each direction) andsummed the feature values.
The model was evalu-ated with the standard test set.We report accuracy, positive class precision (i.e.,percentage of predicted positive paraphrases thathad positive gold-standard labels), and positive classrecall (i.e., percentage of positive gold-standard la-bels that were predicted to be positive paraphrases).We compare to two of the best performance ap-proaches to paraphrase.
One approach, by Wan et al(2006), uses an SVM classifier with features basedon syntactic dependencies, TED, unigram overlap,and BLEU scores (Papineni et al, 2002).
The othersystem, by Das and Smith (2009), is based on aquasi-synchronous grammar (QG; Smith and Eisner,2006), a probabilistic model that allows loose align-ments between trees but prefers tree isomorphism.In addition to syntactic dependencies, the QG modelutilizes entity labels from BBN Identifinder (Bikelet al, 1999) and lexical semantics knowledge fromWordNet.
Das and Smith (2009) also use a productof experts (PoE) (Hinton, 1999) to combine the QGmodel with lexical overlap features.Table 4 shows the test set results for all of the sys-tems.
While the tree edit model did not outperformthe other systems, it produced competitive results.Moreover, the tree edit model does not make useof BLEU scores (Wan et al, 2006), entity labelingcomponents, lexical semantics knowledge sourcessuch as WordNet (beyond lemmatization), or systemcombination techniques (Das and Smith, 2009).5.3 Answer Selection for Question AnsweringA tree edit model was trained for answer selec-tion in question answering (QA).
In this task, aninstance consists of a short factual question (e.g.,Who wrote the ?Tale of Genji??)
and a candidate an-swer sentence retrieved by the information retrievalcomponent of a question answering system.
For apositive instance, the text will correctly answer thequestion?though perhaps indirectly.
It may alsocontain various extraneous information (e.g., Kanoscript made possible the development of a secularJapanese literature, beginning with such Late Heianclassics as Lady Murasaki?s ?Tales of Genji.?).
Fora given set of questions, the task here is to rank can-didate answers (Wang et al, 2007).The experimental setup is the same as in Wanget al (2007).
We trained the tree edit model onthe manually judged positive and negative QA pairsfrom previous QA tracks at the Text REtrieval Con-ference (TREC-8 through TREC-12).
The goal ofthe task is to rank answer candidates rather than clas-sify them; therefore, after training a logistic regres-sion classifier, we rank the answer candidates for agiven question by their posterior probabilities of cor-rectness according to the model.We tested our model with QA pairs from TREC-13.
We report Mean Average Precision (MAP) andMean Reciprocal Rank (MRR), which are informa-tion retrieval measures for ranked lists.Tree edit sequences were extracted only in one di-rection, from answer to question.
We compare ourtree edit model to three other systems as they are re-ported by Wang et al (2007).
Wang et al use a QGmodel, incorporating information from dependency1017trees, entity labels from BBN Identifinder (Bikel etal., 1999), and lexical semantics knowledge fromWordNet (Miller et al, 1990).
Cui et al (2005) de-veloped an information theoretic measure based ondependency trees.
Punyakanok et al (2004) used ageneralization of TED to model the QA pairs.
Fortheir experiments, Wang et al also extended both ofthe latter models to utilize WordNet.Table 5 displays answer selection results, includ-ing test set results for the baseline systems with andwithout lexical semantic information from Word-Net.
The tree edit model, which does not use lex-ical semantics knowledge, produced the best resultreported to date.
The results for the tree edit modelare statistically significantly different (sign test, p <0.01) from the results for all except the Wang et al(2007) system with WordNet (p > 0.05).5.4 DiscussionThe parameter settings learned for the features in Ta-ble 2 were broadly similar for the three tasks.
Forexample, operations involving changes to subjectsand proper nouns tended to be associated with non-paraphrases, false entailments, and incorrect an-swers.
We did not observe any interesting differ-ences in the parameter values.While the tree edit models perform competitivelyin multiple tasks by capturing relevant syntactic phe-nomena, it is clear that syntax alone cannot solvethese semantic tasks.
Fortunately, this approach isamenable to extensions, facilitated by the separa-tion of the representation extraction and classifica-tion steps.
Richer edits could be included; lexical se-mantics could be integrated into the classifier or thesearch heuristic; or edit sequences might be foundfor other types of trees, such as semantic parses.6 Related WorkTED is a widely studied technique with many appli-cations (Klein, 1989; Zhang and Shasha, 1989; Pun-yakanok et al, 2004; Schilder and McInnes, 2006).See Bille (2005) for a review.
Chawathe and Garcia-Molina (1997) describe a tree edit algorithm fordetecting changes in structured documents that in-corporates edits for moving subtrees and reorderingchildren.
However, they make assumptions unsuit-able for natural language, such as the absence of re-cursive syntactic rewrite rules.
Bernard et al (2008)use EM to learn the costs for simple insert, relabel,and delete edits, but they only discuss experimentsfor digit recognition and a task using artificial data.Much research has focused on modeling word re-ordering phenomena and syntactic alignments (e.g.,Gildea, 2003; Smith and Eisner, 2006; inter alia),and such methods have been applied successfully tosemantic tasks (de Marneffe et al, 2006; Wang etal., 2007; Das and Smith, 2009).
While we not de-scribe connections to such approaches in detail dueto space limitations, we note that theoretical con-nections are possible between transformations andalignments (Chawathe and Garcia-Molina, 1997).Tree kernels have been applied to a variety of nat-ural language tasks (Collins and Duffy, 2001; Ze-lenko et al, 2003; Culotta and Sorensen, 2004).
Ofparticular interest, Zanzotto and Moschitti (2006)describe a kernel for RTE that takes tree pairs, ratherthan single trees, as input.
To our knowledge, ouruse of a tree kernel as a search heuristic is novel.7 ConclusionWe described tree edit models that generalize TEDby allowing operations that better account for com-plex reordering phenomena and by learning fromdata how different edits should affect the models de-cisions about output variables of interest (e.g., thecorrectness of answers).
They offer an intuitiveand effective method for modeling sentence pairs.They led to competitive performance for three tasks:paraphrase identification, recognizing textual entail-ment, and answer selection for question answering.AcknowledgmentsWe acknowledge partial support from the Institute of Ed-ucation Sciences, U.S. Department of Education, throughGrant R305B040063 to Carnegie Mellon University; andthe National Science Foundation through a Graduate Re-search Fellowship for the first author and grant IIS-0915187 to the second author.
We thank Mengqiu Wangand Dipanjan Das for their help with the data, Andre?
Mar-tins for his geometric interpretation of our search proce-dure, and the anonymous reviewers for their comments.ReferencesM.
Bernard, L. Boyer, A. Habrard, and M. Sebban.
2008.Learning probabilistic models of tree edit distance.1018Pattern Recognition.D.
M. Bikel, R. Schwartz, and R. M. Weischedel.
1999.An algorithm that learns what?s in a name.
MachineLearning, 34.P.
Bille.
2005.
A survey on tree edit distance and relatedproblems.
Theoretical Computer Science, 337.S.
Chawathe and H. Garcia-Molina.
1997.
Meaningfulchange detection in structured data.
In Proc.
of ACMSIGMOD.M.
Collins and N. Duffy.
2001.
Convolution kernels fornatural language.
In Proc.
of NIPS.H.
Cui, R. Sun, K. Li, M. Kan, , and T. Chua.
2005.Question answering passage retrieval using depen-dency relations.
In Proc.
of ACM-SIGIR.A.
Culotta and J. Sorensen.
2004.
Dependency tree ker-nels for relation extraction.
In Proc.
of ACL.D.
Das and N. A. Smith.
2009.
Paraphrase identifica-tion as probabilistic quasi-synchronous recognition.
InProc.
of ACL-IJCNLP.M.
de Marneffe, B. MacCartney, T. Grenager, D. Cer,A.
Rafferty, and C. D. Manning.
2006.
Learning todistinguish valid textual entailments.
In Proc.
of theSecond PASCAL Challenges Workshop.B.
Dolan, C. Quirk, and C. Brockett.
2004.
Unsuper-vised construction of large paraphrase corpora: Ex-ploiting massively parallel news sources.
In Proc.
ofCOLING.J.
Eisner.
1996.
Three new probabilistic models for de-pendency parsing: An exploration.
In Proc.
of COL-ING.D.
Giampiccolo, B. Magnini, I. Dagan, and B. Dolan, ed-itors.
2007.
The third pascal recognizing textual en-tailment challenge.D.
Gildea.
2003.
Loosely tree-based alignment for ma-chine translation.
In Proc.
of ACL.S.
Harmeling.
2007.
An extensible probabilistictransformation-based approach to the third Recogniz-ing Textual Entailment challenge.
In Proc.
of ACL-PASCAL Workshop on Textual Entailment and Para-phrasing.T.
Hastie, R. Tibshirani, and J. Friedman.
2001.
The Ele-ments of Statistical Learning: Data Mining, Inference,and Prediction.
Springer.D.
Haussler.
1999.
Convolution kernels on discretestructures.
Technical Report ucs-crl-99-10, Universityof California Santa Cruz.G.
E. Hinton.
1999.
Product of experts.
In Proc.
ofICANN.P.
N. Klein.
1989.
Computing the edit-distance betweenunrooted ordered trees.
In Proc.
of European Sympo-sium on Algorithms.M.
Kouylekov and B. Magnini.
2005.
Recognizing tex-tual entailment with tree edit distance algorithms.
InProc.
of the PASCAL RTE Challenge.S.
le Cessie and J. C. van Houwelingen.
1997.
Ridge es-timators in logistic regression.
Applied Statistics, 41.B.
MacCartney and C. D. Manning.
2008.
Modeling se-mantic containment and exclusion in natural languageinference.
In Proc.
of COLING.R.
McDonald, F. Pereira, K. Ribarov, and J. Hajic?.
2005.Non-projective dependency parsing using spanningtree algorithms.
In Proc.
of HLT-EMNLP.G.
A. Miller, R. Beckwith, C. Fellbaum, D. Gross, andK.
J. Miller.
1990.
WordNet: An on-line lexicaldatabase.
International Journal of Lexicography, 3(4).A.
Moschitti.
2006.
Efficient convolution kernels fordependency and constituent syntactic trees.
In Proc.of ECML.K.
Papineni, S. Roukos, T. Ward, and W.-J.
Zhu.
2002.BLEU: a method for automatic evaluation of machinetranslation.
In Proc.
of ACL.J.
Pearl.
1984.
Heuristics: intelligent search strategiesfor computer problem solving.
Addison-Wesley.V.
Punyakanok, D. Roth, and W. Yih.
2004.
Mapping de-pendencies trees: An application to question answer-ing.
In Proc.
of the 8th International Symposium onArtificial Intelligence and Mathematics.A.
Ratnaparkhi.
1996.
A maximum entropy part-of-speech tagger.
In Proc.
of EMNLP.F.
Schilder and B. T. McInnes.
2006.
TLR at DUC2006: approximate tree similarity and a new evalua-tion regime.
In Proc.
of DUC.B.
Scho?lkopf and A. J. Smola.
2001.
Learning with Ker-nels.
MIT Press.D.
A. Smith and J. Eisner.
2006.
Quasi-synchronousgrammars: Alignment by soft projection of syntacticdependencies.
In Proc.
of HLT-NAACL Workshop onStatistical Machine Translation.V.
N. Vapnik.
1995.
The Nature of Statistical LearningTheory.
Springer.E.
M. Voorhees.
2004.
Overview of TREC 2004.
InProc.
of TREC.S.
Wan, M. Dras, R. Dale, and C. Paris.
2006.
Usingdependency-based features to take the ?para-farce?
outof paraphrase.
In Proc.
of the Australasian LanguageTechnology Workshop.M.
Wang, N. A. Smith, and T. Mitamura.
2007.
What isthe Jeopardy model?
A quasi-synchronous grammarfor QA.
In Proc.
of EMNLP-CoNLL.F.
M. Zanzotto and A. Moschitti.
2006.
Automatic learn-ing of textual entailments with cross-pair similarities.In Proc.
of COLING/ACL.D.
Zelenko, C. Aone, and A. Richardella.
2003.
Kernelmethods for relation extraction.
J. of Machine Learn-ing Research, 3.K.
Zhang and D. Shasha.
1989.
Simple fast algorithmsfor the editing distance between trees and related prob-lems.
SIAM Journal of Computing, 18.1019
