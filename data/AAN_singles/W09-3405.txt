Proceedings of the 7th Workshop on Asian Language Resources, ACL-IJCNLP 2009, pages 32?39,Suntec, Singapore, 6-7 August 2009. c?2009 ACL and AFNLPAnnotating Dialogue Acts to Construct Dialogue Systems for ConsultingKiyonori Ohtake Teruhisa Misu Chiori Hori Hideki Kashioka Satoshi NakamuraMASTAR Project, National Institute of Information and Communications TechnologyHikaridai, Keihanna Science City, JAPANkiyonori.ohtake (at) nict.go.jpAbstractThis paper introduces a new corpus of con-sulting dialogues, which is designed fortraining a dialogue manager that can han-dle consulting dialogues through sponta-neous interactions from the tagged dia-logue corpus.
We have collected 130 hof consulting dialogues in the tourist guid-ance domain.
This paper outlines our tax-onomy of dialogue act annotation that candescribe two aspects of an utterances: thecommunicative function (speech act), andthe semantic content of the utterance.
Weprovide an overview of the Kyoto tourguide dialogue corpus and a preliminaryanalysis using the dialogue act tags.1 IntroductionThis paper introduces a new dialogue corpus forconsulting in the tourist guidance domain.
Thecorpus consists of speech, transcripts, speech acttags, morphological analysis results, dependencyanalysis results, and semantic content tags.
In thispaper, we describe the current status of a dialoguecorpus that is being developed by our researchgroup, focusing on two types of tags: speech acttags and semantic content tags.
These speech actand semantic content tags were designed to ex-press the dialogue act of each utterance.Many studies have focused on developing spo-ken dialogue systems.
Their typical task do-mains included the retrieval of information fromdatabases or making reservations, such as airlineinformation e.g., DARPA Communicator (Walkeret al, 2001) and train information e.g., ARISE(Bouwman et al, 1999) and MASK (Lamel et al,2002).
Most studies assumed a definite and con-sistent user objective, and the dialogue strategywas usually designed to minimize the cost of in-formation access.
Other target tasks include tutor-ing and trouble-shooting dialogues (Boye, 2007).In such tasks, dialogue scenarios or agendas areusually described using a (dynamic) tree structure,and the objective is to satisfy all requirements.In this paper, we introduce our corpus, which isbeing developed as part of a project to constructconsulting dialogue systems, that helps the user inmaking a decision.
So far, several projects havebeen organized to construct speech corpora suchas CSJ (Maekawa et al, 2000) for Japanese.
Thesize of CSJ is very large, and a great part of thecorpus consists of monologues.
Although, CSJincludes some dialogues, the size of dialogues isnot enough to construct a dialogue system via re-cent statistical techniques.
In addition, relativelyto consulting dialogues, the existing large dialoguecorpora covered very clear tasks in limited do-mains.However, consulting is a frequently used andvery natural form of human interaction.
We of-ten consult with a sales clerk while shopping orwith staff at a concierge desk in a hotel.
Such dia-logues usually form part of a series of informationretrieval dialogues that have been investigated inmany previous studies.
They also contains variousexchanges, such as clarifications and explanations.The user may explain his/her preferences vaguelyby listing examples.
The server would then sensethe user?s preferences from his/her utterances, pro-vide some information, and then request a deci-sion.It is almost impossible to handcraft a scenariothat can handle such spontaneous consulting dia-logues; thus, the dialogue strategy should be boot-strapped from a dialogue corpus.
If an extensivedialogue corpus is available, we can model thedialogue using machine learning techniques suchas partially observable Markov decision processes(POMDPs) (Thomson et al, 2008).
Hori et al(2008) have also proposed an efficient approach toorganize a dialogue system using weighted finite-state transducers (WFSTs); the system obtains the32Table 2: Overview of Kyoto tour guide dialoguecorpusdialogue type F2F WOZ TEL# of dialogues 114 80 62# of guides 3 2 2avg.
# of utterance 365.4 165.2 324.5/ dialogue (guide)avg.
# of utterance 301.7 112.9 373.5/ dialogue (tourist)structure of the transducers and the weight foreach state transitions from an annotated corpus.Thus, the corpus must be sufficiently rich in in-formation to describe the consulting dialogue toconstruct the statistical dialogue manager via suchtechniques.In addition, a detailed description would bepreferable when developing modules that focuson spoken language understanding and generationmodules.
In this study, we adopt dialogue acts(DAs) (Bunt, 2000; Shriberg et al, 2004; Banga-lore et al, 2006; Rodriguez et al, 2007; Levin etal., 2002) for this information and annotate DAs inthe corpus.In this paper, we describe the design of the Ky-oto tour guide dialogue corpus in Section 2.
Ourdesign of the DA annotation is described in Sec-tion 3.
Sections 4 and 5 respectively describe twotypes of the tag sets, namely, the speech act tagand the semantic content tag.2 Kyoto Tour Guide Dialogue CorpusWe are currently developing a dialogue corpusbased on tourist guidance for Kyoto City as the tar-get domain.
Thus far, we have collected itineraryplanning dialogues in Japanese, in which usersplan a one-day visit to Kyoto City.
There arethree types of dialogues in the corpus: face-to-face (F2F), Wizard of OZ (WOZ), and telephonic(TEL) dialogues.
The corpus consists of 114 face-to-face dialogues, 80 dialogues using the WOZsystem, and 62 dialogues obtained from telephoneconversations with the interface of the WOZ sys-tem.The overview of these three types of dialoguesis shown in Table 2.
Each dialogue lasts for almost30 min.
Most of all the dialogues have been man-ually transcribed.
Table 2 also shows the averagenumber of utterances per a dialogue.Each face-to-face dialogue involved a profes-sional tour guide and a tourist.
Three guides, onemale and two females, were employed to collectthe dialogues.
All three guides were involved inalmost the same number of dialogues.
The guidesused maps, guidebooks, and a PC connected to theinternet.In the WOZ dialogues, two female guides wereemployed.
Each of them was participated in 40dialogues.
The WOZ system consists of two in-ternet browsers, speech synthesis program, andan integration program for the collaborative work.Collaboration was required because in addition tothe guide, operators were employed to operate theWOZ system and support the guide.
Each of theguide and operators used own computer connectedeach other, and they collaboratively operate theWOZ system to serve a user (tourist).In the telephone dialogues, two female guideswho are the same for the WOZ dialogues wereemployed.
In these dialogues, we used the WOZsystem, but we did not need the speech synthesisprogram.
The guide and a tourist shared the sameinterface in different rooms, and they could talk toeach other through the hands-free headset.Dialogues to plan a one-day visit consist of sev-eral conversations for choosing places to visit.
Theconversations usually included sequences of re-quests from the users and provision of informationby the guides as well as consultation in the form ofexplanation and evaluation.
It should be noted thatin this study, enabling the user to access informa-tion is not an objective in itself, unlike informationkiosk systems such as those developed in (Lamelet al, 2002) or (Thomson et al, 2008).
The objec-tive is similar to the problem-solving dialogue ofthe study by Ferguson and Allen (1998), in otherwords, accessing information is just an aspect ofconsulting dialogues.An example of dialogue via face-to-face com-munication is shown in Table 1.
This dialogue isa part of a consultation to decide on a sightseeingspot to visit.
The user asks about the location of aspot, and the guide answers it.
Then, the user pro-vides a follow-up by evaluating the answer.
Thetask is challenging because there are many utter-ances that affect the flow of the dialogue during aconsultation.
The utterances are listed in the orderof their start times with the utterance ids (UID).From the column ?Time?
in the table, it is easy tosee that there are many overlaps.33Table 1: Example dialogue from the Kyoto tour guide dialogue corpusUID Time (ms) Speaker Transcript Speech act tag** Semantic content tag56 76669?78819 UserAto (And,)WH?Question WherenullOhara ga (Ohara is) (activity),locationdono henni (where) (activity),(demonstrative),interrnarimasuka (I?d like to know) (activity),predicate57 80788?81358 Guide kono (here) State Answer?56 (demonstrative),kosoahendesune (is around) (demonstrative),noun58 81358?81841 Guide Ohara ha (Ohara) State Inversion location59 81386?82736 User Chotto (a bit) State Evaluation?57 (transp),(cost),(distance),adverb-phrasehanaresugitemasune (is too far) (transp),(cost),(distance),predicate60 83116?83316 Guide A (Yeah,) Pause Grabber null61 83136?85023 UserKore demo (it)Y/N?Questionnullichinichi dewa (in a day) (activity),(planning),durationdoudeshou (Do you think I can do) (activity),(planning),(demonstrative),interr62 83386?84396 Guide Soudesune (right.)
State Acknowledgment?59 null63 85206?87076 GuideIchinichi (One day)State AffirmativeAnswer?61(activity),(planning),(entity),day-windowareba (is) (activity),(planning),predicatejubuN (enough) (consulting),(activity),adverb-phraseikemasu (to enjoy it.)
(consulting),(activity),action64 88392?90072 GuideOharamo (Ohara is)State Opinion(activity),locationsugoku (very) (recommendation),(activity),adverb-phrasekireidesuyo (a beautiful spot) (recommendation),(activity),predicate65 89889?90759 User Iidesune (that would be nice.)
State Acknowledgment?64 (consulting),(activity),predicateEvaluation?64* Tags are concatenated using a delimiter ?
?
and omitting null values.The number following the ???
symbol denotes the target utterance of the function.3 Annotation of CommunicativeFunction and Semantic Content in DAWe annotate DAs in the corpus in order to de-scribe a user?s intention and a system?s (or the tourguide?s) action.
Recently, several studies have ad-dressed multilevel annotation of dialogues (Levinet al, 2002; Bangalore et al, 2006; Rodriguez etal., 2007); in our study, we focus on the two as-pects of a DA indicated by Bunt (2000).
One is thecommunicative function that corresponds to howthe content should be used in order to update thecontext, and the other is a semantic content thatcorresponds to what the act is about.
We considerboth of them important information to handle theconsulting dialogue.
We designed two differenttag sets to annotate DAs in the corpus.
The speechact tag is used to capture the communicative func-tions of an utterance using domain-independentmultiple function layers.
The semantic content tagis used to describe the semantic contents of an ut-terance using domain-specific hierarchical seman-tic classes.4 Speech Act TagsIn this section, we introduce the speech act (SA)tag set that describes communicative functions ofutterances.
As the base units for tag annotation,we adopt clauses that are detected by applyingthe clause boundary annotation program (Kash-ioka and Maruyama, 2004) to the transcript of thedialogue.
Thus, in the following discussions, ?ut-terance?
denotes a clause.4.1 Tag SpecificationsThere are two major policies in SA annotation.One is to select exactly one label from the tag set(e.g., the AMI corpus1).
The other is to annotatewith as many labels as required.
MRDA (Shriberget al, 2004) and DIT++ (Bunt, 2000) are definedon the basis of the second policy.
We believe thatutterances are generally multifunctional and thismultifunctionality is an important aspect for man-aging consulting dialogues through spontaneousinteractions.
Therefore, we have adopted the latterpolicy.By extending the MRDA tag set and DIT++, wedefined our speech act tag set that consists of sixlayers to describe six groups of function: Gen-eral, Response, Check, Constrain, ActionDiscus-sion, and Others.
A list of the tag sets (excludingthe Others layer is shown in Table 3.
The Generallayer has two sublayers under the labels, Pauseand WH-Question, respectively.
The two sublay-ers are used to elaborate on the two labels, respec-tively.
A tag of the General layer must be labeledto an utterance, but the other layer?s tags are op-tional, in other words, layers other than the Gen-eral layer can take null values when there is no tagwhich is appropriate to the utterance.
In the practi-cal annotation, the most appropriate tag is selectedfrom each layer, without taking into account anyof the other layers.The descriptions of the layers are as follows:General: It is used to represent the basic form1http://corpus.amiproject.org34Table 3: List of speech act tags and their occurrence in the experimentTag Percentage(%) Tag Percentage(%) Tag Percentage(%) Tag Percentage(%)User Guide User Guide User Guide User Guide(General) (Response) (ActionDiscussion) (Constrain)Statement 45.25 44.53 Acknowledgment 19.13 5.45 Opinion 0.52 2.12 Reason 0.64 2.52Pause 12.99 15.05 Accept 4.68 6.25 Wish 1.23 0.05 Condition 0.61 3.09Backchannel 26.05 9.09 PartialAccept 0.02 0.10 Request 0.22 0.19 Elaboration 0.28 4.00Y/N-Question 3.61 2.19 AffirmativeAnswer 0.08 0.20 Suggestion 0.16 1.12 Evaluation 1.35 2.01WH-Question 1.13 0.40 Reject 0.25 0.11 Commitment 1.15 0.29 (Check)Open-Question 0.32 0.32 PartialReject 0.04 0.03 RepetitionRequest 0.07 0.03OR?after-Y/N 0.05 0.02 NegativeAnswer 0.10 0.10 UnderstandingCheck 0.19 0.20OR-Question 0.05 0.03 Answer 1.16 2.57 DoubleCheck 0.36 0.15Statement== 9.91 27.79 ApprovalRequest 2.01 1.07of the unit.
Most of the tags in this layerare used to describe forward-looking func-tions.
The tags are classified into three largegroups: ?Question,?
?Fragment,?
and ?State-ment.?
?Statement==?
denotes the continua-tion of the utterance.Response: It is used to label responses directedto a specific previous utterance made by theaddressee.Check: It is used to label confirmations that arealong a certain expected response.Constrain: It is used to label utterances that re-strict or complement the target of the utter-ance.ActionDiscussion: It is used to label utterancesthat pertain to a future action.Others: It is used to describe various functions ofthe utterance, e.g., Greeting, SelfTalk, Wel-come, Apology, etc.In the General layer, there are two sublayers:?
(1)the Pause sublayer that consists of Hold, Grabber,Holder, and Releaser and (2) the WH sublayer thatlabels the WH-Question type.It should be noted that this taxonomy is in-tended to be used for training spoken dialogue sys-tems.
Consequently, it contains detailed descrip-tions to elaborate on the decision-making process.For example, checks are classified into four cat-egories because they should be treated in variousways in a dialogue system.
UnderstandingCheckis often used to describe clarifications; thus, itshould be taken into account when creating a di-alogue scenario.
In contrast, RepetitionRequest,which is used to request that the missed portionsof the previous utterance be repeated, is not con-cerned with the overall dialogue flow.An example of an annotation is shown in Table1.
Since the Response and Constrain layers are notnecessarily directed to the immediately precedingutterance, the target utterance ID is specified.4.2 EvaluationWe performed a preliminary annotation of thespeech act tags in the corpus.
Thirty dialogues(900 min, 23,169 utterances) were annotated bythree labellers.
When annotating the dialogues, wetook into account textual information, audio infor-mation, and contextual information The result wascross-checked by another labeller.4.2.1 Distributional StatisticsThe frequencies of the tags, expressed as a per-centages, are shown in Table 3.
In the Generallayer, nearly half of the utterances were Statement.This bias is acceptable because 66% of the utter-ances had tag(s) of other layers.The percentages of tags in the Constrain layerare relatively higher than those of tags in the otherlayers.
They are also higher than the percentagesof the corresponding tags of MRDA (Shriberget al, 2004) and SWBD-DAMSL(Jurafsky et al,1997).These statistics characterize the consulting dia-logue of sightseeing planning, where explanationsand evaluations play an important role during thedecision process.4.2.2 ReliabilityWe investigated the reliability of the annotation.Another two dialogues (2,087 utterances) were an-notated by three labelers and the agreement amongthem was examined.
These results are listed in Ta-ble 4.
The agreement ratio is the average of all thecombinations of the three individual agreements.In the same way, we also computed the averageKappa statistic, which is often used to measure theagreement by considering the chance rate.A high concordance rate was obtained for theGeneral layer.
When the specific layers and sub-layers are taken into account, Kappa statistic was35Table 4: Agreement among labellersGeneral layer All layersAgreement ratio 86.7% 74.2%Kappa statistic 0.74 0.680.68, which is considered a good result for thistype of task.
(cf.
(Shriberg et al, 2004) etc.
)4.2.3 Analysis of Occurrence Tendencyduring Progress of EpisodeWe then investigated the tendencies of tag occur-rence through a dialogue to clarify how consult-ing is conducted in the corpus.
We annotated theboundaries of episodes that determined the spotsto visit in order to carefully investigate the struc-ture of the decision-making processes.
In our cor-pus, users were asked to write down their itineraryfor a practical one day tour.
Thus, the beginningand ending of an episode can be determined on thebasis of this itinerary.As a result, we found 192 episodes.
We selected122 episodes that had more than 50 utterances,and analyzed the tendency of tag occurrence.
Theepisodes were divided into five segments so thateach segment had an equal number of utterances.The tendency of tag occurrence is shown in Figure1.
The relative occurrence rate denotes the numberof times the tags appeared in each segment dividedby the total number of occurrences throughout thedialogues.
We found three patterns in the tendencyof occurrence.
The tags corresponding to the firstpattern frequently appear in the early part of anepisode; this typically applies to Open-Question,WH-Question, and Wish.
The tags of the sec-ond pattern frequently appear in the later part, thistypically applies to Evaluation, Commitment, andOpinion.
The tags of the third pattern appear uni-formly over an episode, e.g., Y/N-Question, Ac-cept, and Elaboration.
These statistics characterizethe dialogue flow of sightseeing planning, wherethe guide and the user first clarify the latter?s in-terests (Open, WH-Questions), list and evaluatecandidates (Evaluation), and then the user makesa decision (Commitment).This progression indicates that a session (or di-alogue phase) management is required within anepisode to manage the consulting dialogue, al-though the test-set perplexity2 , which was calcu-2The perplexity was calculated by 10-fold cross validationof the 30 dialogues.    Figure 1: Progress of episodes vs. occurrence ofspeech act tagslated by a 3-gram language model trained with theSA tags, was not high (4.25 using the general layerand 14.75 using all layers).5 Semantic Content TagsThe semantic content tag set was designed to cap-ture the contents of an utterance.
Some might con-sider semantic representations by HPSG (Pollardand Sag, 1994) or LFG (Dalrymple et al, 1994)for an utterance.
Such frameworks require knowl-edge of grammar and experiences to describe themeaning of an utterance.
In addition, the utter-ances in a dialogue are often fragmentary, whichmakes the description more difficult.We focused on the predicate-argument structurethat is based on dependency relations.
Annotatingdependency relations is more intuitive and is easierthan annotating the syntax structure; moreover, adependency parser is more robust for fragmentaryexpressions than syntax parsers.We introduced semantic classes to represent thesemantic contents of an utterance.
Semantic classlabels are applied to each unit of the predicate-argument structure.
The task that identifies thesemantic classes is very similar to named entityrecognition, because the classes of the named en-tities can be equated to the semantic classes thatare used to express semantic content.
However,both nouns and predicates are very important forcapturing the semantic contents of an utterance.For example, ?10 a.m.?
might denote the currenttime in the context of planning, or it might signifythe opening time of a sightseeing spot.
Thus, werepresent the semantic contents on the basis of thepredicate-argument structure.
Each predicate andargument is assigned a semantic category.For example, the sentence ?I would like to see36I would like to see Kinkakuji templewould like to see I Kinkakuji temple( )predicate argumentsautomatically analyzedmanually annotatedwould like to see I Kinkakuji temple( )preference.action preference.spot.namegiven sentencepredicate argument structureannotation resultFigure 2: Example of annotation with semanticcontent tags(preference) (reco m m end a t i o n) (d eci s i o n) (co ns u l t i ng )(s po t ) (a ct i v i t y )(res t a u ra nt ) a ct i o n pred i ca t e(co s t ) (s ch ed u l e) na m e t y pe(m o ney )o b j ectent i t y pred i ca t e(d i s t a nce)(v i ew )a ct i o nna t u rea rch i t ect u re?
?.Figure 3: A part of the semantic category hierar-chyKinkakuji temple.?
is annotated as shown in Fig-ure 2.
In this figure, the semantic content tagpreference.action indicates that the predicate por-tion expresses the speaker?s preference for thespeaker?s action, while the semantic content tagpreference.spot.name indicates the name of thespot as the object of the speaker?s preference.Although we do not define semantic the role(e.g., object (Kinakuji temple) and subject (I))of each argument item in this case, we can useconventional semantic role labeling techniques(Gildea and Jurafsky, 2002) to estimate them.Therefore, we do not annotate such semantic rolelabels in the corpus.5.1 Tag SpecificationsWe defined hierarchical semantic classes to anno-tate the semantic content tags.
There are 33 la-bels (classes) at the top hierarchical level.
The la-bels are, for example, activity, event, meal, spot,transportation, cost, consulting, and location, asshown in Figure 3.
There two kinds of labels,nodes and leaves.
A node must have at least onechild, a node or a leaf.
A leaf has no children.
Thenumber of kinds for nodes is 47, and the numberof kinds for leaves is 47.
The labels of leaves arevery similar to the labels for named entity recog-nition.
For example, there are ?year, date, time,organizer, name, and so on.?
in the labels of theleaves.One of the characteristics of the semantic struc-ture is that the lower level structures are shared bymany upper nodes.
Thus, the lower level structurecan be used in any other domains or target tasks.5.2 Annotation of semantic contents tagsThe annotation of semantic contents tags is per-formed by the following four steps.
First, an ut-terance is analyzed by a morphological analyzer,ChaSen3.
Second, the morphemes are chunkedinto dependency unit (bunsetsu).
Third, depen-dency analysis is performed using a Japanese de-pendency parser, CaboCha4.
Finally, we annotatethe semantic content tags for each bunsetsu unit byusing our annotation tool.
An example of an an-notation is shown in Table 1.
Each row in column?Transcript?
denotes the divided bunsetsu units.The annotation tool interface is shown in Figure4.
In the left side of this figure, the dialogue filesand each utterance of the dialogue information aredisplayed.
The dependency structure of an utter-ance is displayed in the upper part of the figure.The morphological analysis results and chunk in-formation are displayed in the lower part of thefigure.At present, the annotations of semantic con-tent tags are being carried out for 10 dialogues.Approximately 22,000 paths, including paths thatwill not be used, exist if the layered structure isfully expanded.
In the 10 dialogues, 1,380 tags (orpaths) are used.In addition, not only to annotate semantic con-tent tags, but to correct the morphological analyzeresults and dependency analyzed results are beingcarried out.
If we complete the annotation, we willalso obtain these correctly tagged data of Kyototour guide corpus.
These corpora can be used todevelop analyzers such as morphological analyz-3http://sourceforge.jp/projects/chasen-legacy/4http://chasen.org/?taku/software/cabocha/37Figure 4: Annotation tool interface for annotating semantic content tagsers and dependency analyzers via machine learn-ing techniques or to adapt analyzers for this do-main.6 ConclusionIn this paper, we have introduced our spoken di-alogue corpus for developing consulting dialoguesystems.
We designed a dialogue act annotationscheme that describes two aspects of a DA: speechact and semantic content.
The speech act tag setwas designed by extending the MRDA tag set.The design of the semantic content tag set is al-most complete.
If we complete the annotation, wewill obtain speech act tags and semantic contenttags, as well as manual transcripts, morphologi-cal analysis results, dependency analysis results,and dialogue episodes.
As a preliminary analysis,we have evaluated the SA tag set in terms of theagreement between labellers and investigated thepatterns of tag occurrences.In the next step, we will construct automatictaggers for speech act and semantic content tagsby using the annotated corpora and machine learn-ing techniques.
Our future work also includes acondensation or selection of dialogue acts that di-rectly affect the dialogue flow in order to constructa consulting dialogue system using the DA tags asan input.ReferencesSrinivas Bangalore, Giuseppe Di Fabbrizio, andAmanda Stent.
2006.
Learning the structure oftask-driven human-human dialogs.
In Proceedingsof COLING/ACL, pages 201?208.Gies Bouwman, Janienke Sturm, and Louis Boves.1999.
Incorporating Confidence Measures in theDutch Train Timetable Information System Devel-oped in the ARISE Project.
In Proc.
ICASSP.Johan Boye.
2007.
Dialogue Management for Auto-matic Troubleshooting and Other Problem-solvingApplications.
In Proc.
of 8th SIGdial Workshop onDiscourse and Dialogue, pages 247?255.Harry Bunt.
2000.
Dialogue pragmatics and contextspecification.
In Harry Bunt and William Black,editors, Abduction, Belief and Context in Dialogue,pages 81?150.
John Benjamins.Mary Dalrymple, Ronald M. Kaplan, John T. MaxwellIII, and Anni e Zaenen, editors.
1994.
Formal Is-sues in Lexical-Functional Grammar.
CSLI Publi-cations.George Ferguson and James F. Allen.
1998.
TRIPS:An intelligent integrated problem-solving assistant.In Proc.
Fifteenth National Conference on ArtificialIntelligence, pages 567?573.Daniel Gildea and Daniel Jurafsky.
2002.
Automaticlabeling of semantic roles.
Computational Linguis-tics, 28(3):245?288.Chiori Hori, Kiyonori Ohtake, Teruhisa Misu, HidekiKashioka, and Satoshi Nakamura.
2008.
DialogManagement using Weighted Finite-state Transduc-ers.
In Proc.
Interspeech, pages 211?214.38Daniel Jurafsky, Elizabeth Shriberg, and Debra Bi-asca.
1997.
Switchboard SWBD-DAMSL shallow-discourse-function annotation coders manual, draft13.
Technical report, University of Colorado atBoulder & SRI International.Hideki Kashioka and Takehiko Maruyama.
2004.
Seg-mentation of Semantic Unit in Japanese Monologue.In Proc.
ICSLT-O-COCOSDA.Lori F. Lamel, Samir Bennacef, Jean-Luc Gauvain,H.
Dartigues, and J. N. Temem.
2002.
User eval-uation of the MASK kiosk.
Speech Communication,38(1):131?139.Lori Levin, Donna Gates, Dorcas Wallace, Kay Peter-son, Along Lavie, Fabio Pianesi, Emanuele Pianta,Roldano Cattoni, and Nadia Mana.
2002.
Balancingexpressiveness and simplicity in an interlingua fortask based dialogue.
In Proceedings of ACL 2002workshop on Speech-to-speech Translation: Algo-rithms and Systems.Kikuo Maekawa, Hanae Koiso, Sadaoki Furui, and Hi-toshi Isahara.
2000.
Spontaneous speech corpusof Japanese.
In Proceedings of the Second Interna-tional Conference of Language Resources and Eval-uation (LREC2000), pages 947?952.Carl Pollard and Ivan A.
Sag.
1994.
Head-DrivenPhrase Structure Grammar.
The University ofChicago Press.Kepa Joseba Rodriguez, Stefanie Dipper, MichaelGo?tze, Massimo Poesio, Giuseppe Riccardi, Chris-tian Raymond, and Joanna Rabiega-Wisniewska.2007.
Standoff Coordination for Multi-Tool Anno-tation in a Dialogue Corpus.
In Proc.
Linguistic An-notation Workshop, pages 148?155.Elizabeth Shriberg, Raj Dhillon, Sonali Bhagat, JeremyAng, and Hannah Carvey.
2004.
The ICSI Meet-ing Recorder Dialog Act (MRDA) Corpus.
In Proc.5th SIGdial Workshop on Discourse and Dialogue,pages 97?100.Blaise Thomson, Jost Schatzmann, and Steve Young.2008.
Bayesian update of dialogue state for robustdialogue systems.
In Proceedings of ICASSP ?08.Marilyn A. Walker, Rebecca Passonneau, and Julie E.Boland.
2001.
Quantitative and Qualitative Eval-uation of DARPA Communicator Spoken DialogueSystems.
In Proc.
of 39th Annual Meeting of theACL, pages 515?522.39
