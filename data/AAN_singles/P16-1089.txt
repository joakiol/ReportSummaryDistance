Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 941?951,Berlin, Germany, August 7-12, 2016.c?2016 Association for Computational LinguisticsSiamese CBOW: Optimizing Word Embeddingsfor Sentence RepresentationsTom Kenter1Alexey Borisov1, 2Maarten de Rijke1tom.kenter@uva.nl alborisov@yandex-team.ru derijke@uva.nl1University of Amsterdam, Amsterdam2Yandex, MoscowAbstractWe present the Siamese Continuous Bag ofWords (Siamese CBOW) model, a neuralnetwork for efficient estimation of high-quality sentence embeddings.
Averagingthe embeddings of words in a sentencehas proven to be a surprisingly success-ful and efficient way of obtaining sen-tence embeddings.
However, word em-beddings trained with the methods cur-rently available are not optimized for thetask of sentence representation, and, thus,likely to be suboptimal.
Siamese CBOWhandles this problem by training word em-beddings directly for the purpose of be-ing averaged.
The underlying neural net-work learns word embeddings by predict-ing, from a sentence representation, itssurrounding sentences.
We show the ro-bustness of the Siamese CBOW model byevaluating it on 20 datasets stemming froma wide variety of sources.1 IntroductionWord embeddings have proven to be beneficialin a variety of tasks in NLP such as machinetranslation (Zou et al, 2013), parsing (Chen andManning, 2014), semantic search (Reinanda et al,2015; Voskarides et al, 2015), and tracking themeaning of words and concepts over time (Kimet al, 2014; Kenter et al, 2015).
It is not evi-dent, however, how word embeddings should becombined to represent larger pieces of text, likesentences, paragraphs or documents.
Surprisingly,simply averaging word embeddings of all words ina text has proven to be a strong baseline or featureacross a multitude of tasks (Faruqui et al, 2014;Yu et al, 2014; Gershman and Tenenbaum, 2015;Kenter and de Rijke, 2015).Word embeddings, however, are not optimizedspecifically for representing sentences.
In this pa-per we present a model for obtaining word embed-dings that are tailored specifically for the task ofaveraging them.
We do this by directly includinga comparison of sentence embeddings?the aver-aged embeddings of the words they contain?inthe cost function of our network.Word embeddings are typically trained in a fastand scalable way from unlabeled training data.
Asthe training data is unlabeled, word embeddingsare usually not task-specific.
Rather, word embed-dings trained on a large training corpus, like theones from (Collobert and Weston, 2008; Mikolovet al, 2013b) are employed across different tasks(Socher et al, 2012; Kenter and de Rijke, 2015;Hu et al, 2014).
These two qualities?
(i) beingtrainable from large quantities of unlabeled datain a reasonable amount of time, and (ii) robustperformance across different tasks?are highly de-sirable and allow word embeddings to be used inmany large-scale applications.
In this work weaim to optimize word embeddings for sentencerepresentations in the same manner.
We wantto produce general purpose sentence embeddingsthat should score robustly across multiple test sets,and we want to leverage large amounts of unla-beled training material.In the word2vec algorithm, Mikolov et al(2013a) construe a supervised training criterionfor obtaining word embeddings from unsuperviseddata, by predicting, for every word, its surround-ing words.
We apply this strategy at the sentencelevel, where we aim to predict a sentence from itsadjacent sentences (Kiros et al, 2015; Hill et al,2016).
This allows us to use unlabeled trainingdata, which is easy to obtain; the only restrictionis that documents need to be split into sentencesand that the order between sentences is preserved.The main research question we address is941whether directly optimizing word embeddings forthe task of being averaged to produce sentence em-beddings leads to word embeddings that are bettersuited for this task than word2vec does.
There-fore, we test the embeddings in an unsupervisedlearning scenario.
We use 20 evaluation sets thatstem from a wide variety of sources (newswire,video descriptions, dictionary descriptions, mi-croblog posts).
Furthermore, we analyze the timecomplexity of our method and compare it to ourbaselines methods.Summarizing, our main contributions are:?
We present Siamese CBOW, an efficient neuralnetwork architecture for obtaining high-qualityword embeddings, directly optimized for sen-tence representations;?
We evaluate the embeddings produced bySiamese CBOW on 20 datasets, originatingfrom a range of sources (newswire, tweets,video descriptions), and demonstrate the robust-ness of embeddings across different settings.2 Siamese CBOWWe present the Siamese Continuous Bag of Words(CBOW) model, a neural network for efficientestimation of high-quality sentence embeddings.Quality should manifest itself in embeddings ofsemantically close sentences being similar to oneanother, and embeddings of semantically differentsentences being dissimilar.
An efficient and sur-prisingly successful way of computing a sentenceembedding is to average the embeddings of itsconstituent words.
Recent work uses pre-trainedword embeddings (such as word2vec and GloVe)for this task, which are not optimized for sentencerepresentations.
Following these approaches, wecompute sentence embeddings by averaging wordembeddings, but we optimize word embeddingsdirectly for the purpose of being averaged.2.1 Training objectiveWe construct a supervised training criterion byhaving our network predict sentences occurringnext to each other in the training data.
Specifically,for a pair of sentences (si, sj), we define a proba-bility p(si, sj) that reflects how likely it is for thesentences to be adjacent to one another in the train-ing data.
We compute the probability p(si, sj) us-ing a softmax function:p?
(si, sj) =ecos(s?i,s?j)?s??Secos(s?i,s??
), (1)where s?xdenotes the embedding of sentence sx,based on the model parameters ?.
In theory,the summation in the denominator of Equation 1should range over all possible sentences S, whichis not feasible in practice.
Therefore, we replacethe set S with the union of the set S+of sentencesthat occur next to the sentence siin the trainingdata, and S?, a set of n randomly chosen sen-tences that are not observed next to the sentencesiin the training data.
The loss function of thenetwork is categorical cross-entropy:L = ??sj?{S+?S?
}p(si, sj) ?
log(p?
(si, sj)),where p(?)
is the target probability the networkshould produce, and p?(?)
is the prediction it es-timates based on parameters ?, using Equation 1.The target distribution simply is:p(si, sj) ={1|S+|, if sj?
S+0, if sj?
S?.I.e., if there are 2 positive examples (the sen-tences preceding and following the input sentence)and 2 negative examples, the target distribution is(0.5, 0.5, 0, 0).2.2 Network architectureFigure 1 shows the architecture of the proposedSiamese CBOW network.
The input is a projec-tion layer that selects embeddings from a wordembedding matrixW (that is shared across inputs)for a given input sentence.
The word embeddingsare averaged in the next layer, which yields a sen-tence representation with the same dimensionalityas the input word embeddings (the boxes labeledaverageiin Figure 1).
The cosine similarities be-tween the sentence representation for sentenceiand the other sentences are calculated in the penul-timate layer and a softmax is applied in the lastlayer to produce the final probability distribution.2.3 TrainingThe weights in the word embedding matrix are theonly trainable parameters in the Siamese CBOWnetwork.
They are updated using stochastic gradi-ent descent.
The initial learning rate is monoton-ically decreased proportionally to the number oftraining batches.3 Experimental SetupTo test the efficacy of our siamese network forproducing sentence embeddings we use multiple942word embeddings sentence i word embeddings sentence i-1w w waverage averageprediction... ...word embeddings sentence i+1average...negative example 1average...negative example naverage......cosine layersoftmaxword embeddingmatrix W W W W Wi,1 i,2 i,...i i-1 i+1 neg 1 neg nFigure 1: Siamese CBOW network architecture.
(Input projection layer omitted.
)test sets.
We use Siamese CBOW to learn wordembeddings from an unlabeled corpus.
For everysentence pair in the test sets, we compute two sen-tence representations by averaging the word em-beddings of each sentence.
Words that are miss-ing from the vocabulary and, hence, have no wordembedding, are omitted.
The cosine similarity be-tween the two sentence vectors is produced as afinal semantic similarity score.As we want a clean way to directly evalu-ate the embeddings on multiple sets we train ourmodel and the models we compare with on ex-actly the same training data.
We do not com-pute extra features, perform extra preprocessingsteps or incorporate the embeddings in supervisedtraining schemes.
Additional steps like these arevery likely to improve evaluation scores, but theywould obscure our main evaluation purpose in thispaper, which is to directly test the embeddings.3.1 DataWe use the Toronto Book Corpus1to train wordembeddings.
This corpus contains 74,004,228already pre-processed sentences in total, whichare made up of 1,057,070,918 tokens, originatingfrom 7,087 unique books.
In our experiments, weconsider tokens appearing 5 times or more, whichleads to a vocabulary of 315,643 words.3.2 BaselinesWe employ two baselines for producing sentenceembeddings in our experiments.
We obtain simi-larity scores between sentence pairs from the base-lines in the same way as the ones produced bySiamese CBOW, i.e., we calculate the cosine sim-ilarity between the sentence embeddings they pro-duce.1The corpus can be downloaded from http://www.cs.toronto.edu/?mbweb/; cf.
(Zhu et al, 2015).Word2vec We average word embeddingstrained with word2vec.2We use both architec-tures, Skipgram and CBOW, and apply defaultsettings: minimum word frequency 5, wordembedding size 300, context window 5, samplethreshold 10-5, no hierarchical softmax, 5 negativeexamples.Skip-thought As a second baseline we use thesentence representations produced by the skip-thought architecture (Kiros et al, 2015).3Skip-thought is a recently proposed method that learnssentence representations in a different way fromours, by using recurrent neural networks.
This al-lows it to take word order into account.
As it trainssentence embeddings from unlabeled data, like wedo, it is a natural baseline to consider.Both methods are trained on the Toronto BookCorpus, the same corpus used to train SiameseCBOW.
We should note that as we use skip-thought vectors as trained by Kiros et al (2015),skip-thought has an advantage over both word2vecand Siamese CBOW as the vocabulary used forencoding sentences contains 930,913 words, threetimes the size of the vocabulary that we use.3.3 EvaluationWe use 20 SemEval datasets from the SemEval se-mantic textual similarity task in 2012, 2013, 2014and 2015 (Agirre et al, 2012; Agirre et al, 2013;Agirre et al, 2014; Agirre et al, 2015), which con-sist of sentence pairs from a wide array of sources(e.g., newswire, tweets, video descriptions) thathave been manually annotated by multiple humanassessors on a 5 point scale (1: semantically unre-lated, 5: semantically similar).
In the ground truth,the final similarity score for every sentence pair is2The code is available from https://code.google.com/archive/p/word2vec/.3The code and the trained models can be down-loaded from https://github.com/ryankiros/skip-thoughts/.943Table 1: Results on SemEval datasets in terms of Pearson?s r (Spearman?s r).
Highest scores, in termsof Pearson?s r, are displayed in bold.
Siamese CBOW runs statistically significantly different from theword2vec CBOW baseline runs are marked with a ?.
See ?3.3 for a discussion of the statistical test used.Dataset w2v skipgram w2v CBOW skip-thought Siamese CBOW2012MSRpar .3740 (.3991) .3419 (.3521) .0560 (.0843) .4379?
(.4311)MSRvid .5213 (.5519) .5099 (.5450) .5807 (.5829) .4522?
(.4759)OnWN .6040 (.6476) .6320 (.6440) .6045 (.6431) .6444?
(.6475)SMTeuroparl .3071 (.5238) .3976 (.5310) .4203 (.4999) .4503?
(.5449)SMTnews .4487 (.3617) .4462 (.3901) .3911 (.3628) .3902?
(.4153)2013FNWN .3480 (.3401) .2736 (.2867) .3124 (.3511) .2322?
(.2235)OnWN .4745 (.5509) .5165 (.6008) .2418 (.2766) .4985?
(.5227)SMT .1838 (.2843) .2494 (.2919) .3378 (.3498) .3312?
(.3356)headlines .5935 (.6044) .5730 (.5766) .3861 (.3909) .6534?
(.6516)2014OnWN .5848 (.6676) .6068 (.6887) .4682 (.5161) .6073?
(.6554)deft-forum .3193 (.3810) .3339 (.3507) .3736 (.3737) .4082?
(.4188)deft-news .5906 (.5678) .5737 (.5577) .4617 (.4762) .5913?
(.5754)headlines .5790 (.5544) .5455 (.5095) .4031 (.3910) .6364?
(.6260)images .5131 (.5288) .5056 (.5213) .4257 (.4233) .6497?
(.6484)tweet-news .6336 (.6544) .6897 (.6615) .5138 (.5297) .7315?
(.7128)2015answ-forums .1892 (.1463) .1767 (.1294) .2784 (.1909) .2181 (.1469)answ-students .3233 (.2654) .3344 (.2742) .2661 (.2068) .3671?
(.2824)belief .2435 (.2635) .3277 (.3280) .4584 (.3368) .4769 (.3184)headlines .1875 (.0754) .1806 (.0765) .1248 (.0464) .2151?
(.0846)images .2454 (.1611) .2292 (.1438) .2100 (.1220) .2560?
(.1467)the mean of the annotator judgements, and as suchcan be a floating point number like 2.685.The evaluation metric used by SemEval, andhence by us, is Pearson?s r. As Spearman?s r isoften reported as well, we do so too.Statistical significance To see whether SiameseCBOW yields significantly different scores forthe same input sentence pairs from word2vecCBOW?the method it is theoretically most sim-ilar to?we compute Wilcoxon signed-rank teststatistics between all runs on all evaluation sets.Runs are considered statistically significantly dif-ferent for p-values < 0.0001.3.4 NetworkTo comply with results reported in other research(Mikolov et al, 2013b; Kusner et al, 2015) wefix the embedding size to 300 and only considerwords appearing 5 times or more in the trainingcorpus.
We use 2 negative examples (see ?4.2.2for an analysis of different settings).
The embed-dings are initialized randomly, by drawing froma normal distribution with ?
= 0.0 and ?
= 0.01.The batch size is 100.
The initial learning rate ?
is0.0001, which we obtain by observing the loss onthe training data.
Training consists of one epoch.We use Theano (Theano Development Team,2016) to implement our network.4We ran our ex-periments on GPUs in the DAS5 cluster (Bal et al,2016).4The code for Siamese CBOW is available underan open-source license at https://bitbucket.org/TomKenter/siamese-cbow.9444 ResultsIn this section we present the results of our ex-periments, and analyze the stability of SiameseCBOW with respect to its (hyper)parameters.4.1 Main experimentsIn Table 1, the results of Siamese CBOW on 20SemEval datasets are displayed, together with theresults of the baseline systems.
As we can seefrom the table, Siamese CBOW outperforms thebaselines in the majority of cases (14 out of 20).The very low scores of skip-thought on MSRparappear to be a glitch, which we will ignore.It is interesting to see that for the set withthe highest average sentence length (2013 SMT,with 24.7 words per sentence on average) SiameseCBOW is very close to skip-thought, the best per-forming baseline.
In terms of lexical term over-lap, unsurprisingly, all methods have trouble withthe sets with little overlap (2013 FNWN, 2015answers-forums, which both have 7% lexical over-lap).
It is interesting to see, however, that for thenext two sets (2015 belief and 2012 MSRpar, 11%and 14% overlap respectively) Siamese CBOWmanages to get the best performance.
The high-est performance on all sets is 0.7315 Pearson?s rof Siamese CBOW on the 2014 tweet-news set.This figure is not very far from the best perform-ing SemEval run that year which has 0.792 Pear-son?s r. This is remarkable as Siamese CBOW iscompletely unsupervised, while the NTNU systemwhich scored best on this set (Lynum et al, 2014)was optimized using multiple training sets.In recent work, Hill et al (2016) present Fast-Sent, a model similar to ours (see ?5 for a moreelaborate discussion); results are not reported forall evaluation sets we use, and hence, we comparethe results of FastSent and Siamese CBOW sepa-rately, in Table 2.FastSent and Siamese CBOW each outperformthe other on half of the evaluation sets, whichclearly suggests that the differences between thetwo methods are complementary.54.2 AnalysisNext, we investigate the stability of SiameseCBOW with respect to its hyper-parameters.
In5The comparison is to be interpreted with caution as it isnot evident what vocabulary was used for the experiments in(Hill et al, 2016); hence, the differences observed here mightsimply be due to differences in vocabulary coverage.Table 2: Results on SemEval 2014 datasets interms of Pearson?s r (Spearman?s r).
Highestscores (in Pearson?s r) are displayed in bold.
Fast-Sent results are reprinted from (Hill et al, 2016)where they are reported in two-digit precision.Dataset FastSent Siamese CBOWOnWN .74 (.70) .6073 (.6554)deft-forum .41 (.36) .4082 (.4188)deft-news .58 (.59) .5913 (.5754)headlines .57 (.59) .6364 (.6260)images .74 (.78) .6497 (.6484)tweet-news .63 (.66) .7315 (.7128)particular, we look into stability across iterations,different numbers of negative examples, and thedimensionality of the embeddings.
Other parame-ter settings are set as reported in ?3.4.4.2.1 Performance across iterationsIdeally, the optimization criterion of a learning al-gorithm ranges over the full domain of its lossfunction.
As discussed in ?2, our loss functiononly observes a sample.
As such, convergence isnot guaranteed.
Regardless, an ideal learning sys-tem should not fluctuate in terms of performancerelative to the amount of training data it observes,provided this amount is substantial: as trainingproceeds the performance should stabilize.To see whether the performance of SiameseCBOW fluctuates during training we monitor itduring 5 epochs; at every 10,000,000 examples,and at the end of every epoch.
Figure 2 displaysthe results for all 20 datasets.
We observe thaton the majority of datasets the performance showsvery little variation.
There are three exceptions.The performance on the 2014 deft-news datasetsteadily decreases while the performance on 2013OnWN steadily increases, though both seem tostabilize at the end of epoch 5.
The most no-table exception, however, is 2012 MSRvid, wherethe score, after an initial increase, drops consis-tently.
This effect might be explained by the factthat this evaluation set primarily consists of veryshort sentences?it has the lowest average sen-tence length of all set: 6.63 with a standard de-viation of 1.812.
Therefore, a 300-dimensionalrepresentation appears too large for this dataset;this hypothesis is supported by the fact that 200-dimensional embeddings work slightly better forthis dataset (see Figure 4).945Epoch 1- batch 2Epoch 1- batch 4Epoch 1- batch 6Endof epoch1Epoch 2- batch 2Epoch 2- batch 4Epoch 2- batch 6Endof epoch2Epoch 3- batch 2Epoch 3- batch 4Epoch 3- batch 6Endof epoch3Epoch 4- batch 2Epoch 4- batch 4Epoch 4- batch 6Endof epoch4Epoch 5- batch 2Epoch 5- batch 4Epoch 5- batch 6Endof epoch50.10.20.30.40.50.60.70.8Pearson's r2012 MSRpar2012 MSRvid2012 OnWN2012 SMTeuroparl2012 SMTnews2013 FNWN2013 OnWN2013 SMT2013 headlines2014 OnWN2014 deft-forum2014 deft-news2014 headlines2014 images2014 tweet-news2015 answers-forums2015 answers-students2015 belief2015 headlines2015 imagesFigure 2: Performance of Siamese CBOW across 5 iterations.4.2.2 Number of negative examplesIn Figure 3, the results of Siamese CBOW in termsof Pearson?s r are plotted for different numbersof negative examples.
We observe that on mostsets, the number of negative examples has lim-ited effect on the performance of Siamese CBOW.Choosing a higher number, like 10, occasionallyleads to slightly better performance, e.g., on the2013 FNWN set.
However, a small number like 1or 2 typically suffices, and is sometimes markedlybetter, e.g., in the case of the 2015 belief set.
As2012MSRpar2012MSRvid2012OnWN2012SMTeuroparl2012SMTnews2013FNWN2013OnWN2013SMT2013headlines2014OnWN2014deft-forum2014deft-news2014headlines2014images2014tweet-news2015answ-forums2015answ-students2015belief2015headlines2015images0.00.10.20.30.40.50.60.70.8Pearson'srneg 1 neg 2 neg 5 neg 10Figure 3: Performance of Siamese CBOW withdifferent numbers of negative examples.a high number of negative examples comes at asubstantial computational cost, we conclude fromthe findings presented here that, although SiameseCBOW is robust against different settings of thisparameter, setting the number of negative exam-ples to 1 or 2 should be the default choice.4.2.3 Number of dimensionsFigure 4 plots the results of Siamese CBOW fordifferent numbers of vector dimensions.
We ob-serve from the figure that for some sets (mostnotably 2014 deft-forum, 2015 answ-forums and2015 belief) increasing the number of embed-ding dimensions consistently yields higher perfor-mance.
A dimensionality that is too low (50 or2012MSRpar2012MSRvid2012OnWN2012SMTeuroparl2012SMTnews2013FNWN2013OnWN2013SMT2013headlines2014OnWN2014deft-forum2014deft-news2014headlines2014images2014tweet-news2015answ-forums2015answ-students2015belief2015headlines2015images0.00.10.20.30.40.50.60.70.8Pearson'sr50d100d 200d300d 600d1200dFigure 4: Performance of Siamese CBOW acrossnumber of embedding dimensions.100) invariably leads to inferior results.
As, sim-ilar to a higher number of negative examples, ahigher embedding dimension leads to higher com-putational costs, we conclude from these findings946that a moderate number of dimensions (200 or300) is to be preferred.4.3 Time complexityFor learning systems, time complexity comes intoplay in the training phase and in the predictionphase.
For an end system employing sentence em-beddings, the complexity at prediction time is themost crucial factor, which is why we omit an anal-ysis of training complexity.
We focus on compar-ing the time complexity for generating sentenceembeddings for Siamese CBOW, and compare itto the baselines we use.The complexity of all algorithms we consider isO(n), i.e., linear in the number of input terms.
Asin practice the number of arithmetic operations isthe critical factor in determining computing time,we will now focus on these.Both word2vec and the Siamese CBOW com-pute embeddings of a text T = t1, .
.
.
, t|T |by av-eraging the term embeddings.
This requires |T |?1vector additions, and 1 multiplication by a scalarvalue (namely, 1/|T |).
The skip-thought model isa recurrent neural network with GRU cells, whichcomputes a set of equations for every term t in T ,which we reprint for reference (Kiros et al, 2015):rt= ?
(Wrxt+Urht?1)zt= ?
(Wzxt+Uzht?1)ht= tanh(Wxt+U(rtht?1))ht= (1?
zt) ht?1+ zthtAs we can see from the formulas, there are 5|T |vector additions (+/-), 4|T | element-wise multipli-cations by a vector, 3|T | element-wise operationsand 6|T | matrix multiplications, of which the lat-ter, the matrix multiplications, are most expensive.This considerable difference in numbers ofarithmetic operations is also observed in practice.We run tests on a single CPU, using identical codefor extracting sentences from the evaluation sets,Table 3: Time spent per method on all 20 SemEvaldatasets, 17,608 sentence pairs, and the averagetime spent on a single sentence pair (time in sec-onds unless indicated otherwise).20 sets 1 pairSiamese CBOW (300d) 7.7 0.0004word2vec (300d) 7.0 0.0004skip-thought (1200d) 98,804.0 5.6for every method.
The sentence pairs are pre-sented one by one to the models.
We disregardthe time it takes to load models.
Speedups mightof course be gained for all methods by presentingthe sentences in batches to the models, by com-puting sentence representations in parallel and byrunning code on a GPU.
However, as we are inter-ested in the differences between the systems, werun the most simple and straightforward scenario.Table 3 lists the number of seconds each methodtakes to generate and compare sentence embed-dings for an input sentence pair.
The differencebetween word2vec and Siamese CBOW is becauseof a different implementation of word lookup.We conclude from the observations presentedhere, together with the results in ?4.1, that in a set-ting where speed at prediction time is pivotal, sim-ple averaging methods like word2vec or SiameseCBOW are to be preferred over more involvedmethods like skip-thought.4.4 Qualitative analysisAs Siamese CBOW directly averages word em-beddings for sentences, we expect it to learn thatwords with little semantic impact have a low vec-tor norm.
Indeed, we find that the 10 words withlowest vector norm are to, of, and, the, a, in, that,with, on, and as.
At the other side of the spec-trum we find many personal pronouns: had, they,we, me, my, he, her, you, she, I, which is naturalgiven that the corpus on which we train consists offiction, which typically contains dialogues.It is interesting to see what the differences inrelated words are between Siamese CBOW andword2vec when trained on the same corpus.
Forexample, for a cosine similarity > 0.6, the wordsrelated to her in word2vec space are she, his, myand hers.
For Siamese CBOW, the only closelyrelated word is she.
Similarly, for the word me,word2vec finds him as most closely related word,while Siamese CBOW comes up with I and my.It seems from these few examples that SiameseCBOW learns to be very strict in choosing whichwords to relate to each other.From the results presented in this section weconclude that optimizing word embeddings forthe task of being averaged across sentences withSiamese CBOW leads to embeddings that are ef-fective in a large variety of settings.
Furthermore,Siamese CBOW is robust to different parametersettings and its performance is stable across itera-947tions.
Lastly, we show that Siamese CBOW is fastand efficient in computing sentence embeddings atprediction time.5 Related WorkA distinction can be made between supervisedapproaches for obtaining representations of shorttexts, where a model is optimised for a specificscenario, given a labeled training set, and unsu-pervised methods, trained on unlabeled data, thataim to capture short text semantics that are robustacross tasks.
In the first setting, word vectors aretypically used as features or network initialisations(Kenter and de Rijke, 2015; Hu et al, 2014; Sev-eryn and Moschitti, 2015; Yin and Sch?utze, 2015).Our work can be classified in the latter category ofunsupervised approaches.Many models related to the one we present hereare used in a multilingual setting (Hermann andBlunsom, 2014b; Hermann and Blunsom, 2014a;Lauly et al, 2014).
The key difference betweenthis work and ours is that in a multilingual settingthe goal is to predict, from a distributed represen-tation of an input sentence, the same sentence in adifferent language, whereas our goals is to predictsurrounding sentences.Wieting et al (2016) apply a model similar toours in a related but different setting where ex-plicit semantic knowledge is leveraged.
As inour setting, word embeddings are trained by av-eraging them.
However, unlike in our proposal, amargin-based loss function is used, which involvesa parameter that has to be tuned.
Furthermore, toselect negative examples, at every training step,a computationally expensive comparison is madebetween all sentences in the training batch.
Themost crucial difference is that a large set of phrasepairs explicitly marked for semantic similarity hasto be available as training material.
Obtainingsuch high-quality training material is non-trivial,expensive and limits an approach to settings forwhich such material is available.
In our work, weleverage unlabeled training data, of which there isa virtually unlimited amount.As detailed in ?2, our network predicts a sen-tence from its neighbouring sentences.
The no-tion of learning from context sentences is also ap-plied in (Kiros et al, 2015), where a recurrentneural network is employed.
Our way of aver-aging the vectors of words contained in a sen-tence is more similar to the CBOW architectureof word2vec (Mikolov et al, 2013a), in which allcontext word vectors are aggregated to predict theone omitted word.
A crucial difference betweenour approach and the word2vec CBOW approachis that we compare sentence representations di-rectly, rather than comparing a (partial) sentencerepresentation to a word representation.
Giventhe correspondence between word2vec?s CBOWmodel and ours, we included it as a baseline inour experiments in ?3.
As the skip-gram architec-ture has proven to be a strong baseline too in manysettings, we include it too.Yih et al (2011) also propose a siamese ar-chitecture.
Short texts are represented by tf-idfvectors and a linear combination of input weightsis learnt by a two-layer fully connected network,which is used to represent the input text.
The co-sine similarity between pairs of representations iscomputed, but unlike our proposal, the differencesbetween similarities of a positive and negative sen-tence pair are combined in a logistic loss function.Finally, independently from our work, Hill etal.
(2016) also present a log-linear model.
Ratherthan comparing sentence representations to eachother, as we propose, words in one sentence arecompared to the representation of another sen-tence.
As both input and output vectors arelearnt, while we tie the parameters across the en-tire model, Hill et al (2016)?s model has twice asmany parameters as ours.
Most importantly, how-ever, the cost function used in (Hill et al, 2016)is crucially different from ours.
As words in sur-rounding sentences are being compared to a sen-tence representation, the final layer of their net-work produces a softmax over the entire vocabu-lary.
This is fundamentally different from the fi-nal softmax over cosines between sentence repre-sentations that we propose.
Furthermore, the soft-max over the vocabulary is, obviously, of vocab-ulary size, and hence grows when bigger vocabu-laries are used, causing additional computationalcost.
In our case, the size of the softmax is thenumber of positive plus negative examples (see?2.1).
When the vocabulary grows, this size is un-affected.6 ConclusionWe have presented Siamese CBOW, a neural net-work architecture that efficiently learns word em-beddings optimized for producing sentence repre-sentations.
The model is trained using only unla-948beled text data.
It predicts, from an input sentencerepresentation, the preceding and following sen-tence.We evaluated the model on 20 test sets andshow that in a majority of cases, 14 out of 20,Siamese CBOW outperforms a word2vec base-line and a baseline based on the recently pro-posed skip-thought architecture.
As further analy-sis on various choices of parameters show that themethod is stable across settings, we conclude thatSiamese CBOW provides a robust way of generat-ing high-quality sentence representations.Word and sentence embeddings are ubiquitousand many different ways of using them in su-pervised tasks have been proposed.
It is beyondthe scope of this paper to provide a comprehen-sive analysis of all supervised methods using wordor sentence embeddings and the effect SiameseCBOW would have on them.
However, it wouldbe interesting to see how Siamese CBOW embed-dings would affect results in supervised tasks.Lastly, although we evaluated Siamese CBOWon sentence pairs, there is no theoretical limitationrestricting it to sentences.
It would be interestingto see how embeddings for larger pieces of texts,such as documents, would perform in documentclustering or filtering tasks.AcknowledgmentsThe authors wish to express their gratitude for thevaluable advice and relevant pointers of the anony-mous reviewers.
Many thanks to Christophe VanGysel for implementation-related help.
This re-search was supported by Ahold, Amsterdam DataScience, the Bloomberg Research Grant program,the Dutch national program COMMIT, Else-vier, the European Community?s Seventh Frame-work Programme (FP7/2007-2013) under grantagreement nr 312827 (VOX-Pol), the ESF Re-search Network Program ELIAS, the Royal DutchAcademy of Sciences (KNAW) under the EliteNetwork Shifts project, the Microsoft ResearchPh.D.
program, the Netherlands eScience Centerunder project number 027.012.105, the Nether-lands Institute for Sound and Vision, the Nether-lands Organisation for Scientific Research (NWO)under project nrs 727.011.005, 612.001.116,HOR-11-10, 640.006.013, 612.066.930, CI-14-25, SH-322-15, 652.002.001, 612.001.551, theYahoo Faculty Research and Engagement Pro-gram, and Yandex.
All content represents theopinion of the authors, which is not necessarilyshared or endorsed by their respective employersand/or sponsors.ReferencesEneko Agirre, Mona Diab, Daniel Cer, and AitorGonzalez-Agirre.
2012.
Semeval-2012 task 6: A pi-lot on semantic textual similarity.
In Proceedings ofthe First Joint Conference on Lexical and Computa-tional Semantics-Volume 1: Proceedings of the mainconference and the shared task, and Volume 2: Pro-ceedings of the Sixth International Workshop on Se-mantic Evaluation (SemEval 2012), pages 385?393.Eneko Agirre, Daniel Cer, Mona Diab, Aitor Gonzalez-Agirre, and Weiwei Guo.
2013. sem 2013 sharedtask: Semantic textual similarity, including a piloton typed-similarity.
In Second Joint Conference onLexical and Computational Semantics (*SEM), Vol-ume 1: Proceedings of the Main Conference and theShared Task (*SEM 2013), pages 32?43.Eneko Agirre, Carmen Banea, Claire Cardie, DanielCer, Mona Diab, Aitor Gonzalez-Agirre, WeiweiGuo, Rada Mihalcea, German Rigau, and JanyceWiebe.
2014.
Semeval-2014 task 10: Multilingualsemantic textual similarity.
In Proceedings of the8th International Workshop on Semantic Evaluation(SemEval 2014), pages 81?91.Eneko Agirre, Carmen Banea, Claire Cardie, DanielCer, Mona Diab, Aitor Gonzalez-Agirre, WeiweiGuo, I Nigo Lopez-Gazpio, Montse Maritxalar,Rada Mihalcea, German Rigau, Larraitz Uria, andJanyce Wiebe.
2015.
Semeval-2015 task 2: Seman-tic textual similarity, english, spanish and pilot oninterpretability.
In Proceedings of the 9th Interna-tional Workshop on Semantic Evaluation (SemEval2015), pages 252?263.Henri Bal, Dick Epema, Cees de Laat, Rob van Nieuw-poort, John Romein, Frank Seinstra, Cees Snoek,and Harry Wijshoff.
2016.
A medium-scale dis-tributed system for computer science research: In-frastructure for the long term.
Computer, 49(5):54?63.Danqi Chen and Christopher D Manning.
2014.
Afast and accurate dependency parser using neuralnetworks.
In Proceedings of the Conference onEmpirical Methods in Natural Language Processing(EMNLP 2014), pages 740?750.Ronan Collobert and Jason Weston.
2008.
A unifiedarchitecture for natural language processing: Deepneural networks with multitask learning.
In Pro-ceedings of the 25th international conference onMachine learning (ICML 2008), pages 160?167.Manaal Faruqui, Jesse Dodge, Sujay K Jauhar, ChrisDyer, Eduard Hovy, and Noah A. Smith.
2014.Retrofitting word vectors to semantic lexicons.
InProceedings of the North American Chapter of the949Association for Computational Linguistics (NAACL2014).Samuel J. Gershman and Joshua B. Tenenbaum.
2015.Phrase similarity in humans and machines.
In Pro-ceedings of the 37th Annual Conference of the Cog-nitive Science Society, pages 776?781.Karl Moritz Hermann and Phil Blunsom.
2014a.
Mul-tilingual distributed representations without wordalignment.
In Proceedings of the International Con-ference on Learning Representations (ICLR 2014).Karl Moritz Hermann and Phil Blunsom.
2014b.
Mul-tilingual models for compositional distributed se-mantics.
In Proceeedings of the 52nd Annual Meet-ing of the Association for Computational Linguistics(ACL 2014), pages 58?68.Felix Hill, Kyunghyun Cho, and Anna Korhonen.2016.
Learning distributed representations of sen-tences from unlabelled data.
In Proceedings ofthe North American Chapter of the Association forComputational Linguistics (NAACL 2016).Baotian Hu, Zhengdong Lu, Hang Li, and QingcaiChen.
2014.
Convolutional neural network archi-tectures for matching natural language sentences.
InAdvances in Neural Information Processing Systems(NIPS 2014), pages 2042?2050.Tom Kenter and Maarten de Rijke.
2015.
Short textsimilarity with word embeddings.
In Proceedings ofthe 24th ACM International on Conference on Infor-mation and Knowledge Management (CIKM 2015),pages 1411?1420.Tom Kenter, Melvin Wevers, Pim Huijnen, andMaarten de Rijke.
2015.
Ad hoc monitoring of vo-cabulary shifts over time.
In Proceedings of the 24thACM International on Conference on Informationand Knowledge Management (CIKM 2015), pages1191?1200.Yoon Kim, I Yi-Chiu., Kentaro Hanaki, DarshanHegde, and Slav Petrov.
2014.
Temporal analysisof language through neural language models.
Pro-ceeedings of the 52nd Annual Meeting of the Asso-ciation for Computational Linguistics (ACL 2014),pages 61?65.Ryan Kiros, Yukun Zhu, Ruslan R Salakhutdinov,Richard Zemel, Raquel Urtasun, Antonio Torralba,and Sanja Fidler.
2015.
Skip-thought vectors.
InAdvances in Neural Information Processing Systems28 (NIPS 2015), pages 3294?3302.
Curran Asso-ciates, Inc.Matt Kusner, Yu Sun, Nicholas Kolkin, and Kilian QWeinberger.
2015.
From word embeddings to docu-ment distances.
In Proceedings of the 32nd Inter-national Conference on Machine Learning (ICML2015), pages 957?966.Stanislas Lauly, Hugo Larochelle, Mitesh Khapra,Balaraman Ravindran, Vikas C Raykar, and AmritaSaha.
2014.
An autoencoder approach to learningbilingual word representations.
In Advances in Neu-ral Information Processing Systems (NIPS 2014),pages 1853?1861.Andr?e Lynum, Partha Pakray, Bj?orn Gamb?ack, and Ser-gio Jimenez.
2014.
Ntnu: Measuring semantic sim-ilarity with sublexical feature representations andsoft cardinality.
In Proceedings of the 8th Interna-tional Workshop on Semantic Evaluation (SemEval2014), pages 448?453.Tomas Mikolov, Kai Chen, Greg S. Corrado, and Jef-frey Dean.
2013a.
Efficient estimation of wordrepresentations in vector space.
arXiv e-prints,1301.3781.Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S. Cor-rado, and Jeff Dean.
2013b.
Distributed representa-tions of words and phrases and their compositional-ity.
In Advances in Neural Information ProcessingSystems (NIPS 2013), pages 3111?3119.Ridho Reinanda, Edgar Meij, and Maarten de Rijke.2015.
Mining, ranking and recommending entityaspects.
In Proceedings of the 38th InternationalACM SIGIR Conference on Research and Develop-ment in Information Retrieval (SIGIR 2015), pages263?272.Aliaksei Severyn and Alessandro Moschitti.
2015.Learning to rank short text pairs with convolutionaldeep neural networks.
In Proceedings of the 38thInternational ACM SIGIR Conference on Researchand Development in Information Retrieval (SIGIR2015), pages 373?382.Richard Socher, Brody Huval, Christopher D Manning,and Andrew Y Ng.
2012.
Semantic compositional-ity through recursive matrix-vector spaces.
In Pro-ceedings of the 2012 Joint Conference on EmpiricalMethods in Natural Language Processing and Com-putational Natural Language Learning (EMNLP-CoNLL 2012), pages 1201?1211.Theano Development Team.
2016.
Theano: A Pythonframework for fast computation of mathematical ex-pressions.
arXiv e-prints, abs/1605.02688.Nikos Voskarides, Edgar Meij, Manos Tsagkias,Maarten de Rijke, and Wouter Weerkamp.
2015.Learning to explain entity relationships in knowl-edge graphs.
In Proceedings of the 53rd AnnualMeeting of the Association for Computational Lin-guistics and The 7th International Joint Confer-ence on Natural Language Processing of the AsianFederation of Natural Language Processing (ACL-IJCNLP 2015), pages 564?574.John Wieting, Mohit Bansal, Kevin Gimpel, and KarenLivescu.
2016.
Towards universal paraphrasticsentence embeddings.
Proceedings of the Inter-national Conference on Learning Representations(ICLR 2016).950Wentau Yih, Kristina Toutanova, John C. Platt, andChristopher Meek.
2011.
Learning discriminativeprojections for text similarity measures.
In Proceed-ings of the Fifteenth Conference on ComputationalNatural Language Learning, pages 247?256.Wenpeng Yin and Hinrich Sch?utze.
2015.
Convolu-tional neural network for paraphrase identification.In Proceedings of the North American Chapter of theAssociation for Computational Linguistics (NAACL2015), pages 901?911.Lei Yu, Karl Moritz Hermann, Phil Blunsom, andStephen Pulman.
2014.
Deep learning for answersentence selection.
In NIPS 2014 Deep Learningand Representation Learning Workshop.Yukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhut-dinov, Raquel Urtasun, Antonio Torralba, and SanjaFidler.
2015.
Aligning books and movies: Towardsstory-like visual explanations by watching moviesand reading books.
In Proceedings of the IEEE In-ternational Conference on Computer Vision, pages19?27.Will Y. Zou, Richard Socher, Daniel M. Cer, andChristopher D. Manning.
2013.
Bilingual wordembeddings for phrase-based machine translation.In Proceedings of the Conference on EmpiricalMethods in Natural Language Processing (EMNLP2013), pages 1393?1398.951
