2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 558?562,Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational LinguisticsIdentifying Comparable Corpora Using LDAJudita Preissj.preiss@sheffield.ac.ukDepartment of Computer Science, University of Sheffield, Regent Court211 Portobello, Sheffield, S1 4DP, United KingdomAbstractParallel corpora have applications in many ar-eas of Natural Language Processing, but arevery expensive to produce.
Much informationcan be gained from comparable texts, and wepresent an algorithm which, given any bod-ies of text in multiple languages, uses ex-isting named entity recognition software andtopic detection algorithm to generate pairs ofcomparable texts without requiring a paral-lel corpus training phase.
We evaluate thesystem?s performance firstly on data from theonline newspaper domain, and secondly onWikipedia cross-language links.1 IntroductionManual alignment or creation of parallel corpora isexceedingly expensive, requiring highly skilled an-notators or professional translators.
Methods existfor aligning parallel corpora, and extracted parallelsegments can be used to, for example, augment ma-chine translation phrase tables, but the amount ofgenuinely parallel data is limited.
However, paral-lel segments can also be extracted from comparablecorpora (a comparable corpus is one which containssimilar texts in more than one language).
Compara-ble documents, if produced with a confidence value,could also be used to prioritize translation (manualor automatic) when one is searching for further in-formation (which may only be available in a foreignlanguage) to augment information given in an arti-cle in the source language.
We present a techniqueto automatically detect comparable corpora in exist-ing data, and we demonstrate the applicability of ourmethod to any genre by evaluating on crawled onlinenewspaper text, as well as Wikipedia articles.Clearly, texts need to contain some of the samedata in order to be comparable (Harris, 1954), andwe assume:?
To be similar, texts need to share some namedentities, e.g., To?th et al, (2008).?
Comparable texts need to be on the same topic.Construction of multilingual topic models usu-ally requires either parallel data or some number ofaligned documents across multiple languages.
Zhaoand Xing (2007) create bilingual topic models from(at least 25%) of parallel data.
Mimno et al, (2009)start from tuples of equivalent documents to buildmodels, and then the same distribution over topicsholds in both source and target languages.While Zhao and Xing (2007) used their topicmodels for word alignment from comparable cor-pora (combined with underlying parallel data), mul-tilingual topic models are usually applied to data toautomatically detect word translations based on par-allel data, e.g., Vulic?
et al, (2011) exploit a sharedlanguage independent topic distribution to measurethe similarity between topics pertaining to words.The novelty of our work is the transformation of asource language topic model rather than the creationof a language independent model from parallel data.Transforming the source language model to the tar-get language allows the classification of the targetlanguage documents to source language topics.
Thetranslated model is applied to two document collec-tions to demonstrate its ability to detect comparable558corpora.
Our system can be applied to any pair oflanguages for which there is a dictionary.Section 2 describes the tools we employ.
Sec-tion 3 contains a description of our system: themethod for employing NE recognition across lan-guages is presented in Section 3.1, while Section 3.2outlines our technique for employing LDA acrosslanguages.
Our experiments and their results are de-scribed in Section 4.
Section 5 draws our conclu-sions and indicates avenues for future work.2 Tools2.1 Named entity recognitionThe Stanford named entity recognition (NER) soft-ware1 (Finkel et al, 2005) is an implementation oflinear chain Conditional Random Field (CRF) se-quence models, which includes a three class (per-son, organization, location and other) named entityrecognizer for English.2.2 Topic detectionLDA (Blei et al, 2003) is a generative probabilisticmodel where documents are viewed as mixtures overunderlying topics, and each topic is a distributionover words.
Both the document-topic and the topic-word distributions are assumed to have a Dirichletprior.
Given a set of documents and a number oftopics, the model returns ?i, the topic distributionfor each document i, and ?ik, the word distributionfor topic k. We employ the publicly available imple-mentation of LDA, JGibbLDA2 (Phan et al, 2008),which has two main execution methods: parameterestimation (model building) and inference for newdata (classification of a new document).
Both invo-cations produce the following:?ij : p(wordi|topicj)?jk: p(topicj |documentk)tassign: a deterministic topic-word assignment foreach word in every documentThe LDA topic models are created from a ran-domly selected tenth of the Reuters corpus (Roseet al, 2002).31http://nlp.stanford.edu/ner/index.shtml2http://jgibblda.sourceforge.net/3LDA modeling can abstract a model from a relatively smallcorpus and a tenth of the original Reuters corpus is much more2.3 IndexingTo provide quick searching access to the large textcollections, we utilize the high-performance searchengine library Lucene.4 The stemmed and stoplisteddocuments are stored along with the frequency ofoccurrence of each word within a document.2.4 Lemmatization / stemmingEnglish text is lemmatized using the lemmatizeravailable within RASP5 (Briscoe et al, 2006).
Stem-ming is provided for all the non-English languagesincluded in our work within Lucene.3 Identifying comparable corpora3.1 Cross language NERNEs extracted from the English text collections areautomatically translated into the target languages us-ing the BING Translation API6 yielding a singletranslation, which is retained.
The stemmed, trans-lated version of each NE in the source text is soughtin the indexed form of the target language documentcollection, and the frequency of occurrence of theNE is returned.Filtering is applied based on the proportion ofsource language document?s NEs found in the targetdocument (we do not expect all the NEs to be presentin the target language: NEs could be mis-translated,and not all NEs would necessarily be mentionedeven in a comparable document).
The proportionsof all types of NEs required were optimized over asmall manually created set.
While we could assign aweight and not filter documents, this is not believedto be adequate: e.g., a newspaper article containingall the source location mentions (and thus having ahigh weight), but none of the same people, is likelyto be a news story about the same area but a differentevent.manageable in terms of memory and time requirements.4http://lucene.apache.org5http://ilexir.co.uk/applications/rasp/download6The translations could also be retrieved from NE mappinglists, dictionaries (if these are available) or manually translated?
we therefore do not see this step as violating the lack of needfor a parallel corpus.5593.2 Cross language topic identificationBeing non-deterministic, multiple executions of theLDA algorithm are not guaranteed to (and do not)give rise to identical topics (even within one lan-guage).
It is therefore not possible to build a topicmodel in the source language and the target lan-guage separately, as there is no clear alignment be-tween their respective topics.
Traditionally, par-allel corpora are used to generate a language in-dependent topic-document distribution, from whichpolylingual topic models can be created so the un-derlying topics are shared.We propose to translate each word from thesource language topic model using the BING APIand substitute the new wordmap thus creating a tar-get language topic model.
While word distributionsare clearly different across languages, and building ashared topic-document distribution to sample wordsfrom allows words to retain their language specificdistributions, our technique completely avoids theneed for parallel corpora, and merely requires thetranslation of the words in the LDA model (whichcan be performed using dictionary lookup, or NElists instead of the BING API).3.3 Selecting comparable corporaTarget language candidate documents found to sharesufficient proportions of NEs are classified using thetranslated target language LDA model.
This yields?jk (the probability distribution of topic given doc-ument) and classifying the original document usingthe source language LDA model gives ??jk.
The can-didate documents are ranked according to the cosinesimilarity between the two vectors:similarity =?jk ?
??jk??jk???
?jk?By definition, cosine similarity ranges between -1and 1.
Similarity of 1 indicates two documents with?
= ?
?, and thus the higher the similarity, the higherwe rank the document.4 ExperimentsWe present two evaluations: firstly, we manu-ally evaluate the comparable documents generatedfrom online newspaper text in two languages, whilethe second evaluation finds comparable articles insource and target versions of Wikipedia with resultsevaluated against the cross-language links present inWikipedia.4.1 Online newspaper documentsSimple Google search yields a number of links toonline newspapers in any language, these lists (auto-matically retrieved) are used to seed a crawler.
Doc-uments from newspaper sites which allow crawlingare retrieved and only well formed HTML docu-ments are retained,7 and the language of the docu-ments is verified using a Perl implementation of Lin-gua::Ident (Dunning, 1994), an n-gram based modelfor language identification.8A single annotator evaluated 10 randomly se-lected English documents and the comparable doc-uments returned for them from 40,528 Czech news-paper articles (total retrieved within a 24 hour pe-riod).
Since there is no current scheme available forjudging comparability, we employed a four categoryscale:Strong: The documents are about the same newsevent, in a similar style.
(Articles about thesame news event, but elaborating, would be in-cluded here.
)Medium: The documents are about related newsevents.Weak: The documents refers to similar events.None: No overlap in topic in the two documents.Results of the evaluation are presented in Table 1;the top document is scored for each pair, showingthe high precision of the technique.
The 10 Englishdocuments were selected subject to the constraintthat a comparable corpus was retrieved for them: theimposed constraints on NEs make this a high preci-sion / low recall technique.
Many articles found us-ing the crawling approach on news sites (rather thanan RSS feed gathering approach) were discussions,7Note that the crawler is not permitted to leave the domainof the newspaper.8The Lingua::Ident Perl module is available from http://search.cpan.org/?mpiotr/Lingua-Ident-1.7/Ident.pm.
We build the models for the language identi-fication system from downloaded Wikipedia content for eachlanguage.560Strong Medium Weak None4 4 1 1Table 1: Results for English-Czech documentsfor example discussions of strategies in sports, inter-views with actors, rather than topical news stories.From a manual inspection of the target language ar-ticles, many of these articles do not appear to havecomparable equivalents.
Also, enforcing a high pro-portion of NEs shared between the source and targetlanguages frequently rules out documents which aresubsets of each other (this was also apparent in thesecond evaluation).4.2 WikipediaInformation within Wikipedia is connected acrosslanguages using cross-language links.
While thelists of links are not necessarily complete, and thearticles they link may not contain large parallel seg-ments, the linked documents should be comparable(under the definition), and thus provide an empiricalmeasure of the utility of our method.The top comparable articles in Czech were gener-ated for 100 randomly selected English Wikipediaarticles (subject to the constraint that they havecross-language links).
As in our first evaluation, thesystem had a low recall (35%), however precisionwas 83%.
By the design of the experiment, an arti-cle about the same subject has to exist in both lan-guages, and therefore the low recall value is surpris-ing.
Rather than a low cosine value, the low recallis mainly due to the NE filtering step removing the?correct?
article from consideration.
A brief inspec-tion of a small number of articles which had been fil-tered out was performed and substantial differencesbetween the pages were found ?
for example, a sig-nificant portion of the Wikipedia page for Equinox inEnglish contains descriptions of Equinox commem-orations all over the world, which are missing in theCzech version of the Wikipedia article (leading to alarge number of missing NEs).
Similar length of ar-ticles appeared to be a good indicator of both articlescontaining similar data, and our system detecting thetwo texts to be comparable.Please note that while the NE filtering step is re-moving texts from consideration, it is not possibleto compute cosines of the topic vectors of all docu-ments and thus some candidate selection step is nec-essary.4.3 BaselineThere are no standard baselines for the task of cre-ating comparable corpora.
It is possible to trans-late the source language text into the target languageusing BING, however, a cosine comparison of thestemmed, automatically translated document withall documents in the target language collection isextremely time consuming.
Applying NE filtering,automatically translating the remaining target lan-guage candidate texts into the source language us-ing BING, and ranking according to cosine similar-ity gives a precision of 69% for the collection dis-cussed in Section 4.2.5 ConclusionWe have presented an LDA based algorithm applica-ble to large document collections to find comparabledocuments across multi-lingual corpora without theneeding to train with parallel data.
We show, usinga human judge as well as Wikipedia cross-languagelinks, that the system achieves high precision in find-ing comparable documents.The technique strongly relies on the named en-tity method selected, and another technique may bemore suitable.
A comparison with a bilingual topicmodel created from parallel data would also proveinteresting.AcknowledgmentsThis research was supported by EU grant 248347on Analysis and Evaluation of Comparable Corporafor Under-Resourced Areas of Machine Translation(ACCURAT).
My thanks also go to the three review-ers whose comments strengthened the findings ofthis work.ReferencesBlei, D. M., Ng, A. Y., and Jordan, M. I.
(2003).
LatentDirichlet alocation.
Journal of Machine Learning Re-search, 3:993?1022.Briscoe, T., Carroll, J., and Watson, R. (2006).
The sec-ond release of the RASP system.
In Proceedings of theCOLING/ACL 2006 Interactive Presentation Sessions.561Dunning, T. (1994).
Statistical identification of language.Technical Report CRL MCCS-94-273, Computing Re-search Lab, New Mexico State University.Finkel, J. R., Grenager, T., and Manning, C. (2005).
In-corporating non-local information into information ex-traction systems by Gibbs sampling.
In Proceedings ofthe 43nd Annual Meeting of the Association for Com-putational Linguistics, pages 363?370.Harris, Z. S. (1954).
Distributional structure.
Word,10(23):146162.Mimno, D., Wallach, H. M., Naradowsky, J., Smith,D.
A., and McCallum, A.
(2009).
Polylingual topicmodels.
In Proceedings of the Conference on Empir-ical Methods in Natural Language Processing, page880889.Phan, X.-H., Nguyen, L.-M., and Horiguchi, S. (2008).Learning to classify short and sparse text & web withhidden topics from large-scale data collections.
InProceedings of The 17th International World WideWeb Conference (WWW 2008), pages 91?100.Rose, T. G., Stevenson, M., and Whitehead, M. (2002).The Reuters corpus volume 1 - from yesterday?s newsto tomorrow?s language resources.
In Proceedings ofthe Third International Conference on Language Re-sources and Evaluation, pages 827?832.To?th, K., Farkas, R., and Kocsor, A.
(2008).
Sentencealignment of Hungarian-English parallel corpora usinga hybrid algorithm.
Acta Cybernetica, pages 463?478.Vulic, I., Smet, W. D., and Moens, M.-F. (2011).
Iden-tifying word translations from comparable corpora us-ing latent topic models.
In Proceedings of ACL, pages479?484.Zhao, B. and Xing, E. P. (2007).
HM-BiTAM: Bilingualtopic exploration, word alignment, and translation.
InNIPS.562
