Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 336?344,Sydney, July 2006. c?2006 Association for Computational LinguisticsPartially Supervised Coreference Resolution for Opinion Summarizationthrough Structured Rule LearningVeselin Stoyanov and Claire CardieDepartment of Computer ScienceCornell UniversityIthaca, NY 14850, USA{ves,cardie}@cs.cornell.eduAbstractCombining fine-grained opinion informa-tion to produce opinion summaries is im-portant for sentiment analysis applica-tions.
Toward that end, we tackle theproblem of source coreference resolution?
linking together source mentions that re-fer to the same entity.
The partially super-vised nature of the problem leads us to de-fine and approach it as the novel problemof partially supervised clustering.
We pro-pose and evaluate a new algorithm for thetask of source coreference resolution thatoutperforms competitive baselines.1 IntroductionSentiment analysis is concerned with extractingattitudes, opinions, evaluations, and sentimentfrom text.
Work in this area has been motivatedby the desire to provide information analysis ap-plications in the arenas of government, business,and politics (e.g.
Coglianese (2004)).
Addition-ally, sentiment analysis can augment existing NLPapplications such as question answering, informa-tion retrieval, summarization, and clustering byproviding information about sentiment (e.g.
Stoy-anov et al (2005), Riloff et al (2005)).
To date,research in the area (see Related Work section)has focused on the problem of extracting senti-ment both at the document level (coarse-grainedsentiment information), and at the level of sen-tences, clauses, or individual expressions (fine-grained sentiment information).In contrast, our work concerns the summa-rization of fine-grained information about opin-ions.
In particular, while recent research ef-forts have shown that fine-grained opinions (e.g.Riloff and Wiebe (2003), Bethard et al (2004),Wiebe and Riloff (2005)) as well as their sources(e.g.
Bethard et al (2004), Choi et al (2005),Kim and Hovy (2005)) can be extracted auto-matically, little has been done to create opin-ion summaries, where opinions from the samesource/target are combined, statistics are com-puted for each source/target and multiple opinionsfrom the same source to the same target are ag-gregated.
A simple opinion summary is shown infigure 1.1 We expect that this type of opinion sum-mary, based on fine-grained opinion information,will be important for information analysis applica-tions in any domain where the analysis of opinionsis critical.This paper addresses the problem of opinionsummarization by considering the creation of sim-ple opinion summaries like those of figure 1.
Wepropose source coreference resolution ?
the taskof determining which mentions of opinion sourcesrefer to the same entity ?
as the primary mecha-nism for identifying the set of opinions attributedto each real-world source.
For this type of sum-mary, source coreference resolution constitutes anintegral step in the process of generating full opin-ion summaries.
For example, given the opinionexpressions of figure 1, their polarity, and the asso-ciated opinion sources and targets, the bulk of theresulting summary can be produced by recogniz-ing that source mentions ?Zacarias Moussaoui?,?he?, ?my?, and ?Mr.
Moussaoui?
all refer tothe same person; and that source mentions ?Mr.Zerkin?
and ?Zerkin?
refer to the same person.21For simplicity, the example summary does not containany source/target statistics.2In addition, the summary would require the closely re-lated task of target coreference resolution and a means for ag-gregating the conflicting opinions from Zerkin toward Mous-saoui.336At first glance, source coreference resolutionappears equivalent to the task of noun phrasecoreference resolution and therefore amenable totraditional coreference resolution techniques (e.g.Ng and Cardie (2002), Morton (2000)).
We hy-pothesize in Section 3, however, that the task islikely to succumb to a better solution by treatingit in the context of a new machine learning set-ting that we refer to as partially supervised clus-tering.
In particular, due to high coreference an-notation costs, data sets that are annotated withopinion information (like ours) do not typically in-clude supervisory coreference information for allnoun phrases in a document (as would be requiredfor the application of traditional coreference reso-lution techniques), but only for noun phrases thatact as opinion sources (or targets).As a result, we define the task of partially su-pervised clustering, the goal of which is to learna clustering function from a set of partially spec-ified clustering examples (Section 4).
We are notaware of prior work on the problem of partiallysupervised clustering and argue that it differs sub-stantially from that of semi-supervised clustering.We propose an algorithm for partially supervisedclustering that extends a rule learner with structureinformation and is generally applicable to prob-lems that fit the partially supervised clustering def-inition (Section 5).
We apply the algorithm tothe source coreference resolution task and evalu-ate its performance on a standard sentiment analy-sis data set that includes source coreference chains(Section 6).
We find that our algorithm outper-forms highly competitive baselines by a consid-erable margin ?
B3 score of 83.2 vs. 81.8 and67.1 vs. 60.9 F1 score for the identification ofpositive source coreference links.2 Related WorkWork relevant to our problem can be split intothree main areas ?
sentiment analysis, traditionalnoun phrase coreference resolution, and super-vised and weakly supervised clustering.
Relatedwork in the former two areas is summarized brieflybelow.
Supervised and weakly supervised cluster-ing approaches are discussed in Section 4.Sentiment analysis.
Much of the relevant re-search in sentiment analysis addresses sentimentclassification, a text categorization task of extract-ing opinion at the coarse-grained document level.The goal in sentiment classification is to assign to[Source Zacarias Moussaoui] [?
complained] at length todayabout [Target his own lawyer], telling a federal court jury that[Target he] was [?
more interested in achieving fame than sav-ing Moussaoui?s life].Mr.
Moussaoui said he was appearing on the witness stand totell the truth.
And one part of the truth, [Source he] said, is that[Target sending him to prison for life] would be ?[?
a greaterpunishment] than being sentenced to death.??[?
[Target You] have put your interest ahead of [Source my]life],?
[Source Mr. Moussaoui] told his court-appointed lawyerGerald T.
Zerkin....But, [Source Mr. Zerkin] pressed [Target Mr. Moussaoui], wasit [?
not true] that he told his lawyers earlier not to involveany Muslims in the defense, not to present any evidence thatmight persuade the jurors to spare his life?...
[Source Zerkin] seemed to be trying to show the jurorsthat while [Target the defendant] is generally [+ an honestindividual], his conduct shows [Target he] is [?
not stablementally], and thus [?
undeserving] of [Target the ultimatepunishment].MoussaouiZerkinprison for lifeultimate punishment???
?/ +Figure 1: Example text containing opinions(above) and a summary of the opinions (be-low).
Sources and targets of opinions are brack-eted; opinion expressions are shown in italics andbracketed with associated polarity, either positive(+) or negative (-).
The underlined phrase will beexplained later in the paper.a document either positive (?thumbs up?)
or nega-tive (?thumbs down?)
polarity (e.g.
Das and Chen(2001), Pang et al (2002), Turney (2002), Daveet al (2003)).
Other research has concentratedon analyzing fine-grained opinions at, or below,the sentence level.
Recent work, for example, in-dicates that systems can be trained to recognizeopinions and their polarity, strength, and sourcesto a reasonable degree of accuracy (e.g.
Dave etal.
(2003), Riloff and Wiebe (2003), Bethard etal.
(2004), Wilson et al (2004), Yu and Hatzivas-siloglou (2003), Choi et al (2005), Kim and Hovy(2005), Wiebe and Riloff (2005)).
Our work ex-tends research on fine-grained opinion extractionby augmenting the opinions with additional infor-mation that allows the creation of concise opinionsummaries.
In contrast to the opinion extracts pro-duced by Pang and Lee (2004), our summaries arenot text extracts, but rather explicitly identify and337characterize the relations between opinions andtheir sources.Coreference resolution.
Coreference resolutionis a relatively well studied NLP problem (e.g.Morton (2000), Ng and Cardie (2002), Iida et al(2003), McCallum and Wellner (2003)).
Corefer-ence resolution is defined as the problem of decid-ing which noun phrases in the text (mentions) re-fer to the same real world entities (are coreferent).Generally, successful approaches to coreferenceresolution have relied on supervised classificationfollowed by clustering.
For supervised classifica-tion these approaches learn a pairwise function topredict whether a pair of noun phrases is corefer-ent.
Subsequently, when making coreference res-olution decisions on unseen documents, the learntpairwise NP coreference classifier is run, followedby a clustering step to produce the final clusters(coreference chains) of coreferent NPs.
For bothtraining and testing, coreference resolution algo-rithms rely on feature vectors for pairs of nounphrases that encode linguistic information aboutthe NPs and their local context.
Our general ap-proach to source coreference resolution is inspiredby the state-of-the-art performance of one such ap-proach to coreference resolution, which relies on arule learner and single-link clustering as describedin Ng and Cardie (2002).3 Source Coreference ResolutionIn this section we introduce the problem of sourcecoreference resolution in the context of opinionsummarization and argue for the need for novelmethods for the task.The task of source coreference resolution is todecide which mentions of opinion sources refer tothe same entity.
Much like traditional coreferenceresolution, we employ a learning approach; how-ever, our approach differs from traditional coref-erence resolution in its definition of the learn-ing task.
Motivated by the desire to utilize unla-beled examples (discussed later), we define train-ing as an integrated task in which pairwise NPcoreference decisions are learned together withthe clustering function as opposed to treating eachNP pair as a training example.
Thus, our train-ing phase takes as input a set of documents withmanually annotated opinion sources together withcoreference annotations for the sources; it outputsa classifier that can produce source coreferencechains for previously unseen documents contain-ing marked (manually or automatically) opinionsources.
More specifically, the source coreferenceresolution training phase proceeds through the fol-lowing steps:1.
Source-to-NP mapping: We preprocesseach document by running a tokenizer, sen-tence splitter, POS tagger, parser, and an NPfinder.
Subsequently, we augment the set ofNPs found by the NP finder with the help ofa system for named entity detection.
We thenmap the sources to the NPs.
Since there isno one-to-one correspondence, we use a setof heuristics to create the mapping.
More de-tails about why heuristics are needed and theprocess used to map sources to NPs can befound in Stoyanov and Cardie (2006).2.
Feature vector creation: We extract a fea-ture vector for every pair of NPs from the pre-processed corpus.
We use the features intro-duced by Ng and Cardie (2002) for the taskof coreference resolution.3.
Classifier construction: Using the featurevectors from step 2, we construct a trainingset containing one training example per doc-ument.
Each training example consists of thefeature vectors for all pairs of NPs in the doc-ument, including those that do not map tosources, together with the available corefer-ence information for the source noun phrases(i.e.
the noun phrases to which sources aremapped).
The training instances are pro-vided as input to a learning algorithm (seeSection 5), which constructs a classifier thatcan take the instances associated with a new(previously unseen) document and produce aclustering over all NPs in the document.The testing phase employs steps 1 and 2 as de-scribed above, but replaces step 3 by a straightfor-ward application of the learnt classifier.
Since weare interested in coreference information only forthe source NPs, we simply discard the non-sourceNPs from the resulting clustering.The approach to source coreference resolutiondescribed here would be identical to traditionalcoreference resolution when provided with train-ing examples containing coreference informationfor all NPs.
However, opinion corpora in general,and our corpus in particular, contain no corefer-ence information about general NPs.
Neverthe-less, after manual sources are mapped to NPs in338step 1 above, our approach can rely on the avail-able coreference information for the source NPs.Due to the high cost of coreference annotation, wedesire methods that can work in the presence ofonly this limited amount of coreference informa-tion.A possible workaround the absence of full NPcoreference information is to train a traditionalcoreference system only on the labeled part of thedata (indeed that is one of the baselines againstwhich we compare).
However, we believe thatan effective approach to source coreference res-olution has to utilize the unlabeled noun phrasesbecause links between sources might be realizedthrough non-source mentions.
This problem is il-lustrated in figure 1.
The underlined Moussaouiis coreferent with all of the Moussaoui referencesmarked as sources, but, because it is used in anobjective sentence rather than as the source ofan opinion, the reference would be omitted fromthe Moussaoui source chain.
Unfortunately, thisproper noun phrase might be critical in establish-ing the coreference of the final source reference hewith the other mentions of the source Moussaoui.As mentioned previously, in order to utilizethe unlabeled data, our approach differs from tra-ditional coreference resolution, which uses NPpairs as training instances.
We instead follow theframework of supervised clustering (Finley andJoachims, 2005; Li and Roth, 2005) and considereach document as a training example.
As in super-vised clustering, this framework has the additionaladvantage that the learning algorithm can considerthe clustering algorithm when making decisionsabout pairwise classification, which could lead toimprovements in the classifier.
In the next sectionwe describe our approach to classifier constructionfor step 3 and compare our problem to traditionalweakly supervised clustering, characterizing it asan instance of the novel problem of partially su-pervised clustering.4 Partially Supervised ClusteringIn our desire to perform effective source corefer-ence resolution we arrive at the following learningproblem ?
the learning algorithm is presented witha set of partially specified examples of clusteringsand acquires a function that can cluster accuratelyan unseen set of items, while taking advantage ofthe unlabeled information in the examples.This setting is to be contrasted with semi-supervised clustering (or clustering with con-straints), which has received much research at-tention (e.g.
Demiriz et al (1999), Wagstaff andCardie (2000), Basu (2005), Davidson and Ravi(2005)).
Semi-supervised clustering can be de-fined as the problem of clustering a set of itemsin the presence of limited supervisory informa-tion such as pairwise constraints (e.g.
two itemsmust/cannot be in the same cluster) or labeledpoints.
In contrast to our setting, in the semi-supervised case there is no training phase ?
thealgorithm receives all examples (labeled and un-labeled) at the same time together with some dis-tance or cost function and attempts to find a clus-tering that optimizes a given measure (usuallybased on the distance or cost function).Source coreference resolution might alterna-tively be approached as a supervised clusteringproblem.
Traditionally, approaches to supervisedclustering have treated the pairwise link decisionsas a classification problem.
These approaches firstlearn a distance metric that optimizes the pairwisedecisions; and then follow the pairwise classifica-tion with a clustering step.
However, these tradi-tional approaches have no obvious way of utilizingthe available unlabeled information.In contrast, we follow recent approaches to su-pervised clustering that propose ways to learnthe distance measure in the context of the clus-tering decisions (Li and Roth, 2005; Finley andJoachims, 2005; McCallum and Wellner, 2003).This provides two advantages for the problem ofsource coreference resolution.
First, it allows thealgorithm to take advantage of the complexity ofthe rich structural dependencies introduced by theclustering problem.
Viewed traditionally as a hur-dle, the structural complexity of clustering may bebeneficial in the partially supervised case.
We be-lieve that provided with a few partially specifiedclustering examples, an algorithm might be ableto generalize from the structural dependencies toinfer correctly the whole clustering of the items.In addition, considering pairwise decisions in thecontext of the clustering can arguably lead to moreaccurate classifiers.Unfortunately, none of the supervised cluster-ing approaches is readily applicable to the partiallysupervised case.
However, by adapting the for-mal supervised clustering definition, which we donext, we can develop approaches to partially su-pervised clustering that take advantage of the un-339labeled portions of the data.Formal definition.
For partially supervisedclustering we extend the formal definition of su-pervised clustering given by Finley and Joachims(2005).
In the fully supervised setting, an al-gorithm is given a set S of n training examples(x1, y1), ..., (xn, yn) ?
X ?
Y , where X is theset of all possible sets of items and Y is the set ofall possible clusterings of these sets.
For a train-ing example (x, y), x = {x1, x2, ..., xk} is a setof k items and y = {y1, y2, ..., yr} is a clusteringof the items in x with each yi ?
x. Addition-ally, each item can be in no more than one cluster(?i, j.yi ?
yj = ?)
and in the fully supervised caseeach item is in at least one cluster (x = ?
yi).The goal of the learning algorithm is to acquire afunction h : X ?
Y that can accurately cluster a(previously unseen) set of items.In the context of source coreference resolutionthe training set contains one example for each doc-ument.
The items in each training example are theNPs and the clustering over the items is the equiv-alence relation defined by the coreference infor-mation.
For source coreference resolution, how-ever, clustering information is unavailable for thenon-source NPs.
Thus, to be able to deal with thisunlabeled component of the data we arrive to thesetting of partially supervised clustering, in whichwe relax the condition that each item is in at leastone cluster (x = ?
yi) and replace it with the con-dition x ??yi.
The items with no linking infor-mation (items in x \?
yi) constitute the unlabeled(unsupervised) component of the partially super-vised clustering.5 Structured Rule LearnerWe develop a novel method for partially super-vised clustering, which is motivated by the successof a rule learner (RIPPER) for coreference resolu-tion (Ng and Cardie, 2002).
We extend RIPPERso that it can learn rules in the context of single-link clustering, which both suits our task (i.e.
pro-nouns link to their single antecedent) and has ex-hibited good performance for coreference resolu-tion (Ng and Cardie, 2002).
We begin with a briefoverview of RIPPER followed by a description ofthe modifications that we implemented.
For easeof presentation, we assume that we are in the fullysupervised case.
We end this section by describingthe changes for the partially supervised case.procedure StRip(TrainData){GrowData, PruneData = Split(TrainData);//Keep instances from the same document togetherwhile(there are positive uncovered instances) {r = growRule(GrowData);r = pruneRule(r, PruneData);DL = relativeDL(Ruleset);if(DL ?
minDL + d bits)Ruleset.add(r);Mark examples covered by r as +;elseexit loop with Ruleset}}procedure growRule(growData){r = empty rule;for(every unused feature f){if (f is nominal feature) {for(every possible value v of f) {mark all instances that have values of v for f with +;compute the transitive closure of the positive instances//(including instances marked + from previous rules);compute the infoGain for the future/value combination;}} else{ //Numeric featurecreate one bag for each feature value and split the instances into bags;do a forward and a backward pass over the bags keeping a runningclustering and compute the information gain for each value;}}add the future/value pair with the best infoGain to r;growData = growData - all negative instances;return r;}procedure pruneRule(r, pruneData){for(all antecedents a in the rule){apply all antecedents in r up to a to pruneData;compute the transitive closure of the positive instances;compute A(a) ?
the accuracy of the rule up to antecedent a;}Remove all antecedents after the antecedent for which A(a) is maximum.
}Figure 2: The StRip algorithm.
Additions to RIP-PER are shown in bold.5.1 The RIPPER AlgorithmRIPPER (for Repeated Incremental Pruning toProduce Error Reduction) was introduced by Co-hen (1995) as an extension of an existing ruleinduction algorithm.
Cohen (1995) showed thatRIPPER produces error rates competitive withC4.5, while exhibiting better running times.
RIP-PER consists of two phases ?
a ruleset is grownand then optimized.The ruleset creation phase begins by ran-domly splitting the training data into a rule-growing set (2/3 of the training data) and a pruningset (the remaining 1/3).
A rule is then grown onthe former set by repeatedly adding the antecedent(the feature value test) with the largest informationgain until the accuracy of the rule becomes 1.0 orthere are no remaining potential antecedents.
Nextthe rule is applied to the pruning data and any rule-final sequence that reduces the accuracy of the ruleis removed.The optimization phase uses the full training340set to first grow a replacement rule and a revisedrule for each rule in the ruleset.
For each rule,the algorithm then considers the original rule, thereplacement rule, and the revised rule, and keepsthe rule with the smallest description length in thecontext of the ruleset.
After all rules are con-sidered, RIPPER attempts to grow residual rulesthat cover data not already covered by the rule-set.
Finally, RIPPER deletes any rules from theruleset that reduce the overall minimum descrip-tion length of the data plus the ruleset.
RIPPERperforms two rounds of this optimization phase.5.2 The StRip AlgorithmThe property of partially supervised clustering thatwe want to explore is the structured nature of thedecisions.
That is, each decision of whether twoitems (say a and b) belong to the same cluster hasan implication for all items a?
that belong to a?scluster and all items b?
that belong to b?s cluster.We target modifications to RIPPER that will al-low StRip (for Structured RIPPER) to learn rulesthat produce good clusterings in the context ofsingle-link clustering.
We extend RIPPER so thatevery time it makes a decision about a rule, it con-siders the effect of the rule on the overall clus-tering of items (as opposed to considering the in-stances that the rule classifies as positive/negativein isolation).
More precisely, we precede everycomputation of rule performance (e.g.
informa-tion gain or description length) by a transitive clo-sure (i.e.
single link clustering) of the data w.r.t.
tothe pairwise classifications.
Following the transi-tive closure, all pairs of items that are in the samecluster are considered covered by the rule for per-formance computation.The StRip algorithm is given in figure 2, withmodifications to the original RIPPER algorithmshown in bold.
Due to space limitations the op-timization stage of the algorithm is omitted.
Ourmodifications to the optimization stage of RIPPERare in the spirit of the rest of the StRip algorithm.Partially supervised case.
So far we describedStRip only for the fully supervised case.
Weuse a very simple modification to handle the par-tially supervised setting: we exclude the unla-beled pairs when computing the performance ofthe rules.
Thus, the unlabeled items do not countas correct or incorrect classifications when acquir-ing or pruning a rule, although they do participatein the transitive closure.
Links in the unlabeleddata are inferred entirely through the indirect linksbetween items in the labeled component that theyintroduce.
In the example of figure 1, the twoproblematic unlabeled links are the link betweenthe source mention ?he?
and the underlined non-source NP ?Mr.
Moussaoui?
and the link betweenthe underlined ?Mr.
Moussaoui?
to any sourcemention of Moussaoui.
While StRip will not re-ward any rule (or rule set) that covers these twolinks directly, such rules will be rewarded indi-rectly since they put the source he in the chain forthe source Moussaoui.StRip running time.
StRip?s running time isgenerally comparable to that of RIPPER.
We com-pute transitive closure by using a Union-Findstructure, which runs in time O(log?n), which forpractical purposes can be considered linear (O(n))3.
However, when computing the best informationgain for a nominal feature, StRip has to make apass over the data for each value that the featuretakes, while RIPPER can split the data into bagsand perform the computation in one pass.6 Evaluation and ResultsThis section describes the source coreference dataset, the baselines, our implementation of StRip,and the results of our experiments.6.1 Data setFor evaluation we use the MPQA corpus (Wiebeet al, 2005).4 The corpus consists of 535 doc-uments from the world press.
All documents inthe collection are manually annotated with phrase-level opinion information following the annota-tion scheme of Wiebe et al (2005).
Discussionof the annotation scheme is beyond the scope ofthis paper; for our purposes it suffices to say thatthe annotations include the source of each opin-ion and coreference information for the sources(e.g.
source coreference chains).
The corpus con-tains no additional noun phrase coreference infor-mation.For our experiments, we randomly split the dataset into a training set consisting of 400 documentsand a test set consisting of the remaining 135 doc-uments.
We use the same test set for all experi-3For the transitive closure, n is the number of items in adocument, which is O(?k), where k is the number of NPpairs.
Thus, transitive closure is sublinear in the number oftraining instances.4The MPQA corpus is available athttp://nrrc.mitre.org/NRRC/publications.htm.341ments, although some learning runs were trainedon 200 training documents (see next Subsection).The test set contains a total of 4736 source NPs(average of 35.34 source NPs per document) splitinto 1710 total source NP chains (average of 12.76chains per document) for an average of 2.77 sourceNPs per chain.6.2 ImplementationWe implemented the StRip algorithm by modify-ing JRip ?
the java implementation of RIPPER in-cluded in the WEKA toolkit (Witten and Frank,2000).
The WEKA implementation follows theoriginal RIPPER specification.
We changed theimplementation to incorporate the modificationssuggested by the StRip algorithm; we also mod-ified the underlying data representations and datahandling techniques for efficiency.
Also due to ef-ficiency considerations, we train StRip only on the200-document training set.6.3 Competitive baselinesWe compare the results of the new method to threefully supervised baseline systems, each of whichemploys the same traditional coreference resolu-tion approach.
In particular, we use the afore-mentioned algorithm proposed by Ng and Cardie(2002), which combines a pairwise NP corefer-ence classifier with single-link clustering.For one baseline, we train the coreference reso-lution algorithm on the MPQA src corpus ?
thelabeled portion of the MPQA corpus (i.e.
NPsfrom the source coreference chains) with unla-beled instances removed.The second and third baselines investigatewhether the source coreference resolution task canbenefit from NP coreference resolution trainingdata from a different domain.
Thus, we train thetraditional coreference resolution algorithm on theMUC6 and MUC7 coreference-annotated corpora5that contain documents similar in style to those inthe MPQA corpus (e.g.
newspaper articles), butemanate from different domains.For all baselines we targeted the best possi-ble systems by trying two pairwise NP classifiers(RIPPER and an SVM in the SV M light imple-mentation (Joachims, 1998)), many different pa-rameter settings for the classifiers, two differentfeature sets, two different training set sizes (the5We train each baseline using both the development setand the test set from the corresponding MUC corpus.full training set and a smaller training set consist-ing of half of the documents selected at random),and three different instance selection algorithms6.This variety of classifier and training data settingswas motivated by reported differences in perfor-mance of coreference resolution approaches w.r.t.these variations (Ng and Cardie, 2002).
More de-tails on the different parameter settings and in-stance selection algorithms as well as trends in theperformance of different settings can be found inStoyanov and Cardie (2006).
In the experimentsbelow we report the best performance of each ofthe two learning algorithms on the MPQA testdata.6.4 EvaluationIn addition to the baselines described above, weevaluate StRip both with and without unlabeleddata.
That is, we train on the MPQA corpus StRipusing either all NPs or just opinion source NPs.We use the B3 (Bagga and Baldwin, 1998) eval-uation measure as well as precision, recall, andF1 measured on the (positive) pairwise decisions.B3 is a measure widely used for evaluating coref-erence resolution algorithms.
The measure com-putes the precision and recall for each NP mentionin a document, and then averages them to producecombined results for the entire output.
More pre-cisely, given a mention i that has been assignedto chain ci, the precision for mention i is definedas the number of correctly identified mentions inci divided by the total number of mentions in ci.Recall for i is defined as the number of correctlyidentified mentions in ci divided by the number ofmentions in the gold standard chain for i.Results are shown in Table 1.
The first sixrows of results correspond to the fully supervisedbaseline systems trained on different corpora ?MUC6, MUC7, and MPQA src.
The seventh rowof results shows the performance of StRip usingonly labeled data.
The final row of the table showsthe results for partially supervised learning withunlabeled data.
The table lists results from the bestperforming run for each algorithm.Performance among the baselines trained on theMUC data is comparable.
However, the two base-line runs trained on the MPQA src corpus (i.e.
re-sults rows five and six) show slightly better perfor-mance on the B3 metric than the baselines trained6The goal of the instance selection algorithms is to bal-ance the data, which contains many more negative than posi-tive instances342ML Framework Training set Classifier B3 precision recall F1Fully supervised MUC6 SVM 81.2 72.6 52.5 60.9RIPPER 80.7 57.4 63.5 60.3MUC7 SVM 81.7 65.6 55.9 60.4RIPPER 79.7 71.6 48.5 57.9MPQA src SVM 81.8 57.5 62.9 60.2RIPPER 81.8 72.0 52.5 60.6StRip 82.3 76.5 56.1 64.6Partially supervised MPQA all StRip 83.2 77.1 59.4 67.1Table 1: Results for Source Coreference.
MPQA src stands for the MPQA corpus limited to only sourceNPs, while MPQA full contains the unlabeled NPs.on the MUC data, which indicates that for ourtask the similarity of the documents in the train-ing and test sets appears to be more importantthan the presence of complete supervisory infor-mation.
(Improvements over the RIPPER runstrained on the MUC corpora are statistically sig-nificant7, while improvements over the SVM runsare not.
)Table 1 also shows that StRip outperforms thebaselines on both performance metrics.
StRip?sperformance is better than the baselines whentrained on MPQA src (improvement not statisti-cally significant, p > 0.20) and even better whentrained on the full MPQA corpus, which includesthe unlabeled NPs (improvement over the base-lines and the former StRip run statistically signif-icant).
These results confirm our hypothesis thatStRip improves due to two factors: first, consider-ing pairwise decisions in the context of the clus-tering function leads to improvements in the clas-sifier; and, second, StRip can take advantage ofthe unlabeled portion of the data.StRip?s performance is all the more impressiveconsidering the strength of the SVM and RIPPERbaselines, which which represent the best runsacross the 336 different parameter settings testedfor SV M light and 144 different settings tested forRIPPER.
In contrast, all four of the StRip runs us-ing the full MPQA corpus (we vary the loss ratiofor false positive/false negative cost) outperformthose baselines.7 Future WorkSource coreference resolution is only one aspectof opinion summarization.
Additionally, an opin-ion summarization system will need to handle7Statistical significance is measured using both a 2-tailedpaired t-test and the Wilcoxon matched-pairs signed-rankstest (p < 0.05).
The two tests agreed on all significancejudgements, so we will not report them separately.the closely related task of target coreference res-olution in order to cluster targets of opinions8and combine multiple conflicting opinions from asource to the same targets.
Furthermore, a fullyautomatic opinion summarizer requires automaticsource and opinion extractors.
While we antici-pate that target coreference resolution will be sub-ject to error rates similar to those of source coref-erence resolution, incorporating these imperfectopinions and sources will further impair the per-formance of the opinion summarizer.
We are notaware of any measure that can be directly usedto assess the goodness of opinion summaries, butplan to develop such in future work in conjunc-tion with the development of methods for creatingopinion summaries completely automatically.
Theevaluation metrics will likely have to depend onthe task for which the summaries are used.A limitation of our approach to partially super-vised clustering is that we do not directly optimizefor the performance measure (e.g.
B3).
Other ef-forts in the area of supervised clustering (Finleyand Joachims, 2005; Li and Roth, 2005) have sug-gested ways to learn distance measures that canoptimize directly for a desired performance mea-sure.
We plan to investigate algorithms that can di-rectly optimize for complex measures (such as B3)for the problem of partially supervised clustering.Unfortunately, a measure as complex as B3 makesextending existing approaches far from trivial dueto the difficulty of establishing the connection be-tween individual pairwise decisions (the distancemetric) and the score of the clustering algorithm.AcknowledgementsThe authors would like to thank Vincent Ng andArt Munson for providing coreference resolution8We did not tackle the task of target coreference resolu-tion in this paper because the MPQA corpus did not containtarget annotations at the time of publication.343code, members of the Cornell NLP group (es-pecially Yejin Choi and Art Munson) for manyhelpful discussions, and the anonymous review-ers for their insightful comments.
This work wassupported by the Advanced Research and Devel-opment Activity (ARDA), by NSF Grants IIS-0535099 and IIS-0208028, by gifts from Googleand the Xerox Foundation, and by an NSF Gradu-ate Research Fellowship to the first author.ReferencesA.
Bagga and B. Baldwin.
1998.
Entity-based cross-document coreferencing using the vector space model.
InIn Proceedings of COLING/ACL.S.
Basu.
2005.
Semi-supervised Clustering: ProbabilisticModels, Algorithms and Experiments.
Ph.D. thesis, De-partment of Computer Sciences, UT at Austin.S.
Bethard, H. Yu, A. Thornton, V. Hativassiloglou, andD.
Jurafsky.
2004.
Automatic extraction of opinionpropositions and their holders.
In 2004 AAAI Spring Sym-posium on Exploring Attitude and Affect in Text.Y.
Choi, C. Cardie, E. Riloff, and S. Patwardhan.
2005.
Iden-tifying sources of opinions with conditional random fieldsand extraction patterns.
In Proceedings of EMNLP.C.
Coglianese.
2004.
E-rulemaking: Information technologyand regulatory policy: New directions in digital govern-ment research.
Technical report, Harvard University, J. F.Kennedy School of Government.W.
Cohen.
1995.
Fast effective rule induction.
In Proceed-ings of ICML.S.
Das and M. Chen.
2001.
Yahoo for amazon: Extractingmarket sentiment from stock message boards.
In Proceed-ings of APFAAC.K.
Dave, S. Lawrence, and D. Pennock.
2003.
Mining thepeanut gallery: Opinion extraction and semantic classifi-cation of product reviews.
In Proceedings of IWWWC.I.
Davidson and S. Ravi.
2005.
Clustering with constraints:Feasibility issues and the k-means algorithm.
In Proceed-ings of SDM.A.
Demiriz, K. P. Bennett, and M. J. Embrechts.
1999.
Semi-supervised clustering using genetic algorithms.
In Pro-ceeding of ANNIE.T.
Finley and T. Joachims.
2005.
Supervised clustering withsupport vector machines.
In Proceedings of ICML.R.
Iida, K. Inui, H. Takamura, and Y. Matsumoto.
2003.
In-corporating contextual cues in trainable models for coref-erence resolution.
In Proceedings of the EACL Workshopon The Computational Treatment of Anaphora.T.
Joachims.
1998.
Making large-scale support vectormachine learning practical.
In A. Smola B. Scho?lkopf,C.
Burges, editor, Advances in Kernel Methods: SupportVector Machines.
MIT Press, Cambridge, MA.S.
Kim and E. Hovy.
2005.
Identifying opinion holders forquestion answering in opinion texts.
In Proceedings ofAAAI Workshop on Question Answering in Restricted Do-mains.X.
Li and D. Roth.
2005.
Discriminative training of cluster-ing functions: Theory and experiments with entity identi-fication.
In Proceedings of CoNLL.A.
McCallum and B. Wellner.
2003.
Toward conditionalmodels of identity uncertainty with application to propernoun coreference.
In Proceedings of the IJCAI Workshopon Information Integration on the Web.T.
Morton.
2000.
Coreference for NLP applications.
In Pro-ceedings of ACL.V.
Ng and C. Cardie.
2002.
Improving machine learningapproaches to coreference resolution.
In In Proceedingsof ACL.B.
Pang and L. Lee.
2004.
A sentimental education: Senti-ment analysis using subjectivity summarization based onminimum cuts.
In Proceedings of ACL.B.
Pang, L. Lee, and S. Vaithyanathan.
2002.
Thumbsup?
Sentiment classification using machine learning tech-niques.
In Proceedings of EMNLP.E.
Riloff and J. Wiebe.
2003.
Learning extraction patternsfor subjective expressions.
In Proceesings of EMNLP.E.
Riloff, J. Wiebe, and W. Phillips.
2005.
Exploiting sub-jectivity classification to improve information extraction.In Proceedings of AAAI.V.
Stoyanov and C. Cardie.
2006.
Toward opinion summa-rization: Linking the sources.
In Proceedings of the ACLWorkshop on Sentiment and Subjectivity in Text.V.
Stoyanov, C. Cardie, and J. Wiebe.
2005.
Multi-Perspective question answering using the OpQA corpus.In Proceedings of EMNLP.P.
Turney.
2002.
Thumbs up or thumbs down?
Semantic ori-entation applied to unsupervised classification of reviews.In Proceedings of ACL.K.
Wagstaff and C. Cardie.
2000.
Clustering with instance-level constraints.
In Proceedings of the 17-th NationalConference on Artificial Intelligence and 12-th Confer-ence on Innovative Applications of Artificial Intelligence.J.
Wiebe and E. Riloff.
2005.
Creating subjective and objec-tive sentence classifiers from unannotated texts.
In Pro-ceedings of CICLing.J.
Wiebe, T. Wilson, and C. Cardie.
2005.
Annotating ex-pressions of opinions and emotions in language.
Lan-guage Resources and Evaluation, 1(2).T.
Wilson, J. Wiebe, and R. Hwa.
2004.
Just how mad areyou?
Finding strong and weak opinion clauses.
In Pro-ceedings of AAAI.I.H.
Witten and E. Frank.
2000.
Data Mining: Practical ma-chine learning tools with Java implementations.
MorganKaufmann, San Francisco.H.
Yu and V. Hatzivassiloglou.
2003.
Towards answer-ing opinion questions: Separating facts from opinions andidentifying the polarity of opinion sentences.
In Proceed-ings of EMNLP.344
