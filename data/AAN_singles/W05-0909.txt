Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translationand/or Summarization, pages 65?72, Ann Arbor, June 2005. c?2005 Association for Computational LinguisticsMETEOR: An Automatic Metric for MT Evaluation withImproved Correlation with Human JudgmentsSatanjeev Banerjee Alon LavieLanguage Technologies Institute Language Technologies InstituteCarnegie Mellon University Carnegie Mellon UniversityPittsburgh, PA 15213 Pittsburgh, PA 15213banerjee+@cs.cmu.edu alavie@cs.cmu.eduAbstractWe describe METEOR, an automaticmetric for machine translation evaluationthat is based on a generalized concept ofunigram matching between the machine-produced translation and human-producedreference translations.
Unigrams can bematched based on their surface forms,stemmed forms, and meanings; further-more, METEOR can be easily extended toinclude more advanced matching strate-gies.
Once all generalized unigrammatches between the two strings havebeen found, METEOR computes a scorefor this matching using a combination ofunigram-precision, unigram-recall, and ameasure of fragmentation that is designedto directly capture how well-ordered thematched words in the machine translationare in relation to the reference.
Weevaluate METEOR by measuring the cor-relation between the metric scores andhuman judgments of translation quality.We compute the Pearson R correlationvalue between its scores and human qual-ity assessments of the LDC TIDES 2003Arabic-to-English and Chinese-to-Englishdatasets.
We perform segment-by-segment correlation, and show thatMETEOR gets an R correlation value of0.347 on the Arabic data and 0.331 on theChinese data.
This is shown to be an im-provement on using simply unigram-precision, unigram-recall and their har-monic F1 combination.
We also performexperiments to show the relative contribu-tions of the various mapping modules.1 IntroductionAutomatic Metrics for machine translation (MT)evaluation have been receiving significant atten-tion in the past two years, since IBM's BLEU met-ric was proposed and made available (Papineni etal 2002).
BLEU and the closely related NIST met-ric (Doddington, 2002) have been extensively usedfor comparative evaluation of the various MT sys-tems developed under the DARPA TIDES researchprogram, as well as by other MT researchers.
Theutility and attractiveness of automatic metrics forMT evaluation has consequently been widely rec-ognized by the MT community.
Evaluating an MTsystem using such automatic metrics is muchfaster, easier and cheaper compared to humanevaluations, which require trained bilingual evalua-tors.
In addition to their utility for comparing theperformance of different systems on a commontranslation task, automatic metrics can be appliedon a frequent and ongoing basis during system de-velopment, in order to guide the development ofthe system based on concrete performance im-provements.Evaluation of Machine Translation has tradi-tionally been performed by humans.
While themain criteria that should be taken into account inassessing the quality of MT output are fairly intui-tive and well established, the overall task of MTevaluation is both complex and task dependent.MT evaluation has consequently been an area ofsignificant research in itself over the years.
A widerange of assessment measures have been proposed,not all of which are easily quantifiable.
Recentlydeveloped frameworks, such as FEMTI (King et al2003), are attempting to devise effective platformsfor combining multi-faceted measures for MTevaluation in effective and user-adjustable ways.While a single one-dimensional numeric metriccannot hope to fully capture all aspects of MT65evaluation, such metrics are still of great value andutility.In order to be both effective and useful, anautomatic metric for MT evaluation has to satisfyseveral basic criteria.
The primary and most intui-tive requirement is that the metric have very highcorrelation with quantified human notions of MTquality.
Furthermore, a good metric should be assensitive as possible to differences in MT qualitybetween different systems, and between differentversions of the same system.
The metric should beconsistent (same MT system on similar textsshould produce similar scores), reliable (MT sys-tems that score similarly can be trusted to performsimilarly) and general (applicable to different MTtasks in a wide range of domains and scenarios).Needless to say, satisfying all of the above criteriais extremely difficult, and all of the metrics thathave been proposed so far fall short of adequatelyaddressing most if not all of these requirements.Nevertheless, when appropriately quantified andconverted into concrete test measures, such re-quirements can set an overall standard by whichdifferent MT evaluation metrics can be comparedand evaluated.In this paper, we describe METEOR1, an auto-matic metric for MT evaluation which we havebeen developing.
METEOR was designed to ex-plicitly address several observed weaknesses inIBM's BLEU metric.
It is based on an explicitword-to-word matching between the MT outputbeing evaluated and one or more reference transla-tions.
Our current matching supports not onlymatching between words that are identical in thetwo strings being compared, but can also matchwords that are simple morphological variants ofeach other (i.e.
they have an identical stem), andwords that are synonyms of each other.
We envi-sion ways in which this strict matching can be fur-ther expanded in the future, and describe these atthe end of the paper.
Each possible matching isscored based on a combination of several features.These currently include unigram-precision, uni-gram-recall, and a direct measure of how out-of-order the words of the MT output are with respectto the reference.
The score assigned to each indi-vidual sentence of MT output is derived from thebest scoring match among all matches over all ref-erence translations.
The maximal-scoring match-1 METEOR: Metric for Evaluation of Translation with Explicit ORderinging is then also used in order to calculate an aggre-gate score for the MT system over the entire testset.
Section 2 describes the metric in detail, andprovides a full example of the matching and scor-ing.In previous work (Lavie et al, 2004), we com-pared METEOR with IBM's BLEU metric and it?sderived NIST metric, using several empiricalevaluation methods that have been proposed in therecent literature as concrete means to assess thelevel of correlation of automatic metrics and hu-man judgments.
We demonstrated that METEORhas significantly improved correlation with humanjudgments.
Furthermore, our results demonstratedthat recall plays a more important role than preci-sion in obtaining high-levels of correlation withhuman judgments.
The previous analysis focusedon correlation with human judgments at the systemlevel.
In this paper, we focus our attention on im-proving correlation between METEOR score andhuman judgments at the segment level.
High-levelsof correlation at the segment level are importantbecause they are likely to yield a metric that is sen-sitive to minor differences between systems and tominor differences between different versions of thesame system.
Furthermore, current levels of corre-lation at the sentence level are still rather low, of-fering a very significant space for improvement.The results reported in this paper demonstrate thatall of the individual components included withinMETEOR contribute to improved correlation withhuman judgments.
In particular, METEOR isshown to have statistically significant better corre-lation compared to unigram-precision, unigram-recall and the harmonic F1 combination of the two.We are currently in the process of exploringseveral further enhancements to the currentMETEOR metric, which we believe have the po-tential to significantly further improve the sensitiv-ity of the metric and its level of correlation withhuman judgments.
Our work on these directions isdescribed in further detail in Section 4.2 The METEOR Metric2.1 Weaknesses in BLEU Addressed inMETEORThe main principle behind IBM?s BLEU metric(Papineni et al 2002) is the measurement of the66overlap in unigrams (single words) and higher or-der n-grams of words, between a translation beingevaluated and a set of one or more reference trans-lations.
The main component of BLEU is n-gramprecision: the proportion of the matched n-gramsout of the total number of n-grams in the evaluatedtranslation.
Precision is calculated separately foreach n-gram order, and the precisions are com-bined via a geometric averaging.
BLEU does nottake recall into account directly.
Recall ?
the pro-portion of the matched n-grams out of the totalnumber of n-grams in the reference translation, isextremely important for assessing the quality ofMT output, as it reflects to what degree the transla-tion covers the entire content of the translated sen-tence.
BLEU does not use recall because thenotion of recall is unclear when matching simulta-neously against a set of reference translations(rather than a single reference).
To compensate forrecall, BLEU uses a Brevity Penalty, which penal-izes translations for being ?too short?.
The NISTmetric is conceptually similar to BLEU in mostaspects, including the weaknesses discussed below.BLEU and NIST suffer from several weak-nesses, which we attempt to address explicitly inour proposed METEOR metric:The Lack of Recall:  We believe that the fixedbrevity penalty in BLEU does not adequately com-pensate for the lack of recall.
Our experimentalresults strongly support this claim.Use of Higher Order N-grams: Higher orderN-grams are used in BLEU as an indirect measureof a translation?s level of grammatical well-formedness.
We believe an explicit measure forthe level of grammaticality (or word order) canbetter account for the importance of grammatical-ity as a factor in the MT metric, and result in bettercorrelation with human judgments of translationquality.Lack of Explicit Word-matching BetweenTranslation and Reference:  N-gram counts don?trequire an explicit word-to-word matching, but thiscan result in counting incorrect ?matches?, particu-larly for common function words.Use of Geometric Averaging of N-grams:Geometric averaging results in a score of ?zero?whenever one of the component n-gram scores iszero.
Consequently, BLEU scores at the sentence(or segment) level can be meaningless.
AlthoughBLEU was intended to be used only for aggregatecounts over an entire test-set (and not at the sen-tence level), scores at the sentence level can beuseful indicators of the quality of the metric.
Inexperiments we conducted, a modified version ofBLEU that uses equal-weight arithmetic averagingof n-gram scores was found to have better correla-tion with human judgments.2.2 The METEOR MetricMETEOR was designed to explicitly address theweaknesses in BLEU identified above.
It evaluatesa translation by computing a score based on ex-plicit word-to-word matches between the transla-tion and a reference translation.
If more than onereference translation is available, the given transla-tion is scored against each reference independ-ently, and the best score is reported.
This isdiscussed in more detail later in this section.Given a pair of translations to be compared (asystem translation and a reference translation),METEOR creates an alignment between the twostrings.
We define an alignment as a mapping be-tween unigrams, such that every unigram in eachstring maps to zero or one unigram in the otherstring, and to no unigrams in the same string.
Thusin a given alignment, a single unigram in one stringcannot map to more than one unigram in the otherstring.
This alignment is incrementally producedthrough a series of stages, each stage consisting oftwo distinct phases.In the first phase an external module lists all thepossible unigram mappings between the twostrings.
Thus, for example, if the word ?computer?occurs once in the system translation and twice inthe reference translation, the external module liststwo possible unigram mappings, one mapping theoccurrence of ?computer?
in the system translationto the first occurrence of ?computer?
in the refer-ence translation, and another mapping it to the sec-ond occurrence.
Different modules map unigramsbased on different criteria.
The ?exact?
modulemaps two unigrams if they are exactly the same(e.g.
?computers?
maps to ?computers?
but not?computer?).
The ?porter stem?
module maps twounigrams if they are the same after they arestemmed using the Porter stemmer (e.g.
: ?com-puters?
maps to both ?computers?
and to ?com-puter?).
The ?WN synonymy?
module maps twounigrams if they are synonyms of each other.In the second phase of each stage, the largestsubset of these unigram mappings is selected such67that the resulting set constitutes an alignment asdefined above (that is, each unigram must map toat most one unigram in the other string).
If morethan one subset constitutes an alignment, and alsohas the same cardinality as the largest set,METEOR selects that set that has the least numberof unigram mapping crosses.
Intuitively, if the twostrings are typed out on two rows one above theother, and lines are drawn connecting unigramsthat are mapped to each other, each line crossing iscounted as a ?unigram mapping cross?.
Formally,two unigram mappings (ti, rj) and (tk, rl) (where tiand tk are unigrams in the system translationmapped to unigrams rj and rl in the reference trans-lation respectively) are said to cross if and only ifthe following formula evaluates to a negativenumber:(pos(ti) ?
pos(tk)) * (pos(rj) ?
pos(rl))where pos(tx) is the numeric position of the uni-gram tx in the system translation string, and pos(ry)is the numeric position of the unigram ry in the ref-erence string.
For a given alignment, every pair ofunigram mappings is evaluated as a cross or not,and the alignment with the least total crosses isselected in this second phase.
Note that these twophases together constitute a variation of the algo-rithm presented in (Turian et al 2003).Each stage only maps unigrams that have notbeen mapped to any unigram in any of the preced-ing stages.
Thus the order in which the stages arerun imposes different priorities on the mappingmodules employed by the different stages.
That is,if the first stage employs the ?exact?
mappingmodule and the second stage employs the ?porterstem?
module, METEOR is effectively preferringto first map two unigrams based on their surfaceforms, and performing the stemming only if thesurface forms do not match (or if the mappingbased on surface forms was too ?costly?
in termsof the total number of crosses).
Note thatMETEOR is flexible in terms of the number ofstages, the actual external mapping module usedfor each stage, and the order in which the stagesare run.
By default the first stage uses the ?exact?mapping module, the second the ?porter stem?module and the third the ?WN synonymy?
module.In section 4 we evaluate each of these configura-tions of METEOR.Once all the stages have been run and a finalalignment has been produced between the systemtranslation and the reference translation, theMETEOR score for this pair of translations iscomputed as follows.
First unigram precision (P)is computed as the ratio of the number of unigramsin the system translation that are mapped (to uni-grams in the reference translation) to the total num-ber of unigrams in the system translation.Similarly, unigram recall (R) is computed as theratio of the number of unigrams in the systemtranslation that are mapped (to unigrams in the ref-erence translation) to the total number of unigramsin the reference translation.
Next we computeFmean by combining the precision and recall via aharmonic-mean (van Rijsbergen, 1979) that placesmost of the weight on recall.
We use a harmonicmean of P and 9R.
The resulting formula used is:PRPRFmean910+=Precision, recall and Fmean are based on uni-gram matches.
To take into account longermatches, METEOR computes a penalty for a givenalignment as follows.
First, all the unigrams in thesystem translation that are mapped to unigrams inthe reference translation are grouped into the few-est possible number of chunks such that the uni-grams in each chunk are in adjacent positions inthe system translation, and are also mapped to uni-grams that are in adjacent positions in the referencetranslation.
Thus, the longer the n-grams, the fewerthe chunks, and in the extreme case where the en-tire system translation string matches the referencetranslation there is only one chunk.
In the otherextreme, if there are no bigram or longer matches,there are as many chunks as there are unigrammatches.
The penalty is then computed through thefollowing formula:3_##*5.0 ???????
?=matchedunigramschunksPenaltyFor example, if the system translation was ?thepresident spoke to the audience?
and the referencetranslation was ?the president then spoke to theaudience?, there are two chunks: ?the president?and ?spoke to the audience?.
Observe that the pen-alty increases as the number of chunks increases toa maximum of 0.5.
As the number of chunks goesto 1, penalty decreases, and its lower bound is de-cided by the number of unigrams matched.
Theparameters if this penalty function were deter-mined based on some experimentation with de-68veopment data, but have not yet been trained to beoptimal.Finally, the METEOR Score for the givenalignment is computed as follows:)1(* PenaltyFmeanScore ?=This has the effect of reducing the Fmean by themaximum of 50% if there are no bigram or longermatches.For a single system translation, METEOR com-putes the above score for each reference transla-tion, and then reports the best score as the score forthe translation.
The overall METEOR score for asystem is calculated based on aggregate statisticsaccumulated over the entire test set, similarly tothe way this is done in BLEU.
We calculate ag-gregate precision, aggregate recall, an aggregatepenalty, and then combine them using the sameformula used for scoring individual segments.3 Evaluation of the METEOR Metric3.1.
DataWe evaluated the METEOR metric and comparedits performance with BLEU and NIST on theDARPA/TIDES 2003 Arabic-to-English and Chi-nese-to-English MT evaluation data releasedthrough the LDC as a part of the workshop on In-trinsic and Extrinsic Evaluation Measures for MTand/or Summarization, at the Annual Meeting ofthe Association of Computational Linguistics(2005).
The Chinese data set consists of 920 sen-tences, while the Arabic data set consists of 664sentences.
Each sentence has four reference trans-lations.
Furthermore, for 7 systems on the Chinesedata and 6 on the Arabic data, every sentencetranslation has been assessed by two separate hu-man judges and assigned an Adequacy and a Flu-ency Score.
Each such score ranges from one tofive (with one being the poorest grade and five thehighest).
For this paper, we computed a CombinedScore for each translation by averaging the ade-quacy and fluency scores of the two judges for thattranslation.
We also computed an average SystemScore for each translation system by averaging theCombined Score for all the translations producedby that system.
(Note that although we refer tothese data sets as the ?Chinese?
and the ?Arabic?data sets, the MT evaluation systems analyzed inthis paper only evaluate English sentences pro-duced by translation systems by comparing them toEnglish reference sentences).3.2 Comparison with BLEU and NIST MTEvaluation AlgorithmsIn this paper, we are interested in evaluatingMETEOR as a metric that can evaluate translationson a sentence-by-sentence basis, rather than on acoarse grained system-by-system basis.
The stan-dard metrics ?
BLEU and NIST ?
were howeverdesigned for system level scoring, hence comput-ing sentence level scores using BLEU or the NISTevaluation mechanism is unfair to those algo-rithms.
To provide a point of comparison however,table 1 shows the system level correlation betweenhuman judgments and various MT evaluation algo-rithms and sub components of METEOR over theChinese portion of the Tides 2003 dataset.
Specifi-cally, these correlation figures were obtained asfollows: Using each algorithm we computed onescore per Chinese system by calculating the aggre-gate scores produced by that algorithm for that sys-tem.
We also obtained the overall human judgmentfor each system by averaging all the human scoresfor that system?s translations.
We then computedthe Pearson correlation between these system levelhuman judgments and the system level scores foreach algorithm; these numbers are presented intable 1.System ID CorrelationBLEU 0.817NIST 0.892Precision 0.752Recall 0.941F1 0.948Fmean 0.952METEOR 0.964Table 1: Comparison of human/METEOR correlationwith BLEU and NIST/human correlationsObserve that simply using Recall as the MTevaluation metric results in a significant improve-ment in correlation with human judgment overboth the BLEU and the NIST algorithms.
Thesecorrelations further improve slightly when preci-sion is taken into account (in the F1 measure),69when the recall is weighed more heavily than pre-cision (in the Fmean measure) and when a penaltyis levied for fragmented matches (in the mainMETEOR measure).3.3 Evaluation MethodologyAs mentioned in the previous section, our maingoal in this paper is to evaluate METEOR and itscomponents on their translation-by-translationlevel correlation with human judgment.
Towardsthis end, in the rest of this paper, our evaluationmethodology is as follows: For each system, wecompute the METEOR Score for every translationproduced by the system, and then compute the cor-relation between these individual scores and thehuman assessments (average of the adequacy andfluency scores) for the same translations.
Thus weget a single Pearson R value for each system forwhich we have human assessments.
Finally weaverage the R values of all the systems for each ofthe two language data sets to arrive at the overallaverage correlation for the Chinese dataset and theArabic dataset.
This number ranges between -1.0(completely negatively correlated) to +1.0 (com-pletely positively correlated).We compare the correlation between human as-sessments and METEOR Scores produced abovewith that between human assessments and preci-sion, recall and Fmean scores to show the advan-tage of the various components in the METEORscoring function.
Finally we run METEOR usingdifferent mapping modules, and compute the corre-lation as described above for each configuration toshow the effect of each unigram mapping mecha-nism.3.4 Correlation between METEOR Scoresand Human AssessmentsSystem ID Correlationame 0.331ara 0.278arb 0.399ari 0.363arm 0.341arp 0.371Average 0.347Table 2: Correlation between METEOR Scores andHuman Assessments for the Arabic DatasetWe computed sentence by sentence correlationbetween METEOR Scores and human assessments(average of adequacy and fluency scores) for eachtranslation for every system.
Tables 2 and 3 showthe Pearson R correlation values for each system,as well as the average correlation value per lan-guage dataset.System ID CorrelationE09 0.385E11 0.299E12 0.278E14 0.307E15 0.306E17 0.385E22 0.355Average 0.331Table 3: Correlation between METEOR Scores andHuman Assessments for the Chinese Dataset3.5 Comparison with Other MetricsWe computed translation by translation correla-tions between human assessments and other met-rics besides the METEOR score, namely precision,recall and Fmean.
Tables 4 and 5 show the correla-tions for the various scores.Metric CorrelationPrecision 0.287Recall 0.334Fmean 0.340METEOR 0.347Table 4: Correlations between human assessments andprecision, recall, Fmean and METEOR Scores, aver-aged over systems in the Arabic datasetMetric CorrelationPrecision 0.286Recall 0.320Fmean 0.327METEOR 0.331Table 5: Correlations between human assessments andprecision, recall, Fmean and METEOR Scores, aver-aged over systems in the Chinese datasetWe observe that recall by itself correlates withhuman assessment much better than precision, andthat combining the two using the Fmean formula70described above results in further improvement.
Bypenalizing the Fmean score using the chunk countwe get some further marginal improvement in cor-relation.3.6 Comparison between Different Map-ping ModulesTo observe the effect of various unigram mappingmodules on the correlation between the METEORscore and human assessments, we ran METEORwith different sequences of stages with differentmapping modules in them.
In the first experimentwe ran METEOR with only one stage that used the?exact?
mapping module.
This module matchesunigrams only if their surface forms match.
(Thismodule does not match unigrams that belong to alist of ?stop words?
that consist mainly of functionwords).
In the second experiment we ranMETEOR with two stages, the first using the ?ex-act?
mapping module, and the second the ?Porter?mapping module.
The Porter mapping modulematches two unigrams to each other if they areidentical after being passed through the Porterstemmer.
In the third experiment we replaced thePorter mapping module with the WN-Stem map-ping module.
This module maps two unigrams toeach other if they share the same base form inWordNet.
This can be thought of as a differentkind of stemmer ?
the difference from the Porterstemmer is that the word stems are actual wordswhen stemmed through WordNet in this manner.In the last experiment we ran METEOR with threestages, the first two using the exact and the Portermodules, and the third the WN-Synonymy map-ping module.
This module maps two unigramstogether if at least one sense of each word belongsto the same synset in WordNet.
Intuitively, thisimplies that at least one sense of each of the twowords represent the same concept.
This can bethought of as a poor-man?s synonymy detectionalgorithm that does not disambiguate the wordsbeing tested for synonymy.
Note that theMETEOR scores used to compute correlations inthe other tables (1 through 4) used exactly this se-quence of stages.Tables 6 and 7 show the correlations betweenMETEOR scores produced in each of these ex-periments and human assessments for both theArabic and the Chinese datasets.
On both data sets,adding either stemming modules to simply usingthe exact matching improves correlations.
Somefurther improvement in correlation is produced byadding the synonymy module.Mapping module sequenceused (Arabic)CorrelationExact 0.312Exact, Porter 0.329Exact, WN-Stem 0.330Exact, Porter, WN-Synonym 0.347Table 6: Comparing correlations produced by differentmodule stages on the Arabic dataset.Mapping module sequenceused (Chinese)CorrelationExact 0.293Exact, Porter 0.318Exact, WN-Stem 0.312Exact, Porter, WN-Synonym 0.331Table 7: Comparing correlations produced by differentmodule stages, on the Chinese dataset3.7 Correlation using Normalized HumanAssessment ScoresOne problem with conducting correlation ex-periments with human assessment scores at thesentence level is that the human scores are noisy ?that is, the levels of agreement between humanjudges on the actual sentence level assessmentscores is not extremely high.
To partially addressthis issue, the human assessment scores were nor-malized by a group at the MITRE Corporation.
Tosee the effect of this noise on the correlation, wecomputed the correlation between the METEORScore (computed using the stages used in the 4thexperiment in section 7 above) and both the rawhuman assessments as well as the normalized hu-man assessments.Arabic DatasetChineseDatasetRaw human as-sessments 0.347 0.331Normalized hu-man assessments 0.403 0.365Table 8: Comparing correlations between METEORScores and both raw and normalized human assessments71Table 8 shows that indeed METEOR Scores cor-relate better with normalized human assessments.In other words, the noise in the human assessmentshurts the correlations between automatic scoresand human assessments.4 Future WorkThe METEOR metric we described and evaluatedin this paper, while already demonstrating greatpromise, is still relatively simple and na?ve.
Weare in the process of enhancing the metric and ourexperimentation in several directions:Train the Penalty and Score Formulas onData: The formulas for Penalty and METEORscore were manually crafted based on empiricaltests on a separate set of development data.
How-ever, we plan to optimize the formulas by trainingthem on a separate data set, and choosing that for-mula that best correlates with human assessmentson the training data.Use Semantic Relatedness to Map Unigrams:So far we have experimented with exact mapping,stemmed mapping and synonymy mapping be-tween unigrams.
Our next step is to experimentwith different measures of semantic relatedness tomatch unigrams that have a related meaning, butare not quite synonyms of each other.More Effective Use of Multiple ReferenceTranslations:  Our current metric uses multiplereference translations in a weak way: we comparethe translation with each reference separately andselect the reference with the best match.
This wasnecessary in order to incorporate recall in our met-ric, which we have shown to be highly advanta-geous.
As our matching approach improves, theneed for multiple references for the metric may infact diminish.
Nevertheless, we are exploringways in which to improve our matching againstmultiple references.
Recent work by (Pang et al2003) provides the mechanism for producing se-mantically meaningful additional ?synthetic?
refer-ences from a small set of real references.
We planto explore whether using such synthetic referencescan improve the performance of our metric.Weigh Matches Produced by Different Mod-ules Differently: Our current multi-stage approachprefers metric imposes a priority on the differentmatching modules.
However, once all the stageshave been run, unigrams mapped through differentmapping modules are treated the same.
Anotherapproach to treating different mappings differentlyis to apply different weights to the mappings pro-duced by different mapping modules.
Thus ?com-puter?
may match ?computer?
with a score of 1,?computers?
with a score of 0.8 and ?workstation?with a score of 0.3.
As future work we plan to de-velop a version of METEOR that uses suchweighting schemes.AcknowledgementsWe acknowledge Kenji Sagae and ShyamsundarJayaraman for their work on the METEOR system.We also wish to thank John Henderson and Wil-liam Morgan from MITRE for providing us withthe normalized human judgment scores used forthis work.ReferencesGeorge Doddington.
2002.
Automatic Evaluation ofMachine Translation Quality using N-gram Co-occurrence Statistics.
In Proceedings of 2nd HumanLanguage Technologies Conference (HLT-02).
SanDiego, CA.
pp.
128-132.Margaret King, Andrei Popescu-Belis and EduardHovy.
2003.
FEMTI: Creating and Using a Frame-work for MT Evaluation.
In Proceedings of MTSummit IX, New Orleans, LA.
Sept. 2003. pp.
224-231.Alon Lavie, Kenji Sagae and Shyamsundar Jayaraman,2004.
The Significance of Recall in Automatic Met-rics for MT Evaluation.
In Proceedings of AMTA-2004, Washington DC.
September 2004.Bo Pang, Kevin Knight and Daniel Marcu.
2003.
Syn-tax-based Alignment of Multiple Translations: Ex-tracting Paraphrases and Generating NewSentences.
In Proceedings of HLT-NAACL 2003.Edmonton, Canada.
May 2003.Kishore Papineni, Salim Roukos, Todd Ward and Wei-Jing Zhu.
2002.
BLEU: a Method for AutomaticEvaluation of Machine Translation.
In Proceedingsof the 40th Annual Meeting of the Association forComputational Linguistics (ACL-02).
Philadelphia,PA.
July 2002. pp.
311-318.Joseph P. Turian, Luke Shen and I. Dan Melamed.2003.
Evaluation of Machine Translation and itsEvaluation.
In Proceedings of MT Summit IX, NewOrleans, LA.
Sept. 2003.  pp.
386-393.C.
van Rijsbergen.
1979.
Information Retrieval.
But-terworths.
London, England.
2nd Edition.72
