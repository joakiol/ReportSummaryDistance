Proceedings of the 43rd Annual Meeting of the ACL, pages 125?132,Ann Arbor, June 2005. c?2005 Association for Computational LinguisticsInducing Ontological Co-occurrence VectorsPatrick PantelInformation Sciences InstituteUniversity of Southern California4676 Admiralty WayMarina del Rey, CA  90292pantel@isi.eduAbstractIn this paper, we present an unsupervisedmethodology for propagating lexical co-occurrence vectors into an ontology suchas WordNet.
We evaluate the frameworkon the task of automatically attaching newconcepts into the ontology.
Experimentalresults show 73.9% attachment accuracyin the first position and 81.3% accuracy inthe top-5 positions.
This framework couldpotentially serve as a foundation for on-tologizing lexical-semantic resources andassist the development of other large-scale and internally consistent collectionsof semantic information.1 IntroductionDespite considerable effort, there is still today nocommonly accepted semantic corpus, semanticframework, notation, or even agreement on pre-cisely which aspects of semantics are most useful(if at all).
We believe that one important reasonfor this rather startling fact is the absence of trulywide-coverage semantic resources.Recognizing this, some recent work on widecoverage term banks, like WordNet (Miller 1990)and CYC (Lenat 1995), and annotated corpora,like FrameNet (Baker et al 1998), Propbank(Kingsbury et al 2002) and Nombank (Meyers etal.
2004), seeks to address the problem.
But man-ual efforts such as these suffer from two draw-backs: they are difficult to tailor to new domains,and they have internal inconsistencies that canmake automating the acquisition process difficult.In this work, we introduce a general frame-work for inducing co-occurrence feature vectorsfor nodes in a WordNet-like ontology.
We be-lieve that this framework will be useful for a va-riety of applications, including adding additionalsemantic information to existing semantic termbanks by disambiguating lexical-semantic re-sources.Ontologizing semantic resourcesRecently, researchers have applied text- andweb-mining algorithms for automatically creatinglexical semantic resources like similarity lists(Lin 1998), semantic lexicons (Riloff and Shep-herd 1997), hyponymy lists (Shinzato and Tori-sawa 2004; Pantel and Ravichandran 2004), part-whole lists (Girgu et al 2003), and verb relationgraphs (Chklovski and Pantel 2004).
However,none of these resources have been directly linkedinto an ontological framework.
For example, inVERBOCEAN (Chklovski and Pantel 2004), wefind the verb relation ?to surpass is-stronger-thanto hit?, but it is not specified that it is the achiev-ing sense of hit where this relation applies.We term ontologizing a lexical-semantic re-source as the task of sense disambiguating the re-source.
This problem is different but notorthogonal to word-sense disambiguation.
If wecould disambiguate large collections of text withhigh accuracy, then current methods for buildinglexical-semantic resources could easily be appliedto ontologize them by treating each word?s sensesas separate words.
Our method does not requirethe disambiguation of text.
Instead, it relies on theprinciple of distributional similarity and thatpolysemous words that are similar in one senseare dissimilar in their other senses.125Given the enriched ontologies produced byour method, we believe that ontologizing lexical-semantic resources will be feasible.
For example,consider the example verb relation ?to surpass is-stronger-than to hit?
from above.
To disambigu-ate the verb hit, we can look at all other verbs thatto surpass is stronger than (for example, inVERBOCEAN, ?to surpass is-stronger-than toovertake?
and ?to surpass is-stronger-than toequal?).
Now, we can simply compare the lexicalco-occurrence vectors of overtake and equal withthe ontological feature vectors of the senses of hit(which are induced by our framework).
The sensewhose feature vector is most similar is selected.It remains to be seen in future work how wellthis approach performs on ontologizing varioussemantic resources.
In this paper, we focus on thegeneral framework for inducing the ontologicalco-occurrence vectors and we apply it to the taskof linking new terms into the ontology.2 Relevant workOur framework aims at enriching WordNet-likeontologies with syntactic features derived from anon-annotated corpus.
Others have also madesignificant additions to WordNet.
For example, ineXtended WordNet (Harabagiu et al 1999), therich glosses in WordNet are enriched by disam-biguating the nouns, verbs, adverbs, and adjec-tives with synsets.
Another work has enrichedWordNet synsets with topically related words ex-tracted from the Web (Agirre et al 2001).
Whilethis method takes advantage of the redundancy ofthe web, our source of information is a localdocument collection, which opens the possibilityfor domain specific applications.Distributional approaches to building semanticrepositories have shown remarkable power.
Theunderlying assumption, called the DistributionalHypothesis (Harris 1985), links the semantics ofwords to their lexical and syntactic behavior.
Thehypothesis states that words that occur in thesame contexts tend to have similar meaning.
Re-searchers have mostly looked at representingwords by their surrounding words (Lund and Bur-gess 1996) and by their syntactical contexts(Hindle 1990; Lin 1998).
However, these repre-sentations do not distinguish between the differ-ent senses of words.
Our framework utilizes theseprinciples and representations to induce disam-biguated feature vectors.
We describe these rep-resentations further in Section 3.In supervised word sense disambiguation,senses are commonly represented by their sur-rounding words in a sense-tagged corpus (Gale etal.
1991).
If we had a large collection of sense-tagged text, then we could extract disambiguatedfeature vectors by collecting co-occurrence fea-tures for each word sense.
However, since there islittle sense-tagged text available, the feature vec-tors for a random WordNet concept would bevery sparse.
In our framework, feature vectors areinduced from much larger untagged corpora (cur-rently 3GB of newspaper text).Another approach to building semantic reposi-tories is to collect and merge existing ontologies.Attempts to automate the merging process havenot been particularly successful (Knight and Luk1994; Hovy 1998; Noy and Musen 1999).
Theprincipal problems of partial and unbalanced cov-erage and of inconsistencies between ontologiescontinue to hamper these approaches.3 ResourcesThe framework we present in Section 4 propa-gates any type of lexical feature up an ontology.In previous work, lexicals have often been repre-sented by proximity and syntactic features.
Con-sider the following sentence:The tsunami left a trail of horror.In a proximity approach, a word is representedby a window of words surrounding it.
For theabove sentence, a window of size 1 would yieldtwo features (-1:the and +1:left) for the word tsu-nami.
In a syntactic approach, more linguisticallyrich features are extracted by using each gram-matical relation in which a word is involved (e.g.the features for tsunami are determiner:the andsubject-of:leave).For the purposes of this work, we consider thepropagation of syntactic features.
We used Mini-par (Lin 1994), a broad coverage parser, to ana-lyze text.
We collected the statistics on thegrammatical relations (contexts) output by Mini-par and used these as the feature vectors.
Follow-ing Lin (1998), we measure each feature f for aword e not by its frequency but by its pointwisemutual information, mief:126( )( ) ( )fPePfePmief ?=,log4 Inducing ontological featuresThe resource described in the previous sectionyields lexical feature vectors for each word in acorpus.
We term these vectors lexical becausethey are collected by looking only at the lexicalsin the text (i.e.
no sense information is used).
Weuse the term ontological feature vector to refer toa feature vector whose features are for a particu-lar sense of the word.In this section, we describe our framework forinducing ontological feature vectors for eachnode of an ontology.
Our approach employs twophases.
A divide-and-conquer algorithm firstpropagates syntactic features to each node in theontology.
A final sweep over the ontology, whichwe call the Coup phase, disambiguates the featurevectors of lexicals (leaf nodes) in the ontology.4.1 Divide-and-conquer phaseIn the first phase of the algorithm, we propagatefeatures up the ontology in a bottom-up approach.Figure 1 gives an overview of this phase.The termination condition of the recursion ismet when the algorithm processes a leaf node.The feature vector that is assigned to this node isan exact copy of the lexical feature vector for thatleaf (obtained from a large corpus as described inSection 3).
For example, for the two leaf nodeslabeled chair in Figure 2, we assign to both thesame ambiguous lexical feature vector, an excerptof which is shown in Figure 3.When the recursion meets a non-leaf node,like chairwoman in Figure 2, the algorithm firstrecursively applies itself to each of the node?schildren.
Then, the algorithm selects those fea-tures common to its children to propagate up toits own ontological feature vector.
The assump-tion here is that features of other senses ofpolysemous words will not be propagated sincethey will not be common across the children.
Be-low, we describe the two methods we used topropagate features: Shared and Committee.Shared propagation algorithmThe first technique for propagating features to aconcept node n from its children C is the simplestand scored best in our evaluation (see Section5.2).
The goal is that the feature vector for nInput: A node n and a corpus C.Step 1: Termination Condition:If n is a leaf node then assign to n its lexicalfeature vector as described in Section 3.Step 2: Recursion Step:For each child c of n, reecurse on c and C.Assign a feature vector to n by propagatingfeatures from its children.Output: A feature vector assigned to each node of thetree rooted by n.Figure 1.
Divide-and-conquer phase.chair stool armchairchaise-longuetaboret musicstoolstepstoolcuttystooldeskchairchairseatingfurniturefurniturefurniture bedmirror tableconceptleaf nodeLegend:chair chairman president chair-womanvicechairmanvicechairmanchair-womanleaderDecom-posableobjectFigure 2.
Subtrees of WordNet illustrating two sensesof chair.
"chair"conjunction:sofa 77 11.8professor 11 6.0dining room 2 5.6cushion 1 4.5council member 1 4.4President 9 2.9foreign minister 1 2.8nominal subjectOttoman 8 12.1director 22 9.1speaker 8 8.6Joyner 2 8.22recliner 2 7.7candidate 1 3.5Figure 3.
Excerpt of a lexical feature vector for theword chair.
Grammatical relations are in italics (con-junction and nominal-subject).
The first column ofnumbers are frequency counts and the other are mutualinformation scores.
In bold are the features that inter-sect with the induced ontological feature vector for theparent concept of chair?s chairwoman sense.127represents the general grammatical behavior thatits children will have.
For example, for the con-cept node furniture in Figure 2, we would like toassign features like object-of:clean sincemosttypes of furniture can be cleaned.
However,even though you can eat on a table, we do notwant the feature on:eat for the furniture conceptsince we do not eat on mirrors or beds.In the Shared propagation algorithm, wepropagate only those features that are shared by atleast t children.
In our experiments, we experi-mentally set t = min(3, |C|).The frequency of a propagated feature is ob-tained by taking a weighted sum of the frequencyof the feature across its children.
Let fi be the fre-quency of the feature for child i, let ci be the totalfrequency of child i, and let N be the total fre-quency of all children.
Then, the frequency f ofthe propagated feature is given by:?
?=iii Ncff  (1)Committee propagation algorithmThe second propagation algorithm finds a set ofrepresentative children from which to propagatefeatures.
Pantel and Lin (2002) describe an algo-rithm, called Clustering By Committee (CBC),which discovers clusters of words according totheir meanings in test.
The key to CBC is findingfor each class a set of representative elements,called a committee, which most unambiguouslydescribe the members of the class.
For example,for the color concept, CBC discovers the follow-ing committee members:purple, pink, yellow, mauve, turquoise,beige, fuchsiaWords like orange and violet are avoided be-cause they are polysemous.
For a given concept c,we build a committee by clustering its childrenaccording to their similarity and then keep thelargest and most interconnected cluster (seePantel and Lin (2002) for details).The propagated features are then those that areshared by at least two committee members.
Thefrequency of a propagated feature is obtained us-ing Eq.
1 where the children i are chosen onlyamong the committee members.Generating committees using CBC works bestfor classes with many members.
In its originalapplication (Pantel and Lin 2002), CBC discov-ered a flat list of coarse concepts.
In the finergrained concept hierarchy of WordNet, there aremany fewer children for each concept so we ex-pect to have more difficulty finding committees.4.2 Coup phaseAt the end of the Divide-and-conquer phase, thenon-leaf nodes of the ontology contain disam-biguated features1.
By design of the propagationalgorithm, each concept node feature is shared byat least two of its children.
We assume that twopolysemous words, w1 and w2, that are similar inone sense will be dissimilar in its other senses.Under the distributional hypothesis, similar wordsoccur in the same grammatical contexts and dis-similar words occur in different grammatical con-texts.
We expect then that most features that areshared between w1 and w2 will be the grammati-cal contexts of their similar sense.
Hence, mostlydisambiguated features are propagated up the on-tology in the Divide-and-conquer phase.However, the feature vectors for the leafnodes remain ambiguous (e.g.
the feature vectorsfor both leaf nodes labeled chair in Figure 2 areidentical).
In this phase of the algorithm, leafnode feature vectors are disambiguated by look-ing at the parents of their other senses.Leaf nodes that are unambiguous in the ontol-ogy will have unambiguous feature vectors.
Forambiguous leaf nodes (i.e.
leaf nodes that havemore than one concept parent), we apply the al-gorithm described in Figure 4.
Given a polyse-mous leaf node n, we remove from its ambiguous1 By disambiguated features, we mean that the featuresare co-occurrences with a particular sense of a word; thefeatures themselves are not sense-tagged.Input: A node n and the enriched ontology O outputfrom the algorithm in Figure 1.Step 1: If n is not a leaf node then return.Step 2: Remove from n?s feature vector all featuresthat intersect with the feature vector of any ofn?s other senses?
parent concepts, but are notin n?s parent concept feature vector.Output: A disambiguated feature vector for each leafnode  n.Figure 4.
Coup phase.128feature vector those features that intersect withthe ontological feature vector of any of its othersenses?
parent concept but that are not in its ownparent?s ontological feature vector.
For example,consider the furniture sense of the leaf node chairin Figure 2.
After the Divide-and-conquer phase,the node chair is assigned the ambiguous lexicalfeature vector shown in Figure 3.
Suppose thatchair only has one other sense in WordNet,which is the chairwoman sense illustrated in Fig-ure 2.
The features in bold in Figure 3 representthose features of chair that intersect with the on-tological feature vector of chairwoman.
In theCoup phase of our system, we remove these boldfeatures from the furniture sense leaf node chair.What remains are features like ?chair and sofa?,?chair and cushion?, ?Ottoman is a chair?, and?recliner is a chair?.
Similarly, for the chair-woman sense of chair, we remove those featuresthat intersect with the ontological feature vectorof the chair concept (the parent of the other chairleaf node).As shown in the beginning of this section,concept node feature vectors are mostly unambi-guous after the Divide-and-conquer phase.
How-ever, the Divide-and-conquer phase may berepeated after the Coup phase using a differenttermination condition.
Instead of assigning to leafnodes ambiguous lexical feature vectors, we usethe leaf node feature vectors from the Coupphase.
In our experiments, we did not see anysignificant performance difference by skippingthis extra Divide-and-conquer step.5 Experimental resultsIn this section, we provide a quantitative andqualitative evaluation of our framework.5.1 Experimental SetupWe used Minipar (Lin 1994), a broad coverageparser, to parse two 3GB corpora (TREC-9 andTREC-2002).
We collected the frequency countsof the grammatical relations (contexts) output byMinipar and used these to construct the lexicalfeature vectors as described in Section 3.WordNet 2.0 served as our testing ontology.Using the algorithm presented in Section 4, weinduced ontological feature vectors for the nounnodes in WordNet using the lexical co-occurrencefeatures from the TREC-2002 corpus.
Due tomemory limitations, we were only able to propa-gate features to one quarter of the ontology.
Weexperimented with both the Shared and Commit-tee propagation models described in Section 4.1.5.2 Quantitative evaluationTo evaluate the resulting ontological feature vec-tors, we considered the task of attaching newnodes into the ontology.
To automatically evalu-ate this, we randomly extracted a set of 1000noun leaf nodes from the ontology and accumu-lated lexical feature vectors for them using theTREC-9 corpus (a separate corpus than the oneused to propagate features, but of the samegenre).
We experimented with two test sets:?
Full: The 424 of the 1000 random nodes thatexisted in the TREC-9 corpus?
Subset: Subset of Full where only nodes that donot have concept siblings are kept (380 nodes).For each random node, we computed the simi-larity of the node with each concept node in theontology by computing the cosine of the angle(Salton and McGill 1983) between the lexicalfeature vector of the random node ei and the onto-logical feature vector of the concept nodes ej:( ) ????
?=ffeffeffefejijijimimimimieesim22,We only kept those similar nodes that had asimilarity above a threshold ?.
We experimentallyset ?
= 0.1.Top-K accuracyWe collected the top-K most similar conceptnodes (attachment points) for each node in thetest sets and computed the accuracy of finding acorrect attachment point in the top-K list.
Table 1shows the result.We expected the algorithm to perform betteron the Subset data set since only concepts thathave exclusively lexical children must be consid-ered for attachment.
In the Full data set, the algo-rithm must consider each concept in the ontologyas a potential attachment point.
However, consid-ering the top-5 best attachments, the algorithmperformed equally well on both data sets.The Shared propagation algorithm performedconsistently slightly better than the Committeemethod.
As described in Section 4.1, building a129committee performs best for concepts with manychildren.
Since many nodes in WordNet have fewdirect children, the Shared propagation method ismore appropriate.
One possible extension of theCommittee propagation algorithm is to find com-mittee members from the full list of descendantsof a node rather than only its immediate children.Precision and RecallWe computed the precision and recall of our sys-tem on varying numbers of returned attachments.Figure 5 and Figure 6 show the attachment preci-sion and recall of our system when the maximumnumber of returned attachments ranges between 1and 5.
In Figure 5, we see that the Shared propa-gation method has better precision than theCommittee method.
Both methods perform simi-larly on recall.
The recall of the system increasesmost dramatically when returning two attach-ments without too much of a hit on precision.
Thelow recall when returning only one attachment isdue to both system errors and also to the fact thatmany nodes in the hierarchy are polysemous.
Inthe next section, we discuss further experimentson polysemous nodes.
Figure 6 illustrates thelarge difference on both precision and recallwhen using the simpler Subset data set.
All 95%confidence bounds in Figure 5 and Figure 6 rangebetween ?2.8% and ?5.3%.Polysemous nodes84 of the nodes in the Full data set are polyse-mous (they are attached to more than one conceptnode in the ontology).
On average, these nodeshave 2.6 senses for a total of 219 senses.
Figure 7compares the precision and recall of the systemon all nodes in the Full data set vs. the 84polysemous nodes.
The 95% confidence intervalsrange between ?3.8% and ?5.0% for the Full dataset and between ?1.2% and ?9.4% for thepolysemous nodes.
The precision on the polyse-mous nodes is consistently better since these havemore possible correct attachments.Clearly, when the system returns at most oneor two attachments, the recall on the polysemousnodes is lower than on the Full set.
However, it isinteresting to note that recall on the polysemousnodes equals the recall on the Full set after K=3.Table 1.
Correct attachment point in the top-K attachments (with 95% conf.
)K Shared (Full) Committee (Full) Shared (Subset) Committee (Subset)1 73.9% ?
4.5% 72.0% ?
4.9% 77.4% ?
3.6% 76.1% ?
5.1%2 78.7% ?
4.1% 76.6% ?
4.2% 80.7% ?
4.0% 79.1% ?
4.5%3 79.9% ?
4.0% 78.2% ?
4.2% 81.2% ?
3.9% 80.5% ?
4.8%4 80.6% ?
4.1% 79.0% ?
4.0% 81.5% ?
4.1% 80.8% ?
5.0%5 81.3% ?
3.8% 79.5% ?
3.9% 81.7% ?
4.1% 81.3% ?
4.9%Figure 5.
Attachment precision and recall for theShared and Committee propagation methods whenreturning at most K attachments (on the Full set).Precision and Recall (Shared and Committee) vs.Number of Returned Attachments0.50.60.70.80.911 2 3 4 5KPrecision (Shared) Recall (Shared)Precision (Committee) Recall (Committee)Precision and Recall (Full and Subset) vs.Number of Returned Attachments0.50.60.70.80.911 2 3 4 5KPrecision (Full) Recall (Full)Precision (Subset) Recall (Subset)Figure 6.
Attachment precision and recall for theFull and Subset data sets when returning at most Kattachments (using the Shared propagation method).1305.3 Qualitative evaluationInspection of errors revealed that the system oftenmakes plausible attachments.
Table 2 showssome example errors generated by our system.For the word arsenic, the system attached it to theconcept trioxide, which is the parent of the cor-rect attachment.The system results may be useful to help vali-date the ontology.
For example, for the word law,the system attached it to the regulation (as an or-ganic process) and ordinance (legislative act)concepts.
According to WordNet, law has sevenpossible attachment points, none of which are alegislative act.
Hence, the system has found thatin the TREC-9 corpus, the word law has a senseof legislative act.
Similarly, the system discov-ered the symptom sense of vomiting.The system discovered a potential anomaly inWordNet with the word slob.
The system classi-fied slob as follows:fool ?
simpleton ?
someonewhereas WordNet classifies it as:vulgarian ?
unpleasant person ?
unwel-come person ?
someoneThe ontology could use this output to verify iffool should link in the unpleasant person subtree.Capitalization is not very trustworthy in largecollections of text.
One of our design decisionswas to ignore the case of words in our corpus,which in turn caused some errors since WordNetis case sensitive.
For example, the lexical nodeMunch (Norwegian artist) was attached to themunch concept (food) by error because our sys-tem accumulated all features of the word Munchin text regardless of its capitalization.6 DiscussionOne question that remains unanswered is howclean an ontology must be in order for our meth-odology to work.
Since the structure of the ontol-ogy guides the propagation of features, a verynoisy ontology will result in noisy feature vec-tors.
However, the framework is tolerant to someamount of noise and can in fact be used to correctsome errors (as shown in Section 5.3).We showed in Section 1 how our frameworkcan be used to disambiguate lexical-semantic re-sources like hyponym lists, verb relations, andunknown words or terms.
Other avenues of futurework include:Adapting/extending existing ontologiesIt takes a large amount of time to build resourceslike WordNet.
However, adapting existing re-sources to a new corpus might be possible usingour framework.
Once we have enriched the on-tology with features from a corpus, we can rear-range the ontological structure according to theinter-conceptual similarity of nodes.
For example,consider the word computer in WordNet, whichhas two senses: a) a machine; and b) a personwho calculates.
In a computer science corpus,sense b) occurs very infrequently and possibly anew sense of computer (e.g.
a processing chip)occurs.
A system could potentially remove senseb) since the similarity of the other children of b)and computer is very low.
It could also uncoverthe new processing chip sense by finding a highsimilarity between computer and the processingchip concept.Validating ontologiesThis is a holy grail problem in the knowledgerepresentation community.
As a small step, ourframework can be used to flag potential anoma-lies to the knowledge engineer.What makes a chair different from a recliner?Given an enriched ontology, we can remove fromthe feature vectors of chair and recliner thosefeatures that occur in their parent furniture con-cept.
The features that remain describe their dif-ferent syntactic behaviors in text.Figure 7.
Attachment precision and recall on theFull set vs. the polysemous nodes in the Full setwhen the system returns at most K attachments.Precision and Recall(All vs. Polysemous Nodes)0.40.50.60.70.80.911 2 3 4 5KPrecision (All) Recall (All)Precision (Polysemous) Recall (Polysemous)1317 ConclusionsWe presented a framework for inducing ontologi-cal feature vectors from lexical co-occurrencevectors.
Our method does not require the disam-biguation of text.
Instead, it relies on the principleof distributional similarity and the fact thatpolysemous words that are similar in one sensetend to be dissimilar in their other senses.
On thetask of attaching new words to WordNet usingour framework, our experiments showed that thefirst attachment has 73.9% accuracy and that acorrect attachment is in the top-5 attachmentswith 81.3% accuracy.We believe this work to be useful for a varietyof applications.
Not only can sense selection taskssuch as word sense disambiguation, parsing, andsemantic analysis benefit from our framework,but more inference-oriented tasks such as ques-tion answering and text summarization as well.We hope that this work will assist with the devel-opment of other large-scale and internally consis-tent collections of semantic information.ReferencesAgirre, E.; Ansa, O.; Martinez, D.; and Hovy, E. 2001.
EnrichingWordNet concepts with topic signatures.
In Proceedings ofthe NAACL workshop on WordNet and Other Lexical Re-sources: Applications, Extensions and Customizations.
Pitts-burgh, PA.Baker, C.; Fillmore, C.; and Lowe, J.
1998.
The Berkeley Fra-meNet project.
In Proceedings of COLING-ACL.
Montreal,Canada.Chklovski, T., and Pantel, P. VERBOCEAN: Mining the Web forFine-Grained Semantic Verb Relations.
In Proceedings ofEMNLP-2004.
pp.
33-40.
Barcelona, Spain.Gale, W.; Church, K.; and Yarowsky, D. 1992.
A method fordisambiguating word senses in a large corpus.
Computers andHumanities, 26:415-439.Girju, R.; Badulescu, A.; and Moldovan, D. 2003.
Learning se-mantic constraints for the automatic discovery of part-wholerelations.
In Proceedings of HLT/NAACL-03.
pp.
80-87.
Ed-monton, Canada.Harabagiu, S.; Miller, G.; and Moldovan, D. 1999.
WordNet 2 -A Morphologically and Semantically Enhanced Resource.
InProceedings of SIGLEX-99.
pp.1-8.
University of Maryland.Harris, Z.
1985.
Distributional structure.
In: Katz, J. J.
(ed.)
ThePhilosophy of Linguistics.
New York: Oxford UniversityPress.
pp.
26-47.Hovy, E. 1998.
Combining and standardizing large-scale, practi-cal ontologies for machine translation and other uses.
In Pro-ceedings LREC-98.
pp.
535-542.
Granada, Spain.Hindle, D. 1990.
Noun classification from predicate-argumentstructures.
In Proceedings of ACL-90.
pp.
268-275.
Pitts-burgh, PA.Kingsbury, P; Palmer, M.; and Marcus, M. 2002.
Adding seman-tic annotation to the Penn TreeBank.
In Proceedings of HLT-2002.
San Diego, California.Knight, K. and Luk, S. K. 1994.
Building a large-scale knowl-edge base for machine translation.
In Proceedings of AAAI-1994.
Seattle, WA.Lenat, D. 1995.
CYC: A large-scale investment in knowledgeinfrastructure.
Communications of the ACM, 38(11):33-38.Lin, D. 1998.
Automatic retrieval and clustering of similarwords.
In Proceedings of COLING/ACL-98.
pp.
768-774.Montreal, Canada.Lin, D. 1994.
Principar - an efficient, broad-coverage, principle-based parser.
Proceedings of COLING-94.
pp.
42-48.
Kyoto,Japan.Lund, K. and Burgess, C. 1996.
Producing high-dimensionalsemantic spaces from lexical co-occurrence.
Behavior Re-search Methods, Instruments, and Computers, 28:203-208.Meyers, A.; Reeves, R.; Macleod, C.; Szekely, R.; Zielinska, V.;Young, B.; and Grishman, R. Annotating noun argumentstructure for NomBank.
In Proceedings of LREC-2004.
Lis-bon, Portugal.Miller, G. 1990.
WordNet: An online lexical database.
Interna-tional Journal of Lexicography, 3(4).Noy, N. F. and Musen, M. A.
1999.
An algorithm for mergingand aligning ontologies: Automation and tool support.
InProceedings of the Workshop on Ontology Management(AAAI-99).
Orlando, FL.Pantel, P. and Lin, D. 2002.
Discovering Word Senses from Text.In Proceedings of SIGKDD-02.
pp.
613-619.
Edmonton, Can-ada.Riloff, E. and Shepherd, J.
1997.
A corpus-based approach forbuilding semantic lexicons.
In Proceedings of EMNLP-1997.Salton, G. and McGill, M. J.
1983.
Introduction to Modern In-formation Retrieval.
McGraw Hill.Shinzato, K. and Torisawa, K. 2004.
Acquiring hyponymy rela-tions from web documents.
In Proceedings of HLT-NAACL-2004.
pp.
73-80.
Boston, MA.Table 2.
Example attachment errors by our system.Node System AttachmentCorrectAttachmentarsenic* trioxide arsenic OR elementlaw regulation law OR police OR ?Munch?
munch Munchslob fool slobvomiting fever emesis* the system?s attachment was a parent of the correct attachment.?
error due to case mix-up (our algorithm does not differentiatebetween case).132
