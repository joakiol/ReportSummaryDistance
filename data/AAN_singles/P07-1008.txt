Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 57?64,Prague, Czech Republic, June 2007. c?2007 Association for Computational LinguisticsMaking Lexical Ontologies Functional and Context-SensitiveTony VealeComputer Science and InformaticsUniversity College DublinIrelandtony.veale@ucd.ieYanfen HaoComputer Science and InformaticsUniversity College DublinIrelandyanfen.hao@ucd.ieAbstractHuman categorization is neither a binary nora context-free process.
Rather, some con-cepts are better examples of a category thanothers, while the criteria for category mem-bership may be satisfied to different degreesby different concepts in different contexts.In light of these empirical facts, WordNet?sstatic category structure appears both exces-sively rigid and unduly fragile for process-ing real texts.
In this paper we describe asyntagmatic, corpus-based approach to re-defining WordNet?s categories in a func-tional, gradable and context-sensitive fash-ion.
We describe how the diagnostic prop-erties for these definitions are automati-cally acquired from the web, and how theincreased flexibility in categorization thatarises from these redefinitions offers a ro-bust account of metaphor comprehensionin the mold of Glucksberg?s (2001) the-ory of category-inclusion.
Furthermore, wedemonstrate how this competence with figu-rative categorization can effectively be gov-erned by automatically-generated ontologi-cal constraints, also acquired from the web.1 IntroductionLinguistic variation across contexts is often symp-tomatic of ontological differences between contexts.These observable variations can serve as valuableclues not just to the specific senses of words in con-text (e.g., see Pustejovsky, Hanks and Rumshisky,2004) but to the underlying ontological structure it-self (see Cimiano, Hotho and Staab, 2005).
Themost revealing variations are syntagmatic in nature,which is to say, they look beyond individual wordforms to larger patterns of contiguous usage (Hanks,2004).
In most contexts, the similarity betweenchocolate, say, and a narcotic like heroin will mea-gerly reflect the simple ontological fact that both arekinds of substances; certainly, taxonomic measuresof similarity as discussed in Budanitsky and Hirst(2006) will capture little more than this common-ality.
However, in a context in which the addictiveproperties of chocolate are very salient (e.g., an on-line dieting forum), chocolate is more likely to becategorized as a drug and thus be considered moresimilar to heroin.
Look, for instance, at the simi-lar ways in which these words can be used: one canbe ?chocolate-crazed?
or ?chocolate-addicted?
andsuffer ?chocolate-induced?
symptoms (e.g., each ofthese uses can be found in the pages of Wikipedia).In a context that gives rise to these expressions, it isunsurprising that chocolate should appear altogethermore similar to a harmful narcotic.In this paper we computationally model this ideathat language use reflects category structure.
Asnoted by De Leenheer and de Moor (2005), ontolo-gies are lexical representations of concepts, so wecan expect the effects of context on language useto closely reflect the effects of context on ontolog-ical structure.
An understanding of the linguistic ef-fects of context, as expressed through syntagmaticpatterns of word usage, should lead therefore to thedesign of more flexible lexical ontologies that natu-rally adapt to their contexts of use.
WordNet (Fell-57baum, 1998) is just one such lexical ontology thatcan benefit greatly from the added flexibility thatcontext-sensitivity can bring.
Though comprehen-sive in scale and widely used, WordNet suffers froman obvious structural rigidity in which concepts areeither entirely within a category or entirely outsidea category: no gradation of category membershipis allowed, and no contextual factors are brought tobear on criteria for membership.
Thus, a gun is al-ways a weapon in WordNet while an axe is never so,despite the uses (sporting or murderous) to whicheach can be put.In section two we describe a computationalframework for giving WordNet senses a functional,context-sensitive form.
These functional forms si-multaneously represent i) an intensional definitionfor each word sense; ii) a structured query capableof retrieving instances of the corresponding categoryfrom a context-specific corpus; and iii) a member-ship function that assigns gradated scores to theseinstances based on available syntagmatic evidence.In section three we describe how the knowledge re-quired to automate this functional re-definition is ac-quired from the web and linked to WordNet.
In sec-tion four we describe how these re-definitions canproduce a robust model of metaphor, before we eval-uate the descriptive sufficiency of this approach insection five, comparing it to the knowledge alreadyavailable within WordNet.
We conclude with somefinal remarks in section six.2 Functional Context-Sensitive CategoriesWe take a wholly textual view of context and as-sume that a given context can be implicitly charac-terized by a representative text corpus.
This corpuscan be as large as a text archive or an encyclopedia(e.g., the complete text of Wikipedia), or as smallas a single document, a sentence or even a singlenoun-phrase.
For instance, the micro-context ?alco-holic apple-juice?
is enough to implicate the cate-gory Liquor, rather than Juice, as a semantic head,while ?lovable snake?
can be enough of a context tolocally categorize Snake as a kind of Pet.
There is arange of syntagmatic patterns that one can exploit toglean category insights from a text.
For instance, the?X kills?
pattern is enough to categorize X as a kindof Killer, ?hunts X?
is enough to categorize X asa kind of Prey, while ?X-covered?, ?X-dipped?
and?X-frosted?
all indicate that X is a kind of Covering.Likewise, ?army of X?
suggests that a context viewsX as a kind of Soldier, while ?barrage of X?
suggeststhat X should be seen as a kind of Projectile.We operationalize the collocation-type of adjec-tive and noun via the function (attr ADJ NOUN),which returns a number in the range 0...1; thisrepresents the extent to which ADJ is used tomodify NOUN in the context-defining corpus.Dice?s coefficient (e.g., see Cimiano et al, 2005) isused to implement this measure.
A context-sensitivecategory membership function can be defined, as inthat for Fundamentalist in Figure 1:(define Fundamentalist.0 (arg0)(* (max(%isa arg0Person.0)(%isa arg0Group.0))(min(max(attr political arg0)(attr religious arg0))(max(attr extreme arg0)(attr violent arg0)(attr radical arg0)))))Figure 1.
A functional re-definition of the cat-egory Fundamentalist.The function of Figure 1 takes, as a single ar-gument arg0, a putative member of the categoryFundamentalist.0 (note how the sense tag, 0, isused to identify a specific WordNet sense of ?fun-damentalist?
), and returns a membership score inthe range 0...1 for this term.
This score reflects thesyntagmatic evidence for considering arg0to bepolitical or religious, as well as extreme or violentor radical.
The function (%isa arg0CAT) returns avalue of 1.0 if some sense of arg0is a descendentof CAT (here Person.0 or Group.0), otherwise 0.This safeguards ontological coherence and ensuresthat only kinds of people or groups can ever beconsidered as fundamentalists.The example of Figure 1 is hand-crafted, but afunctional form can be assigned automatically tomany of the synsets in WordNet by heuristic means.58For instance, those of Figure 2 are automaticallyderived from WordNet?s morpho-semantic links:(define Fraternity.0 (arg0)(* (%sim arg0Fraternity.0)(max(attr fraternal arg0)(attr brotherly arg0))))(define Orgasm.0 (arg0)(* (%sim arg0Orgasm.0)(max(attr climactic arg0)(attr orgasmic arg0))))Figure 2.
Exploiting the WordNet links be-tween nouns and their adjectival forms.The function (%sim arg0CAT) reflects theperceived similarity between the putative memberarg0and a synset CAT in WordNet, using one ofthe standard formulations described in Budanitskyand Hirst (2006).
Thus, any kind of group (e.g., aglee club, a Masonic lodge, or a barbershop quartet)described in a text as ?fraternal?
or ?brotherly?
(both occupy the same WordNet synset) can beconsidered a Fraternity to the corresponding degree,tempered by its a priori similarity to a Fraternity;likewise, any climactic event can be categorized asan Orgasm to a more or less degree.Alternately, the function of Figure 3 is automat-ically obtained for the lexical concept Espresso byshallow parsing its WordNet gloss: ?strong blackcoffee brewed by forcing steam under pressurethrough powdered coffee beans?.
(define Espresso.0 (arg0)(* (%sim arg0Espresso.0)(min(attr strong arg0)(attr black arg0))))Figure 3.
A functional re-definition of the cat-egory Espresso based on its WordNet gloss.It follows that any substance (e.g., oil or tea)described locally as ?black?
and ?strong?
with anon-zero taxonomic similarity to coffee can beconsidered a kind of Espresso.Combining the contents of WordNet 1.6 andWordNet 2.1, 27,732 different glosses (shared by51,035 unique word senses) can be shallow parsed toyield a definition of the kind shown in Figure 3.
Ofthese, 4525 glosses yield two or more properties thatcan be given functional form via attr.
However, onecan question whether these features are sufficient,and more importantly, whether they are truly diag-nostic of the categories they are used to define.
Inthe next section we consider another source of diag-nostic properties, explicit similes on the web, before,in section 5, comparing the quality of these proper-ties to those available from WordNet.3 Diagnostic Properties on the WebWe employ the Google search engine as a retrievalmechanism for acquiring the diagnostic propertiesof categories from the web, since the Google APIand its support for the wildcard term * allows thisprocess to be fully automated.
The guiding intu-ition here is that looking for explicit similes of theform ?X is as P as Y?
is the surest way of findingthe most salient properties of a term Y; with othersyntagmatic patterns, such as adjective:noun collo-cations, one cannot be sure that the adjective is cen-tral to the noun.Since we expect that explicit similes will tend toexploit properties that occupy an exemplary point ona scale, we first extract a list of antonymous adjec-tives, such as ?hot?
or ?cold?, from WordNet.
Forevery adjective ADJ on this list, we send the query?as ADJ as *?
to Google and scan the first 200 snip-pets returned to extract different noun values for thewildcard *.
From each set of snippets we can alsoascertain the relative frequencies of different nounvalues for ADJ.
The complete set of nouns extractedin this way is then used to drive a second phase ofthe search, in which the query template ?as * as aNOUN?
is used to acquire similes that may havelain beyond the 200-snippet horizon of the originalsearch, or that may hinge on adjectives not includedon the original list.
Together, both phases collecta wide-ranging series of core samples (of 200 hitseach) from across the web, yielding a set of 74,704simile instances (of 42,618 unique types) relating593769 different adjectives to 9286 different nouns3.1 Property FilteringUnfortunately, many of these similes are not suffi-ciently well-formed to identify salient properties.
Inmany cases, the noun value forms part of a largernoun phrase: it may be the modifier of a compoundnoun (as in ?bread lover?
), or the head of complexnoun phrase (such as ?gang of thieves?
or ?woundthat refuses to heal?).
In the former case, the com-pound is used if it corresponds to a compound termin WordNet and thus constitutes a single lexical unit;if not, or if the latter case, the simile is rejected.Other similes are simply too contextual or under-specified to function well in a null context, so if onemust read the original document to make sense ofthe simile, it is rejected.
More surprisingly, per-haps, a substantial number of the retrieved simi-les are ironic, in which the literal meaning of thesimile is contrary to the meaning dictated by com-mon sense.
For instance, ?as hairy as a bowlingball?
(found once) is an ironic way of saying ?ashairless as a bowling ball?
(also found just once).Many ironies can only be recognized using worldknowledge, such as ?as sober as a Kennedy?
and ?astanned as an Irishman?.Given the creativity involved in these construc-tions, one cannot imagine a reliable automatic fil-ter to safely identify bona-fide similes.
For thisreason, the filtering task is performed by a humanjudge, who annotated 30,991 of these simile in-stances (for 12,259 unique adjective/noun pairings)as non-ironic and meaningful in a null context; thesesimiles relate a set of 2635 adjectives to a set of4061 different nouns.
In addition, the judge alsoannotated 4685 simile instances (of 2798 types) asironic; these similes relate a set of 936 adjectivesto a set of 1417 nouns.
Perhaps surprisingly, ironicpairings account for over 13% of all annotated sim-ile instances and over 20% of all annotated types.3.2 Linking to WordNet SensesTo create functional WordNet definitions from theseadjective:noun pairings, we first need to identify theWordNet sense of each noun.
For instance, ?as stiffas a zombie?
might refer either to a re-animatedcorpse or to an alcoholic cocktail (both are sensesof ?zombie?
in WordNet, and drinks can be ?stiff?too).
Disambiguation is trivial for nouns with justa single sense in WordNet.
For nouns with two ormore fine-grained senses that are all taxonomicallyclose, such as ?gladiator?
(two senses: a boxer and acombatant), we consider each sense to be a suitabletarget.
In some cases, the WordNet gloss for as par-ticular sense will literally mention the adjective ofthe simile, and so this sense is chosen.
In all othercases, we employ a strategy of mutual disambigua-tion to relate the noun vehicle in each simile to a spe-cific sense in WordNet.
Two similes ?as A as N1?and ?as A as N2?
are mutually disambiguating if N1and N2are synonyms in WordNet, or if some senseof N1is a hypernym or hyponym of some sense ofN2in WordNet.
For instance, the adjective ?scary?is used to describe both the noun ?rattler?
and thenoun ?rattlesnake?
in bona-fide (non-ironic) similes;since these nouns share a sense, we can assume thatthe intended sense of ?rattler?
is that of a danger-ous snake rather than a child?s toy.
Similarly, theadjective ?brittle?
is used to describe both saltinesand crackers, suggesting that it is the bread sense of?cracker?
rather than the hacker, firework or hillbillysenses (all in WordNet) that is intended.These heuristics allow us to automatically disam-biguate 10,378 bona-fide simile types (85%), yield-ing a mapping of 2124 adjectives to 3778 differentWordNet senses.
Likewise, 77% (or 2164) of thesimile types annotated as ironic are disambiguatedautomatically.
A remarkable stability is observed inthe alignment of noun vehicles to WordNet senses:100% of the ironic vehicles always denote the samesense, no matter the adjective involved, while 96%of bona-fide vehicles always denote the same sense.This stability suggests two conclusions: the dis-ambiguation process is consistent and accurate; butmore intriguingly, only one coarse-grained sense ofany word is likely to be sufficiently exemplary ofsome property to be useful in a simile.4 From Similes to Category FunctionsAs noted in section 3, the filtered web data yields12,259 bona-fide similes describing 4061 targetnouns in terms of 2635 different adjectival prop-erties.
Word-sense disambiguation allows 3778synsets in WordNet to be given a functional re-definition in terms of 2124 diagnostic properties, as60in the definition of Gladiator in Figure 4:(define Gladiator.0 (arg0)(* (%isa arg0Person.0)(* (%sim arg0Gladiator.0)(combine(attr strong arg0)(attr violent arg0)(attr manly arg0)))))Figure 4.
Aweb-based definition of Gladiator.Since we cannot ascertain from the web datawhich properties are necessary and which arecollectively sufficient, we use the function combineto aggregate the available evidence.
This functionimplements a na?
?ve probabilistic or, in which eachpiece of syntagmatic evidence is naively assumed tobe independent, as follows:(combine e0e1) = e0+ e1(1 ?
e0)(combine e0e1...en) = (combine e0(combine e1...en))Thus, any combatant or competitor (such as asportsman) that is described as strong, violent ormanly in a corpus can be categorized as a Gladiatorin that context; the more properties that hold, andthe greater the degree to which they hold, the greaterthe membership score that is assigned.The source of the hard taxonomic constraint(%isa arg0Person.0) is explained in the next sec-tion.
For now, note how the use of %sim in thefunctions of Figures 2, 3 and 4 means that thesemembership functions readily admit both literal andmetaphoric members.
Since the line between lit-eral and metaphoric uses of a category is often im-possible to draw, the best one can do is to acceptmetaphor as a gradable phenomenon (see Hanks,2006).
The incorporation of taxonomic similarityvia %sim ensures that literal members will tend toreceive higher membership scores, and that the mosttenuous metaphors will receive the lowest member-ship scores (close to 0.0).4.1 Constrained Category InclusionSimile and metaphor involve quite different con-ceptual mechanisms.
For instance, anything thatis particularly strong or black might meaningfullybe called ?as black as espresso?
or ?as strongas espresso?, yet few such things can meaning-fully be called just ?espresso?.
While simile is amechanism for highlighting inter-concept similarity,metaphor is at heart a mechanism of category inclu-sion (see Glucksberg, 2001).
As the espresso exam-ple demonstrates, category inclusion is more than amatter of shared properties: humans have strong in-tuitions about the structure of categories and the ex-tent to which they can be stretched to include newmembers.
So while it is sensible to apply the cat-egory Espresso to other substances, preferably liq-uids, it seems nonsensical to apply the category toanimals, artifacts, places and so on.Much as the salient properties of categories canbe acquired form the web (see section 3), so toocan the intuitions governing inclusion amongst cat-egories.
For instance, an attested web-usage of thephrase ?Espresso-like CAT?
tells us that sub-typesof CAT are allowable targets of categorization by thecategory Espresso.
Thus, since the query ?espresso-like substance?
returns 3 hits via Google, types ofsubstance (oil, etc.)
can be described as Espresso ifthey are contextually strong and black.
In contrast,the query ?espresso-like person?
returns 0 hits, sono instance of person can be described as Espresso,no matter how black or how strong.
While this isclearly a heuristic approach to a complex cognitiveproblem, it does allow us to tap into the tacit knowl-edge that humans employ in categorization.
Moregenerally, a concept X can be included in a categoryC if X exhibits salient properties of C and, for somehypernym H of X in WordNet, we can find an at-tested use of ?C-like H?
on the web.If we can pre-fetch all possible ?C-like H?from the web, this will allow comprehension toproceed without having to resort to web analysisin mid-categorization.
While there are too manypossible values of H to make full pre-fetching apractical reality, we can generalize the problemsomewhat, by selecting a range of values for Hfrom the middle-layer of WordNet, such as Person,Substance, Animal, Tool, Plant, Structure, Event,Vehicle, Idea and Place, and by pre-fetching thequery ?C-like H?
for all 4061 nouns collected insection 3, combined with this limited set of Hvalues.
For every noun in our database then, we pre-compile a vector of possible category inclusions.61For instance, ?lattice?
yields the following vector:{structure(1620), substance(8), container(1),vehicle(1)}where numbers in parentheses indicate the web-frequency of the corresponding ?Lattice-like H?query.
Thus, the category Lattice can be used todescribe (and metaphorically include) other kindsof structure (like crystals), types of substance (e.g.,crystalline substances), containers (like honey-combs) and even vehicles (e.g., those with manycompartments).
Likewise, the noun ?snake?
yieldsthe following vector of possibilities:{structure(125), animal(122), person(56), ve-hicle(17), tool(9)}(note, the frequency for ?person?
includes thefrequency for ?man?
and ?woman?).
The categorySnake can also be used to describe and includestructures (like tunnels), other animals (like eels),people (e.g., the dishonest variety), vehicles (e.g.,articulated trucks, trains) and tools (e.g., hoses).
Thenoun ?gladiator?
yields a vector of just one element,{person(1)}, from which the simple constraint(%isa arg0Person.0) in Figure 4 is derived.
In con-trast, ?snake?
is now given the definition of Figure 5:(define Snake.0 (arg0)(* (max(%isa arg0Structure.0)(%isa arg0Animal.0)(%isa arg0Person.0)(%isa arg0Vehicle.0))(* (%sim arg0Snake.0)(combine(attr cunning arg0)(attr slippery arg0)(attr flexible arg0)(attr slim arg0)(attr sinuous arg0)(attr crooked arg0)(attr deadly arg0)(attr poised arg0)))))Figure 5.
A membership function for Snakeusing web-derived category-inclusion constraints.Glucksberg (2001) notes that the same category,used figuratively, can exhibit different qualities indifferent metaphors.
For instance, Snake mightdescribe a kind of crooked person in one metaphor,a poised killer in another metaphor, and a kind offlexible tool in yet another.
The use of combinein Figure 5 means that a single category definitioncan give rise to each of these perspectives in theappropriate contexts.
We therefore do not need adifferent category definition for each metaphoricuse of Snake.To illustrate the high-level workings of category-inclusion, Table 1 generalizes over the set of 3778disambiguated nouns from section 3 to estimate thepropensity for one semantic category, like Person, toinclude members of another category, like Animal,in X-like Y constructs.X-like Y P A Sub T Str(P)erson .66 .05 .03 .04 .09(A)nimal .36 .27 .04 .05 .15(Sub)stance .14 .03 .37 .05 .32(T)ool .08 .03 .07 .22 .34(Str)ucture .04 .03 .03 .03 .43Table 1.
The Likelihood of a category X accommo-dating a category Y.Table 1 reveals that 36% of ?ANIMAL-like?patterns on the web describe a kind of Person,while only 5% of ?PERSON-like?
patterns on theweb describe a kind of Animal.
Category inclusionappears here to be a conservative mechanism, withlike describing like in most cases; thus, types ofPerson are most often used to describe other kindsof Person (comprising 66% of ?PERSON-like?patterns), types of substance to describe other sub-stances, and so on.
The clear exception is Animal,with ?ANIMAL-like?
phrases more often used todescribe people (36%) than other kinds of animal(27%).
The anthropomorphic uses of this categorydemonstrate the importance of folk-knowledge infigurative categorization, of the kind one is morelikely to find in real text, and on the web (as insection 3), rather than in resources like WordNet.625 Empirical EvaluationThe simile gathering process of section 3, abettedby Google?s practice of ranking pages according topopularity, should reveal the most frequently-usedcomparative nouns, and thus, the most useful cat-egories to capture in a general-purpose ontologylike WordNet.
But the descriptive sufficiency ofthese categories is not guaranteed unless the defin-ing properties ascribed to each can be shown tobe collectively rich enough, and individually salientenough, to predict how each category is perceivedand applied by a language user.If similes are indeed a good basis for miningthe most salient and diagnostic properties of cate-gories, we should expect the set of properties foreach category to accurately predict how the cate-gory is perceived as a whole.
For instance, humans?
unlike computers ?
do not generally adopt a dis-passionate view of ideas, but rather tend to asso-ciate certain positive or negative feelings, or affec-tive values, with particular ideas.
Unsavoury activi-ties, people and substances generally possess a nega-tive affect, while pleasant activities and people pos-sess a positive affect.
Whissell (1989) reduces thenotion of affect to a single numeric dimension, toproduce a dictionary of affect that associates a nu-meric value in the range 1.0 (most unpleasant) to 3.0(most pleasant) with over 8000 words in a range ofsyntactic categories (including adjectives, verbs andnouns).
So to the extent that the adjectival proper-ties yielded by processing similes paint an accuratepicture of each category / noun-sense, we should beable to predict the affective rating of each vehiclevia a weighted average of the affective ratings ofthe adjectival properties ascribed to these nouns (i.e.,where the affect rating of each adjective contributesto the estimated rating of a noun in proportion toits frequency of co-occurrence with that noun in oursimile data).
More specifically, we should expectthat ratings estimated via these simile-derived prop-erties should correlate well with the independent rat-ings contained in Whissell?s dictionary.To determine whether similes do offer the clearestperspective on a category?s most salient properties,we calculate and compare this correlation using thefollowing data sets:A. Adjectives derived from annotated bona-fide(non-ironic) similes only.B.
Adjectives derived from all annotated similes(both ironic and non-ironic).C.
Adjectives derived from ironic similes only.D.
All adjectives used to modify a given noun ina large corpus.
We use over 2-gigabytes oftext from the online encyclopaedia Wikipediaas our corpus.E.
The set of 63,935 unique property-of-nounpairings extracted via shallow-parsing fromWordNet glosses in section 2, e.g., strong andblack for Espresso.Predictions of affective rating were made from eachof these data sources and then correlated with theratings reported in Whissell?s dictionary of affectusing a two-tailed Pearson test (p < 0.01).
As ex-pected, property sets derived from bona-fide simi-les only (A) yielded the best correlation (+0.514)while properties derived from ironic similes only(C) yielded the worst (-0.243); a middling corre-lation coefficient of 0.347 was found for all simi-les together, demonstrating the fact that bona-fidesimiles outnumber ironic similes by a ratio of 4to 1.
A weaker correlation of 0.15 was found us-ing the corpus-derived adjectival modifiers for eachnoun (D); while this data provides quite large prop-erty sets for each noun, these properties merely re-flect potential rather than intrinsic properties of eachnoun and so do not reveal what is most diagnosticabout a category.
More surprisingly, property setsderived from WordNet glosses (E) are also poorlypredictive, yielding a correlation with Whissell?s af-fect ratings of just 0.278.
This suggests that theproperties used to define categories in hand-craftedresources like WordNet are not always those that ac-tually reflect how humans think of these categories.6 Concluding RemarksMuch of what we understand about different cate-gories is based on tacit and defeasible knowledge ofthe outside world, knowledge that cannot easily beshoe-horned into the rigid is-a structure of an on-tology like WordNet.
This already-complex picture63is complicated even further by the often metaphoricrelationship between words and the categories theydenote, and by the fact that the metaphor/literal dis-tinction is not binary but gradable.
Furthermore, thegradability of category membership is clearly influ-enced by context: in a corpus describing the exploitsof Vikings, an axe will most likely be seen as a kindof weapon, but in a corpus dedicated to forestry, itwill likely describe a tool.
A resource like WordNet,in which is-a links are reserved for category relation-ships that are always true, in any context, is going tobe inherently limited when dealing with real text.We have described an approach that can be seen asa functional equivalent to the CPA (Corpus PatternAnalysis) approach of Pustejovsky et al (2004), inwhich our goal is not that of automated induction ofword senses in context (as it is in CPA) but the au-tomated induction of flexible, context-sensitive cat-egory structures.
As such, our goal is primarily on-tological rather than lexicographic, though both ap-proaches are complementary since each views syn-tagmatic evidence as the key to understanding theuse of lexical concepts in context.
By defining cat-egory membership in terms of syntagmatic expec-tations, we establish a functional and gradable ba-sis for determining whether one lexical concept (orsynset) in WordNet deserves to be seen as a de-scendant of another in a particular corpus and con-text.
Augmented with ontological constraints de-rived from the usage of ?X-like Y?
patterns on theweb, we also show how these membership functionscan implement Glucksberg?s (2001) theory of cate-gory inclusion.We have focused on just one syntagmatic patternhere ?
adjectival modification of nouns ?
but cate-gorization can be inferred from a wide range of pro-ductive patterns in text, particularly those concern-ing verbs and their case-fillers.
For instance, verb-centred similes of the form ?to V+inf like a|an N?and ?to be V+past like a|an N?
reveal insights intothe diagnostic behaviour of entities (e.g., that preda-tors hunt, that prey is hunted, that eagles soar andbombs explode).
Taken together, adjective-basedproperties and verb-based behaviours can paint aneven more comprehensive picture of each lexicalconcept, so that e.g., political agents that kill canbe categorized as assassins, loyal entities that fightcan be categorized as soldiers, and so on.
An im-portant next step, then, is to mine these behavioursfrom the web and incorporate the correspondingsyntagmatic expectations into our category defini-tions.
The symbolic nature of the resulting defini-tions means these can serve not just as mathematicalmembership functions, but as ?active glosses?, capa-ble of recruiting their own members in a particularcontext while demonstrating a flexibility with cate-gorization and a genuine competence with metaphor.ReferencesAlexander Budanitsky and Graeme Hirst.
2006.
Eval-uating WordNet-based Measures of Lexical SemanticRelatedness.
Computational Linguistics, 32(1), pp 13-47.Christiane Fellbaum (ed.).
1998.
WordNet: An Elec-tronic Lexical Database.
The MIT Press, Cambridge,MA.Cynthia Whissell.
1989.
The dictionary of affect in lan-guage.
In R. Plutchnik & H. Kellerman (Eds.).
Emo-tion: Theory and research.
New York, HarcourtBrace, 113-131.James Pustejovsky, Patrick Hanks and Anna Rumshisky.2004.
Automated Induction of Sense in Context.
InProceedings of COLING 2004, Geneva, pp 924-931.Patrick Hanks.
2006.
Metaphoricity is a Gradable.
In A.Stefanowitsch and S. Gries (eds.).
Corpora in Cog-nitive Linguistics.
Vol.
1: Metaphor and Metonymy.Berlin: Mouton.Patrick Hanks.
2004.
The syntagmatics of metaphor andidiom.
International Journal of Lexicography, 17(3).Philipp Cimiano, Andreas Hotho, and Steffen Staab.2005.
Learning Concept Hierarchies from Text Cor-pora using Formal Concept Analysis.
Journal of AIResearch, 24: 305-339.Pieter De Leenheer and Aldo de Moor.
2005.
Context-driven Disambiguation in Ontology Elicitation.
InShvaiko P. & Euzenat J.
(eds.
), Context and Ontolo-gies: Theory, Practice and Applications, AAAI TechReport WS-05-01.
AAAI Press, pp 17-24.Sam Glucksberg.
2001.
Understanding figurative lan-guage: From metaphors to idioms.
Oxford: OxfordUniversity Press.64
