Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 982?992,MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational LinguisticsEvaluating the Impact of Alternative Dependency Graph Encodings onSolving Event Extraction TasksEkaterina Buyko and Udo HahnJena University Language & Information Engineering (JULIE) LabFriedrich-Schiller-Universita?t JenaFu?rstengraben 30, 07743 Jena, Germany{ekaterina.buyko|udo.hahn}@uni-jena.deAbstractIn state-of-the-art approaches to informationextraction (IE), dependency graphs constitutethe fundamental data structure for syntacticstructuring and subsequent knowledge elicita-tion from natural language documents.
Thetop-performing systems in the BioNLP 2009Shared Task on Event Extraction all shared theidea to use dependency structures generatedby a variety of parsers ?
either directly orin some converted manner ?
and optionallymodified their output to fit the special needsof IE.
As there are systematic differences be-tween various dependency representations be-ing used in this competition, we scrutinize ondifferent encoding styles for dependency in-formation and their possible impact on solv-ing several IE tasks.
After assessing moreor less established dependency representationssuch as the Stanford and CoNLL-X dependen-cies, we will then focus on trimming opera-tions that pave the way to more effective IE.Our evaluation study covers data from a num-ber of constituency- and dependency-basedparsers and provides experimental evidencewhich dependency representations are partic-ularly beneficial for the event extraction task.Based on empirical findings from our studywe were able to achieve the performance of57.2% F-score on the development data set ofthe BioNLP Shared Task 2009.1 IntroductionRelation and event extraction are among the mostdemanding semantics-oriented NLP challenge tasks(both in the newspaper domain such as for ACE1, aswell as in the biological domain such as for BioCre-ative2 or the BioNLP Shared Task3), comparable interms of analytical complexity with recent efforts di-rected at opinion mining (e.g., NTCIR-74 or TRECBlog tracks5) or the recognition of textual entail-ment.6 The most recent BioNLP 2009 Shared Taskon Event Extraction (Kim et al, 2009) required, fora sample of 260 MEDLINE abstracts, to determineall mentioned events ?
to be chosen from a givenset of nine event types, including ?Localization?,?Binding?, ?Gene Expression?, ?Transcription?,?Protein Catabolism?, ?Phosphorylation?, ?Posi-tive Regulation?, ?Negative Regulation?, and (un-specified) ?Regulation?
?
and link them appropri-ately with a priori supplied protein annotations.
Thedemands on text analytics to deal with the complex-ity of this Shared Task in terms of relation diversityand specificity are unmatched by former challenges.For relation extraction in the biomedical domain(the focus of our work), a stunning convergencetowards dependency-based syntactic representationstructures is witnessed by the performance resultsof the top-performing systems in the BioNLP?091http://papers.ldc.upenn.edu/LREC2004/ACE.pdf2http://biocreative.sourceforge.net/3www-tsujii.is.s.u-tokyo.ac.jp/GENIA/SharedTask/4http://research.nii.ac.jp/ntcir/workshop/OnlineProceedings7/pdf/revise/01-NTCIR-OV-MOAT-SekiY-revised-20081216.pdf5http://trec.nist.gov/data/blog08.html6http://pascallin.ecs.soton.ac.uk/Challenges/RTE/982Shared Task on Event Extraction.7 Regarding thefact that dependency representations were alwaysviewed as a vehicle to represent fundamental seman-tic relationships already at the syntactic level, thisis not a great surprise.
Yet, dependency grammaris not a monolithic, consensually shaped and well-defined linguistic theory.
Accordingly, associatedparsers tend to vary in terms of dependency pairingor structuring (which pairs of words join in a depen-dency relation?)
and dependency typing (how aredependency relations for a particular pair labelled?
).Depending on the type of dependency theory orparser being used, various representations emerge(Miyao et al, 2007).
In this paper, we explore thesedifferent representations of the dependency graphsand try, first, to pinpoint their effects on solving theoverall event extraction task and, second, to furtherenhance the potential of JREX, a high-performancerelation and event extractor developed at the JULIELab (Buyko et al, 2009).2 Related WorkIn the biomedical domain, the focus has largely beenon binary relations, in particular protein-proteininteractions (PPIs).
Accordingly, the biomedi-cal NLP community has developed various PPI-annotated corpora (e.g., LLL (Ne?dellec, 2005),AIMED (Bunescu et al, 2005), BIOINFER (Pyysaloet al, 2007)).
PPI extraction does clearly not countas a solved problem, and a deeper look at its bio-logical and representational intricacies is certainlyworthwhile.
The GENIA event corpus (Kim et al,2008) and the BioNLP 2009 Shared Task data (Kimet al, 2009) contain such detailed annotations ofPPIs (amongst others).The BioNLP Shared Task was a first step towardsthe extraction of specific pathways with precise in-formation about the molecular events involved.
Inthat task, 42 teams participated and 24 of them sub-mitted final results.
The winner system, TURKU(Bjo?rne et al, 2009), achieved with 51.95% F-scorethe milestone result in that competition followed bythe JULIELab system (Buyko et al, 2009) whichpeaked at 46.7% F-score.
Only recently, an ex-tension of the TURKU system, the TOKYO system,7www-tsujii.is.s.u-tokyo.ac.jp/GENIA/SharedTask/results/results-master.htmlhas been realized (Miwa et al, 2010).
TOKYO sys-tem?s event extraction capabilities are based on theTURKU system, yet TURKU?s manually crafted rulesystem for post-processing and the combination ofextracted trigger-argument relations is replaced bya machine learning approach in which rich featurescollected from classification steps for triggers andarguments are re-combined.
TOKYO achieves anoverall F-score of 53.29% on the test data, thus out-performing TURKU by 1.34 percentage points.The three now top-performing systems, TOKYO,TURKU and JULIELab, all rely on dependencygraphs for solving the event extraction tasks.
Whilethe TURKU system exploits the Stanford dependen-cies from the McClosky-Charniak parser (Charniakand Johnson, 2005), and the JULIELab system usesthe CoNLL-like dependencies from the GDep parser(Sagae and Tsujii, 2007),8 the TOKYO system over-lays the Shared Task data with two parsing represen-tations, viz.
Enju PAS structure (Miyao and Tsujii,2002) and GDep parser dependencies.
Obviously,one might raise the question as to what extent theperformance of these systems depends on the choiceof the parser and its output representations.
Miyaoet al (2008) already assessed the impact of differentparsers for the task of biomedical relation extraction(PPI).
Here we perform a similar study for the taskof event extraction and focus, in particular, on theimpact of various dependency representations suchas Stanford and CoNLL?X dependencies and addi-tional trimming procedures.For the experiments on which we report here, weperformed experiments with the JULIELab system.Our main goal is to investigate into the crucial roleof proper representation structures for dependencygraphs so that the performance gap from SharedTask results between the best-performing TOKYOsystem and the JULIELab system be narrowed.3 Event Extraction3.1 ObjectiveEvent extraction is a complex task that can be sub-divided into a number of subtasks depending on8The GDep parser has been trained on the GENIA Tree-bank pre-official version of the version 1.0 converted with thescript available from http://w3.msi.vxu.se/?nivre/research/Penn2Malt.html983whether the focus is on the event itself or on the ar-guments involved:Event trigger identification deals with the largevariety of alternative verbalizations of the sameevent type, e.g., whether the event is expressed in averbal or in a nominalized form (?A is expressed?as well as ?the expression of A?
both refer to thesame event type, viz.
Expression(A)).
Since thesame trigger may stand for more than one event type,event trigger ambiguity has to be resolved as well.Event trigger disambiguation selects the correctevent name from the set of alternative event triggers.Argument identification is concerned with find-ing all necessary participants in an event, i.e., thearguments of the relation.Argument ordering assigns each identified par-ticipant its functional role within the event, mostlyAgent and Patient.3.2 JULIELab SystemThe JULIELab solution can best be characterized asa single-step learning approach for event detectionas the system does not separate the overall learn-ing task into independent event trigger and eventargument learning subtasks.9 The JULIELab sys-tem incorporates manually curated dictionaries andmachine learning (ML) methodologies to sort outassociated event triggers and arguments on depen-dency graph structures.
For argument extraction, theJULIELab system uses two ML-based approaches,a feature-based and a kernel-based one.
Giventhat methodological framework, the JULIELab teamscored on 2nd rank among 24 competing teams, with45.8% precision, 47.5% recall and 46.7% F-score onall 3,182 events.
After the competition, this systemwas updated and achieved 57.6% precision, 45.7%recall and 51.0% F-score (Buyko et al, 2010) usingmodified dependency representations from the MSTparser (McDonald et al, 2005).
In this study, weperform event extraction experiments with variousdependency representations that allow us to measuretheir effects on the event extraction task and to in-crease the overall JULIELab system performance interms of F-score.9The JULIELab system considers all relevant lexical items aspotential event triggers which might represent an event.
Onlythose event triggers that can eventually be connected to argu-ments, finally, represent a true event.4 Dependency Graph RepresentationsIn this section, we focus on representation formatsof dependency graphs.
In Section 4.1, we introducefundamental notions underlying dependency pars-ing and consider established representation formatsfor dependency structures as generated by variousparsers.
In Section 4.2, we account for selected trim-ming operations for dependency graphs to ease IE.4.1 Dependency Structures: RepresentationIssuesDependency parsing, in the past years, has increas-ingly been recognized as an alternative to long-prevailing constituency-based parsing approaches,particularly in semantically-oriented applicationscenarios such as information extraction.
Yet evenunder purely methodologically premises, it hasgained wide-spread attention as witnessed by recentactivities performed as part of the ?CoNLL SharedTasks on Multilingual Dependency Parsing?
(Buch-holz and Marsi, 2006).In a nutshell, in dependency graphs of sentences,nodes represent single words and edges account forhead-modifier relations between single words.
De-spite this common understanding, concrete syntacticrepresentations often differ markedly from one de-pendency theory/parser to the other.
The differencesfall into two main categories: dependency pairing orstructuring (which pairs of words join in a depen-dency relation?)
and dependency typing (how aredependency relations for a particular pair labelled?
).The CoNLL?X dependencies, for example, aredefined by 54 relation types,10 while the Stanfordscheme (de Marneffe et al, 2006) incorporates 48types (so called grammatical relations or Stanforddependencies).
The Link Grammar Parser (Sleatorand Temperley, 1991) employs a particularly fine-grained repertoire of dependency relations addingup to 106 types, whereas the well-known MINIPARparser (Lin, 1998) relies on 59 types.
Differences independency structure are at least as common as dif-ferences in dependency relation typing (see below).10Computed by using the conversion script on WSJdata (accessible via http://nlp.cs.lth.se/pennconverter/; see also Johansson and Nugues (2007)for additional information).
From the GENIA corpus, using thisscript, we could only extract 29 CoNLL dependency relations.984Figure 1: Example of CoNLL 2008 dependencies, as used in most of the native dependency parsers.Figure 2: Stanford dependencies, basic conversion from Penn Treebank.In general, dependency graphs can be generatedby syntactic parsers in two ways.
First, native de-pendency parsers output CoNLL?X or Stanford de-pendencies dependent on which representation for-mat they have been trained on.11 Second, in a deriva-tive dependency mode, the output of constituency-based parsers, e.g., phrase structure representations,is subsequently converted either into CoNLL?X orStanford dependencies using Treebank conversionscripts (see below).
In the following, we providea short description of these two established depen-dency graph representations:?
CoNLL?X dependencies (CD).
This depen-dency tree format was used in the CoNLL?XShared Tasks on multi-lingual dependencyparsing (see Figure 1).
It has been adoptedby most native dependency parsers and wasoriginally obtained from Penn Treebank (PTB)trees using constituent-to-dependency conver-sion (Johansson and Nugues, 2007).
It differsslightly in the number and types of dependen-cies being used from various CoNLL rounds(e.g., CoNLL?08 provided a dependency typefor representing appositions).12?
Stanford dependencies (SD).
This format wasproposed by de Marneffe et al (2006) for11We disregard in this study other dependency representa-tions such as MINIPAR and LINK GRAMMAR representations.12For the differences between CoNLL?07 and CoNLL?08 rep-resentations, cf.
http://nlp.cs.lth.se/software/treebank_converter/semantics-sensitive applications using depen-dency representations, and can be obtained us-ing the Stanford tools13 from PTB trees.
TheStanford format is widely used in the biomed-ical domain (e.g., by Miyao et al (2008) orClegg and Shepherd (2005)).There are systematic differences betweenCoNLL?X and Stanford dependencies, e.g., as faras the representation of passive constructions, theposition of auxiliary and modal verbs, or coordi-nation representation is concerned.
In particular,the representation of the passive construction andthe role of the auxiliary verb therein may haveconsiderable effects for semantics-sensitive tasks.While in SD the subject of the passive constructionis represented by a special nsubj dependencylabel, in CD we find the same subject label as foractive constructions SUB(J).
On CoNLL?08 data,the logical subject is marked by the LGS depen-dency edge that connects the passive-indicatingpreposition ?by?
with the logical subject of thesentence.The representation of active constructions aresimilar in CD and SD though besides the role ofauxiliary and modal verbs.
In the Stanford de-pendency representation scheme, rather than takingauxiliaries to be the heads in passive or tense con-structions, main verbs are assigned this grammaticalfunction (see Figure 2).
The CoNLL?X represen-13Available from nlp.stanford.edu/software/lex-parser.shtml985Figure 3: Noun phrase representation inCoNLL?X dependency trees.Figure 4: Trimming procedure noun phrase onCoNLL?X dependency trees.tation scheme is completely different in that auxil-iaries ?
much in common with standard dependencytheory ?
are chosen to occupy the role of the gov-ernor (see Figure 1).
From the perspective of rela-tion extraction, however, the Stanford scheme is cer-tainly closer to the desired predicate-argument struc-ture representations than the CoNLL scheme.4.2 Dependency Graph Modifications in DetailLinguistic intuition suggests that the closer a depen-dency representation is to the format of the targetedsemantic representation, the more likely will it sup-port the semantic application.
This idea is directlyreflected in the Stanford dependencies which narrowthe distance between nodes in the dependency graphby collapsing procedures (the so-called collapsedmode of phrase structure conversion).
An exampleof collapsing is the conversion of ?expression nmod???
?in pmod????
cells?
to ?expression prep in?????
cells?.
An ex-tension of collapsing is the re-structuring of coor-dinations with sharing the dependency relations ofconjuncts (the so-called ccprocessed mode of phrasestructure conversion).According to the Stanford scheme, Buyko et al(2009) proposed collapsing scenarios on CoNLL?Xdependency graphs.
Their so-called trimming op-erations treat three syntactic phenomena, viz.
coor-dinations (coords), auxiliaries/modals (auxiliaries),and prepositions (preps).
For coordinations, theypropagate the dependency relation of the first con-junct to all the other conjuncts within the coordi-nation.
For auxiliaries/modals, they prune the aux-iliaries/modals as governors from the dependencygraph and propagate the dependency relations ofthese nodes to the main verbs.
Finally, for preposi-tions, they collapse a pair of typed dependencies intoa single typed dependency (as illustrated above).For the following experiments, we extended thetrimming procedures and propose the re-structuringof noun phrases with action adjectives to make thedependency representation even more compact forsemantic interpretation.
The original dependencyrepresentation of the noun phrase selects the right-most noun as the head of the NP and thus all re-maining elements are its dependents (see Figure 3).For the noun phrases containing action adjectives(mostly verb derivations) this representation doesnot reflect the true semantic relations between theelements.
For example, in ?IL-10 mediated expres-sion?
it is ?IL-10?
that mediates the expression.Therefore, we re-structure the dependency graph bychanging the head of ?IL-10?
from ?expression?to ?mediated?.
Our re-coding heuristics selects,first, all the noun phrases containing action adjec-tives ending with ?-ed?, ?-ing?, ?-ible?
suffixes andwith words such as ?dependent?, ?specific?, ?like?.In the second step, we re-structure the noun phraseby encoding the adjective as the head of all the nounspreceding this adjective in the noun phrase underscrutiny (see Figure 4).5 Experiments and ResultsIn this section, we describe the experiments andresults related to event extraction tasks based onalternative dependency graph representations.
Forour experiments, we selected the following top-performing parsers ?
the first three phrase structurebased and thus the origin of derivative dependencystructures, the last three fully dependency based formaking native dependency structures available:?
C+J, Charniak and Johnson?s reranking parser(Charniak and Johnson, 2005), with the WSJ-trained parsing model.?
M+C, Charniak and Johnson?s reranking parser(Charniak and Johnson, 2005), with the self-trained biomedical parsing model from Mc-Closky (2010).986?
Bikel, Bikel?s parser (Bikel, 2004) with theWSJ-trained parsing model.?
GDep (Sagae and Tsujii, 2007), a native depen-dency parser.?
MST (McDonald et al, 2005), another nativedependency parser.?
MALT (Nivre et al, 2007), yet another nativedependency parser.The native dependency parsers were re-trained onthe GENIA Treebank (Tateisi et al, 2005) conver-sions.14 These conversions,15 i.e., Stanford basic,CoNLL?07 and CoNLL?08 were produced with thecurrently available conversion scripts.
For the Stan-ford dependency conversion, we used the Stanfordparser tool,16 for CoNLL?07 and CoNLL?08 we usedthe treebank-to-CoNLL conversion scripts17 avail-able from the CoNLL?X Shared Task organizers.The phrase structure based parsers were appliedwith already available models, i.e., the Bikel andC+J parsers as trained on the WSJ corpus, andM+C as trained on the GENIA Treebank corpus.For our experiments, we converted the predictionresults of the phrase structure based parsers intofive dependency graph representations, viz.
Stanfordbasic, Stanford collapsed, Stanford ccprocessed,CoNLL?07 and CoNLL?08, using the same scriptsas for the conversion of the GENIA Treebank.The JULIELab event extraction system was re-trained on the Shared Task data enriched with differ-ent outputs of syntactic parsers as described above.The results for the event extraction task are repre-sented in Table 1.
Due to the space limitation ofthis paper we provide the summarized results of im-portant event extraction sub-tasks only, i.e., resultsfor basic events (Gene Expression, Transcription,Localization, Protein Catabolism) are summarized14For the training of dependency parsers, we used from theavailable Stanford conversion variants only Stanford basic.
Thecollapsed and ccprocessed variants do not provide dependencytrees and are not recommended for training native dependencyparsers.15We used the GENIA Treebank version 1.0, available fromwww-tsujii.is.s.u-tokyo.ac.jp16http://nlp.stanford.edu/software/lex-parser.shtml17http://nlp.cs.lth.se/software/treebank_converter/under SVT-TOTAL; regulatory events are summa-rized under REG-TOTAL; the overall extraction re-sults are listed in ALL-TOTAL (see Table 1).Obviously, the event extraction system trained onvarious dependency representations indeed producestruly different results.
The differences in terms of F-score come up to 2.4 percentage points for the SVT-TOTAL events (cf.
the MALT parser, differencebetween SD basic (75.6% F-score) and CoNLL?07(78.0% F-score)), up to 3.6 points for REG-TOTAL(cf.
the M+C parser, difference between SD ccpro-cessed (40.9% F-score) and CoNLL?07 (44.5% F-score)) and up to 2.5 points for ALL-TOTAL (cf.the M+C parser, difference between SD ccprocessed(52.8% F-score) and CoNLL?07 (55.3% F-score)).The top three event extraction results on the de-velopment data based on different syntactic parsersresults are achieved with M+C parser ?
CoNLL?07representation (55.3% F-score), MST parser ?CoNLL?08 representation (54.6% F-score) andMALT parser ?
CoNLL?08 representation (53.8%F-score) (see Table 1, ALL-TOTAL).
Surprisingly,both the CoNLL?08 and CoNLL?07 formats clearlyoutperform Stanford representations on all event ex-traction tasks.
Stanford dependencies seem to beuseful here only in the basic mode.
The collapsedand ccprocessed modes produce even worse resultsfor the event extraction tasks.Our second experiment focused on trimming op-erations on CoNLL?X dependency graphs.
Herewe performed event extraction after the trimming ofthe dependency trees as described in Section 4.2 indifferent modes: coords ?
re-structuring coordina-tions; preps ?
collapsing of prepositions; auxiliaries?
propagating dependency relations of auxiliars andmodals to main verbs; noun phrase ?
re-structuringnoun phrases containing action adjectives.
Our sec-ond experiment showed that the extraction of se-lected events can profit in particular from the trim-ming procedures coords and auxiliaries, but there isno evidence for a general trimming configuration forthe overall event extraction task.In Table 2 we summarize the best configurationswe found for the events in focus.
It is quite evi-dent that the CoNLL?08 and CoNLL?07 dependen-cies modified for auxiliaries and coordinations arethe best configurations for four events (out of nine).For three events no modifications are necessary and987Parser SD basic SD collapsed SD ccprocessed CoNLL?07 CoNLL?08SVT-TOTALR P F R P F R P F R P F R P FBikel 70.5 75.5 72.9 70.7 74.5 72.5 71.6 73.5 72.5 69.4 75.9 72.5 69.7 75.7 72.6C+J 73.0 77.4 75.1 73.2 77.3 75.2 72.8 77.2 75.0 73.5 78.3 75.8 73.0 77.9 75.4M+C 76.4 78.0 77.2 76.4 77.6 77.0 76.4 77.2 76.8 76.4 79.0 77.7 76.6 79.3 77.9GDEP 77.1 77.5 77.3 N/A N/A N/A N/A N/A N/A 72.5 80.2 76.1 72.6 77.2 74.8MALT 73.1 78.2 75.6 N/A N/A N/A N/A N/A N/A 75.9 80.3 78.0 73.7 78.2 75.9MST 76.4 78.5 77.4 N/A N/A N/A N/A N/A N/A 74.8 78.4 76.6 76.7 80.8 78.7REG-TOTALR P F R P F R P F R P F R P FBikel 35.3 40.6 37.8 33.8 40.3 36.8 34.3 39.6 36.8 33.9 39.2 36.3 34.0 41.0 37.2C+J 36.2 41.8 38.8 37.3 41.8 39.4 36.5 41.9 39.0 38.1 43.9 40.8 37.4 44.0 40.4M+C 39.4 45.5 42.3 38.8 45.3 41.8 38.5 43.7 40.9 41.9 47.4 44.5 40.1 47.9 43.7GDEP 39.6 42.8 41.6 N/A N/A N/A N/A N/A N/A 38.4 43.7 40.9 39.8 44.4 42.0MALT 38.8 44.3 41.4 N/A N/A N/A N/A N/A N/A 39.0 44.3 41.5 39.2 46.4 42.5MST 39.5 43.6 41.4 N/A N/A N/A N/A N/A N/A 39.6 45.6 42.4 40.6 45.8 43.0ALL-TOTALR P F R P F R P F R P F R P FBikel 47.4 51.5 49.4 46.3 50.8 48.5 46.9 50.2 48.5 44.8 50.7 47.6 44.7 51.8 48.0C+J 49.3 53.8 51.5 49.6 52.8 51.2 49.0 53.0 50.9 50.3 54.4 52.3 49.5 54.3 51.8M+C 52.3 56.4 54.3 51.8 55.7 53.7 51.3 54.3 52.8 53.2 57.5 55.3 52.2 58.2 55.0GDEP 52.7 54.5 53.6 N/A N/A N/A N/A N/A N/A 50.6 55.2 52.8 51.3 55.0 53.1MALT 50.4 54.7 52.4 N/A N/A N/A N/A N/A N/A 51.5 56.0 53.7 51.2 56.8 53.8MST 52.3 54.8 53.5 N/A N/A N/A N/A N/A N/A 51.7 56.4 53.9 52.4 56.9 54.6Table 1: Results on the Shared Task development data for Event Extraction Task.
Approximate Span Match-ing/Approximate Recursive Matching.Event Class Best Parser Best Configuration R P FGene Expression MST CoNLL?08, auxiliaries, coords 79.5 81.8 80.6Transcription MALT CoNLL?07, auxiliaries, coords 67.1 75.3 71.0Protein Catabolism MST CoNLL?08, preps 85.7 100 92.3Phosphorylation MALT CoNLL?08 80.9 88.4 84.4Localization MST CoNLL?08, auxiliaries 81.1 87.8 84.3Binding MST CoNLL?07, auxiliaries, coords, noun phrase 51.2 51.0 51.1Regulation MALT CoNLL?07, auxiliaries, coords 30.8 49.5 38.0Positive Regulation M+C CoNLL?07 43.0 49.9 46.1Negative Regulation M+C CoNLL?07 49.5 45.3 47.3Table 2: Best Configurations for Dependency Representations for Event Extraction Task on the development data.Binding R P FCoNLL?07 47.3 46.8 47.0CoNLL?07 auxiliaries, coords 46.8 48.1 47.4CoNLL?07 auxiliaries, coords, noun phrase 51.2 51.0 51.1Table 3: Effects of trimming of CoNLL dependencies on the Shared Task development data for Binding events.
Ap-proximate Span Matching/Approximate Recursive Matching.
The data was processed by the MST parser.988JULIELab JULIELab TOKYO System(M+C, CoNLL?08) Final ConfigurationEvent Class gold R P F R P F R P FGene Expression 356 79.2 80.3 79.8 79.5 81.8 80.6 78.7 79.5 79.1Transcription 82 59.8 72.0 65.3 67.1 75.3 71.0 65.9 71.1 68.4Protein Catabolism 21 76.2 88.9 82.0 85.7 100 92.3 95.2 90.9 93.0Phosphorylation 47 83.0 81.2 82.1 80.9 88.4 84.4 85.1 69.0 76.2Localization 53 77.4 74.6 75.9 81.1 87.8 84.3 71.7 82.6 76.8SVT-TOTAL 559 76.4 79.0 77.7 78.2 82.6 80.3 77.3 77.9 77.6Binding 248 45.6 45.9 45.8 51.2 51.0 51.1 50.8 47.6 49.1EVT-TOTAL 807 66.9 68.7 67.8 69.9 72.5 71.2 69.1 68.1 68.6Regulation 169 32.5 46.2 38.2 30.8 49.5 38.0 36.7 46.6 41.1Positive regulation 617 42.3 49.0 45.4 43.0 49.9 46.1 43.9 51.9 47.6Negative regulation 196 48.5 44.0 46.1 49.5 45.3 47.3 38.8 43.9 41.2REG-TOTAL 982 41.9 47.4 44.5 42.2 48.7 45.2 41.7 49.4 45.2ALL-TOTAL 1789 53.2 57.5 55.3 54.7 60.0 57.2 54.1 58.7 56.3Table 4: Results on the Shared Task development data.
Approximate Span Matching/Approximate Recursive Match-ing.JULIELab JULIELab TOKYO system(Buyko et al, 2010) Final ConfigurationEvent Class gold R P F R P F R P FGene Expression 722 66.3 79.6 72.4 67.0 77.2 71.8 68.7 79.9 73.9Transcription 137 33.6 61.3 43.4 35.0 60.8 44.4 54.0 60.7 57.1Protein Catabolism 14 71.4 90.9 80.0 71.4 90.9 80.0 42.9 75.0 54.6Phosphorylation 135 80.0 85.0 82.4 80.7 84.5 82.6 84.4 69.5 76.3Localization 174 47.7 93.3 63.1 45.4 90.8 60.5 47.1 86.3 61.0SVT-TOTAL 1182 61.4 80.3 69.6 61.8 78.2 69.0 65.3 76.4 70.4Binding 347 47.3 52.4 49.7 47.3 52.2 49.6 52.2 53.1 52.6EVT-TOTAL 1529 58.2 73.1 64.8 58.5 71.7 64.4 62.3 70.5 66.2Regulation 291 24.7 40.5 30.7 26.8 38.2 31.5 28.9 39.8 33.5Positive Regulation 983 35.8 45.4 40.0 34.8 45.8 39.5 38.0 48.3 42.6Negative Regulation 379 37.2 39.7 38.4 37.5 40.9 39.1 35.9 47.2 40.8REG-TOTAL 1653 34.2 43.2 38.2 34.0 43.3 38.0 35.9 46.7 40.6ALL-TOTAL 3182 45.7 57.6 51.0 45.8 57.2 50.9 48.6 59.0 53.3Table 5: Results on the Shared Task test data.
Approximate Span Matching/Approximate Recursive Matching.989only one event profits from trimming of prepositions(Protein Catabolism).
Only the Binding event prof-its significantly from noun phrase modifications (seeTable 3).
The increase in F-score for trimming pro-cedures is 4.1 percentage points for Binding events.In our final experiment we connected the best con-figurations for each of the BioNLP?09 events as pre-sented in Table 2.
The overall event extraction re-sults of this final configuration are presented in Ta-bles 4 and 5.
We achieved an increase of 1.9 per-centage points F-score in the overall event extrac-tion compared to the best-performing single parserconfiguration (M+C, CoNLL?07) (see Table 4, ALL-TOTAL).
The reported results on the developmentdata outperform the results of the TOKYO system by2.6 percentage points F-score for all basic events in-cluding Binding events (see Table 4, EVT-TOTAL)and by 0.9 percentage points in the overall event ex-traction task (see Table 4, ALL-TOTAL).On the test data we achieved an F-score similarto the current JULIELab system trained on modifiedCoNLL?07 dependencies from the MST parser (seeTable 5, ALL-TOTAL).18 The results on the offi-cial test data reveal that the performance differencesbetween various parsers may play a much smallerrole than the proper choice of dependency represen-tations.19 Our empirical findings that the best per-formance results could only be achieved by event-specific dependency graph configurations reveal thatthe syntactic representations of different semanticevents vary considerably at the level of dependencygraph complexity and that the automatic predictionof such syntactic structures can vary from one de-pendency parser to the other.6 DiscussionThe evaluation results from Table 1 show that an in-creased F-score is basically due to a better perfor-mance in terms of precision.
For example, the M+Cevaluation results in the Stanford basic mode pro-vide an increased precision by 2 percentage pointscompared to the Stanford ccprocessed mode.
There-fore, we focus here on the analysis of false positives18The current JULIELab system uses event-specific trimmingprocedures on CoNLL?07 dependencies determined on the de-velopment data set (see Buyko et al (2010)).19Trimmed CoNLL dependencies are used in both systemconfigurations.that the JULIELab system extracts in various modes.For the first analysis we took the outputs of thesystems based on the M+C parsing results.
Wescrutinized on the Stanford basic and ccprocessedfalse positives (fps) and we compared the occur-rences of dependency labels in two data sets, namelythe intersection of false positives from both sys-tem modes (set A) and the false positives producedonly by the system with a worse performance (setB, ccprocessed mode).
About 70% of all fps arecontained in set A.
Our analysis revealed that somedependency labels have a higher occurrence in setB, e.g., nsubjpass, prep on, prep with,prep in, prep for, prep as.
Some depen-dency labels occur only in set B such as agent,prep unlike, prep upon.
It seems that col-lapsing some prepositions, such as ?with?, ?in?,?for?, ?as?, ?on?, ?unlike?, ?upon?, does not havea positive effect on the extraction of argument struc-tures.
In a second step, we compared the Stan-ford basic and CoNLL?07 false positive sets.
Thefps of both systems have an intersection of about70%.
We also compared the intersection of fpsbetween two outputs (set A) and the set of addi-tional fps of the system with worse results (Stan-ford basic mode, set B).
The dependency labels suchas abbrev, dep, nsubj, nsubjpass havea higher occurrence in set B than in set A.
This anal-ysis renders evidence that the distinction of nsubjand nsubjpass does not seem to have been prop-erly learned for event extraction.For the second analysis round we took the out-puts of the MST parsing results.
As in the previ-ous experiments, we compared false positives fromtwo mode outputs, here the CoNLL?07 mode andthe CoNLL?07 modified for auxiliaries and coor-dinations mode.
The fps have an intersection of75%.
The dependency labels such as VC, SUBJ,COORD, and IOBJ occur more frequently in the ad-ditional false positives from the CoNLL?07 modethan in the intersection of false positives from bothsystem outputs.
Obviously, the trimming of auxil-iary and coordination structures has a direct positiveeffect on the argument extraction reducing false pos-itive numbers especially with corresponding depen-dency labels in shortest dependency paths.Our analysis of false positives shows that the dis-tinction between active and passive subject labels,990abbreviation labels, as well as collapsing preposi-tions in the Stanford dependencies, could not havebeen properly learned, which consequently leads toan increased rate of false positives.
The trimmingof auxiliary structures and the subsequent coordina-tion collapsing on CoNLL?07 dependencies has in-deed event-specific positive effects on the event ex-traction.The main focus of this work has been on the eval-uation of effects of different dependency graph rep-resentations on the IE task achievement (here thetask of event extraction).
But we also targeted thetask-oriented evaluation of top-performing syntacticparsers.
The results of this work indicate that theGENIA-trained parsers, i.e., M+C parser, the MST,MALT and GDep, are a reasonable basis for achiev-ing state-of-the art performance in biomedical eventextraction.But the choice of the most suitable parser shouldalso take into account its performance in terms ofparsing time.
Cer et al (2010) and Miyao et al(2008) showed in their experiments that native de-pendency parsers are faster than constituency-basedparsers.
When it comes to scaling event extractionto huge biomedical document collections, such asMEDLINE, the selection of a parser is mainly in-fluenced by its run-time performance.
MST, MALTand GDep parsers or the M+C parser with reducedreranking (Cer et al, 2010) would thus be an appro-priate choice for large-scale event extraction underthese constraints.207 ConclusionIn this paper, we investigated the role different de-pendency representations may have on the accom-plishment of the event extraction task as exemplifiedby biological events.
Different representation for-mats (mainly, Stanford vs. CoNLL) were then ex-perimentally compared employing different parsers(Bikel, Charniak+Johnson, GDep, MST, MALT),both constituency based (for the derivative depen-dency mode) as well as dependency based (forthe native dependency mode), considering differenttraining scenarios (newspaper vs. biology domain).From our experiments we draw the conclusion20For large-scale experiments an evaluation of the M+C withreduced reranking should be provided.that the dependency graph representation has a cru-cial impact on the level of achievement of IE taskrequirements.
Surprisingly, the CoNLL?X depen-dencies outperform the Stanford dependencies forfour from six parsers.
With additionally trimmedCoNLL?X dependencies we could achieve an F-score of 50.9% on the official test data and an F-score of 57.2% on the official development data ofthe BioNLP Shared Task on Event Extraction (seeTable 5, ALL-TOTAL).AcknowledgementsThis research was partially funded by the BOOT-STREP project under grant FP6-028099 within the6th Framework Programme (EC), by the CALBCproject under grant FP7-231727 within the 7thFramework Programme (EC), and by the JENAGEproject under grant 0315581D from the GermanMinistry of Education and Research (BMBF) as partof the GERONTOSYS funding initiative.
We alsowant to thank Kenji Sagae (Institute for CreativeTechnologies, University of Southern California) forkindly providing the models of the GDEP parser.ReferencesDaniel M. Bikel.
2004.
Intricacies of Collins?
parsingmodel.
Computational Linguistics, 30(4):479?511.Jari Bjo?rne, Juho Heimonen, Filip Ginter, Antti Airola,Tapio Pahikkala, and Tapio Salakoski.
2009.
Extract-ing complex biological events with rich graph-basedfeature sets.
In Proceedings BioNLP 2009.
Compan-ion Volume: Shared Task on Event Extraction, pages10?18.
Boulder, Colorado, USA, June 4-5, 2009.Sabine Buchholz and Erwin Marsi.
2006.
CoNLL-X Shared Task on multilingual dependency parsing.In CoNLL-X ?
Proceedings of the 10th Conferenceon Computational Natural Language Learning, pages149?164, New York City, N.Y., June 8-9, 2006.Razvan Bunescu, Ruifang Ge, Rohit J. Kate, Edward M.Marcotte, Raymond J. Mooney, Arun K. Ramani, andYuk Wah Wong.
2005.
Comparative experimentson learning information extractors for proteins andtheir interactions.
Artificial Intelligence in Medicine,33(2):139?155.Ekaterina Buyko, Erik Faessler, Joachim Wermter, andUdo Hahn.
2009.
Event extraction from trimmeddependency graphs.
In Proceedings BioNLP 2009.Companion Volume: Shared Task on Event Extrac-991tion, pages 19?27.
Boulder, Colorado, USA, June 4-5,2009.Ekaterina Buyko, Erik Faessler, Joachim Wermter, andUdo Hahn.
2010.
Syntactic simplification and se-mantic enrichment - Trimming dependency graphs forevent extraction.
Computational Intelligence, 26(4).Daniel Cer, Marie-Catherine de Marneffe, Dan Jurafsky,and Chris Manning.
2010.
Parsing to Stanford De-pendencies: Trade-offs between speed and accuracy.In LREC?2010 ?
Proceedings of the 7th InternationalConference on Language Resources and Evaluation,pages 1628?1632.
Valletta, Malta, May 19-21, 2010.Eugene Charniak and Mark Johnson.
2005.
Coarse-to-fine n-best parsing and MaxEnt discriminative rerank-ing.
In ACL?05 ?
Proceedings of the 43rd AnnualMeeting of the Association for Computational Linguis-tics, pages 173?180.
Ann Arbor, MI, USA, 25-30,June, 2005.Andrew B. Clegg and Adrian J. Shepherd.
2005.
Evalu-ating and integrating Treebank parsers on a biomedicalcorpus.
In Proceedings of the ACL 2005 Workshop onSoftware, pages 14?33.
Ann Arbor, MI, USA, June 30,2005.Marie-Catherine de Marneffe, Bill MacCartney, andChristopher D. Manning.
2006.
Generating typeddependency parses from phrase structure parses.
InLREC?2006 ?
Proceedings of the 5th InternationalConference on Language Resources and Evaluation,pages 449?454.
Genoa, Italy, 24-26 May 2006.Richard Johansson and Pierre Nugues.
2007.
Extendedconstituent-to-dependency conversion for English.
InNODALIDA 2007 ?
Proceedings of the 16th NordicConference of Computational Linguistics, pages 105?112.
Tartu, Estonia, May 24-25, 2007.Jin-Dong Kim, Tomoko Ohta, and Jun?ichi Tsujii.
2008.Corpus annotation for mining biomedical events fromliterature.
BMC Bioinformatics, 9(10).Jin-Dong Kim, Tomoko Ohta, Sampo Pyysalo, Yoshi-nobu Kano, and Jun?ichi Tsujii.
2009.
Overview ofBioNLP?09 Shared Task on Event Extraction.
In Pro-ceedings BioNLP 2009.
Companion Volume: SharedTask on Event Extraction, pages 1?9.
Boulder, Col-orado, USA, June 4-5, 2009.Dekang Lin.
1998.
Dependency-based evaluation ofMINIPAR.
In Proceedings of the LREC?98 Workshopon the Evaluation of Parsing Systems, pages 48?56.Granada, Spain, 28-30 May 1998.David McClosky.
2010.
Any Domain Parsing: Auto-matic Domain Adaptation for Natural Language Pars-ing.
Ph.D. thesis, Department of Computer Science,Brown University.Ryan T. McDonald, Fernando Pereira, Kiril Ribarov, andJan Hajic.
2005.
Non-projective dependency pars-ing using spanning tree algorithms.
In HLT/EMNLP2005 ?
Proceedings of the Human Language Tech-nology Conference and the Conference on EmpiricalMethods in Natural Language Processing, pages 523?530.
Vancouver, B.C., Canada, October 6-8, 2005.Makoto Miwa, Rune S?tre, Jin-Dong Kim, and Jun?ichiTsujii.
2010.
Event extraction with complex eventclassification using rich features.
Journal of Bioinfor-matics and Computational Biology, 8:131?146.Yusuke Miyao and Jun?ichi Tsujii.
2002.
Maximum en-tropy estimation for feature forests.
In HLT 2002 ?Proceedings of the 2nd International Conference onHuman Language Technology Research, pages 292?297.
San Diego, CA, USA, March 24-27, 2002.Yusuke Miyao, Kenji Sagae, and Jun?ichi Tsujii.
2007.Towards framework-independent evaluation of deeplinguistic parsers.
In Proceedings of the GEAF 2007Workshop, CSLI Studies in Computational LinguisticsOnline, page 21 pages.Yusuke Miyao, Rune S?tre, Kenji Sagae, Takuya Mat-suzaki, and Jun?ichi Tsujii.
2008.
Task-oriented eval-uation of syntactic parsers and their representations.
InACL 2008 ?
Proceedings of the 46th Annual Meetingof the Association for Computational Linguistics: Hu-man Language Technologies, pages 46?54.
Columbus,Ohio, USA, June 15-20, 2008.Claire Ne?dellec.
2005.
Learning Language in Logic:Genic interaction extraction challenge.
In ProceedingsLLL-2005 ?
4th Learning Language in Logic Work-shop, pages 31?37.
Bonn, Germany, August 7, 2005.Joakim Nivre, Johan Hall, and Jens Nilsson.
2007.MALTPARSER: A language-independent system fordata-driven dependency parsing.
Natural LanguageEngineering, 13(2):95?135.Sampo Pyysalo, Filip Ginter, Juho Heimonen, JariBjo?rne, Jorma Boberg, Jouni Jarvinen, and TapioSalakoski.
2007.
BIOINFER: A corpus for informa-tion extraction in the biomedical domain.
BMC Bioin-formatics, 8(50).Kenji Sagae and Jun?ichi Tsujii.
2007.
Dependency pars-ing and domain adaptation with LR models and parserensembles.
In EMNLP-CoNLL 2007 ?
Proceedings ofthe Conference on Empirical Methods in Natural Lan-guage Processing and the Conference on Computa-tional Natural Language Learning, pages 1044?1050,Prague, Czech Republic, June 28-30, 2007.Daniel Sleator and Davy Temperley.
1991.
Parsing En-glish with a link grammar.
Technical report, Depart-ment of Computer Science, CMU.Yuka Tateisi, Akane Yakushiji, and Jun?ichi Tsujii.
2005.Syntax annotation for the GENIA corpus.
In IJC-NLP 2005 ?
Proceedings of the 2nd International JointConference on Natural Language Processing, pages222?227.
Jeju Island, Korea, October 11-13, 2005.992
