Proceedings of the 6th Workshop on Statistical Machine Translation, pages 152?158,Edinburgh, Scotland, UK, July 30?31, 2011. c?2011 Association for Computational LinguisticsThe RWTH System Combination System for WMT 2011Gregor Leusch, Markus Freitag, and Hermann NeyRWTH Aachen UniversityAachen, Germany{leusch,freitag,ney}@cs.rwth-aachen.deAbstractRWTH participated in the System Combi-nation task of the Sixth Workshop on Sta-tistical Machine Translation (WMT 2011).For three language pairs, we combined6 to 14 systems into a single consen-sus translation.
A three-level meta-combination scheme combining six dif-ferent system combination setups withthree different engines was applied on theFrench?English language pair.
Depend-ing on the language pair, improvementsversus the best single system are in therange of +1.9% and +2.5% abs.
onBLEU, and between ?1.8% and ?2.4%abs.
on TER.
Novel techniques comparedwith RWTH?s submission to WMT 2010include two additional system combina-tion engines, an additional word alignmenttechnique, meta combination, and addi-tional optimization techniques.1 IntroductionRWTH?s main approach to System Combination(SC) for Machine Translation (MT) is a refinedversion of the ROVER approach in AutomaticSpeech Recognition (ASR) (Fiscus, 1997), withadditional steps to cope with reordering betweendifferent hypotheses, and to use true casing infor-mation from the input hypotheses.
The basic con-cept of the approach has been described by Ma-tusov et al (2006).
Several improvements havebeen added later (Matusov et al, 2008).
This ap-proach includes an enhanced alignment and re-ordering framework.
In contrast to existing ap-proaches (Jayaraman and Lavie, 2005; Rosti etal., 2007b), the context of the whole corpus ratherthan a single sentence is considered in this itera-tive, unsupervised procedure, yielding a more reli-able alignment.
Majority voting on the generatedlattice is performed using prior weights for eachsystem as well as other statistical models suchas a special n-gram language model.
True cas-ing is considered a separate step in RWTH?s ap-proach, which also takes the input hypotheses intoaccount.
The pipeline, and consequently the de-scription of the main pipeline given in this paper, isbased on our pipeline for WMT 2010 (Leusch andNey, 2010), with extensions as described.
Whennecessary, we denote this pipeline as Align-to-Lattice, or A2L .For the French?English task, we used two ad-ditional system combination engines for the firsttime: The first one uses the same alignments asA2L, but generates lattices in the OpenFST frame-work (Allauzen et al, 2007).
The OpenFST de-coder (fstshortestpath) is then used to findthe best path (consensus translation) in this lattice.Analogously, we call this engine A2FST .
The sec-ond additional engine, which we call SCUNC, usesa TER-based alignment, similar to the approach byRosti et al (2007b).
Instead of a lattice rescor-ing, finding the consensus translation is consid-ered a per-node classification problem: For eachslot, which one is the ?correct?
one (i.e.
will givethe ?best?
output)?
This approach is inspired byiROVER (Hillard et al, 2007).
Consensus trans-lations from different settings of these approachescould then be combined again by an additional ap-plication of system combination ?
which we referto as meta combination (Rosti et al, 2007a).
Thesethree approaches are described in more detail inSection 2.
In Section 3 we describe how we tunedthe parameters and decisions of our system combi-nation approaches for WMT 2011.
Section 4 thenlists our experimental setup as well as the experi-mental results we obtained on the WMT 2011 sys-tem combination track.
We conclude this paper inSection 5.2 System Combination Algorithm (A2L)In this section we present the details of our mainsystem combination method, A2L.
The upper partof Figure 1 gives an overview of the system combi-nation architecture described in this section.
Afterpreprocessing the MT hypotheses, pairwise align-152ments between the hypotheses are calculated.
Thehypotheses are then reordered to match the wordorder of a selected primary (skeleton) hypothesis.From this, we create a confusion network (CN)which we then rescore using system prior weightsand a language model (LM).
The single best pathin this CN then constitutes the consensus transla-tion.
The consensus translation is then true casedand post processed.2.1 Word AlignmentThe main proposed alignment approach is a statis-tical one.
It takes advantage of multiple transla-tions for a whole corpus to compute a consensustranslation for each sentence in this corpus.
It alsotakes advantage of the fact that the sentences to bealigned are in the same language.For each of the K source sentences in the testcorpus, we select one of its N translations fromdifferent MT systems E,m=1, .
.
.
, N, as the pri-mary hypothesis.
Then we align the secondary hy-potheses En(n=1, .
.
.
, ;n 6=m) with En to matchthe word order in En.
Since it is not clear whichhypothesis should be primary, i. e. has the ?best?word order, we let several or all hypothesis playthe role of the primary translation, and align allpairs of hypotheses (En, Em); n 6= m.The word alignment is trained in analogy tothe alignment training procedure in statistical MT.The difference is that the two sentences that haveto be aligned are in the same language.
We use theIBM Model 1 (Brown et al, 1993) and the Hid-den Markov Model (HMM, (Vogel et al, 1996))to estimate the alignment model.The alignment training corpus is created from atest corpus of effectively N ?
(N?1) ?K sentencestranslated by the involved MT engines.
Model pa-rameters are trained iteratively using the GIZA++toolkit (Och and Ney, 2003).
The training is per-formed in the directions Em ?
En and En ?Em.
The final alignments are determined usinga cost matrix C for each sentence pair (Em, En).Elements of this matrix are the local costs C(j, i)of aligning a word em,j from Em to a word en,ifrom En.
Following Matusov et al (2004), wecompute these local costs by interpolating thenegated logarithms of the state occupation proba-bilities from the ?source-to-target?
and ?target-to-source?
training of the HMM model.A different approach that has e.g.
been pro-posed by Rosti et al (2007b) is the utilization of aTER alignment (Snover et al, 2006) for this pur-pose.
Because the original TER is insensitive tosmall changes in spellings, synonyms etc., it hasbeen proposed to use more complex variants, e.g.TERp.
For our purposes, we utilized ?poor-man?s-stemming?, i.e.
shortening each word to its firstfour characters when calculating the TER align-ment.
Since a TER alignment already implies areordering between the primary and the secondaryhypothesis, an explicit reordering step is not nec-essary.2.2 Word Reordering and ConfusionNetwork GenerationAfter reordering each secondary hypothesis Emand the rows of the corresponding alignment costmatrix, we determine N ?
1 monotone one-to-onealignments between En as the primary translationand Em,m = 1, .
.
.
, N ;m 6= n. We then con-struct the confusion network.We consider words without a correspondence tothe primary translation (and vice versa) to have anull alignment with the empty word ?, which willbe transformed to an ?-arc in the correspondingconfusion network.The N?1 monotone one-to-one alignments canthen be transformed into a confusion network, asdescribed by Matusov et al (2008).2.3 Voting in the Confusion Network (A2L,A2FST)Instead of choosing a fixed sentence to define theword order for the consensus translation, we gen-erate confusion networks for N possible hypothe-ses as primary, and unite them into a single lattice.In our experience, this approach is advantageousin terms of translation quality compared to a min-imum Bayes risk primary (Rosti et al, 2007b).Weighted majority voting on a single confu-sion network is straightforward and analogous toROVER (Fiscus, 1997).
We sum up the probabil-ities of the arcs which are labeled with the sameword and have the same start state and the sameend state.Compared to A2L, our new A2FST engine al-lows for a higher number of features for each arc.Consequently, we add a binary system feature foreach system in addition to the logarithm of the sumof system weights, as before.
The advantage ofthese features is that the weights are linear withina log-linear model, as opposed to be part of a loga-rithmic sum.
Consequently they can later be opti-mized using techniques designed for linear featureweights, such as MERT, or MIRA.2.4 Language ModelsThe lattice representing a union of several confu-sion networks can then be directly rescored withan n-gram language model (LM).
When regarding153alignmentGIZA++-/TER- Network generation Weighting&Rescoring& ReorderingHyp 1Hyp k... ConsensusTranslationCreatingClassificationProblem& FeaturesClassificationwithin eachslot ConsensusTranslationA2L, A2FSTSCUNCShortestPathPath of"recognized"arcsFigure 1: The system combination architecture.the lattice as a weighted Finite State Transducer(FST), this can be regarded (and implemented) ascomposition with a LM FST.In our approach, we train a trigram LM on theoutputs of the systems involved in system combi-nation.
For LM training, we take the system hy-potheses for the same test corpus for which theconsensus translations are to be produced.
Usingthis ?adapted?
LM for lattice rescoring thus givesbonus to n-grams from the original system hy-potheses, in most cases from the original phrases.Presumably, many of these phrases have a correctword order.
Previous experimental results showthat using this LM in rescoring together with aword penalty notably improves translation quality.This even results in better translations than usinga ?classical?
LM trained on a monolingual train-ing corpus.
We attribute this to the fact that mostof the systems we combine already include suchgeneral LMs.
Nevertheless, one of the SC systemswe use for the French?English task (IV in Sec-tion 4.1) uses a filtered fourgram LM trained onGigaWord and other constrained training data setsfor this WMT tasks as an additional LM.2.5 Extracting Consensus TranslationsTo generate our consensus translation, we ex-tract the single-best path from the rescored lat-tice, using ?classical?
decoding as in MT.
In A2L,this is implemented as shortest-path decoder on apruned lattice.
In A2FST, we use the OpenFSTfstshortestpath decoder, which does not re-quire a pruning step for lattices of the size and den-sity produced here.2.6 Classification in the Confusion Network(SCUNC)Instead of considering the selection of the con-sensus problem as a shortest-path problem in arescored confusion network, we can treat it insteadas a classification problem: For each slot (set ofoutgoing arcs from one node in a CN), we considerone or more arcs to be ?correct?, and train a clas-sifier to identify these certain arcs.
This is the ideaof the iROVER approach in ASR (Hillard et al,2007).
We call our implementation System Com-bination Using N-gram Classifiers, or SCUNC.For the WMT evaluation, we used the ICSI-Boost framework (Favre et al, 2007) as classifier(in binary mode, i.e.
giving a yes/no-decision foreach single arc).
We generated 109 features from8 families: Pairwise equality of words from dif-ferent systems, Number of votes for a word, wordthat would win a simple majority voting, emptyword (also in previous two arcs), position at be-ginning or end of sentence, cross-BLEU-S scoreof hypothesis, equality of system with system oflast slot, and SRILM uni- to trigram scores.
Asthis approach requires strict CN instead of lattices,a union of CNs for different primary hypotheseswas no longer possible.
We decided to selecta fixed single primary system; other approacheswould have been to train an additional classifierfor this purpose, or to select a minimum-Bayes-risk (MBR) skeleton.2.7 Consensus True CasingPrevious approaches to achieve true cased outputin system combination operated on true-cased lat-tices, used a separate input-independent true caser,or used a general true-cased LM to differentiatebetween alternative arcs in the lattice, as describedby Leusch et al (2009).
For WMT 2011, we useper-sentence information from the input systemsto determine the consensus case of each outputword.
Lattice generation, rescoring, and rerank-ing are performed on lower-cased input, with alower-cased consensus hypothesis as their result.For each word in this hypothesis, we count howoften each casing variant occurs in the input hy-potheses for this sentence.
We then use the vari-ant with the highest support for the final consensusoutput.154Table 1: Corpus and Task statistics.avg.
# words #sysTUNE DEV TESTFR?EN 15670 11410 49832 25DE?EN 15508 10878 49395 24ES?EN 15989 11234 50612 15# sent 609 394 20003 Tuning3.1 Feature weightsFor lattice rescoring, we selected a linear combi-nation of BLEU (Papineni et al, 2002) and TER(Snover et al, 2006) as optimization criterion,??
:= argmax?
{BLEU ?
TER} for the A2Lengine, based on previous experience (Mauser etal., 2008).
To achieve more stable results, we usethe case-insensitive variants for both measures, de-spite the explicit use of case information in thepipeline.
System weights were tuned to this cri-terion using the Downhill Simplex method.In the A2FST setup, we were able to generatefull lattices, with separate costs for each individualfeature on all arcs (Power Semiring).
This allowedus to run Lattice MERT (Macherey et al, 2008)on the full lattice, with no need for pruning (andthus additional outer iterations for re-generatinglattices).
We tried different strategies ?
randomlines vs axis-parallel lines, regularization, randomrestarts, etc, and selected the most stable resultson TUNE and DEV for this engine.
Optimizationcriterion here was BLEU.3.2 Training a classifier for SCUNCIn MT system combination, even with given refer-ence translations, there is no simple way to iden-tify the ?correct?
arc in a slot.
This renders aclassifier-based approach even more difficult thaniROVER in ASR.
The problem is even aggravatedbecause both the alignment of words, and their or-der, can be incorrect already in the CN.
We thusconsider an arc to be ?correct?
within this task ex-actly if it gives us the best possible total BLEU-Sscore.1 These ?correct?
arcs, which lie on such an?oracle path?
for BLEU-S, were therefore used asreference classes when training the classifier.3.3 System SelectionWith the large numbers of input systems ?
e.g.,25 for FR?EN ?
and their large spread in transla-tion quality ?
e.g.
from 22.2 to 31.4% in BLEU?
not all systems should participate in the system1We are looking at the sentence level, so we use BLEU-S (Lin and Och, 2004) instead of BLEUcombination process.
This is especially the casesince several of these e.g.
25 systems are oftenonly small variants of each other (contrastive vs.primary submissions), which leads to a low vari-ability of these translations.
We considered severalvariants of the set of input systems, often startingfrom the top, and either replacing some of the sys-tems very similar to others with systems furtherdown the list, or not considering those as primary,adding further systems as additional secondaries.Depending on the engine we were using, we se-lected between 6 and 14 different systems as input.4 Experimental ResultsEach language pair in WMT 2011 had its own setof systems, so we selected and tuned separately foreach language pair .
Due to time constraints, weonly participated in tasks with English as the targetlanguage.
In preliminary experiments, it turnedout that System Combination was not able to geta better result than the best single system on theCzech?English task.
Consequently, we focusedon the language pairs French?English, German?English, and Spanish?English.We split the available tuning data document-wise into a 609-line TUNE set (for tuning), and a394-line DEV set (to verify tuning results).
Morestatistics on these sets can be found in Table 1.Unfortunately, late in the evaluation campaignit turned out that the quality of several referencesentences used in TUNE and DEV was rather low:Many reference sentences contained spelling er-rors, a few dozen lines even contained Frenchphrases or sentences within or after the Englishtext.
We corrected many of these errors manuallyin the references.
In total 101 of 690 lines (16.6%)in TUNE and 58 of 394 lines (14.7%) in DEVwere affected by this.
While it was too late to re-run all of the optimization runs, we re-optimizedat least a few final systems.
All scores within thissection were calculated on the corrected referencetranslations.4.1 FR?ENFor French?English, we built in total seven differ-ent system combination setups to generate a singleconsensus translation and two contrastive transla-tions.
Figure 2 shows the structure and the dataflow of our setup for FR?EN.
Table 2 lists moredetails about the individual engines.Our primary submission was focused on our ex-perience that while rule-based MT systems (suchas RBMT-1..5 and systran) tend to havelower BLEU scores than statistical (SMT) sys-tems, they usually give considerable improve-155aalignm aaalietGIt aZlignAe+-i/T Zlignm ZalignAe+cmu-denkowskicmu-hannemancu-zemanjhukitlia-liglimsiliumonline-Aonline-Brwth-hucksystranudeinrbmt-1rbmt-2rbmt-3rbmt-4rbmt-5alignmZaalignmprimary contrastive 2contrastive 1Bold arrows denote a system that is always considered as skeleton.Note that there are two variants of setup II, see text.Figure 2: System combination pipelines for FR?ENTable 2: Engines and input systems for FR?EN.Engine # Input submitted?I A2L 6 RBMTII A2L I + 6 primaryII?
A2L fix I + 6 for VIIIII SCUNC 6IV A2FST GW, 8V A2L 10 contrastive-2VI A2FST 14VII A2L II?
?VI contrastive-1?GW?
means a 4-gram LM trained on GigaWord.II uses all skeletons, II?
uses I as fixed skeleton.Table 3: Results for FR?EN.TUNE DEVBLEU TER BLEU TERkit 31.56 50.15 30.25 52.88systran 28.18 53.32 26.50 56.07I 27.37 54.73 26.72 57.73II 33.69 48.47 32.45 51.09II?
33.39 48.77 31.81 51.57III 32.74 48.06 31.88 50.87IV 34.16 48.31 31.95 51.64V 33.17 48.95 32.60 51.14VI 33.86 48.69 31.56 52.25VII 34.41 48.20 32.15 51.49kit is the best single system.systran is the best single rule-based system.All scores are case insensitive, and were calculated on thecorrected reference translations.ments to the latter in a SC setup.
Here, though,the number of such systems was too high to sim-ply add them to a reasonable set of SMT systems.Consequently, we first built a SC system (I) com-bining all RBMT/Systran systems, and then a sec-ond SC system (II) which combines the outputof I, and 6 SMT systems.
As further experi-ments showed, allowing all hypotheses as primary(or skeleton) gave significantly better scores thanforcing SC to use the output of I as primary only.But vice versa, when looking at the meta combi-nation scheme, VII, using I as primary only (asetup which we will now denote as II?)
gavemeasurable improvements in the overall transla-tion quality.
We assume this is due to the similarityof the output of II with that of the other setups.Setup III is a SCUNC setup, that is, we builta single CN for each sentence using poor-man?s-stemming-TER, with rwth-huck as primary hy-pothesis.
We then generated a large number of fea-tures for each arc, and trained an ICSIBoost clas-sifier to recognize the arc (or system) that gave thebest BLEU-S score.
This then gave us the consen-sus translation.For IV, we built an OpenFST lattice out of eightsystems, and rescored it with both the Hypothe-sis LM (3-gram), and a 4-gram LM trained on Gi-gaWord and other WMT constrained training datafor this task.
The log-linear weights were trainedusing lattice MERT for BLEU.
Setup V is a clas-sical A2L setup, using ten different input systems.This setup was tuned on BLEU ?
TER using theDownhill-Simplex algorithm.
In setup VI, againthe A2FST engine was used, this time using theHyp LM only, without an additional LM.
Tuning156Table 4: Results for DE?EN.TUNE DEVBLEU TER BLEU TERonline-B 23.13 60.15 26.20 57.20Primary 24.57 58.51 28.11 54.834 best sys 23.85 58.22 27.47 54.966 best sys 24.46 57.74 27.82 54.50online-B is the best single system.was also performed using lattice MERT towardsBLEU.
And finally, setup VII combines the out-put of II?
to IV using the A2L engine again.All the results of system combination on TUNEand DEV are listed in Table 3.
It turns out thatwith the exception of I, all system combinationapproaches were able to achieve a significant im-provement of at least +1.8% abs.
in BLEU com-pared to the best input system.
For I, we needto keep in mind that all other systems were sev-eral BLEU points worse than the best one ?
a sce-nario where we can expect system combination,which is based on the consensus translation afterall, to underperform.
We also see that both A2FSTand SCUNC, with their large number of features,show a tendency to overfitting ?
we see large im-provements on TUNE, but significantly smallerimprovements on DEV.
This tendency is, unfortu-nately, also the case for meta combination: Whilewe see an additional +0.3% abs.
in BLEU overthe best first-level system combination on TUNE,this improvement does not reflect in the scores onDEV: While we still see a +0.2% abs.
improve-ment in BLEU over the setup that performed beston TUNE, there is even a small deterioration of?0.4% in BLEU over the setup that performedbest on DEV.
Because of this effect, we decided tosubmit our meta combination output only as firstcontrastive, and the output that performed wellboth on TUNE and DEV as our primary submis-sion for WMT.
As second contrastive submission,we selected the setup that performed best on DEV.4.2 DE?EN24 systems were available in the German?Englishlanguage pair, but incorporating only 7 of themturned out to deliver optimal results on DEV.
Weran experiments on several settings of systems,but only in our tried and tested A2L framework.We settled for a combination of seven systems(online-B,cmu-dyer,dfki-xu,limsi,online-A,rwth-wuebker,kit) as primarysubmission.
Table 4 also lists two different set-tings.
One setting consists of the four best systemsTable 5: Results for ES?EN.TUNE DEVBLEU TER BLEU TERonline-A 30.58 51.69 30.77 51.95Primary 34.29 48.47 33.41 49.71Contrastive 34.23 48.27 33.30 49.51online-A is the best single system.
(online-B,cmu-dyer,rwth-wuebker,kit) and the other setting contains the six bestsystems (online-B,cmu-dyer,dfki-xu,rwth-wuebker,online-A,kit).
When weadded more systems to system combination, welost performance in both TUNE and DEV.4.3 ES?ENFor Spanish?English, we tried several settingsof systems.
We sticked to our tried and testedA2L framework.
We settled for a combinationof six systems (alacant,koc,online-A,online-B,rbmt-1,systran) as contrastivesubmission, and a combination of ten systems(+rbmt-2,rbmt-3,rbmt-4,udein) as pri-mary submission.
Table 5 lists the results for thistask.
The difference between our primary setup(10 systems) and our contrastive setup (6 systems)is rather small, less than 0.1% abs.
in BLEU.
Nev-ertheless, we see significant improvements overthe best single system of +2.4% abs.
in BLEU,and ?2.2% in TER.5 ConclusionsWe have shown that our system combination ap-proach leads to significant improvements over sin-gle best MT output where a significant number ofcomparably good translations is available on a sin-gle language pair.
A meta combination can giveadditional improvement, but can be sensitive tooverfitting; so in some cases, using one of its in-put system combination hypothesis may be a bet-ter choice.
In any way, both of our new engineshave shown that they can compete with our presentapproach, so we hope to make good use of the newpossibilities they may offer.AcknowledgmentsThis work was partly realized as part of theQuaero Programme, funded by OSEO, FrenchState agency for innovation.
This work waspartly supported by the Defense Advanced Re-search Projects Agency (DARPA) under ContractNo.
HR0011-06-C-0023.157ReferencesC.
Allauzen, M. Riley, J. Schalkwyk, W. Skut, andM.
Mohri.
2007.
OpenFst: A general and efficientweighted finite-state transducer library.
In Proc.
ofthe Twelfth International Conference on Implemen-tation and Application of Automata (CIAA), volume4783 of Lecture Notes in Computer Science, pages11?23.
Springer.P.
F. Brown, S. A. Della Pietra, V. J. Della Pietra, andR.
L. Mercer.
1993.
The mathematics of statisticalmachine translation: parameter estimation.
Compu-tational Linguistics, 19(2):263?311, June.B.
Favre, D. Hakkani-Tu?r, and S. Cuendet.
2007.Icsiboost.
http://code.google.come/p/icsiboost.J.
Fiscus.
1997.
A post-processing system to yield re-duced word error rates: Recognizer output voting er-ror reduction (ROVER).
In IEEE Workshop on Au-tomatic Speech Recognition and Understanding.D.
Hillard, B. Hoffmeister, M. Ostendorf, R. Schlu?ter,and H. Ney.
2007. iROVER: improving sys-tem combination with classification.
In HumanLanguage Technologies 2007: The Conference ofthe North American Chapter of the Associationfor Computational Linguistics; Companion Volume,Short Papers, NAACL-Short ?07, pages 65?68.
As-sociation for Computational Linguistics.S.
Jayaraman and A. Lavie.
2005.
Multi-engine ma-chine translation guided by explicit word matching.In Proc.
of the 10th Annual Conf.
of the EuropeanAssociation for Machine Translation (EAMT), pages143?152, Budapest, Hungary, May.G.
Leusch and H. Ney.
2010.
The rwth system com-bination system for wmt 2010.
In ACL 2010 JointFifth Workshop on Statistical Machine Translationand Metrics MATR, pages 315?320, Uppsala, Swe-den, July.G.
Leusch, E. Matusov, and H. Ney.
2009.
TheRWTH system combination system for WMT 2009.In Fourth Workshop on Statistical Machine Transla-tion, pages 56?60, Athens, Greece, March.
Associa-tion for Computational Linguistics.C.
Y. Lin and F. J. Och.
2004.
Orange: a method forevaluation automatic evaluation metrics for machinetranslation.
In Proc.
COLING 2004, pages 501?507,Geneva, Switzerland, August.W.
Macherey, F. Och, I. Thayer, and J. Uszkoreit.2008.
Lattice-based minimum error rate training forstatistical machine translation.
In Proc.
of the 2008Conference on Empirical Methods in Natural Lan-guage Processing (EMNLP), pages 725?734.
Asso-ciation for Computational Linguistics.E.
Matusov, R. Zens, and H. Ney.
2004.
Symmetricword alignments for statistical machine translation.In COLING ?04: The 20th Int.
Conf.
on Computa-tional Linguistics, pages 219?225, Geneva, Switzer-land, August.E.
Matusov, N. Ueffing, and H. Ney.
2006.
Computingconsensus translation from multiple machine trans-lation systems using enhanced hypotheses align-ment.
In Conference of the European Chapter of theAssociation for Computational Linguistics (EACL),pages 33?40, Trento, Italy, April.E.
Matusov, G. Leusch, R. E. Banchs, N. Bertoldi,D.
Dechelotte, M. Federico, M. Kolss, Y. S. Lee,J.
B. Marino, M. Paulik, S. Roukos, H. Schwenk,and H. Ney.
2008.
System combination for machinetranslation of spoken and written language.
IEEETransactions on Audio, Speech and Language Pro-cessing, 16(7):1222?1237, September.A.
Mauser, S. Hasan, and H. Ney.
2008.
Automaticevaluation measures for statistical machine transla-tion system optimization.
In International Confer-ence on Language Resources and Evaluation, Mar-rakech, Morocco, May.F.
J. Och and H. Ney.
2003.
A systematic comparisonof various statistical alignment models.
Computa-tional Linguistics, 29(1):19?51, March.K.
Papineni, S. Roukos, T. Ward, and W. J. Zhu.
2002.BLEU: a Method for Automatic Evaluation of Ma-chine Translation.
In Proc.
of the 40th Annual Meet-ing of the Association for Computational Linguistics(ACL), pages 311?318, Philadelphia, PA, July.A.
V. Rosti, N. F. Ayan, B. Xiang, S. Matsoukas, R. M.Schwartz, and B. J. Dorr.
2007a.
Combining out-puts from multiple machine translation systems.
InHLT-NAACL?07, pages 228?235.A.
V. Rosti, S. Matsoukas, and R. Schwartz.
2007b.Improved word-level system combination for ma-chine translation.
In Proc.
of the 45th Annual Meet-ing of the Association of Computational Linguis-tics (ACL), pages 312?319, Prague, Czech Republic,June.M.
Snover, B. Dorr, R. Schwartz, L. Micciulla, andJ.
Makhoul.
2006.
A Study of Translation ErrorRate with Targeted Human Annotation.
In Proc.
ofthe 7th Conf.
of the Association for Machine Trans-lation in the Americas (AMTA), pages 223?231,Boston, MA, August.S.
Vogel, H. Ney, and C. Tillmann.
1996.
HMM-based word alignment in statistical translation.
InCOLING ?96: The 16th Int.
Conf.
on ComputationalLinguistics, pages 836?841, Copenhagen, Denmark,August.158
