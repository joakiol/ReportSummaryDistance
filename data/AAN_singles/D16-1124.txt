Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1163?1172,Austin, Texas, November 1-5, 2016. c?2016 Association for Computational LinguisticsGeneralizing and Hybridizing Count-based and Neural Language ModelsGraham Neubig?
and Chris Dyer?
?Carnegie Mellon University, USA?Google DeepMind, United KingdomAbstractLanguage models (LMs) are statistical mod-els that calculate probabilities over sequencesof words or other discrete symbols.
Currentlytwo major paradigms for language model-ing exist: count-based n-gram models, whichhave advantages of scalability and test-timespeed, and neural LMs, which often achievesuperior modeling performance.
We demon-strate how both varieties of models can be uni-fied in a single modeling framework that de-fines a set of probability distributions over thevocabulary of words, and then dynamicallycalculates mixture weights over these distri-butions.
This formulation allows us to createnovel hybrid models that combine the desir-able features of count-based and neural LMs,and experiments demonstrate the advantagesof these approaches.11 IntroductionLanguage models (LMs) are statistical models that,given a sentence wI1 := w1, .
.
.
, wI , calculate itsprobability P (wI1).
LMs are widely used in applica-tions such as machine translation and speech recog-nition, and because of their broad applicability theyhave also been widely studied in the literature.
Themost traditional and broadly used language model-ing paradigm is that of count-based LMs, usuallysmoothed n-grams (Witten and Bell, 1991; Chen1Work was performed while GN was at the Nara Institute ofScience and Technology and CD was at Carnegie Mellon Uni-versity.
Code and data to reproduce experiments is available athttp://github.com/neubig/modlmand Goodman, 1996).
Recently, there has been a fo-cus on LMs based on neural networks (Nakamuraet al, 1990; Bengio et al, 2006; Mikolov et al,2010), which have shown impressive improvementsin performance over count-based LMs.
On the otherhand, these neural LMs also come at the cost of in-creased computational complexity at both trainingand test time, and even the largest reported neuralLMs (Chen et al, 2015; Williams et al, 2015) aretrained on a fraction of the data of their count-basedcounterparts (Brants et al, 2007).In this paper we focus on a class of LMs,which we will call mixture of distributions LMs(MODLMs; ?2).
Specifically, we define MODLMsas all LMs that take the following form, calculat-ing the probabilities of the next word in a sentencewi given preceding context c according to a mix-ture of several component probability distributionsPk(wi|c):P (wi|c) =K?k=1?k(c)Pk(wi|c).
(1)Here, ?k(c) is a function that defines the mixtureweights, with the constraint that ?Kk=1 ?k(c) = 1for all c. This form is not new in itself, and widelyused both in the calculation of smoothing coeffi-cients for n-gram LMs (Chen and Goodman, 1996),and interpolation of LMs of various varieties (Je-linek and Mercer, 1980).The main contribution of this paper is to demon-strate that depending on our definition of c, ?k(c),and Pk(wi|c), Eq.
1 can be used to describe not onlyn-gram models, but also feed-forward (Nakamura etal., 1990; Bengio et al, 2006; Schwenk, 2007) and1163recurrent (Mikolov et al, 2010; Sundermeyer et al,2012) neural network LMs (?3).
This observationis useful theoretically, as it provides a single mathe-matical framework that encompasses several widelyused classes of LMs.
It is also useful practically, inthat this new view of these traditional models allowsus to create new models that combine the desirablefeatures of n-gram and neural models, such as:neurally interpolated n-gram LMs (?4.1), whichlearn the interpolation weights of n-grammodels using neural networks, andneural/n-gram hybrid LMs (?4.2), which add acount-based n-gram component to neural mod-els, allowing for flexibility to add large-scaleexternal data sources to neural LMs.We discuss learning methods for these models (?5)including a novel method of randomly dropping outmore easy-to-learn distributions to prevent the pa-rameters from falling into sub-optimal local minima.Experiments on language modeling benchmarks(?6) find that these models outperform baselines interms of performance and convergence speed.2 Mixture of Distributions LMsAs mentioned above, MODLMs are LMs that takethe form of Eq.
1.
This can be re-framed as the fol-lowing matrix-vector multiplication:pc?
= Dc?c?,where pc is a vector with length equal to vocabularysize, in which the jth element pc,j corresponds toP (wi = j|c), ?c is a size K vector that contains themixture weights for the distributions, and Dc is a J-by-K matrix, where element dc,j,k is equivalent tothe probability Pk(wi = j|c).2 An example of thisformulation is shown in Fig.
1.Note that all columns in D represent probabilitydistributions, and thus must sum to one over the Jwords in the vocabulary, and that all ?
must sumto 1 over the K distributions.
Under this condition,the vector pwill represent a well-formed probabilitydistribution as well.
This conveniently allows us to2We omit the subscript c when appropriate.Probabilities p?
Coefficients ??????
???????????????????????????
?p1=d1,1 d1,2 ?
?
?
d1,K ?1p2 d2,1 d2,2 ?
?
?
d2,K ?2... ... ... .
.
.
... ...pJ dJ,1 dJ,2 ?
?
?
dJ,K ?K?
??
?Distribution matrix DFigure 1: MODLMs as linear equationscalculate the probability of a single word wi = j bycalculating the product of the jth row of Dc and ?c?Pk(wi = j|c) = dc,j?c?.In the sequel we show how this formulation can beused to describe several existing LMs (?3) as well asseveral novel model structures that are more power-ful and general than these existing models (?4).3 Existing LMs as Linear Mixtures3.1 n-gram LMs as Mixtures of DistributionsFirst, we discuss how count-based interpolated n-gram LMs fit within the MODLM framework.Maximum likelihood estimation: n-gram mod-els predict the next word based on the previous N -1words.
In other words, we set c = wi?1i?N+1 andcalculate P (wi|wi?1i?N+1).
The maximum-likelihood(ML) estimate for this probability isPML(wi|wi?1i?N+1) = c(wii?N+1)/c(wi?1i?N+1),where c(?)
counts frequency in the training corpus.Interpolation: Because ML estimation as-signs zero probability to word sequences wherec(wii?N+1) = 0, n-gram models often interpolatethe ML distributions for sequences of length 1 to N .The simplest form is static interpolationP (wi|wi?1i?n+1) =N?n=1?S,nPML(wi|wi?1i?n+1).
(2)?S is a vector where ?S,n represents the weightput on the distribution PML(wi|wi?1i?n+1).
This canbe expressed as linear equations (Fig.
2a) by set-ting the nth column of D to the ML distributionPML(wi|wi?1i?n+1), and ?
(c) equal to ?S .1164Probabilities p?
Heuristic interp.
coefficients ??????
???????????????????????????
?p1=d1,1 d1,2 ?
?
?
d1,N ?1p2 d2,1 d2,2 ?
?
?
d2,N ?2... ... ... .
.
.
... ...pJ dJ,1 dJ,2 ?
?
?
dJ,N ?N?
??
?Count-based probabilities PC(wi = j|wi?1i?n+1)(a) Interpolated n-grams as MODLMsProbabilities p?
Result of softmax(NN(c))????
???????????????????????????
?p1=1 0 ?
?
?
0 ?1p2 0 1 ?
?
?
0 ?2... ... ... .
.
.
... ...pJ 0 0 ?
?
?
1 ?J?
??
?J-by-J identity matrix I(b) Neural LMs as MODLMsFigure 2: Interpretations of existing models as mixtures of distributionsStatic interpolation can be improved by calcu-lating ?
(c) dynamically, using heuristics based onthe frequency counts of the context (Good, 1953;Katz, 1987; Witten and Bell, 1991).
These meth-ods define a context-sensitive fallback probability?
(wi?1i?n+1) for order n models, and recursively cal-culate the probability of the higher order modelsfrom the lower order models:P (wi|wi?1i?n+1) = ?
(wi?1i?n+1)P (wi|wi?1i?n+2)+(1?
?(wi?1i?n+1))PML(wi|wi?1i?n+1).
(3)To express this as a linear mixture, we con-vert ?
(wi?1i?n+1) into the appropriate value for?n(wi?1i?N+1).
Specifically, the probability assignedto each PML(wi|wi?1i?n+1) is set to the product of thefallbacks ?
for all higher orders and the probabilityof not falling back (1?
?)
at the current level:?n(wi?1i?N+1) = (1??(wi?1i?n+1))N?n?=n+1?
(wi?1i?n?+1).Discounting: The widely used technique of dis-counting (Ney et al, 1994) defines a fixed discountd and subtracts it from the count of each word beforecalculating probabilities:PD(wi|wi?1i?n+1) = (c(wii?n+1)?
d)/c(wi?1i?n+1).Discounted LMs then assign the remaining probabil-ity mass after discounting as the fallback probability?D(wi?1i?n+1) =1?J?j=1PD(wi = j|wi?1i?n+1),P (wi|wi?1i?n+1) =?D(wi?1i?n+1)P (wi|wi?1i?n+2)+PD(wi|wi?1i?n+1).
(4)In this case, PD(?)
does not add to one, and thus vi-olates the conditions for MODLMs stated in ?2, butit is easy to turn discounted LMs into interpolatedLMs by normalizing the discounted distribution:PND(wi|wi?1i?n+1) =PD(wi|wi?1i?n+1)?Jj=1 PD(wi = j|wi?1i?n+1),which allows us to replace ?(?)
for ?(?)
and PND(?
)for PML(?)
in Eq.
3, and proceed as normal.Kneser?Ney (KN; Kneser and Ney (1995)) andModified KN (Chen and Goodman, 1996) smooth-ing further improve discounted LMs by adjusting thecounts of lower-order distributions to more closelymatch their expectations as fallbacks for higher or-der distributions.
Modified KN is currently the de-facto standard in n-gram LMs despite occasionalimprovements (Teh, 2006; Durrett and Klein, 2011),and we will express it as PKN (?
).3.2 Neural LMs as Mixtures of DistributionsIn this section we demonstrate how neural networkLMs can also be viewed as an instantiation of theMODLM framework.Feed-forward neural network LMs: Feed-forward LMs (Bengio et al, 2006; Schwenk, 2007)are LMs that, like n-grams, calculate the prob-ability of the next word based on the previouswords.
Given context wi?1i?N+1, these words areconverted into real-valued word representation vec-tors ri?1i?N+1, which are concatenated into an over-all representation vector q = ?
(ri?1i?N+1), where?(?)
is the vector concatenation function.
q is thenrun through a series of affine transforms and non-linearities defined as function NN(q) to obtain avector h. For example, for a one-layer neural net-1165Probabilities p?
Result of softmax(NN(c))????
???????????????????????????
?p1=d1,1 d1,2 ?
?
?
d1,N ?1p2 d1,2 d2,2 ?
?
?
d2,N ?2... ... ... .
.
.
... ...pJ dJ,1 dJ,2 ?
?
?
dJ,N ?N?
??
?Count-based probabilities PC(wi = j|wi?1i?n+1)(a) Neurally interpolated n-gram LMsProbabilities p?
Result of softmax(NN(c))????
???????????????????????????
?p1=d1,1 ?
?
?
d1,N 1 ?
?
?
0 ?1p2 d2,1 ?
?
?
d2,N 0 ?
?
?
0 ?2... ... .
.
.
... ... .
.
.
... ...pJ dJ,1 ?
?
?
dJ,N 0 ?
?
?
1 ?J+N?
??
?Count-based probabilities and J-by-J identity matrix(b) Neural/n-gram hybrid LMsFigure 3: Two new expansions to n-gram and neural LMs made possible in the MODLM frameworkwork with a tanh non-linearity we can defineNN(q) := tanh(qWq + bq), (5)where Wq and bq are weight matrix and bias vec-tor parameters respectively.
Finally, the probabil-ity vector p is calculated using the softmax functionp = softmax(hWs + bs), similarly parameterized.As these models are directly predicting p with noconcept of mixture weights ?, they cannot be inter-preted as MODLMs as-is.
However, we can per-form a trick shown in Fig.
2b, not calculating p di-rectly, but instead calculating mixture weights ?
=softmax(hWs + bs), and defining the MODLM?sdistribution matrix D as a J-by-J identity matrix.This is equivalent to defining a linear mixture of JKronecker ?j distributions, the jth of which assignsa probability of 1 to word j and zero to everythingelse, and estimating the mixture weights with a neu-ral network.
While it may not be clear why it is use-ful to define neural LMs in this somewhat round-about way, we describe in ?4 how this opens up pos-sibilities for novel expansions to standard models.Recurrent neural network LMs: LMs usingrecurrent neural networks (RNNs) (Mikolov et al,2010) consider not the previous few words, but alsomaintain a hidden state summarizing the sentence upuntil this point by re-defining the net in Eq.
5 asRNN(qi) := tanh(qiWq + hi?1Wh + bq),where qi is the current input vector and hi?1 is thehidden vector at the previous time step.
This allowsfor consideration of long-distance dependencies be-yond the scope of standard n-grams, and LMs usingRNNs or long short-term memory (LSTM) networks(Sundermeyer et al, 2012) have posted large im-provements over standard n-grams and feed-forwardmodels.
Like feed-forward LMs, LMs using RNNscan be expressed as MODLMs by predicting ?
in-stead of predicting p directly.4 Novel Applications of MODLMsThis section describes how we can use this frame-work of MODLMs to design new varieties of LMsthat combine the advantages of both n-gram andneural network LMs.4.1 Neurally Interpolated n-gram ModelsThe first novel instantiation of MODLMs that wepropose is neurally interpolated n-gram models,shown in Fig.
3a.
In these models, we setD to be thesame matrix used in n-gram LMs, but calculate?
(c)using a neural network model.
As ?
(c) is learnedfrom data, this framework has the potential to allowus to learn more intelligent interpolation functionsthan the heuristics described in ?3.1.
In addition,because the neural network only has to calculate asoftmax over N distributions instead of J vocabu-lary words, training and test efficiency of these mod-els can be expected to be much greater than that ofstandard neural network LMs.Within this framework, there are several designdecisions.
First, how we decide D: do we use themaximum likelihood estimate PML or KN estimateddistributions PKN?
Second, what do we provide asinput to the neural network to calculate the mixtureweights?
To provide the neural net with the sameinformation used by interpolation heuristics used intraditional LMs, we first calculate three features foreach of the N contexts wi?1i?n+1: a binary feature in-dicating whether the context has been observed inthe training corpus (c(wi?1i?n+1) > 0), the log fre-quency of the context counts (log(c(wi?1i?n+1)) or1166zero for unobserved contexts), and the log frequencyof the number of unique words following the context(log(u(wi?1i?n+1)) or likewise zero).
When using dis-counted distributions, we also use the log of the sumof the discounted counts as a feature.
We can alsooptionally use the word representation vector q usedin neural LMs, allowing for richer representation ofthe input, but this may or may not be necessary in theface of the already informative count-based features.4.2 Neural/n-gram Hybrid ModelsOur second novel model enabled by MODLMs isneural/n-gram hybrid models, shown in Fig.
3b.These models are similar to neurally interpolatedn-grams, but D is augmented with J additionalcolumns representing the Kronecker ?j distributionsused in the standard neural LMs.
In this construc-tion, ?
is still a stochastic vector, but its contentsare both the mixture coefficients for the count-basedmodels and direct predictions of the probabilities ofwords.
Thus, the learned LM can use count-basedmodels when they are deemed accurate, and deviatefrom them when deemed necessary.This model is attractive conceptually for severalreasons.
First, it has access to all information usedby both neural and n-gram LMs, and should be ableto perform as well or better than both models.
Sec-ond, the efficiently calculated n-gram counts arelikely sufficient to capture many phenomena nec-essary for language modeling, allowing the neuralcomponent to focus on learning only the phenom-ena that are not well modeled by n-grams, requiringfewer parameters and less training time.
Third, it ispossible to train n-grams from much larger amountsof data, and use these massive models to bootstraplearning of neural nets on smaller datasets.5 Learning Mixtures of DistributionsWhile the MODLM formulations of standard heuris-tic n-gram LMs do not require learning, the remain-ing models are parameterized.
This section dis-cusses the details of learning these parameters.5.1 Learning MODLMsThe first step in learning parameters is defining ourtraining objective.
Like most previous work onLMs (Bengio et al, 2006), we use a negative log-likelihood loss summed over words wi in every sen-tence w in corpusWL(W) = ?
?w?W?wi?wlogP (wi|c),where c represents all words preceding wi inw thatare used in the probability calculation.
As noted inEq.
2, P (wi = j|c) can be calculated efficientlyfrom the distribution matrix Dc and mixture func-tion output ?c.Given that we can calculate the log likelihood, theremaining parts of training are similar to training forstandard neural network LMs.
As usual, we per-form forward propagation to calculate the probabili-ties of all the words in the sentence, back-propagatethe gradients through the computation graph, andperform some variant of stochastic gradient descent(SGD) to update the parameters.5.2 Block Dropout for Hybrid ModelsWhile the training method described in the previ-ous section is similar to that of other neural networkmodels, we make one important modification to thetraining process specifically tailored to the hybridmodels of ?4.2.This is motivated by our observation (detailed in?6.3) that the hybrid models, despite being strictlymore expressive than the corresponding neural net-work LMs, were falling into poor local minima withhigher training error than neural network LMs.
Thisis because at the very beginning of training, thecount-based elements of the distribution matrix inFig.
3b are already good approximations of the tar-get distribution, while the weights of the single-word?j distributions are not yet able to provide accurateprobabilities.
Thus, the model learns to set the mix-ture proportions of the ?
elements to near zero andrely mainly on the count-based n-gram distributions.To encourage the model to use the ?
mixture com-ponents, we adopt a method called block dropout(Ammar et al, 2016).
In contrast to standarddropout (Srivastava et al, 2014), which drops outsingle nodes or connections, block dropout ran-domly drops out entire subsets of network nodes.
Inour case, we want to prevent the network from over-using the count-based n-gram distributions, so for arandomly selected portion of the training examples(here, 50%) we disable all n-gram distributions and1167force the model to rely on only the ?
distributions.To do so, we zero out all elements in ?
(c) that cor-respond to n-gram distributions, and re-normalizeover the rest of the elements so they sum to one.5.3 Network and Training DetailsFinally, we note design details that were determinedbased on preliminary experiments.Network structures: We used both feed-forwardnetworks with tanh non-linearities and LSTM(Hochreiter and Schmidhuber, 1997) networks.Most experiments used single-layer 200-node net-works, and 400-node networks were used for ex-periments with larger training data.
Word repre-sentations were the same size as the hidden layer.Larger and multi-layer networks did not yield im-provements.Training: We used ADAM (Kingma and Ba,2015) with a learning rate of 0.001, and minibatchsizes of 512 words.
This led to faster convergencethan standard SGD, and more stable optimizationthan other update rules.
Models were evaluated ev-ery 500k-3M words, and the model with the best de-velopment likelihood was used.
In addition to theblock dropout of ?5.2, we used standard dropoutwith a rate of 0.5 for both feed-forward (Srivastavaet al, 2014) and LSTM (Pham et al, 2014) nets inthe neural LMs and neural/n-gram hybrids, but notin the neurally interpolated n-grams, where it re-sulted in slightly worse perplexities.Features: If parameters are learned on the dataused to train count-based models, they will heav-ily over-fit and learn to trust the count-based distri-butions too much.
To prevent this, we performed10-fold cross validation, calculating count-based el-ements of D for each fold with counts trained on theother 9/10.
In addition, the count-based contextualfeatures in ?4.1 were normalized by subtracting thetraining set mean, which improved performance.6 Experiments6.1 Experimental SetupIn this section, we perform experiments to eval-uate the neurally interpolated n-grams (?6.2) andneural/n-gram hybrids (?6.3), the ability of our mod-els to take advantage of information from large datasets (?6.4), and the relative performance comparedPTB Sent Word ASP Sent Wordtrain 42k 890k train 100k 2.1Mvalid 3.4k 70k valid 1.8k 45ktest 3.8k 79k test 1.8k 46kTable 1: Data sizes for the PTB and ASPEC corpora.Dst./Ft.
HEUR FF LSTMML/C 220.5/265.9 146.6/164.5 144.4/162.7ML/CR - 145.7/163.9 142.6/158.4KN/C 140.8/156.5 138.9/152.5 136.8/151.1KN/CR - 136.9/153.0 135.2/149.1Table 2: PTB/ASPEC perplexities for traditionalheuristic (HEUR) and proposed neural net (FF orLSTM) interpolation methods using ML or KN dis-tributions, and count (C) or count+word representa-tion (CR) features.to post-facto static interpolation of already-trainedmodels (?6.5).
For the main experiments, we evalu-ate on two corpora: the Penn Treebank (PTB) dataset prepared by Mikolov et al (2010),3 and the first100k sentences in the English side of the ASPECcorpus (Nakazawa et al, 2015)4 (details in Tab.
1).The PTB corpus uses the standard vocabulary of 10kwords, and for the ASPEC corpus we use a vocabu-lary of the 20k most frequent words.
Our implemen-tation is included as supplementary material.6.2 Results for Neurally Interpolated n-gramsFirst, we investigate the utility of neurally interpo-lated n-grams.
In all cases, we use a history ofN = 5 and test several different settings for themodels:Estimation type: ?
(c) is calculated with heuris-tics (HEUR) or by the proposed method using feed-forward (FF), or LSTM nets.Distributions: We compare PML(?)
and PKN (?
).For heuristics, we use Witten-Bell for ML and theappropriate discounted probabilities for KN.Input features: As input features for the neuralnetwork, we either use only the count-based features(C) or count-based features together with the wordrepresentation for the single previous word (CR).From the results shown in Tab.
2, we can first seethat when comparing models using the same set of3http://rnnlm.org/simple-examples.tgz4http://lotus.kuee.kyoto-u.ac.jp/ASPEC/1168input distributions, the neurally interpolated modeloutperforms corresponding heuristic methods.
Wecan also see that LSTMs have a slight advantageover FF nets, and models using word representa-tions have a slight advantage over those that useonly the count-based features.
Overall, the bestmodel achieves a relative perplexity reduction of 4-5% over KN models.
Interestingly, even when usingsimple ML distributions, the best neurally interpo-lated n-gram model nearly matches the heuristic KNmethod, demonstrating that the proposed model canautomatically learn interpolation functions that arenearly as effective as carefully designed heuristics.56.3 Results for Neural/n-gram HybridsIn experiments with hybrid models, we test aneural/n-gram hybrid LM using LSTM networkswith both Kronecker ?
and KN smoothed 5-gramdistributions, trained either with or without blockdropout.
As our main baseline, we compare toLSTMs with only ?
distributions, which have re-ported competitive numbers on the PTB data set(Zaremba et al, 2014).6 We also report results forheuristically smoothed KN 5-gram models, and thebest neurally interpolated n-grams from the previoussection for reference.The results, shown in Tab.
3, demonstrate thatsimilarly to previous research, LSTM LMs (2)achieve a large improvement in perplexity over n-gram models, and that the proposed neural/n-gramhybrid method (5) further reduces perplexity by 10-11% relative over this strong baseline.Comparing models without (4) and with (5) theproposed block dropout, we can see that this methodcontributes significantly to these gains.
To examinethis more closely, we show the test perplexity for the5Neurally interpolated n-grams are also more efficient thanstandard neural LMs, as mentioned in ?4.1.
While a standardLSTM LM calculated 1.4kw/s on the PTB data, the neurally in-terpolated models using LSTMs and FF nets calculated 11kw/sand 58kw/s respectively, only slightly inferior to 140kw/s ofheuristic KN.6Note that unlike this work, we opt to condition only on in-sentence context, not inter-sentential dependencies, as trainingthrough gradient calculations over sentences is more straight-forward and because examining the effect of cross-boundaryinformation is not central to the proposed method.
Thus ourbaseline numbers are not directly comparable (i.e.
have higherperplexity) to previous reported results on this data, but we stillfeel that the comparison is appropriate.Dist.
Interp.
PPL(1) KN HEUR 140.8/156.5(2) ?
LSTM 105.9/116.9(3) KN LSTM 135.2/149.1(4) KN,?
LSTM -BlDO 108.4/130.4(5) KN,?
LSTM +BlDO 95.3 /104.5Table 3: PTB/ASPEC perplexities for traditionalKN (1) and LSTM LMs (2), neurally interpolated n-grams (3), and neural/n-gram hybrid models without(4) and with (5) block dropout.10 100 1000 InftyFrequency Cutoff102103104105Perplexity(1) KN/heur(2) d/LSTM(3) KN/LSTM(4) KN+d/LSTMFigure 4: Perplexities of (1) standard n-grams, (2)standard LSTMs, (3) neurally interpolated n-grams,and (4) neural/n-gram hybrids on lower frequencywords.three models using ?
distributions in Fig.
5, and theamount of the probability mass in ?
(c) assigned tothe non-?
distributions in the hybrid models.
Fromthis, we can see that the model with block dropoutquickly converges to a better result than the LSTMLM, but the model without converges to a worseresult, assigning too much probability mass to thedense count-based distributions, demonstrating thelearning problems mentioned in ?5.2.It is also of interest to examine exactly why theproposed model is doing better than the more stan-dard methods.
One reason can be found in the be-havior with regards to low-frequency words.
In Fig.4, we show perplexities for words that appear ntimes or less in the training corpus, for n = 10,n = 100, n = 1000 and n = ?
(all words).From the results, we can first see that if we com-pare the baselines, LSTM language models achievebetter perplexities overall but n-gram language mod-els tend to perform better on low-frequency words,corroborating the observations of Chen et al (2015).11690 1 2 3 4 5 61e7100120140160Perplexity (1) d/LSTM(2) KN+d/LSTM -BlDO(3) KN+d/LSTM +BlDO0 1 2 3 4 5 6Training Words Processed 1e70.00.51.0DenseRatioFigure 5: Perplexity and dense distribution ratio ofthe baseline LSTM LM (1), and the hybrid methodwithout (2) and with (3) block dropout.The neurally interpolated n-gram models consis-tently outperform standard KN-smoothed n-grams,demonstrating their superiority within this modelclass.
In contrast, the neural/n-gram hybrid mod-els tend to follow a pattern more similar to that ofLSTM language models, similarly with consistentlyhigher performance.6.4 Results for Larger Data SetsTo examine the ability of the hybrid models to usecounts trained over larger amounts of data, we per-form experiments using two larger data sets:WSJ: The PTB uses data from the 1989 WallStreet Journal, so we add the remaining years be-tween 1987 and 1994 (1.81M sents., 38.6M words).GW: News data from the English Gigaword 5thEdition (LDC2011T07, 59M sents., 1.76G words).We incorporate this data either by training net pa-rameters over the whole large data, or by separatelytraining count-based n-grams on each of PTB, WSJ,and GW, and learning net parameters on only PTBdata.
The former has the advantage of training thenet on much larger data.
The latter has two main ad-vantages: 1) when the smaller data is of a particulardomain the mixture weights can be learned to matchthis in-domain data; 2) distributions can be trainedon data such as Google n-grams (LDC2006T13),which contain n-gram counts but not full sentences.In the results of Fig.
6, we can first see that theneural/n-gram hybrids significantly outperform thetraditional neural LMs in the scenario with largerdata as well.
Comparing the two methods for in-corporating larger data, we can see that the resultsare mixed depending on the type and size of the data0 1 2 3 4 5 6Training Words Processed 1e76080100120140160Perplexity(1) d/p(2) KN+d/p(3) d/w(4) KN+d/w(5) KN+d/p +wLM(6) d/g(7) KN+d/g(8) KN+d/p +gLMFigure 6: Models trained on PTB (1,2), PTB+WSJ(3,4,5) or PTB+WSJ+GW (6,7,8) using standardneural LMs (1,3,6), neural/n-gram hybrids trainedall data (2,4,7), or hybrids trained on PTB with ad-ditional n-gram distributions (5,8).being used.
For the WSJ data, training on all dataslightly outperforms the method of adding distribu-tions, but when the GW data is added this trend re-verses.
This can be explained by the fact that theGW data differs from the PTB test data, and thusthe effect of choosing domain-specific interpolationcoefficients was more prominent.6.5 Comparison with Static InterpolationFinally, because the proposed neural/n-gram hybridmodels combine the advantages of neural and n-gram models, we compare with the more standardmethod of training models independently and com-bining them with static interpolation weights tunedon the validation set using the EM algorithm.
Tab.
4shows perplexities for combinations of a standardneural model (or ?
distributions) trained on PTB, andcount based distributions trained on PTB, WSJ, andGW are added one-by-one using the standard staticand proposed LSTM interpolation methods.
Fromthe results, we can see that when only PTB data isused, the methods have similar results, but with themore diverse data sets the proposed method edgesout its static counterpart.77In addition to better perplexities, neural/n-gram hybrids aretrained in a single pass instead of performing post-facto inter-polation, which may give advantages when training for otherobjectives (Auli and Gao, 2014; Li et al, 2015).1170Interp ?+PTB +WSJ +GWLin.
95.1 70.5 65.8LSTM 95.3 68.3 63.5Table 4: PTB perplexity for interpolation betweenneural (?)
LMs and count-based models.7 Related WorkA number of alternative methods focus on interpo-lating LMs of multiple varieties such as in-domainand out-of-domain LMs (Bulyko et al, 2003; Bac-chiani et al, 2006; Gu?lc?ehre et al, 2015).
Perhapsmost relevant is Hsu (2007)?s work on learning tointerpolate multiple LMs using log-linear models.This differs from our work in that it learns functionsto estimate the fallback probabilities ?n(c) in Eq.
3instead of ?
(c), and does not cover interpolation ofn-gram components, non-linearities, or the connec-tion with neural network LMs.
Also conceptuallysimilar is work on adaptation of n-gram LMs, whichstart with n-gram probabilities (Della Pietra et al,1992; Kneser and Steinbiss, 1993; Rosenfeld, 1996;Iyer and Ostendorf, 1999) and adapt them based onthe distribution of the current document, albeit in alinear model.
There has also been work incorpo-rating binary n-gram features into neural languagemodels, which allows for more direct learning of n-gram weights (Mikolov et al, 2011), but does not af-ford many of the advantages of the proposed modelsuch as the incorporation of count-based probabilityestimates.
Finally, recent works have compared n-gram and neural models, finding that neural modelsoften perform better in perplexity, but n-grams havetheir own advantages such as effectiveness in extrin-sic tasks (Baltescu and Blunsom, 2015) and bettermodeling of rare words (Chen et al, 2015).8 Conclusion and Future WorkIn this paper, we proposed a framework for lan-guage modeling that generalizes both neural net-work and count-based n-gram LMs.
This allowedus to learn more effective interpolation functions forcount-based n-grams, and to create neural LMs thatincorporate information from count-based models.As the framework discussed here is general, it isalso possible that they could be used in other tasksthat perform sequential prediction of words such asneural machine translation (Sutskever et al, 2014)or dialog response generation (Sordoni et al, 2015).In addition, given the positive results using blockdropout for hybrid models, we plan to develop moreeffective learning methods for mixtures of sparseand dense distributions.AcknowledgementsWe thank Kevin Duh, Austin Matthews, ShinjiWatanabe, and anonymous reviewers for valuablecomments on earlier drafts.
This work was sup-ported in part by JSPS KAKENHI Grant Number16H05873, and the Program for Advancing Strate-gic International Networks to Accelerate the Circu-lation of Talented Researchers.ReferencesWaleed Ammar, George Mulcaire, Miguel Ballesteros,Chris Dyer, and Noah A. Smith.
2016.
One parser,many languages.
CoRR, abs/1602.01595.Michael Auli and Jianfeng Gao.
2014.
Decoder inte-gration and expected bleu training for recurrent neuralnetwork language models.
In Proc.
ACL, pages 136?142.Michiel Bacchiani, Michael Riley, Brian Roark, andRichard Sproat.
2006.
Map adaptation of stochas-tic grammars.
Computer Speech and Language,20(1):41?68.Paul Baltescu and Phil Blunsom.
2015.
Pragmatic neurallanguage modelling in machine translation.
In Proc.NAACL, pages 820?829.Yoshua Bengio, Holger Schwenk, Jean-Se?bastienSene?cal, Fre?deric Morin, and Jean-Luc Gauvain.2006.
Neural probabilistic language models.
InInnovations in Machine Learning, volume 194, pages137?186.Thorsten Brants, Ashok C. Popat, Peng Xu, Franz J. Och,and Jeffrey Dean.
2007.
Large language models inmachine translation.
In Proc.
EMNLP, pages 858?867.Ivan Bulyko, Mari Ostendorf, and Andreas Stolcke.2003.
Getting more mileage from web text sources forconversational speech language modeling using class-dependent mixtures.
In Proc.
HLT, pages 7?9.Stanley F. Chen and Joshua Goodman.
1996.
An empir-ical study of smoothing techniques for language mod-eling.
In Proc.
ACL, pages 310?318.W.
Chen, D. Grangier, and M. Auli.
2015.
Strategies forTraining Large Vocabulary Neural Language Models.ArXiv e-prints, December.1171Stephen Della Pietra, Vincent Della Pietra, Robert L Mer-cer, and Salim Roukos.
1992.
Adaptive languagemodeling using minimum discriminant estimation.
InProc.
ACL, pages 103?106.Greg Durrett and Dan Klein.
2011.
An empirical investi-gation of discounting in cross-domain language mod-els.
In Proc.
ACL.Irving J Good.
1953.
The population frequencies ofspecies and the estimation of population parameters.Biometrika, 40(3-4):237?264.C?aglar Gu?lc?ehre, Orhan Firat, Kelvin Xu, KyunghyunCho, Lo?
?c Barrault, Huei-Chi Lin, Fethi Bougares,Holger Schwenk, and Yoshua Bengio.
2015.
On us-ing monolingual corpora in neural machine translation.CoRR, abs/1503.03535.Sepp Hochreiter and Ju?rgen Schmidhuber.
1997.
Longshort-term memory.
Neural computation, 9(8):1735?1780.Bo-June Hsu.
2007.
Generalized linear interpolation oflanguage models.
In Proc.
ASRU, pages 136?140.Rukmini M Iyer and Mari Ostendorf.
1999.
Modelinglong distance dependence in language: Topic mixturesversus dynamic cache models.
Speech and Audio Pro-cessing, IEEE Transactions on, 7(1):30?39.Frederick Jelinek and Robert Mercer.
1980.
Interpolatedestimation of markov source parameters from sparsedata.
In Workshop on pattern recognition in practice.Slava M Katz.
1987.
Estimation of probabilities fromsparse data for the language model component of aspeech recognizer.
IEEE Transactions on Acoustics,Speech and Signal Processing, 35(3):400?401.Diederik Kingma and Jimmy Ba.
2015.
Adam: Amethod for stochastic optimization.
Proc.
ICLR.Reinhard Kneser and Hermann Ney.
1995.
Improvedbacking-off for m-gram language modeling.
In Proc.ICASSP, volume 1, pages 181?184.
IEEE.Reinhard Kneser and Volker Steinbiss.
1993.
On thedynamic adaptation of stochastic language models.
InProc.
ICASSP, pages 586?589.Jiwei Li, Michel Galley, Chris Brockett, Jianfeng Gao,and Bill Dolan.
2015.
A diversity-promoting objec-tive function for neural conversation models.
CoRR,abs/1510.03055.Tomas Mikolov, Martin Karafia?t, Lukas Burget, Jan Cer-nocky`, and Sanjeev Khudanpur.
2010.
Recurrent neu-ral network based language model.
In Proc.
Inter-Speech, pages 1045?1048.Toma?s?
Mikolov, Anoop Deoras, Daniel Povey, Luka?s?Burget, and Jan C?ernocky`.
2011.
Strategies for train-ing large scale neural network language models.
InProc.
ASRU, pages 196?201.
IEEE.Masami Nakamura, Katsuteru Maruyama, TakeshiKawabata, and Kiyohiro Shikano.
1990.
Neural net-work approach to word category prediction for Englishtexts.
In Proc.
COLING.Toshiaki Nakazawa, Hideya Mino, Isao Goto, GrahamNeubig, Sadao Kurohashi, and Eiichiro Sumita.
2015.Overview of the 2nd Workshop on Asian Translation.In Proc.
WAT.Hermann Ney, Ute Essen, and Reinhard Kneser.
1994.On structuring probabilistic dependences in stochasticlanguage modelling.
Computer Speech and Language,8(1):1?38.Vu Pham, The?odore Bluche, Christopher Kermorvant,and Je?ro?me Louradour.
2014.
Dropout improves re-current neural networks for handwriting recognition.In Proc.
ICFHR, pages 285?290.Ronald Rosenfeld.
1996.
A maximum entropy approachto adaptive statistical language modelling.
ComputerSpeech and Language, 10(3):187?228.Holger Schwenk.
2007.
Continuous space languagemodels.
Computer Speech and Language, 21(3):492?518.Alessandro Sordoni, Michel Galley, Michael Auli, ChrisBrockett, Yangfeng Ji, Margaret Mitchell, Jian-YunNie, Jianfeng Gao, and Bill Dolan.
2015.
A neu-ral network approach to context-sensitive generationof conversational responses.
In Proc.
NAACL, pages196?205.Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky,Ilya Sutskever, and Ruslan Salakhutdinov.
2014.Dropout: A simple way to prevent neural networksfrom overfitting.
The Journal of Machine LearningResearch, 15(1):1929?1958.Martin Sundermeyer, Ralf Schlu?ter, and Hermann Ney.2012.
LSTM neural networks for language modeling.In Proc.
InterSpeech.Ilya Sutskever, Oriol Vinyals, and Quoc VV Le.
2014.Sequence to sequence learning with neural networks.In Proc.
NIPS, pages 3104?3112.Yee Whye Teh.
2006.
A Bayesian interpretation of in-terpolated Kneser-Ney.
Technical report, School ofComputing, National Univ.
of Singapore.Will Williams, Niranjani Prasad, David Mrva, Tom Ash,and Tony Robinson.
2015.
Scaling recurrent neuralnetwork language models.
In Proc.
ICASSP.Ian H. Witten and Timothy C. Bell.
1991.
The zero-frequency problem: Estimating the probabilities ofnovel events in adaptive text compression.
IEEETransactions on Information Theory, 37(4):1085?1094.Wojciech Zaremba, Ilya Sutskever, and Oriol Vinyals.2014.
Recurrent neural network regularization.CoRR, abs/1409.2329.1172
