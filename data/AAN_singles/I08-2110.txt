SYNGRAPH: A Flexible Matching Method based on SynonymousExpression Extraction from an Ordinary Dictionary and a Web CorpusTomohide Shibata?, Michitaka Odani?, Jun Harashima?,Takashi Oonishi?
?, and Sadao Kurohashi?
?Kyoto University, Yoshida-honmachi, Sakyo-ku, Kyoto, 606-8501, Japan?
?NEC Corporation, 1753, Shimonumabe, Nakahara-Ku, Kawasaki, Kanagawa 211-8666, Japan{shibata,odani,harashima,kuro}@nlp.kuee.kyoto-u.ac.jpt-onishi@bq.jp.nec.comAbstractThis paper proposes a flexible matchingmethod that can assimilate the expressivedivergence.
First, broad-coverage syn-onymous expressions are automatically ex-tracted from an ordinary dictionary, andamong them, those whose distributionalsimilarity in a Web corpus is high are usedfor the flexible matching.
Then, to overcomethe combinatorial explosion problem in thecombination of expressive divergence, an IDis assigned to each synonymous group, andSYNGRAPH data structure is introduced topack the expressive divergence.
We con-firmed the effectiveness of our method onexperiments of machine translation and in-formation retrieval.1 IntroductionIn natural language, many expressions have almostthe same meaning, which brings great difficulty tomany NLP tasks, such as machine translation (MT),information retrieval (IR), and question answering(QA).
For example, suppose an input sentence (1) isgiven to a Japanese-English example-based machinetranslation system.
(1) hotel nihotelichibanbestchikaineareki wastationdoko-desukawhere isEven if a very similar translation example (TE)?
(2-a) ?
(2-b)?
exists in the TEs, a simple exactmatching method cannot utilize this example for thetranslation.
(2) a. ryokan noJapanese hotelmoyori nonearesteki wastationdoko-desukawhere isb.
Where?s the nearest station to the hotel?How to handle these synonymous expressions hasbecome one of the important research topics in NLP.This paper presents a flexible matching method,which can assimilate the expressive divergence, tosolve this problem.
This method has the followingtwo features:1.
Synonymy relations and hypernym-hyponymrelations are automatically extracted from anordinary dictionary and a Web corpus.2.
Extracted synonymous expressions are effec-tively handled by SYNGRAPH data structure,which can pack the expressive divergence.An ordinary dictionary is a knowledge sourceto provide synonym and hypernym-hyponym rela-tions (Nakamura and Nagao, 1988; Tsurumaru et al,1986).
A problem in using synonymous expressionsextracted from a dictionary is that some of them arenot appropriate since they are rarely used.
For exam-ple, a synonym pair ?suidou?1 = ?kaikyou(strait)?
isextracted.Recently, some work has been done on corpus-based paraphrase extraction (Lin and Pantel, 2001;Barzilay and Lee, 2003).
The basic idea of theirmethods is that two words with similar meaningsare used in similar contexts.
Although their methodscan obtain broad-coverage paraphrases, the obtainedparaphrases are not accurate enough to be utilized1This word usually means ?water supply?.787for achieving precise matching since they containsynonyms, near-synonyms, coordinate terms, hyper-nyms, and inappropriate synonymous expressions.Our approach makes the best use of an ordi-nary dictionary and a Web corpus to extract broad-coverage and precise synonym and hypernym-hyponym expressions.
First, synonymous expres-sions are extracted from a dictionary.
Then, thedistributional similarity of a pair of them is calcu-lated using a Web corpus.
Among extracted syn-onymous expressions, those whose similarity is highare used for the flexible matching.
By utilizing onlysynonymous expressions extracted from a dictionarywhose distributional similarity is high, we can ex-clude synonymous expressions extracted from a dic-tionary that are rarely used, and the pair of wordswhose distributional similarity is high that is not ac-tually a synonymous expression (is not listed in adictionary).Another point of our method is to introduce SYN-GRAPH data structure.
So far, the effectivenessof handling expressive divergence has been shownfor IR using a thesaurus-based query expansion(Voorhees, 1994; Jacquemin et al, 1997).
However,their methods are based on a bag-of-words approachand thus does not pay attention to sentence-levelsynonymy with syntactic structure.
MT requiressuch precise handling of synonymy, and advancedIR and QA also need it.
To handle sentence-levelsynonymy precisely, we have to consider the combi-nation of expressive divergence, which may causecombinatorial explosion.
To overcome this prob-lem, an ID is assigned to each synonymous group,and then SYNGRAPH data structure is introducedto pack expressive divergence.2 Synonymy DatabaseThis section describes a method for constructing asynonymy database.
First, synonym/hypernym re-lations are automatically extracted from an ordinarydictionary, and the distributional similarity of a pairof synonymous expressions is calculated using aWeb corpus.
Then, the extracted synonymous ex-pressions whose similarity is high are used for theflexible matching.2.1 Synonym/hypernym Extraction from anOrdinary DictionaryAlthough there were some attempts to extract syn-onymous expressions from a dictionary (Nakamuraand Nagao, 1988; Tsurumaru et al, 1986), they ex-tracted only hypernym-hyponym relations from thelimited entries.
In contrast, our method extracts notonly hypernym-hyponym relations, but also basicsynonym relations, predicate synonyms, adverbialsynonyms and synonym relations between a wordand a phrase.The last word of the first definition sentence isusually the hypernym of an entry word.
Some defi-nition sentences in a Japanese dictionary are shownbelow (the left word of ?:?
is an entry word, the rightsentence is a definition, and words in bold font is theextracted words):yushoku (dinner) : yugata (evening) no (of)shokuji (meal).jushin (barycenter) : omosa (weight) ga (is)tsuriatte (balance) tyushin (center) tonaru(become) ten (spot).For example, the last word shokuji (meal) can beextracted as the hypernym of yushoku (dinner).
Insome cases, however, a word other than the last wordcan be a hypernym or synonym.
These cases can bedetected by sentence-final patterns as follows (theunderlined expressions represent the patterns):Hypernymsdosei (Saturn) : wakusei (planet) no (of) hitotsu(one).tobi (kite) : taka (hawk) no (of) issyu (kind).Synonyms / Synonymous Phrasesice : ice cream no (of) ryaku (abbreviation).mottomo (most) : ichiban (best).
(?
one word defi-nition)moyori (nearest) : ichiban (best) chikai (near)tokoro (place)2.
(?
less than three phrases)2.2 Calculating the Distributional Similarityusing a Web CorpusThe similarity between a pair of synonymous ex-pressions is calculated based on distributional sim-ilarity (J.R.Firth, 1957; Harris, 1968) using theWeb corpus collected by (Kawahara and Kurohashi,2006).
The similarity between two predicates is de-fined to be one between the patterns of case exam-ples of each predicate (Kawahara and Kurohashi,2001).
The similarity between two nouns are defined2If the last word of a sentence is a highly general term suchas koto (thing) and tokoro (place), it is removed from the syn-onymous expression.788gakkou (school)gakue n (a ca d e m y )<school>s h ogakkou (p r i m a r y  school)s h ogaku (e le m e n t a r y  school)<p r i m a r y  school>koukou (hi g h school)kout ougakkou (se n i or  hi g h)<hi g h school>t okor o (p la ce )<p la ce >h an t e n (b lob )m ad ar a (m ot t le )b uc h i (m a cu la )<b lob >t e n  (sp ot )<sp ot >j us h i n (b a r y ce n t e r )<b a r y ce n t e r > m oy or i (n e a r e st ) i c h i b an (b e st )  c h i kaku(n e a r )<n e a r e st >m ot t om o (m ost )i c h i b an (b e st )<m ost >p oly se m i c w or dhy p e r n y m -hy p on y m  r e la t i on t e n  (sp ot )<sp ot >t e n  (sp ot )p oc h i (d ot )c h i s an a (sm a ll)   s h i r us h i(m a r k )<sp ot > t e n  (sp ot )b as h o (a r e a )i c h i  (loca t i on )<sp ot >s h i r us h i  (m a r k )<m a r k >sy n on y m ou s g r ou pFigure 1: An example of synonymy database.as the ratio of the overlapped co-occurrence wordsusing the Simpson coefficient.
The Simpson coeffi-cient is computed as |T (w1)?T (w2)|min(|T (w1)|,|T (w2)|) , where T (w) isthe set of co-occurrence words of word w.2.3 Integrating the Distributional Similarityinto the Synonymous ExpressionsSynonymous expressions can be extracted from adictionary as described in Section 2.1.
However,some extracted synonyms/hypernyms are not appro-priate since they are rarely used.
Especially, in thecase of that a word has multiple senses, the syn-onym/hypernym extracted from the second or laterdefinition might cause the inappropriate matching.For example, since ?suidou?
has two senses, thetwo synonym pairs, ?suidou?
= ?jyosuidou(watersupply)?
and ?suidou?
= ?kaikyou(strait)?, are ex-tracted.
The second sense is rarely used, and thus ifthe synonymy pair extracted from the second defi-nition is used as a synonym relation, an inappropri-ate matching through this synonymmight be caused.Therefore, only the pairs of synonyms/hypernymswhose distributional similarity calculated in Section2.2 is high are utilized for the flexible matching.The similarity threshold is set to 0.4 for synonymsand to 0.3 for hypernyms.
For example, since thesimilarity between ?suidou?
and ?kaikyou?
is 0.298,this synonym is not utilized.2.4 Synonymy Database ConstructionWith the extracted binomial relations, a synonymydatabase can be constructed.
Here, polysemic wordsshould be treated carefully3.
When the relationsA=B and B=C are extracted, and B is not polysemic,3If a word has two or more definition items in the dictionary,the word can be regarded as polysemic.they can be merged into A=B=C.
However, if B ispolysemic, the synonym relations are not mergedthrough a polysemic word.
In the same way, as forhypernym-hyponym relations, A ?
B and B ?
C,and A ?
B and C ?
B are not merged if B is pol-ysemic.
By merging binomial synonym relationswith the exception of polysemic words, synony-mous groups are constructed first.
They are givenIDs, hereafter called SYNID4.
Then, hypernym-hyponym relations are established between synony-mous groups.
We call this resulting data as syn-onymy database.
Figure 1 shows examples of syn-onymous groups in the synonymy database.
In thispaper, SYNID is denoted by using English glossword, surrounded by ?
?
?
?.3 SYNGRAPH3.1 SYNGRAPH Data StructureSYNGRAPH data structure is an acyclic directedgraph, and the basis of SYNGRAPH is the depen-dency structure of an original sentence (in this paper,a robust parser (Kurohashi and Nagao, 1994) is al-ways employed).
In the dependency structure, eachnode consists of one content word and zero or morefunction words, which is called a basic node here-after.
If the content word of a basic node belongs toa synonymous group, a new node with the SYNID isattached to it, and it is called a SYN node hereafter.For example, in Figure 2, the shaded nodes are basicnodes and the other nodes are SYN nodes5.Then, if the expression conjoining two or more4Spelling variations such as use of Hiragana, Katakanaor Kanji are handled by the morphological analyzer JUMAN(Kurohashi et al, 1994).5The reason why we distinguish basic nodes from SYNnodes is to give priority to exact matching over synonymousmatching.789hotel ni<hotel> ni i c hi b a n( b es t)<m os t> c hi k a i( n ea r )<n ea r es t>0.991 .00.990.991 .01 .0m oy or i( n ea r es t)0.991 .0<n ea r es t>N M S = 0 .
9 8N M S = 0 .
9ek i( s ta ti on ) w a N M S = 1 .
01 .0ek i( s ta ti on ) w a1 .0hotel no<hotel> no0.991 .0Figure 2: SYNGRAPH matching.nodes corresponds to one synonymous group, aSYN node is added there.
In Figure 2, ?nearest?
issuch a SYN node.
Furthermore, if one SYN nodehas a hyper synonymous group in the synonymydatabase, the SYN node with the hyper SYNID isalso added.In this SYNGRAPH data structure, each node hasa score, NS (Node Score), which reflects how muchthe expression of the node is shifted from the orig-inal expression.
We explain how to calculate NSslater.3.2 SYNGRAPH MatchingTwo SYNGRAPHs match if and only if?
all the nodes in one SYNGRAPH can bematched to the nodes in the other one,?
the matched nodes in two SYNGRAPHs havethe same dependency structure, and?
the nodes can cover the original sentences.An example of SYNGRAPH matching is illustratedin Figure 2.
When two SYNGRAPHs match eachother, their matching score is calculated as follows.First, the matching score of the matching two nodes,NMS (Node Match Score) is calculated with theirnode scores, NS1 and NS2,NMS = NS 1 ?
NS 2 ?
FI Penalty,where we define FI Penalty (Function word Incon-sistency Penalty) is 0.9 when their function wordsare not the same, and 1.0 otherwise.Then, the matching score of two SYNGRAPHs,SMS (SYNGRAPH Match Score) is defined as theaverage of NMSs weighted by the number of basicnodes,SMS =?
(# of basic nodes ?
NMS)?# of basic nodes.In an example shown in Figure 2, the NMS of theleft-hand side hotel node and the right-hand side ho-tel node is 0.9 (= 1.0 ?
1.0 ?
0.9).
The NMS of theleft-hand side ?nearest?
node and the right-hand side?nearest?
node is 0.98 (= 0.99 ?
0.99 ?
1.0).
Then,the SMS becomes 0.9?2+0.98?3+1.0?22+3+2 = 0.96.3.3 SYNGRAPH Transformation of SynonymyDatabaseThe synonymy database is transformed into SYN-GRAPHs, where SYNGRAPH matching is itera-tively applied to interpret the mutual relationshipsin the synonymy database, as follows:Step 1: Each expression in each synonymous groupis parsed and transformed into a fundamental SYN-GRAPH.Step 2: SYNGRAPH matching is applied to checkwhether a sub-tree of one expression is matched withany other whole expressions.
If there is a match, anew node with the SYNID of the whole matched ex-pression is assigned to the partially matched nodesgroup.
Furthermore, if the SYNID has a hyper syn-onymous group, another new node with the hyper-nym SYNID is also assigned.
This checking processstarts from small parts to larger parts.We define the NS of the newly assigned SYNnode as the SMS multiplied by a relation penalty.Here, we define the synonymy relation penalty as0.99 and the hypernym relation penalty as 0.7.
Forinstance, the NS of ?underwater?
node is 0.99 andthat of ?inside?
node is 0.7.Step 3: Repeat Step 2, until no more new SYN nodecan be assigned to any expressions.
In the case ofFigure 3 example, the new SYN node, ?diving?
isgiven to ?suityu (underwater) ni (to) moguru (dive)?of ?diving(sport)?
at the second iteration.4 Flexible Matching using SYNGRAPHWe use example-based machine translation (EBMT)as an example to explain how our flexible matchingmethod works (Figure 4).
EBMT generates a trans-lation by combining partially matching TEs with aninput6.
We use flexible matching to fully exploit theTEs.6How to select the best TEs and combine the selected TEsfor generating a translation is omitted in this paper.790ninininisportninimoguru(dive)ni<underwater><inside><diving>0.990.70.991.01.0diving1.0<diving(sport)>mizu(water)suityu(underwater)naka(inside)1.01.01.0<underwater><inside><inside>0.99naka(inside)<inside>moguru(dive)0.991.01.0<inside>0.7mizu(water)1.0sensui(diving)1.0<diving>Synonymy databaseTranslation examplenaka(inside)suityu(underwater) nono1.01.0surusportsuru<diving><diving(sport)>  sensui(diving)0.991.00.931.0 <underwater> 0.99Figure 3: SYNGRAPH transformation of synonymy database.input sentence translation examplestransform into a SYNGRAPHJapanese EnglishFigure 4: Flexible matching using SYNGRAPH inEBMT.First, TEs are transformed into SYNGRAPHs bySYNGRAPH matching with SYNGRAPHs of thesynonymy database.
Since the synonymy databasehas been transformed into SYNGRAPHs, we do notneed to care the combinations of synonymous ex-pressions any more.
In the example shown in Fig-ure 3, ?sensui (diving) suru (do) sport?
in the TE isgiven ?diving(sport)?
node just by looking at SYN-GRAPHs in ?diving(sport)?
synonymous group.Then, an input sentence is transformed into aSYNGRAPH by SYNGRAPH matching, and thenthe SYNGRAPH matching is applied between allthe sub trees of the input SYNGRAPH and SYN-GRAPHs of TEs to retrieve the partially matchingTEs.5 Experiments and Discussion5.1 Evaluation on Machine Translation TaskTo see the effectiveness of the our proposed method,we conducted our evaluations on a MT task us-ing Japanese-English translation training corpus(20,000 sentence pairs) and 506 test sentences ofIWSLT?057.
As an evaluation measure, NIST andBLEU were used based on 16 reference English sen-tences for each test sentence.7http://www.is.cs.cmu.edu/iwslt2005/.Table 1: Size of synonymy database.# of synonymous group 5,046# of hypernym-hyponym relation 18,590The synonymy database used in the experimentswas automatically extracted from the REIKAI-SHOGAKU dictionary (a dictionary for children),which consists of about 30,000 entries.
Table1 shows the size of the constructed synonymydatabase.As a base translation system, we used an EBMTsystem developed by (Kurohashi et al, 2005).
Ta-ble 2 shows the experimental results.
?None?
meansthe baseline system without using the synonymydatabase.
?Synonym?
is the system using onlysynonymous relations, and it performed best andachieved 1.2% improvement for NIST and 0.8%improvement for BLEU over the baseline.
Thesedifferences are statistically significant (p < 0.05).Some TEs that can be retrieved by our flexiblematching are shown below:?
input: fujin (lady) you (for) toile (toilet) ?TE: josei (woman) you (for) toile (toilet)?
input: kantan-ni ieba (in short)?TE: tsumari(in other words)On the other hand, if the system also useshypernym-hyponym relation (?Synonym Hyper-nym?
), the score goes down.
It proves that hyper-nym examples are not necessarily good for trans-lation.
For example, for a translation of depato(department store), its hypernym ?mise(store)?
wasused, and it lowered the score.Major errors are caused by the deficiency of wordsense disambiguation.
When a polysemic word oc-curs in a sentence, multiple SYNIDs are attachedto the word, and thus, the incorrect matching mightbe occurred.
Incorporation of unsupervised word-791Table 2: Evaluation results on MT task.Synonymy DB NIST BLEUNone 8.023 0.375Synonym 8.121 0.378Synonym Hypernym 8.010 0.374Table 3: Evaluation results on IR task.Method Synonymy DB R-precBest IREX system ?
0.493BM25 ?
0.474None 0.492Our method Synonym 0.509Synonym Hypernym 0.514sense-disambiguation of words in dictionary defini-tions and matching sentences is one of our futureresearch targets.5.2 Evaluation on Information Retrieval TaskTo demonstrate the effectiveness of our methodin other NLP tasks, we also evaluated it in IR.More concretely, we extended word-based impor-tance weighting of Okapi BM25 (Robertson et al,1994) to SYN node-based weighting.
We used thedata set of IR evaluation workshop IREX, whichcontains 30 queries and their corresponding relevantdocuments in 2-year volume of newspaper articles8.Table 3 shows the experimental results, which areevaluated with R-precision.
The baseline system isour implementation of OKAPI BM25.
Differentlyfrom the MT task, the system using both synonymand hypernym-hyponym relations performed best,and its improvement over the baseline was 7.8%relative.
This difference is statistically significant(p < 0.05).
This result shows the wide applicabil-ity of our flexible matching method for NLP tasks.Some examples that can be retrieved by our flexiblematching are shown below:?
query: gakkou-ni (school) computer-wo(computer) dounyuu (introduce) ?
docu-ment: shou-gakkou-ni (elementary school)pasokon-wo (personal computer) dounyuu(introduce)6 ConclusionThis paper proposed a flexible matching method byextracting synonymous expressions from an ordi-nary dictionary and a Web corpus, and introducingSYNGRAPH data structure.
We confirmed the ef-fectiveness of our method on experiments of ma-chine translation and information retrieval.8http://nlp.cs.nyu.edu/irex/.Our future research targets are to incorporateword sense disambiguation to our framework, andto extend SYNGRAPH matching to more structuralparaphrases.ReferencesRegina Barzilay and Lillian Lee.
2003.
Learning to paraphrase:An unsupervised approach using multiple-sequence align-ment.
In HLT-NAACL 2003, pages 16?23.Zellig Harris.
1968.
Mathematical Structures of Language.Wiley.Christian Jacquemin, Judith L. Klavans, and Evelyne Tzouker-mann.
1997.
Expansion of multi-word terms for indexingand retrieval using morphology and syntax.
In 35th AnnualMeeting of the Association for Computational Linguistics,pages 24?31.J.R.Firth.
1957.
A synopsis of linguistic theory, 1933-1957.
InStudies in Linguistic Analysis, pages 1?32.
Blackwell.Daisuke Kawahara and Sadao Kurohashi.
2001.
Japanese caseframe construction by coupling the verb and its closest casecomponent.
In Proc.
of HLT 2001, pages 204?210.Daisuke Kawahara and Sadao Kurohashi.
2006.
Case framecompilation from the web using high-performance comput-ing.
In Proc.
of LREC-06.Sadao Kurohashi and Makoto Nagao.
1994.
A syntactic anal-ysis method of long japanese sentences based on the detec-tion of conjunctive structures.
Computational Linguistics,20(4):507?534.Sadao Kurohashi, Toshihisa Nakamura, Yuji Matsumoto, andMakoto Nagao.
1994.
Improvements of Japanese mor-phological analyzer JUMAN.
In Proc.
of the InternationalWorkshop on Sharable Natural Language, pages 22?28.Sadao Kurohashi, Toshiaki Nakazawa, Kauffmann Alexis, andDaisuke Kawahara.
2005.
Example-based machine transla-tion pursuing fully structural NLP.
In Proc.
of IWSLT?05,pages 207?212.Dekang Lin and Patrick Pantel.
2001.
Discovery of inferencerules for question answering.
Natural Language Engineer-ing, 7(4):343?360.Junichi Nakamura and Makoto Nagao.
1988.
Extraction of se-mantic information from an ordinary english dictionary andits evaluation.
In Proc.
of the 12th COLING, pages 459?464.S.
E. Robertson, S. Walker, S. Jones, M.M.
Hancock-Beaulieu,and M. Gatford.
1994.
Okapi at TREC-3.
In the third TextREtrieval Conference (TREC-3).Hiroaki Tsurumaru, Toru Hitaka, and Sho Yoshida.
1986.
Anattempt to automatic thesaurus construction from an ordinaryjapanese language dictionary.
In Proc.
of the 11th COLING,pages 445?447.Ellen M. Voorhees.
1994.
Query expansion using lexical-semantic relations.
In SIGIR, pages 61?69.792
