Global Thresholding and Mult ip le-Pass Parsing*Joshua GoodmanHarvard University40 Oxford St.Cambridge, MA 02138goodman@das.harvard.eduAbstractWe present a variation on classic beamthresholding techniques that is up to an or-der of magnitude faster than the traditionalmethod, at the same performance l vel.
Wealso present a new thresholding technique,global thresholding, which, combined withthe new beam thresholding, gives an ad-ditional factor of two improvement, and anovel technique, multiple pass parsing, thatcan be combined with the others to yieldyet another 50% improvement.
We use anew search algorithm to simultaneously op-timize the thresholding parameters of thevarious algorithms.1 IntroductionIn this paper, we examine thresholding techniquesfor statistical parsers.
While there exist theoreticallyefficient (O (n 3)) algorithms for parsing ProbabilisticContext-Free Grammars (PCFGs) and related for-malisms, practical parsing algorithms usually makeuse of pruning techniques, such as beam threshold-ing, for increased speed.We introduce two novel thresholding techniques,global thresholding and multiple-pass parsing, andone significant variation on traditional beam thresh-olding.
We examine the value of these techniqueswhen used separately, and when combined.
In or-der to examine the combined techniques, we alsointroduce an algorithm for optimizing the settings*This material is based in part upon work supportedby the National Science Foundation under Grant No.IRI-9350192 and a National Science Foundation Grad-uate Student Fellowship.
I would also like to thankMichael Collins, Rebecca Hwa, Lillian Lee, WheelerRuml, and Stuart Shieber for helpful discussions, andcomments on earlier drafts, and the anonymous review-ers for their extensive comments.11of multiple thresholds.
When all three thresholdingmethods are used together, they yield very signif-icant speedups over traditional beam thresholding,while achieving the same level of performance.We apply our techniques to CKY chart parsing,one of the most commonly used parsing methods innatural anguage processing.
In a CKY chart parser,a two-dimensional matrix of cells, the chart, is filledin.
Each cell in the chart corresponds-to a span ofthe sentence, and each cell of the chart contains thenonterminals that could generate that span.
Cellscovering shorter spans are filled in first, so we alsorefer to this kind of parser as a bottom-up chartparser.The parser fills in a cell in the chart by examiningthe nonterminals in lower, shorter cells, and combin-ing these nonterminals according to the rules of thegrammar.
The more nonterminals there are in theshorter cells, the more combinations of nonterminalsthe parser must consider.In some grammars, such as PCFGs, probabilitiesare associated with the grammar ules.
This in-troduces problems, since in many PCFGs, almostany combination of nonterminals is possible, per-haps with some low probability.
The large number ofpossibilities can greatly slow parsing.
On the otherhand, the probabilities also introduce new opportu-nities.
For instance, if in a particular cell in thechart there is some nonterminal that generates thespan with high probability, and another that gen-erates that span with low probability, then we canremove the less likely nonterminal from the cell.
Theless likely nonterminal will probably not be part ofeither the correct parse or the tree returned by theparser, so removing it will do little harm.
This tech-nique is called beam thresholding.If we use a loose beam threshold, removing onlythose nonterminals that are much less probable thanthe best nonterminal in a cell, our parser will runonly slightly faster than with no thresholding, while8078767472706866........................... T :U//////10O00TimeFigure 1: Precision and Recall versus Time in BeamThresholdingperformance measures uch as precision and recallwill remain virtually unchanged.
On the other hand,if we use a tight threshold, removing nonterminalsthat are almost as probable as the best nonterminalin a cell, then we can get a considerable speedup, butat a considerable cost.
Figure 1 shows the tradeoffbetween accuracy and time.In this paper, we will consider three different kindsof thresholding.
The first of these is a variation ontraditional beam search.
In traditional beam search,only the probability of a nonterminal generating theterminals of the cell's span is used.
We have foundthat a minor variation, introduced in Section 2, inwhich we also consider the prior probability thateach nonterminal is part of the correct parse, canlead to nearly an order of magnitude improvement.The problem with beam search is that it onlycompares nonterminals to other nonterminals in thesame cell.
Consider the case in which a particularcell contains only bad nonterminals, all of roughlyequal probability.
We can't threshold out thesenodes, because ven though they are all bad, noneis much worse than the best.
Thus, what we wantis a thresholding technique that uses some globalinformation for thresholding, rather than just us-ing information in a single cell.
The second kind ofthresholding we consider is a novel technique, globalthresholding, described in Section 3.
Global thresh-olding makes use of the observation that for a non-terminal to be part of the correct parse, it must bepart of a sequence of reasonably probable nontermi-nals covering the whole sentence.The last technique we consider, multiple-passparsing, is introduced in Section 4.
The basic idea isthat we can use information from parsing with onegrammar to speed parsing with a~other.
We run twopasses, the first of which is fast and simple, elimi-nating from consideration many unlikely potentialconstituents.
The second pass is more complicatedand slower, but also more accurate.
Because we havealready eliminated many nodes in our first pass, thesecond pass can run much faster, and, despite thefact that we have to run two passes, the added sav-ings in the second pass can easily outweigh the costof the first one.Experimental comparisons of these techniquesshow that they lead to considerable speedups overtraditional thresholding, when used separately.
Wealso wished to combine the thresholding techniques;this is relatively difficult, since searching for the opti-mal thresholding parameters in a multi-dimensionaispace is potentially very time consuming.
We .de-signed a variant on a gradient descent search algo-r ithm to find the optimal parameters.
Using all threethresholding methods together, and the parametersearch algorithm, we achieved our best results, run-ning an estimated 30 times faster than traditionalbeam search, at the same performance l vel.2 Beam Thresho ld ingThe first, and simplest, technique we will examine isbeam thresholding.
While this technique is used aspart of many search algorithms, beam thresholdingwith PCFGs is most similar to beam thresholdingas used in speech recognition.
Beam thresholdingis often used in statistical parsers, such as that ofCollins (1996).Consider a nonterminal X in a cell covering thespan of terminals tj...tk.
We will refer to this as nodeNjXk, since it corresponds to a potential node in thefinal parse tree.
Recall that in beam thresholding,we compare nodes N~, k and N~, k covering the samespan.
If one node is much more likely than the other,then it is unlikely that the less probable node willbe part of the correct parse, and we can remove itfrom the chart, saving time later.There is a subtlety about what it means for a nodeN~, k to be more likely than some other node.
Ac-cording to folk wisdom, the best way to measure thelikelihood of a node N~, k is to use the probabilitythat the nonterminal X generates the span tj...tk,called the inside probability.
Formally, we write thisas  P(X  =~ tj...tk) , and denote it by x ~(Nj,k).
How-ever, this does not give information about the proba-bility of the node in the context of the full parse tree.For instance, two nodes, one an NP and the other aFRA G (fragment), may have equal inside probabili-ties, but since there are far more NPs than there areFRAG clauses, the NP node is more likely overall.12Therefore, we must consider more information thanjust the inside probability.The outside probability of a node N~k is the prob-ability of that node given the surrounding terminalsof the sentence, i.e.
P(S =~ tl...tj-xXtk+l...tn),which we denote by a(N~k ).
Ideally, we would mul-tiply the inside probability by the outside probabil-ity, and normalize.
This product would give us theoverall probability that the node is part of the cor-rect parse.
Unfortunately, there is no good way toquickly compute the outside probability of a nodeduring bottom-up chart parsing (although it can beefficiently computed afterwards).
Thus, we insteadmultiply the inside probability simply by the priorprobability of the nonterminal type, P(X), whichis an approximation to the outside probability.
Ourfinal thresholding measure is P(X) xfl(Nj,Xk).
In Sec-tion 7.4, we will show experiments comparing inside-probability beam thresholding to beam thresholdingusing the inside probability times the prior.
Usingthe prior can lead to a speedup of up to a factor of10, at the same performance l vel.To the best of our knowledge, using the priorprobability in beam thresholding is new, al-though not particularly insightful on our part.Collins (personal communication) independently ob-served the usefulness of this modification, andCaraballo and Charniak (1996) used a related tech-nique in a best-first parser.
We think that themain reason this technique was not used sooner isthat beam thresholding for PCFGs is derived frombeam thresholding in speech recognition using Hid-den Markov Models (HMMs).
In an HMM, theforward probability of a given state corresponds tothe probability of reaching that state from the startstate.
The probability of eventually reaching thefinal state from any state is always 1.
Thus, theforward probability is all that is needed.
The sameis true in some top down probabilistic parsing al-gorithms, such as stochastic versions of Earley's al-gorithm (Stolcke, 1993).
However, in a bottom-upalgorithm, we need the extra factor that indicatesthe probability of getting from the start symbol tothe nonterminal in question, which we approximateby the prior probability.
As we noted, this can bevery different for different nonterminals.3 Global ThresholdingAs mentioned earlier, the problem with beam thresh-olding is that it can only threshold out the worstnodes of a cell.
It cannot threshold out an entirecell, even if there are no good nodes in it.
To rem-edy this problem, we introduce a novel thresholdingtechnique, global thresholding.A B CFigure 2: Global Thresholding MotivationThe key insight of global thresholding is due toRayner and Carter (1996).
Rayner et al noticedthat a particular node cannot be part of the cor-rect parse if there are no nodes in adjacent cells.
Infact, it must be part of a sequence of nodes stretch-ing from the start of the string to the end.
In aprobabilistic framework where almost every nodewill have some (possibly very small) probability, wecan rephrase this requirement as being that the nodemust be part of a reasonably probable sequence.Figure 2 shows an example of this insight.
NodesA, B, and C will not be thresholded out,  becauseeach is part of a sequence from the beginning to theend of the chart.
On the other hand, nodes X, Y,and Z will be thresholded out, because none is partof such a sequence.Rayner et al used this insight for a hierarchical,non-recursive grammar, and only used their tech-nique to prune after the first level of the grammar.They computed a score for each sequence as the min-imum of the scores of each node in the sequence, andcomputed a score for each node in the sequence asthe minimum of three scores: one based on statisticsabout nodes to the left, one based on nodes to theright, and one based on unigram statistics.We wanted to extend the work of Rayner et al togeneral PCFGs, including those that were recursive.Our approach therefore differs from theirs in manyways.
Rayner et al ignore the inside probabilities ofnodes; while this may work after processing only thefirst level of a grammar, when the inside probabilitieswill be relatively homogeneous, it could cause prob-lems after other levels, when the inside probabilityof a node will give important information about itsusefulness.
On the other hand, because long nodeswill tend to have low inside probabilities, taking theminimum of all scores strongly favors sequences ofshort nodes.
Furthermore, their algorithm requirestime O(n a) to run just once.
This is acceptable if thealgorithm is run only after the first level, but run-ning it more often would lead to an overall run timeof O(n4).
Finally, we hoped to find an algorithmthat was somewhat less heuristic in nature.13f loat f \ [1.
.n+l\ ]  := {1,0,0, ...,0};for start := 1 to nfor each node N beginning at startleft := f \ [s tar t \ ] ;score := left ?
Ninside x Nprior;i f  score > f\[start + Nlength\]f\[start + Ntength\] := score;f loat b\[1..n+l\] := {0, ...,0,0, 1};for start := n downto 1for each node N beginning at startright := b\[start + Ntength\];score := right ?
Ninside ?
Nprior;i f  score > b\[start\]b\[start\] := score;bestProb := f \ [n+ 1\];for each node Nleft := f\[N~t~rt\];right := b\[N~t~,rt + Ntenath\];total := left ?
Ninsid~ X Np~or x right;i f  total > bestProb x TGNacuve := TRUE;elseNacuve := FALSE;Figure 3: Global Thresholding AlgorithmOur global thresholding technique thresholds outnode N if the ratio between the most probable se-quence of nodes including node N and the over-all most probable sequence of nodes is less thansome threshold, To.
Formally, denoting sequencesof nodes by L, we threshold node N ifTG relax P(L) > max P(L) LINeLNow, the hard part is determining P(L),  the prob-ability of a node sequence.
Unfortunately, there isno way to do this efficiently as part of the intermedi-ate computation of a bottom-up chart parser.
1 Wewill approximate P(L) as follows:P(L) = I I  P(LilL1...Li_i) ~ H P(Li)i i1Some other parsing techniques, uch as stochasticversions of Earley parsers (Stolcke, 1993), efficientlycompute related probabilities, but we won't explore theseparsers here.
We confess that our real interest is inmore complicated grammars, uch as those that use headwords.
Grammars uch as these can best be parsed bot-tom up.That is, we assume independence between the el-ements of a sequence.
The probability of nodeLi = N~k is just its prior probability times its insideprobability, as before.The most important difference between globalthresholding and beam thresholding is that globalthresholding is global: any node in the chart can helpprune out any other node.
In stark contrast, beamthresholding only compares nodes to other nodescovering the same span.
Beam thresholding typi-cally allows tighter thresholds ince there are fewerapproximations, but does not benefit from global in-formation.3.1 G loba l  Thresho ld ing  A lgor i thmGlobal thresholding is performed in a bottom-upchart parser immediately after each length is com-pleted.
It thus runs n times during the course ofparsing a sentence of length n.We use the simple dynamic programming algo-rithm in Figure 3.
There are O(n 2) nodes in thechart, and each node is examined exactly threetimes, so the run time of this algorithm is O(n2).The first section of the algorithm works forwards,computing, for each i, f\[i\], which contains the scoreof the best sequence covering terminals tl...ti-1.Thus f in+l \ ]  contains the score of the best sequencecovering the whole sentence, maxL P(L).
The algo-rithm works analogously to the Viterbi algorithm forHMMs.
The second section is analogous, but worksbackwards, computing b\[i\], which contains the scoreof the best sequence covering terminals ti...tn.Once we have computed the preceding arrays,computing maXL\]NE L P(L) is straightforward.
Wesimply want the score of the best sequence cover-ing the nodes to the left of N, f\[Nstart\], times thescore of the node itself, times the score of the bestsequence of nodes from Ns~art + Nt~ngth to the end,which is just b\[N~u, rt + Nt~ngth\].
Using this expres-sion, we can threshold each node quickly.Since this algorithm is run n times during thecourse of parsing, and requires time O(n 2) each timeit runs, the algorithm requires time O(n 3) overall.Experiments will show that the time it saves easilyoutweighs the time it uses.4 Mu l t ip le -Pass  Pars ingIn this section, we discuss a novel thresholdingtechnique, multiple-pass parsing.
We show thatmultiple-pass parsing techniques can yield largespeedups.
Multiple-pass parsing is a variation on anew technique in speech recognition, multiple-passspeech recognition (Zavaliagkos et al, 1994), whichwe introduce first.144.1 Multiple-Pass Speech RecognitionIn an idealized multiple-pass speech recognizer, wefirst run a simple pass, computing the forward andbackward probabilities.
This first pass runs rela-tively quickly.
We can use information from thissimple, fast first pass to eliminate most states, andthen run a more complicated, slower second passthat does not examine states that were deemed un-likely by the first pass.
The extra time of runningtwo passes is more than made up for by the timesaved in the second pass.The mathematics of multiple-pass recognition isfairly simple.
In the first simple pass, we record theforward probabilities, c~(S~), and backward proba-bilities, fl(S~), of each state i at each time t. NOW ,~(s~)?~(s~) gives the overall probability of being in ~(s~,.,)state i at time t given the acoustics.
Our second passwill use an HMM whose states are analogous to thefirst pass HMM's states.
If a first pass state at sometime is unlikely, then the analogous second pass stateis probably also unlikely, so we can threshold it out.There are a few complications to multiple-passrecognition.
First, storing all the forward and back-ward probabilities can be expensive.
Second, thesecond pass is more complicated than the first, typ-ically ineaning that it has more states.
So the map-ping between states in the first pass and states in thesecond pass may be non-trivial.
To solve both theseproblems, only states at word transitions are saved.That is, from pass to pass, only information aboutwhere words are likely to start and end is used forthresholding.4.2 Mu l t ip le -Pass  Pars ingWe can use an analogous algorithm for multiple-passparsing.
In particular, we can use two grammars,one fast and simple and the other slower, more com-plicated, and more accurate.
Rather than using theforward and backward probabilities of speech recog-nition, we use the analogous inside and outside prob-abilities, x fl(Nj,k) and a(Nfk ) respectively.
Remem-ber that B(N~i. )
is the probability that Nfk isin the correct parse (given, as always, the model andthe string).
Thus, we run our first pass, computingthis expression for each node.
We can then eliminatefrom consideration i  our later passes all nodes forwhich the probability of being in the correct parsewas too small in the first pass.Of course, for our second pass to be more accu-rate, it will probably be more complicated, typicallycontaining an increased number of nonterminals andproductions.
Thus, we create a mapping functionfor length := 2 to nfor start := 1 to n - length + 1LeftPrev := PrevChart \[length\]\[start\];for each LeftNodePrev E LeftPrevfor each production instance Prod fromLeftNodePrev of size lengthfor each descendant L of ProdLeltfor each descendant R of ProdRightfor each descendant P of Prodpar~n~such that P ~ L Radd P to Chart\[length\]\[start\];Figure 4: Second Pass Parsing Algorithmfrom each first pass nonterminal to a set of secondpass nonterminals, and threshold out those secondpass nonterminals that map from low-scoring firstpass nonterminals.
We call this mapping functionthe descendants function.
2There are many possible xamples of first and sec-ond pass combinations.
For instance, the first passcould use regular nonterminals, uch as NP and VPand the second pass could use nonterminals aug-mented with head-word information.
The descen-dants function then appends the possible head wordsto the first pass nonterminals to get the second passones.Even though the correspondence between for-ward/backward and inside/outside probabilities isvery close, there are important differences betweenspeech-recognition HMMs and natural-languageprocessing PCFGs.
In particular, we have foundthat it is more important o threshold productionsthan nonterminals.
That is, rather than just notic-ing that a particular nonterminal VP spanning thewords "killed the rabbit" is very likely, we also notethat the production VP --~ V NP (and the relevantspans) is likely.Both the first and second pass parsing algorithmsare simple variations on CKY parsing.
In the firstpass, we now keep track of each production instanceassociated with a node, i.e.
N'x~,3 ~ NYi,k gZk+l,j,computing the inside and outside probabilities ofeach.
The second pass requires more changes.
Letus denote the descendants of nonterminal X byX1...Xx.
In the second pass, for each production2In thin paper, we will assume that each second passnonterminal can descend from at most one first pass non-terminal in each cell.
Th~ grammars used here have thisproperty.
If this assumption is violated, multiple-passparsing is still possible, but some of the algorithms needto be changed.l Sof the form N. X. N Y z ~,~ ~ i,k N~+Ij in the first pass thatwasn't thresholded out by multi-pass thresholding,beam thresholding, etc., we consider every descen-dant production instance, that is, all those of theZ.
form N~,~ p ~ Ni, ~ N~+,j,  for appropriate values ofp, q, r. This algorithm is given in Figure 4, whichuses a current pass matrix Chart to keep track ofnonterminals in the current pass, and a previous passmatrix, PrevChart to keep track of nonterminals inthe previous pass.
We use one additional optimiza-tion, keeping track of the descendants of each non-terminal in each cell in PrevChart which are in thecorresponding cell of Chart.We tried multiple-pass thresholding in two differ-ent ways.
In the first technique we tried, production-instance thresholding, we remove from considerationin the second pass the descendants of all productioninstances whose combined inside-outside probabil-ity falls below a threshold.
In the second technique,node thresholding, we remove from consideration thedescendants of all nodes whose inside-outside prob-ability falls below a threshold.
In our pilot exper-iments, we found that in some cases one techniqueworks slightly better, and in some cases the otherdoes.
We therefore ran our experiments using boththresholds together.One nice feature of multiple-pass parsing is thatunder special circumstances, it is an admissiblesearch technique, meaning that we are guaranteedto find the best solution with it.
In particular, ifwe parse using no thresholding, and our grammarshave the property that for every non-zero probabil-ity parse in the second pass, there is an analogousnon-zero probability parse in the first pass, thenmultiple-pass search is admissible.
Under these cir-cumstances, no non-zero probability parse will bethresholded out, but many zero probability parsesmay be removed from consideration.
While we willalmost always wish to parse using thresholds, it isnice to know that multiple-pass parsing can be seenas an approximation to an admissible technique,where the degree of approximation is controlled bythe thresholding parameter.5 Mu l t ip le  Parameter  Opt imizat ionThe use of any one of these techniques does notexclude the use of the others.
There is no rea-son that we cannot use beam thresholding, globalthresholding, and multiple-pass parsing all at thesame time.
In general, it wouldn't make sense to usea technique such as multiple-pass parsing withoutother thresholding techniques; our first pass wouldbe overwhelmingly slow without some sort of thresh-whi le not  Thresholds E ThresholdsSetadd Thresholds to ThresholdsSet;( BaseET ,Base Time ) := ParseAll( Thresholds );for each Threshold E Thresholdsif BaseET > TargetETtighten Threshold;( NewET ,New Time ) :-- ParseAll(Thresholds);Ratio := (BaseTime - NewTime) /(BaseET - NewET);elseloosen Threshold;( NewET ,New Tim e) := ParseAll (Thresholds);Ratio := (BaseET - NewET) /(BaseTime - NewTime);change Threshold with best Ratio;Figure 5: Gradient Descent Multiple ThresholdSearch resh  2. .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
GoalTimeOptimizing forLower  Ent ropy :Steeper is Better.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
GoalThrob 2 ~ e s h  1TimeOptimizing forFaster Speed:Flatter is BetterFigure 6: Optimizing for Lower Entropy versus Op-timizing for Faster Speedolding.There are, however, some practical considerations.To optimize a single threshold, we could simplysweep our parameters over a one dimensional range,and pick the best speed versus performance trade-off.
In combining multiple techniques, we need tofind optimal combinations of thresholding parame-ters.
Rather than having to examine 10 values ina single dimensional space, we might have to exam-ine 100 combinations in a two dimensional space.Later, we show experiments with up to six thresh-olds.
Since we don't have time to parse with onemillion parameter combinations, we need a bettersearch algorithm.Ideally, we would like to be able to pick a perfor-mance level (in terms of either entropy or precisionand recall) and find the best set of thresholds for16achieving that performance l vel as quickly as pos-sible.
If this is our goal, then a normal gradient de-scent technique won't work, since we can't use sucha technique to optimize one function of a set of vari-ables (time as a function of thresholds) while holdinganother one constant (performance).
3We wanted a metric of performance which wouldbe sensitive to changes in threshold values.
In par-ticular, our ideal metric would be strictly increasingas our thresholds loosened, so that every looseningof threshold values would produce a measurable in-crease in performance.
The closer we get to thisideal, the fewer sentences we need to test during pa-rameter optimization.We tried an experiment in which we ran beamthresholding with a tight threshold, and then a loosethreshold, on all sentences of section 0 of length< 40.
For this experiment only, we discarded thosesentences which could not be parsed with the spec-ified setting of the threshold, rather than retryingwith looser thresholds.
We then computed for eachof six metrics how often the metric decreased, stayedthe same, or increased for each sentence between thetwo runs.
Ideally, as we loosened the "threshold, ev-ery sentence should improve on every metric, but inpractice, ,that wasn't the case.
As can be seen, theinside score was by far the most nearly strictly in-creasing metric.
Therefore, we should use the insideprobability as our metric of performance; howeverinside probabilities can become very close to zero, soinstead we measure ntropy, the negative logarithmof the inside probability.MetricInsideViterbiCross BracketZero Cross BracketPrecisionRecalldecrease same increase7 64 16236 1300 388134 1321 23018 1606 61134 1270 281128 1321 236We implemented a variation on a steepest descentsearch technique.
We denote the entropy of the sen-tence after thresholding by ET.
Our search engineis given a target performance l vel ET to search for,3We could use gradient descent to minimize aweighted sum of time and performance, but we wouldn'tknow at the beginning what performance we would haveat the end.
If our goal is to have the best performance wecan while running in real time, or to achieve a minimumacceptable performance l vel with as little time as nec-essary, then a simple gradient descent function wouldn'twork as well as our algorithm.Also, for this algorithm (although not for most experi-ments), our measurement of ime was the total number ofproductions searched, rather than cpu time; we wantedthe greater accuracy of measuring productions.and then tries to find the best combination of pa-rameters that works at approximately this level ofperformance.
At each point, it finds the thresholdto change that gives the most "bang for the buck.
"It then changes this parameter in the correct direc-tion to move towards ET (and possibly overshootit).
A simplified version of the algorithm is given inFigure 5.Figure 6 shows graphically how the algorithmworks.
There are two cases.
In the first case, ifwe are currently above the goal entropy, then weloosen our thresholds, leading to slower speed andlower entropy.
We then wish to get as much entropyreduction as possible per time increase; that is, wewant the steepest slope possible.
On the other hand,if we are trying to increase our entropy, we want asmuch time decrease as possible per entropy increase;that is, we want the flattest slope possible.
Becauseof this difference, we need to compute different ratiosdepending on which side of the goal we axe on.There are several subtleties when thresholds areset very tightly.
When we fail to parse a sentencebecause the thresholds are too tight, we retry theparse with lower thresholds.
This can lead to condi-tions that are the opposite of what we expect; for in-stance, loosening thresholds may lead to faster pars-ing, because we don't need to parse the sentence, fail,and then retry with looser thresholds.
The full al-gorithm contains additional checks that our thresh-olding change had the effect we expected (either in-creased time for decreased entropy or vice versa).
Ifwe get either a change in the wrong direction, or achange that makes everything worse, then we retrywith the inverse change, hoping that that will havethe intended effect.
If we get a change that makesboth time and entropy better, then we make thatchange regardless of the ratio.Also, we need to do checks that the denominatorwhen computing Ratio isn't too small.
If it is verysmall, then our estimate may be unreliable, and wedon't consider changing this parameter.
Finally, theactual algorithm we used also contained a simple"annealing schedule", in which we slowly decreasedthe factor by which we changed thresholds.
Thatis, we actually run the algorithm multiple times totermination, first changing thresholds by a factor of16.
After a loop is reached at this factor, we lowerthe factor to 4, then 2, then 1.414, then 1.15.Note that this algorithm is fairly domain inde-?
pendent.
It can be used for almost any statisticalparsing formalism that uses thresholds, or even forspeech recognition.176 Comparison to Previous WorkBeam thresholding is a common approach.
Whilewe don't know of other systems that have usedexactly our techniques, our techniques are cer-tainly similar to those of others.
For instance,Collins (1996) uses a form of beam thresholding thatdiffers from ours only in that it doesn't use theprior probability of nonterminals as a factor, andCaraballo and Charniak (1996) use a version withthe prior, but with other factors as well.Much of the previous related work on threshold-ing is in the similar area of priority functions foragenda-based parsers.
These parsers try to do "bestfirst" parsing, with some function akin to a thresh-olding function determining what is best.
The bestcomparison of these functions is due to Caraballoand Charniak (1996; 1997), who tried various pri-oritization methods.
Several of their techniques aresimilar to our beam thresholding technique, and oneof their techniques, not yet published (Caraballo andCharniak, 1997), would probably work better.The only technique that Caraballo and Charniak(1996) give that took into account he scores of othernodes in the priority function, the "prefix model,"required O(n 5) time to compute, compared to ourO(n 3) system.
On the other hand, all nodes in theagenda parser were compared to all other nodes, soin some sense all the priority functions were global.Note that agenda-based PCFG parsers in gen-eral require more than O(n 3) run time, because,when better derivations are discovered, they maybe forced to propagate improvements to productionsthat they have previously considered.
For instance,if an agenda-based system first computes the prob-ability for a production S ~ NP VP, and thenlater computes ome better probability for the NP,it must update the probability for the S as well.
Thiscould propagate through much of the chart.
To rem-edy this, Caraballo et al only propagated probabil-ities that caused a large enough change (Caraballoand Charniak, 1997).
Also, the question of when anagenda-based system should stop is a little discussedissue, and difficult since there is no obvious toppingcriterion.
Because of these issues, we chose not toimplement an agenda-based system for comparison.As mentioned earlier, Rayner and Carter (1996)describe a system that is the inspiration for globalthresholding.
Because of the limitation of their sys-tem to non-recursive grammars, and the other dif-ferences discussed in Section 3, global thresholdingrepresents a significant improvement.Collins (1996) uses two thresholding techniques.The first of these is essentially beam thresholdingfor each rule P ~ L Rif nonterminal L in left celli f  nonterminal R in right celladd P to parent cell;Algorithm Onefor each nonterminal L in left cellfor each'nonterminal R in right cellfor each rule P ~ L Radd P to parent cell;Algorithm TwoFigure 7: Two Possible CKY inner loopswithout a prior.
In the second technique, there isa constant probability threshold.
Any nodes witha probability below this threshold are pruned.
Ifthe parse fails, parsing is restarted with the con-stant lowered.
We attempted to duplicate this tech-nique, but achieved only negligible performance im-provements.
Collins (personal communication) re-ports a 38% speedup when this technique is com-bined with loose beam thresholding, compared toloose beam thresholding alone.
Perhaps our lack ofsuccess is due to differences between our grammars,which are fairly different formalisms.
When Collinsbegan using a formalism somewhat closer to ours,he needed to change his beam thresholding to takeinto account he prior, so this is not unlikely.
Hwa(personal communication) using a model similar toPCFGs, Stochastic Lexicalized Tree Insertion Gram-mars, also was not able to obtain a speedup usingthis technique.There is previous work in the speech recognitioncommunity on automatically optimizing some pa-rameters (Schwartz et al, 1992).
However, this pre-vious work differed significantly from ours both inthe techniques used, and in the parameters opti-mized.
In particular, previous work focused on opti-mizing weights for various components, uch as thelanguage model component.
In contrast, we opti-mize thresholding parameters.
Previous techniquescould not be used or easily adapted to thresholdingparameters.7 Experiments7.1 The  Parser  and DataThe inner loop of the CKY algorithm, which deter-mines for every pair of cells what nodes must be18OriginalSX YA B ... G H ZA BBinary BranchingSX yA X~,C,D,E,F ZA B B XC,D,E,F, GIC XD,E,F,G, HD X ~ E,F,G,HE X~,a, HF X~, HG HFigure 8: Converting to Binary Branchingadded to the parent, can be written in several dif-ferent ways.
Which way this is done interacts withthresholding techniques.
There are two possibilities,as shown in Figure 7.
We used the second technique,since the first technique gets no speedup from mostthresholding systems.All experiments were trained on sections 2-18 ofthe Penn Treebank, version II.
A few were tested,where noted, on the first 200 sentences of section 00of length at most 40 words.
In one experiment, weused the first 15 ()f length at most 40, and in the re-mainder of our experiments, we used those sentencesin the first 1001 of length at most 40.
Our param-eter optimization algorithm always used the first 31sentences of length at most 40 words from section19.
We ran some experiments on more sentences,but there were three sentences in this larger test setthat could not be parsed with beam thresholding,even with loose settings of the threshold; we there-fore chose to report the smaller test set, since it isdifficult to compare techniques which did not parseexactly the same sentences.7.2 The  GrammarWe needed several grammars for our experimentsso that we could test the multiple-pass parsing al-gorithm.
The grammar ules, and their associatedprobabilities, were determined by reading them offof the training section of the treebank, in a man-ner very similar to that used by Charniak (1996).The main grammar we chose was essentially of thefollowing form:!
X =v A XB,C,D,E, F !
X~,B,C,D~ ~ A XB,c,D,E,FAX ~ ABThat is, our grammar was binary branching ex-cept that we also allowed unary branching produc-tions.
There were never more than five subscriptedsymbols for any nonterminal, although there couldbe fewer than five if there were fewer than five sym-bols remaining on the right hand sine.
Thus, ourgrammar was a kind of 6-gram model on symbolsin the grammar :  Figure 8 shows an example ofhow we converted trees to binary branching with ourgrammar.
We refer to this grammar as the 6-gramgrammar.
The terminals of the grammar were thepart-of-speech symbols in the treebank.
Any exper-iments that don't mention which grammar we usedwere run with the 6-gram grammar.For a simple grammar, we wanted something thatwould be very fast.
The fastest grammar we canthink of we call the terminal grammar, because it hasone nonterminal for each terminal symbol in the al-phabet.
The nonterminal symbol indicates the firstterminal in its span.
The parses are binary branch-ing in the same way that the 6-gram grammar parsesare.
Figure 9 shows how to convert a parse tree tothe terminal grammar.
Since there is only one non-terminal possible for each Cell of the chart, parsingis quick for this grammar.
For technical and prac-tical reasons, we actually wanted a marginally morecomplicated grammar, which included the "prime"symbol of the 6-gram grammar, indicating that acell is part of the same constituent as its parent.Therefore, we doubled the size of the grammar sothat there would be both primed and non-primedaWe have skipped over details regarding our handlingof unary branching nodes.
Unary branching nodes arein general difficult to deal with (Stolcke, 1993).
The ac-tual gatammars we used contained additional symbols insuch a way that there could not be more than one unarybranch in a row.
This greatly simplified computations,especially of the inside and outside probabilities.
We alsodoubled the number of cells in our parser, having bothunary and binary cells for each length/start pair.19OriginalSNP VPverbdet adj nounTerminalDETDET VERBdet ADJ  verbadj nounTerminal-PrimeDETDET VERBdet ADJ  ~ verbadj nounFigure 9: Converting to Terminal and Terminal-Prime Grammarsversions of each terminal; we call this the terminal-prime grammar, and also show how to convert o itin Figure 9.
This is the grammar we actually usedas the first pass in our multiple-pass parsing exper-iments.7.3 What  we measuredThe goal of a good thresholding algorithm is to tradeoff correctness for increased speed.
We must thusmeasure both correctness and speed, and there aresome subtleties to measuring each.First, the traditional way of measuring correctnessis with metrics uch as precision and recall.
Unfortu-nately, there are two problems with these measures.First, they are two numbers, neither useful with-out the other.
Second, they are subject to consid-erable noise.
In pilot experiments, we found that aswe changed our thresholding values monotonically,precision and recall changed non-monotonically (seeFigure 11).
We attribute this to the fact that wemust choose a single parse from our parse forest,and, as we tighten a thresholding parameter, we maythreshold out either good or bad parses.
Further-more, rather than just changing precision or recallby a small amount, a single thresholded item maycompletely change the shape of the resulting tree.Thus, precision and recall are only smooth with verylarge sets of test data.
However, because of the largenumber of experiments we wished to run, using alarge set of test data was not feasible.
Thus, welooked for a surrogate measure, and decided to usethe total inside probability of all parses, which, withno thresholding, is just the probability of the sen-tence given the model.
If we denote the total insideprobability with no thresholding by I and the to-tal inside probability with thresholding by IT, then/~= is the probability that we did not threshold outthe correct parse, given the model.
Thus, maximiz-ing IT should maximize correctness.
Since proba-bilities can become very small, we instead minimizeentropies, the negative logarithm of the probabili-ties.
Figure 11 shows that with a large data set, en-tropy correlates well with precision and recall, andthat with smaller sets, it is much smoother.
Entropyis smoother because it is a function of many morevariables: in one experiment, here were about 16000constituents which contributed to precision and re-call measurements, versus 151 million productionspotentially contributing to entropy.
Thus, we chooseentropy as our measure of correctness for most ex-periments.
When we did measure precision and re-call, we used the metric as defined by Collins (1996).Note that the fact that entropy changes moothlyand monotonically is critical for the performance ofthe multiple parameter optimization algorithm.
Fur-thermore, we may have to run quite a few iterationsof that algorithm to get convergence, so the fact thatentropy is smooth for relatively small numbers ofsentences i  a large help.
Thus, the discovery thatentropy is a good surrogate for precision and recall isnon-trivial.
The same kinds of observations could beextended to speech recognition to optimize multiplethresholds there (the typical modern speech systemhas quite a few thresholds), a topic for future re-search.Note that for some sentences, with too tightthresholding, the parser will fail to find any parseat all.
We dealt with these cases by restarting theparser with all thresholds lowered by a factor of 5,iterating this loosening until a parse could be found.This is why for some tight thresholds, the parser maybe slower than with looser thresholds: the sentencehas to be parsed twice, once with tight thresholds,and once with loose ones.Next, we needed to choose a measure of time.There are two obvious measures: amount of work20le+0700 ,0'0o 20'00 30'00 40'00 s0'00 80'0o 70'0o 80 0 9g00,0o0oTime6e+075e+074e+07~ ~+072 a.2e+07Figure 10: Productions versus Timedone by the parser, and elapsed time.
If we mea-sure amount of work done by the parser in termsof the number of productions with non-zero prob-ability examined by the parser, we'have a fairlyimplementation-independent, machine-independentmeasure of speed.
On the other hand, because weused many different thresholding algorithms, somewith a fair amount of overhead, this measure seemsinappropriate.
Multiple-pass parsing requires useof the outside algorithm; global thresholding usesits own dynamic programming algorithm; and evenbeam thresholding has some per-node overhead.Thus, we will give most measurements in terms ofelapsed time, not including loading the grammar andother O(1) overhead.
We did want to verify thatelapsed time was a reasonable measure, so we dida beam thresholding experiment to make sure thatelapsed time and number of productions examinedwere well correlated, using 200 sentences and an ex-ponential sweep of the thresholding parameter.
Theresults, shown in Figure 10, clearly indicate thattime is a good proxy for productions examined.7.4 Experiments in Beam ThresholdingOur first goal was to show that entropy is a goodsurrogate for precision and recall.
We thus tried twoexperiments: one with a relatively large test set of200 sentences, and one with a relatively small test setof 15 sentences.
Presumably, the 200 sentence testset should be much less noisy, and fairly indicative ofperformance.
We graphed both precision and recall,and entropy, versus time, as we swept the threshold-ing parameter over a sequence of values.
The resultsare in Figure 11.
As can be seen, entropy is signif-icantly smoother than precision and recall for bothsize test corpora.Our second goal was to check thai the prior prob-ability is indeed helpful.
We ran two experiments,one with the prior and one without.
Since the exper-iments without the prior were much worse than thosewith it, all other beam thresholding experiments in-cluded the prior.
The results, shown in Figure 12,indicate that the prior is a critical component.
Thisexperiment was run on 200 sentences of test data.Notice that as the time increases, the data tendsto approach an asymptote, as shown in the left handgraph of Figure 12.
In order to make these smallasymptotic hanges more clear,-we wished to ex-pand the scale towards the asymptote.
The righthand graph was plotted with this expanded scale,based on log(entropy - asymptote) ,  a slight varia-tion on a normal log scale.
We use this scale in allthe remaining entropy graphs.
A normal logarith-mic scale is used for the time axis.
The fact thatthe time axis is logarithmic is especially useful fordetermining how much more efficient one algorithmis than another at a given performance l vel.
If onepicks a performance l vel on the vertical axis, thenthe distance between the two curves at that levelrepresents the ratio between their speeds.
There isroughly a factor of 8 to 10 difference between usingthe prior and not using it at all graphed performancelevels, with a slow trend towards smaller differencesas the thresholds are loosened.7.5 Experiments in Global ThresholdingWe tried experiments comparing lobal thresholdingto beam thresholding.
Figure 13 shows the results ofthis experiment, and later experiments.
In the bestcase, global thresholding works twice as well as beamthresholding, in the sense that to achieve the samelevel of performance r quires only half as much time,although smaller improvements were more typical.We have found that, in general, global threshold-ing works better on simpler grammars.
In somecomplicated grammars we explored in other work,there were systematic, strong correlations betweennodes, which violated the independence approxima-tion used in global thresholding.
This prevented usfrom using global thresholding with these grammars.In the future, we may modify global thresholding tomodel some of these correlations.7.6 Experiments combining GlobalThresholding and Beam ThresholdingV(hile global thresholding works better than beamthresholding in general, each has its own strengths.Global thresholding can threshold across cells, butbecause of the approximations used, the thresholdsmust generally be looser.
Beam thresholding canonly threshold within a cell, but can do so fairlytightly.
Combining the two offers the potential to21706560555010080787674727068666462Precision/Recall15 Sentencesrecallprecision -+--*//1000TimePrecision/Recall200 Sentencesi i * .-erecallprecision -~---/.
.
.
.
i .
.
.
.
.
.
.
,1 000 10050Time1750Total Entropy15 Sentences170016501600~.
155021500145014001350130012501001750017000165OOo16000I \155001500014500, , , , , , i1000TimeTotal Entropy200 Sentences, , , ,  i , , , , , , , i1000 10000TimeFigure 11: Smoothness for Precision and Recall versus Total Inside for Different Test Data Sizes217500 ..... ?
.
.
.
.
.
.
.
.
.
.
.
.17000 1 \1650016000155001500014500 .
.
.
.
.  '
.
.
.
.
.
.
.
.1000 10000TimeX axis: log(time)Y axis: entropyPriorNo Prior -+--\ii167861578615286214986148861483614806\ Prior - -~",~ No Prior -~---!
"x",\?10000TimeX axis: log(time)Y axis: log(entropy - asymptote)1000Figure 12: Beam Thresholding with and without the Prior Probability, Two Different Scales22oILl0I --744207142070420699206962069520\ Beam o% ~ Global -+  .. .
.,,., .
Beam and Global --a ...."El.
"'-,.~..""El ""~.?
.
.
.
~, I , , , , , ?
~ , "~ ?000 10000TimeFigure 13: Combining Beam and Global Searchget the advantages of both.
We ran a series of experi-ments using the thresholding optimization algorithmof Section 5.
Figure 13 gives the results.
The com-bination of beam and global thresholding together isclearly better than either alone, in some cases run-ning 40% faster than global thresholding alone, whileachieving the same performance l vel.
The combi-nation generally runs twice as fast as beam thresh-olding alone, although up to a factor of three.7.7 Experiments in Multiple-Pass ParsingMultiple-pass parsing improves even further on ourexperiments combining beam and global threshold-ing.
Note that we used both beam and global thresh-olding for both the first and second pass in these ex-periments.
The first pass grammar was the very sim-ple terminal-prime grammar, and the second passgrammar was the usual 6-gram grammar.We evaluated multiple-pass parsing slightly dif-ferently from the other thresholding techniques.
Inthe experiments conducted here, our first and sec-ond pass grammars were very different from eachother.
For a given parse to be returned, it mustbe in the intersection of both grammars, and rea-sonably likely according to both.
Since the first andsecond pass grammars capture different information,parses which are likely according to both are espe-cially good.
The entropy of a sentence measuresits likelihood according to the second pass, but ig-nores the fact that the returned parse must also belikely according to the first pass.
Thus, entropy, ourmeasure in the previous experiments, which mea-sures only likelihood according to the final pass, isnot necessarily the right measure to use.
We there-fore give precision and recall results in this section.We still optimized our thresholding parameters us-ing the same 31 sentence held out corpus, and min-imizing entropy versus number of productions, asbefore.We should note that when we used a first passgrammar that captured a strict subset of the infor-mation in the second pass grammar, we have foundthat entropy is a very good measure of performance.As in our earlier experiments, it tends to be well cor-related with precision and recall but less subject tonoise.
It  is only because of the grammar mismatchthat we have changed the evaluation.Figure 14 shows precision and recall curves for sin-gle pass versus multiple pass experiments.
As in theentropy curves, we can determine the performance2380"U(1)oO~e7978777675\[- : , **"i o ' ".B??
: .
, - ?, ' \ ]? '
/  /74 I ~" i pl tS7372........ :..-X- .................... "X-.
.
.
.
.
.
.
.
.
.
x ' " ' " ' \ ]~ .
.
.
.
.
-  .
.B  - - - - " : - " : ' - " - - : ' - " : - " : ' - " - " : ' : ' : ' : ' : ' : ' : ' : '~ - .x... .~( ........... , ,  * ""EJ" ?
"x ..'"'" "'~'" Beam Reca l l  o~.--" Globa l  Reca l l  -+  ...../" Beam and G loba l  Reca l l  --E~ ..... /" Mul t i -Pass  Reca l l  ~ ........ "/ /  Beam Prec is ion -~ ....,// Globa l  Prec is ion -~ ....f Beam and G loba l  Prec is ion --~ ....Mu l t i -Pass  Prec is ion --+ .......-4..- .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
+ .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
+ .
.
.
.
.
.v;g,.,~, .
.
.
.
.
.
.. .
.
.
+- -  .~ .
.
~- - '~  .
.
.
.
.
.
.
.
.
.
.
.
.
~ .
.
.
.
: :~ .
: .~_~: .~:==~ .
.
.
.
.
~,---2,.+.-"" .
.
.
.
2;:" ........ ~ ....... --- :~"/ .r ,r,/ .
i./ p.~10000T imeFigure 14: Multiple Pass Parsing vs. Beam and Global vs. Beamratio by looking across horizontally.
For instance,the multi-pass recognizer achieves a 74% recall evelusing 2500 seconds, while the best single pass al-gorithm requires about 4500 seconds to reach thatlevel.
Due to the noise resulting from precision andrecall measurements, it is hard to exactly quantifythe advantage from multiple pass parsing, but it isgenerally about 50%.8 Applications and Conclusions8.1 Application to Other FormalismsIn this paper, we only considered applying multiple-pass and global thresholding techniques to pars-ing probabilistic ontext-free grammars.
However,just about any probabilistic grammar formalismfor which inside and outside probabilities can becomputed can benefit from these techniques.
Forinstance, Probabilistic Link Grammars (Lafferty,Sleator, and Temperley, 1992) could benefit fromour algorithms.
We have however had trouble us-ing global thresholding with grammars that stronglyviolated the independence assumptions of globalthresholding.One especially interesting possibility is to applymultiple-pass techniques to formalisms that require>> O(n 3) parsing time, such as Stochastic Brack-eting Transduction Grammar (SBTG) (Wu, 1996)and Stochastic Tree Adjoining Grammars (STAG)(Resnik, 1992; Schabes, 1992).
SBTG is a context-free-like formalism designed for translation from onelanguage to another; it uses a four dimensional chartto index spans in both the source and target lan-guage simultaneously.
It would be interesting to tryspeeding up an SBTG parser by running an O(n 3)first pass on the source language alone, and usingthis to prune parsing of the full SBTG.The STAG formalism is a mildly context-sensitiveformalism, requiring O(n 6) time to parse.
MostSTAG productions in practical grammars are actu-ally context-free.
The traditional way to speed upSTAG parsing is to use the context-free subset of anSTAG to form a Stochastic Tree Insertion Grammar(STIG) (Schabes and Waters, 1994), an O(n 3) for-malism, but this method has problems, because theSTIG undergenerates since it is missing some ele-mentary trees.
A different approach would be to usemultiple-pass parsing.
We could first find a context-free covering rammar for the STAG, and use thisas a first pass, and then use the full STAG for thesecond pass.248.2 Conc lus ionsThe grammars described here are fairly simple, pre-sented for purposes of explication.
In other workin preparation, in which we have used a signifi-cantly more complicated grammar, which we call theProbabilistic Feature Grammar (PFG), the improve-ments from multiple-pass parsing are even more dra-matic: single pass experiments are simply too slowto run at all.We have also found the automatic thresholdingparameter optimization algorithm to be very use-ful.
Before writing the parameter optimization al-gorithm, we developed the PFG grammar and themultiple-pass parsing technique and ran a series ofexperiments using hand optimized parameters.
Werecently ran the optimization algorithm and reranthe experiments, achieving a factor of two speedupwith no performance loss.
While we had not spenta great deal of time hand optimizing these param-eters, we are very encouraged by the optimizationalgorithm's practical utility.This paper introduces four new techniques:beam thresholding with priors, global threshold-ing, multiple-pass parsing, and automatic search forthresholding parameters.
Beam thresholding with?
priors can lead to almost an order of magnitude im-provement over beam thresholding without priors.Global thresholding can be up to three times as ef-ficient as the new beam thresholding technique, al-though the typical improvement is closer to 50%.When global thresholding and beam thresholdingare combined, they are usually two to three timesas fast as beam thresholding alone.
Multiple-passparsing can lead to up to an additional 50% improve-ment with the grammars in this paper.
We expectthe parameter optimization algorithm to be broadlyuseful.Re ferencesCarabailo, Sharon and Eugene Charniak.
1996.
Fig-ures of merit for best-first probabilistic chart pars-ing.
In Proceedings of the Conference on Era-.pirical Methods in Natural Language Processing,pages 127-132, Philadelphia, May.Caraballo, Sharon and Eugene Charniak.
1997.New figures of merit for best first probabilis-tic chart parsing.
In submission.
Available fromhttp ://www.
cs.
brown, edu/people/sc/NewFiguresofMer i t .
ps.
Z .Charniak, Eugene.
1996.
Tree-bank grammars.Technical Report CS-96-02, Department of Com-puter Science, Brown University.
Available fromf tp  : / / f tp .
cs.
brown, edu/pub/t echreport s/96/cs96-O2.ps .
Z .Collins, Michael.
1996.
A new statistical parserbased on bigram lexical dependencies.
In Proceed-ings of the 3~th Annual Meeting of the ACL, pages184-191, Santa Cruz, CA, June.Lafferty, John, Daniel Sleator, and Davy Temper-ley.
1992.
Grammatical trigrams: A probabilis-tic model of link grammar.
In Proceedings of the1992 AAAI Fall Symposium on Probabilistic Ap-proaches to Natural Language, October.Rayner, Manny and David Carter.
1996.
Fast pars-ing using pruning and grammar specialization.
InProceedings of the 3~th Annual Meeting of theACL, pages 223--230, Santa Cruz, CA, June.Resnik, P. 1992.
Probabilistic tree-adjoining gram-mar as a framework for statistical natural an-guage processing.
In Proceedings ofthe l~th Inter-national Conference on Computational Linguis-tics, Nantes, France, August.Schabes, Y. and R. Waters.
1994.
Tree insertiongrammar: A cubic-time parsable formalism thatlexicalizes context-free grammar without changingthe tree produced.
Technical Report TR-94-13,Mitsubishi Electric Research Laboratories.Schabes, Yves.
1992.
Stochastic lexicalized tree-adjoining rammars.
In Proceedings of the l~thInternational Conference on Computational Lin-guistics, pages 426--432, Nantes, France, August.Schwartz, Richard, Steve Austin, Francis Kubala,John Makhoul, Long Nguyen, Paul Placeway,and George Zavaliagkos.
1992.
New uses forthe n-best sentence hypothesis within the byb-los speech recognition system.
In Proceedings ofthe IEEE International Conference on Acoustics,Speech and Signal Processing, volume I, pages 1-4,San Francisco, California.Stolcke, Andreas.
1993.
An efficient probabilisticcontext-free parsing algorithm that computes pre-fix probabilities.
Technical Report TR-93-065, In-ternational Computer Science Institute, Berkeley,CA.Wu, Dekai.
1996.
A polynomial-time algorithm forstatistical machine translation.
In Proceedings ofthe 3~th Annual Meeting of the ACL, pages 152-158, Santa Cruz, CA, June.Zavaliagkos, G., T. Anastasakos, G. Chou, C. Lapre,F.
Kubala, J. Makhoul, L. Nguyen, R. Schwartz,and Y. Zhao.
1994.
Improved search, acousticand language, modeling in the BBN Byblos largevocabulary CSR system.
In Proceedings of theARPA Workshop on Spoken Language Technol-ogy, pages 81-88, Phdnsboro, New Jersey.25
