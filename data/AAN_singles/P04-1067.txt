A Geometric View on Bilingual Lexicon Extraction from ComparableCorporaE.
Gaussier?, J.-M.
Renders?, I.
Matveeva?, C.
Goutte?, H.
De?jean?
?Xerox Research Centre Europe6, Chemin de Maupertuis ?
38320 Meylan, FranceEric.Gaussier@xrce.xerox.com?Dept of Computer Science, University of Chicago1100 E. 58th St. Chicago, IL 60637 USAmatveeva@cs.uchicago.eduAbstractWe present a geometric view on bilingual lexiconextraction from comparable corpora, which allowsto re-interpret the methods proposed so far and iden-tify unresolved problems.
This motivates three newmethods that aim at solving these problems.
Empir-ical evaluation shows the strengths and weaknessesof these methods, as well as a significant gain in theaccuracy of extracted lexicons.1 IntroductionComparable corpora contain texts written in differ-ent languages that, roughly speaking, ?talk aboutthe same thing?.
In comparison to parallel corpora,ie corpora which are mutual translations, compara-ble corpora have not received much attention fromthe research community, and very few methods havebeen proposed to extract bilingual lexicons fromsuch corpora.
However, except for those found intranslation services or in a few international organ-isations, which, by essence, produce parallel docu-mentations, most existing multilingual corpora arenot parallel, but comparable.
This concern is re-flected in major evaluation conferences on cross-language information retrieval (CLIR), e.g.
CLEF1,which only use comparable corpora for their multi-lingual tracks.We adopt here a geometric view on bilingual lex-icon extraction from comparable corpora which al-lows one to re-interpret the methods proposed thusfar and formulate new ones inspired by latent se-mantic analysis (LSA), which was developed withinthe information retrieval (IR) community to treatsynonymous and polysemous terms (Deerwester etal., 1990).
We will explain in this paper the moti-vations behind the use of such methods for bilin-gual lexicon extraction from comparable corpora,and show how to apply them.
Section 2 is devoted tothe presentation of the standard approach, ie the ap-proach adopted by most researchers so far, its geo-metric interpretation, and the unresolved synonymy1http://clef.iei.pi.cnr.it:2002/and polysemy problems.
Sections 3 to 4 then de-scribe three new methods aiming at addressing theissues raised by synonymy and polysemy: in sec-tion 3 we introduce an extension of the standard ap-proach, and show in appendix A how this approachrelates to the probabilistic method proposed in (De-jean et al, 2002); in section 4, we present a bilin-gual extension to LSA, namely canonical correla-tion analysis and its kernel version; lastly, in sec-tion 5, we formulate the problem in terms of prob-abilistic LSA and review different associated simi-larities.
Section 6 is then devoted to a large-scaleevaluation of the different methods proposed.
Openissues are then discussed in section 7.2 Standard approachBilingual lexicon extraction from comparable cor-pora has been studied by a number of researchers,(Rapp, 1995; Peters and Picchi, 1995; Tanaka andIwasaki, 1996; Shahzad et al, 1999; Fung, 2000,among others).
Their work relies on the assump-tion that if two words are mutual translations, thentheir more frequent collocates (taken here in a verybroad sense) are likely to be mutual translations aswell.
Based on this assumption, the standard ap-proach builds context vectors for each source andtarget word, translates the target context vectors us-ing a general bilingual dictionary, and compares thetranslation with the source context vector:1.
For each source word v (resp.
target word w),build a context vector ?
?v (resp.
?
?w ) consistingin the measure of association of each word e(resp.
f ) in the context of v (resp.
w), a(v, e).2.
Translate the context vectors with a generalbilingual dictionary D, accumulating the con-tributions from words that yield identical trans-lations.3.
Compute the similarity between source word vand target word w using a similarity measures,such as the Dice or Jaccard coefficients, or thecosine measure.As the dot-product plays a central role in all thesemeasures, we consider, without loss of generality,the similarity given by the dot-product between ?
?vand the translation of ?
?w :??
?v ,????tr(w)?
=?ea(v, e)?f,(e,f)inDa(w, f)=?
(e,f)?Da(v, e) a(w, f) (1)Because of the translation step, only the pairs (e, f)that are present in the dictionary contribute to thedot-product.Note that this approach requires some generalbilingual dictionary as initial seed.
One way to cir-cumvent this requirement consists in automaticallybuilding a seed lexicon based on spelling and cog-nates clues (Koehn and Knight, 2002).
Another ap-proach directly tackles the problem from scratch bysearching for a translation mapping which optimallypreserves the intralingual association measure be-tween words (Diab and Finch, 2000): the under-lying assumption is that pairs of words which arehighly associated in one language should have trans-lations that are highly associated in the other lan-guage.
In this latter case, the association measureis defined as the Spearman rank order correlationbetween their context vectors restricted to ?periph-eral tokens?
(highly frequent words).
The searchmethod is based on a gradient descent algorithm, byiteratively changing the mapping of a single worduntil (locally) minimizing the sum of squared differ-ences between the association measure of all pairsof words in one language and the association mea-sure of the pairs of translated words obtained by thecurrent mapping.2.1 Geometric presentationWe denote by si, 1 ?
i ?
p and tj , 1 ?
j ?
q thesource and target words in the bilingual dictionaryD.
D is a set of n translation pairs (si, tj), andmay be represented as a p ?
q matrix M, such thatMij = 1 iff (si, tj) ?
D (and 0 otherwise).2Assuming there are m distinct source wordse1, ?
?
?
, em and r distinct target words f1, ?
?
?
, fr inthe corpus, figure 1 illustrates the geometric view ofthe standard method.The association measure a(v, e) may be viewedas the coordinates of the m-dimensional contextvector ?
?v in the vector space formed by the or-thogonal basis (e1, ?
?
?
, em).
The dot-product in (1)only involves source dictionary entries.
The corre-sponding dimensions are selected by an orthogonal2The extension to weighted dictionary entries Mij ?
[0, 1]is straightforward but not considered here for clarity.projection on the sub-space formed by (s1, ?
?
?
, sp),using a p ?
m projection matrix Ps.
Note that(s1, ?
?
?
, sp), being a sub-family of (e1, ?
?
?
, em), isan orthogonal basis of the new sub-space.
Similarly,?
?w is projected on the dictionary entries (t1, ?
?
?
, tq)using a q ?
r orthogonal projection matrix Pt.
AsM encodes the relationship between the source andtarget entries of the dictionary, equation 1 may berewritten as:S(v, w) = ??
?v ,????tr(w)?
= (Ps?
?v )> M (Pt?
?w ) (2)where > denotes transpose.
In addition, notice thatM can be rewritten as S>T , with S an n ?
p andT an n ?
q matrix encoding the relations betweenwords and pairs in the bilingual dictionary (e.g.
Skiis 1 iff si is in the kth translation pair).
Hence:S(v, w)=?
?v>P>s S>TPt?
?w =?SPs?
?v , TPt?
?w ?
(3)which shows that the standard approach amounts toperforming a dot-product in the vector space formedby the n pairs ((s1, tl), ?
?
?
, (sp, tk)), which are as-sumed to be orthogonal, and correspond to transla-tion pairs.2.2 Problems with the standard approachThere are two main potential problems associatedwith the use of a bilingual dictionary.Coverage.
This is a problem if too few corpuswords are covered by the dictionary.
However, ifthe context is large enough, some context wordsare bound to belong to the general language, so ageneral bilingual dictionary should be suitable.
Wethus expect the standard approach to cope well withthe coverage problem, at least for frequent words.For rarer words, we can bootstrap the bilingual dic-tionary by iteratively augmenting it with the mostprobable translations found in the corpus.Polysemy/synonymy.
Because all entries on ei-ther side of the bilingual dictionary are treated as or-thogonal dimensions in the standard methods, prob-lems may arise when several entries have the samemeaning (synonymy), or when an entry has sev-eral meanings (polysemy), especially when onlyone meaning is represented in the corpus.Ideally, the similarities wrt synonyms should notbe independent, but the standard method fails to ac-count for that.
The axes corresponding to synonymssi and sj are orthogonal, so that projections of acontext vector on si and sj will in general be uncor-related.
Therefore, a context vector that is similar tosi may not necessarily be similar to sj .A similar situation arises for polysemous entries.Suppose the word bank appears as both financial in-stitution (French: banque) and ground near a riverPse 2e mve 1 s 1s pv?
(s  ,t  )tt fff(s  ,t  )1 1(s  ,t  ) 21rww?1pPtS Tp k1 iv"w"Figure 1: Geometric view of the standard approach(French: berge), but only the pair (banque, bank)is in the bilingual dictionary.
The standard methodwill deem similar river, which co-occurs with bank,and argent (money), which co-occurs with banque.In both situations, however, the context vectors ofthe dictionary entries provide some additional infor-mation: for synonyms si and sj , it is likely that ?
?siand ?
?sj are similar; for polysemy, if the context vec-tors????
?banque and ??
?bank have few translations pairs incommon, it is likely that banque and bank are usedwith somewhat different meanings.
The followingmethods try to leverage this additional information.3 Extension of the standard approachThe fact that synonyms may be captured throughsimilarity of context vectors3 leads us to questionthe projection that is made in the standard method,and to replace it with a mapping into the sub-spaceformed by the context vectors of the dictionary en-tries, that is, instead of projecting ?
?v on the sub-space formed by (s1, ?
?
?
, sp), we now map it ontothe sub-space generated by (?
?s1 , ?
?
?
,??sp).
With thismapping, we try to find a vector space in which syn-onymous dictionary entries are close to each other,while polysemous ones still select different neigh-bors.
This time, if ?
?v is close to ?
?si and ?
?sj , si andsj being synonyms, the translations of both si andsj will be used to find those words w close to v.Figure 2 illustrates this process.
By denoting Qs,respectively Qt, such a mapping in the source (resp.target) side, and using the same translation mapping(S, T ) as above, the similarity between source andtarget words becomes:S(v, w)=?SQs?
?v , TQt?
?w ?=?
?v>Q>s S>TQt?
?w (4)A natural choice for Qs (and similarly for Qt) is thefollowing m ?
p matrix:Qs = R>s =??
?a(s1, e1) ?
?
?
a(sp, e1).........a(s1, em) ?
?
?
a(sp, em)??
?3This assumption has been experimentally validated in sev-eral studies, e.g.
(Grefenstette, 1994; Lewis et al, 1967).but other choices, such as a pseudo-inverse of Rs,are possible.
Note however that computing thepseudo-inverse of Rs is a complex operation, whilethe above projection is straightforward (the columnsof Q correspond to the context vectors of the dic-tionary words).
In appendix A we show how thismethod generalizes over the probabilistic approachpresented in (Dejean et al, 2002).
The abovemethod bears similarities with the one describedin (Besanc?on et al, 1999), where a matrix similarto Qs is used to build a new term-document ma-trix.
However, the motivations behind their workand ours differ, as do the derivations and the gen-eral framework, which justifies e.g.
the choice ofthe pseudo-inverse of Rs in our case.4 Canonical correlation analysisThe data we have at our disposal can naturally berepresented as an n ?
(m + r) matrix in whichthe rows correspond to translation pairs, and thecolumns to source and target vocabularies:C =e1 ?
?
?
em f1 ?
?
?
fr?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
(s(1), t(1)).....................?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
(s(n), t(n))where (s(k), t(k)) is just a renumbering of the trans-lation pairs (si, tj).Matrix C shows that each translation pair sup-ports two views, provided by the context vectors inthe source and target languages.
Each view is con-nected to the other by the translation pair it repre-sents.
The statistical technique of canonical corre-lation analysis (CCA) can be used to identify direc-tions in the source view (first m columns of C) andtarget view (last r columns of C) that are maximallycorrelated, ie ?behave in the same way?
wrt thetranslation pairs.
We are thus looking for directionsin the source and target vector spaces (defined bythe orthogonal bases (e1, ?
?
?
, em) and (f1, ?
?
?
, fr))such that the projections of the translation pairs onthese directions are maximally correlated.
Intu-itively, those directions define latent semantic axesseeevfff(s  ,t  )121rw1tS Teme1e2m12ssss(s  ,t  )1(s  ,t  )p1kiffr2f tttt12w"v"12pkqivwQ QFigure 2: Geometric view of the extended approachthat capture the implicit relations between transla-tion pairs, and induce a natural mapping across lan-guages.
Denoting by ?s and ?t the directions in thesource and target spaces, respectively, this may beformulated as:?
= max?s,?t?i??s,?
?s (i)???t,?
?t (i)???i??s,?
?s (i)??j??t,?
?t (j)?As in principal component analysis, once the firsttwo directions (?1s , ?1t ) have been identified, the pro-cess can be repeated in the sub-space orthogonalto the one formed by the already identified direc-tions.
However, a general solution based on a set ofeigenvalues can be proposed.
Following e.g.
(Bachand Jordan, 2001), the above problem can be re-formulated as the following generalized eigenvalueproblem:B ?
= ?D ?
(5)where, denoting again Rs and Rt the first m and lastr (respectively) columns of C, we define:B =( 0 RtR>t RsR>sRsR>s RtR>t 0),D =( (RsR>s )2 00 (RtR>t )2), ?
=( ?s?t)The standard approach to solve eq.
5 is to per-form an incomplete Cholesky decomposition of aregularized form of D (Bach and Jordan, 2001).This yields pairs of source and target directions(?1s , ?1t ), ?
?
?
, (?ls, ?lt) that define a new sub-space inwhich to project words from each language.
Thissub-space plays the same role as the sub-space de-fined by translation pairs in the standard method, al-though with CCA, it is derived from the corpus viathe context vectors of the translation pairs.
Onceprojected, words from different languages can becompared through their dot-product or cosine.
De-noting ?s =[?1s , .
.
.
?ls]>, and ?t =[?1t , .
.
.
?lt]>,the similarity becomes (figure 3):S(v, w) = ??s?
?v , ?t?
?w ?
= ?
?v>?>s ?t?
?w (6)The number l of vectors retained in each languagedirectly defines the dimensions of the final sub-space used for comparing words across languages.CCA and its kernelised version were used in (Vi-nokourov et al, 2002) as a way to build a cross-lingual information retrieval system from parallelcorpora.
We show here that it can be used to in-fer language-independent semantic representationsfrom comparable corpora, which induce a similaritybetween words in the source and target languages.5 Multilingual probabilistic latentsemantic analysisThe matrix C described above encodes in each rowk the context vectors of the source (first m columns)and target (last r columns) of each translation pair.Ideally, we would like to cluster this matrix suchthat translation pairs with synonymous words ap-pear in the same cluster, while translation pairs withpolysemous words appear in different clusters (softclustering).
Furthermore, because of the symmetrybetween the roles played by translation pairs and vo-cabulary words (synonymous and polysemous vo-cabulary words should also behave as describedabove), we want the clustering to behave symmet-rically with respect to translation pairs and vocabu-lary words.
One well-motivated method that fulfillsall the above criteria is Probabilistic Latent Seman-tic Analysis (PLSA) (Hofmann, 1999).Assuming that C encodes the co-occurrences be-tween vocabulary words w and translation pairs d,PLSA models the probability of co-occurrence wand d via latent classes ?
:P (w, d) =?
?P (?)
P (w|?)
P (d|?)
(7)where, for a given class, words and translation pairsare assumed to be independently generated fromclass-conditional probabilities P (w|?)
and P (d|?
).Note here that the latter distribution is language-independent, and that the same latent classes areused for the two languages.
The parameters of themodel are obtained by maximizing the likelihood ofthe observed data (matrix C) through Expectation-Maximisation algorithm (Dempster et al, 1977).
Ineeevfff21rw1ee1e2m12ffr2fv"vw(CCA)w"(CCA)m(?1s , ?1t )?1s?is?ls?2s(?ls, ?lt)(?2s , ?2t ) ?1t?lt?s ?t?2t?itFigure 3: Geometric view of the Canonical Correlation Analysis approachaddition, in order to reduce the sensitivity to initialconditions, we use a deterministic annealing scheme(Ueda and Nakano, 1995).
The update formulas forthe EM algorithm are given in appendix B.This model can identify relevant bilingual latentclasses, but does not directly define a similarity be-tween words across languages.
That may be doneby using Fisher kernels as described below.Associated similarities: Fisher kernelsFisher kernels (Jaakkola and Haussler, 1999) de-rive a similarity measure from a probabilistic model.They are useful whenever a direct similarity be-tween observed feature is hard to define or in-sufficient.
Denoting `(w) = lnP (w|?)
the log-likelihood for example w, the Fisher kernel is:K(w1, w2) = ?`(w1)>IF?1?`(w2) (8)The Fisher information matrix IF =E(?`(x)?`(x)>)keeps the kernel indepen-dent of reparameterisation.
With a suitableparameterisation, we assume IF ?
1.
For PLSA(Hofmann, 2000), the Fisher kernel between twowords w1 and w2 becomes:K(w1, w2) =?
?P (?|w1)P (?|w2)P (?)
(9)+?dP?
(d|w1)P?
(d|w2)?
?P (?|d,w1)P (?|d,w2)P (d|?
)where d ranges over the translation pairs.
TheFisher kernel performs a dot-product in a vectorspace defined by the parameters of the model.
Withonly one class, the expression of the Fisher kernel(9) reduces to:K(w1, w2) = 1 +?dP?
(d|w1)P?
(d|w2)P (d)Apart from the additional intercept (?1?
), this isexactly the similarity provided by the standardmethod, with associations given by scaled empir-ical frequencies a(w, d) = P?
(d|w)/?P (d).
Ac-cordingly, we expect that the standard method andthe Fisher kernel with one class should have simi-lar behaviors.
In addition to the above kernel, weconsider two additional versions, obtained:throughnormalisation (NFK) and exponentiation (EFK):NFK(w1, w2) =K(w1, w2)?K(w1)K(w2)(10)EFK(w1, w2) = e?12 (K(w1)+K(w2)?2K(w1,w2))where K(w) stands for K(w, w).6 Experiments and resultsWe conducted experiments on an English-Frenchcorpus derived from the data used in the multi-lingual track of CLEF2003, corresponding to thenewswire of months May 1994 and December 1994of the Los Angeles Times (1994, English) and LeMonde (1994, French).
As our bilingual dictionary,we used the ELRA multilingual dictionary,4 whichcontains ca.
13,500 entries with at least one matchin our corpus.
In addition, the following linguis-tic preprocessing steps were performed on both thecorpus and the dictionary: tokenisation, lemmatisa-tion and POS-tagging.
Only lexical words (nouns,verbs, adverbs, adjectives) were indexed and onlysingle word entries in the dicitonary were retained.Infrequent words (occurring less than 5 times) werediscarded when building the indexing terms and thedictionary entries.
After these steps our corpus con-tains 34,966 distinct English words, and 21,140 dis-tinct French words, leading to ca.
25,000 Englishand 13,000 French words not present in the dictio-nary.To evaluate the performance of our extractionmethods, we randomly split the dictionaries into atraining set with 12,255 entries, and a test set with1,245 entries.
The split is designed in such a waythat all pairs corresponding to the same source wordare in the same set (training or test).
All methodsuse the training set as the sole available resourceand predict the most likely translations of the termsin the source language (English) belonging to the4Available through www.elra.infotest set.
The context vectors were defined by com-puting the mutual information association measurebetween terms occurring in the same context win-dow of size 5 (ie.
by considering a neighborhood of+/- 2 words around the current word), and summingit over all contexts of the corpora.
Different associ-ation measures and context sizes were assessed andthe above settings turned out to give the best perfor-mance even if the optimum is relatively flat.
Formemory space and computational efficiency rea-sons, context vectors were pruned so that, for eachterm, the remaining components represented at least90 percent of the total mutual information.
Afterpruning, the context vectors were normalised so thattheir Euclidean norm is equal to 1.
The PLSA-basedmethods used the raw co-occurrence counts as asso-ciation measure, to be consistent with the underly-ing generative model.
In addition, for the extendedmethod, we retained only the N (N = 200 is thevalue which yielded the best results in our experi-ments) dictionary entries closest to source and tar-get words when doing the projection with Q. Asdiscussed below, this allows us to get rid of spuri-ous relationships.The upper part of table 1 summarizes the resultswe obtained, measured in terms of F-1 score fordifferent lengths of the candidate list, from 20 to500.
For each length, precision is based on the num-ber of lists that contain an actual translation of thesource word, whereas recall is based on the num-ber of translations provided in the reference set andfound in the list.
Note that our results differ from theones previously published, which can be explainedby the fact that first our corpus is relatively smallcompared to others, second that our evaluation re-lies on a large number of candidates, which can oc-cur as few as 5 times in the corpus, whereas previousevaluations were based on few, high frequent terms,and third that we do not use the same bilingual dic-tionary, the coverage of which being an importantfactor in the quality of the results obtained.
Longcandidate lists are justified by CLIR considerations,where longer lists might be preferred over shorterones for query expansion purposes.
For PLSA, thenormalised Fisher kernels provided the best results,and increasing the number of latent classes did notlead in our case to improved results.
We thus dis-play here the results obtained with the normalisedversion of the Fisher kernel, using only one compo-nent.
For CCA, we empirically optimised the num-ber of dimensions to be used, and display the resultsobtained with the optimal value (l = 300).As one can note, the extended approach yieldsthe best results in terms of F1-score.
However, itsperformance for the first 20 candidates are belowthe standard approach and comparable to the PLSA-based method.
Indeed, the standard approach leadsto higher precision at the top of the list, but lowerrecall overall.
This suggests that we could gain inperformance by re-ranking the candidates of the ex-tended approach with the standard and PLSA meth-ods.
The lower part of table 1 shows that this isindeed the case.
The average precision goes upfrom 0.4 to 0.44 through this combination, and theF1-score is significantly improved for all the lengthranges we considered (bold line in table 1).7 DiscussionExtended method As one could expect, the ex-tended approach improves the recall of our bilinguallexicon extraction system.
Contrary to the standardapproach, in the extended approach, all the dictio-nary words, present or not in the context vector of agiven word, can be used to translate it.
This leads toa noise problem since spurious relations are boundto be detected.
The restriction we impose on thetranslation pairs to be used (N nearest neighbors)directly aims at selecting only the translation pairswhich are in true relation with the word to be trans-lated.Multilingual PLSA Even though theoreticallywell-founded, PLSA does not lead to improved per-formance.
When used alone, it performs slightlybelow the standard method, for different numbersof components, and performs similarly to the stan-dard method when used in combination with theextended method.
We believe the use of mere co-occurrence counts gives a disadvantage to PLSAover other methods, which can rely on more sophis-ticated measures.
Furthermore, the complexity ofthe final vector space (several millions of dimen-sions) in which the comparison is done entails alonger processing time, which renders this methodless attractive than the standard or extended ones.Canonical correlation analysis The results we ob-tain with CCA and its kernel version are disappoint-ing.
As already noted, CCA does not directly solvethe problems we mentioned, and our results showthat CCA does not provide a good alternative to thestandard method.
Here again, we may suffer from anoise problem, since each canonical direction is de-fined by a linear combination that can involve manydifferent vocabulary words.Overall, starting with an average precision of 0.35as provided by the standard approach, we were ableto increase it to 0.44 with the methods we consider.Furthermore, we have shown here that such an im-provement could be achieved with relatively simple20 60 100 160 200 260 300 400 500 Avg.
Prec.standard 0.14 0.20 0.24 0.29 0.30 0.33 0.35 0.38 0.40 0.35Ext (N=500) 0.11 0.21 0.27 0.32 0.34 0.38 0.41 0.45 0.50 0.40CCA (l=300) 0.04 0.10 0.14 0.20 0.22 0.26 0.29 0.35 0.41 0.25NFK(k=1) 0.10 0.15 0.20 0.23 0.26 0.27 0.28 0.32 0.34 0.30Ext + standard 0.16 0.26 0.32 0.37 0.40 0.44 0.45 0.47 0.50 0.44Ext + NFK(k=1) 0.13 0.23 0.28 0.33 0.38 0.42 0.44 0.48 0.50 0.42Ext + NFK(k=4) 0.13 0.22 0.26 0.33 0.37 0.40 0.42 0.47 0.50 0.41Ext + NFK (k=16) 0.12 0.20 0.25 0.32 0.36 0.40 0.42 0.47 0.50 0.40Table 1: Results of the different methods; F-1 score at different number of candidate translations.
Ext refersto the extended approach, whereas NFK stands for normalised Fisher kernel.methods.
Nevertheless, there are still a number ofissues that need be addressed.
The most impor-tant one concerns the combination of the differentmethods, which could be optimised on a validationset.
Such a combination could involve Fisher ker-nels with different latent classes in a first step, anda final combination of the different methods.
How-ever, the results we obtained so far suggest that therank of the candidates is an important feature.
It isthus not guaranteed that we can gain over the com-bination we used here.8 ConclusionWe have shown in this paper how the problem ofbilingual lexicon extraction from comparable cor-pora could be interpreted in geometric terms, andhow this view led to the formulation of new solu-tions.
We have evaluated the methods we proposeon a comparable corpus extracted from the CLEFcolection, and shown the strengths and weaknessesof each method.
Our final results show that the com-bination of relatively simple methods helps improvethe average precision of bilingual lexicon extrac-tion methods from comparale corpora by 10 points.We hope this work will help pave the way towardsa new generation of cross-lingual information re-trieval systems.AcknowledgementsWe thank J.-C. Chappelier and M. Rajman whopointed to us the similarity between our extendedmethod and the model DSIR (distributional seman-tics information retrieval), and provided us withuseful comments on a first draft of this paper.
Wealso want to thank three anonymous reviewers foruseful comments on a first version of this paper.ReferencesF.
R. Bach and M. I. Jordan.
2001.
Kernel inde-pendent component analysis.
Journal of MachineLearning Research.R.
Besanc?on, M. Rajman, and J.-C. Chappelier.1999.
Textual similarities based on a distribu-tional approach.
In Proceedings of the Tenth In-ternational Workshop on Database and ExpertSystems Applications (DEX?99), Florence, Italy.S.
Deerwester, S. T. Dumais, G. W. Furnas, T. K.Landauer, and R. Harshman.
1990.
Indexing bylatent semantic analysis.
Journal of the AmericanSociety for Information Science, 41(6):391?407.H.
Dejean, E. Gaussier, and F. Sadat.
2002.
An ap-proach based on multilingual thesauri and modelcombination for bilingual lexicon extraction.
InInternational Conference on Computational Lin-guistics, COLING?02.A.
P. Dempster, N. M. Laird, and D. B. Ru-bin.
1977.
Maximum likelihood from incom-plete data via the EM algorithm.
Journal of theRoyal Statistical Society, Series B, 39(1):1?38.Mona Diab and Steve Finch.
2000.
A statisti-cal word-level translation model for compara-ble corpora.
In Proceeding of the Conferenceon Content-Based Multimedia Information Ac-cess (RIAO).Pascale Fung.
2000.
A statistical view on bilinguallexicon extraction - from parallel corpora to non-parallel corpora.
In J.
Ve?ronis, editor, ParallelText Processing.
Kluwer Academic Publishers.G.
Grefenstette.
1994.
Explorations in AutomaticThesaurus Construction.
Kluwer Academic Pub-lishers.Thomas Hofmann.
1999.
Probabilistic latent se-mantic analysis.
In Proceedings of the FifteenthConference on Uncertainty in Artificial Intelli-gence, pages 289?296.
Morgan Kaufmann.Thomas Hofmann.
2000.
Learning the similarity ofdocuments: An information-geometric approachto document retrieval and categorization.
In Ad-vances in Neural Information Processing Systems12, page 914.
MIT Press.Tommi S. Jaakkola and David Haussler.
1999.
Ex-ploiting generative models in discriminative clas-sifiers.
In Advances in Neural Information Pro-cessing Systems 11, pages 487?493.Philipp Koehn and Kevin Knight.
2002.
Learninga translation lexicon from monolingual corpora.In ACL 2002 Workshop on Unsupervised LexicalAcquisition.P.A.W.
Lewis, P.B.
Baxendale, and J.L.
Ben-net.
1967.
Statistical discrimination of thesynonym/antonym relationship between words.Journal of the ACM.C.
Peters and E. Picchi.
1995.
Capturing the com-parable: A system for querying comparable textcorpora.
In JADT?95 - 3rd International Con-ference on Statistical Analysis of Textual Data,pages 255?262.R.
Rapp.
1995.
Identifying word translations innonparallel texts.
In Proceedings of the AnnualMeeting of the Association for ComputationalLinguistics.I.
Shahzad, K. Ohtake, S. Masuyama, and K. Ya-mamoto.
1999.
Identifying translations of com-pound nouns using non-aligned corpora.
In Pro-ceedings of the Workshop MAL?99, pages 108?113.K.
Tanaka and Hideya Iwasaki.
1996.
Extraction oflexical translations from non-aligned corpora.
InInternational Conference on Computational Lin-guistics, COLING?96.Naonori Ueda and Ryohei Nakano.
1995.
Deter-ministic annealing variant of the EM algorithm.In Advances in Neural Information ProcessingSystems 7, pages 545?552.A.
Vinokourov, J. Shawe-Taylor, and N. Cristian-ini.
2002.
Finding language-independent seman-tic representation of text using kernel canonicalcorrelation analysis.
In Advances in Neural In-formation Processing Systems 12.Appendix A: probabilistic interpretation ofthe extension of standard approachAs in section 3, SQs?
?v is an n-dimensional vector,defined over ((s1, tl), ?
?
?
, (sp, tk)).
The coordinateof SQs?
?v on the axis corresponding to the transla-tion pair (si, tj) is ??
?si ,?
?v ?
(the one for TQt?
?w onthe same axis being ??
?tj ,?
?w ?).
Thus, equation 4 canbe rewritten as:S(v, w) =?(si,tj)??
?si ,?
?v ???
?tj ,?
?w ?which we can normalised in order to get a probabil-ity distribution, leading to:S(v, w) =?
(si,tj)P (v)P (si|v)P (w|tj)P (tj)By imposing P (tj) to be uniform, and by denotingC a translation pair, one arrives at:S(v, w) ?
?CP (v)P (C|v)P (w|C)with the interpretation that only the source, resp.target, word in C is relevant for P (C|v), resp.P (w|C).
Now, if we are looking for those ws clos-est to a given v, we rely on:S(w|v) ?
?CP (C|v)P (w|C)which is the probabilistic model adopted in (Dejeanet al, 2002).
This latter model is thus a special caseof the extension we propose.Appendix B: update formulas for PLSAThe deterministic annealing EM algorithm forPLSA (Hofmann, 1999) leads to the following equa-tions for iteration t and temperature ?
:P (?|w, d) = P (?
)?P (w|?
)?P (d|?)??
?P (?
)?P (w|?
)?P (d|?
)?P (t+?)(?)
= 1?
(w,d) n(w, d)?
(w,d)n(w, d)P (?|w, d)P (t+?)(w|?)
=?d n(w, d)P (?|w, d)?
(w,d) n(w, d)P (?|w, d)P (t+?)(d|?)
=?w n(w, d)P (?|w, d)?
(w,d) n(w, d)P (?|w, d)where n(w, d) is the number of co-occurrences be-tween w and d. Parameters are obtained by iteratingeqs 11?11 for each ?, 0 < ?
?
1.
