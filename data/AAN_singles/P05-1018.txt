Proceedings of the 43rd Annual Meeting of the ACL, pages 141?148,Ann Arbor, June 2005. c?2005 Association for Computational LinguisticsModeling Local Coherence: An Entity-based ApproachRegina BarzilayComputer Science and Artificial Intelligence LaboratoryMassachusetts Institute of Technologyregina@csail.mit.eduMirella LapataSchool of InformaticsUniversity of Edinburghmlap@inf.ed.ac.ukAbstractThis paper considers the problem of auto-matic assessment of local coherence.
Wepresent a novel entity-based representa-tion of discourse which is inspired by Cen-tering Theory and can be computed au-tomatically from raw text.
We view co-herence assessment as a ranking learningproblem and show that the proposed dis-course representation supports the effec-tive learning of a ranking function.
Ourexperiments demonstrate that the inducedmodel achieves significantly higher ac-curacy than a state-of-the-art coherencemodel.1 IntroductionA key requirement for any system that producestext is the coherence of its output.
Not surprisingly,a variety of coherence theories have been devel-oped over the years (e.g., Mann and Thomson, 1988;Grosz et al 1995) and their principles have foundapplication in many symbolic text generation sys-tems (e.g., Scott and de Souza, 1990; Kibble andPower, 2004).
The ability of these systems to gener-ate high quality text, almost indistinguishable fromhuman writing, makes the incorporation of coher-ence theories in robust large-scale systems partic-ularly appealing.
The task is, however, challengingconsidering that most previous efforts have relied onhandcrafted rules, valid only for limited domains,with no guarantee of scalability or portability (Re-iter and Dale, 2000).
Furthermore, coherence con-straints are often embedded in complex representa-tions (e.g., Asher and Lascarides, 2003) which arehard to implement in a robust application.This paper focuses on local coherence, whichcaptures text relatedness at the level of sentence-to-sentence transitions, and is essential for generatingglobally coherent text.
The key premise of our workis that the distribution of entities in locally coherenttexts exhibits certain regularities.
This assumption isnot arbitrary ?
some of these regularities have beenrecognized in Centering Theory (Grosz et al, 1995)and other entity-based theories of discourse.The algorithm introduced in the paper automat-ically abstracts a text into a set of entity transi-tion sequences, a representation that reflects distri-butional, syntactic, and referential information aboutdiscourse entities.
We argue that this representationof discourse allows the system to learn the proper-ties of locally coherent texts opportunistically froma given corpus, without recourse to manual annota-tion or a predefined knowledge base.We view coherence assessment as a ranking prob-lem and present an efficiently learnable model thatorders alternative renderings of the same informa-tion based on their degree of local coherence.
Sucha mechanism is particularly appropriate for gener-ation and summarization systems as they can pro-duce multiple text realizations of the same underly-ing content, either by varying parameter values, orby relaxing constraints that control the generationprocess.
A system equipped with a ranking mech-anism, could compare the quality of the candidateoutputs, much in the same way speech recognizersemploy language models at the sentence level.Our evaluation results demonstrate the effective-ness of our entity-based ranking model within thegeneral framework of coherence assessment.
First,we evaluate the utility of the model in a text order-ing task where our algorithm has to select a max-imally coherent sentence order from a set of can-didate permutations.
Second, we compare the rank-ings produced by the model against human coher-ence judgments elicited for automatically generatedsummaries.
In both experiments, our method yields141a significant improvement over a state-of-the-art co-herence model based on Latent Semantic Analysis(Foltz et al, 1998).In the following section, we provide an overviewof existing work on the automatic assessment of lo-cal coherence.
Then, we introduce our entity-basedrepresentation, and describe our ranking model.Next, we present the experimental framework anddata.
Evaluation results conclude the paper.2 Related WorkLocal coherence has been extensively studied withinthe modeling framework put forward by CenteringTheory (Grosz et al, 1995; Walker et al, 1998;Strube and Hahn, 1999; Poesio et al, 2004; Kibbleand Power, 2004).
One of the main assumptions un-derlying Centering is that a text segment which fore-grounds a single entity is perceived to be more co-herent than a segment in which multiple entities arediscussed.
The theory formalizes this intuition by in-troducing constraints on the distribution of discourseentities in coherent text.
These constraints are for-mulated in terms of focus, the most salient entity ina discourse segment, and transition of focus betweenadjacent sentences.
The theory also establishes con-straints on the linguistic realization of focus, sug-gesting that it is more likely to appear in prominentsyntactic positions (such as subject or object), and tobe referred to with anaphoric expressions.A great deal of research has attempted to translateprinciples of Centering Theory into a robust coher-ence metric (Miltsakaki and Kukich, 2000; Hasler,2004; Karamanis et al, 2004).
Such a translation ischallenging in several respects: one has to specifythe ?free parameters?
of the system (Poesio et al,2004) and to determine ways of combining the ef-fects of various constraints.
A common methodol-ogy that has emerged in this research is to developand evaluate coherence metrics on manually anno-tated corpora.
For instance, Miltsakaki and Kukich(2000) annotate a corpus of student essays with tran-sition information, and show that the distribution oftransitions correlates with human grades.
Karamaniset al (2004) use a similar methodology to comparecoherence metrics with respect to their usefulnessfor text planning in generation.The present work differs from these approachesin two key respects.
First, our method does not re-quire manual annotation of input texts.
We do notaim to produce complete centering annotations; in-stead, our inference procedure is based on a dis-course representation that preserves essential entitytransition information, and can be computed auto-matically from raw text.
Second, we learn patternsof entity distribution from a corpus, without attempt-ing to directly implement or refine Centering con-straints.3 The Coherence ModelIn this section we introduce our entity-based repre-sentation of discourse.
We describe how it can becomputed and how entity transition patterns can beextracted.
The latter constitute a rich feature spaceon which probabilistic inference is performed.Text Representation Each text is representedby an entity grid, a two-dimensional array that cap-tures the distribution of discourse entities across textsentences.
We follow Miltsakaki and Kukich (2000)in assuming that our unit of analysis is the tradi-tional sentence (i.e., a main clause with accompa-nying subordinate and adjunct clauses).
The rows ofthe grid correspond to sentences, while the columnscorrespond to discourse entities.
By discourse en-tity we mean a class of coreferent noun phrases.
Foreach occurrence of a discourse entity in the text, thecorresponding grid cell contains information aboutits grammatical role in the given sentence.
Each gridcolumn thus corresponds to a string from a set ofcategories reflecting the entity?s presence or absencein a sequence of sentences.
Our set consists of foursymbols: S (subject), O (object), X (neither subjectnor object) and ?
(gap which signals the entity?s ab-sence from a given sentence).Table 1 illustrates a fragment of an entity gridconstructed for the text in Table 2.
Since the textcontains six sentences, the grid columns are oflength six.
Consider for instance the grid column forthe entity trial, [O ?
?
?
?
X].
It records that trialis present in sentences 1 and 6 (as O and X respec-tively) but is absent from the rest of the sentences.Grid Computation The ability to identify andcluster coreferent discourse entities is an impor-tant prerequisite for computing entity grids.
Thesame entity may appear in different linguistic forms,e.g., Microsoft Corp., Microsoft, and the company,but should still be mapped to a single entry in thegrid.
Table 1 exemplifies the entity grid for the textin Table 2 when coreference resolution is taken intoaccount.
To automatically compute entity classes,142DepartmentTrialMicrosoftEvidenceCompetitorsMarketsProductsBrandsCaseNetscapeSoftwareTacticsGovernmentSuitEarnings1 S O S X O ?
?
?
?
?
?
?
?
?
?
12 ?
?
O ?
?
X S O ?
?
?
?
?
?
?
23 ?
?
S O ?
?
?
?
S O O ?
?
?
?
34 ?
?
S ?
?
?
?
?
?
?
?
S ?
?
?
45 ?
?
?
?
?
?
?
?
?
?
?
?
S O ?
56 ?
X S ?
?
?
?
?
?
?
?
?
?
?
O 6Table 1: A fragment of the entity grid.
Noun phrasesare represented by their head nouns.1 [The Justice Department]S is conducting an [anti-trusttrial]O against [Microsoft Corp.]X with [evidence]X that[the company]S is increasingly attempting to crush[competitors]O.2 [Microsoft]O is accused of trying to forcefully buy into[markets]X where [its own products]S are not competitiveenough to unseat [established brands]O .3 [The case]S revolves around [evidence]O of [Microsoft]Saggressively pressuring [Netscape]O into merging[browser software]O .4 [Microsoft]S claims [its tactics]S are commonplace andgood economically.5 [The government]S may file [a civil suit]O rulingthat [conspiracy]S to curb [competition]O through[collusion]X is [a violation of the Sherman Act]O.6 [Microsoft]S continues to show [increased earnings]O de-spite [the trial]X.Table 2: Summary augmented with syntactic anno-tations for grid computation.we employ a state-of-the-art noun phrase coref-erence resolution system (Ng and Cardie, 2002)trained on the MUC (6?7) data sets.
The system de-cides whether two NPs are coreferent by exploit-ing a wealth of features that fall broadly into fourcategories: lexical, grammatical, semantic and posi-tional.Once we have identified entity classes, the nextstep is to fill out grid entries with relevant syn-tactic information.
We employ a robust statisticalparser (Collins, 1997) to determine the constituentstructure for each sentence, from which subjects (s),objects (o), and relations other than subject or ob-ject (x) are identified.
Passive verbs are recognizedusing a small set of patterns, and the underlying deepgrammatical role for arguments involved in the pas-sive construction is entered in the grid (see the gridcell o for Microsoft, Sentence 2, Table 2).When a noun is attested more than once with a dif-ferent grammatical role in the same sentence, we de-fault to the role with the highest grammatical rank-ing: subjects are ranked higher than objects, whichin turn are ranked higher than the rest.
For exam-ple, the entity Microsoft is mentioned twice in Sen-tence 1 with the grammatical roles x (for MicrosoftCorp.)
and s (for the company), but is representedonly by s in the grid (see Tables 1 and 2).Coherence Assessment We introduce a methodfor coherence assessment that is based on grid rep-resentation.
A fundamental assumption underlyingour approach is that the distribution of entities incoherent texts exhibits certain regularities reflectedin grid topology.
Some of these regularities are for-malized in Centering Theory as constraints on tran-sitions of local focus in adjacent sentences.
Grids ofcoherent texts are likely to have some dense columns(i.e., columns with just a few gaps such as Microsoftin Table 1) and many sparse columns which willconsist mostly of gaps (see markets, earnings in Ta-ble 1).
One would further expect that entities cor-responding to dense columns are more often sub-jects or objects.
These characteristics will be lesspronounced in low-coherence texts.Inspired by Centering Theory, our analysis re-volves around patterns of local entity transitions.A local entity transition is a sequence {S,O,X,?
}nthat represents entity occurrences and their syntacticroles in n adjacent sentences.
Local transitions canbe easily obtained from a grid as continuous subse-quences of each column.
Each transition will have acertain probability in a given grid.
For instance, theprobability of the transition [S ?]
in the grid fromTable 1 is 0.08 (computed as a ratio of its frequency(i.e., six) divided by the total number of transitionsof length two (i.e., 75)).
Each text can thus be viewedas a distribution defined over transition types.
Webelieve that considering all entity transitions mayuncover new patterns relevant for coherence assess-ment.We further refine our analysis by taking into ac-count the salience of discourse entities.
Centeringand other discourse theories conjecture that the wayan entity is introduced and mentioned depends onits global role in a given discourse.
Therefore, wediscriminate between transitions of salient entitiesand the rest, collecting statistics for each group sep-arately.
We identify salient entities based on their143SSSOSXS?OSOOOXO?XSXOXXX??S?O?X?
?d1 0 0 0 .03 0 0 0 .02 .07 0 0 .12 .02 .02 .05 .25d2 0 0 0 .02 0 .07 0 .02 0 0 .06 .04 0 0 0 .36d3 .02 0 0 .03 0 0 0 .06 0 0 0 .05 .03 .07 .07 .29Table 3: Example of a feature-vector document rep-resentation using all transitions of length two givensyntactic categories: S, O, X, and ?.frequency,1 following the widely accepted view thatthe occurrence frequency of an entity correlates withits discourse prominence (Morris and Hirst, 1991;Grosz et al, 1995).Ranking We view coherence assessment as aranking learning problem.
The ranker takes as inputa set of alternative renderings of the same documentand ranks them based on their degree of local coher-ence.
Examples of such renderings include a set ofdifferent sentence orderings of the same text and aset of summaries produced by different systems forthe same document.
Ranking is more suitable thanclassification for our purposes since in text gener-ation, a system needs a scoring function to com-pare among alternative renderings.
Furthermore, itis clear that coherence assessment is not a categori-cal decision but a graded one: there is often no singlecoherent rendering of a given text but many differentpossibilities that can be partially ordered.As explained previously, coherence constraintsare modeled in the grid representation implicitly byentity transition sequences.
To employ a machinelearning algorithm to our problem, we encode tran-sition sequences explicitly using a standard featurevector notation.
Each grid rendering j of a docu-ment di is represented by a feature vector ?
(xi j) =(p1(xi j), p2(xi j), .
.
.
, pm(xi j)), where m is the num-ber of all predefined entity transitions, and pt(xi j)the probability of transition t in grid xi j .
Note thatconsiderable latitude is available when specifyingthe transition types to be included in a feature vec-tor.
These can be all transitions of a given length(e.g., two or three) or the most frequent transitionswithin a document collection.
An example of a fea-ture space with transitions of length two is illustratedin Table 3.The training set consists of ordered pairs of ren-derings (xi j,xik), where xi j and xik are renderings1The frequency threshold is empirically determined on thedevelopment set.
See Section 5 for further discussion.of the same document di, and xi j exhibits a higherdegree of coherence than xik .
Without loss of gen-erality, we assume j > k. The goal of the trainingprocedure is to find a parameter vector ~w that yieldsa ?ranking score?
function ~w ?
?
(xi j), which mini-mizes the number of violations of pairwise rankingsprovided in the training set.
Thus, the ideal ~w wouldsatisfy the condition ~w ?(?
(xi j)??
(xik)) > 0 ?
j, i,ksuch that j > k. The problem is typically treated asa Support Vector Machine constraint optimizationproblem, and can be solved using the search tech-nique described in Joachims (2002a).
This approachhas been shown to be highly effective in varioustasks ranging from collaborative filtering (Joachims,2002a) to parsing (Toutanova et al, 2004).In our ranking experiments, we use Joachims?
(2002a) SVMlight package for training and testingwith all parameters set to their default values.4 Evaluation Set-UpIn this section we describe two evaluation tasks thatassess the merits of the coherence modeling frame-work introduced above.
We also give details regard-ing our data collection, and parameter estimation.Finally, we introduce the baseline method used forcomparison with our approach.4.1 Text OrderingText structuring algorithms (Lapata, 2003; Barzi-lay and Lee, 2004; Karamanis et al, 2004)are commonly evaluated by their performance atinformation-ordering.
The task concerns determin-ing a sequence in which to present a pre-selected setof information-bearing items; this is an essential stepin concept-to-text generation, multi-document sum-marization, and other text-synthesis problems.
Sincelocal coherence is a key property of any well-formedtext, our model can be used to rank alternative sen-tence orderings.
We do not assume that local coher-ence is sufficient to uniquely determine the best or-dering ?
other constraints clearly play a role here.However, we expect that the accuracy of a coherencemodel is reflected in its performance in the orderingtask.Data To acquire a large collection for trainingand testing, we create synthetic data, wherein thecandidate set consists of a source document and per-mutations of its sentences.
This framework for dataacquisition is widely used in evaluation of orderingalgorithms as it enables large scale automatic evalu-144ation.
The underlying assumption is that the orig-inal sentence order in the source document mustbe coherent, and so we should prefer models thatrank it higher than other permutations.
Since we donot know the relative quality of different permuta-tions, our corpus includes only pairwise rankingsthat comprise the original document and one of itspermutations.
Given k original documents, each withn randomly generated permutations, we obtain k ?
n(trivially) annotated pairwise rankings for trainingand testing.Using the technique described above, we col-lected data in two different genres: newspaper ar-ticles and accident reports written by governmentofficials.
The first collection consists of AssociatedPress articles from the North American News Cor-pus on the topic of natural disasters.
The second in-cludes narratives from the National TransportationSafety Board?s database2 .
Both sets have documentsof comparable length ?
the average number of sen-tences is 10.4 and 11.5, respectively.
For each set, weused 100 source articles with 20 randomly generatedpermutations for training.
The same number of pair-wise rankings (i.e., 2000) was used for testing.
Weheld out 10 documents (i.e., 200 pairwise rankings)from the training data for development purposes.4.2 Summary EvaluationWe further test the ability of our method to assesscoherence by comparing model induced rankingsagainst rankings elicited by human judges.
Admit-tedly, the information ordering task only partiallyapproximates degrees of coherence violation usingdifferent sentence permutations of a source docu-ment.
A stricter evaluation exercise concerns the as-sessment of texts with naturally occurring coherenceviolations as perceived by human readers.
A rep-resentative example of such texts are automaticallygenerated summaries which often contain sentencestaken out of context and thus display problems withrespect to local coherence (e.g., dangling anaphors,thematically unrelated sentences).
A model that ex-hibits high agreement with human judges not onlyaccurately captures the coherence properties of thesummaries in question, but ultimately holds promisefor the automatic evaluation of machine-generatedtexts.
Existing automatic evaluation measures suchas BLEU (Papineni et al, 2002) and ROUGE (Lin2The collections are available from http://www.csail.mit.edu/regina/coherence/.and Hovy, 2003), are not designed for the coherenceassessment task, since they focus on content similar-ity between system output and reference texts.Data Our evaluation was based on materi-als from the Document Understanding Conference(DUC, 2003), which include multi-document sum-maries produced by human writers and by automaticsummarization systems.
In order to learn a rank-ing, we require a set of summaries, each of whichhave been rated in terms of coherence.
We thereforeelicited judgments from human subjects.3 We ran-domly selected 16 input document clusters and fivesystems that had produced summaries for these sets,along with summaries composed by several humans.To ensure that we do not tune a model to a particu-lar system, we used the output summaries of distinctsystems for training and testing.
Our set of train-ing materials contained 4 ?
16 summaries (averagelength 4.8), yielding (42)?16 = 96 pairwise rankings.In a similar fashion, we obtained 32 pairwise rank-ings for the test set.
Six documents from the trainingdata were used as a development set.Coherence ratings were obtained during an elic-itation study by 177 unpaid volunteers, all nativespeakers of English.
The study was conducted re-motely over the Internet.
Participants first saw a setof instructions that explained the task, and definedthe notion of coherence using multiple examples.The summaries were randomized in lists following aLatin square design ensuring that no two summariesin a given list were generated from the same docu-ment cluster.
Participants were asked to use a sevenpoint scale to rate how coherent the summaries werewithout having seen the source texts.
The ratings(approximately 23 per summary) given by our sub-jects were averaged to provide a rating between 1and 7 for each summary.The reliability of the collected judgments is cru-cial for our analysis; we therefore performed sev-eral tests to validate the quality of the annota-tions.
First, we measured how well humans agreein their coherence assessment.
We employed leave-one-out resampling4 (Weiss and Kulikowski, 1991),by correlating the data obtained from each par-ticipant with the mean coherence ratings obtainedfrom all other participants.
The inter-subject agree-3The ratings are available from http://homepages.inf.ed.ac.uk/mlap/coherence/.4We cannot apply the commonly used Kappa statistic formeasuring agreement since it is appropriate for nominal scales,whereas our summaries are rated on an ordinal scale.145ment was r = .768.
Second, we examined the ef-fect of different types of summaries (human- vs.machine-generated.)
An ANOVA revealed a reliableeffect of summary type: F(1;15) = 20.38, p < 0.01indicating that human summaries are perceived assignificantly more coherent than system-generatedones.
Finally, the judgments of our participants ex-hibit a significant correlation with DUC evaluations(r = .41, p < 0.01).4.3 Parameter EstimationOur model has two free parameters: the frequencythreshold used to identify salient entities and thelength of the transition sequence.
These parameterswere tuned separately for each data set on the corre-sponding held-out development set.
For our orderingand summarization experiments, optimal salience-based models were obtained for entities with fre-quency ?
2.
The optimal transition length was ?
3for ordering and ?
2 for summarization.4.4 BaselineWe compare our algorithm against the coherencemodel proposed by Foltz et al (1998) which mea-sures coherence as a function of semantic related-ness between adjacent sentences.
Semantic related-ness is computed automatically using Latent Se-mantic Analysis (LSA, Landauer and Dumais 1997)from raw text without employing syntactic or otherannotations.
This model is a good point of compari-son for several reasons: (a) it is fully automatic, (b) itis a not a straw-man baseline; it correlates reliablywith human judgments and has been used to analyzediscourse structure, and (c) it models an aspect ofcoherence which is orthogonal to ours (their modelis lexicalized).Following Foltz et al (1998) we constructedvector-based representations for individual wordsfrom a lemmatized version of the North AmericanNews Text Corpus5 (350 million words) using aterm-document matrix.
We used singular value de-composition to reduce the semantic space to 100 di-mensions obtaining thus a space similar to LSA.
Werepresented the meaning of a sentence as a vectorby taking the mean of the vectors of its words.
Thesimilarity between two sentences was determined bymeasuring the cosine of their means.
An overall textcoherence measure was obtained by averaging thecosines for all pairs of adjacent sentences.5Our selection of this corpus was motivated by its similarityto the DUC corpus which primarily consists of news stories.In sum, each text was represented by a singlefeature, its sentence-to-sentence semantic similar-ity.
During training, the ranker learns an appropriatethreshold value for this feature.4.5 Evaluation MetricModel performance was assessed in the same wayfor information ordering and summary evaluation.Given a set of pairwise rankings, we measure accu-racy as the ratio of correct predictions made by themodel over the size of the test set.
In this setup, ran-dom prediction results in an accuracy of 50%.5 ResultsThe evaluation of our coherence model was drivenby two questions: (1) How does the proposed modelcompare to existing methods for coherence assess-ment that make use of distinct representations?
(2) What is the contribution of linguistic knowledgeto the model?s performance?
Table 4 summarizes theaccuracy of various configurations of our model forthe ordering and coherence assessment tasks.We first compared a linguistically rich grid modelthat incorporates coreference resolution, expressivesyntactic information, and a salience-based featurespace (Coreference+Syntax+Salience) against theLSA baseline (LSA).
As can be seen in Table 4, thegrid model outperforms the baseline in both orderingand summary evaluation tasks, by a wide margin.We conjecture that this difference in performancestems from the ability of our model to discriminatebetween various patterns of local sentence transi-tions.
In contrast, the baseline model only measuresthe degree of overlap across successive sentences,without taking into account the properties of the en-tities that contribute to the overlap.
Not surprisingly,the difference between the two methods is more pro-nounced for the second task ?
summary evaluation.Manual inspection of our summary corpus revealedthat low-quality summaries often contain repetitiveinformation.
In such cases, simply knowing abouthigh cross-sentential overlap is not sufficient to dis-tinguish a repetitive summary from a well-formedone.In order to investigate the contribution of linguis-tic knowledge on model performance we comparedthe full model introduced above against models us-ing more impoverished representations.
We focusedon three sources of linguistic knowledge ?
syntax,coreference resolution, and salience ?
which play146Model Ordering (Set1) Ordering (Set2) SummarizationCoreference+Syntax+Salience 87.3 90.4 68.8Coreference+Salience 86.9 88.3 62.5Syntax+Salience 83.4 89.7 81.3Coreference+Syntax 76.5 88.8 75.0LSA 72.1 72.1 25.0Table 4: Ranking accuracy measured as the fraction of correct pairwise rankings in the test set.a prominent role in Centering analyses of discoursecoherence.
An additional motivation for our study isexploration of the trade-off between robustness andrichness of linguistic annotations.
NLP tools are typ-ically trained on human-authored texts, and may de-teriorate in performance when applied to automati-cally generated texts with coherence violations.Syntax To evaluate the effect of syntacticknowledge, we eliminated the identification ofgrammatical relations from our grid computationand recorded solely whether an entity is present orabsent in a sentence.
This leaves only the coref-erence and salience information in the model, andthe results are shown in Table 4 under (Corefer-ence+Salience).
The omission of syntactic informa-tion causes a uniform drop in performance on bothtasks, which confirms its importance for coherenceanalysis.Coreference To measure the effect of fully-fledged coreference resolution, we constructed en-tity classes simply by clustering nouns on the ba-sis of their identity.
In other words, each noun in atext corresponds to a different entity in a grid, andtwo nouns are considered coreferent only if theyare identical.
The performance of the model (Syn-tax+Salience) is shown in the third row of Table 4.While coreference resolution improved modelperformance in ordering, it caused a decrease in ac-curacy in summary evaluation.
This drop in per-formance can be attributed to two factors relatedto the nature of our corpus ?
machine-generatedtexts.
First, an automatic coreference resolution toolexpectedly decreases in accuracy because it wastrained on well-formed human-authored texts.
Sec-ond, automatic summarization systems do not useanaphoric expressions as often as humans do.
There-fore, a simple entity clustering method is more suit-able for automatic summaries.Salience Finally, we evaluate the contributionof salience information by comparing our orig-inal model (Coreference+Syntax+Salience) whichaccounts separately for patterns of salient andnon-salient entities against a model that does notattempt to discriminate between them (Corefer-ence+Syntax).
Our results on the ordering task indi-cate that models that take salience information intoaccount consistently outperform models that do not.The effect of salience is less pronounced for thesummarization task when it is combined with coref-erence information (Coreference + Salience).
This isexpected, since accurate identification of coreferringentities is prerequisite to deriving accurate saliencemodels.
However, as explained above, our automaticcoreference tool introduces substantial noise in ourrepresentation.
Once this noise is removed (see Syn-tax+Salience), the salience model has a clear advan-tage over the other models.6 Discussion and ConclusionsIn this paper we proposed a novel framework forrepresenting and measuring text coherence.
Centralto this framework is the entity grid representationof discourse which we argue captures important pat-terns of sentence transitions.
We re-conceptualizecoherence assessment as a ranking task and showthat our entity-based representation is well suited forlearning an appropriate ranking function; we achievegood performance on text ordering and summary co-herence evaluation.On the linguistic side, our results yield empiricalsupport to some of Centering Theory?s main claims.We show that coherent texts are characterized bytransitions with particular properties which do nothold for all discourses.
Our work, however, not onlyvalidates these findings, but also quantitatively mea-sures the predictive power of various linguistic fea-tures for the task of coherence assessment.An important future direction lies in augmentingour entity-based model with lexico-semantic knowl-edge.
One way to achieve this goal is to cluster enti-ties based on their semantic relatedness, thereby cre-147ating a grid representation over lexical chains (Mor-ris and Hirst, 1991).
An entirely different approachis to develop fully lexicalized models, akin to tra-ditional language models.
Cache language mod-els (Kuhn and Mori, 1990) seem particularly promis-ing in this context.In the discourse literature, entity-based theoriesare primarily applied at the level of local coherence,while relational models, such as Rhetorical StructureTheory (Mann and Thomson, 1988; Marcu, 2000),are used to model the global structure of discourse.We plan to investigate how to combine the two forimproved prediction on both local and global levels,with the ultimate goal of handling longer texts.AcknowledgmentsThe authors acknowledge the support of the National ScienceFoundation (Barzilay; CAREER grant IIS-0448168 and grantIIS-0415865) and EPSRC (Lapata; grant GR/T04540/01).We are grateful to Claire Cardie and Vincent Ng for providingus the results of their system on our data.
Thanks to Eli Barzilay,Eugene Charniak, Michael Elhadad, Noemie Elhadad, FrankKeller, Alex Lascarides, Igor Malioutov, Smaranda Muresan,Martin Rinard, Kevin Simler, Caroline Sporleder, Chao Wang,Bonnie Webber and three anonymous reviewers for helpfulcomments and suggestions.
Any opinions, findings, and con-clusions or recommendations expressed above are those of theauthors and do not necessarily reflect the views of the NationalScience Foundation or EPSRC.ReferencesN.
Asher, A. Lascarides.
2003.
Logics of Conversation.Cambridge University Press.R.
Barzilay, L. Lee.
2004.
Catching the drift: Probabilis-tic content models, with applications to generation andsummarization.
In Proceedings of HLT-NAACL, 113?120.M.
Collins.
1997.
Three generative, lexicalised mod-els for statistical parsing.
In Proceedings of theACL/EACL, 16?23.P.
W. Foltz, W. Kintsch, T. K. Landauer.
1998.
Textualcoherence using latent semantic analysis.
DiscourseProcesses, 25(2&3):285?307.B.
Grosz, A. K. Joshi, S. Weinstein.
1995.
Centering:A framework for modeling the local coherence of dis-course.
Computational Linguistics, 21(2):203?225.L.
Hasler.
2004.
An investigation into the use of cen-tering transitions for summarisation.
In Proceedingsof the 7th Annual CLUK Research Colloquium, 100?107, University of Birmingham.T.
Joachims.
2002a.
Optimizing search engines usingclickthrough data.
In Proceesings of KDD, 133?142.N.
Karamanis, M. Poesio, C. Mellish, J. Oberlander.2004.
Evaluating centering-based metrics of coher-ence for text structuring using a reliably annotated cor-pus.
In Proceedings of the ACL, 391?398.R.
Kibble, R. Power.
2004.
Optimising referential co-herence in text generation.
Computational Linguistics,30(4):401?416.R.
Kuhn, R. D. Mori.
1990.
A cache-based natural lan-guage model for speech recognition.
IEEE Transac-tions on PAMI, 12(6):570?583.T.
K. Landauer, S. T. Dumais.
1997.
A solution to Plato?sproblem: The latent semantic analysis theory of ac-quisition, induction and representation of knowledge.Psychological Review, 104(2):211?240.M.
Lapata.
2003.
Probabilistic text structuring: Exper-iments with sentence ordering.
In Proceedings of theACL, 545?552.C.-Y.
Lin, E. H. Hovy.
2003.
Automatic evaluation ofsummaries using n-gram co-occurrence statistics.
InProceedings of HLT-NAACL, 71?78.W.
C. Mann, S. A. Thomson.
1988.
Rhetorical structuretheory.
Text, 8(3):243?281.D.
Marcu.
2000.
The Theory and Practice of DiscourseParsing and Summarization.
MIT Press.E.
Miltsakaki, K. Kukich.
2000.
The role of centeringtheory?s rough-shift in the teaching and evaluation ofwriting skills.
In Proceedings of the ACL, 408?415.J.
Morris, G. Hirst.
1991.
Lexical cohesion computed bythesaural relations as an indicator of the structure oftext.
Computational Linguistics, 1(17):21?43.V.
Ng, C. Cardie.
2002.
Improving machine learningapproaches to coreference resolution.
In Proceedingsof the ACL, 104?111.K.
Papineni, S. Roukos, T. Ward, W.-J.
Zhu.
2002.
Bleu:a method for automatic evaluation of machine transla-tion.
In Proceedings of the ACL, 311?318.M.
Poesio, R. Stevenson, B. D. Eugenio, J. Hitzeman.2004.
Centering: a parametric theory and its instan-tiations.
Computational Linguistics, 30(3):309?363.E.
Reiter, R. Dale.
2000.
Building Natural-LanguageGeneration Systems.
Cambridge University Press.D.
Scott, C. S. de Souza.
1990.
Getting the messageacross in RST-based text generation.
In R. Dale,C.
Mellish, M. Zock, eds., Current Research in Nat-ural Language Generation, 47?73.
Academic Press.M.
Strube, U. Hahn.
1999.
Functional centering ?grounding referential coherence in information struc-ture.
Computational Linguistics, 25(3):309?344.K.
Toutanova, P. Markova, C. D. Manning.
2004.
Theleaf projection path view of parse trees: Exploringstring kernels for HPSG parse selection.
In Proceed-ings of the EMNLP, 166?173.M.
Walker, A. Joshi, E. Prince, eds.
1998.
CenteringTheory in Discourse.
Clarendon Press.S.
M. Weiss, C. A. Kulikowski.
1991.
Computer Sys-tems that Learn: Classification and Prediction Meth-ods from, Statistics, Neural Nets, Machine Learning,and Expert Systems.
Morgan Kaufmann.148
