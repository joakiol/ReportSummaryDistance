Proceedings of the Fifteenth Conference on Computational Natural Language Learning, pages 125?134,Portland, Oregon, USA, 23?24 June 2011. c?2011 Association for Computational LinguisticsLanguage Models as Representations for Weakly-Supervised NLP TasksFei Huang and Alexander YatesTemple UniversityBroad St. and Montgomery Ave.Philadelphia, PA 19122fei.huang@temple.eduyates@temple.eduArun Ahuja and Doug DowneyNorthwestern University2133 Sheridan RoadEvanston, IL 60208a-ahuja@northwestern.eduddowney@eecs.northwestern.eduAbstractFinding the right representation for words iscritical for building accurate NLP systemswhen domain-specific labeled data for thetask is scarce.
This paper investigates lan-guage model representations, in which lan-guage models trained on unlabeled corporaare used to generate real-valued feature vec-tors for words.
We investigate ngram mod-els and probabilistic graphical models, includ-ing a novel lattice-structured Markov RandomField.
Experiments indicate that languagemodel representations outperform traditionalrepresentations, and that graphical model rep-resentations outperform ngram models, espe-cially on sparse and polysemous words.1 IntroductionNLP systems often rely on hand-crafted, carefullyengineered sets of features to achieve strong perfor-mance.
Thus, a part-of-speech (POS) tagger wouldtraditionally use a feature like, ?the previous tokenis the?
to help classify a given token as a noun oradjective.
For supervised NLP tasks with sufficientdomain-specific training data, these traditional fea-tures yield state-of-the-art results.
However, NLPsystems are increasingly being applied to texts likethe Web, scientific domains, and personal commu-nications like emails, all of which have very differ-ent characteristics from traditional training corpora.Collecting labeled training data for each new targetdomain is typically prohibitively expensive.
We in-vestigate representations that can be applied whendomain-specific labeled training data is scarce.An increasing body of theoretical and empiricalevidence suggests that traditional, manually-craftedfeatures limit systems?
performance in this settingfor two reasons.
First, feature sparsity prevents sys-tems from generalizing accurately to words and fea-tures not seen during training.
Because word fre-quencies are Zipf distributed, this often means thatthere is little relevant training data for a substantialfraction of parameters (Bikel, 2004), especially innew domains (Huang and Yates, 2009).
For exam-ple, word-type features form the backbone of mostPOS-tagging systems, but types like ?gene?
and?pathway?
show up frequently in biomedical liter-ature, and rarely in newswire text.
Thus, a classifiertrained on newswire data and tested on biomedicaldata will have seen few training examples related tosentences with features ?gene?
and ?pathway?
(Ben-David et al, 2009; Blitzer et al, 2006).Further, because words are polysemous, word-type features prevent systems from generalizing tosituations in which words have different meanings.For instance, the word type ?signaling?
appears pri-marily as a present participle (VBG) in Wall StreetJournal (WSJ) text, as in, ?Interest rates rose, sig-naling that .
.
.
?
(Marcus et al, 1993).
In biomedicaltext, however, ?signaling?
appears primarily in thephrase ?signaling pathway,?
where it is considereda noun (NN) (PennBioIE, 2005); this phrase neverappears in the WSJ portion of the Penn Treebank(Huang and Yates, 2010a).Our response to these problems with traditionalNLP representations is to seek new representationsthat allow systems to generalize more accurately topreviously unseen examples.
Our approach dependson the well-known distributional hypothesis, whichstates that a word?s meaning is identified with thecontexts in which it appears (Harris, 1954; Hin-dle, 1990).
Our goal is to develop probabilistic lan-125guage models that describe the contexts of individ-ual words accurately.
We then construct represen-tations, or mappings from word tokens and typesto real-valued vectors, from these language models.Since the language models are designed to modelwords?
contexts, the features they produce can beused to combat problems with polysemy.
And bycareful design of the language models, we can limitthe number of features that they produce, controllinghow sparse those features are in training data.In this paper, we analyze the performanceof language-model-based representations on taskswhere domain-specific training data is scarce.
Ourcontributions are as follows:1.
We introduce a novel factorial graphical modelrepresentation, a Partial-Lattice Markov RandomField (PL-MRF), which is a tractable variation ofa Factorial Hidden Markov Model (HMM) for lan-guage modeling.2.
In experiments on POS tagging in a domain adap-tation setting and on weakly-supervised informa-tion extraction (IE), we quantify the performance ofrepresentations derived from language models.
Weshow that graphical models outperform ngram rep-resentations.
The PL-MRF representation achieves astate-of-the-art 93.8% accuracy on the POS taggingtask, while the HMM representation improves overthe ngram model by 10% on the IE task.3.
We analyze how the performance of the differentrepresentations varies due to the fundamental chal-lenges of sparsity and polysemy.The next section discusses previous work.
Sec-tions 3 and 4 present the existing representations weinvestigate and the new PL-MRF, respectively.
Sec-tions 5 and 6 describe our two tasks and the resultsof using our representations on each of them.
Sec-tion 7 concludes.2 Previous WorkThere is a long tradition of NLP research on rep-resentations, mostly falling into one of four cate-gories: 1) vector space models of meaning basedon document-level lexical cooccurrence statistics(Salton and McGill, 1983; Turney and Pantel, 2010;Sahlgren, 2006); 2) dimensionality reduction tech-niques for vector space models (Deerwester et al,1990; Honkela, 1997; Kaski, 1998; Sahlgren, 2005;Blei et al, 2003; Va?yrynen et al, 2007); 3) usingclusters that are induced from distributional similar-ity (Brown et al, 1992; Pereira et al, 1993; Mar-tin et al, 1998) as non-sparse features (Lin and Wu,2009; Candito and Crabbe, 2009; Koo et al, 2008;Zhao et al, 2009); 4) and recently, language models(Bengio, 2008; Mnih and Hinton, 2009) as represen-tations (Weston et al, 2008; Collobert and Weston,2008; Bengio et al, 2009), some of which have al-ready yielded state of the art performance on domainadaptation tasks (Huang and Yates, 2009; Huang andYates, 2010a; Huang and Yates, 2010b; Turian et al,2010) and IE (Ahuja and Downey, 2010; Downey etal., 2007b).
In contrast to this previous work, we de-velop a novel Partial Lattice MRF language modelthat incorporates a factorial representation of latentstates, and demonstrate that it outperforms the pre-vious state-of-the-art in POS tagging in a domainadaptation setting.
We also analyze the novel PL-MRF representation on an IE task, and several repre-sentations along the key dimensions of sparsity andpolysemy.Most previous work on domain adaptation has fo-cused on the case where some labeled data is avail-able in both the source and target domains (Daume?III, 2007; Jiang and Zhai, 2007; Daume?
III andMarcu, 2006; Finkel and Manning, 2009; Dredzeet al, 2010; Dredze and Crammer, 2008).
Learn-ing bounds are known (Blitzer et al, 2007; Man-sour et al, 2009).
Daume?
III et al (2010) use semi-supervised learning to incorporate labeled and unla-beled data from the target domain.
In contrast, weinvestigate a domain adaptation setting where no la-beled data is available for the target domain.3 RepresentationsA representation is a set of features that describeinstances for a classifier.
Formally, let X be aninstance set, and let Z be the set of labels for aclassification task.
A representation is a functionR : X ?
Y for some suitable feature space Y (suchas Rd).
We refer to dimensions of Y as features, andfor an instance x ?
X we refer to values for partic-ular dimensions of R(x) as features of x.3.1 Traditional POS-Tagging RepresentationsAs a baseline for POS tagging experiments and anexample of our terminology, we describe a repre-sentation used in traditional supervised POS taggers.The instance set X is the set of English sentences,and Z is the set of POS tag sequences.
A traditionalrepresentation TRAD-R maps a sentence x ?
X to asequence of boolean-valued vectors, one vector per126Representation FeatureTRAD-R ?w1[xi = w]?s?Suffixes1[xi ends with s]1[xi contains a digit]NGRAM-R ?w?,w?
?P (w?ww??
)/P (w)HMM-TOKEN-R ?k1[yi?
= k]HMM-TYPE-R ?kP (y = k|x = w)I-HMM-TOKEN-R ?j,k1[yi,j?
= k]BROWN-TOKEN-R ?j?{?2,?1,0,1,2}?p?
{4,6,10,20} prefix(yi+j , p)BROWN-TYPE-R ?p prefix(y, p)LATTICE-TOKEN-R ?j,k1[yi,j?
= k]LATTICE-TYPE-R ?kP (y = k|x = w)Table 1: Summary of features provided by our repre-sentations.
?a1[g(a)] represents a set of boolean fea-tures, one for each value of a, where the feature istrue iff g(a) is true.
xi represents a token at positioni in sentence x, w represents a word type, Suffixes ={-ing,-ogy,-ed,-s,-ly,-ion,-tion,-ity}, k (and k) representsa value for a latent state (set of latent states) in a latent-variable model, y?
represents the optimal setting of latentstates y for x, yi is the latent variable for xi, and yi,j isthe latent variable for xi at layer j. prefix(y,p) is the p-length prefix of the Brown cluster y.word xi in the sentence.
Dimensions for each latentvector include indicators for the word type of xi andvarious orthographic features.
Table 1 presents thefull list of features in TRAD-R.
Since our IE taskclassifies word types rather than tokens, this base-line is not appropriate for that task.
Below, we de-scribe how we can learn representations R by usinga variety of language models, for use in both our IEand POS tagging tasks.
All representations for POStagging inherit the features from TRAD-R; all repre-sentations for IE do not.3.2 Ngram RepresentationsN-gram representations model a word type w interms of the n-gram contexts in which w appearsin a corpus.
Specifically, for word w we generatethe vector P (w?ww??
)/P (w), the conditional prob-ability of observing the word sequence w?
to the leftand w??
to the right of w. The experimental sectiondescribes the particular corpora and language mod-eling methods used for estimating probabilities.3.3 HMM-based RepresentationsIn previous work, we have implemented severalrepresentations based on HMMs (Rabiner, 1989),which we used for both POS tagging (Huang andYates, 2009) and IE (Downey et al, 2007b).
AnHMM is a generative probabilistic model that gen-erates each word xi in the corpus conditioned on alatent variable yi.
Each yi in the model takes on in-tegral values from 1 to K, and each one is generatedby the latent variable for the preceding word, yi?1.The joint distribution for a corpus x = (x1, .
.
.
, xN )and a set of state vectors y = (y1, .
.
.
, yN ) isgiven by: P (x,y) = ?i P (xi|yi)P (yi|yi?1).
Us-ing Expectation-Maximization (EM) (Dempster etal., 1977), it is possible to estimate the distributionsfor P (xi|yi) and P (yi|yi?1) from unlabeled data.We construct two different representations fromHMMs, one for POS tagging and one for IE.
ForPOS tagging, we use the Viterbi algorithm to pro-duce the optimal setting y?
of the latent states for agiven sentence x, or y?
= arg maxy P (x,y).
Weuse the value of yi?
as a new feature for xi that repre-sents a cluster of distributionally-similar words.
ForIE, we require features for word types w, rather thantokens xi.
We use the K-dimensional vector thatrepresents the distribution P (y|x = w) as the fea-ture vector for word type w. This set of featuresrepresents a ?soft clustering?
of w into K differentclusters.
We refer to these representations as HMM-TOKEN-R and HMM-TYPE-R, respectively.Because HMM-based representations offer asmall number of discrete states as features, they havea much greater potential to combat feature sparsitythan do ngram models.
Furthermore, for token-based representations, these models can potentiallyhandle polysemy better than ngram language modelsby providing different features in different contexts.We also compare against a variation of the HMMfrom our previous work (Huang and Yates, 2010a),henceforth HY10.
This model independently trainsM separate HMM models on the same corpus, ini-tializing each one randomly.
We can then use theViterbi-optimal decoded latent state of each inde-pendent HMM model as a separate feature for a to-ken.
We refer to this language model as an I-HMM,and the representation as I-HMM-TOKEN-R.Finally, we compare against Brown clusters(Brown et al, 1992) as learned features.
Althoughnot traditionally described as such, Brown cluster-ing involves constructing an HMM model in which127each type is restricted to having exactly one latentstate that may generate it.
Brown et al describe agreedy agglomerative clustering algorithm for train-ing this model on unlabeled text.
Following Turianet al (2010), we use Percy Liang?s implementationof this algorithm for our comparison, and we testruns with 100, 320, and 1000 clusters.
We use fea-tures from these clusters identical to Turian et al?s.1Turian et al have shown that Brown clusters matchor exceed the performance of neural network-basedlanguage models in domain adaptation experimentsfor named-entity recognition, as well as in-domainexperiments for NER and chunking.4 A Novel Lattice Language ModelRepresentationOur final language model is a novel latent-variablelanguage model with rich latent structure, shown inFigure 1.
The model contains a lattice of M ?N la-tent states, where N is the number of words in a sen-tence and M is the number of layers in the model.We can justify the choice of this model from a lin-guistic perspective as a way to capture the multi-dimensional nature of words.
Linguists have longargued that words have many different features in ahigh dimensional space: they can be separately de-scribed by part of speech, gender, number, case, per-son, tense, voice, aspect, mass vs. count, and a hostof semantic categories (agency, animate vs. inani-mate, physical vs. abstract, etc.
), to name a few (Saget al, 2003).
Our model seeks to capture a multi-dimensional representation of words by creating aseparate layer of latent variables for each dimension.The values of the M layers of latent variables for asingle word can be used as M distinct features inour representation.
The I-HMM attempts to modelthe same intuition, but unlike a lattice model the I-HMM layers are entirely independent, and as a re-sult there is no mechanism to enforce that the layersmodel different dimensions.
Duh (2005) previouslyused a 2-layer lattice for tagging and chunking, butin a supervised setting rather than for representationlearning.Let Cliq(x,y) represent the set of all maximalcliques in the graph of the MRF model for x and y.1Percy Liang?s implementation is available athttp://metaoptimize.com/projects/wordreprs/.
Turian et alalso tested a run with 3200 clusters in their experiments, whichwe have been training for months, but which has not finished intime for publication.y4,1y3,1yy4,2y3,2yy4,3y3,3yy4,4y3,4yy4,5y3,5yx12,1y1,1x22,2y1,2x32,3y1,3x42,4y1,4x52,5y1,5Figure 1: The Partial Lattice MRF (PL-MRF) Model for a5-word sentence and a 4-layer lattice.
Dashed gray edgesare part of a full lattice, but not the PL-MRF.Expressing the lattice model in log-linear form, wecan write the marginal probability P (x) of a givensentence x as:?y?c?Cliq(x,y) score(c,x,y)?x?,y??c?Cliq(x?,y?)
score(c,x?,y?
)where score(c,x,y) = exp(?c ?
fc(xc,yc)).
Ourmodel includes parameters for transitions betweentwo adjacent latent variables on layer j: ?transi,s,i+1,s?,jfor yi,j = s and yi+1,j = s?.
It also includes obser-vation parameters for latent variables and tokens, aswell as for pairs of adjacent latent variables in differ-ent layers and their tokens: ?obsi,j,s,w and ?obsi,j,s,j+1,s?,wfor yi,j = s, yi,j+1 = s?, and xi = w.Computationally, the lattice MRF is preferable toa na?
?ve Factorial HMM (Ghahramani and Jordan,1997) representation, which would require O(2M )parameters for an M -layer model.
However, ex-act training and inference in supervised settings arestill intractable for this model (Sutton et al, 2007),and thus it has not yet been explored as a languagemodel, which requires even more difficult, unsuper-vised training.
Training is intractable in part becauseof the difficulty in enumerating and summing overthe exponentially-many configurations y for a givenx.
We address this difficulty in two ways: by modi-fying the model, and by modifying the training pro-cedure.4.1 Partial Lattice MRFInstead of the full lattice model, we construct aPartial Lattice MRF (PL-MRF) model by deleting128certain edges between latent layers of the model(dashed gray edges in Figure 1).
Let c = bN2 c,where N is the length of the sentence.
If i < cand j is odd, or if j is even and i > c, we deleteedges between yi,j and yi,j+1.
The same lattice ofnodes remains, but fewer edges and paths.
A cen-tral ?trunk?
at i = c connects all layers of the lat-tice, and branches from this trunk connect either tothe branches in the layer above or the layer below(but not both).
The result is a model that retainsmost2 of the edges of the full model.
Additionally,the pruned model makes the branches conditionallyindependent from one another, except through thetrunk.
For instance, the right branch at layers 1and 2 in Figure 1 (y1,4, y1,5, y2,4, and y2,5) are dis-connected from the right branch at layers 3 and 4(y3,4, y3,5, y4,4, and y4,5), except through the trunkand the observed nodes.
As a result, excluding theobserved nodes, this model has a low tree-width of2 (excluding observed nodes), and a variety of ef-ficient dynamic programming and message-passingalgorithms for training and inference can be readilyapplied (Bodlaender, 1988).3 Our inference algo-rithm passes information from the branches inwardsto the trunk, and then upward along the trunk, intime O(K4MN).As with our HMM models, we create two repre-sentations from PL-MRFs, one for tokens and onefor types.
For tokens, we decode the model to com-pute y?, the matrix of optimal latent state values forsentence x.
For each layer j and and each possi-ble latent state value k, we add a boolean featurefor token xi that is true iff y?i,j = k. For types,we compute distributions over the latent state space.Let y be the column vector of latent variables forword x.
For each possible configuration of values kof the latent variables y, we add a real-valued fea-tures for x given by P (y = k|x = w).
We referto these two representations as LATTICE-TOKEN-Rand LATTICE-TYPE-R, respectively.4.2 Parameter EstimationWe train the PL-MRF using contrastive estimation,which iteratively optimizes the following objectivefunction on a corpus X:?x?Xlog?y?c?Cliq(x,y) score(c,x,y)?x?
?N (x),y??c?Cliq(x?,y?)
score(c,x?,y?
)2As M, N ?
?, 5 out of every 6 edges are kept.3c.f.
a tree-width of min(M ,N ) for the unpruned modelwhere N (x), the neighborhood of x, indicates aset of perturbed variations of the original sentencex.
Contrastive estimation seeks to move probabilitymass away from the perturbed neighborhood sen-tences and onto the original sentence.
We use aneighborhood function that includes all sentenceswhich can be obtained from the original sentence byswapping the order of a consecutive pair of words.Training uses gradient descent over this non-convexobjective function with a standard software package(Liu and Nocedal, 1989) and converges to a localmaximum (Smith and Eisner, 2005).For tractability, we modify the training procedureto train the PL-MRF one layer at a time.
Let ?i rep-resent the set of parameters relating to features oflayer i, and let ?
?i represent all other parameters.We fix ?
?0 = 0, and optimize ?0 using contrastiveestimation.
After convergence, we fix ?
?1, and opti-mize ?1, and so on.
We use a convergence thresholdof 10?6, and each layer typically converges in under100 iterations.5 Domain Adaptation for a POS TaggerWe evaluate the representations described above ona POS tagging task in a domain adaptation setting.5.1 Experimental SetupWe use the same experimental setup as in HY10:the Penn Treebank (Marcus et al, 1993) Wall StreetJournal portion for our labeled training data; 561MEDLINE sentences (9576 types, 14554 tokens,23% OOV tokens) from the Penn BioIE project(PennBioIE, 2005) for our labeled test set; and all ofthe unlabeled text from the Penn Treebank WSJ por-tion plus a MEDLINE corpus of 71,306 unlabeledsentences to train our language models.
The twotexts come from two very different domains, mak-ing this data a tough test for domain adaptation.We use an open source Conditional Random Field(CRF) (Lafferty et al, 2001) software package4 de-signed by Sunita Sarawagi and William W. Cohento implement our supervised models.
Let X be atraining corpus, Z the corresponding labels, and Ra representation function.
For each token xi in X,we include a parameter in our CRF model for allfeatures R(xi) and all possible labels in Z. Further-more, we include transition parameters for pairs ofconsecutive labels zi, zi+1.4Available from http://sourceforge.net/projects/crf/129For representations, we tested TRAD-R,NGRAM-R, HMM-TOKEN-R, I-HMM-TOKEN-R(between 2 and 8 layers), and LATTICE-TOKEN-R(8, 12, 16, and 20 layers).
Following HY10, eachlatent node in the I-HMMs have 80 possible values,creating 808 ?
1015 possible configurations of the8-layer I-HMM for a single word.
Each node inthe PL-MRF is binary, creating a much smallernumber (220 ?
106) of possible configurations foreach word in a 20-layer representation.
NGRAM-Rwas trained using an unsmoothed trigram model onthe Web 1Tgram corpus.
To keep the feature setmanageable, we included the top 500 most commonngrams for each word type, and then used mutualinformation on the training data to select the top10,000 most relevant ngram features for all wordtypes.
We incorporated ngram features as binaryvalues indicating whether xi appeared with thengram or not.
We also report on the performanceof Brown clusters and Blitzer et al?s StructuralCorrespondence Learning (SCL) (2006) technique,which uses manually-selected ?pivot?
words (like?of?, ?the?)
to learn domain-independent features.Finally, we compare against the self-training CRFtechnique from HY10.5.2 Results and DiscussionFor each representation, we measured the accuracyof the POS tagger on the biomedical test text.
Ta-ble 2 shows the results for the best variation of eachkind of model ?
20 layers for the PL-MRF, 7 lay-ers for the I-HMM, and 1000 clusters for the Brownclustering.
All language model representations sig-nificantly outperform the SCL model and the TRAD-R baseline.
The novel PL-MRF model outperformsthe previous state of the art, the I-HMM model, andmuch of the performance increase comes from a11.3% relative reduction in error on words that ap-pear in biomedical texts but not in newswire texts.Both graphical model representations significantlyoutperform the ngram model, which is trained on farmore text.
For comparison, our best model, the PL-MRF, achieved a 96.8% in-domain accuracy on sec-tions 22-24 of the Penn Treebank, about 0.5% shyof a state-of-the-art in-domain system (Shen et al,2007) with more sophisticated supervised learning.We expected that language model representationsperform well in part because they provide meaning-ful features for sparse and polysemous words.
Totest this, we selected 109 polysemous word typesmodel % error OOV % errorTRAD-R 11.7 32.7TRAD-R+self-training 11.5 29.6SCL 11.1 -BROWN-TOKEN-R 10.8 25.4HMM-TOKEN-R 9.5 24.8NGRAM-R 6.9 24.4I-HMM-TOKEN-R 6.7 24LATTICE-TOKEN-R 6.2 21.3SCL+500bio 3.9 -Table 2: PL-MRF representations reduce error by 7.5%relative to the previous state-of-the-art I-HMM, and ap-proach within 2.3% absolute error a SCL+500bio modelwith access to 500 labeled sentences from the target do-main.
1.8% of the tags in the test set are new tags thatdo not occur in the WSJ training data, so an error rate of3.9+1.8 = 5.7% error is a reasonable bound for the bestpossible performance of a model that has seen no exam-ples from the target domain.from our test data, along with 296 non-polysemousword types, chosen based on POS tags and manualinspection.
We further define sparse word types asthose that appear 5 times or fewer in all of our unla-beled data, and non-sparse word types as those thatappear at least 50 times in our unlabeled data.
Table3 shows results on these subsets of the data.As expected, all of our language models outper-form the baseline by a larger margin on polysemouswords than on non-polysemous words.
The mar-gin between graphical model representations and thengram model also increases on polysemous words,presumably because the Viterbi decoding of thesemodels takes into account the tokens in the sur-rounding sentence.
The same behavior is evident forsparsity: all of the language model representationsoutperform the baseline by a larger margin on sparsewords than not-sparse words, and all of the graphicalmodels perform better relative to the ngram modelon sparse words as well.
Thus representations basedon graphical models address two key issues in build-ing representations for POS tagging.6 Information Extraction ExperimentsIn this section, we evaluate our learned representa-tions on a different task that investigates the abil-ity of each representation to capture semantic, ratherthan syntactic, information.
Specifically, we inves-130POS Tagging Information Extractionpolys.
not polys.
sparse not sparse polys.
not polys.
sparse not sparsetokens/types 159 4321 463 12194 222 210 266 166categories - - - - 12 4 13 3TRAD-R 59.5 78.5 52.5 89.6 - - - -Ngram 68.2 85.3 61.8 94.0 0.07 0.17 0.06 0.25HMM 67.9 83.4 60.2 91.6 0.14 0.26 0.15 0.32(-Ngram) (-0.3) (-1.9) (-1.6) (-2.4) (+0.07) (+0.09) (+0.09) (+0.07)I-HMM 75.6 85.2 62.9 94.5 - - - -(-Ngram) (+7.4) (-0.1) (+1.1) (+0.5) - - - -PL-MRF 70.5 86.9 65.2 94.6 0.09 0.15 0.1 0.19(-Ngram) (+2.3) (+1.6) (+3.4) (+0.6) (+0.02) (-0.02) (+0.04) (-0.06)Table 3: Graphical models consistently outperform ngram models by a larger margin on sparse words than not-sparsewords.
On polysemous words, the difference between graphical model performance and ngram performance growsfor POS tagging, where the context surrounding polysemous words is available to the language model, but not forinformation extraction.
For tagging, we show number of tokens and accuracies.
For IE, we show number of types,categories, and AUCs.tigate a set-expansion task in which we?re given acorpus and a few ?seed?
noun phrases from a se-mantic category (e.g.
Superheroes), and our goal isto identify other examples of the category in the cor-pus.
This is a weakly-supervised task because we aregiven only a handful of examples of the category,rather than a large sample of positively and nega-tively labeled training examples.Existing set-expansion techniques utilize the dis-tributional hypothesis: candidate noun phrases for agiven semantic class are ranked based on how sim-ilar their contextual distributions are to those of theseeds.
Here, we measure how performance on theset-expansion task varies when we employ differentrepresentations for the contextual distributions.6.1 MethodsThe set-expansion task we address is formalized asfollows: given a corpus, a set of seeds from somesemantic category C, and a separate set of candidatephrases P , output a ranking of the phrases in P indecreasing order of likelihood of membership in C.For any given representation R, the set-expansionalgorithm we investigate is straightforward: we cre-ate a prototypical ?seed representation vector?
equalto the mean of the representation vectors for eachof the seeds.
Then, we rank candidate phrases inincreasing order of the distance between the candi-date phrase representation and the seed representa-tion vector.
As a measure of distance between rep-resentations, we compute the average of five stan-dard distance measures, including KL and Jensen-Shannon divergence, and cosine, Euclidean, and L1distance.
In experiments, we found that improvingupon this simple averaging was not easy?in fact,tuning a weighted average of the distance measuresfor each representation did not improve results sig-nificantly on held-out data.Because set expansion is performed at the levelof word types rather than tokens, it requires type-based representations.
We compare HMM-TYPE-R, NGRAM-R, LATTICE-TYPE-R, and BROWN-TYPE-R in this experiment.
We used a 25-stateHMM, and the same PL-MRF as in the previoussection.
Following previous set-expansion experi-ments with n-grams (Ahuja and Downey, 2010), weemploy a trigram model with Kneser-Ney smooth-ing for NGRAM-R. For Brown clusters, instead ofdistance metrics like KL divergence (which assumedistributions), we rank extractions by the numberof matches between a word?s BROWN-TYPE-R fea-tures and seed features.6.2 Data SetsWe utilized a set of approximately 100,000 sen-tences of Web text, joining multi-word named enti-ties in the corpus into single tokens using the Lexalgorithm (Downey et al, 2007a).
This processenables each named entity (the focus of the set-expansion experiments) to be treated as a single to-ken, with a single representation vector for compar-ison.
We developed all word type representations131model AUCHMM-TYPE-R 0.18BROWN-TYPE-R 0.16LATTICE-TYPE-R 0.11NGRAM-R 0.10Random baseline 0.10Table 4: HMM-TYPE-R outperforms the other methods,improving performance by 12.5% over Brown clusters,and by 80% over the traditional NGRAM-R.using this corpus.To obtain examples of multiple semantic cat-egories, we utilized selected Wikipedia ?listOf?pages from (Pantel et al, 2009) and augmented thesewith our own manually defined categories, such thateach list contained at least ten distinct examples oc-curring in our corpus.
In all, we had 432 exam-ples across 16 distinct categories such as Countries,Greek Islands, and Police TV Dramas.6.3 ResultsFor each semantic category, we tested five differ-ent random selections of five seed examples, treatingthe unselected members of the category as positiveexamples, and all other candidate phrases as nega-tive examples.
We evaluate using the area under theprecision-recall curve (AUC) metric.The results are shown in Table 4.
All represen-tations improve performance over a random base-line, equal to the average AUC over five random or-derings for each category, and the graphical modelsoutperform the ngram representation.
HMM-TYPE-R performs the best overall, and Brown clusteringwith 1000 clusters is comparable (320 and 100 clus-ter perform slightly worse).As with POS tagging, we expect that languagemodel representations improve performance on theIE task by providing informative features for sparseword types.
However, because the IE task classifiesword types rather than tokens, we expect the rep-resentations to provide less benefit for polysemousword types.
To test these hypotheses, we measuredhow IE performance changed in sparse or polyse-mous settings.
We identified polysemous categoriesas those for which fewer than 90% of the categorymembers had the category as a clear dominant sense(estimated manually); other categories were consid-ered non-polysemous.
Categories whose membershad a median number of occurrences in the cor-pus less than 30 were deemed sparse, and othersnon-sparse.
IE performance on these subsets of thedata are shown in Table 3.
Both graphical modelrepresentations outperform the ngram representationmore on sparse words, as expected.
For polysemy,the picture is mixed: the PL-MRF outperform n-grams on polysemous categories, whereas HMM?sperformance advantage over n-grams decreases.One surprise on the IE task is that the LATTICE-TYPE-R performs significantly less well than theHMM-TYPE-R, whereas the reverse is true on POStagging.
We suspect that the difference is due to theissue of classifying types vs. tokens.
Because oftheir more complex structure, PL-MRFs tend to de-pend more on transition parameters than do HMMs.Furthermore, our decision to train the PL-MRFsusing contrastive estimation with a neighborhoodthat swaps consecutive pairs of words also tends toemphasize transition parameters.
As a result, webelieve the posterior distribution over latent statesgiven a word type is more informative in our HMMmodel than the PL-MRF model.
We measured theentropy of these distributions for the two models,and found that H(PPL-MRF(y|x = w)) = 9.95 bits,compared with H(PHMM(y|x = w)) = 2.74 bits,which supports the hypothesis that the drop in thePL-MRF?s performance on IE is due to its depen-dence on transition parameters.
Further experimentsare warranted to investigate this issue.7 Conclusion and Future WorkOur investigation into language models as represen-tations shows that graphical models can be used tocombat polysemy and, especially, sparsity in rep-resentations for weakly-supervised classifiers.
Ournovel factorial graphical model yields a state-of-the-art POS tagger for domain adaptation, and HMMsimprove significantly over all other representationsin an information extraction task.
Important direc-tions for future research include models for han-dling polysemy in IE, and richer language modelsthat incorporate more linguistic intuitions about howwords interact with their contexts.AcknowledgmentsThis research was supported in part by NSF grantIIS-1065397 and a Microsoft New Faculty Fellow-ship.132ReferencesArun Ahuja and Doug Downey.
2010.
Improved extrac-tion assessment through better language models.
InProceedings of the Annual Meeting of the North Amer-ican Chapter of the Association of Computational Lin-guistics (NAACL-HLT).Shai Ben-David, John Blitzer, Koby Crammer, AlexKulesza, Fernando Pereira, and Jenn Wortman.
2009.A theory of learning from different domains.
MachineLearning, (to appear).Y.
Bengio, J. Louradour, R. Collobert, and J. Weston.2009.
Curriculum learning.
In International Confer-ence on Machine Learning (ICML).Yoshua Bengio.
2008.
Neural net language models.Scholarpedia, 3(1):3881.Daniel M. Bikel.
2004.
Intricacies of Collins ParsingModel.
Computational Linguistics, 30(4):479?511.David M. Blei, Andrew Y. Ng, and Michael I. Jordan.2003.
Latent dirichlet alocation.
Journal of MachineLearning Research, 3:993?1022, January.John Blitzer, Ryan McDonald, and Fernando Pereira.2006.
Domain adaptation with structural correspon-dence learning.
In EMNLP.John Blitzer, Koby Crammer, Alex Kulesza, FernandoPereira, and Jenn Wortman.
2007.
Learning boundsfor domain adaptation.
In Advances in Neural Infor-mation Processing Systems.Hans L. Bodlaender.
1988.
Dynamic programming ongraphs with bounded treewidth.
In Proc.
15th Interna-tional Colloquium on Automata, Languages and Pro-gramming, pages 105?118.P.
F. Brown, P. V. deSouza, R. L. Mercer, V. J. D. Pietra,and J. C. Lai.
1992.
Class-based n-gram models ofnatural language.
Computational Linguistics, pages467?479.M.
Candito and B. Crabbe.
2009.
Improving generativestatistical parsing with semi-supervised word cluster-ing.
In IWPT, pages 138?141.R.
Collobert and J. Weston.
2008.
A unified architecturefor natural language processing: Deep neural networkswith multitask learning.
In International Conferenceon Machine Learning (ICML).Hal Daume?
III and Daniel Marcu.
2006.
Domain adap-tation for statistical classifiers.
Journal of ArtificialIntelligence Research, 26.Hal Daume?
III, Abhishek Kumar, and Avishek Saha.2010.
Frustratingly easy semi-supervised domainadaptation.
In Proceedings of the ACL Workshop onDomain Adaptation (DANLP).Hal Daume?
III.
2007.
Frustratingly easy domain adapta-tion.
In ACL.S.
C. Deerwester, S. T. Dumais, T. K. Landauer, G. W.Furnas, and R. A. Harshman.
1990.
Indexing by latentsemantic analysis.
Journal of the American Society ofInformation Science, 41(6):391?407.Arthur Dempster, Nan Laird, and Donald Rubin.
1977.Likelihood from incomplete data via the EM algo-rithm.
Journal of the Royal Statistical Society, SeriesB, 39(1):1?38.D.
Downey, M. Broadhead, and O. Etzioni.
2007a.
Lo-cating complex named entities in web text.
In Procs.of the 20th International Joint Conference on ArtificialIntelligence (IJCAI 2007).Doug Downey, Stefan Schoenmackers, and Oren Etzioni.2007b.
Sparse information extraction: Unsupervisedlanguage models to the rescue.
In ACL.Mark Dredze and Koby Crammer.
2008.
Online methodsfor multi-domain learning and adaptation.
In Proceed-ings of EMNLP, pages 689?697.Mark Dredze, Alex Kulesza, and Koby Crammer.
2010.Multi-domain learning by confidence weighted param-eter combination.
Machine Learning, 79.Kevin Duh.
2005.
Jointly labeling multiple sequences: AFactorial HMM approach.
In 43rd Annual Meeting ofthe Assoc.
for Computational Linguistics (ACL 2005),Student Research Workshop.Jenny Rose Finkel and Christopher D. Manning.
2009.Hierarchical bayesian domain adaptation.
In Proceed-ings of HLT-NAACL, pages 602?610.Zoubin Ghahramani and Michael I. Jordan.
1997.
Facto-rial hidden markov models.
Machine Learning, 29(2-3):245?273.Z.
Harris.
1954.
Distributional structure.
Word,10(23):146?162.D.
Hindle.
1990.
Noun classification from predicage-argument structures.
In ACL.T.
Honkela.
1997.
Self-organizing maps of words fornatural language processing applications.
In Proceed-ings of the International ICSC Symposium on SoftComputing.Fei Huang and Alexander Yates.
2009.
Distributionalrepresentations for handling sparsity in supervised se-quence labeling.
In Proceedings of the Annual Meet-ing of the Association for Computational Linguistics(ACL).Fei Huang and Alexander Yates.
2010a.
Exploringrepresentation-learning approaches to domain adapta-tion.
In Proceedings of the ACL 2010 Workshop onDomain Adaptation for Natural Language Processing(DANLP).Fei Huang and Alexander Yates.
2010b.
Open-domainsemantic role labeling by modeling word spans.
InProceedings of the Annual Meeting of the Associationfor Computational Linguistics (ACL).133Jing Jiang and ChengXiang Zhai.
2007.
Instance weight-ing for domain adaptation in NLP.
In ACL.S.
Kaski.
1998.
Dimensionality reduction by randommapping: Fast similarity computation for clustering.In IJCNN, pages 413?418.T.
Koo, X. Carreras, and M. Collins.
2008.
Simple semi-supervised dependency parsing.
In Proceedings of theAnnual Meeting of the Association of ComputationalLinguistics (ACL), pages 595?603.J.
Lafferty, Andrew McCallum, and Fernando Pereira.2001.
Conditional random fields: Probabilistic mod-els for segmenting and labeling sequence data.
In Pro-ceedings of the International Conference on MachineLearning.D.
Lin and X Wu.
2009.
Phrase clustering for discrimi-native learning.
In ACL-IJCNLP, pages 1030?1038.D.C.
Liu and J. Nocedal.
1989.
On the limited mem-ory method for large scale optimization.
Mathemati-cal Programming B, 45(3):503?528.Y.
Mansour, M. Mohri, and A. Rostamizadeh.
2009.
Do-main adaptation with multiple sources.
In Advances inNeural Information Processing Systems.Mitchell P. Marcus, Mary Ann Marcinkiewicz, and Beat-rice Santorini.
1993.
Building a large annotated cor-pus of English: the Penn Treebank.
ComputationalLinguistics, 19(2):313?330.S.
Martin, J. Liermann, and H. Ney.
1998.
Algorithmsfor bigram and trigram word clustering.
Speech Com-munication, 24:19?37.A.
Mnih and G. E. Hinton.
2009.
A scalable hierarchi-cal distributed language model.
In Neural InformationProcessing Systems (NIPS), pages 1081?1088.P.
Pantel, E. Crestan, A. Borkovsky, A. M. Popescu, andV.
Vyas.
2009.
Web-scale distributional similarity andentity set expansion.
In Proc.
of EMNLP.PennBioIE.
2005.
Mining the bibliome project.http://bioie.ldc.upenn.edu/.F.
Pereira, N. Tishby, and L. Lee.
1993.
Distributionalclustering of English words.
In Proceedings of theAnnual Meeting of the Association for ComputationalLinguistics (ACL), pages 183?190.Lawrence R. Rabiner.
1989.
A tutorial on hiddenMarkov models and selected applications in speechrecognition.
Proceedings of the IEEE, 77(2):257?285.Ivan A.
Sag, Thomas Wasow, and Emily M. Bender.2003.
Synactic Theory: A Formal Introduction.
CSLI,Stanford, CA, second edition.M.
Sahlgren.
2005.
An introduction to random index-ing.
In Methods and Applications of Semantic Index-ing Workshop at the 7th International Conference onTerminology and Knowledge Engineering (TKE).M.
Sahlgren.
2006.
The word-space model: Us-ing distributional analysis to represent syntagmaticand paradigmatic relations between words in high-dimensional vector spaces.
Ph.D. thesis, StockholmUniversity.G.
Salton and M.J. McGill.
1983.
Introduction to Mod-ern Information Retrieval.
McGraw-Hill.L.
Shen, G. Satta, and A. Joshi.
2007.
Guided learn-ing for bidirectional sequence classification.
In Pro-ceedings of the 45th Annual Meeting of the Associa-tion of Computational Linguistics (ACL 2007), pages760?767.Noah A. Smith and Jason Eisner.
2005.
Contrastive esti-mation: Training log-linear models on unlabeled data.In Proceedings of the 43rd Annual Meeting of the As-sociation for Computational Linguistics (ACL), pages354?362, Ann Arbor, Michigan, June.Charles Sutton, Andrew McCallum, and Khashayar Ro-hanimanesh.
2007.
Dynamic conditional randomfields: Factorized probabilistic models for labelingand segmenting sequence data.
J. Mach.
Learn.
Res.,8:693?723.Joseph Turian, Lev Ratinov, and Yoshua Bengio.
2010.Word representations: A simple and general methodfor semi-supervised learning.
In Proceedings of theAnnual Meeting of the Association for ComputationalLinguistics (ACL), pages 384?394.P.
D. Turney and P. Pantel.
2010.
From frequency tomeaning: Vector space models of semantics.
Journalof Artificial Intelligence Research, 37:141?188.J.
J. Va?yrynen, T. Honkela, and L. Lindqvist.
2007.Towards explicit semantic features using independentcomponent analysis.
In Proceedings of the Work-shop Semantic Content Acquisition and Representa-tion (SCAR).Jason Weston, Frederic Ratle, and Ronan Collobert.2008.
Deep learning via semi-supervised embedding.In Proceedings of the 25th International Conferenceon Machine Learning.Hai Zhao, Wenliang Chen, Chunyu Kit, and GuodongZhou.
2009.
Multilingual dependency learning: Ahuge feature engineering method to semantic depen-dency parsing.
In CoNLL 2009 Shared Task.134
