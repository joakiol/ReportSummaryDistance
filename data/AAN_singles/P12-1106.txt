Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 1006?1014,Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational LinguisticsCombining Coherence Models and Machine Translation Evaluation Metricsfor Summarization EvaluationZiheng Lin?, Chang Liu?, Hwee Tou Ng?
and Min-Yen Kan??
SAP Research, SAP Asia Pte Ltd30 Pasir Panjang Road, Singapore 117440ziheng.lin@sap.com?
Department of Computer Science, National University of Singapore13 Computing Drive, Singapore 117417{liuchan1,nght,kanmy}@comp.nus.edu.sgAbstractAn ideal summarization system should pro-duce summaries that have high content cov-erage and linguistic quality.
Many state-of-the-art summarization systems focus on con-tent coverage by extracting content-dense sen-tences from source articles.
A current researchfocus is to process these sentences so that theyread fluently as a whole.
The current AE-SOP task encourages research on evaluatingsummaries on content, readability, and over-all responsiveness.
In this work, we adapta machine translation metric to measure con-tent coverage, apply an enhanced discoursecoherence model to evaluate summary read-ability, and combine both in a trained regres-sion model to evaluate overall responsiveness.The results show significantly improved per-formance over AESOP 2011 submitted met-rics.1 IntroductionResearch and development on automatic and man-ual evaluation of summarization systems have beenmainly focused on content coverage (Lin and Hovy,2003; Nenkova and Passonneau, 2004; Hovy et al,2006; Zhou et al, 2006).
However, users may stillfind it difficult to read such high-content coveragesummaries as they lack fluency.
To promote researchon automatic evaluation of summary readability, theText Analysis Conference (TAC) (Owczarzak andDang, 2011) introduced a new subtask on readabilityto its Automatically Evaluating Summaries of Peers(AESOP) task.Most of the state-of-the-art summarization sys-tems (Ng et al, 2011; Zhang et al, 2011; Conroyet al, 2011) are extraction-based.
They extract themost content-dense sentences from source articles.If no post-processing is performed to the generatedsummaries, the presentation of the extracted sen-tences may confuse readers.
Knott (1996) arguedthat when the sentences of a text are randomly or-dered, the text becomes difficult to understand, as itsdiscourse structure is disturbed.
Lin et al (2011)validated this argument by using a trained modelto differentiate an original text from a randomly-ordered permutation of its sentences by looking attheir discourse structures.
This prior work leads usto believe that we can apply such discourse mod-els to evaluate the readability of extract-based sum-maries.
We will discuss the application of Lin etal.
?s discourse coherence model to evaluate read-ability of machine generated summaries.
We alsointroduce two new feature sources to enhance themodel with hierarchical and Explicit/Non-Explicitinformation, and demonstrate that they improve theoriginal model.There are parallels between evaluations of ma-chine translation (MT) and summarization with re-spect to textual content.
For instance, the widelyused ROUGE (Lin and Hovy, 2003) metrics are in-fluenced by BLEU (Papineni et al, 2002): bothlook at surface n-gram overlap for content cover-age.
Motivated by this, we will adapt a state-of-the-art, linear programming-based MT evaluation met-ric, TESLA (Liu et al, 2010), to evaluate the contentcoverage of summaries.TAC?s overall responsiveness metric evaluates the1006quality of a summary with regard to both its con-tent and readability.
Given this, we combine ourtwo component coherence and content models intoan SVM-trained regression model as our surrogateto overall responsiveness.
Our experiments showthat the coherence model significantly outperformsall AESOP 2011 submissions on both initial and up-date tasks, while the adapted MT evaluation metricand the combined model significantly outperform allsubmissions on the initial task.
To the best of ourknowledge, this is the first work that applies a dis-course coherence model to measure the readabilityof summaries in the AESOP task.2 Related WorkNenkova and Passonneau (2004) proposed a manualevaluation method that was based on the idea thatthere is no single best model summary for a collec-tion of documents.
Human annotators construct apyramid to capture important Summarization Con-tent Units (SCUs) and their weights, which is usedto evaluate machine generated summaries.Lin and Hovy (2003) introduced an automaticsummarization evaluation metric, called ROUGE,which was motivated by the MT evaluation met-ric, BLEU (Papineni et al, 2002).
It automati-cally determines the content quality of a summaryby comparing it to the model summaries and count-ing the overlapping n-gram units.
Two configura-tions ?
ROUGE-2, which counts bigram overlaps,and ROUGE-SU4, which counts unigram and bi-gram overlaps in a word window of four ?
have beenfound to correlate well with human evaluations.Hovy et al (2006) pointed out that automatedmethods such as ROUGE, which match fixed lengthn-grams, face two problems of tuning the appropri-ate fragment lengths and matching them properly.They introduced an evaluation method that makesuse of small units of content, called Basic Elements(BEs).
Their method automatically segments a textinto BEs, matches similar BEs, and finally scoresthem.Both ROUGE and BE have been implementedand included in the ROUGE/BE evaluation toolkit1,which has been used as the default evaluation toolin the summarization track in the Document Un-1http://berouge.com/default.aspxderstanding Conference (DUC) and Text AnalysisConference (TAC).
DUC and TAC also manuallyevaluated machine generated summaries by adopt-ing the Pyramid method.
Besides evaluating withROUGE/BE and Pyramid, DUC and TAC also askedhuman judges to score every candidate summarywith regard to its content, readability, and overall re-sponsiveness.DUC and TAC defined linguistic quality to coverseveral aspects: grammaticality, non-redundancy,referential clarity, focus, and structure/coherence.Recently, Pitler et al (2010) conducted experimentson various metrics designed to capture these as-pects.
Their experimental results on DUC 2006 and2007 show that grammaticality can be measured bya set of syntactic features, while the last three as-pects are best evaluated by local coherence.
Con-roy and Dang (2008) combined two manual linguis-tic scores ?
grammaticality and focus ?
with variousROUGE/BE metrics, and showed this helps betterpredict the responsiveness of the summarizers.Since 2009, TAC introduced the task of Auto-matically Evaluating Summaries of Peers (AESOP).AESOP 2009 and 2010 focused on two summaryqualities: content and overall responsiveness.
Sum-mary content is measured by comparing the outputof an automatic metric with the manual Pyramidscore.
Overall responsiveness measures a combi-nation of content and linguistic quality.
In AESOP2011 (Owczarzak and Dang, 2011), automatic met-rics are also evaluated for their ability to assess sum-mary readability, i.e., to measure how linguisticallyreadable a machine generated summary is.
Sub-mitted metrics that perform consistently well on thethree aspects include Giannakopoulos and Karkalet-sis (2011), Conroy et al (2011), and de Oliveira(2011).
Giannakopoulos and Karkaletsis (2011) cre-ated two character-based n-gram graph representa-tions for both the model and candidate summaries,and applied graph matching algorithm to assess theirsimilarity.
Conroy et al (2011) extended the modelin (Conroy and Dang, 2008) to include shallow lin-guistic features such as term overlap, redundancy,and term and sentence entropy.
de Oliveira (2011)modeled the similarity between the model and can-didate summaries as a maximum bipartite matchingproblem, where the two summaries are representedas two sets of nodes and precision and recall are cal-1007w=1.0 w=0.8 w=0.2 w=0.1w=1.0 w=0.8 w=0.1.2s=0.5 s=1.0s=0.5 s=1.0(a) The matching problemw=1.0 w=0.8 w=0.2 w=0.1w=1.0 w=0.8 w=0.1.2w=1.0 w=0.2w=0.s w=0.1(b) The matching solutionFigure 1: A BNG matching problem.
Top andbottom rows of each figure represent BNG fromthe model and candidate summaries, respectively.Links are similarities.
Both n-grams and links areweighted.culated from the matched edges.
However, none ofthe AESOP metrics currently apply deep linguisticanalysis, which includes discourse analysis.Motivated by the parallels between summariza-tion and MT evaluation, we will adapt a state-of-the-art MT evaluation metric to measure summarycontent quality.
To apply deep linguistic analysis,we also enhance an existing discourse coherencemodel to evaluate summary readability.
We focuson metrics that measure the average quality of ma-chine summarizers, i.e., metrics that can rank a setof machine summarizers correctly (human summa-rizers are not included in the list).3 TESLA-S: Evaluating SummaryContentTESLA (Liu et al, 2010) is an MT evaluationmetric which extends BLEU by introducing a lin-ear programming-based framework for improvedmatching.
It also makes use of linguistic resourcesand considers both precision and recall.3.1 The Linear Programming MatchingFrameworkFigure 1 shows the matching of bags of n-grams(BNGs) that forms the core of the TESLA metric.The top row in Figure 1a represents the bag of n-grams (BNG) from the model summary, and thebottom row represents the BNG from the candidatesummary.
Each n-gram has a weight.
The linksbetween the n-grams represent the similarity score,which are constrained to be between 0 and 1.
Math-ematically, TESLA takes as input the following:1.
The BNG of the model summary, X , and theBNG of the candidate summary, Y .
The ith en-try in X is xi and has weight xWi (analogouslyfor yi and yWi ).2.
A similarity score s(xi, yj) between all n-grams xi and yj .The goal of the matching process is to align thetwo BNGs so as to maximize the overall similar-ity.
The variables of the problem are the allocatedweights for the edges,w(xi, yj) ?i, jTESLA maximizes?i,js(xi, yj)w(xi, yj)subject tow(xi, yj) ?
0 ?i, j?jw(xi, yj) ?
xWi ?i?iw(xi, yj) ?
yWj ?jThis real-valued linear programming problem canbe solved efficiently.
The overall similarity S is thevalue of the objective function.
Thus,Precision =S?j yWjRecall =S?i xWiThe final TESLA score is given by the F-measure:F =Precision?
Recall??
Precision + (1?
?)?
RecallIn this work, we set ?
= 0.8, following (Liu et al,2010).
The score places more importance on recallthan precision.
When multiple model summaries areprovided, TESLA matches the candidate BNG witheach of the model BNGs.
The maximum score istaken as the combined score.10083.2 TESLA-S: TESLA for SummarizationWe adapted TESLA for the nuances of summariza-tion.
Mimicking ROUGE-SU4, we construct onematching problem between the unigrams and onebetween skip bigrams with a window size of four.The two F scores are averaged to give the final score.The similarity score s(xi, yj) is 1 if the word sur-face forms of xi and yj are identical, and 0 other-wise.
TESLA has a more sophisticated similaritymeasure that focuses on awarding partial scores forsynonyms and parts of speech (POS) matches.
How-ever, the majority of current state-of-the-art sum-marization systems are extraction-based systems,which do not generate new words.
Although oursimplistic similarity score may be problematic whenevaluating abstract-based systems, the experimen-tal results support our choice of the similarity func-tion.
This reflects a major difference between MTand summarization evaluation: while MT systemsalways generate new sentences, most summarizationsystems focus on locating existing salient sentences.Like in TESLA, function words (words in closedPOS categories, such as prepositions and articles)have their weights reduced by a factor of 0.1, thusplacing more emphasis on the content words.
Wefound this useful empirically.3.3 Significance TestKoehn (2004) introduced a bootstrap resamplingmethod to compute statistical significance of the dif-ference between two machine translation systemswith regard to the BLEU score.
We adapt thismethod to compute the difference between two eval-uation metrics in summarization:1.
Randomly choose n topics from the n giventopics with replacement.2.
Summarize the topics with the list of machinesummarizers.3.
Evaluate the list of summaries from Step 2 withthe two evaluation metrics under comparison.4.
Determine which metric gives a higher correla-tion score.5.
Repeat Step 1 ?
4 for 1,000 times.As we have 44 topics in TAC 2011 summarizationtrack, n = 44.
The percentage of times metric agives higher correlation than metric b is said to bethe significance level at which a outperforms b.Initial UpdateP S K P S KR-2 0.9606 0.8943 0.7450 0.9029 0.8024 0.6323R-SU4 0.9806 0.8935 0.7371 0.8847 0.8382 0.6654BE 0.9388 0.9030 0.7456 0.9057 0.8385 0.68434 0.9672 0.9017 0.7351 0.8249 0.8035 0.60706 0.9678 0.8816 0.7229 0.9107 0.8370 0.66068 0.9555 0.8686 0.7024 0.8981 0.8251 0.660610 0.9501 0.8973 0.7550 0.7680 0.7149 0.550411 0.9617 0.8937 0.7450 0.9037 0.8018 0.629112 0.9739 0.8972 0.7466 0.8559 0.8249 0.640213 0.9648 0.9033 0.7582 0.8842 0.7961 0.627624 0.9509 0.8997 0.7535 0.8115 0.8199 0.6386TESLA-S 0.9807 0.9173 0.7734 0.9072 0.8457 0.6811Table 1: Content correlation with human judgmenton summarizer level.
Top three scores among AE-SOP metrics are underlined.
The TESLA-S score isbolded when it outperforms all others.
ROUGE-2 isshortened to R-2 and ROUGE-SU4 to R-SU4.3.4 ExperimentsWe test TESLA-S on the AESOP 2011 content eval-uation task, judging the metric fitness by compar-ing its correlations with human judgments for con-tent.
The results for the initial and update tasks arereported in Table 1.
We show the three baselines(ROUGE-2, ROUGE-SU4, and BE) and submittedmetrics with correlations among the top three scores,which are underlined.
This setting remains the samefor the rest of the experiments.
We use three cor-relation measures: Pearson?s r, Spearman?s ?, andKendall?s ?
, represented by P, S, and K, respectively.The ROUGE scores are the recall scores, as per con-vention.
On the initial task, TESLA-S outperformsall metrics on all three correlation measures.
On theupdate task, TESLA-S ranks second, first, and sec-ond on Pearson?s r, Spearman?s ?, and Kendall?s ?
,respectively.To test how significant the differences are, we per-form significance testing using Koehn?s resamplingmethod between TESLA-S and ROUGE-2/ROUGE-SU4, on which TESLA-S is based.
The findings are:?
Initial task: TESLA-S is better than ROUGE-2at 99% significance level as measured by Pear-son?s r.?
Update task: TESLA-S is better than ROUGE-SU4 at 95% significance level as measured byPearson?s r.?
All other differences are statistically insignifi-cant, including all correlations on Spearman?s1009?
and Kendall?s ?
.The last point can be explained by the fact thatSpearman?s ?
and Kendall?s ?
are sensitive to onlythe system rankings, whereas Pearson?s r is sensitiveto the magnitude of the differences as well, hencePearson?s r is in general a more sensitive measure.4 DICOMER: Evaluating SummaryReadabilityIntuitively, a readable text should also be coherent,and an incoherent text will result in low readabil-ity.
Both readability and coherence indicate howfluent a text is.
We thus hypothesize that a modelthat measures how coherent a text is can also mea-sure its readability.
Lin et al (2011) introduced dis-course role matrix to represent discourse coherenceof a text.
W first illustrate their model with an exam-ple, and then introduce two new feature sources.
Wethen apply the models and evaluate summary read-ability.4.1 Lin et al?s Discourse Coherence ModelFirst, a free text in Figure 2 is parsed by a dis-course parser to derive its discourse relations, whichare shown in Figure 3.
Lin et al observed thatcoherent texts preferentially follow certain relationpatterns.
However, simply using such patterns tomeasure the coherence of a text can result in fea-ture sparseness.
To solve this problem, they expandthe relation sequence into a discourse role matrix,as shown in Table 2.
The matrix essentially cap-tures term occurrences in the sentence-to-sentencerelation sequences.
This model is motivated bythe entity-based model (Barzilay and Lapata, 2008)which captures sentence-to-sentence entity transi-tions.
Next, the discourse role transition probabili-ties of lengths 2 and 3 (e.g., Temp.Arg1?Exp.Arg2and Comp.Arg1?nil?Temp.Arg1) are calculatedwith respect to the matrix.
For example, the prob-ability of Comp.Arg2?Exp.Arg2 is 2/25 = 0.08 inTable 2.Lin et al applied their model on the task of dis-cerning an original text from a permuted ordering ofits sentences.
They modeled it as a pairwise rank-ing model (i.e., original vs. permuted), and trained aSVM preference ranking model with discourse roleS1 Japan normally depends heavily on the High-land Valley and Cananea mines as well as theBougainville mine in Papua New Guinea.S2 Recently, Japan has been buying copper elsewhere.S3.1 But as Highland Valley and Cananea begin operat-ing,S3.2 they are expected to resume their roles as Japan?ssuppliers.S4.1 According to Fred Demler, metals economist forDrexel Burnham Lambert, New York,S4.2 ?Highland Valley has already started operatingS4.3 and Cananea is expected to do so soon.
?Figure 2: A text with four sentences.
Si.j means thejth clause in the ith sentence.S1           S2          S3.1          S3.2          S4.1          S4.2          S4.3Implicit ComparisonExplicit ComparisonExplicit TemporalImplicit ExpansionExplicit ExpansionFigure 3: The discourse relations for Figure 2.
Ar-rows are pointing from Arg2 to Arg1.S#Termscopper cananea operat depend .
.
.S1 nil Comp.Arg1 nil Comp.Arg1S2Comp.Arg2nil nil nilComp.Arg1S3 nilComp.Arg2 Comp.Arg2nilTemp.Arg1 Temp.Arg1Exp.Arg1 Exp.Arg1S4 nil Exp.Arg2Exp.Arg1nilExp.Arg2Table 2: Discourse role matrix fragment extractedfrom Figure 2 and 3.
Rows correspond to sen-tences, columns to stemmed terms, and cells containextracted discourse roles.
Temporal, Contingency,Comparison, and Expansion are shortened to Temp,Cont, Comp, and Exp, respectively.transitions as features and their probabilities as val-ues.4.2 Two New Feature SourcesWe observe that there are two kinds of informa-tion in Figure 3 that are not captured by Lin et al?s1010model.
The first one is whether a relation is Ex-plicit or Non-Explicit (Lin et al (2010) termed Non-Explicit to include Implicit, AltLex, EntRel, andNoRel).
Explicit relation and Non-Explicit relationhave different distributions on each discourse rela-tion (PDTB-Group, 2007).
Thus, adding this in-formation may further improve the model.
In ad-dition to the set of the discourse roles of ?Rela-tion type .
Argument tag?, we introduce anotherset of ?Explicit/Non-Explicit .
Relation type .
Ar-gument tag?.
The cell Ccananea,S3 now containsComp.Arg2, Temp.Arg1, Exp.Arg1, E.Comp.Arg2,E.Temp.Arg1, and N.Exp.Arg1 (E for Explicit andN for Non-Explicit).The other information that is not in the discourserole matrix is the discourse hierarchy structure,i.e., whether one relation is embedded withinanother relation.
In Figure 3, S3.1 is Arg1 ofExplicit Temporal, which is Arg2 of the higherrelation Explicit Comparison as well as Arg1 ofanother higher relation Implicit Expansion.
Thesedependencies are important for us to know howwell-structured a summary is.
It is representedby the multiple discourse roles in each cell of thematrix.
For example, the multiple discourse roles inthe cell Ccananea,S3 capture the three dependenciesjust mentioned.
We introduce intra-cell bigramsas a new set of features to the original model: fora cell with multiple discourse roles, we sort themby their surface strings and multiply to obtainthe bigrams.
For instance, Ccananea,S3 will pro-duce bigrams such as Comp.Arg2?Exp.Arg1and Comp.Arg2?Temp.Arg1.
When boththe Explicit/Non-Explicit feature source andthe intra-cell feature source are joined to-gether, it also produces bigram features suchas E.Comp.Arg2?Temp.Arg1.4.3 Predicting Readability ScoresLin et al (2011) used the SVMlight (Joachims,1999) package with the preference ranking config-uration.
To train the model, each source text andone of its permutations form a training pair, wherethe source text is given a rank of 1 and the permuta-tion is given 0.
In testing, the trained model predictsa real number score for each instance, and the in-stance with the higher score in a pair is said to bethe source text.In the TAC summarization track, human judgesscored each model and candidate summary with areadability score from 1 to 5 (5 means most read-able).
Thus in our setting, instead of a pair of texts,the training input consists of a list of model and can-didate summaries from each topic, with their anno-tated scores as the rankings.
Given an unseen testsummary, the trained model predicts a real numberscore.
This score essentially is the readability rank-ing of the test summary.
Such ranking can be eval-uated by the ranking-based correlations of Spear-man?s ?
and Kendall?s ?
.
As Pearson?s r measureslinear correlation and we do not know whether thereal number score follows a linear function, we takethe logarithm of this score as the readability scorefor this instance.We use the data from AESOP 2009 and 2010 asthe training data, and test our metrics on AESOP2011 data.
To obtain the discourse relations of asummary, we use the discourse parser2 developed inLin et al (2010).4.4 ExperimentsTable 3 shows the resulting readability correlations.The last four rows show the correlation scores forour coherence model: LIN is the default modelby (Lin et al, 2011), LIN+C is LIN with theintra-cell feature class, LIN+E is enhanced withthe Explicit/Non-Explicit feature class.
We namethe LIN model with both new feature sources (i.e.,LIN+C+E) DICOMER ?
a DIscourse COherenceModel for Evaluating Readability.LIN outperforms all metrics on all correlations onboth tasks.
On the initial task, it outperforms thebest scores by 3.62%, 16.20%, and 12.95% on Pear-son, Spearman, and Kendall, respectively.
Similargaps (4.27%, 18.52%, and 13.96%) are observedon the update task.
The results are much betteron Spearman and Kendall.
This is because LIN istrained with a ranking model, and both Spearmanand Kendall are ranking-based correlations.Adding either intra-cell or Explicit/Non-Explicitfeatures improves all correlation scores, withExplicit/Non-Explicit giving more pronounced im-provements.
When both new feature sources are in-2http://wing.comp.nus.edu.sg/?linzihen/parser/1011Initial UpdateP S K P S KR-2 0.7524 0.3975 0.2925 0.6580 0.3732 0.2635R-SU4 0.7840 0.3953 0.2925 0.6716 0.3627 0.2540BE 0.7171 0.4091 0.2911 0.5455 0.2445 0.16224 0.8194 0.4937 0.3658 0.7423 0.4819 0.36126 0.7840 0.4070 0.3036 0.6830 0.4263 0.314112 0.7944 0.4973 0.3589 0.6443 0.3991 0.306218 0.7914 0.4746 0.3510 0.6698 0.3941 0.285623 0.7677 0.4341 0.3162 0.7054 0.4223 0.3014LIN 0.8556 0.6593 0.4953 0.7850 0.6671 0.5008LIN+C 0.8612 0.6703 0.4984 0.7879 0.6828 0.5135LIN+E 0.8619 0.6855 0.5079 0.7928 0.6990 0.5309DICOMER 0.8666 0.7122 0.5348 0.8100 0.7145 0.5435Table 3: Readability correlation with human judg-ment on summarizer level.
Top three scores amongAESOP metrics are underlined.
Our score is boldedwhen it outperforms all AESOP metrics.Initial Updatevs.
P S K P S KLIN4?
??
??
??
??
?
?LIN+C ??
??
??
??
??
?
?LIN+E ??
??
??
?
??
?
?DICOMER ??
??
??
??
??
?
?DICOMER LIN ?
?
?
?
?
?Table 4: Koehn?s significance test for readability.?
?, ?, and ?
indicate significance level >=99%,>=95%, and <95%, respectively.corporated into the metric, we obtain the best resultsfor all correlation scores: DICOMER outperformsLIN by 1.10%, 5.29%, and 3.95% on the initial task,and 2.50%, 4.74%, and 4.27% on the update task.Table 3 shows that summarization evaluationMetric 4 tops all other AESOP metrics, except inthe case of Spearman?s ?
on the initial task.
Wecompare our four models to this metric.
The resultsof Koehn?s significance test are reported in Table 4,which demonstrates that all four models outperformMetric 4 significantly.
In the last row, we see thatwhen comparing DICOMER to LIN, DICOMER issignificantly better on three correlation measures.5 CREMER: Evaluating OverallResponsivenessWith TESLA-S measuring content coverage and DI-COMER measuring readability, it is feasible to com-bine them to predict the overall responsiveness of asummary.
There exist many ways to combine twovariables mathematically: we can combine them ina linear function or polynomial function, or in a wayInitial UpdateP S K P S KR-2 0.9416 0.7897 0.6096 0.9169 0.8401 0.6778R-SU4 0.9545 0.7902 0.6017 0.9123 0.8758 0.7065BE 0.9155 0.7683 0.5673 0.8755 0.7964 0.62544 0.9498 0.8372 0.6662 0.8706 0.8674 0.70336 0.9512 0.7955 0.6112 0.9271 0.8769 0.716011 0.9427 0.7873 0.6064 0.9194 0.8432 0.679412 0.9469 0.8450 0.6746 0.8728 0.8611 0.685818 0.9480 0.8447 0.6715 0.8912 0.8377 0.668323 0.9317 0.7952 0.6080 0.9192 0.8664 0.695325 0.9512 0.7899 0.6033 0.9033 0.8139 0.6349CREMERLF 0.9381 0.8346 0.6635 0.8280 0.6860 0.5173CREMERPF 0.9621 0.8567 0.6921 0.8852 0.7863 0.6159CREMERRBF 0.9716 0.8836 0.7206 0.9018 0.8285 0.6588Table 5: Responsiveness correlation with humanjudgment on summarizer level.
Top three scoresamong AESOP metrics are underlined.
CREMERscore is bolded when it outperforms all AESOP met-rics.similar to how precision and recall are combinedin F measure.
We applied a machine learning ap-proach to train a regression model for measuringresponsiveness.
The scores predicted by TESLA-S and DICOMER are used as two features.
Weuse SVMlight with the regression configuration, test-ing three kernels: linear function, polynomial func-tion, and radial basis function.
We called this modelCREMER ?
a Combined REgression Model forEvaluating Responsiveness.We train the regression model on AESOP 2009and 2010 data sets, and test it on AESOP 2011.
TheDICOMER model that is trained in Section 4 is usedto predict the readability scores on all AESOP 2009,2010, and 2011 summaries.
We apply TESLA-S topredict content scores on all AESOP 2009, 2010,and 2011 summaries.5.1 ExperimentsThe last three rows in Table 5 show the correlationscores of our regression model trained with SVMlinear function (LF), polynomial function (PF), andradial basis function (RBF).
PF performs better thanLF, suggesting that content and readability scoresshould not be linearly combined.
RBF gives bet-ter performances than both LF and PF, suggestingthat RBF better models the way humans combinecontent and readability.
On the initial task, themodel trained with RBF outperforms all submittedmetrics.
It outperforms the best correlation scores1012by 1.71%, 3.86%, and 4.60% on Pearson, Spear-man, and Kendall, respectively.
All three regressionmodels do not perform as well on the update task.Koehn?s significance test shows that when trainedwith RBF, CREMER outperforms ROUGE-2 andROUGE-SU4 on the initial task at a significancelevel of 99% for all three correlation measures.6 DiscussionThe intuition behind the combined regression modelis that combining the readability and content scoreswill give an overall good responsiveness score.
Thefunction to combine them and their weights can beobtained by training.
While the results showed thatSVM radial basis kernel gave the best performances,this function may not truly mimic how human evalu-ates responsiveness.
Human judges were told to ratesummaries by their overall qualities.
They may takeinto account other aspects besides content and read-ability.
Given CREMER did not perform well on theupdate task, we hypothesize that human judgmentof update summaries may involve more complicatedrankings or factor in additional input that CREMERcurrently does not model.
We plan to devise a bet-ter responsiveness metric in our future work, beyondusing a simple combination.Figure 4 shows a complete picture of Pearson?s rfor all AESOP 2011 metrics and our three met-rics on both initial and update tasks.
We highlightour metrics with a circle on these curves.
On theinitial task, correlation scores for content are con-sistently higher than those for responsiveness withsmall gaps, whereas on the update task, they are al-most overlapping.
On the other hand, correlationscores for readability are much lower than those forcontent and responsiveness, with a gap of about 0.2.Comparing Figure 4a and 4b, evaluation metrics al-ways correlate better on the initial task than on theupdate task.
This suggests that there is much roomfor improvement for readability metrics, and metricsneed to consider update information when evaluat-ing update summarizers.7 ConclusionWe proposed TESLA-S by adapting an MT eval-uation metric to measure summary content cover-age, and introduced DICOMER by applying a dis-0.40.50.60.70.80.91Pearson?srContentResponsivenessReadability(a) Evaluation metric values on the initial task.0.3 0.40.5 0.60.7 0.80.9 1Pearson?srContentResponsivenessReadability(b) Evaluation metric values on the updatetask.Figure 4: Pearson?s r for all AESOP 2011 submittedmetrics and our proposed metrics.
Our metrics arecircled.
Higher r value is better.course coherence model with newly introduced fea-tures to evaluate summary readability.
We com-bined these two metrics in the CREMER metric?
an SVM-trained regression model ?
for auto-matic summarization overall responsiveness evalu-ation.
Experimental results on AESOP 2011 showthat DICOMER significantly outperforms all sub-mitted metrics on both initial and update tasks withlarge gaps, while TESLA-S and CREMER signifi-cantly outperform all metrics on the initial task.
3AcknowledgmentsThis research is supported by the Singapore Na-tional Research Foundation under its InternationalResearch Centre @ Singapore Funding Initiative andadministered by the IDM Programme Office.3Our metrics are publicly available at http://wing.comp.nus.edu.sg/?linzihen/summeval/.1013ReferencesRegina Barzilay and Mirella Lapata.
2008.
Modelinglocal coherence: An entity-based approach.
Computa-tional Linguistics, 34:1?34, March.John M. Conroy and Hoa Trang Dang.
2008.
Mindthe gap: Dangers of divorcing evaluations of summarycontent from linguistic quality.
In Proceedings of the22nd International Conference on Computational Lin-guistics (Coling 2008), Manchester, UK, August.John M. Conroy, Judith D. Schlesinger, Jeff Kubina,Peter A. Rankel, and Dianne P. O?Leary.
2011.CLASSY 2011 at TAC: Guided and multi-lingual sum-maries and evaluation metrics.
In Proceedings of theText Analysis Conference 2011 (TAC 2011), Gaithers-burg, Maryland, USA, November.Paulo C. F. de Oliveira.
2011.
CatolicaSC at TAC 2011.In Proceedings of the Text Analysis Conference (TAC2011), Gaithersburg, Maryland, USA, November.George Giannakopoulos and Vangelis Karkaletsis.
2011.AutoSummENG and MeMoG in evaluating guidedsummaries.
In Proceedings of the Text Analysis Con-ference (TAC 2011), Gaithersburg, Maryland, USA,November.Eduard Hovy, Chin-Yew Lin, Liang Zhou, and JunichiFukumoto.
2006.
Automated summarization evalua-tion with basic elements.
In Proceedings of the FifthConference on Language Resources and Evaluation(LREC 2006).Thorsten Joachims.
1999.
Making large-scale sup-port vector machine learning practical.
In BernhardSchlkopf, Christopher J. C. Burges, and Alexander J.Smola, editors, Advances in Kernel Methods ?
SupportVector Learning.
MIT Press, Cambridge, MA, USA.Alistair Knott.
1996.
A Data-Driven Methodology forMotivating a Set of Coherence Relations.
Ph.D. the-sis, Department of Artificial Intelligence, Universityof Edinburgh.Philipp Koehn.
2004.
Statistical significance tests formachine translation evaluation.
In Proceedings of the2004 Conference on Empirical Methods in NaturalLanguage Processing (EMNLP 2004).Chin-Yew Lin and Eduard Hovy.
2003.
Automatic evalu-ation of summaries using n-gram co-occurrence statis-tics.
In Proceedings of the 2003 Conference of theNorth American Chapter of the Association for Com-putational Linguistics on Human Language Technol-ogy (NAACL 2003), Morristown, NJ, USA.Ziheng Lin, Hwee Tou Ng, and Min-Yen Kan. 2010.
APDTB-styled end-to-end discourse parser.
TechnicalReport TRB8/10, School of Computing, National Uni-versity of Singapore, August.Ziheng Lin, Hwee Tou Ng, and Min-Yen Kan. 2011.
Au-tomatically evaluating text coherence using discourserelations.
In Proceedings of the 49th Annual Meetingof the Association for Computational Linguistics: Hu-man Language Technologies (ACL-HLT 2011), Port-land, Oregon, USA, June.Chang Liu, Daniel Dahlmeier, and Hwee Tou Ng.2010.
TESLA: Translation evaluation of sentenceswith linear-programming-based analysis.
In Proceed-ings of the Joint Fifth Workshop on Statistical MachineTranslation and MetricsMATR, Uppsala, Sweden.
As-sociation for Computational Linguistics.Ani Nenkova and Rebecca Passonneau.
2004.
Evaluat-ing content selection in summarization: The pyramidmethod.
In Proceedings of the 2004 Human LanguageTechnology Conference / North American Chapter ofthe Association for Computational Linguistics AnnualMeeting (HLT-NAACL 2004), Boston, Massachusetts,USA, May.Jun Ping Ng, Praveen Bysani, Ziheng Lin, Min-YenKan, and Chew Lim Tan.
2011.
SWING: Exploit-ing category-specific information for guided summa-rization.
In Proceedings of the Text Analysis Confer-ence 2011 (TAC 2011), Gaithersburg, Maryland, USA,November.Karolina Owczarzak and Hoa Trang Dang.
2011.Overview of the TAC 2011 summarization track:Guided task and AESOP task.
In Proceedings of theText Analysis Conference (TAC 2011), Gaithersburg,Maryland, USA, November.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
BLEU: a method for automatic eval-uation of machine translation.
In Proceedings of the40th Annual Meeting of the Association for Computa-tional Linguistics (ACL 2002), Stroudsburg, PA, USA.PDTB-Group, 2007.
The Penn Discourse Treebank 2.0Annotation Manual.
The PDTB Research Group.Emily Pitler, Annie Louis, and Ani Nenkova.
2010.Automatic evaluation of linguistic quality in multi-document summarization.
In Proceedings of the 48thAnnual Meeting of the Association for ComputationalLinguistics (ACL 2010), Stroudsburg, PA, USA.Renxian Zhang, You Ouyang, and Wenjie Li.
2011.Guided summarization with aspect recognition.
InProceedings of the Text Analysis Conference 2011(TAC 2011), Gaithersburg, Maryland, USA, Novem-ber.Liang Zhou, Chin-Yew Lin, Dragos Stefan Munteanu,and Eduard Hovy.
2006.
Paraeval: Using paraphrasesto evaluate summaries automatically.
In Proceedingsof the Human Language Technology Conference of theNorth American Chapter of the Association for Com-putational Linguistics (HLT-NAACL 2006), Strouds-burg, PA, USA.1014
