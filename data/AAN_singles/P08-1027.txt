Proceedings of ACL-08: HLT, pages 227?235,Columbus, Ohio, USA, June 2008. c?2008 Association for Computational LinguisticsClassification of Semantic Relationships between NominalsUsing Pattern ClustersDmitry DavidovICNCHebrew University of Jerusalemdmitry@alice.nc.huji.ac.ilAri RappoportInstitute of Computer ScienceHebrew University of Jerusalemarir@cs.huji.ac.ilAbstractThere are many possible different semantic re-lationships between nominals.
Classificationof such relationships is an important and dif-ficult task (for example, the well known nouncompound classification task is a special caseof this problem).
We propose a novel pat-tern clusters method for nominal relationship(NR) classification.
Pattern clusters are dis-covered in a large corpus independently ofany particular training set, in an unsupervisedmanner.
Each of the extracted clusters cor-responds to some unspecified semantic rela-tionship.
The pattern clusters are then usedto construct features for training and classifi-cation of specific inter-nominal relationships.Our NR classification evaluation strictly fol-lows the ACL SemEval-07 Task 4 datasets andprotocol, obtaining an f-score of 70.6, as op-posed to 64.8 of the best previous work thatdid not use the manually provided WordNetsense disambiguation tags.1 IntroductionAutomatic extraction and classification of seman-tic relationships is a major field of activity, of bothpractical and theoretical interest.
A prominent typeof semantic relationships is that holding betweennominals1.
For example, in noun compounds manydifferent semantic relationships are encoded by thesame simple form (Girju et al, 2005): ?dog food?
de-notes food consumed by dogs, while ?summer morn-1Our use of the term ?nominal?
follows (Girju et al, 2007),and includes simple nouns, noun compounds and multiword ex-pressions serving as nouns.ing?
denotes a morning that happens in the summer.These two relationships are completely different se-mantically but are similar syntactically, and distin-guishing between them could be essential for NLPapplications such as question answering and ma-chine translation.Relation classification usually relies on a train-ing set in the form of tagged data.
To improve re-sults, some systems utilize additional manually con-structed semantic resources such as WordNet (WN)(Beamer et al, 2007).
However, in many domainsand languages such resources are not available.
Fur-thermore, usage of such resources frequently re-quires disambiguation and connection of the data tothe resource (word sense disambiguation in the caseof WordNet).
Manual disambiguation is unfeasiblein many practical tasks, and an automatic one mayintroduce errors and greatly degrade performance.
Itthus makes sense to try to minimize the usage ofsuch resources, and utilize only corpus contexts inwhich the relevant words appear.A leading method for utilizing context informa-tion for classification and extraction of relationshipsis that of patterns (Hearst, 1992; Pantel and Pen-nacchiotti, 2006).
The standard classification pro-cess is to find in an auxiliary corpus a set of patternsin which a given training word pair co-appears, anduse pattern-word pair co-appearance statistics as fea-tures for machine learning algorithms.In this paper we introduce a novel approach,based on utilizing pattern clusters that are preparedseparately and independently of the training set.
Wedo not utilize any manually constructed resource orany manual tagging of training data beyond the cor-227rect classification, thus making our method applica-ble to fully automated tasks and less domain and lan-guage dependent.
Moreover, our pattern clusteringalgorithm is fully unsupervised.Our method is based on the observation that whileeach lexical pattern can be highly ambiguous, sev-eral patterns in conjunction can reliably define andrepresent a lexical relationship.
Accordingly, weconstruct pattern clusters from a large generic cor-pus, each such cluster potentially representing someimportant generic relationship.
This step is donewithout accessing any training data, anticipating thatmost meaningful relationships, including those in agiven classification problem, will be represented bysome of the discovered clusters.
We then use thetraining set to label some of the clusters, and the la-beled clusters to assign classes to tested items.
Oneof the advantages of our method is that it can be usednot only for classification, but also for further anal-ysis and retrieval of the observed relationships2.The semantic relationships between the compo-nents of noun compounds and between nominals ingeneral are not easy to categorize rigorously.
Sev-eral different relationship hierarchies have been pro-posed (Nastase and Szpakowicz, 2003; Moldovan etal., 2004).
Some classes, like Container-Contained,Time-Event and Product-Producer, appear in sev-eral classification schemes, while classes like Tool-Object are more vaguely defined and are subdivideddifferently.
Recently, SemEval-07 Task 4 (Girju etal., 2007) proposed a benchmark dataset that in-cludes a subset of 7 widely accepted nominal rela-tionship (NR) classes, allowing consistent evalua-tion of different NR classification algorithms.
In theSemEval event, 14 research teams evaluated their al-gorithms using this benchmark.
Some of the teamshave used the manually annotated WN labels pro-vided with the dataset, and some have not.We evaluated our algorithm on SemEval-07 Task4 data, showing superior results over participatingalgorithms that did not utilize WordNet disambigua-tion tags.
We also show how pattern clusters can beused for a completely unsupervised classification of2In (Davidov and Rappoport, 2008) we focus on the pat-tern cluster resource type itself, presenting an evaluation of itsintrinsic quality based on SAT tests.
In the present paper wefocus on showing how the resource can be used to improve aknown NLP task.the test set.
Since in this case no training data isused, this allows the automated discovery of a po-tentially unbiased classification scheme.Section 2 discusses related work, Section 3 out-lines the pattern clustering algorithm, Section 4 de-tails three classification methods, and Sections 5 and6 describe the evaluation protocol and results.2 Related WorkNumerous methods have been devised for classifica-tion of semantic relationships, among which thoseholding between nominals constitute a prominentcategory.
Major differences between these methodsinclude available resources, degree of preprocessing,features used, classification algorithm and the natureof training/test data.2.1 Available ResourcesMany relation classification algorithms utilizeWordNet.
Among the 15 systems presented bythe 14 SemEval teams, some utilized the manuallyprovided WordNet tags for the dataset pairs (e.g.,(Beamer et al, 2007)).
In all cases, usage of WNtags improves the results significantly.
Some othersystems that avoided using the labels used WN asa supporting resource for their algorithms (Costello,2007; Nakov and Hearst, 2007; Kim and Baldwin,2007).
Only three avoided WN altogether (Hen-drickx et al, 2007; Bedmar et al, 2007; Aramakiet al, 2006).Other resources used for relationship discoveryinclude Wikipedia (Strube and Ponzetto, 2006), the-sauri or synonym sets (Turney, 2005) and domain-specific semantic hierarchies like MeSH (Rosarioand Hearst, 2001).While usage of these resources is beneficial inmany cases, high quality word sense annotation isnot easily available.
Besides, lexical resources arenot available for many languages, and their coverageis limited even for English when applied to some re-stricted domains.
In this paper we do not use anymanually annotated resources apart from the classi-fication training set.2.2 Degree of PreprocessingMany relationship classification methods utilizesome language-dependent preprocessing, like deepor shallow parsing, part of speech tagging and228named entity annotation (Pantel et al, 2004).
Whilethe obtained features were shown to improve classi-fication performance, they tend to be language de-pendent and error-prone when working on unusualtext domains and are also highly computationally in-tensive when processing large corpora.
To make ourapproach as language independent and efficient aspossible, we avoided using any such preprocessingtechniques.2.3 Classification FeaturesA wide variety of features are used by differentalgorithms, ranging from simple bag-of-words fre-quencies to WordNet-based features (Moldovan etal., 2004).
Several studies utilize syntactic features.Many other works manually develop a set of heuris-tic features devised with some specific relationshipin mind, like a WordNet-based meronymy feature(Bedmar et al, 2007) or size-of feature (Aramakiet al, 2006).
However, the most prominent featuretype is based on lexico-syntactic patterns in whichthe related words co-appear.Since (Hearst, 1992), numerous works have usedpatterns for discovery and identification of instancesof semantic relationships (e.g., (Girju et al, 2006;Snow et al, 2006; Banko et al 2007)).
Rosenfeldand Feldman (2007) discover relationship instancesby clustering entities appearing in similar contexts.Strategies were developed for discovery of multi-ple patterns for some specified lexical relationship(Pantel and Pennacchiotti, 2006) and for unsuper-vised pattern ranking (Turney, 2006).
Davidov etal.
(2007) use pattern clusters to define general rela-tionships, but these are specific to a given concept.No study so far has proposed a method to define, dis-cover and represent general relationships present inan arbitrary corpus.In (Davidov and Rappoport, 2008) we presentan approach to extract pattern clusters from an un-tagged corpus.
Each such cluster represents someunspecified lexical relationship.
In this paper, weuse these pattern clusters as the (only) source of ma-chine learning features for a nominal relationshipclassification problem.
Unlike the majority of cur-rent studies, we avoid using any other features thatrequire some language-specific information or aredevised for specific relationship types.2.4 Classification AlgorithmVarious learning algorithms have been used for re-lation classification.
Common choices include vari-ations of SVM (Girju et al, 2004; Nastase et al,2006), decision trees and memory-based learners.Freely available tools like Weka (Witten and Frank,1999) allow easy experimentation with commonlearning algorithms (Hendrickx et al, 2007).
In thispaper we did not focus on a single ML algorithm,letting algorithm selection be automatically basedon cross-validation results on the training set, as in(Hendrickx et al, 2007) but using more algorithmsand allowing a more flexible parameter choice.2.5 Training DataAs stated above, several categorization schemes fornominals have been proposed.
Nastase and Sz-pakowicz (2003) proposed a two-level hierarchywith 5 (30) classes at the top (bottom) levels3.
Thishierarchy and a corresponding dataset were used in(Turney, 2005; Turney, 2006) and (Nastase et al,2006) for evaluation of their algorithms.
Moldovanet al (2004) proposed a different scheme with 35classes.
The most recent dataset has been developedfor SemEval 07 Task 4 (Girju et al, 2007).
Thismanually annotated dataset includes a representativerather than exhaustive list of 7 important nominalrelationships.
We have used this dataset, strictly fol-lowing the evaluation protocol.
This made it possi-ble to meaningfully compare our method to state-of-the-art methods for relation classification.3 Pattern Clustering AlgorithmOur pattern clustering algorithm is designed for theunsupervised definition and discovery of generic se-mantic relationships.
The algorithm first discoversand clusters patterns in which a single (?hook?)
wordparticipates, and then merges the resulting clustersto form the final structure.
In (Davidov and Rap-poport, 2008) we describe the algorithm at length,discuss its behavior and parameters in detail, andevaluate its intrinsic quality.
To assist readers ofthe present paper, in this section we provide anoverview.
Examples of some resulting pattern clus-ters are given in Section 6.
We refer to a pattern3Actually, there were 50 relationships at the bottom level,but valid nominal instances were found only for 30.229contained in our clusters (a pattern type) as a ?pat-tern?
and to an occurrence of a pattern in the corpus(a pattern token) as a ?pattern instance?.The algorithm does not rely on any data from theclassification training set, hence we do not need torepeat its execution for different classification prob-lems.
To calibrate its parameters, we ran it a fewtimes with varied parameters settings, producingseveral different configurations of pattern clusterswith different degrees of noise, coverage and granu-larity.
We then chose the best configuration for ourtask automatically without re-running pattern clus-tering for each specific problem (see Section 5.3).3.1 Hook Words and Hook CorporaAs a first step, we randomly sample a set of hookwords, which will be used in order to discover re-lationships that generally occur in the corpus.
Toavoid selection of ambiguous words or typos, we donot select words with frequency higher than a pa-rameter FC and lower than a threshold FB .
We alsolimit the total number N of hook words.
For eachhook word, we now create a hook corpus, the set ofthe contexts in which the word appears.
Each con-text is a window containing W words or punctuationcharacters before and after the hook word.3.2 Pattern SpecificationTo specify patterns, following (Davidov and Rap-poport, 2006) we classify words into high-frequency words (HFWs) and content words (CWs).A word whose frequency is more (less) than FH(FC) is considered to be a HFW (CW).
Our patternshave the general form[Prefix] CW1 [Infix] CW2 [Postfix]where Prefix, Infix and Postfix contain only HFWs.We require Prefix and Postfix to be a single HFW,while Infix can contain any number of HFWs (limit-ing pattern length by window size).
This form mayinclude patterns like ?such X as Y and?.
At this stage,the pattern slots can contain only single words; how-ever, when using the final pattern clusters for nomi-nal relationship classification, slots can contain mul-tiword nominals.3.3 Discovery of Target WordsFor each of the hook corpora, we now extract allpattern instances where one CW slot contains thehook word and the other CW slot contains someother (?target?)
word.
To avoid the selection of com-mon words as target words, and to avoid targets ap-pearing in pattern instances that are relatively fixedmultiword expressions, we sort all target words ina given hook corpus by pointwise mutual informa-tion between hook and target, and drop patterns ob-tained from pattern instances containing the lowestand highest L percent of target words.3.4 Pattern ClusteringWe now have for each hook corpus a set of patterns,together with the target words used for their extrac-tion, and we want to cluster pattern types.
First,we group in clusters all patterns extracted using thesame target word.
Second, we merge clusters thatshare more than S percent of their patterns.
Somepatterns can appear in more than a single cluster.Finally, we merge pattern clusters from differenthook corpora, to avoid clusters specific to a singlehook word.
During merging, we define and utilizecore patterns and unconfirmed patterns, which areweighed differently during cluster labeling (see Sec-tion 4.2).
We merge clusters from different hookcorpora using the following algorithm:1.
Remove all patterns originating from a single hookcorpus only.2.
Mark all patterns of all present clusters as uncon-firmed.3.
While there exists some cluster C1 from corpus DXcontaining only unconfirmed patterns:(a) Select a cluster with a minimal number of pat-terns.
(b) For each corpus D different from DX :i. Scan D for clusters C2 that share at leastS percent of their patterns, and all of theircore patterns, with C1.ii.
Add all patterns of C2 to C1, setting allshared patterns as core and all others asunconfirmed.iii.
Remove cluster C2.
(c) If all of C1?s patterns remain unconfirmed re-move C1.4.
If several clusters have the same set of core patternsmerge them according to rules (i,ii).At the end of this stage, we have a set of patternclusters where for each cluster there are two subsets,core patterns and unconfirmed patterns.2304 Relationship ClassificationUp to this stage we did not access the training set inany way and we did not use the fact that the target re-lations are those holding between nominals.
Hence,only a small part of the acquired pattern clusters maybe relevant for a given NR classification task, whileother clusters can represent completely different re-lationships (e.g., between verbs).
We now use theacquired clusters to learn a model for the given la-beled training set and to use this model for classifi-cation of the test set.
First we describe how we dealwith data sparseness.
Then we propose a HITS mea-sure used for cluster labeling, and finally we presentthree different classification methods that utilize pat-tern clusters.4.1 Enrichment of Provided DataOur classification algorithm is based on contextsof given nominal pairs.
Co-appearance of nomi-nal pairs can be very rare (in fact, some word pairsin the Task 4 set co-appear only once in Yahooweb search).
Hence we need more contexts wherethe given nominals or nominals similar to them co-appear.
This step does not require the training la-bels (the correct classifications), so we do it for bothtraining and test pairs.
We do it in two stages: ex-tracting similar nominals, and obtaining more con-texts.4.1.1 Extracting more wordsFor each nominal pair (w1, w2) in a given sentenceS, we use a method similar to (Davidov and Rap-poport, 2006) to extract words that have a sharedmeaning with w1 or w2.
We discover such wordsby scanning our corpora and querying the web forsymmetric patterns (obtained automatically from thecorpus as in (Davidov and Rappoport, 2006)) thatcontain w1 or w2.
To avoid getting instances ofw1,2 with a different meaning, we also require thatthe second word will appear in the same text para-graph or the same web page.
For example, if we aregiven a pair <loans, students> and we see a sen-tence ?...
loans and scholarships for students andprofessionals ...?, we use the symmetric pattern ?Xand Y?
to add the word scholarships to the group ofloans and to add the word professionals to the groupof students.
We do not take words from the sen-tence ?In European soccer there are transfers andloans...?
since its context does not contain the wordstudents.
In cases where there are only several orzero instances where the two nominals co-appear,we dismiss the latter rule and scan for each nominalseparately.
Note that ?loans?
can also be a verb, sousage of a part-of-speech tagger might reduce noise.If the number of instances for a desired nom-inal is very low, our algorithm trims the firstwords in these nominal and repeats the search (e.g.,<simulation study, voluminous results> becomes<study, results>).
This step is the only one specificto English, using the nature of English noun com-pounds.
Our desire in this case is to keep the headwords.4.1.2 Extracting more contexts using the newwordsTo find more instances where nominals similar tow1 and w2 co-appear in HFW patterns, we constructweb queries using combinations of each nominal?sgroup and extract patterns from the search resultsnapshots (the two line summary provided by searchengines for each search result).4.2 The HITS MeasureTo use clusters for classification we define a HITSmeasure similar to that of (Davidov et al, 2007), re-flecting the affinity of a given nominal pair to a givencluster.
We use the pattern clusters from Section 3and the additional data collected during the enrich-ment phase to estimate a HITS value for each clusterand each pair in the training and test sets.
For a givennominal pair (w1, w2) and cluster C with n core pat-terns Pcore and m unconfirmed patterns Punconf ,HITS(C, (w1, w2)) =|{p; (w1, w2) appears in p ?
Pcore}| /n+??
|{p; (w1, w2) appears in p ?
Punconf}| /m.In this formula, ?appears in?
means that the nomi-nal pair appears in instances of this pattern extractedfrom the original corpus or retrieved from the webat the previous stage.
Thus if some pair appears inmost of the patterns of some cluster it receives a highHITS value for this cluster.
?
(0..1) is a parameterthat lets us modify the relative weight of core andunconfirmed patterns.2314.3 Classification Using Pattern ClustersWe present three ways to use pattern clusters for re-lationship classification.4.3.1 Classification by cluster labelingOne way to train a classifier in our case is to attacha single relationship label to each cluster during thetraining phase, and to assign each unlabeled pair tosome labeled cluster during the test phase.
We usethe following normalized HITS measure to label theinvolved pattern clusters.
Denote by ki the numberof training pairs in class i in training set T .
ThenLabel(C) = argmaxi?p?T,Label(p)=ihits(C, p)/kiClusters where the above sum is zero remain un-labeled.
In the test phase we assign to each test pairp the label of the labeled cluster C that received thehighest HITS(C, p) value.
If there are several clus-ters with a highest HITS value, then the algorithm se-lects a ?clarifying?
set of patterns ?
patterns that aredifferent in these best clusters.
Then it constructsclarifying web queries that contain the test nomi-nal pair inside the clarifying patterns.
The effect isto increment the HITS value of the cluster contain-ing a clarifying pattern if an appropriate pattern in-stance (including the target nominals) was found onthe web.
We start with the most frequent clarifyingpattern and perform additional queries until no clar-ifying patterns are left or until some labeled clusterobtains a highest HITS value.
If no patterns are leftbut there are still several winning clusters, we assignto the pair the label of the cluster with the largestnumber of pattern instances in the corpus.One advantage of this method is that we get asa by-product a set of labeled pattern clusters.
Ex-amination of this set can help to distinguish and an-alyze (by means of patterns) which different rela-tionships actually exist for each class in the train-ing set.
Furthermore, labeled pattern clusters can beused for web queries to obtain additional examplesof the same relationship.4.3.2 Classification by cluster HITS values asfeaturesIn this method we treat the HITS measure for a clus-ter as a feature for a machine learning classificationalgorithm.
To do this, we construct feature vectorsfrom each training pair, where each feature is theHITS measure corresponding to a single pattern clus-ter.
We prepare test vectors similarly.
Once we havefeature vectors, we can use a variety of classifiers(we used those in Weka) to construct a model and toevaluate it on the test set.4.3.3 Unsupervised clusteringIf we are not given any training set, it is still possi-ble to separate between different relationship typesby grouping the feature vectors of Section 4.3.2 intoclusters.
This can be done by applying k-means oranother clustering algorithm to the feature vectorsdescribed above.
This makes the whole approachcompletely unsupervised.
However, it does not pro-vide any inherent labeling, making an evaluation dif-ficult.5 Experimental SetupThe main problem in a fair evaluation of NR classifi-cation is that there is no widely accepted list of pos-sible relationships between nominals.
In our eval-uation we have selected the setup and data fromSemEval-07 Task 4 (Girju et al, 2007).
Selectingthis type of dataset alowed us to compare to 6 sub-mitted state-of-art systems that evaluated on exactlythe same data and to 9 other systems that utilizeadditional information (WN labels).
We have ap-plied our three different classification methods onthe given data set.5.1 SemEval-07 Task 4 OverviewTask 4 (Girju et al, 2007) involves classification ofrelationships between simple nominals other thannamed entities.
Seven distinct relationships werechosen: Cause-Effect, Instrument-Agency, Product-Producer, Origin-Entity, Theme-Tool, Part-Whole,and Content-Container.
For each relationship, theprovided dataset consists of 140 training and 70 testexamples.
Examples were binary tagged as belong-ing/not belonging to the tested relationship.
The vastmajority of negative examples were near-misses, ac-quired from the web using the same lexico-syntacticpatterns as the positives.
Examples appear as sen-tences with the nominal pair tagged.
Nouns in thispair were manually labeled with their correspond-ing WordNet 3 labels and the web queries used to232obtain the sentences.
The 15 submitted systemswere assigned into 4 categories according to whetherthey use the WordNet and Query tags (some systemswere assigned to more than a single category, sincethey reported experiments in several settings).
In ourevaluation we do not utilize WordNet or Query tags,hence we compare ourselves with the correspondinggroup (A), containing 6 systems.5.2 Corpus and Web AccessOur algorithm uses two corpora.
We estimate fre-quencies and perform primary search on a local webcorpus containing about 68GB untagged plain text.This corpus was extracted from the web startingfrom open directory links, comprising English webpages with varied topics and styles (Gabrilovich andMarkovitch, 2005).
To enrich the set of given wordpairs and patterns as described in Section 4.1 andto perform clarifying queries, we utilize the YahooAPI for web queries.
For each query, if the desiredwords/patterns were found in a page link?s snapshot,we do not use the link, otherwise we download thepage from the retrieved link and then extract the re-quired data.
If only several links were found for agiven word pair we perform local crawling to depth3 in an attempt to discover more instances.5.3 Parameters and Learning AlgorithmOur algorithm utilizes several parameters.
Insteadof calibrating them manually, we only provideda desired range for each, and the final parametervalues were obtained during selection of the best-performing setup using 10-fold cross-validation onthe training set.
For each parameter we have esti-mated its desired range using the (Nastase and Sz-pakowicz, 2003) set as a development set.
Note thatthis set uses an entirely different relationship classi-fication scheme.
We ran the pattern clustering phaseon 128 different sets of parameters, obtaining 128different clustering schemes with varied granularity,noise and coverage.The parameter ranges obtained are: FC (meta-pattern content word frequency and upper bound forhook word selection): 100?5000 words per million(wpm); FH (meta-pattern HFW): 10 ?
100 wpm;FB (low word count for hook word filtering): 1?50wpm; N (number of hook words): 100 ?
1000; W(window size): 5 or window = sentence; L (tar-get word mutual information filter): 1/3 ?
1/5; S(cluster overlap filter for cluster merging): 2/3; ?
(core vs. unconfirmed weight for HITS estimation):0.1 ?
0.01; S (commonality for cluster merging):2/3.
As designed, each parameter indeed influencesa certain effect.
Naturally, the parameters are notmutually independent.
Selecting the best configu-ration in the cross-validation phase makes the algo-rithm flexible and less dependent on hard-coded pa-rameter values.Selection of learning algorithm and its algorithm-specific parameters were done as follows.
For eachof the 7 classification tasks (one per relationshiptype), for each of the 128 pattern clustering schemes,we prepared a list of most of the compatible al-gorithms available in Weka, and we automaticallyselected the model (a parameter set and an algo-rithm) which gave the best 10-fold cross-validationresults.
The winning algorithms were LWL (Atke-son et al, 1997), SMO (Platt, 1999), and K* (Clearyand Trigg, 1995) (there were 7 tasks, and differentalgorithms could be selected for each task).
We thenused the obtained model to classify the testing set.This allowed us to avoid fixing parameters that arebest for a specific dataset but not for others.
Sinceeach dataset has only 140 examples, the computa-tion time of each learning algorithm is negligible.6 ResultsThe pattern clustering phase results in 90 to 3000distinct pattern clusters, depending on the parametersetup.
Manual sampling of these clusters indeed re-veals that many clusters contain patterns specific tosome apparent lexical relationship.
For example, wehave discovered such clusters as: {?buy Y accessoryfor X!
?, ?shipping Y for X?, ?Y is available for X?, ?Yare available for X?, ?Y are available for X systems?,?Y for X? }
and {?best X for Y?, ?X types for Y?, ?Ywith X?, ?X is required for Y?, ?X as required for Y?,?X for Y?}.
Note that some patterns (?Y for X?)
canappear in many clusters.We applied the three classification methods de-scribed in Section 4.3 to Task 4 data.
For super-vised classification we strictly followed the SemEvaldatasets and rules.
For unsupervised classificationwe did not use any training data.
Using the k-meansalgorithm, we obtained two nearly equal unlabeled233Method P R F AccUnsupervised clustering (4.3.3) 64.5 61.3 62.0 64.5Cluster Labeling (4.3.1) 65.1 69.0 67.2 68.5HITS Features (4.3.2) 69.1 70.6 70.6 70.1Best Task 4 (no WordNet) 66.1 66.7 64.8 66.0Best Task 4 (with WordNet) 79.7 69.8 72.4 76.3Table 1: Our SemEval-07 Task 4 results.Relation Type F Acc CCause-Effect 69.7 71.4 2Instrument-Agency 76.5 74.2 1Product-Producer 76.4 83.8 1Origin-Entity 65.4 62.6 4Theme-Tool 59.4 58.7 6Part-Whole 74.3 70.9 1Content-Container 72.6 69.2 2Table 2: By-relation Task 4 HITS-based results.
C is thenumber of clusters with positive labels.clusters containing test samples.
For evaluation weassigned a negative/positive label to these two clus-ters according to the best alignment with true labels.Table 1 shows our results, along with the best Task4 result not using WordNet labels (Costello, 2007).For reference, the best results overall (Beamer et al,2007) are also shown.
The table shows precision (P)recall (R), F-score (F), and Accuracy (Acc) (percent-age of correctly classified examples).We can see that while our algorithm is not as goodas the best method that utilizes WordNet tags, resultsare superior to all participants who did not use thesetags.
We can also see that the unsupervised methodresults are above the random baseline (50%).
In fact,our results (f-score 62.0, accuracy 64.5) are betterthan the averaged results (58.0, 61.1) of the groupthat did not utilize WN tags.Table 2 shows the HITS-based classification re-sults (F-score and Accuracy) and the number of pos-itively labeled clusters (C) for each relation.
As ob-served by participants of Task 4, we can see that dif-ferent sets vary greatly in difficulty.
However, wealso obtain a nice insight as to why this happens ?relations like Theme-Tool seem very ambiguous andare mapped to several clusters, while relations likeProduct-Producer seem to be well-defined by the ob-tained pattern clusters.The SemEval dataset does not explicitly markitems whose correct classification requires analysisof the context of the whole sentence in which theyappear.
Since our algorithm does not utilize test sen-tence contextual information, we do not expect it toshow exceptional performance on such items.
Thisis a good topic for future research.Since the SemEval dataset is of a very spe-cific nature, we have also applied our classificationframework to the (Nastase and Szpakowicz, 2003)dataset, which contains 600 pairs labeled with 5main relationship types.
We have used the exactevaluation procedure described in (Turney, 2006),achieving a class f-score average of 60.1, as opposedto 54.6 in (Turney, 2005) and 51.2 in (Nastase et al,2006).
This shows that our method produces supe-rior results for rather differing datasets.7 ConclusionRelationship classification is known to improvemany practical tasks, e.g., textual entailment (Tatuand Moldovan, 2005).
We have presented a novelframework for relationship classification, based onpattern clusters prepared as a standalone resource in-dependently of the training set.Our method outperforms current state-of-the-artalgorithms that do not utilize WordNet tags on Task4 of SemEval-07.
In practical situations, it wouldnot be feasible to provide a large amount of suchsense disambiguation tags manually.
Our methodalso shows competitive performance compared tothe majority of task participants that do utilize WNtags.
Our method can produce labeled pattern clus-ters, which can be potentially useful for automaticdiscovery of additional instances for a given rela-tionship.
We intend to pursue this promising direc-tion in future work.Acknowledgement.
We would like to thankthe anonymous reviewers, whose comments havegreatly improved the quality of this paper.ReferencesAramaki, E., Imai, T., Miyo, K., and Ohe, K., 2007.UTH: semantic relation classification using physicalsizes.
ACL SemEval ?07 Workshop.Atkeson, C., Moore, A., and Schaal, S., 1997.
Lo-cally weighted learning.
Artificial Intelligence Review,11(1?5): 75?113.234Banko, M., Cafarella, M. J., Soderland, S., Broadhead,M., and Etzioni, O., 2007.
Open information extrac-tion from the Web.
IJCAI ?07.Beamer, B., Bhat, S., Chee, B., Fister, A., RozovskayaA., and Girju, R., 2007.
UIUC: A knowledge-rich ap-proach to identifying semantic relations between nom-inals.
ACL SemEval ?07 Workshop.Bedmar, I. S., Samy, D., and Martinez, J. L., 2007.UC3M: Classification of semantic relations betweennominals using sequential minimal optimization.
ACLSemEval ?07 Workshop.Cleary, J. G. , Trigg, L. E., 1995.
K*: An instance-basedlearner using and entropic distance measure.
ICML?95.Costello, F. J., 2007.
UCD-FC: Deducing semantic rela-tions using WordNet senses that occur frequently in adatabase of noun-noun compounds.
ACL SemEval ?07Workshop.Davidov, D., Rappoport, A., 2006.
Efficient unsuper-vised discovery of word categories using symmetricpatterns and high frequency words.
COLING-ACL ?06Davidov D., Rappoport A. and Koppel M., 2007.
Fullyunsupervised discovery of concept-specific relation-ships by Web mining.
ACL ?07.Davidov, D., Rappoport, A., 2008.
Unsupervised discov-ery of generic relationships using pattern clusters andits evaluation by automatically generated SAT analogyquestions.
ACL ?08.Gabrilovich, E., Markovitch, S., 2005.
Feature gener-ation for text categorization using world knowledge.IJCAI ?05.Girju, R., Giuglea, A., Olteanu, M., Fortu, O., Bolohan,O., and Moldovan, D., 2004.
Support vector ma-chines applied to the classification of semantic rela-tions in nominalized noun phrases.
HLT/NAACL ?04Workshop on Computational Lexical Semantics.Girju, R., Moldovan, D., Tatu, M., and Antohe, D., 2005.On the semantics of noun compounds.
ComputerSpeech and Language, 19(4):479-496.Girju, R., Badulescu, A., and Moldovan, D., 2006.
Au-tomatic discovery of part-whole relations.
Computa-tional Linguistics, 32(1).Girju, R., Hearst, M., Nakov, P., Nastase, V., Szpakowicz,S., Turney, P., and Yuret, D., 2007.
Task 04: Classi-fication of semantic relations between nominal at Se-mEval 2007.
4th Intl.
Workshop on Semantic Evalua-tions (SemEval ?07), in ACL ?07.Hearst, M., 1992.
Automatic acquisition of hyponymsfrom large text corpora.
COLING ?92Hendrickx, I., Morante, R., Sporleder, C., and van denBosch, A., 2007.
Machine learning of semantic rela-tions with shallow features and almost no data.
ACLSemEval ?07 Workshop.Kim, S.N., Baldwin, T., 2007.
MELB-KB: Nominalclassification as noun compound interpretation.
ACLSemEval ?07 Workshop.Moldovan, D., Badulescu, A., Tatu, M., Antohe, D., andGirju, R., 2004.
Models for the semantic classifica-tion of noun phrases.
HLT-NAACL ?04 Workshop onComputational Lexical Semantics.Nakov, P., and Hearst, M., 2007.
UCB: System descrip-tion for SemEval Task #4.
ACL SemEval ?07 Work-shop.Nastase, V., Szpakowicz, S., 2003.
Exploring noun-modifier semantic relations.
In Fifth Intl.
Workshopon Computational Semantics (IWCS-5).Nastase, V., Sayyad-Shirabad, J., Sokolova, M., and Sz-pakowicz, S., 2006.
Learning noun-modifier semanticrelations with corpus-based and WordNet-based fea-tures.
In Proceedings of the 21st National Conferenceon Artificial Intelligence, Boston, MA.Pantel, P., Ravichandran, D., and Hovy, E., 2004.
To-wards terascale knowledge acquisition.
COLING ?04.Pantel, P., Pennacchiotti, M., 2006.
Espresso: leveraginggeneric patterns for automatically harvesting semanticrelations.
COLING-ACL ?06.Platt, J., 1999.
Fast training of support vector machinesusing sequential minimal optimization.
In Scholkopf,Burges, and Smola, Advances in Kernel Methods ?Support Vector Learning, pp.
185?208.
MIT Press.Rosario, B., Hearst, M., 2001.
Classifying the semanticrelations in noun compounds.
EMNLP ?01.Rosenfeld, B., Feldman, R., 2007.
Clustering for unsu-pervised relation identification.
CIKM ?07.Snow, R., Jurafsky, D., Ng, A.Y., 2006.
Seman-tic taxonomy induction from heterogeneous evidence.COLING-ACL ?06.Strube, M., Ponzetto, S., 2006.
WikiRelate!
computingsemantic relatedness using Wikipedia.
AAAI ?06.Tatu, M., Moldovan, D., 2005.
A semantic approach torecognizing textual entailment.
HLT/EMNLP ?05.Turney, P., 2005.
Measuring semantic similarity by la-tent relational analysis.
IJCAI ?05.Turney, P., 2006.
Expressing implicit semantic relationswithout supervision.
COLING-ACL ?06.Witten, H., Frank, E., 1999.
Data Mining: Practical Ma-chine Learning Tools and Techniques with Java Imple-mentations.
Morgan Kaufman, San Francisco, CA.235
