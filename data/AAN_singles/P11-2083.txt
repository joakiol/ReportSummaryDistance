Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 473?478,Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational LinguisticsClustering Comparable Corpora For Bilingual Lexicon ExtractionBo Li, Eric GaussierUJF-Grenoble 1 / CNRS, FranceLIG UMR 5217firstname.lastname@imag.frAkiko AizawaNational Institute of InformaticsTokyo, Japanaizawa@nii.ac.jpAbstractWe study in this paper the problem of enhanc-ing the comparability of bilingual corpora inorder to improve the quality of bilingual lexi-cons extracted from comparable corpora.
Weintroduce a clustering-based approach for en-hancing corpus comparability which exploitsthe homogeneity feature of the corpus, andfinally preserves most of the vocabulary ofthe original corpus.
Our experiments illus-trate the well-foundedness of this method andshow that the bilingual lexicons obtained fromthe homogeneous corpus are of better qualitythan the lexicons obtained with previous ap-proaches.1 IntroductionBilingual lexicons are an important resource in mul-tilingual natural language processing tasks such asstatistical machine translation (Och and Ney, 2003)and cross-language information retrieval (Balles-teros and Croft, 1997).
Because it is expensive tomanually build bilingual lexicons adapted to dif-ferent domains, researchers have tried to automat-ically extract bilingual lexicons from various cor-pora.
Compared with parallel corpora, it is mucheasier to build high-volume comparable corpora, i.e.corpora consisting of documents in different lan-guages covering overlapping information.
Severalstudies have focused on the extraction of bilinguallexicons from comparable corpora (Fung and McK-eown, 1997; Fung and Yee, 1998; Rapp, 1999;De?jean et al, 2002; Gaussier et al, 2004; Robitailleet al, 2006; Morin et al, 2007; Garera et al, 2009;Yu and Tsujii, 2009; Shezaf and Rappoport, 2010).The basic assumption behind most studies on lex-icon extraction from comparable corpora is a dis-tributional hypothesis, stating that words which aretranslation of each other are likely to appear in simi-lar context across languages.
On top of this hypoth-esis, researchers have investigated the use of betterrepresentations for word contexts, as well as the useof different methods for matching words across lan-guages.
These approaches seem to have reached aplateau in terms of performance.
More recently, anddeparting from such traditional approaches, we haveproposed in (Li and Gaussier, 2010) an approachbased on improving the comparability of the cor-pus under consideration, prior to extracting bilinguallexicons.
This approach is interesting since there isno point in trying to extract lexicons from a corpuswith a low degree of comparability, as the probabil-ity of finding translations of any given word is lowin such cases.
We follow here the same general ideaand aim, in a first step, at improving the compara-bility of a given corpus while preserving most ofits vocabulary.
However, unlike the previous work,we show here that it is possible to guarantee a cer-tain degree of homogeneity for the improved corpus,and that this homogeneity translates into a signifi-cant improvement of both the quality of the resultingcorpora and the bilingual lexicons extracted.2 Enhancing Comparable Corpora: AClustering ApproachWe first introduce in this section the comparabilitymeasure proposed in former work, prior to describ-ing the clustering-based algorithm to improve the473quality of a given comparable corpus.
For conve-nience, the following discussion will be made in thecontext of the English-French comparable corpus.2.1 The Comparability MeasureIn order to measure the degree of comparability ofbilingual corpora, we make use of the measure Mdeveloped in (Li and Gaussier, 2010): Given a com-parable corpus P consisting of an English part Peand a French part Pf , the degree of comparability ofP is defined as the expectation of finding the trans-lation of any given source/target word in the tar-get/source corpus vocabulary.
Let ?
be a functionindicating whether a translation from the translationset Tw of the word w is found in the vocabulary Pvof a corpus P , i.e.:?
(w,P) ={1 iff Tw ?
Pv 6= ?0 elseand letD be a bilingual dictionary withDve denotingits English vocabulary andDvf its French vocabulary.The comparability measure M can be written as:M(Pe,Pf ) (1)=?w?Pe?Dve?
(w,Pf ) +?w?Pf?Dvf?
(w,Pe)#w(Pe ?
Dve ) + #w(Pf ?
Dvf )where #w(P) denotes the number of differentwords present in P .
One can find from equa-tion 1 that M directly measures the proportion ofsource/target words translated in the target/sourcevocabulary of P .2.2 Clustering Documents for High QualityComparable CorporaIf a corpus covers a limited set of topics, it is morelikely to contain consistent information on the wordsused (Morin et al, 2007), leading to improved bilin-gual lexicons extracted with existing algorithms re-lying on the distributional hypothesis.
The term ho-mogeneity directly refers to this fact, and we will say,in an informal manner, that a corpus is homogeneousif it covers a limited set of topics.
The rationale forthe algorithm we introduce here to enhance corpuscomparability is precisely based on the concept ofhomogeneity.
In order to find document sets whichare similar with each other (i.e.
homogeneous), itis natural to resort to clustering techniques.
Further-more, since we need homogeneous corpora for bilin-gual lexicon extraction, it will be convenient to relyon techniques which allows one to easily prune lessrelevant clusters.
To perform all this, we use in thiswork a standard hierarchical agglomerative cluster-ing method.2.2.1 Bilingual Clustering AlgorithmThe overall process retained to build high quality,homogeneous comparable corpora relies on the fol-lowing steps:1.
Using the bilingual similarity measure definedin Section 2.2.2, cluster English and Frenchdocuments so as to get bilingual dendrogramsfrom the original corpus P by grouping docu-ments with related content;2.
Pick high quality sub-clusters by threshold-ing the obtained dendrograms according to thenode depth, which retains nodes far from theroots of the clustering trees;3.
Combine all these sub-clusters to form a newcomparable corpus PH , which thus containshomogeneous, high-quality subparts;4.
Use again steps (1), (2) and (3) to enrich theremaining subpart of P (denoted as PL, PL =P \ PH ) with external resources.The first three steps aim at extracting the most com-parable and homogeneous subpart of P .
Once thishas been done, one needs to resort to new corporaif one wants to build an homogeneous corpus witha high degree of comparability from PL.
To do so,we simply perform, in step (4), the clustering andthresholding process defined in (1), (2) and (3) ontwo comparable corpora: The first one consists ofthe English part of PL and the French part of an ex-ternal corpus PT ; The second one consists of theFrench part of PL and the English part of PT .
Thetwo high quality subparts obtained from these twonew comparable corpora in step (4) are then com-bined with PH to constitute the final comparablecorpus of higher quality.4742.2.2 Similarity MeasureLet us assume that we have two document sets (i.e.clusters) C1 and C2.
In the task of bilingual lexi-con extraction, two document sets are similar to eachother and should be clustered if the combination ofthe two can complement the content of each singleset, which relates to the notion of homogeneity.
Inother words, both the English part Ce1 of C1 and theFrench part Cf1 of C1 should be comparable to theircounterparts (respectively the same for the Frenchpart Cf2 of C2 and the English part Ce2 of C2).
Thisleads to the following similarity measure for C1 andC2:sim(C1, C2) = ?
?M(Ce1, Cf2 )+ (1??)
?M(Ce2, Cf1 )where ?
(0 ?
?
?
1) is a weight controlling theimportance of the two subparts (Ce1 , Cf2 ) and (Ce2 ,Cf1 ).
Intuitively, the larger one, containing more in-formation, of the two comparable corpora (Ce1 , Cf2 )and (Ce2 , Cf1 ) should dominate the overall similar-ity sim(C1, C2).
Since the content relatedness in thecomparable corpus is basically reflected by the re-lations between all the possible bilingual documentpairs, we use here the number of document pairs torepresent the scale of the comparable corpus.
Theweight ?
can thus be defined as the proportion ofpossible document pairs in the current comparablecorpus (Ce1 , Cf2 ) to all the possible document pairs,which is:?
=#d(Ce1) ?#d(Cf2 )#d(Ce1) ?#d(Cf2 ) + #d(Ce2) ?#d(Cf1 )where #d(C) stands for the number of documents inC.
However, this measure does not integrate the rel-ative length of the French and English parts, whichactually impacts the performance of bilingual lexi-con extraction.
If a 1-to-1 constraint is too strong(i.e.
assuming that all clusters should contain thesame number of English and French documents),having completely unbalanced corpora is also notdesirable.
We thus introduce a penalty function ?aiming at penalizing unbalanced corpora:?
(C) =1(1 + log(1 + |#d(Ce)?#d(Cf )|min(#d(Ce)),#d(Cf )))(2)The above penalty function leads us to a new simi-larity measure siml which is the one finally used inthe above algorithm:siml(C1, C2) = sim(C1, C2) ?
?
(C1 ?
C2) (3)3 Experiments and ResultsThe experiments we have designed in this paper aimat assessing (a) whether the clustering-based algo-rithm we have introduced yields corpora of higherquality in terms of comparability scores, and (b)whether the bilingual lexicons extracted from suchcorpora are of higher quality.
Several corpora wereused in our experiments: the TREC1 AssociatedPress corpus (AP, English) and the corpora usedin the CLEF2 campaign including the Los Ange-les Times (LAT94, English), the Glasgow Herald(GH95, English), Le Monde (MON94, French), SDAFrench 94 (SDA94, French) and SDA French 95(SDA95, French).
In addition, two monolingual cor-pora Wiki-En and Wiki-Fr were built by respectivelyretrieving all the articles below the category Societyand Socie?te?
from the Wikipedia dump files3.
Thebilingual dictionary used in the experiments is con-structed from an online dictionary.
It consists of33k distinct English words and 28k distinct Frenchwords, constituting 76k translation pairs.
In our ex-periments, we use the method described in this pa-per, as well as the one in (Li and Gaussier, 2010)which is the only alternative method to enhance cor-pus comparability.3.1 Improving Corpus QualityIn this subsection, the clustering algorithm describedin Section 2.2.1 is employed to improve the qualityof the comparable corpus.
The corpora GH95 andSDA95 are used as the original corpus P0 (56k En-glish documents and 42k French documents).
Weconsider two external corpora: P1T (109k Englishdocuments and 87k French documents) consisting ofthe corpora LAT94, MON94 and SDA94; P2T (368kEnglish documents and 378k French documents)consisting of Wiki-En and Wiki-Fr.1http://trec.nist.gov2http://www.clef-campaign.org3The Wikipedia dump files can be downloaded athttp://download.wikimedia.org.
In this paper, we use the En-glish dump file on July 13, 2009 and the French dump file onJuly 7, 2009.475P0 P1?
P2?
P1 P2 P1 > P0 P2 > P0Precision 0.226 0.277 0.325 0.295 0.461 0.069, 30.5% 0.235, 104.0%Recall 0.103 0.122 0.145 0.133 0.212 0.030, 29.1% 0.109, 105.8%Table 1: Performance of the bilingual lexicon extraction from different corpora (best results in bold)After the clustering process, we obtain the result-ing corpora P1 (with the external corpus P1T ) andP2 (with P2T ).
As mentioned before, we also usedthe method described in (Li and Gaussier, 2010)on the same data, producing resulting corpora P1?
(with P1T ) and P2?
(with P2T ) from P0.
In termsof lexical coverage, P1 (resp.
P2) covers 97.9%(resp.
99.0%) of the vocabulary of P0.
Hence, mostof the vocabulary of the original corpus has beenpreserved.
The comparability score of P1 reaches0.924 and that of P2 is 0.939.
Both corpora aremore comparable than P0 of which the comparabil-ity is 0.881.
Furthermore, both P1 and P2 are morecomparable than P1?
(comparability 0.912) and P2?
(comparability 0.915), which shows homogeneity iscrucial for comparability.
The intrinsic evaluationshows the efficiency of our approach which can im-prove the quality of the given corpus while preserv-ing most of its vocabulary.3.2 Bilingual Lexicon Extraction ExperimentsTo extract bilingual lexicons from comparable cor-pora, we directly use here the method proposed byFung and Yee (1998) which has been referred toas the standard approach in more recent studies(De?jean et al, 2002; Gaussier et al, 2004; Yu andTsujii, 2009).
In this approach, each word w is rep-resented as a context vector consisting of the wordsco-occurring with w in a certain window in the cor-pus.
The context vectors in different languages arethen bridged with an existing bilingual dictionary.Finally, a similarity score is given to any word pairbased on the cosine of their respective context vec-tors.3.2.1 Experiment SettingsIn order to measure the performance of the lexi-cons extracted, we follow the common practice bydividing the bilingual dictionary into 2 parts: 10%of the English words (3,338 words) together withtheir translations are randomly chosen and used asthe evaluation set, the remaining words being usedto compute the similarity of context vectors.
En-glish words not present in Pe or with no translationin Pf are excluded from the evaluation set.
For eachEnglish word in the evaluation set, all the Frenchwords in Pf are then ranked according to their sim-ilarity with the English word.
Precision and recallare then computed on the first N translation candi-date lists.
The precision amounts in this case to theproportion of lists containing the correct translation(in case of multiple translations, a list is deemed tocontain the correct translation as soon as one of thepossible translations is present).
The recall is theproportion of correct translations found in the liststo all the translations in the corpus.
This evaluationprocedure has been used in previous studies and isnow standard.3.2.2 Results and AnalysisIn a first series of experiments, bilingual lexiconswere extracted from the corpora obtained by our ap-proach (P1 and P2), the corpora obtained by theapproach described in (Li and Gaussier, 2010) (P1?and P2?)
and the original corpus P0, with the fixedN value set to 20.
Table 1 displays the results ob-tained.
Each of the last two columns ?P1 > P0?and ?P2 > P0?
contains the absolute and the rel-ative difference (in %) w.r.t.
P0.
As one can note,the best results (in bold) are obtained from the cor-pora P2 built with the method we have described inthis paper.
The lexicons extracted from the enhancedcorpora are of much higher quality than the ones ob-tained from the original corpus .
For instance, theincrease of the precision is 6.9% (30.5% relatively)in P1 and 23.5% (104.0% relatively) in P2, com-pared with P0.
The difference is more remarkablewithP2, which is obtained from a large external cor-pus P2T .
Intuitively, one can expect to find, in largercorpora, more documents related to a given corpus,an intuition which seems to be confirmed by our re-sults.
One can also notice, by comparing P2 andP2?
as well as P1 and P1?, a remarkable improve-ment when considering our approach and the early476methodology.Intuitively, the value N plays an important rolein the above experiments.
In a second series of ex-periments, we let N vary from 1 to 300 and plot theresults obtained with different evaluation measure inFigure 1.
In Figure 1(a) (resp.
Figure 1(b)), the x-axis corresponds to the values taken by N, and the y-axis to the precision (resp.
recall) scores for the lexi-cons extracted on each of the 5 corporaP0,P1?,P2?,P1 and P2.
A clear fact from the figure is that boththe precision and the recall scores increase accord-ing to the increase of the N values, which coincideswith our intuition.
As one can note, our method con-sistently outperforms the previous work and also theoriginal corpus on all the values considered for N .0 100 200 3000.00.20.40.60.8NPrecisionP2P2'P1P1'P0(a) Precision0 100 200 3000.00.10.20.30.4NRecallP2P2'P1P1'P0(b) RecallFigure 1: Performance of bilingual lexicon extractionfrom different corpora with varied N values from 1 to300.
The five lines from the top down in each subfigureare corresponding to the results for P2, P2?, P1, P1?
andP0 respectively.4 DiscussionAs previous studies on bilingual lexicon extrac-tion from comparable corpora radically differ onresources used and technical choices, it is verydifficult to compare them in a unified framework(Laroche and Langlais, 2010).
We compare in thissection our method with some ones in the same vein(i.e.
enhancing bilingual corpora prior to extract-ing bilingual lexicons from them).
Some works like(Munteanu et al, 2004) and (Munteanu and Marcu,2006) propose methods to extract parallel fragmentsfrom comparable corpora.
However, their approachonly focuses on a very small part of the original cor-pus, whereas our work aims at preserving most ofthe vocabulary of the original corpus.We have followed here the general approach in(Li and Gaussier, 2010) which consists in enhancingthe quality of a comparable corpus prior to extract-ing information from it.
However, despite this latterwork, we have shown here a method which ensureshomogeneity of the obtained corpus, and which fi-nally leads to comparable corpora of higher quality.In turn such corpora yield better bilingual lexiconsextracted.AcknowledgementsThis work was supported by the French National Re-search Agency grant ANR-08-CORD-009.ReferencesLisa Ballesteros and W. Bruce Croft.
1997.
Phrasaltranslation and query expansion techniques for cross-language information retrieval.
In Proceedings of the20th ACM SIGIR, pages 84?91, Philadelphia, Pennsyl-vania, USA.Herve?
De?jean, Eric Gaussier, and Fatia Sadat.
2002.An approach based on multilingual thesauri and modelcombination for bilingual lexicon extraction.
In Pro-ceedings of the 19th International Conference onComputational Linguistics, pages 1?7, Taipei, Taiwan.Pascale Fung and Kathleen McKeown.
1997.
Find-ing terminology translations from non-parallel cor-pora.
In Proceedings of the 5th Annual Workshop onVery Large Corpora, pages 192?202, Hong Kong.Pascale Fung and Lo Yuen Yee.
1998.
An IR approachfor translating new words from nonparallel, compara-ble texts.
In Proceedings of the 17th international con-477ference on Computational linguistics, pages 414?420,Montreal, Quebec, Canada.Nikesh Garera, Chris Callison-Burch, and DavidYarowsky.
2009.
Improving translation lexicon induc-tion from monolingual corpora via dependency con-texts and part-of-speech equivalences.
In CoNLL 09:Proceedings of the Thirteenth Conference on Compu-tational Natural Language Learning, pages 129?137,Boulder, Colorado.E.
Gaussier, J.-M. Renders, I. Matveeva, C. Goutte, andH.
De?jean.
2004.
A geometric view on bilinguallexicon extraction from comparable corpora.
In Pro-ceedings of the 42nd Annual Meeting of the Associ-ation for Computational Linguistics, pages 526?533,Barcelona, Spain.Audrey Laroche and Philippe Langlais.
2010.
Revisitingcontext-based projection methods for term-translationspotting in comparable corpora.
In Proceedings ofthe 23rd International Conference on ComputationalLinguistics (Coling 2010), pages 617?625, Beijing,China, August.Bo Li and Eric Gaussier.
2010.
Improving corpuscomparability for bilingual lexicon extraction fromcomparable corpora.
In Proceedings of the 23rd In-ternational Conference on Computational Linguistics,pages 644?652, Beijing, China.Emmanuel Morin, Be?atrice Daille, Koichi Takeuchi, andKyo Kageura.
2007.
Bilingual terminology mining -using brain, not brawn comparable corpora.
In Pro-ceedings of the 45th Annual Meeting of the Associ-ation for Computational Linguistics, pages 664?671,Prague, Czech Republic.Dragos Stefan Munteanu and Daniel Marcu.
2006.
Ex-tracting parallel sub-sentential fragments from non-parallel corpora.
In Proceedings of the 21st Interna-tional Conference on Computational Linguistics andthe 44th annual meeting of the Association for Compu-tational Linguistics, pages 81?88, Sydney, Australia.Dragos Stefan Munteanu, Alexander Fraser, and DanielMarcu.
2004.
Improved machine translation perfor-mance via parallel sentence extraction from compara-ble corpora.
In Proceedings of the HLT-NAACL 2004,pages 265?272, Boston, MA., USA.Franz Josef Och and Hermann Ney.
2003.
A system-atic comparison of various statistical alignment mod-els.
Computational Linguistics, 29(1):19?51.Reinhard Rapp.
1999.
Automatic identification of wordtranslations from unrelated English and German cor-pora.
In Proceedings of the 37th Annual Meeting ofthe Association for Computational Linguistics, pages519?526, College Park, Maryland, USA.Xavier Robitaille, Yasuhiro Sasaki, Masatsugu Tonoike,Satoshi Sato, and Takehito Utsuro.
2006.
Compil-ing French-Japanese terminologies from the web.
InProceedings of the 11st Conference of the EuropeanChapter of the Association for Computational Linguis-tics, pages 225?232, Trento, Italy.Daphna Shezaf and Ari Rappoport.
2010.
Bilingual lex-icon generation using non-aligned signatures.
In Pro-ceedings of the 48th Annual Meeting of the Associa-tion for Computational Linguistics, pages 98?107, Up-psala, Sweden.Kun Yu and Junichi Tsujii.
2009.
Extracting bilingualdictionary from comparable corpora with dependencyheterogeneity.
In Proceedings of HLT-NAACL 2009,pages 121?124, Boulder, Colorado, USA.478
