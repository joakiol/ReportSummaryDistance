Proceedings of SIGDIAL 2010: the 11th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 245?248,The University of Tokyo, September 24-25, 2010. c?2010 Association for Computational LinguisticsI?ve said it before, and I?ll say it again: An empirical investigation of theupper bound of the selection approach to dialogueSudeep Gandhe and David TraumInstitute for Creative Technologies13274 Fiji way, Marina del Rey, CA 90292{gandhe,traum}@ict.usc.eduAbstractWe perform a study of existing dialoguecorpora to establish the theoretical max-imum performance of the selection ap-proach to simulating human dialogue be-havior in unseen dialogues.
This maxi-mum is the proportion of test utterancesfor which an exact or approximate matchexists in the corresponding training cor-pus.
The results indicate that some do-mains seem quite suitable for a corpus-based selection approach, with over half ofthe test utterances having been seen beforein the corpus, while other domains showmuch more novelty compared to previousdialogues.1 IntroductionThere are two main approaches toward automat-ically producing dialogue utterances.
One is theselection approach, in which the task is to pickthe appropriate output from a corpus of possibleoutputs.
The other is the generation approach, inwhich the output is dynamically assembled usingsome composition procedure, e.g.
grammar rulesused to convert information from semantic repre-sentations and/or context to text.The generation approach has the advantage ofa more compact representation for a given gener-ative capacity.
But for any finite set of sentencesproduced, the selection approach could perfectlysimulate the generation approach.
The generationapproach generally requires more analytical effortto devise a good set of grammar rules that coverthe range of desired sentences but do not admit un-desirable or unnatural sentences.
Whereas, in theselection approach, outputs can be limited to thosethat have been observed in human speech.
Thisaffords complex and human-like sentences with-out much detailed analysis.
Moreover, when theoutput is not just text but presented as speech, thesystem may easily use recorded audio clips ratherthan speech synthesis.
This argument also extendsto multi-modal performances, e.g.
using artist an-imation motion capture or recorded video for an-imating virtual human dialogue characters.
Oftenone is willing to sacrifice some generality in or-der to achieve more human-like behavior than iscurrently possible from generation approaches.The selection approach has been used for anumber of dialogue agents, including question-answering characters at ICT (Leuski et al, 2006;Artstein et al, 2009; Kenny et al, 2007), FAQbots (Zukerman and Marom, 2006; Sellberg andJo?nsson, 2008) and web-site information charac-ters.
It is also possible to use the selection ap-proach as a part of the process, e.g.
from words toa semantic representation or from a semantic rep-resentation to words, while using other approachesfor other parts of dialogue processing.The selection approach presents two challengesfor finding an appropriate utterance:?
Is there a good enough utterance to select??
How good is the selection algorithm at find-ing this utterance?We have previously attempted to address the sec-ond question, by proposing the information or-dering task for evaluating dialogue coherence(Gandhe and Traum, 2008).
Here we try to ad-dress the first question, which would provide atheoretical upper bound in quality for any selec-tion approach.
We examine a number of differentdialogue corpora as to their suitability for the se-lection approach.We make the following assumptions to allowautomatic evaluation across a range of corpora.Actual human dialogues represent a gold-standardfor computer systems to emulate; i.e.
choosing anactual utterance in the correct place is the best pos-sible result.
Other utterances can be evaluated asto how close they come to the original utterance,245using a similarity metric.Our methodology is to examine a test corpus ofhuman dialogue utterances to see how well a se-lection approach could approximate these, given atraining corpus of utterances in that domain.
Welook at exact matches as well as utterances havingtheir similarity score above a threshold.
We in-vestigate the effect of the size of training corpora,which lets us know how much data we might needto achieve a certain level of performance.
We alsoinvestigate the effect of domain of training cor-pora.2 Dialogue CorporaWe examine human dialogue utterances from a va-riety of domains.
Our initial set contains six dia-logue corpora from ICT as well as three other pub-licly available corpora.SGT Blackwell is a question-answering char-acter who answers questions about the U.S. Army,himself, and his technology.
The corpus con-sists of visitors interacting with SGT Blackwell atan exhibition booth at a museum.
SGT Star isa question-answering character, like SGT Black-well, who talks about careers in the U.S. Army.The corpus consists of trained handlers present-ing the system.
Amani is a bargaining characterused as a prototype for training soldiers to performtactical questioning.
The SASO system is a ne-gotiation training prototype in which two virtualcharacters negotiate with a human ?trainee?
aboutmoving a medical clinic.
The Radiobots system isa training prototype that responds to military callsfor artillery fire.
IOTA is an extension of the Ra-diobots system.
The corpus consists of trainingsessions between a human trainee and a human in-structor on a variety of missions.
Yao et al (2010)provides details about the ICT corpora.Other corpora involved dialogues betweentwo people playing specific roles in planning,scheduling problem for railroad transportation,the Trains-93 corpus (Heeman and Allen, 1994)and for emergency services, the Monroe corpus(Stent, 2000).
The Switchboard corpus (Godfreyet al, 1992) consists of telephone conversationsbetween two people, based on provided topics.We divided the data from each corpus into atraining set and a test set, as shown in Table 1.
Thedata consists of utterances from one or more hu-man speakers who engage in dialogue with eithervirtual characters (Radiobots, Blackwell, Amani,Star, SASO) or other humans (Switchboard, Mon-roe, IOTA, Trains-93).
These corpora differ alonga number of dimensions such as the size of thecorpus, dialogue genre (question-answering, task-oriented or conversational), types of tasks (ar-tillery calls, moving and scheduling resources, in-formation seeking) and motivation of the partici-pants (exploring a new technology ?
SGT Black-well , presenting a demo ?
SGT Star, undergo-ing training ?
Amani, IOTA or simply for collect-ing the corpus ?
Switchboard, Trains-93, Monroe).While the set of corpora we include does not coverall points in these dimensions, it does present aninteresting range.3 Dialogue Utterance Similarity MetricsTo answer the question of whether an adequateutterance exists in our training corpus that couldbe selected and used, we need an appropriate-ness measure.
We assume that an utterance pro-duced by a human in a dialogue is appropriate,and thus the problem becomes one of construct-ing an appropriate similarity function to comparethe human-produced utterance with the utterancesavailable from the training corpus.
Given a train-ing corpus Utrain and a similarity function f ,we calculate the score for a test utterance ut as,maxsimf (ut) = maxi f(ut, ui);ui ?
UtrainThere are several choices for the utterance simi-larity function f .
Ideally such a function wouldtake meaning and context into account rather thanjust surface similarity, but these aspects are harderto automate, so for our initial experiments we lookat several surface metrics, as described below.Exact measure returns 1 if the utterances are ex-actly same and 0 otherwise.
1-WER, a similar-ity measure related to word error rate, is definedas min (0, 1?
levenshtein(ut, ui)/length(ut)).METEOR (Lavie and Denkowski, 2009), one ofthe automatic evaluation metrics used in machinetranslation is a good candidate for f .
METEORfinds optimal word-to-word alignment betweentest and reference strings based on several modulesthat match exact words, stemmed words and syn-onyms.
METEOR is a tunable metric and for ouranalysis we used the default parameters tuned forthe Adequacy & Fluency task.
All previous mea-sures take into account the word ordering of testand reference strings.
In contrast, document simi-larity measures used in information retrieval gen-erally follow the bag of words assumption, where a246DomainTrain Test mean(maxsimf ) % of utterancesMET - METEOR# utt words # utt words EOR 1-WER Dice Cosine Exact ?
0.9 ?
0.8Blackwell 17755 84.7k 2500 12.0k 0.913 0.878 0.917 0.921 69.6 75.8 82.1Radiobots 995 6.8k 155 1.2k 0.905 0.864 0.920 0.924 53.6 67.7 83.2SGT Star 2974 16.6k 400 2.2k 0.897 0.860 0.906 0.911 65.0 70.5 78.0SASO 3602 23.3k 510 3.6k 0.821 0.742 0.830 0.837 38.4 48.6 62.6IOTA 4935 50.4k 650 5.6k 0.768 0.697 0.800 0.808 36.2 42.8 51.4Trains 93 5554 47.2k 745 6.0k 0.729 0.633 0.758 0.769 34.5 36.9 42.8SWBD1 19741 138.2k 3173 21.5k 0.716 0.628 0.736 0.753 35.8 37.9 44.2Amani 1455 15.8k 182 1.9k 0.675 0.562 0.694 0.706 18.7 25.8 30.8Monroe 5765 43.0k 917 8.8k 0.594 0.491 0.639 0.658 22.3 23.6 26.1Table 1: Corpus details and within domain resultsstring is converted to a set of tokens.
Here we alsoconsidered Cosine and Dice coefficients using thestandard boolean model.
In our experiments, thesurface text was normalized and all punctuationwas removed.4 ExperimentsResults Within a DomainIn our first experiment, we computed maxsimfscores for all test corpus utterances in a givendomain using the training utterances from thesame domain.
For the domains Blackwell, SGTStar, SASO, Amani & Radiobots which are imple-mented dialogue systems our corpus consists ofuser utterances only.
For Trains 93 and Monroecorpora, we make sure to match the speaker rolesfor ut and ui.
For Switchboard, where speakersdo not have any special roles and for IOTA, wherethe speaker information was not readily accessi-ble, we ignore the speaker information and selectutterances from either speaker.Table 1 reports the mean of maxsimf scores.These can be interpreted as the expectation ofmaxsimf score for a new test utterance.
Thehigher this expectation, the more likely it is thatan utterance similar to the new one has beenseen before and thus the domain will be moreamenable to selection approaches.
This tablealso shows the percentage of utterances that hada maxsimMeteor score above a certain thresh-old.
The correlation between maxsimf for dif-ferent choices of f (except Exact match) is veryhigh (Pearson?s r > 0.94).
The histogram anal-ysis shows that SGT Star, Blackwell, Radiobots1Switchboard (SWBD) is a very large corpus and for run-ning our experiments in a reasonable computing time we onlyselected a small portion of it.Figure 1: maxsimMeteor vs # utterances in train-ing data for different domainsand SASO domains are better suited for selec-tion approaches.
Domains like Trains-93, Monroe,Switchboard and Amani have a more diffuse dis-tribution and are not best suited for selection ap-proaches, at least with the amount of data we haveavailable.
The IOTA domain falls somewhere inbetween these two domain classes.Effect of Training Data SizeFigure 1 shows the effect of training data sizeon the maxsimMeteor score.
Radiobots showsvery high scores even for small amounts of train-ing data.
SGT Star and SGT Blackwell also con-verge fairly early.
Switchboard, on the other hand,does not achieve very high scores even with alarge number of utterances.
For all domains, witharound 2500 training utterances maxsimMeteorreaches 90% of its maximum possible value forthe training set.Comparing Different DomainsIn order to understand the similarities be-tween different dialogue domains, we computedmaxsimMeteor for a test domain using training247Training DomainsIOTA Radio-botsSGTStarBlack-wellAmani SASO Trains-93Monroe SWBDTestingDomainsIOTA 0.768 0.440 0.247 0.334 0.196 0.242 0.255 0.297 0.334Radiobots 0.842 0.905 0.216 0.259 0.161 0.183 0.222 0.270 0.284SGT Star 0.324 0.136 0.897 0.622 0.372 0.438 0.339 0.417 0.527Blackwell 0.443 0.124 0.671 0.913 0.507 0.614 0.424 0.534 0.696Amani 0.393 0.134 0.390 0.561 0.675 0.478 0.389 0.420 0.509SASO 0.390 0.125 0.341 0.516 0.459 0.821 0.443 0.454 0.541Trains 93 0.434 0.112 0.214 0.468 0.272 0.429 0.753 0.627 0.557Monroe 0.409 0.119 0.217 0.428 0.276 0.404 0.534 0.630 0.557SWBD 0.368 0.110 0.280 0.490 0.362 0.383 0.562 0.599 0.716Table 2: Mean of maxsimMeteor for comparing different dialogue domains.
The bold-faced values arethe highest in the corresponding row.sets from other domains.
In this exercise, we ig-nored the speaker information.
Table 2 reportsthe mean values of maxsimMeteor for differenttraining domains.
For all the testing domains,using the training corpus from the same domainproduces the best results.
Notice that Radiobotsalso has good performance with the IOTA train-ing data.
This is as expected since IOTA is anextension of Radiobots and should cover a lot ofutterances from the Radiobots domain.
Switch-board and Blackwell training corpora have a over-all higher score for all testing domains.
This maybe due to the breadth and size of these corpora.
Onthe other extreme, the Radiobots training domainperforms very poorly on all testing domains otherthan itself.5 DiscussionWe have examined how well suited a corpus-based selection approach to dialogue can succeedat mimicking human dialogue performance acrossa range of domains.
The results show that such anapproach has the potential of doing quite well forsome domains, but much less well for others.
Re-sults also show that for some domains, quite mod-est amounts of training data are needed for thisoperation.
Applying this method across corporafrom different domains can also give us a simi-larity metric for dialogue domains.
Our hope isthat this kind of analysis can help inform the de-cision of what kind of language processing meth-ods and dialogue architectures are most appropri-ate for building a dialogue system for a new do-main, particularly one in which the system is toact like a human.AcknowledgmentsThis work has been sponsored by the U.S. Army Re-search, Development, and Engineering Command (RDE-COM).
Statements and opinions expressed do not necessarilyreflect the position or the policy of the United States Gov-ernment, and no official endorsement should be inferred.
Wewould like to thank Ron Artstein and others at ICT for com-piling the ICT Corpora used in this study.ReferencesR.
Artstein, S. Gandhe, J. Gerten, A. Leuski, and D. Traum.2009.
Semi-formal evaluation of conversational charac-ters.
In Languages: From Formal to Natural.
Essays Ded-icated to Nissim Francez on the Occasion of His 65thBirthday, volume 5533 of LNCS.
Springer.S.
Gandhe and D. Traum.
2008.
Evaluation understudy fordialogue coherence models.
In Proc.
of SIGdial 08.J.
J. Godfrey, E. C. Holliman, and J. McDaniel.
1992.Switchboard: Telephone speech corpus for research anddevelopment.
In Proc.
of ICASSP-92, pages 517?520.P.
A. Heeman and J. Allen.
1994.
The TRAINS 93 dialogues.TRAINS Technical Note 94-2, Department of ComputerScience, University of Rochester.P.
Kenny, T. Parsons, J. Gratch, A. Leuski, and A. Rizzo.2007.
Virtual patients for clinical therapist skills training.In Proc.
of IVA 07, Paris, France.
Springer.A.
Lavie and M. J. Denkowski.
2009.
The meteor metricfor automatic evaluation of machine translation.
MachineTranslation, 23:105?115.A.
Leuski, R. Patel, D. Traum, and B. Kennedy.
2006.
Build-ing effective question answering characters.
In Proc.
ofSIGdial 06, pages 18?27, Sydney, Australia.L.
Sellberg and A. Jo?nsson.
2008.
Using random indexing toimprove singular value decomposition for latent semanticanalysis.
In Proc.
of LREC?08, Morocco.A.
J. Stent.
2000.
The monroe corpus.
Technical Report728, Computer Science Dept.
University of Rochester.X.
Yao, P. Bhutada, K. Georgila, K. Sagae, R. Artstein, andD.
Traum.
2010.
Practical evaluation of speech recogniz-ers for virtual human dialogue systems.
In LREC 2010.I.
Zukerman and Y. Marom.
2006.
A corpus-based approachto help-desk response generation.
In CIMCA/IAWTIC ?06.248
