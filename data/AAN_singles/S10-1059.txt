Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 264?267,Uppsala, Sweden, 15-16 July 2010.c?2010 Association for Computational LinguisticsSEMAFOR: Frame Argument Resolution with Log-Linear ModelsDesai Chen Nathan Schneider Dipanjan Das Noah A. SmithSchool of Computer Science, Carnegie Mellon University, Pittsburgh, PA 15213, USA{desaic@andrew,dipanjan@cs,nschneid@cs,nasmith@cs}.cmu.eduAbstractThis paper describes the SEMAFOR sys-tem?s performance in the SemEval 2010task on linking events and their partici-pants in discourse.
Our entry is basedupon SEMAFOR 1.0 (Das et al, 2010a),a frame-semantic probabilistic parser builtfrom log-linear models.
The extended sys-tem models null instantiations, includingnon-local argument reference.
Performanceis evaluated on the task data with and with-out gold-standard overt arguments.
In bothsettings, it fares the best of the submittedsystems with respect to recall and F1.1 IntroductionThe theory of frame semantics (Fillmore, 1982)holds that meaning is largely structured by holis-tic units of knowledge, called frames.
Each frameencodes a conventionalized gestalt event or sce-nario, often with conceptual dependents (partic-ipants, props, or attributes) filling roles to elab-orate the specific instance of the frame.
In theFrameNet lexicon (Fillmore et al, 2003), eachframe defines core roles tightly coupled withthe particular meaning of the frame, as well asmore generic non-core roles (Ruppenhofer et al,2006).
Frames can be evoked with linguistic pred-icates, known as lexical units (LUs); role fillerscan be expressed overtly and linked to the framevia (morpho)syntactic constructions.
However, agreat deal of conceptually-relevant content is leftunexpressed or is not explicitly linked to the framevia linguistic conventions; rather, it is expectedthat the listener will be able to infer the appro-priate relationships pragmatically.
Certain typesof implicit content and implicit reference are for-malized in the theory of null instantiations (NIs)(Fillmore, 1986; Ruppenhofer, 2005).
A completeframe-semantic analysis of text thus incorporatescovert and overt predicate-argument information.In this paper, we describe a system for frame-semantic analysis, evaluated on a semantic rolelabeling task for explicit and implicit arguments(?2).
Extending the SEMAFOR 1.0 frame-semantic parser (Das et al, 2010a; outlined in ?3),we detect null instantiations via a simple two-stagepipeline: the first stage predicts whether a givenrole is null-instantiated, and the second stage (?4)predicts how it is null-instantiated, if it is not overt.We report performance on the SemEval 2010 testset under the full-SRL and NI-only conditions.2 DataThe SemEval 2007 task on frame-semantic pars-ing (Baker et al, 2007) provided a small (about50,000 words and 2,000 sentences) dataset ofnews text, travel guides, and bureaucratic accountsof weapons stockpiles.
Sentences in this datasetwere fully annotated with frames and their argu-ments.
The SemEval 2010 task (Ruppenhofer etal., 2010) adds annotated data in the fiction do-main: parts of two Sherlock Holmes stories byArthur Conan Doyle.
The SemEval 2010 train-ing set consists of the SemEval 2007 data plusone document from the new domain.
This doc-ument has about 7800 words in 438 sentences;it has 1492 annotated frame instances, including3169 (overt and null-instantiated) argument anno-tations.
The test set consists of two chapters fromanother story: Chapter 13 contains about 4000words, 249 sentences, and 791 frames; Chapter 14contains about 5000 words, 276 sentences, and941 frames (see also Table 3).
Figure 1 showstwo annotated test sentences.
All data released forthe 2010 task include part-of-speech tags, lemmas,and phrase-structure trees from a parser, with headannotations for constituents.3 Argument identificationOur starting point is SEMAFOR 1.0 (Das etal., 2010a), a discriminative probabilistic frame-semantic parsing model that operates in threestages: (a) rule-based target selection, (b) proba-bilistic disambiguation that resolves each target toa FrameNet frame, and (c) joint selection of textspans to fill the roles of each target through a sec-ond probabilistic model.11Das et al (2010a) report the performance of this systemon the complete SemEval 2007 task at 46.49% F1.264`` I         THINK   that I shall be in a position to     MAKE   the situation rather   more   CLEAR to you before long .It has been an exceedingly DIFFICULT and most complicated  business .DIFFICULTYdifficult.aDegreeActivityOPINIONthink.vCAUSATIONmake.vOBVIOUSNESSclear.nExperiencerCognizer OpinionActor EffectExperiencerDegree AttributePhenomenonFigure 1.
Two consecutive sentencesin the test set, with frame-semantic an-notations.
Shaded regions representframes: they include the target word inthe sentence, the corresponding framename and lexical unit, and arguments.Horizontal bars mark gold argumentspans?white bars are gold annotationsand black bars show mistakes of ourNI-only system.Chapter 13 Chapter 14Training Data Prec.
Rec.
F1Prec.
Rec.
F1SemEval 2010 data (includes SemEval 2007 data) 0.69 0.50 0.58 0.66 0.48 0.56SemEval 2007 data + 50% new, in-domain data 0.68 0.47 0.55 0.66 0.45 0.54SemEval 2007 data only 0.67 0.41 0.50 0.64 0.40 0.50Table 1.
Overtargument labelingperformance.Stage (c), known as argument identification orSRL, is most relevant here.
In this step, the systemtakes the target (frame-evoking) phrase t and cor-responding frame type f predicted by the previousstages, and independently fills each role of f witha word or phrase from the sentence, or the sym-bol OTHER to indicate that the role has no (local)overt argument.
Features used to inform this de-cision include aspects of the syntactic dependencyparse (e.g.
the path in the parse from the targetto the argument); voice; word overlap of the argu-ment with respect to the target; and part-of-speechtags within and around the argument.
SEMAFORas described in (Das et al, 2010a) does not dis-tinguish between different types of null instantia-tions or find non-local referents.
Given perfectinput to stage (c), the system achieved 68.5% F1on the SemEval 2007 data (exact match, evaluat-ing overt arguments only).
The only differencein our use of SEMAFOR?s argument identificationmodule is in preprocessing the training data: weuse dependency parses transformed from the head-augmented phrase-structure parses in the task data.Table 1 shows the performance of our argumentidentification model on this task?s test data.
TheSRL systems compared in (Ruppenhofer et al,2010) all achieved precision in the mid 60% range,but SEMAFOR achieved substantially higher re-call, F1, and label accuracy on this subtask.
(Thetable also shows how performance of our modeldegrades when half or all of the new data are notused for training; the 9% difference in recall sug-gests the importance of in-domain training data.
)4 Null instantiation detectionIn this subtask, which follows the argument iden-tification subtask (?3), our system seeks to char-acterize non-overt core roles given gold standardlocal frame-argument annotations.
Consider thefollowing passage from the test data:?That?s lucky for him?in fact, it?s lucky for allof you, since you are all on the wrong side of thelaw in this matter.
I am not sure that as a consci-entious detective [Authoritiesmy] first duty is not toarrest [Suspectthe whole household].
[DNICharges?
]The frame we are interested in, ARREST, has fourcore roles, two of which (Authorities and Sus-pect) have overt (local) arguments.
The third corerole, Charges, is annotated as having anaphoricor definite null instantiation (DNI).
?Definite?means that the discourse implies a specific referentthat should be recoverable from context, withoutmarking that referent linguistically.
Some DNIs inthe data are linked to phrases in syntactically non-local positions, such as in another sentence (seeFigure 1).
This one is not (though our model in-correctly labels this matter from the previous sen-tence as a DNI referent for this role).
The fourthcore role, Offense, is not annotated as a null in-stantiation because it belongs to the same CoreSetas Charges?which is to say they are relevant ina similar way to the frame as a whole (both pertainto the rationale for the arrest) and only one is typ-ically expressed.2We will use the term maskedto refer to any non-overt core role which does notneed to be specified as null-instantiated due to astructural connection to another role in its frame.The typology of NIs given in Ruppenhofer(2005) and employed in the annotation distin-guishes anaphoric/definite NIs from existential orindefinite null instantiations (INIs).
Rather thanhaving a specific referent accessible in the dis-course, INIs are left vague or deemphasized, as in2If the FrameNet lexicon marks a pair of roles within aframe as being in a CoreSet or Excludes relationship, thenfilling one of them satisfies the requirement that the other be(expressly or implicitly) present in the use of the frame.265Chapter 13 Chapter 14Training Data Prec.
Rec.
F1Prec.
Rec.
F1NI-onlySemEval 2010 new: 100% 0.40 0.64 0.50 0.53 0.60 0.56SemEval 2010 new: 75% 0.66 0.37 0.50 0.70 0.37 0.48SemEval 2010 new: 50% 0.73 0.38 0.51 0.75 0.35 0.48Full All 0.35 0.55 0.43 0.56 0.49 0.52Table 2.
Performance on thefull task and the NI-only task.The NI model was trained on thenew SemEval 2010 document, ?TheTiger of San Pedro?
(data from the2007 task was excluded becausenone of the null instantiations in thatdata had annotated referents).Predictedovert DNI INI masked inc. totalGoldovert 2068 (1630) 5 362 327 0 2762DNI 64 12 (3) 182 90 0 348INI 41 2 214 96 0 353masked 73 0 240 1394 0 1707inc.
12 2 55 2 0 71total 2258 21 1053 1909 0 3688 correctTable 3.
Instantiation type confusion ma-trix for the full model (argument identifi-cation plus NI detection).
Parenthesizednumbers count the predictions of the cor-rect type which also predicted the same(argument or referent) span.
On the NI-only task, our system has a similar distri-bution of NI detection errors.the thing(s) eaten in the sentence We ate.The problem can be decomposed into two steps:(a) classifying each null instantiation as definite,indefinite, or masked; and (b) resolving the DNIs,which entails finding referents in the non-localcontext.
Instead, our model makes a single NI pre-diction for any role that received no local argument(OTHER) in the argument identification phase (?3),thereby combining classification and resolution.34.1 ModelOur model for this subtask is analogous to the ar-gument identification model: it chooses one fromamong many possible fillers for each role.
How-ever, whereas the argument identification modelconsiders parse constituents as potential localfillers (which might constitute an overt argumentwithin the sentence) along with a special category,OTHER, here the set of candidate fillers consists ofphrases from outside the sentence, along with spe-cial categories INI or MASKED.
When selected, anon-local phrase will be interpreted as a non-localargument and labeled as a DNI referent.These non-local candidate fillers are handleddifferently from candidates within the sentenceconsidered in the argument identification model:they are selected using more restrictive criteria,and are associated with a different set of features.Restricted search space for DNI referents.
Weconsider nouns, pronouns, and noun phrases fromthe previous three sentences as candidate DNI ref-erents.
This narrows the search space considerablyto make learning tractable, but at a cost: manygold DNI referents will not even be considered.In the training data, there are about 250 DNI in-stances with explicit referents; their distribution is3Investigation of separate modeling is left to future work.chaotic.4Judging by the training data, our heuris-tics thus limit oracle recall to about 20% of DNIs.5Modified feature set.
Since it is not obvious howto calculate a syntactic path between two wordsin different sentences, we replaced dependencypath features with simpler features derived fromFrameNet?s lexicographic exemplar annotations.For each candidate span, we use two types of fea-tures to model the affinity between the head wordand the role.
The first indicates whether the headword is used as a filler for this role in at leastone of the lexicographic exemplars.
The secondencodes the maximum distributional similarity toany word heading a filler of that role in the ex-emplars.6In practice, we found that these fea-tures received negligible weight and had virtuallyno effect on performance, possibly due to datasparseness.
An additional change in the featureset is that ordering/distance features (Das et al,2010b, p. 13) were replaced with a feature indicat-ing the number of sentences away the candidateis from the target.7Otherwise, the null identifica-491 DNI referents are found no more than three sentencesprior; another 90 are in the same sentence as the target.
20DNIs have referents which are not noun phrases.
Six appearafter the sentence containing its frame target; 28 appear atleast 25 sentences prior.
60 have no referent.5Our system ignores DNIs with no referent or with a ref-erent in the same sentence as the target.
Experiments withvariants on these assumptions show that the larger the searchspace (i.e.
the more candidate DNI referents are under con-sideration), the worse the trained model performs at distin-guishing NIs from non-NIs (though DNI vs. INI precisionimproves).
This suggests that data sparseness is hinderingour system?s ability to learn useful generalizations about NIs.6Distributional similarity scores are obtainedfrom D. Lin?s Proximity-based Thesaurus (http://webdocs.cs.ualberta.ca/~lindek/Downloads/sims.lsp.gz) and quantized into bi-nary features for intervals: [0, .03), [.03, .06), [.06, .08),[.08,?
).7All of the new features are instantiated in three forms:266tion model uses the same features as the argumentidentification model.The theory of null instantiations holds that thegrammaticality of lexically-licensed NI for a rolein a given frame depends on the LU: for exam-ple, the verbs buy and sell share the same framebut differ as to whether the Buyer or Seller rolemay be lexically null-instantiated.
Our model?sfeature set is rich enough to capture this in a softway, with lexicalized features that fire, e.g., whenthe Seller role is null-instantiated and the targetis buy.
Moreover, (Ruppenhofer, 2005) hypoth-esizes that each role has a strong preference forone interpretation (INI or DNI) when it is lexicallynull-instantiated, regardless of LU.
This, too, ismodeled in our feature set.
In theory these trendsshould be learnable given sufficient data, though itis doubtful that there are enough examples of nullinstantiations in the currently available dataset forthis learning to take place.4.2 EvaluationWe trained the model on the non-overt argumentsin the new SemEval 2010 training document,which has 580 null instantiations?303 DNIs and277 INIs.8,9Then we used the task scoring proce-dure to evaluate the NI detection subtask in isola-tion (given gold-standard overt arguments) as wellas the full task (when this module is combined in apipeline with argument identification).
Results areshown in Table 2.10Table 3 provides a breakdown of our sys-tem?s predictions on the test data by instantiationtype: overt local arguments, DNIs, INIs, and theMASKED category (marking the role as redundantor irrelevant for the particular use of the frame,given the other arguments).
It also shows countsfor incorporated (?inc.?)
roles, which are filled bythe frame-evoking target, e.g.
clear in Figure 1.11This table shows that the system is reasonably ef-fective at discriminating NIs from masked roles,one specific to the frame and the role, one specific to the rolename only, and one to learn the overall bias of the data.8For feature engineering we held out the last 25% of sen-tences from the new training document as development data,retraining on the full training set for final evaluation.9We used Nils Reiter?s FrameNet API, version 0.4(http://www.cl.uni-heidelberg.de/trac/FrameNetAPI) in processing the data.10The other system participating in the NI-only subtaskhad much lower NI recall of 8% (Ruppenhofer et al, 2010).11We do not predict any DNIs without referents or in-corporated roles, though the evaluation script gives us creditwhen we predict INI for these cases.but DNI identification suffers from low recall andINI identification from low precision.
Data sparse-ness is likely the biggest obstacle here.
To put thisin perspective, there are over 20,000 training ex-amples of overt arguments, but fewer than 600 ex-amples of null instantiations, two thirds of whichdo not have referents.
Without an order of mag-nitude more NI data (at least), it is unlikely thata supervised learner could generalize well enoughto recognize on new data null instantiations of theover 7000 roles in the lexicon.5 ConclusionWe have described a system that implements aclean probabilistic model of frame-semantic struc-ture, considering overt arguments as well as var-ious forms of null instantion of roles.
The sys-tem was evaluated on SemEval 2010 data, withmixed success at detecting null instantiations.
Webelieve in-domain data sparseness is the predom-inant factor limiting the robustness of our super-vised model.AcknowledgmentsThis work was supported by DARPA grantNBCH-1080004 and computational resourcesprovided by Yahoo.
We thank the task organizers forproviding data and conducting the evaluation, and tworeviewers for their comments.ReferencesC.
Baker, M. Ellsworth, and K. Erk.
2007.
SemEval-2007Task 19: Frame Semantic Structure Extraction.
In Proc.of SemEval.D.
Das, N. Schneider, D. Chen, and N. A. Smith.
2010a.Probabilistic frame-semantic parsing.
In Proc.
of NAACL-HLT.D.
Das, N. Schneider, D. Chen, and N. A. Smith.
2010b.SEMAFOR 1.0: A probabilistic frame-semantic parser.Technical Report CMU-LTI-10-001, Carnegie MellonUniversity.C.
J. Fillmore, C. R. Johnson, and M. R.L.
Petruck.
2003.Background to FrameNet.
International Journal of Lexi-cography, 16(3).C.
J. Fillmore.
1982.
Frame semantics.
In Linguistics in theMorning Calm, pages 111?137.
Hanshin Publishing Co.,Seoul, South Korea.C.
J. Fillmore.
1986.
Pragmatically controlled zeroanaphora.
In Proc.
of Berkeley Linguistics Society, pages95?107, Berkeley, CA.J.
Ruppenhofer, M. Ellsworth, M. R.L.
Petruck, C. R. John-son, and J. Scheffczyk.
2006.
FrameNet II: extended the-ory and practice.J.
Ruppenhofer, C. Sporleder, R. Morante, C. Baker, andM.
Palmer.
2010.
SemEval-2010 Task 10: LinkingEvents and Their Participants in Discourse.
In Proc.
ofSemEval.J.
Ruppenhofer.
2005.
Regularities in null instantiation.267
