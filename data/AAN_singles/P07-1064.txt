Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 504?511,Prague, Czech Republic, June 2007. c?2007 Association for Computational LinguisticsMaking Sense of Sound:Unsupervised Topic Segmentation over Acoustic InputIgor Malioutov, Alex Park, Regina Barzilay, and James GlassMassachusetts Institute of Technology{igorm,malex,regina,glass}@csail.mit.eduAbstractWe address the task of unsupervised topicsegmentation of speech data operating overraw acoustic information.
In contrast to ex-isting algorithms for topic segmentation ofspeech, our approach does not require in-put transcripts.
Our method predicts topicchanges by analyzing the distribution of re-occurring acoustic patterns in the speech sig-nal corresponding to a single speaker.
Thealgorithm robustly handles noise inherent inacoustic matching by intelligently aggregat-ing information about the similarity profilefrom multiple local comparisons.
Our ex-periments show that audio-based segmen-tation compares favorably with transcript-based segmentation computed over noisytranscripts.
These results demonstrate thedesirability of our method for applicationswhere a speech recognizer is not available,or its output has a high word error rate.1 IntroductionAn important practical application of topic segmen-tation is the analysis of spoken data.
Paragraphbreaks, section markers and other structural cuescommon in written documents are entirely missingin spoken data.
Insertion of these structural markerscan benefit multiple speech processing applications,including audio browsing, retrieval, and summariza-tion.Not surprisingly, a variety of methods fortopic segmentation have been developed in thepast (Beeferman et al, 1999; Galley et al, 2003;Dielmann and Renals, 2005).
These methods typi-cally assume that a segmentation algorithm has ac-cess not only to acoustic input, but also to its tran-script.
This assumption is natural for applicationswhere the transcript has to be computed as part of thesystem output, or it is readily available from othersystem components.
However, for some domainsand languages, the transcripts may not be available,or the recognition performance may not be adequateto achieve reliable segmentation.
In order to processsuch data, we need a method for topic segmentationthat does not require transcribed input.In this paper, we explore a method for topic seg-mentation that operates directly on a raw acousticspeech signal, without using any input transcripts.This method predicts topic changes by analyzing thedistribution of reoccurring acoustic patterns in thespeech signal corresponding to a single speaker.
Inthe same way that unsupervised segmentation algo-rithms predict boundaries based on changes in lexi-cal distribution, our algorithm is driven by changesin the distribution of acoustic patterns.
The centralhypothesis here is that similar sounding acoustic se-quences produced by the same speaker correspondto similar lexicographic sequences.
Thus, by ana-lyzing the distribution of acoustic patterns we couldapproximate a traditional content analysis based onthe lexical distribution of words in a transcript.Analyzing high-level content structure based onlow-level acoustic features poses interesting compu-tational and linguistic challenges.
For instance, weneed to handle the noise inherent in matching basedon acoustic similarity, because of possible varia-504tions in speaking rate or pronunciation.
Moreover,in the absence of higher-level knowledge, informa-tion about word boundaries is not always discerniblefrom the raw acoustic input.
This causes problemsbecause we have no obvious unit of comparison.
Fi-nally, noise inherent in the acoustic matching pro-cedure complicates the detection of distributionalchanges in the comparison matrix.The algorithm presented in this paper demon-strates the feasibility of topic segmentation over rawacoustic input corresponding to a single speaker.
Wefirst apply a variant of the dynamic time warping al-gorithm to find similar fragments in the speech inputthrough alignment.
Next, we construct a compari-son matrix that aggregates the output of the align-ment stage.
Since aligned utterances are separatedby gaps and differ in duration, this representationgives rise to sparse and irregular input.
To obtain ro-bust similarity change detection, we invoke a seriesof transformations to smooth and refine the compar-ison matrix.
Finally, we apply the minimum-cut seg-mentation algorithm to the transformed comparisonmatrix to detect topic boundaries.We compare the performance of our methodagainst traditional transcript-based segmentation al-gorithms.
As expected, the performance of the lat-ter depends on the accuracy of the input transcript.When a manual transcription is available, the gapbetween audio-based segmentation and transcript-based segmentation is substantial.
However, ina more realistic scenario when the transcripts arefraught with recognition errors, the two approachesexhibit similar performance.
These results demon-strate that audio-based algorithms are an effectiveand efficient solution for applications where tran-scripts are unavailable or highly errorful.2 Related WorkSpeech-based Topic Segmentation A variety ofsupervised and unsupervised methods have beenemployed to segment speech input.
Some of thesealgorithms have been originally developed for pro-cessing written text (Beeferman et al, 1999).
Othersare specifically adapted for processing speech inputby adding relevant acoustic features such as pauselength and speaker change (Galley et al, 2003; Diel-mann and Renals, 2005).
In parallel, researchers ex-tensively study the relationship between discoursestructure and intonational variation (Hirschberg andNakatani, 1996; Shriberg et al, 2000).
However,all of the existing segmentation methods require asinput a speech transcript of reasonable quality.
Incontrast, the method presented in this paper doesnot assume the availability of transcripts, which pre-vents us from using segmentation algorithms devel-oped for written text.At the same time, our work is closely related tounsupervised approaches for text segmentation.
Thecentral assumption here is that sharp changes in lex-ical distribution signal the presence of topic bound-aries (Hearst, 1994; Choi et al, 2001).
These ap-proaches determine segment boundaries by identi-fying homogeneous regions within a similarity ma-trix that encodes pairwise similarity between textualunits, such as sentences.
Our segmentation algo-rithm operates over a distortion matrix, but the unitof comparison is the speech signal over a time in-terval.
This change in representation gives rise tomultiple challenges related to the inherent noise ofacoustic matching, and requires the development ofnew methods for signal discretization, interval com-parison and matrix analysis.Pattern Induction in Acoustic Data Our workis related to research on unsupervised lexical acqui-sition from continuous speech.
These methods aimto infer vocabulary from unsegmented audio streamsby analyzing regularities in pattern distribution (deMarcken, 1996; Brent, 1999; Venkataraman, 2001).Traditionally, the speech signal is first converted intoa string-like representation such as phonemes andsyllables using a phonetic recognizer.Park and Glass (2006) have recently shown thefeasibility of an audio-based approach for word dis-covery.
They induce the vocabulary from the au-dio stream directly, avoiding the need for phonetictranscription.
Their method can accurately discoverwords which appear with high frequency in the au-dio stream.
While the results obtained by Park andGlass inspire our approach, we cannot directly usetheir output as proxies for words in topic segmen-tation.
Many of the content words occurring onlya few times in the text are pruned away by thismethod.
Our results show that this data that is toosparse and noisy for robustly discerning changes in505lexical distribution.3 AlgorithmThe audio-based segmentation algorithm identifiestopic boundaries by analyzing changes in the dis-tribution of acoustic patterns.
The analysis is per-formed in three steps.
First, we identify recurringpatterns in the audio stream and compute distortionbetween them (Section 3.1).
These acoustic patternscorrespond to high-frequency words and phrases,but they only cover a fraction of the words that ap-pear in the input.
As a result, the distributional pro-file obtained during this process is too sparse to de-liver robust topic analysis.
Second, we generate anacoustic comparison matrix that aggregates infor-mation from multiple pattern matches (Section 3.2).Additional matrix transformations during this stepreduce the noise and irregularities inherent in acous-tic matching.
Third, we partition the matrix to iden-tify segments with a homogeneous distribution ofacoustic patterns (Section 3.3).3.1 Comparing Acoustic PatternsGiven a raw acoustic waveform, we extract a set ofacoustic patterns that occur frequently in the speechdocument.
Continuous speech includes many wordsequences that lack clear low-level acoustic cues todenote word boundaries.
Therefore, we cannot per-form this task through simple counting of speechsegments separated by silence.
Instead, we use a lo-cal alignment algorithm to search for similar speechsegments and quantify the amount of distortion be-tween them.
In what follows, we first present a vec-tor representation used in this computation, and thenspecify the alignment algorithm that finds similarsegments.MFCC Representation We start by transformingthe acoustic signal into a vector representation thatfacilitates the comparison of acoustic sequences.First, we perform silence detection on the originalwaveform by registering a pause if the energy fallsbelow a certain threshold for a duration of 2s.
Thisenables us to break up the acoustic stream into con-tinuous spoken utterances.This step is necessary as it eliminates spuriousalignments between silent regions of the acousticwaveform.
Note that silence detection is not equiv-alent to word boundary detection, as segmentationby silence detection alone only accounts for 20% ofword boundaries in our corpus.Next, we convert each utterance into a time se-ries of vectors consisting of Mel-scale cepstral co-efficients (MFCCs).
This compact low-dimensionalrepresentation is commonly used in speech process-ing applications because it approximates human au-ditory models.The process of extracting MFCCs from the speechsignal can be summarized as follows.
First, the 16kHz digitized audio waveform is normalized by re-moving the mean and scaling the peak amplitude.Next, the short-time Fourier transform is taken ata frame interval of 10 ms using a 25.6 ms Ham-ming window.
The spectral energy from the Fouriertransform is then weighted by Mel-frequency fil-ters (Huang et al, 2001).
Finally, the discrete cosinetransform of the log of these Mel-frequency spec-tral coefficients is computed, yielding a series of 14-dimensional MFCC vectors.
We take the additionalstep of whitening the feature vectors, which normal-izes the variance and decorrelates the dimensions ofthe feature vectors (Bishop, 1995).
This whitenedspectral representation enables us to use the stan-dard unweighted Euclidean distance metric.
Afterthis transformation, the distances in each dimensionwill be uncorrelated and have equal variance.Alignment Now, our goal is to identify acousticpatterns that occur multiple times in the audio wave-form.
The patterns may not be repeated exactly, butwill most likely reoccur in varied forms.
We capturethis information by extracting pairs of patterns withan associated distortion score.
The computation isperformed using a sequence alignment algorithm.Table 1 shows examples of alignments automati-cally computed by our algorithm.
The correspond-ing phonetic transcriptions1 demonstrate that thematching procedure can robustly handle variationsin pronunciations.
For example, two instances of theword ?direction?
are matched to one another despitedifferent pronunciations, (?d ay?
vs. ?d ax?
in thefirst syllable).
At the same time, some aligned pairsform erroneous matches, such as ?my prediction?matching ?y direction?
due to their high acoustic1Phonetic transcriptions are not used by our algorithm andare provided for illustrative purposes only.506Aligned Word(s) Phonetic Transcriptionthe x direction dh iy eh kcl k s dcl d ax r eh kcl sh ax nD iy Ek^k s d^d @r Ek^S@nthe y direction dh ax w ay dcl d ay r eh kcl sh epi enD @w ay d^ay r Ek^k S@nof my prediction ax v m ay kcl k r iy l iy kcl k sh ax n@v m ay k^k r iy l iy k^k S@nacceleration eh kcl k s eh l ax r ey sh epi enEk^k s El @r Ey S- n"acceleration ax kcl k s ah n ax r eh n epi sh epi en@k^k s 2n @r En - S- n"the derivation dcl d ih dx ih z dcl dh ey sh epi end^d IRIz d^D Ey S- n"a demonstration uh dcl d eh m ax n epi s tcl t r ey sh enUd^d Em @n - s t^t r Ey Sn"Table 1: Aligned Word Paths.
Each group of rowsrepresents audio segments that were aligned to oneanother, along with their corresponding phonetictranscriptions using TIMIT conventions (Garofolo etal., 1993) and their IPA equivalents.similarity.The alignment algorithm operates on the audiowaveform represented by a list of silence-free utter-ances (u1, u2, .
.
.
, un).
Each utterance u?
is a timeseries of MFCC vectors ( ~x?1, ~x?2, .
.
.
, ~x?m).
Giventwo input utterances u?
and u?
?, the algorithm out-puts a set of alignments between the correspondingMFCC vectors.
The alignment distortion score iscomputed by summing the Euclidean distances ofmatching vectors.To compute the optimal alignment we use a vari-ant of the dynamic time warping algorithm (Huanget al, 2001).
For every possible starting alignmentpoint, we optimize the following dynamic program-ming objective:D(ik, jk) = d(ik, jk) + min????
?D(ik ?
1, jk)D(ik, jk ?
1)D(ik ?
1, jk ?
1)In the equation above, ik and jk are alignment end-points in the k-th subproblem of dynamic program-ming.This objective corresponds to a descent througha dynamic programming trellis by choosing right,down, or diagonal steps at each stage.During the search process, we consider not onlythe alignment distortion score, but also the shape ofthe alignment path.
To limit the amount of temporalwarping, we enforce the following constraint:??
(ik ?
i1)?
(jk ?
j1)??
?
R,?k, (1)ik ?
Nx and jk ?
Ny,where Nx and Ny are the number of MFCC samplesin each utterance.
The value 2R + 1 is the width ofthe diagonal band that controls the extent of tempo-ral warping.
The parameter R is tuned on a develop-ment set.This alignment procedure may produce paths withhigh distortion subpaths.
Therefore, we trim eachpath to retain the subpath with lowest average dis-tortion and length at least L. More formally, givenan alignment of length N , we seek to find m and nsuch that:arg min1?m?n?N1n ?
m + 1n?k=md(ik, jk) n?m ?
LWe accomplish this by computing the length con-strained minimum average distortion subsequenceof the path sequence using an O(N log(L)) algo-rithm proposed by Lin et al(2002).
The lengthparameter, L, allows us to avoid overtrimming andcontrol the length of alignments that are found.
Af-ter trimming, the distortion of each alignment pathis normalized by the path length.Alignments with a distortion exceeding a prespec-ified threshold are pruned away to ensure that thealigned phrasal units are close acoustic matches.This parameter is tuned on a development set.In the next section, we describe how to aggregateinformation from multiple noisy matches into a rep-resentation that facilitates boundary detection.3.2 Construction of Acoustic ComparisonMatrixThe goal of this step is to construct an acoustic com-parison matrix that will guide topic segmentation.This matrix encodes variations in the distribution ofacoustic patterns for a given speech document.
Weconstruct this matrix by first discretizing the acousticsignal into constant-length blocks and then comput-ing the distortion between pairs of blocks.507Figure 1: a) Similarity matrix for a Physics lecture constructed using a manual transcript.
b) Similaritymatrix for the same lecture constructed from acoustic data.
The intensity of a pixel indicates the degree ofblock similarity.
c) Acoustic comparison matrix after 2000 iterations of anisotropic diffusion.
Vertical linescorrespond to the reference segmentation.Unfortunately, the paths and distortions generatedduring the alignment step (Section 3.1) cannot bemapped directly to an acoustic comparison matrix.Since we compare only commonly repeated acous-tic patterns, some portions of the signal correspondto gaps between alignment paths.
In fact, in our cor-pus only 67% of the data is covered by alignmentpaths found during the alignment stage.
Moreover,many of these paths are not disjoint.
For instance,our experiments show that 74% of them overlap withat least one additional alignment path.
Finally, thesealignments vary significantly in duration, rangingfrom 0.350 ms to 2.7 ms in our corpus.Discretization and Distortion Computation Tocompensate for the irregular distribution of align-ment paths, we quantize the data by splitting the in-put signal into uniform contiguous time blocks.
Atime block does not necessarily correspond to anyone discovered alignment path.
It may contain sev-eral complete paths and also portions of other paths.We compute the aggregate distortion score D(x, y)of two blocks x and y by summing the distortions ofall alignment paths that fall within x and y.Matrix Smoothing Equipped with a block dis-tortion measure, we can now construct an acousticcomparison matrix.
In principle, this matrix can beprocessed employing standard methods developedfor text segmentation.
However, as Figure 1 illus-trates, the structure of the acoustic matrix is quitedifferent from the one obtained from text.
In a tran-script similarity matrix shown in Figure 1 a), refer-ence boundaries delimit homogeneous regions withhigh internal similarity.
On the other hand, lookingat the acoustic similarity matrix2 shown in Figure 1b), it is difficult to observe any block structure cor-responding to the reference segmentation.This deficiency can be attributed to the sparsity ofacoustic alignments.
Consider, for example, the casewhen a segment is interspersed with blocks that con-tain very few or no complete paths.
Even though therest of the blocks in the segment could be closelyrelated, these path-free blocks dilute segment homo-geneity.
This is problematic because it is not alwayspossible to tell whether a sudden shift in scores sig-nifies a transition or if it is just an artifact of irreg-ularities in acoustic matching.
Without additionalmatrix processing, these irregularities will lead thesystem astray.We further refine the acoustic comparison matrixusing anisotropic diffusion.
This technique has beendeveloped for enhancing edge detection accuracy inimage processing (Perona and Malik, 1990), and hasbeen shown to be an effective smoothing method intext segmentation (Ji and Zha, 2003).
When ap-plied to a comparison matrix, anisotropic diffusionreduces score variability within homogeneous re-2We converted the original comparison distortion matrix tothe similarity matrix by subtracting the component distortionsfrom the maximum alignment distortion score.508gions of the matrix and makes edges between theseregions more pronounced.
Consequently, this trans-formation facilitates boundary detection, potentiallyincreasing segmentation accuracy.
In Figure 1 c), wecan observe that the boundary structure in the dif-fused comparison matrix becomes more salient andcorresponds more closely to the reference segmen-tation.3.3 Matrix PartitioningGiven a target number of segments k, the goal ofthe partitioning step is to divide a matrix into ksquare submatrices along the diagonal.
This pro-cess is guided by an optimization function that max-imizes the homogeneity within a segment or mini-mizes the homogeneity across segments.
This opti-mization problem can be solved using one of manyunsupervised segmentation approaches (Choi et al,2001; Ji and Zha, 2003; Malioutov and Barzilay,2006).In our implementation, we employ the minimum-cut segmentation algorithm (Shi and Malik, 2000;Malioutov and Barzilay, 2006).
In this graph-theoretic framework, segmentation is cast as a prob-lem of partitioning a weighted undirected graphthat minimizes the normalized-cut criterion.
Theminimum-cut method achieves robust analysis byjointly considering all possible partitionings of adocument, moving beyond localized decisions.
Thisallows us to aggregate comparisons from multiplelocations, thereby compensating for the noise of in-dividual matches.4 Evaluation Set-UpData We use a publicly available3 corpus of intro-ductory Physics lectures described in our previouswork (Malioutov and Barzilay, 2006).
This mate-rial is a particularly appealing application area for anaudio-based segmentation algorithm ?
many aca-demic subjects lack transcribed data for training,while a high ratio of in-domain technical terms lim-its the use of out-of-domain transcripts.
This corpusis also challenging from the segmentation perspec-tive because the lectures are long and transitions be-tween topics are subtle.3See http://www.csail.mit.edu/?igorm/acl06.htmlThe corpus consists of 33 lectures, with an aver-age length of 8500 words and an average durationof 50 minutes.
On average, a lecture was anno-tated with six segments, and a typical segment cor-responds to two pages of a transcript.
Three lecturesfrom this set were used for development, and 30 lec-tures were used for testing.
The lectures were deliv-ered by the same speaker.To evaluate the performance of traditionaltranscript-based segmentation algorithms on thiscorpus, we also use several types of transcripts atdifferent levels of recognition accuracy.
In addi-tion to manual transcripts, our corpus contains twotypes of automatic transcripts, one obtained usingspeaker-dependent (SD) models and the other ob-tained using speaker-independent (SI) models.
Thespeaker-independent model was trained on 85 hoursof out-of-domain general lecture material and con-tained no speech from the speaker in the test set.The speaker-dependent model was trained by us-ing 38 hours of audio data from other lectures givenby the speaker.
Both recognizers incorporated wordstatistics from the accompanying class textbook intothe language model.
The word error rates for thespeaker-independent and speaker-dependent modelsare 44.9% and 19.4%, respectively.Evaluation Metrics We use the Pk and WindowD-iff measures to evaluate our system (Beeferman etal., 1999; Pevzner and Hearst, 2002).
The Pk mea-sure estimates the probability that a randomly cho-sen pair of words within a window of length k wordsis inconsistently classified.
The WindowDiff met-ric is a variant of the Pk measure, which penalizesfalse positives and near misses equally.
For both ofthese metrics, lower scores indicate better segmen-tation accuracy.Baseline We use the state-of-the-art mincut seg-mentation system by Malioutov and Barzilay (2006)as our point of comparison.
This model is an appro-priate baseline, because it has been shown to com-pare favorably with other top-performing segmenta-tion systems (Choi et al, 2001; Utiyama and Isa-hara, 2001).
We use the publicly available imple-mentation of the system.As additional points of comparison, we test theuniform and random baselines.
These correspondto segmentations obtained by uniformly placing509Pk WindowDiffMAN 0.298 0.311SD 0.340 0.351AUDIO 0.358 0.370SI 0.378 0.390RAND 0.472 0.497UNI 0.476 0.484Table 2: Segmentation accuracy for audio-basedsegmentor (AUDIO), random (RAND), uniform(UNI) and three transcript-based segmentation algo-rithms that use manual (MAN), speaker-dependent(SD) and speaker-independent (SI) transcripts.
Forall of the algorithms, the target number of segmentsis set to the reference number of segments.boundaries along the span of the lecture and select-ing random boundaries, respectively.To control for segmentation granularity, we spec-ify the number of segments in the reference segmen-tation for both our system and the baselines.Parameter Tuning We tuned the number of quan-tized blocks, the edge cutoff parameter of the min-imum cut algorithm, and the anisotropic diffusionparameters on a heldout set of three developmentlectures.
We used the same development set for thebaseline segmentation systems.5 ResultsThe goal of our evaluation experiments is two-fold.First, we are interested in understanding the condi-tions in which an audio-based segmentation is ad-vantageous over a transcript-based one.
Second, weaim to analyze the impact of various design deci-sions on the performance of our algorithm.Comparison with Transcript-Based Segmenta-tion Table 2 shows the segmentation accuracyof the audio-based segmentation algorithm and threetranscript-based segmentors on the set of 30 Physicslectures.
Our algorithm yields an average Pk mea-sure of 0.358 and an average WindowDiff mea-sure of 0.370.
This result is markedly better thanthe scores attained by uniform and random seg-mentations.
As expected, the best segmentation re-sults are obtained using manual transcripts.
How-ever, the gap between audio-based segmentationand transcript-based segmentation narrows when therecognition accuracy decreases.
In fact, perfor-mance of the audio-based segmentation beats thetranscript-based segmentation baseline obtained us-ing speaker-independent (SI) models (0.358 for AU-DIO versus Pk measurements of 0.378 for SI).Analysis of Audio-based Segmentation A cen-tral challenge in audio-based segmentation is how toovercome the noise inherent in acoustic matching.We addressed this issue by using anisotropic diffu-sion to refine the comparison matrix.
We can quan-tify the effects of this smoothing technique by gener-ating segmentations directly from the similarity ma-trix.
We obtain similarities from the distortions inthe comparison matrix by subtracting the distortionscores from the maximum distortion:S(x, y) = maxsi,sj[D(si, sj)] ?
D(x, y)Using this matrix with the min-cut algorithm, seg-mentation accuracy drops to a Pk measure of 0.418(0.450 WindowDiff).
This difference in perfor-mance shows that anisotropic diffusion compensatesfor noise introduced during acoustic matching.An alternative solution to the problem of irregu-larities in audio-based matching is to compute clus-ters of acoustically similar utterances.
Each of thederived clusters can be thought of as a unique wordtype.4 We compute these clusters, employing amethod for unsupervised vocabulary induction de-veloped by Park and Glass (2006).
Using the out-put of their algorithm, the continuous audio streamis transformed into a sequence of word-like units,which in turn can be segmented using any stan-dard transcript-based segmentation algorithm, suchas the minimum-cut segmentor.
On our corpus, thismethod achieves disappointing results ?
a Pk mea-sure of 0.423 (0.424 WindowDiff).
The result canbe attributed to the sparsity of clusters5 generated bythis method, which focuses primarily on discoveringthe frequently occurring content words.6 Conclusion and Future WorkWe presented an unsupervised algorithm for audio-based topic segmentation.
In contrast to existing4In practice, a cluster can correspond to a phrase, word, orword fragment (See Table 1 for examples).5We tuned the number of clusters on the development set.510algorithms for speech segmentation, our approachdoes not require an input transcript.
Thus, it canbe used in domains where a speech recognizer isnot available or its output is too noisy.
Our ap-proach approximates the distribution of cohesionties by considering the distribution of acoustic pat-terns.
Our experimental results demonstrate the util-ity of this approach: audio-based segmentation com-pares favorably with transcript-based segmentationcomputed over noisy transcripts.The segmentation algorithm presented in this pa-per focuses on one source of linguistic informationfor discourse analysis ?
lexical cohesion.
Multiplestudies of discourse structure, however, have shownthat prosodic cues are highly predictive of changesin topic structure (Hirschberg and Nakatani, 1996;Shriberg et al, 2000).
In a supervised framework,we can further enhance audio-based segmentationby combining features derived from pattern analy-sis with prosodic information.
We can also explorean unsupervised fusion of these two sources of in-formation; for instance, we can induce informativeprosodic cues by using distributional evidence.Another interesting direction for future researchlies in combining the results of noisy recogni-tion with information obtained from distribution ofacoustic patterns.
We hypothesize that these twosources provide complementary information aboutthe audio stream, and therefore can compensate foreach other?s mistakes.
This combination can be par-ticularly fruitful when processing speech documentswith multiple speakers or background noise.7 AcknowledgementsThe authors acknowledge the support of the Microsoft FacultyFellowship and the National Science Foundation (CAREERgrant IIS-0448168, grant IIS-0415865, and the NSF GraduateFellowship).
Any opinions, findings, conclusions or recom-mendations expressed in this publication are those of the au-thor(s) and do not necessarily reflect the views of the NationalScience Foundation.
We would like to thank T.J. Hazen forhis assistance with the speech recognizer and to acknowledgeTara Sainath, Natasha Singh, Ben Snyder, Chao Wang, LukeZettlemoyer and the three anonymous reviewers for their valu-able comments and suggestions.ReferencesD.
Beeferman, A. Berger, J. D. Lafferty.
1999.
Statistical mod-els for text segmentation.
Machine Learning, 34(1-3):177?210.C.
Bishop, 1995.
Neural Networks for Pattern Recognition,pg.
38.
Oxford University Press, New York, 1995.M.
R. Brent.
1999.
An efficient, probabilistically sound algo-rithm for segmentation and word discovery.
Machine Learn-ing, 34(1-3):71?105.F.
Choi, P. Wiemer-Hastings, J. Moore.
2001.
Latent semanticanalysis for text segmentation.
In Proceedings of EMNLP,109?117.C.
G. de Marcken.
1996.
Unsupervised Language Acquisition.Ph.D.
thesis, Massachusetts Institute of Technology.A.
Dielmann, S. Renals.
2005.
Multistream dynamic Bayesiannetwork for meeting segmentation.
In Proceedings Mul-timodal Interaction and Related Machine Learning Algo-rithms Workshop (MLMI?04), 76?86.M.
Galley, K. McKeown, E. Fosler-Lussier, H. Jing.
2003.Discourse segmentation of multi-party conversation.
In Pro-ceedings of the ACL, 562?569.J.
Garofolo, L. Lamel, W. Fisher, J. Fiscus, D. Pallet,N.
Dahlgren, V. Zue.
1993.
TIMIT Acoustic-Phonetic Con-tinuous Speech Corpus.
Linguistic Data Consortium, 1993.M.
Hearst.
1994.
Multi-paragraph segmentation of expositorytext.
In Proceedings of the ACL, 9?16.J.
Hirschberg, C. H. Nakatani.
1996.
A prosodic analysis ofdiscourse segments in direction-giving monologues.
In Pro-ceedings of the ACL, 286?293.X.
Huang, A. Acero, H.-W. Hon.
2001.
Spoken Language Pro-cessing.
Prentice Hall.X.
Ji, H. Zha.
2003.
Domain-independent text segmentationusing anisotropic diffusion and dynamic programming.
InProceedings of SIGIR, 322?329.Y.-L. Lin, T. Jiang, K.-M. Chao.
2002.
Efficient algorithmsfor locating the length-constrained heaviest segments withapplications to biomolecular sequence analysis.
J. Computerand System Sciences, 65(3):570?586.I.
Malioutov, R. Barzilay.
2006.
Minimum cut model forspoken lecture segmentation.
In Proceedings of the COL-ING/ACL, 25?32.A.
Park, J. R. Glass.
2006.
Unsupervised word acquisition fromspeech using pattern discovery.
In Proceedings of ICASSP.P.
Perona, J. Malik.
1990.
Scale-space and edge detection usinganisotropic diffusion.
IEEE Transactions on Pattern Analy-sis and Machine Intelligence, 12(7):629?639.L.
Pevzner, M. Hearst.
2002.
A critique and improvement ofan evaluation metric for text segmentation.
ComputationalLinguistics, 28(1):19?36.J.
Shi, J. Malik.
2000.
Normalized cuts and image segmenta-tion.
IEEE Transactions on Pattern Analysis and MachineIntelligence, 22(8):888?905.E.
Shriberg, A. Stolcke, D. Hakkani-Tur, G. Tur.
2000.Prosody-based automatic segmentation of speech into sen-tences and topics.
Speech Communication, 32(1-2):127?154.M.
Utiyama, H. Isahara.
2001.
A statistical model for domain-independent text segmentation.
In Proceedings of the ACL,499?506.A.
Venkataraman.
2001.
A statistical model for word dis-covery in transcribed speech.
Computational Linguistics,27(3):353?372.511
