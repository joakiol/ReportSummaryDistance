Proceedings of the Seventeenth Conference on Computational Natural Language Learning, pages 29?37,Sofia, Bulgaria, August 8-9 2013. c?2013 Association for Computational LinguisticsSupervised Morphological Segmentation in a Low-Resource LearningSetting using Conditional Random FieldsTeemu Ruokolainena Oskar Kohonena Sami Virpiojaa Mikko Kurimoba Department of Information and Computer Science, Aalto Universityb Department of Signal Processing and Acoustics, Aalto Universityfirstname.lastname@aalto.fiAbstractWe discuss data-driven morphologicalsegmentation, in which word forms aresegmented into morphs, the surface formsof morphemes.
Our focus is on a low-resource learning setting, in which only asmall amount of annotated word forms areavailable for model training, while unan-notated word forms are available in abun-dance.
The current state-of-art methods1) exploit both the annotated and unan-notated data in a semi-supervised man-ner, and 2) learn morph lexicons and sub-sequently uncover segmentations by gen-erating the most likely morph sequences.In contrast, we discuss 1) employing onlythe annotated data in a supervised man-ner, while entirely ignoring the unanno-tated data, and 2) directly learning to pre-dict morph boundaries given their localsub-string contexts instead of learning themorph lexicons.
Specifically, we em-ploy conditional random fields, a populardiscriminative log-linear model for seg-mentation.
We present experiments ontwo data sets comprising five diverse lan-guages.
We show that the fully super-vised boundary prediction approach out-performs the state-of-art semi-supervisedmorph lexicon approaches on all lan-guages when using the same annotateddata sets.1 IntroductionModern natural language processing (NLP) appli-cations, such as speech recognition, informationretrieval and machine translation, perform theirtasks using statistical language models.
For mor-phologically rich languages, estimation of the lan-guage models is problematic due to the high num-ber of compound words and inflected word forms.A successful means of alleviating this data sparsityproblem is to segment words into meaning-bearingsub-word units (Hirsim?ki et al 2006; Creutz etal., 2007; Turunen and Kurimo, 2011).
In lin-guistics, the smallest meaning-bearing units of alanguage are called morphemes and their surfaceforms morphs.
Thus, morphs are natural targetsfor the segmentation.For most languages, existing resources containlarge amounts of raw unannotated text data, onlysmall amounts of manually prepared annotatedtraining data, and no freely available rule-basedmorphological analyzers.
The focus of our work ison performing morphological segmentation in thislow-resource scenario.
Given this setting, the cur-rent state-of-art methods approach the problem bylearning morph lexicons from both annotated andunannotated data using semi-supervised machinelearning techniques (Poon et al 2009; Kohonenet al 2010).
Subsequent to model training, themethods uncover morph boundaries for new wordforms by generating their most likely morph se-quences according to the morph lexicons.In contrast to learning morph lexicons (Poon etal., 2009; Kohonen et al 2010), we study mor-phological segmentation by learning to directlypredict morph boundaries based on their local sub-string contexts.
Specifically, we apply the linear-chain conditional random field model, a populardiscriminative log-linear model for segmentationpresented originally by Lafferty et al(2001).
Im-portantly, we learn the segmentation model fromsolely the small annotated data in a supervisedmanner, while entirely ignoring the unannotateddata.
Despite not using the unannotated data, weshow that by discriminatively learning to predictthe morph boundaries, we are able to outperformthe previous state-of-art.We present experiments on Arabic and Hebrewusing the data set presented originally by Snyderand Barzilay (2008), and on English, Finnish and29Turkish using the Morpho Challenge 2009/2010data sets (Kurimo et al 2009; Kurimo et al2010).
The results are compared against two state-of-art techniques, namely the log-linear model-ing approach presented by Poon et al(2009) andthe semi-supervised Morfessor algorithm (Koho-nen et al 2010).
We show that when employ-ing the same small amount of annotated train-ing data, the CRF-based boundary prediction ap-proach outperforms these reference methods onall languages.
Additionally, since the CRF modellearns from solely the small annotated data set, itstraining is computationally much less demandingcompared to the semi-supervised methods, whichutilize both the annotated and the unannotated datasets.The rest of the paper is organized as follows.
InSection 2, we discuss related work in morpholog-ical segmentation and methodology.
In Section 3,we describe our segmentation method.
Our exper-imental setup is described in Section 4, and theobtained results are presented in Section 5.
In Sec-tion 6, we discuss the method and the results.
Fi-nally, we present conclusions on the work in Sec-tion 7.2 Related workThe CRF model has been widely used in NLP seg-mentation tasks, such as shallow parsing (Sha andPereira, 2003), named entity recognition (McCal-lum and Li, 2003), and word segmentation (Zhaoet al 2006).
Recently, CRFs were also employedsuccessfully in morphological segmentation forArabic by Green and DeNero (2012) as a com-ponent of an English to Arabic machine trans-lation system.
While the segmentation methodof Green and DeNero (2012) and ours is very sim-ilar, our focuses and contributions differ in sev-eral ways.
First, while in our work we considerthe low-resource learning setting, in which a smallannotated data set is available (up to 3,130 wordtypes), their model is trained on the Arabic Tree-bank (Maamouri et al 2004) constituting sev-eral times larger training set (588,244 word to-kens).
Second, we present empirical comparisonbetween the CRF approach and two state-of-artmethods (Poon et al 2009; Kohonen et al 2010)on five diverse languages.
Third, due to being acomponent of a larger system, their presentationon the method and experiments is rather undersp-eficied, while here we are able to provide a morethorough description.In the experimental section, we compare theCRF-based segmentation approach with two state-of-art methods, the log-linear modeling approachpresented by Poon et al(2009) and the semi-supervised Morfessor algorithm (Kohonen et al2010).
As stated previously, the CRF-based seg-mentation approach differs from these methods inthat it learns to predict morph boundaries froma small amount of annotated data, in contrast tolearning morph lexicons from both annotated andlarge amounts of unannotated data.Lastly, there exists ample work on varying un-supervised (and semi-supervised) morphologicalsegmentation methods.
A useful review is givenby Hammarstr?m and Borin (2011).
The funda-mental difference between our approach and thesetechniques is that our method necessarily requiresmanually annotated training data.3 MethodsIn this section, we describe in detail the CRF-based approach for supervised morphological seg-mentation.3.1 Morphological segmentation as aclassification taskWe represent the morphological segmentation taskas a structured classification problem by assign-ing each character to one of four classes, namely{beginning of a multi-character morph (B), mid-dle of a multi-character morph (M), end of a multi-character morph (E), single character morph (S)}.For example, consider the English word formdriverswith a corresponding segmentationdriv + er + s .Using the classification notation, this segmenta-tion is represented asSTART B M M E B E S STOP<w> d r i v e r s </w>where we have assumed additional word startand end markers <w> and </w> with respectiveclasses START and STOP.
As another example,consider the Finnish word formautoilla (with cars)with a corresponding segmentationauto + i + lla .Using the classification notation, this segmenta-tion is represented as30START B M M E S B M E STOP<w> a u t o i l l a </w>Intuitively, instead of the four class set {B, M,E, S}, a segmentation could be accomplished us-ing only a set of two classes {B, M} as in (Greenand DeNero, 2012).
However, similarly to Chi-nese word segmentation (Zhao et al 2006), ourpreliminary experiments suggested that using themore fine-grained four class set {B, M, E, S} per-formed slightly better.
This result indicates thatmorph segments of differerent lengths behave dif-ferently.3.2 Linear-chain conditional random fieldsWe perform the above structured classification us-ing linear-chain conditional random fields (CRFs),a discriminative log-linear model for tagging andsegmentation (Lafferty et al 2001).
The centralidea of the linear-chain CRF is to exploit the de-pendencies between the output variables using achain structured undirected graph, also referred toas a Markov random field, while conditioning theoutput globally on the observation.Formally, the model for input x (characters in aword) and output y (classes corresponding to char-acters) is written asp (y |x;w) ?T?t=2exp(w>f(yt?1, yt,x, t)),(1)where t indexes the characters, T denotes wordlength, w the model parameter vector, and f thevector-valued feature extracting function.The purpose of the feature extraction functionf is to capture the co-occurrence behavior of thetag transitions (yt?1, yt) and a set of features de-scribing character position t of word form x. Thestrength of the CRF model lies in its capability toutilize arbitrary, non-independent features.3.3 Feature extractionThe quality of the segmentation depends heavilyon the choice of features defined by the featureextraction function f .
We will next describe andmotivate the feature set used in the experiments.Our feature set consists of binary indicator func-tions describing the position t of word x usingall left and right substrings up to a maximumlength ?.
For example, consider the problemof deciding if the letter e in the word driversis preceded by a morph boundary.
This deci-sion is now based on the overlapping substringsto the left and right of this potential bound-ary position, that is {v, iv, riv, driv, <w>driv} and{e, er, ers, ers</w>}, respectively.
The substringsto the left and right are considered indepen-dently.
Naturally, if the maximum allowed sub-string length ?
is less than five, the longest sub-strings are discarded accordingly.
In general, theoptimum ?
depends on both the amount of avail-able training data and the language.In addition to the substring functions, we use abias function which returns value 1 independentof the input x.
The bias and substring features arecombined with all the possible tag transitions.To motivate this choice of feature set, considerformulating an intuitive segmentation rule for theEnglish words talked, played and speed with thecorrect segmentations talk + ed, play + ed andspeed, respectively.
Now, as a right context edis generally a strong indicator of a boundary, onecould first formulate a ruleposition t is a segment boundaryif its right context is ed.This rule would indeed correctly segment thewords talked and played, but would incorrectlysegment speed as spe + ed.
This error can be re-solved if the left contexts are utilized as inhibitorsby expanding the above rule asposition t is a segment boundaryif its right context is edand the left context is not spe.Using the feature set defined above, the CRFmodel can learn to perform segmentation in thisrule-like manner according to the training data.For example, using the above example words andsegmentations for training, the CRFs could learnto assign a high score for a boundary given thatthe right context is ed and a high score for a non-boundary given the left context spe.
Subsequent totraining, making segmentation decisions for newword forms can then be interpreted as voting basedon these scores.3.4 Parameter estimationThe CRF model parameters w are estimated basedon an annotated training data set.
Common train-ing criteria include the maximum likelihood (Laf-ferty et al 2001; Peng et al 2004; Zhao et al2006), averaged structured perceptron (Collins,2002), and max-margin (Szummer et al 2008).In this work, we estimate the parameters using theperceptron algorithm (Collins, 2002).31In perceptron training, the required graph infer-ence can be efficiently performed using the stan-dard Viterbi algorithm.
Subsequent to training, thesegmentations for test instances are acquired againusing Viterbi search.Compared to other training criteria, the struc-tured perceptron has the advantage of employingonly a single hyperparameter, namely the numberof passes over training data, making model esti-mation fast and straightforward.
We optimize thehyperparameter using a separate development set.Lastly, we consider the longest substring length ?a second hyperparameter optimized using the de-velopment set.4 Experimental setupThis section describes the data sets, evaluationmetrics, reference methods, and other details con-cerning the evaluation of the methods.4.1 Data setsWe evaluate the methods on two different data setscomprising five languages in total.S&B data.
The first data set we use is the He-brew Bible parallel corpus introduced by Snyderand Barzilay (2008).
It contains 6,192 parallelphrases in Hebrew, Arabic, Aramaic, and Englishand their frequencies (ranging from 5 to 3517).The phrases have been extracted using automaticword alignment.
The Hebrew and Arabic phraseshave manually annotated morphological segmen-tations, and they are used in our experiments.
Thephrases are sorted according to frequency, and ev-ery fifth phrase starting from the first phrase isplaced in the test set, every fifth starting from thesecond phrase in the development set (up to 500phrases), and the rest of the phrases in the train-ing set.
1 The total numbers of word types in thesets are shown in Table 1.
Finally, the word formsin the training set are randomly permuted, and thefirst 25%, 50%, 75%, and 100% of them are se-lected as subsets to study the effect of training datasize.MC data.
The second data set is based on theMorpho Challenge 2010 (Kurimo et al 2010).It includes manually prepared morphological seg-mentations in English, Finnish and Turkish.
The1We are grateful to Dr. Hoifung Poon for providing usinstructions for dividing of the data set.Arabic HebrewTraining 3,130 2,770Development 472 450Test 1,107 1,040Table 1: The numbers of word types in S&B datasets (Snyder and Barzilay, 2008).English Finnish TurkishUnannot.
384,903 2,206,719 617,298Training 1,000 1,000 1,000Develop.
694 835 763Test 10?1,000 10?1,000 10?1,000Table 2: The numbers of word types in the MCdata sets (Kurimo et al 2009; Kurimo et al2010).additional German corpus does not have segmen-tation annotation and is therefore excluded.
Theannotated data sets include training, development,and test sets for each language.
Following Virpi-oja et al(2011), the test set results are based onten randomly selected 1,000 word sets.
Moreover,we divide the annotated training sets into ten par-titions with respective sizes of 100, 200, .
.
.
, 1000words so that each partition is a subset of the alllarger partitions.
The data is divided so that thesmallest set had every 10th word of the originalset, the second set every 10th word and the fol-lowing word, and so forth.
For reference methodsthat require unannotated data, we use the English,Finnish and Turkish corpora from Competition 1of Morpho Challenge 2009 (Kurimo et al 2009).Table 2 shows the sizes of the MC data sets.4.2 Evaluation measuresThe word segmentations are evaluated by compar-ison with linguistic morphs using precision, recall,and F-measure.
The F-measure equals the geo-metric mean of precision (the percentage of cor-rectly assigned boundaries with respect to all as-signed boundaries) and recall (the percentage ofcorrectly assigned boundaries with respect to thereference boundaries).
While using F-measure isa standard procedure, the prior work differ at leastin three details: (1) whether precision and recallare calculated as micro-average over all segmenta-tion points or as macro-average over all the wordforms, (2) whether the evaluation is based on wordtypes or word tokens in a corpus, and (3) if the32reference segmentations have alternative correctchoices for a single word type, and how to dealwith them.For the experiments with the S&B data sets,we follow Poon et al(2009) and apply token-based micro-averages.
For the experiments withthe MC data sets, we follow Virpioja et al(2011)and use type-based macro-averages.
However, dif-fering from their boundary measure, we take thebest match over the alternative reference analyses(separately for precision and recall), since none ofthe methods considered here provide multiple seg-mentations per word type.
For the models trainedwith the full training set, we also report the F-measures of the boundary evaluation method byVirpioja et al(2011) in order to compare to theresults reported in the Morpho Challenge website.4.3 CRF feature extraction and trainingThe features included in the feature vector in theCRF model (1) are described in Section 3.3.
Weinclude all substring features which occur in thetraining data.The CRF model is trained using the averagedperceptron algorithm as described in Section 3.4.The algorithm initializes the model parameterswith zero vectors.
The model performance, mea-sured using F-measure, is evaluated on the devel-opment set after each pass over the training set,and the training is terminated when the perfor-mance has not improved during last 5 passes.
Themaximum length of substrings ?
is optimized byconsidering ?
= 1, 2, 3, .
.
.
, and the search is ter-minated when the performance has not improvedduring last 5 values.
Finally, the algorithm returnsthe parameters yielding the highest F-measure onthe development set.For some words, the MC training sets includeseveral alternative segmentations.
We resolve thisambiguity by using the first given alternative anddiscarding the rest.
During evaluation, the alter-native segmentations are taken into account as de-scribed in Section 4.2.The experiments are run on a standard desktopcomputer using our own single-threaded Python-based implementation2.4.4 Reference methodsWe compare our method?s performance on Arabicand Hebrew data with semi-supervised Morfessor2Available at http://users.ics.aalto.fi/tpruokol/(Kohonen et al 2010) and the results reported byPoon et al(2009).
On Finnish, English and Turk-ish data, we compare the method only with semi-supervised Morfessor as we have no implementa-tion of the model by Poon et al(2009).We use a recently released Python implemen-tation of semi-supervised Morfessor3.
Semi-supervised Morfessor was trained separately foreach training set size, always using the full unan-notated data sets in addition to the annotated sets.The hyperparameters, the unannotated data weight?
and the annotated data weight ?, were optimizedwith a grid search on the development set.
For theS&B data, there are no separate unannotated sets.When the annotated training set size is varied, theremaining parts are utilized as unannotated data.The log-linear model described in (Poon et al2009) and the semi-supervised Morfessor algo-rithm are later referred to as POON-2009 and S-MORFESSOR for brevity.5 ResultsMethod performances for Arabic and Hebrew onthe S&B data are presented in Tables 3 and 4, re-spectively.
The results for the POON-2009 modelare extracted from (Poon et al 2009).
Perfor-mances for English, Finnish and Turkish on theMC data set are presented in Tables 5, 6 and 7,respectively.On the Arabic and Hebrew data sets, the CRFsoutperform POON-2009 and S-MORFESSORsubstantially on all the considered data set sizes.On Finnish and Turkish data, the CRFs outper-form S-MORFESSOR except for the smallest setsof 100 instances.
On English data, the CRFs out-perform S-MORFESSOR when the training set is500 instances or larger.Using our implementation of the CRF model,obtaining the results for Arabic, Hebrew, English,Finnish, and Turkish consumed 10, 11, 22, 32,and 28 minutes, respectively.
These CPU timesinclude model training and hyperparameter opti-mization.
In comparison, S-MORFESSOR train-ing is considerably slower.
For Arabic and He-brew, the S-MORFESSOR total training timeswere 24 and 22 minutes, respectively, and for En-glish, Finnish, and Turkish 4, 22, and 10 days,respectively.
The higher training times of S-MORFESSOR are partly because of the larger3Available at https://github.com/aalto-speech/morfessor33grids in hyperparameter optimization.
Further-more, the S-MORFESSOR training time for eachgrid point grows linearly with the size of theunannotated data set, resulting in particularly slowtraining on the MC data sets.
All reported timesare total CPU times for single-threaded runs, whilein practice grid searches can be parallelized.The perceptron algorithm typically convergedafter 10 passes over the training set, and never re-quired more than 40 passes to terminate.
Depend-ing on the size of the training data, the optimizedmaximum lengths of substrings varied in ranges{3,5}, {2,7}, {3,9}, {3,6}, {3,7}, for Arabic, He-brew, English, Finnish and Turkish, respectively.Method %Lbl.
Prec.
Rec.
F1CRF 25 95.5 93.1 94.3S-MORFESSOR 25 78.7 79.7 79.2POON-2009 25 84.9 85.5 85.2CRF 50 96.5 94.6 95.5S-MORFESSOR 50 87.5 91.5 89.4POON-2009 50 88.2 86.2 87.5CRF 75 97.2 96.1 96.6S-MORFESSOR 75 92.8 83.0 87.6POON-2009 75 89.6 86.4 87.9CRF 100 98.1 97.5 97.8S-MORFESSOR 100 91.4 91.8 91.6POON-2009 100 91.7 88.5 90.0Table 3: Results for Arabic on the S&B dataset (Snyder and Barzilay, 2008).
The column ti-tled %Lbl.
denotes the percentage of the annotateddata used for training.
In addition to the given per-centages of annotated data, POON-2009 and S-MORFESSOR utilized the remainder of the dataas an unannotated set.Finally, Table 8 shows the results of the CRFand S-MORFESSOR models trained with the fullEnglish, Finnish, and Turkish MC data sets andevaluated with the boundary evaluation method ofVirpioja et al(2011).
That is, these numbers aredirectly comparable to the BPR-F column in theresult tables presented at the Morpho Challengewebsite4.
For each of the three languages, CRFclearly outperforms all the Morpho Challenge sub-missions that have provided morphological seg-mentations.4http://research.ics.aalto.fi/events/morphochallenge/Method %Lbl.
Prec.
Rec.
F1CRF 25 90.5 90.6 90.6S-MORFESSOR 25 71.5 85.3 77.8POON-2009 25 78.7 73.3 75.9CRF 50 94.0 91.5 92.7S-MORFESSOR 50 82.1 81.8 81.9POON-2009 50 82.8 74.6 78.4CRF 75 94.0 92.7 93.4S-MORFESSOR 75 84.0 88.1 86.0POON-2009 75 83.1 77.3 80.1CRF 100 94.9 94.0 94.5S-MORFESSOR 100 85.3 91.1 88.1POON-2009 100 83.0 78.9 80.9Table 4: Results for Hebrew on the S&B dataset (Snyder and Barzilay, 2008).
The column ti-tled %Lbl.
denotes the percentage of the annotateddata used for training.
In addition to the given per-centages of annotated data, POON-2009 and S-MORFESSOR utilized the remainder of the dataas an unannotated set.6 DiscussionIntuitively, the CRF-based supervised learning ap-proach should yield high segmentation accuracywhen there are large amounts of annotated train-ing data available.
However, perhaps surprisingly,the CRF model yields state-of-art results alreadyusing very small amounts of training data.
Thisresult is meaningful since for most languages it isinfeasible to acquire large amounts of annotatedtraining data.The strength of the discriminatively trainedCRF model is that overlapping, non-independentfeatures can be naturally employed.
Importantly,we showed that simple, language-independentsubstring features are sufficient for high perfor-mance.
However, adding new, task- and language-dependent features is also easy.
One might, for ex-ample, explore features capturing vowel harmonyin Finnish and Turkish.The CRFs was estimated using the structuredperceptron algorithm (Collins, 2002), which hasthe benefit of being computationally efficient andeasy to implement.
Other training criteria, suchas maximum likelihood (Lafferty et al 2001)or max-margin (Szummer et al 2008), couldalso be employed.
Similarly, other classifiers,such as the Maximum Entropy Markov Models(MEMMs) (McCallum et al 2000), are applica-ble.
However, as the amount of information in-34Method Train.
Prec.
Rec.
F1CRF 100 80.2 74.6 77.3S-MORFESSOR 100 88.1 79.7 83.7CRF 200 84.7 79.2 81.8S-MORFESSOR 200 88.1 79.5 83.6CRF 300 86.7 79.8 83.1S-MORFESSOR 300 88.4 80.6 84.3CRF 400 86.5 80.6 83.4S-MORFESSOR 400 84.6 83.6 84.1CRF 500 88.6 80.7 84.5S-MORFESSOR 500 86.3 82.7 84.4CRF 600 88.1 82.6 85.3S-MORFESSOR 600 86.7 82.5 84.5CRF 700 87.9 83.4 85.6S-MORFESSOR 700 86.0 82.9 84.4CRF 800 89.1 83.2 86.1S-MORFESSOR 800 87.1 82.5 84.8CRF 900 89.0 82.9 85.8S-MORFESSOR 900 86.4 82.6 84.5CRF 1000 89.8 83.5 86.5S-MORFESSOR 1000 88.8 80.1 84.3Table 5: Results for English on the Morpho Chal-lenge 2009/2010 data set (Kurimo et al 2009; Ku-rimo et al 2010).
The column titled Train.
de-notes the number of annotated training instances.In addition to the annotated data, S-MORFESSORutilized an unannotated set of 384,903 word types.corporated in the model would be unchanged, thechoice of parameter estimation criterion and clas-sifier is unlikely to have a dramatic effect on themethod performance.In CRF training, we focused on the supervisedlearning scenario, in which no unannotated data isexploited in addition to the annotated training sets.However, there does exist ample work on extend-ing CRF training to the semi-supervised setting(for example, see Mann and McCallum (2008)and the references therein).
Nevertheless, our re-sults strongly suggest that it is crucial to use thefew available annotated training instances as ef-ficiently as possible before turning model train-ing burdensome by incorporating large amounts ofunannotated data.Following previous work (Poon et al 2009;Kohonen et al 2010; Virpioja et al 2011), weapplied the boundary F-score evaluation measure,while Green and DeNero (2012) reported charac-ter accuracy.
We consider the boundary F-score abetter measure than accuracy, since the boundary-Method Train.
Prec.
Rec.
F1CRF 100 71.4 66.0 68.6S-MORFESSOR 100 69.8 71.0 70.4CRF 200 76.4 71.3 73.8S-MORFESSOR 200 75.5 68.6 71.9CRF 300 80.4 73.9 77.0S-MORFESSOR 300 73.1 71.8 72.5CRF 400 81.0 76.6 78.7S-MORFESSOR 400 73.3 74.3 73.8CRF 500 82.9 77.9 80.3S-MORFESSOR 500 73.5 75.1 74.3CRF 600 82.6 80.6 81.6S-MORFESSOR 600 76.1 73.7 74.9CRF 700 84.3 81.4 82.8S-MORFESSOR 700 75.0 76.6 75.8CRF 800 85.1 83.4 84.2S-MORFESSOR 800 74.1 78.2 76.1CRF 900 85.2 83.8 84.5S-MORFESSOR 900 74.2 78.5 76.3CRF 1000 86.0 84.7 85.3S-MORFESSOR 1000 74.2 78.8 76.4Table 6: Results for Finnish on the Morpho Chal-lenge 2009/2010 data set (Kurimo et al 2009; Ku-rimo et al 2010).
The column titled Train.
de-notes the number of annotated training instances.In addition to the annotated data, S-MORFESSORutilized an unannotated set of 2,206,719 wordtypes.tag distribution is strongly skewed towards non-boundaries.
Nevertheless, for completeness, wecomputed the character accuracy for our Arabicdata set, obtaining the accuracy 99.1%, which isclose to their reported accuracy of 98.6%.
How-ever, these values are not directly comparable dueto our use of the Bible corpus by Snyder and Barzi-lay (2008) and their use of the Penn Arabic Tree-bank (Maamouri et al 2004).7 ConclusionsWe have presented an empirical study in data-driven morphological segmentation employingsupervised boundary prediction methodology.Specifically, we applied conditional random fields,a discriminative log-linear model for segmentationand tagging.
From a methodological perspective,this approach differs from the previous state-of-artmethods in two fundamental aspects.
First, we uti-lize a discriminative model estimated using onlyannotated data.
Second, we learn to predict morph35Method Train.
Prec.
Rec.
F1CRF 100 72.4 79.6 75.8S-MORFESSOR 100 77.9 78.5 78.2CRF 200 83.2 82.3 82.8S-MORFESSOR 200 80.0 83.2 81.6CRF 300 83.9 85.9 84.9S-MORFESSOR 300 80.1 85.6 82.8CRF 400 86.4 86.5 86.4S-MORFESSOR 400 80.7 87.1 83.8CRF 500 87.5 86.4 87.0S-MORFESSOR 500 81.0 87.2 84.0CRF 600 87.8 88.1 87.9S-MORFESSOR 600 80.5 89.9 85.0CRF 700 89.1 88.3 88.7S-MORFESSOR 700 80.9 90.7 85.5CRF 800 88.6 90.3 89.4S-MORFESSOR 800 81.2 91.0 85.9CRF 900 89.2 89.8 89.5S-MORFESSOR 900 81.4 91.2 86.0CRF 1000 89.9 90.4 90.2S-MORFESSOR 1000 83.0 91.5 87.0Table 7: Results for Turkish on the Morpho Chal-lenge 2009/2010 data set (Kurimo et al 2009; Ku-rimo et al 2010).
The column titled Train.
de-notes the number of annotated training instances.In addition to the annotated data, S-MORFESSORutilized an unannotated set of 617,298 word types.boundaries based on their local character substringcontexts instead of learning a morph lexicon.We showed that our supervised method yieldsimproved results compared to previous state-of-art semi-supervised methods using the same smallamount of annotated data, while not utilizing theunannotated data used by the reference methods.This result has two implications.
First, supervisedmethods can provide excellent results in morpho-logical segmentation already when there are onlya few annotated training instances available.
Thisis meaningful since for most languages it is infea-sible to acquire large amounts of annotated train-ing data.
Second, performing morphological seg-mentation by directly modeling segment bound-aries can be advantageous compared to modelingmorph lexicons.A potential direction for future work includesevaluating the morphs obtained by our method inreal world applications, such as speech recognitionand information retrieval.
We are also interestedin extending the method from fully supervised toMethod English Finnish TurkishCRF 82.0 81.9 71.5S-MORFESSOR 79.6 73.5 70.5Table 8: F-measures of the Morpho Chal-lenge boundary evaluation for CRF and S-MORFESSOR using the full annotated trainingdata set.semi-supervised learning.AcknowledgementsThis work was financially supported by Langnet(Finnish doctoral programme in language studies)and the Academy of Finland under the FinnishCentre of Excellence Program 2012?2017 (grantno.
251170), project Multimodally grounded lan-guage technology (no.
254104), and LASTU Pro-gramme (nos.
256887 and 259934).ReferencesM.
Collins.
2002.
Discriminative training methodsfor hidden markov models: Theory and experimentswith perceptron algorithms.
In Proceedings of the2002 Conference on Empirical Methods in NaturalLanguage Processing (EMNLP 2002), volume 10,pages 1?8.
Association for Computational Linguis-tics.M.
Creutz, T. Hirsim?ki, M. Kurimo, A. Puurula,J.
Pylkk?nen, V. Siivola, M. Varjokallio, E. Arisoy,M.
Sara?lar, and A Stolcke.
2007.
Morph-based speech recognition and modeling of out-of-vocabulary words across languages.
ACM Transac-tions on Speech and Language Processing, 5(1):3:1?3:29, December.S.
Green and J. DeNero.
2012.
A class-basedagreement model for generating accurately inflectedtranslations.
In Proceedings of the 50th AnnualMeeting of the Association for Computational Lin-guistics: Long Papers-Volume 1, pages 146?155.Association for Computational Linguistics.H.
Hammarstr?m and L. Borin.
2011.
Unsupervisedlearning of morphology.
Computational Linguistics,37(2):309?350, June.T.
Hirsim?ki, M. Creutz, V. Siivola, M. Kurimo, S. Vir-pioja, and J. Pylkk?nen.
2006.
Unlimited vocabu-lary speech recognition with morph language mod-els applied to Finnish.
Computer Speech and Lan-guage, 20(4):515?541, October.O.
Kohonen, S. Virpioja, and K. Lagus.
2010.
Semi-supervised learning of concatenative morphology.In Proceedings of the 11th Meeting of the ACL Spe-cial Interest Group on Computational Morphology36and Phonology, pages 78?86, Uppsala, Sweden,July.
Association for Computational Linguistics.M.
Kurimo, S. Virpioja, V. Turunen, G. W. Blackwood,and W. Byrne.
2009.
Overview and results of Mor-pho Challenge 2009.
In Working Notes for the CLEF2009 Workshop, Corfu, Greece, September.M.
Kurimo, S. Virpioja, and V. Turunen.
2010.Overview and results of Morpho Challenge 2010.
InProceedings of the Morpho Challenge 2010 Work-shop, pages 7?24, Espoo, Finland, September.
AaltoUniversity School of Science and Technology, De-partment of Information and Computer Science.Technical Report TKK-ICS-R37.J.
Lafferty, A. McCallum, and F.C.N.
Pereira.
2001.Conditional random fields: Probabilistic models forsegmenting and labeling sequence data.
In Proceed-ings of the Eighteenth International Conference onMachine Learning, pages 282?289.M.
Maamouri, A. Bies, T. Buckwalter, and W. Mekki.2004.
The penn arabic treebank: Building a large-scale annotated arabic corpus.
In NEMLAR Con-ference on Arabic Language Resources and Tools,pages 102?109.G.
Mann and A. McCallum.
2008.
Generalized expec-tation criteria for semi-supervised learning of con-ditional random fields.
In Proceedings of ACL-08: HLT, pages 870?878.
Association for Compu-tational Linguistics.A.
McCallum and W. Li.
2003.
Early results fornamed entity recognition with conditional randomfields, feature induction and web-enhanced lexicons.In Proceedings of the seventh conference on Naturallanguage learning at HLT-NAACL 2003-Volume 4,pages 188?191.
Association for Computational Lin-guistics.A.
McCallum, D. Freitag, and F. Pereira.
2000.
Max-imum entropy Markov models for information ex-traction and segmentation.
In Pat Langley, editor,Proceedings of the Seventeenth International Con-ference on Machine Learning (ICML 2000), pages591?598, Stanford, CA, USA.
Morgan Kaufmann.F.
Peng, F. Feng, and A. McCallum.
2004.
Chinesesegmentation and new word detection using condi-tional random fields.
In Proceedings of the 20th In-ternational Conference on Computational Linguis-tics (COLING 2004), page 562.
Association forComputational Linguistics.H.
Poon, C. Cherry, and K. Toutanova.
2009.
Unsuper-vised morphological segmentation with log-linearmodels.
In Proceedings of Human Language Tech-nologies: The 2009 Annual Conference of the NorthAmerican Chapter of the Association for Computa-tional Linguistics, pages 209?217.
Association forComputational Linguistics.F.
Sha and F. Pereira.
2003.
Shallow parsing with con-ditional random fields.
In Proceedings of the 2003Conference of the North American Chapter of theAssociation for Computational Linguistics on Hu-man Language Technology-Volume 1, pages 134?141.
Association for Computational Linguistics.B.
Snyder and R. Barzilay.
2008.
Crosslingual prop-agation for morphological analysis.
In Proceedingsof the AAAI, pages 848?854.M.
Szummer, P. Kohli, and D. Hoiem.
2008.
Learn-ing CRFs using graph cuts.
Computer Vision?ECCV2008, pages 582?595.V.
Turunen and M. Kurimo.
2011.
Speech retrievalfrom unsegmented Finnish audio using statisticalmorpheme-like units for segmentation, recognition,and retrieval.
ACM Transactions on Speech andLanguage Processing, 8(1):1:1?1:25, October.S.
Virpioja, V. Turunen, S. Spiegler, O. Kohonen, andM.
Kurimo.
2011.
Empirical comparison of eval-uation methods for unsupervised learning of mor-phology.
Traitement Automatique des Langues,52(2):45?90.H.
Zhao, C.N.
Huang, and M. Li.
2006.
An improvedchinese word segmentation system with conditionalrandom field.
In Proceedings of the Fifth SIGHANWorkshop on Chinese Language Processing, volume1082117.
Sydney: July.37
