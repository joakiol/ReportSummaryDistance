Stochastic Finite-State models for Spoken Language Machine': anslationSr in ivas  Banga lore  G iuseppe R iccard iAT&T Labs - Research180  Park  AvenueF lorham Park,  NJ  07932{srini, ds'.p3}?research, att.
tomAbst ractStochastic finite-state models are efficiently learn-able from data, effective for decoding and are asso-ciated with a calculus for composing models whichallows for tight integration of constraints frora var-ious levels of language processing.
In this paper,we present a method for stochastic finite-state ma-chine translation that is trained automatically frompairs of source and target utterances.
We use thismethod to develop models for English-Japanese andJapanese-English translation.
We have embeddedthe Japanese-English translation system in a callrouting task of unconstrained speech utterances.
Weevaluate the efficacy of the translation system :in thecontext of this application.1 In t roduct ionFinite state models have been extensively appliedto many aspects of language processing including,speech recognition (Pereira nd Riley, 1997; Riccardiet al, 1996), phonology (Kaplan and Kay, 1994),morphology (Koskenniemi, 1984), chunking (Abney,1991; Srinivas, 1997) and parsing (Roche, 1.999).Finite-state models are attractive mechanisms forlanguage processing since they are (a) efficientlylearnable from data (b) generally effective for de-coding (c) associated with a calculus for composingmodels which allows for straightforward integrationof constraints from various levels of language pro-cessing.
IIn this paper, we develop stochastic finite-statemodels (SFSM) for statistical machine transla-tion (SMT) and explore the performance limits ofsuch models in the context of translation in limiteddomains.
We are also interested in these modelssince they allow for a tight integration with a speechrecognizer for speech-to-speech translation.
In par-ticular we are interested in one-pass decoding andtranslation of speech as opposed to the more preva-lent approach of translation of speech lattices.The problem of machine translation can be viewedas consisting of two phases: (a) lexical choice phase1 Furthermore, software implementing the finite-stal;e cal-culus is available for research purposes.where appropriate target language lexical items arechosen for each source language lexical item and (b)reordering phase where the chosen target languagelexical items are reordered to produce a meaning-ful target language string.
In our approach, we willrepresent hese two phases using stochastic finite-state models which can be composed together toresult in a single stochastic finite-state model forSMT.
Thus our method can be viewed as a directtranslation approach of transducing strings of thesource language to strings of the target language.There are other approaches to statistical machinetranslation where translation is achieved throughtransduction of source language structure to tar-get language structure (Alshawi et al, 1998b; Wu,1997).
There are also large international multi-siteprojects such as VERBMOBIL (Verbmobil, 2000)and CSTAR (Woszczyna et al, 1998; Lavie et al,1999) that are involved in speech-to-speech trans-lation in limited domains.
The systems developedin these projects employ various techniques rangingfrom example-based to interlingua-based translationmethods for translation between English, French,German, Italian, Japanese, and Korean.Finite-state models for SMT have been previ-ously suggested in the literature (Vilar et al, 1999;Knight and A1-Onaizan, 1998).
In (Vilar et al,1999), a deterministic transducer is used to imple-ment an English-Spanish speech translation system.In (Knight and A1-Onaizan, 1998), finite-state ma-chine translation is based on (Brown et al, 1993)and is used for decoding the target language string.However, no experimental results are reported usingthis approach.?
Our approach differs from the previous approachesin both the lexical choice and the reordering phases.Unlike the previous approaches, the lexical choicephase in our approach is decomposed into phrase-level and sentence-level translation models.
Thephrase-level translation is learned based on joint en-tropy reduction of the source and target languagesand a variable length n-gram model (VNSA) (Ric-cardi et al, 1995; Riccardi et al, 1996) is learrmdfor the sentence-level translation.
For the construc-52tion of the bilingual exicon needed for lexical choice,we use the alignment algorithm presented in (A1-shawl et al, 1998b) which takes advantage of hi-erarchical decomposition of strings and thus per-forms a structure-based alignment.
In the previ-ous approaches, a bilingual lexicon is constructedusing a string-based alignment.
Another differencebetween our approach and the previous approachesis in the reordering of the target language lexicalitems.
In (Knight and A1-Onaizan, 1998), an FSMthat represents all strings resulting from the per-mutations of the lexical items produced by lexicalchoice is constructed and the most likely translationis retrieved using a target language model.
In (Vilaret al, 1999), the lexical items are associated withmarkers that allow for reconstruction of the targetlanguage string.
Our reordering step is similar tothat proposed in (Knight and A1-Onalzan, 1998) butdoes not incur the expense of creating a permutationlattice.
We use a phrase-based VNSA target lan-guage model to retrieve the most likely translationfrom the lattice.In addition, we have used the resulting finite-state translation method to implement an English-Japanese speech and text translation system anda Japanese-English text translation system.
Wepresent evaluation results for these systems and dis-cuss their limitations.
We also evaluate the efficacyof this translation model in the context of a telecomapplication such as call routing.The layout of the paper is as follows.
In Section 2we discuss the architecture of the finite-state trans-lation system.
We discuss the algorithm for learninglexical and phrasal translation in Section 3.
The de-tails of the translation model are presented in Sec-tion 4 and our method for reordering the outputis presented in Section 5.
In Section 6 we discussthe call classification application and present moti-vations for embedding translation in such an applica-tion.
In Section 6.1 we present he experiments andevaluation results for the various translation systemson text input.2 Stochastic Machine TranslationIn machine translation, the objective is to map asource symbol sequence Ws = wx,...,WNs (wi ELs) into a target sequence WT = xl, .
.
.
,  XNT (Xi ELT).
The statistical machine translation approachis based on the noisy channel paradigm and theMaximum-A-Posteriori decoding algorithm (Brownet al, 1993).
The sequence Ws is thought as a noisyversion of WT and the best guess I)d~ is then com-puted as^W~ = argmax P(WT{Ws) wT= argmax P(WslWT)P(WT) (1) wTIn (Brown et al, 1993) they propose a method formaximizing P(WTIWs) by estimating P(WT) andP(WsIWT) and solving the problem in equation 1.Our approach to statistical machine translation dif-fers from the model proposed in (Brown et al, 1993)in that:?
We compute the joint model P(Ws, WT) fromthe bilanguage corpus to account for the directmapping of the source sentence Ws into the tar-get sentence I?VT that is ordered according to the?
source language word order.
The target stringIfV~ is then chosen from all possible reorderings 2ofI?VT = argmax P(Ws, WT) (2)WT\[TV~ = arg max P(I~VT IAT) (3)WTE~W Twhere AT is the target language model and AWTare the different reorderings of WT.?
We decompose the translation problem intolocal (phrase-level) and global (sentence-level)source-target s ring transduction.?
We automatically learn stochastic automataand transducers to perform the sentence-leveland phrase-level translation.As shown in Figure 1, the stochastic machinetranslation system consists of two phases, the lexicalchoice phase and the reordering phase.
In the nextsections we describe the finite-state machine com-ponents and the operation cascade that implementsthis translation algorithm.3 Acquir ing Lexical TranslationsIn the problem of speech recognition the alignmentbetween the words and their acoustics is relativelystraightforward since the words appear in the sameorder as their corresponding acoustic events.
In con-trast, in machine" translation, the linear order ofwords in the source language, in general is not main-tained in the target language.The first stage in the process of bilingual phraseacquisition is obtaining an alignment function thatgiven a pair of source and target language sentences,maps source language word subsequences into targetlanguage word subsequences.
For this purpose, weuse the alignment algorithm described in (Alshawi et2 Note that computing the exact set of all possible reorder-ings is computationally expensive.
In Section 5 we discussan approximation for the set of all possible reorderings thatserves for our application.53max P(Ws,W r )WTReorderFigure 1: A block diagram of the stochastic machine translation systemEnglish: I need to make a collect callJapanese: ~l~ ~lzP  b ~--Jt~Alignment: 1 5 0 3 0 2 4English: A T and T calling cardJapanese: ~ 4 ~ -~ -- 7" Y FAlignment: 123456English: I'd like to charge this to my home phoneJapanese: ~./J2 ~ $J,?~ ~69 ~C.
-~-~--~Alignment: 170620345Table 1: Example bitexts and with alignment informational., 1998a).
The result of the alignment procedureis shown in Table 1.3Although the search for bilingual phrases of lengthmore than two words can be incorporated in astraight-forward manner in the alignment module,we find that doing so is computationally prohibitive.We first transform the output of the alignmentinto a representation conducive for further manip-ulation.
We call this a bilanguage TB.
A stringR E TB is represented as follows:R = Wl-Z l ,  W2_Z2,.
.
.
, WN-ZN (4)an example alignment and the source-word-orderedbilanguage strings corresponding to the alignmentshown in Table 1.Having transformed the alignment for each sen-tence pair into a bilanguage string (source word-ordered or target word-ordered), we proceed to seg-ment the corpus into bilingual phrases which can beacquired from the corpus TB by minimizing the jointentropy H(Ls, LT) ~ -1 /M log P(TB).
The proba-bility P(Ws, WT) = P(R)  is computed in the sameway as n-gram model:where wl E LsUe, zi E LTUe, e is the emptystring and wi_zi is the symbol pair (colons are thedelimiters) drawn from the source and target lan-guage.A string in a bilanguage corpus consists of se-quences of tokens where each token (wi-xi) is repre-sented with two components: a source word (\]possi-bly an empty word) as the first component and thetarget word (possibly an empty word) that is thetranslation of the source word as the second com-ponent.
Note that the tokens of a bilanguage couldbe either ordered according to the word order of thesource language or ordered according to the wordorder of the target language.
Thus an alignmentof a pair of source and target language sentenceswill result in two bilanguage strings.
Table 2 shows3The Japanese string was translated and segmented sothat a token boundary in Japanese corresponds tosome tokenboundary in English.P(R) = Il li(5)Using the phrase segmented corpus, we constructa phrase-based variable n-gram translation model asdiscussed in the following section.4 Learn ing  Phrase-based  Var iab leN-gram Trans la t ion  Mode lsOur approach to stochastic language modeling isbased on the Variable Ngram Stochastic Automaton(VNSA) representation and learning algorithmsintroduced in (Riccardi et al, 1995; Pdccardi et al,1996).
A VNSA is a non-deterministic StochasticFinite-State Machine (SFSM) that allows for pars-ing any possible sequence of words drawn from agiven vocabulary 12.
In its simplest implementationthe state q in the VNSA encapsulates the lexical(word sequence) history of a word sequence.
Each54I_i./Ji need_~,~5 9 ~ ~ to_%EPS% make_~--It, ~" a_%EPS% collect_ ~ I /2 b call_hi l~I'd_$L~2c like_ L 1",: ~ ~ '~ '~ to_%EPS% charge_-~-'v - ~ this_,._ ~ ~ to_%EPS% my_*.L~ home.
.~ phone_~-~b=A_m4 T_-~ 4 -- and_T:/V T_~ ~ -- calling_K-- ~) Z/Y" card_2-- VTable 2: Bilanguage strings resulting from alignments shown in Table 1.
(%EPS% represents he null symbol c).state recognizes asymbol wi E lZU {e}, where e is theempty string.
The probability of going from state qito qj (and recognizing the symbol associated to qj)is given by the state transition probability, P(qj \[qi).Stochastic finite-state machines represent m acompact way the probability distribution over allpossible word sequences.
The probability of a wordsequence W can be associated to a state sequence~Jw = ql , .
.
.
,  qj and to the probability P(~Jw)" Fora non-deterministic f nite-state machine the prob-ability of W is then given by P(W) = ~j  P((Jw).Moreover, by appropriately defining the state spaceto incorporate l xical and extra-lexical information,the VNSA formalism can generate a wide class ofprobability distribution (i.e., standard word n-gram,class-based, phrase-based, etc.)
(Riccardi et al,1996; Riccardi et al, 1997; Riccardi and Bangalore,1998).
In Fig.
2, we plot a fragment of a VNSAtrained with word classes and phrases.
State 0 isthe initial state and final states are double circled.The e transition from state 0 to state 1 carriesthe membership probability P(C), where the classC contains the two elements {collect, ca l l ingcard}.
The c transition from state 4 to state 6is a back-off transition to a lower order n-gramprobability.
State 2 carries the information aboutthe phrase ca l l ing  card.
The state transitionfunction, the transition probabilities and statespace are learned via the self-organizing algorithmspresented in (Riccardi et al, 1996).4.1 Extending VNSAs to StochasticTransducersGiven the monolingual corpus T, the VNSA learningalgorithm provides an automaton that recognizes aninput string W (W E yY) and computes P(W) ?
0for each W. Learning VNSAs from the bilingual cor-pus TB leads to the notion of stochastic transducersrST.
Stochastic transducers ST : Ls ?
LT ~ \[0, 1\]map the string Ws E Ls into WT E LT and assigna probability to the transduction Ws ~--~ WT.
Inour case, the VNSA's model will estimate P(Ws ~-~.~"WT) : P(Ws, WT) and the symbol pair wi : xiwill be associated to each transducer state q withinput label wi and output label xl.
The modelrST provides a sentence-level transduction from Wsinto WT.
The integrated sentence and phrase-leveltransduction is then trained directly on the phrase-segmented corpus 7~ described in section 3.5 Reorder ing  the  outputThe stochastic transducers TST takes as input a sen-tence Ws and outputs a set of candidate strings inthe target language with source language word or-der.
Recall that the one-to-many mapping comesfrom the non-determinism of VST.
The maximiza-tion step in equation 2is carried out with Viterbi al-gorithm over the hypothesized strings in LT and I~VTis selected.
The last step to complete the translationprocess is to apply the monolingual target languagemodel A T to re-order the sentence I?VT to produce^W~.
The re-order operation is crucial especiallyin the case the bilanguage phrases in 7~ are notsorted in the target language.
For the re-orderingoperation, the exact approach would be to searchthrough all possible permutations of the words inITVT and select the most likely.
However, that op-eration is computationally very expensive.
To over-come this problem, we approximate the set of thepermutations with the word lattice AWT represent-ing (xl I x2 I .
.
.
XN) N, where xi are the words inITVT.
The most likely string ~V~ in the word latticeis then decoded as follows:^W~ = argmax(~T o ~WT)= arg max P(~VT I)~T)(6)Where o is the composition operation defined forweighted finite-state machines (Pereira and Riley,1997).
The complete operation cascade for the ma-chine translation process is shown in Figure 3.6 Embedd ing  Trans la t ion  in anApp l i ca t ionIn this section, we describe an application inwhich we have embedded our translation model andpresent some of the motivations for doing so.
Theapplication that we are interested in is a call typeclassification task called How May I Help You (Gorinet al, 1997).
The goal is to sufficiently understand55collect/0.5ycs/O.8 ~ ~ ' ~ .
_ ~ ~calYlplease/1 <Figure 2: Example of a Variiable Ngram Stochastic Automaton (VNSA).. .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.zsr 2,-m xe(v ,w ) \] w"ReorderFigure 3: The Machine Translation architecturecaller's responses to the open-ended prompt HowMay I Help You?
and route such a call based on themeaning of the response.
Thus we aim at extractinga relatively small number of semantic actions fromthe utterances ofa very large set of users who are nottrained to the system's capabilities and limitations.The first utterance of each transaction has beentranscribed and marked with a call-type by label-ers.
There are 14 call-types plus a class other forthe complement class.
In particular, we focused ourstudy on the classification of the caller's first utter-ance in these dialogs.
The spoken sentences varywidely in duration, with a distribution distinctivelyskewed around a mean value of 5.3 seconds corre-sponding to 19 words per utterance.
Some examplesof the first utterances are given below:?
Yes ma'am where is area code two zeroone??
I'm tryn'a call and I can't get i~ togo through I wondered if you could tryit for me please??
Hel loWe trained a classifer on the training Set of En-glish sentences each of which was annotated with acall type.
The classifier searches for phrases that arestrongly associated with one of the call types (Gorinet al, 1997) and in the test phase the classifier ex-tracts these phrases from the output of the speechrecognizer and classifies the user utterance.
'\]?hishow the system works when the user speaks English.However, if the user does not speak the languagethat the classifier is trained on, English, in ourcase, the system is unusable.
We propose to solvethis problem by translating the user's utterance,Japanese, in our case, to English.
This extends theusability of the system to new user groups.An alternate approach could be to retrain theclassifier on Japanese text.
However, this approachwould result in replicating the system for each pos-sible input language, a very expensive propositionconsidering, in general, that the system could havesophisticated natural language understanding anddialog components which would have to be repli-cated also.6.1 Experiments and EvaluationIn this section, we discuss issues concerning evalu-ation of the translation system.
The data for theexperiments reported in this section were obtainedfrom the customer side of operator-customer con-versations, with the customer-caxe application de-scribed above and detailed in (Riccardi and Gorin,January 2000; Gorin et al, 1997).
Each of the cus-tomer's utterance transcriptions were then manuallytranslated into Japanese.
A total of 15,457 English-Japanese sentence pairs was split into 12,204 train-ing sentence pairs and 3,253 test sentence pairs.The objective of this experiment is to measurethe performance of a translation system in the con-text of an application.
In an automated call routerthere axe two important performance measures.
Thefirst is the probability of false rejection, where acall is falsely rejected.
Since such calls would betransferred to a human agent, this corresponds toa missed opportunity for automation.
The second56measure is the probability of correct classification.Errors in this dimension lead to misinterpretationsthat must be resolved by a dialog manager (Abellaand Gorin, 1997).Using our approach described in the previoussections, we have trained a unigram, bigram andtrigram VNSA based translation models with andwithout phrases.
Table 3 shows lexical choice (bag-of-tokens) accuracy for these different ranslationmodels measured in terms of recall, precision andF-measure.In order to measure the effectiveness of our trans-lation models for this task we classify Japanese ut-terances based on their English translations.
Fig-ure 4 plots the false rejection rate against the correctclassification rate of the classifier on the English gen-erated by three different Japanese to English trans-lation models for the set of Japanese test sentences.The figure also shows the performance ofthe classi-fier using the correct English text as input.There are a few interesting observations to bemade from the Figure 4.
Firstly, the task per-formance on the text data is asymptotically simi-lar to the task performance on the translation out-put.
In other words, the system performance is notsignificantly affected by the translation process; aJapanese transcription would most often be associ-ated with the same call type after translation as ifthe original were English.
This result is particu-larly interesting inspite of the impoverished reorder-ing phase of the target language words.
We believethat this result is due to the nature of the applicationwhere the classifier is mostly relying on the existenceof certain key words and phrases, not necessarily inany particular order.The task performance improved from theunigram-based translation model to phrase unigram-based translation model corresponding to the im-provement in the lexical choice accuracy in Table 3.Also, at higher false rejection rates, the task perfor-mance is better for trigram-based translation modelthan the phrase trigram-based translation modelsince the precision of lexical choice is better thanthat of the phrase trigram-based model as shown inTable 3.
This difference narrows at lower false rejec-tion rate.We are currently working on evaluating thetranslation system in an application independentmethod and developing improved models of reorder-ing needed for better translation system.7 ConclusionWe have presented an architecture for speech trans-lation in limited domains based on the simple ma-chinery of stochastic finite-state transducers.
Wehave implemented stochastic finite-state models forEnglish-Japanese and Japanese-English translationin limited domains.
These models have been trainedautomatically from source-target utterance pairs.We have evaluated the effectiveness ofsuch a transla-tion model in the context of a call-type classificationtask.Re ferencesA.
Abella and A. L. Gorin.
1997.
Generating se-mantically consistent inputs to a dialog man-ager.
In Proceedings of European Conference onSpeech Communication and Technology, pages1879-1882.Steven Abney.
1991.
Parsing by chunks.
In RobertBerwick, Steven Abney, and Carol Tenny, editors,Principle-based parsing.
Kluwer Academic Pub-lishers.H.
'Alshawi, S. Bangalore, and S. Douglas.
1998a.Learning Phrase-based Head Transduction Mod-els for Translation of Spoken Utterances.
In Thefifth International Conference on Spoken Lan-guage Processing (ICSLP98), Sydney.Hiyan Alshawi, Srinivas Bangalore, and Shona Dou-glas.
1998b.
Automatic acquisition of hierarchi-cal transduction models for machine translation.In Proceedings of the 36 th Annual Meeting of theAssociation for Computational Linguistics, Mon-treal, Canada.P.
Brown, S.D.
Pietra, V.D.
Pietra, and R. Mer-cer.
1993.
The Mathematics of Machine Transla-tion: Parameter Estimation.
Computational Lin-guistics, 16(2):263-312.E.
Giachin.
1995.
Phrase Bigrams for ContinuousSpeech Recognition.
In Proceedings of ICASSP,pages 225-228, Detroit.A.
L. Gorin, G. Riccardi, and J. H Wright.
1997.How May I Help You?
Speech Communication,23:113-127.R.M.
Kaplan and M. Kay.
1994.
Regular modelsof phonological rule systems.
Computational Lin-guistics, 20(3):331-378.Kevin Knight and Y. A1-Onaizan.
1998.
Transla-tion with finite-state devices.
In Machine trans-lation and the information soup, Langhorne, PA,October.K.
K. KoskenniemL 1984.
Two-level morphology: ageneral computation model for word-form recogni-tion and production.
Ph.D. thesis, University ofHelsinki.Alon Lavie, Lori Levin, Monika Woszczyna, DonnaGates, Marsal Gavalda, , and Alex Waibel.1999.
The janus-iii translation system: Speech-to-speech translation in multiple domains.
InProceedings of CSTAR Workshop, Schwetzingen,Germany, September.Fernando C.N.
Pereira and Michael D. Riley.
1997.Speech recognition by composition of weightedfinite automata.
In E. Roche and Schabes Y.,5"7Trans RecallVNSA order (R)Unigram 24.5Bigram 55.3Trigram 61.8Phrase Unigram 43.7Phrase Bigram 62.5Phrase Trigram 65.5Precision(e)83.6F-Measure(2*P*R/(P+R))37.987.3 67.786.4 72.180.3 56.686.3 72.585.5 74.2Table 3: Lexical choice accuracy of the Japanese to English Translation System with and without phrases100ROC curve for English test seti95i i i Trigram !
Phrese-LJnlgram ~ i| i iTexl ; ,~'* i ' ~!
i ; ~.~i f  i i , -  i i i/ ii~  .................. i;; h~ ........................................... i ~ g .." i v i i1~ Unlgram8 .
.
.
.
.
.
.
.
.
.
; .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
8O75 .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.7O 10 20 30 40 50 60 70 80 90False rejection rate (%)Figure 4: Plots for the false rejection rate against he correct classification rate of the classifier on the Englishgenerated by three different Japanese to EnglJish translation modelseditors, Finite State Devices for Natural Lan-guage Processing.
MIT Press, Cambridge, Mas-sachusetts.G.
Riccardi and S. Bangalore.
1998.
Automatic ac-quisition of phrase grammars for stochastic lan-guage modeling.
In Proceedings of A CL Workshopon Very Large Corpora, pages 188-196, Montreal.G.
Riccardi and A.L.
Gorin.
January, 2000.Stochastic Language Adaptation over Time andState in Natural Spoken Dialogue Systems.
IEEETransactions on Speech and Audio, pages 3--10.G.
Riccardi, E. Bocchieri, and It.
Pieraccini.
1995.Non deterministic stochastic language models forspeech recognition.
In Proceedings of ICASSP,pages 247-250, Detroit.G.
Riccardi, R. Pieraccini, and E. Bocchieri.
1996.Stochastic Automata for Language Modeling.Computer Speech and Language, 10(4):265-293.G.
Riccardi, A. L. Gorin, A. Ljolje, and M. Riley.1997.
A spoken language system for automatedcall routing.
In Proceedings of ICASSP, pages1143-1146, Munich.K.
Ries, F.D.
BuO, and T. Wang.
1995.
ImprovedLanguage Modeling by Unsupervised Acquisitionof Structure.
In Proceedings of ICASSP, pages193-196, Detroit.Emmanuel Roche.
1999.
Finite state transducers:parsing free and frozen sentences.
In Andr~ Ko-rnai, editor, Eztened Finite State Models of Lan-guage.
Cambridge University Press.B.
Srinivas.
1997.
Complexity of Lexical Descrip-tions and its .Relevance to Partial Parsing.
Ph.D.thesis, University of Pennsylvania, Philadelphia,PA, August.Verbmobil.
2000.
Verbmobil Web page.http://verbmobil.dfki.de/.J.
Vilar, V.M.
Jim~nez, J. Amengual, A. Castel-lanos, D. Llorens, and E. VidM.
1999.
Textand speech translation by means of subsequentialtransducers.
In Andr~ Kornai, editor, ExtenedFinite State Models of Language.
Cambridge Uni-versity Press.Monika Woszczyna, Matthew Broadhead, DonnaGates, Marsal Gavalda, Alon Lavie, Lori Levin,58and Alex Waibel.
1998.
A modular approach tospoken language translation for large domains.
InProceedings of AMTA-98, Langhorne, Pennsylva-nia, October.Dekai Wu.
1997.
Stochastic Inversion TransductionGrammars and Bilingual Parsing of Parallel Cor-pora.
Computational Linguistics, 23(3):377-404.59
