METER: MEasuring TExt ReusePaul Clough and Robert Gaizauskas and Scott S.L.
Piao and Yorick WilksDepartment of Computer ScienceUniversity of She?eldRegent Court, 211 Portobello Street,She?eld, England, S1 4DPfinitial.surname@dcs.shef.ac.ukgAbstractIn this paper we present results fromthe METER (MEasuring TExt Reuse)project whose aim is to explore issuespertaining to text reuse and derivation,especially in the context of newspapersusing newswire sources.
Although thereuse of text by journalists has beenstudied in linguistics, we are not awareof any investigation using existing com-putational methods for this particulartask.
We investigate the classicationof newspaper articles according to theirdegree of dependence upon, or deriva-tion from, a newswire source using asimple 3-level scheme designed by jour-nalists.
Three approaches to measur-ing text similarity are considered: n-gram overlap, Greedy String Tiling,and sentence alignment.
Measuredagainst a manually annotated corpus ofsource and derived news text, we showthat a combined classier with fea-tures automatically selected performsbest overall for the ternary classica-tion achieving an average F1-measurescore of 0.664 across all three cate-gories.1 IntroductionA topic of considerable theoretical and practicalinterest is that of text reuse: the reuse of existingwritten sources in the creation of a new text.
Ofcourse, reusing language is as old as the retellingof stories, but current technologies for creating,copying and disseminating electronic text, makeit easier than ever before to take some or all ofany number of existing text sources and reusethem verbatim or with varying degrees of mod-ication.One form of unacceptable text reuse, plagia-rism, has received considerable attention andsoftware for automatic plagiarism detection isnow available (see, e.g.
(Clough, 2000) for a re-cent review).
But in this paper we present abenign and acceptable form of text reuse thatis encountered virtually every day: the reuse ofnews agency text (called copy) in the produc-tion of daily newspapers.
The question is notjust whether agency copy has been reused, butto what extent and subject to what transforma-tions.
Using existing approaches from computa-tional text analysis, we investigate their abilityto classify newspapers articles into categories in-dicating their dependency on agency copy.2 Journalistic reuse of a newswireThe process of gathering, editing and publish-ing newspaper stories is a complex and spe-cialised task often operating within specic pub-lishing constraints such as: 1) short deadlines;2) prescriptive writing practice (see, e.g.
Evans(1972)); 3) limits of physical size; 4) readabilityand audience comprehension, e.g.
a tabloid'svocabulary limitations; 5) journalistic bias, e.g.political and 6) a newspaper's house style.
Of-ten newsworkers, such as the reporter and edi-tor, will rely upon news agency copy as the basisof a news story or to verify facts and assess theComputational Linguistics (ACL), Philadelphia, July 2002, pp.
152-159.Proceedings of the 40th Annual Meeting of the Association forimportance of a story in the context of all thoseappearing on the newswire.
Because of the na-ture of journalistic text reuse, dierences willarise between reused news agency copy and theoriginal text.
For example consider the follow-ing:Original (news agency) A drink-driver whoran into the Queen Mother's o?cial Daim-ler was ned $700 and banned from drivingfor two years.Rewrite (tabloid) A DRUNK driver whoploughed into the Queen Mother's limo wasned $700 and banned for two years yes-terday.This simple example illustrates the types ofrewrite that can occur even in a very shortsentence.
The rewrite makes use of slang andexaggeration to capture its readers' attention(e.g.
DRUNK, limo, ploughed).
Deletion (e.g.from driving) has also been used and the addi-tion of yesterday indicates when the event oc-curred.
Many of the transformations we ob-served between moving from news agency copyto the newspaper version have also been re-ported by the summarisation community (see,e.g., McKeown and Jing (1999)).Given the value of the information news agen-cies supply, the ease with which text can bereused and commercial pressures, it would bebenecial to be able to identify those news sto-ries appearing in the newspapers that have reliedupon agency copy in their production.
Potentialuses include: 1) monitoring take-up of agencycopy; 2) identifying the most reused stories ; 3)determining customer dependency upon agencycopy and 4) new methods for charging customersbased upon the amount of copy reused.
Giventhe large volume of news agency copy outputeach day, it would be infeasible to identify andquantify reuse manually; therefore an automaticmethod is required.3 A conceptual frameworkTo begin to get a handle on measuring textreuse, we have developed a document-level clas-sication scheme, indicating the level at whicha newspaper story as a whole is derived fromagency copy, and a lexical-level classicationscheme, indicating the level at which individ-ual word sequences within a newspaper storyare derived from agency copy.
This frameworkrests upon the intuitions of trained journaliststo judge text reuse, and not on an explicit lex-ical/syntactic denition of reuse (which wouldpresuppose what we are setting out to discover).At the document level, newspaper storiesare assigned to one of three possible categoriescoarsely reecting the amount of text reusedfrom the news agency and the dependency ofthe newspaper story upon news agency copyfor the provision of \facts".
The categories in-dicate whether a trained journalist can iden-tify text rewritten from the news agency ina candidate derived newspaper article.
Theyare: 1) wholly-derived (WD): all text inthe newspaper article is rewritten only fromnews agency copy; 2) partially-derived (PD):some text is derived from the news agency, butother sources have also been used; and 3) non-derived (ND): news agency has not been usedas the source of the article; although words maystill co-occur between the newspaper article andnews agency copy on the same topic, the jour-nalist is condent the news agency has not beenused.At the lexical or word sequence level, individ-ual words and phrases within a newspaper storyare classied as to whether they are used to ex-press the same information as words in newsagency copy (i.e.
paraphrases) and or used toexpress information not found in agency copy.Once again, three categories are used, based onthe judgement of a trained journalist: 1) verba-tim: text appearing word-for-word to expressthe same information; 2) rewrite: text para-phrased to create a dierent surface appearance,but express the same information and 3) new:text used to express information not appearingin agency copy (can include verbatim/rewrittentext, but being used in a dierent context).3.1 The METER corpusBased on this conceptual framework, we haveconstructed a small annotated corpus of newstexts using the UK Press Association (PA) asthe news agency source and nine British dailynewspapers1who subscribe to the PA as candi-date reusers.
The METER corpus (Gaizauskaset al, 2001) is a collection of 1716 texts (over500,000 words) carefully selected from a 12month period from the areas of law and courtreporting (769 stories) and showbusiness (175stories).
772 of these texts are PA copy and 944from the nine newspapers.
These texts cover 265dierent stories from July 1999 to June 2000 andall newspaper stories have been manually classi-ed at the document-level.
They include 300wholly-derived, 438 partially-derived and 206non-derived (i.e.
77% are thought to have usedPA in some way).
In addition, 355 have beenclassied according to the lexical-level scheme.4 Approaches to measuring textsimilarityMany problems in computational text analy-sis involve the measurement of similarity.
Forexample, the retrieval of documents to full auser information need, clustering documents ac-cording to some criterion, multi-document sum-marisation, aligning sentences from one lan-guage with those in another, detecting exact andnear duplicates of documents, plagiarism detec-tion, routing documents according to their styleand identifying authorship attribution.
Meth-ods typically vary depending upon the match-ing method, e.g.
exact or partial, the degreeto which natural language processing techniquesare used and the type of problem, e.g.
search-ing, clustering, aligning etc.
We have not hadtime to investigate all of these techniques, noris there space here to review them.
We haveconcentrated on just three: ngram overlap mea-sures, Greedy String Tiling, and sentence align-ment.
The rst was investigated because it of-fers perhaps the simplest approach to the prob-lem.
The second was investigated because it hasbeen successfully used in plagiarism detection, aproblem which at least supercially is quite close1The newspapers include ve popular papers (e.g.
TheSun, The Daily Mail, Daily Star, Daily Mirror) and fourquality papers (e.g.
Daily Telegraph, The Guardian, TheIndependent and The Times).to the text reuse issues we are investigating.
Fi-nally, alignment (treating the derived text as a\translation" of the rst) seemed an intriguingidea, and contrasts, certainly with the ngram ap-proach, by focusing more on local, as opposed toglobal measures of similarity.4.1 Ngram OverlapAn initial, straightforward approach to assessingthe reuse between two texts is to measure thenumber of shared word ngrams.
This methodunderlies many of the approaches used in copydetection including the approach taken by Lyonet al (2001).They measure similarity using the set-theoretic measures of containment and resem-blance of shared trigrams to separate texts writ-ten independently and those with su?cient sim-ilarity to indicate some form of copying.We treat each document as a set of overlap-ping n-word sequences (initially considering onlyn-word types) and compute a similarity scorefrom this.
Given two sets of ngrams, we usethe set-theoretic containment score to measuresimilarity between the documents for ngrams oflength 1 to 10 words.
For a source text A anda possibly derived text B represented by sets ofngrams Sn(A) and Sn(B) respectively, the pro-portion of ngrams in B also in A, the ngram con-tainment Cn(A;B), is given by:Cn(A;B) =j Sn(A) \ Sn(B) jj Sn(B) j(1)Informally containment measures the numberof matches between the elements of ngram setsSn(A) and Sn(B), scaled by the size of Sn(B).In other words we measure the proportion ofunique n-grams in B that are found in A. Thescore ranges from 0 to 1, indicating none to allnewspaper copy shared with PA respectively.We also compare texts by counting only thosengrams with low frequency, in particular thoseoccurring once.
For 1-grams, this is the same ascomparing the hapax legomena which has beenshown to discriminate plagiarised texts fromthose written independently even when lexicaloverlap between the texts is already high (e.g.70%) (Finlay, 1999).
Unlike Finlay's work, wend that repetition in PA copy2drastically re-duces the number of shared hapax legomenathereby inhibiting classication of derived andnon-derived texts.
Therefore we compute thecontainment of hapax legomena (hapax contain-ment) by comparing words occurring once in thenewspaper, i.e.
those 1-grams in S1(B) that oc-cur once with all 1-grams in PA copy, S1(A).This containment score represents the numberof newspaper hapax legomena also appearing atleast once in PA copy.4.2 Greedy String-TilingGreedy String-Tiling (GST) is a substringmatching algorithm which computes the degreeof similarity between two strings, for exam-ple software code, free text or biological subse-quences (Wise, 1996).
Compared with previousalgorithms for computing string similarity, suchas the Longest Common Subsequence orLevenshtein distance, GST is able to deal withtransposition of tokens (in earlier approachestransposition is seen as a number of single inser-tions/deletions rather than a single block move).The GST algorithm performs a 1:1 matchingof tokens between two strings so that as much ofone token stream is covered with maximal lengthsubstrings from the other (called tiles).
In ourproblem, we consider how much newspaper textcan be maximally covered by words from PAcopy.
A minimum match length (MML) can beused to avoid spurious matches (e.g.
of 1 or2 tokens) and the resulting similarity betweenthe strings can be expressed as a quantitativesimilarity match or a qualitative list of commonsubstrings.
Figure 1 shows the result of GST forthe example in Section 2.Figure 1: Example GST results (MML=3)2As stories unfold, PA release copy with new, as wellas previous versions of the storyGiven PA copy A, a newspaper text B and aset of maximal matches, tiles, of a given lengthbetween A and B, the similarity, gstsim(A,B),is expressed as:gstsim(A;B) =Pi2tileslengthij B j(2)4.3 Sentence alignmentIn the past decade, various alignment algorithmshave been suggested for aligning multilingualparallel corpora (Wu, 2000).
These algorithmshave been used to map translation equivalentsacross dierent languages.
In this specic case,we investigate whether alignment can map de-rived texts (or parts of them) to their sourcetexts.
PA copy may be subject to variouschanges during text reuse, e.g.
a single sen-tence may derive from parts of several sourcesentences.
Therefore, strong correlations of sen-tence length between the derived and sourcesentences cannot be guaranteed.
As a result,sentence-length based statistical alignment al-gorithms (Brown et al, 1991; Gale and Church,1993) are not appropriate for this case.
On theother hand, cognate-based algorithms (Simardet al, 1992; Melamed, 1999) are more e?cientfor coping with change of text format.
There-fore, a cognate-based approach is adopted forthe METER task.
Here cognates are dened aspairs of terms that are identical, share the samestems, or are substitutable in the given context.The algorithm consists of two principal com-ponents: a comparison strategy and a scoringfunction.
In brief, the comparison works as fol-lows (more details may be found in Piao (2001)).For each sentence in the candidate derived textDT the sentences in the candidate source textST are compared in order to nd the best match.A DT sentence is allowed to match up to threepossibly non-consecutive ST sentences.
Thecandidate pair with the highest score (see be-low) above a threshold is accepted as a truealignment.
If no such candidate is found, theDT sentence is assumed to be independent ofthe ST. Based on individual DT sentence align-ments, the overall possibility of derivation forthe DT is estimated with a score ranging be-tween 0 and 1.
This score reects the propor-tion of aligned sentences in the newspaper text.Note that not only may multiple sentences inthe ST be aligned with a single sentence in theDT, but also multiple sentences in the DT maybe aligned with one sentence in the ST.Given a candidate derived sentence DS anda proposed (set of) source sentence(s) SS, thescoring function works as follows.
Three basicmeasures are computed for each pair of candi-date DS and SS: SNG is the sum of lengthsof the maximum length non-overlapping sharedn-grams with n  2; SWD is the number ofmatched words sharing stems not in an n-gramguring in SNG; and SUB is the number ofsubstitutable terms (mainly synonyms) not g-uring in SNG or SWD.
Let L1be the length ofthe candidate DS and L2the length of candidateSS.
Then, three scores PD, PS (Dice score) andPV S are calculated as follows:PSD =SWD + SNG + SUBL1PS =2(SWD + SNG + SUB)L1+ L2PSNG =SNGSWD + SNG + SUBThese three scores reect dierent aspects ofrelations between the candidate DS and SS:1.
PSD: The proportion of the DS which isshared material.2.
PS: The proportion of shared terms in DSand SS.
This measure prefers SS's which notonly contain many terms in the DS, but alsodo not contain many additional terms.3.
PSNG: The proportion of matching n-grams amongst the shared terms.
Thismeasure captures the intuition that sen-tences sharing not only words, but word se-quences are more likely to be related.These three scores are weighted and combinedtogether to provide an alignment metric WS(weighted score), which is calculated as follows:WS = ?1PSD+ ?2PS + ?3PSNGwhere ?1+?2+?3= 1.
The three weighting vari-ables ?i(i = 1; 2; 3) have been determined empir-ically and are currently set to: ?1= 0:85; ?2=0:05; ?3= 0:1.5 Reuse ClassiersTo evaluate the previous approaches for measur-ing text reuse at the document-level, we cast theproblem into one of a supervised learning task.5.1 Experimental SetupWe used similarity scores as attributes for a ma-chine learning algorithm and used the Weka 3.2software (Witten and Frank, 2000).
Because ofthe small number of examples, we used tenfoldcross-validation repeated 10 times (i.e.
10 runs)and combined this with stratication to ensureapproximately the same proportion of samplesfrom each class were used in each fold of thecross-validation.
All 769 newspaper texts fromthe courts domain were used for evaluation andrandomly permuted to generate 10 sets.
Foreach newspaper text, we compared PA sourcetexts from the same story to create results inthe form: newspaper; class; score.
These resultswere ordered according to each set to create thesame 10 datasets for each approach thereby en-abling comparison.Using this data we rst trained ve single-feature Naive Bayes classiers to do the ternaryclassication task.
The feature in each case wasa variant of one of the three similarity measuresdescribed in Section 4, computed between thetwo texts in the training set.
The target classi-cation value was the reuse classication categoryfrom the corpus.
A Naive Bayes classier wasused because of its success in previous classi-cation tasks, however we are aware of its naiveassumptions that attributes are assumed inde-pendent and data to be normally distributed.We evaluated results using the F1-measure(harmonic mean of precision and recall givenequal weighting).
For each run, we calculatedthe average F1score across the classes.
Theoverall average F1-measure scores were com-puted from the 10 runs for each class (a singleaccuracy measure would su?ce but the Wekapackage outputs F1-measures).
For the 10 runs,the standard deviation of F1scores was com-puted for each class and F1scores between allapproaches were tested for statistical signi-cance using 1-way analysis of variance at a 99%condence-level.
Statistical dierences betweenresults were identied using Bonferroni analy-sis3.After examining the results of these single fea-ture classiers, we also trained a \combined"classier using a correlation-based lter ap-proach (Hall and Smith, 1999) to select the com-bination of features giving the highest classica-tion score ( correlation-based ltering evaluatesall possible combinations of features).
Featureselection was carried for each fold during cross-validation and features used in all 10 folds werechosen as candidates.
Those which occurred inat least 5 of the 10 runs formed the nal selec-tion.We also tried splitting the training data intovarious binary partitions (e.g.
WD/PD vs. ND)and training binary classiers, using feature se-lection, to see how well binary classicationcould be performed.
Eskin and Bogosian (1998)have observed that using cascaded binary clas-siers, each of which splits the data well, maywork better on n-ary classication problemsthan a single n-way classier.
We then com-puted how well such a cascaded classier shouldperform using the best binary classier results.5.2 ResultsTable 1 shows the results of the single ternaryclassiers.
The baseline F1measure is basedupon the prior probability of a document fallinginto one of the classes.
The gures in parenthe-sis are the standard deviations for the F1scoresacross the ten evaluation runs.
The nal rowshows the results for combining features selectedusing the correlation-based lter.Table 2 shows the result of training binaryclassiers using feature selection to select themost discriminating features for various binarysplits of the training data.For both ternary and binary classiers featureselection produced better results than using all3Using SPSS v10.0 for Windows.Approach Category Avg F-measureBaseline WD 0.340 (0.000)PD 0.444 (0.000)ND 0.216 (0.000)total 0.333 (0.000)3-gram WD 0.631 (0.004)containment PD 0.624 (0.004)ND 0.549 (0.005)total 0.601 (0.003)GST Sim WD 0.669 (0.004)MML = 3 PD 0.633 (0.003)ND 0.556 (0.004)total 0.620 (0.002)GST Sim WD 0.681 (0.003)MML = 1 PD 0.634 (0.003)ND 0.559 (0.008)total 0.625 (0.004)1-gram WD 0.718 (0.003)containment PD 0.643 (0.003)ND 0.551 (0.006)total 0.638 (0.003)Alignment WD 0.774 (0.003)PD 0.624 (0.005)ND 0.537 (0.007)total 0.645 (0.004)hapax WD 0.736 (0.003)containment PD 0.654 (0.003)ND 0.549 (0.010)total 0.646 (0.004)hapax cont.
WD 0.756 (0.002)1-gram cont.
PD 0.599 (0.006)alignment ND 0.629 (0.008)(\combined") total 0.664 (0.004)Table 1: A summary of classication resultspossible features, with the one exception of thebinary classication between PD and ND.5.3 DiscussionFrom Table 1, we nd that all classier resultsare signicantly higher than the baseline (atp < 0:01) and all dierences are signicant ex-cept between hapax containment and alignment.The highest F-measure for the 3-class problemis 0.664 for the \combined" classier, which issignicantly greater than 0.651 obtained with-out.
We notice that highest WD classicationis with alignment at 0.774, highest PD classi-cation is 0.654 with hapax containment andhighest ND classication is 0.629 with combinedfeatures.
Using hapax containment gives higherresults than 1-gram containment alone and infact provides results as good as or better thanthe more complex sentence alignment and GSTapproaches.Previous research by (Lyon et al, 2001) and(Wise, 1996) had shown derived texts could bedistinguished using trigram overlap and tilingwith a match length of 3 or more, respectively.Attributes Category Avg F1Correlation- alignment WD 0.942 (0.008)based ND 0.909 (0.011)lter total 0.926 (0.010)alignment PD/ND 0.870 (0.003)WD 0.770 (0.003)total 0.820 (0.002)alignment WD 0.778 (0.003)PD 0.812 (0.002)total 0.789 (0.002)hapax cont.
WD/PD 0.882 (0.002)alignment ND 0.649 (0.007)1-gram cont.
total 0.763 (0.002)1-gram PD 0.802 (0.002)GST mml 3 ND 0.638 (0.007)GST mml 1 total 0.720 (0.004)alignmentGST mml 1 WD/ND 0.672 (0.002)alignment PD 0.662 (0.003)total 0.668 (0.003)Table 2: Binary Classiers with feature selectionHowever, our results run counter to this be-cause the highest classication scores are ob-tained with 1-grams and an MML of 1, i.e.
asn or MML length increases, the F1scores de-crease.
We believe this results from two factorswhich are characteristic of reuse in journalism.First, since even ND texts are thematically sim-ilar (same events being described) there is highlikelihood of coincidental overlap of ngrams oflength 3 or more (e.g.
quoted speech).
Secondly,when journalists rewrite it is rare for them notto vary the source.For the intended application { helping the PAto monitor text reuse { the cost of dierent mis-classications is not equal.
If the classier makesa mistake, it is better that WD and ND texts aremis-classied as PD, and PD as WD.
Given thedierence in distribution of documents acrossclasses where PD contains the most documents,the classier will be biased towards this classanyway as required.
Table 3 shows the confu-sion matrix for the combined ternary classier.WD PD NDWD 203 55 4PD 79 192 70ND 3 53 109Table 3: Confusion matrix for combined ternaryclassierAlthough the overall F1-measure score is low(0.664), mis-classication of both WD as NDand ND as WD is also very low, as most mis-classications are as PD.
Note the high mis-classication of PD as both WD and ND, re-ecting the di?culty of separating this class.From Table 2, we nd alignment is a selectedfeature for each binary partition of the data.The highest binary classication is achieved be-tween the WD and ND classes using alignmentonly, and the highest three scores show WD isthe easiest class to separate from the others.The PD class is the hardest to isolate, reect-ing the mis-classications seen in Table 3.To predict how well a cascaded binary classi-er will perform we can reason as follows.
Fromthe preceding discussion we see that WD canbe separated most accurately; hence we chooseWD versus PD/ND as the rst binary classier.This forces the second classier to be PD versusND.
From the results in Table 2 and the follow-ing equation to compute the F1measure for atwo-stage binary classierWD + (PD=ND)(PD+ND2)2we obtain an overall F1measure for ternary clas-sication of 0.703, which is signicantly higherthan the best single stage ternary classier.6 ConclusionsIn this paper we have investigated text reuse inthe context of the reuse of news agency copy, anarea of theoretical and practical interest.
Wepresent a conceptual framework in which wemeasure reuse and based on which the METERcorpus has been constructed.
We have presentedthe results of using similarity scores, computedusing n-gram containment, Greedy String Tilingand an alignment algorithm, as attributes fora supervised learning algorithm faced with thetask of learning how to classify newspaper sto-ries as to whether they are wholly, partially ornon-derived from a news agency source.
Weshow that the best single feature ternary clas-sier uses either alignment or simple hapax con-tainment measures and that a cascaded binaryclassier using a combination of features canoutperform this.The results are lower than one might like,and reect the problems of measuring journalis-tic reuse, stemming from complex editing trans-formations and the high amount of verbatimtext overlapping as a result of thematic simi-larity and \expected" similarity due to, e.g., di-rect/indirect quotes.
Given the relative close-ness of results obtained by all approaches wehave considered, we speculate that any compar-ison method based upon lexical similarity willprobably not improve classication results bymuch.
Perhaps improved performance at thistask may possible by using more advanced nat-ural language processing techniques, e.g.
bettermodeling of the lexical variation and syntactictransformation that goes on in journalistic reuse.Nevertheless the results we have obtained arestrong enough in some cases (e.g.
wholly derivedtexts can be identied with > 80% accuracy) tobegin to be exploited.In summary measuring text reuse is an excit-ing new area that will have a number of appli-cations, in particular, but not limited to, mon-itoring and controlling the copy produced by anewswire.7 Future workWe are adapting the GST algorithm to deal withsimple rewrites (e.g.
synonym substitution) andto observe the eects of rewriting upon ndinglongest common substrings.
We are also experi-menting using the more detailed METER corpuslexical-level annotations to investigate how wellthe GST and ngrams approaches can identifyreuse at this level.A prototype browser-based demo of both theGST algorithm and alignment program, allow-ing users to test arbitrary text pairs for simi-larity, is now available4and will continue to beenhanced.AcknowledgementsThe authors would like to acknowledge theUK Engineering and Physical Sciences Re-search Council for funding the METER project(GR/M34041).
Thanks also to Mark Hepple forhelpful comments on earlier drafts.4See http://www.dcs.shef.ac.uk/nlp/meter.ReferencesP.F.
Brown, J.C. Lai, and R.L.
Mercer.
1991.
Aligningsentences in parallel corpora.
In Proceedings of the29th Annual Meeting of the Assoc.
for ComputationalLinguistics, pages 169{176, Berkeley, CA, USA.P Clough.
2000.
Plagiarism in natural and programminglanguages: An overview of current tools and technolo-gies.
Technical Report CS-00-05, Dept.
of ComputerScience, University of She?eld, UK.E.
Eskin and M. Bogosian.
1998.
Classifying text docu-ments using modular categories and linguistically mo-tivated indicators.
In AAAI-98 Workshop on Learningfor Text Classication.H.
Evans.
1972.
Essential English for Journalists, Edi-tors and Writers.
Pimlico, London.S.
Finlay.
1999.
Copycatch.
Master's thesis, Dept.
ofEnglish.
University of Birmingham.R.
Gaizauskas, J.
Foster, Y. Wilks, J. Arundel,P.
Clough, and S. Piao.
2001.
The meter corpus:A corpus for analysing journalistic text reuse.
In Pro-ceedings of the Corpus Linguistics 2001 Conference,pages 214|223.W.A.
Gale and K.W.
Church.
1993.
A program for align-ing sentences in bilingual corpus.
Computational Lin-guistics, 19:75{102.M.A.
Hall and L.A. Smith.
1999.
Feature selection formachine learning: Comparing a correlation-based l-ter approach to the wrapper.
In Proceedings of theFlorida Articial Intelligence Symposium (FLAIRS-99), pages 235{239.C.
Lyon, J. Malcolm, and B. Dickerson.
2001.
Detectingshort passages of similar text in large document collec-tions.
In Conference on Empirical Methods in NaturalLanguage Processing (EMNLP2001), pages 118{125.K.
McKeown and H. Jing.
1999.
The decomposition ofhuman-written summary sentences.
In SIGIR 1999,pages 129{136.I.
Dan Melamed.
1999.
Bitext maps and alignment viapattern recognition.
Computational Linguistics, pages107{130.Scott S.L.
Piao.
2001.
Detecting and measuring textreuse via aligning texts.
Research MemorandumCS-01-15, Dept.
of Computer Science, University ofShe?eld.M.
Simard, G. Foster, and P. Isabelle.
1992.
Usingcognates to align sentences in bilingual corpora.
InProceedings of the 4th Int.
Conf.
on Theoretical andMethodological Issues in Machine Translation, pages67{81, Montreal, Canada.M.
Wise.
1996.
Yap3: Improved detection of similaritiesin computer programs and other texts.
In Proceedingsof SIGCSE'96, pages 130{134, Philadelphia, USA.I.H.
Witten and E. Frank.
2000.
Datamining - practi-cal machine learning tools and techniques with Javaimplementations.
Morgan Kaufmann.D.
Wu.
2000.
Alignment.
In R. Dale and H. Moisl andH.
Somers (eds.
), A Handbook of Natural LanguageProcessing, pages 415{458.
New York: Marcel Dekker.
