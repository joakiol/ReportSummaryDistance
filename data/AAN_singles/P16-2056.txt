Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 344?350,Berlin, Germany, August 7-12, 2016.c?2016 Association for Computational LinguisticsBootstrapped Text-level Named Entity Recognition for LiteratureJulian Brooke Timothy BaldwinComputing and Information SystemsThe University of Melbournejabrooke@unimelb.edu.autb@ldwin.netAdam HammondEnglish and Comparative LiteratureSan Diego State Universityahammond@mail.sdsu.eduAbstractWe present a named entity recogni-tion (NER) system for tagging fiction:LitNER.
Relative to more traditional ap-proaches, LitNER has two importantproperties: (1) it makes no use of hand-tagged data or gazetteers, instead it boot-straps a model from term clusters; and (2)it leverages multiple instances of the samename in a text.
Our experiments show it tosubstantially outperform off-the-shelf su-pervised NER systems.1 IntroductionMuch of the work on applying NLP to the anal-ysis of literature has focused on literary fig-ures/characters in the text, e.g.
in the context of so-cial network analysis (Elson et al, 2010; Agarwalet al, 2013; Ardanuy and Sporleder, 2015) or anal-ysis of characterization (Bamman et al, 2014).Named entity recognition (NER) of person namesis generally the first step in identifying characters;locations are also a prevalent NE type, and can beuseful when tracking different plot threads (Wal-lace, 2012), or trends in the settings of fiction.There are not, to our knowledge, any NERsystems that are specifically targeted at litera-ture, and most related work has used StanfordCoreNLP as an off-the-shelf solution (Bammanet al, 2014; Vala et al, 2015).
In this paper, weshow that it is possible to take advantage of theproperties of fiction texts, in particular the repeti-tion of names, to build a high-performing 3-classNER system which distinguishes people and lo-cations from other capitalized words and phrases.Notably, we do this without any hand-labelleddata whatsoever, bootstrapping a text-level contextclassifier from a low-dimensional Brown cluster-ing of the Project Gutenberg corpus.2 Related workThe standard approach to NER is to treat it as asupervised sequential classification problem, typ-ically using conditional random fields or similarmodels, based on local context features as wellas properties of the token itself.
Relevant to thepresent work is the fact that, despite there beingsome work on enforcing tag consistency acrossmultiple instances of the same token (Finkel et al,2005) and the use of non-local features (Ratinovand Roth, 2009) to improve supervised sequentialmodels, the consensus seems to be that this non-local information has a relatively modest effect onperformance in standard datasets, and as a resultoff-the-shelf NER systems in practice treat eachsentence as a separate document, with multiple in-stances of the same token in different sentencesviewed as entirely independent classification prob-lems.
We also note that although supervised NERis the norm, there is a smaller body of work insemi-supervised and unsupervised approaches toNER and semantic lexicon induction, for instancepattern bootstrapping (Nadeau et al, 2006; Thelenand Riloff, 2002; McIntosh et al, 2011) as well asgenerative approaches (Elsner et al, 2009).In the context of literature, the most closely re-lated task is character identification (Vala et al,2015), which is itself an intermediate task for char-acter speech identification (He et al, 2013), analy-sis of characterization (Bamman et al, 2014), andanalysis of social networks (Elson et al, 2010;Agarwal et al, 2013; Ardanuy and Sporleder,2015).
In addition to NER, character identifica-344tion also involves clustering multiple aliases of thesame character, and discarding person names thatdon?t correspond to characters.
Vala et al (2015)identify some of the failures of off-the-shelf NERwith regards to character identification, and at-tempt to fix them; their efforts are focused, how-ever, on characters that are referred to by descrip-tion rather than names or aliases.3 Method3.1 Corpus preparation and segmentationThe corpus we use for building and testing ourNER system is the 2010 image of the (US)Project Gutenberg corpus,1a reasonably compre-hensive collection of out-of-copyright English lit-erary texts, to our knowledge the largest that ispublicly available in a machine-readable, full-textformat.
We access the texts via the GutenTagtool (Brooke et al, 2015), which allows both filter-ing of texts by genre as well as within-text filteringto remove Project Gutenberg copyright informa-tion, front and back matter (e.g.
table of contents),and headers.
We focus here only on fiction texts(i.e.
novels and short stories); other kinds of liter-ature (e.g.
plays) are rare in the corpus and havevery different properties in terms of the distribu-tion of names.
The final corpus size is 10844 texts.GutenTag also provides an initial segmenta-tion of tokens into potential names, using a sim-ple rule-based system which segments contiguouscapitalized words, potentially with common inter-vening function words like of as well as leadingthe (e.g.
the King of Westeros).
It largely (butnot entirely) overcomes the problem of sentence-initial capitalization in English by generalizingover an entire text; as long as a capitalized wordor phrase appears in a non-sentence initial po-sition at least once in a text, it will be taggedin the sentence-initial position as well.
To im-prove precision, the name tagger in the version ofGutenTag used for this paper (0.1.3) has lowerbounds on token count (at least 10) and an upperbound on the length of names (no longer than 3words).
For this work, however, we remove thoserestrictions to maximize recall.
Though not ourprimary concern, we return to evaluate the qualityof the initial segmentation in Section 5.1http://www.gutenberg.org3.2 Brown clusteringThe next step is to induce Brown clusters (Brownet al, 1992) over the pre-segmented corpus (in-cluding potential names), using the tool of Liang(2005).
Briefly, Brown clusters are formed us-ing an agglomerative hierarchical cluster of termsbased on their immediate context, placing termsinto categories to maximize the probability of con-secutive terms over the entire corpus.
Note thatusing information from Brown clusters is a wellestablished technique in NER, but more typicallyas features within a supervised framework (Milleret al, 2004; Liang, 2005; Ritter et al, 2011); weare unaware of any work using them directly asa source of bootstrapped training examples.
Weused default settings except for the number of clus-ters (c): 50.
The rationale for such a small clustersize?the default is 1000, and NER systems whichuse Brown clusters as features do better with evenmore (Derczynski et al, 2015)?is that we wantto have clusters that correspond to major noun cat-egories (e.g.
PERSON and LOCATION), which weconsider the next most fundamental division be-yond part-of-speech; 50 was selected because it isroughly comparable to the size of the Penn Tree-bank tagset (Marcus et al, 1993).
We did not tunethis number, except to observe that larger num-bers (e.g.
100 or 200) resulted in increasingly frag-mented clusters for our entities of interest.To automatically extract a seed list of peo-ple and locations, we ranked the clusters by thetotal (token) count of names (as identified byGutenTag), and took the first cluster to be PER-SON, and the second to be LOCATION; all otherclusters are considered OTHER, our third, catch-all category.
Alternatively, we could have set chigher and manually grouped the clusters based onthe common words in the clusters, adding a thinlayer of supervision to the process; with a low c,however, this was unnecessary since the composi-tion and ranking of the clusters conformed exactlyto our expectations.
The top-5 clusters by tokencount of names are given in Table 1.2Note thepresence of the multiword name New York in thesecond cluster, as a result of the segmentation.The most common words in the first two clus-ters are mostly what we would expect, thoughthere is a bit of noise, e.g.
Him included as aplace.
The other clusters are messier, but still in-2Note that each cluster generally includes large numbersof non-names, which we ignore.345Count Top-10 name types17.2M Tom, Jack, Dick, Mary, JohnHarry, Peter, Frank, George, Jim2.5M London, England, Paris, New York, FranceHim, America, Rome, Europe, Boston1.8M English, French, Lord, Indian, AmericanGerman, Christian, Indians, King, Italian0.5M Sir, Doctor, Colonel, Madam, MajorProfessor, Dieu, Squire, Heavens, Sire0.5M Christmas, Spanish, British, Irish, RomanLatin, Chinese, European, Dutch, ScotchTable 1: Top-5 Brown clusters derived from PGcorpus, by token count of namesterpretable: e.g.
Cluster 4 is a collection of termsof address.
Note that although we do not con-sider an term like Doctor to be a person name,Doctor Smith or the Doctor would be; in manyliterary contexts characters are referred to by analias, and failure to deal properly with these sit-uations is a significant problem with off-the-shelfNER systems in literature (Vala et al, 2015).
Inany case, Brown clustering works fairly well forcommon names, but for rarer ones, the cluster-ing is haphazard.
Fiction, though, has many rarenames and locations, since authors will often in-vent them.
Another problem with Brown clus-tering is that ignores possible sense distinctions:for instance, Florence is both a city and a personname.
To avoid confusion, authors will generallypreserve one-sense-per-document, but this is nottrue at the corpus level.3.3 Text-level context classifierThe central element of our NER system is a text-level classifier of names based on context.
Bytext-level, we mean that it assumes one-sense-per-document, classifying a name for an entire doc-ument, based on all instances of the name in thedocument (Gale et al, 1992).
It is trained onthe (text-level) ?instances?
of relatively commonnames (appearing more than 100 times in the cor-pus) from the 3 NE label types derived based onthe Brown clustering.
That is, to build a trainingset, we pass through the corpus and each time wecome across a common name in a particular doc-ument, we build a feature vector corresponding toall the contexts in that document, with the labeltaken from the clustering.
Our rationale here isthat the challenging part of NER in literature isnames that appear only in one text; by limitingour context for common words to a single text,we simulate the task for rarer words.
Mary is acommon name, and may be a major character inone text, but a minor one in another; hence, webuild a classifier that deals with both context-richand context-poor situations.
The noisy training setthus constructed has about 1 million examples.Our feature set consists of filtered word fea-tures in a 2-word window (w?2w?1w0w+1w+2)around the token occurrences w0of a target typein a given text, made up of position-indexed uni-grams (w?2, w?1, w+1and w+2) and bigrams(w?2w?1, w+1w+2and w?1w+1), excluding uni-grams when a subsuming bigram feature matched(e.g.
if we match trust in, we do not add trust andin).
For this we used the name-segmented corpus,and when one of the words in the context was alsoa name, we take the category from the Brown clus-tering as the word (so w2for London in from Lon-don to New York is LOCATION, not New).
Acrossmultiple tokens of the same type, we count thesame context only once, creating a binary featurevector which was normalized by dividing by thecount of all non-zero entries once all contexts werecollected.
To be included as features, the n-gramshad to occur with ?
10 different w0target wordtypes.
Note that given our bootstrapping setup,the word type itself cannot be used directly as afeature.For classification, we use logistic regressionfrom scikit-learn (Pedregosa et al, 2011) trainedwith SGD using L2 regularization (C = 1).3Theonly non-standard setting that we use is the ?bal-anced?
option, which weights classes by the in-verse of their count in the training set, counteringthe preference for the majority class; we do thisbecause our bootstrapped distribution is an unre-liable reflection of the true distribution, and alsobecause it makes it a fairer comparison to off-the-shelf models with no access to this distribution.3.4 Improved phrase classificationRelative to (true) supervised models, our boot-strapped model suffers from being able to use onlycontext, and not the identity of the name itself.In the case of names which are phrases, this istroubling because there are many generalizationsto be made; for instance names ending with Cityare locations.
Our final model addresses this fail-ing somewhat by using more information from our3Using cross-validation over the training data, we testedother solvers, L1 regularization, and settings of the C param-eter, but saw no appreciable improvement in performance.346Brown clustering: from each of the initial and fi-nal words across all names, we extract a set ofwords Wsthat appear at least ten times in positions ?
S, S = {initial, final} across all phrases.Let c(w, t, s) be the the number of times a wordw ?
Wsappears in the corpus at position s inphrases which were Brown clustered into the en-tity type t ?
T , and p(t|r) be the original prob-ability of phrase r being type t as determined bythe logistic regression classifier.
For our two ho-mogenous entity types (PERSON and LOCATION),we calculate a new score p?:p?
(t|r) = p(t|r) +?s?S(c(rs, t, s)?t?
?Tc(rs, t?, s)??w??Wsc(w?,t,s)?t?
?Tc(w?,t?,s)|Ws|)(1)The first term in the outermost summation inEquation 1 is the proportion of occurrences of thegiven expression in position s which correspondto type t. To avoid applying too much weight tothe homogeneous classes, the second term in thesummation subtracts the average number of occur-rences in the given position for all words in Ws.As such, the total effect on the score can be neg-ative.
Note that if rs/?
Ws, no modification ismade, and for the OTHER type p?
(t|r) = p(t|r).Once we have calculated p?
(t|r) for each class, wechoose the t with the highest p?
(t|r).4 EvaluationOur interest is in a general NER system for liter-ature.
Though there are a few novels which havebeen tagged for characters (Vala et al, 2015), wewanted to test our system relative to a much widerrange of fiction.
To this end, we randomly sampledtexts, sentences, and then names within those sen-tences from our name-segmented Project Guten-berg corpus to produce a set of 1000 examples.These were tagged by a single annotator, an En-glish native speaker with a PhD in English Liter-ature.
The annotator was presented with the sen-tence and the pre-segmented name of interest, andasked (via written instructions) to categorize theindicated name into PERSON, LOCATION, OTHER,UNCERTAIN due to ambiguity, or segmentation er-ror.
We ran a separate two-annotator agreementstudy over 200 examples which yielded a Cohen?sKappa of 0.84, suggesting high enough reliabilitythat a single annotator was sufficient.
The classSystem Acc FMAll PERSON baseline .696 ?OpenNLP .435 .572LingPipe .528 .536Stanford CoreNLP .786 .751Brown clusters .803 .672LitNER sentence +phrase .757 .671LitNER text ?phrase .855 .771LitNER text +phrase .871 .792Table 2: Performance of NER systemsdistribution for the main annotation was 66.9%PERSON, 10.2% LOCATION, 19.0% OTHER, 2.4%UNCERTAIN, and 1.5% segmentation error.
Forthe main evaluation, we excluded both UNCER-TAIN examples and segmentation errors, but hadour annotator provide correct segmentation for the15 segmentation errors and carried out a separatecomparison on these.We compare our system to a selectionof publicly available, off-the-shelf NER sys-tems: OpenNLP,4LingPipe,5and StanfordCoreNLP (Finkel et al, 2005), as well as theinitial Brown clustering.
OpenNLP allowed usto classify only PERSON and LOCATION, but forStanford CoreNLP and LingPipe we usedthe existing 3-entity systems, with the ORGANI-ZATION tag collapsed into OTHER (as it was in ourguidelines; instances of ORGANIZATION are rarein literature).
Since the exact segmentation guide-lines likely varied across these systems?in par-ticular, we found that Stanford CoreNLP of-ten left off the title in names such as Mr. Smith?and we didn?t want to focus on these issues, wedid not require exact matches of our name seg-mentation; instead, we consider the entire name asPERSON or LOCATION if any of the tokens weretagged as such (names with both tags were con-sidered OTHER).
For our system (LitNER), wetest a version where only the immediate sentencecontext is used (?sentence?
), and versions basedon text context (?text?)
with or without our phraseimprovement (??phrase?
).We evaluate using two standard metrics: accu-racy (?Acc?
), and macroaveraged F-score (?FM?
).5 ResultsThe results in Table 2 show that our system eas-ily bests the off-the-shelf systems when it is given4https://opennlp.apache.org/5http://alias-i.com/lingpipe347the contextual information from the entire text; thedifference is more stark for accuracy (+0.085 ab-solute), though consistent for FM(+0.041 abso-lute).
Stanford CoreNLP is the only compet-itive off-the-shelf system?the other two are fartoo conservative when encountering names theyhaven?t seen before.
LitNER is also clearly betterthan the Brown clusters it was trained on, partic-ularly for FM(+0.120 absolute).
With regards todifferent options for LitNER, we see a major ben-efit from considering all occurrences of the namein the texts rather than just the one we are testingon (Section 3.3), and a more modest benefit fromusing the information on parts of phrases takenfrom the Brown clustering (Section 3.4).For the segmentation errors, we comparedour corrected segmentations with the segmen-tation provided by the CRF-based StanfordCoreNLP system, our best competitor.
Only 2 ofthe 15 were segmented correctly by StanfordCoreNLP.
This potential 0.002 improvement istiny compared to the 0.085 difference in accuracybetween the two systems.6 DiscussionAspects of the method presented here could the-oretically be applied to NER in other genres andother languages, but one important point we wishto make is that our approach clearly takes advan-tage of specific properties of (English) literature.The initial rule-based segmentation, for instance,depends on reliable capitalization of names, whichis often not present in social media, or in most non-European languages.
We have found more subtlegenre effects as well: for comparison, we appliedthe preliminary steps of our approach to anothercorpus of published texts which is of compara-ble (token) size to the Project Gutenberg corpus,namely the Gigaword newswire corpus (Graff andCieri, 2003), and noted degraded performance forboth segmentation and Brown clustering.
With re-spect to the former, the obvious issue is consid-erably more complex proper nouns phrases suchas governmental organizations and related titles.For the latter, there were several clusters in the top10 (including the first one) which corresponded toLOCATION, while the first (fairly) clean PERSONcluster was the 15th largest; in general, individualpeople, organizations, and other groupings of peo-ple (e.g.
by country of origin) were not well dis-tinguished by Brown clustering in the Gigawordcorpus, at least not with the same low number ofclusters that worked well in the Project Gutenbergcorpus.Also less than promising is the potential forusing text-level classification in other genres:whereas the average number of token occurrencesof distinct name types within a single text in theProject Gutenberg corpus is 5.9, this number isjust 1.6 for the much-shorter texts of the Giga-word corpus.
Except in cases where it is possibleto collapse texts into appropriately-sized groupswhere the use of a particular name is likely to beboth common and consistent?an example mightbe a collection of texts written by a single au-thor, which in social media such as Twitter seemsto obey the classic one-sense-per-discourse rule(Gella et al, 2014)?it?s not clear that this ap-proach can be applied successfully in cases wheretexts are relatively short, which is a far more com-mon situation.
We also note that relying primarilyon contextual classification while eschewing re-sources such as gazetteers makes much less senseoutside the context of fiction; we would expect rel-atively few fictitious entities in most genres.LitNER tags names into only two main classes,PERSON and LOCATION, plus a catch-all OTHER.This coarse-grained tag set reflects not only thepractical limitations of the method, but also wherewe believe automatic methods have potential toprovide useful information for literary analysis.The other clusters in Table 1 reflect word cate-gories which are relatively closed-class and muchless central to the fictional narratives as characterand setting; we don?t see a compelling case fortagging them.
When these and non-entities are ex-cluded from OTHER, what remains is eclectic, in-cluding names referring to small groups of people(e.g.
families), animals, gods, ships, and titles ofother works of literature.7 ConclusionIn this paper, we have presented LitNER, an NERsystem targeted specifically at fiction.
Our resultsshow that a simple classifier, trained only withnoisy examples derived in an unsupervised fash-ion, can easily beat a general-purpose supervisedsystem, provided it has access to the full contextof the text.
Finally, we note that the NER taggingprovided by LitNER has been integrated into theGutenTag tool (as of version 0.1.4).66See http://www.projectgutentag.org348ReferencesApoorv Agarwal, Anup Kotalwa, and Owen Rambow.2013.
Automatic extraction of social networks fromliterary text: A case study on Alice in Wonderland.In The Proceedings of the 6th International JointConference on Natural Language Processign (IJC-NLP ?13).Mariona Coll Ardanuy and Caroline Sporleder.
2015.Clustering of novels represented as social networks.Linguistic Issues in Language Technology, 12(4).David Bamman, Ted Underwood, and Noah A. Smith.2014.
A Bayesian mixed effects model of literarycharacter.
In Proceedings of the 52st Annual Meet-ing of the Association for Computational Linguistics(ACL ?14).Julian Brooke, Adam Hammond, and Graeme Hirst.2015.
GutenTag: An NLP-driven tool for digital hu-manities research in the Project Gutenberg corpus.In Proceedings of the 4nd Workshop on Computa-tional Literature for Literature (CLFL ?15).Peter F. Brown, Peter V. deSouza, Robert L. Mer-cer, Vincent J. Della Pietra, and Jenifer C. Lai.1992.
Class-based n-gram models of natural lan-guage.
Computational Linguistics, 18(4):467?479.Leon Derczynski, Sean Chester, and Kenneth S. Bgh.2015.
Tune your Brown clustering, please.
In Pro-ceedings of Recent Advances in Natural LanguageProcessing (RANLP 15).Micha Elsner, Eugene Charniak, and Mark Johnson.2009.
Structured generative models for unsuper-vised named-entity clustering.
In Proceedings ofthe 2009 Annual Conference of the North AmericanChapter of the Association for Computational Lin-guistics: Human Language Technologies (NAACL?09).David K. Elson, Nicholas Dames, and Kathleen R.McKeown.
2010.
Extracting social networks fromliterary fiction.
In Proceedings of the 48th AnnualMeeting of the Association for Computational Lin-guistics (ACL ?10).Jenny Rose Finkel, Trond Grenager, and ChristopherManning.
2005.
Incorporating non-local informa-tion into information extraction systems by gibbssampling.
In Proceedings of the 43rd Annual Meet-ing on Association for Computational Linguistics(ACL ?05).William A. Gale, Kenneth W. Church, and DavidYarowsky.
1992.
One sense per discourse.
In Pro-ceedings of the 4th DARPA Speech and Natural Lan-guage Workshop.Spandana Gella, Paul Cook, and Timothy Baldwin.2014.
One sense per tweeter ... and other lexicalsemantic tales of twitter.
In Proceedings of the 14thConference of the European Chapter of the Associa-tion for Computational Linguistics.David Graff and Christopher Cieri.
2003.
English Gi-gaword.
Linguistic Data Consortium.Hua He, Denilson Barbosa, and Grzegorz Kondrak.2013.
Identification of speakers in novels.
In Pro-ceedings of the 51st Annual Meeting of the Associa-tion for Computational Linguistics (ACL ?13).Percy Liang.
2005.
Semi-supervised learning for nat-ural language.
Master?s thesis, MIT.Mitchell P. Marcus, Beatrice Santorini, and Mary AnnMarcinkiewicz.
1993.
Building a large annotatedcorpus of English: the Penn treebank.
Computa-tional Linguistics, 19(2):313?330.Tara McIntosh, Lars Yencken, James R. Curran, andTimothy Baldwin.
2011.
Relation guided bootstrap-ping of semantic lexicons.
In Proceedings of the49th Annual Meeting of the Association for Com-putational Linguistics: Human Language Technolo-gies (ACL HLT 2011).Scott.
Miller, Jethran.
Guinness, and Alex Zamanian.2004.
Name tagging with word clusters and discrim-inative training.
In Proceedings of the 2004 Confer-ence of the North American Chapter of the Associ-ation for Computational Linguistics: Human Lan-guage Technologies (NAACL HLT ?13).David Nadeau, Peter D. Turney, and Stan Matwin.2006.
Unsupervised named-entity recognition:Generating gazetteers and resolving ambiguity.
InProceedings of the 19th International Conference onAdvances in Artificial Intelligence: Canadian Soci-ety for Computational Studies of Intelligence, AI?06.F.
Pedregosa, G. Varoquaux, A. Gramfort, V. Michel,B.
Thirion, O. Grisel, M. Blondel, P. Pretten-hofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Pas-sos, D. Cournapeau, M. Brucher, M. Perrot, andE.
Duchesnay.
2011.
Scikit-learn: Machine learn-ing in Python.
Journal of Machine Learning Re-search, 12:2825?2830.Lev Ratinov and Dan Roth.
2009.
Design chal-lenges and misconceptions in named entity recog-nition.
In Proceedings of the Thirteenth Confer-ence on Computational Natural Language Learning(CoNLL ?09).Alan Ritter, Sam Clark, Mausam, and Oren Etzioni.2011.
Named entity recognition in tweets: An ex-perimental study.
In Proceedings of the 2011 Con-ference on Empirical Methods in Natural LanguageProcessing (EMNLP 2011).Michael Thelen and Ellen Riloff.
2002.
A bootstrap-ping method for learning semantic lexicons usingextraction pattern contexts.
In Proceedings of the2002 Conference on Empirical Methods in NaturalLanguage Processing (EMNLP 2002).Hardik Vala, David Jurgens, Andrew Piper, and DerekRuths.
2015.
Mr. Bennet, his coachman, and theArchbishop walk into a bar but only one of them gets349recognized: On the difficulty of detecting charactersin literary texts.
In Proceedings of the 2015 Con-ference on Empirical Methods in Natural LanguageProcessing (EMNLP ?15).Byron C. Wallace.
2012.
Multiple narrative disentan-glement: Unraveling Infinite Jest.
In Proceedings ofthe 2012 Conference of the North American Chap-ter of the Association for Computational Linguistics:Human Language Technologies (NAACL-HLT ?12).350
