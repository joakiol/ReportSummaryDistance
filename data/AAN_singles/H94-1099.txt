Automatic Extraction of Grammars From Annotated TextSalim Roukos, Principal Investigatorroukos  @ watson.
ibm.cornIBM T.J. Watson Research CenterEO.
Box  704York town Heights,  NY  10598Project GoalsThe primary objective of this project is to develop a ro-bust, high-performance parser for English by automaticallyextracting a grammar from an annotated corpus of bracketedsentences, called the q~eeebank.
The project is a collabora-tion between the IBM Continuous Speech Recognition Groupand the University of Pennsylvania Department of ComputerSciences t. Our initial focus is the domain of computer man-uals with a vocabulary of 3000 words.
We use a Treebankthat was developed jointly by IBM and the University of Lan-caster, England.In this past year, we have demonstrated that our automaticallybuilt parser produces parses without crossing brackets for 78%of a blind test set.
This improves on the 69% that our manuallybuilt grammar-based parser \[1\] produces.
The grammar hadbeen crafted by a grammarian by examining the same trainingset as the automatically built parser over a period of more than3 years.Parsing ModelTraditionally, parsing relies on a grammar to determine a setof parse trees for a sentence and typically uses a scoringmechanism based on either ule preference or a probabilisticmodel to determine a preferred parse.
In this conventionalapproach, a linguist must specify the basic constituents, therules for combining basic constituents into larger ones, andthe detailed conditions under which these rules may be used.Instead of using a grammar, we rely on a probabilistic model,p(TIW), for the probability that a parse tree, T, is a parsefor sentence W. We use data from the Treebank, with appro-priate statistical modeling techniques, to capture implicitlythe plethora of linguistic details necessary to correctly parsemost sentences.
In our model of parsing, we associate withany parse tree a set of bottom-up derivations; each deriva-tion describing a particular order in which the parse tree isconstructed.
Our parsing model assigns a probability to aderivation, denoted by p(dlW).
The probability of a parse treeis the sum of the probability of all derivations leading to theparse tree.
The probability of a derivation is a product ofz Co-Principal Investigators: Mark Liberman and Mitchell Marcusprobabilities, one for each step of the derivation.
These stepsare of three types:a tagging step: where we want the probability of tagginga word with a tag in the context of the derivation up tothat point.a labeling step: where we want the probability of assign-ing a non terminal label to a node in the derivation.an extension step: where we want to determine the prob~ability that a labeled node is extended, for example, Lothe left or right (i.e.
to combine with the preceding orfollowing constituents).The probability of a step is determined by a decision treeappropriate to the type of the step.
The three decision treesexamine the derivation up to that point to determine the prob-ability of any particular step.The parsing models were trained on 28,000 sentences fromthe Computer Manuals domain, and tested on 1100 unseensentences of length 1 - 25 words.
On this test set, the parserproduced the correct parse, i.e.
a parse which matched thetreebank parse exactly, for 38% of the sentences.
Ignoringpart-of-speech tagging errors, it produced the correct parsetree for 47% of the sentences.Plans for the Coming YearWe plan to continue working with our new parser by complet-ing the following tasks:?
implement a set of detailed questions to capture infor-mation about conjunction, prepositional ttachment, etc.?
improve the speed of the search strategy of the parser.References1.
Black, E., Jelinek, F., Lafferty, \]., Magerman, D. M., Mercer,R., and Roukos, S., 1993.
Towards History-based Grammars:Using Richer Models for Probabilistic Parsing.
In Proceed-ings of the Association for Computational Linguistics, 1993.Columbus, Ohio.456
