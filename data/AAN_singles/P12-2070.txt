Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 359?362,Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational LinguisticsAssessing the Effect of Inconsistent Assessors on Summarization EvaluationKarolina OwczarzakNational Institute of Standards and TechnologyGaithersburg, MD 20899karolina.owczarzak@gmail.comPeter A. RankelUniversity of MarylandCollege Park, Marylandrankel@math.umd.eduHoa Trang DangNational Institute of Standards and TechnologyGaithersburg, MD 20899hoa.dang@nist.govJohn M. ConroyIDA/Center for Computing SciencesBowie, Marylandconroy@super.orgAbstractWe investigate the consistency of human as-sessors involved in summarization evaluationto understand its effect on system ranking andautomatic evaluation techniques.
Using TextAnalysis Conference data, we measure anno-tator consistency based on human scoring ofsummaries for Responsiveness, Readability,and Pyramid scoring.
We identify inconsis-tencies in the data and measure to what ex-tent these inconsistencies affect the rankingof automatic summarization systems.
Finally,we examine the stability of automatic metrics(ROUGE and CLASSY) with respect to theinconsistent assessments.1 IntroductionAutomatic summarization of documents is a re-search area that unfortunately depends on humanfeedback.
Although attempts have been made at au-tomating the evaluation of summaries, none is sogood as to remove the need for human assessors.Human judgment of summaries, however, is not per-fect either.
We investigate two ways of measuringevaluation consistency in order to see what effect ithas on summarization evaluation and training of au-tomatic evaluation metrics.2 Assessor consistencyIn the Text Analysis Conference (TAC) Summariza-tion track, participants are allowed to submit morethan one run (usually two), and this option is of-ten used to test different settings or versions of thesame summarization system.
In cases when the sys-tem versions are not too divergent, they sometimesproduce identical summaries for a given topic.
Sum-maries are randomized within each topic before theyare evaluated, so the identical copies are usually in-terspersed with 40-50 other summaries for the sametopic and are not evaluated in a row.
Given that eachtopic is evaluated by a single assessor, it then be-comes possible to check assessor consistency, i.e.,whether the assessor judged the two identical sum-maries in the same way.For each summary, assessors conduct contentevaluation according to the Pyramid framework(Nenkova and Passonneau, 2004) and assign it Re-sponsiveness and Readability scores1, so assessorconsistency can be checked in these three areas sep-arately.
We found between 230 (in 2009) and 430(in 2011) pairs of identical summaries for the 2008-2011 data (given on average 45 topics, 50 runs, andtwo summarization conditions: main and update),giving in effect anywhere from around 30 to 60 in-stances per assessor per year.
Using Krippendorff?salpha (Freelon, 2004), we calculated assessor con-sistency within each year, as well as total consis-tency over all years?
data (for those assessors whoworked multiple years).
Table 1 shows rankings ofassessors in 2011, based on their Readability, Re-sponsiveness, and Pyramid judgments for identicalsummary pairs (around 60 pairs per assessor).Interestingly, consistency values for Readabilityare lower overall than those for Responsiveness andPyramid, even for the most consistent assessors.Given that Readability and Responsiveness are eval-uated in the same way, i.e.
by assigning a numeri-cal score according to detailed guidelines, this sug-1http://www.nist.gov/tac/2011/Summarization/Guided-Summ.2011.guidelines.html359ID Read ID Resp ID PyrG 0.867 G 0.931 G 0.975D 0.866 D 0.875 D 0.970A 0.801 H 0.808 H 0.935H 0.783 A 0.750 A 0.931F 0.647 F 0.720 E 0.909C 0.641 E 0.711 C 0.886E 0.519 C 0.490 F 0.872Table 1: Annotator consistency in assigning Readabilityand Responsiveness scores and in Pyramid evaluation, asrepresented by Krippendorff?s alpha for interval values,on 2011 data.gests that Readability as a quality of text is inher-ently more vague and difficult to pinpoint.On the other hand, Pyramid consistency valuesare generally the highest, which can be explainedby how the Pyramid evaluation is designed.
Evenif the assessor is inconsistent in selecting Sum-mary Content Units (SCUs) across different sum-maries, as long as the total summary weight is sim-ilar, the summary?s final score will be similar, too.2Therefore, it would be better to look at whether as-sessors tend to find the same SCUs (information?nuggets?)
in different summaries on the same topic,and whether they annotate them consistently.
Thiscan be done using the ?autoannotate?
function ofthe Pyramid process, where all SCU contributors(selected text strings) from already annotated sum-maries are matched against the text of a candidate(un-annotated) summary.
The autoannotate func-tion works fairly well for matching between extrac-tive summaries, which tend to repeat verbatim wholesentences from source documents.For each summary in 2008-2011 data, we autoan-notated it using all remaining manually-annotatedsummaries from the same topic, and then we com-pared the resulting ?autoPyramid?
score with thescore from the original manual annotation for thatsummary.
Ideally, the autoPyramid score shouldbe lower or equal to the manual Pyramid score: itwould mean that in this summary, the assessor se-lected as relevant all the same strings as s/he foundin the other summaries on the same topic, plus possi-bly some more information that did not appear any-2The final score is based on total weight of all SCUs foundin the summary, so the same weight can be obtained by select-ing a larger number of lower-weight SCUs or a smaller numberof higher-weight SCUs (or the same number of similar-weightSCUs which nevertheless denote different content).Figure 1: Annotator consistency in selecting SCUs inPyramid evaluation, as represented by the difference be-tween manual Pyramid and automatic Pyramid scores(mP-aP), on 2011 data.where else.
If the autoPyramid score is higher thanthe manual Pyramid score, it means that either (1)the assessor missed relevant strings in this summary,but found them in other summaries; or (2) the stringsselected as relevant elsewhere in the topic were acci-dental, and as such not repeated in this summary.
Ei-ther way, if we then average out score differences forall summaries for a given topic, it will give us a goodpicture of the annotation consistency in this partic-ular topic.
Higher average autoPyramid scores sug-gest that the assessor was missing content, or other-wise making frequent random mistakes in assigningcontent.
Figure 1 shows the macro-average differ-ence between manual Pyramid scores and autoPyra-mid scores for each assessor in 2011.3 For the mostpart, it mirrors the consistency ranking from Table1, confirming that some assessors are less consistentthan others; however, certain differences appear: forinstance, Assessor A is one of the most consistent inassigning Readability scores, but is not very good atselecting SCUs consistently.
This can be explainedby the fact that the Pyramid evaluation and assigningReadability scores are different processes and mightrequire different skills and types of focus.3 Impact on evaluationSince human assessment is used to rank participat-ing summarizers in the TAC Summarization track,3Due to space constraints, we report figures for only 2011,but the results for other years are similar.360Pearson?s r Spearman?s rho-1 worst -2 worst -1 worst -2 worstReadability 0.995 0.993 0.988 0.986Responsiveness 0.996 0.989 0.986 0.946Pyramid 0.996 0.992 0.978 0.960mP-aP 0.996 0.987 0.975 0.943Table 2: Correlation between the original summarizerranking and the ranking after excluding topics by one ortwo worst assessors in each category.we should examine the potential impact of incon-sistent assessors on the overall evaluation.
Becausethe final summarizer score is the average over manytopics, and the topics are fairly evenly distributedamong assessors for annotation, excluding noisytopics/assessors has very little impact on summa-rizer ranking.
As an example, consider the 2011 as-sessor consistency data in Table 1 and Figure 1.
Ifwe exclude topics by the worst performing assessorfrom each of these categories, recalculate the sum-marizer rankings, and then check the correlation be-tween the original and newly created rankings, weobtain results in Table 2.Although the impact on evaluating automaticsummarizers is small, it could be argued that exclud-ing topics with inconsistent human scoring will havean impact on the performance of automatic evalua-tion metrics, which might be unfairly penalized bytheir inability to emulate random human mistakes.Table 3 shows ROUGE-2 (Lin, 2004), one of thestate-of-the-art automatic metrics used in TAC, andits correlations with human metrics, before and af-ter exclusion of noisy topics from 2011 data.
Theresults are fairly inconclusive: it seems that in mostcases, removing topics does more harm than good,suggesting that the signal-to-noise ratio is still tippedin favor of signal.
The only exception is Readability,where ROUGE records a slight increase in correla-tion; this is unsurprising, given that consistency val-ues for Readability are the lowest of all categories,and perhaps here removing noise has more impact.In the case of Pyramid, there is a small gain whenwe exclude the single worst assessor, but excludingtwo assessors results in a decreased correlation, per-haps because we remove too much valid informationat the same time.A different picture emerges when we examinehow well ROUGE-2 can predict human scores onthe summary level.
We pooled together all sum-Readability Responsiveness Pyramid mP-aPbefore 0.705 0.930 0.954 0.954-1 worst 0.718 0.921 0.961 0.942-2 worst 0.718 0.904 0.952 0.923Table 3: Correlation between the summarizer rankingsaccording to ROUGE-2 and human metrics, before andafter excluding topics by one or two worst assessors inthat category.Readability Responsiveness Pyramid mP-aPbefore 0.579 0.694 0.771 0.771-1 worst 0.626 0.695 0.828 0.752-2 worst 0.628 0.721 0.817 0.741Table 4: Correlation between ROUGE-2 and human met-rics on a summary level before and after excluding topicsby one or two worst assessors in that category.maries annotated by each particular assessor and cal-culated the correlation between ROUGE-2 and thisassessor?s manual scores for individual summaries.Then we calculated the mean correlation over allassessors.
Unsurprisingly, inconsistent assessorstend to correlate poorly with automatic (and there-fore always consistent) metrics, so excluding oneor two worst assessors from each category increasesROUGE?s average per-assessor summary-level cor-relation, as can be seen in Table 4.
The only ex-ception here is when we exclude assessors based ontheir autoPyramid performance: again, because in-consistent SCU selection doesn?t necessarily trans-late into inconsistent final Pyramid scores, exclud-ing those assessors doesn?t do much for ROUGE-2.4 Impact on trainingAnother area where excluding noisy topics might beuseful is in training new automatic evaluation met-rics.
To examine this issue we turned to CLASSY(Rankel et al, 2011), an automatic evaluation met-ric submitted to TAC each year from 2009-2011.CLASSY consists of four different versions, eachaimed at predicting a particular human evaluationscore.
Each version of CLASSY is based on oneof three regression methods: robust regression, non-negative least squares, or canonical correlation.
Theregressions are calculated based on a collection oflinguistic and content features, derived from thesummary to be scored.CLASSY requires two years of marked data toscore summaries in a new year.
In order to predict361the human metrics in 2011, for example, CLASSYuses the human ratings from 2009 and 2010.
It firstconsiders each subset of the features in turn, and us-ing each of the regression methods, fits a model tothe 2009 data.
The subset/method combination thatbest predicts the 2010 scores is then used to pre-dict scores for 2011.
However, the model is first re-trained on the 2010 data to calculate the coefficientsto be used in predicting 2011.First, we trained all four CLASSY versions onall available 2009-2010 topics, and then trainedagain excluding topics by the most inconsistent as-sessor(s).
A different subset of topics was ex-cluded depending on whether this particular versionof CLASSY was aiming to predict Responsiveness,Readability, or the Pyramid score.
Then we testedCLASSY?s performance on 2011 data, ranking ei-ther automatic summarizers (NoModels case) or hu-man and automatic summarizers together (AllPeerscase), separately for main and update summaries,and calculated its correlation with the metrics it wasaiming to predict.
Table 5 shows the result of thiscomparison.
For Pyramid, (a) indicates that ex-cluded topics were selected based on Krippendorff?salpha, and (b) indicates that topics were excludedbased on their mean difference between manual andautomatic Pyramid scores.The results are encouraging; it seems that remov-ing noisy topics from training data does improve thecorrelations with manual metrics in most cases.
Thegreatest increase takes place in CLASSY?s correla-tions with Responsiveness for main summaries inAllPeers case, and for correlations with Readabil-ity.
While none of the changes are large enoughto achieve statistical significance, the pattern of im-provement is fairly consistent.5 ConclusionsWe investigated the consistency of human assessorsin the area of summarization evaluation.
We con-sidered two ways of measuring assessor consistency,depending on the metric, and studied the impact ofconsistent scoring on ranking summarization sys-tems and on the performance of automatic evalu-ation systems.
We found that summarization sys-tem ranking, based on scores for multiple topics,was surprisingly stable and didn?t change signifi-NoModels AllPeersmain update main updatePyramidCLASSY1 Pyr 0.956 0.898 0.945 0.936CLASSY1 Pyr new (a) 0.950 0.895 0.932 0.955CLASSY1 Pyr new (b) 0.960 0.900 0.940 0.955ResponsivenessCLASSY2 Resp 0.951 0.903 0.948 0.963CLASSY2 Resp new 0.954 0.907 0.973 0.950CLASSY4 Resp 0.951 0.927 0.830 0.949CLASSY4 Resp new 0.943 0.928 0.887 0.946ReadabilityCLASSY3 Read 0.768 0.705 0.844 0.907CLASSY3 Read new 0.793 0.721 0.858 0.906Table 5: Correlations between CLASSY and human met-rics on 2011 data (main and update summaries), beforeand after excluding most inconsistent topic from 2009-2010 training data for CLASSY.cantly when several topics were removed from con-sideration.
However, on a summary level, remov-ing topics scored by the most inconsistent assessorshelped ROUGE-2 increase its correlation with hu-man metrics.
In the area of training automatic met-rics, we found some encouraging results; removingnoise from the training data allowed most CLASSYversions to improve their correlations with the man-ual metrics that they were aiming to model.ReferencesDeen G. Freelon.
2010.
ReCal: Intercoder ReliabilityCalculation as a Web Service.
International Journalof Internet Science, Vol 5(1).Chin-Yew Lin.
2004.
ROUGE: A Package for Auto-matic Evaluation of Summaries.
Text SummarizationBranches Out: Proceedings of the ACL-04 Workshop,78?81.
Barcelona, Spain.Ani Nenkova and Rebecca J. Passonneau.
2004.
Evaluat-ing content selection in summarization: The Pyramidmethod.
Proceedings of the Human Language Tech-nology Conference of the North American Chapter ofthe Association for Computational Linguistics, 145?152.
Boston, MA.Rebecca J. Passonneau, Ani Nenkova, Kathleen McKe-own, and Sergey Sigelman.
2005.
Applying the Pyra-mid method in DUC 2005.
Proceedings of the 5thDocument Understanding Conference (DUC).
Van-couver, Canada.Peter A. Rankel, John M. Conroy, and Judith D.Schlesinger.
2012.
Better Metrics to AutomaticallyPredict the Quality of a Text Summary.
Proceedingsof the SIAM Data Mining Text Mining Workshop 2012.362
