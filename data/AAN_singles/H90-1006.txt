The Use of Relative Durationin Syntactic DisambiguationM.
Ostendorf  t P. J.
Prices J.
Bear:  C .W.
Wightmantt Boston University44 Cummington St.Boston, MA 02215Sl:tI International333 Ravenswood Ave.Menlo Park, CA 94025AbstractWe describe the modification of a grammar to take ad-vantage of prosodic information automatically extractedfrom speech.
The work includes (1) the developmentof an integer "break index" representation f prosodicphrase boundary information, (2) the automatic detec-tion of prosodic phrase breaks using a hidden Markovmodel on relative duration of phonetic segments, and(3) the integration of the prosodic phrase break informa-tion in SRI's Spoken Language System to rule out alter-native parses in otherwise syntactically ambiguous sen-tences.
Initial experiments using ambiguous sentencesread by radio announcers achieved good results in bothdetection and parsing.
Automatically detected phrasebreak indices had a correlation greater than 0.86 withhand-labeled data for speaker-dependent models; and,in a subset of sentences with preposition ambiguities,the number of parses was reduced by 25% with a simplegrammar modification.Introduction"Prosody," the suprasegmental information in speech,i.e., information that cannot be localized to a specificsound segment, can mark lexical stress, identify phrasingbreaks and provide information useful for semantic inter-pretation.
Although all of these aspects may be usefulin spoken language systems, particularly important areprosodic phrase breaks which can provide cues to syntac-tic structure to help select among competing hypotheses,and thus help to disambiguate otherwise ambiguous sen-tences.
In speech understanding applications, informa-tion such as prosody that aids disambiguation, is par-ticularly important, since speech input, as opposed totext, introduces a vast increase in the amount of am-biguity a parser must face.
For example, Harringtonand Johnstone \[7\] found that even when all phonemesare correctly identified, the indeterminancy of phonemesequences when word boundaries are unknown yields inexcess of 1000 word string parses for many of their 4 to 10word sentences.
Moreover, these estimates rise dramati-cally as indeterminancy is introduced in the phoneme se-quences: only 2% of the same sentences had fewer than1,000 parses when phonetically similar phonemes wereclustered together (e.g., voiced stops).
This indetermi-nancy vastly increases the work for a parser.The work reported here focuses on the use of rela-tive duration of phonetic segments in the assignment ofsyntactic structure, assuming a known word sequence.Specifically, relative duration of phonemes i estimatedby a speech recognizer constrained to recognize the cor-rect string of words.
These duration values are then usedto compute phrase break indices, which are in turn usedto rule out alternative parses in otherwise syntacticallyambiguous sentences.
In this paper, we begin by pro-viding some theoretical background on prosodic phrasebreaks, and by describing a numerical representation fphrase breaks for use in speech understanding.
Next wedescribe algorithms for automatic recognition of thesebreak indices from speech for known text, and the mod-ification of a grammar to use these indices in parsing.Finally, we present experimental results based on am-biguous entences from three speakers, howing that theuse of prosodic information can significantly reduce thenumber of candidate syntactic parses by pruning mis-matches between prosody and syntax.Prosodic Phrase BreaksIn recent years, there have been significant advances inthe phonology of prosodic phrases.
While this work isnot yet explicit enough to provide rules for automaticallydetermining the prosodic phrase boundaries in speech, itis useful as a foundation for our computational models.Several researchers in linguistics have proposed hierar-chies of prosodic phrases \[9, 12, 10\].
Although not all lev-els of these hierarchies are universally accepted, our dataappear to provide vidence for prosodic words as individ-ual words or clitic groups, groupings of prosodic words,intermediate phrases, intonational phrases, groupings ofintermediate phrases as in parenthetical phrases, andsentences.
Since it is not clear how many of these levelswill be useful in speech understanding, we have repre-sented all seven possible types of boundaries, but focusinitially on the information in the highest levels, sen-tences and intonational phrases.In order to utilize this information in a parser, wedeveloped a numerical representation f this hierarchy25using a sequence of break indices between each word.
Abreak index encodes the degree of prosodic decouplingbetween eighboring words.
For example, an index of 0corresponds to cliticization, and an index of 6 representsa sentence boundary.
1 We found that these break indicescodld be labeled with very high consistency within andacross labelers.
We anticipate that the strongest bound-aries (highest level of the hierarchy) will be both easiestto detect and most useful in parsing, and will refer tothese boundaries (4-6) as major phrase boundaries.The acoustic ues to prosodic breaks vary according tothe different levels of the hierarchy.
For example, thereare phonological rules that apply across some boundariesof the hierarchy but not others (e.g., \[12, 18\]).
Into-nation cues mark intermediate and intonational phraseboundaries \[15, 3\].
Pauses and/or breaths may also markmajor phrase boundaries.
Our initial data indicate thatduration lengthening is a fairly reliable cue to phraseboundaries.
Prepausal engthening is observed in En-glish and many, but not all, other languages (see \[16\] fora summary).
Lengthening without a pause has been ob-served in English in clause-final position (e.g., \[5\]) andin phrase-final position, though lengthening of phones atthe ends of words is disputed in the literature (see \[8\]).The typical representation f syntactic structure is notidentical to prosodic phrase structure.
2 There is somework suggesting methods for predicting rhythmic struc-ture from syntactic structure, but not vice versa \[13, 6\].These results are complicated by the complex relation-ship between intonation and rhythm and the influenceof semantics on intonation.
Although people disagree onthe precise relationship between prosody and syntax, itis generally agreed that there is some relationship.
Wehave shown, for example, that prosody is used by listen-ers to choose the appropriate meaning of otherwise am-biguous sentences with an average accuracy of 86% \[11\].Although listener performance varied as a function ofthe type of syntactic ambiguities, reliability was high forleft vs. right attachment and particle/preposition ambi-guities, both difficult problems in parsing.
An exampleillustrates how the break indices resolve syntactic andword sequence ambiguities for two phonetically identicMsentences:?
Marge 0 would 1 never 2 deal 0 in 2 any 0 guys 6?
Marge 1 would 0 never 0 deal 3 in 0 any 0 guise 6Note that the break index between "deal" and "in"provides an indication of how tightly coupled the twowords are.
For "in" as a particle, we expect ighter con-nection to the preceding verb whose meaning is modifiedthan to the following phrase which is the object of thatverb.
For "in" as a preposition, we expect a tighter con-1 We have changed our scale f rom our previously reported sixlevels to the present  seven levels to achieve a more natura l  mapp ingbetween our  levels and  those descr ibed in the l inguist ics l i terature.2 Steedman,  however, \[14\] had  made some interest ing argumentsconcerning the appropr iateness  of categorial  g rammar  for reflectingprosodic s t ructure .nection to the following object of the preposition thanto the preceding verb.Use of Phrase Breaks in ParsingWith the goal of using prosodic phrase breaks to reducesyntactic ambiguity in a parser, we have developed an al-gorithm for automatically computing break indices, andwe have modified the structure of the grammar to in-corporate this information.
The current effort is focusedon demonstrating the feasibility of this approach; there-fore the problem is restricted in scope.
The techniqueswe have developed are extensible to more general cases;the positive results encourage us to relax some of theserestrictions.
In the next section we describe the basic ap-proach, which is based on the following simplifications.?
We assume knowledge of the orthographic wordtranscription of a sentence and the sentence bound-ary.
Word boundary ambiguities complicate the in-tegration architecture and were not considered inthis study, although this is an important issue toaddress in future work.?
Only relative duration is currently used as a cue fordetecting the break indices, though we expect o im-prove performance in later versions of the algorithmby utilizing other acoustic ues such as intonation.?
Only preposition ambiguities were investigated.The focus on preposition ambiguities was motivatedby the following facts:1.
Prepositions are very frequent: 80-90% of thesentences in our radio news database, in theresource management sentences, and in the.ATIS database contain at least one preposi-tional phrase.2.
Sentences with prepositions are usually syntac-tically ambiguous.3.
Our perceptual experiments suggest thatprosody could be used effectively in many sen-tences with preposition ambiguities.
We be-lieve that the techniques developed can beadapted to more general attachment ambigui-ties.?
The sentences examined were read by professionalFM radio announcers, o the prosody is somewhatmore exaggerated and probably easier to recognizethan it would be with nonprofessional speakers.Phrase  Break  Detect ionUsing a known word sequence as a tightly constrainedgrammar, a speech recognizer can be used to providetime Mignments for the phone sequence of a sentence.We have used the speaker-independent SRI DECIPHERsystem \[17\], which uses phonologicM rules to generatebushy pronunciation etworks that provide a more ac-curate phonetic transcription and alignment.2"7In earlier work \[11\], each phone duration was normal-ized according to speaker- and phone-dependent meansand variances.
Raw break features were generated byaveraging the normalized duration over the final sylla-ble coda of each word and adding a pause factor.
Lex-ically stressed and non-stressed vowels were separatedin the computation of means and variances.
Finally,these features were normalized (relative to the observedphone durations in the current sentence) to obtain inte-ger break indices with a range of 0 to 5.
These indiceshad a high correlation with hand-labeled data (0.85) andwere successfully used in a parser to reduce the numberof syntactic parses by 25% \[2\].
However, the normal-ization algorithm required knowledge of the raw breakfeatures for the entire sentence, which has the disad-vantages that the algorithm is non-causal and may notreflect sentence-internal changes in speaking rate.
In ad-dition, the algorithm used the full scale of break indices,so every sentence was constrained to have at least one 0break index.A new algorithm, using a hidden Markov model(HMM), was investigated for computing the break in-dices from the raw break features described above.
Thealgorithm is not strictly causal (in the same sense thatthe HMM recognizer is not causal - decisions are notmade until sometime after a word has been observed),but does not require any ad hoc scaling.
We anticipatethe time delay associated with HMM prosody decodingto be similar to delays associated with a speech recog-nizer.
In a slight variation from previous work, the rawbreak indices were computed from the rhyme (vowel nu-cleus + coda) of the final syllable instead of the codaalone.
This change did not have an effect on the cor-relation with hand labels.
A second, more importantdifference is that the phone duration means are adaptedaccording to a local speaking rate.
Local speaking rateis given by the average normalized durations over thelast M phones, excluding pauses, where M -- 50 wasdetermined experimentally.
The mean duration for eachphone is adjusted with each new observed phone accord-ing tofia m ~a ?
ar /Nwhere r is the speaking rate, N is a feedback coeffi-cient that is equal to 5000 at steady state, but variesat start-up for faster initial adaptation, g is the stan-dard deviation of the phone's duration, unadapted, and~ua represents the mean duration for phone a.A fully connected seven-state HMM is used to recog-nize break indices, given the raw break feature.
EachHMM state corresponds to a break index (state number= break index) and the output distribution in each statedescribes the raw indices observed while in that state.In this work, we investigated the use of Gaussian outputdistributions of the scalar break feature, but joint use ofseveral features in multivariate output distributions willbest utilize the power of the tIMM approach.
Viterbidecoding was used to obtain the state sequence for anutterance, corresponding to the break index sequence.The parameters of the break HMM were estimated intwo different ways, involving either supervised or unsu-pervised training.
By supervised training, we mean thatthe hand-labeled break indices are given, so the statesequence is fully observable and simple maximum likeli-hood estimation (as opposed to the Estimate-Maximize,or forward-backward, algorithm) is used.
In unsuper-vised training, no hand-labeled ata is used.
Mean out-put distributions of the states are initialized to valueson a scale that increases with the corresponding breakindex, and the transition probabilities were initializedto be essentially uniform.
The forward-backward algo-r ithm was then run, effectively clustering the states, toestimate the final output distribution paramaters.
A sur-prising and very encouraging result was that the unsu-pervised HMM correlated as well with the hand-labeleddata as did the HMM with supervised parameter esti-mates.In tegrat ion  Wi th  a ParserThe question of how best to incorporate prosodic in-formation into a grammar/parser  is a vast area of re-search.
The methodology used here is a novel approach,involving automatic modification of the grammar ulesto incorporate the break indices as a new grammaticalcategory.
We modified an existing, and reasonably largegrammar, the grammar used in SRI's spoken languagesystem.
The parser used is the Core Language Enginedeveloped at SRI in Cambridge.Several steps are involved in the grammar modifica-tion.
The first step is to systematically change all of therules of the form A ~ B C to the form A ~ B L ink  C,where L ink  is a new grammatical  category, that of theprosodic break indices.
Similarly all rules with morethan two right hand side elements need to have L inknodes interleaved at every juncture, e.g., a rule AB C D is changed into A ~ B Link1 C Link2 D.Next, allowance must be made for empty nodes, de-noted e. It is common practice to have rules of the formNP --~ e and PP  --+ e in order to handle wh-movementand relative clauses.
These rules necessitate the incor-poration into the modified grammar of a rule L ink  ~ e;otherwise, the sentence will not parse, because an emptynode introduced by the grammar will either not be pre-ceded by a link, or not followed by one.The introduction of empty links needs to be con-strained to avoid the introduction of spurious parses.If the only place the empty NP or PP could go is atthe end of the sentence, then the only place the emptyL ink  could go is right before it and no extra ambiguityis introduced.
However, if an empty wh-phrase couldbe posited at a place somewhere other than the end ofthe sentence, then there is ambiguity as to whether it ispreceded or followed by the empty link.For instance, for the sentence, "What did you see onSaturday?"
the parser would find both of the followingpossibilities:?
What L did L you L see L empty-NP empty-L on LSaturday?28?
What L did L you L see empty-L empty-NP L on LSaturday?Hence the grammar must be made to automatically ruleout half of these possibilities.
This can be done by con-straining every empty link to be followed immediatelyby an empty wh-phrase, or a constituent containing anempty wh-phrase on its left branch.
It is fairly straight-forward to incorporate this into the routine that au-tomatically modifies the grammar.
The rule that in-troduces empty links gives them a feature-value pair:empty-link = y.
The rules that introduce other emptyconstituents are modified to add to the constituent thefeature-value pair: trace-on-left-branch = y.
The links0 through 6 are given the feature-value pair empty-link = n. The default value for trace-on-left-branchis set to n so that all words in the lexicon have thatvalue.
Rules of the form Ao ~ At  Link1 .
.
.
An are mod-ified to insure that A0 and A1 have the same value forthe feature trace-on-left-branch.
Additionally, if Linklhas empty-link = y then Ai+l must have trace-on-left-branch = y.
These modifications, incorporated into thegrammar modifying routine, suffice to eliminate the spu-rious ambiguity.Additional changes to the grammar were necessary toactually make use of the prosodic break indices.
In thisinitial endeavor, a very conservative change was made af-ter examining the break indices on a set of sentences withpreposition ambiguities.
The rule N ~ N L ink  PP  waschanged to require the value of the link to be between 0and 2 inclusive for the rule to apply.
A similar changewas made to the rule VP ~ V L ink  PP ,  except thatthe link was required to have the value of either 0 or 1.Experimental ResultsWe have achieved encouraging results both in detectionof break indices and in their use in parsing.
The au-tomatic detection algorithm yields break labels havinga high correlation with hand-labeled ata for the vari-ous algorithms described.
In addition, when we chose asubset (14) of these sentences exhibiting prepositionalphrase attachment ambiguities or preposition/particleambiguities, we found that the incorporation of theprosodic information in the Sl:tI grammar esulted in areduction of about 25% in the number of parses, withoutruling out any correct parses.
For sentences to which theprosodic constraints on the rules actually applied, thedecrease in number of parses was about 50%.
In manycases the use of prosodic information allowed the parserto correctly identify a unique parse.
Below we describethe results in more detail.CorpusThe first corpus we examined consisted of a collectionof phonetically ambiguous, structurally different pairsof sentences.
The sentence pairs were read by threefemale professional radio announcers in disambiguatingcontexts.
In order to discourage unnatural exaggerationsCorrelation with Hand LabelsSpeaker SD super.
SD unsup.
SI unsup.F1A 0.89 0.88 0.89F2B 0.86 0.87 \] 0.85Table 1: Average correlation between automatically a-beled break indices and hand-labeled break indices, us-ing different methods of training.
SD,SI = speaker-(in)dependent; super.
= supervised training with hand-labeled data; unsup.
= unsupervised training.of any differences between the sentences, the materialswere recorded in different sessions with several days inbetween.
In each session only one sentence of each pairoccurred.
Seven types of structural ambiguity were in-vestigated: parentheticals, apposition, main-main versusmain-subordinate clauses, tags, near versus far attach-ment, left versus right attachment, and particles versusprepositions.
Each type of ambiguity was represented byfive pairs of sentences.Detection AlgorithmIn finding break indices for the ambiguous sentence pairs,the seventy sentences were concatenated together asthough the speaker read them as a paragraph.
Con-catenation allowed the algorithm to avoid initializationfor every sentence, but since the speaking rate is thentracked across several sentences that were not actuallyread in connection, there was probably some error as-sociated with estimating the speaking rate factor.
TheHMM was used to generate break indices, and the re-sults were evaluated according to how highly correlatedthe automatically generated labels were with the hand-labeled data.
The correlation reported here is the av-erage of the sample correlation for each sentence.
Theexperiments yielded good accuracy on the detected breaklabels, but also some important results on unsupervisedtraining and speaker-independence.In comparing the supervised and unsupervised param-eter estimation approaches for the ttMM, we found thatboth yielded break indices with similar correlation to thehand labeled indices (Table 1).
In addition, the indicesobtained using the two training approaches were veryhighly correlated with each other (> 0.92).
This is avery important result, because it suggests that we maybe able to automatically estimate models without requir-ing hand-labeled ata.
Results for speaker-dependenttraining on two speakers are summarized in Table 1.For the moment, we are mainly interested in detect-ing major phrase breaks (4-6) and not the confusionsbetween these levels, since the parser uses major breaksas constraints on grammar ules.
Using supervised pa-rameter estimation, the false rejection/false acceptancerates are 14%/3% for speaker F1A and 21%/6% forspeaker F2B.
The unsupervised parameter estimationalgorithm has a bias towards more false rejections and29fewer false acceptances.
The most important confusionswere between minor phrase breaks (2,3) and intonationalphrases (4).
Since a boundary tone is an important cueto an intonational phrase, we expect performance to im-prove significantly when intonation is included as a fea-ture.In the experiments comparing supervised to unsuper-vised training, speaker-dependent phone means and vari-ances were estimated from the same data used to trainthe HMM as well as to evaluate the correlation, becauseof the limited amount of speaker-dependent data avail-able.
Though the speaker-dependent experiments wereoptimistic in that they involved testing on the trainingdata, the results are meaningful in the sense that otherspeaker-independent experiments howed the parame-ters were robust with respect to a change in speakers.Using unsupervised training with two speakers to esti-mate both HMM parameters and duration means andvariances for normalization for a different speaker, thecorrelation of the resulting automatically detected breakindices with the hand-labeled indices was close to thespeaker-dependent case (Table 1).
Also, the speaker-dependent predictions and speaker-independent predic-tions were highly correlated with each other (0.96).
Weconclude that, at least for these radio news announcers,the algorithm seems to be somewhat robust with respectto different speakers.
Of course, the news announcershad similar reading styles, and the hand-labeled atafor two speakers had a correlation of 0.94.Overall, the HMM provided improvement over the pre-viously reported algorithm, with a correlation of 0.90compared to 0.86 for six levels.
On the other hand, therewas a small reduction in the correlation when using sevenlevels of breaks (0.87) compared to six levels (0.90).Use  in  Pars ingA subset of 14 sentences with preposition ambiguitieswas chosen for evaluating the integration of the breakindices in the parser.
We evaluated the results by com-paring the number of parses obtained with and with-out the prosodic constraints on the grammar ules, andnoted the differences in parse times.
On average, the in-corporation of prosody resulted in a reduction of about25% in the number of parses found, with an average in-crease in parse times of 37%.
The fact that parse timesincrease is due to the way in which prosodic informationis incorporated.
The parser does a certain amount ofwork for each word, and the effect of adding break in-dices to the sentence is essentially to double the numberof words that the parser must process.
It may be pos-sible to optimize the parser to significantly reduce thisoverhead.The sentences were divided into those to which theadditional constraints would apply (type 'a') and thoseabout which the constraints had nothing to say (type'b').
Essentially the constraints block attachment ifthere is too large a break index between a noun anda following prepositional phrase or between a verb andSentence Humans% correctla 812a 943a 944a 875a 1006a 567a 100TOTAL 87Number of Parses Parse TimeNo With No WithPros.
Pros.
Pros.
Pros.10 4 5.3 5.310 7 3.6 4.32 1 2.3 2.72 1 3.2 4.72 1 1.7 2.52 1 2.5 2.82 1 0.8 1.330 16 19.4 23.6Table 2: Sample of sentences to which the added con-straints applied.
Parse times are in secondsa following particle.
Thus the 'a' sentences had moremajor prosodic breaks at the sites in question than didthe 'b' sentences.The results, shown in Tables 2 and 3, indicate thatfor the 'a' sentences the number of parses was reduced,in many cases to a unique parse.
The 'b' sentences, asexpected, showed no change in the number of parses.
Nocorrect parses were eliminated through the incorporationof prosodic information.This corpus was also used in perceptual experimentsto determine which types of syntactic structures humanscould disambiguate using prosody.
It is interesting tonote that in many cases, sentences which were automat-ically disambiguated using the added constraints werealso reliably disambiguated by humans.
The fact thatthe perceptual results and parsing results are not morecorrelated than they are may be due to the fact thathumans use other prosodic cues such as prominence, inaddition to duration, for disambiguation.Discuss ionWe are encouraged by these initial results and believethat we have found a promising and novel approach forSentence Humans% correctlb 612b 813b 754b 945b 1006b 787b 100TOTAL 84Number of parses Parse TimeNo With No WithPros.
Pros.
Pros.
Pros.10 10 5.3 7.710 10 3.6 4.O2 2 2.3 3.72 2 3.2 5.52 2 1.6 2.92 2 2.5 4.12 2 O.8 1.530 30 19.3 29.4Table 3: Sample of sentences to which the added con-straints did not apply.
Parse times are in seconds.3Oincorporating prosodic information into a natural lan- \[7\]guage processing system.
The break index representa-tion of prosodic phrase levels is a useful formalism whichcan be fairly reliably detected and can be incorporatedinto a parser to rule out prosodically inconsistent syn-tactic hypotheses.
\[8\]The results reported here represent only a small studyof integrating prosody and parsing, and there are manydirections in which we hope to extend the work.
In de- \[9\]tection, integrating duration and intonation cues offersthe potential for a significant decrease in the false re-jection rate of major phrase boundaries, and previouswork by Butzberger on boundary tone detection \[4\] pro- \[10\]vides a mechanism for incorporating intonation.
As forintegration with the parser, investigation of other typesof structural ambiguity should lead to similar improve-ments in the reduction of the number of parses.
Finally, \[11\]we hope to verify and extend these results by consideringa larger database of speech and as well as the prosody ofnonprofessional speakers.
We are already evaluating thetechniques on the ATIS database.
\[12\]AcknowledgementsThe authors gratefully acknowledge the help and advice \[13\]from Stefanie Shattuck-Hufnagel, forher role in definingthe prosodic break representation, and from Hy Murveit,for his assistance in generating the phone alignments.
\[14\]This research was jointly funded by NSF and DARPAunder NSF grant number IRI-8905249, and in part byDARPA under the Office of Naval Research contractN00014-85-C-0013.
\[15\]References\[1\] H. Alshawi, D. M. Carter, J. van Eijck, R. C. Moore,D.
B. Moran, F. C. N. Pereira, S. G. Pulman andA.
G. Smith (1988) Research Programme in Natu-ral Language Processing: July 1988 Annual Report,SRI International Technical Note, Cambridge, Eng-land.\[2\] J.
Bear and P. J.
Price (1990) "Prosody, Syntax andParsing," Proceedings of the ACL Conference.\[3\] M. Beckman and J. Pierrehumbert (1986) "Intona-tional Structure in Japanese and English," Phonol-ogy Yearbook 3, ed.
J. Ohala, pp.
255-309.\[4\] J. Sutzberger (1990) Statistical Methods for Anal-ysis and Recognition of Intonation Patterns inSpeech, M.S.
Thesis, Boston University.\[5\] J. Gaitenby (1965) "The Elastic Word," Status Re-port on Speech Research SR-2, Haskins Laborato-ries, New Haven, CT, 1-12.\[6\] J. P. Gee and F. Grosjean (1983) "PerformanceStructures: A Psycholinguistic and Linguistic Ap-praisal," Cognitive Psychology, Vol.
15, pp.
411-458.\[16\]\[17\]\[18\]J. Harrington and A. Johnstone (1987) "The Effectsof Word Boundary Ambiguity in Continuous SpeechRecognition," Proc.
of X I  Int.
Congr.
Phonetic Sci-ences, Tallin, Estonia, Se 45.5.1-4.D.
Klatt (1975) "Vowel Lengthening is SyntacticallyDetermined in a Connected Discourse," J. Phonet-ics 3, 129-140.M.
Y. Liberman and A. S. Prince (1977) "On Stressand Linguistic Rhythm," Linguistic Inquiry 8,249-336.D.
R. Ladd, (1986) "Intonational Phrasing: theCase for Recursive Prosodic Structure," PhonologyYearbook, 3:311-340.P.
Price, M. Ostendorf and C. Wightman (1989)"Prosody and Parsing," in Proc.
Second DATIPAWorkshop on Speech and Natural Language, Octo-ber 1989, pp.5-11.E.
Selkirk (1980) "The Role of Prosodic Categoriesin English Word Stress," Linguistic Inquiry, Vol.
11,pp.
563-605.E.
Selkirk (1984) Phonology and Syntax: The Rela-tion between Sound and Structure, Cambridge, MA,MIT Press.M.
Steedman (1989) "Intonation and Syntax in Spo-ken Language Systems," presented at the BBN Nat-ural Language Symposium.L.
Streeter, "Acoustic Determinants of PhraseBoundary Perception," Journal of the AcousticalSociety of America, Vol.
64, No.
6, pp.
1582-1592(1978).J.
Vaissiere (1983) "Language-IndependentProsodic Features," in Prosody: Models and Mea-surements, ed.
A. Cutler and D. R. Ladd, pp.
53-66,Springer-Verlag.M.
Weintraub, H. Murveit, M. Cohen, P. Price,J.
Bernstein, G. Baldwin and D. Bell (1989) "Lin-guistic Constraints in Hidden Markov Model BasedSpeech Recognition," in Proc.
IEEE Int.
Conf.Acoust., Speech, Signal Processing, pages 699-702,Glasgow, Scotland.A.
Zwicky (1970) "Auxiliary Reduction in English,"Linguistic Inquiry, Vol.
1,323-336.31
