Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 362?371,Austin, Texas, November 1-5, 2016. c?2016 Association for Computational LinguisticsDiscourse Parsing with Attention-based Hierarchical Neural NetworksQi Li Tianshi Li Baobao ChangKey Laboratory of Computational Linguistics, Ministry of EducationSchool of Electronics Engineering and Computer Science, Peking UniversityNo.5 Yiheyuan Road, Haidian District, Beijing, 100871, ChinaCollaborative Innovation Center for Language Ability, Xuzhou, 221009, Chinaqi.li@pku.edu.cn lts 417@hotmail.com chbb@pku.edu.cnAbstractRST-style document-level discourse parsingremains a difficult task and efficient deeplearning models on this task have rarely beenpresented.
In this paper, we propose anattention-based hierarchical neural networkmodel for discourse parsing.
We also incor-porate tensor-based transformation function tomodel complicated feature interactions.
Ex-perimental results show that our approach ob-tains comparable performance to the contem-porary state-of-the-art systems with little man-ual feature engineering.1 IntroductionA document is formed by a series of coherent textunits.
Document-level discourse parsing is a task toidentify the relations between the text units and todetermine the structure of the whole document thetext units form.
Rhetorical Structure Theory (RST)(Mann and Thompson, 1988) is one of the most in-fluential discourse theories.
According to RST, thediscourse structure of a document can be representedby a Discourse Tree (DT).
Each leaf of a DT denotesa text unit referred to as an Elementary DiscourseUnit (EDU) and an inner node of a DT representsa text span which is constituted by several adjacentEDUs.
DTs can be utilized by many NLP tasks in-cluding automatic document summarization (Louiset al, 2010; Marcu, 2000), question-answering (Ver-berne et al, 2007) and sentiment analysis (Somasun-daran, 2010) etc.Much work has been devoted to the task of RST-style discourse parsing and most state-of-the-art ap-proaches heavily rely on manual feature engineer-ing (Joty et al, 2013; Feng and Hirst, 2014; Jiand Eisenstein, 2014).
While neural network mod-els have been increasingly focused on for their abil-ity to automatically extract efficient features whichreduces the burden of feature engineering, there islittle neural network based work for RST-style dis-course parsing except the work of Li et al (2014a).Li et al (2014a) propose a recursive neural networkmodel to compute the representation for each textspan based on the representations of its subtrees.However, vanilla recursive neural networks sufferfrom gradient vanishing for long sequences and thenormal transformation function they use is weak atmodeling complicated interactions which has beenstated by Socher et al (2013).
As many docu-ments contain more than a hundred EDUs whichform quite a long sequence, those weaknesses maylead to inferior results on this task.In this paper, we propose to use a hierarchicalbidirectional Long Short-Term Memory (bi-LSTM)network to learn representations of text spans.
Com-paring with vanilla recursive/recurrent neural net-works, LSTM-based networks can store informationfor a long period of time and don?t suffer from gra-dient vanishing problem.
We apply a hierarchicalbi-LSTM network because the way words form anEDU and EDUs form a text span is different andthus they should be modeled separately and hierar-chically.
On top of that, we apply attention mecha-nism to attend over all EDUs to pick up prominentsemantic information of a text span.
Besides, we usetensor-based transformation function to model com-plicated feature interactions and thus it can produce362combinatorial features.We summarize contributions of our work as fol-lows:?
We propose to use a hierarchical bidirectionalLSTM network to learn the compositional se-mantic representations of text spans, which nat-urally matches and models the intrinsic hierar-chical structure of text spans.?
We extend our hierarchical bi-LSTM networkwith attention mechanism to allow the networkto focus on the parts of input containing promi-nent semantic information for the composi-tional representations of text spans and thus al-leviate the problem caused by the limited mem-ory of LSTM for long text spans.?
We adopt a tensor-based transformation func-tion to allow explicit feature interactions andapply tensor factorization to reduce the param-eters and computations.?
We use two level caches to intensively acceler-ate our probabilistic CKY-like parsing process.The rest of this paper is organized as follows: Sec-tion 2 gives the details of our parsing model.
Section3 describes our parsing algorithm.
Section 4 givesour training criterion.
Section 5 reports the experi-mental results of our approach.
Section 6 introducesthe related work.
Conclusions are given in section 7.2 Parsing ModelGiven two successive text spans, our parsing modelevaluates the probability to combine them into alarger span, identifies which one is the nucleus anddetermines what is the relation between them.
Aswith the work of Ji and Eisenstein (2014), we setthree classifiers which share the same features as in-put to deal with those problems.
The whole pars-ing model is shown in Figure 1.
Three classi-fiers are on the top.
The semantic representationsof the two given text spans which come from theoutput of attention-based hierarchical bi-LSTM net-work with tensor-based transformation function isthe main part of input to the classifiers.
Additionally,following the previous practice of Li et al (2014a),a small set of handcrafted features is introduced toenhance the model.Figure 1: Schematic structure of our parsing model.2.1 Hierarchical Bi-LSTM Network for TextSpan RepresentationsLong Short-Term Memory (LSTM) networks havebeen successfully applied to a wide range of NLPtasks for the ability to handle long-term dependen-cies and to mitigate the curse of gradient vanishing(Hochreiter and Schmidhuber, 1997; Bahdanau etal., 2014; Rockta?schel et al, 2015; Hermann et al,2015).
A basic LSTM can be described as follows.A sequence {x1, x2, ..., xn} is given as input.
Ateach time-step, the LSTM computation unit takes inone token xt as input and it keeps some informationin a cell state Ct and gives an output ht.
They arecalculated in this way:it = ?
(Wi[ht?1;xt] + bi) (1)ft = ?
(Wf [ht?1;xt] + bf ) (2)C?t = tanh(WC [ht?1;xt] + bC) (3)Ct = ft  Ct?1 + it  C?t (4)ot = ?
(Wo[ht?1;xt] + bo) (5)ht = ot  tanh(Ct) (6)where Wi, bi,Wf , bf ,Wc, bC ,Wo, bo are LSTM pa-rameters, denotes element-wise product and ?
de-notes sigmoid function.
The output at the last token,i.e., hn is taken as the representation of the wholesequence.363Figure 2: Bi-LSTM for computing the compositional semanticrepresentation of an EDU.Since an EDU is a sequence of words, we de-rive the representation of an EDU from the sequenceconstituted by concatenation of word embeddingsand the POS tag embeddings of the words as Figure2 shows.
Previous work on discourse parsing tendsto extract some features from the beginning and endof text units partly because discourse clues such asdiscourse markers(e.g., because, though) are oftensituated at the beginning or end of text units(Fengand Hirst, 2014; Ji and Eisenstein, 2014; Li et al,2014a; Li et al, 2014b; Heilman and Sagae, 2015).Considering the last few tokens of a sequence nor-mally have more influence on the representation ofthe whole sequence learnt with LSTM because theyget through less times of forget gate from the LSTMcomputation unit, to effectively capture the informa-tion from both beginning and end of an EDU, weuse bidirectional LSTM to learn the representationof an EDU.
In other words, one LSTM takes theword sequence in forward order as input, the othertakes the word sequence in reversed order as input.The representation of a sequence is the concatena-tion of the two vector representations calculated bythe two LSTMs.Since a text span is a sequence of EDUs, itsmeaning can be computed from the meanings ofthe EDUs.
So we use another bi-LSTM to derivethe compositional semantic representation of a textspan from the EDUs it contains.
The two bi-LSTMnetworks form a hierarchical structure as Figure 1shows.2.2 AttentionThe representation of a sequence computed by bi-LSTMs is always a vector with fixed dimension de-spite the length of the sequence.
Thus when dealingwith a text span with hundreds of EDUs, bi-LSTMmay not be enough to capture the whole semantic in-formation with its limited output vector dimension.Attention mechanism can attend over the output atevery EDU with global context and pick up promi-nent semantic information and drop the subordinateinformation for the compositional representation ofthe span, so we employ attention mechanism to al-leviate the problem caused by the limited memoryof LSTM networks.
The attention mechanism is in-spired by the work of Rockta?schel et al (2015).
Ourattention-based bi-LSTM network is shown in Fig-ure 3.We combine the last outputs of the span level bi-LSTM to be hs = [?
?h en ,?
?h e1 ].
We also combinethe outputs of the two LSTM at every EDU of thespan: ht = [?
?h t,?
?h t] and thus get a matrix H =[h1;h2; ...;hn]T .
Taking H ?
Rd?n and hs ?
Rd asinputs, we get a vector ?
?
Rn standing for weightsof EDUs to the text span and use it to get a weightedrepresentation of the span r ?
Rd:M = tanh(WyH +Wlhs ?
en) (7)?
= softmax(wT?M) (8)r = H?
(9)where?
denotes Cartesian product , M ?
Rk?n, enis a n dimensional vector of all 1s and we use theCartesian product Wlhs ?
en to repeat the result ofWlhs n times in column to form a matrix and Wy ?Rk?d,Wl,?
Rk?d, w?
?
Rk are parameters.We synthesize the information of r and hs to getthe final representation of the span:wh = ?
(Whrr +Whhhs) (10)h = wh  hs + (1?
wh) r (11)where Whr,Whh ?
Rd?d are parameters, wh ?
Rdis a computed vector representing the element-wiseweight of hs and the element-wise weighted sum-mation h ?
Rd is the final representation of the textspan computed by the attention-based bidirectionalLSTM network.364Figure 3: Attention-based bi-LSTM for computing the compo-sitional semantic representation of a text span.2.3 ClassifiersWe concatenate the representations of the two givenspans: h = [hs1, hs2] and feed h into a full connec-tion hidden layer to obtain a higher level representa-tion v which is the input to the three classifiers:v = Relu(Wh[hs1, hs2] + bh) (12)For each classifier, we firstly transform v ?
Rlinto a hidden layer:vsp = Relu(Whsv + bhs) (13)vnu = Relu(Whnv + bhn) (14)vrel = Relu(Whrv + bhr) (15)where Whs,Whn,Whr ?
Rh?l are transformationmatrices and bhs, bhn, bhr ?
Rh are bias vectors.Then we feed these vectors into the respectiveoutput layer:ysp = ?
(wsvsp + bs) (16)ynu = softmax(Wnvnu + bn) (17)yrel = softmax(Wrvrel + br) (18)where ws ?
Rh, bs ?
R,Wn ?
R3?h,Wn ?R3?h, bn ?
R3,Wr ?
Rnr?h, bn ?
Rnr are pa-rameters and nr is the number of different discourserelations.The first classifier is a binary classifier which out-puts the probability the two spans should be com-bined.
The second classifier is a multiclass classifierwhich identifies the nucleus to be span 1, span 2 orboth.
The third classifier is also a multiclass classi-fier which determines the relation between the twospans.2.4 Tensor-based TransformationTensor-based transformation function has been suc-cessfully utilized in many tasks to allow complicatedinteraction between features (Sutskever et al, 2009;Socher et al, 2013; Pei et al, 2014).
Based onthe intuition that allowing complicated interactionbetween the features of the two spans may help toidentify how they are related, we adopt tensor-basedtransformation function to strengthen our model.A tensor-based transformation function on x ?Rd1 is as follows:y = Wx+ xTT [1:d2]x+ b (19)yi =?jWijxj +?j,kT [i]j,kxjxk + bi (20)where y ?
Rd2 is the output vector, yi ?
R is theith element of y, W ?
Rd2?d1 is the transformationmatrix, T [1:d2] ?
Rd1?d1?d2 is a 3rd-order transfor-mation tensor.
A normal transformation function inneural network models only has the first term Wxwith the bias term.
It means for normal transfor-mation function each unit of the output vector isthe weighted summation of the input vector and thisonly allows additive interaction between the units ofthe input vector.
With the tensor multiplication term,each unit of the output vector is augmented with theweighted summation of the multiplication of the in-put vector units and thus we incorporate multiplica-tive interaction between the units of the input vector.Inevitably, the incorporation of tensor leads toside effects which include the increase in parameternumber and computational complexity.
To remedythis, we adopt tensor factorization in the same wayas Pei et al (2014): we use two low rank matrices toapproximate each tensor slice T [i] ?
Rd1?d1 :T [i] ?
P [i]Q[i] (21)where P [i] ?
Rd1?r, Q[i] ?
Rr?d1 and r  d1.In this way, we drastically reduce parameter numberand computational complexity.365We apply the factorized tensor-based transforma-tion function to the combined text span representa-tion h = [hs1, hs2] to make the features of the twospans explicitly interact with each other:v = Relu(Wh[hs1, hs2] +[hs1, hs2]TP [1:d]h Q[1:d]h [hs1, hs2] + bh) (22)Comparing with Eq.
12, the transformation functionis added with a tensor term.2.5 Handcrafted FeaturesMost previously proposed state-of-the-art systemsheavily rely on handcrafted features (Hernault et al,2010; Feng and Hirst, 2014; Joty et al, 2013; Ji andEisenstein, 2014; Heilman and Sagae, 2015).
Li etal.
(2014a) show that some basic features are stillnecessary to get a satisfactory result for their recur-sive deep model.
Following their practice, we adoptminimal basic features which are utilized by mostsystems to further strengthen our model.
We listthese features in Table 1.
We apply the factorizedtensor-based transformation function to Word/POSfeatures to allow more complicated interaction be-tween them.3 Parsing AlgorithmIn this section, we describe our parsing algorithmwhich utilizes the parsing model to produce theglobal optimal DT for a segmented document.3.1 Probabilistic CKY-like AlgorithmWe adopt a probabilistic CKY-like bottom-up algo-rithm which is also adopted in (Joty et al, 2013;Li et al, 2014a) to produce a DT for a document.This parsing algorithm is a dynamic programmingalgorithm and produces the global optimal DT withour parsing model.
Given a text span which isconstituted by [ei, ei+1, ..., ej ] and the possible sub-trees of [ei, ei+1, ..., ek] and [ek+1, ek+2, ..., ej ] forall k ?
{i, i+1, ..., j?1}with their probabilities, wechoose k and combine the corresponding subtrees toform a combined DT with the following recurrenceformula:k = argmaxk{Psp(i, k, j)Pi,kPk+1,j} (23)where Pi,k and Pk+1,j are the probabilities ofthe most probable subtrees of [ei, ei+1, ..., ek] and[ek+1, ek+2, ..., ej ] respectively, Psp(i, k, j) is theprobability which is predicted by our parsing modelto combine those two subtrees to form a DT.The probability of the most probable DT of[ei, ei+1, ..., ej ] is:Pi,j = maxk {Psp(i, k, j)Pi,kPk+1,j} (24)3.2 Parsing AccelerationComputational complexity of the original proba-bilistic CKY-like algorithm is O(n3) where n is thenumber of EDUs of the document.
But in this work,given each pair of text spans, we compute the rep-resentations of them with hierarchical bi-LSTM net-work at the expense of an additional O(n) computa-tions.
So the computational complexity of our parserbecomesO(n4) and it is unacceptable for long docu-ments.
However, most computations are duplicated,so we use two level caches to drastically accelerateparsing.Firstly, we cache the outputs of the EDU levelbi-LSTM which are the semantic representations ofEDUs.
As for the forward span level LSTM, afterwe get the semantic representation of a span, wecache it too and use it to compute the representationof an extended span.
For example, after we get therepresentation of span constituted by [e1, e2, e3], wetake it with semantic representation of e4 to com-pute the representation of the span constituted by[e1, e2, e3, e4] in one LSTM computation step.
Forthe backward span level LSTM, we do it the sameway just in reversed order.
Thus we decrease thecomputational complexity of computing the seman-tic representations for all possible span pairs whichis the most time-consuming part of the original pars-ing process from O(n4) to O(n2).Secondly, it can be seen that before we applyRelu to the tensor-based transformation function,many calculations from the two spans which includea large part of tensor multiplication are independent.The multiplication between the elements of the rep-resentations of the two spans caused by the tensorsand the element-wise non-linear activation functionRelu terminate the independence between them.
Sowe can further cache the independent calculation re-sults before Relu operation for each span.
Thus wedecrease the computational complexity of a largepart of tensor-based transformation from O(n3) to366Word/POS FeaturesOne-hot representation of the first two words and of the last word of each span.One-hot representation of POS tags of the first two words and of the last word of each span.Shallow FeaturesNumber of EDUs of each span.Number of words of each span.Predicted relations of the two subtrees?
roots.Whether each span is included in one sentence.Whether both spans are included in one sentence.Table 1: Handcrafted features used in our parsing model.O(n2) which is the second time-consuming part ofthe original parsing process.The remaining O(n3) computations include a lit-tle part of tensor-based transformation computa-tions,Relu operation and the computations from thethree classifiers.
These computations take up only alittle part of the original parsing model computationsand thus we greatly accelerate our parsing process.4 Max-Margin TrainingWe use Max-Margin criterion for our model train-ing.
We try to learn a function that maps: X ?
Y ,where X is the set of documents and Y is the set ofpossible DTs.
We define the loss function for pre-dicting a DT y?i given the correct DT yi as:4(yi, y?i) =?r?y?i?1{r 6?
yi} (25)where r is a span specified with nucleus and relationin the predicted DT, ?
is a hyperparameter referredto as discount parameter and 1 is indicator function.We expect the probability of the correct DT to be alarger up to a margin to other possible DTs:Prob(x, yi) ?
Prob(xi, y?i) +4(yi, y?i) (26)The objective function for m training examples isas follows:J(?)
=1mm?i=1li(?
), where (27)li(?)
= maxy?i(Prob(xi, y?i) +4(yi, y?i))?Prob(xi, yi) (28)where ?
denotes all the parameters including ourneural network parameters and all embeddings.The probabilities of the correct DTs increase andthe probabilities of the most probable incorrect DTsdecrease during training.
We adopt Adadelta (Zeiler,2012) with mini-batch to minimize the objectivefunction and set the initial learning rate to be 0.012.5 ExperimentsWe evaluate our model on RST Discourse Treebank1(RST-DT) (Carlson et al, 2003).
It is partitionedinto a set of 347 documents for training and a setof 38 documents for test.
Non-binary relations areconverted into a cascade of right-branching binaryrelations.
The standard metrics of RST-style dis-course parsing evaluation include blank tree struc-ture referred to as span (S), tree structure with nu-clearity (N) indication and tree structure with rhetor-ical relation (R) indication.
Following other RST-style discourse parsing systems, we evaluate the re-lation metric in 18 coarse-grained relation classes.Since our work focus does not include EDU segmen-tation, we evaluate our system with gold-standardEDU segmentation and we apply the same settingon this to other discourse parsing systems for faircomparison.5.1 Experimental SetupThe dimension of word embeddings is set to be50 and the dimension of POS embeddings is set tobe 10.
We pre-trained the word embeddings withGloVe (Pennington et al, 2014) on English Giga-word2 and we fine-tune them during training.
Con-sidering some words are pretrained by GloVe but1https://catalog.ldc.upenn.edu/LDC2002T072https://catalog.ldc.upenn.edu/LDC2011T07367don?t appear in the RST-DT training set, we want touse their embeddings if they appear in test set.
Fol-lowing Kiros et al (2015), we expand our vocabu-lary with those words using a matrix W ?
R50?50that maps word embeddings from the pre-trainedword embedding space to the fine-tuned word em-bedding space.
The objective function for trainingthe matrix W is as follows:minW,b||Vtuned ?
VpretrainedW ?
b||22 (29)where Vtuned, Vpretrained ?
R|V |?50 contain fine-tuned and pre-trained embeddings of words appear-ing in training set respectively, |V | is the size ofRST-DT training set vocabulary and b is the biasterm also to be trained.We lemmatize all the words appeared and rep-resent all numbers with a special token.
We useStanford CoreNLP toolkit (Manning et al, 2014) topreprocess the text including lemmatization, POStagging etc.
We use Theano library (Bergstra etal., 2010) to implement our parsing model.
Werandomly initialize all parameters within (-0.012,0.012) except word embeddings.
We adopt dropoutstrategy (Hinton et al, 2012) to avoid overfitting andwe set the dropout rate to be 0.3.5.2 Results and AnalysisTo show the effectiveness of the components in-corporated into our model, we firstly test the per-formance of the basic hierarchical bidirectionalLSTM network without attention mechanism (ATT),tensor-based transformation (TE) and handcraftedfeatures (HF).
Then we add them successively.
Theresults are shown in Table 2.The performance is improved by adding eachcomponent to our basic model and that shows the ef-fectiveness of attention mechanism and tensor-basedtransformation function.
Even without handcraftedfeatures, the performance is still competitive.
Itindicates that the semantic representations of textspans produced by our attention-based hierarchicalbi-LSTM network are effective and the handcraftedfeatures are complementary to semantic representa-tions produced by the network.We also experiment without mapping the OOVword embeddings and use the same embedding forall OOV words.
The result is shown in TableSystem Setting S N RBasic 82.7 69.7 55.6Basic+ATT 83.6* 70.2* 56.0*Basic+ATT+TE 84.2* 70.4 56.3*Basic+ATT+TE+HF 85.8* 71.1* 58.9*Table 2: Performance comparison for different settings ofour system on RST-DT.
?Basic?
denotes the basic hierarchicalbidirectional LSTM network; ?+ATT?
denotes adding attentionmechanism; ?+TE?
denotes adopting tensor-based transforma-tion; ?+HF?
denotes adding handcrafted features.
* indicatesstatistical significance in t-test compared to the result in the lineabove (p < 0.05).System Setting S N RWithout OOV mapping 85.1 70.7 58.2Full version 85.8* 71.1* 58.9*Table 3: Performance comparison for whether to map OOVembeddings.3.
Without mapping the OOV word embeddingsthe performance decreases slightly, which demon-strates that the relation between pre-trained embed-ding space and the fine-tuned embedding space canbe learnt and it is beneficial to train a matrix to trans-form OOV word embeddings from the pre-trainedembedding space to the fine-tuned embedding space.We compare our system with other state-of-the-artsystems including (Joty et al, 2013; Ji and Eisen-stein, 2014; Feng and Hirst, 2014; Li et al, 2014a;Li et al, 2014b; Heilman and Sagae, 2015).
Systemsproposed by Joty et al (2013), Heilman (2015) andFeng and Hirst (2014) are all based on variants ofCRFs.
Ji and Eisenstein (2014) use a projection ma-trix acting on one-hot representations of features tolearn representations of text spans and build SupportVector Machine (SVM) classifier on them.
Li et al(2014b) adopt dependency parsing methods to dealwith this task.
These systems are all based on hand-crafted features.
Li et al (2014a) adopt a recursivedeep model and use some basic handcrafted featuresto improve their performances which has been statedbefore.Table 4 shows the performance for our systemand those systems.
Our system achieves the bestresult in span and relatively lower performance innucleus and relation identification comparing withthe corresponding best results but still better than368System S N RJoty et al (2013) 82.7 68.4 55.7Ji and Eisenstein (2014) 82.1 71.1 61.6Feng and Hirst (2014) 85.7 71.0 58.2Li et al (2014a) 84.0 70.8 58.6Li et al (2014b) 83.4 73.8 57.8Heilman and Sagae (2015) 83.5 68.1 55.1Ours 85.8 71.1 58.9Human 88.7 77.7 65.8Table 4: Performance comparison with other state-of-the-artsystems on RST-DT.System S N RLi et al (2014a) (no feature) 82.4 69.2 56.8Ours (no feature) 84.2 70.4 56.3Table 5: Performance comparison with the deep learning modelproposed in Li et al (2014a) without handcrafted features.most systems.
No system achieves the best resulton all three metrics.
To further show the effective-ness of the deep learning model itself without hand-crafted features, we compare the performance be-tween our model and the model proposed by Li et al(2014a) without handcrafted features and the resultsare shown in Table 5.
It shows our overall perfor-mance outperforms the model proposed by Li et al(2014a) which illustrates our model is effective.Table 6 shows an example of the weights (W) ofEDUs (see Eq.
8) derived from our attention model.For span1 the main semantic meaning is expressedin EDU32 under the condition described in EDU31.Besides, it is EDU32 that explicitly manifests thecontrast relation between the two spans.
As canbe seen, our attention model assigns less weight toSpan1 (EDU30?EDU32) WThat means that 0.13if the offense deals with one part of thebusiness,0.38you don?t attempt to seize the whole busi-ness;0.49Span2 (EDU33) Wyou attempt to seize assets related to thecrime,1.0Table 6: An example of the weights derived from our attentionmodel.
The relation between span1 and span2 is Contrast.EDU30 and focuses more on EDU32 which is rea-sonable according to our analysis above.6 Related WorkTwo most prevalent discourse parsing treebanksare RST Discourse Treebank (RST-DT) (Carlson etal., 2003) and Penn Discourse TreeBank (PDTB)(Prasad et al, 2008).
We evaluate our system onRST-DT which is annotated in the framework ofRhetorical Structure Theory (Mann and Thompson,1988).
It consists of 385 Wall Street Journal arti-cles and is partitioned into a set of 347 documentsfor training and a set of 38 documents for test.
110fine-grained and 18 coarse-grained relations are de-fined on RST-DT.
Parsing algorithms published onRST-DT can mainly be categorized as shift-reduceparsers and probabilistic CKY-like parsers.
Shift-reduce parsers are widely used for their efficiencyand effectiveness and probabilistic CKY-like parserslead to the global optimal result for the parsingmodels.
State-of-the-art systems belonging to shift-reduce parsers include (Heilman and Sagae, 2015;Ji and Eisenstein, 2014).
Those belonging to prob-abilistic CKY-like parsers include (Joty et al, 2013;Li et al, 2014a).
Besides, Feng and Hirst (2014)adopt a greedy bottom-up approach as their pars-ing algorithm.
Lexical, syntactic, structural and se-mantic features are extracted in these systems.
SVMand variants of Conditional Random Fields (CRFs)are mostly used in these models.
Li et al (2014b)distinctively propose to use dependency structure torepresent the relations between EDUs.
Recursivedeep model proposed by Li et al (2014a) has beenthe only proposed deep learning model on RST-DT.Incorporating attention mechanism into RNN(e.g., LSTM, GRU) has been shown to learn bet-ter representation by attending over the output vec-tors and picking up important information from rel-evant positions of a sequence and this approach hasbeen utilized in many tasks including neural ma-chine translation (Kalchbrenner and Blunsom, 2013;Bahdanau et al, 2014; Hermann et al, 2015), textentailment recognition (Rockta?schel et al, 2015)etc.
Some work also uses tensor-based transforma-tion function to make stronger interaction betweenfeatures and learn combinatorial features and theyget performance boost in their tasks (Sutskever et369al., 2009; Socher et al, 2013; Pei et al, 2014).7 ConclusionIn this paper, we propose an attention-based hier-archical neural network for discourse parsing.
Ourattention-based hierarchical bi-LSTM network pro-duces effective compositional semantic representa-tions of text spans.
We adopt tensor-based trans-formation function to allow complicated interactionbetween features.
Our two level caches accelerateparsing process significantly and thus make it prac-tical.
Our proposed system achieves comparable re-sults to state-of-the-art systems.
We will try extend-ing attention mechanism to obtain the representationof a text span by referring to another text span atminimal additional cost.AcknowledgmentsWe thank the reviewers for their instructive feed-back.
We also thank Jiwei Li for his helpfuldiscussions.
This work is supported by NationalKey Basic Research Program of China under GrantNo.2014CB340504 and National Natural ScienceFoundation of China under Grant No.61273318.The Corresponding author of this paper is BaobaoChang.ReferencesDzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-gio.
2014.
Neural machine translation by jointlylearning to align and translate.
CoRR, abs/1409.0473.James Bergstra, Olivier Breuleux, Fre?de?ric Bastien, Pas-cal Lamblin, Razvan Pascanu, Guillaume Desjardins,Joseph Turian, David Warde-Farley, and Yoshua Ben-gio.
2010.
Theano: a CPU and GPU math expressioncompiler.
In Proceedings of the Python for ScientificComputing Conference (SciPy), June.
Oral Presenta-tion.Lynn Carlson, Daniel Marcu, and Mary Ellen Okurowski.2003.
Building a discourse-tagged corpus in theframework of rhetorical structure theory.
In Currentand new directions in discourse and dialogue, pages85?112.
Springer.Vanessa Wei Feng and Graeme Hirst.
2014.
A linear-time bottom-up discourse parser with constraints andpost-editing.
In ACL (1), pages 511?521.Michael Heilman and Kenji Sagae.
2015.
Fastrhetorical structure theory discourse parsing.
CoRR,abs/1505.02425.Karl Moritz Hermann, Toma?
s Kocisky?, Edward Grefen-stette, Lasse Espeholt, Will Kay, Mustafa Suleyman,and Phil Blunsom.
2015.
Teaching machines to readand comprehend.
CoRR, abs/1506.03340.Hugo Hernault, Helmut Prendinger, David A DuVerle,and Mitsuru Ishizuka.
2010.
Hilda: a discourse parserusing support vector machine classification.
Dialogueand Discourse, 1(3):1?33.Geoffrey E. Hinton, Nitish Srivastava, Alex Krizhevsky,Ilya Sutskever, and Ruslan Salakhutdinov.
2012.
Im-proving neural networks by preventing co-adaptationof feature detectors.
CoRR, abs/1207.0580.Sepp Hochreiter and Ju?rgen Schmidhuber.
1997.
Longshort-term memory.
Neural computation, 9(8):1735?1780.Yangfeng Ji and Jacob Eisenstein.
2014.
Representationlearning for text-level discourse parsing.
In ACL (1),pages 13?24.Shafiq R. Joty, Giuseppe Carenini, Raymond T. Ng, andYashar Mehdad.
2013.
Combining intra- and multi-sentential rhetorical parsing for document-level dis-course analysis.
In ACL.Daniel Jurafsky and James H Martin.
2008.
Speech andlanguage processing, chapter 14.
In Prentice Hall.Nal Kalchbrenner and Phil Blunsom.
2013.
Recurrentcontinuous translation models.
In EMNLP.Ryan Kiros, Yukun Zhu, Ruslan Salakhutdinov,Richard S. Zemel, Antonio Torralba, Raquel Urtasun,and Sanja Fidler.
2015.
Skip-thought vectors.
CoRR,abs/1506.06726.Jiwei Li, Rumeng Li, and Eduard H Hovy.
2014a.
Re-cursive deep models for discourse parsing.
In EMNLP,pages 2061?2069.Sujian Li, Liang Wang, Ziqiang Cao, and Wenjie Li.2014b.
Text-level discourse dependency parsing.
InACL (1), pages 25?35.Annie Louis, Aravind Joshi, and Ani Nenkova.
2010.Discourse indicators for content selection in summa-rization.
In Proceedings of the 11th Annual Meetingof the Special Interest Group on Discourse and Dia-logue, pages 147?156.
Association for ComputationalLinguistics.William C Mann and Sandra A Thompson.
1988.Rhetorical structure theory: Toward a functional the-ory of text organization.
Text-Interdisciplinary Jour-nal for the Study of Discourse, 8(3):243?281.Christopher D. Manning, Mihai Surdeanu, John Bauer,Jenny Rose Finkel, Steven Bethard, and David Mc-Closky.
2014.
The stanford corenlp natural languageprocessing toolkit.
In ACL.Daniel Marcu.
2000.
The theory and practice of dis-course parsing and summarization.
MIT press.370Wenzhe Pei, Tao Ge, and Baobao Chang.
2014.
Max-margin tensor neural network for chinese word seg-mentation.
In ACL (1), pages 293?303.Jeffrey Pennington, Richard Socher, and Christopher D.Manning.
2014.
Glove: Global vectors for word rep-resentation.
In EMNLP.Rashmi Prasad, Nikhil Dinesh, Alan Lee, Eleni Milt-sakaki, Livio Robaldo, Aravind K Joshi, and Bonnie LWebber.
2008.
The penn discourse treebank 2.0.
InLREC.
Citeseer.Tim Rockta?schel, Edward Grefenstette, Karl Moritz Her-mann, Toma?
s Kocisky?, and Phil Blunsom.
2015.
Rea-soning about entailment with neural attention.
CoRR,abs/1509.06664.Richard Socher, Alex Perelygin, Jean Y Wu, JasonChuang, Christopher D Manning, Andrew Y Ng, andChristopher Potts.
2013.
Recursive deep models forsemantic compositionality over a sentiment treebank.In Proceedings of the conference on empirical meth-ods in natural language processing (EMNLP), volume1631, page 1642.
Citeseer.Swapna Somasundaran.
2010.
Discourse-level relationsfor Opinion Analysis.
Ph.D. thesis, University of Pitts-burgh.Ilya Sutskever, Ruslan Salakhutdinov, and Joshua B.Tenenbaum.
2009.
Modelling relational data usingbayesian clustered tensor factorization.
In NIPS.Suzan Verberne, Lou Boves, Nelleke Oostdijk, and Peter-Arno Coppen.
2007.
Evaluating discourse-based an-swer extraction for why-question answering.
In Pro-ceedings of the 30th annual international ACM SIGIRconference on Research and development in informa-tion retrieval, pages 735?736.
ACM.Matthew D. Zeiler.
2012.
Adadelta: An adaptive learn-ing rate method.
CoRR, abs/1212.5701.371
