Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 940?949,Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational LinguisticsMixing Multiple Translation Models in Statistical Machine TranslationMajid Razmara1 George Foster2 Baskaran Sankaran1 Anoop Sarkar11 Simon Fraser University, 8888 University Dr., Burnaby, BC, Canada{razmara,baskaran,anoop}@sfu.ca2 National Research Council Canada, 283 Alexandre-Tache?
Blvd, Gatineau, QC, Canadageorge.foster@nrc.gc.caAbstractStatistical machine translation is often facedwith the problem of combining training datafrom many diverse sources into a single trans-lation model which then has to translate sen-tences in a new domain.
We propose a novelapproach, ensemble decoding, which com-bines a number of translation systems dynam-ically at the decoding step.
In this paper,we evaluate performance on a domain adap-tation setting where we translate sentencesfrom the medical domain.
Our experimentalresults show that ensemble decoding outper-forms various strong baselines including mix-ture models, the current state-of-the-art for do-main adaptation in machine translation.1 IntroductionStatistical machine translation (SMT) systems re-quire large parallel corpora in order to be able toobtain a reasonable translation quality.
In statisti-cal learning theory, it is assumed that the trainingand test datasets are drawn from the same distribu-tion, or in other words, they are from the same do-main.
However, bilingual corpora are only availablein very limited domains and building bilingual re-sources in a new domain is usually very expensive.It is an interesting question whether a model that istrained on an existing large bilingual corpus in a spe-cific domain can be adapted to another domain forwhich little parallel data is present.
Domain adap-tation techniques aim at finding ways to adjust anout-of-domain (OUT) model to represent a target do-main (in-domain or IN).Common techniques for model adaptation adapttwo main components of contemporary state-of-the-art SMT systems: the language model and the trans-lation model.
However, language model adapta-tion is a more straight-forward problem compared totranslation model adaptation, because various mea-sures such as perplexity of adapted language modelscan be easily computed on data in the target domain.As a result, language model adaptation has been wellstudied in various work (Clarkson and Robinson,1997; Seymore and Rosenfeld, 1997; Bacchiani andRoark, 2003; Eck et al, 2004) both for speech recog-nition and for machine translation.
It is also easier toobtain monolingual data in the target domain, com-pared to bilingual data which is required for transla-tion model adaptation.
In this paper, we focused onadapting only the translation model by fixing a lan-guage model for all the experiments.
We expect do-main adaptation for machine translation can be im-proved further by combining orthogonal techniquesfor translation model adaptation combined with lan-guage model adaptation.In this paper, a new approach for adapting thetranslation model is proposed.
We use a novel sys-tem combination approach called ensemble decod-ing in order to combine two or more translationmodels with the goal of constructing a system thatoutperforms all the component models.
The strengthof this system combination method is that the sys-tems are combined in the decoder.
This enablesthe decoder to pick the best hypotheses for eachspan of the input.
The main applications of en-semble models are domain adaptation, domain mix-ing and system combination.
We have modifiedKriya (Sankaran et al, 2012), an in-house imple-mentation of hierarchical phrase-based translationsystem (Chiang, 2005), to implement ensemble de-coding using multiple translation models.We compare the results of ensemble decodingwith a number of baselines for domain adaptation.In addition to the basic approach of concatenation ofin-domain and out-of-domain data, we also traineda log-linear mixture model (Foster and Kuhn, 2007)940as well as the linear mixture model of (Foster et al,2010) for conditional phrase-pair probabilities overIN and OUT.
Furthermore, within the framework ofensemble decoding, we study and evaluate variousmethods for combining translation tables.2 BaselinesThe natural baseline for model adaption is to con-catenate the IN and OUT data into a single paral-lel corpus and train a model on it.
In addition tothis baseline, we have experimented with two moresophisticated baselines which are based on mixturetechniques.2.1 Log-Linear MixtureLog-linear translation model (TM) mixtures are ofthe form:p(e?|f?)
?
exp( M?m?m log pm(e?|f?
))where m ranges over IN and OUT, pm(e?|f?)
is anestimate from a component phrase table, and each?m is a weight in the top-level log-linear model, setso as to maximize dev-set BLEU using minimumerror rate training (Och, 2003).
We learn separateweights for relative-frequency and lexical estimatesfor both pm(e?|f?)
and pm(f?
|e?).
Thus, for 2 compo-nent models (from IN and OUT training corpora),there are 4 ?
2 = 8 TM weights to tune.
Whenevera phrase pair does not appear in a component phrasetable, we set the corresponding pm(e?|f?)
to a smallepsilon value.2.2 Linear MixtureLinear TM mixtures are of the form:p(e?|f?)
=M?m?mpm(e?|f?
)Our technique for setting ?m is similar to thatoutlined in Foster et al (2010).
We first extract ajoint phrase-pair distribution p?
(e?, f?)
from the de-velopment set using standard techniques (HMMword alignment with grow-diag-and symmeteriza-tion (Koehn et al, 2003)).
We then find the setof weights ??
that minimize the cross-entropy of themixture p(e?|f?)
with respect to p?
(e?, f?):??
= argmax??e?,f?p?
(e?, f?)
logM?m?mpm(e?|f?
)For efficiency and stability, we use the EM algo-rithm to find ?
?, rather than L-BFGS as in (Foster etal., 2010).
Whenever a phrase pair does not appearin a component phrase table, we set the correspond-ing pm(e?|f?)
to 0; pairs in p?
(e?, f?)
that do not appearin at least one component table are discarded.
Welearn separate linear mixtures for relative-frequencyand lexical estimates for both p(e?|f?)
and p(f?
|e?
).These four features then appear in the top-levelmodel as usual ?
there is no runtime cost for the lin-ear mixture.3 Ensemble DecodingEnsemble decoding is a way to combine the exper-tise of different models in one single model.
Thecurrent implementation is able to combine hierar-chical phrase-based systems (Chiang, 2005) as wellas phrase-based translation systems (Koehn et al,2003).
However, the method can be easily extendedto support combining a number of heterogeneoustranslation systems e.g.
phrase-based, hierarchicalphrase-based, and/or syntax-based systems.
Thissection explains how such models can be combinedduring the decoding.Given a number of translation models which arealready trained and tuned, the ensemble decoderuses hypotheses constructed from all of the modelsin order to translate a sentence.
We use the bottom-up CKY parsing algorithm for decoding.
For eachsentence, a CKY chart is constructed.
The cells ofthe CKY chart are populated with appropriate rulesfrom all the phrase tables of different components.As in the Hiero SMT system (Chiang, 2005), thecells which span up to a certain length (i.e.
the max-imum span length) are populated from the phrase-tables and the rest of the chart uses glue rules as de-fined in (Chiang, 2005).The rules suggested from the component modelsare combined in a single set.
Some of the rules maybe unique and others may be common with othercomponent model rule sets, though with differentscores.
Therefore, we need to combine the scoresof such common rules and assign a single score to941them.
Depending on the mixture operation used forcombining the scores, we would get different mix-ture scores.
The choice of mixture operation will bediscussed in Section 3.1.Figure 1 illustrates how the CKY chart is filledwith the rules.
Each cell, covering a span, is popu-lated with rules from all component models as wellas from cells covering a sub-span of it.In the typical log-linear model SMT, the posteriorprobability for each phrase pair (e?, f?)
is given by:p(e?
| f?)
?
exp(?iwi?i(e?, f?)?
??
?w??
)Ensemble decoding uses the same framework foreach individual system.
Therefore, the score of aphrase-pair (e?, f?)
in the ensemble model is:p(e?
| f?)
?
exp(w1 ?
?1?
??
?1st model?
w2 ?
?2?
??
?2nd model?
?
?
?)where?
denotes the mixture operation between twoor more model scores.3.1 Mixture OperationsMixture operations receive two or more scores(probabilities) and return the mixture score (prob-ability).
In this section, we explore different optionsfor mixture operation and discuss some of the char-acteristics of these mixture operations.?
Weighted Sum (wsum): in wsum the ensembleprobability is proportional to the weighted sumof all individual model probabilities (i.e.
linearmixture).p(e?
| f?)
?M?m?m exp(wm ?
?m)where m denotes the index of component mod-els, M is the total number of them and ?i is theweight for component i.?
Weighted Max (wmax): where the ensemblescore is the weighted max of all model scores.p(e?
| f?)
?
maxm(?m exp(wm ?
?m))?
Model Switching (Switch): in model switch-ing, each cell in the CKY chart gets populatedonly by rules from one of the models and theother models?
rules are discarded.
This is basedon the hypothesis that each component modelis an expert on certain parts of sentence.
In thismethod, we need to define a binary indicatorfunction ?(f?
,m) for each span and componentmodel to specify rules of which model to retainfor each span.?(f?
,m) =??
?1, m = argmaxn?M?(f?
, n)0, otherwiseThe criteria for choosing a model for each cell,?(f?
, n), could be based on:?
Max: for each cell, the model that has thehighest weighted best-rule score wins:?(f?
, n) = ?n maxe(wn ?
?n(e?, f?))?
Sum: Instead of comparing only thescores of the best rules, the model withthe highest weighted sum of the probabil-ities of the rules wins.
This sum has totake into account the translation table limit(ttl), on the number of rules suggested byeach model for each cell:?(f?
, n) = ?n?e?exp(wn ?
?n(e?, f?
))The probability of each phrase-pair (e?, f?)
iscomputed as:p(e?
| f?)
=M?m?(f?
,m) pm(e?
| f?)?
Product (prod): in Product models or Prod-uct of Experts (Hinton, 1999), the probabilityof the ensemble model or a rule is computed asthe product of the probabilities of all compo-nents (or equally the sum of log-probabilities,i.e.
log-linear mixture).
Product models canalso make use of weights to control the contri-bution of each component.
These models are942Figure 1: The cells in the CKY chart are populated using rules from all component models and sub-span cells.generally known as Logarithmic Opinion Pools(LOPs) where:p(e?
| f?)
?
exp(M?m?m (wm ?
?m))Product models have been used in combiningLMs and TMs in SMT as well as some otherNLP tasks such as ensemble parsing (Petrov,2010).Each of these mixture operations has a specificproperty that makes it work in specific domain adap-tation or system combination scenarios.
For in-stance, LOPs may not be optimal for domain adapta-tion in the setting where there are two or more mod-els trained on heterogeneous corpora.
As discussedin (Smith et al, 2005), LOPs work best when all themodels accuracies are high and close to each otherwith some degree of diversity.
LOPs give veto powerto any of the component models and this perfectlyworks for settings such as the one in (Petrov, 2010)where a number of parsers are trained by changingthe randomization seeds but having the same baseparser and using the same training set.
They no-ticed that parsers trained using different randomiza-tion seeds have high accuracies but there are somediversities among them and they used product mod-els for their advantage to get an even better parser.We assume that each of the models is expert in someparts and so they do not necessarily agree on cor-rect hypotheses.
In other words, product models (orLOPs) tend to have intersection-style effects whilewe are more interested in union-style effects.In Section 4.2, we compare the BLEU scores ofdifferent mixture operations on a French-English ex-perimental setup.3.2 NormalizationSince in log-linear models, the model scores arenot normalized to form probability distributions, thescores that different models assign to each phrase-pair may not be in the same scale.
Therefore, mixingtheir scores might wash out the information in one(or some) of the models.
We experimented with twodifferent ways to deal with this normalization issue.A practical but inexact heuristic is to normalize thescores over a shorter list.
So the list of rules comingfrom each model for a cell in CKY chart is normal-ized before getting mixed with other phrase-tablerules.
However, experiments showed changing thescores with the normalized scores hurts the BLEUscore radically.
So we use the normalized scoresonly for pruning and the actual scores are intact.We could also globally normalize the scores to ob-tain posterior probabilities using the inside-outsidealgorithm.
However, we did not try it as the BLEUscores we got using the normalization heuristic wasnot promissing and it would impose a cost in de-coding as well.
More investigation on this issue hasbeen left for future work.A more principled way is to systematically findthe most appropriate model weights that can avoidthis problem by scaling the scores properly.
Weused a publicly available toolkit, CONDOR (Van-den Berghen and Bersini, 2005), a direct optimizerbased on Powell?s algorithm, that does not require943explicit gradient information for the objective func-tion.
Component weights for each mixture operationare optimized on the dev-set using CONDOR.4 Experiments & Results4.1 Experimental SetupWe carried out translation experiments using the Eu-ropean Medicines Agency (EMEA) corpus (Tiede-mann, 2009) as IN, and the Europarl (EP) corpus1 asOUT, for French to English translation.
The dev andtest sets were randomly chosen from the EMEA cor-pus.2 The details of datasets used are summarized inTable 1.Dataset SentsWordsFrench EnglishEMEA 11770 168K 144KEuroparl 1.3M 40M 37MDev 1533 29K 25KTest 1522 29K 25KTable 1: Training, dev and test sets for EMEA.For the mixture baselines, we used a standardone-pass phrase-based system (Koehn et al, 2003),Portage (Sadat et al, 2005), with the following 7features: relative-frequency and lexical translationmodel (TM) probabilities in both directions; word-displacement distortion model; language model(LM) and word count.
The corpus was word-alignedusing both HMM and IBM2 models, and the phrasetable was the union of phrases extracted from theseseparate alignments, with a length limit of 7.
Itwas filtered to retain the top 20 translations for eachsource phrase using the TM part of the current log-linear model.For ensemble decoding, we modified an in-houseimplementation of hierarchical phrase-based sys-tem, Kriya (Sankaran et al, 2012) which uses thesame features mentioned in (Chiang, 2005): for-ward and backward relative-frequency and lexicalTM probabilities; LM; word, phrase and glue-rulespenalty.
GIZA++(Och and Ney, 2000) has been usedfor word alignment with phrase length limit of 7.In both systems, feature weights were optimizedusing MERT (Och, 2003) and with a 5-gram lan-1www.statmt.org/europarl2Please contact the authors to access the data-sets.guage model and Kneser-Ney smoothing was usedin all the experiments.
We used SRILM (Stolcke,2002) as the langugage model toolkit.
Fixing thelanguage model allows us to compare various trans-lation model combination techniques.4.2 ResultsTable 2 shows the results of the baselines.
The firstgroup are the baseline results on the phrase-basedsystem discussed in Section 2 and the second groupare those of our hierarchical MT system.
Since theHiero baselines results were substantially better thanthose of the phrase-based model, we also imple-mented the best-performing baseline, linear mixture,in our Hiero-style MT system and in fact it achievesthe hights BLEU score among all the baselines asshown in Table 2.
This baseline is run three timesthe score is averaged over the BLEU scores withstandard deviation of 0.34.Baseline PBS HieroIN 31.84 33.69OUT 24.08 25.32IN + OUT 31.75 33.76LOGLIN 32.21 ?LINMIX 33.81 35.57Table 2: The results of various baselines implemented ina phrase-based (PBS) and a Hiero SMT on EMEA.Table 3 shows the results of ensemble decodingwith different mixture operations and model weightsettings.
Each mixture operation has been evalu-ated on the test-set by setting the component weightsuniformly (denoted by uniform) and by tuning theweights using CONDOR (denoted by tuned) on aheld-out set.
The tuned scores (3rd column in Ta-ble 3) are averages of three runs with different initialpoints as in Clark et al (2011).
We also reported theBLEU scores when we applied the span-wise nor-malization heuristic.
All of these mixture operationswere able to significantly improve over the concate-nation baseline.
In particular, Switching:Max couldgain up to 2.2 BLEU points over the concatenationbaseline and 0.39 BLEU points over the best per-forming baseline (i.e.
linear mixture model imple-mented in Hiero) which is statistically significantbased on Clark et al (2011) (p = 0.02).Prod when using with uniform weights gets the944Mixture Operation Uniform Tuned Norm.WMAX 35.39 35.47 (s=0.03) 35.47WSUM 35.35 35.53 (s=0.04) 35.45SWITCHING:MAX 35.93 35.96 (s=0.01) 32.62SWITCHING:SUM 34.90 34.72 (s=0.23) 34.90PROD 33.93 35.24 (s=0.05) 35.02Table 3: The results of ensemble decoding on EMEA for Fr2En when using uniform weights, tuned weights andnormalization heuristic.
The tuned BLEU scores are averaged over three runs with multiple initial points, as in (Clarket al, 2011), with the standard deviations in brackets .lowest score among the mixture operations, how-ever after tuning, it learns to bias the weights to-wards one of the models and hence improves by1.31 BLEU points.
Although Switching:Sum outper-forms the concatenation baseline, it is substantiallyworse than other mixture operations.
One explana-tion that Switching:Max is the best performing op-eration and Switching:Sum is the worst one, despitetheir similarities, is that Switching:Max prefers morepeaked distributions while Switching:Sum favours amodel that has fewer hypotheses for each span.An interesting observation based on the results inTable 3 is that uniform weights are doing reasonablywell given that the component weights are not opti-mized and therefore model scores may not be in thesame scope (refer to discussion in ?3.2).
We suspectthis is because a single LM is shared between bothmodels.
This shared component controls the vari-ance of the weights in the two models when com-bined with the standard L-1 normalization of eachmodel?s weights and hence prohibits models to havetoo varied scores for the same input.
Though, it maynot be the case when multiple LMs are used whichare not shared.Two sample sentences from the EMEA test-setalong with their translations by the IN, OUT and En-semble models are shown in Figure 2.
The boxesshow how the Ensemble model is able to use n-grams from the IN and OUT models to constructa better translation than both of them.
In the firstexample, there are two OOVs one for each of theIN and OUT models.
Our approach is able to re-solve the OOV issues by taking advantage of theother model?s presence.
Similarly, the second exam-ple shows how ensemble decoding improves lexicalchoices as well as word re-orderings.5 Related Work5.1 Domain AdaptationEarly approaches to domain adaptation involved in-formation retrieval techniques where sentence pairsrelated to the target domain were retrieved from thetraining corpus using IR methods (Eck et al, 2004;Hildebrand et al, 2005).
Foster et al (2010), how-ever, uses a different approach to select related sen-tences from OUT.
They use language model per-plexities from IN to select relavant sentences fromOUT.
These sentences are used to enrich the INtraining set.Other domain adaptation methods involve tech-niques that distinguish between general and domain-specific examples (Daume?
and Marcu, 2006).
Jiangand Zhai (2007) introduce a general instance weight-ing framework for model adaptation.
This approachtries to penalize misleading training instances fromOUT and assign more weight to IN-like instancesthan OUT instances.
Foster et al (2010) propose asimilar method for machine translation that uses fea-tures to capture degrees of generality.
Particularly,they include the output from an SVM classifier thatuses the intersection between IN and OUT as pos-itive examples.
Unlike previous work on instanceweighting in machine translation, they use phrase-level instances instead of sentences.A large body of work uses interpolation tech-niques to create a single TM/LM from interpolatinga number of LMs/TMs.
Two famous examples ofsuch methods are linear mixtures and log-linear mix-tures (Koehn and Schroeder, 2007; Civera and Juan,2007; Foster and Kuhn, 2007) which were used asbaselines and discussed in Section 2.
Other meth-ods include using self-training techniques to exploitmonolingual in-domain data (Ueffing et al, 2007;945SOURCE ame?norrhe?e , menstruations irre?gulie`resREF amenorrhoea , irregular menstruationIN amenorrhoea , menstruations irre?gulie`resOUT ame?norrhe?e , irregular menstruationENSEMBLE amenorrhoea , irregular menstruationSOURCE le traitement par naglazyme doit e?tre supervise?
par un me?decin ayant l?
expe?rience dela prise en charge des patients atteints de mps vi ou d?
une autre maladie me?taboliquehe?re?ditaire .REF naglazyme treatment should be supervised by a physician experienced in the manage-ment of patients with mps vi or other inherited metabolic diseases .IN naglazyme treatment should be supervise?
by a doctor the within the management of patients with mps vi or other hereditary metabolic disease .OUT naglazyme ?s treatment must be supervised by a doctor with the experience of the careof patients with mps vi.
or another disease hereditary metabolic .ENSEMBLE naglazyme treatment should be supervised by a physician experiencedin the management of patients with mps vi or other hereditary metabolic disease .Figure 2: Examples illustrating how this method is able to use expertise of both out-of-domain and in-domain systems.Bertoldi and Federico, 2009).
In this approach, asystem is trained on the parallel OUT and IN dataand it is used to translate the monolingual IN dataset.
Iteratively, most confident sentence pairs are se-lected and added to the training corpus on which anew system is trained.5.2 System CombinationTackling the model adaptation problem using sys-tem combination approaches has been experimentedin various work (Koehn and Schroeder, 2007; Hilde-brand and Vogel, 2009).
Among these approachesare sentence-based, phrase-based and word-basedoutput combination methods.
In a similar approach,Koehn and Schroeder (2007) use a feature of the fac-tored translation model framework in Moses SMTsystem (Koehn and Schroeder, 2007) to use multiplealternative decoding paths.
Two decoding paths, onefor each translation table (IN and OUT), were usedduring decoding.
The weights are set with minimumerror rate training (Och, 2003).Our work is closely related to Koehn andSchroeder (2007) but uses a different approach todeal with multiple translation tables.
The MosesSMT system implements (Koehn and Schroeder,2007) and can treat multiple translation tables intwo different ways: intersection and union.
In in-tersection, for each span only the hypotheses wouldbe used that are present in all phrase tables.
Foreach set of hypothesis with the same source andtarget phrases, a new hypothesis is created whosefeature-set is the union of feature sets of all corre-sponding hypotheses.
Union, on the other hand, useshypotheses from all the phrase tables.
The featureset of these hypotheses are expanded to include onefeature set for each table.
However, for the corre-sponding feature values of those phrase-tables thatdid not have a particular phrase-pair, a default logprobability value of 0 is assumed (Bertoldi and Fed-erico, 2009) which is counter-intuitive as it booststhe score of hypotheses with phrase-pairs that do notbelong to all of the translation tables.Our approach is different from Koehn andSchroeder (2007) in a number of ways.
Firstly, un-like the multi-table support of Moses which onlysupports phrase-based translation table combination,our approach supports ensembles of both hierarchi-cal and phrase-based systems.
With little modifica-tion, it can also support ensemble of syntax-basedsystems with the other two state-of-the-art SMT sys-946tems.
Secondly, our combining method uses theunion option, but instead of preserving the featuresof all phrase-tables, it only combines their scoresusing various mixture operations.
This enables usto experiment with a number of different opera-tions as opposed to sticking to only one combinationmethod.
Finally, by avoiding increasing the numberof features we can add as many translation modelsas we need without serious performance drop.
Inaddition, MERT would not be an appropriate opti-mizer when the number of features increases a cer-tain amount (Chiang et al, 2008).Our approach differs from the model combina-tion approach of DeNero et al (2010), a generaliza-tion of consensus or minimum Bayes risk decodingwhere the search space consists of those of multi-ple systems, in that model combination uses forestof derivations of all component models to do thecombination.
In other words, it requires all compo-nent models to fully decode each sentence, computen-gram expectations from each component modeland calculate posterior probabilities over transla-tion derivations.
While, in our approach we onlyuse partial hypotheses from component models andthe derivation forest is constructed by the ensemblemodel.
A major difference is that in the model com-bination approach the component search spaces areconjoined and they are not intermingled as opposedto our approach where these search spaces are inter-mixed on spans.
This enables us to generate newsentences that cannot be generated by componentmodels.
Furthermore, various combination methodscan be explored in our approach.
Finally, main tech-niques used in this work are orthogonal to our ap-proach such as Minimum Bayes Risk decoding, us-ing n-gram features and tuning using MERT.Finally, our work is most similar to that ofLiu et al (2009) where max-derivation and max-translation decoding have been used.
Max-derivation finds a derivation with highest score andmax-translation finds the highest scoring translationby summing the score of all derivations with thesame yield.
The combination can be done in twolevels: translation-level and derivation-level.
Theirderivation-level max-translation decoding is similarto our ensemble decoding with wsum as the mixtureoperation.
We did not restrict ourself to this par-ticular mixture operation and experimented with anumber of different mixing techniques and as Ta-ble 3 shows we could improve over wsum in ourexperimental setup.
Liu et al (2009) used a mod-ified version of MERT to tune max-translation de-coding weights, while we use a two-step approachusing MERT for tuning each component model sep-arately and then using CONDOR to tune componentweights on top of them.6 Conclusion & Future WorkIn this paper, we presented a new approach for do-main adaptation using ensemble decoding.
In thisapproach a number of MT systems are combined atdecoding time in order to form an ensemble model.The model combination can be done using variousmixture operations.
We showed that this approachcan gain up to 2.2 BLEU points over its concatena-tion baseline and 0.39 BLEU points over a powerfulmixture model.Future work includes extending this approach touse multiple translation models with multiple lan-guage models in ensemble decoding.
Differentmixture operations can be investigated and the be-haviour of each operation can be studied in moredetails.
We will also add capability of support-ing syntax-based ensemble decoding and experi-ment how a phrase-based system can benefit fromsyntax information present in a syntax-aware MTsystem.
Furthermore, ensemble decoding can be ap-plied on domain mixing settings in which develop-ment sets and test sets include sentences from dif-ferent domains and genres, and this is a very suit-able setting for an ensemble model which can adaptto new domains at test time.
In addition, we canextend our approach by applying some of the tech-niques used in other system combination approachessuch as consensus decoding, using n-gram features,tuning using forest-based MERT, among other pos-sible extensions.AcknowledgmentsThis research was partially supported by an NSERC,Canada (RGPIN: 264905) grant and a Google Fac-ulty Award to the last author.
We would like tothank Philipp Koehn and the anonymous reviewersfor their valuable comments.
We also thank the de-velopers of GIZA++ and Condor which we used forour experiments.947ReferencesM.
Bacchiani and B. Roark.
2003.
Unsupervised lan-guage model adaptation.
In Acoustics, Speech, andSignal Processing, 2003.
Proceedings.
(ICASSP ?03).2003 IEEE International Conference on, volume 1,pages I?224 ?
I?227 vol.1, april.Nicola Bertoldi and Marcello Federico.
2009.
Do-main adaptation for statistical machine translation withmonolingual resources.
In Proceedings of the FourthWorkshop on Statistical Machine Translation, StatMT?09, pages 182?189, Stroudsburg, PA, USA.
ACL.David Chiang, Yuval Marton, and Philip Resnik.
2008.Online large-margin training of syntactic and struc-tural translation features.
In In Proceedings of theConference on Empirical Methods in Natural Lan-guage Processing.
ACL.David Chiang.
2005.
A hierarchical phrase-based modelfor statistical machine translation.
In ACL ?05: Pro-ceedings of the 43rd Annual Meeting on Associationfor Computational Linguistics, pages 263?270, Mor-ristown, NJ, USA.
ACL.Jorge Civera and Alfons Juan.
2007.
Domain adap-tation in statistical machine translation with mixturemodelling.
In Proceedings of the Second Workshopon Statistical Machine Translation, StatMT ?07, pages177?180, Stroudsburg, PA, USA.
ACL.Jonathan H. Clark, Chris Dyer, Alon Lavie, and Noah A.Smith.
2011.
Better hypothesis testing for statisti-cal machine translation: controlling for optimizer in-stability.
In Proceedings of the 49th Annual Meetingof the Association for Computational Linguistics: Hu-man Language Technologies: short papers - Volume 2,HLT ?11, pages 176?181.
ACL.P.
Clarkson and A. Robinson.
1997.
Language modeladaptation using mixtures and an exponentially decay-ing cache.
In Proceedings of the 1997 IEEE Inter-national Conference on Acoustics, Speech, and Sig-nal Processing (ICASSP ?97)-Volume 2 - Volume 2,ICASSP ?97, pages 799?, Washington, DC, USA.IEEE Computer Society.Hal Daume?, III and Daniel Marcu.
2006.
Domainadaptation for statistical classifiers.
J. Artif.
Int.
Res.,26:101?126, May.John DeNero, Shankar Kumar, Ciprian Chelba, and FranzOch.
2010.
Model combination for machine transla-tion.
In Human Language Technologies: The 2010 An-nual Conference of the North American Chapter of theAssociation for Computational Linguistics, HLT ?10,pages 975?983, Stroudsburg, PA, USA.
ACL.Matthias Eck, Stephan Vogel, and Alex Waibel.
2004.Language model adaptation for statistical machinetranslation based on information retrieval.
In In Pro-ceedings of LREC.George Foster and Roland Kuhn.
2007.
Mixture-modeladaptation for smt.
In Proceedings of the SecondWorkshop on Statistical Machine Translation, StatMT?07, pages 128?135, Stroudsburg, PA, USA.
ACL.George Foster, Cyril Goutte, and Roland Kuhn.
2010.Discriminative instance weighting for domain adapta-tion in statistical machine translation.
In Proceedingsof the 2010 Conference on Empirical Methods in Nat-ural Language Processing, EMNLP ?10, pages 451?459, Stroudsburg, PA, USA.
ACL.Almut Silja Hildebrand and Stephan Vogel.
2009.
CMUsystem combination for WMT?09.
In Proceedings ofthe Fourth Workshop on Statistical Machine Transla-tion, StatMT ?09, pages 47?50, Stroudsburg, PA, USA.ACL.Almut Silja Hildebrand, Matthias Eck, Stephan Vogel,and Alex Waibel.
2005.
Adaptation of the translationmodel for statistical machine translation based on in-formation retrieval.
In Proceedings of the 10th EAMT2005, Budapest, Hungary, May.Geoffrey E. Hinton.
1999.
Products of experts.
In Artifi-cial Neural Networks, 1999.
ICANN 99.
Ninth Interna-tional Conference on (Conf.
Publ.
No.
470), volume 1,pages 1?6.Jing Jiang and ChengXiang Zhai.
2007.
Instance weight-ing for domain adaptation in nlp.
In Proceedings ofthe 45th Annual Meeting of the Association of Com-putational Linguistics, pages 264?271, Prague, CzechRepublic, June.
ACL.Philipp Koehn and Josh Schroeder.
2007.
Experimentsin domain adaptation for statistical machine transla-tion.
In Proceedings of the Second Workshop on Sta-tistical Machine Translation, StatMT ?07, pages 224?227, Stroudsburg, PA, USA.
ACL.Philipp Koehn, Franz Josef Och, and Daniel Marcu.2003.
Statistical phrase-based translation.
In Pro-ceedings of the Human Language Technology Confer-ence of the NAACL, pages 127?133, Edmonton, May.NAACL.Yang Liu, Haitao Mi, Yang Feng, and Qun Liu.
2009.Joint decoding with multiple translation models.
InProceedings of the Joint Conference of the 47th An-nual Meeting of the ACL and the 4th InternationalJoint Conference on Natural Language Processing ofthe AFNLP: Volume 2 - Volume 2, ACL ?09, pages576?584, Stroudsburg, PA, USA.
ACL.F.
J. Och and H. Ney.
2000.
Improved statistical align-ment models.
In Proceedings of the 38th Annual Meet-ing of the ACL, pages 440?447, Hongkong, China, Oc-tober.Franz Josef Och.
2003.
Minimum error rate training forstatistical machine translation.
In Proceedings of the41th Annual Meeting of the ACL, Sapporo, July.
ACL.948Slav Petrov.
2010.
Products of random latent variablegrammars.
In Human Language Technologies: The2010 Annual Conference of the North American Chap-ter of the Association for Computational Linguistics,HLT ?10, pages 19?27, Stroudsburg, PA, USA.
ACL.Fatiha Sadat, Howard Johnson, Akakpo Agbago, GeorgeFoster, Joel Martin, and Aaron Tikuisis.
2005.Portage: A phrase-based machine translation system.In In Proceedings of the ACL Worskhop on Buildingand Using Parallel Texts, Ann Arbor.
ACL.Baskaran Sankaran, Majid Razmara, and Anoop Sarkar.2012.
Kriya an end-to-end hierarchical phrase-basedmt system.
The Prague Bulletin of Mathematical Lin-guistics, 97(97), April.Kristie Seymore and Ronald Rosenfeld.
1997.
Us-ing story topics for language model adaptation.
InGeorge Kokkinakis, Nikos Fakotakis, and EvangelosDermatas, editors, EUROSPEECH.
ISCA.Andrew Smith, Trevor Cohn, and Miles Osborne.
2005.Logarithmic opinion pools for conditional randomfields.
In Proceedings of the 43rd Annual Meeting onAssociation for Computational Linguistics, ACL ?05,pages 18?25, Stroudsburg, PA, USA.
ACL.Andreas Stolcke.
2002.
SRILM ?
an extensible languagemodeling toolkit.
In Proceedings International Con-ference on Spoken Language Processing, pages 257?286.Jorg Tiedemann.
2009.
News from opus - a collectionof multilingual parallel corpora with tools and inter-faces.
In N. Nicolov, K. Bontcheva, G. Angelova,and R. Mitkov, editors, Recent Advances in NaturalLanguage Processing, volume V, pages 237?248.
JohnBenjamins, Amsterdam/Philadelphia.Nicola Ueffing, Gholamreza Haffari, and Anoop Sarkar.2007.
Transductive learning for statistical machinetranslation.
In Proceedings of the 45th Annual Meet-ing of the Association of Computational Linguistics,pages 25?32, Prague, Czech Republic, June.
ACL.Frank Vanden Berghen and Hugues Bersini.
2005.
CON-DOR, a new parallel, constrained extension of pow-ell?s UOBYQA algorithm: Experimental results andcomparison with the DFO algorithm.
Journal of Com-putational and Applied Mathematics, 181:157?175,September.949
