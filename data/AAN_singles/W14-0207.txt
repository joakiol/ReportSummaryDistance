Proceedings of the of the EACL 2014 Workshop on Dialogue in Motion (DM), pages 43?47,Gothenburg, Sweden, April 26-30 2014.c?2014 Association for Computational LinguisticsCollaborative Exploration in Human-Robot Teams:What?s in Their Corpora of Dialog, Video, & LIDAR Messages?Clare R. Voss?Taylor Cassidy?
?Douglas Summers-Stay?
?Army Research Laboratory, Adelphi, MD 20783?IBM T. J. Watson Research Center, Hawthorne, NY 10532{clare.r.voss.civ,taylor.cassidy.ctr,douglas.a.summers-stay.civ}@mail.milAbstractThis paper briefly sketches new work-in-progress (i) developing task-based scenar-ios where human-robot teams collabora-tively explore real-world environments inwhich the robot is immersed but the hu-mans are not, (ii) extracting and construct-ing ?multi-modal interval corpora?
fromdialog, video, and LIDAR messages thatwere recorded in ROS bagfiles during tasksessions, and (iii) testing automated meth-ods to identify, track, and align co-referentcontent both within and across modalitiesin these interval corpora.
The pre-pilotstudy and its corpora provide a unique,empirical starting point for our longer-term research objective: characterizing thebalance of explicitly shared and tacitly as-sumed information exchanged during ef-fective teamwork.1 OverviewRobots that are able to move into areas where peo-ple cannot during emergencies and collaborativelyexplore these environments by teaming with hu-mans, have tremendous potential to impact searchand rescue operations.
For human-robot teamsto conduct such shared missions, humans need totrust that they will be kept apprised, at a miniu-Figure 1: Outside View: Video Image & LIDAR.mum, of where the robot is and what it is sensing,as it moves about without them present.To begin documenting the communication chal-lenges humans face in taking a robot?s perspective,we conducted a pre-pilot study1to record, iden-tify and track the dialog, video, and LIDAR in-formation that is explicitly shared by, or indirectlyavailable to, members of human-robot teams whenconducting collaborative tasks.1.1 ApproachWe enlisted colleagues to be the commander (C) orthe human (R) controlling a mobile physical robotin such tasks.
Neither could see the robot.
OnlyR could ?see for?
the robot, via its onboard videocamera and LIDAR.
C and R communicated bytext chat on their computers, as in this example,R 41: I can see in the entrance.C 42: Enter and scan the first room.R 44: I see a door to the right and a door to the left.C 45: Scan next open room on left.Utterances R 41 & C 42 occur when the robot isoutdoors (Fig.
1) and R 44 & C 45 occur after itmoves indoors (Fig.
2).
Although our approach re-sembles a Wizard and Oz paradigm (Riek, 2012),1Statisticians say pre-pilots are for ?kicking the tires,?early-stage tests of scenarios, equipment, and data collection.Figure 2: Inside View: Video Image & LIDAR.Brightness and contrast of video image increasedfor print publication.43with C as User and R as Wizard controlling therobot, there is no intent for R to deceive C.In these dialog snippets, notice that the doorsmentioned in R 44 are not visible in the imageof that utterance?s time interval and, even if theyhad been visible, their referents were context-dependent and ambiguous.
How are the robot andhuman to refer to the same door?
This challengeentails resolving several types of co-reference (lin-guistic, are they talking about the same door?
vi-sual, are they looking at the door?
navigational, isone backing into a door no longer in view but pre-viosuly stored in its map?)
Successful communi-cation on human-robot teams, where humans sendmessages to direct robot movements and receiverobot-processed messages as the robot navigates,entails effective identification of named referents(such as doors), both within and across availablemodalities during exploratory tasks.
The researchquestion is, how might the identification and align-ment of entities using combinations of (i) NLPon dialog, (ii) image processing on the video andLIDAR stream, with (iii) robot position, motion,and orientation coordinates, support more effec-tive human-robot missions?We conducted the pre-pilot study with ten trialsessions to collect multi-modal data from C-R andR-only scenarios (Table 1).
Each session involveda single participant playing the role of R with con-trol over the physical robot, or two participants,one person playing R and one playing C.Team R?s TaskR only Rotate in place and describe surroundings.R only Move along road, describe surroundings.C, R Follow C?s guidance in navigating build-ing?s perimeter, describe surroundings.C, R Follow C?s guidance in searching buildingsfor specified objects.Table 1: Pre-pilot Scenarios.Participants sat indoors and could not see the robotoutside, roughly 30 meters away.
In each session,R was instructed to act as though he or she weresituated in the robot?s position and to obey C. Rwas to consider the robot?s actions as R?s own,and to consider available video and LIDAR pointcloud feeds as R?s own perceptions.1.2 EquipmentAll participants worked from their own comput-ers.
Each was instructed, for a given scenario, tobe either C or R and to communicate by text only.On their screen they saw a dedicated dialog (chat)window in a Linux terminal.
For sessions withboth C and R, the same dialog content (the ongo-ing sequence of typed-in utterances) appeared inthe dialog window on each of their screens.The physical robot ran under the Robot Operat-ing System (ROS) (Quigley et al., 2009), equippedwith a video camera, laser sensors, magnetome-ter, GPS unit, and rotary encoders.
R could ?seefor the robot?
via two ROS rviz windows with livefeeds for video from the robot?s camera and con-structed 3D point cloud frames.2R had access torotate and zoom functions to alter the screen dis-play of the point cloud.
C saw only a static bird?s-eye-view map of the area.
R remotely controlledover a network connection the robot?s four wheelsand its motion, using the left joystick of an X-Boxcontroller.1.3 CollectionDuring each session, all data from the robot?s sen-sors and dialog window was recorded via the ros-bag tool and stored in a single bagfile.3A bagfilecontains typed messages.
Each message containsa timestamp (specified at nanosecond granularity)and values for that message type?s attributes.
Mes-sage types geometry msgs/PoseStamped, for ex-ample, contain a time stamp, a three-dimensionallocation vector and a four-dimensional orientationvector that indicates an estimate of the robot?s lo-cation and the direction in which it is facing.
Therobot?s rotary encoders generate these messagesas the robot moves.
The primary bagfile messagetypes most relevant to our initial analyses4were:1) instant messenger/StringStampedthat included speaker id, text utterances2) sensor msgs/PointCloud2that included LIDAR data3) sensor msgs/CompressedImagewith compressed, rectified video images4) sensor msgs/GPS, with robot coordinatesMessage types are packaged and published at dif-ferent rates: some are published automatically atregular intervals (e.g., image frames), while oth-ers depend on R, C, or robot activity (e.g., dialogutterances).
And the specific rate of publicationfor some message types can be limited at times bynetwork bandwidth constraints (e.g.
LIDAR data).Summary statistics for our initial pre-pilot collec-2LIDAR measures distance from robot by illuminatingtargets with robot lasers and generates point cloud messages.3http://wiki.ros.org/rosbag4We omit here details of ROS topics, transformation mes-sages, and other sensor data collected in the pre-pilot.44tion consisting of ten task sessions conducted overtwo days, and that together spanned roughly fivehours in real-time, are presented in Table 2.#bagfile msgs 15, 131K #dialog utts 434min per sn 140, 848 min per sn 15max per sn 3, 030K max per sn 116#tokens 3, 750 #image msgs 10, 650min per sn 200 min per sn 417max per sn 793 max per sn 1, 894#unique words 568 #LIDAR msgs 8, 422min per sn 84 min per sn 215max per sn 176 max per sn 2, 250Table 2: Collection Statistics (sn = session).2 From Collection to Interval CorporaAfter collecting millions of messages in the pre-pilot with content in different modalities, the im-mediate research challenge has been identifyingthe time interval that covers the messages directlyrelated to the content in each utterance.We extracted each utterance message u and itscorresponding time stamp t. For a given u, we ex-tracted the five image, five point cloud, and fiveGPS messages immediately preceding and the fiveof each immediately following u, based on mes-sage time-stamps, for a total of thirty sensor mes-sages per utterance.
These message types werepublished independent of the robot?s movement,approximately once per second.
In the secondphase, we assigned the earliest and latest timestamp from the first-phase messages to delimit aninterval [ts, te] and conducted another extractionround from the bagfile, this time pulling out allmessages with time stamps in that interval as pub-lished by the rotary encoders, compass, and iner-tial measurement unit, only when the robot moved.The messages from both phases constitute a ten-second interval corpus for u.These interval corpora serve as a first approx-imation at segmenting the massive stream pub-lished at nanosecond-level into units pertaining tocommander-robot dialog during the task at hand.With manual inspection, we found that manyautomatically-constructed intervals do track rele-vant changes in the robot?s location.
For exam-ple, the latest interval in a task?s time sequencethat was constructed with the robot being outside abuilding is distinct from the first interval that cov-ers when the robot moves inside the building.55This appears likely due to the paced descriptions in R?sutterances.
Another pre-pilot is needed to test this hypothesis.3 Corpora Language ProcessingEach utterance collected from the sessions wastokenized, parsed, and semantically interpretedusing SLURP (Brooks et al., 2012), a well-tested NLP front-end component of a human-robotsystem.6The progression in SLURP?s analysispipeline for utterance C 45 is shown in Figure 3.SLURP extracts a parse tree (top-left), identifiesa sub-tree that constitutes a verb-argument struc-ture, and enumerates possibly matching sense-specific verb frames from VerbNet (Schuler, 2005)(bottom-left).
VerbNet provides a syntactic to se-mantic role mapping for each frame (top-right).SLURP selects the best mapping and generates acompact semantic representation (bottom-right).7In this example, the correct sense of ?scan?
is se-lected (investigate-35.4) along with a frame thatmatches the syntactic parse.
Overall, half the com-mands run through SLURP generated a semanticinterpretation.
Of the other half, roughly one quar-ter failed or had errors at parsing and the otherquarter at the argument matching stage.Figure 3: Analyses of Scan next open room on left.Our next step is to augment SLURP?s lexiconand retrain a parser for new vocabulary so that wecan directly map semantic structures of the pre-pilot corpora into ResearchCyc8, an extensive on-tology, for cross-reference to other events and ob-jects, already stored and possibly originated as vi-sual input.
Following McFate (2010), we will test6https://github.com/PennNLP/SLURP.7Verbnet associates each frame with a conjunction ofboolean semantic predicates that specify how and when eventparticipants interact, for an event variable (not shown).8ResearchCyc and CycL are trademarks of Cycorp, Inc.45Figure 4: Outside View: Image, Zones, Overlaythe mapping of matched VerbNet frames to Re-searchCyc?s semantic predicates to assess its lexi-cal coverage for our corpora.4 Image ProcessingInterval corpus images were labelled by a neu-ral network trained for visual scene classifica-tion (Munoz, 2013) of nine material classes: dirt,foliage, grass, road, sidewalk, sky, wall, wood, andground cover (organic debris).
Figures 4 and 5show the images from Figures 1 and 2 with twoadditional versions: one with colored zones forsystem-recognized class boundaries and anotherwith colored zones as trasparent overlays on theoriginal.
The classes differentiate terrain typesthat work well with route-finding techniques thatleverage them in selecting traversible paths.
As therobot systems are enhanced with more sophisti-cated path planning software, that knowledge maybe combined with recognized zones to send teammembers messages about navigation problems asthe robot explores where they cannot go.Accuracy is limited at the single image level:the actual grass in Figure 4 is mostly mis-classifiedas dirt (blue) along with some correctly identifiedgrass (green), while the floor in Figure 5 is mis-classified as road, although much of what showsthrough the window is correctly classified as fo-liage.
We are experimenting with automaticallyassigning natural language (NL) labels to a rangeof objects and textures recognized in images fromother larger datasets.
We can retrieve labeled im-ages stored in ResearchCyc via NL query con-verted into CycL, allowing a commander to, forexample, ask questions about objects and regionsusing terms related to but not necessarily equal tothe original recognition system-provided labels.5 Related WorkWe are aware of no other multi-modal corporaobtained from human-robot teams conducting ex-ploratory missions with collected dialog, videoand other sensor data.
Corpora with a robotFigure 5: Inside View: Image, Zones, Overlay.Brightness and contrast of video image and over-lay increased for print publication.recording similar data modalities do exist (Greenet al., 2006; Wienke et al., 2012; Maas et al., 2006)but for fundamentally different tasks.
Tellex et al.
(2011) and Matuszek et al.
(2012) pair commandswith formal plans without dialog and Zender et al.
(2008) and Randelli et al.
(2013) build multi-levelmaps but with a situated commander.Eberhard et al.
(2010)?s CReST corpus containsa set-up similar to ours minus the robot; a hu-man task-solver wears a forward-facing camerainstead.
The SCARE corpus (Stoia et al., 2008)records similar modalities but in a virtual environ-ment, where C has full access to R?s video feed.Other projects yielded corpora from virtual envi-ronments that include route descriptions withoutdialog (Marge and Rudnicky, 2011; MacMahon etal., 2006; Vogel and Jurafsky, 2010) or referringexpressions without routes (Sch?utte et al., 2010;Fang et al., 2013), assuming pre-existing abstrac-tions from sensor data.6 Conclusion and Ongoing WorkWe have presented our pre-pilot study with datacollection and corpus construction phases.
Thiswork-in-progress requires further analysis.
We arenow processing dialog utterances for more system-atic semantic interpretation using disambiguatedVerbNet frames that map into ResearchCyc pred-icates.
We will run object recognition softwareretrained on a broader range of objects so thatit can be applied to images that will be labelledand stored in ResearchCyc micro-worlds for sub-sequent co-reference with terms in the dialog ut-terances.
Ultimately we want to establish in realtime links across parts of messages in differentmodalities that refer to the same abstract enti-ties, so that humans and robots can share theirseparately-obtained knowledge about the entitiesand their spatial relations ?
whether seen, sensed,described, or inferred ?
when communicating onshared tasks in environments.46AcknowledgmentsOver a dozen engineers and researchers assistedus in many ways before, during, and after the pre-pilot, providing technical help with equipment anddata collection, as well as participating in the pre-pilot.
We cannot list everyone here, but specialthanks to Stuart Young for providing clear guid-ance to everyone working with us.ReferencesDaniel J. Brooks, Constantine Lignos, Cameron Finu-cane, Mikhail S. Medvedev, Ian Perera, VasumathiRaman, Hadas Kress-Gazit, Mitch Marcus, andHolly A. Yanco.
2012.
Make it so: Continu-ous, flexible natural language interaction with an au-tonomous robot.
In Proc.
AAAI, pages 2?8.Kathleen M. Eberhard, Hannele Nicholson, SandraK?ubler, Susan Gundersen, and Matthias Scheutz.2010.
The indiana ?cooperative remote search task?
(crest) corpus.
In Proc.
LREC.Rui Fang, Changsong Liu, Lanbo She, and Joyce Y.Chai.
2013.
Towards situated dialogue: Revisitingreferring expression generation.
In Proc.
EMNLP,pages 392?402.Anders Green, Helge Httenrauch, and Kerstin Severin-son Eklundh.
2006.
Developing a contextualizedmultimodal corpus for human-robot interaction.
InProc.
LREC.Jan F. Maas, Britta Wrede, and Gerhard Sagerer.
2006.Towards a multimodal topic tracking system for amobile robot.
In Proc.
INTERSPEECH.Matt MacMahon, Brian Stankiewicz, and BenjaminKuipers.
2006.
Walk the talk: Connecting language,knowledge, and action in route instructions.
In Proc.AAAI, pages 1475?1482.Matthew Marge and Alexander I Rudnicky.
2011.The teamtalk corpus: Route instructions in openspaces.
In Proc.
RSS, Workshop on GroundingHuman-Robot Dialog for Spatial Tasks.Cynthia Matuszek, Evan Herbst, Luke S. Zettlemoyer,and Dieter Fox.
2012.
Learning to parse naturallanguage commands to a robot control system.
InProc.
ISER, pages 403?415.Clifton McFate.
2010.
Expanding verb coverage incyc with verbnet.
In Proc.
ACL, Student ResearchWorkshop, pages 61?66.Daniel Munoz.
2013.
Inference Machines: Pars-ing Scenes via Iterated Predictions.
Ph.D. thesis,Carnegie Mellon University.Morgan Quigley, Ken Conley, Brian Gerkey, JoshFaust, Tully B. Foote, Jeremy Leibs, Rob Wheeler,and Andrew Y. Ng.
2009.
ROS: an open-sourcerobot operating system.
In Proc.
ICRA, Workshopon Open Source Software.Gabriele Randelli, Taigo Maria Bonanni, Luca Iocchi,and Daniele Nardi.
2013.
Knowledge acquisitionthrough human?robot multimodal interaction.
Intel-ligent Service Robotics, 6(1):19?31.Laurel D Riek.
2012.
Wizard of oz studies in hri:A systematic review and new reporting guidelines.Journal of Human-Robot Interaction, 1(1).Karin Kipper Schuler.
2005.
Verbnet: A Broad-coverage, Comprehensive Verb Lexicon.
Ph.D. the-sis, University of Pennsylvania.Niels Sch?utte, John D. Kelleher, and Brian MacNamee.
2010.
Visual salience and reference reso-lution in situated dialogues: A corpus-based evalu-ation.
In Proc.
AAAI, Fall Symposium: Dialog withRobots.Laura Stoia, Darla Magdalena Shockley, Donna K. By-ron, and Eric Fosler-Lussier.
2008.
Scare: a situ-ated corpus with annotated referring expressions.
InProc.
LREC.Stefanie Tellex, Thomas Kollar, Steven Dickerson,Matthew R. Walter, Ashis Gopal Banerjee, Seth J.Teller, and Nicholas Roy.
2011.
Understanding nat-ural language commands for robotic navigation andmobile manipulation.
In Proc.
AAAI.Adam Vogel and Daniel Jurafsky.
2010.
Learning tofollow navigational directions.
In Proc.
ACL, pages806?814.Johannes Wienke, David Klotz, and Sebastian Wrede.2012.
A framework for the acquisition of mul-timodal human-robot interaction data sets with awhole-system perspective.
In Proc.
LREC, Work-shop on Multimodal Corpora for Machine Learning.Hendrik Zender, O Mart?
?nez Mozos, Patric Jensfelt, G-JM Kruijff, and Wolfram Burgard.
2008.
Concep-tual spatial representations for indoor mobile robots.Robotics and Autonomous Systems, 56(6):493?502.47
