Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 579?586,Sydney, July 2006. c?2006 Association for Computational LinguisticsIntegrating Pattern-based and Distributional Similarity Methods forLexical Entailment AcquisitionShachar Mirkin     Ido Dagan         Maayan GeffetSchool of Computer Science and EngineeringThe Hebrew University, Jerusalem, Israel,91904mirkin@cs.huji.ac.ilDepartment of Computer ScienceBar-Ilan University, Ramat Gan, Israel,52900{dagan,zitima}@cs.biu.ac.ilAbstractThis paper addresses the problem of acquir-ing lexical semantic relationships, applied tothe lexical entailment relation.
Our main con-tribution is a novel conceptual integrationbetween the two distinct acquisition para-digms for lexical relations ?
the pattern-based and the distributional similarity ap-proaches.
The integrated method exploitsmutual complementary information of thetwo approaches to obtain candidate relationsand informative characterizing features.Then, a small size training set is used to con-struct a more accurate supervised classifier,showing significant increase in both recalland precision over the original approaches.1 IntroductionLearning lexical semantic relationships is a fun-damental task needed for most text understand-ing applications.
Several types of lexicalsemantic relations were proposed as a goal forautomatic acquisition.
These include lexical on-tological relations such as synonymy, hyponymyand meronymy, aiming to automate the construc-tion of WordNet-style relations.
Another com-mon target is learning general distributionalsimilarity between words, following Harris' Dis-tributional Hypothesis (Harris, 1968).
Recently,an applied notion of entailment between lexicalitems was proposed as capturing major inferenceneeds which cut across multiple semantic rela-tionship types (see Section 2 for further back-ground).The literature suggests two major approachesfor learning lexical semantic relations: distribu-tional similarity and pattern-based.
The first ap-proach recognizes that two words (or two multi-word terms) are semantically similar based ondistributional similarity of the different contextsin which the two words occur.
The distributionalmethod identifies a somewhat loose notion ofsemantic similarity, such as between companyand government, which does not ensure that themeaning of one word can be substituted by theother.
The second approach is based on identify-ing joint occurrences of the two words withinparticular patterns, which typically indicate di-rectly concrete semantic relationships.
The pat-tern-based approach tends to yield more accuratehyponymy and (some) meronymy relations, butis less suited to acquire synonyms which onlyrarely co-occur within short patterns in texts.
Itshould be noted that the pattern-based approachis commonly applied also for information andknowledge extraction to acquire factual instancesof concrete meaning relationships (e.g.
born in,located at) rather than generic lexical semanticrelationships in the language.While the two acquisition approaches arelargely complementary, there have been just fewattempts to combine them, usually by pipelinearchitecture.
In this paper we propose a method-ology for integrating distributional similaritywith the pattern-based approach.
In particular,we focus on learning the lexical entailment rela-tionship between common nouns and nounphrases (to be distinguished from learning rela-tionships for proper nouns, which usually fallswithin the knowledge acquisition paradigm).The underlying idea is to first identify candi-date relationships by both the distributional ap-proach, which is applied exhaustively to a localcorpus, and the pattern-based approach, appliedto the web.
Next, each candidate is representedby a unified set of distributional and pattern-based features.
Finally, using a small training setwe devise a supervised (SVM) model that classi-fies new candidate relations as correct or incor-rect.To implement the integrated approach we de-veloped state of the art pattern-based acquisition579methods and utilized a distributional similaritymethod that was previously shown to providesuperior performance for lexical entailment ac-quisition.
Our empirical results show that theintegrated method significantly outperforms eachapproach in isolation, as well as the na?ve com-bination of their outputs.
Overall, our methodreveals complementary types of information thatcan be obtained from the two approaches.2 Background2.1 Distributional Similarity andLexical EntailmentThe general idea behind distributional similarityis that words which occur within similar contextsare semantically similar (Harris, 1968).
In acomputational framework, words are representedby feature vectors, where features are contextwords weighted by a function of their statisticalassociation with the target word.
The degree ofsimilarity between two target words is then de-termined by a vector comparison function.Amongst the many proposals for distributionalsimilarity measures, (Lin, 1998) is maybe themost widely used one, while (Weeds et al, 2004)provides a typical example for recent research.Distributional similarity measures are typicallycomputed through exhaustive processing of acorpus, and are therefore applicable to corpora ofbounded size.It was noted recently by Geffet and Dagan(2004, 2005) that distributional similarity cap-tures a quite loose notion of semantic similarity,as exemplified by the pair country ?
party (iden-tified by Lin's similarity measure).
Consequently,they proposed a definition for the lexical entail-ment relation, which conforms to the generalframework of applied textual entailment (Daganet al, 2005).
Generally speaking, a word w lexi-cally entails another word v if w can substitute vin some contexts while implying v's originalmeaning.
It was suggested that lexical entailmentcaptures major application needs in modelinglexical variability, generalized over several typesof known ontological relationships.
For example,in Question Answering (QA), the word companyin a question can be substituted in the text byfirm (synonym), automaker (hyponym) or sub-sidiary (meronym), all of which entail company.Typically, hyponyms entail their hypernyms andsynonyms entail each other, while entailmentholds for meronymy only in certain cases.In this paper we investigate automatic acquisi-tion of the lexical entailment relation.
For thedistributional similarity component we employthe similarity scheme of (Geffet and Dagan,2004), which was shown to yield improved pre-dictions of (non-directional) lexical entailmentpairs.
This scheme utilizes the symmetric simi-larity measure of (Lin, 1998) to induce improvedfeature weights via bootstrapping.
These weightsidentify the most characteristic features of eachword, yielding cleaner feature vector representa-tions and better similarity assessments.2.2 Pattern-based ApproachesHearst (1992) pioneered the use of lexical-syntactic patterns for automatic extraction oflexical semantic relationships.
She acquired hy-ponymy relations based on a small predefined setof highly indicative patterns, such as ?X, .
.
.
, Yand/or other Z?, and ?Z such as X, .
.
.
and/or Y?,where X and Y are extracted as hyponyms of Z.Similar techniques were further applied to pre-dict hyponymy and meronymy relationships us-ing lexical or lexico-syntactic patterns (Berlandand Charniak, 1999; Sundblad, 2002), and webpage structure was exploited to extract hy-ponymy relationships by Shinzato and Torisawa(2004).
Chklovski and Pantel (2004) used pat-terns to extract a set of relations between verbs,such as similarity, strength and antonymy.
Syno-nyms, on the other hand, are rarely found in suchpatterns.
In addition to their use for learning lexi-cal semantic relations, patterns were commonlyused to learn instances of concrete semantic rela-tions for Information Extraction (IE) and QA, asin (Riloff and Shepherd, 1997; Ravichandran andHovy, 2002; Yangarber et al, 2000).Patterns identify rather specific and informa-tive structures within particular co-occurrencesof the related words.
Consequently, they are rela-tively reliable and tend to be more accurate thandistributional evidence.
On the other hand, theyare susceptive to data sparseness in a limited sizecorpus.
To obtain sufficient coverage, recentworks such as (Chklovski and Pantel, 2004) ap-plied pattern-based approaches to the web.
Thesemethods form search engine queries that matchlikely pattern instances, which may be verifiedby post-processing the retrieved texts.Another extension of the approach was auto-matic enrichment of the pattern set through boot-strapping.
Initially, some instances of the sought580relation are found based on a set of manuallydefined patterns.
Then, additional co-occurrences of the related terms are retrieved,from which new patterns are extracted (Riloffand Jones, 1999; Pantel et al, 2004).
Eventually,the list of effective patterns found for ontologicalrelations has pretty much converged in the litera-ture.
Amongst these, Table 1 lists the patternsthat were utilized in our work.Finally, the selection of candidate pairs for atarget relation was usually based on some func-tion over the statistics of matched patterns.
Toperform more systematic selection Etzioni et al(2004) applied a supervised Machine Learningalgorithm (Na?ve Bayes), using pattern statisticsas features.
Their work was done within the IEframework, aiming to extract semantic relationinstances for proper nouns, which occur quitefrequently in indicative patterns.
In our work weincorporate and extend the supervised learningstep for the more difficult task of acquiring gen-eral language relationships between commonnouns.2.3 Combined ApproachesIt can be noticed that the pattern-based and dis-tributional approaches have certain complemen-tary properties.
The pattern-based method tendsto be more precise, and also indicates the direc-tion of the relationship between the candidateterms.
The distributional similarity approach ismore exhaustive and suitable to detect symmetricsynonymy relations.
Few recent attempts on re-lated (though different) tasks were made to clas-sify (Lin et al, 2003) and label (Pantel andRavichandran, 2004) distributional similarityoutput using lexical-syntactic patterns, in a pipe-line architecture.
We aim to achieve tighter inte-gration of the two approaches, as described next.3 An Integrated Approach for Lexi-cal Entailment AcquisitionThis section describes our integrated approachfor acquiring lexical entailment relationships,applied to common nouns.
The algorithm re-ceives as input a target term and aims to acquirea set of terms that either entail or are entailed byit.
We denote a pair consisting of the input targetterm and an acquired entailing/entailed term asentailment pair.
Entailment pairs are directional,as in bank  company.Our approach applies a supervised learningscheme, using SVM, to classify candidate en-tailment pairs as correct or incorrect.
The SVMtraining phase is applied to a small constantnumber of training pairs, yielding a classificationmodel that is then used to classify new test en-tailment pairs.
The designated training set is alsoused to tune some additional parameters of themethod.
Overall, the method consists of the fol-lowing main components:1: Acquiring candidate entailment pairs forthe input term by pattern-based and distribu-tional similarity methods (Section 3.2);2: Constructing a feature set for all candidatesbased on pattern-based and distributional in-formation (Section 3.3);3: Applying SVM training and classificationto the candidate pairs (Section 3.4).The first two components, of acquiring candidatepairs and collecting features for them, utilize ageneric module for pattern-based extraction fromthe web, which is described first in Section 3.1.3.1 Pattern-based Extraction Mod-uleThe general pattern-based extraction module re-ceives as input a set of lexical-syntactic patterns(as in Table 1) and either a target term or a can-didate pair of terms.
It then searches the web foroccurrences of the patterns with the input term(s).A small set of effective queries is created foreach pattern-terms combination, aiming to re-trieve as much relevant data with as few queriesas possible.Each pattern has two variable slots to be in-stantiated by candidate terms for the sought rela-tion.
Accordingly, the extraction module can be1 NP1 such as NP22 Such NP1 as NP23 NP1 or other NP24 NP1 and other NP25 NP1 ADV known as NP26 NP1 especially NP27 NP1 like NP28 NP1 including NP29 NP1-sg is (a OR an) NP2-sg10 NP1-sg (a OR an) NP2-sg11 NP1-pl are NP2-plTable 1: The patterns we used for entailment ac-quisition based on (Hearst, 1992) and (Pantel et al,2004).
Capitalized terms indicate variables.
pl andsg stand for plural and singular forms.581used in two modes: (a) receiving a single targetterm as input and searching for instantiations ofthe other variable to identify candidate relatedterms (as in Section 3.2); (b) receiving a candi-date pair of terms for the relation and searchingpattern instances with both terms, in order tovalidate and collect information about the rela-tionship between the terms (as in Section 3.3).Google proximity search1 provides a useful toolfor these purposes, as it allows using a wildcardwhich might match either an un-instantiated termor optional words such as modifiers.
For exam-ple, the query "such ** as *** (war OR wars)" isone of the queries created for the input patternsuch NP1 as NP2 and the input target term war,allowing new terms to match the first patternvariable.
For the candidate entailment pair war?
struggle, the first variable is instantiated aswell.
The corresponding query would be: "such *(struggle OR struggles) as *** (war OR wars)?.This technique allows matching terms that aresub-parts of more complex noun phrases as wellas multi-word terms.The automatically constructed queries, cover-ing the possible combinations of multiple wild-cards, are submitted to Google2 and a specifiednumber of snippets is downloaded, while avoid-ing duplicates.
The snippets are passed through aword splitter and a sentence segmenter3, whilefiltering individual sentences that do not containall search terms.
Next, the sentences are proc-essed with the OpenNLP4  POS tagger and NPchunker.
Finally, pattern-specific regular expres-sions over the chunked sentences are applied toverify that the instantiated pattern indeed occursin the sentence, and to identify variable instantia-tions.On average, this method extracted more than3300 relationship instances for every 1MB ofdownloaded text, almost third of them containedmulti-word terms.3.2 Candidate AcquisitionGiven an input target term we first employ pat-tern-based extraction to acquire entailment paircandidates and then augment the candidate setwith pairs obtained through distributional simi-larity.1Previously used by (Chklovski and Pantel, 2004).2http://www.google.com/apis/3 Available from the University of Illinois at Urbana-Champaign, http://l2r.cs.uiuc.edu/~cogcomp/tools.php4www.opennlp.sourceforge.net/3.2.1 Pattern-based CandidatesAt the candidate acquisition phase pattern in-stances are searched with one input target term,looking for instantiations of the other patternvariable to become the candidate related term(the first querying mode described in Section3.1).
We construct two types of queries, in whichthe target term is either the first or second vari-able in the pattern, which corresponds to findingeither entailing or entailed terms that instantiatethe other variable.In the candidate acquisition phase we utilizedpatterns 1-8 in Table 1, which we empiricallyfound as most suitable for identifying directionallexical entailment pairs.
Patterns 9-11 are notused at this stage as they produce too much noisewhen searched with only one instantiated vari-able.
About 35 queries are created for each targetterm in each entailment direction for each of the8 patterns.
For every query, the first n snippetsare downloaded (we used n=50).
Thedownloaded snippets are processed as describedin Section 3.1, and candidate related terms areextracted, yielding candidate entailment pairswith the input target term.Quite often the entailment relation holds be-tween multi-word noun-phrases rather thanmerely between their heads.
For example, tradecenter lexically entails shopping complex, whilecenter does not necessarily entail complex.
Onthe other hand, many complex multi-word nounphrases are too rare to make a statistically baseddecision about their relation with other terms.Hence, we apply the following two criteria tobalance these constraints:1.
For the entailing term we extract only thecomplete noun-chunk which instantiate thepattern.
For example: we extract housingproject ?
complex, but do not extract pro-ject as entailing complex since the head nounalone is often too general to entail the otherterm.2.
For the entailed term we extract both thecomplete noun-phrase and its head in orderto create two separate candidate entailmentpairs with the entailing term, which will bejudged eventually according to their overallstatistics.As it turns out, a large portion of the extractedpairs constitute trivial hyponymy relations,where one term is a modified version of the other,like low interest loan ?
loan.
These pairs wereremoved, along with numerous pairs includingproper nouns, following the goal of learning en-582tailment relationships for distinct commonnouns.Finally, we filter out the candidate pairs whosefrequency in the extracted patterns is less than athreshold, which was set empirically to 3.
Usinga lower threshold yielded poor precision, while athreshold of 4 decreased recall substantially withjust little effect on precision.3.2.2 Distributional SimilarityCandidatesAs mentioned in Section 2, we employ the distri-butional similarity measure of (Geffet and Da-gan, 2004) (denoted here GD04 for brevity),which was found effective for extracting non-directional lexical entailment pairs.
Using localcorpus statistics, this algorithm produces for eachtarget noun a scored list of up to a few hundredwords with positive distributional similarityscores.Next we need to determine an optimal thresh-old for the similarity score, considering wordsabove it as likely entailment candidates.
To tunesuch a threshold we followed the original meth-odology used to evaluate GD04.
First, the top-k(k=40) similarities of each training term aremanually annotated by the lexical entailment cri-terion (see Section 4.1).
Then, the similarityvalue which yields the maximal micro-averagedF1 score is selected as threshold, suggesting anoptimal recall-precision tradeoff.
The selectedthreshold is then used to filter the candidate simi-larity lists of the test words.Finally, we remove all entailment pairs that al-ready appear in the candidate set of the pattern-based approach, in either direction (recall that thedistributional candidates are non-directional).Each of the remaining candidates generates twodirectional pairs which are added to the unifiedcandidate set of the two approaches.3.3 Feature ConstructionNext, each candidate is represented by a set offeatures, suitable for supervised classification.
Tothis end we developed a novel feature set basedon both pattern-based and distributional data.To obtain pattern statistics for each pair, thesecond mode of the pattern-based extractionmodule is applied (see Section 3.1).
As in thiscase, both variables in the pattern are instantiatedby the terms of the pair, we could use all elevenpatterns in Table 1, creating a total of about 55queries per pair and downloading m=20 snippetsfor each query.
The downloaded snippets areprocessed as described in Section 3.1 to identifypattern matches and obtain relevant statistics forfeature scores.Following is the list of feature types computedfor each candidate pair.
The feature set was de-signed specifically for the task of extracting thecomplementary information of the two methods.Conditional Pattern Probability: This type offeature is created for each of the 11 individualpatterns.
The feature value is the estimated con-ditional probability of having the patternmatched in a sentence given that the pair of termsdoes appear in the sentence (calculated as thefraction of pattern matches for the pair amongstall unique sentences that contain the pair).
Thisfeature yields normalized scores for patternmatches regardless of the number of snippetsretrieved for the given pair.
This normalization isimportant in order to bring to equal grounds can-didate pairs identified through either the pattern-based or distributional approaches, since the lat-ter tend to occur less frequently in patterns.Aggregated Conditional Pattern Probability:This single feature is the conditional probabilitythat any of the patterns match in a retrieved sen-tence, given that the two terms appear in it.
It iscalculated like the previous feature, with countsaggregated over all patterns, and aims to captureoverall appearance of the pair in patterns, regard-less of the specific pattern.Conditional List-Pattern Probability: This fea-ture was designed to eliminate the typical non-entailing cases of co-hyponyms (words sharingthe same hypernym), which nevertheless tend toco-occur in entailment patterns.
We thereforealso check for pairs' occurrences in lists, usingappropriate list patterns, expecting that correctentailment pairs would not co-occur in lists.
Theprobability estimate, calculated like the previousone, is expected to be a negative feature for thelearning model.Relation Direction Ratio: The value of this fea-ture is the ratio between the overall number ofpattern matches for the pair and the number ofpattern matches for the reversed pair (a pair cre-ated with the same terms in the opposite entail-ment direction).
We found that this featurestrongly correlates with entailment likelihood.Interestingly, it does not deteriorate performancefor synonymous pairs.Distributional Similarity Score: The GD04 simi-larity score of the pair was used as a feature.
We583also attempted adding Lin's (1998) similarityscores but they appeared to be redundant.Intersection Feature: A binary feature indicatingcandidate pairs acquired by both methods, whichwas found to indicate higher entailment likeli-hood.In summary, the above feature types utilizemutually complementary pattern-based and dis-tributional information.
Using cross validationover the training set we verified that each featuremakes marginal contribution to performancewhen added on top of the remaining features.3.4 Training and ClassificationIn order to systematically integrate different fea-ture types we used the state-of-the-art supervisedclassifier SVMlight (Joachims, 1999) for entail-ment pair classification.
Using 10-fold cross-validation over the training set we obtained theSVM configuration that yields an optimal micro-averaged F1 score.
Through this optimization wechose the RBF kernel function and obtained op-timal values for the J, C and the RBF's Gammaparameters.
The candidate test pairs classified ascorrect entailments constitute the output of ourintegrated method.4 Empirical Results4.1 Data Set and AnnotationWe utilized the experimental data set from Geffetand Dagan (2004).
The dataset includes the simi-larity lists calculated by GD04 for a sample of 30target (common) nouns, computed from an 18million word subset of the Reuters corpus5.
Werandomly picked a small set of 10 terms for train-ing, leaving the remaining 20 terms for testing.Then, the set of entailment pair candidates for allnouns was created by applying the filteringmethod of Section 3.2.2 to the distributionalsimilarity lists, and by extracting pattern-based5Reuters Corpus, Volume 1, English Language, 1996-08-20 to 1997-08-19.candidates from the web as described in Section3.2.1.Gold standard annotations for entailment pairswere created by three judges.
The judges wereguided to annotate as ?Correct?
the pairs con-forming to the lexical entailment definition,which was reflected in two operational tests: i)Word meaning entailment: whether the meaningof the first (entailing) term implies the meaningof the second (entailed) term under some com-mon sense of the two terms; and ii) Substitutabil-ity: whether the first term can substitute thesecond term in some natural contexts, such thatthe meaning of the modified context entails themeaning of the original one.
The obtained Kappavalues (varying between 0.7 and 0.8) correspondto substantial agreement on the task.4.2 ResultsThe numbers of candidate entailment pairs col-lected for the test terms are shown in Table 2.These figures highlight the markedly comple-mentary yield of the two acquisition approaches,where only about 10% of all candidates wereidentified by both methods.
On average, 120candidate entailment pairs were acquired foreach target term.The SVM classifier was trained on a quitesmall annotated sample of 700 candidate entail-ment pairs of the 10 training terms.
Table 3 pre-sents comparative results for the classifier, foreach of the two sets of candidates produced byeach method alone, and for the union of thesetwo sets (referred as Na?ve Combination).
Theresults were computed for an annotated randomsample of about 400 candidate entailment pairsof the test terms.
Following common poolingevaluations in Information Retrieval, recall iscalculated relatively to the total number of cor-rect entailment pairs acquired by both methodstogether.METHOD P R FPattern-based  0.44 0.61 0.51DistributionalSimilarity 0.33 0.53 0.40Na?ve Combina-tion 0.36 1.00 0.53Integrated  0.57 0.69 0.62Table 3: Precision, Recall and F1 figures for thetest words under each method.PATTERN-BASEDDISTRIBU-TIONAL TOTAL1186 1420 2350Table 2: The numbers of distinct entailment paircandidates obtained for the test words by each ofthe methods, and when combined.584The first two rows of the table show quitemoderate precision and recall for the candidatesof each separate method.
The next row shows thegreat impact of method combination on recall,relative to the amount of correct entailment pairsfound by each method alone, validating the com-plementary yield of the approaches.
The inte-grated classifier, applied to the combined set ofcandidates, succeeds to increase precision sub-stantially by 21 points (a relative increase of al-most 60%), which is especially important formany precision-oriented applications like Infor-mation Retrieval and Question Answering.
Theprecision increase comes with the expense ofsome recall, yet having F1 improved by 9 points.The integrated method yielded on average about30 correct entailments per target term.
Its classi-fication accuracy (percent of correct classifica-tions) reached 70%, which nearly doubles thena?ve combination's accuracy.It is impossible to directly compare our resultswith those of other works on lexical semanticrelationships acquisition, since the particular taskdefinition and dataset are different.
As a roughreference point, our result figures do match thoseof related papers reviewed in Section 2, while wenotice that our setting is relatively more difficultsince we excluded the easier cases of propernouns.
(Geffet and Dagan, 2005), who exploitedthe distributional similarity approach over theweb to address the same task as ours, obtainedhigher precision but substantially lower recall,considering only distributional candidates.
Fur-ther research is suggested to investigate integrat-ing their approach with ours.4.3 Analysis and DiscussionAnalysis of the data confirmed that the twomethods tend to discover different types of rela-tions.
As expected, the distributional similaritymethod contributed most (75%) of the synonymsthat were correctly classified as mutually entail-ing pairs (e.g.
assault ?
abuse in Table 4).
Onthe other hand, about 80% of all correctly identi-fied hyponymy relations were produced by thepattern-based method (e.g.
abduction ?
abuse).The integrated method provides a means to de-termine the entailment direction for distributionalsimilarity candidates which by construction arenon-directional.
Thus, amongst the (non-synonymous) distributional similarity pairs clas-sified as entailing, the direction of 73% was cor-rectly identified.
In addition, the integratedmethod successfully filters 65% of the non-entailing co-hyponym candidates (hyponyms ofthe same hypernym), most of them originated inthe distributional candidates, which is a largeportion (23%) of all correctly discarded pairs.Consequently, the precision of distributionalsimilarity candidates approved by the integratedsystem was nearly doubled, indicating the addi-tional information that patterns provide aboutdistributionally similar pairs.Yet, several error cases were detected andcategorized.
First, many non-entailing pairs arecontext-dependent, such as a gap which mightconstitute a hazard in some particular contexts,even though these words do not entail each otherin their general meanings.
Such cases are moretypical for the pattern-based approach, which issometimes permissive with respect to the rela-tionship captured and may also extract candi-dates from a relatively small number of patternoccurrences.
Second, synonyms tend to appearless frequently in patterns.
Consequently, somesynonymous pairs discovered by distributionalsimilarity were rejected due to insufficient pat-tern matches.
Anecdotally, some typos and spell-ing alternatives, like privatization ?privatisation, are also included in this categoryas they never co-occur in patterns.In addition, a large portion of errors is causedby pattern ambiguity.
For example, the pattern"NP1, a|an NP2", ranked among the top IS-A pat-terns by (Pantel et al, 2004), can represent bothapposition (entailing) and a list of co-hyponyms(non-entailing).
Finally, some misclassificationscan be attributed to technical web-based process-ing errors and to corpus data sparseness.Pattern-based Distributionalabduction ?
abuse assault ?
abusegovernment ?organizationgovernment ?administrationdrug therapy ?treatment budget deficit ?gapgap ?
hazard* broker ?
analyst*management ?
issue* government ?
parliament*Table 4: Typical entailment pairs acquired by theintegrated method, illustrating Section 4.3.
Thecolumns specify the method that produced thecandidate pair.
Asterisk indicates a non-entailingpair.5855 ConclusionThe main contribution of this paper is a novelintegration of the pattern-based and distributionalapproaches for lexical semantic acquisition, ap-plied to lexical entailment.
Our investigationhighlights the complementary nature of the twoapproaches and the information they provide.Notably, it is possible to extract pattern-basedinformation that complements the weaker evi-dence of distributional similarity.
Supervisedlearning was found effective for integrating thedifferent information types, yielding noticeablyimproved performance.
Indeed, our analysis re-veals that the integrated approach helps eliminat-ing many error cases typical to each methodalone.
We suggest that this line of research maybe investigated further to enrich and optimize thelearning processes and to address additional lexi-cal relationships.AcknowledgementWe wish to thank Google for providing us withan extended quota for search queries, whichmade this research feasible.ReferencesBerland, Matthew and Charniak, Eugene.
1999.
Find-ing parts in very large corpora.
In Proc.
of ACL-99.Maryland, USA.Chklovski, Timothy and Patrick Pantel.
2004.
VerbO-cean: Mining the Web for Fine-Grained SemanticVerb Relations.
In Proc.
of EMNLP-04.
Barcelona,Spain.Dagan, Ido, Oren Glickman and Bernardo Magnini.2005.
The PASCAL Recognizing Textual Entail-ment Challenge.
In Proc.
of the PASCAL Chal-lenges Workshop for Recognizing TextualEntailment.
Southampton, U.K.Etzioni, Oren, M. Cafarella, D. Downey, S. Kok, A.-M. Popescu, T. Shaked, S. Soderland, D.S.
Weld,and A. Yates.
2004.
Web-scale information extrac-tion in KnowItAll.
In Proc.
of WWW-04.
NY,USA.Geffet, Maayan and Ido Dagan.
2004.
Feature VectorQuality and Distributional Similarity.
In Proc.
ofCOLING-04.
Geneva, Switzerland.Geffet, Maayan and Ido Dagan.
2005.
The Distribu-tional Inclusion Hypothesis and Lexical Entail-ment.
In Proc of ACL-05.
Michigan, USA.Harris, Zelig S. 1968.
Mathematical Structures ofLanguage.
Wiley.Hearst, Marti.
1992.
Automatic Acquisition of Hypo-nyms from Large Text Corpora.
In Proc.
ofCOLING-92.
Nantes, France.Joachims, Thorsten.
1999.
Making large-Scale SVMLearning Practical.
Advances in Kernel Methods -Support Vector Learning, B. Sch?lkopf and C.Burges and A. Smola (ed.
), MIT-Press.Lin, Dekang.
1998.
Automatic Retrieval and Cluster-ing of Similar Words.
In Proc.
of COLING?ACL98, Montreal, Canada.Lin, Dekang, Shaojun Zhao, Lijuan Qin, and MingZhou.
2003.
Identifying synonyms among distribu-tionally similar words.
In Proc.
of  IJCAI-03.
Aca-pulco, Mexico.Pantel, Patrick, Deepak Ravichandran, and EduardHovy.
2004.
Towards Terascale Semantic Acquisi-tion.
In Proc.
of COLING-04.
Geneva, Switzer-land.Pantel, Patrick and Deepak Ravichandran.
2004.Automatically Labeling Semantic Classes.
In Proc.of HLT/NAACL-04.
Boston, MA.Ravichandran, Deepak and Eduard Hovy.
2002.Learning Surface Text Patterns for a Question An-swering System.
In Proc.
of ACL-02.
Philadelphia,PA.Riloff, Ellen and Jessica Shepherd.
1997.
A corpus-based approach for building semantic lexicons.
InProc.
of EMNLP-97.
RI, USA.Riloff, Ellen and Rosie Jones.
1999.
Learning Dic-tionaries for Information Extraction by Multi-LevelBootstrapping.
In Proc.
of AAAI-99.
Florida, USA.Shinzato, Kenji and Kentaro Torisawa.
2004.
Acquir-ing Hyponymy Relations from Web Documents.
InProc.
of HLT/NAACL-04.
Boston, MA.Sundblad, H. Automatic Acquisition of Hyponymsand Meronyms from Question Corpora.
2002.
InProc.
of the ECAI-02 Workshop on Natural Lan-guage Processing and Machine Learning for On-tology Engineering.
Lyon, France.Weeds, Julie, David Weir, and Diana McCarthy.2004.
Characterizing Measures of Lexical Distribu-tional Similarity.
In Proc.
of COLING-04.
Geneva,Switzerland.Yangarber, Roman, Ralph Grishman, Pasi Tapanainenand Silja Huttunen.
2000.
Automatic Acquisitionof Domain Knowledge for Information Extraction.In Proc.
of COLING-00.
Saarbr?cken, Germany.586
