Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1258?1267,Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational LinguisticsHypothesis Mixture Decoding for Statistical Machine TranslationNan Duan, Mu Li, and Ming ZhouSchool of Computer Science and Technology Natural Language Computing GroupTianjin University Microsoft Research AsiaTianjin, China Beijing, Chinav-naduan@microsoft.com {muli,mingzhou}@microsoft.comAbstractThis paper presents hypothesis mixture decoding(HM decoding), a new decoding scheme thatperforms translation reconstruction using hypo-theses generated by multiple translation systems.HM decoding involves two decoding stages:first, each component system decodes indepen-dently, with the explored search space kept foruse in the next step; second, a new search spaceis constructed by composing existing hypothesesproduced by all component systems using a setof rules provided by the HM decoder itself, anda new set of model independent features areused to seek the final best translation from thisnew search space.
Few assumptions are made byour approach about the underlying componentsystems, enabling us to leverage SMT modelsbased on arbitrary paradigms.
We compare ourapproach with several related techniques, anddemonstrate significant BLEU improvements inlarge-scale Chinese-to-English translation tasks.1 IntroductionBesides tremendous efforts on constructing morecomplicated and accurate models for statisticalmachine translation (SMT) (Och and Ney, 2004;Chiang, 2005; Galley et al, 2006; Shen et al, 2008;Chiang 2010), many researchers have concentratedon the approaches that improve translation qualityusing information between hypotheses from one ormore SMT systems as well.System combination is built on top of the N-bestoutputs generated by multiple component systems(Rosti et al, 2007; He et al, 2008; Li et al, 2009b)which aligns multiple hypotheses to build confu-sion networks as new search spaces, and outputsthe highest scoring paths as the final translations.Consensus decoding, on the other hand, can bebased on either single or multiple systems: singlesystem based methods (Kumar and Byrne, 2004;Tromble et al, 2008; DeNero et al, 2009; Kumaret al, 2009) re-rank translations produced by asingle SMT model using either n-gram posteriorsor expected n-gram counts.
Because hypothesesgenerated by a single model are highly correlated,improvements obtained are usually small; recently,dedicated efforts have been made to extend it fromsingle system to multiple systems (Li et al, 2009a;DeNero et al, 2010; Duan et al, 2010).
Such me-thods select translations by optimizing consensusmodels over the combined hypotheses using allcomponent systems?
posterior distributions.Although these two types of approaches haveshown consistent improvements over the standardMaximum a Posteriori (MAP) decoding scheme,most of them are implemented as post-processingprocedures over translations generated by MAPdecoders.
In this sense, the work of Li et al (2009a)is different in that both partial and full hypothesesare re-ranked during the decoding phase directlyusing consensus between translations from differ-ent SMT systems.
However, their method does notchange component systems?
search spaces.This paper presents hypothesis mixture decoding(HM decoding), a new decoding scheme that per-forms translation reconstruction using hypothesesgenerated by multiple component systems.
HMdecoding involves two decoding stages: first, eachcomponent system decodes the source sentenceindependently, with the explored search space keptfor use in the next step; second, a new searchspace is constructed by composing existing hypo-1258theses produced by all component systems using aset of rules provided by the HM decoder itself, anda new set of component model independent fea-tures are used to seek the final best translationfrom this new constructed search space.We evaluate by combining two SMT modelswith state-of-the-art performances on the NISTChinese-to-English translation tasks.
Experimentalresults show that our approach outperforms thebest component SMT system by up to 2.11 BLEUpoints.
Consistent improvements can be observedover several related decoding techniques as well,including word-level system combination, colla-borative decoding and model combination.2 Hypothesis Mixture Decoding2.1 Motivation and OverviewSMT models based on different paradigms haveemerged in the last decade using fairly differentlevels of linguistic knowledge.
Motivated by thesuccess of system combination research, the keycontribution of this work is to make more effectiveuse of the extended search spaces from differentSMT models in decoding phase directly, ratherthan just post-processing their final outputs.
Wefirst begin with a brief review of single systembased SMT decoding, and then illustrate majorchallenges to this end.Given a source sentence  , an SMT decoderseeks for a target translation   that best matchesas its translation by maximizing the followingconditional probability:where      is the feature vector that includes a setof system specific features,   is the weight vector,is a derivation that can yield   and is definedas a sequence of translation rule applications    .Figure 1 illustrates a decoding example, in whichthe final translation is generated by recursivelycomposing partial hypotheses that cover differentranges of the source sentence until the whole inputsentence is fully covered, and the feature vector ofthe final translation is the aggregation of featurevectors of all partial hypotheses used.1However, hypotheses generated by differentSMT systems cannot be combined directly to formnew translations because of two major issues:The first one is the heterogeneous structures ofdifferent SMT models.
For example, a string-to-tree system cannot use hypotheses generated by aphrase-based system in decoding procedure, assuch hypotheses are based on flat structures, whichcannot provide any additional information neededin the syntactic model.The second one is the incompatible featurespaces of different SMT models.
For example,even if a phrase-based system can use the lexicalforms of hypotheses generated by a syntax-basedsystem without considering syntactic structures,the feature vectors of these hypotheses still cannotbe aggregated together in any trivial way, becausethe feature sets of SMT models based on differentparadigms are usually inconsistent.To address these two issues discussed above, wepropose HM decoding that performs translationreconstruction using hypotheses generated by mul-tiple component systems.
2  Our method involvestwo decoding stages depicted as follows:1.
Independent decoding stage, in which eachcomponent system decodes input sentencesindependently based on its own model andsearch algorithm, and the explored searchspaces (translation forests) are kept for use inthe next stage.1 There are also features independent of translation deriva-tions, such as the language model feature.2 In this paper, we will constrain our discussions within CKY-style decoders, in which we find translations for all spans ofthe source sentence.
Although standard implementations ofphrase-based decoders fall out of this scope, they can be stillre-written to work in the CKY-style bottom-up manner at thecost of 1) only BTG-style reordering allowed, and 2) highertime complexity.
As a result, any phrase-based SMT systemcan be used as a component in our HM decoding method.China ?s economic growth[-2.48, 4]China[-0.36, 1]???
??
??
?s[-0.69, 1]economic[-0.51, 1]growth[-0.92, 1]China ?s[-1.05, 2]economic growth[-1.43, 2]Figure 1: A decoding example of a phrase-basedSMT system.
Each hypothesis is annotated with afeature vector, which includes a logarithmic probabil-ity feature and a word count feature.12592.
HM decoding stage, where a mixture searchspace is constructed for translation derivationsby composing partial hypotheses generated byall component systems, and a new decodingmodel with a set of enriched feature functionsare used to seek final translations from thisnewly generated search space.HM decoding can use lexicalized hypotheses ofarbitrary SMT models to derive translation, and aset of component model independent features areused to compute translation confidence.
We dis-cuss mixture search space construction, details ofmodel and feature designs as well as HM decodingalgorithms in Section 2.2, 2.3 and 2.4 respectively.2.2 Mixture Search Space ConstructionLet        denote  component MT systems,denote the span of a source sentence   startingat position   and ending at position  .
We usedenoting the search space ofpredictedby  , anddenoting the mixture searchspace ofconstructed by the HM decoder, whichis defined recursively as follows:?.
This rule adds all compo-nent systems?
search spaces into the mixturesearch space for use in HM decoding.
Thushypotheses produced by all component sys-tems are still available to the HM decoder.
?, in whichand.
is a translationrule provided by HM decoder that composes anew hypothesis using smaller hypotheses inthe search spaces.
Theserules further extendwith hypothesesgenerated by the HM decoder itself.Figure 2 shows an example of HM decoding, inwhich hypotheses generated by two SMT systemsare used together to compose new translations.Since search space pruning is the indispensableprocedure for all SMT systems, we will omit itsexplicit expression in the following descriptionsand algorithms for convenience.2.3 Models and FeaturesFollowing the common practice in SMT research,we use a linear model to formulate the preferenceof translation hypotheses in the mixture searchspace    .
Formally, we are to find a translationthat maximizes the weighted linear combinationof a set of real-valued features as follows:where         is an HM decoding feature with itscorresponding feature weight   .In this paper, the HM decoder does not assumethe availability of any internal knowledge of theunderlying component systems.
The HM decodingfeatures are independent of component models aswell, which fall into two categories:The first category contains a set of consensus-based features, which are inspired by the successof consensus decoding approaches.
These featuresare described in details as follows:1)            : the n-gram posterior feature ofcomputed based on the component searchspace      generated by  :isthe posterior probability of an n-gram   in,       is the number of times thatoccurs in  ,       equals to 1 when   occursin  , and 0 otherwise.Figure 2: An example of HM decoding, in which thetranslations surrounded by the dotted lines are newlygenerated hypotheses.
Hypotheses light-shaded comefrom a phrase-based system, and hypotheses dark-shaded come from a syntax-based system.economic growth of Chinaeconomic growth China ?s?
??
??
?
?development of economyChina ?s development of economyChina ?s economic growthof Chinadevelopment of economy of China?
Rules provided bythe HM decoder12602): the stemmed n-gram posteriorfeature of   computed based on the stemmedcomponent search space.
A word stemdictionary that includes 22,660 entries is usedto convert   and      into their stem formsandby replacing each word into itsstem form.
This feature is computed similarlyto that of            .3)           : the n-gram posterior feature ofcomputed based on the mixture search spacegenerated by the HM decoder:is theposterior probability of an n-gram   in    ,is the posterior probability of onetranslation    given   based on    .4)        : the length posterior feature of thespecific target hypothesis with length   basedon the mixture search space     generatedby the HM decoder:Note here that features in            andwill be computed when the computations of all theremainder features in two categories have alreadyfinished for each   in    , and they will be usedto update current HM decoding model scores.Consensus features based on component searchspaces have already shown effectiveness (Kumaret al, 2009; DeNero et al, 2010; Duan et al,2010).
We leverage consensus features based onthe mixture search space newly generated in HMdecoding as well.
The length posterior feature (Zenand Ney, 2006) is used to adjust the preference ofHM decoder for longer or shorter translations, andthe stemmed n-gram posterior features are used toprovide more discriminative power for HM decod-ing and to decrease the effects of morphologicalchanges in words for more accurate computationof consensus statistics.The second feature category contains a set ofgeneral features.
Although there are more featuresthat can be incorporated into HM decoding besidesthe ones we list below, we only utilize the mostrepresentative ones for convenience:1)             : the word count feature.2)         : the language model feature.3)           : the dictionary-based feature thatcounts how many lexicon pairs can be foundin a given translation pair      .4)           and          : reordering featuresthat penalize the uses of straight and invertedBTG rules during the derivation of   in HMdecoding.
These two features are specific toBTG-based HM decoding (Section 2.4.1):5)            and           : reordering fea-tures that penalize the uses of hierarchical andglue rules during the derivation of   in HMdecoding.
These two features are specific toSCFG-based HM decoding (Section 2.4.2):is the hierarchical rule set provided by theHM decoder itself,       equals to 1 whenis provided by  , and 0 otherwise.6)          : the feature that counts how manyn-grams in   are newly generated by the HMdecoder, which cannot be found in all existingcomponent search spaces:equals to 1 when   doesnot exist in, and 0 otherwise.The MERT algorithm (Och, 2003) is used totune weights of HM decoding features.2.4 Decoding AlgorithmsTwo CKY-style algorithms for HM decoding arepresented in this subsection.
The first one is basedon BTG (Wu, 1997), and the second one is basedon SCFG, similar to Chiang (2005).12612.4.1 BTG-based HM DecodingThe first algorithm, BTG-HMD, is presented inAlgorithm 1, where hypotheses of two consecutivesource spans are composed using two BTG rules:?
Straight rule    .
It combines translations oftwo consecutive blocks into a single largerblock in a straight order.?
Inverted rule   .
It combines translations oftwo consecutive blocks into a single largerblock in an inverted order.These two rules are used bottom-up until thewhole source sentence is fully covered.
We usetwo reordering rule penalty features,           and, to penalize the uses of these two rules.Algorithm 1: BTG-based HM Decoding1: for each component model   do2:  output the search space      for the input3: end for4: for     to       do5:  for all     s.t.
do6:7:   for all   s.t.
do8:    foranddo9:     add                  to10:     add                 to11:    end for12:   end for13:   for each hypothesisdo14:    compute HM decoding features for15:    add   to16:   end for17:   for each hypothesisdo18:compute the n-gram and length posteriorfeatures for   based on19:    update current HM decoding score of20:   end for21:  end for22: end for23: return         with the maximum model scoreIn BTG-HMD, in order to derive translations fora source span, we compose hypotheses of anytwo smaller spansandusing two BTGrules in line 9 and 10,              denotes theoperations that firstly combine    and    using oneBTG rule   and secondly compute HM decodingfeatures for the newly generated hypothesis  .
Wecompute HM decoding features for hypothesescontained in all existing component search spacesas well, and add them to.From line 17 to 20, we update current HM decod-ing scores for all hypotheses inusing then-gram and length posterior features computedbased on.
When the whole source sentenceis fully covered, we return the hypothesis with themaximum model score as the final best translation.2.4.2 SCFG-based HM DecodingThe second algorithm, SCFG-HMD, is presentedin Algorithm 2.
An additional rule set , which isprovided by the HM decoder, is used to composehypotheses.
It includes hierarchical rules extractedusing Chiang (2005)?s method and glue rules.
Tworeordering rule penalty features,            and, are used to adjust the preferences ofusing hierarchical rules and glue rules.Algorithm 2: SCFG-based HM Decoding1: for each component model   do2:  output the search space      for the input3: end for4: for     to       do5:  for all     s.t.
do6:7:   for each rule     that matchesdo8:    for           and           do9:     add                to10:    end for11:   end for12:   for each hypothesisdo13:    compute HM decoding features for14:    add   to15:   end for16:   for each hypothesisdo17:compute the n-gram and length posteriorfeatures for   based on18:    update current HM decoding score of19:   end for20:  end for21: end for22: return         with the maximum model scoreCompared to BTG-HMD, the key differences inSCFG-HMD are located from line 7 to 11, wherethe translation for a given spanis generated byreplacing the non-terminals in a hierarchical rulewith their corresponding target translations,is the source span that is covered by theth non-terminal of  ,        is the search space forpredicted by the HM decoder.12623 Comparisons to Related Techniques3.1 Model Combination and Mixture Modelbased MBR DecodingModel combination (DeNero et al, 2010) is anapproach that selects translations from a conjointsearch space using information from multiple SMTcomponent models; Duan et al (2010) presents asimilar method, which utilizes a mixture model tocombine distributions of hypotheses from differentsystems for Bayes-risk computation, and selectsfinal translations from the combined search spacesusing MBR decoding.
Both of these two methodsshare a common limitation: they only re-rank thecombined search space, without the capability togenerate new translations.
In contrast, by reusinghypotheses generated by all component systems inHM decoding, translations beyond any existingsearch space can be generated.3.2 Co-Decoding and Joint DecodingLi et al (2009a) proposes collaborative decoding,an approach that combines translation systems byre-ranking partial and full translations iterativelyusing n-gram features from the predictions of othermember systems.
However, in co-decoding, allmember systems must work in a synchronous way,and hypotheses between different systems cannotbe shared during decoding procedure; Liu et al(2009) proposes joint-decoding, in which multipleSMT models are combined in either translation orderivation levels.
However, their method relies onthe correspondence between nodes in hypergraphoutputs of different models.
HM decoding, on theother hand, can use hypotheses from componentsearch spaces directly without any restriction.3.3 Hybrid DecodingHybrid decoding (Cui et al, 2010) resembles ourapproach in the motivation.
This method uses thesystem combination technique in decoding directlyto combine partial hypotheses from different SMTmodels.
However, confusion network constructionbrings high computational complexity.
What?smore, partial hypotheses generated by confusionnetwork decoding cannot be assigned exact featurevalues for future use in higher level decoding, andthey only use feature values of 1-best hypothesisas an approximation.
HM decoding, on the otherhand, leverages a set of enriched features, whichare computable for all the hypotheses generated byeither component systems or the HM decoder.4 Experiments4.1 Data and MetricExperiments are conducted on the NIST Chinese-to-English MT tasks.
The NIST 2004 (MT04) dataset is used as the development set, and evaluationresults are reported on the NIST 2005 (MT05), thenewswire portions of the NIST 2006 (MT06) and2008 (MT08) data sets.
All bilingual corporaavailable for the NIST 2008 constrained data trackof Chinese-to-English MT task are used as trainingdata, which contain 5.1M sentence pairs, 128MChinese words and 147M English words after pre-processing.
Word alignments are performed usingGIZA++ with the intersect-diag-grow refinement.The English side of bilingual corpus plus Xinhuaportion of the LDC English Gigaword Version 3.0are used to train a 5-gram language model.Translation performance is measured in terms ofcase-insensitive BLEU scores (Papineni et al,2002), which compute the brevity penalty usingthe shortest reference translation for each segment.Statistical significance is computed using the boot-strap re-sampling approach proposed by Koehn(2004).
Table 1 gives some data statistics.Data Set #Sentence #WordMT04(dev) 1,788 48,215MT05 1,082 29,263MT06 616 17,316MT08 691 17,424Table 1: Statistics on dev and test data sets4.2 Component SystemsFor convenience of comparing HM decoding withseveral related decoding techniques, we includetwo state-of-the-art SMT systems as componentsystems only:?
PB.
A phrase-based system (Xiong et al,2006) with one lexicalized reordering modelbased on the maximum entropy principle.?
DHPB.
A string-to-dependency tree-basedsystem (Shen et al, 2008), which translatessource strings to target dependency trees.
Atarget dependency language model is used asan additional feature.1263Phrasal rules are extracted on all bilingual data,hierarchical rules used in DHPB and reorderingrules used in SCFG-HMD are extracted from aselected data set3.
Reordering model used in PB istrained on the same selected data set as well.
Atrigram dependency language model used inDHPB is trained with the outputs from Berkeleyparser on all language model training data.4.3 Contrastive TechniquesWe compare HM decoding with three multiple-system based decoding techniques:?
Word-Level System Combination (SC).
Were-implement an IHMM alignment based sys-tem combination method proposed by Li et al(2009b).
The setting of the N-best candidatesused is the same as the original paper.?
Co-decoding (CD).
We re-implement it basedon Li et al (2009a), with the only differencethat only two models are included in our re-implementation, instead of three in theirs.
Foreach test set, co-decoding outputs three results,two for two member systems, and one for thefurther system combination.?
Model Combination (MC).
Different from co-decoding, MC produces single one output foreach input sentence.
We re-implement thismethod based on DeNero et al (2010) withtwo component models included.4.4 Comparison to Component SystemsWe compared HM decoding with two componentSMT systems first (in Table 2).
30 features areused to annotate each hypothesis in HM decoding,including: 8 n-gram posterior features computedfrom PB/DHPB forests for      ; 8 stemmedn-gram posterior features computed from stemmedPB/DHPB forests for      ; 4 n-gram post-erior features and 1 length posterior feature com-puted from the mixture search space of HM de-coder for      ; 1 LM feature; 1 word countfeature; 1 dictionary-based feature; 2 grammar-specified rule penalty features for either BTG-HMD or SCFG-HMD; 4 count features for newlygenerated n-grams in HM decoding for      .All n-gram posteriors are computed using the effi-cient algorithm proposed by Kumar et al (2009).3 LDC2003E07, LDC2003E14, LDC2005T06, LDC2005T10,LDC2005E83, LDC2006E26, LDC2006E34, LDC2006E85and LDC2006E92ModelBLEU%MT04 MT05 MT06 MT08PB 38.93 38.21 33.59 29.62DHPB 39.90 39.76 35.00 30.43BTG-HMD 41.24* 41.26* 36.76* 31.69*SCFG-HMD 41.31* 41.19* 36.63* 31.52*Table 2: HM decoding vs. single component systemdecoding (*: significantly better than each componentsystem with   < 0.01)From table 2 we can see, both BTG-HMD andSCFG-HMD outperform decoding results of thebest component system (DHPB) with significantimprovements: +1.50, +1.76, and +1.26 BLEUpoints on MT05, MT06, and MT08 for BTG-HMD;+1.43, +1.63 and +1.09 BLEU points on MT05,MT06, and MT08 for SCFG-HMD.
We also noticethat BTG-HMD performs slight better than SCFG-HMD on test sets.
We think the potential reason isthat more reordering rules are used in SCFG-HMDto handle phrase movements than BTG-HMD do;however, current HM decoding model lacks theability to distinguish the qualities of different rules.We also investigate on the effects of differentHM-decoding features.
For the convenience ofcomparison, we divide them into five categories:?
Set-1.
8 n-gram posterior features based on 2component search spaces plus 3 commonlyused features (1 LM feature, 1 word countfeature and 1 dictionary-based feature).?
Set-2.
8 stemmed n-gram posterior featuresbased on 2 stemmed component search spaces.?
Set-3.
4 n-gram posterior features and 1length posterior feature based on the mixturesearch space of the HM decoder.?
Set-4.
2 grammar-specified reordering rulepenalty features.?
Set-5.
4 count features for unseen n-gramsgenerated by HM decoder itself.Except for the dictionary-based feature, all thefeatures contained in Set-1 are used by the latestmultiple-system based consensus decoding tech-niques (DeNero et al, 2010; Duan et al, 2010).We use them as the starting point.
Each time, weadd one more feature set and describe the changesof performances by drawing two curves for eachHM decoding algorithm on MT08 in Figure 3.1264Figure 3: Effects of using different sets of HM decodingfeatures on MT08With Set-1 used only, HM-decoding has alreadyoutperformed the best component system, whichshows the strong contributions of these features asproved in related work; small gains (+0.2 BLEUpoints) are achieved by using 8 stemmed n-gramposterior features in Set-2, which shows consensusstatistics based on n-grams in their stem forms arealso helpful; n-gram and length posterior featuresbased on mixture search space bring improvementsas well; reordering rule penalty features and countfeatures for unseen n-grams boost newly generatedhypotheses specific for HM decoding, and theycontribute to the overall improvements.4.5 Comparison to System CombinationWord-level system combination is state-of-the-artmethod to improve translation performance usingoutputs generated by multiple SMT systems.
Inthis paper, we compare our HM decoding with thecombination method proposed by Li et al (2009b).Evaluation results are shown in Table 3.ModelBLEU%MT04 MT05 MT06 MT08SC 41.14 40.70 36.04 31.16BTG-HMD 41.24 41.26+ 36.76+ 31.69+SCFG-HMD 41.31+ 41.19+ 36.63+ 31.52+Table 3: HM decoding vs. system combination (+: sig-nificantly better than SC with   < 0.05)Compared to word-level system combination,both BTG-HMD and SCFG-HMD can providesignificant improvements.
We think the potentialreason for these improvements is that, systemcombination can only use a small portion of thecomponent systems?
search spaces; HM decoding,on the other hand, can make full use of the entiretranslation spaces of all component systems.4.6 Comparison to Consensus DecodingConsensus decoding is another decoding techniquethat motivates our approach.
We compare our HMdecoding with two latest multiple-system basedconsensus decoding approaches, co-decoding andmodel combination.
We list the comparison resultsin Table 4, in which CD-PB and CD-DHPB denotethe translation results of two member systems inco-decoding respectively, CD-Comb denotes theresults of further combination using outputs ofCD-PB and CD-DHPB, MC denotes the results ofmodel combination.ModelBLEU%MT04 MT05 MT06 MT08CD-PB 40.39 40.34 35.20 30.39CD-DHPB 40.81 40.56 35.73 30.87CD-Comb 41.27 41.02 36.37 31.54MC 41.19 40.96 36.30 31.43BTG-HMD 41.24 41.26+ 36.76+ 31.69SCFG-HMD 41.31 41.19 36.63+ 31.52Table 4: HM decoding vs. consensus decoding (+: sig-nificantly better than the best result of consensus decod-ing methods with   < 0.05)Table 4 shows that after an additional systemcombination procedure, CD-Comb performs slightbetter than MC.
Both BTG-HMD and SCFG-HMD perform consistent better than CD and MCon all blind test sets, due to its richer generativecapability and usage of larger search spaces.4.7 System Combination over BTG-HMDand SCFG-HMD OutputsAs BTG-HMD and SCFG-HMD are based on twodifferent decoding grammars, we could performsystem combination over the outputs of these twosettings (SCBTG+SCFG) for further improvements aswell, just as Li et al (2009a) did in co-decoding.We present evaluation results in Table 5.ModelBLEU%MT04 MT05 MT06 MT08BTG-HMD 41.24 41.26 36.76 31.69SCFG-HMD 41.31 41.19 36.63 31.52SCBTG+SCFG 41.74+ 41.53+ 37.11+ 32.06+Table 5: System combination based on the outputs ofBTG-HMD and SCFG-HMD (+: significantly betterthan the best HM decoding algorithm (SCFG-HMD)with   < 0.05)30.530.730.931.131.331.531.731.9Set-1 Set-2 Set-3 Set-4 Set-5BTG-HMDSCFG-HMD1265After system combination, translation results aresignificantly better than all decoding approachesinvestigated in this paper: up to 2.11 BLEU pointsover the best component system (DHPB), up to1.07 BLEU points over system combination, up to0.74 BLEU points over co-decoding, and up to0.81 BLEU points over model combination.4.8 Evaluation of Oracle TranslationsIn the last part, we evaluate the quality of oracletranslations on the n-best lists generated by HMdecoding and all decoding approaches discussed inthis paper.
Oracle performances are obtained usingthe metric of sentence-level BLEU score proposedby Ye et al (2007), and each decoding approachoutputs its 1000-best hypotheses, which are usedto extract oracle translations.ModelBLEU%MT04 MT05 MT06 MT08PB 49.53 48.36 43.69 39.39DHPB 50.66 49.59 44.68 40.47SC 51.77 50.84 46.87 42.11CD-PB 50.26 50.10 45.65 40.52CD-DHPB 51.91 50.61 46.23 41.01CD-Comb 52.10 51.00 46.95 42.20MC 52.03 51.22 46.60 42.23BTG-HMD 52.69+ 51.75+ 47.08 42.71+SCFG-HMD 52.94+ 51.40 47.27+ 42.45+SCBTG+SCFG 53.58+ 52.03+ 47.90+ 43.07+Table 6: Oracle performances of different methods (+:significantly better than the best multiple-system baseddecoding method (CD-Comb) with   < 0.05)Results are shown in Table 6: compared to eachsingle component system, decoding methods basedon multiple SMT systems can provide significantimprovements on oracle translations; word-levelsystem combination, collaborative decoding andmodel combination show similar performances, inwhich CD-Comb performs best; BTG-HMD,SCFG-HMD and SCBTG+SCFG can obtain significantimprovements than all the other approaches, andSCBTG+SCFG performs best on all evaluation sets.5 ConclusionIn this paper, we have presented the hypothesismixture decoding approach to combine multipleSMT models, in which hypotheses generated bymultiple component systems are used to composenew translations.
HM decoding method integratesthe advantages of both system combination andconsensus decoding techniques into a unifiedframework.
Experimental results across differentNIST Chinese-to-English MT evaluation data setshave validated the effectiveness of our approach.In the future, we will include more SMT modelsand explore more features, such as syntax-basedfeatures, helping to improve the performance ofHM decoding.
We also plan to investigate morecomplicated reordering models in HM decoding.ReferencesDavid Chiang.
2005.
A Hierarchical Phrase-basedModel for Statistical Machine Translation.
In Pro-ceedings of the Association for Computational Lin-guistics, pages 263-270.David Chiang.
2010.
Learning to Translate with Sourceand Target Syntax.
In Proceedings of the Associationfor Computational Linguistics, pages 1443-1452.Lei Cui, Dongdong Zhang, Mu Li, Ming Zhou, andTiejun Zhao.
2010.
Hybrid Decoding: Decoding withPartial Hypotheses Combination over Multiple SMTSystems.
In Proceedings of the International Confe-rence on Computational Linguistics, pages 214-222.John DeNero, David Chiang, and Kevin Knight.
2009.Fast Consensus Decoding over Translation Forests.In Proceedings of the Association for ComputationalLinguistics, pages 567-575.John DeNero, Shankar Kumar, Ciprian Chelba andFranz Och.
2010.
Model Combination for MachineTranslation.
In Proceedings of the North AmericanAssociation for Computational Linguistics, pages975-983.Nan Duan, Mu Li, Dongdong Zhang, and Ming Zhou.2010.
Mixture Model-based Minimum Bayes RiskDecoding using Multiple Machine Translation Sys-tems.
In Proceedings of the International Conferenceon Computational Linguistics, pages 313-321.Michel Galley, Jonathan Graehl, Kevin Knight, DanielMarcu, Steve DeNeefe, Wei Wang, and IgnacioThayer.
2006.
Scalable Inference and Training ofContext-Rich Syntactic Translation Models.
In Pro-ceedings of the Association for Computational Lin-guistics, pages 961-968.Xiaodong He, Mei Yang, Jianfeng Gao, PatrickNguyen, and Robert Moore.
2008.
Indirect-HMM-based Hypothesis Alignment for Combining Outputsfrom Machine Translation Systems.
In Proceedingsof the Conference on Empirical Methods on NaturalLanguage Processing, pages 98-107.1266Philipp Koehn.
2004.
Statistical Significance Tests forMachine Translation Evaluation.
In Proceedings ofthe Conference on Empirical Methods on NaturalLanguage Processing, pages 388-395.Shankar Kumar and William Byrne.
2004.
MinimumBayes-Risk Decoding for Statistical Machine Trans-lation.
In Proceedings of the North American Asso-ciation for Computational Linguistics, pages 169-176.Shankar Kumar, Wolfgang Macherey, Chris Dyer, andFranz Och.
2009.
Efficient Minimum Error RateTraining and Minimum Bayes-Risk Decoding forTranslation Hypergraphs and Lattices.
In Proceed-ings of the Association for Computational Linguis-tics, pages 163-171.Mu Li, Nan Duan, Dongdong Zhang, Chi-Ho Li, andMing Zhou.
2009a.
Collaborative Decoding: PartialHypothesis Re-Ranking Using Translation Consen-sus between Decoders.
In Proceedings of the Associ-ation for Computational Linguistics, pages 585-592.Chi-Ho Li, Xiaodong He, Yupeng Liu, and Ning Xi.2009b.
Incremental HMM Alignment for MT systemCombination.
In Proceedings of the Association forComputational Linguistics, pages 949-957.Yang Liu, Haitao Mi, Yang Feng, and Qun Liu.
2009.Joint Decoding with Multiple Translation Models.
InProceedings of the Association for ComputationalLinguistics, pages 576-584.Franz Och.
2003.
Minimum Error Rate Training in Sta-tistical Machine Translation.
In Proceedings of theAssociation for Computational Linguistics, pages160-167.Franz Och and Hermann Ney.
2004.
The AlignmentTemplate Approach to Statistical Machine Transla-tion.
Computational Linguistics, 30(4): 417-449.Kishore Papineni, Salim Roukos, Todd Ward, andWeijing Zhu.
2002.
BLEU: a method for automaticevaluation of machine translation.
In Proceedings ofthe Association for Computational Linguistics, pages311-318.Libin Shen, Jinxi Xu, and Ralph Weischedel.
2008.
Anew String-to-Dependency Machine Translation Al-gorithm with a Target Dependency Language Model.In Proceedings of the Association for ComputationalLinguistics, pages 577-585.Antti-Veikko Rosti, Spyros Matsoukas, and RichardSchwartz.
2007.
Improved Word-Level SystemCombination for Machine Translation.
In Proceed-ings of the Association for Computational Linguistics,pages 312-319.Roy Tromble, Shankar Kumar, Franz Och, and Wolf-gang Macherey.
2008.
Lattice Minimum Bayes-RiskDecoding for Statistical Machine Translation.
InProceedings of the Conference on Empirical Me-thods on Natural Language Processing, pages 620-629.Dekai Wu.
1997.
Stochastic Inversion TransductionGrammars and Bilingual Parsing of Parallel Corpora.Computational Linguistics, 23(3): 377-404.Deyi Xiong, Qun Liu, and Shouxun Lin.
2006.
Maxi-mum Entropy based Phrase Reordering Model forStatistical Machine Translation.
In Proceedings ofthe Association for Computational Linguistics, pages521-528.Yang Ye, Ming Zhou, and Chin-Yew Lin.
2007.
Sen-tence Level Machine Translation Evaluation as aRanking Problem: one step aside from BLEU.
InProceedings of the Second Workshop on StatisticalMachine Translation, pages 240-247.1267
