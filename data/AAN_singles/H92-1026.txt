Towards History-based Grammars:Using Richer Models for Probabil ist ic Parsing*Ezra Black Fred Jelinek John Lafferty David M. MagermanRobert Mercer Salim RoukosIBM T.  J .
Watson  Research  CenterABSTRACTWe describe a generative probabilistic model of natural an-guage, which we call HBG, that takes advantage of detailedlinguistic information to resolve ambiguity.
HBG incorpo-rates lexical, syntactic, semantic, and structural informationfrom the parse tree into the disambiguation process in a novelway.
We use a corpus of bracketed sentences, called a Tree-bank, in combination with decision tree building to tease outthe relevant aspects of a parse tree that will determine thecorrect parse of a sentence.
This stands in contrast o theusual approach of further grammar tailoring via the usuallinguistic introspection i the hope of generating the correctparse.
In head-to-head tests against one of the best existingrobust probabflistic parsing models, which we call P-CFG,the HBG model significantly outperforms P-CFG, increasingthe parsing accuracy rate from 60% to 75%, a 37% reductionin error.1.
In t roduct ionAlmost any natural language sentence is ambiguous instructure, reference, or nuance of meaning.
Humansovercome these apparent ambiguities by examining thecon~ezt of the sentence.
But what exactly is context?Frequently, the correct interpretation is apparent fromthe words or constituents immediately surrounding thephrase in question.
This observation begs the followingquestion: How much information about the context ofa sentence or phrase is necessary and sufficient to de-termine its meaning?
This question is at the crux ofthe debate among computational linguists about the ap-plication and implementation of statistical methods innatural anguage understanding.Previous work on disambiguation and probabilistic pars-ing has offered partial answers to this question.
HiddenMarkov models of words and their tags, introduced in \[1\]and \[11\] and popularized in the natural anguage commu-nity by Church \[5\], demonstrate the power of short-termn-gram statistics to deal with lexical ambiguity.
Hindleand Rooth \[8\] use a statistical measure of lexical asso-ciations to resolve structural ambiguities.
Brent \[2\] ac-quires likely verb subcategorization patterns using the*Thanks to Philip Resnik and Stanley Chen for their valuedinput.frequencies of verb-object-preposition triples.
Mager-man and Marcus \[10\] propose a model of context thatcombines the n-gram model with information from dom-inating constituents.
All of these aspects of context arenecessary for disambiguation, yet none is sufficient.We propose a probabilistic model of context for disam-biguation in parsing, HBG, which incorporates the intu-itions of these previous works into one unified framework.Let p(T, w~) be the joint probabil ity of generating theword string w~ and the parse tree T. Given w~, ourparser chooses as its parse tree that tree T* for whichT* = argmaxp(T ,  w~) (1)TeP(wNwhere "P(w~) is the set of all parses produced by thegrammar for the sentence w~.
Many aspects of the inputsentence that might be relevant o the decision-makingprocess participate in the probabilistic model, provid-ing a very rich if not the richest model of context everattempted in a probabilistic parsing model.In this paper, we will motivate and define the HBGmodel, describe the task domain, give an overview ofthe grammar, describe the proposed HBG model, andpresent he results of experiments comparing HBG withan existing state-of-the-art model.2.
Mot ivat ion  fo r  H is tory -basedGrammarsOne goal of a parser is to produce a grammatical  inter-pretation of a sentence which represents the syntacticand semantic intent of the sentence.
To achieve thisgoal, the parser must have a mechanism for estimatingthe coherence of an interpretation, both in isolation andin context.
Probabilistic language models provide sucha mechanism.A probabilistic language model attempts to estimate theprobability of a sequence of sentences and their respec-tive interpretations (parse trees) occurring in the lan-guage, "P(S1 T1 $2 T2 ... S~ T~).The difficulty in applying probabilistic models to natu-134ral language is deciding what aspects of the sentence andthe discourse are relevant o the model.
Most previousprobabilistic models of parsing assume the probabilitiesof sentences in a discourse are independent of other sen-tences.
In fact, previous works have made much strongerindependence assumptions.
The P-CFG model consid-ers the probability of each constituent rule independentof all other constituents in the sentence.
The Pearl \[10\]model includes a slightly richer model of context, allow-ing the probability of a constituent rule to depend uponthe immediate parent of the rule and a part-of-speech tri-gram from the input sentence.
But none of these modelscome close to incorporating enough context to disam-biguate many cases of ambiguity.A significant reason researchers have limited the contex-tual information used by their models is because of thedifficulty in estimating very rich probabilistic models ofcontext.
In this work, we present a model, the history-based grammar model, which incorporates a very richmodel of context, and we describe a technique for es-timating the parameters for this model using decisiontrees.
The history-based grammar model provides amechanism for taking advantage of contextual informa-tion from anywhere in the discourse history.
Using deci-sion tree technology, any question which can be asked ofthe history (i.e.
Is the subject of the previous entenceanimate?
Was the previous sentence a question?
etc.
)can be incorporated into the language model.3.
The History-based Grammar ModelThe history-based grammar model defines context of aparse tree in terms of the leftmost derivation of the tree.Following \[7\], we show in Figure 1 a context-free gram-mar (CFG) for anb ~ and the parse tree for the sentenceaabb.
The leftmost derivation of the tree T in Figure 1is:S ~ ASB --% aSB --h aABB --% aaBB -% aabB -A aabb(2)where the rule used to expand the i-th node of the treeis denoted by ri.
Note that we have indexed the non-terminal (NT) nodes of the tree with this leftmost order.We denote by t~ the sentential form obtained just beforewe expand node i.
Hence, t~ corresponds to the senten-tial form aSB or equivalently to the string rlr2.
In aleftmost derivation we produce the words in left-to-rightorder.Using the one-to-one correspondence b tween leftmostderivations and parse trees, we can rewrite the jointS ~ ASBIABA ~ aB ~ ba a b bFigure i: Grammar and parse tree for aabb.probability in (1) as:p(T, w~) = ~IP(r~\]tf-)i=1In a probabilistic ontext-free grammar (P-CFG), theprobability of an expansion at node i depends only onthe identity of the non-terminal N~, i.e., p(ri ITS-) -- p(r~).Thusp(T, w~) = l~P( r , )i----1So in P-CFG the derivation order does not affect theprobabilistic model 1.A tess crude approximation than the usual P-CFG is touse a decision tree to determine which aspects of the left-most derivation have a bearing on the probability of hownode i will be expanded.
In other words, the probabil-ity distribution p(rilt~" ) will be modeled by p(r~lE\[t~\])where E\[~\] is the equivalence class of the history t as de-termined by the decision tree.
This allows our probabilis-tic model to use any information anywhere in the partialderivation tree to determine the probability of differentexpansions of the i-th non-terminal.
The use of deci-sion trees and a large bracketed corpus may shift someof the burden of identifying the intended parse from thegrammarian to the statistical estimation methods.
Werefer to probabilistic methods based on the derivation asHistory-based Grammars (HBG).iNote the a'buse of notat ion since we denote by p(r~) the con-ditional probabil ity of rewriting the non-terminal Ni.135In this paper, we explored a restricted implementationof this model in which only the path from the currentnode to the root of the derivation along with the indexof a branch (index of the child of a parent ) are examinedin the decision tree model to build equivalence classes ofhistories.
Other parts of the subtree are not examinedin the implementation f HBG.4.
Task  DomainWe have chosen computer manuals as a task domain.We picked the most frequent 3000 words in a corpus of600,000 words from 10 manuals as our vocabulary.
Wethen extracted a few million words of sentences that arecompletely covered by this vocabulary from 40,000,000words of computer manuals.
A randomly chosen sen-tence from a sample of 5000 sentences from this corpusis:396.
It indicates whether a call completed suc-cessfully or if some error was detected thatcaused the call to fail.To define what we mean by a correct parse, we use acorpus of manually bracketed sentences at the Universityof Lancaster called the Treebank.
The Treebank uses 17non-terminal labels and 240 tags.
The bracketing of theabove sentence is shown in Figure 2.\[N It_PPH1 N\]\[V indicates_VVZ\[Fn \[Fn&whether_CSWIN a_AT1 call_NN1 N\]\[V completed_VVD successfully_RR V\]Fn~\]or_CC\[Fn+ iLCSWIN some_DD error_NN1 N\]@IV was_VBDZ detected_VVN V\]@\[Fr that_CST\[V caused_VVDIN the_AT call_NN1 N\]\[Ti to_TO fail_VVI Wi\]V\]Fr\]Fn+\]Fn\]V\]._.Figure 2: Sample bracketed sentence from LancasterTreebank.A parse produced by the grammar is judged to be correctif it agrees with the Treebank parse structurally and theNT labels agree.
The grammar has a significantly richerNT label set (more than 10000) than the Treebank butwe have defined an equivalence mapping between thegrammar NT labels and the Treebank NT labels.
In thispaper, we do not include the tags in the measure of acorrect parse.We have used about 25,000 sentences to help the gram-marian develop the grammar with the goal that the cor-rect (as defined above) parse is among the proposed (bythe grammar) parses for a sentence.
Our most commontest set consists of 1600 sentences that are never seen bythe grammarian.5.
The  GrammarThe grammar used in this experiment is a broad-coverage, feature-based unification grammar.
The gram-mar is context-free but uses unification to express ruletemplates for the the context-free productions.
For ex-ample, the rule template:(3): n unspec  : ncorresponds to three CFG productions where the secondfeature : n is either s, p, or : n. This rule templatemay elicit up to 7 non-terminals.
The grammar has 21features whose range of values maybe from 2 to about100 with a median of 8.
There are 672 rule templates ofwhich 400 are actually exercised when we parse a corpusof 15,000 sentences.
The number of productions thatare realized in this training corpus is several hundredthousand.5 .1 .
P -CFGWhile a NT in the above grammar is a feature vector, wegroup several NTs into one class we call a mnemonicrepresented by the one NT that is the least specified inthat class.
For example, the mnemonic VBOPASTSG*corresponds to all NTs that unify with:pos=v \]v - type = betense - aspect -- past(4)We use these mnemonics to label a parse tree and we alsouse them to estimate a P-CFG, where the probabil ityof rewriting a NT is given by the probability of rewrit-ing the mnemonic.
So from a training set we inducea CFG from the actual mnemonic productions that areelicited in parsing the training corpus.
Using the Inside-Outside algorithm, we can estimate P-CFG from a largecorpus of text.
But since we also have a large corpusof bracketed sentences, we can adapt the Inside-Outsidealgorithm to reestimate the probability parameters sub-ject to the constraint hat only parses consistent withthe Treebank (where consistency is as defined earlier)136contribute to the reestimation.
From a training run of15,000 sentences we observed 87,704 mnemonic produc-tions, with 23,341 NT mnemonics of which 10,302 werelexical.
Running on a test set of 760 sentences 32% ofthe rule templates were used, 7% of the lexical mnemon-ics, 10% of the constituent mnemonics, and 5% of themnemonic productions actually contributed to parses oftest sentences.5 .2 .
Grammar  and  Mode l  Per fo rmanceMet r i csTo evaluate the performance of a grammar and an ac-companying model, we use two types of measurements:?
the any-consistent rate, defined as the percentageof sentences for which the correct parse is proposedamong the many parses that the grammar providesfor a sentence.
We also measure the parse base,which is defined as the geometric mean of the num-ber of proposed parses on a per word basis, to quan-tify the ambiguity of the grammar.?
the Viterbi rate defined as the percentage of sen-tences for which the most likely parse is consistent.The arty-consistent ra e is a measure of the grammar'scoverage of linguistic phenomena.
The Viterbi rate eval-uates the grammar's  coverage with the statistical modelimposed on the grammar.
The goal of probabilisticmodelling is to produce a Viterbi rate close to the arty-consistent rate.The any-consistent rate is 90% when we require thestructure and the labels to agree and 96% when unla-beled bracketing is required.
These results are obtainedon 760 sentences from 7 t0 17 words long from test ma-terial that has never been seen by the grammarian.
Theparse base is 1.35 parses/word.
This translates to about23 parses for a 12-word sentence.
The unlabeled Viterbirate stands at 64% and the labeled Viterbi rate is 60%.While we believe that the above Vitevbi rate is close ifnot the state-of-the-art performance, there is room forimprovement by using a more refined statistical modelto achieve the labeled arty-cortsistertt ra e of 90% withthis grammar.
There is a significant gap between thelabeled Viterbi and arty-cortsistent ra es: 30 percentagepoints.Instead of the usual approach where a grammarian triesto fine tune the grammar in the hope of improving theViterbi rate we use the combination of a large Treebankand the resulting derivation histories with a decision treebuilding algorithm to extract statistical parameters thatwould improve the Viterbi rate.
The grammarian's taskremains that of improving the arty-consistertt ra e.The history-based grammar model is distinguished fromthe context-free grammar model in that each constituentstructure depends not only on the input string, but alsothe entire history up to that point in the sentence.
InHBGs, history is interpreted as any element of the out-put structure, or the parse tree, which has already beendetermined, including previous words, non-terminal cat-egories, constituent structure, and any other linguisticinformation which is generated as part of the parse struc-ture.6.
The  HBG Mode lUnlike P-CFG which assigns a probability to a mnemonicproduction, the HBG model assigns a probability to arule template.
Because of this the HBG formulation al-lows one to handle any grammar formalism that has aderivation process.For the HBG model, we have defined about 50 syntacticcategories, referred to as Syn, and about 50 semanticcategories, referred to as Sem.
Each NT (and thereforemnemonic) of the grammar has been assigned a syntactic(Syn) and a semantic (Sere) category.
We also associatewith a non-terminal a primary lexical head, denoted byH1, and a secondary lexical head, denoted by H2.
2 Whena rule is applied to a non-terminal, it indicates whichchild will generate the lexical primary head and whichchild will generate the secondary lexical head.The proposed generative model associates for each con-stituent in the parse tree the probability:p( Syn, Sere, R, H1, H2 ISynp, Sernp, Rp, Ipc, Hip, H2p)In HBG, we predict the syntactic and semantic labels ofa constituent, its rewrite rule, and its two lexical headsusing the labels of the parent constituent, the parent'slexical heads, the parent's rule Rp that lead to the con-stituent and the constituent's index Ipc as a child of Rp.As we discuss in a later section, we have also used withsuccess more information about the derivation tree thanthe immediate parent in conditioning the probability ofexpanding a constituent.We have approximated the above probability by the fol-lowing five factors:1. p(Syn IRp, Ipc, Hip, Synp, Semp)~The pr imary  lexical head H1 corresponds (roughly) to the lin-guistic not ion of a lexical head.
The secondary lexicM head H2has  no l inguistic parallel.
It merely represents a word in the con-s t i tuent  besides the head which conta ins predict ive in format ionabout  the const i tuent .1372. p( Sem ISyn, Rp, Ip?, Hip, It2p, Synp, Serf+)3. p( R ISyn, Sere, Rp, Ip?, Hip, H2p, Synp, Sern~ )4. p( ul \[R, Syn, Sere, Rp, Ipo, Hip, H2p )5. p(H2 \]Hi, R, Syn, Sem, Rp, Ip~, Synp)While a different order for these predictions is possible,we only experimented with this one.6.1.
Parameter  Es t imat ionWe only have built a decision tree to the rule probabil-ity component (3) of the model.
For the moment, weare using n-gram models with the usual deleted interpo-lation for smoothing for the other four components ofthe model.We have assigned bit strings to the syntactic and seman-tic categories and to the rules manually.
Our retention isthat bit strings differing in the least significant bit posi-tions correspond to categories of non-terminals or rulesthat are similar.
We also have assigned bitstrings forthe words in the vocabulary (the lexical heads) usingautomatic lustering algorithms using the bigram mu-tual information clustering algorithm (see \[4\]).
Giventhe bitsting of a history, we then designed a decisiontree for modeling the probability that a rule will be usedfor rewriting a node in the parse tree.Since the grammar produces parses which may be moredetailed than the Treebank, the decision tree was builtusing a training set constructed in the following man-ner.
Using the grammar with the P-CFG model we de-termined the most likely parse that is consistent withthe Treebank and considered the resulting sentence-treepair as an event.
Note that the grammar parse will alsoprovide the lexical head structure of the parse?
Then, weextracted using leftmost derivation order tuples of a his-tory (truncated to the definition of a history in the HBGmodel) and the corresponding rule used in expanding anode.
Using the resulting data set we built a decisiontree by classifying histories to locally minimize the en-tropy of the rule template.With a training set of about 9000 sentence-tree pairs, wehad about 240,000 tuples and we grew a tree with about40,000 nodes.
This required 18 hours on a 25 MIPSRISC-based machine and the resulting decision tree wasnearly 100 megabytes.6.2.
Immediate  vs.  Funct iona l  ParentsThe HBG model employs two types of parents, the im-mediate parent and the functional parent.
The immedi-ate parent is the constituent that immediately dominateswithR: PP iSyn:  PPSem: Wi th -DataHi:  l i s tH2 : w i thSem: DataHi: l i s tH2: aSyn:  Na Sem: DataHi:  l i s tH2: *Il i s tFigure 3: Sample representation f "with a list" in HBGmodel.the constituent being predicted?
If the immediate parentof a constituent has a different syntactic type from thatof the constituent, hen the immediate parent is also thefunctional parent; otherwise, the functional parent is thefunctional parent of the immediate parent?
The distinc-tion between functional parents and immediate parentsarises primarily to cope with unit productions?
Whenunit productions of the form XP2 ~ XP1 occur, the im-mediate parent of XP1 is XP2.
But, in general, the con-stituent XP2 does not contain enough useful informationfor ambiguity resolution.
In particular, when consider-ing only immediate parents, unit rules such as NP2 -+NP1 prevent he probabilistic model from allowing theNP1 constituent to interact with the VP rule which isthe functional parent of NP1.When the two parents are identical as it often hap-pens, the duplicate information will be ignored.
How-ever, when they differ, the decision tree will select thatparental context which best resolves ambiguities.138Figure 3 shows an example of the representation of ahistory in HBG for the prepositional phrase "with a list.
"In this example, the immediate parent of the N1 node isthe NBAR4 node and the functional parent of N1 is thePP1 node.7.
Resu l tsWe compared the performance of HBG to the "broad-coverage" probabilistic ontext-free grammar, P-CFG.The any-consisgen$ rate of the grammar is 90% on testsentences of 7 to 17 words.
The Vigerbi rate of P-CFGis 60% on the same test corpus of 760 sentences used inour experiments.
On the same test sentences, the HBCmodel has a Viterbi rate of 75%.
This is a reduction of37% in error rate.AccuracyP-CFG 59.8%HBG 74.6%Error Reduction 36.8%Figure 4: Parsing accuracy: P-CFG vs. HBGIn developing HBG, we experimented with similar mod-els of varying complexity.
One discovery made duringthis experimentation is that models which incorporatedmore context than HBG performed slightly worse thanHBG.
This suggests that the current raining corpus maynot contain enough sentences to estimate richer models.Based on the results of these experiments, it appearslikely that significantly increasing the size of the train-ing corpus should result in a corresponding improvementin the accuracy of HBG and richer HBG-like models.To 'check the value of the above detailed history, we triedthe simpler model:1. p(Ht IH~p, H=p, Rp, Ipc)2. p(H2 IHx, Hxp, H2p, Rp, Ip~)3. p(Sy  IH1, Rp, I 0)4. p(sem ISy., H1, Rp, Ipc)5. p(R ISyn, Sam, H1, H2)This model corresponds to a P-CFG with NTs that arethe crude syntax and semantic ategories annotated withthe lexical heads.
The Viterbi rate in this case was 66%,a small improvement over the P-CFG model indicatingthe value of using more context from the derivation tree.8.
ConclusionsThe success of the HBG model encourages future de-velopment of general history-based grammars as a morepromising approach than the usual P-CFG.
More ex-perimentation is needed with a larger Treebank thanwas used in this study and with different aspects of thederivation history.
In addition, this paper illustrates anew approach to grammar development where the pars-ing problem is divided (and hopefully conquered) intotwo subproblems: one of grammar coverage for the gram-marian to address and the other of statistical modelingto increase the probability of picking the correct parseof a sentence.References1.
Baker, J. K., 1975.
Stochastic Modeling for AutomaticSpeech Understanding.
In Speech Recognition, edited byRaj Reddy, Academic Press, pp.
521-542.2.
Brent, M. R. 1991.
Automatic Acquisition of Subcate-gorization Frames from Untagged Free-text Corpora.
InProceedings of the 29th Annual Meeting of the Associa-tion for Computational Linguistics.
Berkeley, California.3.
Brill, E., Magerman, D., Marcus, M., and Santorini,B.
1990.
Deducing Linguistic Structure from the Statis-tics of Large Corpora.
In Proceedings of the June 1990DARPA Speech and Natural Language Workshop.
Hid-den Valley, Pennsylvania.4.
Brown, P. F., Della Pietra, V. J., deSouza, P. V., Lai,J.
C., and Mercer, R. L. Class-based n-gram Models ofNatural Language.
In Proceedings of the IBM NaturalLanguage ITL, March, 1990.
Paris, France.5.
Church, K. 1988.
A Stochastic Parts Program and NounPhrase Parser for Unrestricted Text.
In Proceedings ofthe Second Conference on Applied Natural LanguageProcessing.
Austin, Texas.6.
Gale, W. A. and Church, K. 1990.
Poor Estimates ofContext are Worse than None.
In Proceedings of theJune 1990 DARPA Speech and Natural Language Work-shop.
Hidden Valley, Pennsylvania.7.
Harrison, M. A.
1978.
Introduction to Formal LanguageTheory.
Addison-Wesley Publishing Company.8.
Hindle, D. and Rooth, M. 1990.
Structural Ambiguityand Lexical Relations.
In Proceedings of the June 1990DARPA Speech and Natural Language Workshop.
Hid-den Valley, Pennsylvania.9.
Jelinek, F. 1985.
Self-organizing Language Modeling forSpeech Recognition.
IBM Report.10.
Magerman, D. M. and Marcus, M. P. 1991.
Pearl: AProbabilistic Chart Parser.
In Proceedings of the Febru-ary 1991 DARPA Speech and Natural Language Work-shop.
Asilomar, California.11.
Derouault, A., and Merialdo, B., 1985.
ProbabilistieGrammar for Phonetic to French Transcription.
ICASSP85 Proceedings.
Tampa, Florida, pp.
1577-1580.12.
Sharman, R. A., Jelinek, F., and Mercer, R. 1990.
Gen-erating a Grammar for Statistical Training.
In Proceed-ings of the June 1990 DARPA Speech and Natural Lan-guage Vv*orkshop.
Hidden Valley, Pennsylvania.139
