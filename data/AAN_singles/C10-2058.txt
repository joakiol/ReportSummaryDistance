Coling 2010: Poster Volume, pages 507?515,Beijing, August 2010Challenges from Information Extraction to Information FusionHeng JiComputer Science DepartmentQueens College and Graduate CenterCity University of New Yorkhengji@cs.qc.cuny.eduAbstractInformation Extraction (IE) technology is fac-ing new challenges of dealing with large-scaleheterogeneous data sources from differentdocuments, languages and modalities.
Infor-mation fusion, a new emerging area derivedfrom IE, aims to address these challenges.
Wespecify the requirements and possible solu-tions to perform information fusion.
The is-sues include redundancy removal, contradic-tion resolution and uncertainty reduction.
Webelieve this is a critical step to advance IE to ahigher level of performance and portability.1 IntroductionLatest development of Information Extraction(IE) techniques has made it possible to extract?facts?
(entities, relations and events) from un-structured documents, and converting them intostructured representations (e.g.
databases).
Oncethe collection grows beyond a certain size, anissue of critical importance is how a user canmonitor a compact knowledge base or identifythe interesting portions without having to (re)read large amounts of facts.
In this situation us-ers are often more concerned with the speed inwhich they obtain results, rather than obtainingthe exact answers to their queries (Jagadish etal., 1999).
The facts extracted from heterogene-ous data sources (e.g.
text, images, speech andvideos) must then be integrated in a knowledgebase, so that it can be queried in a uniform way.This provides unparalleled challenges and op-portunities for improved decision making.Data can be noisy, incorrect, or misleading.Unstructured data, mostly text, is difficult to in-terpret.
In practice it is often the case that thereare multiple sources which need to be extractedand compressed.
In a large, diverse, and inter-connected system, it is difficult to assure accu-racy or even coherence among the data sources.In this environment, traditional IE would be oflittle value.
Most current IE systems focus onprocessing a single document and language, andare customized for a single data modality.
In ad-dition, automatic IE systems are far from perfectand tend to produce errors.Achieving really advances in IE requires thatwe take a broader view, one that looks outside asingle source.
We feel the time is now ripe toincorporate some information integration tech-niques in the database community (e.g.
Seligmanet al, 2010) to extend the IE paradigm to real-time information fusion and raise IE to a higherlevel of performance and portability.
This re-quires us to work on a more challenging problemof information fusion - to remove redundancy,resolve contradictions and uncertainties by mul-tiple information providers and design a generalframework for the veracity analysis problem.The goal of this paper is to lay out the currentstatus and potential challenges of informationfusion, and suggest the following possible re-search avenues.?
Cross-document: We will discuss how toeffectively aggregate facts across documentsvia entity and event coreference resolution.?
Cross-lingual: A shrinking fraction of theworld?s Web pages are written in English,and so the ability to access pages across arange of languages is becoming increasinglyimportant for many applications.
This needcan be addressed in part by cross-lingual in-formation fusion.
We will discuss the chal-507lenges of extraction and translation respec-tively.?
Cross-media: Advances in speech and im-age processing make the application of IEpossible on other data modalities, beyondtraditional textual documents.2 Cross-Document Information FusionMost current IE systems focus on processing onedocument at a time, and except for coreferenceresolution, operate one sentence at a time.
Thesystems make only limited use of ?facts?
alreadyextracted in the current document.
The outputcontains rich structures about entities, relationsand events involving such entities.
However, dueto noise, uncertainty, volatility and unavailabilityof IE components, the collected facts may beincomplete, noisy and erroneous.
Several recentstudies have stressed the benefits of using infor-mation fusion across documents.
These methodsinvestigate quite different angles while follow acommon research theme, namely to exploitglobal background knowledge.2.1 Information InferenceAchieving really high performance (especially,recall) of IE requires deep semantic knowledgeand large costly hand-labeled data.
Many sys-tems also exploited lexical gazetteers.
However,such knowledge is relatively static (it is not up-dated during the extraction process), expensiveto construct, and doesn?t include any probabilis-tic information.
Error analysis on relation extrac-tion shows that a majority (about 78%) of errorsoccur on nominal mentions, and more than 90%missing errors occur due to the lack of enoughpatterns to capture the context between two en-tity mentions.
For instance, to describe the ?lo-cated?
relation between a bomber and a bus,there are more than 50 different interveningstrings (e.g.
?killed many people on a?, ?
?s at-tack on a?, ?blew apart a?, ?blew himself up ona?, ?drove his explosives-laden car into a?,?had rigged the?, ?set off a bomb on a?, etc.
),but the ACE1 training corpora only cover about1/3 of these expressions.Several recent studies have stressed the bene-fits of using information redundancy on estimat-ing the correctness of the IE output (Downey et1 http://www.itl.nist.gov/iad/mig/tests/ace/al., 2005), improving disease event extraction(Yangarber, 2006), Message UnderstandingConference event extraction (Mann, 2007; Pat-wardhan and Riloff, 2009) and ACE event ex-traction (Ji and Grishman, 2008).
This approachis based on the premise that many facts will bereported multiple times from different sources indifferent forms.
This may occur both within thesame document and within a cluster of topicallyrelated and successive documents.
Therefore, byaggregating similar facts across documents andconducting statistical global inference by favor-ing interpretation consistency, enhanced extrac-tion performance can be achieved with heteroge-neous data than uniform data.The underlying hypothesis of cross-documentinference is that the salience of a fact should becalculated by taking into consideration both itsconfidence and the confidence of other factsconnected to it, which is inspired by PageRank(Page et al, 1998) and LexRank (Erkan andRadev, 2004).
For example, a vote by linked en-tities which are highly voted on by other entitiesis more valuable than a vote from unlinked enti-ties.
There are two major heuristics: (1) an as-sertion that several information providers agreeon is usually more trustable than that only oneprovider suggests; and (2) an information pro-vider is trustworthy if it provides many pieces oftrue information, and a piece of information islikely to be true if it is provided by many trust-worthy providers.
(Yin et al, 2008) used theabove heuristics in a progressive, iterative en-hancement process for information fusion.The results from the previous work are prom-ising, but the heuristic inferences are highly de-pendent on the order of applying rules, and theperformance may have been limited by thethresholds which may overfit a small develop-ment corpus.
One promising method might beusing Markov Logic Networks (Richardson andDomingos, 2006), a statistical relational learninglanguage, to model these global inference rulesmore declaratively.
Markov Logic will make itpossible to compactly specify probability distri-butions over the complex relational inferences.
Itcan capture non-deterministic (soft) rules thattend to hold among facts but do not have to.
Ex-ploiting this approach will also provide greaterflexibility to incorporate additional linguistic andworld knowledge into inference.508The information fused across documents canbe represented as an information network (Ji,2009) in which entities can be viewed as verticeson the graph and they can be connected by sometype of static relationship (e.g.
those attributesdefined in NIST TAC-KBP task (McNamee andDang, 2009)), or as a temporal chain linking dy-namic events (e.g.
Bethard and Martin, 2008;Chambers and Jurafsky, 2009; Ji et al, 2009a).The latter representation is more attractive be-cause business or international affairs analystsoften review many news reports to track people,companies, and government activities andtrends.
The query logs from the commercialsearch engines show that there is a fair numberof news related queries (Mishne & de Rijke,2006), suggesting that blog search users have aninterest in the blogosphere response to news sto-ries as they develop.
For example, (Ji et al,2009a) extracted centroid entities and thenlinked events centered around the same centroidentities on a time line.Temporal ordering is a challenging task inparticular because about half of the event men-tions don?t include explicit time arguments.
Thetext order by itself is a poor predictor of chrono-logical order (only 3% temporal correlation withthe true order).
Single-document IE techniquecan identify and normalize event time argumentsfrom the texts, which results in a much bettercorrelation score of 44% (Ji et al, 2009a).
Butthis is still far from the ideal performance forreal applications.
In order to alleviate this bottle-neck, a possible solution is to exploit globalknowledge from the related documents andWikipedia, and related events to recover andpredict some implicit time arguments (Filatovaand Hovy, 2001; Mani et al, 2003; Mann, 2007;Eidelman, 2008; Gupta and Ji, 2009).2.2 Coreference ResolutionOne of the key challenges for information fusionis cross-document entity coreference ?
preciseclustering of mentions into correct entities.There are two principal challenges: the sameentity can be referred to by more than one namestring and the same name string can refer tomore than one entity.
The recent research hasbeen mainly promoted in the web people searchtask (Artiles et al, 2007) such as (Balog et al,2008), ACE2008 such as (Baron and Freedman,2008) and NIST TAC KBP (McNamee andDang, 2009) evaluations.
Interestingly, the qual-ity of information can often be improved by thefused fact network itself, which can be called asself-boosting of information fusion.
For exam-ple, if two GPE entities are involved in a ?con-flict-attack?
event, then they are unlikely to beconnected by a ?part-whole?
relation; ?Mah-moud Abbas?
and ?Abu Mazen?
are likely to becoreferential if they get involved in the same?life-born?
event.
Some prior work (Ji et al,2005; Jing et al, 2007) demonstrated the effec-tiveness of using semantic relations to improveentity coreference resolution; while (Downey etal., 2005; Sutton and McCallum, 2004; Finkel etal., 2005; Mann, 2007) experimented with in-formation fusion of relations across multipledocuments.
The TextRunner system (Banko etal., 2007)  can collapse and compress redundantfacts extracted from multiple documents basedon coreference resolution (Yates and Etzioni,2009), semantic similarity computation and nor-malization.Two relations are central for event fusion:contradiction ?
part of one event mention con-tradicts part of another, and redundancy ?
part ofone event mention conveys the same content as(or is entailed by) part of another.
Once thesecentral relations are identified they will providea basis for identifying more complex relationssuch as elaboration, presupposition or conse-quence.
It is important to note that redundancyand contradiction among event mentions arelogical relations that are not captured by tradi-tional topic-based techniques for similarity de-tection (e.g.
Brants and Stolle, 2002).
Contradic-tions also arise from complex differences in thestructure of assertions, discrepancies based onworld-knowledge, and lexical contrasts.
Ritter etal.
(2009) described a contradiction detectionmethod based on functional relations and pointedout that many contradictory fact pairs from theWeb appear consistent, and that requires back-ground knowledge to predict.Assessing event coreference is essential: fortexts to contradict, they must refer to the sameevent.
Event coreference resolution is more chal-lenging than entity coreference because eachlinking decision needs to be made based uponthe overall similarity of the event trigger andmultiple arguments.
Hasler and Orasan (2009)509further found that in many cases even coreferen-tial even arguments are not good indicators forevent coreference.Earlier work on event coreference resolution(e.g.
Bagga and Baldwin, 1999) was limited toseveral MUC scenarios.
Recent work (Chen etal., 2009) focus on much wider coverage ofevent types defined in ACE.
The methods fromthe knowledge fusion community (e.g.
Appriouet al, 2001; Gregoire, 2006) mostly focus onresolving conflicts rather than identifying them(i.e.
inconsistency problem rather than ambigu-ity).
These approaches allow the conflicts to beresolved in a straightforward way but they relyon the availability of meta-data (e.g., distributionof weights between attributes, probability as-signment etc.).
However, it is not always clearwhere to get this meta-data.The event attributes such as Modality, Polarity,Genericity and Tense (Sauri et al, 2006) willplay an important role in event coreference reso-lution because two event mentions cannot becoreferential if any of the attributes conflict witheach other.
Such attempts have been largely ne-glected in the prior research due to the lowweights of attribute labeling in the ACE scoringmetric.
(Chen et al, 2009) demonstrated thatsimple automatic event attribute labeling cansignificantly improve event coreference resolu-tion.
In addition, some very recent work includ-ing (Nicolae and Nicolae, 2006; Ng, 2009; Chenet al, 2009) found that graph-cut based cluster-ing can improve coreference resolution.
Thechallenge lies in computing the affinity matrix.3 Cross-Lingual Information FusionCross-lingual comparable corpora are alsoprevalent now because almost all the influentialevents can be reported in multi-languages at thefirst time, but probably in different aspects.Therefore, linked fact networks can be con-structed and lots of research tasks can benefitfrom such structures.
Since the two networks aresimilar in structure but not homogeneous, we cando alignment and translation which may advanceinformation fusion.
Cross-lingual informationfusion is concerned with technologies that fusethe information available in various languagesand present the fused information in the user-preferred language.
The following fundamentalcross-lingual IE pipelines can be employed: (1)Translate source language texts into target lan-guage, and then run target language IE on thetranslated texts.
(2) Run source language IE onthe source language texts, and then use machinetranslation (MT) word alignments to translate(project) extracted information into target lan-guages.
Regardless of the different architectures,both pipelines are facing the following chal-lenges from extraction and translation.3.1 Extraction ChallengesSome recent fusion work focus on cross-lingualinteraction and inference to improve both sidessynchronously, beyond the parallel comparisonsof cross-lingual IE pipelines in (e.g.
Riloff et al,2002).
One of such examples is on cross-lingualco-training (e.g.
Cao et al, 2003; Chen and Ji,2009).
In co-training (Blum and Mitchell, 1998),the uncertainty of a classifier is defined as theportion of instances on which it cannot makeclassification decisions.
Exchanging tagged datain bootstrapping can help reduce the uncertain-ties of classifiers.
The cross-lingual fusion proc-ess satisfies the co-training algorithm?s assump-tions about two views (in this case, two lan-guages): (1) the two views are individually suffi-cient for classification (IE systems in both lan-guages were learned from annotated corporawhich are enough for reasonable extraction per-formance); (2) the two views are conditionallyindependent given the class (IE systems in dif-ferent languages may use different features andresources).
(Cao et al, 2003) indicated that uncertaintyreduction is an important factor for enhancingthe performance of co-training.
It?s important todesign new uncertainty measures for represent-ing the degree of uncertainty correlation of thetwo classifiers in co-training.
(Chen and Ji, 2009)proposed a new co-training framework usingcross-lingual information projection.
They dem-onstrated that this framework is particularly ef-fective for a challenging IE task which is situ-ated at the end of a pipeline and thus suffersfrom the errors propagated from upstream proc-essing and has low-performance baseline.3.2 Translation ChallengesBecause the facts are aggregated from multiplelanguages, the translation errors will bring usgreat challenges.
However, in order to extend510cross-lingual information fusion techniques tomore language pairs, we can start from the muchmore scalable task of ?information?
translation(Etzioni et al, 2007).
The additional processingmay take the form of machine translation (MT)of extracted facts such as names and events.
IEtasks performed notably worse on machine trans-lated texts than on texts originally written inEnglish, and error analysis indicated that a majorcause was the low quality of name translation (Jiet al, 2009b).
Traditional MT systems focus onthe overall fluency and accuracy of the transla-tion but fall short in their ability to translate cer-tain informationally critical words.
In particular,it appears that better entity name translation cansubstantially improve cross-lingual informationfusion.Some recent work (e.g.
Klementiev and Roth,2006; Ji, 2009) has exploited comparable cor-pora to enhance information translation.
Thereare no document-level or sentence-level align-ments across languages, but important facts suchas names, relations and events in one language insuch corpora tend to co-occur with their coun-terparts in the other.
(Ji, 2009) used a bootstrap-ping approach to align the information networksfrom bilingual comparable corpora, and discovername translations and extract relations links si-multaneously.
The general idea is to start from asmall seed set of common name pairs, and thenrely on the link attributes to align their relatednames.
Then the new name translations areadded to the seed set for the next iteration.
Thisbootstrapping procedure is repeated until no newtranslations are produced.
This approach is basedon graph traverses and doesn?t need a nametransliteration module to serve as baseline, orcompute document-wise temporal distributions.The novelty of using comparable corpora liesin constructing and mining multi-lingual infor-mation fusion framework which is capable ofself-boosting.
First, this approach can generateinformation translation pairs with high accuracyby using a small seed set.
Second, the shortcom-ings of traditional approaches are due to theirlimited use of IE techniques, and this approachcan effectively integrate extraction and transla-tion based on reliable confidence estimation.Third, compared to bitexts this approach cantake advantage of much less expensive compara-ble corpora.
This approach can be extended tofoster the research in other aspects for informa-tion fusion.
For example, the aligned sub-graphswith names, relations and events can be used toreduce information redundancy; the outlier (mis-aligned) sub-graphs can be used to detect thenovel or local information described in one lan-guage but not in the other after the fusion proc-ess.
It does happen that the two persons havebeen explicitly reported as Father and Son rela-tionship in one language, but in the other lan-guage, they are just reported as two commonpersons.4 Cross-Media Information FusionThe research challenges discussed so far con-cerned with textual data.
Besides written texts,ever-increasing human generated data is avail-able as speech recordings, microblogs, imagesand videos.
We now discuss how to developtechniques for fusing a variety of media sources.State-of-the-art IE techniques have been devel-oped primarily on newspaper articles and a fewweb texts, and it is not clear how systems wouldperform on other sources and how to integrate allavailable information.4.1 Coreference ResolutionThe main challenge is on designing a coherentinformation fusion framework that is able to ex-ploit information across different parts of multi-media documents and link them via cross-mediacoreference resolution.
The framework will han-dle multimedia information by considering notonly the document?s text and images data butalso the layout structure which determines how agiven text block is related to a particular imageor video.
For example, a Web news page about?Health Care Reform in America?
is composedby text describing some event (e.g., Final Senatevote for the reform plans, Obama signs the re-form agreement), images (e.g., images aboutvarious government involvements over decades)and videos (e.g.
Obama?s speech video about thedecisions) containing additional information re-garding the real extent of the event or providingevidence corroborating the text part.Current state-of-the-art information fusion ap-proaches can be divided into two groups: formal?top-down?
methods from the generic knowl-edge fusion community and quantitative ?bot-tom-up?
techniques from the applied Semantic511Web community (Appriou et al, 2001; Gregoire,2006).
Both approaches have their limitations.
Itwill be beneficial to combine both types of ap-proaches so that the fusion decision can be madedepending on the type of problem and theamount of domain information it possesses.
Sag-gion et al (2004) described a multimedia extrac-tion approach to create composite index frommultiple and multi-lingual sources.
Magalhaeset al (2008) described a semantic similarity met-ric based on key word vectors for multi-mediafusion.
Iria and Magalhaes (2009) exploited in-formation across different parts of a multimediadocument to improve document classification.
Itis important to go beyond key words and attemptrepresenting the documents by the semantic factsidentified by IE.One possible solution is to exploit the linkageinformation.
Specifically, coreference resolutionmethods should be applied to four types of cross-media data: (1) between the captions of imagesand context texts; (2) detecting HTML cross-media associations and quantifying the level ofimage and text block correlation (3) between thetexts embedded in images and context texts; (4)between the transcribed texts from the speech invideo clips (via automatic speech recognition)and context texts.
We can apply a similaritygraph to incorporate virtual linkages.
For exam-ple, when we see images of two web documentscontaining the same object, we can raise ourconfidence that such documents are semanti-cally correlated even if the two web documentsare from different sources.4.2 Uncertainty ReductionWhen we combine information from images andtheir associated texts (e.g.
meta-data, captions,surrounding text, transcription), one of the chal-lenges lies in the uncertainty of text representa-tion.
Therefore it is important to study both howto learn good models from different sources withdifferent kinds of associated uncertainty, andhow to make use of these, along with their levelof uncertainty in supporting coherent decisions,taking into account characteristics of the data aswell as of its source.The descriptions are usually generated by hu-mans and thus are prone to error or subjectiv-ity.
The images, especially the web images, aretypically labeled by different users in differentlanguages and cultural backgrounds.
It is unreal-istic to expect descriptions to be consistent.
Inspeech conversations, many facts are often em-bedded in questions such as ?It's OK to put De-mocratic career politicians at the Pentagon andthe Justice Department if they're Democrats butnot if they're Republicans, is that right??
Thischallenge can be generally addressed bystrengthening semantic attribute classificationmethods for Modality, Polarity and Genericity.And if the data sources are comparable, a moredirect method of committee-based voting canalso be exploited.However, the fusion process may itself causedata uncertainties.
We can follow the co-trainingframework as described in section 3.1 to reduceuncertainty in fusion.
To handle the missing la-bels, a promising approach is to use graph-basedlabel propagation (Deshpande et al, 2009),which can capture complex uncertainties andcorrelations in the data in a uniform manner.
It?salso worth importing the multi-dimensional un-certainty analysis framework described in datamining community (Aggarwal, 2010).
Themulti-dimensional uncertainty analysis methodexactly suits the multi-media fusion needs: itallows us to combine first-order logic with prob-abilities, modeling inferential uncertainty aboutmultiple aspects - both the context of facts andintended meanings.4.3 Joint ModelingIE is generally applied on top of machine gener-ated transcription and automatic structuring thatsuffer from errors compared to the true contentof relations and events.
In the context of infor-mation fusion we can divide the problem of ad-aptation into two types: (1) radical adaptationsuch as from newswire to biomedical articles;(2) modest adaptation such as from newswire towikipedia or automatic speech recognition(ASR) output.
(1) requires a great deal of newdevelopment such as ontology definition anddata annotation; while (2) can be partially ad-dressed during the information fusion process.For example, while dealing with speech input,IE systems need to be robust to the noise intro-duced by earlier speech processing tasks such asASR, sentence segmentation, salience detectionand and speaker identification.
Some earlierwork (Makhoul et al, 2005; Favre et al, 2008)512showed that using an IE system trained fromnewswire, the performance degrades notablywhen the system is tested on automatic speechrecognition output.
But no general solutionshave been proposed to address the genre-specificchallenges for speech data.More specifically, pronoun resolution is oneof the major challenges (Jing et al, 2007).
Forexample, in wikipedia a lot of pronouns mayrefer to the entry entity; while in speech conver-sation we will need to resolve first and secondperson pronouns based on automatic speaker roleidentification; and improve cross-sentence thirdpronoun resolution by exploiting gender andanimacy knowledge discovery methods.The processing methods of text and other me-dia are typically organized as a pipeline architec-ture of processing stages (e.g.
from pattern rec-ognition, to information fusion, and to summari-zation).
Each of these stages has been studiedseparately and quite intensively over the pastdecade.
It?s critical to move away from ap-proaches that make chains of independent localdecisions, and instead toward methods that makemultiple decisions jointly using global informa-tion.
Joint inference techniques (Roth and Yih,2004; Ji et al, 2005; McCallum, 2006) can trans-form the integration of multi-media into a bene-fit by reducing the errors in individual stages.
Indoing so, we can take advantage (among otherproperties) of the coherence of a discourse: that acorrect analysis of a text discourse reveals alarge number of connections from the image in-formation in its context, and so (in general) amore tightly connected analysis is more likely tobe correct.
For example, prior work has demon-strated the benefit of jointly modeling name tag-ging and n-best hypotheses, ASR lattices orword confusion networks (Hakkani-T?r et al,2006).5 ConclusionIn the current information explosion era, IEtechnology is facing new challenges of dealingwith heterogeneous data sources from differentdocuments, languages and media which maycontain a multiplicity of aspects on particularentities, relations and events.
This new phenom-ena requires IE to perform both traditional lowerlevel processing as well as information fusion offactual data based on implicit inferences.
Thispaper investigated the issues of information fu-sion on a massive scale and the challenges havenot been discussed in previous work.
We speci-fied the requirements and possible solutions forvarious dimensions to perform information fu-sion.
We also overviewed some recent work todemonstrate how these goals can be achieved.The field of information fusion is relativelynew; and the nature of different data sourcesprovides new ideas and challenges which are notpresent in other research.
While much researchhas been performed in the area of data fusion,the context of automatic extraction provides adifferent perspective in which the fusion is per-formed in the context of a lot of uncertainty andnoise.
This new task will provide connectionsbetween NLP and other areas such as data min-ing and knowledge discovery.
The progress onthis task would save, anybody concerned withstaying informed, an enormous amount of time.These are certainly ambitious goals and requirelong-term development of fusion and adaptationmethods.
But we hope that this outline of theresearch challenges will bring us closer to thegoal.AcknowledgementThis work was supported by the U.S. Army Re-search Laboratory under Cooperative AgreementNumber W911NF-09-2-0053, the U.S. NSFCAREER Award under Grant IIS-0953149,Google, Inc., DARPA GALE Program, CUNYResearch Enhancement Program, PSC-CUNYResearch Program, Faculty Publication Programand GRTI Program.
The views and conclusionscontained in this document are those of the au-thors and should not be interpreted as represent-ing the official policies, either expressed or im-plied, of the Army Research Laboratory or theU.S.
Government.
The U.S. Government is au-thorized to reproduce and distribute reprints forGovernment purposes notwithstanding any copy-right notation here on.ReferencesCharu Aggarwal.
2010.
On Multi-dimensional Sharp-ening of Uncertain Data.
SIAM: SIAM Conferenceon Data Mining (SDM10).A.
Appriou-, A. Ayoun, et al 2001.
Fusion: Generalconcepts and characteristics.
International Journalof Intelligent Systems 16(10).513Javier Artiles, Julio Gonzalo and Satoshi Sekine.2007.
The SemEval-2007 WePS Evaluation: Es-tablishing a benchmark for the Web People SearchTask.
Proc.
Semeval-2007.Amit Bagga and Breck Baldwin.
1999.
Cross-document Event Coreference: Annotations, Ex-periments, and Observations.
Proc.
ACL1999Workshop on Coreference and Its Applications.K.
Balog, L. Azzopardi, M. de Rijke.
2008.
PersonalName Resolution of Web People Search.
Proc.WWW2008 Workshop: NLP Challenges in the In-formation Explosion Era (NLPIX 2008).Michele Banko, Michael J Cafarella, StephenSoderland and Oren Etzioni.
2007.
Open Informa-tion Extraction from the Web.
Proc.
IJCAI 2007.Alex Baron and Marjorie Freedman.
2008. Who isWho and What is What: Experiments in Cross-Document Co-Reference.
Proc.
EMNLP 2008.Steven Bethard and James H. Martin.
2008.
Learningsemantic links from a corpus of parallel temporaland causal relations.
Proc.
ACL-HLT 2008.Avrim Blum and Tom Mitchell.
1998.
CombiningLabeled and Unlabeled Data with Co-training.Proc.
of the Workshop on Computational LearningTheory.
Morgan Kaufmann Publishers.T.
Brants and R. Stolle.
2002.
Finding Similar Docu-ments in Document Collections.
Proc.
LRECWorkshop on Using Semantics for InformationRetrieval and Filtering.Yunbo Cao, Hang Li and Li Lian.
2003.
UncertaintyReduction in Collaborative Bootstrapping:Measure and Algorithm.
Proc.
ACL 2003.Nathanael Chambers and Dan Jurafsky.
2009.
Unsu-pervised Learning of Narrative Schemas andtheir Participants.
Proc.
ACL 09.Zheng Chen and Heng Ji.
2009.
Can One LanguageBootstrap the Other: A Case Study on Event Ex-traction.
Proc.
HLT-NAACL Workshop on Semi-supervised Learning for Natural Language Proc-essing.
Boulder, Co.Zheng Chen, Heng Ji and Robert Harallick.
2009.
APairwise Coreference Model, Feature Impact andEvaluation for Event Coreference Resolution.Proc.
RANLP 2009 workshop on Events in Emerg-ing Text Types.Amol Deshpande, Lise Getoor and Prithviraj Sen.2009.
Graphical Models for Uncertain Data.Managing and Mining Uncertain Data (Editedby Charu Aggarwal).
Springer.Doug Downey, Oren Etzioni, and Stephen Soderland.2005.
A Probabilistic Model of Redundancy in In-formation Extraction.
Proc.
IJCAI 2005.Vladimir Eidelman.
2008.
Inferring Activity Time inNews through Event Modeling.
Proc.
ACL-HLT2008.Gunes Erkan and Dragomir R. Radev.
2004.LexPageRank: Prestige in multi-document textsummarization.
Proc.
EMNLP 2004.Oren Etzioni, Kobi Reiter, Stephen Soderland andMarcus Sammer.
2007.
Lexical Translation withApplication to Image Search on the Web.
Proc.Machine Translation Summit XI.Benoit Favre, Ralph Grishman, Dustin Hillard, HengJi, Dilek Hakkani-Tur and Mari Ostendorf.
2008.Punctuating Speech for Information Extraction.Proc.
ICASSP 2008.Elena Filatova and Eduard Hovy.
2001.
AssigningTime-Stamps to Event-Clauses.
Proc.
ACL 2001Workshop on Temporal and Spatial InformationProcessing.Jenny Rose Finkel, Trond Grenager and ChristopherManning.
2005.
Incorporating Non-local Informa-tion into Information Extraction Systems by GibbsSampling.
Proc.
ACL 2005.E.
Gregoire.
2006.
An unbiased approach to iteratedfusion by weakening.
Information Fusion.
7(1).Prashant Gupta and Heng Ji.
2009.
Predicting Un-known Time Arguments based on Cross-eventpropagation.
Proc.
ACL-IJCNLP 2009.Dilek Hakkani-T?r, Fr?d?ric B?chet, Giuseppe Ric-cardi, Gokhan Tur.
2006.
Beyond ASR 1-Best: Us-ing Word Confusion Networks in Spoken Lan-guage Understanding.
Journal of Computer Speechand Language, Vol.
20, No.
4, pp.
495-514.Laura Hasler and Constantin Orasan.
2009.
Docoreferential arguments make event mentionscoreferential?
Proc.
the 7th Discourse Anaphoraand Anaphor Resolution Colloquium (DAARC2009).Jose Iria and Joao Magalhaes.
2009.
ExploitingCross-Media Correlations in the Categorizationof Multimedia Web Documents.
Proc.
CIAM2009.H.
V. Jagadish, Jason Madar, and Raymond Ng.
1999.Semantic compression and pattern extractionwith fascicles.
VLDB, pages 186?197.Heng Ji, David Westbrook and Ralph Grishman.2005.
Using Semantic Relations to Refine Corefer-ence Decisions.
Proc.
HLT/EMNLP 05.Heng Ji and Ralph Grishman.
2008.
Refining EventExtraction Through Cross-document Inference.Proc.
ACL 2008.Heng Ji.
2009.
Mining Name Translations fromComparable Corpora by Creating Bilingual In-formation Networks.
Proc.
ACL-IJCNLP 2009workshop on Building and Using ComparableCorpora (BUCC 2009): from parallel to non-parallel corpora.Heng Ji, Ralph Grishman, Dayne Freitag, MatthiasBlume, John Wang, Shahram Khadivi, RichardZens and Hermann Ney.
2009a.
Name Transla-514tion for Distillation.
Book chapter for GlobalAutomatic Language Exploitation.Heng Ji, Ralph Grishman, Zheng Chen and PrashantGupta.
2009b.
Cross-document Event Extraction,Ranking and Tracking.
Proc.
RANLP 2009.Hongyan Jing, Nanda Kambhatla and Salim Roukos.2007.
Extracting Social Networks and Bio-graphical Facts From Conversational SpeechTranscripts.
Proc.
ACL 2007.A.
Klementiev and D. Roth.
2006.
Named EntityTransliteration and Discovery from MultilingualComparable Corpora.
Proc.
HLT-NAACL 2006.Joao Magalhaes, Fabio Ciravegna and Stefan Ruger.2008.
Exploring Multimedia in a Keyword Space.Proc.
ACM Multimedia.Inderjeet Mani, Barry Schiffman and Jianping Zhang.2003.
Inferring Temporal Ordering of Events inNews.
Proc.
HLT-NAACL 2003.John Makhoul, Alex Baron, Ivan Bulyko, LongNguyen, Lance Ramshaw, David Stallard, RichardSchwartz and Bing Xiang.
2005.
The Effects ofSpeech Recognition and Punctuation on Informa-tion Extraction Performance.
Proc.
Interspeech.Gideon Mann.
2007.
Multi-document RelationshipFusion via Constraints on Probabilistic Data-bases.
Proc.
HLT/NAACL 2007.Andrew McCallum.
2006.
Information Extraction,Data Mining and Joint Inference.
Proc.
SIGKDD.Paul McNamee and Hoa Dang.
2009.
Overview ofthe TAC 2009 Knowledge Base PopulationTrack.
Proc.
TAC 2009 Workshop.Gilad Mishne and Maarten de Rijke.
2006.
CapturingGlobal Mood Levels using Blog Posts.
Proc.
AAAI2006 Spring Symposium on Computational Ap-proaches to Analysing Weblogs.Vincent Ng.
2009.
Graph-Cut-Based AnaphoricityDetermination for Coreference Resolution.
Proc.HLT-NAACL 2009.Lawrence Page, Sergey Brin, Rajeev Motwani andTerry Winograd.
1998.
The PageRank CitationRanking: Bringing Order to the Web.
Proc.
WWW.Siddharth Patwardhan and Ellen Riloff.
2009.
A Uni-fied Model of Phrasal and Sentential Evidencefor Information Extraction.
2009.
Proc.
EMNLP.Matt Richardson and Pedro Domingos.
2006.
MarkovLogic Networks.
Machine Learning.
62:107-136.Ellen Riloff, Charles Schafer, and David Yarowsky.2002.
Inducing Information Extraction Systemsfor New Languages via Cross-Language Projec-tion.
Proc.
COLING 2002.Alan Ritter; Stephen Soderland; Doug Downey; OrenEtzioni.
2009.
It?s a Contradiction ?
no, it?s not:A Case Study using Functional Relations.
Proc.EMNLP 2009.Dan Roth and Wen-tau Yih.
2004.
A Linear Pro-gramming Formulation for Global Inference inNatural Language Tasks.
Proc.
CONLL2004.Saggion, H., Cunningham, H., Bontcheva, K., May-nard, D., Hamza, O., and Wilks, Y.
2004.
Multi-media indexing through multi-source and multi-language information extraction: the MUMIS pro-ject.
Data Knowlege Engineering, 48, 2, pp.
247-264.Roser Saur?
and Marc Verhagen and James Puste-jovsky.
2006.
Annotating and Recognizing EventModality in Text.
Proc.
FLAIRS 2006.Len Seligman, Peter Mork, Alon Halevy, Ken Smith,Michael J. Carey, Kuang Chen, Chris Wolf,Jayant Madhavan and Akshay Kannan.
2010.OpenII: An Open Source Information IntegrationToolkit.
Proc.
the 2010 international conferenceon Management of data.Charles Sutton and Andrew McCallum.
2004.Collective Segmentation and Labeling of DistantEntities in Information Extraction.
Proc.
ICMLWorkshop on Statistical Relational Learning andIts Connections to Other Fields.Roman Yangarber.
2006.
Verification of Facts acrossDocument Boundaries.
Proc.
InternationalWorkshop on Intelligent Information Access.Alexander Yates and Oren Etzioni.
2009.
Unsuper-vised Methods for Determining Object and Rela-tion Synonyms on the Web.
Journal of ArtificialIntelligence.
Res.
(JAIR) 34: 255-296.Xiaoxin Yin, Jiawei Han and Philip S. Yu.
2008.Truth Discovery with multiple conflicting infor-mation providers on the web.
IEEE Trans.Knowledge and Data Eng., 20:796-808.515
