Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,pages 355?364, Dublin, Ireland, August 23-29 2014.Inducing Word Sense with Automatically Learned Hidden ConceptsBaobao Chang Wenzhe Pei Miaohong ChenKey Laboratory of Computational Linguistics, Ministry of EducationSchool of Electronics Engineering and Computer Science, Peking UniversityBeijing, P.R.China, 100871{chbb,peiwenzhe,miaohong-chen}@pku.edu.cnAbstractWord Sense Induction (WSI) aims to automatically induce meanings of a polysemous word fromunlabeled corpora.
In this paper, we first propose a novel Bayesian parametric model to WSI.Unlike previous work, our research introduces a layer of hidden concepts and view senses asmixtures of concepts.
We believe that concepts generalize the contexts, allowing the model tomeasure the sense similarity at a more general level.
The Zipf?s law of meaning is used as away of pre-setting the sense number for the parametric model.
We further extend the parametricmodel to non-parametric model which not only simplifies the problem of model selection butalso brings improved performance.
We test our model on the benchmark datasets released bySemeval-2010 and Semeval-2007.
The test results show that our model outperforms state-of-the-art systems.1 IntroductionWord Sense Induction (WSI) aims to automatically induce meanings of a polysemous word from unla-beled corpora.
It discriminates among meanings of a word by identifying clusters of similar contexts.Unlike the task of Word Sense Disambiguation (WSD), which classifies polysemous words accordingto a pre-existing and usually hand-crafted inventory of senses, WSI makes it attractive to researchers byeliminating dependence on a particular sense inventory and learning word meaning distinction directlybased on the contexts as observed in corpora.Almost all WSI work relies on the distributional hypothesis, which states that words occurring insimilar contexts will have similar meanings.
To effectively discriminate among contexts, proper repre-sentation of contexts would be a key issue.
Basically, context can be represented as a vector of wordsco-occurring with the target word within a fixed context window.
The similarity between two contextsof the target word can then be measured by the geometrical distance between the corresponding vectors.To ease the sparse problem and capture more semantic content, some kinds of generalizations or abstrac-tions are needed.
For example, a context of bank including money may not share similarity with thatincluding cash measured at word level.
However, given the conceptual relationship between money andcash, the two contexts actually share high similarity.One straightforward way of introducing conceptualization is to assign semantic code to context words,where semantic codes could be derived from WordNet or other resources like thesauruses.
However, twoproblems remain to be tackled.
The first one concerns ambiguities of context words.
Context words mayhave multiple semantic codes and thus word sense disambiguation to context words or other extra costis needed.
The second one concerns the nature of WSI task.
WSI actually is target-word-specific, whichmeans the conceptualization should be done specifically to different target words.
A general purposeconceptualization defined by a thesaurus may not well meet this requirement and may not be equallysuccessful in discriminating contexts of different target words.To address these problems, we first propose a parametric Bayesian model which jointly finds concep-tual representations of context words and the sense of the target word.
We do this by introducing a layerof target-specific conceptual representation between the target sense layer and the context words layerThis work is licenced under a Creative Commons Attribution 4.0 International License.
Page numbers and proceedings footerare added by the organizers.
License details: http://creativecommons.org/licenses/by/4.0/355Figure 1: Architecture of our model Figure 2: Graphical notation of the Basic Modelthrough a Bayesian framework as illustrated in Figure 1.
From the generative perspective, the sense ofthe target word is first sampled.
Then the sense generates different conceptual configurations which inturn generate different contexts.
With a deeper architecture, our model makes it possible to induce wordsenses at a more abstract level, i.e.
the concept level, which is not only less sparse but also more seman-tically oriented.
Both the senses of the target word and the latent concepts are inferred automatically andunsupervisedly with inference procedure given enough contexts involving a target word.
The latent con-cepts inferred with the model share similarities with those defined in thesauruses, as both of them clustersemantically related words.
However, since the latent concepts are inferred with regard to individualtarget words, they are target-word-specific and thus fit the WSI task better than general purpose conceptsdefined in thesauruses.
Context words may still correspond to multiple latent concepts.
However, thedisambiguation is implicitly done in the process of the word sense induction.Setting the number of senses that the algorithm should arrive at is another problem frequently exer-cising the minds of WSI people.
Instead of trying different sense numbers on a word-by-word basis,we propose to use Zipf?s law of meaning (Zipf, 1945) to guide the selection of the sense numbers inthis paper.
With the law of meaning, sense numbers could be set on an all-word basis, rather than on aword-by-word basis.
This is not only simple but also efficient, especially in the case where there are alarge number of target words to be concerned.We further extend the parametric model into a non-parametric model, as it allows adaptation of modelcomplexity to data.
By extending our model to non-parametric model, the need to preset the numbers ofsenses and latent concepts are totally removed and, moreover, the model performance is also improved.We evaluate our model on the commonly used benchmark datasets released by both Semeval-2010(Manandhar et al., 2010) and Semeval-2007 (Agirre and Soroa, 2007).
The test results show that ourmodels perform much better than the state-of-the-art systems.2 The parametric model2.1 Basic ModelThe main point of our work is that different senses are signaled by contexts with different concept con-figurations, where different concepts are formally defined as different distributions over context words.Formally, we denote by P (s) the global multinomial distribution over senses of an ambiguous wordand by P (w|z) the multinomial distributions over context words w given concept z.
Context words aregenerated by a mixture of different concepts whose mixture proportion is defined by P (z|s), such that:P (wi) =?jP (s = j)?kP (zi= k|s = j)P (wi|zi= k)Following the model, each context word wisurrounding the target word is generated as follows: First, asense s is sampled from P (s) for the target word.
Then for each context word position i, a concept ziissampled according to mixture proportion P (z|s) and wiis finally sampled from P (w|z).Figure 2 shows the model with the graphical notation, where M is the number of instances of contextsregarding to a concerned target word and Nmis the number of word tokens in context m. smis thesense label for target word in context m. wm,nis the n-th context word in context m. zm,nis theconcept label associated with wm,n.
I is the total number of senses to be induced.
J is the total number356Figure 3: Graphical notation of the non-parametric WSI modelof concepts.~?
is the notational shorthand for the sense distribution P (s), ~?iis the shorthand for thei-th sense-concept distribution P (z|s = i), and ~?jis the j-th concept-word distribution P (w|z = j).Following conventional Bayesian practice,~?, ~?iand ~?jare assumed to be drawn from Dirichlet priorswith symmetric parameter ?, ?, ?
respectively.
The observed variable is represented with shaded nodeand hidden variable with unshaded node.2.2 Zipf?s law of meaningMost of the WSI work requires that the number of senses to be induced be specified ahead of time.One straightforward way to deal with this problem is to repeatedly try different numbers of senses ona development set and select the best performed number.
However, this should be done in principle ona word-by-word basis, and thus could be time-consuming and prohibitive when there are lots of targetwords to be concerned.
A more systematic way of setting sense numbers in Bayesian models is extendingthe parametric model into a non-parametric model, which will be described in detail in section 3.To work with our parametric model, we propose in this paper that an empirical law, Zipf?s law ofmeaning (Zipf, 1945), could be used to guide the sense number selection.
Zipf?s law of meaning statesthat the number of sense of a word is proportional to its frequency as shown in the following equation:I = K ?
fb(1)where I is the number of word senses and f is the frequency of the word.
K is the coefficient ofproportionality which is unknown and b is about 0.404 according to an experimental study done byEdmonds (2006).Certainly, Zipf?s law of meaning is not as strict as a rigorous mathematical law.
However, it sketchesthe distribution of the sense numbers with word frequencies of all words and allows us to estimate thesense numbers on an all-word basis by selecting appropriate coefficient K. This is not only simple butalso efficient, especially in the case that there are a large number of target words to be concerned.3 Non-parametric ModelA limitation of the parametric model is that the sense number I of the target word and the number Jof latent concepts need to be fixed beforehand.
Bayesian non-parametric (BNP) models offer elegantapproach to the problem of model selection and adaption.
Rather than comparing models that vary incomplexity, the BNP approach is to fit a single model that can adapt its complexity to the data.
Unlikethe parametric approach, BNP approach assumes an infinite number of clusters, among which only a feware active given the training data.
Our basic model can be naturally extended into a BNP model as shownin Figure 3.
Instead of assuming a finite number of senses, we place a nonparametric, Dirichlet process(DP) prior on the sense distribution as follows:G ?
DP (?,H)sm?
G,m = 1, 2, .
.
.
,Mwhere ?
is the concentration parameter and H is the base measure of the Dirichlet process.357For each sense siof the target words, we place a Hierarchical Dirichlet process (HDP) prior on themixture proportion to latent concepts shown as follows:G0?
DP (?,H0)Gi?
DP (?,G0), i = 1, 2, .
.
.zm,n?
Gi, n = 1, 2, .
.
.
, Nmwm,n?
~?zm,nwhere ?
and ?
are concentration parameters to G0and Gi, H0is the base measure of G0.By using HDP priors, we make sure that the same set of concept-word distributions is shared acrossall senses and all contexts of a target word, since each random measure Giinherits its set of conceptsfrom the same G0.As in parametric model, ~?jis the j-th concept-word distribution P (w|z = j), however, there are nowan infinite number of such distributions.
So is the number of senses.
However, with a fixed number ofcontexts of the target word, only a finite number of senses and concepts are active and they could beinferred automatically by the inference procedure.4 Model InferenceWe use Gibbs sampling (Casella and George, 1992) for inference to both the parametric and nonpara-metric model.
As a particular Markov Chain Monte Carlo (MCMC) method, Gibbs sampling is widelyused for inference in various Bayesian models (Teh et al., 2006; Li and Li, 2013; Li and Cardie, 2014).4.1 The Parametric ModelFor the parametric model, we use collapsed Gibbs sampling, in which the sense distribution~?, sense-concept distribution ~?iand concept-word distribution ~?jare integrated out.
At each iteration, the senselabel smof the target word in context m is sampled from conditional distribution p(sm|~s?m, ~z, ~w),and the concept label zm,nfor the context word wm,nis sampled from conditional distributionp(zm,n|~s, ~z?
(m,n), ~w).
Here ~s?mrefers to all current sense assignments other than smand ~z?
(m,n)refersto all current concept assignment other than zm,n.The conditional distribution p(sm|~s?m, ~z, ~w) and p(zm,n|~s, ~z?
(m,n), ~w) can be derived as shown inequation (2) and (3) respectively:p(sm= i|~s?m, ~z, ~w;?, ?, ?)
?
(c?mi+ ?)
?
?Jj=1?fm,jx=1(c?mi,j+ ?
+ x?
1)?fm,?x=1(?Jj=1c?mi,j+ J ?
?
+ x?
1)(2)p(zm,n= j|~s, ~z?
(m,n), ~w;?, ?, ?)
?
(c?
(m,n)sm,j+ ?)
?(c?
(m,n)j,wm,n+ ?)?Vt=1c?
(m,n)j,t+ V ?
?
(3)Here, c?miis the number of instances with sense i. c?mi,jis the number of concept j in instances with sensei.
Both of them are counted without the m-th instance of the target word.
c?
(m,n)sm,jis defined in a similarway with c?mi,jbut without counting the word position (m,n).
c?
(m,n)j,wm,nis the number of times word wm,nis assigned to concept j without counting word position (m,n).
fm,jis the number of concept j assignedto context words in instance m and fm,?is the total number of words in contexts of instance m. V standsfor the size of the word dictionary, i.e.
the number of different words in the data.
x is an index whichiterates from 1 to fm,?.~?, ~?iand ~?jcan be estimated in a similar way, we now only show as example the estimation of ~?i,parameters for sense-concept distributions.
According to their definitions as multinomial distributionswith Dirichlet prior, applying Bayes?
rule yields:p(~?i|~z;~?)
=p(~?i;~?)
?
p(~z|~?i;~?
)Z~?i= Dir(~?i|~ci+ ~?
)358where ~ciis the vector of concept counts for sense i.
Using the expectation of the Dirichlet distribution,values of ?i,jcan be worked out as follows:?i,j=ci,j+ ?
?Jk=1ci,k+ J ?
?Different read-outs of ?i,jare then averaged to produce the final estimation.4.2 The Non-parametric ModelChinese restaurant process (CRP) and Chinese restaurant franchise (CRF) process (Teh et al., 2006)have been widely used as sampling scheme for DP and HDP respectively.
As our non-parametric modelinvolves both DP and HDP, we use both CRP and CRF based sampling for model inference.In the CRP metaphor to DP, there is one Chinese restaurant with an infinite number of tables, each ofwhich can seat an infinite number of customers.
The first customer enters the restaurant and sits at thefirst table.
The second customer enters and decides either to sit with the first customer or by herself ata new table.
In general, the n + 1st customer either joins an already occupied table k with probabilityproportional to the number nkof customers already sitting there, or sits at a new table with probabilityproportional to ?.
As in our model, when we sample the sense smfor each context, we assume thattables correspond to senses of target words and customers correspond to whole contexts in which thetarget word occurs.In the CRF metaphor to HDP, there are multiple Chinese restaurants, and each one has infinitely manytables.
On each table the restaurant serves one of infinitely many dishes that other restaurants may serveas well.
At each table of each restaurant one dish is ordered from the menu by the first customer whosits there, and it is shared among all customers who sit at that table.
The menu is shared by all therestaurants.
To be specific to our model, when we sample the concept zm,nfor each context word, weassume each sense smof the target word corresponds to a restaurant and each word wm,ncorrespondsto a customer while concept zm,ncorresponds to the dishes served to the customer by the restaurant.Neither the number of restaurant nor the number of dishes is finite in our model.For model inference, we first sample smusing CRP-based sampling and then we sample zm,nfor eachsmusing CRF-based sampling.
The sampling of smand zm,nare done alternately, but not independently.The sampling of smis conditional on the current value of zm,nand vice versa, conforming to the schemeof Gibbs Sampling.The equation for sampling smis derived as in equation (4):p(sm= i|~s?m, ~z, ~w) ?{c?mi?
p(~zm|~z?m, sm= i) if i = old?
?
p(~zm|~z?m, sm= inew) elsewherep(~zm|~z?m, sm= i) =?Jj=1?fm,jx=1(c?mi,j+ ?
?c?mt,jc?mt,?+?+ x?
1)?fm,?x=1(?Jj=1c?mi,j+ ?
+ x?
1)(4)Here p(~zm|~z?m, sm= i) is estimated block-wise for context m according to the CRF metaphor.
c?miand c?mi,jare defined in the same way as that in equation (2).
c?mt,jis the number of tables with dish j inall restaurants but m and c?mt,?means the number of tables in all restaurants but m. x is an index whichiterates from 1 to fm,?.Sampling zm,nneeds more steps than sampling smas we need to record the table assignment for eachdish (concept).
For each dish zm,nof a customer wm,n, we first sample the table at which the customersits according to the following equations:p(tm,n= t|~t?
(m,n), ~z?
(m,n), wm,n, sm= i) ?{c?(m,n)i,t?
p?
(m,n)j(wm,n) if t = old?
?
p(wm,n|~t?
(m,n), tm,n= t, ~z?
(m,n), wm,n) elsewherep?
(m,n)j(wm,n) = p(wm,n|zm,n= j, ~w?
(m,n)) =c?
(m,n)j,wm,n+ ??Vt=1c?
(m,n)j,t+ V ?359Basic Model BNP?
1.0 0.2?
0.05 0.01?
0.05 0.2?
N/A 0.001K 0.27 N/AConcept number 20 N/AContext window ?
5 words ?
9 wordsTable 1: Hyperparamters of our modelsHere c?
(m,n)i,tis the number of customers on table t in restaurant i and c?
(m,n)j,wm,nhas the same meaning asin equation (3).
If the sampled table t is previously occupied, then zm,nis set to the dish j assigned tot according to the CRF metaphor.
If the sampled table t is new, the probability p(wm,n|~t?
(m,n), tm,n=t, ~z?
(m,n), wm,n) is calculated using equation (5), which is the sum of the probability of all previouslyordered dishes and the newly ordered dish.p(wm,n|~t?
(m,n), tm,n= t, ~z?
(m,n), wm,n) =J?j=1c?(m,n)t,jc?
(m,n)t,?+ ??
p?
(m,n)j(wm,n) +?c?
(m,n)t,?+ ??
p?
(m,n)jnew(5)Because a new table is added, we then sample a new dish for this table according to equation (6).p(zm,n= j|~t, ~z?
(m,n)) ?{c?(m,n)t,j?
p?
(m,n)j(wm,n) if j = old?
?
p?
(m,n)jnew(wm,n) if j = new(6)After the dish j is sampled, it is assigned to the new table and the number of table serving dish j is added.Parameters~?, ~?iand ~?jcan be estimated in the same way as described in section 4.1.5 Experiment5.1 Experiment SetupData Our primary WSI evaluation is based on the standard dataset in Semeval-2010 Word sense induc-tion & Disambiguation task (Manandhar et al., 2010).
The target word dataset consists of 100 words, 50nouns and 50 verbs.
There are a total number of 879,807 sentences in training set and 8,915 sentences intesting set.
The average number of word senses in the data is 3.79.Model Selection The trail data of Semeval-2010 WSI task is used as development set for parametertuning, which consists of training and test portions of 4 verbs.
The 4 verbs are different words than the100 target words in the training data.
There are only about 138 instances on average for each target wordin the training part of the trial data.
To make a development set of more reasonable size, the trial dataare supplemented with 6K instances of the 4 verbs extracted from the British National Corpus (BNC)1corpus.
As we use the Zipf?s law of meaning to guide the selection of number of senses, BNC was alsoused to count word frequencies.The final hyper-parameters are set as in Table 1.
In all the following experiments, Gibbs sampler isrun for 2000 iterations with burn-in period of 500 iterations.
Every 10th sample is read out for parameterestimating after the burn-in period to avoid autocorrelation.
Due to the randomized property of Gibbssampler, all results in the next sections are averaged over 5 runs.
The average running time for each targetword is about 7 minutes on a computer equipped with an Intel Core i5 processor working at 3.1GHz and8GB RAM.Pre-Processing For each instance of the target word in training data and testing data, all words arelemmatized and stop words like ?of ?, ?the?, ?a?
which are irrelevant to word sense distinction are filtered.Words occurring less than twice are removed.Evaluation method Semeval-2010 WSI task presents two evaluation schemes which are supervisedevaluation and unsupervised evaluation.
In supervised evaluation, the gold standard dataset is split into1www.natcorp.ox.ac.uk/360ModelSupervised Evaluation Unsupervised EvaluationAveraged #s80-20 split 60-40 split V-Measure Paired-FscoreBasic Model 64.12 63.68 11.52 44.42 5Basic Model + Zipf 66.4 65.25 15.2 35.12 7.66BNP 69.3 68.9 21.4 23.1 15.62Table 2: Test results with different configurations.Figure 4: Examples of concepts induced with the BNP model specific to the target word address.n (withcidenoting concept)a mapping and an evaluation parts.
The first part is used to map the automatically induced senses togold standard senses.
The mapping is then used to calculate the system?s F-Score on the second part.According to the size of mapping data and evaluation data, the evaluation results are measured on twodifferent splits which are 80-20 splits and 60-40 splits.
80-20 splits means that 80% of the test data areused for mapping and 20% are used for evaluation.
In unsupervised evaluation, the system outputs arecompared by using metrics V-Measure (Rosenberg and Hirschberg, 2007) and Paired F-Score (Artiles etal., 2009).5.2 Experiment ResultsTable 2 lists all experiment results.
The Basic Model stands for the parametric model with fixed numberof senses for all target words.
The number of senses is set to 5 which gives the best performance ondevelopment set.
Basic Model + Zipf is the model with the number of sense estimated by Zipf?s law ofmeaning.
BNP stands for our non-parametric model.
As we can see, compared with the Basic Model withfixed sense number, the model using Zipf?s law of meaning achieves improved performance.
This meansZipf?s law of meaning has positive effect in setting the sense number of the WSI task.
BNP achieves thebest performance on both supervised evaluation and V-measure evaluation.
In terms of Paired F-score,however, the Basic Model gets the best results while BNP performs worst.
This is consistent with whatclaimed by Manandhar et al.
(2010), that Paired F-score tends to penalize the model with higher numberof clusters.As stated before, our models not only perform word sense induction but also group the context wordsinto concepts.
Figure 4 shows 4 of the concepts induced by BNP with regard to the target word address.n.Senses of address.n are defined as the mixture of concepts and concepts are defined as distributions overcontext words.
We only list the top five words with the highest probabilities under each concept.
Asshown in Table 2, the non-parametric model induces much finer granularity of senses than the goldstandard, it makes distinction among email address, web address, and even ip address.
A possiblesolution is to further measure the closeness of senses based on the sense representations induced andmerge similar senses to produce coarser granularity of senses.361Model F-score(%)BNP+position 69.7BNP 69.3Basic Model + Zipf 66.4Basic Model 64.1HDP 65.8HDP+position (Lau et al., 2012) 68distNB (Choe and Charniak, 2013) 65.4UoY (Korkontzelos and Manandhar, 2010) 62.4Model F-score(%)BNP+position 88.0BNP 86.1HDP (Yao and Van Durme, 2011) 85.7HDP+position (Lau et al., 2012) 86.9Feature-LDA (Brody and Lapata, 2009) 85.51-layer-LDA (Brody and Lapata, 2009) 84.6HRG (Klapaftis and Manandhar, 2010) 87.6I2R (Niu et al., 2007) 86.8Table 3: Comparison with state-of-the-arts on Semeval-2010 data (left) and Semeval-2007 data (right)5.3 Comparison with previous workMuch previous work (Brody and Lapata, 2009; Klapaftis and Manandhar, 2010; Yao and Van Durme,2011) tested their models only on Semeval-2007 dataset (Agirre and Soroa, 2007) which consists ofroughly 27K instances of 65 target verbs and 35 target nouns, coming from the Wall Street Journalcorpus (WSJ) (Agirre and Soroa, 2007).
For a complete comparison, we also test our model on theSemeval-2007 dataset.
Since training data was not provided as part of the original Semeval-2007 dataset,we follow the approach of previous work (Brody and Lapata, 2009; Yao and Van Durme, 2011; Lau etal., 2012) to construct training data for each target word by extracting instances from the BNC corpus.Following paractices as much previous work (Brody and Lapata, 2009; Yao and Van Durme, 2011; Lauet al., 2012) did, we compare with previous work with supervised F-score on 80-20 data split in Semeval-2010 and noun data in Semeval-2007.Table 3 (left) compares our models against the state-of-the-art systems tested on 80-20 data split inSemeval-2010.
HDP+position (Lau et al., 2012) improved the HDP model (Yao and Van Durme, 2011)by including a position feature.
distNB (Choe and Charniak, 2013) extends the naive Bayes modelby reweighting the conditional probability of a context word given the sense by its distance to the tar-get word.
UoY (Korkontzelos and Manandhar, 2010) is the best performing system in Semeval-2010competition which used a graph-based model.
We re-implemented and tested the HDP model on theSemeval-2010 dataset since Yao and Van Durme (2011) and Lau et al.
(2012) did not report their HDPresults on this dataset.Different with normal practice in WSI work, there is no feature engineering in our model.
However,our BNP model outperformed all the systems on supervised evaluation.
Even the Basic Model outper-formed the best performing Semeval-2010 system.
Especially, our BNP model performs much betterthan the HDP model.
Both Lau et al.
(2012) and Choe and Charniak (2013) show benefit of using po-sitional information.
Since our model does not exclude further feature engineering, we also introduce aposition feature2into our non-parametric model (BNP+position) as in Lau et al.
(2012).
This contributesto a further 0.4% rise in performance.Table 3 (right) compares our models with previous work on the nouns dataset in Semeval-2007.
Wedivides systems being compared into two groups.
The first group model the WSI task with Bayesianframework, while the second group uses models other than Bayesian model.
Feature-LDA is the LDA-based model proposed by Brody and Lapata (2009) which incorporates a large number of features into themodel.
The 1-layer-LDA is their model with only bag-of-words features.
HRG is a hierarchical randomgraph model.
I2R is the best performing system in Semeval-2007.
As shown in Table 3 (right), ourBNP model with position feature (BNP+position) outperforms all systems.
If we restrict our attentionto the first group in which all models are Bayesian model, our BNP model without feature engineeringoutperforms the HDP model which is also non-parametric model without feature engineering.6 Related WorkA large body of previous work is devoted to the task of Word Sense Induction.
Almost all work relieson the distributional hypothesis, which states that words occurring in similar contexts will have similarmeanings.
Different work exploits distributional information in different forms, including context clus-tering models (Sch?utze, 1998; Niu et al., 2007; Pedersen, 2010; Elshamy et al., 2010; Kern et al., 2010),graph-based models (Korkontzelos and Manandhar, 2010; Klapaftis and Manandhar, 2010) and Bayesian2Formally, the position feature is the context words with its relative position to the target word.362models.
For Bayesian models, Brody and Lapata (2009) firstly introduced a Bayesian model to WSI task.They used the LDA-based model in which contexts of target word were viewed as documents as in theLDA model (Blei et al., 2003) and senses as topics.
They trained a separate model for each target wordand included a variety of features such as words, part-of-speech and dependency information.
Yao andVan Durme (2011) extended LDA-based model into non-parametric HDP model but removed the featureengineering.
Lau et al.
(2012) showed improved supervised F-score by including position feature to theHDP model.
Choe and Charniak (2013) proposed a reweighted naive Bayes model by incorporating theidea that words closer to the target word are more relevant in predicting the sense.Our model differs from the context clustering models and graph-based models, as it is a Bayesianprobabilistic model.
Our work also differs from the LDA-based models.
LDA topics were actuallyre-interpreted as senses of target word as Brody and Lapata (2009) applied the LDA to WSI tasks, sodid Yao and Van Durme (2011) and Lau et al.
(2012).
They induced word senses by firstly tagging(sampling) senses (of target words) to context words and selecting the mostly tagged sense as sense oftarget words.
Our model could be viewed as an extension of LDA, but fit the WSI task more naturallyand much better.
We distinguish senses of target words from concepts of context words and assume thatthey are separate.
Therefore, our model has two hidden layers corresponding to the sense of the targetword and the concepts of the context words respectively.
Basically, one decide the sense of the targetword based on the concept configuration of context words, instead of tagging senses of target word tocontext words.
The separation of senses of target word and concepts of context words is actually notonly required by linguistic intuition but also leads to improvement by our experiment.
Our model is alsodifferent from the naive Bayes model since our model induces senses of the target word at concept levelwhile naive Bayes model works at word level and does not involve conceptualization to context words atall.7 ConclusionIn this paper, we first proposed a parametric Bayesian generative model to the task of Word Sense Induc-tion.
It is distinct from previous work in that it introduces a layer of latent concepts that generalize thecontext words and thus enable the model to measure the sense similarity at a more general level.
We alsoshow in this paper that Zipf?s law of meaning can be used to guide the setting of sense numbers on anall-word basis, which is not only simple but also independent of the clustering methods being used.
Wefurther extend our parametric model to non-parametric model which not only simplifies the problem ofmodel selection but also bring improved performance.
The test results on the benchmark datasets showthat our model outperforms the state-of-the-art systems.AcknowledgmentsThis work is supported by National Natural Science Foundation of China under Grant No.
61273318and National Key Basic Research Program of China 2014CB340504.ReferencesEneko Agirre and Aitor Soroa.
2007.
Semeval-2007 task 02: Evaluating word sense induction and discriminationsystems.
In Proceedings of the 4th International Workshop on Semantic Evaluations, pages 7?12.
Associationfor Computational Linguistics.Javier Artiles, Enrique Amig?o, and Julio Gonzalo.
2009.
The role of named entities in web people search.
InProceedings of the 2009 Conference on Empirical Methods in Natural Language Processing: Volume 2-Volume2, pages 534?542.
Association for Computational Linguistics.David M Blei, Andrew Y Ng, and Michael I Jordan.
2003.
Latent dirichlet allocation.
the Journal of machineLearning research, 3:993?1022.Samuel Brody and Mirella Lapata.
2009.
Bayesian word sense induction.
In Proceedings of the 12th Conferenceof the European Chapter of the Association for Computational Linguistics, pages 103?111.
Association forComputational Linguistics.George Casella and Edward I George.
1992.
Explaining the gibbs sampler.
The American Statistician, 46(3):167?174.363Do Kook Choe and Eugene Charniak.
2013.
Naive Bayes word sense induction.
In Proceedings of the 2013Conference on Empirical Methods in Natural Language Processing, pages 1433?1437, Seattle, Washington,USA, October.
Association for Computational Linguistics.Phillip Edmonds.
2006.
Disambiguation, lexical.
Encyclopedia of Language and Linguistics.
Second Edition.Elsevier.Wesam Elshamy, Doina Caragea, and William H Hsu.
2010.
Ksu kdd: Word sense induction by clustering in topicspace.
In Proceedings of the 5th international workshop on semantic evaluation, pages 367?370.
Associationfor Computational Linguistics.Roman Kern, Markus Muhr, and Michael Granitzer.
2010.
Kcdc: Word sense induction by using grammaticaldependencies and sentence phrase structure.
In Proceedings of the 5th international workshop on semanticevaluation, pages 351?354.
Association for Computational Linguistics.Ioannis P Klapaftis and Suresh Manandhar.
2010.
Word sense induction & disambiguation using hierarchicalrandom graphs.
In Proceedings of the 2010 conference on empirical methods in natural language processing,pages 745?755.
Association for Computational Linguistics.Ioannis Korkontzelos and Suresh Manandhar.
2010.
Uoy: Graphs of unambiguous vertices for word sense in-duction and disambiguation.
In Proceedings of the 5th international workshop on semantic evaluation, pages355?358.
Association for Computational Linguistics.Jey Han Lau, Paul Cook, Diana McCarthy, David Newman, and Timothy Baldwin.
2012.
Word sense inductionfor novel sense detection.
In Proceedings of the 13th Conference of the European Chapter of the Associationfor Computational Linguistics, pages 591?601.
Association for Computational Linguistics.Jiwei Li and Claire Cardie.
2014.
Timeline generation: Tracking individuals on twitter.
In Proceedings of the23rd international conference on World wide web, pages 643?652.Jiwei Li and Sujian Li.
2013.
Evolutionary hierarchical dirichlet process for timeline summarization.
In Proceed-ings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),pages 556?560, Sofia, Bulgaria, August.
Association for Computational Linguistics.Suresh Manandhar, Ioannis P Klapaftis, Dmitriy Dligach, and Sameer S Pradhan.
2010.
Semeval-2010 task14: Word sense induction & disambiguation.
In Proceedings of the 5th International Workshop on SemanticEvaluation, pages 63?68.
Association for Computational Linguistics.Zheng-Yu Niu, Dong-Hong Ji, and Chew-Lim Tan.
2007.
I2r: Three systems for word sense discrimination,chinese word sense disambiguation, and english word sense disambiguation.
In Proceedings of the 4th Interna-tional Workshop on Semantic Evaluations, pages 177?182.
Association for Computational Linguistics.Ted Pedersen.
2010.
Duluth-wsi: Senseclusters applied to the sense induction task of semeval-2.
In Proceed-ings of the 5th international workshop on semantic evaluation, pages 363?366.
Association for ComputationalLinguistics.Andrew Rosenberg and Julia Hirschberg.
2007.
V-measure: A conditional entropy-based external cluster evalua-tion measure.
In EMNLP-CoNLL, volume 7, pages 410?420.Hinrich Sch?utze.
1998.
Automatic word sense discrimination.
Computational linguistics, 24(1):97?123.Yee Whye Teh, Michael I Jordan, Matthew J Beal, and David M Blei.
2006.
Hierarchical dirichlet processes.Journal of the american statistical association, 101(476).Xuchen Yao and Benjamin Van Durme.
2011.
Nonparametric bayesian word sense induction.
In Graph-basedMethods for Natural Language Processing, pages 10?14.George Kingsley Zipf.
1945.
The meaning-frequency relationship of words.
The Journal of General Psychology,33(2):251?256.364
