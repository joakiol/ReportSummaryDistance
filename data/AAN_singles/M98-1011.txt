NYU:Description of the Proteus/PET System as Used for MUC-7 STRoman Yangarber and Ralph GrishmanComputer Science DepartmentNew York University715 Broadway, 7thoorNew York, NY 10003, USAfroman|grishmang@cs.nyu.eduINTRODUCTIONThrough the history of the MUC's, adapting Information Extraction (IE) systems to a new class of eventshas continued to be a time-consuming and expensive task.
Since MUC-6, the Information Extraction eortat NYU has focused on the problem of portability and customization, especially at the scenario level.
Tobegin to address this problem, we have built a set of tools, which allow the user to adapt the system to newscenarios rapidly by providing examples of events in text, and examples of associated database entries tobe created.
The system automatically uses this information to create general patterns, appropriate for textanalysis.
The present system operates on two tiers: Proteus { core extraction engine, an enhanced version of the one employed at MUC-6, [3] PET { GUI front end, through which the user interacts with Proteus, (as described recently in [5, 6])It is our hope that the example-based approach will facilitate the customization of IE engines; we areparticularly interested, (as are other sites), in providing the non-technical user { such as a domain analyst,unfamiliar with system internals, { with the capability to perform IE eectively in a xed domain.In this paper we discuss the system's performance on the MUC-7 Scenario Template task (ST).
The topicscovered in the following sections are: the Proteus core extraction engine; the example-based PET interfaceto Proteus; a discussion of how these were used to accommodate the MUC-7 Space Launch scenario task.We conclude with the evaluation of the system's performance and observations regarding possible areas ofimprovement.STRUCTURE OF THE PROTEUS IE SYSTEMFigure 1 shows an overview of our IE system.1The system is a pipeline of modules, each drawing onattendant knowledge bases (KBs) to process its input, and passes its output to the next module.
The modulardesign ensures that control is encapsulated in immutable, domain-independent core components, while thedomain-specic information resides in the knowledge bases.
It is the latter which need to be customized foreach new domain and scenario, as discussed in the next section.The lexical analysis module (LexAn) is responsible for splitting the document into sentences, and thesentences into tokens.
LexAn draws on a set of on-line dictionaries; these include the general COMLEXsyntactic dictionary, and domain-specic lists of words and names.
As the result, each token receives areading, or a list of alternative readings, in case the token is syntactically ambiguous.
A reading consists of1For a detailed description of the system, see [3, 5]Figure 1: IE system architecturea list of features and their values (e.g., \syntactic category = Noun").
LexAn optionally invokes a statisticalpart-of-speech tagger, which eliminates unlikely readings for each token.The next three phases operate by deterministic, bottom-up, partial parsing, or pattern matching; thepatterns are regular expressions which trigger associated actions.
This style of text analysis, { as contrastedwith full syntactic parsing, { has gained the wider popularity due to limitations on the accuracy of fullsyntactic parsers, and the adequacy of partial, semantically-constrained, parsing for this task [3, 2, 1].The name recognition patterns identify proper names in the text by using local contextual cues, suchas capitalization, personal titles (\Mr.
", \Esq.
"), and company suxes (\Inc.
", \Co.
").2The next modulends small syntactic units, such as basic NPs and VPs.
When it identies a phrase, the system marks thetext segment with semantic information, e.g.
the semantic class of the head of the phrase.3The next phasends higher-level syntactic constructions using local semantic information: apposition, prepositional phraseattachment, limited conjunctions, and clausal constructions.The actions operate on the logical form representation (LF) of the discourse segments encountered so far.The discourse is thus a sequence of LFs corresponding to the entities, relationships, and events encounteredin the analysis.
A LF is an object with named slots (see example in gure 2).
One slot in each LF, named\Class", has distinguished status, and determines the number and type of other slots that the object maycontain.
E.g., an entity of class \Company" has a slot called \Name".
It also contains a slot \Location"which points to another entity, thereby establishing a relation between the location entity and the matrixentity.
Events are specic kinds of relations, usually having several operands.The subsequent phases operate on the logical forms built in the pattern matching phases.
Referenceresolution (RefRes) links anaphoric pronouns to their antecedents and merges other co-referring expressions.The discourse analysis module uses higher-level inference rules to build more complex event structures, where2At present, the result of the NYU MENE system, as used in the NE evaluation, does not yet feed into the ST processing.3These marks are pointers to the corresponding entities, which are created and added to the list of logical forms representingthe discourse.Slot ValueClass SatelliteName {Manufacturer Loral Corp.Owner Intelsat: : : : : :Figure 2: LF for the NP: \a satellite built by Loral Corp. of New York for Intelsat"the information needed to extract a single complex fact is spread across several clauses.
For example, there isa rule that merge a Mission entity with a corresponding Launch event.
At this stage, we also convert all dateexpressions ("yesterday", "last month", etc.)
to starting and ending dates as required for the MUC templates.Another set of rules formats the resultant LF into such a form as is directly translatable, in a one-to-onefashion, into the MUC template structure, the translation performed by the nal template-generation phase.PET USER INTERFACEOur prior MUC experience has shown that building eective patterns for a new domain is a complexand time-consuming part of the customization process; it is highly error-prone, and usually requires detailedknowledge of system internals.
With this in view, we have sought a disciplined method of customization ofknowledge bases, and the pattern base in particular.Organization of PatternsThe pattern base is organized in layers, corresponding to dierent levels of processing.
This straticationnaturally reects the range of applicability of the patterns.
At the lowest level are the most general patterns;they are applied rst, and capture the most basic constructs.
These include the proper names, temporalexpressions, expressions for numeric entities, and currencies.
At the next level are the patterns that performpartial syntactic analysis (noun and verb groups).
These are domain-independent patterns, useful in a widerange of tasks.
At the next level, are domain-specic patterns, useful across a narrower range of scenarios,but still having considerable generality.
These patterns nd relationships among entities, such as betweenpersons and organizations.
Lastly, at the highest level will be the scenario-specic patterns, such as the clausalpatterns that capture events.Proteus treats the patterns at the dierent levels dierently.
The lowest level patterns, having the widestapplicability, are built in as a core part of the system.
These change little when the system is ported.
The mid-range patterns, applicable in certain commonly encountered domains, are provided as pattern libraries, whichcan be plugged in as required by the extraction task.
For example, for the domain of \business/economicnews", Proteus has a library with patterns that capture: entities { organization/company, person, location; relations { person/organization, organization/location, parent organization/subsidiary.Lastly, the system acquires the most specic patterns directly from the user, on a per-scenario basis,through PET, a set of interactive graphical tools.
In the process of building the custom pattern base, PETengages the user only at the level of surface representations, hiding the internal operation.
The user's inputis reduced to providing examples of events of interest in text, and describing the corresponding output structures to be created.Companyz }| {Arianespace Co.vgz }| {has launchedsatellitez }| {an Intelsat communications satelliteFigure 3: Initial analysisBased on this information, PET automatically creates the appropriate patterns to extract the user-specied structures from the user-specied text suggests generalizations for the newly created patterns to broaden coverage.Pattern AcquisitionThe initial pattern base consists of the built-in patterns and the plugged-in pattern libraries correspondingto the domain of interest.
These serve as the foundation for example-based acquisition.
The developmentcycle, from the user's perspective, consists of iteratively acquiring patterns to augment the pattern base.
Theacquisition process entails several steps:Enter an example: the user enters a sentence containing a salient event, (or copies-pastes text from adocument through the corpus browser, a tool provided in the PET suite).
We will consider the example\Arianespace Co. has launched an Intelsat communications satellite.
"Choose an event template: the user selects from a menu of event names.
A list of events, with theirassociated slots, must be given to the system at the outset, as part of the scenario denition.
This examplewill generate an event called \Launch", with slots as in gure 4: Vehicle, Payload, Agent, Site, etc.Apply existing patterns: the system applies the current patterns to the example, to obtain an initialanalysis, as in gure 3.
In the example shown, the system identied some noun/verb groups and theirsemantic types.
For each element it matches, the system applies minimal generalization, (in the sense that tobe any less general, the element would have to match the example text literally).
The system then presentsthe analysis to the user and initiates an interaction with her:np(C-company) vg(Launch) np(Satellite)Tune pattern elements: the user can modify each pattern element in several ways: choose the appropriatelevel of generalization of its concept class, within the semantic concept hierarchy; force the element to matchthe corresponding text in the original example literally; make the element optional; remove it; etc.
In thisexample, the user should likely generalize \satellite" to match any phrase designating a payload, and generalizethe verb \launch" to a class containing its synonyms, (e.g.
\re"):np(C-company) vg(C-Launch) np(C-Payload)Fill event slots: the user species how pattern elements are used to ll slots in the event template.
Clickingon an element displays its logical form (LF).
The user can drag-and-drop the LF, or any sub-componentthereof, into a slot in the target event, as in gure 4.Build pattern: when the user \accepts" it, the system builds a new pattern to match the example, andcompiles the associated action; the action will re when the pattern matches, and will ll the slots in theevent template as in the example.
The pattern is then added to the pattern base, which can be saved forlater use.Syntactic generalization: Actually, the pattern base would acquire much more than the basic pattern thatthe user accepted.
The system applies built-in meta-rules [1, 4], to produce a set of syntactic transformationsSlot ValueClass Predicate-LaunchAgent entity =) <Arianespace>Payload entity =) <Intelsat satellite>Site : : :: : : : : :Figure 4: Event LF corresponding to a clausefrom a simple active clause pattern or a bare noun phrase.
For this, active example, the pattern base willautomatically acquire its variants: the passive, relative, relative passive, reduced relative, etc.4Proteusalso inserts optional modiers into the generated variants { such as sentence adjuncts, etc., { to broaden thecoverage of the pattern.
In consequence, a passive pattern which the system acquires from this simple examplewill match the event in the walk-through message, \... said Televisa expects a second Intelsat satellite to belaunched by Arianespace from French Guyana later this month ...", with the help of lower-level patterns fornamed objects, and locative and temporal sentence adjuncts.5PERFORMANCE ON THE LAUNCH SCENARIOScenario PatternsThis section describes how the Proteus/PET system was adapted to accommodate the MUC-7 scenario.The scenario-specic patterns were primarily of two types: those for launch events (\NASA launched arocket.
", \The terrorists red a missile.")
and those for missions (\the retrieval of a satellite").
Starting frompatterns for simple active clauses, the system automatically generated patterns for the syntactic variants, suchas the passive, relative, and reduced relative clauses.
The missions added information regarding payloads andmission functions to a launch event, but did not directly generate a launch event.
In some cases, the missionwas syntactically tied to a particular launch event (\... launched the shuttle to deploy a satellite").
If therewas no direct connection, the post-processing inference rules attempted to tie the mission to a launch event.Inference RulesConsider the event in gure 4: the surface representation contains a generic \Agent" role.
The agent can beof several types, e.g.
it can be a launch vehicle, an organization, or even a launch site, in case the agent is acountry.
In this case, the role is lled by an organization, which, in principle, further admits the possibilityof either the payload owner or the vehicle owner.
The scenario specication mandates that the function of the\agent" be precisely specied, although at the surface it is underspecied.
In this case, the function can bedetermined on the basis of the semantic class of the agent, and the observation that the payload-owner slotis already occupied unambiguously by another organization entity.This type of computation is performed by scenario-specic inference rules; in general, this determinationcan be quite complex.
Translating the surface representations into those mandated by the task specicationcan involve many-to-many relations, such as ones that exist between payloads and launch events, wheremultiple payloads correspond to a single event, and multiple launch events are concerned with a singlepayload.One technique that appeared fruitful in the Launch scenario was extending our set of inference rules withheuristics.
Often a slot in an event cannot be lled, as when patterns fail to nd a syntactically suitable4The expert user can view the variants which the system generates, and make changes to them directly.5The tools can be used to acquire non-clausal patterns as well, e.g.
patterns for noun groups and complex noun phrases, toextend an existing pattern library.Task Template Element Scenario TemplateRecall 71 31Precision 83 68F-Measure 76.50 42.73Figure 5: NYU scores on MUC-7 taskscandidate.
The idea was to make intelligent guesses about llers for these empty slots.
For example, considerthe rst sentence of the walk-through message:Xichang, China, Feb. 15 (Bloomberg) { A Chinese rocket carrying an Intelsat satellite explodedas it was being launched today, delivering a blow : : :Here we nd two similar problems: concerning the launch date and the launch site.
Our patterns recognizethe corresponding locative and temporal noun phrases, however, because neither stands in a direct syntacticrelation to the main launch event clause (here, headed by the verb \explode"), they fail to ll the appropriateslots in the event.
We use a simple heuristic rule to recover from this problem: if the launch event has anempty date, and if the sentence contains a unique expression of the correct type (i.e.
date), use the expressionto ll the empty slot.We have experimented with a variety of heuristics for several slots, including organizations for vehicle andpayload owners and manufacturers, dates and sites.
At present, the contribution of these heuristics to ourscore accounts for just under 10% of the F-measure.
It is also apparent that some of the heuristics actuallyovergenerate, though we have yet to analyze their eect in detail.We believe that the overall approach of example-based pattern acquisition is more appropriate thanautomatic training from annotated corpora, as the amount of training data for ST-level tasks is usually quitelimited.
We have found pattern editing tool reasonably eective.
However, we discovered that much of thetask involved creation and tuning of post-processing rules and we do yet not have support in the tool for thisactivity.
This consumed a considerable part of the customization eort .
This points to an important problemthat needs to be addressed, especially for tasks where the structure of output templates diers substantiallyfrom the structure of entities and events as picked up by the syntactic analysis.We did not specically focus on the TE task within the launch scenario, and simply used the same systemwe had used for the ST task.
Table 5 is a summary of the scores of our system.AcknowledgementWe wish to thank Kristofer Franzen of Stockholm University for his assistance during the MUC-7 formalrun.REFERENCES[1] Douglas Appelt, Jerry Hobbs, John Bear, David Israel, Megumi Kameyama, Andy Kehler, David Martin,Karen Meyers, and Mabry Tyson.
SRI International FASTUS system: MUC-6 test results and analysis.
InProc.
Sixth Message Understanding Conf.
(MUC-6), Columbia,MD, November 1995.
Morgan Kaufmann.
[2] Douglas Appelt, Jerry Hobbs, John Bear, David Israel, and Mabry Tyson.
FASTUS: A nite-state proces-sor for information extraction from real-world text.
In Proc.
13th Int'l Joint Conf.
Articial Intelligence(IJCAI-93), pages 1172{1178, August 1993.
[3] Ralph Grishman.
The NYU system for MUC-6, or where's the syntax.
In Proc.
Sixth Message Under-standing Conf., pages 167{176, Columbia, MD, November 1995.
Morgan Kaufmann.
[4] Ralph Grishman.
The NYU system for MUC-6 or where's the syntax?
In Proc.
Sixth Message Under-standing Conf.
(MUC-6), Columbia, MD, November 1995.
Morgan Kaufmann.
[5] Ralph Grishman.
Information extraction: Techniques and challenges.
In Maria Teresa Pazienza, editor,Information Extraction.
Springer-Verlag, Lecture Notes in Articial Intelligence, Rome, 1997.
[6] Roman Yangarber and Ralph Grishman.
Customization of information extraction systems.
In Paola Ve-lardi, editor, Proc.
International Workshop on Lexically Driven Information Extraction, Frascati, Italy,July 1997.
Universita di Roma.
