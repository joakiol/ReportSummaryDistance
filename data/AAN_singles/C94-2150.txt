\?c fldy 17{.c, trlct, c.d St,()chasti(: (;;rammars:Ricks op den Akker ~nd Itugo tcr DoestUniversity o:\[ Tw('ntc, l)epartment of CompuSer ScienceP.O.Box 217, 7,500 A\]i', Ensch(;de, The NetherlandsKeywords: stochastic languages, grammars, grammarinference.AbstractA new type of stochatstic grammars i introduced tor inw~s-tigation: weakly restricted stochastic grammars.
In thisp~Lper we will concentrate oil the consistcl,cy prol)lcnt.
~1'ofind conditions for stoch~stlc gramma.rs to be consistm~t,the theory of multitype Galton-Watson branciting pro-cesses and gelmrating functions is of central ilnporl,~utce.The unrestricted stochastic grammar formalism generatesthe s~tme class of langu~gcs as the wc~kly rcstricl.cd formalisln, q'}te inside-outside algorithm is adapt.cd for usewith weakly restricted granlmars.1 Introduct ionl\[' W( COilSJdcr ~ natura l  l t tnguages  as a sLr~,lcLtire lno( l -elled by a formal grammar we do not consider il anymore as a language thal.
is used.
Formal (context-free) grammars are often advocated as a model lbrl, he "linguistic competence" of an ideal }tal;ural ~m-guage user.
It is also 1,oticed that this mat}mmaticalconcept is far froln at su{Iicieut model for describing allaspects of the language.
What cannot be expressed bythis model is tile fact thai.
some sentences or phrasesare more likeley to occur than others.
'\]'his notion ofoccurrence refers to the use of language and thereR)reconsidering this kind of statistical knowledge aboutlanguage has to do with the pragmatics of languagelaid down in a corpus of the language.
With a partic-ular context of use in mind a syntactically ambiguoussentence will often have a most, likely meaning andhence a most likely mlalysis.
Some of the shortcom-ings of the pure (context-free) grammar model canmaybe be solved by stochastic gralmnars, a modelthat makes it possible to incorporate certain statisti-cal facts about the language nse into a model of thepossible structures of sentences as we conceive themfrom a mathematical, formal, point of view.
Naturallanguages are now seen as stochastic; a user of a lan-gnage as a stochastic source producing sentences.
Astochastic language over some alphabet E is simply aformal language L over i3 together with a probabilityfunction 05 assigning to each string a: in the languagea real mn-nber 05(a,) in \[0, 1\].
Since 05(a:) is interpretedas the chance that tile event x, or the event that alanguage-source produces x, will occur, it will be clearthat the sum of 4)(x) where x ranges over all possiblesentences i equal to one.
Tile stochastic language iscalled context free if tile language L is context-free.The usual grammatical model for a stochasticcontext-free language is a context-free gramm~u' I,o-gcther with a probability function f that assigns areal mnnber in \[0, 1\] to each of the productions of thegrammar.
The Ineaning of this Nnction is the follow-int.
A step in a derivation of a sententia\] form, inwhich a nonterminal A is rewritten using productionp has chance f(p) to occur, independent of which Ais rewri{,ten in the sentential form and indepelidentof the history of the proces l, hat produced the sen-tential form.
The probability of a derivation(- tree) isthe product of the probabilities of bile derivation stepsthat produces the tree.
The probability of a sentencegenerated by tin, gramnm.r is the sum of the probabili-ties of all the.
trees of' a sentence.
So given a stochasticgrammar we can compute the probabilities of all itssentences.
The distribution language generated by astochastic grammar G, I)L(G), is defiued as the seto\[' all deriw~tion trees with their probabilities.
'Chestochast, ic language generated by a stochastic gram-mar (7, HL(G), is defined as the set of all sentencesgenerated by the grammar with their probabilities.A stochastic gr~mmlar G is an adequate model of alanguage L if on its basis we can correctly computetile probabilities of the sentences in the hmgnage L.Of course this assumes a statistical analysis of a lan-guage corpus.
A stochastic grammar that generates astochastic language is called consistent.Def in i t ion  1.1 A stochastic grammar G is calledconsistent if for the probability measure p reduced b?l(; onto the laT~g~agc enerated by its underlying ram-Otherwise the grammar is called inconsistent.Not all stochastic grammars generate a stochastic lan-guage.
Even proper, and reduced grammars 1 do not1A gran lmed is culled proper if for all nontcrminals  A, talcstlnl of the probabil it ies assigned to the rules for A is 1.
Agrammar  is c~dled reduced if all nont.erminals are reachable andcan produce a termina l  st, ring.929necessarily generate a stochastic language.
This isi l lustrated in the following example.
"Example  1.1 Consider the stochastic grammar Gwith nonterminal set VN = {S}, terminal set V'r ={a}.
The productions with their probabilities aregiven by:S ~ SSS ---' aFollowing the technique presented in \[2\] we findthat the production generating function is given bygl(St ) = qs~ + 1-  q, and that, the first moment matr ix/2 is given by \[2q\].
We can conclude that the gram-mar is consistent if and only if q _< 1/2.
For details werefer to \[5\].
No~ice thai, all the different rees of stringa r~ have Lhe same probability.
Hence, they cannot bedistinguished according to their probabilities.
\[\]It has been noticed that the usual model of a stoch-astic grammar as presented above, and which wefrom now on call the unrestricted stochastic gram.marmodel, has some disadvantages for modelling "real"languages.
In this paper we present a more ade-quate model, the weakly restricted stochastic gram-mar model.
We give necessary and sufficient con-ditions to test in an efficient way whether such agrammar defines a stochastic language.
Moreover, wewill show that these grammars can be transformedinto an equivalent model of the usual type.
The nicething about the new model is that it models "context-dependent" probabilities of production-rules directlyin terms of the grammar specification of the languageand not in terms of some particular implementationof the grammar as a parser.
The latter is done byBriscoe and Carroll \[3\] by assigning probabilities tothe transitions of the LR-parser constructed for thegrammar.
In section 2 weakly restricted grammarsare introduced, in section 3 conditions for their con-sistency are investigated; in section 4 it is proven thatweakly restricted grammars and unrestricted gram-mars generate the same class of stochastic languagesand section 5 presents the inside-outside algorithm forweakly restricted grammars.2 Weakly Restricted StochasticGrammars~Ib add context-sensitivity to the assignment of proba-bilities to the application of production rules, we takeinto account (and distinguish) the occurrences of thenonterminals.
Then, for each nonterminal occurrencedistinct probabilities can be given for the productionrules that can be used to rewrite the nonterminal.This way of assigning probabilities to the application2Although we found in \[7\] by Jelinek and Lafferty the (false)s ta tement  that  a stochast ic  g rammar  is cons istent  if and  onlyif it is proper, given that the underlying rammar is reduced.The example gives a clear counter example of their  s ta tement .of production rules seems unknown in literature, al-though we found some other fornlalisms that were de-signed to add context-sensitivity to the assignment ofprobabilities.
For instance, the definition of stoch-astic grammars by Salomaa in \[8\] is somewhat differ-ent from the definition we gave in our introduction:the probabil ity of a production to be applied is heredependent on the production that was last applied.To escape the bootstrap problem (when a deriva-tions is started, there is no last applied production)an initial stochastic vector is added to the grammar.Weakly restricted stochastic grammars are introducedin \[1\].
In the following definition Ca, denotes the setof productions for Ai and l~(Ai) denotes the numberof r ight-hand side occurrences of nonterminal A~.Def in i t ion  2.1 A weakly restricted stochastic gram-mat" Gzv is a pair" (Cc, A), where Cc = (VN, ~,},, P, X)is a conlex#free flrammar and A is a set of functionsA = {p~lA~ C VN}where, if j E 1 ...t~(Ai) and k E 1...\[CA,I, pi(j,k) =Pij~" G \[0, 1\] The set of productions P contains ezacilyone produclion for start symbol S.In words, Plj~ stands for the probability that the k-thproduction with left~hand side Ai is used for rewrit-ing the j - th  right-hand side-occurrence of nonterminalAi.
The usefullness of this context-dependency canbe seen immediately fi'om the following unrestrictedstochastic grammar, which is taken (in part) from theexample grammar in \[3\] (p. 29):S ~ NP VPVP o~ Vt NPN P o~ \[5,N PNP Q Dd NNP ~ NP PPUnrestricted stochastic grammars cannot model con-text dependent use of productions.
For example, anNP is more likely to be expanded as a pronoun in sub-ject position than elsewhere.
Exactly this dependenceon where a nonterminal was introduced can be mod-eled by using a weakly restricted stochastic grammar.Since in a weakly restricted stochastic grammar theprobabilities of applying a production are dependenton the particular occurrence of a nonterminal in theright-hand side of a production, it is useflfl to requirethat there is only one start production.The characteristic grammar of a weakly restrictedgrammar is the underlying context free grammar.The next step is to compute probabilities for stringswith respect to weakly restricted stochas~aic gram-mars.
For this purpose a tree is written in termsof its subtrees (trees with a nonterminal as root) asq\[tiljl, f.i2j~, ?
.., ti,~(q)j,(q)\], in which q is a production,n(q) is the number of nonterminals in the right-handside of q and tij denotes a (sub)tree with the j - th930occurrence of' nonterminal Ai at its root.
A tree forwhich n(q)---0 is written as \[\].Def in i t ion  2.2 The probabilily of a derivation tree twi~h respect o o weakly reslriclcd s~ochastie grammaris deflated r'ecurswely a,~(q)m .~ ,n m3, , , )rn=lwhere 1 < k,,~ < \ [CA , , .
\].The probability of a string is deiined as the sun: ofthe probabilities of all distinct derivation trees ~hatyield this string.l )e f in i t ion  2.3 73e probability of a str ing x Z~lL(G~) is defined asThe distribution langnage \])L(Ow) and stochasticlanguage ,5'/;((\]~) of a wealdy restrieted gramuxar((;~,A) are detined ana\]oguous to :;l~e distributionlanguage and stochastic language of an nnrestrictedgran l l i l a r .3 Cons is tencyIn this section consistency of we~kly rest, rict, ed stoch-astic grammars will be considered.
The theory ofnmlt iwpe hranching processes will be used to cometo a similar theorem as is given in \[2\] for unrestrictedstochastic grammars.Def in i t ion  3.1 l~or the j-th occurrence of ~ontermi-hal Ai ~ VN the production generating fnnetAon \]orweakly restricted stochaslic grammars is defined as:9i~(~1,1, ...,sk,It(A~)) =ICAil k t~(Am)u=l  m~:\ ]  n : .
: lwhere r ,m(k) is 1 if nonlerminal-occurrence A .... ap-pears in the righ1-hand side of the k-lh production rulewilh nonIerminal Ai as left-hand side and 0 otherwise.Note that for each right-hand side nonterminal oc-currence a dummy-variable is introduced: sij corre-sponds to the j - th  occurrence of nonterminal Ai.
Aspecial variable is Sl,:: it corresponds to the s~artsymbol which is the right-hand side of the start pro-duction s ~ P of the form Z ~ S, The genera-~ing function for nonterminal occnrrenee A0 entailsfor each production for Ai a term.
If 91j has a termof the formO~Si: Si  2 .
.
.
Si, ,then we know thai it corresponds to a prodnetion forAi of the fbrmA i -+  : t : i :A i l  X i2g i  ~ .
.
.
zi,~Ai,,:ci,,+~where the .~:ij ~ 1.4\[.
The production has, if it is usedfor rewriting oceurrence Aij, probability ct of beingapplied.
In Example 3 it will be illustrated how theterms of t~he genera.tAng timctions correspond to theproductions of the grammar.Theorem 3.1 Let Aq =~ ct," thus ~he j-th oecurre~.ccof nonlerminal Ai is re.written using ezactly one pro-dTtclion.
73c probability tha~ (~ contains lhe ,..th oc-currence of nontermznal Am is given byc~gii(Sl,t .
.
.
.
.
st',tt(Ak))Proof h~ general ~he generating function can be writ-ten  as9 i j (S IA , .
.
.
,Sk , I~(A~))  =: 91 j (S t ,1 , .
.
.
,Sk~/{(A~)) - I  e l iwhere g l j ( s l j , , .
.
,  s~,Jt(a~)) only contains terms de-pendent on s l j , .
.
.
,sk,la(A~) and where eij is a (;Oll-stant~ term.
The terms dependent on S:,l, .
.
.
,  S~.,R(Ak)come from productions for Ai that contain nontermi..nals in their right-h~md sides and the constant ermsfl'om produetAons for Ai that only contain terminalsin their right-hand sides.
When partial derivativesare taken from 9ij we can just as well consider .qlj,since the constant erm will become zero.
Wc knowthal.
the terms in g~j do not contain any powers higherthan 1. of the variables in it.
This leads us to the in-sight, that taking the mn-th partial derivative of .qijresults in at most one term consisting of the formpo,f (s i , :  .
.
.
.
, s#,/~(Ak)) where f does not depend ons ..... and Pij, is one of the probabilities resulting fromapplying Pi to j and some h in 1 .
.
.
\]('~fA,\].
If we substi-.lute i for all remaining variables in the partial deriw~-live we find as value for eijmn the probability that thej - th  occurrence of nor,terminal Ai is rewritten by theproduction that contains in its right-hand side non-terminal occur rence  A ,7  m .
DThe first-moment matr ix for weakly restricted gram-mars is defined just like the first-moment matr ix forunrestricted grammars:Def in i t ion  3.2 The f irst-moment matrix E associ-ated wzth the weakly restricted grammar G is/; = \[~u,,-dWe order the set of eigenvalues of the first-momentmatrix from the largest one to the smallest, such thatP: presenl,s the maximum.Theorem 3.2 A proper weakly restric~ed grammar isconsistenl if pl < 1 a'nd is nol consisZcnl if pj > 1931The proof of this theorem is analoguous to the proofof the related theorem in \[2\] and we will not trea.t ithere (see \[5\] for a proof).Example  3.1 Consider the weakly restricted stoch-astic grammar (G~, A) where G~ = (VN, VT, P, Z) =((Z, S}, (a}, P, Z) and P ~md A are as follows:z -~ s (p, 1 - p)s -~ s s (q, ~ - q ) ( r ,  1 - r )For a reason at.
the of the example to become clear,we assume that p ?
0.
The production generatingfunctions are given bygll(s11, st2,sla) = ps12s13 + 1 - pg12(S11~ Sl2~ 813) "~-- qs12813 @ 1 - qg13(S11, 812,813) ---~ rs12813 + 1 - rThe first-moment matrix E is given t)y0 q q0 7' rThe characteristic equation is given by ?
(x) = x((x -q)(q-- r ) -q r )  = x2(x -  (q+r))  = 0.
Thus, the eigen-values of the matrix are 0 and x = q + r. Accordingto Theorem 3.2 the grammar is consistent if q + r < 1and inconsistent i fq+r  > I. I fq+r  = 1 the theo-.rein does not decide tile consistency of the grammar.From the characteristic equation it follows that thevalue of p does not influence the consistency of thegrannnar.
However, looking at the gramnrar we findthat it is consistent if p = 0, regardless of probabil-ities q and r. Therefore, before Theorern 3.2 can beused for checking the consistency of tt~e grammar, thegrammar must be stripped of productions having foreach nonterminal occurrence probability zero of beingapplied.
\[\]Def in i t ion  3.3 A final class C ofnonterminal occur-rences is a subset of tile set of all nontcrminal occur-fences having tile property that any occurrence in Chas probability 1 of producing, when rewritten usingone production rule, exactly one occurrence also inC.Theorem 3.3 A weakly restricted s~ochastic gram-mar is consistent if and only if Pl <_ 1 and there areno .final classes.For the proof of Theorem 3.3 we refer to \[5\].
Ap-plying this theorem to the example learns us that ifq + r = 1, the grammar is consistent if and only ifthere is no final class of nonterminals.
Looking at thegrammar we see that there is a final class of occur-rences ifq = 1 or r = 1 (or both); the final classes thenare {S2},{Ss} and {$2, $3}, respectively; if in addi-tion p = 1, then the final classes are {S1, S2},{S1,  $3}and {$1,$2, $3}, respectively.
Hence, the grammar isconsistent if and only if q + r < 1 A q ?
1 A r ?i 1.Notice that if q 7~ r then all trees of a ~ have difi~rentprobabilities.4 EquivalenceIn this section we will show that a weakly restrictedstochastic grammar can be transformed into an equiv-alent unrestricted grammar.
We define two grammarsG and H to be equivalent if DL(G) = DL(II).The transformation is pertbrmed as follows.
Witheach nonterminal occurrence Aij in the right-handside of a production rule associate a new unique non-terminal Aij; for each new nonterminal Aij  copy theset of production rules with nonterminal Ai as left-hand side, replace the left-hand sides with Aij  andreplace in the right-hand sides each nonterminal withits new (associated) nonterminal; assign probabilityPijk to the k-th production rule with left-hand sideAij.
We formalized this in the following algorithm.A lgor i thm 4.1Associate with the j-th occurrence of nontermi-nal Ai in the right-hand sides of tile productionrules a (new) unique nonterminal Aij (clearlyj ~ I , .
.
.
,R(A~)) .
\[\['he set ofnonterminals forthe rewritten grammar C' is denoted by V/~ andis the set of associated nonterminals plus thestart symbol S from the we~fldy restricted gram-mar  G.2 This step is given in pseudo-pascahfor i :=  1 to IVNI dofor j := \] to t~(Ai) do\[" := _P' U CA, (j)ododKlwhere CA,(j) is the set of productions CA, withleft-hand sides Ai replaced by Aij and the nonter-minals in tile right-hand sides of the productionrules replaced by their associated nonterminals.The probabilities to be assigned to tim produc-tion rules in CA,(j) are deduced from the Ply -~(Pijl,.. ",PijlCAil): the \]c-th production rule inCA,(j) is assigned probability pij;:.Theorem 4.1 For every weakly restricted stoch-astic grammar there is an unrestricted stochasticgrammar which, is distributively equivalent.Proof We can prove the theorem by proving thatthe algorithm finds for every weakly restricted gram-mar an unrestricted grammar that is distributivelyequivalent.
From the algorithm it immediately fol-lows that the languages (without the probabilities)generated by the weakly restricted grammar and theunrestricted grammar generated by the algorithm areequal.
The production rules introduced by the al-gorithm in the unrestricted grammar cannot gener-ate any other strings than the string generated by932tile weakly restricted gr~mnnar.
Also it.
cart be seenl\[rom tile algorithm that the unrestricted grammar as-sociates the same probabilities with its strings as tt~eunrestricted grammar.
IIence, the theorem holds.
\[\]A corollary of this theorem is that %r each weak\]yrestrict, ed grammar there exists an unrestricted gram-mar that is stochaslically equivalent.q'he time-complexity of the algorithm can easily befound.
YWe obserw~ that, if we denote the number ofnonterminals in the weakly restricted grammar by k,each step can be done in in O(k) steps.
Then thetotal t ime complexity is O(k).
We, deiine the size ofa grannriar to be tile product of the number of non-terminals and the nmnber of productions.
The size ofthe newly created grammar c~al be found to he poly-nomial in the size of tile weakly restricted gralnm~u'.5 In fe renceThe inside-outside algorithm is originally a reestilna-Lion procedure for the rule probabilities of an un-restricted stochastic grammar in Chomksy NormalForm (CNF) \[4\].
It, takes as input an initial m~re-stricted stochastic grammar (; in CNF and a sam-pie set b7 of strings and it itcralJvely reestimates ruleprobabilities to ma~ximize the probability that thegrammar would produce the samt)le set,.The basic idea of" the inside-outside algorithm is l,otlse the cllrrent rl.tle probabilities to cstirnate from thesample set the expected frequencies of certain deriva-tion steps, and them compute new rule probabilityestimates as appropriate frequency rates.
Therefore,each iteration of the algorithm starts by c~deulatingthe inside and outside probabilities for all strings inthe sample set.
These probabilities are.
ill fact.
prob-ability functions which haw~ as arguments a stringw from the sample set, indexe~ which inclicate whatsubstring of w is to be considered, and an occurrenceof a nonterminal, say A.
With i;hese arguments, theinside proi~abiliw now is the probability that the oc-currence of A derives the substring of w; the oulsideprobability is the probabil ity that the occurrence ofnonterminal A appears in the intermediate string ofsome deriw~tion of string w.In what follows, we will take I@,V7, as tixed n =Iv~l ,  ~ - IV r l ,  and ass.n,e that  VN :- {z  -- A~,,,S' =A1,A2,.
.
.
,A,~} and l/!t, = {a\] .
.
.
.
,ct~}.
By definitionit is required t, hat the grammar has one production forstart, symbol Z: Z -+ ?'.
Parallel to the definition ofgenerating fnnctions for weakly restricted grammars,we have to d ist inguish all nontermina l  occurrences inr ight-hand sides of productions; we remind that theprobahility of each production depends on the par:ticular nonterminal occurrence to be rewritten.
Theinside and outside t>rohabilities now have to he spec:ilied for ea.ch nonterminal occurrence seperately.
Asalready stated in the introduction, the inside-outsidealgorithm is designed only for context-free grammarsin CNI i'.
Using this fact we can sirnplify the way non-terminal occurrences are indexed: A,ffp.,.)
(A,.(vq.))
de-notes the occurrence of ./lq (At) ill the productionAp --* AqA,.
; for this production also the notation(pqr) is used and for the production Ap---~ aq (pq).Similarly the probabil ity of occurrence Aq(p.r) to berewritten using rule (qst) is denotes by Pq(p.r)(q~t).For the start production a special provision has to hetaken: the norlterminal occurrence in its right-handside is denoted by Z~l (0 .
. )
.
A stochastic grammar inCNF over these sets can then be specified bytg (Ad l l f liprobabilities.
Since wc require stochastic gr~mmarsto he proper, we know theft for p, q, r = l , .
.
.
,  ~,~_.p,~(>,.
)(q~.t) t- )_~ PqO,.,)(,.)
= 1.s, t sI f  we  want  to  use  the  ins ide -outs ide  a lgor i thm forgrammar inf'erence, then the.
grammar prohabilitieshaw; to meet the above condition in order for tile rees-t imation to make sense.If string w - w\]'w2...wl~d, then 1.tvj~ 0 ~ i <j < IWl denotes the substring wi+i .
.
.wj .
The in-side probahil i ,y Pq , ( i , j )  estimates the likelihoodP\ [C  - r )  .
.
,that occurrence Av(,i.,. )
derives iwj, wlnle the outsideprobability ,O ~l,(q.,,)~i, j) estimates the likelihood of de--riving otl~iAp(q,,)j~t)lw I \[roln the start symbol S. Themsideq~robability for st, ring w and nontermin~d occur-rellce Ap(q.,,) is defined by the recurrent relationl~,}q.,.
)(i-- t, i) = p,,(q.,.
)(p~), where a, = wlw (,(~,.,.
)(~, a:) :=) w ?
.
tos,t i<j<kSimilarly, the outsideq~robabilities f'or shorter spansof w can he computed from the inside probabilitiesand the outside probabilities for longer spans by thefollowing recurrence:w o,,(,,,)(0, I,~1) = \], if q - -  1tu o,,(~,o(o, I,~1) = 0, otherwiseo; ,%, , ) ( i ,  =i - -  1~ w ' w ' E E Oq(s.t) (J' ~')\],.(qp.
)(3, i)pq(a.t)(qpr)~,t j=OThe second equation above is somewhat simplerthan the corresponding one For unrestricted stoch-astic grammars, because the occurrence Ap(q.r) forwhich the outside probabil ity O~(q ,.
)(/, k) is com-puted specifics the production use(~}~r c eating it andconsequently the prohability for Ap(v,. )
to generatecl'wiAp(q.,.
)j'w\[w I i s the sum of lnnch less possibilities.Once the inside and outside probabilities are con}-tinted for each string in the sample set E, the reesti-mated probability of binary rules, ~Kf.,.
)0,~t) , and tile933reestimated probability of unary rules, ~q(p.,.
)(q~), arecomputed using the following reestimation formulae:/Sv(q.,.
)0,.,, ) =1 \[ Pv(q'")(w*)I~(q't)(i'J) \]wee O<i<J<k<\[wl- - I~ .)
(J'k)O~ v(q., ") (i,k)- Zweel)v(q:,')(v~) =1wEE l<_i<_la\[,wi=a,pp(q.r)(p,QOpW(q.,,) (i -- 1, i)wEEwhere P~ is the probability assigned by the currentmodel to string wP~ = I7<0 )(0, I,ol)and P~ is the probability assigned by the currentmodel to the set of derivations involving some in-stance of Ap'~ = \[~ i ' ~ i '0_<i<j_<\[wlThe denominator of the estimates /3p(q.,.
)(p~,) and}p(q.r)(ps) estimates the probability that a derivationof a string w C E will involve at least one expansionof the nonterminal occurrence Ap(q.~).
The numeratorof \])p(q.r)(pa~) estimates the probability that a deriva-tion of a string w C E will involve rule A,~ ~ AqAr,while the numerator of 7~p(q.,,)(pa) estimates the prob-ability that a derivation of a string w ~ E will rewriteAp to aa.
Thus  Dp(q.,')(pst) estimates the probabilitythat a rewrite of Ap(q.r) in a string from E will userule Ap --+ A~A,, and Dp(q.~')(ps) estimates the proba-bility that occurrence Av(q.~ ) in a string from E willbe rewritten to a,.
Clearly, these are the best cur-rent estimates for the binary and unary ruie proba-bilities.
The process is then repeated with the reesti-mated probabilities until the increase in the estimatedprobability of the sample set given the model becomesnegligible.
We presented the inside, outside and (esti-mated) production probabilities only for the nonter-minal occurrences of the form Ap(q.r); for occurrencesAp(qr.)
these can simply be found by adapting theequations we have given for them.
'\]?he reestimation algorithm can be used both torefine the current estimated probabilities of a stoch-astic grammar and to infer a stochastic grammar fromscratch.
The former application can be said to beincremental.
In the latter case, the initial weaklyrestricted grammar for the inside-outside algorithmconsists of all possible CNF rules over the given setsVN of nonterminals and liT of terminals, with suitablenonzero probabilities assigned to the nontm'minal oc-currences.6 ConclusionsIn this paper we have investigated consistency ofweakly restricted stochastic grammars and presentedan adapted version of the inside-outside algorithm.Other issues concerning stochastic grammars and es-pecially weakly restricted grammars that are being in-vestigated at the moment are stochastic grammaticalinference and parsing using weakly restricted gram-mars.
By stochastic grammatical inference we meangrammatical inference whereby the production prob-abilities are computed simultaneously.
Consistencyof stochastic grammars and stochastic inference willbe treated in full in the master thesis of H.W.L.
terDoest, which is to appear in 1994 \[5\].Acknowledgement  We are grateful to JormaTarhio, presently at the University of Berkeley, Cali-fornia, for stimulating discnssions.References\[1\] R. op den Akker.
Stochastic Gram.mars: fl, eoryand applications.
University of Twente, Depart-ment of Computer Science, Memoranda Informat-ica 93-19, 1993.\[2\] T.L.
Booth, R.A. Thompson.
Applying Probabil-ity Measures to Abstract Languages.
In: IEEETransactions on ComputersVoh C-22, No.
5, May1973.\[3\] T. Briscoe, J. Carroll.
Generalized ProbabilisticLt~ Parsing of Natural Language (Corpora) withUnification-Based Grammars.
hr: ComputationalLinguistics, Vol.
19, No.
1.\[4\] K. Lari and S.J.
Young.
Applications of Stoch-astic Context-free Grammars Using the Inside-Outside Algorithm.
In: Computer Speech and Lan-guage,Vol.
5,pp.
237-257,1991.\[5\] II.W.L.
ter Doest.
Stochastic Grammars: Consis-tency and Inference.
M. Sc.
Thesis, University ofTwente, Enschede, in preparation, The Nether-lands.\[6\] T.E.
Harris.
Th.e Theory of Branching Processes.Springer-Verlag (Berlin and New York), 1963.\[7\] F. aelinek, J.D.
Lafferty.
Computation of theProbability of Initial Substring Generation byStochastic Context-Free Grammars.
In: Compu-tational Linguistics, Vol.
17, No.
3.\[8\] A. Salomaa.
Probabilistic and Weighted Gram-mars.
In: Information and Sciences, Vol.
15(1969), pp.
529-544.C.S.
Wetherell.
Probabilistic Languages: A Re-view and Some Open Questions.
In: ComputingSurveys , Vol.
12, No.
4, pp.
361-379, December1980.\[9\]934
