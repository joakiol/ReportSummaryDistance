First Joint Conference on Lexical and Computational Semantics (*SEM), pages 310?318,Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational LinguisticsUiO1: Constituent-Based Discriminative Ranking for Negation ResolutionJonathon Read Erik Velldal Lilja ?vrelid Stephan OepenUniversity of Oslo, Department of Informatics{jread,erikve,liljao,oe}@ifi.uio.noAbstractThis paper describes the first of two systemssubmitted from the University of Oslo (UiO)to the 2012 *SEM Shared Task on resolvingnegation.
Our submission is an adaption ofthe negation system of Velldal et al (2012),which combines SVM cue classification withSVM-based ranking of syntactic constituentsfor scope resolution.
The approach further ex-tends our prior work in that we also identifyfactual negated events.
While submitted forthe closed track, the system was the top per-former in the shared task overall.1 IntroductionThe First Joint Conference on Lexical and Compu-tational Semantics (*SEM 2012) hosts a shared taskon resolving negation (Morante and Blanco, 2012).This involves the subtasks of (i) identifying nega-tion cues, (ii) identifying the in-sentence scope ofthese cues, and (iii) identifying negated (and factual)events.
This paper describes a system submitted bythe Language Technology Group at the University ofOslo (UiO).
Our starting point is the negation systemdeveloped by Velldal et al (2012) for the domain ofbiomedical texts, an SVM-based system for classi-fying cues and ranking syntactic constituents to re-solve cue scopes.
However, we extend and adaptthis system in several important respects, such as interms of the underlying linguistic formalisms thatare used, the textual domain, handling of morpho-logical cues and discontinuous scopes, and in thatthe current system also identifies negated events.The data sets used for the shared task includethe following, all based on negation-annotated Co-nan Doyle (CD) stories (Morante and Daelemans,2012): a training set of 3644 sentences (hereafterreferred to as CDT), a development set of 787 sen-tences (CDD), and a held-out evaluation set of 1089sentences (CDE).
We will refer to the combinationof CDT and CDD as CDTD.
An example of an an-notated sentence is shown in (1) below, where thecue is marked in bold, the scope is underlined, andthe event marked in italics.
(1) There was no answer.We describe two different system configurations,both of which were submitted for the closed track(hence we can only make use of the data providedby the task organizers).
The systems only differwith respect to how they were optimized.
In thefirst configuration, (hereafter I), all components inthe pipeline had their parameters tuned by 10-foldcross-validation across CDTD.
The second config-uration (II) is tuned against CDD using CDT fortraining.
The rationale for this strategy is to guardagainst possible overfitting effects that could resultfrom either optimization scheme, given the limitedsize of the data sets.
For the held-out testing all mod-els are estimated on the entire CDTD.Unless otherwise noted, all reported scores aregenerated using the evaluation script provided by theorganizers, which breaks down performance with re-spect to cues, events, scope tokens, and two vari-ants of scope-level exact match (one requiring exactmatch of cues and the other only partial cue match).The latter two scores are identical for our systemhence are not duplicated in this paper.
Furthermore,as we did not optimize for the scope tokens measurethis is only reported for the final evaluation.Note also that the evaluation actually includestwo variants of the metrics mentioned above; a setof primary measures with precision computed asP = TP/(TP + FP ) and a set of so-called B mea-sures that instead uses P = TP/S, where S is the310total number of predictions made by the system.
Thereason why S is not identical with TP + FP isthat partial matches are only counted as FNs (andnot FPs) in order to avoid double penalties.
Wedo not report the B measures for development test-ing as they were only introduced for the final eval-uation and hence were not considered in our sys-tem optimization.
We note though, that the relative-ranking of participating systems for the primary andB measures is identical, and that the correlation be-tween the paired lists of scores is nearly perfect(r = 0.997).The paper is structured according to the compo-nents of our system.
Section 2 details the process ofidentifying instances of negation through the disam-biguation of known cue words and affixes.
Section 3describes our hybrid approach to scope resolution,which utilizes both heuristic and data-driven meth-ods to select syntactic constituents.
Section 4 dis-cusses our event detection component, which firstapplies a classifier to filter out non-factual eventsand then uses a learned ranking function to selectevents among in-scope tokens.
End-to-end resultsare presented in Section 5.2 Cue DetectionCue identification is based on the light-weight clas-sification scheme presented by Velldal et al (2012).By treating the set of cue words as a closed class,Velldal et al (2012) showed that one could greatlyreduce the number of examples presented to thelearner, and correspondingly the number of fea-tures, while at the same time improving perfor-mance.
This means that the classifier only attemptsto ?disambiguate?
known cue words, while ignoringany words not observed as cues in the training data.The classifier applied in the current submissionis extended to also handle morphological or affixalnegation cues, such as the prefix cue in impatience,the infix in carelessness, and the suffix of colourless.The negation affixes observed in CDTD are; the pre-fixes un, dis, ir, im, and in; the infix less (we inter-nally treat this as the suffixes lessly and lessness);and the suffix less.
Of the total set of 1157 cues inthe training and development data, 192 are affixal.There are, however, a total of 1127 tokens matchingone of the affix patterns above, and while we main-tain the closed class assumption also for the affixes,the classifier will need to consider their status as acue or non-cue when attaching to any such token, asin image, recklessness, and bless.2.1 FeaturesIn the initial formulation of Velldal (2011), an SVMclassifier was applied using simple n-gram featuresover words, both full forms and lemmas, to theleft and right of the candidate cues.
In addition tothese token-level features, the classifier we applyhere includes features specifically targeting affixalcues.
The first such feature records character n-grams from both the beginning and end of the basethat an affix attaches to (up to five positions).
Fora context like impossible we would record n-gramssuch as {possi, poss, .
.
.}
and {sible, ible, .
.
.
}, andcombine this with information about the affix itself(im) and the token part-of-speech (?JJ?
).For the second type of affix-specific features, wetry to emulate the effect of a lexicon look-up of theremaining substring that an affix attaches to, check-ing its status as an independent base form and itspart-of-speech.
In order to take advantage of suchinformation while staying within the confines of theclosed track, we automatically generate a lexiconfrom the training data, counting the instances of eachPoS tagged lemma in addition to n-grams of word-initial characters (again recording up to five posi-tions).
For a given match of an affix pattern, a fea-ture will then record these counts for the substring itattaches to.
The rationale for this feature is that theoccurrence of a substring such as un in a token suchas underlying should be less likely as a cue giventhat the first part of the remaining string (e.g., derly)would be an unlikely way to begin a word.It is also possible for a negation cue to span multi-ple tokens, such as the (discontinuous) pair neither /nor or fixed expressions like on the contrary.
Thereare, however, only 16 instances of such multiwordcues (MWCs) in the entire CDTD.
Rather than let-ting the classifier be sensitive to these corner cases,we cover such MWC patterns using a small set ofsimple post-processing heuristics.
A small stop-listis used for filtering out the relevant words from theexamples presented to the classifier (on, the, etc.
).Note that, in terms of training the final classifiers,CDTD provides us with a total of 1162 positive and311Data set Model Prec Rec F1CDTDBaseline 92.25 88.50 90.34ClassifierI 94.99 95.07 95.03CDDBaseline 90.68 84.39 87.42ClassifierII 93.75 95.38 94.56CDEBaseline 87.10 92.05 89.51ClassifierI 91.42 92.80 92.10ClassifierII 89.17 93.56 91.31Table 1: Detecting negation cues using the two clas-sifiers and the majority-usage baseline.1100 negative training examples, given our closed-class treatment of cues.Before we turn to the results, note that the dif-ference between the two submitted versions of theclassifier (I and II) only concerns the orders of then-grams used for the token-level features.12.2 ResultsTable 1 presents the results for our cue classifier.
Asan informed baseline, we also tried classifying eachword based on its most frequent use as a cue or non-cue in the training data.
(Affixal cue occurrences arecounted by looking at both the affix-pattern and thebase it attaches to, basically treating the entire tokenas a cue.
Tokens that end up being classified as cuesare then matched against the affix patterns observedduring training in order to correctly delimit the an-notation of the cue.)
This simple majority-usageapproach actually provides a fairly strong baseline,yielding an F1 of 90.34 on CDTD.
Compare this tothe F1 of 95.03 obtained by the classifier on the samedata set.
However, when applying the models to theheld-out set, with models estimated over the entireCDTD, the classifier suffers a slight drop in perfor-mance, leaving the baseline even more competitive:While our best performing final cue classifier (I)achieves F1=92.10, the baseline achieves F1=89.51,and even outperforms four of the ten cue detectionsystems submitted for the shared task (three of the12 shared task submissions use the same classifier).1Classifier I records the lemma and full form of the targettoken, and lemmas two positions left/right.
Classifier II recordsthe lemma, form, and PoS of the target, full forms three posi-tions to the left and one to the right, PoS one position right/left,and lemmas three positions to the right.
The affixal-specific fea-tures are the same for both configurations as described above.SNPEXThereVPVBDwasNPDTnoNNanswer..Figure 1: Example parse tree provided in the data,highlighting our candidate scope constituents.Inspecting the predictions of the classifier onCDD, which comprises a total of 173 gold anno-tated cues, we find that Classifier I mislabels 11false positives (FPs) and seven false negatives (FNs).Of the FPs, we find five so-called false negationcues (Morante et al, 2011), including three in-stances of the fixed expression none the less.
Theothers are affixal cues, of which two are clearlywrong (underworked, universal) while others mightarguably be due to annotation errors (insuperable,unhappily, endless, listlessly).
Among the FNs, twoare due to MWCs not covered by our heuristics (e.g.,no more), with the remainder concerning affixes.3 Constituent-Based Scope ResolutionDuring the development of our scope resolution sys-tem we have pursued both a rule-based and data-driven approach.
Both are rooted in the assumptionthat the scope of negations corresponds to a syntac-tically meaningful unit.
Our starting point here willbe the syntactic analyses provided by the task or-ganizers (see Figure 1), generated using the rerank-ing parser of Charniak and Johnson (2005).
How-ever, as alignment between scope annotations andsyntactic units is not straightforward for all cases,we apply several exception rules that ?slacken?
therequirements for alignment, as discussed in Sec-tion 3.1.
In Sections 3.2 and 3.3 we detail ourrule-based and data-driven approaches, respectively.Note that the predictions of the rule-based compo-nent will be incorporated as features in the learnedmodel, similarly to the set-up described by Read etal.
(2011).
Section 3.4 details the post-processingwe apply to handle cases of discontinuous scope, be-312fore Section 3.5 finally presents development resultstogether with a brief error analysis.3.1 Constituent Alignment and SlackeningIn order to test our initial assumption that syntacticunits correspond to scope annotations, we quantifythe alignment of scopes with constituents in CDT,excluding 97 negations that do not have a scope.We find that the initial alignment is rather low at52.42%.
We therefore formulate a set of slacken-ing heuristics, designed to improve on this alignmentby removing certain constituents at the beginning orend of a scope.
First of all, removing constituent-initial and -final punctuation improves alignment to72.83%.
We then apply the following slackeningrules, with examples indicating the resulting scopefollowing slackening (not showing events):- Remove coordination (CC) and following con-juncts if the coordination is a rightwards siblingof an ancestor of the cue and it is not directlydominated by an NP.
(2) Since we have been so unfortunate as to miss himand have no notion [.
.
.
]- Remove S* to the right of cue, if delimited bypunctuation.
(3) ?There is no other claimant, I presume ?
?- Remove constituent-initial SBAR.
(4) If it concerned no one but myself I would nottry to keep it from you.
?- Remove punctuation-delimited NPs.
(5) ?But I can?t forget them, Miss Stapleton,?
said I.- Remove constituent-initial RB, CC, UH,ADVP or INTJ.
(6) And yet it was not quite the last.The slackening rules are based on a few obser-vations.
First, scope rarely crosses coordinationboundaries (with the exception of nominal coordi-nation).
Second, scope usually does not cross clauseboundaries (indicated by S/SBAR).
Furthermore, ti-tles and other nominals of address are not includedin the scope.
Finally, sentence and discourse adver-bials are often excluded from the scope.
Since theseexpress semantic distinctions, we approximate thisRB//VP/SBAR if SBAR\WH*RB//VP/SRB//SDT/NP if NP/PPDT//SBAR if SBAR\WHADVPDT//SJJ//ADJPVP/S if S\VP\VB*[@lemma="be"]JJ/NP/NP if NP\PPJJ//NPUHIN/PPNN/NP//S/SBAR if SBAR\WHNPNN/NP//SCC/SINVFigure 2: Scope resolution heuristics.notion syntactically using parts-of-speech and con-stituent category labels expressing adverbials (RB),coordinations (CC), various types of interjections(UH, INTJ) and adverbial phrases (ADVP).
We maynote here that syntactic categories are not alwayssufficient to express semantic distinctions.
Preposi-tional phrases, for instance, are often used to expressthe same type of discourse adverbials, but can alsoexpress a range of other distinctions (e.g., tempo-ral or locative adverbials), which are included in thescope.
So a slackening rule removing initial PPs wastried but not found to improve overall alignment.After applying the above slackening rules thealignment rate for CDT improves to 86.13%.
Thisalso represents an upper-bound on our performance,as we will not be able to correctly predict a scopethat does not align with a (slackened) constituent.3.2 Heuristics Operating over ConstituentsThe alignment of constituents and scopes reveal con-sistent patterns and we therefore formulate a set ofheuristic rules over constituents.
These are basedon frequencies of paths from the cue to the scope-aligned constituent for the annotations in CDT, aswell as the annotation guidelines (Morante et al,2011).
The rules are formulated as paths over con-stituent trees and are presented in Figure 2.
Thepath syntax is based on LPath (Lai and Bird, 2010).The rules are listed in order of execution, showinghow more specific rules are consulted before moregeneral ones.
We furthermore allow for some ad-ditional functionality in the interpretation of rulesby enabling simple constraints that are applied tothe candidate constituent.
For example, the ruleRB//VP/SBAR if SBAR\WH* will be activated whenthe cue is an adverb having some ancestor VP whichhas a parent SBAR, where the SBAR must contain aWH-phrase among its children.313In cases where no rule is activated we use a de-fault scope prediction, which expands the scope toboth the left and the right of the cue until either thesentence boundary or a punctuation mark is reached.The rules are evaluated individually in Section 3.5below and the rule predictions are furthermore em-ployed as features for the ranker described below.3.3 Constituent RankingOur data-driven approach to scope resolution in-volves learning a ranking function over candidatesyntactic constituents.
The approach has similari-ties to discriminative parse selection, except that wehere rank subtrees rather than full parses.When defining the training data, we begin by se-lecting negations for which the parse tree containsa constituent that (after slackening) aligns with thegold scope.
We then select an initial candidate byselecting the smallest constituent that spans all thewords in the cue, and then generate subsequent can-didates by traversing the path to the root of thetree (see Figure 1).
This results in a mean ambi-guity of 4.9 candidate constituents per negation (inCDTD).
Candidates whose projection correspondsto the gold scope are labeled as correct; all others arelabeled as incorrect.
Experimenting with a variety offeature types (listed in Table 2), we use the imple-mentation of ordinal ranking in the SVMlight toolkit(Joachims, 2002) to learn a linear scoring functionfor preferring correct candidate scopes.The most informative feature type is the LPathfrom cue, which in addition to recording the fullpath from the cue to the candidate constituent(e.g., the path to the correct candidate in Fig-ure 1 is no/DT/NP/VP/S), also includes delexicalized(./DT/NP/VP/S), generalized (no/DT//S), and gen-eralized delexicalized versions (./DT//S).Note that the rule prediction feature facilitates ahybrid approach by recording whether the candidatematches the boundaries of the scope predicted by therules of Section 3.2, as well as the degree of overlap.3.4 Handling Discontinuous Scope10.3% of the scopes in the training data are what(Morante et al, 2011) refer to as discontinuous.
Thismeans that the scope contains two or more partswhich are bridged by tokens other than the cue.Feature types I IILPath from cue ?
?LPath from cue bigrams and trigrams ?
?LPath from cue to left/right boundary ?LPath to left/right boundary ?LPath to root ?Punctuation to left/right ?
?Rule prediction ?Sibling bigrams ?Size in tokens, relative to sentence (%) ?
?Surface bigrams ?
?Tree distance from cue ?
?Table 2: Features used to describe candidate con-stituents for scope resolution, with indications ofpresence in our two system configurations.
(7) I therefore spent the day at my club and did notreturn to Baker Street until evening.
(8) There was certainly no physical injury of any kind.The sentence in (7) exemplifies a common causeof scopal discontinuity in the data, namely ellipsis(Morante et al, 2011).
Almost all of these are casesof coordination, as in example (7) where the cue isfound in the final conjunct (did not return [.
.
. ])
andthe scope excludes the preceding conjunct(s) (there-fore spent the day at my club).
There are also somecases of adverbs that are excluded from the scope,causing discontinuity, as in (8), where the adverbcertainly is excluded from the scope.In order to deal with discontinuous scopes we for-mulate two simple post-processing heuristics, whichare applied after rules/ranking: (1) If the cue is ina conjoined phrase, remove the previous conjunctsfrom the scope, and (2) remove sentential adverbsfrom the scope (where a list of sentential adverbswas compiled from the training data).3.5 ResultsOur development procedure evaluated all permuta-tions of feature combinations, searching for opti-mal parameters using gold-standard cues.
Table 2indicates which features are included in our tworanker configurations, i.e., tuning by 10-fold cross-validation on CDTD (I) vs. a train/test-split forCDT/CDD(II).Table 3 lists the results of our scope resolutionapproaches applied to gold cues.
As a baseline, all314Data set Model Prec Rec F1CDTDBaseline 98.31 33.18 49.61Rules 100.00 71.37 83.29RankerI 100.00 73.55 84.76CDDBaseline 100.00 36.31 53.28Rules 100.00 69.64 82.10RankerII 100.00 70.24 82.52CDEBaseline 96.47 32.93 49.10Rules 98.73 62.65 76.66RankerI 98.77 64.26 77.86RankerII 98.75 63.45 77.26Table 3: Scope resolution for gold cues using thetwo versions of the ranker, also listing the perfor-mance of the rule-based approach in isolation.cases are assigned the default scope prediction of therule-based approach.
On CDTD this results in an F1of 49.61 (P=98.31, R=33.18); compare to the rankerin Configuration I on the same data set (F1=84.76,P=100.00, R=73.55).
We note that our different op-timization procedures do not appear to have mademuch difference to the learned ranking functions asboth perform similarly on the held-out data, thoughsuffering a slight drop in performance compared tothe development results.
We also evaluate the rulesand observe that this approach achieves similar held-out results.
This is particularly note-worthy giventhat there are only fourteen rules plus the defaultscope baseline.
Note that, as the rankers performedbetter than the rules in isolation on both CDTD andCDD during development, our final system submis-sions are based on rankers I and II from Table 3.We performed a manual error analysis of ourscope resolution system (RankerII) on the basis ofCDD (using gold cues).
First, we may note thatparse errors are a common sources of scope res-olution errors.
It is well-known that coordina-tion presents a difficult construction for syntacticparsers, and we often find incorrectly parsed coordi-nate structures among the system errors.
Since coor-dination is used both in the slackening rules and theanalysis of discontinuous scopes, these errors haveclear effects on system performance.
We may fur-ther note that discourse-level adverbials, such as inthe second place in example (9) below, are often in-cluded in the scope assigned by our system, whichthey should not be according to the gold annotation.
(9) But, in the second place, why did you not come at once?There are also quite a few errors related to the scopeof affixal cues, which the ranker often erroneouslyassigns a scope that is larger than simply the basewhich the affix attaches to.4 Event DetectionOur event detection component implements twostages: First we apply a factuality classifier, andthen we identify negated events2 for those contextsthat have been labeled as factual.
We detail the twostages in order below.4.1 Factuality DetectionThe annotation guidelines of Morante et al (2011)specify that events should only be annotated fornegations that have a scope and that occur in fac-tual statements.
This means that we can view the*SEM data sets to implicitly annotate factuality andnon-factuality, and take advantage of this to train anSVM factuality classifier.
We take positive exam-ples to correspond to negations annotated with botha scope and an event, while negative examples corre-spond to scope negations with no event.
For CDTD,this strategy gives 738 positive and 317 negative ex-amples, spread over a total of 930 sentences.
Notethat we do not have any explicit annotation of cuewords for these examples.
All we have are instancesof negation that we know to be within a factual ornon-factual context, but the indication of factualitymay typically be well outside the annotated nega-tion scope.
For our experiments here, we thereforeuse the negation cue itself as a place-holder for theabstract notion of context that we are really classi-fying.
Given the limited amount of data, we onlyoptimize our factuality classifier by 10-fold cross-validation on CDTD (i.e., the same configuration isused for submissions I and II).The feature types we use are all variations overbag-of-words (BoW) features.
We include left- andright-oriented BoW features centered on the nega-tion cue, recording forms, lemmas, and PoS, and us-ing both unigrams and bigrams.
The features are ex-2Note that the annotation guidelines use the term eventrather broadly as referring to a process, action, state, or prop-erty (Morante et al, 2011).315Data set Model Prec Rec F1 AccCDTDBaseline 69.95 100.00 82.32 69.95Classifier 84.51 96.07 89.92 83.98CDEBaseline 69.48 100.00 81.99 69.48Classifier 77.73 95.91 85.86 78.31Table 4: Results for factuality detection (using goldnegation cues and scopes).
Due to the limited train-ing data for factuality, the classifier is only opti-mized by 10-fold cross-validation on CDTD.tracted from the sentence as a whole, as well as froma local window of six tokens to each side of the cue.Table 4 provides results for factuality classifica-tion using gold-standard cues and scopes.3 We alsoinclude results for a baseline approach that simplyconsiders all cases to be factual, i.e., the majorityclass.
In this case precision is identical to accuracyand recall is 100%.
For precision and accuracy wesee that the classifier improves substantially over thebaseline on both data sets, although there is a bit of adrop in performance when going from the 10-fold toheld-out results.
There also seem to be some signsof overfitting, given that roughly 70% of the trainingexamples end up as support vectors.4.2 Ranking EventsHaving filtered out non-factual contexts, events areidentified by applying a similar approach to that ofthe scope-resolving ranker described in Section 3.3.In this case, however, we rank tokens as candidatesfor events.
For simplicity in this first round of de-velopment we make the assumption that all eventsare single words.
Thus, the system will be unable tocorrectly predict the event in the 6.94% of instancesin CDTD that are multi-word.We select candidate words from all those markedas being in the scope (including substrings of to-kens with affixal cues).
This gives a mean ambigu-ity of 7.8 candidate events per negation (in CDTD).Then, discarding multi-word training examples, weuse SVMlight to learn a ranking function for identi-fying events among the candidates.Table 5 shows the features employed, with in-3As this is not singled out as a separate subtask in the sharedtask itself, these are the only scores in the paper not computedusing the script provided by the organizers.Feature type I IIContains affixal cue ?Following lemma ?Lemma ?
?LPath to scope constituent ?
?LPath to scope constituent bigrams ?
?Part-of-speech ?
?Position in scope ?
?Preceding lemma ?
?Preceding part-of-speech ?
?Token distance from cue ?
?Table 5: Features used to describe candidates forevent detection, with indications of presence in ourtwo system configurations.Data set Model Prec Rec F1CDTD RankerI 91.49 90.83 91.16CDD RankerII 92.11 91.30 91.70CDERankerI 83.73 83.73 83.73RankerII 84.94 84.95 84.94Table 6: Event detection for gold scopes and goldfactuality information.dications as to their presence in our two configu-rations (after an exhaustive search of feature com-binations).
The most important feature was LPathto scope constituent.
For example, in Figure 1the scope constituent is the S root of the tree;the path that describes the correct candidate isanswer/NN/NP/VP/S.
As discussed in Section 3.3,we also record generalized, delexicalized and gener-alized delexicalized paths.Table 6 lists the results of the event ranker appliedto gold-standard cues, scopes, and factuality.
For acomparative baseline, we implemented a keyword-based approach that simply searches in-scope wordsfor instances of events previously observed in thetraining set, sorted according to descending fre-quency.
This baseline achieves F1=29.44 on CDD.For comparison, the ranker (II) achieves F1=91.70on the same data set, as seen in Table 6.
We alsosee that Configuration II appears to generalize best,with over 1.2 points improvement over the F1 of I.An analysis of the event predictions for CDD in-dicates that the most frequent errors (41.2%) are in-stances where the ranker correctly predicts part ofthe event but our single word assumption is invalid.Another apparent error is that the system fails to316Submission I Submission IIPrec Rec F1 Prec Rec F1Cues 91.42 92.80 92.10 89.17 93.56 91.31Scopes 87.43 61.45 72.17 83.89 60.64 70.39Scope Tokens 81.99 88.81 85.26 75.87 90.08 82.37Events 60.50 72.89 66.12 60.58 75.00 67.02Full negation 83.45 43.94 57.57 79.87 45.08 57.63Cues B 89.09 92.80 90.91 86.97 93.56 90.14Scopes B 59.30 61.45 60.36 56.55 60.64 58.52Events B 57.62 72.89 64.36 58.60 75.00 65.79Full negation B 42.18 43.94 43.04 41.90 45.08 43.43Table 7: End-to-end results on the held-out data.predict a main verb for the event, and instead pre-dicts nouns (17.7% of all errors), modals (17.7%) orprepositions (11.8%).5 Held-Out EvaluationTable 7 presents our final results for both systemconfigurations on the held-out evaluation data (alsoincluding the B measures, as discussed in the intro-duction).
Comparing submission I and II, we findthat the latter has slightly better scores end-to-end.However, as seen throughout the paper, the picture isless clear-cut when considering the isolated perfor-mance of each component.
When ranked accordingto the Full Negation measures, our submissions wereplaced first and second (out of seven submissions inthe closed track, and twelve submissions total).
Itis difficult to compare system performance on sub-tasks, however, as each component will be affectedby the performance of the previous.6 ConclusionsThis paper has presented two closed-track submis-sions for the *SEM 2012 shared task on negationresolution.
The systems were ranked first and sec-ond overall in the shared task end-to-end evaluation,and the submissions only differ with respect to thedata sets used for parameter tuning.
There are fourcomponents in the pipeline: (i) An SVM classifierfor identifying negation cue words and affixes, (ii)an SVM-based ranker that combines empirical evi-dence and manually-crafted rules to resolve the in-sentence scope of negation, (iii) a classifier for de-termining whether a negation is in a factual or non-factual context, and (iv) a ranker that determines(factual) negated events among in-scope tokens.For future work we would like to try training sepa-rate classifiers for affixal and token-level cues, giventhat largely separate sets of features are effective forthe two cases.
The system might also benefit fromsources of information that would place it in theopen track.
These include drawing information fromother parsers and formalisms, generating cue fea-tures from an external lexicon, and using additionaltraining data for factuality detection, e.g., FactBank(Saur??
and Pustejovsky, 2009).From observations on CDTD we note that approx-imately 14% of scopes will be unresolvable as theyare not aligned with constituents (see Section 3.1).This can perhaps be tackled by ranking tokens ascandidates for left and right scope boundaries (sim-ilar to the event ranker in the current work).
Thiswould improve the upper-bound to 100% at the ex-pense of greatly increasing the number of candi-dates.
However, the strong discriminative power ofour current approach can still be incorporated usingconstituent-based features.AcknowledgmentsWe thank Roser Morante and Eduardo Blanco fortheir work in organizing this shared task and com-mitment to producing quality data.
We also thankthe anonymous reviewers for their feedback.
Large-scale experimentation was carried out with the TI-TAN HPC facilities at the University of Oslo.317ReferencesEugene Charniak and Mark Johnson.
2005.
Coarse-to-fine n-best parsing and MaxEnt discriminative rerank-ing.
In Proceedings of the Forty-Third Annual Meetingof the Association for Computational Linguistics, AnnArbor, MI.Thorsten Joachims.
2002.
Optimizing search enginesusing clickthrough data.
In Proceedings of the EighthACM International Conference on Knowledge Discov-ery and Data Mining, Alberta.Catherine Lai and Steven Bird.
2010.
Querying linguis-tic trees.
Journal of Logic, Language and Information,19:53?73.Roser Morante and Eduardo Blanco.
2012.
*SEM 2012shared task: Resolving the scope and focus of nega-tion.
In Proceedings of the First Joint Conference onLexical and Computational Semantics, Montreal.Roser Morante and Walter Daelemans.
2012.ConanDoyle-neg: Annotation of negation in ConanDoyle stories.
In Proceedings of the Eighth Interna-tional Conference on Language Resources and Evalu-ation, Istanbul.Roser Morante, Sarah Schrauwen, and Walter Daele-mans.
2011.
Annotation of negation cues and theirscope: Guidelines v1.0.
Technical report, Univer-sity of Antwerp.
CLIPS: Computational Linguistics& Psycholinguistics technical report series.Jonathon Read, Erik Velldal, Stephan Oepen, and Lilja?vrelid.
2011.
Resolving speculation and negationscope in biomedical articles using a syntactic con-stituent ranker.
In Proceedings of the Fourth Inter-national Symposium on Languages in Biology andMedicine, Singapore.Roser Saur??
and James Pustejovsky.
2009.
Factbank:a corpus annotated with event factuality.
LanguageResources and Evaluation, 43(3):227?268.Erik Velldal, Lilja ?vrelid, Jonathon Read, and StephanOepen.
2012.
Speculation and negation: Rules,rankers and the role of syntax.
Computational Lin-guistics, 38(2).Erik Velldal.
2011.
Predicting speculation: A simple dis-ambiguation approach to hedge detection in biomedi-cal literature.
Journal of Biomedical Semantics, 2(5).318
