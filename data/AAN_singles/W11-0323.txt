Proceedings of the Fifteenth Conference on Computational Natural Language Learning, pages 200?209,Portland, Oregon, USA, 23?24 June 2011. c?2011 Association for Computational LinguisticsFilling the Gap:Semi-Supervised Learning for Opinion Detection Across DomainsNing YuIndiana Universitynyu@indiana.eduSandra Ku?blerIndiana Universityskuebler@indiana.eduAbstractWe investigate the use of Semi-SupervisedLearning (SSL) in opinion detection both insparse data situations and for domain adapta-tion.
We show that co-training reaches the bestresults in an in-domain setting with small la-beled data sets, with a maximum absolute gainof 33.5%.
For domain transfer, we show thatself-training gains an absolute improvement inlabeling accuracy for blog data of 16% overthe supervised approach with target domaintraining data.1 IntroductionRich and free opinions published electronically and,more recently, on the WWW offer ample opportuni-ties to discover individual?s attitudes towards certaintopics, products, or services.
To capitalize on thisenormous body of opinions, researchers have beenworking in the area of opinion mining since the late1990s.
Opinion detection seeks to automatically de-termine the presence or absence of opinions in a text,and it is therefore a fundamental task for opinionmining.In order to capture subtle and creative opinions,opinion detection systems generally assume that alarge body of opinion-labeled data are available.However, collections of opinion-labeled data are of-ten limited, especially at the granularity level of sen-tences; and manual annotation is tedious, expensiveand error-prone.
The shortage of opinion-labeleddata is less challenging in some data domains (e.g.,reviews) than in others (e.g., blog posts).
A sim-ple method for improving accuracies in challengingdomains would be to borrow opinion-labeled datafrom a non-target data domain; but this approachoften fails because opinion detection strategies de-signed for one data domain generally do not performwell in another domain.
One reason for failure ofthe simple transfer approach is that the informationused for opinion detection is typically lexical, andlexical means of expressing opinions may vary notonly from domain to domain, but also from registerto register.
For example, while the word ?awesome?is a good indicator of an opinion in blogs, it is lesslikely to occur in the same role in newspaper texts.While it is difficult to obtain opinion-labeled data,one can easily collect almost infinite unlabeled user-generated data that contain opinions.
The use ofSemi-Supervised Learning (SSL), motivated by lim-ited labeled data and plentiful unlabeled data in thereal world, has achieved promising results in vari-ous NLP studies (e.g., (Fu?rstenau and Lapata, 2009;Talukdar and Pereira, 2010)), yet it has not beenfully investigated for use in opinion detection.
Al-though studies have shown that simple SSL meth-ods are promising for extracting opinion featuresor patterns using limited opinion-labeled data (e.g.,(Wiebe and Riloff, 2005)), few efforts have beenmade either to apply SSL directly to opinion detec-tion or to examine more sophisticated SSL methods.This research is intended to fill the gap regarding ap-plication of SSL in opinion detection.
We investi-gate a range of SSL algorithms with a focus on self-training and co-training in three types of electronicdocuments: edited news articles, semi-structuredmovie reviews, and the informal and unstructuredcontent of the blogosphere.
We conclude that SSLis a successful method for handling the shortage ofopinion labeled data and the domain transfer prob-lem.2002 Background and Related WorkThere is a wide range of literature on opinion detec-tion.
We concentrate here on supervised and semi-supervised approaches.2.1 Supervised Learning for Opinion DetectionSupervised learning algorithms that can automati-cally learn important opinion-bearing features froman annotated corpus have been adopted and inves-tigated for opinion detection and yielded satisfyingresults (Wiebe et al, 2004; Yu and Hatzivassiloglou,2003; Zhang and Yu, 2007).
With no classifica-tion techniques developed specifically for opiniondetection, state-of-the-art topical supervised classifi-cation algorithms can achieve performance compa-rable to complex linguistic approaches when usingbinary values (i.e., presence or absence) and incor-porating different types of features.
Commonly usedopinion-bearing features include bag-of-words, POStags, ngrams, low frequency words or unique words(Wiebe et al, 2004; Yang et al, 2007), semanticallyoriented adjectives (e.g., ?great?, ?poor?)
and morecomplex linguistic patterns.
Both the scale and qual-ity of the annotated corpus play an important role inthe supervised learning approach.2.2 SSL for Opinion DetectionIn contrast to supervised learning, SSL learns fromboth labeled and unlabeled data.
SSL assumes that,although unlabeled data hold no information aboutclasses (e.g., ?opinion?
or ?non-opinion?
), they docontain information about joint distribution overclassification features.
Therefore, when a limited setof labeled data is available in the target domain, us-ing SSL with unlabeled data is expected to achievean improvement over supervised learning.Self-training Self-training is the simplest andmost commonly adopted form of SSL for opiniondetection.
Self-training was originally used to fa-cilitate automatic identification of opinion-bearingfeatures.
For example, Riloff and Wiebe (2003) pro-posed a bootstrapping process to automatically iden-tify subjective patterns.
Self-training has also beenapplied directly for identifying subjective sentencesby following a standard self-training procedure: (1)train an initial supervised classifier on the labeleddata; (2) apply this classifier to unlabeled data andselect the most confidently labeled data, as deter-mined by the classifier, to augment the labeled dataset; and (3) re-train the classifier by restarting thewhole process.
Wiebe and Riloff (2005) used a self-trained Na?
?ve Bayes classifier for classifying subjec-tive sentences and achieved better recall with modestprecision over several rule-based classifiers.One shortcoming of self-training is that the result-ing data may be biased: That is, the final labeled datamay consist of examples that are easiest for this par-ticular opinion detector to identify.Co-training The core idea of co-training is to usetwo classifiers and trade additional examples be-tween them, assuming that the resulting union ofclassified examples is more balanced than examplesresulting from using either classifier alone.
Whenlabeling new examples, a final decision is made bycombining the predictions of the two updated learn-ers.
The original co-training algorithm assumes re-dundancy in the training data and thus more thanone view can be used to represent and classify eachexample independently and successfully (Blum andMitchell, 1998).
For example, an image can be nat-urally represented by its text description or by itsvisual attributes.
Even when a natural split in thefeature set is not available, studies have shown thatthe key to co-training is the existence of two largelydifferent initial learners, regardless of whether theyare built by using two feature sets or two learningalgorithms (Wang and Zhou, 2007).When there are different views for the target ex-amples, co-training is conceptually clearer than self-training, which simply mixes features.
Since co-training uses each labeled example twice, it requiresless labeled data and converges faster than self-training.
However, the lack of natural feature splitshas kept researchers from exploring co-training foropinion detection.
To the best of our knowledge,the only co-training application for opinion detec-tion was reported by Jin et al (2009), who createddisjoint training sets for building two initial classi-fiers and successfully identified opinion sentences incamera reviews by selecting auto-labeled sentencesagreed upon by both classifiers.EM-Based SSL Expectation-Maximization (EM)refers to a class of iterative algorithms formaximum-likelihood estimation when dealing with201incomplete data.
Nigam et al (1999) combinedEM with a Na?
?ve Bayes classifier to resolve theproblem of topical classification, where unlabeleddata were treated as incomplete data.
The EM-NBSSL algorithm yielded better performance than ei-ther an unsupervised lexicon-based approach or asupervised approach for sentiment classification indifferent data domains, including blog data (Aue andGamon, 2005; Takamura et al, 2006).
No opiniondetection applications of EM-based SSL have beenreported in the literature.S3VMs Semi-Supervised Support Vector Ma-chines (S3VMs) are a natural extension of SVMs inthe semi-supervised spectrum.
They are designed tofind the maximal margin decision boundary in a vec-tor space containing both labeled and unlabeled ex-amples.
Although SVMs are the most favored super-vised learning method for opinion detection, S3VMshave not been used in opinion detection.
Graph-based SSL learning has been successfully applied toopinion detection (Pang and Lee, 2004) but is notappropriate for dealing with large scale data sets.2.3 Domain Adaptation for Opinion DetectionWhen there are few opinion-labeled data in thetarget domain and/or when the characteristics ofthe target domain make it challenging to detectopinions, opinion detection systems usually borrowopinion-labeled data from other data domains.
Thisis especially common in opinion detection in the bl-ogosphere (Chesley et al, 2006).
To evaluate thisshallow approach, Aue and Gamon (2005) com-pared four strategies for utilizing opinion-labeleddata from one or more non-target domains and con-cluded that using non-targeted labeled data withoutan adaptation strategy is less efficient than using la-beled data from the target domain, even when themajority of labels are assigned automatically by aself-training algorithm.Blitzer et al (2007) and Tan et al (2009) imple-mented domain adaptation strategies for sentimentanalysis.
Although promising, their domain adapta-tion strategies involved sophisticated and computa-tionally expensive methods for selecting general fea-tures to link target and non-target domains.3 Motivation and ObjectiveWhile SSL is especially attractive for opinion de-tection because it only requires a small number oflabeled examples, the studies described in the previ-ous section have concentrated on simple SSL meth-ods.
We intend to fill this research gap by comparingthe feasibility and effectiveness of a range of SSLapproaches for opinion detection.
Specifically, weaim to achieve the following goals:First, to gain a more comprehensive understand-ing of the utility of SSL in opinion detection.
Weexamine four major SSL methods: self-training, co-training, EM-NB, and S3VM.
We focus on self-training and co-training because they are both wrap-per approaches that can be easily adopted by any ex-isting opinion detection system.Second, to design and evaluate co-training strate-gies for opinion detection.
Since recent work hasshown that co-training is not restricted by the orig-inal multi-view assumption for target data and thatit is more robust than self-training, we evaluate newco-training strategies for opinion detection.Third, to approach domain transfer using SSL,assuming that SSL can overcome the problem ofdomain-specific features by gradually introducingtargeted data and thus diminishing bias from thenon-target data set.4 SSL ExperimentsOur research treats opinion detection as a binaryclassification problem with two categories: subjec-tive sentences and objective sentences.
It is evalu-ated in terms of classification accuracy.Since a document is normally a mixture of factsand opinions (Wiebe et al, 2001), sub-documentlevel opinion detection is more useful and meaning-ful than document-level opinion detection.
Thus, weconduct all experiments on the sentence level.The remainder of this section explains the datasets and tools used in this study and presents the ex-perimental design and parameter settings.4.1 Data SetsThree types of data sets have been explored in opin-ion detection studies: news articles, online reviews,and online discourse in blogs or discussion forums.These three types of text differ from one another in202terms of structure, text genre (e.g., level of formal-ity), and proportion of opinions found therein.
Weselected a data set from each type in order to inves-tigate the robustness and adaptability of SSL algo-rithms for opinion detection and to test the feasibil-ity of SSL for domain adaptation.Movie Review One of the standard data sets inopinion detection is the movie review data set cre-ated by Pang and Lee (2004).
It contains 5,000 sub-jective sentences or snippets from the Rotten Toma-toes pages and 5,000 objective sentences or snip-pets from IMDB plot summaries, all in lowercase.Sentences containing less than 10 tokens were ex-cluded and the data set was labeled automaticallyby assuming opinion inheritance: every sentence inan opinion-bearing document expresses an opinion,and every sentence in a factual document is factual.Although this assumption appears to be acceptablefor movie review data, it is generally unreliable forother domains.News Article The Wall Street Journal part of thePenn Treebank III has been manually augmentedwith opinion related annotations.
This set is widelyused as a gold-standard corpus in opinion detectionresearch.
According to the coding manual (Wiebeet al, 1999), subjective sentences are those express-ing evaluations, opinions, emotions, and specula-tions.
For our research, 5,174 objective sentencesand 5,297 subjective sentences were selected basedon the absence or presence of manually labeled sub-jective expressions.JDPA Blog Post The JDPA corpus (Kessler et al,2010) is a new opinion corpus released in 2010.
Itconsists of blog posts that express opinions aboutautomobile and digital cameras with named entitiesand sentiments expressed about them manually an-notated.
For our purpose, we extracted all sentencescontaining sentiment-bearing expressions as subjec-tive sentences and manually chose objective sen-tences from the rest by eliminating subjective sen-tences that were not targeted to any labeled entities.After this process, we had approximately 10,000subjective sentences and 4,348 objective sentences.To balance the number of subjective and objectivesentences, we used 4,348 sentences from each cate-gory.4.2 Data PreparationWe removed a small number of stop words.
Nostemming was conducted since the literature showsno clear gain from stemming in opinion detection.One reason for this may be that stemming actuallyerases subtle opinion cues such as past tense verbs.All words were converted to lowercase and numberswere replaced by a placeholder #.
Both unigramsand bigrams were generated for each sentence.Each data set was randomly split into three por-tions: 5% of the sentences were selected as the eval-uation set and were not available during SSL andsupervised learning (SL) runs; 90% were treated asunlabeled data (U) for SSL runs and i% (1 ?
i ?
5)as labeled data (L) for both SL and SSL runs.
Foreach SSL run, a baseline SL run was designed withthe same number of labeled sentences (i%) and afull SL run was designed with all available sentences(90% + i%).
If effective, an SSL run would signifi-cantly outperform its corresponding baseline SL runand approach the performance of a full SL run.4.3 Experimental DesignWe conducted three groups of experiments: 1) to in-vestigate the effectiveness of the SSL approach foropinion detection; 2) to explore different co-trainingstrategies; and 3) to evaluate the applicability of SSLfor domain adaptation.4.3.1 General Settings for SSLThe Na?
?ve Bayes classifier was selected as theinitial classifier for self-training because of its abil-ity to produce prediction scores and to work wellwith small labeled data sets.
We used binary valuesfor unigram and bigram features, motivated by thebrevity of the text unit at the sentence level as wellas by the characteristics of opinion detection, whereoccurrence frequency has proven to be less influen-tial.
We implemented two feature selection options:Chi square and Information Gain.Parameters for SSL included: (1) Threshold k fornumber of iterations.
If k is set to 0, the stoppingcriterion is convergence; (2) Number of unlabeledsentences available in each iteration u (u << U );(3) Number of opinion and non-opinion sentences,p and n, to augment L during each iteration; and (4)Weighting parameter ?
for auto-labeled data.
When?
is set to 0, auto-labeled and labeled data are treated203equally; when ?
is set to 1, feature values in anauto-labeled sentence are multiplied by the predic-tion score assigned to the sentence.We used the WEKA data mining software (Hallet al, 2009) for data processing and classifica-tion of the self-training and co-training experi-ments.
EM implemented in LingPipe (Alias-i, 2008)was used for the EM-NB runs.
S3VMs imple-mented in SVMlight (Joachims, 1999) and basedon local search were adopted for the S3VM runs.Since hyper-parameter optimization for EM-NB andS3VM is not the focus of this research and prelim-inary explorations on parameter settings suggestedno significant benefit, default settings were appliedfor EM-NB and S3VM.4.3.2 Co-Training StrategiesFor co-training, we investigated five strategies forcreating two initial classifiers following the criteriathat these two classifiers either capture different fea-tures or based on different learning assumptions.Two initial classifiers were generated: (1) Us-ing unigrams and bigrams respectively to create twoclassifiers based on the assumption that low-ordern-grams and high-order n-grams contain redundantinformation and represent different views of an ex-ample: content and context; (2) Randomly splittingfeature set into two; (3) Randomly splitting train-ing set into two; (4) Applying two different learn-ing algorithms (i.e., Na?
?ve Bayes and SVM) withdifferent biases; and (5) Applying a character-basedlanguage model (CLM) and a bag-of-words (BOW)model where the former takes into consideration thesequence of words while the latter does not.
In prac-tice, for strategy (1), bigrams were used in combina-tion with unigrams because bigrams alone are weakfeatures when extracted from limited labeled data atsentence level.Auto-labeled sentences were selected if they wereassigned a label that both classifiers agreed on withhighest confidence.
Because our initial classifiers vi-olated the original co-training assumptions, forcingagreement between confident predictions improvedthe maintenance of high precision.4.3.3 Self-Training for Domain AdaptationBased on the literature and our preliminary re-sults (Yu and Ku?bler, 2010), movie reviews achieve# Labeled ExamplesType 100 200 300 400 500 allSelf-tr 85.2 86.6 87.0 87.2 86.6SL 63.8 73.6 77.2 79.4 80.2 89.4Co-tr.
92.2 93.8 92.6 93.2 91.4SL 75.8 80.8 82.6 85.2 84.8 95.2EM-NB 88.1 88.7 88.6 88.4 89.0SL 73.5 78.7 81.3 82.8 83.9 91.6S3VM 59.0 68.4 67.8 67.0 75.2SL 70.0 72.8 75.6 76.2 80.0 90.0Table 1: Classification accuracy(%) of SSL and SL onmovie reviewsthe highest accuracy while news articles and blogreviews are considerably more challenging.
Thus,we decided to use movie reviews as source dataand news articles and blog posts as target data do-mains.
While the data split for the target domain re-mains the same as in section 4.2, all sentences in thesource domain, except for the 5% evaluation data,were treated as labeled data.
For example, in orderto identify opinion-bearing sentences from the blogdata set, all 9,500 movie review sentences and i%of blog sentences were used as labeled data, 90% ofblog sentences were used as unlabeled data, and 5%as evaluation data.
We also applied a parameter togradually decrease the weight of the source domaindata, similar to the work done by Tan et al (2009).5 Results and EvaluationOverall, our results suggest that SSL improves ac-curacy for opinion detection although the contribu-tion of SSL varies across data domains and differentstrategies need to be applied to achieve optimizedperformance.
For the movie review data set, almostall SSL runs outperformed their corresponding base-line SL runs and approached full SL runs; for thenews article data set, SSL performance followed asimilar trend but with only a small rate of increase;for the blog post data set, SSL runs using only blogdata showed no benefits over the SL baseline, butwith labeled movie review data, SSL runs producedresults comparable with full SL result.5.1 SSL vs. SLTable 1 reports the performance of SSL and SL runson movie review data based on different numbers204of initial labeled sentences.
Both the self- and co-training runs reported here used the same parame-ter settings: k=0, u=20, p=2, n=2, ?
=0, with nofeature selection.
The co-training results in Table1 used a CLM and a BOW model (see section 5.2).SL runs for co-training classified sentences based onthe highest score generated by two classifiers; SLruns for S3VM applied the default SVM setting inSVMlight; and SL runs for EM-NB used the Na?
?veBayes classifier in the EM-NB implementation inLingPipe.Table 1 shows that, except for S3VM, SSL al-ways outperforms the corresponding SL baseline onmovie reviews: When SSL converges, it achievesimprovement in the range of 8% to 34% over the SLbaseline.
The fewer initial labeled data, the morebenefits an SSL run gained from using unlabeleddata.
For example, using 100 labeled sentences, self-training achieved a classification accuracy of 85.2%and outperformed the baseline SL by 33.5%.
Al-though this SSL run was surpassed by 4.9% by thefull SL run using all labeled data, a great amountof effort was saved by labeling only 100 sentencesrather than 9,500.
Co-training produced the bestSSL results.
For example, with only 200 labeledsentences, co-training yielded accuracy as high as93.8%.
Overall, SSL for opinion detection on moviereviews shows similar trends to SSL for traditionaltopical classification (Nigam and Ghani, 2000).However, the advantages of SSL were not as sig-nificant in other data domains.
Figure 1 demon-strates the performance of four types of SSL runsrelative to corresponding baseline and full SL runsfor all three data sets.
All SSL runs reported hereused 5% data as labeled data.
Lines with differentpatterns indicate different data sets, green trianglesmark baseline SL runs, green dots mark full SL runs,and red crosses mark SSL runs.
Numbers next tosymbols indicate classification accuracy.
For eachline, if the red cross is located above the triangle,it indicates that the SSL run improved over the SLbaseline; and, the closer the red cross to the upperdot, the more effective was the SSL run.
Figure 1shows that S3VM degrades in performance for allthree data sets and we exclude it from the follow-ing discussion.
From movie reviews to news articlesto blog posts, the classification accuracy of baselineSL runs as well as the improvement gained by SSLFigure 1: Classification accuracy(%) of SSL and SL onthree data sets (i=5)runs decreased: With greater than 80% baseline ac-curacy on movie reviews, SSL runs were most effec-tive; with slightly above 70% baseline accuracy onnews articles, self-training actually decreased per-formance of the corresponding SL baseline whileco-training and EM-NB outperformed the SL base-line only slightly; and with 60% or so baseline accu-racy on blog posts, none of the SSL methods showedimprovement.
We assume that the lower the baselineaccuracy, the worse the quality of auto-labeled data,and, therefore, the less advantages is application ofSSL.
We also found that the average sentence lengthin blog posts (17 words) is shorter than the averagesentence length in either movie reviews (23.5 words)or news articles (22.5 words), which posed an addi-tional challenge because there is less information forthe classifier in terms of numbers of features.Overall, for movie reviews and news articles, co-training proved to be most robust and effective andEM-NB showed consistent improvement over theSL baseline.
For news articles, EM-NB increasedaccuracy from 63.5% to 68.8% with only 100 la-beled sentences.
For movie reviews, a close look atEM-NB iterations shows that, with only 32 labeledsentences, EM-NB was able to achieve 88% clas-sification accuracy, which is close to the best per-formance of simple Na?
?ve Bayes self-training using300 labeled sentences.
This implies that the prob-205Figure 2: Performance of four co-training strategies on movie review datalem space of opinion detection may be successfullydescribed by the mixture model assumption of EM.As for blog posts, since the performance of the base-line classifiers was only slightly better than chance(50%), we needed to improve the baseline accuracyin order for SSL to work.
One solution was to intro-duce high quality features.
We augmented featureset with domain independent opinion lexicons thathave been suggested as effective in creating highprecision opinion classifier, but improvement wasonly minimal.
An alternative solution was to bor-row more labeled data from non-blog domains(s).Section 5.3 discusses dealing with a ?difficult?
datadomain using data from an ?easy?
domain.The preliminary exploration of different parame-ter settings for both self- and co-training showed nosignificant benefit gained by setting the weight pa-rameter ?
or applying feature selection; and usinga larger number of unlabeled sentences u availablefor each iteration did not improve results.
Furtherinvestigation is needed for an in-depth explanation.5.2 Co-trainingThe best co-training runs reported in Table 1 andFigure 1 used an 8-grams CLM to train one clas-sifier and a BOW model to train the other classifier.These two classifiers differ both in feature represen-tation (i.e., character vs. word) and in learning al-gorithm (language model vs. pure statistical model).To investigate whether the two different classifiersimprove each other?s performance during iterations,we analyzed the CLM and BOW classifiers individ-ually.
When comparing the BOW classifier duringco-training iterations to the performance of corre-sponding SL runs based on BOW, the former us-ing both CLM and BOW classifiers always outper-formed the latter, indicating that the BOW classi-fier learned from CLM.
Similarly, the CLM classi-fier also gained from the BOW classifier during co-training.Figure 2 shows that for the movie review do-main, other simple co-training configurations alsoproduced promising results by using different fea-ture sets (e.g., unigrams and the union of unigramsand bigrams, or randomly split feature sets) or differ-ent training sets.
In the news domain, we observedsimilar trends.
This shows the robustness and greatpotential of co-training.
Because even with the lo-gistic model to output probabilistic scores for theSVM classifier, the difference in probabilities wastoo small to select a small number of top predic-tions, adding an SVM classifier for co-training did206not improve accuracy and is not discussed here.An observation of the performance of self-training and co-training over iterations confirmedthat co-training used labeled data more effectivelyfor opinion detection than self-training, as sug-gested for traditional topical classification.
Wefound that, overall, co-training produces better per-formance than self-training and reaches optimal per-formance faster.
For instance, with 500 labeled sen-tences, a self-training run reached an optimal classi-fication accuracy of 88.2% after adding 4,828 auto-matically annotated sentences for training, while theco-training run reached an optimal performance of89.4% after adding only 2,588 sentences.5.3 Domain TransferEven without any explicit domain adaptation meth-ods, results indicate that simple self-training aloneis promising for tackling domain transfer betweenthe source domain movie reviews and the target do-mains news articles and blog posts.Target domain news articles We used 9,500 la-beled movie review sentences to train a Na?
?ve Bayesclassifier for news articles.
Although this classi-fier produced a fairly good classification accuracyof 89.2% on movie review data, its accuracy waspoor (64.1%) on news data (i.e., domain-transferSL), demonstrating the severity of the domain trans-fer problem.
Self-training with Na?
?ve Bayes usingunlabeled data from the news domain (i.e., domain-transfer SSL run) improved the situation somewhat:it achieved a classification accuracy of 75.1% sur-passing the domain-transfer SL run by more than17%.
To finvestigate how well SSL handles the do-main transfer problem, a full in-domain SL run thatused all labeled news sentences was also performed.This full SL run achieved 76.9% classification accu-racy, only 1.8% higher than the domain-transfer SSLrun, which did not use any labeled news data.Target domain blog posts Because blog data aremore challenging than news data, we kept 5% blogdata as labeled data.
Both SSL runs with and withoutout-of-domain data are depicted in Figure 3.
Self-training using only blog data decreases SL baselineperformance (dashed black line).
Keeping the samesettings, we added additional labeled data from themovie reviews, and self-training (gray line) cameFigure 3: Self-training for domain transfer betweenmovie reviews (source domain) and blogs (target domain)closer to the performance of the full SL run (redline), which used 90% of the labeled blog data.
Wethen added a control factor that reduced the impactof movie review data gradually (i.e., a decrease of0.001 in each iteration).
Using this control, the self-training run (solid black line) reached and occasion-ally exceeded the performance of the full SL run.6 Conclusion and Future WorkWe investigated major SSL methods for identify-ing opinionated sentences in three domains.
Formovie review data, SSL methods attained state-of-the-art results with a small number of labeled sen-tences.
Even without a natural feature split, dif-ferent co-training strategies increased the baselineSL performance and outperformed other SSL meth-ods.
Due to the nature of the movie review data, wesuspect that opinion detection on movie reviews isan ?easy?
problem because it relies, strictly speak-ing, on distinguishing movie reviews from plot sum-maries, which also involves genre classification.
Forother manually created data sets that are expectedto reflect real opinion characteristics, the SSL ap-proach was impeded by low baseline precision andshowed limited improvement.
With the addition ofout-of-domain labeled data, however, self-trainingexceeded full SL.
This constitutes a successful newapproach to domain adaptation.Future work will include integrating opinion lex-icons to bootstrap baseline precision and exploringco-training for domain adaptation.207ReferencesAlias-i.
2008.
LingPipe (version 4.0.1).
Available fromhttp://alias-i.com/lingpipe.Anthony Aue and Michel Gamon.
2005.
Customizingsentiment classifiers to new domains: A case study.
InProceedings of the International Conference on RecentAdvances in Natural Language Processing (RANLP),Borovets, Bulgaria.John Blitzer, Mark Dredze, and Fernando Pereira.
2007.Biographies, Bollywood, boom-boxes and blenders:Domain adaptation for sentiment classification.
InProceedings of the 45th Annual Meeting of the Associ-ation of Computational Linguistics (ACL), pages 440?447, Prague, Czech Republic.Avrim Blum and Tom Mitchell.
1998.
Combining la-beled and unlabeled data with co-training.
In Proceed-ings of the 11th Annual Conference on ComputationalLearning Theory, pages 92?100, Madison, WI.Paula Chesley, Bruce Vincent, Li Xu, and Rohini K. Sri-hari.
2006.
Using verbs and adjectives to automati-cally classify blog sentiment.
In Proceedings of AAAI-CAAW-06, the Spring Symposia on Computational Ap-proaches to Analyzing Weblogs, Menlo Park, CA.Hagen Fu?rstenau and Mirella Lapata.
2009.
Semi-supervised semantic role labeling.
In Proceedings ofthe 12th Conference of the European Chapter of theACL (EACL), pages 220?228, Athens, Greece.Mark Hall, Eibe Frank, Geoffrey Holmes, BernhardPfahringer, Peter Reutemann, and Ian H. Witten.2009.
The WEKA data mining software: An update.SIGKDD Explorations, 11(1).Wei Jin, Hung Hay Ho, and Rohini K. Srihari.
2009.OpinionMiner: A novel machine learning system forweb opinion mining.
In Proceedings of the 15thACM SIGKDD International Conference on Knowl-edge Discovery and Data Mining, Paris, France.Thorsten Joachims.
1999.
Making large-scale SVMlearning practical.
In B. Scho?lkopf, C. Burges, andA.
Smola, editors, Advances in Kernel Methods - Sup-port Vector Learning.
MIT-Press.Jason S. Kessler, Miriam Eckert, Lyndsie Clark, andNicolas Nicolov.
2010.
The ICWSM 2010 JDPA sen-timent corpus for the automotive domain.
In 4th Inter-national AAAI Conference on Weblogs and Social Me-dia Data Workshop Challenge (ICWSM-DWC), Wash-ington, D.C.Kamal Nigam and Rayid Ghani.
2000.
Analyzing theeffectiveness and applicability of co-training.
In Pro-ceedings of the Ninth International Conference on In-formation and Knowledge Management, McLean, VA.Kamal Nigam, Andrew Kachites Mccallum, SebastianThrun, and Tom Mitchell.
1999.
Text classificationfrom labeled and unlabeled documents using EM.
Ma-chine Learning, 39:103?134.Bo Pang and Lillian Lee.
2004.
A sentimental edu-cation: Sentiment analysis using subjectivity summa-rization based on minimum cuts.
In Proceedings ofthe 42nd Annual Meeting on Association for Compu-tational Linguistics, Barcelona, Spain.Ellen Riloff and Janyce Wiebe.
2003.
Learning extrac-tion patterns for subjective expressions.
In Proceed-ings of the Conference on Empirical Methods in Natu-ral Language Processing (EMNLP), Sapporo, Japan.Hiroya Takamura, Takashi Inui, and Manabu Okumura.2006.
Latent variable models for semantic orientationsof phrases.
In Proceedings of the 11th Conference ofthe European Chapter of the Association for Compu-tational Linguistics (EACL), Trento, Italy.Partha Pratim Talukdar and Fernando Pereira.
2010.Experiments in graph-based semi-supervised learningmethods for class-instance acquisition.
In Proceed-ings of the 48th Annual Meeting of the Association forComputational Linguistics (ACL), pages 1473?1481,Uppsala, Sweden.Songbo Tan, Xueqi Cheng, Yufen Wang, and Hongbo Xu.2009.
Adapting naive Bayes to domain adaptation forsentiment analysis.
In Proceedings of the 31st Eu-ropean Conference on Information Retrieval (ECIR),Toulouse, France.Wei Wang and Zhi-Hua Zhou.
2007.
Analyzing co-training style algorithms.
In Proceedings of the 18thEuropean Conference on Machine Learning, Warsaw,Poland.Janyce Wiebe and Ellen Riloff.
2005.
Creating sub-jective and objective sentence classifiers from unan-notated texts.
In Proceedings of the 6th InternationalConference on Intelligent Text Processing and Compu-tational Linguistics (CICLing), Mexico City, Mexico.Janyce Wiebe, Rebecca Bruce, and Thomas O?Hara.1999.
Development and use of a gold standard dataset for subjectivity classifications.
In Proceedings ofthe 37th Annual Meeting of the Association for Com-putational Linguistics (ACL), College Park, MD.Janyce Wiebe, Rebecca Bruce, Matthew Bell, MelanieMartin, and Theresa Wilson.
2001.
A corpus study ofevaluative and speculative language.
In Proceedingsof the 2nd ACL SIGdial Workshop on Discourse andDialogue, Aalborg, Denmark.Janyce Wiebe, Theresa Wilson, Rebecca Bruce, MatthewBell, and Melanie Martin.
2004.
Learning subjectivelanguage.
Computational Linguistics, 30(3):277?308.Kiduk Yang, Ning Yu, and Hui Zhang.
2007.
WIDITin TREC-2007 blog track: Combining lexicon-basedmethods to detect opinionated blogs.
In Proceed-ings of the 16th Text Retrieval Conference (TREC),Gaithersburg, MD.208Hong Yu and Vasileios Hatzivassiloglou.
2003.
Towardsanswering opinion questions: Separating facts fromopinions and identifying the polarity of opinion sen-tences.
In Proceedings of the Conference on EmpiricalMethods in Natural Language Processing (EMNLP),Sapporo, Japan.Ning Yu and Sandra Ku?bler.
2010.
Semi-supervisedlearning for opinion detection.
In Proceedings of theIEEE/WIC/ACM International Conference on Web In-telligence and Intelligent Agent Technology, volume 3,pages 249?252, Toronto, Canada.Wei Zhang and Clement Yu.
2007.
UIC at TREC 2007blog track.
In Proceedings of the 16th Text RetrievalConference (TREC), Gaithersburg, MD.209
