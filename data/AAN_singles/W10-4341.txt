Proceedings of SIGDIAL 2010: the 11th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 229?232,The University of Tokyo, September 24-25, 2010. c?2010 Association for Computational LinguisticsPamini: A framework for assembling mixed-initiative human-robotinteraction from generic interaction patternsJulia Peltason and Britta WredeApplied Informatics, Faculty of TechnologyBielefeld University, Germanyjpeltaso, bwrede@techfak.uni-bielefeld.deAbstractDialog modeling in robotics suffers fromlack of generalizability, due to the factthat the dialog is heavily influenced bythe tasks the robot is able to perform.We introduce interleaving interaction pat-terns together with a general protocol fortask communication which enables us tosystematically specify the relationship be-tween dialog structure and task structure.We argue that this approach meets the re-quirements of advanced dialog modelingon robots and at the same time exhibits abetter scalability than existing concepts.1 IntroductionThe need for interaction modeling on robots iswidely acknowledged, not only for instructingthem but also for enabling them to learn from hu-mans within interaction.
Yet, today?s robotic sys-tems often do not have a dedicated dialog systembut employ simple command-matching techniques(e.g.
(Bo?hme et al, 2003)).
Other systems relyon finite-state based dialog managers (e.g.
(Baueret al, 2009)) that couple dialog and task man-agement which hampers maintainability and reuseand does not scale well for more complex interac-tion scenarios.On the other hand, concepts for reusable dialogframeworks have been established within the spo-ken dialog community for traditional information-seeking domains where the system first collectsthe required parameters, then presents the desiredinformation to the user, e.g.
an accommodationor travel information (e.g.
(Bohus and Rudnicky,2009)).
However, these concepts are not directlytransferable to robotics, due to the situated natureof human-robot interaction.
A robot perceives theworld and acts on it ?
and it is supposed to com-municate about that.
For instance, a robot manip-Figure 1: The Curious Robot set-up.ulating objects needs to be able to verbalize its in-tended actions and give feedback about its internalstate, but it also needs to react on feedback com-ments or corrections uttered by the human tutor.
Inorder to equip a robot with such fine-grained com-munication skills, tight integration of action exe-cution into interaction is required.
In simple terms,while the dialog frameworks mentioned can be re-ferred to as information-oriented, we need ratheraction-oriented approaches for robotics.R: What is that?
?points?H: This is a lemon.R: A melon.
OK.H: No, I said ?lemon?
!R: A lemon.
OK.R: How should I grasp the lemon?H: That way.
?demonstrates grip?R: Alright, with the power grasp.R: I am going to grasp it ?starts grasping?R: I start grasping now.H: Robot, what objects do you know?R: Lemon.R: OK, I finished grasping.
?stops grasping?Table 1: A typical interaction with the Curious Robot.2 The Pamini frameworkThis paper presents the Pamini (Pattern-basedmixed-initiative) human-robot interaction frame-work.
Pamini proposes a new approach for dialog229modeling on robots that includes (1) a task-stateprotocol providing a fine-grained interface to thecomplex domain processing of the robotic systemand (2) the concept of generic interaction patternsthat support rapid prototyping of human-robot in-teractions and can be combined in a flexible way.Previous versions of the Pamini frameworkhave been applied in several mixed-initiativelearning scenarios.
For example, in the Home-Tour scenario a mobile robot builds up a spatialmodel of its environment and gradually improvesits model by attempting to obtain information fromthe human (Peltason et al, 2009).
In the CuriousRobot scenario shown in figure 1, an anthropomor-phic robot learns to label and grasp objects, apply-ing a proactive dialog strategy that provides guid-ance for untrained users (Lu?tkebohle et al, 2009).A dialog excerpt is shown in table 1.2.1 The task state protocolA dialog system for robotics needs to coordinatewith a number of components, e.g.
for perceptualanalysis, motor control or components generatingnonverbal feedback.
To realize this, we use theconcept of tasks that can be performed by com-ponents.
Tasks are described by an execution stateand a task specification containing the informationrequired for execution.
A protocol specifies taskstates relevant for coordination and possible tran-sitions between them as shown in figure 2.
Taskupdates, i.e.
updates of the task state and possi-bly the task specification, cause event notificationswhich are delivered to the participating compo-nents whereupon they take an appropriate action.update requestedrunningcancel requestedinitiatedCANCELLEDDONEfailedacceptedresult_availablerejected cancel_failedcancelaccept, rejectupdateFigure 2: The task life-cycle.
A task gets initiated, ac-cepted, may be cancelled or updated, may deliver intermedi-ate results and finally is completed.
Alternatively, it can berejected by the handling component or execution may fail.Tight integration with action execution Arobot performing e.g.
a grasp action supervisedby the human requires multi-step communicationbetween the dialog system and the arm control asillustrated in figure 3.
Generally, with the acceptedstate, the proposed protocol enables the dialog sys-tem to provide feedback during slow-going actionsindicating the internal system state.
Further, withthe update and result available states, it supportsthe modification of the task specification duringexecution and thus gives the robot the ability toreact to comments, corrections and commands on-the-fly.Arm ControlSpeech recognition Text-to-Speech Dialogaccept Grasp3:cancel_failed7:complete Grasp9:receive (Grasp the apple)1:receive (Stop)5:initiate Grasp2: say(I am going to grasp the apple.
)4:cancel Grasp6:say(I can not stop)8:say(I finished)10:Figure 3: The component communication for a grasp ac-tion requested by the human.
As the dialog manager (DLG)receives the grasp command, it initiates a grasp task whichis accepted by the arm control.
The DLG is notified aboutthe task state update and acknowledges task execution.
Asthe human commands cancelling, the DLG sets the task statecancel.
Since the arm control fails to cancel the task, it setsthe task state cancel failed which the DLG reacts on by ex-pressing an excuse.
Finally the task is completed, and theDLG acknowledges successful task execution.Mixed-initiative interaction The Pamini dialogmanager offers dialog tasks for other components,e.g.
greeting the human, informing the humanabout anything or conversely requesting informa-tion from the human.
While human initiative is re-alized whenever input from a speech understand-ing component is received, robot initiative occurswhen a system component requests a dialog task tobe executed.
Situation permitting, the dialog man-ager will accept the dialog task, go into interactionwith the human, and finally complete the dialogtask.
Thus, it can react to the system?s and the hu-man?s initiative using the same task-state protocolLearning within interaction The task state pro-tocol supports robotic learning within interactionby establishing mechanisms for information trans-fer from the dialog system to the robotic sub-system.
Once information is available from thehuman, Pamini augments the task specification230with the new information and sets the task stateresult available.
Since this transition may betaken multiple times, given information can becorrected.
Also, mixed-initiative enables activelearning, where the learner provokes a situationproviding new information instead of waiting un-til such situation eventually presents itself.2.2 Interaction patternsIn an interaction, dialog acts are not unrelatedevents, but form coherent sequences.
For exam-ple, a question is usually followed by an answer,and a request is typically either accepted or re-jected.
Influenced by the concepts of adjacencypairs (Schegloff and Sacks, 1973), conversationpolicies (Winograd, 1986) and software designpatterns, we propose the concept of interactionpatterns that describe recurring dialog structureson a high level.
Interaction patterns can be formal-ized as transducer augmented with internal stateactions, consisting of?
a set of human dialog acts H and a set of robot dialogacts R, e.g.
H.request or R.assert;?
a set of incoming task events T , e.g.
accepted or failed;?
a set of states S representing the interaction state;?
a set of actions A the dialog manager performs, e.g.initiating or updating a task or reset interaction;?
an input alphabet ?
?
(H ?
T );?
an output alphabet ?
?
R;?
a transition function T : S ?
??
??
S ?A?
?
?
?.By admitting task events as input and internalactions that perform task initiation and update,the dialog level is linked with the domain level.The patterns have been implemented as statecharts(Harel, 1987), an extended form of finite state ma-chines, which provides both an executable modeland an understandable graphical representation asshown in figure 5.
For instance, the cancellablestate nameaction, when enteredH.dialog-act / state nameH.dialog-act / R.dialog-act state nametask event / R.dialog-act //Figure 5: Interaction patterns are represented as transducerthat takes as input human dialog acts and task events and pro-duces robot dialog acts as output.action request pattern shown in figure 4 describesan action request initiated by the human that canbe cancelled during execution.
The normal courseof events is that the human requests the action tobe executed, the dialog manager initiates the do-main task, the responsible system component ac-cepts execution so that the dialog manager willassert execution.
Finally, the task is completedand the robot acknowledges.
In contrast, the cor-rectable information request pattern is initiated bythe human.
Here, on receiving the respective di-alog task request, the dialog manager will ask forthe desired information and accept the dialog task.Once the human provides the answer, the robotwill repeat it as implicit confirmation that can becorrected if necessary.
Table 2 lists all patterns thathave been identified so far.Initiated by user Initiated by robotCancellable action request Self-initiated cancellable actionSimple action request Self-initiated simple actionInformation request Correctable information requestInteraction opening Simple information requestInteraction closing ClarificationInteraction restartSystem resetTable 2: Available interaction patterns.Pattern configuration The patterns themselvesdo not determine what kind of task is to be ex-ecuted or what kind of information to obtain ex-actly.
These specifics are defined by the configu-ration associated with each pattern, and a concretescenario is realized by configuring a set of patternsusing a domain-specific language and registeringthem with the dialog manager.In detail, it needs to be specified for the human?sdialog acts what kind of (possibly multimodal) in-put is interpreted as a given dialog act which isdone by formulating conditions over the input.
Forthe robot?s dialog acts, their surface form needs tobe specified.
Up to now, speech output and point-ing gestures are implemented as output modalitiesand can be combined.
Moreover, also the taskcommunication needs to be configured, i.e.
thetask specification itself as well as possible taskspecification updates.
In addition, the developercan define context variables and use them to pa-rameterize the robot?s dialog acts and in task spec-ification updates.
This is how e.g.
for the robot?sinformation request the answer is transferred fromthe human to the responsible system component.Interleaving patterns during interaction Dur-ing interaction, the registered patterns are em-ployed in a flexible way by admitting patterns tobe interrupted by other patterns and possibly re-sumed later which leads to interleaving patterns.By default, simpler patterns are permitted to benested within temporally extended patterns.
Forexample, it seems reasonable to permit monitoringquestions uttered by the human to be embedded inthe robot?s slow-going grasp execution as shown231initiateinitiate-system-task(ShortTerm)stae nemH.d.
asserted mimHeloHgm-ogcce/HeR.d.ktgmmeaHrefusedmimHeloHgm-oaevecHeR.d.ktaenme cancel_requestedupdate-system-task-state(abort)stcgce.d.failed mimHeloHgm-ogeR.d.ktg/eterminated mimHeloHgm-ocl/eH R.d.ktgc-eRestcgce.d.
.d..d. .d.mimHeloHgm-ocgceogeR.d.ktaenme mimHeloHgm-ocgceeR.d.ktgc-eRest ae nm sHe.
dsil o.Hsta.tHsHegs- - emHe c/RkvnetHdl  stee vsteR/RemesHe smmi.
dsil o.Hsta.tme- .nm sHenm sHe.
dsil o.Hsta.tHsHegetniHssdisiec/RkvemesHv- l e-HR/R - l e nm sHe.
dsil o.Hsta.tHsHeg- l mieHe c R/R  dt - l e veosHeR/R/R/Rkv- l tH.netHdl Figure 4: Two example interaction patterns.
Cancellable action request: an action request which is initiated by the humanand can be cancelled during execution.
Correctable information request: an information request with implicit confirmationinitiated by the robot where information can be corrected later if necessary.in table 1 which equips the robot with multitaskingcapabilities.
Interleaving is realized by organizingactive patterns on a stack.
Whenever an input isreceived, the dialog manager attempts to interpretit in the context provided by the topmost pattern.If it fails, the lower and inactive patterns are tried.3 Discussion and ConclusionThe presented approach to dialog modeling onrobots relies on the concept of interaction patternsthat constitute configurable (and thus reusable)building blocks of interaction.
A fine-grained taskstate protocol links dialog and domain level.
Withinterleaving patterns, flexible dialog modeling isachieved that goes beyond current state-of-the-artdialog modeling on robots.
Further, by encapsulat-ing both the subtleties of dialog management andthe complexity of component integration, the pro-posed interaction patterns support rapid prototyp-ing of human-robot interaction scenarios.The evaluation of the approach needs to exam-ine framework usability, framework functionalityand usability of the resulting dialogs.
With respectto framework usability, we already showed thatdevelopers unfamiliar with the framework wereable to build a simple interaction scenario withinone hour (Peltason and Wrede, 2010).
With re-spect to framework functionality, we demonstratedthat the robot?s mixed-initiative interaction capa-bilities enable human and robot in the Home-Tourscenario to jointly build up a common represen-tation of their environment and even compensatefor classification errors (Peltason et al, 2009).As to dialog usability, a video study indicatesthat the Curious Robot?s proactive dialog strat-egy guides unexperienced users (Lu?tkebohle et al,2009).
Further, given a dialog system architecturethat supports rapid prototyping, comparative stud-ies become possible.
Therefore, we currently pre-pare a study to compare the curiosity strategy witha user-directed strategy that provides more free-dom but also more uncertainty to the user.
Lastbut not least, we will evaluate the patterns them-selves and pattern interleavability.
Are users likelyto interrupt a robot?s action by asking questions oreven giving new commands?
Also, are there otherkinds of interaction patterns that occur in a real in-teraction but are not captured yet?ReferencesA.
Bauer, D. Wollherr, and M. Buss.
2009.
Information re-trieval system for human-robot communication asking fordirections.
In International Conference on Robotics andAutomation.H.-J.
Bo?hme, T. Wilhelm, J.
Key, C. Schauer, C. Schro?ter,H.-M.
Gro?, and T. Hempel.
2003.
An approach to multi-modal human-machine interaction for intelligent servicerobots.
Robotics and Autonomous Systems, 44(1).D.
Bohus and A. I. Rudnicky.
2009.
The ravenclaw dialogmanagement framework: Architecture and systems.
Com-puter Speech & Language, 23(3):332?361.D.
Harel.
1987.
Statecharts: A visual formalism for complexsystems.
Science of Computer Programming, 8:231?274.I.
Lu?tkebohle, J. Peltason, L. Schillingmann, C. Elbrechter,B.
Wrede, S. Wachsmuth, and R. Haschke.
2009.
Thecurious robot - structuring interactive robot learning.
InInternational Conference on Robotics and Automation.J.
Peltason and B. Wrede.
2010.
Modeling human-robot in-teraction based on generic interaction patterns.
In AAAITechnical Report: Dialog with Robots.
submitted.J.
Peltason, F. Siepmann, T. Spexard, B. Wrede, M. Han-heide, and E. Topp.
2009.
Mixed-initiative in humanaugmented mapping.
In International Conference onRobotics and Automation.E.
A. Schegloff and H. Sacks.
1973.
Opening up closings.Semiotica, 8(4):289?327.T.
Winograd.
1986.
A language/action perspective on thedesign of cooperative work.
In Conference on Computer-supported cooperative work.232
