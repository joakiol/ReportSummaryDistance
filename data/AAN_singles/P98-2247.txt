Detecting Verbal Participation in Diathesis AlternationsDiana  McCar thyCognitive & Computing Sciences,University of SussexBrighton BN1 9QH, UKAnna KorhonenComputer Laboratory,University of Cambridge, Pembroke Street,Cambridge CB2 3QG, UKAbstractWe present a method for automatically identi-fying verbal participation in diathesis alterna-tions.
Automatically acquired subcategoriza-tion frames are compared to a hand-crafted clas-sification for selecting candidate verbs.
Themin imum description length principle is thenused to produce a model and cost for storing thehead noun instances from a training corpus atthe relevant argument slots.
Alternating sub-categorization frames are identified where thedata from corresponding argument slots in therespective frames can be combined to producea cheaper model than that produced if the datais encoded separately.
I.1 Introduct ionDiathesis alternations are regular variations inthe syntactic expressions of verbal arguments,for example The boy broke the window ~-.
Thewindow broke.
Levin's (1993) investigation ofalternations summarises the research done anddemonstrates the utility of alternation i forma-tion for classifying verbs.
Some studies have re-cently recognised the potential for using diathe-sis alternations within automatic lexical acquisi-tion (Ribas, 1995; Korhonen, 1997; Briscoe andCarroll, 1997).This paper shows how corpus data can beused to automatically detect which verbs un-dergo these alternations.
Automatic acquisi-tion avoids the costly overheads of a manualapproach and allows for the fact that pred-icate behaviour varies between sublanguages,domains and across time.
Subcategorizationframes (SCFs) are acquired for each verb and1This work was partially funded by CEC LE1 project"SPARKLE".
We also acknowledge support from UKEPSRC project "PSET: Practical Simplification of En-glish Text".a hand-crafted classification of diathesis alter-nations filters potential candidates with thecorrect SCFs.
Models representing the selec-tional preferences ofeach verb for the argumentslots under consideration are then used to indi-cate cases where the underlying arguments haveswitched position in alternating SCFs.
The se-lectional preferences models are produced fromargument head data stored specific to SCF andslot.The preference models are obtained using theminimum description length (MDL) principle.MDL selects an appropriate model by compar-ing potential candidates in terms of the cost ofstoring the model and the data stored using thatmodel for each set of argument head data.
Wecompare the cost of representing the data at al-ternating argument slots separately with thatwhen the data is combined to indicate evidencefor participation in an alternation.2 SCF  Ident i f i ca t ionThe SCFs applicable to each verb are extractedautomatically from corpus data using the sys-tem of Briscoe and Carroll (1997).
This compre-hensive verbal acquisition system distinguishes160 verbal SCFs.
It produces a lexicon of verbentries each organised by SCF with argumenthead instances enumerated at each slot.The hand-crafted iathesis alternation clas-sification links Levin's (1993) index of alterna-tions with the 160 SCFs to indicate which classesare involved in alternations.3 Se lect iona l  P re ference  Acqu is i t ionSelectional preferences can be obtained for thesubject, object and prepositional phrase slotsfor any specified SCF classes.
The input dataincludes the target verb, SCF and slot alongwith the noun frequency data and any prepo-1493sition (for PPs).
Selectional preferences arerepresented as Association Tree Cut Models(ATCMS) aS described by Abe and Li (1996).These are sets of classes which cut across theWordNet hypernym noun hierarchy (Miller etal., 1993) covering all leaves disjointly.
Associ-ation scores, given by ~ are calculated for p(c) 'the classes.
These scores are calculated fromthe frequency of nouns occurring with the tar-get verb and irrespective of the verb.
The scoreindicates the degree of preference between theclass (c) and the verb (v) at the specified slot.Part of the ATCM for the direct object slot ofbuild is shown in Figure 1.
For another verb adifferent level for the cut might be required.
Forexample eat might require a cut at the FOODhyponym of OB JECT.Finding the best set of classes is key to ob-taining a good preference model.
Abe and Liuse MDL to  do  this.
MDL is a principle from in-formation theory (Rissanen, 1978) which statesthat the best model minimises the sum of i thenumber of bits to encode the model, and ii thenumber of bits to encode the data in the model.This makes the compromise between a simplemodel and one which describes the data effi-ciently.Abe and Li use a method of encoding tree cutmodels using estimated frequency and probabil-ity distributions for the data description length.The sample size and number of classes in thecut are used for the model description length.They provide a way of obtaining the ATCMS us-ing the identity p(clv ) = A(c, v) ?
p(c).
Initiallya tree cut model is obtained for the marginalprobability p(c) for the target slot irrespectiveof the verb.
This is then used with the condi-tional data and probability distribution p(clv )to obtain an ATCM aS a by-product of obtainingthe model for the conditional data.
The actualcomparison used to decide between two cuts iscalculated as in equation 1 where C representsthe set of classes on the cut model currentlybeing examined and Sv represents the samplespecific to the target verb.
2.IClloglSvl + -freqc x log P(ClV) (1)2 o,c p(c)In determining the preferences the actual en-SAil logarithms are to the base 2\ [~"  }Figure 1: ATCM for build Object slotcoding in bits is not required, only the relativecost of the cut models being considered.
TheWordNet hierarchy is searched top down to findthe best set of classes under each node by locallycomparing the description length at the nodewith the best found beneath.
The final com-parison is done between a cut at the root andthe best cut found beneath this.
Where detailis warranted by the specificity of the data thisis manifested in an appropriate l vel of general-isation.
The description length of the resultantcut model is then used for detecting diathesisalternations.4 Ev idence  for  D ia thes isA l te rnat ionsFor verbs participating in an alternation onemight expect that the data in the alternatingslots of the respective SCFs might be rather ho-mogenous.
This will depend on the extent towhich the alternation applies to the predomi-nant sense of the verb and the majority of sensesof the arguments.
The hypothesis here is thatif the alternation is reasonably productive andcould occur for a substantial majority of the in-stances then the preferences at the correspond-ing slots should be similar.
Moreover we hy-pothesis that if the data at the alternating slotsis combined then the cost of encoding this datain one ATCM will be less than the cost of encod-ing the data in separate models, for the respec-tive slot and SCF.Taking the causative-inchoative alternationas an example, the object of the transitive frameswitches to the subject of the intransitive frame:The boy broke the window ~ The window broke.Our strategy is to find the cost of encoding thedata from both slots in separate ATCMS andcompare it to the cost of encoding the combineddata.
Thus the cost of an ATCM for / the sub-1494Table 1: Causative-Inchoative Evaluationverbstrue positives begin end Changeswingfalse positives cuttrue negatives choose like helpcharge expect addfeel believe askfalse negativestotalmove91I115ject of the intransitive and ii the object of thetransitive should exceed the cost of an ATCM forthe combined ata only for verbs to which thealternation applies.5 Exper imenta l  Resu l tsA subcategorization lexicon was produced from10.8 million words of parsed text from theBritish National Corpus.
In this preliminarywork a small sample of 30 verbs were examined.These were selected for the range of SCFs thatthey exhibit.
The primary alternation selectedwas the causative-inchoative because a reason-able number of these verbs (15) take both sub-categorization frames involved.
ATCM modelswere obtained for the data at the subject of theintransitive frame and object of the transitive.The cost of these models was then compared tothe cost of the model produced when the twodata sets were combined.Table 1 shows the results for the 15 verbswhich took both the necessary frames.
The sys-tem's decision as to whether the verb partici-pates in the alternation or not was comparedto the verdict of a human judge.
The accuracywas 87%( 4+9 ~ Random choice would give ~, 4+1+9+1/"a baseline of 50%.
The cause for the one falsepositive cut was that cut takes the middle alter-nation (The butcher cuts the meat ~-~ the meatcuts easily).
This alternation cannot be distin-guished from the causative-inchoative becausethe scF acquisition system drops the adverbialand provides the intransitive classification.Performance on the simple reciprocal in-transitive alternation (John agreed with MaryMary and John agreed) was less satisfac-tory.
Three potential candidates were selectedby virtue of their SCFs swing;with add;to andagree;with.
None of these were identified as tak-ing the alternation which gave rise to 2 true neg-atives and I false negative.
From examining theresults it seems that many of the senses found atthe intransitive slot of agree e.g.
policy wouldnot be capable of alternating.
It is at least en-couraging that the difference in the cost of theseparate and combined models was low.6 Conclus ionsUsing MDL to detect alternations eems to bea useful strategy in cases where the majority ofsenses in alternating slot position do indeed per-mit the alternation.
In other cases the methodis at least conservative.
Further work will ex-tend the results to include a wider range of al-ternations and verbs.
We also plan to use thismethod to investigate the degree of compressionthat the respective alternations can make to thelexicon as a whole.ReferencesNaoki Abe and Hang Li.
1996.
Learning wordassociation orms using tree cut pair models.In Proceedings of the 13th International Con-ference on Machine Learning ICML, pages 3-11.Ted Briscoe and John Carroll.
1997.
Automaticextraction of subcategorization from corpora.In Fifth Applied Natural Language ProcessingConference., pages 356-363.Anna Korhonen.
1997.
Acquiring subcategori-sation from textual corpora.
Master's thesis,University of Cambridge.Beth Levin.
1993.
English Verb Classes and Al-ternations: a preliminary investigation.
Uni-versity of Chicago Press, Chicago and Lon-don.George Miller, Richard Beckwith, ChristineFelbaum, David Gross, and KatherineMiller, 1993.
Introduction to Word-Net: An On-Line Lezical Database.ftp//darity.princeton.edu/pub/WordNet /5papers.ps.Francesc Ribas.
1995.
On Acquiring Appropri-ate Selectional Restrictions from Corpora Us-ing a Semantic Taxonomy.
Ph.D. thesis, Uni-versity of Catalonia.J.
Rissanen.
1978.
Modeling by shortest datadescription.
Automatica, 14:465-471.1495
