Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 898?906,October 25-29, 2014, Doha, Qatar.c?2014 Association for Computational LinguisticsAn Experimental Comparison of Active Learning Strategies forPartially Labeled SequencesDiego MarcheggianiIstituto di Scienza e Tecnologie dell?InformazioneConsiglio Nazionale delle RicerchePisa, Italydiego.marcheggiani@isti.cnr.itThierry Arti`eresLIP6Pierre et Marie Curie UniversityParis, Francethierry.artieres@lip6.frAbstractActive learning (AL) consists of asking humanannotators to annotate automatically selecteddata that are assumed to bring the most bene-fit in the creation of a classifier.
AL allows tolearn accurate systems with much less anno-tated data than what is required by pure super-vised learning algorithms, hence limiting thetedious effort of annotating a large collectionof data.We experimentally investigate the behav-ior of several AL strategies for sequencelabeling tasks (in a partially-labeled sce-nario) tailored on Partially-Labeled Condi-tional Random Fields, on four sequence la-beling tasks: phrase chunking, part-of-speechtagging, named-entity recognition, and bio-entity recognition.1 IntroductionToday, the state-of-the-art methods in most natural lan-guage processing tasks are supervised machine learn-ing approaches.
Their main problem lies in their needof large human-annotated training corpus, which re-quires a tedious and expensive work from domain ex-perts.
The process of active learning (AL) employs oneor more human annotators by asking them to label newsamples which are supposed to be the most informa-tive in the creation of a new classifier.
A classifier isincrementally retrained with all the data labeled by theannotator.
AL has been demonstrated to work well andto produce accurate classifiers while saving much hu-man annotation effort.
One critical issue is to definea measure of the informativeness which should reflecthow much new information a new example would givein the learning of a new classifier once annotated.A lot of work has been done on the AL field inthe past years (see (Settles, 2012) for an exhaustiveoverview).
In particular, AL proved its usefulness in se-quence labeling tasks (Settles and Craven, 2008).
Yet,researchers have always adopted as annotation unit anentire sequence (i.e., the annotator is asked to anno-tate the whole sequence) while it looks like it could bemuch more relevant to ask for labeling only small partsof it (e.g., the ones with highest ambiguity).
A fewworks have investigated this idea.
For instance, Wan-varie et al.
(2011) proposed to use Partially-LabeledConditional Random Fields (PL-CRFs) (Tsuboi et al.,2008), a semi-supervised variation of Conditional Ran-dom Fields (CRFs) (Lafferty et al., 2001) able to dealwith partially-labeled sequences, thus enabling to adoptas annotation unit single tokens and still learning fromfull sequences.
AL with partially labeled sequenceshas proven to be effective in substantially reducing theamount of annotated data with respect to common ALapproaches (see (Wanvarie et al., 2011)).In this work we focus on AL strategies for partiallylabeled sequences adopting the single token as annota-tion unit and PL-CRFs as learning algorithm given itsnature in dealing with partially labeled sequences.
Wepropose several AL strategies based on measures of un-certainty adapted for the AL with partially labeled se-quences scenario and tailored on PL-CRFs.
We furtherpropose two strategies that exploit the finer granularitygiven by the partially-labeled scenario.
We also showthat the choice of single-token annotation can bringto unpredictable results on sequence labeling tasks inwhich the structure of the sequences is not regular, e.g.,named-entity recognition.
We propose a first solutionto the problem of unpredictability.
The aim of thiswork is thoroughly compare the effectiveness and thebehavior of all the proposed AL strategies on four stan-dard sequence labeling tasks, phrase chunking, part-of-speech tagging, named-entity recognition and bio-entity recognition.The remainder of this paper is as follows.
In Sec-tion 2 we summarize the related work in AL, in Sec-tion 3 we describe PL-CRFs, the semi-supervised al-gorithm we adopt in this work.
Section 4 describesin details the AL framework and the AL strategies wepropose.
Section 5 provides a description of the experi-mental setting, the datasets, and discusses the empiricalresults.
Section 6 summarizes our findings.2 Related WorkOur work belongs to the pool-based AL framework.
Itconsiders the case in which a large amount (pool) ofunlabeled examples is available, from which samplesto be labeled must be chosen.
This framework fits allthe sequence labeling problems we consider here.
Fora more exhaustive survey on other AL frameworks see898(Settles, 2012).Most of the AL works on sequence labeling adoptedthe entire sequence as annotation unit (Settles andCraven, 2008) which was demonstrated by Wanvarieet al.
(2011) to be less effective than using the singletoken as annotation unit.
The main AL works in thislatter line of work are (Shen et al., 2004), (Tomanekand Hahn, 2009) and (Wanvarie et al., 2011).
Shenet al.
(2004) adopted SVMs as learning algorithm andproposed two strategies that combine three criteria, in-formativeness, representativeness and diversity.
SVMsallowed them to use as annotation unit a subset of thetokens in a sequence, without annotating, in any way,the rest of the tokens in the sequence.
In (Tomanek andHahn, 2009), the most uncertain tokens of the sequenceare singularly annotated, but the rest of the labels in thesequence are then chosen by the classifier in a semi-supervised fashion.
Wanvarie et al.
(2011) is the clos-est work to ours, they adopt a minimum confidence se-lection strategy with re-estimation using the PL-CRFs.Differently from our work, Wanvarie et al.
(2011) showthat adopting the AL with partially labeled sequencesusing re-estimation, the annotation cost can be dramat-ically reduced (by annotating from 8% to 10% of thetokens of the entire training set), obtaining the samelevel of performance of the classifier trained on the en-tire, fully-labeled, training set.
We started our workfrom this conclusion and we focused on AL with par-tially labeled sequences using re-estimation by compar-ing several AL strategies in order to find the strategythat allows to create the best classifier with the mini-mum annotation effort.3 Partially-LabeledConditional Random FieldsNowadays, CRFs are the de-facto standard for the so-lution of sequence labeling tasks (Sarawagi, 2008).
Intraditional CRFs (Lafferty et al., 2001) the conditionalprobability of a sequence of labels y given a sequenceof observed feature vectors x is given by:p(y|x) =1Z(x)T?t=1?t(y,x) (1)where a standard choice for sequence labeling tasks arethe so called Linear-chain CRFs:p(y|x) =1Z(x)T?t=1?t(yt, yt?1,xt) (2)with:?t(yt, yt?1,xt) = ?u(yt, xt)?b(yt, yt?1) (3)where ?u(yt, xt) models the co-occurrence betweenfeatures xt, and label ytat time t, and ?b(yt, yt?1)models the co-occurrence between two adjacent labelsytand yt?1.PL-CRFs introduced by Tsuboi et al.
(2008) allow tolearn a CRF model using partially-labeled sequences,marginalizing on those tokens that do not have an as-signed label.
In PL-CRFs, L denotes a partially labeledinformation about a sequence.
It consists of a sequenceof sets Ltin which Lt= Y (where Y is the set of allthe possible labels) if there is no label information fortoken at time t. Ltis a singleton containing ytif thelabel of the token at time t is known, and YLis the setof label sequences that fits the partial label informationL.
Then the probability of a partial labeling may becomputed as:p(YL|x) =?y?YLp(y|x) (4)In order to perform inference and parameter learningon PL-CRFs, some modifications on traditional CRFsinference algorithms are required.3.1 Forward-Backward AlgorithmDifferently from traditional CRFs, the forward andbackward scores (respectively ?
and ?
), are calculatedas follows:?t,L(j) =??????
?0 if j 6?
Lt?1(j, y0, x1) else if t = 1and j ?
LtSA(j) otherwise(5)?t,L(i) =??????
?0 if j 6?
Lt1 else if t = Tand j ?
LtSB(j) otherwise(6)whereSA(j) =?i?Lt?1?t?1,L(i)?t(j, i, xt) (7)SB(j) =?j?Lt+1?t+1,L(j)?t+1(j, i, xt+1) (8)and y0is a special label that encodes the beginning ofa sequence.3.2 Marginal ProbabilityThe marginal probability p(yt= j|x,L) is calculatedas:p(yt= j|x,L) =?t,L(j) ?
?t,L(j)ZL(x)(9)with:?t, ZL(x) =?j?Lt?t,L(j) ?
?t,L(j) (10)In case there is no label information, the formulas forforward and backward scores (Equations (5) and (6))and for the marginal probabilities (Equation (9)) yieldthe standard results of CRFs.8993.3 Viterbi AlgorithmThe most probable sequence assignment may be de-rived with a Viterbi algorithm by recursively comput-ing the following quantities:?t,L(j) =??????
?0 if j 6?
Lt?1(j, y0, x1) else if t = 1and j ?
LtM(j) otherwise(11)whereM(j) = maxi?Lt?1?t?1,L(i)?t(j, i, xt) (12)The most probable assignment is then calculated as:y?= argmaxyp(y|x,L)3.4 Log-LikelihoodPL-CRFs?s parameters ?
are learnt through maximumlog-likelihood estimation, that is to maximize the log-likelihood function LL(?):LL(?)
=N?i=1log p(YL(i)|x(i))=N?i=1logZYL(i)(x(i))?
logZY(x(i))(13)The parameters ?
that maximize Equation (13) arecomputed via the LBFGS optimization method (Byrdet al., 1994).4 Active Learning StrategiesPool-based AL (see (Lewis and Catlett, 1994)) is prob-ably the most common scenario in AL, where one hasa large amount (pool) of unlabeled examples U1and asmall amount of labeled examples T1.
In this scenario,the process of AL consists in a series of n iterationswhere a classifier ?iis trained with labeled examplesTi, and then is used to classify the unlabeled examplesUi.
At this point an AL strategy S will select a numberof examplesB that once labeled will hopefully improvethe performance of the next classifier ?i+1.Algorithm 1 shows the pool-based AL frameworkfor partially annotated sequences as introduced in(Wanvarie et al., 2011).
Differently from AL for fullylabeled sequences (Esuli et al., 2010), thanks to thefiner granularity of the partially labeled model, we usethe token as basic annotation unit, instead of the entiresequence.The point of using the partial labeling is in saving therequest for human annotations on tokens whose labelsare already known (inferred) by the classifier and con-centrate on those tokens that the classifier finds hard tolabel.
Using the semi-supervised approach of the PL-CRFs we can take advantage of single-labeled tokensinstead of an entire labeled sequence.The entire pool-based AL process with partially la-beled sequences is summarized in Algorithm 1.
TheAlgorithm 1 Pool-based active learning frameworkRequire: T1, the initial training setU1, the initial unlabeled setS, the selected AL strategyn, the number of iterationsB, the dimension of the update batch1: for i?
1 to n do2: ?i?
train(Ti)3: Li?
?i(Ui)4: for b?
1 to B do5: x(b)??
arg minxt?x,x?LiS(t,x)6: Li?Li?
x(b)?
?i(x(b), y?
)7: Ui?
Ui?
x(b)??
(x(b)?, y?
)8: Ti?
Ti?
x(b)??
(x(b)?, y?
)9: Ui+1?
Ui10: Ti+1?
Tifunction S(t,x) is what, hereafter, we call an AL strat-egy.
S(t,x) takes as input an automatically annotatedsequence x and an element t of this sequence, from theset of sequences Liannotated by the PL-CRF classi-fier ?i, and returns a measure of informativeness as afunction of the classifier decision.For each iteration through the update batch B, themost informative element x(b)?, according to the ALstrategy, is chosen.
The subscript ?, in this case, repre-sents the most informative token, while the superscript(b) represents the sequence in which the token appears.After the choice of the most informative token the setsLi, Uiand Tiare updated.
Liis updated by remov-ing the annotated sequence x(b)and all the informa-tion given by the classifier, and by adding the same se-quence with the new manually labeled token (y?)
andall the re-estimated annotation given by the classifier?i(x(b), y?).
In the unlabeled set Uiand the trainingset Tithe most informative token x(b)?is updated withits manually labeled version (x(b)?, y?)1.
After B tokenannotations, the unlabeled set and the training set forthe next iteration, respectively Ui+1and Ti+1, are up-dated.The inference methods of Section 3 allow not onlyto train a CRF model with partially labeled sequences,but give the possibility of classifying partially labeledsequences, using the known labels as support for theprediction of the other ones.
Thus, in this AL scenario,each time a token is chosen it is immediately labeled,and this new information, as we can see from line 6 ofAlgorithm 1, is promptly used to re-estimate the infor-mativeness of the other tokens in the sequence in whichthe chosen token appears.One may argue that, for a human annotator, anno-1In order to have a light notation we omit the fact thatwhen the most informative token is the first annotated tokenof a sentence, the whole sentence, with just one annotatedtoken, is added to the training set Ti900tating only one or few tokens, instead of the entire se-quence, is a difficult task.
This would be correct inthe scenario in which the text is presented to the hu-man annotator without any visual clue about the an-notations.
However, in (Culotta and McCallum, 2005)it is shown that presenting to the human annotator thehighlighted sequence to be annotated along with the as-sociated sequence of labels obtained by the classifierrequires much less effort from the annotator than per-forming the annotation without any visual and contex-tual clue.4.1 Greedy StrategiesIn this section we present three AL strategies that selectthe most informative tokens, regardless of the assign-ment performed by the Viterbi algorithm.
The ratio-nale behind these strategies is that, even though we arelooking for the most probable sequence assignment, wealso want to annotate the most informative tokens sin-gularly.The Minimum Token Probability (MTP) strategyemploys as measure of informativeness the probabilityof the most probable assignment at time t. This strategygreedily samples the tokens whose highest probabilityamong the labels is lowest.SMTP(t,x) = maxj?Yp(yt= j|x,L) (14)The Maximum Token Entropy (MTE) strategy relieson the entropy measure to evaluate the ambiguity aboutthe label of a token.
The rationale of it is that, if morethan one label have the same assigned marginal proba-bility, the entropy will be high, that is,SMTE(t,x) =?j?Yp(yt= j|x,L) ?
log p(yt= j|x,L)(15)In order to directly plug the SMTEstrategy into the ALframework of Algorithm 1, we removed the minus signat the beginning of the entropy formula.
This allowus to use the min operator with a maximum entropyapproach.The Minimum Token Margin (MTM) strategy isa variant of the margin sampling strategy introducedin (Scheffer et al., 2001).
It calculates the informative-ness by considering the two most probable assignmentsand by subtracting the highest probability by the low-est.
With max?that calculates the second maximumvalue, MTM is defined as:SMTM(t,x) =maxj?Yp(yt=j|x,L)?maxj?Y?p(yt= j|x,L)(16)4.2 Viterbi StrategiesThe following AL strategies take into consideration themost probable sequence assignments obtained from theViterbi algorithm computed on already known labels inthe sequence.The rationale is that, with these strategies, the mea-sure of uncertainty is chosen according to the informa-tion obtained from the outcome of the Viterbi algorithm(i.e., the most probable sequence assignment).The Minimum Viterbi Probability (MVP) is thebase strategy adopted in (Wanvarie et al., 2011).
Ittakes as measure of informativeness the probability ofthe label chosen by the Viterbi algorithm.SMV P(t,x) = p(y?t|x,L) (17)where y?tis the label assignment chosen by the Viterbialgorithm.
In general, the token assignments that max-imize the probability of the sequence assignment y?tare different from the token assignments that maxi-mize the probability of the individual token assign-ments argmaxj?Yp(yt= j).The Maximum Viterbi Pseudo-Entropy (MVPE)strategy calculates for each token the ?pseudo?
entropyof the most probable sequences at the variation of thelabel at position t. The prefix pseudo is used becauseeven though it is calculated as an entropy, the summa-tion is over all the possible labels that can be associatedto a token, and not all the possible sequence assign-ments.SMV PE(t,x) =?j?Yp(y?yt=j|x,L) ?
log p(y?yt=j|x,L)(18)where y?yt=jrepresents the most probable assignmentwith the label at time t constrained to the value j. Asin the MTE strategy the minus sign is removed in orderto plug the functions directly into the AL framework ofAlgorithm 1.The Minimum Viterbi Margin (MVM) strategycalculates the difference of the sequence probabili-ties of the two most probable sequence assignmentsat the variation of the label at time t. When the dif-ference at time t is low, the Viterbi algorithm, in thattime, chooses between two almost equally probable, se-quence assignments.
Formally:SMVM(t,x) = p(y?y?t|x,L)?
p(y??y?
?t|x,L) (19)where y?
?is the second most probable assignment.PL-CRFs allow us to inspect one token at time in or-der to decide if it is worth to annotate.
This fact giveus the possibility of exploit two quantities in order toestimate the informativeness of a token, the sequenceprobability, usually adopted in the traditional AL forsequence labeling, and the marginal probabilities of thesingle tokens as in Section 4.1.
The Minimum Expec-tation (ME) strategy combines the marginal probabili-ties, p(yt= j|x,L) and p(y?yt=j|x,L).SME(t,x) =?j?Yp(yt= j|x,L) ?
p(y?yt=j|x,L)(20)901Here the maximum sequence probability is seen as afunction, and what we calculate is the expected value ofthis very function.
The rationale of this strategy is pick-ing those tokens in which both, the sequence probabil-ity returned by the Viterbi algorithm, and the marginalprobability of the considered labels are low.Given that the ME strategy gives a high weight tothe sequence probability, one might expect that tokensthat belongs to longer sequences are selected more fre-quently, given that, the sequence probability of longersequences is usually lower than shorter ones.
One wayto normalize this difference is subtracting the currentmaximum sequence probability, that is, the maximumsequence probability calculated without any new labelestimation, to the expected value obtained from the es-timation of the label assignment of the token.
This isthe Minimum Expectation Difference (MED) strat-egy.SMED(t,x) = SME(t,x)?
p(y?|x,L) (21)The rationale of this strategy is that when the expectedvalue is far from the maximum value, that is the valuereturned by the Viterbi algorithm, it means that we haveuncertainty on the token taken into consideration.The Random (RAND) strategy samples random to-kens without any external information.
It is used asbaseline to compare the real effectiveness of the pro-posed strategy.At the best of our knowledge the strategies presentedin this section (with the exception of the MVP strategy)have never been applied in the context of AL with par-tially labeled sequences scenario.5 Experiments5.1 DatasetsWe have experimented and evaluated the AL strate-gies of Section 4 on four sequence labeling tasks,part-of-speech tagging, phrase chunking, named-entityrecognition and bio-entity recognition.
We used theCoNLL2000 dataset (Tjong Kim Sang and Buchholz,2000) for the phrase chunking task, the CoNLL2003dataset (Tjong Kim Sang and De Meulder, 2003),for the named-entity recognition task, the NLPBA2004dataset (Kim et al., 2004), for the biomedical entityrecognition task and the CoNLL2000POS dataset2forthe part-of-speech labeling task.
All the datasets arepublicly available and are standard benchmarks in se-quence labeling tasks.
Table 1 shows some statistics ofthe datasets in terms of dimensions, number of labels,distribution of the labels, etc.
The data heterogeneity ofthe different datasets allowed us to test the AL strate-gies on different ?experimental settings?, thus to havea more robust empirical evaluation.2This is the CoNLL2000 dataset annotated with part-of-speech labels instead of chunking labels.5.2 Experimental SettingWe tested the AL strategies described in Section 4on test sets composed by 2012 sequences and 47377tokens for the CoNLL2000 and CoNLL2000POSdatasets, by 3452 sequences and 46394 tokens forthe CoNLL2003 dataset and by 3856 sequences and101039 tokens for the NLPBA2004 dataset.
Wechose an initial training set T1of ?5 sequences onCoNLL2000 and CoNLL2000POS datasets, ?7 se-quences on CoNLL2003 dataset and ?4 sequences onNLPBA2004 dataset, for a total of?100 labeled tokensfor each dataset.
The dimension of the batch update Bhas been chosen as a trade-off between an ideal case inwhich the system is retrained after every single anno-tation (i.e., B = 1) and a practical case with higher Bto limit the algorithmic complexity (since the PL-CRFclassifier must be retrained every iteration).
We used inour experiments B = 50.
We fixed the number of ALiterations n at 40 because what matters here is how thestrategies behave in the beginning of AL process whenthe annotation effort remains low.
For each strategyand for each dataset, we report averaged results of threeruns with a different randomly sampled initial trainingset T1.For each dataset we adopted a standard set of fea-tures.
For the CoNLL2000 dataset we adopted the samestandard features used in (Wanvarie et al., 2011) for thesame dataset, for the CoNLL2003 and the NLPBA2004dataset we adopted the features used in (Wanvarie etal., 2011) for the CoNLL2003 dataset, while for theCoNLL2000POS dataset we used the features pre-sented in (Ratnaparkhi, 1996).
As evaluation measurewe adopted the token variant of the F1measure, intro-duced by Esuli and Sebastiani (2010).
This variant, in-stead of the entire annotation (chunk/entity), calculatesTP s, FP s, and FNs, singularly for each token thatcompose the annotation, bringing to a finer evaluation.5.3 ResultsFrom the learning curves of Figure 1 and Figure 2 itis clear that most of the strategies have the same trendthroughout the different datasets.
This results is some-what different from the results obtained in (Settles andCraven, 2008) in which there is not a clear winneramong the strategies they proposed in a fully-labeledscenario.
The strategies that perform particularly bad(worse than the RAND strategy in CoNLL2000POSand in CoNLL2003 dataset) in all the datasets are theMTE and MTP.
This is expected, because the choiceof the measure of informativeness related to the tokenwithout taking in consideration the Viterbi path is sub-optimal in this task.
Surprisingly, the MTM strategyeven though based on the same principle of MTE andMTP, is very effective in most of the datasets.
Themost effective strategies, that is, the ones that are thefaster at helping the classifier to reach a better accu-racy are the MTM, MVM, and MVP, in particular themargin-based strategies perform very good in all the902Table 1: Training Data Statistics.
#S is the number of total sequences in the dataset, #T is the number of tokensin the dataset, #L is the number of positive labels (labels different from the negative label O), AAL is the averagelength, in tokens, of annotations (sequence of tokens that refer to the same instance of a label), APT is the averagenumber of token in a sequence annotated with a positive label, ASL is the average length of a sequence, AA is theaverage number of annotations in a sequence, %AC is the percentage of sequences with more than one positiveannotation, %DAC is the percentage of sequences that have two or more annotations with different labels.Dataset #S #T #L AAL APT ASL AA %AC %DACCoNLL2000 8936 211727 11 1.6 20.6 24 12.0 98% 98%CoNLL2000POS 8936 211727 35 1.0 20.8 24 20.8 100% 99%CoNLL2003 17290 254979 4 1.4 2.5 15 2.2 45% 32%NLPBA2004 18546 492551 5 2.5 5.9 27 3.1 72% 47%0 500 1000 1500 2000number annotated tokens0.750.800.850.900.95F1 MTPMTEMTMMVPMVPEMVMMEMEDRAND0 500 1000 1500 2000number annotated tokens0.50.60.70.80.9F1 MTPMTEMTMMVPMVPEMVMMEMEDRANDFigure 1: F1results on CoNLL2000 dataset (left) and CoNLL2000POS dataset (right).
For both datasets themaximum number of annotated tokens used (2100) represents ?1% of the entire training set.datasets.
The MVPE strategy performs particularly badin the CoNLL2003 dataset but it performs better thanthe RAND strategy in the other datasets.
The perfor-mance of the ME strategy is always above the aver-age, in particular it is the best performing strategy inthe NLPBA2004 dataset.
However, in the CoNLL2003dataset its performance is similar to the RAND?s per-formance.
Looking at the data, as expected, ME tendsto choose tokens belonging to the longest sequences,regardless if the sequence is already partially anno-tated, that is, it tends to choose tokens from the samesequences.
This behavior is not particularly relevant onthe CoNLL2003 dataset given that the average num-ber of positive tokens per sentence is not high (2.5,see Table 1).
For the other datasets, the average num-ber of positive tokens per sentence is high, and sothe ME strategy is particularly effective.
The MEDstrategy has the most heterogeneous behavior amongthe datasets.
It shows average performances in theCoNLL2000 dataset and NLPBA2004 dataset, but isslower than the RAND strategy in the CoNLL2003 andCoNLL2000POS datasets.In Figure 2 (left) we can notice that there are somestrategies that are consistently worse than the RANDstrategy.
The difference between the strategies belowthe RAND strategy and the RAND strategy itself mightbe due to the fact that those strategies ask to label to-kens that are ?outliers?
(if we imagine tokens as pointsof the features space) that rarely appear in the trainingand test set, and on which the classifier is very uncer-tain.
Given that we are in a semi-supervised setting,with very few training examples, these ?outliers?
canintroduce a lot of noise in the created models and soyielding poor results.
This phenomenon does not hap-pen in the RAND strategy given that it samples uni-formly from the unlabeled set and given that the ?out-liers?
(special cases) are not many, the probability ofrandomly selecting an ?outlier?
is low.5.3.1 Performance DropThe AL strategies applied on the CoNLL2003 dataset(Figure 2 (left)) suffer of some ?random?
drop of per-formance.
We believe that the first reason that yieldsuch a behavior is that named entities often appear oncein a sentence, and have heterogeneous structures withrespect to some homogenous structures as the chunkand POS.
The second reason is that, it may happen thatthe strategies are not accurate enough to localize pre-cisely the best token to label or that getting the labelof an isolated token does not help the classifier much9030 500 1000 1500 2000number annotated tokens0.20.30.40.50.60.7F1MTPMTEMTMMVPMVPEMVMMEMEDRAND0 500 1000 1500 2000number annotated tokens0.10.20.30.40.50.6F1MTPMTEMTMMVPMVPEMVMMEMEDRANDFigure 2: F1results on CoNLL2003 dataset (left) and NLPBA2004 dataset (right).
2100 annotated tokens repre-sent the ?0.8% and ?0.4% respectively of the CoNLL2003 training set and the NLPBA2004 training set.for the remaining of the (unlabeled) tokens in the se-quence.0 500 1000 1500 2000number annotated tokens0.30.40.50.60.7F1 MTPMTEMTMMVPMVPEMVMMEMEDRANDFigure 3: F1results on CoNLL2003 dataset, three to-kens annotation.
6100 annotated tokens represent the?2.4% of the CoNLL2003 training set.A similar phenomenon, called missed class effect(Tomanek et al., 2009), happens in AL when the strate-gies inspect regions of the example space around thedecision boundaries, bringing to a slow AL process.
In(Tomanek et al., 2009) the missed class effect prob-lem is solved by helping the AL strategies to inspectregions far from the decision boundaries, that is, bychoosing an entire sequence instead of a single to-ken.
This solution is not suitable in this context giventhat we will loose all the advantages we have in thepartially-labeled scenario, thus, we decided to anno-tate for each chosen token the previous token and thenext token.
The learning curves of the AL strategiesadopting this method (Figure 3) show a monotonicallyincreasing performance in function of the number ofannotated tokens.By annotating three tokens at time, the tokens thatwere considered ?outliers?
in the scenario with a singletoken annotation are now supported by other tokens ofthe sequence.
This fact helps to decrease the noise in-troduced in the semi-supervised model yielding betterresults.5.3.2 Statistical AnalysisFigure 4 reports a few statistics that highlight the be-havior of the methods on one of the datasets.
One maysee for instance that the MVM and ME strategies arevery different from the other methods in that they se-lect tokens that belong to significantly longer sentenceson average.
Also it may be seen that MVM in partic-ular selects tokens that are far from already annotatedtokens in the sentence.
This strategy probably yields aparticular behavior with respect to exploration and ex-ploitation that seems to suit the two tasks well.
Theother strategies do exhibit different behaviors that intu-itively should not work well.
For instance the MED andthe MVPE strategies select tokens from new fully unla-beled sentences (not shown statistics), preferably short,so that the distance from selected tokens to already la-beled tokens in the sentence (when any) is low.
Thesecurves look like relevant indicators of the behavior ofthe methods, and it would probably be worth monitor-ing these all along the AL process to make sure thelearning exhibit a suitable behavior.
This will be a fu-ture study that is out of the scope of this work.6 ConclusionIn this paper we have presented several AL strategiestailored for the PL-CRFs in a pool-based scenario.
Wehave tested the proposed strategies on four differentdatasets for four different sequence labeling tasks.
Dif-ferently from other similar work in the field of AL,in this study we have shown that margin-based strate-gies constantly achieve good performance on four taskswith very different data characteristics.
Furthermore,we have found that on datasets with certain character-istics a particular phenomenon that makes the entire9040 5 10 15 20 25 30 35 40Iterations102030405060LengthMTPMTEMTMMVPMVPEMVMMEMEDRAND0 5 10 15 20 25 30 35 40Iterations02468101214DistanceMTPMTEMTMMVPMVPEMVMMEMEDRANDFigure 4: Behavior of the methods on CoNLL2000 dataset as a function of the number of the iterations (x-axis,from 1 to 40).
Average length of the sentence the tokens that are selected by the AL strategy belong to (left) andaverage distance from a token that is selected to the closest already labeled token in the sentence, if any (right).AL process highly unpredictable shows up.
This phe-nomenon consists in random drops of accuracy of theclassifiers learnt during the AL process.
We have pro-posed a first solution for this problem that does not havea relevant impact on the human annotation effort.AcknowledgmentsWe kindly thank Fabrizio Sebastiani and Andrea Esulifor their help and valuable comments, and DittayaWanvarie for providing us her implementation ofpartially-labeled CRFs.ReferencesRichard H. Byrd, Jorge Nocedal, and Robert B. Schn-abel.
1994.
Representations of quasi-Newton matri-ces and their use in limited memory methods.
Math-emathical Programming, 63:129?156.Aron Culotta and Andrew McCallum.
2005.
Reduc-ing labeling effort for structured prediction tasks.
InProceedings of the Twentieth National Conferenceon Artificial Intelligence and the Seventeenth Inno-vative Applications of Artificial Intelligence Confer-ence, (AAAI 2005), pages 746?751, Pittsburgh, US.Andrea Esuli and Fabrizio Sebastiani.
2010.
Evalu-ating information extraction.
In Proceedings of theConference on Multilingual and Multimodal Infor-mation Access Evaluation (CLEF 2010), pages 100?111, Padova, IT.Andrea Esuli, Diego Marcheggiani, and Fabrizio Se-bastiani.
2010.
Sentence-based active learningstrategies for information extraction.
In Proceedingsof the First Italian Information Retrieval Workshop(IIR 2010), pages 41?45, Padua, Italy.Jin-Dong Kim, Tomoko Ohta, Yoshimasa Tsuruoka,Yuka Tateisi, and Nigel Collier.
2004.
Introduc-tion to the bio-entity recognition task at JNLPBA.
InProceedings of the International Joint Workshop onNatural Language Processing in Biomedicine and itsApplications, pages 70?75, Geneva, CH.John Lafferty, Andrew McCallum, and FernandoPereira.
2001.
Conditional random fields: Proba-bilistic models for segmenting and labeling sequencedata.
In Proceedings of the 18th International Con-ference on Machine Learning (ICML 2001), pages282?289, Williamstown, US.David D. Lewis and Jason Catlett.
1994.
Hetero-geneous uncertainty sampling for supervised learn-ing.
In Proceedings of 11th International Confer-ence on Machine Learning (ICML 1994), pages 148?156, New Brunswick, US.Adwait Ratnaparkhi.
1996.
A maximum entropymodel for part-of-speech tagging.
In Proceedings ofthe conference on empirical methods in natural lan-guage processing, volume 1, pages 133?142.Sunita Sarawagi.
2008.
Information extraction.
Foun-dations and Trends in Databases, 1(3):261?377.Tobias Scheffer, Christian Decomain, and Stefan Wro-bel.
2001.
Active hidden Markov models for infor-mation extraction.
In Proceedings of the 4th Inter-national Conference on Advances in Intelligent DataAnalysis (IDA 2001), pages 309?318, Cascais, PT.Burr Settles and Mark Craven.
2008.
An analy-sis of active learning strategies for sequence label-ing tasks.
In Proceedings of the Conference onEmpirical Methods in Natural Language Processing(EMNLP 2008), pages 1070?1079, Honolulu, US.Burr Settles.
2012.
Active learning.
Synthesis Lec-tures on Artificial Intelligence and Machine Learn-ing.
Morgan & Claypool Publishers.Dan Shen, Jie Zhang, Jian Su, Guodong Zhou, andChew-Lim Tan.
2004.
Multi-criteria-based active905learning for named entity recognition.
In Proceed-ings of the 42nd Meeting of the Association for Com-putational Linguistics (ACL 2004), pages 589?596,Barcelona, ES.Erik F. Tjong Kim Sang and Sabine Buchholz.2000.
Introduction to the CoNLL-2000 shared task:Chunking.
In Proceedings of the 2nd Workshopon Learning Language in Logic and 4th Confer-ence on Computational Natural Language Learning(LLL/CoNLL 2000), pages 127?132.
Lisbon, PT.Erik F. Tjong Kim Sang and Fien De Meulder.2003.
Introduction to the CoNLL-2003 shared task:Language-independent named entity recognition.
InProceedings of the 7th Conference on Natural Lan-guage Learning (CONLL 2003), pages 142?147, Ed-monton, CA.Katrin Tomanek and Udo Hahn.
2009.
Semi-supervised active learning for sequence labeling.
InProceedings of the 47th Annual Meeting of the As-sociation for Computational Linguistics and the 4thInternational Joint Conference on Natural LanguageProcessing of the AFNLP (ACL-IJCNLP 2009),pages 1039?1047, Singapore.Katrin Tomanek, Florian Laws, Udo Hahn, and Hin-rich Sch?utze.
2009.
On proper unit selection inactive learning: co-selection effects for named en-tity recognition.
In Proceedings of the NAACL HLT2009 Workshop on Active Learning for Natural Lan-guage Processing, pages 9?17, Boulder, US.Yuta Tsuboi, Hisashi Kashima, Hiroki Oda, ShinsukeMori, and Yuji Matsumoto.
2008.
Training condi-tional random fields using incomplete annotations.In Proceedings of the 22nd International Confer-ence on Computational Linguistics (COLING 2008),pages 897?904, Manchester, UK.Dittaya Wanvarie, Hiroya Takamura, and Manabu Oku-mura.
2011.
Active learning with subsequence sam-pling strategy for sequence labeling tasks.
Informa-tion and Media Technologies, 6(3):680?700.906
