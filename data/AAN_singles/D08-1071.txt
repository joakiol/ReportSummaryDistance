Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 680?688,Honolulu, October 2008. c?2008 Association for Computational LinguisticsCross-Task Knowledge-Constrained Self TrainingHal Daume?
IIISchool of ComputingUniversity of UtahSalt Lake City, UT 84112me@hal3.nameAbstractWe present an algorithmic framework forlearning multiple related tasks.
Our frame-work exploits a form of prior knowledge thatrelates the output spaces of these tasks.
Wepresent PAC learning results that analyze theconditions under which such learning is pos-sible.
We present results on learning a shal-low parser and named-entity recognition sys-tem that exploits our framework, showing con-sistent improvements over baseline methods.1 IntroductionWhen two NLP systems are run on the same data, weexpect certain constraints to hold between their out-puts.
This is a form of prior knowledge.
We proposea self-training framework that uses such informationto significantly boost the performance of one of thesystems.
The key idea is to perform self-trainingonly on outputs that obey the constraints.Our motivating example in this paper is the taskpair: named entity recognition (NER) and shallowparsing (aka syntactic chunking).
Consider a hid-den sentence with known POS and syntactic struc-ture below.
Further consider four potential NER se-quences for this sentence.POS: NNP NNP VBD TO NNP NNChunk: [- NP -][-VP-][-PP-][-NP-][-NP-]NER1: [- Per -][- O -][-Org-][- 0 -]NER2: [- Per -][- O -][- O -][- O -][- O -]NER3: [- Per -][- O -][- O -][- Org -]NER4: [- Per -][- O -][- O -][-Org-][- O -]Without ever seeing the actual sentence, can weguess which NER sequence is correct?
NER1 seemswrong because we feel like named entities shouldnot be part of verb phrases.
NER2 seems wrong be-cause there is an NNP1 (proper noun) that is not partof a named entity (word 5).
NER3 is amiss becausewe feel it is unlikely that a single name should spanmore than one NP (last two words).
NER4 has noneof these problems and seems quite reasonable.
Infact, for the hidden sentence, NER4 is correct2.The remainder of this paper deals with the prob-lem of formulating such prior knowledge into aworkable system.
There are similarities betweenour proposed model and both self-training and co-training; background is given in Section 2.
Wepresent a formal model for our approach and per-form a simple, yet informative, analysis (Section 3).This analysis allows us to define what good andbad constraints are.
Throughout, we use a runningexample of NER using hidden Markov models toshow the efficacy of the method and the relation-ship between the theory and the implementation.
Fi-nally, we present full-blown results on seven dif-ferent NER data sets (one from CoNLL, six fromACE), comparing our method to several competi-tive baselines (Section 4).
We see that for many ofthese data sets, less than one hundred labeled NERsentences are required to get state-of-the-art perfor-mance, using a discriminative sequence labeling al-gorithm (Daume?
III and Marcu, 2005).2 BackgroundSelf-training works by learning a model on a smallamount of labeled data.
This model is then evalu-1When we refer to NNP, we also include NNPS.2The sentence is: ?George Bush spoke to Congress today?680ated on a large amount of unlabeled data.
Its predic-tions are assumed to be correct, and it is retrainedon the unlabeled data according to its own predic-tions.
Although there is little theoretical supportfor self-training, it is relatively popular in the natu-ral language processing community.
Its success sto-ries range from parsing (McClosky et al, 2006) tomachine translation (Ueffing, 2006).
In some cases,self-training takes into account model confidence.Co-training (Yarowsky, 1995; Blum and Mitchell,1998) is related to self-training, in that an algorithmis trained on its own predictions.
Where it differs isthat co-training learns two separate models (whichare typically assumed to be independent; for in-stance by training with disjoint feature sets).
Thesemodels are both applied to a large repository of un-labeled data.
Examples on which these two mod-els agree are extracted and treated as labeled for anew round of training.
In practice, one often alsouses a notion of model confidence and only extractsagreed-upon examples for which both models areconfident.
The original, and simplest analysis of co-training is due to Blum and Mitchell (1998).
It doesnot take into account confidence (to do so requires asignificantly more detailed analysis (Dasgupta et al,2001)), but is useful for understanding the process.3 ModelWe define a formal PAC-style (Valiant, 1994) modelthat we call the ?hints model?3.
We have an instancespace X and two output spaces Y1 and Y2.
We as-sume two concept classes C1 and C2 for each outputspace respectively.
Let D be a distribution over X ,and f1 ?
C1 (resp., f2 ?
C2) be target functions.
Thegoal, of course, is to use a finite sample of examplesdrawn from D (and labeled?perhaps with noise?by f1 and f2) to ?learn?
h1 ?
C1 and h2 ?
C2, whichare good approximations to f1 and f2.So far we have not made use of any notion of con-straints.
Our expectation is that if we constrain h1and h2 to agree (vis-a-vis the example in the Intro-duction), then we should need fewer labeled exam-ples to learn either.
(The agreement should ?shrink?the size of the corresponding hypothesis spaces.)
Toformalize this, let ?
: Y1 ?
Y2 ?
{0, 1} be a con-3The name comes from thinking of our knowledge-basedconstraints as ?hints?
to a learner as to what it should do.straint function.
We say that two outputs y1 ?
Y1and y2 ?
Y2 are compatible if ?
(y1, y2) = 1.
Weneed to assume that ?
is correct:Definition 1.
We say that ?
is correct with respectto D, f1, f2 if whenever x has non-zero probabilityunder D, then ?
(f1(x), f2(x)) = 1.RUNNING EXAMPLEIn our example, Y1 is the space of all POS/chunksequences and Y2 is the space of all NER se-quences.
We assume that C1 and C2 are bothrepresented by HMMs over the appropriate statespaces.
The functions we are trying to learn are f1,the ?true?
POS/chunk labeler and f2, the ?true?NER labeler.
(Note that we assume f1 ?
C1,which is obviously not true for language.
)Our constraint function ?
will require the follow-ing for agreement: (1) any NNP must be part of anamed entity; (2) any named entity must be a sub-sequence of a noun phrase.
This is precisely the setof constraints discussed in the introduction.The question is: given this additional source ofknowledge (i.e., ?
), has the learning problem be-come easier?
That is, can we learn f2 (and/or f1) us-ing significantly fewer labeled examples than if wedid not have ??
Moreover, we have assumed that ?is correct, but is this enough?
Intuitively, no: a func-tion ?
that returns 1 regardless of its inputs is clearlynot useful.
Given this, what other constraints mustbe placed on ?.
We address these questions in Sec-tions 3.3.
However, first we define our algorithm.3.1 One-sided Learning with HintsWe begin by considering a simplified version of the?learning with hints?
problem.
Suppose that all wecare about is learning f2.
We have a small amount ofdata labeled by f2 (call thisD) and a large amount ofdata labeled by f1 (call this Dunlab??unlab?
becauseas far as f2 is concerned, it is unlabeled).RUNNING EXAMPLEIn our example, this means that we have a smallamount of labeled NER data and a large amount oflabeled POS/chunk data.
We use 3500 sentencesfrom CoNLL (Tjong Kim Sang and De Meulder,2003) as the NER data and section 20-23 of theWSJ (Marcus et al, 1993; Ramshaw and Marcus,1995) as the POS/chunk data (8936 sentences).
Weare only interested in learning to do NER.
Detailsof the exact HMM setup are in Section 4.2.681We call the following algorithm ?One-SidedLearning with Hints,?
since it aims only to learn f2:1: Learn h2 directly on D2: For each example (x, y1) ?
Dunlab3: Compute y2 = h2(x)4: If ?
(y1, y2), add (x, y2) to D5: Relearn h2 on the (augmented) D6: Go to (2) if desiredRUNNING EXAMPLEIn step 1, we train an NER HMM on CoNLL.
Ontest data, this model achieves an F -score of 50.8.In step 2, we run this HMM on all the WSJ data,and extract 3145 compatible examples.
In step 3,we retrain the HMM; the F -score rises to 58.9.3.2 Two-sided Learning with HintsIn the two-sided version, we assume that we have asmall amount of data labeled by f1 (call this D1), asmall amount of data labeled by f2 (call thisD2) anda large amount of unlabeled data (call this Dunlab).The algorithm we propose for learning hypothesesfor both tasks is below:1: Learn h1 on D1 and h2 on D2.2: For each example x ?
Dunlab:3: Compute y1 = h1(x) and y2 = h2(x)4: If ?
(y1, y2) add (x, y1) to D1, (x, y2) to D25: Relearn h1 on D1 and h2 on D2.6: Go to (2) if desiredRUNNING EXAMPLEWe use 3500 examples from NER and 1000 fromWSJ.
We use the remaining 18447 examples asunlabeled data.
The baseline HMMs achieve F -scores of 50.8 and 76.3, respectively.
In step 2, weadd 7512 examples to each data set.
After step 3,the new models achieve F -scores of 54.6 and 79.2,respectively.
The gain for NER is lower than be-fore as it is trained against ?noisy?
syntactic labels.3.3 AnalysisOur goal is to prove that one-sided learning withhints ?works.?
That is, if C2 is learnable fromlarge amounts of labeled data, then it is also learn-able from small amounts of labeled data and largeamounts of f1-labeled data.
This is formalized inTheorem 1 (all proofs are in Appendix A).
How-ever, before stating the theorem, we must define an?initial weakly-useful predictor?
(terminology fromBlum and Mitchell(1998)), and the notion of noisyPAC-learning in the structured domain.Definition 2.
We say that h is a weakly-useful pre-dictor of f if for all y: PrD [h(x) = y] ?
and PrD [f(x) = y | h(x) = y?
6= y] ?PrD [f(x) = y] + .This definition simply ensures that (1) h is non-trivial: it assigns some non-zero probability to everypossible output; and (2) h is somewhat indicative off .
In practice, we use the hypothesis learned on thesmall amount of training data during step (1) of thealgorithm as the weakly useful predictor.Definition 3.
We say that C is PAC-learnable withnoise (in the structured setting) if there exists analgorithm with the following properties.
For anyc ?
C, any distribution D over X , any 0 ?
?
?1/ |Y|, any 0 <  < 1, any 0 < ?
< 1 and any?
?
?0 < 1/ |Y|, if the algorithm is given accessto examples drawn EX?SN (c,D) and inputs , ?
and?0, then with probability at least 1 ?
?, the algo-rithm returns a hypothesis h ?
C with error at most.
Here, EX?SN (c,D) is a structured noise oracle,which draws examples from D, labels them by c andrandomly replaces with another label with prob.
?.Note here the rather weak notion of noise: en-tire structures are randomly changed, rather than in-dividual labels.
Furthermore, the error is 0/1 lossover the entire structure.
Collins (2001) establisheslearnability results for the class of hyperplane mod-els under 0/1 loss.
While not stated directly in termsof PAC learnability, it is clear that his results apply.Taskar et al (2005) establish tighter bounds for thecase of Hamming loss.
This suggests that the re-quirement of 0/1 loss is weaker.As suggested before, it is not sufficient for ?
tosimply be correct (the constant 1 function is cor-rect, but not useful).
We need it to be discriminating,made precise in the following definition.Definition 4.
We say the discrimination of ?
for h0is PrD[?
(f1(x), h0(x))]?1.In other words, a constraint function is discrim-inating when it is unlikely that our weakly-usefulpredictor h0 chooses an output that satisfies the con-straint.
This means that if we do find examples (inour unlabeled corpus) that satisfy the constraints,they are likely to be ?useful?
to learning.682RUNNING EXAMPLEIn the NER HMM, let h0 be the HMM obtained bytraining on the small labeled NER data set and f1is the true syntactic labels.
We approximate PrDby an empirical estimate over the unlabeled distri-bution.
This gives a discrimination is 41.6 for theconstraint function defined previously.
However, ifwe compare against ?weaker?
constraint functions,we see the appropriate trend.
The value for the con-straint based only on POS tags is 39.1 (worse) andfor the NP constraint alone is 27.0 (much worse).Theorem 1.
Suppose C2 is PAC-learnable withnoise in the structured setting, h02 is a weakly use-ful predictor of f2, and ?
is correct with respect toD, f1, f2, h02, and has discrimination ?
2(|Y| ?
1).Then C2 is also PAC-learnable with one-sided hints.The way to interpret this theorem is that it tellsus that if the initial h2 we learn in step 1 of the one-sided algorithm is ?good enough?
(in the sense that itis weakly-useful), then we can use it as specified bythe remainder of the one-sided algorithm to obtainan arbitrarily good h2 (via iterating).The dependence on |Y| is the discriminationbound for ?
is unpleasant for structured problems.
Ifwe wish to find M unlabeled examples that satisfythe hints, we?ll need a total of at least 2M(|Y| ?
1)total.
This dependence can be improved as follows.Suppose that our structure is represented by a graphover vertices V , each of which can take a label froma set Y .
Then, |Y| =?
?Y V?
?, and our result requiresthat ?
be discriminating on an order exponential inV .
Under the assumption that ?
decomposes overthe graph structure (true for our example) and thatC2 is PAC-learnable with per-vertex noise, then thediscrimination requirement drops to 2 |V | (|Y | ?
1).RUNNING EXAMPLEIn NER, |Y | = 9 and |V | ?
26.
This meansthat the values from the previous example look notquite so bad.
In the 0/1 loss case, they are com-pared to 1025; in the Hamming case, they are com-pared to only 416.
The ability of the one-sided al-gorithm follows the same trends as the discrimi-nation values.
Recall the baseline performance is50.8.
With both constraints (and a discriminationvalue of 41.6), we obtain a score of 58.9.
With justthe POS constraint (discrimination of 39.1), we ob-tain a score of 58.1.
With just the NP constraint(discrimination of 27.0, we obtain a score of 54.5.The final question is how one-sided learning re-lates to two-sided learning.
The following definitionand easy corollary shows that they are related in theobvious manner, but depends on a notion of uncor-relation between h01 and h02.Definition 5.
We say that h1 and h2 are un-correlated if PrD [h1(x) = y1 | h2(x) = y2, x] =PrD [h1(x) = y1 | x].Corollary 1.
Suppose C1 and C2 are both PAC-learnable in the structured setting, h01 and h02 areweakly useful predictors of f1 and f2, and ?
iscorrect with respect to D, f1, f2, h01 and h02, andhas discrimination ?
4(|Y| ?
1)2 (for 0/1 loss) or?
4 |V |2 (|Y |?1)2 (for Hamming loss), and that h01and h02 are uncorrelated.
Then C1 and C2 are alsoPAC-learnable with two-sided hints.Unfortunately, Corollary 1 depends quadraticallyon the discrimination term, unlike Theorem 1.4 ExperimentsIn this section, we describe our experimental results.We have already discussed some of them in the con-text of the running example.
In Section 4.1, webriefly describe the data sets we use.
A full descrip-tion of the HMM implementation and its results arein Section 4.2.
Finally, in Section 4.3, we presentresults based on a competitive, discriminatively-learned sequence labeling algorithm.
All results forNER and chunking are in terms of F-score; all re-sults for POS tagging are accuracy.4.1 Data SetsOur results are based on syntactic data drawn fromthe Penn Treebank (Marcus et al, 1993), specifi-cally the portion used by CoNLL 2000 shared task(Tjong Kim Sang and Buchholz, 2000).
Our NERdata is from two sources.
The first source is theCoNLL 2003 shared task date (Tjong Kim Sang andDe Meulder, 2003) and the second source is the 2004NIST Automatic Content Extraction (Weischedel,2004).
The ACE data constitute six separate datasets from six domains: weblogs (wl), newswire(nw), broadcast conversations (bc), United Nations(un), direct telephone speech (dts) and broadcastnews (bn).
Of these, bc, dts and bn are all speechdata sets.
All the examples from the previous sec-tions have been limited to the CoNLL data.6834.2 HMM ResultsThe experiments discussed in the preceding sectionsare based on a generative hidden Markov model forboth the NER and syntactic chunking/POS taggingtasks.
The HMMs constructed use first-order tran-sitions and emissions.
The emission vocabulary ispruned so that any word that appears?
1 time in thetraining data is replaced by a unique *unknown*token.
The transition and emission probabilities aresmoothed with Dirichlet smoothing, ?
= 0.001 (thiswas not-aggressively tuned by hand on one setting).The HMMs are implemented as finite state modelsin the Carmel toolkit (Graehl and Knight, 2002).The various compatibility functions are also im-plemented as finite state models.
We implementthem as a transducer from POS/chunk labels to NERlabels (though through the reverse operation, theycan obviously be run in the opposite direction).
Theconstruction is with a single state with transitions:?
(NNP,?)
maps to B-* and I-*?
(?,B-NP) maps to B-* and O?
(?,I-NP) maps to B-*, I-* and O?
Single exception: (NNP,x), where x is not an NPtag maps to anything (this is simply to avoidempty composition problems).
This occurs in100 of the 212k words in the Treebank data andmore rarely in the automatically tagged data.4.3 One-sided Discriminative LearningIn this section, we describe the results of one-sideddiscriminative labeling with hints.
We use the truesyntactic labels from the Penn Treebank to derivethe constraints (this is roughly 9000 sentences).
Weuse the LaSO sequence labeling software (Daume?
IIIand Marcu, 2005), with its built-in feature set.Our goal is to analyze two things: (1) what is theeffect of the amount of labeled NER data?
(2) whatis the effect of the amount of labeled syntactic datafrom which the hints are constructed?To answer the first question, we keep theamount of syntactic data fixed (at 8936 sentences)and vary the amount of NER data in N ?
{100, 200, 400, 800, 1600}.
We compare modelswith and without the default gazetteer informationfrom the LaSO software.
We have the followingmodels for comparison:?
A default ?Baseline?
in which we simply trainthe NER model without using syntax.Hints Self-T Hintsvs Base vs Base vs Self-TWin 29 20 24Tie 6 12 11Lose 0 3 0Table 1: Comparison between hints, self-training and the(best) baseline for varying amount of labeled data.?
In ?POS-feature?, we do the same thing, but wefirst label the NER data using a tagger/chunkertrained on the 8936 syntactic sentences.
Theselabels are used as features for the baseline.?
A ?Self-training?
setting where we use the8936 syntactic sentences as ?unlabeled,?
labelthem with our model, and then train on theresults.
(This is equivalent to a hints modelwhere ?
(?, ?)
= 1 is the constant 1 func-tion.)
We use model confidence as in Blum andMitchell (1998).4The results are shown in Figure 1.
The trends wesee are the following:?
More data always helps.?
Self-training usually helps over the baseline(though not always: for instance in wl and partsof cts and bn).?
Adding the gazetteers help.?
Adding the syntactic features helps.?
Learning with hints, especially for ?
1000training data points, helps significantly, evenover self-training.We further compare the algorithms by looking athow many training setting has each as the winner.
Inparticular, we compare both hints and self-trainingto the two baselines, and then compare hints to self-training.
If results are not significant at the 95%level (according to McNemar?s test), we call it a tie.The results are in Table 1.In our second set of experiments, we consider therole of the syntactic data.
For this experiment, wehold the number of NER labeled sentences constant(at N = 200) and vary the amount of syntactic datain M ?
{500, 1000, 2000, 4000, 8936}.
The resultsof these experiments are in Figure 2.
The trends are:?
The POS feature is relatively insensitive to theamount of syntactic data?this is most likelybecause it?s weight is discriminatively adjusted4Results without confidence were significantly worse.6840 1000 20000.20.30.40.50.60.7wl0 1000 20000.40.50.60.70.8nw0 1000 20000.40.50.60.70.80.9conll0 1000 20000.40.50.60.70.8bc0 1000 20000.20.30.40.50.60.7un0 1000 20000.750.80.850.90.95cts0 1000 200000.20.40.60.8bnPOS?featureHints (no gaz)Baseline (no gaz)Hints (w/ gaz)Baseline (w/ gaz)Self?train (no gaz)Self?train (w/ gaz)Figure 1: Results of varying the amount of NER labeled data, for a fixed amount (M = 8936) of syntactic data.Hints Self-T Hintsvs Base vs Base vs Self-TWin 34 28 15Tie 1 7 20Lose 0 0 0Table 2: Comparison between hints, self-training and the(best) baseline for varying amount of unlabeled data.by LaSO so that if the syntactic information isbad, it is relatively ignored.?
Self-training performance often degrades as theamount of syntactic data increases.?
The performance of learning with hints in-creases steadily with more syntactic data.As before, we compare performance between thedifferent models, declaring a ?tie?
if the differenceis not statistically significant at the 95% level.
Theresults are in Table 2.In experiments not reported here to save space,we experimented with several additional settings.
Inone, we weight the unlabeled data in various ways:(1) to make it equal-weight to the labeled data; (2)at 10% weight; (3) according to the score producedby the first round of labeling.
None of these had asignificant impact on scores; in a few cases perfor-mance went up by 1, in a few cases, performancewent down about the same amount.4.4 Two-sided Discriminative LearningIn this section, we explore the use of two-sideddiscriminative learning to boost the performance ofour syntactic chunking, part of speech tagging, andnamed-entity recognition software.
We continue touse LaSO (Daume?
III and Marcu, 2005) as the se-quence labeling technique.The results we present are based on attempting toimprove the performance of a state-of-the-art systemtrain on all of the training data.
(This is in contrast tothe results in Section 4.3, in which the effect of us-ing limited amounts of data was explored.)
For thePOS tagging and syntactic chunking, we being withall 8936 sentences of training data from CoNLL.
Forthe named entity recognition, we limit our presenta-tion to results from the CoNLL 2003 NER sharedtask.
For this data, we have roughly 14k sentencesof training data, all of which are used.
In both cases,we reserve 10% as development data.
The develop-ment data is use to do early stopping in LaSO.As unlabeled data, we use 1m sentences extractedfrom the North American National Corpus of En-6850 5000 100000.350.40.450.50.550.60.65wl0 5000 100000.550.60.650.70.750.8nw0 5000 100000.50.60.70.80.9conll0 5000 100000.50.60.70.80.9bc0 5000 100000.20.30.40.50.60.7un0 5000 100000.750.80.850.90.95cts0 5000 100000.20.40.60.81bnPOS?featureHints (no gaz)Baseline (no gaz)Hints (w/ gaz)Baseline (w/ gaz)Self?train (no gaz)Self?train (w/ gaz)Figure 2: Results of varying amount of syntactic data for a fixed amount of NER data (N = 200 sentences).glish (previously used for self-training of parsers(McClosky et al, 2006)).
These 1m sentences wereselected by dev-set relativization against the unionof the two development data sets.Following similar ideas to those presented byBlum and Mitchell (1998), we employ two slightmodifications to the algorithm presented in Sec-tion 3.2.
First, in step (2b) instead of adding allallowable instances to the labeled data set, we onlyadd the top R (for some hyper-parameter R), where?top?
is determined by average model confidence forthe two tasks.
Second, Instead of using the full un-labeled set to label at each iteration, we begin witha random subset of 10R unlabeled examples and an-other add random 10R every iteration.We use the same baseline systems as in one-sidedlearning: a Baseline that learns the two tasks inde-pendently; a variant of the Baseline on which theoutput of the POS/chunker is used as a feature forthe NER; a variant based on self-training; the hints-based method.
In all cases, we do use gazetteers.
Werun the hints-based model for 10 iterations.
For self-training, we use 10R unlabeled examples (so that ithad access to the same amount of unlabeled data asthe hints-based learning after all 10 iterations).
Weused three values of R: 50, 100, 500.
We select theChunking NERBaseline 94.2 87.5w/POS N/A 88.0Self-trainR = 50 94.2 88.0R = 100 94.3 88.6R = 500 94.1 88.4HintsR = 50 94.2 88.5R = 100 94.3 89.1R = 500 94.3 89.0Table 3: Results on two-sided learning with hints.best-performing model (by the dev data) over theseten iterations.
The results are in Table 3.As we can see, performance for syntactic chunk-ing is relatively stagnant: there are no significantimprovements for any of the methods over the base-line.
This is not surprising: the form of the con-straint function we use tells us a lot about the NERtask, but relatively little about the syntactic chunkingtask.
In particular, it tells us nothing about phrasesother than NPs.
On the other hand, for NER, we seethat both self-training and learning with hints im-prove over the baseline.
The improvements are not686enormous, but are significant (at the 95% level, asmeasured by McNemar?s test).
Unfortunately, theimprovements for learning with hints over the self-training model are only significant at the 90% level.5 DiscussionWe have presented a method for simultaneouslylearning two tasks using prior knowledge about therelationship between their outputs.
This is relatedto joint inference (Daume?
III et al, 2006).
How-ever, we do not require that that a single data setbe labeled for multiple tasks.
In all our examples,we use separate data sets for shallow parsing as fornamed-entity recognition.
Although all our exper-iments used the LaSO framework for sequence la-beling, there is noting in our method that assumesany particular learner; alternatives include: condi-tional random fields (Lafferty et al, 2001), indepen-dent predictors (Punyakanok and Roth, 2001), max-margin Markov networks (Taskar et al, 2005), etc.Our approach, both algorithmically and theoreti-cally, is most related to ideas in co-training (Blumand Mitchell, 1998).
The key difference is that inco-training, one assumes that the two ?views?
areon the inputs; here, we can think of the two out-put spaces as being the difference ?views?
and thecompatibility function ?
being a method for recon-ciling these two views.
Like the pioneering workof Blum and Mitchell, the algorithm we employ inpractice makes use of incrementally augmenting theunlabeled data and using model confidence.
Alsolike that work, we do not currently have a theoret-ical framework for this (more complex) model.5 Itwould also be interesting to explore soft hints, wherethe range of ?
is [0, 1] rather than {0, 1}.Recently, Ganchev et al (2008) proposed a co-regularization framework for learning across multi-ple related tasks with different output spaces.
Theirapproach hinges on a constrained EM frameworkand addresses a quite similar problem to that ad-dressed by this paper.
Chang et al (2008) alsopropose a ?semisupervised?
learning approach quitesimilar to our own model.
The show very promis-ing results in the context of semantic role labeling.5Dasgupta et al (2001) proved, three years later, that a for-mal model roughly equivalent to the actual Blum and Mitchellalgorithm does have solid theoretical foundations.Given the apparent (very!)
recent interest in thisproblem, it would be ideal to directly compare thedifferent approaches.In addition to an analysis of the theoretical prop-erties of the algorithm presented, the most com-pelling avenue for future work is to apply this frame-work to other task pairs.
With a little thought, onecan imagine formulating compatibility functions be-tween tasks like discourse parsing and summariza-tion (Marcu, 2000), parsing and word alignment, orsummarization and information extraction.AcknowledgmentsMany thanks to three anonymous reviewers of thispapers whose suggestions greatly improved thework and the presentation.
This work was partiallyfunded by NSF grant IIS 0712764.A ProofsThe proof of Theorem 1 closes follows that of Blumand Mitchell (1998).Proof (Theorem 1, sketch).
Use the following nota-tion: ck = PrD[h(x) = k], pl = PrD[f(x) = l],ql|k = PrD[f(x) = l | h(x) = k].
Denote by Athe set of outputs that satisfy the constraints.
We areinterested in the probability that h(x) is erroneous,given that h(x) satisfies the constraints:p (h(x) ?
A\{l} | f(x) = l)=?k?A\{l}p (h(x) = k | f(x) = l) =?k?A\{l}ckql|k/pl?
?k?Ack(|Y| ?
1 + ?l 6=k1/pl) ?
2?k?Ack(|Y| ?
1)Here, the second step is Bayes?
rule plus definitions,the third step is by the weak initial hypothesis as-sumption, and the last step is by algebra.
Thus, inorder to get a probability of error at most ?, we need?k?A ck = Pr[h(x) ?
A] ?
?/(2(|Y| ?
1)).The proof of Corollary 1 is straightforward.Proof (Corollary 1, sketch).
Write out the probabil-ity of error as a double sum over true labels y1, y2and predicted labels y?1, y?2 subject to ?
(y?1, y?2).
Usethe uncorrelation assumption and Bayes?
to split thisinto the product two terms as in the proof of Theo-rem 1.
Bound as before.687ReferencesAvrim Blum and Tom Mitchell.
1998.
Combining la-beled and unlabeled data with co-training.
In Pro-ceedings of the Conference on Computational Learn-ing Theory (COLT), pages 92?100.Ming-Wei Chang, Lev Ratinov, Nicholas Rizzolo, andDan Roth.
2008.
Learning and inference with con-straints.
In Proceedings of the National Conferenceon Artificial Intelligence (AAAI).Michael Collins.
2001.
Parameter estimation forstatistical parsing models: Theory and practice ofdistribution-free methods.
In International Workshopon Parsing Technologies (IWPT).Sanjoy Dasgupta, Michael Littman, and DavidMcAllester.
2001.
PAC generalization boundsfor co-training.
In Advances in Neural InformationProcessing Systems (NIPS).Hal Daume?
III and Daniel Marcu.
2005.
Learning assearch optimization: Approximate large margin meth-ods for structured prediction.
In Proceedings of the In-ternational Conference on Machine Learning (ICML).Hal Daume?
III, Andrew McCallum, Ryan McDonald,Fernando Pereira, and Charles Sutton, editors.
2006.Workshop on Computationally Hard Problems andJoint Inference in Speech and Language Process-ing.
Proceedings of the Conference of the NorthAmerican Chapter of the Association for Computa-tional Linguistics and Human Language Technology(NAACL/HLT).Kuzman Ganchev, Joao Graca, John Blitzer, and BenTaskar.
2008.
Multi-view learning over structured andnon-identical outputs.
In Proceedings of the Conver-ence on Uncertainty in Artificial Intelligence (UAI).Jonathan Graehl and Kevin Knight.
2002.
Carmel fi-nite state transducer package.
http://www.isi.edu/licensed-sw/carmel/.John Lafferty, Andrew McCallum, and Fernando Pereira.2001.
Conditional random fields: Probabilistic mod-els for segmenting and labeling sequence data.
In Pro-ceedings of the International Conference on MachineLearning (ICML).Daniel Marcu.
2000.
The Theory and Practice of Dis-course Parsing and Summarization.
The MIT Press,Cambridge, Massachusetts.Mitch Marcus, Mary Ann Marcinkiewicz, and BeatriceSantorini.
1993.
Building a large annotated corpus ofEnglish: The Penn Treebank.
Computational Linguis-tics, 19(2):313?330.David McClosky, Eugene Charniak, and Mark Johnson.2006.
Effective self-training for parsing.
In Proceed-ings of the Conference of the North American Chapterof the Association for Computational Linguistics andHuman Language Technology (NAACL/HLT).Vasin Punyakanok and Dan Roth.
2001.
The use of clas-sifiers in sequential inference.
In Advances in NeuralInformation Processing Systems (NIPS).Lance A. Ramshaw and Mitchell P. Marcus.
1995.
Textchunking using transformation-based learning.
In Pro-ceedings of the Third ACL Workshop on Very LargeCorpora.Ben Taskar, Vassil Chatalbashev, Daphne Koller, andCarlos Guestrin.
2005.
Learning structured predic-tion models: A large margin approach.
In Proceedingsof the International Conference on Machine Learning(ICML), pages 897?904.Erik F. Tjong Kim Sang and Sabine Buchholz.
2000.Introduction to the CoNLL-2000 shared task: Chunk-ing.
In Proceedings of the Conference on Natural Lan-guage Learning (CoNLL).Erik F. Tjong Kim Sang and Fien De Meulder.
2003.
In-troduction to the CoNLL-2003 shared task: Language-independent named entity recognition.
In Proceedingsof Conference on Computational Natural LanguageLearning, pages 142?147.Nicola Ueffing.
2006.
Self-training for machine trans-lation.
In NIPS workshop on Machine Learning forMultilingual Information Access.Leslie G. Valiant.
1994.
A theory of the learnable.
An-nual ACM Symposium on Theory of Computing, pages436?445.Ralph Weischedel, editor.
2004.
Automatic Content Ex-traction Workshop (ACE-2004), Alexandria, Virginia,September 20?22.David Yarowsky.
1995.
Unsupervised word sense dis-ambiguation rivaling supervised methods.
In Proceed-ings of the Conference of the Association for Compu-tational Linguistics (ACL).688
