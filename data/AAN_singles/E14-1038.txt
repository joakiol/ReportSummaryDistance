Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 358?367,Gothenburg, Sweden, April 26-30 2014.c?2014 Association for Computational LinguisticsCorrecting Grammatical Verb ErrorsAlla RozovskayaColumbia UniversityNew York, NY 10115ar3366@columbia.eduDan RothUniversity of IllinoisUrbana, IL 61801danr@illinois.eduVivek SrikumarStanford UniversityStanford, CA 94305svivek@cs.stanford.eduAbstractVerb errors are some of the most com-mon mistakes made by non-native writersof English but some of the least studied.The reason is that dealing with verb er-rors requires a new paradigm; essentiallyall research done on correcting grammat-ical errors assumes a closed set of trig-gers ?
e.g., correcting the use of prepo-sitions or articles ?
but identifying mis-takes in verbs necessitates identifying po-tentially ambiguous triggers first, and thendetermining the type of mistake made andcorrecting it.
Moreover, once the verb isidentified, modeling verb errors is chal-lenging because verbs fulfill many gram-matical functions, resulting in a variety ofmistakes.
Consequently, the little earlierwork done on verb errors assumed that theerror type is known in advance.We propose a linguistically-motivated ap-proach to verb error correction that makesuse of the notion of verb finiteness to iden-tify triggers and types of mistakes, beforeusing a statistical machine learning ap-proach to correct these mistakes.
We showthat the linguistically-informed model sig-nificantly improves the accuracy of theverb correction approach.1 IntroductionWe address the problem of correcting grammati-cal verb mistakes made by English as a SecondLanguage (ESL) learners.
Recent work in ESL er-ror correction has focused on errors in article andpreposition usage (Han et al., 2006; Felice andPulman, 2008; Gamon et al., 2008; Tetreault etal., 2010; Gamon, 2010; Rozovskaya and Roth,2010b; Dahlmeier and Ng, 2011).While verb errors occur as often as article andpreposition mistakes, with a few exceptions (Leeand Seneff, 2008; Gamon et al., 2009; Tajiri et al.,2012), there has been little work on verbs.
Thereare two reasons for why it is difficult to deal withverb mistakes.
First, in contrast to articles andprepositions, verbs are more difficult to identifyin text, as they can often be confused with otherparts of speech, and processing tools are known tomake more errors on noisy ESL data (Nagata et al.,2011).
Second, verbs are more complex linguisti-cally: they fulfill several grammatical functions,and these different roles imply different types oferrors.These difficulties have led all previous workon verb mistakes to assume prior knowledge ofthe mistake type; however, identifying the specificcategory of a verb error is nontrivial, since the sur-face form of the verb may be ambiguous, espe-cially when that verb is used incorrectly.
Considerthe following examples of verb mistakes:1.
?We discusses*/discuss this every time.?2.
?I will be lucky if I {will find}*/find something thatfits.?3.
?They wanted to visit many places withoutspend*/spending a lot of money.?4.
?They arrived early to organized*/organize every-thing?.These examples illustrate three grammaticalverb properties: Agreement, Tense, and non-finiteForm choice that encompass the most commongrammatical verb problems for ESL learners.
Thefirst two examples show mistakes on verbs thatfunction as main verbs in a clause: sentence (1)shows an example of subject-verb Agreement er-ror; (2) is an example of a Tense mistake wherethe ambiguity is between {will find} (Future tense)358and find (Present tense).
Examples (3) and (4) dis-play Form mistakes: confusing the infinitive andgerund forms in (3) and including an inflection onan infinitive verb in (4).This paper addresses the specific challenges ofverb error correction that have not been addressedpreviously ?
identifying candidates for mistakesand determining which class of errors is present,before proceeding to correct the error.
The ex-perimental results show that our linguistically-motivated approach benefits verb error correction.In particular, in order to determine the error type,we build on the notion of verb finiteness to distin-guish between finite and non-finite verbs (Quirk etal., 1985), that correspond to Agreement and Tensemistakes (examples (1) and (2) above) and Formmistakes (examples (3) and (4) above), respec-tively (see Sec.
3).
The approach presented in thiswork was evaluated empirically and competitivelyin the context of the CoNLL shared task on errorcorrection (Ng et al., 2013) where it was imple-mented as part of the highest-scoring Universityof Illinois system (Rozovskaya et al., 2013) anddemonstrated superior performance on the verb er-ror correction sub-task.This paper makes the following contributions:?We present a holistic, linguistically-motivatedframework for correcting grammatical verb mis-takes; our approach ?starts from scratch?
with-out any knowledge of which mistakes should becorrected or of the mistake type; in doing thatwe show that the specific challenges of verb errorcorrection are better addressed by first identifyingthe finiteness of the verb in the error identificationstage.?
Within the proposed model, we describe andevaluate several methods of selecting verb candi-dates, an algorithm for determining the verb type,and a type-driven verb error correction system.
?We annotate a subset of the FCE data set withgold verb candidates and gold verb type.12 Related WorkEarlier work in ESL error correction follows themethodology of the context-sensitive spelling cor-rection task (Golding and Roth, 1996; Goldingand Roth, 1999; Banko and Brill, 2001; Carlsonet al., 2001; Carlson and Fette, 2007).
Most ofthe effort in ESL error correction so far has been1The annotation is available at http://cogcomp.cs.illinois.edu/page/publication view/743on article and preposition usage errors, as theseare some of the most common mistakes amongnon-native English speakers (Dalgish, 1985; Lea-cock et al., 2010).
These phenomena are generallymodeled as multiclass classification problems: asingle classifier is trained for a given error typewhere the set of classes includes all articles or thetop n most frequent English prepositions (Izumiet al., 2003; Han et al., 2006; Felice and Pul-man, 2008; Gamon et al., 2008; Tetreault et al.,2010; Rozovskaya and Roth, 2010b; Rozovskayaand Roth, 2011; Dahlmeier and Ng, 2011).Mistakes on verbs have attracted significantlyless attention in the error correction literature.Moreover, the little earlier work done on verb er-rors only considered subsets of these errors andassumed the error sub-type is known in advance.Gamon et al.
(2009) mentioned a model for learn-ing gerund/infinitive confusions and auxiliary verbpresence/choice.
Lee and Seneff (2008) proposedan approach based on pattern matching on treescombined with word n-gram counts for correctingagreement misuse and some types of verb formerrors.
However, they excluded tense mistakes,which is the most common error category for ESLlearners (40% of all verb errors, Sec.
3).
Tajiriet al.
(2012) considered only tense mistakes.
Inthe above studies, it was assumed that the type ofmistake that needs to be corrected is known, andirrelevant verb errors were excluded (e.g., Tajiriet al.
(2012) addressed only tense mistakes andexcluded from the evaluation other kinds of verberrors).
In other words, it was assumed that partof the task was solved.
But, unlike in article andpreposition error correction where the type of mis-take is known based on the surface form of theword, in verb error correction, it is not obvious.The key distinction of our work is that we pro-pose a holistic approach that starts from ?scratch?and, given an instance, first detects a mistake andidentifies its type, and then proceeds to correctit.
We also evaluate several methods for select-ing verb candidates and show the significance ofthis step for improving verb error correction per-formance, while earlier studies do not discuss thisaspect of the problem.
In the CoNLL shared task(Ng et al., 2013) that included verb errors in agree-ment and form, the participating teams did not pro-vide details on how specific challenges were han-dled, but the University of Illinois system obtainedthe highest score on the verb sub-task, even though359Tag Error type Rel.
freq.
(%)TV Tense 40.0FV Form 22.3AGV Verb-subject agreement 11.5MV Missing verb 11.7UV Unneccesary verb 7.3IV Inflection 5.4DV Derivation 1.8Total 6640Table 1: Grammatical verb errors in FCE.all teams used similar resources (Ng et al., 2013).3 Verb Errors in ESL WritingVerb-related errors are very prominent amongnon-native English speakers: grammatical mis-use of verbs constitutes one of the most com-mon errors in several learner corpora, includingthose previously used (Izumi et al., 2003; Leeand Seneff, 2008) and the one employed in thiswork.
We study verb errors using the FCE cor-pus (Yannakoudakis et al., 2011).
The corpuspossesses several desirable characteristics: it islarge (500,000 words), has been annotated by na-tive English speakers, and contains data by learn-ers of multiple first-language backgrounds.
TheFCE corpus contains 5056 determiner errors, 5347preposition errors, and 6640 grammatical verbmistakes (Table 1).3.1 Verb FinitenessThere are many grammatical categories for whichEnglish verbs can be marked.
The linguistic no-tion of verb finiteness or verb type (Radford, 1988;Quirk et al., 1985) distinguishes between verbsthat function on their own in a clause as main verbs(finite) and those that do not (non-finite).
Gram-matical properties associated with each group aremutually exclusive: tense and agreement markers,for example, do not apply to non-finite verbs; non-finite verbs are not marked for many grammaticalfunctions but may appear in several forms.The most common verb problems for ESLlearners ?
Tense, Agreement, non-finite Form ?involve verbs both in finite and non-finite roles.Table 2 illustrates contexts that license finite andnon-finite verbs.Our intuition is that, because properties associ-ated with each verb type are mutually exclusive,verb finiteness should benefit verb error correc-tion models: an observed verb error may be dueto several grammatical phenomena, and knowingwhich phenomena are active depends on the func-tion of the verb in the current context.
Note thatAgreement, Tense, and Form errors account forCategory Agreement Kappa RandomCorrect verbs 0.97 0.95 0.51Erroneous verbs 0.88 0.81 0.41Table 3: Inter-annotator agreement based on 250 verberrors and 250 correct verbs, randomly selected.about 74% of all grammatical verb errors in Ta-ble 1 but the finiteness distinction applies to allEnglish verbs ?
every verb is either finite or non-finite in a specific syntactic context ?
and is alsorelevant for the remaining mistakes not addressedhere.24 Annotation for Verb FinitenessIn order to evaluate the quality of the algorithmfor verb finiteness and of the candidate selectionmethods, we annotated all verbs ?
correct and er-roneous ?
in a random set of 124 documents fromour corpus with the information about verb finite-ness.
We refer to these 124 documents as gold sub-set.
We also annotated erroneous verbs in the re-maining 1120 documents of the corpus.
The anno-tation was performed by two students with back-ground in Linguistics.
The inter-annotator agree-ment is shown in Table 3 and is high.Annotating Verb Errors For each verb error thatwas tagged as Tense (TV), Agreement (AGV), andForm (FV), the annotators marked verb finiteness.Additionally, the annotators also specified the typeof error (Tense, Agreement, or Form) (Table 4),since the FCE tags do not always correspond tothe three error types we study here.
For exam-ple, the FV tag may mark errors on finite verbs.Overall, about 7% of verb errors have to do withphenomena different from the three verb proper-ties considered in this work and thus are excludedfrom the present study.Annotating Correct Verbs Correct verbs wereidentified in text using an automated proce-dure that relies on part-of-speech information(Sec.
5.1).
Valid candidates were specified forverb finiteness.
The candidates that were iden-tified incorrectly due to mistakes by the part-of-speech tagger were marked as invalid.5 The Computational ModelThe verb error correction problem is formulatedas a classification task in the spirit of the learn-2For instance, the missing verb errors (MV, 11.7%) re-quire an additional step to identify contexts for missing verbs,and then appropriate verb properties need to be determinedbased on verb finiteness.360Verb type Example Verb propertiesAgreement Tense FormFinite?He discussed this with me last week?
- Past Simple -?He discusses this with me every week.?
3rd person,Sing.
Present Simple -Non-finite?He left without discussing it with me.?
- - Gerund?They let him discuss this with me.?
- - Infinitive?To discuss this now would be ill-advised.?
- - to-InfinitiveTable 2: Contexts that license finite and non-finite verbs and the corresponding active properties.Error on Verb Type Subcategory ExampleFinite (67.7%)Agreement (20%) ?We discusses*/discuss this every time.
?Tense (80%) ?If you buy something, you {would be}*/{will be} happy.
?Non-finite (25.3%)?If one is famous he has to accept the disadvantages of be*/being famous.?
?I am veryglad {for receiving}*/{to receive} it.?
?They arrived early to organized*/organize everything.
?Other errors (7.0%)Passive/Active(42.3%) ?Our end-of-conference party {is included}*/includes dinner and dancing.
?Compound (40.7%) ?You ask me for some informations*/information- here they*/it are*/is.
?Other (16.8%) ?Nobody {has to be}*/{should be} late.
?Table 4: Verb error classification based on 4864 mistakes marked as TV, AGV, and FV errors in the FCE corpus.ing paradigm commonly used for correcting otherESL errors (Sec.
2), with the exception that theverb model includes additional components.
Allof the components are listed below:1.
Candidate selection (5.1)2.
Verb finiteness prediction (5.2)3.
Feature generation (5.3)4.
Error identification (5.4)5.
Error correction (5.5)After verb candidates are selected, verb finite-ness is determined and features are generated foreach candidate.
The finiteness prediction is usedin the error identification component.
Given theoutput of the error identification stage, the corre-sponding classifiers for each error type are invokedto propose an appropriate correction.We split the corpus documents into two equalparts ?
training and test.
We chose a train-test splitand not cross-validation, since the FCE data set isquite large to allow for such a split.
The trainingdata is also used to develop the components forcandidate selection and verb finiteness prediction.5.1 Candidate SelectionThis stage selects the set of verb instances thatare presented as input to the classifier.
A verb in-stance refers to the verb, including its auxiliariesor the infinitive marker (e.g.
?found?, ?will find?,?to find?).
Candidate selection is a crucial step formodels that correct mistakes on open-class wordsbecause those errors that are missed at this stagehave no chance of being detected.
We implementfour candidate selection methods.
Method (1) ex-tracts all verbs heading a verb phrase, as identi-fied by a shallow parser (Punyakanok and Roth,2001).3Method (2) also includes words taggedwith one of the verb tags: {VB, VBN, VBG,VBD, VBP, VBZ} predicted by the POS tagger.4However, relying on the POS information is notgood enough, since the POS tagger performanceon ESL data is known to be suboptimal (Nagata etal., 2011).
For example, verbs lacking agreementmarkers are likely to be mistagged as nouns (Leeand Seneff, 2008).
Methods (3) and (4) addressthe problem of pre-processing errors.
Method (3)adds words that are on the list of valid Englishverb lemmas; the lemma list is constructed us-ing a POS-tagged version of the NYT section ofthe Gigaword corpus and contains about 2,600 offrequently-occurring words tagged as VB; for ex-ample, (3) will add shop but not shopping, but (4)will add both.For methods (3) and (4), we developed verb-Morph,5a tool that performs morphological anal-ysis on verbs and is used to lemmatize verbs andto generate morphological variants.
The modulemakes uses of (1) the verb lemma list and (2) a listof irregular English verbs.The quality of the candidate selection methodsis evaluated in Table 5 on the gold subset by com-puting the recall, i.e.
the percentage of erroneousverbs that have been selected as candidates.
Meth-ods that address pre-processing mistakes are ableto recover more erroneous verb candidates in text.It is also interesting to note that across all methods,the highest recall is obtained for tense errors.
Thissuggests that the POS tagger is more prone to fail-3http://cogcomp.cs.illinois.edu/demo/shallowparse4http://cogcomp.cs.illinois.edu/page/software view/POS5The tool and more detail about it can be found athttp://cogcomp.cs.illinois.edu/page/publication view/743361Method Recall Recall by error group (%)(%) Agr.
Tense Form(1) All verb phrases 83.00 86.62 93.55 59.08(2) + tokens tagged as verbs 91.96 90.30 94.33 87.79(3) + tokens that are validverb lemmas95.50 95.99 96.46 93.23(4) + tokens with inflectionsthat are valid verb lemmas96.09 96.32 96.62 94.84Table 5: Candidate selection methods performance.ure due to errors in agreement and form.
The eval-uation in Table 5 uses recall, as the goal is to assessthe ability of the methods to select erroneous verbsas candidates.
In Sec.
6.1, the contribution of eachmethod to error identification is evaluated.5.2 Predicting Verb FinitenessPredicting verb finiteness is not trivial, as almostall English verbs can occur in both finite and non-finite form and the surface forms of a verb in finiteand non-finite form may be the same (see Table 2).While we cannot learn verb type automaticallydue to lack of annotation, we show, however, that,for the majority of verbs, finiteness can be reliablypredicted using linguistic knowledge.
We imple-ment a decision-list classifier that makes use oflinguistically-motivated rules (Table 6).
The algo-rithm covers about 92% of all verb candidates, ab-staining on the remaining highly-ambiguous 8%.The evaluation of the method on the gold sub-set (last column in Table 6) shows that despite itssimplicity, this method is highly effective: 98% oncorrect verbs and over 89% on errors.5.3 FeaturesThe baseline features are word n-grams in the 4-word window around the verb instance.
Addi-tional features are intended to characterize a givenerror type and are selected based on previous stud-ies: for Agreement and Form errors, we use aparser (Klein and Manning, 2003) and define fea-tures that reflect dependency relations between theverb and its neighbors.
We denote these featuresby syntax.
Syntactic knowledge via tree patternshas been shown useful for Agreement mistakes(Lee and Seneff, 2008).
Features for Tense in-clude temporal adverbs in the sentence and tensesof other verbs in the sentence and are similar tothe features used in other verb classification tasks(Reichart and Rappoport, 2010; Lee, 2011; Tajiriet al., 2012).
The features are shown in Table 7.5.4 Error IdentificationThe goal of this stage is to identify errors and topredict their type.
We define a linear model where,given a verb, a weight vector w assigns a scoreto each label in the label space {Correct, Form,Agreement, Tense}.
The prediction of the classi-fier is the label with the highest score.The baseline error identification model, calledcombined, is agnostic to the type of the verb.
Inthe combined model, for each verb v and label l,we generate a feature vector, ?
(v, l) and the bestlabel is predicted asargmaxlwT?
(v, l).The combined model makes use of all the fea-tures we have defined earlier for each verb.The type-based model uses the verb finitenessprediction made by the verb finiteness classifier.A soft way to use the finiteness prediction is toadd the predicted finiteness value as a feature.
Theother ?
hard-decision approach ?
is to use onlya subset of the features depending on the pre-dicted finiteness: Agreement and Tense for the fi-nite verbs, and Form features for non-finite.
Thehard-decision type-driven approach defines a fea-ture vector for a verb based on its type.
Thus,given the verb v and its type t, we define fea-tures ?
(v, t, l) for each label l. Thus, the label ispredicted asargmaxlwT?
(v, t, l).5.5 Error CorrectionThe correction module consists of three compo-nents, one for each type of mistake.
Given theoutput of the error identification model, the ap-propriate correction component is run for each in-stance predicted to be a mistake.6The verb finite-ness prediction is used to select finite instances fortraining the Agreement and Tense components andnon-finite ?
for the Form component.
The labelspace for Tense specifies tense and aspect prop-erties of the English verbs (see Tajiri et al., 2012for more detail), the Agreement component spec-ifies the person and number properties, while theForm component includes the commonly confus-able non-finite English forms (see Table 2).
Thesecomponents are trained as multiclass classifiers.6We assume that each verb contains at most one mistake.Less than 1% of all erroneous verbs have more than one errorpresent.362A verb is Non-Finite if any of the following hold: A verb is Finite if any of the following hold Accuracy onCorrect Erroneousverbs verbs(1) All verbs identified by shallow parser98.01 89.4(1) [numTokens = 2] ?
[firstToken = to] (2) can; could(2) firstToken = be (3) [numTokens = 1] ?
[pos ?
{V BD, V BP, V BZ}](3) [numTokens = 1] ?
[pos = V BG] (4) [numTokens = 2] ?
[firstToken!
= to](5) numTokens > 2Table 6: Algorithm for determining verb type.
numTokens denotes the number of tokens in the verb instance, e.g., for theverb instance ?to go?, numTokens = 2.
Verbs not covered by the rules, e.g.
those that are not tagged with a verb-related POSin methods (3) and (4), are not assigned any verb type.
The last column shows algorithm accuracy on the gold subset separatelyfor correct and incorrect verbs.Agreement Description(1) subjHead, subjPOS The surface form and the POS tag of the subject head(2) subjDet {those,this,..} Determiner of the subject phrase(3) subjDistance Distance between the verb and the subject head(4) subjNumber {Sing, Pl} Sing ?
singular pronouns and nouns; Pl ?
plural pronouns and nouns(5) subjPerson {3rdSing, Not3rdSing, 1stSing} 3rdSing ?
she,he,it,singular nouns; Not3rdSing ?
we,you,they, plural nouns; 1stSing ?
?I?
(6) conjunctions (1)&(3);(4)&(5)Tense Description(1) verb phrase (VP) verb lemma, negation, surface forms and POS tags of all words in the verb phrase(2) verbs in sentence(4 features) tenses and lemmas of the finite verbs preceding and following the verb instance(3) time adverbs (2 features) temporal adverb before and after the verb instance(4) bag-of-words (BOW) (8 features) Includes the following words in the sentence: {if, when, since, then, wish, hope, when, since,after}Form Description(1) closest word surface form, lemma, POS tag, and distance of the closest open-class word to the left of theverb(2) governor surface form, POS tag and dependency type of the target(3) preposition if the verb is preceded by a preposition: preposition itself and the surface form, POS tag anddependency of the governor of the preposition(4) pos and lemma POS tag and lemma of the verb and their conjunctions with features in (2) and (3) and wordngramsTable 7: Features used, grouped by error type.6 ExperimentsThe main goal of this work is to propose a uni-fied framework for correcting verb mistakes andto address the specific challenges of the problem.We thus do not focus on features or on the spe-cific learning algorithm.
Our experimental studyaddresses the following research questions:I. Linguistic questions: (i) candidate selectionmethods; (ii) verb finiteness contribution toerror identificationII.
Computational Framework: error identifi-cation vs. correctionIII.
Gold annotation: (i) using gold candidatesand verb type vs. automatic; (ii) performancecomparison by error typeLearning Framework There is a lot of under-standing for which algorithmic methods workbest for ESL correction tasks, how they compareamong themselves, and how they compare to n-gram based methods.
Specifically, despite their in-tuitive appeal, language models were shown to notwork well on these tasks, while the discriminativelearning framework has been shown to be superiorto other approaches and thus is commonly usedfor error correction tasks (see Sec.
2).
Since wedo not address the algorithmic aspect of the prob-lem, we refer the reader to Rozovskaya and Roth(2011) for a discussion of these issues.
We trainall our models with the SVM learning algorithmimplemented in JLIS (Chang et al., 2010).Evaluation We report both Precision/Recallcurves and AAUC (as a summary).
Error cor-rection is generally evaluated using F1 (Dale etal., 2012); Precision and Recall (Gamon, 2010;Tajiri et al., 2012); or Average Area Under Curve(AAUC) (Rozovskaya and Roth, 2011).
For a dis-cussion on these metrics with respect to error cor-rection tasks, we refer the reader to Rozovskaya(2013).
AAUC (Hanley and McNeil, 1983)) is ameasure commonly used to generate a summarystatistic, computed as an average precision valueover a range of recall points.
In this paper, AAUCis computed over the first 15 recall points:AAUC =115?15?i=1Precision(i).6.1 Linguistic QuestionsCandidate Selection Methods The contributionof the candidate selection component with respectto error identification is evaluated in Table 8, us-ing the methods presented in Sec.
5.1.
Overall,363Recall of candidate AAUCselection method (%) Combined Type-based(1) (83.00) 73.38 79.49(2) (91.96) 80.36 86.48(3) (95.50) 81.39 87.05(4) (96.09) 81.27 86.81Table 8: Impact of candidate selection methods on erroridentification performance.
The first column shows the per-centage of erroneous verbs selected by each method.
Type-based models are discussed in Sec.
6.1.Correct verbs Erroneous verbs Error rateTraining 41721 1981 4.75%Test 41836 2014 4.81%Table 9: Training and test data statistics.
Candidates areselected using method (3).better performance is achieved by methods withhigher recall, with the exception of method (4); itsperformance on error identification is behind thatof method (3), perhaps due to the amount of noisethat is also added.
While the difference is small,method (3) is also simpler than method (4).
Wethus use method (3) in the rest of the paper.
Table9 shows the number of verb instances in trainingand test selected with this method.Verb Finiteness Sec.
5.4 presented two ways ofadding verb finiteness: (1) adding the predictedverb type as a feature and (2) selecting only therelevant features depending on the finiteness of theverb.
Table 10 shows the results of using verb typein the error identification stage.
While the firstapproach does not provide improvement over thecombined model, the second method is very ef-fective.
We conjecture that because verb type pre-diction is quite accurate, the second, hard-decisionapproach is preferred, as it provides knowledge ina direct way.
Henceforth, we will use the secondmethod in the type-based model.Fig.
1 compares the performance of the com-bined and the hard-decision type-based modelsshown in Table 10.
Precision/Recall curves aregenerated by varying the threshold on the confi-dence of the classifier.
This graph reveals the be-havior of the systems at multiple recall points: weobserve that at every recall point the type-basedclassifier has higher precision.So far, the models used all features defined inSec.
5.3.
Table 11 reveals that the type-drivenModel AAUCCombined 81.39Type-based I (soft) 81.11Type-based II (hard) 87.05Table 10: Verb finiteness contribution to error identifi-cation.707580859095  02468101214PRECISIONRECALLCombinedType-basedFigure 1: Verb finiteness contribution to error identifi-cation: key result.
AAUC shown in Table 10.
The combinedmodel uses no verb type information.
In the hard-decisiontype-based model, each verb uses the features according toits finiteness.
The differences are statistically significant (Mc-Nemar?s test, p < 0.0001).Feature set AAUCCombined Type-basedBaseline 46.62 49.72All?Syntax 79.47 84.88Full feature set 81.39 87.05Table 11: Verb finiteness contribution to error identifi-cation for different features.approach is superior to the combined approachacross different feature sets, and the performancegap increases with more sophisticated feature sets,which is to be expected, since more complex fea-tures are tailored toward relevant verb errors.
Fur-thermore, adding features specific to each errortype significantly improves the performance overthe word n-gram features.
The rest of the experi-ments use all features (denoted Full feature set).6.2 Identification vs. CorrectionAfter running the error identification component,we apply the appropriate correction models tothose instances identified as errors.
The resultsfor identification and correction are shown in Ta-ble 12.
The correction models are also finiteness-aware models trained on the relevant verb in-stances (finite or non-finite), as predicted by theverb finiteness classifier.We evaluate the correction components by fix-ing a recall point in the error identification stage.7We observe the relatively low recall obtained bythe models.
Error correction models tend to havelow recall (see, for example, the recent sharedtasks on ESL error correction (Dale and Kilgar-riff, 2011; Dale et al., 2012; Ng et al., 2013)).
Thekey reason for the low recall is the error sparsity:over 95% of verbs are correct, as shown in Table 9.7We can increase recall using a different threshold buthigher precision is preferred in error correction tasks.364Error type Correction IdentificationP R F1 P R F1Agreement 90.62 9.70 17.52 90.62 9.70 17.52Tense 60.51 7.47 13.31 86.62 10.70 19.05Form 81.82 16.34 27.24 83.47 16.67 27.79Total 71.94 10.24 17.94 85.81 12.22 21.20Table 12: Performance of the complete model after thecorrection stage.
The results on Agreement mistakes are thesame, since Agreement errors are always binary decisions,unlike Tense and Form mistakes.The only way to improve over this 95% baseline isby forcing the system to have very good precision(at the expense of recall).
The performance shownin Table 12 corresponds to an accuracy of 95.60%in identification (error reduction of 8.7%) and95.40% in correction (error reduction of 4.5%)over the baseline of 95.19%.6.3 Analysis on Gold DataTo further study the impact of each step of the sys-tem, we analyze our model on the gold subset ofthe data.
The gold subset contains two additionalpieces of information not available for the rest ofthe corpus: gold verb candidates and gold verbfiniteness (Sec.
4).
The set contains 7784 goldverbs, including 464 errors.
Experiments are runin 10-fold cross-validation where on each run 90%of the documents are used for training and the re-maining 10% are used for evaluation.
The goldannotation can be used instead of automatic pre-dictions in two system components: (1) candidateselection and (2) verb finiteness.Table 13 shows the performance on error identi-fication when gold vs. automatic settings are used.As expected, using the gold verb type is more ef-fective than using the automatic one, both with au-tomatic and gold candidates.
The same is true forcandidate selection.
For instance, the combinedmodel improves by 14 AAUC points (from 55.90to 69.86) with gold candidates.
These results indi-cate that candidate selection is an important com-ponent of the verb error correction system.Note that compared to the performance on theentire data set (Table 10), the performance of themodels shown here that use automatic componentsis lower, since the training size is smaller.
On theother hand, because of the smaller training size,the gain due to the type-based approach is largeron the gold subset (19 vs. 6 AAUC points).Finally, in Table 14, we evaluate the contribu-tion of verb finiteness to error identification by er-ror type.
While performance varies by error, it isclear that all errors benefit from verb typing.Candidate selection Verb type prediction AAUCAutomaticNone 55.90Automatic 74.72Gold 89.45GoldNone 69.86Automatic 90.89Gold 96.42Table 13: Gold subset: error identification with gold vs.automatic candidates and finiteness information.
ValueNone for verb type prediction denotes the combined model.Error type AAUCCombined Type-based Type-basedAutomatic GoldAgreement 86.80 88.43 89.21Tense 18.07 25.62 26.87Form 97.08 98.23 98.36Table 14: Gold subset: gold vs. automatic finiteness con-tribution to error identification by error type.7 ConclusionVerb errors are commonly made by ESL writersbut difficult to address due to to their diversityand the fact that identifying verbs in (noisy) textmay itself be difficult.
We develop a linguistically-inspired approach that first identifies verb candi-dates in noisy learner text and then makes useof verb finiteness to identify errors and character-ize the type of mistake.
This is important, sincemost errors made by non-native speakers cannotbe identified by considering only closed classes(e.g., prepositions and articles).
Our model inte-grates a statistical machine learning approach witha rule-based system that encodes linguistic knowl-edge to yield the first general correction approachto verb errors (that is, one that does not assumeprior knowledge of which mistake was made).This work thus provides a first step in consider-ing more general algorithmic paradigms for cor-recting grammatical errors and paves the way fordeveloping models to address other ?open-class?mistakes.AcknowledgmentsThe authors thank Graeme Hirst, Julia Hockenmaier, MarkSammons, and the anonymous reviewers for their helpfulfeedback.
This work was done while the first and the thirdauthors were at the University of Illinois.
This material isbased on research sponsored by DARPA under agreementnumber FA8750-13-2-0008 and by the Army Research Lab-oratory (ARL) under agreement W911NF-09-2-0053.
Anyopinions, findings, conclusions or recommendations are thoseof the authors and do not necessarily reflect the view of theagencies.365ReferencesM.
Banko and E. Brill.
2001.
Scaling to very verylarge corpora for natural language disambiguation.In Proceedings of 39th Annual Meeting of the Asso-ciation for Computational Linguistics, pages 26?33,Toulouse, France, July.A.
Carlson and I. Fette.
2007.
Memory-based context-sensitive spelling correction at web scale.
In Pro-ceedings of the IEEE International Conference onMachine Learning and Applications (ICMLA).A.
Carlson, J. Rosen, and D. Roth.
2001.
Scaling upcontext sensitive text correction.
In Proceedings ofthe National Conference on Innovative Applicationsof Artificial Intelligence (IAAI), pages 45?50.M.
Chang, V. Srikumar, D. Goldwasser, and D. Roth.2010.
Structured output learning with indirect su-pervision.
In Proc.
of the International Conferenceon Machine Learning (ICML).D.
Dahlmeier and H. T. Ng.
2011.
Grammatical er-ror correction with alternating structure optimiza-tion.
In Proceedings of the 49th Annual Meeting ofthe Association for Computational Linguistics: Hu-man Language Technologies, pages 915?923, Port-land, Oregon, USA, June.
Association for Computa-tional Linguistics.R.
Dale and A. Kilgarriff.
2011.
Helping Our Own:The HOO 2011 pilot shared task.
In Proceedings ofthe 13th European Workshop on Natural LanguageGeneration.R.
Dale, I. Anisimoff, and G. Narroway.
2012.
Areport on the preposition and determiner error cor-rection shared task.
In Proc.
of the NAACL HLT2012 Seventh Workshop on Innovative Use of NLPfor Building Educational Applications, Montreal,Canada, June.
Association for Computational Lin-guistics.G.
Dalgish.
1985.
Computer-assisted ESL research.CALICO Journal, 2(2).R.
De Felice and S. Pulman.
2008.
A classifier-basedapproach to preposition and determiner error correc-tion in L2 English.
In Proceedings of the 22nd In-ternational Conference on Computational Linguis-tics (Coling 2008), pages 169?176, Manchester, UK,August.M.
Gamon, J. Gao, C. Brockett, A. Klementiev,W.
Dolan, D. Belenko, and L. Vanderwende.
2008.Using contextual speller techniques and languagemodeling for ESL error correction.
In Proceedingsof IJCNLP.M.
Gamon, C. Leacock, C. Brockett, W. B. Dolan,J.
Gao, D. Belenko, and A. Klementiev.
2009.
Us-ing statistical techniques and web search to correctESL errors.
CALICO Journal, Special Issue on Au-tomatic Analysis of Learner Language, 26(3):491?511.M.
Gamon.
2010.
Using mostly native data to correcterrors in learners?
writing.
In NAACL, pages 163?171, Los Angeles, California, June.A.
R. Golding and D. Roth.
1996.
Applying Winnowto context-sensitive spelling correction.
In Proc.
ofthe International Conference on Machine Learning(ICML), pages 182?190.A.
R. Golding and D. Roth.
1999.
A Winnowbased approach to context-sensitive spelling correc-tion.
Machine Learning, 34(1-3):107?130.N.
Han, M. Chodorow, and C. Leacock.
2006.
De-tecting errors in English article usage by non-nativespeakers.
Journal of Natural Language Engineer-ing, 12(2):115?129.J.
Hanley and B. McNeil.
1983.
A method of com-paring the areas under receiver operating character-istic curves derived from the same cases.
Radiology,148(3):839?843.E.
Izumi, K. Uchimoto, T. Saiga, T. Supnithi, andH.
Isahara.
2003.
Automatic error detection inthe Japanese learners?
English spoken data.
In TheCompanion Volume to the Proceedings of 41st An-nual Meeting of the Association for ComputationalLinguistics, pages 145?148, Sapporo, Japan, July.T.-H. Kao, Y.-W. Chang, H. w. Chiu, T-.H.
Yen, J. Bois-son, J. c. Wu, and J.S.
Chang.
2013.
Conll-2013shared task: Grammatical error correction nthu sys-tem description.
In Proceedings of the SeventeenthConference on Computational Natural LanguageLearning: Shared Task, pages 20?25, Sofia, Bul-garia, August.
Association for Computational Lin-guistics.D.
Klein and C. D. Manning.
2003.
Fast exact in-ference with a factored model for natural languageparsing.
In Advances in Neural Information Pro-cessing Systems 15 NIPS, pages 3?10.
MIT Press.C.
Leacock, M. Chodorow, M. Gamon, and J. Tetreault.2010.
Automated Grammatical Error Detection forLanguage Learners.
Morgan and Claypool Publish-ers.J.
Lee and S. Seneff.
2008.
Correcting misuse of verbforms.
In ACL, pages 174?182, Columbus, Ohio,June.
Association for Computational Linguistics.J.
Lee.
2011.
Verb tense generation.
Social and Be-havioral Sciences, 27:122?130.R.
Nagata, E. Whittaker, and V. Sheinman.
2011.
Cre-ating a manually error-tagged and shallow-parsedlearner corpus.
In ACL, pages 1210?1219, Portland,Oregon, USA, June.
Association for ComputationalLinguistics.H.
T. Ng, S. M. Wu, Y. Wu, Ch.
Hadiwinoto, andJ.
Tetreault.
2013.
The CoNLL-2013 shared taskon grammatical error correction.
In Proc.
of theSeventeenth Conference on Computational Natural366Language Learning.
Association for ComputationalLinguistics.V.
Punyakanok and D. Roth.
2001.
The use of classi-fiers in sequential inference.
In The Conference onAdvances in Neural Information Processing Systems(NIPS), pages 995?1001.
MIT Press.R.
Quirk, S. Greenbaum, G. Leech, and J. Svartvik.1985.
A Comprehensive Grammar of the EnglishLanguage.
Longman, New York.A.
Radford.
1988.
Transformational Grammar.
Cam-bridge University Press.R.
Reichart and A. Rappoport.
2010.
Tense sensedisambiguation: A new syntactic polysemy task.In Proceedings of the 2010 Conference on Empiri-cal Methods in Natural Language Processing, pages325?334, Cambridge, MA, October.
Association forComputational Linguistics.A.
Rozovskaya and D. Roth.
2010a.
Annotating ESLerrors: Challenges and rewards.
In Proceedings ofthe NAACL Workshop on Innovative Use of NLP forBuilding Educational Applications.A.
Rozovskaya and D. Roth.
2010b.
Trainingparadigms for correcting errors in grammar and us-age.
In Proceedings of the NAACL-HLT.A.
Rozovskaya and D. Roth.
2011.
Algorithm selec-tion and model adaptation for esl correction tasks.In ACL, Portland, Oregon, 6.
Association for Com-putational Linguistics.A.
Rozovskaya, K.-W. Chang, M. Sammons, andD.
Roth.
2013.
The University of Illinois systemin the CoNLL-2013 shared task.
In CoNLL SharedTask.A.
Rozovskaya.
2013.
Automated Methods for TextCorrection.
Ph.D. thesis.T.
Tajiri, M. Komachi, and Y. Matsumoto.
2012.
Tenseand aspect error correction for esl learners usingglobal context.
In Proceedings of the 50th AnnualMeeting of the Association for Computational Lin-guistics (Volume 2: Short Papers), pages 198?202,Jeju Island, Korea, July.
Association for Computa-tional Linguistics.J.
Tetreault, J.
Foster, and M. Chodorow.
2010.
Us-ing parse features for preposition selection and errordetection.
In ACL.H.
Yannakoudakis, T. Briscoe, and B. Medlock.
2011.A new dataset and method for automatically gradingesol texts.
In Proceedings of the 49th Annual Meet-ing of the Association for Computational Linguis-tics, pages 180?189, Portland, Oregon, USA, June.Association for Computational Linguistics.367
