Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 912?920,Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational LinguisticsA Ranking-based Approach to Word Reorderingfor Statistical Machine Translation?Nan Yang?, Mu Li?, Dongdong Zhang?, and Nenghai Yu?
?MOE-MS Key Lab of MCCUniversity of Science and Technology of Chinav-nayang@microsoft.com, ynh@ustc.edu.cn?Microsoft Research Asia{muli,dozhang}@microsoft.comAbstractLong distance word reordering is a majorchallenge in statistical machine translation re-search.
Previous work has shown using sourcesyntactic trees is an effective way to tacklethis problem between two languages with sub-stantial word order difference.
In this work,we further extend this line of exploration andpropose a novel but simple approach, whichutilizes a ranking model based on word or-der precedence in the target language to repo-sition nodes in the syntactic parse tree of asource sentence.
The ranking model is auto-matically derived from word aligned paralleldata with a syntactic parser for source lan-guage based on both lexical and syntacticalfeatures.
We evaluated our approach on large-scale Japanese-English and English-Japanesemachine translation tasks, and show that it cansignificantly outperform the baseline phrase-based SMT system.1 IntroductionModeling word reordering between source and tar-get sentences has been a research focus since theemerging of statistical machine translation.
Inphrase-based models (Och, 2002; Koehn et al,2003), phrase is introduced to serve as the funda-mental translation element and deal with local re-ordering, while a distance based distortion model isused to coarsely depict the exponentially decayedword movement probabilities in language transla-tion.
Further work in this direction employed lexi-?This work has been done while the first author was visitingMicrosoft Research Asia.calized distortion models, including both generative(Koehn et al, 2005) and discriminative (Zens andNey, 2006; Xiong et al, 2006) variants, to achievefiner-grained estimations, while other work took intoaccount the hierarchical language structures in trans-lation (Chiang, 2005; Galley and Manning, 2008).Long-distance word reordering between languagepairs with substantial word order difference, such asJapanese with Subject-Object-Verb (SOV) structureand English with Subject-Verb-Object (SVO) struc-ture, is generally viewed beyond the scope of thephrase-based systems discussed above, because ofeither distortion limits or lack of discriminative fea-tures for modeling.
The most notable solution to thisproblem is adopting syntax-based SMT models, es-pecially methods making use of source side syntac-tic parse trees.
There are two major categories in thisline of research.
One is tree-to-string model (Quirket al, 2005; Liu et al, 2006) which directly usessource parse trees to derive a large set of translationrules and associated model parameters.
The otheris called syntax pre-reordering ?
an approach thatre-positions source words to approximate target lan-guage word order as much as possible based on thefeatures from source syntactic parse trees.
This isusually done in a preprocessing step, and then fol-lowed by a standard phrase-based SMT system thattakes the re-ordered source sentence as input to fin-ish the translation.In this paper, we continue this line of work andaddress the problem of word reordering based onsource syntactic parse trees for SMT.
Similar to mostprevious work, our approach tries to rearrange thesource tree nodes sharing a common parent to mimic912the word order in target language.
To this end, wepropose a simple but effective ranking-based ap-proach to word reordering.
The ranking model isautomatically derived from the word aligned paralleldata, viewing the source tree nodes to be reorderedas list items to be ranked.
The ranks of tree nodes aredetermined by their relative positions in the targetlanguage ?
the node in the most front gets the high-est rank, while the ending word in the target sentencegets the lowest rank.
The ranking model is trainedto directly minimize the mis-ordering of tree nodes,which differs from the prior work based on maxi-mum likelihood estimations of reordering patterns(Li et al, 2007; Genzel, 2010), and does not requireany special tweaking in model training.
The rankingmodel can not only be used in a pre-reordering basedSMT system, but also be integrated into a phrase-based decoder serving as additional distortion fea-tures.We evaluated our approach on large-scaleJapanese-English and English-Japanese machinetranslation tasks, and experimental results show thatour approach can bring significant improvements tothe baseline phrase-based SMT system in both pre-ordering and integrated decoding settings.In the rest of the paper, we will first formallypresent our ranking-based word reordering model,then followed by detailed steps of modeling train-ing and integration into a phrase-based SMT system.Experimental results are shown in Section 5.
Section6 consists of more discussions on related work, andSection 7 concludes the paper.2 Word Reordering as Syntax Tree NodeRankingGiven a source side parse tree Te, the task of wordreordering is to transform Te to T ?e, so that e?
canmatch the word order in target language as much aspossible.
In this work, we only focus on reorderingthat can be obtained by permuting children of everytree nodes in Te.
We use children to denote direct de-scendants of tree nodes for constituent trees; whilefor dependency trees, children of a node include notonly all direct dependents, but also the head worditself.
Figure 1 gives a simple example showing theword reordering between English and Japanese.
Byrearranging the position of tree nodes in the EnglishI am trying to play music??
???
??
????
???
?PRP VBP VBG TO VB NNNPVPVPNPSVPVPSI amtryingtoplaymusicPRP VBPVBGTOVBNNNPVPVPNPSVPVP??
???
??
????
???
?OriginalTreeReordered TreeSj0 j1 j2 j3 j4e0 e1 e2 e3 e4 e5j0 j1 j2 j3 j4e0 e1 e2 e3 e4 e5Figure 1: An English-to-Japanese sentence pair.
Bypermuting tree nodes in the parse tree, the sourcesentence is reordered into the target language or-der.
Constituent tree is shown above the sourcesentence; arrows below the source sentences showhead-dependent arcs for dependency tree; wordalignment links are lines without arrow between thesource and target sentences.parse tree, we can obtain the same word order ofJapanese translation.
It is true that tree-based re-ordering cannot cover all word movement operationsin language translation, previous work showed thatthis method is still very effective in practice (Xu etal., 2009, Visweswariah et al, 2010).Following this principle, the word reordering taskcan be broken into sub-tasks, in which we onlyneed to determine the order of children nodes forall non-leaf nodes in the source parse tree.
For atree node t with children {c1, c2, .
.
.
, cn}, we re-arrange the children to target-language-like order{cpi(i1), cpi(i2), .
.
.
, cpi(in)}.
If we treat the reorderedposition pi(i) of child ci as its ?rank?, the reorder-913ing problem is naturally translated into a rankingproblem: to reorder, we determine a ?rank?
for eachchild, then the children are sorted according to their?ranks?.
As it is often impractical to directly assigna score for each permutation due to huge number ofpossible permutations, a widely used method is touse a real valued function f to assign a value to eachnode, which is called a ranking function (Herbrichet al, 2000).
If we can guarantee (f(i)?
f(j)) and(pi(i) ?
pi(j)) always has the same sign, we can getthe same permutation as pi because values of f areonly used to sort the children.
For example, con-sider the node rooted at trying in the dependencytree in Figure 1.
Four children form a list {I, am, try-ing, play} to be ranked.
Assuming ranking functionf can assign values {0.94, ?1.83, ?1.50, ?1.20}for {I, am, trying, play} respectively, we can get asorted list {I, play, trying, am}, which is the desiredpermutation according to the target.More formally, for a tree node t with children{c1, c2, .
.
.
, cn}, our ranking model assigns a rankf(ci, t) for each child ci, then the children are sortedaccording to the rank in a descending order.
Theranking function f has the following form:f(ci, t) =?j?j(ci, t) ?
wj (1)where the ?j is a feature representing the tree node tand its child ci, and wj is the corresponding featureweight.3 Ranking Model TrainingTo learn ranking function in Equation (1), we need todetermine the feature set ?
and learn weight vectorw from reorder examples.
In this section, we firstdescribe how to extract reordering examples fromparallel corpus; then we show our features for rank-ing function; finally, we discuss how to train themodel from the extracted examples.3.1 Reorder Example AcquisitionFor a sentence pair (e, f, a) with syntax tree Te onthe source side, we need to determine which re-ordered tree T ?e?
best represents the word order intarget sentence f .
For a tree node t in Te, if its chil-dren align to disjoint target spans, we can simply ar-range them in the order of their corresponding targetProb lem w ith latter procedure??lies?
??
???
?
?in ??
?
?Prob lem w ith latter procedure??lies?
??
???
?
?in ??
??
( a)  gold alignment( b )  auto alignmentFigure 2: Fragment of a sentence pair.
(a) showsgold alignment; (b) shows automatically generatedalignment which contains errors.spans.
Figure 2 shows a fragment of one sentencepair in our training data.
Consider the subtree rootedat word ?Problem?.
With the gold alignment, ?Prob-lem?
is aligned to the 5th target word, and ?withlatter procedure?
are aligned to target span [1, 3],thus we can simply put ?Problem?
after ?with latterprocedure?.
Recursively applying this process downthe subtree, we get ?latter procedure with Problem?which perfectly matches the target language.As pointed out by (Li et al, 2007), in practice,nodes often have overlapping target spans due to er-roneous word alignment or different syntactic struc-tures between source and target sentences.
(b) inFigure 2 shows the automatically generated align-ment for the sentence pair fragment.
The word?with?
is incorrectly aligned to the 6th Japaneseword ?ha?
; as a result, ?with latter procedure?
nowhas target span [1, 6], while ?Problem?
aligns to[5, 5].
Due to this overlapping, it becomes unclearwhich permutation of ?Problem?
and ?with latterprocedure?
is a better match of the target phrase; weneed a better metric to measure word order similar-ity between reordered source and target sentences.We choose to find the tree T ?e?
with minimal align-ment crossing-link number (CLN) (Genzel, 2010)to f as our golden reordered tree.1 Each crossing-1A simple solution is to exclude all trees with overlappingtarget spans from training.
But in our experiment, this method914link (i1j1, i2j2) is a pair of alignment links crossingeach other.
CLN reaches zero if f is monotonicallyaligned to e?, and increases as there are more wordreordering between e?
and f .
For example, in Fig-ure 1, there are 6 crossing-links in the original tree:(e1j4, e2j3), (e1j4, e4j2), (e1j4, e5j1), (e2j3, e4j2),(e2j3, e5j1) and (e4j2, e5j1); thus CLN for the origi-nal tree is 6.
CLN for the reordered tree is 0 as thereare no crossing-links.
This metric is easy to com-pute, and is not affected by unaligned words (Gen-zel, 2010).We need to find the reordered tree with minimalCLN among all reorder candidates.
As the numberof candidates is in the magnitude exponential withrespect to the degree of tree Te 2, it is not alwayscomputationally feasible to enumerate through allcandidates.
Our solution is as follows.First, we give two definitions.?
CLN(t): the number of crossing-links(i1j1, i2j2) whose source words e?i1 and e?i2both fall under sub span of the tree node t.?
CCLN(t): the number of crossing-links(i1j1, i2j2) whose source words e?i1 and e?i2 fallunder sub span of t?s two different childrennodes c1 and c2 respectively.Apparently CLN of a tree T ?
equals toCLN(root of T ?
), and CLN(t) can be recur-sively expressed as:CLN(t) = CCLN(t) +?child c of tCLN(c)Take the original tree in Figure 1 for example.
At theroot node trying, CLN(trying) is 6 because there aresix crossing-links under its sub-span: (e1j4, e2j3),(e1j4, e4j2), (e1j4, e5j1), (e2j3, e4j2), (e2j3, e5j1)and (e4j2, e5j1).
On the other hand, CCLN(trying)is 5 because (e4j2, e5j1) falls under its child nodeplay, thus does not count towards CCLN of trying.From the definition, we can easily see thatCCLN(t) can be determined solely by the order oft?s direct children, and CLN(t) is only affected bydiscarded too many training instances and led to degraded re-ordering performance.2In our experiments, there are nodes with more than 10 chil-dren for English dependency trees.the reorder in the subtree of t. This observation en-ables us to divide the task of finding the reorderedtree T ?e?
with minimal CLN into independently find-ing the children permutation of each node with min-imal CCLN.
Unfortunately, the time cost for the sub-task is stillO(n!)
for a node with n children.
Insteadof enumerating through all permutations, we onlysearch the Inversion Transduction Grammar neigh-borhood of the initial sequence (Tromble, 2009).
Aspointed out by (Tromble, 2009), the ITG neighbor-hood is large enough for reordering task, and can besearched through efficiently using a CKY decoder.After finding the best reordered tree T ?e?
, we canextract one reorder example from every node withmore than one child.3.2 FeaturesFeatures for the ranking model are extracted fromsource syntax trees.
For English-to-Japanese task,we extract features from Stanford English Depen-dency Tree (Marneffe et al, 2006), including lexi-cons, Part-of-Speech tags, dependency labels, punc-tuations and tree distance between head and depen-dent.
For Japanese-to-English task, we use a chunk-based Japanese dependency tree (Kudo and Mat-sumoto, 2002).
Different from features for English,we do not use dependency labels because they arenot available from the Japanese parser.
Additionally,Japanese function words are also included as fea-tures because they are important grammatical clues.The detailed feature templates are shown in Table 1.3.3 Learning MethodThere are many well studied methods available tolearn the ranking function from extracted examples.,ListNet (?)
etc.
We choose to use RankingSVM(Herbrich et al, 2000), a pair-wised ranking method,for its simplicity and good performance.For every reorder example t with children{c1, c2, .
.
.
, cn} and their desired permutation{cpi(i1), cpi(i2), .
.
.
, cpi(in)}, we decompose it into aset of pair-wised training instances.
For any twochildren nodes ci and cj with i < j , we extract apositive instance if pi(i) < pi(j), otherwise we ex-tract a negative instance.
The feature vector for bothpositive instance and negative instance is (?ci?
?cj ),where ?ci and ?cj are feature vectors for ci and cj915E-Jcl cl ?
dst cl ?
pctcl ?
dst ?
pct cl ?
lcl cl ?
rclcl ?
lcl ?
dst cl ?
rcl ?
dst cl ?
clexcl ?
clex cl ?
clex ?
dst cl ?
clex ?
dstcl ?
hlex cl ?
hlex cl ?
hlex ?
dstcl ?
hlex ?
dst cl ?
clex ?
pct cl ?
clex ?
pctcl ?
hlex ?
pct cl ?
hlex ?
pctJ-Ectf ctf ?
dst ctf ?
lctctf ?
rct ctf ?
lct ?
dst cl ?
rct ?
dstctf ?
clex ctf ?
clex ctf ?
clex ?
dstctf ?
clex ?
dst ctf ?
hf ctf ?
hfctf ?
hf ?
dst ctf ?
hf ?
dst ctf ?
hlexctf ?
hlex ctf ?
hlex ?
dst ctf ?
hlex ?
dstTable 1: Feature templates for ranking function.
Alltemplates are implicitly conjuncted with the pos tagof head node.c: child to be ranked; h: head nodelc: left sibling of c; rc: right sibling of cl: dependency label; t: pos taglex: top frequency lexiconsf : Japanese function worddst: tree distance between c and hpct: punctuation node between c and hrespectively.
In this way, ranking function learningis turned into a simple binary classification problem,which can be easily solved by a two-class linear sup-port vector machine.4 Integration into SMT systemThere are two ways to integrate the ranking reorder-ing model into a phrase-based SMT system: the pre-reorder method, and the decoding time constraintmethod.For pre-reorder method, ranking reorder modelis applied to reorder source sentences during bothtraining and decoding.
Reordered sentences can gothrough the normal pipeline of a phrase-based de-coder.The ranking reorder model can also be integratedinto a phrase based decoder.
Integrated method takesthe original source sentence e as input, and rankingmodel generates a reordered e?
as a word order ref-erence for the decoder.
A simple penalty schemeis utilized to penalize decoder reordering violatingranking reorder model?s prediction e?.
In this paper,our underlying decoder is a CKY decoder follow-ing Bracketing Transduction Grammar (Wu, 1997;Xiong et al, 2006), thus we show how the penaltyis implemented in the BTG decoder as an example.Similar penalty can be designed for other decoderswithout much effort.Under BTG, three rules are used to derive transla-tions: one unary terminal rule, one straight rule andone inverse rule:A ?
e/fA ?
[A1, A2]A ?
?A1, A2?We have three penalty triggers when any rules areapplied during decoding:?
Discontinuous penalty fdc: it fires for all ruleswhen source span of either A, A1 or A2 ismapped to discontinuous span in e?.?
Wrong straight rule penalty fst: it fires forstraight rule when source spans of A1 and A2are not mapped to two adjacent spans in e?
instraight order.?
Wrong inverse rule penalty fiv: it fires for in-verse rule when source spans of A1 and A2 arenot mapped to two adjacent spans in e?
in in-verse order.The above three penalties are added as additionalfeatures into the log-linear model of the phrase-based system.
Essentially they are soft constraintsto encourage the decoder to choose translations withword order similar to the prediction of ranking re-order model.5 ExperimentsTo test our ranking reorder model, we carry out ex-periments on large scale English-To-Japanese, andJapanese-To-English translation tasks.5.1 Data5.1.1 Evaluation DataWe collect 3,500 Japanese sentences and 3,500English sentences from the web.
They come from916a wide range of domains, such as technical docu-ments, web forum data, travel logs etc.
They aremanually translated into the other language to pro-duce 7,000 sentence pairs, which are split into twoparts: 2,000 pairs as development set (dev) and theother 5,000 pairs as test set (web test).Beside that, we collect another 999 English sen-tences from newswire domain which are translatedinto Japanese to form an out-of-domain test data set(news test).5.1.2 Parallel CorpusOur parallel corpus is crawled from the web,containing news articles, technical documents, blogentries etc.
After removing duplicates, we haveabout 18 million sentence pairs, which contain about270 millions of English tokens and 320 millions ofJapanese tokens.
We use Giza++ (Och and Ney,2003) to generate the word alignment for the parallelcorpus.5.1.3 Monolingual CorpusOur monolingual Corpus is also crawled from theweb.
After removing duplicate sentences, we have acorpus of over 10 billion tokens for both English andJapanese.
This monolingual corpus is used to traina 4-gram language model for English and Japaneserespectively.5.2 ParsersFor English, we train a dependency parser as (Nivreand Scholz, 2004) on WSJ portion of Penn Tree-bank, which are converted to dependency trees us-ing Stanford Parser (Marneffe et al, 2006).
We con-vert the tokens in training data to lower case, andre-tokenize the sentences using the same tokenizerfrom our MT system.For Japanese parser, we use CABOCHA, achunk-based dependency parser (Kudo and Mat-sumoto, 2002).
Some heuristics are used to adaptCABOCHA generated trees to our word segmenta-tion.5.3 Settings5.3.1 Baseline SystemWe use a BTG phrase-based system with a Max-Ent based lexicalized reordering model (Wu, 1997;Xiong et al, 2006) as our baseline system forboth English-to-Japanese and Japanese-to-EnglishExperiment.
The distortion model is trained on thesame parallel corpus as the phrase table using ahome implemented maximum entropy trainer.In addition, a pre-reorder system using manualrules as (Xu et al, 2009) is included for the English-to-Japanese experiment (ManR-PR).
Manual rulesare tuned by a bilingual speaker on the developmentset.5.3.2 Ranking Reordering SystemRanking reordering model is learned from thesame parallel corpus as phrase table.
For efficiencyreason, we only use 25% of the corpus to train ourreordering model.
LIBLINEAR (Fan et al, 2008) isused to do the SVM optimization for RankingSVM.We test it on both pre-reorder setting (Rank-PR)and integrated setting (Rank-IT).5.4 End-to-End Resultsystem dev web test news testE-JBaseline 21.45 21.12 14.18ManR-PR 23.00 22.42 15.61Rank-PR 22.92 22.51 15.90Rank-IT 23.14 22.85 15.72J-EBaseline 25.39 24.20 14.26Rank-PR 26.57 25.56 15.42Rank-IT 26.72 25.87 15.27Table 2: BLEU(%) score on dev and test data forboth E-J and J-E experiment.
All settings signifi-cantly improve over the baseline at 95% confidencelevel.
Baseline is the BTG phrase system system;ManR-PR is pre-reorder with manual rule; Rank-PRis pre-reorder with ranking reorder model; Rank-ITis system with integrated ranking reorder model.From Table 2, we can see our ranking reorderingmodel significantly improves the performance forboth English-to-Japanese and Japanese-to-Englishexperiments over the BTG baseline system.
It alsoout-performs the manual rule set on English-to-Japanese result, but the difference is not significant.5.5 Reordering PerformanceIn order to show whether the improved performanceis really due to improved reordering, we would liketo measure the reorder performance directly.917As we do not have access to a golden re-ordered sentence set, we decide to use the align-ment crossing-link numbers between aligned sen-tence pairs as the measure for reorder performance.We train the ranking model on 25% of our par-allel corpus, and use the rest 75% as test data(auto).
We sample a small corpus (575 sentencepairs) and do manual alignment (man-small).
Wedenote the automatic alignment for these 575 sen-tences as (auto-small).
From Table 3, we can seesetting auto auto-small man-smallNone 36.3 35.9 40.1E-JOracle 4.3 4.1 7.4ManR 13.4 13.6 16.7Rank 12.1 12.8 17.2J-EOracle 6.9 7.0 9.4Rank 15.7 15.3 20.5Table 3: Reorder performance measured bycrossing-link number per sentence.
None means theoriginal sentences without reordering; Oracle meansthe best permutation allowed by the source parsetree; ManR refers to manual reorder rules; Rankmeans ranking reordering model.our ranking reordering model indeed significantlyreduces the crossing-link numbers over the originalsentence pairs.
On the other hand, the performanceof the ranking reorder model still fall far short of or-acle, which is the lowest crossing-link number of allpossible permutations allowed by the parse tree.
Bymanual analysis, we find that the gap is due to botherrors of the ranking reorder model and errors fromword alignment and parser.Another thing to note is that the crossing-linknumber of manual alignment is higher than auto-matic alignment.
The reason is that our annotatorstend to align function words which might be left un-aligned by automatic word aligner.5.6 Effect of Ranking FeaturesHere we examine the effect of features for rankingreorder model.
We compare their influence on Rank-ingSVM accuracy, alignment crossing-link number,end-to-end BLEU score, and the model size.
AsTable 4 shows, a major part of reduction of CLNcomes from features such as Part-of-Speech tags,Features Acc.
CLN BLEU Feat.#E-Jtag+label 88.6 16.4 22.24 26k+dst 91.5 13.5 22.66 55k+pct 92.2 13.1 22.73 79k+lex100 92.9 12.1 22.85 347k+lex1000 94.0 11.5 22.79 2,410k+lex2000 95.2 10.7 22.81 3,794kJ-Etag+fw 85.0 18.6 25.43 31k+dst 90.3 16.9 25.62 65k+lex100 91.6 15.7 25.87 293k+lex1000 92.4 14.8 25.91 2,156k+lex2000 93.0 14.3 25.84 3,297kTable 4: Effect of ranking features.
Acc.
is Rank-ingSVM accuracy in percentage on the training data;CLN is the crossing-link number per sentence onparallel corpus with automatically generated wordalignment; BLEU is the BLEU score in percentageon web test set on Rank-IT setting (system with in-tegrated rank reordering model); lexn means n mostfrequent lexicons in the training corpus.dependency labels (for English), function words (forJapanese), and the distance and punctuations be-tween child and head.
These features also corre-spond to BLEU score improvement for End-to-Endevaluations.
Lexicon features generally continue toimprove the RankingSVM accuracy and reduce CLNon training data, but they do not bring further im-provement for SMT systems beyond the top 100most frequent words.
Our explanation is that lessfrequent lexicons tend to help local reordering only,which is already handled by the underlying phrase-based system.5.7 Performance on different domainsFrom Table 2 we can see that pre-reorder method hashigher BLEU score on news test, while integratedmodel performs better on web test set which con-tains informal texts.
By error analysis, we find thatthe parser commits more errors on informal texts,and informal texts usually have more flexible trans-lations.
Pre-reorder method makes ?hard?
decisionbefore decoding, thus is more sensitive to parser er-rors; on the other hand, integrated model is forcedto use a longer distortion limit which leads to moresearch errors during decoding time.
It is possible to918use system combination method to get the best ofboth systems, but we leave this to future work.6 Discussion on Related WorkThere have been several studies focusing on compil-ing hand-crafted syntactic reorder rules.
Collins etal.
(2005), Wang et al (2007), Ramanathan et al(2008), Lee et al (2010) have developed rules forGerman-English, Chinese-English, English-Hindiand English-Japanese respectively.
Xu et al (2009)designed a clever precedence reordering rule set fortranslation from English to several SOV languages.The drawback for hand-crafted rules is that they de-pend upon expert knowledge to produce and are lim-ited to their targeted language pairs.Automatically learning syntactic reordering ruleshave also been explored in several work.
Li etal.
(2007) and Visweswariah et al (2010) learnedprobability of reordering patterns from constituenttrees using either Maximum Entropy or maximumlikelihood estimation.
Since reordering patternsare matched against a tree node together with allits direct children, data sparseness problem willarise when tree nodes have many children (Li etal., 2007); Visweswariah et al (2010) also men-tioned their method yielded no improvement whenapplied to dependency trees in their initial experi-ments.
Genzel (2010) dealt with the data sparsenessproblem by using window heuristic, and learned re-ordering pattern sequence from dependency trees.Even with the window heuristic, they were unableto evaluate all candidates due to the huge num-ber of possible patterns.
Different from the pre-vious approaches, we treat syntax-based reorderingas a ranking problem between different source treenodes.
Our method does not require the sourcenodes to match some specific patterns, but encodesreordering knowledge in the form of a ranking func-tion, which naturally handles reordering betweenany number of tree nodes; the ranking function istrained by well-established rank learning method tominimize the number of mis-ordered tree nodes inthe training data.Tree-to-string systems (Quirk et al, 2005; Liu etal., 2006) model syntactic reordering using minimalor composed translation rules, which may containreordering involving tree nodes from multiple treelevels.
Our method can be naturally extended to dealwith such multiple level reordering.
For a tree-to-string rule with multiple tree levels, instead of rank-ing the direct children of the root node, we rank allleaf nodes (Most are frontier nodes (Galley et al,2006)) in the translation rule.
We need to redesignour ranking feature templates to encode the reorder-ing information in the source part of the translationrules.
We need to remember the source side con-text of the rules, the model size would still be muchsmaller than a full-fledged tree-to-string system be-cause we do not need to explicitly store the targetvariants for each rule.7 Conclusion and Future WorkIn this paper we present a ranking based reorder-ing method to reorder source language to match theword order of target language given the source sideparse tree.
Reordering is formulated as a task to rankdifferent nodes in the source side syntax tree accord-ing to their relative position in the target language.The ranking model is automatically trained to min-imize the mis-ordering of tree nodes in the trainingdata.
Large scale experiment shows improvement onboth reordering metric and SMT performance, withup to 1.73 point BLEU gain in our evaluation test.In future work, we plan to extend the rankingmodel to handle reordering between multiple lev-els of source trees.
We also expect to explore bet-ter way to integrate ranking reorder model into SMTsystem instead of a simple penalty scheme.
Alongthe research direction of preprocessing the sourcelanguage to facilitate translation, we consider to notonly change the order of the source language, butalso inject syntactic structure of the target languageinto source language by adding pseudo words intosource sentences.AcknowledgementsNan Yang and Nenghai Yu were partially supportedby Fundamental Research Funds for the CentralUniversities (No.
WK2100230002), National Nat-ural Science Foundation of China (No.
60933013),and National Science and Technology Major Project(No.
2010ZX03004-003).919ReferencesDavid Chiang.
2005.
A Hierarchical Phrase-BasedModel for Statistical Machine Translation.
In Proc.ACL, pages 263-270.Michael Collins, Philipp Koehn and Ivona Kucerova.2005.
Clause restructuring for statistical machinetranslation.
In Proc.
ACL.R.-E.
Fan, K.-W. Chang, C.-J.
Hsieh, X.-R. Wang, andC.-J.
Lin.
2008.
LIBLINEAR: A library for large lin-ear classification.
In Journal of Machine Learning Re-search.Michel Galley, Jonathan Graehl, Kevin Knight, DanielMarcu, Steve DeNeefe, Wei Wang, and IgnacioThayer.
2006.
Scalable Inference and Training ofContext-Rich Syntactic Translation Models.
In Proc.ACL-Coling, pages 961-968.Michel Galley and Christopher D. Manning.
2008.
ASimple and Effective Hierarchical Phrase ReorderingModel.
In Proc.
EMNLP, pages 263-270.Dmitriy Genzel.
2010.
Automatically Learning Source-side Reordering Rules for Large Scale Machine Trans-lation.
In Proc.
Coling, pages 376-384.Ralf Herbrich, Thore Graepel, and Klaus Obermayer2000.
Large Margin Rank Boundaries for Ordinal Re-gression.
In Advances in Large Margin Classifiers,pages 115-132.Philipp Koehn, Amittai Axelrod, Alexandra BirchMayne, Chris Callison-Burch, Miles Osborne andDavid Talbot.
2005.
Edinborgh System Descriptionfor the 2005 IWSLT Speech Translation Evaluation.
InInternational Workshop on Spoken Language Transla-tion.Philipp Koehn, Franz J. Och, and Daniel Marcu.
2003.Statistical Phrase-Based Translation.
In Proc.
HLT-NAACL, pages 127-133.Taku Kudo, Yuji Matsumoto.
2002.
Japanese Depen-dency Analysis using Cascaded Chunking.
In Proc.CoNLL, pages 63-69.Young-Suk Lee, Bing Zhao and Xiaoqiang Luo.
2010.Constituent reordering and syntax models for English-to-Japanese statistical machine translation.
In Proc.Coling.Chi-Ho Li, Minghui Li, Dongdong Zhang, Mu Li andMing Zhou and Yi Guan 2007.
A Probabilistic Ap-proach to Syntax-based Reordering for Statistical Ma-chine Translation.
In Proc.
ACL, pages 720-727.Yang Liu, Qun Liu, and Shouxun Lin.
2006.
Tree-to-String Alignment Template for Statistical MachineTranslation.
In Proc.
ACL-Coling, pages 609-616.Marie-Catherine de Marneffe, Bill MacCartney andChristopher D. Manning.
2006.
Generating TypedDependency Parses from Phrase Structure Parses.
InLREC 2006Joakim Nivre and Mario Scholz 2004.
Deterministic De-pendency Parsing for English Text.
In Proc.
Coling.Franz J. Och.
2002.
Statistical Machine Translation:From Single Word Models to Alignment Template.Ph.D.Thesis, RWTH Aachen, GermanyFranz J. Och and Hermann Ney.
2003.
A SystematicComparison of Various Statistical Alignment Models.Computational Linguistics, 29(1): pages 19-51.Chris Quirk, Arul Menezes, and Colin Cherry.
2005.
De-pendency Treelet Translation: Syntactically InformedPhrasal SMT.
In Proc.
ACL, pages 271-279.A.
Ramanathan, Pushpak Bhattacharyya, JayprasadHegde, Ritesh M. Shah and Sasikumar M. 2008.Simple syntactic and morphological processing canhelp English-Hindi Statistical Machine Translation.In Proc.
IJCNLP.Roy Tromble.
2009.
Search and Learning for the Lin-ear Ordering Problem with an Application to MachineTranslation.
Ph.D. Thesis.Karthik Visweswariah, Jiri Navratil, Jeffrey Sorensen,Vijil Chenthamarakshan and Nandakishore Kamb-hatla.
2010.
Syntax Based Reordering with Automat-ically Derived Rules for Improved Statistical MachineTranslation.
In Proc.
Coling, pages 1119-1127.Chao Wang, Michael Collins, Philipp Koehn.
2007.
Chi-nese syntactic reordering for statistical machine trans-lation.
In Proc.
EMNLP-CoNLL.Dekai Wu.
1997.
Stochastic Inversion TransductionGrammars and Bilingual Parsing of Parallel Corpora.Computational Linguistics, 23(3): pages 377-403.Deyi Xiong, Qun Liu, and Shouxun Lin.
2006.
Maxi-mum Entropy Based Phrase Reordering Model for Sta-tistical Machine Translation.
In Proc.
ACL-Coling,pages 521-528.Peng Xu, Jaeho Kang, Michael Ringgaard, Franz Och.2009.
Using a Dependency Parser to Improve SMTfor Subject-Object-Verb Languages.
In Proc.
HLT-NAACL, pages 376-384.Richard Zens and Hermann Ney.
2006.
DiscriminativeReordering Models for Statistical Machine Transla-tion.
In Proc.
Workshop on Statistical Machine Trans-lation, HLT-NAACL, pages 127-133.920
