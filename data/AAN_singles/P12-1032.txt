Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 302?310,Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational LinguisticsLearning Translation Consensus with Structured Label Propagation?Shujie Liu*, ?Chi-Ho Li, ?Mu Li and ?Ming Zhou?
Harbin Institute of Technology ?Microsoft Research AsiaHarbin, China Beijing, Chinashujieliu@mtlab.hit.edu.cn{chl, muli, mingzhou}@microsoft.comAbstractIn this paper, we address the issue forlearning better translation consensus inmachine translation (MT) research, andexplore the search of translation consensusfrom similar, rather than the same, sourcesentences or their spans.
Unlike previouswork on this topic, we formulate theproblem as structured labeling over a muchsmaller graph, and we propose a novelstructured label propagation for the task.We convert such graph-based translationconsensus from similar source strings intouseful features both for n-best output re-ranking and for decoding algorithm.Experimental results show that, our methodcan significantly improve machinetranslation performance on both IWSLTand NIST data, compared with a state-of-the-art baseline.1 IntroductionConsensus in translation has?
gained more andmore attention in recent years.
The principle ofconsensus can be sketched as ?a translationcandidate is deemed more plausible if it issupported by other translation candidates.?
Theactual formulation of the principle depends onwhether the translation candidate is a completesentence or just a span of it, whether the candidateis the same as or similar to the supportingcandidates, and whether the supporting candidatescome from the same or different MT system.?
This work has been done while the first author was visitingMicrosoft Research Asia.Translation consensus is employed in thoseminimum Bayes risk (MBR) approaches where theloss function of a translation is defined withrespect to all other translation candidates.
That is,the translation with the minimal Bayes risk is theone to the greatest extent similar to othercandidates.
These approaches include the work ofKumar and Byrne (2004), which re-ranks the n-best output of a MT decoder, and the work ofTromble et al (2008) and Kumar et al (2009),which does MBR decoding for lattices andhypergraphs.Others extend consensus among translationsfrom the same MT system to those from differentMT systems.
Collaborative decoding (Li et al,2009) scores the translation of a source span by itsn-gram similarity to the translations by othersystems.
Hypothesis mixture decoding (Duan et al,2011) performs a second decoding process wherethe search space is enriched with new hypothesescomposed out of existing hypotheses from multiplesystems.All these approaches are about utilizingconsensus among translations for the same (spanof) source sentence.
It should be noted thatconsensus among translations of similar sourcesentences/spans is also helpful for good candidateselection.
Consider the examples in Figure 1.
Forthe source (Chinese) span ???
?
??
?
?
?,the MT system produced the correct translation forthe second sentence, but it failed to do so for thefirst one.
If the translation of the first sentencecould take into consideration the translation of thesecond sentence, which is similar to but notexactly the same as the first one, the finaltranslation output may be improved.Following this line of reasoning, adiscriminative learning method is proposed toconstrain the translation of an input sentence using302the most similar translation examples fromtranslation memory (TM) systems (Ma et al,2011).
A classifier is applied to re-rank the n-bestoutput of a decoder, taking as features theinformation about the agreement with those similartranslation examples.
Alexandrescu and Kirchhoff(2009) proposed a graph-based semi-supervisedmodel to re-rank n-best translation output.
Notethat these two attempts are about translationconsensus for similar sentences, and about re-ranking of n-best output.
It is still an open questionwhether translation consensus for similarsentences/spans can be applied to the decodingprocess.
Moreover, the method in Alexandrescuand Kirchhoff (2009) is formulated as a typical andsimple label propagation, which leads to very largegraph, thus making learning and search inefficient.(c.f.
Section 3.
)In this paper, we attempt to leverage translationconsensus among similar (spans of) sourcesentences in bilingual training data, by a novelgraph-based model of translation consensus.Unlike Alexandrescu and Kirchhoff (2009), wereformulate the task of seeking translationconsensus among source sentences as structuredlabeling.
We propose a novel label propagationalgorithm for structured labeling, which is muchmore efficient than simple label propagation, andderive useful MT decoder features out of it.
Weconduct experiments with IWSLT and NIST data,and experimental results show that, our methodcan improve the translation performancesignificantly on both data sets, compared with astate-of-the-art baseline.2 Graph-based Translation ConsensusOur MT system with graph-based translationconsensus adopts the conventional log-linearmodel.
For the source string ?
, the conditionalprobability of a translation candidate ?
is definedas:???|??
?
exp ??
??????
?, ????
??
?exp??
???????
?, ????
?????????
(1)where ?
is the feature vector, ?
is the featureweights, and ????
is the set of translationhypotheses in the search space.Based on the commonly used features, twokinds of feature are added to equation (1), one isgraph-based consensus features, which are aboutconsensus among the translations of similarsentences/spans; the other is local consensusfeatures, which are about consensus among thetranslations of the same sentence/span.
Wedevelop a structured label propagation method,which can calculate consensus statistics fromtranslation candidates of similar sourcesentences/spans.In the following, we explain why the standard,simple label propagation is not suitable fortranslation consensus, and then introduce how theproblem is formulated as an instance of structuredlabeling, with the proposed structured labelpropagation algorithm, in section 3.
Beforeelaborating how the graph model of consensus isconstructed for both a decoder and N-best outputre-ranking in section 5, we will describe how theconsensus features and their feature weights can betrained in a semi-supervised way, in section 4.3 Graph-based Structured LearningIn general, a graph-based model assigns labels toinstances by considering the labels of similarinstances.
A graph is constructed so that eachinstance is represented by a node, and the weightof the edge between a pair of nodes represents thesimilarity between them.
The gist of graph-basedmodel is that, if two instances are connected by astrong edge, then their labels tend to be the same(Zhu, 2005).IWSLT Chinese to English Translation TaskSrc ?
???
??
?
??
?
?
?Ref Do you have any tea under fivehundred dollars ?Best1 Do you have any less than fivehundred dollars tea ?Src ?
??
??
?
??
?
?
.Ref I would like some tea under fivehundred dollars .Best1 I would like tea under five hundreddollars .Figure 1.
Two sentences from IWSLT(Chinese to English) data set.
"Src" stands forthe source sentence, and "Ref" means thereference sentence.
"Best1" is the final outputof the decoder.303In MT, the instances are source sentences orspans of source sentences, and the possible labelsare their translation candidates.
This scenariodiffers from the general case of graph-based modelin two aspects.
First, there are an indefinite, oreven intractable, number of labels.
Each of them isa string of words rather than a simple category.
Inthe following we will call these labels as structuredlabels (Berlett et al, 2004).
Second, labels arehighly ?instance-dependent?.
In most cases, for anytwo different (spans of) source sentences, howeversmall their difference is, their correct labels(translations) are not exactly the same.
Therefore,the principle of graph-based translation consensusmust be reformulated as, if two instances (sourcespans) are similar, then their labels (translations)tend to be similar (rather than the same).Note that Alexandrescu and Kirchhoff (2009) donot consider translation as structured labeling.
Intheir graph, a node does not represent only asource sentence but a pair of source sentence andits candidate translation, and there are only twopossible labels for each node, namely, 1 (this is agood translation pair) and 0 (this is not a goodtranslation pair).
Thus their graph-based model is anormal example of the general graph-based model.The biggest problem of such a perspective isinefficiency.
An average MT decoder considers avast amount of translation candidates for eachsource sentence, and therefore the correspondinggraph also contains a vast amount of nodes, thusrendering learning over a large dataset is infeasible.3.1 Label Propagation for General Graph-based ModelsA general graph-based model is iteratively trainedby label propagation, in which ?
?,?, the probabilityof label l for the node ?, is updated with respect tothe corresponding probabilities for ?
?s neighboringnodes ????
.
In Zhu (2005), the updating rule isexpressed in a matrix calculation.
For convenience,the updating rule is expressed for each label here:??,????
?
?
??
?, ????,????????
(2)where ??
?, ?
?,  the propagating probability, isdefined as:??
?, ??
?
??,??
??,?????????(3)??,?
defines the weight of the edge, which is asimilarity measure between nodes ?
and ?.Note that the graph contains nodes for traininginstances, whose correct labels are known.
Theprobability of the correct label to each traininginstance is reset to 1 at the end of each iteration.With a suitable measure of instance/node similarity,it is expected that an unlabeled instance/node willfind the most suitable label from similar labelednodes.3.2 Structured Label Propagation for Graph-based LearningIn structured learning like MT, different instanceswould not have the same correct label, and so theupdating rule (2) is no longer valid, as the value of??,?
should not be calculated based on ??,?
.
Herewe need a new updating rule so that ??,?
can beupdated with respect to ??,??
, where in general?
?
?
?.Let us start with the model in Alexandrescu andKirchhoff (2009).
According to them, a node in thegraph represents the pair of some sourcesentence/span ??
and its translation candidate ??
.The updating rule (for the label 1 or 0) is:???,?????
?
?
???
?, ?
?, ??
?, ????????,???????,????????,???
?4?where ???
?, ??
is the set of neighbors of the node?
?, ?
).When the problem is reformulated as structuredlabeling, each node represents the sourcesentence/span only, and the translation candidatesbecome labels.
The propagating probability???
?, ?
?, ??
?, ?????
has to be reformulatedaccordingly.
A natural way is to decompose it intoa component for nodes and a component for labels.Assuming that the two components areindependent, then:???
?, ?
?, ??
?, ????
?
???
?, ???
???
?, ???????????
?5?where ???
?, ???
is the propagating probability from source sentence/span ??
to ?
, and ???
?, ???
is that from translation candidate  ??
to ?.The set of neighbors ???
?, ??
of a pair ?
?, ?
?has also to be reformulated in terms of the set ofneighbors ????
of a source sentence/span ?:???
?, ??
?
???
?, ???|??
?
???
?, ??
?
????????6?304where???????is?the?set?of?translation?candidates?for?source???.
?The new updating rule will then be:???,????
?
?
???
?, ???
???
?, ??????,??????????,??????????
?
?
???
?, ???
???
?, ??????,????????????????????
?
???
?, ???
?
???
?, ??????,????????????????????
?7?The new rule updates the probability of atranslation ?
of a source sentence/span ?
withprobabilities of similar translations ?
?s  of somesimilar source sentences/spans ?
?s.Propagation probability ???
?, ???
is as defined in equation (3), and ???
?, ???
is defined given some similarity measure ????
?, ???
between labels ?
and??:???
?, ???
?
????
?, ????
????
?, ?????????????
?
????????????
?8?Note that rule (2) is a special case of rule (7),when ????
?, ???
is defined as:????
?, ???
?
?10???????????????
?
???;?????????
;4 Features and TrainingThe last section sketched the structured labelpropagation algorithm.
Before elaborating thedetails of how the actual graph is constructed, wewould like to first introduce how the graph-basedtranslation consensus can be used in an MT system.4.1 Graph-based Consensus FeaturesThe probability as estimated in equation (7) istaken as a group of new features in either adecoder or an n-best output re-ranker.
We will callthese features collectively as graph-basedconsensus features (GC):???
?, ??
?????????????????????????????????????????????????????????????????9??log??
?
???
?, ???
?
???
?, ??????,??????????????????
?Recall that, ????
refers to source sentences/spanswhich are similar with ?
, and ?????
refers totranslation candidates of ??
.
???,??
is initializedwith the translation posterior of ??
given ??
.Thetranslation posterior is normalized in the n-best list.For the nodes representing the training sentencepairs, this posterior is fixed.
?
???
?, ???
is the propagating probability in equation (8), with thesimilarity measure ????
?, ???
defined as the Diceco-efficient over the set of all n-grams in ?
andthose in ??.
That is,????
?, ???
?
???????????
?, ????????
?where ???????
is the set of n-grams in string ?, and ?????
?, ??
is the Dice co-efficient over sets ?and ?:?????
?, ??
?
2|?
?
?||?| ?
|?|We take 1 ?
?
?
4  for similarity betweentranslation candidates, thus leading to four features.The other propagating probability ???
?, ???
, as defined in equation (3),  takes symmetricalsentence level BLEU as similarity measure1:??,??
?12 ????
??????
?, ???
?
???
???????
?, ??
?where ???
??????
?, ???
is defined as follows (Liang et al, 2006):???
??????
?, ???
??
?
?
?????
?, ???2????????????
?10?where ?
?
?????
?, ???
is the IBM BLEU scorecomputed over i-grams for hypothesis ?
using ?
?as reference.In theory we could use other similarity measuressuch as edit distance, string kernel.
Here simple n-gram similarity is used for the sake of efficiency.4.2 Other FeaturesIn addition to graph-based consensus features, wealso propose local consensus features, defined overthe n-best translation candidates as:???
?, ??
?
log ?
?
????|??
????
?, ????????????
(11)1 BLEU is not symmetric, which means, different scores areobtained depending on which one is reference and which oneis hypothesis.305where ????|???
is translation posterior.
Like ??
,there are four features with respect to the value ofn in n-gram similarity measure.We also use other fundamental features, such astranslation probabilities, lexical weights, distortionprobability, word penalty, and language modelprobability.4.3 Training MethodWhen graph-based consensus is applied to an MTsystem, the graph will have nodes for training data,development (dev) data, and test data (details inSection 5).
There is only one label/translation foreach training data node.
For each dev/test datanode, the possible labels are the n-best translationcandidates from the decoder.
Note that there ismutual dependence between the consensus graphand the decoder.
On the one hand, the MT decoderdepends on the graph for the GC features.
On theother hand, the graph needs the decoder to providethe translation candidates as possible labels, andtheir posterior probabilities as initial values ofvarious ??,?
.
Therefore, we can alternativelyupdate graph-based consensus features and featureweights in the log-linear model.Algorithm 1 Semi-Supervised Learning???
?
0;??=?????????
?, ???
?, ????
;while not converged do??
?
????????????
?, ?????
?, ???
?, ????
?, ???.?????
?
????????????.????
?
????????
?, ???
?, ?????
?end whilereturn last (???,???
)Algorithm 1 outlines our semi-supervisedmethod for such alternative training.
The entireprocess starts with a decoder without consensusfeatures.
Then a graph is constructed out of alltraining, dev, and test data.
The subsequentstructured label propagation provides ??
featurevalues to the MT decoder.
The decoder then addsthe new features and re-trains all the featureweights?by Minimum Error Rate Training (MERT)(Och, 2003).
The decoder with new featureweights then provides new n-best candidates andtheir posteriors for constructing another consensusgraph, which in turn gives rise to next round ofMERT.
This alternation of structured labelpropagation and MERT stops when the BLEUscore on dev data converges, or a pre-set limit (10rounds) is reached.5 Graph ConstructionA technical detail is still needed to complete thedescription of graph-based consensus, namely,how the actual consensus graph is constructed.
Wewill divide the discussion into two sectionsregarding how the graph is used.5.1 Graph Construction for Re-RankingWhen graph-based consensus is used for re-ranking the n-best outputs of a decoder, each nodein the graph corresponds to a complete sentence.
Aseparate node is created for each source sentencein training data, dev data, and test data.
For anynode from training data (henceforth training node),it is labeled with the correct translation, and ??,?
isfixed as 1.
If there are sentence pairs with the samesource sentence but different translations, all thetranslations will be assigned as labels to thatsource sentence, and the correspondingprobabilities are estimated by MLE.
There is noedge between training nodes, since we suppose allthe sentences of the training data are correct, and itis pointless to re-estimate the confidence of thosesentence pairs.Each node from dev/test data (henceforth testnode) is unlabeled, but it will be given an n-bestlist of translation candidates as possible labelsfrom a MT decoder.
The decoder also providestranslation posteriors as the initial confidences of1, e1 a1 c b2, e1 a1 b c3, e2 a1 b cE A B C1, f1 b c d12, f1 d1 b c3, f2 d1 b ce1 a1 m n e1 a1 b n e1 d1 b n0.50.50.75 0.5Figure 2.
A toy graph constructed for re-ranking.306the labels.
A test node can be connected to trainingnodes and other test nodes.
If the source sentencesof a test node and some other node are sufficientlysimilar, a similarity edge is created between them.In our experiment we measure similarity bysymmetrical sentence level BLEU of sourcesentences, and 0.3 is taken as the threshold foredge creation.Figure 2 shows a toy example graph.
Each nodeis depicted as rectangle with the upper halfshowing the source sentence and the lower halfshowing the correct or possible labels.
Trainingnodes are in grey while test nodes are in white.The edges between the nodes are weighted by thesimilarities between the corresponding sourcesentences.5.2 Graph Construction for DecodingGraph-based consensus can also be used in thedecoding algorithm, by re-ranking the translationcandidates of not only the entire source sentencebut also every source span.
Accordingly the graphdoes not contain only the nodes for sourcesentences but also the nodes for all source spans.
Itis needed to find the candidate labels for eachsource span.It is not difficult to handle test nodes, since thepurpose of MT decoder is to get al possiblesegmentations of a source sentence in dev/test data,search for the translation candidates of each sourcespan, and calculate the probabilities of thecandidates.
Therefore, the cells in the search spaceof a decoder can be directly mapped as test nodesin the graph.Training nodes can be handled similarly, byapplying forced alignment.
Forced alignmentperforms phrase segmentation and alignment ofeach sentence pair of the training data using thefull translation system as in decoding (Wuebker etal., 2010).
In simpler term, for each sentence pairin training data, a decoder is applied to the sourceside, and all the translation candidates that do notmatch any substring of the target side are deleted.The cells of in such a reduced search space of thedecoder can be directly mapped as training nodesin the graph, just as in the case of test nodes.
Notethat, due to pruning in both decoding andtranslation model training, forced alignment mayfail, i.e.
the decoder may not be able to producetarget side of a sentence pair.
In such case we stillmap the cells in the search space as training nodes.Note also that the shorter a source span is, themore likely it appears in more than one sourcesentence.
All the translation candidates of the samesource span in different source sentences aremerged.Edge creation is the same as that in graphconstruction for n-best re-ranking, except that twonodes are always connected if they are about aspan and its sub-span.
This exception ensures thatshorter spans can always receive propagation fromlonger ones, and vice versa.Figure 3 shows a toy example.
There is onenode for the training sentence "E A M N" and twonodes for the test sentences "E A B C" and "F D BC".
All the other nodes represent spans.
The node"M N" and "E A" are created according to theforced alignment result of the sentence "E A M N".As we see, the translation candidates for "M N"and "E A" are not the sub-strings from the targetsentence of "E A M N".
There are two kinds ofedges.
Dash lines are edges connecting nodes of aspan and its sub-span, such as the one between "EA B C" and "E".
Solid lines are edges connectingnodes with sufficient source side n-gram similarity,such as the one between "E A M N" and "E A BC".Figure 3.
A toy example graph for decoding.Edges in dash line indicate relation between aspan and its sub-span, whereas edges of solidline indicate source side similarity.3076 Experiments and ResultsIn this section, graph-based translation consensusis tested on the Chinese to English translation tasks.The evaluation method is the case insensitive IBMBLEU-4 (Papineni et al, 2002).
Significant testingis carried out using bootstrap re-sampling methodproposed by Koehn (2004) with a 95% confidencelevel.6.1 Experimental Data Setting and BaselinesWe test our method with two data settings: one isIWSLT data set, the other is NIST data set.
Ourbaseline decoder is an in-house implementation ofBracketing Transduction Grammar (Dekai Wu,1997) (BTG) in CKY-style decoding with a lexicalreordering model trained with maximum entropy(Xiong et al, 2006).
The features we used arecommonly used features as standard BTG decoder,such as translation probabilities, lexical weights,language model, word penalty and distortionprobabilities.Our IWSLT data is the IWSLT 2009 dialog taskdata set.
The training data include the BTEC andSLDB training data.
The training data contains 81ksentence pairs, 655k Chinese words and 806English words.
The language model is 5-gramlanguage model trained with the target sentences inthe training data.
The test set is devset9, and thedevelopment set for MERT comprises bothdevset8 and the Chinese DIALOG set.
Thebaseline results on IWSLT data are shown in Table1.devset8+dialog devset9Baseline 48.79 44.73Table 1.
Baselines for IWSLT dataFor the NIST data set, the bilingual training datawe used is NIST 2008 training set excluding theHong Kong Law and Hong Kong Hansard.
Thetraining data contains 354k sentence pairs, 8MChinese words and 10M English words.
Thelanguage model is 5-gram language model trainedwith the Giga-Word corpus plus the Englishsentences in the training data.
The developmentdata utilized to tune the feature weights of ourdecoder is NIST?03 evaluation set, and test sets areNIST?05 and NIST?08 evaluation sets.
Thebaseline results on NIST data are shown in Table 2.NIST'03 NIST'05 NIST'08Baseline 38.57 38.21 27.52Table 2.
Baselines for NIST data6.2 Experimental ResultTable 3 shows the performance of our consensus-based re-ranking and decoding on the IWSLT dataset.
To perform consensus-based re-ranking, wefirst use the baseline decoder to get the n-best listfor each sentence of development and test data,then we create graph using the n-best lists andtraining data as we described in section 5.1, andperform semi-supervised training as mentioned insection 4.3.
As we can see from Table 3, ourconsensus-based re-ranking (G-Re-Rank)outperforms the baseline significantly, not only forthe development data, but also for the test data.Instead of using graph-based consensusconfidence as features in the log-linear model, weperform structured label propagation (Struct-LP) tore-rank the n-best list directly, and the similaritymeasures for source sentences and translationcandidates are symmetrical sentence level BLEU(equation (10)).
Using Struct-LP, the performanceis significantly improved, compared with thebaseline, but not as well as G-Re-Rank.devset8+dialog devset9Baseline 48.79 44.73Struct-LP 49.86 45.54G-Re-Rank 50.66 46.52G-Re-Rank-GC 50.23 45.96G-Re-Rank-LC 49.87 45.84G-Decode 51.20 47.31G-Decode-GC 50.46 46.21G-Decode-LC 50.11 46.17Table 3.
Consensus-based re-ranking and decodingfor IWSLT data set.
The results in bold type aresignificantly better than the baseline.We use the baseline system to perform forcedalignment procedure on the training data, andcreate span nodes using the derivation tree of theforced alignment.
We also saved the spans of thesentences from development and test data, whichwill be used to create the responding nodes forconsensus-based decoding.
In such a way, wecreate the graph for decoding, and perform semi-308supervised training to calculate graph-basedconsensus features, and tune the weights for all thefeatures we used.
In Table 3, we can see that ourconsensus-based decoding (G-Decode) is muchbetter than baseline, and also better thanconsensus-based re-ranking method.
That isreasonable since the neighbor/local similarityfeatures not only re-rank the final n-best output,but also the spans during decoding.To test the contribution of each kind of features,we first remove all the local consensus featuresand perform consensus-based re-ranking anddecoding (G-Re-Rank-GC and G-Decode-GC),and then we remove all the graph-based consensusfeatures to test the contribution of local consensusfeatures (G-Re-Rank-LC and G-Decode-LC).Without the graph-based consensus features, ourconsensus-based re-ranking and decoding issimplified into a consensus re-ranking andconsensus decoding system, which only re-rankthe candidates according to the consensusinformation of other candidates in the same n-bestlist.From Table 3, we can see, the G-Re-Rank-LCand G-Decode-LC improve the performance ofdevelopment data and test data, but not as much asG-Re-Rank and G-Decode do.
G-Re-Rank-GC andG-Decode-GC improve the performance ofmachine translation according to the baseline.
G-Re-Rank-GC does not achieve the sameperformance as G-Re-Rank-LC does.
Comparedwith G-Decode-LC, the performance with G-Decode-GC is much better.NIST'03 NIST'05 NIST'08Baseline 38.57 38.21 27.52Struct-LP 38.79 38.52 28.06G-Re-Rank 39.21 38.93 28.18G-Re-Rank-GC 38.92 38.76 28.21G-Re-Rank-LC 38.90 38.65 27.88G-Decode 39.62 39.17 28.76G-Decode-GC 39.42 39.02 28.51G-Decode-LC 39.17 38.70 28.20Table 4.
Consensus-based re-ranking and decodingfor NIST data set.
The results in bold type aresignificantly better than the baseline.We also conduct experiments on NIST data, andresults are shown in Table 4.
The consensus-basedre-ranking methods are performed in the same wayas for IWSLT data, but for consensus-baseddecoding, the data set contains too many sentencepairs to be held in one graph for our machine.
Weapply the method of Alexandrescu and Kirchhoff(2009) to construct separate graphs for eachdevelopment and test sentence without losingglobal connectivity information.
We performmodified label propagation with the separategraphs to get the graph-based consensus for n-bestlist of each sentence, and the graph-basedconsensus will be recorded for the MERT to tunethe weights.From Table 4, we can see that, Struct-LPimproves the performance slightly, but notsignificantly.
Local consensus features (G-Re-Rank-LC and G-Decode-LC) improve theperformance slightly.
The combination of graph-based and local consensus features can improvethe translation performance significantly on SMTre-ranking.
With graph-based consensus features,G-Decode-GC achieves significant performancegain, and combined with local consensus features,G-Decode performance is improved farther.7 Conclusion and Future WorkIn this paper, we extend the consensus method bycollecting consensus statistics, not only fromtranslation candidates of the same sourcesentence/span, but also from those of similar ones.To calculate consensus statistics, we develop anovel structured label propagation method forstructured learning problems, such as machinetranslation.
Note that, the structured labelpropagation can be applied to other structuredlearning tasks, such as POS tagging and syntacticparsing.
The consensus statistics are integrated intothe conventional log-linear model as features.
Thefeatures and weights are tuned with an iterativesemi-supervised method.
We conduct experimentson IWSLT and NIST data, and our method canimprove the performance significantly.In this paper, we only tried Dice co-efficient ofn-grams and symmetrical sentence level BLEU assimilarity measures.
In the future, we will exploreother consensus features and other similaritymeasures, which may take document levelinformation, or syntactic and semantic informationinto consideration.
We also plan to introducefeature to model the similarity of the source309sentences, which are reflected by only one score inour paper, and optimize the parameters with CRFmodel.ReferencesAndrei Alexandrescu, Katrin Kirchhoff.
2009.
Graph-based learning for statistical machine translation.
InProceedings of Human Language Technologies andAnnual Conference of the North American Chapterof the ACL, pages 119-127.Peter L. Bertlett, Michael Collins, Ben Taskar andDavid McAllester.
2004.
Exponentiated gradientalgorithms for large-margin structured classification.In Proceedings of Advances in Neural InformationProcessing Systems.John DeNero, David Chiang, and Kevin Knight.
2009.Fast consensus decoding over translation forests.
InProceedings of the Association for ComputationalLinguistics, pages 567-575.John DeNero, Shankar Kumar, Ciprian Chelba andFranz Och.
2010.
Model combination for machinetranslation.
In Proceedings of the North AmericanAssociation for Computational Linguistics, pages975-983.Nan Duan, Mu Li, Dongdong Zhang, and Ming Zhou.2010.
Mixture model-based minimum bayes riskdecoding using multiple machine translation Systems.In Proceedings of the International Conference onComputational Linguistics, pages 313-321.Philipp Koehn.
2004.
Statistical significance tests formachine translation evaluation.
In Proceedings of theConference on Empirical Methods on NaturalLanguage Processing, pages 388-395.Shankar Kumar and William Byrne.
2004.
Minimumbayes-risk decoding for statistical machinetranslation.
In Proceedings of the North AmericanAssociation for Computational Linguistics, pages169-176.Shankar Kumar, Wolfgang Macherey, Chris Dyer, andFranz Och.
2009.
Efficient minimum error ratetraining and minimum bayes-risk decoding fortranslation hypergraphs and lattices.
In Proceedingsof the Association for Computational Linguistics,pages 163-171.Mu Li, Nan Duan, Dongdong Zhang, Chi-Ho Li, andMing Zhou.
2009.
Collaborative decoding: partialhypothesis re-ranking using translation consensusbetween decoders.
In Proceedings of the Associationfor Computational Linguistics, pages 585-592.Percy Liang, Alexandre Bouchard-Cote, Dan Klein, andBen Taskar.
2006.
An end-to-end discriminativeapproach to machine translation.
In Proceedings ofthe International Conference on ComputationalLinguistics and the ACL, pages 761-768Yanjun Ma, Yifan He, Andy Way, Josef van Genabith.2011.
Consistent translation using discriminativelearning: a translation memory-inspired approach.
InProceedings of the Association for ComputationalLinguistics, pages 1239-1248.Franz Josef Och.
2003.
Minimum error rate training instatistical machine translation.
In Proceedings of theAssociation for Computational Linguistics, pages160-167.Kishore Papineni, Salim Roukos, Todd Ward and Wei-jing Zhu.
2002.
BLEU: a method for automaticevaluation of machine translation.
In Proceedings ofthe Association for Computational Linguistics, pages311-318.Roy Tromble, Shankar Kumar, Franz Och, andWolfgang Macherey.
2008.
Lattice minimum bayes-risk decoding for statistical machine translation.
InProceedings of the Conference on EmpiricalMethods on Natural Language Processing, pages620-629.Dekai Wu.
1997.
Stochastic inversion transductiongrammars and bilingual parsing of parallel corpora.Computational Linguistics, 23(3).Joern Wuebker, Arne Mauser and Hermann Ney.
2010.Training phrase translation models with leaving-one-out.
In Proceedings of the Association forComputational Linguistics, pages 475-484.Deyi Xiong, Qun Liu and Shouxun Lin.
2006.Maximum entropy based phrase reordering model forstatistical machine translation.
In Proceedings of theAssociation for Computational Linguistics, pages521-528.Xiaojin Zhu.
2005.
Semi-supervised learning withgraphs.
Ph.D. thesis, Carnegie Mellon University.CMU-LTI-05-192.310
