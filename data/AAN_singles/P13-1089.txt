Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 905?913,Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational LinguisticsMargin-based Decomposed Amortized InferenceGourab Kundu?
and Vivek Srikumar?
and Dan RothUniversity of Illinois, Urbana-ChampaignUrbana, IL.
61801{kundu2, vsrikum2, danr}@illinois.eduAbstractGiven that structured output prediction istypically performed over entire datasets,one natural question is whether it is pos-sible to re-use computation from earlierinference instances to speed up inferencefor future instances.
Amortized inferencehas been proposed as a way to accomplishthis.
In this paper, first, we introduce a newamortized inference algorithm called theMargin-based Amortized Inference, whichuses the notion of structured margin toidentify inference problems for which pre-vious solutions are provably optimal.
Sec-ond, we introduce decomposed amortizedinference, which is designed to addressvery large inference problems, where ear-lier amortization methods become less ef-fective.
This approach works by decom-posing the output structure and applyingamortization piece-wise, thus increasingthe chance that we can re-use previous so-lutions for parts of the output structure.These parts are then combined to a globalcoherent solution using Lagrangian relax-ation.
In our experiments, using the NLPtasks of semantic role labeling and entity-relation extraction, we demonstrate thatwith the margin-based algorithm, we needto call the inference engine only for a thirdof the test examples.
Further, we show thatthe decomposed variant of margin-basedamortized inference achieves a greater re-duction in the number of inference calls.1 IntroductionA wide variety of NLP problems can be natu-rally cast as structured prediction problems.
For* These authors contributed equally to this work.some structures like sequences or parse trees, spe-cialized and tractable dynamic programming algo-rithms have proven to be very effective.
However,as the structures under consideration become in-creasingly complex, the computational problem ofpredicting structures can become very expensive,and in the worst case, intractable.In this paper, we focus on an inference tech-nique called amortized inference (Srikumar et al,2012), where previous solutions to inference prob-lems are used to speed up new instances.
Themain observation that leads to amortized inferenceis that, very often, for different examples of thesame size, the structures that maximize the scoreare identical.
If we can efficiently identify that twoinference problems have the same solution, thenwe can re-use previously computed structures fornewer examples, thus giving us a speedup.This paper has two contributions.
First, we de-scribe a novel algorithm for amortized inferencecalled margin-based amortization.
This algorithmis on an examination of the structured margin ofa prediction.
For a new inference problem, if thismargin is larger than the sum of the decrease in thescore of the previous prediction and any increasein the score of the second best one, then the previ-ous solution will be the highest scoring one for thenew problem.
We formalize this intuition to derivean algorithm that finds provably optimal solutionsand show that this approach is a generalization ofpreviously identified schemes (based on Theorem1 of (Srikumar et al, 2012)).Second, we argue that the idea of amortizationis best exploited at the level of parts of the struc-tures rather than the entire structure because weexpect a much higher redundancy in the parts.We introduce the notion of decomposed amor-tized inference, whereby we can attain a significantimprovement in speedup by considering repeatedsub-structures across the dataset and applying anyamortized inference algorithm for the parts.905We evaluate the two schemes and their combi-nation on two NLP tasks where the output is en-coded as a structure: PropBank semantic role la-beling (Punyakanok et al, 2008) and the problemof recognizing entities and relations in text (Rothand Yih, 2007; Kate and Mooney, 2010).
In theseproblems, the inference problem has been framedas an integer linear program (ILP).
We compareour methods with previous amortized inferencemethods and show that margin-based amortizationcombined with decomposition significantly out-performs existing methods.2 Problem Definition and NotationStructured output prediction encompasses a widevariety of NLP problems like part-of-speech tag-ging, parsing and machine translation.
The lan-guage of 0-1 integer linear programs (ILP) pro-vides a convenient analytical tool for representingstructured prediction problems.
The general set-ting consists of binary inference variables each ofwhich is associated with a score.
The goal of in-ference is to find the highest scoring global assign-ment of the variables from a feasible set of assign-ments, which is defined by linear inequalities.While efficient inference algorithms exist forspecial families of structures (like linear chainsand trees), in the general case, inference can becomputationally intractable.
One approach to dealwith the computational complexity of inferenceis to use an off-the-shelf ILP solver for solv-ing the inference problem.
This approach hasseen increasing use in the NLP community overthe last several years (for example, (Roth andYih, 2004; Clarke and Lapata, 2006; Riedel andClarke, 2006) and many others).
Other approachesfor solving inference include the use of cuttingplane inference (Riedel, 2009), dual decomposi-tion (Koo et al, 2010; Rush et al, 2010) andthe related method of Lagrangian relaxation (Rushand Collins, 2011; Chang and Collins, 2011).
(Srikumar et al, 2012) introduced the notion ofan amortized inference algorithm, defined as aninference algorithm that can use previous predic-tions to speed up inference time, thereby giving anamortized gain in inference time over the lifetimeof the program.The motivation for amortized inference comesfrom the observation that though the number ofpossible structures could be large, in practice, onlya small number of these are ever seen in realdata.
Furthermore, among the observed structures,a small subset typically occurs much more fre-quently than the others.
Figure 1 illustrates thisobservation in the context of part-of-speech tag-ging.
If we can efficiently characterize and iden-tify inference instances that have the same solu-tion, we can take advantage of previously per-formed computation without paying the high com-putational cost of inference.Figure 1: Comparison of number of instances and the num-ber of unique observed part-of-speech structures in the Gi-gaword corpus.
Note that the number of observed structures(blue solid line) is much lower than the number of sentences(red dotted line) for all sentence lengths, with the differencebeing very pronounced for shorter sentences.
Embedded inthe graph are three histograms that show the distribution ofobserved structures for sentences of length 15, 20 and 30.
Inall cases, we see that a small number of tag sequences aremuch more frequent than the others.We denote inference problems by the bold-faced letters p and q.
For a problem p, the goalof inference is to jointly assign values to the partsof the structure, which are represented by a col-lection of inference variables y ?
{0, 1}n. For allvectors, subscripts represent their ith component.Each yi is associated with a real valued cp,i ?
<which is the score for the variable yi being as-signed the value 1.
We denote the vector com-prising of all the cp,i as cp.
The search spacefor assignments is restricted via constraints, whichcan be written as a collection of linear inequalities,MTy ?
b.
For a problem p, we denote this fea-sible set of structures by Kp.The inference problem is that of finding the fea-sible assignment to the structure which maximizesthe dot product cTy.
Thus, the prediction problemcan be written asarg maxy?KpcTy.
(1)906We denote the solution of this maximization prob-lem as yp.Let the set P = {p1,p2, ?
?
? }
denote previouslysolved inference problems, along with their re-spective solutions {y1p,y2p, ?
?
?
}.
An equivalenceclass of integer linear programs, denoted by [P ],consists of ILPs which have the same number ofinference variables and the same feasible set.
LetK[P ] denote the feasible set of an equivalence class[P ].
For a program p, the notation p ?
[P ] indi-cates that it belongs to the equivalence class [P ].
(Srikumar et al, 2012) introduced a set of amor-tized inference schemes, each of which provides acondition for a new ILP to have the same solu-tion as a previously seen problem.
We will brieflyreview one exact inference scheme introduced inthat work.
Suppose q belongs to the same equiv-alence class of ILPs as p. Then the solution to qwill be the same as that of p if the following con-dition holds for all inference variables:(2yp,i ?
1)(cq,i ?
cp,i) ?
0.
(2)This condition, referred to as Theorem 1 in thatwork, is the baseline for our experiments.In general, for any amortization schemeA, we can define two primitive operatorsTESTCONDITIONA and SOLUTIONA.
Givena collection of previously solved prob-lems P and a new inference problem q,TESTCONDITIONA(P,q) checks if the solu-tion of the new problem is the same as thatof some previously solved one and if so,SOLUTIONA(P,q) returns the solution.3 Margin-based AmortizationIn this section, we will introduce a new methodfor amortizing inference costs over time.
The keyobservation that leads to this theorem stems fromthe structured margin ?
for an inference problemp ?
[P ], which is defined as follows:?
= miny?K[P ],y 6=ypcTp(yp ?
y).
(3)That is, for all feasible y, we have cTpyp ?
cTpy+?.
The margin ?
is the upper limit on the change inobjective that is allowed for the constraint setK[P ]for which the solution will not change.For a new inference problem q ?
[P ], we define?
as the maximum change in objective value thatcan be effected by an assignment that is not theA B = ypcpcq?
?decrease invalue of ypincreasingobjectivecpTypTwo assignmentsFigure 2: An illustration of the margin-based amortizationscheme showing the very simple case with only two compet-ing assignments A and B.
Suppose B is the solution yp forthe inference problem p with coefficients cp, denoted by thered hyperplane, and A is the second-best assignment.
For anew coefficient vector cq, if the margin ?
is greater than thesum of the decrease in the objective value of yp and the max-imum increase in the objective of another solution (?
), thenthe solution to the new inference problem will still be yp.
Themargin-based amortization theorem captures this intuition.solution.
That is,?
= maxy?K[P ],y 6=yp(cq ?
cp)T y (4)Before stating the theorem, we will provide an in-tuitive explanation for it.
Moving from cp to cq,consider the sum of the decrease in the value ofthe objective for the solution yp and ?, the maxi-mum change in objective value for an assignmentthat is not the solution.
If this sum is less than themargin ?, then no other solution will have an ob-jective value higher than yp.
Figure 2 illustratesthis using a simple example where there are onlytwo competing solutions.This intuition is captured by our main theoremwhich provides a condition for problems p and qto have the same solution yp.Theorem 1 (Margin-based Amortization).
Let pdenote an inference problem posed as an inte-ger linear program belonging to an equivalenceclass [P ] with optimal solution yp.
Let p havea structured margin ?, i.e., for any y, we havecTpyp ?
cTpy + ?.
Let q ?
[P ] be another infer-ence instance in the same equivalence class andlet ?
be defined as in Equation 4.
Then, yp is thesolution of the problem q if the following holds:?
(cq ?
cp)Typ + ?
?
?
(5)907Proof.
For some feasible y, we havecTqyp ?
cTqy ?
cTqyp ?
cTpy ???
cTqyp ?
cTpyp + ?
???
0The first inequality comes from the definition of ?in (4) and the second one follows from the defini-tion of ?.
The condition of the theorem in (5) givesus the final step.
For any feasible y, the objectivescore assigned to yp is greater than the score as-signed to y according to problem q.
That is, yp isthe solution to the new problem.The margin-based amortization theorem pro-vides a general, new amortized inference algo-rithm.
Given a new inference problem, we checkwhether the inequality (5) holds for any previouslyseen problems in the same equivalence class.
If so,we return the cached solution.
If no such problemexists, then we make a call to an ILP solver.Even though the theorem provides a conditionfor two integer linear programs to have the samesolution, checking the validity of the condition re-quires the computation of ?, which in itself is an-other integer linear program.
To get around this,we observe that if any constraints in Equation 4are relaxed, the value of the resulting maximumcan only increase.
Even with the increased ?, ifthe condition of the theorem holds, then the restof the proof follows and hence the new problemwill have the same solution.
In other words, wecan solve relaxed, tractable variants of the maxi-mization in Equation 4 and still retain the guaran-tees provided by the theorem.
The tradeoff is that,by doing so, the condition of the theorem will ap-ply to fewer examples than theoretically possible.In our experiments, we will define the relaxationfor each problem individually and even with therelaxations, the inference algorithm based on themargin-based amortization theorem outperformsall previous amortized inference algorithms.The condition in inequality (5) is, in fact, a strictgeneralization of the condition for Theorem 1 in(Srikumar et al, 2012), stated in (2).
If the lattercondition holds, then we can show that ?
?
0 and(cq ?
cp)Typ ?
0.
Since ?
is, by definition, non-negative, the margin-based condition is satisfied.4 Decomposed Amortized InferenceOne limitation in previously considered ap-proaches for amortized inference stems from theexpectation that the same full assignment maxi-mizes the objective score for different inferenceproblems, or equivalently, that the entire structureis repeated multiple times.
Even with this assump-tion, we observe a speedup in prediction.However, intuitively, even if entire structuresare not repeated, we expect parts of the assign-ment to be the same across different instances.
Inthis section, we address the following question:Can we take advantage of the redundancy in com-ponents of structures to extend amortization tech-niques to cases where the full structured output isnot repeated?
By doing so, we can store partialcomputation for future inference problems.For example, consider the task of part of speechtagging.
While the likelihood of two long sen-tences having the same part of speech tag sequenceis not high, it is much more likely that shorter sec-tions of the sentences will share the same tag se-quence.
We see from Figure 1 that the number ofpossible structures for shorter sentences is muchsmaller than the number of sentences.
This im-plies that many shorter sentences share the samestructure, thus improving the performance of anamortized inference scheme for such inputs.
Thegoal of decomposed amortized inference is to ex-tend this improvement to larger problems by in-creasing the size of equivalence classes.To decompose an inference problem, we use theapproach of Lagrangian Relaxation (Lemare?chal,2001) that has been used successfully for variousNLP tasks (Chang and Collins, 2011; Rush andCollins, 2011).
We will briefly review the under-lying idea1.
The goal is to solve an integer linearprogram q, which is defined asq : maxMTy?bcTqyWe partition the constraints into two sets, say C1denoting M1Ty ?
b1 and C2, denoting con-straints M2Ty ?
b2.
The assumption is that inthe absence the constraints C2, the inference prob-lem becomes computationally easier to solve.
Inother words, we can assume the existence of a sub-routine that can efficiently compute the solution ofthe relaxed problem q?:q?
: maxM1Ty?b1cTqy1For simplicity, we only write inequality constraints inthe paper.
However, all the results here are easily extensibleto equality constraints by removing the non-negativity con-straints from the corresponding dual variables.908We define Lagrange multipliers ?
?
0, with one?i for each constraint in C2.
For problem q, wecan define the Lagrangian asL(y,?)
= cTqy ?
?T(M2Ty ?
b2)Here, the domain of y is specified by the constraintset C1.
The dual objective isL(?)
= maxM1Ty?b1cTqy ?
?T(M2Ty ?
b2)= maxM1Ty?b1(cq ?
?TM2)T y + ?Tb2.Note that the maximization in the definition of thedual objective has the same functional form as q?and any approach to solve q?
can be used here tofind the dual objective L(?).
The dual of the prob-lem q, given by min?
?0 L(?
), can be solved us-ing subgradient descent over the dual variables.Relaxing the constraints C2 to define the prob-lem q?
has several effects.
First, it can make the re-sulting inference problem q?
easier to solve.
Moreimportantly, removing constraints can also lead tothe merging of multiple equivalence classes, lead-ing to fewer, more populous equivalence classes.Finally, removing constraints can decompose theinference problem q?
into smaller independentsub-problems {q1,q2, ?
?
? }
such that no constraintthat is inC1 has active variables from two differentsets in the partition.For the sub-problem qi comprising of variablesyi, let the corresponding objective coefficients becqi and the corresponding sub-matrix of M2 beMi2.
Now, we can define the dual-augmented sub-problem asmaxMi1Ty?bi1(cqi ?
?TMi2)Tyi (6)Solving all such sub-problems will give us a com-plete assignment for all the output variables.We can now define the decomposed amortizedinference algorithm (Algorithm 1) that performssub-gradient descent over the dual variables.
Theinput to the algorithm is a collection of previ-ously solved problems with their solutions, a newinference problem q and an amortized inferencescheme A (such as the margin-based amortizationscheme).
In addition, for the task at hand, we firstneed to identify the set of constraints C2 that canbe introduced via the Lagrangian.First, we check if the solution can be obtainedwithout decomposition (lines 1?2).
Otherwise,Algorithm 1 Decomposed Amortized InferenceInput: A collection of previously solved infer-ence problems P , a new problem q, an amor-tized inference algorithm A.Output: The solution to problem q1: if TESTCONDITION(A, q, P ) then2: return SOLUTION(A, q, P )3: else4: Initialize ?i ?
0 for each constraint in C2.5: for t = 1 ?
?
?T do6: Partition the problem q into sub-problems q1,q2, ?
?
?
such that no con-straint in C1 has active variables fromtwo partitions.7: for partition qi do8: yi ?
Solve the maximization prob-lem for qi (Eq.
6) using the amortizedscheme A.9: end for10: Let y?
[y1;y2; ?
?
?
]11: if M2y ?
b2 and (b2 ?M2y)i?i = 0then12: return y13: else14: ??[??
?t(b2 ?M2Ty)]+15: end if16: end for17: return solution of q using a standard infer-ence algorithm18: end ifwe initialize the dual variables ?
and try to ob-tain the solution iteratively.
At the tth itera-tion, we partition the problem q into sub-problems{q1,q2, ?
?
? }
as described earlier (line 6).
Eachpartition defines a smaller inference problem withits own objective coefficients and constraints.
Wecan apply the amortization scheme A to each sub-problem to obtain a complete solution for the re-laxed problem (lines 7?10).
If this solution satis-fies the constraints C2 and complementary slack-ness conditions, then the solution is provably themaximum of the problem q.
Otherwise, we take asubgradient step to update the value of ?
using astep-size ?t, subject to the constraint that all dualvariables must be non-negative (line 14).
If we donot converge to a solution in T iterations, we callthe underlying solver on the full problem.In line 8 of the algorithm, we make multiplecalls to the underlying amortized inference pro-cedure to solve each sub-problem.
If the sub-909problem cannot be solved using the procedure,then we can either solve the sub-problem using adifferent approach (effectively giving us the stan-dard Lagrangian relaxation algorithm for infer-ence), or we can treat the full instance as a cachemiss and make a call to an ILP solver.
In our ex-periments, we choose the latter strategy.5 Experiments and ResultsOur experiments show two results: 1.
The margin-based scheme outperforms the amortized infer-ence approaches from (Srikumar et al, 2012).2.
Decomposed amortized inference gives furthergains in terms of re-using previous solutions.5.1 TasksWe report the performance of inference on twoNLP tasks: semantic role labeling and the task ofextracting entities and relations from text.
In bothcases, we used an existing formulation for struc-tured inference and only modified the inferencecalls.
We will briefly describe the problems andthe implementation and point the reader to the lit-erature for further details.Semantic Role Labeling (SRL) Our first task isthat of identifying arguments of verbs in a sen-tence and annotating them with semantic roles(Gildea and Jurafsky, 2002; Palmer et al, 2010).
For example, in the sentence Mrs. Haag playsEltiani., the verb plays takes two arguments: Mrs.Haag, the actor, labeled as A0 and Eltiani, therole, labeled as A1.
It has been shown in priorwork (Punyakanok et al, 2008; Toutanova et al,2008) that making a globally coherent predictionboosts performance of SRL.In this work, we used the SRL system of (Pun-yakanok et al, 2008), where one inference prob-lem is generated for each verb and each infer-ence variables encodes the decision that a givenconstituent in the sentence takes a specific role.The scores for the inference variables are obtainedfrom a classifier trained on the PropBank cor-pus.
Constraints encode structural and linguisticknowledge about the problem.
For details aboutthe formulations of the inference problem, pleasesee (Punyakanok et al, 2008).Recall from Section 3 that we need to define arelaxed version of the inference problem to effi-ciently compute ?
for the margin-based approach.For a problem instance with coefficients cq andcached coefficients cp, we take the sum of thehighest n values of cq ?
cp as our ?, where n isthe number of argument candidates to be labeled.To identify constraints that can be relaxed forthe decomposed algorithm, we observe that mostconstraints are not predicate specific and apply forall predicates.
The only constraint that is predi-cate specific requires that each predicate can onlyaccept roles from a list of roles that is defined forthat predicate.
By relaxing this constraint in thedecomposed algorithm, we effectively merge allthe equivalence classes for all predicates with aspecific number of argument candidates.Entity-Relation extraction Our second task isthat of identifying the types of entities in a sen-tence and the relations among them, which hasbeen studied by (Roth and Yih, 2007; Kate andMooney, 2010) and others.
For the sentenceOswald killed Kennedy, the words Oswald andKennedy will be labeled by the type PERSON, andthe KILL relation exists between them.We followed the experimental setup as de-scribed in (Roth and Yih, 2007).
We defined oneinference problem for each sentence.
For everyentity (which is identified by a constituent in thesentence), an inference variable is introduced foreach entity type.
For each pair of constituents, aninference variable is introduced for each relationtype.
Clearly, the assignment of types to entitiesand relations are not independent.
For example, anentity of type ORGANIZATION cannot participatein a relation of type BORN-IN because this rela-tion label can connect entities of type PERSON andLOCATION only.
Incorporating these natural con-straints during inference were shown to improveperformance significantly in (Roth and Yih, 2007).We trained independent classifiers for entities andrelations and framed the inference problem as in(Roth and Yih, 2007).
For further details, we referthe reader to that paper.To compute the value of ?
for the margin-basedalgorithm, for a new instance with coefficients cqand cached coefficients cp, we define ?
to be thesum of all non-negative values of cq ?
cp.For the decomposed inference algorithm, if thenumber of entities is less than 5, no decomposi-tion is performed.
Otherwise, the entities are par-titioned into two sets: set A includes the first fourentities and set B includes the rest of the entities.We relaxed the relation constraints that go acrossthese two sets of entities to obtain two independentinference problems.9105.2 Experimental SetupWe follow the experimental setup of (Srikumar etal., 2012) and simulate a long-running NLP pro-cess by caching problems and solutions from theGigaword corpus.
We used a database engine tocache ILP and their solutions along with identi-fiers for the equivalence class and the value of ?.For the margin-based algorithm and the Theo-rem 1 from (Srikumar et al, 2012), for a new in-ference problem p ?
[P ], we retrieve all infer-ence problems from the database that belong tothe same equivalence class [P ] as the test prob-lem p and find the cached assignment y that hasthe highest score according to the coefficients ofp.
We only consider cached ILPs whose solutionis y for checking the conditions of the theorem.This optimization ensures that we only process asmall number of cached coefficient vectors.In a second efficiency optimization, we prunedthe database to remove redundant inference prob-lems.
A problem is redundant if solution to thatproblem can be inferred from the other problemsstored in the database that have the same solutionand belong to the same equivalence class.
How-ever, this pruning can be computationally expen-sive if the number of problems with the same so-lution and the same equivalence class is very large.In that case, we first sampled a 5000 problems ran-domly and selected the non-redundant problemsfrom this set to keep in the database.5.3 ResultsWe compare our approach to a state-of-the-art ILPsolver2 and also to Theorem 1 from (Srikumaret al, 2012).
We choose this baseline becauseit is shown to give the highest improvement inwall-clock time and also in terms of the num-ber of cache hits.
However, we note that the re-sults presented in our work outperform all the pre-vious amortization algorithms, including the ap-proximate inference methods.We report two performance metrics ?
the per-centage decrease in the number of ILP calls, andthe percentage decrease in the wall-clock infer-ence time.
These are comparable to the speedupand clock speedup defined in (Srikumar et al,2012).
For measuring time, since other aspectsof prediction (like feature extraction) are the sameacross all settings, we only measure the time takenfor inference and ignore other aspects.
For both2We used the Gurobi optimizer for our experiments.tasks, we report the runtime performance on sec-tion 23 of the Penn Treebank.
Note that our amor-tization schemes guarantee optimal solution.
Con-sequently, using amortization, task accuracy re-mains the same as using the original solver.Table 1 shows the percentage reduction in thenumber of calls to the ILP solver.
Note that forboth the SRL and entity-relation problems, themargin-based approach, even without using de-composition (the columns labeled Original), out-performs the previous work.
Applying the de-composed inference algorithm improves both thebaseline and the margin-based approach.
Overall,however, the fewest number of calls to the solver ismade when combining the decomposed inferencealgorithm with the margin-based scheme.
For thesemantic role labeling task, we need to call thesolver only for one in six examples while for theentity-relations task, only one in four examples re-quire a solver call.Table 2 shows the corresponding reduction inthe wall-clock time for the various settings.
Wesee that once again, the margin based approachoutperforms the baseline.
While the decomposedinference algorithm improves running time forSRL, it leads to a slight increase for the entity-relation problem.
Since this increase occurs inspite of a reduction in the number of solver calls,we believe that this aspect can be further improvedwith an efficient implementation of the decom-posed inference algorithm.6 DiscussionLagrangian Relaxation in the literature In theliterature, in applications of the Lagrangian relax-ation technique (such as (Rush and Collins, 2011;Chang and Collins, 2011; Reichart and Barzilay,2012) and others), the relaxed problems are solvedusing specialized algorithms.
However, in both therelaxations considered in this paper, even the re-laxed problems cannot be solved without an ILPsolver, and yet we can see improvements from de-composition in Table 1.To study the impact of amortization on runningtime, we modified our decomposition based infer-ence algorithm to solve each sub-problem usingthe ILP solver instead of amortization.
In these ex-periments, we ran Lagrangian relaxation for untilconvergence or at most T iterations.
After T itera-tions, we call the ILP solver and solve the originalproblem.
We set T to 100 in one set of exper-911% ILP Solver calls requiredMethod Semantic Role Labeling Entity-Relation ExtractionOriginal + Decomp.
Original + Decomp.ILP Solver 100 ?
100 ?
(Srikumar et al, 2012) 41 24.4 59.5 57.0Margin-based 32.7 16.6 28.2 25.4Table 1: Reduction in number of inference calls% time required compared to ILP SolverMethod Semantic Role Labeling Entity-Relation ExtractionOriginal + Decomp.
Original + Decomp.ILP Solver 100 ?
100 ?
(Srikumar et al, 2012) 54.8 40.0 81 86Margin-based 45.9 38.1 58.1 61.3Table 2: Reduction in inference timeiments (call it Lag1) and T to 1 (call it Lag2).In SRL, compared to solving the original problemwith ILP Solver, both Lag1 and Lag2 are roughly2 times slower.
For entity relation task, comparedto ILP Solver, Lag1 is 186 times slower and Lag2is 1.91 times slower.
Since we used the same im-plementation of the decomposition in all experi-ments, this shows that the decomposed inferencealgorithm crucially benefits from the underlyingamortization scheme.Decomposed amortized inference The decom-posed amortized inference algorithm helps im-prove amortized inference in two ways.
First,since the number of structures is a function of itssize, considering smaller sub-structures will allowus to cache inference problems that cover a largersubset of the space of possible sub-structures.
Weobserved this effect in the problem of extractingentities and relations in text.
Second, removing aconstraint need not always partition the structureinto a set of smaller structures.
Instead, by re-moving the constraint, examples that might haveotherwise been in different equivalence classes be-come part of a combined, larger equivalence class.Increasing the size of the equivalence classes in-creases the probability of a cache-hit.
In our ex-periments, we observed this effect in the SRL task.7 ConclusionAmortized inference takes advantage of the reg-ularities in structured output to re-use previouscomputation and improve running time over thelifetime of a structured output predictor.
In this pa-per, we have described two approaches for amor-tizing inference costs over datasets.
The first,called the margin-based amortized inference, is anew, provably exact inference algorithm that usesthe notion of a structured margin to identify previ-ously solved problems whose solutions can be re-used.
The second, called decomposed amortizedinference, is a meta-algorithm over any amortizedinference that takes advantage of previously com-puted sub-structures to provide further reductionsin the number of inference calls.
We show via ex-periments that these methods individually give areduction in the number of calls made to an infer-ence engine for semantic role labeling and entity-relation extraction.
Furthermore, these approachescomplement each other and, together give an addi-tional significant improvement.AcknowledgmentsThe authors thank the members of the Cognitive Computa-tion Group at the University of Illinois for insightful discus-sions and the anonymous reviewers for valuable feedback.This research is sponsored by the Army Research Laboratory(ARL) under agreement W911NF-09-2-0053.
The authorsalso gratefully acknowledge the support of the Defense Ad-vanced Research Projects Agency (DARPA) Machine Read-ing Program under Air Force Research Laboratory (AFRL)prime contract no.
FA8750-09-C-0181.
This material alsois based on research sponsored by DARPA under agreementnumber FA8750-13-2-0008.
This work has also been sup-ported by the Intelligence Advanced Research Projects Ac-tivity (IARPA) via Department of Interior National BusinessCenter contract number D11PC20155.
The U.S. Govern-ment is authorized to reproduce and distribute reprints forGovernmental purposes notwithstanding any copyright an-notation thereon.
Any opinions, findings, and conclusionsor recommendations expressed in this material are those ofthe author(s) and do not necessarily reflect the view of ARL,DARPA, AFRL, IARPA, DoI/NBC or the US government.912ReferencesY-W. Chang and M. Collins.
2011.
Exact decoding ofphrase-based translation models through Lagrangianrelaxation.
EMNLP.J.
Clarke and M. Lapata.
2006.
Constraint-basedsentence compression: An integer programming ap-proach.
In ACL.D.
Gildea and D. Jurafsky.
2002.
Automatic labelingof semantic roles.
Computational Linguistics.R.
Kate and R. Mooney.
2010.
Joint entity and relationextraction using card-pyramid parsing.
In Proceed-ings of the Fourteenth Conference on ComputationalNatural Language Learning, pages 203?212.
Asso-ciation for Computational Linguistics.T.
Koo, A. M. Rush, M. Collins, T. Jaakkola, andD.
Sontag.
2010.
Dual decomposition for parsingwith non-projective head automata.
In EMNLP.C.
Lemare?chal.
2001.
Lagrangian Relaxation.
InComputational Combinatorial Optimization, pages112?156.M.
Palmer, D. Gildea, and N. Xue.
2010.
SemanticRole Labeling, volume 3.
Morgan & Claypool Pub-lishers.V.
Punyakanok, D. Roth, and W. Yih.
2008.
The im-portance of syntactic parsing and inference in se-mantic role labeling.
Computational Linguistics.R.
Reichart and R. Barzilay.
2012.
Multi event extrac-tion guided by global constraints.
In NAACL, pages70?79.S.
Riedel and J. Clarke.
2006.
Incremental integerlinear programming for non-projective dependencyparsing.
In EMNLP.S.
Riedel.
2009.
Cutting plane MAP inference forMarkov logic.
Machine Learning.D.
Roth and W. Yih.
2004.
A linear programmingformulation for global inference in natural languagetasks.
In Hwee Tou Ng and Ellen Riloff, editors,CoNLL.D.
Roth and W. Yih.
2007.
Global inference for entityand relation identification via a linear programmingformulation.
In Lise Getoor and Ben Taskar, editors,Introduction to Statistical Relational Learning.A.M.
Rush and M. Collins.
2011.
Exact decoding ofsyntactic translation models through Lagrangian re-laxation.
In ACL, pages 72?82, Portland, Oregon,USA, June.A.
M. Rush, D. Sontag, M. Collins, and T. Jaakkola.2010.
On dual decomposition and linear program-ming relaxations for natural language processing.
InEMNLP.V.
Srikumar, G. Kundu, and D. Roth.
2012.
On amor-tizing inference cost for structured prediction.
InEMNLP.K.
Toutanova, A. Haghighi, and C. D. Manning.
2008.A global joint model for semantic role labeling.Computational Linguistics, 34:161?191.913
