Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 933?943,MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational LinguisticsAssessing Phrase-Based Translation Models with Oracle DecodingGuillaume Wisniewski and Alexandre Allauzen and Fran?cois YvonUniv.
Paris Sud ; LIMSI?CNRS91403 ORSAY CEDEXFrance{wisniews,allauzen,yvon}@limsi.frAbstractExtant Statistical Machine Translation (SMT) sys-tems are very complex softwares, which embed mul-tiple layers of heuristics and embark very large num-bers of numerical parameters.
As a result, it is diffi-cult to analyze output translations and there is a realneed for tools that could help developers to betterunderstand the various causes of errors.In this study, we make a step in that direction andpresent an attempt to evaluate the quality of thephrase-based translation model.
In order to identifythose translation errors that stem from deficienciesin the phrase table (PT), we propose to compute theoracle BLEU-4 score, that is the best score that asystem based on this PT can achieve on a referencecorpus.
By casting the computation of the oracleBLEU-1 as an Integer Linear Programming (ILP)problem, we show that it is possible to efficientlycompute accurate lower-bounds of this score, and re-port measures performed on several standard bench-marks.
Various other applications of these oracle de-coding techniques are also reported and discussed.1 Phrase-Based Machine Translation1.1 PrincipleA Phrase-Based Translation System (PBTS) consists of aruleset and a scoring function (Lopez, 2009).
The ruleset,represented in the phrase table, is a set of phrase1pairs{(f, e)}, each pair expressing that the source phrase fcan be rewritten (translated) into a target phrase e. Trans-lation hypotheses are generated by iteratively rewritingportions of the source sentence as prescribed by the rule-set, until each source word has been consumed by exactlyone rule.
The order of target words in an hypothesis isuniquely determined by the order in which the rewrite op-eration are performed.
The search space of the translationmodel corresponds to the set of all possible sequences of1Following the usage in statistical machine translation literature, weuse ?phrase?
to denote a subsequence of consecutive words.rules applications.
The scoring function aims to rank allpossible translation hypotheses in such a way that the bestone has the highest score.A PBTS is learned from a parallel corpus in two inde-pendent steps.
In a first step, the corpus is aligned at theword level, by using alignment tools such as Giza++(Och and Ney, 2003) and some symmetrisation heuris-tics; phrases are then extracted by other heuristics (Koehnet al, 2003) and assigned numerical weights.
In thesecond step, the parameters of the scoring function areestimated, typically through Minimum Error Rate train-ing (Och, 2003).Translating a sentence amounts to finding the best scor-ing translation hypothesis in the search space.
Becauseof the combinatorial nature of this problem, translationhas to rely on heuristic search techniques such as greedyhill-climbing (Germann, 2003) or variants of best-firstsearch like multi-stack decoding (Koehn, 2004).
More-over, to reduce the overall complexity of decoding, thesearch space is typically pruned using simple heuristics.For instance, the state-of-the-art phrase-based decoderMoses (Koehn et al, 2007) considers only a restrictednumber of translations for each source sequence2 and en-forces a distortion limit3 over which phrases can be re-ordered.
As a consequence, the best translation hypothe-sis returned by the decoder is not always the one with thehighest score.1.2 Typology of PBTS ErrorsAnalyzing the errors of a SMT system is not an easy task,because of the number of models that are combined, thesize of these models, and the high complexity of the vari-ous decision making processes.
For a SMT system, threedifferent kinds of errors can be distinguished (Germannet al, 2004; Auli et al, 2009): search errors, inductionerrors and model errors.
The former corresponds to caseswhere the hypothesis with the best score is missed bythe search procedure, either because of the use of an ap-2the ttl option of Moses, defaulting to 20.3the dl option of Moses, whose default value is 7.933proximate search method or because of the restrictions ofthe search space.
Induction errors correspond to caseswhere, given the model, the search space does not containthe reference.
Finally, model errors correspond to caseswhere the hypothesis with the highest score is not the besttranslation according to the evaluation metric.Model errors encompass several types of errors that oc-cur during learning (Bottou and Bousquet, 2008)4.
Ap-proximation errors are errors caused by the use of a re-stricted and oversimplistic class of functions (here, finite-state transducers to model the generation of hypothesesand a linear scoring function to discriminate them) tomodel the translation process.
Estimation errors corre-spond to the use of sub-optimal values for both the phrasepairs weights and the parameters of the scoring function.The reasons behind these errors are twofold: first, train-ing only considers a finite sample of data; second, it re-lies on error prone alignments.
As a result, some ?good?phrases are extracted with a small weight, or, in the limit,are not extracted at all; and conversely that some ?poor?phrases are inserted into the phrase table, sometimes witha really optimistic score.Sorting out and assessing the impact of these variouscauses of errors is of primary interest for SMT systemdevelopers: for lack of such diagnoses, it is difficult tofigure out which components of the system require themost urgent attention.
Diagnoses are however, given thetight intertwining among the various component of a sys-tem, very difficult to obtain: most evaluations are limitedto the computation of global scores and usually do notimply any kind of failure analysis.1.3 Contribution and organizationTo systematically assess the impact of the multipleheuristic decisions made during training and decoding,we propose, following (Dreyer et al, 2007; Auli et al,2009), to work out oracle scores, that is to evaluate thebest achievable performances of a PBTS.
We aim at bothstudying the expressive power of PBTS and at providingtools for identifying and quantifying causes of failure.Under standard metrics such as BLEU (Papineni et al,2002), oracle scores are difficult (if not impossible) tocompute, but, by casting the computation of the oracleunigram recall and precision as an Integer Linear Pro-gramming (ILP) problem, we show that it is possible toefficiently compute accurate lower-bounds of the oracleBLEU-4 scores and report measurements performed onseveral standard benchmarks.The main contributions of this paper are twofold.
Wefirst introduce an ILP program able to efficiently findthe best hypothesis a PBTS can achieve.
This programcan be easily extended to test various improvements to4We omit here optimization errors.phrase-base systems or to evaluate the impact of differ-ent parameter settings.
Second, we present a number ofcomplementary results illustrating the usage of our or-acle decoder for identifying and analyzing PBTS errors.Our experimental results confirm the main conclusions of(Turchi et al, 2008), showing that extant PBTs have thepotential to generate hypotheses having very high BLEU-4 score and that their main bottleneck is their scoringfunction.The rest of this paper is organized as follows: in Sec-tion 2, we introduce and formalize the oracle decodingproblem, and present a series of ILP problems of increas-ing complexity designed so as to deliver accurate lower-bounds of oracle score.
This section closes with variousextensions allowing to model supplementary constraints,most notably reordering constraints (Section 2.5).
Ourexperiments are reported in Section 3, where we first in-troduce the training and test corpora, along with a de-scription of our system building pipeline (Section 3.1).We then discuss the baseline oracle BLEU scores (Sec-tion 3.2), analyze the non-reachable parts of the referencetranslations, and comment several complementary resultswhich allow to identify causes of failures.
Section 4 dis-cuss our approach and findings with respect to the exist-ing literature on error analysis and oracle decoding.
Weconclude and discuss further prospects in Section 5.2 Oracle Decoder2.1 The Oracle Decoding ProblemDefinition To get some insights on the errors of phrase-based systems and better understand their limits, we pro-pose to consider the oracle decoding problem defined asfollows: given a source sentence, its reference transla-tion5 and a phrase table, what is the ?best?
translationhypothesis a system can generate?
As usual, the qualityof an hypothesis is evaluated by the similarity betweenthe reference and the hypothesis.
Note that in the ora-cle decoding problem, we are only assessing the abilityof PBT systems to generate good candidate translations,irrespective of their ability to score them properly.We believe that studying this problem is interesting forvarious reasons.
First, as described in Section 3.4, com-paring the best hypothesis a system could have gener-ated and the hypothesis it actually generates allows us tocarry on both quantitative and qualitative failure analysis.The oracle decoding problem can also be used to assessthe expressive power of phrase-based systems (Auli etal., 2009).
Other applications include computing accept-able pseudo-references for discriminative training (Till-mann and Zhang, 2006; Liang et al, 2006; Arun and5The oracle decoding problem can be extended to the case of multi-ple references.
For the sake of simplicity, we only describe the case ofa single reference.934Koehn, 2007) or combining machine translation systemsin a multi-source setting (Li and Khudanpur, 2009).
Wehave also used oracle decoding to identify erroneous ordifficult to translate references (Section 3.3).Evaluation Measure To fully define the oracle de-coding problem, a measure of the similarity between atranslation hypothesis and its reference translation hasto be chosen.
The most obvious choice is the BLEU-4score (Papineni et al, 2002) used in most machine trans-lation evaluations.However, using this metric in the oracle decodingproblem raises several issues.
First, BLEU-4 is a met-ric defined at the corpus level and is hard to interpret atthe sentence level.
More importantly, BLEU-4 is not de-composable6: as it relies on 4-grams statistics, the con-tribution of each phrase pair to the global score dependson the translation of the previous and following phrasesand can not be evaluated in isolation.
Because of its non-decomposability, maximizing BLEU-4 is hard; in partic-ular, the phrase-level decomposability of the evaluationmetric is necessary in our approach.To circumvent this difficulty, we propose to evaluatethe similarity between a translation hypothesis and a ref-erence by the number of their common words.
Thisamounts to evaluating translation quality in terms of un-igram precision and recall, which are highly correlatedwith human judgements (Lavie et al, ).
This measureis closely related to the BLEU-1 evaluation metric andthe Meteor (Banerjee and Lavie, 2005) metric (when it isevaluated without considering near-matches and the dis-tortion penalty).
We also believe that hypotheses thatmaximize the unigram precision and recall at the sen-tence level yield corpus level BLEU-4 scores close themaximal achievable.
Indeed, in the setting we will intro-duce in the next section, BLEU-1 and BLEU-4 are highlycorrelated: as all correct words of the hypothesis will becompelled to be at their correct position, any hypothesiswith a high 1-gram precision is also bound to have a high2-gram precision, etc.2.2 Formalizing the Oracle Decoding ProblemThe oracle decoding problem has already been consid-ered in the case of word-based models, in which all trans-lation units are bound to contain only one word.
Theproblem can then be solved by a bipartite graph matchingalgorithm (Leusch et al, 2008): given a n?m binary ma-trix describing possible translation links between sourcewords and target words7, this algorithm finds the subsetof links maximizing the number of words of the referencethat have been translated, while ensuring that each word6Neither at the sentence (Chiang et al, 2008), nor at the phrase level.7The (i, j) entry of the matrix is 1 if the ith word of the source canbe translated by the jth word of the reference, 0 otherwise.is translated only once.Generalizing this approach to phrase-based systemsamounts to solving the following problem: given a setof possible translation links between potential phrases ofthe source and of the target, find the subset of links so thatthe unigram precision and recall are the highest possible.The corresponding oracle hypothesis can then be easilygenerated by selecting the target phrases that are alignedwith one source phrase, disregarding the others.
In ad-dition, to mimic the way OOVs are usually handled, wematch identical OOV tokens appearing both in the sourceand target sentences.
In this approach, the unigram pre-cision is always one (every word generated in the oraclehypothesis matches exactly one word in the reference).As a consequence, to find the oracle hypothesis, we justhave to maximize the recall, that is the number of wordsappearing both in the hypothesis and in the reference.Considering phrases instead of isolated words has amajor impact on the computational complexity: in thisnew setting, the optimal segmentations in phrases of boththe source and of the target have to be worked out in ad-dition to links selection.
Moreover, constraints have tobe taken into account so as to enforce a proper segmenta-tion of the source and target sentences.
These constraintsmake it impossible to use the approach of (Leusch et al,2008) and concur in making the oracle decoding prob-lem for phrase-based models more complex than it is forword-based models: it can be proven, using argumentsborrowed from (De Nero and Klein, 2008), that this prob-lem is NP-hard even for the simple unigram precisionmeasure.2.3 An Integer Program for Oracle DecodingTo solve the combinatorial problem introduced in the pre-vious section, we propose to cast it into an Integer Lin-ear Programming (ILP) problem, for which many genericsolvers exist.
ILP has already been used in SMT to findthe optimal translation for word-based (Germann et al,2001) and to study the complexity of learning phrasealignments (De Nero and Klein, 2008) models.
Follow-ing the latter reference, we introduce the following vari-ables: fi,j (resp.
ek,l) is a binary indicator variable thatis true when the phrase contains all spans from between-word position i to j (resp.
k to l) of the source (resp.target) sentence.
We also introduce a binary variable, de-noted ai,j,k,l, to describe a possible link between sourcephrase fi,j and target phrase ek,l.
These variables arebuilt from the entries of the phrase table according to se-lection strategies introduced in Section 2.4.
In the fol-lowing, index variables are so that:0 ?
i < j ?
n, in the source sentence and0 ?
k < l ?
m, in the target sentence,935where n (resp.
m) is the length of the source (resp.
target)sentence.Solving the oracle decoding problem then amounts tooptimizing the following objective function:maxi,j,k,l?i,j,k,lai,j,k,l ?
(l ?
k) , (1)under the constraints:?x ?
J1,mK :?k,l s.t.
k?x?lek,l ?
1 (2)?y ?
J1, nK :?i,j s.t.
i?y?jfi,j = 1 (3)?k, l :?i,jai,j,k,l = fk,l (4)?i, j :?k,lai,j,k,l = ei,j (5)The objective function (1) corresponds to the numberof target words that are generated.
The first set of con-straints (2) ensures that each word in the reference e ap-pears in no more than one phrase.
Maximizing the objec-tive under these constraints amounts to maximizing theunigram recall.
The second set of constraints (3) ensuresthat each word in the source f is translated exactly once,which guarantees that the search space of the ILP prob-lem is the same as the search space of a phrase-based sys-tem.
Constraints (4) bind the fk,l and ai,j,k,l variables,ensuring that whenever a link ai,j,k,l is active, the corre-sponding phrase fk,l is also active.
Constraints (5) play asimilar role for the reference.The Relaxed Problem Even though it accuratelymodels the search space of a phrase-based decoder,this programs is not really useful as is: due to out-of-vocabulary words or missing entries in the phrase table,the constraint that all source words should be translatedyields infeasible problems8.
We propose to relax thisproblem and allow some source words to remain untrans-lated.
This is done by replacing constraints (3) by:?y ?
J1, nK :?i,j s.t.
i?y?jfi,j ?
1To better reflect the behavior of phrase-based decoders,which attempt to translate all source words, we also needto modify the objective function as follows:?i,j,k,lai,j,k,l ?
(l ?
k) +?i,jfi,j ?
(j ?
i) (6)The second term in this new objective ensures that opti-mal solutions translate as many source words as possible.8An ILP problem is said to be infeasible when every possible solu-tion violates at least one constraint.The Relaxed-Distortion Problem A last caveatwith the Relaxed optimization program is caused byfrequently occurring source tokens, such as functionwords or punctuation signs, which can often align withmore than one target word.
For lack of taking distor-tion information into account in our objective function,all these alignments are deemed equivalent, even if someof them are clearly more satisfactory than others.
Thissituation is illustrated on Figure 1.le chat et le chienthe cat and the dogFigure 1: Equivalent alignments between ?le?
and ?the?.
Thedashed lines corresponds to a less interpretable solution.To overcome this difficulty, we propose a last changeto the objective function:?i,j,k,lai,j,k,l ?
(l ?
k) +?i,jfi,j ?
(j ?
i)??
?i,j,k,lai,j,k,l|k ?
i| (7)Compared to the objective function of the relaxed prob-lem (6), we introduce here a supplementary penalty factorwhich favors monotonous alignments.
For each phrasepair, the higher the difference between source and targetpositions, the higher this penalty.
If ?
is small enough,this extra term allows us to select, among all the opti-mal alignments of the relaxed problem, the one withthe lowest distortion.
In our experiments, we set ?
tomin {n,m} to ensure that the penalty factor is alwayssmaller than the reward for aligning two single words.2.4 Selecting Indicator VariablesIn the approach introduced in the previous sections, theoracle decoding problem is solved by selecting, amonga set of possible translation links, the ones that yield thesolution with the highest unigram recall.We propose two strategies to build this set of possibletranslation links.
In the first one, denoted exact match,an indicator ai,j,k,l is created if there is an entry (f, e) sothat f spans from word position i to j in the source ande from word position k to l in the target.
In this strat-egy, the ILP program considers exactly the same rulesetas conventional phrase-based decoders.We also consider an alternative strategy, which couldhelp us to identify errors made during the phrase extrac-tion process.
In this strategy, denoted inside match, anindicator ai,j,k,l is created when the following three cri-teria are met: i) f spans from position i to j of the source;ii) a substring of e, denoted e?, spans from position k to l936of the reference; iii) (f, e?)
is not an entry of the phrase ta-ble.
The resulting set of indicator variables thus contains,at least, all the variables used in the exact match strategy.In addition, we license here the use of phrases containingwords that do not occur in the reference.
In fact, usingsuch solutions can yield higher BLEU scores when thereward for additional correct matches exceeds the costincurred by wrong predictions.
These cases are symp-toms of situations where the extraction heuristic failed toextract potentially useful subphrases.2.5 Oracle Decoding with Reordering ConstraintsThe ILP problem introduced in the previous section canbe extended in several ways to describe and test variousimprovements to phrase-based systems or to evaluate theimpact of different parameter settings.
This flexibilitymainly stems from the possibility offered by our frame-work to express arbitrary constraints over variables.
Inthis section, we illustrate these possibilities by describinghow reordering constraints can easily be considered.As a first example, the Moses decoder uses a distortionlimit to constrain the set of possible reorderings.
Thisconstraint ?enforces (...) that the last word of a phrasechosen for translation cannot be more than d9 words fromthe leftmost untranslated word in the source?
(Lopez,2009) and is expressed as:?aijkl, ai?j?k?l?
s.t.
k > k?,aijkl ?
ai?j?k?l?
?
|j ?
i?
+ 1| ?
d,The maximum distortion limit strategy (Lopez, 2009) isalso easily expressed and take the following form (assum-ing this constraint is parameterized by d):?l < m?
1,ai,j,k,l?ai?,j?,l+1,l?
?
|i?
?
j ?
1| < dImplementing the ?local?
or MJ-d (Kumar and Byrne,2005) reordering strategy is also straightforward, and im-plies using the following constraints:?i, k,???????i??iai?,j?,k?,l?
??k??kai?,j?,k?,l????????
dSimilarly, It is possible to simulate decoding under theso-called IBM(d) reordering constraints10 by consideringthe following constraints:??
?
m, maxi,k,lj?
?ai,j,k,l ?
j ?
?i,j,k,lai,j,k,l ?
(j ?
i) ?
d9This corresponds to the dl parameter of Moses10Under IBM(d) constraints, the translation is done, phrase by phrase,from the beginning of the sentence until the end and only one of the firstd untranslated phrase can be selected for translation.In these constraints, the first factor corresponds to therightmost translated word of the source and the secondone to the number of translated source words.
The con-straints simply enforce that, at each step of the decoding,there are no more than d source words that were skipped.Note that the constraints introduced above are not alllinear in the problem variables; however they can eas-ily be linearized using standard ILP techniques (Roth andYih, 2005).3 Oracle Decoding for Failure Analysis3.1 Experimental SettingWe propose to use our oracle decoder to study the abilityof a PBTS to translate from English to French and fromGerman to English.
These two languages pairs presentdifferent challenges: English to French translation is con-sidered a relatively easy pair, notwithstanding the diffi-culties of generating the right inflection marks in French.Translating from German into English is more difficult,notably due to the productivity of inflectional and com-pounding processes in German, and also to significantdifferences in word ordering between these languages.Our experiments are based on the corpora distributedfor the WMT?09 constrained tasks (Callison-Burch etal., 2009).
All data are tokenized, cleaned and con-verted to lowercase letters using the tools providedby the organizers.
We then used a standard trainingpipeline to construct the translation model: the bitextswere aligned using Giza++11, symmetrized using thegrow-diag-final-and heuristic; the phrase tablewas extracted and scored using the tools distributed withMoses.12 Finally, baseline systems were optimized usingWMT?08 test set as development using MERT.
Note that,for all these steps, we used the default value of the var-ious parameters.
The extracted phrase table is then usedto find the oracle alignment on the task test set.
Recallthat oracle decoding do not use the scores estimated byMoses in any way.In the experiments reported below, two settings areconsidered.
In the first one, denoted NEWSCO, Moseswas trained only on a small data set taken from the NewsCommentary corpus.
Using a small sized corpus reducesboth training time and decoding time, which allows us toquickly test different configurations of the decoder.
In asecond setting, denoted EUROPARL, Moses was trainedon a larger corpora containing the entirety of the EuroparlCorpus, but no in-domain data, to provide results on morerealistic conditions.
Statistics regarding the different cor-pora used are reported in Table 1.
These statistics arecomputed on the lowercase cleaned corpora.11http://www.fjoch.com/GIZA++.html12http://statmt.org/moses937en ?
fr de ?
enNEWSCO EUROPARL NEWSCO EUROPARL#words 1, 023, 401 21, 616, 114 1, 530, 693 22, 898, 644#sentences 51, 375 1, 050, 398 71, 691 1, 118, 399#vocabulary 31, 416 78, 071 78, 140 242, 219#phrase table 3, 061, 701 46, 003, 525 4, 133, 190 44, 402, 367% OOV 5.3% 3.1% 8.0% 5.2%Table 1: Statistics regarding the training corpora: number of words, number of sentences, vocabulary and phrase table size andpercentage of test words not appearing in the train set (OOV).Finding the oracle alignment amounts to solving theILP problems introduced above.
Even though ILP prob-lems are NP-hard in general, there exist several off-the-shelf ILP solvers able to efficiently find an optimal solu-tion or decide that the problem is infeasible.
In our exper-iments, we used the free solver SCIP (Achterberg, 2007).An optimal solution was found for all problems we con-sidered.
Decoding the 3, 027 sentences of WMT?09 testset takes about 10 minutes (wall time) for the NEWSCOsetting, and several hours for the EUROPARL setting13.3.2 Oracle BLEU ScoreTable 2 reports, for all considered settings, the BLEU-4scores14 achieved by our oracle decoder, as well as thenumber of source words used to generate the oracle hy-pothesis and the number of target words that are reach-able.
In these experiments, two objective functions wereconsidered: first, we only consider the objective functioncorresponding to the relaxed problem defined by Eq.
(6);second, we introduced an extra term in the objective topenalize distortion, as described by Eq.
(7).
Unless ex-plicitly stated otherwise, we always used the exact matchstrategy.The main result in 2 is that, for the two language pairsconsidered, the expressive power of PBTS is not the lim-iting factor to achieve high translation performance.
Infact, for most sentences in the test set, excellent oraclehypotheses, which contain a very high proportion of ref-erence words, are found.
This remains true even when thephrase table is extracted from a small corpus.
Given thatthe best BLEU-4 scores achieved during the WMT?09evaluation are about 28 for the English to French taskand 24 for the German to English task ((Callison-Burchet al, 2009), Tables 26 and 25), these results stronglysuggest that the main bottleneck of current phrase-basedtranslation systems is their scoring function rather thantheir expressive power.
As we will discuss in Section 4,similar conclusions were drawn by (Auli et al, 2009) and(Turchi et al, 2008).Several additional comments on these numbers are in13All our experiments are run on a 8 cores computer, each core beinga 2.2GHz Intel Processor; the decoder is multi-threaded.14These are computed on lowercase with the default tokenization.order.
Despite these very high BLEU scores, in mostcases, the reference is only partly translated.
In the mostfavorable case, for the English to French EUROPARL set-ting, only 26% of the references could be fully gener-ated15.
These numbers are consistent with the results re-ported in (Auli et al, 2009).
Similarly, only about 31%of the source sentences are completely translated by theoracle decoder, which supports our choice to consider arelaxed version of the ILP problem.
Finally, Table 2 alsoshows that introducing the distortion penalty does not af-fect the oracle performance of the decoder.Considering the inside match strategy improves theperformance of the oracle decoder: for instance, for theEnglish to French NEWSCO setting, oracle decoder withthe inside match strategy achieves a BLEU-4 score of70.15 (a 2.5 points improvement over the baseline).
Toachieve this score, 21.45% of the phrases used during de-coding were phrases that are not considered by the exactmatch strategy.
Similar results can be observed for othersettings, which highlights the significance of one kind offailure of the extraction heuristic: useful ?subphrases?
ofactual phrase pairs are not always extracted.The numbers in Table 2, no matter how good they maylook, should be considered with caution: they only implythat, for most test sentences, all the information necessaryto produce a good translation is available in the phrase ta-ble.
However, the alignment decisions underlying theseoracle hypotheses are sometimes hard to justify, and onehas to accept that part of these good hypotheses transla-tions are due to a series of lucky alignment errors.
Thisis illustrated on Figure 2, which displays one such luckyoracle alignment based on the misalignment, during train-ing, of the French preposition ?des?
(of the) with the En-glish noun ?stock?.
Such lucky errors are naturally alsoobserved in the outputs of conventional decoders, eventhough phrase table filtering heuristics probably makesthem somewhat more rare.3.3 Analyzing Non-Reachable Parts of a ReferenceTable 3 contains typical examples of sentence pairs thatcould not be fully generated by our oracle decoder.
They15Similar numbers were obtained, albeit much more slowly, with the--constraint option of Moses.938training set objective function % source translated % target generated 4-BLEUen ?
frNEWSCORELAXED 86.04% 84.74% 67.65RELAXED-DISTORTION 85.99% 84.77% 67.77EUROPARLRELAXED 93.66% 93.06% 85.05RELAXED-DISTORTION 93.65% 93.06% 85.08de ?
enNEWSCORELAXED 82.57% 82.33% 64.60RELAXED-DISTORTION 82.59% 82.30% 64.65EUROPARLRELAXED 90.34% 91.16% 81.77RELAXED-DISTORTION 90.36% 91.12% 81.77Table 2: Translation score of the ILP oracle decoder for the various settings described in Section 3.1stock fall in asiachute des actions en asieFigure 2: Example of alignment obtained by our oracle decoderillustrate the three main reasons which cause some partsof the reference to remain unreachable:?
phrases are missing from the phrase table, eitherbecause they do not occur in the training corpus(OOVs) or because they failed to be extracted.
InTable 3, OOV errors are mainly due to past tenseforms translated into verbs conjugated in passe?
sim-ple (?rejeta?, ?rencontre`rent?, ?renoua?)
a Frenchliterary tense, mostly used in formal writings.?
obvious errors (misspelled words, misinterpretationor mistranslation, ...) in the reference.
The refer-ence of the fifth example contains one such error:the state name ?Nevada?
is translated to ?n?e?vadiez?
(literally ?have not escaped?
), yielding a very poorreference sentence.?
parts of the reference have no translation equiva-lence in the source.
This can be either because ref-erences are produced in ?context?
and some piecesof information are moved across sentence bound-aries or because these references are non-literal.
Thefourth example, which seems to be the translation ofa title, falls into this category: the French part con-tains a reference to the context (?les SA?
is referringto the bacteria the text is talking about) which is notin the source text.
Non-literal translation are illus-trated by the third example, where English ?Mon-day?
is translated into French ?la veille?
(the daybefore).While the first kind of errors is inherent to the use ofa statistical approach, the last two kinds result from thequality of the data used in the evaluation and directly im-pact both training and evaluation of automatic translationsystems: if they should not distort too much comparisonsof MT systems, these errors prevent us from assessingthe ?global?
quality of automatic translation and, if sim-ilar errors are found in the train set, they make learningharder as some probability mass is wasted to model them.To provide a more quantitative analysis, we manuallylooked at all the non-aligned parts of some WMT?09 ref-erences and found that out of 800 references, more than133 contain either an obvious translation error or can notbe achieved by a PBTS16.
Note that, while identifyingthese errors could be done in many ways, our oracle de-coder makes it far easier.3.4 Identifying Causes of FailureBy comparing the hypotheses found by the oracle de-coder and the ones found by the phrase-based decoder,causes of failure can be easily identified.
In this section,we will present several measures that allow us to identifyand quantify several causes of failure.Errors Caused by Search Space Pruning Recall fromSection 1.1 that Moses uses several heuristics to prune thesearch space.
In particular, there is a distortion limit anda limit on the number of target phrases considered for onesource phrase.
In this paragraph, we evaluate the impactof these two heuristics on translation quality.Table 4 presents the average distortion computed onthe oracle hypotheses, as well as the percentage ofphrases used that have a distortion strictly greater than6 (the default distortion limit of Moses).
All these num-bers are obtained by solving the RELAXED-DISTORTIONproblem.
Surprisingly enough, the average distortion oforacle hypotheses is quite small, even for the German toEnglish task, and the distortion constraint seems to be vi-olated only in a few cases.
It also appears that the distor-tion of the hypotheses generated in the NEWSCO settingis significantly larger than in the EUROPARL setting.
Thiscan be explained by the extra degrees of freedom in the16Annotation at a finer level is an on-going effort; the annotatedcorpus is available from http://www.limsi.fr/Individu/wisniews/oracle decoding.939?
?
On Monday the American House of Representatives rejected the plan to support the financialsystem, into which up to 700 billion dollars (nearly 12 billion Czech crowns) was to be invested.?
Lundi, la chambre des repre?sentants ame?ricaine rejeta le projet de soutient du syste`me financier,auquel elle aurait du?
consacrer jusqu?a` 700 milliards de dollars (pre`s de 12 bilions de kc?).?
?
Representatives of the legislators met with American Finance Minister Henry Paulson Saturdaynight in order to give the government fund a final form.?
Dans la nuit de samedi a` dimanche, des repre?sentants des le?gislateurs rencontre`rent le ministredes finances ame?ricain Henry Paulson, afin de donner au fond du gouvernement une forme finale.?
?
The Prague Stock Market immediately continued its fall from Monday at the beginning ofTuesday?s trading , when it dropped by nearly six percent.?
Mardi, de`s le de?but des e?changes, la bourse de prague renoua avec sa chute de la veille,lorsqu?elle perdait presque six pour cent.?
?
Antibiotic Resistance?
Les SA re?sistent aux antibiotiques.?
?
According to Nevada Democratic senator Harry Reid, that is how that legislators are trying tohave Congress to reach a definitive agreement as early as on Sunday.?
D?apre`s le se?nateur de`mocrate n?e?vadiez Harry Reid, les le?gislateurs font de sorte que le Congre`saboutisse a` un accord de?finitif de`s dimanche.Table 3: Output examples of our oracle decoder on the English to French task.
Words in bold are non-aligned words and words initalic are non-aligned out-of-vocabulary words.
For clarity the examples have been detokenized and recased.training set avg.distortion%phraseswith a dist.> 6en ?
frNEWSCO 4.57 22.02%EUROPARL 3.21 13.32%de ?
enNEWSCO 5.16 25.37%EUROPARL 3.81 17.21%Table 4: Average distortion and percentage of phrases with adistortion greater that Moses default distortion limit.alignment decisions enabled by the use of larger trainingcorpora and phrase table.To evaluate the impact of the second heuristic, we com-puted the number of phrases discarded by Moses (be-cause of the default ttl limit) but used in the oracle hy-potheses.
In the English to French NEWSCO setting,they account for 34.11% of the total number of phrasesused in the oracle hypotheses.
When the oracle decoderis constrained to use the same phrase table as Moses, itsBLEU-4 score drops to 42.78.
This shows that filteringthe phrase table prior to decoding discards many usefulphrase pairs and is seriously limiting the best achievableperformance, a conclusion shared with (Auli et al, 2009).Search Errors Search errors can be identified by com-paring the score of the best hypothesis found by Mosesand the score of the oracle hypothesis.
If the score of theoracle hypothesis is higher, then there has been a searcherror; on the contrary, there has been an estimation errorwhen the score of the oracle hypothesis is lower than thescore of the best hypothesis found by Moses.Based on the comparison of the score of Moses hy-potheses and of oracle hypotheses for the English toFrench NEWSCO setting, our preliminary conclusion isthat the number of search errors is quite limited: onlyabout 5% of the hypotheses of our oracle decoder are ac-tually getting a better score than Moses solutions.
Again,this shows that the scoring function (model error) isone of the main bottleneck of current PBTS.
Compar-ing these hypotheses is nonetheless quite revealing: whileMoses mostly selects phrase pairs with high translationscores and generates monotonous alignments, our ILP de-coder uses larger reorderings and less probable phrasesto achieve better solutions: on average, the reorderingscore of oracle solutions is ?5.74, compared to ?76.78for Moses outputs.
Given the weight assigned throughMERT training to the distortion score, no wonder thatthese hypotheses are severely penalized.The Impact of Phrase Length The observed outputsdo not only depend on decisions made during the search,but also on decisions made during training.
One suchdecision is the specification of maximal length for thesource and target phrases.
In our framework, evaluatingthe impact of this decision is simple: it suffices to changethe definition of indicator variables so as to consider onlyalignments between phrases of a given length.In the English-French NEWSCO setting, the most re-strictive choice, when only alignments between singlewords are authorized, yields an oracle BLEU-4 of 48.68;however, authorizing phrases up to length 2 allows toachieve an oracle value of 66.57, very close to the scoreachieved when considering all extracted phrases (67.77).940This is corroborated with a further analysis of our ora-cle alignments, which use phrases whose average sourcelength is 1.21 words (respectively 1.31 for target words).If many studies have already acknowledged the predomi-nance of ?small?
phrases in actual translations, our oraclescores suggest that, for this language pair, increasing thephrase length limit beyond 2 or 3 might be a waste ofcomputational resources.4 Related WorkTo the best of our knowledge, there are only a few worksthat try to study the expressive power of phrase-based ma-chine translation systems or to provide tools for analyzingpotential causes of failure.The approach described in (Auli et al, 2009) is verysimilar to ours: in this study, the authors propose to findand analyze the limits of machine translation systems bystudying the reference reachability.
A reference is reach-able for a given system if it can be exactly generatedby this system.
Reference reachability is assessed usingMoses in forced decoding mode: during search, all hy-potheses that deviate from the reference are simply dis-carded.
Even though the main goal of this study was tocompare the search space of phrase-based and hierarchi-cal systems, it also provides some insights on the impactof various search parameters in Moses, delivering con-clusions that are consistent with our main results.
As de-scribed in Section 1.2, these authors also propose a typol-ogy of the errors of a statistical translation systems, butdo not attempt to provide methods for identifying them.The authors of (Turchi et al, 2008) study the learn-ing capabilities of Moses by extensively analyzing learn-ing curves representing the translation performances as afunction of the number of examples, and by corruptingthe model parameters.
Even though their focus is moreon assessing the scoring function, they reach conclusionssimilar to ours: the current bottleneck of translation per-formances is not the representation power of the PBTSbut rather in their scoring functions.Oracle decoding is useful to compute reachablepseudo-references in the context of discriminative train-ing.
This is the main motivation of (Tillmann and Zhang,2006), where the authors compute high BLEU hypothe-ses by running a conventional decoder so as to maximizea per-sentence approximation of BLEU-4, under a simple(local) reordering model.Oracle decoding has also been used to assess thelimitations induced by various reordering constraints in(Dreyer et al, 2007).
To this end, the authors proposeto use a beam-search based oracle decoder, which com-putes lower bounds of the best achievable BLEU-4 us-ing dynamic programming techniques over finite-state(for so-called local and IBM constraints) or hierarchicallystructured (for ITG constraints) sets of hypotheses.
Eventhough the numbers reported in this study are not directlycomparable with ours17, it seems that our decoder is notonly conceptually much simpler, but also achieves muchmore optimistic lower-bounds of the oracle BLEU score.The approach described in (Li and Khudanpur, 2009) em-ploys a similar technique, which is to guide a heuristicsearch in an hypergraph representing possible translationhypotheses with n-gram counts matches, which amountsto decoding with a n-gram model trained on the sole ref-erence translation.
Additional tricks are presented in thisarticle to speed-up decoding.Computing oracle BLEU scores is also the subject of(Zens and Ney, 2005; Leusch et al, 2008), yet with adifferent emphasis.
These studies are concerned withfinding the best hypotheses in a word graph or in a con-sensus network, a problem that has various implicationsfor multi-pass decoding and/or system combination tech-niques.
The former reference describes an exponentialapproximate algorithm, while the latter proves the NP-completeness of this problem and discuss various heuris-tic approaches.
Our problem is somewhat more complexand using their techniques would require us to built wordgraphs containing all the translations induced by arbitrarysegmentations and permutations of the source sentence.5 ConclusionsIn this paper, we have presented a methodology for ana-lyzing the errors of PBTS, based on the computation ofan approximation of the BLEU-4 oracle score.
We haveshown that this approximation could be computed fairlyaccurately and efficiently using Integer Linear Program-ming techniques.
Our main result is a confirmation ofthe fact that extant PBTS systems are expressive enoughto achieve very high translation performance with respectto conventional quality measurements.
The main effortsshould therefore strive to improve on the way phrases andhypotheses are scored during training.
This gives furthersupport to attempts aimed at designing context-dependentscoring functions as in (Stroppa et al, 2007; Gimpel andSmith, 2008), or at attempts to perform discriminativetraining of feature-rich models.
(Bangalore et al, 2007).We have shown that the examination of difficult-to-translate sentences was an effective way to detect errorsor inconsistencies in the reference translations, makingour approach a potential aid for controlling the quality orassessing the difficulty of test data.
Our experiments havealso highlighted the impact of various parameters.Various extensions of the baseline ILP program havebeen suggested and/or evaluated.
In particular, the ILPformalism lends itself well to expressing various con-straints that are typically used in conventional PBTS.
In17The best BLEU-4 oracle they achieve on Europarl German to En-glish is approximately 48; but they considered a smaller version of thetraining corpus and the WMT?06 test set.941our future work, we aim at using this ILP framework tosystematically assess various search configurations.
Weplan to explore how replacing non-reachable referenceswith high-score pseudo-references can improve discrim-inative training of PBTS.
We are also concerned by de-termining how tight is our approximation of the BLEU-4 score is: to this end, we intend to compute the bestBLEU-4 score within the n-best solutions of the oracledecoding problem.AcknowledgmentsWarm thanks to Houda Bouamor for helping us with theannotation tool.
This work has been partly financed byOSEO, the French State Agency for Innovation, underthe Quaero program.ReferencesTobias Achterberg.
2007.
Constraint Integer Program-ming.
Ph.D. thesis, Technische Universita?t Berlin.http://opus.kobv.de/tuberlin/volltexte/2007/1611/.Abhishek Arun and Philipp Koehn.
2007.
Online learningmethods for discriminative training of phrase based statis-tical machine translation.
In Proc.
of MT Summit XI, Copen-hagen, Denmark.Michael Auli, Adam Lopez, Hieu Hoang, and Philipp Koehn.2009.
A systematic analysis of translation model searchspaces.
In Proc.
of WMT, pages 224?232, Athens, Greece.Satanjeev Banerjee and Alon Lavie.
2005.
METEOR: Anautomatic metric for MT evaluation with improved correla-tion with human judgments.
In Proc.
of the ACL Workshopon Intrinsic and Extrinsic Evaluation Measures for MachineTranslation and/or Summarization, pages 65?72, Ann Arbor,Michigan.Srinivas Bangalore, Patrick Haffner, and Stephan Kanthak.2007.
Statistical machine translation through global lexi-cal selection and sentence reconstruction.
In Proc.
of ACL,pages 152?159, Prague, Czech Republic.Le?on Bottou and Olivier Bousquet.
2008.
The tradeoffs of largescale learning.
In Proc.
of NIPS, pages 161?168, Vancouver,B.C., Canada.Chris Callison-Burch, Philipp Koehn, Christof Monz, and JoshSchroeder.
2009.
Findings of the 2009 Workshop on Sta-tistical Machine Translation.
In Proc.
of WMT, pages 1?28,Athens, Greece.David Chiang, Steve DeNeefe, Yee Seng Chan, and Hwee TouNg.
2008.
Decomposability of translation metrics forimproved evaluation and efficient algorithms.
In Proc.
ofECML, pages 610?619, Honolulu, Hawaii.John De Nero and Dan Klein.
2008.
The complexity of phrasealignment problems.
In Proc.
of ACL: HLT, Short Papers,pages 25?28, Columbus, Ohio.Markus Dreyer, Keith B.
Hall, and Sanjeev P. Khudanpur.
2007.Comparing reordering constraints for smt using efficient bleuoracle computation.
In NAACL-HLT/AMTA Workshop onSyntax and Structure in Statistical Translation, pages 103?110, Rochester, New York.Ulrich Germann, Michael Jahr, Kevin Knight, Daniel Marcu,and Kenji Yamada.
2001.
Fast decoding and optimal decod-ing for machine translation.
In Proc.
of ACL, pages 228?235,Toulouse, France.Ulrich Germann, Michael Jahr, Kevin Knight, Daniel Marcu,and Kenji Yamada.
2004.
Fast and optimal decoding formachine translation.
Artificial Intelligence, 154(1-2):127?143.Ulrich Germann.
2003.
Greedy decoding for statistical ma-chine translation in almost linear time.
In Proc.
of NAACL,pages 1?8, Edmonton, Canada.Kevin Gimpel and Noah A. Smith.
2008.
Rich source-sidecontext for statistical machine translation.
In Proc.
of WMT,pages 9?17, Columbus, Ohio.Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003.
Sta-tistical phrase-based translation.
In Proc.
of NAACL, pages48?54, Edmonton, Canada.Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan,Wade Shen, Christine Moran, Richard Zens, Chris Dyer, On-drej Bojar, Alexandra Constantin, and Evan Herbst.
2007.Moses: Open source toolkit for statistical machine transla-tion.
In Proc.
of ACL, demonstration session.Philipp Koehn.
2004.
Pharaoh: A beam search decoder forphrase-based statistical machine translation models.
In Proc.of AMTA, pages 115?124, Washington DC.Shankar Kumar and William Byrne.
2005.
Local phrase re-ordering models for statistical machine translation.
In Proc.of HLT, pages 161?168, Vancouver, Canada.Alon Lavie, Kenji Sagae, and Shyamsundar Jayaraman.
Thesignificance of recall in automatic metrics for MT evaluation.In In Proc.
of AMTA, pages 134?143, Washington DC.Gregor Leusch, Evgeny Matusov, and Hermann Ney.
2008.Complexity of finding the BLEU-optimal hypothesis in aconfusion network.
In Proc.
of EMNLP, pages 839?847,Honolulu, Hawaii.Zhifei Li and Sanjeev Khudanpur.
2009.
Efficient extractionof oracle-best translations from hypergraphs.
In Proc.
ofNAACL, pages 9?12, Boulder, Colorado.Percy Liang, Alexandre Bouchard-Co?te?, Dan Klein, and BenTaskar.
2006.
An end-to-end discriminative approach to ma-chine translation.
In Proc.
of ACL, pages 761?768, Sydney,Australia.Adam Lopez.
2009.
Translation as weighted deduction.
InProc.
of EACL, pages 532?540, Athens, Greece.Franz Josef Och and Hermann Ney.
2003.
A systematic com-parison of various statistical alignment models.
Comput.Linguist., 29(1):19?51.Franz Josef Och.
2003.
Minimum error rate training in statis-tical machine translation.
In Proc.
of ACL, pages 160?167,Sapporo, Japan.Kishore Papineni, Salim Roukos, ToddWard, andWei-jing Zhu.2002.
Bleu: A method for automatic evaluation of machinetranslation.
Technical report, Philadelphia, Pennsylvania.D.
Roth and W. Yih.
2005.
Integer linear programming infer-ence for conditional random fields.
In Proc.
of ICML, pages737?744, Bonn, Germany.Nicolas Stroppa, Antal van den Bosch, and Andy Way.
2007.Exploiting source similarity for smt using context-informed942features.
In Andy Way and Barbara Gawronska, editors,Proc.
of TMI, pages 231?240, Sko?vde, Sweden.Christoph Tillmann and Tong Zhang.
2006.
A discriminativeglobal training algorithm for statistical mt.
In Proc.
of ACL,pages 721?728, Sydney, Australia.Marco Turchi, Tijl De Bie, and Nello Cristianini.
2008.
Learn-ing performance of a machine translation system: a statisticaland computational analysis.
In Proc.
of WMT, pages 35?43,Columbus, Ohio.Richard Zens and Hermann Ney.
2005.
Word graphs for sta-tistical machine translation.
In Proc.
of the ACL Workshopon Building and Using Parallel Texts, pages 191?198, AnnArbor, Michigan.943
