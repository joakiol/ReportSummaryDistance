Proceedings of the 12th Conference of the European Chapter of the ACL, pages 211?219,Athens, Greece, 30 March ?
3 April 2009. c?2009 Association for Computational LinguisticsInference Rules and their Application to Recognizing Textual EntailmentGeorgiana DinuSaarland UniversityCampus, D-66123 Saarbru?ckendinu@coli.uni-sb.deRui WangSaarland UniversityCampus, D-66123 Saarbru?ckenrwang@coli.uni-sb.deAbstractIn this paper, we explore ways of improv-ing an inference rule collection and its ap-plication to the task of recognizing textualentailment.
For this purpose, we start withan automatically acquired collection andwe propose methods to refine it and ob-tain more rules using a hand-crafted lex-ical resource.
Following this, we derivea dependency-based structure representa-tion from texts, which aims to provide aproper base for the inference rule appli-cation.
The evaluation of our approachon the recognizing textual entailment datashows promising results on precision andthe error analysis suggests possible im-provements.1 IntroductionTextual inference plays an important role in manynatural language processing (NLP) tasks.
In recentyears, the recognizing textual entailment (RTE)(Dagan et al, 2006) challenge, which focuses ondetecting semantic inference, has attracted a lot ofattention.
Given a text T (several sentences) and ahypothesis H (one sentence), the goal is to detectif H can be inferred from T.Studies such as (Clark et al, 2007) attest thatlexical substitution (e.g.
synonyms, antonyms) orsimple syntactic variation account for the entail-ment only in a small number of pairs.
Thus, oneessential issue is to identify more complex expres-sions which, in appropriate contexts, convey thesame (or similar) meaning.
However, more gener-ally, we are also interested in pairs of expressionsin which only a uni-directional inference relationholds1.1We will use the term inference rule to stand for such con-cept; the two expressions can be actual paraphrases if the re-lation is bi-directionalA typical example is the following RTE pair inwhich accelerate to in H is used as an alternativeformulation for reach speed of in T.T: The high-speed train, scheduled for a trial run on Tues-day, is able to reach a maximum speed of up to 430 kilome-ters per hour, or 119 meters per second.H: The train accelerates to 430 kilometers per hour.One way to deal with textual inference isthrough rule representation, for example X wroteY ?
X is author of Y.
However, manually buildingcollections of inference rules is time-consumingand it is unlikely that humans can exhaustivelyenumerate all the rules encoding the knowledgeneeded in reasoning with natural language.
In-stead, an alternative is to acquire these rules au-tomatically from large corpora.
Given such a rulecollection, the next step to focus on is how to suc-cessfully use it in NLP applications.
This papertackles both aspects, acquiring inference rules andusing them for the task of recognizing textual en-tailment.For the first aspect, we extend and refine an ex-isting collection of inference rules acquired basedon the Distributional Hypothesis (DH).
One of themain advantages of using the DH is that the onlyinput needed is a large corpus of (parsed) text2.For the extension and refinement, a hand-craftedlexical resource is used for augmenting the origi-nal inference rule collection and exclude some ofthe incorrect rules.For the second aspect, we focus on applyingthese rules to the RTE task.
In particular, we usea structure representation derived from the depen-dency parse trees of T and H, which aims to cap-ture the essential information they convey.The rest of the paper is organized as follows:Section 2 introduces the inference rule collection2Another line of work on acquiring paraphrases uses com-parable corpora, for instance (Barzilay and McKeown, 2001),(Pang et al, 2003)211we use, based on the Discovery of Inference Rulesfrom Text (henceforth DIRT) algorithm and dis-cusses previous work on applying it to the RTEtask.
Section 3 focuses on the rule collection it-self and on the methods in which we use an exter-nal lexical resource to extend and refine it.
Sec-tion 4 discusses the application of the rules for theRTE data, describing the structure representationwe use to identify the appropriate context for therule application.
The experimental results will bepresented in Section 5, followed by an error analy-sis and discussions in Section 6.
Finally Section 7will conclude the paper and point out future workdirections.2 BackgroundA number of automatically acquired inferencerule/paraphrase collections are available, such as(Szpektor et al, 2004), (Sekine, 2005).
In ourwork we use the DIRT collection because it is thelargest one available and it has a relatively goodaccuracy (in the 50% range for top generated para-phrases, (Szpektor et al, 2007)).
In this section,we describe the DIRT algorithm for acquiring in-ference rules.
Following that, we will overviewthe RTE systems which take DIRT as an externalknowledge resource.2.1 Discovery of Inference Rules from TextThe DIRT algorithm has been introduced by (Linand Pantel, 2001) and it is based on what is calledthe Extended Distributional Hypothesis.
The orig-inal DH states that words occurring in similarcontexts have similar meaning, whereas the ex-tended version hypothesizes that phrases occur-ring in similar contexts are similar.An inference rule in DIRT is a pair of binaryrelations ?
pattern1(X,Y ), pattern2(X,Y ) ?which stand in an inference relation.
pattern1 andpattern2 are chains in dependency trees3 while Xand Y are placeholders for nouns at the end of thischain.
The two patterns will constitute a candi-date paraphrase if the sets of X and Y values ex-hibit relevant overlap.
In the following example,the two patterns are prevent and provide protectionagainst.Xsubj????
preventobj???
YXsubj????
provideobj???
protectionmod????
againstpcomp?????
Y3obtained with the Minipar parser (Lin, 1998)X put emphasis on Y?
X pay attention to Y?
X attach importance to Y?
X increase spending on Y?
X place emphasis on Y?
Y priority of X?
X focus on YTable 1: Example of DIRT algorithm output.
Mostconfident paraphrases of X put emphasis on YSuch rules can be informally defined (Szpek-tor et al, 2007) as directional relations betweentwo text patterns with variables.
The left-hand-side pattern is assumed to entail the right-hand-side pattern in certain contexts, under the samevariable instantiation.
The definition relaxes theintuition of inference, as we only require the en-tailment to hold in some and not all contexts, mo-tivated by the fact that such inferences occur oftenin natural text.The algorithm does not extract directional in-ference rules, it can only identify candidate para-phrases; many of the rules are however uni-directional.
Besides syntactic rewriting or lexi-cal rules, rules in which the patterns are rathercomplex phrases are also extracted.
Some of therules encode lexical relations which can also befound in resources such as WordNet while oth-ers are lexical-syntactic variations that are unlikelyto occur in hand-crafted resources (Lin and Pan-tel, 2001).
Table 1 gives a few examples of rulespresent in DIRT4.Current work on inference rules focuses onmaking such resources more precise.
(Basili etal., 2007) and (Szpektor et al, 2008) propose at-taching selectional preferences to inference rules.These are semantic classes which correspond tothe anchor values of an inference rule and havethe role of making precise the context in which therule can be applied 5.
This aspect is very impor-tant and we plan to address it in our future work.However in this paper we investigate the first andmore basic issue: how to successfully use rules intheir current form.4For simplification, in the rest of the paper we will omitgiving the dependency relations in a pattern.5For example X won Y entails X played Y only when Yrefers to some sort of competition, but not if Y refers to amusical instrument.2122.2 Related WorkIntuitively such inference rules should be effectivefor recognizing textual entailment.
However, onlya small number of systems have used DIRT as a re-source in the RTE-3 challenge, and the experimen-tal results have not fully shown it has an importantcontribution.In (Clark et al, 2007)?s approach, semanticparsing to clause representation is performed andtrue entailment is decided only if every clausein the semantic representation of T semanticallymatches some clause in H. The only variation al-lowed consists of rewritings derived from Word-Net and DIRT.
Given the preliminary stage of thissystem, the overall results show very low improve-ment over a random classification baseline.
(Bar-Haim et al, 2007) implement a proofsystem using rules for generic linguistic struc-tures, lexical-based rules, and lexical-syntacticrules (these obtained with a DIRT-like algorithmon the first CD of the Reuters RCV1 corpus).
Theentailment considers not only the strict notion ofproof but also an approximate one.
Given premisep and hypothesis h, the lexical-syntactic compo-nent marks all lexical noun alignments.
For ev-ery pair of alignment, the paths between the twonouns are extracted, and the DIRT algorithm isapplied to obtain a similarity score.
If the scoreis above a threshold the rule is applied.
Howeverthese lexical-syntactic rules are only used in about3% of the attempted proofs and in most cases thereis no lexical variation.
(Iftene and Balahur-Dobrescu, 2007) use DIRTin a more relaxed manner.
A DIRT rule is em-ployed in the system if at least one of the anchorsmatch in T and H, i.e.
they use them as unaryrules.
However, the detailed analysis of the sys-tem that they provide shows that the DIRT com-ponent is the least relevant one (adding 0.4% ofprecision).In (Marsi et al, 2007), the focus is on the use-fulness of DIRT.
In their system a paraphrase sub-stitution step is added on top of a system based ona tree alignment algorithm.
The basic paraphrasesubstitution method follows three steps.
Initially,the two patterns of a rule are matched in T andH (instantiations of the anchors X , Y do not haveto match).
The text tree is transformed by apply-ing the paraphrase substitution.
Following this,the transformed text tree and hypothesis trees arealigned.
The coverage (proportion of aligned con-X write Y ?X author YX, founded in Y ?X, opened in YX launch Y ?
X produce YX represent Z ?
X work for Ydeath relieved X?
X diedX faces menace from Y ?
X endangered by YX, peace agreement for Y?
X is formulated to end war in YTable 2: Example of inference rules needed inRTEtent words) is computed and if above some thresh-old, entailment is true.
The paraphrase compo-nent adds 1.0% to development set results and only0.5% to test sets, but a more detailed analysis onthe results of the interaction with the other systemcomponents is not given.3 Extending and refining DIRTBased on observations of using the inference rulecollection on the real data, we discover that 1)some of the needed rules still lack even in a verylarge collection such as DIRT and 2) some system-atic errors in the collection can be excluded.
Onboth aspects, we use WordNet as additional lexi-cal resource.Missing RulesA closer look into the RTE data reveals thatDIRT lacks many of the rules that entailment pairsrequire.Table 2 lists a selection of such rules.
Thefirst rows contain rules which are structurally verysimple.
These, however, are missing from DIRTand most of them also from other hand-crafted re-sources such as WordNet (i.e.
there is no shortpath connecting the two verbs).
This is to be ex-pected as they are rules which hold in specific con-texts, but difficult to be captured by a sense dis-tinction of the lexical items involved.The more complex rules are even more difficultto capture with a DIRT-like algorithm.
Some ofthese do not occur frequently enough even in largeamounts of text to permit acquiring them via theDH.Combining WordNet and DIRTIn order to address the issue of missing rules,we investigate the effects of combining DIRT withan exact hand-coded lexical resource in order tocreate new rules.For this we extended the DIRT rules by adding213X face threat of Y?
X at risk of Yface?
confront, front, look, face upthreat?
menace, terror, scourgerisk?
danger, hazard, jeopardy,endangerment, perilTable 3: Lexical variations creating new rulesbased on DIRT rule X face threat of Y ?
X at riskof Yrules in which any of the lexical items involvedin the patterns can be replaced by WordNet syn-onyms.
In the example above, we consider theDIRT rule X face threat of Y ?
X, at risk of Y(Table 3).Of course at this moment due to the lack ofsense disambiguation, our method introduces lotsof rules that are not correct.
As one can see, ex-pressions such as front scourge do not make anysense, therefore any rules containing this will beincorrect.
However some of the new rules createdin this example, such as X face threat of Y ?
X,at danger of Y are reasonable ones and the ruleswhich are incorrect often contain patterns that arevery unlikely to occur in natural text.The idea behind this is that a combination ofvarious lexical resources is needed in order tocover the vast variety of phrases which humanscan judge to be in an inference relation.The method just described allows us to identifythe first four rules listed in Table 2.
We also ac-quire the rule X face menace of Y ?
X endangeredby Y (via X face threat of Y ?
X threatened by Y,menace ?
threat, threaten ?
endanger).Our extension is application-oriented thereforeit is not intended to be evaluated as an independentrule collection, but in an application scenario suchas RTE (Section 6).In our experiments we also made a step towardsremoving the most systematic errors present inDIRT.
DH algorithms have the main disadvantagethat not only phrases with the same meaning areextracted but also phrases with opposite meaning.In order to overcome this problem and sincesuch errors are relatively easy to detect, we ap-plied a filter to the DIRT rules.
This eliminatesinference rules which contain WordNet antonyms.For such a rule to be eliminated the two patternshave to be identical (with respect to edge labelsand content words) except from the antonymouswords; an example of a rule eliminated this way isX have confidence in Y ?
X lack confidence in Y.As pointed out by (Szpektor et al, 2007) a thor-ough evaluation of a rule collection is not a trivialtask; however due to our methodology we can as-sume that the percentage of rules eliminated thisway that are indeed contradictions gets close to100%.4 Applying DIRT on RTEIn this section we point out two issues that are en-countered when applying inference rules for tex-tual entailment.
The first issue is concerned withcorrectly identifying the pairs in which the knowl-edge encoded in these rules is needed.
Follow-ing this, another non-trivial task is to determinethe way this knowledge interacts with the rest ofinformation conveyed in an entailment pair.
In or-der to further investigate these issues, we apply therule collection on a dependency-based representa-tion of text and hypothesis, namely Tree Skeleton.4.1 ObservationsA straightforward experiment can reveal the num-ber of pairs in the RTE data which contain rulespresent in DIRT.
For all the experiments in this pa-per, we use the DIRT collection provided by (Linand Pantel, 2001), derived from the DIRT algo-rithm applied on 1GB of news text.
The resultswe report here use only the most confident rulesamounting to more than 4 million rules (top 40 fol-lowing (Lin and Pantel, 2001)).6Following the definition of an entail-ment rule, we identify RTE pairs in whichpattern1(w1, w2) and pattern2(w1, w2) arematched one in T and the other one in H and?pattern1(X,Y ), pattern2(X,Y )?
is an infer-ence rule.
The pair bellow is an example of this.T: The sale was made to pay Yukos US$ 27.5 billion taxbill, Yuganskneftegaz was originally sold for US$ 9.4 bil-lion to a little known company Baikalfinansgroup which waslater bought by the Russian state-owned oil company Ros-neft.H: Baikalfinansgroup was sold to Rosneft.6Another set of experiments showed that for this particu-lar task, using the entire collection instead of a subset gavesimilar results.214On average, only 2% of the pairs in the RTEdata is subject to the application of such inferencerules.
Out of these, approximately 50% are lexicalrules (one verb entailing the other).
Out of theselexical rules, around 50% are present in WordNetin a synonym, hypernym or sister relation.
At amanual analysis, close to 80% of these are correctrules; this is higher than the estimated accuracy ofDIRT, probably due to the bias of the data whichconsists of pairs which are entailment candidates.However, given the small number of inferencerules identified this way, we performed anotheranalysis.
This aims at determining an upperbound of the number of pairs featuring entailmentphrases present in a collection.
Given DIRT andthe RTE data, we compute in how many pairsthe two patterns of a paraphrase can be matchedirrespective of their anchor values.
An example isthe following pair,T: Libya?s case against Britain and the US concerns thedispute over their demand for extradition of Libyans chargedwith blowing up a Pan Am jet over Lockerbie in 1988.H: One case involved the extradition of Libyan suspectsin the Pan Am Lockerbie bombing.This is a case in which the rule is correct andthe entailment is positive.
In order to determinethis, a system will have to know that Libya?s caseagainst Britain and the US in T entails one casein H. Similarly, in this context, the dispute overtheir demand for extradition of Libyans chargedwith blowing up a Pan Am jet over Lockerbie in1988 in T can be replaced with the extradition ofLibyan suspects in the Pan Am Lockerbie bombingpreserving the meaning.Altogether in around 20% of the pairs, patternsof a rule can be found this way, many times withmore than one rule found in a pair.
However, inmany of these pairs, finding the patterns of an in-ference rule does not imply that the rule is trulypresent in that pair.Considering a system is capable of correctlyidentifying the cases in which an inference ruleis needed, subsequent issues arise from the waythese fragments of text interact with the surround-ing context.
Assuming we have a correct rulepresent in an entailment pair, the cases in whichthe pair is still not a positive case of entailmentcan be summarized as follows:?
The entailment rule is present in parts of thetext which are not relevant to the entailmentvalue of the pair.?
The rule is relevant, however the sentencesin which the patterns are embedded block theentailment (e.g.
through negative markers,modifiers, embedding verbs not preservingentailment)7?
The rule is correct in a limited number of con-texts, but the current context is not the correctone.To sum up, making use of the knowledge en-coded with such rules is not a trivial task.
If rulesare used strictly in concordance with their defini-tion, their utility is limited to a very small numberof entailment pairs.
For this reason, 1) instead offorcing the anchor values to be identical as mostprevious work, we allow more flexible rule match-ing (similar to (Marsi et al, 2007)) and 2) fur-thermore, we control the rule application processusing a text representation based on dependencystructure.4.2 Tree SkeletonThe Tree Skeleton (TS) structure was proposed by(Wang and Neumann, 2007), and can be viewedas an extended version of the predicate-argumentstructure.
Since it contains not only the predi-cate and its arguments, but also the dependencypaths in-between, it captures the essential part ofthe sentence.Following their algorithm, we first preprocessthe data using a dependency parser8 and thenselect overlapping topic words (i.e.
nouns) in Tand H. By doing so, we use fuzzy match at thesubstring level instead of full match.
Startingwith these nouns, we traverse the dependencytree to identify the lowest common ancestor node(named as root node).
This sub-tree without theinner yield is defined as a Tree Skeleton.
Figure1 shows the TS of T of the following positiveexample,T For their discovery of ulcer-causing bacteria, Aus-tralian doctors Robin Warren and Barry Marshall have re-ceived the 2005 Nobel Prize in Physiology or Medicine.H Robin Warren was awarded a Nobel Prize.Notice that, in order to match the inference ruleswith two anchors, the number of the dependency7See (Nairn et al, 2006) for a detailed analysis of theseaspects.8Here we also use Minipar for the reason of consistence215Figure 1: Dependency structure of text.
Treeskeleton in boldpaths contained in a TS should also be two.
Inpractice, among all the 800 T-H pairs of the RTE-2 test set, we successfully extracted tree skeletonsin 296 text pairs, i.e., 37% of the test data is cov-ered by this step and results on other data sets aresimilar.Applying DIRT on a TSDependency representations like the tree skele-ton have been explored by many researchers, e.g.
(Zanzotto and Moschitti, 2006) have utilized a treekernel method to calculate the similarity betweenT and H, and (Wang and Neumann, 2007) chosesubsequence kernel to reduce the computationalcomplexity.
However, the focus of this paper is toevaluate the application of inference rules on RTE,instead of exploring methods of tackling the taskitself.
Therefore, we performed a straightforwardmatching algorithm to apply the inference ruleson top of the tree skeleton structure.
Given treeskeletons of T and H, we check if the two left de-pendency paths, the two right ones or the two rootnodes contain the patterns of a rule.In the example above, the rule X obj???receivesubj????
Y ?
Xobj2????
awardobj1????
Y satisfiesthis criterion, as it is matched at the root nodes.Notice that the rule is correct only in restrictedcontexts, in which the object of receive is some-thing which is conferred on the basis of merit.However in this pair, the context is indeed the cor-rect one.5 ExperimentsOur experiments consist in predicting positive en-tailment in a very straightforward rule-based man-ner (Table 4 summarizes the results using threedifferent rule collections).
For each collection weselect the RTE pairs in which we find a tree skele-ton and match an inference rule.
The first numberin our table entries represents how many of suchpairs we have identified, out the 1600 of devel-opment and test pairs.
For these pairs we simplypredict positive entailment and the second entryrepresents what percentage of these pairs are in-deed positive entailment.
Our work does not fo-cus on building a complete RTE system; however,we also combine our method with a bag of wordsbaseline to see the effects on the whole data set.5.1 Results on a subset of the dataIn the first two columns (DirtTS and Dirt+WNTS)we consider DIRT in its original state and DIRTwith rules generated with WordNet as describedin Section 3; all precisions are higher than 67%9.After adding WordNet, approximately in twice asmany pairs, tree skeletons and rules are matched,while the precision is not harmed.
This may in-dicate that our method of adding rules does notdecrease precision of an RTE system.In the third column we report the results of us-ing a set of rules containing only the trivial iden-tity ones (IdTS).
For our current system, this canbe seen as a precision upper bound for all theother collections, in concordance with the fact thatidentical rules are nothing but inference rules ofhighest possible confidence.
The fourth column(Dirt+Id+WNTS) contains what can be consid-ered our best setting.
In this setting considerablymore pairs are covered using a collection contain-ing DIRT and identity rules with WordNet exten-sion.Although the precision results with this settingare encouraging (65% for RTE2 data and 72% forRTE3 data), the coverage is still low, 8% for RTE2and 6% for RTE3.
This aspect together with an er-ror analysis we performed are the focus of Section7.The last column (Dirt+Id+WN) gives the preci-sion we obtain if we simply decide a pair is trueentailment if we have an inference rule matched init (irrespective of the values of the anchors or ofthe existence of tree skeletons).
As expected, onlyidentifying the patterns of a rule in a pair irrespec-tive of tree skeletons does not give any indicationof the entailment value of the pair.9The RTE task is considered to be difficult.
The aver-age accuracy of the systems in the RTE-3 challenge is around61% (Giampiccolo et al, 2007)216RTE Set DirtTS Dirt + WNTS IdTS Dirt + Id + WNTS Dirt + Id + WNRTE2 49/69.38 94/67.02 45/66.66 130/65.38 673/50.07RTE3 42/69.04 70/70.00 29/79.31 93/72.05 661/55.06Table 4: Coverage/precision with various rule collectionsRTE Set BoW MainRTE2 (85 pairs) 51.76% 60.00%RTE3 (64 pairs) 54.68% 62.50%Table 5: Precision on the covered RTE dataRTE Set (800 pairs) BoW Main & BoWRTE2 56.87% 57.75%RTE3 61.12% 61.75%Table 6: Precision on full RTE data5.2 Results on the entire dataAt last, we also integrate our method with a bagof words baseline, which calculates the ratio ofoverlapping words in T and H. For the pairs thatour method covers, we overrule the baseline?s de-cision.
The results are shown in Table 6 (Mainstands for the Dirt + Id + WNTS configuration).On the full data set, the improvement is still smalldue to the low coverage of our method, howeveron the pairs that are covered by our method (Ta-ble 5), there is a significant improvement over theoverlap baseline.6 DiscussionIn this section we take a closer look at the data inorder to better understand how does our methodof combining tree skeletons and inference ruleswork.
We will first perform error analysis on whatwe have considered our best setting so far.
Fol-lowing this, we analyze data to identify the mainreasons which cause the low coverage.For error analysis we consider the pairs incor-rectly classified in the RTE3 test data set, consist-ing of a total of 25 pairs.
We classify the errorsinto three main categories: rule application errors,inference rule errors, and other errors (Table 7).In the first category, the tree skeleton fails tomatch the corresponding anchors of the inferencerules.
For instance, if someone founded the Insti-tute of Mathematics (Instituto di Matematica) atthe University of Milan, it does not follow that theyfounded The University of Milan.
The Institute ofMathematics should be aligned with the Univer-sity of Milan, which should avoid applying the in-ference rule for this pair.A rather small portion of the errors (16%) arecaused by incorrect inference rules.
Out of these,two are correct in some contexts but not in the en-tailment pairs in which they are found.
For exam-ple, the following rule X generate Y ?
X earn Y isused incorrectly, however in the restricted contextof money or income, the two verbs have similarmeaning.
An example of an incorrect rule is X is-sue Y ?
X hit Y since it is difficult to find a contextin which this holds.The last category contains all the other errors.In all these cases, the additional information con-veyed by the text or the hypothesis which cannotbe captured by our current approach, affects theentailment.
For example an imitation diamond isnot a diamond, and more than 1,000 membersof the Russian and foreign media does not entailmore than 1,000 members from Russia; these arenot trivial, since lexical semantics and fine-grainedanalysis of the restrictors are needed.For the second part of our analysis we discussthe coverage issue, based on an analysis of uncov-ered pairs.
A main factor in failing to detect pairsin which entailment rules should be applied is thefact that the tree skeleton does not find the corre-sponding lexical items of two rule patterns.Issues will occur even if the tree skeleton struc-ture is modified to align all the corresponding frag-ments together.
Consider cases such as threaten toboycott and boycott or similar constructions withother embedding verbs such as manage, forget, at-tempt.
Our method can detect if the two embeddedverbs convey a similar meaning, however not howthe embedding verbs affect the implication.Independent of the shortcomings of our treeskeleton structure, a second factor in failing to de-tect true entailment still lies in lack of rules.
Forinstance, the last two examples in Table 2 are en-tailment pair fragments which can be formulatedas inference rules, but it is not straightforward toacquire them via the DH.217Source of error % pairsIncorrect rule application 32%Incorrect inference rules 16%Other errors 52%Table 7: Error analysis7 ConclusionThroughout the paper we have identified impor-tant issues encountered in using inference rules fortextual entailment and proposed methods to solvethem.
We explored the possibility of combin-ing a collection obtained in a statistical, unsuper-vised manner, DIRT, with a hand-crafted lexicalresource in order to make inference rules have alarger contribution to applications.
We also inves-tigated ways of effectively applying these rules.The experiment results show that although cover-age is still not satisfying, the precision is promis-ing.
Therefore our method has the potential to besuccessfully integrated in a larger entailment de-tection framework.The error analysis points out several possiblefuture directions.
The tree skeleton representationwe used needs to be enhanced in order to capturemore accurately the relevant fragments of the text.A different issue remains the fact that a lot of ruleswe could use for textual entailment detection arestill lacking.
A proper study of the limitations ofthe DH as well as a classification of the knowledgewe want to encode as inference rules would be astep forward towards solving this problem.Furthermore, although all the inference rules weused aim at recognizing positive entailment cases,it is natural to use them for detecting negativecases of entailment as well.
In general, we canidentify pairs in which the patterns of an inferencerule are present but the anchors are mismatched, orthey are not the correct hypernym/hyponym rela-tion.
This can be the base of a principled methodfor detecting structural contradictions (de Marn-effe et al, 2008).8 AcknowledgmentsWe thank Dekang Lin and Patrick Pantel forproviding the DIRT collection and to GrzegorzChrupa?a, Alexander Koller, Manfred Pinkal andStefan Thater for very useful discussions.
Geor-giana Dinu and Rui Wang are funded by the IRTGand PIRE PhD scholarship programs.ReferencesRoy Bar-Haim, Ido Dagan, Iddo Greental, Idan Szpek-tor, and Moshe Friedman.
2007.
Semantic inferenceat the lexical-syntactic level for textual entailmentrecognition.
In Proceedings of the ACL-PASCALWorkshop on Textual Entailment and Paraphrasing,pages 131?136, Prague, June.
Association for Com-putational Linguistics.Regina Barzilay and Kathleen R. McKeown.
2001.Extracting paraphrases from a parallel corpus.
InProceedings of 39th Annual Meeting of the Associ-ation for Computational Linguistics, pages 50?57,Toulouse, France, July.
Association for Computa-tional Linguistics.Roberto Basili, Diego De Cao, Paolo Marocco, andMarco Pennacchiotti.
2007.
Learning selectionalpreferences for entailment or paraphrasing rules.
InIn Proceedings of RANLP, Borovets, Bulgaria.Peter Clark, Phil Harrison, John Thompson, WilliamMurray, Jerry Hobbs, and Christiane Fellbaum.2007.
On the role of lexical and world knowledgein rte3.
In Proceedings of the ACL-PASCAL Work-shop on Textual Entailment and Paraphrasing, pages54?59, Prague, June.
Association for ComputationalLinguistics.Ido Dagan, Oren Glickman, and Bernardo Magnini.2006.
The pascal recognising textual entailmentchallenge.
In Lecture Notes in Computer Science,Vol.
3944, Springer, pages 177?190.
Quionero-Candela, J.; Dagan, I.; Magnini, B.; d?Alch-Buc, F.Machine Learning Challenges.Marie-Catherine de Marneffe, Anna N. Rafferty, andChristopher D. Manning.
2008.
Finding contradic-tions in text.
In Proceedings of ACL-08: HLT, pages1039?1047, Columbus, Ohio, June.
Association forComputational Linguistics.Danilo Giampiccolo, Bernardo Magnini, Ido Dagan,and Bill Dolan.
2007.
The third pascal recognizingtextual entailment challenge.
In Proceedings of theACL-PASCAL Workshop on Textual Entailment andParaphrasing, pages 1?9, Prague, June.
Associationfor Computational Linguistics.Adrian Iftene and Alexandra Balahur-Dobrescu.
2007.Hypothesis transformation and semantic variabilityrules used in recognizing textual entailment.
InProceedings of the ACL-PASCAL Workshop on Tex-tual Entailment and Paraphrasing, pages 125?130,Prague, June.
Association for Computational Lin-guistics.Dekang Lin and Patrick Pantel.
2001.
Dirt.
discov-ery of inference rules from text.
In KDD ?01: Pro-ceedings of the seventh ACM SIGKDD internationalconference on Knowledge discovery and data min-ing, pages 323?328, New York, NY, USA.
ACM.Dekang Lin.
1998.
Dependency-based evaluation ofminipar.
In Proc.
Workshop on the Evaluation ofParsing Systems, Granada.218Erwin Marsi, Emiel Krahmer, and Wauter Bosma.2007.
Dependency-based paraphrasing for recog-nizing textual entailment.
In Proceedings of theACL-PASCAL Workshop on Textual Entailment andParaphrasing, pages 83?88, Prague, June.
Associa-tion for Computational Linguistics.Rowan Nairn, Cleo Condoravdi, and Lauri Karttunen.2006.
Computing relative polarity for textual infer-ence.
In Proceedings of ICoS-5 (Inference in Com-putational Semantics, Buxton, UK.Bo Pang, Kevin Knight, and Daniel Marcu.
2003.Syntax-based alignment of multiple translations:Extracting paraphrases and generating new sen-tences.
In HLT-NAACL, pages 102?109.Satoshi Sekine.
2005.
Automatic paraphrase discoverybased on context and keywords between NE pairs.In Proceedings of International Workshop on Para-phrase, pages 80?87, Jeju Island, Korea.Idan Szpektor, Hristo Tanev, Ido Dagan, and Bonaven-tura Coppola.
2004.
Scaling web-based acquisi-tion of entailment relations.
In In Proceedings ofEMNLP, pages 41?48.Idan Szpektor, Eyal Shnarch, and Ido Dagan.
2007.Instance-based evaluation of entailment rule acqui-sition.
In Proceedings of the 45th Annual Meeting ofthe Association of Computational Linguistics, pages456?463, Prague, Czech Republic, June.
Associa-tion for Computational Linguistics.Idan Szpektor, Ido Dagan, Roy Bar-Haim, and JacobGoldberger.
2008.
Contextual preferences.
In Pro-ceedings of ACL-08: HLT, pages 683?691, Colum-bus, Ohio, June.
Association for Computational Lin-guistics.Rui Wang and Gu?nter Neumann.
2007.
Recognizingtextual entailment using sentence similarity based ondependency tree skeletons.
In Proceedings of theACL-PASCAL Workshop on Textual Entailment andParaphrasing, pages 36?41, Prague, June.
Associa-tion for Computational Linguistics.Fabio Massimo Zanzotto and Alessandro Moschitti.2006.
Automatic learning of textual entailmentswith cross-pair similarities.
In ACL-44: Proceed-ings of the 21st International Conference on Com-putational Linguistics and the 44th annual meetingof the Association for Computational Linguistics,pages 401?408, Morristown, NJ, USA.
Associationfor Computational Linguistics.219
