Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and ComputationalNatural Language Learning, pp.
296?305, Prague, June 2007. c?2007 Association for Computational LinguisticsWhy doesn?t EM find good HMM POS-taggers?Mark JohnsonMicrosoft Research Brown UniversityRedmond, WA Providence, RIt-majoh@microsoft.com Mark Johnson@Brown.eduAbstractThis paper investigates why the HMMs es-timated by Expectation-Maximization (EM)produce such poor results as Part-of-Speech(POS) taggers.
We find that the HMMs es-timated by EM generally assign a roughlyequal number of word tokens to each hid-den state, while the empirical distributionof tokens to POS tags is highly skewed.This motivates a Bayesian approach usinga sparse prior to bias the estimator towardsuch a skewed distribution.
We investigateGibbs Sampling (GS) and Variational Bayes(VB) estimators and show that VB con-verges faster than GS for this task and thatVB significantly improves 1-to-1 tagging ac-curacy over EM.
We also show that EM doesnearly as well as VB when the number ofhidden HMM states is dramatically reduced.We also point out the high variance in allof these estimators, and that they requiremany more iterations to approach conver-gence than usually thought.1 IntroductionIt is well known that Expectation-Maximization(EM) performs poorly in unsupervised inductionof linguistic structure (Carroll and Charniak, 1992;Merialdo, 1994; Klein, 2005; Smith, 2006).
In ret-rospect one can certainly find reasons to explain thisfailure: after all, likelihood does not appear in thewide variety of linguistic tests proposed for identi-fying linguistic structure (Fromkin, 2001).This paper focuses on unsupervised part-of-speech (POS) tagging, because it is perhaps the sim-plest linguistic induction task.
We suggest that onereason for the apparent failure of EM for POS tag-ging is that it tends to assign relatively equal num-bers of tokens to each hidden state, while the em-pirical distribution of POS tags is highly skewed,like many linguistic (and non-linguistic) phenomena(Mitzenmacher, 2003).
We focus on first-order Hid-den Markov Models (HMMs) in which the hiddenstate is interpreted as a POS tag, also known as bitagmodels.In this setting we show that EM performs poorlywhen evaluated using a ?1-to-1 accuracy?
evalua-tion, where each POS tag corresponds to at most onehidden state, but is more competitive when evaluatedusing a ?many-to-1 accuracy?
evaluation, where sev-eral hidden states may correspond to the same POStag.
We explain this by observing that the distribu-tion of hidden states to words proposed by the EM-estimated HMMs is relatively uniform, while theempirical distribution of POS tags is heavily skewedtowards a few high-frequency tags.
Based on this,we propose a Bayesian prior that biases the sys-tem toward more skewed distributions and show thatthis raises the 1-to-1 accuracy significantly.
Finally,we show that a similar increase in accuracy can beachieved by reducing the number of hidden states inthe models estimated by EM.There is certainly much useful information thatbitag HMMs models cannot capture.
Toutanova etal.
(2003) describe a wide variety of morphologi-cal and distributional features useful for POS tag-ging, and Clark (2003) proposes ways of incorporat-ing some of these in an unsupervised tagging model.However, bitag models are rich enough to captureat least some distributional information (i.e., the tag296for a word depends on the tags assigned to its neigh-bours).
Moreover, more complex models add addi-tional complicating factors that interact in ways stillpoorly understood; for example, smoothing is gen-erally regarded as essential for higher-order HMMs,yet it is not clear how to integrate smoothing into un-supervised estimation procedures (Goodman, 2001;Wang and Schuurmans, 2005).Most previous work exploiting unsupervisedtraining data for inferring POS tagging models hasfocused on semi-supervised methods in the in whichthe learner is provided with a lexicon specifying thepossible tags for each word (Merialdo, 1994; Smithand Eisner, 2005; Goldwater and Griffiths, 2007)or a small number of ?prototypes?
for each POS(Haghighi and Klein, 2006).
In the context of semi-supervised learning using a tag lexicon, Wang andSchuurmans (2005) observe discrepencies betweenthe empirical and estimated tag frequencies similarto those observed here, and show that constrainingthe estimation procedure to preserve the empiricalfrequencies improves tagging accuracy.
(This ap-proach cannot be used in an unsupervised settingsince the empirical tag distribution is not available).However, as Banko and Moore (2004) point out, theaccuracy achieved by these unsupervised methodsdepends strongly on the precise nature of the su-pervised training data (in their case, the ambiguityof the tag lexicon available to the system), whichmakes it more difficult to understand the behaviourof such systems.2 EvaluationAll of the experiments described below have thesame basic structure: an estimator is used to infera bitag HMM from the unsupervised training cor-pus (the words of Penn Treebank (PTB) Wall StreetJournal corpus (Marcus et al, 1993)), and then theresulting model is used to label each word of thatcorpus with one of the HMM?s hidden states.
Thissection describes how we evaluate how well thesesequences of hidden states correspond to the gold-standard POS tags for the training corpus (here, thePTB POS tags).
The chief difficulty is determiningthe correspondence between the hidden states andthe gold-standard POS tags.Perhaps the most straightforward method of es-tablishing this correspondence is to deterministicallymap each hidden state to the POS tag it co-occursmost frequently with, and return the proportion ofthe resulting POS tags that are the same as the POStags of the gold-standard corpus.
We call this themany-to-1 accuracy of the hidden state sequence be-cause several hidden states may map to the samePOS tag (and some POS tags may not be mappedto by any hidden states at all).As Clark (2003) points out, many-to-1 accuracyhas several defects.
If a system is permitted to positan unbounded number of hidden states (which is notthe case here) then it can achieve a perfect many-to-1 accuracy by placing every word token into its ownunique state.
Cross-validation, i.e., identifying themany-to-1 mapping and evaluating on different sub-sets of the data, would answer many of these objec-tions.
Haghighi and Klein (2006) propose constrain-ing the mapping from hidden states to POS tags sothat at most one hidden state maps to any POS tag.This mapping is found by greedily assigning hiddenstates to POS tags until either the hidden states orPOS tags are exhausted (note that if the number ofhidden states and POS tags differ, some will be unas-signed).
We call the accuracy of the POS sequenceobtained using this map its 1-to-1 accuracy.Finally, several authors have proposed usinginformation-theoretic measures of the divergencebetween the hidden state and POS tag sequences.Goldwater and Griffiths (2007) propose using theVariation of Information (VI) metric described byMeila?
(2003).
We regard the assignments of hid-den states and POS tags to the words of the cor-pus as two different ways of clustering those words,and evaluate the conditional entropy of each clus-tering conditioned on the other.
The VI is the sumof these conditional entropies.
Specifically, given acorpus labeled with hidden states and POS tags, ifp?
(y), p?
(t) and p?
(y, t) are the empirical probabilitiesof a hidden state y, a POS tag t, and the cooccuranceof y and t respectively, then the mutual informationI , entropies H and variation of information VI aredefined as follows:H(Y ) = ??yp?
(y) log p?
(y)H(T ) = ??tp?
(t) log p?
(t)I(Y, T ) =?y,tp?
(y, t) logp?
(y, t)p?(y)p?
(t)H(Y |T ) = H(Y )?
I(Y, T )297H(T |Y ) = H(T )?
I(Y, T )VI (Y, T ) = H(Y |T ) +H(T |Y )As Meila?
(2003) shows, VI is a metric on the spaceof probability distributions whose value reflects thedivergence between the two distributions, and onlytakes the value zero when the two distributions areidentical.3 Maximum Likelihood viaExpectation-MaximizationThere are several excellent textbook presentations ofHidden Markov Models and the Forward-Backwardalgorithm for Expectation-Maximization (Jelinek,1997; Manning and Schu?tze, 1999; Bishop, 2006),so we do not cover them in detail here.
Conceptu-ally, a Hidden Markov Model generates a sequenceof observations x = (x0, .
.
.
, xn) (here, the wordsof the corpus) by first using a Markov model to gen-erate a sequence of hidden states y = (y0, .
.
.
, yn)(which will be mapped to POS tags during evalua-tion as described above) and then generating eachword xi conditioned on its corresponding state yi.We insert endmarkers at the beginning and endingof the corpus and between sentence boundaries, andconstrain the estimator to associate endmarkers witha state that never appears with any other observationtype (this means each sentence can be processed in-dependently by first-order HMMs; these endmarkersare ignored during evaluation).In more detail, the HMM is specified by multi-nomials ?y and ?y for each hidden state y, where?y specifies the distribution over states following yand ?y specifies the distribution over observations xgiven state y.yi | yi?1 = y ?
Multi(?y)xi | yi = y ?
Multi(?y)(1)We used the Forward-Backward algorithm to per-form Expectation-Maximization, which is a proce-dure that iteratively re-estimates the model param-eters (?, ?
), converging on a local maximum of thelikelihood.
Specifically, if the parameter estimate attime ` is (?
(`), ?
(`)), then the re-estimated parame-ters at time `+ 1 are:?
(`+1)y?|y = E[ny?,y]/E[ny] (2)?
(`+1)x|y = E[nx,y]/E[ny]6.95E+067.00E+067.05E+067.10E+067.15E+060 250 500 750 1000?loglikelihoodIterationFigure 1: Variation in negative log likelihood withincreasing iterations for 10 EM runs from differentrandom starting points.where nx,y is the number of times observation x oc-curs with state y, ny?,y is the number of times statey?
follows y and ny is the number of occurences ofstate y; all expectations are taken with respect to themodel (?
(`), ?
(`)).We took care to implement this and the other al-gorithms used in this paper efficiently, since optimalperformance was often only achieved after severalhundred iterations.
It is well-known that EM oftentakes a large number of iterations to converge in like-lihood, and we found this here too, as shown in Fig-ure 1.
As that figure makes clear, likelihood is stillincreasing after several hundred iterations.Perhaps more surprisingly, we often found dra-matic changes in accuracy in the order of 5% occur-ing after several hundred iterations, so we ran 1,000iterations of EM in all of the experiments describedhere; each run took approximately 2.5 days compu-tation on a 3.6GHz Pentium 4.
It?s well-known thataccuracy often decreases after the first few EM it-erations (which we also observed); however in ourexperiments we found that performance improvesagain after 100 iterations and continues improvingroughly monotonically.
Figure 2 shows how 1-to-1accuracy varies with iteration during 10 runs fromdifferent random starting points.
Note that 1-to-1accuracy at termination ranges from 0.38 to 0.45; aspread of 0.07.We obtained a dramatic speedup by working di-rectly with probabilities and rescaling after each ob-servation to avoid underflow, rather than workingwith log probabilities (thanks to Yoshimasa Tsu-2980.350.370.390.410.430.450.470 250 500 750 10001-to-1accuracyIterationFigure 2: Variation in 1-to-1 accuracy with increas-ing iterations for 10 EM runs from different randomstarting points.ruoka for pointing this out).
Since we evaluatedthe accuracy of the estimated tags after each iter-ation, it was important that decoding be done effi-ciently as well.
While most researchers use Viterbidecoding to find the most likely state sequence, max-imum marginal decoding (which labels the observa-tion xi with the state yi that maximizes the marginalprobability P(yi|x, ?, ?))
is faster because it re-usesthe forward and backward tables already constructedby the Forward-Backward algorithm.
Moreover, inseparate experiments we found that the maximummarginal state sequence almost always scored higherthan the Viterbi state sequence in all of our evalua-tions, and at modest numbers of iterations (up to 50)often scored more than 5% better.We also noticed a wide variance in the perfor-mance of models due to random initialization (both?
and ?
are initially jittered to break symmetry); thiswide variance was observed with all of the estima-tors investigated in this paper.
This means we cannotcompare estimators on the basis of single runs, so weran each estimator 10 times from different randomstarting points and report both mean and standarddeviation for all scores.Finally, we also experimented with annealing, inwhich the parameters ?
and ?
are raised to the power1/T , where T is a ?temperature?
parameter that isslowly lowered toward 1 at each iteration accord-ing to some ?annealing schedule?.
We experimentedwith a variety of starting temperatures and annealingschedules (e.g., linear, exponential, etc), but wereunable to find any that produced models whose like-0E+01E+52E+5FrequencyTag / hidden state (sorted by frequency)PT BV BEMEM 25Figure 3: The average number of words labeled witheach hidden state or tag for the EM, VB (with ?x =?y = 0.1) and EM-25 estimators (EM-25 is the EMestimator with 25 hidden states).lihoods were significantly higher (i.e., the models fitbetter) than those found without annealing.The evaluation of the models produced by theEM and other estimators is presented in Table 1.It is difficult to compare these with previous work,but Haghighi and Klein (2006) report that in acompletely unsupervised setting, their MRF model,which uses a large set of additional features and amore complex estimation procedure, achieves an av-erage 1-to-1 accuracy of 41.3%.
Because they pro-vide no information about the variance in this accu-racy it is difficult to tell whether there is a signifi-cant difference between their estimator and the EMestimator, but it is clear that when EM is run longenough, the performance of even very simple mod-els like the bitag HMM is better than generally rec-ognized.As Table 1 makes clear, the EM estimator pro-duces models that are extremely competitive inmany-to-1 accuracy and Variation of Information,but are significantly worse in 1-to-1 accuracy.
Wecan understand these results by comparing the dis-tribution of words to hidden states to the distributionof words to POS tags in the gold-standard evaluationcorpus.
As Figure 3 shows, the distribution of wordsto POS tags is highly skewed, with just 6 POS tags,NN, IN, NNP, DT, JJ and NNS, accounting for over55% of the tokens in the corpus.
By contrast, theEM distribution is much flatter.
This also explainswhy the many-to-1 accuracy is so much better thanthe one-to-one accuracy; presumably several hidden299Estimator 1-to-1 Many-to-1 VI H(T |Y ) H(Y |T )EM (50) 0.40 (0.02) 0.62 (0.01) 4.46 (0.08) 1.75 (0.04) 2.71 (0.06)VB(0.1, 0.1) (50) 0.47 (0.02) 0.50 (0.02) 4.28 (0.09) 2.39 (0.07) 1.89 (0.06)VB(0.1, 10?4) (50) 0.46 (0.03) 0.50 (0.02) 4.28 (0.11) 2.39 (0.08) 1.90 (0.07)VB(10?4, 0.1) (50) 0.42 (0.02) 0.60 (0.01) 4.63 (0.07) 1.86 (0.03) 2.77 (0.05)VB(10?4, 10?4) (50) 0.42 (0.02) 0.60 (0.01) 4.62 (0.07) 1.85 (0.03) 2.76 (0.06)GS(0.1, 0.1) (50) 0.37 (0.02) 0.51 (0.01) 5.45 (0.07) 2.35 (0.09) 3.20 (0.03)GS(0.1, 10?4) (50) 0.38 (0.01) 0.51 (0.01) 5.47 (0.04) 2.26 (0.03) 3.22 (0.01)GS(10?4, 0.1) (50) 0.36 (0.02) 0.49 (0.01) 5.73 (0.05) 2.41 (0.04) 3.31 (0.03)GS(10?4, 10?4) (50) 0.37 (0.02) 0.49 (0.01) 5.74 (0.03) 2.42 (0.02) 3.32 (0.02)EM (40) 0.42 (0.03) 0.60 (0.02) 4.37 (0.14) 1.84 (0.07) 2.55 (0.08)EM (25) 0.46 (0.03) 0.56 (0.02) 4.23 (0.17) 2.05 (0.09) 2.19 (0.08)EM (10) 0.41 (0.01) 0.43 (0.01) 4.32 (0.04) 2.74 (0.03) 1.58 (0.05)Table 1: Evaluation of models produced by the various estimators.
The values of the Dirichlet prior param-eters for ?x and ?y appear in the estimator name for the VB and GS estimators, and the number of hiddenstates is given in parentheses.
Reported values are means over all runs, followed by standard deviations.10 runs were performed for each of the EM and VB estimators, while 5 runs were performed for the GSestimators.
Each EM and VB run consisted of 1,000 iterations, while each GS run consisted of 50,000 it-erations.
For the estimators with 10 runs, a 3-standard error 95% confidence interval is approximately thesame as the standard deviation.states are being mapped onto a single POS tag.
Thisis also consistent with the fact that the cross-entropyH(T |Y ) of tags given hidden states is relatively low(i.e., given a hidden state, the tag is relatively pre-dictable), while the cross-entropy H(Y |T ) is rela-tively high.4 Bayesian estimation via Gibbs Samplingand Variational BayesA Bayesian estimator combines a likelihood termP(x|?, ?)
and a prior P(?, ?)
to estimate the poste-rior probability of a model or hidden state sequence.We can use a Bayesian prior to bias our estimatortowards models that generate more skewed distri-butions.
Because HMMs (and PCFGs) are prod-ucts of multinomials, Dirichlet distributions are aparticularly natural choice for the priors since theyare conjugate to multinomials, which simplifies boththe mathematical and computational aspects of theproblem.
The precise form of the model we investi-gated is:?y | ?y ?
Dir(?y)?y | ?x ?
Dir(?x)yi | yi?1 = y ?
Multi(?y)xi | yi = y ?
Multi(?y)Informally, ?y controls the sparsity of the state-to-state transition probabilities while ?x controls thesparsity of the state-to-observation emission proba-bilities.
As ?x approaches zero the prior stronglyprefers models in which each hidden state emitsas few words as possible.
This captures the intu-ition that most word types only belong to one POS,since the minimum number of non-zero state-to-observation transitions occurs when each observa-tion type is emitted from only one state.
Similarly,as ?y approaches zero the state-to-state transitionsbecome sparser.There are two main techniques for Bayesian esti-mation of such models: Markov Chain Monte Carlo(MCMC) and Variational Bayes (VB).
MCMC en-compasses a broad range of sampling techniques,including component-wise Gibbs sampling, whichis the MCMC technique we used here (Robert andCasella, 2004; Bishop, 2006).
In general, MCMCtechniques do not produce a single model that char-acterizes the posterior, but instead produce a streamof samples from the posterior.
The application ofMCMC techniques, including Gibbs sampling, toHMM inference problems is relatively well-known:see Besag (2004) for a tutorial introduction andGoldwater and Griffiths (2007) for an applicationof Gibbs sampling to HMM inference for semi-300supervised and unsupervised POS tagging.The Gibbs sampler produces state sequences ysampled from the posterior distribution:P(y|x, ?)
?
?P(x,y|?, ?
)P(?|?y)P(?|?x) d?
d?Because Dirichlet priors are conjugate to multino-mials, it is possible to integrate out the model pa-rameters ?
and ?
to yield the conditional distribu-tion for yi shown in Figure 4.
For each observationxi in turn, we resample its state yi conditioned onthe states y?i of the other observations; eventuallythe distribution of state sequences converges to thedesired posterior.Each iteration of the Gibbs sampler is much fasterthan the Forward-Backward algorithm (both taketime linear in the length of the string, but for anHMM with s hidden states, each iteration of theGibbs sampler takes O(s) time while each iterationof the Forward-Backward algorithm takes O(s2)time), so we ran 50,000 iterations of all samplers(which takes roughly the same elapsed time as 1,000Forward-Backward iterations).As can be seen from Table 1, the posterior statesequences we obtained are not particularly good.Further, when we examined how the posterior like-lihoods varied with increasing iterations of Gibbssampling, it became apparent that the likelihood wasstill increasing after 50,000 iterations.
Moreover,when comparing posterior likelihoods from differ-ent runs with the same prior parameters but differ-ent random number seeds, none of the likelihoodscrossed, which one would expect if the samplershad converged and were mixing well (Robert andCasella, 2004).
Just as with EM, we experimentedwith a variety of annealing regimes, but were unableto find any which significantly improved accuracy orposterior likelihood.We also experimented with evaluating state se-quences found using maximum posterior decoding(i.e., model parameters are estimated from the pos-terior sample, and used to perform maximum poste-rior decoding) rather than the samples from the pos-terior produced by the Gibbs sampler.
We found thatthe maximum posterior decoding sequences usuallyscored higher than the posterior samples, but thescores converged after the first thousand iterations.Since the posterior samples are produced as a by-product of Gibbs sampling while maximum poste-rior decoding requires an additional time consumingstep that does not have much impact on scores, weused the posterior samples to produce the results inTable 1.In contrast to MCMC, Variational Bayesian in-ference attempts to find the function Q(y, ?, ?)
thatminimizes an upper bound of the negative log likeli-hood (Jordan et al, 1999):?
log P(x)= ?
log?Q(y, ?, ?
)P(x,y, ?, ?
)Q(y, ?, ?
)dy d?
d??
?
?Q(y, ?, ?)
logP(x,y, ?, ?
)Q(y, ?, ?
)dy d?
d?
(3)The upper bound in (3) is called the Variational FreeEnergy.
We make a ?mean-field?
assumption thatthe posterior can be well approximated by a factor-ized modelQ in which the state sequence y does notcovary with the model parameters ?, ?
(this will betrue if, for example, there is sufficient data that theposterior distribution has a peaked mode):P(x,y, ?, ?)
?
Q(y, ?, ?)
= Q1(y)Q2(?, ?
)The calculus of variations is used to minimize theKL divergence between the desired posterior distri-bution and the factorized approximation.
It turnsout that if the likelihood and conjugate prior be-long to exponential families then the optimalQ1 andQ2 do too, and there is an EM-like iterative pro-cedure that finds locally-optimal model parameters(Bishop, 2006).This procedure is especially attractive for HMMinference, since it involves only a minor modifica-tion to the M-step of the Forward-Backward algo-rithm.
MacKay (1997) and Beal (2003) describeVariational Bayesian (VB) inference for HMMs indetail, and Kurihara and Sato (2006) describe VBfor PCFGs (which only involves a minor modifica-tion to the M-step of the Inside-Outside algorithm).Specifically, the E-step for VB inference for HMMsis the same as in EM, while theM-step is as follows:??
(`+1)y?|y = f(E[ny?,y] + ?y)/f(E[ny] + s?y) (4)??
(`+1)x|y = f(E[nx,y] + ?x)/f(E[ny] +m?x)f(v) = exp(?(v))?
(v) = (v > 7) ?
g(v ?
12) : (?
(v + 1)?
1)/vg(x) ?
log(x) + 0.04167x?2 + 0.00729x?4+0.00384x?6 ?
0.00413x?8 .
.
.
(5)301P(yi|x,y?i, ?)
?
(nxi,yi + ?xnyi +m?x) (nyi,yi?1 + ?ynyi?1 + s?y) (nyi+1,yi + I(yi?1 = yi = yi+1) + ?ynyi + I(yi?1 = yi))Figure 4: The conditional distribution for state yi used in the Gibbs sampler, which conditions on the statesy?i for all observations except xi.
Here m is the number of possible observations (i.e., the size of thevocabulary), s is the number of hidden states and I(?)
is the indicator function (i.e., equal to one if itsargument is true and zero otherwise), nx,y is the number of times observation x occurs with state y, ny?,y isthe number of times state y?
follows y, and ny is the number of times state y occurs; these counts are from(x?i,y?i), i.e., excluding xi and yi.0 120  1  2Figure 5: The scaling function y = f(x) =exp?
(x) (curved line), which is bounded above bythe line y = x and below by the line y = x?
0.5.where ?
is the digamma function (the derivative ofthe log gamma function; (5) gives an asymptotic ap-proximation), and the remaining quantities are justas in the EM updates (2), i.e., nx,y is the number oftimes observation x occurs with state y, ny?,y is thenumber of times state y?
follows y, ny is the numberof occurences of state y, s is the number of hiddenstates and m is the number of observations; all ex-pectations are taken with respect to the variationalparameters (??
(`), ??
(`)).A comparison between (4) and (2) reveals two dif-ferences between the EM and VB updates.
First,the Dirichlet prior parameters ?
are added to theexpected counts.
Second, these posterior counts(which are in fact parameters of the Dirichlet pos-terior Q2) are passed through the function f(v) =exp?
(v), which is plotted in Figure 5.
When v0, f(v) ?
v ?
0.5, so roughly speaking, VB formultinomials involves adding ?
?0.5 to the expectedcounts when they are much larger than zero, where?
is the Dirichlet prior parameter.
Thus VB canbe viewed as a more principled version of the well-known ad hoc technique for approximating Bayesianestimation with EM that involves adding ?
?1 to theexpected counts.
However, in the ad hoc approachthe expected count plus ?
?1 may be less than zero,resulting in a value of zero for the corresponding pa-rameter (Johnson et al, 2007; Goldwater and Grif-fiths, 2007).
VB avoids this problem because f(v) isalways positive when v > 0, even when v is small.Note that because the counts are passed through f ,the updated values for ??
and ??
in (4) are in generalnot normalized; this is because the variational freeenergy is only an upper bound on the negative loglikelihood (Beal, 2003).We found that in general VB performed much bet-ter than GS.
Computationally it is very similar toEM, and each iteration takes essentially the sametime as an EM iteration.
Again, we experimentedwith annealing in the hope of speeding convergence,but could not find an annealing schedule that signifi-cantly lowered the variational free energy (the quan-tity that VB optimizes).
While we had hoped that theBayesian prior would bias VB toward a common so-lution, we found the same sensitivity to initial condi-tions as we found with EM, so just as for EM, we ranthe estimator for 1,000 iterations with 10 differentrandom initializations for each combination of priorparameters.
Table 1 presents the results of VB runswith several different values for the Dirichlet priorparameters.
Interestingly, we obtained our best per-formance on 1-to-1 accuracy when the Dirchlet prior?x = 0.1, a relatively large number, but best per-formance on many-to-1 accuracy was achieved witha much lower value for the Dirichlet prior, namely?x = 10?4.
The Dirichlet prior ?y that controls302sparsity of the state-to-state transitions had little ef-fect on the results.
We did not have computationalresources to fully explore other values for the prior(a set of 10 runs for one set of parameter values takes25 computer days).As Figure 3 shows, VB can produce distributionsof hidden states that are peaked in the same way thatPOS tags are.
In fact, with the priors used here, VBproduces state sequences in which only a subset ofthe possible HMM states are in fact assigned to ob-servations.
This shows that rather than fixing thenumber of hidden states in advance, the Bayesianprior can determine the number of states; this idea ismore fully developed in the infinite HMM of Beal etal.
(2002) and Teh et al (2006).5 Reducing the number of hidden statesEM already performs well in terms of the many-to-1accuracy, but we wondered if there might be someway to improve its 1-to-1 accuracy and VI score.
Insection 3 we suggested that one reason for its poorperformance in these evaluations is that the distri-butions of hidden states it finds tend to be fairlyflat, compared to the empirical distribution of POStags.
As section 4 showed, a suitable Bayesian priorcan bias the estimator towards more peaked distribu-tions, but we wondered if there might be a simplerway of achieving the same result.We experimented with dramatic reductions in thenumber of hidden states in the HMMs estimatedby EM.
This should force the hidden states to bemore densely populated and improve 1-to-1 accu-racy, even though this means that there will be nohidden states that can possibly map onto the less fre-quent POS tags (i.e., we will get these words wrong).In effect, we abandon the low-frequency POS tagsin the hope of improving the 1-to-1 accuracy of thehigh-frequency tags.As Table 1 shows, this markedly improves boththe 1-to-1 accuracy and the VI score.
A 25-stateHMM estimated by EM performs effectively as wellas the best VB model in terms of both 1-to-1 accu-racy and VI score, and runs 4 times faster because ithas only half the number of hidden states.6 Conclusion and future workThis paper studied why EM seems to do so badly inHMM estimation for unsupervised POS tagging.
Infact, we found that it doesn?t do so badly at all: thebitag HMM estimated by EM achieves a mean 1-to-1 tagging accuracy of 40%, which is approximatelythe same as the 41.3% reported by (Haghighi andKlein, 2006) for their sophisticated MRF model.Then we noted the distribution of words to hiddenstates found by EM is relatively uniform, comparedto the distribution of words to POS tags in the eval-uation corpus.
This provides an explanation of whythe many-to-1 accuracy of EM is so high while the1-to-1 accuracy and VI of EM is comparatively low.We showed that either by using a suitable Bayesianprior or by simply reducing the number of hiddenstates it is possible to significantly improve both the1-to-1 accuracy and the VI score, achieving a 1-to-1tagging accuracy of 46%.We also showed that EM and other estimators takemuch longer to converge than usually thought, andoften require several hundred iterations to achieveoptimal performance.
We also found that there isconsiderable variance in the performance of all ofthese estimators, so in general multiple runs fromdifferent random starting points are necessary in or-der to evaluate an estimator?s performance.Finally, there may be more sophisticated ways ofimproving the 1-to-1 accuracy and VI score thanthe relatively crude methods used here that primar-ily reduce the number of available states.
For ex-ample, we might obtain better performance by us-ing EM to infer an HMM with a large number ofstates, and then using some kind of distributionalclustering to group similar HMM states; these clus-ters, rather than the underlying states, would be in-terpreted as the POS tag labels.
Also, the Bayesianframework permits a wide variety of different priorsbesides Dirichlet priors explored here.
For example,it should be possible to encode linguistic knowledgesuch markedness preferences in a prior, and thereare other linguistically uninformative priors, suchthe ?entropic priors?
of Brand (1999), that may beworth exploring.AcknowledgementsI would like to thank Microsoft Research for pro-viding an excellent environment in which to con-duct this work, and my friends and colleagues atMicrosoft Research, especially Bob Moore, ChrisQuirk and Kristina Toutanova, for their helpful com-ments on this paper.303ReferencesMichele Banko and Robert C. Moore.
2004.
Part ofspeech tagging in context.
In Proceedings, 20th In-ternational Conference on Computational Linguistics(Coling 2004), pages 556?561, Geneva, Switzerland.M.J.
Beal, Z. Ghahramani, and C.E.
Rasmussen.
2002.The infinite Hidden Markov Model.
In T. Dietterich,S.
Becker, and Z. Ghahramani, editors, Advances inNeural Information Processing Systems, volume 14,pages 577?584.
The MIT Press.Matthew J. Beal.
2003.
Variational Algorithms for Ap-proximate Bayesian Inference.
Ph.D. thesis, GatsbyComputational Neuroscience unit, University CollegeLondon.Julian Besag.
2004.
An introduction to Markov ChainMonte Carlo methods.
In Mark Johnson, Sanjeev P.Khudanpur, Mari Ostendorf, and Roni Rosenfeld, ed-itors, Mathematical Foundations of Speech and Lan-guage Processing, pages 247?270.
Springer, NewYork.Christopher M. Bishop.
2006.
Pattern Recognition andMachine Learning.
Springer.M.
Brand.
1999.
An entropic estimator for structure dis-covery.
Advances in Neural Information ProcessingSystems, 11:723?729.Glenn Carroll and Eugene Charniak.
1992.
Two experi-ments on learning probabilistic dependency grammarsfrom corpora.
In Proceedings of the AAAI Workshopon Statistically-Based Natural Language ProcessingTechniques, San Jose, CA.Alexander Clark.
2003.
Combining distributional andmorphological information for part of speech induc-tion.
In 10th Conference of the European Chapter ofthe Association for Computational Linguistics, pages59?66.
Association for Computational Linguistics.Victoria Fromkin, editor.
2001.
Linguistics: An Intro-duction to Linguistic Theory.
Blackwell, Oxford, UK.Sharon Goldwater and Tom Griffiths.
2007.
A fullyBayesian approach to unsupervised part-of-speech tag-ging.
In Proceedings of the 45th Annual Meeting of theAssociation for Computational Linguistics.Joshua Goodman.
2001.
A bit of progress in languagemodeling.
Computer Speech and Language, 14:403?434.Aria Haghighi and Dan Klein.
2006.
Prototype-drivenlearning for sequence models.
In Proceedings ofthe Human Language Technology Conference of theNAACL, Main Conference, pages 320?327, New YorkCity, USA, June.
Association for Computational Lin-guistics.Frederick Jelinek.
1997.
Statistical Methods for SpeechRecognition.
The MIT Press, Cambridge, Mas-sachusetts.Mark Johnson, Tom Griffiths, and Sharon Goldwater.2007.
Bayesian inference for PCFGs via Markovchain Monte Carlo.
In Human Language Technologies2007: The Conference of the North American Chap-ter of the Association for Computational Linguistics;Proceedings of the Main Conference, pages 139?146,Rochester, New York.
Association for ComputationalLinguistics.Michael I. Jordan, Zoubin Ghahramani, Tommi S.Jaakkola, and Lawrence K. Sau.
1999.
An introduc-tion to variational methods for graphical models.
Ma-chine Learning, 37(2):183?233.Dan Klein.
2005.
The Unsupervised Learning of NaturalLanguage Structure.
Ph.D. thesis, Stanford Univer-sity.Kenichi Kurihara and Taisuke Sato.
2006.
VariationalBayesian grammar induction for natural language.
In8th International Colloquium on Grammatical Infer-ence.David J.C. MacKay.
1997.
Ensemble learning for hiddenMarkov models.
Technical report, Cavendish Labora-tory, Cambridge.Chris Manning and Hinrich Schu?tze.
1999.
Foundationsof Statistical Natural Language Processing.
The MITPress, Cambridge, Massachusetts.Michell P. Marcus, Beatrice Santorini, and Mary AnnMarcinkiewicz.
1993.
Building a large annotated cor-pus of English: The Penn Treebank.
ComputationalLinguistics, 19(2):313?330.Marina Meila?.
2003.
Comparing clusterings by the vari-ation of information.
In Bernhard Scho?lkopf and Man-fred K. Warmuth, editors, COLT 2003: The SixteenthAnnual Conference on Learning Theory, volume 2777of Lecture Notes in Computer Science, pages 173?187.Springer.Bernard Merialdo.
1994.
Tagging English text witha probabilistic model.
Computational Linguistics,20:155?171.M.
Mitzenmacher.
2003.
A brief history of generativemodels for power law and lognormal distributions.
In-ternet Mathematics, 1(2):226?251.Christian P. Robert and George Casella.
2004.
MonteCarlo Statistical Methods.
Springer.Noah A. Smith and Jason Eisner.
2005.
Contrastiveestimation: Training log-linear models on unlabeleddata.
In Proceedings of the 43rd Annual Meeting of the304Association for Computational Linguistics (ACL?05),pages 354?362, Ann Arbor, Michigan, June.
Associa-tion for Computational Linguistics.Noah A. Smith.
2006.
Novel Estimation Methods forUnsupervised Discovery of Latent Structure in Natu-ral Language Text.
Ph.D. thesis, Johns Hopkins Uni-versity.Y.
W. Teh, M. I. Jordan, M. J. Beal, and D. M. Blei.
2006.Hierarchical Dirichlet processes.
Journal of the Amer-ican Statistical Association, 101(476):1566?1581.Kristina Toutanova, Dan Klein, Christopher D. Manning,and Yoram Singer.
2003.
Feature-rich part-of-speechtagging with a cyclic dependency network.
In Pro-ceedings of the 2003 Human Language TechnologyConference of the North American Chapter of the As-sociation for Computational Linguistics, pages 252?259.Qin Iris Wang and Dale Schuurmans.
2005.
Improvedestimation for unsupervised part-of-speech tagging.
InProceedings of the 2005 IEEE International Confer-ence on Natural Language Processing and KnowledgeEngineering (IEEE NLP-KE?2005), pages 219?224,Wuhan, China.305
