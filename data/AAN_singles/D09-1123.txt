Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1182?1191,Singapore, 6-7 August 2009. c?2009 ACL and AFNLPA Syntactified Direct Translation Model with Linear-time DecodingHany HassanCairo TDCIBMCairo, Egypthanyh@eg.ibm.comKhalil Sima?anLanguage and ComputationUniversity of AmsterdamAmsterdam, The Netherlandsk.simaan@uva.nlAndy WaySchool of ComputingDublin City UniversityDublin, Irelandaway@computing.dcu.ieAbstractRecent syntactic extensions of statisti-cal translation models work with a syn-chronous context-free or tree-substitutiongrammar extracted from an automaticallyparsed parallel corpus.
The decoders ac-companying these extensions typically ex-ceed quadratic time complexity.This paper extends the Direct Transla-tion Model 2 (DTM2) with syntax whilemaintaining linear-time decoding.
Weemploy a linear-time parsing algorithmbased on an eager, incremental interpre-tation of Combinatory Categorial Gram-mar (CCG).
As every input word is pro-cessed, the local parsing decisions resolveambiguity eagerly, by selecting a singlesupertag?operator pair for extending thedependency parse incrementally.
Along-side translation features extracted fromthe derived parse tree, we explore syn-tactic features extracted from the incre-mental derivation process.
Our empiri-cal experiments show that our model sig-nificantly outperforms the state-of-the artDTM2 system.1 IntroductionSyntactic structure is gradually showing itself toconstitute a promising enrichment of state-of-the-art Statistical Machine Translation (SMT) models.However, it would appear that the decoding algo-rithms are bearing the brunt of this improvement interms of time and space complexity.
Most recentextensions work with a synchronous context-freeor tree-substitution grammar extracted from an au-tomatically parsed parallel corpus.
While attrac-tive in many ways, the decoders that are neededfor these types of grammars usually have timeand space complexities that are far beyond linear.Leaving pruning aside, there is a genuine ques-tion as to whether syntactic structure necessarilyimplies more complex decoding algorithms.
Thispaper shows that this need not necessarily be thecase.In this paper we extend the Direct TranslationModel (DTM2) (Ittycheriah and Roukos, 2007)with target language syntax while maintaininglinear-time decoding.
With this extension wemake three novel contributions to SMT.
Our firstcontribution is to define a linear-time syntacticparser that works as incrementally as standardSMT decoders (Tillmann and Ney, 2003; Koehn,2004a).
At every word position in the target lan-guage string, this parser spans at most a singleparse-state to augment the translation states inthe decoder.
The parse state summarizes previ-ous parsing decisions and imposes constraints onthe set of valid future extensions such that a well-formed sequence of parse states unambiguouslydefines a dependency structure.
This approachis based on an incremental interpretation of themechanisms of Combinatory Categorial Grammar(CCG) (Steedman, 2000).Our second contribution lies in extending theDMT2 model with a novel set of syntactically-oriented feature functions.
Crucially, these featurefunctions concern the derived (partial) dependencystructure as well as local aspects of the derivationprocess, including such information as the CCGlexical categories (supertag), the CCG operatorsand the intermediate parse states.
This accom-plishment is interesting both from a linguistic andtechnical point of view.Our third contribution is the extension of thestandard phrase-based decoder with the syntacticstructure and definition of new grammar-specificpruning techniques that control the size of thesearch space.
Interestingly, because it is eager,the incremental parser used in this work is hardpushed to perform at a parsing level close to state-1182of-the-art cubic-time parsers.
Nevertheless, theparsing information it provides allows for signif-icant improvement in translation quality.We test the new model, called the Dependency-based Direct Translation Model (DDTM), on stan-dard Arabic?English translation tasks used in thecommunity, including LDC and GALE data.
Weshow that our DDTM system provides significantimprovements in BLEU (Papineni et al, 2002) andTER (Snover et al, 2006) scores over the alreadyextremely competitive DTM2 system.
We alsoprovide results of manual, qualitative analysis ofthe system output to provide insight into the quan-titative results.This paper is organized as follows.
Section 2reviews the related work.
Section 3 discusses theDTM2 baseline model.
Section 4 presents the gen-eral workings of the incremental CCG parser lay-ing the foundations for integrating it into DTM2.Section 5 details our own DDTM, the dependency-based extension of the DTM2 model.
Section 6reports on extensive experiments and their results.Section 7 provides translation output to shed fur-ther detailed insight into the characteristics of thesystems.
Finally, Section 8 concludes, and dis-cusses future work.2 Related WorkIn (Marcu et al, 2006), it is demonstrated that?syntactified?
target language phrases can im-prove translation quality for Chinese?English.
Astochastic, top-down transduction process is em-ployed that assigns a joint probability to a sourcesentence and each of its alternative syntactifiedtranslations; this is done by specifying a rewrit-ing process of the target parse-tree into a sourcesentence.Likewise, the model in (Zollmann and Venu-gopal, 2006) extends (Chiang, 2005) by augment-ing the hierarchical phrases with syntactic cate-gories derived from parsing the target side of aparallel corpus.
They use an existing parser toparse the target side of the parallel corpus in or-der to extract a syntactically motivated, bilingualsynchronous grammar as in (Chiang, 2005).The above-mentioned approaches for incor-porating syntax into Phrase-based SMT (Marcuet al, 2006; Zollmann and Venugopal, 2006)share common drawbacks.
Firstly, they arebased on syntactic phrase-structure parse treesincorporated into a Synchronous CFG or Tree-Substitution Grammar, which makes for a diffi-cult match with non-constituent phrases that arecommon within Phrase-based SMT.
These ap-proaches usually resort to ad hoc solutions toenrich the non-constituent phrases with syntacticstructures.
Secondly, they deploy chart-based de-coders with a high computational cost comparedwith the phrase-based beam search decoders, e.g.,(Tillmann and Ney, 2003; Koehn, 2004a).
Thirdly,due to the large parse space, some of the pro-posed approaches are forced to employ small lan-guage models compared to what is usually usedin phrase-based systems.
To circumvent thesecomputational limitations, various pruning tech-niques are usually needed, e.g., (Huang and Chi-ang, 2007).Other recent approaches, e.g., (Birch et al,2007; Hassan et al, 2007; Hassan et al, 2008a)incorporate a linear-time supertagger into SMT totake the role of a syntactic language model along-side the standard language model.
While these ap-proaches share with our work the use of lexical-ized grammars, they never seek to build a full de-pendency tree or employ syntactic features in or-der to directly influence the reordering probabili-ties in the decoder.
In the current work, we ex-pand our previous work in (Hassan et al, 2007;Hassan et al, 2008a) to introduce the capabilitiesof building a full dependency structure and em-ploying syntactic features to influence the decod-ing process.Recently, (Shen et al, 2008) introduced an ap-proach for incorporating a dependency-based lan-guage model into SMT.
They proposed to extractString-to-Dependency trees from the parallel cor-pus.
As the dependency trees are not constituentsby nature, they handle non-constituent phrases aswell.
While this work is in the same generaldirection as our work, namely aiming at incor-porating dependency parsing into SMT, there re-main three major differences.
Firstly, (Shen et al,2008) resorted to heuristics to extract the String-to-Dependency trees, whereas our approach em-ploys the well formalized CCG grammatical the-ory.
Secondly, their decoder works bottom-upand uses a chart parser with a limited languagemodel capability (3-grams), while we build on theefficient, linear-time decoder commonly used inphrase-based SMT.
Thirdly, (Shen et al, 2008)deploys the dependency language model to aug-ment the lexical language model probability be-1183tween two head words but never seek a full de-pendency graph.
In contrast, our approach inte-grates an incremental parsing capability, that pro-duces the partial dependency structures incremen-tally while decoding, and thus provides for betterguidance for the search of the decoder for moregrammatical output.
To the best of our knowledge,our approach is the first to incorporate incrementaldependency parsing capabilities into SMT whilemaintaining the linear-time and -space decoder.3 Baseline: Direct Translation Model 2The Direct Translation Model (DTM) (Papineniet al, 1997) employs the a posteriori conditionaldistribution P (T |S) of a target sentence T givena source sentence S, instead of the common in-version into P (S|T ) based on the source chan-nel approach (Brown et al, 1990).
DTM2, in-troduced in (Ittycheriah and Roukos, 2007), ex-presses the phrase-based translation task in a uni-fied log-linear probabilistic framework consistingof three components: (i) a prior conditional dis-tribution P0(.|S), (ii) a number of feature func-tions ?i() that capture the translation and languagemodel effects, and (iii) the weights of the features?ithat are estimated under MaxEnt (Berger et al,1996), as in (1):P (T |S) =P0(T, J |S)Zexp?i?i?i(T, J, S) (1)Here J is the skip reordering factor for the phrasepair captured by ?i() and represents the jump fromthe previous source word, and Z is the per sourcesentence normalization term.
The prior probabil-ity P0is the prior distribution for the phrase prob-ability which is estimated using the phrase nor-malized counts commonly used in conventionalPhrase?based SMT systems, e.g., (Koehn et al,2003).DTM2 differs from other Phrase?based SMTmodels in that it extracts from a word-aligned par-allel corpus only a non-redundant set of minimalphrases in the sense that no two phrases overlapwith each other.Baseline DTM2 Features: The baseline em-ploys the following five types of features (besidethe language model):?
Lexical Micro Features examining sourceand target words of the phrases,?
Lexical Context Features encoding thesource and target phrase context (i.e.
previ-ous and next source and previous target),?
Source Morphological Features encodingmorphological and segmentation characteris-tics of source words.?
Part-of-Speech Features encoding source andtarget POS tags as well as the POS tags of thesurrounding contexts of phrases.The DTM2 approach based on MaxEnt providesa flexible framework for incorporating other avail-able feature types as we demonstrate below.DTM2 Decoder: The decoder for the baseline isa beam search decoder similar to decoders used instandard phrase-based log-linear systems such as(Tillmann and Ney, 2003) and (Koehn, 2004a).The main difference between the DTM2 decoderand the standard Phrase?based SMT decoders isthat DTM2 deploys Maximum Entropy probabilis-tic models to obtain the translation costs and var-ious feature costs by deploying the features de-scribed above in a discriminative MaxEnt fashion.In the rest of this paper we adopt the DTM2 for-malization of translation as a discriminative task,and we describe the CCG-based incremental de-pendency parser that we use for extending theDTM2 decoder, and then list a new set of syntac-tic dependency feature functions that extend theDTM2 feature set.
We also discuss pruning andother details of the approach.4 The Incremental Dependency ParserAs it processes an input sentence left-to-rightword-by-word, the incremental dependency modelbuilds?for each prefix of the input sentence?apartial parse that is a subgraph of the partial parsethat it builds for a longer prefix.
The dependencygraph is constructed incrementally, in that the sub-graph constructed at a preceding step is never al-tered or revised in any later steps.
The followingschematic view in (2) exhibits the general work-ings of this parser:S0o1w1,st1//S1o2w2,st2//S2Sioiwi,sti//Si+1Sn(2)The syntactic process is represented by a sequenceof transitions between adjacent syntactic states Si.1184A transition from state Si?1to Siscans the cur-rent word wiand stochastically selects a com-plex lexical descriptor/category stiand an oper-ator oigiven the local context in the transition se-quence.
The syntactic state Sisummarizes all thesyntactic information about fragments that havealready been processed and registers the syntac-tic arguments which are to be expected next.
Onlyan impoverished deterministic procedure (called a?State-Realizer?)
is needed in order to compose astate Siwith the previous states S0.
.
.
Si?1in or-der to obtain a fully connected intermediate depen-dency structure at every position in the input.To implement the incremental parsing schemedescribed above we use the parser described in(Hassan et al, 2008b; Hassan et al, 2009), whichis based on Combinatory Categorial Grammar(CCG) (Steedman, 2000).
We only briefly de-scribe this parser as its full description is beyondthe scope of this paper.
The notions of a supertagas a lexical category and the process of supertag-ging are both crucial here (Bangalore and Joshi,1999).
Fortunately, CCG specifies the desired kindof lexical categories (supertags) stifor every wordand a small set of combinatory operators oithatcombine the supertag stiwith a previous parsestate Si?1into the next parse state Si.
In termsof CCG representations, the parse state is a CCGcomposite category which specifies either a func-tor and the arguments it expects to the right of thecurrent word, or is itself an argument for a functorthat will follow it to the right.
At the first word inthe sentence, the parse state consists solely of thesupertag of that word.Attacks rocked RiyadhS0NP (S\NP)/NP NP> NOPS1: NP> TRFCS2: S/NP> FAS3: SFigure 1: A sentence and possible supertag-,operator- and state-sequences.
NOP: No Oper-ation; TRFC: Type Raise-Forward Composition;FA: Forward Application.
The CCG operatorsused show that Attacks and Riyadh are bothdependents of rocked.Figure 1 exhibits an example of the workings ofthis parser.
Practically speaking, after POS tag-ging the input sentence, the parser employs twocomponents:?
A Supertag-Operator Tagger which proposesa supertag?operator pair for the current word,?
A deterministic State-Realizer, which real-izes the current state by applying the currentoperator to the previous state and the currentsupertag.The Supertag-Operator Tagger is a probablisticcomponent while the State-Realizer is a determin-istic component.
The generative model underlyingthis component concerns the probability P (W,S)of a word sequence W = wn1and a parse-statesequence S = Sn1, with associated supertag se-quence ST = stn1and operator sequence O = on1,which represents a possible derivation.
Note thatgiven the choice of supertags stiand operator oi,the state Siis calculated deterministically by theState-Realizer.A generative version of this model is describedin (3):P (W,S) =n?i=1Word Predictor?
??
?P (wi|Wi?1Si?1).Supertagger?
??
?P (sti|Wi) .Operator Tagger?
??
?P (oi|Wi, Si?1, STi) (3)In (3):?
P (W,S) represents the product of the pro-duction probabilities at each parse-state andis similar to the structured language modelrepresentation introduced in (Chelba, 2000).?
P (wi|Wi?1Si?1) is the probability of wigiven the previous sequence of words Wi?1and the previous sequence of states Si?1,?
P (sti|Wi): is the supertag stiprobabilitygiven the word sequence Wiup to the cur-rent position.
Basically, this represents a se-quence tagger (a ?supertagger?).?
P (oi|Wi, Si?1, STi) represents the probabil-ity of the operator oigiven the previouswords, supertags and state sequences up tothe current position.
This represents a CCGoperator tagger.The different local conditional components (forevery i) in (3) are estimated as discriminativeMaxEnt submodels trained on a corpus of incre-mental CCG derivations.
This corpus was ex-tracted from the CCGbank (Hockenmaier, 2003)1185by transforming every normal form derivation intostrictly left-to-right CCG derivations, with theCCG operators only slightly redesigned to allowincrementality while still satisfying the dependen-cies in the CCGbank (cf.
(Hassan et al, 2008b;Hassan et al, 2009)).As mentioned before, the State-Realizer is adeterministic function.
Starting at the first wordwith (obviously) a null previous state, the realizerperforms the following deterministic steps foreach word in turn: (i) set the current supertagand operator to those of the current word; (ii) atthe current state, apply the current operator to theprevious state and current supertag; (iii) add edgesto the dependency graphs between words that arelinked as CCG arguments; and (iv) if not at theend of the sentence, set the previous state to thecurrent one, then set the current word to the nextone, and iterate from (i).It is worth noting that the proposed dependencyparser is deterministic in the sense that it maintainsonly one parse state per word.
This characteris-tic is crucial for its incorporation into a large-scaleSMT system to avoid explosion of the translationspace during decoding.5 Dependency-based DTM (DDTM)In this section we extend the DTM2 model withincremental target dependency-based syntax.
Wecall the resulting model the Dependency-based Di-rect Translation Model (DDTM).
This extensiontakes place by (i) extracting syntactically enrichedminimal phrase pairs, (ii) including a new set ofsyntactic feature functions among the exponen-tial model features, and (iii) adapting the decoderfor dealing with syntax, including various pruningstrategies and enhancements.
Next we describeeach extension in turn.5.1 Phrase Table: Incremental SyntaxThe target-side sentences in the word-aligned par-allel corpus used for training are parsed usingthe incremental dependency parser described insection 4.
This results in a word-aligned par-allel corpus where the words of the target sen-tences are tagged with supertags and operators.From this corpus we extract the set of minimalphrase pairs using the method described in (Itty-cheriah and Roukos, 2007), extracting along withevery target phrase the associated sequences of su-pertags and operators.
As shown in (4), a sourcephrase s1, .
.
.
, sntranslates into a target phrasew1, .
.
.
, wmwhere every word wiis labeled witha supertag sti, and a possible parsing operator oiappearing with it in the parsed parallel corpus:s1...sn//[w1, st1, o1]...[wm, stm, om] (4)Hence, our phrase table associates with everytarget phrase an incremental parsing subgraph.These subgraphs along with their probabilitiesrepresent our phrase table augmented with incre-mental dependency parsing structure.This representation turns the complicated prob-lem of MT with incremental parsing into a sequen-tial classification problem in which the classifierdeploys various features from the source sentenceand the candidate target translations to specify asequence of decisions that finally results in an out-put target string along with its associated depen-dency graph.
The classification decisions are per-formed in sequence step-by-step while traversingthe input string to provide decisions on possiblewords, supertags, operators and states.
A beamsearch decoder simultaneously decides which se-quence is the most probable.5.2 DDTM FeaturesThe exponential model and the MaxEnt frame-work used in DTM2 and DDTM enabled us to ex-plore the utility of incremental syntactic parsingwithin a rich feature space.
In our DDTM sys-tem, we implemented a set of features alongsidethe baseline DTM2 features that were discussed inSection 3.
The features described here encode allthe probabilistic components in (3) within a loglinear interpretation along with some more empir-ically intuitive features.?
Supertag-Word features: these features ex-amine the target phrase words with their as-sociated supertags and is related to the Su-pertagger component in (3).?
Supertag sequence features: these featuresencode n-gram supertags (equivalent to the n-gram supertags Language Model).
This fea-ture is related to the supertagger componentas well.?
Supertag-Operator features: these featuresencode supertags and associated operatorswhich is related to the Operator Tagger com-ponent in (3).1186?
Supertag-State features: these features regis-ter state and supertag co-occurrences.?
State sequence features: these features en-code n-gram state features and are equiva-lent to an n-gram Language Model over parsestate sequences which is related to the multi-plication in (3).?
Word-State sequence features: these fea-tures encode words and states co-occurrenceswhich is related to the Word Predictor com-ponent in (3).The exponential model and the MaxEnt frame-work used in DTM2 and DDTM enable us to ex-plore the utility of incremental syntactic parsingwith the use of minimal phrases within a rich fea-ture space.5.3 DDTM DecoderIn order to support incremental dependency pars-ing, we extend the DTM2 decoder in three ways:firstly, by constructing the syntactic states duringdecoding; secondly, by extending the hypothesisstructures to incorporate the syntactic states andthe partial dependency derivations; and thirdly, bymodifying the pruning strategy to handle the largesearch space.At decoding time, each hypothesis state is as-sociated with a parse-state which is constructedwhile decoding using the incremental parsing ap-proach introduced in ((Hassan et al, 2008b; Has-san et al, 2009)).
The previous state, the se-quences of supertags and CCG incremental opera-tors are deployed in a deterministic manner to re-alize the parse-states as well as the intermediatedependency graphs between words.Figure 2 shows the DDTM decoder while de-coding a sentence with the English translation ?At-tacks rocked Riyadh?.
Each hypothesis is asso-ciated with a parse-state Siand a partial depen-dency graph (shown for some states only).
More-over, each transition is associated with an opera-tor o that combines the previous state and the cur-rent supertag st to construct the next state Si.
Thedecoder starts from a null state S1and then pro-ceeds with a possible expansion with the word ?at-tacks?, supertag NP and operator NOP to pro-duce the next hypothesis with state S2and cate-gory NP .
Further expansion for that path with theverb ?rocked?, supertag ?
(S\NP )/NP and oper-ator TRFC will produce the state S5with cat-egory S/NP .
The partial dependency graph forstate S5is shown above the state where a depen-dency relation between the two words is estab-lished.
Furthermore, another expansion with theword ?Riyadh?, supertag NP and operator FAproduces state S7with category S and a completeddependency graph as shown above the state.
An-other path which spans the states S1, S3, S6andS8ends with a state category S/NP and a partialdependency graph as shown under state S8wherethe dependency graph is still missing its object(e.g.
?Riyadh attacks rocked the Saudi Govt.?
).The addition of parse-states may result in a verylarge search space due to the fact that the samephrase/word may have many possible supertagsand many possible operators.
Moreover, the sameword sequences may have many parse-state se-quences and, therefore, many hypotheses that rep-resent the same word sequence.
The search spaceis definitely larger than the baseline search space.We adopt the following three pruning heuristics tolimit the search space.5.3.1 Grammatical PruningAny hypothesis which does not constitute a validparse-state is discarded, i.e.
if the previous parse-state and the current supertag sequence cannotconstruct a valid state using the associated oper-ator sequence, then the expansion is discarded.Therefore, this pruning strategy maintains onlyfully connected graphs and discards any partiallyconnected graphs that might result during the de-coding process.As shown in Figure 2, the expansion from stateS1to state S4(with the dotted line) is pruned andnot expanded further because the proposed expan-sion is the verb ?attacks?, supertag (S\NP )/NPand operator TRFC .
Since the previous state isNULL, it cannot be combined with the verb usingthe TRFC operator.
This would produce an un-defined state and thus the hypothesis is discarded.5.3.2 Supertags and Operators ThresholdWe limit the supertag and operator variants per tar-get phrase to a predefined number of alternatives.We tuned this on the MT03 DevSet for the bestaccuracy while maintaining a manageable searchspace.
The supertags limit was set to four alterna-tives while the operators limit was set to three.As shown in Figure 2, each word can have manyalternatives with different supertags.
In this exam-ple the word ?attacks?
has two forms, namely a1187e:a : --------P:1S1:NULLe: attacksa: *----P:=.162ST=NPS2=NPe: attacksa: *-------P:=.092ST=(S\NP)/NPS4= UNDEFO:TRFCe: Riyadha: -*------P:=.142ST=NP/NPS3=NP/NPe: rockeda: --*--P:=.083ST=(S\NP)/NPS5=S/NPO:NOPO:NOPO:TRFCe: rockeda: --*------P:=.01ST=(S\NP)/NPS8=S/NPattacksattacks rockede: Riyadha: --*--P:=.04ST=NPS7=SO:FCattacks rocked Riyadhe: attacksa: *-------P:=.07ST=NPS6=NPO:TRFCRiyadh attacks rockedO:FAFigure 2: DDTM Decoder: each hypothesis has a parse state and a partial dependency structure.noun and a verb, with different supertags and op-erators.
The proposed thresholds limit the possiblealternatives to a reasonable number.5.3.3 Merging HypothesesStandard Phrase?based SMT decoders mergetranslation hypotheses if they cover the samesource words and share the same n-gram lan-guage model history.
Similarly, DDTM decodermerges translation hypotheses if they cover thesame source words, share the same n-gram lan-guage model history and share the same parse-state history.
This helps in reducing the searchspace by merging paths that will not constitute apart of the best path.6 ExperimentsWe conducted experiments on an Arabic-to-English translation task using LDC parallel dataand GALE parallel data.
We used the UN paral-lel corpus and LDC news corpus together with theGALE parallel corpus, totaling 7.8M parallel sen-tences.
The 5-gram Language Model was trainedon the English Gigaword Corpus and the Englishpart of the parallel corpus.
Our baseline system issimilar to the system described in (Ittycheriah andRoukos, 2007).
We report results on NIST MT05and NIST MT06 evaluations test sets using BLEUand TER as automatic evaluation metrics.To train the DDTM model, we use the incre-mental parser introduced in (Hassan et al, 2008b;Hassan et al, 2009) to parse the target side of theparallel training data.
Each sentence is associatedwith supertag, operator and parse-state sequences.We then train models with different feature sets.Results: We compared the baseline DTM2 (It-tycheriah and Roukos, 2007) with our DDTM sys-tem with the features listed above.
We examinethe effect of all features on system performance.In this set of experiments we used LDC paralleldata only which is composed of 3.7M sentencesand the results are reported on MT05 test set.
Eachof the examined systems deploys DTM2 featuresin addition to a number of newly added syntacticfeatures.
The systems examined are:?
DTM2: Direct Translation model 2 baseline.?
D-SW: DTM2 + Supertag-Word features.?
D-SLM: DTM2 + Supertag-Word and su-pertag n-gram features.?
D-SO: DTM2+ Supertag-Operator features.?
D-SS : DTM2 + supertags and states featureswith parse-state construction.?
D-WS : DTM2 + words and states featureswith parse-state construction.?
D-STLM: DTM2 + state n-gram featureswith parse-state construction.?
DDTM: fully fledged system with all fea-tures that proved useful above which are:Supertag-Word features, supertag n-gram1188features, supertags and states features andstate n-gram features .System BLEU Score on MT05DTM2-Baseline 52.24D-SW 52.28D-SLM 52.29D-SO 52.01D-SS 52.39D-WS 52.03D-STLM 52.53DDTM 52.61Table 1: DDTM Results with various features.As shown in Table 1, the DTM baseline systemdemonstrates a very high BLEU score, unsurpris-ingly given its top-ranked performance in two re-cent major MT evaluation campaigns.
Among thefeatures we tried, supertags and n-gram supertagssystems (D-SW and D-SLM systems) give slightyet statistically insignificant improvements.
Onthe other hand, the states n-gram sequence features(D-SS and DDTM systems) give small yet statis-tically significant improvements (as calculated viabootstrap resampling (Koehn, 2004b)).
The D-WSsystem shows a small degradation in performance,probably due to the fact that the states-words inter-actions are quite sparse and could not be estimatedwith good evidence.
Similarly, the D-SO systemshows a small degradation in performance.
Whenwe investigated the features types, we found outthat all features that deploy the operators had badeffect on the model.
We think this is due to the factthat the operator set is a small set with high evi-dence in many training instances such that it haslow discriminative power on it is own.
However,it implicitly helps in producing the state sequencewhich proved useful.System DTM2-Baseline DDTMMT05 (BLEU) 55.28 55.66MT05 (TER) 38.79 38.48MT06 (BLEU) 43.56 43.91MT06 (TER) 49.08 48.65Table 2: DDTM Results on MT05 and MT06.We examined a combination of the best fea-tures in our DDTM system on a larger trainingdata comprising 7.8M sentences from both NISTand GALE parallel corpora.
Table 2 shows theresults on both MT05 and MT06 test sets.
Asshown, DDTM significantly outperforms the state-of-the-art baseline system.
It is worth noting thatDDTM outperforms this baseline even when verylarge amounts of training data are used.
Despitethe fact that the actual scores are not so different,we found that the baseline translation output andthe DDTM translation outout are significantly dif-ferent.
We measured this by calculating the TERbetween the baseline translation and the DDTMtranslation for the MT05 test set, and found thisto be 25.9%.
This large difference has not beenrealized by the BLEU or TER scores in compari-son to the baseline.
We believe that this is due tothe fact that most changes that match the syntac-tic constraints do not bring about the best matchwhere the automatic evaluation metrics are con-cerned.
Accordingly, in the next section we de-scribe the outcome of a detailed manual analysisof the output translations.7 Manual Analysis of ResultsAlthough the BLEU score does not mark a largeimprovement by the dependency-based systemover the baseline system, human inspection of thedata gives us important insights into the pros andcons of the dependency-based model.
We ana-lyzed a randomly selected set of 100 sentencesfrom the MT05 test set.
In this sample, the base-line and the DDTM system perform similarly in68% of the sentences.
The outputs of both systemare similar though not identical.
In these cases,the systems may choose equivalent paraphrases.However, the translations using syntactic struc-tures are rather similar.
It is worth noting that theDDTM system tends to produce more concise sys-ntactic structures which may lead to less BLUEscore due to penalizing the translation length al-though the translation might be equivelent to thebaseline if not better.In 28% of the sentences, the DDTM system pro-duces remarkably better translations.
The exam-ples here illustrate the behaviour of the baselineand the DDTM systems which can be observedconsistently throughout the test set.
We only high-light some of the examples for illustration pur-poses.
DDTM manages to insert verbs which aredeleted by any standard phrase-based SMT sys-tem.
DDTM prefers to deploy verbs since theyhave complex and more detailed syntactic struc-tures which give better and more likely state se-1189quences.
Furthermore, the DDTM system avoidslonger noun phrases and instead uses some prepo-sitions in-between.
Again, this is probably due tothe fact that like verbs, prepositions have a com-plex syntactic description that give rise to morelikely state sequences.On the other hand, the baseline produced bettertranslation in 8% of the analysis sample.
We ob-served that the baseline is doing better mainly intwo cases.
The first when the produced translationis very poor and producing poor sysntatctic struc-ture due to out of vocabularies or hard to trans-late sentences.
The second case is with sentenceswith long noun phrases, in such cases the DDTMsystem prefres to introduce verbs or prepositionsin the middle of long noun phrase and thus thebaseline would produce better translations.
Thisis maybe due to the fact that noun phrases haverelatively simple structure in CCG such that it didnot help in constructing long noun phrases.Source: ??Q???
 ZAJ.
? Yg A?Qk.HA??j??
?
?X Y?K.?
?k?Reference: He then underwent medical examinations by a po-lice doctor .Baseline: He was subjected after that tests conducted by doc-tors of the police .DDTM: Then he underwent tests conducted by doctors of thepolice .Source: 	?KPA J?.?A??j.?
?J?
 ZA??
?AKQ ?Q?
Y???Jjj?
?Reference: Riyadh was rocked tonight by two car bomb at-tacks..Baseline: Riyadh rocked today night attacks by two booby -trapped cars.DDTM: Attacks rocked Riyadh today evening in two carbombs.Figure 3: DDTM provides better syntactic struc-ture with more concise translations.Figure 3 shows two examples where DDTMprovides better and more concise syntactic struc-ture.
As we can see, there is not much agree-ment between the reference and the proposedtranslation.
However, longer translations enhancethe possibility of picking more common n-grammatches via the BLEU score and so increases thechance of better scores.
This well-known biasdoes not favour the more concise output derivedby our DDTM system, of course.8 Conclusion and Future WorkIn this paper, we presented a novel model of de-pendency phrase-based SMT which integrates in-cremental dependency parsing into the transla-tion model while retaining the linear decoding as-sumed in conventional Phrase?based SMT sys-tems.
To the best of our knowledge, this modelconstitutes the first effective attempt at integratinga linear-time dependency parser that builds a con-nected tree incrementally into SMT systems withlinear-time decoding.
Crucially, it turns out thatincremental dependency parsing based on lexical-ized grammars such as CCG and LTAG can pro-vide valuable incremental parsing information tothe decoder even if their output is imperfect.
Webelieve this robustness in the face of imperfectparser output to be a property of the probabilisticformulation and statistical estimation used in theDirect Translation Model.
A noteworthy aspect ofour proposed approach is that it integrates featuresfrom the derivation process as well as the derivedtree.
We think that this is possible due to the im-portance of the notion of a derivation in linguisticframeworks such as CCG and LTAG.Future work will attempt further extensions ofour DDTM system to allow for the exploitationof long-range aspects of the dependency struc-ture.
We will work on expanding the featuresset of DDTM system to leverage features fromthe constructed dependency structure itself.
Fi-nally, we will work on enabling the deploymentof source side dependency structures to influencethe construction of the target dependency structurebased on a bilingually enabled dependency pars-ing mechanism using the discriminative modelingcapabilities.AcknowledgmentsWe would like to thank Salim Roukos, IBM TJWatson Research Center, for fruitful, insightfuldiscussions and for his support during this work.We also would like to thank Abe Ittycheriah, IBMTJ Watson Research Center, for providing theDTM2 baseline and for his support during de-veloping this system.
Finally, we would like tothank the anonymous reviewers for their helpfuland constructive comments.1190ReferencesBangalore, S. and Joshi, A.
(1999).
?Supertagging: AnApproach to Almost Parsing?, Computational Lin-guistics 25(2):237?265, 1999.Berger, A. and Della Pietra, S. and Della Pietra, V.J.(1996).
Maximum Entropy Approach to NaturalLanguage Processing Computational Linguistics,22(1): 39?71, 1996.Birch, A., Osborne, M. and Koehn, P. (2007).
CCG Su-pertags in Factored Statistical Machine Translation.In Proceedings of the Second Workshop on Statisti-cal Machine Translation, ACL-07, pp.9?16, 2007.Brown, P., Cocke,J., Della Pietra, S., Jelinek, F., DellaPietra, V.J.
Lafferty, R. Mercer and Roossin, P. ?AStatistical Approach to Machine Translation?
Com-putational Linguistics 16(2):79?85, 1990.Chelba, C. (2000).
Exploiting Syntactic Structure forNatural Language Modeling.
PhD thesis, JohnsHopkins University, Baltimore, MD.Chiang, D. (2005).
A Hierarchical Phrase-BasedModel for Statistical Machine Translation.
In 43rdAnnual Meeting of the Association for Computa-tional Linguistics (ACL05), pp.263?270, Ann Arbor,MI.Hassan, H., Sima?an, K., and Way, A.. (2009).
Lex-icalized Semi-Incremental Dependency Parsing.
InProceedings of RANLP 2009, the International Con-ference on Recent Advances in Natural LanguageProcessing, Borovets, Bulgaria (to appear).Hassan, H., Sima?an, K., and Way, A.
(2008a).
Syntac-tically Lexicalized Phrase-Based Statistical Transla-tion.
IEEE Transactions on Audio, Speech and Lan-guage Processing, 6(7):1260?1273.Hassan, H., Sima?an, K., and Way, A.. (2008b).
A Syn-tactic Language Model Based on Incremental CCGParsing.
In Proceedings IEEE Workshop on SpokenLanguage Technology (SLT) 2008, Goa, India.Hassan, H., Sima?an, K., and Way, A.
(2007).
Inte-grating Supertags into Phrase-based Statistical Ma-chine Translation.
In Proceedings of the ACL-2007,Prague, Czech Republic, pp.288?295, 2007.Hockenmaier, J.
(2003).
Data and Models for Statisti-cal Parsing with Combinatory Categorial Grammar,Ph.D Thesis, University of Edinburgh, UK, 2003.Huang, L. and Chiang, D. (2007).
Forest Rescoring:Faster Decoding with Integrated Language Models.In Proceedings of the ACL-2007, Prague, Czech Re-public, 2007.Ittycheriah, A. and Roukos, S. (2007).
Direct trans-lation model 2.
In Human Language Technologies2007: The Conference of the North American Chap-ter of the Association for Computational Linguis-tics; Proceedings of the Main Conference, pp.57?64,Rochester, NY.Koehn, P. (2004a).
Pharaoh: A Beam Search De-coder for phrase-based Statistical Machine Transla-tion Models.
Machine Translation: From Real Usersto Research.
In Proceedings of 6th Conference of theAssociation for Machine Translation in the Ameri-cas, AMTA 2004, pp.115?124, Washington, DC.Koehn, P. (2004b).
Statistical Significance Tests forMachine Translation Evaluation.
In Proceedings ofthe 2004 Conference on Empirical Methods in Nat-ural Language Processing (EMNLP), pp.388?395,Edmonton, AB, Canada.Koehn, P. Och, F.J. and Marcu, D. (2003).
Statisti-cal phrase-based translation.
In Proceedings of theJoint Human Language Technology Conference andthe Annual Meeting of the North American Chap-ter of the Association for Computational Linguistics(HLT-NAACL 2003), pp.127?133, Edmonton, AL,Canada.Marcu, D., Wang, W., Echihabi, A., and Knight, K.(2006).
SPMT: Statistical Machine Translation withSyntactified Target Language Phrases.
In Proceed-ings of the 2006 Conference on Empirical Methodsin Natural Language Processing (EMNLP 2006),pp.44?52, Sydney, Australia.Papineni, K., Roukos, S., and Ward, T. (1997).Feature-Based Language Understanding.
In Pro-ceedings of 5th European Conference on SpeechCommunication and Technology EUROSPEECH?97 , pp.1435?1438, Rhodes, Greece.Papineni, K., Roukos, S., Ward, T. and Zhu, W-J.(2002).
BLEU: a Method for Automatic Evalua-tion of Machine Translation.
In 40th Annual Meet-ing of the Association for Computational Linguistics(ACL?02), pp.311?318, Philadelphia, PA.Shen, L., Xu, J., and Weischedel, R. (2008).
A newstring-to-dependency machine translation algorithmwith a target dependency language model.
In Pro-ceedings of ACL-08: HLT, pp.577?585, Columbus,OH.Snover, M., Dorr, B., Schwartz, R., Micciulla, L. andMakhoul, J.
(2006) A Study of Translation Edit Ratewith Targeted Human Annotation.
In AMTA 2006:Proceedings of the 7th Conference of the Associationfor Machine Translation in the Americas, pp.223?231, Cambridege, MA.Steedman, M. (2000).
The Syntactic Process.
MITPress, Cambridge, MA.Tillmann, C. and Ney, H. (2003).
Word reordering anda dynamic programming beam search algorithm forstatistical machine translation.
Computational Lin-guistics, 29(1):97?133.Zollmann, A. and Venugopal, A.
(2006).
Syntax aug-mented machine translation via chart parsing.
InProceedings of the Workshop on Statistical MachineTranslation, HLT/NAACL, pp.138?141, New York,NY.1191
