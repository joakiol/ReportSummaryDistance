Elements of a Computational Model ofCooperative Response Generation*Brant  A. Che ikes  and  Bonn ie  L. WebberUn ivers i ty  of  Pennsy lvan iaDepar tment  of  Computer  and  In fo rmat ion  Sc iencePh i lade lph ia ,  PA  19104-6389brant@l inc .c i s .upenn.edubonn ie@cent ra l .
c i s .upenn.eduMarch 9, 1989AbstractIf natural language question-answering (NLQA) systems are to be truly effective and useful, they mustrespond to queries cooperatively, recognizing and accommodating in their replies a questioner's goals,plans, and needs.
This paper concerns the design of cooperative response generation (CRG) systems,NLQA systems that are able to produce integrated cooperative r sponses.
We propose two characteristicsof a computational model of cooperative r sponse generation.
First, we argue that CRG systems houldbe able to explicitly reason about and choose among the different response options available to them in agiven situation.
Second, we suggest that some choices of response content motivate others--that througha process called reflection, respondents detect he need to explain, justify, clarify or otherwise augmentinformation they have already decided to convey.1 In t roduct ionOur success in day-to-day affairs depends to a great extent on the cooperation of those with whom weinteract.
Studies of man-machine interaction show that we expect the same from the (complex) systemswe deal with \[14\].
Here we consider the case of natural language question-answering (NLQA) systems.
Tobe cooperative, any system must recognize and accommodate he goals, plans, and needs of its users.
ForNLQA systems, this means that they must take the initiative when responding, rather than answering queriespassively.
They cannot simply translate input queries into transactions on database or expert systems--theyhave to apply many more complex reasoning mechanisms to the task of deciding how to respond.
It has beensuggested that cooperative NLQA systems must be able to provide extended responses \[15, 16\], combiningsuch elements as:?
a direct answer;?
information or action that is pertinent o the direct answer;?
information or action pertinent o one or more of the questioner's stated or inferred goals.Cooperative behavior in natural language has been studied by many researchers.
At the University ofPennsylvania lone, several projects have focused on different aspects of cooperative response production,*This research was partially supported by DARPA grant N0014-85-K0018.216including Kaplan's work on responding when presuppositions fail \[7\], Mays' work both on responding whenqueries fail intensionally \[8\] and on determining competent monitor offers \[9\], McKeown's TEXT systemfor explaining concepts known to a database system \[11\], McCoy's system for correcting object-relatedmisconceptions \[10\], Hirschberg's work on scalar implicatures and their use in avoiding the production ofmisleading responses \[5\], and Pollack's plan inference model for recognizing and responding to discrepanciesbetween the system's and the user's beliefs about domain plans and actions \[12\].
Other explorations ofcooperative communication i clude \[1\], \[2\], \[4\], \[6\], \[13\], \[15\], and \[17\].
For more complete references, thereader is referred to Cheikes \[3\].The results of these studies have been highly informative.
Many different kinds of cooperative behaviorhave seen identified, and computational models of them proposed.
What is of interest o us here is the factthat all efforts to date in this area share the same implicit assumption--that cooperative r sponse generationcan be decomposed into separate reasoning processes.
But this "decomposability assumption" in turn raisesthe inlegralion problem--the problem of getting those elements to work together in the production of a singleresponse.
This so far has largely has been ignored.Solving the integration problem means devising an architecture for cooperative response generalion (CRG)systems--NLQA systems that can combine in their responses instances of different kinds of cooperativebehavior appropriate to the situation.
Now that the study of cooperativeness in natural anguage is beyondits infancy (although still far from mature), it is an appropriate time to confront he integration problemand study the design of CRG systems.
This paper describes the beginnings of such a computational modelof CRG.2 Toward a Mode l  of Cooperat ive Response Generat ionWhat characteristics of the CRG process might be used to motivate the design of a CRG system?
Analysis ofvarious transcripts of natural anguage question-answer dialogues leads us to propose two: (1) CRG systemsshould be able to explicitly reason about and choose among the different response options available to themin a given situation, and (2) CRG systems hould be able to reflect on their selections of response contentbefore producing any output.
Some choices of response content motivate others, and the process of reflectionallows respondents o detect a need to explain, justify, or clarify information they have already decided toconvey.Before continuing, we should explain a few terms.
A response option describes a specific communicativeact that might be performed as part of a response.
Given a question like "is P true," one response optionmight be "inform the questioner of P's truth value."
A respondent might choose to exercise a response option,meaning that she decides to perform the act described by the response option in her response.
Exercising theoption "inform the questioner of P's truth value" might lead to the appearance of "P is true" in a naturallanguage response, assuming P were in fact true.
Thus, response options are goals that a respondent wantsto achieve in her response.
The actual response is produced by forming and executing a response plan thatachieves the goals specified by the response options.The main arguments in this section will make use of two examples hown in Figure 1.
These exampleswere extracted from transcripts of electronic mail conversations between consultants and users of the Unix 1operating system at the University of California at Berkeley (henceforth the "Berkeley transcripts").2.1 Se lect ing  among mul t ip le  response  opt ionsWe begin by observing that the utterances comprising a query may license various response options, onlysome of which are exercised in any actual response.
In Example 1, Q2 has requested to be informed if anetwork link exists between his computer and ucb-euler.
There are several pieces of information which Rmight consider conveying in her response to this question.
She could certainly give the direct answer, telling1 Unix is a trademark ofAT&T.2Throughout this paper, 'Q' (or a masculine pronoun) will denote the questioner, 'R' (or a feminine pronoun) the respondent.217Example 1Q: Do you know if there is any network link to ucb-euler?
I want to send some mail overthere.R: Gary, just do this: mail ruby!euler!
(login name).
Let us know if it doesn't work.
Euleris only reached thru the ruby machine.Example 2Q: Is there a way to send mail to ucb-euler from ucb-cory?R: Yes, it is letter y on the Berknet.
So mail user@y.CC.
If you have further problemswith it, mail to serge@cory.
He is the euler system manager.Figure 1: Two Cooperative ExchangesQ that there is in fact a network link between the two machines.
In an effort to be helpful, she might tell Qhow many such links there are, assuming a count of this kind is meaningful.
Or she might tell Q what kindof links there are, e.g., a high-speed ethernet link, a low-speed phonenet connection, and so forth.
Recast asresponse options, R might identify her options as "inform Q that a network link exists," "inform Q of thecount of network links," and "inform Q of the type of each network link.
"R's possible response options follow from general principles of interaction and reasoning, based on beliefsabout Q's goals and intended actions.
On general principle, for example, queries that ask "whether P" canalways be answered by informing Q of P's truth value.
In response to "is there a P," one might includea count of the Ps if there is more than one--"over-answering" the question, in the sense of Wahlster \[15\].Reasoning on her beliefs about Q's plan, R may be able to identify potential obstacles \[1\] that can beeliminated by providing Q with some information.In Example 1, R does not produce all possible response options.
In fact, she does not even explicitly givethe direct answer.
The point is that the direct answer can be deduced from the response: there clearly issome network connection between the machines, at least one that permits the transmittal of mail messages.So either R has decided to convey the direct answer implicitly, or she has decided to leave it out, with thedirect answer being implied purely by happenstance.We take this as evidence that R considered her plausible response options and decided which to includein the final response.
In terms of a computational model, R first identifies the available response optionsand then decides which ones to actually exercise.There are different bases for rejecting (deciding not to exercise) a response option.
For example, exercisingone response option may make another unnecessary--replying "there are three Ps" when asked "is there aP" makes the direct answer of "yes" unnecessary.
Alternatively, one response option may, in the givencircumstance, be considered more important han another: correcting a misconception evident from Q'squery may be more important than answering his question.
Finally, a respondent may have to rejectresponse options simply to avoid a lengthy reply.The Berkeley transcripts contain many examples of open-ended queries in which Q primarily describeshis goal, leaving it up to R to decide how to respond:Q: I 'm using a hardcopy printing terminal on a dialup line.
In ex 3 I use the space barto indent lines in a program I 'm entering.
After I indent on one line, the next lineautomatically indents the same number of spaces, but I don't want it to.
I got out ofex and then went back in and the indent was still set where it had been.
Logging outremoved the automatic indenting, but that's a hard way to work!
Any suggestions?3In Unix, "ex" refers to a line-oriented text editor.218Q's query ("any suggestions?")
is a general request for help with the described problem.
Here is anothersituation in which, in choosing her response, P~ may have to weigh different options.
For example, she mightrecommend that Q switch to a different editor, or if automatic indentation indicates an improperly configuredterminal, R might want to point that out.Based on these arguments, we propose that a rating/selection phase be part of a computational model ofcooperative response generation.
The system should first collect all the response options that it deems arepotentially relevant and useful.
These options are then rated according to the kinds of criteria we mentionedearlier.
The ratings then influence which options are chosen to be exercised in the system's cooperativeresponse.We want to emphasize that we believe rating/selection is a necessary attribute of CRG systems, not nec-essarily part of a model of human cooperative activity.
Given a question, NLQA systems are not constrainedas much as people are to begin responding immediately.
They should be more circumspect in their choiceof response options, and take more care to be brief yet informative, not to mislead, and to appropriatelyjustify their assertions or directives.
The rating/selection process enables CRG systems to be as cooperativeas they can and need to be.2.2 Ref lect ing on earlier decisionsWe base our argument for including reflection in a computational model of CRG on Example 2 (Figure 1).Notice that 1~ asserts:He \[serge@cory\] is the euler system manager.What might have motivated R to make this statement?
There doesn't seem to be anything in Q's request,explicit or implicit, that suggests a need to be told anything about serge@cory or about euler's systemmanager.
To account for this phenomenon, we start by examining the immediately preceding statement:If you have further problems with it, mail to serge@cory.Why might R have included this statement in her response?
Since we are trying to develop a computationalmodel of R's response, a better question is: what process could explain why R made the above statementpart of her response?Our analysis goes as follows: first, we can assume that R inferred Q's goal of sending mail to ucb-eulerbecause the first part of her response helps Q reach that goal, by informing him of the correct electronic mailaddress yntax.
That is, she chose to exercise the response option "inform Q how to send mail to ucb-euler.
"R's next statement, "if you have further problems... ," is a further attempt o help Q send mail successfullyto euler.
Several explanations for its appearance exist, including:1.
R believes that Q may still have problems (not involving incorrect address yntax) sending mail toeuler, and therefore needs to know of a better way (better than asking R) to handle those problems.2.
R is unsure that user@y.CC is in fact the correct address.
However, she knows that serge@cory definitelyknows the right way to send mail, so she points Q to him.3.
As a matter of policy, 1~ routinely directs users to those people with the most expertise in the givenproblem area.All explanations suggest hat R's second utterance is still part of her effort to help Q reach his goal.
Sherecognizes his goal, identifies what she takes to be his mistake (he was using the wrong address syntax),corrects his mistake~ and then, allowing that other difficulties may arise, she points him to a better sourceof information.
The presence of her third (last) utterance, though, seems to have a different explanation.Looking at its effect on Q, "serge@cory is the euler system manager" explains R's second statement whichmentions erge@cory for the first time.
That is, having decided to refer to serge@cory, R realizes that sheshould also explain who he is.219This process we call reflection, to capture the idea that after selecting an initial set of response optionsto exercise, a respondent "reviews" or "reflects" on those choices and may be able to identify new responseoptions that are suddenly relevant, relevant by dint of the information to be conveyed by response optionsalready chosen.
So some response options are chosen because they address the questioner's goals, plans andneeds, while other response options are selected to justify, clarify, or explain options that the respondent hasalready decided to exercise.Through reflection, a respondent also may decide to generate a new plan for the questioner and commu-nicate the important details to him.
Consider this example:Q: The Linc Lab laserwriter is down again.
What's Ira's office phone number?R: Ira's not in his office.
Call Dawn at extension 3191.
She may be able to fix theproblem, or can page Ira if necessary.After inferring Q's plan, R evMuates 4 it and discovers an error: Q believes that by calling Ira's office, he willcontact Ira.
R then decides to point that error out.
Upon reflection, R notices that she has not helped Qreach his goal, so she decides to try to find another plan that Q could execute in order to get the Linc Lablaserwriter fixed.
She finds such a plan, and decides to communicate the important details--that Q shouldcontact Dawn at extension 3191.Note that the example also shows that reflection make occur over several cycles before a final response isdetermined.
R's final statement in the example xplains to Q why he should call Dawn--it explains the planthat R has decided to communicate.
Computationally, reflecting upon her decision to tell Q to call Dawn,R decides that a justification of that plan is necessary.Reflection is an important part of the cooperative response generation process.
It allows CRG systemsnot only to explain, justify, or clarify their statements, but also allows them to perform other kinds of helpfulactions such as suggesting new plans.3 Concluding RemarksWe have proposed two characteristics of a computational model of cooperative r sponse generation.
The ideasdescribed in this paper are embodied in a prototype CRG system that is being designed and implementedat the University of Pennsylvania.
For more details on the motivation for and architecture of that system,see Cheikes \[3\].References\[1\] James F. Allen.
Recognizing intentions from natural language utterances.
In Michael Brady andRobert C. Berwick, editors, Computational Models of Discourse, pages 107-166, The MIT Press, Cam-bridge, MA, 1983.\[2\] Sandra Carberry.
Plan recognition and its use in understanding dialogue.
In Alfred Kobsa and WolfgangWahlster, editors, User Models in Dialog Systems, Springer Verlag, Berlin-New York, 1988.\[3\] Brant A. Cheikes.
The Architecture of a Cooperative Respondent.
Technical Report MS-CIS-89-13,Department of Computer and Information Science, University of Pennsylvania, 1989.\[4\] Annie Gal.
A natural anguage database interface that provides cooperative answers.
In Proceedings ofthe Second Conference on Artificial Intelligence Applications, pages 352-357, 1985.\[5\] Julia B. Hirschberg.
A Theory of Scalar Implicature.
PhD thesis, Department of Computer and Infor-mation Science, University of Pennsylvania, December 1985.4 Plan evaluation is discussed at length in Cheikes \[3\].
Briefly, plan evaluation isthe process by which a respondent identifieserrors in an inferred plan.220\[6\] Aravind K. Joshi, Bonnie L. Webber, and Ralph Weischedel.
Living up to expectations: computingexpert responses.
In Proceedings of the 4 th National Conference on Artificial Intelligence, August 1984.\[7\] S. Jerrold Kaplan.
Cooperative responses from a portable natural language database query system.
InMichael Brady, editor, Computational Models of Discourse, The MIT Press, Cambridge, MA, 1982.\[8\] Eric Mays.
Failures in natural language systems: application to data base query systems.
In Proceedingsof the 1 st National Conference on Artificial Intelligence, Stanford, August 1980.\[9\] Eric Mays.
A temporal logic for reasoning about changing data bases in the context of natural languagequestion-answering.
In Lawrence Kerschberg, editor, Expert Database Systems, Benjamin Cummings,New York, 1985.\[1O\] Kathleen F. McCoy.
Correcting Object-Related Misconceptions.
PhD thesis, Department of Computerand Information Science, University of Pennsylvania, December 1985.\[11\] Kathleen R. McKeown.
Text Generation: Using Discourse Strategies and Focus Constraints to GenerateNatural Language Text.
Cambridge University Press, Cambridge, 1985.\[12\] Martha E. Pollack.
A model of plan inference that distinguishes between actor's and observer's beliefs.In Proceedings of the 24 th Annual Meeting of the Association for Computational Linguistics, New York,June 1986.\[13\] Alex Quilici, Michael Dyer, and Margot Flowers.
Detecting and responding to plan-oriented miscon-ceptions.
In Alfred Kobsa and Wolfgang Wahlster, editors, User Models in Dialog Systems, SpringerVerlag, Berlin-New York, 1988.\[14\] Lucy A. Suchman.
Plans and Situated Actions.
Cambridge University Press, Cambridge, 1987.\[15\] Wolfgang Wahlster, Heinz Marburger, Anthony Jameson, and Stephan Busemann.
Over-answeringyes-no questions: extended responses in a NL interface to a vision system.
In Proceedings of the 8 thInternational Conference on Artificial Intelligence, pages 643-646, Karlsruhe, August 1983.\[16\] Bonnie L. Webber.
Questions, answers, and responses: a guide for knowledge based systems.
In M.Brodie and J. Mylopoulos, editors, On Knowledge Base Systems, Springer-Verlag, Amsterdam, 1986.\[17\] Robert Wilensky, James Mayfield, Anthony Albert, David Chin, Charles Cox, Marc Luria, JamesMartin, and Dekai Wu.
UC: A Progress Report.
Technical Report UCB/CSD 87/303, ComputerScience Division, University of California, Berkeley, July 1986.221
