2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 538?542,Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational LinguisticsAutomatic Parallel Fragment Extraction from Noisy DataJason Riesa and Daniel MarcuInformation Sciences InstituteViterbi School of EngineeringUniversity of Southern California{riesa, marcu}@isi.eduAbstractWe present a novel method to detect parallelfragments within noisy parallel corpora.
Isolat-ing these parallel fragments from the noisy datain which they are contained frees us from noisyalignments and stray links that can severelyconstrain translation-rule extraction.
We dothis with existing machinery, making use of anexisting word alignment model for this task.We evaluate the quality and utility of the ex-tracted data on large-scale Chinese-English andArabic-English translation tasks and show sig-nificant improvements over a state-of-the-artbaseline.1 IntroductionA decade ago, Banko and Brill (2001) showed thatscaling to very large corpora is game-changing for avariety of tasks.
Methods that work well in a small-data setting often lose their luster when moving tolarge data.
Conversely, other methods that seem toperform poorly in that same small-data setting, mayperform markedly differently when trained on largedata.Perhaps most importantly, Banko and Brillshowed that there was no significant variation in per-formance among a variety of methods trained at-scale with large training data.
The takeaway?
If youdesire to scale to large datasets, use a simple solutionfor your task, and throw in as much data as possible.The community at large has taken this message toheart, and in most cases it has been an effective wayto increase performance.Today, for machine translation, more data thanwhat we already have is getting harder and harderto come by; we require large parallel corpora toFigure 1: Example of a word alignment resulting fromnoisy parallel data.
The structure of the resulting align-ment makes it difficult to find and extract parallel frag-ments via the standard heuristics or simply by inspection.How can we discover automatically those parallel frag-ments hidden within such data?train state-of-the-art statistical, data-driven models.Groups that depend on clearinghouses like LDC fortheir data increasingly find that there is less of a man-date to gather parallel corpora on the scale of whatwas produced in the last 5-10 years.
Others, who di-rectly exploit the entire web to gather such data willnecessarily run up against a wall after all that datahas been collected.We need to learn how to do more with the datawe already have.
Previous work has focused ondetecting parallel documents and sentences on theweb, e.g.
(Zhao and Vogel, 2002; Fung and Che-ung, 2004; Wu and Fung, 2005).
Munteanu andMarcu (2006), and later Quirk et al (2007), extendthe state-of-the-art for this task to parallel fragments.In this paper, we present a novel method for de-tecting parallel fragments in large, existing and po-tentially noisy parallel corpora using existing ma-538chinery and show significant improvements to twostate-of-the-art MT systems.
We also depart fromprevious work in that we only consider parallel cor-pora that have previously been cleaned, sanitized,and thought to be non-noisy, e.g.
parallel corporaavailable from LDC.2 Detecting Noisy DataIn order to extract previously unextractable goodparallel data, we must first detect the bad data.
Indoing so, we will make use of existing machinery ina novel way.
We directly use the alignment model todetect weak or undesirable data for translation.2.1 Alignment Model as Noisy Data DetectorThe alignment model we use in our experiments isthat described in (Riesa et al, 2011), modified tooutput full derivation trees and model scores alongwith alignments.
Our reasons for using this particu-lar alignment method are twofold: it provides a natu-ral way to hierarchically partition subsentential seg-ments, and is also empirically quite accurate in mod-eling word alignments, in general.
This latter qualityis important, not solely for downstream translationquality, but also for the basis of our claims with re-spect to detecting noisy or unsuitable data:The alignment model we employ is discrimina-tively trained to know what good alignments be-tween parallel data look like.
When this model pre-dicts an alignment with a low model score, given aninput sentence pair, we might say the model is ?con-fused.?
In this case, the alignment probably doesn?tlook like the examples it has been trained on.1.
It could be that the data is parallel, but the modelis very confused.
(modeling problem)2.
It could be that the data is noisy, and the modelis very confused.
(data problem)The general accuracy of the alignment model weemploy makes the former case unlikely.
Therefore,a key assumption we make is to assume a low modelscore accompanies noisy data, and use this data ascandidates from which to extract non-noisy parallelsegments.2.2 A Brief ExampleAs an illustrative example, consider the follow-ing sentence pair in our training corpus taken fromLDC2005T10.
This is the sentence pair shown inFigure 1:fate brought us together on that wonderful summer dayand one year later , shou ?
tao and i were married not onlyin the united states but also in taiwan .?
??
?
??
, ?
?
?
????
?
???
?
?
???
; ?
??
?
??
?
?
?
?
, ?
?
?
??
?
?
??
.In this sentence pair there are only two parallelphrases, corresponding to the underlined and double-underlined strings.
There are a few scattered wordpairs which may have a natural correspondence,1 butno other larger phrases.2In this work we are concerned with finding largephrases,3 since very small phrases tend to be ex-tractible even when data is noisy.
Bad alignmentstend to cause conflicts when extracting large phrasesdue to unexpected, stray links in the alignment ma-trix; smaller fragments will have less opportunity tocome into conflict with incorrect, stray links due tonoisy data or alignment model error.
We considerlarge enough phrases for our purposes to be phrasesof size greater than 3, and ignore smaller fragments.2.3 Parallel Fragment Extraction2.3.1 A Hierarchical Alignment Model and itsDerivation TreesThe alignment model we use, (Riesa et al,2011), is a discriminatively trained model which atalignment-time walks up the English parse-tree and,at every node in the tree, generates alignments by re-cursively scoring and combining alignments gener-ated at the current node?s children, building up largerand larger alignments.
This process works similarlyto a CKY parser, moving bottom-up and generatinglarger and larger constituents until it has predictedthe full tree spanning the entire sentence.
How-1For example, (I, ?)
and (Taiwan, ??
)2The rest of the Chinese describes where the couple is from;the speaker, she says, is an American raised in New Jersey.3We count the size of the phrase according to the number ofEnglish words it contains; one could be more conservative byconstraining both sides.539??????aINfantasticyetrealisticJJCCJJADJPNPNNadventure??
[14.2034] PP [9.5130]NP [-0.5130]with multi-sensory experiencesFigure 2: From LDC2004T08, when the NP fragmentshown here is combined to make a larger span with a sis-ter PP fragment, the alignment model objects due to non-parallel data under the PP, voicing a score of -0.5130.
Weextract and append to our training corpus the NP fragmentdepicted, from which we later learn 5 additional transla-tion rules.ever, instead of generating syntactic structures, weare generating alignments.In moving bottom-up along the tree, just as thereis a derivation tree for a CKY parse, we can also fol-low backpointers to extract the derivation tree of the1-best alignment starting from the root node.
Thisderivation tree gives a hierarchical partitioning of thealignment and the associated word-spans.
We canalso inspect model scores at each node in the deriva-tion tree.2.3.2 Using the Alignment Model to DetectParallel FragmentsFor each training example in our parallel cor-pus, we have an alignment derivation tree.
Be-cause the derivation tree is essentially isomorphicto the English parse tree, the derivation tree repre-sents a hierarchical partitioning of the training ex-ample into syntactic segments.
We traverse the treetop-down, inspecting the parallel fragments impliedby the derivation at each point, and their associatedmodel scores.The idea behind this top-down traversal is that al-though some nodes, and perhaps entire derivations,may be low-scoring, there are often high-scoringfragments that make up the larger derivation whichare worthy of extraction.
Figure 2 shows an ex-ample.
We recursively traverse the derivation, top-down, extracting the largest fragment possible atany derivation node whose alignment model score ishigher than some threshold ?, and whose associatedEnglish and foreign spans meet a set of importantconstraints:1.
The parent node in the derivation has a score lessthan ?.2.
The length of the English span is > 3.3.
There are no unaligned foreign words inside thefragment that are also aligned to English wordsoutside the fragment.Once a fragment has been extracted, we do not re-curse any further down the subtree.Constraint 1 is a candidate constraint, and forcesus to focus on segments of parallel sentences withlow model scores; these are segments likely to con-sist of bad alignments due to noisy data or alignererror.Constraint 2 is a conservativity constraint ?
weare more confident in model scores over larger frag-ments with more context than smaller ones with min-imal context.
This constraint also parameterizes thenotion that larger fragments are the type more oftenprecluded from extraction due to stray or incorrectword-alignment links; additionally, we are alreadylikely to be able to extract smaller fragments usingstandard methods, and as such, they are less usefulto us here.Constraint 3 is a content constraint, limiting usfrom extracting fragments with blocks of unalignedforeign words that don?t belong in this particularfragment because they are aligned elsewhere.
If wethrew out this constraint, then in translating fromChinese to English, we would erroneously learn todelete blocks of Chinese words that otherwise shouldbe translated.
When foreign words are unaligned ev-erywhere within a parallel sentence, then they canbe included within the extracted fragment.
Commonexamples in Chinese are function words such as ?,?, and ?.
Put another way, we only allow globallyunaligned words in extracted fragments.Computing ?.
In computing our extraction thresh-old ?, we must decide what proportion of fragmentswe consider to be low-scoring and least likely to beuseful for translation.
We make the rather strong as-540sumption that this is the bottom 10% of the data.43 EvaluationWe evaluate our parallel fragment extraction in alarge-scale Chinese-English and Arabic-English MTsetting.
In our experiments we use a tree-to-stringsyntax-based MT system (Galley et al, 2004), andevaluate on a standard test set, NIST08.
We parse theEnglish side of our parallel corpus with the Berkeleyparser (Petrov et al, 2006), and tune parameters oftheMT systemwithMIRA (Chiang et al, 2008).
Wedecode with an integrated language model trained onabout 4 billion words of English.Chinese-English We align a parallel corpus of8.4M parallel segments, with 210M words of En-glish and 193M words of Chinese.
From this weextract 868,870 parallel fragments according to theprocess described in Section 2, and append thesefragments to the end of the parallel corpus.
In doingso, we have created a larger parallel corpus of 9.2Mparallel segments, consisting of 217M and 198Mwords of English and Chinese, respectively.Arabic-English We align a parallel corpus of9.0M parallel segments, with 223M words of En-glish and 194M words of Arabic.
From this we ex-tract 996,538 parallel fragments, and append thesefragments to the end of the parallel corpus.
The re-sulting corpus has 10M parallel segments, consistingof 233M and 202Mwords of English and Arabic, re-spectively.Results are shown in Table 1.
Using our parallelfragment extraction, we learn 68M additional uniqueArabic-English rules that are not in the baseline sys-tem; likewise, we learn 38M new unique Chinese-English rules not in the baseline system for that lan-guage pair.
Note that we are not simply duplicat-ing portions of the parallel data.
While each se-quence fragment of source and target words we ex-tract will be found elsewhere in the larger parallelcorpus, these fragments will largely not make it intofruitful translation rules to be used in the downstreamMT system.We see gains in BLEU score across two differ-ent language pairs, showing empirically that we are4One may wish to experiment with different ranges here, buteach requires a separate time-consuming downstream MT ex-periment.
In this work, it turns out that scrutinizing 10% of thedata is productive and empirically reasonable.Corpus Extracted Rules BLEUBaseline (Ara-Eng) 750M 50.0+Extracted fragments 818M 50.4Baseline (Chi-Eng) 270M 31.5+Extracted fragments 308M 32.0Table 1: End-to-end translation experiments with andwithout extracted fragments.
We are learning many moreunique rules; BLEU score gains are significant with p <0.05 for Arabic-English and p < 0.01 for Chinese-English.learning new and useful translation rules we previ-ously were not in our grammars.
These results aresignificant with p < 0.05 for Arabic-English andp < 0.01 for Chinese-English.4 DiscussionAll alignment models we have experimented withwill fall down in the presence of noisy data.
Impor-tantly, even if the alignment model were able to yield?perfect?
alignments with no alignment links amongnoisy sections of the parallel data precluding us fromextracting reasonable rules or phrase pairs, wewouldstill have to deal with downstream rule extractionheuristics and their tendency to blow up a translationgrammar in the presence of large swaths of unalignedwords.
Absent a mechanism within the alignmentmodel itself to deal with this problem, we provide asimple way to recover from noisy data without theintroduction of new tools.Summing up, parallel data in the world is notunlimited.
We cannot always continue to doubleour data for increased performance.
Parallel datacreation is expensive, and automatic discovery isresource-intensive (Uszkoreit et al, 2010).
We havepresented a technique that helps to squeeze more outof an already large, state-of-the-art MT system, us-ing existing pieces of the pipeline to do so in a novelway.AcknowledgementsThis work was supported byDARPABOLT via BBN sub-contract HR0011-12-C-0014.
We thank our three anony-mous reviewers for thoughtful comments.
Thanks also toKevin Knight, David Chiang, Liang Huang, and PhilippKoehn for helpful discussions.541ReferencesMichele Banko and Eric Brill.
2001.
Scaling to very verylarge corpora for natural language disambiguation.
InProc.
of the ACL, pages 26?33.David Chiang, Yuval Marton, and Philip Resnik.
2008.Online large-margin training of syntactic and struc-tural translation features.
In Proc.
of EMNLP, pages224?233.Pascale Fung and Percy Cheung.
2004.
Mining very non-parallel corpora: Parallel sentence and lexicon extrac-tion via boostrapping and EM.
In Proc.
of EMNLP,pages 57?63.Michel Galley, Mark Hopkins, Kevin Knight, and DanielMarcu.
2004.
What?s in a translation rule?
In Proc.
ofHLT-NAACL, pages 273?280.Dragos Stefan Munteanu and Daniel Marcu.
2006.
Ex-tracting parallel sub-sentential fragments from non-parallel corpora.
In Proc.
of COLING/ACL, Sydney,Australia.Slav Petrov, Leon Barrett, Romain Thibaux, and DanKlein.
2006.
Learning accurate, compact, and inter-pretable tree annotation.
In Proc.
of COLING-ACL.Chris Quirk, Raghavendra Udupa, and Arul Menezes.2007.
Generative models of noisy translations with ap-plications to parallel fragment extraction.
In Proceed-ings of MT Summit XI.Jason Riesa, Ann Irvine, and Daniel Marcu.
2011.Feature-rich language-independent syntax-basedalignment for statistical machine translation.
In Proc.of EMNLP, pages 497?507.Jakob Uszkoreit, Jay Ponte, Ashok Popat, andMoshe Du-biner.
2010.
Large scale parallel document miningfor machine translation.
In Proc.
of COLING, pages1101?1109.Dekai Wu and Pascale Fung.
2005.
Inversion transduc-tion grammar constraints for mining parallel sentencesfrom quasi-comparable corpora.
In Proc.
of IJCNLP,pages 257?268.Bing Zhao and Stephan Vogel.
2002.
Adaptive paral-lel sentences mining from web bilingual news collec-tion.
In IEEE International Conference on Data Min-ing, pages 745?748, Maebashi City, Japan.542
