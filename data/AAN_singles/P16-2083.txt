Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 512?517,Berlin, Germany, August 7-12, 2016.c?2016 Association for Computational LinguisticsWord Embedding Calculus in Meaningful Ultradense SubspacesSascha Rothe and Hinrich Sch?utzeCenter for Information and Language ProcessingLMU Munich, Germanysascha@cis.lmu.deAbstractWe decompose a standard embeddingspace into interpretable orthogonal sub-spaces and a ?remainder?
subspace.
Weconsider four interpretable subspaces inthis paper: polarity, concreteness, fre-quency and part-of-speech (POS) sub-spaces.
We introduce a new calculusfor subspaces that supports operations like?
?1 ?
hate = love?
and ?give me a neu-tral word for greasy?
(i.e., oleaginous).This calculus extends analogy computa-tions like ?king?man+woman = queen?.For the tasks of Antonym Classificationand POS Tagging our method outperformsthe state of the art.
We create test sets forMorphological Analogies and for the newtask of Polarity Spectrum Creation.1 IntroductionWord embeddings are usually trained on an ob-jective that ensures that words occurring in simi-lar contexts have similar embeddings.
This makesthem useful for many tasks, but has drawbacks forothers; e.g., antonyms are often interchangeablein context and thus have similar word embeddingseven though they denote opposites.
If we thinkof word embeddings as members of a (commuta-tive or Abelian) group, then antonyms should beinverses of (as opposed to similar to) each other.In this paper, we use DENSIFIER (Rothe et al,2016) to decompose a standard embedding spaceinto interpretable orthogonal subspaces, includinga one-dimensional polarity subspace as well asconcreteness, frequency and POS subspaces.
Weintroduce a new calculus for subspaces in whichantonyms are inverses, e.g., ?
?1 ?
hate = love?.The formula shows what happens in the polaritysubspace; the orthogonal complement (all the re-maining subspaces) is kept fixed.
We show be-low that we can predict an entire polarity spec-trum based on the subspace, e.g., the four-wordspectrum hate, dislike, like, love.
Similar to polar-ity, we explore other interpretable subspaces anddo operations such as: given a concrete word likefriend find the abstract word friendship (concrete-ness); given the frequent word friend find a lessfrequent synonym like comrade (frequency); andgiven the noun friend find the verb befriend (POS).2 Word Embedding TransformationWe now give an overview of DENSIFIER; seeRothe et al (2016) for details.
Let Q ?
Rd?dbe an orthogonal matrix that transforms the orig-inal word embedding space into a space in whichcertain types of information are represented by asmall number of dimensions.
The orthogonalitycan be seen as a hard regularization of the trans-formation.
We choose this because we do not wantto add or remove any information from the origi-nal embeddings space.
This ensures that the trans-formed word embeddings behave differently onlywhen looking at subspaces, but behave identicallywhen looking at the entire space.
By choosing anorthogonal and thus linear transformation we alsoassume that the information is already encodedlinearly in the original word embedding.
This is afrequent assumption, as we generally use the vec-tor addition for word embeddings.Concretely, we learn Q such that the dimen-sions Dp?
{1, .
.
.
, d} of the resulting space cor-respond to a word?s polarity information and the{1, .
.
.
, d}\Dpremaining dimensions correspondto non-polarity information.
Analogously, the setsof dimensions Dc, Dfand Dmcorrespond to aword?s concreteness, frequency and POS (or mor-phological) information, respectively.
In this pa-per, we assume that these properties do not corre-512Figure 1: Illustration of the transformed embeddings.
The horizontal axis is the polarity subspace.All non-polarity information, including concreteness, frequency and POS, is projected into a two di-mensional subspace for visualization (gray plane).
A query word (bold) specifies a line parallel to thehorizontal axis.
We then construct a cylinder around this line.
Words in this cylinder are considered tobe part of the word spectrum.late and therefore the ultradense subspaces do notoverlap.
E.g.,Dp?Dc= ?.
This might not be truefor other settings, e.g., sentiment and semantic in-formation.
As we are using only four propertiesthere is also a subspace which is in the orthogonalcomplement of all trained subspaces.
This sub-space includes the not classified information, e.g.,genre information in our case (e.g., ?clunker?
is acolloquial word for ?automobile?
).If ev?
Rdis the original embedding of word v,the transformed representation is uv= Qev.
Weuse ?
as a placeholder for polarity (p), concrete-ness (c), frequency (f ) and POS/morphology (m)and call d?= |D?| the dimensionality of the ultra-dense subspace of property ?.
For each ultradensesubspace, we create P??
Rd?
?d, an identity ma-trix for the dimensions inD?.
Thus, the ultradense(UD) representation u?v?
Rd?of word v is definedas:u?v:= P?Qev(1)For notational simplicity, u?vwill either refer to avector in Rd?or to a vector in Rdwhere all dimen-sions /?
D?are set to zero.For training, the orthogonal transformation Qwe assume we have a lexicon resource.
Let L?6?be a set of word index pairs (v, w) with differentlabels, e.g., positive/negative, concrete/abstract ornoun/verb.
We want to maximize the distance forpairs in this group.
Thus, our objective is:argminQ???{p,c,f,m}?(v,w)?L?6???P?Q(ev?
ew)?
(2)subject to Q being an orthogonal matrix.
Anothergoal is to minimize the distance of two words withidentical labels.
Let L?
?be a set of word indexpairs (v, w) with identical labels.
In contrast toEq.
2, we now want to minimize each distance.Thus, the objective is given by:argminQ???{p,c,f,m}?(v,w)?L???P?Q(ev?ew)?
(3)subject toQ being an orthogonal matrix.
For train-ing Eq.
2 is weighted with ?
?and Eq.
3 with1 ?
??.
We do a batch gradient descent whereeach batch contains the same number of positiveand negative examples.
This means the number ofexamples in the lexica ?
which give rise to morenegative than positive examples ?
does not influ-ence the training.3 Setup and MethodEqs.
2/3 can be combined to train an orthogonaltransformation matrix.
We use pretrained 300-dimensional English word embeddings (Mikolovet al, 2013) (W2V).
To train the transformationmatrix, we use a combination of MPQA (Wil-son et al, 2005), Opinion Lexicon (Hu and Liu,2004) and NRC Emotion lexicons (Mohammadand Turney, 2013) for polarity; BWK, a lexiconof 40,000 English words (Brysbaert et al, 2014),for concreteness; the order in the word embed-ding file for frequency; and the training set of theFLORS tagger (Schnabel and Sch?utze, 2014) forPOS.
The application of the transformation ma-513trix to the word embeddings gives us four sub-spaces for polarity, concreteness, frequency andPOS.
These subspaces and their orthogonal com-plements are the basis for an embedding calculusthat supports certain operations.
Here, we investi-gate four such operations.
The first operation com-putes the antonym of word v:antonym(v) = nn(uv?
2upv) (4)where nn : Rd?
V returns the word whose em-bedding is the nearest neighbor to the input.
Thus,our hypothesis is that antonyms are usually verysimilar in semantics except that they differ on asingle ?semantic axis,?
the polarity axis.1The sec-ond operation is ?neutral version of word v?
:neutral(v) = nn(uv?
upv) (5)Thus, our hypothesis is that neutral words arewords with a value close to zero in the polaritysubspace.
The third operation produces the polar-ity spectrum of v:spectrum(v) = {nn(uv+ xupv) | ?x ?
R} (6)This means that we keep the semantics of thequery word fixed, while walking along the polar-ity axis, thus retrieving different shades of polarity.Figure 1 shows two example spectra.
The fourthoperation is ?word v with POS of word w?
:POSw(v) = nn(uv?
umv+ umw) (7)This is similar to analogies like king ?
man +woman, except that the analogy is inferred by thesubspace relevant for the analogy.We create word spectra for some manually cho-sen words using the Google News corpus (W2V)and a Twitter corpus.
As the transformation wasorthogonal and therefore did not change the lengthof a dimension, we multiply the polarity dimen-sion with 30 to give it a high weight, i.e., payingmore attention to it.
We then use Eq.
6 with a suf-ficiently small step size for x, i.e., further reduc-ing the step size does not increase the spectrum.We also discard words that have a cosine distanceof more than .6 in the non-polarity space.
Ta-ble 1 shows examples.
The results are highly do-main dependent, with Twitter?s spectrum indicat-ing more negative views of politicians than news.While fall has negative associations, autumn?s arepositive ?
probably because autumn is of a higherregister in American English.1See discussion/experiments below for exceptionsCorpus, Type SpectrumNews,Polarityhypocrite, politician, legislator, busi-nessman, reformer, statesman, thinkerfall, winter, summer, spring, autumndrunks, booze, liquor, lager, beer, beers,wine, beverages, wines, tastingsTwitter,Polaritycorrupt, coward, politician, journalist,citizen, musician, representativestalker, neighbour, gf, bf, cousin, frnd,friend, mentor#stupid, #problems, #homework,#mylife, #reality, #life, #happinessNews,Concretenessimperialist, conflict, war, Iraq, VietnamWar, battlefields, soldierslove, friendship, dear friend, friends,friend, girlfriendNews,Frequencyredesigned, newer, revamped, newintellect, insights, familiarity, skills,knowledge, experienceTable 1: Example word spectra for polarity, con-creteness and frequency on two different corpora.Queries are bold.dev set test setP R F1P R F1Adel, 2014 .79 .65 .72 .75 .58 .66our work .81 .90 .85 .76 .88 .82Table 2: Results for Antonym Classification4 Evaluation4.1 Antonym Classification.We evaluate on Adel and Sch?utze (2014)?s data;the task is to decide for a pair of words whetherthey are antonyms or synonyms.
The set has 2,337positive and negative pairs each and is split into80% training, 10% dev and 10% test.
Adel andSch?utze (2014) collected positive/negative exam-ples from the nearest neighbors of the word em-beddings to make it hard to solve the task usingword embeddings.
We train an SVM (RBF kernel)on three features that are based on the intuition de-picted in Figure 1: the three cosine distances in:the polarity subspace; the orthogonal complement;and the entire space.
Table 2 shows that improve-ment of precision is minor (.76 vs. .75), but recalland F1improve by a lot (+.30 and +.16).4.2 Polarity Spectrum Creationconsists of two subtasks.
PSC-SET: Given a queryword how well can we predict a spectrum?
PSC-ORD: How good is the order in the spectrum?Our gold standard is Word Spectrum, included inthe Oxford American Writer?s Thesaurus (OAWT)and therefore also in MacOS.
For each query word514newsgroups reviews weblogs answers emails wsjALL OOV ALL OOV ALL OOV ALL OOV ALL OOV ALL OOV1 LSJU 89.11?56.02?91.43?58.66?94.15?77.13?88.92?49.30?88.68?58.42?96.83 90.252 SVM 89.14?53.82?91.30?54.20?94.21?76.44?88.96?47.25?88.64?56.37?96.63 87.96?3 F 90.86 66.42?92.95 75.29?94.71 83.64?90.30 62.15?89.44 62.61?96.59 90.374 F+W2V 90.51 72.26 92.46?78.03 94.70 86.05 90.34 65.16 89.26 63.70?96.44 91.365 F+UD 90.79 72.20 92.84 78.80 94.84 86.47 90.60 65.48 89.68 66.24 96.61 92.36Table 3: Results for POS tagging.
LSJU = Stanford.
SVM = SVMTool.
F=FLORS.
We show three state-of-the-art taggers (lines 1-3), FLORS extended with 300-dimensional embeddings (4) and extended withUD embeddings (5).
?
: significantly better than the best result in the same column (?
= .05, one-tailedZ-test).this dictionary returns a list of up to 80 words ofshades of meaning between two polar opposites.We look for words that are also present in Adeland Sch?utze (2014)?s Antonym Classification dataand retrieve 35 spectra.
Each word in a spectrumcan be used as a query word; after intersecting thespectra with our vocabulary, we end up with 1301test cases.To evaluate PSC-SET, we calculate the 10 near-est neighbors of the m words in the spectrum andrank the 10m neighbors by the distance to ourspectrum, i.e., the cosine distance in the orthog-onal complement of the polarity subspace.
We re-port mean average precision (MAP) and weightedMAP where each MAP is weighted by the num-ber of words in the spectrum.
As shown in Table 4there is no big difference between both numbers,meaning that our algorithm does not work betteror worse on smaller or larger spectra.To evaluate PSC-ORD, we calculate Spear-man?s ?
of the ranks in OAWT and the values onthe polarity dimension.
Again, there is no signifi-cant difference between average and weighted av-erage of ?.
Table 4 also shows that the varianceof the measures is low for PSC-SET and high forPSC-ORD; thus, we do well on certain spectra andworse on others.
The best one, beautiful?
ugly,is given as an example.
The worst performingspectrum is fat?
skinny (?
= .13) ?
presumablybecause both extremes are negative, contradictingour modeling assumption that spectra go from pos-itive to negative.
We test this hypothesis by sepa-rating the spectrum into two subspectra.
We thenreport the weighted average correlation of the op-timal separation.
For fat ?
skinny, this improves?
to .67.PSC-SET: MAP PSC-ORD: ?
avg(?1, ?2)average .48 .59 .70weighted avg.
.47 .59 .70variance .004 .048 .014beautiful/ugly .48 .84 .84fat/skinny .56 .13 .67absent/present .43 .72 .76Table 4: Results for Polarity Spectrum Creation:MAP, Spearman?s ?
(one spectrum) and average ?
(two subspectra)4.3 Morphological Analogy.The previous two subspaces were one-dimensional.
Now we consider a POS subspace,because POS is not one-dimensional and cannotbe modeled as a single scalar quantity.
Wecreate a word analogy benchmark by extractingderivational forms from WordNet (Fellbaum,1998).
We discard words with ?2 derivationalforms of the same POS and words not in themost frequent 30,000.
We then randomly se-lect 26 pairs for every POS combination forthe dev set and 26 pairs for the test set.2Anexample of the type of equation we solve here isprediction?
predict + symbolize = symbol (fromthe dev set).
W2V embeddings are our baseline.We can also rewrite the left side of the equationas POS(prediction) + Semantics(symbolize); notethat this cannot be done using standard word em-beddings.
In contrast, our method can use mean-ingful UD embeddings and Eq.
7 with POS(v) be-ing umvand Semantics(v) being uv?
umv.
The devset indicates that a 8-dimensional POS subspace isoptimal and Table 5 shows that this method out-2This results in an even number of 25 ?
26 = 650 ques-tions per POS combination, 4?2?650 = 5200 in total (4 POScombinations, where each POS can be used as query POS).515W2V UDA?B B?A A?B B?Anoun-verb 35.69 6.62 59.69?50.46?adj-noun 30.77 27.38 53.85?43.85?adj-verb 20.62 3.08 32.15?24.77?adj-adverb 45.38 35.54 46.46?43.08?all 25.63 44.29?Table 5: Accuracy @1 on test for MorphologicalAnalogy.
?
: significantly better than the corre-sponding result in the same row (?
= .05, one-tailed Z-test).performs the baseline.4.4 POS TaggingOur final evaluation is extrinsic.
We use FLORS(Schnabel and Sch?utze, 2014), a state-of-the-artPOS tagger which was extended by Yin et al(2015) with word embeddings as additional fea-tures.
W2V gives us a consistent improvement onOOVs (Table 3, line 4).
However, training thismodel requires about 500GB of RAM.
When weuse the 8-dimensional UD embeddings (the sameas for Morphological Analogy), we outperformW2V except for a virtual tie on news (Table 3, line5).
So we perform better even though we only use8 of 300 dimensions!
However, the greatest advan-tage of UD is that we only need 100GB of RAM,80% less than W2V.5 Related WorkYih et al (2012) also tackled the problem ofantonyms having similar embeddings.
In theirmodel, the antonym is the inverse of the en-tire vector whereas in our work the antonym isonly the inverse in an ultradense subspace.
Ourmodel is more intuitive since antonyms invertonly part of the meaning, not the entire mean-ing.
Schwartz et al (2015) present a method thatswitches an antonym parameter on or off (depend-ing on whether a high antonym-synonym similar-ity is useful for an application) and learn multipleembedding spaces.
We only need a single space,but consider different subspaces of this space.An unsupervised approach using linguistic pat-terns that ranks adjectives according to their inten-sity was presented by de Melo and Bansal (2013).Sharma et al (2015) present a corpus-independentapproach for the same problem.
Our results (Ta-ble 1) suggest that polarity should not be consid-ered to be corpus-independent.There is also much work on incorporatingthe additional information into the original wordembedding training.
Examples include (Bothaand Blunsom, 2014) and (Cotterell and Sch?utze,2015).
However, postprocessing has several ad-vantages.
DENSIFIER can be trained on a normalwork station without access to the original train-ing corpus.
This makes the method more flexible,e.g., when new training data or desired propertiesare available.On a general level, our method bears some re-semblance with (Weinberger and Saul, 2009) inthat we perform supervised learning on a set of de-sired (dis)similarities and that we can think of ourmethod as learning specialized metrics for particu-lar subtypes of linguistic information or particulartasks.
Using the method of Weinberger and Saul(2009), one could learn k metrics for k subtypesof information and then simply represent a wordwas the concatenation of (i) the original embeddingand (ii) k representations corresponding to the kmetrics.3In case of a simple one-dimensional typeof information, the corresponding representationcould simply be a scalar.
We would expect thisapproach to have similar advantages for practicalapplications, but we view our orthogonal transfor-mation of the original space as more elegant and itgives rise to a more compact representation.6 ConclusionWe presented a new word embedding calculusbased on meaningful ultradense subspaces.
Weapplied the operations of the calculus to AntonymClassification, Polarity Spectrum Creation, Mor-phological Analogy and POS Tagging.
Our eval-uation shows that our method outperforms pre-vious work and is applicable to different typesof information.
We have published test sets andword embeddings at http://www.cis.lmu.de/?sascha/Ultradense/.AcknowledgmentsThis research was supported by DeutscheForschungsgemeinschaft (DFG, grant 2246/10-1).3We would like to thank an anonymous reviewer for sug-gesting this alternative approach.516ReferencesHeike Adel and Hinrich Sch?utze.
2014.
Using minedcoreference chains as a resource for a semantic task.In Proceedings of EMNLP.Jan A Botha and Phil Blunsom.
2014.
Compositionalmorphology for word representations and languagemodelling.
arXiv preprint arXiv:1405.4273.Marc Brysbaert, Amy Beth Warriner, and Victor Ku-perman.
2014.
Concreteness ratings for 40 thou-sand generally known english word lemmas.
Behav-ior research methods, 46(3):904?911.Ryan Cotterell and Hinrich Sch?utze.
2015.
Morpho-logical word-embeddings.
In Proceedings of the2015 Conference of the North American Chapterof the Association for Computational Linguistics:Human Language Technologies, pages 1287?1292,Denver, Colorado, May?June.
Association for Com-putational Linguistics.Gerard de Melo and Mohit Bansal.
2013.
Good, great,excellent: Global inference of semantic intensities.Transactions of the Association for ComputationalLinguistics, 1:279?290.Christiane Fellbaum.
1998.
WordNet: An ElectronicLexical Database.
Bradford Books.Minqing Hu and Bing Liu.
2004.
Mining and Summa-rizing Customer Reviews.
In KDD.Tomas Mikolov, Kai Chen, Greg Corrado, and Jef-frey Dean.
2013.
Efficient estimation of wordrepresentations in vector space.
arXiv preprintarXiv:1301.3781.Saif M. Mohammad and Peter D. Turney.
2013.Crowdsourcing a Word-Emotion Association Lexi-con.
Computational Intelligence, 29(3).Sascha Rothe, Sebastian Ebert, and Hinrich Sch?utze.2016.
Ultradense word embeddings by orthogonaltransformation.
arXiv preprint arXiv:1602.07572.Tobias Schnabel and Hinrich Sch?utze.
2014.
Flors:Fast and simple domain adaptation for part-of-speech tagging.
Transactions of the Association forComputational Linguistics, 2:15?26.Roy Schwartz, Roi Reichart, and Ari Rappoport.
2015.Symmetric pattern based word embeddings for im-proved word similarity prediction.
In Proceedingsof CoNLL.Raksha Sharma, Mohit Gupta, Astha Agarwal, andPushpak Bhattacharyya.
2015.
Adjective intensityand sentiment analysis.
In Proceedings of EMNLP.Kilian Q. Weinberger and Lawrence K. Saul.
2009.Distance metric learning for large margin near-est neighbor classification.
J. Mach.
Learn.
Res.,10:207?244.Theresa Wilson, Janyce Wiebe, and Paul Hoff-mann.
2005.
Recognizing contextual polarity inphrase-level sentiment analysis.
In Proceedings ofHLT/EMNLP.Wen-tau Yih, Geoffrey Zweig, and John C Platt.
2012.Polarity inducing latent semantic analysis.
In Pro-ceedings of EMNLP.Wenpeng Yin, Tobias Schnabel, and Hinrich Sch?utze.2015.
Online updating of word representations forpart-of-speech tagging.
In Proceedings of EMNLP.517
