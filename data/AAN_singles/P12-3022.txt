Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 127?132,Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational LinguisticsAkamon: An Open Source Toolkitfor Tree/Forest-Based Statistical Machine Translation?Xianchao Wu?, Takuya Matsuzaki?, Jun?ichi Tsujii??
Baidu Inc.?National Institute of Informatics?
Microsoft Research Asiawuxianchao@gmail.com,takuya-matsuzaki@nii.ac.jp,jtsujii@microsoft.comAbstractWe describe Akamon, an open source toolkitfor tree and forest-based statistical machinetranslation (Liu et al, 2006; Mi et al, 2008;Mi and Huang, 2008).
Akamon implementsall of the algorithms required for tree/forest-to-string decoding using tree-to-string trans-lation rules: multiple-thread forest-based de-coding, n-gram language model integration,beam- and cube-pruning, k-best hypothesesextraction, and minimum error rate training.In terms of tree-to-string translation rule ex-traction, the toolkit implements the tradi-tional maximum likelihood algorithm usingPCFG trees (Galley et al, 2004) and HPSGtrees/forests (Wu et al, 2010).1 IntroductionSyntax-based statistical machine translation (SMT)systems have achieved promising improvements inrecent years.
Depending on the type of input, thesystems are divided into two categories: string-based systems whose input is a string to be simul-taneously parsed and translated by a synchronousgrammar (Wu, 1997; Chiang, 2005; Galley et al,2006; Shen et al, 2008), and tree/forest-based sys-tems whose input is already a parse tree or a packedforest to be directly converted into a target tree orstring (Ding and Palmer, 2005; Quirk et al, 2005;Liu et al, 2006; Huang et al, 2006; Mi et al, 2008;Mi and Huang, 2008; Zhang et al, 2009; Wu et al,2010; Wu et al, 2011a).
?Work done when all the authors were in The University ofTokyo.Depending on whether or not parsers are explic-itly used for obtaining linguistically annotated dataduring training, the systems are also divided into twocategories: formally syntax-based systems that donot use additional parsers (Wu, 1997; Chiang, 2005;Xiong et al, 2006), and linguistically syntax-basedsystems that use PCFG parsers (Liu et al, 2006;Huang et al, 2006; Galley et al, 2006; Mi et al,2008; Mi and Huang, 2008; Zhang et al, 2009),HPSG parsers (Wu et al, 2010; Wu et al, 2011a), ordependency parsers (Ding and Palmer, 2005; Quirket al, 2005; Shen et al, 2008).
A classification1 ofsyntax-based SMT systems is shown in Table 1.Translation rules can be extracted from alignedstring-string (Chiang, 2005), tree-tree (Ding andPalmer, 2005) and tree/forest-string (Galley et al,2004; Mi and Huang, 2008; Wu et al, 2011a)data structures.
Leveraging structural and linguis-tic information from parse trees/forests, the lattertwo structures are believed to be better than theirstring-string counterparts in handling non-local re-ordering, and have achieved promising translationresults.
Moreover, the tree/forest-string structure ismore widely used than the tree-tree structure, pre-sumably because using two parsers on the sourceand target languages is subject to more problemsthan making use of a parser on one language, suchas the shortage of high precision/recall parsers forlanguages other than English, compound parse errorrates, and inconsistency of errors.
In Table 1, notethat tree-to-string rules are generic and applicableto many syntax-based models such as tree/forest-to-1This classification is inspired by and extends the Table 1 in(Mi and Huang, 2008).127Source-to-target Examples (partial) Decoding Rules Parsertree-to-tree (Ding and Palmer, 2005) ?
dep.-to-dep.
DGforest-to-tree (Liu et al, 2009a) ?
??
tree-to-tree PCFGtree-to-string (Liu et al, 2006) ?
tree-to-string PCFG(Quirk et al, 2005) ?
dep.-to-string DGforest-to-string (Mi et al, 2008) ?
??
tree-to-string PCFG(Wu et al, 2011a) ?
??
tree-to-string HPSGstring-to-tree (Galley et al, 2006) CKY tree-to-string PCFG(Shen et al, 2008) CKY string-to-dep.
DGstring-to-string (Chiang, 2005) CKY string-to-string none(Xiong et al, 2006) CKY string-to-string noneTable 1: A classification of syntax-based SMT systems.
Tree/forest-based and string-based systems are split by a line.All the systems listed here are linguistically syntax-based except the last two (Chiang, 2005) and (Xiong et al, 2006),which are formally syntax-based.
DG stands for dependency (abbreviated as dep.)
grammar.
?
and ?
denote top-downand bottom-up traversals of a source tree/forest.string models and string-to-tree model.However, few tree/forest-to-string systems havebeen made open source and this makes it diffi-cult and time-consuming to testify and follow exist-ing proposals involved in recently published papers.The Akamon system2, written in Java and follow-ing the tree/forest-to-string research direction, im-plements all of the algorithms for both tree-to-stringtranslation rule extraction (Galley et al, 2004; Miand Huang, 2008; Wu et al, 2010; Wu et al, 2011a)and tree/forest-based decoding (Liu et al, 2006; Miet al, 2008).
We hope this system will help re-lated researchers to catch up with the achievementsof tree/forest-based translations in the past severalyears without re-implementing the systems or gen-eral algorithms from scratch.2 Akamon Toolkit FeaturesLimited by the successful parsing rate and coverageof linguistic phrases, Akamon currently achievescomparable translation accuracies compared withthe most frequently used SMT baseline system,Moses (Koehn et al, 2007).
Table 2 shows the auto-matic translation accuracies (case-sensitive) of Aka-mon and Moses.
Besides BLEU and NIST score, wefurther list RIBES score3, , i.e., the software imple-mentation of Normalized Kendall?s ?
as proposed by(Isozaki et al, 2010a) to automatically evaluate thetranslation between distant language pairs based onrank correlation coefficients and significantly penal-2Code available at https://sites.google.com/site/xianchaowu20123Code available at http://www.kecl.ntt.co.jp/icl/lirg/ribesizes word order mistakes.In this table, Akamon-Forest differs fromAkamon-Comb by using different configurations:Akamon-Forest used only 2/3 of the total trainingdata (limited by the experiment environments andtime).
Akamon-Comb represents the system com-bination result by combining Akamon-Forest andother phrase-based SMT systems, which made useof pre-ordering methods of head finalization as de-scribed in (Isozaki et al, 2010b) and used the total 3million training data.
The detail of the pre-orderingapproach and the combination method can be foundin (Sudoh et al, 2011) and (Duh et al, 2011).Also, Moses (hierarchical) stands for the hi-erarchical phrase-based SMT system and Moses(phrase) stands for the flat phrase-based SMT sys-tem.
For intuitive comparison (note that the resultachieved by Google is only for reference and not acomparison, since it uses a different and unknowntraining data) and following (Goto et al, 2011), thescores achieved by using the Google online transla-tion system4 are also listed in this table.Here is a brief description of Akamon?s main fea-tures:?
multiple-thread forest-based decoding: Aka-mon first loads the development (with sourceand reference sentences) or test (with sourcesentences only) file into memory and then per-form parameter tuning or decoding in a paral-lel way.
The forest-based decoding algorithmis alike that described in (Mi et al, 2008),4http://translate.google.com/128Systems BLEU NIST RIBESGoogle online 0.2546 6.830 0.6991Moses (hierarchical) 0.3166 7.795 0.7200Moses (phrase) 0.3190 7.881 0.7068Moses (phrase)* 0.2773 6.905 0.6619Akamon-Forest* 0.2799 7.258 0.6861Akamon-Comb 0.3948 8.713 0.7813Table 2: Translation accuracies of Akamon and the base-line systems on the NTCIR-9 English-to-Japanese trans-lation task (Wu et al, 2011b).
* stands for only using2 million parallel sentences of the total 3 million data.Here, HPSG forests were used in Akamon.i.e., first construct a translation forest by ap-plying the tree-to-string translation rules to theoriginal parsing forest of the source sentence,and then collect k-best hypotheses for the rootnode(s) of the translation forest using Algo-rithm 2 or Algorithm 3 as described in (Huangand Chiang, 2005).
Later, the k-best hypothe-ses are used both for parameter tuning on addi-tional development set(s) and for final optimaltranslation result extracting.?
language models: Akamon can make use ofone or many n-gram language models trainedby using SRILM5 (Stolcke, 2002) or the Berke-ley language model toolkit, berkeleylm-1.0b36(Pauls and Klein, 2011).
The weights of multi-ple language models are tuned under minimumerror rate training (MERT) (Och, 2003).?
pruning: traditional beam-pruning and cube-pruning (Chiang, 2007) techniques are incor-porated in Akamon to make decoding feasi-ble for large-scale rule sets.
Before decoding,we also perform the marginal probability-basedinside-outside algorithm based pruning (Mi etal., 2008) on the original parsing forest to con-trol the decoding time.?
MERT: Akamon has its own MERT modulewhich optimizes weights of the features so asto maximize some automatic evaluation metric,such as BLEU (Papineni et al, 2002), on a de-velopment set.5http://www.speech.sri.com/projects/srilm/6http://code.google.com/p/berkeleylm/e.tokcorpusf.segtokenize word segmente.tok.lw f.seg.lwlowercase lowercasecleane.clean f.cleanGIZA++alignmentRule setrule extractionSRILMAkamon Decoder (MERT)N-gram LMe.tokdev.etokenizee.tok.lwlowercasee.forestsEnjue.forestsEnjudevf.segdev.fwordsegmentationf.seg.lwlowercasepre-processingFigure 1: Training and tuning process of the Akamon sys-tem.
Here, e = source English language, f = target foreignlanguage.?
translation rule extraction: as former men-tioned, we extract tree-to-string translationrules for Akamon.
In particular, we imple-mented the GHKM algorithm as proposed byGalley et al (2004) from word-aligned tree-string pairs.
In addition, we also implementedthe algorithms proposed by Mi and Huang(2008) and Wu et al (2010) for extracting rulesfrom word-aligned PCFG/HPSG forest-stringpairs.3 Training and Decoding FrameworksFigure 1 shows the training and tuning progress ofthe Akamon system.
Given original bilingual par-allel corpora, we first tokenize and lowercase thesource and target sentences (e.g., word segmentationof Chinese and Japanese, punctuation segmentationof English).The pre-processed monolingual sentences will beused by SRILM (Stolcke, 2002) or BerkeleyLM(Pauls and Klein, 2011) to train a n-gram languagemodel.
In addition, we filter out too long sentences129here, i.e., only relatively short sentence pairs will beused to train word alignments.
Then, we can useGIZA++ (Och and Ney, 2003) and symmetric strate-gies, such as grow-diag-final (Koehn et al, 2007),on the tokenized parallel corpus to obtain a word-aligned parallel corpus.The source sentence and its packed forest, the tar-get sentence, and the word alignment are used fortree-to-string translation rule extraction.
Since a 1-best tree is a special case of a packed forest, we willfocus on using the term ?forest?
in the continuingdiscussion.
Then, taking the target language model,the rule set, and the preprocessed development setas inputs, we perform MERT on the decoder to tunethe weights of the features.The Akamon forest-to-string system includes thedecoding algorithm and the rule extraction algorithmdescribed in (Mi et al, 2008; Mi and Huang, 2008).4 Using Deep Syntactic StructuresIn Akamon, we support the usage of deep syn-tactic structures for obtaining fine-grained transla-tion rules as described in our former work (Wu etal., 2010)7.
Similarly, Enju8, a state-of-the-art andfreely available HPSG parser for English, can beused to generate packed parse forests for sourcesentences9.
Deep syntactic structures are includedin the HPSG trees/forests, which includes a fine-grained description of the syntactic property and asemantic representation of the sentence.
We extractfine-grained rules from aligned HPSG forest-stringpairs and use them in the forest-to-string decoder.The detailed algorithms can be found in (Wu et al,2010; Wu et al, 2011a).
Note that, in Akamon, wealso provide the codes for generating HPSG forestsfrom Enju.Head-driven phrase structure grammar (HPSG) isa lexicalist grammar framework.
In HPSG, linguis-tic entities such as words and phrases are representedby a data structure called a sign.
A sign gives a7However, Akamon still support PCFG tree/forest basedtranslation.
A special case is to yield PCFG style trees/forestsby ignoring the rich features included in the nodes of HPSGtrees/forests and only keep the POS tag and the phrasal cate-gories.8http://www-tsujii.is.s.u-tokyo.ac.jp/enju/index.html9Until the date this paper was submitted, Enju supports gen-erating English and Chinese forests.Feature DescriptionCAT phrasal categoryXCAT fine-grained phrasal categorySCHEMA name of the schema applied in the nodeHEAD pointer to the head daughterSEM HEAD pointer to the semantic head daughterCAT syntactic categoryPOS Penn Treebank-style part-of-speech tagBASE base formTENSE tense of a verb (past, present, untensed)ASPECT aspect of a verb (none, perfect,progressive, perfect-progressive)VOICE voice of a verb (passive, active)AUX auxiliary verb or not (minus, modal,have, be, do, to, copular)LEXENTRY lexical entry, with supertags embeddedPRED type of a predicateARG?x?
pointer to semantic arguments, x = 1..4Table 3: Syntactic/semantic features extracted fromHPSG signs that are included in the output of Enju.
Fea-tures in phrasal nodes (top) and lexical nodes (bottom)are listed separately.factored representation of the syntactic features ofa word/phrase, as well as a representation of theirsemantic content.
Phrases and words represented bysigns are composed into larger phrases by applica-tions of schemata.
The semantic representation ofthe new phrase is calculated at the same time.
Assuch, an HPSG parse tree/forest can be consideredas a tree/forest of signs (c.f.
the HPSG forest in Fig-ure 2 in (Wu et al, 2010)).An HPSG parse tree/forest has two attractiveproperties as a representation of a source sentencein syntax-based SMT.
First, we can carefully controlthe condition of the application of a translation ruleby exploiting the fine-grained syntactic descriptionin the source parse tree/forest, as well as those in thetranslation rules.
Second, we can identify sub-treesin a parse tree/forest that correspond to basic unitsof the semantics, namely sub-trees covering a pred-icate and its arguments, by using the semantic rep-resentation given in the signs.
Extraction of trans-lation rules based on such semantically-connectedsub-trees is expected to give a compact and effectiveset of translation rules.A sign in the HPSG tree/forest is represented by atyped feature structure (TFS) (Carpenter, 1992).
ATFS is a directed-acyclic graph (DAG) wherein theedges are labeled with feature names and the nodes130SheignorefactwantIdisputeARG1ARG2ARG1 ARG1ARG2ARG2JohnkillMary ARG2ARG1Figure 2: Predicate argument structures for the sentencesof ?John killed Mary?
and ?She ignored the fact that Iwanted to dispute?.
(feature values) are typed.
In the original HPSG for-malism, the types are defined in a hierarchy and theDAG can have arbitrary shape (e.g., it can be of anydepth).
We however use a simplified form of TFS,for simplicity of the algorithms.
In the simplifiedform, a TFS is converted to a (flat) set of pairs offeature names and their values.
Table 3 lists the fea-tures used in our system, which are a subset of thosein the original output from Enju.In the Enju English HPSG grammar (Miyao etal., 2003) used in our system, the semantic contentof a sentence/phrase is represented by a predicate-argument structure (PAS).
Figure 2 shows the PASof a simple sentence, ?John killed Mary?, and a morecomplex PAS for another sentence, ?She ignored thefact that I wanted to dispute?, which is adopted from(Miyao et al, 2003).
In an HPSG tree/forest, eachleaf node generally introduces a predicate, whichis represented by the pair of LEXENTRY (lexicalentry) feature and PRED (predicate type) feature.The arguments of a predicate are designated by thepointers from the ARG?x?
features in a leaf nodeto non-terminal nodes.
Consequently, Akamon in-cludes the algorithm for extracting compact com-posed rules from these PASs which further lead toa significant fast tree-to-string decoder.
This is be-cause it is not necessary to exhaustively generate thesubtrees for all the tree nodes for rule matching anymore.
Limited by space, we suggest the readers torefer to our former work (Wu et al, 2010; Wu et al,2011a) for the experimental results, including thetraining and decoding time using standard English-to-Japanese corpora, by using deep syntactic struc-tures.5 Content of the DemonstrationIn the demonstration, we would like to provide abrief tutorial on:?
describing the format of the packed forest for asource sentence,?
the training script on translation rule extraction,?
the MERT script on feature weight tuning on adevelopment set, and,?
the decoding script on a test set.Based on Akamon, there are a lot of interestingdirections left to be updated in a relatively fast wayin the near future, such as:?
integrate target dependency structures, espe-cially target dependency language models, asproposed by Mi and Liu (2010),?
better pruning strategies for the input packedforest before decoding,?
derivation-based combination of using othertypes of translation rules in one decoder, as pro-posed by Liu et al (2009b), and?
taking other evaluation metrics as the opti-mal objective for MERT, such as NIST score,RIBES score (Isozaki et al, 2010a).AcknowledgmentsWe thank Yusuke Miyao and Naoaki Okazaki fortheir invaluable help and the anonymous reviewersfor their comments and suggestions.ReferencesBob Carpenter.
1992.
The Logic of Typed Feature Struc-tures.
Cambridge University Press.David Chiang.
2005.
A hierarchical phrase-based modelfor statistical machine translation.
In Proceedings ofACL, pages 263?270, Ann Arbor, MI.David Chiang.
2007.
Hierarchical phrase-based transla-tion.
Computational Lingustics, 33(2):201?228.Yuan Ding and Martha Palmer.
2005.
Machine trans-lation using probabilistic synchronous dependency in-sertion grammers.
In Proceedings of ACL, pages 541?548, Ann Arbor.Kevin Duh, Katsuhito Sudoh, Xianchao Wu, HajimeTsukada, and Masaaki Nagata.
2011.
Generalizedminimum bayes risk system combination.
In Proceed-ings of IJCNLP, pages 1356?1360, November.Michel Galley, Mark Hopkins, Kevin Knight, and DanielMarcu.
2004.
What?s in a translation rule?
In Pro-ceedings of HLT-NAACL.131Michel Galley, Jonathan Graehl, Kevin Knight, DanielMarcu, Steve DeNeefe, Wei Wang, and IgnacioThayer.
2006.
Scalable inference and training ofcontext-rich syntactic translation models.
In Proceed-ings of COLING-ACL, pages 961?968, Sydney.Isao Goto, Bin Lu, Ka Po Chow, Eiichiro Sumita, andBenjamin K. Tsou.
2011.
Overview of the patent ma-chine translation task at the ntcir-9 workshop.
In Pro-ceedings of NTCIR-9, pages 559?578.Liang Huang and David Chiang.
2005.
Better k-bestparsing.
In Proceedings of IWPT.Liang Huang, Kevin Knight, and Aravind Joshi.
2006.Statistical syntax-directed translation with extendeddomain of locality.
In Proceedings of 7th AMTA.Hideki Isozaki, Tsutomu Hirao, Kevin Duh, KatsuhitoSudoh, and Hajime Tsukada.
2010a.
Automatic eval-uation of translation quality for distant language pairs.In Proc.of EMNLP, pages 944?952.Hideki Isozaki, Katsuhito Sudoh, Hajime Tsukada, andKevin Duh.
2010b.
Head finalization: A simple re-ordering rule for sov languages.
In Proceedings ofWMT-MetricsMATR, pages 244?251, July.Philipp Koehn, Hieu Hoang, Alexandra Birch, ChrisCallison-Burch, Marcello Federico, Nicola Bertoldi,Brooke Cowan, Wade Shen, Christine Moran, RichardZens, Chris Dyer, Ondr?ej Bojar, Alexandra Con-stantin, and Evan Herbst.
2007.
Moses: Open sourcetoolkit for statistical machine translation.
In Proceed-ings of the ACL 2007 Demo and Poster Sessions, pages177?180.Yang Liu, Qun Liu, and Shouxun Lin.
2006.
Tree-to-string alignment templates for statistical machinetransaltion.
In Proceedings of COLING-ACL, pages609?616, Sydney, Australia.Yang Liu, Yajuan Lu?, and Qun Liu.
2009a.
Improvingtree-to-tree translation with packed forests.
In Pro-ceedings of ACL-IJCNLP, pages 558?566, August.Yang Liu, Haitao Mi, Yang Feng, and Qun Liu.
2009b.Joint decoding with multiple translation models.
InProceedings of ACL-IJCNLP, pages 576?584, August.Haitao Mi and Liang Huang.
2008.
Forest-based transla-tion rule extraction.
In Proceedings of EMNLP, pages206?214, October.Haitao Mi and Qun Liu.
2010.
Constituency to depen-dency translation with forests.
In Proceedings of ACL,pages 1433?1442, July.Haitao Mi, Liang Huang, and Qun Liu.
2008.
Forest-based translation.
In Proceedings of ACL-08:HLT,pages 192?199, Columbus, Ohio.Yusuke Miyao, Takashi Ninomiya, and Jun?ichi Tsujii.2003.
Probabilistic modeling of argument structuresincluding non-local dependencies.
In Proceedings ofRANLP, pages 285?291, Borovets.Franz Josef Och and Hermann Ney.
2003.
A system-atic comparison of various statistical alignment mod-els.
Computational Linguistics, 29(1):19?51.Franz Josef Och.
2003.
Minimum error rate training instatistical machine translation.
In Proceedings of ACL,pages 160?167.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
Bleu: a method for automatic evalu-ation of machine translation.
In Proceedings of ACL,pages 311?318.Adam Pauls and Dan Klein.
2011.
Faster and smaller n-gram language models.
In Proceedings of ACL-HLT,pages 258?267, June.Chris Quirk, Arul Menezes, and Colin Cherry.
2005.
De-pendency treelet translation: Syntactically informedphrasal smt.
In Proceedings of ACL, pages 271?279.Libin Shen, Jinxi Xu, and Ralph Weischedel.
2008.
Anew string-to-dependency machine translation algo-rithm with a target dependency language model.
InProceedings of ACL-08:HLT, pages 577?585.Andreas Stolcke.
2002.
Srilm-an extensible languagemodeling toolkit.
In Proceedings of InternationalConference on Spoken Language Processing, pages901?904.Katsuhito Sudoh, Kevin Duh, Hajime Tsukada, MasaakiNagata, Xianchao Wu, Takuya Matsuzaki, andJun?ichi Tsujii.
2011.
Ntt-ut statistical machine trans-lation in ntcir-9 patentmt.
In Proceedings of NTCIR-9Workshop Meeting, pages 585?592, December.Xianchao Wu, Takuya Matsuzaki, and Jun?ichi Tsujii.2010.
Fine-grained tree-to-string translation rule ex-traction.
In Proceedings of ACL, pages 325?334, July.Xianchao Wu, Takuya Matsuzaki, and Jun?ichi Tsujii.2011a.
Effective use of function words for rule gen-eralization in forest-based translation.
In Proceedingsof ACL-HLT, pages 22?31, June.Xianchao Wu, Takuya Matsuzaki, and Jun?ichi Tsujii.2011b.
Smt systems in the university of tokyo forntcir-9 patentmt.
In Proceedings of NTCIR-9 Work-shop Meeting, pages 666?672, December.Dekai Wu.
1997.
Stochastic inversion transductiongrammars and bilingual parsing of parallel corpora.Computational Linguistics, 23(3):377?403.Deyi Xiong, Qun Liu, and Shouxun Lin.
2006.
Maxi-mum entropy based phrase reordering model for statis-tical machine translation.
In Proceedings of COLING-ACL, pages 521?528, July.Hui Zhang, Min Zhang, Haizhou Li, Aiti Aw, andChew Lim Tan.
2009.
Forest-based tree sequenceto string translation model.
In Proceedings of ACL-IJCNLP, pages 172?180, Suntec, Singapore, August.132
