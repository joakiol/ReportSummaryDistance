An Annotation Tool for Multimodal Dialogue Corporausing Global Document AnnotationKazunari ITO and Hiroaki SAITOKeio UniversityDepartment of Science for Open and Environmental Systems3-14-1 Hiyoshi, Kohoku-ku, Yokohama, Japan, 223-8522{k_ito , hxs}@nak.ics.keio.ac.jpAbstractThis paper reports a tool which assists theuser in annotating a video corpus and en-ables the user to search for a semantic orpragmatic structure in a GDA tagged cor-pus.
An XQL format is allowed for searchpatterns as well as a plain phrase.
Thistool is capable of generating a GDA time-stamped corpus from a video file  manu-ally.
It will be publicly available for aca-demic purposes.1 IntroductionTo achieve a natural communication environmentbetween computers and the users, many interactiveprototype systems that can talk with the user havebeen developed using multimodal information(face expressions, voice tones, gestures, etc .
).Since multimodalness of these systems is manuallybuilt in, achieving free and effective communica-tion or enhancing communication ablilities is noteasy.
Thus automatic learning from huge data ishoped for.Recently such various video data as TV dramas,news, and language teaching materials are avail-able, from which natural interactiveness should beextracted.
Such interactiveness from an intellectualcontent is also valuable for the fields of machinetranslation, information retrieval, handling ques-tion responses, and knowledge discovery systems.GDA1 (Global Document Annotation), which isan XML tag set, adds information on syntax, se-mantics, and pragmatics to texts (Hashida 1998).The texts with GDA organically corresponding tovoice and a video will contribute to the basic re-search into these technologies and promote the ap-plication development.2 GDA tagged corpusThis chapter explains the GDA tag set and amethod which relates tagged data with the videoimage.2.1 GDAThe GDA Initiative aims at having Internet authorsannotate their electronic documents with a com-mon standard tag set which allows machines toautomatically recognize the semantic and prag-matic structures of the documents.
A huge amountof annotated data is expected to emerge, whichshould serve not just as tagged linguistic corporabut also as a worldwide, self-extending knowl-edge-base mainly consisting of examples of howour knowledge manifests.
It describes the meaningof sentence analysis (semantics and pragmatics)basically.
It also describes information on the  sub-ject role, the rhetoric relation, and correspondence.Figure 1 shows an example of the text ?????????
(an ear is covered)?
tagged with GDA.Note that GDA is totally language independent,although all the following examples include Japa-nese texts.1 http://www.i-content.org/GDA/tagset.html<q who="A"><su syn=?fc?
id="kakure"><vp syn="f"><n arg="X">?
?</n><ad sem="obj">?</ad><adp syn="f"><v>??
?</v><ad>?</ad></adp><v>?</v></vp></su></q>Figure 1: A fragment of GDA corpusIn Figure 1, <q> represents a word where thatpart is spoken by someone, and the value of whoattribute shows who utters it.
<su> indicates a sen-tence, having no syntactic relation to other parts ofthe utterance.
Attribute ?syn?
means it is a syntaxstructure of the sentence, and ?fc?
means a forwardlink dependency.
<v> and <vp> elements mean averb and a verb phrase, respectively.
<n> elementrepresents a noun or a noun phrase.
<ad> and<adp> elements are an adverb, and a postpositionalphrase.
As these examples display, the GDA tagset has been determined to show syntactic structureeffectively where a word is assumed to be a unit.2.2 Adding Time -information to GDA taggedcorporaWhen you relate the video image file with its textfile, it is widely used to embed the time  informa-tion indicating when an utterance is spoken.
Wedefine the following two kinds of new formats torelate a video file with its GDA corpus file .1. btime and etime attributes: These attributes canbe added to an arbitrary tag.
Attribute btime showsthe start time of an utterance.
Attribute etimeshows the finished time.
The format is described asfollows.<any btime=?utterance start time(sec)?etime=?utterance end time(sec)?
>sentence</any>With these attributes, the voiceless section of anutterance can be precisely indicated and the over-lap event is detected in a multi-speaker environ-ment.2.
tst (time stamp) tag: Tag ?tst?
is an empty ele-ment tag and described in the following format.<tst val = ?utterance start time(sec)?
/>A tst tag can be inserted in an arbitrary place.
Theending time of an utterance can be determinedfrom the value of the next btime attribute or valattribute of the next tst tag.
Moreover, it is alsopossible that the tst tag has btime and etime attrib-ute.
Figure 2 shows an example when time infor-mation is added to the GDA tagged corpus inFigure 1.
You can see a man allocated ?A?
speaksa word ???
(an ear)?
from 60.81 to 61.03 sec, aword ??
(is)?
from 61.10 to 61.90 sec.<q who=?A?><su id=?kakure?><vp syn=?f?><n arg="X" btime=?60.815?etime="61.034?>?
?</n><ad sem=?obj?>?<tst val=?61.100?/></ad><adp syn=?f?><v>??
?<tst val="61.907"/></v><ad>?<tst val="62.192"/></ad></adp><v>?<tst val="62.383"/></v></vp></su></q>Figure 2: An example of GDA corpus with time-stampThese time annotation is often inserted afterGDA tagging, because time information is inde-pendent from the standard syntactic/pragmaticGDA.3 An annotation tool for multimodal dia-logue corpusWe have developed a multimodal annotation toolfor a video corpus and its annotated data (JEITA2001).
This tool runs on any platform that accom-modates Java2 and Java Media  Framework (JMF2)ver.2.0 or higher.
It also requires Java-based XMLparser Xerces-J 3  and Java-based XQL engine2 http://java.sun.com/products/java-media/jmf/3 http://xml.apache.org/xerces-j/GMD-IPSI XQL4.
Moreover, users can easily ex-tend functions by mounting plug-ins.
The proto-type is equipped with two different types of plug-ins.
One is ?XQL search plug-in?
in which the usercan search for various syntactic and semantic  pat-terns in a GDA corpus.
An XQL format query aswell as a plain word is allowed for search patterns.Another is ?Annotation plug-in?
which assists theannotator in generating a time-stamped GDA cor-pus from a video file.4 Basic FunctionsA screenshot of basic composition is shown inFigure 3.
The whole window is composed of sev-eral internal windows.Figure 3: Screenshot of basic  compositionThe left internal window is ?time table window?which displays each sentence in a table format.When the user clicks one of the columns in thetable, the video corresponding to the sentence isplayed.
Rows of the table are highlighted consecu-tively during that part of the video is being dis-played.
The top right is ?video image window?.
Itdisplays and plays the video image.5 Extended FunctionsThis section explains the outline of two plug-insfirst and usage of  these after that.5.1 XQL search plug-in4 http://xml.darmstadt.gmd.de/xql/Figure 4 shows the screenshot when the XQLsearch plug-in is loaded, you can see new windowappears at the bottom left of the tool.
The user cantype a query into the text field at the bottom of thenew window.Figure 4: Screenshot when XQL search plug-in isloadedAn acceptable query format is a plain text or aquery text defined by XQL (XML Query Lan-guage).
XQL is a subset of query language XQuerythat uses XML as a data  model, a recommendationby W3C.
XQL has already been mounted on soft-ware over many fields like Web browsers, docu-ment archiving systems, XML middleware, Perllibraries, and a command line utility.
XQL alwaysreturns a part of the document.
In XQL, the hierar-chy of the node is written by ?/?, an arbitrary hie r-archy is written by ?//?, the attribute name by?
@name?, the tag name by ?name?
as it is.
A regu-lar expression and a conditional expression are alsoacceptable.
For a exmple, ?
//q[@who='A']?
returns?q?
node with the value of attribute name ?who?equals ?A?.Figure 5 shows search results.
In Figure 5, fivewords [????
(a hair), ????
(a forelock), ???
(a prominent forehead), ??
(a face), ??
(anear)] are matched on the condition that the part ofspeech is noun and the value of the attribute ?arg?equals to ?X?
(see the fourth line of Figure 2).Moreover, since the label ?X?
is attached to theutterance "???
(this person)" in this GDA file,this query becomes a meaning of extracting a nounphrase whose subject is ?this person?.
The user cansearch for the subject even if an object is omittedin suited sentences.Figure 6:  Screenshot when Annoation plug-in is loadedA query of extracting a synonym of a certainword, only the utterance of a specific speaker toanother, and a sentence that of the response for acertain utterance, etc.
are possible.
Semantic  ordiscourse structures are also extractable if suchinformation is annotated in the file.
Clicking anycolumn of the table, the corresponding media sec-tion is played like the ?time table Window?.Figure 5: Result by query [a noun and value of at-tribute ?arg?
equals ?X?
]5.2 Annotation plug-inCreating a GDA file with timestamps record auto-matically with less time and less labor is indispen-sable to make large quantities of them and tospread them.
From the accuracy of the currentspeech recognition technology, it is difficult to at-tach timestamps record automatically and accu-rately by taking synchronization of a video image.Annotation plug-in increases the efficiency oftime-stamping a GDA file by visual operation.Figure 6 shows the screenshot when it is loaded.The window located on the lower part of Figure6 called ?Annotation board?
which displays infor-mation on a GDA file with a time-stamp visually.You can also see a horizontal axis which expressesthe time on media, and two layers in the board.Upper layer displays utterances of ?Speaker A?,lower layer displays ?Speaker B?
in this case.
Rec-tangles on one layer represent each speaker's utter-ances according to the time series.
The utteranceitseft is displayed in the rectangle, color of whichis different for each speaker.
Length and the pos i-tion indicate time information of the utterance.
Aperson edits the annotation by operating two kindsof lines on the board.
"Current line" shows a cur-rent playback position on the media.
"Base line"indicates the start or the end point of the time-stamp when the utterance sentence is newly in-serted.
Various functions (change of line?s position,and deletion or insertion of an utterance) can beexecuted by mouse operation.
When the annotatorclicks on the board, the ?Current line?
moves and aframe to which a video image corresponds is dis-played.
Thus, an annotator can attach the informa-tion of the start time and the end time and utterancetext itself manually.
A prototype system whichautomatically converts the GDA file from raw textfiles with a morphological analysis and a parsingtool in addition to the original filter has alreadybeen proposed (Suzuki 2001).6 Current developmentThe core functions of the tool are complete andstable.
Still, there is much room for expansion.
Thefollowing functions are being developed.6.1 User-friendly GUI-based search interfaceNeedless to say, there is no guarantee that a clauseor a sentence which agrees with an XQL queryexists.
Moreover the user has to know the XQLexpression for search.
We believe it necessary thata retrieval way by the top-down philosophy whichnarrows the candidates while presenting suitedclauses sequentially.It is very difficult at present for an XQL to ex-press dependency relations among the search con-dition.
Now, a query language of XML has beenintegrated into Xquery.
Hence, we are scheduled tobulid an Xquery engine.
A user-friendly GUI-based search window for the retrieval which doesnot require an explicit XQL query is currently be-ing developed.6.2 Uniting with other multimodal dataThere are many kinds of specifications to describemultimodal data.
For example, J-ToBI(Venditti,1995) which describes prosodic information ofvoice, FACS(Ekman, etc.
1978) which annotatesperson's expression, etc.
We are scheduled to de-sign the specification to integrate these informationinto GDA in the XML format.
As a result, the userwill be able to present a condition, for example, ofa word ?Truth?
of doubt type or ?I see?
of  hesita-tion.It is also scheduled to relate visual informationof video data with GDA.
They can contain infor-mation on motion, glance and the place of the ob-ject in video image.
A reverse-search whichextracts a corresponding text from visual informa-tion in a video image becomes available, too.6.3 Coordinated functionsWe intend to enhance a relation with other annota-tion tools.
Concretely, various formats of outputfiles can be taken in this tool in XML formats.
Weshall define a DTD (data conversion definition) toenable export and import to/from other tools.
Afunction of date exchange enables the user to en-hance flexibility and accessbility of this tool.6.4 Retrieval for multiple filesThe user can retrieve only a single file in a localmachine at present.
This tool will cope with theclient-server model that it requests retrieval de-mand to the corpora database servers on a network,downloads only necessary files to the local ma-chine and analyzes them.7 Related worksMost of recent multilmodal annotation tool pro-jects are almost Java-based, use XML for file  ex-change and have an object-oriented design:MATE (Carletta, etc.
1999) is an annotationworkbench that allows highly generic  specificationvia stylesheets that determine  shape andfunctionality of the user?s implemation.
Speed andstability for the tool are both still problematic forreal annotation.
Also, the generic approachincreases the initial effort to set up the tool sinceyou basically have to write your own tool using theMATE style language.EUDICO (Brugman et al 2000) is an effort toallow multi-user annotation of a centrally localtedcorpus via a web-based interface.
The tools that areavailable to work with the  multimodal corpusmake it possible to analyze their content and to addnew free defined annotations to them.
A EUDICOclient can choose a subset of the corpus data.Anvil (Kipp, 2001) is a generic video annota-tion tool which allows frame-accurate, hierarchicalmulti-layered annotation with objects that containattribute-value pairs.
Layers and attributes are alluser-defineable.
A time-aligned view and someconfiguration options make coding work quite in-tuitive.ATLAS project (Steven, etc.
2000) deals withall types of annotation and is theoretically based onthe idea of annotation graphs where nodes repre-sent time points and edges indicate annotation la-bels.8 ConclusionWe have reported an annotation tool for multimo-dal dialogue corpora.
This tool enables semanticand pragmatic search from a video data with anno-tated texts in the GDA format.
This tool is plat-form-independent and equipped with easy-to-useinterface.
It will be of use to researchers dealingwith multimodal dialogue for exploratory studiesas well as annotation.
Core functions are completeand various extension facilities are now being de-veloped.
This prototype will be publicly availablesoon.9 AcknowledgementsWe wish to express our gratitute for the membersof the committee on methods and standards of dia-logic content in Japan Electronics and InformationTechnology Industries Association (JEITA).
Espe-cially we would like to thank Koichi Hashida forinsightful advices.ReferencesHasida, K. (1998) Intellectual contents of all-roundbased on GDA meaning modification.
The transac-tion of Japanese Society for Artificial Intelligence.,Vol.
13, No.4, pp.528-535 (in Japanese).JEITA (2001) Servey Report about natural languageprocessing system.
pp.49-56(in Japanese).J.
Suzuki and K. Hasida (2001) Proposal of answer ex-traction system using GDA tag.
The 7th annual meet-ing of Language Processing Society (in Japanese).Ekman, P. and Friesen, W.(1978) Facial action codingsystem : a technique for the measurement of facia l-movement, Consulting Psychologists Press, 1978J.J.Venditti,(1995) Japanese ToBI Labelling Guide lines.Ohio-State University,Columbus,U.S.A.,1995.Michael Kipp (2001) Anvil - A Generic AnnotationTool for Multimodal Dialogue, Proceedings of Eu-rospeech 2001, pp.1367-1370.Carletta, J. and Isard, A (1999) The MATE AnnotationWorkbench, In Proceedings of the ACL Workshop,Towards Standards and Tools for Discourse Tagging.,pp.11-17.H.
Brugman, A. Russel, D. Broeder, and P.Wittenburg(2000) EUDICO.
Annotation and Exploitation ofMulti Media Corpora, Proceedings of LREC 2000Workshop.Steven Bird, David Day, John Garofolo, John Hender-son, Christophe Laprun, and Mark Liberman (2000)ATLAS: A Flexible and Extensible Architecture forLinguistic Annotation, Proceedings of the Second In-ternational Conference on Language Resource andEvaluation, pp.1699-1706.
