CORRECTING ILLEGAL NP  OMISS IONS US ING LOCAL FOCUSLinda Z. Suri 1Department of Computer and Information SciencesUniversity of DelawareNewark DE 19716Internet: suri@udel.edu1 INTRODUCTIONThe work described here is in the context of de-veloping a system that will correct he written En-liSh of native users of American Sign LanguageSL) who are learning English as a second lan-guage.
In this paper we focus on one error classthat we have found to be particularly prevalent:the illegal omission of NP's.Our previous analysis of the written English ofASL natives has led us to conclude that languagetransfer (LT) can explain many errors, and shouldthus be taken advantage of by an instructional sys-tem (Suri, 1991; Suri and McCoy, 1991).
We be-lieve that many of the omission errors we havefound are among the errors explainable by LT.Lillo-Martin (1991) investigates null argumentstructures in ASL.
She identifies two classes of ASLverbs that allow different types of null argumentstructures.
Plain verbs do not carry morphologicalmarkings for subject or object agreement and yetallow null argument structures in some contexts.These structures, she claims, are analogous to thenull argument structures found in languages (likeChinese) that allow a null argument if the argumentco-specifies the topic of a previous entence (ttuang,1984).
Such languages are said to be discourse-oriented languages.As it turns out, our writing samples collectedfrom deaf writers contain many instances of omit-ted NP's where those NP's are the topic of a pre-vious sentence and where the verb involved wouldbe a plain verb in ASL.
We believe these errors canbe explained as a result of the ASL native carry-ing over conventions of (discourse-oriented) ASL to(sentence-oriented) English.If this is the case, then these omissions can becorrected if we track the topic, or, in computa-tional linguistics terms, the local focus, and theactor focus.
2 We propose to do this by develop-ing a modified version of Sidner's focus trackingalgorithm (1979, 1983) that includes mechanismsfor handling complex sentence types and illegallyomitted NP's.1Thls research was supported in part  by NSF Grant~IRI-9010112.
Support  was also provided by the NemoursFotuldation.
We thank Gallaudet U~fiversity, the NationalTechnical Inst itute for the Deaf, the Pennsylvalfia School forthe Deaf, the Margaret S. Sterck School, and the BiculturalCenter for providing us with writing samples.2 Grosz, Joshi had Weinstein (1983) use the notion of cen-tering to track something similar to local focus and argueagainst he use of a separate actor focus.
However, we thinkthat the example they use does not argue against a separateactor focus, but il lustrates the need for extensions to Sial-her's algorithm to specify how complex sentences should beprocessed.2732 FOCUS TRACKINGOur focusing algorithm is based on Sidner's fo-cusing algorithm for tracking local and actor foci(Sidner 1979; Sidner 1983).
3 In each sentence, theactor focus (AF) is identified with the (thematic)agent of the sentence.
The Potential Actor FocusList (PAFL) contains all NP's that specify an ani-mate element of the database but are not the agentof the sentence.Tracking local focus is more complex.
The firstsentence in a text can be said to be about some-thing.
That something is called the current focus(.CF) of the sentence and can generally be identifiedvia syntactic means, taking into consideration thethematic roles of the elements in the sentence.
Inaddition to the CF, an initial sentence introducesa number of other items (any of which can becomethe focus of the next sentence).
Thus, these itemsare recorded in a potential focus list (PFL).At any given point in a well-formed text, afterthe first sentence, the writer has a number of op-tions:?
Continue talking about the same thing; in thiscase, the CF doesn't change.?
Talk about something just introduced; in thiscase, the CF is selected from the previous en-tence's PFL.?
Return to a topic of previous discussion; inthis case, that topic must have been the CF ofa previous entence.?
Discuss an item previously introduced, butwhich was not the topic of previous discussion;in this case, that item must have been on thePFL of a previous entence.The decision (by the reader/hearer/algorithm) asto which of these alternatives was chosen by thespeaker is based on the thematic roles (with par-ticular attention to the agent role) held by theanaphora of the current sentence, and whethertheir co-specification is the CF, a previous CF, ora member of the current PFL or a previous PFL.Confirmation of co-specifications requires inferenc-ing based on general knowledge and semantics.At each sentence in the discourse, the CF andPFL of the previous sentence are stacked for thepossibility of subsequent return.
4 When one ofthese items is returned to, the stacked CF's andPFL's above it are popped, and are thus no longeravailable for return.3 Carter.
(1987) extended Sichler s work to haaldle in-trasententlal naphora,  but  for space reasons we do not dis-cuss these extensions.4Sidner did not stack PFL's.
Our reasons for stackingPFL's  are discussed in section 4.2.1 F ILL ING IN A MISS ING NPWe propose extending this algorithm to iden-tify an illegally omitted NP.
To do this, we treatthe omitted NP as an anaphor which, like Sidner'streatment of full definite NP's and personal pro-nouns, co-specifies an element recorded by the fo-cusing algorithm.
This approach is based on thebelief that an omitted NP is likely to be the topic ofa previous entence.
We define preferences amongthe focus data structures which are similar to Sid-ner's preferences.More specifically, when we encounter an omit-ted NP that is not the agent, we first try to fillthe deleted NP with the CF of the immediatelypreceding sentence.
If syntax, semantics or infer-encing based on general knowledge cause this co-specification to be rejected, we then consider mem-bers of the PFL of the previous sentence as fillersfor the deleted NP.
If these too are rejected, we con-sider stacked CF's and elements of stacked PFL's,taking into account preferences (yet to be deter-mined) among these elements.When we encounter an omitted agent NP, in asimple sentence or a sentence-initial clause, we firsttest the AF of the previous entence as co-specifier,then members of the PAFL, the previous CF, andfinally stacked AF's, CF's and PAFL's.
To iden-tify a missing agent NP in a non-sentence-initialclause, our algorithm will first test the AF of theprevious clause, and then follow the same prefer-ences just given.
Further preferences are yet to bedetermined, including those between the stackedAF, stacked PAFL, and stacked CF.2.2 COMPUTING THE CFTo compute the CF of a sentence without anyillegally omitted NP's, we prefer the CF of the lastsentence over members of the PFL, and PFL mem-bers over members of the focus stacks.
Exceptionsto these preferences involve picking a non-agentanaphor co-specifying a PFL member over an agentco-specifying the CF, and preferring a PFL memberco-specified by a pronoun to the CF co-specified bya full definite description.To compute the CF of a sentence with an illegallyomitted NP, our algorithm treats illegally omittedNP's as anaphora since they (implicitly) co-specifysomething in the preceding discourse.
However, itis important o remember that discourse-orientedlanguages allow deletions of NP's that are the topicof the discourse.
Thus, we prefer a deleted non-agent as the focus, as long as it closely ties tothe previous entence.
Therefore, we prefer the co-specifier of the omitted non-agent NP as the (new)CF if it co-specifies either the last CF or a memberof the last PFL.
If the omitted NP is the thematicagent, we prefer for the new CF to be a pronomi-nal (or, as a second choice, full definite description)non-agent anaphor co-specifying either the last CFor a member of the last PFL (allowing the deletedagent NP to be the AF and keeping the AF and CFdifferent).
5 If no anaphor meets these criteria, then5As future work, we will explore how to resolve morethan one non-agent anaphor  in a sentence co-specifying PFLelements.274the members of the CF and PFL focus stacks willbe considered, testing a co-specifier of the omittedNP before co-specifiers of pronouns and definite de-scriptions at each stack level.3 EXAMPLEBelow, we describe the behavior of the extendedalgorithm on an example from our collected textscontaining both a deleted non-agent and agent.Example :  "($1) First, in summer I live at homewith my parenls.
($2) I can budget money easily.
($3) I did not spend lot of money at home becauseal home we have lot of good foods, I ate lot of foods.
(S4) While living at college I spend lot of moneybecause_ go out to eat almost everyday.
($5) Athome, sometimes my parents gave me some moneyright away when I need_.
"After S1, the AF is I, the CF is I, and the PFLcontains SUMMER, HOME, and the LIVE VP.
For $2,I is the only anaphor, so it becomes the CF, thePFL contains HONEY and the BUDGET VP, and thefocus stack contains I and the previous PFL.$3 is a complex sentence using the conjunction"because."
Such sentences are not explicitly han-dled by Sidner's algorithm.
Our analysis so farsuggests that we should not split this sentence intotwo 6, and should prefer elements of the main clauseas focus candidates.
Thus, we take the CF fromthe first clause, and rank other elements in thatclause before elements in the second clause on thePFL.
7 In this case, we have several anaphora: I,money, at home ....
The AF remains I.
The CF be-comes MONEY since it co-specifies a member of thePFL and since the co-specifier of the last CF is theagent.
Ordering the elements of the first clause be-fore the elements in the second results in the PFLcontaining HOME, the NOT SPEND VP, GOOD FOOD,and the HAVE VP.
We stack the CF and the PFL of$2.Note that $4 has a missing agent in the sec-ond clause.
To identify the missing agent in anon-sentence-initiM clause, our algorithm will firsttest the AF of the preceding clause for possible co-specification.
Because this co-specification wouldcause no contradiction, the omitted NP is filledwith 'T', which is eventually taken as the AF of$4.
The CF is computed by first considering thefirst clause of $4, since the X clause is the pre-ferred clause of an X BECAUSE Y construct.
Since"money" co-specifies the CF of $3, and nothing elsein the preferred clause co-specifies a member of thePFL, MONEY remains the CF.
The PFL containsCOLLEGE, the SPEND VP, EVER.Y DAY, the TO EATVP, and the GO OUT TO EAT VP.
We stack the CFand PFL of $3.$5 contains a subordinate clause with a miss-ing non-agent.
Our algorithm first considers the6If we were to split the sentence up, then tile focus wouldshift away from MONEY when we process the second clause(which contradicts our intuit ion of what the focus is in thisparagraph) .7The appropriateness of placing elements from bothclauses in one PFL and ranking them according to clausemenlbership will be further investigated.
This construct ("XBECAUSE Y") is further discussed in section 4.CF, MONEY, as the co-specifier of the omitted NP;syntax, semantics and general knowledge inferenc-ing do not prevent this co-specification, so it isadopted.
MONEY is also chosen as the CF since itis the co-specifier of the omitted NP occurring inthe verb complement clause which is the preferredclause in this type of construct.4 D ISCUSSION OF  EXTENSIONSOne of the major extensions needed in Sidner'salgorithm is a mechanism for handling complex sen-tences.
Based on a limited analysis of sample texts,we propose computing the CF and PFL of a com-plex sentence based on a classification of sentencetypes.
For instance, for a sentence of the form "XBECAUSE Y" or "BECAUSE Y, X", we prefer theexpected focus of the effect clause as CF, and or-der elements of the X clause on the PFL before el-ements of the Y clause.
Analogous PFL orderingsapply to other sentence types described here.
For asentence of the form "X CONJ Y", where X and Yare sentences, and CONJ is "and", "or", or "but",we prefer the expected focus of the Y clause.
For asentence of the form "IF X (THEN) Y", we preferthe expected focus of the THEN clause, while for"X, IF Y", we prefer the expected focus of the Xclause.
Further study is needed to determine otherpreferences and actions (including how to furtherorder elements on the PFL) for these and othersentence types.
These preferences will likely de-pend on thematic roles and syntactic riteria (e.g.,whether an element occurs in the clause containingthe expected CF).The decisions about how these and other exten-sions should proceed have been or will be based onanalysis of both standard written English and thewritten English of deaf students.
The algorithmwill be developed to match the intuitions of nativeEnglish speakers as to how focus shifts.A second difference between our algorithm andSidner's is that we stack the PFL's as well as theCF's.
We think that stacking the PFL's may beneeded for processing standard English (and notjust for our purposes) since focus sometimes re-volves around the theme of one of the clauses ofa complex sentence, and later returns to revolvearound items of another clause.
Further investiga-tion may indicate that we need to add new datastructures or enhance xisting ones to handle focusshifts related to these and other complex discoursepatterns.We should note that while we prefer the CF asthe co-specifier of an omitted NP, Sidner's recencyrule suggests that perhaps we should prefer a mem-ber of the PFL if it is the last constituent of theprevious entence (since a null argument seems im-ilar to pronominal reference).
However, our studiesshow that a rule analogous to the recency rule doesnot seem to be needed for resolving the co-specifierof an omitted NP.
In addition, Carter (1987) feelsthe recency rule leads to unreliable predictions forco-specifiers of pronouns.
Thus, we do not expectto change our algorithm to reflect the recency rule.
(We also believe we will abandon the recency rulefor resolving pronouns.
)275Another task is to specify focus preferencesamong stacked PFL's and stacked CF's, perhapsusing thematic and syntactic information.An important question raised by our analy-sis is how to handle a paragraph-initial, but notdiscourse-initial, sentence.
Do we want to treat itas discourse-initial, or as any other non-discourse-initial sentence?
We suggest (based on analysis ofsamples) that we should treat the sentence as anynon-discourse-initial sentence, unless its sentencetype matches one of a set of sentence types (whichoften mark focus movement from one element o anew one).
In this latter case, we will treat the sen-tence as discourse-initial by calculating the CF andPFL in the same manner as a discourse-initial sen-tence, but we will retain the focus stacks.
We haveidentified a number of sentence types that shouldbe included in the set of types which trigger thelatter treatment; we will explore whether other sen-tence types should be included in this set.5 CONCLUSIONSWe have discussed proposed extensions to Sid-ner's algorithm to track local focus in the pres-ence of illegally omitted NP's, and to use the ex-tended focusing algorithm to identify the intendedco-specifiers of omitted NP's.
This strategy is rea-sonable since LT may lead a native signer of ASLto use discourse-oriented strategies that allow theomission of an NP that is the topic of a precedingsentence when writing English.REFERENCESDavid Carter (1987).
Interpreting Anaphors inNatural Language Texts.
John Wiley and Sons,New York.Barbara J. Grosz, Aravind K. Joshi and Scott We-instein (1983).
Providing a unified account ofdefinite noun phrases in discourse.
In Proceed-ings of the 21st Annual Meeting of the Associa-tion for Computational Linguistics, 44-50.C.-T. James Huang (1984).
On the distributionand reference of empty pronouns.
Linguistic In-quiry, 15(4):531-574.Diane C. Lillo-Martin (1991).
Universal Grammarand American Sign Language.
Kluwer AcademicPublishers, Boston.Candace L. Sidner (1979).
Towards a Computa-tional Theory of Definite Anaphora Comprehen-sion in English Discourse.
Ph.D. thesis, M.I.T.,Cambridge, MA.Candace L. Sidner (1983).
Focusing in the com-prehension of definite anaphora.
In Robert C.Berwick and Michael Brady, eds., ComputationalModels of Discourse, chapter 5,267-330.
M.I.T.Press, Cambridge, MA.Linda Z. Sur i  and Kathleen F. McCoy (1991).Language transfer in deaf writing: A correctionmethodology for an instructional system.
TR-91-20, Dept.
of CIS, University of Delaware.Linda Z. Suri (1991).
Language transfer: A foun-dation for correcting the written English of ASLsigners.
TR-91-19, Dept.
of CIS, University ofDelaware.
