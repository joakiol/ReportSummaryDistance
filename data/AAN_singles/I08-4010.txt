The Fourth International Chinese Language Processing Bakeoff: ChineseWord Segmentation, Named Entity Recognition and Chinese POS TaggingGuangjin JinInstitute of Applied linguisticsM.O.E., P.R.C.No.51, Chaonei NanxiaojieDong Cheng District, Beijing, Chinaguangjin2000@163.comXiao ChenDept of Chinese, Translation & LinguisticsCity University of Hong Kong83 Tat Chee AvenueKowloon, Hong Kong, Chinacxiao2@student.cityu.edu.hkAbstractThe Fourth International Chinese LanguageProcessing Bakeoff was held in 2007 to as-sess the state of the art in three importanttasks: Chinese word segmentation, namedentity recognition and Chinese POS tagging.Twenty-eight groups submitted result sets inthe three tasks across two tracks and a totalof seven corpora.
Strong results have beenfound in all the tasks as well as continuingchallenges.1 IntroductionChinese is a kind of language which does not useword delimiters in its writing system.
Now a days,under the background of information explosion,many application oriented natural language process-ing task become more and more important, such asparsing and machine translation.
Chinese tokeniza-tion, as the foundation of many downstream pro-cessing tasks, has attracted lots of research interest.However, it is still a significant challenge for all theresearchers.SIGHAN, the Special Interest Group for ChineseLanguage Processing of the Association for Compu-tational Linguistics, conducted three prior word seg-mentation bakeoffs, in 2003, 2005 and 2006(Sproatand Emerson, 2003; Emerson, 2005; Levow, 2006),which established benchmarks for word segmenta-tion and named entity recognition.
The bakeoff pre-sentations at SIGHAN workshops highlighted newapproaches in this field.The fourth bakeoff was jointly held with the FirstCIPS Chinese Language Processing Evaluation inthe summer of 2007, and co-organized by SIGHAN,Chinese LDC, and the Verifying Center of ChineseLanguage and Character Standards of the State Lan-guage Commission of P.R.C.
In this bakeoff, wecontinue the Chinese word segmentation and namedentity recognition tasks.
Furthermore, a new evalu-ation task has been augmented, the task for ChinesePOS tagging.
In this evaluation task, a participatingsystem will take a given segmented corpus as the in-put, and only the POS tagging performance will beevaluated.
Both closed and open track are availablefor this task.2 Details of the Evaluation2.1 CorporaSeven corpora were provided for the evaluation:five in Simplified characters and two in traditionalcharacters.
The Simplified character corpora wereprovided by Microsoft Research Asia (MSRA) forNER, by University of Pennsylvania/University ofColorado (CTB) for WS and POS tagging, byPeking University for NER and POS tagging, byShanxi University for WS.
The Traditional char-acter corpora were provided by City University ofHong Kong (CITYU) for WS, NER and POS tag-ging, by the Chinese Knowledge Information Pro-cessing Laboratory (CKIP) of the Academia Sinica,Taiwan forWS and POS tagging.
Each data provideroffered separate training and test corpora.
Statisticalinformation for each corpus appears in Table1.
All69Sixth SIGHAN Workshop on Chinese Language Processingdata providers were requested to supply the trainingand test corpora in both the standard local encodingand in Unicode (UTF-16).
For all providers, missingencodings were transcoded by the organizers usingthe appropriate software.
Primary training and truthdata for word segmentation were generated by theorganizers via a C++ program by uniforming sen-tence end tags and delimiters.
For test data, all tagsremoved except sentence end tags.Comparable XML format data was also providedfor all corpora and all tasks.
Except as noted above,no additional changes were made to the data fur-nished by the providers.Table 1: Corpora for Bakeoff-4Source Encoding CWS NER TAGaCITYU BIG5HKSCS/UTF-16?
?
?CKIP BIG5/UTF-16?
?CTB GB/UTF-16?
?MSRA GB/UTF-16?NCC GB/UTF-16?
?PKU GB/UTF-16?SXU GB/UTF-16?aTAG:Chinese POS tagging2.2 Rules and ProceduresThe fourth Bakeoff followed the structure of the for-mer three word segmentation bakeoffs.
The onlydifference is that participating groups (?sites?)
reg-istered online and for those who could not accessour web site, email registration is acceptable; Onregistration, all the groups are asked to identify thecorpora and tasks of interest.
Training data was re-leased for download from the online registration sys-tem on August 25, 2007.
Test data was released onSeptember 25, 2007 and results were due 12:00 Bei-jing Time on September 28, 2007.
Scores for all sub-mitted runs were emailed to the individual groups onOctober 15, and were made available to all groupson a web page a few days later.Groups could participate in either or both of twotracks for each task and corpus:In the open track, participants could use any ex-ternal data they chose in addition to the providedtraining data.
Groups were required to specify thisinformation in their system descriptions.In the closed track, participants could only useinformation found in the provided training data.Groups were required to submit fully automatic runsand were prohibited from testing on corpora whichthey had previously used.Scoring was performed automatically using aC++ program.
In cases where naming errors or mi-nor divergences from required file formats arose, amix of manual intervention and automatic conver-sion was employed to enable scoring.
The primaryscoring program was made available to participantsfor follow up experiments.3 Participating sitesA total of 42 sites registered, and 28 submitted re-sults for scoring.
A summary of participating groupswith task and track information appears in Table 2.A total of 263 official runs were scored: 166 forword segmentation, 33 for named entity recognitionand 64 for POS tagging.4 Results and Discussion4.1 Word Segmentation Results & DiscussionThere are five corpus provided in the CWS track.The statistics for these corpora are in Table 3.
Weintroduce a type-token ration(TTR) to indicate thevocabulary diversity in each corpus.To provide a basis for comparison, we computedbaseline and possible topline scores for each ofthe corpora.
The baseline was constructed by left-to-right maximal match algorithm, using the train-ing corpus vocabulary.
The topline employed thesame procedure, but instead used the test vocabu-lary.
These results are shown in Tables 5 and 6.For the CWS task, we computed the following mea-sures: recall (R), precision (P), equally weighted F-measure (F = 2PR/(P + R)), the recall, preci-sion and F-measure on OOV (ROOV , POOV , FOOV ),and recall, precision and F-measure on in vocabu-lary words (RIV , PIV , FIV ).
In and out of vocabu-lary status are defined relative to the training corpus.Following previous bakeoffs, we employ the CentralLimit Theorem for Bernoulli trials (Grinstead and70Sixth SIGHAN Workshop on Chinese Language ProcessingSiteIDSiteNameCITYUCWSCKIPCWSCTBCWSNCCCWSSXUCWSCITYUNERMSRANERCITYUTAGCKIPTAGCTBTAGNCCTAGPKUTAG1InstituteofAutomation,ChineseAcademyofSciencesO2CityUniversityofHongKongCCCCCC(a-b)OC O3ComputingLaboratory,UniversityofOxfordOOOOO5Dept.ofDecisionSciences,TheChineseUniversityofHongKongCC OCC O(a-b)C7NaraInstituteofScienceandTechnology,JAPANC(a-d)C(a-d)C(a-d)C(a-d)C(a-d)8NanjingNormalUniversityCC(a-b)C(a-b)O(a-c)C(a-d)O(a-c)9NationalCentralUniversityCCCCCCCC11Instituteofsoftware,ChineseAcademyofSciencesO14SchoolofComputerandInformationTechnology,ShanxiUniversityO15TungnanUniversityCC16DepartmentofChinese,TranslationandLinguistics,CityUniversityofHongKongCCCC18InstituteofComputationalLinguistics,EECS,PekingUniversityC(a-b)CCC(a-c)C(a-c)C(a-c)C(a-b)O(a-b)19NiCT/ATRCCCCCCCCCC21FacultyofScienceandTechnologyofUniversityofMacau,INESCMacauC(a-d)C(a-b)C(a-d)C(a-b)C(a-b)C(a-b)C(a-b)22CenterofIntelligenceScienceandTechnologyRe-searchofBeijingUniversityofPostsandTelecommu-nicationsOOOO(a-b)OOO23TheChineseUniversityofHongKongOO24FranceTelecomR&DBeijingCo.LtdC(a-b)O(a-b)C(a-b)O(a-b)C(a-b)O(a-b)C(a-b)C(a-b)C OC OCCCC26MicrosoftResearchAsiaandNortheasternUniversityofChinaCCCCC27SimonFraserUniversityCCCCC28StateKeyLaboratoryofMachinePerception,CenterforInformationScience,SchoolofElectronicsEngineering&ComputerScience,PekingUniversityC OC OC O(a-b)C OC OC OCC OC OC OC OC O29ITNLPLab,ComputerScienceofTechnology,HarbinInstituteofTechnologyC OOCC O30Yahoo!IncC(a-b)C(a-b)C(a-b)C(a-b)31DalianUniversityofTechnologyOOC(a-d)O(a-b)C(a-d)O(a-b)C(a-d)O(a-b)COC(a-b)O(a-b)C(a-b)O(a-b)C(a-c)O(a-b)33PohangUniversityofScienceandTechnologyCCCCC34NOKIA(CHINA)INVESTMENTCO.,LTD.Nokiare-searchcenter,BeijingC OC37FudanuniversityCCCCC39LanguageComputerCorporationOOOOOOOOOOOOTable2:ParticipatingSitesbyCorpus,Task,andTrack71Sixth SIGHAN Workshop on Chinese Language ProcessingSnell, 1997) to compute 95% confidence interval as?2?p(1?p)n .Chinese Word Segmentation results for all runsgrouped by corpus and track appear in Tables 6-15;all tables are sorted by F-score.Across all corpora, the best closed track F-scorewas achieved in the SXU corpus at 0.9623.In theopen track, two systems that has exceeded thetopline in the CTB corpus, and there are also threeruns approaching the topline.
This might because ofthe overlapping of testing data in this bakeoff andthe training data in the last bakeoff.According to the statistics on all the corpus forthis bakeoff, there is no clear negative linear cor-relation between the OOV rate of a corpus and thehighest score achieved on it, since the OOV wordsare not the only obstacle for segmentation systemsto overcome.There are some difference in the segmentationscoring system between this bakeoff and the for-mer ones.
The precision and F-measure for both IVand OOV are appended.
It could be observed that,from the result tables in every corpus, the highesttotal F-measure is always coming up with the high-est OOV and IV F-measure rather than the recall ofthem.
So, we consider the F-measure of both IVand OOV words a more powerful indicator for theperformance of the segmentation systems in somesense.4.2 Named Entity Recognition Results &DiscussionThere are only two corpus CITYU and MSRA fornamed entity recognition task in this bakeoff.
Forstatistics, we compute the OOV rate of named en-tities for each corpus, which denotes the proportionof named entities in testing data that are not seen intraining corpus.For each submission for named entity recogni-tion, like the former bakeoff, we compute over-all phrase precision (P), recall(R), and F-measure(F), as well as the F-measure for each entity type(PER,ORG,LOC).
The only difference is the recalland precision for each entity type is appended.We compute a baseline for each corpus as in thebakeoff-3.
A left-to-right maximum match algo-rithm was applied on the testing data with a namedentity list generated from the training data.
This al-gorithm only detects those named entities with oneunique tag in training data, others are considered asincorrectly tagged.
These scores for all NER corporaare found in Table 18.Named entity recognition results for all runsgrouped by corpus and track appear in Tables 19-22;all tables are sorted by F-score.It is shown in the result table that the baselineand the system performance for MSRA corpus arebetter than those for CITYU corpus.
However,thestatistics is showing that the number of named en-tities in CITYU training corpus is twice as largeas the number in MSRA corpus.
The system per-formance for these two corpus are consist with theOOV rate for these two corpora.
Therefore,it seemsthat OOV named entities is a principal challengefor named entity recognition systems.
Furthermore,the F-measure of organization name recognition isthe lowest one in every participant?s result on ev-ery corpus.
This phenomenon is potentially imply-ing that the organization name is the most difficultone among the three categories of named entities.There are several systems participating both theclosed and open track on the same corpus.
All ofthem perform better in the open track.
This phe-nomenon is implying that proper external informa-tion can strongly affect the performance of namedentity recognition system.since the testing data MSRA is a subset of thetraining data for last bakeoff, two sites have achievednovelly high scores in the open track.4.3 POS Tagging Result & DiscussionThere are five corpora in the Chinese POS taggingtask, each of them is built on different tag set andtagging standard.
For statistics and evaluation, wedefine several terms for this task:?
Multi-tag words: the words that been assignedmore than one POS-tag in either the trainingcorpus or testing corpus.
For instance, if an IV72Sixth SIGHAN Workshop on Chinese Language ProcessingTable 3: Chinese Word Segmentation Training and Truth data statisticsTraining TruthSource Token WTa TTRb Token WT TTR OOVc ROOV dCITYU 1092687 43639 0.0399 235631 23303 0.0989 19382 0.0823CKIP 721549 48114 0.0667 90678 14662 0.1617 6718 0.0741CTB 642246 42159 0.0656 80700 12188 0.1510 4480 0.0555NCC 913466 58592 0.0641 152354 21352 0.1401 7218 0.0474SXU 528238 32484 0.0614 113527 12428 0.1095 5815 0.0512Table 4: Chinese Word Segmentation BaselineSource R P F ROOV POOV FOOV RIV PIV FIVCITYU .9006 .8225 .8598 .0970 .2262 .1358 .9727 .8424 .9029CKIP .8978 .8232 .8589 .0208 .0678 .0319 .9680 .8393 .8990CTB .8864 .8427 .8640 .0283 .0769 .0414 .9369 .8579 .8956NCC .9200 .8716 .8951 .0273 .1858 .0476 .9644 .8761 .9181SXU .9238 .8679 .8949 .0251 .0867 .0389 .9723 .8789 .9232Table 5: Chinese Word Segmentation ToplineSource R P F ROOV POOV FOOV RIV PIV FIVCITYU .9787 .9840 .9813 .9917 .9678 .9796 .9775 .9855 .9815CKIP .9823 .9880 .9852 .9932 .9642 .9784 .9815 .9900 .9857CTB .9710 .9825 .9767 .9920 .9707 .9812 .9698 .9832 .9764NCC .9735 .9817 .9776 .9933 .9203 .9554 .9725 .9850 .9787SXU .9820 .9867 .9844 .9942 .9480 .9705 .9813 .9890 .9851Table 6: CITYU: Word Segmentation: Closed TrackID RunID R Cr P Cp F ROOV POOV FOOV RIV PIV FIV2 .9526 .000875 .9493 .000903 .9510 .7495 .7912 .7698 .9708 .9626 .96675 .9513 .000887 .9430 .000955 .9471 .7339 .7752 .7540 .9707 .9570 .96388 .9465 .000927 .9443 .000945 .9454 .7721 .7244 .7475 .9621 .9653 .963724 a .9450 .000939 .9437 .000949 .9443 .7716 .7099 .7395 .9605 .9666 .963626 .9490 .000906 .9372 .000999 .9430 .6780 .7591 .7163 .9733 .9511 .962118 b .9421 .000962 .9339 .001023 .9380 .7074 .7050 .7062 .9631 .9543 .958728 .9367 .001003 .9377 .000996 .9372 .6295 .7394 .6800 .9642 .9526 .958427 .9386 .000988 .9325 .001033 .9355 .6708 .6840 .6773 .9626 .9541 .958418 a .9296 .001054 .9290 .001058 .9293 .6862 .6541 .6698 .9514 .9549 .953233 .9285 .001061 .9261 .001077 .9273 .6866 .6326 .6585 .9502 .9548 .95257 c .9237 .001093 .9234 .001095 .9236 .6830 .5934 .6350 .9453 .9579 .95167 b .9237 .001093 .9234 .001095 .9236 .6830 .5934 .6350 .9453 .9579 .95167 a .9238 .001093 .9234 .001095 .9236 .6830 .5934 .6351 .9453 .9579 .95167 d .9197 .001119 .9169 .001137 .9183 .6558 .5690 .6093 .9434 .9532 .948315 .9191 .001123 .9014 .001228 .9102 .5466 .5588 .5527 .9525 .9308 .941521 b .9219 .001105 .8951 .001262 .9083 .4703 .5899 .5234 .9624 .9159 .938621 a .9221 .001104 .8947 .001264 .9082 .4697 .5891 .5227 .9627 .9155 .938521 d .9120 .001167 .8974 .001250 .9047 .5263 .5333 .5297 .9466 .9290 .937719 .8884 .001296 .8817 .001330 .8850 .6114 .6030 .6072 .9133 .9069 .910121 c .0155 .000509 .0155 .000508 .0155 .0047 .0049 .0048 .0165 .0164 .0165Table 7: CITYU: Word Segmentation: Open TrackID RunID R Cr P Cp F ROOV POOV FOOV RIV PIV FIV24 a .9670 .000736 .9725 .000674 .9697 .8988 .8525 .8750 .9731 .9839 .978524 b .9657 .000750 .9715 .000685 .9686 .8963 .8411 .8678 .9719 .9841 .978039 .9181 .001129 .9024 .001222 .9102 .6656 .5843 .6223 .9407 .9346 .937728 .8860 .001309 .9349 .001016 .9098 .6595 .5657 .6090 .9063 .9764 .94013 .0445 .000862 .0446 .000863 .0446 .0226 .0229 .0227 .0465 .0466 .0465aWT: word type.bTTR: type-token ratio = type count / token count.cOOV: number of OOV.dROOV : OOV Rate73Sixth SIGHAN Workshop on Chinese Language ProcessingTable 8: CKIP: Word Segmentation: Closed TrackID RunID R Cr P Cp F ROOV POOV FOOV RIV PIV FIV2 .9501 .001445 .9440 .001527 .9470 .7404 .7649 .7524 .9669 .9577 .962326 .9497 .001451 .9361 .001624 .9429 .6556 .7481 .6988 .9732 .9490 .96105 .9455 .001507 .9371 .001612 .9413 .7004 .7373 .7184 .9651 .9521 .958628 .9383 .001597 .9396 .001582 .9390 .6962 .6780 .6870 .9577 .9612 .959419 .9432 .001536 .9333 .001657 .9383 .6882 .6885 .6883 .9637 .9527 .95818 a .9412 .001562 .9345 .001643 .9378 .7228 .6688 .6948 .9586 .9575 .958018 .9369 .001615 .9270 .001727 .9319 .6636 .6624 .6630 .9587 .9480 .953324 a .9345 .001643 .9289 .001707 .9317 .7124 .6602 .6853 .9522 .9521 .952224 b .9336 .001653 .9277 .001720 .9306 .7091 .6589 .6831 .9515 .9508 .951227 .9354 .001632 .9173 .001828 .9263 .5521 .6877 .6125 .9661 .9316 .94858 b .9247 .001753 .9162 .001840 .9204 .6859 .5896 .6341 .9438 .9467 .945233 .9241 .001758 .9165 .001836 .9203 .6746 .6195 .6459 .9441 .9424 .94327 c .9233 .001767 .9161 .001841 .9197 .6801 .5846 .6287 .9428 .9471 .94497 a .9233 .001767 .9162 .001840 .9197 .6801 .5849 .6289 .9428 .9471 .94507 d .9224 .001777 .9153 .001849 .9188 .6672 .5732 .6166 .9428 .9473 .945015 .9150 .001852 .9001 .001991 .9075 .4751 .5689 .5178 .9502 .9216 .935621 b .9074 .001925 .8897 .002080 .8985 .4405 .5020 .4692 .9447 .9161 .930221 a .9076 .001923 .8896 .002081 .8985 .4406 .5028 .4697 .9449 .9159 .93027 b .8588 .002312 .8850 .002118 .8717 .6204 .4183 .4997 .8779 .9447 .9101Table 9: CKIP: Word Segmentation: Open TrackID RunID R Cr P Cp F ROOV POOV FOOV RIV PIV FIV5 .9586 .001323 .9541 .001389 .9563 .7804 .8050 .7925 .9728 .9656 .969228 .9507 .001438 .9503 .001443 .9505 .7391 .7704 .7544 .9676 .964 .965824 b .9367 .001616 .9360 .001625 .9364 .7527 .6911 .7206 .9515 .9575 .954524 a .9324 .001667 .9326 .001665 .9325 .7459 .6631 .7021 .9473 .9571 .952239 .9218 .001782 .8960 .002027 .9087 .6454 .5901 .6165 .944 .9221 .93293 .3977 .003245 .3944 .003240 .3961 .3405 .3359 .3382 .4025 .3994 .4009Table 10: CTB: Word Segmentation: Closed TrackID RunID R Cr P Cp F ROOV POOV FOOV RIV PIV FIV2 .9583 .001408 .9596 .001386 .9589 .7730 .7761 .7745 .9691 .9704 .969726 .9538 .001477 .9527 .001493 .9533 .7031 .7491 .7254 .9685 .9639 .966231 b .9505 .001527 .9528 .001492 .9517 .7580 .6886 .7216 .9618 .9701 .965931 a .9503 .001529 .9520 .001505 .9512 .7540 .6845 .7176 .9619 .9694 .965627 .9494 .001543 .9508 .001522 .9501 .7208 .7012 .7108 .9628 .9659 .964418 .9487 .001553 .9514 .001513 .9500 .7507 .6753 .7110 .9603 .9696 .96508 b .9482 .001560 .9516 .001511 .9499 .7596 .6740 .7142 .9592 .9702 .96478 a .9481 .001561 .9514 .001513 .9498 .7614 .6742 .7152 .9591 .9700 .964531 d .9487 .001552 .9509 .001520 .9498 .7583 .6812 .7177 .9599 .9687 .96439 .9471 .001575 .9500 .001533 .9486 .7670 .6736 .7173 .9577 .9688 .963224 a .9451 .001603 .9521 .001503 .9486 .7694 .6714 .7171 .9555 .9713 .963331 c .9495 .001542 .9474 .001571 .9485 .6638 .7456 .7023 .9663 .9579 .962128 .9429 .001633 .9535 .001481 .9482 .7536 .6661 .7072 .954 .9730 .963424 b .9456 .001596 .9492 .001545 .9474 .7565 .6613 .7057 .9567 .9688 .96275 .9434 .001626 .9459 .001592 .9447 .6911 .6883 .6897 .9582 .9612 .959737 .9459 .001592 .9418 .001648 .9439 .6589 .6698 .6643 .9628 .9574 .960133 .9402 .001669 .9433 .001628 .9417 .7317 .6517 .6894 .9524 .9628 .95767 c .9350 .001736 .9378 .001700 .9364 .7132 .5796 .6395 .9480 .9641 .95607 a .9350 .001735 .9379 .001699 .9364 .7132 .5800 .6397 .9480 .9642 .95607 d .9342 .001745 .9366 .001715 .9354 .6998 .5706 .6286 .9480 .9634 .95567 b .9099 .002015 .9250 .001854 .9174 .6911 .4834 .5689 .9227 .9638 .942821 b .9077 .002037 .9078 .002037 .9077 .4728 .5603 .5128 .9333 .9248 .929021 a .9078 .002037 .9073 .002041 .9075 .4703 .5583 .5105 .9335 .9244 .928921 d .8992 .002119 .9063 .002051 .9027 .5301 .5029 .5161 .9209 .9316 .926221 c .8992 .002119 .9062 .002052 .9027 .5299 .5029 .5160 .9210 .9315 .926219 .8773 .002310 .8788 .002297 .8780 .6714 .5886 .6273 .8894 .8985 .893974Sixth SIGHAN Workshop on Chinese Language ProcessingTable 11: CTB: Word Segmentation: Open TrackID RunID R Cr P Cp F ROOV POOV FOOV RIV PIV FIV28 a .9914 .000648 .9926 .000602 .9920 .9685 .9623 .9654 .9928 .9944 .993624 a .9760 .001077 .9826 .000920 .9793 .9420 .8655 .9021 .9780 .9902 .984031 a .9766 .001065 .9721 .001158 .9743 .9089 .8553 .8813 .9805 .9794 .980024 b .9702 .001196 .9753 .001092 .9728 .9145 .8361 .8736 .9735 .9844 .978928 b .9665 .001266 .9738 .001123 .9702 .8821 .8857 .8839 .9715 .9790 .975331 b .9589 .001397 .9612 .001359 .9601 .7922 .7902 .7912 .9687 .9713 .97003 .9485 .001556 .9498 .001536 .9491 .7261 .6769 .7006 .9615 .9672 .964339 .9461 .001590 .9372 .001707 .9416 .7223 .6764 .6986 .9592 .9535 .95638 a .9370 .001710 .9321 .001770 .9346 .6556 .6139 .6341 .9535 .9521 .95288 b .9270 .001831 .9319 .001773 .9294 .6576 .6099 .6329 .9428 .9525 .947622 .9251 .001853 .9261 .001841 .9256 .5967 .7337 .6581 .9444 .9352 .93988 c .9089 .002025 .8346 .002615 .8702 .2011 .3336 .2509 .9505 .8505 .8977Table 12: NCC: Word Segmentation: Closed TrackID RunID R Cr P Cp F ROOV POOV FOOV RIV PIV FIV2 .9402 .001214 .9407 .001210 .9405 .6179 .5984 .6080 .9562 .9583 .957326 .9452 .001166 .9320 .001289 .9386 .4502 .6196 .5215 .9698 .9430 .95625 .9365 .001249 .9365 .001249 .9365 .6158 .5542 .5834 .9524 .9577 .955134 .9417 .001200 .9272 .001331 .9344 .4001 .6454 .4940 .9687 .9356 .951831 b .9387 .001229 .9301 .001306 .9344 .5561 .5728 .5643 .9577 .9472 .952431 a .9389 .001226 .9298 .001309 .9343 .5556 .5743 .5648 .9580 .9467 .952337 .9396 .001220 .9286 .001319 .9341 .5007 .5411 .5201 .9614 .9462 .953719 .9328 .001282 .9353 .001260 .9340 .5907 .5218 .5542 .9498 .9588 .954331 d .9307 .001301 .9318 .001292 .9312 .6309 .5222 .5715 .9456 .9566 .951131 c .9380 .001235 .9223 .001371 .9301 .4709 .6247 .5370 .9613 .9331 .94724 a .9251 .001348 .9347 .001266 .9299 .6577 .4968 .5660 .9384 .9643 .951227 .9300 .001307 .9291 .001314 .9296 .5459 .5138 .5294 .9491 .9511 .950124 b .9246 .001352 .9332 .001279 .9289 .6524 .4932 .5617 .9381 .9629 .950328 .9193 .001395 .9378 .001237 .9285 .6516 .4833 .5549 .9326 .9695 .950718 b .9278 .001326 .9250 .001349 .9264 .5529 .4966 .5232 .9464 .9488 .947629 .9268 .001334 .9260 .001341 .9264 .6094 .4948 .5462 .9426 .9527 .947618 a .9278 .001326 .9249 .001350 .9263 .5486 .4940 .5199 .9466 .9488 .947718 c .9264 .001338 .9241 .001356 .9253 .5707 .4977 .5317 .9441 .9486 .94639 .9236 .001361 .9269 .001333 .9252 .6474 .4941 .5604 .9373 .9556 .94647 c .9086 .001476 .9110 .001459 .9098 .5957 .4080 .4843 .9241 .9485 .93617 d .9071 .001487 .9106 .001461 .9088 .5907 .3987 .4761 .9228 .9494 .935921 a .8997 .001539 .8992 .001542 .8995 .4232 .3710 .3954 .9234 .9294 .926421 b .8995 .001540 .8992 .001542 .8994 .4224 .3702 .3946 .9233 .9295 .92647 a .7804 .002121 .8581 .001788 .8174 .5409 .2134 .3060 .7924 .9561 .86667 b .7747 .002140 .8513 .001823 .8112 .5405 .2014 .2935 .7864 .9568 .863333 .3082 .002367 .3073 .002365 .3078 .2217 .1678 .1910 .3125 .3166 .3145Table 13: NCC: Word Segmentation: Open TrackID RunID R Cr P Cp F ROOV POOV FOOV RIV PIV FIV34 .9735 .000823 .9779 .000753 .9757 .8893 .8867 .8880 .9777 .9824 .980022 .9568 .001041 .9616 .000984 .9592 .8264 .8144 .8204 .9633 .9691 .966231 b .9620 .000980 .9496 .001120 .9557 .6337 .7673 .6941 .9783 .9569 .967531 a .9528 .001086 .9478 .001139 .9503 .7109 .7619 .7355 .9648 .9563 .96065 a .9440 .001177 .9517 .001098 .9478 .7305 .6381 .6812 .9547 .9698 .96225 b .9376 .001239 .9521 .001093 .9448 .7826 .6110 .6862 .9453 .9745 .959714 .9446 .001171 .9263 .001339 .9354 .4643 .7160 .5633 .9685 .9328 .95033 .9324 .001286 .9349 .001263 .9337 .6070 .5296 .5657 .9486 .9583 .953428 .9191 .001396 .9380 .001235 .9285 .6543 .4840 .5564 .9323 .9697 .950629 .9268 .001334 .9279 .001325 .9273 .6265 .5032 .5581 .9417 .9546 .948139 .9323 .001287 .9134 .001440 .9228 .6075 .5820 .5945 .9485 .9303 .939375Sixth SIGHAN Workshop on Chinese Language ProcessingTable 14: SXU: Word Segmentation: Closed TrackID RunID R Cr P Cp F ROOV POOV FOOV RIV PIV FIV2 .9622 .001132 .9625 .001127 .9623 .7429 .7159 .7292 .974 .9764 .975226 .9623 .001131 .9554 .001225 .9588 .6454 .7022 .6726 .9794 .9678 .973628 .9549 .001231 .9611 .001148 .9580 .6626 .6639 .6632 .9707 .9772 .973918 b .9543 .001239 .9568 .001206 .9556 .7273 .6232 .6712 .9666 .9781 .97235 .9558 .001219 .9552 .001228 .9555 .6922 .6638 .6777 .9701 .9716 .970824 a .9523 .001264 .9569 .001205 .9546 .7506 .6129 .6748 .9632 .9801 .971618 c .9528 .001258 .9560 .001217 .9544 .7369 .6164 .6713 .9645 .9782 .971331 a .9594 .001171 .9493 .001302 .9543 .6653 .6694 .6674 .9753 .9642 .96978 a .9534 .001250 .9544 .001238 .9539 .7395 .6275 .6789 .9650 .9754 .97028 b .9536 .001248 .9541 .001242 .9538 .7352 .6287 .6778 .9654 .9748 .970131 d .9535 .001249 .9532 .001253 .9533 .7305 .6257 .6741 .9656 .9740 .969831 b .9593 .001173 .9474 .001324 .9533 .6463 .6749 .6603 .9762 .9613 .968718 a .9518 .001270 .9547 .001234 .9533 .7020 .6020 .6481 .9653 .9772 .97128 d .9512 .001278 .9553 .001226 .9532 .7462 .6275 .6817 .9623 .9767 .96948 c .9509 .001282 .9544 .001238 .9526 .7396 .6281 .6793 .9623 .9754 .968824 b .9499 .001295 .9536 .001249 .9517 .7271 .5966 .6554 .9619 .9774 .969627 .9514 .001276 .9511 .001279 .9512 .6834 .6202 .6502 .9658 .9709 .96849 .9505 .001287 .9515 .001275 .9510 .7326 .6106 .6660 .9623 .9738 .968037 .9554 .001224 .9459 .001342 .9507 .6206 .6113 .6159 .9735 .9641 .968834 .9558 .001220 .9442 .001362 .9500 .5176 .6966 .5939 .9794 .9539 .966531 c .9558 .001219 .9441 .001363 .9499 .5788 .7154 .6399 .9762 .9539 .964933 .9387 .001423 .9392 .001418 .9390 .6741 .5627 .6134 .9530 .9638 .95847 a .9378 .001434 .9390 .001420 .9384 .6731 .5110 .5810 .9520 .9701 .96107 b .9376 .001435 .9391 .001419 .9383 .6729 .5107 .5807 .9519 .9701 .96097 c .9377 .001434 .9389 .001421 .9383 .6731 .5110 .5810 .9520 .9699 .96097 d .9360 .001452 .9369 .001443 .9365 .6550 .4949 .5638 .9512 .9691 .960021 b .9185 .001624 .9107 .001692 .9146 .4898 .4423 .4648 .9416 .9386 .940121 a .9185 .001624 .9106 .001693 .9145 .4886 .4414 .4638 .9417 .9386 .940119 .7820 .002450 .7793 .002460 .7807 .4969 .3538 .4133 .7976 .8125 .8050Table 15: SXU: Word Segmentation: Open TrackID RunID R Cr P Cp F ROOV POOV FOOV RIV PIV FIV31 a .9768 .000894 .9703 .001007 .9735 .7825 .8415 .8109 .9872 .9767 .982031 b .9738 .000948 .9620 .001134 .9679 .7089 .8040 .7534 .9881 .9694 .978628 .9547 .001233 .9622 .001132 .9584 .6705 .6628 .6666 .9701 .9787 .97448 a .9545 .001236 .9572 .001201 .9559 .7543 .6400 .6925 .9654 .9776 .97148 b .9639 .001108 .9479 .001319 .9558 .6103 .7089 .6559 .9829 .9587 .97078 c .9586 .001182 .9467 .001333 .9526 .6126 .6967 .6519 .9773 .9583 .967739 .9575 .001197 .9461 .001339 .9518 .7274 .6920 .7093 .9699 .9604 .96523 .9516 .001273 .9515 .001275 .9516 .6843 .6174 .6491 .9661 .9716 .968822 .8777 .001945 .8705 .001993 .8741 .5621 .6371 .5972 .8947 .8815 .8880word has only one POS-tag in the training cor-pus, but has other POS-tags in the testing cor-pus, it is a multi-tag word.?
OOV tag: If a tag of a word is found in the testcorpus, but not in the training corpus, or theword itself is an OOV word, the correspondingword-tag pair is called OOV tag.?
IV tag: if the pair of word and tag does occurin the training corpus, the pair is called IV tag.?
IV multi-tag words: the multi-tag words thatoccurred in training data.For each submission, we compute total accuracy(ATotal),IV recall (RIV ), OOV recall (ROOV ), andIV Multi-tag word recall (RMTIV ) for evaluation.The formula for total accuracy is: ATotal =NcorrectNtruth,where Ncorrect denotes the number of words that arecorrectly tagged, and Ntruth denotes the number ofwords in the truth corpus.The recall for IV, OOV and IV Multi-tag wordsare supposed to indicate participating system?s per-formance on these three categories.As Chinese word segmentation task, a baselineand a topline for each corpus are computed to reflect76Sixth SIGHAN Workshop on Chinese Language ProcessingTable 16: Named Entity Recognition Training and Truth data statisticsTraining TruthSource NEa PERb LOCc ORGd NE PER LOC ORGCITYU 66255 16552 36213 13490 13014 4940 4847 3227MSRA 37811 9028 18522 10261 7707 1864 3658 2185Table 17: Named Entity Recognition Truth data OOV statisticsNE PER LOC ORGSource OOV ROOV e OOV ROOV OOV ROOV OOV ROOVCITYU 6354 0.4882 3878 0.7850 900 0.1857 1576 0.4884MSRA 1651 0.2142 564 0.3026 315 0.0861 772 0.3533Table 18: Named Entity Recognition BaselineSource R P F RPER PPER FPER RLOC PLOC FLOC RORG PORG FORGCITYU .4912 .7562 .5955 .2130 .7056 .3272 .7681 .8438 .8042 .5011 .6341 .5598MSRA .5451 .6937 .6105 .6459 .9205 .7591 .4513 .7847 .5731 .6160 .5091 .5575Table 19: CITYU: Named Entity Recognition: Closed TrackID RunID R P F RPER PPER FPER RLOC PLOC FLOC RORG PORG FORG24 .8247 .8768 .8499 .8615 .9240 .8917 .9098 .8612 .8848 .6402 .8221 .71992 a .7556 .8850 .8152 .7688 .9165 .8362 .8659 .8695 .8677 .5699 .8589 .68522 b .7541 .8846 .8142 .7638 .9167 .8333 .8675 .8684 .8680 .5689 .8596 .684718 c .7608 .8751 .8140 .7771 .9143 .8401 .8692 .8551 .8621 .5730 .8451 .682928 .7570 .8585 .8046 .7682 .8976 .8279 .8750 .8314 .8526 .5624 .8462 .675718 b .7286 .8933 .8026 .7306 .9254 .8165 .8535 .8789 .8660 .5380 .8650 .663418 a .7277 .8926 .8017 .7287 .9252 .8153 .8529 .8781 .8653 .5380 .8633 .662821 a .0874 .1058 .0957 .0656 .0962 .0780 .1388 .1200 .1288 .0437 .0789 .056221 b .0211 .0326 .0256 .0128 .0218 .0161 .0390 .0433 .0410 .0068 .0192 .0101Table 20: CITYU: Named Entity Recognition: Open TrackID RunID R P F RPER PPER FPER RLOC PLOC FLOC RORG PORG FORG23 .8743 .9342 .9033 .9526 .9721 .9623 .9342 .9235 .9288 .6644 .8805 .75732 .8579 .9179 .8869 .8822 .9449 .9125 .9336 .9099 .9216 .7072 .8852 .786228 .8826 .8826 .8826 .9168 .8947 .9056 .9329 .8942 .9132 .7546 .8411 .795524 .8975 .8616 .8792 .9474 .9153 .9311 .9389 .8966 .9173 .7589 .7274 .742839 .7163 .8000 .7559 .7180 .8194 .7653 .8389 .7845 .8108 .5296 .7986 .6369Table 21: MSRA: Named Entity Recognition: Closed TrackID RunID R P F RPER PPER FPER RLOC PLOC FLOC RORG PORG FORG24 .9186 .9377 .9281 .9437 .9665 .9549 .9423 .9428 .9426 .8577 .9036 .880018 b .8862 .9304 .9078 .9195 .9651 .9418 .9043 .9379 .9208 .8275 .8871 .85632 .8779 .9274 .9020 .9029 .9628 .9319 .9101 .9341 .9219 .8027 .8841 .841418 a .8752 .9255 .8996 .9040 .9618 .9320 .8991 .9346 .9165 .8105 .8780 .842928 .8822 .9156 .8986 .9126 .9461 .9290 .9079 .9248 .9163 .8133 .8724 .841831 .8058 .9107 .8550 .9029 .9519 .9268 .8185 .9278 .8697 .7016 .8405 .764837 .8331 .8730 .8526 .8557 .8084 .8314 .8576 .9138 .8848 .7730 .8666 .8171aNE: Number of Named Entities.bPER: Number of Person names.cLOC: Number of Location names.dORG: Number of Organization nameseROOV :OOV rate77Sixth SIGHAN Workshop on Chinese Language ProcessingTable 22: MSRA: Named Entity Recognition: Open TrackID RunID R P F RPER PPER FPER RLOC PLOC FLOC RORG PORG FORG24 .9995 .9982 .9988 1 .9989 .9995 .9997 .9975 .9986 .9986 .9986 .99862 .9961 .9956 .9958 1 1 1 .9992 .9929 .9960 .9876 .9963 .99201 .9377 .9603 .9489 .9657 .9574 .9615 .9593 .9769 .9680 .8778 .9338 .904923 .9111 .9471 .9288 .9458 .9833 .9642 .9336 .9397 .9366 .8439 .9280 .884018 a .9135 .9321 .9227 .9560 .9601 .9581 .9221 .9388 .9304 .8627 .8959 .879018 b .9084 .9278 .9180 .9544 .9575 .9559 .9169 .9322 .9245 .8549 .8938 .873922 b .8675 .9163 .8912 .9217 .9630 .9419 .8445 .9352 .8875 .8600 .8502 .855129 .8791 .9035 .8911 .9549 .9498 .9524 .9194 .9129 .9161 .7469 .8408 .791111 .8674 .9003 .8836 .9083 .9216 .9149 .8989 .9166 .9077 .7799 .8516 .814131 .8238 .9038 .8619 .9206 .9517 .9359 .8362 .9424 .8862 .7204 .7966 .756522 a .8452 .8720 .8584 .8734 .9498 .9100 .8710 .8909 .8808 .7780 .7798 .778939 .7890 .8347 .8112 .8771 .9196 .8979 .8365 .8331 .8348 .6343 .7557 .6897the different degree of difficulty of tagging individ-ual corpora.
The algorithm of baseline and topline isbriefly described as follows: Baseline indicates thedifferent degree of difficulty of tagging individualcorpus.The baseline of each corpus is calculated by gen-erating a list of words and POS tags from the train-ing corpus, then: 1. tagging those IV words in thetesting corpus which have only one POS tag in thelist.
2. for those IV words that have not only onetag in training corpus, the unique most frequent tagin training corpus will be assigned to them.
3. foreach IV word that does not have a unique most fre-quent tag in training corpus, one of its tag which ismost frequent in the overall phase is assigned to it;4. for those words that do not fall into any of theformer three categories are assigned with a overallmost frequent tag.The topline algorithm is similar to baseline, in-stead the list of words and POS tags is generatedfrom testing corpus.Chinese POS tagging results for all runs groupedby corpus and track appear in Tables 27-36; all ta-bles are sorted by ATotal.The baseline and topline has shown that, with pre-liminary knowledge and mechanical algorithm, itis easy to achieve an accuracy over approximately0.85.
When excluding the effect caused by OOVtags, the accuracy can even be over 0.93.There are two kind of problem in POS taggingtask we should cope with: multi tag disambiguationand unknown words guessing.
We could considerthat the value of (topline - baseline) is the accuracydrop caused by unknown words guessing, and thevalue of (1 - topline) is the accuracy drop causedby multi tag disambiguation.
The average of thesetwo value is 0.0628 and 0.0600, therefore these twokind of problem can equally affect the performanceof POS tagging system.For this reason, unlike the topline of Chineseword segmentation, the topline of Chinese POS tag-ging could be easily exceeded by tagging systems,because the algorithm of this topline just excludesthe effect of OOV tags, which is not a dominant de-terminant in this task.In closed track, the highest total accuracy isachieved in the NCC corpus which has the low-est OOV tag rate, and the lowest total accuracy isachieved in the CITYU corpus which has the high-est OOV tag rate.Most of the participants outperformed baseline,some have exceeded topline.
When comparingthe OOV recall and IV multi tag word recall withtopline, participant?s system can easily approachingor surpass the IV multi tag word recall, but nonesystem could successfully approach the OOV recall.This might because participant?s systems do better insolving the multi tag disambiguation problem thanin coping with the unknown words guessing prob-lem.5 Conclusions & Future DirectionsThe Fourth SIGHAN Chinese Language ProcessingBakeoff successfully brought together a collectionof 28 strong research groups to assess the progressof research in three important tasks, Chinese word78Sixth SIGHAN Workshop on Chinese Language Processingsegmentation, named entity recognition and Chi-nese POS tagging, that in turn enable other impor-tant language processing technologies.
The individ-ual group presentations at the SIGHAN workshopwill detail the approaches that yielded strong perfor-mance for both tasks.
Issues of out-of-vocabularyword handling, annotation consistency and unknownguessing all continue to challenge system designersand bakeoff organizers alike.In future analysis, we hope to develop additionalanalysis tools to better assess progress in these fun-damental tasks, in a more corpus independent fash-ion.
Such developments will guide the planning offuture evaluations.Finally, while Chinese word segmentation, namedentity recognition and Chinese POS tagging are im-portant in themselves, these three enabling technolo-gies are also the foundation of those upper level ap-plications such as parsing, reference resolution ormachine translation.
To evaluate the impact of im-provement in these three technologies on the sub-sequent applications is still the future work for thisevaluation.6 AcknowledgementsWe gratefully acknowledge the generous assistanceof the organizations listed below who provided thedata for this bakeoff; without their support, it couldnot have taken place:?
City University of Hong Kong, Hong Kong;?
Chinese Knowledge Information ProcessingGroup, Academia Sinica, Taiwan;?
Institute of Applied linguistics, M.O.E., China;?
Microsoft Research Asia, China;?
Peking University, China;?
Shanxi University, China;?
University of Colorado, USA;We also thank Olivia Oi Yee Kwong, the co-organizers of the sixth SIGHAN workshop, in con-junction with which this bakeoff takes place, andYongsheng Guo from Institute of Applied Linguis-tics, M.O.E., P.R.C.
who has made great effort forthis bakeoff.Professor Changning Huang merits special thanksfor his help in this bakeoff.
Finally, we thank all theparticipating sites who enabled the success of thisbakeoff.Table 25: Chinese POS tagging BaselineSource ATotala RIV b ROOV c RMTIVdCITYU .8425 .9021 .2543 .8083CKIP .8861 .9451 .2814 .8740CTB .8609 .8967 .3313 .8057NCC .9159 .9543 .2242 .8636PKU .8809 .9237 .2038 .8296Table 26: Chinese POS tagging ToplineSource ATotal RIV ROOV RMTIVCITYU .9310 .9330 .9107 .8727CKIP .9606 .9597 .9699 .9103CTB .9147 .9120 .9555 .8369NCC .9588 .9593 .9507 .8822PKU .9351 .9354 .9305 .8600Table 27: CITYU:POS tagging Closed TrackID RunID ATotal RIV ROOV RMTIV30 b .8951 .9389 .4637 .874530 a .8929 .9367 .4608 .870528 .8905 .9328 .4733 .86879 .8865 .9326 .4322 .870719 .8693 .9284 .2868 .858524 .8564 .9149 .2805 .850621 b .2793 .2969 .1051 .253821 a .1890 .2031 .0550 .1704Table 28: CITYU:POS tagging Open TrackID RunID ATotal RIV ROOV RMTIV28 .8900 .9329 .4670 .869539 .8669 .9089 .4537 .8495Table 29: CKIP:POS tagging Closed TrackID RunID ATotal RIV ROOV RMTIV30 b .9295 .9629 .5869 .912330 a .9286 .9618 .5875 .909928 .9220 .9556 .5772 .90889 .9160 .9504 .5631 .906516 .9124 .9549 .4756 .895319 .8994 .9561 .3169 .900124 .8793 .9334 .3247 .8943aATotal: total accuracybRIV : IV recallcROOV : OOV recalldRMTIV : MTIV recall79Sixth SIGHAN Workshop on Chinese Language ProcessingTable 23: Chinese POS tagging Training data statisticsSource Token WT TTa ATNb MTIV c RMTIVdCITYU 1092687 43639 44 1.2588 585056 0.5354CKIP 721551 48045 60 1.0851 335017 0.4643CTB 642246 42133 37 1.1690 334317 0.5205NCC 535023 45108 60 1.0673 178078 0.3328PKU 1116754 55178 103 1.1194 490243 0.4390Table 24: Chinese POS tagging Truth data statisticsSource Token WT TT ATN OOV ROOV e MTIV RMTIVCITYU 184314 17827 43 1.1446 16977 0.0921 92934 0.5042CKIP 91071 15331 63 1.0530 8085 0.0888 38640 0.4243CTB 59955 9797 35 1.1227 3794 0.0633 30513 0.5089NCC 102344 17493 55 1.0675 5392 0.0527 33853 0.3308PKU 156407 17643 103 1.1270 9295 0.0594 68065 0.4352aTT: number of tag type.bATN: Average Tag Number per word.cMTIV : number of IV Multi-Tag worddRMTIV : coverage rate of IV Multi-Tag wordseROOV : OOV tag rateTable 30: CKIP:POS tagging Open TrackID RunID ATotal RIV ROOV RMTIV28 .9211 .9542 .5813 .908239 .9004 .9327 .5686 .8936Table 31: CTB:POS tagging Closed TrackID RunID ATotal RIV ROOV RMTIV28 .9428 .9557 .7522 .91979 .9401 .9554 .7135 .918316 .9234 .9507 .5200 .905124 .9203 .9460 .5390 .905519 .9133 .9438 .4620 .898331 a .9088 .9374 .4866 .880531 b .8065 .8608 .0040 .7395Table 32: CTB:POS tagging Open TrackID RunID ATotal RIV ROOV RMTIV22 .9689 .9767 .8537 .955428 .9646 .9714 .8648 .949539 .9271 .9400 .7354 .901631 a .9120 .9374 .5361 .880531 b .8076 .8608 .0206 .7396Table 33: NCC:POS tagging Closed TrackID RunID ATotal RIV ROOV RMTIV30 b .9541 .9738 .5998 .919530 a .9525 .9717 .6059 .913528 .9494 .9690 .5959 .91299 .9456 .9658 .5822 .911616 .9395 .9690 .4086 .905919 .9336 .9687 .3017 .905031 a .9313 .9604 .4080 .880929 .9277 .9664 .2329 .900024 .9172 .9498 .3312 .896331 b .8940 .9303 .2411 .7948Table 34: NCC:POS tagging Open TrackID RunID ATotal RIV ROOV RMTIV28 .9496 .9694 .5938 .914131 a .9326 .9604 .4336 .880939 .9280 .9477 .5749 .895422 .9096 .9377 .4045 .893531 b .8940 .9303 .2411 .794825 .0836 .0855 .0488 .0645Table 35: PKU:POS tagging Closed TrackID RunID ATotal RIV ROOV RMTIV30 b .9450 .9679 .5818 .925230 a .9420 .9648 .5813 .918428 .9396 .9608 .6036 .91739 .9368 .9591 .5832 .917316 .9266 .9574 .4386 .907929 .9113 .9518 .2708 .895837 .9065 .9269 .5836 .890331 a .9053 .9451 .2751 .875819 .8815 .9158 .3386 .889731 b .8527 .8936 .2043 .764631 c .8450 .8855 .2039 .7471Table 36: PKU:POS tagging Open TrackID RunID ATotal RIV ROOV RMTIV28 .9411 .9622 .6057 .920031 a .9329 .9518 .6332 .897229 .9197 .9512 .4222 .899039 .9134 .9341 .5862 .889431 b .8427 .8935 .0398 .764322 .6649 .6796 .4308 .649580Sixth SIGHAN Workshop on Chinese Language ProcessingReferencesThomas Emerson.
2005.
The second international Chi-nese word segmentation bakeoff.
In Proceedings ofthe Fourth SIGHAN Workshop on Chinese LanguageProcessing, pages 123?133, Jeju Island, Korea.Charles Grinstead and J. Laurie Snell.
1997.
Introduc-tion to Probability.
American Mathematical Society,Providence, RI.Gina-Anne Levow.
2006.
The third international chineselanguage processing bakeoff: Word segmentation andnamed entity recognition.
In Proceedings of the FifthSIGHAN Workshop on Chinese Language Processing,pages 108?117, Sydney, Australia, July.
Associationfor Computational Linguistics.Richard Sproat and Thomas Emerson.
2003.
The firstinternational Chinese word segmentation bakeoff.
InThe Second SIGHAN Workshop on Chinese LanguageProcessing, pages 133?143, Sapporo, Japan.81Sixth SIGHAN Workshop on Chinese Language Processing
