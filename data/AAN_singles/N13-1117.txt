Proceedings of NAACL-HLT 2013, pages 969?977,Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational LinguisticsA Systematic Bayesian Treatment of the IBM Alignment ModelsYarin GalDepartment of EngineeringUniversity of CambridgeCambridge, CB2 1PZ, United Kingdomyg279@cam.ac.ukPhil BlunsomDepartment of Computer ScienceUniversity of OxfordOxford, OX1 3QD, United KingdomPhil.Blunsom@cs.ox.ac.ukAbstractThe dominant yet ageing IBM and HMMword alignment models underpin mostpopular Statistical Machine Translationimplementations in use today.
Thoughbeset by the limitations of implausibleindependence assumptions, intractableoptimisation problems, and an excess oftunable parameters, these models providea scalable and reliable starting point forinducing translation systems.
In this paper webuild upon this venerable base by recastingthese models in the non-parametric Bayesianframework.
By replacing the categoricaldistributions at their core with hierarchicalPitman-Yor processes, and through the useof collapsed Gibbs sampling, we provide amore flexible formulation and sidestep theoriginal heuristic optimisation techniques.The resulting models are highly extendible,naturally permitting the introduction ofphrasal dependencies.
We present extensiveexperimental results showing improvementsin both AER and BLEU when benchmarkedagainst Giza++, including significantimprovements over IBM model 4.1 IntroductionThe IBM and HMM word alignment models (Brownet al 1993; Vogel et al 1996) have underpinned themajority of statistical machine translation systemsfor almost twenty years.
The key attraction of thesemodels is their principled probabilistic formulation,and the existence of (mostly) tractable algorithmsfor their training.The dominant Giza++ implementation of theIBM models (Och and Ney, 2003) employs avariety of exact and approximate EM algorithmsto optimise categorical alignment distributions.While effective, this parametric approach results ina significant number of parameters to be tuned andintractable summations over the space of alignmentsfor models 3 and 4.
Giza++ hides the hyper-parameters with defaults and approximates theintractable expectations using restricted alignmentneighbourhoods.
However this approach was shownto often return alignments with probabilities wellbelow the true maxima (Ravi and Knight, 2010).To overcome perceived limitations with the wordbased and non-syntactic nature of the IBM modelsmany alternative approaches to word alignment havebeen proposed (e.g.
(DeNero et al 2008; Cohn andBlunsom, 2009; Levenberg et al 2012)).
Whileinteresting results have been reported, these alterna-tives have failed to dislodge the IBM approach.In this paper we proposed to retain the originalgenerative stories of the IBM models, whilereplacing the inflexible categorical distributionswith hierarchical Pitman-Yor (PY) processes ?
amathematical tool which has been successfullyapplied to a range of language tasks (Teh, 2006b;Goldwater et al 2006; Blunsom and Cohn,2011).
In the context of language modelling, thehierarchical PY process was shown to roughlycorrespond to interpolated Kneser-Ney (Kneser andNey, 1995; Teh, 2006a).
The key contribution ofthe hierarchical PY formulation is that it providesa principle probabilistic framework that easilyextends to latent variable models, such as those used969for alignment, for which a Kneser-Ney formulationis unclear.
While Bayesian priors have previouslybeen applied to IBM model 1 (Riley and Gildea,2012), in this work we go considerably further byimplementing non-parametric priors for the fullGiza++ training pipeline.Inference for the proposed models and theirhyper-parameters is done with Gibbs sampling.This eliminates the intractable summationsover alignments and the need for tuning hyper-parameters.
Further, we exploit the highlyextendible nature of the hierarchical PY process toimplement improvements to the original modelssuch as the introduction of phrasal dependencies.We present extensive experimental resultsshowing improvements in both BLEU scores andAER when compared to Giza++.
The demonstratedimprovements over IBM model 4 suggest that theheuristics used in the implementation of the EMalgorithm for this model were suboptimal.We begin with a formal presentation of the hier-archical PY process used in our Bayesian approachto replace the original categorical distributions.
Sec-tion 3 introduces our Bayesian formulation of theword alignment models, while its inference schemeis presented in the following section.
Finally, theexperimental results evaluating our models againstthe originals are given in section 5, demonstratingthe superiority of the non-parametric approach.2 The hierarchical PY processBefore giving the formal definition for the hierar-chical Pitman-Yor (PY) process, we first give someintuition into how this distribution works and whyit is commonly used to model problems in naturallanguage.The hierarchical PY process is an atomic distri-bution that can share its atoms between differentlevels in a hierarchy.
When used for language mod-elling it captures the probability of observing a wordafter any given sequence of n words.
It does so byinterpolating the observed frequency of the wholesequence followed by the word of interest, with theobserved frequency of a shorter sequence followedby the word of interest.
This interpolation is done insuch a way that tokens in a more specific distributionare interpolated with types in a less specific one.If there is sufficient evidence for the whole wordsequence, i.e.
it is not sparse in the corpus, higherweight will be given to the frequency of the wordof interest after the more specific sequence thanthe shorter one.
If the sequence was not observedfrequently and there is not enough information tomodel whether the word of interest follows after itfrequently or not, the process will back-off to theshorter sequence and assign higher weight to its fre-quency instead.
This is done in a recursive fashion,decreasing the sequence length by one every timeuntil the probability is interpolated with the uniformdistribution, much like interpolated Kneser-Ney, thestate of the art for language modelling.Unlike Kneser-Ney, the hierarchical PY approachnaturally extends to model complicated conditionaldistributions involving latent variables.
Moreover,almost all instances of priors with categorical distri-butions can be replaced by the PY process, where inits most basic representation (with no conditional) itprovides a flexible model of power law frequencies.The PY process generalises a number of simplerdistributions.
The Dirichlet distribution is a distri-bution over discrete probability mass functions of acertain given length which is often used to modelprior beliefs on parameter sparsity in machine learn-ing problems.
The Dirichlet process generalises theDirichlet distribution to a distribution over infinitesequences of non-negative reals that sum to one andis often used for nonparametric Bayesian inference.The PY process is used in the context of natural lan-guage processing as it further generalises the Dirich-let process by adding an additional degree of free-dom that enables it to produce power-law discreteprobability mass functions that resemble those seenexperimentally in corpora (Goldwater et al 2006).Formally, draws from the PY processG1 ?
PY (d, ?,G0) with a discount parameter0 ?
d < 1, a strength parameter ?
> ?d, and a basedistribution G0, are constructed using a Chineserestaurant process analogy as follows:Xn+1|X1, ..., Xn ?K?k=1mk ?
d?
+ n?yk +?
+ dK?
+ nG0Where mk denotes the number of Xis (customers)assigned to yk (a table) and K is the total number ofyks drawn from G0.970Hierarchical PY processes (Teh, 2006b), PYprocesses where the base distribution is itself aPY process, were developed as an extension whichis often used in the context of natural languageprocessing due to their relationship to back-offsmoothing.
Denoting a context of atoms u as(wi?l, ..., wi?1), the hierarchical PY process isdefined using the above definition of the PY processby:wi ?GuGu ?PY (d|u|, ?|u|, Gpi(u))...G(wi?1) ?PY (d1, ?1, G?)G?
?PY (d0, ?0, G0)where pi(u) = (wi?l+1, ..., wi?1) is the suffix of u,|u| denotes the length of context u, and G0 is a basedistribution.3 A Bayesian approach to word alignmentIn this work we replace the categorical distributionsat the heart of statistical alignment models with PYprocesses.
We start by describing the revised modelsfor IBM model 1 and the HMM alignment model,before continuing to the more advanced IBM mod-els 3 and 4.
Throughout this section, we assumethat the base distributions in our models (denotedG0, H0, etc.)
are uniform over all atoms, and omitthe strength and concentration parameters of the PYprocess for clarity.
We use subscripts to denotethe hierarchy, and lower-case superscripts to denotea fixed condition (for example, Gm0 is the (uni-form) base distribution that is determined uniquelyfor each possible foreign sentence length m).3.1 Model 1 and the HMM alignment modelThe most basic word alignment model, IBM model1, can be described using the following generativeprocess (Brown et al 1993): Given an English sen-tence E = e1, ..., el, first choose a length m forthe foreign sentence F .
Next, choose a vector ofrandom word positions from the English sentenceA = a1, ..., am to be the alignment, and then foreach foreign word fi choose a translation from theEnglish word eai aligned to it by A.
The existenceof a NULL word at the beginning of the English sen-tence is assumed, a word to which spurious words inthe foreign sentence can align.
From this generativeprocess the following probability model is derived:P (F,A|E) = p(m|l)?m?i=1p(ai)p(fi|eai)Where p(ai) = 1l+1 is uniform over all alignmentsand p(fi|eai) ?
Categorical.In our approach we model these distributionsusing hierarchical PY processes instead of thecategorical distributions.
Thus we place thefollowing assumptions on IBM model 1:ai|m ?
Gm0fi|eai ?
HeaiHeai ?
PY (H?)H?
?
PY (H0)In this probability modelling we assume that thealignment positions are determined using the uni-form distribution, and that word translations are gen-erated depending on the source word ?
the probabil-ity of translating to a specific foreign word dependson the observed frequency of pairs of the foreignword and the given source word.
We back-off tothe frequencies of the foreign words when the sourceword wasn?t observed frequently.The HMM alignment model uses the HiddenMarkov Model to find word alignments.
It treats thetranslations of the words of the English sentence asobservables and the alignment positions as the latentvariables to be discovered.
Its generative processcan be described in an abstract way as follows: wedetermine the length of the foreign sentence andthen iterate over the words of the source sentenceemitting translations for each word to fill-in thewords in the foreign sentence from left to right.The following probability model is derived for theHMM alignment model (Vogel et al 1996):P (F,A|E) =p(m|l)?m?i=1p(ai|ai?1,m)?
p(fi|eai)For the HMM alignment model we replacethe categorical translation distribution p(fi|eai)with the same one we used for model 1, and971replace the categorical distribution for the transitionp(ai|ai?1,m) with a hierarchical PY process witha longer sequence of alignment positions in theconditional:ai|ai?1,m ?
Gmai?1Gmai?1 ?
PY (Gm?
)Gm?
?
PY (Gm0 )We use a unique distribution for each foreign sen-tence length, and condition the position on the pre-vious alignment position, backing-off to the HMM?sstationary distribution over alignment positions.3.2 Models 3 and 4IBM models 3 and 4 introduce the concept of aword?s fertility, the number of foreign words that aregenerated from a specific English word.
These mod-els can be described using the following generativeprocess.
Given an English sentence E, first deter-mine the length of the foreign sentence: for eachword in the English sentence ei choose a fertility,denoted ?i.
Every time a word is generated, an addi-tional spurious word from the NULL word in theEnglish sentence can be generated with a fixed prob-ability.
After the foreign sentence length is deter-mined translate each English word into its foreignequivalent (including the NULL word) in the sameway as for model 1.
Finally, non-spurious wordsare rearranged into the final word positions and thespurious words inserted into the empty positions.
Inmodel 3 this is done with a zero order HMM, and inmodel 4 with two first order HMMs.
One controlsthe distortion of the head of each English word (thefirst foreign word generated from it) relative to thecentre (denoted here ) of the set of foreign wordsgenerated from the previous English word, and theother controls the distortion within the set itself byconditioning the word position on the previous wordposition.For the probability model, we follow the notationof Och and Ney (2003) and define the alignment asa function from the source sentence positions i toBi ?
{1, ...,m} where the Bi?s form a partition ofthe set {1, ...,m}.
The fertility of the English wordi is ?i = |Bi|, and we use Bi,k to refer to the kthelement of Bi in ascending order.Using the above notation, the following probabil-ity model is derived (Och and Ney, 2003):P (F,A|E) =p(B0|B1, ..., Bl)?l?i=1p(Bi|Bi?1, ei)?l?i=0?j?Bip(fj |ei)For model 3 the dependence on previousalignment sets is ignored and the probabilityp(Bi|Bi?1, ei) is modelled asp(Bi|Bi?1, ei) = p(?i|ei)?i!
?j?Bip(j|i,m),whereas in model 4 it is modelled using two HMMs:p(Bi|Bi?1, ei) =p(?i|ei)?
p=1(Bi,1 ?
(Bi?1)|?)?
?i?k=2p>1(Bi,k ?Bi,k?1|?
)For both these models the spurious word genera-tion is controlled by a binomial distribution:p(B0|B1, ..., Bl) =(m?
?0?0)(1?
p0)m?2?0p?011?0!for some parameters p0 and p1.Replacing the categorical priors with hierarchicalPY process ones, we set the translation and fertilityprobabilities p(?i|ei)?j?Bip(fj |ei) using a com-mon prior that generates translation sequences:(f1, ..., f?i)|ei ?
HeiHei ?
PY (HFTei )HFTei ((f1, ..., f?i)) = HFei (?i)?jHT(fj?1,ei)(fj)HFei ?
PY (HF?
)HF?
?
PY (HF0 )HT(fj?1,ei) ?
PY (HTei)HTei ?
PY (HT?
)HT?
?
PY (HT0 )Here we used superscripts for the indexing of wordswhich do not have to occur sequentially in the sen-tence.
We generate sequences instead of individ-ual words and fertilities, and fall-back onto theseonly in sparse cases.
For example, when aligningthe English sentence ?I don?t speak French?
to its972French translation ?Je ne parle pas franc?ais?, theword ?not?
will generate the phrase (?ne?, ?pas?
),which will later on be distorted into its place aroundthe verb.The distortion probability for model 3, p(j|i,m),is modelled simply as depending on the position ofthe source word i and its class:j|(C(ei), i),m ?
Gm(C(ei),i)Gm(C(ei),i) ?
PY (Gmi )Gmi ?
PY (Gm?
)Gm?
?
PY (Gm0 )where we back-off to the source word position andthen to the frequencies of the alignment positions.As opposed to this simple mechanism, in the dis-tortion probability for IBM model 4 there exist twodistinct probability distributions.
The first probabil-ity distribution p=1 controls the head distortion:Bi,1 ?
(Bi?1) | (C(ei), C(fBi,1)),m?
Gm(C(ei),C(fBi,1 ))Gm(C(ei),C(fBi,1 ))?
PY (GmC(fBi,1 ))GmC(fBi,1 )?
PY (Gm?
)Gm?
?
PY (Gm0 )In this probability modelling we model the jumpsize itself, as depending on the word class for thesource word and the word class for the proposedforeign word, backing-off to the proposed foreignword class and then to the relative jump frequencies.The second probability distribution p>1 controlsthe distortion within the set of words:Bi,j ?Bi,j?1|C(fBi,j ),m ?
HmC(fBi,j )HmC(fBi,j )?
PY (Hm?
)Hm?
?
PY (Hm0 )Here we again model the jump size as dependingon the word class for the proposed foreign word,backing-off to the relative jump frequencies.Lastly, we add to this probability model a treat-ment for fertility and translation of NULL words.The fertility generation follows the idea of the orig-inal model, where the number of spurious words isdetermined by a binomial distribution created froma set of Bernoulli experiments, each one performedafter the translation of a non-spurious word.
We usean indicator function I to signal whether a spuri-ous word was generated after a non-spurious word(I = 1) or not (I = 0).I = 0, 1|l ?
HNFlHNFl ?
PY (HNF?
)HNF?
?
PY (HNF0 )Then, the translation of spurious words is done in astraightforward manner:fi ?
HNT?HNT?
?
PY (HNT0 )4 InferenceThe Gibbs sampling inference scheme together withthe Chinese Restaurant Franchise process (Teh andJordan, 2009) are used to induce alignments for aparallel corpus.
A set of restaurants S is constructedand initialised either randomly or through a pipelineof alignment results from simpler models, and thenat each iteration each alignment position is removedfrom the restaurants and re-sampled, conditioned onthe rest of the alignment positions.Denoting e, f ,a the sets of all source sen-tences, their translations, and their correspondingalignments in our corpus, and denoting E,F,A aspecific source sentence, its translation, and theircorresponding alignment, where ei is the i?th wordof the source sentence and fj , aj are the j?th wordin the foreign sentence and its alignment into thesource sentence, we sample a new value for aj usingthe univariate conditional distribution:P (aj = i|E,F,A?j , e?E , f?F ,a?A,S?aj )?
P (F, (A?j , aj = i)|E, e?E , f?F ,a?A,S?aj )Where a minus sign in the subscript denotes thestructure without the mentioned element, and S?ajdenotes the configuration of the restaurants afterremoving the alignment aj .This univariate conditional distribution is propor-tional to the probability assigned by the differentmodels to an alignment sequence, where the restau-rants replace the counts of the alignment positions9731 1>H 1>H>3 1>H>3>426.026.527.027.528.028.529.029.5BLEUChinese -> English PipelinePY-IBMGiza++Figure 1: BLEU scores of pipelinedGiza++ and pipelined PY-IBM trans-lating from Chinese into English1 1>H 1>H>3 1>H>3>413.5013.7514.0014.2514.5014.75BLEUEnglish -> Chinese PipelinePY-IBMGiza++Figure 2: BLEU scores of pipelinedGiza++ and pipelined PY-IBM trans-lating from English into Chinese1 1>H 1>H>3 1>H>3>4323334353637383940AERAER PipelineGiza++PY-IBMFigure 3: AER of pipelined Giza++and pipelined PY-IBM aligningChinese and Englishin the prior.
Maximum marginal decoding (Johnsonand Goldwater, 2009) can then be used to get theMAP estimate of the probability distributions overthe alignment positions for each sentence from thesamples extracted from the Gibbs sampler.In addition to sampling the alignments, wealso place a uniform Beta prior on the discountparameters and a vague Gamma prior on thestrength parameters, and sample them using slicesampling (Neal, 2003).
The end result is that thealignment models have no free parameters to tune.5 Experimental resultsIn order to assess our PY process alignmentmodels (referred to as PY-IBM henceforth) severalexperiments were carried out to benchmark themagainst the original models (as implemented inGiza++).
We evaluated the BLEU scores (Papineniet al 2002) of translations from Chinese intoEnglish and from English into Chinese, as wellas the alignment error rates (AER) of the inducedsymmetrised alignments compared to a humanaligned dataset.
Moses (Koehn et al 2007) wasused for the training of the SMT system andthe symmetrisation (using the grow-diag-finalprocedure), with MERT (Och, 2003) used for tuningof the weights, and SRILM (Stolcke, 2002) to buildthe language model (5-grams based).
The corpusused for training and evaluation was the ChineseFBIS corpus.
MT02 was used for tuning, and MT03was used for evaluation.
In each case we usedone reference sentence in Chinese and 4 referencesentences in English.Most translation systems rely on the Giza++ pack-age in which the implementation of the originalmodels is done by combining them in a pipeline.Model 1 and the HMM alignment model are runsequentially each for 5 iterations; then models 3 and4 are run sequentially for 3 iterations each.
Thisfollows the observation of Och and Ney (2003) thatbootstrapping from previous results assists the fer-tility algorithms find the best alignment neighbour-hood in order to estimate the expectations.We assessed the proposed models against theoriginal models in a pipeline experiment whereboth systems were trained on a corpus startingat model 1, and used the results of the previousrun to initialise the next one ?
noting the BLEUscores and AER at each step.
The Gibbs samplersfor the pipelined PY-IBM models were run for 50iterations for each model and started accumulatingsamples after a burn-in period of 10 iterations,each experiment was repeated three times andthe results averaged.
As can be seen in figures1 to 3, the pipelined PY-IBM models achievedhigher BLEU scores across all steps, with thehighest improvement of 1.6 percentage points in thepipelined HMM alignment models when translating974HMM Model Model 4202122232425262728BLEUChinese -> EnglishPY-IBMGiza++ 10 iter.Giza++Figure 4: BLEU scores of Giza++?sand PY-IBM?s HMM model andmodel 4 translating from Chinese intoEnglishHMM Model Model 410.511.011.512.012.513.013.514.0BLEUEnglish -> ChinesePY-IBMGiza++ 10 iter.Giza++Figure 5: BLEU scores of Giza++?sand PY-IBM?s HMM model andmodel 4 translating from English intoChineseHMM Model Model 435404550556065AERAERGiza++Giza++ 10 iter.PY-IBMFigure 6: AER of Giza++?s and PY-IBM?s HMM model and model 4aligning Chinese and English0 20 40 60 80 100 120 140 160iteration (after burn-in)0.20.40.60.81.01.21.4# of alignmentpositiondisagreements 1e6Alignment Disagreement pipeline zh->enFigure 7: Alignment disagreement of the Chinese toEnglish pipelined PY-IBM models for the 3 repetitionsfrom Chinese into English, and an improvementof 1.2 percentage points in the overall results afterfinishing the pipeline.We also saw an improvement in AER for allmodels, where the pipelined PY-IBM model 4achieved an error rate of 32.9, as opposed to theresult obtained by the Giza++ pipelined model 4of 34.4.
We note an interesting observation thatboth Giza++ and PY-IBM model 3 underperformedcompared to the previously run HMM alignmentmodel, as seen in the English to Chinese pipelineresults and the AER pipeline results.The alignment disagreement (the number ofchanged alignment positions between subsequentiterations) of the Chinese to English pipelinedPY-IBM models (1 to 4) can be seen in fig.
7.
Thisgraph shows that each model in the pipeline reachesan alignment disagreement equilibrium after about20 iterations, and that earlier models have greaterinitial deviation from their equilibrium than latermodels ?
which have an overall lower disagreement.In order to assess the dependence of the fertil-ity based models on the initialisation step anotherset of experiments was carried out.
The modelswere trained with a randomly initialised set of align-ments and assessed after a set number of iterationsfor the Giza++ models (5 and 10 for the Giza++HMM alignment model, and 3 and 10 for the Giza++IBM model 4), or after 100 iterations with a burn-in period of 10 iterations for the PY-IBM ones (wereport the average of three runs for both models).The results, reported in figures 4 to 6, show againthat the PY-IBM model outperformed the Giza++implementations, and to a large extent in the caseof IBM model 4.
This provides further evidencethat the supposition underlying the neighbourhood9750 10 20 30 40 50 60 70 80 90iteration (after burn-in)2.53.03.54.04.55.0# of alignmentpositiondisagreements 1e5Alignment Disagreement Model 4 zh->enFigure 8: Alignment disagreement of the Chinese toEnglish PY-IBM model 4 for the 3 repetitionsapproximation for training models 3 and 4 ?
thatthere exists a small set of alignments on which mostof the probability mass concentrates ?
is poor.
Aninteresting observation to note is that the BLEUscore of the non-pipelined PY-IBM model 4 is thesame as the PY-IBM HMM model translating inboth directions, as opposed to an improvement inthe pipelined case.
This suggests that the samplermight not have fully converged after 100 iterationsfor model 4 (the number of alignment disagreementsfor this experiment can be seen in figure 8).
Furtherconfirmation for this comes from the higher standarddeviation of 0.54 observed for the PY-IBM model 4,as opposed to a standard deviation for the PY-IBMHMM model of 0.21 (which is still more significantthan that of the pipelined PY-IBM model 4, whosestandard deviation was 0.13).Both the PY-IBM and the Giza++ trained mod-els run in a linear time in the number of sentences,where due to the nature of MCMC sampling tech-niques, more iterations are required for its conver-gence.
In our experiments, the running time ofthe unoptimised Gibbs sampler was 50 times slowerthan the optimised EM.6 DiscussionThe models described in this paper allow one touse non-parametric approaches to flexibly modelword alignment distributions, overcoming a numberof limitations of the EM algorithm for the fertilitybased alignment models.
The models achieved asignificant improvement in BLEU scores and AERon the tested corpus, and are easy to extend withoutthe need for additional modelling tools.The alignment models proposed mostly follow theoriginal generative stories while introducing addi-tional phrasal conditioning into models 3 and 4.However there are many other areas in which wecould make use of hierarchical tools to introducenew dependencies easily without running into spar-sity problems.One example is the extension of the transitionhistory used in the HMM alignment model: IBMmodel 1 uses a uniform distribution over transitions,model 2 conditions on relative sentence positions,and the HMM model uses a first order dependency.One extension would be to use longer histories of nprevious positions, handling sparsity with back-off.Previously proposed approaches to extend theHMM alignment model include Och and Ney(2003)?s use of word classes and smoothing, andthe combination of part-of-speech information ofthe words surrounding the source word (Brunninget al 2009).
Using our hierarchical model onecould easily introduce such dependencies on thecontext words of the word to be translated and theirpart-of-speech information.
This could assist inboth translation and reordering disambiguation, andwould incorporate back-off by using smaller andsmaller contexts when such information is sparse.Further improvements to models 3 and 4 couldbe carried out by introducing longer dependenciesin the fertility and distortion distributions.
Insteadof conditioning on the previous word, one coulduse further information such as PoS tags, previouslytranslated words, or previous fertilities.
Additionalresearch would involve the use of more effectivevariational inference algorithms for hierarchical PYprocess based models.The PY-IBM models described in this paper wereimplemented within the Giza++ code base, andare available as an open source package for furtherdevelopment and research.1ReferencesPhil Blunsom and Trevor Cohn.
2011.
A hierarchicalPitman-Yor process HMM for unsupervised part of1Available at github.com/yaringal/Giza-sharp976speech induction.
In Proceedings of the 49th AnnualMeeting of the Association for Computational Linguis-tics: Human Language Technologies, pages 865?874,Portland, Oregon, USA, June.
Association for Compu-tational Linguistics.Peter F. Brown, Stephen A. Della Pietra, Vincent J. DellaPietra, and Robert L. Mercer.
1993.
The mathematicsof statistical machine translation: Parameter estima-tion.
Computational Linguistics, 19(2):263?311.Jamie Brunning, Adria` de Gispert, and William Byrne.2009.
Context-dependent alignment models for statis-tical machine translation.
In Proceedings of HumanLanguage Technologies: The 2009 Annual Conferenceof the North American Chapter of the Association forComputational Linguistics, NAACL ?09, pages 110?118.Trevor Cohn and Phil Blunsom.
2009.
A Bayesian modelof syntax-directed tree to string grammar induction.In Proceedings of the 2009 Conference on Empiri-cal Methods in Natural Language Processing, pages352?361, Singapore, August.
Association for Compu-tational Linguistics.John DeNero, Alexandre Bouchard-Co?te?, and Dan Klein.2008.
Sampling alignment structure under a Bayesiantranslation model.
In Proceedings of the 2008 Confer-ence on Empirical Methods in Natural Language Pro-cessing, pages 314?323, Honolulu, Hawaii, October.Association for Computational Linguistics.Sharon Goldwater, Tom Griffiths, and Mark John-son.
2006.
Interpolating between types and tokensby estimating power-law generators.
In Y. Weiss,B.
Scho?lkopf, and J. Platt, editors, Advances in NeuralInformation Processing Systems 18, pages 459?466.MIT Press, Cambridge, MA.Mark Johnson and Sharon Goldwater.
2009.
Improvingnonparameteric Bayesian inference: experiments onunsupervised word segmentation with adaptor gram-mars.
In Proceedings of Human Language Technolo-gies: The 2009 Annual Conference of the North Ameri-can Chapter of the Association for Computational Lin-guistics, NAACL ?09, pages 317?325.Reinhard Kneser and Hermann Ney.
1995.
Improvedbacking-off for m-gram language modeling.
Acous-tics, Speech, and Signal Processing, IEEE Interna-tional Conference on, 1:181?184.Philipp Koehn, Hieu Hoang, Alexandra Birch, ChrisCallison-Burch, Marcello Federico, Nicola Bertoldi,Brooke Cowan, Wade Shen, Christine Moran, RichardZens, Chris Dyer, Ondrej Bojar, Alexandra Con-stantin, and Evan Herbst.
2007.
Moses: Open sourcetoolkit for statistical machine translation.
In Proc.of the 45th Annual Meeting of the ACL (ACL-2007),Prague.Abby Levenberg, Chris Dyer, and Phil Blunsom.
2012.A Bayesian model for learning SCFGs with discon-tiguous rules.
In Proceedings of the 2012 JointConference on Empirical Methods in Natural Lan-guage Processing and Computational Natural Lan-guage Learning, pages 223?232, Jeju Island, Korea,July.
Association for Computational Linguistics.Radford Neal.
2003.
Slice sampling.
Annals of Statis-tics, 31:705?767.Franz Josef Och and Hermann Ney.
2003.
A system-atic comparison of various statistical alignment mod-els.
Computational Linguistics, 29(1):19?52.Franz Josef Och.
2003.
Minimum error rate training instatistical machine translation.
In Proc.
of the 41stAnnual Meeting of the ACL (ACL-2003), pages 160?167, Sapporo, Japan.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
Bleu: a method for automatic eval-uation of machine translation.
In Proc.
of the 40thAnnual Meeting of the ACL and 3rd Annual Meeting ofthe NAACL (ACL-2002), pages 311?318, Philadelphia,Pennsylvania.Sujith Ravi and Kevin Knight.
2010.
Does Giza++ makesearch errors?
Computational Linguistics, 36(3):295?302, September.Darcey Riley and Daniel Gildea.
2012.
Improving theibm alignment models using variational bayes.
In Pro-ceedings of the 50th Annual Meeting of the Associationfor Computational Linguistics: Short Papers - Volume2, ACL ?12, pages 306?310.A.
Stolcke.
2002.
SRILM ?
an extensible language mod-eling toolkit.
In Proc.
of the International Conferenceon Spoken Language Processing.Y.
W. Teh and M. I. Jordan, 2009.
Hierarchical BayesianNonparametric Models with Applications.
CambridgeUniversity Press.Yee Whye Teh.
2006a.
A Bayesian interpretation ofinterpolated Kneser-Ney.
Technical report, NationalUniversity of Singapore School of Computing.Yee Whye Teh.
2006b.
A hierarchical bayesian languagemodel based on pitman-yor processes.
In Proceedingsof the 21st International Conference on ComputationalLinguistics and the 44th annual meeting of the Asso-ciation for Computational Linguistics, ACL-44, pages985?992, Morristown, NJ, USA.
Association for Com-putational Linguistics.Stephen Vogel, Hermann Ney, and Christoph Tillmann.1996.
HMM-based word alignment in statistical trans-lation.
In Proc.
of the 16th International Conferenceon Computational Linguistics (COLING ?96), pages836?841, Copenhagen, Denmark, August.977
