Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 166?175,Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational LinguisticsWord Alignment Modeling with Context Dependent Deep Neural NetworkNan Yang1, Shujie Liu2, Mu Li2, Ming Zhou2, Nenghai Yu11University of Science and Technology of China, Hefei, China2Microsoft Research Asia, Beijing, China{v-nayang,shujliu,muli,mingzhou}@microsoft.comynh@ustc.edu.cnAbstractIn this paper, we explore a novel bilin-gual word alignment approach based onDNN (Deep Neural Network), which hasbeen proven to be very effective in var-ious machine learning tasks (Collobertet al, 2011).
We describe in detailhow we adapt and extend the CD-DNN-HMM (Dahl et al, 2012) method intro-duced in speech recognition to the HMM-based word alignment model, in whichbilingual word embedding is discrimina-tively learnt to capture lexical translationinformation, and surrounding words areleveraged to model context informationin bilingual sentences.
While being ca-pable to model the rich bilingual corre-spondence, our method generates a verycompact model with much fewer parame-ters.
Experiments on a large scale English-Chinese word alignment task show that theproposed method outperforms the HMMand IBM model 4 baselines by 2 points inF-score.1 IntroductionRecent years research communities have seen astrong resurgent interest in modeling with deep(multi-layer) neural networks.
This trending topic,usually referred under the name Deep Learning, isstarted by ground-breaking papers such as (Hin-ton et al, 2006), in which innovative training pro-cedures of deep structures are proposed.
Unlikeshallow learning methods, such as Support VectorMachine, Conditional Random Fields, and Maxi-mum Entropy, which need hand-craft features asinput, DNN can learn suitable features (represen-tations) automatically with raw input data, given atraining objective.DNN did not achieve expected success until2006, when researchers discovered a proper wayto intialize and train the deep architectures, whichcontains two phases: layer-wise unsupervised pre-training and supervised fine tuning.
For pre-training, Restricted Boltzmann Machine (RBM)(Hinton et al, 2006), auto-encoding (Bengio et al,2007) and sparse coding (Lee et al, 2007) are pro-posed and popularly used.
The unsupervised pre-training trains the network one layer at a time, andhelps to guide the parameters of the layer towardsbetter regions in parameter space (Bengio, 2009).Followed by fine tuning in this region, DNN isshown to be able to achieve state-of-the-art per-formance in various area, or even better (Dahl etal., 2012) (Kavukcuoglu et al, 2010).
DNN alsoachieved breakthrough results on the ImageNetdataset for objective recognition (Krizhevsky etal., 2012).
For speech recognition, (Dahl et al,2012) proposed context-dependent neural networkwith large vocabulary, which achieved 16.0% rel-ative error reduction.DNN has also been applied in Natural Lan-guage Processing (NLP) field.
Most works con-vert atomic lexical entries into a dense, low di-mensional, real-valued representation, called wordembedding; Each dimension represents a latent as-pect of a word, capturing its semantic and syntac-tic properties (Bengio et al, 2006).
Word embed-ding is usually first learned from huge amount ofmonolingual texts, and then fine-tuned with task-specific objectives.
(Collobert et al, 2011) and(Socher et al, 2011) further apply Recursive Neu-ral Networks to address the structural predictiontasks such as tagging and parsing, and (Socheret al, 2012) explores the compositional aspect ofword representations.Inspired by successful previous works, we pro-pose a new DNN-based word alignment method,which exploits contextual and semantic similari-ties between words.
As shown in example (a) ofFigure 1, in word pair {?juda?
??mammoth?
},the Chinese word ?juda?
is a common word, but166mammothwill be ajiang shi yixiang juda gongchengjob(a)?
?
??
??
?
?A :farmer Yibula saidnongmin yibula shuo : ??(b)??
???
?Figure 1: Two examples of word alignmentthe English word ?mammoth?
is not, so it is veryhard to align them correctly.
If we know that?mammoth?
has the similar meaning with ?big?,or ?huge?, it would be easier to find the corre-sponding word in the Chinese sentence.
As wementioned in the last paragraph, word embedding(trained with huge monolingual texts) has the abil-ity to map a word into a vector space, in which,similar words are near each other.For example (b) in Figure 1, for the word pair{?yibula?
?
?Yibula?
}, both the Chinese word?yibula?
and English word ?Yibula?
are rare nameentities, but the words around them are very com-mon, which are {?nongmin?, ?shuo?}
for Chineseside and {?farmer?, ?said?}
for the English side.The pattern of the context {?nongmin X shuo??
?farmer X said?}
may help to align the wordpair which fill the variableX , and also, the pattern{?yixiang X gongcheng??
?a X job?}
is helpfulto align the word pair {?juda???mammoth?}
forexample (a).Based on the above analysis, in this paper, boththe words in the source and target sides are firstlymapped to a vector via a discriminatively trainedword embeddings, and word pairs are scored by amulti-layer neural network which takes rich con-texts (surrounding words on both source and targetsides) into consideration; and a HMM-like distor-tion model is applied on top of the neural networkto characterize structural aspect of bilingual sen-tences.In the rest of this paper, related work aboutDNN and word alignment are first reviewed inSection 2, followed by a brief introduction ofDNN in Section 3.
We then introduce the detailsof leveraging DNN for word alignment, includingthe details of our network structure in Section 4and the training method in Section 5.
The mer-its of our approach are illustrated with the experi-ments described in Section 6, and we conclude ourpaper in Section 7.2 Related WorkDNN with unsupervised pre-training was firstlyintroduced by (Hinton et al, 2006) for MNISTdigit image classification problem, in which, RBMwas introduced as the layer-wise pre-trainer.
Thelayer-wise pre-training phase found a better localmaximum for the multi-layer network, thus led toimproved performance.
(Krizhevsky et al, 2012)proposed to apply DNN to do object recognitiontask (ImageNet dataset), which brought down thestate-of-the-art error rate from 26.1% to 15.3%.
(Seide et al, 2011) and (Dahl et al, 2012) applyContext-Dependent Deep Neural Network withHMM (CD-DNN-HMM) to speech recognitiontask, which significantly outperforms traditionalmodels.Most methods using DNN in NLP start with aword embedding phase, which maps words intoa fixed length, real valued vectors.
(Bengio etal., 2006) proposed to use multi-layer neural net-work for language modeling task.
(Collobert et al,2011) applied DNN on several NLP tasks, suchas part-of-speech tagging, chunking, name entityrecognition, semantic labeling and syntactic pars-ing, where they got similar or even better resultsthan the state-of-the-art on these tasks.
(Niehuesand Waibel, 2012) shows that machine transla-tion results can be improved by combining neurallanguage model with n-gram traditional language.
(Son et al, 2012) improves translation quality ofn-gram translation model by using a bilingual neu-ral language model.
(Titov et al, 2012) learns acontext-free cross-lingual word embeddings to fa-cilitate cross-lingual information retrieval.For the related works of word alignment, themost popular methods are based on generativemodels such as IBM Models (Brown et al, 1993)and HMM (Vogel et al, 1996).
Discriminative ap-proaches are also proposed to use hand crafted fea-tures to improve word alignment.
Among them,(Liu et al, 2010) proposed to use phrase and rulepairs to model the context information in a log-linear framework.
Unlike previous discriminativemethods, in this work, we do not resort to any handcrafted features, but use DNN to induce ?features?from raw words.1673 DNN structures for NLPThe most important and prevalent features avail-able in NLP are the words themselves.
To ap-ply DNN to NLP task, the first step is to trans-form a discrete word into its word embedding, alow dimensional, dense, real-valued vector (Ben-gio et al, 2006).
Word embeddings often implic-itly encode syntactic or semantic knowledge ofthe words.
Assuming a finite sized vocabulary V ,word embeddings form a (L?|V |)-dimension em-bedding matrix WV , where L is a pre-determinedembedding length; mapping words to embed-dings is done by simply looking up their respec-tive columns in the embedding matrix WV .
Thelookup process is called a lookup layer LT , whichis usually the first layer after the input layer in neu-ral network.After words have been transformed to their em-beddings, they can be fed into subsequent classi-cal network layers to model highly non-linear re-lations:zl = fl(M lzl?1 + bl) (1)where zl is the output of lth layer, M l is a |zl| ?|zl?1| matrix, bl is a |zl|-length vector, and flis an activation function.
Except for the lastlayer, fl must be non-linear.
Common choices forfl include sigmoid function, hyperbolic function,?hard?
hyperbolic function etc.
Following (Col-lobert et al, 2011), we choose ?hard?
hyperbolicfunction as our activation function in this work:htanh(x) =??
?1 if x is greater than 1?1 if x is less than -1x otherwise(2)If probabilistic interpretation is desired, a softmaxlayer (Bridle, 1990) can be used to do normaliza-tion:zli =ezl?1i|zl|?j=1ezl?1j(3)The above layers can only handle fixed sized in-put and output.
If input must be of variable length,convolution layer and max layer can be used, (Col-lobert et al, 2011) which transform variable lengthinput to fixed length vector for further processing.Multi-layer neural networks are trained withthe standard back propagation algorithm (LeCun,1985).
As the networks are non-linear and thetask specific objectives usually contain many lo-cal maximums, special care must be taken in theoptimization process to obtain good parameters.Techniques such as layerwise pre-training(Bengioet al, 2007) and many tricks(LeCun et al, 1998)have been developed to train better neural net-works.
Besides that, neural network training alsoinvolves some hyperparameters such as learningrate, the number of hidden layers.
We will addressthese issues in section 4.4 DNN for word alignmentOur DNN word alignment model extends classicHMM word alignment model (Vogel et al, 1996).Given a sentence pair (e, f), HMM word alignmenttakes the following form:P (a, e|f) =|e|?i=1Plex(ei|fai)Pd(ai ?
ai?1) (4)where Plex is the lexical translation probabilityand Pd is the jump distance distortion probability.One straightforward way to integrate DNNinto HMM is to use neural network to computethe emission (lexical translation) probability Plex.Such approach requires a softmax layer in the neu-ral network to normalize over all words in sourcevocabulary.
As vocabulary for natural languagesis usually very large, it is prohibitively expen-sive to do the normalization.
Hence we give upthe probabilistic interpretation and resort to a non-probabilistic, discriminative view:sNN (a|e, f) =|e|?i=1tlex(ei, fai |e, f)td(ai, ai?1|e, f)(5)where tlex is a lexical translation score computedby neural network, and td is a distortion score.In the classic HMM word alignment model,context is not considered in the lexical translationprobability.
Although we can rewrite Plex(ei|fai)to Plex(ei|context of fai) to model context, it in-troduces too many additional parameters and leadsto serious over-fitting problem due to data sparse-ness.
As a matter of fact, even without any con-texts, the lexical translation table in HMM al-ready contains O(|Ve| ?
|Vf |) parameters, where|Ve| and Vf denote source and target vocabularysizes.
In contrast, our model does not maintaina separate translation score parameters for everysource-target word pair, but computes tlex througha multi-layer network, which naturally handlescontexts on both sides without explosive growthof number of parameters.168InputSource window e Target window f)( 323 bzM ??
)( 212 bzM ?
?ii-1 i+1 j-1 j j+1LookupLT0zLayer f1 1zLayer f2 2z??
???
?
farmer yibula said)( 101 bzM ?
?htanhhtanhLayer f3 ),|,( fefet jilexFigure 2: Network structure for computing contextdependent lexical translation scores.
The examplecomputes translation score for word pair (yibula,yibulayin) given its surrounding context.Figure 2 shows the neural network we usedto compute context dependent lexical transla-tion score tlex.
For word pair (ei, fj), we takefixed length windows surrounding both ei and fjas input: (ei?
sw2 , .
.
.
, ei+ sw2 , fj?
tw2 , .
.
.
, fj+ tw2 ),where sw, tw stand window sizes on source andtarget side respectively.
Words are converted toembeddings using the lookup table LT , and thecatenation of embeddings are fed to a classic neu-ral network with two hidden-layers, and the outputof the network is the our lexical translation score:tlex(ei, fj |e, f)= f3 ?
f2 ?
f1 ?
LT (window(ei), window(fj))(6)f1 and f2 layers use htanh as activation functions,while f3 is only a linear transformation with noactivation function.For the distortion td, we could use a lexicalizeddistortion model:td(ai, ai?1|e, f) = td(ai ?
ai?1|window(fai?1))(7)which can be computed by a neural network sim-ilar to the one used to compute lexical transla-tion scores.
If we map jump distance (ai ?
ai?1)to B buckets, we can change the length of theoutput layer to B, where each dimension in theoutput stands for a different bucket of jump dis-tances.
But we found in our initial experimentson small scale data, lexicalized distortion does notproduce better alignment over the simple jump-distance based model.
So we drop the lexicalizeddistortion and reverse to the simple version:td(ai, ai?1|e, f) = td(ai ?
ai?1) (8)Vocabulary V of our alignment model consistsof a source vocabulary Ve and a target vocabu-lary Vf .
As in (Collobert et al, 2011), in additionto real words, each vocabulary contains a specialunknown word symbol ?unk?
to handle unseenwords; two sentence boundary symbols ?s?
and?/s?, which are filled into surrounding windowwhen necessary; furthermore, to handle null align-ment, we must also include a special null symbol?null?.
When fj is null word, we simply fill thesurrounding window with the identical null sym-bols.To decode our model, the lexical translationscores are computed for each source-target wordpair in the sentence pair, which requires goingthrough the neural network (|e| ?
|f|) times; af-ter that, the forward-backward algorithm can beused to find the viterbi path as in the classic HMMmodel.The majority of tunable parameters in ourmodel resides in the lookup table LT , which isa (L ?
(|Ve| + |Vf |))-dimension matrix.
For areasonably large vocabulary, the number is muchsmaller than the number of parameters in classicHMM model, which is in the order of (|Ve|?|Vf |).1The ability to model context is not unique toour model.
In fact, discriminative word alignmentcan model contexts by deploying arbitrary features(Moore, 2005).
Different from previous discrim-inative word alignment, our model does not usemanually engineered features, but learn ?features?automatically from raw words by the neural net-work.
(Berger et al, 1996) use a maximum en-tropy model to model the bag-of-words context forword alignment, but their model treats each wordas a distinct feature, which can not leverage thesimilarity between words as our model.5 TrainingAlthough unsupervised training technique such asContrastive Estimation as in (Smith and Eisner,2005), (Dyer et al, 2011) can be adapted to train1In practice, the number of non-zero parameters in clas-sic HMM model would be much smaller, as many words donot co-occur in bilingual sentence pairs.
In our experiments,the number of non-zero parameters in classic HMM modelis about 328 millions, while the NN model only has about 4millions.169our model from raw sentence pairs, they are toocomputational demanding as the lexical transla-tion probabilities must be computed from neu-ral networks.
Hence, we opt for a simpler su-pervised approach, which learns the model fromsentence pairs with word alignment.
As we donot have a large manually word aligned corpus,we use traditional word alignment models such asHMM and IBM model 4 to generate word align-ment on a large parallel corpus.
We obtain bi-directional alignment by running the usual grow-diag-final heuristics (Koehn et al, 2003) on uni-directional results from both directions, and usethe results as our training data.
Similar approachhas been taken in speech recognition task (Dahl etal., 2012), where training data for neural networkmodel is generated by forced decoding with tradi-tional Gaussian mixture models.Tunable parameters in neural network align-ment model include: word embeddings in lookuptable LT , parametersW l, bl for linear transforma-tions in the hidden layers of the neural network,and distortion parameters sd of jump distance.
Wetake the following ranking loss with margin as ourtraining criteria:loss(?)
=?every (e,f)max{0, 1?
s?
(a+|e, f) + s?
(a?|e, f)}(9)where ?
denotes all tunable parameters, a+ isthe gold alignment path, a?
is the highest scor-ing incorrect alignment path under ?, and s?
ismodel score for alignment path defined in Eq.
5.
One nuance here is that the gold alignment af-ter grow-diag-final contains many-to-many links,which cannot be generated by any path.
Our solu-tion is that for each source word alignment multi-ple target, we randomly choose one link among allcandidates as the golden link.Because our multi-layer neural network is in-herently non-linear and is non-convex, directlytraining against the above criteria is unlikely toyield good results.
Instead, we take the followingsteps to train our model.5.1 Pre-training initial word embedding withmonolingual dataMost parameters reside in the word embeddings.To get a good initial value, the usual approach isto pre-train the embeddings on a large monolin-gual corpus.
We replicate the work in (Collobertet al, 2011) and train word embeddings for sourceand target languages from their monolingual cor-pus respectively.
Our vocabularies Vs and Vt con-tain the most frequent 100,000 words from eachside of the parallel corpus, and all other words aretreated as unknown words.
We set word embed-ding length to 20, window size to 5, and the lengthof the only hidden layer to 40.
Follow (Turian etal., 2010), we randomly initialize all parametersto [-0.1, 0.1], and use stochastic gradient descentto minimize the ranking loss with a fixed learn-ing rate 0.01.
Note that embedding for null wordin either Ve and Vf cannot be trained from mono-lingual corpus, and we simply leave them at theinitial value untouched.Word embeddings from monolingual corpuslearn strong syntactic knowledge of each word,which is not always desirable for word align-ment between some language pairs like Englishand Chinese.
For example, many Chinese wordscan act as a verb, noun and adjective without anychange, while their English counter parts are dis-tinct words with quite different word embeddingsdue to their different syntactic roles.
Thus wehave to modify the word embeddings in subse-quent steps according to bilingual data.5.2 Training neural network based on localcriteriaTraining the network against the sentence levelcriteria Eq.
5 directly is not efficient.
Instead, wefirst ignore the distortion parameters and train neu-ral networks for lexical translation scores againstthe following local pairwise loss:max{0, 1?
t?
((e, f)+|e, f) + t?
((e, f)?|e, f)}(10)where (e, f)+ is a correct word pair, (e, f)?
is awrong word pair in the same sentence, and t?
is asdefined in Eq.
6 .
This training criteria essentiallymeans our model suffers loss unless it gives cor-rect word pairs a higher score than random pairsfrom the same sentence pair with some margin.We initialize the lookup table with embed-dings obtained from monolingual training, andrandomly initialize all W l and bl in linear layersto [-0.1, 0.1].
We minimize the loss using stochas-tic gradient descent as follows.
We randomly cy-cle through all sentence pairs in training data; foreach correct word pair (including null alignment),we generate a positive example, and generate twonegative examples by randomly corrupting either170side of the pair with another word in the sentencepair.
We set learning rate to 0.01.
As there is noclear stopping criteria, we simply run the stochas-tic optimizer through parallel corpus for N itera-tions.
In this work, N is set to 50.To make our model concrete, there are stillhyper-parameters to be determined: the windowsize sw and tw, the length of each hidden layerLl.
We empirically set sw and tw to 11, L1 to120, and L2 to 10, which achieved a minimal losson a small held-out data among several settings wetested.5.3 Training distortion parametersWe fix neural network parameters obtained fromthe last step, and tune the distortion parameterssd with respect to the sentence level loss usingstandard stochastic gradient descent.
We use aseparate parameter for jump distance from -7 and7, and another two parameters for longer for-ward/backward jumps.
We initialize all parame-ters in sd to 0, set the learning rate for the stochas-tic optimizer to 0.001.
As there are only 17 param-eters in sd, we only need to run the optimizer overa small portion of the parallel corpus.5.4 Tuning neural network based on sentencelevel criteriaUp-to-now, parameters in the lexical translationneural network have not been trained against thesentence level criteria Eq.
5.
We could achievethis by re-using the same online training methodused to train distortion parameters, except that wenow fix the distortion parameters and let the lossback-propagate through the neural networks.
Sen-tence level training does not take larger context inmodeling word translations, but only to optimizethe parameters regarding to the sentence level loss.This tuning is quite slow, and it did not improvealignment on an initial small scale experiment; so,we skip this step in all subsequent experiment inthis work.6 Experiments and ResultsWe conduct our experiment on Chinese-to-Englishword alignment task.
We use the manually alignedChinese-English alignment corpus (Haghighi etal., 2009) which contains 491 sentence pairs astest set.
We adapt the segmentation on the Chineseside to fit our word segmentation standard.6.1 DataOur parallel corpus contains about 26 millionunique sentence pairs in total which are minedfrom web.The monolingual corpus to pre-train word em-beddings are also crawled from web, whichamounts to about 1.1 billion unique sentences forEnglish and about 300 million unique sentencesfor Chinese.
As pre-processing, we lowercase allEnglish words, and map all numbers to one spe-cial token; and we also map all email addressesand URLs to another special token.6.2 SettingsWe use classic HMM and IBM model 4 as ourbaseline, which are generated by Giza++ (Och andNey, 2000).
We train our proposed model from re-sults of classic HMM and IBM model 4 separately.Since classic HMM, IBM model 4 and our modelare all uni-directional, we use the standard grow-diag-final to generate bi-directional results for allmodels.Models are evaluated on the manually alignedtest set using standard metric: precision, recall andF1-score.6.3 Alignment ResultIt can be seen from Table 1, the proposed modelconsistently outperforms its corresponding base-line whether it is trained from alignment of classicHMM or IBM model 4.
It is also clear that thesetting prec.
recall F-1HMM 0.768 0.786 0.777HMM+NN 0.810 0.790 0.798IBM4 0.839 0.805 0.822IBM4+NN 0.885 0.812 0.847Table 1: Word alignment result.
The first rowand third row show baseline results obtained byclassic HMM and IBM4 model.
The second rowand fourth row show results of the proposed modeltrained from HMM and IBM4 respectively.results of our model also depends on the qualityof baseline results, which is used as training dataof our model.
In future we would like to explorewhether our method can improve other word align-ment models.We also conduct experiment to see the effecton end-to-end SMT performance.
We train hier-171archical phrase model (Chiang, 2007) from dif-ferent word alignments.
Despite different align-ment scores, we do not obtain significant differ-ence in translation performance.
In our C-E exper-iment, we tuned on NIST-03, and tested on NIST-08.
Case-insensitive BLEU-4 scores on NIST-08test are 0.305 and 0.307 for models trained fromIBM-4 and NN alignment results.
The result is notsurprising considering our parallel corpus is quitelarge, and similar observations have been made inprevious work as (DeNero and Macherey, 2011)that better alignment quality does not necessarilylead to better end-to-end result.6.4 Result Analysis6.4.1 Error AnalysisFrom Table 1 we can see higher F-1 score of ourmodel mainly comes from higher precision, withrecall similar to baseline.
By analyzing the results,we found out that for both baseline and our model,a large part of missing alignment links involvesstop words like English words ?the?, ?a?, ?it?
andChinese words ?de?.
Stop words are inherentlyhard to align, which often requires grammaticaljudgment unavailable to our models; as they arealso extremely frequent, our model fully learnstheir alignment patterns of the baseline models,including errors.
On the other hand, our modelperforms better on low-frequency words, espe-cially proper nouns.
Take person names for ex-ample.
Most names are low-frequency words, onwhich baseline HMM and IBM4 models show the?garbage collector?
phenomenon.
In our model,different person names have very similar word em-beddings on both English side and Chinese side,due to monolingual pre-training; what is more, dif-ferent person names often appear in similar con-texts.
As our model considers both word embed-dings and contexts, it learns that English personnames should be aligned to Chinese person names,which corrects errors of baseline models and leadsto better precision.6.4.2 Effect of contextTo examine how context contribute to alignmentquality, we re-train our model with different win-dow size, all from result of IBM model 4.
FromFigure 3, we can see introducing context increasethe quality of the learned alignment, but the ben-efit is diminished for window size over 5.
On theother hand, the results are quite stable even withlarge window size 13, without noticeable over-0.740.760.780.80.820.840.861 3 5 7 9 11 13Figure 3: Effect of different window sizes on wordalignment F-score.fitting problem.
This is not surprising consider-ing that larger window size only requires slightlymore parameters in the linear layers.
Lastly, itis worth noticing that our model with no context(window size 1) performs much worse than set-tings with larger window size and baseline IBM4.Our explanation is as follows.
Our model usesthe simple jump distance based distortion, whichis weaker than the more sophisticated distortionsin IBM model 4; thus without context, it does notperform well compared to IBM model 4.
Withlarger window size, our model is able to producemore accurate translation scores based on morecontexts, which leads to better alignment despitethe simpler distortions.IBM4+NN F-11-hidden-layer 0.8342-hidden-layer 0.8473-hidden-layer 0.843Table 3: Effect of different number of hidden lay-ers.
Two hidden layers outperform one hiddenlayer, while three hidden layers do not bring fur-ther improvement.6.4.3 Effect of number of hidden layersOur neural network contains two hidden layers be-sides the lookup layer.
It is natural to ask whetheradding more layers would be beneficial.
To an-swer this question, we train models with 1, 2 and3 layers respectively, all from result of IBM model4.
For 1-hidden-layer setting, we set the hiddenlayer length to 120; and for 3-hidden-layer set-ting, we set hidden layer lengths to 120, 100, 10respectively.
As can be seen from Table 3, 2-hidden-layer outperforms the 1-hidden-layer set-ting, while another hidden layer does not bring172word good history british served labs zetian laggardsLMbad tradition russian worked networks hongzhang underperformersgreat culture japanese lived technologies yaobang transfereesstrong practice dutch offered innovations keming megabankstrue style german delivered systems xingzhi mutualseasy literature canadian produced industries ruihua non-startersWAnice historical uk offering lab hongzhang underperformersgreat historic britain serving laboratories qichao illiteratesbest developed english serve laboratory xueqin transfereespretty record classic delivering exam fuhuan matriculantsexcellent recording england worked experiments bingkun megabanksTable 2: Nearest neighbors of several words according to their embedding distance.
LM shows neighborsof word embeddings trained by monolingual language model method; WA shows neighbors of wordembeddings trained by our word alignment model.improvement.
Due to time constraint, we havenot tuned the hyper-parameters such as length ofhidden layers in 1 and 3-hidden-layer settings, norhave we tested settings with more hidden-layers.It would be wise to test more settings to verifywhether more layers would help.6.4.4 Word EmbeddingFollowing (Collobert et al, 2011), we show somewords together with its nearest neighbors using theEuclidean distance between their embeddings.
Aswe can see from Table 2, after bilingual training,?bad?
is no longer in the nearest neighborhood of?good?
as they hold opposite semantic meanings;the nearest neighbor of ?history?
is now changedto its related adjective ?historical?.
Neighbors ofproper nouns such as person names are relativelyunchanged.
For example, neighbors of word?zetian?
are all Chinese names in both settings.As Chinese language lacks morphology, the singleform and plural form of a noun in English oftencorrespond to the same Chinese word, thus it isdesirable that the two English words should havesimilar word embeddings.
While this is true forrelatively frequent nouns such as ?lab?
and ?labs?,rarer nouns still remain near their monolingualembeddings as they are only modified a few timesduring the bilingual training.
As shown in lastcolumn, neighborhood of ?laggards?
still consistsof other plural forms even after bilingual training.7 ConclusionIn this paper, we explores applying deep neu-ral network for word alignment task.
Our modelintegrates a multi-layer neural network into anHMM-like framework, where context dependentlexical translation score is computed by neuralnetwork, and distortion is modeled by a sim-ple jump-distance scheme.
Our model is dis-criminatively trained on bilingual corpus, whilehuge monolingual data is used to pre-train word-embeddings.
Experiments on large-scale Chinese-to-English task show that the proposed methodproduces better word alignment results, comparedwith both classic HMM model and IBM model 4.For future work, we will investigate more set-tings of different hyper-parameters in our model.Secondly, we want to explore the possibility ofunsupervised training of our neural word align-ment model, without reliance of alignment resultof other models.
Furthermore, our current modeluse rather simple distortions; it might be helpfulto use more sophisticated model such as ITG (Wu,1997), which can be modeled by Recursive NeuralNetworks (Socher et al, 2011).AcknowledgmentsWe thank anonymous reviewers for insightfulcomments.
We also thank Dongdong Zhang, LeiCui, Chunyang Wu and Zhenyan He for fruitfuldiscussions.ReferencesYoshua Bengio, Holger Schwenk, Jean-Se?bastienSene?cal, Fre?deric Morin, and Jean-Luc Gauvain.2006.
Neural probabilistic language models.
Inno-vations in Machine Learning, pages 137?186.Yoshua Bengio, Pascal Lamblin, Dan Popovici, and173Hugo Larochelle.
2007.
Greedy layer-wise trainingof deep networks.
Advances in neural informationprocessing systems, 19:153.Yoshua Bengio.
2009.
Learning deep architectures forai.
Foundations and Trends R?
in Machine Learning,2(1):1?127.Adam L. Berger, Vincent J. Della Pietra, and StephenA.
Della Pietra.
1996.
A maximum entropy ap-proach to natural language processing.
Comput.Linguist., 22(1):39?71, March.JS Bridle.
1990.
Neurocomputing: Algorithms, archi-tectures and applications, chapter probabilistic inter-pretation of feedforward classification network out-puts, with relationships to statistical pattern recogni-tion.Peter F Brown, Vincent J Della Pietra, Stephen A DellaPietra, and Robert L Mercer.
1993.
The mathemat-ics of statistical machine translation: Parameter esti-mation.
Computational linguistics, 19(2):263?311.David Chiang.
2007.
Hierarchical phrase-based trans-lation.
computational linguistics, 33(2):201?228.Ronan Collobert, Jason Weston, Le?on Bottou, MichaelKarlen, Koray Kavukcuoglu, and Pavel Kuksa.2011.
Natural language processing (almost) fromscratch.
The Journal of Machine Learning Re-search, 12:2493?2537.George E Dahl, Dong Yu, Li Deng, and Alex Acero.2012.
Context-dependent pre-trained deep neuralnetworks for large-vocabulary speech recognition.Audio, Speech, and Language Processing, IEEETransactions on, 20(1):30?42.John DeNero and Klaus Macherey.
2011.
Model-based aligner combination using dual decomposi-tion.
In Proc.
ACL.Chris Dyer, Jonathan Clark, Alon Lavie, and Noah ASmith.
2011.
Unsupervised word alignment with ar-bitrary features.
In Proceedings of the 49th AnnualMeeting of the Association for Computational Lin-guistics: Human Language Technologies-Volume 1,pages 409?419.
Association for Computational Lin-guistics.Aria Haghighi, John Blitzer, John DeNero, and DanKlein.
2009.
Better word alignments with su-pervised itg models.
In Proceedings of the JointConference of the 47th Annual Meeting of the ACLand the 4th International Joint Conference on Natu-ral Language Processing of the AFNLP: Volume 2-Volume 2, pages 923?931.
Association for Compu-tational Linguistics.Geoffrey E Hinton, Simon Osindero, and Yee-WhyeTeh.
2006.
A fast learning algorithm for deep be-lief nets.
Neural computation, 18(7):1527?1554.Koray Kavukcuoglu, Pierre Sermanet, Y-Lan Boureau,Karol Gregor, Michae?l Mathieu, and Yann LeCun.2010.
Learning convolutional feature hierarchies forvisual recognition.
Advances in Neural InformationProcessing Systems, pages 1090?1098.Philipp Koehn, Franz Josef Och, and Daniel Marcu.2003.
Statistical phrase-based translation.
InProceedings of the 2003 Conference of the NorthAmerican Chapter of the Association for Computa-tional Linguistics on Human Language Technology-Volume 1, pages 48?54.
Association for Computa-tional Linguistics.Alex Krizhevsky, Ilya Sutskever, and Geoff Hinton.2012.
Imagenet classification with deep convolu-tional neural networks.
In Advances in Neural Infor-mation Processing Systems 25, pages 1106?1114.Yann LeCun, Le?on Bottou, Yoshua Bengio, and PatrickHaffner.
1998.
Gradient-based learning applied todocument recognition.
Proceedings of the IEEE,86(11):2278?2324.Yann LeCun.
1985.
A learning scheme for asymmet-ric threshold networks.
Proceedings of Cognitiva,85:599?604.Honglak Lee, Alexis Battle, Rajat Raina, and An-drew Y Ng.
2007.
Efficient sparse coding algo-rithms.
Advances in neural information processingsystems, 19:801.Shujie Liu, Chi-Ho Li, and Ming Zhou.
2010.
Dis-criminative pruning for discriminative itg alignment.In Proceedings of the 48th Annual Meeting of the As-sociation for Computational Linguistics, ACL, vol-ume 10, pages 316?324.Y MarcAurelio Ranzato, Lan Boureau, and Yann Le-Cun.
2007.
Sparse feature learning for deep beliefnetworks.
Advances in neural information process-ing systems, 20:1185?1192.Robert C Moore.
2005.
A discriminative frameworkfor bilingual word alignment.
In Proceedings ofthe conference on Human Language Technology andEmpirical Methods in Natural Language Process-ing, pages 81?88.
Association for ComputationalLinguistics.Jan Niehues and Alex Waibel.
2012.
Continuousspace language models using restricted boltzmannmachines.
In Proceedings of the nineth Interna-tional Workshop on Spoken Language Translation(IWSLT).Franz Josef Och and Hermann Ney.
2000.
Giza++:Training of statistical translation models.Frank Seide, Gang Li, and Dong Yu.
2011.
Conversa-tional speech transcription using context-dependentdeep neural networks.
In Proc.
Interspeech, pages437?440.174Noah A Smith and Jason Eisner.
2005.
Contrastiveestimation: Training log-linear models on unlabeleddata.
In Proceedings of the 43rd Annual Meetingon Association for Computational Linguistics, pages354?362.
Association for Computational Linguis-tics.Richard Socher, Cliff C Lin, Andrew Y Ng, andChristopher D Manning.
2011.
Parsing naturalscenes and natural language with recursive neu-ral networks.
In Proceedings of the 26th Inter-national Conference on Machine Learning (ICML),volume 2, page 7.Richard Socher, Brody Huval, Christopher D Manning,and Andrew Y Ng.
2012.
Semantic compositional-ity through recursive matrix-vector spaces.
In Pro-ceedings of the 2012 Joint Conference on Empiri-cal Methods in Natural Language Processing andComputational Natural Language Learning, pages1201?1211.
Association for Computational Linguis-tics.Le Hai Son, Alexandre Allauzen, and Franc?ois Yvon.2012.
Continuous space translation models withneural networks.
In Proceedings of the 2012 confer-ence of the north american chapter of the associa-tion for computational linguistics: Human languagetechnologies, pages 39?48.
Association for Compu-tational Linguistics.Ivan Titov, Alexandre Klementiev, and Binod Bhat-tarai.
2012.
Inducing crosslingual distributed rep-resentations of words.Joseph Turian, Lev Ratinov, and Yoshua Bengio.
2010.Word representations: a simple and general methodfor semi-supervised learning.
Urbana, 51:61801.Stephan Vogel, Hermann Ney, and Christoph Tillmann.1996.
Hmm-based word alignment in statisticaltranslation.
In Proceedings of the 16th conferenceon Computational linguistics-Volume 2, pages 836?841.
Association for Computational Linguistics.Dekai Wu.
1997.
Stochastic inversion transductiongrammars and bilingual parsing of parallel corpora.Computational linguistics, 23(3):377?403.175
