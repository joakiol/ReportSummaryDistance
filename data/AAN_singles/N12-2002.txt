Proceedings of the NAACL HLT 2012 Student Research Workshop, pages 7?10,Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational LinguisticsAutomatic Animacy ClassificationSamuel R. BowmanDepartment of LinguisticsStanford University450 Serra MallStanford, CA 94305-2150sbowman@stanford.eduHarshit ChopraDepartment of Computer ScienceStanford University353 Serra MallStanford, CA 94305-9025harshitc@stanford.eduAbstractWe introduce the automatic annotation ofnoun phrases in parsed sentences with tagsfrom a fine-grained semantic animacy hierar-chy.
This information is of interest within lex-ical semantics and has potential value as a fea-ture in several NLP tasks.We train a discriminative classifier on an an-notated corpus of spoken English, with fea-tures capturing each noun phrase?s constituentwords, its internal structure, and its syntacticrelations with other key words in the sentence.Only the first two of these three feature setshave a substantial impact on performance, butthe resulting model is able to fairly accuratelyclassify new data from that corpus, and showspromise for binary animacy classification andfor use on automatically parsed text.1 IntroductionAn animacy hierarchy, in the sense of Zaenen et al(2004), is a set of mutually exclusive categories de-scribing noun phrases (NPs) in natural language sen-tences.
These classes capture the degree to whichthe entity described by an NP is capable of human-like volition: a key lexical semantic property whichhas been shown to trigger a number of morphologi-cal and syntactic phenomena across languages.
An-notating a corpus with this information can facili-tate statistical semantic work, as well as providing apotentially valuable feature?discussed in Zaenen etal.
?for tasks like relation extraction, parsing1, and1Using our model in parsing would require bootstrappingfrom c oarser parses, as our model makes use of some syntacticfeatures.machine translation.The handful of papers that we have found onanimacy annotation?centrally Ji and Lin (2009),?vrelid (2005), and Orasan and Evans (2001)?classify only the basic ANIMATE/INANIMATE con-trast, but show some promise in doing so.
Theirwork shows success in automatically classifying in-dividual words, and related work has shown that an-imacy can be used to improve parsing performance(?vrelid and Nivre, 2007).We adopt the class set presented in Zaenen et al(2004), and build our model around the annotatedcorpus presented in that work.
Their hierarchy con-tains ten classes, meant to cover a range of cate-gories known to influence animacy-related phenom-ena cross-linguistically.
They are HUMAN, ORG (or-ganizations), ANIMAL, MAC (automata), VEH (vehi-cles), PLACE, TIME, CONCRETE (other physical ob-jects), NONCONC (abstract entities), and MIX (NPsdescribing heterogeneous groups of entities).
Theclass definitions are straightforward?every NP de-scribing a vehicle is a VEH?and Zaenen et al of-fer a detailed treatment of ambiguous cases.
Unlikethe class sets used in named entity recognition work,these classes are crucially meant to cover all NPs.This includes freestanding nouns like people, as wellas pronominals like that one, for which the choice ofclass often depends on contextual information notcontained within the NP, or even the sentence.In the typical case where the head of an NP be-longs unambiguously to a single animacy class, thephrase as a whole nearly always takes on the classof its head: The Panama hat I gave to my uncleon Tuesday contains numerous nominals of differ-7ent animacy classes, but hat is the unique syntactichead, and determines the phrase to be CONCRETE.Heads can easily be ambiguous, though: My stereospeakers and the speakers at the panel session be-long to different classes, but share a (polysemous)head.The corpus that we use is Zaenen et al?s animacy-annotated subset of the hand-parsed Switchboardcorpus of conversational American English.
It isbuilt on, and now included in, Calhoun et al?s(2010) NXT version of Switchboard.
This anno-tated section consists of about 110,000 sentenceswith about 300,000 NPs.
We divide these sentencesinto a training set (80%), a development set (10%),and a test set (10%).2 Every NP in this section is ei-ther assigned a class or marked as problematic, andwe train and test on all the NPs for which the an-notators were able to agree (after discussion) on anassignment.2 MethodsWe use a standard maximum entropy classifier(Berger et al, 1996) to classify constituents: Foreach labeled NP in the corpus, the model selectsthe locally most probable class.
Our features are de-scribed in this section.We considered features that required dependen-cies between consecutively assigned classes, allow-ing large NPs to depend on smaller NPs containedwithin them, as in conjoined structures.
Theseachieved somewhat better coverage of the rare MIXclass, but did not yield any gains in overall perfor-mance, and are not included in our results.2.1 Bag-of-words featuresOur simplest feature set, HASWORD-(tag-)word,simply captures each word in the NP, both with andwithout its accompanying part-of-speech (POS) tag.2.2 Internal syntactic featuresMotivated by the observation that syntactic headstend to determine animacy class, we introduce twofeatures: HEAD-tag-word contains the head word ofthe phrase (extracted automatically from the parse)2We inadvertently did some initial feature selection usingtraining data that included both our training and test sets.
Whilewe have re-run all of those experiments, this introduces a possi-ble bias towards features which perform well on our test set.and its POS tag.
HEADSHAPE-tag-shape attemptsto cover unseen head words by replacing the wordstring with its orthographic shape (substituting, forexample, Stanford with Ll and 3G-related with dL-l).2.3 External syntactic featuresThe information captured by our tag set overlapsconsiderably with the information that verbs use toselect their arguments.3 The subject of see, for ex-ample, must be a HUMAN, MAC, ANIMAL, or ORG,and the complement of above cannot be a TIME.
Assuch, we expect the verb or preposition that an NPdepends upon and the type of dependency involved(subject, direct object, or prepositional complement)to be powerful predictors of animacy, and introducethe following features: SUBJ(-OF-verb), DOBJ(-OF-verb) and PCOMP(-OF-prep)(-WITH-verb).
We ex-tract these dependency relations from our parses,and mark an occurrence of each feature both withand without each of its optional (parenthetical) pa-rameters.3 ResultsThe following table shows our model?s precisionand recall (as percentages) for each class and themodel?s overall accuracy (the percent of labeledNPs which were labeled correctly), as well as thenumber of instances of each class in the test set.Class Count Precision RecallVEH 534 88.56 39.14TIME 1,101 88.24 80.38NONCONC 12,173 83.39 93.32MAC 79 63.33 24.05PLACE 754 64.89 63.00ORG 1,208 58.26 27.73MIX 29 7.14 3.45CONCRETE 1402 58.82 37.58ANIMAL 137 69.44 18.25HUMAN 11,320 91.19 93.30Overall 28,737 Accuracy: 84.90The next table shows the performance of eachfeature bundle when it alone is used in classification,as well as the performance of the model when each3See Levin and Rappaport Hovav (2005) for a survey of ar-gument selection criteria, including animacy.8feature bundle is excluded.
We offer for comparisona baseline model that always chooses the mostfrequent class, NONCONC.Only these features: Accuracy (%)Bag of words 83.04Internal Syntactic 75.85External Syntactic 50.35All but these features: ?Bag of words 77.02Internal syntactic 83.36External syntactic 84.58Most frequent class 42.36Full model 84.903.1 Binary classificationWe test our model?s performance on thesomewhat better-known task of binary(ANIMATE/INANIMATE) classification by mergingthe model?s class assignments into two sets afterclassification, following the grouping defined in Za-enen et al4 While none of our architectural choiceswere made with binary classification in mind, it isheartening to know that the model performs well onthis easier task.Overall accuracy is 93.50%, while a baselinemodel that labels each NP ANIMATE achieves only53.79%.
All of the feature sets contribute mea-surably to the binary model, and external syntacticfeatures do much better on this task than on fine-grained classification, despite remaining the worstof the three sets: They achieve 78.66% when usedalone.
We have found no study on animacy in spo-ken English with which to compare these results.3.2 Automatically parsed dataIn order to test the robustness of our model to theerrors introduced by an automatic parser, we trainan instance of the Stanford parser (Klein and Man-ning, 2002) on our training data (which is relativelysmall by parsing standards), re-parse the linearizedtest data, and then train and test our classifier on theresulting trees.Since we can only confidently evaluate classifi-cation choices for correctly parsed constituents, we4HUMAN, VEH, MAC, ORG, ANIMAL, and HUMAN are con-sidered animate, and the remaining classes inanimate.consider accuracy measured only over those hypoth-esized NPs which encompass the same string ofwords as an NP in the gold standard data.
Ourparser generated correct (evaluable) NPs with preci-sion 88.63% and recall 73.51%, but for these evalu-able NPs, accuracy was marginally better than onhand-parsed data: 85.43% using all features.
Theparser likely tended to misparse those NPs whichwere hardest for our model to classify.3.3 Error analysisA number of the errors made by the model pre-sented above stem from ambiguous cases wherehead words, often pronouns, can take on referents ofmultiple animacy classes, and where there is no clearevidence within the bounds of the sentence of whichone is correct.
In the following example the modelincorrectly assigns mine the class CONCRETE, andnothing in the sentence provides evidence for thesurprising correct class, HUMAN.Well, I?ve used mine on concrete treatedwood.For a model to correctly treat cases like this, it wouldbe necessary to draw on a simple co-reference reso-lution system and incorporate features dependent onplausibly co-referent sentences elsewhere in the text.The distinction between an organization (ORG)and a non-organized group of people (HUMAN) inthis corpus is troublesome for our model.
It hingeson whether the group shares a voice or purpose,which requires considerable insight into the mean-ing of a sentence to assess.
For example, people inthe below is an ORG, but no simple lexical or syntac-tic cues distinguish it from the more common classHUMAN.The only problem is, of course, that, uh,that requires significant commitment frompeople to actually decide they want to putthings like that up there.Our performance on the class MIX, which marksNPs describing multiple heterogeneous entities, wasvery poor.
The highlighted NP in the sentence belowwas incorrectly classified NONCONC:But the same money could probably be farbetter spent on, uh, uh, lunar bases and9solar power satellite research and, youknow, so forth.It is quite plausible that some more sophisticatedapproaches to modeling this unique class might besuccessful, but no simple feature that we tried hadany success, and the effect of missing MIX on overallperformance is negligible.There are finally some cases where our attemptsto rely on the heads of NPs were thwarted by the rel-atively flat structure of the parses.
Under any main-stream theory of syntax, home is more prominentthan nursing in the phrase a nursing home: It is theunique head of the NP.
However, the parse provideddoes not attribute any internal structure to this con-stituent, making it impossible for the model to deter-mine the relative prominence of the two nouns.
Hadthe model known that the unique head of the phrasewas home, it would have likely have correctly clas-sified it as a PLACE, rather than the a priori moreprobable NONCONC.4 Conclusion and future workWe succeeded in developing a classifier capable ofannotating texts with a potentially valuable feature,with a high tolerance for automatically generatedparses, and using no external or language-specificsources of knowledge.We were somewhat surprised, though, by the rel-atively poor performance of the external syntacticfeatures in this model: When tested alone, theyachieved an accuracy of only about 50%.
This sig-nals one possible site for further development.Should this model be used in a setting where ex-ternal knowledge sources are available, two seemespecially promising.
Synonyms and hypernymsfrom WordNet (Fellbaum, 2010) or a similar lexi-con could be used to improve the model?s handlingof unknown words?demonstrated successfully withthe aid of a word sense disambiguation systemin Orasan and Evans (2001) for binary animacyclassification on single words.
A lexical-semanticdatabase like FrameNet (Baker et al, 1998) couldalso be used to introduce semantic role labels (whichare tied to animacy restrictions) as features, poten-tially rescuing the intuition that governing verbs andprepositions carry animacy information.AcknowledgmentsWe are indebted to Marie-Catherine de Marneffe andJason Grafmiller, who first suggested we model thiscorpus, and to Chris Manning and our reviewers forvaluable advice.ReferencesC.F.
Baker, C.J.
Fillmore, and J.B. Lowe.
1998.
TheBerkeley Framenet Project.
In Proc.
of the 36thAnnual Meeting of the Association for Computa-tional Linguistics and 17th International Conferenceon Computational Linguistics.A.L.
Berger, V.J Della Pietra, and S.A. Della Pietra.1996.
A maximum entropy approach to natural lan-guage processing.
Computational Linguistics, 22(1).S.
Calhoun, J. Carletta, J.M.
Brenier, N. Mayo, D. Juraf-sky, M. Steedman, and D. Beaver.
2010.
The NXT-format Switchboard Corpus.
Language resources andevaluation, 44(4).C.
Fellbaum.
2010.
Wordnet.
In Theory and Applica-tions of Ontology: Computer Applications.
Springer.H.
Ji and D. Lin.
2009.
Gender and animacy knowledgediscovery from web-scale N-grams for unsupervisedperson mention detection.
Proc.
of the 23rd PacificAsia Conference on Language, Information and Com-putation.D.
Klein and C.D.
Manning.
2002.
Fast exact infer-ence with a factored model for natural language pars-ing.
Advances in neural information processing sys-tems, 15(2002).B.
Levin and M. Rappaport Hovav.
2005.
Argument Re-alization.
Cambridge.C.
Orasan and R. Evans.
2001.
Learning to identify an-imate references.
Proc.
of the Workshop on Computa-tional Natural Language Learning, 7.L.
?vrelid and J. Nivre.
2007.
When word order andpart-of-speech tags are not enough?Swedish depen-dency parsing with rich linguistic features.
In Proc.
ofthe International Conference on Recent Advances inNatural Language Processing.Lilja ?vrelid.
2005.
Animacy classification basedon morphosyntactic corpus frequencies: some exper-iments with Norwegian nouns.
In Proc.
of the Work-shop on Exploring Syntactically Annotated Corpora.A.
Zaenen, J. Carletta, G. Garretson, J. Bresnan,A.
Koontz-Garboden, T. Nikitina, M.C.
O?Connor, andT.
Wasow.
2004.
Animacy encoding in English: whyand how.
In Proc.
of the Association for Computa-tional Linguistics Workshop on Discourse Annotation.10
