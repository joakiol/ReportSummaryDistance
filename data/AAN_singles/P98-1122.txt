Characterizing and Recognizing Spoken Corrections inHuman-Computer DialogueGina-Anne LevowMIT AI LaboratoryRoom 769, 545 Technology SqCambridge, MA 02139gina@ai.mit.eduAbst rac tMiscommunication i speech recognition sys-tems is unavoidable, but a detailed character-ization of user corrections will enable speechsystems to identify when a correction is takingplace and to more accurately recognize the con-tent of correction utterances.
In this paper weinvestigate the adaptations of users when theyencounter recognition errors in interactions witha voice-in/voice-out spoken language system.
Inanalyzing more than 300 pairs of original and re-peat correction utterances, matched on speakerand lexical content, we found overall increasesin both utterance and pause duration from orig-inal to correction.
Interestingly, corrections ofmisrecognition errors (CME) exhibited signifi-cantly heightened pitch variability, while cor-rections of rejection errors (CRE) showed only asmall but significant decrease in pitch minimum.CME's demonstrated much greater increases inmeasures of duration and pitch variability thanCRE's.
These contrasts allow the developmentof decision trees which distinguish CME's fromCRE's and from original inputs at 70-75% ac-curacy based on duration, pitch, and amplitudefeatures.1 IntroductionThe frequent recognition errors which plaguespeech recognition systems present a signifi-cant barrier to widespread acceptance of thistechnology.
The difficulty of correcting sys-tem misrecognitions is directly correlated withuser assessments of system quality.
The in-creased probability of recognition errors imme-diately after an error compounds this prob-lem.
Thus, it becomes crucially importantto characterize the differences between origi-nal utterances and user corrections of systemrecognition failures both in order to recognizewhen a user attempts a correction, indicating aprior recognition error, and to improve recogni-tion accuracy on these problematic utterances.Analysis of data drawn from a field trial ofa telephone-based voice-in/voice-out conversa-tional system demonstrates significant differ-ences between original inputs and corrections inmeasures of duration, pause, and pitch.
Thesedifferences in turn aid in the development of de-cision trees which distinguish between ew in-put and user corrections.2 Overv iewWe begin by describing the framework in whichthe utterances used for these experiments werecollected.
We then describe the acoustic fea-tures used in the data analysis and decision treedesign and motivate this choice by consideringsome related work.
A report of the results of thedescriptive analysis follows and demonstratesthe significant differences among original inputsand different ypes of corrections.
We then turnto the issues involved in the actual constructionand assessment of decision trees which identifyspoken corrections.
The best of these classi-fiers achieves 75% success rates.
Finally, weconclude with a plan to use these results toimprove recognition accuracy on spoken correc-tions and ease recovery from human-computermiscommunication.3 Sys tem,  Subjects, and Er rorsThe utterances used in the design and analy-sis of the decision tree classifiers were drawnfrom approximately 60 hours of user interac-tions in a field trial of the Sun MicrosystemsSpeechActs ystem (Yankelovich et al, 1995).SpeechActs provides a voice-only interface tocommon desktop applications uch as e-mail,calendar, weather reports, stock quotes, and736time and currency conversions.
The systemallows conversational interactions with speechrecognition provided by BBN's HARK recog-nizer and speech synthesis through Centigram'sTruVoice system.
A sample interaction appearsbelow.SpeechActs:User:SpeechActs:User:SpeechActs:User:SpeechActs:User:SpeechActs:User:SpeechActs:User:You have 14 new messages and47 old messages.
The first newmessage is from Eric Baatzregarding "report completed.
"Let me hear it.
"The first draft is ready foryour comments.
Eric.
"Reply and include current message.Preparing message to Eric Baatz.Rejection: no recognition result; no actionBegin recording after the tone.When finished, pause several seconds.
?f this paper will identify common acousticchanges which characterize this error correction Eric, I'll get those to youthis afternoon.Say cancel, send, or review.Send.Message sent.
What now?Next message.New message two from StuartAdams, subject "Can we meet at3:00 today?
"Switch to calendar... 1Overall there were 7752 user utterancesrecorded, of which 1961 resulted in a label of ei-ther 'Error' or 'Rejection', giving an error rateof 25%.
1250 utterances, almost two-thirds ofthe errors, produced outright rejections, while706 errors were substitution misrecognitions.The remainder of the errors were due to sys-tem crashes or parser errors.
The probabilityof experiencing a recognition failure after a cor-rect recognition was 16%, but immediately afteran incorrect recognition it was 44%, 2.75 timesgreater.
This increase in error likelihood sug-gests a change in speaking style which divergesfrom the recognizer's model.
The remainderThe field trial involved a group of nineteensubjects.
Four of the participants were membersof the system development s aff, fourteen werevolunteers drawn from Sun Microsystems' taff,and a final class of subjects consisted of one-time guest users There were three female andsixteen male subjects.All interactions with the system wererecorded and digitized in standard telephoneaudio quality format at 8kHz sampling in 8-bitmu-law encoding during the conversation.
Inaddition, speech recognition results, parser re-sults, and synthesized responses were logged.
Apaid assistant then produced a correct verbatimtranscript of all user utterances and, by compar-ing the transcription to the recognition results,labeled each utterance with one of four accuracycodes as described below.OK: recognition correct; action correctError Minor: recognition ot exact; action correctError: recognition incorrect; action incorrectspeaking style.
This description leads to the de-velopment of a decision tree classifier which canlabel utterances as corrections or original input.4 Re la ted  WorkSince full voice-in/voice-out spoken languagesystems have only recently been developed, lit-tle work has been done on error correction di-alogs in this context.
Two areas of related re-search that have been investigated are the iden-tification of self-repairs and disfluencies, wherethe speaker self-interrupts o change an utter-ance in progress, and some preliminary effortsin the study of corrections in speech input.In analyzing and identifying self-repairs,(Bear et al, 1992) and (Heeman and Allen,1994) found that the most effective methodsrelied on identifying shared textual regions be-tween the reparandum and the repair.
However,these techniques are limited to those instanceswhere a reliable recognition string is available;in general, that is not the case for most speechrecognition systems currently available.
Alter-native approaches described in (Nakatani andHirschberg, 1994) and (Shriberg et al, 1997),have emphasized acoustic-prosodic cues, includ-ing duration, pitch, and amplitude as discrimi-nating features.The few studies that have focussed on spokencorrections of computer misrecognitions, (Ovi-att et al, 1996) and (Swerts and Ostendorf,1995), also found significant effects of duration,and in Oviatt et al, pause insertion and length-737ening played a role.
However, in only one ofthese studies was input "conversational", theother was a form-filling application, and nei-ther involved spoken system responses, relyinginstead on visual displays for feedback, with po-tential impact on speaking style.5 Er ror  Data ,  Features ,  andExamplesFor these experiments, we selected pairs of ut-terances: the first (original) utterance is thefirst attempt by the user to enter an input ora query; the second (repeat) follows a systemrecognition error, either misrecognition or re-jection, and tries to correct he mistake in thesame words as the original.
For example,SYSTEM SAID: Please say mail, calendar,weather, stock quotes orstart over to begin again.USER SAID: MAILSYSTEM HEARD: MAILCODE: OKSYSTEM SAID: Switching to mail.Your first message is ..USER SAID:Read message four eight nineSYSTEM HEARD: "nothing"CODE: RejectionSYSTEM SAID: Sorry ?USER SAID:Read message four eight nineSYSTEM HEARD: "nothing"CODE: RejectionSYSTEM SAID: Still no luck.
Speak clearly,but don't over emphasize .USER SAID:Go to message four eight nineSYSTEM HEARD: Go to message fourplease ummCODE: ERRORSYSTEM SAID: I heard goto new message 4.In total, there were 302 of these original-repeatpairs: 214 resulting from rejections, and 88 frommisrecognitions.Following (Oviatt et al, 1996), (Shriberg etal., 1997), and (Ostendorf et al, 1996), wecoded a set of acoustic-prosodic features to de-scribe the utterances.
These features fall intofour main groups: durational, pause, pitch, andamplitude.
We further selected variants of thesefeature classes that could be scored automati-cally, or at least mostly automatically with someFigure 1: A lexically matched pair where therepeat (bottom) has an 18% increase in totalduration and a 400% increase in pause duration.minor hand-adjustment.
We hoped that thesefeatures would be available during the recog-nition process so that ultimately the original-repeat correction contrasts would be identifiedautomatically.5.1 Durat ionThe basic duration measure is total utteranceduration.
This value is obtained through a two-step procedure.
First we perform an automaticforced alignment of the utterance to the ver-batim transcription text using the OGI CSLUCSLUsh Toolkit (Colton, 1995).
Then thealignment is inspected and, if necessary, ad-justed by hand to correct for any errors, suchas those caused by extraneous background noiseor non-speech sounds.
A typical alignment ap-pears in Figure 1.
In addition to the sim-ple measure of total duration in milliseconds,a number of derived measures also prove useful.Some examples of such measures are speakingrate in terms of syllables per second and a ra-tio of the actual utterance duration to the meanduration for that type of utterance.5.2 PauseA pause is any region of silence internal to anutterance and longer than 10 milliseconds in du-ration.
Silences preceding unvoiced stops andaffricates were not coded as pauses due to thedifficulty of identifying the onset of consonantsof these classes.
Pause-based features includenumber of pauses, average pause duration, totalpause duration, and silence as a percentage oftotal utterance duration.
An example of pause738.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
, ?
iL?, .Figure 2: Contrasting Falling (top) and Rising(bottom) Pitch Contoursinsertion and lengthening appear in Figure 1.5.3 PitchTo derive pitch features, we first apply theF0 (fundamental frequency) analysis functionfrom the Entropic ESPS Waves+ system (Se-crest and Doddington, 1993) to produce a basicpitch track.
Most of the related work reportedabove had found relationships between the mag-nitude of pitch features and discourse functionrather than presence of accent type, used moreheavily by (Pierrehumbert and Hirschberg,1990), (Hirschberg and Litman, 1993).
Thus,we chose to concentrate on pitch features of theformer type.
A trained analyst examines thepitch track to remove any points of doubling orhalving due to pitch tracker error, non-speechsounds, and excessive glottalization of > 5 sam-ple points.
We compute several derived mea-sures using simple algorithms to obtain F0 max-imum, F0 minimum, F0 range, final F0 contour,slope of maximum pitch rise, slope of maximumpitch fall, and sum of the slopes of the steep-est rise and fall.
Figure 2 depicts a basic pitchcontour.5.4 AmplitudeAmplitude, measuring the loudness of an utter-ance, is also computed using the ESPS Waves+system.
Mean amplitudes are computed overall voiced regions with amplitude > 30dB.
Am-plitude features include utterance mean ampli-tude, mean amplitude of last voiced region, am-plitude of loudest region, standard deviation,and difference from mean to last and maximumto last.6 Descript ive Acoust ic AnalysisUsing the features described above, we per-formed some initial simple statistical analysesto identify those features which would be mostuseful in distinguishing original inputs from re-peat corrections, and corrections of rejection er-rors (CRE) from corrections of misrecognitionerrors (CME).
The results for the most inter-esting features, duration, pause, and pitch, aredescribed below.6.1 DurationTotal utterance duration is significantly greaterfor corrections than for original inputs.
In ad-dition, increases in correction duration relativeto mean duration for the utterance prove signif-icantly greater for CME's than for CRE's.6.2 PauseSimilarly to utterance duration, total pauselength increases from original to repeat.
Fororiginal-repeat pairs where at least one pauseappears, paired t-test on log-transformed datareveal significantly greater pause durations forcorrections than for original inputs.6.3 PitchWhile no overall trends reached significance forpitch measures, CRE's and CME's, when con-sidered separately, did reveal some interestingcontrasts between corrections and original in-puts within each subset and between the twotypes of corrections.
Specifically, male speakersshowed a small but significant decrease in pitchminimum for CRE's.CME's produced two unexpected results.First they displayed a large and significant in-crease in pitch variability f rom original to re-peat as measured the slope of the steepest rise,while CRE's exhibited a corresponding decreaserising slopes.
In addition, they also showed sig-nificant increases in steepest rise measures whencompared with CRE's.7 DiscussionThe acoustic-prosodic measures we have exam-ined indicate substantial differences not only be-tween original inputs and repeat corrections,but also between the two correction classes,those in response to rejections and those in re-sponse to misrecognitions.
Let us consider therelation of these results to those of related work739and produce a more clear overall picture of spo-ken correction behavior in human-computer di-alogue.7.1 Durat ion  and Pause:Conversat iona l  to Clear SpeechDurational measures, particularly increases induration, appear as a common phenomenonamong several analyses of speaking style\[ (Oviatt et al, 1996), (Ostendorf et al,1996), (Shriberg et al, 1997)\].
Similarly, in-creases in number and duration of silence re-gions are associated with disfluencies (Shriberget al, 1997), self-repairs (Nakatani andHirschberg, 1994), and more careful speech(Ostendorf et al, 1996) as well as with spo-ken corrections (Oviatt et al, 1996).
Thesechanges in our correction data fit smoothly intoan analysis of error corrections as invoking shiftsfrom conversational to more "clear" or "careful"speaking styles.
Thus, we observe a parallel be-tween the changes in duration and pause fromoriginal to repeat correction, described as con-versational to clear in (Oviatt et al, 1996),and from casual conversation to carefully readspeech in (Ostendorf et al, 1996).7.2 P i tchPitch, on the other hand, does not fit smoothlyinto this picture of corrections taking on clearspeech characteristics similar to those found incarefully read speech.
First of all.
(Ostendorfet al, 1996) did not find any pitch measuresto be useful in distinguishing speaking modeon the continuum from a rapid conversationalstyle to a carefully read style.
Second, pitchfeatures eem to play little role in corrections ofrejections.
Only a small decrease in pitch min-imum was found, and this difference can easilybe explained by the combination of two simpletrends.
First, there was a decrease in the num-ber of final rising contours, and second, therewere increases in utterance length, that, evenunder constant rates of declination, will yieldlower pitch minima.
Third, this feature pro-duces a divergence in behavior of CME's fromCRE's.While CRE's exhibited only the change inpitch minimum described above, corrections ofmisrecognition errors displayed some dramaticchanges in pitch behavior.
Since we observedthat simple measures of pitch maximum, min-imum, and range failed to capture even thebasic contrast of rising versus falling contour,we extended our feature set with measures ofslope of rise and slope of fall.
These mea-sures may be viewed both as an attempt tocreate a simplified form of Taylor's rise-fall-continuation model (Taylor, 1995) and as anattempt to provide quantitative measures ofpitch accent.
Measures of pitch accent and con-tour had shown some utility in identifying cer-tain discourse relations \[ (Pierrehumbert andHirschberg, 1990), (Hirschberg and Litman,1993).
Although changes in pitch maxima andminima were not significant in themselves, theincreases in rise slopes for CME's in contrast oflattening of rise slopes in CRE's combined toform a highly significant measure.
While notdefining a specific overall contour as in (Tay-lor, 1995), this trend clearly indicates increasedpitch accentuation.
Future work will seek to de-scribe not only the magnitude, but also the formof these pitch accents and their relation to thoseoutlined in (Pierrehumbert and Hirschberg,1990).7.3 SummaryIt is clear that many of the adaptations asso-ciated with error corrections can be attributedto a general shift from conversational to clearspeech articulation.
However, while this modelmay adequately describe corrections of rejectionerrors, corrections of misrecognition errors ob-viously incorporate additional pitch accent fea-tures to indicate their discourse function.
Thesecontrasts will be shown to ease the identificationof these utterances as corrections and to high-light their contrastive intent.8 Dec is ion  Tree  Exper imentsThe next step was to develop predictive classi-tiers of original vs repeat corrections and CME'svs CRE's informed by the descriptive analysisabove.
We chose to implement these classifierswith decision trees (using Quinlan's {Quinlan,1992) C4.5) trained on a subset of the original-repeat pair data.
Decision trees have two fea-tures which make them desirable for this task.First, since they can ignore irrelevant attributes,they will not be misled by meaningless noise inone or more of the 38 duration, pause, pitch,and amplitude features coded.
Since these fea-tures are probably not all important, it is desir-740able to use a technique which can identify thosewhich are most relevant.
Second, decision treesare highly intelligible; simple inspection of treescan identify which rules use which attributesto arrive at a classification, unlike more opaquemachine learning techniques such as neural nets.8.1 Dec is ion  Trees: Results &:DiscussionThe first set of decision tree trials attemptedto classify original and repeat correction utter-ances, for both correction types.
We used a setof 38 attributes: 18 based on duration and pausemeasures, 6 on amplitude, five on pitch heightand range, and 13 on pitch contour.
Trials weremade with each of the possible subsets of thesefour feature classes on over 600 instances withseven-way cross-validation.
The best results,33% error, were obtained using attributes fromall sets.
Duration measures were most impor-tant, providing an improvement of at least 10%in accuracy over all trees without duration fea-tures.The next set of trials dealt with the two er-ror correction classes separately.
One focussedon distinguishing CME's from CRE's, whilethe other concentrated on differentiating CME'salone from original inputs.
The test attributesand trial structure were the same as above.
Thebest error rate for the CME vs. CRE classi-fier was 30.7%, again achieved with attributesfrom all classes, but depending most heavily ondurational features.
Finally the most success-ful decision trees were those separating originalinputs from CME's.
These trees obtained anaccuracy rate of 75% (25% error) using simi-lar attributes to the previous trials.
The mostimportant splits were based on pitch slope anddurational features.
An exemplar of this typeof decision tree in shown below.normdurat ion l  > 0 .2335 : r (39.0/4.9)normdurat ion l  <= 0.2335 :normdurat ion2  <= 20.471 :normdurat ion3  <= 1.0116 :normdurat ion l  > -0 .0023 : o (51/3)Inormdurat ion l  <= -0 .0023 :I p i tchs lope  > 0.265 : o (19/4))I p i tchs lope  <= 0.265 :II p i tch las tmin  <= 25 .2214: r (11 /2)II p i tch las tmin  > 25.2214:III m ins lope  <= -0 .221: r (18 /5)IIII m ins lope  > -0 .221:o(15 /5)normdurat ion3  > 1.0116 :Inormdurat ion4  > 0 .0615 : r (7.0/1.3)Inormdurat ion4  <= 0 .0615 :l lnormdurat ion3  <= 1.0277 : r (8.0/3.5)l lnormdurat ion3  > 1.0277 : o (19.0/8.0)normdurat ion2  > 20.471 :I p i tchs lope  <= 0.281 : r (24.0/3.7)I p i tchs lope  > 0.281 : o (7.0/2.4)These decision tree results in conjunctionwith the earlier descriptive analysis provide ev-idence of strong contrasts between original in-puts and repeat corrections, as well as betweenthe two classes of corrections.
They suggest thatdifferent error rates after correct and after erro-neous recognitions are due to a change in speak-ing style that we have begun to model.In addition, the results on corrections of mis-recognition errors are particularly encouraging.In current systems, all recognition results aretreated as new input unless a rejection occurs.User corrections of system misrecognitions cancurrently only be identified by complex reason-ing requiring an accurate transcription.
In con-trast, the method described here provides a wayto use acoustic features uch as duration, pause,and pitch variability to identify these particu-larly challenging error corrections without strictdependence on a perfect textual transcriptionof the input and with relatively little computa-tional effort.9 Conc lus ions  &: Future  WorkUsing acoustic-prosodic features uch as dura-tion, pause, and pitch variability to identify er-ror corrections in spoken dialog systems howspromise for resolving this knotty problem.
Wefurther plan to explore the use of more accu-rate characterization of the contrasts betweenoriginal and correction inputs to adapt standardrecognition procedures to improve recognitionaccuracy in error correction interactions.
Help-ing to identify and successfully recognize spokencorrections will improve the ease of recoveringfrom human-computer miscommunication a dwill lower this hurdle to widespread acceptanceof spoken language systems.741ReferencesJ.
Bear, J. Dowding, and E. Shriberg.
1992.
In-tegrating multiple knowledge sources for de-tection and correction of repairs in human-computer dialog.
In Proceedings of the A CL,pages 56-63, University of Delaware, Newark,DE.D.
Colton.
1995.
Course manual for CSE 553speech recognition laboratory.
Technical Re-port CSLU-007-95, Center for Spoken Lan-guage Understanding, Oregon Graduate In-stitute, July.P.A.
Heeman and J. Allen.
1994.
Detecting andcorrecting speech repairs.
In Proceedings ofthe A CL, pages 295-302, New Mexico StateUniversity, Las Cruces, NM.Julia Hirschberg and Diane Litman.
1993.Empirical studies on the disambiguationof cue phrases.
Computational linguistics,19(3):501-530.C.H.
Nakatani and J. Hirschberg.
1994.
Acorpus-based study of repair cues in sponta-neous peech.
Journal of the Acoustic Societyof America, 95(3):1603-1616.M.
Ostendorf, B. Byrne, M. Bacchiani,M.
Finke, A. Gunawardana, K. Ross,S.
Rowels, E. Shribergand D. Talkin,A.
"vVaibel, B. Wheatley, and T. Zeppenfeld.1996.
Modeling systematic variations in pro-nunciation via a language-dependent hiddenspeaking mode.
In Proceedings of the In-ternational Conference on Spoken LanguageProcessing.
supplementary paper.S.L.
Oviatt, G. Levow, M. MacEarchern, andK.
Kuhn.
1996.
Modeling hyperarticulatespeech during human-computer er or resolu-tion.
In Proceedings of the International Con-ference on Spoken Language Processing, vol-ume 2, pages 801-804.Janet Pierrehumbert and Julia Hirschberg.1990.
The meaning of intonational contoursin the interpretation f discourse.
In P. Co-hen, J. Morgan, and M. Pollack, editors, In-tentions in Communication, pages 271-312.MIT Press, Cambridge, MA.J.R.
Quinlan.
1992.
C4.5: Programs for Ma-chine Learning.
Morgan Kaufmann.B.
G. Secrest and G. R. Doddington.
1993.
Anintegrated pitch tracking algorithm for speechsystems.
In ICASSP 1993.E.
Shriberg, R. Bates, and A. Stolcke.
1997.A prosody-only decision-tree model for dis-fluency detection.
In Eurospeech '97.M.
Swerts and M. Ostendorf.
1995.
Discourseprosody in human-machine interactions.
InProceedings of the ECSA Tutorial and Re-search Workshop on Spoken Dialog Systems- Theories and Applications.Paul Taylor.
1995.
The rise/fall/continuationmodel of intonation.
Speech Communication,15:169-186.N.
Yankelovich, G. Levow, and M. Marx.
1995.Designing SpeechActs: Issues in speech userinterfaces.
In CHI '95 Conference on HumanFactors in Computing Systems, Denver, CO,May.742
