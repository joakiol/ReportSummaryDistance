Proceedings of the Workshop on Multiword Expressions: from Parsing and Generation to the Real World (MWE 2011), pages 65?73,Portland, Oregon, USA, 23 June 2011. c?2011 Association for Computational LinguisticsA machine learning approach to relational noun mining in GermanBerthold CrysmannArbeitsbereich Sprache und KommunikationUniversita?t Bonncrysmann@uni-bonn.deAbstractIn this paper I argue in favour of a col-location extraction approach to the acquisi-tion of relational nouns in German.
We an-notated frequency-based best lists of noun-preposition bigrams and subsequently traineddifferent classifiers using (combinations of)association metrics, achieving a maximum F-measure of 69.7 on a support vector machine(Platt, 1998).
Trading precision for recall, wecould achieve over 90% recall for relationalnoun extraction, while still halving the anno-tation effort.1 Mining relational nouns: almost a MWEextraction problemA substantial minority of German nouns are char-acterised by having an internal argument structurethat can be expressed as syntactic complements.
Anon-negligeable number of relational nouns are de-verbal, inheriting the semantic argument structure ofthe verbs they derive from.
In contrast to verbs, how-ever, complements of nouns are almost exclusivelyoptional.The identification of relational nouns is of greatimportance for a variety of content-oriented applica-tions: first, precise HPSG parsing for German can-not really be achieved, if a high number of nouncomplements is systematically analysed as modi-fiers.
Second, recent extension of Semantic Role La-beling to the argument structure of nouns (Meyerset al, 2004) increases the interest in lexicographicmethods for the extraction of noun subcategorisa-tion information.
Third, relational nouns are alsoa valuable resource for machine translation, sepa-rating the more semantic task of translating modi-fying prepositions from the more syntactic task oftranslating subcategorised for prepositions.
Despiteits relevance for accurate deep parsing, the GermanHPSG grammar developed at DFKI (Mu?ller andKasper, 2000; Crysmann, 2003; Crysmann, 2005)currently only includes 107 entries for propositiontaking nouns, and lacks entries for PP-taking nounsentirely.In terms of subcategorisation properties, rela-tional nouns in German can be divided up into 3classes:?
nouns taking genitival complements (e.g., Be-ginn der Vorlesung ?beginning of the lecture?,Zersto?rung der Stadt ?destruction of the city?
)?
nouns taking propositional complements, ei-ther a complementiser-introduced finite clause(der Glaube, da?
die Erde flach ist ?the beliefthat earth is flat?
), or an infinitival clause (dieHoffnung, im Lotto zu gewinnen ?the hope towin the lottery?
), or both?
nouns taking PP complementsIn this paper, I will be concerned with nouns tak-ing prepositional complements, although the methoddescribed here can also be easily applied to the caseof complementiser-introduced propositional com-plements.11In fact, I expect the task of mining relational nouns tak-ing finite propositional complements to be far easier, owing toa reduced ambiguity of the still relatively local complementiser65The prepositions used with relational nouns allcome from a small set of basic prepositions, mostlylocative or directional.A characteristic of these prepositions when usedas a noun?s complement, is that their choice becomesrelatively fixed, a property shared with MWEs ingeneral.
Furthermore, choice of preposition is of-ten arbitrary, sometimes differing between relationalnouns and the verbs they derive from, e.g., Interessean ?lit: interest at?
vs. interessieren fu?r ?lit: to inter-est for?.
Owing to the lack of alternation, the prepo-sition by itself does not compositionally contributeto sentence meaning, its only function being the en-coding of a thematic property of the noun.
Thus, insyntacto-semantic terms, we are again dealing withprototypical MWEs.The fact that PP complements of nouns, like mod-ifiers, are syntactically optional, together with thefact that their surface form is indistinguishable fromadjunct PPs, makes the extraction task far from triv-ial.
It is clear that grammar-based error mining tech-niques (van Noord, 2004; Cholakov et al, 2008)that have been highly successful in other areas ofdeep lexical acquisition (e.g., verb subcategorisa-tion) cannot be applied here: first, given that an al-ternative analysis as a modifier is readily availablein the grammar, missing entries for relational nounswill never incur any coverage problems.
Further-more, since PP modifiers are highly common wecannot expect a decrease in tree probability either.Instead, I shall exploit the MWE-like properties ofrelational nouns, building on the expectation that thepresence of a subcategorisation requirement towardsa fixed, albeit optional, prepositional head shouldleave a trace in frequency distributions.
Thus, build-ing on previous work in MWE extraction, I shallpursue a data-driven approach that builds on a va-riety of association metrics combined in a proba-bilistic classifier.
Despite the difference of the task,da?.
Although complement that-clauses in German can indeedcan be extraposed, corpus studies on relative clause extraposi-tion (Uszkoreit et al, 1998) have shown that the great majorityof extrapositions operates at extremely short surface distance,typically crossing the verb or verb particle in the right sentencebracket.
Since locality conditions on complement clause extra-position are more strict than those for relative clause extrapo-sition (Kiss, 2005; Crysmann, to appear), I conjecture that theactual amount of non-locality found in corpora will be equallylimited.the approach suggested here shares some significantsimilarity to previous classifier-based approaches toMWE (Pecina, 2008).2 Data2.1 Data preparationAs primary data for relational noun extraction, Iused the deWaC corpus (Baroni and Kilgariff, 2006),a 1.6 billion token corpus of German crawled fromthe web.
The corpus is automatically tagged andlemmatised by TreeTagger (Schmid, 1995).
Fromthis corpus, I extracted all noun (NN) and prepo-sition (APPR) unigrams and noun?preposition bi-grams.
Noun unigrams occuring less than ten timesin the entire corpus were subsequently removed.
Inaddition to the removal of hapaxes, I also filtered outany abbreviations.Frequency counts were lemma-based, a deci-sion that was motivated by the intended applica-tion, namely mining of relational noun entries fora lemma-based HPSG lexicon.From the corpus, I extracted a best-list, basedon bigram frequency, a well-established heuristicalmeasure for collocational status (Krenn, 2000).
Us-ing a frequency based best list not only minimisesinitial annotation effort, but also ensures the quick-est improvement of the target resource, the gram-mar?s lexicon.
Finally, the use of ranked best listswill also ensure that we will always have enoughpositive items in our training data.2.2 AnnotationThe ranked best list was subsequently annotated bytwo human annotators (A1,A2) with relatively littleprior training in linguistics.
In order to control forannotation errors, the same list was annotated a sec-ond time by a third year student of linguistics (A3).In order to operationalise the argument/modifierannotators were asked to take related verbs intoconsideration, as well as to test (local and tempo-ral) prepositions for paradigmatic interchangeabil-ity.
Furthermore, since we are concerned with logi-cal complements of nouns but not possessors, whichcan be added quite freely, annotators were advised tofurther distinguish whether a von-PP was only pos-sible as a possessor or also as a noun complement.An initial comparison of annotation decisions66showed an agreement of .82 between A1 and A3,and an agreement of .84 between A2 and A3.
Ina second round discrepancies between annotatorswere resolved, yielding a gold standard annotationof 4333 items, out of which 1179 (=27.2%) wereclassified as relational nouns.3 ExperimentsAll experiments reported here were carried out us-ing WEKA, a Java platform for data explorationand experimentation developed at the University ofWaikato (Hall et al, 2009).Since our task is to extract relational nouns andsince we are dealing with a binary decision, per-formance measures given here report on relationalnouns only.
Thus, we do not provide figures for theclassification of non-relational nouns or any uninfor-mative (weighted) averages of the two.23.1 LearnersIn a pre-study, we conducted experiments with a sin-gle feature set, but different classifiers in order to de-termine which ones performed best on our data set.Amongst the classifiers we tested were 2 Bayesianclassifiers (Naive Bayes and Bayesian Nets), a Sup-port Vector Machine, a Multilayer Perceptron clas-sifier, as well as the entire set of decision tree clas-sifiers offered by WEKA 3.6.4 (cf.
the WEKA doc-umentation for an exhaustive list of references).
Alltest runs were performed with default settings.
Un-less otherwise indicated, all tests were carried outusing 10-fold cross-validation.Among these, decision tree classifiers performquite well in general, with NBTree, a hybrid de-cision tree classifier using Naive Bayes classifiersat leave nodes producing optimal results.
Perfor-mance of the Naive Bayes classifier was subopti-mal, with respect to both precision and recall.
Over-all performance of the Bayesian Net classifier (witha K2 learner) was competitive to average decisiontree classifiers, delivering particularly good recall,but fell short of the best classifiers in terms of preci-sion and F-measure.2A base-line classifier that consistently choses the majorityclass (non-relational) and therefore does not detect a single re-lational noun, already achieves an F-measure for non-relationalnouns of 84.3, and a weighted F-measure of 61.3%.Thus, for further experimentation, we concen-trated on the two best-performing classifiers, i.e.,NBTree (Kohavi, 1996), which achieved the high-est F-score and the second best precision, and SMO(Platt, 1998), a support vector machine, which pro-duced the best precision value.After experimentation regarding optimal featureselection (see next section), we re-ran our experi-ments with the modified feature set, in order to con-firm that the classifiers we chose were still optimal.The results of these runs are presented in table 1.Prec.
Rec.
F-meas.ADTree 68.3 61.1 64.5BFTree 75.0 51.7 61.2DecisionStump 52.5 80.2 63.5FT 73.8 59.1 65.7J48 72.9 58.4 64.8J48graft 72.6 58.4 64.7LADTree 70.5 57.5 63.3LMT 74.9 59.8 66.5NBTree 74.9 62.8 68.7RandomForest 67.4 63.4 65.3RandomTree 61.8 61.1 61.4REPTree 74.5 61.2 67.2Naive Bayes 70.5 53.9 61.1Bayes Net 60.6 71.4 65.6SMO 76.5 57.7 65.8MultilayerPerceptron 67.5 64.5 65.9Bagging (RepTree) 75.9 62.4 68.5Voting (maj) 72.7 66.3 69.4Voting (av) 71.3 68.4 69.8Table 1: Performance of different classifiersFinally, we did some sporadic test using a vot-ing scheme incorporating 3 classifiers with high pre-cision values (SMO, NBTree, Bagging(REPTree)(Breiman, 1996)), as well as two classifiers withhigh recall (BayesNet, recall-oriented SMO, see be-low).
Using averaging, we managed to bring the F-measure up to 69.8, the highest value we measuredin all our experiments.3.2 FeaturesFor NBTree, our best-performing classifier, we sub-sequently carried out a number of experiments to as-sess the influence and predictive power of individualassociation measures and to study their interactions.67Essentially, we make use of two basic types offeatures: string features, like the form of the preposi-tion or the prefixes and suffixes of the noun, and as-sociation measures.
As for the latter, we drew on theset of measures successfully used in previous studieson collocation extraction:Mutual information (MI) An information theo-retic measure proposed by (Church and Hanks,1990) which measures the joint probability ofthe bigram in relation to the product of themarginal probabilities, i.e., the expected proba-bility.MI =p(noun, prep)p(noun) ?
p(prep)MI2 A squared variant of mutal information, previ-ously suggested by (Daille, 1994).
Essentially,the idea behind squaring the joint probability isto counter the negative effect of extremely lowmarginal probabilities yielding high MI scores.MI2 =(p(noun, prep))2p(noun) ?
p(prep)Likelihood ratios A measure suggested by (Dun-ning, 1993) that indicates how much morelikely the cooccurence is than mere coinci-dence.LR = logL(pi, k1, n1) + logL(p2, k2, n2)?
logL(p, k1, n1) ?
logL(p, k2, n2)wherelogL(p, n, k) = k log p+ (n?
k) log(1 ?
p)andp1 =k1n1, p2 =k2n2, p =k1 + k2n1 + n2t-score The score of Fisher?s t-test.
Although theunderlying assumption regarding normal distri-bution is incorrect (Church and Mercer, 1993),the score has nevertheless been used with re-peated success in collocation extraction tasks(Krenn, 2000; Krenn and Evert, 2001; Evertand Krenn, 2001).tscore =p(noun, prep) ?
(p(noun) ?
p(prep))?
?2NAs suggested by (Manning and Schu?tze, 1999)we use p as an approximation of ?2.Association Strength (Smadja, 1993)A factor indicating how many times the stan-dard deviation a bigram frequency differs fromthe average.Strength =freqi ?
f?
?Best Indicates whether a bigram is the most fre-quent one for the given noun or not.Best-Ratio A relative version of the previous fea-ture indicating the frequency ratio between thecurrent noun?preposition bigram and the bestbigram for the given noun.In addition to the for,m of the preposition, we in-cluded information about the noun?s suffixes or pre-fixes:Noun suffix We included common string suffixesthat may be clues as to the relational nature ofthe noun, as, e.g., the common derviational suf-fixes -ion, -schaft, -heit, -keit as well as the end-ings -en, which are found inter alia with nom-inalised infinitives, and -er, which are found,inter alia with agentive nominals.
All other suf-fixes were mapped to the NONE class.Noun prefix Included were prefixes that commonlyappear as verb prefixes.
Again, this was usedas a shortcut for true lexical relatedness.As illustrated by the diagrams in Figure 1, theaforementioned association measures align differ-ently with the class of relational nouns (in black):The visually discernible difference in alignmentbetween association metrics and relational nounswas also confirmed by testing single-feature classi-fiers: as detailed in Table 2, MI, MI2, and t-scoreall capable to successfully identify relational nounsby themselves, whereas best, best-ratio and strength68Figure 1: Distribution of relational and non-relational nouns across features (created with WEKA 3.6.4)are entirely unable to partition the data appropri-ately.
LR assumes an intermediate position, suffer-ing mainly from recall problems.Prec.
Rec.
F-meas.MI 65.2 45.2 53.4MI2 62.2 50.7 55.9LR 60 23.5 33.8T-score 66.4 42 51.5Strength 0 0 0Best 0 0 0Best-Ratio 0 0 0Table 2: Classification by a single association metricThe second experiment regarding features differsfrom the first by the addition of form features:Two things are worth noting here: first, the valuesachieved by MI and T-score now come very close tothe values obtained with much more elaborate fea-ture sets, confirming previous results on the useful-ness of these metrics.
Second, all association mea-sures now display reasonable performance.
BothPrec.
Rec.
F-meas.MI 74.2 61.2 67.1MI2 72.5 56.4 63.5LR 73.1 54.4 62.4T-score 74.9 60.6 67Strength 72.5 52.4 60.9Best 69.7 48.7 57.3Best-Ratio 72.1 53.4 61.3Table 3: Classification by a single association metric +form features (preposition, noun prefix, noun suffix)these effects can be traced to a by-category samplingintroduced by the form features.
The most clear-cutcase is probably the best feature: as shown in Fig-ure 1, there is a clear increase in relational nouns inthe TRUE category of the Boolean best feature, yet,they still do not represent a majority.
Thus, a clas-sifier with a balanced cost function will always pre-fer the majority vote.
However, for particular nounclasses (and prepositions for that matter) majoritiescan be tipped.69Figure 2: MI-values of relational nouns relative to prepositionAs depicted by the preposition-specific plot of MIvalues in Figure 2, some prepositions have a clearbias for their use with relational nouns (e.g., von?of?)
or against it (e.g., ab ?from?
), while others ap-pear non-commital (e.g., fu?r ?for?).
Similar observa-tions can be made for noun suffixes and prefixes.The next set of experiments were targetted at op-timisation.
Assuming that the candidate sets se-lected by different metrics will not stand in a sub-set relation I explored which combination of met-rics yielded the best results.
To do this, I startedout with a full set of features and compared this tothe results obtained with one feature left out.
In asecond and third step of iteration, I tested whethersimultaneously leaving out some features for whichwe observed some gain would produce an even moreoptimised classifier.Table 4 presents the result of the first step.
Here,two outcomes are of particular interest: deletinginformation about the noun suffix is detrimental,Prec.
Rec.
F-meas.All 74.4 61.2 67.2?T-score 75.3 62.4 68.3?MI 72.8 62.3 67.1?MI2 75.1 61.6 67.7?LR 74.1 60.1 66.3?Strength 73.4 62 67.2?Best 73.7 60.7 66.6?Best-Ratio 74.2 61.8 67.4?Prep 74.7 61.1 67.2?Noun-Prefix 74.7 61.1 67.2?Noun-Suffix 71.3 55.3 62.3Table 4: Effects of leaving one feature outwhereas ignoring the t-score value appears to bebeneficial to overall performance.In a second (and third) iteration, I tested whetherany additional feature deletion apart from t-scorewould give rise to any further improvements.70?t-score Prec.
Rec.
F-meas.75.3 62.4 68.3?MI 74.4 57.6 64.9?LR 74.8 61.3 67.4?MI2 74.1 61.7 67.4?Strength 75.1 62.8 68.4?Best 74.1 61.5 67.2?Best-Ratio 75.4 62.6 68.4?Best-Ratio ?Strength 74.9 63.4 68.7Table 5: Effects of leaving two or more features outIn fact, removal of the Strength feature providedgood results, whether taken out individually or incombination, which may be due to this feature?s in-herently poor statistical properties (cf.
Figure 1).
Ig-noring best-ratio was also beneficial, probably dueto the fact that most of its benefical properties are al-ready covered by the best feature and that non-bestnoun-preposition combinations hardly ever give riseto positive hits.As a matter of fact, simultaneous removal of best-ratio and strength, in addition to the removal of t-score of course, yielded best overall results.
As aconsequence, all remaining test runs were based onthis feature set.
In separate test runs with the SMOclassifier, I finally confirmed that the optimality ofthis feature set was not just an artifact of the classi-fier, but that it generalises to SVMs as well.3.3 Trade-offsSince our main aim in relational noun mining isthe improvement of the accuracy of our grammar?slexicon, and since the quickest improvement areexpected for highly frequent noun-preposition bi-grams, I tested whether I could bring the recall of ourclassifiers up, at the expense of moderate losses inprecision.
For this evaluation, I used again our best-performing classifier (NBTree), as well as SMO,which had the highest head-room in terms of preci-sion, while already providing satisfactory recall.
Tothis end, I manipulated the classifier?s cost matrixduring training and testing, gradually increasing thecosts for false negatives compared to false positives.The results of this evaluation are given in Figure3.
First, we obtained a new optimal f-measure forthe SMO classifier: at a cost factor of 2.1 for falsenegatives, the f-measure peaks at 69.7, with a recallof 75.1% and precision still acceptable (65.1%).
Atthis level, we still save more than two thirds of theannotation effort.By way of penalising false negatives 6 times morethan false positives, the suppport vector machinewas able to detect over 90% of all relational nouns,at a precision of 50%.
At these levels, we can stillsave more than half of the entire annotation effort.Going further down the Zipf distribution, we ex-pect the savings in terms of annotation effort to gofurther up, since our bigram frequency ranking en-sures that relational nouns are overrepresented at thetop of the list, a rate that will gradually go down.Finally, including false positives in the data tobe annotated will also ensure that we always haveenough positive and negative training data for learn-ing a classifier on an extended data set.3.4 OutlookAlthough results are already useful at this point, Ihope to further improve precision and recall ratesby means of additional features.
Evaluating theNBTree classifier on the training data, we observean F-measure of only 74.7%, which suggests thatthe current set of features models the training datastill quite imperfectly.
Thus, one needs to incorpo-rate further independent evidence in order to predictrelation nouns more reliably.
Owing to the seman-tic nature of the relational vs. non-relational dis-tinction one type of additional evidence could comefrom multilingual resources: as a first step, I en-visage incorporating the classification of nouns inthe English Resource Grammar (ERG; (Copestakeand Flickinger, 2000)) as prior information regard-ing relational status.
In a second step I shall explorewhether one can exploit information from parallelcorpora, using in particular item-specific divergenceof preposition choice to detect whether we are deal-ing with a contentful or rather a functional prepo-sition.3 The intuition behind using cross-linguisticevidence to try and boost the performance of thelearner is based on the observation that predicate ar-gument structure in closely related languages suchas English and German tends to be highly similar,with differences mostly located in syntactic proper-3I expect that arbitrary divergence in the choice of preposi-tion provides an indicator of grammaticalisation.710 2 4 6 8 10 12 14 16405060708090100PrecisionRecallF-measurePrecisionRecallF-measureFigure 3: Effect of trading precision for recall (NBTree: white; SMO: black)ties such as selection for case or choice of preposi-tion.
As a consequence, I do not expect to be able topredict the actual form of the German preposition,but rather gain additional evidence as to whether agiven noun has some relational use at all or not.The second type of information that I plan to usemore systematically in the future is morphologicaland lexical relatedness which is only approximatedat present by the noun sufix and noun prefix fea-tures which hint at the derived (deverbal) natureof the noun under discussion.
In addition to thesebrute-force features, I plan to incorporate the HPSGgrammar?s verb subcategorisation lexicon, pairingnouns and verbs by means of minimum edit dis-tance.4 In essence, we hope to provide a moregeneral approach to lexical relatedness between re-lational nouns and the non-unary verbal predicatesthey derive from: in the current feature set, this wasonly suboptimally approximated by the use of nounsuffix and prefix features, resulting in most nounsbeing mapped to the unpredictive class NONE.5Finally, I plan to apply the current approach tothe extraction of nouns taking propositional comple-ments.
Given the comparative ease of that task com-pared to the extraction of PP-taking nouns, I shall in-vestigate whether we can exploit the fact that many4Being aware of the fact that lexical derivation may give riseto arbitrary changes in syntactic subcategorisation, I minimallyexpect to gather evidence regarding the arity of the derived nounpredicate.
To what extent actual selectional properties as to theshape of the functional preposition are maintained by deriva-tional processes remains a matter of empirical research.5The inclusion of noun prefixes, which are actually verb pre-fixes, is inherently limited to mimick lexical relatedness to pre-fix verbs.relational nouns taking propositional complements(e.g., der Glaube, da?
... ?the belief that?)
also takePP-complements (der Glaube an ?the belief in?)
inorder to further improve our present classifier.
In asimilar vein, I shall experiment whether it is possibleto extrapolate from relational nouns taking von-PPsto genitive complements.4 ConclusionIn this paper I have suggested to treat the task ofmining relational nouns in German as a MWE ex-traction problem.
Based on the first 4333 hand-annotated items of a best-list ranked by bigram fre-quencies, several classifiers have been trained in or-der to determine which learner and which (combina-tion of) association measures performed best for thetask.Testing different classifiers and different metrics,we found that optimal results were obtained us-ing a support vector machine (Platt, 1998), includ-ing Mutual Information (MI), its squared variant(MI2), and Likelihood Ratios (LR) as associationmeasures, together with information about the iden-tity of the preposition and the noun?s prefix and suf-fix.
The second best classifier, a hybrid decision treewith Naive Bayes classifiers at the leaves producedhighly competitive results.
T-scores, while being agood predictor on its own, however, led to a slightdecrease in performance, when a full feature set wasused.
Likewise, performance suffered when Associ-ation Strength (Smadja, 1993) was included.
Overallperformance of the best individual classifier figuredat an F-score of 69.7.72ReferencesMarco Baroni and Adam Kilgariff.
2006.
Largelinguistically-processed web corpora for multiple lan-guages.
In Proceedings of EACL 2006.Leo Breiman.
1996.
Bagging predictors.
MachineLearning, 24(2):123?140.Kostadin Cholakov, Valia Kordoni, and Yi Zhang.
2008.Towards domain-independent deep linguistic process-ing: Ensuring portability and re-usability of lexicalisedgrammars.
In Coling 2008: Proceedings of the work-shop on Grammar Engineering Across Frameworks,pages 57?64, Manchester, England, August.
Coling2008 Organizing Committee.Kenneth Church and Patrick Hanks.
1990.
Word asso-ciation norms, mutual information, and lexicography.Computational Linguistics, 16(1):22?29.Kenneth Church and Robert Mercer.
1993.
Introductionto the special issue on computational linguistics usinglarge corpora.
Computational Linguistics, 19:1?24.Ann Copestake and Dan Flickinger.
2000.
An open-source grammar development environment and broad-coverage English grammar using HPSG.
In Proceed-ings of the Second conference on Language Resourcesand Evaluation (LREC-2000), Athens.Berthold Crysmann.
2003.
On the efficient implemen-tation of German verb placement in HPSG.
In Pro-ceedings of RANLP 2003, pages 112?116, Borovets,Bulgaria.Berthold Crysmann.
2005.
Relative clause extrapositionin German: An efficient and portable implementation.Research on Language and Computation, 3(1):61?82.Berthold Crysmann.
to appear.
On the locality of com-plement clause and relative clause extraposition.
InGert Webelhuth, Manfred Sailer, and Heike Walker,editors, Rightward Movement in a Comparative Per-spective.
John Benjamins, Amsterdam.Be?atrice Daille.
1994.
Approche mixte pour l?extractionautomatique de terminologie : statistique lexicale etfiltres linguistiques.
Ph.D. thesis, Universite?
Paris 7.Ted Dunning.
1993.
Accurate methods for the statisticsof surprise and coincidence.
Computational Linguis-tics, 19:61?74.Stefan Evert and Brigitte Krenn.
2001.
Methods for thequalitative evaluation of lexical association measures.In Proceedings of the 39th Annual Meeting of theAssociation for Computational Linguistics, Toulouse,France, pages 188?195.Mark Hall, Eibe Frank, Geoffrey Holmes, BernhardPfahringer, Peter Reutemann, and Ian H. Witten.2009.
The WEKA data mining software: an update.SIGKDD Explorations, 11(1):10?18.Tibor Kiss.
2005.
Semantic constraints on relative clauseextraposition.
Natural Language and Linguistic The-ory, 23:281?334.Ron Kohavi.
1996.
Scaling up the accuracy of naive-bayes classifiers: A decision-tree hybrid.
In Sec-ond International Conference on Knowledge Discov-ery and Data Mining, pages 202?207.Brigitte Krenn and Stefan Evert.
2001.
Can we do betterthan frequency?
a case study on extracting PP-verbcollocations.
In Proceedings of the ACL Workshop onCollocations, Toulouse, France, pages 39?46.Brigitte Krenn.
2000.
The Usual Suspects: Data-oriented Models for the Identification and Representa-tion of Lexical Collocations.
Ph.D. thesis, Universita?tdes Saarlandes.Christopher Manning and Hinrich Schu?tze.
1999.
Foun-dations of Statistical Natural Language Processing.MIT Press.A.
Meyers, R. Reeves, C. Macleod, R. Szekely, V. Zielin-ska, B.
Young, and R. Grishman.
2004.
The nombankproject: An interim report.
In A. Meyers, editor, HLT-NAACL 2004 Workshop: Frontiers in Corpus Annota-tion, pages 24?31, Boston, Massachusetts, USA, May2 - May 7.
Association for Computational Linguistics.Stefan Mu?ller and Walter Kasper.
2000.
HPSG analy-sis of German.
In Wolfgang Wahlster, editor, Verb-mobil: Foundations of Speech-to-Speech Translation,pages 238?253.
Springer, Berlin.Pavel Pecina.
2008.
A machine learning approach tomultiword expression extraction.
In Proceedings ofthe LREC Workshop Towards a Shared Task for Multi-word Expressions (MWE 2008), pages 54?61.J.
Platt.
1998.
Fast training of support vector ma-chines using sequential minimal optimization.
InB.
Schoelkopf, C. Burges, and A. Smola, editors, Ad-vances in Kernel Methods - Support Vector Learning.MIT Press.Helmut Schmid.
1995.
Improvements in part-of-speechtagging with an application to German.
In Proceed-ings of the ACL SIGDAT-Workshop, March.Frank Smadja.
1993.
Retrieving collocations from text:Xtract.
Computational Linguistics, 19(1):143?177.Hans Uszkoreit, Thorsten Brants, Denys Duchier,Brigitte Krenn, Lars Konieczny, Stephan Oepen, andWojciech Skut.
1998.
Studien zur performanzori-entierten Linguistik.
Aspekte der Relativsatzextrapo-sition im Deutschen.
Kognitionswissenschaft, 7:129?133.Gertjan van Noord.
2004.
Error mining for wide cover-age grammar engineering.
In Proceedings of the 42ndMeeting of the Association for Computational Linguis-tics (ACL?04), Barcelona, Spain, pages 446?453.73
