Linguistic Profiling for Author Recognition and VerificationHans van HalterenLanguage and Speech, Univ.
of NijmegenP.O.
Box 9103NL-6500 HD, Nijmegen, The Netherlandshvh@let.kun.nlAbstractA new technique is introduced, linguisticprofiling, in which large numbers ofcounts of linguistic features are used as atext profile, which can then be comparedto average profiles for groups of texts.The technique proves to be quite effectivefor authorship verification and recogni-tion.
The best parameter settings yield aFalse Accept Rate of 8.1% at a False Re-ject Rate equal to zero for the verificationtask on a test corpus of student essays,and a 99.4% 2-way recognition accuracyon the same corpus.1 IntroductionThere are several situations in language researchor language engineering where we are in need ofa specific type of extra-linguistic informationabout a text (document) and we would like todetermine this information on the basis of lin-guistic properties of the text.
Examples are thedetermination of the language variety or genre ofa text, or a classification for document routing orinformation retrieval.
For each of these applica-tions, techniques have been developed focusingon specific aspects of the text, often based onfrequency counts of functions words in linguis-tics and of content words in language engineer-ing.In the technique we are introducing in this paper,linguistic profiling, we make no a priori choicefor a specific type of word (or more complex fea-ture) to be counted.
Instead, all possible featuresare included and it is determined by the statisticsfor the texts under consideration, and the distinc-tion to be made, how much weight, if any, eachfeature is to receive.
Furthermore, the frequencycounts are not used as absolute values, but ratheras deviations from a norm, which is again deter-mined by the situation at hand.
Our hypothesis isthat this technique can bring a useful contributionto all tasks where it is necessary to distinguishone group of texts from another.
In this paper thetechnique is tested for one specific type of group,namely the group of texts written by the sameauthor.2 Tasks and Application ScenariosTraditionally, work on the attribution of a text toan author is done in one of two environments.The first is that of literary and/or historical re-search where attribution is sought for a work ofunknown origin (e.g.
Mosteller & Wallace, 1984;Holmes, 1998).
As secondary information gener-ally identifies potential authors, the task is au-thorship recognition: selection of one author froma set of known authors.
Then there is forensiclinguistics, where it needs to be determined if asuspect did or did not write a specific, probablyincriminating, text (e.g.
Broeders, 2001; Chaski,2001).
Here the task is authorship verification:confirming or denying authorship by a singleknown author.
We would like to focus on a thirdenvironment, viz.
that of the handling of largenumbers of student essays.For some university courses, students have towrite one or more essays every week and submitthem for grading.
Authorship recognition isneeded in the case the sloppy student, who for-gets to include his name in the essay.
If we couldlink such an essay to the correct student our-selves, this would prevent delays in handling theessay.
Authorship verification is needed in thecase of the fraudulous student, who has decidedthat copying is much less work than writing anessay himself, which is only easy to spot if theoriginal is also submitted by the original author.In both scenarios, the test material will be siz-able, possibly around a thousand words, and atleast several hundred.
Training material can besufficiently available as well, as long as text col-lection for each student is started early enough.Many other authorship verification scenarios donot have the luxury of such long stretches of testtext.
For now, however, we prefer to test the ba-sic viability of linguistic profiling on such longerstretches.
Afterwards, further experiments canshow how long the test texts need to be to reachan acceptable recognition/verification quality.2.1 Quality MeasuresFor recognition, quality is best expressed as thepercentage of correct choices when choosing be-tween N authors, where N generally depends onthe attribution problem at hand.
We will use thepercentage of correct choices between two au-thors, in order to be able to compare with previ-ous work.
For verification, quality is usuallyexpressed in terms of erroneous decisions.
Whenthe system is asked to verify authorship for theactual author of a text and decides that the textwas not written by that author, we speak of aFalse Reject.
The False Reject Rate (FRR) is thepercentage of cases in which this happens, thepercentage being taken from the cases whichshould be accepted.
Similarly, the False AcceptRate (FAR) is the percentage of cases wheresomebody who has not written the test text is ac-cepted as having written the text.
With increasingthreshold settings, FAR will go down, while FRRgoes up.
The behaviour of a system can be shownby one of several types of FAR/FRR curve, suchas the Receiver Operating Characteristic (ROC).Alternatively, if a single number is preferred, apopular measure is the Equal Error Rate (EER),viz.
the threshold value where FAR is equal toFRR.
However, the EER may be misleading,since it does not take into account the conse-quences of the two types of errors.
Given the ex-ample application, plagiarism detection, we donot want to reject, i.e.
accuse someone of plagia-rism, unless we are sure.
So we would like tomeasure the quality of the system with the FalseAccept Rate at the threshold at which the FalseReject Rate becomes zero.2.2 The Test CorpusBefore using linguistic profiling for any real task,we should test the technique on a benchmarkcorpus.
The first component of the Dutch Au-thorship Benchmark Corpus (ABC-NL1) appearsto be almost ideal for this purpose.
It containswidely divergent written texts produced by first-year and fourth-year students of Dutch at theUniversity of Nijmegen.
The ABC-NL1 consistsof 72 Dutch texts by 8 authors, controlled for ageand educational level of the authors, and for reg-ister, genre and topic of the texts.
It is assumedthat the authors?
language skills were advanced,but their writing styles were as yet at only weaklydeveloped and hence very similar, unlike those inliterary attribution problems.Each author was asked to write nine texts ofabout a page and a half.
In the end, it turned outthat some authors were more productive thanothers, and that the text lengths varied from 628to 1342 words.
The authors did not know that thetexts were to be used for authorship attributionstudies, but instead assumed that their writingskill was measured.
The topics for the nine textswere fixed, so that each author produced threeargumentative non-fiction texts, on the televisionprogram Big Brother, the unification of Europeand smoking, three descriptive non-fiction texts,about soccer, the (then) upcoming new millen-nium and the most recent book they read, andthree fiction texts, namely a fairy tale about LittleRed Riding Hood, a murder story at the univer-sity and a chivalry romance.The ABC-NL1 corpus is not only well-suitedbecause of its contents.
It has also been used inpreviously published studies into authorship at-tribution.
A ?traditional?
authorship attributionmethod, i.e.
using the overall relative frequenciesof the fifty most frequent function words and aPrincipal Components Analysis (PCA) on thecorrelation matrix of the corresponding 50-dimensional vectors, fails completely (Baayen etal., 2002).
The use of Linear Discriminant Analy-sis (LDA) on overall frequency vectors for the 50most frequent words achieves around 60% cor-rect attributions when choosing between two au-thors, which can be increased  to around 80%  bythe application of cross-sample entropy weight-ing (Baayen et al, 2002).
Weighted ProbabilityDistribution Voting (WPDV) modeling on thebasis of a very large number of features achieves97.8% correct attributions (van Halteren et al, ToAppear).
Although designed to produce a hardrecognition task, the latter result show that veryhigh recognition quality is feasible.
Still, this ap-pears to be a good test corpus to examine the ef-fectiveness of a new technique.3 Linguistic ProfilingIn linguistic profiling, the occurrences in a textare counted of a large number of linguistic fea-tures, either individual items or combinations ofitems.
These counts are then normalized for textlength and it is determined how much (i.e.
howmany standard deviations) they differ from themean observed in a profile reference corpus.
Forthe authorship task, the profile reference corpusconsists of the collection of all attributed andnon-attributed texts, i.e.
the entire ABC-NL1corpus.
For each text, the deviation scores arecombined into a profile vector, on which a vari-ety of distance measures can be used to positionthe text in relation to any group of other texts.3.1 FeaturesMany types of linguistic features can be profiled,such as features referring to vocabulary, lexicalpatterns, syntax, semantics, pragmatics, informa-tion content or item distribution through a text.However, we decided to restrict the current ex-periments to a few simpler types of features todemonstrate the overall techniques and method-ology for profiling before including every possi-ble type of feature.
In this paper, we first showthe results for lexical features and continue withsyntactic features, since these are the easiest onesto extract automatically for these texts.
Otherfeatures will be the subject of further research.3.2 Authorship Score CalculationIn the problem at hand, the system has to decideif an unattributed text is written by a specificauthor, on the basis of attributed texts by that andother authors.
We test our system?s ability tomake this distinction by means of a 9-fold cross-validation experiment.
In each set of runs of thesystem, the training data consists of attributedtexts for eight of the nine essay topics.
The testdata consists of the unattributed texts for theninth essay topic.
This means that for all runs, thetest data is not included in the training data and isabout a different topic than what is present in thetraining material.
During each run within a set,the system only receives information aboutwhether each training text is written by one spe-cific author.
All other texts are only marked as?not by this author?.3.3 Raw ScoreThe system first builds a profile to represent textwritten by the author in question.
This is simplythe featurewise average of the profile vectors ofall text samples marked as being written by theauthor in question.
The system then determines araw score for all text samples in the list.
Ratherthan using the normal distance measure, we optedfor a non-symmetric measure which is aweighted combination of two factors: a) the dif-ference between sample score and author scorefor each feature and b) the sample score by itself.This makes it possible to assign more importanceto features whose count deviates significantlyfrom the norm.
The following distance formula isused:?T = (?
|Ti?Ai| D  |Ti| S) 1/(D+S)In this formula, Ti and Ai are the values for the ithfeature for the text sample profile and the authorprofile respectively, and D and S are the weight-ing factors that can be used to assign more or lessimportance to the two factors described.
We willsee below how the effectiveness of the measurevaries with their setting.
The distance measure isthen transformed into a score by the formulaScoreT = (?
|Ti|(D+S)) 1/(D+S)   ?
?TIn this way, the score will grow with the similar-ity between text sample profile and author pro-file.
Also, the first component serves as acorrection factor for the length of the text sampleprofile vector.3.4 Normalization and RenormalizationThe order of magnitude of the score values varieswith the setting of D and S. Furthermore, the val-ues can fluctuate significantly with the samplecollection.
To bring the values into a range whichis suitable for subsequent calculations, we ex-press them as the number of standard deviationsthey differ from the mean of the scores of the textsamples marked as not being written by the au-thor in question.In the experiments described in this paper, arather special condition holds.
In all tests, weknow that the eight test samples are comparablein that they address the same topic, and that theauthor to be verified produced exactly one of theeight test samples.
Under these circumstances,we should expect one sample to score higher thanthe others in each run, and we can profit fromthis knowledge by performing a renormalization,viz.
to the number of standard deviations thescore differs from the mean of the scores of theunattributed samples.
However, this renormaliza-tion only makes sense in the situation that wehave a fixed set of authors who each producedone text for each topic.
This is in fact yet a dif-ferent task than those mentioned above, say au-thorship sorting.
Therefore, we will report on theresults with renormalization, but only as addi-tional information.
The main description of theresults will focus on the normalized scores.4 Profiling with Lexical FeaturesThe most straightforward features that can beused are simply combinations of tokens in thetext.4.1 Lexical featuresSufficiently frequent tokens, i.e.
those that wereobserved at least a certain amount of times (inthis case 5) in some language reference corpus(in this case the Eindhoven corpus; uit denBoogaart, 1975) are used as features by them-selves.
For less frequent tokens we determine atoken pattern consisting of the sequence of char-acter types, e.g., the token ?Uefa-cup?
is repre-sented by the pattern ?#L#6+/CL-L?, where thefirst ?L?
indicates low frequency, 6+ the sizebracket, and the sequence ?CL-L?
a capital letterfollowed by one or more lower case letters fol-lowed by a hyphen and again one or more lowercase letters.
For lower case words, the final threeletters of the word are included too, e.g.
?waar-maken?
leads to ?#L#6+/L/ken?.
These patternshave been originally designed for English andDutch and will probably have to be extendedwhen other languages are being handled.In addition to the form of the token, we also usethe potential syntactic usage of the token as afeature.
We apply the first few modules of amorphosyntactic tagger (in this case Wotan-Lite;Van Halteren et al, 2001) to the text, which de-termine which word class tags could apply toeach token.
For known words, the tags are takenfrom a lexicon; for unknown words, they are es-timated on the basis of the word patterns de-scribed above.
The three (if present) most likelytags are combined into a feature, e.g.
?niet?
leadsto ?#H#Adv(stell,onverv)-N(ev,neut)?
and?waarmaken?
to ?#L#V(inf)-N(mv,neut)-V(verldw, onverv)?.
Note that the most likelytags are determined on the basis of the token it-self and that the context is not consulted.
Themodules of the tagger which do context depend-ent disambiguation are not applied.Op top of the individual token and tag featureswe use all possible bi- and trigrams which can bebuilt with them, e.g.
the token combination ?konniet waarmaken?
leads to features such as?wcw=#H#kon#H#Adv(stell,onverv)-N(ev,neut)#L#6+/L/ken?.
Since the number of featuresquickly grows too high for efficient processing,we filter the set of features by demanding that afeature occurs in a set minimum number of textsin the profile reference corpus (in this case two).A feature which is filtered out instead contributesto a rest category feature, e.g.
the feature abovewould contribute to ?wcw=<OTHER>?.
For thecurrent corpus, this filtering leads to a feature setof about 100K features.The lexical features currently also include fea-tures for utterance length.
Each utterance leads totwo such features, viz.
the exact length (e.g.?len=15?)
and the length bracket (e.g.
?len=10-19?
).4.2 Results with lexical featuresA very rough first reconnaissance of settings forD and S suggested that the best results could beachieved with D between 0.1 and 2.4 and S be-tween 0.0 and 1.0.
Further examination of thisarea leads to FAR FRR=0  scores ranging down toaround 15%.
Figure 1 shows the scores at varioussettings for D and S. The z-axis is inverted (i.e.
1- FAR FRR=0  is used) to show better scores aspeaks rather than troughs.The most promising area is the ridge along thetrough at D=0.0, S=0.0.
A closer investigation ofthis area shows that the best settings are D=0.575and S=0.15.
The FAR FRR=0  score here is 14.9%,i.e.
there is a threshold setting such that if alltexts by the authors themselves are accepted,only 14.9% of texts by other authors are falselyaccepted.The very low value for S is surprising.
It indi-cates that it is undesirable to give too much atten-tion to features which deviate much in the samplebeing measured; still, in the area in question, thescore does peak at a positive S value, indicatingthat some such weighting does have effect.
Suc-cessful low scores for S can also be seen in thehill leading around D=1.0, S=0.3, which peaks atan FAR FRR=0  score of around 17 percent.
Fromthe shape of the surface it would seem that aninvestigation of the area across the S=0.0 dividemight still be worthwhile, which is in contradic-tion with the initial finding that negative valuesproduce no useful results.5 Beyond Lexical FeaturesAs stated above, once the basic viability of thetechnique was confirmed, more types of featureswould be added.
As yet, this is limited to syntac-tic features.
We will first describe the systemquality using only syntactic features, and thendescribe the results when using lexical and syn-tactic features in combination.5.1 Syntactic FeaturesWe used the Amazon parser to derive syntacticconstituent analyses of each utterance (Coppen,2003).
We did not use the full rewrites, but ratherconstituent N-grams.
The N-grams used were:?
left hand side label, examining constituentoccurrence?
left hand side label plus one label from theright hand side, examining dominance?
left hand side plus label two labels fromthe right hand side, in their actual order,examining dominance and linear prece-denceFor each label, two representations are used.
Thefirst is only the syntactic constituent label, thesecond is the constituent label plus the headword.
This is done for each part of the N-gramsindependently, leading to 2, 4 and 8 features re-spectively for the three types of N-gram.
Fur-thermore, each feature is used once by itself,once with an additional marking for the depth ofthe rewrite in the analysis tree, once with an addi-tional marking for the length of the rewrite, andonce with both these markings.
This means an-other multiplication factor of four for a total of 8,16 and 32 features respectively.
After filtering forminimum number of observations, again at leastan observation in two different texts, there areabout 900K active syntactic features, nine timesas many as for the lexical features.Investigation of the results for various settingshas not been as exhaustive as for the lexical fea-tures.
The best settings so far, D=1.3, S=1.4,yield an FAR FRR=0  of 24.8%, much worse thanthe 14.9% seen for lexical features.5.2 Combining Lexical and Syntactic Fea-turesFrom the FAR FRR=0  score, it would seem thatsyntactic features are not worth pursuing any fur-Figure 1: The variation of FAR (or rather 1-FAR)as a function of D and S, with D ranging from 0.1to 2.4 and S from 0.0 to 1.0.ther, since they perform much worse than lexicalones.
However, they might still be useful if wecombine their scores with those for the lexicalfeatures.
For now, rather than calculating newcombined profiles, we just added the scores fromthe two individual systems.
The combination ofthe best two individual systems leads to an FARFRR=0  of 10.3%,  a solid improvement over lexicalfeatures  by themselves.
However, the best indi-vidual systems are not necessarily the best com-biners.
The best combination systems produceFAR FRR=0  measurements down to 8.1%, withsettings in different parts of the parameter space.It should be observed that the improvementgained by combination is linked to the chosenquality measure.
If we examine the ROC-curvesfor several types of systems (plotting the FARagainst the FRR; Figure 2), we see that the com-bination curves as a whole do not differ muchfrom the lexical feature curve.
In fact, the EERfor the ?best?
combination system is worse thanthat for the best lexical feature system.
Thismeans that we should be very much aware of therelative importance of FAR and FRR in any spe-cific application when determining the ?optimal?features and parameters.6 Parameter SettingsA weak point in the system so far is that there isno automatic parameter selection.
The best re-sults reported above are the ones at optimal set-tings.
One would hope that optimal settings ontraining/tuning data will remain good settings fornew data.
Further experiments on other data willhave to shed more light on this.
Another choicewhich cannot yet be made automatically is that ofa threshold.
So far, the presentation in this paperhas been based on a single threshold for all au-thor/text combinations.
That there is an enormouspotential for improvement can be shown by as-suming a few more informed methods of thresh-old selection.The first method uses the fact that, in our ex-periments, there are always one true and sevenfalse authors.
This means we can choose thethreshold at some point below the highest of theeight scores.
We can hold on to the single thresh-old strategy  if we first renormalize, as  describedin Section 3.4, and then choose a single value tothreshold the renormalized values against.
Thesecond method assumes that we will be able tofind an optimal threshold for each individual runof the system.
The maximum effect of this can beestimated with an oracle providing the optimalthreshold.
Basically, since the oracle thresholdwill be at the score for the text by the author, weFigure 2: ROC (FAR plotted against FRR) for avarying threshold at good settings of D and S fordifferent types of features.
The top pane shows thewhole range (0 to 1)  for FAR and FRR.
The bottompane shows the area from 0.0 to 0.2.are examining how many texts by other authorsscore better than the text by the actual author.Table 1 compares the results for the best settingsfor these two new scenarios with the results pre-sented above.
Renormalizing already greatly im-proves the results.
Interestingly, in this scenario,the syntactic features outperform the lexical ones,something which certainly merits closer investi-gation after the parameter spaces have beencharted more extensively.
The full potential ofprofiling becomes clear in the Oracle thresholdscenario, which shows extremely good scores.Still, this potential will yet have to be realized byfinding the right automatic threshold determina-tion mechanism.7 Comparison to Previous AuthorshipAttribution WorkAbove, we focused on the authorship verificationtask, since it is the harder problem, given that thepotential group of authors is unknown.
However,as mentioned in Section 2, previous work withthis data has focused on the authorship recogni-tion problem, to be exact on selecting the correctauthor out of two potential authors.
We repeat thepreviously published results in Table 2, togetherwith linguistic profiling scores, both for the 2-way and for the 8-way selection problem.To do attribution with linguistic profiling, wecalculated the author scores for each author fromthe set for a given text, and then selected the au-thor with the highest score.
The results are shownin Table 2, using lexical or syntactic features orboth, and with and without renormalization.
TheOracle scenario is not applicable as we are com-paring rather than thresholding.In each case, the best results are not just found ata single parameter setting, but rather over a largerarea in the parameter space.
This means that thechoice of optimal parameters will be more robustwith regard to changes in authors and text types.We also observe that the optimal settings for rec-ognition are very different from those for verifi-cation.
A more detailed examination of theresults is necessary to draw conclusions aboutthese differences, which is again not possibleuntil the parameter spaces have been chartedmore exhaustively.LexicalFeaturesSyntacticFeaturesCom-bina-tionSinglethreshold14.9% 24.8% 8.1%Singlethreshold afterrenormalization9.3% 6.0% 2.4%Oracle thresh-old per run0.8% 1.6% 0.2%Table 1: Best FAR FRR=0 scores for verification withvarious feature types and threshold selection mecha-nisms.2-wayerrors/5042-waypercentcorrect8-wayerrors/728-waypercentcorrect50 func-tion words,PCA?
50%followedby LDA?
60%LDA withcross-sampleentropyweighting?
80%all tokens,WPDVmodeling97.8%Lexical 6 98.8% 5 93%Syntactic 14 98.2% 10 86%Combined 3 99.4% 2 97%Lexical(renorm.
)1 99.8% 1 99%Syntactic(renorm.
)4 99.2% 3 96%Combined(renorm.
)0 100.0% 0 100%Table 2: Authorship recognition quality for variousmethods.All results with normalized scores are alreadybetter than the previously published results.When applying renormalization, which might beclaimed to be justified in this particular author-ship attribution problem, the combination systemreaches the incredible level of making no mis-takes at all.8 ConclusionLinguistic profiling has certainly shown its worthfor authorship recognition and verification.
Atthe best settings found so far, a profiling systemusing combination of lexical and syntactic fea-tures is able select the correct author for 97% ofthe texts in the test corpus.
It is also able to per-form the verification task in such a way that itrejects no texts that should be accepted, whileaccepting only 8.1% of the texts that should berejected.
Using additional knowledge about thetest corpus can improve this to 100% and 2.4%.The next step in the investigation of linguisticprofiling for this task should be a more exhaus-tive charting of the parameter space, and espe-cially the search for an automatic parameterselection procedure.
Another avenue of futureresearch is the inclusion of even more types offeatures.
Here, however, it would be useful todefine an even harder verification task, as thecurrent system scores already very high and fur-ther improvements might be hard to measure.With the current corpus, the task might be madeharder by limiting the size of the test texts.Other corpora might also serve to provide moreobstinate data, although it must be said that thecurrent test corpus was already designed specifi-cally for this purpose.
Use of further corpora willalso help with parameter space charting, as theywill show the similarities and/or differences inbehaviour between data sets.
Finally, with theright types of corpora, the worth of the techniquefor actual application scenarios could be investi-gated.So there are several possible routes to furtherimprovement.
Still, the current quality of the sys-tem is already such that the system could be ap-plied as is.
Certainly for authorship recognitionand verification, as we hope to show by our par-ticipation in Patrick Juola?s Ad-hoc AuthorshipAttribution Contest (to be presented atALLC/ACH 2004), for language verification (cf.van Halteren and Oostdijk, 2004), and possiblyalso for other text classification tasks, such aslanguage or language variety recognition, genrerecognition, or document classification for IRpurposes.ReferencesHarald Baayen, Hans van Halteren, Anneke Neijt, andFiona Tweedie.
2002.
An Experiment in Author-ship Attribution.
Proc.
JADT 2002, pp.
69-75.Ton Broeders.
2001.
Forensic Speech and AudioAnalysis, Forensic Linguistics 1998-2001 ?
A Re-view.
Proc.
13th Interpol Forensic Science Sympo-sium, Lyon, France.C.
Chaski.
2001.
Empirical Evaluations of Language-Based Author Identification Techniques.
ForensicLinguistics 8(1): 1-65.Peter Arno Coppen.
2003.
Rejuvenating the Amazonparser.
Poster presentation CLIN2003, Antwerp,Dec.
19, 2003.David Holmes.
1998.
Authorship attribution.
Literaryand Linguistic Computing 13(3):111-117.F.
Mosteller, and D.L.
Wallace.
1984.
Applied Bayes-ian and Classical Inference in the Case of the Fed-eralist Papers (2nd edition).
Springer Verlag, NewYork.P.
C. Uit den Boogaart.
1975.
Woordfrequenties ingeschreven en gesproken Nederlands.
Oosthoek,Scheltema & Holkema, Utrecht.Hans van Halteren, Jakub Zavrel, and Walter Daele-mans.
2001.
Improving accuracy in word class tag-ging through the combination of machine learningsystems.
Computational Linguistics 27(2):199-230.Hans van Halteren and Nelleke Oostdijk, 2004.
Lin-guistic Profiling of Texts for the Purpose of Lan-guage Verification.
Proc.
COLING 2004.Hans van Halteren, Marco Haverkort, Harald Baayen,Anneke Neijt, and Fiona Tweedie.
To appear.
NewMachine Learning Methods Demonstrate the Exis-tence of a Human Stylome.
Journal of QuantitativeLinguistics.
