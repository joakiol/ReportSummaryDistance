Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 510?518,Beijing, August 2010Open Entity Extraction from Web Search Query LogsAlpa JainYahoo!
Labsalpa@yahoo-inc.comMarco PennacchiottiYahoo!
Labspennac@yahoo-inc.comAbstractIn this paper we propose a completely un-supervised method for open-domain en-tity extraction and clustering over querylogs.
The underlying hypothesis is thatclasses defined by mining search user activ-ity may significantly differ from those typ-ically considered over web documents, inthat they better model the user space, i.e.users?
perception and interests.
We showthat our method outperforms state of the art(semi-)supervised systems based either onweb documents or on query logs (16% gainon the clustering task).
We also report evi-dence that our method successfully supportsa real world application, namely keywordgeneration for sponsored search.1 IntroductionSearch engines are increasingly moving beyond thetraditional keyword-in document-out paradigm, andare improving user experience by focusing on user-oriented tasks such as query suggestions and searchpersonalization.
A fundamental building block ofthese applications is recognizing structured infor-mation, such as, entities (e.g., mentions of people,organizations, or locations) or relations among en-tities (Cao et al, 2008; Hu et al, 2009).
For this,search engines typically rely on large collections ofentities and relations built using information extrac-tion (IE) techniques (Chaudhuri et al, 2009).Commonly used IE techniques follow two mainassumptions: (1) IE focuses on extracting infor-mation from syntactically and semantically ?well-formed?
pieces of texts, such as, news corpora andweb documents (Pennacchiotti and Pantel, 2009);(2) extraction processes are bootstrapped with somepre-existing knowledge of the target domain (e.gentities are typically extracted for pre-defined cat-egories, such as Actors, Manufacturers, Persons,Locations (Grishman and Sundheim, 1996)).
Priorwork (Banko et al, 2007), has looked into relax-ing the second assumption and proposed open in-formation extraction (OIE), a domain-independentand scalable extraction paradigm, which howeverfocuses mostly on web corpora.In this paper, we argue that for user-oriented ap-plications discussed earlier, IE techniques shouldgo beyond the traditional approach of using ?well-formed?
text documents.
With this in mind, we ex-plore the utility of search query logs, a rich sourceof user behaviors and perception, and build tech-niques for open entity extraction and clusteringover query logs.
We hypothesize that web docu-ments and query logs model two different spaces:web documents model the web space, i.e.
generalknowledge about entities and concepts in an objec-tive and generic way; search query logs model theuser space, i.e.
the users?
view and perception ofthe world in a more specific fashion, where avail-able information directly expresses users?
needsand intents.
For example, in a web space, ?brit-ney spears?
will tend to be similar and be clus-tered with other singers, such as ?celine dion?
and?bruce springsteen?.
On the contrary, in the users?space, she is highly similar and clustered with othergossiped celebrities like ?paris hilton?
and ?serenawilliams?
: the users?
space better models the users?perception of that person; such a space is thenhighly valuable for all those applications whereusers?
perceptions matters.To computationally model our hypothesis forOIE over search query logs, we present a two phaseapproach to OIE for search query logs.
The firstphase (entity extraction) extracts entities from thesearch query logs using an unsupervised approach,by applying pattern-based heuristics and statisticalmeasures.
The second phase (entity clustering) in-duces classes over these entities by applying clus-tering techniques.
In summary, our main contribu-510tions are: (1) We propose and instantiate a novelmodel for open information extraction over websearch query logs; and we apply it to the task ofentity extraction and clustering.
(2) We show howwe characterize each extracted entity to capture the?user space?, and induce classes over the entities.
(3) We present an extensive evaluation over real-lifedatasets showing that query logs is a rich source fordomain-independent user-oriented extraction tasks(Section 3).
We also show the practicality of ourapproach by incorporating it into a real-world appli-cation, namely keyword suggestions for sponsoredsearch (Section 4).2 Open Entity Extraction on Query LogIn this section, we present our method for openentity extraction from query logs.
We first de-scribe our heuristic method for extracting entities(Section 2.1), and then three different feature ?userspaces?
to cluster the entities (Section 2.2).2.1 Entity ExtractionIn our setting, entities correspond to Named Enti-ties.
i.e.
they are defined using the standard namedentity types described in (Sekine et al, 2002)1.
Inthis paper, we use a set of entities extracted fromquery log, obtained by applying a simple algorithm(any other query log entity extraction method wouldapply here, e.g.
(Pasca, 2007b)).
The algorithm isbased on the observation that oftentimes users con-struct their search query by copy-pasting phrasesfrom existing texts.
Due to this phenomenon, userqueries often carry over surface-level propertiessuch as capitalization and tokenization information.Our approach realizes this observation by iden-tifying contiguous capitalized words from a userquery.
(In our experiments, we observed that 42%of the queries had at least one upper-case character.
)Specifically, given a query Q = q1 q2 q3 ?
?
?
qn,we define a candidate entity E = e1 e2 ?
?
?
em asthe maximal sequence of words (i.e., alpha-numericcharacters) in the query such that each word ei inthe entity begins with an uppercase character.
Theset of candidate entities is then cleaned by apply-ing a set of heuristics, thus producing the final setof entities.
In particular, for each extracted entity,1We exclude ?Time?
and ?Numerical Expressions?, whichare out of the scope of our study.we assign two confidence scores: a Web-based rep-resentation score and a query-log-based standalonescore.
The representation score checks if the case-sensitive representation observed for E in Q, is themost likely representation for E, as observed ona Web corpus (e.g., ?DOor HANGing TIps?
is as-signed a low representation score).
The standalonescore is based on the observation that a candidateE should often occur in a standalone form amongthe search query logs, in order to get the status ofa proper named entity as defined in (Sekine et al,2002; Grishman and Sundheim, 1996).
In practice,among the query logs we must find queries of theform Q == E, capturing the fact that users arelooking to learn more about the given entity2.2.2 Entity ClusteringThe clustering phase takes as input any of the fea-ture spaces presented in the rest of this section, andgroups the entities according to the similarity oftheir vectors in the space.
The desiderata for a clus-tering algorithm for the task of open-domain infor-mation extraction are the following: (1) The algo-rithm must be highly scalable, efficient, and ableto handle high dimensionality, since the number ofqueries and the size of the feature vectors can belarge; (2) We do not know in advance the numberof clusters; therefore, the algorithm needs not to re-quire a pre-defined number of clusters.Any clustering algorithm fulfilling the above re-quirements would fit here.
In our experiments, weadopt a highly scalable Map-Reduce implementa-tion of the hard-clustering version of Clustering byCommittee (CBC), a state-of-the-art clustering al-gorithm presented in (Pantel and Lin, 2002).Context Feature Space.
The basic hypothesis forthe context feature space, is that an entity can be ef-fectively represented by the set of contexts in whichit appears in queries.
This allows to capture theusers?
view of the entity, i.e.
what people query,and want to know about the entity.
This is similarto that proposed by Pasca (2007b; 2007a), i.e.
thatqueries provide good semantics cues for modelingnamed entities.Our query log feature space may significantlydiffer from a classical contextual feature space com-2We refer the readers to (Jain and Pennacchiotti, 2010) fordetails on the entity extraction algorithms.511puted over a Web corpus, since the same entity canbe differently perceived and described in the twocorpora (query log and Web).
Consider for exam-ple the entity ?galapagos islands?.
Typical contextson the Web and query log for this entity are:web: endemic birdsweb: big turtlesweb: charles darwin foundationweb: sensitive waterqlog : trip toqlog : divingqlog : where are theqlog : travel packageThe difference between the two representationsimplies that entities that are similar on the Web, arenot necessarily similar on query logs.
For exam-ple, on the Web ?galapagos islands?
is very simi-lar to other countries such as ?tasmania?, ?guinea?and ?luxemburg?
; while on query log is similar toother sea-side travel destination and related con-cepts, such as ?greek isle?, ?kauai snorkeling?
and?south america cruise?.
Our new similarity com-puted over query log, is potentially useful for thoseapplications in which is more important to representusers?
intents, than an objective description of enti-ties (e.g.
in query suggestion and intent modeling).To obtain our contextual representation we pro-ceed as follows.
For each entity e, we identifyall queries in the query log, in which e appears.Then, we collect the set of all suffixes and postfixesof the entity in those queries.
For example, giventhe entity ?galapagos islands?
and the query ?sum-mer 2008 galapagos islands tour?, the contexts are:?summer 2008?
and ?tour?.Once the set of all contexts of all entities has beencollected, we discard contexts appearing less than?
-times in the query log, so to avoid statistical bi-ases due to data sparseness (in the reported experi-ments we set ?
= 200).
We then compute the cor-rected pointwise mutual information (cpmi) (Pan-tel and Ravichandran, 2004) between each instanceand each context c as:cpmi(e, c) = log2f(e, c) ?
f(?, ?
)f(e) ?
f(c) ?M (1)where f(e, c) is the number of times e and coccur in the same query; f(e) and f(c) is thecount of the entity and the context in the querylog; f(?, ?)
the overall count of all co-occurrencesbetween contexts and entities; and M is the correc-tion factor presented in (Pantel and Ravichandran,2004), that eases the pmi?s bias towards infrequententities/features.
Each instance is then representedin the feature space of all contexts, by the computedpmi values.
Note that our method does not use anyNLP parsing, since queries rarely present syntacticstructure.
This guarantees the method to be com-putationally inexpensive and easily adaptable tolanguages other than English.Clickthrough Feature Space.
During a searchsession, users issue a search query for which thesearch engine presents a list of result urls.
Of thesearch results, users choose those urls that are rep-resentative of their intent.
This interaction is cap-tured by means of a click, which is logged by mostsearch engines as click-through data.
For instance,a search log may contain the following clicked urlsfor a query ?flv converter?, for different users:user1: www.flv-converter.comuser2: www.videoconverterdownload.com/flv/user3: www.ripzor.com/flv.htmlOur main motivation behind clustering entitiesbased on past user click behavior is that non-identical queries that generate clicks on the sameurls capture similar user intent.
Thus, grouping en-tities that were issued as a query and generated userclicks on the same url may be considered similar.For instance, the query ?convert flv?
may also gen-erate clicks on one of the above urls, thus hintingthat the two entities are similar.
We observed thatwebsites tend to dedicate a url per entity.
There-fore, grouping by click urls can lead to clusters withsynonyms (i.e., different ways of representing thesame entity) or variants (e.g., spelling errors).
Toget more relevant clusters, instead of grouping en-tities by the click urls, we use the base urls.
Forinstance, the url www.ripzor.com/flv.htmlis generalized to www.ripzor.com.With the advent of encyclopedic web-sites such as, www.wikipedia.org andwwww.youtube.com, naively clustering entitiesby the clickthrough data can led to non-similarentities to be placed in the same cluster.
Forinstance, we observed the most frequently clickedbase url for both ?gold retriever?
and ?abrahamlincoln?
is www.wikipedia.org.
To addressthis issue, in our experiments we employed a512stop-list by eliminating top-5 urls based on theirinverse document frequency, where an entity isintended as the ?document?.In practice, each extracted entity e is representedby a feature vector of size equal to the number ofdistinct base urls in the click-through data, acrossall users.
Each dimension in the vector represents aurl in the click-through information.
The value f ofan entity e for the dimension associated with url jis computed as:f(e, j) =???w(e,j)?
?|U|i w(e,i)2if url j clicked for query e;0 otherwise.where U is the set of base urls found in click-through data when entity e was issued as a query;and w(e, i) is the number of time the base url i wasclicked when e was a query.Hybrid Feature Space.
We also experiment a hy-brid feature space, which is composed by the nor-malized union of the two feature spaces above (i.e.context and clickthrough).
Though more complexhybrid models could be applied, such as one basedon ensemble clustering, we here opt for a simplesolution which allows to better read and compare toother methods.3 Experimental EvaluationIn this section, we report experiments on our clus-tering method.
The goal of the experiment is two-fold: (1) evaluate the intrinsic quality of the cluster-ing methods, i.e.
if two entities in the same clusterare similar or related from a web user?s perspec-tive; (2) verify if our initial hypothesis holds, i.e.if query log based features spaces capture differentproperties than Web based feature spaces (i.e.
the?user space?).
In Section 3.1 we describe our ex-perimental setup; and, in 3.2 we provide the results.We couple this intrinsic evaluation with an extrinsicapplication-driven one in Section 4.3.1 Experimental SettingsIn the experiments we use the following datasets:Query log: A random sample of 100 million, fullyanonymized queries collected by the Yahoo!
searchengine in the first 3 months of 2009, along with theirfrequency.
This dataset is used to generate both thecontext and the clickthrough feature spaces for theclustering step.Web documents: A collection of 500 million webpages crawled by a Yahoo!
search engine crawl.This data set is used to implement a web-based fea-ture space that we will compare to in Section 3.2.Entity set: A collection of 2,067,385 entities, ex-tracted with the method described in 2.1, whichshows a precision of 0.705 ?0.044.
Details onthe evaluation of such method are available in (Jainand Pennacchiotti, 2010), where a full comparisonwith state-of-the-art systems such as (Pasca, 2007b)and (Banko et al, 2007) are also reported.Evaluation methodology: Many clustering evalu-ation metrics have been proposed, ranging from Pu-rity to Rand-statistics and F-Measure.
We first se-lect from the original 2M entity set, a random set ofn entities biased by their frequency in query logs,so to keep the experiment more realistic (more fre-quent entities have more chances to be picked inthe sample).
For each entity e in the sample set,we derived a random list of k entities that are clus-tered with e. In our experiments, we set n = 10and k = 20.
We then present to a pool of paid edi-tors, each entity e along with the list of co-clusteredentities.
Editors are requested to classify each co-clustered entity ei as correct or incorrect.
An entityei is deemed as correct, if it is similar or related to efrom a web user?s perspective: to capture this intu-ition, the editor is asked the question: ?If you wereinterested in e, would you be also interested in eiin any intent?
?.3 Annotators?
agreement over a ran-dom set of 30 entities is kappa = 0.64 (MarquesDe Sa?, 2003), corresponding to substantial agree-ment.
Additionally, we ask editors to indicate therelation type between e and ei (synonyms, siblings,parent-child, topically related).Compared methods:CL-CTX: A CBC run, based on the query log con-text feature space (Section 2.2).CL-CLK: A CBC run, based on the clickthroughfeature space (Section 2.2).3For example, if someone is interested in ?hasbro?, he couldbe probably also be interested in ?lego?, when the intent is buy-ing a toy.
The complete set of annotation guidelines is reportedin (Jain and Pennacchiotti, 2010).513method # cluster avg cluster sizeCL-Web 1,601 240CL-CTX 875 1,182CL-CLK 4,385 173CL-HYB 1,580 478Table 1: Statistics on the clustering results.CL-HYB: A CBC run, based on the hybrid spacethat combines CL-CTXand CL-CLK(Section 2.2).CL-Web: A state-of-the-art open domain methodbased on features extracted from the Web docu-ments data set (Pantel et al, 2009).
This methodruns CBC over a space where features are the con-texts in which an entity appears (noun chunks pre-ceding and following the target entity); and featurevalue is the pmi between the entity and the chunks.Evaluation metrics: We evaluate each method us-ing accuracy, intended as the percentage of correctjudgments.3.2 Experimental ResultsTable 3 reports accuracy results.
CL-HYB is thebest performing method, achieving 0.85 accuracy,respectively +4% and +11% above CL-CLK andCL-Web.
CL-CTX shows the lowest performance.Our results suggest that query log spaces are moresuitable to model the ?user space?
wrt web features.Specifically, clickthrough information are most use-ful confirming our hypothesis that queries that gen-erate clicks on the same urls capture similar userintents.To have an anecdotal and practical intuition onthe results, in Table 2 we report some entities andexamples of other entities from the same clusters, asobtained from the CL-HYB and CL-Web methods.The examples show that CL-HYB builds clustersaccording to a variety of relations, while CL-Webmostly capture sibling-like relations.One relevant of such relations is topicality.
Forexample, for ?aaa insurance?
the CL-HYB clustermostly contains entities that are topically related tothe American Automobile Association, while theCL-Web cluster contains generic business compa-nies.
In this case, the CL-HYB approach sim-ply chose to group together entities having clicksto ?aaa.com?
and appearing in contexts as ?autoclub?.
On the contrary, CL-Web grouped accord-ing to contexts such as ?selling?
and ?company?.The entity ?hip osteoarthritis?
shows a similar be-entity CL-HYB CL-Webaaa insurance roadside assistance loanmaxpersonal liability insurance pilot car serviceinternational driving permits localnetaaa minnesota fibermarktravelers checks country companiesinsuranceparis hilton brenda costa julia robertsadriana sklenarikova brad pittkelly clarkson nicole kidmananja rubik al pacinofederica ridolfi tom hanksgoldie hawn bonnie hunt julia robertsbrad pitt brad pitttony curtis nicole kidmannicole kidman al pacinonicholas cage tom hanksbasic algebra numerical analysis math tablesdiscrete math trigonometry helplattice theory mathtutornonlinear physics surface area formularamsey theory multiplying fractionship osteoarthritis atherosclerosis wrist arthritispneumonia disc replacementhip fracture rotator cuff tearsbreast cancer shoulder replacementanorexia nervosa american orthopedicsocietyacer america acer aspire accessories microsoftaspireone casio computeracer monitors borland softwareacer customer service sonyacer usa nortel networksTable 2: Sample of the generated entity clusters.havior: CL-HYB groups entities topically relatedto orthopedic issues, since most of the entities aresharing contexts such as ?treatment?
and ?recovery?and, at the same time, clicks to urls such as ?or-thoinfo.aaos.org?
and ?arthirtis.about.com?.Another interesting observation regards entitiesreferring to people.
The ?paris hilton?
and ?goldiehawn?
examples show that the CL-Web approachgroups famous people according to their category?
i.e.
profession in most cases.
On the contrary,query log approaches tend to group people accord-ing to their social attitude, when this prevails overthe profession.
In the example, CL-HYB clustersthe actress ?goldie hawn?
with other actors, while?paris hilton?
is grouped with an heterogeneous setof celebrities that web users tend to query and clickin a same manner: In this case, the social per-sona of ?paris hilton?
prevails over its profession(actress/singer).
This aspect is important in manyapplications, e.g.
in query suggestion, where onewants to propose to the user entities that have beensimilarly queried and clicked.In order to check if the above observations arenot anecdotal, we studied the relation type annota-tion provided by the editors (Table 4).
Table shows514method PrecisionCL-Web 0.735CL-CTX 0.460CL-CLK 0.815 ?CL-HYB 0.850 ?Table 3: Precision of various clustering methods(?
indicates statistical-significant better than theCL-Web method, using t-test).that query log based methods are more varied in thetype of clusters they build.
Table 5 shows the dif-ference between the clustering obtained using thedifferent methods and the overlap between the pro-duced clusters.
For example, 40% of the relationsfor the CL-HYB system are topical, while 32% aresibiling ones.
On the contrary, the CL-Web methodis highly biased towards sibling relations.As regard a more attentive analysis of the dif-ferent query log based methods, CL-CTX has thelowest performance.
This is mainly due to the factthat contextual data are sometimes too sparse andgeneric.
For example ?mozilla firefox?
is clusteredwith ?movie program?
and ?astro reading?
becausethey share only some very generic contexts such as?free downloads?.
In order to get more data, one op-tion is to relax the ?
threshold (see Section 2) so toinclude more contexts in the semantic space.
Unfor-tunately, this would have a strong drawback, in thatlow-frequency context tend to be idiosyncratic andspurious.
A typical case regards recurring queriessubmitted by robots for research purposes, such as?who is X?, ?biography of X?, or ?how to X?.
Thesequeries tend to build too generic clusters containingpeople or objects.
Another relevant problem of theCL-CTX method is that even when using a high ?cut, clusters still tend to be too big and generic, asstatistics in Table 4 shows.CL-CTX, despite the low performance, is veryuseful when combined with CL-CLK.
Indeed theCL-HYB system improves +4% over the CL-CLKsystem alone.
This is because the CL-HYB methodis able to recover some misleading or incompleteevidence coming from the CL-CLK using featuresprovided by CL-CLK.
For example, editors judgedas incorrect 11 out of 20 entities co-clustered withthe entity ?goldie hawn?
by CL-CLK.
Most of theseerrors are movies (e.g.
?beverly hills cops?)
soapoperas (e.g.
?sortilegio?)
and directors, because allhave clicks to ?imdb.com?
and ?movies.yahoo.com?.class methodCL-Web CL-CTX CL-CLK CL-HYBtopic 0.27 0.46 0.46 0.40sibling 0.72 0.43 0.29 0.32parent - 0.09 0.13 0.09child 0.01 - 0.01 0.02synonym 0.01 0.03 0.12 0.16Table 4: Fraction of entities that have been classi-fied by editors in the different relation types.method labelled clustersCL-CTX CL-CLK CL-HYB CL-WebCL-CTX - 0.2 0.53 0.29CL-CLK 0.21 - 0.54 0.34CL-HYB 0.53 0.51 - 0.31CL-Web 0.33 0.35 0.41 -Table 5: Purity of clusters for each method usingclusters from other methods as ?labelled?
data.CL-HYB recovers these errors by including featurescoming from CL-CTX such as ?actress?.In summary, query log spaces group together en-tities that are similar by web users (this being topi-cal similarity or social attitude), thus constituting apractical model of the ?user space?
to be leveragedby web applications.4 Keywords for Sponsored SearchIn this section we explore the use of our methods forkeyword generation for sponsored search.
In spon-sored search, a search company opens an auction,where on-line advertisers bid on specific keywords(called bidterms).
The winner is allowed to put itsad and link on the search result page of the searchcompany, when the bidterm is queried.
Compa-nies such as Google and Yahoo are investing effortsfor improving their bidding platforms, so to attractmore advertisers in the auctions.
Bidterm sugges-tion tools (adWords, 2009; yahooTool, 2009) areused to help advertiser in selecting bidterms: theadvertisers enters a seed keyword (seed) express-ing the intent of its ad, and the tool returns a listof suggested keywords (suggestions) that it can usefor bidding ?
e.g for the seed ?mp3 player?, a sug-gestion could be ?ipod nano?.
The task of gen-erating bid suggestions (i.e.
keyword generation)is typically automatic, and has received a grow-ing attention in the search community for its im-pact on search company revenue.
The main prob-lem of existing methods for suggestion (adWords,2009; yahooTool, 2009; wordTracker, 2009) is that515they produce only suggestions that contain the ini-tial seed (e.g.
?belkin mp3 player?
for the seed ?mp3player?
), while nonobvious (and potentially less ex-pensive) suggestions not containing the seed are ne-glected (e.g.
?ipod nano?
for ?mp3 player?).
Forexample for ?galapagos islands?, a typical produc-tion system suggests ?galapagos islands tour?
whichcost almost 5$ per click; while the less obvious ?islasanta cruz?
would cost only 0.35$.
Below we showour method to discover such nonobvious sugges-tions, by retrieving entities in the same cluster ofa given seed.4.1 Experimental SettingWe evaluate the quality of the suggestions proposedby different methods for a set of seed bidterms.,adopting the evaluation schema in (Joshi and Mot-wani, 2006)Dataset Creation.
To create the set of seeds, weuse Google skTool4.
The tool provides a list ofpopular bid terms, organized in a taxonomy of ad-vertisement topics.
We select 3 common topics:tourism, vehicles and consumer-electronics.
Foreach topic, we randomly pick 5 seeds among the800 most popular bid terms, which also appear inour entity set described in Section 3.1.5.
We evalu-ate a system by collecting all its suggestions for the15 seeds, and then extracting a random sample of20 suggestions per seed.Evaluation and Metrics.
We use precision andNonobviousness.
Precision is computed by ask-ing two experienced human experts to classify eachsuggestion of a given seed, as relevant or irrelevant.A suggestion is deemed as relevant if any advertiserwould likely choose to bid for the suggestion, hav-ing as intent the seed.
Annotator agreement, evalu-ated on a subset of 120 suggestions is kappa = 0.72(substantial agreement).
Precision is computed asthe percentage of suggestions judged as relevant.Nonobviousness is a metric introduced in (Joshiand Motwani, 2006), capturing how nonobvious thesuggestions are.
It simply counts how many sug-4http://www.google.com/sktool5The final set of 15 bid terms is: tourism:galapagosislands,holiday insurance,hotel booking,obertauern,wagrain;vehicles:audi q7,bmw z4,bmw dealers,suzuki grand vi-tara,yamaha banshee; consumer electr:canon rebel xti,divxconverter,gtalk,pdf reader,flv converter.gestions for a given seed do not contain the seed it-self (or any of its variants): this metric is computedautomatically using string matching and a simplestemmer.Comparisons.
We compare the suggestions pro-posed by CL-CTX, CL-CLK, and CL-HYB, againstWeb and two reference state-of-the-art produc-tion systems: Google AdWords (GOO) and YahooSearch Marketing Tool (YAH).
As concerns ourmethods, we extract as suggestions the entities thatoccur in the same cluster of a given seed.
For theproduction systems, we rely on the suggestions pro-posed on the website of the tools.4.2 Experimental ResultsPrecision results are reported in the second columnof Table 6.
Both CL-CLK and CL-HYB outper-form Web in precision, CL-HYB being close to theupper-bound of the two production systems.
As ex-pected, production systems show a very high pre-cision but their suggestions are very obvious.
Ourresults are fairly in line with those obtained on asimilar dataset, by Joshi and Motwani (2006).A closer look at the results shows that most of theerrors for CL-CTX are caused by the same problemoutlined in Section 3.2: Some entities are wronglyassigned to a cluster, because they have some highcpmi context feature which is shared with the clus-ter centroid, but which is not very characteristicfor the entity itself.
This is particularly evident forsome of the low frequency entities, where cpmi val-ues could not reflect the actual semantics of the en-tity.
For example the entity ?nickelodeon?
(a kids tvchannel in UK) is assigned to the cluster of ?galapa-gos islands?, because of the feature ?cruise?
: indeed,some people query about ?nickelodeon cruise?
be-cause the tv channel organizes some kids cruises.Other mistakes are due to feature ambiguity.
Forexample, the entity ?centurion boats?
is assignedto the cluster of ?obertauern?
(a ski resort in Aus-tria), because they share the ambiguous feature ?ski?
(meaning either winter-ski or water-ski).
As for theCL-CLK system, some of the errors are caused bythe fact that some base url can refer to very differ-ent types of entities.
For example the entity ?colorcopier?
is suggested for the the camera ?canon rebelxti?, since they both share clicks to the Canon web-site.
The CL-HYB system achieves a higher preci-516method Precision NonobviousnessGOO 0.982 0.174YAH 0.966 0.195Web 0.814 0.827CL-CTX 0.547 0.963CL-CLK 0.827 0.630CL-HYB 0.946 0.567Table 6: Results for keyword generation.sion wrt CL-CTX and CL-CLK: the combination ofthe two spaces decreases the impact of misleadingfeatures ?e.g.
for ?yamaha bunshee?, all CL-HYB ?ssuggestions are correct, while almost all CL-CLK ?ssuggestions are incorrect: the hybrid system recov-ered the negative effect of the misleading featureebay.com, by backing up on features from thecontextual subspace (e.g.
?custom?, ?specs?, ?usedparts?
).Nonobviousness results are reported in columnthree of Table 6.
All our systems return a high num-ber of nonobvious suggestions (all above 50%).6On the contrary, GOO and YAH show low perfor-mance, as both systems are heavily based on thesubstring matching technique.
This strongly moti-vates the use of semantic approaches as those wepropose, that guarantee at the same time both ahigher linguistic variety and an equally high preci-sion wrt the production systems.
For example, forthe seeds ?galapagos islands?, GOO returns simplesuggestions such as ?galapagos islands vacations?and ?galapagos islands map?
; while CL-HYB re-turns ?caribbean mexico?
and ?pacific dawn?, twoterms that are semantically related but dissimilarfrom the seed.
Remember that these letter terms arerelated to the seed because they are similar in theuser space, i.e.
users looking at ?galapagos islands?tend to similarly look for ?caribbean mexico?
and?pacific dawn?.
These suggestions would then bevery valuable for tourism advertisers willing to im-prove their visibility through a non-trivial and pos-sibly less expensive set of bid terms.5 Related WorkWhile literature abounds with works on entity ex-traction from web documents (e.g.
(Banko et al,2007; Chaudhuri et al, 2009; Pennacchiotti andPantel, 2009)), the extraction of classes of entities6Note that very high values for CL-CTX may be mislead-ing, as many of the suggestions proposed by this system areincorrect (see precision results) and hence non-obvious (e.g.,?derek lewis?
for ?galapagos islands?
).over query logs is a pretty new task, recently intro-duced in (Pasca, 2007b).
Pasca?s system extractsentities of pre-defined classes in a semi-supervisedfashion, starting with an input class represented by aset of seeds, which are used to induce typical query-contexts for the class.
Contexts are then used toextract and select new candidate instances for theclass.
A similar approach is also adopted in (Sekineand Suzuki, 2007).
Pasca shows an improvementof about 20% accuracy, compared to existing Web-based systems.
Our extraction algorithm differsfrom Pasca?s work in that it is completely unsuper-vised.
Also, Pasca?s cannot be applied to OIE, i.e.it only works for pre-defined classes.
Our cluster-ing approach is related to Lin and Wu?s work (Linand Wu, 2009).
Authors propose a semi-supervisedalgorithm for query classification.
First, they ex-tract a large set of 20M phrases from a query log, asthose unique queries appearing more than 100 timesin a Web corpus.
Then, they cluster the phrasesusing the K-means algorithm, where features arethe phrases?
bag-of-words contexts computed overa web corpus.
Finally, they classify queries usinga logistic regression algorithm.
Our work differsfrom Lin and Wu, as we focus on entities instead ofphrases.
Also, the features we use for clustering arefrom query logs and click data, not web contexts.6 ConclusionsWe presented an open entity extraction approachover query logs that goes beyond the traditional webcorpus, with the goal of modeling a ?user-space?
asopposed to an established ?web-space?.
We showedthat the clusters generated by query logs substan-tially differ from those by a Web corpus; and thatour method is able to induce state-of-the-art qual-ity classes on a user-oriented evaluation on the realworld task of keyword generation for sponsoredsearch.
As future work we plan to: (i) experimentdifferent clustering algorithms and feature models,e.g.
soft-clustering for handling ambiguous enti-ties; (ii) integrate the Web space and the query logspaces; (iii) embed our methods in in existing toolsfor intent modeling, query suggestion and similia,to check its impact in production systems.517ReferencesadWords.
2009.
Google adwords.
ad-words.google.com/select/keywordtoolexternal.Banko, Michele, Michael Cafarella, Stephen Soderland,Matthew Broadhead, and Oren Etzioni.
2007.
Openinformation extraction from the web.
In Proceedingsof IJCAI.Cao, Huanhuan, Daxin Jiang, Jian Pei, Qi He, ZhenLiao, Enhong Chen, and Hang Li.
2008.
Context-aware query suggestion by mining click-through andsession data.
In Proceedings of KDD-08.Chaudhuri, Surajit, Venkatesh Ganti, and Dong Xin.2009.
Exploiting web search to generate synonymsfor entities.
In Proceedings of WWW-09.Grishman, R. and B. Sundheim.
1996.
Message under-standing conference- 6: A brief history.
In Proceed-ings of COLING.Hu, Jian, Gang Wang, Fred Lochovsky, Jian tao Sun,and Zheng Chen.
2009.
Understanding user?s queryintent with Wikipedia.
In Proceedings of WWW-09.Jain, Alpa and Marco Pennacchiotti.
2010.
Open In-formation Extraction from Web Search Query Logs.Technical Report YL-2010-003, Yahoo!
Labs.Joshi, Amruta and Rajeev Motwani.
2006.
Keywordgeneration for search engine advertising.
In Proceed-ings of Sixth IEEE-ICDM.Lin, Dekang and Xiaoyun Wu.
2009.
Phrase clusteringfor discriminative learning.
In Proceedings of ACL-IJCNLP-2009.Marques De Sa?, Joaquim P. 2003.
Applied Statistics.Springer Verlag.Pantel, Patrick and Dekang Lin.
2002.
Discoveringword senses from text.
In Proceedings KDD-02.Pantel, Patrick and Deepak Ravichandran.
2004.
Auto-matically labeling semantic classes.
In Proceeding ofHLT-NAACL-2004.Pantel, Patrick, Eric Crestan, Arkady Borkovsky, Ana-Maria Popescu, and Vishnu Vyas.
2009.
Web-scaledistributional similarity and entity set expansion.
InProceedings of EMNLP-09.Pasca, Marius.
2007a.
Organizing and searching theworld wide web of facts - step two: Harnessing thewisdom of the crowds.
In Proceedings of the WWW-2007.Pasca, Marius.
2007b.
Weakly-supervised discovery ofnamed entities using web search queries.
In Proceed-ings of CIKM-2007.Pennacchiotti, Marco and Patrick Pantel.
2009.
Entityextraction via ensemble semantics.
In Proceedings ofEMNLP-2009.Sekine, Satoshi and Hisami Suzuki.
2007.
Acquiringontological knowledge from query logs.
In Proceed-ings of WWW-07.Sekine, Satoshi, Kiyoshi Sudo, and Chikashi Nobata.2002.
Extended named entity hierarchy.
In Proceed-ings of LREC-2002.wordTracker.
2009.
Word tracker.www.wordtracker.com.yahooTool.
2009.
Yahoo search marketing.
searchmar-keting.yahoo.com.518
