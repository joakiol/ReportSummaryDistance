Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 306?314,Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLPThe Contribution of Linguistic Features to Automatic MachineTranslation EvaluationEnrique Amigo?1 Jesu?s Gime?nez2 Julio Gonzalo 1 Felisa Verdejo11UNED, Madrid{enrique,julio,felisa}@lsi.uned.es2UPC, Barcelonajgimenez@lsi.upc.eduAbstractA number of approaches to AutomaticMT Evaluation based on deep linguisticknowledge have been suggested.
How-ever, n-gram based metrics are still to-day the dominant approach.
The mainreason is that the advantages of employ-ing deeper linguistic information have notbeen clarified yet.
In this work, we pro-pose a novel approach for meta-evaluationof MT evaluation metrics, since correla-tion cofficient against human judges donot reveal details about the advantages anddisadvantages of particular metrics.
Wethen use this approach to investigate thebenefits of introducing linguistic featuresinto evaluation metrics.
Overall, our ex-periments show that (i) both lexical andlinguistic metrics present complementaryadvantages and (ii) combining both kindsof metrics yields the most robust meta-evaluation performance.1 IntroductionAutomatic evaluation methods based on similarityto human references have substantially acceleratedthe development cycle of many NLP tasks, suchas Machine Translation, Automatic Summariza-tion, Sentence Compression and Language Gen-eration.
These automatic evaluation metrics allowdevelopers to optimize their systems without theneed for expensive human assessments for eachof their possible system configurations.
However,estimating the system output quality according toits similarity to human references is not a trivialtask.
The main problem is that many NLP tasksare open/subjective; therefore, different humansmay generate different outputs, all of them equallyvalid.
Thus, language variability is an issue.In order to tackle language variability in thecontext of Machine Translation, a considerable ef-fort has also been made to include deeper linguis-tic information in automatic evaluation metrics,both syntactic and semantic (see Section 2 for de-tails).
However, the most commonly used metricsare still based on n-gram matching.
The reason isthat the advantages of employing higher linguisticprocessing levels have not been clarified yet.The main goal of our work is to analyze to whatextent deep linguistic features can contribute to theautomatic evaluation of translation quality.
Forthat purpose, we compare ?
using four differenttest beds ?
the performance of 16 n-gram basedmetrics, 48 linguistic metrics and one combinedmetric from the state of the art.Analyzing the reliability of evaluation met-rics requires meta-evaluation criteria.
In this re-spect, we identify important drawbacks of thestandard meta-evaluation methods based on cor-relation with human judgements.
In order toovercome these drawbacks, we then introduce sixnovel meta-evaluation criteria which represent dif-ferent metric reliability dimensions.
Our analysisindicates that: (i) both lexical and linguistic met-rics have complementary advantages and differentdrawbacks; (ii) combining both kinds of metricsis a more effective and robust evaluation methodacross all meta-evaluation criteria.In addition, we also perform a qualitative analy-sis of one hundred sentences that were incorrectlyevaluated by state-of-the-art metrics.
The analysisconfirms that deep linguistic techniques are neces-sary to avoid the most common types of error.Section 2 examines the state of the art Section 3describes the test beds and metrics considered inour experiments.
In Section 4 the correlation be-tween human assessors and metrics is computed,with a discussion of its drawbacks.
In Section 5different quality aspects of metrics are analysed.Conclusions are drawn in the last section.3062 Previous Work on MachineTranslation Meta-EvaluationInsofar as automatic evaluation metrics for ma-chine translation have been proposed, differentmeta-evaluation frameworks have been graduallyintroduced.
For instance, Papineni et al (2001)introduced the BLEU metric and evaluated its re-liability in terms of Pearson correlation with hu-man assessments for adequacy and fluency judge-ments.
With the aim of overcoming some of thedeficiencies of BLEU, Doddington (2002) intro-duced the NIST metric.
Metric reliability wasalso estimated in terms of correlation with humanassessments, but over different document sourcesand for a varying number of references and seg-ment sizes.
Melamed et al (2003) argued, at thetime of introducing the GTM metric, that Pearsoncorrelation coefficients can be affected by scaleproperties, and suggested, in order to avoid thiseffect, to use the non-parametric Spearman corre-lation coefficients instead.Lin and Och (2004) experimented, unlike pre-vious works, with a wide set of metrics, includingNIST, WER (Nie?en et al, 2000), PER (Tillmannet al, 1997), and variants of ROUGE, BLEU andGTM.
They computed both Pearson and Spearmancorrelation, obtaining similar results in both cases.In a different work, Banerjee and Lavie (2005) ar-gued that the measured reliability of metrics canbe due to averaging effects but might not be robustacross translations.
In order to address this issue,they computed the translation-by-translation cor-relation with human judgements (i.e., correlationat the segment level).All that metrics were based on n-gram over-lap.
But there is also extensive research fo-cused on including linguistic knowledge in met-rics (Owczarzak et al, 2006; Reeder et al, 2001;Liu and Gildea, 2005; Amigo?
et al, 2006; Mehayand Brew, 2007; Gime?nez and Ma`rquez, 2007;Owczarzak et al, 2007; Popovic and Ney, 2007;Gime?nez and Ma`rquez, 2008b) among others.
Inall these cases, metrics were also evaluated bymeans of correlation with human judgements.In a different research line, several authorshave suggested approaching automatic evalua-tion through the combination of individual metricscores.
Among the most relevant let us cite re-search by Kulesza and Shieber (2004), Albrechtand Hwa (2007).
But finding optimal metriccombinations requires a meta-evaluation criterion.Most approaches again rely on correlation withhuman judgements.
However, some of them mea-sured the reliability of metric combinations interms of their ability to discriminate between hu-man translations and automatic ones (human like-ness) (Amigo?
et al, 2005).
.In this work, we present a novel approach tometa-evaluation which is distinguished by the useof additional easily interpretable meta-evaluationcriteria oriented to measure different aspects ofmetric reliability.
We then apply this approach tofind out about the advantages and challenges of in-cluding linguistic features in meta-evaluation cri-teria.3 Metrics and Test Beds3.1 Metric SetFor our study, we have compiled a rich set of met-ric variants at three linguistic levels: lexical, syn-tactic, and semantic.
In all cases, translation qual-ity is measured by comparing automatic transla-tions against a set of human references.At the lexical level, we have included severalstandard metrics, based on different similarity as-sumptions: edit distance (WER, PER and TER),lexical precision (BLEU and NIST), lexical recall(ROUGE), and F-measure (GTM and METEOR).
Atthe syntactic level, we have used several familiesof metrics based on dependency parsing (DP) andconstituency trees (CP).
At the semantic level, wehave included three different families which op-erate using named entities (NE), semantic roles(SR), and discourse representations (DR).
A de-tailed description of these metrics can be found in(Gime?nez and Ma`rquez, 2007).Finally, we have also considered ULC, whichis a very simple approach to metric combina-tion based on the unnormalized arithmetic meanof metric scores, as described by Gime?nez andMa`rquez (2008a).
ULC considers a subset of met-rics which operate at several linguistic levels.
Thisapproach has proven very effective in recent eval-uation campaigns.
Metric computation has beencarried out using the IQMT Framework for Auto-matic MT Evaluation (Gime?nez, 2007)1.
The sim-plicity of this approach (with no training of themetric weighting scheme) ensures that the poten-tial advantages detected in our experiments are notdue to overfitting effects.1http://www.lsi.upc.edu/?nlp/IQMT3072004 2005AE CE AE CE#references 5 5 5 4#systemsassessed 5 10 5+1 5#casesassessed 347 447 266 272Table 1: NIST 2004/2005 MT Evaluation Cam-paigns.
Test bed description3.2 Test BedsWe use the test beds from the 2004 and 2005NIST MT Evaluation Campaigns (Le and Przy-bocki, 2005)2.
Both campaigns include two dif-ferent translations exercises: Arabic-to-English(?AE?)
and Chinese-to-English (?CE?).
Human as-sessments of adequacy and fluency, on a 1-5 scale,are available for a subset of sentences, each eval-uated by two different human judges.
A brief nu-merical description of these test beds is availablein Table 1.
The corpus AE05 includes, apart fromfive automatic systems, one human-aided systemthat is only used in our last experiment.4 Correlation with Human Judgements4.1 Correlation at the Segment vs. SystemLevelsLet us first analyze the correlation with humanjudgements for linguistic vs. n-gram based met-rics.
Figure 1 shows the correlation obtained byeach automatic evaluation metric at system level(horizontal axis) versus segment level (verticalaxis) in our test beds.
Linguistic metrics are rep-resented by grey plots, and black plots representmetrics based on n-gram overlap.The most remarkable aspect is that there existsa certain trade-off between correlation at segmentversus system level.
In fact, this graph producesa negative Pearson correlation coefficient betweensystem and segment levels of 0.44.
In other words,depending on how the correlation is computed,the relative predictive power of metrics can swap.Therefore, we need additional meta-evaluation cri-teria in order to clarify the behavior of linguisticmetrics as compared to n-gram based metrics.However, there are some exceptions.
Somemetrics achieve high correlation at both levels.The first one is ULC (the circle in the plot), whichcombines both kind of metrics in a heuristic way(see Section 3.1).
The metric nearest to ULC is2http://www.nist.gov/speech/tests/mtFigure 1: Averaged Pearson correlation at systemvs.
segment level over all test beds.DP-Or-?, which computes lexical overlapping buton dependency relationships.
These results are afirst evidence of the advantages of combining met-rics at several linguistic processing levels.4.2 Drawbacks of Correlation-basedMeta-evaluationAlthough correlation with human judgements isconsidered the standard meta-evaluation criterion,it presents serious drawbacks.
With respect tocorrelation at system level, the main problem isthat the relative performance of different metricschanges almost randomly between testbeds.
Oneof the reasons is that the number of assessed sys-tems per testbed is usually low, and then correla-tion has a small number of samples to be estimatedwith.
Usually, the correlation at system level iscomputed over no more than a few systems.For instance, Table 2 shows the best 10 met-rics in CE05 according to their correlation withhuman judges at the system level, and then theranking they obtain in the AE05 testbed.
Thereare substantial swaps between both rankings.
In-deed, the Pearson correlation of both ranks is only0.26.
This result supports the intuition in (Baner-jee and Lavie, 2005) that correlation at segmentlevel is necessary to ensure the reliability of met-rics in different situations.However, the correlation values of metrics atsegment level have also drawbacks related to theirinterpretability.
Most metrics achieve a Pearsoncoefficient lower than 0.5.
Figure 2 shows twopossible relationships between human and metric308Table 2: Metrics rankings according to correlationwith human judgements using CE05 vs. AE05Figure 2: Human judgements and scores of twohypothetical metrics with Pearson correlation 0.5produced scores.
Both hypothetical metrics A andB would achieve a 0.5 correlation.
In the caseof Metric A, a high score implies a high humanassessed quality, but not the reverse.
This is thetendency hypothesized by Culy and Riehemann(2003).
In the case of Metric B, the high scoredtranslations can achieve both low or high qualityaccording to human judges but low scores ensurelow quality.
Therefore, the same Pearson coeffi-cient may hide very different behaviours.
In thiswork, we tackle these drawbacks by defining morespecific meta-evaluation criteria.5 Alternatives to Correlation-basedMeta-evaluationWe have seen that correlation with human judge-ments has serious limitations for metric evalua-tion.
Therefore, we have focused on other aspectsof metric reliability that have revealed differencesbetween n-gram and linguistic based metrics:1.
Is the metric able to accurately reveal im-provements between two systems?2.
Can we trust the metric when it says that atranslation is very good or very bad?Figure 3: SIP versus SIR3.
Are metrics able to identify good translationswhich are dissimilar from the models?We now discuss each of these aspects sepa-rately.5.1 Ability of metrics to Reveal SystemImprovementsWe now investigate to what extent a significantsystem improvement according to the metric im-plies a significant improvement according to hu-man assessors, and viceversa.
In other words: arethe metrics able to detect any quality improve-ment?
Is a metric score improvement a strong ev-idence of quality increase?
Knowing that a metrichas a 0.8 Pearson correlation at the system level or0.5 at the segment level does not provide a directanswer to this question.In order to tackle this issue, we compare met-rics versus human assessments in terms of pre-cision and recall over statistically significant im-provements within all system pairs in the testbeds.
First, Table 3 shows the amount of signif-icant improvements over human judgements ac-cording to the Wilcoxon statistical significant test(?
?
0.025).
For instance, the testbed CE2004consists of 10 systems, i.e.
45 system pairs; fromthese, in 40 cases (rightmost column) one of thesystems significantly improves the other.Now we would like to know, for every metric, ifthe pairs which are significantly different accord-ing to human judges are also the pairs which aresignificantly different according to the metric.Based on these data, we define two meta-metrics: Significant Improvement Precision (SIP)and Significant Improvement Recall (SIR).
SIP309Systems System pairs Sig.
imp.CE2004 10 45 40AE2004 5 10 8CE2005 5 10 4AE2005 5 10 6Total 25 75 58Table 3: System pairs with a significant differenceaccording to human judgements (Wilcoxon test)(precision) represents the reliability of improve-ments detected by metrics.
SIR (recall) representsto what extent the metric is able to cover the sig-nificant improvements detected by humans.
LetIh be the set of significant improvements detectedby human assessors and Im the set detected by themetric m. Then:SIP =|Ih ?
Im||Im|SIR =|Ih ?
Im||Ih|Figure 3 shows the SIR and SIP values obtainedfor each metric.
Linguistic metrics achieve higherprecision values but at the cost of an important re-call decrease.
Given that linguistic metrics requirematching translation with references at additionallinguistic levels, the significant improvements de-tected are more reliable (higher precision or SIP),but at the cost of recall over real significant im-provements (lower SIR).This result supports the behaviour predicted in(Gime?nez and Ma`rquez, 2009).
Although linguis-tic metrics were motivated by the idea of model-ing linguistic variability, the practical effect is thatcurrent linguistic metrics introduce additional re-strictions (such as dependency tree overlap, for in-stance) for accepting automatic translations.
Thenthey reward precision at the cost of recall in theevaluation process, and this explains the high cor-relation with human judgements at system levelwith respect to segment level.All n-gram based metrics achieve SIP and SIRvalues between 0.8 and 0.9.
This result suggeststhat n-gram based metrics are reasonably reliablefor this purpose.
Note that the combined met-ric, ULC (the circle in the figure), achieves re-sults comparable to n-gram based metrics withthis test3.
That is, combining linguistic and n-gram based metrics preserves the good behaviorof n-gram based metrics in this test.3Notice that we just have 75 significant improvementsamples, so small differences in SIP or SIR have no relevance5.2 Reliability of High and Low MetricScoresThe issue tackled in this section is to what extenta very low or high score according to the metricis reliable for detecting extreme cases (very goodor very bad translations).
In particular, note thatdetecting wrong translations is crucial in order toanalyze the system drawbacks.In order to define an accuracy measure for thereliability of very low/high metric scores, it is nec-essary to define quality thresholds for both thehuman assessments and metric scales.
Definingthresholds for manual scores is immediate (e.g.,lower than 4/10).
However, each automatic evalu-ation metric has its own scale properties.
In orderto solve scaling problems we will focus on equiva-lent rank positions: we associate the ith translationaccording to the metric ranking with the qualityvalue manually assigned to the ith translation inthe manual ranking.Being Qh(t) and Qm(t) the human and met-ric assessed quality for the translation t, and beingrankh(t) and rankm(t) the rank of the translationt according to humans and the metric, the normal-ized metric assessed quality is:QNm(t) = Qh(t?
)| (rankh(t?)
= rankm(t))In order to analyze the reliability of metricswhen identifying wrong or high quality transla-tions, we look for contradictory results betweenthe metric and the assessments.
In other words,we look for metric errors in which the quality es-timated by the metric is low (QNm(t) ?
3) but thequality assigned by assessors is high (Qh(t) ?
5)or viceversa (QNm(t) ?
7 and Qh(t) ?
4).The vertical axis in Figure 4 represents the ra-tio of errors in the set of low scored translationsaccording to a given metric.
The horizontal axisrepresents the ratio of errors over the set of highscored translations.
The first observation is thatall metrics are less reliable when they assign lowscores (which corresponds with the situation A de-scribed in Section 4.2).
For instance, the best met-ric erroneously assigns a low score in more than20% of the cases.
In general, the linguistic met-rics do not improve the ability to capture wrongtranslations (horizontal axis in the figure).
How-ever, again, the combining metric ULC achievesthe same reliability as the best n-gram based met-ric.310In order to check the robustness of these results,we computed the correlation of individual metricfailures between test beds, obtaining 0.67 Pearsonfor the lowest correlated test bed pair (AE2004 andCE2005) and 0.88 for the highest correlated pair(AE2004 and CE2004).Figure 4: Counter sample ratio for high vs lowmetric scored translations5.2.1 Analysis of Evaluation SamplesIn order to shed some light on the reasons for theautomatic evaluation failures when assigning lowscores, we have manually analyzed cases in whicha metric score is low but the quality according tohumans is high (QNm ?
3 and Qh ?
7).
Wehave studied 100 sentence evaluation cases fromrepresentatives of each metric family including: 1-PER, BLEU, DP-Or-?, GTM (e = 2), METEORand ROUGEL.
The evaluation cases have been ex-tracted from the four test beds.
We have identifiedfour main (non exclusive) failure causes:Format issues, e.g.
?US ?
vs ?United States?
).Elements such as abbreviations, acronyms or num-bers which do not match the manual translation.Pseudo-synonym terms, e.g.
?US Scheduled theRelease?
vs. ?US set to Release?). )
In most ofthese cases, synonymy can only be identified fromthe discourse context.
Therefore, terminologicalresources (e.g., WordNet) are not enough to tacklethis problem.Non relevant information omissions, e.g.
?Thank you?
vs. ?Thank you very much?
or?dollar?
vs. ?US dollar?)).
The translationsystem obviates some information which, incontext, is not considered crucial by the humanassessors.
This effect is specially important inshort sentences.Incorrect structures that change the meaningwhile maintaining the same idea (e.g., ?BushPraises NASA ?s Mars Mission?
vs ?
Bush praisesnasa of Mars mission?
).Note that all of these kinds of failure - exceptformatting issues - require deep linguistic process-ing while n-gram overlap or even synonyms ex-tracted from a standard ontology are not enough todeal with them.
This conclusion motivates the in-corporation of linguistic processing into automaticevaluation metrics.5.3 Ability to Deal with Translations that areDissimilar to References.The results presented in Section 5.2 indicate that ahigh score in metrics tends to be highly related totruly good translations.
This is due to the fact thata high word overlapping with human references isa reliable evidence of quality.
However, in somecases the translations to be evaluated are not sosimilar to human references.An example of this appears in the test bedNIST05AE which includes a human-aided sys-tem, LinearB (Callison-Burch, 2005).
This systemproduces correct translations whose words do notnecessarily overlap with references.
On the otherhand, a statistics based system tends to produceincorrect translations with a high level of lexicaloverlapping with the set of human references.
Thiscase was reported by Callison-Burch et al (2006)and later studied by Gime?nez and Ma`rquez (2007).They found out that lexical metrics fail to pro-duce reliable evaluation scores.
They favor sys-tems which share the expected reference sublan-guage (e.g., statistical) and penalize those whichdo not (e.g., LinearB).We can find in our test bed many instances inwhich the statistical systems obtain a metric scoresimilar to the assisted system while achieving alower mark according to human assessors.
For in-stance, for the following translations, ROUGELassigns a slightly higher score to the output of astatistical system which contains a lot of grammat-ical and syntactical failures.Human assisted system: The Chinese President made un-precedented criticism of the leaders of Hong Kong afterpolitical failings in the former British colony on Mon-day .
Human assessment=8.5.Statistical system: Chinese President Hu Jintao today un-precedented criticism to the leaders of Hong Kongwake political and financial failure in the formerBritish colony.
Human assessment=3.311Figure 5: Maximum translation quality decreasingover similarly scored translation pairs.In order to check the metric resistance to becheated by translations with high lexical over-lapping, we estimate the quality decrease thatwe could cause if we optimized the human-aidedtranslations according to the automatic metric.
Forthis, we consider in each translation case c, theworse automatic translation t that equals or im-proves the human-aided translation th accordingto the automatic metric m. Formally the averagedquality decrease is:Quality decrease(m) =Avgc(maxt(Qh(th)?Qh(t)|Qm(th) ?
Qm(t)))Figure 5 illustrates the results obtained.
Allmetrics are suitable to be cheated, assigning sim-ilar or higher scores to worse translations.
How-ever, linguistic metrics are more resistant.
In addi-tion, the combined metric ULC obtains the best re-sults, better than both linguistic and n-gram basedmetrics.
Our conclusion is that including higherlinguistic levels in metrics is relevant to preventungrammatical n-gram matching to achieve simi-lar scores than grammatical constructions.5.4 The Oracle System TestIn order to obtain additional evidence about theusefulness of combining evaluation metrics at dif-ferent processing levels, let us consider the follow-ing situation: given a set of reference translationswe want to train a combined system that takesthe most appropriate translation approach for eachtext segment.
We consider the set of translationssystem presented in each competition as the trans-lation approaches pool.
Then, the upper bound onthe quality of the combined system is given by theMetric OSTmaxOST 6.72ULC 5.79ROUGEW 5.71DP-Or-?
5.70CP-Oc-?
5.70NIST 5.70randOST 5.20minOST 3.67Table 4: Metrics ranked according to the OracleSystem Testpredictive power of the employed automatic eval-uation metric.
This upper bound is obtained by se-lecting the highest scored translation t accordingto a specific metric m for each translation case c.The Oracle System Test (OST) consists of com-puting the averaged human assessed quality Qhof the selected translations according to human as-sessors across all cases.
Formally:OST(m) = Avgc(Qh(Argmaxt(Qm(t))|t ?
c))We use the sum of adequacy and fluency, bothin a 1-5 scale, as a global quality measure.
Thus,OST scores are in a 2-10 range.
In summary,the OST represents the best combined system thatcould be trained according to a specific automaticevaluation metric.Table 4 shows OST values obtained for the bestmetrics.
In the table we have also included a ran-dom, a maximum (always pick the best transla-tion according to humans) and a minimum (al-ways pick the worse translation according to hu-man) OST for all 4.
The most remarkable resultin Table 4 is that metrics are closer to the randombaseline than to the upperbound (maximum OST).This result confirms the idea that an improvementon metric reliability could contribute considerablyto the systems optimization process.
However, thekey point is that the combined metric, ULC, im-proves all the others (5.79 vs. 5.71), indicatingthe importance of combining n-gram and linguis-tic features.6 ConclusionsOur experiments show that, on one hand, tradi-tional n-gram based metrics are more or equally4In all our experiments, the meta-metric values are com-puted over each test bed independently before averaging inorder to assign equal relevance to the four possible contexts(test beds)312reliable for estimating the translation quality at thesegment level, for predicting significant improve-ment between systems and for detecting poor andexcellent translations.On the other hand, linguistically motivated met-rics improve n-gram metrics in two ways: (i) theyachieve higher correlation with human judgementsat system level and (ii) they are more resistant toreward poor translations with high word overlap-ping with references.The underlying phenomenon is that, ratherthan managing the linguistics variability, linguis-tic based metrics introduce additional restrictionsfor assigning high scores.
This effect decreasesthe recall over significant system improvementsachieved by n-gram based metrics and does notsolve the problem of detecting wrong translations.Linguistic metrics, however, are more difficult tocheat.In general, the greatest pitfall of metrics is thelow reliability of low metric values.
Our qualita-tive analysis of evaluated sentences has shown thatdeeper linguistic techniques are necessary to over-come the important surface differences betweenacceptable automatic translations and human ref-erences.But our key finding is that combining both kindsof metrics gives top performance according to ev-ery meta-evaluation criteria.
In addition, our Com-bined System Test shows that, when training acombined translation system, using metrics at sev-eral linguistic processing levels improves substan-tially the use of individual metrics.In summary, our results motivate: (i) work-ing on new linguistic metrics for overcoming thebarrier of linguistic variability and (ii) perform-ing new metric combining schemes based on lin-ear regression over human judgements (Kuleszaand Shieber, 2004), training models over hu-man/machine discrimination (Albrecht and Hwa,2007) or non parametric methods based on refer-ence to reference distances (Amigo?
et al, 2005).AcknowledgmentsThis work has been partially supported by theSpanish Government, project INES/Text-Mess.We are indebted to the three ACL anonymous re-viewers which provided detailed suggestions toimprove our work.ReferencesJoshua Albrecht and Rebecca Hwa.
2007.
Regressionfor Sentence-Level MT Evaluation with Pseudo Ref-erences.
In Proceedings of the 45th Annual Meet-ing of the Association for Computational Linguistics(ACL), pages 296?303.Enrique Amigo?, Julio Gonzalo, Anselmo Pe nas, andFelisa Verdejo.
2005.
QARLA: a Framework forthe Evaluation of Automatic Summarization.
InProceedings of the 43rd Annual Meeting of the Asso-ciation for Computational Linguistics (ACL), pages280?289.Enrique Amigo?, Jesu?s Gime?nez, Julio Gonzalo, andLlu?
?s Ma`rquez.
2006.
MT Evaluation: Human-Like vs. Human Acceptable.
In Proceedings ofthe Joint 21st International Conference on Com-putational Linguistics and the 44th Annual Meet-ing of the Association for Computational Linguistics(COLING-ACL), pages 17?24.Satanjeev Banerjee and Alon Lavie.
2005.
METEOR:An Automatic Metric for MT Evaluation with Im-proved Correlation with Human Judgments.
In Pro-ceedings of ACL Workshop on Intrinsic and Extrin-sic Evaluation Measures for MT and/or Summariza-tion.Chris Callison-Burch, Miles Osborne, and PhilippKoehn.
2006.
Re-evaluating the Role of BLEU inMachine Translation Research.
In Proceedings of11th Conference of the European Chapter of the As-sociation for Computational Linguistics (EACL).Chris Callison-Burch.
2005.
Linear B system descrip-tion for the 2005 NIST MT evaluation exercise.
InProceedings of the NIST 2005 Machine TranslationEvaluation Workshop.Christopher Culy and Susanne Z. Riehemann.
2003.The Limits of N-gram Translation Evaluation Met-rics.
In Proceedings of MT-SUMMIT IX, pages 1?8.George Doddington.
2002.
Automatic Evaluationof Machine Translation Quality Using N-gram Co-Occurrence Statistics.
In Proceedings of the 2nd In-ternational Conference on Human Language Tech-nology, pages 138?145.Jesu?s Gime?nez and Llu?
?s Ma`rquez.
2007.
Linguis-tic Features for Automatic Evaluation of Heteroge-neous MT Systems.
In Proceedings of the ACLWorkshop on Statistical Machine Translation, pages256?264.Jesu?s Gime?nez and Llu?
?s Ma`rquez.
2008a.
Hetero-geneous Automatic MT Evaluation Through Non-Parametric Metric Combinations.
In Proceedings ofthe Third International Joint Conference on NaturalLanguage Processing (IJCNLP), pages 319?326.Jesu?s Gime?nez and Llu?
?s Ma`rquez.
2008b.
On the Ro-bustness of Linguistic Features for Automatic MTEvaluation.
(Under submission).313Jesu?s Gime?nez and Llu?
?s Ma`rquez.
2009.
On the Ro-bustness of Syntactic and Semantic Features for Au-tomatic MT Evaluation.
In Proceedings of the 4thWorkshop on Statistical Machine Translation (EACL2009).Jesu?s Gime?nez.
2007.
IQMT v 2.0.
Technical Manual(LSI-07-29-R).
Technical report, TALP ResearchCenter.
LSI Department.
http://www.lsi.upc.edu/?nlp/IQMT/IQMT.v2.1.pdf.Alex Kulesza and Stuart M. Shieber.
2004.
A learn-ing approach to improving sentence-level MT evalu-ation.
In Proceedings of the 10th International Con-ference on Theoretical and Methodological Issues inMachine Translation (TMI), pages 75?84.Audrey Le and Mark Przybocki.
2005.
NIST 2005 ma-chine translation evaluation official results.
In Offi-cial release of automatic evaluation scores for allsubmissions, August.Chin-Yew Lin and Franz Josef Och.
2004.
AutomaticEvaluation of Machine Translation Quality UsingLongest Common Subsequence and Skip-BigramStatics.
In Proceedings of the 42nd Annual Meet-ing of the Association for Computational Linguistics(ACL).Ding Liu and Daniel Gildea.
2005.
Syntactic Featuresfor Evaluation of Machine Translation.
In Proceed-ings of ACL Workshop on Intrinsic and ExtrinsicEvaluation Measures for MT and/or Summarization,pages 25?32.Dennis Mehay and Chris Brew.
2007.
BLEUATRE:Flattening Syntactic Dependencies for MT Evalu-ation.
In Proceedings of the 11th Conference onTheoretical and Methodological Issues in MachineTranslation (TMI).I.
Dan Melamed, Ryan Green, and Joseph P. Turian.2003.
Precision and Recall of Machine Transla-tion.
In Proceedings of the Joint Conference on Hu-man Language Technology and the North AmericanChapter of the Association for Computational Lin-guistics (HLT-NAACL).Sonja Nie?en, Franz Josef Och, Gregor Leusch, andHermann Ney.
2000.
An Evaluation Tool for Ma-chine Translation: Fast Evaluation for MT Research.In Proceedings of the 2nd International Conferenceon Language Resources and Evaluation (LREC).Karolina Owczarzak, Declan Groves, Josef Van Gen-abith, and Andy Way.
2006.
Contextual Bitext-Derived Paraphrases in Automatic MT Evaluation.In Proceedings of the 7th Conference of the As-sociation for Machine Translation in the Americas(AMTA), pages 148?155.Karolina Owczarzak, Josef van Genabith, and AndyWay.
2007.
Labelled Dependencies in MachineTranslation Evaluation.
In Proceedings of the ACLWorkshop on Statistical Machine Translation, pages104?111.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2001.
Bleu: a method for automatic eval-uation of machine translation, RC22176.
Technicalreport, IBM T.J. Watson Research Center.Maja Popovic and Hermann Ney.
2007.
Word ErrorRates: Decomposition over POS classes and Appli-cations for Error Analysis.
In Proceedings of theSecond Workshop on Statistical Machine Transla-tion, pages 48?55, Prague, Czech Republic, June.Association for Computational Linguistics.Florence Reeder, Keith Miller, Jennifer Doyon, andJohn White.
2001.
The Naming of Things andthe Confusion of Tongues: an MT Metric.
In Pro-ceedings of the Workshop on MT Evaluation ?Whodid what to whom??
at Machine Translation SummitVIII, pages 55?59.Christoph Tillmann, Stefan Vogel, Hermann Ney,A.
Zubiaga, and H. Sawaf.
1997.
Accelerated DPbased Search for Statistical Translation.
In Proceed-ings of European Conference on Speech Communi-cation and Technology.314
