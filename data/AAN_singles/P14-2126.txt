Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 779?784,Baltimore, Maryland, USA, June 23-25 2014.c?2014 Association for Computational LinguisticsRNN-based Derivation Structure Prediction for SMTFeifei Zhai, Jiajun Zhang, Yu Zhou and Chengqing ZongNational Laboratory of Pattern RecognitionInstitute of Automation, Chinese Academy of Sciences, Beijing, 100190, China{ffzhai, jjzhang, yzhou, cqzong}@nlpr.ia.ac.cnAbstractIn this paper, we propose a novel deriva-tion structure prediction (DSP) modelfor SMT using recursive neural network(RNN).
Within the model, two steps areinvolved: (1) phrase-pair vector represen-tation, to learn vector representations forphrase pairs; (2) derivation structure pre-diction, to generate a bilingual RNN thataims to distinguish good derivation struc-tures from bad ones.
Final experimentalresults show that our DSP model can sig-nificantly improve the translation quality.1 IntroductionDerivation structure is important for SMT decod-ing, especially for the translation model basedon nested structures of languages, such as BTG(bracket transduction grammar) model (Wu, 1997;Xiong et al, 2006), hierarchical phrase-basedmodel (Chiang, 2007), and syntax-based model(Galley et al, 2006; Marcu et al, 2006; Liu etal., 2006; Huang et al, 2006; Zhang et al, 2008;Zhang et al, 2011; Zhai et al, 2013).
In general,derivation structure refers to the tuple that recordsthe used translation rules and their compositionsduring decoding, just as Figure 1 shows.Intuitively, a good derivation structure usuallyyields a good translation, while bad derivations al-ways result in bad translations.
For example inFigure 1, (a) and (b) are two different derivationsfor Chinese sentence ???
?
?9 ?1?!?.
Comparing the two derivations, (a) is morereasonable and yields a better translation.
How-ever, (b) wrongly translates phrase ??
?9?
to?and Sharon?
and combines it with [??
;Bush]incorrectly, leading to a bad translation.To explore the derivation structure?s potentialon yielding good translations, in this paper, wepropose a novel derivation structure prediction(DSP) model for SMT decoding.
(a) (b)??Bush???
?
?held a talk?
?
?with Sharon??
?
?
?held a talk?
?
?with Sharon??Bush??
?
?
?held a talk?
?
?with Sharon?
?BushBush and?
??Sharon????Bush???
?
?held a talk?
?
?and Sharon?
?
?and Sharon??
?
?
?held a talkFigure 1: Two different derivation structures ofBTG translation model.
In the structure, leafnodes denote the used translation rules.
For eachnode, the first line is the source string, while thesecond line is its corresponding translation.The proposed DSP model is built on recur-sive neural network (RNN).
Within the model,two steps are involved: (1) phrase-pair vectorrepresentation, to learn vector representations forphrase pairs; (2) derivation structure prediction,to build a bilingual RNN that aims to distinguishgood derivation structures from bad ones.
Ex-tensive experiments show that the proposed DSPmodel significantly improves the translation qual-ity, and thus verify the effectiveness of derivationstructure on indicating good translations.We make the following contributions in thiswork:?
We propose a novel RNN-based model to doderivation structure prediction for SMT de-coding.
To our best knowledge, this is thefirst work on this issue in SMT community;?
In current work, RNN has only been verifiedto be useful on monolingual structure learn-ing (Socher et al, 2011a; Socher et al, 2013).We go a step further, and design a bilingualRNN to represent the derivation structure;?
To train the RNN-based DSP model, we pro-pose a max-margin objective that prefers goldderivations yielded by forced decoding ton-best derivations generated by the conven-tional BTG translation model.7792 The DSP ModelThe basic idea of DSP model is to represent thederivation structure by RNN (Figure 2).
Here, webuild the DSP model for BTG translation model,which is naturally compatible with RNN.
We be-lieve that the DSP model is also beneficial to othertranslation models.
We leave them as our futurework.2.1 Phrase-Pair Vector RepresentationPhrase pairs, i.e., the used translation rules, are theleaf nodes of derivation structure.
Hence, to repre-sent the derivation structure by RNN, we need firstto represent the phrase pairs.
To do this, we usetwo unsupervised recursive autoencoders (RAE)(Socher et al, 2011b), one for the source phraseand the other for the target phrase.
We call the unitof the two RAEs the Leaf Node Network (LNN).Using n-dimension word embedding, RAE canlearn a n-dimension vector for any phrase.
Mean-while, RAE will build a binary tree for the phrase,as Figure 2 (in box) shows, and compute a re-construction error to evaluate the vector.
We useE(Tph) to denote the reconstruction error given byRAE, where ph is the phrase and Tphis the corre-sponding binary tree.
In RAE, higher error corre-sponds to worse vector.
More details can be foundin (Socher et al, 2011b).Given a phrase pair (sp, tp), we can use LNNto generate two n-dimension vectors, representingsp and tp respectively.
Then, we concatenate thetwo vectors directly, and get a vector r ?
R2ntorepresent phrase pair (sp, tp) (shown in Figure2).
The vector r is evaluated by combining thereconstruction error on both sides:E(Tsp, Ttp) =12[E(Tsp) + E(Ttp) ?NsNt](1)where Tspand Ttpare the binary trees for sp andtp.
Nsand Ntdenote the number of nodes in Tspand Ttp.
Note that in order to unify the errors onthe two sides, we use ratio Ns/Ntto eliminate theinfluence of phrase length.Then, according to Equation (1), we computean LNN score to evaluate the vector of all phrasepairs, i.e., leaf nodes, in derivation d:LNN(d) = ??
(sp,tp)E(Tsp, Ttp)(2)where (sp, tp) is the used phrase pair in derivationd.
Obviously, the derivation with better phrase-pair representations will get a higher LNN score.???
??
with Sharon??
?
??
held a talkBushFigure 2: Illustration of DSP model, based on thederivation structure in Figure 1(a).The LNN score will serve as part of the DSPmodel for predicting good derivation structures.2.2 Derivation Structure PredictionUsing the vector representations of phrase pairs,we then build a Derivation Structure Network(DSN) for prediction (Figure 2).In DSN, the derivation structure is repre-sented by repeatedly applying unit neural net-work (UNN, Figure 3) at each non-leaf node.
TheUNN receives two node vectors r1?
R2nandr2?
R2nas input, and induces a vector p ?
R2nto represent the parent node.r1 r2pscoreFigure 3: The unit neural network used in DSN.For example, in Figure 2, node [?
?9; withSharon] serves as the first child with vector r1,and node [?1?!
; held a talk] as the secondchild with vector r2.
The parent node vector p,representing [?
?9 ?1?!
; held a talkwith Sharon], is computed by merging r1and r2:p = f(WUNN[r1; r2] + bUNN) (3)where [r1; r2] ?
R4n?1is the concatenation of r1and r2, WUNN?
R2n?4nand bUNN?
R2n?1arethe network?s parameter weight matrix and biasterm respectively.
We use tanh(?)
as function f .Then, we compute a local score using a simpleinner product with a row vector WscoreUNN?
R1?2n:s(p) = WscoreUNN?
p (4)The score measures how well the two child nodesr1and r2are merged into the parent node p.As we all know, in BTG derivations, we havetwo different ways to merge translation candi-dates, monotone or inverted, meaning that we780merge two candidates in a monotone or invertedorder.
We believe that different merging or-der (monotone or inverted) needs different UNN.Hence, we keep two different ones in DSN, one formonotone order (with parameter Wmono, bmono,and Wscoremono), and the other for inverted (with pa-rameter Winv, binv, and Wscoreinv).
The idea is thatthe merging order of the two candidates will de-termine which UNN will be used to generate theirparent?s vector and compute the score in Equa-tion (4).
Using a set of gold derivations, we cantrain the network so that correct order will receivea high score by Equation (4) and incorrect one willreceive a low score.Thus, when we merge the candidates of two ad-jacent spans during BTG-based decoding, the lo-cal score in Equation (4) is useful in two aspects:(1) for the same merging order, it evaluates howwell the two candidates are merged; (2) for the dif-ferent order, it compares the candidates generatedby monotone order and inverted order.Further, to assess the entire derivation structure,we apply UNN to each node recursively, until theroot node.
The final score utilized for derivationstructure prediction is the sum of all local scores:DSN(d) =?ps(p) (5)where d denotes the derivation structure and p isthe non-leaf node in d. Obviously, by this score,we can easily assess different derivations.
Goodderivations will get higher scores while bad oneswill get lower scores.Li et al (2013) presented a network to predicthow to merge translation candidates, in monotoneor inverted order.
Our DSN differs from Li?s workin two points.
For one thing, DSN can not onlypredict how to merge candidates, but also evaluatewhether two candidates should be merged.
For an-other, DSN focuses on the entire derivation struc-ture, rather than only the two candidates for merg-ing.
Therefore, the translation decoder will pursuegood derivation structures via DSN.
Actually, Li?swork can be easily integrated into our work.
Weleave it as our future work.3 TrainingIn this section, we present the method of trainingthe DSP model.
The parameters involved in thisprocess include: word embedding, parameters ofthe two unsupervised RAEs in LNN, and parame-ters in DSN.3.1 Max-Margin FrameworkIn DSP model, our goal is to assign higher scoresto gold derivations, and lower scores to bad ones.To reach this goal, we adopt a max-margin frame-work (Socher et al, 2010; Socher et al, 2011a;Socher et al, 2013) for training.Specifically, suppose we have a training datalike (ui,G(ui),A(ui)), where uiis the inputsource sentence, G(ui) is the gold derivation setcontaining all gold derivations of ui1, and A(ui)is the possible derivation set that contains allpossible derivations of ui.
We want to minimizethe following regularized risk function:J(?)
=1NN?i=1Ri(?)
+?2?
?
?2, whereRi(?)
= max?d?A(ui)(s(?, ui,?d)+ ?(?d,G(ui)))?
maxd?G(ui)(s(?, ui, d))(6)Here, ?
is the model parameter.
s(?, ui, d) is theDSP score for sentence ui?s derivation d. It iscomputed by summing LNN score (Equation (2))and DSN score (Equation (5)):s(?, u, d) = LNN?
(d) +DSN?
(d) (7)?
(?d,G(ui)) is the structure loss margin, whichpenalizes derivation?d more if it deviates morefrom gold derivations.
It is formulated as:?(?d,G(ui))=?pi??d?s?
{pi 6?
G(ui)}+ ?tDist(y(?d), ref)(8)The margin includes two parts.
For the first part,pi is the source span in derivation?d, ?
{?}
is anindicator function.
We use the first part to countthe number of source spans in derivation?d, butnot in gold derivations.
The second part is fortarget side.
Dist(y(?d), ref) computes the edit-distance between the translation result y(?d) de-fined by derivation?d and the reference translationref .
Obviously, this margin can effectively esti-mate the difference between derivation?d and goldderivations, both on source side and target side.Note that ?sand ?tare only two hyperparametersfor scaling.
They are independent of each other,and we set ?s= 0.1 and ?t= 0.1 respectively.1We investigate the general case here and suppose thatone sentence could have several different gold derivations.Inthe experiment, we only use one gold derivation for simpleimplementation.7813.2 LearningAs the risk function, Equation (6) is not differ-entiable.
We train the model via the subgradientmethod (Ratliff et al, 2007; Socher et al, 2013).For parameter ?, the subgriadient of J(?)
is:?J?
?=1N?i?s(?, ui,?dm)???
?s(?, ui, dm)??+?
?where?dmis the derivation with the highest DSPscore, and dmdenotes the gold derivation with thehighest DSP score.
We adopt the diagonal vari-ant of AdaGrad (Duchi et al, 2011; Socher et al,2013) to minimize the risk function for training.3.3 Training Instances CollectionIn order to train the model, we need to collect thegold derivation set G(ui) and possible derivationset A(ui) for input sentence ui.For G(ui) , we define it by force decodingderivation (FDD).
Basically, FDD refers to thederivation that produces the exact reference trans-lation (single reference in our training data).
Forexample, since ?Bush held a talk with Sharon?
isthe reference of test sentence ???
?
?9 ?1?!
?, then Figure 1(a) is one of the FDDs.As FDD can produce reference translation, we be-lieve that FDD is of high quality, and take them asgold derivations for training.For A(ui), it should contain all possible deriva-tions of ui.
However, it is too difficult to obtainall derivations.
Thus, we use n-best derivations ofSMT decoding to simulate the complete derivationspace, and take them as the derivations in A(ui).4 Integrating the DSP Model into SMTTo integrate the DSP model into decoding, we takeit (named DSP feature) as one of the features in thelog-linear framework of SMT.
During decoding,the DSP feature is distributed to each node in thederivation structure.
For the leaf node, the scorein Equation (2), i.e., LNN score, serves as the fea-ture.
For the non-leaf node, Equation (4) playsthe role.
In order to give positive feature value tothe log-linear framework (for logarithm), we nor-malize the DSP scores to [0,1] during decoding.Due to the length limit, we ignore the specific nor-malization methods here.
We just preform somesimple transformations (such as adding a constant,computing reciprocal), and convert the scores pro-portionally to [0,1] at last.5 Experiments5.1 Experimental SetupTo verify the effectiveness of our DSP model, weperform experiments on Chinese-to-English trans-lation.
The training data contains about 2.1M sen-tence pairs with about 27.7M Chinese words and31.9M English words2.
We train a 5-gram lan-guage model by the Xinhua portion of Gigawordcorpus and the English part of the training data.We obtain word alignment by GIZA++, and adoptthe grow-diag-final-and strategy to generate thesymmetric alignment.
We use NIST MT 2003 dataas the development set, and NIST MT04-083asthe test set.
We use MERT (Och, 2004) to tune pa-rameters.
The translation quality is evaluated bycase-insensitive BLEU-4 (Papineni et al, 2002).The statistical significance test is performed bythe re-sampling approach (Koehn, 2004).
Thebaseline system is our in-house BTG system (Wu,1997; Xiong et al, 2006; Zhang and Zong, 2009).To train the DSP model, we first use Word2Vec4toolkit to pre-train the word embedding on large-scale monolingual data.
The used monolingualdata contains about 1.06B words for Chinese and1.12B words for English.
The dimensionality ofour vectors is 50.
The detiled training process isas follows:(1) Using the BTG system to perform force de-coding on FBIS part of the bilingual training data5,and collect the sentences succeeded in force de-coding (86,902 sentences in total)6.
We then col-lect the corresponding force decoding derivationsas gold derivations.
Here, we only use the bestforce decoding derivation for simple implementa-tion.
In future, we will try to use multiple forcedecoding derivations for training.
(2) Collecting the bilingual phrases in the leafnodes of gold derivations.
We train LNN by thesephrases via L-BFGS algorithm.
Finally, we get351,448 source phrases to train the source sideRAE and 370,948 target phrases to train the tar-get side RAE.2LDC category number : LDC2000T50, LDC2002E18,LDC2003E07, LDC2004T07, LDC2005T06, LDC2002L27,LDC2005T10 and LDC2005T34.3For MT06 and MT08, we only use the part of news data.4https://code.google.com/p/word2vec/5Here we only use the high quality corpus FBIS to guar-antee the quality of force decoding derivation.6Many sentence pairs fail in forced decoding due to manyreasons, such as reordering limit, noisy alignment, and phraselength limit (Yu et al, 2013).782(3) Decoding the 86902 sentences by the BTGsystem to get n-best translations and correspond-ing derivations.
The n-best derivations are used tosimulate the entire derivation space.
We retain atmost 200-best derivations for each sentence.
(4) Leveraging force decoding derivations andn-best derivations to train the DSP model.
Notethat all parameters, including word embedding andparameters in LNN and DSN, are tuned together inthis step.
It takes about 15 hours to train the entirenetwork using a 16-core, 2.9 GHz Xeon machine.5.2 Experimental ResultsWe compare baseline BTG system and the DSP-augmented BTG system in this section.
The finaltranslation results are shown in Table 1.After integrating the DSP model into BTG sys-tem, we get significant improvement on all testsets, about 1.0 BLEU points over BTG system onaverage.
This comparison strongly demonstratesthat our DSP model is useful and will be a goodcomplement to current translation models.SystemsBLEU(%)MT04 MT05 MT06 MT08 AverBTG 36.91 34.69 33.83 27.17 33.15BTG+DSP 37.41 35.77 35.08 28.42 34.17Table 1: Final translation results.
Bold numbersdenote that the result is significantly better thanbaseline BTG system (p < 0.05).
Column ?Aver?gives the average BLEU points of the 4 test sets.To have a better intuition for the effectivenessof our DSP model, we give a case study in Figure4.
It depicts two derivations built by BTG systemand BTG+DSP system respectively.From Figure 4(b), we can see that BTG systemyields a bad translation due to the bad derivationstructure.
In the figure, BTG system makes threemistakes.
It attaches candidates [??
; achieve-ments], [?
? ; has reached] and [#\?
;singapore] to the big candidate [?U?n?,; cannot be regarded as a natural].
Conse-quently, the noun phrase ?#\?
?
?  ???
is translated separately, rather than as a whole,leading to a bad translation.Differently, the DSP model is designed for pre-dicting good derivations.
In Figure 4(c), the usedtranslation rules are actually similar to Figure 4(b).However, under a better guidance to build goodderivation structure, BTG+DSP system generatesa much better translation result than BTG system.
(c) an example derivation structure generated by the DSP+BTG system?
??
?
has reached??achievements?
?
?
?
?cannot be regarded as a ????
natural?
?
?
??
????
cannot be regarded as a natural??
?
?
?
??
????
achievements cannot be regarded as a natural?
??
?
??
?
?
?
??
????
has reached achievements cannot be regarded as a natural???
?
??
?
??
?
?
?
??
????
singapore has reached  achievements cannot be regarded as a natural???singapore?
??
?attained by ?
?
?
?
?cannot be regarded as a ????
natural?
?
?
??
???
?cannot be regarded as a natural???
?
??
?
??
?
?
?
??
????
the achievements attained by singapore cannot be regarded as a natural???singapore???
?
??
?attained by singapore?
?the achievements???
?
??
?
?
?the achievements attained by singapore(b) an example derivation structure generated by BTG system??
?x inj iap o  ?
??
?suo dadao de ?
?c heng j iu ?
?
?
?
?bu neng bei dangzuo ????
lisuodangransing ap ore reac hed the ac hiev em ents  c annot b e taken f or g ranted(a) the example test sentence and its corresponding reference.Figure 4: Different derivation structures.6 ConclusionIn this paper, we explored the method of derivationstructure prediction for SMT.
To fulfill this task,we have made several major efforts as follows:(1) We propose a novel derivation structure pre-diction model based on RNN, including two closeand interactive parts: LNN and DSN.
(2) We extend monolingual RNN to bilingualRNN to represent the derivation structure.
(3) We train LNN and DSN by derivations fromforce decoding.
In this way, the DSP model learnsa preference to good derivation structures.Experimental results show that the proposedDSP model improves the translation performancesignificantly.
By this, we verify the effectivenessof derivation structure on indicating good trans-lations.
We believe that our work will shed newlights to SMT decoding.AcknowledgementWe would like to thank the three anonymous re-viewers for their valuable comments and sugges-tions.
The research work has been partially fundedby the Natural Science Foundation of China underGrant No.
61333018 and 61303181, and the KeyProject of Knowledge Innovation Program of Chi-nese Academy of Sciences.783ReferencesDavid Chiang.
2007.
Hierarchical phrase-based trans-lation.
Computational Linguistics, 33(2):201?228.John Duchi, Elad Hazan, and Yoram Singer.
2011.Adaptive subgradient methods for online learningand stochastic optimization.
The Journal of Ma-chine Learning Research, 12:2121?2159.Michel Galley, Jonathan Graehl, Kevin Knight, DanielMarcu, Steve DeNeefe, Wei Wang, and IgnacioThayer.
2006.
Scalable inference and training ofcontext-rich syntactic translation models.
In Pro-ceedings of the 21st International Conference onComputational Linguistics and 44th Annual Meet-ing of the Association for Computational Linguis-tics, pages 961?968, Sydney, Australia, July.
Asso-ciation for Computational Linguistics.Liang Huang, Kevin Knight, and Aravind Joshi.
2006.A syntax-directed translator with extended domainof locality.
In Proceedings of AMTA.Peng Li, Yang Liu, and Maosong Sun.
2013.
Recur-sive autoencoders for itg-based translation.
In Pro-ceedings of the Conference on Empirical Methods inNatural Language Processing.Yang Liu, Qun Liu, and Shouxun Lin.
2006.
Tree-to-string alignment template for statistical machinetranslation.
In Proceedings of the 21st Interna-tional Conference on Computational Linguistics and44th Annual Meeting of the Association for Compu-tational Linguistics, pages 609?616, Sydney, Aus-tralia, July.
Association for Computational Linguis-tics.Daniel Marcu, Wei Wang, Abdessamad Echihabi, andKevin Knight.
2006.
Spmt: Statistical machinetranslation with syntactified target language phrases.In Proceedings of the 2006 Conference on Em-pirical Methods in Natural Language Processing,pages 44?52, Sydney, Australia, July.
Associationfor Computational Linguistics.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
Bleu: a method for automaticevaluation of machine translation.
In Proceedings ofthe 40th annual meeting on association for compu-tational linguistics, pages 311?318.
Association forComputational Linguistics.Nathan D Ratliff, J Andrew Bagnell, and Martin AZinkevich.
2007.
(online) subgradient methodsfor structured prediction.
In Eleventh InternationalConference on Artificial Intelligence and Statistics(AIStats).Richard Socher, Christopher D Manning, and An-drew Y Ng.
2010.
Learning continuous phraserepresentations and syntactic parsing with recursiveneural networks.
In Proceedings of the NIPS-2010Deep Learning and Unsupervised Feature LearningWorkshop.Richard Socher, Cliff C Lin, Chris Manning, and An-drew Y Ng.
2011a.
Parsing natural scenes and nat-ural language with recursive neural networks.
InProceedings of the 28th International Conference onMachine Learning (ICML-11), pages 129?136.Richard Socher, Jeffrey Pennington, Eric H Huang,Andrew Y Ng, and Christopher D Manning.
2011b.Semi-supervised recursive autoencoders for predict-ing sentiment distributions.
In Proceedings of theConference on Empirical Methods in Natural Lan-guage Processing, pages 151?161.Richard Socher, John Bauer, Christopher D Manning,and Andrew Y Ng.
2013.
Parsing with composi-tional vector grammars.
In Proceedings of ACL.Dekai Wu.
1997.
Stochastic inversion transductiongrammars and bilingual parsing of parallel corpora.Computational linguistics, 23(3):377?403.Deyi Xiong, Qun Liu, and Shouxun Lin.
2006.
Maxi-mum entropy based phrase reordering model for sta-tistical machine translation.
In Proceedings of ACL-COLING, pages 505?512.Heng Yu, Liang Huang, Haitao Mi, and Kai Zhao.2013.
Max-violation perceptron and forced decod-ing for scalable MT training.
In Proceedings ofthe 2013 Conference on Empirical Methods in Natu-ral Language Processing, pages 1112?1123, Seattle,Washington, USA, October.
Association for Compu-tational Linguistics.Feifei Zhai, Jiajun Zhang, Yu Zhou, and ChengqingZong.
2013.
Unsupervised tree induction for tree-based translation.
Transactions of Association forComputational Linguistics(TACL), pages 291?300.Jiajun Zhang and Chengqing Zong.
2009.
A frame-work for effectively integrating hard and soft syn-tactic rules into phrase based translation.
In Pro-ceedings of the 23rd Pacific Asia Conference on Lan-guage, Information and Computation, pages 579?588, Hong Kong, December.
City University ofHong Kong.Min Zhang, Hongfei Jiang, Aiti Aw, Haizhou Li,Chew Lim Tan, and Sheng Li.
2008.
A treesequence alignment-based tree-to-tree translationmodel.
In Proceedings of ACL-08: HLT, pages 559?567, Columbus, Ohio, June.
Association for Compu-tational Linguistics.Jiajun Zhang, Feifei Zhai, and Chengqing Zong.
2011.Augmenting string-to-tree translation models withfuzzy use of source-side syntax.
In Proceedings ofthe 2011 Conference on Empirical Methods in Nat-ural Language Processing, pages 204?215, Edin-burgh, Scotland, UK., July.
Association for Compu-tational Linguistics.784
