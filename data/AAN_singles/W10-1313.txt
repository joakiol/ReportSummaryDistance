Proceedings of the NAACL HLT 2010 Workshop on Speech and Language Processing for Assistive Technologies, pages 98?106,Los Angeles, California, June 2010. c?2010 Association for Computational LinguisticsAutomated Skimming in Response to Questions for NonVisual Readers?
?Debra Yarrington Kathleen F. McCoyDept.
of Computer and Information Science Dept.
of Computer and Information ScienceUniversity of Delaware University of DelawareNewark, DE, 19716, USA Newark, DE, 19716, USAyarringt@eecis.udel.edu mccoy@cis.udel.eduAbstractThis paper presents factors in designing a sys-tem for automatically skimming text docu-ments in response to a question.
The systemwill take a potentially complex question and asingle document and return a Web page con-taining links to text related to the question.The goal is that these text areas be those thatvisual readers would spend the most time onwhen skimming for the answer to a question.To identify these areas, we had visual readersskim for an answer to a complex questionwhile being tracked by an eye-tracking sys-tem.
Analysis of these results indicates thattext with semantic connections to the questionare of interest, but these connections are muchlooser than can be identified with traditionalQuestion-Answering or Information Retrievaltechniques.
Instead, we are expanding tradi-tional semantic treatments by using a Websearch.
The goal of this system is to give non-visual readers information similar to what vis-ual readers get when skimming through adocument in response to a question.1 IntroductionThis paper describes semantic considerations indeveloping a system for giving nonvisual readersinformation similar to what visual readers gleanwhen skimming through a document in response toa question.
Our eventual system will be unique inthat it takes both simple and complex questions,will work in an unrestricted domain, will locateanswers within a single document, and will returnnot just an answer to a question, but the informa-tion visual skimmers acquire when skimmingthrough a document.1.1 GoalsProduction of our skimming system will requirethe attainment of three major goals:1.
Achieving an understanding of what informationin the document visual skimmers pay attentionto when skimming in response to a question2.
Developing Natural Language Processing (NLP)techniques to automatically identify areas of textvisual readers focus on as determined in 1.3.
Developing a user interface to be used in con-junction with screen reading software to deliverthe visual skimming experience.In this paper we focus on the first two of thesegoals.
Section 2 will discuss experiments analyzingvisual skimmers skimming for answers to ques-tions.
Section 3 will discuss developing NLP tech-niques to replicate the results of Section 2.
Section4 will discuss future work.1.2 ImpetusThe impetus for this system was work done by theauthor with college students with visual impair-ments who took significantly longer to completehomework problems than their visually readingcounterparts.
Students used both ScreenReaders,which read electronic text aloud, and screen mag-nifiers, which increase the size of text on a screen.While these students were comfortable listening tothe screenreader reading at rates of up to 500words per minute, their experience was quite dif-ferent from their visual-reading peers.
Even afterlistening to an entire chapter, when they wanted toreturn to areas of text that contained text relevantto the answer, they had to start listening from thebeginning and traverse the document again.
Doing98homework was a tedious, time-consuming taskwhich placed these students at a serious disadvan-tage.
It is clear that individuals with visual im-pairments struggle in terms of education.
Bydeveloping a system that levels the playing field inat least one area, we may make it easier for at leastsome individuals to succeed.2 Visual SkimmingIf our intention is to convey to nonvisual readersinformation similar to what visual readers acquirewhen skimming for answers to questions, we firstmust determine what information visual readers getwhen skimming.
For our purposes, we were inter-ested in what text readers focused on in connectionto a question.
While many systems exist that focuson answering simple, fact-based questions, wewere more interested in more complex questions inwhich the answer could not be found using patternmatching and in which the answer would require atleast a few sentences, not necessarily contiguouswithin a document.
From an NLP standpoint, lo-cating longer answers with relevant informationoccuring in more than one place that may or maynot have words or word sequences in common withthe question poses an interesting and difficult prob-lem.
The problem becomes making semantic con-nections within any domain that are more looselyassociated than the synonyms, hypernyms, hypo-nyms, etc.
provided by WordNet (Felbaum, 1998).Indeed, the questions that students had the mostdifficulty with were more complex in nature.
Thuswe needed to find out whether visual skimmerswere able to locate text in documents relevant tocomplex questions and, if so, what connectionsvisual skimmers are making in terms of the textthey choose to focus on.2.1 Task DescriptionTo identify how visual readers skim documents toanswer questions, we collected 14 questions ob-tained from students?
homework assignments,along with an accompanying document per ques-tion from which the answer could be obtained.
Thequestions chosen were on a wide variety of topicsand were complex in nature.
An example of a typi-cal question is, ?According to Piaget, what tech-niques do children use to adjust to theirenvironment as they grow??
Documents largelyconsisted of plain text, although each had a title onthe first page.
They held no images and few sub-titles or other areas users might find visually inter-esting.
Twelve of the documents were two pages inlength, one was eight pages in length, and one wasnine pages long.
In each case, the answer to thequestion was judged by the researchers to be foundwithin a single paragraph in the document.Forty-three visual reading subjects skimmed forthe answer to between 6 ?
13 questions.
The sub-jects sat in front of a computer screen to which theEye Tracker 1750 by Tobii Technologies was in-stalled.
The questions and accompanying docu-ments were displayed on the computer screen and,after being calibrated, subjects were tracked asthey skimmed for the answer.
For the two-pagedocuments, the question appeared at the top of thefirst page.
For the longer documents, the questionappeared at the top of each page.
Subjects had notime limit for skimming and switched pages bypressing the space bar.
When done skimming eachdocument, subjects were asked to select a best an-swer in multiple choice form (to give them a rea-son to take the skimming task seriously).2.2 ResultsResults showed that subjects were reliably able tocorrectly answer the multiple choice question afterskimming the document.
Of the 510 questions, 423(about 86%) were answered correctly.
The twoquestions from longer documents were the leastlikely to be answered correctly (one had 10 correctanswers of 21 total answers, and the other had 10incorrect answers and only one correct answer).Clearly for the shorter documents, subjects wereable to get appropriate information out of the doc-ument to successfully answer the question.
Withthat established, we were interested in analyzingthe eye tracking data to see if there was a connec-tion between where subjects spent the most time inthe document and the question.
If there was an un-derstandable connection, the goal then became toautomatically replicate those connections and thusautomatically locate places in the text where sub-jects were most likely to spend the most time.The Tobii Eye Tracking System tracks the pathand length of time a subject gazes at a particularpoint as a subject skims through a document.
Thesystem allows us to define Areas of Interest (AOIs)and then track the number of prolonged gaze points99within those areas of interest.
For our analysis, wedefined areas of interest as being individual para-graphs.
While we purposely chose documents thatwere predominantly text, each had a title as well.Titles and the few subtitles and lists that occurredin the documents were also defined as separateAOIs.
For each skimming activity, the eye trackingsystem gave us a gaze plot showing the order inwhich individuals focused on particular areas, anda hot spot image showing the gaze points, withduration indicated with color intensity, that oc-curred in each AOI (see Figure 1).In looking at the hot spot images, we found thatsubjects used three techniques to peruse a docu-ment.
One technique subjects used was to movetheir gaze slowly throughout the entire document,indicating that they were most likely reading thedocument.
A second technique used was to moverandomly and quickly from top to bottom of thedocument (described as ?fixations distributed in arough zig-zag down the page?
by McLaughlin inreference to speed reading (1969)), without everfocusing on one particular area for a longer periodof time.
This technique was the least useful to usbecause it gave very little information A thirdtechnique was a combination of the first two, inwhich the subject?s gaze darted quickly and ran-domly around the page, and then appeared to focuson a particular area for an extended period of time.Figure 1 is a good example of this technique.
Thedata from this group was clearly relevant to ourtask since their fixation points clearly showed whatareas subjects found most interesting while skim-ming for an answer to a question.2.3 Analysis of Skimming DataTo determine exactly which AOIs subjects focusedon most frequently, we counted the number of gazepoints (or focus points) in each AOI (defined asparagraphs, titles, subtitles) across all subjects.
Inlooking at what information individuals focused onwhile skimming, we found that individuals did fo-cus on the title and subtitles that occurred in thedocuments.
Subjects frequently focused on the firstparagraph or paragraphs of a document.
There wasless of a tendency, but still a trend for focusing onthe first paragraph on each page.
Interestingly, al-though a few subjects focused on the first line ofeach paragraph, this was not a common practice.This is significant because it is a technique availa-ble to users of screenreaders, yet it clearly does notgive these users the same information that visualskimmers get when skimming through a document.We also wanted to look at AOIs that did nothave physical features that may have attracted at-tention.
Our conjecture was that these AOIs werefocused on by subjects because of their semantic?Figure 1.
Hot spot image results of skimming for the answer to the question, ?What are two dietary factorsthought to raise and lower cholesterol??
using the Tobii Eye Tracking System100relationship to the question.
Indeed, we did findevidence of this.
Results indicated that subjects didfocus on the areas of text containing the answer tothe question.
As an example, one of the questionsused in the study was,?How do people catch the West Nile Vi-rus?
?The paragraph with the most gaze points for themost subjects was:?In the United States, wild birds, especial-ly crows and jays, are the main reservoirof West Nile virus, but the virus is actuallyspread by certain species of mosquitoes.Transmission happens when a mosquitobites a bird infected with the West Nile vi-rus and the virus enters the mosquito'sbloodstream.
It circulates for a few daysbefore settling in the salivary glands.
Thenthe infected mosquito bites an animal or ahuman and the virus enters the host'sbloodstream, where it may cause seriousillness.
The virus then probably multipliesand moves on to the brain, crossing theblood-brain barrier.
Once the viruscrosses that barrier and infects the brainor its linings, the brain tissue becomes in-flamed and symptoms arise.
?This paragraph contains the answer to the ques-tion, yet it has very few words in common with thequestion.
The word it does have in common withthe question, ?West Nile Virus?, is the topic of thedocument and occurs fairly frequently throughoutthe document, and thus cannot account for sub-jects' focusing on this particular paragraph.The subjects must have made semantic connec-tions between the question and the answer thatcannot be explained by simple word matching oreven synonyms, hypernyms and hyponyms.
In theabove example, the ability of the user to locate theanswer hinged on their ability to make a connec-tion between the word ?catch?
in the question andits meaning ?to be infected by?.
Clearly simplekeyword matching won?t suffice in this case, yetequally clearly subjects successfully identified thisparagraph as being relevant to the question.
Thissuggests that when skimming subjects were able tomake the semantic connections necessary to locatequestion answers, even when the answer was of avery different lexical form than the question.Other areas of text focused on also appear tohave a semantic relationship with the question.
Forexample, with the question,?Why was Monet?s work criticized by thepublic?
?the second most frequently focused on paragraphwas:?In 1874, Manet, Degas, Cezanne, Renoir,Pissarro, Sisley and Monet put together anexhibition, which resulted in a large finan-cial loss for Monet and his friends andmarked a return to financial insecurity forMonet.
It was only through the help ofManet that Monet was able to remain inArgenteuil.
In an attempt to recoup someof his losses, Monet tried to sell some ofhis paintings at the Hotel Drouot.
This,too, was a failure.
Despite the financialuncertainty, Monet?s paintings never be-came morose or even all that sombre.
In-stead, Monet immersed himself in the taskof perfecting a style which still had notbeen accepted by the world at large.
Mo-net?s compositions from this time were ex-tremely loosely structured, with colorapplied in strong, distinct strokes as if noreworking of the pigment had been at-tempted.
This technique was calculated tosuggest that the artist had indeed captureda spontaneous impression of nature.
?Of the 30 subjects who skimmed this document,15 focused on this paragraph, making it the secondmost focused on AOI in the document, second onlyto the paragraph that contained the answer (fo-cused on by 21 of the subjects).
The above para-graph occurred within the middle of the secondpage of the document, with no notable physicalattributes that would have attracted attention.
Uponcloser inspection of the paragraph, there are refer-ences to ?financial loss,?
?financial insecurity,??losses,?
?failure,?
and ?financial uncertainty.
?The paragraph also includes ?morose?
and ?somb-er?
and even ?had not been accepted by the worldat large.?
Subjects appeared to be making a con-nection between the question topic, Monet?s workbeing criticized by the public, and the above terms.Intuitively, we do seem to make this connection.Yet the connection being made is not straightfor-ward and cannot be replicated using the direct se-101mantic connections that are available via WordNet.Indeed, the relationships made are more similar toHovy and Lin?s (1997) Concept Signatures createdby clustering words in articles with the same edi-tor-defined classification from the Wall StreetJournal.
Our system must be able to replicate theseconnections automatically.Upon further examination, we found other pa-ragraphs that were focused on by subjects for rea-sons other than their physical appearance orlocation, yet their semantic connection to the ques-tion was even more tenuous.
For instance, whenskimming for the answer to the question,?How does marijuana affect the brain?
?the second most frequently focused on paragraph(second to the paragraph with the answer) was,?The main active chemical in marijuana isTHC (delta-9-tetrahydrocannabinol).
Theprotein receptors in the membranes of cer-tain cells bind to THC.
Once securely inplace, THC kicks off a series of cellularreactions that ultimately lead to the highthat users experience when they smokemarijuana.
?While this paragraph does appear to have loosesemantic connections with the question, the con-nections are less obvious than paragraphs that fol-low it, yet it was this paragraph that subjects choseto focus on.
The paragraph is the third to last para-graph on the first page, so its physical locationcould not explain its attraction to subjects.
Howev-er, when we looked more closely at the previousparagraphs, we saw that the first paragraph dealswith definitions and alternate names for marijuana(with no semantic links to the question), and thesecond and third paragraph deal with statistics onpeople who use marijuana (again, with no semanticconnection to the question).
The fourth paragraph,the one focused on, represents a dramatic semanticshift towards the topic of the question.
Intuitively itmakes sense that individuals skimming through thedocument would pay more attention to this para-graph because it seems to represent the start of thearea that may contain the answer, not to mentionconveying topological information about the layoutof the document and general content informationas well.Data collected from these experiments suggestthat subjects do make and skim for semantic con-nections.
Subjects not only glean information thatdirectly answers the question, but also on contentwithin the document that is semantically related tothe question.
While physical attributes of text doattract the attention of skimmers, and thus we mustinclude methods for accessing this data as well, itis clear that in order to create a successful skim-ming device that conveys information similar towhat visual skimmers get when skimming for theanswer to a question, we must come up with a me-thod for automatically generating loose semanticconnections and then using those semantic connec-tions to locate text skimmers considered relevantwithin the document.3 NLP TechniquesIn order to automatically generate the semanticconnections identified above as being those visualskimmers make, we want to explore Natural Lan-guage Processing (NLP) techniques.3.1 Related ResearchPotentially relevant methodologies may be foundin Open Domain Question Answering Systems.Open Domain Question Answering Systems in-volve connecting questions within any domain andpotential answers.
These systems usually do notrely on external knowledge sources and are limitedin the amount of ontological information that canbe included in the system.
The questions are usual-ly fact-based in form (e.g., ?How tall is Mt.
Ever-est??).
These systems take a question and query apotentially large set of documents (e.g., the WorldWide Web) to find the answer.
A common tech-nique is to determine a question type (e.g., ?Howmany ???
would be classified as ?numerical?,whereas ?Who was ???
would be classified as?person?, etc.)
and then locate answers of the cor-rect type (Abney et al, 2000; Kwok et al, 2001;Srihari and Li, 2000; Galea, 2003).
Questions arealso frequently reformulated for pattern matching(e.g., ?Who was the first American Astronaut inspace??
becomes, ?The first American Astronautin space was?
(Kwok et al, 2001; Brill et al,2002)).
Many systems submit multiple queries to adocument corpus, relying on redundancy of theanswer to handle incorrect answers, poorly con-structed answers or documents that don?t containthe answer (e.g., Brill et al, 2002; Kwok et al,1022001).
For these queries, systems often includesynonyms, hypernyms, hyponyms, etc.
in the queryterms used for document and text retrieval (Hovyet al,2000; Katz et al, 2005).
In an attempt to an-swer more complex relational queries, Banko et al(2007) parsed training data into relational tuplesfor use in classifying text tagged for part of speech,chunked into noun phrases, and then tagged therelations for probability.
Soricut and Brill (2006)trained data on FAQ knowledge bases from theWorld Wide Web, resulting in approximately 1million question-answer pairs.
This system relatedpotential answers to questions using probabilitymodels computed using the FAQ knowledge base.Another area of research that may lend usefultechniques for connecting and retrieving relevanttext to a question is query-biased text summariza-tion.
With many summarization schemes, a gooddeal of effort has been placed on identifying themain topic or topics of the document.
In query bi-ased text summarization, however, the topic isidentified a priori, and the task is to locate relevanttext within a document or set of documents.
Inmultidocument summarization systems, redundan-cy may be indicative of relevance, but should beeliminated from the resulting summary.
Thus aconcern is measuring relevance versus redundancy(Carbonell and Goldstein, 1998; Hovy et al, 2005;Otterbacher et al, 2006).
Like Question Answeringsystems, many summarization systems simplymatch the query terms, expanded to include syn-onyms, hypernyms, hyponyms, etc., to text in thedocument or documents (Varadarajan and Hristi-dis, 2006; Chali, 2002)Our system is unique in that it has as its goal notjust to answer a question or create a summary, butto return information visual skimmers glean whileskimming through a document.
Questions posed tothe system will range from simple to complex innature, and the answer must be found within a sin-gle document, regardless of the form the answertakes.
Questions can be on any topic.
With com-plex questions, it is rarely possible to categorizethe type of question (and thus the expected answertype).
Intuitively, it appears equally useless to at-tempt reformulation of the query for pattern match-ing.
This intuition is born out by Soricut and Brill(2006) who stated that in their study reformulatingcomplex questions more often hurt performancethan improved it.
Answering complex questionswithin a single document when the answer may notbe straightforward in nature poses a challengingproblem.3.2 Baseline ProcessingOur baseline system attempted to identify areas ofinterest by matching against the query in the tradi-tion of Open Domain Question Answering.
For ourbaseline, we used the nonfunction words in eachquestion as our query terms.
The terms wereweighted with a variant of TF/IDF (Salton andBuckley, 1988) in which terms were weighted bythe inverse of the number of paragraphs they oc-curred in within the document.
This weightingscheme was designed to give lower weight towords associated with the document topic and thusconveying less information about relevance to thequestion.
Each query term was matched to text ineach paragraph, and paragraphs were ranked formatching using the summation of, for each queryterm, the number of times it occurred in the para-graph multiplied by its weight.Results of this baseline ranking were poor.
Innone of the 14 documents did this method connectthe question to the text relevant to the answer.
Thiswas expected.
This original set of questions waspurposely chosen because of the complex relation-ship between the question and answer text.Next we expanded the set of query terms to in-clude synonyms, hypernyms, and hyponyms asdefined in WordNet (Felbaum, 1998).
We includedall senses of each word (query term).
Irrelevantsenses resulted in the inclusion of terms that wereno more likely to occur frequently than any otherrandom word, and thus had no effect on the result-ing ranking of paragraphs.
Again, each of thewords in the expanded set of query terms wasweighted as described above, and paragraphs wereranked accordingly.Again, results were poor.
Paragraphs rankedhighly were no more likely to contain the answer,nor were they likely to be areas focused on by thevisual skimmers in our collected skimming data.Clearly, for complex questions, we need to ex-pand on these basic techniques to replicate the se-mantic connections individuals make whenskimming.
As our system must work across a vastarray of domains, our system must make theseconnections ?on the fly?
without relying on pre-viously defined ontological or other general know-ledge.
And our system must work quickly: asking103individuals to wait long periods of time while thesystem creates semantic connections and locatesappropriate areas of text would defeat the purposeof a system designed to save its users time.3.3 Semantically-Related Word ClustersOur solution is to use the World Wide Web to formclusters of topically-related words, with the topicbeing the question.
The cluster of words will beused as query terms and matched to paragraphs asdescribed above for ranking relevant text.Using the World Wide Web as our corpus has anumber of advantages.
Because of the vast numberof documents that make up the World Wide Web,we can rely on the redundancy that has proved souseful for Question Answering and Text Summari-zation systems.
By creating the word clusters fromdocuments returned from a search using questionwords, the words that occur most frequently in therelated document text will most likely be related insome way to the question words.
Even relativelyinfrequently occurring word correlations can mostlikely be found in some document existing on theWeb, and thus strangely-phrased questions orquestions with odd terms will still most likelybring up some documents that can be used to forma cluster.
The Web covers virtually all domains.Somewhere on the Web there is almost certainly ananswer to questions on even the most obscure top-ics.
Thus questions containing words unique touncommon domains or questions containing un-usual word senses will return documents with ap-propriate cluster words.
Finally, the Web isconstantly being updated.
Terms that might nothave existed even a year ago will now be found onthe Web.Our approach is to use the nonstop words in aquestion as query terms for a Web search.
Thesearch engine we are using is Google(www.google.com).
For each search engine query,Google returns an ranked list of URLs it considersrelevant, along with a snippet of text it considersmost relevant to the query (usually because ofwords in the snippet that exactly match the queryterms).
To create the cluster of words related se-mantically to the question, we are taking the top 50URLs, going to their correlating Web page, locat-ing the snippet of text within the page, and creatinga cluster of words using a 100-word window sur-rounding the snippet.
We are using only nonstopwords in the cluster, and weighting the wordsbased on their total number of occurrences in thewindows.
These word clusters, along with the ex-panded baseline words, are used to locate and rankparagraphs in our question document.Our approach is similar in spirit to other re-searchers using the Web to identify semantic rela-tions.
Matsuo et al (2006) looked at the number ofhits of each of two words as a single keyword ver-sus the number of hits using both words as key-words to rate the semantic similarity of two words.Chen et al (2006) used a similar approach to de-termine the semantic similarity between twowords: with a Web search using word P as thequery term, they counted the number of times wordQ occurred in the snippet of text returned, and viceversa.
Bollegala et al (2007) determined semanticrelationships by extracting lexico-syntactic patternsfrom the snippets returned from a search on twokeywords (e.g.,??x?
is a ?y??)
and extracting therelationship of the two words based on the pattern.Sahami and Heilman (2006) used the snippets froma word search to form a set of words weighted us-ing TF/IDF, and then determined the semantic si-milarity of two keywords by the similarity of twoword sets returned in those snippets.Preliminary results from our approach have beenencouraging.
For example, with the question,?How does Marijuana affect the brain?
?, the ex-panded set of keywords included, ?hippocampus,receptors, THC, memory, neuron?.
These wordswere present in both the paragraph containing theanswer and the second-most commonly focused onparagraph in our study.
While neither our baselinenor our expanded baseline identified either para-graph as an area of interest, the semantically-related word clusters did.4 Future WorkThis system is a work in progress.
There are manyfacets still under development, including a fineranalysis of visual skimming data, a refinement ofthe ranking system for locating areas of interestwithin a document, and the development of thesystem?s user interface.4.1 Skimming Data AnalysisFor our initial analysis, we focused on the length oftime users spent gazing at text areas.
In future104analysis, we will look at the order of the gazepoints to determine exactly where the subjects firstgazed before choosing to focus on a particulararea.
This may give us even more informationabout the type of semantic connection subjectsmade before choosing to focus on a particular area.In addition, in our initial analysis, we defined AOIsto be paragraphs.
We may want to look at smallerAOIs.
For example, with longer paragraphs, thetext that actually caught the subject?s eye may haveoccurred only in one portion of the paragraph, yetas the analysis stands now the entire content of theparagraph is considered relevant and thus we aretrying to generate semantic relationships betweenthe question and potentially unrelated text.
Whilethe system only allows us to define AOIs as rec-tangular areas (and thus we can?t do a sentence-by-sentence analysis), we may wish to define AOIs assmall as 2 lines of text to narrow in on exactlywhere subjects chose to focus.4.2 Ranking System RefinementIt is worth mentioning that, while a good deal ofresearch has been done on evaluating the goodnessof automatically generated text summaries (Maniet al,2002; Lin and Hovy, 2003; Santos et al,2004) our system is intended to mimic the actionsof skimmers when answering questions, and thusour measure of goodness will be our system?sability to recreate the retrieval of text focused onby our visual skimmers.
This gives us a distinctadvantage over other systems in measuring good-ness, as defining a measure of goodness can provedifficult.
In future work, we will be exploring dif-ferent methods of ranking text such that the systemreturns results most similar to the results obtainedfrom the visual skimming studies.
The system willthen be used on other questions and documents andcompared to data to be collected of visual skim-mers skimming for answers to those questions.Many variations on the ranking system are poss-ible.
These will be explored to find the bestmatches with our collected visual skimming data.Possibilities include weighting keywords different-ly according to where they came from (e.g., direct-ly from the question, from the text in retrievedWeb pages, from text from a Web page rankedhigh on the returned URL list or lower, etc.
), orconsidering how a diversity of documents mightaffect results.
For instance, if keywords include?falcon?
and ?hawk?
the highest ranking URLs willmost likely be related to birds.
However, in G.I.Joe, there are two characters, Lieutenant Falconand General Hawk.
To get the less common con-nection between falcon and hawk and G.I.
Joe, onemay have to look for diversity in the topics of thereturned URLs.
Another area to be explored willbe the effect of varying the window size surround-ing the snippet of text to form the bag of words.4.3 User InterfaceThe user interface for our system poses some inter-esting questions.
It is important that the output ofthe system provide the user with information about(1) document topology, (2) document semantics,and (3) information most relevant to answering thequestion.
At the same time, it is important that us-ing the output be relatively fast.
The output of thesystem is envisioned as a Web page with rankedlinks at the top pointing to sections of the text like-ly to be relevant to answering the question.An important issue that must be explored indepth with potential users of the system is the ex-act form of the output web page.
We need to ex-plore the best method for indicating text areas ofinterest and the overall topology.
The goal is thatreading the links simulate what a visual skimmergets from lightly skimming.
The user would actual-ly follow the links that appeared to be ?worth read-ing?
in more detail in the same way that skimmersfocus in on particular text segments that appearworth reading.5 ConclusionThis system attempts to correlate NLP techniquesfor creating semantic connections with the seman-tic connections individuals make.
Using the WorldWide Web, we may be able to make those seman-tic connections across any topic in a reasonableamount of time without any previously definedknowledge.
We have ascertained that people canand do make semantic links when skimming foranswers to questions, and we are currently explor-ing the best use of the World Wide Web in repli-cating those connections.
In the long run, weenvision a system that is user-friendly to nonvisualand low vision readers that will give them an intel-ligent way to skim through documents for answersto questions.105ReferencesS.
Abney, M. Collins, and A. Singhal.
2000.
AnswerExtraction.
In Proceedings of ANLP 2000, 296-301.M.
Banko, M. J. Cafarella, S. Soderland, M. Broadhead,and O. Etzioni.
2007.
Open information extractionfrom the Web.
In Proceedings of the 20th Interna-tional Joint Conference on Artificial Intelligence,2670-2676.D.
Bollegala, Y. Matsuo, and M. Ishizuka.
2007.
Mea-suring semantic similarity between words using Websearch engines.
In Proceedings of WWW 2007.
757-766.Brill, E., Lin, J., Banko, M., Domais, S. and Ng, A.2001.
Data-Intensive Question Answering.
In Pro-ceedings of the TREC-10 Conference, NIST, Gai-thersburg, MD, 183-189.J.
Carbonell and J. Goldstein.
1998.
The use of MMR,diversity-based reranking for reordering documentsand producing summaries.
In Proceedings of SIGIR?98, New York, NY, USA, 335-336.Y.
Chali.
2002.
Generic and query-based text summari-zation using lexical cohesion, In Proceedings of theFifteenth Canadian Conference on Artificial Intelli-gence, Calgary, May, 293-303.H.
Chen, M. Lin, and Y. Wei.
2006.
Novel associationmeasures using web search with double checking.
InProceedings of the COLING/ACL 2006.
1009-1016.C.
Felbaum.
1998.
WordNet an Electronic Database,Boston/Cambridge: MIT Press.A.
Galea.2003.
Open-domain Surface-Based QuestionAnswering System.
In Proceedings of the ComputerScience Annual Workshop (CSAW), University ofMalta.E.
H. Hovy, L. Gerber, U. Hermjakob, M. Junk, and C.-Y.
Lin.
2000.
Question Answering in Webclopedia.In Proceedings of the TREC-9 Conference.
NIST,Gaithersburg, MD.
November 2000.
655-664.E.
Hovy and C.Y.
Lin.
1997.
Automated Text Summa-rization in SUMMARIST.
In Proceedings of theWorkshop on Intelligent Scalable Text Summariza-tion, Madrid, Spain, 18-24.Boris Katz, Gregory Marton, Gary Borchardt, AlexisBrownell, Sue Felshin, Daniel Loreto, Jesse Louis-Rosenberg, Ben Lu, Federico Mora, Stephan Stiller,Ozlem Uzuner, and Angela Wilcox.
2005.
ExternalKnowledge Sources for Question Answering Pro-ceedings of the 14th Annual Text REtrieval Confe-rence (TREC2005), November 2005, Gaithersburg,MD.C.
Kwok, O. Etzioni, and D.S.
Weld.
2001.
ScalingQuestion Answering to the Web.
In Proceedings ofthe 10th World Wide Web Conference, Hong Kong,150-161.M.
Sahami and T. Heilman.
2006.
A Web-based kernelfunction for measuring the similarity of short textsnippets.
In Proceedings of 15th International WorldWide Web Conference.
377-386.Chin-Yew Lin and E.H. Hovy.
2003.
Automatic Evalua-tion of Summaries Using N-gram Co-occurrence Sta-tistics, In Proceedings of HLT-NAACL, 71?78.I.
Mani, G. Klein, D. House, L. Hirschman, T. Firmin,and B. Sundheim.
2002.
SUMMAC: a text summari-zation evaluation, Natural Language Engineering, 8(1):43-68.Y, Matsuo, T. Sakaki, K. Uchiyama, and M. Ishizuka.2006.
Graph-based word clustering using Web searchengine.
In Proceedings of EMNLP 2006, 542-550.G.
Harry McLaughlin.
1969.
Reading at ?Impossible?Speeds.
Journal of Reading, 12(6):449-454,502-510.Radu Soricut and Eric Brill.
2006.
Automatic questionanswering using the web: Beyond the factoid.
Jour-nal of Information Retrieval - Special Issue on WebInformation Retrieval, 9:191?206.G.
Salton, and C. Buckley.
1988.
Term-weighting ap-proaches in automatic text retrieval.
InformationProcessing & Management, 24, 5, 513-523.E.
J. Santos, A.
A. Mohamed, and Q. Zhao.
2004.
"Au-tomatic Evaluation of Summaries Using DocumentGraphs," Text Summarization Branches Out.
Pro-ceedings of the ACL-04 Workshop, Barcelona, Spain,66-73.R.
Srihari and W.A.
Li.
2000.
Question Answering Sys-tem Supported by Information Extraction.
In Pro-ceedings of the 1st Meeting of the North AmericanChapter of the Association for Computational Lin-guistics (NAACL-00), 166-172.R.
Varadarajan and V. Hristidis.
2006.
A system forquery-specific document summarization, ACM 15thConference on Information and Knowledge Man-agement (CIKM), Arlington, VA, 622-631.106
