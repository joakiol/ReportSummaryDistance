Preferential Presentation of Japanese Near-SynonymsUsing Definition StatementsHiroyuki OKAMOTO Kengo SATO Hiroaki SAITODepartment of Information and Computer ScienceKeio University3?14?1 Hiyoshi, Kouhoku-ku, Yokohama 223?8522, JapanTel: (+81?45)563?1151 (ex 43250), Fax: (+81?45)566?1747{motch, satoken, hxs}@nak.ics.keio.ac.jpAbstractThis paper proposes a new method ofranking near-synonyms ordered by theirsuitability of nuances in a particular con-text.
Our method distincts near-synonymsby semantic features extracted from theirdefinition statements in an ordinary dictio-nary, and ranks them by the types of fea-tures and a particular context.
Our methodis an initial step to achieve a semanticparaphrase system for authoring support.1 IntroductionMost researches on automatic paraphrasing aim ei-ther at document modification for a wide rangeof NLP applications (Shirai et al, 1998; Tomuroand Lytinen, 2001), at reading comprehension sup-port (Inui and Yamamoto, 2001), or at transforma-tion based on external constraints (Dras, 1998).
Onthe other hand, authoring / revision support is knownas another type of paraphrasing which targets at textsin preparation.
However, there are not so many re-searches of such paraphrasing.Paraphrase systems which aim at revising docu-ments can be classified into three types:?
Syntactic suitabilityThis type of systems points out spelling orgrammatical mistakes and corrects them, suchas a grammar checker (Heidorn, 2000).?
ReadabilitySimilar to reading comprehension support,this type of paraphrase systems aims tosimplify difficult / complicated sentences orphrases (Suganuma et al, 1990; Inui andOkada, 2000).?
Semantic suitabilityTo reflect authors?
intentions precisely, theseparaphrase systems replace words, which aresemantically ambiguous or inadequate, to oneswhich are suitable for their contexts.Almost all known authoring / revision support sys-tems aim at syntactic suitability or readability, whileresearches of the third type of paraphrasing, whichhandle semantics, are very rare.Let us consider a kind of authoring support sys-tem, which first presents near-synonyms (wordscounted among the same semantic category) of atarget word in an input sentence.
Then, based onuser?s choise, the system paraphrases the target wordto the selected one with keeping syntactic and se-mantic consistency through paraphrasing.
Espe-cially for semantic consistency, it is important toexpress semantic differences between paraphrasedword pairs clearly.
If fine-grained meanings of allnear-synonyms (not only a paraphrased pair) can beextracted at a time, the system would be able topresent semantically suitable near-synonyms.
Basedon this idea, this paper proposes a new method ofranking Japanese near-synonyms ordered by theirsuitability of nuances in a particular context.
First,this paper describes an overview of the method inSection 2.
Next, Section 3 shows the classificationof fine-grained meanings of a word and a methodof extracting those fine-grained meanings from adefinition statement of the word, to identify se-mantic differences between near-synonyms.
Then,Section 4 presents our method of ranking near-synonyms using fine-grained meanings described inSection 3.
Finally, this paper shows conclusion andfurther works in Section 5.2 Overview of our method of preferentialpresentationThough some word processing applications (e.g.Microsoft Word) have a function of showing near-synonyms of a word, it is not easy to choose themost adequate word from the near-synonyms be-cause they are not ordered by their semantic similar-ity or suitability.
Also, a simple replacement froma word to one of its near-synonyms is very danger-ous, because there are some differences between thewords in their modification rules and in their fine-grained meanings.Against these semantic problems, we propose anew method of presenting near-synonyms orderedby their semantic suitability in a particular context.When a target word is given from an input sentence,first our method obtains all near-synonyms of thetarget word from an existing thesaurus, and differen-tiates them semantically by features extracted fromtheir definition statements.
Next, our method ranksthose near-synonyms by relations between the typeof features and the context of the input sentence.
Fi-nally, the ranking of near-synonyms are presentedwith information of variation in the original sen-tence for each near-synonym.
This process enablesthe user to choose a word suitable for the input con-text, and helps prevention of semantic variation (orredundancy / loss) in paraphrasing.3 Semantic differentiation betweennear-synonymsAs the first step to realize the preferential suggestionof near-synonyms, we identify fine-grained wordsenses of near-synonyms in order to differentiatethem semantically, by using sentences written in anordinal dictionary (definition statements) and wordco-occurrence information extracted from large cor-pora.3.1 Fine-grained word sensesThere are some researches which deal with fine-grained word senses for a lexical choice in languagegeneration (DiMarco et al, 1993; Edmonds, 1999).Edmonds roughly classified semantic differencesbetween near-synonyms into four categories: deno-tational (difference in nuances of near-synonyms),expressive (in attitudes or emotions), stylistic (informalities or dialects), and collocational (as idiomsor in co-occurrence restrictions).
In addition, heclassified them into 35 types and proposed an on-tology for describing their differences formally.Edmonds implemented I-Saurus, a prototype im-plementation of this ontology, to achieve a lexicalchoice in machine translation and denoted the effec-tiveness of differences between near-synonyms fora lexical choice.
Though, there is a crucial prob-lem that he did not mention how to obtain those dif-ferences automatically.
Against this problem, ourmethod extracts such differences by using definitionstatements for each near-synonym.
Although (Fu-jita and Inui, 2001) has already focused on usingdefinition statements in order to determine a pair ofnear-synonyms whether one can be paraphrased tothe other or not, it was only a kind of matching be-tween two statements and did not identify individ-ual features in each statement.
Therefore, this paperdefines three types of semantic features as follows,which can be extracted from definition statements:?
Core meaning indicates the basic sense of aword.
All near-synonyms in a category mustalways have the same core meaning, such asthe name of the category which they belong to.?
Denotation, which can be paraphrased to ?nu-ance?, is defined as ?the thing that is actuallydescribed by a word rather than the feelings orideas it suggests?
in Longman web dictionary1.In this paper, this feature is defined as a mean-ing included in a word, which partially qualifythe core meaning.
It is similar to a denotationalconstraint in (Edmonds, 1999).?
Lexical restriction of a word is a constraint onthe range of co-occurrence of the word.
Thisfeature is almost the same as a collocationalconstraint in (Edmonds, 1999).An example of these features is shown in Figure 1.We divide our method into two steps to extracteach feature from a definition statement.
First, weextract a word defined as a core meaning and allother content words (in Section 3.2).
Then, the ex-tracted words except the core meaning are classifiedinto denotations or lexical restrictions by using eachco-occurrence information obtained from large cor-pora (in Section 3.3).1http://www.longmanwebdict.com/Word:??????
saikon(rebuilding of shrines / temples)Definition statement:??????????????
?jinja (shrine) bukkaku (temple) wo(OBJ) tate (to build) naosu (to repair)koto (matter)(To build a shrine or a temple to repair.Core meaning: ??
tate (build)Denotation: ??
naosu (repair)Lexical restriction: ??
jinja (shrine)??
bukkaku (temple)Figure 1: Features in a definition statement3.2 Extraction of fine-grained word sensesIn this paper, we assume that a definition statementof a word (hereafter an entry) in a dictionary con-sists of four types of materials as follows:?
Core meaning is a word which exactly de-scribes a particular semantic category whichthe entry belongs to.?
Fine-grained meaning semantically differen-ciates the entry from its near-synonyms.
Itis defined as a core meaning of some contentwords in the definition statement.
Fine-grainedmeaning can be divided into ?denotation?
or?lexical restriction?.?
Stop word indicates a content word whichcommonly and frequently appears in any def-inition statement.?
Others include function words and symbols.According to this assumption, the ?core mean-ing?
and ?fine-grained meanings?
of an entry areextracted from a definition statement, using ofKadokawa thesaurus (Ohno and Hamanishi, 1981)2.A procedure of this method is given as follows:Step 1.
For each morpheme in the morpheme dic-tionary of ChaSen (Matsumoto et al, 2002),a Japanese morphological analyzer, add alabel of a semantic category in KadokawaThesaurus, which the morpheme belongs to.Step 2.
Assign semantic labels to each morpheme ina definition statement of an entry e, by ap-plying ChaSen to the statement.2Kadokawa thesaurus semantically categorizes 57,130 en-tries into 2,924 categories and each entry has a definition state-ment.Step 3.
Give a word c as a ?provisional?
core mean-ing if c is classified into the same semanticcategory as e.Step 4.
Extract all semantic labels, which are as-signed to all content words except c, as fine-grained meanings.Step 5.
Recursively apply Step 2?4 to the definitionstatement of c until no core meaning is ex-tracted from the definition statement.Step 6.
Define c extracted at last as the ?true?
coremeaning of e.According to this procedure, some fine-grainedmeanings could be extracted from stop words.
Thus,we give a semantic weight to each fine-grainedmeaning, by the reciprocal of its occurrence prob-ability in all definition statements.
These weightscan distinct true fine-grained meanings from onesextracted from stop words.A result of this method is shown in Figure 2,where the bold numbers show their categories andthe italics show their weights.Word: [394]??????
saikon(rebuilding of shrines / temples)Core meaning:[394]???
tateru (to build)Fine-grained meaning:[727a]??
jinja (shrine: 5687)[940c]??
bukkaku (temple: 6184)[277b]??
naosu (to alter: 1441)[277c]??
naosu (to recover: 2359)[392]??
naosu (to repair: 7494)[417a]??
naosu (to get right: 3703)[811]??
koto (matter: 30)Figure 2: Example of extraction of core-meaningand fine-grained meanings3.3 Classification of fine-grained word sensesAfter obtaining features in Section 3.2, our methodclassifies fine-grained meanings into denotationsand lexical restrictions, according to the followingheuristics:?
If a word w includes a denotation d, w seldomco-occurs with any word whose core meaningis d. For example, one possible paraphrase of asentenceHe is extremely angry.isHe is enraged.where the word extremely is deleted, becauseenraged has a denotation ?extremely?
if angryis defined as the core meaning of enraged.?
If w involves a lexical restriction l, w often co-occurs with words whose core meaning is l. Forexample, ?a rancid butter?
is more appropriatethan ?a rotten butter?, because rancid has a lex-ical restriction ?oily or fatty food?, while rottendoes not.Based on these heuristics, our method classifies fine-grained meanings of an entry as follows:Step 1.
Assign semantic labels to all words in cor-pora (consisting of 1.93 million sentences,including newspapers 3 and novels 4).Step 2.
Obtain co-occurrence frequencies of allpairs between a word and a semantic labelof a neighbor word from the corpora.Step 3.
Delete the entry e from the thesaurus if edoes not appear in the corpora at all.Step 4.
For each fine-grained meaning f of e whichbelongs to a semantic category C, computeco-occurrence probabilitiesP (f, C) =?i nsif?i Nsi(1)P (f, e) = nefNe (2)where si is a near-synonym of e, nab is theco-occurrence frequency between a word aand a label b, and Na is the frequency of a.Step 5.
Remove f if P (f, C) = 0.Step 6.
Define f as a denotation if P (f, e) = 0.
Theweight of the denotation is the product ofP (f, C) and the weight of f .Step 7.
Define f as a lexical restriction if P (f, e) 6=0.
The weight of the lexical restriction is theproduct of P (f,e)P (f,C) and the weight of f .Figure 3 shows an example of classification aboutthe word ?saikon (??)?.
In Figure 3, under-linedfeatures are the results of word sense disambiguationand elimination of stop words.3Mainichi Shimbun CD-ROMhttp://cl.aist-nara.ac.jp/lab/resource/cdrom/Mainichi/MS.html4Aozora Bunko http://www.aozora.gr.jp/Word: [394]??????
saikon(rebuilding of shrines / temples)Denotation:[277b]??
naosu (to alter: 1.45)[392]??
naosu (to repair: 4.19)Lexical restriction:[727a]??
jinja (shrine: 8518)[940c]??
bukkaku (temple: 5859)[277c]??
naosu (to recover: 3504)[417a]??
naosu (to get right: 2135)[811]??
koto (matter: 15)Figure 3: Classification example of fine-grainedmeanings3.4 Evaluation and discussionsWe applied these procedures to all 57,130 entries inKadokawa thesaurus (2,924 categories).
As a result,36,434 entries, which consist of one core meaningand 0 or more fine-grained meanings, and 1,857 en-tries, which has no core meaning but is refered as acore meaning to other entries, were obtained.
Oneentry has 4.7 denotations and 5.1 lexical restrictionson average.To evaluate our methods, we compared the resultsof automatic extraction against manually extractedones for randomly selected 50 entries.
Table 1 showsthe result of extracting core meanings, and the resultof the classification is shown in Table 2.number of entriescorrects 40errors 10(direct) (4)(indirect) (6)precision 80 %Table 1: Result of extracting core meaningsFailure results of extractions of core meanings ap-peared in the following cases; a core meaning ina definition statement does not belong to the samesemantic category as the entry; the correct coremeaning involves negative expressions in a defini-tion statement; or two or more near-synonyms areappeared in one definition statement.
Therefore, theextraction of core meanings needs to be estimatedwithout relying on their semantic categories, thatis, with other information such as modification re-resultrecall [%]denotation lexical restrictionanswerdenotation 56 13 81.2lexical restriction 22 20 47.6precision [%] 71.8 60.6Table 2: Result of classificationlations of a definition statement.Table 2 shows that both the precision and therecall of the classification into lexical restrictionsare worse than the ones of denotations.
A sparsedata problems could cause it.
In our classificationmethod, if a feature of an entry does not co-occurwith the entry, the feature is classified into a denota-tion or deleted, even though it is expected to be de-fined as a lexical restriction.
It would be improvedby increasing domains and the size of corpora, or byusing information of modification relations just asthe extraction of core meanings.4 Preferential presentation ofnear-synonymsWe secondly propose a method of ranking near-synonyms by using information derived in Sec-tion 3.
Though (Edmonds, 1999) proposed a rankingmethod for lexical choice by using information offine-grained meanings in I-Saurus, it requires moredetailed information than the one which can be ex-tracted from a definition statement.
Thus, this pa-per proposes a ranking method as follows: whena target word in a sentence is given, our methodobtains all near-synonyms5 of the target word andtheir semantic features.
Then, our method ranks thenear-synonyms with respect to their suitability be-tween the input context and features of each near-synonym.
Additionally, if a paraphrase to a near-synonym causes neighbor words in the input sen-tence to arrange in order to keep semantic consis-tency, our method adds such information to the near-synonym when the ranking is presented.4.1 Comparison between denotations andcontexts?Denotations?
can appear in any word, includinga target word in an input sentence.
Therefore, all5There are sometimes two or more core meanings in onesemantic category.
We treat whole core meanings as the exactlysame meaning here.denotations of each near-synonym have to be com-pared not only with the input context but with deno-tations of a target word.
Our method determines thepropriety of paraphrasing between a target word wand its near-synonym si for each denotation dij ofsi, with the following cases:Case 1.
No denotation appears in neither w nor si:?
w can be directly paraphrased to si.Case 2. w has a denotation dw equivalent to dij :?
w can be paraphrased to si on the senseof dij .Case 3. dw does not match with any dij :?
w can be paraphrased to si with addingdw to the input sentence.Case 4. dij does not match with any dw:(a) if dij can be covered with a neighborword w?
of w in the input sentence:?
w can paraphrase to si with deletingw?
from the input sentence.
(b) if dij can not be covered with any wordsin the input sentence:?
w can not be paraphrased to si.In Case 3 and Case 4a, some arrangements (addi-tion / deletion of words) to the input sentence areneeded.
Our method presents these information withthe presentation of near-synonyms rankings (in Sec-tion 4.3).According to these cases, the total denotationalscore Sd of si is defined bySd =?jpWj (3)where Wj is the weight of dij (one of the denota-tions of si) andp =????
?1 (in Case 1, 2, 4a)0 (in Case 3)?1 (in Case 4b)Note that Case 3 gives no weight, because the casedoes not consider any denotation of si but comparesonly between dw and its context.4.2 Comparison between lexical restrictionsand contexts?Lexical restriction?, the other fine-grained mean-ing, is the feature which notably often co-occur withits target word, as described in Section 3.3.
In fact,however, a word which often co-occurs with a targetword does not have to belong exactly to one of thelexical restrictions of the target word.
They couldbe the ?similar?
words.
Therefore, it is necessary tocompute the similarity between a lexical restrictionand a context in order to compare them.The thesaurus used in our method has a tree struc-ture and each entry belongs to the node at 4 or 5 indepth.
The similarity can be defined by a heuristicapproach that any two words are semantically inde-pendent if the depth of their root node is less than3, such as the categories between [588] ?rebels?
and[506] ?private and public?.
Hence, our method de-fines the similarity between a lexical restriction viand a semantic label qi of a word in an input contextas follows:sim(vi, qi) = log2(dep (root(vi, qi))?
4dep(vi) + dep(qi))(4)where root(a, b) is the root node of the minimumsubtree which includes both a and b, and dep(a) isthe depth of a in the thesaurus.To determine the score of a lexical restriction,there is another problem.
An input sentence has sev-eral content words outside of the target word, andsome of them belong to several semantic categoriesbecause of their ambiguities.
Also, the target wordoften has two or more lexical restrictions.
Thus,each lexical restriction must select a semantic labelwhich has the highest similarity with the lexical re-striction from the input sentence.
Against the prob-lem, first, our method computes the similarities of allpossible pairs which consist of a lexical restrictionand a semantic label extracted from the sentence.Then, our method extracts pairs in descending or-der of the similarity with no overlap in any categoryor any lexical restriction.Based on this process, we can compute the totalscore Sv of each near-synonym si of a target wordw in an input sentence, with all extracted pairs of alexical restriction vj and a semantic label qj in theinput sentence bySv =?j(Wj ?
sim(vj , qj)) (5)where Wj is the weight of vj .4.3 Ranking methodThis section describes our method of ranking near-synonyms with respect to the scores defined in Sec-tion 4.2 and Section 4.1, which is the aim of thispaper.
The criterion of ranking is simply the sum ofnormalized Sd and Sv6.
Our method presents near-synonyms according to their ranking, and if neces-sary, information of arrangements to an input sen-tence (extracted in Section 4.1) are shown with eachnear-synonym.4.4 An exampleWhen an input sentence is????????
?tera (joss house) wo (OBJ) tate (to build)naosu (to repair)(Someone rebuilds a joss house.
)and the word ???
(?)
(tate(ru), to build)?
is givenas a target, the semantic labels assigned to each con-tent word in the sentence are?
tera [727b] temple??
tate [394] to build??
naosu [277b] to alter [277c] to recover[392] to repair [417a] to get rightand 24 near-synonyms of tateru are extracted.
Then,our method computes Sd and Sv for each near-synonym.
For example, the scores of a word ???????
(saikon, rebuilding of shrines / temples)?, which in-cludes features shown in Figure 3, are given as fol-lows:?
Sd (the denotational score)For the denotations of saikon, [277b] (to al-ter: 1.45) and [392] (to repair: 4.19) could beobtained, where the italic numbers show theirweight.
They match to the labels in the wordnaosu, thus Sd of saikon is 5.64 and the wordnaosu is given as a deletion information.?
Sv (the score in lexical restriction)For the lexical restrictions of saikon, [277c] (torecover: 3504), [417a] (to get right: 2135),[727a] (shrine: 8518), [811] (matter: 15) and[940c] (temple: 5859) could be obtained, thenthe extracted pairs and their similarity are cal-culated as follows:6Each score has to be normalized because the place of Sdfar differs from that of Sv .lexicalcontext similarityrestriction[277c] ?
[277c] 1.00[417a] ?
[417a] 1.00[727a] ?
[727b] 0.68[811] ?
[392] ?1.00[940c] ?
[277b] ?1.32Therefore, Sv of saikon is calculated as 3682.Finally, by computing Sd and Sv of all theother near-synonyms, our method ranks the near-synonyms and presents them as shown in Figure 4.In Figure 4, the first 9 near-synonyms can be para-phrased from the target word appropriately.
How-ever, saikon is ranked next to fushin contrary to ourexpectation that it would be ranked as the first, be-cause saikon and the fifth word saiken has the sameorthography, and thus the co-occurrence informationof saikon is imprecise by mixture with the informa-tion of saiken.4.5 Evaluation and discussionsTo evaluate our ranking method, we randomly ex-tracted 40 sentences from corpora and applied ourmethod to a certain word in each sentence.
Also, foreach case, we manually selected all near-synonymswhich can be paraphrased7.
We evaluated the rank-ing results of our method by the measure of non-interpolated average precision (NAP):NAP = 1Rn?i=1zii(1 +i?1?k=1zk)(6)where R is the number of near-synonyms which canbe paraphrased, n is the number of presented near-synonyms, andzi =??
?1 if a near synonym in rank i can beparaphrased0 otherwizeTable 3 shows the result.Table 3 shows that our method is remarkably ef-fective for the judgement of semantic suitability ofnear-synonyms if a target word is not ambiguous.However, the average precision is worse for ambigu-ous words, thus it is important to disambiguate thosetarget words before applying to our method.7For the criterion if a word can paraphrase to another or not,we dissemble any addition / deletion informations.
That is, weassume that a word can paraphrase if the paraphrased sentencehas the same meaning as the original with some changes to theircontext.ambiguity of NAP [%]target word our method non-(sentences) Sd Sv Sd + Sv ordereddistinct (21) 74.2 63.8 71.2 60.0vague (19) 48.8 48.3 51.0 42.1both (40) 62.8 56.9 62.2 52.0Table 3: Average precision of rankingMost of failure results are caused by the follow-ing cases; incorrect core meanings or fine-grainedmeanings were extracted in Section 3; adequate re-lations between a near-synonym and an input con-text could not be identified because of the ambiguityof neighbor words in the input sentence; or the se-mantic range of the label of a denotation or a lexi-cal restriction is too wide to express the fine-grainedmeaning of the near-synonym clearly.In addition, Table 3 shows that the average preci-sion by only Sv is worse than the one by only Sd.
Itcould be caused by the low precision of classifica-tion into lexical restrictions and by the inadequacyin the measure of similarity described in Section 4.2.To improve those problems, another measure such assemantical similarities without using a structure of athesaurus is needed.
Also, we would learn from amethod of lexical choice with knowledge about col-locational behavior (Inkpen and Hirst, 2002).Though we have not discussed the evaluation ofthe propriety of arrangements to an input sentence,it seems that the information of addition often occursimprecisely, against that the information of deletionappears infrequently but almost correctly, because,in our method, all denotations of a target word aregiven as the information of addition when they donot match with any denotation of a near-synonym.Therefore, we must define the importance of eachaddition information and to present selected ones.5 Conclusion and future workThis paper proposed a new method of preferentialpresentation of Japanese near-synonyms in order totreat with semantic suitability against contexts, as afirst step of semantic paraphrase system for elabo-ration.
We achieved the effectiveness of using def-inition statements for extracting fine-grained mean-ings, especially for denotations.
Also, the experi-mental results showed that our method could ranknear-synonyms of an unambiguous word for 71%1.
??
fushin (??
: ??)
(delete naosu) 6.
??
chikuzo(Construct or repair a house / a temple / a road) (Build or construct)2.??????
saikon (??
: ??)
(delete naosu) 7.
???
tateru(Rebuild a shrine / a temple) (Build)3.
??
shuchiku (??
: ??)
(delete naosu) 8.
??
kizuku(Repair a house etc.)
(Build)4.
??
konryu 9.
??
kenzo(Build a chapel / a tower of a temple) (Construct a buildiing / a ship)5.??????
saiken 10.
????
tatemashi(Rebuild or Reconstruction) (Add to a building)Figure 4: Result of preferential presentation of ?tera wo tate naosu.
?in accuracy by non-interpolated average precision,about 10 points higher than non-ordered.We have discussed only the initial step of the elab-oration system, thus one of our future work is tohandle syntactic and semantic constraints on actualparaphrasings after applying this method.AcknowledgementsWe would like to thank Mainichi Shinbun-sha andAozora Bunko for allowing us to use their corpora,and Kadokawa Sho-ten for providing us with theirthesaurus.
We are also grateful to our colleagues forhelping our experiment.ReferencesAkira Suganuma, Masanori Kurata and Kazuo Ushijima.1990.
A textual Analysis Method to Extract NegativeExpressions in writing Tools for Japanese Documents.Journal of Information Processing Society of Japan,31(6):792?800.
(In Japanese)Atsushi Fujita and Kentaro Inui.
2001.
Paraphrase ofCommon Nouns to Its Synonyms by Using DefinitionStatements.
The Seventh Annual Meeting of The As-sociation for Natural Language Processing, 331?334.
(In Japanese)Chrysanne DiMarco, Graeme Hirst and Manfred Stede.1993.
The semantic and stylistic differentiation ofsynonyms and near-synonyms.
AAAI Spring Sympo-sium on Building Lexicons for Machine Translation,114?121.Diana Zaiu Inkpen and Graeme Hirst.
2002.
Acquir-ing Collocations for Lexical Choice between Near-Synonyms.
ACL 2002 Workshop on UnsupervisedLexical Acquisition, Philadelphia.George E. Heidorn.
2000.
Intelligent Writing Assis-tance.
In Robert Dale, Hermann Moisl and HaroldSomers (eds.
), A Handbook of Natural Language Pro-cessing, Marcel Dekker, New York.
Chapter 8.Hiroko Inui and Naoyuki Okada.
2000.
Is a Long Sen-tence Always Incomprehensible?
: A Structural Anal-ysis of Readability Factors.
Information Process-ing Society of Japan SIGNotes Natural Language,135(9):63?70.
(In Japanese)Kentaro Inui and Satomi Yamamoto.
2001.
Corpus-Based Acquisition of Sentence Readability RankingModels for Deaf People.
Proceedings of the sixthNatural Language Processing Pacific Rim Symposium(NLPRS), 159?166, Tokyo.Mark Dras.
1998.
Search in Constraint-Based Paraphras-ing.
Proceedings of the second International Confer-ence on Natural Language Processing and IndustrialApplications, 213?219, Moncton.Noriko Tomuro and Steven L. Lytinen.
2001.
SelectingFeatures for Paraphrasing Question Sentences.
Pro-ceedings of the Workshop on Automatic Paraphrasingat Natural Language Processing Pacific Rim Sympo-sium (NLPRS), 55?62, Tokyo.Philip Edmonds.
1999.
Semantic Representations ofNear-Synonyms for Automatic Lexical Choice.
Ph.D.thesis, Department of Computer Science, University ofToronto.Satoshi Shirai, Satoru Ikehara, Akio Yokoo and Yoshi-fumi Ooyama.
1998.
Automatic Rewriting Methodfor Internal Expressions in Japanese to English MTand Its Effects.
Proceedings of the second Interna-tional Workshop on Controlled Language Applications(CLAW-98), 62?75.Shin Ohno and Masato Hamanishi.
1981.
New SynonymDictionary.
Kadokawa Shoten, Tokyo.Yuji Matsumoto, Akira Kitauchi, Tatsuo Yamashita,Yoshitaka Hirano, Hiroshi Matsuda, Kazuma Takaokaand Masayuki Asahara.
2002.
Morphological Anal-ysis System ChaSen 2.2.9 Users Manual.
Nara Ad-vanced Institute of Science and Technology, Nara.
