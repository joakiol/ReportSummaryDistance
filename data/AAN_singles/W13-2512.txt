Proceedings of the 6th Workshop on Building and Using Comparable Corpora, pages 95?104,Sofia, Bulgaria, August 8, 2013. c?2013 Association for Computational LinguisticsUsing a Random Forest Classifier to recognise translations of biomedicalterms across languagesGeorgios Kontonatsios1,2 Ioannis Korkontzelos1,2 Jun?ichi Tsujii3 Sophia Ananiadou1,2National Centre for Text Mining, University of Manchester, Manchester, UK1School of Computer Science, University of Manchester, Manchester, UK2Microsoft Research Asia, Beijing, China3{gkontonatsios,ikorkontzelos,sananiadou}@cs.man.ac.ukjtsujii@microsoft.comAbstractWe present a novel method to recognisesemantic equivalents of biomedical termsin language pairs.
We hypothesise thatbiomedical term are formed by seman-tically similar textual units across lan-guages.
Based on this hypothesis, weemploy a Random Forest (RF) classifierthat is able to automatically mine higherorder associations between textual unitsof the source and target language whentrained on a corpus of both positive andnegative examples.
We apply our methodon two language pairs: one that uses thesame character set and another with a dif-ferent script, English-French and English-Chinese, respectively.
We show thatEnglish-French pairs of terms are highlytransliterated in contrast to the English-Chinese pairs.
Nonetheless, our methodperforms robustly on both cases.
We eval-uate RF against a state-of-the-art align-ment method, GIZA++, and we report astatistically significant improvement.
Fi-nally, we compare RF against SupportVector Machines and analyse our results.1 IntroductionGiven a term in a source language and term in atarget language the task of this paper is to classifythis pair as a translation or not.
We investigate theperformance of the proposed classifier by apply-ing it on a balanced classification problem, i.e.
ourexperimental datasets contain an equal number ofpositive and negative examples.
The proposedclassification model can be used as a component ofa larger system that automatically compiles bilin-gual dictionaries of technical terms across lan-guages.
Bilingual dictionaries of terms are impor-tant resources for many Natural Language Pro-cessing (NLP) applications including StatisticalMachine Translation (SMT) (Feng et al 2004;Huang and Vogel, 2002; Wu et al 2008), Cross-Language Information Retrieval (Ballesteros andCroft, 1997) and Question Answering systems(Al-Onaizan and Knight, 2002).
Especially in thebiomedical domain, manually creating and moreimportantly updating such resources is an expen-sive process, due to the vast amount of neologisms,i.e.
newly introduced terms (Pustejovsky et al2001).
The UMLS metathesaurus which is one themost popular hub of multilingual resources in thebiomedical domain, contains technical terms in 21languages that are linked together using a con-cept identifier.
In Spanish, the second most popu-lar language in UMLS, only 16.44% of the 7.6MEnglish terms are covered while other languagesfluctuate between 0.0052% (for Hebrew terms) to3.26% (for Japanese terms).
Hence, these lex-ica are far for complete and methods that semi-automatically (i.e., in a post-processing step, cu-rators can manually remove erroneous dictionaryentries) discover pairs of terms across languagesare needed to enrich such multilingual resources.Our method can be applied to parallel, aligned cor-pora, where we expect approximately the same,balanced classification problem.
However, incomparable corpora the search space of candidatealignments is of vast size, i.e., quadratic the thesize of the input data.
To cope with this heavilyunbalanced classification problem, we would needto narrow down the number of negative instancesbefore classification.We hypothesise that there are language in-dependent rules that apply to biomedical termsacross many languages.
Often the same or simi-lar textual units (e.g., morphemes and suffixes) areconcatenated to realise the same terms in differentlanguages.
For example, Table 1 illustrates howa morpheme expressing pain (ache in English) isused to realise the same terms in English, Chineseand French.
The realisations of the term ?head-95English Morpheme: -ache Chinese Morpheme: ?
French Morpheme: -malhead-ache ?-?
mal de te?teback-ache ?-?
mal au dosear-ache ??-?
mal d?oreilleTable 1: An example of English, Chinese and French terms consisting of the same morphemesache?
is expected to consist of the units for ?head?and ?ache?
regardless of the language of realisa-tion.
Hence, knowing the translations of ?head?and ?ache?
allows the reconstruction ?headache?in a target language.In our method, we use a Random Forest (RF) clas-sifier (Breiman, 2001) to learn the underlying rulesaccording to which terms are being constructedacross languages.
An RF is an ensemble of De-cision Trees voting for the most popular class.
RFclassifiers are popular in the biomedical domainfor various tasks: classification of microarray data(D?
?az-Uriarte and De Andres, 2006), compoundclassification in cheminformatics (Svetnik et al2003), classification of microRNA data (Jiang etal., 2007) and protein-protein interactions in Sys-tems Biology (Chen and Liu, 2005).
In NLP, RFclassifiers have been used for: Language Mod-elling (Xu and Jelinek, 2004) and semantic pars-ing (Nielsen and Pradhan, 2004).
To the best ofthe authors?
knowledge, this is the first attempt toemploy RF for identifying translation equivalentsof biomedical terms.We prefer RF over other traditional machine learn-ing approaches such as Support Vector Machines(SVMs) for a number of reasons.
Firstly, RF isable to automatically construct correlation pathsfrom the feature space, i.e.
decision rules that cor-respond to the translation rules that we intendto capture.
Secondly, RF is considered one ofthe most accurate classifier available (D?
?az-Uriarteand De Andres, 2006; Jiang et al 2007).
Finally,RF is reported to cope well with datasets where thenumber of features is larger than the number of ob-servations (D?
?az-Uriarte and De Andres, 2006).
Inour dataset, the number of features is almost fourtimes more than that of the observations.We represent pairs of terms using character gramfeatures (i.e., first order features).
Such shal-low features have been proven effective in a num-ber of NLP applications including: Named En-tity Recognition (Klein et al 2003), Multilin-gual Named Entity Transliteration (Klementievand Roth, 2006; Freitag and Khadivi, 2007) andpredicting authorship (Stamatatos, 2006).
In ad-dition, by selecting character n-grams instead ofword n-grams, one avoids to segment words inChinese which has been proven to be a challengingtopic (Sproat and Emerson, 2003).
We evaluateour proposed method on two datasets of biomed-ical terms (English-French and English-Chinese)that contain equal numbers of positive and neg-ative instances.
RF achieves higher classifica-tion performance than baseline methods.
To boostSVM?s performance further, we used a second or-der feature space to represent the data.
It consistsof pairs of character grams that co-occur in trans-lation pairs.
In the second order feature space, theperformance of SVMs improved significantly.The rest of the paper is structured as follows.
InSection 2, we present previous approaches in iden-tifying translation equivalents of terms or namedentities.
In Section 3, we define the classifica-tion problem, we formulate the RF classifier andwe discuss the first and second order feature spacethat we use to represent pairs of terms.
In Sec-tion 4, we show that RF achieves superior classi-fication performance.
In Section 5, we overviewour method and we discuss how it can be used tocompile large-scale bilingual dictionaries of termsfrom comparable corpora.2 Related WorkIn this section, we review previous approachesthat exploit the internal structure of sequences toalign terms or named entities across languages.
(Klementiev and Roth, 2006; Freitag and Khadivi,2007) use character gram features, similar to thefeature space that we propose in this paper, to traindiscriminative, supervised models.
Klementievand Roth (2006) introduce a supervised Percep-tron model for English and Russian named enti-ties.
They construct a character gram feature spaceas follows: firstly, they extract all distinct charac-ter grams from both source and target named en-tity.
Then, they pair character grams of the sourcenamed entity with character grams of the corre-sponding target named entity into features.
In or-96der to reduce the number of features, they linkonly those character grams whose position offsetsin the source and target sequence differs by -1, 0or 1.
Freitag and Khadivi (2007) employ the samecharacter gram feature space but they do not con-straint the included character-grams to their rela-tive position offsets in the source and target se-quence.
The boolean features are defined for ev-ery distinct character-grams observed in the dataof length k or shorter.
Using this feature spacethey train an Averaged Perceptron model, able toincorporate an arbitrary number of features in theinput vectors, for English and Arabic named en-tities.
The above character gram based methodsmainly focused on aligning named entities of thegeneral domain, i.e.
person names, locations, or-ganizations, etc., that are transliterated, i.e.
presentphonetic similarities, across languages.SMT-based approaches built on top of existingSMT frameworks to identify translation pairs ofterms (Tsunakawa et al 2008; Wu et al 2008).Tsunakawa et al(2008), align terms betweena source language Ls and a target language Ltusing a pivot language Lp.
They assume thattwo bilingual dictionaries exist: from Ls to Lpand from Lp to Lt. Then, they train GIZA++(Och and Ney, 2003) on both directions and theymerge the resulting phrase tables into one tablebetween Ls and Lt, using grow-diag-final heuris-tics (Koehn et al 2007).
Wu et al(2008), usemorphemes instead of words as translation unitsto train a phrase based SMT system for technicalterms in English and Chinese.
The use of shorterlexical fragments, e.g.
lemmas, stems and suf-fixes, as translation units has reportedly reducedthe Out-Of-Vocabulary problem (Virpioja et al2007; Popovic and Ney, 2004; Oflazer and El-Kahlout, 2007).Hybrid methods exploit that a term or a named en-tity can be translated in various ways across lan-guages (Shao and Ng, 2004; Feng et al 2004; Luand Zhao, 2006).
For instance, person names areusually translated by transliteration (i.e., wordsexhibiting pronunciation similarities across lan-guages, are likely to be mutual translations) whiletechnical terms are likely to be translated bymeaning (i.e., the same semantic units are used togenerate the translation of the term in the targetlanguage).
The resulting hybrid systems were re-ported to perform at least as well as existing SMTsystems (Feng et al 2004).Lepage and Denoual (2005) presented an analog-ical learning machine translation system as partof the IWSLT task (Eck and Hori, 2005) that re-quires no training process and it is able to achievestate-of-the art performance.
The core methodof their system models relationships between se-quences of characters, e.g., sentences, phrases orwords, across languages using proportional analo-gies, i.e., [a : b = c : d], ?a is to b as c is to d?, andis able to solve unknown analogical equations,i.e., [x : y = z :?]
(Lepage, 1998).
Analogicallearning has been proven effective in translatingunseen words (Langlais and Patry, 2007).
Further-more, analogical learning is reported to achieve abetter precision but a lower recall than a phrase-based machine translation system when translatingmedical terms (Langlais et al 2009).3 MethodologyLet em = (e1, ?
?
?
, em) be an English termconsisting of m translation units and fn =(f1, ?
?
?
, fn) a French or Chinese term consist-ing of n units.
As translation units, we con-sider character grams.
We define a function f :(em, fn) ??
{0, 1}:f(em, fn) ={1, if em translates into fn0, otherwiseThe function can be learned by training a RandomForest (RF) classifier1.
Let N be the number oftraining instances, |?| the total number of features,i.e.
the number of dimensions of the feature space,|?
| a predefined number of random decision treesand |?| a predefined number of random features.An RF classifier is defined as a collection of fullygrown decision tree classifiers, ?i(X) (Breiman,2001):RF = {?1(X), ?
?
?
, ??
(X)}, X = (em, chn)(1)A pair of terms is classified as a translation pairif the majority of the trees is voting for this classlabel.
Let I(?i(X)) be the vote of the ith treein the forest and avj?
{0,1} the average number ofvotes for class labels 0 (translation) and 1 (non-translation).
The function f of ?
decision treescan be written as the majority function:f(em, chn) = Maj (I(?1(X)), ?
?
?
, I(??
(X)))=?12?
?1 I(?i(X)) + 1/2(?1)r??
(2)1The WEKA implementation (Hall et al 2009) of RF wasused for all experiments of this paper.97The majority function returns 1 if the majorityof I(?i(X)) is 1, or returns 0 if the majority ofI(?i(X)) is 0.
Adding or subtracting 1/2 controlswhether a tie is resolved towards 1 or 0, respec-tively.
In RF ties are resolved randomly.
To rep-resent this, the negative unit (?1) is raised to arandomly chosen positive integer r ?
N+.We tuned the RF classifier using 140 randomtrees and |?| = log2 |?|+ 1 features as suggestedin Breiman (Breiman, 2001).The RF mechanism that triggers term constructionrules across languages lies in the decision trees.A RF grows a decision tree by selecting the mostinformative feature, i.e.
corresponding to thelowest entropy, out of ?
random features.
Foreach selected feature, a node is created and thisprocess is repeated for all ?
random features ofthe unprunned decision trees.
In other words, theprocess starts with the most informative featureand builds association rules between all randomfeatures.
These are the construction rules thatwe are interested in.
Figure 1 illustrates a pathin one of the decision trees of an RF classifiertaken from the experiments we conducted onthe English-Chinese dataset.
In only one ofthousands of branches of the forest, the classifieris able to partially trigger the construction rule ofkinase, a type of enzyme, between English andChinese.
The translation rule correctly associatesthe English n-grams kin and as with their Chinesetranslation ??.
In addition, the translation rulecontains both positive and negative associationsbetween features.
The English n-grams ing andor are negatively correlated with the term kinase.3.1 Feature EngineeringEach pair of terms is represented as a feature vec-tor of character n-grams.
We further define twotypes of character n-gram features, namely firstorder and second order.
First order character n-grams are boolean features that designate the oc-currence of a corresponding character gram of pre-defined length in the input term.
These features aremonolingual, extracted separately from the sourceand target term.
The RF classifier is shown to ben-efit from only monolingual features and achievesthe best observed performance.
In contrast, SVMswere shown not to perform well using the first or-der feature space because they cannot directly as-sociate the source with the target character grams.To enhance the performance of SVMs, we con-structed a second order feature space that containsassociations between first order features.
A sec-ond order feature is a tuple of a source and a tar-get character gram that co-occur in one or moretranslation pairs.
Table 2 illustrates an example.Second order character n-grams are multilingualfeatures and are defined over true translation pairs.For this reason, we extract second order featuresfrom the training data only.In all experiments, the features were sorted in de-creasing order of frequency of occurrence.
Wetrained a RF and two SVM classifiers, namelylinear-SVM and RBF-SVM, using a gradually in-creasing number of features, always starting fromthe top of the list.
SMT frameworks cannot betrained on an increasing number of features be-cause each training instance needs to correspondto at least one known translation unit (i.e., first or-der features).
Therefore, GIZA++ is trained on thecomplete set of translation units.4 ExperimentsIn this section, we discuss the employed datasetsof biomedical terms in English-French andEnglish-Chinese and three baseline methods.
Wecompare and discuss RF and SVMs trained on thefirst order and second order features.
Finally, wereport results of all classification methods evalu-ated on the same datasets.4.1 DatasetsFor our experiments, we used an online bilin-gual dictionary2 for English-Chinese terms and theUMLS metathesaurus3 for English-French terms.The former contains 31, 700 entries while the lat-ter is a much larger dictionary containing 84, 000entries.
For training, we used the same number ofinstances for both language pairs (i.e., 21, 000 en-tries) in order not to bias the performance towardsthe larger English-French dataset.
The remain-ing instances were used for testing (i.e., 10, 7000and 63, 000 English-Chinese and English-Frenchrespectively).
In the case where a source term cor-responded to more that one target terms accordingto the seed dictionary, we randomly selected onlyone translation.
Negative instances were createdby randomly matching non-translation pairs ofterms.
Since we are dealing with a balanced clas-2www2.chkd.cnki.net/kns50/3nlm.nih.gov/research/umls98Figure 1: Example of a term construction rule as a branch in a decision tree.Input pair of English-French terms : (e1, e2, e3, f1, f2, f3)English first order French first order Second order?1(e1, e2) ?1(f1, f2) ?1(e1e2, f1f2), ?1(e1e2, f2f3)?1(e2, e3) ?1(f2, f3) ?1(e2e3, f1f2), ?1(e2e3, f2f3)Table 2: Example of first and second order features using a predefined n-gram size of 2.sification problem, we created as many negativeinstances as the positive ones in all our datasets.In all experiments we performed a 3-fold cross-validation.4.2 BaselinesWe evaluated RF against three classification meth-ods, namely SVMs, GIZA++ and a Levenshteindistance-based classifier.SVMs coordinate a hyperplane in the hyperspacedefined by the features to best separate the posi-tive and negative instances, i.e.
aligned from non-aligned pairs.
In contrast to RF, SVMs do not sup-port building association rules between features,i.e., translation units, which in our task seems to bea deficiency.
SVMs produce one final associationrule, i.e.
the classification boundary which sepa-rates positive from negative examples.
Its abil-ity to distinguish aligned from non-aligned pairof terms depends on how separable the two clus-ters are.
We evaluated several settings for theSVM classifier.
Apart from the default linear ker-nel function, we applied a radial basis function,i.e.
RBF-SVM.
RBF-SVM uses the kernel trick toproject the instances in a higher dimensional spaceto better separate the two clusters.
While tuningthe SVM?s classification cost C, we observed op-timal performance for a value of 100.
Secondly,we seeded the association rules of translation unitsto the SVM classifier by creating a second or-der feature space, discussed in detail in section3.1.
We employed the LIBSVM implementation(Chang and Lin, 2011) of SVMs using both thelinear and RBF kernels.The second baseline method is GIZA++, anopen source implementation of the 5 IBM-models(Brown et al 1993).
GIZA++ is traditionallytrained on a bilingual, parallel corpus of alignedsentences and estimates the probability P (s|t) of asource translation unit (typically a word), s, givena target unit t. To apply GIZA++ on our dataset,we consider the list of terms as parallel sentences.GIZA++, trained on a list of terms, estimatesthe alignment probability of English-Chinese andEnglish-French textual units, i.e.
character n-grams.
Each entry i, j in the translation tableis the probability P (si|tj), where si and tj arethe source and target character n-grams in row iand column j, respectively.
Further details abouttraining a SMT toolkit for aligning technical termscan be found in (Tsunakawa et al 2008; Freitagand Khadivi, 2007; Wu et al 2008).
After train-ing GIZA++ we estimate the posterior probabil-ity P (cfn|em) that a test, Chinese or French termcfn = {cf1, ?
?
?
, cfn} is aligned with a given En-glish term em = {e1, ?
?
?
, em} as follows:p(cfn|em) = n?mn?i=1m?j=1P (cfi|ej) (3)A threshold ?
was defined to classify a pair ofterms into translations or non-translations:f(em, cfn) ={1, if p(cfn|em) ?
?0, otherwise(4)We experimented with different values of ?
(greedy search) and we selected a value that max-imizes classification performance.In order to estimate how phonetically similar thetwo language pairs are, we employed a third base-99(a) English-French dataset (b) English-Chinese datasetFigure 2: F-Score of the RF and SVM, GIZA++ and Levenshtein distance-based classifier on the firstorder datasetline method that uses the Edit/Levenshtein dis-tance of pairs of terms to classify instances astranslations or not.
The Levenshtein distance isdefined as the minimum edit operations, i.e., inser-tion, deletions and substitution, required to trans-form one sequence of characters to another.
Wecannot directly calculate the Levenshtein distancebetween English-Chinese pairs of terms since thetwo languages are using different scripts.
There-fore, before we applied the Levenshtein distance-based classifier, we converted the Chinese termsto their pinyin form, i.e., Romanization system ofChinese characters.
As with GIZA++, we selecteda threshold ?
that maximizes the performance ofthe classifier.4.3 ResultsWe hypothesise that a RF classifier is able to formassociation paths between first order features.
Wealso have the theoretical intuition that SVM clas-sifiers are not able to form such association paths.As a result, we expect limited performance on thefirst order feature set, because it does not containany associations among character grams.Figure 2 shows the F-Score achieved by RF, linear-SVM, RBF-SVM, GIZA++ and Levenshtein/Editdistance-based classifier on the English-Frenchand English-Chinese datasets.
RF and SVMs aretrained on an increasing number of features.
Thebehaviour of the classifiers is approximately thesame in both datasets.
Performance is greater onthe English-French dataset since English is moresimilar to French than to Chinese.We also observe that linear-SVM and RBF-SVMdo not behave consistently.
RBF-SVM?s perfor-mance quickly climbs to a maximum and after-wards it declines while linear-SVM?s performanceis constantly increasing until it balances to a veryhigh error rate, almost corresponding to randomclassification.
The linear-SVM classifier performspoorly using first order features only, indicatingthat this feature space is non-linearly separable,i.e.
there exists no hyperplane that separates trans-lation from non-translation instances.
Contrary,RBF-SVM is able to construct a higher dimen-sional space by applying the kernel trick so asto take full advantage of a small number of fre-quent and informative first order features.
In thishigher dimensional space of few but informativefirst order features, the RBF-SVM classifier coor-dinates a hyperplane that effectively separates pos-itive from negative instances.
However, increas-ing the number of features introduces noise thataffects the performance.The RF is able to profit from larger sets of firstorder features; thus, its performance is continu-ously increasing until it stabilises at 6, 000 fea-tures.
The branches of the decision trees are shownto manage features correctly to construct most ofthe translation rules.
Increasing the size of the fea-ture space minimises the classification error, be-cause more translation rules that generalize wellon unseen data are constructed.The bilingual dictionary that we use for ourexperiments contains heterogeneous biomedicalterms of diverse semantic categories.
For ex-ample, our data-set contains common medicalterms such as Intellectual Products (e.g.
PainManagement, prise en charge de la douleur, ????)
or complex biological concepts suchas Enzymes (e.g.
homogentisate 1,2-dioxygenase,100(a) English-French dataset (b) English-Chinese datasetFigure 3: F-Score of the RF and SVM, GIZA++ and Levenshtein distance-based classifier on the secondorder datasetEnglish-French pairs English-Chinese pairsP R F1 P R F1GIZA++ 0.901 0.826 0.862 0.907 0.742 0.816Levenshtein Distance 0.762 0.821 0.791 0.501 0.990 0.668SVM -RBFsecond-order 0.946 0.884 0.914 0.750 0.899 0.818Linear-SVMsecond-order 0.866 0.887 0.8763 0.765 0.893 0.824RFfirst-order 0.962 0.874 0.916 0.779 0.940 0.851Table 3: Best observed performance of RF, SVM and GIZA++ and Levenshtein Distanceacide homogentisique-oxydase, ???1,2-???).
Therefore, we would expect poor perfor-mance of the supervised methods using only asmall portion of the total set of first order featuresdue to the high diversity of the terms.
For exam-ple the morpheme ache/ mal/ ?
is more frequentin Disease or Syndrome named entities rather thanEnzyme named entities.
However, the results indi-cate that RF can generalize well on heterogeneousterms.
Figure 2 shows that the RF classifier out-performs SMT based methods, using only 1000features.The Levenshtein distance-based classifier per-forms considerably better in the English-Frenchdataset than in English-Chinese.
In fact, its bestperformance for the English-Chinese dataset isachieved when classifying every pair of terms asa translation, i.e.
100% recall but 50% precision.In a second experiment, we attempted to explorewhether the performance of SVMs can be im-proved by providing cross-language associationfeatures.
We employed the second order featureset discussed in subsection 3.1.
We used a constantnumber of 6, 000 first order features, the num-ber of features that achieved maximum F-Scorefor RF in the previous experiment.
Besides thesefirst order features, we added an increasing num-ber of second order ones.
Figure 3 shows the F-Score curves of the RF, linear-SVM, RBF-SVM,GIZA++ and Levenshtein distance using this fea-ture space.We observe that second order features improvedthe performance of both SVMs considerably.
Incontrast to the previous experiment, the two SVMspresent consistent bevaviour.
Interestingly, theperformance of the RF slightly decreased whenusing a small number of second order features.A possible explanation of this behaviour is thatthe second order associative features added noise,since the RF had already formed the associationrules from first order features.
In addition, for mEnglish and n Chinese or French first order fea-tures there were m ?
n possible combinations ofsecond order features as explained in Subsection3.1.
Hence, there was a large number of secondorder features that we excluded from the train-ing process.
Consequently, decision tree brancheswere populated with incomplete association ruleswhile the RF was able to form these associa-tions automatically.
Nevertheless, as more sec-ond order features were added, more associationrules were explored and the RF performance in-101creased.
Table 3 summarises the highest perfor-mance achieved by the RF, SVMs, GIZA++ andLevenshtein distance all trained and tested on thesame dataset.
The resulting performance of the RFcompared with GIZA++ is statistically significant(p < 0.0001) in all experiments.
Comparing theRF with the SVMs, we note that in the English-French dataset, the performance of the SVM-RBFis approximately the same with the performanceof our proposed method.
However, this comeswith a cost.
Firstly, SVMs can possibly achievea comparable performance to the RF when us-ing multilingual, second order features.
In con-trast, our experiments show that RF benefit frommonolingual, first order features only.
Secondly,SVMs need a large number of additional multi-lingual features, (6.000 second order features ormore) to perform similarly to RF.
As a conse-quence, the resulting models of the SVM classi-fiers are more complex.
We measured the aver-age time needed by the two classifiers to decidefor a single pair of terms.
The RF is approx-imately 30 times faster than SVMs (on average0.010 and 0.292 seconds, respectively).
Finally,in the English-Chinese dataset the RF performedsignificantly better than both SVMs.5 Discussion And Future WorkIn this paper, we presented a novel classificationmethod that uses Random Forest (RF) to recognisetranslations of biomedical terms across languages.Our approach is based on the hypothesis that inmany languages, there exist some rules for com-bining textual units, e.g.
n-grams, to form biomed-ical terms.
Based on this assumption, we de-fined a first order feature space of character gramsand demonstrated that an RF classifier is able todiscover such cross language translation rules forterms.
We experimented with two diverse lan-guage pairs: English-French and English-Chinese.In the former case, pairs of terms exhibit high pho-netic similarity while in the latter case they do not.Our results showed that the proposed method per-forms robustly in both cases and achieves a signif-icantly better performance than GIZA++.
We alsoevaluated Support Vector Machines (SVM) clas-sifiers on the same first order feature space andshowed that they fail to form translation rules inboth language pairs, possibly because it cannotassociate first order features with each other suc-cessfully.
We attempted to boost the performanceof the SVM classifier by adding association evi-dence of textual units to the features.
We extractedsecond order features from the training data andwe defined a new feature set consisting of both firstorder and second order features.
In this featurespace, the performance of the SVMs improved sig-nificantly.In addition to this, we observe from the reportedexperiments that RF achieves a better F-Score per-formance than GIZA++ in all datasets.
Nonethe-less, GIZA++ presents a better precision (butlower recall) in one dataset, i.e., English/Chinese.Based on this observation we plan to investigatethe performance of a hybrid system combining RFwith MT approaches.One trivial approach to apply the proposed methodfor compiling large-scale bilingual dictionaries ofterms from comparable corpora would be to di-rectly classify all possible pairs of terms intotranslations or non-translations.
However, incomparable corpora, the size of the search spaceis quadratic to the input data.
Therefore, the clas-sification task is much more challenging since thedistribution of positive and negative instances ishighly skewed.
To cope with the vast search spaceof comparable corpora, we plan to incorporatecontext-based approaches with the RF classifica-tion method.
Context-based approaches, such asdistributional vector similarity (Fung and McKe-own, 1997; Rapp, 1995; Koehn and Knight, 2002;Haghighi et al 2008), can be used to limit thenumber of candidate translations by filtering outpairs of terms with low contextual similarity.Finally, the proposed method can be also used toonline augment the phrase table of Statistical Ma-chine Translation (SMT) in order to better han-dle the Out-of-Vocabulary problem i.e.
inabilityto translate textual units that consist of one ormore words and do not occur in the training data(Habash, 2008).AcknowledgementsThe work described in this paper is partiallyfunded by the European Community?s SeventhFramework Program (FP7/2007-2013) under grantagreement no.
318736 (OSSMETER).102ReferencesY.
Al-Onaizan and K. Knight.
2002.
Translatingnamed entities using monolingual and bilingual re-sources.
In Proceedings of the 40th Annual Meet-ing on Association for Computational Linguistics,pages 400?408.
Association for Computational Lin-guistics.L.
Ballesteros and W.B.
Croft.
1997.
Phrasal trans-lation and query expansion techniques for cross-language information retrieval.
In ACM SIGIR Fo-rum, volume 31, pages 84?91.
ACM.L.
Breiman.
2001.
Random forests.
Machine learn-ing, 45(1):5?32.P.F.
Brown, V.J.D.
Pietra, S.A.D.
Pietra, and R.L.
Mer-cer.
1993.
The mathematics of statistical machinetranslation: Parameter estimation.
Computationallinguistics, 19(2):263?311.C.C.
Chang and C.J.
Lin.
2011.
Libsvm: a libraryfor support vector machines.
ACM Transactions onIntelligent Systems and Technology (TIST), 2(3):27.X.W.
Chen and M. Liu.
2005.
Prediction of protein?protein interactions using random decision forestframework.
Bioinformatics, 21(24):4394?4400.R.
D?
?az-Uriarte and S.A. De Andres.
2006.
Gene se-lection and classification of microarray data usingrandom forest.
BMC bioinformatics, 7(1):3.Matthias Eck and Chiori Hori.
2005.
Overview of theiwslt 2005 evaluation campaign.
In Proc.
of the In-ternational Workshop on Spoken Language Transla-tion, pages 1?22.D.
Feng, Y. Lv, and M. Zhou.
2004.
A new approachfor english-chinese named entity alignment.
In Em-pirical Methods in Natural Language Processing,pages 372?379.D.
Freitag and S. Khadivi.
2007.
A sequence align-ment model based on the averaged perceptron.
InConference on Empirical methods in Natural Lan-guage Processing, pages 238?247.P.
Fung and K. McKeown.
1997.
A technical word-and term-translation aid using noisy parallel cor-pora across language groups.
Machine Translation,12(1):53?87.N.
Habash.
2008.
Four techniques for online han-dling of out-of-vocabulary words in arabic-englishstatistical machine translation.
In Proceedings of the46th Annual Meeting of the Association for Compu-tational Linguistics on Human Language Technolo-gies: Short Papers, pages 57?60.
Association forComputational Linguistics.A.
Haghighi, P. Liang, T. Berg-Kirkpatrick, andD.
Klein.
2008.
Learning bilingual lexiconsfrom monolingual corpora.
Proceedings of ACL-08:HLT, pages 771?779.M.
Hall, E. Frank, G. Holmes, B. Pfahringer, P. Reute-mann, and I.H.
Witten.
2009.
The weka data miningsoftware: an update.
ACM SIGKDD ExplorationsNewsletter, 11(1):10?18.F.
Huang and S. Vogel.
2002.
Improved named en-tity translation and bilingual named entity extrac-tion.
In International Conference on Multimodal In-teraction, pages 253?258.
IEEE.P.
Jiang, H. Wu, W. Wang, W. Ma, X.
Sun, and Z. Lu.2007.
Mipred: classification of real and pseudomicrorna precursors using random forest predictionmodel with combined features.
Nucleic acids re-search, 35(suppl 2):W339?W344.D.
Klein, J. Smarr, H. Nguyen, and C.D.
Manning.2003.
Named entity recognition with character-levelmodels.
In Proceedings of the seventh conferenceon Natural language learning at HLT-NAACL, pages180?183.
Association for Computational Linguis-tics.A.
Klementiev and D. Roth.
2006.
Weakly supervisednamed entity transliteration and discovery from mul-tilingual comparable corpora.
In Proceedings ofthe 21st International Conference on ComputationalLinguistics and the 44th annual meeting of the Asso-ciation for Computational Linguistics, pages 817?824.
Association for Computational Linguistics.P.
Koehn and K. Knight.
2002.
Learning a transla-tion lexicon from monolingual corpora.
In Proceed-ings of the ACL-02 workshop on Unsupervised lex-ical acquisition-Volume 9, pages 9?16.
Associationfor Computational Linguistics.P.
Koehn, H. Hoang, A. Birch, C. Callison-Burch,M.
Federico, N. Bertoldi, B. Cowan, W. Shen,C.
Moran, R. Zens, et al2007.
Moses: Opensource toolkit for statistical machine translation.
InProceedings of the 45th Annual Meeting of the ACLon Interactive Poster and Demonstration Sessions,pages 177?180.
Association for Computational Lin-guistics.Philippe Langlais and Alexandre Patry.
2007.
Trans-lating unknown words by analogical learning.
InProceedings of EMNLP-CoNLL, pages 877?886.Philippe Langlais, Franc?ois Yvon, and Pierre Zweigen-baum.
2009.
Improvements in analogical learning:application to translating multi-terms of the medicaldomain.
In Proceedings of the 12th Conference ofthe European Chapter of the Association for Com-putational Linguistics, pages 487?495.
Associationfor Computational Linguistics.Yves Lepage.
1998.
Solving analogies on words: analgorithm.
In Proceedings of the 17th internationalconference on Computational linguistics-Volume 1,pages 728?734.
Association for Computational Lin-guistics.M.
Lu and J. Zhao.
2006.
Multi-feature based chinese-english named entity extraction from comparablecorpora.
pages 131?141.103R.D.
Nielsen and S. Pradhan.
2004.
Mixing weaklearners in semantic parsing.
In Empirical Methodsin Natural Language Processing.F.J.
Och and H. Ney.
2003.
A systematic comparisonof various statistical alignment models.
Computa-tional linguistics, 29(1):19?51.K.
Oflazer and I.D.
El-Kahlout.
2007.
Exploringdifferent representational units in english-to-turkishstatistical machine translation.
In Proceedings ofthe Second Workshop on Statistical Machine Trans-lation, pages 25?32.
Association for ComputationalLinguistics.Maja Popovic and Hermann Ney.
2004.
Towards theUse of Word Stems and Suffixes for Statistical Ma-chine Translation.
In 4th International Conferenceon Language Resources and Evaluation (LREC),pages 1585?1588, Lisbon,Portugal.J.
Pustejovsky, J. Castano, B. Cochran, M. Kotecki,and M. Morrell.
2001.
Automatic extractionof acronym-meaning pairs from medline databases.Studies in health technology and informatics,(1):371?375.R.
Rapp.
1995.
Identifying word translations in non-parallel texts.
In Proceedings of the 33rd annualmeeting on Association for Computational Linguis-tics, pages 320?322.
Association for ComputationalLinguistics.L.
Shao and H.T.
Ng.
2004.
Mining new word trans-lations from comparable corpora.
In Proceedingsof the 20th international conference on Computa-tional Linguistics, page 618.
Association for Com-putational Linguistics.R.
Sproat and T. Emerson.
2003.
The first internationalchinese word segmentation bakeoff.
In Proceedingsof the second SIGHAN workshop on Chinese lan-guage processing-Volume 17, pages 133?143.
Asso-ciation for Computational Linguistics.Efstathios Stamatatos.
2006.
Ensemble-based authoridentification using character n-grams.
In In Proc.of the 3rd Int.
Workshop on Textbased InformationRetrieval, pages 41?46.V.
Svetnik, A. Liaw, C. Tong, J.C. Culberson, R.P.Sheridan, and B.P.
Feuston.
2003.
Random forest:a classification and regression tool for compoundclassification and qsar modeling.
Journal of chemi-cal information and computer sciences, 43(6):1947?1958.T.
Tsunakawa, N. Okazaki, and J. Tsujii.
2008.Building bilingual lexicons using lexical translationprobabilities via pivot languages.
In Proceedingsof the Sixth International Conference on LanguageResources and Evaluation (LREC?08), Marrakech,Morocco, may.S.
Virpioja, J.J. Va?yrynen, M. Creutz, and M. Sade-niemi.
2007.
Morphology-aware statistical machinetranslation based on morphs induced in an unsu-pervised manner.
Machine Translation Summit XI,2007:491?498.X.
Wu, N. Okazaki, T. Tsunakawa, and J. Tsujii.
2008.Improving English-to-Chinese Translation for Tech-nical Terms Using Morphological Information.
InAMTA-2008.
MT at work: Proceedings of the EighthConference of the Association for Machine Trans-lation in the Americas, pages 202?211, Waikiki,Hawai?i, October.P.
Xu and F. Jelinek.
2004.
Random forests in lan-guage modeling.
In Empirical Methods in NaturalLanguage Processing, pages 325?332.
Associationfor Computational Linguistics.104
