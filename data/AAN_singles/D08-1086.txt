Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 820?828,Honolulu, October 2008. c?2008 Association for Computational LinguisticsIntegrating Multi-level Linguistic Knowledge with a Unified Framework forMandarin Speech RecognitionXinhao Wang, Jiazhong Nie, Dingsheng Luo, Xihong Wu?Speech and Hearing Research Center,Key Laboratory of Machine Perception (Ministry of Education),School of Electronics Engineering and Computer Science,Peking University, Beijing, 100871, China{wangxh,niejz,wxh,dsluo}@cis.pku.edu.cnAbstractTo improve the Mandarin large vocabularycontinuous speech recognition (LVCSR), aunified framework based approach is intro-duced to exploit multi-level linguistic knowl-edge.
In this framework, each knowledgesource is represented by a Weighted FiniteState Transducer (WFST), and then they arecombined to obtain a so-called analyzer for in-tegrating multi-level knowledge sources.
Dueto the uniform transducer representation, anyknowledge source can be easily integrated intothe analyzer, as long as it can be encodedinto WFSTs.
Moreover, as the knowledge ineach level is modeled independently and thecombination is processed in the model level,the information inherently in each knowledgesource has a chance to be thoroughly ex-ploited.
By simulations, the effectivenessof the analyzer is investigated, and then aLVCSR system embedding the presented ana-lyzer is evaluated.
Experimental results revealthat this unified framework is an effective ap-proach which significantly improves the per-formance of speech recognition with a 9.9%relative reduction of character error rate onthe HUB-4 test set, a widely used Mandarinspeech recognition task.1 IntroductionLanguage modeling is essential for large vocabu-lary continuous speech recognition (LVCSR), whichaims to determine the prior probability of a supposedword string W , p(W ).
Although the word-based n-gram language model remains the mainstream for?Corresponding author: Xihong Wumost speech recognition systems, the utilization oflinguistic knowledge is too limited in this model.Consequently, many researchers have focused onintroducing more linguistic knowledge in languagemodeling, such as lexical knowledge , syntax andsemantics of language (Wang and Vergyri, 2006;Wang et al, 2004; Charniak, 2001; Roark, 2001;Chelba, 2000; Heeman, 1998; Chelba et al, 1997).Recently, structured language models have beenintroduced to make use of syntactic hierarchi-cal characteristics (Roark, 2001; Charniak, 2001;Chelba, 2000).
Nevertheless, the computationalcomplexity of decoding will be heavily increased, asthey are parser-based models.
In contrast, the class-based language model groups the words that havesimilar functions of syntax or semantics into mean-ingful classes.
As a result, it handles the questions ofdata sparsity and generalization of unseen event.
Inpractice, the part-of-speech (POS) information, cap-turing the syntactic role of words, has been widelyused in clustering words (Wang and Vergyri, 2006;Maltese et al, 2001; Samuelsson and Reichl, 1999).In Heeman?s POS language model (Heeman, 1998),the joint probability of word sequence and associ-ated POS sequence was estimated directly, whichhas been demonstrated to be superior to the condi-tional probability previously used in the class-basedmodels (Johnson, 2001).
Moreover, a SuperARVlanguage model was presented (Wang and Harper,2002), in which lexical features and syntactic con-straints were tightly integrated into a linguistic struc-ture of SuperARV serving as a class in the model.Thus, these knowledge was integrated in the rep-resentation level, and then the joint probabilities820of words and corresponding SuperARVs were esti-mated.
However, in the class-based language mod-els, words are taken as the model units, while otherunits smaller or larger than words are unfeasible formodeling simultaneously, such as the Chinese char-acters for Chinese names.Usually, speech recognition systems can only rec-ognize the words within a predefined dictionary.With the increase of unknown words, i.e., out-of-vocabulary (OOV) words, the performance will de-grade dramatically.
This is because not only thoseunknown words cannot be recognized correctly, butthe words surrounding them will be affected.
Thus,many efforts have been made to deal with the is-sue of OOV words (Martins et al, 2006; Galescu,2003; Bazzi and Glass, 2001), and various modelunits smaller than words have been examined to rec-ognize OOVs from speech, such as phonemes (Bazziand Glass, 2000a), variable-length phoneme se-quence (Bazzi and Glass, 2001), syllable (Bazzi andGlass, 2000b) and sub-word (Galescu, 2003).
Sincethe proper name is a typical category of OOV wordsand usually takes a very large proportion among allkinds of OOV words, it has been specially addressedin (Hu et al, 2006; Tanigaki et al, 2000).All those attempts mentioned above succeed inutilizing linguistic knowledge in language modelingin some degree respectively.
In this study, a uni-fied framework based approach, which aims to ex-ploit information from multi-level linguistic knowl-edge, is presented.
Here, the Weighted Finite StateTransducer (WFST) turns to be an ideal choice forour purpose.
WFSTs were formerly introduced tosimplify the integration of models in speech recog-nition, including acoustic models, phonetic mod-els and word n-gram (Mohri, 1997; Mohri et al,2002).
In recent years, the WFST has been suc-cessfully applied in several state-of-the-art speechrecognition systems, such as systems developed bythe AMI project (Hain et al, 2006), IBM (Saon etal., 2003) and AT&T (Mohri et al, 1996), and invarious fields of natural language processing, suchas smoothed n-gram model, partial parsing (Abney,1996), named entities recognition (Friburger andMaurel, 2004), semantic interpretation (Raymond etal., 2006) and machine translation (Tsukada and Na-gata, 2004).
In (Takaaki Hori and Minami, 2003),the WFST has been further used for language modeladaptation, where language models of different vo-cabularies that represented different styles were in-tegrated through the framework of speech transla-tion.
In WFST-based systems, all of the models arerepresented uniformly by WFSTs, and the generalcomposition algorithm (Mohri et al, 2000) com-bines these representations flexibly and efficiently.Thereby, rather than integrating the models step bystep in decoding stage, a complete search network isconstructed in advance.
The combined WFST willbe more efficient by optimizing with determiniza-tion, minimization and pushing algorithms of WF-STs (Mohri, 1997).
Besides, the researches on opti-mizing the search space and improving WFST-basedspeech recognition has been carried out, especiallyon how to perform on-the-fly WFSTs compositionmore efficiently (Hori et al, 2007; Diamantino Ca-seiro, 2002).In this study, we extend the linguistic knowledgeused in speech recognition.
As WFSTs provide acommon and natural representation for lexical con-straints, n-gram language model, Hidden MarkovModel models and context-dependency, multi-levelknowledge sources can be encoded into WFSTs un-der the uniform transducer representation.
Then thisgroup of WFSTs is flexibly combined together toobtain an analyzer representing knowledge of per-son and location names as well as POS information.Afterwards, the presented analyzer is incorporatedinto LVCSR to evaluate the linguistic correctness ofrecognition candidates by an n-best rescoring.Unlike other methods, this approach holds twodistinct features.
Firstly, as all multi-level knowl-edge sources are modeled independently, the modelunits such as character, words, phrase, etc., can bechosen freely.
Meanwhile, the integration of theseinformation sources is conducted in the model levelrather than the representation level.
This setup willhelp to model each knowledge source sufficientlyand may promote the accuracy of speech recogni-tion.
Secondly, under this unified framework, it iseasy to combine additional knowledge source intothe framework with the only requirement that thenew knowledge source can be represented by WF-STs.
Moreover, since all knowledge sources are fi-nally represented by a single WFST, additional ef-forts are not required for decoding the new knowl-edge source.821The remainder of this paper is structured as fol-lows.
In section 2, we introduce our analyzer in de-tail, and incorporate it into a Mandarin speech recog-nition system.
In section 3, the simulations are per-formed to evaluate the analyzer and test its effective-ness when being applied to LVCSR.
The conclusionappears in section 4.2 Incorporation of Multi-level linguisticknowledge in LVCSRIn this section, we start by giving a brief descrip-tion on WFSTs.
Then some special characteristicsof Chinese are investigated, and the model units arefixed.
Afterwards, each knowledge source is rep-resented with WFSTs, and then they are combinedinto a final WFST, so-called analyzer.
At last, thisanalyzer is incorporated into Mandarin LVCSR.2.1 Weighted Finite State TransducersThe Weighted Finite State Transducer (WFST) is thegeneralization of the finite state automata, in which,besides of an input label, an output label and aweight are also placed on each transition.
With theselabels, a WFST is capable of realizing a weighted re-lation between strings.
In our system, log probabili-ties are adopted as transition weights and the relationbetween two strings is associated with a weight indi-cating the probability of the mapping between them.Given a group of WFSTs, each of which models astage of a mapping cascade, the composition opera-tion provides an efficient approach to combine theminto a single one (Mohri et al, 2002; Mohri et al,1996).
In particular, for two WFSTs R and S, thecomposition T = RoS represents the compositionof relations realized by R and S. The combinationis performed strictly on R?s output and S?s input.
Itmeans for each path in T, mapping string r to strings, there must exist a path mapping r to some stringt in R and a path mapping t to s in S. Decoding onthe combined WFST enables to find the joint opti-mal results for multi-level weighted relations.2.2 Model Unit SelectionThis study primarily takes the person and locationnames as well as the POS information into account.To deal with Chinese OOV words, different fromthe western language in which the phoneme, sylla-ble or sub-word are used as the model units (Bazziand Glass, 2000a; Bazzi and Glass, 2000b; Galescu,2003), Chinese characters are taken as the basicunits.
In general, a person name of Han nation-ality consists of a surname and a given name usu-ally with one or two characters.
Surnames com-monly come from a fixed set that has been histori-cally used.
According to a recent investigation onsurnames involving 296 million people, 4100 sur-names are found, and 129 most used surnames ac-count for 87% (conducted by the Institute of Genet-ics and Developmental Biology, Chinese Academyof Sciences).
In contrast, the characters used ingiven names can be selected freely, and in many situ-ations, some commonly used words may also appearin names, such as ????
(victory) and ????
(theChangjiang River).
Therefore, both Chinese charac-ters and words are considered as model units in thisstudy, and a word re-segmentation process on recog-nition hypotheses is necessary, where an n-gram lan-guage model based on word classes is adopted.2.3 Representation and Integration ofMulti-level KnowledgeIn this work, we ignore the word boundaries of n-best hypotheses and perform a word re-segmentationfor names recognition.
Given an input Chinesecharacter, it is encoded by a finite state acceptorFSAinput.
For example, the input ???????
(while synthesizing molecule) is represented as inFigure 1(a).
Then a dictionary is represented by a50 3210?:??:??:???:?(a)(b)4?:??:?
?:?
?:??:???:??:??:??:?
?:??:???:????
?
?
?1 3654210987Figure 1: (a) is an example of the FSA representing agiven input; (b) is the FST representing a toy dictionary.822transducer with empty weights, denoted as FSTdict.Figure 1(b) illustrates a toy dictionary listed in Ta-ble 1, in which a successful path encodes a mappingfrom a Chinese character sequence to some wordin the dictionary.
In practice, all Chinese charac-Chinese Words English Words??
synthesize??
element??
molecule??
the period of the day from11 p.m.to l a.m.?
together?
presentTable 1: The Toy dictionaryters should appear in the dictionary for further in-corporating models of names.
Then the combinationof FSAinput and FSTdict, FSTseg = FSAinput ?FSTdict, will result in a WFST embracing all thepossible candidate segmentations.
Afterwards an n-gram language model based on word classes is usedto weight the candidate segmentations.
As in Fig-ure 2, a toy bigram with three words is depicted byWFSTn?gram, and the word classes are defined inTable 2.
Here, both in the training and test stages,0w1/un(w1)w2/un(w2)w3/un(w3)4w3/un(w3)?/back(w1)w1/un(w1)?/back(w3)w2/un(w2)?/back(w2)w1/bi(w2,w1)w2/bi(w3,w2)w2/bi(w1,w2)w3/bi(w2,w3)w1/bi(w3,w1)w3/bi(w1,w3)231Figure 2: The WFST representing a toy bigram languagemodel, in which un(w1) denotes the unigram of w1;bi(w1, w2) and back(w1) respectively denotes the bi-gram of w2 and the backoff weight given the word historyw1.the strings of numbers or letters in sentences are ex-Classes Descriptionwi Each word wi listed in the dictionaryCNAME Person names of Han nationalityTNAME Translated person namesLOC Location namesNUM Number expressionsLETTER Letter stringsNON Other non Chinese character stringsBEGIN Beginning of sentenceEND End of sentenceTable 2: The Definition of word classestracted according to the rules, and then substitutedwith the class tags, ?NUM?
and ?LETTER?
respec-tively.
At the same time, the words, such as ???
?and ?A?
?, are replaced with ?NUM??
and ?LET-TER??
in the dictionary.
In addition, name classes,including ?CNAME?, ?TNAME?
and ?LOC?, willbe set according to names recognition.Hidden Markov Models (HMMs) are adoptedboth for names recognition and POS tagging.
Here,each HMM is represented with two WFSTs.
Tak-ing the POS tagging as an example, the toy POSWFSTs with 3 different tags are illustrated in Fig-ure 3.
The emission probability of a word by a POS,(P (word/pos)), is represented as in Figure 3(a),and the bigram transition probabilities between POStags are represented as in Figure 3(b), similar to theword n-gram.
In terms of names recognition, theHMM states correspond to 30 role tags of names,some for model units of Chinese characters, such assurname, the first or second character of a given per-son name with two characters, the first or last charac-ter of a location name and so on, but others for modelunits of words, such as the word before or after aname, the words in a name and so on.
When rec-ognizing the person names, since there is a big dif-ference between the translated names and the namesof Han nationality, two types of person names aremodeled separately, and substituted with two differ-ent class tags in the segmentation language model,as ?TNAME?
and ?CNAME?.
Some rules, whichcan be encoded into WFSTs, are responsible for thetransformation from a role sequence to correspond-ing name class (for example, a role sequence mightconsist of the surname, the first character of the8230pos1/un(pos1)pos2/un(pos2)pos3/un(pos3)pos1/bi(pos2,pos1)pos3/bi(pos2,pos3)pos2/bi(pos1,pos2)pos3/bi(pos1,pos3)pos1/bi(pos3,pos1)pos2/bi(pos3,pos2)(a)(b)word: pos/p(word/pos)3210Figure 3: The toy POS WFSTs.
(a) is the WFST rep-resenting the relationship between the word and the pos;(b) is the WFSA representing the bigram transition prob-abilities between POS tagsgiven name, and the second character of the givenname, which will be transformed to ?CNAME?
inFSTseg).
Hence, taking names recognition into ac-count, a WFST, including all possible segmentationsas well as recognized candidates of names, can beobtained as below, denoted as WFSTwords:FSAinput ?
FSTdict ?WFSTne ?WFSAn?gram(1)POS information is integrated as follows.(?
?WFSTwords) ?WFSTPOS (2)Consequently, the desired analyzer, a combinedWFST that represents multi-level linguistic knowl-edge sources, has been obtained.2.4 Incorporation in LVCSRThe presented analyzer models linguistic knowledgeat different levels, which will be useful to find anoptimal words sequence among a large number ofspeech recognition hypotheses.
Thus in this re-search, the analyzer is incorporated after the firstpass recognition, and the n-best hypotheses arereranked according to the total path scores adjustedwith the analyzer scores as follows.W?
= argmaxW??
?log (PAM (O|W ))+?
?
log (PLM (W ))+?
?
log (PAnalyzer (W ))???
(3)where PAM (O|W ) and PLM (W ) are the acousticand language scores produced in first pass decoding,and PAnalyzer (W ) reflects the linguistic correctnessof one hypothesis scored by the analyzer.
Throughthe reranking paradigm, a new best sentence hypoth-esis is obtained.3 SimulationUnder the unified framework, multi-level linguisticknowledge is represented by the analyzer as men-tioned above.
To guarantee the effectiveness ofthe introduced framework in integrating knowledgesources, the analyzer is evaluated in this section.Then the experiments using an LVCSR system inwhich the analyzer is embedded are performed.3.1 Analyzer EvaluationConsidering the function of the analyzer, cascadedsubtasks of word segmentation, names recognitionand POS tagging can be processed jointly, whilethey are traditionally handled in a pipeline manner.Hence, a comparison between the analyzer and thepipeline system can be used to evaluate the effec-tiveness of the introduced framework for knowledgeintegration.
As illustrated in Figure 4, two systemsbased on the presented analyzer and the pipelinemanner are constructed respectively.The evaluation data came from the People?s Dailyof China in 1998 from January to June (annotated bythe Institute of Computational Linguistics of PekingUniversity1), among which the January to May datawas taken as the training set, and the June data wastaken as the test set (consisted of 21,143 sentencesand about 1.2 million words).
The first two thou-sand sentences from the June data were extractedas the development set, used to fix the compositionweight ?
in equation 2.
A dictionary including about113,000 words was extracted from the training data,1http://icl.pku.edu.cn/icl res/824input d ict ne n gramF SA F ST W F ST W F ST q q qDecodeThe best segmentationposWFSTCCompose omposeDecode DecodePipeline System Presented Analyzeroutput outputFigure 4: The pipeline system vs The analyzerin which a person or location name was accountedas a word in vocabulary, only when the number ofits appearances was no less than three.In Figure 5, the analyzer is compared with thepipeline system, where the analyzer outperforms thepipeline manner on all the subtasks in terms of F1-score metric.
Furthermore to detect the differences,the statistical significance test using approximaterandomization approach (Yeh, 2000) is done on theword segmentation results.
Since there are morethan 21,000 sentences in the test set, which is notappropriate for approximate randomization test, tensets (500 sentences for each) are randomly selectedfrom the test corpus.
For each set, we run 1048576shuffles twice and calculate the significance level p-value according to the shuffled results.
It has beenshown that all p-value are less than 0.001 on the tensets.
Accordingly the improvement is statisticallysignificant.
Actually, this significant improvementis reasonable, since the joint processing avoids errorpropagation and provides the opportunity of shar-ing information between different level knowledgesources.
The superiority of this analyzer also showsthat the integration of multi-level linguistic knowl-edge under the unified framework is effective, whichmay lead to improved LVCSR.95.991.189.996.891.888.590.9889296 Pipeline AnalyzerIntegrated Analyzer83.38084Word Segmentation POS Tagging Person Name Recognition Location Name RecognitionFigure 5: The Performance comparison between thepipeline system and the analyzer.
The system perfor-mances are measured with the F1-score in the tasksof word segmentation, POS tagging, the person namesrecognition and the location names recognition.3.2 Experimental Setup for Mandarin SpeechRecognitionIn the baseline speech recognition system, theacoustic models consisted of context-dependentInitial-Final models, in which the left-to-right modeltopology was used to represent each unit.
Accord-ing to the phonetic structures, the number of statesin each model was set to 2 or 3 for initials, and 4or 5 for tonal finals.
Each state was trained to have32 Gaussian mixtures.
The used 39-dimension fea-ture vector comprised 12 MFCC coefficients, en-ergy, and their first-order and second-order deltas.Since in this work we focused on modeling knowl-edge of language in Mandarin LVCSR, only cleanmale acoustic models were trained with a speechdatabase that contained about 360 hours speech ofover 750 male speakers.
This training data waspicked up from three continuous Mandarin speechcorpora: the 863-I, 863-II and Intel corpora.
Thebrief information about these three speech corporawas listed in Table 3.
As in this work, the eval-uation data was the 1997 HUB-4 Mandarin broad-cast news evaluation data (HUB-4 test set), to bet-ter fit this task, the acoustic models were adaptedby the approach of maximum a posterior (MAP)adaptation.
The adaption data was drawn from theHUB4 training set, excluding the HUB-4 develop-825Corpus Speakers Amount of Speech(hours)863-I (male) 83 56.67863-II(male) 120 78.08Intel (male) 556 227.30total 759 362.05Table 3: The information of the speech training dataing set, where only the cleaned male speech data(data under condition f0 defined as (Doddington,1996)) was used.
The partition for the clean datawas done with the acoustic segmentation softwareCMUseg 0.52 (Siegler et al, 1997), and finally 8.6hours adaptation data was obtained.The language model was a word-based trigrambuilt on 60,000 words entries and trained with a cor-pus about 1.5 billion characters.
The training setconsisted of broadcast news data from the XinhuaNews Agency released by LDC (Xinhua part of Chi-nese Gigaword), seven years data of People?s Dailyof China from 1995 to 2002 released by People?sDaily Online3, and some other data from news web-sites, such as yahoo, sina and so on.In addition, the analyzer incorporated in speechrecognition was trained with a larger corpus fromPeople?s Daily of China, including the data in 1998from January to June and the data in 2000 fromJanuary to November (annotated by the Instituteof Computational Linguistics of Peking University).The December data in 2000 was taken as the devel-opment set used to fix the composition weight ?
inequation 2.3.3 Experimental ResultsIn our experiments, the clean male speech data fromthe Hub-4 test set was used, and 238 sentences werefinally extracted for testing.
The weight of the ana-lyzer was empirically derived from the developmentset, including 649 clean male sentences from the de-vSet of HUB-4 Evaluation.
The recognition resultsare shown in Table 4.
The baseline system has acharacter error rate (CER) of 14.85%.
When the an-alyzer is incorporated, a 9.9% relative reduction is2Acoustic segmentation software downloaded fromhttp://www.nist.gov/speech/tools/CMUseg 05targz.htm.3http://www.people.com.cnSystem Err.
Sub.
Del.
Ins.Baseline 14.85 13.02 0.76 1.07Analyzer 13.38 11.78 1.00 0.60incorporationTable 4: The Speech recognition resultsachieved.
Furthermore, we ran the statistical signif-icance test to detect the performance improvement,in which the approximate randomization approach(Yeh, 2000) was modified to output the significancelevel, p-value, for the CER metric.
The p-levels pro-duced through two rounds of 1048576 shuffles are0.0058 and 0.0057 respectively, both less than 0.01.Thus the performance improvement imposed by theutilization of the analyzer is statistically significant.4 ConclusionAddressing the challenges of Mandarin large vocab-ulary continuous speech recognition task, within theunified framework of WFSTs, this study presentsan analyzer integrating multi-level linguistic knowl-edge.
Unlike other methods, model units, such ascharacters and words, can be chosen freely in thisapproach since multi-level knowledge sources aremodeled independently.
As a consequence, the fi-nal analyzer can be derived from the combinationof better optimized models based on proper modelunits.
Along with two level knowledge sources, i.e.,the person and location names as well as the part-of-speech information, the analyzer is built and evalu-ated by a comparative simulation.
Further evaluationis also conducted on an LVCSR system in which theanalyzer is embedded.
Experimental results consis-tently reveal that the approach is effective, and suc-cessfully improves the performance of speech recog-nition by a 9.9% relative reduction of character errorrate on the HUB-4 test set.
Also, the unified frame-work based approach provides a property of integrat-ing additional linguistic knowledge flexibly, such asorganization name and syntactic structure.
Further-more, the presented approach has a benefit of ef-ficiency that additional efforts are not required fordecoding as new knowledge comes, since all knowl-edge sources are finally encoded into a single WFST.826AcknowledgmentsThe work was supported in part by the NationalNatural Science Foundation of China (60435010;60535030; 60605016), the National High Tech-nology Research and Development Program ofChina (2006AA01Z196; 2006AA010103), the Na-tional Key Basic Research Program of China(2004CB318005), and the New-Century TrainingProgram Foundation for the Talents by the Ministryof Education of China.ReferencesSteven Abney.
1996.
Partial parsing via finite-state cas-cades.
Natural Language Engineering, 2(4):337?344.Issam Bazzi and James R. Glass.
2000a.
Modeling out-of-vocabulary words for robust speech recognition.
InProc.
of 6th International Conference on Spoken Lan-guage Processing, pages 401?404, Beijing, China, Oc-tober.Issam Bazzi and James Glass.
2000b.
Heterogeneouslexical units for automatic speech recognition: prelim-inary investigations.
In Proc.
of ICASSP, pages 1257?1260, Istanbul, Turkey, June.Issam Bazzi and James Glass.
2001.
Learning unitsfor domain-independent out-of-vocabulary word mod-elling.
In Proc.
of EUROSPEECH, pages 61?64, Aal-borg, Denmark, September.Eugene Charniak.
2001.
Immediate-head parsing forlanguage models.
In Proc.
of ACL, pages 116?123,Toulouse, France, July.Ciprian Chelba, David Engle, Frederick Jelinek, Vic-tor Jimenez, Sanjeev Khudanpur, Lidia Mangu, HarryPrintz, Eric Ristad, Ronald Rosenfeld, Andreas Stol-cke, and Dekai Wu.
1997.
Structure and performanceof a dependency language model.
In Proc.
of EU-ROSPEECH, pages 2775?2778, Rhodes, Greece.Ciprian Chelba.
2000.
Exploiting Syntactic Structure forNatural Language Modeling.
Ph.D. thesis, Johns Hop-kins University.Isabel Trancoso Diamantino Caseiro.
2002.
Using dy-namic WFST composition for recognizing broadcastnews.
In Proc.
of ICSLP, pages 1301?1304, Denver,Colorado, USA, September.George Doddington.
1996.
The 1996 hub-4 annotation specification for evaluation ofspeech recognition on broadcast news.
Inftp://jaguar.ncsl.nist.gov/csr96/h4/h4annot.ps.N.
Friburger and D. Maurel.
2004.
Finite-state trans-ducer cascades to extract named entities in texts.
The-oretical Computer Science, 313(1):93?104.Lucian Galescu.
2003.
Recognition of out-of-vocabularywords with sub-lexical language models.
In Proc.of EUROSPEECH, pages 249?252, Geneva, Switzer-land, September.Thomas Hain, Lukas Burget, John Dines, Giulia Garau,Martin Karafiat, Mike Lincoln, Jithendra Vepa, andVincent Wan.
2006.
The AMI meeting transcriptionsystem: Progress and performance.
In Proc.
of RichTranscription 2006 Spring Meeting Recognition Eval-uation.Peter A. Heeman.
1998.
Pos tagging versus classes inlanguage modeling.
In Proc.
of the 6th Workshop onvery large corpora, pages 179?187, Montreal, Canada.Takaaki Hori, Chiori Hori, Yasuhiro Minami, and At-sushi Nakamura.
2007.
Efficient WFST-based one-pass decoding with on-the-fly hypothesis rescoring inextremely large vocabulary continuous speech recog-nition.
IEEE Transactions on audio, speech, and lan-guage processing, 15(4):1352?1365.Xinhui Hu, Hirofumi Yamamoto, Genichiro Kikui, andYoshinori Sagisaka.
2006.
Language modeling ofchinese personal names based on character units forcontinuous chinese speech recognition.
In Proc.
ofINTERSPEECH, pages 249?252, Pittsburgh, USA,September.Mark Johnson.
2001.
Joint and conditional estimation oftagging and parsing models.
In Proc.
of ACL, pages322 ?
329, Toulouse, France.G.
Maltese, P. Bravetti, H. Cr?py, B. J. Grainger, M. Her-zog, and F. Palou.
2001.
Combining word- andclass-based language models: A comparative study inseveral languages using automatic and manual word-clustering techniques.
In Proc.
of EUROSPEECH,pages 21?24, Aalborg, Denmark, September.Ciro Martins, Antonio Texeira, and Joao Neto.
2006.Dynamic vocabulary adaptation for a daily and real-time broadcast news transcription system.
In Proc.
ofSpoken Language Technology Workshop, pages 146?149, December.Mehryar Mohri, Fernando Pereira, and Michael Riley.1996.
Weighted automata in text and speech process-ing.
In ECAI-96 Workshop.Mehryar Mohri, Fernando Pereira, and Michael Riley.2000.
The design principles of a weighted finite-state transducer library.
Theoretical Computer Sci-ence, 231(1):17?32.Mehryar Mohri, Fernando Pereira, and Michael Ri-ley.
2002.
Weighted finite-state transducers inspeech recognition.
Computer Speech and Language,16(1):69?88.Mehrya Mohri.
1997.
Finite-state transducers in lan-guage and speech processing.
Computational Linguis-tics, 23(2):269?311.827Christan Raymond, Fre de ric Be chet, Renato D. Mori,and Ge raldine Damnati.
2006.
On the use of finitestate transducers for semantic interpretation.
SpeechCommunication, 48(3-4):288?304.Brian Roark.
2001.
Probabilistic top-down parsingand language modeling.
Computational Linguistics,27(2):249?276.Christer Samuelsson and Wolfgang Reichl.
1999.
Aclass-based language model for large-vocabularyspeechrecognition extracted from part-of-speechstatistics.
In Proc.
of ICASSP, pages 537?540,Phoenix, Arizona, USA, March.George Saon, Geoffrey Zweig, Brain KingsBury, LidiaMangu, and Upendra Canudhari.
2003.
An architec-ture for rapid decoding of large vocabulary conversa-tional speech.
In Proc.
of Eurospeech, pages 1977?1980, Geneva, Switzerland, September.Matthew A. Siegler, Uday Jain, Bhiksha Raj, andRichard M. Stern.
1997.
Automatic segmentation,classification and clustering of broadcast news audio.In Proc.
of DARPA Speech Recognition Workshop,pages 97?99, Chantilly, Virginia, February.Daniel Willett Takaaki Hori and Yasuhiro Minami.2003.
Language model adaptation using WFST-basedspeaking-style translation.
In Proc.
of ICASSP, pagesI.228?I.231, Hong Kong, April.Koichi Tanigaki, Hirofumi Yamamoto, and YoshinoriSagisaka.
2000.
A hierarchical language model incor-porating class-dependent word models for oov wordsrecognition.
In Proc.
of 6th International Conferenceon Spoken Language Processing, pages 123?126, Bei-jing, China, October.Hajime Tsukada and Masaaki Nagata.
2004.
Efficientdecoding for statistical machine translation with a fullyexpanded WFST model.
In Proc.
of EMNLP, pages427?433, Barcelona, Spain, July.Wen Wang and Mary P. Harper.
2002.
The superarv lan-guage model: investigating the effectiveness of tightlyintegrating multiple knowledge sources.
In Proc.
ofEMNLP, pages 238?247, Philadelphia, USA, July.Wen Wang and Dimitra Vergyri.
2006.
The use of wordn-grams and parts of speech for hierarchical clusterlanguage modeling.
In Proc.
of ICASSP, pages 1057?1060, Toulouse, France, May.Wen Wang, Andreas Stolcke, and Mary P. Harper.
2004.The use of a linguistically motivated language modelin conversational speech recognition.
In Proc.
ofICASSP, pages 261?264, Montreal, Canada, May.Alexander Yeh.
2000.
More accurate tests for the sta-tistical significance of result differences.
In Proc.
ofCOLING, pages 947?953, Saarbr?cken, August.828
