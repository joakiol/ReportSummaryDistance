Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,pages 1091?1102, Dublin, Ireland, August 23-29 2014.A Self-adaptive Classifier for Efficient Text-stream ProcessingNaoki YoshinagaInstitute of Industrial Science,the University of Tokyo,Meguro-ku, Tokyo 153-8505, Japanynaga@tkl.iis.u-tokyo.ac.jpMasaru KitsuregawaInstitute of Industrial Science,the University of Tokyo,Meguro-ku, Tokyo 153-8505, JapanandNational Institute of Informatics,Chiyoda-ku, Tokyo 101-8430, Japankitsure@tkl.iis.u-tokyo.ac.jpAbstractA self-adaptive classifier for efficient text-stream processing is proposed.
The proposed classifieradaptively speeds up its classification while processing a given text stream for various NLP tasks.The key idea behind the classifier is to reuse results for past classification problems to solveforthcoming classification problems.
A set of classification problems commonly seen in a textstream is stored to reuse the classification results, while the set size is controlled by removing theleast-frequently-used or least-recently-used classification problems.
Experimental results withTwitter streams confirmed that the proposed classifier applied to a state-of-the-art base-phrasechunker and dependency parser speeds up its classification by factors of 3.2 and 5.7, respectively.1 IntroductionThe rapid growth in popularity of microblogs (e.g., Twitter) is enabling more and more people to in-stantly publish their experiences or thoughts any time they want from mobile devices.
Since informationin text posted by hundreds of millions of those people covers every space and time in the real world,analyzing such a text stream tells us what is going on in the real world and is therefore beneficial for re-ducing damage caused by natural disasters (Sakaki et al., 2010; Neubig et al., 2011a; Varga et al., 2013),monitoring political sentiment (Tumasjan et al., 2010) and disease epidemics (Aramaki et al., 2011), andpredicting stock market (Gilbert and Karahalios, 2010) and criminal incident (Wang et al., 2012).Text-stream processing, however, faces a new challenge; namely, the quality (content) and quantity(volume of flow) changes dramatically, reflecting a change in the real world.
Current studies on pro-cessing microblogs have focused mainly on the difference between the quality of microblogs (or spokenlanguages) and news articles (or written languages) (Gimpel et al., 2011; Foster et al., 2011; Ritter et al.,2011; Han and Baldwin, 2011), and they have not addressed the issue of so-called ?bursts?
that increasethe volume of text.
Although it is desirable to use NLP analyzers with the highest possible accuracy forprocessing a text stream, high accuracy is generally attained by costly structured classification or classi-fication with rich features, typically conjunctive features (Liang et al., 2008).
It is therefore inevitable totrade accuracy for speed by using only a small fraction of features to assure real-time processing.In this study, the aforementioned text-quantity issue concerning processing a text stream is addressed,and a self-adaptive algorithm that speeds up an NLP classifier trained with many conjunctive features (orwith a polynomial kernel) for a given text stream is proposed and validated.
Since globally-observableevents such as natural disasters or sports events incur a rapid growth in the number of posts (Twitter, Inc.,2011), a text stream is expected to contain similar contents concerning these events when the volume offlow in a text stream increases.
To adaptively speed up the NLP classifier, the proposed algorithm thusenumerates common classification problems from seen classification problems and keeps their classifi-cation results as partial results for use in solving forthcoming classification problems.The proposed classifier was evaluated by applying it to streams of classification problems generatedduring the processing of the Twitter streams on the day of the 2011 Great East Japan Earthquake and onanother day in March 2012 using a state-of-the-art base-phrase chunker (Sassano, 2008) and dependencyparser (Sassano, 2004), and the obtained results confirm the effectiveness of the proposed algorithm.This work is licenced under a Creative Commons Attribution 4.0 International License.
Page numbers and proceedings footerare added by the organizers.
License details: http://creativecommons.org/licenses/by/4.0/10912 Related workA sentence is the processing unit used for fundamental NLP tasks such as word segmentation, part-of-speech tagging, phrase chunking, syntactic parsing, and semantic role labeling.
Most efficient algorithmssolving these tasks thus aim at speeding up the processing based on this unit (Kaji et al., 2010; Koo etal., 2010; Rush and Petrov, 2012), and few studies have attempted to speed up the processing of a giventext (a set of sentences) as a whole.
In the following, reported algorithms that adaptively speed up NLPanalyzers for a given text are introduced.A method of speeding up a classifier trained with many conjunctive features by using precomputedresults for common classification problems was proposed by Yoshinaga and Kitsuregawa (2009; 2012).It solves classification problems that commonly appear in the processing of a large amount of text inadvance and stores the results in a trie, so that they can be reused as partial results for solving new classi-fication problems.
This method was reported to achieve speed-up factors of 3.3 and 10.6 for base-phrasechunking and dependency parsing, respectively.
An analogous algorithm for integer linear program (ILP)used to solve structured classification was proposed by Srikumar, Kundu, and Roth (2012; 2013).
Thealgorithm was reported to achieve speed-up factors of 2.6 and 1.6 for semantic role labeling and entity-relation extraction, respectively.
Although these two algorithms can be applied to various NLP tasks thatcan be solved by using a linear classifier or an ILP solver, how effective they are for processing a textstream is not clear.A method of feature sharing for beam-search incremental parsers was proposed by Goldberg et al.(2013).
Motivated by the observation that beam parsers solve similar classification problems in differentparts of the beam, this method reuses partial results computed in the previous beam items.
It reportedlyachieved a speed-up factor of 1.2 for arc-standard and arc-eager dependency parsers.
The key differencesbetween the method proposed in this study and their feature-sharing method are twofold.
First, the featuresharing in Goldberg et al.
(2013) is performed in a token-wise manner in the sense that a key to retrieve acached result is represented by a bag of tokens that invoke features, which manner prevents fine-grainedcaching.
Second, the feature sharing is dynamically performed during parsing, but the cached results arecleared after processing each sentence.An adaptive pruning method for fast HPSG parsing was proposed by van Noord (2009).
This methodpreprocesses a large amount of text by using a target parser to collect derivation steps that are unlikely tocontribute to the best parse, and it speeds up the parser by filtering out those unpromising derivation steps.Although this method was reported to attain a speed-up factor of four while keeping parsing accuracy, itneeds to be tuned to trade parsing accuracy and speed for each domain.
It is difficult to derive the truepotential of their method in regard to processing a text stream whose domain shifts from time to time.It has been demonstrated by Wachsmuth et al.
(2011) that tuning a pipeline schedule of an informationextraction (IE) system improves the efficiency of the system.
Furthermore, the self-supervised learningalgorithm devised by Wachsmuth et al.
(2013) predicts the processing time for each possible pipelineschedule of an IE system, and the prediction is used to adaptively change the pipeline schedule for a giventext stream.
This method and the proposed method for speeding up an NLP classifier are complementary,and a combination of both methods is expected to synergistically speed up various NLP-systems.In this study, based on the classifier proposed by Yoshinaga and Kitsuregawa (2009), a self-adaptiveclassifier that enumerates common classification problems from a given text stream and reuses theirresults is proposed.
As a result, the proposed classifier adaptively speeds up the classification of forth-coming classification problems.3 PreliminariesAs the basis of the proposed classifier, the previously-presented classifier that uses results of commonclassification problems (Yoshinaga and Kitsuregawa, 2009) is described as follows.
This base classifiertargets a linear classifier trained with many conjunctive features (including one converted from a classifiertrained with polynomial kernel (Isozaki and Kazawa, 2002)) that are widely used for many NLP tasks.Although this classifier (and also the one proposed in this paper) can handle a multi-class classificationproblem, a binary classification problem is assumed here for brevity.1092A binary classifier such as a perceptron and a support vector machine determines label y ?
{+1,?1}of input classification problem x by using the following equation (from which the bias term is omittedfor brevity):m(x;?,w) =wT?
(x) =?iwi?i(x) (1)y ={+1 (m(x;?,w) ?
0)?1 (m(x;?,w) < 0).
(2)Here, ?iis a feature function, wiis a weight for ?iobtained as a result of training, and m(x;?,w) is amargin between x and the separating hyperplane.In most NLP tasks, feature functions are mostly indicator (or binary) functions that typically representparticular linguistic constraints.
Here, feature functions are assumed to be indicator functions that return{0, 1}, and margin m(x;?,w) is represented by the following equation:m(x;?,w) =?iwi?i(x) =?i,?i(x)=1wi.
(3)Feature function ?iis hereafter referred to as feature ?i; when ?i(x) = 1 for given x, x is said to?include?
feature ?ior feature ?iis ?active?
in x (denoted as ?i?
x).
Having the number of activefeatures, |?
(x)| ?
|{?i| ?i?
x}|, Eq.
3 requires O(|?
(x)|) when the weights for the active featuresare summed up.To speed up the summation in Eq.
3, classification results for some classification problems xcareprecomputed as Mxc?
m(xc;?,w) in advance, and then these precomputed results are reused aspartial results for solving an input classification problem, x:m(x;xc,?,w) =Mxc+?i,?i?x,?i/?xcwi(4)where ??j?
xc, ?j?
x.Note that for Eq.
4 to be computed faster than Eq.
3, Mxcmust be retrieved in time less thanO(|?
(xc)|).It is actually possible when xcincludes conjunctive feature ?i,j(xc) = ?i(xc)?j(xc).
If it is necessaryto retrieve margin Mxcprecomputed for xcincluding ?i, ?j, and ?i,j, it is necessary to check onlynon-conjunctive (or primitive) features ?iand ?j, since ?i,jis active whenever ?iand ?jare active (sochecking ?i,jcan be skipped).
The second term of Eq.
4 sums up the weights of the remaining featuresthat are not included in xcbut are included in x.
For example, under the assumption that x includesfeatures ?i, ?j, ?k, ?i,j, ?i,k, and ?j,kand that margin Mxchas been obtained for xc(including ?i,?j, and ?i,j), five features must be checked (two to retrieve Mxcand three to sum up the weights ofthe remaining features ?k, ?i,kand ?j,k) by using Eq.
4.
On the other hand, to compute m(x;?,w) byEq.
3, the weights for the six features must be checked.To maximize the speed-up obtained by Eq.
4, reuse of margin Mxcof common classification problemxcshould minimize the number of remaining features included only in x.
In other words, xcshouldbe as similar to x as possible (ideally, xc= x).
It is not, however, realistic to precompute marginMx?
m(x;?,w) for every possible classification problem x since it requires O(2|?
?|) space where|?
?| is the number of primitive features (??
?
?)
and |?
?| is usually more than 10,000 in NLP tasksdue to lexical features.
Yoshinaga and Kitsuregawa (2009) therefore preprocess a large amount of textto enumerate possible classification problems, and select common classification problems, Xc?
2?
?,according to their probability and the reduction in the number of features to be checked by Eq 4.Yoshinaga and Kitsuregawa (2009) then represent the common classification problems xc?
Xcbysequences of active primitive feature indices, and store those feature (index) sequences as keys in aprefix trie with precomputed margin M(xc) as their values.
To reuse a margin of common classificationproblem that is similar to input x in Eq.
4, features are ordered according to their frequency to form afeature sequence of xc.
A longest-prefix search for the trie thereby retrieves a common classificationproblem similar to the input classification problem in linear time with respect to the number of primitivefeatures in xc, O(|??
(xc)|).1093Algorithm 1 A self-adaptive classifier for enumerating common classification problemsINPUT: x, ?, ??
?
?,w ?
R|?|, Xc?
2?
?, k > 0OUTPUT: m(x;?,w) ?
R, Xc1: INITIALIZE: xcs.t.
??
(xc) = 0, Mxc?
02: repeat3: xoldc?
xc4: ?
?i= argmax??i?x,??i/?xcFREQ(?
?i) (extract a primitive feature according to its frequency)5: ??i(xc)?
1 (construct a new common-classification problem)6: if xc/?
Xcthen7: Mxc?
m(xc;xoldc,?,w) (compute margin by using Eq.
4)8: if |Xc| = k then9: Xc?
Xc?
{USELESS(Xc)}10: Xc?
Xc?
{xc}11: until ??
(xc) 6= ??
(x)12: return m(x;?,w) = Mxc, Xc4 Proposed methodThe classifier described in Section 3 is extended so that it dynamically enumerates common classificationproblems from a given text stream1 to adaptively speed up the classification.
This classification ?speedup?
faces two challenges: which (partial) classification problems should be chosen to reuse their resultsfrom a given stream of classification problems, and how to efficiently maintain the extracted commonclassification problems.
These two challenges are addressed in Sections 4.1 and 4.2, respectively.4.1 Enumerating common classification problems dynamically from a text streamAlthough Yoshinaga and Kitsuregawa (2009) select common classification problems according to theirprobability, such statistics cannot be known before a target text stream is entirely seen.
A set of commonclassification problems was thus kept updated adaptively while processing a text stream; that is, classifi-cation problems are added when they will be useful, while they are removed when they will be useless,so that the number of common classification problems, |Xc|, does not exceed a pre-defined threshold, k.Algorithm 1 depicts the proposed self-adaptive classifier for enumerating common classification prob-lems from an input classification problem, x.
To incrementally construct common classification problemxc(Line 4-5), the algorithm extracts the primitive features (?
?i) included in x one by one according totheir probability of appearing in the training data of the classifier.
When the resulting xcis included inthe current set of common classification problems, Xc, stored margin Mxcis reused.
Otherwise, marginMxc= m(xc;xoldc,?,w) is computed by using Eq.
4 (Line 7), and xcis registered in Xcas a newcommon classification problem (Line 10).An important issue is how to define function USELESS, which selects a common classification problemthat will not contribute to speeding up the forthcoming classification, when the number of commonclassification problems, |Xc|, reaches the pre-defined threshold k. To address this issue, the followingtwo policies (designed originally for CPU caching) are proposed and compared in terms of the efficiencyof the classifier in experiments:Least Frequently Used (LFU) This policy counts frequency of common classification problems in aseen text stream, and it maintains only the top-k common classification problems by removing theleast-common classification problem from Xc:USELESSLFU(Xc) = argminxc?XcFREQ(xc) (5)1More precisely, a stream of classification problems generated during the analysis of a text stream.1094A space-saving algorithm, (Metwally et al., 2005), is used to efficiently count the approximatedfrequency of k classification problems at most and to remove the common classification problemrejected by the space-saving algorithm.Least Recently Used (LRU) When the volume of flow in a text stream rapidly increases, it is likely torelate to a burst of a certain topic.
To exploit this characteristics, this policy preserves k commonclassification problems whose results are most recently reused:USELESSLRU(Xc) = argminxc?XcTIME(xc) (6)Common classification problems are associated with the last timing when their results are reused,and the least-recently-reused common classification problem is removed when |Xc| = k. To realizethis policy, a circular linked-list of size k is used to maintain precomputed results, and the oldestelement is just overwritten while the corresponding classification problem is removed.Fixed threshold k is used throughout the processing of a text stream, and its impact on classificationspeed was evaluated by experiments.Since Algorithm 1 naively constructs common classification problems using all the active primitivefeatures in input classification problem x, it might repeatedly add and remove classification problemsthat include rare primitive features such as lexical features.
This will incur serious overhead costs.
Toavoid this situation, the margin computation is terminated as soon as it is determined that the remainingcomputation does not change the sign of margin (namely, classification label y) of x.When x and xcare given, lower- and upper-bounds of m(x;?,w) can be computed by accumulatingbounds of a partial margin computed by adding remaining active primitive features, {??j?
x | ??j/?
xc},one by one to xc.
It is assumed that primitive feature ?
?iis newly activated in xcand xoldcrefers to xcwithout ?
?ibeing activated.
The partial margin, m(xc;?,w)?m(xoldc;?,w), is computed by summingup the weights of primitive feature ?
?iand conjunctive features that are composed of ?
?iand one or moreprimitive features ??j?
xoldc.
This partial margin is upper- and lower-bounded as follows:m(xc;?,w)?m(xoldc;?,w)?max(wmini|{?j?
xc| ?j/?
xoldc}|,W?i) (7)m(xc;?,w)?m(xoldc;?,w)?min(wmaxi|{?j?
xc| ?j/?
xoldc}|,W+i), (8)where wminiand wmaxirefer to minimum and maximum weights among all the features regarding ?
?i,while W+iand W?irefer to summations of all the features regarding ?
?iwith positive and negativeweights, respectively; that is, this upper or lower-bound is computed by assuming all the features regard-ing ?
?ito have a maximum or minimum weight (each bounded by W+ior W?i).
Accumulating thesebounds for each remaining primitive feature makes it possible to obtain the bounds of m(x;?,w) andthereby judge whether the sign of the margin can be changed by processing the remaining features.4.2 Maintaining common classification problems with dynamic double-array trieTo maintain the enumerated common classification problems, a double-array trie (Aoe, 1989) is applied.The trie associates common classification problem xcwith unique index i(1 ?
i ?
k), which is fur-ther associated with computed margin Mxcand frequency or access time as described in Section 4.1.Although a double-array trie provides an extremely fast look-up, it had been considered that updateoperation (adding a new key to a double-array trie) is slow.
However, in a recent study (Yata et al.,2009), the update speed of a double-array trie approaches that of a hash table.
In the following section, adouble-array trie similar to that of Yata et al.
(2009) is used to maintain common classification problems.Efficient dynamic double-array trie with deletionA double-array trie (Aoe, 1989) and an algorithm that allows a fast update (Yata et al., 2009) are brieflyintroduced in the following.
A double array is a data structure for a compact trie, which consists of twoone-dimensional arrays called BASE and CHECK.
In a double-array trie, each trie node occupies oneelement in BASE and CHECK, respectively.2 For each node, p, BASE stores the offset address of its child2Although the original double-array (Aoe, 1989) realizes a minimal-prefix trie by using another array (called TAIL) to storesuffix nodes with only one child, TAIL is not adopted here since it is difficult to support space-efficient deletion with TAIL.1095nodes, so a child node takes the address c = BASE[p] XOR3 l when the node is traversed from p by label l.For each node, c, CHECK stores the address of its parent node, p, and is used to confirm the validity ofthe traversal by checking whether CHECK[c] = p is held after the node is reached by c = BASE[p] XOR l.Adding a new node to a trie could cause a conflict, meaning that the newly added node could beassigned to the address taken by an existing node in the trie.
In such a case, it is necessary to collect allthe sibling nodes of either the newly added node or the existing node that took the conflicting address,and then relocate either branching (with a lower number of child nodes) to empty addresses that are nottaken by other nodes in the trie.
This relocation is time-consuming, and is the reason for the slow update.To perform this relocation quickly, Yata et al.
(2009) introduced two additional one-dimensional ar-rays, called NLINK (node link) and BLOCK.
For each node, NLINK stores the label needed to reach itsfirst child and the label needed to reach from its parent the sibling node next to the node.
It therebymakes it possible to quickly enumerate the sibling nodes for relocation.
BLOCK stores information onempty addresses within each 256 consecutive addresses called a block4 in BASE and CHECK.
Each blockis classified into three types, called ?full,?
?closed?
and ?open.?
Full blocks have no empty addresses andare excluded from the target of relocation.
Closed blocks have only one empty address or have failed tobe relocated more times than a pre-specified threshold.
Open blocks are other blocks, which have morethan one empty address.
The efficient update of the trie is enabled by choosing appropriate blocks torelocate a branching; a branching with one child node is relocated to a closed block, while a branchingwith multiple child nodes is relocated to an open block.The above-described double-array trie was modified to support a deletion operation, which simplyregisters to each block empty addresses resulting from deletion.
In consideration that a new key (commonclassification problem) will be stored immediately after the deletion (Line 10 in Algorithm 1), the double-array trie is not packed as in Yata et al.
(2007) after a key is deleted.Engineering a double-array trie to reduce trie sizeTo effectively maintain common classification problems in a trie, it is critical to reduce the number oftrie nodes accessed in look-up, update, and deletion operations.
The number of trie nodes was thereforereduced as much as possible by adopting a more compact representation of keys (common classificationproblems) and by elaborating the way to store values for the keys in the double-array trie.Gap-based key representation To compress representations of common classification problems (fea-ture sequences) in the trie, frequency-based indices are allocated to primitive features (Yoshinaga andKitsuregawa, 2010).
A gap representation (used to compress posting lists in information retrieval (Man-ning et al., 2008, Chapter 5)) is used to encode feature sequences.
Each feature index is replaced with agap from the preceding feature index (the first feature index is used as is).
Each gap is then encoded byvariable-byte coding (Williams and Zobel, 1999) to obtain shorter representations of feature sequences.A reduced double-array trie The standard implementation of a double-array trie stores an (integer)index with a key at a child node (value node) traversed by a terminal symbol ?\0?
(or an alphabetnot included in a key, e.g., ?#?)
from the node reached after reading the entire key (Yoshinaga andKitsuregawa, 2009; Yasuhara et al., 2013).
However, when a key is not a prefix to the other keys, thevalue node has no sibling node, so a value can be directly embedded on the BASE of the node reachedafter reading the entire key instead of the offset address of the child (value) node.
All the value nodes forthe longest prefixes are thereby eliminated from the trie.
The resulting double-array trie is referred to asa reduced double-array trie.These two tricks reduce the number of trie nodes (memory usage), and make the trie operations faster.A reduced double-array trie is also used to compactly store the weights of conjunctive features, as de-scribed in Yoshinaga and Kitsuregawa (2010).
Interested readers may refer to cedar,5 open-source soft-ware of a dynamic double-array trie, for further implementation details of the reduced double-array trie.3The original double-array (Aoe, 1989) uses addition instead of XOR operation to obtain a child address.4Note that the XOR operation guarantees that all the child nodes are located within a certain block i (assuming 1 byte (0-255)for each label, l, child nodes of a node, p, are all located in addresses (256i ?
c = BASE[p] XOR l < 256(i + 1)).5http://www.tkl.iis.u-tokyo.ac.jp/?ynaga/cedar/1096base-phrase chunking dependency parsingNumber of features |?| 645,951 2,084,127Number of primitive features |?
?| 11,509 27,063Accuracy (partial) 99.01% 92.23%Accuracy (complete) 94.16% 58.38%Table 1: Model statistics for base-phrase chunking and dependency parsing.5 ExperimentsThe proposed self-adaptive classifier was experimentally evaluated by applying it to streams of classifi-cation problems.
The streams of classification problems were generated by processing Twitter streamsusing a state-of-the-art base-phrase chunker and dependency parser.
All experiments were conductedwith an Intel R?
CoreTM i7-3720QM 2.6-GHz CPU server with 16-GB main memory.5.1 SetupSince March 11, 2011 (the day of the Great East Japan Earthquake; ?3.11 earthquake?
hereafter), Twitterstreams were crawled by using Twitter API.6 Tweets from famous Japanese users were crawled first.Next, timelines of those users were obtained.
Then, the set of users were repeatedly expanded by tracingretweets and mentions in their timelines to collect as many tweets as possible.
In the following experi-ments, two sets of 24-hour Twitter streams from the crawled tweets were used.
The first Twitter streamwas taken from the day of 3.11 earthquake (12:00 on Friday, March 11, 2011 to 12:00 on Saturday,March 12, 2011), and the second one was taken from the second weekend in March, 2012 (12:00 onFriday, March 9, 2012 to 12:00 on Saturday, March 10, 2012).
The first Twitter stream is intended toevaluate the classifier performance on days with a significant, continuous burst, while the second one is toevaluate the performance on days without such a burst.
No special events, other than a small earthquake(02:25 on March 10), occurred from March 9 to 10, 2012.
Because the input to base-phrase chunkingand dependency parsing is a sentence, each post was split by using punctuations as clues.Although it might be better to evaluate the chunking and parsing speed with the proposed classifierfor a text stream, the classification speed was evaluated for streams of classification problems generatedin processing the Twitter streams by a deterministic base-phrase chunker (Sassano, 2008) and a shift-reduce dependency parser (Sassano, 2004), which are implemented in J.DepP.12 Note that the chunkerand parser are known to spend most of the time for classification (Yoshinaga and Kitsuregawa, 2012),and reducing the classification time leads to efficient processing of Twitter streams.The base-phrase chunker processes each token in a sentence identified by a morphological analyzer,MeCab,7 and judges whether the token is the beginning of a base-phrase chunk in Japanese (called abunsetsu8) or not.
The shift-reduce dependency parser processes each chunk in the chunked sentencesand determines whether the head candidate chosen by the parser is correct head or not.The classifiers for base-phrase chunking and dependency parsing were trained by using a variant of apassive-aggressive algorithm (PA-I) (Crammer et al., 2006) with a standard split9 of the Kyoto-UniversityText Corpus (Kawahara et al., 2002) Version 4.0.10 A third-order polynomial kernel was used to considercombinations of up-to three primitive features.
The features used for training the classifiers were identicalto those implemented in J.DepP.
The polynomial kernel expanded (Kudo and Matsumoto, 2003) wasused to make the number of resulting conjunctive features tractable without harming the accuracy.Table 1 lists the statistics of the models trained for chunking and parsing.
In Table 1, ?accuracy(partial)?
is the ratio of chunks (or dependency arcs) correctly identified by the chunker (or the parser),while ?accuracy (complete)?
is the exact-match accuracy of complete chunks (or dependency arcs) in asentence.
The accuracy of the resulting parser on the standard split was better than any published results6https://dev.twitter.com/docs/api7http:://mecab.sourceforge.net/8A bunsetsu is a linguistic unit consisting of one or more content words followed by zero or more function words.924,263, 4,833 and 9,284 sentences (234,685, 47,571 and 89,874 base phrases) for training, development, and testing.10http://nlp.ist.i.kyoto-u.ac.jp/EN/index.php?Kyoto%20University%20Text%20Corpus1097March 11-12, 2011 March 9-10, 2012Number of posts 9,291,767 6,360,392Number of posts/s 108 74Number of sentences 24,722,596 13,521,196Number of classification problems (chunking) 220,490,401 109,452,133Number of classification problems (parsing) 70,096,105 34,380,385Table 2: Twitter stream used for evaluation.012345612:0014:0016:0018:0020:0022:0000:0002:0004:0006:0008:0010:0012:00Relative#posts/min.timeMarch 11-12, 2011March 9-10, 2012(a) Change in number of posts per min.012345612:0014:0016:0018:0020:0022:0000:0002:0004:0006:0008:0010:0012:00Relative#posts/min.timeMarch 11-12, 2011March 9-10, 2012(b) Change in number of classifications per min.
(parsing)Figure 1: Volume of flow of Twitter streams from March 11 to 12, 2011 and from March 9 to 10, 2012.March 11-12, 2011 March 9-10, 2012method space speed ratio space speed ratio[MiB] [ms/sent.]
[MiB] [ms/sent.
]baseline 12.01 0.0221 1.00 12.01 0.0188 1.00Y&K ?09 30.46 0.0118 1.87 30.46 0.0112 1.69proposed k = 216 18.05 0.0092 2.40 17.93 0.0098 1.93(LFU) k = 220 90.70 0.0088 2.51 90.78 0.0089 2.12k = 224 463.04 0.0081 2.73 473.60 0.0076 2.48proposed k = 216 17.32 0.0086 2.57 17.32 0.0093 2.02(LRU) k = 220 85.89 0.0077 2.88 86.09 0.0085 2.22k = 224 399.17 0.0070 3.16 409.59 0.0068 2.76Table 3: Experimental results obtained with the reduced double array trie: base phrase chunking.for this dataset other than those reported for a parser based on ?stacking?
(Iwatate, 2012).11Table 2 lists the detail of the Twitter streams used for evaluating the proposed classifier.
Figures 1(a)and 1(b) show the change in the number of posts and classifications for parsing per minute, when theaverage number of posts and classifications per minute before the 3.11 earthquake is counted as one,respectively.
The dataset shows a rapid growth in the number of posts after the 3.11 earthquake occurred(14:46:18).
This event also incurs a rapid growth in the number of classifications for parsing.
Althoughspace limitations precluded the number of classifications for chunking, it had the same tendency as forparsing.
It should be noted that the official retweets (reposts) occupied 25.8% (2,394,025) and 8.5%(542,726) of the entire posts from March 11 to 12, 2011 and from March 9 to 10, 2012, respectively.5.2 ResultsTables 3 and 4 list the timings needed to solve the classification problems generated for each sentence byprocessing the Twitter streams listed in Table 2 using the base-phrase chunker and the dependency parser,respectively.
In Table 3, ?baseline?
refers to the classifier using Eq.
3, while ?Y&K ?09?
refers to Yoshi-naga and Kitsuregawa (2009) who used Eq.
4, and enumerates common classification problems from thetraining corpus9 of the classifier in advance.
To highlight the performance gap caused by the algorithmicdifferences and make the memory consumptions comparable, the experiments were conducted using thesame implementation of the reduced double-array trie, described in Section 4.2, for all the methods.
Theproposed classifier (with 65,536 (k = 216) common classification problems) achieved higher classifica-11The best reported accuracy of a non-stacking parser is 91.96% (partial) and 57.44% (complete) for Kyoto-University TextCorpus Version 3.0 (Iwatate et al., 2008), and is better than that achieved by the MST algorithm (McDonald et al., 2005).1098March 11-12, 2011 March 9-10, 2012method space speed ratio space speed ratio[MiB] [ms/sent.]
[MiB] [ms/sent.
]baseline 31.50 0.1187 1.00 31.50 0.0979 1.00Y&K ?09 99.91 0.0738 1.61 99.91 0.0651 1.51proposed k = 216 43.21 0.0469 2.53 43.01 0.0542 1.81(LFU) k = 220 113.40 0.0293 4.06 113.27 0.0399 2.45k = 224 904.32 0.0222 5.35 905.62 0.0285 3.44proposed k = 216 42.68 0.0497 2.39 42.66 0.0546 1.79(LRU) k = 220 108.88 0.0283 4.20 108.94 0.0421 2.32k = 224 840.85 0.0208 5.71 840.93 0.0280 3.50Table 4: Experimental results obtained with the reduced double array trie: dependency parsing.00.51.01.52.0210212214216218220222224226Averagetime[?s]/classifyklrulfuY&K ?09(a) base-phrase chunking051015202530210212214216218220222224226Averagetime[?s]/classifyklrulfuY&K ?09(b) dependency parsingFigure 2: Average classification time per classification problem as a function of number of commonclassification problems k (2011 tweet stream).tion speed than that achieved by Y&K ?09 (with 943,864 (chunking) and 2,902,679 (parsing) commonclassification problems).
Although the speed up is evident for both tweet datasets, the speed-up is moreobvious in the case of the 2011 tweet stream.
In the following experiments, in view of space limitationsand redundancy, the 2011 tweet stream was used; however, note that the same conclusions are drawnfrom the results with the 2012 twitter stream.Figure 2 shows the time needed for solving each classification problem for chunking and parsing ofthe 2011 tweet stream when the threshold to the number of common classification problems k is variedfrom 210 to 226, respectively.
In both tasks, the proposed classifier with the LRU policy outperforms theproposed classifier with the LFU policy when k was increased.
This is not only because the LFU policyhas higher overheads than the LRU policy but also because the LFU policy selects useless classificationproblems that include lexical features related to a burst in the past.
The speed-up is saturated in the caseof base-phrase chunking at k = 222 (Figure 2(a)).
This is because the proposed classifier often terminatesmargin computation without seeing lexical features for base phrase chunking, so it rarely reuses resultsof common classification problems including lexical features that are preserved when k is increased.On the other hand in dependency parsing, the classifier relies on lexical features to resolve semanticambiguities, so it cannot terminate margin computation without seeing lexical features and thus exploitscommon classification problems including lexical features.Figure 3 shows the change in the time needed for solving classification problems generated from aone-minute text stream for chunking and parsing of the 2011 tweet stream.
The y-axis shows the relativeclassification time, when the average classification time of the baseline method before the 3.11 earth-quake is counted as one.
The classification time of the baseline method and Yoshinaga and Kitsuregawa(2009)?s method rapidly increased in response to the increase of the number of classification problems,while the proposed classifier suppressed the increase in classification time.
It is thus concluded that theproposed classifier is more robust in terms of real-time processing for a text stream.Finally, the contributions of the three tricks of the proposed classifier to the classification performancefor dependency parsing were evaluated.
The three tricks are a gap-based key representation and a reduceddouble-array trie (Section 4.1), as well as the early termination of margin computation (Section 4.1).1099012345612:0014:0016:0018:0020:0022:0000:0002:0004:0006:0008:0010:0012:00Relativetime/min.poststimebaselineY&K ?09lru (k = 216)lru (k = 220)lru (k = 224)(a) base-phrase chunking012345612:0014:0016:0018:0020:0022:0000:0002:0004:0006:0008:0010:0012:00Relativetime/min.poststimebaselineY&K ?09lru (k = 216)lru (k = 220)lru (k = 224)(b) dependency parsingFigure 3: Change in classification time of one-minute posts (2011 tweet stream).plain (no tricks) + gap-based key + reduced double array + early terminationmethod space speed ratio space speed ratio space speed ratio space speed ratio[MiB] [ms/sent.]
[MiB] [ms/sent.]
[MiB] [ms/sent.]
[MiB] [ms/sent.
]baseline 39.88 0.1413 0.84 n/a n/a n/a 31.50 0.1187 1.00 38.21 0.0745 1.59Y&K ?09 117.66 0.0922 1.29 110.52 0.0904 1.31 99.91 0.0738 1.61 106.61 0.0406 2.93proposed k = 216 44.64 0.1037 1.14 44.50 0.1009 1.18 36.09 0.0845 1.41 42.68 0.0497 2.39(LRU) k = 220 117.21 0.0590 2.01 114.57 0.0572 2.07 105.21 0.0492 2.41 108.88 0.0283 4.20k = 224 969.48 0.0412 2.88 923.96 0.0398 2.98 897.70 0.0350 3.39 840.85 0.0208 5.71Table 5: Contribution of each trick to classification performance; 2011 tweet dataset (underlined numbersare quoted from Table 4).Table 5 lists the classification times per sentence in the case of dependency parsing, when each trickis cumulatively applied to plain classifiers without all the tricks.
Classification is significantly speededup by early termination of the margin computation and the reduced double-array trie.
These tricks alsocontribute to speeding up the baseline method and Yoshinaga and Kitsuregawa (2009)?s method.6 ConclusionAiming to efficiently process a real-world text stream (such as a Twitter stream) in real-time, a self-adaptive classifier that becomes faster for a given text stream is proposed.
It enumerates common clas-sification problems that are generated during the processing of a text stream, and reuses the results ofthose classification problems as partial results for solving forthcoming classification problems.The proposed classifier was evaluated by applying it to the streams of classification problems generatedby processing two sets of Twitter streams on the day of the 2011 Great East Japan Earthquake and thesecond weekend in March 2012 using a state-of-the-art base-phrase chunker and dependency parser.
Theproposed classifier speeds up the classification by factors of 3.2 (chunking) and 5.7 (parsing), which aresignificant factors in regard to processing a massive text stream.It is planned to evaluate the classifier on other NLP tasks.
A linear classifier with conjunctive fea-tures is widely used for NLP tasks such as word segmentation, part-of-speech tagging (Neubig et al.,2011b), and dependency parsing (Nivre and McDonald, 2008).
Even for NLP tasks in which structuredclassification is effective (e.g., named entity recognition), structure compilation (Liang et al., 2008) (or?uptraining?
(Petrov et al., 2010)) gives state-of-the-art accuracy when a linear classifier with manyconjunctive features is used.
The proposed classifier is expected to be applied to a range of NLP tasks.All the codes have been available for the research community as open-source software, includingpecco (a self-adaptive classifier)12 and J.DepP (a base-phrase chunker and dependency parser).13AcknowledgmentsThis work was supported by the Research and Development on Real World Big Data Integration andAnalysis program of the Ministry of Education, Culture, Sports, Science, and Technology, JAPAN.12http://www.tkl.iis.u-tokyo.ac.jp/?ynaga/pecco/13http://www.tkl.iis.u-tokyo.ac.jp/?ynaga/jdepp/1100ReferencesJun?ichi Aoe.
1989.
An efficient digital search algorithm by using a double-array structure.
IEEE Transactions onSoftware Engineering, 15(9):1066?1077.Eiji Aramaki, Sachiko Maskawa, and Mizuki Morita.
2011.
Twitter catches the flu: Detecting influenza epidemicsusing Twitter.
In Proceedings of EMNLP, pages 1568?1576.Koby Crammer, Ofer Dekel, Joseph Keshet, Shai Shalev-Shwartz, and Yoram Singer.
2006.
Online passive-aggressive algorithms.
JMLR, 7:551?585, March.Jennifer Foster, ?Ozlem C?etinoglu, Joachim Wagner, Joseph Le Roux, Stephen Hogan, Joakim Nivre, DeirdreHogan, and Josef van Genabith.
2011.
#hardtoparse: POS tagging and parsing the Twitterverse.
In Proceedingsof the AAAI-11 Workshop on Analyzing Microtext.Eric Gilbert and Karrie Karahalios.
2010.
Widespread worry and the stock market.
In Proceedings of ICWSM,pages 58?65.Kevin Gimpel, Nathan Schneider, Brendan O?Connor, Dipanjan Das, Daniel Mills, Jacob Eisenstein, MichaelHeilman, Dani Yogatama, Jeffrey Flanigan, and Noah A. Smith.
2011.
Part-of-speech tagging for Twitter:Annotation, features, and experiments.
In Proceedings of ACL-HLT, pages 42?47.Yoav Goldberg, Kai Zhao, and Liang Huang.
2013.
Efficient implementation of beam-search incremental parsers.In Proceedings of ACL, Short Papers, pages 628?633.Bo Han and Timothy Baldwin.
2011.
Lexical normalisation of short text messages: Makn sens a #twitter.
InProceedings of ACL-HLT, pages 368?378.Hideki Isozaki and Hideto Kazawa.
2002.
Efficient support vector classifiers for named entity recognition.
InProceedings of COLING, pages 1?7.Masakazu Iwatate, Masayuki Asahara, and Yuji Matsumoto.
2008.
Japanese dependency parsing using a tourna-ment model.
In Proceedings of COLING, pages 361?368.Masakazu Iwatate.
2012.
Development of Pairwise Comparison-based Japanese Dependency Parsers and Appli-cation to Corpus Annotation.
Ph.D. thesis, Graduate School of Information Science, Nara Institute of Scienceand Technology.Nobuhiro Kaji, Yasuhiro Fujiwara, Naoki Yoshinaga, and Masaru Kitsuregawa.
2010.
Efficient staggered decod-ing for sequence labeling.
In Proceedings of ACL, pages 485?494.Daisuke Kawahara, Sadao Kurohashi, and Ko?iti Hasida.
2002.
Construction of a Japanese relevance-taggedcorpus.
In Proceedings of LREC, pages 2008?2013.Terry Koo, Alexander M. Rush, Michael Collins, Tommi Jaakkola, and David Sontag.
2010.
Dual decompositionfor parsing with non-projective head automata.
In Proceedings of EMNLP, pages 1288?1298.Taku Kudo and Yuji Matsumoto.
2003.
Fast methods for kernel-based text analysis.
In Proceedings of ACL, pages24?31.Gourab Kundu, Vivek Srikumar, and Dan Roth.
2013.
Margin-based decomposed amortized inference.
In Pro-ceedings of EMNLP, pages 905?913.Percy Liang, Hal Daume?
III, and Dan Klein.
2008.
Structure compilation: trading structure for features.
InProceedings of ICML, pages 592?599.Christopher D. Manning, Prabhakar Raghavan, and Hinrich Schu?tze.
2008.
Introduction to Information Retrieval.Cambridge University Press.Twitter, Inc. 2011.
Twitter?s 2011 year in review.
http://yearinreview.twitter.com/en/tps.html.Ryan McDonald, Koby Crammer, and Fernando Pereira.
2005.
Online large-margin training of dependencyparsers.
In Proceedings of ACL, pages 523?530.Ahmed Metwally, Divyakant Agrawal, and Amr El Abbadi.
2005.
Efficient computation of frequent and top-kelements in data streams.
In Proceedings of ICDT, pages 398?412.1101Graham Neubig, Yuichiroh Matsubayashi, Masato Hagiwara, and Koji Murakami.
2011a.
Safety informationmining - what can NLP do in a disaster -.
In Proceedings of IJCNLP, pages 965?973.Graham Neubig, Yosuke Nakata, and Shinsuke Mori.
2011b.
Pointwise prediction for robust, adaptable japanesemorphological analysis.
In Proceedings of ACL, pages 529?533.Joakim Nivre and Ryan McDonald.
2008.
Integrating graph-based and transition-based dependency parsers.
InProceedings of ACL-HLT, pages 950?958.Slav Petrov, Pi-Chuan Chang,Michael Ringgaard, and Hiyan Alshawi.
2010.
Uptraining for accurate deterministicquestion parsing.
In Proceedings of EMNLP, pages 705?713.Alan Ritter, Sam Clark, Mausam, and Oren Etzioni.
2011.
Named entity recognition in tweets: An experimentalstudy.
In Proceedings of EMNLP, pages 1524?1534.Alexander Rush and Slav Petrov.
2012.
Vine pruning for efficient multi-pass dependency parsing.
In Proceedingsof NAACL-HLT, pages 498?507.Takeshi Sakaki, Makoto Okazaki, and Yutaka Matsuo.
2010.
Earthquake shakes Twitter users: Real-time eventdetection by social sensors.
In Proceedings of WWW, pages 851?860.Manabu Sassano.
2004.
Linear-time dependency analysis for Japanese.
In Proceedings of COLING, pages 8?14.Manabu Sassano.
2008.
An experimental comparison of the voted perceptron and support vector machines inJapanese analysis tasks.
In Proceedings of IJCNLP, pages 829?834.Vivek Srikumar, Gourab Kundu, and Dan Roth.
2012.
On amortizing inference cost for structured prediction.
InProceedings of EMNLP-CoNLL, pages 1114?1124.Andranik Tumasjan, Timm O. Sprenger, Philipp G. Sandner, and Isabell M. Welpe.
2010.
Predicting electionswith Twitter: What 140 characters reveal about political sentiment.
In Proceedings of ICWSM, pages 178?185.Gertjan van Noord.
2009.
Learning efficient parsing.
In Proceeding of EACL, pages 817?825.Istva?n Varga, Motoki Sano, Kentaro Torisawa, Chikara Hashimoto, Kiyonori Ohtake, Takao Kawai, Jong-HoonOh, and Stijn De Saeger.
2013.
Aid is out there: Looking for help from tweets during a large scale disaster.
InProceedings of ACL, pages 1619?1629.Henning Wachsmuth, Benno Stein, and Gregor Engels.
2011.
Constructing efficient information extractionpipelines.
In Proceedings of CIKM, pages 2237?2240.Henning Wachsmuth, Benno Stein, and Gregor Engels.
2013.
Learning efficient information extraction on hetero-geneous texts.
In Proceedings of IJCNLP, pages 534?542.Xiaofeng Wang, Matthew S. Gerber, and Donald E. Brown.
2012.
Automatic crime prediction using eventsextracted from Twitter posts.
In Proceedings of SBP, pages 231?238.Hugh E. Williams and Justin Zobel.
1999.
Compressing integers for fast file access.
The Computer Journal,42(3):193?201.Makoto Yasuhara, Toru Tanaka, Jun ya Norimatsu, and Mikio Yamamoto.
2013.
An efficient language modelusing double-array structures.
In Proceedings of EMNLP, pages 222?232.Susumu Yata, Masaki Oono, Kazuhiro Morita, Masao Fuketa, and Jun ichi Aoe.
2007.
An efficient deletionmethod for a minimal prefix double array.
Journal of Software: Practice and Experience, 37(5):523?534.Susumu Yata, Masahiro Tamura, Kazuhiro Morita, Masao Fuketa, and Jun?ichi Aoe.
2009.
Sequential insertionsand performance evaluations for double-arrays.
In Proceedings of the 71st National Convention of IPSJ, pages1263?1264.
(In Japanese).Naoki Yoshinaga and Masaru Kitsuregawa.
2009.
Polynomial to linear: Efficient classification with conjunctivefeatures.
In Proceedings of EMNLP, pages 1542?1551.Naoki Yoshinaga and Masaru Kitsuregawa.
2010.
Kernel slicing: Scalable online training with conjunctive fea-tures.
In Proceedings of COLING, pages 1245?1253.Naoki Yoshinaga and Masaru Kitsuregawa.
2012.
Efficient classification with conjunctive features.
Journal ofInformation Processing, 20(1):228?227.1102
