Proceedings of the 7th Workshop on Statistical Machine Translation, pages 442?449,Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational LinguisticsThe Trouble with SMT ConsistencyMarine Carpuat and Michel SimardNational Research Council Canada283 Alexandre-Tache?
BoulevardBuilding CRTL, Room F-2007Gatineau (Que?bec) J8X 3X7Firstname.Lastname@nrc.caAbstractSMT typically models translation at the sen-tence level, ignoring wider document context.Does this hurt the consistency of translateddocuments?
Using a phrase-based SMT sys-tem in various data conditions, we show thatSMT translates documents remarkably con-sistently, even without document knowledge.Nevertheless, translation inconsistencies oftenindicate translation errors.
However, unlike inhuman translation, these errors are rarely dueto terminology inconsistency.
They are moreoften symptoms of deeper issues with SMTmodels instead.1 IntroductionWhile Statistical Machine Translation (SMT) mod-els translation at the sentence level (Brown et al,1993), human translators work on larger translationunits.
This is partly motivated by the importanceof producing consistent translations at the documentlevel.
Consistency checking is part of the quality as-surance process, and complying with the terminol-ogy requirements of each task or client is crucial.In fact, many automatic tools have been proposed toassist humans in this important task (Itagaki et al,2007; Dagan and Church, 1994, among others).This suggests that wider document-level contextinformation might benefit SMT models.
However,we do not have a clear picture of the impact ofsentence-based SMT on the translation of full doc-uments.
From a quality standpoint, it seems safe toassume that translation consistency is as desirablefor SMT as for human translations.
However, con-sistency needs to be balanced with other quality re-quirements.
For instance, strict consistency mightresult in awkward repetitions that make translationsless fluent.
From a translation modeling standpoint,while typical SMT systems do not explicitly enforcetranslation consistency, they can learn lexical choicepreferences from training data in the right domain.In this paper, we attempt to get a better under-standing of SMT consistency.
We conduct an em-pirical analysis using a phrase-based SMT system ina variety of experimental settings, focusing on twosimple, yet understudied, questions.
Is SMT outputconsistent at the document level?
Do inconsistenciesindicate translation errors?We will see that SMT consistency issues are quitedifferent from consistency issues in human transla-tions.
In fact, while inconsistency errors in SMToutput might be particularly obvious to the humaneye, SMT is globally about as consistent as humantranslations.
Furthermore, high translation consis-tency does not guarantee quality: weaker SMT sys-tems trained on less data translate more consistentlythan stronger larger systems.
Yet, inconsistent trans-lations often indicate translation errors, possibly be-cause words and phrases that translate inconsistentlyare the hardest to translate.After discussing related work on consistency anddocument modeling for SMT (Section 2), we de-scribe our corpora in Section 3 and our generalmethodology in Section 4.
In Section 5, we dis-cuss the results of an automatic analysis of transla-tion consistency, before turning to manual analysisin Section 6.4422 Related workWhile most SMT systems operate at the sentencelevel, there is increased interest in modeling docu-ment context and consistency in translation.In earlier work (Carpuat, 2009), we investigatewhether the ?one sense per discourse?
heuristiccommonly used in word sense disambiguation (Galeet al, 1992) can be useful in translation.
We showthat ?one translation per discourse?
largely holdsin automatically word-aligned French-English newsstories, and that enforcing translation consistency asa simple post-processing constraint can fix some ofthe translation errors in a phrase-based SMT sys-tem.
Ture et al (2012) provide further empiricalsupport by studying the consistency of translationrules used by a hierarchical phrase-based system toforce-decode Arabic-English news documents fromthe NIST evaluation.Several recent contributions integrate translationconsistency models in SMT using a two-pass de-coding approach.
In phrase-based SMT, Xiao etal.
(2011) show that enforcing translation consis-tency using post-processing and redecoding tech-niques similar to those introduced in Carpuat (2009)can improve the BLEU score of a Chinese-Englishsystem.
Ture et al (2012) also show signifi-cant BLEU improvements on Arabic-English andChinese-English hierarchical SMT systems.
Dur-ing the second decoding pass, Xiao et al (2011)use only translation frequencies from the first passto encourage consistency, while Ture et al (2012)also model word rareness by adapting term weight-ing techniques from information retrieval.Another line of work focuses on cache-basedadaptive models (Tiedemann, 2010a; Gong et al,2011), which lets lexical choice in a sentence be in-formed by translations of previous sentences.
How-ever, cache-based models are sensitive to error prop-agation and can have a negative impact on some datasets (Tiedemann, 2010b).
Moreover, this approachblurs the line between consistency and domain mod-eling.
In fact, Gong et al (2011) reports statisticallysignificant improvements in BLEU only when com-bining pure consistency caches with topic and simi-larity caches, which do not enforce consistency butessentially perform domain or topic adaptation.There is also work that indirectly addresses con-sistency, by encouraging the re-use of translationmemory matches (Ma et al, 2011), or by using agraph-based representation of the test set to promotesimilar translations for similar sentences (Alexan-drescu and Kirchhoff, 2009).All these results suggest that consistency can bea useful learning bias to improve overall translationquality, as measured by BLEU score.
However, theydo not yet give a clear picture of the translation con-sistency issues faced by SMT systems.
In this paper,we directly check assumptions on SMT consistencyin a systematic analysis of a strong phrase-basedsystem in several large data conditions.3 Translation TasksWe use PORTAGE, the NRC?s state-of-the-artphrase-based SMT system (Foster et al, 2009), in anumber of settings.
We consider different languagepairs, translation directions, training sets of differ-ent nature, domain and sizes.
Dataset statistics aresummarized in Table 1, and a description follows.Parliament condition These conditions are de-signed to illustrate an ideal situation: a SMT systemtrained on large high-quality in-domain data.The training set consists of Canadian parliamen-tary text, approximately 160 million words in eachlanguage (Foster et al, 2010).
The test set aloconsists of documents from the Canadian parlia-ment: 807 English and 476 French documents.
Eachdocument contains transcript of speech by a singleperson, typically focusing on a single topic.
Thesource-language documents are relatively short: thelargest has 1079 words, the average being 116 wordsfor English documents, 124 for French.
For eachdocument, we have two translations in the other lan-guage: the first is our SMT output; the second is apostedited version of that output, produced by trans-lators of the Canadian Parliamentary Translation andInterpretation services.Web condition This condition illustrates a per-haps more realistic situation: a ?generic?
SMT sys-tem, trained on large quantities of heterogeneousdata, used to translate slightly out-of-domain text.The SMT system is trained on a massive corpusof documents harvested from the Canadian federalgovernment?s Web domain ?gc.ca?
: close to 40M443lang train data # tgt words test data #tgt words #docs BLEU WERen-fr parl 167M parl 104k 807 45.2 47.1fr-en parl 149M parl 51k 446 58.0 31.9en-fr gov web 641M gov doc 336k 3419 29.4 60.4zh-en small (fbis) 10.5M nist08 41k 109 23.6 68.9zh-en large (nist09) 62.6M nist08 41k 109 27.2 66.1Table 1: Experimental dataunique English-French sentence pairs.
The test setcomes from a different source to guarantee that thereis no overlap with the training data.
It consists ofmore than 3000 English documents from a Canadianprovincial government organization, totalling 336kwords.
Reference translations into French were pro-duced by professional translators (not postedited).Documents are quite small, each typically focus-ing on a specific topic over a varied range of do-mains: agriculture, environment, finance, human re-sources, public services, education, social develop-ment, health, tourism, etc.NIST conditions These conditions illustrate thesituation with a very different language pair,Chinese-to-English, under two different scenarios:a system built using small in-domain data and oneusing large more heterogeneous data.Following Chen et al (2012), in the Small datacondition, the SMT system is trained using the FBISChinese-English corpus (10.5M target words); theLarge data condition uses all the allowed bilingualcorpora from NIST Open Machine Translation Eval-uation 2009 (MT09), except the UN, Hong KongLaws and Hong Kong Hansard datasets, for a totalof 62.6M target words.
Each system is then usedto translate 109 Chinese documents from the 2008NIST evaluations (MT08) test set.
For this dataset,we have access to four different reference transla-tions.
The documents are longer on average thanfor the previous conditions, with approximately 470words per document.4 Consistency Analysis MethodWe study repeated phrases, which we define as apair ?p, d?
where d is a document and p a phrasetype that occurs more than once in d.Since this study focuses on SMT lexical choiceconsistency, we base our analysis on the actual trans-lation lexicon used by our phrase-based translationsystem (i.e., its phrase-table.)
For each documentd in a given collection of documents, we identifyall source phrases p from the SMT phrase-table thatoccur more than once.
We only consider sourcephrases that contain at least one content word.We then collect the set of translations T for eachoccurrence of the repeated phrase in d. Using theword-alignment between source and translation, foreach occurrence of p in d, we check whether p isaligned to one of its translation candidates in thephrase-table.
A repeated phrase is translated consis-tently if all the strings in T are identical ?
ignoringdifferences due to punctuation and stopwords.The word-alignment is given by the SMT decoderin SMT output, and is automatically infered fromstandard IBM models for the reference1.Note that, by design, this approach introduces abias toward components of the SMT system.
A hu-man annotator asked to identify translation incon-sistencies in the same data would not tag the exactsame set of instances.
Our approach might detecttranslation inconsistencies that a human would notannotate, because of alignment noise or negligiblevariation in translations for instance.
We addressthese limitations in Section 6.
Conversely, a humanannotator would be able to identify inconsistenciesfor phrases that are not in the phrase-table vocabu-lary.
Our approach is not designed to detect these in-consistencies, since we focus on understanding lex-ical choice inconsistencies based on the knowledgeavailable to our SMT system at translation time.1We considered using forced decoding to align the referenceto the source, but lack of coverage led us to use IBM-style wordalignment instead.444lang train test translator #repeatedphrasesconsistent(%)avgwithindocfreq(inconsistent)avgwithindocfreq(all)#docswithre-peatedphrases%consistentthatmatchreference%inconsistentthatmatchreference%easyfixesen-fr parl parl SMT 4186 73.03 2.627 2.414 529 70.82 34.37 10.12en-fr parl parl reference 3250 75.94 2.542 2.427 468fr-en parl parl SMT 2048 85.35 2.453 2.351 303 82.72 52.67 3.52fr-en parl parl reference 1373 82.08 2.455 2.315 283en-fr gov web gov doc SMT 79248 88.92 6.262 3.226 2982 60.71 13.05 15.53en-fr gov web gov doc reference 25300 82.73 4.071 2.889 2166zh-en small nist08 SMT 2300 63.61 2.983 2.725 109 56.25 18.40 9.81zh-en small nist08 reference 1431 71.49 2.904 2.695 109zh-en large nist08 SMT 2417 60.20 3.055 2.717 109 60.00 17.88 10.89zh-en large nist08 reference 1919 68.94 2.851 2.675 109Table 2: Statistics on the translation consistency of repeated phrases for SMT and references in five translation tasks.See Section 5 for details5 Automatic AnalysisTable 2 reports various statistics for the translationsof repeated phrases in SMT and human references,for all tasks described in Section 3.5.1 Global SMT consistencyFirst, we observe that SMT is remarkably consis-tent.
This suggests that consistency in the source-side local context is sufficient to constrain the SMTphrase-table and language model to produce consis-tent translations for most of the phrases consideredin our experiments.The column ?consistent (%)?
in Table 2 showsthat the majority of repeated phrases are translatedconsistently for all translation tasks considered.
ForFrench-English tasks, the percentage of repeatedphrases ranges from 73 to 89% .
The consistencypercentages are lower for Chinese-English, a moredistant language pair.
The Parliament task showsthat translating into the morphologically richer lan-guage yields slightly lower consistency, all other di-mensions being identical.
However, morphologicalvariations only explain part of the difference: trans-lating into French under the Web condition yields thehighest consistency percentage of all tasks, whichmight be explained by the very short and repetitivenature of the documents.
As can be expected, incon-sistently translated phrases are repeated in a docu-ment more often than average for all tasks (columns?avg within doc freq?
).Interestingly, the smaller and weaker Chinese-English translation system (23.6 BLEU) is moreconsistent than its stronger counterpart (27.2BLEU) according to the consistency percent-ages.The smaller training condition yields a smallerphrase-table with a lower coverage of the nist08source, fewer translation alternatives and there-fore more consistent translations.
Clearly consis-tency does not correlate with translation quality, andglobal consistency rates are not indicators of thetranslation quality of particular system.5.2 Consistency of reference translationsSurprisingly, the percentage of consistently trans-lated phrases are very close in SMT output and hu-man references, and even higher in SMT for 2 out of5 tasks (Table 2).Note that there are fewer instances of repeatedphrases for human references than for SMT, becausethe phrase-table used as a translation lexicon natu-rally covers SMT output better than independentlyproduced human translations.
Word alignment isalso noisier between source and reference.445lang train test translator #repeatedphrasesconsistent(%)avgwithindocfreq(inconsistent)avgwithindocfreq(all)#docswithre-peatedphrases%consistentthatmatchreference%inconsistentthatmatchreference%easyfixeszh-en small nist08 human1 1496 71.59 2.974 2.725 109 68.91 34.59 9.71human2 1356 69.40 2.913 2.687 109 73.22 36.63 7.60human2 1296 71.60 2.870 2.671 109 71.88 36.68 8.15zh-en large nist08 human1 2017 70.25 2.943 2.692 109 66.13 30.83 9.64human2 1855 67.17 2.854 2.667 109 69.42 31.86 9.16human3 1739 69.70 2.854 2.660 109 68.23 33.78 8.31Table 3: Statistics on the translation consistency of repeated phrases in the multiple human references available on theChinese-English NIST08 test set.
See Section 5 for detailsThere is a much wider gap in coherence per-centages between references and SMT for Chinese-English than French-English tasks, as can be ex-pected for the harder language pair.
In addition,the same nist08 reference translations are more con-sistent according to the phrase-table learned in thesmall training condition than according to the largerphrase-table.
This confirms that consistency can sig-nal a lack of coverage for new contexts.5.3 Consistency and correctnessWhile translation consistency is generally assumedto be desirable, it does not guarantee correctness:SMT translations of repeated phrases can be consis-tent and incorrect, or inconsistent and correct.
In or-der to evaluate correctness automatically, we checkwhether translations of repeated phrases are foundin the corresponding reference sentences.
This isan approximation since the translation of a sourcephrase can be correct even if it is not found in thereference, and a target phrase found in the refer-ence sentence is not necessarily a correct translationof the source phrase considered.
Post-edited refer-ences alleviate some approximation errors for theParliament tasks: if the translated phrase matchesthe references, it means that it was considered cor-rect by the human post-editor who left it in.
How-ever, phrases modified during post-edition are notnecessarily incorrect.
We will address this approxi-mation in Section 6.The columns ?% consistent that match reference?and ?% inconsistent that match reference?
in Ta-ble 2 show that consistently translated phrases matchthe references more often than the inconsistent ones.With the post-edited references in the Parliamentcondition, a non-negligible percentage of consis-tently translated phrases are wrong: 17% whentranslating into English, and 30% when translatinginto French.
In contrast, inconsistently translatedphrases are more likely to be incorrect: more than65% into French and 47% into English.
For all othertasks, fewer translations match the references sincethe references are not produced by post-edition, butwe still observe the same trend as in the Parliamentcondition: inconsistent translations are more likelyto be incorrect than consistent translations overall.Four reference translations are available for theChinese-English nist08 test set.
We only use the firstone as a reference translation (in order to minimizesetting variations with French-English conditions.
)The three remaining human translations are used dif-ferently.
We compare them against the reference, ex-actly as we do for SMT output.
The resulting statis-tics are given in Table 3.
Since we know that thehuman translations are correct, this shows that manycorrect translations are not identified when using oursimple match technique to check correctness.
How-ever, it is interesting to note that (1) consistent hu-man translations tend to match the human referencesmore often than the inconsistent ones, and (2) incon-sistent MT translations match references much lessoften than inconsistent human references.446Language Examples False Inconsistencies?p, d?
Same lemma Misaligneden?fr 79 15 (19%) 8 (10%)fr?en 92 12 (13%) 24 (26%)Total 171 27 (16%) 32 (19%)Table 4: False positives in the automatic identification oftranslation inconsistencies.What goes wrong when inconsistent translationsare incorrect?
This question is hard to answer withautomatic analysis only.
As a first approximation,we check whether we could correct translations byreplacing them with machine translations producedelsewhere in the document.
In Table 2, we refer tothis as ?easy fixes?
and show that only very few in-consistency errors can be corrected this way.
Theseerrors are therefore unlikely to be fixed by post-processing approaches that enforce hard consistencyconstraints (Carpuat, 2009).6 Manual AnalysisIn order to better understand what goes wrong withinconsistent translations, we conduct a manual anal-ysis of these errors in the Parliament test condition(see Table 1).
We randomly sample inconsistentlytranslated phrases, and examine a total of 174 re-peated phrases (?p, d?
pairs, as defined in Section 4.
)6.1 Methodological IssuesWe first try to quantify the limitations of our ap-proach, and verify whether the inconsistencies de-tected automatically are indeed real inconsistencies.The results of this analysis are presented in Table 4.Given the set of translations for a repeated phrase,we ask questions relating to morphology and auto-matic word-level alignment:Morphology Are some of the alternate transla-tions for phrase p only different inflections of thesame lemma?
Assuming that inflectional morphol-ogy is governed by language-internal considerationsmore often than translational constraints, it is prob-ably inaccurate to label morphological variations ofthe same word as inconsistencies.
The annotationsreveal that this only happens for 16% of our sam-ple (column ?Same lemma?
in Table 4).
Work isunder way to build an accurate French lemmatizerto automatically abstract away from morphologicalvariations.Alignment Are some of the alternate translationsonly a by-product of word alignment errors?
Thishappens for instance when the French word partisis identified as being translated in English some-times as parties and sometimes as political in thesame document: the apparent inconsistency is ac-tually due to an incorrect alignment within the fre-quent phrase political parties.
We identify 19% ofword alignment issues in our manually annotatedsample (column ?Misaligned?
in Table 4).
Whileit is clear that alignment errors should be avoided,it is worth noting that such errors are sometimes in-dicative of translation problems: this happens, forinstance, when a key content word is left untrans-lated by the SMT system.Overall, this analysis confirms that, despite theapproximations used, a majority of the examples de-tected by our method are real inconsistencies.6.2 Analysis of Translation ErrorsWe then directly evaluate translation accuracy in oursample by checking whether the system translationmatch the post-edited references.
Here we focus ourattention on those 112 examples from our sample ofinconsistently translated phrases that do not sufferfrom lemmatization or misalignment problems.
Forcomparison, we also analyze 200 randomly sampledexamples of consistently translated phrases.
Notethat the identification of consistent phrases is notsubject to alignment and lemmatization problems,which we therefore ignore in this case.
Details ofthis analysis can be found in Table 5.We first note that 40% of all inconsistently trans-lated phrase types were not postedited at all: theirtranslation can therefore be considered correct.
Inthe case of consistently translated phrases, the rateof unedited translations rises to 75%.Focusing now on those phrases whose translationwas postedited, we classify each in one of threebroad categories of MT errors: meaning, terminol-ogy, and style/syntax errors (columns labeled ?Typeof Correction?
in Table 5).Terminology Errors Surprisingly, among the in-consistently translated phrases, we find only 13%of true terminological consistency errors, where447Language Examples Unedited (%) Type of Correction (% of edited examples)?p, d?
Meaning Terminology Style/SyntaxInconsistent en?fr 56 20 (36%) 8 (22%) 4 (11%) 27 (75%)Translations fr?en 56 25 (45%) 10 (32%) 5 (16%) 20 (65%)Total 112 45 (40%) 16 (24%) 9 (13%) 47 (70%)Consistent en?fr 100 70 (70%) 3 (10%) 0 (0%) 27 (90%)Translations fr?en 100 79 (79%) 5 (24%) 0 (0%) 16 (76%)Total 200 149 (75%) 8 (16%) 0 (0%) 43 (84%)Table 5: Manual Classification of Posteditor Corrections on the Parliament Taskthe SMT output is acceptable but different fromstandard terminology in the test domain.
For in-stance, the French term personnes handicape?es canbe translated as either persons with disabilities orpeople with disabilities, but the former is preferedin the Parliament domain.
In the case of consis-tently translated phrases, no such errors were de-tected.
This contrasts with human translation, whereenforcing term consistency is a major concern.
Inthe large-data in-domain condition considered here,SMT mostly translates terminology consistently andcorrectly.
It remains to be seen whether this stillholds when translating out-of-domain, or for differ-ent genres of documents.Meaning Errors Meaning errors occur when theSMT output fails to convey the meaning of thesource phrase.
For example, in a medical con-text, our MT system sometimes translates the Frenchword examen into English as review instead of thecorrect test or investigation.
Such errors make up24% of all corrections on inconsistently translatedphrases, 16% in the case of consistent translations.Style/Syntax Errors By far the most frequent cat-egory turns out to be style/syntax errors (70% of cor-rections on inconsistently translated phrases, 84%on consistently translated phrases): these are situ-ations where the SMT output preserves the mean-ing of the source phrase, but is still post-edited forsyntactic or stylistic preference.
This category actu-ally covers a wide range of corrections.
The morebenign cases are more cosmetic in nature, for ex-ample when the posteditor changes the MT output?In terms of the cost associated with...?
into ?Withregard to spending related to...?.
In the more se-vere cases, the posteditor completely rewrites a seri-ously disfluent machine translation.
However, errorsto which we have assigned this label have a com-mon denominator: the inconsistent phrase that is thefocus of our attention is not the source of the er-ror, but rather ?collateral damage?
in the war againstmediocre translations.Taken together, these results show that transla-tion inconsistencies in SMT tend to be symptoms ofgeneric SMT problems such as meaning and fluencyor syntax errors.
Only a minority of observed in-consistencies turn out to be the type of terminologyinconsistencies that are a concern in human transla-tions.7 ConclusionWe have presented an in-depth study of machinetranslation consistency, using state-of-the-art SMTsystems trained and evaluated under various realis-tic conditions.
Our analysis highlights a number ofimportant, and perhaps overlooked, issues regardingSMT consistency.First, SMT systems translate documents remark-ably consistently, even without explicit knowledgeof extra-sentential context.
They even exhibit globalconsistency levels comparable to that of professionalhuman translators.Second, high translation consistency does not cor-relate with better quality: as can be expected inphrase-based SMT, weaker systems trained on lessdata produce translations that are more consistentthan higher-quality systems trained on larger moreheterogeneous data sets.However, this does not imply that inconsistenciesare good either: inconsistently translated phrases co-incide with translation errors much more often thanconsistent ones.
In practice, translation inconsis-tency could therefore be used to detect words andphrases that are hard to translate for a given system.Finally, manual inspection of inconsistent transla-448tions shows that only a small minority of errors arethe kind of terminology problems that are the mainconcern in human translations.
Instead, the major-ity of errors highlighted by inconsistent translationsare symptoms of other problems, notably incorrectmeaning translation, and syntactic or stylistic issues.These problems are just as prevalent with consistentas with inconsistent translations.While directly enforcing translation consistencyin MT may prove useful in some situations, ouranalysis suggests that the phrase-based SMT sys-tems considered here would benefit more from di-rectly tackling the underlying ?- and admittedlymore complex ?
problems of meaning and syntac-tic errors.In future work, we plan to improve our analysis byextending our diagnosis methods, and consider ad-ditional data conditions and genres.
We also plan toexplore the potential of consistency for confidenceestimation and error detection.AcknowledgmentsWe would like to thank the Canadian TranslationBureau and the Portage team at the National Re-search Council for providing the post-edited and ma-chine translations used in this study.ReferencesAndrei Alexandrescu and Katrin Kirchhoff.
2009.Graph-based learning for statistical machine transla-tion.
In Proceedings of Human Language Technolo-gies: The 2009 Annual Conference of the North Ameri-can Chapter of the Association for Computational Lin-guistics, pages 119?127, Boulder, CO, June.Peter E. Brown, Vincent J. Della Pietra, Stephen A. DellaPietra, and Robert L. Mercer.
1993.
The Mathematicsof Statistical Machine Translation: Parameter Estima-tion.
Computational Linguistics, 19(2):263?312.Marine Carpuat.
2009.
One translation per discourse.In Proceedings of the Workshop on Semantic Evalu-ations: Recent Achievements and Future Directions(SEW-2009), pages 19?27, Boulder, CO, June.Boxing Chen, Roland Kuhn, and Samuel Larkin.
2012.PORT: a Precision-Order-Recall MT evaluation metricfor Tuning.
In Proceedings of the 50th Annual Meet-ing of the Association for Computational Linguistics(ACL-2012).Ido Dagan and Ken Church.
1994.
Termight: Identify-ing and translating technical terminology.
In Proceed-ings of the Fourth Conference on Applied Natural Lan-guage Processing, pages 34?40, Stuttgart, Germany,October.George Foster, Boxing Chen, Eric Joanis, Howard John-son, Roland Kuhn, and Samuel Larkin.
2009.PORTAGE in the NIST 2009 MT Evaluation.
Tech-nical report, NRC-CNRC.George Foster, Pierre Isabelle, and Roland Kuhn.
2010.Translating structured documents.
In Proceedings ofthe Ninth Conference of the Association for MachineTranslation in the Americas (AMTA 2010), Denver,Colorado, November.William A. Gale, Kenneth W. Church, and DavidYarowsky.
1992.
One Sense Per Discourse.
In Pro-ceedings of the workshop on Speech and Natural Lan-guage, Harriman, NY, February.Zhengxian Gong, Min Zhang, and Guodong Zhou.
2011.Cache-based document-level statistical machine trans-lation.
In Proceedings of the 2011 Conference onEmpirical Methods in Natural Language Processing,pages 909?919, July.Masaki Itagaki, Takako Aikawa, and Xiaodong He.2007.
Automatic validation of terminology transla-tion consistency with statistical method.
In Proceed-ings of Machine Translation Summit XI, pages 269?274, September.Yanjun Ma, Yifan He, Andy Way, and Josef van Gen-abith.
2011.
Consistent translation using discrim-inative learning - a translation memory-inspired ap-proach.
In Proceedings of the 49th Annual Meetingof the Association for Computational Linguistics: Hu-man Language Technologies, pages 1239?1248, Port-land, Oregon, USA, June.Jo?rg Tiedemann.
2010a.
Context adaptation in statisticalmachine translation using models with exponentiallydecaying cache.
In Proceedings of the 2010 Workshopon Domain Adaptation for Natural Language Process-ing, pages 8?15, Uppsala, Sweden, July.Jo?rg Tiedemann.
2010b.
To Cache or Not To Cache?Experiments with Adaptive Models in Statistical Ma-chine Translation.
In Joint Fifth Workshop on Sta-tistical Machine Translation and MetricsMATR, pages195?200, Uppsala, Sweden, July.Ferhan Ture, Douglas W. Oard, and Philip Resnik.
2012.Encouraging consistent translation choices.
In Pro-ceedings of the 2012 Conference of the North Amer-ican Chapter of the Association for ComputationalLinguistics: Human Language Technologies (NAACLHLT 2012), Montreal, Canada, June.Tong Xiao, Jingbo Zhu, Shujie Yao, and Hao Zhang.2011.
Document-level Consistency Verification inMachine Translation.
In Machine Translation SummitXIII, pages 131?138, Xiamen, China, September.449
