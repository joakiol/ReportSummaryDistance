Thematic segmentation of texts: two methods for two kinds of textsOlivier FERRETLIMSI-CNRSB~t.
508 - BP 133F-91403, Orsay Cedex, Franceferret @ limsi, frBrigitte GRAULIMSI-CNRSBrit.
508 - BP 133F-91403, Orsay Cedex, Francegrau @ l imsi.frNicolas MASSONLIMSI-CNRSB~t.
508 - BP 133F-91403, Orsay Cedex, Francemasson@limsi.frAbstractTo segment texts in thematic units, wepresent here how a basic principlerelying on word distribution can beapplied on different kind of texts.
Westart from an existing method welladapted for scientific texts, and wepropose its adaptation to other kinds oftexts by using semantic links betweenwords.
These relations are found in alexical network, automatically built froma large corpus.
We will compare theirresults and give criteria to choose themore suitable method according to textcharacteristics.1.
Int roduct ionText segmentation according to a topicalcriterion is a useful process in manyapplications, such as text summarization orinformation extraction task.
Approaches thataddress this problem can be classified inknowledge-based approaches or word-basedapproaches.
Knowledge-based systems asGrosz and Sidner's (1986) require anextensive manual knowledge engineeringeffort to create the knowledge base (semanticnetwork and/or frames) and this is onlypossible in very limited and well-knowndomains.To overcome this limitation, and to process alarge amount of texts, word-based approacheshave been developed.
Hearst (1997) andMasson (1995) make use of the worddistribution in a text to find a thematicsegmentation.
These works are well adapted totechnical or scientific texts characterized by aspecific vocabulary.
To process narrative orexpository texts such as newspaper articles,Kozima's (1993) and Morris and Hirst's(1991) approaches are based on lexicalcohesion computed from a lexical network.These methods depend on the presence of thetext vocabulary inside their network.
So, toavoid any restriction about domains in suchkinds of texts, we present here a mixed methodthat augments Masson's system (1995), basedon word distribution, by using knowledgerepresented by a lexical co-occurrencenetwork automatically built from a corpus.
Bymaking some experiments with these two lattersystems, we show that adding lexicalknowledge is not sufficient on its own to havean all-purpose method, able to process eithertechnical texts or narratives.
We will thenpropose some solutions to choose the moresuitable method.2.
Overv iewIn this paper, we propose to apply one and thesame basic idea to find topic boundaries intexts, whatever  kind they are,scientific/technical rticles or newspaperarticles.
This main idea is to consider smallesttextual units, here the paragraphs, and try tolink them to adjacent similar units to createlarger thematic units.
Each unit ischaracterized by a set of descriptors, i.e.
singleand compound content words, defining avector.
Descriptor values are the number ofoccurrences of the words in the unit, modifiedby the word distribution in the text.
Then, eachsuccessive units are compared through theirdescriptors to know if they refer to a sametopic or not.This kind of approach is well adapted toscientific articles, often characterized bydomain technical term reiteration since there isoften no synonym for such specific terms.
But,we will show that it is less efficient onnarratives.
Although the same basic principleabout word distribution applies, topics are notso easily detectable.
In fact, narrative orexpository texts often refer to a same entitywith a large set of different words.
Indeed,authors avoid repetitions and redundancies byusing hyperonyms,  synonyms andreferentially equivalent expressions.To deal with this specificity, we havedeveloped another method that augments thefirst method by making use of informationcoming from a lexical co-occurrence n twork.392This network allows a mutual reinforcement ofdescriptors that are different but stronglyrelated when occurring in the same unit.Moreover, it is also possible to create newdescriptors for units in order to link unitssharing semantically close words.In the two methods, topic boundaries aredetected by a standard distance measurebetween each pair of adjacent vectors.
Thus,the segmentation process produces a textrepresentation with thematic blocks includingparagraphs about the same topic.The two methods have been tested on differentkinds of texts.
We will discuss these results andgive criteria to choose the more suitablemethod according to text characteristics.3.
Pre-processing of the textsAs we are interested in the thematic dimensionof the texts, they have to be represented bytheir significant features from that point ofview.
So, we only hold for each text thelemmatized form of its nouns, verbs andadjectives.
This has been done by combiningexisting tools.
MtSeg from the Multext projectpresented in V6ronis and Khouri (1995) isused for segmenting the raw texts.
Ascompound nouns are less polysemous thansingle ones, we have added to MtSeg theability to identify 2300 compound nouns.
Wehave retained the most frequent compoundnouns in 11 years of the French Le Mondenewspaper.
They have been collected with theINTEX tool of Silberztein (1994).
The part ofspeech tagger TreeTagger of Schmid (1994) isapplied to disambiguate he lexical category ofthe words and to provide their lemmatizedform.
The selection of the meaningful words,which do not include proper nouns andabbreviations, ends the pre-processing.
Thisone is applied to the texts both for buildingthe collocation network and for their thematicsegmentation.4.
Building the collocation networkOur segmentation mechanism relies onsemantic relations between words.
In order toevaluate it, we have built a network of lexicalcollocations from a large corpus.
Our corpus,whose size is around 39 million words, is madeup of 24 months of the Le Monde newspapertaken from 1990 to 1994.
The collocationshave been calculated according to the methoddescribed in Church and Hanks (1990) bymoving a window on the texts.
The corpus waspre-processed as described above, whichinduces a 63% cut.
The window in which thecollocations have been collected is 20 wordswide and takes into account he boundaries ofthe texts.
Moreover, the collocations here areindifferent to order.These three choices are motivated by our taskpoint of view.
We are interested in finding iftwo words belong to the same thematicdomain.
As a topic can be developed in a largetextual unit, it requires a quite large window todetect hese thematic relations.
But the processmust avoid jumping across the textsboundaries as two adjacent texts from thecorpus are rarely related to a same domain.Lastly, the collocation wl-w2 is equivalent tothe collocation w2-wl as we only try tocharacterize a thematic relation between wland w2.After filtering the non-significant collocations(collocations with less than 6 occurrences,which represent 2/3 of the whole), we obtain anetwork with approximately 31000 words and14 million relations.
The cohesion betweentwo words is measured as in Church and Hanks(1990) by an estimation of the mutualinformation based on their col locationfrequency.
This value is normalized by themaximal mutual information with regard tothe corpus, which is given by:/max = log2 N2(Sw - 1)with N: corpus size and Sw: window size5.
Thematic segmentation withoutlexical networkThe first method, based on a numericalanalysis of the vocabulary distribution in thetext, is derived from the method described inMasson (1995).A basic discourse unit, here a paragraph, isrepresented  as a te rm vectorGi =(gil,gi2,...,git) where gi is the number ofoccurrences of a given descriptor in Gi.The descriptors are the words extracted by thepre-processing of the current text.
Termvectors are weighted.
The weighting policy istf.idf which is an indicator of the importanceof a term according to its distribution in a text.It is defined by:wij = ~).
logwhere tfij is the number of occurrences of adescriptor Tj in a paragraph i; dfi is thenumber of paragraphs in which Tj occurs and393N the total number of paragraphs in the text.Terms that are scattered over the wholedocument are considered to be less importantthan those which are concentrated in particularparagraphs.Terms that are not reiterated are considered asnon significant o characterize the text topics.Thus, descriptors whose occurrence counts arebelow a threshold are removed.
According tothe length of the processed texts, the thresholdis here three occurrences.The topic boundaries are then detected by astandard istance measure between all pairs ofadjacent paragraphs: first paragraph iscompared to second paragraph, second one tothird one and so on.
The distance measure isthe Dice coefficient, defined for two vectorsX= (x 1, x2 .
.
.
.
.
xt) and Y= (Yl, Y2 .
.
.
.
.
Yt) by:C(X,Y)=t 2 w(xi)w(yi)i=lt t w(xi)2?
w(yi) 2i= l  i= lwhere w(xi)  is the number of occurrences of adescriptor xi weighted by tf.idf factorLow coherence values show a thematic shift inthe text, whereas high coherence values showlocal thematic onsistency.6.
Thematic segmentat ion withlexical  networkTexts such as newspaper articles often refer toa same notion with a large set of differentwords l inked by semantic or pragmaticrelations.
Thus, there is often no reiteration ofterms representative of the text topics and thefirst method described before becomes lessefficient.
In this case, we modify the vectorrepresentation by adding information comingfrom the lexical network.Mod i f i ca t ions  act on the vector ia lrepresentation of paragraphs by addingdescriptors and modifying descriptor values.They aim at bringing together paragraphswhich refer to the same topic and whose wordsare not reiterated.
The main idea is that, if twowords A and B are linked in the network, then" when A is present in a text, B is also a littlebit evoked, and vice versa "That is to say that when two descriptors of atext A and B are linked with a weight w in thelexical network, their weights are reinforcedinto the paragraphs to which theysimultaneously belong.
Moreover, the missingdescriptor is added in the paragraph if absent.In case of reinforcement, if the descriptor A isreally present k times and B really present ntimes in a paragraph, then we add wn to thenumber of A occurrences and wk to thenumber of B occurrences.
In case ofdescriptor addition, the descriptor weight is setto the number of occurrences of the linkeddescriptor multiplied by w. All the couples oftext descriptors are processed using theoriginal number of their occurrences tocompute modified vector values.These vector modifications favor emergenceof significant descriptors.
If a set of wordsbelonging to neighboring paragraphs arelinked each other, then they are mutuallyreinforced and tend to bring these paragraphsnearer.
If there is no mutual reinforcement, thevector modifications are not significant.These modifications are computed beforeapplying a tf.idf like factor to the vector terms.The descriptor addition may add manydescriptors in all the text paragraphs becauseof the numerous links, even weak, betweenwords in the network.
Thus, the effect of tf.
idfis smoothed by the standard-deviation f thecurrent descriptor distribution.
The resultingfactor is:-Nlog(-7=- (1 ~ )) dj6with k, the paragraphs where Tj occurs.7.
Experiments and discussionWe have tested the two methods presentedabove on several kinds of texts.0.8 .
.
.
.0.60.20m e ~  1 - -~t /~a  2 .
.
.
.!
:: i. .
.
.
i .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.1 2 3 4 5 $ ?Figure 1 - Improvement by the second methodwith low word reiteration394Figure 1 shows the results for a newspaperarticle from Le Monde made of 8 paragraphs.The cohesion value associated to a paragraph iindicates the cohesion between paragraphs iand i+l .
The graph for the first method israther flat, with low values, which would apriori mean that a thematic shift would occurafter each paragraph.
But significant words inthis article are not repeated a lot although thepaper is rather thematically homogeneous.The second method, by the means of the linksbetween the text words in the collocationnetwork, is able to find the actual topicsimilarity between paragraphs 4 and 5 or 7and 8.The improvement resulting from the use oflexical cohesion also consists in separatingparagraphs that would be set together by theonly word reiteration criterion.
It is illustratedin Figure 2 for a passage of a book by JulesVerne 1.
A strong link is found by the firstmethod between paragraphs 3 and 4 althoughit is not thematically justified.
This situationoccurs when too few words are left by the lowfrequency word and tf.idffilters.0.8  ' ?0 .60 .40 .2: " ~?.e~d 1 - -: : Mt .hod  2 - - -1 2 3 4 SFigure 2 - Improvement by the second methodwhen too many words are filteredMore generally, the second method, even if ithas not so impressive an effect as in Figures 1and 2, allows to refine the results of the firstmethod by proceeding with more significantwords.
Several tests have been made onnewspaper articles that show this tendency.Experiments with scientific texts have alsobeen made.
These texts use specific reiteratedvocabulary (technical terms).
By applying thefirst method, significant results are obtainedI De la Terre ~ la Lune.2Le vin jaune, Pour la science (French edition ofScientific American), October 1994, p. 18because of this specificity (see Figure 3, thecoherence graph in solid line).Cl im0.8 " ' "%60,40 .20i : .
t~  t Di : , , .
,~ .4  2 - - -.
.
.
.
.
.  '
: , , "  .
.
.
.
.
.
.
.
.
.
.
.
i .
.
.
.
.
.
.
.  "
.
.
.
.
.
.
.
.
.
.
.
.
.
L " ; :~ , .
.
,  .
.
.
.
.
.
.
!
.
.
.
.
.
.6 $ 10Figure 3 - Test on a scientific paper 2 in aspecialized omainOn the contrary, by applying the secondmethod to the same text, poor results aresometimes observed (see Figure 3, thecoherence graph in dash line).
This is due tothe absence of highly specific descriptors, usedfor Dice coefficient computation, in the lexicalnetwork.
It means that descriptors reinforcedor added are not really specific of the textdomain and are nothing but noise in this case.The two methods have been tested on 16 textsincluding 5 scientific articles and 11expository or narrative texts.
They have beenchosen according to their vocabularyspecificity, their size (between 1 to 3 pages)and their paragraphs size.
Globally, the secondmethod gives better esults than the first one: itmodulates ome cohesion values.
But thesecond method cannot always be appliedbecause problems arise on some scientificpapers due to the lack of important specializeddescriptors in the network.
As the network isbuilt from the recurrence of collocationsbetween words, such words, even belonging tothe training corpus, would be too scarce to beretained.
So, specialized vocabulary will alwaysbe missing in the network.
This observationhas lead us to define the following process tochoose the more suitable method:Apply method 1;If x% of the descriptors whose value is notnull after the application of tf.
idf are notfound in the network,then continue with method 1otherwise apply method 2.According to our actual studies, x has beensettled to 25.3958.
Related worksWithout taking into account the collocationnetwork, the methods described above rely onthe same principles as Hearst (1997) andNomoto and Nitta (1994).
Although Hearstconsiders that paragraph breaks are sometimesinvoked only for lightening the physicalappearance of  texts, we have chosenparagraphs as basic units because they aremore natural thematic units than somewhatarbitrary sets of words.
We assume thatparagraph breaks that indicate topic changesare always present in texts.
Those which are setfor visual reasons are added between them andthe segmentation algorithm is able to jointhem again.
Of course, the size of actualparagraphs are sometimes irregular.
So theircomparison result is less reliable.
But thecollocation network in the second methodtends to solve this problem by homogenizingthe paragraph representation.As in Kozima (1993), the second methodexploits lexical cohesion to segment exts, butin a different way.
Kozima's approach relieson computing the lexical cohesiveness of awindow of words by spreading activation intoa lexical network built from a dictionary.
Wethink that this complex method is speciallysuitable for segmenting small parts of text butnot large texts.
First, it is too expensive andsecond, it is too precise to clearly show themajor thematic shifts.
In fact, Kozima'smethod and ours do not take place at the samegranularity level and so, are complementary.9 .
Conc lus ionFrom a first method that considers paragraphsas basic units and computes a similaritymeasure between adjacent paragraphs forbuilding larger thematic units, we havedeveloped a second method on the sameprinciples, making use of a lexical collocationnetwork  to augment  the vector ia lrepresentation of the paragraphs.
We haveshown that this second method, if well adaptedfor processing such texts as newspapersarticles, has less good results on scientific texts,because the characteristic terms do not emergeas well as in the first method, due to theaddition of related words.
So, in order to builda text segmentation system independent of thekind of processed text, we have proposed tomake a shal low analysis of the textcharacteristics to apply the suitable method.10 .
ReferencesKenneth W. Church and Patrick Hanks.
(1990)WordAssociation Norms, Mutual Information, AndLexicography.
Computational Linguistics, 16/1,pp.
22--29.Barbara J. Grosz and Candace L. Sidner.
(1986)Attention, Intentions and the Structure ofDiscourse.
Computational Linguistics, 12, pp.175--204.Marti A. Hearst.
(1997) TextTiling: Segmenting Textinto Multi-paragraph Subtopic Passages.Computational Linguistics, 23/1, pp.
33--64.Hideki Kozima.
(1993) Text Segmentation Based onSimilarity between Words.
In Proceedings of the31th Annual Meeting of the Association forComputational Linguistics (Student Session),Colombus, Ohio, USA.Nicolas Masson.
(1995) An Automatic Method forDocument Structuring.
In Proceedings of the 18thAnnual International ACM-SIGIR Conference onResearch and Development in InformationRetrieval, Seattle, Washington, USA.Jane Morris and Graeme Hirst.
(1991) LexicalCohesion Computed by Thesaural Relations as anIndicator of the Structure of Text.
ComputationalLinguistics, 17/1, pp.
21 48.Tadashi Nomoto and Yoshihiko Nitta.
(1994) AGrammatico-Statistical Approach To DiscoursePartitioning.
In Proceedings of the 15thInternational Conference on ComputationalLinguistics (COLING), Kyoto, Japan.Helmut Schmid.
(1994) Probabilistic Part-of-SpeechTagging Using Decision Trees.
In Proceedings ofthe International Conference on New Methods inLanguage Processing, Manchester, UK.Max D. Silberztein.
(1994) INTEX: A CorpusProcessing System.
In Proceedings of the 15thInternational Conference on ComputationalLinguistics (COLING), Kyoto, Japan.Jean V6ronis and Liliane Khouri.
(1995) Etiquetagegrammatical multilingue: le projet MULTEXT.TAL, 36/1-2, pp.
233--248.396
