In: Proceedings of CoNLL-2000 and LLL-2000, pages 160-162, Lisbon, Portugal, 2000.Phrase Parsing with Rule Sequence Processors:an Appl icat ion to the Shared CoNLL TaskMarc  V i la in  and Dav id  DayThe MITRE CorporationBedford, MA 01730, USA{mbv, day}Omitre,  orgFor several years, chunking has been an inte-gral part of MITRE's approach to informationextraction.
Our work exploits chunking in twoprincipal ways.
First, as part of our extractionsystem (Alembic) (Aberdeen et al, 1995), thechunker delineates descriptor phrases for entityextraction.
Second, as part of our ongoing re-search in parsing, chunks provide the first levelof a stratified approach to syntax - the secondlevel is defined by grammatical relations, muchas in the SPARKLE effort (Carroll et al, 1997).Because of our ongoing work with chunking,we were naturally interested in evaluating ourapproach on the common CoNLL task.
In thisnote, we thus present three different evaluationsof our work on phrase-level parsing.
The firstis a baseline of sorts, our own version of the"chunking as tagging" approach introduced byRamshaw and Marcus (Ramshaw and Marcus,1995).
The second set of results reports theperformance of a trainable rule-based system,the Alembic phrase rule parser.
As a point ofcomparison, we also include a third set of mea-sures produced by running the standard Alem-bic chunker on the common task with little orno adaptation.1 Chunking as TaggingFor this first experiment, we coerced our part-of-speech tagger to generate chunk labels.
Wedid so in what can only count as the most rudi-mentary way: by training the tagger to mappart-of-speech labels to the chunk labels of thecommon task.
The learning procedure is a re-implementation f Brill's transformation-basedapproach (Brill, 1993), extended to cover ap-proximately an order of magnitude more ruleschemata.
As input, the training corpus wastagged with the parts-of-speech from the corn-N100020004000accuracy precision86 7789 8289 81recall FB= 177 7782 8281 81Table 1: Performance of the brute-force re-tagging approachmon data set: these provided an initial labelingof the data which was then directly convertedto chunk labels through the action of transfor-mation rules (Brill's so-called contextual rules).Because the learning procedure is none theswiftest, we restricted ourselves to subsets ofthe training data, acquiring rules from the first1000, 2000, and 4000 sentences of the trainingset.
In each case, we acquired 500 transforma-tion rules.
We measured the following perfor-mance of these rules on the test set.These results are hardly stellar, falling some10 points of F below the performance of previ-ous approaches to noun group detection.
To besure, the chunking task is more demanding thanthe simple identification of noun group bound-aries, so one would expect lower performance onthe harder problem.
But the rudimentary wayin which we implemented the approach is likelyalso to blame.There are a number of clear-cut ways in whichwe could attempt o improve our performanceusing this approach.
In particular, we would ex-pect to obtain better esults if we did not oblit-erate the part-of-speech of a lexeme in the pro-cess of tagging it with a chunk label.
Indeed,in our experiments, the learning procedure ac-quired transformations that simply replaced thepart-of-speech tag with a chunking tag, therebyinhibiting potentially useful downstream rulesfor accessing the part-of-speech information of160a chunk-tagged word.2 Chunk ing  w i th  the  Phrase  Ru leParserOur main interest in this common evaluation,however, was not to set new high-water markswith the approach of Ramshaw and Marcus, butto exercise our phrase rule parser.The Alembic phrase rule parser (Vilain andDay, 1996) provides the core of the system'ssyntactic processing.
In our extraction appli-cations, the phraser (as we call it) initially tagsnamed entities and other fixed-class constructs(like titles).
The phraser also treats as atomicunits the stereotypical combinations of namedentities that one finds in newswire text, e.g.,the person-title-organization apposition "U.N.secretary general Kofi Anan".
The three com-ponents of the apposition axe initially parsedas fixed-class entities, and are then combinedto form a single person-denoting phrase.
Thesepreliminary parsing steps provide part of the in-put to the chunker, which is itself implementedas a phrase rule parser.The architecture of the parser is based onBrill's approach.
The parser follows a sequenceof rules in order to build phrases out of parse is-lands.
These islands are initially introduced byinstantiating partial phrases around individuallexemes (useful for name tagging), or aroundruns of certain parts of speech (useful for bothname tagging and chunking).
It is the job of thephrase parsing rules to grow the boundaries ofthese phrases to the left or right, and to assignthem a type, e.g., a name tag or a chunk la-bel.
As with other rule sequence processors, thephraser proceeds in sequence through its cata-logue of rules, applying each in turn wherever itmatches, and then discarding it to proceed onto the next rule in the sequence.For example, in name tagging, we seedinitial phrases around runs of capitalizedwords.
A phrase such as "meetings in Parisand Rome" would produce an initial phraseanalysis of "meetings in <?>Par is</?> and<?>Rome</?>",  where the "?"
on the phrasesare initial labels that indicate the phrase hasnot received a type.The patterns that are implemented byphraseparsing rules are similar to those in Brill'stransformation-based p-o-s tagger.
A rule cantest for the presence of a given part of speech, ofa lexeme, of a list of lexemes, and so on.
Thesetests are themselves anchored to a specific locus(a phrase or lexeme) and are performed relativeto that locus.
As actions, the rules can grow theboundaries of a phrase, and set or modify its la-bel.
For example, a typical name tagging rulewould assign a LOCATION tag to any phrasepreceded by the preposition "in".
And indeed,this very rule tends to emerge as the very firstrule acquired in training a phraser-based nametagger.
We show it here with no further com-ment, trusting that its syntax is self-evident.
(def-phraser-rule:conditions (:left-i :lex "in"):actions (:set-label :LOCATION))In our particular example ("meetings in<?>Par is</?> and <?>Rome</?>") ,  thisrule would re-label the <?> phrase around Pariswith the LOCATION tag.
A subsequent rulemight then exploit the coordination to inferthat "Rome" is a location as well, implement-ing the transformation "LOCATION and <?>"--+ "LOCATION and LOCATION".
This incre-mental patching of errors is the hallmark ofBrill's approach.An interesting property of this rule languageis that the phraser can be operated either asa trainable procedure, using standard error-driven transformation learning, or as a hand-engineered system.
For the purpose of the com-mon CoNLL task, let us first present our resultsfor the trainable case.We again approached the task in a relativelyrudimentary way, in this case by applying thephrase rule learning procedure with no partic-ular adaptation to the task.
Indeed, the proce-dure can be parameterized by word lists whichit can then exploit to improve its performance.Since our main interest here was to see our base-line performance on the task, we did not harvestsuch word lists from the training data (there isan automated way to do this).
We ran a num-ber of training runs based on different partitionsof the training data, with the following overallperformance on test data, averaged across runs.accuracy 89 \]precision 89 I recall161test data precision The constituents hat were most accuratelyrecognized were noun groups (F=88), with verbgroups a close second (F=87).
These werefollowed by the ostensibly easy cases of PP's(F=86), SBAR's (F=79), and ADVP's (F=75).Our lowest performing constituent for whichthe learning procedure actually generated ruleswas ADJP's (F=37), with no rules generatedto identify CONJP's, INTJ's, LST's, or PRT's(F=0 in all these cases).In general, precision, tended to be severalpoints of F higher than recall, and in the caseof ADJP's average precision was 76 comparedto average recall of 24!3 Chunk ing  w i th  theHand-Eng ineered  SystemAs a point of comparison, we also applied ourhand-engineered chunker to the CoNLL task.We expected that it would not perfbrm at itsbest on this task, since it was designed witha significantly different model of chunking inmind, and indeed, unmodified, it produced is-appointing results:accuracy precision recall\[ Ffl- 184 80 75 \[ 77The magnitude of our error term was some-thing of a surprise.
With production runson standard newswire stories (several hundredwords in lengths) the chunker typically producesfewer errors per story than one can count on onehand.
The discrepancy with the results mea-sured on the CoNLL task is of course due tothe fact that our manually engineered parserwas designed to produce chunks to a differentstandard.The standard was carefully defined so as tobe maximally informative to downstream pro-cessing.
Generally speaking, this means that ittends to make distinctions that are not made inthe CoNLL data, e.g., splitting verbal runs suchas "failed to realize" into individual verb groupswhen more than one event is denoted.Our curiosity about these discrepancies inow piqued.
As a point of further investiga-tion, we intend to apply the phraser's trainingprocedure to adapt the manual chunker to theCoNLL task.
With transformation-based rulesequences, this is easy to do: one merely trainsthe procedure to transform the output requiredADJPADVPCONJPINT JLSTNPPPPRTSBARVPall75.89%80.64%0.00%0.00%0.00%87.85%91.77%0.00%91.36%90.34%88.82%recall Fp=l24.43% 36.9670.21% 75.060.00% 0.000.00% 0.000.0O% 0.0087.77% 87.8180.42% 85.720.00% 0.0069.16% 78.7284.13% 87.1382.91% 85.76Table 2: The results of the phrase rule parser.for the one task into that required for the other.The rules acquired in this way are then sim-ply tacked on to the end of the original rulesequence (a half dozen such rules written byhand bring the performance of the chunker upto F=82, for example).A more interesting point of investigation,however, would be to analyze the discrepan-cies between current chunk standards from thestandpoint of syntactic and semantic riteria.We look forward to reporting on this at somefuture point.Re ferencesJ.
Aberdeen, J. Burger, D. Day, L. Hirschman,P.
Robinson, and M. Vilain.
1995.
Mitre: De-scription of the alembic system used for muc-6.In Proc.
6th Message Understanding Conference(MUC-6).
Defense Advanced Research ProjectsAgency, November.E.
Brill.
1993.
A Corpus-based Approach to Lan-guage Learning.
Ph.D. thesis, U. Pennsylvania.J.
Carroll, T. Briscoe, N. Calzolari, S. Fed-erici, S. Montemagni, V. Pirrelli, G. Grefen-stette, A. Sanfilippo, G. Carroll, and M. Rooth.1997.
SPARKLE work package 1, specifica-tion of phrasal parsing, final report.
Avail-able at http://www, i lc .
pi.
cnr.
i t /spark le / -sparkle, htm, November.L.
Ramshaw and M. Marcus.
1995.
Text chunkingusing transformation-based learning.
In Proc.
ofthe 3rd Workshop on Very Large Corpora, pages82-94, Cambridge, MA, USA.M.
Vilain and D. Day.
1996.
Finite-state phraseparsing by rule sequences.
In Proceedings of the16th Intl.
Conference on Computational Linguis-tics (COLING-96).162
