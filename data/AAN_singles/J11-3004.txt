Dependency Parsing Schemata and MildlyNon-Projective Dependency ParsingCarlos Go?mez-Rodr?
?guez?Universidade da Corun?a, SpainJohn Carroll?
?University of Sussex, UKDavid Weir?University of Sussex, UKWe introduce dependency parsing schemata, a formal framework based on Sikkel?s parsingschemata for constituency parsers, which can be used to describe, analyze, and compare depen-dency parsing algorithms.
We use this framework to describe several well-known projective andnon-projective dependency parsers, build correctness proofs, and establish formal relationshipsbetween them.
We then use the framework to define new polynomial-time parsing algorithmsfor various mildly non-projective dependency formalisms, including well-nested structures withtheir gap degree bounded by a constant k in time O(n5+2k), and a new class that includes allgap degree k structures present in several natural language treebanks (which we call mildlyill-nested structures for gap degree k) in time O(n4+3k).
Finally, we illustrate how the parsingschema framework can be applied to Link Grammar, a dependency-related formalism.1.
IntroductionDependency parsing involves finding the structure of a sentence as expressed by a setof directed links (called dependencies) between individual words.
Dependency for-malisms have attracted considerable interest in recent years, having been successfullyapplied to tasks such as machine translation (Ding and Palmer 2005; Shen, Xu, andWeischedel 2008), textual entailment recognition (Herrera, Pen?as, and Verdejo 2005),relation extraction (Culotta and Sorensen 2004; Fundel, Ku?ffner, and Zimmer 2006),and question answering (Cui et al 2005).
Key characteristics of the dependency parsingapproach are that dependency structures specify head?modifier and head?complementrelationships, which form the basis of predicate?argument structure, but are not rep-resented explicitly in constituency trees; there is no need for dependency parsers topostulate the existence of non-lexical nodes; and some variants of dependency parsers?
Facultade de Informa?tica, Universidade da Corun?a Campus de Elvin?a, s/n, 15071 A Corun?a, Spain.E-mail: cgomezr@udc.es.??
School of Informatics, University of Sussex, Falmer, Brighton BN1 9QJ, UK.E-mail: J.A.Carroll@sussex.ac.uk.?
School of Informatics, University of Sussex, Falmer, Brighton BN1 9QJ, UK.E-mail: D.J.Weir@sussex.ac.uk.Submission received: 21 October 2009; revised submission received: 23 December 2010; accepted forpublication: 29 January 2011.?
2011 Association for Computational LinguisticsComputational Linguistics Volume 37, Number 3are able to represent non-projective structures (McDonald et al 2005), which is impor-tant when parsing free word order languages where discontinuous constituents arecommon.The formalism of parsing schemata, introduced by Sikkel (1997), is a useful tool forthe study of constituency parsers, supporting precise, high-level descriptions of parsingalgorithms.
Potential applications of parsing schemata include devising correctnessproofs, extending our understanding of relationships between different algorithms,deriving new variants of existing algorithms, and obtaining efficient implementationsautomatically (Go?mez-Rodr?
?guez, Vilares, and Alonso 2009).
The formalism was origi-nally defined for context-free grammars (CFG) and since then has been applied to otherconstituency-based formalisms, such as tree-adjoining grammars (Alonso et al 1999).This article considers the application of parsing schemata to the task of dependencyparsing.
The contributions of this article are as follows. We introduce dependency parsing schemata, a novel adaptation of theoriginal parsing schemata framework (see Section 2). We use the dependency parsing schemata to define and compare a numberof existing dependency parsers (projective parsers are presented inSection 3, and their formal properties discussed in Sections 4 and 5; anumber of non-projective parsers are presented in Section 6). We present parsing algorithms for several sets of mildly non-projectivedependency structures, including a parser for a new class of structures wecall mildly ill-nested, which encompasses all the structures in a number ofexisting dependency treebanks (see Section 7). We adapt the dependency parsing schema framework to the formalism ofLink Grammar (Sleator and Temperley 1991, 1993) (see Section 8).Although some of these contributions have been published previously, this articlepresents them in a thorough and consistent way.
The definition of dependency parsingschemata was first published by Go?mez-Rodr?
?guez, Carroll, and Weir (2008), along withsome of the projective schemata presented here and their associated proofs.
The resultsconcerning mildly non-projective parsing in Section 7 were first published by Go?mez-Rodr?
?guez, Weir, and Carroll (2008, 2009).
On the other hand, the material on Nivreand Covington?s projective parsers, as well as all the non-projective parsers and theapplication of the formalism to Link Grammar, are entirely new contributions of thisarticle.The notion of a parsing schema comes from considering parsing as a deductionprocess which generates intermediate results called items.
In particular, items in parsingschemata are sets of partial constituency trees taken from the set of all partial parsetrees that do not violate the constraints imposed by a grammar.
A parsing schema canbe used to obtain a working implementation of a parser by using deductive enginessuch as the ones described by Shieber et al (1995) and Go?mez-Rodr?
?guez, Vilares, andAlonso (2009), or the Dyna language (Eisner, Goldlust, and Smith 2005).2.
Dependency Parsing SchemataAlthough parsing schemata were originally defined for CFG parsers, they have sincebeen adapted to other constituency-based grammar formalisms.
This involves finding542Go?mez-Rodr?
?guez, Carroll, and Weir Dependency Parsing Schemataa suitable definition of the set of structures contained in items, and a way to definededuction steps that captures the formalism?s composition rules (Alonso et al 1999).Although it is less clear how to adapt parsing schemata to dependency parsing, a num-ber of dependency parsers have the key property of being constructive: They proceed bycombining smaller structures to form larger ones, terminating when a complete parsefor the input sentence is found.
We show that this makes it possible to define a variantof the traditional parsing schemata framework, where the encodings of intermediatedependency structures are defined as items, and the operations used to combine themare expressed as inference rules.
We begin by addressing a number of preliminaryissues.Traditional parsing schemata are used to define grammar-driven parsers, in whichthe parsing process is guided by some set of rules which are used to license deductionsteps.
For example, an Earley PREDICTOR step is tied to a particular grammar rule, andcan only be executed if such a rule exists.
Some dependency parsers are also grammar-driven.
For example, those described by Lombardo and Lesmo (1996), Barbero et al(1998), and Kahane, Nasr, and Rambow (1998) are based on the formalizations of depen-dency grammar CFG-like rules described by Hays (1964) and Gaifman (1965).
However,many of the algorithms (Eisner 1996; Yamada and Matsumoto 2003) are not traditionallyconsidered to be grammar-driven, because they do not use an explicit formal grammar;decisions about which dependencies to create are taken individually, using probabilis-tic models (Eisner 1996) or classifiers (Yamada and Matsumoto 2003).
These are calleddata-driven parsers.
To express such algorithms as deduction systems, we use thenotion of D-rules (Covington 1990).
D-rules have the form (a, i) ?
(b, j), which speci-fies that a word b located at position j in the input string can have the word a inposition i as a dependent.
Deduction steps in data-driven parsers can be associated withthe D-rules corresponding to the links they create, so that parsing schemata for suchparsers are defined using grammars of D-rules.
In this way, we obtain a representationof some of the declarative aspects of these parsing strategies that is independent of theparticular model used to make the decisions associated with each D-rule.
Note thatthis representation is useful for designing control structures or probabilistic models forthe parsers, because it makes explicit the choice points where the models will haveto make probabilistic decisions, as well as the information available at each of thosechoice points.
Additionally, D-rules allow us to use an uniform description that is validfor both data-driven and grammar-driven parsers, because D-rules can function likegrammatical rules.The fundamental structures in dependency parsing are dependency trees.
There-fore, just as items for constituency parsers encode sets of partial constituency trees,items for dependency parsers can be defined using partial dependency trees.
How-ever, dependency trees cannot express the fact that a particular structure has beenpredicted, but not yet built; this is required for grammar-based algorithms such asthose of Lombardo and Lesmo (1996) and Kahane, Nasr, and Rambow (1998).
Theformalism can be made general enough to include these parsers by using a novel way ofrepresenting intermediate states of dependency parsers based on a form of dependencytrees that include nodes labelled with preterminals and terminals (Go?mez-Rodr?
?guez,Carroll, and Weir 2008; Go?mez-Rodr?
?guez 2009).
For simplicity of presentation, we willonly use this representation (called extended dependency trees) in the grammar-basedalgorithms that need it, and we will define the formalism and the rest of the algo-rithms with simple dependency trees.
Some existing dependency parsing algorithms,for example, the algorithm of Eisner (1996), involve steps that connect spans whichcan represent disconnected dependency graphs.
Such spans cannot be represented by543Computational Linguistics Volume 37, Number 3a single dependency tree.
Therefore, our formalism allows items to be sets of forests ofpartial dependency trees, rather than sets of trees.We are now ready to define the concepts needed to specify item sets for dependencyparsers.Definition 1An interval (with endpoints i and j) is a set of natural numbers of the form [i..j] = {k |i ?
k ?
j}.
We will use the notation i..j for the ordered list of the numbers in [i..j].
Adependency graph for a string w = w1 .
.
.wn is a graph G = (V, E), where V ?
[1..n]and E ?
V ?
V.The edge (i, j) is written i ?
j, and each such edge is called a dependency link,encoding the fact that the word wi is a syntactic dependent (or child) of wj or, conversely,that wj is the parent, governor, or head of wi.
We write i ? j to denote that there existsa (possibly empty) path from i to j.
The projection of a node i, denoted i, is the set ofreflexive-transitive dependents of i, that is, i = { j ?
V | j ? i}.
In contexts where werefer to different dependency graphs, we use the notation iG to specify the projectionof a node i in the graph G.Definition 2A dependency graph T for a string w1 .
.
.wn is called a dependency tree for that stringif it contains no cycles and all of its nodes have exactly one parent, except for one nodethat has none and is called the root or head of the tree T, denoted head(T).
The yield ofa dependency tree T, denoted yield(T), is the ordered list of its nodes.
We will use theterm dependency forest to refer to a set of dependency trees for the same string,1 andthe generic term dependency structure to refer to a dependency tree or forest.A dependency tree is said to be a parse tree for a string w1 .
.
.wn if its yield is 1..n.Definition 3We say that a dependency graph G = (V, E) for a string w1 .
.
.wn is projective if i is aninterval for every i ?
V.Definition 4Let ?
(G) be the set of dependency trees which are syntactically well-formed accordingto a given grammar G (which may be a grammar of D-rules or of CFG-like rules, asexplained previously).
We define an item set for dependency parsing as a set I ?
?,where ?
is a partition of the power set, (?
(G)), of the set ?(G).
Each element of I,called an item, is a set of dependency forests for strings.
For example, each member ofthe item [1, 5] in the item set of the parser by Yamada and Matsumoto (2003) that willbe explained in Section 3.4 is a dependency forest with two projective trees, one withhead 1 and the other with head 5, and such that the concatenation of their yields is 1..5.Figure 1 shows the three dependency forests that constitute the contents of this itemunder a specific grammar of D-rules.Following Sikkel (1997), items are sets of syntactic structures and tuples are a short-hand notation for such sets, as seen in the previous example.
An alternative approach,1 Note that the trees in a dependency forest can have different yields, because the node set of a dependencytree for a string w1 .
.
.wn can be any subset of [1..n].
In fact, all the forests used by the parsers in thisarticle contain trees with non-overlapping yields, although this is not required by the definition.544Go?mez-Rodr?
?guez, Carroll, and Weir Dependency Parsing SchemataFigure 1Contents of the item [1, 5] from the Yamada and Matsumoto (2003) parsing schema under agrammar of D-rules {(w2, 2) ?
(w1, 1) , (w3, 3) ?
(w1, 1) , (w3, 3) ?
(w5, 5) , (w3, 3) ?
(w4, 4) ,(w4, 4) ?
(w5, 5)}.following Shieber, Schabes, and Pereira (1995), would be to define items as tuplesthat denote sets of syntactic structures.
Although the latter approach provides moreflexibility, this makes defining the relationships between parsers less straightforward.In any case, because tuple notation is used to write schemata under both approaches,the schemata we provide are compatible with both interpretations.Having defined an item set for dependency parsing, the remaining definitions areanalogous to those in Sikkel?s theory of constituency parsing (Sikkel 1997), and are notpresented in full detail.
A dependency parsing system is a deduction system (I, H, D)where I is a dependency item set as defined here, H is a set containing initial itemsor hypotheses (not necessarily contained in I), and D ?
( (H ?
I ) ?
I ) is a set ofdeduction steps defining an inference relation.Final items in this formalism will be those containing some forest F containing aparse tree for some string w1 .
.
.wn.
In parsers for general non-projective structures, anyitem containing such a tree will be called a coherent final item for w1 .
.
.wn.
In schematafor parsers that are constrained to a more restrictive class T of dependency trees, such asprojective parsers, coherent final items will be those containing parse trees for w1 .
.
.wnthat are in T .
For example, because we expect correct projective parsers to produce onlyprojective structures, coherent final items for projective parsers will be those containingprojective parse trees for w1 .
.
.wn.
Correctness proofs typically define a set of coherentitems, such that its intersection with final items produces the set of coherent final items.The definition of coherent items depends on each particular proof.2For each input string, a parsing schema?s deduction steps allow us to infer a set ofitems, called derivable items, for that string.3 A parsing schema is said to be sound if allderivable final items it produces for any arbitrary string are coherent for that string.
Aparsing schema is said to be complete if all coherent final items are derivable.
A correctparsing schema is one which is both sound and complete.Parsing schemata are located at a higher abstraction level than parsing algorithms,and formalize declarative aspects of their logic: A parsing schema specifies a set ofintermediate results that are obtained by the algorithm (items) and a set of operationsthat can be used to obtain new such results from existing ones (deduction steps); but itmakes no claim about the order in which to execute the operations or the data structuresto use for storing the results.3.
Projective SchemataIn this section, we show how dependency parsing schemata can be used to describeseveral existing projective dependency parsers.2 Coherent (final) items are called correct (final) items in the original formulation by Sikkel (1997).3 Derivable items are called valid items in the original formulation by Sikkel (1997).545Computational Linguistics Volume 37, Number 3Figure 2Representation of the [i, j, h] item in Collins?s parser, together with one of the dependencystructures contained in it (left side); and of the antecedents and consequents of an L-LINK step(right side).
White rectangles in an item represent intervals of nodes that have been assigned ahead by the parser, and dark squares represent nodes that have no head.3.1 Collins (1996)One of the most straightforward projective dependency parsing strategies was intro-duced by Collins (1996), and is based on the CYK bottom?up parsing strategy (Kasami1965; Younger 1967).
Collins?s parser works with dependency trees which are linked toeach other by creating links between their heads.
The schema for this parser maps everyset of D-rules G and input string w1 .
.
.wn to an instantiated dependency parsing system(ICol96,H, DCol96) such that:Item set: The item set is defined as ICol96 = {[i, j, h] | 1 ?
i ?
h ?
j ?
n}, where item [i, j, h]is defined as the set of forests containing a single projective dependency tree T of ?
(G)such that yield(T) is of the form i..j and head(T) = h (see Figure 2, left side).
From nowon, we will implicitly assume that the dependency trees appearing in items of a parsingschema for a grammar G are taken from the set ?
(G) of syntactically well-formed treesaccording to G.This means that Collins?s parser will be able to infer an item [i, j, h] in the presenceof an input string w1 .
.
.wn if, using our set of D-rules, it is possible to build a projectivedependency tree headed at h that spans the substring wi .
.
.wj of the input.Hypotheses: For an input string w1 .
.
.wn, the set of hypotheses is H = {[i, i, i] | 0 ?
i ?n + 1}, where each item contains a forest with a single dependency tree having onlyone node i.
This same set of hypotheses is used for all the parsers, so is not repeatedfor subsequent schemata.4 Note that the nodes 0 and n + 1 used in the definition do notcorrespond to actual input words?these are dummy nodes that we call beginning-of-sentence and end-of-sentence markers, respectively, and will be needed by several ofthe parsers described subsequently.Final items: The set of final items is {[1, n, h] | 1 ?
h ?
n}.
These items trivially representparse trees for the input sentence whose head is some node h, expressing that the wordwh is the sentence?s syntactic head.4 Although in the parsers described in Section 7 we use a different notation for the hypotheses, they stillare the same, as explained later.546Go?mez-Rodr?
?guez, Carroll, and Weir Dependency Parsing SchemataDeduction steps: The set of deduction steps, DCol96, is the union of the following:R-LINK:[i, j, h1][ j + 1, k, h2][i, k, h2](wh1 , h1) ?
(wh2 , h2) L-LINK:[i, j, h1][ j + 1, k, h2][i, k, h1](wh2 , h2) ?
(wh1 , h1)allowing us to join two contiguous trees by linking their heads with a rightward orleftward link, respectively.
Figure 2 (right side) shows a graphical representation ofhow trees are joined by the L-LINK step.
Note that, because this parsing strategy isdata-driven, D-rules are used as side conditions for the parser?s deduction steps.
Sideconditions restrict the inference relation by specifying which combinations of values arepermissible for the variables appearing in the antecedents and consequent of deductionsteps.This parsing schema specifies a recognizer: Given a set of D-rules and an inputstring w1 .
.
.wn, the sentence can be parsed (projectively) under those D-rules if andonly if the deduction system infers a coherent final item.
When executing this schemawith a deductive engine, the parse forest can be recovered by following back pointers,as in constituency parsers (Billot and Lang 1989).This schema formalizes a parsing logic which is independent of the order and theway linking decisions are taken.
Statistical models can be used to determine whethera step linking words a and b in positions i and j?i.e., having (a, i) ?
(b, j) as a sidecondition?is executed or not, and probabilities can be attached to items in order to as-sign different weights to different analyses of the sentence.
The side conditions providean explicit representation of the choice points where probabilistic decisions are made bythe control mechanism that is executing the schema.
The same principle applies to allD-rule-based parsers described in this article.3.2 Eisner (1996)Based on the number of free variables used in deduction steps of Collins?s parser, it isapparent that its time complexity is O(n5): There are O(n5) combinations of index valueswith which each of its LINK steps can be executed.5 This complexity arises because aparentless word (head) may appear in any position in the items generated by the parser;the complexity can be reduced to O(n3) by ensuring that parentless words only appearat the first or last position of an item.
This is the idea behind the parser defined byEisner (1996), which is still in wide use today (McDonald, Crammer, and Pereira 2005;Corston-Oliver et al 2006).
The parsing schema for this algorithm is defined as follows.Item set: The item set isIEis96 = {[i, j, True, False] | 0 ?
i ?
j ?
n} ?
{[i, j, False, True] | 0 ?
i ?
j ?
n}?
{[i, j, False, False] | 0 ?
i ?
j ?
n},where item [i, j, True, False] corresponds to [i, j, j] ?
ICol96, item [i, j, False, True] corre-sponds to item [i, j, i] ?
ICol96, and item [i, j, False, False] is defined as the set of forests5 For this and the rest of the complexity results in this article, we assume that the linking decisionassociated with a D-rule can be made in constant time.547Computational Linguistics Volume 37, Number 3of the form {T1, T2} such that T1, T2 are projective, head(T1) = i, head(T2) = j, and thereis some k (i ?
k < j) such that yield(T1) = i..k and yield(T2) = k + 1..j.The flags b, c in [i, j, b, c] indicate whether the nodes i and j, respectively, have aparent in the item.
Items with one of the flags set to True represent dependency treeswhere the node i or j is the head, whereas items with both flags set to False representpairs of trees headed at nodes i and j which jointly dominate the substring wi .
.
.wj.Items of this kind correspond to disconnected dependency graphs.Deduction steps: The set of deduction steps is as follows:6INITTER:[i, i, i][i + 1, i + 1, i + 1][i, i + 1, False, False]COMBINESPANS:[i, j, b, c][ j, k, not(c), d][i, k, b, d]R-LINK:[i, j, False, False][i, j, True, False](wi, i) ?
(wj, j) L-LINK:[i, j, False, False][i, j, False, True](wj, j) ?
(wi, i)where the R-LINK and L-LINK steps establish a dependency link between the heads ofan item containing two trees (i.e., having both flags set to False), producing a new itemcontaining a single tree.
The COMBINESPANS step is used to join two items that overlapat a single word, which must have a parent in only one of the items, so that the resultof joining trees coming from both items (without creating any dependency link) is awell-formed dependency tree.Final items: The set of final items is {[0, n, False, True]}.
Note that these items represent de-pendency trees rooted at the beginning-of-sentence marker 0, which acts as a ?dummyhead?
for the sentence.
In order for the algorithm to parse sentences correctly, we needto define D-rules to allow the real sentence head to be linked to the node 0.3.3 Eisner and Satta (1999)Eisner and Satta (1999) define an O(n3) parser for split head automaton grammarswhich can be used for dependency parsing.
This algorithm is conceptually simpler thanEisner?s (1996) algorithm, because it only uses items representing single dependencytrees, avoiding items of the form [i, j, False, False].Item set: The item set is IES99 = {[i, j, i] | 0 ?
i ?
j ?
n} ?
{[i, j, j] | 0 ?
i ?
j ?
n}, whereitems are defined as in Collins?s parsing schema.Deduction steps: The deduction steps for this parser are the following:R-LINK:[i, j, i][ j + 1, k, k][i, k, k](wi, i) ?
(wk, k) L-LINK:[i, j, i][ j + 1, k, k][i, k, i](wk, k) ?
(wi, i)R-COMBINER:[i, j, i][ j, k, j][i, k, i]L-COMBINER:[i, j, j][ j, k, k][i, k, k]6 We could have used items [i, i + 1, False, False] as hypotheses for this parsing schema, and not requirean Initter step, but we prefer a standard set of hypotheses valid for all parsers as it facilitates morestraightforward proofs of the relations between schemata.548Go?mez-Rodr?
?guez, Carroll, and Weir Dependency Parsing Schematawhere LINK steps create a dependency link between two dependency trees spanningadjacent segments of the input, and COMBINER steps join two overlapping trees by agraph union operation that does not create new links.
COMBINER steps follow the samemechanism as those in the algorithm of Eisner (1996), and LINK steps work analogouslyto those of Collins (1996), so this schema can be seen as being intermediate betweenthose two algorithms.
These relationships will be formally described in Section 4.Final items: The set of final items is {[0, n, 0]}.
By convention, parse trees have thebeginning-of-sentence marker 0 as their head, as in the previous algorithm.When described for head automaton grammars (Eisner and Satta 1999), this algo-rithm appears to be more complex to understand and implement than the previousone, requiring four different kinds of items to keep track of the state of the automataused by the grammars.
However, this abstract representation of its underlying seman-tics reveals that this parsing strategy is, in fact, conceptually simpler for dependencyparsing.3.4 Yamada and Matsumoto (2003)Yamada and Matsumoto (2003) define a deterministic, shift-reduce dependency parserguided by support vector machines, which achieves over 90% dependency accuracy onSection 23 of the Wall Street Journal Penn Treebank.
Parsing schemata cannot specifycontrol strategies that guide deterministic parsers; schemata work at an abstractionlevel, defining a set of operations without procedural constraints on the order inwhich they are applied.
However, deterministic parsers can be viewed as optimizationsof underlying nondeterministic algorithms, and we can represent the actions of theunderlying parser as deduction steps, abstracting away from the deterministic im-plementation details, obtaining a potentially interesting nondeterministic dependencyparser.Actions in Yamada and Matsumoto?s parser create links between two target nodes,which act as heads of neighboring dependency trees.
One of the actions creates a linkwhere the left target node becomes a child of the right one, and the head of a treelocated directly to the left of the target nodes becomes the new left target node.
Theother action is symmetric, performing the same operation with a right-to-left link.
AnO(n3) nondeterministic parser generalizing this behavior can be defined as follows.Item set: The item set is IYM03 = {[i, j] | 0 ?
i ?
j ?
n + 1}, where each item [i, j] corre-sponds to the item [i, j, False, False] in IEis96.Deduction steps: The deduction steps are as follows:INITTER:[i, i, i][i + 1, i + 1, i + 1][i, i + 1]R-LINK:[i, j][ j, k][i, k](wj, j) ?
(wk, k) L-LINK:[i, j][ j, k][i, k](wj, j) ?
(wi, i)where a LINK step joins a pair of items containing forests with two trees each andoverlapping at a head node, and creates a dependency link from their common head toone of the peripheral heads.
Note that this is analogous to performing an Eisner LINKstep immediately followed by an Eisner COMBINE step, as will be further analyzed inSection 4.Final items: The set of final items is {[0, n + 1]}.
For this set to be well-defined, thegrammar must not have D-rules of the form (wi, i) ?
(wn+1, n + 1), that is, it must not549Computational Linguistics Volume 37, Number 3Figure 3Grounded extended dependency tree and associated dependency structure.allow the end-of-sentence marker to govern any words.
If the grammar satisfies thiscondition, it is trivial to see that every forest in an item of the form [0, n + 1] must containa parse tree rooted at the beginning-of-sentence marker and with yield 0..n.As can be seen from the schema, this algorithm requires less bookkeeping than theother parsers described here.3.5 Lombardo and Lesmo (1996) and Other Earley-Based ParsersThe algorithms presented so far are based on making individual decisions about de-pendency links, represented by D-rules.
Other parsers, such as that of Lombardo andLesmo (1996), use grammars with CFG-like rules which encode the preferred order ofdependents for each given governor.
For example, a rule of the form N(Det ?
PP) is usedto allow N to have Det as left dependent and PP as right dependent.
The algorithmby Lombardo and Lesmo is a version of Earley?s CFG parser (Earley 1970) that usesGaifman?s dependency grammar (Gaifman 1965).As this algorithm predicts dependency relations before building them, item setscontain extended dependency trees, trees that have two kinds of nodes: preterminalnodes and terminal nodes.
Depending on whether all the preterminal nodes have beenlinked to terminals, extended dependency trees can either be grounded, in which casethey are isomorphic to traditional dependency graphs (see Figure 3), or ungrounded, asin Figure 4, in which case they capture parser states in which some structure has beenpredicted, but not yet found.
Note that a dependency graph can always be extractedfrom such a tree, but in the ungrounded case different extended trees can be associatedwith the same graph.
Go?mez-Rodr?
?guez, Carroll, and Weir (2008) present extendeddependency trees in more detail.Item set: The item set is ILomLes = {[A(?
?
?
), i, j] | A(??)
?
P ?
1 ?
i ?
j + 1 ?
n} where?
and ?
are strings; P is a set of CFG-like rules;7 and each item [A(?
?
?
), i, j] representsthe set of projective extended dependency trees rooted at A, where the direct children ofA are ?
?, and the subtrees rooted at ?
have yield i..j.
Note that Lombardo and Lesmo?sparser uses both grounded trees (in items [A(??
), i, j]) and non-grounded trees (in items7 A CFG-like rule A(?
?
?)
rewrites a preterminal A to strings ?x?
over terminals and preterminals,where ?, ?
are strings of preterminals and x is a terminal of category A (the head of the rule).
A specialrule ?
(S) is used to state that the preterminal S can act as the root of an extended dependency tree.550Go?mez-Rodr?
?guez, Carroll, and Weir Dependency Parsing SchemataFigure 4Non-grounded extended dependency tree: A determiner and adjective have been found and,according to the grammar, we expect a noun that will act as their common head.
As this headhas not been read, no dependency links have been established.[A(?
?
?
), i, j], where ?
is nonempty).
Items in this parser can represent infinite sets ofextended dependency trees, as in Earley?s CFG parser but unlike items in D-rule-basedparsers, which are finite sets.Deduction steps: The deduction steps for this parsing schema are as follows:INITTER:[(?S), 1, 0] ?
(S) ?
P PREDICTOR:[A(?
?
B?
), i, j][B(??
), j + 1, j] B(?)
?
PSCANNER:[A(?
?
?
), i, h ?
1][h, h, h][A(?
 ??
), i, h] wh IS A COMPLETER:[A(?
?
B?
), i, j][B(??
), j + 1, k][A(?B ?
?
), i, k]Final items: The final item set is {[(S?
), 1, n]}.The schema for Lombardo and Lesmo?s parser is a variant of the Earley constituencyparser (cf.
Sikkel 1997), with minor changes to adapt it to dependency grammar (forexample, the SCANNER always moves the dot over the head symbol ?, rather thanover a terminal symbol).
Analogously, other dependency parsing schemata based onCFG-like rules can be obtained by modifying CFG parsing schemata of Sikkel (1997):The algorithm of Barbero et al (1998) can be obtained from the left-corner parser, andthe parser described by Courtin and Genthial (1998) is a variant of the head-cornerparser.3.6 Nivre (2003)Nivre (2003) describes a shift-reduce algorithm for projective dependency parsing,later extended by Nivre, Hall, and Nilsson (2004).
With linear-time performance andcompetitive parsing accuracy (Nivre et al 2006; Nivre and McDonald 2008), it is oneof the parsers included in the MaltParser system (Nivre et al 2007), which is currentlywidely used (e.g., Nivre et al 2007; Surdeanu et al 2008).The parser proceeds by reading the sentence from left to right, using a stack and fourdifferent kinds of transitions between configurations.
The transition system defined byall the possible configurations and transitions is nondeterministic, and machine learningtechniques are used to train a mechanism that produces a deterministic parser.A deduction system describing the transitions of the parser is defined by Nivre,Hall, and Nilsson (2004), with the following set of rules that describes transitions551Computational Linguistics Volume 37, Number 3between configurations (we use the symbol ?
for a stack and the notation ?
:: h for thestack resulting from pushing h into ?, and ?i to represent a buffer of the form wi .
.
.wn):Initter(?
?, ?0, ?
)Shift(?, ?f , V)(?
:: f, ?f+1, V)Reduce(?
:: l, ?f , V)(?, ?f , V)?al ?
ak ?
VL-Link(?
:: l, ?f , V)(?
:: l :: f, ?f+1, V ?
{af ?
al})af ?
al | af ?
ak ?
VR-Link(?
:: l, ?f , V)(?, ?f , V ?
{al ?
af})al ?
af | al ?
ak ?
VThis set of inference rules is not a parsing schema, however, because the entities itworks with are not items.
Although the antecedents and consequents in this deductionsystem are parser configurations, they do not correspond to disjoint sets of dependencystructures (several configurations may correspond to the same dependency structures),and therefore do not conform to the definition of an item set.
It would be possibleto define parsing schemata in a different way with a weaker definition of item setsallowing these configurations as items, but this would make it harder to formalizerelations between parsers, because they rely on the properties of item sets.A parsing schema for Nivre?s parser can be obtained by abstracting away therules in the system that are implementing control structures, however, and expressingonly declarative aspects of the parser?s tree building logic.
To do this, we first obtaina simplified version of the deduction system.
This version of the parser is obtainedby storing an index f rather than the full buffer ?f in each configuration, and thengrouping configurations that share common features, making them equivalent for theside conditions of the system: Instead of storing the full set of dependency links that thealgorithm has constructed up to a given point (denoted by V), we only keep track ofwhether elements in the stack have been assigned a head or not; and we represent thisby using a stack of pairs (l, b), where l is the position of a word in the string and b is aflag which is True if the corresponding node has been assigned a head or False if it hasnot:Initter(?
?, 0) Shift(?, f )(?
:: ( f, False), f + 1)Reduce(?
:: (l, True), f )(?, f )L-Link(?
:: (l, h), f )(?
:: (l, h) :: ( f, True), f + 1)(wf , f ) ?
(wl, l) R-Link(?
:: (l, False), f )(?, f )(wl, l) ?
(wf , f )To obtain a parsing schema from this deduction system, we retain only rules per-taining to the way in which the parser joins dependency structures and builds linksbetween them.
In particular, the Reduce step is just a mechanism to select which of a setof possible ?linkable words?
to link to the word currently being read.
Two differentconfigurations corresponding to the same dependency structure may have differentlists of words in the stack depending on which Reduce steps have been executed.
Inthe parsing schema, these configurations must correspond to the same item, as theyinvolve the same dependency structures.
To define an item set for this parser, we mustestablish which words could be on the stack at each configuration.A node in a dependency graph T is right-linkable if it is not a dependent of anynode situated to its right, and is not covered by any dependency link ( j is covered by552Go?mez-Rodr?
?guez, Carroll, and Weir Dependency Parsing Schematathe link i ?
k if i < j < k or i > j > k).
A link cannot be created between a non-right-linkable node and any node to the right of T without violating the projectivity property.When the parser is reading a particular word at position f , the following properties holdfor all nodes to the left of f (nodes 0 .
.
.
f ?
1): If the node i is not right-linkable, then it cannot be on the stack. If the node i does not have a head, then it must be on the stack.
(Note thatnodes that do not have a head assigned are always right-linkable.
) If the node i has a head and it is right-linkable, then it may or may not beon the stack, depending on the transitions that we have executed.A dependency parsing schema represents items with lists (instead of stacks) containingall the nodes found so far which are right-linkable, and a flag associated with each nodeindicating whether it has been assigned a head or not.
Instead of using Reduce steps todecide which node to choose as a head of the one corresponding to the currently-readword, we allow any node in the list that does not have a headless node to its right tobe the head; this is equivalent to performing several Reduce transitions followed by anL-link transition.Item set: The item set isINiv = {[i, ?
(i1, b1), .
.
.
, (ik, bk)?]
| 0 ?
i ?
n + 1 ?
0 ?
i1 ?
.
.
.
?
ik ?
n ?
bj ?
{False, True}}where an item [i, L] represents the set of forests of projective trees of the form F ={T1, .
.
.
, Tw} (w > 0) satisfying the following: The concatenation of the yields of T1, .
.
.
, Tw is 0..i, The heads of the trees T1, .
.
.
, Tw?1 are the nodes j where ( j, False) ?
L;and the head of the tree Tw is the node i, The right-linkable nodes in the dependency graph corresponding tothe union of the trees in F are the nodes j where ( j, b) ?
L, withb ?
{False, True}.Final items: The set of final items is {[n + 1, ?
(0, False), (v1, True), .
.
.
, (vk, True)?]
| 1 ?
vj ?n}, the set of items containing a forest with a single projective dependency tree T headedat the dummy node 0, whose yield spans the whole input string, and which containsany set of right-linkable words.Deduction steps: The deduction steps are as follows:INITTER:[0, ??]
ADVANCE:[i, ?
(i1, b1), .
.
.
, (ik, bk)?
][i + 1, ?
(i1, b1), .
.
.
, (ik, bk), (i, False)?
]L-LINK:[i, ?
(i1, b1), .
.
.
, (ik, bk), (l, b), (v1, True), .
.
.
, (vr, True)?
][i + 1, ?
(i1, b1), .
.
.
, (ik, bk), (l, b), (i, True)?
](wi, i) ?
(wl, l)R-LINK:[i, ?
(i1, b1), .
.
.
, (ik, bk), (h, False), (v1, True), .
.
.
, (vr, True)?
][i, ?
(i1, b1), .
.
.
, (ik, bk)?
](wh, h) ?
(wi, i)553Computational Linguistics Volume 37, Number 3Note that a naive nondeterministic implementation of this schema in a genericdeductive engine would have exponential complexity.
The linear complexity in Nivre?salgorithm is achieved by using a control strategy that deterministically selects a singletransition at each state.3.7 Covington?s (2001) Projective ParserCovington (2001) defines a non-projective dependency parser, and a projective vari-ant called Algorithm LSUP (for List-based Search with Uniqueness and Projectivity).Unfortunately, the algorithm presented in Covington (2001) is not complete: It doesnot parse all projective dependency structures, because when creating leftward linksit assumes that the head of a node i must be a reflexive-transitive head of the node i ?
1,which is not always the case.
For instance, the structure shown in Figure 5 cannot beparsed because the constraints imposed by the algorithm prevent it from finding thehead of 4.The MaltParser system (Nivre et al 2007) includes an implementation of a completevariant of Covington?s LSUP parser where these constraints have been relaxed.
Thisimplementation has the same tree building logic as the parser described by Nivre (2003),differing from it only with respect to the control structure.
Thus, it can be seen as adifferent realization of the schema shown in Section 3.6.4.
Relations Between Dependency ParsersThe parsing schemata framework can be exploited to establish how different algorithmsare related, improving our understanding of the features of these parsers, and poten-tially exposing new algorithms that combine characteristics of existing parsers in novelways.
Sikkel (1994) defines various relations between schemata that fall into two cate-gories: generalization relations, which are used to obtain more fine-grained versions ofparsers, and filtering relations, which can be seen as the converse of generalization andare used to reduce the number of items and/or steps needed for parsing.
Informally, aparsing schema can be generalized from another via the following transformations: Item refinement: P2 is an item refinement of P1, written P1ir??
P2, if thereis a mapping between items in both parsers such that single items in P1are mapped into multiple items in P2 and individual deductions arepreserved. Step refinement: P1sr??
P2 if the item set of P1 is a subset of that of P2and every deduction step in P1 can be emulated by a sequence of stepsin P2.Figure 5A projective dependency structure that cannot be parsed with Covington?s LSUP algorithm.554Go?mez-Rodr?
?guez, Carroll, and Weir Dependency Parsing SchemataA schema can be obtained from another by filtering in the following ways: Static/dynamic filtering: P1sf/df????
P2 if the item set of P2 is a subset of thatof P1 and P2 allows a subset of the direct inferences in P1.
Sikkel (1994)explains the distinction between static and dynamic filtering, which is notused here. Item contraction: The inverse of item refinement: P1ic??
P2 if P2ir??
P1. Step contraction: The inverse of step refinement: P1sc??
P2 if P2sr??
P1.Many of the parsing schemata described in Section 3 can be related (see Figure 6), butfor space reasons we sketch proofs for only the more interesting cases.Theorem 1Yamada and Matsumoto (2003) sr??
Eisner (1996).Proof 1It is easy to see from the schema definitions that IYM03 ?
IEis96.
We must verify thatevery deduction step in the Yamada and Matsumoto (2003) schema can be emulated bya sequence of inferences in the Eisner (1996) schema.
For the INITTER step this is trivialas the INITTERs of both parsers are equivalent.
Expressing the R-LINK step of Yamadaand Matsumoto?s parser in the notation used for Eisner items gives:R-Link[i, j, False, False] [ j, k, False, False][i, k, False, False](wj, j) ?
(wk, k)This can be emulated in Eisner?s parser by an R-LINK step followed by aCOMBINESPANS step:[ j, k, False, False][ j, k, True, False] (by R-LINK)[ j, k, True, False], [i, j, False, False][i, k, False, False] (by COMBINESPANS)Figure 6Relating several well-known dependency parsers.
Arrows pointing up correspond togeneralization relations, while those pointing down correspond to filtering.
The specific subtypeof relation is shown in each arrow?s label, following the notation in Section 4.555Computational Linguistics Volume 37, Number 3Symmetrically, the L-LINK step in Yamada and Matsumoto?s parser can be emu-lated by an L-LINK followed by a COMBINESPANS in Eisner?s.
Theorem 2Eisner and Satta (1999) sr??
Eisner (1996).Proof 2Writing R-LINK in Eisner and Satta?s parser in the notation used for Eisner items givesR-LINK:[i, j, False, True] [ j + 1, k, True, False][i, k, True, False](wi, i) ?
(wk, k)This inference can be emulated in Eisner?s parser as follows:[ j, j + 1, False, False] (by INITTER)[i, j, False, True], [ j, j + 1, False, False][i, j + 1, False, False] (by COMBINESPANS)[i, j + 1, False, False], [ j + 1, k, True, False][i, k, False, False] (by COMBINESPANS)[i, k, False, False][i, k, True, False] (by R-LINK)The proof corresponding to the L-LINK step is symmetric.
As for the R-COMBINER andL-COMBINER steps in Eisner and Satta?s parser, it is easy to see that they are particularcases of the COMBINESPANS step in Eisner?s, and therefore can be emulated by a singleapplication of COMBINESPANS.
Note that, in practice, these two relations mean that the parsers by Eisner and Satta(1999) and Yamada and Matsumoto (2003) are more efficient, at the schema level, thanthat of Eisner (1996), in that they generate fewer items and need fewer steps to performthe same deductions.
These two parsers also have the interesting property that theyuse disjoint item sets (one uses items representing trees while the other uses itemsrepresenting pairs of trees); and the union of these disjoint sets is the item set usedby Eisner?s parser.
The optimization in Yamada and Matsumoto?s parser comes fromcontracting deductions in Eisner?s parser so that linking operations are immediatelyfollowed by combining operations; whereas Eisner and Satta?s parser does the opposite,forcing combining operations to be followed by linking operations.By generalizing the linking steps in Eisner and Satta?s parser so that the head ofeach item can be in any position, we obtain an O(n5) parser which can be filtered intothe parser of Collins (1996) by eliminating the COMBINER steps.
From Collins?s parser,we obtain an O(n5) head-corner parser based on CFG-like rules by an item refinementin which each Collins item [i, j, h] is split into a set of items [A(?
?
?
?
?
), i, j, h].
Therefinement relation between these parsers only holds if for every D-rule B ?
A there isa corresponding CFG-like rule A ?
.
.
.B .
.
.
in the grammar used by the head-cornerparser.
Although this parser uses three indices i, j, h, using CFG-like rules to guide link-ing decisions makes the h indices redundant.
This simplification is an item contractionwhich results in an O(n3) head-corner parser.
From here, we can follow the proceduredescribed by Sikkel (1994) to relate this head-corner algorithm to parsers analogousto other algorithms for CFGs.
In this way, we can refine the head-corner parser to avariant of the algorithm by de Vreught and Honig (1989) (Sikkel 1997), and by successivefilters we reach a left-corner parser which is equivalent to the one described by Barberoet al (1998), and a step contraction of the Earley-based dependency parser by Lombardo556Go?mez-Rodr?
?guez, Carroll, and Weir Dependency Parsing Schemataand Lesmo (1996).
The proofs for these relations are the same as those given by Sikkel(1994), except that the dependency variants of each algorithm are simpler (due to theabsence of epsilon rules and the fact that the rules are lexicalized).
The names usedfor schemata dVH1, dVH2, dVH3, and buLC shown in Figure 6 come from Sikkel (1994,1997).
These dependency parsing schemata are versions of the homonymous schematawhose complete description can be found in Sikkel (1997), adapted for dependencyparsing.
Go?mez-Rodr?
?guez (2009) gives a more thorough explanation of these relationsand schemata.5.
Proving CorrectnessAnother use of the parsing schemata framework is that it is helpful in establishingthe correctness of a parser.
Furthermore, relations between schemata can be used toestablish the correctness of one schema from that of related ones.
In this section, weshow that the schemata for Yamada and Matsumoto (2003) and Eisner and Satta (1999)are correct, and use this to prove the correctness of the schema for Eisner (1996).Theorem 3The Eisner and Satta (1999) parsing schema is correct.Proof 3To prove correctness, we must show both soundness and completeness.
To verifysoundness we need to check that every individual deduction step in the parser infers acoherent consequent item when applied to coherent antecedents (i.e., in this case, thatsteps always generate non-empty items that conform to the definition in Section 3.3).This is shown by checking that, given two antecedents of a deduction step that containa tree licensed by a set of D-rules G, the consequent of the step also contains such a tree.The tree for the consequent is built from the trees corresponding to the antecedents: bya graph union operation, in the case of COMBINER steps; or by linking the heads of bothtrees with a dependency relation licensed by G, in the case of LINK steps.To prove completeness we prove that all coherent final items are derivable byproving the stronger result that all coherent items are derivable.
We show this by stronginduction on the length of items, where the length of an item ?
= [i, k, h] is defined aslength(?)
= k ?
i + 1.
Coherent items of length 1 are the hypotheses of the schema (of theform [i, i, i]) which are trivially derivable.
We show that, if all coherent items of length mare derivable for all 1 ?
m < l, then items of length l are also derivable.Let [i, k, i] be an item of length l in IES99 (thus, l = k ?
i + 1).
If this item is coher-ent, it contains a dependency tree T such that yield(T) = i..k and head(t) = i.
By con-struction, the root of T is labelled i.
Let j be the rightmost daughter of i in T. Because Tis projective, we know that the yield of j must be of the form l..k, where i < l ?
j ?
k. Ifl < j, then l is the leftmost transitive dependent of j in T, and if k > j, then we know thatk is the rightmost transitive dependent of j in T.Let Tj be the subtree of T rooted at j, T1 be the tree obtained from removing Tj byT,8 T2 be the tree obtained by removing all the nodes to the right of j from Tj, and T3be the tree obtained by removing all the nodes to the left of j from Tj.
By construction,8 Removing a subtree from a dependency tree involves removing all the nodes in the subtree from itsvertex set, and all the outgoing links from nodes in the subtree from its edge set.557Computational Linguistics Volume 37, Number 3T1 belongs to a coherent item [i, l ?
1, i], T2 belongs to a coherent item [l, j, j], and T3belongs to a coherent item [ j, k, j].
Because these three items have a length strictly lessthan l, by the inductive hypothesis, they are derivable.
Thus the item [i, k, i] is alsoderivable, as it can be obtained from these derivable items by the following inferences:[i, l ?
1, i], [l, j, j][i, j, i] (by the L-LINK step)[i, j, i], [ j, k, j][i, k, i] (by the L-COMBINER step)This proves that all coherent items of length l which are of the form [i, k, i] are derivableunder the induction hypothesis.
The same can be shown for items of the form [i, k, k] bysymmetric reasoning.
Theorem 4The Yamada and Matsumoto (2003) parsing schema is correct.Proof 4Soundness is verified by building forests for the consequents of steps from those cor-responding to the antecedents.
To prove completeness we use strong induction on thelength of items, where the length of an item [i, j] is defined as j ?
i + 1.
The inductionstep involves considering any coherent item [i, k] of length l > 2 (l = 2 is the base casehere because items of length 2 are generated by the Initter step) and showing that itcan be inferred from derivable antecedents of length less than l, so it is derivable.
Ifl > 2, either i has at least one right dependent or k has at least one left dependent inthe item.
Suppose i has a right dependent; if T1 and T2 are the trees rooted at i andk in a forest in [i, k], we call j the rightmost daughter of i and consider the followingtrees: V = the subtree of T1 rooted at j, U1 = the tree obtained by removing V from T1, U2 = the tree obtained by removing all nodes to the right of j from V, U3 = the tree obtained by removing all nodes to the left of j from V.The forest {U1, U2} belongs to the coherent item [i, j], and {U3, T2} belongs to the co-herent item [ j, k].
From these two items, we can obtain [i, k] by using the L-LINK step.Symmetric reasoning can be applied if i has no right dependents but k has at least oneleft dependent, analogously to the case of the previous parser.
Theorem 5The Eisner (1996) parsing schema is correct.Proof 5By using the previous proofs and the relationships between schemata established ear-lier, we show that the parser of Eisner (1996) is correct: Soundness is straightforward,and completeness can be shown by using the properties of other algorithms.
Becausethe set of final items in the Eisner (1996) and Eisner and Satta (1999) schemata are thesame, and the former is a step refinement of the latter, the completeness of Eisner andSatta?s parser directly implies the completeness of Eisner?s parser.
Alternatively, we canuse Yamada and Matsumoto?s parser to prove the correctness of Eisner?s parser if we558Go?mez-Rodr?
?guez, Carroll, and Weir Dependency Parsing Schemataredefine the set of final items in the latter to be items of the form [0, n + 1, False, False],which are equally valid as final items since they always contain parse trees.
This ideacan be applied to transfer proofs of completeness across any refinement relation.
6.
Non-Projective SchemataThe parsing schemata presented so far define parsers that are restricted to projectivedependency structures, that is, structures in which the set of reflexive-transitive de-pendents of each node forms a contiguous substring of the input.
We now show thatthe dependency parsing schema formalism can also describe various non-projectiveparsers.6.1 Pseudo-ProjectivityPseudo-projective parsers generate non-projective analyses in polynomial time by usinga projective parsing strategy and postprocessing the results to establish non-projectivelinks.
This projective parsing strategy can be represented by dependency parsingschemata such as those seen in Section 3.
For example, the algorithm of Kahane, Nasr,and Rambow (1998) uses a strategy similar to Lombardo and Lesmo (1996), but with thefollowing initializer step instead of the INITTER and PREDICTOR:INITTER:[A(??
), i, i ?
1] A(?)
?
P ?
1 ?
i ?
nThe initialization step specified by Kahane, Nasr, and Rambow (1998) differs from this(directly consuming a nonterminal from the input) but this gives an incomplete algo-rithm.
The problem can be fixed either by using the step shown here instead (bottom?upEarley strategy) or by adding an additional step turning it into a bottom?up left-cornerparser.6.2 Attardi (2006)The non-projective parser of Attardi (2006) extends the algorithm of Yamada andMatsumoto (2003), adding additional shift and reduce actions to handle non-projectivedependency structures.
These extra actions allow the parser to link to nodes that areseveral positions deep in the stack, creating non-projective links.
In particular, Attardiuses six non-projective actions: two actions to link to nodes that are two positionsdeep, another two actions for nodes that are three positions deep, and a third pairof actions that generalizes the previous ones to n positions deep for any n. Thus, themaximum depth in the stack to which links can be created can be configured accordingto the actions allowed.
We use Attd for the variant of the algorithm that allows linksonly up to depth d, and Att?
for the original, unrestricted algorithm with unlimiteddepth actions.
A nondeterministic version of the algorithm Attd can be described asfollows.Item set: The item set is IAtt = {[h1, h2, .
.
.
, hm] | 0 ?
h1 < .
.
.
< hm ?
n + 1} where [h1,h2, .
.
.
, hm] is the set of dependency forests of the form {T1, T2, .
.
.
, Tm} such that:head(Ti) = hi for each i ?
[1..m]; and the projections of the nodes h1, h2, .
.
.
, hm are pair-wise disjoint, and their union is [h1..hm].559Computational Linguistics Volume 37, Number 3Deduction steps: The set of deduction steps for Attd is the following:INITTER:[i, i, i][i + 1, i + 1, i + 1][i, i + 1]COMBINE:[h1, h2, .
.
.
, hm][hm, hm+1, .
.
.
, hp][h1, h2, .
.
.
, hp]LINK:[h1, h2, .
.
.
, hm][h1, h2, .
.
.
, hi?1, hi+1, .
.
.
, hm](whi , hi) ?
(whj , hj), 1 < i < m, 1 ?
j ?
m, j = i, | j ?
i |?
dDeduction steps for Att?
are obtained by removing the constraint | j ?
i |?
d from thisset (this restriction corresponds to the maximum stack depth to which dependency linkscan be created).Final items: The set of final items is {[0, n + 1]}.
Although similar to the final item set forYamada and Matsumoto?s parser, they differ in that an Attardi item of the form [0, n + 1]may contain forests with non-projective dependency trees.Given the number of indices manipulated in the schema, a nondeterministic im-plementation of Attd has exponential complexity with respect to input length (thoughin the implementation of Attardi [2006], control structures determinize the algorithm).Soundness of the algorithm Att?
is shown as in the previous algorithms, and complete-ness can be shown by reasoning that every coherent final item [0, n + 1] can be obtainedby first performing n + 1 INITTER steps to obtain items [i, i + 1] for each 0 ?
i ?
n,then using n COMBINERs to join all of these items into [0, 1, .
.
.
, n, n + 1], and thenperforming the LINK steps corresponding to the links in a tree contained in [0, n + 1]to obtain this final item.
The algorithm Attd where d is finite is not correct with respectto the set of non-projective dependency structures, because it only parses a restrictedsubset of them (Attardi 2006).
Note that the algorithm Attd is a static filter of Attd+1for every natural number d, since the set of deduction steps of Attd is a subset of thatof Attd+1.6.3 The MHk ParserWe now define a novel variant of Attardi?s parser with polynomial complexity by lim-iting the number of trees in each forest contained in an item (rather than limiting stackdepth), producing a parsing schema MHk (standing for multi-headed with at most kheads per item).
Its item set is IMHk = {[h1, h2, .
.
.
, hm] | 0 ?
h1 < .
.
.
< hm ?
n + 1 ?
2 ?m ?
k} where [h1, h2, .
.
.
, hm] is defined as in IAtt, and the deduction steps are thefollowing:INITTER:[i, i, i][i + 1, i + 1, i + 1][i, i + 1]COMBINE:[h1, h2, .
.
.
, hm][hm, hm+1, .
.
.
, hp][h1, h2, .
.
.
, hp]p ?
kLINK:[h1, h2, .
.
.
, hm][h1, h2, .
.
.
, hi?1, hi+1, .
.
.
, hm](whi , hi) ?
(whj , hj), 1 < i < m, 1 ?
j ?
m, j = iAs with the Attd parser, MHk parses a restricted subset of non-projective dependencystructures, such that the set of structures parsed by MHk is always a subset of thoseparsed by MHk+1.
The MH?
parser, obtained by assuming that the number of treesper forest is unbounded, is equivalent to Att?, and therefore correct with respect to560Go?mez-Rodr?
?guez, Carroll, and Weir Dependency Parsing Schematathe set of non-projective dependency structures.
For finite values of k, MHd+2 is astatic filter of Attd, because its sets of items and deduction steps are subsets of thoseof Attd.
Therefore, the set of structures parsed by MHd+2 is also a subset of those parsedby Attd.The complexity of the MHk parser is O(nk).
For k = 3, MH3 is a step refinementof the parser by Yamada and Matsumoto (2003) that parses projective structures only,but by modifying the bound k we can define polynomial-time algorithms that parselarger sets of non-projective dependency structures.
The MHk parser has the propertyof being able to parse any possible dependency structure as long as we make k largeenough.6.4 MST Parser (McDonald et al 2005)McDonald et al (2005) describe a parser which finds a nonprojective analysis fora sentence in O(n2) time under a strong independence assumption called an edge-factored model: Each dependency decision is assumed to be independent of all theothers (McDonald and Satta 2007).
Despite the restrictiveness of this model, this max-imum spanning tree (MST) parser achieves state-of-the-art performance for projectiveand non-projective structures (Che et al 2008; Nivre and McDonald 2008; Surdeanuet al 2008).
The parser considers the weighted graph formed by all the possible de-pendencies between pairs of input words, and applies an MST algorithm to find adependency tree covering all the words in the sentence and maximizing the sum ofweights.The MST algorithm for directed graphs suggested by McDonald et al (2005) isnot fully constructive: It does not work by building structures and combining theminto large structures until it finds the solution.
Instead, the algorithm works by usinga greedy strategy to select a candidate set of edges for the spanning tree, potentiallycreating cycles and forming an illegal dependency tree.
A cycle elimination procedureis iteratively applied to this graph until a legal dependency tree is obtained.
It is stillpossible to express declarative aspects of the parser with a parsing schema, althoughthe importance of the control mechanism in eliminating cycles makes this schemaless informative than other cases we have considered, and we will not discuss it indetail here.
Go?mez-Rodr?
?guez (2009) gives a complete description and discussion of theschema for the MST parser.6.5 Covington?s (1990, 2001) Non-Projective ParserCovington?s non-projective parsing algorithm (Covington 1990, 2001) reads the inputfrom left to right, establishing dependency links between the current word and previouswords in the input.
The parser maintains two lists: one with all the words encounteredso far, and one with those that do not yet have a head assigned.
A new word can belinked as a dependent of any of the words in the first list, and as a head of any of thewords in the second list.
The following parsing schema expresses this algorithm.Item set: The item set is ICovNP = {[i, ?h1, h2, .
.
.
, hk?]
| 1 ?
h1 ?
.
.
.
?
hk ?
i ?
n} wherean item [i, ?h1, h2, .
.
.
, hk?]
represents the set of forests of the form F = {T1, T2, .
.
.
, Tk}such that head(Tj) = hj for every Tj in F; the projections of the nodes h1, h2, .
.
.
, hk arepairwise disjoint, and their union is [1..i].561Computational Linguistics Volume 37, Number 3Deduction steps: The set of deduction steps is as follows:INITTER:[1, ?1?
]R-LINK:[i, ?h1, .
.
.
, hj?1, hj, hj+1, .
.
.
, hk?
][i, ?h1, .
.
.
, hj?1, hj+1, .
.
.
, hk?
](whj , hj) ?
(wi, i)( j < i)ADVANCE:[i, ?h1, .
.
.
, hk?
][i + 1, ?h1, .
.
.
, hk, i + 1?
]L-LINK:[i, ?h1, .
.
.
, hk, i?
][i, ?h1, .
.
.
, hk?
](wi, i) ?
(wj, j)( j < i)Final items: The set of final items is {[n, ?h?]
| 1 ?
h ?
n}, the set of items containing aforest with a single dependency tree T headed at an arbitrary position h of the string,and whose yield spans the whole input string.
The time complexity of the algorithm isexponential in the input length n.Note that this parsing schema is not correct, because Covington?s algorithm doesnot prevent the generation of cycles in the dependency graphs it produces.
QuotingCovington (2001, page 99),Because the parser operates one word at a time, unity can only be checked at the endof the whole process: did it produce a tree with a single root that comprises all of thewords?Therefore, a postprocessing mechanism is needed to determine which of the gener-ated structures are, in fact, valid trees.
In the parsing schema, this is reflected by the factthat the schema is complete but not sound.
Nivre (2007) uses a variant of this algorithmin which cycle detection is used to avoid generating incorrect structures.Other non-projective parsers not covered here can also be represented under theparsing schema framework.
For example, Kuhlmann (2010) presents a deduction sys-tem for a non-projective parser which uses a grammar formalism called regular de-pendency grammars.
This deduction system can easily be converted into a parsingschema by associating adequate semantics with items.
However, we do not show thishere for space reasons, because we would first have to explain the formalism of regulardependency grammars.7.
Mildly Non-Projective Dependency ParsingFor reasons of computational efficiency, many practical implementations of dependencyparsing are restricted to projective structures.
However, some natural language sen-tences appear to have non-projective syntactic structure, something that arises in manylanguages (Havelka 2007), and is particularly common in free word order languagessuch as Czech.
Parsing without the projectivity constraint is computationally complex:Although it is possible to parse non-projective structures in quadratic time with respectto input length under a model in which each dependency decision is independent ofall the others (as in the parser of McDonald et al [2005], discussed in Section 6.4), theproblem is intractable in the absence of this assumption (McDonald and Satta 2007).Nivre and Nilsson (2005) observe that most non-projective dependency structuresappearing in practice contain only small proportions of non-projective arcs.
This hasled to the study of sub-classes of the class of all non-projective dependency struc-tures (Kuhlmann and Nivre 2006; Havelka 2007).
Kuhlmann (2010) investigates sev-eral such classes, based on well-nestedness and gap degree constraints (Bodirsky,Kuhlmann, and Mo?hl 2005), relating them to lexicalized constituency grammar for-malisms.
Specifically, Kuhlmann shows that linear context-free rewriting systems562Go?mez-Rodr?
?guez, Carroll, and Weir Dependency Parsing Schemata(LCFRS) with fan-out k (Vijay-Shanker, Weir, and Joshi 1987; Satta 1992) induce theset of dependency structures with gap degree at most k ?
1; coupled CFG in whichthe maximal rank of a nonterminal is k (Hotz and Pitsch 1996) induces the set of well-nested dependency structures with gap degree at most k ?
1; and finally, LTAG (Joshiand Schabes 1997) induces the set of well-nested dependency structures with gap degreeat most 1.These results establish that there are polynomial-time dependency parsing algo-rithms for well-nested structures with bounded gap degree, because such parsers existfor their corresponding lexicalized constituency-based formalisms.
Developing efficientdependency parsing strategies for these sets of structures has considerable practicalinterest, in particular, making it possible to parse directly with dependencies in adata-driven manner rather than indirectly by constructing intermediate constituencygrammars and extracting dependencies from constituency parses.
In this section, wemake four contributions to this enterprise.Firstly, we define a parser for well-nested structures of gap degree 1, and proveits correctness.
The parser runs in time O(n7), the same complexity as the best existingalgorithms for LTAG (Eisner and Satta 2000), and can be optimized to O(n6) in the non-lexicalized case.
Secondly, we generalize our algorithm to any well-nested dependencystructure with gap degree at most k, resulting in an algorithm with time complexityO(n5+2k).
Thirdly, we generalize the previous parsers in order to include ill-nestedstructures with gap degree at most k satisfying certain constraints, giving a parser thatruns in time O(n4+3k).
Note that parsing unrestricted ill-nested structures, even whenthe gap degree is bounded, is NP-complete: These structures are equivalent to LCFRSfor which the recognition problem is NP-complete (Satta 1992).
Finally, we characterizethe set of structures covered by this parser, which we call mildly ill-nested structures,and show that it includes all the trees present in a number of dependency treebanks.We now define the concepts of gap degree and well-nestedness (Kuhlmann andNivre 2006).
Let T be a dependency tree for the string w1 .
.
.wn:Definition 5The gap degree of a node k in T is the minimum g ?
(N ?
{0}) such that k (the projec-tion of the node k) can be written as the union of g + 1 intervals, that is, the number ofdiscontinuities in k.
The gap degree of the dependency tree T is the maximum of thegap degrees of its nodes.Note that T has gap degree 0 if and only if T is projective.Definition 6The subtree induced by the node u in a dependency tree T is the tree Tu = (u, Eu)where Eu = {i ?
j ?
E | j ?
u}.
The subtrees induced by nodes p and q are interleavedif p ?
q = ?
and there are nodes i, j ?
p and k, l ?
q such that i < k < j < l. Adependency tree T is well-nested if it does not contain two interleaved subtrees, anda tree that is not well-nested is said to be ill-nested.Projective trees are always well-nested, but well-nested trees are not always projective.7.1 The WG1 ParserWe now define WG1, a polynomial-time parser for well-nested dependency structuresof gap degree at most 1.
In this and subsequent schemata, each dependency forest in563Computational Linguistics Volume 37, Number 3an item is a singleton set containing a dependency tree, so we will not make explicitmention of these forests, referring directly to their trees instead.
Also note that in theparsers in this section we use D-rules to express parsing decisions, so dependency treesare assumed to be taken from the set of trees licensed by a given set of D-rules.
Theschema for the WG1 parser is defined as follows:Item set: The item set is IWG1 = I1 ?
I2, withI1 = {[i, j, h, , ] | i, j, h ?
N, 1 ?
h ?
n, 1 ?
i ?
j ?
n, h = j, h = i ?
1}where each item of the form [i, j, h, , ] represents the set of all well-nested dependencytrees with gap degree at most 1, rooted at h, and such that h = {h} ?
[i..j], andI2 = {[i, j, h, l, r] | i, j, h, l, r ?
N, 1 ?
h ?
n, 1 ?
i < l ?
r < j ?
n, h = j, h = i ?
1, h = l ?
1, h = r}where each item of the form [i, j, h, l, r] represents the set of all well-nested dependencytrees rooted at h such that h = {h} ?
([i..j] \ [l..r]), and all the nodes (except possiblyh) have gap degree at most 1.
We call items of this form gapped items, and the interval[l..r] the gap of the item.
Figure 7 shows two WG1 items, one from I1 and the other fromI2, together with one of the trees contained in each of them.
Note that the constraintsh = j, h = i + 1, h = l ?
1, h = r are added to items to avoid redundancy in the item set.Because the result of the expression {h} ?
([i..j] \ [l..r]) for a given head can be the samefor different sets of values of i, j, l, r, we restrict these values so that we cannot get twodifferent items representing the same dependency structures.
Items ?
violating theseconstraints always have an alternative representation that does not violate them, whichwe can express with a normalizing function nm(?)
as follows:nm([i, j, j, l, r]) = [i, j ?
1, j, l, r] (if r ?
j ?
1 or r = ), or [i, l ?
1, j, , ] (if r = j ?
1).nm([i, j, l ?
1, l, r]) = [i, j, l ?
1, l ?
1, r](if l > i + 1), or [r + 1, j, l ?
1, , ] (if l = i + 1).nm([i, j, i ?
1, l, r]) = [i ?
1, j, i ?
1, l, r].nm([i, j, r, l, r]) = [i, j, r, l, r ?
1] (if l < r), or [i, j, r, , ] (if l = r).nm([i, j, h, l, r]) = [i, j, h, l, r] for all other items.When defining the deduction steps for this and other parsers, we assume that theyalways produce normalized items.
For clarity, we do not explicitly write this in thededuction steps, writing ?
instead of nm(?)
as antecedents and consequents of steps.Figure 7Representation of the WG1 items [i, j, h, , ] and [i, j, h, l, r], each together with one of thedependency structures contained in it.564Go?mez-Rodr?
?guez, Carroll, and Weir Dependency Parsing SchemataInitial items: The set of initial items (hypotheses) is defined as the setH = {[h, h, h, , ] | h ?
[1..n]}where each item [h, h, h, , ] represents the set containing the trivial dependency treeconsisting of a single node h and no links.
This is the same set of hypotheses used bythe parsers defined in previous sections, but we use the notation [h, h, h, , ] rather than[h, h, h] here for convenience when defining deduction steps.
The same set of hypothesesis used for all the mildly non-projective parsers, so we do not make it explicit forsubsequent schemata.
Note that initial items are separate from the item set IWG1 andnot subject to its constraints, so they do not require normalization.Final items: The set of final items for strings of length n in WG1 is defined as the setF = {[1, n, h, , ] | h ?
[1..n]},which is the set of the items in IWG1 containing dependency trees for the complete inputstring (from position 1 to n), with their head at any position h.Deduction steps: The deduction steps of the WG1 parser are the following:LINK UNGAPPED:[h1, h1, h1, , ][i2, j2, h2, , ][i2, j2, h1, , ](wh2 , h2) ?
(wh1 , h1)such that h2 ?
[i2..j2] ?
h1 /?
[i2..j2]LINK GAPPED:[h1, h1, h1, , ][i2, j2, h2, l2, r2][i2, j2, h1, l2, r2](wh2 , h2) ?
(wh1 , h1)such that h2 ?
[i2..j2] \ [l2..r2] ?
h1 /?
[i2..j2] \ [l2..r2]COMBINE UNGAPPED:[i, j, h, , ][ j + 1, k, h, , ][i, k, h, , ] COMBINE OPENING GAP:[i, j, h, , ][k, l, h, , ][i, l, h, j + 1, k ?
1] j < k ?
1COMBINE KEEPING GAP LEFT:[i, j, h, l, r][ j + 1, k, h, , ][i, k, h, l, r]COMBINE KEEPING GAP RIGHT:[i, j, h, , ][ j + 1, k, h, l, r][i, k, h, l, r]COMBINE CLOSING GAP:[i, j, h, l, r][l, r, h, , ][i, j, h, , ] COMBINE SHRINKING GAP CENTRE:[i, j, h, l, r][l, r, h, l2, r2][i, j, h, l2, r2]COMBINE SHRINKING GAP LEFT:[i, j, h, l, r][l, k, h, , ][i, j, h, k + 1, r]COMBINE SHRINKING GAP RIGHT:[i, j, h, l, r][k, r, h, , ][i, j, h, l, k ?
1]The WG1 parser proceeds bottom?up, building dependency subtrees and combiningthem into larger subtrees, until a complete dependency tree for the input sentence is565Computational Linguistics Volume 37, Number 3found.
The parser logic specifies how items corresponding to the subtree induced by aparticular node are inferred, given the items for the subtrees induced by the direct de-pendents of that node.
Suppose that, in a complete dependency analysis for a sentencew1 .
.
.wn, the node h has d1 .
.
.
dp as direct dependents (i.e., we have dependency linksd1 ?
h, .
.
.
, dp ?
h).
The item corresponding to the subtree induced by h is obtainedfrom the ones corresponding to the subtrees induced by d1 .
.
.
dp as follows.First, apply the LINK UNGAPPED or LINK GAPPED step to each of the items cor-responding to the subtrees induced by the direct dependents, and to the hypothesis[h, h, h, , ].
We infer p items representing the result of linking each of the dependentsubtrees to the new head h. Second, apply the various COMBINE steps to join all itemsobtained in the previous step into a single item.
The COMBINE steps perform a unionoperation between subtrees.
Therefore, the result is a dependency tree containing all thedependent subtrees, and with all of them linked to h?this is the subtree induced by h.This process is applied repeatedly to build larger subtrees, until, if the parsing process issuccessful, a final item is found containing a dependency tree for the complete sentence.A graphical representation of this process is shown in Figure 8.Figure 8Example WG1 parse, following the notation of Figure 7.
LINK steps link an item to a newhead, while COMBINE steps are used to join a pair of items sharing the same head.
DifferentCOMBINE steps correspond to different relative positions of items that can be joined andtheir gaps.566Go?mez-Rodr?
?guez, Carroll, and Weir Dependency Parsing SchemataThe WG1 schema provides an abstract mechanism for finding all the dependencystructures in the class of well-nested structures of gap degree at most 1, for an inputstring under a set of D-rules.
Concrete implementations of the schema may use proba-bilistic models or machine learning techniques to make the linking decisions associatedwith the D-rules, as explained in Section 3.1.
The definition of such statistical modelsfor guiding the execution of schemata falls outside the scope of this article.7.2 Proof of Correctness for WG1We define a set of coherent items for the schema, in such a way that final items inthis set satisfy the general definition of coherent final items; and then prove the strongerclaims that all derivable items are coherent and all coherent items are derivable.
The fullcorrectness proof has previously been published (Go?mez-Rodr?
?guez, Weir, and Carroll2008; Go?mez-Rodr?
?guez 2009), so for reasons of space we only sketch the proof here.To define the set of coherent items for WG1, we provide a definition of the trees thatthese items must contain.
Let T be a well-nested dependency tree headed at a node h,with all its edges licensed by our set of D-rules.
We call such a tree a properly formedtree for the algorithm WG1 if it satisfies the following conditions.1.
h is either of the form {h} ?
[i..j] or {h} ?
([i..j] \ [l..r]).2.
All the nodes in T have gap degree at most 1 except for h, which can havegap degree up to 2.An item [i, j, h, l, r] ?
IWG1 is coherent if it contains a properly formed tree headed ath, such that h = {h} ?
([i..j] \ [l..r]).
Similarily for items of the form [i, j, h, , ], whereh = {h} ?
[i..j].
A coherent final item [1, n, h, , ] for an input string contains at leastone well-nested parse of gap degree ?
1 for that string.
With these sets of coherent andcoherent final items, we prove the soundness and completeness of WG1.Theorem 6WG1 is sound.Proof 6Proving the soundness of the WG1 parser involves showing that all derivable final itemsare coherent.
We do this by proving the stronger claim that all derivable items arecoherent.
As in previous proofs, this is done by showing that each deduction step inthe parser infers a coherent consequent item when applied to coherent antecedents.
Weproceed step by step, showing that if each of the antecedents of a given step contains atleast one properly formed tree, we obtain a properly formed tree that is an element of thecorresponding consequent.
In the case of LINK steps, this properly formed consequenttree is obtained by creating a dependency link between the heads of the properly formedantecedent trees; for COMBINE steps, it is obtained from the union of the antecedenttrees.
To prove that these consequent trees are properly formed, we show that they arewell-nested, have a projection corresponding to the indices in the consequent item, andsatisfy the gap degree constraint 2 required for the trees to be properly formed.
Eachof these properties is proven individually, based on the properties of the antecedenttrees.
567Computational Linguistics Volume 37, Number 3Theorem 7WG1 is complete.Proof 7Proving completeness of the WG1 parser involves proving that all coherent final itemsin WG1 are derivable.
We show this by proving the following, stronger claim.Lemma 1If T is a dependency tree headed at a node h, which is a properly formed tree for WG1,then:1.
If h = {h} ?
[i..j], then the item [i, j, h, , ] containing T is a derivableitem in the WG1 parser.2.
If h = {h} ?
([i..j] \ [l..r]), then the item [i, j, h, l, r] containing T is aderivable item in the WG1 parser.This implies that all coherent final items are derivable, and therefore that WG1 iscomplete.
The lemma is proven by strong induction on the number of elements in h,which we denote #(h).The base case of the induction is trivial, because the case #(h) = 1 corresponds toa tree contained in an initial item, which is derivable by definition.
For the inductionstep, we take T to be a properly formed dependency tree rooted at a node h, such that#(h) = N for some N > 1.
Lemma 1 holds for T if it holds for every properly formeddependency tree T?
rooted at h?
such that #(h?) < N. Let p be the number of directchildren of h in the tree T. We have p ?
1, because by hypothesis #(h) > 1.
With this,the induction step proof is divided into two cases, according to whether p = 1 or p > 1.When p = 1, the item that Lemma 1 associates with the subtree of T induced by thesingle direct dependent of h is known to be derivable by the induction hypothesis.
Itcan be shown case by case that the item corresponding to h by Lemma 1 can be inferredusing LINK steps, thus completing the case for p = 1.
For p > 1, we use the concept oforder annotations (Kuhlmann and Mo?hl 2007; Kuhlmann 2010).
Order annotations arestrings that encode the precedence relation between the nodes of a dependency tree.
Theorder annotation for a given node encodes the shape (with respect to this precedencerelation) of the projection of each of the children of that node, that is, the number ofintervals in each projection, the number of gaps, and the way in which intervals andgaps are interleaved.
The concepts of projectivity, gap degree, and well-nestedness areassociated with particular constraints on order annotations.The completeness proof for p > 1 is divided into cases according to the orderannotation of the head h. The fact that the tree T is properly formed imposes constraintson the form of this order annotation.
With this information, we divide the possibleorder annotations into a number of cases.
Using the induction hypotheses and somerelevant properties of order annotations we find that, for each of this cases, we can finda sequence of COMBINE steps to infer the item corresponding to T from smaller coherentitems.
7.3 Computational Complexity of WG1The time complexity of WG1 is O(n7), as the step COMBINE SHRINKING GAP CENTREworks with seven free string positions.
This complexity with respect to the length of the568Go?mez-Rodr?
?guez, Carroll, and Weir Dependency Parsing Schematainput is as expected for this set of structures, because Kuhlmann (2010) shows that theyare equivalent to LTAG, and the best existing parsers for this formalism also performin O(n7) (Eisner and Satta 2000).9 Note that the COMBINE step that is the bottleneckonly uses seven indices, not any additional entities such as D-rules.
Hence, the O(n7)complexity does not involve additional factors relating to grammar size.Given unlexicalized D-rules specifying the possibility of dependencies betweenpairs of categories rather than pairs of words, a variant of this parser can be constructedwith time complexity O(n6), as with parsers for unlexicalized TAG.
We expand theitem set with unlexicalized items of the form [i, j, C, l, r], where C is a category, distinctfrom the existing items [i, j, h, l, r].
Steps in the parser are duplicated, to work both withlexicalized and unlexicalized items, except for the LINK steps, which always work witha lexicalized item and an unlexicalized hypothesis to produce an unlexicalized item,and the COMBINE SHRINKING GAP steps, which work only with unlexicalized items.Steps are added to obtain lexicalized items from their unlexicalized equivalents bybinding the head to particular string positions.
Finally, we need certain variants of theCOMBINE SHRINKING GAP steps that take two unlexicalized antecedents and producea lexicalized consequent; an example is the following:COMBINE SHRINKING GAP CENTRE L:[i, j, C, l, r] [l + 1, r, C, l2, r2][i, j, l, l2, r2]cat(wl)=CAlthough this version of the algorithm reduces time complexity to O(n6), it also addsa factor related to the number of categories, as well as constant factors due to havingmore kinds of items and steps than the original WG1 algorithm.7.4 The WGk ParserThe WG1 parsing schema can be generalized to obtain a parser for all well-nesteddependency structures with gap degree bounded by a constant k (k ?
1), which we callthe WGk parser.
We extend the item set so that it contains items with up to k gaps, andmodify the deduction steps to work with these multi-gapped items.Item set: The item set for the WGk parsing schema isIWGk = {[i, j, h, ?
(l1, r1), .
.
.
, (lg, rg)?
]}where i, j, h ?
(N ?
{0}), 0 ?
g ?
k, 1 ?
h ?
n, 1 ?
i ?
j ?
n, h = j, h = i ?
1; and for eachp ?
{1, 2, .
.
.
, g}: lp, rp ?
N, i < lp ?
rp < j, rp < lp+1 ?
1, h = lp ?
1, h = rp.
An item [i, j, h,?
(l1, r1), .
.
.
, (lg, rg)?]
represents the set of all well-nested dependency trees rooted at hsuch that h = {h} ?
([i..j] \?gp=1[lp..rp]), where each interval [lp..rp] is called a gap.The constraints h = j, h = i + 1, h = lp ?
1, h = rp are added to avoid redundancy, andnormalization is defined as in WG1.Final items: The set of final items is defined as the set F = {[1, n, h, ??]
| h ?
[1..n]}.
Notethat this set is the same as in WG1, as these are the items that we denoted [1, n, h, , ] inthat parser.9 Although standard TAG parsing algorithms run in time O(n6) with respect to the input length, they alsohave a complexity factor related to grammar size.
Eisner and Satta (2000) show that, in the case oflexicalized TAG, this factor is a function of the input length n; hence the additional complexity.569Computational Linguistics Volume 37, Number 3Deduction steps: The parser has the following deduction steps:LINK:[h1, h1, h1, ??]
[i2, j2, h2, ?
(l1, r1), .
.
.
, (lg, rg)?
][i2, j2, h1, ?
(l1, r1), .
.
.
, (lg, rg)?
](wh2 , h2) ?
(wh1 , h1)such that h2 ?
[i2..j2] \g?p=1[lp..rp] ?
h1 /?
[i2..j2] \g?p=1[lp..rp]COMBINE OPENING GAP:[i, lq ?
1, h, ?
(l1, r1), .
.
.
, (lq?1, rq?1)?
][rq + 1, m, h, ?
(lq+1, rq+1), .
.
.
, (lg, rg)?
][i, m, h, ?
(l1, r1), .
.
.
, (lg, rg)?
]g ?
k ?
lq ?
rqCOMBINE KEEPING GAPS:[i, j, h, ?
(l1, r1), .
.
.
, (lq, rq)?
][ j + 1, m, h, ?
(lq+1, rq+1), .
.
.
, (lg, rg)?
][i, m, h, ?
(l1, r1), .
.
.
, (lg, rg)?
]g ?
kCOMBINE SHRINKING GAP LEFT:[i, j, h, ?
(l1, r1), .
.
.
, (lq, rq), (l?, rs), (ls+1, rs+1), .
.
.
, (lg, rg)?
][l?, ls ?
1, h, ?
(lq+1, rq+1), .
.
.
, (ls?1, rs?1)?
][i, j, h, ?
(l1, r1), .
.
.
, (lg, rg)?
]g ?
kCOMBINE SHRINKING GAP RIGHT:[i, j, h, ?
(l1, r1), .
.
.
, (lq?1, rq?1), (lq, r?
), (ls, rs), .
.
.
, (lg, rg)?
][rq + 1, r?, h, ?
(lq+1, rq+1), .
.
.
, (ls?1, rs?1)?
][i, j, h, ?
(l1, r1), .
.
.
, (lg, rg)?
]g ?
kCOMBINE SHRINKING GAP CENTRE:[i, j, h, ?
(l1, r1), .
.
.
, (lq, rq), (l?, r?
), (ls, rs), .
.
.
, (lg, rg)?
][l?, r?, h, ?
(lq+1, rq+1), .
.
.
, (ls?1, rs?1)?
][i, j, h, ?
(l1, r1), .
.
.
, (lg, rg)?
]g ?
kAs expected, the WG1 parser corresponds to WGk for k = 1.
WGk works in the sameway as WG1, except that COMBINE steps can create items with more than one gap.
In allthe parsers described in this section, COMBINE steps may be applied in different ordersto produce the same result, causing spurious ambiguity.
In WG1 and WGk, this can beavoided when implementing the schemata by adding flags to items so as to impose aparticular order on the execution of these steps.7.5 Proof of Correctness for WGkThe proof of correctness for WGk is analogous to that of WG1, but generalizing thedefinition of properly formed trees to a higher gap degree.
A properly formed tree inWGk is a dependency tree T, headed at node h, such that the following hold.1.
h is of the form {h} ?
([i..j] \?gp=1[lp..rp]), with 0 ?
g ?
k.2.
All the nodes in T have gap degree at most k except for h, which can havegap degree up to k + 1.With this, we define coherent items and coherent final items as for WG1.
Soundnessis shown as for WG1, changing the constraints on nodes so that any node can havegap degree up to k and the head of a properly formed tree can have gap degree k + 1.570Go?mez-Rodr?
?guez, Carroll, and Weir Dependency Parsing SchemataCompleteness is shown by induction on #(h).
The base case is the same as for WG1,and for the induction step, we consider the direct children d1 .
.
.
dp of h. The case wherep = 1 is proven by using LINK steps just as in WG1.
In the case for p ?
1, we also baseour proof on the order annotation for h, but we have to take into account that the set ofpossible annotations is larger when we allow the gap degree to be greater than 1, so wemust consider more cases in this part of the proof.7.6 Computational Complexity of WGkThe WGk parser runs in time O(n5+2k).
As in the case of WG1, the step with most freevariables is COMBINE SHRINKING GAP CENTRE with 5 + 2k free indices.
Again, thiscomplexity result is in line with what could be expected from previous research inconstituency parsing: Kuhlmann (2010) shows that the set of well-nested dependencystructures with gap degree at most k is closely related to coupled CFG in which themaximal rank of a nonterminal is k + 1.
The constituency parser defined by Hotz andPitsch (1996) for these grammars also adds an n2 factor for each unit increment of k. Notethat a small value of k appears to be sufficient to account for the vast majority of thenon-projective sentences found in natural language treebanks.
For instance, the PragueDependency Treebank (Hajic?
et al 2006) contains no structures with gap degree greaterthan 4.
Thus, a WG4 parser would be able to analyze all the well-nested structures inthis treebank, which represent 99.89% of the total (WG1 would be able to parse 99.49%).Increasing k beyond 4 would not produce further improvements in coverage.7.7 Parsing Ill-Nested Structures: MG1 and MGkThe WGk parser analyzes dependency structures with bounded gap degree as long asthey are well-nested.
Although this covers the vast majority of the structures that occurin natural language treebanks (Kuhlmann and Nivre 2006), a significant number of sen-tences contain ill-nested structures.
Maier and Lichte (2011) provide examples of somelinguistic phenomena that cause ill-nestedness.
Unfortunately, the general problem ofparsing ill-nested structures is NP-complete, even when the gap degree is bounded.This set of structures is closely related to LCFRS with bounded fan-out and unboundedproduction length, and parsing in this formalism is known to be NP-complete (Satta1992).
The reason for this complexity is the problem of unrestricted crossing configura-tions, appearing when dependency subtrees are allowed to interleave in every possi-ble way.Ill-nested structures can be parsed in polynomial time with bounds on the gapdegree and the number of dependents allowed per node: Kuhlmann (2010) presentsa parser based on this idea, using a kind of grammar that resembles LCFRS, calledregular dependency grammar.
This parser is exponential in the gap degree, as well as inthe maximum number of dependents allowed per node: Its complexity is O(nk(m+1)),where k is the maximum gap degree and m is the maximum number of dependents pernode.
In contrast, the parsers presented here are data-driven and thus do not need anexplicit grammar.
Furthermore, they are able to parse dependency structures with anynumber of dependents per node, and their computational complexity is independent ofthis parameter m.In line with the observation that most non-projective structures appearing in prac-tice are only ?slightly?
non-projective (Nivre and Nilsson 2005), we characterize asense in which the structures appearing in treebanks are only ?slightly?
ill-nested.
We571Computational Linguistics Volume 37, Number 3generalize the algorithms WG1 and WGk to parse a proper superset of the set of well-nested structures in polynomial time, and give a characterization of this new set ofstructures, which includes all the structures in several dependency treebanks.The WGk parser for well-nested structures presented previously is based on abottom?up process, where LINK steps are used to link completed subtrees to a head,and COMBINE steps are used to join subtrees governed by a common head to obtain alarger structure.
As WGk is a parser for well-nested structures of gap degree up to k,its COMBINER steps correspond to all the ways in which we can join two sets of siblingsubtrees meeting these constraints, and having a common head, into another.
Therefore,this parser does not use COMBINER steps that produce interleaved subtrees, becausethese would generate items corresponding to ill-nested structures.We obtain a polynomial parser for a larger set of structures of gap degree at mostk, including some ill-nested ones, by having COMBINER steps representing all ways inwhich two sets of sibling subtrees of gap degree at most k with a common head can bejoined into another, including those producing interleaved subtrees.
This does not meanthat we build every possible ill-nested structure.
Some structures with complex crossedconfigurations have gap degree k, but cannot be built by combining two structures ofthat gap degree.
More specifically, this algorithm will parse a dependency structure(well-nested or not) if there exists a binarization of that structure that has gap degree atmost k. The parser works by implicitly finding such a binarization, because COMBINEsteps are always applied to two items and no intermediate item generated by them canexceed gap degree k (not counting the position of the head in the projection).Definition 7Let w1 .
.
.wn be a string, and T a dependency tree headed at a node h. A binarization ofT is a tree B in which each node has at most two children, such that:1.
Each node in B can be unlabelled, or labelled with a word position i.Several nodes may have the same label (in contrast to the dependencygraphs, where a word occurrence cannot appear twice in the graph).2.
A node labelled i is a descendant of j in B if and only if i ? j in T.The projection of a node in a binarization is the set of reflexive-transitive children ofthat node.
With this, condition (2) of Definition 7 can be rewritten i ?
 jB ?
i ?
 jT,and the gap degree of a binarization can be defined as with a dependency structure,allowing us to define mildly ill-nested structures as follows.Definition 8A dependency structure is mildly ill-nested for gap degree k if it has at least onebinarization of gap degree ?
k. Otherwise, it is strongly ill-nested for gap degree k.The set of mildly ill-nested structures for gap degree k includes all well-nested structureswith gap degree up to k. We define MG1, a parser for mildly ill-nested structures for gapdegree 1, as follows.Item set and final item set: The item set and the final item set are the same as for WG1,except that items can contain any mildly ill-nested structures for gap degree 1, insteadof being restricted to well-nested structures.572Go?mez-Rodr?
?guez, Carroll, and Weir Dependency Parsing SchemataDeduction steps: Deduction steps include those in WG1, plus the following.COMBINE INTERLEAVING:[i, j, h, l, r][l, k, h, r + 1, j][i, k, h, , ]COMBINE INTERLEAVINGGAP C:[i, j, h, l, r][l, k, h, m, j][i, k, h, m, r]m < r + 1COMBINE INTERLEAVINGGAP L:[i, j, h, l, r][l, k, h, r + 1, u][i, k, h, j + 1, u]u > j COMBINE INTERLEAVINGGAP R:[i, j, h, l, r][k, m, h, r + 1, j][i, m, h, l, k ?
1] k > lThese extra COMBINE steps allow the parser to combine interleaved subtrees withsimple crossing configurations.
The MG1 parser still runs in O(n7), as these new stepsdo not use more than seven string positions.
To generalize this algorithm to mildlyill-nested structures for gap degree k, we add a COMBINE step for every possibleway of joining two structures of gap degree at most k into another.
This is done in asystematic way by considering a set of strings over an alphabet of three symbols: aand b to represent intervals of words in the projection of each of the structures, andg to represent intervals that are not in the projection of either of the structures andwill correspond to gaps in the joined structure.
The legal combinations of structuresfor gap degree k will correspond to strings where symbols a and b each appear at mostk + 1 times, g appears at most k times and is not the first or last symbol, and there isno more than one consecutive appearance of any symbol.
Given a string of this form,of length n, with a?s located at positions a1 .
.
.
ap(1 ?
a1 < .
.
.
< ap ?
n), b?s at positionsb1 .
.
.
bq(1 ?
b1 < .
.
.
< bq ?
n), and g?s at positions g1 .
.
.
gr(2 ?
g1 < .
.
.
< gr ?
n ?
1),such that p + q + r = n, the corresponding COMBINE step is as follows.
[ia1 , iap+1 ?
1, h, ?
(ia1+1, ia2 ?
1), .
.
.
, (iap?1+1, iap ?
1)?
][ib1 , ibq+1 ?
1, h, ?
(ib1+1, ib2 ?
1), .
.
.
, (ibq?1+1, ibq ?
1)?
][imin(a1,b1 ), imax(ap+1,bq+1) ?
1, h, ?
(ig1 , ig1+1 ?
1), .
.
.
, (igr , igr+1 ?
1)?
]For example, the COMBINE INTERLEAVING GAP C step in MG1 is obtained from thestring abgab.
Therefore, we define the parsing schema for MGk, a parser for mildly ill-nested structures for gap degree k, as the schema where the item set is the same as thatof WGk, except that items now contain mildly ill-nested structures for gap degree k; andthe set of deduction steps consists of the LINK step in WGk, plus a set of COMBINE stepsobtained as explained herein.7.8 Computational Complexity of MGkBecause the string used to generate a COMBINER step can have length at most 3k + 2,and the resulting step contains an index for each symbol of the string plus two extraindices, the MGk parser has complexity O(n3k+4) with respect to the length of theinput.
Note that this expression denotes the complexity with respect to n of the MGkparser obtained for a given k: Taking k to be a variable would add an additional O(33k)complexity factor, because the number of different COMBINER steps that can be appliedto a given item grows exponentially with k.573Computational Linguistics Volume 37, Number 37.9 Proof of Correctness for MGkAs for previous parsers, we only show here a sketch of the proof that MGk is correct.The detailed proof has been published previously (Go?mez-Rodr?
?guez, Weir, and Carroll2008; Go?mez-Rodr?
?guez 2009).Theorem 8MGk is correct.Proof 8As with WGk, we define the sets of properly formed trees and coherent items for thisalgorithm.
Let T be a dependency tree headed at a node h. We call such a tree a properlyformed tree for the algorithm MGk if it satisfies the following.1.
h is of the form {h} ?
([i..j] \?gp=1[lp..rp]), with 0 ?
g ?
k.2.
There is a binarization of T such that all the nodes in it have gap degree atmost k except for its root node, which can have gap degree up to k + 1.The sets of coherent and coherent final items are defined as in previous proofs.
Sound-ness is shown as for previous algorithms, where we show that consequent trees areproperly formed by building a binarization for them from the binarizations obtainedfrom antecedent items.
This part of the proof involves imposing additional constraintson binarizations, which are useful to provide a suitable way of combining binarizationsobtained from antecedents of steps.
Completeness is proven by showing the following,stronger claim.Proposition 1Let T be a dependency tree headed at node h, and properly formed for MGk.
Then, ifh = {h} ?
([i..j] \?gp=1[lp..rp]), for g ?
k, the item [i, j, h, ?
(l1, r1), .
.
.
, (lg, rg)?]
containingT is derivable under this parser.To prove this, we say that a binarization of a properly formed tree is a well-formedbinarization for MGk if each of its nodes has gap degree ?
k except possibly the head,which can have gap degree k + 1.
We then reduce the proof to establishing the followinglemma.Lemma 2Let B be a well-formed binarization of a dependency tree T, headed at a node h andproperly formed for MGk.
If the projection of h in T is hT = hB = {h} ?
([i..j] \?gp=1[lp..rp]), for g ?
k, the item [i, j, h, ?
(l1, r1), .
.
.
, (lg, rg)?]
containing T is derivableunder this parser.The lemma is shown by strong induction on the number of nodes of B (denoted #B).The base case where #B = 1 is trivial.
For the induction step, we consider different casesdepending on the number and type of children of the head node h of B.
When h has asingle child, we obtain the item corresponding to T from a smaller item, shown to bederivable by the induction hypothesis, by using a LINK step.
Where h has two childrenin B, the relevant item is obtained by using a COMBINER step.
574Go?mez-Rodr?
?guez, Carroll, and Weir Dependency Parsing SchemataFigure 9A structure which is strongly ill-nested for gap degree 1, but only mildly ill-nested for gapdegree ?
2.7.10 Mildly Ill-Nested Dependency StructuresThe MGk algorithm parses mildly ill-nested structures for gap degree k in polynomialtime.
The mildly ill-nested structures for gap degree k are those with a binarization ofgap degree ?
k. Because a binarization of a dependency structure cannot have lower gapdegree than the original structure, the mildly ill-nested structures for gap degree k allhave gap degree ?
k. Given the relation between MGk and WGk, we know they containall well-nested structures with gap degree ?
k. Figure 9 shows a structure with gapdegree 1, but which is strongly ill-nested for gap degree 1.
For all trees up to 10 nodes(excluding the dummy root node at position 0) all structures of gap degree k withlength smaller than 10 are well-nested or only mildly ill-nested for that gap degree.Even if T is strongly ill-nested for a gap degree, there is always an m ?
N such thatT is mildly ill-nested for m (every structure can be binarized, and binarizations havefinite gap degree).
For example, the structure in Figure 9 is mildly ill-nested for gapdegree 2.
Therefore, MGk parsers have the property of being able to parse any arbitrarydependency structure as long as we make k large enough.
Structures like the one inFigure 9 do not arise in dependency treebanks.
None of the treebanks for nine differentlanguages10 contain structures that are strongly ill-nested for their gap degree (Table 1).Therefore, in any of these treebanks, the MGk parser can parse every sentence with gapdegree at most k in time O(n3k+4).8.
Link Grammar SchemataLink Grammar (LG), introduced by Sleator and Temperley (1991, 1993), is a theoryof syntax whose structural representation of sentences is closely related to projectivedependency representations, but with some important differences.11Undirected links: Like dependency formalisms, LG represents the structure of sentencesas a set of links between words.
However, whereas dependency links are directed,the links used in LG are undirected: There is no distinction made between heads anddependents.10 Arabic (Hajic?
et al 2004), Czech (Hajic?
et al 2006), Danish (Kromann 2003), Dutch (van der Beek et al2002), Latin (Bamman and Crane 2006), Portuguese (Afonso et al 2002), Slovene (Dz?eroski et al 2006),Swedish (Nilsson, Hall, and Nivre 2005), and Turkish (Atalay, Oflazer, and Say 2003; Oflazer et al 2003).11 A complete treatment of LG is beyond the scope of this article: Schneider (1998) gives a detailedcomparison of Link Grammar and dependency formalisms.575Computational Linguistics Volume 37, Number 3Table 1Counts of dependency structures in treebanks for several languages, classified by projectivity,gap degree, and mild and strong ill-nestedness (for their gap degree).Language StructuresTotal NonprojectiveTotal By gap degree By nestednessGapdeg1Gapdeg2Gapdeg3Gapdeg> 3Well-nestedMildlyill-nestedStronglyill-nestedArabic 2,995 205 189 13 2 1 204 1 0Czech 87,889 20,353 19,989 359 4 1 20,257 96 0Danish 5,430 864 854 10 0 0 856 8 0Dutch 13,349 4,865 4,425 427 13 0 4,850 15 0Latin 3,473 1,743 1,543 188 10 2 1,552 191 0Portuguese 9,071 1,718 1,302 351 51 14 1,711 7 0Slovene 1,998 555 443 81 21 10 550 5 0Swedish 11,042 1,079 1,048 19 7 5 1,008 71 0Turkish 5,583 685 656 29 0 0 665 20 0Cycles: The sets of links representing the structure of sentences in LG may contain cycles,in contrast to dependency structures.LG is a grammar-based formalism in which a grammar G consists of a set of words,each of which is associated with a set of linking requirements.
Given a link grammar G,a set of labelled links between the words of a sentence w1 .
.
.wn is said to be a linkagefor that sentence if it satisfies the following conditions: planarity (the links do not crosswhen drawn above the words), connectivity (the undirected graph defined by links isconnected), and satisfaction (the links satisfy the linking requirements of all the words inthe input).
An input sentence is considered grammatical with respect to a link grammarG if it is possible to build a linkage for the sentence with the grammar G.The linking requirements of a word are expressed as a set of rules specifying thelabels of the links that can be established between that word and other words locatedto its left or to its right.
Linking requirements can include constraints on the order ofthe links, for example, a requirement can specify that a word w can be linked to twowords located to its left in such a way that the link to the farthest (leftmost) word has aparticular label L2 and the link to the closest word has a label L1.We use the disjunctive form notation (Sleator and Temperley 1991) to denotelinking requirements: The requirements of words are expressed as a set of disjuncts.Each disjunct corresponds to one way of satisfying the requirements of the word.
Werepresent a disjunct for a word w as a pair of strings ?
= (R1R2 .
.
.Rq, L1L2 .
.
.
Lp) whereL1, L2, .
.
.
Lp are the labels of the links that must connect w to words located to the leftof w, which must be monotonically increasing in distance from w (e.g., Lp links to theleftmost word that is directly linked to w), and R1, R2, .
.
.Rp are the labels of the linksthat must connect w to words to its right, also monotonically increasing in distance fromw (e.g., Rq links to the rightmost word that is directly connected to w).576Go?mez-Rodr?
?guez, Carroll, and Weir Dependency Parsing SchemataParsing schemata for LG parsers follow the same principles used for constituencyand dependency formalisms.
Item sets for LG parsing schemata are defined as sets ofpartial syntactic structures, which in this case are partial linkages:Definition 9Given a link grammar G and a string w1 .
.
.wn, a partial linkage is any edge-labeledundirected graph H such that the following conditions hold. The graph H has n vertices {v1, .
.
.
, vn}, where each vertex vi is a tuple(wi, i, ?i) such that ?i is a disjunct for wi in the grammar G. The graph H is connected and satisfies the planarity requirement withrespect to the order v1, .
.
.
, vn of vertices (i.e., if we draw vertices in thatorder, with the given links, the links do not cross). Given a vertex vi = (wi, i, ?i) such that ?i = (R1R2 .
.
.Rq, L1L2 .
.
.
Lp), thefollowing conditions are satisfied:?
Every edge {vi, vj} with j < i must be labelled Ls for some 1 ?
s ?
p.?
For every pair of edges {vi, vj}, {vi, vk} such that k < j < i, we havethat {vi, vj} is labelled Ls1 , {vi, vk} is labelled Ls2 , and s1 < s2.?
Every edge {vi, vj} with j > i must be labelled Rt for some 1 ?
t ?
q.?
For every pair of edges {vi, vj}, {vi, vk} such that k > j > i, we havethat {vi, vj} is labelled Rt1 , {vi, vk} is labelled Rt2 , and t1 < t2.Informally, a partial linkage is the result of choosing a particular disjunct from thoseavailable for each word in the input string, and then adding labelled links betweenwords that are compatible with the requirements of the disjunct.
Compatibility meansthat, for each word wi associated with a disjunct ?i = (R1R2 .
.
.Rq, L1L2 .
.
.
Lp), the listof labels of links connecting vi to words to its right, ordered from the leftmost to therightmost such word, is of the form Ri1 , Ri2 , .
.
.Rir , with 0 < i1 < i2 < .
.
.
< ir ?
q and,symmetrically, the list of labels of links connecting vi to words to its left, ordered fromthe rightmost to the leftmost, is of the form Lj1 , Lj2 , .
.
.
Ljl , with 0 < j1 < j2 < .
.
.
< jl ?
p.Given such a linkage, the right linking requirements Ri1 , Ri2 , .
.
.Rir of the word wi aresatisfied, and the same for the left linking requirements Lj1 , Lj2 , .
.
.
Ljl of wi.
Linkingrequirements that are not satisfied (e.g., the requirement of a link Rk in the disjunct asso-ciated with word wi, with 0 < k ?
q, such that k /?
{i1, .
.
.
, ir}) are said to be unsatisfied.The definition of item sets for LG resembles those for dependency parsers (Defini-tion 4), where items come from a partition of the set of partial linkages for a given linkgrammar G. With these item sets, LG parsing schemata are analogous to the depen-dency and constituency cases.
As an example of an LG parsing schema, we describe theoriginal LG parser by Sleator and Temperley (1991), and show how projective parsingschemata, such as those seen in Section 3, can be adapted to obtain new LG parsers.8.1 Sleator and Temperley?s LG ParserThe LG parser of Sleator and Temperley (1991) is a dynamic programming algorithmthat builds linkages top?down: A link between vi and vk is always added before linksbetween vi and vj or between vj and vk, if i < j < k. This contrasts with many of the577Computational Linguistics Volume 37, Number 3dependency parsers seen in previous sections (Eisner 1996; Eisner and Satta 1999;Yamada and Matsumoto 2003), which build dependency graphs bottom?up.Item set: The item set for Sleator and Temperley?s parser isISlT = {[i, j, ?
?
?, ?
?
?, B, C] | 0 ?
i ?
j ?
n + 1?B, C ?
{True, False} and ?, ?, ?, ?
are strings of link labels}where an item [i, j, ?
?
?, ?
?
?, B, C] represents the set of partial linkages over the sub-string wi .
.
.wj of the input, wi is linked to words in that substring by links labelled ?and has right linking requirements ?
unsatisfied, wj is linked to words in the substringby links labelled ?
and has left linking requirements ?
unsatisfied, B is True if and onlyif there is a direct link between wi and wj, and C is True if and only if all the inner wordsin the span are transitively reflexively linked to one of the end words wi or wj, and haveall of their linking requirements satisfied.String positions referenced by the items in ISlT range from 0 to n + 1.
Position 0corresponds to an artificial word w0 (the wall) that the LG formalism inserts at thebeginning of every input sentence (Sleator and Temperley 1991).
Therefore, we assumethat strings are extended with this symbol.
On the other hand, position n + 1 corre-sponds to a dummy word wn+1 that must not be linkable to any other, and is used bythe parser for convenience, as in the schema for Yamada and Matsumoto?s dependencyparser (Section 3.4).We use the notation [i, ?, ?]
as shorthand for the item [i, i, ?
?, ?
?, False, True], whichis an item used to select a particular disjunct for a word wi.Deduction steps: The set of deduction steps is the following:SELECTDISJUNCT:[i, RqRq?1 .
.
.R1, LpLp?1 .
.
.
L1]such that wi has a disjunct ?
= (R1R2 .
.
.Rq, L1L2 .
.
.
Lp)INITTER:[0, ?, ?]
[n + 1, , ][0, n + 1, ?
?, , False, False]LEFTPREDICT:[i, j, ?
?
?, ?
?
?, B1, False] [z, ?, ?
][i, z, ?
?, ?
?, False, (z ?
i = 1)] i < z < jLEFTLINKPREDICT (vi ?b??
vz):[i, j, ?
?
b?, ?
?
?, B1, False] [z, ?, b?
][i, z, b ?
?, b ?
?, True, (z ?
i = 1)] i < z < jRIGHTPREDICT:[i, j, ?
?
?, ?
?
?, B1, False] [z, ?, ?
][z, j, ?
?, ?
?, False, ( j ?
z = 1)] i < z < jRIGHTLINKPREDICT (vz ?b??
vj):[i, j, ?
?
?, ?
?
b?, B1, False] [z, b?, ?
][z, j, b ?
?, b ?
?, True, ( j ?
z = 1)] i < z < jCOMPLETER:[i, j, ?
?
?, ?
?
?, B1, False][i, z, ?
?, ?
?, B2, True] [z, j, ?
?, ?
?, B3, True] [z, ?, ?
][i, j, ??
?, ??
?, B1, True]B2 ?
B3An annotation of the form (vi ?b??
vj) near the name of a step in this and subsequentLG schemata indicates that the corresponding step adds a link labelled b between nodes578Go?mez-Rodr?
?guez, Carroll, and Weir Dependency Parsing Schematavi and vj, and can be used to recover a set of complete linkages contained in a finalitem from each sequence of deduction steps that generates it.
The SELECTDISJUNCTstep chooses one of the available disjuncts for a given word wi.
The INITTER step startsthe top?down process by constructing a linkage that spans the whole string w1 .
.
.wn,but where no links have been constructed yet.
Then, the PREDICT and LINKPREDICTsteps repeatedly divide the problem of finding a linkage for a substring wi .
.
.wj intothe smaller subproblems of finding linkages for wi .
.
.wz and wz .
.
.wj, with i < z < j. Inparticular, the LEFTPREDICT step poses the subproblem of finding a linkage for wi .
.
.wzin which wi is not directly linked to wz, and LEFTLINKPREDICT poses the same problemwhile building a direct link from wi to wz.
RIGHTPREDICT and RIGHTLINKPREDICTproceed analogously for the substring wz .
.
.wj.
After these two smaller linkages havebeen found, they are combined by a COMPLETER step into a larger linkage; the flagsb and c in items are used by the COMPLETER step to ensure that its resulting itemwill contain a valid linkage satisfying the connectivity constraint.
An example of thisprocess, where a particular substring is parsed by using the LEFTLINKPREDICT andRIGHTPREDICT steps to divide it into smaller substrings, is shown in Figure 10.
Thealgorithm runs in time O(n3) with respect to the length of the input, because none of itsdeduction steps uses more than three independent string position indices.Final items: The set of final items is {[0, n + 1, ?
?, ?
?, B, True]}.
Items of this form containfull valid linkages for the string w0 .
.
.wn, because having the second boolean flag set toTrue implies that their linkages for w0 .
.
.wn+1 have at most two connected components,and we have assumed that the word wn+1 cannot be linked to any other, so one of thecomponents must link w0 .
.
.wn.8.2 Adapting Projective Dependency Parsers to Link GrammarWe now exploit similarities between LG linkages and projective dependency structuresto adapt projective dependency parsers to the LG formalism.
As an example we presentan LG version of the parser by Eisner (1996), but the same principles can be applied toother parsers: schemata for LG versions of the parsers by Eisner and Satta (1999) andYamada and Matsumoto (2003) can be found in Go?mez-Rodr?
?guez (2009).Item sets from dependency parsers are adapted to LG parsers by considering theforests contained in each dependency item.
The corresponding LG items contain link-ages with the same structure as these forests.
For example, because each forest in an itemof the form [i, j, False, False] in Eisner?s dependency parsing schema contains two treesFigure 10An example of LG parsing with the schema for Sleator and Temperley?s parser.579Computational Linguistics Volume 37, Number 3headed at the words wi and wj, the analogous item in the corresponding LG parsingschema will contain linkages with two connected components, one containing the wordwi and the other containing wj.
The notion of a head is lost in the conversion becausethe undirected LG linkages do not make distinctions between heads and dependents.This simplifies the notation used to denote items in some cases: For instance, we do notneed to make a distinction between Eisner items of the form [i, j, True, False] and thoseof the form [i, j, False, True], because their structure is the same other than the directionof the links.
Therefore, items in the LG version of Eisner?s parser will use a single flag,indicating whether linkages contained in them have one or two connected components.The combining and linking steps of the dependency parsers are directly translatedto LG.
If the original dependency steps always produce items containing projectivedependency forests, the resulting LG steps produce items with planar linkages.
Whenthe original dependency steps have constraints related to the position of the head initems (like combiner steps in Eisner?s parser, where we can combine [i, j, True, False] with[ j, k, True, False] but not with [ j, k, False, True]), we ignore these constraints, allowing anyword in a linkage to be its ?head?
for the purpose of linking it to other linkages.Because projective dependency parsers do not allow cyclic structures, we add stepsor remove constraints to allow cycles, so that the parsers are able to link two words thatare already in the same connected component of a linkage.
In the schema obtained fromEisner?s parser, this is done by allowing LINK steps to be applied to items representingfully connected linkages; in the schema corresponding to Eisner and Satta?s parser weallow COMBINER steps to create a link in addition to joining two linkages; and in theschema for Yamada and Matsumoto?s parser we add a step that creates two links at thesame time, combining the functionality of the L-LINK and R-LINK steps.Finally, because LG is a grammar-based formalism where the set of valid linkagesis constrained by disjuncts associated with words, we include disjunct information initems in order to ensure that only grammatical linkages are constructed.
This is similarto the schema for Sleator and Temperley?s parser, but in this case items need to specifyboth left and right linking requirements for each of their end words: These bottom?up parsers establish links from end words of an item to words outside the item?s span(which can be to the left or to the right of the span) rather than to words inside the span(which are always to the right of the left end word, and to the left of the right end word).Based on this, the following is an LG variant of the projective dependency parser ofEisner (1996).Item set: The item set isIEisLG = {[i, j, ?1 ?
?1, ?2 ?
?2, ?3, ?4, B] | 0 ?
i ?
j ?
nB ?
{True, False} and ?1, ?1, ?2, ?2, ?3, ?4 are strings of link labels}where an item of the form [i, j, ?1 ?
?1, ?2 ?
?2, ?3, ?4, B] represents the set of partiallinkages over the substring wi .
.
.wj of the input, satisfying the following conditions. All words in positions k, such that i < k < j, have all linking requirementssatisfied. The word in position i has left linking requirements ?3 not satisfied, andright linking requirements ?1?1, where the requirements ?1 are satisfiedby links to words within the item?s span, and the requirements ?1 are notsatisfied.
Requirements appear in ?3 and ?1?1 in increasing order of linkdistance.580Go?mez-Rodr?
?guez, Carroll, and Weir Dependency Parsing Schemata The word in position j has right linking requirements ?4 not satisfied, andleft linking requirements ?2?2, where the requirements ?2 are satisfied bylinks to words within the item?s span, and the requirements ?2 are notsatisfied.
Requirements appear in ?4 and ?2?2 in increasing order of linkdistance. The partial linkage is connected if B equals True, or has exactly twoconnected components (one containing the node vi and the othercontaining vj) if B equals False.Deduction steps: The set of deduction steps for this parser is as follows:INITTER:[i, i + 1, ?R, ?L, ?L, ?R, False]0 ?
i ?
n ?
1such that wi has a disjunct ?i = (?R, ?L) and wi+1 has a disjunct ?i+1 = (?R, ?L).LINK (vi ?b??
vj):[i, j, ?1 ?
b?1, ?2 ?
b?2, ?3, ?4, B][i, j, ?1b ?
?1, ?2b ?
?2, ?3, ?4, True]COMBINE:[i, j, ?1 ?
?1, ?2?, ?3, ?4, B1][ j, k, ?4?, ?2 ?
?2, ?2, ?4, B2][i, k, ?1 ?
?1, ?2 ?
?2, ?3, ?4, B1 ?
B2]B1 ?
B2These steps resemble those in the schema for Eisner?s dependency parser, with theexception that the LINK step is able to build links on items that contain fully connectedlinkages (equivalent to the [i, j, True, False] and [i, j, False, True] items of the dependencyparser).
A version of the parser restricted to acyclic linkages can be obtained by addingthe constraint that B must equal False in the LINK step.Final items: The set of final items is {[0, n, ?
?, ?
?, , , True]}, corresponding to the set ofitems containing fully connected linkages for the whole input string.LG parsing schemata based on the parsers of Eisner and Satta (1999) and Yamadaand Matsumoto (2003) are not shown here for space reasons, but are presented byGo?mez-Rodr?
?guez (2009).
The relationships between these three LG parsing schemataare the same as the corresponding dependency parsing schemata, that is, the LG vari-ants of Eisner and Satta?s and Yamada and Matsumoto?s dependency parsers are stepcontractions of the LG variant of Eisner?s parser.
As with the algorithm of Sleator andTemperley, these bottom?up LG parsers run in cubic time with respect to input length.9.
Conclusions and Future WorkThe parsing schemata formalism of Sikkel (1997) has previously been used to define,analyze, and compare algorithms for constituency-based parsing.
We have shown howto extend the formalism to dependency parsers, as well as the related Link Grammarformalism.Deductive approaches have been used in the past to describe individual depen-dency parsers: In Kuhlmann (2007, 2010) a grammatical deduction system was used todefine a parser for regular dependency grammars.581Computational Linguistics Volume 37, Number 3McDonald and Nivre (2007) give an alternative framework for dependency parsers,viewing them as transition systems.
That model is based on parser configurations andtransitions, and has no clear relationship to the approach described here.To demonstrate the theoretical uses of dependency parsing schemata, we have usedthem to describe a wide range of existing projective and non-projective dependencyparsers.
We have also clarified various relations between parsers which were origi-nally formulated very differently?for example, establishing the relation between thedynamic programming algorithm of Eisner (1996) and the transition-based parser ofYamada and Matsumoto (2003).
We have also used the parsing schemata framework asa formal tool to verify the correctness of parsing algorithms.Not only are dependency parsing schemata useful when describing and extendingexisting parsing algorithms, they can be used to define new parsers.
We have presentedan algorithm that can parse any well-nested dependency structure with gap degreebounded by a constant k with time complexity O(n2k+5), and additionally, have defineda wider set of structures that we call mildly ill-nested for a given gap degree k, andpresented an algorithm that can parse these in time O(n3k+4).
The practical relevanceof this set of structures can be seen in the data obtained from several dependencytreebanks, showing that all the sentences contained in them are mildly ill-nested fortheir gap degree, and thus they are parsable with this algorithm.
The strategy usedby this algorithm for parsing mildly ill-nested structures has been adapted to solvethe problem of finding minimal fan-out binarizations of LCFRS to improve parsingefficiency (see Go?mez-Rodr?
?guez et al 2009).An interesting line of future work would be to provide implementations of themildly non-projective dependency parsers presented here, using probabilistic models toguide their linking decisions, and compare their practical performance and accuracy tothose of other non-projective dependency parsers.
Additionally, our definition of mildlyill-nested structures is closely related to the way the corresponding parser works.
Itwould be interesting to find a more grammar-oriented definition that would providelinguistic insight into this set of structures.An alternative generalization of the concept of well-nestedness has recently beenintroduced by Maier and Lichte (2011).
The definition of this property of structures,called k-ill-nestedness, is more declarative than that of mildly ill-nestedness.
However,it is based on properties that are not local to projections or subtrees, and there is noevidence that k-ill-nested structures are parsable in polynomial time.Finally, we observe that that some well-known parsing algorithms discussed here(Nivre 2003; McDonald et al 2005) rely on statistically-driven control mechanisms thatfall below the abstraction level of parsing schemata.
Therefore, it would be useful tohave an extension of parsing schemata allowing the description and comparison ofthese control structures in a general way.AcknowledgmentsThis work was partially supported byMEC and FEDER (HUM2007-66607-C04)and Xunta de Galicia(PGIDIT07SIN005206PR,INCITE08E1R104022ES,INCITE08ENA305025ES,INCITE08PXIB302179PR, Rede Galega deProc.
da Linguaxe e Recup.
de Informacio?n,Rede Galega de Lingu??
?stica de Corpus,Bolsas Estad?
?as INCITE/FSE cofinanced).ReferencesAfonso, Susana, Eckhard Bick, Renato Haber,and Diana Santos.
2002.
?Florestasinta?(c)tica?
: A treebank for Portuguese.
InProceedings of the 3rd InternationalConference on Language Resources andEvaluation (LREC 2002), pages 1968?1703,Las Palmas.Alonso, Miguel A., David Cabrero, E?ricVillemonte de la Clergerie, and ManuelVilares.
1999.
Tabular algorithms for TAG582Go?mez-Rodr?
?guez, Carroll, and Weir Dependency Parsing Schemataparsing.
In Proceedings of the NinthConference of the European Chapter of theAssociation for Computational Linguistics(EACL-99), pages 150?157, Bergen.Atalay, Nart B., Kemal Oflazer, and Bilge Say.2003.
The annotation process in theTurkish treebank.
In Proceedings of EACLWorkshop on Linguistically InterpretedCorpora (LINC-03), pages 243?246,Budapest.Attardi, Giuseppe.
2006.
Experiments with amultilanguage non-projective dependencyparser.
In Proceedings of the 10th Conferenceon Computational Natural Language Learning(CoNLL-X), pages 166?170, New York, NY.Bamman, David and Gregory Crane.
2006.The design and use of a Latin dependencytreebank.
In Proceedings of the FifthWorkshop on Treebanks and LinguisticTheories (TLT 2006), pages 67?78, Prague.Barbero, Cristina, Leonardo Lesmo,Vincenzo Lombardo, and Paola Merlo.1998.
Integration of syntactic andlexical information in a hierarchicaldependency grammar.
In Proceedings ofCOLING-ACL ?98 Workshop on Processing ofDependency-Based Grammars, pages 58?67,Montreal.van der Beek, Leonoor, Gosse Bouma, RobertMalouf, and Gertjan van Noord.
2002.
TheAlpino dependency treebank.
In Languageand Computers, Computational Linguistics inthe Netherlands 2001.
Selected Papers fromthe Twelfth CLIN Meeting, pages 8?22,Amsterdam.Billot, Sylvie and Bernard Lang.
1989.
Thestructure of shared forests in ambiguousparsing.
In Proceedings of the 27th AnnualMeeting of the Association for ComputationalLinguistics (ACL?89), pages 143?151,Montreal.Bodirsky, Manuel, Marco Kuhlmann, andMathias Mo?hl.
2005.
Well-nested drawingsas models of syntactic structure (extendedversion).
Technical report, SaarlandUniversity.Che, Wanxiang, Zhenghua Li, Yuxuan Hu,Yongqiang Li, Bing Qin, Ting Liu, andSheng Li.
2008.
A cascaded syntactic andsemantic dependency parsing system.In Proceedings of the 12th Conference onComputational Natural Language Learning(CoNLL 2008), pages 238?242, Manchester.Collins, Michael John.
1996.
A new statisticalparser based on bigram lexicaldependencies.
In Proceedings of the 34thAnnual Meeting of the Association forComputational Linguistics (ACL?96),pages 184?191, Santa Cruz, CA.Corston-Oliver, Simon, Anthony Aue, KevinDuh, and Eric Ringger.
2006.
Multilingualdependency parsing using Bayes PointMachines.
In Proceedings of the HumanLanguage Technology Conference of the NorthAmerican Chapter of the Association forComputational Linguistics (NAACL HLT2006), pages 160?167, New York, NY.Courtin, Jacques and Damien Genthial.1998.
Parsing with dependency relationsand robust parsing.
In Proceedings ofCOLING-ACL ?98 Workshop on Processing ofDependency-Based Grammars, pages 88?94,Montreal.Covington, Michael A.
1990.
A dependencyparser for variable-word-order languages.Technical Report AI-1990-01, Universityof Georgia, Athens, GA.Covington, Michael A.
2001.
A fundamentalalgorithm for dependency parsing.
InProceedings of the 39th Annual ACMSoutheast Conference, pages 95?102,Athens, GA.Cui, Hang, Renxu Sun, Keya Li, Min-YenKan, and Tat-Seng Chua.
2005.
Questionanswering passage retrieval usingdependency relations.
In SIGIR ?05:Proceedings of the 28th Annual InternationalACM SIGIR Conference on Research andDevelopment in Information Retrieval,pages 400?407, Salvador.Culotta, Aron and Jeffrey Sorensen.
2004.Dependency tree kernels for relationextraction.
In ACL ?04: Proceedings of the42nd Annual Meeting of the Association forComputational Linguistics, pages 423?429,Barcelona.Ding, Yuan and Martha Palmer.
2005.Machine translation using probabilisticsynchronous dependency insertiongrammars.
In ACL ?05: Proceedings of the43rd Annual Meeting of the Association forComputational Linguistics, pages 541?548,Ann Arbor, MI.Dz?eroski, Sas?o, Tomaz?
Erjavec,Nina Ledinek, Petr Pajas, Zdene?kZ?abokrtsky?, and Andreja Z?ele.
2006.Towards a Slovene dependency treebank.In Proceedings of the 5th InternationalConference on Language Resources andEvaluation (LREC 2006), pages 1388?1391,Genoa.Earley, Jay.
1970.
An efficient context-freeparsing algorithm.
Communications of theACM, 13(2):94?102.Eisner, Jason.
1996.
Three new probabilisticmodels for dependency parsing: Anexploration.
In Proceedings of the 16thInternational Conference on Computational583Computational Linguistics Volume 37, Number 3Linguistics (COLING-96), pages 340?345,Copenhagen.Eisner, Jason, Eric Goldlust, and Noah A.Smith.
2005.
Compiling comp ling:Weighted dynamic programmingand the Dyna language.
In Proceedingsof Human Language TechnologyConference and Conference on EmpiricalMethods in Natural Language Processing(HLT-EMNLP 2005), pages 281?290,Vancouver.Eisner, Jason and Giorgio Satta.
1999.Efficient parsing for bilexical context-freegrammars and head automaton grammars.In Proceedings of the 37th Annual Meetingof the Association for ComputationalLinguistics (ACL?99), pages 457?464,College Park, MD.Eisner, Jason and Giorgio Satta.
2000.
Afaster parsing algorithm for lexicalizedtree-adjoining grammars.
In Proceedingsof the 5th Workshop on Tree-AdjoiningGrammars and Related Formalisms (TAG+5),pages 14?19, Paris.Fundel, Katrin, Robert Ku?ffner, and RalfZimmer.
2006.
RelEx?Relation extractionusing dependency parse trees.Bioinformatics, 23(3):365?371.Gaifman, Haim.
1965.
Dependency systemsand phrase-structure systems.
Informationand Control, 8:304?337.Go?mez-Rodr?
?guez, Carlos.
2009.
ParsingSchemata for Practical Text Analysis.
Ph.D.thesis, Universidade da Corun?a, Spain.Go?mez-Rodr?
?guez, Carlos, John Carroll, andDavid Weir.
2008.
A deductive approach todependency parsing.
In Proceedings of the46th Annual Meeting of the Association forComputational Linguistics: Human LanguageTechnologies (ACL?08:HLT), pages 968?976,Columbus, OH.Go?mez-Rodr?
?guez, Carlos, MarcoKuhlmann, Giorgio Satta, and David Weir.2009.
Optimal reduction of rule length inlinear context-free rewriting systems.
InProceedings of NAACL HLT 2009: theConference of the North American Chapter ofthe Association for Computational Linguistics,pages 539?547, Boulder, CO.Go?mez-Rodr?
?guez, Carlos, Jesu?s Vilares, andMiguel A. Alonso.
2009.
A compiler forparsing schemata.
Software: Practice andExperience, 39(5):441?470.Go?mez-Rodr?
?guez, Carlos, David Weir, andJohn Carroll.
2008.
Parsing mildlynon-projective dependency structures(extended version).
Technical ReportCSRP 600, Department of Informatics,University of Sussex.Go?mez-Rodr?
?guez, Carlos, David Weir, andJohn Carroll.
2009.
Parsing mildlynon-projective dependency structures.
InProceedings of the 12th Conference of theEuropean Chapter of the Association forComputational Linguistics (EACL-09),pages 291?299, Athens.Hajic?, Jan, Jarmila Panevova?, Eva Hajic?ova?,Jarmila Panevova?, Petr Sgall, Petr Pajas,Jan S?te?pa?nek, Jir???
Havelka, and MarieMikulova?.
2006.
Prague DependencyTreebank 2.0.
CDROM CAT: LDC2006T01,ISBN 1-58563-370-4.
Linguistic DataConsortium, University of Pennsylvania.Hajic?, Jan, Otakar Smrz?, Petr Zema?nek, JanS?naidauf, and Emanuel Bes?ka.
2004.Prague Arabic dependency treebank:Development in data and tools.
InProceedings of the NEMLAR InternationalConference on Arabic Language Resources andTools, pages 110?117, Cairo.Havelka, Jir???.
2007.
Beyond projectivity:Multilingual evaluation of constraintsand measures on non-projectivestructures.
In ACL 2007: Proceedings of the45th Annual Meeting of the Association forComputational Linguistics, pages 608?615,Prague.Hays, David.
1964.
Dependency theory: aformalism and some observations.Language, 40:511?525.Herrera, Jesu?s, Anselmo Pen?as, and FelisaVerdejo.
2005.
Textual entailmentrecognition based on dependency analysisand WordNet.
In J. Quin?onero-Camdela,I.
Dagan, B. Magnini, and F. d?Alche?-Buc,editors, Machine Learning Challenges.Lecture Notes in Computer Science,vol.
3944.
Springer-Verlag, Berlin-Heidelberg-New York, pages 231?239.Hotz, Gu?nter and Gisela Pitsch.
1996.
Onparsing coupled-context-free languages.Theoretical Computer Science,161(1-2):205?233.Joshi, Aravind K. and Yves Schabes.
1997.Tree-adjoining grammars.
In G. Rozenbergand A. Salomaa, editors, Handbook ofFormal Languages, vol.
3: Beyond Words,Springer-Verlag, New York, NY,pages 69?123.Kahane, Sylvain, Alexis Nasr, and OwenRambow.
1998.
Pseudo-projectivity: Apolynomially parsable non-projectivedependency grammar.
In Proceedings of the36th Annual Meeting of the Association forComputational Linguistics and the 17thInternational Conference on ComputationalLinguistics (COLING-ACL?98),pages 646?652, San Francisco, CA.584Go?mez-Rodr?
?guez, Carroll, and Weir Dependency Parsing SchemataKasami, Tadao.
1965.
An efficient recognitionand syntax algorithm for context-freelanguages.
Scientific ReportAFCRL-65-758, Air Force CambridgeResearch Lab, Bedford, MA.Kromann, Matthias T. 2003.
The Danishdependency treebank and the underlyinglinguistic theory.
In Proceedings of the 2ndWorkshop on Treebanks and LinguisticTheories (TLT), pages 217?220, Va?xjo?.Kuhlmann, Marco.
2007.
DependencyStructures and Lexicalized Grammars.
D. Phildissertation, Saarland University,Saarbru?cken, Germany.Kuhlmann, Marco.
2010.
DependencyStructures and Lexicalized Grammars: AnAlgebraic Approach.
Lecture Notes inComputer Science, vol.
6270.
Springer,New York, NY.Kuhlmann, Marco and Mathias Mo?hl.
2007.Mildly context-sensitive dependencylanguages.
In Proceedings of the 45th AnnualMeeting of the Association for ComputationalLinguistics (ACL 2007), pages 160?167,Prague.Kuhlmann, Marco and Joakim Nivre.
2006.Mildly non-projective dependencystructures.
In Proceedings of theCOLING/ACL 2006 Main Conference PosterSessions, pages 507?514, Montreal.Lombardo, Vincenzo and Leonardo Lesmo.1996.
An Earley-type recognizer fordependency grammar.
In Proceedings ofthe 16th International Conference onComputational Linguistics (COLING 96),pages 723?728, San Francisco, CA.Maier, Wolfgang and Timm Lichte.
2011.Characterizing discontinuity inconstituent treebanks.
In P. de Grook,M.
Egg, and L. Kallmeyer, editors,Formal Grammar, volume 5591 ofLecture Notes in Computer Science.Springer-Verlag, Berlin-Heidelberg-New York, pages 164?179.McDonald, Ryan, Koby Crammer, andFernando Pereira.
2005.
Onlinelarge-margin training of dependencyparsers.
In ACL ?05: Proceedings of the 43rdAnnual Meeting of the Association forComputational Linguistics, pages 91?98,Ann Arbor, MI.McDonald, Ryan and Joakim Nivre.
2007.Characterizing the errors of data-drivendependency parsing models.
In Proceedingsof the 2007 Joint Conference on EmpiricalMethods in Natural Language Processing andComputational Natural Language Learning(EMNLP-CoNLL 2007), pages 122?131,Prague.McDonald, Ryan, Fernando Pereira, KirilRibarov, and Jan Hajic?.
2005.
Non-projective dependency parsing usingspanning tree algorithms.
In HLT/EMNLP2005: Proceedings of the Conference onHuman Language Technology and EmpiricalMethods in Natural Language Processing,pages 523?530, Vancouver.McDonald, Ryan and Giorgio Satta.
2007.On the complexity of non-projectivedata-driven dependency parsing.
In IWPT2007: Proceedings of the 10th InternationalConference on Parsing Technologies,pages 121?132, Prague.Nilsson, Jens, Johan Hall, and JoakimNivre.
2005.
MAMBA meets TIGER:Reconstructing a Swedish treebank fromantiquity.
In Proceedings of NODALIDA2005 Special Session on Treebanks,pages 119?132, Joensuu.Nivre, Joakim.
2003.
An efficient algorithmfor projective dependency parsing.
InProceedings of the 8th International Workshopon Parsing Technologies (IWPT 03),pages 149?160, Nancy.Nivre, Joakim.
2007.
Incrementalnon-projective dependency parsing.
InProceedings of NAACL HLT 2007: TheAnnual Conference of the North AmericanChapter of the Association for ComputationalLinguistics, pages 396?403, Rochester, NY.Nivre, Joakim, Johan Hall, Sandra Ku?bler,Ryan McDonald, Jens Nilsson, SebastianRiedel, and Deniz Yuret.
2007.
The CoNLL2007 shared task on dependency parsing.In Proceedings of the CoNLL Shared TaskSession of EMNLP-CoNLL 2007,pages 915?932, Prague.Nivre, Joakim, Johan Hall, and Jens Nilsson.2004.
Memory-based dependencyparsing.
In Proceedings of the 8th Conferenceon Computational Natural LanguageLearning (CoNLL-2004), pages 49?56,Boston, MA.Nivre, Joakim, Johan Hall, Jens Nilsson,Atanas Chanev, Gu?ls?en Eryig?it, SandraKu?bler, Stetoslav Marinov, and ErwinMarsi.
2007.
MaltParser: A language-independent system for data-drivendependency parsing.
Natural LanguageEngineering, 13(2):99?135.Nivre, Joakim, Johan Hall, Jens Nilsson,Gu?ls?en Eryig?it, and Stetoslav Marinov.2006.
Labeled pseudo-projectivedependency parsing with support vectormachines.
In Proceedings of the 10thConference on Computational NaturalLanguage Learning (CoNLL-X),pages 221?225, Sydney.585Computational Linguistics Volume 37, Number 3Nivre, Joakim and Ryan McDonald.2008.
Integrating graph-based andtransition-based dependency parsers.
InProceedings of the 46th Annual Meeting of theAssociation for Computational Linguistics:Human Language Technologies (ACL-08:HLT), pages 950?958, Columbus, OH.Nivre, Joakim and Jens Nilsson.
2005.Pseudo-projective dependency parsing.
InACL ?05: Proceedings of the 43rd AnnualMeeting of the Association for ComputationalLinguistics, pages 99?106, Ann Arbor, MI.Oflazer, Kemal, Bilge Say, Dilek ZeynepHakkani-Tu?r, and Go?khan Tu?r.
2003.Building a Turkish treebank.
In A. Abeille,editor, Building and Exploiting Syntactically-annotated Corpora.
Kluwer, Dordrecht,pages 261?277.Satta, Giorgio.
1992.
Recognition of linearcontext-free rewriting systems.
InProceedings of the 30th Annual Meeting of theAssociation for Computational Linguistics(ACL?92), pages 89?95, Newark, DE.Schneider, Gerold.
1998.
A linguisticcomparison of constituency, dependency,and link grammar.
M.Sc.
thesis, Universityof Zurich, Switzerland.Shen, Libin, Jinxi Xu, and Ralph Weischedel.2008.
A new string-to-dependencymachine translation algorithm with atarget dependency language model.
InProceedings of the 46th Annual Meeting of theAssociation for Computational Linguistics:Human Language Technologies (ACL-08:HLT), pages 577?585, Columbus, OH.Shieber, Stuart M., Yves Schabes, andFernando C. N. Pereira.
1995.
Principlesand implementation of deductive parsing.Journal of Logic Programming, 24:3?36.Sikkel, Klaas.
1994.
How to compare thestructure of parsing algorithms.
InProceedings of ASMICS Workshop on ParsingTheory, pages 21?39, Milano.Sikkel, Klaas.
1997.
Parsing Schemata ?
AFramework for Specification and Analysis ofParsing Algorithms.
Texts in TheoreticalComputer Science ?
An EATCS Series.Springer-Verlag, Berlin-Heidelberg-New York.Sleator, Daniel and Davy Temperley.
1991.Parsing English with a Link Grammar.Technical report CMU-CS-91-196, CarnegieMellon University, Pittsburgh, PA.Sleator, Daniel and Davy Temperley.
1993.Parsing English with a Link Grammar.In Proceedings of the Third InternationalWorkshop on Parsing Technologies (IWPT?93),pages 277?292, Tilburg.Surdeanu, Mihai, Richard Johansson, AdamMeyers, Llu?
?s Ma`rquez, and Joakim Nivre.2008.
The CoNLL-2008 shared task onjoint parsing of syntactic and semanticdependencies.
In Proceedings of the 12thConference on Computational NaturalLanguage Learning (CoNLL-2008),pages 159?177, Manchester.Vijay-Shanker, K., David J. Weir, andAravind K. Joshi.
1987.
Characterizingstructural descriptions produced byvarious grammatical formalisms.
InProceedings of the 25th Annual Meeting of theAssociation for Computational Linguistics(ACL?87), pages 104?111, Stanford, CA.de Vreught, J. P. M. and H. J. Honig.
1989.A tabular bottom?up recognizer.Report 89-78, Delft University ofTechnology, Delft, the Netherlands.Yamada, Hiroyasu and Yuji Matsumoto.2003.
Statistical dependency analysis withsupport vector machines.
In Proceedingsof 8th International Workshop on ParsingTechnologies (IWPT 2003), pages 195?206,Nancy.Younger, Daniel H. 1967.
Recognition andparsing of context-free languages in timen3.
Information and Control, 10(2):189?208.586
