Coling 2010: Poster Volume, pages 1453?1461,Beijing, August 2010Discriminant Ranking for Efficient TreebankingYi Zhang Valia KordoniLT-Lab, German Research Center for Artificial Intelligence (DFKI GmbH)Department of Computational Linguistics, Saarland University{yzhang,kordoni}@coli.uni-sb.deAbstractTreebank annotation is a labor-intensiveand time-consuming task.
In this paper,we show that a simple statistical rankingmodel can significantly improve treebank-ing efficiency by prompting human an-notators, well-trained in disambiguationtasks for treebanking but not necessarilygrammar experts, to the most relevant lin-guistic disambiguation decisions.
Experi-ments were carried out to evaluate the im-pact of such techniques on annotation ef-ficiency and quality.
The detailed analysisof outputs from the ranking model showsstrong correlation to the human annotatorbehavior.
When integrated into the tree-banking environment, the model brings asignificant annotation speed-up with im-proved inter-annotator agreement.
?1 IntroductionThe development of a large-scale treebank (Mar-cus et al, 1993; Hajic?
et al, 2000; Brants et al,2002) with rich syntactic annotations is a highlyrewarding task.
But the huge amount of man-ual labor required for the annotation task itself,as well as the difficulties in standardizing linguis-tic analyses, results in long development cyclesof such valuable language resources, which typ-ically amounts to years or even decades.
Despitethe profound scientific and practical value of de-tailed syntactic treebanks, the requirement and ne-cessity for long-term commitment raises the risk?The first author thanks the German Excellence Clusterof Multimodal Computing and Interaction for the support ofthe work.cost of such projects, a fact which often makesthem not feasible in many current economical en-vironments.In recent years, computational grammars havebeen employed to assist the construction of suchlanguage resources.
A typical development modelinvolves a parser which generates candidate anal-yses, and human annotators who manually iden-tify the desired tree structure.
This treebankingmethod dramatically reduces the cost of train-ing annotators, for they are not required to spon-taneously produce linguistic solutions to vari-ous phenomena.
Instead, they are trained toassociate their language intuition with specificlinguistically-relevant decisions.
How to selectand carefully present such decisions to the an-notators is thus crucial for achieving high an-notation speed and quality.
On the other hand,for large treebanking projects, parallel annota-tion with multiple annotators is usually neces-sary.
Inter-annotator agreement is a crucial qualitymeasure in such cases.
But improvements on an-notation speed should not be achieved at expenseof the quality of the treebank.With both speed and quality in mind, a goodtreebank annotation method should acknowledgethe complexity of the decision-making process;for instance, the same tree can be disambiguatedby different sets of individual decisions whichare mutually dependent.
The annotation methodshould also strive to create a distraction-free en-vironment for annotators who can then focus onmaking the judgments.
To this effect, we present asimple statistical model that learns from the anno-tation history, and offers a ranking of disambigua-tion decisions from the most to the least relevant1453ones, which enables well-trained human annota-tors to speed-up treebanking without compromis-ing on the quality of the linguistic decisions guid-ing the annotation task.The remaining of this paper is structured as fol-lows: Section 2 gives an overview of the diffi-culties in syntactic annotation, and the potentialways of improving the annotation efficiency with-out damaging the quality; Section 3 presents thenew annotation method which is based on a statis-tical discriminant ranking model; Sections 4 and 5describe the setup and results of a series of anno-tation experiments; Section 6 concludes the paperand proposes future research directions.2 BackgroundLarge-scale full syntactic annotation has for quitesome time been approached with mixed feelingsby researchers.
On the one hand, detailed syn-tactic annotation serves as a basis for corpus-linguistic study and data-driven NLP methods.Especially, when combined with popular super-vised machine learning methods, richly annotatedlanguage resources, like, for instance, treebanks,play a key role in modern computational linguis-tics.
The public availability of large-scale tree-banks in recent years has stimulated the blossom-ing of data-driven approaches to syntactic and se-mantic parsing.On the other hand, though, the creation of de-tailed syntactic structures turns out to be an ex-tremely challenging task.
From the choice ofthe appropriate linguistic framework and the de-sign of the annotation scheme to the choice of thetext source and the working protocols on the syn-cronization of the parallel development, as well asthe quality assurance, none of these steps in theentire annotation procedure is considered a solvedissue.
Given the vast design choices, very fewof the treebanking projects have made it throughall these difficult annotation stages.
Even themost outstanding projects have not been com-pleted without receiving criticisms.Our treebanking project is no exception.
Theaim of the project is to provide annotationsof the Wall Street Journal (henceforward WSJ)sections of the Penn Treebank (henceforwardPTB (Marcus et al, 1993)) with the help ofthe English Resource Grammar (henceforwardERG; (Flickinger, 2002)), a hand-written large-scale and wide-coverage grammar of English inthe framework of Head-Driven Phrase StructureGrammar (HPSG; (Pollard and Sag, 1994)).
Suchannotations are very rich linguistically, since apartfrom syntax they also incorporate semantic infor-mation.
The annotation cycle is organized intoiterations of parsing, treebanking in the sense ofdisambiguating syntactic and semantic analysesof the various linguistic phenomena contained inthe corpus, error analysis and grammar/treebankupdate cycles.
That is, sentences from the WSJare first parsed with the PET parser (Callmeier,2001), an efficient unification-based parser, usingthe ERG.
The parsing results are then manuallydisambiguated by human annotators.
However,instead of considering individual trees, the annota-tion process is mostly invested on binary decisionswhich are made on either accepting or rejectingconstructions or lexical types.
Each of such deci-sions, called discriminants, as we will also see inthe following, reduces the number of the trees sat-isfying the constraints.
The process is presentedin the next section in more detail.
What should,though, be clear is that the aforementioned multi-cycle annotation procedure is as time-consumingand human-error prone as any other, despite thefact that at the center of the entire annotation cy-cle lies a valuable linguistic resource, which hasbeen developed with a lot of effort over manyyears, namely the ERG.
For the first period of thisproject, we have established an average speed of40 sentences per annotator hour, meaning a totalof ?1200 hours of annotation for the entire WSJ.Including the long training period at the beginningof the project, and periodical grammar and tree-bank updates, the project period is roughly twoyears with two part-time annotators employed.3 Statistical Discriminant Ranking3.1 Discriminants & DecisionsOne common characteristic of modern treebank-ing efforts ?
especially, in so-called dynamic tree-banking platforms (cf., for instance, (Oepen et al,2002) and http://redwoods.stanford.edu), like the one we are describing and referring1454to extensively in the following, is that the can-didate trees are constructed automatically by thegrammar, and then manually disambiguated byhuman annotators.
In doing so, linguistically richannotation is built efficiently with minimum man-ual labor.
In order to further improve the manualdisambiguation efficiency, systems like [incrtsdb()] (Oepen, 2001) compute the differencebetween candidate analyses.
Instead of looking atthe huge parse forest, the treebank annotators se-lect or reject the features that distinguish betweendifferent parses, until no ambiguity remains (ei-ther one analysis is accepted from the parse forest,or all of them are rejected).
The number of deci-sions for each sentence is normally around log2nwhere n is the total number of candidate trees.For a sentence with 5000 candidate readings, onlyabout 12 treebanking decisions are required for acomplete disambiguation.
A similar method wasalso proposed in (Carter, 1997).Formally, an attribute that distinguishes be-tween different parses is called a discriminant.For Redwoods-style treebanks, this is extracted ei-ther from the syntactic derivation trees or the se-mantic representations (in the form of MinimalRecursion Semantics (MRS; (Copestake et al,2005))).Figure 1 shows an example graphical annota-tion interface.
At the top of the window, a listof action buttons shows the operations permittedon the sentence level.
Then the sentence in itsoriginal PTB bracket format is shown.
15 : 0indicates that at the current disambiguation state,15 trees remain to be disambiguated while 0 hasbeen eliminated.
On the left large panel, the can-didate trees are shown in their simplified phrase-structure representation.
Note that the actualHPSG analyses are not shown in the screenshotand can be displayed on request.
On the right largepanel, the list of effective discriminants (see Sec-tion 3.2) up to this disambiguation state is shown.The highlighted discriminant in Figure 1 suggestsa possibility of constructing the entire sentence bychoosing a subject-head rule (SUBJH), taking ?ms.Haag?
as the subject and ?plays Elianti.?
as thehead daughter.
When the discriminant is clicked,the annotator can say yes or no to it, hence nar-rowing the remaining trees to the In Parses or OutParses.
The unknown button is used to mark theuncertainties and is rarely used.Note that in this interface, the discriminantsare sorted in descending order according to theirlength, meaning that the discriminants related tohigher level constructions are shown before thelexical type choices.
When up to 500 parsesare stored in the forest, the average number ofdiscriminants per forest is about 100.
Scan-ning through the long list manually can be time-consuming and distracting.Kordoni and Zhang (2009) show that annota-tors tend to start with the decisions with the mostcertainty, and delay the ?hard?
decisions as muchas possible.
As the decision progresses, many ofthe ?hard?
discriminants will receive an inferredvalue from the certain decisions.
Our annotationguideline only describes specific decisions.
Theorder in which discriminants are chosen is left un-derspecified and very much depends on personalstyles.
In practice, we see that our annotatorsgradually developed complex strategies which in-volve both top-down and bottom-up pruning.One potential drawback of such a discriminant-based treebanking method is that the process isvery sensitive to decision errors.
One wrong judg-ment can rule out the correct tree and ruin theanalysis of the sentence.
In such a case, the an-notators usually resort to backtracking to previousdecisions they had made.
To compensate for this,we ask our annotators to double-check the tree-banked analysis before saving the disambiguationresult.
And in case of doubt, they are instructed toavoid ambivalent decisions as much as possible.3.2 Maximum-Entropy-Based DiscriminantRanking ModelSuppose for a sentence ?, a parse forest Y wasgenerated by the grammar.
Note that for effi-ciency reasons, the parse forest might have beentrimmed to only contain up to n top readingsranked by the parse disambiguation model.
Forconvenience, we note the parse forest Y as a setof parses {y1, y2, .
.
.
, yn}.
Each discriminant ddefines a binary valued function ?
on the parseforest (?
: Y 7?
{0, 1}) , which can be inter-preted as whether a parse yi has attribute d or not.By the nature of this definition, each discriminant1455Figure 1: Screenshot of the discriminant-based treebanking graphical annotator interfacefunction defines a bi-partition of the parse forest.When both subsets of the partition are non-empty,i.e., there exists at least one yp and yq such that?
(yp) = 0 and ?
(yq) = 1, the discriminant is con-sidered effective on the forest Y .
In the followingdiscussion, we are only considering the set of ef-fective discriminants D for parse forest Y .Instead of directly predicting the outcome ofdisambiguation decision on each discriminant(i.e., whether the GOLD tree has discriminantfunction value 0 or 1), our model tries to measurethe probability of a discriminant being chosen byhuman annotators, regardless of the yes/no deci-sion.
For each discriminant d, and the parse forestY , a set of feature functions f1, f2, .
.
.
, fk receivereal values, and contribute to the following log-linear model:P (d|Y,D) = exp(?ki=1 ?ifi(d, Y ))?d?
?D exp(?ki=1 ?ifi(d?, Y ))(1)where ?1, ?2, .
.
.
, ?k are the parameters of themodel.To estimate these model parameters, we gatherthe annotation logs from our treebank annotatorson the completed datasets with detailed informa-tion about each discriminant.
Apart from thenecessary information to reconstruct the discrim-inants from the forest, the log also contains thestatus information of i) whether the discriminanttakes value 0 or 1 on the gold tree; ii) whetherthe human annotator has said yes or no to the dis-criminant.
Note that the human annotator does notneed to manually decide on the value of each dis-criminant.
Whenever a new decision is made, theforest will be pruned to the subset of trees compat-ible with the decision.
And all remaining discrim-inants are checked for effectiveness on the prunedforest.
Discriminants which become ineffectivefrom previous decisions are said to have receivedinferred values.The parameters of the model are estimated bythe open-source maximum entropy parameter es-1456timation toolkit TADM1.
For training, we useall the manually disambiguated discriminants aspositive instances, and automatically inferred dis-criminants as negative instances.The discriminant ranking model is applied dur-ing the manual annotation sessions.
When a parseforest is loaded and the discriminants are con-structed, each discriminant is assigned an (un-normalized) score ?ki=1 ?ifi(d, Y ), and the listof discriminants is sorted by descending orderof the score accordingly.
The scoring and sort-ing adds negligible additional computation on thetreebanking software, and is not noticeable to thehuman annotators.
By putting those discriminantsthat are potentially to be manually judged near thetop of the list, the model saves manual labor onscanning through the lengthy list by filtering outambivalent discriminants.Note that this discriminant ranking model pre-dicts the possibility of a discriminant being man-ually disambiguated.
It is not modeling the spe-cific decision that the human annotator makes onthe discriminant.
Including the decision outcomein the model can potentially damage the annota-tion quality if annotators develop a habit of over-trusting the model prediction, making the wholemanual annotation pointless.
A discriminant rank-ing model, however, only suggestively re-ordersthe discriminants on the presentation level, whichare much safer when the annotation quality is con-cerned.3.3 Feature Model for SyntacticDiscriminantsIn practice, there are different ways of finding dis-criminants from the parse forest.
For instance, the[incr tsdb()] system supports both syntax-based and semantics-based discriminants.
Thesyntax-based discriminants are extracted from thederivation trees of the HPSG analyses.
All HPSGrule applications (unary or binary) and choicesof lexical entries are picked as candidate dis-criminants and checked for effectiveness.
Thesemantics-based discriminants, on the other hand,represent the differences on the semantic struc-tures (MRS in the cases of DELPH-IN2 gram-1http://tadm.sourceforge.net/2http://www.delph-in.net/mars).
With a few exceptions, many DELPH-INHPSG treebanks choose to use the syntactic dis-criminants which allow human annotators to pickthe low-level constructions.
The above proposedranking model works for different types of dis-criminants (and potentially a mixture of differentdiscriminant types).
But for the evaluation of thispaper, we show the feature model designed for thesyntactic discriminants only.The syntactic discriminants record the differ-ences between derivation trees by memorizing di-rect rule applications and lexical choices.
Besidethe rule or lexical entry name, the discriminantalso records the information concerning the corre-sponding constituent, e.g., the category and span-ning of the constituent, the parent and daughtersof the constituent, etc.
Furthermore, given the dis-criminant d and the parse forest Y , we can calcu-late the distribution of parses over the value of thediscriminant function ?, which can be character-ized by ?y?Y ?
(y)/|Y |.
This numeric feature in-dicates how many parses can be ruled out with thegiven discriminant.As example, for the highlighted discriminant inFigure 1, the extracted features are listed in Ta-ble 1.4 Experiment SetupTo test the effectiveness of the discriminant rank-ing models, we carried out a series of experi-ments, investigating their effects on both annota-tion speed and quality.
The experiment was donein the context of our ongoing annotation projectof the WSJ sections of the PTB described in Sec-tion 2.
Despite sharing the source of texts, thenew project aims to create an independently an-notated corpus.
Therefore, the trees from the PTBwere not used to guide the disambiguation pro-cess.
In this annotation project, two annotators(both graduate students, referred to as A and Bbelow) are employed to manually disambiguatethe parsing outputs of the ERG.
For quality con-trol and adjudication in case of disagreement, athird linguist/grammarian annotates parts of thetreebank in parallel.With the help of our annotation log files, whichrecord in details the manual decision-making pro-cess, we trained three discriminant ranking mod-1457Feature Possible Values Examplediscriminant type RULE/LEX RULEedge position FULL/FRONT/BACK FULLedge span length(constituent)/length(sentence) 4/4edge category rule or lexical type name SUBJHlevel of discrimination ?y?Y ?
(y)/|Y | 4/15branch splitting length(left-dtr)/length(constituent) 2/4Table 1: Features for syntactic discriminant ranking model and example values for the highlighteddiscriminant in Figure 1els with the datasets completed so far: MODEL-A and MODEL-B trained with annotation logsfrom two annotators separately, and MODEL-BOTH trained jointly with data from both annota-tors.
For each annotator?s model (MODEL-A andMODEL-B), we used about 6,000 disambiguatedparse forests for training.
For each of these 6,000forests, the log file contains about 600,000 effec-tive discriminants, among which only ?6% re-ceived a manual decision.To evaluate the treebanking speed, we havethe annotators work under a distraction-free en-vironment and record their annotation speed.
Thespeed is averaged over several 1-hour annotationsessions.
Different discriminant ranking modelswere used without the annotators being informedof the details of the setting.As testing dataset, we use the texts from thePARC 700 Dependency Bank (King et al, 2003),which include 700 carefully selected sentencesfrom the WSJ sections of the PTB.
These sen-tences were originally chosen for the purpose ofparser evaluation.
Many linguistically challeng-ing phenomena are included in these sentences,although the sentence length is shorter in averagethan the sentence length in the entire WSJ.
Thelanguage is also less related to the financial do-main specific language observed in the WSJ.
Weparsed the dataset with the Feb. 2009 version ofthe ERG, and recorded up to 500 trees per sen-tence (ranked by a MaxEnt parse selection modeltrained on previously treebanked WSJ sections).5 ResultsAlthough we employed a typical statistical rank-ing model in our system, it is difficult to directlyevaluate the absolute performance of the predictedranking.
Annotators only annotate a very smallsubset of the discriminants, and their order is notfully specified.
To compare the behavior of mod-els trained with data annotated by different anno-tators, we plot the relative ranking (normalized to[0, 1] for each sentence, with 0 being the highestrank and 1 the lowest) of discriminants for 50 sen-tences in Figure 2.The plot shows a strong positive linear correla-tion between the two ranking models.
The partic-ularly strong correlation at the low and high endsof the ranking shows that the two annotators sharea similar behavior pattern concerning the most andleast preferred discriminants.
The correlation isslightly weaker in the middle ranking zone, wheredifferent preferences or annotation styles can beobserved.To further visualize the effect of the rankingmodel, we highlighted with color the discrimi-nants which are manually annotated by annotatorB under a basic setting without using the rankingmodels.
75% of these ?prominent?
discriminantsare grouped within the top-25% region of the plot.Without surprise, the model B gives an averagerelative ranking of 0.18 as oppose to 0.21 withmodel A.
The overall distribution of rankings formanually disambiguated discriminants are shownin Figure 3.In Table 2, the average treebanking speed oftwo annotators over multiple annotation sessionsis shown.
The baseline model ranks the discrim-inants by the spanning length of the correspond-ing constituent, and uses the alphabetical orderof the rule or lexical type names as tie-breaker.The own-model refers to the annotation sessionswhich have been carried out by the annotators us-145800.20.40.60.810  0.2  0.4  0.6  0.8  1ModelBRankingModel A RankingFigure 2: Correlation of discriminant ranks withdifferent models and manual annotation01020304050600 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 0.5 0.55 0.6 0.65 0.7 0.75 0.8 0.85 0.9 0.95 1Number of DiscriminantsRelative RankingModel AModel BFigure 3: Histogram of rankings given by twomodels on discriminants manually picked by an-notator Bing their own ranking model.
The peer-modelrefers to the annotation sessions where the annota-tors use their peer colleague?s model.
And finally,the joint-model refers to the annotations done bythe jointly trained model.The annotation efficiency was boosted by over50% with all the discriminant ranking models.The own-model setting achieved best speed.
Thisis probably due to the fact that the model mostclosely reflects the annotation habit of the annota-tor.
But the advantage over other models is verysmall.To measure the inter-annotator agreement, wecalculate the Cohen?s KAPPA (Carletta, 1996) onthe constituents of the derivation trees:?
= Pr(a)?
Pr(e)1?
Pr(e) (2)Ranking Model Speed (s/h) Speed-up (%)Baseline 61.9 ?Own-model 96.1 55%Peer-model 94.6 53%Joint-model 95.0 53%Table 2: Average annotation speed with differentdiscriminant ranking modelswhere Pr(a) is the relative observed agreementbetween annotators, and Pr(e) is the probabilityof two annotators agreeing by chance.
The calcu-lation of Pr(a) can be done in a similar way tothe calculation of PARSEVAL labeled bracketingaccuracy, while Pr(e) is estimated by averagingthe agreement over a large set of tree pairs ran-domly sampled from the parse forest.
Since thecalculation of ?
takes into account the agreementoccurring by chance, it is a safer (though has thetendency of being overly conservative) measure ofagreement.Ranking Model Cohen?s KAPPA (?
)Baseline 0.5404Own-model 0.5798Peer-model 0.5567Joint-model 0.5536Table 3: Inter-annotator agreement measured byconstituent-level Cohen?s KAPPAThe numbers in Table 3 show that the use ofdiscriminant ranking models results in a small im-provement to the inter-annotator agreement, withthe best agreement achieved by each annotator us-ing the model trained with their own annotationrecords.
These numbers are comforting in that theannotation quality is not damaged by our new wayto present the linguistic decisions.Note that the relatively low inter-annotatoragreement in this experiment is due to the fact thatwe used a dataset which involves non-trivial lin-guistic phenomena that are on average more dif-ficult than the texts in the WSJ corpus.
Anotherfact is that these annotations were done under timepressure.
The annotators are not encouraged to gobackwards to check and correct the previous sen-tences during these sessions.
On the entire WSJ,we have recorded a stable and persistently higher1459agreement level at ?
= 0.6.
Given the highly de-tailed linguistic annotations specified by the gram-mar (over 260 rules and 800 lexical types), thisfigure indicates a very substantial agreement be-tween our annotators.
Our further investigationhas shown that the agreement figure hits the ceil-ing at around ?
= 0.65.
Further training and dis-cussion is not rewarded with sustainable improve-ment of annotation quality.Apart from the numerical evaluation, we alsointerview our annotators for subjective feelingsabout the various ranking models.
There is gen-erally a very positive attitude towards all the rank-ing models over the baseline.
An easily decid-able discriminant is usually found within the top-3with very few exceptions, which leads to a self-noticeable speed-up that confirms our numericfindings.
It is also interesting to note that, de-spite the substantial difference between the statis-tical models, the difference is hardly noticed bythe annotators.
And the results only show smallvariations in both the annotation speed, as well asthe inter-annotator agreement.The annotators also claim that the speed-upis somewhat diminished over the ?rejected?
sen-tences, for which none of the candidate trees areacceptable.
In such cases, the annotators still haveto go through a long sequence of discriminants,and sometimes have to redo the previous steps infear of the chain-effect of wrong decisions.
Howto compensate for the psychological insatisfactionof rejecting all analyses while maintaining goodannotation speed and quality is a new topic for ourfuture research.6 Conclusion & Future WorkWe propose to use a statistical ranking model toassist the discriminant-based treebank annotation.Our experiment shows that such a model, trainedon annotation history, brings a huge efficiency im-provement together with slightly improved inter-annotator agreement.Although the reported experiments were car-ried out on the specific HPSG treebank, we be-lieve that the proposed ranked discriminant-basedannotation method can be applied in annotationtasks concerning different linguistic frameworks,or even different layers of linguistic representa-tion.
Apart from the specific features presentedin Section 3.3, the model itself does not assume aphrase-structure tree annotation, and the discrimi-nants can take various forms.
Assuming a ?gram-mar?
produces a number of candidate analyses,the annotators can rely on the ranking model to ef-ficiently pick relevant discriminants, and focus onmaking linguistically relevant decisions.
This isespecially suitable for large annotation tasks aim-ing for parallel rich annotation by multiple anno-tators, where fully manual annotation is not fea-sible and high inter-annotator agreement hard toachieve.The ranking model is based on annotation his-tory and influences the future progress of tree-banking.
It can be dynamically integrated into thetreebank development cycles in which the anno-tation habit evolves over time.
Such a model canalso shorten the training period for new annota-tors, which is an interesting aspect for our futureinvestigation.From a different point of view, the rankings ofthe discriminants show annotators?
confidence onvarious ambiguities.
The clearly uneven distri-bution over discriminants can also provide gram-mar writers with interesting feedback, helpingwith the improvement of the linguistic analysis.We would also like to integrate confidence mea-sures into the computer-assisted treebank annota-tion process, which could potentially help annota-tors make difficult decisions, such as whether toreject all trees for a sentence.References[Brants et al2002] Brants, Sabine, Stefanie Dipper,Silvia Hansen, Wolfgang Lezius, and George Smith.2002.
The tiger treebank.
In Proceedings ofthe workshop on treebanks and linguistic theories,pages 24?41.
[Callmeier2001] Callmeier, Ulrich.
2001.
Effi-cient parsing with large-scale unification gram-mars.
Master?s thesis, Universita?t des Saarlandes,Saarbru?cken, Germany.
[Carletta1996] Carletta, Jean.
1996.
Assessing agree-ment on classification tasks: the kappa statistic.Computational Linguistics, 22(2):249?254.
[Carter1997] Carter, David.
1997.
The treebanker: atool for supervised training of parsed corpora.
In1460Proceedings of the ACL Workshop on Computa-tional Environments for Grammar Development andLinguistic Engineering, pages 9?15, Madrid, Spain.
[Copestake et al2005] Copestake, Ann, DanFlickinger, Carl J. Pollard, and Ivan A. Sag.2005.
Minimal recursion semantics: an introduc-tion.
Research on Language and Computation,3(4):281?332.
[Flickinger2002] Flickinger, Dan.
2002.
On build-ing a more efficient grammar by exploiting types.In Oepen, Stephan, Dan Flickinger, Jun?ichi Tsu-jii, and Hans Uszkoreit, editors, Collaborative Lan-guage Engineering, pages 1?17.
CSLI Publications.[Hajic?
et al2000] Hajic?, Jan, Alena Bo?hmova?, EvaHajic?ova?, and Barbora Vidova?-Hladka?.
2000.
ThePrague Dependency Treebank: A Three-Level An-notation Scenario.
In Abeille?, A., editor, Treebanks:Building and Using Parsed Corpora, pages 103?127.
Amsterdam:Kluwer.
[King et al2003] King, Tracy H., Richard Crouch, Ste-fan Riezler, Mary Dalrymple, and Ronald M. Ka-plan.
2003.
The PARC 700 Dependency Bank.
InProceedings of the 4th International Workshop onLinguistically Interpreted Corpora, held at the 10thConference of the European Chapter of the Associa-tion for Computational Linguistics (EACL?03), Bu-dapest, Hungary.
[Kordoni and Zhang2009] Kordoni, Valia andYi Zhang.
2009.
Annotating wall street jour-nal texts using a hand-crafted deep linguisticgrammar.
In Proceedings of The Third LinguisticAnnotation Workshop (LAW III), Singapore.
[Marcus et al1993] Marcus, Mitchell P., Beatrice San-torini, and Mary Ann Marcinkiewicz.
1993.
Build-ing a large annotated corpus of english: The penntreebank.
Computational Linguistics, 19(2):313?330.
[Oepen et al2002] Oepen, Stephan, KristinaToutanova, Stuart Shieber, Christopher Man-ning, Dan Flickinger, and Thorsten Brants.
2002.The LinGO Redwoods treebank: motivation andpreliminary applications.
In Proceedings of COL-ING 2002: The 17th International Conference onComputational Linguistics: Project Notes, Taipei,Taiwan.
[Oepen2001] Oepen, Stephan.
2001.
[incr tsdb()] ?competence and performance laboratory.
User man-ual.
Technical report, Computational Linguistics,Saarland University, Saarbru?cken, Germany.
[Pollard and Sag1994] Pollard, Carl J. and Ivan A. Sag.1994.
Head-Driven Phrase Structure Grammar.University of Chicago Press, Chicago, USA.1461
