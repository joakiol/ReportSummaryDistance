Proceedings of the ACL Workshop on Building and Using Parallel Texts, pages 9?16,Ann Arbor, June 2005. c?Association for Computational Linguistics, 2005Cross language Text Categorization by acquiringMultilingual Domain Models from Comparable CorporaAlfio Gliozzo and Carlo StrapparavaITC-Irstvia Sommarive, I-38050, Trento, ITALY{gliozzo,strappa}@itc.itAbstractIn a multilingual scenario, the classicalmonolingual text categorization problemcan be reformulated as a cross languageTC task, in which we have to cope withtwo or more languages (e.g.
English andItalian).
In this setting, the system istrained using labeled examples in a sourcelanguage (e.g.
English), and it classifiesdocuments in a different target language(e.g.
Italian).In this paper we propose a novel ap-proach to solve the cross language textcategorization problem based on acquir-ing Multilingual Domain Models fromcomparable corpora in a totally unsuper-vised way and without using any externalknowledge source (e.g.
bilingual dictio-naries).
These Multilingual Domain Mod-els are exploited to define a generalizedsimilarity function (i.e.
a kernel function)among documents in different languages,which is used inside a Support Vector Ma-chines classification framework.
The re-sults show that our approach is a feasi-ble and cheap solution that largely outper-forms a baseline.1 IntroductionText categorization (TC) is the task of assigning cat-egory labels to documents.
Categories are usuallydefined according to a variety of topics (e.g.
SPORT,POLITICS, etc.)
and, even if a large amount ofhand tagged texts is required, the state-of-the-art su-pervised learning techniques represent a viable andwell-performing solution for monolingual catego-rization problems.On the other hand in the worldwide scenario ofthe web age, multilinguality is a crucial issue to dealwith and to investigate, leading us to reformulatemost of the classical NLP problems.
In particular,monolingual Text Categorization can be reformu-lated as a cross language TC task, in which we haveto cope with two or more languages (e.g.
Englishand Italian).
In this setting, the system is trainedusing labeled examples in a source language (e.g.English), and it classifies documents in a differenttarget language (e.g.
Italian).In this paper we propose a novel approach to solvethe cross language text categorization problem basedon acquiring Multilingual Domain Models (MDM)from comparable corpora in an unsupervised way.A MDM is a set of clusters formed by terms in dif-ferent languages.
While in the monolingual settingssemantic domains are clusters of related terms thatco-occur in texts regarding similar topics (Gliozzo etal., 2004), in the multilingual settings such clustersare composed by terms in different languages ex-pressing concepts in the same semantic field.
Thus,the basic relation modeled by a MDM is the domainsimilarity among terms in different languages.
Ourclaim is that such a relation is sufficient to capturerelevant aspects of topic similarity that can be prof-itably used for TC purposes.The paper is organized as follows.
After a briefdiscussion about comparable corpora, we introduce9a multilingual Vector Space Model, in which docu-ments in different languages can be represented andthen compared.
In Section 4 we define the MDMsand we present a totally unsupervised techniqueto acquire them from comparable corpora.
Thismethodology does not require any external knowl-edge source (e.g.
bilingual dictionaries) and it isbased on Latent Semantic Analysis (LSA) (Deer-wester et al, 1990).
MDMs are then exploited todefine a Multilingual Domain Kernel, a generalizedsimilarity function among documents in differentlanguages that exploits a MDM (see Section 5).
TheMultilingual Domain Kernel is used inside a Sup-port Vector Machines (SVM) classification frame-work for TC (Joachims, 2002).
In Section 6 we willevaluate our technique in a Cross Language catego-rization task.
The results show that our approach isa feasible and cheap solution, largely outperforminga baseline.
Conclusions and future works are finallyreported in Section 7.2 Comparable CorporaComparable corpora are collections of texts in dif-ferent languages regarding similar topics (e.g.
a col-lection of news published by agencies in the sameperiod).
More restrictive requirements are expectedfor parallel corpora (i.e.
corpora composed by textswhich are mutual translations), while the class ofthe multilingual corpora (i.e.
collection of texts ex-pressed in different languages without any addi-tional requirement) is the more general.
Obviouslyparallel corpora are also comparable, while compa-rable corpora are also multilingual.In a more precise way, let L = {L1, L2, .
.
.
, Ll}be a set of languages, let T i = {ti1, ti2, .
.
.
, tin} be acollection of texts expressed in the language Li ?
L,and let ?
(tjh, tiz) be a function that returns 1 if tiz isthe translation of tjh and 0 otherwise.
A multilingualcorpus is the collection of texts defined by T ?
=?i T i.
If the function ?
exists for every text tiz ?
T ?and for every language Lj , and is known, then thecorpus is parallel and aligned at document level.For the purpose of this paper it is enough to as-sume that two corpora are comparable, i.e.
they arecomposed by documents about the same topics andproduced in the same period (e.g.
possibly from dif-ferent news agencies), and it is not known if a func-tion ?
exists, even if in principle it could exist andreturn 1 for a strict subset of document pairs.There exist many interesting works about us-ing parallel corpora for multilingual applications(Melamed, 2001), such as Machine Translation,Cross language Information Retrieval (Littman etal., 1998), lexical acquisition, and so on.However it is not always easy to find or build par-allel corpora.
This is the main reason because theweaker notion of comparable corpora is a matter re-cent interest in the field of Computational Linguis-tics (Gaussier et al, 2004).The texts inside comparable corpora, being aboutthe same topics (i.e.
about the same semantic do-mains), should refer to the same concepts by usingvarious expressions in different languages.
On theother hand, most of the proper nouns, relevant enti-ties and words that are not yet lexicalized in the lan-guage, are expressed by using their original terms.As a consequence the same entities will be denotedwith the same words in different languages, allow-ing to automatically detect couples of translationpairs just by looking at the word shape (Koehn andKnight, 2002).
Our hypothesis is that comparablecorpora contain a large amount of such words, justbecause texts, referring to the same topics in differ-ent languages, will often adopt the same terms todenote the same entities1 .However, the simple presence of these sharedwords is not enough to get significant results in TCtasks.
As we will see, we need to exploit these com-mon words to induce a second-order similarity forthe other words in the lexicons.3 The Multilingual Vector Space ModelLet T = {t1, t2, .
.
.
, tn} be a corpus, and V ={w1, w2, .
.
.
, wk} be its vocabulary.
In the mono-lingual settings, the Vector Space Model (VSM) is ak-dimensional space Rk, in which the text tj ?
Tis represented by means of the vector ~tj such thatthe zth component of ~tj is the frequency of wz in tj .The similarity among two texts in the VSM is thenestimated by computing the cosine of their vectorsin the VSM.1According to our assumption, a possible additional crite-rion to decide whether two corpora are comparable is to esti-mate the percentage of terms in the intersection of their vocab-ularies.10Unfortunately, such a model cannot be adopted inthe multilingual settings, because the VSMs of dif-ferent languages are mainly disjoint, and the similar-ity between two texts in different languages wouldalways turn out zero.
This situation is representedin Figure 1, in which both the left-bottom and therigth-upper regions of the matrix are totally filled byzeros.A first attempt to solve this problem is to ex-ploit the information provided by external knowl-edge sources, such as bilingual dictionaries, to col-lapse all the rows representing translation pairs.
Inthis setting, the similarity among texts in differentlanguages could be estimated by exploiting the clas-sical VSM just described.
However, the main dis-advantage of this approach to estimate inter-lingualtext similarity is that it strongly relies on the avail-ability of a multilingual lexical resource containinga list of translation pairs.
For languages with scarceresources a bilingual dictionary could be not eas-ily available.
Secondly, an important requirementof such a resource is its coverage (i.e.
the amountof possible translation pairs that are actually con-tained in it).
Finally, another problem is that am-biguos terms could be translated in different ways,leading to collapse together rows describing termswith very different meanings.On the other hand, the assumption of corporacomparability seen in Section 2, implies the pres-ence of a number of common words, represented bythe central rows of the matrix in Figure 1.As we will show in Section 6, this model is ratherpoor because of its sparseness.
In the next section,we will show how to use such words as seeds to in-duce a Multilingual Domain VSM, in which secondorder relations among terms and documents in dif-ferent languages are considered to improve the sim-ilarity estimation.4 Multilingual Domain ModelsA MDM is a multilingual extension of the conceptof Domain Model.
In the literature, Domain Mod-els have been introduced to represent ambiguity andvariability (Gliozzo et al, 2004) and successfullyexploited in many NLP applications, such us WordSense Disambiguation (Strapparava et al, 2004),Text Categorization and Term Categorization.A Domain Model is composed by soft clusters ofterms.
Each cluster represents a semantic domain,i.e.
a set of terms that often co-occur in texts hav-ing similar topics.
Such clusters identifies groups ofwords belonging to the same semantic field, and thushighly paradigmatically related.
MDMs are DomainModels containing terms in more than one language.A MDM is represented by a matrix D, contain-ing the degree of association among terms in all thelanguages and domains, as illustrated in Table 1.MEDICINE COMPUTER SCIENCEHIV e/i 1 0AIDSe/i 1 0viruse/i 0.5 0.5hospitale 1 0laptope 0 1Microsofte/i 0 1clinicai 1 0Table 1: Example of Domain Matrix.
we denotesEnglish terms, wi Italian terms and we/i the com-mon terms to both languages.MDMs can be used to describe lexical ambiguity,variability and inter-lingual domain relations.
Lexi-cal ambiguity is represented by associating one termto more than one domain, while variability is rep-resented by associating different terms to the samedomain.
For example the term virus is associatedto both the domain COMPUTER SCIENCE and thedomain MEDICINE while the domain MEDICINE isassociated to both the terms AIDS and HIV.
Inter-lingual domain relations are captured by placing dif-ferent terms of different languages in the same se-mantic field (as for example HIV e/i, AIDSe/i,hospitale, and clinicai).
Most of the named enti-ties, such as Microsoft and HIV are expressed usingthe same string in both languages.When similarity among texts in different lan-guages has to be estimated, the information con-tained in the MDM is crucial.
For example the twosentences ?I went to the hospital to make an HIVcheck?
and ?Ieri ho fatto il test dell?AIDS in clin-ica?
(lit.
yesterday I did the AIDS test in a clinic)are very highly related, even if they share no to-kens.
Having an ?a priori?
knowledge about theinter-lingual domain similarity among AIDS, HIV,hospital and clinica is then a useful information to11????????????????????????????????
?English documents Italian documentsde1 de2 ?
?
?
den?1 den di1 di2 ?
?
?
dim?1 dimwe1 0 1 ?
?
?
0 1 0 0 ?
?
?EnglishLexiconwe2 1 1 ?
?
?
1 0 0. .
.... .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
... 0 ...wep?1 0 1 ?
?
?
0 0. .
.
0wep 0 1 ?
?
?
0 0 ?
?
?
0 0common wi we/i1 0 1 ?
?
?
0 0 0 0 ?
?
?
1 0... .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.wi1 0 0 ?
?
?
0 1 ?
?
?
1 1ItalianLexiconwi2 0. .
.
1 1 ?
?
?
0 1... ... 0 ... .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.wiq?1.
.
.
0 0 1 ?
?
?
0 1wiq ?
?
?
0 0 0 1 ?
?
?
1 0????????????????????????????????
?Figure 1: Multilingual term-by-document matrixrecognize inter-lingual topic similarity.
Obviouslythis relation is less restrictive than a stronger associ-ation among translation pair.
In this paper we willshow that such a representation is sufficient for TCpuposes, and easier to acquire.In the rest of this section we will provide a formaldefinition of the concept of MDM, and we definesome similarity metrics that exploit it.Formally, let V i = {wi1, wi2, .
.
.
, wiki} be the vo-cabulary of the corpus T i composed by documentexpressed in the language Li, let V ?
= ?i V i bethe set of all the terms in all the languages, andlet k?
= |V ?| be the cardinality of this set.
LetD = {D1, D2, ..., Dd} be a set of domains.
A DMis fully defined by a k?
?
d domain matrix D rep-resenting in each cell di,z the domain relevance ofthe ith term of V ?
with respect to the domain Dz .The domain matrix D is used to define a functionD : Rk?
?
Rd, that maps the document vectors ~tjexpressed into the multilingual classical VSM, intothe vectors ~t?j in the multilingual domain VSM.
Thefunction D is defined by22In (Wong et al, 1985) the formula 1 is used to define aGeneralized Vector Space Model, of which the Domain VSM isa particular instance.D(~tj) = ~tj(IIDFD) = ~t?j (1)where IIDF is a diagonal matrix such that iIDFi,i =IDF (wli), ~tj is represented as a row vector, andIDF (wli) is the Inverse Document Frequency of wlievaluated in the corpus T l.The matrix D can be determined for example us-ing hand-made lexical resources, such as WORD-NET DOMAINS (Magnini and Cavaglia`, 2000).
Inthe present work we followed the way to acquireD automatically from corpora, exploiting the tech-nique described below.4.1 Automatic Acquisition of MultilingualDomain ModelsIn this work we propose the use of Latent Seman-tic Analysis (LSA) (Deerwester et al, 1990) to in-duce a MDM from comparable corpora.
LSA is anunsupervised technique for estimating the similar-ity among texts and terms in a large corpus.
In themonolingual settings LSA is performed by meansof a Singular Value Decomposition (SVD) of theterm-by-document matrix T describing the corpus.SVD decomposes the term-by-document matrix Tinto three matrixes T ' V?k?UT where ?k?
is thediagonal k?k matrix containing the highest k ?
k12eigenvalues of T, and all the remaining elements areset to 0.
The parameter k?
is the dimensionality ofthe Domain VSM and can be fixed in advance (i.e.k?
= d).In the literature (Littman et al, 1998) LSA hasbeen used in multilingual settings to define a mul-tilingual space in which texts in different languagescan be represented and compared.
In that work LSAstrongly relied on the availability of aligned parallelcorpora: documents in all the languages are repre-sented in a term-by-document matrix (see Figure 1)and then the columns corresponding to sets of trans-lated documents are collapsed (i.e.
they are substi-tuted by their sum) before starting the LSA process.The effect of this step is to merge the subspaces (i.e.the right and the left sectors of the matrix in Figure1) in which the documents have been originally rep-resented.In this paper we propose a variation of this strat-egy, performing a multilingual LSA in the case inwhich an aligned parallel corpus is not available.It exploits the presence of common words amongdifferent languages in the term-by-document matrix.The SVD process has the effect of creating a LSAspace in which documents in both languages are rep-resented.
Of course, the higher the number of com-mon words, the more information will be providedto the SVD algorithm to find common LSA dimen-sion for the two languages.
The resulting LSA di-mensions can be perceived as multilingual clustersof terms and document.
LSA can then be used todefine a Multilingual Domain Matrix DLSA.DLSA = INV??k?
(2)where IN is a diagonal matrix such that iNi,i =1??
~w?i, ~w?i?, ~w?i is the ith row of the matrix V??k?
.Thus DLSA3 can be exploited to estimate simi-larity among texts expressed in different languages(see Section 5).3When DLSA is substituted in Equation 1 the Domain VSMis equivalent to a Latent Semantic Space (Deerwester et al,1990).
The only difference in our formulation is that the vectorsrepresenting the terms in the Domain VSM are normalized bythe matrix IN, and then rescaled, according to their IDF value,by matrix IIDF.
Note the analogy with the tf idf term weightingschema, widely adopted in Information Retrieval.4.2 Similarity in the multilingual domain spaceAs an example of the second-order similarity pro-vided by this approach, we can see in Table 2 the fivemost similar terms to the lemma bank.
The similar-ity among terms is calculated by cosine among therows in the matrix DLSA, acquired from the dataset used in our experiments (see Section 6.2).
It isworth noting that the Italian lemma banca (i.e.
bankin English) has a high similarity score to the Englishlemma bank.
While this is not enough to have a pre-cise term translation, it is sufficient to capture rele-vant aspects of topic similarity in a cross-languagetext categorization task.Lemma#Pos Similarity Score Languagebanking#n 0.96 Engcredit#n 0.90 Engamro#n 0.89 Engunicredito#n 0.85 Itabanca#n 0.83 ItaTable 2: Terms with high similarity to the Englishlemma bank#n, in the Multilingual Domain Model5 The Multilingual Domain KernelKernel Methods are the state-of-the-art supervisedframework for learning, and they have been success-fully adopted to approach the TC task (Joachims,2002).The basic idea behind kernel methods is to em-bed the data into a suitable feature space F via amapping function ?
: X ?
F , and then to use alinear algorithm for discovering nonlinear patterns.Kernel methods allow us to build a modular system,as the kernel function acts as an interface betweenthe data and the learning algorithm.
Thus the ker-nel function becomes the only domain specific mod-ule of the system, while the learning algorithm is ageneral purpose component.
Potentially any kernelfunction can work with any kernel-based algorithm,as for example Support Vector Machines (SVMs).During the learning phase SVMs assign a weight?i ?
0 to any example xi ?
X .
All the labeledinstances xi such that ?i > 0 are called SupportVectors.
Support Vectors lie close to the best sepa-rating hyper-plane between positive and negative ex-amples.
New examples are then assigned to the class13of the closest support vectors, according to equation3.f(x) =n?i=1?iK(xi, x) + ?0 (3)The kernel function K(xi, x) returns the simi-larity between two instances in the input space X ,and can be designed just by taking care that someformal requirements are satisfied, as described in(Scho?lkopf and Smola, 2001).In this section we define the Multilingual DomainKernel, and we apply it to a cross language TC task.This kernel can be exploited to estimate the topicsimilarity among two texts expressed in differentlanguages by taking into account the external knowl-edge provided by a MDM.
It defines an explicit map-ping D : Rk ?
Rk?
from the Multilingual VSMinto the Multilingual Domain VSM.
The Multilin-gual Domain Kernel is specified byKD(ti, tj) =?D(ti),D(tj)???D(tj),D(tj)??D(ti),D(ti)?
(4)where D is the Domain Mapping defined in equa-tion 1.
Thus the Multilingual Domain Kernel re-quires Multilingual Domain Matrix D, in particularDLSA that can be acquired from comparable cor-pora, as explained in Section 4.1.To evaluate the Multilingual Domain Kernel wecompared it to a baseline kernel function, namely thebag of words kernel, that simply estimates the topicsimilarity in the Multilingual VSM, as described inSection 3.
The BoW kernel is a particular case ofthe Domain Kernel, in which D = I, and I is theidentity matrix.6 EvaluationIn this section we present the data set (two compara-ble English and Italian corpora) used in the evalua-tion, and we show the results of the Cross LanguageTC tasks.
In particular we tried both to train thesystem on the English data set and classify Italiandocuments and to train using Italian and classify theEnglish test set.
We compare the learning curves ofthe Multilingual Domain Kernel with the standardBoW kernel, which is considered as a baseline forthis task.6.1 Implementation detailsAs a supervised learning device, we used the SVMimplementation described in (Joachims, 1999).
TheMultilingual Domain Kernel is implemented bydefining an explicit feature mapping as explainedabove, and by normalizing each vector.
All the ex-periments have been performed with the standardSVM parameter settings.We acquired a Multilingual Domain Model byperforming the Singular Value Decomposition pro-cess on the term-by-document matrices representingthe merged training partitions (i.e.
English and Ital-ian), and we considered only the first 400 dimen-sions4.6.2 Data set descriptionWe used a news corpus kindly put at our dis-posal by ADNKRONOS, an important Italian newsprovider.
The corpus consists of 32,354 Ital-ian and 27,821 English news partitioned byADNKRONOS in a number of four fixed cate-gories: Quality of Life, Made in Italy,Tourism, Culture and School.
The corpusis comparable, in the sense stated in Section 2, i.e.they covered the same topics and the same period oftime.
Some news are translated in the other language(but no alignment indication is given), some othersare present only in the English set, and some othersonly in the Italian.
The average length of the newsis about 300 words.
We randomly split both the En-glish and Italian part into 75% training and 25% test(see Table 3).
In both the data sets we postagged thetexts and we considered only the noun, verb, adjec-tive, and adverb parts of speech, representing themby vectors containing the frequencies of each lemmawith its part of speech.6.3 Monolingual ResultsBefore going to a cross-language TC task, we con-ducted two tests of classical monolingual TC bytraining and testing the system on Italian and En-glish documents separately.
For these tests we usedthe SVM with the BoW kernel.
Figures 2 and 3 re-port the results.4To perform the SVD operation we used LIBSVDChttp://tedlab.mit.edu/?dr/SVDLIBC/.14English ItalianCategories Training Test Total Training Test TotalQuality of Life 5759 1989 7748 5781 1901 7682Made in Italy 5711 1864 7575 6111 2068 8179Tourism 5731 1857 7588 6090 2015 8105Culture and School 3665 1245 4910 6284 2104 8388Total 20866 6955 27821 24266 8088 32354Table 3: Number of documents in the data set partitions0.50.550.60.650.70.750.80.850.90.9510 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1F1measureFraction of training dataBoW KernelFigure 2: Learning curves for the English part of thecorpus0.50.550.60.650.70.750.80.850.90.9510 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1F1measureFraction of training dataBoW KernelFigure 3: Learning curves for the Italian part of thecorpus6.4 A Cross Language Text Categorization taskAs far as the cross language TC task is concerned,we tried the two possible options: we trained on theEnglish part and we classified the Italian part, andwe trained on the Italian and classified on the En-0.150.20.250.30.350.40.450.50.550.60 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1F1measureFraction of training dataMultilingual Domain KernelBow KernelFigure 4: Cross-language (training on Italian, test onEnglish) learning curves0.250.30.350.40.450.50.550.60.650.70 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1F1measureFraction of training dataMultilingual Domain KernelBow KernelFigure 5: Cross-language (training on English, teston Italian) learning curvesglish part.
The Multilingual Domain Model was ac-quired running the SVD only on the joint (Englishand Italian) training parts.Table 4 reports the vocabulary dimensions of theEnglish and Italian training partitions, the vocabu-15# lemmataEnglish training 22,704Italian training 26,404English + Italian 43,384common lemmata 5,724Table 4: Number of lemmata in the training parts ofthe corpuslary of the merged training, and how many com-mon lemmata are present (about 14% of the total).Among the common lemmata, 97% are nouns andmost of them are proper nouns.
Thus the initial term-by-document matrix is a 43,384 ?
45,132 matrix,while the DLSA matrix is 43,384 ?
400.
For thistask we consider as a baseline the BoW kernel.The results are reported in Figures 4 and 5.
An-alyzing the learning curves, it is worth noting thatwhen the quantity of training increases, the per-formance becomes better and better for the Multi-lingual Domain Kernel, suggesting that with moreavailable training it could be possible to go closer totypical monolingual TC results.7 ConclusionIn this paper we proposed a solution to cross lan-guage Text Categorization based on acquiring Mul-tilingual Domain Models from comparable corporain a totally unsupervised way and without using anyexternal knowledge source (e.g.
bilingual dictionar-ies).
These Multilingual Domain Models are ex-ploited to define a generalized similarity function(i.e.
a kernel function) among documents in differ-ent languages, which is used inside a Support Vec-tor Machines classification framework.
The basis ofthe similarity function exploits the presence of com-mon words to induce a second-order similarity forthe other words in the lexicons.
The results haveshown that this technique is sufficient to capture rel-evant aspects of topic similarity in cross-languageTC tasks, obtaining substantial improvements overa simple baseline.
As future work we will investi-gate the performance of this approach to more thantwo languages TC task, and a possible generaliza-tion of the assumption about equality of the commonwords.AcknowledgmentsThis work has been partially supported by theONTOTEXT project, funded by the AutonomousProvince of Trento under the FUP-2004 program.ReferencesS.
Deerwester, S. T. Dumais, G. W. Furnas, T.K.
Lan-dauer, and R. Harshman.
1990.
Indexing by latent se-mantic analysis.
Journal of the American Society forInformation Science, 41(6):391?407.E.
Gaussier, J. M. Renders, I. Matveeva, C. Goutte, andH.
Dejean.
2004.
A geometric view on bilingual lexi-con extraction from comparable corpora.
In Proceed-ings of ACL-04, Barcelona, Spain, July.A.
Gliozzo, C. Strapparava, and I. Dagan.
2004.
Unsu-pervised and supervised exploitation of semantic do-mains in lexical disambiguation.
Computer Speechand Language, 18:275?299.T.
Joachims.
1999.
Making large-scale SVM learningpractical.
In B. Scho?lkopf, C. Burges, and A. Smola,editors, Advances in kernel methods: support vectorlearning, chapter 11, pages 169 ?
184.
The MIT Press.T.
Joachims.
2002.
Learning to Classify Text using Sup-port Vector Machines.
Kluwer Academic Publishers.P.
Koehn and K. Knight.
2002.
Learning a translationlexicon from monolingual corpora.
In Proceedings ofACL Workshop on Unsupervised Lexical Acquisition,Philadelphia, July.M.
Littman, S. Dumais, and T. Landauer.
1998.
Auto-matic cross-language information retrieval using latentsemantic indexing.
In G. Grefenstette, editor, CrossLanguage Information Retrieval, pages 51?62.
KluwerAcademic Publishers.B.
Magnini and G. Cavaglia`.
2000.
Integrating subjectfield codes into WordNet.
In Proceedings of LREC-2000, Athens, Greece, June.D.
Melamed.
2001.
Empirical Methods for ExploitingParallel Texts.
The MIT Press.B.
Scho?lkopf and A. J. Smola.
2001.
Learning with Ker-nels.
Support Vector Machines, Regularization, Opti-mization, and Beyond.
The MIT Press.C.
Strapparava, A. Gliozzo, and C. Giuliano.
2004.
Pat-tern abstraction and term similarity for word sensedisambiguation.
In Proceedings of SENSEVAL-3,Barcelona, Spain, July.S.K.M.
Wong, W. Ziarko, and P.C.N.
Wong.
1985.
Gen-eralized vector space model in information retrieval.In Proceedings of the 8th ACM SIGIR Conference.16
