Proceedings of the Sixth Workshop on Innovative Use of NLP for Building Educational Applications, pages 87?95,Portland, Oregon, 24 June 2011. c?2011 Association for Computational LinguisticsMeasuring Language Development in Early Childhood Education: A CaseStudy of Grammar Checking in Child Language TranscriptsKhairun-nisa HassanaliComputer Science DepartmentThe University of Texas at DallasRichardson, TX, USAnisa@hlt.utdallas.eduYang LiuComputer Science DepartmentThe University of Texas at DallasRichardson, TX, USAyangl@hlt.utdallas.eduAbstractLanguage sample analysis is an importanttechnique used in measuring language devel-opment.
At present, measures of grammati-cal complexity such as the Index of ProductiveSyntax (Scarborough, 1990) are used to mea-sure language development in early childhood.Although these measures depict the overallcompetence in the usage of language, they donot provide for an analysis of the grammati-cal mistakes made by the child.
In this paper,we explore the use of existing Natural Lan-guage Processing (NLP) techniques to providean insight into the processing of child lan-guage transcripts and challenges in automaticgrammar checking.
We explore the automaticdetection of 6 types of verb related grammat-ical errors.
We compare rule based systemsto statistical systems and investigate the useof different features.
We found the statisticalsystems performed better than the rule basedsystems for most of the error categories.1 IntroductionAutomatic grammar checking and correction hasbeen used extensively in several applications.
Onesuch application is in word processors where theuser is notified of a potential ungrammatical sen-tence.
This feature makes it easier for the users todetect and correct ungrammatical sentences.
Au-tomatic grammar checking can also be beneficialin language learning where students are given sug-gestions on potential grammatical errors (Lee andSeneff, 2006).
Another application of grammarchecking is in improving a parser?s performance forungrammatical sentences.
Since most parsers aretrained on written data consisting mostly of gram-matical sentences, the parsers face issues when pars-ing ungrammatical sentences.
Automatic detectionand correction of these ungrammatical sentenceswould improve the parser?s performance by detect-ing the ungrammatical sentences and performinga second parse on the corrected sentences (Cainesand Buttery, 2010).
From an education perspective,measuring language skills has been extensively ex-plored.
There are systems in place that automaticallydetect and correct errors for second language learn-ers (Eeg-Olofsson and Knuttson, 2003; Leacock etal., 2010).One method used in measuring language devel-opment is the analysis of transcripts of child lan-guage speech.
Child language transcripts are sam-ples of a child?s utterances during a specified pe-riod of time.
Educators and speech language pathol-ogists use these samples to measure language de-velopment.
In particular, speech language pathol-ogists score these transcripts for grammatical mea-sures of complexity amidst other measures.
Sincemanual analysis of transcripts is time consuming,many of these grammatical complexity measures re-quire the speech language pathologists to look forjust a few examples.
The Index of Productive Syn-tax (IPSyn) (Scarborough, 1990) is one such mea-sure of morphological and syntactic structure devel-oped for measuring language samples of preschoolchildren.
The advantage of measures such as IPSynis that they give a single score that can be used toholistically measure language development.
How-ever, they focus on grammatical constructs that the87child uses correctly and do not take into accountthe number and type of grammatical errors that aremade by the child.Educators wishing to measure language develop-ment and competence in a child will benefit fromhaving access to the grammatical errors made by achild.
Analysis of these grammatical errors will en-able educators and speech language pathologists toidentify shortcomings in the child?s language andrecommend intervention techniques customized tothe child.
Since manual identification of grammat-ical errors is both cumbersome and time consum-ing, a tool that automatically does grammar check-ing would be of great use to clinicians.
Addition-ally, we see several uses of automatic grammar de-tection.
For example, we can use the statistics ofgrammatical errors as features in building classifiersthat predict language impairment.
Furthermore, wecould also use the statistics of these grammatical er-rors to come up with a measure of language develop-ment that takes into account both grammatical com-petence and grammatical deficiencies.In this paper, we use existing NLP techniques toautomatically detect grammatical errors from childlanguage transcripts.
Since children with LanguageImpairment (LI) have a greater problem with correctusage of verbs compared to Typically Developing(TD) children (Rice et al, 1995), we focus mainlyon verb related errors.
We compare rule based sys-tems to statistical systems and investigate the useof different features.
We found the statistical sys-tems performed better than the rule based systemsfor most error categories.2 Related WorkWhile there has been considerable work (Sagae etal., 2007) done on annotating child language tran-scripts for grammatical relations, as far as we know,there has been no work done on automatic gram-mar checking of child language transcripts.
Mostof the existing work in automatic grammar check-ing has been done on written text.
Spoken languageon the other hand, presents challenges such as dis-fluencies and false restarts which are not present inwritten text.
We believe that the specific researchchallenges that are encountered in detecting and cor-recting child language transcripts warrant a more de-tailed examination.Caines and Buttery (2010) focused on identify-ing sentences with the missing auxiliary verb in theprogressive aspect constructions.
They used logisticregression to predict the presence of zero auxiliaryoccurrence in the spoken British National Corpus(BNC).
An example of a zero auxiliary constructionis ?You talking to me??.
They first identified con-structions with the progressive aspect and annotatedthe constructions for the following features: sub-ject person, subject case, perfect aspect, presence ofnegation and use of pronouns.
Their model identi-fied zero auxiliary constructions with 96.9% accu-racy.
They also demonstrated how their model canbe integrated into existing parsing tools, thereby in-creasing the number of successful parses for zeroauxiliary constructions by 30%.Lee and Seneff (2008) described a system for verberror correction using template matching on parsetrees in two ways.
Their work focused on correct-ing the error types related to subject-verb agreement,auxiliary agreement and complementation.
Theyconsidered the irregularities in parse trees causedby verb error forms and used n-gram counts to fil-ter proposed corrections.
They used the AQUAINTCorpus of English News Text to detect the irregular-ities in the parse trees caused by verb error forms.They reported an accuracy of 98.93% for verb er-rors related to subject-verb agreement, and 98.94%for verb errors related to auxiliary agreement andcomplementation.
Bowden and Fox (2002) devel-oped a system to detect and explain errors made bynon-native English speakers.
They used classifica-tion and pattern matching rules instead of thoroughparsing.
Their system searched for the verb-relatederrors and noun-related errors one by one in one sen-tence by narrowing down the classification of errors.Lee and Seneff (2006) developed a system to auto-matically correct grammatical errors related to arti-cles, verbs, prepositions and nouns.Leacock et al (2010) discuss automated gram-matical error detection for English language learn-ers.
They focus on errors that language learners findmost difficult - constructions that contain preposi-tions, articles, and collocations.
They discuss theexisting systems in place for automated grammati-cal error detection and correction for these and otherclasses of errors in a number of languages.
Addi-88Label Meaning Example0 No error I like it.1 Missing auxiliary verb You talking to me?2 Missing copulae She lovely.3 Subject-auxiliary verb agreement You is talking to me.4 Incorrect auxiliary verb used e.g.
using does instead of is She does dead girl.5 Missing verb She her a book.6 Wrong verb usage including subject-verb disagreement He love dogs.7 Missing preposition The book is the table.8 Missing article She ate apple.9 Missing subject before verb I know loves me.10 Missing infinitive marker ?to?
I give it her.11 Other errors not covered in 1-10 The put.Table 1: Different types of errors considered in this studytionally, they touch on error annotations and systemevaluation for grammatical error detection.3 DataFor the purpose of our experiments, we used the Par-adise dataset (Paradise et al, 2005).
This datasetcontains 677 transcripts corresponding to 677 chil-dren aged six that were collected in the course ofa study of the relationship of otitis media and childdevelopment.
The only household language spokenby these children was English.
The transcripts inthe Paradise set consist of conversations between achild and his/her caretaker.
We retained only thechild?s utterances and removed all other utterances.The Paradise dataset (considering only the child?sutterances) contains a total of 108,711 utterances,394,290 words, and an average Mean Length of Ut-terance of 3.64.
Gabani (2009) used scores on thePeabody Picture Vocabulary Test (Dunn, 1965), totalpercentage phonemes repeated correctly on a non-word repetition task and mean length of utterancein morphemes to label these transcripts for languageimpairment.
A transcript was labeled as having beenproduced by a child with LI if the child scored 1.5or more standard deviations below the mean of theentire sample on at least two of the three tests.
Ofthe 677 transcripts, 623 were labeled as TD and 54as LI.We manually annotated each utterance in the tran-scripts for 10 different types of errors.
Table 1 givesthe different types of errors we considered alongwith examples.
We focused on these 10 differenttypes of errors since children with LI have problemswith the usage of verbs in particular.
The list of er-rors we arrived at was based on the errors we ob-served in the transcripts.
Since an utterance couldhave more than one error, we annotated each ut-terance in the transcript for all the errors presentin the utterance.
While annotating the utterances,we observed that there were utterances that couldcorrespond to multiple types of error.
For exam-ple, consider the following sentence: ?She go toschool?.
The error in this sentence could be an er-ror of a missing auxiliary and a wrong verb formin which case the correct sentence would be ?She isgoing to school?
; or it could be a missing modal, inwhich case the correct form would be ?She will go toschool?
; or it could just be a subject-verb disagree-ment in which case ?She goes to school?
would bethe correct form.
Therefore, although we know thatthe utterance definitely has an error, it is not alwayspossible to assign a single error.
We also observedseveral utterances had both a missing subject and amissing auxiliary verb error.
For example, instead ofsaying ?I am going to play?, some children say ?Go-ing to play?, which misses both the subject and aux-iliary verb.
In this case, the utterance was annotatedas having two errors: missing subject and missingauxiliary.
Finally, single word utterances were la-beled as being correct.Table 2 gives the distribution of the errors in thecorpus and percentage of TD and LI population that89No Error Type Percentage(Count)% of LI childrenmaking error% of TD childrenmaking error1 Missing auxiliary 8.43% (641) 7% 5%2 Missing copulae 36.67% (2788) 77.78% 45%3 Subject-auxiliary agreement 6.31% (480) 40.74% 35%4 Incorrect auxiliary verb used 0.71%(54) 11.47% 3%5 Missing verb 5% (380) 29.63% 10%6 Wrong verb usage 14.59% (1109) 68.5% 50%7 Missing preposition 5% (380) 7.4% 5%8 Missing article 3.97% (302) 29.63% 35%9 Missing subject 7.69% (585) 3.7% 5%10 Missing infinitive marker ?To?
1.58% (120) 7.5% 11.67%11 Other errors 10.05% (764) 56.7% 23.2%Table 2: Statistics of Errorsmade the error at least once in the entire transcript.As we can see from Table 2, 36.67% of the errors inthe corpus are due to missing copulae.
Wrong verbusage was the next most common error contributingto 14.59% of the errors in the corpus.
We observedthat there was a higher percentage of children withLI that made errors on all error categories except forerrors related to missing article and missing subject.We observed that on average, the transcripts belong-ing to children with LI had fewer utterances as com-pared to transcripts belonging to TD children.
Ad-ditionally, children with LI used many single wordand two word utterances.One annotator labeled the entire corpus for gram-matical errors.
To calculate inter-annotator agree-ment, we randomly selected 386 utterances anno-tated by the first annotator with different error types.The second annotator was provided these utterancesalong with the labels given by the first annotator1.In case of a disagreement, the second annotator pro-vided a different label/labels.
The annotator agree-ment using the average Cohen?s Kappa coeffiecientwas 77.7%.
Out of the 386 utterances, there were43 disagreements between the annotators.
We foundthat for some error categories such as the missingauxiliary, there was high inter-annotator agreementof 95.32%, whereas for other categories such aswrong verb usage and missing articles, there was1We will perform independent annotation of the errors andcalculate inter-annotator agreement based on these independentannotationsless agreement (64.2% and 65.3% respectively).
Inparticular, we found low inter-annotator agreementon utterances that have errors that could be assignedto multiple categories.4 ExperimentsThe transcripts were parsed using the Charniakparser (Charniak, 2000).
Since the Paradise datasetconsists of children?s utterances, and many of themhave not mastered the language, we observed thatprocessing these transcripts is challenging.
As isprevalent in spoken language corpora, these tran-scripts had disfluencies, false restarts and incom-plete utterances, which sometimes pose problems tothe parser.We conducted experiments in detecting errors re-lated to the usage of the -ing participle, subject-auxiliary agreement, missing copulae, missingverb, subject-verb agreement and missing infinitivemarker ?to?.
For each of these categories, we con-structed one rule based classifier using regular ex-pressions based on the parse tree structure, an alter-nating decision tree classifier that used rules as fea-tures and a naive Bayes multinomial classifier thatused a variety of features.
For every category, weperformed 10 fold cross validation using all the ut-terances.
We used the naive Bayes multinomial clas-sifier and the alternating decision tree classifier fromthe WEKA toolkit (Hall et al, 2009).
Table 3 givesthe results using the three classifiers for the differentcategories of errors, where (P/R) F1 stands for (Pre-90Error Rule Based System(P/R)F1Decision Tree Clas-sifier using Rules asfeatures (P/R)F1Naive Bayes Classifierusing a variety of fea-tures (P/R)F1Usage of -ing participle (0.984/0.978) 0.981 (0.986/1) 0.993 (0.736/0.929) 0.821Missing copulae (0.885/0.9) 0.892 (0.912/0.94) 0.926 (0.82/0.86) 0.84Missing verb (0.875/0.932) 0.903 (0.92/0.89) 0.905 (0.87/0.91) 0.9Subject-auxiliary agree-ment(0.855/0.932) 0.888 (0.95/0.84) 0.892 (0.89/0.934) 0.912Subject-verb agreement (0.883/0.945) 0.892 (0.92/0.877) 0.898 (0.91/0.914) 0.912Missing infinitive marker?To?
(0.97/0.954) 0.962 (0.94/0.84) 0.887 (0.95/0.88) 0.914Overall (0.935/0.923) 0.929 (0.945/0.965) 0.955 (0.956/0.978) 0.967Table 3: Detection of errors using rule based system, alternating decision tree classifier and naive Bayes classifierNo Feature Type1 Verb Adjective Bigram2 Auxiliary Noun Bigram3 Auxiliary Progressive-verb Bigram4 Pronoun Auxiliary Bigram5 Wh-Pronoun Progressive verb Bigram6 Progressive-verb Wh-adverb Bigram7 Adverb Auxiliary Skip-18 Pronoun Auxiliary Skip-19 Wh-adverb Progressive-verb Skip-110 Auxiliary Preposition Skip-2Table 4: Top most bigram features useful for detectingmisuse of -ing participlecision/Recall) F1-measure.
Below we describe thedifferent experiments we conducted.4.1 Misuse of the -ing ParticipleThe -ing participle can be used as a progressive as-pect, a verb complementation, or a prepositionalcomplementation.
In the progressive aspect, it isnecessary that the progressive verb be preceded byan auxiliary verb.
When used as a verb comple-mentation, the -ing participle should be preceded bya verb and similiarly when used as a prepositionalcomplement, the -ing participle should be precededby a preposition.Rule based systemThe -ing participle is denoted by the VBG tag in thePenn tree bank notation.
VP and PP correspond tothe verb phrase and prepositional phrase structuresrespectively.
The rules that we formed were as fol-lows:1.
Check that the utterance has a VBG tag (if itdoes not have a VBG tag, it does not contain an-ing participle).2.
If none of the following conditions are met,there is an error in the usage of -ing participle:(a) The root of the subtree that contains the-ing participle should be a VP with thehead being a verb if used as a verb com-plementation(b) The root of the subtree that contains the-ing participle should be a PP if used as aprepositional complement(c) The root of the subtree that contains the-ing participle should be a VP with thehead being an auxiliary verb if used as aprogressive aspectPredictive modelThe features that we considered were:1.
Bigrams from POS tags2.
Skip bigrams from POS tagsWe used the skip bigrams to account for thefact that there could be other POS tags betweenan auxiliary verb and the progressive aspect ofthe verb such as adverbs.
A skip-n bigram isa sequence of 2 POS tags with a distance of nbetween them.
We used skip-1 and skip-2 bi-grams in this study.91AnalysisAs we can see from Table 3, the alternating decisiontree classifier with rules as features gave the best re-sults with an F1-measure of 0.993.
Table 4 givesthe topmost 10 features extracted using feature se-lection.
We got the best results when we used thereduced set of features as opposed to using all bi-grams and skip-1 and skip-2 bigrams.
We also usedthe results reported by (Caines and Buttery, 2010)to see if their method was successful in identifyingzero auxiliary constructs on our corpus.
When weused logistic regression with the coefficients and fea-tures used by (Caines and Buttery, 2010), we got arecall of 0%.
When we trained the logistic regres-sion model on our data with their features, we got aprecision of 1.09%, recall of 53.6% and F1-measureof 2.14%.
This leads us to conclude that the featuresthat were used by them are not suitable for child lan-guage transcripts.
Additionally, we also observedthat based on the features they used, in some casesit is difficult to distinguish zero auxiliary constructsfrom those with auxiliary constructs.
For example,?You talking to me??
and ?Are you talking to me?
?would have the same values for their features, al-though the former is a zero auxiliary construct andthe latter is not.4.2 Identifying Missing CopulaeA copular verb is a verb that links a subject to itscomplement.
In English, the most common copularverb is ?be?.
Examples of sentences that contain acopular verb is ?She is lovely?
and ?The child whofell sick was healthy earlier?.
An example of a sen-tence that misses a copular verb is ?She lovely?.Rule based systemThe rule that we used was as follows:If an Adjective Phrase follows a noun phrase, ora Noun phrase follows a noun phrase, the likelihoodthat the utterance is missing a copular verb is quitehigh.
However, there are exceptions to such rules,for example, ?Apple Pie?.
We formed additionalrules to identify such utterances and examined theirparse trees to determine the function of the two nounphrases.Predictive modelThe features we used were as follows:1.
Does the utterance contain a noun phrase fol-lowed by a noun phrase?2.
Does the utterance contain a noun phrase fol-lowed by an adjective phrase?3.
Is the parent a verb phrase?4.
Is the parent a prepositional phrase?5.
Is the parent the root of the parse tree?6.
Is there an auxiliary verb or a verb between thenoun phrase and/or adjective phrase?AnalysisAs we can see from Table 3, the alternating deci-sion tree classifier performed the best with an F1-measure of 0.926.
Our rules capture simple con-structs that are used by young children.
The majorityof the utterances that missed a copulae consisted ofnoun phrase and an adjective phrase or a noun phraseand a noun phrase.
Hence, the rules based systemperformed the best.
Some of the false positives weredue to utterances like ?She an apple?
where it is un-likely that the missing verb is a copular verb.4.3 Identifying Missing VerbsErrors of this type occur when a sentence is miss-ing the verb.
For example, the sentence ?You canan apple?
lacks the main verb after the modal verb?can?.
Similarly, ?I did not it?
lacks a main verb af-ter ?did not?.
For the purpose of this experiment, weconsider only utterances that contain a modal or anauxiliary verb but do not have a main verb.
We alsoconsider utterances that use the verb ?do?
and detectthe main missing verb in such cases.Rule based systemThe rule we used was to check if the utterance con-tains an auxiliary verb or a modal verb but not a mainverb.
In this case, the utterance is definitely missinga main verb.
In order to identify utterances where thewords ?did?, ?do?
and ?does?
are auxiliary verbs, weuse the following procedure: If the negation ?not?is present after did/do/does, then did/do/does is anauxiliary verb and needs to be followed by a mainverb.
In the case of the utterance being a question,the presence of did/do/does at the beginning of theutterances indicates the use as an auxiliary verb.
In92such a case, we need to check for the presence of amain verb.
The same holds for the other auxiliaryverbs.Predictive modelWe used the following as features:1.
Is an auxiliary verb present?2.
Is a modal verb present?3.
Is a main verb present after the auxiliary verb?4.
Is a main verb present after the modal verb?5.
Type of utterance - interrogative, declarative6.
Is a negation (not) present?AnalysisAs we can see from Table 3, the alternating decisiontree classifier using rules as features gave the bestresult with an F1-measure of 0.905.
At present, wehandle only a subset of missing verbs and specif-ically those verbs that contain an auxiliary verb.Since most of the utterances are simple constructs,the alternating decision tree classifier performs well.4.4 Identifying Subject-auxiliary AgreementIn the case of the subject-auxiliary agreement andsubject-verb agreement, the first verb in the verbphrase has to agree with the subject unless the firstverb is a modal verb.
In the sentence ?The girls hasbought a nice car?, since the subject ?The girls?
isa plural noun phrase, the auxiliary verb should be inthe plural form.
While considering the number andperson of the subject, we take into account whetherthe subject is an indefinite pronoun or contains aconjunction since special rules apply to these cases.Indefinite pronouns are words which replace nounswithout specifying the nouns they replace.
Some in-definite pronouns such as all, any and more take bothsingular and plural forms.
On the other hand, indefi-nite pronouns like somebody and anyone always takethe singular form.Rule based systemThe rule we used to identify subject-auxiliary agree-ment was as follows:1.
Extract the number (singular, plural) of the sub-ject and the auxiliary verb in the verb phrase.2.
If the number of the subject and auxiliary verbdo not match, there is a subject-auxiliary agree-ment error.Predictive modelThe features were as follows:1.
Number of subject - singular or plural2.
Type of noun phrase - pronoun or other nounphrase3.
Person of noun phrase - first, second, third4.
Presence of a main verb in the utterance (we arelooking at the agreement only for the auxiliaryverb)AnalysisAs we can see from Table 3, the naive Bayes multi-nomial classifier performed the best with an F1-measure of 0.912.
We found that our system didnot detect the subject-auxiliary agreement correctlyif there was an error in the subject such as numberagreement.4.5 Identifying Subject-verb AgreementIn order to achieve subject-verb agreement, the num-ber and person of the subject and verb must agree.The subject-verb agreement applies to the first verbin the verb phrase.
We consider cases wherein thefirst verb is a main verb or contains a modal verb.An example of a sentence that has subject-verb dis-agreement is ?The boy have an ice cream?.
Thenumber and person of the subject ?The boy?
and theverb ?have?
do not match.Rule based systemThe rule we used to identify subject-verb agreementwas as follows:1.
Extract the number (singular, plural) and per-son (first, second, third) of the subject and thefirst verb in the verb phrase.2.
If the verb is not a modal verb and the num-ber and person of the subject and verb do notmatch, there is a subject-verb agreement error.Predictive modelWe used the following features to be used in a statis-tical setup:931.
Type of sentence - interrogative or declarative2.
Number of subject - singular or plural3.
Person of subject if pronoun - first, second orthird4.
Number of verb - singular or plural5.
Person of verb - first, second or third6.
Type of verb - modal, mainAnalysisWe found that our system did not detect errors incases where there was a number disagreement.
Forexample, in the sentence ?The two dog is playing?,our system based on the POS tag would assumethat the subject is singular and therefore there is nosubject-verb error.
One way to improve this wouldbe to detect number disagreement in the subject andcorrect it before detecting the subject-verb agree-ment.4.6 Identifying Missing Infinitive Marker ?To?Errors of this type occur when the sentence lacks theinfinitive marker ?to?.
An example of such a sen-tence would be ?She loves sleep?.
In this case, ?Sheloves to sleep?
would be the correct form.
On theother hand, this statement is ambiguous since sleepcould be used as a noun sense or a verb sense.
Weconcentrated on identifying utterances that have theprogressive verb followed by the verb in the infini-tive form.
Examples of such sentences are: ?She isgoing cry?.
In this case, we can see that the sentenceis missing the ?to?.Rule based systemIf the utterance contains a progressive verb followedby a verb in its infinitive form, it is missing the in-finitive marker ?to?.Predictive modelThe features we used are:1.
Presence of a progressive verb followed by theinfinitive2.
Presence of infinitive marker ?to?
before the in-finitiveAnalysisThe naive Bayes multinomial classifier performedthe best with an F1-measure of 0.967.
We encoun-tered exceptions with words like ?saying?.
An ex-ample of such a sentence would be ?He was sayingplay?.
Most of our false positives were due to sen-tences such as this.
We considered a subset of utter-ances in which the infinitive was used along with theprogressive verb.
The missing infinitive marker ?to?is also found in other utterances such as ?I wouldlove to swim?
in which case we have two verbs thatare in the base form - ?love?
and ?swim?.4.7 Combining the ClassifiersFinally, we perform sentence level binary classifica-tion - does the sentence have a grammatical error?Since an utterance can contain more than one error,we serially apply the binary classifiers that we de-scribed above for each error category.
If any one ofthe classifiers reports an error in the utterance, weflag the utterance as having a grammatical error.
Forevaluation, as long as the utterance had any gram-matical error, we considered the decision to be cor-rect.
As we can see from Table 3, the best resultfor detecting the overall errors was obtained by se-rially applying the classifiers that used the featuresthat were not rule based.5 Conclusions and Future WorkIn this paper, we described a study of grammati-cal errors in child language transcripts.
Our studyshowed that a higher percentage of children withLI made at least one mistake than TD children onmost error categories.
We created different systemsincluding rule based systems that used parse treetemplate matching and classifiers to detect errors re-lated to missing verbs, subject-auxiliary agreement,subject-verb agreement, missing infinitive marker?to?, missing copulae and wrong usage of -ing par-ticiple.
In all cases, we had a recall higher than 84%.When combining the classifiers to detect sentenceswith grammatical errors, the classifiers that used fea-tures other than rules performed the best with an F1-measure of 0.967.The error categories that we detect at present arerestricted in their scope to specific kind of errors.In future, we plan to enhance our systems to de-94tect other grammatical errors such as missing arti-cles, missing prepositions and missing main verbsin utterances that do not have an auxiliary verb.
Fur-thermore, we will investigate methods to address is-sues in child language transcripts due to incompleteutterances and disfluencies.At present, we treat sentences that conform toformal English language as correct.
We could en-hance our systems to look at dialect specific con-structs and grammatical errors made across differ-ent demographics.
For example, African Americanchildren have a different dialect and do not alwaysfollow the formal English language while speaking.Therefore, in the context of detecting language im-pairment, it would be interesting to see whether bothTD children and LI children make the same errorsthat are otherwise considered the norm in the dialectthey speak.AcknowledgmentsThe authors thank Chris Dollaghan for sharing theParadise data, and Thamar Solorio for discussions.This research is partly supported by an NSF awardIIS-1017190.ReferencesMari I. Bowden and Richard K. Fox.
2002.
A DiagnosticApproach to the Detection of Syntactic Errors in En-glish for Non-Native Speakers.
Technical report, TheUniversity of Texas-Pan American.Andrew Caines and Paula Buttery.
2010.
You talking tome?
: A predictive model for zero auxiliary construc-tions.
In Proceedings of the 2010 Workshop on NLPand Linguistics: Finding the Common Ground, pages43?51.Eugene Charniak.
2000.
A maximum-entropy-inspiredparser.
In Proceedings of the 1st North Americanchapter of the Association for Computational Linguis-tics conference, pages 132?139.Lloyd M. Dunn.
1965.
Peabody picture vocabulary test.American Guidance Service Circle Pines, MN.Jens Eeg-Olofsson and Ola Knuttson.
2003.
Automaticgrammar checking for second language learners-theuse of prepositions.
In Proceedings of NoDaLiDa.Keyur Gabani.
2009.
Automatic identification of lan-guage impairment in monolingual English-speakingchildren.
Master?s thesis, The University Of Texas AtDallas.Mark Hall, Eibe Frank, Geoffrey Holmes, BernhardPfahringer, Peter Reutemann, and Ian H. Witten.2009.
The WEKA data mining software: An update.ACM SIGKDD Explorations Newsletter, 11(1):10?18.Claudia Leacock, Martin Chodorow, Michael Gamon,and Joel Tetreault.
2010.
Automated Grammatical Er-ror Detection for Language Learners.
Synthesis Lec-tures on Human Language Technologies, 3(1):1?134.John Lee and Stephanie Seneff.
2006.
Automatic gram-mar correction for second-language learners.
In Pro-ceedings of INTERSPEECH-2006, pages 1978?1981.John Lee and Stephanie Seneff.
2008.
Correcting misuseof verb forms.
In Proceedings of ACL-08:HLT, pages174?182.Jack L. Paradise, Thomas F. Campbell, Christine A.Dollaghan, Heidi M. Feldman, Bernard S. Bernard,D.
Kathleen Colborn, Howard E. Rockette, Janine E.Janosky, Dayna L. Pitcairn, Marcia Kurs-Lasky, et al2005.
Developmental outcomes after early or delayedinsertion of tympanostomy tubes.
New England Jour-nal of Medicine, 353(6):576?586.Mabel L. Rice, Kenneth Wexler, and Patricia L. Cleave.1995.
Specific language impairment as a period ofextended optional infinitive.
Journal of Speech andHearing Research, 38(4):850.Kenji Sagae, Eric Davis, Alon Lavie, Brian MacWhin-ney, and Shuly Wintner.
2007.
High-accuracy annota-tion and parsing of CHILDES transcripts.
In Proceed-ings of the Workshop on Cognitive Aspects of Compu-tational Language Acquisition, pages 25?32.Hollis S. Scarborough.
1990.
Index of productive syntax.Applied Psycholinguistics, 11(01):1?22.95
