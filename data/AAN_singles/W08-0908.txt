Proceedings of the Third ACL Workshop on Innovative Use of NLP for Building Educational Applications, pages 62?70,Columbus, Ohio, USA, June 2008. c?2008 Association for Computational LinguisticsAutomatic Identification of Discourse Moves in Scientific ArticleIntroductionsNick Pendar and Elena CotosApplied Linguistics and Technology ProgramIowa State UniversityAmes, IA 50011 USA{pendar,ecotos}@iastate.eduAbstractThis paper reports on the first stage of build-ing an educational tool for international gradu-ate students to improve their academic writingskills.
Taking a text-categorization approach,we experimented with several models to au-tomatically classify sentences in research ar-ticle introductions into one of three rhetori-cal moves.
The paper begins by situating theproject within the larger framework of intelli-gent computer-assisted language learning.
Itthen presents the details of the study with veryencouraging results.
The paper then concludesby commenting on how the system may be im-proved and how the project is intended to bepursued and evaluated.1 Introduction and BackgroundInterest in automated evaluation systems in the fieldof language assessment has been growing rapidlyin the last few years.
Performance-based andhigh-stakes standardized tests (e.g., ACT, GMAT,TOEFL, etc.)
have employed such systems due totheir potential to yield evidence about the learners?language proficiency and/or subject matter masterybased on analyses of their constructed responses.Automated writing evaluation applications are alsobeginning to draw the attention of pedagogues whoare much interested in assessment for learning, i.e.,assessment used as a tool in gaining direction forremediation.
Arguably, these technological innova-tions open up a wide range of possibilities for high-quality formative evaluation that can closely matchteaching goals and tailor instruction to individuallearners by providing them with feedback and direc-tion on their attainment of knowledge.Traditionally, automated evaluation has been usedfor essay grading, but its potential could be success-fully extrapolated to other genres in both first lan-guage (L1) and second language (L2) academic con-texts.
Existing scoring systems can assess variousconstructs such as topical content, grammar, style,mechanics, syntactic complexity, and even devianceor plagiarism (Burstein, 2003; Elliott, 2003; Lan-dauer et al, 2003; Mitchell et al, 2002; Page, 2003;Rudner and Liang, 2002).
Because learner writingis generally highly erroneous, an emerging researchtrend has focused on automated error detection inL2 output finding novel approaches to develop in-telligent ways to assess ill-formed learner responses(Burstein and Chodorow, 1999; Chodorow et al,2007; Han et al, 2006; Leacock and Chodorow,2003).
Various NLP and statistical techniques alsoallow for the evaluation of text organization, whichis however limited to recognizing the five-paragraphessay format, thesis, and topic sentences.
At present,to our knowledge, there is only one automated eval-uation system, AntMover (Anthony and Lashkia,2003), that applies intelligent technological possibil-ities to the genre of research reports?a major chal-lenge for new non-native speaker (NNS) members ofacademia.
AntMover is able to automatically iden-tify the structure of abstracts in various fields anddisciplines.Academic writing pedagogues have been strug-gling to find effective ways to teach academic writ-ing.
Frodesen (1995) argues that the writing instruc-tion for non-native speaker students should ?help62initiate writers into their field-specific research com-munities?
(p. 333).
In support of this opinion,(Kushner, 1997) reasons that graduate NNS courseshave to combine language and discourse with theskill of writing within professional norms.
Vari-ous pedagogical approaches have been attempted toachieve this goal.
For instance, (Vann and Myers,2001) followed the inductive analysis approach, inwhich students examined the format, content, gram-matical, and rhetorical conventions of each sectionof research reports.
Supplements to this approachwere tasks that required students to write journal en-tries about the rhetorical conventions of prominentjournals in their disciplines and tasks that placedthe experience of writing up research ?in the frame-work of an interactive, cooperative effort with cross-cultural interaction?
(Vann and Myers, 2001, p. 82).Later, after having followed a primarily skill-basedapproach, in which students wrote field-specific lit-erature reviews, summaries, paraphrases, data com-mentaries, and other discipline-specific texts, Levisand Levis-Muller (2003) reported on transformingthe course into a project-based writing one.
Theproject consisted of carrying out original research,the topic of which, for the purpose of coping withdiscipline diversity, was the same for all studentsand was determined by the instructor.
From thestart, the students were provided with a limited setof articles, for instance, on cross-cultural adjust-ment, with which they worked to identify potentialresearch questions for a further investigation and towrite the literature review.
This approach placed aheavy emphasis on collaboration as students workedin small groups on developing data-collection instru-ments and on data analysis.
Oral presentations ongroup-research projects wrapped up the course.The academic writing course discussed in theparagraph above is corpus- and genre-based, com-bining a top-down approach to genre analysis anda bottom-up approach to the analysis of corpora(Cortes, 2006).
Cortes (2006) explains that thecourse was designed to better address the issues ofgenre-specificity and disciplinarity since some stu-dents who took the previous form of the courseclaimed that, although they were taught usefulthings, they did not learn to write the way re-searchers in their disciplines generally do.
In thepresent format of the course, each student is pro-vided with a corpus of research articles published intop journals of his/her discipline.
Students conductclass analyses of their corpus according to guide-lines from empirical findings in applied linguisticsabout the discourse tendencies in research articlewriting.
Their task is to discover organizational andlinguistic patterns characteristic of their particulardiscipline, report on their observations, and applythe knowledge they gain from the corpus analyseswhen writing a research article for the final projectin the course.2 MotivationAlthough each of the pedagogical approaches men-tioned in the previous section has its advantages,they all fail to provide NNS students with sufficientpractice and remediational guidance through exten-sive individualized feedback during the process ofwriting.
An NLP-based academic discourse eval-uation software application could account for thisdrawback if implemented as an additional instruc-tional tool.
However, an application with such ca-pabilities has not yet been developed.
Moreover, asmentioned above, the effects of automated formativefeedback are not fully investigated.
The long-termgoal of this research project is the design and imple-mentation of a new automated discourse evaluationtool as well as the analysis of its effectiveness forformative assessment purposes.
Named IADE (In-telligent Academic Discourse Evaluator), this appli-cation will draw from second language acquisitionmodels such as interactionist views and SystemicFunctional Linguistics as well as from the Skill Ac-quisition Theory of learning.
Additionally, it will beinformed by empirical research on the provision offeedback and by Evidence Centered Design princi-ples (Mislevy et al, 2006).IADE will evaluate students?
drafts of their aca-demic writing in accordance with the course materi-als in terms of an adapted model of Swales?
(Swales,1990; Swales, 2004) move schema as partially pre-sented in Table 1.
IADE will achieve this by con-ducting a sentence-level classification of the inputtext for rhetorical shifts.
Given a draft of a researcharticle, IADE will identify the discourse moves inthe paper, compare it with other papers in the samediscipline and provide feedback to the user.63Move 1 Establishing a TerritoryStep 1: Claiming CentralityStep 2: Making topic generalization(s)and/orStep 3: Reviewing previous researchMove 2 Establishing a nicheStep 1A: Indicating a gap orStep 1B: Highlighting a problem orStep 1C: Question-raising orStep 1D: Hypothesizing orStep 1E: Adding to what is known orStep 1F: Presenting justificationMove 3 Occupying the nicheStep 1A: Announcing present research de-scriptively orStep 1: Announcing present research pur-posefullyStep 2A: Presenting research questions orStep 2B: Presenting hypothesesStep 3: Definitional clarifications and/orStep 4: Summarizing methods and/orStep 5: Announcing principal outcomesand/orStep 6: Stating the value of the present re-search and/orStep 7: Outlining the structure of the paperTable 1: Discourse move model for research article intro-ductions based on (Swales, 1990; Swales, 2004)The development of IADE is guided by the prin-ciples of Evidence Centered Design (ECD), ?anapproach to constructing and implementing edu-cational assessments in terms of evidentiary argu-ments?
(Mislevy et al, 2006, p. 15).
This designallows the program to identify the discourse ele-ments of students?
work products that constitute ev-idence and to characterize the strength of this evi-dence about the writing proficiencies targeted for thepurpose of formative assessment.3 Discourse Move Identification3.1 Data and Annotation SchemeThe discussions above imply that the first step inthe development of IADE is automatic identifica-tion of discourse moves in research articles.
Wehave approached this task as a classification prob-Discipline Files1.
Accounting 202.
Aero-space engineering 203.
Agronomy 214.
Applied linguistics 205.
Architecture 206.
Biology 207. Business 208.
Chemical engineering 209.
Computer engineering 2010.
Curriculum and instruction 2011.
Economics 2012.
Electrical engineering and powersystem2013.
Environmental engineering 2014.
Food science & food service 2015.
Health & human performance 2016.
Industrial engineering 2017.
Journalism 2018.
Mechanical engineering 2019.
Sociology 2020.
Urban and regional planning 20Table 2: Disciplines represented in the corpus for articleintroductionslem.
In other words, given a sentence and a finite setof moves and steps, what move/step does the sen-tence signify?
This task is very similar to identi-fying the discourse structure of short argumentativeessays discussed in (Burstein et al, 2003), the dif-ference being in the genre of the essays and type ofthe discourse functions in question.The corpus used in this study was compiled froman existing corpus of published research articles in44 disciplines, used in an academic writing graduatecourse for international students.
The corpus con-tains 1,623 articles and 1,322,089 words.
The aver-age length of articles is 814.09 words.
We made astratified sampling of 401 introduction sections rep-resentative of 20 academic disciplines (see Table 2)from this corpus of research articles.
The size of thissub-corpus is 267,029 words; each file is on average665.91 words long, resulting in 11,149 sentences asdata instances.The sub-corpus was manually annotated based onSwales?
framework by one of the authors for moves64and steps (see Figure 1 for an example).
The markupscheme includes the elements presented in Table 1.Annotation was performed at sentence level, eachsentence being assigned at least one move and al-most always a step within that move as specified inthe markup scheme.1 The scheme allowed for mul-tiple layers of annotation for cases when the samesentence signified more than one move or more thanone step.
This made it possible to capture an arrayof the semantic shades rendered by a given sentence.<intro_m3 step="description"><intro_m3 step="method"><intro_m3 step="purpose">This paper presents anapplication of simulation,multivariate statistics,and simulation metamodelsto analyze throughput ofmultiproduct batch chemicalplants.</intro_m3></intro_m3></intro_m3>Figure 1: A sample annotated sentence3.2 Feature SelectionIn order to classify sentences correctly, we firstneed to identify features that can reliably indicatea move/step.
We have taken a text-categorizationapproach to this problem.2 In this framework eachsentence is treated as a data item to be classified,and is represented as an n-dimensional vector in theRn Euclidean space.
More formally, a sentence siis represented as the vector s?i = ?f1, f2, .
.
.
, fn?where each component fj of the vector s?i repre-sents a measure of feature j in the sentence si.
Thetask of the learning algorithm is to find a functionF : S ?
C that would map the sentences in the cor-pus S to classes in M = {m1,m2,m3} (where m1,m2, and m3 stand for Move 1, Move 2, and Move 3,respectively).
In this paper, for simplicity, we are as-suming that F is a many-to-one function; however,it should be kept in mind that since sentences may1Only in two instances a step was not assigned.2For an excellent review, see (Sebastiani, 2002).signify multiple moves, in reality the relation maybe many-to-many.An important problem here is choosing featuresthat would allow us to classify our data instancesinto the classes in question properly.
In this studywe focused on automatically identifying the majormoves in the introduction section of research articles(i.e., m1,m2,m3).
Due to the sparseness of data, wehave not attempted to identify the steps within themoves at this time.We extracted word unigrams, bigrams and tri-grams (i.e., single words, two word sequences, andthree word sequences) from the annotated corpus.Subsection 3.5 reports the results of some of our ex-periments with these feature sets.The following steps were taken in preprocessing:1.
All tokens were stemmed using the NLTK3port of the Porter Stemmer algorithm (Porter,1980).
This allows us to represent lexically re-lated items as the same feature, thus reducinginterdependence among features and also help-ing with the sparse data problem.2.
All numbers in the texts were replaced by thestring _number_.3.
In case of bigrams and trigrams, the tokens in-side each n-gram were alphabetized to capturethe semantic similarity among n-grams con-taining the same words but in a different or-der.
This tactic also reduces interdependenceamong features and helps with the sparse dataproblem.4.
All n-grams with a frequency of less than fivewere excluded.
This measure was also takento avoid overfitting the classifier to the trainingdata.The total number of each set of n-grams extracted isshown in Table 3.To identify which n-grams are better indicatorsof moves, odds ratios were calculated for each asfollows:OR(ti,mj) =p(ti|mj) ?
(1 ?
p(ti|m?j))(1 ?
p(ti|mj)) ?
p(ti|m?j)(1)3http://www.nltk.org65n-gram Numberunigrams 3,951bigrams 8,916trigrams 3,605Table 3: Total number of n-grams extractedwhere OR(ti,mj) is the odds ratio of the term (n-gram) ti occurring in move mj; p(ti|mj) is the prob-ability of seeing the term ti given the move mj ;and p(ti|m?j) is the probability of seeing the termti given any move other than mj .
The above condi-tional probabilities are calculated as maximum like-lihood estimates.p(ti|mj) =count(ti in mj)?Nk=1 count(tk in mj)(2)where N is the total number of n-grams in the cor-pus of sentences S.Finally, we selected terms with maximum oddsratios as features.
Subsection 3.5 reports on our ex-periments with classifiers using n-grams with high-est odds ratios.3.3 Sentence RepresentationAs mentioned in the previous subsection, each sen-tence is represented as a vector, where each vectorcomponent fi represents a measure of feature i in thesentence.
Usually, in text categorization this mea-sure is calculated as what is commonly known as thetf.idf (term frequency times the inverse documentfrequency), which is a measure of the importanceof a term in a document.
However, since our ?doc-uments?
are all sentences and therefore very short,we decided to only record the presence or absenceof terms in the sentences as Boolean values; that is,a vector component will contain either a 0 for theabsence of the corresponding term or a 1 for its pres-ence in the sentence.3.4 ClassifierWe chose to use Support Vector Machines (SVM)for our classifier (Basu et al, 2003; Burges, 1998;Cortes and Vapnik, 1995; Joachims, 1998; Vapnik,1995).
SVMs are commonly used to solve classifica-tion problems by finding hyperplanes that best clas-sify data while providing the widest margin possiblebetween classes.
SVMs have proven to be amongthe most powerful classifiers provided that the repre-sentation of the data captures the patterns we are try-ing to discover and that the parameters of the SVMclassifier itself are properly set.SVM learning is a supervised learning techniquewhere the system is provided a set of labeled datafor training.
The performance of the system is thenmeasured by providing the learned model a set ofnew (labeled) data, which were not present duringthe training phase.
The system then applies thelearned model on the new data and provides its owninferred labels.
The labels provided by the systemare then compared with the ?true?
labels alreadyavailable.
In this study, we used a common tech-nique known as v-fold cross validation, in whichdata are divided into v equal-sized groups (either byrandom sampling or by stratified sampling).
Then,the system is trained on all but one of the groups andtested on the remaining group.
This process is re-peated v times until all data items have been usedin training and validation.
This technique providesa fairly accurate view of how a model built on thewhole data set will perform when given completelynew data.
All the results reported in the followingsubsection are based on five-fold cross validation ex-periments.We predominantly used the machine learning en-vironment RAPIDMINER (Mierswa et al, 2006) inthe experimentation phase of the project.
The SVMswere set to use the RBF kernel, which maps samplesinto a higher dimensional space allowing for captur-ing non-linear relationships among the data and la-bels.
The RBF kernel has two parameters, C and ?.These parameters help against overfitting the clas-sifier on the training data.
The values of these pa-rameters is not known before hand for each data setand may be found through an exhaustive search ofdifferent parameter settings (Hsu et al, 2008).
Inthis study, we used C = 23 and ?
= 2?9, whichwere arrived at through a search of different param-eter settings on the feature set with 3,000 unigrams.The search was performed by performing five-foldcross validation on the whole data set using modelsbuilt with various combinations of C and ?
values.Admittedly, these parameters are not necessarily thebest parameters for the other feature sets on whichexhaustive searches should be performed.
This is66the next step in our project.3.5 EvaluationWe performed five-fold cross validation on 14 dif-ferent feature sets as summarized in Table 4.
Theresults of these experiments are summarized in Fig-ures 2?4.
Accuracy shows the proportion of clas-sifications that agreed with the manually assignedlabels.
The other two performance measures, pre-cision and recall, are commonly used in informationretrieval, text categorization, and other NLP appli-cations.
For each category, precision measure whatproportion of the items assigned to that category ac-tually belonged to it, and recall measures what pro-portion of the items actually belonging to a cate-gory were labeled correctly.
The measures reportedhere (macro-precision p?iM and macro-recall ?
?M ) areweighted means of class precision and recall overthe three moves.p?i?
= TPTP + FP (3)???
= TPTP + FN (4)p?iM =?|C|i=1 wip?ii|C| (5)?
?M =?|C|i=1 wi?
?i|C| (6)The figures show that the unigram models resultin the best recall and the trigram models, the bestprecision.
Generally, we attribute lower recall to thesparseness of the data.
Access to more training datawill help improve recall.
We should also note thebehavior of the models with respect to bigram fea-tures.
As seen on Figures 3 and 4, increasing the sizeof the bigram feature set causes a decline in modelprecision and a rise in model recall.
Considering thatthere are far more frequent bigrams than unigrams ortrigrams (cf.
Table 4), this behavior is not surprising.Including more bigrams will increase recall becausethere are more possible phrases to indicate a move,but that will also result in a decline in precision be-cause those bigrams may also frequently appear inother moves.
It also seems that a model employingunigram, bigram and trigrams all will perform bet-ter than each individual model.
We are planning toexperiment with these feature sets, as well.Terms N1 Unigrams 1,0002 2,0003 3,0004 Bigrams 1,0005 2,0006 3,0007 4,0008 5,0009 6,00010 7,00011 8,00012 Trigrams 1,00013 2,00014 3,000Table 4: Feature sets used in experimentsFigure 2: Model accuracy for different feature setsError analysis revealed that Move 2 is the hardestmove to identify.
It most frequently gets misclassi-fied as Move 1.
In the future, it might be helpful tomake use of the relative position of the sentence intext in order to disambiguate the move involved.
Inaddition, further investigation is needed to see whatpercentage of Move 2 sentences identified as Move 1by the system also have been labeled Move 1 by theannotator.
Recall that some of the sentences hadmultiple labels and in this study we are only con-sidering single labels per sentence.One question that might arise is how much infor-67Figure 3: Model precision for different feature setsFigure 4: Model recall for different feature setsmation about the discipline of the article contributesto classification accuracy.
In other words, howdiscipline-dependent are our features?
We also ran aset of experiments with the same features plus infor-mation about the scientific discipline in which eachsentence was written.
The change in system perfor-mance was not significant by any means, which sug-gests that our extracted features are not discipline-dependent.3.6 Interannotator agreementIn order to get a clearer picture of the difficulty ofthe problem, we asked a second annotator to anno-tate a portion of the sub-corpus used in this study.The second annotations were done on a sample offiles across all the disciplines adding up to 487 sen-tences.
Table 5 contains a summary of the agree-ments between the two annotators.Move 1 Move 2 Move 3No.
agreed 457 452 480P (A) 0.938 0.928 0.986?
0.931 0.919 0.984Table 5: Interannotator agreement on 487 sentences.Interannotator agreement ?, which is the proba-bility of agreement minus chance agreement, is cal-culated as follows:?
= P (A) ?
P (E)1 ?
P (E) (7)where P (A) represents observed probability ofagreement, and P (E) is the expected probabil-ity of agreement, i.e., chance agreement.
Giventhree moves and uniform distribution among them,P (E) = (13)2.
Therefore, the two annotators had anaverage ?
of 0.945 over the three moves.3.7 LimitationsThis research is in its initial stages and naturally ithas many limitations.
One issue involves some ofthe choices we made in our experiments such aschoosing to alphabetize the n-grams and choosingparticular values for C and ?.
We will be experi-menting with non-alphabetized n-grams and also ex-perimenting with different kernel parameters to findoptimal models.4 DiscussionThis paper set out to identify rhetorical moves inresearch article introductions automatically for thepurpose of developing IADE, an educational toolfor helping international university students in theUnited States to improve their academic writingskills.
The results of our models based on a rela-tively small data set are very encouraging, and re-search on improving the results is ongoing.68Apart from system accuracy, there are also somepedagogical issues that we need to keep in mindin the development of IADE.
Warschauer andWare (2006) call for the development of a classroomresearch agenda that would help evaluate and guidethe application of automated essay scoring in thewriting pedagogy.
Based on a categorization devel-oped by Long (1984), they propose three directionsfor research: product, process, and process/product,where ?product refers to educational outcome (i.e.,what results from using the software), process refersto learning and teaching process (i.e., how the soft-ware is used), and process/product refers to the in-teraction between use and outcome?
(p. 10).
On thelevel of evaluating technology for language learn-ing in general, Chapelle (2007) specifies three tar-gets for evaluation: ?what is taught in a com-plete course?, ?what is taught through technologyin a complete course?, and ?what is taught throughtechnology?
(p. 30).
In the first case, an entiretechnology-based course is evaluated, in the secondcase, CALL materials used for learning a subset ofcourse objectives, and in the third case, the use oftechnology as support and enhancement of a face-to-face course.This project needs to pursue the third direction inboth of these trends by investigating the potential ofthe IADE program specifically designed to be im-plemented as an additional component of a graduatecourse to improve non-native speaker students?
aca-demic writing skills.
Since this program will repre-sent a case of innovative technology, its evaluation,as well as the evaluation of any other new CALLapplications, according to Chapelle (2007), is ?per-haps the most significant challenge teachers and cur-riculum developers face when attempting to intro-duce innovation into language education?
(p. 30).Therefore, the analysis of the effectiveness of IADEwill be conducted based on Chapelle?s (2001) frame-work, which has proven to provide excellent guid-ance for research of evaluative nature4.ReferencesLaurence Anthony and George V. Lashkia.
2003.
Mover:A machine learning tool to assist in the reading and4see (Jamieson et al, 2005)writing of technical papers.
IEEE Transactions onProfessional Communication, 46(3):185?193.A.
Basu, C. Watters, and M. Shepherd.
2003.
Supportvector machines for text categorization.
In HICSS ?03:Proceedings of the 36th Annual Hawaii InternationalConference on System Sciences (HICSS?03) - Track 4,page 103.3, Washington, DC, USA.
IEEE ComputerSociety.Christopher J. C. Burges.
1998.
A tutorial on supportvector machines for pattern recognition.
Data Miningand Knowledge Discovery, 2(2):121?167.Jill C. Burstein and Martin Chodorow.
1999.
Automatedessay scoring for nonnative english.
In Proceedingsof the ACL99 Workshop on Computer-Mediated Lan-guage Assessment and Evaluation of Natural Lan-guage Processing, pages 68?75, Joint Symposium ofthe Association of Computational Linguistics and theInternational Association of Language Learning Tech-nologies, College Park, Maryland.Jill C. Burstein, Daniel Marcu, and Kevin Knight.
2003.Finding the WRITE stuff: automatic identification ofdiscourse structure in student essays.
IEEE IntelligentSystems, 18(1):32?39.Jill C. Burstein.
2003.
The e-rater text registered scor-ing engine: Automated essay scoring with natural lan-guage processing.
In Shermis and Burstein (Shermisand Burstein, 2003), pages 113?121.Carol Chapelle.
2001.
Computer applications in secondlanguage acquisition.
Cambridge University Press,New York.Carol Chapelle.
2007.
Challenges in evaluation of inno-vation: Observations from technology research.
Inno-vation in Language Learning and Teaching, 1(1):30?45.Martin Chodorow, Joel R. Tetreault, and Na-Rae Han.2007.
Detection of grammatical errors involvingprepositions.
In Proceedings of the 4th ACL-SIGSEMWorkshop on Prepositions, pages 25?30.Corinna Cortes and Vladimir Vapnik.
1995.
Support-vector networks.
Machine Learning, 20:273?297.Viviana Cortes.
2006.
Exploring genre and corpora inthe English for academic writing class.
Manuscriptsubmitted for publication.
Manuscript submitted forpublication.Scott Elliott.
2003.
IntellimetricTM: From here to valid-ity.
In Shermis and Burstein (Shermis and Burstein,2003), pages 71?86.Jan Frodesen.
1995.
Negotiating the syllabus: Alearning-centered, interactive approach to ESL grad-uate writing course design.
In Diane Belcher andGeorge Braine, editors, Academic Writing in a SecondLanguage: Essays on Research and Pedagogy, pages331?350.
Ablex Publishing Corporation, NJ.69Na-Rae Han, Martin Chodorow, and Claudia Leacock.2006.
Detecting errors in English article usage bynon-native speakers.
Natural Language Engineering,2(2):115?129.Chih-Wei Hsu, Chih-Chung Chang, and Chih-Jen Lin.2008.
A practical guide to support vector classifica-tion.
Unpublished manuscript.Joanne Jamieson, Carol Chapelle, and Sherry Preiss.2005.
CALL evaluation by developers, a teacher, andstudents.
CALICO Journal, 23(1):93?138.T Joachims.
1998.
Text categorization with support vec-tor machines: Learning with many relevant features.In In Proceedings of ECML-98, 10th European Con-ference on Machine Learning.Shimona Kushner.
1997.
Tackling the needs of foreignacademic writers: A case study.
IEEE Transactions onProfessional Communication, 40:20?25.Thomas K. Landauer, Darrell Laham, and Peter W. Foltz.2003.
Automated scoring and annotation of essayswith the Intelligent Essay Assessor.
In Shermis andBurstein (Shermis and Burstein, 2003), pages 87?112.Claudia Leacock and Martin Chodorow.
2003.
Auto-mated grammatical error detection.
In Shermis andBurstein (Shermis and Burstein, 2003), pages 195?207.John Levis and Greta Muller-Levis.
2003.
A project-based approach to teaching research writing to nonna-tive writers.
IEEE Transactions on Professional Com-munication, 46(3):210?220.Michael Long.
1984.
Process and product in ESL pro-gramme evaluation.
TESOL Quarterly, 18(3):409?425.I.
Mierswa, M. Wurst, R. Klinkenberg, M. Scholz, and T.Euler.
2006.
YALE (now: RAPIDMINER: Rapid pro-totyping for complex data mining tasks.
In Proceed-ings of the ACM SIGKDD International Conference onKnowledge Discovery and Data Mining (KDD 2006).R.
Mislevy, l. Steinberg, R. Almond, and J. Lukas.
2006.Concepts, terminology, and basic models of evidence-centered design.
In D. Williamson, R. Mislevy, and I.Bejar, editors, Automated scoring of complex tasks incomputer-based testing, pages 15?47.
Lawrence Erl-baum Associates, Mahwah, NJ.Tom Mitchell, Terry Russell, Peter Broomhead, andNicola Aldridge.
2002.
Towards robust computerisedmarking of free-text responses.
In Proceedings of the6th International Computer Assisted Assessment Con-ference, pages 233?249, Loughborough University.Ellis Batten Page.
2003.
Project Essay Grade.
In Sher-mis and Burstein (Shermis and Burstein, 2003), pages43?54.Martin F. Porter.
1980.
An algorithm for suffix stripping.Program, 14(3):130?137.Lawrence M. Rudner and Tahung Liang.
2002.
Auto-mated essay scoring using Bayes?
theorem.
The Jour-nal of Technology, Learning and Assessment, 1(2):3?21.Fabrizio Sebastiani.
2002.
Machine learning in auto-mated text categorization.
ACM Computing Surveys,34(1):1?47.Mark D. Shermis and Jill C. Burstein, editors.
2003.
Au-tomated Essay Scoring: A cross-disciplinary perspec-tive.
Lawrence Erlbaum Associates, Mahwah, NJ.John Swales.
1990.
English in Academic and ResearchSettings.
Cambridge University Press, Cambridge.John Swales.
2004.
Research Genres: Explorationand applications.
Cambridge University Press, Cam-bridge.Roberta Vann and Cynthia Myers.
2001.
Academic ESLoptions in a large research university.
In Ilona Leki,editor, Academic Writing Programs, Case Studies inTESOL Practice Series.
TESOL, Alexandria, VA.Vladimir N. Vapnik.
1995.
The Nature of StatisticalLearning Theory.
Springer, Berlin.Mark Warschauer and Paige Ware.
2006.
Automatedwriting evaluation: defining the classroom researchagenda.
Language Teaching Research, 10(2):1?24.70
