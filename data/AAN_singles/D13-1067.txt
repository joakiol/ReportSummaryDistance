Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 715?725,Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational LinguisticsCollective Personal Profile Summarization with Social NetworksZhongqing Wang, Shoushan Li*, Kong Fang, and Guodong ZhouNatural Language Processing Lab, School of Computer Science and TechnologySoochow University, Suzhou, 215006, China{wangzq.antony, shoushan.li}@gmail.com,{kongfang, gdzhou}@suda.edu.cnAbstractPersonal profile information on social medialike LinkedIn.com and Facebook.com is at thecore of many interesting applications, such astalent recommendation and contextual advertis-ing.
However, personal profiles usually lack or-ganization confronted with the large amount ofavailable information.
Therefore, it is always achallenge for people to find desired informationfrom them.
In this paper, we address the task ofpersonal profile summarization by leveragingboth personal profile textual information and so-cial networks.
Here, using social networks ismotivated by the intuition that, people withsimilar academic, business or social connections(e.g.
co-major, co-university, and co-corporation) tend to have similar experience andsummaries.
To achieve the learning process, wepropose a collective factor graph (CoFG) modelto incorporate all these resources of knowledgeto summarize personal profiles with local textualattribute functions and social connection factors.Extensive evaluation on a large-scale datasetfrom LinkedIn.com demonstrates the effective-ness of the proposed approach.
*1 IntroductionWeb 2.0 has empowered people to actively interactwith each other, forming social networks aroundmutually interesting information and publishing alarge amount of useful user-generated content(UGC) online (Lappas et al 2011; Tan et al2011).
One popular and important type of UGC isthe personal profile, where people post detailed* Corresponding authorinformation on online portals about their education,experiences and other personal information.
Socialwebsites like Facebook.com and LinkedIn.comhave created a viable business as profile portals,with the popularity and success partially attributedto their comprehensive personal profiles.Generally, online personal profiles provide val-uable resources for businesses, especially for hu-man resource managers to find talents, and helppeople connect with others of similar backgrounds(Yang et al 2011a; Guy et al 2010).
However, asthere is always large-scale information of experi-ence and education fields, it is hardly for us to finduseful information from the profile.
Therefore, it isalways a challenge for people to find desired in-formation from them.
For this regard, it is highlydesirable to develop reliable methods to generate asummary of a person through his profile automati-cally.To the best of our knowledge, this is the first re-search that explores automatic summarization ofpersonal profiles in social media.
A straightfor-ward approach is to consider personal profilesummarization as a traditional document summari-zation problem, which treating each personal pro-file independently and generate a summary foreach personal profile individually.
For example,the well-known extraction and ranking approaches(e.g.
PageRank, HITS) extract a certain amount ofimportant sentences from a document according tosome ranking measurements to form a summary(Wan and Yang, 2008; Wan, 2011).However, such straightforward approaches arenot sufficient to benefit from the carrier of person-al profiles.
As the centroid of social networking,people are usually connected to others with similar715background in social media (e.g.
co-major, co-corporation).
Therefore, it is reasonable to lever-age social connection to improve the performanceof profile summarizing.
For example if there areco-major, co-university, co-corporation or otheracademic and business relationships between twopersons, we consider them sharing similar experi-ence and having similar summaries.The remaining challenge is how to incorporateboth the profile textual information and the con-nection knowledge in the social networks.
In thisstudy, we propose a collective factor graph model(CoFG) to summarize the text of personal profilein social networks with local textual informationand social connection information.
The CoFGframework utilizes both the local textual attributefunctions of an individual person and the socialconnection factor between different persons to col-lectively summarize personal profile on one person.In this study, we treat the profile summarizationas a supervised learning task.
Specifically, wemodel each sentence of the profile as a vector.
Inthe training phase, we use the vectors with the so-cial connection between each person to build theCoFG model; while in the testing phase, we per-form collective inference for the importance ofeach sentence and select a subset of sentences asthe summary according to the trained model.
Eval-uation on a large-scale data from LinkedIn.comindicates that our proposed joint model and socialconnection information improve the performanceof profile summarization.The remainder of our paper is structured as fol-lows.
We go over the related work in Section 2.
InSection 3, we introduce the data we collected fromLinkedIn.com and the annotated corpus we con-structed.
In Section 4, we present some motiva-tional analysis.
In Section 5, we explain our pro-posed model and describe algorithms for parame-ter estimation and prediction.
In Section 6, we pre-sent our experimental results.
We sum up our workand discuss future directions in Section 7.2 Related WorkIn this section, we will introduce the related workon the traditional topic-based summarization, so-cial-based summarization and factor graph modelrespectively.2.1 Topic-based SummarizationGenerally, traditional topic-based summarizationcan be categorized into two categories: extractive(Radev et al 2004) and abstractive (Radev andMcKeown, 1998) summarization.
The former se-lects a subset of sentences from original docu-ment(s) to form a summary; the latter reorganizessome sentences to form a summary where severalcomplex technologies, such as information fusion,sentence compression and reformulation are nec-essarily employed (Wan and Yang, 2008; Celiky-ilmaz and Hakkani-Tur, 2011; Wang and Zhou,2012).
This study focuses on extractive summari-zation.Radev et al(2004) proposed a centroid-basedmethod to rank the sentences in a document set,using various kinds of features, such as the clustercentroid, position and TF-IDF features.
Ryang andAbekawa (2012) proposed a reinforcement learn-ing approach on text summarization, which modelsthe summarization within a reinforcement learn-ing-based framework.Compared to unsupervised approaches, super-vised learning for summarization is relatively rare.A typical work is Shen et al (2007) which presenta Conditional Random Fields (CRF) based frame-work to treat the summarization task as a sequencelabeling problem.
However, different from all ex-isting studies, our work is the first attempt to con-sider both textual information and social relation-ship information for supervised summarization.2.2 Social-based SummarizationAs web 2.0 has empowered people to actively in-teract with each other, studies focusing on socialmedia have attracted much attention recently(Meeder et al 2011; Rosenthal and McKeown,2011; Yang et al 2011a).
Social-based summari-zation is exactly a special case of summarizationwhere the social connection is employed to helpobtaining the summarization.
Although topic-based summarization has been extensively studied,studies on social-based summarization are relativenew and rare.Hu et al (2011) proposed an unsupervised Pag-eRank-based social summarization approach byincorporating both document context and user con-text in the sentence evaluation process.
Meng et al(2012) proposed a unified optimization frameworkto produce opinion summaries of tweets through716integrating information from dimensions of topic,opinion and insight, as well as other factors (e.g.topic relevancy, redundancy and language styles).Unlike all the above studies, this paper focuseson a novel task, profile summarization.
Further-more, we employ many other kinds of social in-formation in profiles, such as co-major, and co-corporation between two people.
They are shownto be very effective for profile summarization.2.3 Factor Graph ModelAs social network has been investigated for sever-al years (Leskovec et al 2010; Tan et al 2011;Lu et al 2010; Guy et al 2010) and Factor GraphModel (FGM) is a popular approach to describethe relationship of social network (Tang et al2011a; Zhuang et al 2012).
Factor Graph Modelbuilds a graph to represent the relationship ofnodes on the social networks, and the factor func-tions are always considered to represent the rela-tionship of the nodes.Tang et al(2011a) and Zhuang et al(2012)formalized the problem of social relationshiplearning into a semi-supervised framework, andproposed Partially-labeled Pairwise Factor GraphModel (PLP-FGM) for learning to infer the type ofsocial ties.
Dong et al(2012) gave a formal defini-tion of link recommendation across heterogeneousnetworks, and proposed a ranking factor graphmodel (RFG) for predicting links in social net-works, which effectively improves the predictiveperformance.
Yang et al (2011b) generated sum-maries by modeling tweets and social contexts intoa dual wing factor graph (DWFG), which utilizedthe mutual reinforcement between Web documentsand their associated social contexts.Different from all above researches, this paperproposes a pair-wise factor graph model to collec-tively utilize both textual information and socialconnection factor to generate summary of profile.3 Data Collection and StatisticsThe personal profile summarization is a novel taskand there exists no related data for accessing thisissue.
Therefore, in this study, we collect a data setcontaining personal summaries with the corre-sponding knowledge, such as the self-introductionand personal profiles.
In this section, we will in-troduce this data set in detail.3.1 Data CollectionWe collect our data set from LinkedIn.com1 .
Itcontains a large number of personal profiles gen-erated by users, containing various kinds of infor-mation, such as personal overview, summary, edu-cation, experience, projects and skills.John Smith2OverviewCurrent Applied Researcher at Apple Inc.PreviousSenior Research Scientist at IBM?EducationMIT,Georgia Institute of Technology,?SummaryMachine learning researcher and engineer onmany fields:Query understanding.
Automatic Informationextraction?ExperienceApplied ResearcherApple Inc., September 2012 ~Query recognition and relevance?EducationMITPh.D., Electrical Engineering, 2002 ?
2008?Figure 1: An example of a profile webpage fromLinkedIn.comIn this study, the data set is crawled in the fol-lowing ways.
To begin with, 10 random people?spublic profiles are selected as seed profiles, andthen the profiles from their ?People Also Viewed?field were collected.
The data is composed of3,182 public profiles3 in total.
We do not collectpersonal names in public profiles to protect peo-ple?s privacy.
Figure 1 shows an example of a per-son?s profile from LinkedIn.com.
The profile in-cludes following fields:?
Overview: It gives a structure description of aperson?s general information, such as cur-rent/previous position and workplace, brief1 http://www.linkedin.com2 The information of the example is a pseudo one.3 We collect all the data from LinkedIn.com at Dec 17,2012.717education background and general technicalbackground.?
Summary: It summarizes a person?s work,experience and education.?
Experience: It details a person?s work experi-ence.?
Education: It details a person?s educationbackground.Among these fields, the Overview is requiredand the others are optional, such as Project,Course and Interest groups.
However, comparedwith Overview, Summary, Experience, Educationfields, they seem to be less important for summari-zation of personal profiles.
Thus, we ignore themin our study.3.2 Data Statistics of Major FieldsWe collected 3,182 personal profiles fromLinkedIn.com.
Table 1 shows the statistics of ma-jor fields in our data collection.Field#Non-emptyfieldsAveragefieldlengthOverview 3,182 45.1Summary 921 25.8Experience 3,148 192.1Education 2,932 33.6Table 1: Statistics of major fields in our data set, i.e.
thenumber of non-empty fields and the average length foreach fieldFrom Table 1, we can see that,?
The information of each profile is incom-plete and inconsistent, That is, not all kindsof fields are available in each personal?sprofile.?
Most people provide their experience andeducation information.
However, the Sum-mary fields are popularly missing (Onlyabout 30% of people provide it).
This ismainly because writing summary is nor-mally more difficult than other fields.Therefore, it is highly desirable to developreliable automatic methods to generate asummary of a person through his/her pro-file.?
The length of the Experience field is thelongest one, and work experience alwayscould represent general information ofpeople.3.3 Corpus Construction and AnnotationAmong the 921 profiles that contain the summary,we manually select 497 profiles with high qualitysummary to construct the corpus for our research.These high-quality summaries are all written bythe authors themselves.
Here, the quality is meas-ured by manually checking that whether they arewell capable of summarizing their profiles.
That is,they are written carefully, and could give an over-view of a person and represent the education andexperience information of a person.After carefully seeing the profiles, we observethat the Experience field contains the most abun-dant information of a person.
Thus, we treat thetext of Experience field as the source of summaryfor each profile.
Besides, we collect social contextinformation from Education and Experience field,and these social contexts are including byLinkedIn explicitly.
Table 2 shows the averagelength of summary and experience fields we usedfor evaluating our summarization approach.FieldAveragelengthSummary(the summary of theprofile)37.2Experience(the source text for thesummarizing)372.0Table 2: Average length of the high-quality summaryand corresponding experience fieldsFrom Table 2, we can see that,?
Compared with the average length of 25.8in Table 1, summaries of high quality havelonger length because they contain more in-formation of the profiles.?
The compression ratio of our proposed cor-pus is 0.1 (37.2/372.0).4 Motivation and AnalysisIn this section, we propose the motivation of socialconnection to address the task of personal profilesummarization.
To preliminarily support the moti-vation, some statistics of the social connection areprovided.718Figure 2: An example of personal profile network.Red is for female, blue is for male, and the dotted linemeans the social connection between two persons.We first describe the social connections whichwe used.
Figure 2 shows an example of socialconnection between people from the profiles ofLinkedIn.
We find that people are sometimes con-nected by several social connections.
For example,John and Lucy are connected by co_unvi relation-ship, while Lily and Linda are connected byco_corp relationship.
From LinkedIn, four kinds ofsocial relationship between people are extractedfrom the Education field and Experience field.They are:?
co_major denotes that two persons have thesame major at school?
co_univ denotes that two persons are graduat-ed from the same university?
co_title denotes that two persons have thesame title at corporation.?
co_corp denotes that two persons work at thesame corporation.Our basic motivation of using social connectionlies in the fact that ?connected?
people will tend tohold related experience and similar summaries.We then give the statistics of edges of socialconnection.
Table 3 shows basic statistics acrossthese edges.
From Table 3, we can see that thenumber of users is 497 while the number of socialconnection edges is 14,307.
The latter is muchlarger than the former.
The number of the edgesfrom Education field is similar with the number ofthe edges from Experience filed.
Among all therelationships, co_unvi is the most common one.Numbers# users 497co_major 1,288co_unvi 6,015# education field 7,303co_title 3,228co_corp 3,776# experience field 7,004# total edges 14,307Table 3: The statistic of edges for our main datasets5 Collective Factor Graph ModelIn this section, we propose a collective factorgraph (CoFG) model for learning and summarizingthe text of personal profile with local textual in-formation and social connection.5.1 Overview of Our FrameworkTo generate summaries for profiles, a straightfor-ward approach is to treat each personal profile in-dependently and generating a summary for eachpersonal profile individually.
As we mentioned onSection 3.3, we use the sentences of Experiencefield as a text document and consider it as thesource of summary for each profile.Instead, we formalize the problem of personalprofile summarization in a pair-wise factor graphmodel and propose an approach referred to asLoopy Belief Propagation algorithm to learn themodel for generating the summary of the profile.Our basic idea is to define the correlations usingdifferent types of factor functions.
An objectivefunction is defined based on the joint probabilityof the factor functions.
Thus, the problem of col-lective personal profile summarization modellearning is cast as learning model parameters thatmaximizes the joint probability of the input con-tinuous dynamic network.The overview of the proposed method is a su-pervised framework (as shown in Figure 3).
First,we treat each sentence of the training data and test-ing data as vectors with textual information (localtextual attribute functions); Second, all the vectorsare connected by social connection relationships(social connection factors) and we model thesevectors and their relationships into the collectivefactor graph; third, we propose Loopy Belief Prop-JohnAntonyBillLilyLucyLindaco_majorco_univco_corpco_corpco_titleco_titleco_majorco_univ719agation algorithm to learn the model and predictthe sentences of testing data; finally, we select asubset of sentences of each testing profile as thesummary according to the models with top-n pre-diction score.
Thus, the core issues of our frame-work are 1) how to define the collective factorgraph model to connection profiles with socialconnection; 2) how to learn and predict the pro-posed CoFG model; 3) how to predict the sentenc-es from the testing data with the proposed CoFGmodel, and generate the summary by the predictscores.
We will discuss these issues on the follow-ing subsections.Figure 3: The overview of our proposed framework5.2 Model DefinitionFormally, given a network ( , , , )L UG V S S X?
,each sentenceis  is associated with an attributevectorix  of the profile and a label iy  indicatingwhether the sentence is selected as a summary ofthe profile (The value ofiy  is binary.
1 means thatthe sentence is selected as a summary sentence,whereas 0 stands for the opposite).
V denotes theauthors of the profiles, LS  denotes the labeledtraining data, and US denotes the unlabeled testingdata.
Let { }iX x?
and { }iY y?
.
Then, we have thefollowing formulation?
?
?
?
?
??
?, || , ,P X G Y P YP Y X G P X G?
(1)Here, G denotes all forms of network infor-mation.
This probabilistic formulation indicatesthat labels of skills depend on not only local at-tributes X, but also the structure of the network G.According to Bayes?
rule, we have?
?
?
?
?
??
??
?
?
?, || ,,| |P X G Y P YP Y X GP X GP X Y P Y G??
(2)Where ( | )P Y G represents the probability of labelsgiven the structure of the network and ( | )P X Ydenotes the probability of generating attributes Xassociated to their labels Y .
We assume that thegenerative probability of attributes given the labelof each edge is conditionally independent, thus wehave?
?
?
?
?
?| , | |i iiP Y X G P Y G P x y?
?
(3)Where ( | )i iP x y  is the probability of generatingattributesix given the label iy .
Now, the problembecomes how to instantiate the probability( | )P Y G and ( | )i iP x y .
We model them in a Mar-kov random field, and thus according to the Ham-mersley-Clifford theorem (Hammersley andClifford, 1971), the two probabilities can be in-stantiated as follows:?
?
?
?111| exp ,di i j j ij ijP x y f x yZ ???
??
?
??
??(4)?
?
?
?
( )21| exp ,i j NB iP Y G g i jZ ??
??
?
??
??
?
(5)Where1 2 and Z Z  are normalization factors.
Eq.
4indicates that we define an attribute function( , )i if x y  for each attribute ijxassociated withsentenceis .
j?
is the weight of the jth attribute.
Eq.5 represents that we define a set of correlation fac-tor functions ( , )g i j  over each pair ( , )i j in thenetwork.
( )NB i  denotes the set of social relation-ship neighbors nodes of i.TrainingSetSocialConnectionSocialConnectionTestingSetSentence ScoringSentence SelectionSummarized ProfileProfilesProfilesCollective Factor GraphModeling720132f (v1,y1)y2y1y3y4y5y6S1S2S3S4S5S6f (v1,y2)f (v6,y6)CoFG modelNodes of sentenceswith different peopley1=0y2=1y3=1y4=0y6=?y5=?
g (y3,y5)Figure 4: Graph representation of CoFGThe left figure shows the personal profile network.
Each dotted line indicates a social connection.
Each dottedsquare denotes a person, and the grey square denotes the sentence selected in the summary, and the white squaredenotes a sentence that is not selected as the summary..The right figure shows the CoFG model derived from left figure.
Each eclipse denotes a sentence vector of aperson, and each circle indicates the hidden variable yi.
f(vi,yi) indicates the attribute factor function.
g(yi,yj) indi-cates the social connection factor function.456co_majorco_corpPerson APerson BPerson CWe now briefly introduce possible ways to de-fine the attribute functions{ ( , )}ij i jf x y, and factorfunction ( , )g i j  .Local textual attribute functions{ ( , )}ij i jf x y:It denotes the attribute value associated with eachsentence i.
We define the local textual attribute asa feature (Lafferty et al 2001).
We can accumu-late all the attribute functions and obtain local en-tropy for a person:?
?11 exp ,k k ik ii kf x yZ ??
??
??
???
(6)The textual attributes include following features(Shen et al 2007; Yang et al 2011b):1) BOW: the bag-of-words of each sentence, weuse unigram features as the basic textual fea-tures for each sentence.2) Length: the number of terms of each sentence.3) Topic_words: these are the most frequentwords in the sentence after the stop words areremoved.4) PageRank_scores: as shown in the relatedwork section, a document can be treated as agraph and applying a graph-based ranking al-gorithm (Wan and Yang., 2008).
We thus usethe PageRank score to reflect the importanceof each sentence.Social connection factor function ( , )i jg y y:For the social correlation factor function, we de-fine it through the pairwise network structure.
Thatis, if the person of sentence i and the person ofsentence j have a social relationship, a factor func-tion for this social connection is defined (Tang etal., 2011a; Tang et al 2011b), i.e.,?
?
?
??
?2, expi j ij i jg y y y y??
?
(7)The person-person social relationships are de-fined on Section 4, e.g.
co_major, co_univ, co_title,and co_corp.
We define that if two persons have atleast one social connection edge, they have a so-cial relationship.
In addition,ij?
is the weight ofthe function, representing the influence degree of ion j.To better understand our model, one example offactor decomposition is given in Figure 4.
In thisexample, there are six sentences from three pro-files.
Among them, four sentences are labeled (twoare labeled with the category of ?1?, i.e,  1y ?
andthe other two are labeled with the category of ?0?,i.e., 0y ? )
and two sentences are unlabeled (theyare represented by y=?).
We have six attributefunctions.
For example,1( , )if v y  denotes the set721of local textual attribute functions ofiy .
We alsohave five pairwise relationships (e.g.,2 4( , )y y ,3 5( , )y y ) based on the structure of the input per-sonal profile social network.
For example,3 5( , )g y y  denotes social connection between 3yand5y , while they share the co_major relationshipon the left figure.5.3 Model LearningWe now address the problem of estimating the freeparameters.
The objective of learning the CoFGmodel is to estimate a parameter configuration({ },{ })?
?
??
to maximize the log-likelihood ob-jective function ( ) log ( | , )L P Y X G??
?
, i.e.,?
?
* argmax L?
??
(9)To solve the objective function, we adopt a gra-dient descent method.
We use ?
(the weight ofthe social connection factor function ( , )i jg y y) asthe example to explain how we learn the parame-ters (the algorithm also applies to tune ?
by simp-ly replacing ?
with?
).
Specifically, we first writethe gradient of eachk?
with regard to the objectivefunction (Eq.
9) :?
?
?
?
?
?
( | , ), ,kP Y X GkL E g i j E g i j???
?
?
?
?
?
??
?
?
?
(10)Where [ ( , )]E g i j is the expectation of factorfunction ( , )g i j  given the data distribution (essen-tially it can be considered as the average value ofthe factor function ( , )g i j over all pair in the train-ing data); and( | , ) [ ( , )]k Y X GPE g i j?is the expectation offactor function ( , )g i j under the distribution( | , )kP Y X G?given by the estimated model.
Asimilar gradient can be derived for parameterja.We approximate the marginal distribution( | , ) [ ( , )]k Y X GPE g i j?using LBP (Tang et al 2011;Zhuang et al 2012).
With the marginal probabili-ties, the gradient can be obtained by summing overall triads.
It is worth noting that we need to per-form the LBP process twice for each iteration: oneis to estimate the marginal distribution of unknownvariables ?iy ?
and the other is to estimate themarginal distribution over all pairs.
In this way,the algorithm essentially performs a transfer learn-ing over the complete network.
Finally, with theobtained gradient, we update each parameter witha learning rate?
.
The learning algorithm is sum-marized in Figure 5.Input: Network G , Learning rate ?Output: Estimated parameters ?Initialize 0?
?Repreat1) Perform LBP to calculate themarginal distribution of unknownvariables, i.e., ?
?| ,i iP y x G2) Perform LBP to calculate themarginal distribution of eachvariables, i.e., ?
?
( , ), | ,i j i jP y y X G3) Calculate the gradient ofk?
ac-cording to Eq.
10 (for a  with asimilar formula)4) Update parameter ?
with thelearning rate ??
?new oldL ??
?
?
??
?Until ConvergenceFigure 5: The Learning Algorithm for CoFG model5.4 Model Prediction and Summary Gener-atedWe can see that in the learning process, the learn-ing algorithm uses an additional loopy belief prop-agation to infer the label of unknown relationships.With the estimated parameter ?
, the summariza-tion process is to find the most likely configurationof Y  for a given profile.
This can be obtained by?
?
* argmax | , ,Y L Y X G ??
(11)Finally, we select a subset of sentences of eachtesting profile as the summary according to thetrained models with top-n prediction scores by *Y(Tang et al 2011b; Dong et al2012).6 ExperimentationIn this section, we describe the settings of our ex-periment and present the experimental results ofour proposed CoFG model.7226.1 Experiment SettingsIn the experiment, we use the corpus collectedfrom LinkedIn.com that contains 497 profiles (seemore details in Section 3).
The existing summariesin these profiles are served as the reference sum-mary (the standard answers).
As discussed in sub-section 3.3, the average length of summary isabout 40 words.
Thus, we extract 40 words to con-struct the summary for each profile.
We use 200personal profiles as the testing data, and the re-maining ones as the training data.We use the ROUGE-1.5.5 (Lin and Hovy, 2004)toolkit for evaluation, a popular tool that has beenwidely adopted by several evaluations such asDUC and TAC (Wan and Yang, 2008; Wan, 2011).We provide four of the ROUGE F-measure scoresin the experimental results: ROUGE-2 (bigram-based), ROUGE-L (based on longest commonsubsequences), ROUGE-W (based on weightedlongest common subsequence, weight=1.2), andROUGE-SU4 (based on skip bigram with a maxi-mum skip distance of 4).6.2 Experimental ResultsWe compare the proposed CoFG approach withthree baselines illustrated as follows:?
Random: we randomly select sentences ofeach profile to generate the summary for theprofile.?
HITS: we employ the HITS algorithm to per-form profile summarization (Wan and Yang,2008).
In detail, we first consider the words ashubs the sentences as authorities; Then, werank the sentences with the authorities?
scoresfor each profile individually; Finally, thehighest ranked sentences are chosen to consti-tute the summary.?
PageRank: we employ the PageRank algo-rithm to perform profile summarization (Wanand Yang, 2008).
In detail, we first connectthe sentences of the profile with cosine text-based similar measure to construct a graph;Then, we apply PageRank algorithm to rankthe sentence through the graph for each pro-file individually; Finally, the highest rankedsentences are chosen to constitute the sum-mary.?
MaxEnt: as a supervised learning approach,maximum entropy uses textual attribute asfeatures to train a classification model.
Then,the classification model is employed to pre-dict which sentences can be selected to gener-ate the summary.
For the implementation ofMaxEnt, we employ the tool of mallenttoolkits4.Table 4 shows the comparison results of our ap-proach (CoFG) and the baseline approaches.
FromTable 4, we can see that 1) either HITS or Pag-eRank outperforms the approach of  random selec-tion; 2) The supervised approach i.e.
MaxEnt, out-performs both the HITS algorithm and the Pag-eRank approach; 3) CoFG model performs bestand it greatly outperforms both the unsupervisedand supervised learning baseline approaches interms of the ROUGE-2 F-measure score.
This re-sult verifies the effectiveness of considering thesocial connection between the sentences in differ-ent profiles,Figure 6 shows the performance of our proposedCoFG model with different sizes of training data.From Figure 6, we can see that CoFG model withsocial connection always performs better thanMaxEnt, and the performance of our approach de-scends slowly when the training dataset becomessmall.
Specifically, the performance of CoFG us-ing only 10% training data achieves better perfor-mance than MaxEnt using 100% training data.4 http://mallet.cs.umass.edu/ROUGE-2 ROUGE-L ROUGE-W ROUGE-SU4Random 0.0219 0.1363 0.0831 0.0288HITS 0.0295 0.1499 0.0905 0.0355PageRank 0.0307 0.1574 0.0944 0.0383MaxEnt 0.0349 0.1659 0.0995 0.0377CoFG 0.0383 0.1696 0.1015 0.0415Table 4: Performances of different approaches to profile summarization in terms of different measurements723Figure 6:  The performance of CoFG with differenttraining data sizeTable 5 shows the contribution of the socialedges with CoFG.
Specifically, CoFG is our pro-posed approach with both education and experi-ence information, CoFG-edu means that the CoFGmodel considers the social edges of education field(co_major, co_univ) only, and CoFG-exp meansthat the CoFG model considers the social edges ofwork experience field (co_title, co_corp) only.MaxEnt can be considered as using textual infor-mation only.ROUGE-2MaxEnt 0.0349CoFG 0.0383CoFG-edu 0.0382CoFG-exp 0.0381Table 5: ROUGE-2 F-Measure score of the contribu-tion of social edgesFrom Table 5, we can see that all of our pro-posed approaches, i.e., CoFG-edu, CoFG-exp, andCoFG, outperform the baseline approach, i.e.,MaxEnt.
However, the performance of CoFG-edu,CoFG-exp and CoFG are similar.
This result ismainly due to the fact that the information of so-cial connection is redundant.
For example, twopersons who are connected by co_major (educa-tion field) might also be connected by co_corp(experience field).7 Conclusion and Future WorkIn this paper, we present a novel task named pro-file summarization and propose a novel approachcalled collective factor graph model to address thistask.
One distinguishing feature of the proposedapproach lies in its incorporating the social con-nection.
Empirical studies demonstrate that thesocial connection is effective for profile summari-zation, which enables our approach outperformsome competitive supervised and unsupervisedbaselines.The main contribution of this paper is to exploresocial context information to help generate thesummary of the profiles, which represents an in-teresting research direction in social network min-ing.
In the future work, we will explore more kindsof social context information and investigate betterways of incorporating them into profile summari-zation and a wider range of social network mining.AcknowledgmentsThis research work is supported by the NationalNatural Science Foundation of China(No.61273320, No.61272257, No.61331011 andNo.61375073), and National High-tech Researchand Development Program of China(No.2012AA011102).We thank Dr. Jie Tang and Honglei Zhuang forproviding their software and useful suggestionsabout PGM.
We acknowledge Dr. Xinfang Liu,Yunxia Xue and Yulai Shen for corpus construc-tion and insightful comments.
We also thankanonymous reviewers for their valuable sugges-tions and comments.ReferencesBaeza-Yates R. and B. Ribeiro-Neto.
1999.
ModernInformation Retrieval.
ACM Press and Addison Wes-ley, 1999Celikyilmaz A. and D. Hakkani-Tur.
2011.
Discoveryof Topically Coherent Sentences for ExtractiveSummarization.
In Proceeding of ACL-11.Dong Y., J. Tang, S. Wu, J. Tian, N. Chawla, J. Rao,and H. Cao.
2012.
Link Prediction and Recommen-dation across Heterogeneous Social Networks.
InProceedings of ICDM-12.Elson D., N. Dames and K. McKeown.
2010.
ExtractingSocial Networks from Literary Fiction.
In Proceed-ing of ACL-10.Erkan G. and D. Radev.
2004.
LexPageRank: Prestigein Multi-document Text Summarization.
In Proceed-ings of EMNLP-04.Guy I., N. Zwerdling, I.  Ronen, D. Carmel, E. Uziel.2010.
Social Media Recommendation based on Peo-ple and Tags.
In Proceeding of SIGIR-10.0.0300.0320.0340.0360.0380.0400.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1ROUGE-2size of training dataPageRank MaxEnt CFG724Hammersley J. and P. Clifford.
1971.
Markov Field onFinite Graphs and Lattices, Unpublished manuscript.1971.Hu P., C. Sun, L. Wu, D. Ji and C. Teng.
1011.
SocialSummarization via Automatically Discovered SocialContext.
In Proceeding of IJCNLP-11.Lafferty J, A. McCallum, and F. Pereira.
2001.
Condi-tional Random Fields: Probabilistic Models for Seg-menting and Labeling Sequence Data.
In Proceed-ings of ICML-01.Lappas T., K. Punera and T. Sarlos.
2011.
Mining TagsUsing Social Endorsement Networks.
In Proceedingof SIGIR-11.Leskovec J., D. Huttenlocher and J. Kleinberg.
2010.Predicting Positive and Negative Links in Online So-cial Networks.
In Proceedings of WWW-10.Lin, C. 2004.
ROUGE: a Package for Automatic Evalu-ation of Summaries.
In Proceedings of ACL-04Workshop on Text Summarization Branches Out.Lu Y., P. Tsaparas, A. Ntoulas and L. Polanyi.
2010.Exploiting Social Context for Review Quality Pre-diction.
In Proceeding of WWW-10.Meng X?F.
Wei?
X. Liu?
M. Zhou?
S. Li and H.Wang.
2012.
Entity-Centric Topic-Oriented OpinionSummarization in Twitter.
In Proceeding of KDD-12.Murphy K., Y. Weiss, and M. Jordan.
1999.
Loopy Be-lief Propagation for Approximate Inference: An Em-pirical Study.
In Proceedings of UAI-99.Radev D. and K. McKeown.
1998.
Generating NaturalLanguage Summaries from Multiple On-line Sources.Computational Linguistics, 24(3):469?500.Radev D., H. Jing, M. Stys, and D. Tam.
2004.
Cen-troid-based Summarization of Multiple Documents.Information Processing and Management.
40 (2004),919-938.Rosenthal S. and K. McKeown.
2011.
Age Prediction inBlogs: A Study of Style, Content, and OnlineBehav-ior in Pre- and Post-Social Media Generations.
InProceeding of ACL-11.Ryang S. and T. Abekawa.
2012.
Framework of Auto-matic Text Summarization Using ReinforcementLearning.
In Proceeding of EMNLP-2012.Shen D., J.
Sun, H. Li, Q. Yang and Zheng Chen.
2007.Document Summarization using Conditional Ran-dom Fields.
In Proceeding of IJCAI-07.Tan C., L. Lee, J. Tang, L. Jiang, M. Zhou and P. Li.2011.
User-Level Sentiment Analysis IncorporatingSocial Networks.
In Proceedings of KDD-11.Tang W., H. Zhuang, and J. Tang.
2011a.
Learning toInfer Social Ties in Large Networks.
In Proceedingsof ECML/PKDD-11.Tang J., Y. Zhang, J.
Sun, J. Rao, W. Yu, Y. Chen, andA.
Fong.
2011b.
Quantitative Study of IndividualEmotional States in Social Networks.
IEEE Transac-tions on Affective Computing.
vol.3(2), Pages 132-144.Wan X. and J. Yang.
2008.
Multi-document Summari-zation using Cluster-based Link Analysis.
In Pro-ceedings of SIGIR-08.Wan X.
2011.
Using Bilingual Information for Cross-Language Document Summarization.
In Proceedingsof ACL-11.Wang H. and G. Zhou.
2012.
Toward a Unified Frame-work for Standard and Update Multi-DocumentSummarization.
ACM Transactions on Asian Lan-guage Information Processing.
vol.11(2).Xing E, M. Jordan, and S. Russell.
2003.
A GeneralizedMean Field Algorithm for Variational Inference inExponential Families.
In Proceedings of UAI-03.Yang S., B.
Long, A. Smola, N. Sadagopan, Z. Zhengand H. Zha.
2011a.
Like like alike ?
Joint Friend-ship and Interest Propagation in Social Networks.
InProceeding of WWW-11.Yang Z., K. Cai, J. Tang, L. Zhang, Z. Su and J. Li.2011b.
Social Context Summarization.
In Proceed-ing of SIGIR-11.Zhuang H, J. Tang, W. Tang, T. Lou, A. Chin, and X.Wang.
2012.
Actively Learning to Infer Social Ties.In Proceedings of Data Mining and Knowledge Dis-covery (DMKD-12), vol.25 (2), pages 270-297.725
