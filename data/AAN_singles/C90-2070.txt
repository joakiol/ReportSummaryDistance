Extending the Lexicon by Exploiting Subregularities*Robert WilenskyComputer Science DivisionDepartment of EECSUniversity of California, BerkeleyBerkeley, CA 94720wilensky@teak.berkeley.ednL lntrmiucfionThis paper is concerned with the acquisition of the lexi-con.
In particular, we propose a method that uses ann olo~,ical reasoning to hypothesize new polysemous wordse,ses.
This method is one of a number of knowledgeacquisition devices to be included in DIRC (DomainIndependent Retargetable Consultant).
DIRC is a kindol intelligent, natm'al anguage-capable consultant kitthat can be retargeted at different domains.
DIRC isessentially "emptyoUC" (UNIX Consultant, Wilenskyet al, 1988).
DIRC is to include the language and rea-soning mechanisms of UC, plus a large grammar and age, neral lexicon.
The user must then add domainkr~owledge, user knowledge and lexieal kqlowledge forth,~ area of interest.2.
Previous Work ha Acq~fisifio~n of the Le.~fico~.Th~'re have been ~mmerous attempts to build systemsthat automatic;ally acquir~ word ~~anings.
Mostly,th~se have f~:n either dietion~,~y ~'e~qders o  attempts ~ohypothesize mcanings of cor~plctely unfamiliai ~wordsfrom context (e.g., Selfridge (1982), (;rangcr (1977)).h~ contrast, wc have focussed on the px'obleln of acquiroinl; word senses that are related to ones already known.gle meaning may be involved in any number of senses,each of which has grammatical or other differences.Typically, a word has at least one core meaning fromwhich the meanings involved in other senses are in somesense synchronically based.For example, the word "open" has adjectival and verbalsenses; the verbal senses include some whose meaningis, roughly, making physical access available to anenclosed region by moving some object (e.g., "open ajar", "open a draw", "open the door").
This is prob-ably a core meaning of the word.
There are severalsenses involvin~g this meaning, just among the verbalsenses.
These senses are differentiated from one anotherby how the components of the meaning relate to theverb's valence.
For example, one sense has the objectmoved as the patient, and hence as the direct object oftile transitive verb (as in "open the door"); another usesthe container itself as the direct object (e.g., "open thejea"'); pedlaps another involves some sort of aperturethat widens (e.g., "open your throat" or "open the pupilof your eyC').
Additionally, each of these componentsof the meaning can be realized as patients by api~aringas the subject of the intransitive version of the verb.
Weconsider each differentiable valence structure for boththe Wansitive amt intransitive verb forms as constitutingdifferent senses, although we presume that the sameconceptual structure is in all of these xamples.22.o A Note on W~d ~er~sesFc~,' the pmlx):~es at hand, we ~'c only concerned withw(nd senses that a~c ~;ynchronicaily rclatcd.
These maybe polysemou.,; senses of bldividl~als words, ~u wellrelated senses of different words, in addition, we distin-.gu~sh meanings or conception structures of a word fromsenses.
(We will use file term "meaning" and "con-ceptual structure" interehangely in this context.)
A sin-*'l|ae research reported here is the product of the BerkeleyArtilicial Inteiligt~ce and Natural Language Processing seminar;contribmers include Michael Bravennan, Narciseo Jaramillo,DalJ Jurafsky, Eric Kadson, Marc Lufia, Peter Norris, MichaelSchiff, Nigel Ward, and Dekai Wu.
This research was sponsoredby the Defense Advanced Research Projects Agency (DoD),mo~fitored by Space and Naval Warfare Systems Commandunder Contract N00039-88-C~()292 andby the Office of NavalResearch, under contract N00014-89-J-3205.Yet other senses of "open" have the meaning of caus-ing an information-containing item to come intoexistence (e.g., "open a bank account" or "open a fileoll someone").
This second meaning is probably ba.~'lon the first one.
Also, the various adjective uses (e.g.,"the open door") are separate senses in this view hav-ing some as yet unspecified relation to the sensesdescribed above.
Finally, other words, e.g., "close",have senses that we presume to be related to the varioussenses of "open" just discussed.2.2.
MIDASPreviously, we have succeeded in doing some automaticlexical acquisition by exploiting conventional metaphorsas motivations for linguistic forms.
In particular, Martin(1988) implemented the MIDAS system which both usesmetaphoric word senses to help with language under?407standing, and to extend the lexicon when a new meta-phoric use of a word is encountered.
For example, thesentence "John have Mary a cold."
is presumed tomake recourse to a "a cold is a possession" metaphor.We call such a conventionalized metaphor a core  meta-phor, since it seems to serve as the basis for relatedmetaphoric uses.
Thus, the sentence "John gave Mary acold" is presumed to involve the "infecting with a coldis giving the cold" metaphor, which entails the previous"cold is possession" metaphor.Suppose the system encounters an utterance like "Johngot the flu from Mary", but is not familiar with this useof the verb "get",  nor with the notion of a flu beingtreated as a possession.
Then both the available non?metaphoric sense of "get" ,  along with the metaphorsinvolving diseases and possession, are brought o bear tohypothesize the word sense that might be in playoHypotheses are generated by two kinds of lexical extentsion processes: core extension and similarity extension.Understanding " et a cold" given an understanding of"give a cold" involves core extension, as the core meta~phor "cold is possession" is extende~ to the "ge~i~g"concept; understanding " et the flu ~ given a~a under.
?standing of "get a cold" involves imfiharity extension~as the generalization about a role in the metaphoricstructure must be extended from colds to diseases ingeneral Understanding "get the flu" given an under~standing of "give a cold" involves both kinds of exten-sion.The MIDAS system has been used in conjunction witt~UC to extend metaphoric word senses in the computerdomain.
The following is an example of MIDAS le~i~ing a new sense of the word "kilF~ given that it knowssome metaphoric extension~ of th~s ~nse o~s~de thecomputer do~nMn.# How can I kill a proces~?No valid interpretations.
Attemptingto extend existing metaphor.Searching for related known metaphors.Metaphors found: Kil l-ConversationKil l-Delete-Line Kil l-Sports-DefeatSelecting metaphor Kil l-Conversationto extend from.Attempting a similarity extensioninference.Extending similar metaphorConversation with targetTerminate-Conversation.Kill-conceptAbstracting Terminate-Conversation toancestor conceptCreating new metaphor:Mapping main source concept Kill ingto main target conceptTerminate-Computer-ProcessMapping source role kil ler to targetrole c-proc-termer.Mapping source role ki l l -victim totarget role c-proc-termed.Call ing UC:You can kill a computer p~ocess bytyping "c to the shell?Here MIDA~ tirst retdeves a mm~ber of metaphorsrelated to the input; of these,, "KilbConversafion" ~schosen as most applicable.
A simple similarity exten~,sion is attempted, resulting in a proposed "Temfina~e..Compnterq~ocess" metaphor for interpretation of ~eillpUt?
Th~ inteipretation ~hus provided is passed ale~gto UC, which can answer ~his question.
Meanwhi~e~ d emetaphor is incorporated into OC's k~towledge ba~(~,.which ahows UC~s language generator to use the ,~eterminology in encoding the answer?MIDAS is discussed in detain in Marti~ (1988)o30 Why M~DA~ W~rknWe believe that MIDAS works because it is exploitingmetaphoric subregulafity by a form of analogical rea~soning.
That is, it finds a metaphorical usage that isclosest o the given case according to some conceptualmetric; it then exploits the structure of the prior meta-phor usage to construct an analogous one for the case athand, and proposes this new structure as a hypotheticalword sense.
Note that according to this explanation,metaphor does not play a crucial role in the extensionprocess.
Rather, it is the fact that the metaphor is asubregularity rather than the fact that it is a metaphorthat makes it amenable to analogical exploitation.Analogy, of course, has played a prominent role in tradi-tional inguistics.
Indeed, rather influential linguists (forexample, Paul (1891) and Bloomfield (1933) seemed toattribute all novel language use to analogy.
However,today, analogy seems almost entirely relegated todiachronic processses.
A notable xception to this trend408 2is the wo~k of Skon~n (in press), who appears to advo.~catea vk~w quite similar to our own, although the pri-mary foclL~ of his work is morphological.Analogy has also been widely studied in artificial intelli-gence and cognitive psychology.
The work of C~bonell(1982) and Burstein (1983) is most relevant to our enter-prise, as it explores the role of analogy in knowledgeacquisition.
Similarly, Alterman's (1985, 1988)approach to planning shares ome of the stone concerns.However, many of the details of Carbonell's andAlterman's proposals are specific to problem solving,and Burstem'z wo~k is focused on fommlating cou-straints on rite relations to be considetv.d for analogicalmapping.
"!bus, their work does not appear to have anobvious application to our problem.
Many of the difter~ences between analogical reasoning for problem solvingand language knowledge acquisition are discussed atlength in Martin (1988).Another ihte of related work is the connectionistapproach iinitiated by Rumelhart and McClelland (1987),and explicitly considered as an alterative to acquisitionby analogy by MacWhinney et al (1989).
However,there are numerous reasons we believe an explicitly ana~logical framework to be superior.
The Rumelhart-McClelland model maintains no memory of specificcases, but only a statistical summary of them.
Also, theanalogy-b~L~d model can use its knowledge more tlexi~bly, for example, to infer that a word encountered is filepast tense of a known word, a task that an associationistnetworks could not easily be made to perform.
In addle~Jon, we interpret as evidence supportive of a position?li_ke ours psycholinguistic results uch as those of Cutler(1983) and Butterworth (1983), which suggest thatwords are represented in theh ~ lull "nndecomposed '~~'onn, along with some sorts of relations between ielate~iwords.3.L Other K~nds of Lexical Subregularitiesff MIDAS works by applying analogicM reasoning to~xploit metaphoric subregularities, then the question~ises as what other kinds of lexicM subregularities theremight be.
One set of candidates i approached in the~vork of Brugman (1981, 1984) and Norvig and Lakoff(1987).
In particular, Norvig and Lakoff (1987) offersix types of links between word senses in what they call~exical network theory.
However, their theory is con?
(:erned only with senses of one word.
Also, there appear~o be many more links than these.
Indeed, we have noJ~eason to believe that the number of such subregularitics~s bounded in principle.We present a partial ist of some of the subregularities~ve have encountered.
The list below uses a rather infororeal rule fi~rmat, and gives a couple of examples ofwords to which the rule is applicable.
It is hoped thatexplicating a few examples below will let the readerinfer the meanings of some of the others:(1) function-object-noun -> primary-activity-"detemfinerless"-noun("the bed" ---> "hi bed, go to bed"; "a  school ~> atschool"; "any lunch ~-> at lunch"; "the conference ->in conference")(2) noun ~-> lesembling-in-appearance-noun("tree" ~> "(rose) tree"; "nee" ~-> "(shoe) tree");"tiger" --> "(stuffed) tiger", "pencil" -> "pencil (ofligb0")(3) noun -> having-the-same-functiononoun("bed" -> "bed (of leaves)")(4) noun -> "involve-concretion"-verb("a tree" -> "to tree (a ea0"; "a knife" --> "to knife(someone)")(5) verb -> verb-w~role-splitting("take a book" -> "take a book to Mary", "Johnshaved" -> "John shaved Bill")(6) verb -> profiled-componentoverb("take a book" -> " t~e a book to the Cape")(7) verb--> framedmposifion..verb("take a book" -> " t~c  someone to dinner', "go '~ L_>"go dancing")(8) acfivity-verb~t -> concrefion~.activity-verboi("eat an apple" -> "eat \[a meal\]", "drink a coke" -->"drink \[alcohol\]", "feed the dog" -> "file dog feeds")(9) acfivity-verb-t -> dobj-subj-middle-voice-verbq("drive a car" --> "the car drives well")(10) activity-verbq o-> activity-verb+primaryocategory("John dreamed" -> "John dreamed a dream"; "Johnslept" -> "John slept the sleep of the innocent")(11) activity~verboi -> do-cause-activity-verb-t(patientas subject)("John slept" -> "The bed sleeps five")(12) activity~verb -> activity-of-noun("to cry" -> "a cry (in the wilderness)"; "to punch"-> "a punch (in the mouth)")(13) activity-verb <-> product-of-activity-noun("copy the paper" <-> "a copy of the paper"; xerox,telegram, telegraph)(14) functional-noun-> use-function-verb("the telephone" -> "telephone John"; machine,motorcycle, telegraph)(15) object-noun -> central-component-of-object("a bed" -> "bought abed \[=frame with not mattress\];"an apple" -> "eat an apple \[=without the core\]"))Consider the first rule.
This rule states that, for somenoun whose core meaning is a functional object, there isanother sense, also a noun, that occurs without determi-nation, and means the primary activity associated withthe first sense.
For example, the word "bed" has as acore meaning a functional object used for sleeping.However, the Word can also be used in utterances like"go to bed" and "before bed" (but not, say, "*duringbed").
In these cases, the noun is determinerless, andmeans something akin to sleeping.
Other examplesinclude "jail", "conference", school" and virtuallyall the meal terms, e.g., "lunch", "tea", "dinner".British English allows "in hospital", while AmericanEnglish &~es not.The dialect difference underscores the point that this istruly a subregularity: concepts that might be expressedthis way ,are not necessarily expressed this way.
Also,we chose this example not because it in itself is a partic-ularly important generalization about English, but pre-cisely because it is not.
That is, there appear to be manysuch facts of limited scope, and each of them may beuseful for learning analogous cases.Consider also rule 4, which relates function nouns toverbs.
Examples of this are "tree" as in "The dogtreed the cat" and "knife" as in "The murderer knifedhis victim".
The applicable rule states that the verbmeans ome specific activity involving the core meaningof the noun.
I.e., the verbs are treated as a sort of con-ventionalized denominalization.
Note that he activity ispresumed to be specific, and that the way in which itmust be "concreted" is assumed to be pragmaticallydetermined.
Thus, the rule can only tell us that "tree-ing" involves a tree, but only our world knowledgemight suggest to us that it involves cornering; similarly,the rule can tell us that "knifing" involves the use of aknife, but cannot ell us that it means tabbing aperson,and not say, just cutting.As a final illustration, consider ule 5, so-called "rolesplitting" (this is the same as Norvig and Lakoffssemantic role differentiation link).
This rule suggeststhat, given a verb in which two thematic roles are real-ized by a single complement may have another sense inwhich these two complements are realized separately.For example, in "John took a book from Mary", John isboth the recipient and the agent.
However, in "Johntook a book to Mary", John is only the agent, and Maryis the recipient.
Thus, the sense of "take" involved in4the first sentence, which we suggest corresponds to acore meaning, is the basis for the sense used in thesecond, in which the roles coinciding in the first are real-ized separately.
A similar prediction might be madefrom an intransitive verb like "shave", in which agentand patient coincide, to the existence of a transitive verb"shave" in which the patient is realized separately asthe direct object.
(Of course, the tendency of patients toget realized as direct objects in English should also helpmotivate this fact, and can presumably also be exploitedanalogically.)4.
An Analogy-based Model of Lexical AcquisitionWe have been attempting toextend MIDAS.-style wordhypothesizing to be able to propose new word senses byusing analogy to exploit these other kinds of lexicalsubregularities.
At this point, our work has been ratherpreliminary, but we can at least sketch out the basicarchitecture of our proposal and comment on the prob-lems we have yet to resolve.
(A) Detect unknown word sense.
For example, supposethe system encountered the following phrase:"at breakfast"Suppose further that the function noun "breakfast"were known to the system, but the determinerless u agewere not.
In this case, the system would hypothesizethat it is lacking a word sense because of a failure toparse the sentence.
(B) Find relevant cases/subregularities.
Cues from theinput would be used to suggest prior relevant lexiealknowledge.
In our example, the retrieved cases mightinclude the following:bed-I/bed-3, class-I/class-4Here we have numbered word senses o that the firstelement of each pair designates a sense involving acoremeaning, and the latter a determineless-activity type ofsense.
We may have also already computed and storedrelevant subregularities.
If so, then these would beretrieved as well.Relevant issues here are the indexing and retrieval ofcases and subregularities.
Our assumption isthat we canretrieve relevant cases by a conjunction of simple cues,like "noun", "functional meaning", "extended etex-minerless noun sense", etc., and then rely on the nextphase to discriminate further among these.
(C) Chose the most pertinent case or subregularity.Again, by analogy to MIDAS, some distance metric isused to pick the best datum to analogize from.
In this410 bca~e, perhaps the correct choice would be the following:class- 1/cl~t~s-4One motivation for this selection is that "class" is com-patible with "at" ,  as is the case in point.Finding the right metric is the primary issue here.
TheMIDAS metric is a simple sum of two factors: (i) thelength of the core-relationship from the input source tothe source of the candidate metaphor, and (ii) hierarchi-cal distance between the two concepts.
Both factors aremeasured by the number of links in the representationthat must be traversed to get from one concept o theother.
The hierarchical distance factor of the MIDASmetric seems directly relevant to other cases.
However,there is no obvious counterpart to the core-relationshipcomponent.
One possible reason for this is that met&phoric extensions are more complex than most otherkinds; if so, then the MIDAS metric may still be applica-ble to the other subregularities, which are just simplerspecial cases.
(D) Analogize to a new meaning.
Given the best case orsubregularity, the system will attempt to hypothesize anew word sense.
For example, in the case at hand, wewo~dd like a representation for the meaning in quotes tobe produced.class- 1/class-4 ::breakfast-1/"period of eating breakfast"In Ihe case of MIDAS, the metaphoric structure of pre-vio~ls examples was assumed to be available.
Then,once a best match was established, it is relativelystraightforward togeneralize or extend this structure toapply to the new input.
The same would be true in thegeneral case, provided that the relation between storedpolysemous word senses is readily available.
(E) Determine the extent of generalization.
Supposingthat a single new word sense can be successfully pro-pos~, the que.,;tion arises as to whether just this particu-lar word sense is all the system can hypothesize, orwhether some "local productivity" is possible.
Forexample, if this is the first meal tema the system has seenas having a determinerless activity sense, we suspectthat only the single sense should be generated.
How-ever, if it is the second such meal term, then the first onewould have been the likely basis for the analogy, and ageneralization to meal terms in general may beattempted.09 Record a new entry.
The new sense needs to bestorcA in the lexicon, and indexed for further eference.Thi,; task may interact closely with (E), although gen-eralizing to unattested cases and computing explicitsubregularities are logically independent.There are many additional problems to be addressedbeyond the ones alluded to above.
In particular, there isthe issue of the role of world knowledge in the proposedprocess.
In the example above, the system must knowthat the activity of caring is the primary one associatedwith breakfast.
A more dramatic example is the role ofworld knowledge in hypothesizing the meaning of"treed" in expressions like "the dog treed the cat",assuming that the system is acquainted with the noun"tree".
All an analogical reasoning mechanism can dois suggest hat some specific activity associated withtrees is involved; the application of world knowledgewould have to do the rest.5.
Other Directions of InvestigationWe have also been investigating exploiting subregalari-ties in "intelligent dictionary reading".
This projectinvolves an additional idea, namely, that one could bestuse a dictionary to gain lexical knowledge by bringing tobear on it a fall natural anguage processing capability.One problem we have encountered is that dictionariesare full of inaccuracies about he meaning of words.
Forexample, even relatively good dictionaries have poorentries for the likes of determinerless nouns like "bed".E.g., Webster's New World (Second Edition) simplylists "bedtime" as a sense of "bed"; Longman's Die-tionary of Contemporary English (New Edition) uses"in bed" as an example of the ordinary noun "bed",then explicitly lists the phrase "time for bed" as mean-ing "time to go to sleep", and gives a few other deter-minerless usages, leaving it to the reader to infer a gen-eralization.
* However, a dictionary reader withknowledge of the subregularity mentioned above mightbe able to correct such deficiencies, and come up with abetter meaning that the one the dictionary supplies.Thus, we plan to explore augmenting our intelligent dic-tionary reader with the ability to use subregularities tocompensate for inadequate dictionary entries.We are also attempting to apply the same approach toacquiring the semantics of constructions.
In particular,we are investigating verb-particular combinations andconventionalized noun phrases (e.g., nominal com-pounds).
We are also looking at constructions like theditransitive (i.e., dative alternation), which seem also todisplay a kind of polysemy.
Specifically, Goldberg(1989, 1990) has argued that much of the data on thisconstruction can be accounted for in terms of subclassesthat are conventionally associated with the constructionitself, rather than with lexical rules and transformationsas proposed, for example, by Gropen et al (1989).
Ifso, then the techniques for the acquisition of polysemous*Longman's also defines "make the bed" as "make it ready forsleeping in".
We have no idea how to oope with such errors, butthey do underscore the problem.411lexical items should prove equally applicable to theacquisition of knowledge about such constructions.
Weare attempting todetermine whether this is the case.6.
ReferencesAlterman, Richard.
Adaptive Planning: Refitting OldPlans To New Situations.
In the Proceedings of TheSeventh Annual Conference of the Cognitive ScienceSociety, 1985.Alterman, Richard.
Adaptive Planning.
In CognitiveScience, vol.
12, pp.
393-421, 1988.Bloomfield, L. Language.
New York: Holt, Rinehart &Winston, 1933.Brugman, Claudia.
The Story of Over.
University ofCalifornia, Berkeley M.A.
thesis, unpublished.
Distri-buted by the Indiana University Linguistics Club.
1981.Brugman, Claudia.
The Very Idea: A Case-Study inPolysemy and Cross-Lexical Generalization.
In Papersfrom the Twentieth Regional Meeting of the ChicagoLinguistics Society.
pp.
21-38.
1984.Burstein, Mark H. Concept Formation by IncrementalAnalogical Reasoning and Debugging.
In R. S. Michal-ski, J.
Go CarboneU, & T. M. Mitchell (eds.
), MachineLearning: An Artificial Intelligence Approach, vol.
II.Tioga Press, Palo Alto, California, 1982.Butterworth, B. Lexical representation.
In B. Butter-worth (ed.
), Language Production , vol.
2.
AcademicPress, New York, 1983.Carbonell, Jaime.
Learning by analogy: Formulatingand Generalizing Plans from Past Experience.
In R. S.Michalski, J. G. Carbonell, & T. M. MiteheU (eds.
)Machine Learning: An Artificial Intelligence Approach.Tioga Press, Palo Alto, California, 1982.Cutler, A. Lexical complexity and sentence processing.In G. B. Flores d'Areais & and R. J. Jarvella (eds.
), TheProcess of Language Understanding, pp.
43-79.
Wiley,New York, 1983.Goldberg, Adele.
A Unified Account of the Semanticsof the Ditransitive Construction.
BLS 15, 1989.Goldberg, Adele.
The Inherent Semantics of ArgumentStructure: The Case of the English Ditransitive Con-struction.
Unpublished manuscript, 1990.Granger, R. H. FOUL-UP: A Program that figures outthe meanings of words from context.
In the Proceedingsof the Fifth International Joint Conference onArtificialIntelligence.
Cambridge, MA.
1977.MacWhinney, B.
Competition and Lexical Categoriza-tion.
In R. Corrigan, F. Eckman and M. Noonan,Linguistic Categorization.
John Benjamins PublishingCompany, Amsterdam/Philadelphia, 1989.Martin, James.
Knowledge Acquisition through NaturalLanguage Dialogue.
In the Proceedings of the 2ndConference on Artificial Intelligence Applications.Miami, Florida.
December, 1985.Martin, James.
A Computational Theory of Metaphor.Berkeley Computer Science Technical Report no.UCB/CSD 88/465.
November 1988.Norvig, Peter.
Building a large lexicon with lexical net-work theory.
In the Proceedings of the IJCAI Workshopon Lexical Acquisition.
August 1989.Norvig, Peter and Lakoff, George.
Taking: A Study inLexical Network Theory.
In the Proceedings of theThirteenth Annual Meeting of the Berkeley LinguisticsSociety.
Berkeley, CA.
February 1987.Paul, H. Principles of the History of Language.
Long-mans, Green, London, 1891.Rumelhart, D., & McClelland, J. Learniug the pasttenses of English verbs: Implicit rules of parallel distri-buted processes?
In B. MacWhinney (ed.
), Mechan-isms of Language Acquisition.
Lawrence ErlbaumAssociates, Hillsdale, New Jersey, 1987.Selfridge, M. Computer Modeling of ComprehensionDevelopment.
In W. O. Lehnert & M. H. Ringle, Stra-tegies for Natural Language Processing.
Lawrence Erl-baum Associates, HiUsdale, New Jersey, 1982.Skousen, R. Analogical Models of Language.
Kluwer,Dordrecht, (in press).Wilensky, R., Mayfield, J., Chin, D., Luria, M., Martin,J.
and Wu, D. The Berkeley UNIX Consultant Project.Computational Linguistics 14-4, December 1988.412
