Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 231?237,Denver, Colorado, May 31 ?
June 5, 2015.c?2015 Association for Computational LinguisticsSpinning Straw into Gold: Using Free Text to Train Monolingual AlignmentModels for Non-factoid Question AnsweringRebecca Sharp1, Peter Jansen1, Mihai Surdeanu1, and Peter Clark21University of Arizona, Tucson, AZ, USA2Allen Institute for Artificial Intelligence, Seattle, WA, USA{bsharp,pajansen,msurdeanu}@email.arizona.edupeterc@allenai.orgAbstractMonolingual alignment models have beenshown to boost the performance of questionanswering systems by ?bridging the lexicalchasm?
between questions and answers.
Themain limitation of these approaches is thatthey require semistructured training data in theform of question-answer pairs, which is diffi-cult to obtain in specialized domains or low-resource languages.
We propose two inex-pensive methods for training alignment mod-els solely using free text, by generating ar-tificial question-answer pairs from discoursestructures.
Our approach is driven by two rep-resentations of discourse: a shallow sequen-tial representation, and a deep one based onRhetorical Structure Theory.
We evaluate theproposed model on two corpora from differ-ent genres and domains: one from Yahoo!Answers and one from the biology domain,and two types of non-factoid questions: man-ner and reason.
We show that these align-ment models trained directly from discoursestructures imposed on free text improve per-formance considerably over an information re-trieval baseline and a neural network languagemodel trained on the same data.1 IntroductionQuestion Answering (QA) is a challenging task thatdraws upon many aspects of NLP.
Unlike searchor information retrieval, answers infrequently con-tain lexical overlap with the question (e.g.
Whatshould we eat for breakfast?
?
Zoe?s Diner hasgood pancakes), and require QA models to drawupon more complex methods to bridge this ?lexicalchasm?
(Berger et al, 2000).
These methods rangefrom robust shallow models based on lexical seman-tics, to deeper, explainably-correct, but much morebrittle inference methods based on first order logic.Berger et al (2000) proposed that this ?lexicalchasm?
might be partially bridged by repurposingstatistical machine translation (SMT) models forQA.
Instead of translating text from one language toanother, these monolingual alignment models learnto translate from question to answer1, learning com-mon associations from question terms such as eat orbreakfast to answer terms like kitchen, pancakes, orcereal.While monolingual alignment models have en-joyed a good deal of recent success in QA (seerelated work), they have expensive training datarequirements, requiring a large set of aligned in-domain question-answer pairs for training.
For low-resource languages or specialized domains like sci-ence or biology, often the only option is to enlist adomain expert to generate gold QA pairs ?
a processthat is both expensive and time consuming.
All ofthis means that only in rare cases are we accordedthe luxury of having enough high-quality QA pairsto properly train an alignment model, and so thesemodels are often underutilized or left struggling forresources.Making use of recent advancements in discourseparsing (Feng and Hirst, 2012), here we address thisissue, and investigate whether alignment models forQA can be trained from artificial question-answerpairs generated from discourse structures imposedon free text.
We evaluate our methods on twocorpora, generating alignment models for an open-domain community QA task using Gigaword2, andfor a biology-domain QA task using a biology text-book.1In practice, alignment for QA is often done from answer toquestion, as answers tend to be longer and provide more oppor-tunity for association (Surdeanu et al, 2011).2LDC catalog number LDC2012T21231The contributions of this work are:1.
We demonstrate that by exploiting the dis-course structure of free text, monolingual align-ment models can be trained to surpass the per-formance of models built from expensive in-domain question-answer pairs.2.
We compare two methods of discourse pars-ing: a simple sequential model, and a deepmodel based on Rhetorical Structure Theory(RST) (Mann and Thompson, 1988).
We showthat the RST-based method captures within andacross-sentence alignments and performs bet-ter than the sequential model, but the sequentialmodel is an acceptable approximation when adiscourse parser is not available.3.
We evaluate the proposed methods on two cor-pora, including a low-resource domain wheretraining data is expensive (biology).4.
We experimentally demonstrate that mono-lingual alignment models trained using ourmethod considerably outperform state-of-the-art neural network language models in low re-source domains.2 Related WorkLexical semantic models have shown promise inbridging Berger et al?s (2000) ?lexical chasm.?
Ingeneral, these models can be classified into align-ment models (Echihabi and Marcu, 2003; Soricutand Brill, 2006; Riezler et al, 2007; Surdeanu etal., 2011; Yao et al, 2013) which require structuredtraining data, and language models (Jansen et al,2014; Sultan et al, 2014; Yih et al, 2013), whichoperate over free text.
Here, we close this gap in re-source availability by developing a method to trainan alignment model over free text by making use ofdiscourse structures.Discourse has been previously applied to QA tohelp identify answer candidates that contain ex-planatory text (e.g.
Verberne et al (2007)).
Jansenet al (2014) proposed a reranking model that usedboth shallow and deep discourse features to iden-tify answer structures in large answer collectionsacross different tasks and genres.
Here we use dis-course to impose structure on free text to createinexpensive knowledge resources for monolingualalignment.
Our work is conceptually complemen-tary to that of Jansen et al ?
where they exploredHe makes it each autumn .He uses the apples grown in his orchardBob toldZoey he likescider He usesthe apples grown inhis orchard.
.He makes it each autumnBob told Zoey he likes cider .. .Bob told Zoey he likes cider.
He uses the apples grown in his orchard.
He makes it each autumn.Sequential ModelRST Modelelaborationelaborationattribution elaborationsequentialsequentialFigure 1: An example of the alignments produced bythe two discourse models.
The sequential model alignspairs of consecutive sentences, capturing intersentenceassociations such as cider?apples, and orchard?autumn.The RST model generates alignment pairs from par-ticipants in all (binary) discourse relations, capturingboth intrasentence and intersentence alignments, includ-ing apples?orchard, cider?apples, and cider?autumn.largely unlexicalized discourse structures to identifyexplanatory text, we use discourse to learn lexical-ized models for semantic similarity.Our work is conceptually closest to that of Hicklet al (2006), who created artificially aligned pairsfor textual entailment.
Taking advantage of thestructure of news articles, wherein the first sentencetends to provide a broad summary of the article?scontents, Hickl et al aligned the first sentence ofeach article with its headline.
By making use of au-tomated discourse parsing, here we go further andimpose alignment structure over an entire text.3 ApproachA written text is not simply a collection of sentences,but rather a flowing narrative where sentences andsentence elements depend on each other for meaning?
a concept known as cohesion (Halliday and Hasan,2014).
Here we examine two methods for generat-ing alignment training data from free text that makeuse of cohesion: a shallow method that uses only in-tersentence structures, and a deep method that usesboth intrasentence and intersentence structures.
Weadditionally attempt to separate the contribution ofdiscourse from that of alignment in general by com-paring these models against a baseline alignmentmodel which aligns sentences at random.The first model, the sequential discourse model(SEQ), considers that each sentence continues the232narrative of the previous one, and creates artificialquestion-answer pairs from all pairs of consecutivesentences.
Thus, this model takes advantage of in-tersentence cohesion by aligning the content words3in each sentence with the content words in the fol-lowing sentence.
For example, in the passage in Fig-ure 1, this model would associate cider in the firstsentence with apples and orchard in the second sen-tence.The second model uses RST to capture discoursecohesion both within and across sentence bound-aries.
We extracted RST discourse structures usingan in-house parser (Surdeanu et al, 2015), whichfollows the architecture introduced by Hernault etal.
(2010) and Feng and Hirst (2012).
The parserfirst segments text into elementary discourse units(EDUs), which may be at sub-sentence granular-ity, then recursively connects neighboring units withbinary discourse relations, such as Elaboration orContrast.4Our parser differs from previous workwith respect to feature generation in that we imple-ment all features that rely on syntax using solelydependency syntax.
For example, a crucial featureused by the parser is the dominance relations ofSoricut and Marcu (2003), which capture syntacticdominance between discourse units located in thesame sentence.
While originally these dominancerelations were implemented using constituent syn-tax, we provide an equivalent implementation thatrelies on dependency syntax.
The main advantage tothis approach is speed: the resulting parser performsat least an order of magnitude faster than the parserof Feng and Hirst (2012).Importantly, we generate artificial alignment pairsfrom this imposed structure by aligning the govern-ing text (nucleus) with its dependent text (satellite).5Turning again to the example in Figure 1, this RST-based model captures additional alignments that areboth intrasentence, e.g., apples?orchard, and inter-sentence, e.g., cider?autumn.3In pilot experiments, we found that aligning only nouns,verbs, adjectives, and adverbs yielded higher performance.4The RST parser performs better on relations which occurmore frequently.
We use only relations that occurred at least1% of the time.
This amounted to six relations: elaboration,attribution, background, contrast, same-unit, and joint.
Usingall relations slightly improves performance by 0.3% P@1.5Pilot experiments showed that this direction of alignmentperformed better than aligning from satellite to nucleus.4 Models and FeaturesWe evaluate the contribution of these align-ment models using a standard reranking architec-ture (Jansen et al, 2014).
The initial ranking of can-didate answers is done using a shallow candidate re-trieval (CR) component.6Then, these answers arereranked using a more expressive model that incor-porates alignment features alongside the CR score.As a learning framework we use SVMrank, a Sup-port Vector Machine tailored for ranking.7We com-pare this alignment-based reranking model againstone that uses a state-of-the-art recurrent neural net-work language model (RNNLM) (Mikolov et al,2010; Mikolov et al, 2013), which has been success-fully applied to QA previously (Yih et al, 2013).Alignment Model: The alignment matrices weregenerated with IBM Model 1 (Brown et al, 1993)using GIZA++ (Och and Ney, 2003), and the cor-responding models were implemented as per Sur-deanu et al (2011) with a global alignment prob-ability.
We extend this alignment model with fea-tures from Fried et al (In press) that treat each(source) word?s probability distribution (over des-tination words) in the alignment matrix as a dis-tributed semantic representation, and make use theJensen-Shannon distance (JSD)8between these con-ditional distributions.
A summary of all these fea-tures is shown in Table 1.RNNLM: We learned word embeddings using theword2vec RNNLM of Mikolov et al (2013),and include the cosine similarity-based features de-scribed in Table 1.5 ExperimentsWe tested our approach in two different domains,open-domain and cellular biology.
For consistencywe use the same corpora as Jansen et al (2014),which are described briefly here.Yahoo!
Answers (YA): Ten thousand open-domainhow questions were randomly chosen from the Ya-6We use the same cosine similarity between question andanswer lemmas as Jansen et al (2014), weighted using tf.idf.7http://www.cs.cornell.edu/people/tj/svm_light/svm_rank.html8Jensen-Shannon distance is based on Kullback-Liebler di-vergence but is a distance metric (finite and symmetric).233Feature Group Feature DescriptionsAlignmentModelsGlobal Alignment Probability p(Q|A) according to IBM Model 1 (Brown et al, 1993)Jenson-Shannon Distance (JSD) Pairwise JSDs were found between the probability distribution of eachcontent word in the question and those in the answer.
The mean, mini-mum, and maximum JSD values were used as features.
Additionally,composite vectors were formed which represented the entire questionand the entire answer and the overall JSD between these two vectorswas also included as a feature.
See Fried et.
al (In press) for additionaldetails.RNNLMCosine Similarity Similar to Jansen et al (2014), we include as features the maximumand average pairwise cosine similarity between question and answerwords, as well as the overall similarity between the composite questionand answer vectors.Table 1: Feature descriptions for alignment models and RNNLM baseline.hoo!
Answers9community question answering cor-pus and divided: 50% for training, 25% for devel-opment, and 25% for test.
Candidate answers for agiven question are selected from the correspondinganswers proposed by the community (each questionhas an average of 9 answers).Biology QA (Bio): 183 how and 193 why questionsin the cellular biology domain were hand-crafted bya domain expert, and paired with gold answers in theCampbell?s Biology textbook (Reece et al, 2011).Each paragraph in the textbook was considered as acandidate answer.
As there were few questions, fivefold cross-validation was used with three folds fortraining, one for development, and one for test.Alignment Corpora: To train the alignment mod-els we generated alignment pairs from two differ-ent resources: Annotated Gigaword (Napoles et al,2012) for YA, and the textbook for Bio.
Each wasdiscourse parsed with the RST discourse parser de-scribed in Section 3, which is implemented in theFastNLPProcessor toolkit10, using the MaltParser11for syntactic analysis.5.1 Results and DiscussionFigure 2 shows the performance of the discoursemodels against the number of documents used totrain the alignment model.12We used the standardimplementation for P@1 (Manning et al, 2008)with the adaptations for Bio described in Jansen etal.
(2014).
We address the following questions.9http://answers.yahoo.com10http://github.com/sistanlp/processors11http://www.maltparser.org/12For space reasons the graph for Bio how is not shown, butthe pattern is essentially identical to Bio why.BioYAFigure 2: Overall performance for the two discourse-basedalignment models, compared against the CR baseline,random baselines, and a RNNLM-based reranker.
The xaxis indicates the number of training documents used toconstruct all models.
Each point represents the averageof 10 samples of training documents.How does the performance of the RST and SEQmodels compare?
Comparing the two principalalignment models, the RST-based model signifi-cantly outperforms the SEQ model by about 0.5%P@1 in both domains (p < 0.001 for Bio and p <0.01 for YA)13.
This shows that deep discourse anal-13All reported statistics were performed at the endpoints, i.e.,when all training data is used, using bootstrap resampling with234ysis (as imperfect as it is today) is beneficial.How does the performance of the RST modelcompare to a model trained on in-domain pairs?Both the RST and SEQ results for YA are higherthan that of an alignment model trained on explicitin-domain question-answer pairs.
Fried et.
al (Inpress) trained an identical alignment model usingapproximately 65k QA pairs from the YA corpus,and report a performance of 27.24% P@1, or nearly2 points lower than our model trained using 10,000Gigaword documents.
This is an encouraging re-sult, which further demonstrates that: (a) discourseanalysis can be exploited to generate artificial semi-structured data for alignment, and (b) the sequen-tial model, which also outperforms Fried et.
al, canbe used as a reasonable proxy for discourse when aparser is not available.How does the performance of the RST modelcompare to previous work?
Comparing our workto Jansen et al (2014), the most relevant prior work,we notice two trends.
First, our discourse-basedalignment models outperform their CR + RNNLMmodel, which peaks at 26.6% P@1 for YA and31.7% for Bio why.
While some of this differencecan be assigned to implementation differences (e.g.,we consider only content words for both alignmentand RNNLM, where they used all words), this re-sult again emphasizes the value of our approach.Second, the partially lexicalized discourse structuresused by Jansen et.
al to identify explanatory text incandidate answers perform better than our approach,which relies solely on lexicalized alignment.
How-ever, we expect that our two approaches are comple-mentary, because they address different aspects ofthe QA task (structure vs. similarity).How do the RST and SEQ models compare to thenon-alignment baselines?
In Bio, both the RSTand SEQ alignment models significantly outperformthe RNNLM and CR baselines (p < 0.001).
In YA,the RST and SEQ models significantly outperformthe CR baseline (p < 0.001), and though they con-siderably outperform the the RNNLM baseline formost training document sizes, when all 10,000 doc-uments are used for training, they do not performbetter.
This shows that alignment models are more10,000 iterations.robust to little training data, but RNNLMs catch upwhen considerable data is available.How does the SEQ model compare to the RNDbaseline?
In Bio, the SEQ model significantlyoutperforms the RND baseline (p < 0.001) but inYA it does not.
This is likely due to differencesin the size of the document which was randomized.In YA, the sentences were randomized within Gi-gaword articles, which are relatively short (averag-ing 19 sentences), whereas in Bio the randomizationwas done at the textbook level.
In practice, as docu-ment size decreases, the RND model approaches theSEQ model.Why does performance plateau in YA and not inBio?
With Bio, we exploit all of the limited in-domain training data, and continue to see perfor-mance improvements.
With YA, however, perfor-mance asymptotes for the alignment models whentrained beyond 10,000 documents, or less than 1%of the Gigaword corpus.
Similarly, when trainedover the entirety of Gigaword (two orders of mag-nitude more data), our RNNLM improves onlyslightly, peaking at approximately 30.5% P@1 (or, alittle over 1% P@1 higher).
We hypothesize that thislimitation comes from failing to take context into ac-count.
In open domains, alignments such as apple ?orchard may interfere with those from different con-texts, e.g., apple ?
computer, and add noise to theanswer selection process.6 ConclusionWe propose two inexpensive methods for trainingalignment models using solely free text, by gener-ating artificial question-answer pairs from discoursestructures.
Our experiments indicate that thesemethods are a viable solution for constructing state-of-the-art QA systems for low-resource domains, orlanguages where training data is expensive and/orlimited.
Since alignment models have shown utilityin other tasks (e.g.
textual entailment), we hypothe-size that these methods for creating inexpensive andhighly specialized training data could be useful fortasks other than QA.AcknowledgmentsWe thank the Allen Institute for AI for funding thiswork.235ReferencesAdam Berger, Rich Caruana, David Cohn, Dayne Frey-tag, and Vibhu Mittal.
2000.
Bridging the lexicalchasm: Statistical approaches to answer finding.
InProceedings of the 23rd Annual International ACM SI-GIR Conference on Research &Development on Infor-mation Retrieval, Athens, Greece.Peter F. Brown, Stephen A. Della Pietra, Vincent J. DellaPietra, and Robert L. Mercer.
1993.
The mathematicsof statistical machine translation: Parameter estima-tion.
Computational Linguistics, 19(2):263?311.Abdessamad Echihabi and Daniel Marcu.
2003.
Anoisy-channel approach to question answering.
InProceedings of the 41st Annual Meeting on Associ-ation for Computational Linguistics-Volume 1, pages16?23.
Association for Computational Linguistics.Vanessa Wei Feng and Graeme Hirst.
2012.
Text-leveldiscourse parsing with rich linguistic features.
In Pro-ceedings of the Association for Computational Lin-guistics.Daniel Fried, Peter Jansen, Gustave Hahn-Powell, MihaiSurdeanu, and Peter Clark.
In press.
Higher-order lex-ical semantic models for non-factoid answer rerank-ing.
Transactions of the Association for Computa-tional Linguistics.Michael Alexander Kirkwood Halliday and RuqaiyaHasan.
2014.
Cohesion in english.
Routledge.H.
Hernault, H. Prendinger, D. duVerle, and M. Ishizuka.2010.
HILDA: A discourse parser using support vec-tor machine classification.
Dialogue and Discourse,1(3):1?33.Andrew Hickl, John Williams, Jeremy Bensley, KirkRoberts, Bryan Rink, and Ying Shi.
2006.
Recogniz-ing textual entailment with lccs groundhog system.
InProceedings of the Second PASCAL Challenges Work-shop.Peter Jansen, Mihai Surdeanu, and Peter Clark.
2014.Discourse complements lexical semantics for non-factoid answer reranking.
In Proceedings of the 52ndAnnual Meeting of the Association for ComputationalLinguistics (ACL).William C. Mann and Sandra A. Thompson.
1988.Rhetorical structure theory: Toward a functional the-ory of text organization.
Text, 8(3):243?281.Christopher D. Manning, Prabhakar Raghavan, and Hin-rich Sch?utze.
2008.
Introduction to Information Re-trieval.
Cambridge University Press.Tomas Mikolov, Martin Karafiat, Lukas Burget, Jan Cer-nocky, and Sanjeev Khudanpur.
2010.
Recurrent neu-ral network based language model.
In Proceedingsof the 11th Annual Conference of the InternationalSpeech Communication Association (INTERSPEECH2010).Tomas Mikolov, Kai Chen, Greg Corrado, and JeffreyDean.
2013.
Efficient estimation of word represen-tations in vector space.
In Proceedings of the In-ternational Conference on Learning Representations(ICLR).Courtney Napoles, Matthew Gormley, and BenjaminVan Durme.
2012.
Annotated gigaword.
In Pro-ceedings of the Joint Workshop on Automatic Knowl-edge Base Construction andWeb-scale Knowledge Ex-traction, AKBC-WEKEX ?12, pages 95?100, Strouds-burg, PA, USA.
Association for Computational Lin-guistics.Franz Josef Och and Hermann Ney.
2003.
A system-atic comparison of various statistical alignment mod-els.
Computational Linguistics, 29(1):19?51.J.B.
Reece, L.A. Urry, M.L.
Cain, S.A. Wasserman, andP.V.
Minorsky.
2011.
Campbell Biology.
PearsonBenjamin Cummings.Stefan Riezler, Alexander Vasserman, Ioannis Tsochan-taridis, Vibhu Mittal, and Yi Liu.
2007.
Statisticalmachine translation for query expansion in answer re-trieval.
In Proceedings of the 45th Annual Meeting ofthe Association for Computational Linguistics (ACL),pages 464?471, Prague, Czech Republic.Radu Soricut and Eric Brill.
2006.
Automatic questionanswering using the web: Beyond the factoid.
Journalof Information Retrieval - Special Issue on Web Infor-mation Retrieval, 9(2):191?206.R.
Soricut and D. Marcu.
2003.
Sentence level discourseparsing using syntactic and lexical information.
InProceedings of the Human Language Technology andNorth American Association for Computational Lin-guistics Conference.Md.
Arafat Sultan, Steven Bethard, and Tamara Sum-ner.
2014.
Back to basics for monolingual align-ment: Exploiting word similarity and contextual evi-dence.
Transactions of the Association for Computa-tional Linguistics, 2:219?230.Mihai Surdeanu, Massimiliano Ciaramita, and HugoZaragoza.
2011.
Learning to rank answers to non-factoid questions from web collections.
Computa-tional Linguistics, 37(2):351?383.Mihai Surdeanu, Thomas Hicks, and Marco A.Valenzuela-Esc?arcega.
2015.
Two practical rhetor-ical structure theory parsers.
In Proceedings of theNorth American Chapter of the Association for Com-putational Linguistics (NAACL): Software Demonstra-tions.Susan Verberne, Lou Boves, Nelleke Oostdijk, Peter-Arno Coppen, et al 2007.
Discourse-based answer-ing of why-questions.
Traitement Automatique desLangues, Discours et document: traitements automa-tiques, 47(2):21?41.236Xuchen Yao, Benjamin Van Durme, Chris Callison-Burch, and Peter Clark.
2013.
Semi-markov phrase-based monolingual alignment.
In Proceedings ofEMNLP.Wen-tau Yih, Ming-Wei Chang, Christopher Meek, andAndrzej Pastusiak.
2013.
Question answering usingenhanced lexical semantic models.
In Proceedings ofthe 51st Annual Meeting of the Association for Com-putational Linguistics (ACL).237
