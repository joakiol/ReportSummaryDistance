Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 114?118,Dublin, Ireland, August 23-24, 2014.AUEB: Two Stage Sentiment Analysis of Social Network MessagesRafael Michael Karampatsis, John Pavlopoulos and Prodromos Malakasiotismpatsis13@gmail.com, annis@aueb.gr, rulller@aueb.grDepartment of InformaticsAthens University of Economics and BusinessPatission 76, GR-104 34 Athens, GreeceAbstractThis paper describes the system submit-ted for the Sentiment Analysis in TwitterTask of SEMEVAL 2014 and specificallythe Message Polarity Classification sub-task.
We used a 2?stage pipeline approachemploying a linear SVM classifier at eachstage and several features including mor-phological features, POS tags based fea-tures and lexicon based features.1 IntroductionRecently, Twitter has gained significant popularityamong the social network services.
Lots of usersoften use Twitter to express feelings or opinionsabout a variety of subjects.
Analysing this kind ofcontent can lead to useful information for fields,such as personalized marketing or social profiling.However such a task is not trivial, because the lan-guage used in Twitter is often informal presentingnew challenges to text analysis.In this paper we focus on sentiment analysis,the field of study that analyzes people?s sentimentand opinions from written language (Liu, 2012).Given some text (e.g., tweet), sentiment analysissystems return a sentiment label, which most oftenis positive, negative, or neutral.
This classificationcan be performed directly or in two stages; in thefirst stage the system examines whether the textcarries sentiment and in the second stage, the sys-tem decides for the sentiment?s polarity (i.e., posi-tive or negative).1This decomposition is based onthe assumption that subjectivity detection and sen-timent polarity detection are different problems.This work is licensed under a Creative Commons At-tribution 4.0 International Licence.
Page numbers and pro-ceedings footer are added by the organisers.
Licence details:http://creativecommons.org/licenses/by/4.0/1For instance a 2?stage approach is better suited to sys-tems that focus on subjectivity detection; e.g., aspect basedsentiment analysis systems which extract aspect terms onlyfrom evaluative texts.We choose to follow the 2?stage approach, be-cause it allows us to focus on each of the two prob-lems separately (e.g., features, tuning, etc.).
In thefollowing we will describe the system with whichwe participated in the Message Polarity Classi-fication subtask of Sentiment Analysis in Twit-ter (Task 9) of SEMEVAL 2014 (Rosenthal et al.,2014).
Specifically Section 2 describes the dataprovided by the organizers of the task.
Sections 3and 4 present our system and its performance re-spectively.
Finally, Section 5 concludes and pro-vides hints for future work.2 DataAt first, we describe the data used for this year?stask.
For system tuning the organizers released thetraining and development data of SEMEVAL 2013Task 2 (Wilson et al., 2013).
Both these sets areallowed to be used for training.
The organizersalso provided the test data of the same Task to beused for development only.
As argued in (Malaka-siotis et al., 2013) these data suffer from class im-balance.
Concerning the test data, they contained8987 messages broken down in the following 5datasets:?
LJ14: 2000 sentences from LIVEJOURNAL.?
SMS13: SMS test data from last year.?
TW13: Twitter test data from last year.?
TW14: 2000 new tweets.?
TWSARC14: 100 tweets containing sarcasm.The details of the test data were made available tothe participants only after the end of the Task.
Re-call that SMS13and TW13were also provided asdevelopment data.
In this way the organizers wereable to check, i) the progress of the systems sincelast year?s task, and ii) the generalization capabil-ity of the participating systems.1143 System OverviewThe main objective of our system is to detectwhether a message M expresses positive, negativeor no sentiment.
To achieve that we follow a 2?stage approach.
During the first stage we detectwhether M expresses sentiment (?subjective?)
ornot; this process is called subjectivity detection.In the second stage we classify the ?subjective?messages of the first stage as ?positive?
or ?neg-ative?.
Both stages utilize a Support Vector Ma-chine (SVM (Vapnik, 1998)) classifier with lin-ear kernel.2Similar approaches have also beenproposed in (Pang and Lee, 2004; Wilson et al.,2005; Barbosa and Feng, 2010; Malakasiotis et al.,2013).
Finally, we note that the 2?stage approach,in datasets such the one here (Malakasiotis et al.,2013), alleviates the class imbalance problem.3.1 Data preprocessingA very essential part of our system is data pre-processing.
At first, each message M is passedthrough a twitter specific tokenizer and part-of-speech (POS) tagger (Owoputi et al., 2013) to ob-tain the tokens and the corresponding POS tags,which are necessary for some sets of features.3We then use a dictionary to replace any slang withthe actual text.4We also normalize the text ofeach message by combining a trie data structure(De La Briandais, 1959) with an English dictio-nary.5In more detail, we replace every token of Mnot in the dictionary with the most similar word ofthe dictionary.
Finally, we obtain POS tags of allthe new tokens.3.2 Sentiment lexiconsAnother key attribute of our system is the use ofsentiment lexicons.
We have used the following:?
HL (Hu and Liu, 2004).?
SENTIWORDNET (Baccianella et al., 2010).?
SENTIWORDNET lexicon with POS tags(Baccianella et al., 2010).?
AFINN (Nielsen, 2011).?
MPQA (Wilson et al., 2005).2We used the LIBLINEAR distribution (Fan et al., 2008)3Tokens could be words, emoticons, hashtags, etc.
Nolemmatization or stemming has been applied4See http://www.noslang.com/dictionary/.5We used the OPENOFFICE dictionary?
NRC Emotion lexicon (Mohammad and Tur-ney, 2013).?
NRC S140 lexicon (Mohammad et al.,2013).?
NRC Hashtag lexicon (Mohammad et al.,2013).?
The three lexicons created from the trainingdata in (Malakasiotis et al., 2013).Note that concerning the MPQA Lexicon weapplied preprocessing similar to Malakasiotis et al.
(2013) to obtain the following sub?lexicons:S+: Contains strong subjective expressions withpositive prior polarity.S?
: Contains strong subjective expressions withnegative prior polarity.S?
: Contains strong subjective expressions witheither positive or negative prior polarity.S0: Contains strong subjective expressions withneutral prior polarity.W+: Contains weak subjective expressions withpositive prior polarity.W?
: Contains weak subjective expressions withnegative prior polarity.W?
: Contains weak subjective expressions witheither positive or negative prior polarity.W0: Contains weak subjective expressions withneutral prior polarity.3.3 Feature engineeringOur system employs several types of featuresbased on morphological attributes of the mes-sages, POS tags, and lexicons of section 3.2.63.3.1 Morphological features?
The existence of elongated tokens (e.g.,?baaad?).?
The number of elongated tokens.?
The existence of date references.?
The existence of time references.6All the features are normalized to [?1, 1]115?
The number of tokens that contain only uppercase letters.?
The number of tokens that contain both upperand lower case letters.?
The number of tokens that start with an uppercase letter.?
The number of exclamation marks.?
The number of question marks.?
The sum of exclamation and question marks.?
The number of tokens containing only excla-mation marks.?
The number of tokens containing only ques-tion marks.?
The number of tokens containing only excla-mation or question marks.?
The number of tokens containing only ellip-sis (...).?
The existence of a subjective (i.e., positive ornegative) emoticon at the message?s end.?
The existence of an ellipsis and a link at themessage?s end.?
The existence of an exclamation mark at themessage?s end.?
The existence of a question mark at the mes-sage?s end.?
The existence of a question or an exclamationmark at the message?s end.?
The existence of slang.3.3.2 POS based features?
The number of adjectives.?
The number of adverbs.?
The number of interjections.?
The number of verbs.?
The number of nouns.?
The number of proper nouns.?
The number of urls.?
The number of subjective emoticons.7?
The number of positive emoticons.8?
The number of negative emoticons.9?
The average, maximum and minimum F1scores of the message?s POS bigrams for thesubjective and the neutral classes.10?
The average, maximum and minimum F1scores of the message?s POS bigrams for thepositive and the negative classes.11For a bigram b and a class c, F1is calculated as:F1(b, c) =2 ?
Pre(b, c) ?Rec(b, c)Pre(b, c) +Rec(b, c)(1)where:Pre(b, c) =#messages of c containing b#messages containing b(2)Rec(b, c) =#messages of c containing b#messages of c(3)3.3.3 Sentiment lexicon based featuresFor each lexicon we use seven different featuresbased on the scores provided by the lexicon foreach word present in the message.12?
Sum of scores.?
Maximum of scores.?
Minimum of scores.?
Average of scores.?
The count of words with scores.?
The score of the last word of the message thatappears in the lexicon.?
The score of the last word of the message.7This feature is used only for subjectivity detection.8This feature is used only for polarity detection.9This feature is used only for polarity detection.10This feature is used only for subjectivity detection.11This feature is used only for polarity detection.12If a word does not appear in the lexicon it is assignedwith a score of 0 and it is not considered in the calculation ofthe average, maximum, minimum and count scores.
Also, wehave removed from SENTIWORDNET any instances havingpositive and negative scores that sum to zero.
Moreover, theMPQA lexicon does not provide scores, so, for each word inthe lexicon we assume a score equal to 1.116We also created features based on the Pre andF1scores of MPQA and the train data generatedlexicons in a similar manner to that described in(Malakasiotis et al., 2013), with the difference thatthe features are stage dependent.
Thus, for subjec-tivity detection we use the subjective and neutralclasses and for polarity detection we use the posi-tive and negative classes to compute the scores.3.3.4 Miscellaneous featuresNegation.
Negation not only is a good subjec-tivity indicator but it also may change thepolarity of a message.
We therefore add 7more features, one indicating the existenceof negation, and the remaining six indicat-ing the existence of negation that precedeswords from lexicons S?, S+, S?, W?, W+and W?.13Each feature is used in the appro-priate stage.14We have not implement thistype of feature for other lexicons but it mightbe a good addition to the system.Carnegie Mellon University?s Twitter clusters.Owoputi et al.
(2013) released a dataset of938 clusters containing words coming fromtweets.
Words of the same clusters sharesimilar attributes.
We try to exploit thisobservation by adding 938 features, each ofwhich indicates if a message?s token appearsor not in the corresponding attributes.3.4 Feature SelectionTo allow our model to better scale on unseen datawe have performed feature selection.
More specif-ically, we first merged training and developmentdata of SEMEVAL 2013 Task 2.
Then, we rankedthe features with respect to their information gain(Quinlan, 1986) on this dataset.
To obtain the bestset of features we started with a set containing thetop 50 features and we kept adding batches of 50features until we have added all of them.
At eachstep we evaluated the corresponding feature set onthe TW13and SMS13datasets and chose the fea-ture set with the best performance.
This resulted ina system which used the top 900 features for Stage1 and the top 1150 features for Stage 2.13We use a list of words with negation.
We assume that atoken precedes a word if it is in a distance of at most 5 tokens.14The features concerning S?and W?are used in subjec-tivity detection and the remaining four in polarity detection.Test Set AUEB Median BestLJ1470.75 65.48 74.84SMS1364.32 57.53 70.28TW1363.92 62.88 72.12TW1466.38 63.03 70.96TWSARC1456.16 45.77 58.16AVGall64.31 56.56 68.78AVG1464.43 57.97 67.62Table 1: F1(?)
scores per dataset.Test Set RankingLJ149/50SMS138/50TW1321/50TW1414/50TWSARC144/50AVGall6/50AVG145/50Table 2: Rankings of our system.4 Experimental ResultsThe official measure of the Task is the average F1score of the positive and negative classes (F1(?
)).Table 1 illustrates the F1(?)
score per evaluationdataset achieved by our system along with the me-dian and best F1(?).
In the same table AVGallcorresponds to the average F1(?)
across the fivedatasets while AVG14corresponds to the averageF1(?)
across LJ14, TW14and TWSARC14.
Weobserve that in all cases our results are above themedian.
Table 2 illustrates the ranking of our sys-tem according to F1(?).
Our system ranked 6thaccording to AVGalland 5th according to AVG14among the 50 participating systems.
Note that ourbest results were achieved on the new test sets(LJ14, TW14, TWSARC14) meaning that our sys-tem has a good generalization ability.5 Conclusion and future workIn this paper we presented our approach for theMessage Polarity Classification subtask of theSentiment Analysis in Twitter Task of SEMEVAL2014.
We proposed a 2?stage pipeline approach,which first detects sentiment and then decidesabout its polarity.
The results indicate that our sys-tem handles well the class imbalance problem andhas a good generalization ability.
A possible ex-planation is that we do not use bag-of-words fea-117tures which often suffer from over?fitting.
Never-theless, there is still some room for improvement.A promising direction would be to improve the1st stage (subjectivity detection) either by addingmore data or by adding more features, mostly be-cause the performance of stage 1 greatly affectsthat of stage 2.
Finally, the addition of more datafor the negative class on stage 2 might be a goodimprovement because it would further reduce theclass imbalance of the training data for this stage.ReferencesStefano Baccianella, Andrea Esuli, and Fabrizio Se-bastiani.
2010.
Sentiwordnet 3.0: An enhancedlexical resource for sentiment analysis and opin-ion mining.
In Nicoletta Calzolari (ConferenceChair), Khalid Choukri, Bente Maegaard, JosephMariani, Jan Odijk, Stelios Piperidis, Mike Ros-ner, and Daniel Tapias, editors, Proceedings of theSeventh International Conference on Language Re-sources and Evaluation (LREC?10), Valletta, Malta,may.Luciano Barbosa and Junlan Feng.
2010.
Robust sen-timent detection on twitter from biased and noisydata.
In Proceedings of the 23rd InternationalConference on Computational Linguistics: Posters,COLING ?10, pages 36?44, Beijing, China.Rene De La Briandais.
1959.
File searching usingvariable length keys.
In Papers Presented at the theMarch 3-5, 1959, Western Joint Computer Confer-ence, IRE-AIEE-ACM ?59 (Western), pages 295?298, New York, NY, USA.Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-Rui Wang, and Chih-Jen Lin.
2008.
Liblinear: Alibrary for large linear classification.
The Journal ofMachine Learning Research, 9:1871?1874.Minqing Hu and Bing Liu.
2004.
Mining and summa-rizing customer reviews.
In Proceedings of the TenthACM SIGKDD International Conference on Knowl-edge Discovery and Data Mining, KDD ?04, pages168?177, New York, NY, USA.Bing Liu.
2012.
Sentiment analysis and opinion min-ing.
Synthesis Lectures on Human Language Tech-nologies, 5(1):1?167.Prodromos Malakasiotis, Rafael Michael Karampat-sis, Konstantina Makrynioti, and John Pavlopoulos.2013.
nlp.cs.aueb.gr: Two stage sentiment analysis.In Second Joint Conference on Lexical and Com-putational Semantics (*SEM), Volume 2: Proceed-ings of the Seventh International Workshop on Se-mantic Evaluation (SemEval 2013), pages 562?567,Atlanta, Georgia, June.Saif Mohammad and Peter Turney.
2013.
Crowdsourc-ing a word-emotion association lexicon.
29(3):436?465.Saif Mohammad, Svetlana Kiritchenko, and XiaodanZhu.
2013.
Nrc-canada: Building the state-of-the-art in sentiment analysis of tweets.
In Proceedingsof the Seventh International Workshop on SemanticEvaluation (SemEval 2013), Atlanta, Georgia, USA,June.Finn?Arup Nielsen.
2011.
A new anew: evaluation ofa word list for sentiment analysis in microblogs.
InMatthew Rowe, Milan Stankovic, Aba-Sah Dadzie,and Mariann Hardey, editors, Proceedings of theESWC2011 Workshop on ?Making Sense of Micro-posts?
: Big things come in small packages, volume718 of CEUR Workshop Proceedings, pages 93?98,May.Olutobi Owoputi, Brendan OConnor, Chris Dyer,Kevin Gimpel, Nathan Schneider, and Noah ASmith.
2013.
Improved part-of-speech tagging foronline conversational text with word clusters.
InProceedings of NAACL.Bo Pang and Lillian Lee.
2004.
A sentimental educa-tion: sentiment analysis using subjectivity summa-rization based on minimum cuts.
In Proceedings ofthe 42nd Annual Meeting on Association for Com-putational Linguistics, ACL ?04, Barcelona, Spain.Ross Quinlan.
1986.
Induction of decision trees.Mach.
Learn., 1(1):81?106, March.Sara Rosenthal, Preslav Nakov, Alan Ritter, andVeselin Stoyanov.
2014.
SemEval-2014 Task 9:Sentiment Analysis in Twitter.
In Preslav Nakov andTorsten Zesch, editors, Proceedings of the 8th In-ternational Workshop on Semantic Evaluation, Se-mEval ?14, Dublin, Ireland.Vladimir Vapnik.
1998.
Statistical learning theory.Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.2005.
Recognizing contextual polarity in phrase-level sentiment analysis.
In Proceedings of the con-ference on Human Language Technology and Em-pirical Methods in Natural Language Processing,pages 347?354.Theresa Wilson, Zornitsa Kozareva, Preslav Nakov,Sara Rosenthal, Veselin Stoyanov, and Alan Ritter.2013.
SemEval-2013 task 2: Sentiment analysis intwitter.
In Proceedings of the International Work-shop on Semantic Evaluation, SemEval ?13, June.118
