Chinese NER Using CRFs and Logic for the Fourth SIGHAN Bakeoff ?Xiaofeng YU Wai LAM Shing-Kit CHAN Yiu Kei WU Bo CHENInformation Systems LaboratoryDepartment of Systems Engineering & Engineering ManagementThe Chinese University of Hong KongShatin, N.T., Hong Kong{xfyu,wlam,skchan,ykwu,bchen}@se.cuhk.edu.hkAbstractWe report a high-performance Chinese NERsystem that incorporates Conditional RandomFields (CRFs) and first-order logic for the fourthSIGHAN Chinese language processing bake-off (SIGHAN-6).
Using current state-of-the-art CRFs along with a set of well-engineeredfeatures for Chinese NER as the base model,we consider distinct linguistic characteristics inChinese named entities by introducing varioustypes of domain knowledge into Markov LogicNetworks (MLNs), an effective combination offirst-order logic and probabilistic graphical mod-els for validation and error correction of enti-ties.
Our submitted results achieved consistentlyhigh performance, including the first place on theCityU open track and fourth place on the MSRAopen track respectively, which show both the at-tractiveness and effectiveness of our proposedmodel.1 IntroductionWe participated in the Chinese named entity recognition(NER) task for the fourth SIGHAN Chinese languageprocessing bakeoff (SIGHAN-6).
We submitted resultsfor the open track of the NER task.
Our official re-sults achieved consistently high performance, includingthe first place on the CityU open track and fourth place onthe MSRA open track.
This paper presents an overviewof our system due to space limit.
A more detailed de-scription of our model is presented in (Yu et al, 2008).Our Chinese NER system combines the strength of twographical discriminative models, Conditional Random?The work described in this paper is substantially supportedby grants from the Research Grant Council of the Hong KongSpecial Administrative Region, China (Project Nos: CUHK4179/03E, CUHK4193/04E, and CUHK4128/07) and the Di-rect Grant of the Faculty of Engineering, CUHK (Project Codes:2050363 and 2050391).
This work is also affiliated with theMicrosoft-CUHK Joint Laboratory for Human-centric Comput-ing and Interface Technologies.Fields (CRFs) and Markov Logic Networks (MLNs).First, we employ CRFs, a discriminatively trained undi-rected graphical model which has been shown to be aneffective approach to segmenting and labeling sequencedata, as our base system.
Second, we model the linguis-tic and structural information in Chinese named entitycomposition.
We exploit a variety of domain knowledgewhich can capture essential characteristics of Chinesenamed entities into Markov Logic Networks (MLNs), apowerful combination of first-order logic and probability,to (1) validate and correct errors made in the base sys-tem and (2) find and extract new entity candidates.
Thesedomain knowledge is easy to obtain and can be well andconcisely formulated in first-order logic and incorporatedinto MLNs.2 Conditional Random Fields as BaseModelConditional Random Fields (CRFs) (Lafferty et al, 2001)are undirected graphical models trained to maximize theconditional probability of the desired outputs given thecorresponding inputs.
CRFs have been shown to performwell on Chinese NER shared task on SIGHAN-4 (Zhou etal.
(2006), Chen et al (2006a), Chen et al (2006b)).
Weemploy CRFs as the base model in our framework.
In thisbase model, we design features similar to the state-of-the-art CRF models for Chinese NER.
We use character fea-tures, word segmentation features, part-of-speech (POS)features, and dictionary features, as described below.Character features: These features are the current char-acter, 2 characters preceding the current character and 2following the current character.
We extend the windowsize to 7 but find that it slightly hurts.
The reason is thatCRFs can deal with non-independent features.
A largerwindow size may introduce noisy and irrelevant features.Word segmentation and POS features: We train ourown model for conducting Chinese word segmentationand POS tagging.
We employ a unified framework tointegrate cascaded Chinese word segmentation and POStagging tasks by joint decoding that guards against vi-102Sixth SIGHAN Workshop on Chinese Language Processingolations of those hard-constraints imposed by segmenta-tion task based on dual-layer CRFs introduced by Shi andWang (2007).We separately train the Chinese word segmentationand POS tagging CRF models using 8-month and 2-month PKU 2000 corpus, respectively.
The original PKU2000 corpus contains more than 100 different POS tags.To reduce the training time for POS tagging experiment,we merge some similar tags and obtain only 42 tags fi-nally.
For example, {ia, ib, id, in, iv}?i.
We usethe same features as described in (Shi and Wang, 2007),except that we do not use the HowNet features for wordsegmentation.
Instead, we use max-matching segmenta-tion features based on a word dictionary.
This dictionarycontains 445456 words which are extracted from People?sDaily corpus (January-June, 1998), CityU, MSRA, andPKU word segmentation training corpora in SIGHAN-6.For decoding, we first perform individual decoding foreach task.
We then set 10-best segmentation and POStagging results for reranking and joint decoding in orderto find the most probable joint decodings for both tasks.Dictionary features: We obtain a named entity dictio-nary extracted from People?s Daily 1998 corpus and PKU2000 corpus, which contains 68305 PERs, 28408 LOCsand 55596 ORGs.
We use the max-matching algorithmto search whether a string exists in this dictionary.In summary, we list the features used for our CRF basemodel in Table 1.
Besides the unigram feature template,CRFs also allow bigram feature template.
With this tem-plate, a combination of the current output token and pre-vious output token (bigram) is automatically generated.We use CRF++ toolkit (version 0.48) (Kudo, 2005) inour experiments.
We find that setting the cut-off thresholdf for the features not only decreases the training time, butimproves the NER performance.
CRFs can use the fea-tures that occurs no less than f times in the given trainingdata.
We set f = 5 in our system.We extend the BIO representation for the chunk tagwhich was employed in the CoNLL-2002 and CoNLL-2003 evaluations.
We use the BIOES representation inwhich each character is tagged as either the beginning ofa named entity (B tag), a character inside a named en-tity (I tag), the last character in an entity (E tag), single-character entities (S tag), or a character outside a namedentity (O tag).
We find that BIOES representation ismore informative and yields better results than BIO rep-resentation.3 Markov Logic Networks as ErrorCorrection ModelEven though the CRF model is able to accommodate alarge number of well-engineered features which can beeasily obtained across languages, some NEs, especiallyTable 1: Feature template for CRF model.Character features (1.1) Cn, n ?
[?2, 2](1.2) CnCn+1, n ?
[?2, 1]Word features (1.3) Wn, n ?
[?3, 3](1.4) WnWn+1, n ?
[?3, 2]POS features (1.5) Pn, n ?
[?3, 3](1.6) PnPn+1, n ?
[?3, 2]Dictionary features (1.7) Dn, n ?
[?2, 2](1.8) DnDn+1, n ?
[?2, 1](1.9) D?1D+1LOCs and ORGs are difficult to identify due to the lackof linguistic or structural characteristics.We incorporate domain knowledge that can be wellformulated into first-order logic to extract entity candi-dates from CRF results.
Then, the Markov Logic Net-works (MLNs), an undirected graphical model for statis-tical relational learning, is used to validate and correctthe errors made in the base model.MLNs conduct relational learning by incorporatingfirst-order logic into probabilistic graphical models undera single coherent framework (Richardson and Domingos,2006).
Traditional first-order logic is a set of hard con-straints in which a world violates even one formula haszero probability.
The advantage of MLNs is to softenthese constraints so that when the fewer formulae a worldviolates, the more probable it is.
MLNs have been appliedto tackle the problems of gene interaction discovery frombiomedical texts and citation entity resolution from cita-tion texts with state-of-the-art performance (Riedel andKlein (2005), Singla and Domingos (2006)).We use the Alchemy system (Beta version) (Kok et al,2005) in our experiment, which is a software packageproviding a series of algorithms for statistical relationallearning and probabilistic logic inference, based on theMarkov logic representation.3.1 Domain KnowledgeWe extract 165 location salient words and 843 organiza-tion salient words from Wikipedia and the LDC Chinese-English bi-directional NE lists compiled from XinhuaNews database.
We also make a punctuation list whichcontains 18 items and some stopwords which ChineseNEs cannot contain.
We extract new NE candidates fromthe CRF results according to the following consideration:?
If a chunk (a series of continuous characters) occurs in thetraining data as a PER or a LOC or an ORG, then thischunk should be a PER or a LOC or an ORG in the testingdata.
In general, a unique string is defined as a PER, itcannot be a LOC somewhere else.?
If a tagged entity ends with a location salient word, it is aLOC.
If a tagged entity ends with an organization salientword, it is an ORG.103Sixth SIGHAN Workshop on Chinese Language ProcessingTable 2: Statistics of NER training and testing corpora.Corpus Training NEs PERs/LOCs/ORGs Testing NEs PERs/LOCs/ORGsCityU 66255 16552/36213/13490 13014 4940/4847/3227MSRA 37811 9028/18522/10261 7707 1864/3658/2185NEs: Number of named entities; PERs: Number of person names;LOCs: Number of location names; ORGs: Number of organization names.Table 3: OOV Rate of NER testing corpora.Corpus Overall (IVs/OOVs/OOV-Rate) PER (IVs/OOVs/OOV-Rate) LOC (IVs/OOVs/OOV-Rate) ORG (IVs/OOVs/OOV-Rate)CityU 6660/6354/0.4882 1062/3878/0.7850 3947/900/0.1857 1651/1576/0.4884MSRA 6056/1651/0.2142 1300/564/0.3026 3343/315/0.0861 1413/772/0.3533IVs: number of IV (named entities in vocabulary); OOVs: number of OOV(named entities out of vocabulary); OOV-Rate: ratio of named entities out of vocabulary.?
If a tagged entity is close to a subsequent location salientword, probably they should be combined together as aLOC.
The closer they are, the more likely that they shouldbe combined.?
If a series of consecutive tagged entities are close to a sub-sequent organization salient word, they should probablybe combined together as an ORG because an ORG maycontain multiple PERs, LOCs and ORGs.?
Similarly, if there exists a series of consecutive tagged en-tities and the last one is tagged as an ORG, it is likely thatall of them should be combined as an ORG.?
Entity length restriction: all kinds of tagged entities can-not exceed 25 Chinese characters.?
Stopword restriction: intuitively, all tagged entities cannotcomprise any stopword.?
Punctuation restriction: in general, all tagged entities can-not span any punctuation.?
Since all NEs are proper nouns, the tagged entities shouldend with noun words.?
For a chunk with low conditional probabilities, all theabove assumptions are adopted.3.2 First-Order Logic ConstructionAll the above domain knowledge can also be formulatedas first-order logic to construct the structure of MLNs.First-order formulae are recursively constructed fromatomic formulae using logical connectives and quanti-fiers.
Atomic formulae are constructed using constants,variables, functions, and predicates.For example, we use the predicate organization(candidate) to specify whether a candidate is an ORG.If ?
?I?/China Government?
is mis-tagged as aLOC by the CRF model, but it contains the organizationsalient word ??/Government?.
The correspondingformula endwith(r, p)?orgsalientword(p)?organization(r) means if a tagged entity r endswith an organization salient word p, then it is extracted asa new ORG entity.
Typically only a small number (e.g.,10-20) of formulae are needed.
We declare 14 predi-cates and 15 first-order formulae according to the domainknowledge mentioned in Section 3.1.3.3 Training and Inference for Named EntityCorrectionEach extracted new NE candidate is represented by oneor more strings appearing as arguments of ground atomsin the database.
The goal of NE prediction is to deter-mine whether the candidates are entities and the types ofentities (query predicates), given the evidence predicatesand other relations that can be deterministically derivedfrom the database.We extract all the NEs from the official training cor-pora, and then convert them to the first-order logic repre-sentation according to the domain knowledge.
The MLNtraining database that consists of predicates, constants,and ground atoms was built automatically.
We also ex-tract new entity candidates from CRF results and con-struct MLN testing database in the same way.During MLN learning, each formula is converted toConjunctive Normal Form (CNF), and a weight is learnedfor each of its clauses.
These weights reflect how oftenthe clauses are actually observed in the training data.
In-ference is performed by grounding the minimal subsetof the network required for answering the query pred-icates.
Conducting maximum a posteriori (MAP) in-ference which finds the most likely values of a set ofvariables given the values of observed variables can beperformed via approximate solution using Markov chainMonte Carlo (MCMC) algorithms.
Gibbs sampling canbe adopted by sampling each non-evidence variable inturn given its Markov blanket, and counting the fractionof samples that each variable is in each state.4 Experiment Details4.1 Data and PreprocessingThe training corpora provided by the SIGHAN bakeofforganizers were in the CoNLL two column format, withone Chinese character per line and hand-annotated namedentity chunks in the second column.
The CityU corpuswas traditional Chinese.
We converted this corpus to sim-plified Chinese and we used UTF-8 encoding in all theexperiments so that all the resources (e.g., word dictio-nary and named entity dictionary) are compatible in our104Sixth SIGHAN Workshop on Chinese Language ProcessingTable 4: Official results on CityU andMSRA open tracks.Precision Recall F?=1CityUPER 97.21% 95.26% 96.23LOC 92.35% 93.42% 92.88ORG 88.05% 66.44% 75.73Overall 93.42% 87.43% 90.33MSRAPER 98.33% 94.58% 96.42LOC 93.97% 93.36% 93.66ORG 92.80% 84.39% 88.40Overall 94.71% 91.11% 92.88system.Table 2 shows the statistics of NER training and testingcorpora and Table 3 shows the OOV (Out of Vocabulary)rate of NER testing corpora 1.
The number of NEs inCityU corpus is almost twice as many as that in MSRAcorpus.
The OOV rate in CityU corpus is much higherthan in MSRA corpus for PERs, LOCs and ORGs.
Thesenumbers indicate that NER on CityU corpus is muchmore difficult to handle.4.2 Model DevelopmentWe performed holdout methodology to develop ourmodel.
We randomly selected 5000 sentences fromCityUtraining corpus for development testing and the rest fortraining.
We did the same thing for MSRA training cor-pus.To avoid overfitting for CRF model, we penalizedthe log-likelihood by the commonly used zero-meanGaussian prior over the parameters.
Also, the MLNswere trained using a Gaussian prior with zero mean andunit variance on each weight to penalize the pseudo-likelihood, and with the weights initialized at the modeof the prior (zero).We found an optimal value for the parameter c 2 forCRFs.
Using held-out data, we tested all c values, c ?
[0.2, 2.2], with an incremental step of 0.4.
Finally, we setc = 1.8 for CityU corpus and c = 1.0 for MSRA corpus.5 Official ResultsTable 4 presents the results obtained on the official CityUand MSRA test sets.
Our results are consistently good:we obtained the first place on the CityU open track (90.33overall F-measure) and fourth place on the MSRA opentrack (92.88 overall F-measure) respectively.
The lower1The NER on the PKU corpus was cancelled by the orga-nizer due to the tagging inconsistency of this corpus.2This parameter trades the balance between overfitting andunderfitting.
With larger c value, CRF tends to overfit to the givetraining corpus.
The results will significantly be influenced bythis parameterF-measure obtained on CityU corpus can be attributed tothe higher OOV rate of this corpus.6 ConclusionWe have described a Chinese NER system incorporatingprobabilistic graphical models and first-order logic whichachieves state-of-the-art performance on the open track ofSIGHAN-6.
We exploited domain knowledge which cancapture the essential features of Chinese NER and canbe concisely formulated in MLNs, allowing the trainingand inference algorithms to be directly applied to them.Our proposed framework can also be extendable to NERfor other languages, due to the simplicity of the domainknowledge we could access.ReferencesAitao Chen, Fuchun Peng, Roy Shan, and Gordon Sun.
Chi-nese named entity recognition with conditional probabilisticmodels.
In 5th SIGHAN Workshop on Chinese LanguageProcessing, Australia, July 2006.Wenliang Chen, Yujie Zhang, and Hitoshi Isahara.
Chinesenamed entity recognition with conditional random fields.
In5th SIGHAN Workshop on Chinese Language Processing,Australia, July 2006.Stanley Kok, Parag Singla, Matthew Richardson, and PedroDomingos.
The Alchemy system for statistical relationalAI.
Technical report, Department of Computer Science andEngineering, University of Washington, Seattle, WA, 2005.http://www.cs.washington.edu/ai/alchemy.Taku Kudo.
CRF++: Yet another CRF tool kit.http://crfpp.sourceforge.net/, 2005.John Lafferty, Andrew McCallum, and Fernando Pereira.
Con-ditional random fields: Probabilistic models for segment-ing and labeling sequence data.
In Proceedings of ICML-01, pages 282?289.
Morgan Kaufmann, San Francisco, CA,2001.Matthew Richardson and Pedro Domingos.
Markov logic net-works.
Machine Learning, 62(1-2):107?136, 2006.Sebastian Riedel and Ewan Klein.
Genic interaction extractionwith semantic and syntactic chains.
In Proceedings of theLearning Language in Logic Workshop (LLL-05), pages 69?74, 2005.Yanxin Shi and Mengqiu Wang.
A dual-layer CRFs basedjoint decoding method for cascaded segmentation and label-ing tasks.
In Proceedings of IJCAI-07, pages 1707?1712,Hyderabad, India, 2007.Parag Singla and Pedro Domingos.
Entity resolution withMarkov logic.
In Proceedings of ICDM-06, pages 572?582,Hong Kong, 2006.Xiaofeng Yu, Wai Lam, and Shing-Kit Chan.
A frameworkbased on graphical models with logic for Chinese named en-tity recognition.
In Proceedings of IJCNLP-08, Hyderabad,India, 2008.
To appear.Junsheng Zhou, Liang He, Xinyu Dai, and Jiajun Chen.
Chinesenamed entity recognition with a multi-phase model.
In 5thSIGHAN Workshop on Chinese Language Processing, Aus-tralia, July 2006.105Sixth SIGHAN Workshop on Chinese Language Processing
