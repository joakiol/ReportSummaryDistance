Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 1000?1007,Prague, Czech Republic, June 2007. c?2007 Association for Computational LinguisticsText Analysis for Automatic Image AnnotationKoen Deschacht and Marie-Francine MoensInterdisciplinary Centre for Law & ITDepartment of Computer ScienceKatholieke Universiteit LeuvenTiensestraat 41, 3000 Leuven, Belgium{koen.deschacht,marie-france.moens}@law.kuleuven.ac.beAbstractWe present a novel approach to automati-cally annotate images using associated text.We detect and classify all entities (personsand objects) in the text after which we de-termine the salience (the importance of anentity in a text) and visualness (the extent towhich an entity can be perceived visually)of these entities.
We combine these mea-sures to compute the probability that an en-tity is present in the image.
The suitabilityof our approach was successfully tested on100 image-text pairs of Yahoo!
News.1 IntroductionOur society deals with a growing bulk of un-structured information such as text, images andvideo, a situation witnessed in many domains (news,biomedical information, intelligence information,business documents, etc.).
This growth comes alongwith the demand for more effective tools to searchand summarize this information.
Moreover, there isthe need to mine information from texts and imageswhen they contribute to decision making by gov-ernments, businesses and other institutions.
Thecapability to accurately recognize content in thesesources would largely contribute to improved index-ing, classification, filtering, mining and interroga-tion.Algorithms and techniques for the disclosure ofinformation from the different media have been de-veloped for every medium independently during thelast decennium, but only recently the interplay be-tween these different media has become a topic ofinterest.
One of the possible applications is to helpanalysis in one medium by employing informationfrom another medium.
In this paper we study textthat is associated with an image, such as for instanceimage captions, video transcripts or surrounding textin a web page.
We develop techniques that extractinformation from these texts to help with the diffi-cult task of accurate object recognition in images.Although images and associated texts never containprecisely the same information, in many situationsthe associated text offers valuable information thathelps to interpret the image.The central objective of the CLASS project1 is todevelop advanced learning methods that allow ima-ges, video and associated text to be automaticallyanalyzed and structured.
In this paper we test thefeasibility of automatically annotating images by us-ing textual information in near-parallel image-textpairs, in which most of the content of the imagecorresponds to content of the text and vice versa.We will focus on entities such as persons and ob-jects.
We will hereby take into account the text?s dis-course structure and semantics, which allow a morefine-grained identification of what content might bepresent in the image, and will enrich our model withworld knowledge that is not present in the text.We will first discuss the corpus on which we ap-ply and test our techniques in section 2, after whichwe outline what techniques we have developed: westart with a baseline system to annotate images withperson names (section 3) and improve this by com-puting the importance of the persons in the text (sec-tion 4).
We will then extend the model to include all1http://class.inrialpes.fr/1000Hiram Myers, of Edmond, Okla., walks across thefence, attempting to deliver what he called a ?people?sindictment?
of Halliburton CEO David Lesar, outside thesite of the annual Halliburton shareholders meeting inDuncan, Okla., leading to his arrest, Wednesday, May 17,2006.Figure 1: Image-text pair with entity ?Hiram Myers?appearing both in the text and in the image.types of objects (section 5) and improve it by defin-ing and computing the visualness measure (section6).
Finally we will combine these different tech-niques in one probabilistic model in section 7.2 The parallel corpusWe have created a parallel corpus consisting of 1700image-text pairs, retrieved from the Yahoo!
Newswebsite2.
Every image has an accompanying textwhich describes the content of the image.
This textwill in general discuss one or more persons in theimage, possibly one or more other objects, the loca-tion and the event for which the picture was taken.An example of an image-text pair is given in fig.
1.Not all persons or objects who are pictured in theimages are necessarily described in the texts.
Theinverse is also true, i.e.
content mentioned in thetext may not be present in the image.We have randomly selected 100 text-pairs fromthe corpus, and one annotator has labeled everyimage-text pair with the entities (i.e.
persons and2http://news.yahoo.com/other objects) that appear both in the image and inthe text.
For example, the image-text pair shown infig.
1 is annotated with one entity, ?Hiram Myers?,since this is the only entity that appears both in thetext and in the image.
On average these texts contain15.04 entities, of which 2.58 appear in the image.To build the appearance model of the text, wehave combined different tools.
We will evaluateevery tool separately on 100 image-text pairs.
Thisway we have a detailed view on the nature of theerrors in the final model.3 Automatically annotating person namesGiven a text that is associated with an image, wewant to compute a probabilistic appearance model,i.e.
a collection of entities that are visible in theimage.
We will start with a model that holds thenames of the persons that appear in the image, suchas was done by (Satoh et al, 1999; Berg et al, 2004),and extend this model in section 5 to include allother objects.3.1 Named Entity RecognitionA logical first step to detect person names is NamedEntity Recognition (NER).
We use the OpenNLPpackage3, which detects noun phrase chunks in thesentences that represent persons, locations, organi-zations and dates.
To improve the recognition ofperson names, we use a dictionary of names, whichwe have extracted from the Wikipedia4 website.
Wehave manually evaluated performance of NER onour test corpus and found that performance was sa-tisfying: we obtained a precision of 93.37% and a re-call of 97.69%.
Precision is the percentage of iden-tified person names by the system that correspondsto correct person names, and recall is the percentageof person names in the text that have been correctlyidentified by the system.The texts contain a small number of noun phrasecoreferents that are in the form of pronouns, we haveresolved these using the LingPipe5 package.3.2 Baseline systemWe want to annotate an image using the associatedtext.
We try to find the names of persons which are3http://opennlp.sourceforge.net/4http://en.wikipedia.org/5http://www.alias-i.com/lingpipe/1001both described in the text and visible in the image,and we want to do so by relying only on an analysisof the text.
In some cases, such as the followingexample, the text states explicitly whether a personis (not) visible in the image:President Bush [...] with Danish PrimeMinister Anders Fogh Rasmussen, notpictured, at Camp David [...].Developing a system that could extract this informa-tion is not trivial, and even if we could do so, only avery small percentage of the texts in our corpus con-tain this kind of information.
In the next section wewill look into a method that is applicable to a widerange of (descriptive) texts and that does not rely onspecific information within the text.To evaluate the performance of this system, wewill compare it with a simple baseline system.
Thebaseline system assumes that all persons in the textare visible in the image, which results in a precisionof 71.27% and a recall of 95.56%.
The (low) preci-sion can be explained by the fact that the texts oftendiscuss people which are not present in the image.4 Detection of the salience of a personNot all persons discussed in a text are equally im-portant.
We would like to discover what personsare in the focus of a text and what persons are onlymentioned briefly, because we presume that moreimportant persons in the text have a larger proba-bility of appearing in the image than less importantpersons.
Because of the short lengths of the docu-ments in our corpus, an analysis of lexical cohesionbetween terms in the text will not be sufficient fordistinguishing between important and less importantentities.
We define a measure, salience, which is anumber between 0 and 1 that represents the impor-tance of an entity in a text.
We present here a methodfor computing this score based on an in depth ana-lysis of the discourse of the text and of the syntacticstructure of the individual sentences.4.1 Discourse segmentationThe discourse segmentation module, which we de-veloped in earlier research, hierarchically and se-quentially segments the discourse in different topicsand subtopics resulting in a table of contents of atext (Moens, 2006).
The table shows the main en-tities and the related subtopic entities in a tree-likestructure that also indicates the segments (by meansof character pointers) to which an entity applies.
Thealgorithm detects patterns of thematic progression intexts and can thus recognize the main topic of a sen-tence (i.e., about whom or what the sentence speaks)and the hierarchical and sequential relationships be-tween individual topics.
A mixture model, takinginto account different discourse features, is trainedwith the Expectation Maximization algorithm on anannotated DUC-2003 corpus.
We use the resultingdiscourse segmentation to define the salience of in-dividual entities that are recognized as topics of asentence.
We compute for each noun entity er in thediscourse its salience (Sal1) in the discourse tree,which is proportional with the depth of the entity inthe discourse tree -hereby assuming that deeper inthis tree more detailed topics of a text are described-and normalize this value to be between zero and one.When an entity occurs in different subtrees, its max-imum score is chosen.4.2 Refinement with sentence parseinformationBecause not all entities of the text are captured in thediscourse tree, we implement an additional refine-ment of the computation of the salience of an entitywhich is inspired by (Moens et al, 2006).
The seg-mentation module already determines the main topicof a sentence.
Since the syntactic structure is oftenindicative of the information distribution in a sen-tence, we can determine the relative importance ofthe other entities in a sentence by relying on the re-lationships between entities as signaled by the parsetree.
When determining the salience of an entity, wetake into account the level of the entity mention inthe parse tree (Sal2), and the number of children forthe entity in this structure (Sal3), where the normal-ized score is respectively inversely proportional withthe depth of the parse tree where the entity occurs,and proportional with the number of children.We combine the three salience values (Sal1,Sal2 and Sal3) by using a linear weighting.
Wehave experimentally determined reasonable coeffi-cients for these three values, which are respectively0.8, 0.1 and 0.1.
Eventually, we could learn thesecoefficients from a training corpus (e.g., with the1002Precision Recall F-measureNER 71.27% 95.56% 81.65%NER+DYN 97.66% 92.59% 95.06%Table 1: Comparison of methods to predict what per-sons described in the text will appear in the image,using Named Entity Recognition (NER), and thesalience measure with dynamic cut-off (DYN).Expectation Maximization algorithm).We do not separately evaluate our technologyfor salience detection as this technology wasalready extensively evaluated in the past (Moens,2006).4.3 Evaluating the improved systemThe salience measure defines a ranking of all thepersons in a text.
We will use this ranking to improveour baseline system.
We assume that it is possibleto automatically determine the number of faces thatare recognized in the image, which gives us an indi-cation of a suitable cut-off value.
This approach isreasonable since face detection (determine whether aface is present in the image) is significant easier thanface recognition (determine which person is presentin the image).
In the improved model we assumethat persons which are ranked higher than, or equalto, the cut-off value appear in the image.
For ex-ample, if 4 faces appear in the image, we assumethat only the 4 persons of which the names in thetext have been assigned the highest salience appearin the image.
We see from table 1 that the precision(97.66%) has improved drastically, while the recallremained high (92.59%).
This confirms the hypoth-esis that determining the focus of a text helps in de-termining the persons that appear in the image.5 Automatically annotating persons andobjectsAfter having developed a reasonable successful sys-tem to detect what persons will appear in the image,we turn to a more difficult case : Detecting personsand all other objects that are described in the text.5.1 Entity detectionWe will first detect what words in the text refer to anentity.
For this, we perform part-of-speech tagging(i.e., detecting the syntactic word class such as noun,verb, etc.).
We take that every noun in the text rep-resents an entity.
We have used LTPOS (Mikheev,1997), which performed the task almost errorless(precision of 98.144% and recall of 97.36% on thenouns in the test corpus).
Person names which weresegmented using the NER package are also markedas entities.5.2 Baseline systemWe want to detect the objects and the names of per-sons which are both visible in the image and de-scribed in the text.
We start with a simple baselinesystem, in which we assume that every entity in thetext appears in the image.
As can be expected, thisresults in a high recall (91.08%), and a very low pre-cision (15.62%).
We see that the problem here isfar more difficult compared to detecting only per-son names.
This can be explained by the fact thatmany entities (such as for example August, idea andhistory) will never (or only indirectly) appear in animage.
In the next section we will try to determinewhat types of entities are more likely to appear inthe image.6 Detection of the visualness of an entityThe assumption that every entity in the text appearsin the image is rather crude.
We will enrich ourmodel with external world knowledge to find enti-ties which are not likely to appear in an image.
Wedefine a measure called visualness, which is definedas the extent to which an entity can be perceived vi-sually.6.1 Entity classificationAfter we have performed entity detection, we wantto classify every entity according to a certain seman-tic database.
We use the WordNet (Fellbaum, 1998)database, which organizes English nouns, verbs, ad-jectives and adverbs in synsets.
A synset is a col-lection of words that have a close meaning and thatrepresent an underlying concept.
An example ofsuch a synset is ?person, individual, someone, some-body, mortal, soul?.
All these words refer to a hu-1003man being.
In order to correctly assign a noun ina text to its synset, i.e., to disambiguate the senseof this word, we use an efficient Word Sense Dis-ambiguation (WSD) system that was developed bythe authors and which is described in (Deschachtand Moens, 2006).
Proper names are labeled bythe Named Entity Recognizer, which recognizes per-sons, locations and organizations.
These labels inturn allow us to assign the corresponding WordNetsynset.The combination of the WSD system and theNER package achieved a 75.97% accuracy in classi-fying the entities.
Apart from errors that resultedfrom erroneous entity detection (32.32%), errorswere mainly due to the WSD system (60.56%) andin a smaller amount to the NER package (8.12%).6.2 WordNet similarityWe determine the visualness for every synset us-ing a method that was inspired by Kamps and Marx(2002).
Kamps and Marx use a distance measuredefined on the adjectives of the WordNet databasetogether with two seed adjectives to determine theemotive or affective meaning of any given adjective.They compute the relative distance of the adjectiveto the seed synsets ?good?
and ?bad?
and use thisdistance to define a measure of affective meaning.We take a similar approach to determine the visu-alness of a given synset.
We first define a similaritymeasure between synsets in the WordNet database.Then we select a set of seed synsets, i.e.
synsetswith a predefined visualness, and use the similarityof a given synset to the seed synsets to determine thevisualness.6.3 Distance measureThe WordNet database defines different relations be-tween its synsets.
An important relation for nouns isthe hypernym/hyponym relation.
A noun X is a hy-pernym of a noun Y if Y is a subtype or instance ofX.
For example, ?bird?
is a hypernym of ?penguin?
(and ?penguin?
is a hyponym of ?bird?).
A synsetin WordNet can have one or more hypernyms.
Thisrelation organizes the synsets in a hierarchical tree(Hayes, 1999).The similarity measure defined by Lin (1998) usesthe hypernym/hyponym relation to compute a se-mantic similarity between two WordNet synsets S1and S2.
First it finds the most specific (lowest in thetree) synset Sp that is a parent of both S1 and S2.Then it computes the similarity of S1 and S2 assim(S1, S2) =2logP (Sp)logP (S1) + logP (S2)Here the probability P (Si) is the probability oflabeling any word in a text with synset Si or withone of the descendants of Si in the WordNet hier-archy.
We estimate these probabilities by countingthe number of occurrences of a synset in the Sem-cor corpus (Fellbaum, 1998; Landes et al, 1998),where all noun chunks are labeled with their Word-Net synset.
The probability P (Si) is computed asP (Si) =C(Si)?Nn=1 C(Sn)+ ?Kk=1 P (Sk)where C(Si) is the number of occurrences of Si,N is the total number of synsets in WordNet andK is the number of children of Si.
The Word-Net::Similarity package (Pedersen et al, 2004) im-plements this distance measure and was used by theauthors.6.4 Seed synsetsWe have manually selected 25 seed synsets in Word-Net, where we tried to cover the wide range of topicswe were likely to encounter in the test corpus.
Wehave set the visualness of these seed synsets to either1 (visual) or 0 (not visual).
We determine the visu-alness of all other synsets using these seed synsets.A synset that is close to a visual seed synset gets ahigh visualness and vice versa.
We choose a linearweighting:vis(s) =?ivis(si)sim(s, si)C(s)where vis(s) returns a number between 0 and 1 de-noting the visualness of a synset s, si are the seedsynsets, sim(s, t) returns a number between 0 and 1denoting the similarity between synsets s and t andC(s) is constant given a synset s:C(s) =?isim(s, si)10046.5 Evaluation of the visualness computationTo determine the visualness, we first assign the cor-rect WordNet synset to every entity, after which wecompute a visualness score for these synsets.
Sincethese scores are floating point numbers, they arehard to evaluate manually.
During evaluation, wemake the simplifying assumption that all entitieswith a visualness below a certain threshold are notvisual, and all entities above this threshold are vi-sual.
We choose this threshold to be 0.5.
This re-sults in an accuracy of 79.56%.
Errors are mainlycaused by erroneous entity detection and classifica-tion (63.10%) but also because of an incorrect as-signment of the visualness (36.90%) by the methoddescribed above.7 Creating an appearance model usingsalience and visualnessIn the previous section we have created a method tocalculate a visualness score for every entity, becausewe stated that removing the entities which can neverbe perceived visually will improve the performanceof our baseline system.
An experiment proves thatthis is exactly the case.
If we assume that only theentities that have a visualness above a 0.5 thresh-old are visible and will appear in the image, we geta precision of 48.81% and a recall of 87.98%.
Wesee from table 2 that this is already a significant im-provement over the baseline system.In section 4 we have seen that the salience mea-sure helps in determining what persons are visible inthe image.
We have used the fact that face detectionin images is relatively easily and can thus supply acut-off value for the ranked person names.
In thepresent state-of-the-art, we are not able to exploit asimilar fact when detecting all types of entities.
Wewill thus use the salience measure in a different way.We compute the salience of every entity, and weassume that only the entities with a salience scoreabove a threshold of 0.5 will appear in the image.We see that this method drastically improves preci-sion to 66.03%, but also lowers recall until 54.26%.We now create a last model where we combineboth the visualness and the salience measures.
Wewant to calculate the probability of the occurrence ofan entity eim in the image, given a text t, P (eim|t).We assume that this probability is proportional withPrecision Recall F-measureEnt 15.62% 91.08% 26.66%Ent+Vis 48.81% 87.98% 62.78%Ent+Sal 66.03% 54.26% 59.56%Ent+Vis+Sal 70.56% 67.82% 69.39%Table 2: Comparison of methods to predict the en-tities that appear in the image, using entity detec-tion (Ent), and the visualness (Vis) and salience (Sal)measures.the degree of visualness and salience of eim in t. Inour framework, P (eim|t) is computed as the productof the salience of the entity eim and its visualnessscore, as we assume both scores to be independent.Again, for evaluation sake, we choose a thresholdof 0.4 to transform this continuous ranking into abinary classification.
This results in a precision of70.56% and a recall of 67.82%.
This model is thebest of the 4 models for entity annotation which havebeen evaluated.8 Related ResearchUsing text that accompanies the image for annotat-ing images and for training image recognition is notnew.
The earliest work (only on person names) isby Satoh (1999) and this research can be consideredas the closest to our work.
The authors make a dis-tinction between proper names, common nouns andother words, and detect entities based on a thesauruslist of persons, social groups and other words, thusexploiting already simple semantics.
Also a rudi-mentary approach to discourse analysis is followedby taking into account the position of words in atext.
The results were not satisfactory: 752 wordswere extracted from video as candidates for being inthe accompanying images, but only 94 were correctwhere 658 were false positives.
Mori et al (2000)learn textual descriptions of images from surround-ing texts.
These authors filter nouns and adjectivesfrom the surrounding texts when they occur abovea certain frequency and obtain a maximum hit rateof top 3 words that is situated between 30% and40%.
Other approaches consider both the textualand image features when building a content modelof the image.
For instance, some content is selectedfrom the text (such as person names) and from the1005image (such as faces) and both contribute in describ-ing the content of a document.
This approach wasfollowed by Barnard (2003).Westerveld (2000) combines image features andwords from collateral text into one semantic space.This author uses Latent Semantic Indexing for rep-resenting the image/text pair content.
Ayache et al(2005) classify video data into different topical con-cepts.
The results of these approaches are often dis-appointing.
The methods here represent the text as abag of words possibly augmented with a tf (term fre-quency) x idf (inverse document frequency) weightof the words (Amir et al, 2005).
In exceptionalcases, the hierarchical XML structure of a text doc-ument (which was manually annotated) is taken intoaccount (Westerveld et al, 2005).
The most inter-esting work here to mention is the work of Berget al (2004) who also process the nearly parallelimage-text pairs found in the Yahoo!
news corpus.They link faces in the image with names in the text(recognized with named entity recognition), but donot consider other objects.
They consider pairs ofperson names (text) and faces (image) and use clus-tering with the Expectation Maximization algorithmto find all faces belonging to a certain person.
Intheir model they consider the probability that an en-tity is pictured given the textual context (i.e., thepart-of-speech tags immediately prior and after thename, the location of the name in the text and thedistance to particular symbols such as ?(R)?
), whichis learned with a probabilistic classifier in each stepof the EM iteration.
They obtained an accuracy of84% on person face recognition.In the CLASS project we work together withgroups specialized in image recognition.
In futurework we will combine face and object recognitionwith text analysis techniques.
We expect the recog-nition and disambiguation of faces to improve ifmany image-text pairs that treat the same person areused.
On the other hand our approach is also valu-able when there are few image-text pairs that picturea certain person or object.
The approach of Berget al could be augmented with the typical featuresthat we use, namely salience and visualness.
In De-schacht et al (2007) we have evaluated the rankingof persons and objects by the method we have de-scribed here and we have shown that this rankingcorrelates with the importance of persons and ob-jects in the picture.None of the above state-of-the-art approachesconsider salience and visualness as discriminatingfactors in the entity recognition, although these as-pects could advance the state-of-the-art.9 ConclusionOur society in the 21st century produces giganticamounts of data, which are a mixture of differentmedia.
Our repositories contain texts interwovenwith images, audio and video and we need auto-mated ways to automatically index these data andto automatically find interrelationships between thevarious media contents.
This is not an easy task.However, if we succeed in recognizing and aligningcontent in near-parallel image-text pairs, we mightbe able to use this acquired knowledge in index-ing comparable image-text pairs (e.g., in video) byaligning content in these media.In the experiment described above, we analyzethe discourse and semantics of texts of near-parallelimage-text pairs in order to compute the probabilitythat an entity mentioned in the text is also present inthe accompanying image.
First, we have developedan approach for computing the salience of each en-tity mentioned in the text.
Secondly, we have usedthe WordNet classification in order to detect the vi-sualness of an entity, which is translated into a vi-sualness probability.
The combined salience and vi-sualness provide a score that signals the probabilitythat the entity is present in the accompanying image.We extensively evaluated all the different modulesof our system, pinpointing weak points that could beimproved and exposing the potential of our work incross-media exploitation of content.We were able to detect the persons in the textthat are also present in the image with a (evenlyweighted) F-measure of more than 95%, and in addi-tion were able to detect the entities that are presentin the image with a F-measure of more than 69%.These results have been obtained by relying only onan analysis of the text and were substantially betterthan the baseline approach.
Even if we can not re-solve all ambiguity, keeping the most confident hy-potheses generated by our textual hypotheses willgreatly assist in analyzing images.In the future we hope to extrinsically evaluate1006the proposed technologies, e.g., by testing whetherthe recognized content in the text, improves imagerecognition, retrieval of multimedia sources, miningof these sources, and cross-media retrieval.
In addi-tion, we will investigate how we can build more re-fined appearance models that incorporate attributesand actions of entities.AcknowledgmentsThe work reported in this paper was supportedby the EU-IST project CLASS (Cognitive-LevelAnnotation using Latent Statistical Structure, IST-027978).
We acknowledge the CLASS consortiumpartners for their valuable comments and we are es-pecially grateful to Yves Gufflet from the INRIAresearch team (Grenoble, France) for collecting theYahoo!
News dataset.ReferencesArnon Amir, Janne Argillander, Murray Campbell,Alexander Haubold, Giridharan Iyengar, ShahramEbadollahi, Feng Kang, Milind R. Naphade, ApostolNatsev, John R. Smith, Jelena Tes?io?, and Timo Volk-mer.
2005.
IBM Research TRECVID-2005 VideoRetrieval System.
In Proceedings of TRECVID 2005,Gaithersburg, MD.Ste?phane Ayache, Gearges M. Qunot, Jrme Gensel, andShin?Ichi Satoh.
2005.
CLIPS-LRS-NII Experimentsat TRECVID 2005.
In Proceedings of TRECVID2005, Gaithersburg, MD.Kobus Barnard, Pinar Duygulu, Nando de Freitas, DavidForsyth, David Blei, and Michael I. Jordan.
2003.Matching Words and Pictures.
Journal of MachineLearning Research, 3(6):1107?1135.Tamara L. Berg, Alexander C. Berg, Jaety Edwards, andD.A.
Forsyth.
2004. Who?s in the Picture?
In NeuralInformation Processing Systems, pages 137?144.Koen Deschacht and Marie-Francine Moens.
2006.
Ef-ficient Hierarchical Entity Classification Using Con-ditional Random Fields.
In Proceedings of the2nd Workshop on Ontology Learning and Population,pages 33?40, Sydney, July.Koen Deschacht, Marie-Francine Moens, andW Robeyns.
2007.
Cross-media entity recogni-tion in nearly parallel visual and textual documents.In Proceedings of the 8th RIAO Conference on Large-Scale Semantic Access to Content (Text, Image, Videoand Sound).
Cmu.
(in press).Christiane Fellbaum.
1998.
WordNet: An ElectronicLexical Database.
The MIT Press.Brian Hayes.
1999.
The Web of Words.
American Sci-entist, 87(2):108?112, March-April.Jaap Kamps and Maarten Marx.
2002.
Words with Atti-tude.
In Proceedings of the 1st International Confer-ence on Global WordNet, pages 332?341, India.Shari Landes, Claudia Leacock, and Randee I. Tengi.1998.
Building Semantic Concordances.
In Chris-tiane Fellbaum, editor, WordNet: An Electronic Lex-ical Database.
The MIT Press.Dekang Lin.
1998.
An Information-Theoretic Definitionof Similarity.
In Proc.
15th International Conf.
on Ma-chine Learning.Andrei Mikheev.
1997.
Automatic Rule Induction forUnknown-Word Guessing.
Computational Linguis-tics, 23(3):405?423.Marie-Francine Moens, Patrick Jeuniaux, RoxanaAngheluta, and Rudradeb Mitra.
2006.
Measur-ing Aboutness of an Entity in a Text.
In Proceed-ings of HLT-NAACL 2006 TextGraphs: Graph-basedAlgorithms for Natural Language Processing, EastStroudsburg.
ACL.Marie-Francine Moens.
2006.
Using Patterns of The-matic Progression for Building a Table of Content ofa Text.
Journal of Natural Language Engineering,12(3):1?28.Yasuhide Mori, Hironobu Takahashi, and Ryuichi Oka.2000.
Automatic Word Assignment to Images Basedon Image Division and Vector Quantization.
In RIAO-2000 Content-Based Multimedia Information Access,Paris, April 12-14.Ted Pedersen, Siddharth Patwardhan, and Jason Miche-lizzi.
2004.
WordNet::Similarity - Measuring the Re-latedness of Concepts.
In The Proceedings of Fifth An-nual Meeting of the North American Chapter of the As-sociation for Computational Linguistics (NAACL-04),Boston, May.Shin?ichi Satoh, Yuichi Nakamura, and Takeo Kanade.1999.
Name-It: Naming and Detecting Faces in NewsVideos.
IEEE MultiMedia, 6(1):22?35, January-March.Thijs Westerveld, Jan C. van Gemert, Roberto Cornac-chia, Djoerd Hiemstra, and Arjen de Vries.
2005.
AnIntegrated Approach to Text and Image Retrieval.
InProceedings of TRECVID 2005, Gaithersburg, MD.Thijs Westerveld.
2000.
Image Retrieval: Content versusContext.
In Content-Based Multimedia InformationAccess, RIAO 2000 Conference Proceedings, pages276?284, April.1007
