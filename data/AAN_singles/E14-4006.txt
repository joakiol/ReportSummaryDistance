Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 28?32,Gothenburg, Sweden, April 26-30 2014.c?2014 Association for Computational LinguisticsProjecting the Knowledge Graph to Syntactic ParsingAndrea Gesmundo and Keith B. HallGoogle, Inc.{agesmundo,kbhall}@google.comAbstractWe present a syntactic parser trainingparadigm that learns from large scaleKnowledge Bases.
By utilizing theKnowledge Base context only duringtraining, the resulting parser has noinference-time dependency on the Knowl-edge Base, thus not decreasing the speedduring prediction.
Knowledge Base infor-mation is injected into the model using anextension to the Augmented-loss trainingframework.
We present empirical resultsthat show this approach achieves a signif-icant gain in accuracy for syntactic cat-egories such as coordination and apposi-tion.1 IntroductionNatural Language Processing systems requirelarge amounts of world knowledge to achievestate-of-the-art performance.
Leveraging Knowl-edge Bases (KB) provides allows us to inject hu-man curated world-knowledge into our systems.As these KBs have increased in size, we are nowable to leverage this information to improve uponthe state-of-the-art.
Large scale KB have been de-veloped rapidly in recent years, adding large num-bers of entities and relations between the entities.Such entities can be of any kind: an object, a per-son, a place, a company, a book, etc.
Entitiesand relations are stored in association with rele-vant data that describes the particular entity or re-lation; for example, the name of a book, it?s author,other books by the same author, etc.. Large scaleKB annotation efforts have focused on the collec-tion of both current and historical entities, but arebiased towards the contemporary entities.Of the many publicly available KBs, we focusthis study on the use of Freebase1: a large collab-orative Knowledge Base composed and updatedby a member community.
Currently it containsroughly 40 million entities and 1.1 billion rela-tions.The aim of the presented work is to use the in-formation provided by the KB to improve the ac-curacy of the statistical dependency parsing task(Kubler et al., 2009).
In particular we focus on therecognition of relations such as coordination andapposition.
This choice is motivated by the factthat the KB stores information about real-worldentities while many of the errors associated withcoordination and apposition is the lack of knowl-edge of these real-world entities.We begin by defining the task (section 2).
Fol-lowing, we present the modified augmented-losstraining framework (section 3).
In section 4, wedefine how the Knowledge Base data is integratedinto the training process.
Finally, we discuss theempirical results (section 5).2 TaskApposition is a relation between two adjacentnoun-phrases, where one noun-phrase specifies ormodifying the other.
For example, in the sentence?My friend Anna?, the nouns ?friend?
and ?Anna?are in apposition.
Coordination between nounsrelates two or more elements of the same kind.The coordination is often signaled by the appear-ance of a coordinating conjunction.
For example,in the sentence ?My friend and Anna?, the nouns?friend?
and ?Anna?
are in coordination.
The se-mantic difference between the two relations is thatthe nouns in apposition refer to the same entity,1www.freebase.com28while the nouns in coordination refer to distinctentities of the same kind or sharing some proper-ties.Statistical parsers are inaccurate in classifyingrelations involving proper nouns that appear rarelyin the training set.
In the sentence:?They invested in three companies, Google,Microsoft, and Yahoo.??companies?
is in apposition with the coordina-tion ?Google, Microsoft, and Yahoo?.
By integrat-ing the information provided by a large scale KBinto the syntactic parser, we attempt to increasethe ability to disambiguate the relations involvingthese proper nouns, even if the parser has beentrained on a different domain.3 ModelWe present a Syntactic Parsing model that learnsfrom the KB.
An important constraint that we im-pose, is that the speed of the Syntactic Parser mustnot decrease when this information is integrated.As the queries to the KB would significantly slowdown the parser, we limit querying the KB to train-ing.
This constraint reduces the impact that the KBcan have on the accuracy, but allows us to design aparser that can be substituted in any setting, evenin the absence of the KB.We propose a solution based on the Augmented-loss framework (Hall et al., 2011a).
Augmented-loss is a training framework for structured predic-tion tasks such as parsing.
It can be used to ex-tend a standard objective function with additionalloss-functions and be integrated with the struc-tured perceptron training algorithm.
The inputis enriched with multiple datasets each associatedwith a loss function.
The algorithm iterates overthe datasets triggering parameter updates when-ever the loss function is positive.Loss functions return a positive value if the pre-dicted output is ?worse?
than the gold standard.Augmented-loss allows for the inclusion of mul-tiple objective functions, either based on intrinsicparsing quality or task-specific extrinsic measuresof quality.
In the original formalization, both theintrinsic and extrinsic losses require gold standardinformation.
Thus, each dataset must specify agold standard output for each input.We extend the Augmented-loss framework toapply it when the additional dataset gold-standardis unknown.
Without the gold standard, it is notpossible to trigger updates using a loss function.Instead, we use a sampling function, S(?
), that isdefined such that: if y?
is a candidate parse tree,then S(y?)
returns a parse tree that is guaranteed tobe ?not worse?
than y?.
In other words:LS(y?, S(y?))
?
0 (1)Where the LS(?)
is the implicit loss function.
Thisformalization will allow us to avoid stating explic-itly the loss function.
Notice that S(y?)
is not guar-anteed to be the ?best?
parse tree.
It can be anyparse tree in the search space that is ?not worse?than y?.
S(y?)
can represent an incremental im-provement over y?.Algorithm 1 Augmented-loss extension1: {Input loss function: L(?
)}2: {Input sample function: S(?
)}3: {Input data sets}:4: DL= {dLi= (xLi, yLi) | 1 ?
i ?
NL}5: DS= {dSi= (xSi) | 1 ?
i ?
NS}6: ?
=~07: repeat8: for i = 1 .
.
.
NLdo9: y?
= F?
(xLi)10: if L(y?, yLi) > 0 then11: ?
= ?
+ ?(yLi)?
?(y?
)12: end if13: end for14: for i = 1 .
.
.
NSdo15: y?
= F?
(xSi)16: y?= S(y?
)17: ?
= ?
+ ?(y?)?
?(y?
)18: end for19: until converged20: {Return model ?
}Algorithm 1 summarizes the extension to theAugmented-loss algorithm.The algorithm takes as input: the loss func-tion L(?
); the sample function S(?
); the loss func-tion data samples DL; and the sample functiondata samples DS.
Notice that DLspecifies thegold standard parse yLifor each input sentence xLi.While, DSspecifies only the input sentence xSi.The model parameter are initialized to the zerovector (line 6).
The main loop iterates until themodel reaches convergence (lines 7-19).
Afterwhich the model parameters are returned.The first inner loop iterates over DL(lines 8-13) executing the standard on-line training.
Thecandidate parse, y?, for the current input sentence,29xLi, is predicted given the current model parame-ters, ?
(line 9).
In the structured perceptron setting(Collins and Roark, 2004; Daum?e III et al., 2009),we have that:F?
(x) = argmaxy?Y?
?
?
(y) (2)Where ?(?)
is the mapping from a parse tree y toa high dimensional feature space.
Then, the algo-rithm tests if the current prediction is wrong (line10).
In which case the model is updated promot-ing features that fire in the gold-standard ?
(yLi),and penalizing features that fire in the predictedoutput, ?(y?)
(line 11).The second inner loop iterates over DS(lines14-18).
First, the candidate parse, y?, is predicted(line 15).
Then the sample parse, y?, is pro-duced by the sample function (line 16).
Finally,the parameters are updated promoting the featuresof y?.
The updates are triggered without test-ing if the loss is positive, since it is guaranteedthat LS(y?, y?)
?
0.
Updating in cases whereLS(y?, y?)
= 0 does not harm the model.
To opti-mize the algorithm, updates can be avoided wheny?
= y?.In order to simplify the algorithmic descrip-tion, we define the algorithm with only one lossfunction and one sample function, and we formal-ized it for the specific task we are considering.This definitions can be trivially generalized to in-tegrate multiple loss/sample functions and to beformalized for a generic structured prediction task.This generalization can be achieved following theguidelines of (Hall et al., 2011a).
Furthermore, wedefined the algorithm such that it first iterates overDLand then over DS.
In practice, the algorithmcan switch between the data sets with a desired fre-quency by using a scheduling policy as describedin (Hall et al., 2011a).
For the experiments, wetrained on 8 samples ofDLfollowed by 1 samplesof DS, looping over the training sets.4 Sample FunctionWe integrate the Knowledge Base data into thetraining algorithm using a sampling function.
Theidea is to correct errors in the candidate parseby using the KB.
The sample function correctsonly relations among entities described in the KB.Thus, it returns a better or equal parse tree thatmay still contain errors.
This is sufficient to guar-antee the constraint on the implicit loss function(equation 1).The sample function receives as input the can-didate dependency parse and the input sentenceenriched with KB annotation.
Then, it correctsthe labels of each arc in the dependency tree con-necting two entities.
The labels are corrected ac-cording to the predictions produced by a classifier.As classifier we use a standard multi-class percep-tron (Crammer and Singer, 2003).
The classifier istrained in a preprocessing step on a parsed corpusenriched with KB data.
The features used by theclassifier are:?
Lexical features of the head and modifier.?
Sentence level features: words distance be-tween head and modifier; arc direction (L/R);neighboring words.?
Syntactic features: POS and syntactic label ofhead and modifier and modifier?s left sibling.?
Knowledge Base features: types defined forentities and for their direct relations.5 ExperimentsThe primary training corpus is composed of manu-ally annotated sentences with syntactic tress whichare converted to dependency format using theStanford converter v1.6 (de Marneffe et al., 2006).We run experiments using 10k sentences or 70ksentences from this corpus.
The test set contains16k manually syntactically annotated sentencescrawled from the web.
The test and train sets arefrom different domains.
This setting may degradethe parser accuracy in labelling out-of-domain en-tities, as we discussed in section 2.
Thus, we useweb text as secondary training set to be used forthe Augmented-loss loss sample training.
Webtext is available in any quantity, and we do notneed to provide gold-standard parses in order tointegrate it in the Augmented-loss sample train-ing.
The classifier is trained on 10k sentences ex-tracted from news text which has been automati-cally parsed.
We chose to train the classifier onnews data as the quality of the automatic parses ismuch higher than on general web text.
We do thisdespite the fact that we will apply the classifier toa different domain (the web text).As dependency parser, we use an implemen-tation of the transition-based dependency parsingframework (Nivre, 2008) with the arc-eager tran-sition strategy.
The part of Augmented-loss train-ing based on the standard loss function, applies30Training set size Model appos F1 conj F1 LAS UAS70k sentences Baseline 54.36 83.72 79.55 83.50Augmented-loss 55.64 84.47 79.71 83.7110k sentences Baseline 45.13 80.36 75.99 86.02Augmented-loss 48.06 81.63 76.16 86.18Table 1: Accuracy Comparison.the perceptron algorithm as in (Zhang and Clark,2008) with a beam size of 16.
The baseline is thesame model but trained only the primary trainingcorpus without Augmented-loss.Table 1 reports the results of the accuracy com-parison.
It reports the metrics for Labeled At-tachment Score (LAS) and Unlabeled AttachmentScore (UAS) to measure the overall accuracy.
Thesyntactic classes that are affected the most are ap-position (appos) and conjunction (conj).
On thedevelopment set we measured that the percentageof arcs connecting 2 entities that are labeled asconjunction is 36.11%.
While those that are la-belled as apposition is 25.06%.
Each of the other40 labels cover a small portion of the remaining38.83%.Training the models with the full primary train-ing corpus (70k sentences), shows a significantgain for the Augmented-loss model.
AppositionF1 gains 1.28, while conjunction gains 0.75.
TheLAS gain is mainly due to the gain of the two men-tioned classes.
It is surprising to measure a simi-lar gain also for the unlabeled accuracy.
Since theclassifier can correct the label of an arc but neverchange the structure of the parse.
This impliesthat just by penalizing a labeling action, the modellearns to construct better parse structures.Training the model with 10k sentences shows asignificantly bigger gain on all the measures.
Thisresults shows that, in cases where the set of la-beled data is small, this approach can be appliedto integrate in unlimited amount of unlabeled datato boost the learning.6 Related WorkAs we mentioned, Augmented-loss (Hall et al.,2011a; Hall et al., 2011b) is perhaps the closest toour framework.
Another difference with its origi-nal formalization is that it was primarily aimed tocases where the additional weak signal is preciselywhat we wish to optimize.
Such as cases wherewe wish to optimize parsing to be used as an inputto a downstream natural language processing tasksand the accuracies to be optimized are those of thedownstream task and not directly the parsing ac-curacy.
While our work is focused on integratingadditional data in a semi-supervised fashion withthe aim of improving the primary task?s accuracyand/or adapt it to a different domain.Another similar idea is (Chang et al., 2007)which presents a constraint driven learning.
In thisstudy, they integrate a weak signal into the trainingframework with the aim to improve the structuredprediction models on the intrinsic evaluation met-rics.7 ConclusionWe extended the Augmented-loss frameworkdefining a method for integrating new types of sig-nals that require neither gold standard data nor anexplicit loss function.
At the same time, they al-low the integration of additional information thatcan inform training to learn for specific types ofphenomena.This framework allows us to effectively inte-grate large scale KB in the training of structuredprediction tasks.
This approach integrates the dataat training time without affecting the predictiontime.Experiments on syntactic parsing show that asignificant gain for categories that model relationbetween entities defined in the KB.ReferencesMing-Wei Chang, Lev Ratinov, and Dan Roth.
2007.Guiding semi-supervision with constraint-drivenlearning.
In ACL ?07: Proceedings of the 45th Con-ference of the Association for Computational Lin-guistics.Michael Collins and Brian Roark.
2004.
Incrementalparsing with the perceptron algorithm.
In ACL ?04:Proceedings of the 42rd Conference of the Associa-tion for Computational Linguistics.Koby Crammer and Yoram Singer.
2003.
Ultracon-servative online algorithms for multiclass problems.Journal of Machine Learning Research, 3:951?991.31Hal Daum?e III, John Langford, and Daniel Marcu.2009.
Search-based structured prediction.
Submit-ted to Machine Learning Journal.Marie-Catherine de Marneffe, Bill MacCartney, andChristopher D. Manning.
2006.
Generating typeddependency parses from phrase structure trees.
InLREC.Keith Hall, Ryan McDonald, Jason Katz-brown, andMichael Ringgaard.
2011a.
Training dependencyparsers by jointly optimizing multiple objectives.
InEMNLP ?11: Proceedings of the 2011 Conferenceon Empirical Methods in Natural Language Pro-cessing.Keith Hall, Ryan McDonald, and Slav Petrov.
2011b.Training structured prediction models with extrinsicloss functions.
In Domain Adaptation Workshop atNIPS, October.Sandra Kubler, Ryan McDonald, and Joakim Nivre.2009.
Dependency parsing.
In Synthesis Lectureson Human Language Technologies.
Morgan & Clay-pool Publishers.Joakim Nivre.
2008.
Algorithms for deterministic in-cremental dependency parsing.
volume 34, pages513?553.Yue Zhang and Stephen Clark.
2008.
A tale oftwo parsers: Investigating and combining graph-based and transition-based dependency parsing.
InEMNLP ?08: Proceedings of the 2008 Conferenceon Empirical Methods in Natural Language Pro-cessing, pages 562?571.32
