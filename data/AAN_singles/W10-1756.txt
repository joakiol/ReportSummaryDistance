Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 365?374,Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational LinguisticsA Unified Approach to Minimum Risk Training and DecodingAbhishek Arun, Barry Haddow and Philipp KoehnSchool of InformaticsUniversity of EdinburghEdinburgh, EH8 9AB, UKa.arun@sms.ed.ac.uk, {bhaddow,pkoehn}@inf.ed.ac.ukAbstractWe present a unified approach to perform-ing minimum risk training and minimumBayes risk (MBR) decoding with BLEUin a phrase-based model.
Key to our ap-proach is the use of a Gibbs sampler thatallows us to explore the entire probabil-ity distribution and maintain a strict prob-abilistic formulation across the pipeline.We also describe a new sampling algo-rithm called corpus sampling which al-lows us at training time to use BLEU in-stead of an approximation thereof.
Ourapproach is theoretically sound and givesbetter (up to +0.6%BLEU) and more sta-ble results than the standard MERT opti-mization algorithm.
By comparing our ap-proach to lattice MBR, we are also able togain crucial insights about both methods.1 IntroductionAccording to statistical decision theory, the opti-mal decision rule for any statistical model is thesolution that minimizes its risk (expected loss).This solution is often referred to as the MinimumBayes Risk (MBR) solution (Kumar and Byrne,2004).
Since machine translation (MT) mod-els are typically evaluated by BLEU (Papineni etal., 2002), a loss function which rewards partialmatches, the MBR solution is to be preferred tothe Maximum A Posteriori (MAP) solution.In most statistical MT (SMT) systems, MBRis implemented as a reranker of a list1 of trans-lations generated by a first-pass decoder.
This de-coder typically assigns unnormalised log probabil-ities (known as scores) to each translation hypoth-1We use the term list to denote any enumerable represen-tation of translation hypotheses e.g n-best list, translation lat-tice or forest.esis, so these scores must be converted to proba-bilities in order to apply MBR.
In order to performthis conversion, it is first necessary to compute thenormalization function Z.
Since Z is defined asan intractable sum over all possible translations, itis approximated by summing over the translationsin the list.
The second step is to find the correctscale factor for the scores using a hyper-parametersearch over held-out data.
This is needed becausethe model parameters for the first-pass decoder arenormally learnt using MERT (Och, 2003), whichis invariant under scaling of the scores.Both these steps are theoretically unsatisfactorymethods of estimating the posterior probabilitydistribution since the approximation to Z is an un-bounded term and the scaling factor is an artificialway of inducing a probability distribution.Recently, (Tromble et al, 2008; Kumar et al,2009) have shown that using a search lattice to im-prove the estimation of the true probability distri-bution can lead to improved MBR performance.However, these approaches still rely on MERT fortraining the base model, and in fact introduce sev-eral extra parameters which must also be estimatedusing either grid search or a second MERT run.The lattice pruning required to make these tech-niques tractable is quite drastic, and is in addi-tion to the pruning already performed during thesearch.
Such extensive pruning is liable to renderany probability estimates heavily biased (Blunsomand Osborne, 2008; Bouchard-Co?te?
et al, 2009).Here, we present a unified approach to trainingand decoding in a phrase-based translation model(Koehn et al, 2003) which keeps the objectiveconstant across the translation pipeline and so ob-viates the need for any extra hyper-parameter fit-ting.
We use the phrase-based Gibbs sampler ofArun et al (2009) at training time to compute thegradient of our minimum risk training objective inorder to apply first-order optimization techniques,365and at test time we use it to estimate the posteriordistribution required by MBR (Section 3).We experimented with two different objectivefunctions for training (Section 4).
First, follow-ing (Arun et al, 2009), we define our objectiveat the sentence-level using a sentence-level variantof BLEU.
Then, in order to reduce the mismatchbetween training and test loss functions, we alsotried directly optimising the expected corpus levelBLEU, where we introduce a novel sampling tech-nique, which we call corpus sampling to calculatethe required expectations.The methods presented in this paper are theo-retically sound.
Moreover, experimental evidenceon three language pairs shows that our trainingregime is more stable than MERT, able to gener-alize better and generally leads to improvement intranslation when used with sampling based MBR(Section 5).
An added benefit is that the trainedweights also lead to better performance when usedwith a beam-search based decoder.2 Inference methods for MTWe assume a phrase-based machine translationmodel, defined with a log-linear form, with featurefunction vector h and parametrized by weight vec-tor ?, as described in Koehn et al (2003).
The in-put sentence, f , is segmented into phrases, whichare sequences of adjacent words.
Each sourcephrase is translated into the target language, toproduce an output sentence e and an alignmenta representing the mapping from source to targetphrases.
Phrases are allowed to be reordered.p(e, a|f ;?)
=exp [?
?
h(e, a, f)]??e?,a??
exp [?
?
h(e?, a?, f)](1)MAP decoding under this model consists offinding the most likely output string, e?:e?
= argmaxe?a?4(e,f)p(e, a|f) (2)where4(e, f) is the set of all derivations of outputstring e given source string f .Summing over all the derivations is intractable,making approximations necessary.
The most com-mon of these approximations is the Viterbi approx-imation, which simply chooses the most likelyderivation ?e?, a??.
This approximation can becomputed in polynomial time via dynamic pro-gramming (DP).
Though fast and effective formany problems, it has two serious drawbacks forprobabilistic inference.
First, the error incurredby the Viterbi maximum with respect to the truemodel maximum is unbounded.
Second, the DPsolution requires substantial pruning and restrictsthe use of non-local features.
The latter problempersists even in the variational approximations ofLi et al (2009), which attempt to solve the former.2.1 Gibbs sampling for phrase-based MTAn alternate approximate inference method forphrase-based MT without any of the previouslymentioned drawbacks is the Gibbs sampler (Ge-man and Geman, 1984) of Arun et al (2009)which draws samples from the posterior distribu-tion of the translation model.
For the work pre-sented in this paper, we use this sampler.The sampler produces a sequence of samples,SN1 = (e1, a1) .
.
.
(eN , aN ), that are drawn fromthe distribution p(e, a|f).
These samples can beused to estimate the expectation of a functionh(e, a, f) as follows:Ep(a,e|f)[h] = limN?
?1NN?i=1h(ai, ei, f) (3)3 DecodingIn this work, we are interested in performing MBRdecoding with BLEU.
We define the MBR decisionrule following Tromble et al (2008):e?
= argmaxe?H?e??EBLEUe(e?
)p(e?|f) (4)where H refers to the hypothesis space fromwhich translations are chosen, E refers to theevidence space used for calculating risk andBLEUe(e?)
is a gain function that indicates the re-ward of hypothesising e?
when the reference solu-tion is e.To perform MBR decoding using the sampler,let the function h in Equation 3 be the indica-tor function h = ?
(a, a?)?
(e, e?).
Then, Equa-tion 3 provides an estimate of p(a?, e?|f), and usingh = ?
(e, e?)
marginalizes over all derivations a?,yielding an estimate of p(e?|f).
MBR is computedat the sentence-level while BLEU is a corpus-levelmetric, so instead we use a sentence-level approx-imation of BLEU.2The sampler can be used to perform two otherdecoding tasks: the mode of the estimated dis-tribution p(a?, e?|f) is the maximum derivation(MaxDeriv) solution while the mode of p(e?|f) isthe maximum translation (MaxTrans) solution.2The ngram precision counts are smoothed by adding 0.01for n > 13664 Minimum Risk TrainingIn order to train models suitable for use with Max-Trans or MBR decoding, we need to employ atraining method which takes account of the wholedistribution.
To this end, we employ minimum risktraining to find weights ?
for Equation 1 that mini-mize the expected loss on the training set.
We con-sider two variants of minimum risk training: sen-tence sampling optimizes an objective defined atthe sentence level and corpus sampling a corpus-based objective.4.1 Sentence samplingSince BLEU, the metric we care about, is a gainfunction, our objective function maximizes the ex-pected gain of our model.
The expected gain, Gof a probabilistic translation model on a corpus D,defined with respect to the gain function BLEUe?
(e)is given byG =??e?,f?
?D?e,ap(e, a|f)BLEUe?
(e) (5)where e?
is the reference translation, e is a hypoth-esis translation and BLEU refers to the sentence-level approximation of the metric.Using the probabilistic formulation of Equation1, the optimization of the objective in (5) is facil-itated by the fact that it is continuous and differ-entiable with respect to the model parameters ?
togive?G??k=??e?,f??D?e,aBLEUe?(e)?p??kwhere?p?
?k=(hk ?
Ep(e,a|f)[hk])p(e, a|f)(6)Since the gradient is expressed in terms of ex-pectations of feature values, it can easily be calcu-lated using the sampler and then first-order opti-mization techniques can be applied to find optimalvalues of ?.
Because of the noise introduced bythe sampler, we used stochastic gradient descent(SGD), with a learning rate that gets updated aftereach step proportionally to difference in succes-sive gradients (Schraudolph, 1999).While our initial formulation of minimum risktraining is similar to that of Arun et al (2009), inpreliminary experiments we observed a tendencyfor translation performance on held-out data toquickly increase to a maximum and then plateau.Hypothesizing that we were being trapped in lo-cal maxima as G is non-convex, we decided toemploy deterministic annealing (Rose, 1998) tosmooth the objective function to ensure that theoptimizer explored as large a region as possible ofthe space before it settled on an optimal weight set.Our instantiation of deterministic annealing (DA)is based on the work of Smith and Eisner (2006),and involves the addition of an entropic prior tothe objective in Equation 5 to giveG?
=??e?,f?
?D[(?e,ap(e, a|f)BLEUe?
(e))+ T.H(p)]where H(p) is the entropy of the probability dis-tribution p(e, a|f), and T > 0 is a temperatureparamater which is gradually lowered as the opti-mization progresses according to some annealingschedule.Differentiating with respect to ?k then showsthat the annealed gradient is given by the follow-ing expression:??e?,f??D?e,a(BLEUe?(e)?
T (1 + log p))?p??kwhere?p?
?k=(hk ?
Ep(e,a|f)[hk])p(e, a|f)A high value of T leads the optimizer to findweights which describe a fairly flat distribution,whereas a lower value of T pushes the optimizertowards a more peaked distribution.
We perform10 to 20 iterations of SGD at each temperature.In their deterministic annealing formulation,(Smith and Eisner, 2006; Li and Eisner, 2009), ex-press the parameterization of the distribution ?
as???
(where ?
is the scaling factor) and perform op-timization in two steps, the first optimizing ??
andthe second optimizing ?.
We experimented withthis two stage optimization process, but found thatsimply performing an unconstrained optimizationon ?
gave better results.4.2 Corpus samplingWhile the objective functions in Equations 5 and4.1 use a sentence-level variant of BLEU, themodel?s test-time performance is evaluated withcorpus level BLEU.
The lack of correlation be-tween sentence-level BLEU and corpus BLEU iswell-known (Chiang et al, 2008a).
Therefore, inan effort to address this issue, we tried maximizingexpected corpus BLEU directly.In other words, given a training corpus of theform ?CF , CE??
where CF is a set of source sen-tences and CE?
its corresponding reference transla-tions, we consider a gain function defined on the367hypothesized translation CE of the input CF withrespect to CE?
.The objective in equation 5 therefore becomes:G =?CEP (CE |CF )BLEUCE?
(CE) (7)The pair (CE , CF ) is denoted as a cor-pus sample corresponding to a sequence(e1, a1), .
.
.
, (eN , aN ) of derivations of thecorresponding source strings f1, .
.
.
, fN ofsource corpus CF .Although the sampler described in Section 2generates samples at the sentence level, we can useit to generate corpus samples by applying the fol-lowing procedure (see Figure 1).
For each sourcesentence f i in the corpus, we generate a sequenceof samples (ei1, ai1), .
.
.
, (ein, ain) using the sam-pler.
From each of these sequences of samples, wethen resample new sequences of derivation sam-ples, one for each source sentence in the corpus.The first corpus sample is then obtained by iter-ating through the source sentences and taking thefirst resampled derivation for each sentence, thenthe second corpus sample by taking the second re-sampled derivation, and so on.
The resamplingstep is necessary to eliminate any biases due to theorder of the generated samples.The corpus sampling procedure invariably gen-erates a set of samples which are all distinct and sowould give us a uniform estimate of the probabil-ity distribution P (CE |CF ).
However this is not aproblem since we are not interested in evaluatingthe actual distribution; we just need to calculateexpectations of feature values and BLEU scoresover the distribution.
The feature values of a cor-pus sample are the average of the feature values ofits constituting derivations and its BLEU score iscomputed based on the yield of its derivations.When training using corpus sampling we pro-cess the training corpus in batches ?CF , CE?
?, treat-ing each batch as a corpus in its own right, andupdating the weights after each batch.The gradient for the objective function in (7) is:?G??k=?CEBLEUCE?
(CE)?P??kwhere?P?
?k=(hCk ?
EP (CE |CF )[hCk ])P (CE |CF )where hCk is the k-th component of a corpussample feature vector.During deterministic annealing for sentencesampling, the entropy term is computed over thef1 f2 f3A D KB E LA F LC G LB H Mf1 f2 f3A F LB E LSAMPLEFROMEMPIRICALDISTRIBUTIONExtract CorpusSamplesf1 f2 f3{A, F, L }Corpus Sample 1{B, E, L }Corpus Sample 2SAMPLE FROMP(e,a | f)Figure 1: Example illustrating the extraction of 2corpus samples for a corpus of source sentencesf1, f2, f3.
In the first step, we sample 5 deriva-tions for each source sentence.
We then resample2 derivations from the empirical distributions ofeach source sentence.distribution p(e, a|f) of each individual sentence.While corpus sampling, we are considering thedistribution P (CE |CF ) but the estimated distribu-tion is always uniform.
So we define the entropicprior term over the distribution p(e, a|f) of thesentences making up the corpus sample.The annealed corpus sampling objective istherefore:?CEP (CE |CF )BLEUCE?
(CE)+T|CF |?f?CFH(p(e, a|f))The gradient of this objective is of similar formto the sentence sampling gradient in Equation (6).5 Experiments5.1 Training Data and PreparationThe experiments in this section were performedusing the Europarl section of the French-Englishand German-English parallel corpora from theWMT09 shared translation task (Callison-Burch etal., 2009), as well as 300k parallel Arabic-Englishsentences from the NIST MT evaluation train-ing data.3 For all language pairs, we constructed3The Arabic-English training data consists of theeTIRR corpus (LDC2004E72), the Arabic news corpus(LDC2004T17), the Ummah corpus (LDC2004T18), and the368a phrase-based translation model as described inKoehn et al (2003), limiting the phrase length to5.
The target side of the parallel corpus was usedto train 3-gram language models.
For the Germanand French systems, the DEV2006 set was usedfor model tuning and the first half of TEST2007(in-domain) for heldout testing.
Final testing wasperformed on NEWS-DEV2009B (out-of-domain)and the first half of TEST2008 (in-domain).
Forthe Arabic system, the MT02 set (10 referencetranslations) was used for tuning and MT03 andMT05 (4 reference translations, each) were usedfor held-out testing and final testing respectively.To reduce the size of the phrase table, we used theassociation-score technique suggested by Johnsonet al (2007).
Translation quality is reported usingcase-insensitive BLEU.5.2 BaselineOur baseline system is phrase-basedMoses (Koehn et al, 2007) with feature weightstrained using MERT.
Moses and the Gibbssampler use identical feature sets.4The MERT optimization algorithm uses multi-ple random restarts to avoid getting stuck in a poorlocal optima.
Therefore, every time MERT is run,it produces a slightly different final weight vectorleading to varying test set results.
While this char-acteristic of MERT is typically ignored, we ac-count for it by performing MERT training 10 timesfor each of the 3 language pairs, decoding the testsets with each of the 10 optimized weight sets.
Wepresent the best and the worst test set results alongwith the mean and the standard deviation (?)
ofthese results in Table 1.
We report results usingthe Moses implementation of Viterbi, nbest MBRand lattice MBR decoding (Kumar et al, 2009).
5For both nbest and lattice MBR decoding, the hy-pothesis set was composed of the top 1000 uniquetranslations produced by the Viterbi decoder, andthe same 1000 translations were used as evidenceset for nbest MBR.As Table 1 shows, translation results usingMERT optimized weights vary markedly from onesentences with confidence c > 0.995 in the ISI automaticallyextracted web parallel corpus (LDC2006T02).4We use 5 translation model scores, distance-based distor-tion, language model and word penalty.
The reordering limitis set to 6 for all experiments.5For nbest and lattice MBR decoding, we optimized forthe scaling factor using a grid-search on held-out data.
Forlattice MBR decoding, we optimized the lattice density andset the p and r parameters as per Tromble et al (2008).tuning run to the other, with results varying froma range of 0.3% BLEU to 1.3% BLEU when usingViterbi decoding.
We also see that, bar in-domainGerman to English, MBR decoding gives a smallimprovement on all other datasets.Surprisingly, lattice MBR only gives improve-ments on two datasets and actually leads to a dropin performance on the other 3 datasets.
We discusspossible reasons for this in Section 6.5.3 Sentence samplingAt training time, the optimization algorithm is ini-tialized with zero weights and the sampler is ini-tialized with a random derivation from Moses.
Toget rid of any initialization biases, the first 100samples are discarded.6 We then run the samplerfor 1000 iterations after which we perform reheat-ing whereby the distribution is progressively flat-tened.
Samples are not collected during this pe-riod.
Reheating allows the sampler more mobil-ity around the search space thus possibly escapingany local optima it might be trapped in.
We subse-quently run the sampler for 1000 more iterations.We denote this procedure as running 2 chains ofthe sampler.
We use batch sizes of 96 randomlyselected sentences for SGD optimization.During DA, our cooling schedule is an exponen-tially decaying one with decay rate set to 0.9, per-forming 20 iterations of SGD optimization at eachtemperature setting.
Five training runs were per-formed and the BLEU scores averaged.
The fea-ture weights were output every 50 iterations andperformance measured on the heldout set by run-ning the sampler as a decoder.
At decode time,we use the same sampler configurations as duringtraining but run 2 chains each for 5000 iterations.For MBR decoding, we use the entirety of thissample set as our evidence set and use the top 1000most probable translations as the hypothesis set.5.4 Corpus samplingFor our corpus sampling experiments, we sampleusing the same procedure as in sentence samplingbut using 2 chains of 2000 iterations.
We thenresample 2000 corpus samples from the empiri-cal distribution estimated from the first 4000 sam-ples.
For Arabic-English training, we used batchsizes of 100 randomly selected sentences for ex-periments without DA and batches of 400 random6This procedure is referred to as burn-in in the MCMCliterature.369Viterbi nMBR lMBRmin max mean ?
min max mean ?
min max mean ?AR-EN MT05 43.7 44.3 44.0 0.17 44.2 44.5 44.4 0.13 44.2 44.6 44.5 0.12FR-EN In 33.1 33.4 33.3 0.10 33.2 33.6 33.4 0.12 32.3 32.7 32.6 0.13FR-EN Out 19.1 19.6 19.4 0.18 19.3 19.7 19.5 0.12 19.1 19.4 19.3 0.12DE-EN In 27.6 27.9 27.8 0.10 27.6 27.9 27.7 0.10 27.2 27.5 27.4 0.10DE-EN Out 14.9 16.2 15.7 0.33 15.0 16.3 15.7 0.33 15.3 16.4 16.0 0.30Table 1: Baseline results - MERT trained models decoded using Viterbi, nbest MBR (nMBR) and latticeMBR (lMBR).
MERT was run 10 times for each language pair.
We report minimum, maximum, meanand standard deviation of test set BLEU scores across the 10 runs.200 400 600 80015202530Training iterationsBleuSentence Sampling, Without DAl l l l l l l l l l l l l l l l l27.4l MaxDerivMaxTransMBR 100 300 500 70015202530Training iterationsBleuSentence Sampling, With DAlllll ll l l l l l l l l 28.2l MaxDerivMaxTransMBR 50 100 150 200 25015202530Training iterationsBleuCorpus Sampling, Without DAl l l l l l l l l l l l l 28.1l MaxDerivMaxTransMBR 50 150 250 35015202530Training iterationsBleuCorpus Sampling, With DAl ll ll l ll l l l l l l l l l 28.5l MaxDerivMaxTransMBRFigure 2: Heldout performance for German-English training averaged across 5 minimum risk trainingruns.
Best scores achieved are indicated by dotted line.sentences with DA.
The size of the batches cor-responds to the number of sentences that form acorpus sample.
For German/French to English ex-periments, we used batches of 100 random sen-tences for training with and without DA.
We per-form 10 optimizations at each temperature settingduring deterministic annealing.
Test time condi-tions are identical to the sentence sampling onesand we measure performance on a held-out set af-ter every 20 iterations of the learner.5.5 ResultsFigures 2 and 3 show the scores on the German-English and Arabic-English held-out sets respec-tively comparing all four training regimes: corpusvs sentence sampling, DA vs without DA.
Resultsfor French-English training are similar.We focus our analysis on the Arabic-English ex-perimental setup.
Without deterministic anneal-ing, the learner converges quickly, usually afterjust 20 iterations, after which performance de-grades steadily.
The magnitudes of the weightsare large, sharpening the distribution.
There isnot much diversity amongst the sampled deriva-tions, i.e.
the entropy of the sample set is low.Therefore, all 3 decoding regimes give very simi-lar results.
With the addition of the entropic prior,the model is slow to converge before the so-calledphase transition occurs (usually after around 50iterations), after which performance goes up toreach a peak (45.2 BLEU) higher than that withoutthe prior (44.2 BLEU), before steadily declining.The entropic prior encourages diversity among thesample set, especially at high temperature settings.In the presence of diversity, the benefits ofmarginalization over derivations is clear: Max-Trans does better than MaxDeriv and MBR doesbest, confirm recent findings of (Blunsom et al,2008; Arun et al, 2009) that MaxTrans improvesover MaxDeriv decoding for models trained to ac-count for multiple derivations.
As the temperaturedecreases to zero, the model sharpens, effectivelyintent on maximizing one-best performance andthus voiding the benefits of MaxTrans and MBR.Figures 2 and 3 also show that corpus samplingimproves over sentence sampling, although not bymuch (+ 0.3 BLEU).5.6 Comparison with MERT baselineHaving established the superiority of the pipelineof expected corpus BLEU training with DA fol-lowed by MBR decoding over other alternativesconsidered, we compare it to the best results ob-tained with MERT optimized Moses (bold scoresfrom Table 1).
To account for sampler varianceduring both training and decoding, we averagescores across 50 runs; 10 decoding runs each usingthe best weight set from 5 training runs.
Results3700 200 600 1000 14003035404550Training iterationsBleuSentence Sampling, Without DAl l l l l l l l l l l l l l l l l l l l l l l l l l l l l44.2l MaxDerivMaxTransMBR 0 500 1000 15003035404550Training iterationsBleuSentence Sampling With DAll l ll l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l 45.2l MaxDerivMaxTransMBR 20 40 60 80 100 1403035404550Training iterationsBleuCorpus Sampling, Without DAl l l l l l l 44.5l MaxDerivMaxTransMBR 100 200 300 4003035404550Training iterationsBleuCorpus Sampling, With DAll ll l l l l l l l l l l l l l l l l l 45.5l MaxDerivMaxTransMBRFigure 3: Heldout performance for Arabic-English training averaged across 5 minimum risk trainingruns.
Best scores achieved are indicated by dotted line.are shown in Table 2.7We observe that on 3 out of 5 datasets, the sam-pler results are much more stable than MERT andas stable on the other 2 datasets.
We attribute theimproved stability to the more powerful optimiza-tion algorithm used by the sampler which uses gra-dient information to steer the model towards betterweights.
MERT, alternatively, optimizes one fea-ture at a time using line search and therefore doesnot explore the full feature space as thoroughly.Translation results with the sampler are betterthan with MERT on 2 datasets, are equal on an-other 2 and worse in one case.
The improvementswithe the sampler are obtained in the case of out-of-domain data suggesting that the minimum risktraining objective generalizes better than the 1-best objective of MERT.MERT/Moses SamplerTest set Best ?
MBR ?AR-EN MT05 44.5 (lMBR) 0.12 44.5 0.14FR-EN In 33.4 (nMBR) 0.12 33.2 0.06FR-EN Out 19.5 (nMBR) 0.12 19.8 0.05DE-EN In 27.8 (Viterbi) 0.10 27.8 0.11DE-EN Out 16.0 (lMBR) 0.30 16.6 0.12Table 2: Final results comparing MERT/Mosespipeline with unified sampler pipeline.
Sampleruses corpus sampling during training and MBRdecoding at test time.
Moses results are aver-aged across decoding runs using weights from10 MERT runs and sampler results are averagedacross 10 decoding runs for each of 5 differenttraining runs.
We report BLEU scores and standarddeviation (?
).7The MBR decoding times, averaged over 10 decodingruns of 50 sentences each, are 10 secs/sent for Moses nbestMBR, 40 secs/sent for Moses lattice MBR and 180 secs/sentfor the sampler.Viterbi nMBR lMBR SamplerMBRAR-EN MT05 44.2 44.4 44.8 44.8FR-EN In 33.1 33.2 33.3 33.3FR-EN Out 19.6 19.8 19.9 19.9DE-EN In 27.7 27.9 28.0 28.0DE-EN Out 16.0 16.3 16.6 16.6Table 3: Comparison of decoding methods usingexpected BLEU trained weights.
We report Viterbi,nbest MBR (nMBR) and lattice MBR (lMBR) de-coding scores vs best sampler MBR decoding per-formance.
We selected the best weight set basedon performance on heldout data.5.7 Moses with expected BLEU weightsIn a final set of experiments, we reran the Mosesdecoder this time using weights obtained throughexpected BLEU optimization.
Here, for each lan-guage pair, we picked the weight set that gave thebest results on held-out data.
Note that the resultswhich we show in Table 3 are over one run only,so are not strictly comparable to those in Table 2which are averaged over several training and de-coding runs.
We also report the best results ob-tained with the sampler MBR decoder using theseweights.In contrast to Table 1, here we see a consistentimprovement across all test-sets when going fromViterbi decoding to n-best then to lattice MBR.Except for in-domain French-English, the transla-tion results are superior to the best scores shown(in bold) in Table 1, confirming that the minimumrisk training objective is able to find good weightsets.
Interestingly, we also observe that samplerMBR gets the same exact results for all test sets aslattice MBR.3716 DiscussionWe have shown that the sampler of Arun et al(2009) can be used to perform minimum risk train-ing over an unpruned search space.
Our pro-posed corpus sampling technique, like MERT, isable to optimize corpus BLEU directly whereasalternate parameter estimation techniques usuallyemployed in SMT optimize approximations ofBLEU.
Chiang et al (2008b) accounts for the on-line nature of the MIRA optimization algorithmby smoothing the sentence-level BLEU precisioncounts of a translation with a weighted average ofthe precision counts of previously decoded sen-tences, thus approximating corpus BLEU.
Asfor minimum risk training, prior implementationshave either used sentence-level BLEU (Zens et al,2007) or a linear approximation to BLEU (Smithand Eisner, 2006; Li and Eisner, 2009).At test time, the sampler works best as an MBRdecoder, but also allows us to verify past claimsabout the benefits of marginalizing over align-ments during decoding.
We compare the sam-pler MBR decoder?s performance against MERT-optimized Moses run under three different decod-ing regimes, finding that the sampler does as wellor better on 4 out of 5 datasets.Our training and testing pipeline has the advan-tage of being able to handle a large number of bothlocal and global features so we expect in the futureto outperform the standard MERT and dynamicprogramming-based search pipeline further.As shown in Section 5.2, lattice MBR in somecases leads to a marked drop in performance.
(Ku-mar et al, 2009) mention that the linear approx-imation to BLEU used in their lattice MBR algo-rithm is not guaranteed to match corpus BLEU, es-pecially on unseen test sets.
To account for thesecases, they allow their algorithm to back-off to theMAP solution.
One possible reason for the dropin performance in our lattice MBR experiments isthat the implementation we use does not employthis back-off strategy.Table 3 provides valuable insights as to the mer-its of the lattice MBR approach versus our ownsampling based pipeline.
Firstly, whereas withMERT optimized weights, the benefits of latticeMBR are debatable (Table 1), running Moses withminimum risk trained weights gives results thatare in line with what we would expect - latticeMBR does systematically better than competingdecoding algorithms.
This suggests that the unbi-ased minimum risk training criterion used by thesampler is a better fit for lattice MBR than theMERT criterion, and also that the mismatch be-tween linear and corpus BLEU mentioned beforemight not be the reason for the results in Table 1.Secondly, we find that sampling MBR matcheslattice MBR on the minimum risk trained weights.The MBR sampler uses samples drawn from thedistribution as hypothesis and evidence sets, typi-cally 1000 samples for the former and 10000 sam-ples for the latter.
In the lattice MBR experimentsof Tromble et al (2008), it is shown that this sizeof hypothesis set is sufficient.
Their evidence set,however, is significantly larger than ours.8Table 3suggests that, since it is not biased by heuris-tic pruning, the sampler?s limited evidence set isenough to give a good estimate of the probabil-ity distribution whereas beam-search based MBRneeds to scale from using n-best lists to lattices toget equivalent results.Sampling the phrase-based model is expensive,meaning that lattice MBR is still faster (around4x) to run than sampler MBR.
However, due tothe unified nature of the training and decoding cri-terion in our approach, the minimum risk trainedweights can be plugged directly into the sam-pler MBR decoder, whereas lattice MBR requiresan additional expensive step of tuning the modelhyper-parameters (Kumar et al, 2009).In future work, we also intend to look at moreefficient ways of generating samples.
One pos-sibility is to interleave Gibbs sampling steps us-ing low order ngram language model distributionswith Metropolis-Hasting steps that use higher or-der language model distributions.7 Related WorkExpected BLEU training for phrase-based modelshas been successfully attempted by (Smith andEisner, 2006; Zens et al, 2007), however they bothused biased n-best lists to approximate the pos-terior distribution.
Li and Eisner (2009) presentwork on performing expected BLEU training withdeterministic annealing on translation forests gen-erated by Hiero (Chiang, 2007).
Since BLEU doesnot factorize over the search graph, they use thelinear approximation of Tromble et al (2008) in-stead.Pauls et al (2009) present an alternate trainingcriterion over translation forests called CoBLEU,8up to 1081 as per Tromble et al (2008)372similar in spirit to expected BLEU training, butaimed to maximize the expected counts of n-gramsappearing in reference translations.
This trainingcriterion is used in conjunction with consensus de-coding (DeNero et al, 2009), a linear-time ap-proximation of MBR.In contrast to the approaches above, the algo-rithms presented in this paper are able to explorean unpruned search space.
By using corpus sam-pling, we can perform minimum risk training withcorpus BLEU rather than any approximations ofthis metric.
Also, since we maintain a probabilis-tic formulation across training and decoding, ourapproach does not require a grid-search for a scal-ing factor as in Tromble et al (2008).8 ConclusionsWe have presented a unified approach to the taskof parameter estimation and decoding for a phrase-based system using the standard translation eval-uation metric, BLEU.
Using a Gibbs sampler toexplore the entire probability distribution allowsus to implement two probabilistic sound algo-rithms, minimum risk training and its equivalent,MBR decoding, in an unbiased way.
The proba-bilistic formulation also allows us to use gradientbased optimization techniques which produce sta-ble model parameters.
At decoding time, we showthe benefits of marginalizing over derivations andthat MBR gives better results than other decodingcriteria.Since our optimization algorithm can cope witha large number of features, in future work, weplan to incorporate more expressive features inthe model.
We use a Gibbs sampler for inferenceso there is scope for exploring non-local featureswhich might not easily be added to dynamic pro-gramming based models.AcknowledgmentsThis research was supported in part by the GALE pro-gram of the Defense Advanced Research Projects Agency,Contract No.
HR0011-06-2-001; and by the EuroMa-trix project funded by the European Commission (6thFramework Programme).
The project made use ofthe resources provided by the Edinburgh Compute andData Facility (http://www.ecdf.ed.ac.uk/).
TheECDF is partially supported by the eDIKT initiative(http://www.edikt.org.uk/).ReferencesAbhishek Arun, Chris Dyer, Barry Haddow, Phil Blunsom,Adam Lopez, and Philipp Koehn.
2009.
Monte carlo in-ference and maximization for phrase-based translation.
InProceedings of CoNLL, pages 102?110.Phil Blunsom and Miles Osborne.
2008.
Probabilistic infer-ence for machine translation.
In Proc.
of EMNLP 2008.Phil Blunsom, Trevor Cohn, and Miles Osborne.
2008.
Adiscriminative latent variable model for statistical machinetranslation.
In Proc.
of ACL-HLT.Alexandre Bouchard-Co?te?, Slav Petrov, and Dan Klein.2009.
Randomized pruning: Efficiently calculating ex-pectations in large dynamic programs.
In Advances inNeural Information Processing Systems 22, pages 144?152.Chris Callison-Burch, Philipp Koehn, Christoph Monz, andJosh Schroeder, editors.
2009.
Proc.
of Workshop on Ma-chine Translations.David Chiang, Steve DeNeefe, Yee Seng Chan, andHwee Tou Ng.
2008a.
Decomposability of transla-tion metrics for improved evaluation and efficient algo-rithms.
In Proceedings of the 2008 Conference on Empir-ical Methods in Natural Language Processing, pages 610?619, Honolulu, Hawaii, October.
Association for Compu-tational Linguistics.David Chiang, Yuval Marton, and Philip Resnik.
2008b.
On-line large-margin training of syntactic and structural trans-lation features.
In Proceedings of the 2008 Conferenceon Empirical Methods in Natural Language Processing,pages 224?233, Honolulu, Hawaii, October.
Associationfor Computational Linguistics.D.
Chiang.
2007.
Hierarchical phrase-based translation.Computational Linguistics, 33(2):201?228.John DeNero, David Chiang, and Kevin Knight.
2009.
Fastconsensus decoding over translation forests.
In Proceed-ings of ACL/AFNLP, pages 567?575.Stuart Geman and Donald Geman.
1984.
Stochastic relax-ation, Gibbs distributions and the Bayesian restoration ofimages.
IEEE Transactions on Pattern Analysis and Ma-chine Intelligence, 6:721?741.J.H.
Johnson, J. Martin, G. Foster, and R. Kuhn.
2007.Improving translation quality by discarding most of thephrasetable.
In Proc.
of EMNLP-CoNLL, Prague.P.
Koehn, F.J. Och, and D. Marcu.
2003.
Statistical phrase-based translation.
In Proc.
of HLT-NAACL, pages 48?54,Morristown, NJ, USA.P.
Koehn, H. Hoang, A. Birch Mayne, C. Callison-Burch,M.
Federico, N. Bertoldi, B. Cowan, W. Shen, C. Moran,R.
Zens, C. Dyer, O. Bojar, A. Constantin, and E. Herbst.2007.
Moses: Open source toolkit for statistical machinetranslation.
In Proceedings of ACL Demos, pages 177?180.S.
Kumar and W. Byrne.
2004.
Minimum Bayes-risk decod-ing for statistical machine translation.
In Processings ofHLT-NAACL.Shankar Kumar, Wolfgang Macherey, Chris Dyer, and FranzOch.
2009.
Efficient minimum error rate training andminimum bayes-risk decoding for translation hypergraphsand lattices.
In Proceedings of ACL/AFNLP, pages 163?171.Zhifei Li and Jason Eisner.
2009.
First- and second-orderexpectation semirings with applications to minimum-risktraining on translation forests.
In Proceedings of EMNLP,pages 40?51.373Zhifei Li, Jason Eisner, and Sanjeev Khudanpur.
2009.
Vari-ational decoding for statistical machine translation.
InProceedings of ACL/AFNLP, pages 593?601.F.
Och.
2003.
Minimum error rate training in statistical ma-chine translation.
In Proceedings of ACL, pages 160?167.K.
Papineni, S. Roukos, T. Ward, and W.-J.
Zhu.
2002.BLEU: a method for automatic evaluation of machinetranslation.
In Proceedings of ACL, pages 311?318.Adam Pauls, John Denero, and Dan Klein.
2009.
Consensustraining for consensus decoding in machine translation.
InProceedings of EMNLP, pages 1418?1427.Kenneth Rose.
1998.
Deterministic annealing for clustering,compression, classification, regression, and related opti-mization problems.
In Proceedings of the IEEE, pages2210?2239.Nicol N. Schraudolph.
1999.
Local gain adaptation instochastic gradient descent.
Technical Report IDSIA-09-99, IDSIA.David A. Smith and Jason Eisner.
2006.
Minimum risk an-nealing for training log-linear models.
In Proceedings ofCOLING-ACL, pages 787?794.Roy Tromble, Shankar Kumar, Franz Och, and WolfgangMacherey.
2008.
Lattice Minimum Bayes-Risk decod-ing for statistical machine translation.
In Proceedings ofEMNLP, pages 620?629.Richard Zens, Sasa Hasan, and Hermann Ney.
2007.
A sys-tematic comparison of training criteria for statistical ma-chine translation.
In Proceedings of EMNLP, pages 524?532.374
