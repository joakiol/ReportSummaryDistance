Use of Mutual Information Based Character Clusters inDictionary-less Morphological Analysis of JapaneseHideki Kashioka, Yasuhiro Kawata, Yumiko Kinjo,Andrew F inch  and Ezra W. Black{kashioka, ykawata, kinjo, finch, black}~it l .atr .co.
jpATR Interpret ing Telecommunicat ions Reserach LaboratoriesAbstractFor languages whose character set is very largeand whose orthography does not require spac-ing between words, such as Japanese, tokenizingand part-of-speech tagging are often the diffi-cult parts of any morphological analysis.
Forpractical systems to tackle this problem, un-controlled heuristics are primarily used.
Theuse of information on character sorts, however,mitigates this difficulty.
This paper presentsour method of incorporating character cluster-ing based on mutual information into Decision-Tree Dictionary-less morphological nalysis.
Byusing natural classes, we have confirmed thatour morphological analyzer has been signifi-cantly improved in both tokenizing and taggingJapanese text.1 In t roduct ionRecent papers have reported cases of successfulpart-of-speech tagging with statistical languagemodeling techniques (Church 1988; Cutting et.al.
1992; Charniak et.
al.
1993; Brill 1994;Nagata 1994; Yamamoto 1996).
Morphologicalanalysis on Japanese, however, is more complexbecause, unlike European languages, no spacesare inserted between words.
In fact, even nativeJapanese speakers place word boundaries incon-sistently.
Consequently, individual researchershave been adopting different word boundariesand tag sets based on their own theory-internaljustifications.For a practical system to utilize the differentword boundaries and tag sets according to thedemands of an application, it is necessary to co-ordinate the dictionary used, tag sets, and nu-merous other parameters.
Unfortunately, sucha task is costly.
Furthermore, it is difficult tomaintain the accuracy needed to regulate theword boundaries.
Also, depending on the pur-pose, new technical terminology may have to becollected, the dictionary has to be coordinated,but the problem of unknown words would stillremain.The above problems will arise so long as adictionary continue to play a principal role.
Inanalyzing Japanese, a Decision-Tree approachwith no need for a dictionary (Kashioka, et.
al.1997) has led us to employ, among other param-eters, mutual information (MI) bits of individ-ual characters derived from large hierarchicallyclustered sets of characters in the corpus.This paper therefore proposes a type ofDecision-Tree morphological nalysis using theMI of characters but with no need for a dic-tionary.
Next the paper describes the use ofinformation on character sorts in morpholog-ical analysis involving the Japanese language,how knowing the sort of each character is use-ful when tokenizing a string of characters intoa string of words and when assigning parts-of-speech to them, and our method of clusteringcharacters based on MI bits.
Then, it proposesa type of Decision-Tree analysis where the no-tion of MI-based character and word clusteringis incorporated.
Finally, we move on to an ex-perimental report and discussions.2 Use  of  In fo rmat ion  on  CharactersMany languages in the world do not inserta space between words in the written text.Japanese is one of them.
Moreover, the num-ber of characters involved in Japanese is verylarge.
1a Unlike English being basically written in a 26-character alphabet, the domain of possible charactersappearing in an average Japanese text is a set involvingtens of thousands of characters,6582.1 Character  SortThere are three clearly identifiable charactersorts in Japanese: 2Kanj i  are Chinese characters adopted forhistorical reasons and deeply rooted inJapanese.
Each character carries a seman-tic sense.H i ragana are basic Japanes e phonograms rep-resenting syllables.
About fifty of themconstitute the syllabary.Katakana  are characters corresponding to hi-ragana, but their use is restricted mainlyto foreign loan words.Each character sort has a limited number of el-ements, except for Kanji whose exhaustive listis hard to obtain.Identifying each character sort in a sen-tence would help in predicting the word bound-aries and subsequently in assigning the parts-of-speech.
For example, between characters of dif-ferent sorts, word boundaries are highly likely.Accordingly, in formalizing heuristics, charactersorts must be assumed.2.2 Character  C lusterApart from the distinctions mentioned above,are there things such as natural classes with re-spect to the distribution of characters in a cer-tain set of sentences (therefore, the classes areempirically learnable)?
If there are, how can weobtain such knowledge?It seems that only a certain group of charac-ters tends to occur in a certain restricted con-text.
For example, in Japanese, there are manynumerical classifier expressions attached imme-diately after numericals.
3 If such is the case,these classifiers can be clustered in terms oftheir distributions with respect o a presumablynatural class called numericals.
Supposing oneof a certain group of characters often occurs asa neighbor to one of the other groups of char-acters, and supposing characters are clusteredand organized in a hierarchical fashion, then itis possible to refer to such groupings by pointing~Other sorts found in ordinary text are Arabic nu-merics, punctuations, other symbols, etc.3For example, " 3 ~ (san-satsu)" for bound ob-jects "3 copies of", "2 ~ (ni-mai)" for flat objects "2pieces~sheets of".out a certain node in the structure.
Having away of organizing classes of characters i  clearlyan advantage in describing facts in Japanese.The next section presents uch a method.3 Mutua l  In fo rmat ion-BasedCharacter  C lus ter ingOne idea is to sort words out in terms of neigh-boring contexts.
Accordingly research as beencarried out on n-gram models of word cluster-ing (Brown et.
al.
1992) to obtain hierarchicalclusters of words by classifying words in such away so as to minimizes the reduction of MI.This idea is general in the clustering of anykind of list of items into hierarchical classes.
4We therefore have adopted this approach notonly to compute word classes but also to com-pute character clusterings in Japanese.The basic algorithm for clustering itemsbased on the amount of MI is as follows: s1) Assign a singleton class to every item in theset.2) Choose two appropriate classes to create anew class which subsumes them.3) Repeat 2) until the additional new itemsinclude all of the items in the set.With this method, we conducted an experi-mental clustering over the ATR travel conver-sation corpus.
6 As a result, all of the charac-ters in the corpus were hierarchically clusteredaccording to their distributions.Example:  A partial character clustering-+ .
.
.
.
.
.
.
.
.
~: 0000000110111+-+-+-+- - -  ~lJ 0000000111000000I I +-+-  ~ 00000001110000010I I * -  f-~ 00000001110000011\[ + .
.
.
.
.
~ 000000011100001+ ~_~ 00000001110001000Each node represents a subset of all of thedifferent characters found in the training data.We represent tree structured clusters with bitstrings, so that we may specify any node in thestructure by using a bit substring.4Brown, et.
al.
(1992) for details.5This algorithm, however, is too costly because theamount of computation exponentially increases depend-ing on the number of items.
For practical processing,the basic procedure is carried out over a certain limitednumber of items, while a new item is supplied to theprocessing set each time clustering is done.880,000 sentences, with a total number of 1,585,009characters and 1,831 different characters.659Numerous significant clusters are foundamong them.
r They are all natural classescomputed based on the events in the trainingset.4 Dec is ion -Tree  Morpho log ica lAnalysisThe Decision-Tree model consists of a set ofquestions structured into a dendrogram witha probability distribution associated with eachleaf of the tree.
In general, a decision-tree is acomplex of n-ary branching trees in which ques-tions are associated with each parent node, anda choice or class is associated with each childnode.
8 We represent answers to questions asbits.Among other advantages to using decision-trees, it is important o note that they are ableto assign integrated costs for classification byall types of questions at different feature levelsprovided each feature has a different cost.4.1 Mode lLet us assume that an input sentence C =cl c2 ... cn denotes a sequence of n charac-ters that constitute words 1?
= Wl w2 ... win,where each word wi is assigned a tag ti (T =tl t2 ... tin).The morphological analysis task can be for-mally defined as finding a set of word segmenta-tions and part-of-speech assignments hat maxi-mizes the joint probability of the word sequenceand tag sequence P(W,T \ [C) .The joint probability P(W, TIC) is calculatedby the following formulae:P(W, TIC ) =I-I;~l P(wi,  ti iwl,... ,  wi -1 ,  t l ,  ..., t i -1 ,  C )P( wi, ti I Wl, ..., wi-1, tl , ..., ti-1, C) =P(wi  \[wl, ..., wi-1, q ,  ..., t~-l,  C) 9 *P( ti\[wl , ..., wi, tl , ..., ti-1, C) 10The Word Model decision-tree is used as theword tokenizer.
While finding word bound-rFor example, katakana, numerical classifiers, numer-ics, postpositional case particles, and prefixes of demon-strative pronouns.SThe work described here employs only binarydecision-trees.
Multiple alternative questions are rep-resented in more than two yes/no questions.
The mainreason for this is the computational efficiency.
Allowingquestions to have more answers complicates the decision-tree growth algorithm.OWe call this the "Word Model".1?~,Ve call this the "Tagging Model".aries, we use two different labels: Word+ andWord- .
In the training data, we label Word+to a complete word string, and Word-  to ev-ery substring of a relevant word since these sub-strings are not in fact a word in the current con-text.
11 The probability of a word estimates theassociated istributions of leaves with a worddecision-tree.We use the Tagging Model decision-tree asour part-of-speech tagger.
For an input sentenceC, let us consider the character sequence fromCl to %-1 (assigned Wl w2 ... wk-1) and thefollowing character sequence from p to p + l tobe the word wk; also, the word wk is assumedto be assigned the tag tk.We approximate the probability of the wordwk assigned with tag tk as follows: P(tk) =p(ti\[wl, ..., wk,q, .
.
.
,  tk-1, C).
This probabilityestimates the associated istributions of leaveswith a part-of-speech tag decision-tree.4.2 Growing  Dec is ion -TreesGrowing a decision-tree requires two steps: se-lecting a question to ask at each node; and de-termining the probability distribution for eachleaf from the distribution of events in the train-ing set.
At each node, we choose from among allpossible questions, the question that maximizesthe reduction in entropy.The two steps are repeated until the followingconditions are no longer satisfied:?
The number of leaf node events exceeds theconstant number.?
The reduction in entropy is more than thethreshold.Consequently, the list of questions is optimallystructured in such a way that, when the dataflows in the decision-tree, at each decision point,the most efficient question is asked.Provided a set of training sentences with wordboundaries in which each word is assigned witha part-of-speech tag, we have a) the neces-sary structured character clusters, and b) thenecessary structured word clusters; 12 both ofthem are based on the n-gram language model.laFor instance, for the word "mo-shi-mo-shi" (hello),"mo-shi-mo-shi" is labeled Word-I - ,  and "mo-shi-mo","mo-shi',  "mo" are all labeled Word- .
Note that "mo-shi" or "mo-shi-mo" may be real words in other contexts,e.g., "mo-shi/wa-ta-shi/ga .
.
.
(If I do .
.
.  )
' .12Here, a word token is based only on a word string,not on a word string tagged with a part-of-speech.660We also have c) the necessary decision-treesfor word-splitting and part-of-speech tagging,each of which contains a set of questions aboutevents.
We have considered the following pointsin making decision-tree questions.1) M I  character  bitsWe define self-organizing character classesrepresented by binary trees, each of whosenodes are significant in the n-gram lan-guage model.
We can ask which node acharacter is dominated by.2) M I  word  bitsLikewise, MI word bits (Brown et.
al.1992) are also available so that we may askwhich node a word is dominated by.3) Quest ions  about  the  target  wordThese questions mostly relate to the mor-phology of a word (e.g., Is it ending in '-shi-i' (an adjective nding)?
Does it startwith 'do-'?
).4) Quest ions  about  the  contextMany of these questions concern continu-ous part-of-speech tags (e.g., Is the pre-vious word an adjective?).
However, thequestions may concern information at dif-ferent remote locations in a sentence (e.g.,Is the initial word in the sentence a noun?
).These questions can be combined in order toform questions of greater complexity.5 Ana lys i s  w i th  Dec is ion -TreesOur proposed morphological nalyzer processeseach character in a string from left to right.Candidates for a word are examined, and atag candidate is assigned to each word.
Wheneach candidate for a word is checked, it is givena probability by the word model decision-tree.We can either exhaustively enumerate and scoreall of the cases or use a stack decoder algorithm(Jelinek 1969; Paul 1991) to search through themost probable candidates.The fact that we do not use a dictionary, 13is one of the great advantages.
By using a dic-tionary, a morphological nalyzer has to dealwith unknown words and unknown tags, 14 andis also fooled by many words sharing commonsubstrings.
In practical contexts, the systema3Here, a dictionary is a listing of words attached topart-of-speech tags.14Words that are not found in the dictionary and nec-essary tags that are not assigned in the dictionary.Table 1: Travel ConversationTraining1,000+MIChr-MIChr2,000+MIChr-MIChr3,000+MIChr-MIChr4,000+MIChr-MIChr5,000+MIChr-MIChr\[I A(%)  B (%)80.67 69.9370.03 62.2486.61 76.4369.65 63.3688.60 79.3371.97 66.4788.26 80.1172.55 67.2489.42 81.9472.41 67.72Training: number of sentenceswith/without Character ClusteringA: Correct word/system output wordsB: Correct tags/system output wordsrefers to the dictionary by using heuristic rulesto find the more likely word boundaries, e.g., theminimum number of words, or the maximumword length available at the minimum cost.
Ifthe system could learn how to find word bound-aries without a dictionary, then there would beno need for such an extra device or process.6 Exper imenta l  Resu l t sWe tested our morphological nalyzer with twodifferent corpora: a) ATR-travel, which is atask oriented dialogue in a travel context, andb) EDR Corpus, (EDR 1996) which consists ofrather general written text.For each experiment, we used the charac-ter clustering based on MI.
Each question forthe decision-trees was prepared separately, withor without questions concerning the characterclusters.
Evaluations were made with respectto the original tagged corpora, from which boththe training and test sentences were taken.The analyzer was trained for an incrementallyenlarged set of training data using or not us-ing character clustering.
15 Table 1 shows re-sults obtained from training sets of ATR-travel.The upper figures in each box indicate the re-sults when using the character clusters, and thelower without using them.
The actual test set of4,147 sentences (55,544 words) was taken from15Another 2,231 sentences (28,933 words) in the samedomain are used for the smoothing.661Table 2: General Written TextTraining3,000+MIChr-MIChr5,000+MIChr-MIChr7,000+MIChr-MIChr9,000+MIChr-MIChr10,000+MIChr-MIChrA (%)IB (%)II83.80 78.1977.56 72.4985.50 80.4278.68 73.8485.97 81.6679.32 75.3086.08 81.2O78.59 74.0586.22 81.3978.94 74.41the same domain.The MI-word clusters were constructed ac-cording to the domain of the training set.
Thetag set consisted of 209 part-of-speech tags.
16For the word model decision-tree, three of 69questions concerned the character clusters andthree of 63 the tagging model.
Their presenceor absence was the deciding parameter.The analyzer was also trained for the EDRCorpus.
The same character clusters as with theconversational corpus were used.
A tag set inthe corpus consisted of 15 parts-of-speech.
Forthe word model, 45 questions were prepared; 18for the Tagging model.
Just a couple of themwere involved in the character clusters.
The re-sults are shown in Table 2.7 Conc lus ion  and  D iscuss ionBoth results show that the use of character clus-ters significantly improves both tokenizing andtagging at every stage of the training.
Consid-ering the results, our model with MI charactersis useful for assigning parts of speech as wellas for finding word boundaries, and overcomingthe unknown word problem.The consistent experimental results obtainedfrom the training data with different wordboundaries and different tag sets in theJapanese text, suggests the method is generallyapplicable to various different sets of corporaconstructed for different purposes.
We believethat with the appropriate number of adequatel~These include common noun, verb, post-position,auxiliary verb, adjective, adverb, etc.
The purposeof this tag set is to perform machine translation fromJapanese to English, German and Korean.questions, the method is transferable to otherlanguages that have word boundaries not indi-cated in the text.In conclusion, we should note that ourmethod, which does not require a dictionary,has been significantly improved by the charac-ter cluster information provided.Our plans for further research include inves-tigating the correlation between accuracy andthe training data size, the number of questionsas well as exploring methods for factoring in-formation from a "dictionary" into our model.Along these lines, a fruitful approach may beto explore methods of coordinating probabilis-tic decision-trees to obtain a higher accuracy.ReferencesBrill, E. (1994) "Some Advances in Transformation-Based Part of Speech Tagging," AAAI-94, pp.722-727.Brown, P., Della Pietra, V., de Souza, P., Lai, J., andMercer, R. (1992) "Class-based n-gram modelsof natural anguage," Computational Linguistics,Vol.
18, No.
4, pp.
467-479.Cutting, D., Kupiec, J., Pedersen, J., and Sibun,P.
(1992) "A Practical Part-of-Speech Tagger,"ANLP-92, pp.
133-140.Charniak, E., Hendrickson, C., Jacobson, N., andPerkowits, M. (1993) "Equations for Part-of-Speech Tagging," AAAI-93, pp.
784-789.Church, K. (1988) "A Stochastic Parts Program andNoun Phrase Parser for Unrestricted Text," Pro-ceedings of the 2nd Conference on Applied Natu-ral Language Processing, Austin-Marriott at theCapitol, Austin, Texas, USA, 1988, pp.
136-143.EDR (1996) EDR Electronic Dictionary Version 1.5Technical Guide.
EDR TR2-007.Jelinek, F. (1969) "A fast sequential decoding algo-rithm using a stack," IBM Journal of Researchand Development, Vol.
13, pp.
675-685.Kashioka, H., Black, E., and Eubank, S. (1997)"Decision-Tree Morphological Analysis without aDictionary for Japanese," Proceedings of NLPRS97, pp.
541-544.Nagata, M. (1994) "A Stochastic Japanese Morpho-logical Analyzer Using a Forward-DP Backward-A* N-Best Search Algorithm," Proceedings ofCOLING-94, pp.
201-207.Paul, D. (1991) "Algorithms for an optimal a*search and linearizing the search in the stack de-coder," Proceedings, ICASSP 91, pp.
693-696.Yamamoto, M. (1996) "A Re-estimation Method forStochastic Language Modeling from AmbiguousObservations," WVLC-4, pp.
155-167.662
