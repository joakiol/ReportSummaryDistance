Compilat ion of Weighted Finite-State Transducers fromDecision TreesRichard  SproatBell Laboratories700 Mounta in  AvenueMurray  Hill, N J, USArws@bel l - labs ,  tomMichae l  R i leyAT&T Research600 Mounta in  AvenueMurray Hill, N J, USAri ley@research, att.
comAbst rac tWe report on a method for compilingdecision trees into weighted finite-statetransducers.
The key assumptions arethat the tree predictions specify how torewrite symbols from an input string,and the decision at each tree node isstateable in terms of regular expressionson the input string.
Each leaf nodecan then be treated as a separate rulewhere the left and right contexts areconstructable from the decisions madetraversing the tree from the root to theleaf.
These rules are compiled into trans-ducers using the weighted rewrite-rulerule-compilation algorithm described in(Mohri and Sproat, 1996).1 In t roduct ionMuch attention has been devoted recently tomethods for inferring linguistic models from data.One powerful inference method that has beenused in various applications are decision trees,and in particular classification and regression trees(Breiman et al, 1984).An increasing amount of attention has alsobeen focussed on finite-state methods for imple-menting linguistic models, in particular finite-state transducers and weighted finite-state trans-ducers; see (Kaplan and Kay, 1994; Pereira et al,1994, inter alia).
The reason for the renewed in-terest in finite-state mechanisms is clear.
Finite-state machines provide a mathematically well-understood computational framework for repre-senting awide variety of information, both in NLPand speech processing.
Lexicons, phonologicalrules, Hidden Markov Models, and (regular) gram-mars are all representable as finite-state machines,and finite-state operations such as union, intersec-tion and composition mean that information fromthese various sources can be combined in useful215and computationally attractive ways.
The readeris referred to the above-cited papers (among oth-ers) for more extensive justification.This paper eports on a marriage of these twostrands of research in the form of an algorithm forcompiling the information in decision trees intoweighted finite-state transducers.
1 Given this al-gorithm, information i ferred from data and rep-resented in a tree can be used directly in a systemthat represents other information, such as lexiconsor grammars, in the form of finite-state machines.2 Qu ick  Rev iew o f  T ree-BasedMode l ingA general introduction to classification and regres-sion trees ('CART') including the algorithm forgrowing trees from data can be found in (Breimanet al, 1984).
Applications of tree-based modelingto problems in speech and NLP are discussed in(Riley, 1989; Riley, 1991; Wang and Hirschberg,1992; Magerman, 1995, inter alia).
In this sectionwe presume that one has already trained a treeor set of trees, and we merely remind the readerof the salient points in the interpretation f thosetrees.Consider the tree depicted in Figure 1, whichwas trained on the TIMIT database (Fisher et al,1987), and which models the phonetic realizationof the English phoneme/aa/ ( /a / )  in various en-vironments (Riley, 1991).
When this tree is usedin predicting the allophonic form of a particularinstance of /aa/, one starts at the root of thetree, and asks questions about the environmentin which the /aa / i s  found.
Each non-leaf node n,dominates two daughter nodes conventionally la-beled as 2n and 2n+ 1; the decision on whether togo left to 2n or right to 2n + 1 depends on the an-swer to the question that is being asked at node n.1The work reported here can thus be seen as com-plementary to recent reports on methods for directlyinferring transducers from data (Oncina et al, 1993;Gildea and Jurafsky, 1995).A concrete xample will serve to illustrate.
Con-sider that we have/aa / in  some environment.
Thefirst question that is asked concerns the number ofsegments, including the /aa/ i tsel f ,  that occur tothe left of the /aa / in  the word in wh ich /aa /oc -curs.
(See Table 1 for an explanation of the sym-bols used in Figure 1.)
In this case, if the /aa /is initial - -  i.e., lseg is 1, one goes left; if thereis one or more segments to the left in the word,go right.
Let us assume that this /aa / i s  initialin the word, in which case we go left.
The nextquestion concerns the consonantal 'place' of artic-ulation of the segment o the right o f /an / ;  if itis alveolar go left; otherwise, if it is of some otherquality, or if the segment to the right o f /aa / i s  nota consonant, hen go right.
Let us assume that thesegment to the right i s /z / ,  which is alveolar, so wego left.
This lands us at terminal node 4.
The treein Figure 1 shows us that in the training data 119out of 308 occurrences o f /aa / in  this environmentwere realized as \[ao\], or in other words that we canestimate the probability o f /aa /be ing  realized as\[ao\] in this environment as .385.
The full set ofrealizations at this node with estimated non-zeroprobabilities is as follows (see Table 2 for a rele-vant set of ARPABET-IPA correspondences):phone probability - log prob.
(weight)ao  0 .385  0 .95aa 0.289 1.24q+aa 0.103 2.27q+ao 0.096 2.34ah 0.069 2.68ax 0.058 2.84An important point to bear in mind is that adecision tree in general is a complete description,in the sense that for any new data point, therewill be some leaf node that corresponds to it.
Sofor the tree in Figure 1, each new novel instanceo f /aa /w i l l  be handled by (exactly) one leaf nodein the tree, depending upon the environment inwhich the /an / f inds  itself.Another important point is that each deci-sion tree considered here has the property thatits predictions pecify how to rewrite a symbol (incontext) in an input string.
In particular, theyspecify a two-level mapping from a set of inputsymbols (phonemes) to a set of output symbols(allophones).3 Qu ick  Rev iew o f  Ru leCompi la t ionWork on finite-state phonology (Johnson, 1972;Koskenniemi, 1983; Kaplan and Kay, 1994) hasshown that systems of rewrite rules of the famil-iar form ?
--* ?/)~ p, where ?, ?, A and p are216regular expressions, can be represented computa-tionally as finite-state transducers (FSTs): notethat ?
represents the rule's input rule, ?
the out-put, and ~ and p, respectively, the left and rightcontexts.Kaplan and Kay (1994) have presented a con-crete algorithm for compiling systems of suchrules into FSTs.
These methods can be ex-tended slightly to include the compilation of prob-abilistic or weighted rules into weighted finite-state-transducers (WFSTs - -  see (Pereira et al,1994)): Mohri and Sproat (1996) describe a rule-compilation algorithm which is more efficient hanthe Kaplan-Kay algorithm, and which has beenextended to handle weighted rules.
For presentpurposes it is sufficient o observe that given thisextended algorithm, we can allow ?
in the expres-sion ?
--~ ?
/~ p, to represent a weighted reg-ular expression.
The compiled transducer corre-sponding to that rule will replace ?
with ?
withthe appropriate weights in the context A p.4 The  Tree  Compi la t ion  A lgor i thmThe key requirements on the kind of decision treesthat we can compile into WFSTs are (1) the pre-dictions at the leaf nodes specify how to rewritea particular symbol in an input string, and (2)the decisions at each node are stateable as regu-lar expressions over the input string.
Each leafnode represents a single rule.
The regular expres-sions for each branch describe one aspect of theleft context )~, right context p, or both.
The leftand right contexts for the rule consist of the inter-sections of the partial descriptions of these con-texts defined for each branch traversed betweenthe root and leaf node.
The input ?
is prede-fined for the entire tree, whereas the output ?
isdefined as the union of the set of outputs, alongwith their weights, that are associated with theleaf node.
The weighted rule belonging to the leafnode can then be compiled into a transducer us-ing the weighted-rule-compilation algorithm refer-enced in the preceding section.
The transducer forthe entire tree can be derived by the intersectionof the entire set of transducers associated with theleaf nodes.
Note that while regular relations arenot generally closed under intersection, the subsetof same-length (or more strictly speaking length-preserving) relations is closed; see below.To see how this works, let us return to the ex-ample in Figure 1.
To start with, we know thatthis tree models the phonetic realization o f /aa / ,so we can immediately set ?
to be aa for the wholetree.
Next, consider again the traversal of the treefrom the root node to leaf node 4.
The first deci-sion concerns the number of segments to the leftof the /aa /  in the word, either none for the left0/349 69/12810 11,many6no,sec2080/2080 415/43914 15Figure 1: Tree modeling the phonetic realization of /aa/ .
All phones are given in ARPABET.
Table 2 givesARPABET-IPA conversions for symbols relevant o this example.
See Table 1 for an explanation of othersymbolscpncp-nplace of articulation of consonant n segments to the rightplace of articulation of consonant n segments to the leftvalues: alveolar; bilabial; labiodental; dental; palatal; velar; pharyngeal;n /a  if is a vowel, or there is no such segmentvpnvp-nplace of articulation of vowel n segments to the rightplace of articulation of vowel n segments to the leftvalues: central-mid-high; back-low; back-mid-low; back-high; front-low;front-mid-low; front-mid-high; front-high; central-mid-low; back-mid-highn /a  if is a consonant, or there is no such segmentIseg number of preceding segments including the segment of interest within the wordrseg number of following segments including the segment of interest within the wordvalues: 1, 2, 3, manystr stress assigned to this vowelvalues: primary, secondary, no (zero) stressn /a  if there is no stress markTable 1: Explanation of symbols in Figure 1.217aa (\]aoaxah ^q-baa 7(\]q+ao ?~Table 2: ARPABET-IPA conversion for symbols relevant for Figure 1.branch, or one or more for the right branch.
As-suming that we have a symbol a representing asingle segment, the symbol # representing a wordboundary, and allowing for the possibility of in-tervening optional stress marks ~ which donot count as segments, these two possibilities canbe represented by the regular expressions for A in(a) of Table 3.
2 At this node there is no deci-sion based on the righthand context, so the right-hand context is free.
We can represent his bysetting p at this node to be E*, where E (con-ventionally) represents the entire alphabet: notethat the alphabet is defined to be an alphabet ofall ?:?
correspondence pairs that were determinedempirically to be possible.The decision at the left daughter of the rootnode concerns whether or not the segment o theright is an alveolar.
Assuming we have definedclasses of segments alv, blab, and so forth (repre-sented as unions of segments) we can represent theregular expression for p as in (b) of Table 3.
Inthis case it is A which is unrestricted, so we canset that at ~*.We can derive the ~ and p expressions forthe rule at leaf node 4 by intersecting togetherthe expressions for these contexts defined for eachbranch traversed on the way to the leaf.
Forleaf node 4, A = #Opt( ' )N  E* = #Opt(') ,  andp = E* n Opt(')(alv) = Opt(')(alv).
3 The ruleinput ?
has already been given as aa.
The output?
is defined as the union of all of the possible ex-pressions - -  at the leaf node in question - -  that aacould become, with their associated weights (neg-ative log probabilities), which we represent here assubscripted floating-point numbers:?
= a00.95 U aal.24 O q+aa2.27 O q-l-ao2.34Uah2.6s U ax2.s4Thus the entire weighted rule can be written as2As far as possible, we use the notation of Kaplanand Kay (1994).3Strictly speaking, since the As and ps at eachbranch may define expressions of different lengths, itis necessary to left-pad each )~ with ~*, and right-padeach p with ~*.
We gloss over this point here in orderto make the regular expressions somewhat simpler tounderstand218follows:aa --~ (aoo.95Uaal.24tdq+aa2.27Uq-bao2.34t.Jah2.6sUax2.s4)/#Opt(') Opt(')(alv)By a similar construction, the rule at node 6, forexample, would be represented as:aa --* (aa0.40 U ao l .n ) /  N(Z*((cmh) U (bl) U (bml) U (bh))) r :Each node thus represents a rule which statesthat a mapping occurs between the input symbol?
and the weighted expression ?
in the conditiondescribed by A p. Now, in cases where ?
findsitself in a context hat is not subsumed by A p,the rule behaves exactly as a two-level surface co-ercion rule (Koskenniemi, 1983): it freely allows?
to correspond to any ?
as specified by the al-phabet of pairs.
These ?:?
correspondences are,however, constrained by other rules derived fromthe tree, as we shall see directly.The interpretation of the full tree is that itrepresents the conjunction of all such mappings:for rules 1, 2 .
.
.n ,  ?
corresponds to ?1 given con-dition ~1__P l  and  ?
corresponds to ?~ givencondition ~2 P2 .
.
.and  ?
corresponds to ?,,given condition ~ p~.
But this conjunction issimply the intersection of the entire set of trans-ducers defined for the leaves of the tree.
Observenow that the ?:?
correspondences that were leftfree by the rule of one leaf node, are constrainedby intersection with the other leaf nodes: since, asnoted above, the tree is a complete description, itfollows that for any leaf node i, and for any contextA p not subsumed by hi Pi, there is someleaf node j such that )~j pj subsumes ~ p.Thus, the transducers compiled for the rules atnodes 4 and 6, are intersected together, along withthe rules for all the other leaf nodes.
Now, asnoted above, and as discussed by Kaplan and Kay(1994) regular relations - -  the algebraic ounter-part of FSTs - -  are not in general closed underintersection; however, the subset of same-lengthregular elations is closed under intersection, sincethey can be thought of as finite-state acceptors ex-(a )  left branch A = #Opt(')p = E*right branch A(b) left branch A = E*p = Opt(O(alv )= (#Opt(')aOpt(')) U (#Opt(')aOpt(')aOpt('))U( #Opt(')aOpt(')aOpt(')( aOpt(') +)= (Opt(')~)+Opt(')right branch A =p = ( Opt(')( blab) U ( Opt(')( labd) U ( Opt(')( den )) U ( Opt(')(pal) )U(Opt(')(vel)) U (Opt(')(pha)) U (Opt(')(n/a))Table 3: Regular-expression interpretation of the decisions involved in going from the root node to leaf node4 in the tree in Figure 1.
Note that, as per convention, superscript '+'  denotes one or more instances of anexpression.pressed over pairs of symbols.
4 This point canbe extended somewhat o include relations thatinvolve bounded eletions or insertions: this is pre-cisely the interpretation necessary for systems oftwo-level rules (Koskenniemi, 1983), where a sin-gle transducer expressing the entire system maybe constructed via intersection of the transduc-ers expressing the individual rules (Kaplan andKay, 1994, pages 367-376).
Indeed, our decisiontree represents neither more nor less than a set ofweighted two-level rules.
Each of the symbols inthe expressions for A and p actually represent (setsof) pairs of symbols: thus alp, for example, rep-resents all lexical alveolars paired with all of theirpossible surface realizations.
And just as each treerepresents a system of weighted two-level rules, soa set of trees - -  e.g., where each tree deals withthe realization of a particular phone - -  representsa system of weighted two-level rules, where eachtwo-level rule is compiled from each of the indi-vidual trees.We can summarize this discussion more for-mally as follows.
We presume a function Compilewhich given a rule returns the WFST computingthat rule.
The WFST for a single leaf L is thusdefined as follows, where CT is the input symbolfor the entire tree, eL is the output expression de-fined at L, t95 represents the path traversed fromthe root node to L, p is an individual branch on4One can thus define intersection for transducersanalogously with intersection for acceptors.
Giventwo machines Gz and G2, with transition functions51 and 52, one can define the transition functionof G, 5, as follows: for an input-output pair (i,o),5((ql, q2), (i, o)) = (q~, q~) if and only if 5z(ql, (i, o)) =q~ and 62(q2, (i, o)) = q~.219that path, and Ap and pp are the expressions forA and p defined at p:RuleL = Compite(?
-  eL/ N aP N ;P)PEPL pEPLThe transducer for an entire tree T is defined as:RuleT ---- D RuleLLETFinally, the transducer for a forest F of trees isjust:RuleF = N RuleTTEF5 Empi r i ca l  Ver i f i ca t ion  o f  theMethod.The algorithm just described has been empiri-cally verified on the Resource Management (RM)continuous peech recognition task (Price et al,1988).
Following somewhat the discussion in(Pereira et al, 1994; Pereira and Riley, 1996),we can represent he speech recognition task asthe problem of finding the best path in the com-position of a grammar (language model) G, thetransitive-closure of a dictionary D mapping be-"tween words and their phonemic representation,a model of phone realization (I), and a weightedlattice representing the acoustic observations A.Thus:BestPath(G o D* o ?
o A) (1)The transducer ?
fo= ~e~ RuleT can be con-structed out of the r of 40 trees, one foreach phoneme, trained on the TIMIT database.The size of the trees range from 1 to 23 leaf nodes,with a totM of 291 leaves for the entire forest.The model was tested on 300 sentences fromthe RM task containing 2560 word tokens, andapproximately 10,500 phonemes.
A version of themodel of recognition given in expression (1), whereq~ is a transducer computed from the trees, wascompared with a version where the trees were useddirectly following a method described in (Ljoljeand Riley, 1992).
The phonetic realizations andtheir weights were identical for both methods, thusverifying the correctness of the compilation algo-rithm described here.The sizes of the compiled transducers can bequite large; in fact they were sufficiently large thatinstead of constructing ?b beforehand, we inter-sected the 40 individual transducers with the lat-tice D* at runtime.
Table 4 gives sizes for theentire set of phone trees: tree sizes are listed interms of number of rules (terminal nodes) and rawsize in bytes; transducer sizes are listed in termsof number of states and arcs.
Note that the entirealphabet comprises 215 symbol pairs.
Also givenin Table 4 are the compilation times for the indi-vidual trees on a Silicon Graphics R4400 machinerunning at 150 MHz with 1024 Mbytes of memory.The times are somewhat slow for the larger trees,but still acceptable for off-line compilation.While the sizes of the resulting transducersseem at first glance to be unfavorable, it is im-portant to bear in mind that size is not the onlyconsideration i deciding upon a particular epre-sentation.
WFSTs possess everal nice propertiesthat are not shared by trees, or handwritten rule-sets for that matter.
In particular, once compiledinto a WFST, a tree can be used in the same wayas a WFST derived from any other source, such asa lexicon or a language model; a compiled WFSTcan be used directly in a speech recognition modelsuch as that of (Pereira and Riley, 1996) or in aspeech synthesis text-analysis model such as thatof (Sproat, 1996).
Use of a tree directly requiresa special-purpose interpreter, which is much lessflexible.It should also be borne in mind that the sizeexplosion evident in Table 4 also characterizesrules that are compiled from hand-built rewriterules (Kaplan and Kay, 1994; Mohri and Sproat,1996).
For example, the text-analysis ruleset for220the Bell Labs German text-to-speech (TTS) sys-tem (see (Sproat, 1996; Mohri and Sproat, 1996))contains ets of rules for the pronunciation of var-ious orthographic symbols.
The ruleset for <a>,for example, contains 25 ordered rewrite rules.Over an alphabet of 194 symbols, this compiles,using the algorithm of (Mohri and Sproat, 1996),into a transducer containing 213,408 arcs and1,927 states.
This is 72% as many arcs and 48%as many states as the transducer fo r /ah / in  Ta-ble 4.
The size explosion is not quite as great here,but the resulting transducer is still large comparedto the original rule file, which only requires 1428bytes of storage.
Again, the advantages of rep-resenting the rules as a transducer outweigh theproblems of size.
56 Future  App l i ca t ionsWe have presented a practical algorithm for con-verting decision trees inferred from data intoweighted finite-state transducers that directly im-plement he models implicit in the trees, and wehave empirically verified that the algorithm is cor-rect.Several interesting areas of application cometo mind.
In addition to speech recognition, wherewe hope to apply the phonetic realization modelsdescribed above to the much larger North Amer-ican Business task (Paul and Baker, 1992), thereare also applications to TTS where, for example,the decision trees for prosodic phrase-boundaryprediction discussed in (Wang and Hirschberg,1992) can be compiled into transducers and useddirectly in the WFST-based model of text analysisused in the multi-lingual version of the Bell Lab-oratories TTS system, described in (Sproat, 1995;Sproat, 1996).7 AcknowledgmentsThe authors wish to thank Fernando Pereira,Mehryar Mohri and two anonymous referees foruseful comments.Re ferencesLeo Breiman, Jerome H. Friedman, Richard A.Olshen, and Charles J.
Stone.
1984.
Clas-5Having said this, we note that obviously one wouldlike to decrease the size of the resulting transducers ifthat is possible.
We are currently investigating waysto avoid precompiling the transducers beforehand, butrather to construct 'on the fly', only those portionsof the transducers that are necessary for a particularintersection.ARPABET phone  nodes size of tree (bytes) # arcs # states time (sec)zh 1 47 215 1 0.3jh 2 146 675 6 0.8aw 2 149 1,720 8 1f 2 119 669 6 0.9ng 2 150 645 3 0.8oy 2 159 1,720 8 1uh 2 126 645 3 0.9p 3 252 6,426 90 4ay 3 228 4,467 38 2m 3 257 2,711 27 1ow 3 236 3,010 14 3sh 3 230 694 8 1v 3 230 685 8 1b 4 354 3,978 33 2ch 4 353 3,010 25 4th 4 373 1,351 11 2dh 5 496 1,290 6 3ey 5 480 11,510 96 27g 6 427 372,339 3,000 21k 6 500 6,013 85 9aa 6 693 18,441 106 15ah 7 855 40,135 273 110y 7 712 9,245 43 12ao 8 1,099 85,439 841 21eh 8 960 16,731 167 13er 8 894 101,765 821 31w 8 680 118,154 1,147 51hh 9 968 17,459 160 101 9 947 320,266 3,152 31uw 9 1,318 44,868 552 28z 9 1,045 1,987 33 5s 10 1,060 175,901 2,032 25ae 11 1,598 582,445 4,152 231iy 11 1,196 695,255 9,625 103d 12 1,414 36,067 389 38n 16 1,899 518,066 3,827 256r 16 1,901 131,903 680 69ih 17 2,748 108,970 669 71t 22 2,990 1,542,612 8,382 628ax 23 4,281 295,954 3,966 77Table 4: Sizes of transducers corresponding to each of the individual phone trees.221sification and Regression Trees.
Wadsworth& Brooks, Pacific Grove CA.William Fisher, Victor Zue, D. Bernstein, andDavid Pallet.
1987.
An acoustic-phoneticdata base.
Journal of the Acoustical Societyof America, 91, December.
Supplement 1.Daniel Gildea and Daniel Jurafsky.
1995.
Au-tomatic induction of finite state transducersfor simple phonological rules.
In 33rd AnnualMeeting of the Association for ComputationalLinguistics, pages 9-15, Morristown, NJ.
As-sociation for ComputationM Linguistics.C.
Douglas Johnson.
1972.
Formal Aspects ofPhonological Description.
Mouton, Mouton,The Hague.Ronald Kaplan and Martin Kay.
1994.
Regularmodels of phonological rule systems.
Compu-tational Linguistics, 20:331-378.Kimmo Koskenniemi.
1983.
Two-Level Mor-phology: a General Computational Modelfor Word-Form Recognition and Production.Ph.D.
thesis, University of Helsinki, Helsinki.Andrej Ljolje and Michael D. Riley.
1992.
Op-timal speech recognition using phone recogni-tion and lexical access.
In Proceedings of IC-SLP, pages 313-316, Banff, Canada, October.David Magerman.
1995.
Statistical decision-treemodels for parsing.
In 33rd Annual Meetingof the Association for Computational Linguis-tics, pages 276-283, Morristown, NJ.
Associ-ation for Computational Linguistics.Mehryar Mohri and Richard Sproat.
1996.
An ef-ficient compiler for weighted rewrite rules.
In34rd Annual Meeting of the Association forComputational Linguistics, Morristown, NJ.Association for Computational Linguistics.Jos~ Oncina, Pedro Garela, and Enrique Vidal.1993.
Learning subsequential transducers forpattern recognition tasks.
IEEE Transactionson Pattern Analysis and Machine Intelligence,15:448-458.Douglas Paul and Janet Baker.
1992.
The designfor the Wall Street Journal-based CSR corpus.In Proceedings of International Conference onSpoken Language Processing, Banff, Alberta.ICSLP.Fernando Pereira nd Michael Riley.
1996.
Speechrecognition by composition of weighted finiteautomata.
CMP-LG archive paper 9603001.Fernando Pereira, Michael Riley, and RichardSproat.
1994.
Weighted rational transduc-tions and their application to human lan-guage processing.
In ARPA Workshop onHuman Language Technology, pages 249-254.222Advanced Research Projects Agency, March8-11.Patty Price, William Fisher, Jared Bernstein, andDavid Pallett.
1988.
The DARPA 1000-wordResource Management Database for contin-uous speech recognition.
In Proceedings ofICASSP88, volume 1, pages 651-654, NewYork.
ICASSP.Michael Riley.
1989.
Some applications of tree-based modelling to speech and language.
InProceedings of the Speech and Natural Lan-guage Workshop, pages 339-352, Cape CodMA, October.
DARPA, Morgan Kaufmann.Michael Riley.
1991.
A statistical model for gener-ating pronunciation networks.
In Proceedingsof the International Conference on Acoustics,Speech, and Signal Processing, pages Sl1.1.-Sll.4.
ICASSP91, October.Richard Sproat.
1995.
A finite-state architecturefor tokenization and grapheme-to-phonemeconversion for multilingual text analysis.
InSusan Armstrong and Evelyne Tzoukermann,editors, Proceedings of the EACL SIGDATWorkshop, pages 65-72, Dublin, Ireland.
As-sociation for Computational Linguistics.Richard Sproat.
1996.
Multilingual text analy-sis for text-to-speech synthesis.
In Proceed-ings of the ECAI-96 Workshop on ExtendedFinite State Models of Language, Budapest,Hungary.
European Conference on ArtificialIntelligence.Michelle Wang and Julia Hirschberg.
1992.
Au-tomatic classification of intonational phraseboundaries.
Computer Speech and Language,6:175-196.
