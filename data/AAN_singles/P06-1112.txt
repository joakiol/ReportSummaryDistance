Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 889?896,Sydney, July 2006. c?2006 Association for Computational LinguisticsExploring Correlation of Dependency Relation Pathsfor Answer ExtractionDan ShenDepartment of Computational LinguisticsSaarland UniversitySaarbruecken, Germanydshen@coli.uni-sb.deDietrich KlakowSpoken Language SystemsSaarland UniversitySaarbruecken, Germanyklakow@lsv.uni-saarland.deAbstractIn this paper, we explore correlation ofdependency relation paths to rank candi-date answers in answer extraction.
Usingthe correlation measure, we compare de-pendency relations of a candidate answerand mapped question phrases in sentencewith the corresponding relations in ques-tion.
Different from previous studies, wepropose an approximate phrase mappingalgorithm and incorporate the mappingscore into the correlation measure.
Thecorrelations are further incorporated intoa Maximum Entropy-based ranking modelwhich estimates path weights from train-ing.
Experimental results show that ourmethod significantly outperforms state-of-the-art syntactic relation-based methodsby up to 20% in MRR.1 IntroductionAnswer Extraction is one of basic modules in opendomain Question Answering (QA).
It is to furtherprocess relevant sentences extracted with Passage /Sentence Retrieval and pinpoint exact answers us-ing more linguistic-motivated analysis.
Since QAturns to find exact answers rather than text snippetsin recent years, answer extraction becomes moreand more crucial.Typically, answer extraction works in the fol-lowing steps:?
Recognize expected answer type of a ques-tion.?
Annotate relevant sentences with varioustypes of named entities.?
Regard the phrases annotated with the ex-pected answer type as candidate answers.?
Rank candidate answers.In the above work flow, answer extraction heav-ily relies on named entity recognition (NER).
Onone hand, NER reduces the number of candidateanswers and eases answer ranking.
On the otherhand, the errors from NER directly degrade an-swer extraction performance.
To our knowledge,most top ranked QA systems in TREC are sup-ported by effective NER modules which may iden-tify and classify more than 20 types of named en-tities (NE), such as abbreviation, music, movie,etc.
However, developing such named entity rec-ognizer is not trivial.
Up to now, we haven?t foundany paper relevant to QA-specific NER develop-ment.
So, it is hard to follow their work.
In this pa-per, we just use a general MUC-based NER, whichmakes our results reproducible.A general MUC-based NER can?t annotate alarge number of NE classes.
In this case, allnoun phrases in sentences are regarded as candi-date answers, which makes candidate answer setsmuch larger than those filtered by a well devel-oped NER.
The larger candidate answer sets resultin the more difficult answer extraction.
Previousmethods working on surface word level, such asdensity-based ranking and pattern matching, maynot perform well.
Deeper linguistic analysis hasto be conducted.
This paper proposes a statisti-cal method which exploring correlation of depen-dency relation paths to rank candidate answers.
Itis motivated by the observation that relations be-tween proper answers and question phrases in can-didate sentences are always similar to the corre-sponding relations in question.
For example, thequestion ?What did Alfred Nobel invent??
and the889candidate sentence ?...
in the will of Swedish in-dustrialist Alfred Nobel, who invented dynamite.
?For each question, firstly, dependency relationpaths are defined and extracted from the questionand each of its candidate sentences.
Secondly,the paths from the question and the candidate sen-tence are paired according to question phrase map-ping score.
Thirdly, correlation between two pathsof each pair is calculated by employing DynamicTime Warping algorithm.
The input of the cal-culation is correlations between dependency re-lations, which are estimated from a set of train-ing path pairs.
Lastly, a Maximum Entropy-basedranking model is proposed to incorporate the pathcorrelations and rank candidate answers.
Further-more, sentence supportive measure are presentedaccording to correlations of relation paths amongquestion phrases.
It is applied to re-rank the can-didate answers extracted from the different candi-date sentences.
Considering phrases may providemore accurate information than individual words,we extract dependency relations on phrase levelinstead of word level.The experiment on TREC questions shows thatour method significantly outperforms a density-based method by 50% in MRR and three state-of-the-art syntactic-based methods by up to 20%in MRR.
Furthermore, we classify questions byjudging whether NER is used.
We investigatehow these methods perform on the two questionsets.
The results indicate that our method achievesbetter performance than the other syntactic-basedmethods on both question sets.
Especially formore difficult questions, for which NER may nothelp, our method improves MRR by up to 31%.The paper is organized as follows.
Section 2discusses related work and clarifies what is new inthis paper.
Section 3 presents relation path corre-lation in detail.
Section 4 and 5 discuss how to in-corporate the correlations for answer ranking andre-ranking.
Section 6 reports experiment and re-sults.2 Related WorkIn recent years?
TREC Evaluation, most topranked QA systems use syntactic information inanswer extraction.
Next, we will briefly discussthe main usages.
(Kaisser and Becker, 2004) match a questioninto one of predefined patterns, such as ?Whendid Jack Welch retire from GE??
to the pattern?When+did+NP+Verb+NPorPP?.
For each ques-tion pattern, there is a set of syntactic structures forpotential answer.
Candidate answers are rankedby matching the syntactic structures.
This methodworked well on TREC questions.
However, itis costing to manually construct question patternsand syntactic structures of the patterns.
(Shen et al, 2005) classify question words intofour classes target word, head word, subject wordand verb.
For each class, syntactic relation pat-terns which contain one question word and oneproper answer are automatically extracted andscored from training sentences.
Then, candidateanswers are ranked by partial matching to the syn-tactic relation patterns using tree kernel.
However,the criterion to classify the question words is notclear in their paper.
Proper answers may have ab-solutely different relations with different subjectwords in sentences.
They don?t consider the cor-responding relations in questions.
(Tanev et al, 2004; Wu et al, 2005) comparesyntactic relations in questions and those in an-swer sentences.
(Tanev et al, 2004) reconstructa basic syntactic template tree for a question, inwhich one of the nodes denotes expected answerposition.
Then, answer candidates for this ques-tion are ranked by matching sentence syntactictree to the question template tree.
Furthermore, thematching is weighted by lexical variations.
(Wu etal., 2005) combine n-gram proximity search andsyntactic relation matching.
For syntactic rela-tion matching, question tree and sentence subtreearound a candidate answer are matched from nodeto node.Although the above systems apply the differentmethods to compare relations in question and an-swer sentences, they follow the same hypothesisthat proper answers are more likely to have samerelations in question and answer sentences.
Forexample, in question ?Who founded the Black Pan-thers organization?
?, where, the question word?who?
has the dependency relations ?subj?
with?found?
and ?subj obj nn?
with ?Black Panthersorganization?, in sentence ?Hilliard introducedBobby Seale, who co-founded the Black PantherParty here ...?, the proper answer ?Bobby Seale?has the same relations with most question phrases.These methods achieve high precision, but poorrecall due to relation variations.
One meaningis often represented as different relation combi-nations.
In the above example, appositive rela-890tion frequently appears in answer sentences, suchas ?Black Panther Party co-founder Bobby Sealeis ordered bound and gagged ...?
and indicatesproper answer Bobby Seale although it is asked indifferent way in the question.
(Cui et al, 2004) propose an approximate de-pendency relation matching method for both pas-sage retrieval and answer extraction.
The simi-larity between two relations is measured by theirco-occurrence rather than exact matching.
Theystate that their method effectively overcomes thelimitation of the previous exact matching meth-ods.
Lastly, they use the sum of similarities ofall path pairs to rank candidate answers, which isbased on the assumption that all paths have equalweights.
However, it might not be true.
For ex-ample, in question ?What book did Rachel Carsonwrite in 1962?
?, the phrase ?Rachel Carson?
lookslike more important than ?1962?
since the formeris question topic and the latter is a constraint forexpected answer.
In addition, lexical variationsare not well considered and a weak relation pathalignment algorithm is used in their work.Based on the previous works, this paper ex-plores correlation of dependency relation paths be-tween questions and candidate sentences.
Dy-namic time warping algorithm is adapted to cal-culate path correlations and approximate phrasemapping is proposed to cope with phrase varia-tions.
Finally, maximum entropy-based rankingmodel is developed to incorporate the correlationsand rank candidate answers.3 Dependency Relation Path CorrelationIn this section, we discuss how the method per-forms in detail.3.1 Dependency Relation Path ExtractionWe parse questions and candidate sentences withMiniPar (Lin, 1994), a fast and robust parser forgrammatical dependency relations.
Then, we ex-tract relation paths from dependency trees.Dependency relation path is defined as a struc-ture P =< N1, R,N2 > where, N1, N2 aretwo phrases and R is a relation sequence R =<r1, ..., ri > in which ri is one of the predefined de-pendency relations.
Totally, there are 42 relationsdefined in MiniPar.
A relation sequence R be-tween two phrases N1, N2 is extracted by travers-ing from the N1 node to the N2 node in a depen-dency tree.Q: What book did Rachel Carson write in 1962?Paths for Answer RankingN1 (EAP)           R                               N2What               det                             bookWhat               det obj subj                 Rachel CarsonWhat               det obj                        writeWhat               det obj mod pcomp-n    1962Paths for Answer Re-rankingbook                obj subj                       Rachel Carsonbook                obj                              writebook                obj mod pcomp-n          1962...S: Rachel Carson ?s 1962 book " Silent Spring " saiddieldrin causes mania.Paths for Answer RankingN1 (CA)              R                              N2Silent Spring      title                           bookSilent Spring      title gen                     Rachel CarsonSilent Spring      title num                    1962Paths for Answer Re-rankingbook                    gen                         Rachel Carsonbook                    num                        1962...Figure 1: Relation Paths for sample question andsentence.
EAP indicates expected answer position;CA indicates candidate answerFor each question, we extract relation pathsamong noun phrases, main verb and questionword.
The question word is further replaced with?EAP?, which indicates the expected answer po-sition.
For each candidate sentence, we firstlyextract relation paths between answer candidatesand mapped question phrases.
These paths willbe used for answer ranking (Section 4).
Secondly,we extract relation paths among mapped questionphrases.
These paths will be used for answer re-ranking (Section 5).
Question phrase mapping willbe discussed in Section 3.4.
Figure 1 shows somerelation paths extracted for an example questionand candidate sentence.Next, the relation paths in a question and eachof its candidate sentences are paired according totheir phrase similarity.
For any two relation pathPi and Pj which are extracted from the ques-tion and the candidate sentence respectively, ifSim(Ni1, Nj1) > 0 and Sim(Ni2, Nj2) > 0,Pi and Pj are paired as < Pi, Pj >.
The ques-tion phrase ?EAP?
is mapped to candidate answerphrase in the sentence.
The similarity between two891Path Pairs for Answer RankingN1 (EAP / CA)     Rq                               Rs               N2Silent Spring      det                              title             bookSilent Spring      det obj subj                  title gen       Rachel CarsonSilent Spring      det obj mod pcomp-n     title num      1962Path Pairs for Answer Re-rankingN1                     Rq                               Rs               N2book                  obj subj                        gen             Rachel Carsonbook                  obj mod pcomp-n           num            1962...Figure 2: Paired Relation Pathphrases will be discussed in Section 3.4.
Figure 2further shows the paired relation paths which arepresented in Figure 1.3.2 Dependency Relation Path CorrelationComparing a proper answer and other wrong can-didate answers in each sentence, we assume thatrelation paths between the proper answer andquestion phrases in the sentence are more corre-lated to the corresponding paths in question.
So,for each path pair < P1, P2 >, we measure thecorrelation between its two paths P1 and P2.We derive the correlations between paths byadapting dynamic time warping (DTW) algorithm(Rabiner et al, 1978).
DTW is to find an optimalalignment between two sequences which maxi-mizes the accumulated correlation between twosequences.
A sketch of the adapted algorithm isas follows.Let R1 =< r11, ..., r1n >, (n = 1, ..., N)and R2 =< r21, ..., r2m >, (m = 1, ...,M) de-note two relation sequences.
R1 and R2 consistof N and M relations respectively.
R1(n) =r1n and R2(m) = r2m.
Cor(r1, r2) denotesthe correlation between two individual relationsr1, r2, which is estimated by a statistical modelduring training (Section 3.3).
Given the corre-lations Cor(r1n, r2m) for each pair of relations(r1n, r2m) within R1 and R2, the goal of DTW isto find a path, m = map(n), which map n onto thecorresponding m such that the accumulated corre-lation Cor?
along the path is maximized.Cor?
= maxmap(n){ N?n=1Cor(R1(n), R2(map(n))}A dynamic programming method is used to de-termine the optimum path map(n).
The accumu-lated correlation CorA to any grid point (n,m)can be recursively calculated asCorA(n,m) = Cor(r1n, r2m) + maxq?m CorA(n?
1, q)Cor?
= CorA(N,M)The overall correlation measure has to be nor-malized as longer sequences normally give highercorrelation value.
So, the correlation between twosequences R1 and R2 is calculated asCor(R1, R2) = Cor?/max(N,M)Finally, we define the correlation between tworelation paths P1 and P2 asCor(P1, P2) = Cor(R1, R2)?
Sim(N11, N21)?
Sim(N12, N22)Where, Sim(N11, N21) and Sim(N12, N22)are the phrase mapping score when pairingtwo paths, which will be described in Section3.4.
If two phrases are absolutely differentCor(N11, N21) = 0 or Cor(N12, N22) = 0, thepaths may not be paired since Cor(P1, P2) = 0.3.3 Relation Correlation EstimationIn the above section, we have described how tomeasure path correlations.
The measure requiresrelation correlations Cor(r1, r2) as inputs.
Weapply a statistical method to estimate the relationcorrelations from a set of training path pairs.
Thetraining data collecting will be described in Sec-tion 6.1.For each question and its answer sentences intraining data, we extract relation paths between?EAP?
and other phrases in the question andpaths between proper answer and mapped ques-tion phrases in the sentences.
After pairing thequestion paths and the corresponding sentencepaths, correlation of two relations is measured bytheir bipartite co-occurrence in all training pathpairs.
Mutual information-based measure (Cui etal., 2004) is employed to calculate the relation cor-relations.Cor(rQi , rSj ) = log???
?
(rQi , rSj )fQ(rQi )?
fS(rSj )where, rQi and rSj are two relations in questionpaths and sentence paths respectively.
fQ(rQi ) andfS(rSj ) are the numbers of occurrences of rQi inquestion paths and rSj in sentence paths respec-tively.
?
(rQi , rSj ) is 1 when rQi and rSj co-occurin a path pair, and 0 otherwise.
?
is a factor todiscount the co-occurrence value for long paths.
Itis set to the inverse proportion of the sum of pathlengths of the path pair.8923.4 Approximate Question Phrase MappingBasic noun phrases (BNP) and verbs in questionsare mapped to their candidate sentences.
A BNPis defined as the smallest noun phrase in whichthere are no noun phrases embedded.
To addresslexical and format variations between phrases, wepropose an approximate phrase mapping strategy.A BNP is separated into a set of headsH = {h1, ..., hi} and a set of modifiers M ={m1, ...mj}.
Some heuristic rules are applied tojudge heads and modifiers: 1.
If BNP is a namedentity, all words are heads.
2.
The last word ofBNP is head.
3.
Rest words are modifiers.The similarity between two BNPsSim(BNPq, BNPs) is defined as:Sim(BNPq, BNPs) = ?Sim(Hq, Hs)+ (1?
?
)Sim(Mq,Ms)Sim(Hq, Hs) =?hi?Hq?hj?HsSim(hi,hj)|Hq?Hs|Sim(Mq,Ms) =?mi?Mq?mj?MsSim(mi,mj)|Mq?Ms|Furthermore, the similarity between two headsSim(hi, hj) are defined as:?
Sim = 1, if hi = hj after stemming;?
Sim = 1, if hi = hj after format alternation;?
Sim = SemSim(hi, hj)These items consider morphological, formatand semantic variations respectively.
1.
The mor-phological variations match words after stemming,such as ?Rhodes scholars?
and ?Rhodes scholar-ships?.
2.
The format alternations cope withspecial characters, such as ?-?
for ?Ice-T?
and?Ice T?, ?&?
for ?Abercrombie and Fitch?
and?Abercrombie & Fitch?.
3.
The semantic simi-larity SemSim(hi, hj) is measured using Word-Net and eXtended WordNet.
We use the samesemantic path finding algorithm, relation weightsand semantic similarity measure as (Moldovan andNovischi, 2002).
For efficiency, only hypernym,hyponym and entailment relations are consideredand search depth is set to 2 in our experiments.Particularly, the semantic variations are not con-sidered for NE heads and modifiers.
Modifier sim-ilarity Sim(mi,mj) only consider the morpho-logical and format variations.
Moreover, verb sim-ilarity measure Sim(v1, v2) is the same as headsimilarity measure Sim(hi, hj).4 Candidate Answer RankingAccording to path correlations of candidate an-swers, a Maximum Entropy (ME)-based model isapplied to rank candidate answers.
Unlike (Cui etal., 2004), who rank candidate answers with thesum of the path correlations, ME model may es-timate the optimal weights of the paths based ona training data set.
(Berger et al, 1996) gave agood description of ME model.
The model weuse is similar to (Shen et al, 2005; Ravichandranet al, 2003), which regard answer extraction as aranking problem instead of a classification prob-lem.
We apply Generalized Iterative Scaling formodel parameter estimation and Gaussian Priorfor smoothing.If expected answer type is unknown duringquestion processing or corresponding type ofnamed entities isn?t recognized in candidate sen-tences, we regard all basic noun phrases as can-didate answers.
Since a MUC-based NER losesmany types of named entities, we have to handlelarger candidate answer sets.
Orthographic fea-tures, similar to (Shen et al, 2005), are extracted tocapture word format information of candidate an-swers, such as capitalizations, digits and lengths,etc.
We expect they may help to judge what properanswers look like since most NER systems workon these features.Next, we will discuss how to incorporate pathcorrelations.
Two facts are considered to affectpath weights: question phrase type and pathlength.
For each question, we divide questionphrases into four types: target, topic, constraintand verb.
Target is a kind of word which indicatesthe expected answer type of the question, such as?party?
in ?What party led Australia from 1983 to1996??.
Topic is the event/person that the ques-tion talks about, such as ?Australia?.
Intuitively, itis the most important phrase of the question.
Con-straint are the other phrases of the question ex-cept topic, such as ?1983?
and ?1996?.
Verb isthe main verb of the question, such as ?lead?.
Fur-thermore, since shorter path indicates closer rela-tion between two phrases, we discount path corre-lation in long question path by dividing the corre-lation by the length of the question path.
Lastly,we sum the discounted path correlations for eachtype of question phrases and fire it as a feature,such as ?Target Cor=c, where c is the correla-tion value for question target.
ME-based rank-ing model incorporate the orthographic and path893correlation features to rank candidate answers foreach of candidate sentences.5 Candidate Answer Re-rankingAfter ranking candidate answers, we select thehighest ranked one from each candidate sentence.In this section, we are to re-rank them accordingto sentence supportive degree.
We assume that acandidate sentence supports an answer if relationsbetween mapped question phrases in the candidatesentence are similar to the corresponding ones inquestion.
Relation paths between any two ques-tion phrases are extracted and paired.
Then, corre-lation of each pair is calculated.
Re-rank formulais defined as follows:Score(answer) = ??
?iCor(Pi1, Pi2)where, ?
is answer ranking score.
It is the nor-malized prediction value of the ME-based rankingmodel described in Section 4.
?iCor(Pi1, Pi2) isthe sum of correlations of all path pairs.
Finally,the answer with the highest score is returned.6 ExperimentsIn this section, we set up experiments on TRECfactoid questions and report evaluation results.6.1 Experiment SetupThe goal of answer extraction is to identify ex-act answers from given candidate sentence col-lections for questions.
The candidate sentencesare regarded as the most relevant sentences to thequestions and retrieved by IR techniques.
Quali-ties of the candidate sentences have a strong im-pact on answer extraction.
It is meaningless toevaluate the questions of which none candidatesentences contain proper answer in answer extrac-tion experiment.
To our knowledge, most of cur-rent QA systems lose about half of questions insentence retrieval stage.
To make more questionsevaluated in our experiments, for each of ques-tions, we automatically build a candidate sentenceset from TREC judgements rather than use sen-tence retrieval output.We use TREC99-03 questions for training andTREC04 questions for testing.
As to build trainingdata, we retrieve all of the sentences which con-tain proper answers from relevant documents ac-cording to TREC judgements and answer patterns.Then, We manually check the sentences and re-move those in which answers cannot be supported.As to build candidate sentence sets for testing, weretrieve all of the sentences from relevant docu-ments in judgements and keep those which containat least one question key word.
Therefore, eachquestion has at least one proper candidate sentencewhich contains proper answer in its candidate sen-tence set.There are 230 factoid questions (27 NIL ques-tions) in TREC04.
NIL questions are excludedfrom our test set because TREC doesn?t supplyrelevant documents and answer patterns for them.Therefore, we will evaluate 203 TREC04 ques-tions.
Five answer extraction methods are evalu-ated for comparison:?
Density: Density-based method is used asbaseline, in which we choose candidate an-swer with the shortest surface distance toquestion phrases.?
SynPattern: Syntactic relation patterns(Shen et al, 2005) are automatically ex-tracted from training set and are partiallymatched using tree kernel.?
StrictMatch: Strict relation matching fol-lows the assumption in (Tanev et al, 2004;Wu et al, 2005).
We implement it by adapt-ing relation correlation score.
In stead oflearning relation correlations during training,we predefine them as: Cor(r1, r2) = 1 ifr1 = r2; 0, otherwise.?
ApprMatch: Approximate relation match-ing (Cui et al, 2004) aligns two relation pathsusing fuzzy matching and ranks candidatesaccording to the sum of all path similarities.?
CorME: It is the method proposed in this pa-per.
Different from ApprMatch, ME-basedranking model is implemented to incorpo-rate path correlations which assigns differentweights for different paths respectively.
Fur-thermore, phrase mapping score is incorpo-rated into the path correlation measure.These methods are briefly described in Section2.
Performance is evaluated with Mean ReciprocalRank (MRR).
Furthermore, we list percentages ofquestions correctly answered in terms of top 5 an-swers and top 1 answer returned respectively.
Noanswer validations are used to adjust answers.894Table 1: Overall performanceDensity SynPattern StrictMatch ApprMatch CorMEMRR 0.45 0.56 0.57 0.60 0.67Top1 0.36 0.53 0.49 0.53 0.62Top5 0.56 0.60 0.67 0.70 0.746.2 ResultsTable 1 shows the overall performance of the fivemethods.
The main observations from the tableare as follows:1.
The methods SynPattern, StrictMatch, Ap-prMatch and CorME significantly improveMRR by 25.0%, 26.8%, 34.5% and 50.1%over the baseline method Density.
The im-provements may benefit from the various ex-plorations of syntactic relations.2.
The performance of SynPattern (0.56MRR)and StrictMatch (0.57MRR) are close.
Syn-Pattern matches relation sequences of can-didate answers with the predefined relationsequences extracted from a training dataset, while StrictMatch matches relation se-quences of candidate answers with the cor-responding relation sequences in questions.But, both of them are based on the assump-tion that the more number of same rela-tions between two sequences, the more sim-ilar the sequences are.
Furthermore, sincemost TREC04 questions only have one or twophrases and many questions have similar ex-pressions, SynPattern and StrictMatch don?tmake essential difference.3.
ApprMatch and CorME outperform SynPat-tern and StrictMatch by about 6.1% and18.4% improvement in MRR.
Strict matchingoften fails due to various relation representa-tions in syntactic trees.
However, such vari-ations of syntactic relations may be capturedby ApprMatch and CorME using a MI-basedstatistical method.4.
CorME achieves the better performance by11.6% than ApprMatch.
The improvementmay benefit from two aspects: 1) ApprMatchassigns equal weights to the paths of a can-didate answer and question phrases, whileCorME estimate the weights according tophrase type and path length.
After training aME model, the weights are assigned, such as5.72 for topic path ; 3.44 for constraints pathand 1.76 for target path.
2) CorME incorpo-rates approximate phrase mapping scores intopath correlation measure.We further divide the questions into two classesaccording to whether NER is used in answer ex-traction.
If the expected answer type of a ques-tion is unknown, such as ?How did James Deandie??
or the type cannot be annotated by NER,such as ?What ethnic group/race are Crip mem-bers?
?, we put the question in Qw/oNE set, oth-erwise, we put it in QwNE.
For the questions inQw/oNE, we extract all basic noun phrases andverb phrases as candidate answers.
Then, answerextraction module has to work on the larger can-didate sets.
Using a MUC-based NER, the rec-ognized types include person, location, organiza-tion, date, time and money.
In TREC04 questions,123 questions are put in QwNE and 80 questionsin Qw/oNE.Table 2: Performance on two question sets QwNEand Qw/oNEQwNE Qw/oNEDensity 0.66 0.11SynPattern 0.71 0.36StrictMatch 0.70 0.36ApprMatch 0.72 0.42CorME 0.79 0.47We evaluate the performance on QwNE andQw/oNE respectively, as shown in Table 2.The density-based method Density (0.11MRR)loses many questions in Qw/oNE, which indi-cates that using only surface word informationis not sufficient for large candidate answer sets.On the contrary, SynPattern(0.36MRR), Strict-Pattern(0.36MRR), ApprMatch(0.42MRR) andCorME (0.47MRR) which capture syntactic infor-mation, perform much better than Density.
Ourmethod CorME outperforms the other syntactic-based methods on both QwNE and Qw/oNE.
Es-895pecially for more difficult questions Qw/oNE, theimprovements (up to 31% in MRR) are more ob-vious.
It indicates that our method can be used tofurther enhance state-of-the-art QA systems evenif they have a good NER.In addition, we evaluate component contribu-tions of our method based on the main idea ofrelation path correlation.
Three components aretested: 1.
Appr.
Mapping (Section 3.4).
We re-place approximate question phrase mapping withexact phrase mapping and withdraw the phrasemapping scores from path correlation measure.
2.Answer Ranking (Section 4).
Instead of usingME model, we sum all of the path correlations torank candidate answers, which is similar to (Cuiet al, 2004).
3.
Answer Re-ranking (Section5).
We disable this component and select top 5answers according to answer ranking scores.Table 3: Component ContributionsMRROverall 0.67- Appr.
Mapping 0.63- Answer Ranking 0.62- Answer Re-ranking 0.66The contribution of each component is evalu-ated with the overall performance degradation af-ter it is removed or replaced.
Some findings areconcluded from Table 3.
Performances degradewhen replacing approximate phrase mapping orME-based answer ranking, which indicates thatboth of them have positive effects on the systems.This may be also used to explain why CorME out-performs ApprMatch in Table 1.
However, remov-ing answer re-ranking doesn?t affect much.
Sinceshort questions, such as ?What does AARP standfor?
?, frequently occur in TREC04, exploring thephrase relations for such questions isn?t helpful.7 ConclusionIn this paper, we propose a relation pathcorrelation-based method to rank candidate an-swers in answer extraction.
We extract and pairrelation paths from questions and candidate sen-tences.
Next, we measure the relation path cor-relation in each pair based on approximate phrasemapping score and relation sequence alignment,which is calculated by DTW algorithm.
Lastly,a ME-based ranking model is proposed to incor-porate the path correlations and rank candidateanswers.
The experiment on TREC questionsshows that our method significantly outperformsa density-based method by 50% in MRR and threestate-of-the-art syntactic-based methods by up to20% in MRR.
Furthermore, the method is espe-cially effective for difficult questions, for whichNER may not help.
Therefore, it may be used tofurther enhance state-of-the-art QA systems evenif they have a good NER.
In the future, we are tofurther evaluate the method based on the overallperformance of a QA system and adapt it to sen-tence retrieval task.ReferencesAdam L. Berger, Stephen A. Della Pietra, and Vin-cent J. Della Pietra.
1996.
A maximum entropyapproach to natural language processing.
Compu-tational Linguisitics, 22:39?71.Hang Cui, Keya Li, Renxu Sun, Tat-Seng Chua, andMin-Yen Kan. 2004.
National university of singa-pore at the trec-13 question answering.
In Proceed-ings of TREC2004, NIST.M.
Kaisser and T. Becker.
2004.
Question answeringby searching large corpora with linguistic methods.In Proceedings of TREC2004, NIST.Dekang Lin.
1994.
Principar?an efficient, broad-coverage, principle-based parser.
In Proceedings ofCOLING1994, pages 42?488.Dan Moldovan and Adrian Novischi.
2002.
Lexicalchains for question answering.
In Proceedings ofCOLING2002.L.
R. Rabiner, A. E. Rosenberg, and S. E. Levinson.1978.
Considerations in dynamic time warping al-gorithms for discrete word recognition.
In Proceed-ings of IEEE Transactions on acoustics, speech andsignal processing.Deepak Ravichandran, Eduard Hovy, and Franz JosefOch.
2003.
Statistical qa - classifier vs. re-ranker:What?s the difference?
In Proceedings of ACL2003workshop on Multilingual Summarization and Ques-tion Answering.Dan Shen, Geert-Jan M. Kruijff, and Dietrich Klakow.2005.
Exploring syntactic relation patterns for ques-tion answering.
In Proceedings of IJCNLP2005.H.
Tanev, M. Kouylekov, and B. Magnini.
2004.
Com-bining linguisitic processing and web mining forquestion answering: Itc-irst at trec-2004.
In Pro-ceedings of TREC2004, NIST.M.
Wu, M. Y. Duan, S. Shaikh, S. Small, and T. Strza-lkowski.
2005.
University at albany?s ilqua in trec2005.
In Proceedings of TREC2005, NIST.896
