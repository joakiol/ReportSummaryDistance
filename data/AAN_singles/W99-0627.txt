Taking the load off the conference chairs: towardsa digital paper-routing assistantDavid Yarowsky  and  Radu FlorianComputer  Science Depar tment  and Center  for Language and Speech ProcessingJohns  Hopkins Univers i tyBa l t imore,  Mary land  21218{yarowsky, rf lor ian}@cs.
jhu.eduAbst ractThis paper describes and extensively evaluates a sys-tem for the automatic routing of submitted papers toreviewers and area committees, without he need forany human annotation from the reviewers or the pro-gram chair.
Routing is based on a profile of previouswritings obtainable on-line for the reviewer pool, agenerally stable and reusable resource that requiresno manual adaptation for new submission streams.The paper explores a wide set of variations and ex-tensions on the core model, and achieves ystem ac-curacy approaching that of several human judges onthe same task.1 In t roduct ion  and  Prob lemStatementRouting submitted papers, abstracts or grant pro-posals to qualified reviewers i a central task of theacademic enterprise, and a remarkably difficult one.Typically it is conducted under significant time pres-sure in a conference r viewing cycle.
As the numberof submissions and size of the reviewer pool grows, itbecomes increasingly difficult for a conference chairto be familiar with the different expertise of all mem-bers of the program committee.
It is also difficultfor one person to master the subtleties of fine sub-ject area distinctions as topic diversity in a confer-ence becomes large.
For these reasons, conferencessuch as ACL (the Association for ComputationalLinguistics) often use a hierarchical program com-mittee structure, where submitted papers are firstrouted to area committees, and then more special-ized area chairs have the task of assigning papers toindividual reviewers in the committee.
However, ina diverse and multidisciplinary field such as natu-ral language processing, it is often difficult to defineclear cut committee descriptions and the programchair still must be cognizant of the detailed exper-tise of the area committee members in order to routeatypical or multidisciplinary papers to committeeswith the most appropriate pool of reviewers.
Thelow inter-rater consistency results shown in Table12 indicate that humans find even area committeerouting to be a difficult task.The following paper focuses on a range of auto-mated solutions to this task of routing papers totheir most appropriate area committee.
It presentsextensive mpirical investigation and evaluation of awide range of issues related to this task.Previous published research into the problem ofautomatic routing of conference paper submissions isurprisingly limited.
Approaches to this task can beessentially broken down into four major strategies:The first strategy is keyword based.
Authors arerequired to specify a list of topic/subtopic areas fortheir papers (often from a prespecified term list),and reviewers then complete a survey of their rela-tive level of expertise on this list of topics/subtopics.This approach is followed by AAAI conference re-viewing.
It suffers from the problem that authorsoften have a difficult time selecting keywords to ad-equately describe their work.
It works best in con-ferences that are very broad, and is least effective inmore focused workshops where routing distinctionsin subject area and paradigm are more subtle.The second strategy is to build a statistical profileof reviewers' expertise by eliciting relevance judg-ments on a set of abstract data.
AAAI also re-quires its reviewers to rank (bid on) submitted ab-stracts, and there is currently unpublished work ex-ploring the application of supervised routing to theranked reviewer bids on AAAI submitted abstracts(Hirsh, personal communication).
In groundbreak-ing work, Dumais and Nielsen (1992) developed asystem for the routing of Hypertext'91 abstractsusing latent semantic indexing (Deerwester et al,1990), trained from available text sources includinga small set of reviewer-submitted abstracts, on-linebooks and ACM articles as a source for the term-by-document matrix used in their singular value de-compositions.
Reviewers manually ranked their in-terest in all submitted abstracts, and best perfor-mance was achieved when reviewers were assignedtwice their target number of abstracts and asked tochoose their preferred half.One problem with the modeling of reviewer rank-ings/bids is that these may be based more onwhat the reviewer finds interesting rather than what220Paper 185word sense disambiguation with an ensemble of naive Bayesian classifierscommittee Rough committee character i za t ioncom4 statistical NLP (focus on sense tagging)corn3 statistical NLP (focus on MT, statistical parsing)corn6 j generation and systemsscore0.3770.3650.2580.2420.2280.224com5corn2lexicons, some non-statistical sense taggingsyntax/parsing (mostly non-statistical)coml discourse/dialogTable 1: Committee routing system outputhe/she is most qualified to review.
Even withinstructions, there may be a natural human ten-dency to bid higher on exciting/interesting abstractsin a more distant area and possibly bid lower onweak/uninteresting papers in the reviewer's core ex-pertise.
In addition, AAAI reviewers also report thisas a long and tedious abstract ranking process, thatshifts the burden of labor for paper routing onto thereviewers rather than the program chairs.A third option is to learn the reviewer/committeeprofiles by having the program chair assign a por-tion of the submissions to reviewers and/or commit-tees and then attempt o model these assignments inorder to compute the assignment of remaining sub-missions to the reviewer pool based on these models.We evaluate such a strategy below.A fourth option, the focus of this paper, is tocreate a statistical profile of reviewers' expertiseby modeling the collection of their previously pub-lished papers and other writings or statements ofresearch interest extracted automatically from whatis available on the web.
One advantage of this ap-proach is that frees the reviewer from a laboriousabstract ranking/bidding process.
Another is thatprofiles based on a large collection of the reviewer'sown writings is perhaps a better model of areas ofdemonstrated expertise rather than simply the pa-pers a reviewer finds most "interesting".
And, fi-nally, such profiles based on collected writings tendto be relatively stable and reusable from conference-to-conference (reviewers often serve on many pro-gram committees) and may optionally be updatedwhen a reviewer's representative publications growof change significantly.
The  effort in creating suchpublications-based profiles need not be repeated asthe pool of submitted abstracts change.
This ap-proach is remarkably cost effective and the empiri-cal results below indicate that it can achieve perfor-mance competitive with human paper routers.2 Task  Descr ip t ionThe primary task investigated in this paper is therouting of full-length submitted conference papers toScore0.5400.4260.4200.4140.3690.3680.3510.3440.3370.327Reviewer Committeeng corn4bruce com4roth com4golding com4wiebe comlresnik com3daelemans com4shin l com3lee corn4hang corn5Table 2: Reviewer outing system output (for paper185, above)one of 6 area committees for ACL'99, with the com-mittees ranked in order of appropriateness in Table 1(actual output of the system on sample paper #185).The 6 committees are best defined by their members(listed with their committee numbers in Figures 2),but they are very roughly characterized in Table 1.A secondary task is to provide a proposed orderedlist of appropriate reviewers, as shown in Table 2.Note that this list can be filtered to include just thefirst choice committee, or can include the most ap-propriate reviewers independent of committee struc-ture.2.1 The  DataThe evaluation data used in these experiments con-sisted of full-length articles submitted to the gen-eral session of ACL'99.
Thematic session submis-sions were ignored because the reviewing commit-tee was preselected by the author in these cases.The ACL'99 call for papers included a statement re-questing voluntary submission of electronic versionsof their papers for a paper routing experiment.
Ofthe 180 general session authors, 51% (92) partici-pated in the study through electronic submission.As noted above, electronic opies of representa-tive papers were also solicited from members of the221general session area program committees.
Partici-pants had the option of including a numeric rank-ing (1 to 10) indicating the representativeness ofthe papers with respect o their areas of expertise,but few chose to do so.
In the numerous caseswhere none or insufficient numbers of papers werereceived from reviewers, their self-selected sampleof previous publications were augmented by largenumbers of downloaded reviewer papers from cmp-lg (xxx.lanl.gov/cmp-lg), their own home pages andthe www.cora.jprc.com 1 archive.Papers were received and processed from 5 accept-able formats: latex, postscript, plain text, portabledocument format (pdf) and html, all of which wereconverted to a marked-up lain text normal for-mat.
Distinct regions of the papers (title, abstract,main body, bibliography) were identified and ex-tracted, when possible, in support of differential re-gion weighting.2.2 Evaluation MethodologyThe primary "gold standard" for evaluation consistedof the committee numbers actually assigned to eachpaper by the ACL'99 program chair performing thecommittee routing.
These judgments were obtainedprior to his seeing the results of the automatic rout-ing experiments.
Because the program chair con-sidered other factors including potential conflicts ofinterest in assigning papers, this is not a perfect an-notation of the most appropriate committee basedstrictly on mass of reviewer expertise.
Three otherjudges (2 NLP faculty members and one 3rd yearNLP grad student) also routed those papers volun-tarily submitted from the authors for the routingexperiments, with their names, addresses and insti-tutions stripped.
Greatest committee appropriate-ness based on topic and reviewer expertise was thesole criterion for these paper assignments.
A sec-ond evaluation gold standard was obtained from theweighted consensus of the 4 reviewers (described inSection 7 below).The 92 submitted papers were divided into twoequal halves: a primary test set on which all ma-jor results were evaluated, and a secondary devtestset, via which some global parameters were esti-mated and the one instance of supervised trainingtook place.Several evaluation measures were used to reflectsystem performance.
The first is exact match clas-sification accuracy (the percentage of the papers onwhich the gold standard and system agreed exactlyon the committee assignment).
Because the systemreturns a full preferred rank order of the 6 commit-tees for all papers, a second natural performancemeasure is the average position of the truth (goldIThis is a web search engine specialized in searching Com-puter Science related papers (see (McCallum et al, 1999)).standard committee selection) in this rank list.
Thismeasure gives an assessment of how many commit-tees the human judge would have to consider, onaverage, before it found the correct classification;smaller is better.
Because in many cases there aretwo equally viable committee contenders, a thirdmeasure One-of-best-2 indicates the percentage ofcases where the gold standard classification is in thetop two choices ranked by the system.
In manycases, the whole histogram is given, indicating theposition of the gold standard classification i  the sys-tem's committee ranking.3 Routing MethodologiesThere are numerous methods described in the infor-mation retrieval literature for article routing.
As-suming that there are n classes and a set of m arti-cles, the article routing task attributes each of them articles to one of the n classes.
It is clear thatour task fits well in this paradigm; each paper hasto go to one committee.
The two major approachestested in this model are the standard Salton-stylevector space model (Salton and McGill, 1983) andthe Naive Bayes classifier (Mosteller and Wallace,1964).
2 These and several permutations and exten-sions are detailed and evaluated below.3.1 Vector Rout ing ModelUnless we specify otherwise, we shall assume thatthe vocabulary is selected by removing a set of com-mon (stop) words from the text.
Both the submit-ted papers and the reviewer papers are representedin the space \[0;oo) Irl, as vectors Dr: D~ i = cij .wj,where cij is the count (the number of occurrences)of the j th word in document D~ and wj is an "im-portance" weight associated with the jth word.
Onetypical weighting function is IDF (Inverse Documentw1 log N Frequency): = ,(~ + d~) , .
where N is thetotal number of documents and docfj is the docu-ment frequency of the j th word (the number of doc-uments the word appears in).
One can measure thesimilarity between 2 documents by using the cosinesimilarity between their vector epresentations:cosine_similarity (Di, Dj) =jkthe dot product of the normalized 3 vectors (see(Salton and McGill, 1983)).
This measure of sim-ilarity yields values close to 1 for similar vectors andclose to 0 for dissimilar ones.2Routing using these and other models is a centraltask in information retrieval, discussed in depth in (Hull,1994),(Lewis and Gale, 1994), (Larkey and Croft, 1996) and(Voorhees and Harman, 1998) and many other articles.311.112 being the Euclidean orm.222Keywords,Baseline Keywords Only Title,Abstract OnlyAccuracy: 19.6% 36.9% 52.2%Average position: 3.02 2.48 2.28One-of-best-2: 50.0% 60.9% 65.2%Histogram:i:JH~lnm V~o~aTable 3: Baseline performance measuresThe main algorithm proceeds as follows:1.
For the ith reviewer (i = 1,...), compute a cen-troid Ri - a vector presumably associated withthe main research interests of the reviewer:Rij = E r (P) .cj (P) .wj (1)PE~iwhere Pl is the pool of papers for ith reviewer,r (P) is the weight/relevance of paper P andcj (P) is the word count of jth word in paper P(a given word might weight differently in differ-ent regions - see region weighting below).2.
For each committee, compute its centroid as thesum of the composing reviewers' centroids:= Z (2)Ri EC~where Ck is the pool of reviewers for committeek.3.
For each paper, rank all the committees basedon the cosine similarity between the paper's vec-tor and the committee centroids - the one thatranks highest is chosen as the classification ofthe paper:classification (Pt) = argmax (cosine_similarity (Pl, Ck))k=l .
.
.6  (3)Table 3 gives results for several basic baseline mod-els.
Section 2.2 describes these measures.Clearly different regions of a paper have differentimportance in determining its semantic ontext.
Weautomatically separate the text into title, abstract,keywords, body and bibliography regions and inves-tigate different weighting parameters for these re-gions.The results for full text and region weighting aregiven in Table 5.
Consensus evaluation is describedRouter  cora l  corn2 corn3 corn4 corn5 corn61corn1 1 0 0 0 0 2com2 1 9 1 0 0 3com3 0 0 6 2 0 1corn4 0 0 4 3 0 2com5 0 0 1 0 5 0coms 0 1 1 0 1 2Table 4: Confusion matrix for the full text, regionweighting casein Section 7.
A confusion matrix 4 showing regionweighting results is given in Table 4.
Note that theprimary confusion is between the difficult to distin-gnish committees 3 and 4.The remainder of this section describes the modifi-cations made to this model, the results we obtained,conclusions and explanations of the results.3.1.1 Weighting Paper Sources DifferentlyAs noted before, the reviewers' papers were obtainedfrom different sources, with potentially different rel-ative indicativeness of a reviewer's expertise.
A vari-ety of relative weighting parameters for these sourceswere explored on the devtest.
None yielded a signif-icant improvement over the equally weighted model.3.1.2 Term Selection and WeightingExperiments were conducted to test the efficacy oftwo variants of IDF (based on the concepts of I doc-ument per reviewer and I document per committee),entropy-based term weighting, use of stemming, and4A cell (i,j) in the confusion matrix shows how many timescommittee i was chosen where committee j was the true as-signment.
It is an indication of the nature of the misclassifi-cation observed, not merely its absolute number.223AccuracyAverage positionOne-of-best-2Histogram:Full Text,Equal Weighting47.8%2.0969.6%zs~Full Text,Region Weighting56.5%1.9671.7%k+o+Full Text, (RW)ConsensusEvaluation67.4%1.7278.3%f++Table 5: Performance on Full Text RoutingETitle Abstract \[ B~dy 1 I T?pi~0 I ~3o(  3o \[Bibliography AreaTable 6: Word Weights Based on Regionvocabulary selection based on statistically significantcross-class frequency variation.
No variation outper-formed the region weighting model shown in Table5.3.2 Naive Bayes ClassifierThe naive Bayes model makes an independence as-sumption relative to the words in a text.
It choosesthe committee Cj that maximizes the probabilityP (Cj \[P0; formallyP(cj).P(P, iC~) argmaxj P (Cj IPi) = argmax/ P(PO~-  argmax/P(Cj) \[Iwkepl P(wklCJ)= argmaxj (Iog(P(Cj))+ ~w.eP+ l?g(P(wklCJ)))and, furthermore, if one assumes equal a prioriprobability on the committees (P (Cj) = ct), thenone looks forargmax (E  log(P(w~\[Cj)))J \wh6Piwhere the words wk are the target words in thearticle Pi (usually all the non-stopwords).One of the issues that need to be addressed whenconsidering naive Bayes approaches i smoothing.One cannot afford to have null probabilities, asthey would just nullify the results.
The smoothingmethod used in this approach is the simple additivesmoothing method, that adjusts the maximum like-lihood estimates as follows:6 + c (Wk,P (wklC#) = 6.
IVl + g (c#)Accuracy:Average position:One-of-best-2:Histogram:52.2%2.2067.4%J:Table 7: Results for the Naive Bayes Classifierwhere N (Cj) = ~ C (wk, Cj) and 1; is the wholekvocabulary.
This is a very simple strategy, but webelieve that it works relatively well for unigrams.Results are shown in Table 7; it underperforms theregion weighted vector-based model with similar pa-rameters.To check whether unseen words are a problem inour case,we varied the parameter 5.
Since the resultswere almost he same for 5 values varying from 0.01to 1, we conclude that more sophisticated smoothingmethods (e.g.
Good-Turing, Knesser-Ney) wouldnot have made a difference, ither.3.3 VotingAs an alternative approach to the top-down hier-archical routing strategy, we investigated the initialdirect assignment of papers to reviewers, and thenallowed the top k reviewers vote for his or her owncommittee.
Although optimal performance h re wasslightly lower than for the reference system (46.5%,2.22), the gold standard is based on the primacy ofhuman committee assignments and have no guar-antee that the committee has an adequate num-ber of well qualified reviewers.
Without the ability224Accuracy:Average pos i t ion:One-of-best-2:H is togram:Simtezt only Simtezt and Simbib= 0.76Simbib only63.0% 39.1%Z=056.5%1.9671.7%T ~ p ~H~m~L1.85 2.3573.9% 56.5%iLConsensus evaluation= 0.76 consensus69.0%1.6578.3%Table 8: Performance of routing based on bibliographic similarityfor cross-committee r viewing, a committee with 3moderately-well qualified reviewers would probablybe preferable to a committee with only a single qual-ified reviewer but with extremely strong expertise.3.4 Rout ing  based  on  ( t rans i t ive)bib l iographic  s imi lar i tyAppropriate reviewers for a paper can often be deter-mined through analysis of the paper's bibliography.Clearly direct citation of a potential reviewer is par-tial evidence of that person's suitability to reviewthe paper.
This relation is also somewhat ransi-tive, as the authors who cite or are cited by an au-thor directly cited in the paper also have increasedlikelihood of being relevant reviewers.The goal of this section is to identify tran-sitively related authors via chains of the bib-liographic relations Cites(authori,authorj) andCoauthor(authori, authorj).
To estimate these re-lations, we automatically extracted and normalizedbibliographic itations from a large body of on-linetexts including all of the reviewer-submitted papers.Via transitive use of this extensive citation data,reviewer-paper similarity could be estimated evenwhen there was no direct mention of the reviewerin the text to be routed.To formalize this approach, let us assumethat there exists an indexed set of authors.A={al, .
.
.
,an~ }.
The reviewers are part of this set;let T~ = {rx .. .
rn. }
denote the set of reviewers.
Wealso dispose of a set of papers submitted by review-ers, P = {Pl , .
.
.
,Pn~}.
Using the set P we compute2 matrices: Cites and Coauthor:N Co,,oj)Cite .
, j  = Coau.,or,  = .?(?"?
')E E E No o,,o )/?=1 pET  ~ /*=1where Np (ai, aj) is the number of times a~ was citedin the paper p if ai is an author of p, 0 otherwise, andNc (ai, aj) is the number of papers in which ai anda 3 were coauthors identified either from the head ofD is tance  d 1 2 3 4Accuracy  30.3% 30.4% 32.6% 34.5%Average Pos i t ion  2.28 2.22 2.17 2.17One-of -best -2  65.2% I 71.7% 73.9% 71.7%Table 9: Performance comparison at different levelsof parameter d, A = 0.8 and fl = 1, evaluated ondevtest dataa paper p 6 ~, or a bibliographic itation extractedfrom p. The relation Cited_by can be captured bythe transposition of the citation matrix Cites T.A symmetric similarity matrix combining thesebase relations is defined as:Sire I = (Cites + Cites T) +(1 - A) ?
(Coauthor +Coauthor T) (4)where A is a weighting factor between the contribut-ing sources of similarity.
The index 1 (Sim*) denotes"direct" (non-transitive) bibliographic similarity.
Weenforced that Sim~i = 1 for all authors i.The submitted articles, P , , .
.
.
, Pnp were routedto committees based on similarities between the au-thors cited in the paper and the reviewers forming acommittee:1 Sim(PhC~) =~ ~ Sim(Pt,ri) (5)flECkwhere"" C (Pt, aj) ?
Sire* (aj,ri)j= lC (Pl, aj) being the number of times author aj wascited in paper Pz- A paper is routed to the commit-tee that maximizes the paper/committee similaritygiven in (5).
Tuning the parameter A on the trainingset yielded A = 0.8.The similarity relation computed in formula (4) isvery sparse, as a large number of values are 0.
Tocompute a more robust similarity, one can consider225the transitive closure of the graph defined by Sire 1.The weights in the resulting raphs are:Sirn ??
(i,j) = E C(ix.. .
in)i = i~,.
.
.
, i ,  = j (6)il ~ ivwhere Sire ~ (i, j) is the similarity between the i thand jth author.
The similarity along one path couldbe any function of the weights of the composinglinks.
The one we considered is:rL--1C (ix... i,) = H Sirn~ (ik, ik+l)k=lComputing the values in (6) proves to be com-putationally expensive, and it appears that extend-ing the transitive similarity relationship indefinitelymay become counterproductive.
Therefore, we lim-ited the length of the paths involved in computingthe formula (6):dSire d (i, j) = Ert~lE C (ix... i,~)i= ix , .
.
.
,in = jil ~ ivLet us observe that Sirn ??
(i,j) = limd~oo Sirn d (i,j),hence the name.
In Table 9, one can observe thatthe routing performance increases as d increases upthrough atransitive distance of 3, with mixed resultsbeyond that point.Section 3.4 has, until now, described a routingsimilarity based only on transitive bibliographic ci-tation and co-authorship (Simbib).
However, rout-ing a paper solely on this basis is not optimal as itignores imilarity between the the terms in the fulltext (Simte=t), as described in Section 3.1 using re-gion weighting.
We combined these two measuresthrough interpolation:S/m (P/, r i )  = fl" S imblb (Pl, r i )  +(1 - ~) Simtext (P .
r~) (7)On the training set, a value offl = 0.76 was found tomaximize performance, for d = 3 and the previouslyfixed ,~ = 0.8.The full evaluation of the transitive bibliographicsimilarity measure are given in Table 8.
Perfor-mance using exclusively Simbib (~ = 1) is consid-erably lower (39.1%) than the previous best text-based similarity (Simtezt) performance of56.5% ex-act match accuracy.
However, combining the twoevidence sources yields a substantially higher rout-ing accuracy of 63.0%.
This result is also observedwhen evaluating on the consensus gold standard e-scribed in Section 7, where combined model accu-racy of 69.0% exceeds the Sirnte=t only accuracy of67.4%.
As shall be shown, for both evaluation stan-dards the combined system accuracy rivals that ofseveral human judges.Accuracy:Average position:One-of-best-2:Histogram:52.2%2.0069.6%Table 10: Author-based paper routing3.4.1 Routing based exclusively on thepaper's authorPrior to now, we have ignored a submitted paper'sauthor(s) when making the routing decision.
How-ever, ACL'99 reviewing was not blind and an inter-esting question is what is routing performance whenclassification is based exclusively on the authors'identity.
Using only Simbib(aUthor, eviewerj) forthe paper's author(s), exact match accuracy com-pletely ignoring the submitted paper (52.5%) ap-proaches that of the accuracy using only the submit-ted text (56.5%), as shown in Table 10.
This sug-gests that an author's identity alone is largely suf-ficient for routing the paper to the committee mostappropriate for evaluating her or his work.4 Superv ised  Learn ingThe algorithms presented so far are unsupervised;the only use for labeled data in the devtest was forglobal parameter optimization.
This is a strength ofthe approach presented here, because it can be usedsuccessfully without any human annotation.
In thissection, we tested the efficacy of training supervisedmodels based on initial program chair annotation ofa portion of the submitted papers.
Models of thetypes of papers initially assigned to each committeecan help select further papers appropriate for thatcommittee.
Using the vector model, we can definethe centroid Dij of papers initially routed to a givencommittee as in (2), where Dij = ~P~eC, c (wj, Pk)and c (wj, Pk) is the count associated with paper Pkand the jth word.
Rather than use these modelsin isolation, we combine them with the previouslydescribed reviewer centroids for each committee Cijinto C~1 = Cij + A ?
Dij, where the parameter Awas optimized in the devtest o be 3.
The resultsare presented in Table 11, and outperform the sim-ple unsupervised model 60.9% to 56.5%, given initialprogram chair annotation of1/2 of the data (the de-vtest set).The updates to the base centroids were made off-line in our method; however, this is not required;226Accuracy:  60.9%Average position: 1.98One-of -best-2:Histogram:76.1%Table 11: Adaptation to the primary judge partialannotation of the dataonce the decision is made (a new paper is routed),the "true" label can be used to update the corre-sponding centroid.
There are numerous methodsthat could be borrowed from AI and IR to imple-ment this strategy, including Active Learning (Lewisand Gale, 1994).
Such online adaptation can maxi-mally leverage program chair feedback and minimizethe need for initial tagged training data.5 Automat ic  Area  Commit teeGenerat ionIn a hierarchical routing system, clearly the composi-tion of the committees i  crucial.
Suboptimal resultsare achieved if the 3 most appropriate reviewers fora paper are spread out over different committees.As an experiment to see if the committee organi-zation could possibly be improved, we investigatedempirically committee structures using several clus-tering strategies.In the first test, we generated a hierarchical ag-glomerative cluster of the entire reviewer set basedon the pairwise cosine similarity between their pub-lication vectors, using maximal linkage clustering(Duda and Hart, 1973; Jain and Dubes, 1988).
Theresults are given in Figure 2a, showing the full treeand extracted cluster list.
The numbers in bracketsindicate the actual committee assignment of the re-viewers; basic inspection will indicate that the de-rived clusters correspond closely to existing com-mittee compositions (although this information wascompletely ignored in the clustering process).
Anal-ysis of the substructure in the tree shows a naturalsub-clustering by research subfocus (e.g.
((isabelle(knight (fung wu))) somers)).
Inspection will alsoshow that people with close research focus are spreadout among 3 or more different committees, raising?
some doubts about the optimality of any committee-based routing process.In another experiment, we tested the extent towhich committees could more productively be re-formed by beginning with the initial committee cen-troids and redistributing the reviewers using K-means clustering.
We used a modified version ofit to obtain reviewer groups that are balanced insize similar to the original committees.
This wasdone by limiting the class size to the the maximumnumber of reviewers in an original class; the start-ing point of the algorithm was based on the originalcommittees.
The resulting clusters are shown in Fig-ure 2b.
The basic initial committee composition ispreserved, with some outliers reassigned.A third experiment was conducted to see if com-mittees could be reconstructed to better match thecommittee assignment of papers as proposed bythe program chair.
Specifically, we "reversed" therouting problem by computing committee centroidsbased on the set of submitted papers assigned to thecommittee by the program chair, and then routedthe reviewers to the committees as if each reviewerwas an abstract.
In this case, we did not impose anyrestriction on committee size.
The results are shownin Figure 2c.
One can still see the original commit-tees in the new organization; the fact that the thirdcommittee is large (21 reviewers, almost one third ofthe whole population) can be probably explained bythe fact that the papers routed to committee 3 wereinterdisciplinary, therefore they had a lot in commonwith many reviewers.Another meaningful measure for clustering is theSimbib (authori, authorj) based on transitive bibli-ographic citation and co-authorship (Section 3.4).Figure 2d shows the results of applying maximumlinkage agglomerative clustering to this similaritymeasure.
This also shows some correlation with themanually chosen committees.Finally, it is readily noted by the human judgesthat certain committees ( uch as 3 and 4) were quitesimilar and difficult to distinguish.
We can use ag-glomerative hierarchical clustering of our committeeprofile centroids to achieve some measure of relativecommittee distance.
The following tree confirms hu-man intuition regarding committee similarity:com5corn2COI~~ com4l coml --com6One application of this tree and associated is-tances is to weight the cost of committee misassign-ments by the severity of the error.
The majority ofthe system errors noted in Table 4 are between (3,4)and (1,6), which this empirical clustering would in-dicate are relatively low cost mistakes.227~!~_~-.
.
~e|\[~\]~.~.~..;;_~' ?
kurohashi\]2\] fillmore\]5\[ fellhaum\[g\] refenstette\[5\] cal~olari\[5\] par?Ill\]5\[~:...~-- ?
tsujii\[3\] bangalore\[2\] manning\]2\[ briscoe\[3\] charniak\[4\] bouma\[2\] johnson\]2\[ lin\[4\[~.~'~'=.
:..~ ?
mohri\[3\] chen\[4\] mat .
.
.
.
to\]3\[ wiebe\]l\], bruce\]4\[ shln\[3\] lee\[4\[ hang\]5\[=r..:.':'-.
* tanaka\]3\[ ng\[4\] daelemans\[4\] golding\[4\] roth\[4\] pedro?r\]5\[ resnlk\[3\] rock?own\]6\[, ~ ?
isabelle\[3\] knight\]3\[ lung\]3\[ wu\[4\] \[6\] .
.
.
.
.
.~ 0 1  * kaplan\[2\] cristea\[1\] satta\[2\] rogers\]2\[ becket\]2\[ welr\]2\]?
stone\]l\[ dieugenio\[1\] ash?rill poesio\[1\] bush\]f\]:.... * rayner\[3\] mccoy\[6\[ linden\]6\[ paris\]6\[ milosv, vljevic\[6\] oberlander\[6\] hahn\[l\[ bat?man\]6\[bus?mann\]6\[ elhadad\]6\] rambow\[6\[?
steedman\[2\[ green\]l\[ moore\]l\[ earberry\[6\] hirst\[6\] sidner\[l\] zukerman\]6\]r obtained by average-linkage agglomerative clustering of reviewer papers?
kcoml  ash?rill busa\[5\[ calzo|ari\[5\] eristea\[1\] dieugenio\[1\] hahn\]l\[ paris\]6\[ poesio\[l\[ sidner\[1\] stone\]l\[ wiebe\[11?
kcorng bffingalore\[2\] becker\[2\] bouma\[2\] johnson\[2\] lin\[4\[ manning\]2\] mohri\[3\] rogers\]2\] satta\[2\] tsujii\]3\] weir\]2\]?
kcom3 briscoe\[3\] che~rniak\[4\] fung\]3\] ieabelle\]3\] knight\]3\[ matsumoto\[3\] rock?own\[6\] rayner\[3\] resnlk\[3\[ shin\]3\[ wu\]4\]?
kcorn4 bruce\]4\[ chen\[4\] daelemans\[4\] golding\[4\] lee\]4\] ng\[4\[ ratnaparkhi\[4\] roth\[4\]?
k?omg bel\]5\] fellbaum\[5\] fillmore\]5\[ grefenstette\[8\] hang\[5\] kaplan\]2\] kurohashi\[2) palmer\]5\] pirelli\[5\[ sos?re\]6\[ tanaka\]3\]?
kcom6 batemsn\[6\] bus?mann{6\[ carberry\[6\[ elhadad\[6\] green\]l} hirst\[6\] linden\]6\[ mccoy\]6\[ milosavljevic\[6\] moore\]l\[ oberlander\[6\] rambow\[6\] steedman\[2\[ zukerman\[6\]Figure 2b: Committees obtained by k-means reclustering of initial committees?
r coml  ssher\[l\] cristea\[1\] dieugenio\[l\] green\]l\[ moore\]l\[ poe?loll\] sidner\[l\] steedman\]2\] pirel|i\[5\] carberry\[6\] hlrst\[6\]?
r?om2 hahn\]l\[ becker\[2\] bouma\[2\] johnson\]2\[ mannlng\[2\] rogers\]2\[ satta\]2\] weir\]2\[ briscoe\]3\[ tsujii\[3\] lln\[4\]?
room3 wiebe\[l\] bangaIore\[2\] isabe||e\]3\] knight\[3\] matsumoto\]3\] mohri\[3\] rayner\[3\] shin\[3\] bruce\]4\[ chen\[4\] daelemans\[4\] Eolding\[4\[ lee\]4\[ ratnaparkhi\[4\]?
room4 funs\[3\[ resnik\[3\] tsnaka\[3\[ charniak\[4\] ng\[4\] hang\]5\[ mckeown\[6\]?
rcomg kurohashi\[2\[ fellbaum\[5\] finmore\[5\] gre fenstet te \ [5 \ ]?
room6 kaplan\[2\] bel\[5\] busa\[5\] batem~n\[6\] bus?mann\]6\[ elhadad\[6\] linden\]6\] milosavljevic\[6\] oberlander\[6\] paris\]6\[ ramhow\]6\] sukerman\[6\[Figure 2c: Committees obtained by reverse routing reviewers to the centroids of assigned papers.
.
.
.
.
~I ?
kurohashi\[2\] mckeown\[6\] bell5\[~ ~ .
~ i ~ g ' ~ ' ~ :  ?
mi!
.
.
.
.
ljevic\[6\] oberlander\[61 bat .
.
.
.
\[6\] .
.
.
.
.
\[I\] linden\]6\] paris\]6\[ .
.
.
.
Y\[61 auk .
.
.
.
.
161 - -  ~-~V.~)~ ~-~'.,~ :'~":~:~-~-==~.~r~ ~, isabel|e\]3\] sos?re\]6\] elhadad\[6\[ knight\]3\] bu.
.
.
.
nn\[6\] rayner\[3\] bus~\[5\] kaplan\[2\]" ~;~: :~ ' j -~ ,  ?
tanv, ka\[3\] tsujii\[3\] mEtnning\[2\] briscoe\[3\] parr?Ill\]5\[ grenfenstette\[5\] ca|solari\[5\] ?
H=z,i ~=T:.% _ ,? '
_ _  ~!
:~_~'~l--., ?
lee\]4\] li\]5\] m~tsumoto\[3\] mohri\[3\] bangalore\]2\] ratnaparkhi\[4\] ng\[4\] wiebe\[1\] bruce\]4\[: '  ~ .=~H~T~T~;~!~.~ fungi3\[ wu\[4\] chang\]3\] golding\[4\] roth\[4\] chen\[4\]---I , ~ ~'~-- :~'-" ?
fellbaum\[5\] flnmore\[5\] resnik\[2\] sstta\]2\[ bouma\[2\] lin\[4\] johnson\]2\[ charniak\[4\] rogers\]2\]__  i - -  \ [~  ~\[~ ' weir\]2\[ hahn\[l\[ dae|emans\[4\[?
_ _  k~:-  .,poesio\]l\] hit?t\]6\[ green\]l\[ carberry\[O\] pa|mer\[5\[ sidner\[l\] steedman\[2\] atone\[l\[L,~i!~\] ~i~i ~'~: ?
asher\[1\]crl .
.
.
.
\[1\]di eugenio\[1\]Figure 2d: Reviewer clusters based on agglomerative clustering using bibliographical similarity6 System Usage and  Conf idenceMeasures  for  Rout ingThe routing algorithms presented here have two nat-ural modes of application.
The system's commit-tee recommendations can be used either for post-hoc routing error identification (as a sanity check)or for pre-hoc initial automatic assignment with hu-man verification 5 .
The latter strategy requires somemeasure of system confidence for optimal applica-tion.
Such a measure would help a human judgeminimize the time spent in performing the task.
Ifthe system is very confident, one might even decideto accept the decision without careful review.
Onthe other hand, in cases where the system is notconfident, full attention is required.Based on the ranked output of the system, we5The former strategy was actual ly employed in ACL'99reviewing.searched for feature transformations whose outputcan be used in determining confidence intervals.
Areasonable one is 5 = =1-=2 where the Xl and x2 =1are the scores associated with the first and secondchoices of the system.
A plot of the averaged ac-curacy of this operator is depicted in Figure 1 (thevalue interval was divided in 10 equal and partiallyoverlapping bins and average accuracy was com-puted on each one of them).
The graph on theright shows the accuracy in the case where rankingthe gold standard as the system's econd committeechoice is not considered an error.One conclusion that can be drawn from the plotsis that one can be relatively confident in the sys-tem classification if the value of 6 is above the 0.25threshold, while 6 < 0.1 tends to indicate lowest ex-pected accuracy and greatest need for careful humaninspection.
Such confidence measures may also be2281o.a~ o.4-+~4 +?
+~ ?++ *??
~?o.~, o:, o.
', o~ o;, o:, o.~ o'.,Relative Difference B tween First 2 Scoreso.~ o.~"O o.sP.
"~ o.e8?qo.4o~ o2 JU< o ?
~ ~+o.~ o'., o.
', o~ o.~ o:, o.h oi,Relative Difference B tween First 2 Scoreso~Figure 1: Measures of confidenceused in posthoc orrection of human assignments torank the most likely human errors for re-inspection.7 Human Per fo rmance  andConsensus  Generat ionArea committee routing is a difficult task for hu-mans.
Table 12 shows the relatively low inter-judgeagreement rates for the 4 judges mentioned in Sec-tion 2.2 when annotating the 46-word primary testset.
Judge 1 (the program chair) had a slightly dif-ferent objective function for routing (including theavoidance of conflicts of interest and perhaps omecommittee size balancing), explaining slightly loweragreement rates than that between the two facultymembers (Judges 2 and 3) who had the same taskdescription of finding the most appropriate commit-tee without constraint.
Judge 4 was a knowledgeablebut less experienced 3rd year graduate student, andhis lower performance r lative to his colleagues mayhave been due to more limited familiarity with thereviewers and their expertise.In order to improve the quality of the gold stan-dard, a consensus standard was generated by takingthe majority vote of Judges 1-3.
In case of a tie,the program chair was used as the definitive assign-ment.
In nearly 80% of the data, the consensus wasidentical to the program chair's assignment.Table 13 illustrates the performance of Judge 4and the reference Systems (Section 3.1 and 3.4) forboth the Judge 1 and Consensus gold standards.Both Judge 4 and the System agreed substantiallymore with the consensus than the Judge 1 standard,providing some evidence for the relative merit of theconsensus tandard.
The most interesting result,however, is that the system performed better thanthe graduate student Judge 4 for both standards (al-though generally lower than the performance of themore experienced faculty members).
This suggeststhat system performance, by virtue of its inherentlymuch greater familiarity with the publications andJudge1 Judge2 Judge3 Judge4Judgel  100 60.9 65.2 45.6Judge2 60.9 100 73.9 47.8Judge3 65.2 73.9 100 52.2Judge4 45.6 47.8 52.2 100Table 12: Human judge agreementhence the expertise of the reviewers, more than com-pensates for its rather limited skills at generalizationand inference.
This would suggest that the proposedalgorithm may be as effective (or even more effec-tive than) human paper outers except for the mostknowledgeable human judges.The final observation is that in cases where thereis high agreement among the human judges, systemrouting accuracy is also very high.
Table 14 dividesthe data by thresholds of minimum agreement be-tween the Judges 1-3, as the primary partitioningprinciple using the Section 3.1 system without theSimbib extension.
Given a certain level of agree-ment (e.g.
all 3 judges agree), it's also useful toconsider whether the 4th Judge agreed or not withthat consensus.
By giving the less-experienced 4thJudge an effective 1/2 vote, further refinement inthe granularity of consensus can be obtained with-out effectiving the primacy of the votes of Judges1-3.
In the 57% of the data where only the first 3judges agree, system accuracy exceeds 80%.
In themost confidently classified 35% of the data whereall 4 judges agree, system accuracy approaches 88%and in 100% of these cases the consensus commit-tee was one of the system's top two choices.
Theseresults strongly suggest hat in the clear-cut caseswhere humans consistently agree on a classification,system performance is very reliable too.
The largebulk of system "errors" are in cases where humanstend to disagree as well.229Judgel  Judge2 Judge3 Judge4 System 3.1Judge1 assignment = Truth 100 60.9 65.2 45.6 56.5Consensus = Truth i 78.3 82.6 82.6 56.5 67.4Table 13: Human judge and system agreement with 2 goldstandardsSystem 3.463.069.0MinimumAgreement of data1 1001.5 982 872.5 723 573.5 35SystemAccuracy67.4%68.8%75.0%78.8%8O.8%87.5%AveragePosition1.721.641.381.301.231.12One-of-2-best78.3%80.0%87.5%90.9%96.2%100.0%Table 14: Routing results given levels of minimum human agreement on committee assignment8 Conc lus ionsThis paper has presented and extensively evaluateda class of algorithms for automatic routing of sub-mitted papers to reviewers and area committees,without the need for any human annotation fromthe reviewers or the program chair.
Routing isbased on a profile of previous writings obtainableon-line for the reviewer pool, a generally stable andreusable resource that requires no manual adapta-tion for new submission streams.
The paper ex-plored a wide set of variations and extensions on thecore model, and system accuracy approaches or ex-ceeds that of human judges on the same task.
Thisresearch demonstrates that such automated paperrouting techniques may have merit for paper rout-ing for future conferences, especially those with rel-atively large and diverse program committees whereit is difficult for one person to be familiar with thefull range of expertise of all committee members.9 AcknowledgementsMany thanks are owed to Ken Church and RobertDale, ACL'99 Program Committee co-Chairs, theACL'99 area chairs, and the participating review-ers and authors for their essential support of thisproject.
It is hoped that insights gained in this fea-sibility study will be useful for future program chairsdeciding if, where and how to utilize automatic rout-ing strategies in support of their committee and re-viewer assignment tasks.
This research was partiallysupported by the National Science Foundation grantIRI-9618874.ReferencesS.
Deerwester, S. Dumals, T. Landauer, G. Furnas,and R. Harsham.
1990.
Indexing by latent seman-tic analysis.
Journal of the Society for Informa-tion Science, 41(6):391-407.Richard O. Duda and Peter E. Hart.
1973.
PatternClassification and Scene Analysis.
John Wiley.Susan Dumals and Jakob Nielsen.
1992.
Automat-ing the assignment of submitted manuscripts oreviewers.
In Proceedings of SIGIR '92, pages233-244, Copenhagen, Denmark.Haym Hirsh.
Personal communication.D.
Hull.
1994.
Improving text retrieval for the rout-ing problem using latent semantic indexing.
InProceedings of SIGIR '94, pages 282-291, NewYork.Anil K. Jain and Richard C. Dubes.
1988.
Algo-rithms for Clustering Data.
Prentice Hall.L.
Larkey and W.B.
Croft.
1996.
Combining classi-tiers in text categorization.
In Proceedings of SI-GIR '96.D.
Lewis and W. Gale.
1994.
A sequential algo-rithm for training text classifiers.
In Proceedingsof the Seventeenth Annual International ACM-SIGIR Conference on Research and Developmentin Information Retrieval, pages 3-12, Dublin.Andrew McCallum, Kamal Nigam, Jason Rennie,and Kristie Seymore.
1999.
Building domain-specific search engines with machine learning tech-niques.
In Proceedings of the AAAI Spring Sym-posium on Intelligent Agents in Cyberspace.F.
Mosteller and D. Wallace.
1964.
Inference andDisputed Authorship: The Federalist.
Addison-Wesley, Reading, Massachusetts.G.
Salton and M. McGill.
1983.
An Introduc-tion to Modern Information Retrieval.
New York,McGraw-Hill.E.
Voorhees and D. Harman.
1998.
Overview ofthe 6th text retrieval conference (trec-6).
In Pro-ceedings of the Sixth Text REtrieval Conference(TREC-6).
NIST Special Publication, 500-240.230
