Proceedings of ACL-08: HLT, Short Papers (Companion Volume), pages 145?148,Columbus, Ohio, USA, June 2008. c?2008 Association for Computational LinguisticsEvent Matching Using the Transitive Closure of Dependency RelationsDaniel M. Bikel and Vittorio CastelliIBM T. J. Watson Research Center1101 Kitchawan RoadYorktown Heights, NY 10598{dbikel,vittorio}@us.ibm.comAbstractThis paper describes a novel event-matchingstrategy using features obtained from the tran-sitive closure of dependency relations.
Themethod yields a model capable of matchingevents with an F-measure of 66.5%.1 IntroductionQuestion answering systems are evolving from theirroots as factoid or definitional answering systemsto systems capable of answering much more open-ended questions.
For example, it is one thing to askfor the birthplace of a person, but it is quite anotherto ask for all locations visited by a person over aspecific period of time.Queries may contain several types of arguments:person, organization, country, location, etc.
By far,however, the most challenging of the argument typesare the event or topic arguments, where the argumenttext can be a noun phrase, a participial verb phraseor an entire indicative clause.
For example, the fol-lowing are all possible event arguments:?
the U.S. invasion of Iraq?
Red Cross admitting Israeli and Palestiniangroups?
GM offers buyouts to union employeesIn this paper, we describe a method to matchan event query argument to the sentences thatmention that event.
That is, we seek to modelp(s contains e | s, e), where e is a textual descriptionof an event (such as an event argument for a GALEdistillation query) and where s is an arbitrary sen-tence.
In the first example above, ?the U.S. inva-sion of Iraq?, such a model should produce a veryhigh score for that event description and the sentence?The U.S. invaded Iraq in 2003.?2 Low-level featuresAs the foregoing implies, we are interested in train-ing a binary classifier, and so we represent eachtraining and test instance in a feature space.
Con-ceptually, our features are of three different varieties.This section describes the first two kinds, which wecall ?low-level?
features, in that they attempt to cap-ture how much of the basic information of an evente is present in a sentence s.2.1 Lexical featuresWe employ several types of simple lexical-matchingfeatures.
These are similar to the ?bag-of-words?
features common to many IR and question-answering systems.
Specifically, we compute thevalue overlap(s, e) = ws?we|we |1 , where we (resp: ws) isthe {0,1}-valued word-feature vector for the event(resp: sentence).
This value is simply the fractionof distinct words in e that are present in s. We thenquantize this fraction into the bins [0, 0], (0, 0.33],(0.33, 0.66], (0.66, 0.99], (0.99, 1], to produce oneof five, binary-valued features to indicate whethernone, few, some, many or all of the words match.12.2 Argument analysis and submodelsSince an event or topic most often involves entitiesof various kinds, we need a method to recognizethose entity mentions.
For example, in the event?Abdul Halim Khaddam resigns as Vice Presidentof Syria?, we have a ??????
mention, an ??????-????
mention and a ???
(geopolitical entity) mention.We use an information extraction toolkit (Florianet al, 2004) to analyze each event argument.
Thetoolkit performs the following steps: tokenization,part-of-speech tagging, parsing, mention detection,within-document coreference resolution and cross-document coreference resolution.
We also apply thetoolkit to our entire search corpus.After determining the entities in an event descrip-tion, we rely on lower-level binary classifiers, eachof which has been trained to match a specific type1Other binnings did not significantly alter the performanceof the models we trained, and so we used the above binningstrategy for all experiments reported in this paper.145of entity.
For example, we use a ?????
?-matchingmodel to determine if, say, ?Abdul Halim Khad-dam?
from an event description is mentioned in asentence.2 We build binary-valued feature functionsfrom the output of our four lower-level classifiers.3 Dependency relation featuresEmploying syntactic or dependency relations to aidquestion answering systems is by no means new (At-tardi et al, 2001; Cui et al, 2005; Shen and Klakow,2006).
These approaches all involved various de-grees of loose matching of the relations in a queryrelative to sentences.
More recently, Wang et al(2007) explored the use a formalism called quasi-synchronous grammar (Smith and Eisner, 2006) inorder to find a more explicit model for matching theset of dependencies, and yet still allow for loosenessin the matching.3.1 The dependency relationIn contrast to previous work using relations, we donot seek to model explicitly a process that trans-forms one dependency tree to another, nor do weseek to come up with ad hoc correlation measuresor path similarity measures.
Rather, we propose touse features based on the transitive closure of thedependency relation of the event and that of the de-pendency relation of the sentence.
Our aim was toachieve a balance between the specificity of depen-dency paths and the generality of dependency pairs.In its most basic form, a dependency tree fora sentence w = ?
?1, ?w, .
.
.
, ?k?
is a rooted tree?
= ?V, E, r?, where V = {1, .
.
.
, k}, E ={(i, j) : ?i is the child of ?
j}and r ?
{1, .
.
.
, k} :?r is the root word.
Each element ?i of our wordsequence, rather than being a simple lexical itemdrawn from a finite vocabulary, will be a complexstructure.
With each word wi we associate a part-of-speech tag ti, a morph (or stem) mi (which is wiitself if wi has no variant), a set of nonterminal labelsNi, a set of synonyms S i for that word and a canon-ical mention cm(i).
Formally, we let each sequenceelement be a sextuple ?i = ?wi, ti,mi, Ni, S i, cm(i)?.2This is not as trivial as it might sound: the model must dealwith name variants (parts of names, alternate spellings, nick-names) and with metonymic uses of titles (?Mr.
President?
re-ferring to Bill Clinton or George W. Bush).S(ate)NP(Cathy)CathyVP(ate)ateFigure 1: Simple lexicalized tree.We derive dependency trees from head-lexicalized syntactic parse trees.
The set ofnonterminal labels associated with each word is theset of labels of the nodes for which that word wasthe head.
For example, in the lexicalized tree inFigure 1, the head word ?ate?
would be associatedwith both the nonterminals S and VP.
Also, if ahead word is part of an entity mention, then the?canonical?
version of that mention is associatedwith the word, where canonical essentially meansthe best version of that mention in its coreferencechain (produced by our information extractiontoolkit), denoted cm(i).
In Figure 1, the first wordw1 = Cathy would probably be recognized as a??????
mention, and if the coreference resolverfound it to be coreferent with a mention earlierin the same document, say, Cathy Smith, thencm(1) = Cathy Smith.3.2 Matching on the transitive closureSince E represents the child-of dependency relation,let us now consider the transitive closure, E?, whichis then the descendant-of relation.3 Our features arecomputed by examining the overlap between E?e andE?s, the descendant-of relation of the event descrip-tion e and the sentence s, respectively.
We use thefollowing, two-tiered strategy.Let de, ds be elements of E?e and E?s, with dx.d de-noting the index of the word that is the descendantin dx and dx.a denoting the ancestor.
We define thefollowing matching function to match the pair of de-scendants (or ancestors):matchd(de, ds) = (1)(mde.d = mds.d)?
(cm(de.d) = cm(ds.d))where matcha is defined analogously for ancestors.That is, matchd(de, ds) returns true if the morph ofthe descendant of de is the same as the morph ofthe descendant of ds, or if both descendants havecanonical mentions with an exact string match; the3We remove all edges (i, j) from E?
where either wi or w j isa stop word.146function returns false otherwise, and matcha is de-fined analogously for the pair of ancestors.
Thus,the pair of functions matchd,matcha are ?morph ormention?
matchers.
We can now define our mainmatching function in terms of matchd and matcha:match(de, ds) = matchd(de, ds) ?
matcha(de, ds).
(2)Informally, match(de, ds) returns true if the pairof descendants have a ?morph-or-mention?
matchand if the pair of ancestors have a ?morph-or-mention?
match.
When match(de, ds) = true, weuse ?morph-or-mention?
matching features.If match(de, ds) = false we then attempt to per-form matching based on synonyms of the words in-volved in the two dependencies (the ?second tier?
ofour two-tiered strategy).
Recall that S de.d is the setof synonyms for the word at index de.d.
Since wedo not perform word sense disambiguation, S de.d isthe union of all possible synsets for wde.d.
We thendefine the following function for determining if twodependency pairs match at the synonym level:synmatch(de, ds) = (3)(S de.d ?
S ds.d , ?)?
(S de.a ?
S ds.a , ?
).This function returns true iff the pair of descen-dants share at least one synonym and the pair of an-cestors share at least one synonym.
If there is a syn-onym match, we use synonym-matching features.3.3 Dependency matching featuresThe same sorts of features are produced whetherthere is a ?morph-or-mention?
match or a synonymmatch; however, we still distinguish the two typesof features, so that the model may learn differentweights according to what type of matching hap-pened.
The two matching situations each producefour types of features.
Figure 2 shows these fourtypes of features using the event of ?Abdul HalimKhaddam resigns as Vice President of Syria?
and thesentence ?The resignation of Khaddam was abrupt?as an example.
In particular, the ?depth?
features at-tempt to capture the ?importance?
the dependencymatch, as measured by the depth of the ancestor inthe event dependency tree.We have one additional type of feature: we com-pute the following kernel function on the two sets ofdependencies E?e and E?s and create features based onquantizing the value:K(E?e, E?s) = (4)?
(de,ds)?E?e?E?s : match(de,ds)(?
(de) ?
?(ds))?1,?
((i, j)) being the path distance in ?
from node i to j.4 Data and experimentsWe created 159 queries to test this model frame-work.
We adapted a publicly-available search en-gine (citation omitted) to retrieve documents au-tomatically from the GALE corpus likely to berelevant to the event queries, and then used aset of simple heuristics?a subset of the low-level features described in ?2?to retrieve sen-tences that were more likely than not to be rel-evant.
We then had our most experienced an-notator annotate sentences with five possible tags:relevant, irrelevant, relevant-in-context,irrelevant-in-context and garbage (to dealwith sentences that were unintelligible ?wordsalad?
).4 Crucially, the annotation guidelines forthis task were that an event had to be explicitly men-tioned in a sentence in order for that sentence to betagged relevant.We separated the data roughly into an 80/10/10split for training, devtest and test.
We then trainedour event-matching model solely on the examplesmarked relevant or irrelevant, of which therewere 3546 instances.
For all the experiments re-ported, we tested on our development test set, whichcomprised 465 instances that had been markedrelevant or irrelevant.We trained the kernel version of an averaged per-ceptron model (Freund and Schapire, 1999), using apolynomial kernel with degree 4 and additive term 1.As a baseline, we trained and tested a model usingonly the lexical-matching features.
We then trainedand tested models using only the low-level featuresand all features.
Figure 3 shows the performancestatistics of all three models, and Figure 4 shows theROC curves of these models.
Clearly, the depen-dency features help; at our normal operating point of0, F-measure rises from 62.2 to 66.5.
Looking solely4The *-in-context tags were to be able to re-use the datafor an upstream system capable of handling the GALE distilla-tion query type ?list facts about [event]?.147Feature type Example CommentMorph bigram x-resign-Khaddam Sparse, but helpful.Tag bigram x-VBZ-NNPNonterminal x-VP-NP All pairs from Ni ?
N j for (i, j) ?
E?e.Depth x-eventArgHeadDepth=0 Depth is 0 because ?resigns?
is root of event.Figure 2: Types of dependency features.
Example features are for e = ?Abdul Halim Khaddam resigns as VicePresident of Syria?
and s = ?The resignation of Khaddam was abrupt.?
In example features, x ?
{m, s}, depending onwhether the dependency match was due to ?morph-or-mention?
matching or synonym matching.Model R P Flex 36.6 76.3 49.5low-level 63.9 60.5 62.2all 69.1 64.1 66.5Figure 3: Performance of models.00.20.40.60.810  0.2  0.4  0.6  0.8  1True positive rateFalse positive rateall featureslow-level featureslexical featuresFigure 4: ROC curves of model with only low-level fea-tures vs. model with all features.at pairs of predictions, McNemar?s test reveals dif-ferences (p  0.05) between the predictions of thebaseline model and the other two models, but notbetween those of the low-level model and the modeltrained with all features.5 DiscussionThere have been several efforts to incorporate de-pendency information into a question-answeringsystem.
These have attempted to define either adhoc similarity measures or a tree transformation pro-cess, whose parameters must be learned.
By usingthe transitive closure of the dependency relation, webelieve that?especially in the face of a small dataset?we have struck a balance between the represen-tative power of dependencies and the need to remainagnostic with respect to similarity measures or for-malisms; we merely let the features speak for them-selves and have the training procedure of a robustclassifier learn the appropriate weights.AcknowledgementsThis work supported by DARPA grant HR0011-06-02-0001.
Special thanks to Radu Florian and JeffreySorensen for their helpful comments.ReferencesGiuseppe Attardi, Antonio Cisternino, FrancescoFormica, Maria Simi, Alessandro Tommasi, Ellen M.Voorhees, and D. K. Harman.
2001.
Selectively usingrelations to improve precision in question answering.In TREC-10, Gaithersburg, Maryland.Hang Cui, Renxu Sun, Keya Li, Min-Yen Kan, and Tat-Seng Chua.
2005.
Question answering passage re-trieval using dependency relations.
In SIGIR 2005,Salvador, Brazil, August.Radu Florian, Hani Hassan, Abraham Ittycheriah,Hongyan Jing, Nanda Kambhatla, Xiaoqiang Luo,Nicholas Nicolov, and Salim Roukos.
2004.
A statis-tical model for multilingual entity detection and track-ing.
In HLT-NAACL 2004, pages 1?8.Yoav Freund and Robert E. Schapire.
1999.
Large mar-gin classification using the perceptron algorithm.
Ma-chine Learning, 37(3):277?296.Dan Shen and Dietrich Klakow.
2006.
Exploring corre-lation of dependency relation paths for answer extrac-tion.
In COLING-ACL 2006, Sydney, Australia.David A. Smith and Jason Eisner.
2006.
Quasi-synchronous grammars: Alignment by soft projectionof syntactic dependencies.
In HLT-NAACL Workshopon Statistical Machine Translation, pages 23?30.Mengqiu Wang, Noah A. Smith, and Teruko Mita-mura.
2007.
What is the Jeopardy model?
a quasi-synchronous grammar for QA.
In EMNLP-CoNLL2007, pages 22?32.148
