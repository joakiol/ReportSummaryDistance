Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 676?681,Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational LinguisticsUnary Constraints for Efficient Context-Free ParsingNathan Bodenstab?
Kristy Hollingshead?
and Brian Roark??
Center for Spoken Language Understanding, Oregon Health & Science University, Portland, OR?University of Maryland Institute for Advanced Computer Studies, College Park, MD{bodensta,roark}@cslu.ogi.edu hollingk@umiacs.umd.eduAbstractWe present a novel pruning method forcontext-free parsing that increases efficiencyby disallowing phrase-level unary productionsin CKY chart cells spanning a single word.Our work is orthogonal to recent work on?closing?
chart cells, which has focused onmulti-word constituents, leaving span-1 chartcells unpruned.
We show that a simple dis-criminative classifier can learn with high ac-curacy which span-1 chart cells to close tophrase-level unary productions.
Eliminatingthese unary productions from the search canhave a large impact on downstream process-ing, depending on implementation details ofthe search.
We apply our method to four pars-ing architectures and demonstrate how it iscomplementary to the cell-closing paradigm,as well as other pruning methods such ascoarse-to-fine, agenda, and beam-search prun-ing.1 IntroductionWhile there have been great advances in the statis-tical modeling of hierarchical syntactic structure inthe past 15 years, exact inference with such modelsremains very costly and most rich syntactic mod-eling approaches resort to heavy pruning, pipelin-ing, or both.
Graph-based pruning methods suchas best-first and beam-search have both be usedwithin context-free parsers to increase their effi-ciency.
Pipeline systems make use of simpler mod-els to reduce the search space of the full model.
Forexample, the well-known Charniak parser (Char-niak, 2000) uses a simple grammar to prune thesearch space for a richer model in a second pass.Roark and Hollingshead (2008; 2009) have re-cently shown that using a finite-state tagger to closecells within the CKY chart can reduce the worst-caseand average-case complexity of context-free pars-ing, without reducing accuracy.
In their work, wordpositions are classified as beginning and/or endingmulti-word constituents, and all chart cells not con-forming to these constraints can be pruned.
Zhanget al (2010) and Bodenstab et al (2011) both ex-tend this approach by classifying chart cells with afiner granularity.
Pruning based on constituent spanis straightforwardly applicable to all parsing archi-tectures, yet the methods mentioned above only con-sider spans of length two or greater.
Lexical andunary productions spanning a single word are neverpruned, and these can, in many cases, contribute sig-nificantly to the parsing effort.In this paper, we investigate complementarymethods to prune chart cells with finite-state pre-processing.
Informally, we use a tagger to re-strict the number of unary productions with non-terminals on the right-hand side that can be includedin cells spanning a single word.
We term these sin-gle word constituents (SWCs) (see Section 2 for aformal definition).
Disallowing SWCs alters span-1cell population from potentially containing all non-terminals to just pre-terminal part-of-speech (POS)non-terminals.
In practice, this decreases the num-ber of active states in span-1 chart cells by 70%,significantly reducing the number of allowable con-stituents in larger spans.
Span-1 chart cells are alsothe most frequently queried cells in the CKY algo-rithm.
The search over possible midpoints will al-ways include two cells spanning a single word ?
oneas the first left child and one as the last right child.
Itis therefore critical that the number of active states676(a) Original tree (b) Transformed tree (c) Dynamic programming chartFigure 1: Example parse structure in (a) the original Penn treebank format and (b) after standard transformations have beenapplied.
The black cells in (c) indicate CKY chart cells containing a single-word constituent from the transformed tree.in these cells be minimized so that the number ofgrammar access requests is also minimized.
Note,however, that some methods of grammar access ?such as scanning through the rules of a grammar andlooking for matches in the chart ?
achieve less of aspeedup from diminished cell population than oth-ers, something we investigate in this paper.Importantly, our method is orthogonal to priorwork on tagging chart constraints and we expect ef-ficiency gains to be additive.
In what follows, wewill demonstrate that a finite-state tagger can learn,with high accuracy, which span-1 chart cells can beclosed to SWCs, and how such pruning can increasethe efficiency of context-free parsing.2 Grammar and Parsing PreliminariesGiven a probabilistic context-free grammar (PCFG)defined as the tuple (V, T, S?, P, ?)
where V is theset of non-terminals, T is the set of terminals, S?
is aspecial start symbol, P is the set of grammar produc-tions, and ?
is a mapping of grammar productions toprobabilities, we divide the set of non-terminals Vinto two disjoint subsets VPOS and VPHR such thatVPOS contains all pre-terminal part-of-speech tagsand VPHR contains all phrase-level non-terminals.We define a single word constituent (SWC) unaryproduction as any production A?B ?
P such thatA ?
VPHR and A spans (derives) a single word.
Anexample SWC unary production, VP?
VBP, can beseen in Figure 1b.
Note that ROOT ?
SBAR andRB ?
?quickly?
in Figure 1b are also unary pro-ductions, but by definition they are not SWC unaryproductions.One implementation detail necessary to leveragethe benefits of sparsely populated chart cells is thegrammar access method used by the inner loop ofthe CKY algorithm.1 In bottom-up CKY parsing,to extend derivations of adjacent substrings into newconstituents spanning the combined string, one caneither iterate over all binary productions in the gram-mar and test if the new derivation is valid (gram-mar loop), or one can take the cross-product of ac-tive states in the cells spanning the substrings andpoll the grammar for possible derivations (cross-product).
With the cross-product approach, feweractive states in either child cell leads to fewer gram-mar access operations.
Thus, pruning constituentsin lower cells directly affects the overall efficiencyof parsing.
On the other hand, with the grammarloop method there is a constant number of gram-mar access operations (i.e., the number of grammarrules) and the number of active states in each childcell has no impact on efficiency.
Therefore, withthe grammar loop implementation of the CYK algo-rithm, pruning techniques such as unary constraintswill have very little impact on the final run-time effi-ciency of the parser.
We will report results in Section5 with parsers using both approaches.3 Treebank Unary ProductionsIn this section, we discuss the use of unary produc-tions both in the Penn WSJ treebank (Marcus et al,1999) and during parsing by analyzing their func-tion and frequency.
All statistics reported here arecomputed from sections 2-21 of the treebank.A common pre-processing step in treebank pars-ing is to transform the original WSJ treebank be-fore training and evaluation.
There is some flex-1Some familiarity with the CKY algorithm is assumed.
Fordetails on the algorithm, see Roark and Sproat (2007).677Orig.
Trans.Empty nodes 48,895 0Multi-Word Const.
unaries 1,225 36,608SWC unaries 98,467 105,973Lexical unaries 950,028 950,028Pct words with SWC unary 10.4% 11.2%Table 1: Unary production counts from sections 2-21 of theoriginal and transformed WSJ treebank.
All multisets are dis-joint.
Lexical unary count is identical to word count.ibility in this process, but most pre-processing ef-forts include (1) affixing a ROOT unary productionto the root symbol of the original tree, (2) removalof empty nodes, and (3) striping functional tags andcross-referencing annotations.
See Figure 1 for anexample.
Additional transforms include (4) remov-ing X?
X unary productions for all non-terminalsX, (5) collapsing unary chains to a single (possiblycomposite) unary production (Klein and Manning,2001), (6) introducing new categories such as AUX(Charniak, 1997), and (7) collapsing of categoriessuch as PRT and ADVP (Collins, 1997).
For thispaper we only apply transforms 1-3 and otherwiseleave the treebank in its original form.
We also notethat ROOT unaries are a special case that do not af-fect search, and we choose to ignore them for theremainder of this paper.These tree transformations have a large impacton the number and type of unary productions inthe treebank.
Table 1 displays the absolute countsof unaries in the treebank before and after process-ing.
Multi-word constituent unary productions in theoriginal treebank are rare and used primarily to markquantifier phrases as noun phrases.
But due to theremoval of empty nodes, the transformed treebankcontains many more unary productions that spanmultiple words, such as S ?
VP, where the nounphrase was left unspecified in the original clause.The number of SWC unaries is relatively un-changed after processing the original treebank, butnote that only 11.2% of words in the transformedtreebank are covered by SWCs.
This implies thatwe are unnecessarily adding SWC productions to al-most 90% of span-1 chart cells during search.
Onemay argue that an unsmoothed grammar will nat-urally disallow most SWC productions since theyare never observed in the training data, for exampleMk2 Mk2+S Latent|VPOS| 45 45 582|VPHR| 26 26 275SWC grammar rules 159 1,170 91,858Active VPOS states 2.5 45 75Active VPHR states 5.9 26 152Table 2: Grammar statistics and averaged span-1 active statecounts for exhaustive parsing of section 24 using a Markovorder-2 (Mk2), a smoothed Markov order-2 (Mk2+S), and theBerkeley latent variable (Latent) grammars.VP ?
DT.
This is true to some extent, but gram-mars induced from the WSJ treebank are notoriousfor over-generation.
In addition, state-of-the-art ac-curacy in context-free parsing is often achieved bysmoothing the grammar, so that rewrites from anyone non-terminal to another are permissible, albeitwith low probability.To empirically evaluate the impact of SWCs onspan-1 chart cells, we parse the development set(section 24) with three different grammars inducedfrom sections 2-21.
Table 2 lists averaged countsof active Viterbi states (derivations with probabil-ity greater than zero) from span-1 cells within thedynamic programming chart, as well as relevantgrammar statistics.
Note that these counts are ex-tracted from exhaustive parsing ?
no pruning hasbeen applied.
We notice two points of interest.First, although |VPOS| > |VPHR|, for the unsmoothedgrammars more phrase-level states are active withinthe span-1 cells than states derived from POS tags.When parsing with the Markov order-2 grammar,70% of active states are non-terminals from VPHR,and with the latent-variable grammar, 67% (152 of227).
This is due to the highly generative natureof SWC productions.
Second, although using asmoothed grammar maximizes the number of activestates, the unsmoothed grammars still provide manypossible derivations per word.Given the infrequent use of SWCs in the treebank,and the search-space explosion incurred by includ-ing them in exhaustive search, it is clear that restrict-ing SWCs in contexts where they are unlikely to oc-cur has the potential for large efficiency gains.
In thenext section, we discuss how to learn such contextsvia a finite-state tagger.6784 Tagging Unary ConstraintsTo automatically predict if word wi from sentencew can be spanned by an SWC production, we train abinary classifier from supervised data using sections2-21 of the Penn WSJ Treebank for training, section00 as heldout, and section 24 as development.
Theclass labels of all words in the training data are ex-tracted from the treebank, where wi ?
U if wi isobserved with a SWC production and wi ?
U other-wise.
We train a log linear model with the averagedperceptron algorithm (Collins, 2002) using unigramword and POS-tag2 features from a five word win-dow.
We also trained models with bi-gram and tri-gram features, but tagging accuracy did not improve.Because the classifier output is imposing hardconstraints on the search space of the parser, wemay want to choose a tagger operating point that fa-vors precision over recall to avoid over-constrainingthe downstream parser.
To compare the tradeoff be-tween possible precision/recall values, we apply thesoftmax activation function to the perceptron outputto obtain the posterior probability of wi ?
U :P (U |wi, ?)
= (1 + exp(?f(wi) ?
?
))?1 (1)where ?
is a vector of model parameters and f(?)
is afeature function.
The threshold 0.5 simply choosesthe most likely class, but to increase precision wecan move this threshold to favor U over U .
To tunethis value on a per-sentence basis, we follow meth-ods similar to Roark & Hollingshead (2009) andrank each word position with respect to its poste-rior probability.
If the total number of words wiwith P (U |wi, ?)
< 0.5 is k, we decrease the thresh-old value from 0.5 until ?k words have been movedfrom class U to U , where ?
is a tuning parameter be-tween 0 and 1.
Although the threshold 0.5 producestagging precision and recall of 98.7% and 99.4%respectively, we can adjust ?
to increase precisionas high as 99.7%, while recall drops to a tolerable82.1%.
Similar methods are used to replicate cell-closing constraints, which are combined with unaryconstraints in the next section.2POS-tags were provided by a separately trained tagger.5 Experiments and ResultsTo evaluate the effectiveness of unary constraints,we apply our technique to four parsers: an exhaus-tive CKY chart parser (Cocke and Schwartz, 1970);the Charniak parser (Charniak, 2000), which usesagenda-based two-level coarse-to-fine pruning; theBerkeley parser (Petrov and Klein, 2007a), a multi-level coarse-to-fine parser; and the BUBS parser(Bodenstab et al, 2011), a single-pass beam-searchparser with a figure-of-merit constituent rankingfunction.
The Berkeley and BUBS parsers bothparse with the Berkeley latent-variable grammar(Petrov and Klein, 2007b), while the Charniakparser uses a lexicalized grammar, and the exhaus-tive CKY algorithm is run with a simple Markovorder-2 grammar.
All grammars are induced fromthe same data: sections 2-21 of the WSJ treebank.Figure 2 contrasts the merit of unary constraintson the three high-accuracy parsers, and several inter-esting comparisons emerge.
First, as recall is tradedfor precision within the tagger, each parser reactsquite differently to the imposed constraints.
We ap-ply constraints to the Berkeley parser during the ini-tial coarse-pass search, which is simply an exhaus-tive CKY search with a coarse grammar.
Applyingunary and cell-closing constraints at this point in thecoarse-to-fine pipeline speeds up the initial coarse-pass significantly, which accounted for almost halfof the total parse time in the Berkeley parser.
In ad-dition, all subsequent fine-pass searches also bene-fit from additional pruning as their search is guidedby the remaining constituents of the previous pass,which is the intersection of standard coarse-to-finepruning and our imposed constraints.We apply constraints to the Charniak parser dur-ing the first-pass agenda-based search.
Because anagenda-based search operates at a constituent levelinstead of a cell/span level, applying unary con-straints alters the search frontier instead of reduc-ing the absolute number of constituents placed in thechart.
We jointly tune lambda and the internal searchparameters of the Charniak parser until accuracy de-grades.Application of constraints to the CKY and BUBSparsers is straightforward as they are both singlepass parsers ?
any constituent violating the con-straints is pruned.
We also note that the CKY and679Figure 2: Development set results applying unary constraintsat multiple values of ?
to three parsers.BUBS parsers both employ the cross-product gram-mar access method discussed in Section 2, whilethe Berkeley parser uses the grammar loop method.This grammar access difference dampens the benefitof unary constraints for the Berkeley parser.3Referring back to Figure 2, we see that both speedand accuracy increase in all but the Berkeley parser.Although it is unusual that pruning leads to higheraccuracy during search, it is not unexpected here asour finite-state tagger makes use of lexical relation-ships that the PCFG does not.
By leveraging thisnew information to constrain the search space, weare indirectly improving the quality of the model.Finally, there is an obvious operating point foreach parser at which the unary constraints are toosevere and accuracy deteriorates rapidly.
For testconditions, we set the tuning parameter ?
based onthe development set results to prune as much of thesearch space as possible before reaching this degra-dation point.Using lambda-values optimized for each parser,we parse the unseen section 23 test data and presentresults in Table 3.
We see that in all cases, unaryconstraints improve the efficiency of parsing withoutsignificant accuracy loss.
As one might expect, ex-haustive CKY parsing benefits the most from unaryconstraints since no other pruning is applied.
Buteven heavily pruned parsers using graph-based andpipelining techniques still see substantial speedups3The Berkeley parser does maintain meta-information aboutwhere non-terminals have been placed in the chart, giving itsome of the advantages of cross-product grammar access.Parser F-score Seconds SpeedupCKY 72.2 1,358+ UC (?=0.2) 72.6 1,125 1.2x+ CC 74.3 380 3.6x+ CC + UC 74.6 249 5.5xBUBS 88.4 586+ UC (?=0.2) 88.5 486 1.2x+ CC 88.7 349 1.7x+ CC + UC 88.7 283 2.1xCharniak 89.7 1,116+ UC (?=0.2) 89.7 900 1.2x+ CC 89.7 716 1.6x+ CC + UC 89.6 679 1.6xBerkeley 90.2 564+ UC (?=0.4) 90.1 495 1.1x+ CC 90.2 320 1.8x+ CC + UC 90.2 289 2.0xTable 3: Test set results applying unary constraints (UC) andcell-closing (CC) constraints (Roark and Hollingshead, 2008)to various parsers.with the additional application of unary constraints.Furthermore, unary constraints consistently providean additive efficiency gain when combined with cell-closing constraints.6 ConclusionWe have presented a new method to constraincontext-free chart parsing and have shown it to be or-thogonal to many forms of graph-based and pipelinepruning methods.
In addition, our method parallelsthe cell closing paradigm and is an elegant com-plement to recent work, providing a finite-state tag-ging framework to potentially constrain all areas ofthe search space ?
both multi-word and single-wordconstituents.AcknowledgmentsWe would like to thank Aaron Dunlop for his valu-able discussions, as well as the anonymous review-ers who gave very helpful feedback.
This researchwas supported in part by NSF Grants #IIS-0447214,#IIS-0811745 and DARPA grant #HR0011-09-1-0041.
Any opinions, findings, conclusions or recom-mendations expressed in this publication are those ofthe authors and do not necessarily reflect the viewsof the NSF or DARPA.680ReferencesNathan Bodenstab, Aaron Dunlop, Keith Hall, and BrianRoark.
2011.
Beam-width prediction for efficientcontext-free parsing.
In Proceedings of the 49th An-nual Meeting of the Association for ComputationalLinguistics, Portland, Oregon.
Association for Com-putational Linguistics.Eugene Charniak.
1997.
Statistical parsing with acontext-free grammar and word statistics.
In Proceed-ings of the Fourteenth National Conference on Arti-ficial Intelligence, pages 598?603, Menlo Park, CA.AAAI Press/MIT Press.Eugene Charniak.
2000.
A maximum-entropy-inspiredparser.
In Proceedings of the 1st North Americanchapter of the Association for Computational Linguis-tics conference, pages 132?139, Seattle, Washington.Morgan Kaufmann Publishers Inc.John Cocke and Jacob T. Schwartz.
1970.
Programminglanguages and their compilers.
Technical report Pre-liminary notes, Courant Institute of Mathematical Sci-ences, NYU.Michael Collins.
1997.
Three generative, lexicalisedmodels for statistical parsing.
In Proceedings of theeighth conference on European chapter of the Associ-ation for Computational Linguistics, page 1623, Mor-ristown, NJ, USA.
Association for Computational Lin-guistics.Michael Collins.
2002.
Discriminative training meth-ods for hidden Markov models: theory and experi-ments with perceptron algorithms.
In Proceedingsof the ACL-02 conference on Empirical Methods inNatural Language Processing, volume 10, pages 1?8, Philadelphia, July.
Association for ComputationalLinguistics.Dan Klein and Christopher D. Manning.
2001.
Parsingwith treebank grammars: Empirical bounds, theoret-ical models, and the structure of the Penn treebank.In Proceedings of 39th Annual Meeting of the Associ-ation for Computational Linguistics, pages 338?345,Toulouse, France, July.
Association for ComputationalLinguistics.Mitchell P Marcus, Beatrice Santorini, Mary AnnMarcinkiewicz, and Ann Taylor.
1999.
Treebank-3.Linguistic Data Consortium, Philadelphia.Slav Petrov and Dan Klein.
2007a.
Improved inferencefor unlexicalized parsing.
In Human Language Tech-nologies 2007: The Conference of the North Ameri-can Chapter of the Association for Computational Lin-guistics; Proceedings of the Main Conference, pages404?411, Rochester, New York, April.
Association forComputational Linguistics.Slav Petrov and Dan Klein.
2007b.
Learning and in-ference for hierarchically split PCFGs.
In AAAI 2007(Nectar Track).Brian Roark and Kristy Hollingshead.
2008.
Classify-ing chart cells for quadratic complexity context-freeinference.
In Donia Scott and Hans Uszkoreit, ed-itors, Proceedings of the 22nd International Confer-ence on Computational Linguistics (COLING 2008),pages 745?752, Manchester, UK, August.
Associationfor Computational Linguistics.Brian Roark and Kristy Hollingshead.
2009.
Linearcomplexity context-free parsing pipelines via chartconstraints.
In Proceedings of Human Language Tech-nologies: The 2009 Annual Conference of the NorthAmerican Chapter of the Association for Computa-tional Linguistics, pages 647?655, Boulder, Colorado,June.
Association for Computational Linguistics.Brian Roark and Richard W Sproat.
2007.
Computa-tional Approaches to Morphology and Syntax.
OxfordUniversity Press, New York.Yue Zhang, Byung gyu Ahn, Stephen Clark, Curt VanWyk, James R. Curran, and Laura Rimell.
2010.Chart pruning for fast lexicalised-grammar parsing.
InProceedings of the 23rd International Conference onComputational Linguistics, pages 1472?1479, Beijing,China, June.681
