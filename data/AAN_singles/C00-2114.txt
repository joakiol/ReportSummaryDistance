A Dynamic Language ModelBased on Individual Word DomainsE.
I. Sicilia-Garcia, Ji Ming, F. J. SmithSchool Computer ScienceQueen's University of BelfastBelfast BT7 INN, Northern Irelande.sicilia@qub.ac.ukAbstractWe present a new statistical languagemodel based on a Colnbination ofindividual word language models.
Eachword model is built from an individualcorpus which is formed by extractingthose subsets of the entire training corpuswhich contain that significant word.
Wealso present a novel way of combininglanguage models called the "unionmodel", based on a logical union ofintersections, and use this to combine thelanguage models obtained for thesignificant words from a cache.
Theinitial results with the new model providea 20% reduction in language modelperplexity over the standard 3-gramapproach.IntroductionStatistical language models are based oninformation obtained fiom the analysis of largesamples of a target language.
Such modelsestimate the conditional probability of a wordgiven a sequence of preceding words.
Theconditional probability can be further used todetermine the likelihood of a sentence throughlhe product of the individual word probabilities.A popular type of statistical language model islhe dynamic language model, which dynamicallymodifies conditional probabilities depending onthe recent word history.
For example the cached-based natural anguage models (Kuhn R. & DeMori R., 1990) incorporates a cache componentinto the model, which estimates the probabilityof a word depending upon its recent usage.Trigger based models go a step further bytriggering associated words to each content wordin a cache giving each associated word a higherprobability (Lau et al, 1993).Our statistical language model, basedupon individual word domains, extends theseideas by creating a new language model for eachsignificant word in the cache.
A significant wordis hard to define; it is any word that significantlycontributes to the content of the text.
We defineit as any word which is not a stop word, i. e.articles, prepositions and some of the mostfiequcntly used words in the language such as"will", "now", "very", etc.
Our model combinesindividual word language models with a standardglobal n-gram language model.A training corpus for each significantword is formed from the amalgamation of thetext fiagments taken fiom the global trainingcorpus in which that word appears.
As such thesecorpora are smaller and closely constrained;hence the individual anguage models are moreprecise than the global language model andthereby should offer performance gains.
Oneaspect of the performance of this joint model ishow the global language model is to becombined with the individual word languagemodels.
This is explored later.This paper is organised as follows.Section 1 explains the basis for this model.
Themathematical background and how the modelsare combined are explained in section 2.
In thethird section, a novel method of combining theword models, the probabilistic-union model isexplained.
Finally, results and conclusion aredrawn.7891 Dynamic Language Model based onWord Language ModelsOur dynamic language model builds alanguage model for each individual word.
Inorder to do this we need to select which wordsare to be classified as significant and furthermorecreate a language model for them.
We excludedall the stop words ('is', 'to', 'all', 'some') due totheir high frequency within tile text and theirlimited contribution to the thematic of the text.
Alist of stop words was obtained by mergingtogether the lists used by various www searchengines, for example Altavista.Secondly we need to create a dictionarythat contains the frequency of each word ill thecorpus.
This is needed because we want toexclude those non-stop words which appear toooften in the training corpus, for example wordslike 'dollars', 'point', etc.
A hash file isconstructed to store large amounts of informationso that it can be retrieved quickly.The next step is to create tile globallanguage model by obtaining the text phrases andtheir probabilities.
Frequencies of words andphrases are derived fiom a large text corpus andthe conditional probability of a word given asequence of preceding words is estimated.
Theseconditional probabilities are combined toproduce an overall language model probabilityfor any given word sequence.
The probability ofa sequence of words is:P(w 1 "" w,, ) = P(w\[' ) =- -P (w l )X  P(w2 I w l )x""  P(w" \[ w~'-l) = (1)i=Iwhere w\[' ={wl,w2,w3,...,w,, } is a sentence orsequence of words.
The individual conditionalprobabilities are approximated by the maximumlikelihoods:PML(W~ Iw l -b  --  f req(w\ [ )  _ f , ' eq(w,  .
.
.
w~_,wpfreq(wl q) freq(wl "" ~h-l) (2)where freq(X)is the frequency of thephrase X in the text.In equation (2), there are often unknownsequences of words i.e.
phrases which are not inthe dictionary.
The maximum likelihoodprobability is then zero.
In order to improve thisprediction of all unseen event, and hence thelanguage model, a number of techniques havebeen explored, for example, the Good-Turingestimate (Good I. J., 1953), the backing-offmethod (Katz S. M., 1987), deleted interpolation(Jelinek F. and Mercer R. L., 1984) or theweighted average n-gram model (O'Boyle P.,Owens M. and Smith F. J., 1994).
We use theweighted average n-gram technique (WA), whichcombines n-grain ~ phrase distributions of severalorders using a series of weighting fnnctions.
TheWA n-gram model has been shown to exhibitsimilar predictive powers to other n-gramtechniques whilst enjoying several benefits.Firstly an algorithm for a WA model is relativelystraightforward to implement ill computersoftware, secondly it is a variable n-gram modelwith the length depending on the context andfinally it facilitates easy model extension 2.
Theweighted average probability of a woM given thepreceding words is,,tj,,,, (w) + ~ & l,,,,.
(,, I ,',,,, ,...,,',.,)&,,, (,,, I ,,,, ..-w,,, ) - '-' (3)i 0where the weighted funct ions  are:,,t o = Ln( N),  (4)-- 2N is tile number of tokens ill tile corpus andfreq(wm+l_i...w,,~) is the frequency of tilesenteuce Win+l_  i " ' "  W m in the text.The maximum likelihood probability of a wordis:J A n-gram model contains the conditional probabilityof a word dependant on the previous i1 words.
(JclinekF., Mercer R.L.
and Bahl L. R., 1983)2 Tile "ease of extension" applies to the fact thatadditional training data can be incorporated into anexisting WA model without the need to re-estimatesmoothing parameters.790P,,,,.(,,,)--?
?q (')  (5)NJ?eq(w) is the frequency of the word w in thetext.
This language model (defined by equation(3) and (5)) is what we term a standard n-gramlanguage model or global language model.Finally the last step is the creation of ahmguage model for each significant word, whichis formed in the same manner as the globallanguage model.
The word language-trainingcorpus to be used is tlle amalgamation of the textfiagments taken from the global training corpusin which the significant word appears.
A numberof choices can be made as to how the word-training corpus for each significant word can beselected.
We initially construct what we termedthe "paragraph context model", entailing that theglobal training corpus is scanned for a particularword and each time the word is found theparagraph containing that word is extracted.
Theparagraphs of text extracted for a particular wordare joined together to form an individual word-training corpus, from which an individual wordlanguage model is built.
Alternative methodsinclude storing only the sentences where theword appears or extracting a piece of the text M-words before and M + words after the searchword .Additionally some restrictions on thenumber of words were imposed.
This was donedue to the high frequency of certain words.
Suchwords were omitted since the additionalinformation that they provide is minimal(conversely language models for "rare" wordsare desirable as they provide significantadditional iuformation to that contained withinthe global language model).
Once individuallanguage models have been formed for eachsignificaut word (trained using the standard n-grain approach as used for the global lnodel),the.m remains the problem of how the individualword language models will be combined togetherwith the global language model.2 Combining the ModelsWe need to combine the probabilitiesobtained from each word language model andfiom the global language model, in order toobtain a conditional probability for a word givena sequence of words.
The first model to be testedis an arithmetic combination of the globalhmguage model and the word language models.All the word hmguage models and the globallanguage model are weighted equally.
Webelieve that words, which appear far away in theprevious word history, do not have as nmchimportance as the ones closest to the word.Therefore we need to lnake a restriction in thenumber of language models.
First, theconditional probabilities obtained from the wordhmguage models and the global languagemodel can be combined in a linearinterpolated model as follows:mP(w Iw") = ;co e?,,o,.., (w Iw;') + Z & v, (w I w',' )i-~ (6)I l lwhere 2 c + .y_, 2 i = 1 (7)i Iand l ' (wl , , i ' ) is  the conditional probability inthe word language model for the significant wordw i, 2 iare the correspondent weights and m isthe maxinmm number of word models that weare including.If the same weight is given to all theword language models but not to the globallanguage model and if a restriction on timlmmber of word language models to be includedis enforced, the weighted model is defined as:and ~ is a parameter which is chosen to optimisethe model.Furthermore, a method was used basedon all exponential decay of the word modelprobabilities with distance.
This stands to reason,as a word appearing several words previouslywill generally be less relevant han more recent791words.
Given a sequence of words, for example,"We had happy times in America..."We ltad Happy Times In America5 4 3 2 1where 5, 4, 3, 2, 1 represent the distance of theword from the word America, Happy and Timesare significant words for which we have anindividual word language models.
Theexponential decay model for the word w, wherein this case w represents the significant wordAmerica, is as follows:/ .
.
.
.
.
.
/ I {;iot,,,t( w\[ w I ) + P.,.,py (w I wl )' exp(-3/d)P(wlw, )=\[  + 1,,,,,,,,(~, \]w, ).exp(21d) ) (9)l + exp(-3/d) + exp(-2/d)where Patot,,t(wl (') is the conditionalprobability of the word w following a phrasewl "" w,, in the global language model.Pmppy(Wl w~') is the conditional probability of theword w following a phrase w 1. .
.%wordlanguage model for the significant word Happy.The same defnition applies for the word modelTimes.
d is the exponential decay distance withd=5, 10, 15,etc.
The decaying factor exp(-I/d)introduces a cut off:if l>d ~ exp(-l/d)=Owhere l is the word modelto word distanced is the decay distancePresently the combination methodsoutlined above have been experimentallyexplored.
However, they offer a reasonablysimplistic means of combining the individual andglobal language models.
More sophisticatedmodels are likely to offer improved performancegains.3 The Probabilistic-Union ModelThe next method is the Probabilistic-Union model.
This model is based on the logicalconcept of a disjunction of conjunction which isimplemented as a sum of products.
The unionmodel has been previously applied in problemsof noisy speech recognition, (Ming J. et al,1999).
Noisy conditions during speechrecognition can have a serious effect on thelikelihood of some features which are normallycombined using the geometric mean.
This noisehas a zeroing effect upon the overall likelihoodproduced for that particular speech frame.
Theuse of the probabilistic-union reduces the overalleffect that each feature has in the colnbination,therefore loosening any zeroing effect.For the word language model, some of theconditional probabilities are zero or very smalldue to the small size of some of the word modelcorpora.
For these word models, many of thewords in the global training corpus are not in theword-model training-corpus dictionary.
And so,the conditional probability will be in many caseszero or near zero reducing the overallprobability.
As in noisy speech @cognition wewish to reduce the effect of this zeroing in thecombined model.
The probabilistic-union modelis one of the possible solutions for the zeroingproblem when combining language models.The union model is best illustrated withan example when the number of word models tobe included is m=4 and if they are assumed to beindependent probabilities.
(') , w,(e, ( lo)  &,,i,,,, (" ) = /2" 8-%9;P= .
@v, p2 @ Pu,,io,, (w)  P3 P4 "" )  (1 1)/} (3 )  z .
u,,io,,tw)-- W3(I:~P2 @ P,P~ ?
PiP4 @'") (12)v(4) , .
't'4(P, ?
? )
u,,io,, tW) = \]}2 P'~ @ P4 (13)where P'~,io,,(w) =P~U,,io,, (wlw;') is the nnionmodel of order k. P/ = P,.
(w\[ w(') is theconditional probability for the significant wordw i and ~ is a normalizing constant.
Thesymbol '?'
is a probabilistic sum, i.e.
itsequivalent for 1 and 2 is:8,,,,d2 =8 ?
",2 --e, +/'2 -8P2 (14)Tile combination of the global languagemodel with the probabilistic-union model is792defined as follows:p(w\[ w~') = ~J~,o~,,,~(wl ,,\[')+(l-a)/}j,,,,,,(w \[ i') (15)ResultsTo evaluate the behaviour of one language modelwith respect to others we use perplexity.
Itmeasures the average branching factor (perword) of the sequence at every new word, withrespect to some source model.
The lower thebranching l'actor, the lower the model errors rate.Therefore, the lower the branching (l~erplexity)the better the model.
Let w i be a word in thelanguage model and w\[" = {wl, w 2, w3,..-, w,,, }a sentence or sequence of words.
The perplexityof this sequence of words is:Peq) lex i ty (  w 1 w2 ...  w,, ) = PP(w~'  ) =t 1 " ._ \]= Z ('", I ,,'i' ))J I t  i=1(J6)The Wall Street Journal (versionWSJ03) contains about 38 million words, and adictionary of approxilnately 65,000 words.
Weselect one quarter of the articles in the globaltraining corpus as our training corpus (since theglobal training corpus is large and thenormalisation process takes time).
To test thenew language model we use a subset of the testfile given by WSJ0, selected at random.
Thetraining corpus that we are using contains172,796 paragraphs, 376,589 sentences,9526,187 tokens.
The test file contains 150paragraphs, 486 sentences, 8824 tokens and 1908words types.
Although the size of this test file issmall, limher experilnents with bigger trainingcorpora and test files are planned.Although in our first experiments we use5--grams in the calculation of the word models,the size of the n-gram has been reduced to 3-grains because the process of norlnalisation isslow in these experiments.The model based on a simple weightedcolnbination offers ilnproved results, up to 10%when o~=0.6 in Eq.
(8) and a combination of amaxilnuln of 10 word lnodels.
Better esults werefound when the word models were weighteddepending on their distance from the currentword, that is, for the exponential decay model inEq.
(9) where d=7 and the number of wordmodels is selected by the exponential cut off(Table 1 ).
For this model ilnprovelnents of over17% have been found.F ~  Decay d.,11 5 I 6 I 7 I4d 15.53% 16.31% 16.46% 16.44%5d 15.90% 16.42% 16.52% 16.43%6d 15.92% 16.45% 16.53% 16.41%7d 16.02% 16,4~% 16.51% 16.40%8d 16.1)2% 16.46% 16.51% 16.39%9(1 15.97% 16.45% 16.51% 16.39%Table 1.
Improvement in perplexity for theexponetial decay models with respect o the GlobalLanguage Model over the basic 3-gram model.For tile probabilistic-union model, wehave as many nlodels as numbers of wordlanguage nlodels.
For example, if we wish toinclude m=4 word language Jnodels, tile fourunion models are those with orders I to 4(equation (13) to (15)).
The results for theprobabilistic union model when the number ofwords models is m=5 and m=6 are shown in thetables below.Union Model Order5 I 4 I 3 I 2 I 10.3 13% 15% -2% -15% -25%0.4 13% 18% 6% -3% -10%0.5 12% 19% I 1% 4% -I%0.6 12% 19% 13% 9% 5%0.7 11% 18% 14% 11% 8%0.8 9% 15% 13% 11% 9%0.9 6% 10% 9% 8% 8%3 CSR-I(WSJ0) Sennheiser, published by LDC ,ISBN: 1-58563-007-I793t t U, fiox%odel Order t t0.3 13% 15% -2% -13% -22% -30%0.4 13% 18% 6% -2% -8% -13%0.5 13% 20% 11% 5% 1% -3%0.6 12% 20% 14% 9% 6% 3%0.7 11% 18% 14% 11% 9% 7%0.8 9% 16% 13% I I% 10% 9%0.9 6% 11% 10% 9% 8% 7%Table 2.
Improvement in perplexity of theProbabilistie-Union Model with respect to theGlobal Language Model over the basic 3-grammodel.The best result obtained so far, is animprovement of 20% when a maximum ot' 6 wordmodels and the order is 5, i.e.
sums of theproducts of pairs (Table 2).The value of alpha is0.6.ConclusionIn this paper we have introduced theconcept of individual word language models toimprove language model performance.Individual word language models permit anaccurate capture of the domains in whichsignificant words occur and hence improve thelanguage model performance.
We also describe anew method of combining models called theprobabilistic union model, which has yet to befully explored but the first results show goodperformance.
Even though the results arepreliminary, they indicate that individual wordmodels combined with the union model offer apromising means of reducing the perplexity.Weighted Eq.
(8) 10%Exponential Decay Eq.
(9) 17%Union Model 5 words 19%Union Model 6 words 20%Union Model 7 words 19%Table 3. hnprovement in perplexity for differentcombinations of word models.AcknowledgementsOur thanks go to Dr. Phil Halma for hiscollaboration i  this research.ReferencesGood I. J.
(1953) "The Population Frequencies ot'Species and the Estimation of PopulationParameters".
Biometrika, Vol.
40, pp.237-254.Jelinek F., Mercer R. L. and Bahl L. R. (1983) "AMaximum Likelihood Approach to ContinuousSpeech Recognition".
IEEE Transactions onPattern Analysis and Machine Intelligence.
Vol.
5,pp.
179-190.Jelinek F. and Mercer R. L. (1984) "Interpolatedestimation of Markov Source Parameters fromSparse Data".
Pattern Recognition in Practice.Gelsema E., Kanal L. eds.
Amsterdam: Norlh-Holland Publishing Co.Katz S. M. (1987) "Estimation of Probabilities fromSparse Data for the Language Model Component ofa Speech Recogniser".
IEEE Transactions OnAcoustic Speech and Signal Processing.
Vol.
35(3),pp.
400-401.Kuhn R. and De Mori R. (1990) "A Cache-BasedNatural Language Model for Speech Recognition".IEEE Transactions on Pattern Analysis andMachine Intelligence.
Vol.
12 (6), pp.
570-583.Lau R., Rosenfeld R., Roukos S. (1993).
"Trigger-based Language models: A Maximum entropyapproach".
IEEE ICASSP 93 Vo12, pp 45-48,Minneapolis, MN, U.S.A., April.Ming J., Stewart D., Hanna P. and Smith F. J.
(1999)"A probabilistic Union Model Jbr Partial andtemporal Corruption ql~ Speech ''.
Automatic SpeechRecognition and Understanding Workshop.Keystone, Colorado, U. S. A., December.O'Boyle P., Owens M. and Smith F. J.
(1994)"Average n-gram Model of Natural Language".Computer Speech and Language.
Vol.
8 pp 337-349.794
