Proceedings of the 3rd Workshop on Computational Approaches to Subjectivity and Sentiment Analysis, pages 61?69,Jeju, Republic of Korea, 12 July 2012. c?2012 Association for Computational LinguisticsUnifying Local and Global Agreement and Disagreement Classificationin Online DebatesJie YinCSIRO ICT CentreNSW, Australiajie.yin@csiro.auNalin NarangUniversity of New South WalesNSW, Australianalinnarang@gmail.comPaul ThomasCSIRO ICT CentreACT, Australiapaul.thomas@csiro.auCecile ParisCSIRO ICT CentreNSW, Australiacecile.paris@csiro.auAbstractOnline debate forums provide a powerfulcommunication platform for individual usersto share information, exchange ideas and ex-press opinions on a variety of topics.
Under-standing people?s opinions in such forums isan important task as its results can be usedin many ways.
It is, however, a challeng-ing task because of the informal language useand the dynamic nature of online conversa-tions.
In this paper, we propose a new methodfor identifying participants?
agreement or dis-agreement on an issue by exploiting infor-mation contained in each of the posts.
Ourproposed method first regards each post inits local context, then aggregates posts to es-timate a participant?s overall position.
Wehave explored the use of sentiment, emotionaland durational features to improve the accu-racy of automatic agreement and disagree-ment classification.
Our experimental resultshave shown that aggregating local positionsover posts yields better performance than non-aggregation baselines when identifying users?global positions on an issue.1 IntroductionWith their increasing popularity, social media appli-cations provide a powerful communication channelfor individuals to share information, exchange ideasand express their opinions on a wide variety of top-ics.
An online debate is an open forum where aparticipant starts a discussion by posting his opin-ion on a particular topic, such as regional politics,health or the military, while other participants statetheir support or opposition by posting their opinions.Understanding participants?
opinions in online de-bates has become an increasingly important task asits results can be used in many ways.
For example,by analysing customers?
online discussions, compa-nies can better understand customers?
reviews abouttheir products or services.
For government agencies,it could help gather public opinions about policies,legislation, laws, or elections.
For social science, itcan assist scientists to understand a breadth of socialphenomena from online observations of large num-bers of individuals.Despite the potentially wide range of applications,understanding participants?
positions in online de-bates remains a difficult task.
One reason is thatonline conversations are very dynamic in nature.Unlike spoken conversations (Thomas et al, 2006;Wang et al, 2011), users in online debates are notguaranteed to participate in a discussion at all times.They may enter or exit the online discussion at anypoint, so it is not appropriate to use models assumingcontinued conversation.
In addition, most discus-sions in online debates are essentially dialogic; par-ticipants could choose to implicitly respond to a pre-vious post, or explicitly quote some content from anearlier post and make a response.
Therefore, an as-sumption has to be made about what a participant?spost is in response to, particularly when an explicitquote is not present; in most cases, a post is assumedto be in response to the most recent post in the thread(Murakami and Raymond, 2010).In this paper, we address the problem of detectingusers?
positions with respect to the main topic in on-line debates; we call this the global position of userson an issue.
It is inappropriate to identify each user?sglobal position with respect to a main topic directly,because most expressions of opinion are made not61for the main topic but for posts in a local context.This poses a difficulty in directly building a globalclassifier for agreement and disagreement.
We illus-trate this with the example below.
Here, the topic ofthe thread is ?Beijing starts gating, locking migrantvillages?
and the discussion is started with a seedpost criticising the Chinese government1.Seed post: I?m most sure there will be someChina sympathisers here justifying these ac-tions imposed by the Communist Chinese gov-ernment.
.
.
.Reply 1: Not really seeing a problem there.From you article.
They can come and go.
Peo-ple in my country pay hundreds of thousandsof pounds for security like that in their gatedcommunities..Reply 2: So, you are OK with living in a PoliceState?
.
.
.The author of Reply 1 argues that the Chinese pol-icy is not as presented, and is in fact defensible.
Thisopposes the seed post, so that the author?s global po-sition for the main topic is ?disagree?.
The opin-ion expressed in Reply 2, however, is not a responseto the seed post: it relates to Reply 1.
It indicatesthat the author of Reply 2 disagrees with the opinionmade in Reply 1, and thus indirectly implies agree-ment with the seed post.
From this example, we cansee that it is hard to infer the global position of Re-ply 2?s author only from the text of their post.
How-ever, we can exploit information in the local context,such as the relationship between Replies 1 and 2, toindirectly infer the author?s opinion with regard tothe seed post.Motivated by this observation, we propose athree-step method for detecting participants?
globalagreement or disagreement positions by exploitinglocal information in the posts within the debate.First, we build a local classifier to determine whethera pair of posts agree with each other or not.
Sec-ond, we aggregate over posts for each pair of partic-ipants in one discussion to determine whether theyagree with each other.
Third, we infer the global po-sitions of participants with respect to the main topic,so that participants can be classified into two classes:1Spelling of the posts is per original on the website.agree and disagree.
The advantage of our proposedmethod is that it builds a unified framework whichenables the classification of participants?
local andglobal positions in online debates; the aggregationof local estimates also tends to reduce error in theglobal classification.In order to evaluate the performance of ourmethod, we have conducted experiments on data setscollected from two online debate forums.
We haveexplored the use of sentiment, emotional and du-rational features for automatic agreement and dis-agreement classification, and our feature analysissuggests that they can significantly improve the per-formance of baselines using only word features.
Ex-perimental results have also demonstrated that ag-gregating local positions over posts yields better per-formance for identifying users?
global positions onan issue.The rest of the paper is organised as follows.
Sec-tion 2 discusses previous work on agreement anddisagreement classification.
Section 3 presents ourproposed method for both local and global positionclassification, which we validate in Section 4 withexperiments on two real-world data sets.
Section 5concludes the paper and discusses possible direc-tions for future work.2 Related WorkPrevious work in automatic identification of agree-ment and disagreement has mainly focused onanalysing conversational speech.
Thomas et al(2006) presented a method based on support vectormachines to determine whether the speeches madeby participants represent support or opposition toproposed legislation, using transcripts of U.S. con-gressional floor debates.
This method showed thatthe classification of participants?
positions can beimproved by introducing the constraint that a sin-gle speaker retains the same position during onedebate.
Wang et al (2011) presented a condi-tional random field based approach for detectingagreement/disagreement between speakers in En-glish broadcast conversations.
Galley et al (2004)proposed the use of Bayesian networks to modelpragmatic dependencies of previous agreement ordisagreement on the current utterance.
These differfrom our work in that the speakers are assumed to62be present all the time during the conversation, andtherefore, user speech models can be built, and theirdependencies can be explored to facilitate agreementand disagreement classification.
Our aggregationtechnique does, however, presuppose consistency ofopinions, in a similar way to Thomas et al (2006).There has been other related work which aimsto analyse informal texts for opinion mining and(dis)agreement classification in online discussions.Agrawal et al (2003) described an observation thatreply-to activities always show disagreement withprevious authors in newsgroup discussions, and pre-sented a clustering approach to group users into twoparties: support and opposition, based on reply-to graphs between users.
Murakami and Raymond(2010) proposed a method for deriving simple rulesto extract opinion expressions from the content ofposts and then applied a similar graph clustering al-gorithm for partitioning participants into supportingand opposing parties.
By combining both text andlink information, this approach was demonstrated tooutperform the method proposed by Agrawal et al(2003).
Due to the nature of clustering mechanisms,the output of these methods are two user parties, ineach of which users most agree or disagree with eachother.
However, users?
positions in the two partiesdo not necessarily correspond to the global positionwith respect to the main issue in a debate, whichis our interest here.
Balasubramanyan and Cohen(2011) proposed a computational method to classifysentiment polarity in blog comments and predict thepolarity based on the topics discussed in a blog post.Finally, Somasundaran and Wiebe (2010) exploredthe utility of sentiment and arguing opinions in ideo-logical debates and applied a support vector machinebased approach for classifying stances of individualposts.
In our work, we focus on classifying people?sglobal positions on a main issue by exploiting andaggregating local positions expressed in individualposts.3 Our Proposed MethodTo infer support or opposition positions with respectto the seed post, we propose a three-step method.First, we consider each post in its local context andbuild a local classifier to classify each pair of postsas agreeing with each other or not.
Second, we ag-gregate over posts for each pair of participants inone discussion to determine whether they agree witheach other.
Third, we infer global positions of par-ticipants with respect to the seed post based on thethread structure.3.1 Classifying Local Positions between PostsTo classify local positions between posts, we need toextract the reply-to pairs of posts from the threadingstructure.
The web forums we work with tend not topresent thread structure, so we consider two typesof reply-to relationships between individual posts.When a post explicitly quotes the content from anearlier post, we create an explicit link between thepost and the quoted post.
When a post does notcontain a quote, we assume that it is a reply to thepreceding post, and thus create an implicit link be-tween the two adjacent posts.
After obtaining ex-plicit/implicit links, we build a classifier to classifyeach pair of posts as agreeing or disagreeing witheach other.3.1.1 FeaturesTo build a classifier for identifying local agree-ment and disagreement, we explored different typesof features from individual posts with the aim to un-derstand which have predictive power for our agree-ment/disagreement classification task.Words We extract unigram and bigram featuresto capture the lexical information from each post.Since many words are topic related and might beused by both parties in a debate, we mainly use un-igrams for adjectives, verbs and adverbs becausethey have been demonstrated to possess discrimi-native power for sentiment classification (Benamaraet al, 2007; Subrahmanian and Regorgiato, 2008).Typical examples of such unigrams include ?agree?,?glad?, ?indeed?, and ?wrong?.
In addition, we ex-tract bigrams to capture phrases expressing argu-ments, for example, ?don?t think?
and ?how odd?could indicate disagreement, while ?I concur?
couldindicate agreement.Sentiment features In order to detect sentimentopinions, we use a sentiment lexicon referred to asSentiWordNet (Baccianella et al, 2010).
This lexi-con assigns a positive and negative score to a largenumber of words in WordNet.
For example, the63ABDCCBseed(a) Estimate P (y|x) for eachpostABDCL(B,C)=disagreeL(A,B)=agreeL(A,C)=disagreeL(C,D)=disagree(b) Aggregate these overpairs of users to get localagreement L(m,n)ABDCagree disagreeagree(c) Infer the global positionof each user by walking thetreeFigure 1: Local agreement/disagreement and participants?
global positions.
We first estimate P (y|xi, xj), the prob-ability of two posts xi and xj being in agreement or disagreement with each other, then aggregate over posts todetermine L(m,n), the position between two users.
Finally, we infer the global position for any user by walking thisgraph back to the seed.word ?odd?
has a positive score of 1.125, and a neg-ative score of 1.625.
To aggregate the sentimentpolarity of each post, we calculate the overall pos-itive and negative scores for all the words that canbe found in SentiWordNet, and use these two sumsas two features for each post.Emotional features We observe that personalemotions could be a good indicator of agree-ment/disagreement expression in online debates.Therefore, we include a set of emotional features,including occurrences of emoticons, number of cap-ital letters, number of foul words, number of excla-mation marks, and number of question marks con-tained in a post.
Intuitively, use of foul words mightbe linked to emotion in a visceral way, which if used,could be a sign of strong argument and disagree-ment.
The presence of question marks could be in-dicative of disagreement, and the use of exclama-tion marks and capital letters could be an emphasisplaced on opinions.Durational features Inspired by conversationanalysis (Galley et al, 2004; Wang et al, 2011), weextract durational features, such as the length of apost in words and in characters.
These features areanalogous to the ones used to capture the duration ofa speech for conversation analysis.
Intuitively, peo-ple tend to respond with a short post if they agreewith a previous opinion.
Otherwise, when there is astrong argument, people tend to use a longer post tostate and defend their own opinions.
Moreover, wealso consider the time difference between adjacentposts as additional features.
Presumably, when a de-bate is controversial, participants would be activelyinvolved in the discussions, and the thread would un-fold quickly over time.
Thus, the time difference be-tween adjacent posts would be smaller in the debate.3.1.2 Classification ModelWe use logistic regression as the basic classi-fier for local position classification because it hasbeen demonstrated to provide good predictive per-formance across a range of text classification tasks,such as document classification and sentiment anal-ysis (Zhang and Oles, 2001; Pan et al, 2010).
In ad-dition to the predicted class, logistic regression canalso generate probabilities of class memberships,64which are quite useful in our case for aggregatinglocal positions between participants.Formally, logistic regression estimates the condi-tional probability of y given x in the form ofPw(y = ?1|x) =11 + e?ywTx, (1)where x is the feature vector, y is the class label, andw ?
Rn is the weight vector.
Given the training data{xi, yi}li=1, xi ?
Rn, yi ?
{1,?1}, we consider thefollowing form of regularised logistic regressionminwf(w) =12wTw + Cl?i=1log(1 + e?yiwTxi),(2)which aims to minimise the regularised negative log-likelihood of the training data.
Above, wTw/2 isused as a regularisation term to achieve good gen-eralisation abilities.
Parameter C > 0 is a penaltyfactor which controls the balance of the two termsin Equation 2.
The above optimisation problem canbe solved using different iterative methods, such asconjugate gradient and Newton methods (Lin et al,2008).
As a result, an optimal estimate of w can beobtained.Given a representation of a post xm, we can useEquation 1 to estimate its membership probabil-ity of belonging to each class, P (agree|xm) andP (disagree|xm), respectively.3.2 Estimating Local Positions betweenParticipantsAfter obtaining local position between posts, thisstep aims to aggregate over posts to determinewhether each pair of participants agree with eachother.
The intuition is that, in one threaded dis-cussion, most of the participants tend to retain theirpositions in the course of their arguments.
This as-sumption holds for the ground-truth annotations wehave obtained in our data sets.
Given local predic-tions obtained from the previous step, we adopt theweighted voting scheme to determine the local posi-tion for each pair of participants.
Specifically, givena pair of users i and j, we aggregate over all thereply-to posts between them to calculate the overallagreement score r(i, j) as follows:r(i, j) =N(i,j)?k=1P (agree|xk)?N(i,j)?k=1P (disagree|xk).
(3)Here, N(i, j) denotes the number of post exchangesbetween users i and j, and r(i, j) indicates the de-gree of agreement between users i and j.
Let L(i, j)denote the local position between two users i andj.
If r(i, j) > 0, we have L(i, j) = agree, that is,user i agrees with user j.
Otherwise, if r(i, j) ?
0,we have L(i, j) = disagree, that is, user i disagreeswith user j.Let us consider the example in Figure 1(a) and1(b).
There are two posts exchanged between usersB and C. For each of these posts, two probabilitiesof class membership can be obtained:P (agree|x1) = 0.1, P (disagree|x1) = 0.9,P (agree|x2) = 0.3, P (disagree|x2) = 0.7.Then we can calculate the agreement score r(B,C)between users B and C by aggregating over twoposts, that is, r(B,C) = (0.1+0.3)?
(0.9+0.7) =?1.2 < 0.
We can conclude that user B dis-agrees with user C in the threaded discussion andthat L(B,C) = disagree.3.3 Identifying Participants?
Global PositionsAfter estimating local positions between partici-pants, we now can infer a participant?s global sup-port or opposition position with regards to the seedpost.
For this purpose, a thread structure must beconsidered.
A thread begins with a seed post, whichis further followed by other response posts.
Of theseresponses, many employ a quote mechanism to ex-plicitly state which post they reply to, whereas oth-ers are assumed to be in response to the most recentpost in the thread.
We construct a tree-like threadstructure by examining all the posts in a thread anddetermining the parent of each post.
Then, travers-ing through the thread structure from top to bottomallows us to infer the global position of each userwith respect to the seed post.
When there is morethan one path from the seed to a user, the shortestpath is used to infer the user?s global position on themain issue.We illustrate this inference process using Figure1, an example thread with four users and six posts.65Let L(m,n) denote the local position between twousers m and n. In the figure, the local position be-tween user B and user A (the author of the seedpost), L(A,B), is in agreement, while users B andC, A and C, as well as C and D each disagree.Walking the shortest path between D and the seedin Figure 1(a), we have L(C,D) = disagree andL(A,C) = disagree, so we can infer that the globalposition between user D and user A is in agreement.That is, user D agrees with the seed post.
Had thelocal position between user A and user C, L(A,C),been in agreement, then we would have concludedthat user D disagrees with the seed post.4 ExperimentsIn this section, we describe our experiments on tworeal-world data sets and report our experimental re-sults for local and global (dis)agreement classifica-tion.4.1 Data SetsWe used two data sets to evaluate our pro-posed method in our experiments.
They werecrawled from the U.S.
Message Board (www.usmessageboard.com) and the Political Forum(www.politicalforum.com).
The two datasets are referred to as usmb and pf, respectively, inour discussion.
The detailed characteristics of thetwo data sets are given in Table 1.Table 1: Characteristics of data setsusmb pf# of threads 88 33# of posts 818 170# of participants 270 103Mean # of posts per thread 9.3 5.2Mean # of participants per thread 3.1 3.1Mean # of posts per participant 3.0 1.7For the evaluation, each post was labelled withtwo annotations.
The first was a global annotationwith respect to the thread?s seed post, and the otherwas a local annotation with respect to the immediateparent.
Seed posts themselves were not annotated,nor were they classified by our algorithms.Global annotations were made by two postgrad-uate students.
Each was instructed to read all theposts in a thread, then label each post with agree ifthe author agreed with the seed post; disagree if theydisagreed; or neutral if opinions were mixed or un-clear.
The annotators used training data until theyreached 85% agreement, then annotated posts sepa-rately.
At no time were they allowed to confer.
Lo-cal annotations were reverse-engineered from theseglobal annotations.
The ratio of posts annotated asagree to those as disagree is about 2 to 1 on bothdatasets.For our proposed three-stage method, local an-notations were taken as input to train the classi-fier and then used as ground truth to evaluate theperformance of local agreement/disagreement clas-sification, while the global annotations were onlyused to evaluate our final accuracy of global agree-ment/disagreement identification.
In contrast, thebaseline classifiers that we compare against forglobal classification were directly trained and evalu-ated using global annotations.4.2 Evaluation MetricsWe used two evaluation metrics to evaluate the per-formance of agreement/disagreement classification.The first metric is accuracy, which is computed asthe percentage of correctly classified examples overall the test data:accuracy =|{x : x ?
Dtest?h(x) = y}||Dtest|,where Dtest denotes the test data, y is the groundtruth annotation label and h(x) is the predicted classlabel.Accuracy can be biased in situations with un-even division between classes, so we also evaluateour classifiers with the F-measure.
For each classi ?
{agree, disagree}, we first calculate precisionP (i) and recall R(i), and the F-measure is computedasF1(i) =2P (i)R(i)P (i) +R(i).For our binary task, we report the average F-measureover both classes.4.3 Local Agree/Disagree ClassificationIn our experiments, we used the implementationof L2-regularised logistic regression in Fan et al(2008) as our local classifier.
For each data set,66Table 2: Classification performance for local (dis)agreementusmb pfAccuracy F-measure Accuracy F-measureNaive Bayes, all features 0.46 0.42 0.52 0.51SVM, all features 0.56 0.60 0.55 0.52Logistic regression, all features 0.62 0.65 0.68 0.77Table 3: Feature analysis for local (dis)agreement using logistic regressionusmb pfAccuracy F-measure Accuracy F-measurewords 0.50 0.55 0.55 0.63words, sentiment 0.53 0.59 0.61 0.71words, sentiment, emotional 0.54 0.51 0.55 0.65words, sentiment, durational 0.58 0.61 0.64 0.72words, sentiment, emotional, durational 0.62 0.65 0.68 0.77we used 70% of posts as training and the other30% were held out for testing.
We compared reg-ularised logistic regression against two baselines:naive Bayes and support vector machines (SVMs),which have been used for (dis)agreement classifica-tion in previous works (Thomas et al, 2006; Soma-sundaran and Wiebe, 2010).
For SVMs, we usedthe toolbox LIBSVM in Chang and Lin (2011) toimplement the classification and probability estima-tion.
We tuned the parameter C in regularised logis-tic regression and SVM, using cross-validation onthe training data, and thereafter the optimal C wasused on the test data for evaluation.Table 2 compares the local classification accuracyof the three methods on data sets usmb and pf, re-spectively.
We can see from the table that logisticregression outperforms naive Bayes and SVM onthe two evaluation metrics for local classification.Although logistic regression and SVM have beenshown to yield comparable performance on sometext categorisation tasks Li and Yang (2003), inour problem, regularised logistic regression was ob-served to outperform SVM for local (dis)agreementclassification.Experiments were also carried out to investigatehow the performance of local classification would bechanged by using different types of features.
Table 3shows the classification accuracy of logistic regres-sion using different types of features on the two datasets.
We can see from the table that using both wordsand sentiment features can improve the performanceas compared to using only words features.
On theusmb dataset, adding emotional features slightly im-proves the accuracy but degrades F-measure, whileon the pf dataset, it degrades on accuracy and F-measure.
In addition, durational features substan-tially improve the classification performance on thetwo metrics.
Overall, the highest classification ac-curacy and F-measure can be achieved by using allfour types of features.4.4 Global Support/Opposition IdentificationWe also conducted experiments to validate the ef-fectiveness of our proposed method for global posi-tion identification.
Table 4 reports the performanceof global classification using the three methods onthe two data sets.
Classifiers ?without aggregation?were trained directly on global annotations, with-out considering local positions at all; those ?withaggregation?
were developed with our three-stagemethod, estimating global positions by aggregatinglocal positions L(m,n).As before, logistic regression generally outper-forms SVM or naive Bayes classifiers, althoughSVM does well on usmb when aggregation (viaL(m,n)) is used.
Although SVM scores well for67Table 4: Classification performance for global (dis)agreementusmb pfAccuracy F-measure Accuracy F-measureWithout aggregationNaive Bayes, all features 0.42 0.41 0.48 0.47SVM, all features 0.62 0.46 0.68 0.40Logistic regression, all features 0.60 0.63 0.65 0.77With aggregationNaive Bayes, all features 0.54 0.67 0.65 0.70SVM, all features 0.64 0.77 0.48 0.60Logistic regression, all features 0.64 0.77 0.68 0.76classification accuracy without aggregation, it hasdegraded and classifies everything as the majorityclass in these cases.
The F-measure is correspond-ingly poor due to a low recall.
This observation isconsistent with the findings reported in Agrawal etal.
(2003).In all cases ?
bar logistic regression on the pf set?
aggregation of local classifications improves theperformance of global classification.
This is moremarked in the usmb data set, which has slightlymore exchanges between each pair of users (mean1.33 per pair per topic, vs. 1.19 for the pf dataset) and therefore more potential for aggregation.We believe that this improvement is because localclassification is sometimes error prone, especiallywhen opinions are not expressed clearly in individ-ual posts.
If so, and assuming that users tend to re-tain their stances within a debate, aggregation can?wash out?
local classification errors.5 Conclusion and Future WorkIn this paper, we have proposed a new method foridentifying participants?
agreement or disagreementon an issue by exploiting local information con-tained in individual posts.
Our proposed methodbuilds a unified framework which enables the clas-sification of participants?
local and global positionsin online debates.
To evaluate the performance ofour proposed method, we conducted experiments ontwo real-world data sets collected from two onlinedebate forums.
Our experiments have shown thatregularised logistic regression is useful for this typeof task; it has a built-in automatic feature selectionby assigning a coefficient to each specific feature,and directly estimates probabilities of class mem-berships, which is quite useful for aggregating localpositions between users.
Our feature analysis hassuggested that using sentiment, emotional and du-rational features can significantly improve the per-formance over only using word features.
Experi-mental results have also shown that, for identifyingusers?
global positions on an issue, aggregating lo-cal positions over posts results in better performancethan no-aggregation baselines and that more benefitseems to accrue as users exchange more posts.We consider extending this work along several di-rections.
First, we would like to examine what otherfactors would have predictive power in online de-bates and thus could be utilised to improve the per-formance of agreement/disagreement classification.Second, we have so far focused on classifying users?positions into two categories: agree and disagree.However, there do exist a portion of posts falling intothe neutral category; that means posts/users do notexpress any position towards an issue.
We will ex-plore how to extend our computational framework toclassify the neutral class.
Finally, in online debates,it is not uncommon to have off-topic or topic-driftposts, especially for long threaded discussions.
Off-topic posts are the ones totally irrelevant to the mainissue being discussed, and topic-drift posts usuallyexist when the topic of a debate has shifted overtime.
Taking these posts into consideration wouldincrease the difficulty of automatic agreement anddisagreement classification, and therefore it is an-other important issue we plan to investigate.68ReferencesRakesh Agrawal, Sridhar Rajagopalan, RamakrishnanSrikant, and Yirong Xu.
2003.
Mining newsgroupsusing networks arising from social bahavior.
In Pro-ceedings of the 12th International World Wide WebConference, pages 529?535, Budapest, Hungary, May.Stefano Baccianella, Andrea Esuli, , and Fabrizio Sebas-tiani.
2010.
SENTIWORDNET 3.0: An enhancedlexical resource for sentiment analysis and opinionmining.
In Proceedings of the 7th Conference on In-ternatinal Language Resources and Evaluation, pages2200?2204, Valletta, Malta, May.Ramnath Balasubramanyan and William W. Cohen.2011.
What pushes their buttons?
Predicting com-ment polarity from the content of political blog posts.In Proceedings of the ACL Workshop on Language inSocial Media, pages 12?19, Porland, Oregon, USA,June.Farah Benamara, Carmine Cesarano, Antonio Picariello,Diego Reforgiato, and V. S. Subrahmanian.
2007.Sentiment analysis: Adjectives and adverbs are betterthan adjectives alone.
In Proceedings of the Interna-tional AAAI Conference on Weblogs and Social Media,Boulder, CO, USA, March.Chih-Chung Chang and Chih-Jen Lin.
2011.
LIBSVM:A library for support vector machines.
ACM Transac-tions on Intelligent Systems and Technology, 2(27):1?27.Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-RuiWang, and Chih-Jen Lin.
2008.
LIBLINEAR: A li-brary for large linear classification.
Journal of Ma-chine Learning Research, 9:1871?1874.Michel Galley, Kathleen McKeown, Julia Hirschberg,and Elizabeth Shriberg.
2004.
Identifying agree-ment and disagreement in conversational speech: Useof Bayesian networks to model pragmatic dependen-cies.
In Proceedings of the 42nd Meeting of the Asso-ciation for Computational Linguistics, pages 669?676,Barcelona, Spain, July.Fan Li and Yiming Yang.
2003.
A loss function analy-sis for classification methods in text categorisation.
InProceedings of the 20th International Conference onMachine Learning, pages 472?479, Washington, DC,USA, July.Chih-Jen Lin, Ruby C. Weng, and S. Sathiya Keerthi.2008.
Trust region Newton method for large-scale lo-gistic regression.
Journal of Machine Learning Re-search, 9:627?650.Akiko Murakami and Rudy Raymond.
2010.
Support oroppose?
Classifying positions in online debates fromreply activities and opinion expressions.
In Proceed-ings of the 23rd International Conference on Compu-tational Linguistics, pages 869?875, Beijing, China,August.Sinno Jialin Pan, Xiaochuan Ni, Jian-Tao Sun, QiangYang, and Zheng Chen.
2010.
Cross-domain senti-ment classification via spectral feature alignment.
InProceedings of the 19th International World Wide WebConference, pages 751?760, Raleigh, NC, USA, April.Swapna Somasundaran and Janyce Wiebe.
2010.
Recog-nizing stances in ideological on-line debates.
In Pro-ceedings of the NAACL HLT 2010 Workshop on Com-putational Approaches to Analysis and Generation ofEmotion in Text, pages 116?124, Los Angeles, CA,USA, June.V.
S. Subrahmanian and Diego Regorgiato.
2008.AVA: Adjective-verb-adverb combinations for senti-ment analysis.
Intelligent Systems, 23(4):43?50.Matt Thomas, Bo Pang, and Lillian Lee.
2006.
Get outthe vote: Determining support or opposition from con-gressional floor-debate transcripts.
In Proceedings ofthe Conference on Empirical Methods in Natural Lan-guage Processing, pages 327?335, Sydney, Australia,July.Wen Wang, Sibel Yaman, Kristin Precoda, ColleenRichey, and Geoffrey Raymond.
2011.
Detectionof agreement and disagreement in broadcast conver-sations.
In Proceedings of the 49th Annual Meeting ofthe Association for Computational Linguistics, pages374?378, Porland, Oregon, USA, June.Tong Zhang and Frank J. Oles.
2001.
Text categorisationbased on regularised linear classification methods.
In-formation Retrieval, 4(1):5?31.69
