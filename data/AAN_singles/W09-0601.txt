Proceedings of the 12th European Workshop on Natural Language Generation, pages 1?8,Athens, Greece, 30 ?
31 March 2009. c?2009 Association for Computational LinguisticsUsing NLG to Help Language-Impaired Users Tell Stories andParticipate in Social DialoguesEhud Reiter, Ross TurnerUniversity of AberdeenAberdeen, UKe.reiter@abdn.ac.ukcsc272@abdn.ac.ukNorman Alm, Rolf Black,Martin Dempster, Annalu WallerUniversity of DundeeDundee, UK{nalm,rolfblack,martindempster,awaller}@computing.dundee.ac.ukAbstractAugmentative and Alternative Communication(AAC) systems are communication aids forpeople who cannot speak because of motor orcognitive impairments.
We are developingAAC systems where users select informationthey wish to communicate, and this is ex-pressed using an NLG system.
We believethis model will work well in contexts whereAAC users wish to go beyond simply makingrequests or answering questions, and havemore complex communicative goals such asstory-telling and social interaction.1 IntroductionMany people have difficulty in communicatinglinguistically because of cognitive or motor im-pairments.
Such people typically use communi-cation aids to help them interact with other peo-ple.
Such communication aids range from sim-ple tools that do not involve computers, such aspicture cards, to complex software systems thatattempt to ?speak?
for the impaired user.From a technological perspective, even themost complex communication aids have typi-cally been based on fixed (canned) texts or sim-ple fill-in-the-blank templates; essentially theuser selects a text or template from a set of pos-sible utterances, and the system utters it.
Webelieve that while this may be adequate if theuser is simply making a request (e.g., please giveme a drink) or answering a question (e.g., I liveat home), it is not adequate if the user has a morecomplex communicative goal, such as engagingin social interaction, or telling a story.We are exploring the idea of supporting suchinteractions by building a system which uses ex-ternal data and/or knowledge sources, plus do-main and conversational models, to dynamicallysuggest possible messages (event, facts, or opin-ions, represented as ontology instances) whichare appropriate to the conversation.
The user se-lects the specific message which he wishes thesystem to speak, and possibly adds simple anno-tations (e.g., I like this) or otherwise edits themessage.
The system then creates an appropriatelinguistic utterance from the selected message,taking into consideration contextual factors.In this paper we describe two projects onwhich we are working within this framework.The goal of the first project is to help non-speaking children tell stories about their day atschool to their parents; the goal of the secondproject is to help non-speaking adults engage insocial conversation.2 Background2.1 Augmentative and alternative commu-nicationAugmentative and alternative communication(AAC) is a term that describes a variety of meth-ods of communication for non-speaking peoplewhich can supplement or replace speech.
Theterm covers techniques which require no equip-ment, such as sign language and cards with im-ages; and also more technologically complexsystems which use speech synthesis and a varietyof strategies to create utterances.The most flexible AAC systems allow users tospecify arbitrary words, but communication ratesare extremely low, averaging 2-10 words perminute.
This is because many AAC users interactslowly with computers because of their impair-ments.
For example, some of the children wework with cannot use their hands, so they usescanning interfaces with head switches.
In otherwords, the computer displays a number of op-1tions to them, and then scans through these,briefly highlighting each option.
When the de-sired option is highlighted, the child selects it bypressing a switch with her head.
This is ade-quate for communicating basic needs (such ashunger or thirst); the computer can display amenu of possible needs, and the child can selectone of the items.
But creating arbitrary messageswith such an interface is extremely slow, even ifword prediction is used; and in general such in-terfaces do not well support complex social in-teractions such as story telling (Waller, 2006).A number of research projects in AAC havedeveloped prototype systems which attempt tofacilitate this type of human-human interaction.At their most basic, these systems provide userswith a library of fixed ?conversational moves?which can be selected and uttered.
These movesare based on models of the usual shape and con-tent of conversational encounters (Todman &Alm, 2003), and for example include standardconversational openings and closings, such asHello and How are you.
They also include back-channel communication such as Uh-huh, Great!,and Sorry, can you repeat that.It would be very useful to go beyond standardopenings, closings, and backchannel messages,and allow the user to select utterances whichwere relevant to the particular communicativecontext and goals.
Dye et al(1998) developed asystem based on scripts of common interactions(Schank & Abelson, 1977).
For example, a usercould activate the MakeAnAppointment script,and then could select utterances relevant to thisscript, such as I would like to make an appoint-ment to see the doctor.
As the interaction pro-gressed, the system would update the selectionsoffered to the user based on the current stage ofthe script; for example during time negotiation apossible utterance would be I would like to seehim next week.
This system proved effective intrials, but needed a large number of scripts to begenerally effective.
Users could author their owntexts, which were added to the scripts, but thiswas time-consuming and had to be done in ad-vance of the conversation.Another goal of AAC is to help users narratestories.
Narrative and storytelling play a veryimportant part in the communicative repertoire ofall speakers (Schank, 1990).
In particular, theability to draw on episodes from one?s life his-tory in current conversation is vital to maintain-ing a full impression of one?s personality in deal-ing with others (Polkinghorne, 1991).
Story tell-ing tools for AAC users have been developed,which include ways to introduce a story, tell it atthe pace required (with diversions) and givefeedback to comments from listeners (Waller,2006); but again these tools are based on a li-brary of fixed texts and templates.2.2 NLG and AACNatural language generation (NLG) systemsgenerate texts in English and other human lan-guages from non-linguistic input (Reiter andDale, 2000).
In their review of NLP and AAC,Newell, Langer, and Hickey (1998) suggest thatNLG could be used to generate complete utter-ances from the limited input that AAC users areable to provide.
For example, the Compansionproject (McCoy, Pennington, Badman 1998)used NLP and NLG techniques to expand tele-graphic user input, such as Mary go store?, intocomplete utterances, such as Did Mary go to thestore?
Netzer and Elhadad (2006) allowed usersto author utterances in the symbolic languageBLISS, and used NLG to translate this to Englishand Hebrew texts.In recent years there has been growing interestin data-to-text NLG systems (Reiter, 2007);these systems generate texts based on sensor andother numerical data, supplemented with ontolo-gies that specify domain knowledge.
In princi-ple, it seems that data-to-text techniques shouldallow NLG systems to provide more assistancethan the syntactic help provided by Compansion.For example, if the user wanted to talk about arecent football (soccer) match, a data-to-text sys-tem could get actual data about the match fromthe web, and generate potential utterances fromthis data, such as Arsenal beat Chelsea 2-1 andVan Persie scored two goals; the user could thenselect one of these to utter.In addition to helping users interact with otherpeople, NLG techniques can also be used to edu-cate and encourage children with disabilities.The STANDUP system (Manurung, Ritchie etal., 2008), for example, used NLG and computa-tional humour techniques to allow children whouse AAC devices to generate novel punningjokes.
This provided the children with successfulexperiences of controlling language, gave theman opportunity to play with language and explorenew vocabulary (Waller et al, in press).
In asmall study with nine children with cerebralpalsy, the children used their regular AAC toolsmore and also performed better on a test measur-ing linguistic abilities after they used STANDUPfor ten weeks.23 Our ArchitectureOur goal is help AAC users engage in com-plex social interaction by using NLG and data-to-text technology to create potential utterancesand conversational contributions for the users.The general architecture is shown in Figure 1,and Sections 4 and 5 describe two systems basedon this architecture.The system has the following components:Data analysis: read in data, from sensors,web information sources, databases, and so forth.This module analyses this data and identifiesmessages (in the sense of Reiter and Dale(2000)) that the user is likely to want to commu-nicate; this analysis is partially based on domain,conversation, and user models, which may berepresented as ontologies.Editing: allow the user to edit the messages.Editing ranges from adding simple annotations tospecify opinions (e.g., add BAD to Arsenal beatChelsea 2-1 if the user is a Chelsea fan), to usingan on-screen keyboard to type free-text com-ments.
Users can also delete messages, specifywhich messages they are most likely to want toutter, and create new messages.
Editing is donebefore the actual conversation, so the user doesnot have to do this under time pressure.
Theamount of editing which can be done partiallydepends on the extent of the user?s disabilities.Narration: allows the user to select mes-sages, and perhaps conversational moves (e.g.,Hello), in an actual conversational context.
Edit-ing is possible, but is limited by the need to keepthe conversation flowing.NLG and Speech Synthesis: Generates actualutterances from the selected messages, takinginto account linguistic context, especially a dia-logue model.4 Narrative for Children: How wasSchool TodayThe goal of the How was School Today project isto enable non-speaking children with major mo-tor disabilities but reasonable cognitive skills totell a story about what they did at school duringthe day.
The particular children we are workingwith have cerebral palsy, and use wheelchairs.
Afew of them can use touch screens, but most ofthem use a head switch and scanning interface,as described above.
By ?story?, we mean some-thing similar to Labov?s (1972) conversationalnarrative, i.e., a series of linked real-world eventswhich are unusual or otherwise interesting, pos-sibly annotated with information about thechild?s feelings, which can be narrated orally.We are not expecting stories in the literary sense,with character development and complex plots.The motivation of the project is to provide thechildren with successful narrative experience.Typically developing children develop narrativeskills from an early age with adults scaffoldingconversations to elicit narrative, e.g.
?What didyou do at school today??
(Bruner, 1975).
As thechild?s vocabulary and language competencedevelops, scaffolding is reduced.
This progres-sion is seldom seen in children with complexcommunication needs ?
they respond to closedquestions but seldom take control of conversa-SensordataWeb infosourcesOtherexternal dataData analysis:select possiblemessages tocommunicateConversationmodelDomain modelUser modelEditing: User addsannotationsUserNLG:GenerateutteranceDialoguemodelSpeechsynthesisConversationpartnerNarration: Userselects what to sayPrepare contentNarrate contentFigure 1:  General architecture3tion (von Tetzchner and Grove, 2003).
Manychildren who use AAC have very limited narra-tive skills (Soto et al 2006).
Research has shownthat providing children who use AAC with suc-cessful narrative experiences by providing fullnarrative text can help the development of writ-ten and spoken narrative skills  (Waller, 2008).The system follows the architecture describedabove.
Input data comes from RFID sensors thattrack where the child went during the day; anRFID reader is mounted on the child?s wheel-chair, and RFID tags are placed around theschool, especially in doorways so we can moni-tor children entering and leaving rooms.
Teach-ers have also been given RFID swipe cardswhich they can swipe against a reader, to recordthat they are interacting with the child; this ismore robust than attempting to infer interactionautomatically by tracking teachers?
position.Teachers can also record interactions with ob-jects (toys, musical instruments, etc), by usingspecial swipe cards associated with these objects.Last but not least, teachers can record spokenmessages about what happened during the day.An example of how the child?s wheelchair is setup is shown in Figure 2.Figure 2: System configurationThe data analysis module combines sensor-derived location and interaction data with a time-table which records what the child was expectedto do during the day, and a domain knowledgebase which includes information about typicalactivities (e.g., if the child?s location is Swim-mingPool, the child?s activity is probablySwimming).
From this it creates a series ofevents (each of which contain a number of mes-sages) which describe the child?s lessons andactivities, including divergences from what isexpected in the timetable.
Several messages maybe associated with an event.
The data analysismodule also infers which events and messages itbelieves are most interesting to the child; this ispartially based on heuristics about what childrenare interested in (e.g., swimming is more inter-esting than lunch), and partially based on thegeneral principle that unexpected things (diver-gences from the timetable) are more interestingthan expected things.
No more than five eventsare flagged as interesting, and only these eventsare shown in the editing interface.The editing interface allows children to re-move events they do not want to talk about (per-haps for privacy reasons) from the list of interest-ing events.
It also allows children to add mes-sages that express simple opinions about events;i.e., I liked it or I didn?t like it.
The interface isdesigned to be used with a scanning interface,and is based on symbols that represent events,annotations, etc.The narration interface, shown in Figure 3, issimilar to the editing interface.
It allows childrento choose a specific event to communicate,which must be one of the ones they selected dur-ing the editing phase.
Children are encouragedto tell events in temporal order (this is one of thenarration skills we are trying to teach), but this isnot mandated, and they can deviate from tempo-ral order if they wish.Figure 3: Narration InterfaceThe NLG system generates actual texts fromthe events selected by the children.
Most of thisTablet PC with NLG system andswipe-card RFID sensorlong rangeRFIDsensor forlocationtrackingEventsOpinion AnnotationsMessagesfor event4is fairly simple, since the system deliberatelyuses simple ?child-like?
language (Section 6).However, the system does need to make somedecisions based on discourse context, includingchoosing appropriate referring expressions (es-pecially pronouns), and temporal expressions(especially when children deviate from puretemporal order).4.1 ExampleFor example, assume that the timetable speci-fies the following informationAssume that the sensors then recorded the fol-lowing informationEvent 1Location: CL_SEC2Time: 13:23:00.0 - 14:07:00.0Interactions: Mrs. Smith, Rolf, RossEvent 2Location: HALLTime: 14:10:00.0 ?
14:39:00.0Interactions: noneThe data analysis module associates Event 1 withthe Arts and Crafts timetable entry, since the lo-cation is right, the timetabled teacher is present,and the times approximately match.
From thistwo messages are produced: one correspondingto I had Arts and Crafts this afternoon with Mrs.Smith (the core activity description), and the oth-er corresponding to Rolf and Ross were there(additional information about people not time-tabled to be there).
The child can add opinionsusing the editing interface; for example, if headded a positive annotation to the event, thiswould become an additional message corre-sponding to It was great.For Event 2, the data analysis module notesthat it does not match a timetabled event.
Thetimetable indicates the child should be at Physio-therapy after Art and Crafts; however, the sensorinformation indicates they were in the hall.
Thesystem generates a single message correspondingto Then I went to the Hall instead of Physiother-apy to describe this event.
If the child added anegative annotation to this message, this wouldbecome an additional message expressed as Ididn?t like it.4.2 EvaluationWe conducted an initial evaluation of the Howwas School Today system in January, 2009.Two children used the system for four days: Ju-lie, age 11, who had good cognitive skills butwas non-verbal because of severe motor impair-ments; and Jessica, age 13, who had less severemotor impairments but who had some cognitiveand memory impairments (these are not the chil-drens?
real names).
Julie used the system as acommunication and interaction aid, as describedabove; Jessica used the system partially as amemory aid.
The evaluation was primarilyqualitative: we observed how Julie and Jessicaused the system, and interviewed their teachers,speech therapists, care assistants, and Julie?smother (Jessica?s parents were not available).The system worked very well for Julie; shelearned it quickly, and was able to use it to havereal conversations about her day with adults, al-most for the first time in her life.
This validatedour vision that our technology could help AACusers engage in real interaction, and go beyondsimple question answering and communicationof basic needs.
The system also worked rea-sonably well as a memory aid for Jessica, but shehad a harder time using it, perhaps because of hercognitive impairments.Staff and Julie?s mother were very supportiveand pleased with the system.
They had sugges-tions for improving the system, including a widerrange of annotations; more phrases about theconversation itself, such as Guess what happenedat school today; and allowing children to requestteenager language (e.g., really cool).From a technical perspective, the systemworked well overall.
School staff were happy touse the swipe cards, which worked well.
Therewere some problems with the location sensors,we need better techniques for distinguishing realreadings from noise.
A surprising amount ofeffort was needed to enter up-to-date knowledge(e.g., daily lunch menus), this would need to beaddressed if the system was used for a period ofmonths as opposed to days.5 Social Conversation for AdultsIn our second project, we want to build a tool tohelp adults with cerebral palsy engage in socialconversation about a football match, movie,weather, and so forth.
Many people with severedisabilities have great difficulty developing newinterpersonal relationships, and indeed report thatforming new relationships and taking part in newTime Activity Location Teacher??
??
??
?
?13.20 -14 Arts andCraftsCL_SEC2 Mrs Smith14 -14.40 Physiotherapy PHYSIO1 Mrs Jones??
??
??
?
?5activities are major priorities in their lives (Datil-lo et al, 2007).
Supporting these goals throughthe development of appropriate technologies isimportant as it could lead to improved social out-comes.This project builds on the TALK system(Todman and Alm, 2003), which helped AACusers engage in active social conversation.TALK partially overcame the problem of lowcommunication rate by requiring users to pre-author their conversational material ahead oftime, so that when it was needed it could simplybe selected and output.
TALK also used insightsfrom Conversation Analysis (Sacks, 1995) toprovide appropriate functionality in the systemfor social conversation.
For example, it sup-ported opening and closing statements, stepwisetopic change, and the use of quick-fire utterancesto provide fast, idiomatic responses to commonlyencountered situations.
This approach led tomore dynamic AAC-facilitated interactions withhigher communication rates, and had a positiveimpact on the perceived communicative compe-tence of the user (Todman, Alm et al, 2007).TALK requires the user to spend a substantialamount of time pre-authoring material; this isperhaps its greatest weakness.
Our idea is to re-duce the amount of pre-authoring needed, by us-ing the architecture shown in Fig 1, where muchof the material is automatically created from datasources, ontologies, etc, and the user?s role islargely to edit and annotate this material, not tocreate it from scratch.We developed an initial prototype system todemonstrate this concept in the domain of foot-ball results (Dempster, 2008).
We are nowworking on another prototype, whose goal is tosupport social conversations about movies, mu-sic, television shows, etc (which is a muchbroader domain than football).
We have createdan ontology which can describe events such aswatching a film, listening to a music track, orreading a book.
Each ?event?
has both temporaland spatial properties which allow descriptions tobe produced about where and when an event tookplace, and other particulars relating to that par-ticular class of event.
For example, if the userlistened to a radio show, we record the name ofthe show, the presenter and the station it wasbroadcast on.
Ultimately we plan to obtain in-formation about movies, music tracks, etc fromweb-based databases such as IMDB (movies)and last.fm (music).Of course, databases such as IMDB do notcontain information such as what the userthought of the movie, or who he saw it with.Hence we will allow users to add annotationswith such information.
Some of these annota-tions will be entered via a structured tool, such asa calendar interface that allows users to specifywhen they watched or listened to something.
Wewould like to use NaturalOWL (Galanis and An-droutsopoulos, 2007) as the NLG component ofthe system; it is well suited to describing objects,and is intended to be integrated with an ontology.As with the How Was School Today project,some of the main low-level NLG challenges arechoosing appropriate referring expressions andtemporal references, based on the current dis-course context.
Speech output is done using Ce-reproc (Aylett and Pidcock, 2007).An example of our current narration interfaceis shown in Figure 4.
In the editing interface, theuser has specified that he went to a concert at8pm on Thursday, and that he rated it 8 out of10.
The narration interface gives the user achoice of a number of messages based on thisinformation, together with some standard mes-sages such as Thanks and Agree.Note that unlike the How Was School Todayproject, in this project we do not attempt to inferevent information from sensors, but we allow(and expect) the user to enter much more infor-mation at the editing stage.
We could in princi-ple use sensors to pick up some information,such as the fact that the user was in the cinemafrom 12 to 2PM on Tuesday, but this is not theresearch focus of this project.We plan to evaluate the system using groupsof both disabled and non-disabled users.
Thishas been shown in the past to be an effective ap-proach for the evaluation of prototype AAC sys-tems (Higginbotham, 1995).
Initially pairs ofnon-disabled participants will be asked to pro-duce short conversations with one person usingthe prototype and the other conversing normally.Quantitative measures of the communication rate6will be taken as well as more qualitative observa-tions relating to the usability of the system.
Af-ter this evaluation we will improve the systembased on our findings, and then conduct a finalevaluation with a small group of AAC users.6 Discussion: Challenges for NLGFrom an NLG perspective, generating AAC textsof the sort we describe here presents differentchallenges from many other NLG applications.First of all, realization and even microplanningare probably not difficult, because in this contextthe AAC system should generate short simplesentences if possible.
This is because the systemis speaking ?for?
someone with limited or devel-oping linguistic abilities, and it should try to pro-duce something similar to what the user wouldsay himself if he or she had the time to explicitlywrite a text using an on-screen keyboard.To take a concrete example, we had originallyconsidered using past-perfect tense (a fairlycomplex linguistic construct) in the How wasSchool project, when the narrative jumped to anearlier point in time.
For example I ate lunch at12.
I had gone swimming at 11.
But it was clearfrom corpora of child-written texts that thesechildren never used perfect tenses, so instead weopted for I ate lunch at 12.
I went swimming at11.
This is less linguistically polished, but muchmore in line with what the children might actu-ally produce.Given this desire for linguistic simplicity, re-alisation is very simple, as is lexical choice (usesimple words) and aggregation (keep sentencesshort).
The main microplanning challenges re-late to discourse coherence, in particular refer-ring expressions and temporal descriptions.On the other hand, there are major challengesin document planning.
In particular, in the HowWas School project, we want the output to be aproper narrative, in the sense of Labov (1972).That is, not just a list of facts and events, but astructure with a beginning and end, and with ex-planatory and other links between components(e.g., I had math in the afternoon because wewent swimming in the morning, if the child nor-mally has math in the morning).
We also wantedthe narrative to be interesting and hold the inter-est of the person the child is communicatingwith.
As pointed out by Reiter et al(2008), cur-rent NLG systems do not do a good job of gener-ating narratives.Similarly, in the Social Conversations projectwe want the system to generate a social dialogue,not just a list of facts about movies and songs.Little previous research has been done on gener-ating social (as opposed to task-oriented) dia-logues.
One exception is the NECA Socialitesystem (van Deemter et al 2008), but this fo-cused on techniques for expressing affect, not onhigh-level conversational structure.For both stories and social conversations, itwould be extremely useful to be able to monitorwhat the conversational partner is saying.
This issomething we hope to investigate in the future.As most AAC users interact with a small numberof conversational partners, it may be feasible touse a speech dictation system to detect at leastsome of what the conversational partner says.Last but not least, a major challenge implicitin our systems and indeed in the general architec-ture is letting users control the NLG system.Our systems are intended to be speaking aids,ideally they should produce the same utterancesas the user would if he was able to talk.
Thismeans that users must be able to control the sys-tems, so that it does what they want it to do, interms of both content and expression.
To thebest of our knowledge, little is known about howusers can best control an NLG system.7 ConclusionMany people are in the unfortunate position ofnot being able to speak or type, due to cognitiveand/or motor impairments.
Current AAC toolsallow such people to engage in simple needs-based communication, but they do not providegood support for richer use of language, such asstory-telling and social conversation.
We aretrying to develop more sophisticated AAC toolswhich support such interactions, by using exter-nal data and knowledge sources to produce can-didate messages, which can be expressed usingNLG and speech synthesis technology.
Ourwork is still at an early stage, but we believe thatit has the potential to help AAC users engage inricher interactions with other people.AcknowledgementsWe are very grateful to Julie, Jessica, and theirteachers, therapists, carers, and parents for theirhelp in building and evaluating the system de-scribed in Section 4.
Many thanks to the anony-mous referees and our colleagues at Aberdeenand Dundee for their very helpful comments.This research is supported by EPSRC grantsEP/F067151/1 and EP/F066880/1, and by aNorthern Research Partnership studentship.7ReferencesAylett, M. and C. Pidcock (2007).
The CereVoiceCharacterful Speech Synthesiser SDK.
Proceed-ings of Proceedings of the 7th International Con-ference on Intelligent Virtual Agents, pages 413-414.Bruner, J.
(1975).
From communication to language:A psychological perspective.
Cognition 3: 255-289.Datillo, J., G. Estrella, L. Estrella, J.
Light, D.McNaughton and M. Seabury (2007).
"I have cho-sen to live life abundantly": Perceptions of leisureby adults who use Augmentative and AlternativeCommunication.
Augmentative & AlternativeCommunication 24(1): 16-28.van Deemter, K., B Krenn, P Piwek, M Klesen, MSchr?der and S Baumann.
Fully generated scripteddialogue for embodied agents.
Artificial Intelli-gence 172: 1219?1244.Dempster, M. (2008).
Using natural language genera-tion to encourage effective communication in non-speaking people.
Proceedings of Young Research-ers Consortium, ICCHP'08.Dye, R., N. Alm, J. Arnott, G. Harper, and A. Morri-son (1998).
A script-based AAC system for trans-actional interaction.
Natural Language Engineer-ing, 4(1), 57-71.Galanis, D. and I. Androutsopoulos (2007).
Generat-ing Multilingual Descriptions from LinguisticallyAnnotated OWL Ontologies: the NaturalOWL Sys-tem.
Proceedings of ENLG 2007.Higginbotham, D. J.
(1995).
Use of nondisabled sub-jects in AAC Research : Confessions of a researchinfidel.
Augmentative and Alternative Communica-tion 11(1): 2-5.Labov, W (1972).
Language in the Inner City.
Uni-versity of Pennsylvania Press.Manurung, R., G. Ritchie, H. Pain, A. Waller, D.O'Mara and R. Black (2008).
The Construction of aPun Generator for Language Skills Development.Applied Artificial Intelligence 22(9): 841 ?
869.McCoy, K., C. Pennington and A. Badman (1998).Compansion: From research prototype to practicalintegration.
Natural Language Engineering 4:73-95.Netzer, Y and Elhadad, M (2006).
Using SemanticAuthoring for Blissymbols CommunicationBoards.
In Proc of HLT-2006.Newell, A., S. Langer and M. Hickey (1998).
The roleof natural language processing in alternative andaugmentative communication.
Natural LanguageEngineering 4:1-16.Polkinghorne, D. (1991).
Narrative and self-concept.Journal of Narrative and Life History, 1(2/3), 135-153Reiter, E (2007).
An Architecture for Data-to-TextSystems.
In Proceedings of ENLG-2007, pages147-155.Reiter, E. and R. Dale (2000).
Building Natural Lan-guage Generation Systems.
Cambridge UniversityPress.Reiter, E,  A. Gatt, F Portet, and M van der Meulen(2008).
The Importance of Narrative and OtherLessons from an Evaluation of an NLG Systemthat Summarises Clinical Data (2007).
In Proceed-ings of INLG-2008, pages 97-104.Sacks, H. (1995).
Lectures on Conversation.
G. Jef-ferson.
Cambridge, MA, Blackwell.Schank, R. C. (1990).
Tell me a story: A new look atreal and artificial intelligence.
New York, Macmil-lan Publishing Co.Schank, R., and R. Abelson (1977).
Scripts, plans,goals, and understanding.
New Jersey: LawrenceErlbaum.Soto, G., E. Hartmann, and D. Wilkins (2006).
Ex-ploring the Elements of Narrative that Emerge inthe Interactions between an 8-Year-Old Child whouses an AAC Device and her Teacher.
Augmenta-tive and Alternative Communication 4:231 ?
241.Todman, J. and N. A. Alm (2003).
Modelling conver-sational pragmatics in communication aids.
Jour-nal of Pragmatics 35: 523-538.Todman, J., N. A. Alm, D. J. Higginbotham and P.File (2007).
Whole Utterance Approaches in AAC.Augmentative and Alternative Communication24(3): 235-254.von Tetzchner, S. and N. Grove (2003).
The devel-opment of alternative language forms.
In S. vonTetzchner and N. Grove (eds), Augmentative andAlternative Communication: Developmental Issues,pages 1-27.
Wiley.Waller, A.
(2006).
Communication Access to Conver-sational Narrative.
Topics in Language Disorders26(3): 221-239.Waller, A.
(2008).
Narrative-based Augmentative andAlternative Communication: From transactional tointeractional conversation.
Proceedings of ISAAC2008, pages 149-160.Waller, A., R. Black, D. A. O'Mara, H. Pain, G. Rit-chie and R. Manurung (In Press).
Evaluating theSTANDUP Pun Generating Software with Chil-dren with Cerebral Palsy.
ACM Transactions onAccessible Computing.8
