Proceedings of the 53rd Annual Meeting of the Association for Computational Linguisticsand the 7th International Joint Conference on Natural Language Processing, pages 524?533,Beijing, China, July 26-31, 2015.c?2015 Association for Computational LinguisticsEncoding Distributional Semantics into Triple-Based Knowledge Rankingfor Document EnrichmentMuyu Zhang1?, Bing Qin1, Mao Zheng1, Graeme Hirst2, and Ting Liu11Research Center for Social Computing and Information RetrievalHarbin Institute of Technology, Harbin, China2Department of Computer Science, University of Toronto, Toronto, ON, Canada{myzhang,qinb,mzheng,tliu}@ir.hit.edu.cngh@cs.toronto.eduAbstractDocument enrichment focuses on retriev-ing relevant knowledge from external re-sources, which is essential because text isgenerally replete with gaps.
Since conven-tional work primarily relies on special re-sources, we instead use triples of Subject,Predicate, Object as knowledge and in-corporate distributional semantics to rankthem.
Our model first extracts these triplesautomatically from raw text and convertsthem into real-valued vectors based on theword semantics captured by Latent Dirich-let Allocation.
We then represent thesetriples, together with the source documentthat is to be enriched, as a graph of triples,and adopt a global iterative algorithm topropagate relevance weight from sourcedocument to these triples so as to select themost relevant ones.
Evaluated as a rank-ing problem, our model significantly out-performs multiple strong baselines.
More-over, we conduct a task-based evaluationby incorporating these triples as additionalfeatures into document classification andenhances the performance by 3.02%.1 IntroductionDocument enrichment is the task of acquiring rel-evant background knowledge from external re-sources for a given document.
This task is essen-tial because, during the writing of text, some ba-sic but well-known information is usually omittedby the author to make the document more concise.For example, Baghdad is the capital of Iraq isomitted in Figure 1a.
A human will fill these gapsautomatically with the background knowledge inhis mind.
However, the machine lacks both the?This work was partly done while the first author wasvisiting University of Toronto.necessary background knowledge and the abilityto select.
The task of document enrichment is pro-posed to tackle this problem, and has been provedhelpful in many NLP tasks such as web search(Pantel and Fuxman, 2011), coreference resolu-tion (Bryl et al, 2010), document cluster (Hu etal., 2009) and entity disambiguation (Sen, 2012).We can classify previous work into two classesaccording to the resources they rely on.
The firstline of work uses Wikipedia, the largest on-line en-cyclopedia, as a resource and introduces the con-tent of Wikipedia pages as external knowledge(Cucerzan, 2007; Kataria et al, 2011; He et al,2013).
Most research in this area relies on the textsimilarity (Zheng et al, 2010; Hoffart et al, 2011)and structure information (Kulkarni et al, 2009;Sen, 2012; He et al, 2013) between the mentionand the Wikipedia page.
Despite the apparent suc-cess of these methods, most Wikipedia pages con-tain too much information, most of which is notrelevant enough to the source document, and thiscauses a noise problem.
Another line of work triesto improve the accuracy by introducing ontolo-gies (Fodeh et al, 2011; Kumar and Salim, 2012)and structured knowledge bases such as WordNet(Nastase et al, 2010), which provide semantic in-formation about words such as synonym (Sun etal., 2011) and antonym (Sansonnet and Bouchet,2010).
However, these methods primarily rely onspecial resources constructed with supervision oreven manually, which are difficult to expand andin turn limit their applications in practice.In contrast, we wish to seek the benefits of bothcoverage and accuracy from a better representa-tion of background knowledge: triples of Subject,Predicate, Object (SPO).
According to Hoffart etal.
(2013), these triples, such as LeonardCohen,wasBornIn, Montreal, can be extracted automat-ically from Wikipedia and other sources, whichis compatible with the RDF data model (Staaband Studer, 2009).
Moreover, by extracting these524S1: Coalition may never know if   Iraqi   president   SaddamHussein survived a U.S. air strike yesterday.S2: A B-1 bomber dropped four 2,000-pound bombs on a buildingin a residential area of   Baghdad  .S3: They had got an intelligence reports senior officials weremeeting there, possibly including Saddam Hussein and  his sons .BaghdadIraqi hasCapitalSaddam Hussein diedInQusay HusseinSaddam Hussein hasChildKadhimiyak1:k2:k3:GlobalRanking(a) Source document (b) Top-3 background knowledgeS1: Coalition may never know if   Iraqi   president   Saddam Husseinsurvived a U.S. air strike yesterday.S2: A B-1 bomber dropped four 2,000-pound bombs on a buildingin a residential area of   Baghdad  .S3: They had got an intelligence reports senior officials weremeeting there, possibly including  Saddam Hussein  and   his sons .
(a) Source document (b) Two relevant background knowledgeIraqBaghdadSaddam HusseinCaptainDied In KadhimiyaGlobalRankingk1:k2:S1: Coalition may never know if   Iraqi   president   SaddamHussein survived a U.S. air strike yesterday.S2: A B-1 bomber dropped four 2,000-pound bombs on a buildingin a residential area of   Baghdad  .S3: They had got an intelligence reports senior officials weremeeting there, possibly including Saddam Hussein and  his sons .BaghdadIraqi hasCapitalSaddam Hussein diedInQusay HusseinSaddam Hussein hasChildKadhimiyak1:k2:k3:GlobalRanking(a) Source document(b) Top-3 background knowledgeS1: Coalition may never know if   Iraqi   president   Saddam Husseinsurvived a U.S. air strike yesterday.S2: A B-1 bomber dropped four 2,000-pound bombs on a buildingin a residential area of   Baghdad  .S3: They had got an intelligence reports senior officials weremeeting there, possibly including  Saddam Hussein  and   his sons .
(a) Source document(b) Two relevant background knowledgeIraqBaghdadSaddam HusseinCaptainDied In KadhimiyaGlobalRankingk1:k2:S1: The coalition may never know if   Iraqi   president SaddamHussein survived a U.S. air strike yesterday.S2: A B-1 bomber dropped four 2,000-pound bombs on a buildingin a residential area of   Baghdad  .S3: They had  received intelligence reports that senior officials weremeeting there, possibly including  Saddam Hussein  and his sons .IraqBaghdadSaddam HusseinCapitalhasChild Qusay Husseink1:k2:(a) Source document: air strike aiming at Saddam in BaghdadS1: Coalition may never know if   Iraqi   president   SaddamHussein survived a U.S. air strike yesterday.S2: A B-1 bomber dropped four 2,000-pound bombs on a buildingin a residential area of   Baghdad  .S3: They had ot n intelligence reports senior officials weremeeting there, possibly including Saddam Hussein and  his sons .BaghdadIraqi hasCapitalSaddam Hussein diedInQusay HusseinSaddam Hussein hasChildKadhimiyak1:k2:k3:GlobalRanking(a) Source document (b) Top-3 background knowledgeS1: Coalition may never know if   Iraqi   president   Saddam Husseinsurvived a U.S. air strike yesterday.S2: A B-1 bomber dropped four 2,000-pound bombs on a buildingin a residential area of   Baghdad  .S3: They had got an intelligence reports senior officials weremeeting there, possibly including  Saddam Hussein  and   his sons .
(a) Source document (b) Two releva t background knowledgeIraqBaghdadSaddam HusseinCaptainDied In KadhimiyaGlobalRankingk1:k2:S1: Coalition may never know if   Iraqi   president   SaddamHussein survived a U.S. air strike yesterday.S2: A B-1 bomber dropped four 2,000-pound bombs on a buildingin a residential area of   Baghdad  .S3: They had got an intelligence reports senior officials weremeeting there, possibly including Saddam Hussein and  his sons .BaghdadIraqi hasCapitalSaddam Hussein diedInQusay HusseinSaddam Hussein hasChildKadhimiyak1:k2:k3:GlobalRanking(a) Source document(b) Top-3 background knowledgeS1: Coalition may never know if   Iraqi   president   Saddam Husseinsurvived a U.S. air strike yesterday.S2: A B-1 bomber dropped four 2,000-pound bombs on a buildingin a residential area of   Baghdad  .S3: They had got an intelligence reports senior officials weremeeting there, possibly including  Saddam Hussein  and   his sons .
(a) Source document(b) Two relevant background knowledgeIraqBaghdadSaddam HusseinCaptainDied In KadhimiyaGlobalRankingk1:k2:S1: The coalition may never know if   Iraqi   president SaddamHuss in survived a U.S. air strike yesterday.S2: A B-1 bomber dropped four 2,000-pound bombs on a buildingin a residential area f   Baghdad  .S3: They had  received intelligence reports that senior officials weremeeting there, possibly including  Saddam Hussein  and his sons .IraqBaghdadSaddam HusseinCapitalhasChild Qusay Husseink1:k2:(b) Two omitted relevant pieces of background knowledgeFigure 1: An example of document enrichment:A source document about a U.S. air strike omit-ting two important pieces of background knowl-edge which are acquired by our framework.triples from multiple sources, we also get bettercoverage.
Therefore, one can expect that this rep-resentation is helpful for better document enrich-ment by incorporating both accuracy and cover-age.
In fact, there is already evidence that thisrepresentation is helpful.
Zhang et al (2014) pro-posed a triple-based document enrichment frame-work which uses triples of SPO as backgroundknowledge.
They first proposed a search engine?based method to evaluate the relatedness betweenevery pair of triples, and then an iterative propa-gation algorithm was introduced to select the mostrelevant triples to a given source document (seeSection 2), which achieved a good performance.However, to evaluate the semantic relatednessbetween two triples, Zhang et al (2014) primar-ily relied on the text of triples and used searchengines, which makes their method difficult tore-implement and in turn limits its application inpractice.
Moreover, they did not carry out anytask-based evaluation, which makes it uncertainwhether their method will be helpful in real appli-cations.
Therefore, we instead use topic models,especially Latent Dirichlet Allocation (LDA), toencode distributional semantics of words and con-vert every triple into a real-valued vector, whichis then used to evaluate the relatedness betweena pair of triples.
We then incorporate these triplesinto the given source document and represent themtogether as a graph of triples.
Then a modified it-erative propagation is carried out over the entiregraph to select the most relevant triples of back-ground knowledge to the given source document.To evaluate our model, we conduct two series ofexperiments: (1) evaluation as a ranking problem,and (2) task-based evaluation.
We first treat thistask as a ranking problem which inputs one doc-ument and outputs the top N most-relevant triplesof background knowledge.
Second, we carry out atask-based evaluation by incorporating these rele-vant triples acquired by our model into the origi-nal model of document classification as additionalfeatures.
We then perform a direct comparison be-tween the classification models with and withoutthese triples, to determine whether they are help-ful or not.
On the first series of experiments, weachieve a MAP of 0.6494 and a P@N of 0.5597 inthe best situation, which outperforms the strongestbaseline by 5.87% and 17.21%.
In the task-basedevaluation, the enriched model derived from thetriples of background knowledge performs betterby 3.02%, which demonstrates the effectiveness ofour framework in real NLP applications.2 BackgroundThe most closely related work in this area is ourown (Zhang et al, 2014), which used the triplesof SPO as background knowledge.
In that work,we first proposed a triple graph to represent thesource document and then used a search engine?based iterative algorithm to rank all the triples.
Wedescribe this work in detail below.Triple graph Zhang et al (2014) proposed thetriple graph as a document representation, wherethe triples of SPO serve as nodes, and the edgesbetween nodes indicate their semantic relatedness.There are two kinds of nodes in the triple graph:(1) source document nodes (sd-nodes), which aretriples extracted from source documents, and (2)background knowledge nodes (bk-nodes), whichare triples extracted from external sources.
Bothof them are extracted automatically with Reverb, awell-known Open Information Extraction system(Etzioni et al, 2011).
There are also two kindsof edges: (1) an edge between a pair of sd-nodes,and (2) an edge between one sd-node and anotherbk-node, both of which are unidirectional.
In theoriginal representation, there are no edges betweentwo bk-nodes because they treat the bk-nodes asrecipients of relevance weight only.
In this paper,we modify this setup and connect every pair of bk-nodes with an edge, so the bk-nodes serve as in-termediate nodes during the iterative propagationprocess and contribute to the final performance tooas shown in our experiments (see Section 5.1).525Relevance evaluation To compute the weight ofa edge, Zhang et al (2014) evaluate the seman-tic relatedness between two nodes with a searchengine?based method.
They first convert everynode, which is a triple of SPO, into a query bycombining the text of Subject and Object together.Then for every pair of nodes tiand tj, they con-struct three queries: p, q, and p?
q, which corre-spond to the queries of ti, tj, and tj?
tj, the com-bination of tiand tj.
All these queries are put intoa search engine to get H(p), H(q), and H(p?
q),the numbers of returned pages for query p, p, andp?q.
Then the WebJaccard Coefficient (Bollegalaet al, 2007) is used to evaluate r(i, j), the related-ness between tiand tj, according to Formula 1.r(i, j) = WebJaccard(p,q) =??
?0 if H(p?q)?CH(p?q)H(p)+H(q)?H(p?q)otherwise.
(1)Using r(i, j), Zhang et al (2014) further definep(i, j), the probability of tiand tjpropagating toeach other, as shown in Formula 2.
Here N isthe set of all nodes, and ?
(i, j) denotes whetheran edge exists between two nodes or not.p(i, j) =r(i, j)??
(i, j)?n?Nr(n, j)??
(n, j)(2)Iterative propagation Considering that thesource document D is represented as a graph ofsd-nodes, so the relevance of background knowl-edge tbto D is naturally converted into that of tbtothe graph of sd-nodes.
Zhang et al (2014) evalu-ate this relevance by propagating relevance weightfrom sd-nodes to tbiteratively.
After convergence,the relevance weight of tbwill be treated as the fi-nal relevance to D. There are in total n?
n pairsof nodes, and their p(i, j) are stored in a matrix P.Zhang et al (2014) use~W =(w1,w2, .
.
.
,wn) to de-note the relevance weights of nodes, where wiin-dicates the relevance of tito D. At the beginning,each wiof bk-nodes is initialized to 0, and eachthat of sd-nodes is initialized to its importance toD.
Then~W is updated to~W?after every iterationaccording to Formula 3.
They keep updating theweights of both sd-nodes and bk-nodes until con-vergence and do not distinguish them explicitly.~W?=~W ?P=~W ????
?p(1,1) p(1,2) .
.
.
p(1,n)p(2,1) p(2,2) .
.
.
p(2,n).
.
.
.
.
.
.
.
.
.
.
.p(n,1) p(n,2) .
.
.
p(n,n)????
(3)3 MethodologyThe key idea behind this work is that every doc-ument is composed of several units of informa-tion, which can be extracted into triples automat-ically.
For every unit of background knowledgeb, the more units that are relevant to b and themore relevant they are, the more relevant b willbe to the source document.
Based on this intu-ition, we first present both source document infor-mation and background knowledge together as adocument-level triple graph as illustrated in Sec-tion 2.
Then we use LDA to capture the distribu-tional semantics of a triple by representing it as avector of distributional probabilities over k topicsand evaluate the relatedness between two tripleswith cosine-similarity.
Finally, we propose a mod-ified iterative process to propagate the relevancescore from the source document information to thebackground knowledge and select the top n rele-vant ones.3.1 Encoding distributional semanticsLDA LDA is a popular generative probabilisticmodel, which was first introduced by Blei et al(2003).
LDA views every document as a mixtureover underlying topics, and each topic as a distri-bution over words.
Both the document-topic andthe topic-word distributions are assumed to have aDirichlet prior.
Given a set of documents and anumber of topics, the model returns ?d, the topicdistribution for each document d, and ?z, the worddistribution for every topic z.LDA assumes the following generative processfor each document in a corpus D:1.
Choose N ?
Poisson(?
).2.
Choose ?
?
Dir(?).
(a) Choose a topic zn?Multinomial(?).
(b) Choose a word wnfrom p(wn|zn,? )
con-ditioned on the topic zn.Here the dimensionality k of the Dirichlet distribu-tion (and thus the dimensionality of the topic vari-526Figure 2: Graphical representation of LDA.
Theboxes represents replicates, where the inner boxrepresents the repeated choice of N topics andwords within a document, while the outer one rep-resents the repeated generation of M documents.able z) is assumed to be known and fixed; ?
is a k-dimensional Dirichlet random variable, where theparameter ?
is a k-vector with components ?i> 0;and the ?
indicates the word probabilities overtopics, which is a matrix with ?i j= p(wj= 1|zi=1).
Figure 2 shows the representation of LDA asa probabilistic graphical model with three levels.There are two corpus-level parameters ?
and ?
,which are assumed to be sampled once in the pro-cess of generating a corpus; one document-levelvariable ?d, which is sampled once per document;and two word-level variables zdnand wdn, whichare sampled once for each word in each document.We employ the publicly available implementa-tion of LDA, JGibbLDA21(Phan et al, 2008),which has two main execution methods: param-eter estimation (model building) and inference fornew data (classification of a new document).Relevance evaluation Given a set of documentsand the number of topics k, LDA will return ?z,the word distribution over the topic z.
So for everyword wn, we get k distributional probabilities overk topics.
We use pwnzito denote the probabilitythat wnappears in the ithtopic zi, where i?
k, zi?Z, the set of k topics.
Then we combine these kpossibilities together as a real-valued vector~vwntorepresent wnas shown in Formula 4.~vwn= (pwnz1, pwnz2, .
.
.
, pwnzk) (4)After getting the vectors of words, we employan intuitive method to compute the vector of atriple t, by accumulating all the correspondingvectors of words appearing in t according to For-mula 5.
Considering that the elements of thisnewly generated vector indicate the distributionalprobabilities of t over k topics, we then normalize1http://jgibblda.sourceforge.net/it according to Formula 6 so that its elements sumto 1.
This gives us ~vt, the real-valued vector oftriple t, which captures its distributional probabil-ities over k topics.
Here t corresponds to a tripleof background knowledge or of source document,ptziindicates the possibility of t to appear in the ithtopic zi, and wn?
t means that wnappears in t.ptzi=?wn?tpwnzi(5)~vt=(ptz1, ptz2, .
.
.
, ptzk)?ki=1ptzi(6)Using the vectors of triples, we can easily com-pute the semantic relatedness between a pair oftriples as their cosine-similarity according to For-mula 7.
Here A, B correspond to the real-valuedvectors of two triples, r(A,B) denotes their se-mantic relatedness, and k is the number of topics,which is also the length of A (or B).
A high valueof r(A,B) usually indicates a close relatedness be-tween A and B, and thus a higher probability ofpropagating to each other in the following modi-fied iterative propagation illustrated in Section 3.2.r(A,B) =cos(A,B) =AB?A??B?=?ki=1AiBi??ki=1(Ai)2?
?ki=1(Bi)2(7)3.2 Modified iterative propagationIn this part, we propose a modified iterative prop-agation based ranking model to select the most-relevant triples of background knowledge.
Thereare three primary modifications to the originalmodel of Zhang et al (2014), all of which areshown more powerful in our experiments.First of all, the original model (Zhang et al,2014) does not reset the relevance weight of sd-nodes after every iteration.
This results in a contin-ued decrease of the relevance weight of sd-nodes,which weakens the effect of sd-nodes during theiterative propagation and in turn affects the fi-nal performance.
To tackle this problem, we de-crease the relevance weight of bk-nodes and in-crease that of sd-nodes according to a fixed ratioafter every iteration, so as to ensure that the to-tal weight of sd-nodes is always higher than thatof bk-nodes.
Note that although the relevanceweights of bk-nodes are changed after the redis-tribution, the corresponding ranking of them is notchanged because the redistribution is carried out527J oh n L e nnon Y oko O no?
?B e at le ss d - nod ebk- nod e bk- nod eFigure 3: The edge between two bk-nodes helpsin the better evaluation of relatedness between thebk-node Yoko Ono and the sd-node Beatles.over all nodes accordingly.
In our experiments, wetried different ratios and finally chose 10:1, withsd-nodes corresponding to 10 and bk-nodes to 1,which achieved the best performance.In addition, we also modify the triple graph, therepresentation of a document illustrated in Section2, by connecting every pair of bk-nodes with anedge, which is not allowed in the original model.This modification was motivated by the intuitionthat the relatedness between bk-nodes also con-tributes to the better evaluation of relevance to thesource document, because the bk-nodes can serveas the intermediate nodes during the iterative prop-agation over the entire graph.
Figure 3 shows anexample, where the bk-node John Lennon is closeto both the sd-node Beatles and to another bk-node Yoko Ono, so the relatedness between twobk-nodes John Lennon and Yoko Ono helps in bet-ter evaluation of the relatedness between the bk-node Yoko Ono and the sd-node Beatles.We also modify the definition of p(i, j), theprobability of two nodes tiand tjpropagating toeach other.
Zhang et al (2014) compute this prob-ability according to Formula 2, which highlightsthe number of neighbors, but weakens the related-ness between nodes, due to the normalization.
Forinstance, if a node txhas only one neighbor ty, nomatter how low their relatedness is, their p(x,y)will still be equal to 1 in the original model, whileanother node with two equally but closely relatedneighbors will only get a probability of 0.5 foreach neighbor.
We modify this setup by removingthe normalization process and computing p(i, j) asthe relatedness between tiand tjdirectly, which isevaluated according to Formula 1 .4 Encoding background knowledge intodocument classificationIn this part, we demonstrate that the introductionof relevant knowledge could be helpful to realNLP applications.
In particular, we choose thedocument classification task as a demonstration,which aims to classify documents into predefinedcategories automatically (Sebastiani, 2002).
Wechoose this task for two reasons: (1) This taskhas witnessed a booming interest in the last 20years, due to the increased availability of docu-ments in digital form and the ensuing need to orga-nize them, so it is important in both research andapplication.
(2) The state-of-the-art performanceof this task is achieved by a series of topic model?based methods, which rely on the same model aswe do, but make use of source document informa-tion only.
However, there is always some omittedinformation and relevant knowledge, which can-not be captured from the source document.
In-tuitively, the recovery of this information will behelpful.
If we can improve the performance by in-troducing extra background knowledge into exist-ing framework of document classification, we caninference naturally that the improvement benefitsfrom the introduction of this knowledge.Traditional methods primarily use topic modelsto represent a document as a topic vector.
Then aSVM classifier takes this vector as input and out-puts the class of the document.
In this work, wepropose a new framework for document classifica-tion to incorporate extra knowledge.
Given a doc-ument to be classified, we select the top N most-relevant triples of background knowledge with ourmodel introduced in Section 3, all of which arerepresented as vectors of ~vt= (ptz1, ptz2, .
.
.
, ptzk).Then we combine these N triples as a new vec-tor~v?t, which is then incorporated into the originalframework of document classification.
AnotherSVM classifier takes ~v?t, together with the originalfeatures extracted from the source document, asinput and outputs the category of the source doc-ument.
To combine N triples as one, we employan intuitive method by computing the average ofN corresponding vectors in every dimension.One possible problem is how to decide N, thenumber of triples to be introduced.
We first intro-duce a fixed amount of triples for every document.Moreover, we also select the triples according totheir relevance weight to the source document (seeSection 3.2) by setting a threshold of relevanceweight first and selecting the triples whose weightsare higher than the threshold.
We further discussthe impact of different thresholds in Section 5.2.5285 ExperimentsTo evaluate our model, we conduct two seriesof experiments: (1) We first treat this task as aranking problem, which takes a document as in-put and outputs the ranked triples of backgroundknowledge, and evaluate the ranking performanceby computing the scores of MAP and P@N. (2)We also conduct a task-based evaluation, wheredocument classification (see Section 4) is chosenas a demonstration, by enriching the backgroundknowledge to the original framework as additionalfeatures and performing a direct comparison.5.1 Evaluation as a ranking problemData preparation The data is composed of twoparts: source documents and background knowl-edge.
For source documents, we use a publiclyavailable Chinese corpus which consists of 17,199documents and 13,719,428 tokens extracted fromInternet news2including 9 topics: Finance, IT,Health, Sports, Travel, Education, Jobs, Art, Mil-itary.
We then randomly but equally select 600articles as the set of source documents from 9 top-ics without data bias.
We use all the other 16,599documents of the same corpus as the source ofbackground knowledge, and then introduce a well-known Chinese open source tool (Che et al, 2010)to extract the triples of background knowledgefrom the raw text automatically.
So the back-ground knowledge also distributes evenly acrossthe same 9 topics.
We use the same tool to extractthe triples of source documents too.Baseline systems As Zhang et al (2014) argued,it is difficult to use the methods in traditionalranking tasks, such as information retrieval (Man-ning et al, 2008) and entity linking (Han et al,2011; Sen, 2012), as baselines in this task, becauseour model takes triples as basic input and thuslacks some crucial information such as link struc-ture.
For better comparison, we implement threemethods as baselines, which have been proved ef-fective in relevance evaluation: (1) Vector SpaceModel (VSM), (2) Word Embedding (WE), and(3) Latent Dirichlet Allocation (LDA).
Note thatour model captures the distributional semantics oftriples with LDA, while WE serves as a baselineonly, where the word embeddings are acquiredover the same corpus mentioned previously with2http://www.sogou.com/labs/dl/c.htmlthe publicly available tool word2vec3.Here we use ti, D, and wito denote a triple ofbackground knowledge, a source document, andthe relevance of tito D. For VSM, we representboth tiand D with a tf-idf scheme first (Saltonand McGill, 1986) and compute wias their cosine-similarity.
For WE, we first convert both tiand thetriples extracted from D into real-valued vectorswith WE and then compute wiby accumulating allthe cosine-similarities between tiand every triplefrom D. For LDA, we represent tias a vector withour model introduced in Section 3.1 and get thevector of D directly with LDA.
Then we evaluatetheir relevance of tito D by computing the cosine-similarity of two corresponding vectors.Moreover, to determine whether our modifiediterative propagation is helpful or not, we alsocompare our full model (Ours) against a simpli-fied version without iterative propagation (Ours-S).
In Ours-S, we represent both tiand the triplesextracted from D as real-valued vectors with ourmodel introduced in Section 3.1.
Then we com-pute wiby accumulating all the cosine-similaritiesbetween tiand the triples extracted from D. For allthe baselines, we rank the triples of backgroundknowledge according to wi, their relevance to D.Experimental setup Previous research relies onmanual annotation to evaluate the ranking perfor-mance (Zhang et al, 2014), which costs a lot,and in which it is difficult to get high consistency.In this paper, we carry out an automatic evalua-tion.
The corpus we used consists of 9 differentclasses, from which we extract triples of back-ground knowledge.
So correspondingly, there willbe 9 sets of triples too.
Then we randomly select200 triples from every class and mix 200?
9 =1800 triples together as S, the set of triples ofbackground knowledge.
For every document Dto be enriched, our model selects the top N most-relevant triples from S and returns them to D asenrichments.
We treat a triple tiselected by ourmodel as positive only if tiis extracted from thesame class as D. We evaluate the performance ofour model with two well-known criteria in rankingproblem: MAP and P@N (Voorhees et al, 2005).Statistically significant differences of performanceare determined using the two-tailed paired t-testcomputed at a 95% confidence level based on theaverage performance per source document.3https://code.google.com/p/word2vec/529Model MAP 5 P@5 MAP 10 P@10VSM 0.4968 0.3435 0.4752 0.3841WE 0.4356 0.3354 0.4624 0.3841LDA 0.6134 0.4775 0.6071 0.5295Ours-S 0.5325 0.3762 0.5012 0.4054Ours 0.6494 0.5597 0.6338 0.5502Table 1: The performance evaluated as a rankingtask.
Here Ours corresponds to our full model,while Ours-S is a simplified version of our modelwithout iterative propagation (see Section 3.2).Results The performance of multiple models isshown in Table 1.
Overall, our full model Oursoutperforms all the baseline systems significantlyin every metric.
When evaluating the top 10 tripleswith the highest relevance weight, our frameworkoutperforms the best baseline LDA by 4.4% inMAP and by 3.91% in P@N. When evaluating thetop 5 triples, our framework performs even betterand significantly outperforms the best baseline by5.87% in MAP and by 17.21% in P@N.To analyze the results further, Ours-S, the sim-plified version of our model without iterativepropagation, outperforms two strong baselinesVSM and WE, which indicates the effectivenessof encoding distributional semantics.
However,the performance of this simplified model is not asgood as that of LDA, because Ours-S evaluates therelevance with simple accumulation, which failsto capture the relatedness between multiple triplesfrom the source document.
We tackle this prob-lem by incorporating the modified iterative propa-gation over the entire triple graph into Ours, whichachieves the best performance.
One possible prob-lem is why WE has a poor performance, the reasonof which lies in the setup of our evaluation, wherewe label positive and negative instances accordingto the class information of triples and documents.This is better fit for topic model?based methods.Discussion We further analyze the impact of thethree modifications we made to the original model(see Section 3.2).
We first focus on the impactof decreasing the relevance weight of bk-nodesand increasing that of sd-nodes after every itera-tion.
As mentioned previously, we change theirrelevance weight according to a fixed ratio, whichis important to the performance.
Figure 4 showsthe performance of models with different ratios.With any increase of the ratio, our model improvesits performance in every metric, which shows thell ll l l l l l l1 2 3 4 5 6 7 8 9 100.350.450.550.65Ratio (sd?nodes / bk?nodes)Valuel MAP_5P@5MAP_10P@10Figure 4: The performance of our model with dif-ferent ratios between sd-nodes and bk-nodes.effectiveness of this setup.
The performance re-mains stable from the value of 10:1, which is thuschosen as the final value in our experiments.
Wethen turn to the other two modifications about theedges between bk-nodes and the setup of propaga-tion probability.
Table 2 shows the performance ofour full model and the simplified models withoutthese two modifications.
With the edges betweenbk-nodes, our model improves the performance by1.48% in MAP 5 and by 1.82% in P@5.
With themodified iterative propagation, we achieve a evengreater improvement of 13.99% in MAP 5 and24.27% in P@5.
All these improvements are sta-tistically significant, which indicates the effective-ness of these modifications to the original model.Model MAP 5 P@5 MAP 10 P@10Full 0.6494 0.5597 0.6338 0.5502Full?bb 0.6399 0.5497 0.6254 0.5404Full?p 0.5697 0.4504 0.5485 0.4409Table 2: The performance of our full model (Full)and two simplified models without modifications:(1) without edges between bk-nodes (Full?bb),(2) without the newly proposed definition of prop-agation probability between nodes (Full?p).5.2 Task-based evaluationData preparation To carry out the task-basedevaluation, we use the same Chinese corpus as thatin previous experiments, which consists of 17,199documents extracted from Internet news in 9 top-ics.
We also use the same tool (Che et al, 2010) toextract triples of both source document and back-ground knowledge.
For every document D to beclassified, we first use our model to get the top Nmost-relevant triples to D, and then use them asextra features for the original model.
We conducta direct comparison between the models with and530Model P R FVSM+one-hot 0.8214 0.8146 0.8168VSM+tf-idf 0.8381 0.8333 0.8336LDA+SVM 0.8512 0.8422 0.8436LDA+SVM+Ours-S 0.8584 0.8489 0.8501LDA+SVM+Ours 0.8748 0.8689 0.8691Table 3: The performance of document classifica-tion with (LDA+SVM+Ours-S, LDA+SVM+Ours)and without (others) background knowledge.without background knowledge to evaluate the im-pact of introducing background knowledge.Baseline systems We first illustrate two base-lines without background knowledge based onVSM and LDA.
For VSM, the test document Dis represented as a bag of words, where the worddistribution over candidate topics is trained onthe same corpus mentioned previously.
Then weevaluate the similarity between D and a candi-date topic with cosine-similarity directly, wherethe topic with the highest similarity will be chosenas the final class.
We use two setups: (1) VSM-one-hot represents a word as 1 if it appears in adocument or topic, or 0 if not.
(2) VSM-tf-idf rep-resents a word as the value of tf-idf.
For LDA,we re-implement the state-of-the-art system as an-other baseline, which represents D as a topic vec-tor ~vdin the parameter estimation step, and thenintroduces a SVM classifier to take~vdas input anddecide the final class in the inference step.We also evaluate the impact of knowledge qual-ity by proposing two different models to introducebackground knowledge: our full model introducedin Section 3 (Ours), and a simplified version ofour model without iterative propagation (Ours-S).They have different performances on introducingbackground knowledge as shown in previous ex-periments (see Section 5.1).
We then conduct a di-rect comparison between the document classifica-tion models with these conditions, whose differingperformances demonstrates the impact of differentqualities of background knowledge on this task.Results Table 3 shows the results.
We use P, R, Fto evaluate the performance, which are computedas the micro-average over 9 topics.
Both modelswith background knowledge (LDA+SVM+Ours-S, LDA+SVM+Ours) outperform systems withoutknowledge, which shows that the introduction ofbackground knowledge helps in better classifica-ll l l l l l l lll6.0 6.2 6.4 6.6 6.8 7.00.850.860.87Threshold of relevance weightValuel PRFFigure 5: The performance of document classifica-tion models with different thresholds.
The knowl-edge whose relevance weight to the source docu-ment exceeds the threshold will be introduced asbackground knowledge.tion of documents.
The system with the simpli-fied version of our model without iterative prop-agation (LDA+SVM+Ours-S) achieves a F-valueof 0.8501, which outperforms the other baselineswithout knowledge too.
Moreover, the systemwith our full model (LDA+SVM+Ours) achievesthe best performance, a F-value of 0.8691, andoutperforms the best baseline LDA+SVM signif-icantly.
This shows that introducing better qual-ity of background knowledge is helpful to the bet-ter classification of documents.
Statistical signif-icance is also verified using the two-tailed pairedt-test computed at a 95% confidence level basedon the results of classification over the test set.Discussion One important question here is howmuch background knowledge to include.
As men-tioned in Section 4, we have tried two differentsolutions: (1) introducing a fixed amount of back-ground knowledge for every document, and (2)setting a threshold and selecting knowledge whoserelevance weight exceeds the threshold.
The re-sults are shown in Table 4, where the systemswith threshold outperform that with fixed amount,which shows that the threshold helps in better in-troduction of background knowledge.Model P R FOurs-S+Top5 0.8522 0.8444 0.8456Ours-S+ThreD 0.8584 0.8489 0.8501Ours+Top5 0.8769 0.8667 0.8677Ours+ThreD 0.8748 0.8689 0.8691Table 4: The performance of document classifica-tion with the full model (Ours) and the simplifiedmodel (Ours-S) to introduce knowledge.531We also evaluate the impact of different thresh-olds as shown in Figure 5.
The performance keepsimproving as the threshold increases up to 6.4 andbecomes steady from 6.4 to 6.7, while it begins todecline sharply from 6.7.
This is reasonable be-cause at the beginning, as the threshold increases,we recall more background knowledge and pro-vide more information.
However, with the furtherincrease of the threshold, we introduce more noise,which decreases the performance.
In our experi-ments, we choose 6.4 as the final threshold.6 Conclusion and Future WorkThis study encodes distributional semantics intothe triple-based background knowledge rankingmodel (Zhang et al, 2014) for better documentenrichment.
We first use LDA to represent ev-ery triple as a real-valued vector, which is used toevaluate the relatedness between triples, and thenpropose a modified iterative propagation model torank all the triples of background knowledge.
Forevaluation, we conduct two series of experiments:(1) evaluation as ranking problem, and (2) task-based evaluation, especially for document classifi-cation.
In the first set of experiments, our modeloutperforms multiple strong baselines based onVSM, LDA, and WE.
In the second set of exper-iments, our full model with background knowl-edge outperforms the state-of-the-art systems sig-nificantly.
Moreover, we also explore the impactof knowledge quality and show its importance.In our future work, we wish to explore a betterway to encode distributional semantics by propos-ing a modified LDA for better triples representa-tion.
In addition, we also want to explore the ef-fect of introducing background knowledge in con-junction with other NLP tasks, especially with dis-course parsing (Marcu, 2000; Pitler et al, 2009).AcknowledgmentsWe would like to thank our colleagues fortheir great help.
This work was partly sup-ported by National Natural Science Foundationof China via grant 61133012, the National 863Leading Technology Research Project via grant2015AA015407, and the National Natural ScienceFoundation of China Surface Project via grant61273321.ReferencesDavid M Blei, Andrew Y Ng, and Michael I Jordan.2003.
Latent Dirichlet alocation.
Journal of Ma-chine Learning Research, 3:993?1022.Danushka Bollegala, Yutaka Matsuo, and MitsuruIshizuka.
2007.
Measuring semantic similarity be-tween words using web search engines.
Proceedingsof the 16th International Conference on World WideWeb, 7:757?766.Volha Bryl, Claudio Giuliano, Luciano Serafini, andKateryna Tymoshenko.
2010.
Using backgroundknowledge to support coreference resolution.
InProceedings of the 2010 Conference on ECAI 2010:19th European Conference on Artificial Intelligence,volume 10, pages 759?764.Wanxiang Che, Zhenghua Li, and Ting Liu.
2010.
Ltp:A Chinese language technology platform.
In Pro-ceedings of the 23rd International Conference onComputational Linguistics: Demonstrations, pages13?16.
Association for Computational Linguistics.Silviu Cucerzan.
2007.
Large-scale named entitydisambiguation based on Wikipedia data.
In Pro-ceedings of the 2007 Joint Conference on EmpiricalMethods in Natural Language Processing and Com-putational Natural Language Learning, volume 7,pages 708?716.Oren Etzioni, Anthony Fader, Janara Christensen,Stephen Soderland, and Mausam Mausam.
2011.Open information extraction: The second genera-tion.
In Proceedings of the Twenty-Second inter-national joint conference on Artificial Intelligence-Volume Volume One, pages 3?10.
AAAI Press.Samah Fodeh, Bill Punch, and Pang-Ning Tan.
2011.On ontology-driven document clustering using coresemantic features.
Knowledge and Information Sys-tems, 28(2):395?421.Xianpei Han, Le Sun, and Jun Zhao.
2011.
Collectiveentity linking in web text: a graph-based method.
InProceedings of the 34th international ACM SIGIRconference on Research and development in Infor-mation Retrieval, pages 765?774.
ACM.Zhengyan He, Shujie Liu, Mu Li, Ming Zhou, LongkaiZhang, and Houfeng Wang.
2013.
Learning entityrepresentation for entity disambiguation.
Proceed-ings of the 51st Annual Meeting of the Associationfor Computational Linguistics (Volume 2: Short Pa-pers), August.Johannes Hoffart, Mohamed Amir Yosef, Ilaria Bor-dino, Hagen F?urstenau, Manfred Pinkal, Marc Span-iol, Bilyana Taneva, Stefan Thater, and GerhardWeikum.
2011.
Robust disambiguation of namedentities in text.
In Proceedings of the Conference onEmpirical Methods in Natural Language Process-ing, pages 782?792.
Association for ComputationalLinguistics.532Johannes Hoffart, Fabian M Suchanek, KlausBerberich, and Gerhard Weikum.
2013.
Yago2: aspatially and temporally enhanced knowledge basefrom Wikipedia.
Artificial Intelligence, 194:28?61.Xiaohua Hu, Xiaodan Zhang, Caimei Lu, Eun K Park,and Xiaohua Zhou.
2009.
Exploiting Wikipediaas external knowledge for document clustering.
InProceedings of the 15th International Conference onKnowledge Discovery and Data Mining, pages 389?396.
ACM.Saurabh S Kataria, Krishnan S Kumar, Rajeev R Ras-togi, Prithviraj Sen, and Srinivasan H Sengamedu.2011.
Entity disambiguation with hierarchical topicmodels.
In Proceedings of the 17th InternationalConference on Knowledge Discovery and Data Min-ing, pages 1037?1045.
ACM.Sayali Kulkarni, Amit Singh, Ganesh Ramakrishnan,and Soumen Chakrabarti.
2009.
Collective annota-tion of Wikipedia entities in web text.
In Proceed-ings of the 15th International Conference on Knowl-edge Discovery and Data Mining, pages 457?466.ACM.Yogan Jaya Kumar and Naomie Salim.
2012.
Au-tomatic multi document summarization approaches.Journal of Computer Science, 8(1).Christopher D Manning, Prabhakar Raghavan, andHinrich Sch?utze.
2008.
Introduction to informationretrieval, volume 1.
Cambridge University PressCambridge.Daniel Marcu.
2000.
The rhetorical parsing of unre-stricted texts: A surface-based approach.
Computa-tional Linguistics, 26(3):395?448.Vivi Nastase, Michael Strube, Benjamin B?orschinger,C?acilia Zirn, and Anas Elghafari.
2010.
Wikinet: Avery large scale multi-lingual concept network.
InProceeding of the 7th International Conference onLanguage Resources and Evaluation.Patrick Pantel and Ariel Fuxman.
2011.
Jigs and lures:Associating web queries with structured entities.
InProceedings of the 49th Annual Meeting of the Asso-ciation for Computational Linguistics: Human Lan-guage Technologies-Volume 1, pages 83?92.
Associ-ation for Computational Linguistics.Xuan-Hieu Phan, Le-Minh Nguyen, and SusumuHoriguchi.
2008.
Learning to classify short andsparse text & web with hidden topics from large-scale data collections.
In Proceedings of the 17thInternational Conference on World Wide Web, pages91?100.
ACM.Emily Pitler, Annie Louis, and Ani Nenkova.
2009.Automatic sense prediction for implicit discourse re-lations in text.
In Proceedings of the Joint Confer-ence of the 47th Annual Meeting of the ACL and the4th International Joint Conference on Natural Lan-guage Processing of the AFNLP: Volume 2-Volume2, pages 683?691.
Association for ComputationalLinguistics.Gerard Salton and Michael J. McGill.
1986.
Intro-duction to Modern Information Retrieval.
McGraw-Hill, Inc., New York, NY, USA.Jean-Paul Sansonnet and Franc?ois Bouchet.
2010.Extraction of agent psychological behaviors fromglosses of WordNet personality adjectives.
In Proc.of the 8th European Workshop on Multi-Agent Sys-tems (EUMAS10).Fabrizio Sebastiani.
2002.
Machine learning in au-tomated text categorization.
ACM Computing Sur-veys, 34(1):1?47, March.Prithviraj Sen. 2012.
Collective context-aware topicmodels for entity disambiguation.
In Proceedingsof the 21st International Conference on World WideWeb, pages 729?738.
ACM.Steffen Staab and Rudi Studer.
2009.
Handbook onOntologies.
Springer Publishing Company, Incor-porated, 2nd edition.Koun-Tem Sun, Yueh-Min Huang, and Ming-ChiLiu.
2011.
A WordNet-based near-synonyms andsimilar-looking word learning system.
EducationalTechnology & Society, 14(1):121?134.Ellen M Voorhees, Donna K Harman, et al 2005.TREC: Experiment and evaluation in informationretrieval, volume 63.
MIT press Cambridge.Muyu Zhang, Bing Qin, Ting Liu, and Mao Zheng.2014.
Triple based background knowledge rankingfor document enrichment.
In Proceedings of COL-ING 2014, the 25th International Conference onComputational Linguistics: Technical Papers, pages917?927, Dublin, Ireland, August.
Dublin City Uni-versity and Association for Computational Linguis-tics.Zhicheng Zheng, Fangtao Li, Minlie Huang, and Xi-aoyan Zhu.
2010.
Learning to link entities withknowledge base.
In Human Language Technolo-gies: The 2010 Annual Conference of the NorthAmerican Chapter of the Association for Computa-tional Linguistics, pages 483?491.
Association forComputational Linguistics.533
