Transactions of the Association for Computational Linguistics, 1 (2013) 165?178.
Action Editor: David Chiang.Submitted 11/2012; Revised 3/2013; Published 5/2013.
c?2013 Association for Computational Linguistics.Learning to translate with products of novices: a suite of open-endedchallenge problems for teaching MTAdam Lopez1, Matt Post1, Chris Callison-Burch1,2, Jonathan Weese, Juri Ganitkevitch,Narges Ahmidi, Olivia Buzek, Leah Hanson, Beenish Jamil, Matthias Lee, Ya-Ting Lin,Henry Pao, Fatima Rivera, Leili Shahriyari, Debu Sinha, Adam Teichert,Stephen Wampler, Michael Weinberger, Daguang Xu, Lin Yang, and Shang Zhao?Department of Computer Science, Johns Hopkins University1Human Language Technology Center of Excellence, Johns Hopkins University2Computer and Information Science Department, University of PennsylvaniaAbstractMachine translation (MT) draws from severaldifferent disciplines, making it a complex sub-ject to teach.
There are excellent pedagogicaltexts, but problems in MT and current algo-rithms for solving them are best learned bydoing.
As a centerpiece of our MT course,we devised a series of open-ended challengesfor students in which the goal was to im-prove performance on carefully constrainedinstances of four key MT tasks: alignment,decoding, evaluation, and reranking.
Studentsbrought a diverse set of techniques to the prob-lems, including some novel solutions whichperformed remarkably well.
A surprising andexciting outcome was that student solutionsor their combinations fared competitively onsome tasks, demonstrating that even newcom-ers to the field can help improve the state-of-the-art on hard NLP problems while simulta-neously learning a great deal.
The problems,baseline code, and results are freely available.1 IntroductionA decade ago, students interested in natural lan-guage processing arrived at universities having beenexposed to the idea of machine translation (MT)primarily through science fiction.
Today, incomingstudents have been exposed to services like GoogleTranslate since they were in secondary school or ear-lier.
For them, MT is science fact.
So it makes senseto teach statistical MT, either on its own or as a unit?
The first five authors were instructors and the remaining au-thors were students in the worked described here.
This researchwas conducted while Chris Callison-Burch was at Johns Hop-kins University.in a class on natural language processing (NLP), ma-chine learning (ML), or artificial intelligence (AI).
Acourse that promises to show students how GoogleTranslate works and teach them how to build some-thing like it is especially appealing, and several uni-versities and summer schools now offer such classes.There are excellent introductory texts?dependingon the level of detail required, instructors can choosefrom a comprehensive MT textbook (Koehn, 2010),a chapter of a popular NLP textbook (Jurafsky andMartin, 2009), a tutorial survey (Lopez, 2008), oran intuitive tutorial on the IBM Models (Knight,1999b), among many others.But MT is not just an object of academic study.It?s a real application that isn?t fully perfected, andthe best way to learn about it is to build an MT sys-tem.
This can be done with open-source toolkitssuch as Moses (Koehn et al 2007), cdec (Dyer etal., 2010), or Joshua (Ganitkevitch et al 2012), butthese systems are not designed for pedagogy.
Theyare mature codebases featuring tens of thousands ofsource code lines, making it difficult to focus ontheir core algorithms.
Most tutorials present themas black boxes.
But our goal is for students to learnthe key techniques in MT, and ideally to learn bydoing.
Black boxes are incompatible with this goal.We solve this dilemma by presenting studentswith concise, fully-functioning, self-contained com-ponents of a statistical MT system: word alignment,decoding, evaluation, and reranking.
Each imple-mentation consists of a na?
?ve baseline algorithm inless than 150 lines of Python code.
We assign themto students as open-ended challenges in which thegoal is to improve performance on objective eval-uation metrics as much as possible.
This settingmirrors evaluations conducted by the NLP research165community and by the engineering teams behindhigh-profile NLP projects such as Google Translateand IBM?s Watson.
While we designate specific al-gorithms as benchmarks for each task, we encour-age creativity by awarding more points for the bestsystems.
As additional incentive, we provide a web-based leaderboard to display standings in real time.In our graduate class on MT, students took a va-riety of different approaches to the tasks, in somecases devising novel algorithms.
A more exciting re-sult is that some student systems or combinations ofsystems rivaled the state of the art on some datasets.2 Designing MT Challenge ProblemsOur goal was for students to freely experiment withdifferent ways of solving MT problems on real data,and our approach consisted of two separable com-ponents.
First, we provided a framework that stripskey MT problems down to their essence so studentscould focus on understanding classic algorithms orinvent new ones.
Second, we designed incentivesthat motivated them to improve their solutions asmuch as possible, encouraging experimentation withapproaches beyond what we taught in class.2.1 Decoding, Reranking, Evaluation, andAlignment for MT (DREAMT)We designed four assignments, each correspondingto a real subproblem in MT: alignment, decoding,evaluation, and reranking.1 From the more generalperspective of AI, they emphasize the key problemsof unsupervised learning, search, evaluation design,and supervised learning, respectively.
In real MTsystems, these problems are highly interdependent,a point we emphasized in class and at the end of eachassignment?for example, that alignment is an exer-cise in parameter estimation for translation models,that model choice is a tradeoff between expressivityand efficient inference, and that optimal search doesnot guarantee optimal accuracy.
However, present-ing each problem independently and holding all elseconstant enables more focused exploration.For each problem we provided data, a na?
?ve solu-tion, and an evaluation program.
Following Bird etal.
(2008) and Madnani and Dorr (2008), we imple-mented the challenges in Python, a high-level pro-1http://alopez.github.io/dreamtgramming language that can be used to write veryconcise programs resembling pseudocode.2,3 By de-fault, each baseline system reads the test data andgenerates output in the evaluation format, so setuprequired zero configuration, and students could be-gin experimenting immediately.
For example, on re-ceipt of the alignment code, aligning data and eval-uating results required only typing:> align | gradeStudents could then run experiments within minutesof beginning the assignment.Three of the four challenges also included unla-beled test data (except the decoding assignment, asexplained in ?4).
We evaluated test results against ahidden key when assignments were submitted.2.2 Incentive DesignWe wanted to balance several pedagogical goals: un-derstanding of classic algorithms, free explorationof alternatives, experience with typical experimentaldesign, and unhindered collaboration.Machine translation is far from solved, so we ex-pected more than reimplementation of prescribed al-gorithms; we wanted students to really explore theproblems.
To motivate exploration, we made the as-signments competitive.
Competition is a powerfulforce, but must be applied with care in an educa-tional setting.4 We did not want the consequencesof ambitious but failed experiments to be too dire,and we did not want to discourage collaboration.For each assignment, we guaranteed a passinggrade for matching the performance of a specific tar-get alrithm.
Typically, the target was importantbut not state-of-the-art: we left substantial room forimprovement, and thus competition.
We told stu-dents the exact algorithm that produced the target ac-curacy (though we expected them to derive it them-selves based on lectures, notes, or literature).
Wedid not specifically require them to implement it, butthe guarantee of a passing grade provided a power-ful incentive for this to be the first step of each as-signment.
Submissions that beat this target receivedadditional credit.
The top five submissions receivedfull credit, while the top three received extra credit.2http://python.org3Some well-known MT systems have been implemented inPython (Chiang, 2007; Huang and Chiang, 2007).4Thanks to an anonymous reviewer for this turn of phrase.166This scheme provided strong incentive to continueexperimentation beyond the target alrithm.5For each assignment, students could form teamsof any size, under three rules: each team had to pub-licize its formation to the class, all team membersagreed to receive the same grade, and teams couldnot drop members.
Our hope was that these require-ments would balance the perceived competitive ad-vantage of collaboration against a reluctance to take(and thus support) teammates who did not contributeto the competitive effort.6 This strategy worked: outof sixteen students, ten opted to work collaborativelyon at least one assignment, always in pairs.We provided a web-based leaderboard that dis-played standings on the test data in real time, iden-tifying each submission by a pseudonymous han-dle known only to the team and instructors.
Teamscould upload solutions as often as they liked beforethe assignment deadline.
The leaderboard displayedscores of the default and target alrithms.
This in-centivized an early start, since teams could verifyfor themselves when they met the threshold for apassing grade.
Though effective, it also detractedfrom realism in one important way: it enabled hill-climbing on the evaluation metric.
In early assign-ments, we observed a few cases of this behavior,so for the remaining assignments, we modified theleaderboard so that changes in score would only bereflected once every twelve hours.
This strategytrades some amount of scientific realism for somemeasure of incentive, a strategy that has proveneffective in other pedagogical tools with real-timefeedback (Spacco et al 2006).To obtain a grade, teams were required to sub-mit their results, share their code privately with theinstructors, and publicly describe their experimen-tal process to the class so that everyone could learnfrom their collective effort.
Teams were free (but notrequired) to share their code publicly at any time.5Grades depend on institutional norms.
In our case, high gradesin the rest of class combined with matching all assignment tar-get alrithms would earn a B+; beating two target alrithmswould earn an A-; top five placement on any assignment wouldearn an A; and top three placement compensated for weakergrades in other course criteria.
Everyone who completed allfour assignments placed in the top five at least once.6The equilibrium point is a single team, though this team wouldstill need to decide on a division of labor.
One student contem-plated organizing this team, but decided against it.Some did so after the assignment deadline.3 The Alignment ChallengeThe first challenge was word alignment: given a par-allel text, students were challenged to produce word-to-word alignments with low alignment error rate(AER; Och and Ney, 2000).
This is a variant of aclassic assignment not just in MT, but in NLP gen-erally.
Klein (2005) describes a version of it, and weknow several other instructors who use it.7 In mostof these, the object is to implement IBM Model 1or 2, or a hidden Markov model.
Our version makesit open-ended by asking students to match or beat anIBM Model 1 baseline.3.1 DataWe provided 100,000 sentences of parallel data fromthe Canadian Hansards, totaling around two millionwords.8 This dataset is small enough to align ina few minutes with our implementation?enablingrapid experimentation?yet large enough to obtainreasonable results.
In fact, Liang et al(2006) reportalignment accuracy on data of this size that is withina fraction of a point of their accuracy on the com-plete Hansards data.
To evaluate, we used manualalignments of a small fraction of sentences, devel-oped by Och and Ney (2000), which we obtainedfrom the shared task resources organized by Mihal-cea and Pedersen (2003).
The first 37 sentencesof the corpus were development data, with manualalignments provided in a separate file.
Test data con-sisted of an additional 447 sentences, for which wedid not provide alignments.93.2 ImplementationWe distributed three Python programs with thedata.
The first, align, computes Dice?s coefficient(1945) for every pair of French and English words,then aligns every pair for which its value is above anadjustable threshold.
Our implementation (most of7Among them, Jordan Boyd-Graber, John DeNero, PhilippKoehn, and Slav Petrov (personal communication).8http://www.isi.edu/natural-language/download/hansard/9This invited the possibility of cheating, since alignments of thetest data are publicly available on the web.
We did not adver-tise this, but as an added safeguard we obfuscated the data bydistributing the test sentences randomly throughout the file.167Listing 1 The default aligner in DREAMT: thresh-olding Dice?s coefficient.for (f, e) in bitext:for f_i in set(f):f_count[f_i] += 1for e_j in set(e):fe_count[(f_i,e_j)] += 1for e_j in set(e):e_count[e_j] += 1for (f_i, e_j) in fe_count.keys():dice[(f_i,e_j)] = \2.0 * fe_count[(f_i, e_j)] / \(f_count[f_i] + e_count[e_j])for (f, e) in bitext:for (i, f_i) in enumerate(f):for (j, e_j) in enumerate(e):if dice[(f_i,e_j)] >= cutoff:print "%i-%i " % (i,j)which is shown in Listing 1) is quite close to pseu-docode, making it easy to focus on the algorithm,one of our pedagogical goals.
The grade programcomputes AER and optionally prints an alignmentgrid for sentences in the development data, showingboth human and automatic alignments.
Finally thecheck program verifies that the results representa valid solution, reporting an error if not?enablingstudents to diagnose bugs in their submissions.The default implementation enabled immediateexperimentation.
On receipt of the code, studentswere instructed to align the first 1,000 sentences andcompute AER using a simple command.> align -n 1000 | grade By varying thenumber of input sentences and the threshold for analignment, students could immediately see the effectof various parameters on alignment quality.We privately implemented IBM Model 1 (Brownet al 1993) as the target alrithm for a passinggrade.
We ran it for five iterations with Englishas the target language and French as the source.Our implementation did not use null alignmentor symmetrization?leaving out these common im-provements offered students the possibility of dis-covering them independently, and thereby rewarded.AER?1002030405060-16days-14days-12days-10days-8days-6days-4days-2daysdueFigure 1: Submission history for the alignment challenge.Dashed lines represent the default and baseline systemperformance.
Each colored line represents a student, andeach dot represents a submission.
For clarity, we showonly submissions that improved the student?s AER.3.3 Challenge ResultsWe received 209 submissions from 11 teams over aperiod of two weeks (Figure 1).
Everyone eventuallymatched or exceeded IBM Model 1 AER of 31.26.Most students implemented IBM Model 1, but wesaw many other solutions, indicating that many trulyexperimented with the problem:?
Implementing heuristic constraints to requirealignment of proper names and punctuation.?
Running the algorithm on stems rather than sur-face words.?
Initializing the first iteration of Model 1 withparameters estimated on the observed align-ments in the development data.?
Running Model 1 for many iterations.
Most re-searchers typically run Model 1 for five itera-tions or fewer, and there are few experimentsin the literature on its behavior over many iter-ations, as there are for hidden Markov modeltaggers (Johnson, 2007).
Our students carriedout these experiments, reporting runs of 5, 20,100, and even 2000 iterations.
No improve-ment was observed after 20 iterations.168?
Implementing various alternative approachesfrom the literature, including IBM Model 2(Brown et al 1993), competitive linking(Melamed, 2000), and smoothing (Moore,2004).One of the best solutions was competitive linkingwith Dice?s coefficient, modified to incorporate theobservation that alignments tend to be monotonic byrestricting possible alignment points to a window ofeight words around the diagonal.
Although simple,it acheived an AER of 18.41, an error reduction overModel 1 of more than 40%.The best score compares unfavorably against astate-of-the-art AER of 3.6 (Liu et al 2010).
Butunder a different view, it still represents a significantamount of progress for an effort taking just over twoweeks: on the original challenge from which we ob-tained the data (Mihalcea and Pedersen, 2003) thebest student system would have placed fifth out offifteen systems.
Consider also the combined effort ofall the students: when we trained a perceptron clas-sifier on the development data, taking each student?sprediction as a feature, we obtained an AER of 15.4,which would have placed fourth on the original chal-lenge.
This is notable since none of the systemsincorporated first-order dependencies on the align-ments of adjacent words, long noted as an impor-tant feature of the best alignment models (Och andNey, 2003).
Yet a simple system combination of stu-dent assignments is as effective as a hidden MarkovModel trained on a comparable amount of data (Ochand Ney, 2003).It is important to note that AER does not neces-sarily correlate with downstream performance, par-ticularly on the Hansards dataset (Fraser and Marcu,2007).
We used the conclusion of the assignment asan opportunity to emphasize this point.4 The Decoding ChallengeThe second challenge was decoding: given a fixedtranslation model and a set of input sentences, stu-dents were challenged to produce translations withthe highest model score.
This challenge introducedthe difficulties of combinatorial optimization undera deceptively simple setup: the model we providedwas a simple phrase-based translation model (Koehnet al 2003) consisting only of a phrase table and tri-gram language model.
Under this simple model, fora French sentence f of length I , English sentencee of length J , and alignment a where each elementconsists of a span in both e and f such that everyword in both e and f is aligned exactly once, theconditional probability of e and a given f is as fol-lows.10p(e, a|f) =??i,i?,j,j??
?ap(f i?i |ej?j )J+1?j=1p(ej |ej?1, ej?2)(1)To evaluate output, we compute the conditionalprobability of e as follows.p(e|f) =?ap(e, a|f) (2)Note that this formulation is different from the typ-ical Viterbi objective of standard beam search de-coders, which do not sum over all alignments, butapproximate p(e|f) by maxa p(e, a|f).
Though thecomputation in Equation 2 is intractable (DeNeroand Klein, 2008), it can be computed in a few min-utes via dynamic programming on reasonably shortsentences.
We ensured that our data met this crite-rion.
The corpus-level probability is then the prod-uct of all sentence-level probabilities in the data.The model includes no distortion limit or distor-tion model, for two reasons.
First, leaving out thedistortion model slightly simplifies the implementa-tion, since it is not necessary to keep track of the lastword translated in a beam decoder; we felt that thisdetail was secondary to understanding the difficultyof search over phrase permutations.
Second, it actu-ally makes the problem more difficult, since a simpledistance-based distortion model prefers translationswith fewer permutations; without it, the model mayeasily prefer any permutation of the target phrases,making even the Viterbi search problem exhibit itstrue NP-hardness (Knight, 1999a; Zaslavskiy et al2009).Since the goal was to find the translation with thehighest probability, we did not provide a held-outtest set; with access to both the input sentences and10For simplicity, this formula assumes that e is padded with twosentence-initial symbols and one sentence-final symbol, andignores the probability of sentence segmentation, which wetake to be uniform.169the model, students had enough information to com-pute the evaluation score on any dataset themselves.The difficulty of the challenge lies simply in findingthe translation that maximizes the evaluation.
In-deed, since the problem is intractable, even the in-structors did not know the true solution.114.1 DataWe chose 48 French sentences totaling 716 wordsfrom the Canadian Hansards to serve as test data.To create a simple translation model, we used theBerkeley aligner to align the parallel text from thefirst assignment, and extracted a phrase table usingthe method of Lopez (2007), as implemented in cdec(Dyer et al 2010).
To create a simple languagemodel, we used SRILM (Stolcke, 2002).4.2 ImplementationWe distributed two Python programs.
The first,decode, decodes the test data monotonically?using both the language model and translationmodel, but without permuting phrases.
The imple-mentation is completely self-contained with no ex-ternal dependencies: it implements both models anda simple stack decoding algorithm for monotonictranslation.
It contains only 122 lines of Python?orders of magnitude fewer than most full-featureddecoders.
To see its similarity to pseudocode, com-pare the decoding algorithm (Listing 2) with thepseudocode in Koehn?s (2010) popular textbook (re-produced here as Algorithm 1).
The second pro-gram, grade, computes the log-probability of a setof translations, as outline above.We privately implemented a simple stack decoderthat searched over permutations of phrases, similarto Koehn (2004).
Our implementation increased thecodebase by 44 lines of code and included param-eters for beam size, distortion limit, and the maxi-mum number of translations considered for each in-put phrase.
We posted a baseline to the leaderboardusing values of 50, 3, and 20 for these, respectively.11We implemented a version of the Lagrangian relaxation algo-rithm of Chang and Collins (2011), but found it difficult toobtain tight (optimal) solutions without iteratively reintroduc-ing all of the original constraints.
We suspect this is due tothe lack of a distortion penalty, which enforces a strong pref-erence towards translations with little reordering.
However,the solution found by this algorithm is only approximates theobjective implied by Equation 2, which sums over alignments.We also posted an oracle containing the most prob-able output for each sentence, selected from amongall submissions received so far.
The intent of thisoracle was to provide a lower bound on the best pos-sible output, giving students additional incentive tocontinue improving their systems.4.3 Challenge ResultsWe received 71 submissions from 10 teams (Fig-ure 2), again exhibiting variety of solutions.?
Implementation of greedy decoder which ateach step chooses the most probable translationfrom among those reachable by a single swapor retranslation (Germann et al 2001; Langlaiset al 2007).?
Inclusion of heuristic estimates of future cost.?
Implementation of a private oracle.
Some stu-dents observed that the ideal beam setting wasnot uniform across the corpus.
They ran theirdecoder under different settings, and then se-lected the most probable translation of eachsentence.Many teams who implemented the standard stackdecoding algorithm experimented heavily with itspruning parameters.
The best submission used ex-tremely wide beam settings in conjunction with areimplementation of the future cost estimate used inMoses (Koehn et al 2007).
Five of the submissionsbeat Moses using its standard beam settings after ithad been configured to decode with our model.We used this assignment to emphasize the im-portance of good models: the model score of thesubmissions was generally inversely correlated withBLEU, possibly because our simple model had nodistortion limits.
We used this to illustrate the differ-ence between model error and search error, includ-ing fortuitous search error (Germann et al 2001)made by decoders with less accurate search.5 The Evaluation ChallengeThe third challenge was evaluation: given a test cor-pus with reference translations and the output of sev-eral MT systems, students were challenged to pro-duce a ranking of the systems that closely correlatedwith a human ranking.170Listing 2 The default decoder in DREAMT: a stack decoder for monotonic translation.stacks = [{} for _ in f] + [{}]stacks[0][lm.begin()] = initial_hypothesisfor i, stack in enumerate(stacks[:-1]):for h in sorted(stack.itervalues(),key=lambda h: -h.logprob)[:alpha]:for j in xrange(i+1,len(f)+1):if f[i:j] in tm:for phrase in tm[f[i:j]]:logprob = h.logprob + phrase.logproblm_state = h.lm_statefor word in phrase.english.split():(lm_state, word_logprob) = lm.score(lm_state, word)logprob += word_logproblogprob += lm.end(lm_state) if j == len(f) else 0.0new_hypothesis = hypothesis(logprob, lm_state, h, phrase)if lm_state not in stacks[j] or \stacks[j][lm_state].logprob < logprob:stacks[j][lm_state] = new_hypothesiswinner = max(stacks[-1].itervalues(), key=lambda h: h.logprob)def extract_english(h):return "" if h.predecessor is None else "%s%s " %(extract_english(h.predecessor), h.phrase.english)print extract_english(winner)Algorithm 1 Basic stack decoding algorithm,adapted from Koehn (2010), p. 165.place empty hypothesis into stack 0for all stacks 0...n?
1 dofor all hypotheses in stack dofor all translation options doif applicable thencreate new hypothesisplace in stackrecombine with existing hypothesisprune stack if too big5.1 DataWe chose the English-to-German translation sys-tems from the 2009 and 2011 shared task at the an-nual Workshop for Machine Translation (Callison-Burch et al 2009; Callison-Burch et al 2011), pro-viding the first as development data and the secondas test data.
We chose these sets because BLEU(Papineni et al 2002), our baseline metric, per-formed particularly poorly on them; this left roomfor improvement in addition to highlighting somelog 10p(e|f)?C-1200-1250-1300-1350-1400-20days-18days-16days-14days-12days-10days-8days-6days-4days-2daysdueFigure 2: Submission history for the decoding challenge.The dotted green line represents the oracle over submis-sions.deficiencies of BLEU.
For each dataset we pro-vided the source and reference sentences along withanonymized system outputs.
For the developmentdata we also provided the human ranking of the sys-171tems, computed from pairwise human judgementsaccording to a formula recommended by Bojar et al(2011).125.2 ImplementationWe provided three simple Python programs:evaluate implements a simple ranking of the sys-tems based on position-independent word error rate(PER; Tillmann et al 1997), which computes a bag-of-words overlap between the system translationsand the reference.
The grade program computesSpearman?s ?
between the human ranking and anoutput ranking.
The check program simply ensuresthat a submission contains a valid ranking.We were concerned about hill-climbing on the testdata, so we modified the leaderboard to report newresults only twice a day.
This encouraged students toexperiment on the development data before postingnew submissions, while still providing intermittentfeedback.We privately implemented a version of BLEU,which obtained a correlation of 38.6 with the humanrankings, a modest improvement over the baselineof 34.0.
Our implementation underperforms the onereported in Callison-Burch et al(2011) since it per-forms no tokenization or normalization of the data.This also left room for improvement.5.3 Evaluation Challenge ResultsWe received 212 submissions from 12 teams (Fig-ure 3), again demonstrating a wide range of tech-niques.?
Experimentation with the maximum n-gramlength and weights in BLEU.?
Implementation of smoothed versions of BLEU(Lin and Och, 2004).?
Implementation of weighted F-measure to bal-ance both precision and recall.?
Careful normalization of the reference and ma-chine translations, including lowercasing andpunctuation-stripping.12This ranking has been disputed over a series of papers (Lopez,2012; Callison-Burch et al 2012; Koehn, 2012).
The paperwhich initiated the dispute, written by the first author, was di-rectly inspired by the experience of designing this assignment.Spearman?s?0.80.60.4-7days-6days-5days-4days-3days-2days-1daysdueFigure 3: Submission history for the evaluation chal-lenge.?
Implementation of several techniques used inAMBER (Chen and Kuhn, 2005).The best submission, obtaining a correlation of83.5, relied on the idea that the reference and ma-chine translation should be good paraphrases of eachother (Owczarzak et al 2006; Kauchak and Barzi-lay, 2006).
It employed a simple paraphrase sys-tem trained on the alignment challenge data, us-ing the pivot technique of Bannard and Callison-Burch (2005), and computing the optimal alignmentbetween machine translation and reference under asimple model in which words could align if theywere paraphrases.
When compared with the 20systems submitted to the original task from whichthe data was obtained (Callison-Burch et al 2011),this system would have ranked fifth, quite near thetop-scoring competitors, whose correlations rangedfrom 88 to 94.6 The Reranking ChallengeThe fourth challenge was reranking: given a test cor-pus and a large N -best list of candidate translationsfor each sentence, students were challenged to selecta candidate translation for each sentence to producea high corpus-level BLEU score.
Due to an errorour data preparation, this assignment had a simplesolution that was very difficult to improve on.
Nev-ertheless, it featured several elements that may beuseful for future courses.1726.1 DataWe obtained 300-best lists from a Spanish-Englishtranslation system built with the Joshua toolkit(Ganitkevitch et al 2012) using data and resourcesfrom the 2011 Workshop on Machine Translation(Callison-Burch et al 2011).
We provided 1989training sentences, consisting of source and refer-ence sentences along with the candidate translations.We also included a test set of 250 sentences, forwhich we provided only the source and candidatetranslations.
Each candidate translation included sixfeatures from the underlying translation system, outof an original 21; our hope was that students mightrediscover some features through experimentation.6.2 ImplementationWe conceived of the assignment as one in which stu-dents could apply machine learning or feature engi-neering to the task of reranking the systems, so weprovided several tools.
The first of these, learn,was a simple program that produced a vector offeature weights using pairwise ranking optimization(PRO; Hopkins and May, 2011), with a perceptronas the underlying learning algorithm.
A second,rerank, takes a weight vector as input and reranksthe sentences; both programs were designed to workwith arbitrary numbers of features.
The grade pro-gram computed the BLEU score on developmentdata, while check ensured that a test submissionis valid.
Finally, we provided an oracle program,which computed a lower bound on the achievableBLEU score on the development data using a greedyapproximation (Och et al 2004).
The leaderboardlikewise displayed an oracle on test data.
We didnot assign a target alrithm, but left the assignmentfully open-ended.6.3 Reranking Challenge OutcomeFor each assignment, we made an effort to createroom for competition above the target alrithm.However, we did not accomplish this in the rerank-ing challenge: we had removed most of the featuresfrom the candidate translations, in hopes that stu-dents might reinvent some of them, but we left onehighly predictive implicit feature in the data: therank order of the underlying translation system.
Stu-dents discovered that simply returning the first can-didate earned a very high score, and most of themquickly converged to this solution.
Unfortunately,the high accuracy of this baseline left little room foradditional competition.
Nevertheless, we were en-couraged that most students discovered this by acci-dent while attempting other strategies to rerank thetranslations.?
Experimentation with parameters of the PROalgorithm.?
Substitution of alternative learning algorithms.?
Implementation of a simplified minimumBayes risk reranker (Kumar and Byrne, 2004).Over a baseline of 24.02, the latter approach ob-tained a BLEU of 27.08, nearly matching the scoreof 27.39 from the underlying system despite an im-poverished feature set.7 Pedagogical OutcomesCould our students have obtained similar results byrunning standard toolkits?
Undoubtedly.
However,our goal was for students to learn by doing: theyobtained these results by implementing key MT al-gorithms, observing their behavior on real data, andimproving them.
This left them with much more in-sight into how MT systems actually work, and inthis sense, DREAMT was a success.
At the end ofclass, we requested written feedback on the designof the assignments.
Many commented positively onthe motivation provided by the challenge problems:?
The immediate feedback of the automatic grad-ing was really nice.?
Fast feedback on my submissions and my rela-tive position on the leaderboard kept me bothmotivated to start the assignments early and toconstantly improve them.
Also knowing howwell others were doing was a good way togauge whether I was completely off track or notwhen I got bad results.?
The homework assignments were very engag-ing thanks to the clear yet open-ended setupand their competitive aspects.Students also commented that they learned a lotabout MT and even research in general:173Question 1 2 3 4 5 N/AFeedback on my work for this course is useful - - - 4 9 3This course enhanced my ability to work effectively in a team 1 - 5 8 2 -Compared to other courses at this level, the workload for this course is high - 1 7 6 1 1Table 1: Response to student survey questions on a Likert scale from 1 (strongly disagree) to 5 (strongly agree).?
I learned the most from the assignments.?
The assignments always pushed me one stepmore towards thinking out loud how the par-ticular task can be completed.?
I appreciated the setup of the homework prob-lems.
I think it has helped me learn how toset up and attack research questions in an or-ganized way.
I have a much better sense forwhat goes into an MT system and what prob-lems aren?t solved.We also received feedback through an anonymoussurvey conducted at the end of the course beforeposting final grades.
Each student rated aspectsof the course on a five point Likert scale, from 1(strongly disagree) to 5 (strongly agree).
Severalquestions pertained to assignments (Table 1), and al-lay two possible concerns about competition: moststudents felt that the assignments enhanced their col-laborative skills, and that their open-endedness didnot result in an overload of work.
For all surveyquestions, student satisfaction was higher than av-erage for courses in our department.8 DiscussionDREAMT is inspired by several different ap-proaches to teaching NLP, AI, and computer sci-ence.
Eisner and Smith (2008) teach NLP usinga competitive game in which students aim to writefragments of English grammar.
Charniak et al(2000) improve the state-of-the-art in a reading com-prehension task as part of a group project.
Christo-pher et al(1993) use NACHOS, a classic tool forteaching operating systems by providing a rudimen-tary system that students then augment.
DeNero andKlein (2010) devise a series of assignments basedon Pac-Man, for which students implement severalclassic AI techniques.
A crucial element in such ap-proaches is a highly functional but simple scaffold-ing.
The DREAMT codebase, including grading andvalidation scripts, consists of only 656 lines of code(LOC) over four assignments: 141 LOC for align-ment, 237 LOC for decoding, 86 LOC for evalua-tion, and 192 LOC for reranking.
To simplify imple-mentation further, the optional leaderboard could bedelegated to Kaggle.com, a company that organizesmachine learning competitions using a model sim-ilar to the Netflix Challenge (Bennet and Lanning,2007), and offers pro bono use of its services foreducational challenge problems.
A recent machinelearning class at Oxford hosted its assignments onKaggle (Phil Blunsom, personal communication).We imagine other uses of DREAMT.
It could beused in an inverted classroom, where students viewlecture material outside of class and work on prac-tical problems in class.
It might also be useful inmassive open online courses (MOOCs).
In this for-mat, course material (primarily lectures and quizzes)is distributed over the internet to an arbitrarily largenumber of interested students through sites such ascoursera.org, udacity.com, and khanacademy.org.
Inmany cases, material and problem sets focus on spe-cific techniques.
Although this is important, there isalso a place for open-ended problems on which stu-dents apply a full range of problem-solving skills.Automatic grading enables them to scale easily tolarge numbers of students.On the scientific side, the scale of MOOCs mightmake it possible to empirically measure the effec-tiveness of hands-on or competitive assignments,by comparing course performance of students whowork on them against that of those who do not.Though there is some empirical work on competi-tive assignments in the computer science educationliterature (Lawrence, 2004; Garlick and Akl, 2006;Regueras et al 2008; Ribeiro et al 2009), theygenerally measure student satisfaction and retentionrather than the more difficult question of whethersuch assignments actually improve student learning.However, it might be feasible to answer such ques-174tions in large, data-rich virtual classrooms offeredby MOOCs.
This is an interesting potential avenuefor future work.Because our class came within reach of state-of-the-art on each problem within a matter of weeks,we wonder what might happen with a very largebody of competitors.
Could real innovation oc-cur?
Could we solve large-scale problems?
It maybe interesting to adopt a different incentive struc-ture, such as one posed by Abernethy and Frongillo(2011) for crowdsourcing machine learning prob-lems: rather than competing, everyone works to-gether to solve a shared task, with credit awardedproportional to the contribution that each individualmakes.
In this setting, everyone stands to gain: stu-dents learn to solve problems as they are found inthe real world, instructors learn new insights into theproblems they pose, and, in the long run, users ofAI technology benefit from overall improvements.Hence it is possible that posing open-ended, real-world problems to students might be a small pieceof the puzzle of providing high-quality NLP tech-nologies.AcknowledgmentsWe are grateful to Colin Cherry and Chris Dyerfor testing the assignments in different settings andproviding valuable feedback, and to Jessie Youngfor implementing a dual decomposition solution tothe decoding assignment.
We thank Jason Eis-ner, Frank Ferraro, Yoav Goldberg, Matt Gormley,Ann Irvine, Rebecca Knowles, Ben Mitchell, Court-ney Napoles, Michael Rushanan, Joanne Selinski,Svitlana Volkova, and the anonymous reviewers forlively discussion and helpful comments on previousdrafts of this paper.
Any errors are our own.ReferencesJ.
Abernethy and R. M. Frongillo.
2011.
A collaborativemechanism for crowdsourcing prediction problems.
InProc.
of NIPS.C.
Bannard and C. Callison-Burch.
2005.
Paraphrasingwith bilingual parallel corpora.
In Proc.
of ACL.J.
Bennet and S. Lanning.
2007.
The netflix prize.
InProc.
of the KDD Cup and Workshop.S.
Bird, E. Klein, E. Loper, and J. Baldridge.
2008.Multidisciplinary instruction with the natural languagetoolkit.
In Proc.
of Workshop on Issues in TeachingComputational Linguistics.O.
Bojar, M.
Ercegovc?evic?, M. Popel, and O. Zaidan.2011.
A grain of salt for the WMT manual evaluation.In Proc.
of WMT.P.
E. Brown, S. A. D. Pietra, V. J. D. Pietra, and R. L.Mercer.
1993.
The mathematics of statistical machinetranslation: Parameter estimation.
Computational Lin-guistics, 19(2).C.
Callison-Burch, P. Koehn, C. Monz, and J. Schroeder.2009.
Findings of the 2009 workshop on statisticalmachine translation.
In Proc.
of WMT.C.
Callison-Burch, P. Koehn, C. Monz, and O. Zaidan.2011.
Findings of the 2011 workshop on statisticalmachine translation.
In Proc.
of WMT.C.
Callison-Burch, P. Koehn, C. Monz, M. Post, R. Sori-cut, and L. Specia.
2012.
Findings of the 2012 work-shop on statistical machine translation.
In Proc.
ofWMT.Y.-W. Chang and M. Collins.
2011.
Exact decoding ofphrase-based translation models through Lagrangianrelaxation.
In Proc.
of EMNLP.E.
Charniak, Y. Altun, R. de Salvo Braz, B. Garrett,M.
Kosmala, T. Moscovich, L. Pang, C. Pyo, Y. Sun,W.
Wy, Z. Yang, S. Zeiler, and L. Zorn.
2000.
Read-ing comprehension programs in a statistical-language-processing class.
In Proc.
of Workshop on Read-ing Comprehension Tests as Evaluation for Computer-Based Language Understanding Systems.B.
Chen and R. Kuhn.
2005.
AMBER: A modifiedBLEU, enhanced ranking metric.
In Proc.
of WMT.D.
Chiang.
2007.
Hierarchical phrase-based translation.Computational Linguistics, 33(2).W.
A. Christopher, S. J. Procter, and T. E. Anderson.1993.
The nachos instructional operating system.
InProc.
of USENIX.J.
DeNero and D. Klein.
2008.
The complexity of phrasealignment problems.
In Proc.
of ACL.J.
DeNero and D. Klein.
2010.
Teaching introductoryarticial intelligence with Pac-Man.
In Proc.
of Sym-posium on Educational Advances in Artificial Intelli-gence.L.
R. Dice.
1945.
Measures of the amount of ecologicassociation between species.
Ecology, 26(3):297?302.C.
Dyer, A. Lopez, J. Ganitkevitch, J. Weese, F. Ture,P.
Blunsom, H. Setiawan, V. Eidelman, and P. Resnik.2010.
cdec: A decoder, alignment, and learningframework for finite-state and context-free translationmodels.
In Proc.
of ACL.J.
Eisner and N. A. Smith.
2008.
Competitive grammarwriting.
In Proc.
of Workshop on Issues in TeachingComputational Linguistics.175A.
Fraser and D. Marcu.
2007.
Measuring word align-ment quality for statistical machine translation.
Com-putational Linguistics, 33(3).J.
Ganitkevitch, Y. Cao, J. Weese, M. Post, andC.
Callison-Burch.
2012.
Joshua 4.0: Packing, PRO,and paraphrases.
In Proc.
of WMT.R.
Garlick and R. Akl.
2006.
Intra-class competitiveassignments in CS2: A one-year study.
In Proc.
ofInternational Conference on Engineering Education.U.
Germann, M. Jahr, K. Knight, D. Marcu, and K. Ya-mada.
2001.
Fast decoding and optimal decoding formachine translation.
In Proc.
of ACL.L.
Huang and D. Chiang.
2007.
Forest rescoring: Fasterdecoding with integrated language models.
In Proc.
ofACL.M.
Johnson.
2007.
Why doesn?t EM find good HMMPOS-taggers?
In Proc.
of EMNLP.D.
Jurafsky and J. H. Martin.
2009.
Speech and Lan-guage Processing.
Prentice Hall, 2nd edition.D.
Kauchak and R. Barzilay.
2006.
Paraphrasing forautomatic evaluation.
In Proc.
of HLT-NAACL.D.
Klein.
2005.
A core-tools statistical NLP course.
InProc.
of Workshop on Effective Tools and Methodolo-gies for Teaching NLP and CL.K.
Knight.
1999a.
Decoding complexity in word-replacement translation models.
Computational Lin-guistics, 25(4).K.
Knight.
1999b.
A statistical MT tutorial workbook.P.
Koehn, F. J. Och, and D. Marcu.
2003.
Statisticalphrase-based translation.
In Proc.
of NAACL.P.
Koehn, H. Hoang, A. Birch, C. Callison-Burch,M.
Federico, N. Bertoldi, B. Cowan, W. Shen,C.
Moran, R. Zens, C. Dyer, O. Bojar, A. Constantin,and E. Herbst.
2007.
Moses: Open source toolkit forstatistical machine translation.
In Proc.
of ACL.P.
Koehn.
2004.
Pharaoh: a beam search decoder forphrase-based statistical machine translation models.In Proc.
of AMTA.P.
Koehn.
2010.
Statistical Machine Translation.
Cam-bridge University Press.P.
Koehn.
2012.
Simulating human judgment in machinetranslation evaluation campaigns.
In Proc.
of IWSLT.S.
Kumar and W. Byrne.
2004.
Minimum bayes-riskdecoding for statistical machine translation.
In Proc.of HLT-NAACL.P.
Langlais, A. Patry, and F. Gotti.
2007.
A greedy de-coder for phrase-based statistical machine translation.In Proc.
of TMI.R.
Lawrence.
2004.
Teaching data structures usingcompetitive games.
IEEE Transactions on Education,47(4).P.
Liang, B. Taskar, and D. Klein.
2006.
Alignment byagreement.
In Proc.
of NAACL.C.-Y.
Lin and F. J. Och.
2004.
ORANGE: a method forevaluating automatic evaluation metrics for machinetranslation.
In Proc.
of COLING.Y.
Liu, Q. Liu, and S. Lin.
2010.
Discriminative wordalignment by linear modeling.
Computational Lin-guistics, 36(3).A.
Lopez.
2007.
Hierarchical phrase-based translationwith suffix arrays.
In Proc.
of EMNLP.A.
Lopez.
2008.
Statistical machine translation.
ACMComputing Surveys, 40(3).A.
Lopez.
2012.
Putting human assessments of machinetranslation systems in order.
In Proc.
of WMT.N.
Madnani and B. Dorr.
2008.
Combining open-sourcewith research to re-engineer a hands-on introductoryNLP course.
In Proc.
of Workshop on Issues in Teach-ing Computational Linguistics.I.
D. Melamed.
2000.
Models of translational equiv-alence among words.
Computational Linguistics,26(2).R.
Mihalcea and T. Pedersen.
2003.
An evaluation ex-ercise for word alignment.
In Proc.
on Workshop onBuilding and Using Parallel Texts.R.
C. Moore.
2004.
Improving IBM word alignmentmodel 1.
In Proc.
of ACL.F.
J. Och and H. Ney.
2000.
Improved statistical align-ment models.
In Proc.
of ACL.F.
J. Och and H. Ney.
2003.
A systematic comparison ofvarious statistical alignment models.
ComputationalLinguistics, 29.F.
J. Och, D. Gildea, S. Khudanpur, A. Sarkar, K. Ya-mada, A. Fraser, S. Kumar, L. Shen, D. Smith, K. Eng,V.
Jain, Z. Jin, and D. Radev.
2004.
A smorgasbord offeatures for statistical machine translation.
In Proc.
ofNAACL.K.
Owczarzak, D. Groves, J. V. Genabith, and A. Way.2006.
Contextual bitext-derived paraphrases in auto-matic MT evaluation.
In Proc.
of WMT.K.
Papineni, S. Roukos, T. Ward, and W.-J.
Zhu.
2002.Bleu: a method for automatic evaluation of machinetranslation.
In Proc.
of ACL.L.
Regueras, E.
Verdu?, M.
Verdu?, M. Pe?rez, J. de Castro,and M. Mun?oz.
2008.
Motivating students throughon-line competition: An analysis of satisfaction andlearning styles.P.
Ribeiro, M. Ferreira, and H. Simo?es.
2009.
Teach-ing artificial intelligence and logic programming in acompetitive environment.
Informatics in Education,(Vol 8 1):85.J.
Spacco, D. Hovemeyer, W. Pugh, J. Hollingsworth,N.
Padua-Perez, and F. Emad.
2006.
Experiences withmarmoset: Designing and using an advanced submis-sion and testing system for programming courses.
InProc.
of Innovation and technology in computer sci-ence education.176A.
Stolcke.
2002.
SRILM - an extensible language mod-eling toolkit.
In Proc.
of ICSLP.C.
Tillmann, S. Vogel, H. Ney, A. Zubiaga, and H. Sawaf.1997.
Accelerated DP based search for statisticaltranslation.
In Proc.
of European Conf.
on SpeechCommunication and Technology.M.
Zaslavskiy, M. Dymetman, and N. Cancedda.
2009.Phrase-based statistical machine translation as a trav-eling salesman problem.
In Proc.
of ACL.177178
