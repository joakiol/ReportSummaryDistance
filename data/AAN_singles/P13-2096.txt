Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 538?542,Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational LinguisticsNeighbors Help: Bilingual Unsupervised WSD Using ContextSudha Bhingardive Samiulla Shaikh Pushpak BhattacharyyaDepartment of Computer Science and Engineering,IIT Bombay, Powai,Mumbai, 400076.
{sudha,samiulla,pb}@cse.iitb.ac.inAbstractWord Sense Disambiguation (WSD) is oneof the toughest problems in NLP, and inWSD, verb disambiguation has proved tobe extremely difficult, because of high de-gree of polysemy, too fine grained senses,absence of deep verb hierarchy and low in-ter annotator agreement in verb sense an-notation.
Unsupervised WSD has receivedwidespread attention, but has performedpoorly, specially on verbs.
Recently anunsupervised bilingual EM based algo-rithm has been proposed, which makesuse only of the raw counts of the transla-tions in comparable corpora (Marathi andHindi).
But the performance of this ap-proach is poor on verbs with accuracylevel at 25-38%.
We suggest a modifica-tion to this mentioned formulation, usingcontext and semantic relatedness of neigh-boring words.
An improvement of 17% -35% in the accuracy of verb WSD is ob-tained compared to the existing EM basedapproach.
On a general note, the workcan be looked upon as contributing to theframework of unsupervised WSD throughcontext aware expectation maximization.1 IntroductionThe importance of unsupervised approaches inWSD is well known, because they do not needsense tagged corpus.
In multilingual unsuper-vised scenario, either comparable or parallel cor-pora have been used by past researchers for disam-biguation (Dagan et al, 1991; Diab and Resnik,2002; Kaji and Morimoto, 2002; Specia et al,2005; Lefever and Hoste, 2010; Khapra et al,2011).
Recent work by Khapra et al, (2011) hasshown that, in comparable corpora, sense distribu-tion of a word in one language can be estimatedusing the raw counts of translations of the targetwords in the other language; such sense distribu-tions contribute to the ranking of senses.
Sincetranslations can themselves be ambiguous, Expec-tation Maximization based formulation is used todetermine the sense frequencies.
Using this ap-proach every instance of a word is tagged with themost probable sense according to the algorithm.In the above formulation, no importance isgiven to the context.
That would do, had the ac-curacy of disambiguation on verbs not been poor25-35%.
This motivated us to propose and inves-tigate use of context in the formulation by Khapraet al (2011).For example consider the sentence in chem-istry domain, ?Keep the beaker on the flat table.
?In this sentence, the target word ?table?
will betagged as ?the tabular array?
sense since it is dom-inant in the chemistry domain by their algorithm.But its actual sense is ?a piece of furniture?
whichcan be captured only if context is taken into con-sideration.
In our approach we tackle this problemby taking into account the words from the contextof the target word.
We use semantic relatednessbetween translations of the target word and thoseof its context words to determine its sense.Verb disambiguation has proved to be extremelydifficult (Jean, 2004), because of high degree ofpolysemy (Khapra et al, 2010), too fine grainedsenses, absence of deep verb hierarchy and low in-ter annotator agreement in verb sense annotation.On the other hand, verb disambiguation is veryimportant for NLP applications like MT and IR.Our approach has shown significant improvementin verb accuracy as compared to Khapra?s (2011)approach.The roadmap of the paper is as follows.
Sec-tion 2 presents related work.
Section 3 covers thebackground work.
Section 4 explains the modifiedEM formulation using context and semantic relat-edness.
Section 5 presents the experimental setup.538Results are presented in section 6.
Section 7 cov-ers phenomena study and error analysis.
Conclu-sions and future work are given in the last section,section 8.2 Related workWord Sense Disambiguation is one of the hard-est problems in NLP.
Successful supervised WSDapproaches (Lee et al, 2004; Ng and Lee, 1996)are restricted to resource rich languages and do-mains.
They are directly dependent on availabil-ity of good amount of sense tagged data.
Creat-ing such a costly resource for all language-domainpairs is impracticable looking at the amount oftime and money required.
Hence, unsupervisedWSD approaches (Diab and Resnik, 2002; Kajiand Morimoto, 2002; Mihalcea et al, 2004; Jean,2004; Khapra et al, 2011) attract most of the re-searchers.3 BackgroundKhapra et al (2011) dealt with bilingual unsuper-vised WSD.
It uses EM algorithm for estimatingsense distributions in comparable corpora.
Ev-ery polysemous word is disambiguated using theraw counts of its translations in different senses.Synset algned multilingual dictionary (Mohantyet al, 2008) is used for finding its translations.In this dictionary, synsets are linked, and afterthat the words inside the synsets are also linked.For example, for the concept of ?boy?, the Hindisynset {ladakaa, balak, bachhaa} is linked withthe Marathi synset {mulagaa, poragaa, por}.
TheMarathi word ?mulagaa?
is linked to the Hindiword ?ladakaa?
which is its exact lexical substi-tution.Suppose words u in language L1 and v in lan-guage L2 are translations of each other and theirsenses are required.
The EM based formulation isas follows:E-Step:P (SL1 |u) =?vP (piL2 (SL1)|v) ?
#(v)?SL1i?xP (piL2 (SL1i )|x) ?
#(x)where, SL1i ?
synsetsL1 (u)v ?
crosslinksL2 (u, SL1)x ?
crosslinksL2 (u, SL1i )M-Step:P (SL2 |v) =?uP (piL1 (SL2)|u) ?
#(u)?SL2i?yP (piL1 (SL2i )|y) ?
#(y)where, SL2i ?
synsetsL2 (v)u ?
crosslinksL1 (v, SL2)y ?
crosslinksL1 (v, SL2i )Here,?
?#?
indicates the raw count.?
crosslinksL1 (a, SL2) is the set of possibletranslations of the word ?a?
from language L1to L2 in the sense SL2 .?
piL2 (SL1) means the linked synset of thesense SL1 in L2.E and M steps are symmetric except for thechange in language.
In both the steps, we esti-mate sense distribution in one language using rawcounts of translations in another language.
Butthis approach has following limitations:Poor performance on verbs: This approach givespoor performance on verbs (25%-38%).
See sec-tion 6.Same sense throughout the corpus: Every oc-currence of a word is tagged with the single sensefound by the algorithm, throughout the corpus.Closed loop of translations: This formulationdoes not work for some common words whichhave the same translations in all senses.
For ex-ample, the verb ?karna?
in Hindi has two differ-ent senses in the corpus viz., ?to do?
(S1) and ?tomake?
(S2).
In both these senses, it gets trans-lated as ?karne?
in Marathi.
The word ?karne?
alsoback translates to ?karna?
in Hindi through both itssenses.
In this case, the formulation works out asfollows:The probabilities are initialized uniformly.Hence, P (S1|karna) = P (S2|karna) = 0.5.Now, in first iteration the sense of ?karne?
will beestimated as follows (E-step):P (S1|karne) =P (S1|karna) ?
#(karna)#(karna)= 0.5,539P (S2|karne) =P (S2|karna) ?
#(karna)#(karna)= 0.5Similarly, in M-step, we will get P (S1|karna) =P (S2|karna) = 0.5.
Eventually, it will end upwith initial probabilities and no strong decisioncan be made.To address these problems we have introducedcontextual clues in their formulation by using se-mantic relatedness.4 Modified Bilingual EM approachWe introduce context in the EM formulation statedabove and treat the context as a bag of words.
Weassume that each word in the context influencesthe sense of the target word independently.
Hence,p(S|w,C) =?ci?Cp(S|w, ci)where, w is the target word, S is one of the candi-date synsets of w, C is the set of words in context(sentence in our case) and ci is one of the contextwords.Suppose we would have sense tagged data,p(S|w, c) could have been computed as:p(S|w, c) = #(S,w, c)#(w, c)But since the sense tagged corpus is not avail-able, we cannot find #(S,w, c) from the corpusdirectly.
However, we can estimate it using thecomparable corpus in other language.
Here, weassume that given a word and its context wordin language L1, the sense distribution in L1 willbe same as that in L2 given the translation of aword and the translation of its context word in L2.But these translations can be ambiguous, hencewe can use Expectation Maximization approachsimilar to (Khapra et al, 2011) as follows:E-Step:P (SL1 |u, a) =?v,bP (piL2 (SL1)|v, b) ?
?
(v, b)?SL1i?x,bP (piL2 (SL1i )|x, b) ?
?
(x, b)where, SL1i ?
synsetsL1(u)a ?
context(u)v ?
crosslinksL2 (u, SL1)b ?
crosslinksL2 (a)x ?
crosslinksL2 (u, SL1i )crosslinksL1(a) is the set of all possible transla-tions of the word ?a?
from L1 to L2 in all its senses.?
(v, b) is the semantic relatedness between thesenses of v and senses of b.
Since, v and b go overall possible translations of u and a respectively.?
(v, b) has the effect of indirectly capturing thesemantic similarity between the senses of u anda.
A symetric formulation in the M-step belowtakes the computation back from language L2 tolanguage L1.
The semantic relatedness comes asan additional weighing factor, capturing context,in the probablistic score.M-Step:P (SL2 |v, b) =?u,aP (piL1 (SL2)|u, a) ?
?
(u, a)?SL2i?y,bP (piL1 (SL2i )|y, a) ?
?
(y, a)where, SL2i ?
synsetsL2 (v)b ?
context(v)u ?
crosslinksL1 (v, SL2)a ?
crosslinksL1(b)y ?
crosslinksL1 (v, SL2i )?
(u, a) is the semantic relatedness between thesenses of u and senses of a and contributes to thescore like ?
(v, b).Note how the computation moves back andforth between L1 and L2 considering translationsof both target words and their context words.In the above formulation, we could have con-sidered the term #(word, context word) (i.e.,the co-occurrence count of the translations ofthe word and the context word) instead of?
(word, context word).
But it is very unlikelythat every translation of a word will co-occur with540Algorithm HIN-HEALTH MAR-HEALTHNOUN ADV ADJ VERB Overall NOUN ADV ADJ VERB OverallEM-C 59.82 67.80 56.66 60.38 59.63 62.90 62.54 53.63 52.49 59.77EM 60.68 67.48 55.54 25.29 58.16 63.88 58.88 55.71 35.60 58.03WFS 53.49 73.24 55.16 38.64 54.46 59.35 67.32 38.12 34.91 52.57RB 32.52 45.08 35.42 17.93 33.31 33.83 38.76 37.68 18.49 32.45Table 1: Comparison(F-Score) of EM-C and EM for Health domainAlgorithm HIN-TOURISM MAR-TOURISMNOUN ADV ADJ VERB Overall NOUN ADV ADJ VERB OverallEM-C 62.78 65.10 54.67 55.24 60.70 59.08 63.66 58.02 55.23 58.67EM 61.16 62.31 56.02 31.85 57.92 59.66 62.15 58.42 38.33 56.90WFS 63.98 75.94 52.72 36.29 60.22 61.95 62.39 48.29 46.56 57.47RB 32.46 42.56 36.35 18.29 32.68 33.93 39.30 37.49 15.99 32.65Table 2: Comparison(F-Score) of EM-C and EM for Tourism domainevery translation of its context word considerablenumber of times.
This term may make sense onlyif we have arbitrarily large comparable corpus inthe other language.4.1 Computation of semantic relatednessThe semantic relatedness is computed by takingthe inverse of the length of the shortest path amongtwo senses in the wordnet graph (Pedersen et al,2005).
All the semantic relations (including cross-part-of-speech links) viz., hypernymy, hyponymy,meronymy, entailment, attribute etc., are used forcomputing the semantic relatedness.Sense scores thus obtained are used to disam-biguate all words in the corpus.
We consider allthe content words from the context for disam-biguation of a word.
The winner sense is the onewith the highest probability.5 Experimental setupWe have used freely available in-domain compa-rable corpora1 in Hindi and Marathi languages.These corpora are available for health and tourismdomains.
The dataset is same as that used in(Khapra et al, 2011) in order to compare the per-formance.6 ResultsTable 1 and Table 2 compare the performance ofthe following two approaches:1.
EM-C (EM with Context): Our modified ap-proach explained in section 4.2.
EM: Basic EM based approach by Khapra etal., (2011).1http://www.cfilt.iitb.ac.in/wsd/annotated corpus/3.
WFS: Wordnet First Sense baseline.4.
RB: Random baseline.Results clearly show that EM-C outperforms EMespecially in case of verbs in all language-domainpairs.
In health domain, verb accuracy is increasedby 35% for Hindi and 17% for Marathi, while intourism domain, it is increased by 23% for Hindiand 17% for Marathi.
The overall accuracy is in-creased by (1.8-2.8%) for health domain and (1.5-1.7%) for tourism domain.
Since there are lessnumber of verbs, the improved accuracy is not di-rectly reflected in the overall performance.7 Error analysis and phenomena studyOur approach tags all the instances of a word de-pending on its context as apposed to basic EM ap-proach.
For example, consider the following sen-tence from the tourism domain:vh p? ?l rh T?
(vaha patte khel rahe the)(They were playing cards/leaves)Here, the word p? (plural form of p?A) has twosenses viz., ?leaf?
and ?playing card?.
In tourismdomain, the ?leaf?
sense is more dominant.
Hence,basic EM will tag p? with ?leaf?
sense.
But it?strue sense is ?playing card?.
The true sense is cap-tured only if context is considered.
Here, the word?lnA (to play) (root form of ?l) endorses the?playing card?
sense of the word p?A.
This phe-nomenon is captured by our approach through se-mantic relatedness.But there are certain cases where our algorithmfails.
For example, consider the following sen-tence:541vh pX ? Enc p? ?l rh T?
(vaha ped ke niche patte khel rahe the)(They were playing cards/leaves below the tree)Here, two strong context words pX (tree) and?l (play) are influencing the sense of the wordp?.
Semantic relatedness between pX (tree) andp?A (leaf) is more than that of ?l (play) and p?A(playing card).
Hence, the ?leaf sense?
is assignedto p?A.This problem occurred because we consideredthe context as a bag of words.
This problem canbe solved by considering the semantic structureof the sentence.
In this example, the word p?A(leaf/playing card) is the subject of the verb ?lnA(to play) while pX (tree) is not even in the sameclause with p?A (leaf/playing cards).
Thus wecould consider ?lnA (to play) as the stronger cluefor its disambiguation.8 Conclusion and Future WorkWe have presented a context aware EM formula-tion building on the framework of Khapra et al(2011).
Our formulation solves the problems of?inhibited progress due to lack of translation diver-sity?
and ?uniform sense assignment, irrespectiveof context?
that the previous EM based formula-tion of Khapra et al suffers from.
More impor-tantly our accuracy on verbs is much higher andmore than the state of the art, to the best of ourknowledge.
Improving the performance on otherparts of speech is the primary future work.
Fu-ture directions also point to usage of semantic roleclues, investigation of familialy apart pair of lan-guages and effect of variation of measures of se-mantic relatedness.ReferencesIdo Dagan, Alon Itai, and Ulrike Schwall.
1991.
Twolanguages are more informative than one.
In Dou-glas E. Appelt, editor, ACL, pages 130?137.
ACL.Mona Diab and Philip Resnik.
2002.
An unsupervisedmethod for word sense tagging using parallel cor-pora.
In Proceedings of the 40th Annual Meetingon Association for Computational Linguistics, ACL?02, pages 255?262, Morristown, NJ, USA.
Associ-ation for Computational Linguistics.Ve?ronis Jean.
2004.
Hyperlex: Lexical cartographyfor information retrieval.
In Computer Speech andLanguage, pages 18(3):223?252.Hiroyuki Kaji and Yasutsugu Morimoto.
2002.
Unsu-pervised word sense disambiguation using bilingualcomparable corpora.
In Proceedings of the 19th in-ternational conference on Computational linguistics- Volume 1, COLING ?02, pages 1?7, Stroudsburg,PA, USA.
Association for Computational Linguis-tics.Mitesh M. Khapra, Anup Kulkarni, Saurabh Sohoney,and Pushpak Bhattacharyya.
2010.
All words do-main adapted wsd: Finding a middle ground be-tween supervision and unsupervision.
In Jan Ha-jic, Sandra Carberry, and Stephen Clark, editors,ACL, pages 1532?1541.
The Association for Com-puter Linguistics.Mitesh M Khapra, Salil Joshi, and Pushpak Bhat-tacharyya.
2011.
It takes two to tango: A bilingualunsupervised approach for estimating sense distribu-tions using expectation maximization.
In Proceed-ings of 5th International Joint Conference on Nat-ural Language Processing, pages 695?704, ChiangMai, Thailand, November.
Asian Federation of Nat-ural Language Processing.K.
Yoong Lee, Hwee T. Ng, and Tee K. Chia.
2004.Supervised word sense disambiguation with supportvector machines and multiple knowledge sources.In Proceedings of Senseval-3: Third InternationalWorkshop on the Evaluation of Systems for the Se-mantic Analysis of Text, pages 137?140.Els Lefever and Veronique Hoste.
2010.
Semeval-2010 task 3: cross-lingual word sense disambigua-tion.
In Katrin Erk and Carlo Strapparava, editors,SemEval 2010 : 5th International workshop on Se-mantic Evaluation : proceedings of the workshop,pages 15?20.
ACL.Rada Mihalcea, Paul Tarau, and Elizabeth Figa.
2004.Pagerank on semantic networks, with application toword sense disambiguation.
In COLING.Rajat Mohanty, Pushpak Bhattacharyya, PrabhakarPande, Shraddha Kalele, Mitesh Khapra, and AdityaSharma.
2008.
Synset based multilingual dic-tionary: Insights, applications and challenges.
InGlobal Wordnet Conference.Hwee Tou Ng and Hian Beng Lee.
1996.
Integratingmultiple knowledge sources to disambiguate wordsense: an exemplar-based approach.
In Proceedingsof the 34th annual meeting on Association for Com-putational Linguistics, pages 40?47, Morristown,NJ, USA.
ACL.T.
Pedersen, S. Banerjee, and S. Patwardhan.
2005.Maximizing Semantic Relatedness to Perform WordSense Disambiguation.
Research Report UMSI2005/25, University of Minnesota SupercomputingInstitute, March.Lucia Specia, Maria Das Grac?as, Volpe Nunes, andMark Stevenson.
2005.
Exploiting parallel texts toproduce a multilingual sense tagged corpus for wordsense disambiguation.
In In Proceedings of RANLP-05, Borovets, pages 525?531.542
