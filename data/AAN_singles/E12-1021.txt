Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 204?213,Avignon, France, April 23 - 27 2012. c?2012 Association for Computational LinguisticsIncorporating Lexical Priors into Topic ModelsJagadeesh JagarlamudiUniversity of MarylandCollege Park, USAjags@umiacs.umd.eduHal Daume?
IIIUniversity of MarylandCollege Park, USAhal@umiacs.umd.eduRaghavendra UdupaMicrosoft ResearchBangalore, Indiaraghavu@microsoft.comAbstractTopic models have great potential for help-ing users understand document corpora.This potential is stymied by their purely un-supervised nature, which often leads to top-ics that are neither entirely meaningful noreffective in extrinsic tasks (Chang et al2009).
We propose a simple and effectiveway to guide topic models to learn topicsof specific interest to a user.
We achievethis by providing sets of seed words that auser believes are representative of the un-derlying topics in a corpus.
Our modeluses these seeds to improve both topic-word distributions (by biasing topics to pro-duce appropriate seed words) and to im-prove document-topic distributions (by bi-asing documents to select topics related tothe seed words they contain).
Extrinsicevaluation on a document clustering taskreveals a significant improvement when us-ing seed information, even over other mod-els that use seed information na?
?vely.1 IntroductionTopic models such as Latent Dirichlet Allocation(LDA) (Blei et al 2003) have emerged as a pow-erful tool to analyze document collections in anunsupervised fashion.
When fit to a documentcollection, topic models implicitly use documentlevel co-occurrence information to group seman-tically related words into a single topic.
Since theobjective of these models is to maximize the prob-ability of the observed data, they have a tendencyto explain only the most obvious and superficialaspects of a corpus.
They effectively sacrifice per-formance on rare topics to do a better job in mod-eling frequently occurring words.
The user is thenleft with a skewed impression of the corpus, andperhaps one that does not perform well in extrin-sic tasks.To illustrate this problem, we ran LDA onthe most frequent five categories of the Reuters-21578 (Lewis et al 2004) text corpus.
This doc-ument distribution is very skewed: more than halfof the collection belongs to the most frequent cat-egory (?Earn?).
The five topics identified by theLDA are shown in Table 1.
A brief observationof the topics reveals that LDA has roughly allo-cated topics 1 & 2 for the most frequent class(?Earn?)
and one topic for the subsequent twofrequent classes (?Acquisition?
and ?Forex?)
andmerged the least two frequent classes (?Crude?and ?Grain?)
into a single topic.
The red coloredwords in topic 5 correspond to the ?Crude?
classand blue words are from the ?Grain?
class.This leads to the situation where the topicsidentified by LDA are not in accordance with theunderlying topical structure of the corpus.
Thisis a problem not just with LDA: it is potentiallya problem with any extension thereof that havefocused on improving the semantic coherence ofthe words in each topic (Griffiths et al 2005;Wallach, 2005; Griffiths et al 2007), the doc-ument topic distributions (Blei and McAuliffe,2008; Lacoste-Julien et al 2008) or other aspects(Blei.
and Lafferty., 2009).We address this problem by providing some ad-ditional information to the model.
Initially, alongwith the document collection, a user may providehigher level view of the document collection.
Forinstance, as discussed in Section 4.4, when runon historical NIPS papers, LDA fails to find top-ics related to Brain Imaging, Cognitive Science orHardware, even though we know from the call for204mln, dlrs, billion, year, pct, company, share, april, record, cts, quarter, march, earnings, stg, first, paymln, NUM, cts, loss, net, dlrs, shr, profit, revs, year, note, oper, avg, shrs, sales, includeslt, company, shares, corp, dlrs, stock, offer, group, share, common, board, acquisition, shareholdersbank, market, dollar, pct, exchange, foreign, trade, rate, banks, japan, yen, government, rates, todayoil, tonnes, prices, mln, wheat, production, pct, gas, year, grain, crude, price, corn, dlrs, bpd, opecTable 1: Topics identified by LDA on the frequent-5 categories of the Reuters corpus.
The categories are Earn,Acquisition, Forex, Grain and Crude (in the order document frequency).1 company, billion, quarter, shrs, earnings2 acquisition, procurement, merge3 exchange, currency, trading, rate, euro4 grain, wheat, corn, oilseed, oil5 natural, gas, oil, fuel, products, petrolTable 2: An example for sets of seed words (seed top-ics) for the frequent-5 categories of the Reuters-21578categorization corpus.
We use them as running exam-ple in the rest of the paper.papers that such topics should exist in the corpus.By allowing the user to provide some seed wordsrelated to these underrepresented topics, we en-courage the model to find evidence of these top-ics in the data.
Importantly, we only encouragethe model to follow the seed sets and do not forceit.
So if it has compelling evidence in the datato overcome the seed information then it still hasthe freedom to do so.
Our seeding approach incombination with the interactive topic modeling(Hu et al 2011) will allow a user to both explorea corpus, and also guide the exploration towardsthe distinctions that he/she finds more interesting.2 Incorporating SeedsOur approach to allowing a user to guide the topicdiscovery process is to let him provide seed infor-mation at the level of word type.
Namely, the userprovides sets of seed words that are representativeof the corpus.
Table 2 shows an example of seedsets one might use for the Reuters corpus.
Thiskind of supervision is similar to the seeding inbootstrapping literature (Thelen and Riloff, 2002)or prototype-based learning (Haghighi and Klein,2006).
Our reliance on seed sets is orthogonalto existing approaches that use external knowl-edge, which operate at the level of documents(Blei and McAuliffe, 2008), tokens (Andrzejew-ski and Zhu, 2009) or pair-wise constraints (An-drzejewski et al 2009).We build a model that uses the seed wordsin two ways: to improve both topic-word anddocument-topic probability distributions.
Forease of exposition, we present these ideas sep-arately and then in combination (Section 2.3).To improve topic-word distributions, we set upa model in which each topic prefers to gener-ate words that are related to the words in a seedset (Section 2.1).
To improve document-topicdistributions, we encourage the model to selectdocument-level topics based on the existence ofinput seed words in that document (Section 2.2).Before moving on to the details of our mod-els, we briefly recall the generative story of theLDA model and the reader is encouraged to referto (Blei et al 2003) for further details.1.
For each topic k = 1 ?
?
?
T,?
choose ?k ?
Dir(?).2.
For each document d, choose ?d ?
Dir(?).?
For each token i = 1 ?
?
?Nd:(a) Select a topic zi ?
Mult(?d).
(b) Select a word wi ?
Mult(?zi).where T is the number of topics, ?, ?
are hyper-parameters of the model and ?k and ?d are topic-word and document-topic Multinomial probabil-ity distributions respectively.2.1 Word-Topic Distributions (Model 1)In regular topic models, each topic k is definedby a Multinomial distribution ?k over words.
Weextend this notion and instead define a topic as amixture of two Multinomial distributions: a ?seedtopic?
distribution and a ?regular topic?
distribu-tion.
The seed topic distribution is constrained toonly generate words from a corresponding seedset.
The regular topic distribution may generateany word (including seed words).
For example,seed topic 4 (in Table 2) can only generate thefive words in its set.
The word ?oil?
can be gener-ated by seed topics 4 and 5, as well as any regular205?sT?rT?s1?r1docz=1 z=2 z=T?
?
?
?
?
?
?
?
?
?11 ?
?1 ?T1?
?TFigure 1: Tree representation of a document in Model1.topic.
We want to emphasize that, like any regulartopic, each seed topic is a non-uniform probabil-ity distribution over the words in its set.
The useronly inputs the sets of seed words and the modelwill infer their probability distributions.For the sake of simplicity, we describe ourmodel by assuming a one-to-one correspondencebetween seed and regular topics.
This assumptioncan be easily relaxed by duplicating the seed top-ics when there are more regular topics.
As shownin Fig.
1, each document is a mixture over T top-ics, where each of those topics is a mixture ofa regular topic (?r? )
and its associated seed topic(?s? )
distributions.
The parameter ?k controls theprobability of drawing a word from the seed topicdistribution versus the regular topic distribution.For our first model, we assume that the corpus isgenerated based on the following generative pro-cess (its graphical notation is shown in Fig.
2(a)):1.
For each topic k=1?
?
?
T,(a) Choose regular topic ?rk ?
Dir(?r).
(b) Choose seed topic ?sk ?
Dir(?s).
(c) Choose ?k ?
Beta(1, 1).2.
For each document d, choose ?d ?
Dir(?).?
For each token i = 1 ?
?
?Nd:(a) Select a topic zi ?
Mult(?d).
(b) Select an indicator xi ?
Bern(?zi)(c) if xi is 0?
Select a word wi ?
Mult(?rzi).// choose from regular topic(d) if xi is 1?
Select a word wi ?
Mult(?szi).// choose from seed topicThe first step is to generate Multinomial distribu-tions for both seed topics and regular topics.
Theseed topics are drawn in a way that constrainstheir distribution to only generate words in thecorresponding seed set.
Then, for each token in adocument, we first generate a topic.
After choos-ing a topic, we flip a (biased) coin to pick eitherthe seed or the regular topic distribution.
Oncethis distribution is selected we generate a wordfrom it.
It is important to note that although thereare 2?T topic-word distributions in total, eachdocument is still a mixture of only T topics (asshown in Fig.
1).
This is crucial in relating seedand regular topics and is similar to the way top-ics and aspects are tied in TAM model (Paul andGirju, 2010).To understand how this model gathers wordsrelated to seed words, consider a seed topic (saythe fourth row in Table 2) with seed words {grain,wheat, corn, etc.
}.
Now by assigning all the re-lated words such as ?tonnes?, ?agriculture?, ?pro-duction?
etc.
to its corresponding regular topic,the model can potentially put high probabilitymass on topic z = 4 for agriculture related doc-uments.
Instead, if it places these words in an-other regular topic, say z = 3, then the documentprobability mass has to be distributed among top-ics 3 and 4 and as a result the model will pay asteeper penalty.
Thus the model uses seed topicto gather related words into its associated regu-lar topic and as a consequence the document-topicdistributions also become focussed.We have experimented with two ways of choos-ing the binary variable xi (step 2b) of the gener-ative story.
In the first method, we fix this sam-pling probability to a constant value which is in-dependent of the chosen topic (i.e.
?i = ?
?, ?i =1 ?
?
?
T).
And in the second method we learn theprobability as well (Sec.
4).2.2 Document-Topic distributions (Model 2)In the previous model we used seed words to im-prove topic-word probability distributions.
Herewe propose a model to explore the use of seedwords to improve document-topic probability dis-tributions.
Unlike the previous model, we willpresent this model in the general case where thenumber of seed topics is not equal to the numberof regular topics.
Hence, we associate each seedset (we refer seed set as group for conciseness)with a Multinomial distribution over the regulartopics which we call group-topic distribution.To give an overview of our model, first, wetransfer the seed information from words onto206DT?
?
?r ?r?sNdx zw(a) Model 1DT???
?
?r~b?r Nd?
?zwg(b) Model 2DT???
??r~b?r?sNd?
?x zwg(c) SeededLDAFigure 2: The graphical notation of all the three models.
In Model 1 we use seed topics to improve the topic-wordprobability distributions.
In Model 2, the seed topic information is first transfered to the document level basedon the document tokens and then it is used to improve document-topic distributions.
In the final, SeededLDA,model we combine both the models.
In Model 1 and SeededLDA, we dropped the dependency of ?s on hyperparameter ?s since it is observed.
And, for clarity, we also dropped the dependency of x on ?.the documents that contain them.
Then, thedocument-topic distribution is drawn in a two stepprocess: we sample a seed set (g for group) andthen use its group-topic distribution (?g) as priorto draw the document-topic distribution (?d).
Weused this two step process, to allow flexible num-ber of seed and regular topics, and to tie the topicdistributions of all the documents within a group.We assume the following generative story and itsgraphical notation is shown in Fig.
2(b).1.
For each k = 1?
?
?
T,(a) Choose ?rk ?
Dir(?r).2.
For each seed set s = 1?
?
?
S,(a) Choose group-topic distribution ?s ?Dir(?).
// the topic distribution for sthgroup (seed set) ?
a vector of length T.3.
For each document d,(a) Choose a binary vector~b of length S.(b) Choose a document-group distribution?d ?
Dir(?~b).
(c) Choose a group variable g ?
Mult(?d)(d) Choose ?d ?
Dir(?g).
// of length T(e) For each token i = 1 ?
?
?Nd:i.
Select a topic zi ?
Mult(?d).ii.
Select a word wi ?
Mult(?rzi).We first generate T topic-word distributions(?k) and S group-topic distributions (?s).
Thenfor each document, we generate a list of seed setsthat are allowed for this document.
This list isrepresented using the binary vector ~b.
This bi-nary vector can be populated based on the docu-ment words and hence it is treated as an observedvariable.
For example, consider the (very short!
)document ?oil companies have merged?.
Accord-ing to the seed sets from Table 2, we define a bi-nary vector that denotes which seed topics containwords in this document.
In this case, this vec-tor ~b = ?1, 1, 0, 1, 1?, indicating the presence ofseeds from sets 1, 2, 4 and 5.1 As discussed in(Williamson et al 2010), generating binary vec-tor is crucial if we want a document to talk abouttopics that are less prominent in the corpus.The binary vector ~b, that indicates which seedsexist in this document, defines a mean of aDirichlet distribution from which we sample adocument-group distribution, ?d (step 3b).
Weset the concentration of this Dirichlet to a hy-perparamter ?
, which we set by hand (Sec.
4);thus, ?d ?
Dir(?~b).
From the resulting multino-mial, we draw a group variable g for this docu-ment.
This group variable brings clustering struc-ture among the documents by grouping the docu-ments that are likely to talk about same seed set.Once the group variable (g) is drawn, wechoose the document-topic distribution (?d) froma Dirichlet distribution with the group?s-topic dis-tribution as the prior (step 3d).
This step ensuresthat the topic distributions of documents withineach group are related.
The remaining sampling1As a special case, if no seed word is found in the docu-ment,~b is defined as the all-ones vector.207process proceeds like LDA.
We sample a topicfor each word and then generate a word from itscorresponding topic-word distribution.
Observethat, if the binary vector is all ones and if weset ?d = ?d then this model reduces to the LDAmodel with ?
and ?r as the hyperparameters.2.3 SeededLDABoth of our models use seed words in differentways to improve topic-word and document-topicdistributions respectively.
We can combine boththe above models easily.
We refer to the combinedmodel as SeededLDA and its generative story isas follows (its graphical notation is shown in Fig.2(c)).
The variables have same semantics as in theprevious models.1.
For each k=1?
?
?
T,(a) Choose regular topic ?rk ?
Dir(?r).
(b) Choose seed topic ?sk ?
Dir(?s).
(c) Choose ?k ?
Beta(1, 1).2.
For each seed set s = 1?
?
?
S,(a) Choose group-topic distribution ?s ?Dir(?).3.
For each document d,(a) Choose a binary vector~b of length S.(b) Choose a document-group distribution?d ?
Dir(?~b).
(c) Choose a group variable g ?
Mult(?d).
(d) Choose ?d ?
Dir(?g).
// of length T(e) For each token i = 1 ?
?
?Nd:i.
Select a topic zi ?
Mult(?d).ii.
Select an indicator xi ?
Bern(?zi).iii.
if xi is 0?
Select a word wi ?
Mult(?rzi).iv.
if xi is 1?
Select a word wi ?
Mult(?szi).In the SeededLDA model, the process for gen-erating group variable of a document is same asthe one described in the Model 2.
And like in theModel 2, we sample a document-topic probabilitydistribution as a Dirichlet draw with the group-topic distribution of the chosen group as prior.Subsequently, we choose a topic for each tokenand then flip a biased coin.
We choose either theseed or the regular topic based on the result of thecoin toss and then generate a word from its distri-bution.2.4 Automatic Seed SelectionIn (Andrzejewski and Zhu, 2009; Andrzejewskiet al 2009), the seed information is providedmanually.
Here, we describe the use of feature se-lection techniques, prevalent in the classificationliterature, to automatically derive the seed sets.
Ifwe want the topicality structure identified by theLDA to align with the underlying class structure,then the seed words need to be representative ofthe underlying topicality structure.
To enable this,we first take class labeled data (doesn?t need tobe multi-class labeled data unlike (Ramage et al2009)) and identify the discriminating features foreach class.
Then we choose these discriminatingfeatures as the initial sets of seed words.
In prin-ciple, this is similar to the prototype driven unsu-pervised learning (Haghighi and Klein, 2006).We use Information Gain (Mitchell, 1997) toidentify the required discriminating features.
TheInformation Gain (IG) of a word (w) in a class (c)is given byIG(c, w) = H(c) ?H(c|w)whereH(c) is the entropy of the class andH(c|w)is the conditional entropy of the class given theword.
In computing Information Gain, we bina-rize the document vectors and consider whether aword occurs in any document of a given class ornot.
Thus obtained ranked list of words for eachclass are filtered for ambiguous words and thenused as initial sets of seed words to be input to themodel.3 Related WorkSeed-based supervision is closely related to theidea of seeding in the bootstrapping literature forlearning semantic lexicons (Thelen and Riloff,2002).
The goals are similar as well: growinga small set of seed examples into a much largerset.
A key difference is the type of semantic in-formation that the two approaches aim to capture:semantic lexicons are based on much more spe-cific notions of semantics (e.g.
all the countrynames) than the generic ?topic?
semantics of topicmodels.
The idea of seeding has also been usedin prototype-driven learning (Haghighi and Klein,2006) and shown similar efficacies for these semi-supervised learning approaches.LDAWN (Boyd-Graber et al 2007) modelssets of words for the word sense disambiguation208task.
It assumes that a topic is a distributionover synsets and relies on the Wordnet to obtainthe synsets.
The most related prior work is thatof (Andrzejewski et al 2009), who propose theuse Dirichlet Forest priors to incorporate MustLink and Cannot Link constraints into the topicmodels.
This work is analogous to constrainedK-means clustering (Wagstaff et al 2001; Basuet al 2008).
A must link between a pair wordtypes represents that the model should encourageboth the words to have either high or low prob-ability in any particular topic.
A cannot link be-tween a word pair indicates both the words shouldnot have high probability in a single topic.
In theDirichlet Forest approach, the constraints are firstconverted into trees with words as the leaves andedges having pre-defined weights.
All the treesare joined to a dummy node to form a forest.
Thesampling for a word translates into a random walkon the forest: starting from the root and selectingone of its children based on the edge weights untilyou reach a leaf node.While the Dirichlet Forest method requires su-pervision in terms of Must link and Cannot linkinformation, the Topics In Sets (Andrzejewski andZhu, 2009) model proposes a different approach.Here, the supervision is provided at the tokenlevel.
The user chooses specific tokens and re-strict them to occur only with in a specified list oftopics.
While this needs minimal changes to theinference process of LDA, it requires informationat the level of tokens.
The word type level seedinformation can be converted into token level in-formation (like we do in Sec.
4) but this preventstheir model from distinguishing the tokens basedon the word senses.Several models have been proposed which usesupervision at the document level.
SupervisedLDA (Blei and McAuliffe, 2008) and DiscLDA(Lacoste-Julien et al 2008) try to predict the cat-egory labels (e.g.
sentiment classification) forthe input documents based on a document labeleddata.
Of these models, the most related one toSeededLDA is the LabeledLDA model (Ramageet al 2009).
Their model operates on multi-classlabeled corpus.
Each document is assumed to bea mixture over a known subset of topics (classes)with each topic being a distribution over words.The process of generating document topic distri-bution in LabeledLDA is similar to the processof generating group distribution in our Model 2(Sec.
2.2).
However our model differs from La-beledLDA in the subsequent steps.
Rather thanusing the group distribution directly, we sam-ple a group variable and use it to constrain thedocument-topic distributions of all the documentswithin this group.
Moreover, in their model thebinary vector is observed directly in the form ofdocument labels while, in our case, it is automat-ically populated based on the document tokens.Interactive topic modeling brings the user intothe loop, by allowing him/her to make suggestionson how to improve the quality of the topics at eachiteration (Hu et al 2011).
In their approach, theauthors use Dirichlet Forest method to incorpo-rate the user?s preferences.
In our experiments(Sec.
4), we show that SeededLDA performs bet-ter than Dirichlet Forest method, so SeededLDAwhen used with their framework can allow an userto explore a document collection in a more mean-ingful manner.4 ExperimentsWe evaluate different aspects of the model sep-arately.
Our experimental setup proceeds as fol-lows: a) Using an existing model, we evaluate theeffectiveness of automatically derived constraintsindicating the potential benefits of adding seedwords into the topic models.
b) We evaluate eachof our proposed models in different settings andcompare with multiple baseline systems.Since our aim is to overcome the domi-nance of majority topics by encouraging thetopicality structure identified by the topic mod-els to align with that of the document cor-pus, we choose extrinsic evaluation as theprimary evaluation method.
We use docu-ment clustering task and use frequent-5 cate-gories of Reuters-21578 corpus (Lewis et al2004) and four classes from the 20 News-groups data set (i.e.
?rec.autos?, ?sci.electronics?,?comp.hardware?
and ?alt.atheism?).
For boththe corpora we do the standard preprocessingof removing stopwords and infrequent words(Williamson et al 2010).For all the models, we use a Collapsed Gibbssampler (Griffiths and Steyvers, 2004) for the in-ference process.
We use the standard hyperparam-eters values ?
= 1.0, ?
= 0.01 and ?
= 1.0 andrun the sampler for 1000 iterations, but one canuse techniques like slice sampling to estimate thehyperparameters (Johnson and Goldwater, 2009).209Reuters 20 NewsgroupsF-measure VI F-measure VILDA 0.64 (?.05) 1.26 (?.16) 0.77 (?.06) 0.9 (?.13)Dirichlet Forest 0.67?
(?.02) 1.17 (?.11) 0.79(?.01) 0.83?(?.03)?
over LDA (+4.68%) (-7.1%) (+2.6%) (-7.8%)Table 3: The effect of adding constraints by Dirichlet Forest Encoding.
For Variational Information (VI) a lowerscore indicates a better clustering.
?
indicates statistical significance at p = 0.01 as measured by the t-test.
Allthe four improvements are significant at p = 0.05.We run all the models with the same number oftopics as the number of clusters.
Then, for eachdocument, we find the topic that has maximumprobability in the posterior document-topic distri-bution and assign it to that cluster.
The accuracyof the document clustering is measured in termsof F-measure and Variation of Information.
F-measure is calculated based on the pairs of doc-uments, i.e.
if two documents belong to a clusterin both ground truth and the clustering proposedby the system then it is counted as correct, other-wise it is counted as wrong.
Variational Informa-tion (VI) of two clusterings X and Y is given as(Meila?, 2007):VI(X,Y ) = H(X) +H(Y ) ?
2I(X,Y )whereH(X) denotes the entropy of the clusteringX and I(X,Y ) denotes the mutual informationbetween the two clusterings.
For VI, a lower valueindicates a better clustering.
All the accuracies areaveraged over 25 different random initializationsand all the significance results are measured usingthe t-test at p = 0.01.4.1 Seed ExtractionThe seeds were extracted automatically (Sec.
2.4)based on a small sample of labeled data other thanthe test data.
We first extract 25 seeds words pereach class and then remove the seed words thatappear in more than one class.
After this filtering,on an average, we are left with 9 and 15 words pereach seed topic for Reuters and 20 Newsgroupscorpora respectively.We use the existing Dirichlet Forest method toevaluate the effectiveness of the automatically ex-tracted seed words.
The Must and Cannot linksrequired for the supervision (Andrzejewski et al2009) are automatically obtained by adding amust-link between every pair of words belongingto the same seed set and a split constraint betweenevery pair of words belonging to different sets.The accuracies are averaged over 25 different ran-dom initializations and are shown in Table 3.
Wehave also indicated the relative performance gainscompared to LDA.
The significant improvementover the plain LDA demonstrates the effectivenessof the automatic extraction of seed words in topicmodels.4.2 Document ClusteringIn the next experiment, we compare our modelswith LDA and other baselines.
The first baseline(maxCluster) simply counts the number of tokensin each document from each of the seed topics andassigns the document to the seed topic that hasmost tokens.
This results in a clustering of doc-uments based on the seed topic they are assignedto.
This baseline evaluates the effectiveness of theseed words with respect to the underlying cluster-ing.
Apart from the maxCluster baseline, we useLDA and z-labels (Andrzejewski and Zhu, 2009)as our baselines.
For z-labels, we treat all the to-kens of a seed word in the same way.
Table 4shows the comparison of our models with respectto the baseline systems.2 Comparing the perfor-mance of maxCluster to that of LDA, we observethat the seed words themselves do a poor job inclustering the documents.We experimented with two variants of Model 1.In the first run (Model 1) we sample the ?k value,i.e.
the probability of choosing a seed topic foreach topic.
While in the ?Model 1 (??
= 0.7)?
run,we fix this probability to a constant value of 0.7 ir-respective of the topic.3 Though both the models2The code used for LDA baseline in Tables 3 and 4are different.
For Table 3, we use the code available fromhttp://pages.cs.wisc.edu/?andrzeje/research/df lda.html.We use our own version for Table 4.
We tried to producea comparable baseline by running the former for moreiterations and with different hyperparameters.
In Table 3,we report their best results.3We chose this value based on intuition; it is not tuned.210Reuters 20 NewsgroupsF-measure VI F-measure VImaxCluster 0.53 1.75 0.58 1.44LDA 0.66 (?.04) 1.2 (?.12) 0.76 (?.06) 0.9 (?.14)z-labels 0.73 (?.01) 1.04 (?.01) 0.8 (?.00) 0.82 (?.01)?
over LDA (+10.6%) (-13.3%) (+5.26%) (-8.8%)Model 1 0.69 (?.00) 1.13 (?.01) 0.8 (?.01) 0.81 (?.02)Model 1 (??
= 0.7) 0.73 (?.00) 1.09 (?.01) 0.8 (?.01) 0.81 (?.02)Model 2 0.66 (?.04) 1.22 (?.1) 0.77 (?.07) 0.85 (?.12)SeededLDA 0.76?
(?.01) 0.99?
(?.03) 0.81?
(?.01) 0.75?
(?.02)?
over LDA (+15.5%) (-17.5%) (+6.58%) (-16.7%)Table 4: Accuracies on document clustering task with different models.
?
indicates significant improvementcompared to the z-labels approach, as measured by the t-test with p = 0.01.
The relative performance gains arewith respect to the LDA model and are provided for comparison with Dirichlet Forest method (in Table 3.
)performed better than LDA, fixing the probabil-ity gave better results.
When we attempt to learnthis value, the model chooses to explain some ofthe seed words by the regular topics.
On the otherhand, when ?
is fixed, it explains almost all theseed words based on the seed topics.
The nextrow (Model 2) indicates the performance of oursecond model on the same data sets.
The firstmodel seems to be performing better than the sec-ond model, which is justifiable since the latteruses seed topics indirectly.
Though the variantsof Model 1 and Model 2 performed better thanthe LDA, they fell short of the z-labels approach.Table 4 also shows the performance of our com-bined model (SeededLDA) on both the corpora.When the models are combined, the performanceimproves over each of them and is also better thanthe baseline systems.
As explained before, our in-dividual models improve both the topic-word anddocument-topic distributions respectively.
But itturns out that the knowledge learnt by both the in-dividual models is complementary to each other.As a result the combined model performed betterthan the individual models and other baseline sys-tems.
Comparing the last rows of Tables 4 and 3,we notice that the relative performance gains ob-served in the case of SeededLDA is significantlyhigher than the performance gains obtained byincorporating the constraints using the DirichletForest method.
Moreover, as indicated in the Ta-ble 4, SeededLDA achieves significant gains overthe z-labels approach as well.We have also provided the standard intervalsfor each of the approaches.
A quick inspection ofthese intervals reveals the superior performanceof SeededLDA compared to all the baselines.
Thestandard deviation of the F-measures over dif-ferent random initializations of our our model isabout 1% for both the corpora while it is 4% and6% for the LDA on Reuters and 20 Newsgroupscorpora respectively.
The reduction in the vari-ance, across all the approaches that use seed infor-mation, shows the increased robustness of the in-ference process when using seed words.
From theaccuracies in both the tables, it is clear that Seed-edLDA model out-performs other models whichtry to incorporate seed information into the topicmodels.4.3 Effect of Ambiguous SeedsIn the following experiment we study the effectof ambiguous seeds.
We allow a seed word to oc-cur in multiple seed sets.
Table 6 shows the cor-responding results.
The performance drops whenwe add ambiguous seed words, but it is still higherthan that of the LDA model.
This suggests that thequality of the seed topics is determined by the dis-criminative power of the seed words rather thanthe number of seed words in each seed topic.
Thetopics identified by the SeededLDA on Reuterscorpus are shown in the Table 5.
With the help ofthe seed sets, the model is able to split the ?Grain?and ?Crude?
into two separate topics which weremerged into a single topic by the plain LDA.4.4 Qualitative Evaluation on NIPS papersWe ran LDA and SeededLDA models on the NIPSpapers from 2001 to 2010.
For this corpus, theseed words are chosen from the call for proposal.211group, offer, common, cash, agreement, shareholders, acquisition, stake, merger, board, saleoil, price, prices, production, lt, gas, crude, 1987, 1985, bpd, opec, barrels, energy, first, petroleum0, mln, cts, net, loss, 2, dlrs, shr, 3, profit, 4, 5, 6, revs, 7, 9, 8, year, note, 1986, 10, 0, salestonnes, wheat, mln, grain, week, corn, department, year, export, program, agriculture, 0, soviet, pricesbank, market, pct, dollar, exchange, billion, stg, today, foreign, rate, banks, japan, yen, rates, tradeTable 5: Topics identified by SeededLDA on the frequent-5 categories of Reuters corpusReuters 20 NewsgroupsF VI F VILDA 0.66 1.2 0.76 0.9SeededLDA 0.76 0.99 0.81 0.75SeededLDA 0.71 1.08 0.79 0.78(amb)Table 6: Effect of ambiguous seed words on Seed-edLDA.There are 10 major areas with sub areas undereach of them.
We ran both the models with 10 top-ics.
For SeededLDA, the words in each of the ar-eas are selected as seed words and we filter out theambiguous seed words.
Upon a qualitative obser-vation of the output topics, we found that LDA hasidentified seven major topics and left out ?BrainImaging?, ?Cognitive Science and Artificial In-telligence?
and ?Hardware Technologies?
areas.Not surprisingly, but reassuringly, these areas areunderrepresented among the NIPS papers.
On theother hand, SeededLDA successfully identifies allof the major topics.
The topics identified by LDAand SeededLDA are shown in the supplementarymaterial.5 DiscussionIn traditional topic models, a symmetric Dirich-let distribution is used as prior for topic-word dis-tributions.
A first attempt method to incorporateseed words into the model is to use an asymmetricDirichlet distribution as prior for the topic-worddistributions (also called as Informed priors).
Forexample, to encourage Topic 5 to align with a seedset we can choose an asymmetric prior of the form~?5 = {?, ?
?
?
, ?
+ c, ?
?
?
, ?
}, i.e.
we increasethe component values corresponding to the seedwords by a positive constant value.
This favorsthe desired seed words to be drawn with a higherprobability from this topic.
But, it is argued else-where that words drawn from such distributionsrarely pick words other than the seed words (An-drzejewski et al 2009).
Moreover, since, in ourmethod each seed topic is a distribution over theseed words, the convex combination of regularand seed topics can be seen as adding differentweights (ci) to different components of the priorvector.
Thus our Model 1 can be seen as an asym-metric generalization of the Informed priors.For comparability purposes, in this paper, weexperimented with same number of regular topicsas the number of seed topics.
But as explained inthe modeling part, our model is general enoughto handle situation with unequal number of seedand regular topics.
In this case, we assume thatthe seed topics indicate a higher level of topical-ity structure of the corpus and associate each seedtopic (or group) with a distribution over the regu-lar topics.
On the other hand, in many NLP appli-cations, we tend to have only a partial informationrather than high-level supervision.
In such cases,one can create some empty seed sets and tweakthe model 2 to output a 1 in the binary vector cor-responding to these seed sets.
In this paper, weused information gain to select the discriminatingseed words.
But in the real world applications,one can use publicly available ODP categorizationdata to obtain the higher level seed words and thusexplore the corporal in a more meaningful way.In this paper, we have explored two methodsto incorporate lexical prior into the topic mod-els, combining them into a single model that wecall SeededLDA.
From our experimental analysis,we found that automatically derived seed wordscan improve clustering performance significantly.Moreover, we found out that allowing a seed wordto be shared across multiple sets of seed words de-grades the performance.6 AcknowledgmentsWe thank the anonymous reviewers for their help-ful comments.
This material is partially supportedby the National Science Foundation under GrantNo.
IIS-1153487.212ReferencesAndrzejewski, D. and Zhu, X.
(2009).
Latent dirichletallocation with topic-in-set knowledge.
In Proceed-ings of the NAACL HLT 2009 Workshop on Semi-Supervised Learning for Natural Language Pro-cessing, SemiSupLearn ?09, pages 43?48, Morris-town, NJ, USA.
Association for Computational Lin-guistics.Andrzejewski, D., Zhu, X., and Craven, M. (2009).
In-corporating domain knowledge into topic modelingvia dirichlet forest priors.
In ICML ?09: Proceed-ings of the 26th Annual International Conferenceon Machine Learning, pages 25?32, New York, NY,USA.
ACM.Basu, S., Ian, D., and Wagstaff, K. (2008).
Con-strained Clustering : Advances in Algorithms, The-ory, and Applications.
Chapman & Hall/CRC Pres.Blei, D. and McAuliffe, J.
(2008).
Supervised topicmodels.
In Advances in Neural Information Pro-cessing Systems 20, pages 121?128, Cambridge,MA.
MIT Press.Blei., D. M. and Lafferty., J.
(2009).
Topic models.
InText Mining: Theory and Applications.
Taylor andFrancis.Blei, D. M., Ng, A. Y., and Jordan, M. I.
(2003).
La-tent dirichlet alcation.
Journal of Maching Learn-ing Research, 3:993?1022.Boyd-Graber, J., Blei, D. M., and Zhu, X.
(2007).
Atopic model for word sense disambiguation.
In Em-pirical Methods in Natural Language Processing.Chang, J., Boyd-Graber, J., Wang, C., Gerrish, S., andBlei, D. M. (2009).
Reading tea leaves: How hu-mans interpret topic models.
In Neural InformationProcessing Systems.Griffiths, T., Steyvers, M., and Tenenbaum, J.
(2007).Topics in semantic representation.
PsychologicalReview, 114(2):211?244.Griffiths, T. L. and Steyvers, M. (2004).
Finding sci-entific topics.
Proceedings of National Academy ofSciences USA, 101 Suppl 1:5228?5235.Griffiths, T. L., Steyvers, M., Blei, D. M., and Tenen-baum, J.
B.
(2005).
Integrating topics and syntax.In Advances in Neural Information Processing Sys-tems, volume 17, pages 537?544.Haghighi, A. and Klein, D. (2006).
Prototype-drivenlearning for sequence models.
In Proceedings ofthe main conference on Human Language Tech-nology Conference of the North American Chap-ter of the Association of Computational Linguis-tics, HLT-NAACL ?06, pages 320?327, Strouds-burg, PA, USA.
Association for Computational Lin-guistics.Hu, Y., Boyd-Graber, J., and Satinoff, B.
(2011).
In-teractive topic modeling.
In Proceedings of the 49thAnnual Meeting of the Association for Computa-tional Linguistics: Human Language Technologies- Volume 1, HLT ?11, pages 248?257, Stroudsburg,PA, USA.
Association for Computational Linguis-tics.Johnson, M. and Goldwater, S. (2009).
Improvingnonparameteric bayesian inference: experimentson unsupervised word segmentation with adap-tor grammars.
In Proceedings of Human Lan-guage Technologies: The 2009 Annual Conferenceof the North American Chapter of the Associationfor Computational Linguistics, NAACL ?09, pages317?325, Stroudsburg, PA, USA.
Association forComputational Linguistics.Lacoste-Julien, S., Sha, F., and Jordan, M. (2008).DiscLDA: Discriminative learning for dimensional-ity reduction and classification.
In Proceedings ofNIPS ?08.Lewis, D. D., Yang, Y., Rose, T. G., and Li, F. (2004).Rcv1: A new benchmark collection for text catego-rization research.
J. Mach.
Learn.
Res., 5:361?397.Meila?, M. (2007).
Comparing clusterings?an infor-mation based distance.
J. Multivar.
Anal., 98:873?895.Mitchell, T. M. (1997).
Machine Learning.
McGraw-Hill, New York.Paul, M. and Girju, R. (2010).
A two-dimensionaltopic-aspect model for discovering multi-facetedtopics.
In AAAI.Ramage, D., Hall, D., Nallapati, R., and Manning,C.
D. (2009).
Labeled LDA: a supervised topicmodel for credit attribution in multi-labeled cor-pora.
In Proceedings of the 2009 Conference onEmpirical Methods in Natural Language Process-ing: Volume 1 - Volume 1, EMNLP ?09, pages 248?256, Morristown, NJ, USA.
Association for Com-putational Linguistics.Thelen, M. and Riloff, E. (2002).
A bootstrappingmethod for learning semantic lexicons using extrac-tion pattern contexts.
In In Proc.
2002 Conf.
Empir-ical Methods in NLP (EMNLP).Wagstaff, K., Cardie, C., Rogers, S., and Schro?dl, S.(2001).
Constrained k-means clustering with back-ground knowledge.
In Proceedings of the Eigh-teenth International Conference on Machine Learn-ing, ICML ?01, pages 577?584, San Francisco, CA,USA.
Morgan Kaufmann Publishers Inc.Wallach, H. M. (2005).
Topic modeling: beyond bag-of-words.
In NIPS 2005 Workshop on BayesianMethods for Natural Language Processing.Williamson, S., Wang, C., Heller, K. A., and Blei,D.
M. (2010).
The IBP compound dirichlet pro-cess and its application to focused topic modeling.In ICML, pages 1151?1158.213
