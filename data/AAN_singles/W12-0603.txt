Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 18?27,Avignon, France, April 23 - 27 2012. c?2012 Association for Computational LinguisticsTowards Scalable Speech Act Recognition in Twitter:Tackling Insufficient Training DataRenxian Zhang Dehong Gao Wenjie LiDepartment of ComputingThe Hong Kong Polytechnic University{csrzhang, csdgao, cswjli}@comp.polyu.edu.hkAbstractRecognizing speech act types in Twitter is ofmuch theoretical interest and practical use.Our previous research did not adequatelyaddress the deficiency of training data for thismulti-class learning task.
In this work, we setout by assuming only a small seed training setand experiment with two semi-supervisedlearning schemes, transductive SVM andgraph-based label propagation, which canleverage the knowledge about unlabeled data.The efficacy of semi-supervised learning isestablished by our extensive experiments,which also show that transductive SVM ismore suitable than graph-based labelpropagation for our task.
The empiricalfindings and detailed evidences cancontribute to scalable speech act recognitionin Twitter.1.
IntroductionThe social media platform of Twitter makesavailable a plethora of data to probe thecommunicative act of people in a social networkwoven by interesting events, people, topics, etc.Communicative acts such as disseminatinginformation, asking questions, or expressingfeelings all fall in the purview of ?speech act?, along established area in pragmatics (Austin1962).
The automatic recognition of speech actin tons of tweets has both theoretical andpractical appeal.
Practically, it helps tweeters tofind topics to read or tweet about based onspeech act compositions.
Theoretically, itintroduces a new dimension to study socialmedia content as well as providing real-life datato validate or falsify claims in the speech acttheory.Different taxonomies of speech act have beenproposed by linguists and computationallinguists, ranging from a few to over a hundredtypes.
In this work, we adopt the 5 types ofspeech act used in our previous work (Zhang etal.
2011), which are in turn inherited from(Searle 1975): statement, question, suggestion,comment, and miscellaneous.
Our choice isbased on the fact that unlike face-to-facecommunication, twittering is more in a?broadcasting?
style than on a personal basis.Statement and comment, which are usuallyintended to make one?s knowledge, thought, andsentiment known, thus befit Twitter?scommunicative style.
Question and suggestionon Twitter are usually targeted at other tweetersin general or one?s followers.
More interpersonalspeech acts such as ?threat?
or ?thank?
as well asrare speech acts in Twitter (Searle?s (1975)?commissives?
and ?declaratives?)
are relegatedto ?miscellaneous?.
Some examples from ourexperimental datasets are provided in Table 1.18Tweet Speech ActLibya Releases 4 TimesJournalists -http://www.photozz.com/?104kStatement#sincewebeinghonest why u soobsessed with what me n herdo??
Don't u got ya ownman????
Oh wait.....QuestionRT @NaonkaMixon: I willdonate 10 $ to the Red CrossJapan Earthquake fund forevery person that retweets this!#PRAYFORJAPANSuggestionis enjoying this new season of#CelebrityApprentice.... NikkiTaylor = Yum!!Comment65.
I want to get married tosomeone i meet in highschool.#100factsaboutmeMiscellaneousTable 1.
Example Tweets with Speech actsAssuming one tweet demonstrates only onespeech act, the automatic recognition of thosespeech act types in Twitter is a multi-classclassification task.
We concede that thisassumption may not always hold in realsituations.
But given the short length of tweets,multi-speech act tweets are rare and we find thissimplifying assumption effective in reducing thecomplexity of our problem.
A major problemwith this task is the deficiency of training data.Tweeters as well as face-to-face interlocutors donot often identify their speech acts; humanannotation is costly and time-consuming.Although our previous research (Zhang et al2011) sheds light on the preparation of trainingdata, it did not adequately address this problem.Our contribution in this work is to directlyaddress the problem of training data deficiencyby using two well-known semi-supervisedlearning techniques that leverage the relationshipbetween a small seed of training data and a largebody of unlabeled data: transductive SVM andgraph-based label propagation.
The empiricalresults show that the knowledge about unlabeleddata provides promising solutions to the datadeficiency problem, and that transductive SVMis more competent for our task.
Our explorationwith different training/unlabeled data ratios forthree major Twitter categories and a mixed-typecategory provides solid evidential support forfuture research.The rest of the paper is organized as follows.Section 2 reviews works related to speech actrecognition and semi-supervised learning;Section 3 briefly discusses supervised learning ofspeech act types developed in our earlier workand complementing the previous findings withlearning curves.
The technical details of semi-supervised learning are presented in Section 4.Then we report and discuss the results of ourexperiments in Section 5.
Finally, Section 6concludes the paper and outlines futuredirections.2.
Related WorkThe automatic recognition of speech act, alsoknown as ?dialogue act?, has attracted sustainedinterest in computational linguistics and speechtechnology for over a decade (Searle 1975;Stolcke et al 2000).
A few annotated corporasuch as Switchboard-DAMSL (Jurafsky et al1997) and Meeting Recorder Dialog Act (Dhillonet al 2004) are widely used, with datatranscribed from telephone or face-to-faceconversation.Prior to the flourish of microblogging servicessuch as Twitter, speech act recognition has beenextended to electronic media such as email anddiscussion forum (Cohen et al 2004; Feng et al2006) in order to study the behavior of email ormessage senders.The annotated corpora for ordinary verbalcommunications and the methods developed foremail, or discussion forum cannot be directlyused for our task because Twitter text has adistinctive Netspeak style that is situatedbetween speech and text but resembles neither(Crystal 2006, 2011).
Compared with email orforum post, it is rife with linguistic noises suchas spelling mistakes, random coinages, mixeduse of letters and symbols.Speech act recognition in Twitter is a fairlynew task.
In our pioneering work (Zhang et al2011), we show that Twitter text normalization isunnecessary and even counterproductive for thistask.
More importantly, we propose a set ofuseful features and draw empirical conclusionabout the scope of this task, such as recognizingspeech act on the coarse-grade category levelworks as well as on the fine-grade topic level.
Inthis work, we continue to adopt this frameworkincluding other learning details (speech act typesand feature selection for tweets), but the newquest starts where the old one left: tacklinginsufficient training data.19As in many practical applications, sufficientannotated data are hard to obtain.
Therefore,unsupervised and semi-supervised learningmethods are actively pursued.
Whileunsupervised sentence classification is rule-basedand domain-dependent (Deshpande et al 2010),semi-supervised methods that both alleviate thedata deficiency problem and leverage the powerof state-of-the-art classifiers hold more promisesfor different domains (Medlock and Briscoe2007; Erkan et al 2007).In the machine learning literature, a classicsemi-supervised learning scheme is proposed byYarowsky (1995), which is a classical self-teaching process that makes no use of labeleddata before they are classified.
More theoreticalanalyses are made by (Culp and Michailidis 2007)and (Haffari and Sarkar 2007).Transductive SVM (Joachims 1999) extendsthe state-of-the-art inductive SVM by explicitlyconsidering the relationship between labeled andunlabeled data.
The graph-based labelpropagation model (Zhu et al 2003; Zhou et al2004) using a harmonic function alsoaccommodates the knowledge about unlabeleddata.
We will adapt both of them to our multi-class classification task.Jeong et al (2009) report a semi-supervisedapproach to classifying speech acts in emails andonline forums.
But their subtree-based method isnot applicable to our task because Twitter?s noisytextual quality cannot be found in the muchcleaner email or forum texts.3.
Supervised Learning of Speech ActTypesSupervised learning of speech act types inTwitter relies heavily on a good set of featuresthat capture the textual characteristics of bothTwitter and speech act utterances.
As in ourprevious work, we use speech act-specific cues,special words (abbreviations and acronyms,opinion words, vulgar words, and emoticons),and special characters (Twitter-specificcharacters and a few punctuations).
Tweet-external features such as tweeter profile may alsohelp, but that is beyond the focus of this paper.Although it has been empirically shown thatspeech act recognition in Twitter can be donewithout using training data specific to topics oreven categories, it is not clear how much trainingdata is needed to achieve desirable performance.In order to answer this question, we adopt thesame experimental setup and datasets as reportedin (Zhang et al 2011) and plot the learningcurves shown in Figure 1.Figure 1.
Learning Curves of Each Category andAll TweetsFor all individual experiments, the test data area randomly sampled 10% set of all annotateddata.
When training data reach 90%, we actuallyduplicate the reported results.
However, Figure 1shows that it is unnecessary to use so muchtraining data to achieve good classificationperformance.
For News and Entity, theclassification makes little noticeableimprovement after the training data ratio reaches40% (training : test = 4 : 1).
For Mixed (theaggregate of the News, Entity, LST datasets) andLST, performance peaks even earlier at 20%training data (training : test = 2 : 1) and 10%(training : test = 1 : 1).It is delightful to see that only a moderatenumber of annotated data are needed for speechact recognition.
But even that number (for theMixed dataset, 10% training data are over 800annotated tweets) may not be available and inmany situations, test data may be much morethan training data.
Taking this challenge is thenext important step we make.4.
Semi-Supervised Learning of SpeechAct TypesThe problem setting of a small seed training(labeled) set and a much larger test (labeled) setfits the semi-supervised learning scheme.
Classicsemi-supervised learning approaches such asself-teaching methods (e.g., Yarowsky 1995) aremainly concerned with incrementing high-confidence labeled data in each round of training.They do not, however, directly take into accountthe knowledge about unlabeled data.
The recentresearch emphasis is on leveraging knowledgeabout unlabeled data during training.
In thissection, we discuss two such approaches.204.1 Transductive SVMThe standard SVM classifier popularly used intext classification is also known as inductiveSVM as a model is induced from training data.The model is solely dependent on the trainingdata and agnostic about the test data.
In contrast,transductive SVM (Vapnik 1998; Joachims 1999)predicts test labels by using the knowledge abouttest data.
In the case of test (unlabeled) data faroutnumbering training (labeled) data,transductive SVM provides a feasible scheme ofsemi-supervised learning.For a single-class classification problem {xi, yi}that focuses on only one speech act type, wherexi is the ith tweet and yi is the correspondinglabel and { 1, 1}iy ?
?
?
denotes whether xicontains the speech act or not, inductive SVM isformulated to find an optimal hyperplanesign(w?xi ?
b) to maximize the soft marginbetween positive and negative objects, or tominimize:21/ 2 iiC ??
?ws.t.
( ) 1i i iy b ??
?
?
?x w , 0i?
?wherei?
is a slack variable.
Adopting the sameformulation, transductive SVM further considerstest data xi* during training by finding a labelingyj* and a hyperplane to maximize the soft marginbetween both training and test data, or tominimize:21 21/ 2 i ii iC C?
??
??
?ws.t.
( ) 1i i iy b ??
?
?
?x w , 0i?
?
* *( ) 1i i iy b ??
?
?
?x w , 0i?
?wherei?
is a slack variable for the test data.
Infact, labeling test data is done during training.As the maximal margin approach proves veryeffective for text classification, its transductivevariant that effectively uses the knowledge abouttest data holds promises of handling thedeficiency of labeled data.4.2 Graph-based Label PropagationAn alternative way of using unlabeled data insemi-supervised learning is based on the intuitionthat similar objects should belong to the sameclass, which can be translated into labelsmoothness on a graph with weights indicatingobject similarities.
This is the idea underlyingZhu et al?s (2003) graph-based label propagationmodel using Gaussian random fields.We again focus on a single-class classificationproblem.
Formally, {x1, ?
xN} are N tweets,having their actual speech act labels y = {y1, ?yL, ?
yN} (yi ?
{1, 0} denoting whether xicontains the speech act or not) with the first L ofthem known, and f = {f1, ?
fL, ?
fN} are theirpredicted labels.
Let L = {x1, ?
xL} and U ={xL+1, ?
xN} and the task is to determine{fL+1, ?
fN} for U.
We further define a graph G =(V, E), where V = L?U and E is weighted by W= [wij]N?N  with wij denoting the similaritybetween xi and xj.
Preferring label smoothness onG and preserving the given labels, we want tominimize the loss function:2,( ) 1/ 2 ( ) Tij i ji j L UE w f f?
??
?
?
?f f ?fs.t.
fi = yi (i = 1, ?, L)where ?
= D ?
W is the combinatorial graphLaplacian with D being a diagonal matrix [dij]N?Nandii ijjd w?
?.This can be expressed as a harmonic function,h = argmin fL = yLE(f), which satisfies thesmoothness property on the graph:( ) 1/ ( ( ))ii ikkh i d w h k?
?.
If we define/ij ij ikkp w w?
?and collect pij and h(i) intomatrix P and column vector h, solving ?h = 0 s.t.hL = yL is equivalent to solving h = Ph.To find the solution, we can use L and U topartition h and P:LU?
??
?
??
?hh h, ,,LL LUUL UU?
??
?
??
?P PP P Pand it can be shown that 1( )U UU UL L??
?h I P P y.To get the final classification result, thoseelements in hU that are greater than a threshold(0.5) become 1 and the others become 0.This approach propagates labels from labeleddata to unlabeled data on the principle of labelsmoothness.
If the assumption about similartweets having same speech acts holds, it shouldwork well for our problem.4.3 Multi-class ClassificationIn the previous formulations, we emphasized?single-class classification?
because both21transductive SVM and graph-based labelpropagation are inherently one class-oriented.Since our problem is a multi-class one, wetransform the problem to single-classclassifications by using the one-vs-all scheme.Specifically, for each class (speech act type) ci,we label all training instances belonging to ci as+1 and all those belonging to other classes as ?1and then do binary classification.
For ourproblem with 5 speech act types, we make 5 suchtransformations.
The final prediction is made bychoosing the class with the highest classificationscore from the 5 binary classifiers.
Bothtransductive SVM and graph-based labelpropagation produce real-valued classificationscores and are amenable to this scheme.5.
ExperimentsOur experiments are designed to answer twoquestions: 1) How useful is semi-supervisedspeech act learning in comparison withsupervised learning?
2) Which semi-supervisedlearning approach is more appropriate for ourproblem?5.1 Experimental SetupWe use the 6 datasets in our previous study1 ,which fall into 3 categories: News, Entity, Long-standing Topic (LST).
Each of the total 8613tweets is labeled with one of the followingspeech act types: sta (statement), que (question),sug (suggestion), com (comment), mis(miscellaneous).
In addition, we randomly select1000 tweets from each of the categories to createa Mixed category of 3000 tweets.
Figures 2 to 5illustrate the distributions of the speech act typesin the 3 original categories and the Mixedcategory.Figure 2.
Speech Act Distribution (News)1 http://www4.comp.polyu.edu.hk/~csrzhangFigure 3.
Speech Act Distribution (Entity)Figure 4.
Speech Act Distribution (LST)Figure 5.
Speech Act Distribution (Mixed)For each category, we use twolabeled/unlabeled data settings, with labeled dataaccounting for 5% and 10% of the total so thatthe labeled/unlabeled ratios are set atapproximately 1:19 and 1:9.
The labeled data ineach category are randomly selected in astratified way: using the same percentage toselect labeled data with each speech act type.
Thestratified selection is intended to keep the speechact distributions in both labeled and unlabeleddata.
Table 2 and Table 3 list the details of datasplitting using the two settings.Category # Labeled # Unlabeled TotalNews 155 2995 3150Entity 72 1391 1463LST 198 3802 4000Mixed 147 2853 3000Table 2.
Stratified Data Splitting with 5% asLabeled22Category # Labeled # Unlabeled TotalNews 312 2838 3150Entity 144 1319 1463LST 399 3601 4000Mixed 298 2702 3000Table 3.
Stratified Data Splitting with 10% asLabeledFor comparison with supervised learning, wealso use inductive SVM.
The inductive andtransductive SVM classifications areimplemented by using the SVMlight tool2 with alinear kernel.
For the graph-based labelpropagation method, we populate the similaritymatrix W with weights calculated by a Gaussianfunction.
Given two tweets xi and xj,22exp( )2i jijw ???
?
x xwhere ?.?
is the L2 norm.
Empirically, theGaussian function measure leads to better resultsthan other measures such as cosine.
Then weconvert the graph to an ?NN graph (Zhu andGoldberg 2009) by removing edges with weightless than a threshold because the ?NN graphempirically outperforms the fully connectedgraph.
The threshold is set to be ?
+ ?, the meanof all weights plus one standard deviation.5.2 ResultsTo better evaluate the performance of semi-supervised learning on speech act recognition inTwitter, we report the classification scores forboth multi-class and individual classes, as well asconfusion matrices.Multi-class EvaluationTable 4 lists the macro-average F scores andweighted average F scores for all classifiers andall categories at the 5% labeled data setting.Macro-average F is chosen because it gives equalweight to all classes.
Since some classes (e.g., sta)have much more instances than others (e.g., que),macro-average F ensures that significant scorechange on minority classes will not beovershadowed by small score change on majorityclasses.
In contrast, weighted average F iscalculated according to class instance numbers,which is chosen mainly because we want tocompare the result with supervised learning(reported in Zhang et al 2011 and Figure 1).
In2 http://svmlight.joachims.org/this and the following tables, iSVM, tSVM, andGLP denote inductive SVM, transductive SVM,and graph-based label propagation.Macro-average F Weighted average FiSVM tSVM GLP iSVM tSVM GLPNews .374 .502 .285 .702 .759 .643Entity .312 .395 .329 .493 .534 .436LST .295 .360 .216 .433 .501 .376Mixed .383 .424 .245 .539 .537 .391Table 4.
Multi-class F scores (5% labeled data)Almost without exception, transductive SVMachieves the best performance.
Measured bymacro-average F, it outperforms inductive SVMwith a gain of 10.7% (Mixed) to 34.2% (News).Consistent with supervised learning results,semi-supervised learning results degrade withNews > Entity > LST, indicating that both semi-supervised learning and supervised learning aresensitive to dataset characteristics.
More uniformtweet set (e.g., News) leads to betterclassification and greater improvement by semi-supervised learning.
That also explains why theMixed category, composed of the mostdiversified tweets, benefits least from semi-supervised learning.Conversely, supervised learning (inductiveSVM) on the Mixed category benefits from thedata hodgepodge even though the test data are 19times the training data.
Its macro-average F ishigher than the other categories although it doesnot have the most training data.
Its weighted-average F using inductive SVM is even higherthan using transductive SVM.It is a little surprising to find that the graph-based label propagation performs very poorly.
Inall but one place, the GLP score is lower than itsiSVM counterpart.
This may indicate that thegraph method cannot adapt well to the multi-class scenario and we will show more evidencesin the next two sections.To understand the effectiveness of semi-supervised learning, a better way than doingnumerical calculation is juxtaposing semi-supervised data settings with their comparablesupervised data settings, which is shown in Table5.
The supervised data settings are of those withthe closest weighted average F (waF) to thesemi-supervised (tSVM) waF from our previousresults (Figure 1).23# labeled labeled :unlabeled waFSemi-supervised (tSVM)News 155 1 : 19 .759Entity 72 1 : 19 .534LST 198 1 : 19 .501Mixed 147 1 : 19 .537Supervised (with closest waF)News 945 1 : 0.3 .768Entity 146 1 : 1 .589LST 800 1 : 0.5 .501Mixed 861 1 : 1 .596Table 5.
Semi-supervised Learning vs.Supervised LearningObviously semi-supervised learning bytransductive SVM can achieve classificationperformance comparable to supervised learningby inductive SVM, with less training data andmuch lower labeled/unlabeled ratio.
This showsthat semi-supervised learning such astransductive SVM holds much promise forscalable speech act recognition in Twitter.It is tempting to think that with more labeleddata and higher labeled/unlabeled ratio, semi-supervised learning performance should improve.To put this conjecture to test, we double thelabeled data (from 5% to 10%) andlabeled/unlabeled ratio (from 1/19 to 1/9), withresults in Table 6.Macro-average F Weighted average FiSVM tSVM GLP iSVM tSVM GLPNews .403 .524 .298 .731 .762 .647Entity .441 .440 .311 .587 .575 .406LST .335 .397 .216 .459 .512 .384Mixed .435 .463 .284 .557 .553 .415Table 6.
Multi-class F scores (10% labeled data)Compared with Table 4, increased labeled datadoes lead to some improvement, but not much aswe would expect, the largest gain being 15.9%(macro-average F on Mixed, using GLP).
Notethat this is achieved at the cost of labeling twiceas much data and predicting half as much.
Incontrast, the inductive SVM performance isimproved by as much as 41.3% (macro-averageF on Entity).
Such evidence shows that semi-supervised learning of speech acts in Twitterbenefits disproportionately little from increasedlabeled data, or at least the gain is not worth thepain.
In fact, this is good news for scalablespeech act recognition.Individual Class EvaluationFor more microscopic inspection, we also reportthe classification results on individual classes forall categories.
In Table 7, we list the rankings ofF measures by each classifier for each speech acttype and each category.
The one-letter notations i,t, g are short for iSVM, tSVM, and GLP.Therefore, t > g > i means tSVM outperformsGLP, which outperforms iSVM, in terms of Fmeasure.
The labeled data are 5%.Sta Que Sug Com MisNews t >g>i t >i>g t >i>g t >i>g t >g>iEntity t >g>i t >i>g g >t>i i >t>g t >g>iLST i >g>t t >i>g i >t>g t >i>g t >g>iMixed i >t>g t >i>g t >i>g i >t>g t >g>iTable 7.
Classifier Rankings for Each SpeechAct Type and Category (5% Labeled Data)In 15 out of the 20 rankings, transductiveSVM or graph-based label propagation beatsinductive SVM, which shows the efficacy ofsemi-supervised learning in this class-basedperspective.
Transductive SVM is the champion,claiming 14 top places.We also find that the overall performance ofgraph-based label propagation is the poorest,claiming 12 out of 20 bottom places.
Afterinspecting the data, we observe that theunderlying assumption of GLP that similarobjects belong to the same class is questionablefor speech act recognition in Twitter.
Tweetswith different speech acts (e.g., question andcomment) may appear very similar on the graph.The maximal margin approach is apparentlymore appropriate for our problem.On the other hand, the GLP performanceevaluated on individual classes is better thanevaluated on the multi-class if we compare Table7 and Table 4, where GLP is almost always thelowest achiever.
This indicates that in multi-classclassification, GLP suffers further from the one-vs-all converting scheme, a point we will makeclearer in the following.24Confusion matricesConfusion matrix provides another perspective tounderstand the multi-class classificationperformance.
For brevity?s sake, we present theconfusion matrices of the three classifiers on theNews category with 5% labeled data in Figure 6to Figure 8.
Similar patterns are also observed forthe other categories and with 10% labeled data.Note that the rows represent true classes and thecolumns represent predicted classes.Sta Que Sug Com MisSta 2043 0 5 14 0Que 46 7 2 9 0Sug 211 1 61 21 0Com 276 2 10 164 0Mis 120 0 1 2 0Figure 6.
Confusion Matrix of iSVM (News, 5%Labeled Data)Sta Que Sug Com MisSta 1848 4 56 90 64Que 19 17 7 20 1Sug 95 0 158 31 10Com 143 5 19 275 10Mis 94 3 4 15 7Figure 7.
Confusion Matrix of tSVM (News, 5%Labeled Data)Sta Que Sug Com MisSta 1852 0 4 11 195Que 19 6 0 0 39Sug 123 0 25 2 144Com 134 0 0 47 271Mis 102 0 0 1 20Figure 8.
Confusion Matrix of GLP (News, 5%Labeled Data)The News category is typically biased towardsthe statement speech act, which accounts for69% of the total tweets according to Figure 2.
Asa result, the iSVM tends to classify tweets of theother speech acts as statement.
Figure 6 alsoshows that the prediction accuracy is correlatedwith the training amount.
The two classes withthe least training data, question andmiscellaneous, demonstrate the lowest accuracy.Clearly, supervised learning suffers from trainingdata deficiency.Both tSVM and GLP show the effect ofleveraging unlabeled data as they assign newlabels to some instances wrongly classified asstatement.
Transductive SVM is more successfulin that it moves most of the Sug and Cominstances to the diagonal.
The situation for Queand Mis is also better, though the predictionaccuracy still suffers from lack of training data.Figure 8, however, reveals an intrinsic problemof applying graph-based label propagation tomulti-class classification.
Most instances arepredicted as either Sta or Mis.
The wrongprediction as Mis cannot be explained byimbalance of training data.
Rather, it is due to thefact that the single-class scores for Mis aftersmoothing on the graph are generally higher thanthose for Que, Sug, or Com.
In other words, thegraph-based method is highly sensitive to classdifferences when multi-class prediction isconverted from single-class predictions on ascheme like one-vs-all.In contrast, transductive SVM does not suffermuch from class differences according to Figure7, proving to be more suitable for multi-classclassification than graph-based label propagation.5.3 SummaryFor the task of recognizing speech acts in Twitter,we have made some interesting findings from theextensive empirical study.
To wrap up, let?ssummarize the most important of them in thefollowing.1) Semi-supervised learning approaches,especially transductive SVM, performcomparably to supervised learning approaches,such as inductive SVM, with considerably lesstraining data and lower training/test ratio.Increasing training data cannot improveperformance proportionately.2) Transductive SVM proves to be moreeffective than graph-based label propagation forour task.
The performance of the latter is hurt bytwo factors: a) the inappropriate assumptionabout similar tweets having the same speech actand b) its vulnerability to class differences underthe one-vs-all multi-class conversion scheme.3) For supervised learning as well as semi-supervised learning for multi-class classification,training data imbalance poses no lesser threatthan training data deficiency.256.
Conclusion and Future WorkSpeech act recognition in Twitter facilitatescontent-based user behavior study.
Realizing thatit is obsessed with insufficient training data, westart where previous research left.We are not aware of previous study of semi-supervised learning of speech acts in Twitter andin this paper we contribute to scalable speech actrecognition by drawing conclusions fromextensive experiments.
Specifically, we1) extend the work of (Zhang et al 2011) byestablishing the practicality of semi-supervisedlearning that leverages the knowledge ofunlabeled data as a promising solution toinsufficient training data;2) show that transductive SVM is moreeffective than graph-based label propagation forour problem, which aptly extends the maximalmargin approach to unlabeled data and is moreamenable to the multi-class scenario;3) provide detailed empirical evidences ofmulti-class and single-class results, which caninform future extensions in this direction anddesign of practical systems.At this stage, we are not sure whether the one-vs-all scheme is a bottleneck to one class-oriented classifiers (it appears to be so for thegraph-based method).
Therefore we will nextexplore other multi-class conversion schemesand also consider semi-supervised learning usinginherently multi-class classifiers such as Na?veBayes or Decision Tree.
In the future, we willalso explore unsupervised approaches torecognizing speech acts in Twitter.AcknowledgmentsThe work described in this paper was supportedby the grants GRF PolyU 5217/07E and PolyU5230/08E.26ReferencesAustin, J.
1962.
How to Do Things with Words.Oxford: Oxford University Press.Cohen, W., Carvalho, V., and Mitchell, T. 2004.Learning to Classify Email into ?Speech Acts?.
InProceedings of Empirical Methods in NaturalLanguage Processing (EMNLP-04), 309?316.Crystal, D. 2006.
Language and the Internet, 2ndedition.
Cambridge, UK: Cambridge UniversityPress.Crystal, D. 2011.
Internet linguistics.
London:Routledge.Culp M. and Michailidis, G. 2007.
An IterativeAlgorithm for Extending Learners to aSemisupervised Setting.
In The 2007 JointStatistical Meetings (JSM).Deshpande S. S., Palshikar, G. K., and Athiappan, G.2010.
An Unsupervised Approach to SentenceClassification, In International Conference onManagement of Data (COMAD 2010), Nagpur,India.Dhillon, R., Bhagat, S., Carvey, H., and Shriberg, E.2004.
Meeting Recorder Project: Dialog ActLabeling Guide.
Technical report, InternationalComputer Science Institute.Erkan, G., ?zg?r, A., and Radev, D. 2007.
Semi-Supervised Classification for Extracting ProteinInteraction Sentences Using Dependency Parsing.In Proceedings of the 2007 Joint Conference onEmpirical Methods in Natural LanguageProcessing and Computational Natural LanguageLearning, 228?237.Feng, D., Shaw, E., Kim, J., and Hovy.
E. H. 2006.Learning to Detect Conversation Focus ofThreaded Discussions.
In Proceedings of HLT-NAACL, 208?215.Haffari G.R.
and Sarkar.
A.
2007.
Analysis of semi-supervised learning with the Yarowsky algorithm.In 23rd Conference on Uncertainty in ArtificialIntelligence (UAI).Jeong, M., Lin, C-Y., and Lee, G. 2009.
Semi-supervised Speech Act Recognition in Emails andForums.
In Proceedings of EMNLP, pages 1250?1259.Joachims, T. 1999.
Transductive Inference for TextClassification using Support Vector Machines.
InProceedings of the 16th International Conferenceon Machine Learning (ICML).Jurafsky, D., Shriberg, E., and Biasca, D. 1997.Switchboard SWBD-DAMSL Labeling ProjectCoder?s Manual, Draft 13.
Technical report,University of Colorado Institute of CognitiveScience.Medlock, B., and Briscoe, T. 2007.
WeaklySupervised Learning for Hedge Classification inScientific Literature.
In Proceedings of the 45thAnnual Meeting of the Association ofComputational Linguistics, 992?999.Searle, J.
1975.
Indirect speech acts.
In P. Cole and J.Morgan (eds.
), Syntax and semantics, vol.
iii:Speech acts (pp.
59?82).
New York: AcademicPress.Stolcke, A., Ries, K., Coccaro, N., Shriberg, E., Bates,R., Jurafsky, D., Taylor, P., Martin, R. Van Ess-Dykema, C., and Meteer, M. 2000.
Dialogue ActModeling for Automatic Tagging and Recognitionof Conversational Speech.
ComputationalLinguistics, 26(3):339?373.Vapnik, V. 1998.
Statistical Learning Theory.
NewYork: John Wiley & Sons.Yarowsky, D. 1995.
Unsupervised Word SenseDisambiguation Rivaling Supervised Methods.
InProceedings of the 33rd Annual Meeting of theAssociation for Computational Linguistics (ACL-1995), 189?196.Zhang, R., Gao, D., and Li, W. 2011.
What AreTweeters Doing: Recognizing Speech Acts inTwitter.
In AAAI-11 Workshop on AnalyzingMicrotext.Zhou, D., Bousquet, O., Lal, T. N., Weston, J., andScholkopf, B.
2004.
Learning with Local andGlobal Consistency.
Advances in NeuralInformation Processing Systems (NIPS), vol.
16,Cambridge, MA: MIT Press.Zhu, X., Ghahramani, Z., and Lafferty, J. D. 2003.Semi-supervised Learning Using Gaussian Fieldsand Harmonic Functions.
In Proceedings of theTwentieth International Conference on MachineLearning (ICML), 912?919, Washington, DC.Zhu, X. and Goldberg, A.
B., 2009.
Introduction toSemi-Supervised Learning.
Morgan & ClaypoolPublishers.27
