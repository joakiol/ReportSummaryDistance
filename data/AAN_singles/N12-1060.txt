2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 533?537,Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational LinguisticsImproved Reordering for Shallow-n Grammar based HierarchicalPhrase-based TranslationBaskaran Sankaran and Anoop SarkarSchool of Computing ScienceSimon Fraser UniversityBurnaby BC.
Canada{baskaran, anoop}@cs.sfu.caAbstractShallow-n grammars (de Gispert et al, 2010)were introduced to reduce over-generation inthe Hiero translation model (Chiang, 2005) re-sulting in much faster decoding and restrictingreordering to a desired level for specific lan-guage pairs.
However, Shallow-n grammarsrequire parameters which cannot be directlyoptimized using minimum error-rate tuningby the decoder.
This paper introduces somenovel improvements to the translation modelfor Shallow-n grammars.
We introduce tworules: a BITG-style reordering glue rule and asimpler monotonic concatenation rule.
We useseparate features for the new rules in our log-linear model allowing the decoder to directlyoptimize the feature weights.
We show thisformulation of Shallow-n hierarchical phrase-based translation is comparable in translationquality to full Hiero-style decoding (withoutshallow rules) while at the same time beingconsiderably faster.1 IntroductionHierarchical phrase-based translation (Chiang,2005; Chiang, 2007) extends the highly lexicalizedmodels from phrase-based translation systems inorder to model lexicalized reordering and discon-tiguous phrases.
However, a major drawback in thisapproach, when compared to phrase-based systems,is the total number of rules that are learnt are severalorders of magnitude larger than standard phrasetables, which leads to over-generation and searcherrors and contribute to much longer decodingtimes.
Several approaches have been proposed toaddress these issues: from filtering the extractedsynchronous grammar (Zollmann et al, 2008; Heet al, 2009; Iglesias et al, 2009) to alternativeBayesian approaches for learning minimal gram-mars (Blunsom et al, 2008; Blunsom et al, 2009;Sankaran et al, 2011).
The idea of Shallow-n gram-mars (de Gispert et al, 2010) takes an orthogonaldirection for controlling the over-generation andsearch space in Hiero decoder by restricting thedegree of nesting allowed for Hierarchical rules.We propose an novel statistical model forShallow-n grammars which does not require addi-tional non-terminals for monotonic re-ordering andalso eliminates hand-tuned parameters and insteadintroduces an automatically tunable alternative.
Weintroduce a BITG-style (Saers et al, 2009) reorder-ing glue rule (?
3) and a monotonic X-glue rule(?
4).
Our experiments show the resulting Shallow-ndecoding is comparable in translation quality to fullHiero-style decoding while at the same time beingconsiderably faster.All the experiments in this paper were done usingKriya (Sankaran et al, 2012) hierarchical phrase-based system which also supports decoding withShallow-n grammars.
We extended Kriya to addi-tionally support reordering glue rules as well.2 Shallow-n GrammarsFormally a Shallow-n grammar G is defined as a 5-tuple: G = (N,T,R,Rg, S), such that T is a set offinite terminals and N a set of finite non-terminals{X0, .
.
.
, XN}.
Rg refers to the glue rules thatrewrite the start symbol S:S ?
<X, X> (1)S ?
<SX, SX> (2)R is the set of finite production rules in G and hastwo types, viz.
hierarchical (3) and terminal (4).
Thehierarchical rules at each level n are additionallyconditioned to have at least one Xn?1 non-terminal533in them.
?
represents the indices for aligning non-terminals where co-indexed non-terminal pairs arerewritten synchronously.Xn ?
<?, ?, ?
>, ?, ?
?
{{Xn?1} ?
T+} (3)X0 ?
<?, ?>, ?, ?
?
T+ (4)de Gispert et al (2010) also proposed additionalnon-terminals Mk to enable reordering over longerspans by concatenating the hierarchical rules withinthe span.
It also uses additional parameters suchas monotonicity level (K1 and K2), maximum andminimum rule spans allowed for the non-terminals(?3.1 and 3.2 in de Gispert et al (2010)).
The mono-tonicity level parameters determine the number ofnon-terminals that are combined in monotonic or-der at the N ?
1 level and can be adapted to thereordering requirements of specific language pairs.The maximum and minimum rule spans further con-trol the usage of hierarchical rule in a derivation bystipulating the underlying span to be within a rangeof values.
Intuitively, this avoids hierarchical rulesbeing used for a source phrase that is either too shortor too long.
While these parameters offer flexibilityfor adapting the translation system to specific lan-guage pairs, they have to be manually tuned whichis tedious and error-prone.We propose an elegant and automatically tun-able alternative for the Shallow-n grammars setting.Specifically, we introduce a BITG-style reorderingglue rule (?
3) and a monotonic X-glue rule (?
4).Our experiments show the resulting Shallow-n de-coding to perform to the same level as full-Hiero de-coding at the same time being faster.In addition, our implementation of Shallow-ngrammar differs from (de Gispert et al, 2010) inat least two other aspects.
First, their formula-tion constrains the X in the glue rules to be at thetop-level and specifically they define them to be:S ?
<SXN , SXN> and S ?
<XN , XN>,where XN is the non-terminal corresponding to thetop-most level.
Interestingly, this resulted in poorBLEU scores and we found the more generic gluerules (as in (1) and (2)) to perform significantly bet-ter, as we show later.Secondly, they also employ pattern-based filter-ing (Iglesias et al, 2009) in order to reducing redun-dancies in the Hiero grammar by filtering it based oncertain rule patterns.
However in our limited experi-ments, we observed the filtered grammar to performworse than the full grammar, as also noted by (Zoll-mann et al, 2008).
Hence, we do not employ anygrammar filtering in our experiments.3 Reordering Glue RuleIn this paper, we propose an additional BITG-styleglue rule (called R-glue) as in (5) for reordering thephrases along the left-branch of the derivation.S ?
<SX, XS> (5)In order to use this rule sparsely in the derivation,we use a separate feature for this rule and apply apenalty of 1.
Similar to the case of regular gluerules, we experimented with a variant of the reorder-ing glue rule, where X is restricted to the top-level:S ?
<SXN , XNS> and S ?
<XN , XN>.3.1 Language Model IntegrationThe traditional phrase-based decoders using beamsearch generate the target hypotheses in the left-to-right order.
In contrast, Hiero-style systems typ-ically use CKY chart-parsing decoders which canfreely combine target hypotheses generated in inter-mediate cells with hierarchical rules in the highercells.
Thus the generation of the target hypothesesare fragmented and out of order compared to the leftto right order preferred by n-gram language models.This leads to challenges in the estimation of lan-guage model scores for partial target hypothesis,which is being addressed in different ways in theexisting Hiero-style systems.
Some systems add asentence initial marker (<s>) to the beginning ofeach path and some other systems have this implic-itly in the derivation through the translation mod-els.
Thus the language model scores for the hypoth-esis in the intermediate cell are approximated, withthe true language model score (taking into accountsentence boundaries) being computed in the last cellthat spans the entire source sentence.We introduce a novel improvement in computingthe language model scores: for each of the targethypothesis fragment, our approach finds the best po-sition for the fragment in the final sentence and usesthe corresponding score.
We compute three differentscores corresponding to the three positions wherethe fragment can end up in the final sentence, viz.534sentence initial, middle and final: and choose thebest score.
As an example for fragment tf consist-ing of a sequence of target tokens, we compute LMscores for i) <s> tf , ii) tf and iii) tf </s> and usethe best score for pruning alone1.This improvement significantly reduces thesearch errors while performing cube pruning (Chi-ang, 2007) at the cost of additional language modelqueries.
While this approach works well for theusual glue rules, it is particularly effective in the caseof reordering glue rules.
For example, a partial can-didate covering a non-final source span might trans-late to the final position in the target sentence.
If wejust compute the LM score for the target fragmentas is done normally, this might get pruned early onbefore being reordered by the new glue rule.
Our ap-proach instead computes the three LM scores and itwould correctly use the last LM score which is likelyto be the best, for pruning.4 Monotonic Concatenation Glue ruleThe reordering glue rule facilitates reordering at thetop-level.
However, this is still not sufficient to allowlong-distance reordering as the shallow-decoding re-stricts the depth of the derivation.
Consider the Chi-nese example in Table 1, in which translation of theChinese word corresponding to the English phrasethe delegates involves a long distance reordering tothe beginning of the sentence.
Note that, three of thefour human references prefer this long distance re-ordering, while the fourth one avoids the movementby using a complex construction with relative clauseand a sentence initial prepositional phrase.Such long distance reordering is very difficult inconventional Hiero decoding and more so with theShallow-n grammars.
While the R-glue rule per-mit such long distance movements, it also requiresa long phrase generated by a series of rules to bemoved as a block.
We address this issue, by addinga monotonic concatenation (called X-glue) rule thatconcatenates a series of hierarchical rules.
In orderto control overgeneration, we apply this rule only atthe N ?
1 level similar to de Gispert et al (2010).XN?1 ?
<XN?1XN?1, XN?1XN?1> (6)1This ensures the the LM score estimates are never underes-timated for pruning.
We retain the LM score for fragment (caseii) for estimating the score for the full candidate sentence later.However unlike their approach, we use this rule asa feature in the log-linear model so that its weightcan be optimized in the tuning step.
Also, our ap-proach removes the need for additional parametersK1 and K2 for controlling monotonicity, which wasbeing tuned manually in their work.
For the Chineseexample above, shallow-1 decoding using R and X-glue rules achieve the complex movement resultingin a significantly better translation than full-Hierodecoding as shown in the last two lines in Table 1.5 ExperimentsWe present results for Chinese-English translationas it often requires heavy reordering.
We use theHK parallel text and GALE phase-1 corpus consist-ing of?2.3M sentence pairs for training.
For tuningand testing, we use the MTC parts 1 and 3 (1928sentences) and MTC part 4 (919 sentences) respec-tively.
We used the usual pre-processing pipelineand an additional segmentation step for the Chineseside of the bitext using the LDC segmenter2.Our log-linear model uses the standard featuresconditional (p(e|f) and p(f |e)) and lexical (pl(e|f)and pl(f |e)) probabilities, phrase (pp) and word(wp) penalties, language model and regular gluepenalty (mg) apart from two additional features forR?glue (rg) and X?glue (xg).Table 2 shows the BLEU scores and decodingtime for the MTC test-set.
We provide the IBMBLEU (Papineni et al, 2002) scores for the Shallow-n grammars for order: n = 1, 2, 3 and compare it tothe full-Hiero baseline.
Finally, we experiment withtwo variants of the S glue rules, i) a restricted ver-sion where the glue rules combine only X at levelN , (column ?Glue: XN ?
in table), ii) more free vari-ant where they are allowed to use any X freely (col-umn ?Glue: X?
in table).As it can be seen, the unrestricted glue rules vari-ant (column ?Glue: X?)
consistently outperformsthe glue rules restricted to the top-level non-terminalXN , achieving a maximum BLEU score of 26.24,which is about 1.4 BLEU points higher than the lat-ter and is also marginally higher than full Hiero.
Thedecoding speeds for free-Glue and restricted-Gluevariants were mostly identical and so we only pro-vide the decoding time for the latter.
Shallow-2 and2We slightly modified the LDC segmenter, in order to cor-rectly handle non-Chinese characters in ASCII and UTF8.535Source ??????????????????????????????????
?Gloss in argentine capital beunos aires participate united nations global climate conference delegates continueto work.Ref 0 delegates attending the un conference on world climate continue their work in the argentine capital ofbuenos aires.Ref 1 the delegates to the un global climate conference held in Buenos aires, capital city of argentina, go on withtheir work.Ref 2 the delegates continue their works at the united nations global climate talks in buenos aires, capital ofargentinaRef 3 in buenos aires, the capital of argentina, the representatives attending un global climate meeting continuedtheir work.Full-Hiero:Baselinein the argentine capital of buenos aires to attend the un conference on global climate of representativescontinue to work.Sh-1 Hiero: R-glue & X-gluethe representatives were in the argentine capital of beunos aires to attend the un conference on global climatecontinues to work.Table 1: An example for the level of reordering in Chinese-English translationGrammar Glue: XN Glue: X TimeFull Hiero 25.96 0.71Shallow-1 23.54 24.04 0.24+ R-Glue 23.41 24.15 0.25+ X-Glue 23.75 24.74 0.72Shallow-2 24.54 25.12 0.55+ R-Glue 24.75 25.60 0.57+ X-Glue 24.33 25.43 0.69Shallow-3 24.88 25.89 0.62+ R-Glue 24.77 26.24 0.63+ X-Glue 24.75 25.83 0.69Table 2: Results for Chinese-English.
The decoding timeis in secs/word on the Test set for column ?Glue: X?.Bold font indicate best BLEU for each shallow-order.shallow-3 free glue variants achieve BLEU scorescomparable to full-Hiero and at the same time being12?
20% faster.R-glue (rg) appears to contribute more thanthe X-glue (xg) as can be seen in shallow-2 andshallow-3 cases.
Interestingly, xg is more helpful forthe shallow-1 case specifically when the glue rulesare restricted.
As the glue rules are restricted, theX-glue rules concatenates other lower-order rulesbefore being folded into the glue rules.
Both rg andxg improve the BLEU scores by 0.58 over the plainshallow case for shallow orders 1 and 2 and performscomparably for shallow-3 case.
We have also con-ducted experiments for Arabic-English (Table 3) andwe notice that X-glue is more effective and that R-glue is helpful for higher shallow orders.Grammar Glue: X TimeFull Hiero 37.54 0.67Shallow-1 36.90 0.40+ R-Glue 36.98 0.43+ X-Glue 37.21 0.57Shallow-2 36.97 0.57+ R-Glue 36.80 0.58+ X-Glue 37.36 0.61Shallow-3 36.88 0.61+ R-Glue 37.18 0.63+ X-Glue 37.31 0.64Table 3: Results for Arabic-English.
The decoding timeis in secs/word on the Test set.5.1 Effect of our novel LM integrationHere we analyze the effect of our novel LM integra-tion approach in terms of BLEU score and search er-rors comparing it to the naive method used in typicalHiero systems.
In shallow setting, our method im-proved the BLEU scores by 0.4 for both Ar-En andCn-En.
In order to quantify the change in the searcherrors, we compare the model scores of the (corre-sponding) candidates in the N-best lists obtained bythe two methods and compute the % of high scor-ing candidates in each.
Our approach was clearlysuperior with 94.6% and 77.3% of candidates hav-ing better scores respectively for Cn-En and Ar-En.In full decoding setting the margin of improvementswere reduced slightly- BLEU improved by 0.3 andabout 57?69% of target candidates had better modelscores for the two language pairs.536ReferencesPhil Blunsom, Trevor Cohn, and Miles Osborne.
2008.Bayesian synchronous grammar induction.
In Pro-ceedings of Neural Information Processing Systems.Phil Blunsom, Trevor Cohn, Chris Dyer, and Miles Os-borne.
2009.
A gibbs sampler for phrasal synchronousgrammar induction.
In Proceedings of Association ofComputational Linguistics, pages 782?790.David Chiang.
2005.
A hierarchical phrase-based modelfor statistical machine translation.
In Proceedings ofAssociation of Computational Linguistics, pages 263?270.David Chiang.
2007.
Hierarchical phrase-based transla-tion.
Computational Linguistics, 33.Adria` de Gispert, Gonzalo Iglesias, Graeme Blackwood,Eduardo R. Banga, and William Byrne.
2010.
Hier-archical phrase-based translation with weighted finite-state transducers and shallow-n grammars.
Computa-tional Linguistics, 36.Zhongjun He, Yao Meng, and Hao Yu.
2009.
Discardingmonotone composed rule for hierarchical phrase-basedstatistical machine translation.
In Proceedings of the3rd International Universal Communication Sympo-sium, pages 25?29.Gonzalo Iglesias, Adria` de Gispert, Eduardo R. Banga,and William Byrne.
2009.
Rule filtering by patternfor efficient hierarchical translation.
In Proceedings ofthe 12th Conference of the European Chapter of theACL, pages 380?388.Kishore Papineni, Salim Roukos, Todd Ward, and Weijing Zhu.
2002.
Bleu: a method for automatic eval-uation of machine translation.
In Proceedings of theAnnual Meeting of Association of Computational Lin-guistics, pages 311?318.Markus Saers, Joakim Nivre, and Dekai Wu.
2009.Learning stochastic bracketing inversion transductiongrammars with a cubic time biparsing algorithm.
InProceedings of the 11th International Conference onParsing Technologies, pages 29?32.
Association forComputational Linguistics.Baskaran Sankaran, Gholamreza Haffari, and AnoopSarkar.
2011.
Bayesian extraction of minimal scfgrules for hierarchical phrase-based translation.
In Pro-ceedings of the Sixth Workshop on Statistical MachineTranslation, pages 533?541.Baskaran Sankaran, Majid Razmara, and Anoop Sarkar.2012.
Kriya ?
an end-to-end hierarchical phrase-basedmt system.
The Prague Bulletin of Mathematical Lin-guistics, (97):83?98, April.Andreas Zollmann, Ashish Venugopal, Franz Och, andJay Ponte.
2008.
A systematic comparison of phrase-based, hierarchical and syntax-augmented statisticalmt.
In Proceedings of the 22nd International Confer-ence on Computational Linguistics, pages 1145?1152.537
