Proceedings of NAACL-HLT 2013, pages 518?523,Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational LinguisticsSupervised Bilingual Lexicon Induction with Multiple Monolingual SignalsAnn IrvineCenter for Language and Speech ProcessingJohns Hopkins UniversityChris Callison-Burch?Computer and Information Science Dept.University of PennsylvaniaAbstractPrior research into learning translations fromsource and target language monolingual textshas treated the task as an unsupervised learn-ing problem.
Although many techniques takeadvantage of a seed bilingual lexicon, thiswork is the first to use that data for super-vised learning to combine a diverse set of sig-nals derived from a pair of monolingual cor-pora into a single discriminative model.
Evenin a low resource machine translation setting,where induced translations have the potentialto improve performance substantially, it is rea-sonable to assume access to some amount ofdata to perform this kind of optimization.
Ourwork shows that only a few hundred transla-tion pairs are needed to achieve strong per-formance on the bilingual lexicon inductiontask, and our approach yields an average rel-ative gain in accuracy of nearly 50% over anunsupervised baseline.
Large gains in accu-racy hold for all 22 languages (low and highresource) that we investigate.1 IntroductionBilingual lexicon induction is the task of identifyingword translation pairs using source and target mono-lingual corpora, which are often comparable.
Mostapproaches to the task are based on the idea thatwords that are translations of one another have sim-ilar distributional properties across languages.
Priorresearch has shown that contextual similarity (Rapp,1995), temporal similarity (Schafer and Yarowsky,2002), and topical information (Mimno et al 2009)?Performed while faculty at Johns Hopkins Universityare all good signals for learning translations frommonolingual texts.Most prior work either makes use of only one ortwo monolingual signals or uses unsupervised meth-ods (like rank combination) to aggregate orthogonalsignals (Schafer and Yarowsky, 2002; Klementievand Roth, 2006).
Surprisingly, no past research hasemployed supervised approaches to combine diversemonolingually-derived signals for bilingual lexiconinduction.
The field of machine learning has showndecisively that supervised models dramatically out-perform unsupervised models, including for closelyrelated problems like statistical machine translation(Och and Ney, 2002).For the bilingual lexicon induction task, a super-vised approach is natural, particularly because com-puting contextual similarity typically requires a seedbilingual dictionary (Rapp, 1995), and that samedictionary may be used for estimating the param-eters of a model to combine monolingual signals.Alternatively, in a low resource machine transla-tion (MT) setting, it is reasonable to assume a smallamount of parallel data from which a bilingual dic-tionary can be extracted for supervision.
In this set-ting, bilingual lexicon induction is critical for trans-lating source words which do not appear in the par-allel data or dictionary.We frame bilingual lexicon induction as a binaryclassification problem; for a pair of source and tar-get language words, we predict whether the two aretranslations of one another or not.
For a given sourcelanguage word, we score all target language can-didates separately and then rerank them.
We usea variety of signals derived from source and target518monolingual corpora as features and use supervisionto estimate the strength of each.
In this work we:?
Use the following similarity metrics derivedfrom monolingual corpora to score word pairs:contextual, temporal, topical, orthographic, andfrequency.?
For the first time, explore using supervision tocombine monolingual signals and learn a dis-criminative model for predicting translations.?
Present results for 22 low and high resourcelanguages paired with English and show largeaccuracy gains over an unsupervised baseline.2 Previous WorkPrior work suggests that a wide variety of mono-lingual signals, including distributional, temporal,topic, and string similarity, may inform bilinguallexicon induction (Rapp, 1995; Fung and Yee, 1998;Rapp, 1999; Schafer and Yarowsky, 2002; Schafer,2006; Klementiev and Roth, 2006; Koehn andKnight, 2002; Haghighi et al 2008; Mimno etal., 2009; Mausam et al 2010).
Klementiev et al(2012) use many of those signals to score an exist-ing phrase table for end-to-end MT but do not learnany new translations.
Schafer and Yarowsky (2002)use an unsupervised rank-combination method forcombining orthographic, contextual, temporal, andfrequency similarities into a single ranking.Recently, Ravi and Knight (2011), Dou andKnight (2012), and Nuhn et al(2012) have workedtoward learning a phrase-based translation modelfrom monolingual corpora, relying on deciphermenttechniques.
In contrast to that work, we use aseed bilingual lexicon for supervision and multiplemonolingual signals proposed in prior work.Haghighi et al(2008) and Daume?
and Jagarla-mudi (2011) use some supervision to learn how toproject contextual and orthographic features into alow-dimensional space, with the goal of represent-ing words which are translations of one anotheras vectors which are close together in that space.However, both of those approaches focus on onlytwo signals, high resource languages, and frequentwords (frequent nouns, in the case of Haghighi etal.
(2008)).
In our classification framework, we canincorporate any number of monolingual signals, in-Language #Words Language #WordsNepali 0.4 Somali 0.5Uzbek 1.4 Azeri 2.6Tamil 3.7 Albanian 6.5Bengali 6.6 Welsh 7.5Bosnian 12.9 Latvian 40.2Indonesian 21.8 Romanian 24.1Serbian 25.8 Turkish 31.2Ukrainian 37.6 Hindi 47.4Bulgarian 49.5 Polish 104.5Slovak 124.3 Urdu 287.2Farsi 710.3 Spanish 972Table 1: Millions of monolingual web crawl andWikipedia word tokenscluding contextual and string similarity, and directlylearn how to combine them.3 Monolingual Data and Signals3.1 DataThroughout our experiments, we seek to learn howto translate words in a given source language intoEnglish.
Table 1 lists our languages of interest,along with the total amount of monolingual datathat we use for each.
We use web crawled time-stamped news articles to estimate temporal sim-ilarity, Wikipedia pages which are inter-linguallylinked to English pages to estimate topic similarity,and both datasets to estimate frequency and contex-tual similarity.
Following Irvine et al(2010), weuse pairs of Wikipedia page titles to train a simpletransliterator for languages written in a non-Romanscript, which allows us to compute orthographicsimilarity for pairs of words in different scripts.3.2 SignalsOur definitions of orthographic, topic, temporal, andcontextual similarity are taken from Klementiev etal.
(2012), and the details of each may be foundthere.
Here, we give briefly describe them and giveour definition of a novel, frequency-based signal.Orthographic We measure orthographic similar-ity between a pair of words as the normalized1 editdistance between the two words.
For non-Romanscript languages, we transliterate words into the Ro-man script before measuring orthographic similarity.TopicWe use monolingual Wikipedia pages to es-timate topical signatures for each source and target1Normalized by the average of the lengths of the two words519language word.
Signature vectors are the length ofthe number of inter-lingually linked source and En-glish Wikipedia pages and contain counts of howmany times the word appears on each page.
We usecosine similarity to compare pairs of signatures.Temporal We use time-stamped web crawl datato estimate temporal signatures, which, for a givenword, are the length of the number of time-stamps(dates) and contain counts of how many times theword appears in news articles with the given date.We use a sliding window of three days and use co-sine similarity to compare signatures.
We expectthat source and target language words which aretranslations of one another will appear with similarfrequencies over time in monolingual data.Contextual We score monolingual contextualsimilarity by first collecting context vectors for eachsource and target language word.
The context vectorfor a given word contains counts of how many timeswords appear in its context.
We use bag of wordscontexts in a window of size two.
We gather bothsource and target language contextual vectors fromour web crawl data and Wikipedia data (separately).Frequency Words that are translations of one an-other are likely to have similar relative frequenciesin monolingual corpora.
We measure the frequencysimilarity of two words as the absolute value of thedifference between the logs of their relative mono-lingual corpus frequencies.4 Supervised Bilingual Lexicon Induction4.1 BaselineOur unsupervised baseline method is based onranked lists derived from each of the signals listedabove.
For each source word, we generate rankedlists of English candidates using the following sixsignals: Crawls Context, Crawls Time, WikipediaContext, Wikipedia Topic, Edit distance, and LogFrequency Difference.
Then, for each English can-didate we compute its mean reciprocal rank2 (MRR)based on the six ranked lists.
The baseline ranks En-glish candidates according to the MRR scores.
Forevaluation, we use the same test sets, accuracy met-ric, and correct translations described below.2The MRR of the jth English word, ej , is 1NPNi=11rankij,where N is the number of signals and rankij is ej?s rank ac-cording to signal i.4.2 Supervised ApproachIn addition to the monolingual resources describedin Section 3.1, we have a bilingual dictionary foreach language, which we use to project context vec-tors and for supervision and evaluation.
For eachlanguage, we choose up to 8, 000 source languagewords among those that occur in the monolingualdata at least three times and that have at least onetranslation in our dictionary.
We randomly dividethe source language words into three equally sizedsets for training, development, and testing.
We usethe training data to train a classifier, the develop-ment data to choose the best classification settingsand feature set, and the test set for evaluation.For all experiments, we use a linear classifiertrained by stochastic gradient descent to minimizesquared error3 and perform 100 passes over thetraining data.4 The binary classifiers predict whethera pair of words are translations of one another or not.The translations in our training data serve as posi-tive supervision, and the source language words inthe training data paired with random English words5serve as negative supervision.
We used our develop-ment data to tune the number of negative examplesto three for each positive example.
At test time, af-ter scoring all source language words in the test setpaired with all English words in our candidate set,6we rank the English candidates by their classifica-tion scores and evaluate accuracy in the top-k trans-lations.4.3 FeaturesOur monolingual features are listed below and arebased on raw similarity scores as well as ranks:?
Crawls Context: Web crawl context similarity score?
Crawls Context RR: reciprocal rank of crawls con-text3We tried using logistic rather than linear regression, butperformance differences on our development set were verysmall and not statistically significant.4We use http://hunch.net/?vw/ version 6.1.4, andrun it with the following arguments that affect how updates aremade in learning: ?exact adaptive norm ?power t 0.55Among those that appear at least five times in our monolin-gual data, consistent with our candidate set.6All English words appearing at least five times in ourmonolingual data.
In practice, we further limit the set to thosethat occur in the top-1000 ranked list according to at least oneof our signals.520??
?0.00.20.40.60.81.0Accuracy inTop?10CrawlContext EditDist CrawlTime WikiContext WikiTopic Is?Ident.
DiffLg?Frq DiscrimAllFigure 1: Each box-and-whisker plot summarizes per-formance on the development set using the given fea-ture(s) across all 22 languages.
For each source wordin our development sets, we rank all English target wordsaccording to the monolingual similarity metric(s) listed.All but the last plot show the performance of individualfeatures.
Discrim-All uses supervised data to train classi-fiers for each language based on all of the features.?
Crawls Time: Web crawl temporal similarity score?
Crawls Time RR: reciprocal rank of crawls time?
Edit distance: normalized (by average length ofsource and target word) edit distance?
Edit distance RR: reciprocal rank of edit distance?
Wiki Context: Wikipedia context similarity score?
Wiki Context RR: recip.
rank of wiki context?
Wiki Topic: Wikipedia topic similarity score?
Wiki Topic RR: recip.
rank of wiki topic?
Is-Identical: source and target words are the same?
Difference in log frequencies: Difference betweenthe logs of the source and target word monolingualfrequencies?
Log Freqs Diff RR: reciprocal rank of difference inlog frequenciesWe train classifiers separately for each source lan-guage, and the learned weights vary based on, forexample, corpora size and the relatedness of thesource language and English (e.g.
edit distance isinformative if there are many cognates).
In order touse the trained classifiers to make top-k translationpredictions for a given source word, we rank candi-dates by their classification scores.4.4 Feature Evaluation and SelectionAfter training initial classifiers, we use our develop-ment data to choose the most informative subset offeatures.
Figure 1 shows the top-10 accuracy on thedevelopment data when we use individual features?0.00.20.40.60.81.0Accuracy inTop?10WikiTopic WikiContext DiffLog?Freq EditDist.
EditDist.
RR CrawlContext AllFeaturesFigure 2: Performance on the development set goes upas features are greedily added to the feature space.
Meanperformance is slightly higher using this subset of six fea-tures (second to last bar) than using all features (last bar).to predict translations.
Top-10 accuracy refers to thepercent of source language words for which a correctEnglish translation appears in the top-10 ranked En-glish candidates.
Each box-and-whisker plot sum-marizes performance over the 22 languages.
Wedon?t display reciprocal rank features, as their per-formance is very similar to that of the correspond-ing raw similarity score.
It?s easy to see that featuresbased on the Wikipedia topic signal are the most in-formative.
It is also clear that training a supervisedmodel to combine all of the features (the last plot)yields performance that is dramatically higher thanusing any individual feature alone.Figure 2, from left to right, shows a greedy searchfor the best subset of features among those listedabove.
Again, the Wikipedia topic score is the mostinformative stand-alone feature, and the Wikipediacontext score is the most informative second feature.Adding features to the model beyond the six shownin the figure does not yield additional performancegains over our set of languages.4.5 Learning Curve AnalysisFigure 3 shows learning curves over the number ofpositive training instances.
In all cases, the numberof randomly generated negative training instancesis three times the number of positive.
For all lan-guages, performance is stable after about 300 cor-rect translations are used for training.
This showsthat our supervised method for combining signalsrequires only a small training dictionary.521?????
??
?0 200 400 600 800 1000 12000.00.20.40.60.81.0???????
??
?
?????????
?
?
??????
??
??
??????
??
????????
????????
?
??????
????????????????
?SpanishRomanianPolishBulgarianIndonesianWelshSlovakBosnianLatvianAlbanianUkrainianTurkishAzeriSerbianHindiBengaliUzbekFarsiSomaliTamilUrduNepaliPositive training data instancesAccuracy inTop?10Figure 3: Learning curves over number of positive train-ing instances, up to 1250.
For some languages, 1250positive training instances are not available.
In all cases,evaluation is on the development data and the number ofnegative training instances is three times the number ofpositive.
For all languages, performance is fairly stableafter about 300 positive training instances.5 ResultsWe use a model based on the six features shownin Figure 2 to score and rank English translationcandidates for the test set words in each language.Table 2 gives the result for each language for theMRR baseline and our supervised technique.
Acrosslanguages, the average top-10 accuracy using theMRR baseline is 30.4, and the average using ourtechnique is 43.9, a relative improvement of about44%.
We did not attempt a comparison with moresophisticated unsupervised rank aggregation meth-ods.
However, we believe the improvements weobserve drastically outweigh the expected perfor-mance differences between different rank aggrega-tion methods.
Figure 4 plots the accuracies yieldedby our supervised technique versus the total amountof monolingual data for each language.
An increasein monolingual data tends to improve accuracy.
Thecorrelation isn?t perfect, however.
For example, per-formance on Urdu and Farsi is relatively poor, de-spite the large amounts of monolingual data avail-able for each.
This may be due to the fact that wehave large web crawls for those languages, but theirWikipedia datasets, which tend to provide a strongtopic signal, are relatively small.AzeriBengaliBosnianWelshHindiIndonesianLatvianNepaliRomanianSlovakSomaliAlbanianSerbianTamilUzbekFarsiSpanishUrduTurkishBulgarianUkranianPolishMillions of Monolingual Word TokensAccuracy1e?01 1e+00 1e+01 1e+02 1e+030.00.20.40.60.81.0Figure 4: Millions of monolingual word tokens vs. Lex-icon Induction Top-10 AccuracyLang MRR Supv.
Lang MRR Supv.Nepali 11.2 13.6 Somali 16.7 18.1Uzbek 23.2 29.6 Azeri 16.1 29.4Tamil 28.4 33.3 Albanian 32.0 45.3Bengali 19.3 32.8 Welsh 36.1 56.4Bosnian 32.6 52.8 Latvian 29.6 47.7Indonesian 41.5 63.5 Romanian 53.3 71.6Serbian 29.0 33.3 Turkish 31.4 52.1Ukrainian 29.7 46.0 Hindi 18.2 34.6Bulgarian 40.2 57.9 Polish 47.4 67.1Slovak 34.6 53.5 Urdu 13.2 21.2Farsi 10.5 21.1 Spanish 74.8 85.0Table 2: Top-10 Accuracy on test set.
Performanceincreases for all languages moving from the baseline(MRR) to discriminative training (Supv).6 ConclusionsOn average, we observe relative gains of more than44% over an unsupervised rank-combination base-line by using a seed bilingual dictionary and a di-verse set of monolingual signals to train a supervisedclassifier.
Using supervision for bilingual lexicon in-duction makes sense.
In some cases a dictionary isalready assumed for computing contextual similar-ity, and, in the remaining cases, one could be com-piled easy, either automatically, e.g.
Haghighi et al(2008), or through crowdsourcing, e.g.
Irvine andKlementiev (2010) and Callison-Burch and Dredze(2010).
We have shown that only a few hundredtranslation pairs are needed to achieve good perfor-mance.
Our framework has the additional advantagethat any new monolingually-derived similarity met-rics can easily be added as new features.5227 AcknowledgementsThis material is based on research sponsored byDARPA under contract HR0011-09-1-0044 and bythe Johns Hopkins University Human LanguageTechnology Center of Excellence.
The views andconclusions contained in this publication are thoseof the authors and should not be interpreted as repre-senting official policies or endorsements of DARPAor the U.S. Government.ReferencesChris Callison-Burch and Mark Dredze.
2010.
Creatingspeech and language data with Amazon?s MechanicalTurk.
In Proceedings of the NAACL Workshop on Cre-ating Speech and Language Data with Amazon?s Me-chanical Turk.Hal Daume?, III and Jagadeesh Jagarlamudi.
2011.
Do-main adaptation for machine translation by mining un-seen words.
In Proceedings of the Conference of theAssociation for Computational Linguistics (ACL).Qing Dou and Kevin Knight.
2012.
Large scale deci-pherment for out-of-domain machine translation.
InProceedings of the Joint Conference on EmpiricalMethods in Natural Language Processing and Com-putational Natural Language Learning.Pascale Fung and Lo Yuen Yee.
1998.
An IR approachfor translating new words from nonparallel, compara-ble texts.
In Proceedings of the Conference of the As-sociation for Computational Linguistics (ACL).Aria Haghighi, Percy Liang, Taylor Berg-Kirkpatrick,and Dan Klein.
2008.
Learning bilingual lexiconsfrommonolingual corpora.
In Proceedings of the Con-ference of the Association for Computational Linguis-tics (ACL).Ann Irvine and Alexandre Klementiev.
2010.
Using me-chanical turk to annotate lexicons for less commonlyused languages.
In Proceedings of the NAACL Work-shop on Creating Speech and Language Data withAmazon?s Mechanical Turk.Ann Irvine, Chris Callison-Burch, and Alexandre Kle-mentiev.
2010.
Transliterating from all languages.
InProceedings of the Conference of the Association forMachine Translation in the Americas (AMTA).Alexandre Klementiev and Dan Roth.
2006.
Weaklysupervised named entity transliteration and discoveryfrom multilingual comparable corpora.
In Proceed-ings of the Conference of the Association for Compu-tational Linguistics (ACL).Alex Klementiev, Ann Irvine, Chris Callison-Burch, andDavid Yarowsky.
2012.
Toward statistical machinetranslation without parallel corpora.
In Proceedings ofthe Conference of the European Association for Com-putational Linguistics (EACL).Philipp Koehn and Kevin Knight.
2002.
Learning atranslation lexicon from monolingual corpora.
In ACLWorkshop on Unsupervised Lexical Acquisition.Mausam, Stephen Soderland, Oren Etzioni, Daniel S.Weld, Kobi Reiter, Michael Skinner, Marcus Sammer,and Jeff Bilmes.
2010.
Panlingual lexical transla-tion via probabilistic inference.
Artificial Intelligence,174:619?637, June.DavidMimno, HannaWallach, Jason Naradowsky, DavidSmith, and Andrew McCallum.
2009.
Polylingualtopic models.
In Proceedings of the Conference onEmpirical Methods in Natural Language Processing(EMNLP).Malte Nuhn, Arne Mauser, and Hermann Ney.
2012.Deciphering foreign language by combining languagemodels and context vectors.
In Proceedings of theConference of the Association for Computational Lin-guistics (ACL).Franz Josef Och and Hermann Ney.
2002.
Discrimina-tive training and maximum entropy models for statisti-cal machine translation.
In Proceedings of the Confer-ence of the Association for Computational Linguistics(ACL).Reinhard Rapp.
1995.
Identifying word translations innon-parallel texts.
In Proceedings of the Conference ofthe Association for Computational Linguistics (ACL).Reinhard Rapp.
1999.
Automatic identification of wordtranslations from unrelated English and German cor-pora.
In Proceedings of the Conference of the Associ-ation for Computational Linguistics (ACL).Sujith Ravi and Kevin Knight.
2011.
Deciphering for-eign language.
In Proceedings of the Conference ofthe Association for Computational Linguistics (ACL).Charles Schafer and David Yarowsky.
2002.
Induc-ing translation lexicons via diverse similarity measuresand bridge languages.
In Proceedings of the Confer-ence on Natural Language Learning (CoNLL).Charles Schafer.
2006.
Translation Discovery Using Di-verse Similarity Measures.
Ph.D. thesis, Johns Hop-kins University.523
