Proceedings of the COLING/ACL 2006 Interactive Presentation Sessions, pages 65?68,Sydney, July 2006. c?2006 Association for Computational LinguisticsAn Implemented Description of Japanese:The Lexeed Dictionary and the Hinoki TreebankSanae Fujita, Takaaki Tanaka, Francis Bond, Hiromi NakaiwaNTT Communication Science Laboratories,Nippon Telegraph and Telephone Corporation{sanae, takaaki, bond, nakaiwa}@cslab.kecl.ntt.co.jpAbstractIn this paper we describe the current stateof a new Japanese lexical resource: theHinoki treebank.
The treebank is builtfrom dictionary definition sentences, anduses an HPSG based Japanese grammar toencode both syntactic and semantic infor-mation.
It is combined with an ontologybased on the definition sentences to give adetailed sense level description of the mostfamiliar 28,000 words of Japanese.1 IntroductionIn this paper we describe the current state of anew lexical resource: the Hinoki treebank.
Theultimate goal of our research is natural languageunderstanding ?
we aim to create a system thatcan parse text into some useful semantic represen-tation.
This is an ambitious goal, and this pre-sentation does not present a complete solution,but rather a road-map to the solution, with someprogress along the way.The first phase of the project, which we presenthere, is to construct a syntactically and semanti-cally annotated corpus based on the machine read-able dictionary Lexeed (Kasahara et al, 2004).This is a hand built self-contained lexicon: it con-sists of headwords and their definitions for themost familiar 28,000 words of Japanese.
Eachdefinition and example sentence has been parsed,and the most appropriate analysis selected.
Eachcontent word in the sentences has been markedwith the appropriate Lexeed sense.
The syntac-tic model is embodied in a grammar, while the se-mantic model is linked by an ontology.
This makesit possible to test the use of similarity and/or se-mantic class based back-offs for parsing and gen-eration with both symbolic grammars and statisti-cal models.In order to make the system self sustaining webase the first growth of our treebank on the dic-tionary definition sentences themselves.
We thentrain a statistical model on the treebank and parsethe entire lexicon.
From this we induce a the-saurus.
We are currently tagging other genres withthe same information.
We will then use this infor-mation and the thesaurus to build a parsing modelthat combines syntactic and semantic information.We will also produce a richer ontology ?
for ex-ample extracting selectional preferences.
In thelast phase, we will look at ways of extending ourlexicon and ontology to less familiar words.2 The Lexeed Semantic Database ofJapaneseThe Lexeed Semantic Database of Japanese con-sists of all Japanese words with a familiaritygreater than or equal to five on a seven pointscale (Kasahara et al, 2004).
This gives 28,000words in all, with 46,000 different senses.
Defini-tion sentences for these sentences were rewrittento use only the 28,000 familiar words (and somefunction words).
The defining vocabulary is ac-tually 16,900 different words (60% of all possi-ble words).
A simplified example entry for thelast two senses of the word doraibfla?driver?
is given in Figure 1, with English glossesadded, but omitting the example sentences.
Lex-eed itself consists of just the definitions, familiar-ity and part of speech, all the underlined featuresare those added by the Hinoki project.3 The Hinoki TreebankThe structure of our treebank is inspired by theRedwoods treebank of English (Oepen et al,2002) in which utterances are parsed and the anno-tator selects the best parse from the full analysesderived by the grammar.
We had four main rea-sons for selecting this approach.
The first was thatwe wanted to develop a precise broad-coverage65???????????????????????????????
?INDEX doraiba?POS noun Lexical-Type noun-lexFAMILIARITY 6.5 [1?7] (?
5) Frequency 37 Entropy 0.79SENSE 1 .
.
.SENSE 2P(S2) = 0.84??????
?DEFINITION 1/ / 1/ / 1/Someone who drives a car.HYPERNYM 1 hito ?person?SEM.
CLASS ?292:chauffeur/driver?
(?
?5:person?
)WORDNET driver1??????
?SENSE 3P(S2) = 0.05?????????
?DEFINITION 1/ / / 1/ / / 3/ / /In golf, a long-distance club.
A number one wood.HYPERNYM 3 kurabu ?club?SEM.
CLASS ?921:leisure equipment?
(?
921)WORDNET driver5DOMAIN 1 gorufu ?golf??????????????????????????????????????????
?Figure 1: Entry for the Word doraibfla ?driver?
(with English glosses)grammar in tandem with the treebank, as part ofour research into natural language understanding.Treebanking the output of the parser allows usto immediately identify problems in the grammar,and improving the grammar directly improves thequality of the treebank in a mutually beneficialfeedback loop.The second reason is that we wanted to annotateto a high level of detail, marking not only depen-dency and constituent structure but also detailedsemantic relations.
By using a Japanese gram-mar (JACY: Siegel (2000)) based on a monostrataltheory of grammar (Head Driven Phrase StructureGrammar) we could simultaneously annotate syn-tactic and semantic structure without overburden-ing the annotator.
The treebank records the com-plete syntacto-semantic analysis provided by theHPSG grammar, along with an annotator?s choiceof the most appropriate parse.
From this record,all kinds of information can be extracted at variouslevels of granularity: A simplified example of thelabeled tree, minimal recursion semantics repre-sentation (MRS) and semantic dependency viewsfor the definition of 2 doraibfla ?driver?is given in Figure 2.The third reason was that use of the grammar asa base enforces consistency ?
all sentences anno-tated are guaranteed to have well-formed parses.The last reason was the availability of a reason-ably robust existing HPSG of Japanese (JACY),and a wide range of open source tools for de-veloping the grammars.
We made extensive useof tools from the the Deep Linguistic Process-ing with HPSG Initiative (DELPH-IN: http://www.delph-in.net/) These existing resourcesenabled us to rapidly develop and test our ap-proach.3.1 Syntactic AnnotationThe construction of the treebank is a two stageprocess.
First, the corpus is parsed (in our caseusing JACY), and then the annotator selects thecorrect analysis (or occasionally rejects all anal-yses).
Selection is done through a choice of dis-criminants.
The system selects features that distin-guish between different parses, and the annotatorselects or rejects the features until only one parseis left.
The number of decisions for each sentenceis proportional to log2 in the length of the sentence(Tanaka et al, 2005).
Because the disambiguat-ing choices made by the annotators are saved, itis possible to semi-automatically update the tree-bank when the grammar changes.
Re-annotationis only necessary in cases where the parse has be-come more ambiguous or, more rarely, existingrules or lexical items have changed so much thatthe system cannot reconstruct the parse.The Lexeed definition sentences were alreadyPOS tagged.
We experimented with using the POStags to mark trees as good or bad (Tanaka et al,2005).
This enabled us to reduce the number ofannotator decisions by 20%.One concern with Redwoods style treebankingis that it is only possible to annotate those treesthat the grammar can parse.
Sentences for whichno analysis had been implemented in the grammaror which fail to parse due to processing constraintsare left unannotated.
This makes grammar cov-66UTTERANCENPVP NPP VN CASE-P V Vjidflosha o unten suru hitocar ACC drive do personParse Tree?h0,x1{h0 :proposition m(h1)h1 :hito n(x1) ?person?h2 :ude f q(x1,h1,h6)h3 : jidosha n(x2) ?car?h4 :ude f q(x2,h3,h7)h5 :unten s(e1,x1,x2)}?
?drive?MRS{x1 :e1 :unten s(ARG1 x1 : hito n,ARG2 x2 : jidosha n)r1 : proposition m(MARG e1 : unten s)}Semantic DependencyFigure 2: Parse Tree, Simplified MRS and Dependency Views for 2 doraibfla ?driver?erage a significant issue.
We extended JACY byadding the defining vocabulary, and added somenew rules and lexical-types (more detail is givenin Bond et al (2004)).
None of the rules are spe-cific to the dictionary domain.
The grammaticalcoverage over all sentences is now 86%.
Around12% of the parsed sentences were rejected by thetreebankers due to an incomplete semantic repre-sentation.
The total size of the treebank is cur-rently 53,600 definition sentences and 36,000 ex-ample sentences: 89,600 sentences in total.3.2 Sense AnnotationAll open class words were annotated with theirsense by five annotators.
Inter-annotator agree-ment ranges from 0.79 to 0.83.
For example, theword kurabu ?club?
is tagged as sense 3 inthe definition sentence for driver3, with the mean-ing ?golf-club?.
For each sense, we calculate theentropy and per sense probabilities over four cor-pora: the Lexeed definition and example sentencesand Newspaper text from the Kyoto University andSenseval 2 corpora (Tanaka et al, 2006).4 Applications4.1 Stochastic Parse RankingUsing the treebanked data, we built a stochasticparse ranking model.
The ranker uses a maximumentropy learner to train a PCFG over the parsederivation trees, with the current node, two grand-parents and several other conditioning features.
Apreliminary experiment showed the correct parseis ranked first 69% of the time (10-fold cross val-idation on 13,000 sentences; evaluated per sen-tence).
We are now experimenting with extensionsbased on constituent weight, hypernym, semanticclass and selectional preferences.4.2 Ontology AcquisitionTo extract hypernyms, we parse the first defini-tion sentence for each sense (Nichols et al, 2005).The parser uses the stochastic parse ranking modellearned from the Hinoki treebank, and returns thesemantic representation (MRS) of the first rankedparse.
In cases where JACY fails to return a parse,we use a dependency parser instead.
The highestscoping real predicate is generally the hypernym.For example, for doraibfla2 the hypernym is hito?person?
and for doraib fla3 the hypernym iskurabu ?club?
(see Figure 1).
We also extractother relationships, such as synonym and domain.Because the words are sense tags, we can special-ize the relations to relations between senses, ratherthan just words: ?hypernym: doraiba?3, kurabu3?.Once we have synonym/hypernym relations, wecan link the lexicon to other lexical resources.
Forexample, for the manually constructed Japaneseontology Goi-Taikei (Ikehara et al, 1997) we linkto its semantic classes by the following heuristic:look up the semantic classes C for both the head-word (wi) and hypernym(s) (wg).
If at least one ofthe index word?s classes is subsumed by at leastone of the genus?
classes, then we consider the re-lationship confirmed.
To link cross-linguistically,we look up the headwords and hypernym(s) in atranslation lexicon and compare the set of trans-lations ci ?
C(T (wi)) with WordNet (Fellbaum,1998)).
Although looking up the translation addsnoise, the additional filter of the relationship tripleeffectively filters it out again.Adding the ontology to the dictionary interfacemakes a far more flexible resource.
For example,by clicking on the ?hypernym: doraiba?3, goru f u1?link, it is possible to see a list of all the senses re-67lated to golf, a link that is inaccessible in the paperdictionary.4.3 Semi-Automatic GrammarDocumentationA detailed grammar is a fundamental componentfor precise natural language processing.
It pro-vides not only detailed syntactic and morphologi-cal information on linguistic expressions but alsoprecise and usually language-independent seman-tic structures of them.
To simplify grammar de-velopment, we take a snapshot of the grammarused to treebank in each development cycle.
Fromthis we extract information about lexical itemsand their types from both the grammar and tree-bank and convert it into an electronically accesi-ble structured database (the lexical-type database:Hashimoto et al, 2005).
This allows grammar de-velopers and treebankers to see comprehensive up-to-date information about lexical types, includingdocumentation, syntactic properties (super types,valence, category and so on), usage examples fromthe treebank and links to other dictionaries.5 Further WorkWe are currently concentrating on three tasks.
Thefirst is improving the coverage of the grammar,so that we can parse more sentences to a cor-rect parse.
The second is improving the knowl-edge acquisition, in particular learning other in-formation from the parsed defining sentences ?such as lexical-types, semantic association scores,meronyms, and antonyms.
The third task is addingthe knowledge of hypernyms into the stochasticmodel.The Hinoki project is being extended in severalways.
For Japanese, we are treebanking other gen-res, starting with Newspaper text, and increasingthe vocabulary, initially by parsing other machinereadable dictionaries.
We are also extending theapproach multilingually with other grammars inthe DELPH-IN group.
We have started with theEnglish Resource Grammar and the Gnu Contem-porary International Dictionary of English and areinvestigating Korean and Norwegian through co-operation with the Korean Research Grammar andNorSource.6 ConclusionIn this paper we have described the current state ofthe Hinoki treebank.
We have further showed howit is being used to develop a language-independentsystem for acquiring thesauruses from machine-readable dictionaries.With the improved the grammar and ontology,we will use the knowledge learned to extend ourmodel to words not in Lexeed, using definitionsentences from machine-readable dictionaries orwhere they appear within normal text.
In this way,we can grow an extensible lexicon and thesaurusfrom Lexeed.AcknowledgementsWe thank the treebankers, Takayuki Kurib-ayashi, Tomoko Hirata and Koji Yamashita, fortheir hard work and attention to detail.ReferencesFrancis Bond, Sanae Fujita, Chikara Hashimoto, KanameKasahara, Shigeko Nariyama, Eric Nichols, Akira Ohtani,Takaaki Tanaka, and Shigeaki Amano.
2004.
The Hinokitreebank: A treebank for text understanding.
In Proceed-ings of the First International Joint Conference on NaturalLanguage Processing (IJCNLP-04).
Springer Verlag.
(inpress).Christine Fellbaum, editor.
1998.
WordNet: An ElectronicLexical Database.
MIT Press.Chikara Hashimoto, Francis Bond, Takaaki Tanaka, andMelanie Siegel.
2005.
Integration of a lexical typedatabase with a linguistically interpreted corpus.
In 6thInternational Workshop on Linguistically Integrated Cor-pora (LINC-2005), pages 31?40.
Cheju, Korea.Satoru Ikehara, Masahiro Miyazaki, Satoshi Shirai, AkioYokoo, Hiromi Nakaiwa, Kentaro Ogura, YoshifumiOoyama, and Yoshihiko Hayashi.
1997.
Goi-Taikei ?A Japanese Lexicon.
Iwanami Shoten, Tokyo.
5 vol-umes/CDROM.Kaname Kasahara, Hiroshi Sato, Francis Bond, TakaakiTanaka, Sanae Fujita, Tomoko Kanasugi, and ShigeakiAmano.
2004.
Construction of a Japanese semantic lex-icon: Lexeed.
SIG NLC-159, IPSJ, Tokyo.
(in Japanese).Eric Nichols, Francis Bond, and Daniel Flickinger.
2005.
Ro-bust ontology acquisition from machine-readable dictio-naries.
In Proceedings of the International Joint Confer-ence on Artificial Intelligence IJCAI-2005, pages 1111?1116.
Edinburgh.Stephan Oepen, Kristina Toutanova, Stuart Shieber,Christoper D. Manning, Dan Flickinger, and ThorstenBrant.
2002.
The LinGO redwoods treebank: Motivationand preliminary applications.
In 19th International Con-ference on Computational Linguistics: COLING-2002,pages 1253?7.
Taipei, Taiwan.Melanie Siegel.
2000.
HPSG analysis of Japanese.
In Wolf-gang Wahlster, editor, Verbmobil: Foundations of Speech-to-Speech Translation, pages 265?
280.
Springer, Berlin,Germany.Takaaki Tanaka, Francis Bond, and Sanae Fujita.
2006.
TheHinoki sensebank ?
a large-scale word sense tagged cor-pus of Japanese ?.
In Frontiers in Linguistically Anno-tated Corpora 2006.
Sydney.
(ACL Workshop).Takaaki Tanaka, Francis Bond, Stephan Oepen, and SanaeFujita.
2005.
High precision treebanking ?
blazing usefultrees using POS information.
In ACL-2005, pages 330?337.68
