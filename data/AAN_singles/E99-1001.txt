Proceedings of EACL '99Named Entity Recognition without GazetteersAndrei Mikheev, Marc Moens and Claire GroverHCRC Language Technology Group,University of Edinburgh,2 Buccleuch Place, Edinburgh EH8 9LW, UK.mikheev@harlequin,  co. uk M. Moens@ed.
ac.
Uk C. Grover@ed.
ac.
ukAbst rac tIt is often claimed that Named En-tity recognition systems need extensivegazetteers--lists of names of people, or-ganisations, locations, and other namedentities.
Indeed, the compilation of suchgazetteers i sometimes mentioned as abottleneck in the design of Named En-tity recognition systems.We report on a Named Entity recogni-tion system which combines rule-basedgrammars with statistical (maximum en-tropy) models.
We report on the sys-tem's performance with gazetteers ofdif-ferent ypes and different sizes, using testmaterial from the MUC-7 competition.We show that, for the text type and taskof this competition, it is sufficient o userelatively small gazetteers of well-knownnames, rather than large gazetteers oflow-frequency names.
We conclude withobservations about the domain indepen-dence of the competition and of our ex-periments.1 IntroductionNamed Entity recognition involves processing atext and identifying certain occurrences of wordsor expressions as belonging to particular cate-gories of Named Entities (NE).
NE recognitionsoftware serves as an important preprocessing toolfor tasks such as information extraction, informa-tion retrieval and other text processing applica-tions.What counts as a Named Entity depends onthe application that makes use of the annotations.One such application is document retrieval or au-tomated document forwarding: documents an-noted with NE information can be searched more"Now also at Harlequin Ltd. (Edinburgh office)accurately than raw text.
For example, NE an-notation allows you to search for all texts thatmention the company "Philip Morris", ignoringdocuments about a possibly unrelated person bythe same name.
Or you can have all documentsforwarded to you about a person called "Gates",without receiving documents about things calledgates.
In a document collection annotated withNamed Entity information you can more easilyfind documents about Java the programming lan-guage without getting documents about Java thecountry or Java the coffee.Most common among marked categories arenames of people, organisations and locations aswell as temporal and numeric expression.
Hereis an example of a text marked up with NamedEntity information:<ENAMEX TYPE='PERSON' >FlavelDonne</ENAMEX> is an analyst with <ENAMEXTYPE= ' ORGANIZATION ' >General Trends</ENAMEX>, which has been based in <ENAMEXTYPE='LOCATION'>Little Spring</ENAMEX> since<TIMEX TYPE='DATE' >July 1998</TIMEX>.In an article on the Named Entity recognitioncompetition (part of MUC-6) Sundheim (1995) re-marks that "common organization ames, firstnames of people and location names can be han-dled by recourse to list lookup, although there aredrawbacks" (Sundheim 1995: 16).
In fact, par-ticipants in that competition from the Univer-sity of Durham (Morgan et al, 1995) and fromSRA (Krupka, 1995) report that gazetteers didnot make that much of a difference to their sys-tem.
Nevertheless, in a recent article Cucchiarelliet al (1998) report that one of the bottlenecksin designing NE recognition systems is the lim-ited availability of large gazetteers, particularlygazetteers for different languages (Cucchiarelli etal.
1998: 291).
People also use gazetteers of verydifferent sizes.
The basic gazetteers in the Iso-quest system for MUC?7 contain 110,000 names,but Krupka and Hausman (1998) show that sys-tem performance does not degrade much when theProceedings of EACL '99gazetteers are reduced to 25,000 and 9,000 names;conversely, they also show that the addition of anextra 42 entries to the gazetteers improves perfor-mance dramatically.This raises several questions: how importantare gazetteers?
is it important hat they are big?if gazetteers are important but their size isn't,then what are the criteria for building gazetteers?One might think that Named Entity recognitioncould be done by using lists of (e.g.)
names of peo-ple, places and organisations, but that is not thecase.
To begin with, the lists would be huge: itis estimated that there are 1.5 million unique sur-names just in the U.S.
It is not feasible to list allpossible surnames in the world in a Named Entityrecognition system.
There is a similar problemwith company names.
A list of all current compa-nies worldwide would be huge, if at all available,and would immediately be out of date since newcompanies are formed all the time.
In addition,company names can occur in variations: a list ofcompany names might contain "The Royal Bankof Scotland plc", but that company might alsobe referred to as "The Royal Bank of Scotland","The Royal" or "The Royal plc".
These variationswould all have to be listed as well.Even if it was possible to list all possible or-ganisations and locations and people, there wouldstill be the problem of overlaps between the lists.Names such as Emerson or Washington could benames of people as well as places; Philip Morriscould be a person or an organisation.
In addition,such lists would also contain words like "Hope"and "Lost" (locations) and "Thinking Machines"and "Next" (companies), whereas these wordscould also occur in contexts where they don't referto named entities.Moreover, names of companies can be complexentities, consisting of several words.
Especiallywhere conjunctions are involved, this can createproblems.
In "China International Trust and In-vestment Corp decided to do something", it's notobvious whether there is a reference here to onecompany or two.
In the sentence "Mason, Dailyand Partners lost their court case" it is clear that"Mason, Daily and Partners" is the name of acompany.
In the sentence "Unfortunately, Dailyand Partners lost their court case" the name of thecompany does not include the word "unfortunate-ly", but it still includes the word "Daily", whichis just as common a word as "unfortunately".In this paper we report on a Named Entityrecognition system which was amongst the highestscoring in the recent MUC-7 Message Understand-ing Conference/Competition (MUC).
One of thefeatures of our system is that even when it is runwithout any lists of name.,; of organisations or peo-ple it still performs at a level comparable to that ofmany other MUC-systems.
We report on exper-iments which show the di\[fference in performancebetween the NE system with gazetteers of differ-ent sizes for three types of named entities: people,organisations and locations.2 The  MUC Compet i t ionThe MUC competition for which we built our sys-tem took place in March 1998.
Prior to the com-petition, participants received a detailed codingmanual which specified what should and shouldnot be marked up, and how the markup shouldproceed.
They also received a few hundred arti-cles from the New York Times Service, markedup by the organisers according to the rules of thecoding manual.For the competition itself, participants received100 articles.
They then had 5 days to perform thechosen information extraction tasks (in our case:Named Entity recognition) without human inter-vention, and markup the text with the Named En-tities found.
The resulting marked up file then hadto be returned to the organisers for scoring.Scoring of the results is done automatically bythe organisers.
The scoring software compares aparticipant's answer file against a carefully pre-pared key file; the key file is considered to be the"correctly" annotated file.
Amongst many otherthings, the scoring software calculates a system'srecall and precision scores:Recall:  Number of correct ags in the answer fileover total number of tags in the key file.Prec is ion:  Number of correct ags in the answerfile over total number of tags in the answerfile.Recall and precision are generally accepted waysof measuring system performance in this field.
Forexample, suppose you have a text which is 1000words long, and 20 of these words express a lo-cation.
Now imagine a system which assigns theLOCATION tag to every single word in the text.This system will have tagged correctly all 20 lo-cations, since it tagged everything as LOCATION;its recall score is 20/20, or 100%.
But of the 1000LOCATION tags it assigned, only those 20 were cor-rect; its precision is therefore only 20/1000, or 2%.Proceedings of EACL '99categoryorganizationpersonlocationlearned listsrecall I precision49 7526 9276 93common lists combined listsrecall lprecision recall lprecision3 51 50 7231 81 47 8574 94 86 90Figure 1: NE recognition with simple list lookup.3 F ind ing  Named Ent i t ies3.1 A s imple sys temWe decided first to test to what extent NE recog-nition can be carried out merely by recourse to listlookup.
Such a system could be domain and lan-guage independent.
It would need no grammarsor even information about tokenization but simplymark up known strings in the text.
Of course, thedevelopment and maintenance of the name listswould become more labour intensive.
(Palmer and Day, 1997) evaluated the perfor-mance of such a minimal NE recognition systemequipped with name lists derived from MUC-6training texts.
The system was tested on news-wire texts for six languages.
It achieved a recallrate of about 70% for Chinese, Japanese and Por-tuguese and about 40% for English and French.The precision of the system was not calculatedbut can be assumed to be quite high because itwould only be affected by cases where a capitalizedword occurs in more than one list (e.g.
"Columbi-a" could occur in the list of organisations a well aslocations) or where a capitalised word occurs in alist but could also be something completely differ-ent (e.g.
"Columbia" occurs in the list of locationsbut could also be the name of a space shuttle).We trained a similar minimal system using theMUC-7 training data (200 articles) and ran it onthe test data set (100 articles).
The corpus weused in our experiments were the training and testcorpora for the MUC-7 evaluation.From the training data we collected 1228 personnames, 809 names of organizations and 770 namesof locations.
The resulting name lists were theonly resource used by the minimal NE recognitionsystem.
It nevertheless achieved relatively highprecision (around 90%) and recall in the range 40-70%.
The results are summarised in Figure 1 inthe "learned lists" column.Despite its simplicity, this type of system doespresuppose the existence of training texts, andthese are not always available.
To cope withthe absence of training material we designed andtested another variation of the minimal system.Instead of collecting lists from training texts we in-stead collected lists of commonly known entities--we collected a list of 5000 locations (countries andAmerican states with their five biggest cities) fromthe CIA World Fact Book, a list of 33,000 orga-nization names (companies, banks, associations,universities, etc.)
from financial Web sites, and alist of 27,000 famous people from several websites.The results of this run can be seen in Figure 1 inthe "common lists" column.
In essence, this sys-tem's performance was comparable to that of thesystem using lists from the training set as far as lo-cation was concerned; it performed slightly worseon the person category and performed badly onorganisations.In a final experiment we combined the twogazetteers, the one induced from the training textswith the one acquired from public resources, andachieved some improvement in recall at the ex-pense of precision.
The results of this test run aregiven in the "combined lists" column in Figure 1.We can conclude that the pure list lookupapproach performs reasonably well for locations(precision of 90-94%; recall of 75-85%).
For theperson category and especially for the organiza-tion category this approach does not yield goodperformance: although the precision was not ex-tremely bad (around 75-85%), recall was too low(lower than 50%)--i.e.
every second person nameor organization failed to be assigned.For document retrieval purposes low recall isnot necessarily a major problem since it is oftensufficient o recognize just one occurrence of eachdistinctive ntity per document, and many of theunassigned person and organization ames werejust repetitions of their full variants.
But for manyother applications, and for the MUC competition,higher recall and precision are necessary.3.2 Combin ing  rules and stat ist icsThe system we fielded for MUC-7 makes exten-sive use of what McDonald (1996) calls inter-nal (phrasal) and external (contextual) evidencein named entity recognition.
The basic philos-ophy underlying our approach is as follows.
AProceedings of EACL '99Context Rule Assign ExampleXxxx+ is?
a?
JJ* PROFXxxx+ is?
a?
JJ* KELXxxx+ himselfXxxx+, DD+,shares in Xxxx+PROF of/at/with Xxxx+Xxxx+ areaPERSPERSPERSPERS0RG0RGL0CYuri Gromov, a former directorJohn White is beloved brotherWhite himselfWhite, 33,shares in Trinity Motorsdirector of Trinity MotorsBeribidjan areaFigure 2: Examples of sure-fire transduction material for NE.
Xxxx+ is a sequence of capitalized words;DD is a digit; PROF is a profession; REL is a relative; J J* is a sequence of zero or more adjectives;LOC is a known location.string of words like "Adam Kluver" has an inter-nal (phrasal) structure which suggests that thisis a person name; but  we know that it can alsobe used as a shortcut for a name of organization("Adam Kluver Ltd.") or location ("Adam Klu-ver Country Park").
Looking it up on a list willnot necessarily help: the string may not be ona list, may be on more than one list, or may beon the wrong list.
However, somewhere in thetext, there is likely to be some contextual materialwhich makes it clear what type of named entity itis.
Our strategy is to only make a decision once wehave identified this bit of contextual information.We further assume that, once we have identi-fied contextual material which makes it clear that"Adam Kluver" is (e.g.)
the name of a company,then any other mention of "Adam Kluver" in thatdocument is likely to refer to that company.
If theauthor at some point in the same text also wantsto refer to (e.g.)
a person called "Adam Kluver",s/he will provide some extra context o make thisclear, and this context will be picked up in the firststep.
The fact that at first it is only an assump-tion rather than a certainty that "Adam Kluver"is a company, is represented explicitly, and laterprocessing components try to resolve the uncer-tainty.If no suitable context is found anywhere in thetext to decide what sort of Named Entity "AdamKluver" is, the system can check other resources,e.g.
a list of known company names and applycompositional phrasal grammars for different cat-egories.
Such grammars for instance can statethat if a sequence of capitalized words ends withthe word "Ltd." it is a name of organization orif a known first name is followed by an unknowncapitalized word this is a person name.In our MUC system, we implemented this ap-proach as a staged combination of a rule-basedsystem with probabilistic partial matching.
Wedescribe ach stage in turn.3.3 Step 1.
Sure-fire RulesIn the first step, the system applies sure-fire gram-mar rules.
These rules combine internal and ex-ternal evidence, and only fire when a possible can-didate expression is surrounded by a suggestivecontext.
Sure-fire rules rely on known corporatedesignators (Ltd., Inc., etc.
), person titles (Mr.,Dr., Sen.), and definite contexts such as thosein Figure 2.
The sure-fire rules apply after POStagging and simple semantic tagging, so at thisstage words like "former" have already been iden-tified as J J  (adjective), words like "analyst" havebeen identified as PROF (professions), and wordslike "brother" as REL (relatives).At this stage our MUC system treats informa-tion from the lists as likely rather than definiteand always checks if the context is either sugges-tive or non-contradictive.
For example, a likelycompany name with a conjunction (e.g.
"ChinaInternational Trust and Investment Corp") is leftuntagged at this stage if the company is not listedin a list of known companies.
Similarly, the systempostpones the markup of unknown organizationswhose name starts with a sentence initial commonword, as in "Suspended Ceiling Contractors Ltddenied the charge".Names of possible locations found in ourgazetteer of place names are marked as LOCATIONonly if they appear with a context hat is sugges-tive of location.
"Washington", for example, canjust as easily be a surname or the name of an or-ganization.
Only in a suggestive context, like "inWashington", will it be marked up as location.3.4 Step 2.
Par t ia l  Match  1After the sure-fire symbolic transduction the sys-tem performs a probabiiistic partial match of theidentified entities.
First, the system collects allnamed entities already identified in the document.4Proceedings of EACL '99It then generates all possible partial orders ofthe composing words preserving their order, andmarks them if found elsewhere in the text.
Forinstance, if "Adam Kluver Ltd" had already beenrecognised as an organisation by the sure-fire rule,in this second step any occurrences of "KluverLtd", "Adam Ltd" and "Adam Kluver" are alsotagged as possible organizations.
This assignment,however, is not definite since some of these words(such as "Adam") could refer to a different entity.This information goes to a pre-trained maxi-mum entropy model (see Mikheev (1998) for moredetails on this aproach).
This model takes into ac-count contextual information for named entities,such as their position in the sentence, whetherthey exist in lowercase in general, whether theywere used in lowercase lsewhere in the same docu-ment, etc.
These features are passed to the modelas attributes of the partially matched words.
Ifthe model provides a positive answer for a partialmatch, the system makes a definite assignment.3.5 Step 3.
Rule RelaxationOnce this has been done, the system again appliesthe grammar ules.
But this time the rules havemuch more relaxed contextual constraints and ex-tensively use the information from already exist-ing markup and from the lexicon compiled dur-ing processing, e.g.
containing partial orders of al-ready identified named entities.At this stage the system will mark word se-quences which look like person names.
For thisit uses a grammar of names: if the first capital-ized word occurs in a list of first names and thefollowing word(s) are unknown capitalized words,then this string can be tagged as a PERSON.
Notethat it is only at this late stage that a list of namesis used.
At this point we are no longer concernedthat a person name can refer to a company.
If thename grammar had applied earlier in the process,it might erroneously have tagged "Adam Kluver"as a PERSON instead of an ORGANIZATION.
But atthis point in the chain of N~.
processing, that is nota problem anymore: "Adam Kluver" will by nowalready have been identified as an ORGANIZATIONby the sure-fire rules or during partial matching.If it hasn't, then it is likely to be the name of aperson.At this stage the system will also attempt o re-solve conjunction problems in names of organisa-tions.
For example, in "China International Trustand Investment Corp", the system checks if pos-sible parts of the conjunctions were used in thetext on their own and thus are names of differentorganizations; if not, the system has no reasonto assume that more than one company is beingtalked about.In a similar vein, the system resolves the at-tachment of sentence initial capitalized modifiers,the problem alluded to above with the "SuspendedCeiling Contractors Ltd" example: if the modifierwas seen with the organization ame elsewhere inthe text, then the system has good evidence thatthe modifier is part of the company name; if themodifier does not occur anywhere lse in the textwith the company name, it is assumed not to bepart of it.This strategy is also used for expressions like"Murdoch's News Corp'.
The genitival "Mur-doch's" could be part of the name of the organisa-tion, or could be a possessive.
Further inspectionof the text reveals that Rupert Murdoch is referredto in contexts which support a person interpreta-tion; and "News Corp" occurs on its own, withoutthe genitive.
On the basis of evidence like this, thesystem decides that the name of the organisationis "News Corp',  and that "Murdoch" should betagged separately as a person.At this stage known organizations and locationsfrom the lists available to the system are markedin the text, again without checking the context inwhich they occur.3.6 Step 4.
Par t ia l  Match  2At this point, the system has exhausted its re-sources (rules about internal and external evi-dence for named entities, as well as its gazetteers).The system then performs another partial matchto annotate names like "White" when "JamesWhite" had already been recognised as a person,and to annotate company names like "Hughes"when "Hughes Communications Ltd." had al-ready been identified as an organisation.As in Partial Match 1, this process of par-tial matching is again followed by a probabilis-tic assignment supported by the maximum en-tropy model.
For example, conjunction resolutionmakes use of the fact that in this type of text it ismore common to have conjunctions of like entities.In "he works for Xxx and Yyy", if there is evidencethat Xxx and Yyy are two entities rather than one,then it is more likely that Xxx and Yyy are twoentities of the same type, i.e.
both organisationsor are both people, rather than a mix of the two.This means that, even if only one of the entities inthe conjunction has been recognised as definitelyof a certain type, the conjunction rule will helpdecide on the type of the other entity.
One ofthe texts in the competition contained the string"UTited States and Russia".
Because of the typoin "UTited States", it wasn't found in a gazetteer.But there was internal evidence that it could beProceedings of EACL '99Stage ORGANIZAT ION PERSON LOCATIONSure-fire RulesPartial Match 1Relaxed RulesPartial Match 2Title AssignmentR: 42 P: 98R: 75 P: 98R: 83 P: 96R: 85 P: 96R: 91 P: 95R: 40 P: 99R: 80 P: 99R: 90 P: 98R: 93 P: 97R: 95 P: 97R: 36 P: 96R: 69 P: 93R: 86 P: 93R: 88 P: 93R: 95 P: 93Figure 3: Scores obtained by the system through different stages of the analysis.
R - recall P - precision.a location (the fact that it contained the word"States"); and there was external evidence that itcould be a location (the fact that it occurred ina conjunction with "Russia", a known location).These two facts in combination meant that thesystem correctly identified "UTited States" as alocation.3.7 Step 5.
Title AssignmentBecause titles of news wires are in capital etters,they provide little guidance for the recognition ofnames.
In the final stage of NE processing, enti-ties in the title are marked up, by matching orpartially matching the entities found in the text,and checking against a maximum entropy modeltrained on document titles.
For example, in "GEN-ERAL TRENDS ANALYST PREDICTS LITTLE SPRINGEXPLOSION" "GENERAL TRENDS" will be taggedas an organization because it partially matches"General Trends Inc" elsewhere in the text, and"LITTLE SPRING" will be tagged as a locationbecause elsewhere in the text there is support-ing evidence for this hypothesis.
In the headline"MURDOCH SATELLITE EXPLODES ON TAKE-OFF","Murdoch" is correctly identified as a person be-cause of mentions of Rupert Murdoch elsewherein the text.
Applying a name grammar on thiskind of headline without checking external evi-dence might result in erroneously tagging "MUR-DOCH SATELLITE" as a person (because "Mur-doch" is also a first name, and "Satellite" in thisheadline starts with a capital letter).4 MUC resu l tsIn the MUC competition, our system's combinedprecision and recall score was 93.39%.
This wasthe highest score, better in a statistically signifi-cant way than the score of the next best system.Scores varied from 93.39% to 69.67%.
Further de-tails on this can be found in (Mikheev et al, 1998).The table in Figure 3 shows the progress of theperformance of the system we fielded for the MUCcompetition through the five stages.As one would expect, the sure-fire rules givevery high precision (around 96-98%), but verylow recall--in other words, they don't find manynamed entities, but the ones they find are correct.Subsequent phases of processing add graduallymore and more named entities (recall increasesfrom around 40% to around 90%), but on occa-sion introduce errors (resulting in a slight dropin precision).
Our final score for 0RGhNISATION,PERSON and LOCATION is given in the bottom lineof Figure 3.5 The  ro le  o f  gazet teersOur system fielded for the MUC competition madeextensive use of gazetteers, containing around4,900 names of countries and other place names,some 30,000 names of companies and other organ?isations, and around 10,000 first names of peo-ple.
As explained in the previous section, theselists were used in a judicious way, taking into ac-count other internal and external evidence beforemaking a decision about a named entity.
Onlyin step 3 is information from the gazetteers usedwithout context-checking.It is not immediately obvious from Figure 3what exactly the impact is of these gazetteers.
Totry and answer this question, we ran our systemover 70 articles of the MUC competition in differ-ent modes; the remaining 30 articles were usedto compile a limited gazetteer as described belowand after that played no role in the experiments.Full  gazetteers .
We first ran the system againwith the full gazetteers, i.e.
the gazetteers usedin the official MUC system.
There are minor dif-ferences in Recall and Precision compared to theofficial MUC results, due to the fact that we wereusing a slightly different (smaller) corpus.No gazetteers.
We then ran the system with-out any gazetteers.
In this mode, the system canstill use internal evidence (e.g.
indicators suchas "Mr" for people or "Ltd" for organisations) aswell as external evidence (contexts uch as "XXX,the chairman of YYY" as evidence that XXX is aperson and YYY an organisation).The hypothesis was that names of organisationsProceedings of EACL '99Full gazetteer Ltd gazetteer Some locations No gazetteersrecall prec'n recall prec'n recall prec'n recall prec'norganisation 90 93 87 90 87 89 86 85person 96 98 92 97 90 97 90 95location 95 94 91 92 85 90 46 59Figure 4: Our MUC system with extensive gazetteers, with limited gazetteers, with short list of locations,and without gazetteers, tested on 70 articles from the MUC-7 competition.and names of people should still be handled rel-atively well by the system, since they have muchinternal and external evidence, whereas names oflocations have fewer reliable contextual clues.
Forexample, expressions uch as "XXX is based inYYY" is not sure-fire vidence that YYY is a lo-cation - it could also be an organisation.
Andsince many locations are so well-known, they re-ceive very little extra context ("in China", "inParis", vs "in the small town of Ekeren").Some locat ions.
We then ran the system withsome locational information: about 200 namesof countries and continents from www.
yahoo, corn/Reg iona l /and ,  because MUC rules say explicitlythat names of planets should be marked up aslocations, the names of the 8 planets of our so-lar system.
The hypothesis was that even withthose reasonably common location names, NamedEntity recognition would already dramatically im-prove.
This hypothesis was confirmed, as can beseen in Figure 4.Inspection of the errors confirms that the sys-tem makes most mistakes when there is no inter-nal or external evidence to decide what sort ofNamed Entity is involved.
For example, in a ref-erence to "a Hamburg hospital", "Hamburg" nolonger gets marked up as a location, because theword occurs nowhere else in the text, and thatcontext is not sufficient o assume it indicates a lo-cation (cf.
a Community Hospital, a Catholic Hos-pital, an NHS Hospital, a Trust-Controlled Hos-pital, etc).
Similarly, in a reference to "the Bonngovernment", Bonn" is no longer marked up as alocation, because of lack of supportive context (cf.the Clinton government, he Labour government,etc).
And in financial newspaper articles NYSEwill be used without any indication that this is anorganisation (the New York Stock Exchange).L imi ted  gazet teers .
The results so far sug-gest that the most useful gazetteers are those thatcontain very common ames, names which the au-thors can expect their audience already to knowabout, rather than far-fetched examples of littleknown places or organisations.This suggests that it should be possible to tunea system to the kinds of Named Entities that oc-cur in its particular genre of text.
To test thishypothesis, we wanted to know how the systemwould perform if it started with no gazetteers,started processing texts, then built up gazetteersas it goes along, and then uses these gazetteers ona new set of texts in the same domain.
We sim-ulated these conditions by taking 30 of the 100official MUC articles and extracting all the namesof people, organisations and locations and usingthese as the only gazetteers, thereby ensuring thatwe had extracted Named Entities from articles inthe same domain as the test domain.Since we wanted to test how easy it was to buildgazetteers automatically, we wanted to minimisethe amount of processing done on Named Enti-ties already found.
We decided to only used firstnames of people, and marked them all as "likely"first names: the fact that "Bill" actually occurs asa first name does not guarantee it will definitely bea first name next time you see it.
Company namesfound in the 30 articles were put in the companygazetteer, irrespective of whether they were fullcompany names (e.g.
"MCI Communications Cor-p" as well as "MCI" and "MCI Communication-s").
Names of locations found in the 30 texts weresimply added to the list of 200 location names al-ready used in the previous experiments.The hope was that, despite the little effort in-volved in building these limited gazetteers, therewould be an improved performance of the NamedEntity recognition system.Figure 4 summarises the Precision and Recallresults for each of these modes and confirms thehypotheses.6 D iscuss ionThe hypotheses were correct: without gazetteersthe system still scores in the high eightiesfor names of orga~isations and people.
Loca-tions come out badly.
But even with a verysmall number of country names performance forthose named entities also goes up into the mid-Proceedings of EACL '99eighties.
And simple techniques for extending thegazetteers on the basis of a sample of just 30 arti-cles already makes the system competitive again.These experiments suggest hat the collectionof gazetteers need not be a bottleneck: through ajudicious use of internal and external evidence rel-atively small gazetteers are sufficient to give goodPrecision and Recall.
In addition, when collectingthese gazetteers one can concentrate on the obvi-ous examples of locations and organisations, sincethese are exactly the ones that will be introducedin texts without much helpful context.However, our experiments only show the useful-ness of gazetteers on a particular type of text, viz.journalistic English with mixed case.
The rules aswell as the maximum entropy models make use ofinternal and external evidence in that type of textwhen trying to identify named entities, and it isobvious that this system cannot be applied with-out modification to a different ype of text, e.g.scientific articles.
Without further formal eval-uations with externally supplied evaluation cor-pora it is difficult to judge how general this texttype is.
It is encouraging tonote that Krupka andHausman (1998) point out that the MUC-7 articleswhich we used in our experiments have less exter-nal evidence than do Wall Street Journal articles,which suggests that on Wall Street Journal arti-cles our system might perform even better thanon MUC-7 articles.AcknowledgementsThe work reported in this paper was supportedin part by grant GR/L21952 (Text TokenisationTool) from the Engineering and Physical SciencesResearch Council, UK.
We would like to thankSteve Finch and Irina Nazarova s well as ColinMatheson and other members of the LanguageTechnology Group for help in building varioustools and other resources that were used in thedevelopment of the MUC system.ReferencesAlessandro Cucchiarelli, Danilo Luzi, and PaolaVelardi.
1998.
Automatic semantic taggingof unknown proper names.
In Proceedings ofthe 36th Annual Meeting of the Association forComputational Linguistics and Proceedings ofthe 17th International Conference on Compu-tational Linguistics, pages 286-292, Montr4al,Canada, August 10-14.George R. Krupka and Kevin Hausman.
1998.Isoquest, Inc: Description of the NetOwl(TM)extractor system as used for MUC-7.
InSeventh Message Understanding Conference(MUC-7): Proceedings of a Conference heldin Fairfax, Virginia, 29 April-1 May, 1998.http://www, muc.
sale.
com/proceedings/muc_7_toc, html.George R. Krupka.
1995.
Description of the SRAsystem as used for MUC-6.
In Sixth MessageUnderstanding Conference (MUC-6): Proceed-ings of a Conference ,~eld in Columbia, Mary-land, November 6-8, 1995, pages 221-235, LosAltos, Ca.
Morgan Kaufmann.David D. McDonald.
1996.
Internal and externalevidence in the identification and semantic at-egorization of proper names.
In Bran Boguraevand James Pustejovsky, editors, Corpus Pro-cessing for Lexical Acquisition, chapter 2, pages21-39.
The MIT Press, Cambridge, MA.Andrei Mikheev, Claire Grover, and Marc Moens.1998.
Description of the LTG system used forMUC-7.
In Seventh Message UnderstandingConference (MUC-7): Proceedings of a Con-ference held in Fairfax, Virginia, 29 April-1 May, 1998. http://w,r~.muc.saic.cota/proceedings/muc_7_toc, html.Andrei Mikheev.
1998.
Feature lattices for max-imum entropy modelling.
In Proceedings ofthe 36th Annual Meeting of the Association forComputational Linguistics and Proceedings ofthe 17th International Conference on Compu-tational Linguistics, pages 848-854, Montreal,Quebec, August 10-14.Richard Morgan, Roberto Garigliano, PaulCallaghan, Sanjay Poria, Mark Smith, Ag-nieszka Urbanowicz, Russel Collingham, MarcoCostantino, and Chris Cooper.
1995.
Descrip-tion of the LOLITA system as used in MUC-6.
In Sixth Message Understanding Conference(MUC-6): Proceedings of a Conference heldin Columbia, Maryland, November 6-8, 1995,pages 71-86, Los Altos, Ca.
Morgan Kaufmann.D.
Palmer and D. Day.
1997.
A statistical profileof the Named Entity task.
In Proceedings of theFifth Conference on Applied Natural LanguageProcessing, pages 190--193, Washington D.C.Beth Sundheim.
1995.
Overview of results ofthe MUC-6 evaluation.
In Sixth Message Un-derstanding Conference (MUC-6): Proceedingsof a Conference held in Columbia, Maryland,November 6-8, 1995, pages 13-32, Los Altos,Ca.
Morgan Kaufmann.
