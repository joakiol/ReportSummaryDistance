Large-scale Controlled Vocabulary Indexing for Named EntitiesMark WassonLEXIS-NEXIS, a Division of  Reed Elsevier plc9443 Springboro PikeMiamisburg, Ohio 45342 USAmark.wasson@lexis-nexis.comAbstractA large-scale controlled vocabulary indexingsystem is described.
The system currentlycovers almost 70,000 named entity topics,and applies to documents from thousands ofnews publications.
Topic definitions are builtthrough substantially automated knowledgeengineering.1 IntroductionThe goal of the Entity Indexing R&D program atLEXIS-NEXIS is to add controlled vocabularyindexing for named entities to searchable fields inappropriate news documents across thousands ofnews publications, where documents include bothincoming news articles as well as news articlesalready in the LEXIS-NEXIS archives.
A con-trolled vocabulary term (CVT) is a consistentlyspecified topic indicator that users can incorporateinto their queries in order to retrieve documentsabout the corresponding topic.
When a CVT isadded to an appropriate field in a document, it canbe included in a Boolean query usingfield-name(controlled vocabulary term)The initial Entity Indexing release focused oncompanies as topics.
For company indexing, theprimary CFT is a standard form of the companyname.
When we add a company CVT to a docu-ment, we often also want to add secondary CFTs tothe document that specify attributes of that com-pany.
Attributes may include the ticker symbol,SIC product codes, industry codes and companyheadquarters information.
Secondary CVTs allowcustomers to easily search on groups of companiesthat have one or more attributes in common, suchas searching for documents about banks in our setof companies that are headquartered in Utah.It is generally easy to get high recall with Booleanqueries when searching for documents aboutnamed entities.
Typically the query will only needa short form of the entity's name.
For example, thequeryAmericanwill retrieve virtually every document that men-tions American Airlines.
Of course, this query re-sults in poor precision due to the ambiguity ofAmerican.
The problem we wanted to address withcontrolled vocabulary indexing is to help onlinecustomers limit their search results to only thosedocuments that contain a major reference to thetopic, that is, to documents that are substantiallyabout he topic.Because of the volume of news data we have, it isnecessary that we fully automate the documentcategorization a d indexing step in our data prepa-ration process.
LEXIS-NEXIS adds 100,000 newsarticles daily to its collection of over 2 billiondocuments.For marketing and product positioning reasons, wewant to provide indexing for tens of thousands ofcompanies, where companies are targeted based ontheir presence on the New York, American andNASDAQ exchanges or on revenue-based criteria.Although such selection criteria help us explain theproduct feature to customers, it does not ensurethat the targeted companies actually appear all thatoften in the news.
In fact, for many targeted com-panies there is little training and test data available.Our company indexing system should address thefollowing business product requirements:?
Assign primary and corresponding secondaryCVTs to appropriate documents276?
Add CVTs only to those documents hat con-tain major eferences to the topic(s)?
Process documents fast; target 30,000 charac-ters per CPU second?
Apply tens of thousands of topics to the data ina single pass* Minimize the cost of developing and main-taining topic definitionsAlso, we target 90% recall and 95% precisionwhen using the CVTs to retrieve major referencedocuments about he corresponding companies.2 Related WorkThe Carnegie Group's Text Categorization Shell(TCS) (Hayes, 1992) uses shallow knowledge n-gineering techniques to categorize documents withrespect o large sets of predefined topics.
Eachtopic requires the development of a rule set thatincludes terms, contextual information, weighting,if-then rules and other pattern matching operations.This initially involved a manual, iterative approachto rule development, although Hayes (1992) dis-cusses their intent o explore ways to automate this.TCS accuracy is quite good.
One application de-ployed at Reuters achieved 94% recall and 84%precision.
Other reported tests achieved recall andprecision rates of 90% or better.SRA's NameTag (Krupka, 1995) uses a pattemrecognition process that combines a rule base withlexicons to identify and extract targeted tokenclasses in text, such as company, people and placenames.
It achieved 96% recall and 97% precisionwhen tested on Wall Street Journal news articles atMUC-6.
Aone et al (1997) describes a NameTag-based application for indexing English and Japa-nese language texts.
NameTag addresses a keyweakness of approaches that use predefined topicsets.
Predefined topic sets are inherently limited intheir coverage to those topics that have been ex-plicitly defined.
NameTag can recognize any num-ber of companies or entities of other domainswhose names have structure that the rules can rec-ognize.
(Not all entity domains have structure thatpattern recognition processes can exploit, e.g.,product names).A problem for pattern recognition approaches hasto do with our requirement toassign CVTs.
Patternrecognition approaches extract patterns such ascompany names as they appear in the text.
Limitedcoreference resolution may link variant forms ofnames with one another to support choosing thebest variant as a "semi-controlled" vocabularyterm, but this does not allow for the assignment oftrue primary and secondary CVTs.
SRA has at-tempted to address this through its Name Resolverfunction, which reconciles extracted names with anauthority file, but the authority file also limitsscope of coverage for CVTs to those that are de-fined and maintained in the authority file.
Thesystem must also go beyond straight recognition iorder to make a distinction between documentswith major references to the targeted entities anddocuments with lesser or passing references.SRA's NameTag addresses this with the calcula-tion of relevance scores for each set of linked vari-ant forms.Preliminary research suggests that recognizingnamed entities in data and queries may lead to asignificant improvement in retrieval quality(Thompson & Dozier, 1999).
Such an approachmay complement Entity Indexing, but it does notyet meet the controlled vocabulary indexing andaccuracy requirements for Entity Indexing.Our own Term-based Topic Identification (TFI)system (Leigh, 1991) combines knowledge ngi-neering with limited learning in support of docu-ment categorization and indexing by CVTs.
Wehave used TI'I since 1990 to support a number oftopically-related news or legal document collec-tions.
Categories are defined through topic defini-tions.
A definition includes terms to look up, termweights, term frequency thresholds, document se-lection scoring thresholds, one or more CVTs, andsource-specific document structure information.Although creating TI'I topic definitions is primar-ily an iterative, manual task, limited regression-based supervised learning tools are available tohelp identify functionally redundant terms, and tosuggest erm weights, frequency thresholds andscoring thresholds.
When these tools have beenused in building topic definitions, recall and preci-sion have topped 90% in almost all tests.2773 Approach"ITI was originally proposed as a tool to supportcontrolled vocabulary indexing, and most earlytests focused on narrowly defined legal and newstopics such as insurable interest and earthquakes.TH was also tested on a number of companies,people, organizations and places as topics.
TTI wasfirst put into production to categorize documentsby broadly-defined topics such as Europe politicaland business news and federal tax law.When we began investigating the possibility ofcreating Entity Indexing, TTI was a natural startingpoint.
It had demonstrated high accuracy and flexi-bility across a variety of topics and data types.Three problems were also apparent.
First, TTIwould not scale to support several thousand topics.Second, it took a long time to build a topic defini-tion, about one staff day each.
Third, topics weredefined on a publication-specific basis.
With then-700 publications in our news archives in combina-tion with our scale goals and the time needed tobuild topic definitions, the definition building costswere too high.
We needed to scale the technology,and we needed to substantially automate the topicdefinition-building process.For Entity Indexing, we addressed scale concernsthrough software tuning, substantially improvedmemory management, a more efficient hash func-tion in the lookup algorithm, and moving domain-specific functionality from topic definitions intothe software.
The rest of this paper focuses on thecost of building the definitions.3.1 Analyzing Companies in the NewsIn order to reduce definition building costs, weoriginally believed that we would focus on in-creasing our reliance on TTrs training tools.Training data would have to include documentsfrom a variety of publications if we were to be ableto limit definitions to one per topic regardless ofthe publications covered.Unfortunately the data did not cooperate.
Using alist of all companies on the major U.S. stock ex-changes, we randomly selected 89 companies forinvestigation.
Boolean searches were used to re-trieve documents that mentioned those companies.We found that several of these companies wererarely mentioned in the news.
One company wasnot mentioned at all in our news archives, a secondone was mentioned only once, and twelve werementioned only in passing.
Several appeared asmajor references in only a few documents.
In asecond investigation involving 40,000 companiesfrom various ources, fully half appeared in zero oronly one news document in one two-year window.We questioned whether we even wanted to createtopic definitions for such rarely occurring compa-nies.
Again, marketing and product positioningreasons dictated that we do so: it is easier to tellcustomers that the product feature covers all com-panics that meet one of a few criteria than it is togive customers a list of the individual companiescovered.
It is also reasonable to assume that publicand larger companies may appear in the news atsome future point.3.2 Company Name UsageWhile analyzing news articles about hese compa-nies, we noted how company names and their vari-ants were used.
For most companies discussed indocuments that contained major references to thecompany, some form of the full company nametypically appears in the leading text.
Shorter vari-ants typically are used for subsequent mentions aswell as in the headline.
Corresponding ticker sym-bols often appear in the headline or leading text,but only after a variant of the company name ap-pears.
In some publications, ticker symbols areused as shorter variants throughout the document.Acronyms are somewhat rare; when they are used,they behave like other shorter variants of the name.Shorter variants typically are substrmgs of the fullcompany name beginning with the leftmost wordsin the name.
For a company namedSamson Computing Supply Inc.shorter variants might includeSamson Computing SupplySamson ComputingSamsonAcronyms typically exist only for companieswhose names consist of at least two words in addi-tion to company designators such as Inc. or Corp.There is no consistency as to whether the company278designators contribute to acronyms.
Thus for theabove xampleSCSISCSare both potential acronyms.Generating such variants from a full form of acompany name is straightforward.
Similarly, rulesworking with a table of equivalences can handlecommon abbreviations, sothe variantSamson Computing Supply Incorporatedcan be generated fromSamson Computing Supply lnc.We assign weights to term variants based on termlength and the presence or absence of companydesignators.
Longer variants with designators areregarded as less ambiguous than shorter variantswithout designators, and thus have higher weights.A table of particularly problematic one-word vari-ants, such as American, National and General, isused to make a weighting distinction between theseand other one-word variants.
One-word variantsare also marked so they do not match lower casestrings during lookup.
A label is assigned to eachvariant o indicate its function and relative strengthin the document categorization process.3.3 Generating Topic DefmitionsOur company controlled vocabulary indexing pro-cess requires definition builders to provide a pri-mary CVT and zero or more secondary CVTs foreach targeted company.
The CVTs are the primaryinput to the automatic topic definition generationprocess.
If the definition builder provides the com-pany name and ticker symbol to be used as CVTsfor some company, as in#CO = Samson Computing Supply lnc.#TS = SMCS (NrSE)the following definition can be generated auto-matically:#NAME1 = Samson Computing Supply Inc.#NAME1 = Samson Computing Supply Incor-porated#NAME2 = Samson Computing Supply#NAME3 = Samson Computing#NAME4 = Samson#TS @= SMCS {upper case only}#ACRONYM @= SCSI {upper case only}#ACRONYM @= SCS {upper case only}#BLOCK @= samson {do not match lowercase}That two acronyms were generated points out apotential problem with using robust variant gen-eration as a means to automatically build topicdefinitions.
Overgeneration will produce somename variants that have nothing to do with thecompany.
However, although overgeneration fvariants routinely occurs, testing showed that suchovergeneration has little adverse ffect.This approach to automatically generating topicdefinitions is successful for most companies, in-cluding those that appear arely in our data, be-cause most company names and their variants haveconsistent structure and use patterns.
There areexceptions.
Some companies are so well-lmownthat they often appear in news articles without hecorresponding full company name.
Large compa-nies and companies with high visibility (e.g., Mi-crosoft, AT&T, NBC and Kmart) are among these.Other companies simply have unusual names.
Ourauthority file is an editable text file where defini-tion builders not only store and maintain the pri-mary and secondary CVTs for each company, butit also allows builders to specify exception infor-mation that can be used to override any or all of theresults of the automatic definition generation proc-ess.
In addition, builders can use two additionallabels to identify especially strong name variants(e.g., IBM for International Business Machines)and related terms whose presence in a documentprovide disambiguating context (e.g., Delta, air-port and flights for American Airlines, often re-ferred to only as American).
For our initial releaseof 15,000 companies, 17% of the definitions hadsome manual intervention beyond providing pri-mary and secondary CVTs.
Entity definitions builtentirely manually usually took less than thirty min-utes apiece.
Overall, on average less than five min-utes were spent per topic on definition building.This includes the time used to identify the targetedcompanies and add their primary and secondaryCVTs to the authority file.
Populating the authority279file is required regardless of the technical approachused.3.4 Applying Definitions to DocumentsAll topic definitions contain a set of labeled termsto look up.
The document categorization processcombines these into a large lookup table.
A lookupstep applies the table to a document and recordsterm frequency information.
If a match occurs inthe headline or leading text, extra "frequency" isrecorded in order to place extra emphasis onlookup matches in those parts of the document.
Ifthe same term is in several definitions (e.g., Ameri-can is a short name variant in hundreds of defini-tions), frequency information is recorded for eachdefinition.Once the end of the document is reached, fre-quency and term label-based weights are used tocalculate a score for each topic.
If the score ex-ceeds some threshold, corresponding CVTs areadded to the document.
Typically a few matches ofhigh-weight terms or a variety of lower-weightedterms are necessary to produce scores above thethreshold.
A document may be about more thanone targeted topic.3.5 System ImplementationThe tools used to build and maintain topic defini-tions were implemented in C/C++ on UNIX-basedworkstations.
The document categorization processwas implemented in PL1 and a proprietary lexicalscanner, and operates in a mainframe MVS envi-ronment.4 EvaluationIn the fmal pre-release t st, Entity Indexing wasapplied to more than 13,500 documents from 250publications.
Each document in the test was re-viewed by a data analyst.
Several of these werealso reviewed by a researcher to verify the ana-lysts' consistency with the formal evaluation crite-ria.
Recall was 92.0% and precision was 96.5%when targeting documents with major references.Additional spot tests were done after the processwas applied in production to archived ocumentsand to incoming documents.
These tests routinelyshowed recall and precision to be in the 90% to96% range on over 100,000 documents examined.Some recall errors were due to company nameswith unusual structure.
Many such problems canbe addressed through manual intervention i  thetopic definitions.
Some publication styles also ledto recall errors.
One publisher introduces a varietyof unanticipated abbreviations, uch as Intnl forInternational.
Trade publications tend to use onlyshort forms of company names even for lesserknown companies.
Those companies may be well-known only within an industry and thus to theaudience of the trade publication.
These types ofproblems can be addressed through manual inter-vention in the topic definitions, although for theabbreviations problem this is little more thanpatching.Capitalized and all upper case text in headlines andsection headings was a routine source of precisionerrors.
These often led to unwanted term matching,particularly affecting acronyms and one-wordcompany name variants.
Different companies withsimilar names also led to precision problems.
Thiswas particularly tree for subsidiaries of the samecompany whose names differed only by geo-graphically-distinct company designators.5 DiscussionEntity Indexing applies in production to tens ofthousands of documents daily from thousands ofnews publications.
Further tests have shown thatwe can reach comparably high levels of accuracyfor company topics when Entity Indexing is ap-plied to financial, patents and public recordssources.
Accuracy rates dropped about 10% intests applied to case law parties.Since the initial completion of the company in-dexing work, Entity Indexing has been extended tocover more companies and other named entities,including people, organizations and places.
Topicdefinitions have been built for almost 70,000 enti-ties, including over 30,000 companies.
Entity In-dexing applies these definitions during a singlepass of the data, processing more than 86,000characters (approximately 16news documents) perCPU second.The approach used for the other named entity typesis similar to that for companies.
However, becausemost of the place names we targeted lacked useful280internal structure, manual intervention was a partof creating all 800 definitions for places.
Accuracyrates for people, organization and geographic in-dexing are comparable to those for company in-dexing.Knowledge engineering can be a bottleneck inbuilding large-scale applications, which is whymachine learning-based approaches are often pre-ferred, but there has been little work in quantifyingthe difference between the two approaches in lin-guistic tasks (Brill & Ngai, 1999).
In our case,adopting amachine learning-based approach was aproblem not just because we lacked annotatedtraining data, but for many of the topics we wererequired to target we had little or no available dataat all.
However, because of the regularity we ob-served in company name variants and their useacross a variety of news sources, we determinedthat the knowledge ngineering task would be quiterepetitive and thus could be automated for mostcompanies.Our Entity Indexing system meets all of our busi-ness requirements for accuracy, scale and perform-ance.
We have also substantially automated thedefinition building process.
Even if a number ofdocuments were available for each targeted entity,it is unlikely that we would see the time needed topopulate the authority file and to create and anno-tate appropriate training data in a machine learn-ing-based approach fall much below the under fiveminutes we average per definition with our chosenapproach.AcknowledgmentsI would like to thank colleagues Mary Jane Battle,Ellen Hamilton, Tom Kresin, Sharon Leigh, MarkShewhart, Chris White, Christi Wilson and othersfor their roles in the research, development andongoing production support of Entity Indexing.ReferencesAone, C., Charocopos, N., & Gorlinsky, J.
(1997).An Intelligent Multilingual Information Browsingand Retrieval System Using Information Extrac-tion.
Proceedings of the Fifth Conference on Ap-plied Natural Language Processing.Brill, E., & Ngai, G. (1999).
Man vs. Machine: ACase Study in Base Noun Phrase Learning.
Pro-ceedings of the 37th Annual Meeting of the As-sociation for Computational Linguistics.Hayes, P. (1992).
Intelligent High-volume TextProcessing Using Shallow, Domain-specificTechniques.
In P. Jacobs (ed.
), Text-based Intel-ligent Systems.
Lawrence Erlbaum.Krupka, G. (1995).
SRA.
Description of the SRASystem as Used for MUC-6.
Proceedings of theSixth Message Understanding Conference(MUC-6).Leigh, S. (1991).
The Use of Natural LanguageProcessing in the Development ofTopic SpecificDatabases.
Proceedings of the Twelfth NationalOnline Meeting.Thompson, P., & Dozier, C. (1999).
Name Recog-nition and Retrieval Performance.
In T.Strzalkowski (ed.
), Natural Language Informa-tion Retrieval.
Kluwer Academic,281
