Controlling User Perceptions of LinguisticStyle: Trainable Generation ofPersonality TraitsFranc?ois Mairesse?University of CambridgeMarilyn A.
Walker?
?University of California, Santa CruzRecent work in natural language generation has begun to take linguistic variation into account,developing algorithms that are capable of modifying the system?s linguistic style based eitheron the user?s linguistic style or other factors, such as personality or politeness.
While stylisticcontrol has traditionally relied on handcrafted rules, statistical methods are likely to be needed forgeneration systems to scale to the production of the large range of variation observed in humandialogues.
Previous work on statistical natural language generation (SNLG) has shown that thegrammaticality and naturalness of generated utterances can be optimized from data; howeverthese data-driven methods have not been shown to produce stylistic variation that is perceivedby humans in the way that the system intended.
This paper describes PERSONAGE, a highlyparameterizable language generator whose parameters are based on psychological findings aboutthe linguistic reflexes of personality.
We present a novel SNLG method which uses parameterestimation models trained on personality-annotated data to predict the generation decisionsrequired to convey any combination of scalar values along the five main dimensions of personal-ity.
A human evaluation shows that parameter estimation models produce recognizable stylisticvariation along multiple dimensions, on a continuous scale, and without the computational costincurred by overgeneration techniques.1.
IntroductionAlthough language can be seen as simply a method for exchanging information, it alsohas a social function (Goffman 1970; Dunbar 1996; Labov 2006).
Speakers use linguisticcues to project social aspects of utterances, such as the speaker?s personality, emotions,and social group, and hearers use these cues to infer properties about the speaker.Although some cues appear to be produced through automatic cognitive processes(Levelt and Kelter 1982; Pickering and Garrod 2004), speakers may also overload their?
This work was done at the University of Sheffield.
The author?s present address is: EngineeringDepartment, University of Cambridge, Trumpington Street, Cambridge CB2 1PZ, United Kingdom,E-mail: f.mairesse@eng.cam.ac.uk.??
Baskin School of Engineering, University of California, 1156 High Street, SOE-3, Santa Cruz, CA 95064,E-mail: maw@soe.ucsc.edu.Submission received: 20 January 2009; revised submission received: 18 October 2010; accepted forpublication: 30 November 2010.?
2011 Association for Computational LinguisticsComputational Linguistics Volume 37, Number 3communicative intentions to try to satisfy multiple goals simultaneously (Pollack 1991;Stone and Webber 1998; Jordan 2000), such as projecting a specific image to the hearerwhile communicating information and minimizing communicative effort (Clark andBrennan 1991; Brennan and Clark 1996).
The combination of these pragmatic effectsresults in the large range of linguistic variation observed between individual speakers(Biber 1988).Much of the research on generating utterances that manifest different linguisticstyles has focused on text generation applications such as journalistic writing or in-struction manuals (Hovy 1988; Scott and de Souza 1990; Paris and Scott 1994; Green andDiMarco 1996; Bouayad-Agha, Scott, and Power 2000; Power, Scott, and Bouayad-Agha2003; Inkpen and Hirst 2004).
Recent research in language generation for dialogue appli-cations has also begun to take linguistic variation into account, developing algorithmsto modify the system?s linguistic style based on either the user?s linguistic style, or otherfactors such as the user?s emotional state, her personality, or considerations of politenessstrategies (Walker, Cahn, and Whittaker 1997; Lester, Towns, and Fitzgerald 1999; Lester,Stone, and Stelling 1999; Andre?
et al 2000; Cassell and Bickmore 2003; Piwek 2003).There is growing evidence that dialogue systems such as intelligent tutoring systemsare more effective if they can generate a range of different types of stylistic linguisticvariation (Litman and Forbes-Riley 2004, 2006; Porayska-Pomsta and Mellish 2004;Wang et al 2005; McQuiggan, Mott, and Lester 2008; Tapus and Mataric 2008).
Mostof this work uses either templates or handcrafted rules to generate utterances.
Thisguarantees high quality, natural outputs, which is useful for demonstrating the utilityof stylistic variation.Handcrafted approaches mean that utterances have to be constructed by handfor each new application, however, leading to problems of portability and scalability(Rambow, Rogati, and Walker 2001).
Statistical natural language generation (SNLG)has the potential to address such scalability issues by relying on annotated data ratherthan manual parameter tuning.
It also offers the promise of techniques for producingcontinuous stylistic variation over multiple stylistic factors by automatically learning amodel of the relation between stylistic factors and properties (parameters) of generatedutterances (Paiva and Evans 2004, 2005).
It is difficult to produce such continuousvariation over multiple factors with a rule-based or template-based approach (but seeBouayad-Agha, Scott, and Power 2000).
Moreover, to date, no one has shown thathumans correctly perceive the generated variation as the system intended, nor hasanyone shown that an SNLG approach can produce outputs that are natural enough tobe used in dialogue applications such as intelligent tutoring systems, interactive dramasystems, and conversational agents, where some types of stylistic variation have alreadybeen shown to be useful.In previous work, we argue that the Big Five model of personality provides auseful framework for modeling some types of stylistic linguistic variation.
This modelof human personality has become widely accepted in psychology over the last 50 years(Funder 1997).
Table 1 tabulates each Big Five trait along with some of the importanttrait adjectives associated with the extremes of each trait.
We believe that these traitadjectives provide an intuitive, meaningful definition of linguistic style.
In previouswork we describe a rule-based version of PERSONAGE, which here we will refer to asPERSONAGE-RB (Mairesse and Walker 2007; Mairesse 2008).
In PERSONAGE-RB, gen-eration parameters are implemented, and their values are set based on correlations be-tween linguistic cues and the Big Five traits that have been systematically documentedin the psychology literature (Scherer 1979; Furnham 1990; Pennebaker and King 1999;Mehl, Gosling, and Pennebaker 2006).
For example, parameters for the extraversion456Mairesse and Walker Trainable Generation of Personality TraitsTable 1Example adjectives associated with the extremes of all Big Five traits.High LowExtraversion warm, gregarious, assertive,sociable, excitement seeking,active, spontaneous, optimistic,talkativeshy, quiet, reserved, passive,solitary, moody, joylessEmotional stability calm, even-tempered, reliable,peaceful, confidentneurotic, anxious, depressed,self-conscious, oversensitive,vulnerableAgreeableness trustworthy, friendly, considerate,generous, helpful, altruisticunfriendly, selfish, suspicious,uncooperative, maliciousConscientiousness competent, disciplined, dutiful,achievement striving,deliberate, careful, orderlydisorganized, impulsive,unreliable, careless, forgetfulOpenness to experience creative, intellectual, imaginative,curious, cultured, complexnarrow-minded, conservative,ignorant, simpletrait include verbosity, sentence length, and the production of positive content.
Weshowed experimentally that humans perceive utterances generated by PERSONAGE-RBas conveying the extremes of all Big Five traits (e.g., neuroticism (low) vs. emotionallystable (high), see Table 1).
Our evaluation uses a validated perceptual questionnairefrom the personality psychology literature (Gosling, Rentfrow, and Swann 2003).PERSONAGE-RB only generates 10 discrete personalities emphasizing either thehigh or the low end of one trait; however, psychologists measure personality traitson continuous scales (Norman 1963; Goldberg 1990; Marcus et al 2006), and humanlanguage simultaneously manifests multiple personality traits.
Some computationalapplications may require more than a small set of personality types, which suggeststhat systems adapting their linguistic style to the user would benefit from fine-grainedpersonality models.
We believe that the only way to robustly and efficiently learn suchfine-grained variation is to model personality as a continuous variable, rather than usingarbitrary discrete personality classes.
Personality generation models should thus learnto map continuous target personality scores to discrete utterances.
In order to achievethis, the handcrafted rule-based approach would require the manual examination ofpsycholinguistic findings, followed by testing in the application domain, to determinethe appropriate range for each parameter value.
Extending this approach to continuousvariation that can project multiple traits simultaneously does not appear to be tractable.The objective of this article is to present and evaluate a language generator thatis trained with a novel method, and which learns to generate stylistic variation ex-pressing multiple continuous stylistic dimensions (in this case multiple personality traits).Before presenting our method, let us review existing paradigms for statistical languagegeneration.1.1 Previous Statistical Language Generation MethodsPrevious work on SNLG has focused on three main approaches: (a) learning statisti-cal language models (SLMs) from corpora in order to rerank a set of pre-generated457Computational Linguistics Volume 37, Number 3utterances; (b) learning utterance reranking models from user feedback rather thancorpora; and (c) learning generation parameters directly from data.The first approach has used SLMs to rerank a large set of candidate utterances, andfocused on grammaticality and naturalness (Bangalore and Rambow 2000; Langkilde-Geary 2002; Chambers and Allen 2004; Nakatsu and White 2006).
The seminal work ofLangkilde and Knight (1998) in this area showed that high quality paraphrases can begenerated from an underspecified representation of meaning, by first applying a veryunderconstrained, rule-based overgeneration phase, whose outputs are then ranked byan SLM scoring phase.
The SLM scoring gives a low score (rank) to any ungrammaticaloutput produced by the rule-based generator.
We will refer to this as the overgenerateand scoring (OS) approach.In a novel twist, Isard, Brockmann, and Oberlander (2006) applied this method tothe generation of dialogues in which conversational agents with different personalitiesdiscuss movies.
The SLM ranking model blends SLMs from blogs annotated with BigFive personality traits with SLMs from Switchboard, a much larger conversationaldialogue corpus.
Their CRAG-2 generator discretizes the blog personality ratings intothree groups (low, medium, and high), and models personality with three distinct SLMmodels for each trait.
Each model estimates the likelihood of the utterance given thepersonality type.
A cache model based on recently used linguistic forms can also becombined, in order to model recency effects and alignment (Pickering and Garrod2004).
This approach was integrated into a demonstrator, but it does not generatecontinuous variation (discretization of personality ratings), and to our knowledge ithas never been evaluated to test whether the variation produced is perceivable byusers.A second approach to SNLG is a variant of the OS technique that trains the scor-ing phase to replicate human judgments rather than relying on the probabilities orfrequencies of a SLM.
This approach typically uses higher-level syntactic, semantic,and discourse features rather than only n-grams, with typical results demonstratingthat the performance of the scoring models approaches the gold-standard human rank-ing with a relatively small training set (Rambow, Rogati, and Walker 2001; Stent andGuo 2005; Nakatsu and White 2006).
An advantage of this approach is that humanjudgments can be based on any aspect of the output, such as stylistic differences inpoliteness or personality.
Walker et al (2007) showed that this technique can be usedto model individual preferences in rhetorical structure, syntactic form, and contentordering.In previous work, we also applied this method to scoring randomly producedoutputs of PERSONAGE (Mairesse 2008).
The resulting statistical generator is referredto as PERSONAGE-OS.
We randomly varied PERSONAGE?s non-deterministic decisionspoints to generate a large number of paraphrases.
We then computed post hoc fea-tures consisting of the actual generation decisions, surface word n-grams, and content-analysis features from the Linguistic Inquiry and Word Count (LIWC) tool (Pennebaker,Francis, and Booth 2001) and the MRC psycholinguistic database (Coltheart 1981).
Ex-ample content-analysis features include the ratio of words related to positive emotions(e.g., good), social interactions (e.g., pal), or the average frequency of use of each word.Scoring models trained on personality ratings of random utterances (in-domain data)outperformed the mean value baseline for all Big Five traits, with the best results foragreeableness, extraversion, and emotional stability.
The models for those traits predictthe ratings of unseen utterances with correlations of r = .52, r = .37, and r = .29, respec-tively.
We also trained models on out-of-domain data, that is, 96 personality-annotatedconversation extracts (without any generation decision features).
Results show that the458Mairesse and Walker Trainable Generation of Personality Traitsout-of-domain models perform worse for all traits, only outperforming the baselinefor agreeableness and conscientiousness.
We also explored several hybrid methods fortraining that mix and blend data from different sources.
Inspired by recent work ondomain adaptation, we tested whether the performance of the out-of-domain modelscan be improved when training includes a small amount of data from the target do-main, by applying the method of Daume?
(2007).
Whereas adding out-of-domain dataimproved performance for some traits, we find that adding a single domain feature per-forms as well as Daume?s method.
The results showed that mixing randomly generatedin-domain utterances with rule-based in-domain utterances improves performance; therule-based utterances provide a way to incorporate knowledge from the personalitypsychology literature into an SNLG approach.
Thus, personality scoring models canbe effective, although the computational cost of the OS approach remains a majordrawback.The third SNLG approach estimates the generation parameters directly from data,without any overgeneration phase.
If the language generator is constrained to be a gen-erative SLM, the parameters can then be learned through standard maximum-likelihoodestimation.
Whereas n-gram SLMs can only model local linguistic phenomena, Belzshowed that a context-free grammar (PCFG) can successfully model individual differ-ences in the production of weather reports (Belz 2005, 2008).
This method provides aprincipled way to produce utterances matching the linguistic style of a specific corpus(e.g., of an individual author) without any overgeneration phase.
However, standardPCFG generation methods require a treebank-annotated corpus, and they cannot modelcontext-dependent generation decisions, such as the control of sentence length or thegeneration of referring expressions.Paiva and Evans (2005) adopt a more general framework by learning a regressionmodel mapping generation decisions to stylistic dimensions extracted from a corpus,independently of the language generation mechanism.
Factors are identified by apply-ing factor analysis to a corpus exhibiting stylistic variation, and expressed as a linearcombination of linguistic features (Biber 1988).
Textual outputs are generated with arule-based generator in the target domain that is allowed to randomly vary the gener-ation parameters, while logging the parameter settings corresponding to each output.Then the same factors found in the original corpus are measured in the random outputs,and linear regression is applied to learn which generation parameters predict the factormeasurements.
The generation parameters can then be manipulated to hit multiplestylistic targets on a continuous scale (because factors are measured continuously) bysearching for the parameter setting yielding the target stylistic scores according to thelinear models.
The generator of Paiva and Evans, trained in this way, can reproduce in-tended factor levels across several factors, such as sentence length and type of referringexpression, thus modeling the stylistic variation as measured in the original corpus.Again, it has not been shown that humans perceive the stylistic differences that thisapproach produces.1.2 Parameter Estimation ModelsIn the previous sections, we referred to two existing methods for controlling the param-eters of PERSONAGE to produce stylistic variation: PERSONAGE-RB uses handcraftedgeneration parameter values for every target style of interest, and PERSONAGE-OSuses a statistical rescoring model to rerank a set of randomly generated utterances.The following sections develop and evaluate PERSONAGE-PE, a trainable generator459Computational Linguistics Volume 37, Number 3which uses a direct generation method inspired by Paiva and Evans?
approach (2005), toproduce the stylistic variation found in personality traits, without any overgenerationphase.
Whereas Paiva and Evans learn models predicting the target stylistic scores fromthe generation parameters, we train parameter estimation models (PE) to estimate theoptimal generation parameters given target personality scores, which are then used bythe base generator to produce the output utterance.
As parameter estimation modelslearn the reverse relationship to Paiva and Evans?
regression models, there is no needto search for the optimal generation parameter values at generation time.
We evaluatethe PE approach using the PERSONAGE base generator, whose parameters, architecture,and capabilities are described in Section 2.
Our experimental method is described inSection 3, together with an analysis of the data required to train our models.
Section 4analyzes some of the learned models, and evaluates the quality of the generated outputsusing human judges, to compare our approach with the handcrafted PERSONAGE-RBgenerator of our previous work.
Finally, Section 5 discusses the implications of ourresults and suggests many areas of future work.This article makes several contributions.
First, we present a novel method for train-ing an SNLG system that can produce multiple stylistic dimensions simultaneously,over continuous dimensions, without overgeneration or search.
In order to evaluate ourapproach, we present the first empirical results showing that humans correctly perceivethe stylistic variations (of any kind based on any utterance dimensions) that a statisticallanguage generator intended to produce.
Our evaluation of PERSONAGE-RB is the onlyother result that we know of for non-statistical generators (Mairesse and Walker 2007).Our experiments show that PERSONAGE-PE produces utterances perceived by humansas portraying different personalities, while maintaining a reasonable naturalness level(4.0 on a scale of 1 to 7).
We do not know of any other human evaluation of anSNLG system that produces stylistic variation.
Additionally, we test a wide range ofmachine learning algorithms to determine the best model for each generation decisionin Section 4.1.
We are not aware of any other work on SNLG to test such a wide rangeof algorithms.2.
The PERSONAGE Base GeneratorThe architecture of the PERSONAGE base generator is shown in Figure 1; it is discussedin detail in (Mairesse 2008) and (Mairesse and Walker 2010), and is only briefly summa-rized here.The PERSONAGE architecture (Figure 1) builds on a standard natural languagegeneration (NLG) pipeline architecture as described in Reiter and Dale (2000), Kittredge,Korelsky, and Rambow (1991), and Walker and Rambow (2002).
We assume that theinputs to the generator are (1) a high-level communicative goal; (2) a content pool thatcan be used to achieve that goal, and (3) a set of generation parameter values.
In adialogue system, the communicative goal is provided by the dialogue manager.
Twotypes of communicative goals are currently supported by PERSONAGE: recommendationand comparison of restaurants.
PERSONAGE?s content pool is based on a database ofrestaurants in New York City, with associated scalar values representing evaluativeratings for six attributes: food quality, service, cuisine, location, price, and atmosphere.The first component of the architecture shown in Figure 1 is the content planner,which specifies the structure of the information to be conveyed.
The resulting contentplan tree is then processed by the sentence planner, which selects syntactic templatesfor expressing individual propositions, and aggregates them to produce the utterance?s460Mairesse and Walker Trainable Generation of Personality TraitsFigure 1The architecture of the PERSONAGE base generator.full syntactic structure.
The pragmatic marker insertion component then modifies thesyntactic structure locally to produce various pragmatic effects, depending on the mark-ers?
insertion constraints.
The lexical choice component selects the most appropriatelexeme for each content word, given the lexical selection parameters.
Finally, the Real-Pro surface realizer (Lavoie and Rambow 1997) converts the final syntactic structureinto a string by applying surface grammatical rules, such as morphological inflectionand function word insertion.
When integrated into a dialogue system, the output of therealizer is annotated for prosodic information by the prosody assigner before being sentto the text-to-speech engine to be converted into an acoustic signal.
PERSONAGE doesnot currently express personality through prosody, although there are studies that couldbe used to develop such parameters (Scherer 1979; Furnham 1990).Figure 1 also indicates the modules in which PERSONAGE introduces parameters toproduce and control personality-based linguistic variation.
The generation parametersare shown in Table 2 and organized into blocks that correspond to the modules of thearchitecture in Figure 1; compare Figure 1 to Table 2.
As mentioned previously, allof PERSONAGE?s parameters are motivated by findings in the personality psychologyliterature.
The mapping from a finding to parameters represents a set of hypothesesabout how the finding can be implemented, however, as discussed in more detail inMairesse and Walker (2007) and Mairesse (2008).Table 2 includes a description for each parameter that explains what the parameterdoes and often includes an example.
For example, there are 12 content planning param-eters shown in the first block of Table 2; these control aspects of utterances such as theirverbosity, rhetorical structure, content selection parameters such as positive content,and the level of redundancy and restatement (Walker 1993).
Table 2 also includes 13pragmatic marker parameters, which we believe to be completely novel.
These includethe introduction of HEDGES and TAG QUESTIONS.
We are not aware of any other gen-erators that produce the range of pragmatic variation illustrated here.
Note also that461Computational Linguistics Volume 37, Number 3Table 2PERSONAGE?s generation parameters.
The Type column indicates whether the stylistic effect ismodeled as a continuous (C) or binary (B) parameter (i.e., resulting in continuous or binaryparameter estimation models).
Aggregation operation parameters are selection probabilities (C).Parameter Type DescriptionContent planning:VERBOSITY C Control the number of propositions in the utteranceRESTATEMENTS C Paraphrase an existing proposition, e.g., ?Chanpen Thai hasgreat service, it has fantastic waiters?REPETITIONS C Repeat an existing propositionCONTENT POLARITY C Control the polarity of the propositions expressed,i.e., referring to negative or positive attributesREPETITIONS POLARITY C Control the polarity of the restated propositionsCONCESSIONS Emphasize one attribute over another, e.g., ?even ifChanpen Thai has great food, it has bad service?CONCESSIONS POLARITY C Determine whether positive or negative attributes areemphasizedPOLARIZATION C Control whether the expressed polarity is neutralor extremePOSITIVE CONTENT FIRST C Determine whether positive propositions?includingthe claim?are uttered firstREQUEST CONFIRMATION B Begin the utterance with a confirmation of the restaurant?sname, e.g., ?did you say Chanpen Thai?
?INITIAL REJECTION B Begin the utterance with a mild rejection, e.g., ?I?m not sure?COMPETENCE MITIGATION B Express the speaker?s negative appraisal of the hearer?srequest, e.g., ?everybody knows that .
.
.
?Syntactic template selection:SELF-REFERENCES C Control the number of first person pronounsSYNTACTIC COMPLEXITY C Control the syntactic complexity (syntactic embedding)TEMPLATE POLARITY C Control the connotation of the claim, i.e., whether positiveor negative affect is expressedAggregation operations:PERIOD C Leave two propositions in their own sentences, e.g.,?Chanpen Thai has great service.
It has nice decor.
?RELATIVE CLAUSE C Aggregate propositions with a relative clause, e.g.,?Chanpen Thai, which has great service, has nice decor?WITH CUE WORD C Aggregate propositions using with, e.g., ?Chanpen Thai hasgreat service, with nice decor?CONJUNCTION C Join two propositions using a conjunction, or a commaif more than two propositionsMERGE C Merge the subject and verb of two propositions, e.g.,?Chanpen Thai has great service and nice decor?ALSO CUE WORD C Join two propositions using also, e.g.,?Chanpen Thai hasgreat service, also it has nice decor?CONTRAST - CUE WORD C Contrast two propositions using while, but, however, on theother hand, e.g., ?While Chanpen Thai has great service,it has bad decor?, ?Chanpen Thai has great service, but ithas bad decor?JUSTIFY - CUE WORD C Justify a proposition using because, since, so, e.g.,?Chanpen Thai is the best, because it has great service?CONCEDE - CUE WORD C Concede a proposition using although, even if, but/though,e.g., ?Although Chanpen Thai has great service, it has baddecor?, ?Chanpen Thai has great service, but it has baddecor though?462Mairesse and Walker Trainable Generation of Personality TraitsTable 2(continued)Parameter Type DescriptionMERGE WITH COMMA C Restate a proposition by repeating only the object, e.g.,?Chanpen Thai has great service, nice waiters?OBJECT ELLIPSIS C Restate a proposition after replacing its object by anellipsis, e.g., ?Chanpen Thai has .
.
.
, it has great service?Pragmatic markers:SUBJECT IMPLICITNESS C Make the restaurant implicit by moving the attribute tothe subject, e.g., ?the service is great?STUTTERING C Duplicate the first letters of a restaurant?s name, e.g.,?Ch-ch-anpen Thai is the best?PRONOMINALIZATION C Replace occurrences of the restaurant?s name by pronounsNEGATION B Negate a verb by replacing its modifier by its antonym,e.g., ?Chanpen Thai doesn?t have bad service?SOFTENER HEDGES B Insert syntactic elements (sort of, kind of, somewhat, quite,around, rather, I think that, it seems that, it seems to me that)to mitigate the strength of a proposition, e.g., ?ChanpenThai has kind of great service?
or ?It seems to me thatChanpen Thai has rather great service?EMPHASIZER HEDGES B Insert syntactic elements (really, basically, actually, just) tostrengthen a proposition, e.g., ?Chanpen Thai has really greatservice?
or ?Basically, Chanpen Thai just has great service?ACKNOWLEDGMENTS B Insert an initial back-channel (yeah, right, ok, I see, oh, well),e.g., ?Well, Chanpen Thai has great service?FILLED PAUSES B Insert syntactic elements expressing hesitancy (like, I mean,err, mmhm, you know), e.g., ?I mean, Chanpen Thai hasgreat service, you know?
or ?Err... Chanpen Thai has, like,great service?EXCLAMATION B Insert an exclamation mark, e.g., ?Chanpen Thai has greatservice!
?EXPLETIVES B Insert a swear word, e.g., ?the service is damn great?NEAR EXPLETIVES B Insert a near-swear word, e.g., ?the service is darn great?TAG QUESTION B Insert a tag question, e.g., ?the service is great, isn?t it?
?IN-GROUP MARKER B Refer to the hearer as a member of the same social group,e.g., pal, mate, and buddyLexical choice:LEXICON FREQUENCY C Control the average frequency of use of each content word,according to BNC frequency countsLEXICON WORD LENGTH C Control the average number of letters of each content wordVERB STRENGTH C Control the strength of the verbs, e.g., ?I would suggest?vs.
?I would recommend?Figure 1 indicates that the lexical choice parameters in Table 2 make use of multiple on-line lexical resources such as WordNet and VERBOCEAN to support lexical variation.The LEXICAL FREQUENCY parameter is calculated with respect to a corpus.Furthermore, whereas some parameters primarily have a linear effect on an utter-ance (e.g., verbosity), other parameters are highly non-linear (e.g., the effect of insertingtwo expletives rather than one is not as strong as the effect of inserting one expletiverather than none).
Parameters are therefore modeled as having either continuous (C) orbinary (B) values, as illustrated in column Type of Table 2.
The models for continuous463Computational Linguistics Volume 37, Number 3and binary parameters are trained using different algorithms.
Section 3 will provideexamples of learned models of both types.In addition, because generation decisions can be non-deterministic, some contin-uous parameter values are generation decision probabilities; for example, the inputto aggregation parameters such as CONJUNCTION is the probability that the aggrega-tion operation is selected to combine any pair of propositions in the utterance (e.g.,CONJUNCTION aggregates two propositions with the conjunction and).
If the propo-sitions cannot be aggregated because of syntactic constraints, another aggregationoperation is sampled until the aggregation is successful.
Complete details on the im-plementation of individual parameters can be found in Mairesse (2008) and Mairesseand Walker (2010).To make PERSONAGE as domain-independent as possible, the input parametervalues are normalized between 0 and 1 for continuous parameters, and to 0 or 1 forbinary parameters.
For example, a VERBOSITY parameter of 1 maximizes the utterance?sverbosity given the input, regardless of the actual number of propositions expressed.In order to ensure naturalness over the full parameter range, the maximum value ofsome continuous parameters is associated with an input-independent threshold (e.g.,there cannot be more than two repeated propositions per utterance).
Although thegoal of the base generator is to satisfy its input parameters, it cannot guarantee thatall input parameter values will be reflected in the utterance due to constraints onthe input content plan and other parameters.
A consequence is that non-deterministicdecision points are introduced to satisfy these naturalness constraints (e.g., if too manypragmatic marker parameters are enabled, only a random subset will appear in theutterance).
Therefore, the only assumption we make regarding the impact of parametervalues on the generation process is that they affect the likelihood of observing theirintended effect over a large set of utterances.3.
Generation of Personality through Data-Driven Parameter EstimationWhereas PERSONAGE-RB uses handcrafted parameter settings to convey different per-sonality traits, PERSONAGE-PE relies on parameter estimation models to estimate theparameter values in Table 2 from target personality scores.
At training time, our methodrequires the following steps:1.
Use a base generator to produce multiple utterances by randomly varyingits parameters (see Section 3.1).2.
Ask human subjects to evaluate (rate) the personality/style of eachutterance.3.
Train statistical models predicting the parameter values from thepersonality ratings (see Section 4.1).4.
Select the best model for each parameter via cross-validation (seeSection 4.2).At generation time, the models are used to predict the optimal set of generation param-eters given a set of target personality scores, and the base generator is called oncewith the predicted parameter values.
The architecture for the PE method is shown inFigure 2.464Mairesse and Walker Trainable Generation of Personality TraitsFigure 2PERSONAGE-PE?s parameter estimation framework.In contrast with the overgenerate?score (OS) method discussed in Section 1.1,parameter estimation models predict generation decisions directly from input person-ality scores, in the spirit of the approach of Paiva and Evans (2005).
However, whereasPaiva and Evans?
approach searches for the generation decisions that will yield theoptimal target scores according to their model, our PE method does not involve anysearch, as generation decisions are assumed to be conditionally independent given thetarget personality, and treated as dependent variables in individual models.This section further details the steps required for training parameter estimationmodels.
We first explain in Section 3.1 how we collect the judge?s ratings for our trainingset.
Then Section 3.2 analyzes the coverage and naturalness of the collected data.
Finally,Section 3.3 describes how the models are trained.3.1 Collecting Judgments of Random SampleIn order to train the parameter estimation models, the first step is to collect a dataset mapping generation decisions to personality ratings.
This involves the followingsubsteps:1.
Generate a sample of random utterances that produces examples coveringthe full range of all of the 67 PERSONAGE parameters as shown in Table 2.2.
Log the generation decisions that were made to produce each utterance.3.
Judges rate the random sample with a standard personality test shown inFigure 3, based on Gosling, Rentfrow, and Swann (2003).
This results ineach utterance in the sample being labelled with five scalar values, one foreach of the Big Five traits.To be the basis for training a high performing statistical generator, the random samplemust satisfy two properties.
First, it must cover the full range of scalar values foreach Big Five trait or there will not be enough training data to learn how to produceutterances manifesting those values.
Second, the randomly produced utterances mustbe natural enough to produce stable personality judgments.
The only way to verifythat the random sample satisfies these properties is by first generating the randomsample and then analyzing the judge?s ratings.
We generated 160 random utterances465Computational Linguistics Volume 37, Number 3Figure 3The Ten Item Personality Inventory used in our experiments to calculate values for the Big Fivetraits, as modified for our experimental setting.to constitute our random sample.
Table 3 shows examples of random utterances andthe scalar ratings for each trait that result from the judgment collection process.A major advantage of the Big Five framework is that it offers standard validatedquestionnaires (John, Donahue, and Kentle 1991; Costa and McCrae 1992; Gosling,Rentfrow, and Swann 2003).
Figure 3 shows the Ten Item Personality Inventory (TIPI)that we used to collect the personality judgments (Gosling, Rentfrow, and Swann 2003),adapted to our domain and task.
The TIPI produces a scalar rating for each of theBig Five traits ranging from 1 (e.g., highly neurotic) to 7 (e.g., very stable), and it wasshown to correlate well with longer questionnaires such as the Big Five Inventory, withconvergent correlations of .87, .70, .75, .81, and .65 for extraversion, emotional stability,agreeableness, conscientiousness, and openness to experience, respectively (Gosling,Rentfrow, and Swann 2003).
Although the TIPI has mostly been used as a self-reportmeasure of personality, it has also been used to assess personality perceptions ofobservers, for example, based on short social interactions (Srivastava, Guglielmo, andBeer 2010) or social networking Web sites (Gosling, Gaddis, and Vazire 2007).
Thejudges in our experiment were researchers and postgraduate students in psychology,history, and anthropology who were familiar with the Big Five trait theory, but not withnatural language generation.
They were all native speakers of English.
As illustrated inFigure 3, the judges were asked to rate each utterance in the random sample using theTIPI scale.
They were instructed to rate the utterance as if it had been uttered by a friendresponding in a dialogue to a request to recommend restaurants.
Each judge rated thesame sets of utterances corresponding to 20 communicative goals, 16 utterances pergoal, one set at a time.
The order of the sets and the order of the utterances within eachset were both randomized.
The judges were asked to read all the utterances in a set466Mairesse and Walker Trainable Generation of Personality TraitsTable 3Example outputs of PERSONAGE with random parameter settings (random utterances), andscalar personality trait values after collection of TIPI judgments.
Extra = extraversion, ems =emotional stability, agree = agreeableness, consc = conscientiousness, and open = opennessto experience.# Communicative goal PERSONAGE?s output Ratings1.a compare (Kin Khao,Tossed)Kin Khao and Tossed are bloody outstanding.Kin Khao just has rude staff.
Tossedfeatures sort of unmannered waiters,even if the food is somewhat quiteadequate.extra=4.75ems=6.00agree=3.75consc=4.00open=3.251.b compare (Kin Khao,Tossed)Err...
I am not really sure.
Tossed offerskind of decent food.
Mmhm... however,Kin Khao, which has quite ad-ad-adequatefood, is a thai place.
You would probablyenjoy these restaurants.extra=4.00ems=4.00agree=5.50consc=4.75open=5.002.a compare (Le Rivage,Pintaile?s Pizza)Ok, right, Pintaile?s Pizza is rather cheap.The atmosphere is bad, isn?t it?
I guessLe Rivage is in Manhattan, also its priceis... it?s expensive, you know.extra=1.75ems=3.50agree=5.25consc=4.00open=2.752.b compare (Le Rivage,Pintaile?s Pizza)Yeah, I thought everybody knew thatLe Rivage?s price is around 40 dollars,it?s located in Manhattan with kind ofpoor atmosphere, Pintaile?s Pizza islocated in the Upper East Side and itsprice is 14 dollars.
They?re...
I wouldadvise these restaurants, alright?extra=5.00ems=4.25agree=4.25consc=5.75open=3.003.a recommend(Chimi-churri Grill)Obviously, oh God Chimichurri Grill haslike, quite adequate food, so it?s the onlyre-restaurant I would consider, and it?s alatin american place and located in MidtownWest with ra-ra-rather friendly staff.extra=4.50ems=3.75agree=5.25consc=4.00open=4.503.b recommend(Chimi-churri Grill)I see, oh Chimichurri Grill is a latin americanplace with sort of poor atmosphere.
Althoughit doesn?t have rather nasty food, its price is41 dollars.
I suspect it?s kind of alright.extra=2.50ems=4.50agree=3.50consc=4.75open=4.254.a recommend(Cent anni)Did you say Ce-Cent?anni?
I see, I mean,I would consider it because it has friendlystaff and tasty food, you know buddy.extra=4.75ems=5.00agree=6.25consc=6.25open=5.254.b recommend(Cent anni)I am not sure.
Cent?anni is... it?s located inManhattan, also the atmosphere is somewhatbloody poor, but it features tasty foodthough.
Actually, this eating house, whichprovides quite acceptable service, is an italianrestaurant.
It?s sort of the best eating placeof its kind.extra=4.25ems=4.50agree=4.25consc=4.25open=5.75467Computational Linguistics Volume 37, Number 3before rating them.
Eight utterances out of sixteen were randomly generated foreach communicative goal.
The remaining utterances were generated using thehandcrafted parameter settings of PERSONAGE-RB for each end of each Big Five trait(Mairesse and Walker 2007).
The rule-based utterances are used as a comparison point,not for training the models.
The same methodology was used to collect additionalextraversion ratings for another set of 160 random and 80 rule-based utterances in aseparate experiment, resulting in 320 random and 240 rule-based utterances for thattrait, and 160 random utterances and 40 rule-based utterances for each of the other fourtraits.
Examples of the resulting scalar ratings are shown in Table 3.
The judges alsoevaluated the naturalness of each utterance on the same scale.3.2 Generation Range and NaturalnessAnalysis of the collected ratings of the random utterances shows that 67.8% of theutterances were rated as natural (rating above or equal to 4), with an average nat-uralness rating of 4.38 out of 7.
Figure 4 shows the distributions of openness to ex-perience and naturalness ratings.
Figure 4a illustrates that most randomly generatedutterances are not perceived as projecting an extreme personality.
Table 4 examineswhether randomly generated utterances can hit the extreme ends of each trait scale bytabulating the most extreme ratings obtained from the 8 random utterances generatedfor each communicative goal with the ratings of the rule-based utterance generatedfrom the same goal.
This comparison provides useful information regarding (a) thepotential of data-driven models to outperform handcrafted methods, and (b) whetherour training corpus is large enough to capture the range of behavior we intend toconvey.
Paired t-tests over 20 communicative goals show that on average the mostextreme random utterance is significantly more extreme for the positive end of the ex-traversion, emotional stability, and agreeableness scales, and significantly more extremefor both ends of the conscientiousness and openness to experience scales (p < .05, two-Figure 4The distribution of training data samples for openness personality judgments and naturalness.468Mairesse and Walker Trainable Generation of Personality TraitsTable 4For each communicative goal, the most extreme rating of the random utterances (Random)is compared with the ratings obtained for rule-based utterances (Rule-based).
Ratings areaveraged over 20 content plans and over all judges.
?
= the ratings of the random utterancesare significantly more extreme (?
= more moderate) than the ratings of rule-based utterances(p < .05, two-tailed).Method Rule-based RandomTrait Low High Lowest HighestExtraversion 2.96 5.98 3.60 ?
6.23 ?Emotional stability 3.29 5.96 3.05 6.25 ?Agreeableness 3.41 5.66 3.26 6.01 ?Conscientiousness 3.71 5.53 3.11 ?
5.93 ?Openness to experience 2.89 4.21 2.28 ?
5.48 ?Naturalness 4.59 4.38tailed).
However, random utterances are not perceived as introverted as those generatedusing the introvert parameter settings (see Rule-based/Low column for extraversion).Compare the distributions of judgments for the rule-based extraversion utterances withthe judgments on the random sample shown in Figure 5.
Nevertheless, these resultssuggest that randomizing PERSONAGE?s parameters produces a wide range of variationwith an utterance sample of less than 10 utterances, for any communicative goal.The bottom row of Table 4 also compares the naturalness of the random utter-ances with the naturalness of the rule-based utterances produced by PERSONAGE-RB(Mairesse and Walker 2007; Mairesse 2008).
Results suggest that the random utterancesFigure 5The distribution of extraversion judgments for utterances generated using random parameters(Random) and handcrafted parameters derived from psychology studies (Rule-based).469Computational Linguistics Volume 37, Number 3Table 5Average inter-rater correlation for the rule-based and random utterances.
All correlations aresignificant at the p < .05 level (two-tailed).Parameter set Rule-based RandomExtraversion .73 .30Emotional stability .67 .33Agreeableness .54 .40Conscientiousness .42 .26Openness to experience .44 .28are less natural than the rule-based utterances, and this difference is close to significance(p = .075, two-tailed t-test).It is also important to quantify the quality of the annotations by evaluating the inter-rater agreement between the judges.
Table 5 shows that the judges agree significantlyon the ratings of random utterances for all Big Five traits (p < .05, two-tailed), withcorrelations ranging from .26 (conscientiousness) to .40 (agreeableness), which are highcorrelations for human perceptual judgments.
However the agreement is lower thanon the rule-based utterances.
A possible explanation of both the naturalness differencesand rater agreement is that the random generation decisions sometimes produce ut-terances with inconsistent personality cues, which can be interpreted in different waysby the judges.
For example, the utterance ?Err...
I am sure you would like Chanpen Thai!
?expresses markers of both introversion (filled pause) and extraversion (exclamationmark).3.3 Training Parameter Estimation ModelsParameter estimation requires a series of pre-processing steps, in order to ensure thatthe models?
output is re-usable by the PERSONAGE base generator.
The initial data setincludes the random sample annotated with the generation decision features shownin Table 2, together with the average judges?
ratings along each Big Five dimension,as described in Section 3.1.
The following transformations are performed before thelearning phase: Reverse input and output: As parameter estimation models map frompersonality scores to generation parameters, the generation decisions areset as the data set?s output variables and the averaged personality ratingsas the input features. Predict parameters individually: A new data set is created for each outputvariable (i.e., generation parameter) as the statistical models we use onlypredict one output.
We thus make the simplifying assumption thatPERSONAGE?s generation parameters are independent.1 Map output variables into PERSONAGE?s input space: The generationdecisions made when generating each utterance in the random samplewere recorded.
In order to ensure that the parameter estimation models?1 Although this assumption is violated by the internal constraints of PERSONAGE?s generation process,Section 4.3 investigates the extent to which this violation affects the models?
accuracy.470Mairesse and Walker Trainable Generation of Personality Traitsoutput is re-usable by the base generator, the generation decision spaceis mapped to PERSONAGE?s input parameter space.
The conversion isdependent on the type of generation parameter:?
Continuous parameters: Generation decision values are normalized overall random utterances, resulting in values between 0 and 1.
For example,a VERBOSITY parameter value of 1 indicates the utterance with the largestnumber of propositions in the utterance set.?
Aggregation operation probabilities: Frequency counts of aggregationoperations realizing a specific rhetorical relation are divided by thenumber of occurrences of the rhetorical relation in the utterance.
Thisratio is the maximum likelihood estimate of the conditional probabilityof the aggregation operation given the rhetorical relation.
For example,if out of four INFER relations in the utterance, only one is realized usingthe MERGE operation, the value for the INFER - MERGE parameter is.25 for that utterance.?
Binary parameters: No processing is required as generation decisions arealready boolean.
For example, if an exclamation mark was inserted in theutterance, the EXCLAMATION parameter value is set to 1 rather than 0. Feature selection: Personality traits that do not correlate with a generationparameter with a Pearson?s correlation coefficient above .1 are removedfrom that parameter?s data set.
This has the effect of removing parametersthat do not correlate strongly with any trait, which are set to a constantdefault value at generation time.Once the data is partitioned into data sets mapping the relevant personality dimensions(the features) to each generation parameter (the dependent variable), it can be used totrain parameter estimation models predicting the most appropriate parameter valuegiven target personality scores.
Parameters are estimated using either regression orclassification models, depending on whether they are continuous (e.g., VERBOSITY) orbinary (e.g., EXCLAMATION).
Recall that Table 2 indicated for each parameter whether itis continous (C) or binary (B).
In order to identify what model should be used for eachparameter, we compare various learning algorithms using the Weka toolbox (Wittenand Frank 2005).Continuous parameters in Table 2 are modeled with a linear regression model(LR), an M5?
model tree (M5), and a model based on support vector machines with alinear kernel (SVM).
As regression models can extrapolate beyond the [0, 1] interval, theoutput parameter values are truncated if needed?at generation time?before being sentto the base generator.
Regression models are evaluated using the correlation betweenthe model?s predictions and the actual parameter values in the test data.Binary parameters in Table 2 are modeled using classifiers that predict whether theparameter should be enabled or disabled.
We test a Naive Bayes classifier (NB), a C4.5decision tree (J48), a nearest-neighbor classifier using one neighbor (NN), the Ripperrule-based learner (JRIP), the AdaBoost boosting algorithm (ADA), and a support vectormachines classifier with a linear kernel (SVM).
Unless specified, the learning algorithmsuse Weka?s default parameter values.4.
EvaluationThis section first details some of the parameter estimation models trained on the datacollected in Section 3.
The models?
predictive power is then evaluated by doing a471Computational Linguistics Volume 37, Number 310-fold cross-validation in Section 4.2.
Finally, Section 4.3 evaluates human perceptionsof utterances generated using the models.4.1 Qualitative Model EvaluationBefore discussing our quantitative results, we use Figures 6, 7, and 8 to illustrate howthe learned models predict generation parameters from input personality scores.
Notethat sometimes the best performing model is non-linear.
For example, given input traitvalues, the AdaBoost model in Figure 6 outputs the class yielding the largest sum ofweights for the rules returning that class.
The first rule of the EXCLAMATION model inFigure 6 shows that an extraversion score above 6.42 out of 7 would increase the weightof the enabled class by 1.81.
The fifth rule indicates that a target agreeableness above5.13 would further increase the weight by .42.
Figure 6 also illustrates how personalitytraits that do not have an effect on the parameter are removed, i.e., extraversion andagreeableness are the traits that affect the use of exclamation marks.
The STUTTERINGmodel tree in Figure 7 lets us calculate that a low emotional stability (1.0) together with aneutral conscientiousness (4.0) and openness to experience (4.0) yield a parameter valueof .62 (see bottom-left linear model), whereas a neutral emotional stability decreases thevalue down to .17.
The full parameter range obtained when varying both emotionalstability and conscientiousness is illustrated in Figure 9, which shows that the .5 cut-offFigure 6AdaBoost model predicting the EXCLAMATION parameter.
Given input trait values, the modeloutputs the class yielding the largest sum of weights for the rules returning that class.Figure 7M5?
model tree predicting the STUTTERING parameter.472Mairesse and Walker Trainable Generation of Personality TraitsFigure 8SVM model with a linear kernel predicting the CONTENT POLARITY parameter.Figure 9Variation of the predicted STUTTERING parameter value based on the model in Figure 7 fordifferent emotional stability and conscientiousness target scores.
All other trait scores are setto 4.0 out of 7.point can be reached for low emotional stability scores and mid-range conscientiousnessscores.
The linear model in Figure 8 shows that agreeableness has a strong effect on theCONTENT POLARITY parameter (.97 weight), but emotional stability, conscientiousness,and openness to experience also influence the parameter value.Inspection of the learned models provides interesting information about whetherfindings in the psychology literature carry over to our domain.
However, in order tooptimize the overall generation performance, we rely on a quantitative analysis forselecting individual models.4.2 Cross-Validation on Corpus of Expert JudgmentsWe identify the best performing model(s) for each generation parameter via a 10-foldcross-validation.
For continuous parameters, Table 6 evaluates modeling accuracy bycomparing the correlations between the model?s predictions and the actual parametervalues in the test folds.
Table 7 reports results for binary parameter classifiers, bycomparing the F-measures of the enabled class.
The F-measure measures how well themodels predict the enabled class given the small proportion of instances labelled as473Computational Linguistics Volume 37, Number 3Table 6Pearson?s correlation coefficient between parameter model predictions and continuousparameter values, for different regression models.
Parameters that do not correlate with anytrait are omitted.
Results are averaged over a 10-fold cross-validation, and the best result foreach parameter is in bold.Continuous parameters LR M5 SVMContent planning:VERBOSITY 0.24 0.26 0.21RESTATEMENTS 0.14 0.14 0.04REPETITIONS 0.13 0.13 0.08CONTENT POLARITY 0.46 0.46 0.47REPETITION POLARITY 0.02 0.15 0.06CONCESSIONS 0.23 0.23 0.12CONCESSION POLARITY ?0.01 0.16 0.07POLARIZATION 0.20 0.21 0.20Syntactic template selection:SYNTACTIC COMPLEXITY 0.10 0.33 0.26TEMPLATE POLARITY 0.04 0.04 0.05Aggregation operations:INFER - ALSO CUE WORD 0.10 0.10 0.06JUSTIFY - SINCE CUE WORD 0.03 0.07 0.05JUSTIFY - SO CUE WORD 0.07 0.07 0.04JUSTIFY - PERIOD 0.36 0.35 0.21CONTRAST - PERIOD 0.27 0.26 0.26RESTATE - MERGE WITH COMMA 0.18 0.18 0.09CONCEDE - ALTHOUGH CUE WORD 0.08 0.08 0.05CONCEDE - EVEN IF CUE WORD 0.05 0.05 0.03Pragmatic markers:SUBJECT IMPLICITNESS 0.13 0.13 0.04STUTTERING 0.16 0.23 0.17PRONOMINALIZATION 0.22 0.20 0.17Lexical choice:LEXICON FREQUENCY 0.21 0.21 0.19LEXICON WORD LENGTH 0.18 0.18 0.15enabled in the training utterances, namely, it is less sensitive to class-imbalance thanclassification accuracy.
Models producing the best cross-validation results are identifiedin bold for each parameter; parameters that produce a poor modeling accuracy are omit-ted.
Because of the large number of parameters tested simultaneously in each trainingutterance, many reported accuracies are relatively low.
As our training approach aimsat including all parameters that can potentially convey personality, we include modelswith correlations or F-measures above .05 in our system, and let individual models learnthe extent to which their parameter will affect the trained system.Table 6 shows that the CONTENT POLARITY parameter is modeled the most accu-rately, with the SVM model in Figure 8 producing a correlation of .47 with the trueparameter values in Table 6.
Models of the PERIOD aggregation operation also performwell, with a linear regression model yielding a correlation of .36 when realizing ajustification, and .27 when contrasting two propositions.
The SYNTACTIC COMPLEXITYand VERBOSITY parameters are also modeled successfully, with correlations of .33 and.26 using a model tree.
The model tree controlling the STUTTERING parameter illustrated474Mairesse and Walker Trainable Generation of Personality TraitsTable 7F-measure of the enabled class for classification models of binary parameters.
Parameters that donot correlate with any trait are omitted.
Results are averaged over a 10-fold cross-validation, andthe best result for each parameter is in bold.Binary parameters NB J48 NN JRIP ADA SVMContent planning:REQUEST CONFIRMATION 0.00 0.00 0.07 0.05 0.04 0.04Pragmatic markers:SOFTENER HEDGESkind of 0.00 0.00 0.16 0.13 0.11 0.10quite 0.14 0.08 0.09 0.09 0.07 0.06ok 0.13 0.07 0.06 0.05 0.05 0.05FILLED PAUSESerr 0.32 0.20 0.24 0.24 0.22 0.19EXCLAMATION 0.23 0.34 0.36 0.37 0.38 0.34EXPLETIVES 0.27 0.18 0.24 0.20 0.17 0.15IN-GROUP MARKER 0.40 0.31 0.31 0.26 0.24 0.21TAG QUESTION 0.32 0.21 0.21 0.17 0.15 0.13in Figure 7 produces a correlation of .23.
Concerning binary parameters, althoughdifferences between the best performing models are not significant, Table 7 suggeststhat the Naive Bayes classifier is generally the most accurate, with F-measures of .40 forthe IN-GROUP MARKER parameter, and .32 for both the insertion of filled pauses (err)and tag questions.
The results suggest that the AdaBoost learning algorithm performsbest for predicting the EXCLAMATION parameter, with an F-measure of .38 for the modelin Figure 6.These results show that there are large variations between model performance for agiven parameter (e.g., CONCESSION POLARITY).
This suggests that exploring differentlearning algorithms for individual parameters is beneficial.
While the overall modelingaccuracy can seem relatively low?for example, there are only four parameters withcorrelations above r = .25 in Table 6?it is important to keep in mind that the utteranceswere randomly generated, hence the effect of a specific parameter is likely to be affectedby other random parameter values.
The next section therefore evaluates whether themodels are good enough to produce reliable effects on user perceptions.4.3 Evaluation with Naive SubjectsThe evaluation presented in Section 4.2 measures how well the PE models predict theparameters, using parameter settings in the random sample and expert judge ratingsas the predictors.
We relied on a small number of expert judges to minimize ratinginconsistencies and facilitate the learning process.
In order to test whether the PEmethod produces high quality outputs manifesting personality, we ran an experimentwith 24 native English speakers (12 male and 12 female graduate students from a rangeof disciplines from both the U.K. and the U.S.).
We produced a set of 50 utterancesfor this experiment using the best performing models for each generation parametershown in Tables 6 and 7.
Given this model, we generate 5 utterances for each of 10 inputcommunicative goals.
Each utterance targets an extreme value for two traits (either 1or 7 out of 7) and neutral values for the remaining three traits (4 out of 7).
The goal isfor each utterance to project multiple traits on a continuous scale.
Here, we test whether475Computational Linguistics Volume 37, Number 3Figure 10Distribution of the 20 emotional stability target scores, normally distributed over both extremeswith a standard deviation of 10% of the full scale.PE models can convey personality in extreme regions of the Big Five space.
In order togenerate a range of alternatives for each input communicative goal, all target scores arerandomized around their initial value according to a normal probability distributionwith a standard deviation of 10% of the full scale (see Figure 10).All 50 utterances were evaluated by the 24 naive subjects using the TIPI in Figure 3(Gosling, Rentfrow, and Swann 2003).
As in the training data collection (Section 3.1),the subjects rated one set of five utterances at a time, one for each communicativegoal.
The communicative goals and the utterances were presented in random order.To limit the experiment?s duration, only the two traits with extreme target values areevaluated for each utterance.
As a result, 20 utterances are evaluated for each trait, 10of which were generated to convey the low end of that trait, and 10 of which target thehigh end of that trait.
Each utterance was also evaluated for its naturalness as before.Subjects thus answered 5 questions for 50 utterances, two from the TIPI for each extremetrait and one about naturalness (250 judgments in total per subject).
Subjects were nottold that the utterances were intended to manifest extreme trait values.Table 8 shows several sample outputs and the mean personality ratings from thenaive subjects for two communicative goals.
For example, utterance 1.a projects ahigh extraversion through the insertion of an exclamation mark based on the modelin Figure 6, whereas utterance 2.a conveys introversion by beginning with the filledpause err.
The same utterance also projects a low agreeableness by focusing on negativepropositions, through a low CONTENT POLARITY parameter value produced by themodel in Figure 8.4.3.1 Naturalness and Inter-Rater Agreement.
Figure 11 shows the distribution of the nat-uralness ratings.
A Shapiro-Wilk test confirms that the ratings are normally distributed(W = .97, p = .31), with only one utterance out of 50 rated below 2.5 out of 7 on average.The average naturalness is 3.98 out of 7, with a rating of 4 indicating neither agreementnor disagreement.
This is lower than the naturalness scores obtained in Section 3.2for the random training utterances collected using a small number of expert judges.476Mairesse and Walker Trainable Generation of Personality TraitsTable 8Example outputs from PERSONAGE-PE for a comparison (#1) and a recommendation (#2), withthe average judges?
personality (Rating) and naturalness (Nat) scores.
Ratings are on a scale from1 to 7, with 1 = very low (e.g., introvert) and 7 = very high (e.g., extravert).# Trait and target Rating Nat Output utterance1.a Extraversion high6.904.42 4.79 Radio Perfecto?s price is 25 dollarsbut Les Routiers provides adequatefood.
I imagine they?re alright!Agreeableness high7.004.941.b Emotional stability high7.005.35 5.04 Let?s see, Les Routiers and RadioPerfecto... You would probablyappreciate them.
Radio Perfectois in the East Village with kind ofacceptable food.
Les Routiers islocated in Manhattan.
Its priceis 41 dollars.Conscientiousness high6.215.211.c Agreeableness low1.284.04 4.58 Les Routiers and Radio Perfecto areoutstanding.
Radio Perfecto?s priceis 25 dollars, isn?t it?
The food isacceptable, also it?s located inthe East Village.
Err... althoughLes Routiers offers adequate food,it?s damn costly.Openness toexperiencehigh7.004.522.a Extraversion low1.923.65 3.21 Err... you would probably appreciateTrattoria Rustica, wouldn?t you?
It?sin Manhattan, also it?s an italianrestaurant.
It offers poor ambience,also it?s quite costly.Agreeableness low1.734.022.b Emotional stability low1.974.13 4.50 Trattoria Rustica isn?t as bad as theothers.
Err... even if it?s costly, itoffers kind of adequate food,alright?
It?s an italian place.Openness toexperiencelow1.003.852.c Agreeableness low1.003.27 3.58 Trattoria Rustica is the only eatingplace that is any good.
Err... it?slocated in Manhattan.
This restaurantis an italian place with poor ambience.It?s bloody costly, even if this eatinghouse has friendly waiters you see?Openness toexperiencelow1.333.94The differences in naturalness judgments could possibly be due to (a) the different setof judges; (b) the fact that utterances conveying extreme personality are likely to beperceived as less natural; or (c) the fact that the expert judges made a very large numberof judgments, and thus became accustomed to judging the outputs.Table 9 reports the inter-rater correlation over all personality ratings, averaged overthe 276 pairs of judges.
The level of agreement between the naive subjects reflectsthe difficulty of the personality recognition task for humans, thus providing an upperbound on the performance to be expected from a model trained on human data.
The477Computational Linguistics Volume 37, Number 3Figure 11Distribution of naturalness ratings over the 50 utterances, averaged over all 24 judges.
The meannaturalness rating is 3.98, with a standard deviation of 1.07.Table 9Average and standard deviation of the inter-rater correlations over the 276 pairs of judges.Trait r?inter ?interExtraversion .33 .22Emotional stability .41 .17Agreeableness .28 .23Conscientiousness .34 .18Openness to experience .17 .25All .34 .10judges agree modestly, with correlations ranging from .17 (openness to experience) to.41 (emotional stability).
This agreement is lower than that for rule-based utterances,which could be due to the nature of the personality cues conveyed by PERSONAGE-RB?s handcrafted parameters.
However, this difference could also result from the use ofnaive judges, which we believe are less consistent in their personality judgments.4.3.2 Modeling Accuracy.
Modeling accuracy is measured using both the correlation andmean absolute error (on a scale from 1 to 7) between the target personality scores and thejudges?
ratings.
Given the relatively small number of distinct utterances being evaluated(20 per trait), it is important to take the non-determinism of PERSONAGE into accountwhen evaluating the correlation between the target scores and the judges?
ratings.
Eval-uating correlations over the 480 ratings of each judge and utterance pair is likely to resultin an inflated significance level, as it does not account for the possibility that a specificoutcome of PERSONAGE?s random generation decisions could produce the intendedpersonality, rather than accurate personality modeling.
In order to address this issue,we report correlations ravg between the target personality scores and the 20 personalityratings averaged over all 24 judges (i.e., the reported significance levels do take thenumber of distinct test utterances into account).
Table 10 shows that extraversion is thedimension modeled the most accurately by the parameter estimation models, produc-478Mairesse and Walker Trainable Generation of Personality TraitsTable 10Pearson?s correlation coefficient ravg, correlation significance level p, and absolute error ebetween the target personality scores and the mean utterance ratings averaged over 24 judges.Trait ravg p eExtraversion .80 ?
p < .001 1.89Emotional stability .64 ?
p = .002 2.14Agreeableness .68 ?
p < .001 2.38Conscientiousness ?.02 2.79Openness to experience .41 ?
p = .07 2.51?
Statistically significant correlation, p < .05; ?
p = .07 (two-tailed).ing a .80 correlation between the target extraversion and the average subjects?
ratings(p < .001).
Emotional stability and agreeableness ratings also correlate strongly withthe target scores, with correlations of .64 and .68, respectively (p < .005 and p < .001).The correlation for openness to experience is also relatively strong (.41), although it isnot significant at the p < .05 level (p = .07).
These correlations are unexpectedly high;in corpus analyses, significant correlations as low as .05 to .15 are typically observedbetween averaged personality ratings and linguistic markers (Pennebaker and King1999; Mehl, Gosling, and Pennebaker 2006).
Although each utterance is used to testtwo hypotheses (i.e., rated for two traits), results for extraversion, emotional stability,and agreeableness remain largely significant even after applying Bonferroni correction(p < .001, p < .005, and p < .005 respectively).Conscientiousness is the only dimension whose ratings do not correlate with thetarget scores.
The comparison with rule-based results in Table 11 suggests that this isnot because conscientiousness cannot be exhibited in our domain or manifested in asingle utterance, so perhaps this arises from differing perceptions of conscientiousnessbetween the expert and naive judges.
It is also possible that inconsistencies in thetraining data prevented the models from learning accurate cues for conscientiousness,as Table 5 shows that the judges disagreed the most over that trait when rating thetraining utterances.Table 10 shows that the mean absolute error varies between 1.89 and 2.79 on a scalefrom 1 to 7.
Such large errors result from the decision to ask judges to answer just theTIPI questions for the two traits that were the extreme targets, because the judges tendTable 11Comparison between the ratings of PERSONAGE-PE?s utterances with extreme target values(Param models) and the expert judges?
ratings for utterances generated using the Rule-basedmethod of our previous work.Method Rule-based Param modelsTrait Low High Low HighExtraversion 2.96 5.98 3.69 ?
5.06 ?Emotional stability 3.29 5.96 3.75 4.75 ?Agreeableness 3.41 5.66 3.42 4.33 ?Conscientiousness 3.71 5.53 4.16 4.15 ?Openness to experience 2.89 4.21 3.71 ?
4.06?
Significant decrease of the variation range over the average rule-based ratings (p < .05, two-tailed).479Computational Linguistics Volume 37, Number 3Table 12Pearson?s correlation coefficient between the target personality scores and averaged ratings (ravg)for each group of extreme targets, as well as the target score range.Correlation ravg RangeTrait Low High Low HighExtraversion .01 .05 .92 1.31Emotional stability .46 .55 ?
.97 .98Agreeableness .20 ?.17 1.13 .70Conscientiousness .03 ?.32 .97 .79Openness to experience .08 .46 .84 1.52?
Correlation significant at the p = .08 (two-tailed).to use the whole scale, with normally distributed ratings (Shapiro-Wilk tests, p > .05).This means that although the judges make distinctions leading to high correlations,the averaged ratings result in a compressed scale.
This explains the large correlationsdespite the magnitude of the absolute error.It is important to emphasize that generation parameter values were predicted basedon five target personality scores.
Thus, the results show that individual traits are per-ceived even when utterances project other traits as well, as would be expected accordingto the Big Five theory.Table 11 compares the mean ratings of the utterances generated by PERSONAGE-PEwith ratings of 20 utterances generated with the rule-based parameter settings for eachextreme of each Big Five trait (40 for extraversion, resulting in 240 rule-based utterancesin total).
Although rating differences could be due to the different set of judges, Table 11suggests that the handcrafted parameter settings project a more extreme personality for6 traits out of 10.
However, the parameter estimation models are not significantly worsethan the rule-based generator for neuroticism, disagreeableness, unconscientiousness,and openness to experience.
Moreover, the differences between low and high averageratings for the parameter models shown in the right-hand-side of Table 11 are significantfor all traits but conscientiousness (p ?
.001).
Thus parameter estimation models can beused in applications that only require discrete binary variation.Additionally, we evaluate whether the naive subjects perceive the small differenceswithin each group of extreme utterances.
At the beginning of this section, we mentionedthat the target scores used in the evaluation experiment were randomized according to anormal distribution around 1 or 7, with a standard deviation of 10% of the full scale (.60).Figure 10 shows the distribution of the target scores for emotional stability.
We computethe correlation between the target scores and the average judges?
ratings over eachgroup of extreme target scores.2 Table 12 shows that the naive subjects failed to detectthe small variation within each group, although results are close to significance for thepositive end of the emotional stability scale, with a correlation of .55 for the emotionallystable group (p = .08).
This suggests that parameter estimation models could modelthat trait with a high granularity given more training samples, as all that is needed isfor our models to learn to trigger relevant personality markers within extreme regionsof the stylistic space.
For example, Figure 9 shows that the STUTTERING parameter value2 Because target scores are truncated to fit PERSONAGE-PE?s input range (between 1 and 7), approximatelyhalf of the values in each group are either 1.0 or 7.0.480Mairesse and Walker Trainable Generation of Personality Traitspredicted by the model in Figure 7 varies considerably with emotional stability.
Giveninput conscientiousness and openness to experience scores of 4 out of 7, the STUTTERINGparameter becomes enabled (i.e., above .5) for emotional stability scores below 1.81 (seemiddle leaf node in Figure 7).The low correlation observed for other traits (e.g., extraversion) shows that thehigh accuracy reported in Table 10 is due to the successful modeling of large variationsbetween each end of the scale, rather than the small-scale variations within one side ofthe dimension.
Although these results are promising, future work should evaluate thegranularity of parameter estimation models over the full range of the Big Five scales.5.
DiscussionThis article presents a novel SNLG method based on parameter estimation models thatestimate optimal generation parameters given target stylistic scores, which are thenused directly by the base generator to produce the output utterance.
We believe thatdialogue applications such as spoken dialogue systems, interactive drama systems, andintelligent tutoring systems would benefit from taking individual stylistic differencesinto account, and that theories of human personality traits represent an appropriate setof dimensions for a computational model of this adaptation.
Starting from a commu-nicative goal, we show how personality affects all phases of the language generationprocess, and that certain parameters such as the polarity of the content selected havea strong effect on the perception of personality.
We present, to our knowledge, thefirst results of a human evaluation experiment using the perceptions of naive subjectsto evaluate the stylistic variation of a language generator.
The parameter estimationmethod is a computationally tractable generation method that does not require anyovergeneration phase, and our results show that naive judges can recognize the in-tended system personality for most traits.Whereas most previous work on SNLG is based on the overgenerate and scoring(OS) framework (Langkilde and Knight 1998; Walker, Rambow, and Rogati 2002; Isard,Brockmann, and Oberlander 2006), our results show that direct parameter optimizationis a viable alternative for stylistic control.
Given the cost of an overgeneration phase,parameter estimation methods might be the only alternative for real-time generation,which is necessary for spoken dialogue interaction.
The fact that the accuracy of OSmethods depends on the complexity of the overgeneration phase prevents them fromscaling to the large generation space required for stylistic variation.
For example, an ex-haustive search over PERSONAGE?s output space would require more than 267 parame-ter combinations, for each of which an utterance would have to be generated and scored.On the other hand, the tractability of OS methods could be improved by prun-ing candidates throughout the generation process, as well as by using compact datastructures?such as in the OpenCCG chart realizer (White, Rajkumar, and Martin2007)?although we do not know of any SNLG system using this approach.
The mainadvantage of the OS approach is that it can model global utterance features reflectingmultiple generation decisions as well as input dependencies (e.g., sentence length).
Thepredictive accuracy of scoring models trained on our data discussed in the introductionsuggests that OS methods could be useful for stylistic SNLG when limited to a milddata-driven overgeneration phase, e.g., to rerank candidate utterances produced bystochastic parameter estimation models.
The OS approach therefore can be seen as away to relax the parameter independence assumptions required to learn robust models.Parameter estimation models are trained on utterances generated in the applicationdomain.
In contrast with out-of-domain corpus based methods (Isard, Brockmann, and481Computational Linguistics Volume 37, Number 3Oberlander 2006), in-domain personality annotations not only reduce data sparsity byconstraining the range of outputs, they also allow us to explicitly model the relationbetween stylistic factors and generation decisions.
The parameter estimation approachis inspired by previous work by Paiva and Evans (2005), who present a data-drivenmethod for stylistic control that does not require an overgeneration phase.
We extendthis work in multiple ways.
First, we focus on the control of the speaker?s personality,rather than stylistic dimensions extracted from corpora.
Second, we present a methodfor learning parameter estimation models predicting generation decisions directly frominput personality scores, whereas the method of Paiva and Evans (2005) requires asearch for the optimal generation decision over the model?s input space.
Third, wepresent a perceptual evaluation of the generated stylistic variation, using naive humanjudges.We modeled stylistic variation in terms of the Big Five personality traits, becausethese traits are widely accepted as the most important dimensions of behavioral vari-ation within human beings (Norman 1963; Goldberg 1990).
Collecting reliable per-sonality annotations for in-domain utterances is a non-trivial task, as reflected by themoderate inter-rater agreement reported in Section 4.3.1.
The difficulty is increased bythe use of random generation decisions, which is necessary to ensure that the learnedrelation between personality ratings and generation parameters is not due to artificialcorrelations between generation decisions.
One way to increase inter-rater agreementwould be to reduce the number of parameters varied simultaneously for each utterance,thereby decreasing the chances of observing inconsistent markers.
While this wouldrequire collecting a larger training set to model the same number of parameters, onecould filter out irrelevant parameters in a first data collection phase, to focus on dataquality in a second phase.
A second way to improve the quality of the annotationswould be to increase the size of each text sample, by simultaneously rating multipleutterances generated from the same parameters, rather than a single utterance.
This islikely to produce models that are more robust to input variation, given a small increasein annotation effort.
Despite the difficulty of the annotation task, our experiments showthat parameter estimation models are able to detect a large range of personality markersin our training set.
The fact that those markers are successfully recognized by naivejudges suggests that our approach is robust to ambiguities in the data.While the Big Five traits are widely accepted in the psychology literature, theydiffer in terms of their impact on linguistic production.
Results suggest that perceptionsof agreeableness and extraversion are easier to model, whereas conscientiousness andopenness to experience are more difficult.
A possible explanation is that these traitsare not conveyed well in PERSONAGE?s narrow domain.
However, previous personalityrecognition results suggest that observed openness to experience is difficult to modelusing general conversational data as well (Mairesse et al 2007).
It is therefore possiblethat this trait may not be expressed through spoken language as clearly as other traits.The evaluation using naive judges in Table 10 shows that the average ratingscorrelate strongly with the target personality scores for all traits but conscientiousness,with correlation coefficients ranging from .41 (openness to experience) up to .80(extraversion).
It is important to note that the magnitudes of the model correlations arehigh compared to traditional correlations between personality and linguistic markers,which typically range from .05 to .10 (Pennebaker and King 1999; Mehl, Gosling,and Pennebaker 2006).
This is possibly due to our experimental method, whichsimultaneously tests a small number of personality markers in a controlled experiment,whereas such markers are harder to extract from the varied language samples used inpsychology studies.482Mairesse and Walker Trainable Generation of Personality TraitsIn terms of limitations, even though PERSONAGE?s parameters were suggested bypsychological studies, some of them are not modeled successfully by the parameter es-timation approach, and thus were omitted from Tables 6 and 7.
This could be due to therelatively small training set size (160 utterances to optimize 67 parameters).
However,even with a larger training set, it is possible that some of the parameters in Table 2 arenot perceivable in our domain.
Additionally, the parameter-independence assumptionof the PE method could be responsible for the poor accuracy of some models.
Whilethis issue could possibly be resolved by training statistical models that simultaneouslypredict multiple dependent variables (e.g., structured prediction methods), this wouldexponentially increase the size of the parameter prediction space and further aggravatedata sparsity issues.We also find that parameter estimation models perform slightly worse when pro-jecting extreme traits than the rule-based generator in the same domain (see Sec-tion 4.3.2).
We believe this is due to the lack of extreme utterances in the training data tolearn from.
Future work should consider whether it is possible to flatten the distributionof training data, perhaps using more knowledge or active learning methods for NLG(Mairesse et al 2010).Another limitation of the current approach is that we assume the existence ofa generation dictionary containing syntactic templates that, in some cases, expressvarious pragmatic effects, even though most of PERSONAGE?s variation is generatedautomatically.
Our dictionary is currently handcrafted, but future work could build onresearch on methods for extracting the generation dictionary from data (Barzilay andLee 2002; Higashinaka, Walker, and Prasad 2007; Snyder and Barzilay 2007).Finally, it is likely that both the accuracy and coverage of parameter estimationmodels could be improved with a larger sample of judges and random utterances atdevelopment time.
A larger number of judges would also smooth out rating inconsis-tencies and individual differences in personality perception, thus allowing the directmodeling of laypeople?s perceptions by removing the need for expert judges.Another important area for future work is to explore the interface between thelanguage generator and the dialogue manager and text-to-speech (TTS) engine, becausepersonality also affects aspects of dialogue such as initiative and prosodic realization(Scherer 1979; Vogel and Vogel 1986).
It might be possible to apply a similar methodol-ogy to the parameterization of a dialogue manager and a TTS component, in order toproject a consistent personality to the user, or to use a reinforcement learning approachto train the dialogue manager (Walker, Fromer, and Narayanan 1998; Singh et al 2002),with personality judgments as the objective function.
Our results suggest that person-ality can be recognized by manipulating the linguistic cues of a single utterance, but itis likely that additional cues (e.g., characterizing the system?s dialogue strategy) couldonly make the user?s perception of the system?s personality more robust.Nevertheless it is clear that the PE SNLG approach offers many advantages overhandcrafted methods.
Firstly, SNLG methods can scale more easily to other domainsand other types of stylistic variation, as collecting data requires less effort than tuninga large number parameters.
In order to apply our method to a new domain or task, thebase generator would have to be modified.
Although PERSONAGE was implementedwith domain independence in mind by using general parameters, handling a new com-municative goal (e.g., a user request) would require modifying the syntactic aggregationoperations as well as the pragmatic marker insertion rules.
Porting PERSONAGE toa new information presentation domain would only require modifying the syntactictemplates in the generation dictionary.
Once a base generator is available in the targetdomain, it can be used to collect ratings for any stylistic dimension.
Our parameter483Computational Linguistics Volume 37, Number 3estimation method can then learn to control the generator?s output from data, withoutany further handcrafting or parameter tuning.
A second advantage of our parameterestimation method over handcrafted approaches is that it can learn to target scalarcombinations of the Big Five traits, whereas handcrafting all possible ways in whichmultiple stylistic dimensions can affect output utterances requires considerable effort.Thirdly, our parameter estimation method can easily model continuous stylistic dimen-sions, while producing such models by hand is not tractable.
For example, one couldinterpolate various handcrafted parameter sets to handle continuous input scales (e.g.,to generate an utterance that is 75% extravert); however, the various weights can onlybe learned reliably from data.Our results also suggest that SNLG systems may still require handcrafting to guar-antee the naturalness of the output, but that data-driven models can be used to controlvarious pragmatic effects.
As naturalness depends on the combination of multiplegeneration decisions, modeling naturalness in the PE framework would require takinginter-parameter dependencies into account.
At training time, this can be achieved byextending each PE model with independent variables for naturalness as well as allprevious generation decisions made.
During the generation process, the models wouldpredict future generation decisions given all previous decisions and a high input natu-ralness value.
Although we believe this approach is tractable, it optimizes parameterssequentially in a greedy fashion.
A global optimization method would require jointlyoptimizing all (past and future) parameters simultaneously.
However, predictions insuch a large output space would require a much larger training set.
Nevertheless, webelieve that future work should evaluate methods for jointly optimizing multiple NLGdecisions.Finally, this work focuses on controlling the perceptions of the output of a dialoguesystem, but an important next step is to use these techniques together with personality-based user modeling methods (Mairesse et al 2007) to simultaneously model thepersonality of the user and the system in dialogue.
Both models provide the toolsfor testing various hypotheses regarding personality-based alignment, such as thesimilarity?attraction effect (Isard, Brockmann, and Oberlander 2006).
Furthermore,the optimal personality of the system is likely to be application-dependent; it wouldthus be useful to evaluate how the user?s and the system?s personality affect taskperformance in different applications.The PERSONAGE language generator is available for download at http://mi.eng.cam.ac.uk/?farm2/personage, as well as the personality-annotated corpus col-lected for our experiments.
An on-line demonstrator and a tutorial for customiz-ing PERSONAGE for a new domain can be found at http://people.csail.mit.edu/francois/research/personage.AcknowledgmentsThis work was funded by a Royal SocietyWolfson Research Merit Fellowship toMarilyn Walker and a Vice Chancellor?sstudentship to Franc?ois Mairesse.
Thanks toChris Mellish, Donia Scott, Rob Gaizauskas,and Roger Moore for detailed commentson earlier versions of this work.ReferencesAndre?, Elisabeth, Thomas Rist, Susannevan Mulken, Martin Klesen, and StephanBaldes.
2000.
The automated designof believable dialogues for animatedpresentation teams.
In J. Sullivan,S.
Prevost, J. Cassell, and E. Churchill,editors, Embodied ConversationalAgents.
MIT Press, Cambridge, MA,pages 220?255.Bangalore, Srinivas and Owen Rambow.2000.
Exploiting a probabilistic hierarchicalmodel for generation.
In Proceedingsof the 18th International Conference onComputational Linguistics (COLING),pages 42?48, Saarbru?cken.484Mairesse and Walker Trainable Generation of Personality TraitsBarzilay, Regina and Lillian Lee.
2002.Bootstrapping lexical choice viamultiple-sequence alignment.
InProceedings of EMNLP, pages 164?171,Philadelphia, PA.Belz, Anja.
2005.
Corpus-driven generationof weather forecasts.
In Proceedings ofthe Third Corpus Linguistics Conference,Birmingham.
http://www.icwsm.org/papers/paper30.html.Belz, Anja.
2008.
Automatic generationof weather forecast texts usingcomprehensive probabilistic generation-space models.
Natural LanguageEngineering, 14(4):431?455.Biber, Douglas.
1988.
Variation Across Speechand Writing.
Cambridge University Press.Bouayad-Agha, Nadjet, Donia R. Scott, andRichard Power.
2000.
Integrating contentand style in documents: A case study ofpatient information leaflets.
InformationDesign Journal, 9(2-3):161?176.Brennan, Susan E. and Herbert H. Clark.1996.
Conceptual pacts and lexical choicein conversation.
Journal of ExperimentalPsychology: Learning, Memory AndCognition, 22:1482?1493.Cassell, Justine and Timothy Bickmore.
2003.Negotiated collusion: Modeling sociallanguage and its relationship effects inintelligent agents.
User Modeling andUser-Adapted Interaction, 13:89?132.Chambers, Nathanael and James Allen.2004.
Stochastic language generationin a dialogue system: Toward a domainindependent generator.
In Proceedings ofthe 5th SIGdial Workshop on Discourse andDialogue, pages 9?18, Cambridge, MA.Chklovski, Timothy and Patrick Pantel.2004.
VERBOCEAN: Mining the Web forfine-grained semantic verb relations.
InProceedings of the Conference on EmpiricalMethods in Natural Language Processing(EMNLP), pages 33?40, Barcelona.Clark, Herbert H. and Susan E. Brennan.1991.
Perspectives on socially sharedcognition.
In L. B. Resnick, J. Levine,and S. D. Bahrend, editors, Groundingin Communication, pages 127?149,American Psychological Association,Washington, DC.Coltheart, Max.
1981.
The MRCpsycholinguistic database.
QuarterlyJournal of Experimental Psychology,33A:497?505.Costa, Paul T. and Robert R. McCrae,1992.
NEO PI-R Professional Manual.Psychological Assessment Resources,Odessa, FL.Daume?
III, Hal.
2007.
Frustratingly easydomain adaptation.
In Proceedings of the45th Annual Meeting of the Associationfor Computational Linguistics (ACL),pages 256?263, Prague.Dunbar, Robin.
1996.
Grooming, Gossip,and the Evolution of Language.
HarvardUniversity Press, Cambridge, MA.Fellbaum, Christiane.
1998.
WordNet: AnElectronic Lexical Database.
MIT Press,Cambridge, MA.Funder, David C. 1997.
The Personality Puzzle.W.
W. Norton & Company, New York,2nd edition.Furnham, Adrian.
1990.
Language andpersonality.
In H. Giles and W. Robinson,editors, Handbook of Language and SocialPsychology.
Wiley, Chichester, pages 73?95.Goffman, Erving.
1970.
Strategic Interaction.Blackwell, Oxford.Goldberg, Lewis R. 1990.
An alternative?description of personality?
: The Big-Fivefactor structure.
Journal of Personality andSocial Psychology, 59:1216?1229.Gosling, Samuel D., Sam Gaddis, andSimine Vazire.
2007.
Personalityimpressions based on facebook profiles.In Proceedings of the International Conferenceon Weblogs and Social Media, Boulder, CO.http://www.icwsm.org/papers/paper30.html.Gosling, Samuel D., Peter J. Rentfrow, andWilliam B. Swann.
2003.
A very briefmeasure of the big five personalitydomains.
Journal of Research in Personality,37:504?528.Green, Stephen J. and Chrysanne DiMarco.1996.
Stylistic decision-making in naturallanguage generation.
Lecture Notes inComputer Science, Trends in NaturalLanguage Generation: An ArtificialIntelligence Perspective, 1036:125?143.Higashinaka, Ryuichiro, Marilyn A.Walker, and Rashmi Prasad.
2007.An unsupervised method for learninggeneration lexicons for spoken dialoguesystems by mining user reviews.
ACMTransactions on Speech and LanguageProcessing, 4(4):Article 8.Hovy, Eduard.
1988.
Generating NaturalLanguage Under Pragmatic Constraints.Lawrence Erlbaum Associates,Mahwah, NJ.Inkpen, Diana Zaiu and Graeme Hirst.
2004.Near-synonym choice in natural languagegeneration.
In Nicolas Nicolov, KalinaBontcheva, Galia Angelova, and RuslanMitkov, editors, Recent Advances in NaturalLanguage Processing III, pages 141?152,485Computational Linguistics Volume 37, Number 3John Benjamins Publishing Company,Amsterdam.Isard, Amy, Carsten Brockmann, and JonOberlander.
2006.
Individuality andalignment in generated dialogues.In Proceedings of the 4th InternationalNatural Language Generation Conference(INLG), pages 22?29, Sydney.John, Oliver P., Eileen M. Donahue, andRobert L. Kentle.
1991.
The ?Big Five?Inventory: Versions 4a and 5b.
Universityof California, Berkeley, Institute ofPersonality and Social Research.Jordan, Pamela W. 2000.
Intentional Influenceson Object Redescriptions in Dialogue:Evidence from an Empirical Study.
Ph.D.thesis, Intelligent Systems Program,University of Pittsburgh.Kittredge, Richard, Tanya Korelsky, andOwen Rambow.
1991.
On the need fordomain communication knowledge.Computational Intelligence, 7(4):305?314.Labov, William.
2006.
The Social Stratificationof English in New York City.
CambridgeUniversity Press.Langkilde, Irene and Kevin Knight.
1998.Generation that exploits corpus-basedstatistical knowledge.
In Proceedings ofthe 36th Annual Meeting of the Associationfor Computational Linguistics (ACL),pages 704?710, Montreal.Langkilde-Geary, Irene.
2002.
An empiricalverification of coverage and correctness fora general-purpose sentence generator.
InProceedings of the International Conference onNatural Language Generation, pages 17?24,Harriman, NY.Lavoie, Benoit and Owen Rambow.
1997.A fast and portable realizer for textgeneration systems.
In Proceedingsof the 3rd Conference on Applied NaturalLanguage Processing, pages 265?268,Washington, D.C.Lester, James C., Brian Stone, and GaryStelling.
1999.
Lifelike pedagogical agentsfor mixed-initiative problem solving inconstructivist learning environments.User Modeling and User-AdaptedInteraction, 9(1-2):1?44.Lester, James C., Stuart G. Towns, andPatrick J. Fitzgerald.
1999.
Achievingaffective impact: Visual emotivecommunication in lifelike pedagogicalagents.
The International Journal of ArtificialIntelligence in Education, 10(3-4):278?291.Levelt, Willem J. and Stephanie Kelter.
1982.Surface form and memory in questionanswering.
Cognitive Psychology,14(1):78?106.Litman, Diane J. and Kate Forbes-Riley.2004.
Predicting student emotions incomputer?human tutoring dialogues.
InProceedings of the 42nd Annual Meeting of theAssociation for Computational Linguistics(ACL), pages 352?359, Barcelona.Litman, Diane J. and Kate Forbes-Riley.2006.
Recognizing student emotions andattitudes on the basis of utterances inspoken tutoring dialogues with bothhuman and computer tutors.
SpeechCommunication, 48(5):559?590.Mairesse, Franc?ois.
2008.
Learning to Adapt inDialogue Systems: Data-driven Models forPersonality Recognition and Generation.Ph.D.
thesis, Department of ComputerScience, University of Sheffield.Mairesse, Franc?ois, Milica Gas?ic?, FilipJurc??
?c?ek, Simon Keizer, Blaise Thomson,Kai Yu, and Steve Young.
2010.Phrase-based statistical languagegeneration using graphical modelsand active learning.
In Proceedings of the48th Annual Meeting of the Associationfor Computational Linguistics (ACL),pages 1552?1561, Uppsala.Mairesse, Franc?ois and Marilyn A. Walker.2007.
PERSONAGE: Personalitygeneration for dialogue.
In Proceedingsof the 45th Annual Meeting of the Associationfor Computational Linguistics (ACL),pages 496?503, Prague.Mairesse, Franc?ois and Marilyn A. Walker.2010.
Towards personality-based useradaptation: Psychologically-informedstylistic language generation.
UserModeling and User-Adapted Interaction,20(3):227?278.Mairesse, Franc?ois, Marilyn A. Walker,Matthias R. Mehl, and Roger K. Moore.2007.
Using linguistic cues for theautomatic recognition of personality inconversation and text.
Journal of ArtificialIntelligence Research (JAIR), 30:457?500.Marcus, David K., Scott O. Lilienfeld, John F.Edens, and Norman G. Poythress.
2006.
Isantisocial personality disorder continuousor categorical?
A taxometric analysis.Psychological Medicine, 36(11):1571?1582.McQuiggan, Scott W., Bradford W. Mott,and James C. Lester.
2008.
Modelingself-efficacy in intelligent tutoringsystems: An inductive approach.
UserModeling and User-Adapted Interaction,18(1):81?123.Mehl, Matthias R., Samuel D. Gosling, andJames W. Pennebaker.
2006.
Personalityin its natural habitat: Manifestations andimplicit folk theories of personality in486Mairesse and Walker Trainable Generation of Personality Traitsdaily life.
Journal of Personality and SocialPsychology, 90:862?877.Nakatsu, Crystal and Michael White.
2006.Learning to say it well: Rerankingrealizations by predicted synthesis quality.In Proceedings of the 44th Annual Meeting ofthe Association for Computational Linguistics(ACL), pages 1113?1120.Norman, Warren T. 1963.
Toward anadequate taxonomy of personalityattributes: Replicated factor structurein peer nomination personality rating.Journal of Abnormal and Social Psychology,66:574?583.Paiva, Daniel S. and Roger Evans.
2004.A framework for stylistically controlledgeneration.
In Proceedings of theInternational Conference on NaturalLanguage Generation, pages 120?129,Sydney.Paiva, Daniel S. and Roger Evans.
2005.Empirically-based control of naturallanguage generation.
In Proceedings ofthe 43rd Annual Meeting of the Associationfor Computational Linguistics (ACL),pages 58?65, Ann Arbor, MI.Paris, Ce?cile and Donia R. Scott.
1994.Stylistic variation in multilingualinstructions.
In Proceedings of the 7thInternational Workshop on NaturalLanguage Generation, pages 45?52,Kennebunkport, ME.Pennebaker, James W., Martha E. Francis,and Roger J. Booth.
2001.
Inquiry and WordCount: LIWC 2001.
Lawrence ErlbaumAssociates, Mahwah, NJ.Pennebaker, James W. and Laura A. King.1999.
Linguistic styles: Language use as anindividual difference.
Journal of Personalityand Social Psychology, 77:1296?1312.Pickering, Martin J. and Simon Garrod.
2004.Towards a mechanistic theory of dialogue.Behavioral and Brain Sciences, 27:169?225.Piwek, Paul.
2003.
A flexible pragmatics-driven language generator for animatedagents.
In Proceedings of the Annual Meetingof the European Chapter of the Associationfor Computational Linguistics (EACL),pages 151?154, Budapest.Pollack, Martha E. 1991.
Overloadingintentions for efficient practical reasoning.Nou?s, 25:513?536.Porayska-Pomsta, Kaska and Chris Mellish.2004.
Modelling politeness in naturallanguage generation.
In Proceedings of theInternational Conference on Natural LanguageGeneration, pages 141?150, Sydney.Power, Richard, Donia R. Scott, and NadjetBouayad-Agha.
2003.
Generating textswith style.
In Proceedings of the 4thInternational Conference on Intelligent TextProcessing and Computational Linguistics,pages 444?452, Mexico City.Rambow, Owen, Monica Rogati, andMarilyn A. Walker.
2001.
Evaluating atrainable sentence planner for a spokendialogue travel system.
In Proceedings ofthe 39th Annual Meeting of the Associationfor Computational Linguistics (ACL),pages 434?441, Toulouse.Reiter, Ehud and Robert Dale.
2000.
BuildingNatural Language Generation Systems.Cambridge University Press.Scherer, Klaus R. 1979.
Personality markersin speech.
In K. R. Scherer and H. Giles,editors, Social Markers in Speech.Cambridge University Press,pages 147?209.Scott, Donia R. and Clarisse Sieckenius deSouza.
1990.
Getting the message across inRST-based text generation.
In R. Dale,C.
Mellish, and M. Zock, editors, CurrentResearch in Natural Language Generation.Academic Press, London, pages 47?73.Singh, Satinder, Diane J. Litman, MichaelKearns, and Marilyn A. Walker.
2002.Optimizing dialogue management withreinforcement learning: Experiments withthe NJFun system.
Journal of ArtificialIntelligence Research, 16:105?133.Snyder, Benjamin and Regina Barzilay.
2007.Database-text alignment via structuredmultilabel classification.
In Proceedings ofthe International Joint Conference on ArtificialIntelligence (IJCAI), pages 1713?1718.Srivastava, Sanjay, Steve Guglielmo, andJennifer S. Beer.
2010.
Perceivingothers?
personalities: Examining thedimensionality, assumed similarityto the self, and stability of perceivereffects.
Journal of Personality and SocialPsychology, 98(3):520?534.Stent, Amanda and Hui Guo.
2005.
A newdata-driven approach for multimediapresentation generation.
In Proceedingsof EuroIMSA, Grindelwald.
http://www.actapress.com/Content of Proceeding.aspx?ProceedingID=293.Stent, Amanda, Rashimi Prasad, andMarilyn A. Walker.
2004.
Trainablesentence planning for complexinformation presentations in spokendialog systems.
In Proceedings of the42nd Annual Meeting of the Associationfor Computational Linguistics (ACL),pages 79?86, Barcelona.Stone, Matthew and Bonnie Webber.
1998.Textual economy through close coupling487Computational Linguistics Volume 37, Number 3of syntax and semantics.
In Proceedings ofthe 1998 International Workshop on NaturalLanguage Generation, pages 178?187,Niagara-on-the-Lake.Tapus, Adriana and Maja Mataric.
2008.Socially assistive robots: The link betweenpersonality, empathy, physiologicalsignals, and task performance.
InProceedings of the AAAI Spring Symposiumon Emotion, Personality and Social Behavior,pages 133?140, Palo Alto, CA.Vogel, Klaus and Sigrid Vogel.
1986.L?interlangue et la personalite?
del?apprenant.
International Journal ofApplied Linguistics, 24(1):48?68.Walker, Marilyn A.
1993.
InformationalRedundancy and Resource Bounds inDialogue.
Ph.D. thesis, University ofPennsylvania.Walker, Marilyn A., Janet E. Cahn, andStephen J. Whittaker.
1997.
Improvisinglinguistic style: Social and affective basesfor agent personality.
In Proceedings of the1st Conference on Autonomous Agents,pages 96?105, Marina Del Rey, CA.Walker, Marilyn A., Jeanne C. Fromer, andShrikanth Narayanan.
1998.
Learningoptimal dialogue strategies: A case studyof a spoken dialogue agent for email.In Proceedings of the 36th Annual Meeting ofthe Association for Computational Linguistics,COLING/ACL 98, pages 1345?1352,Montreal.Walker, Marilyn A. and Owen Rambow.2002.
Spoken language generation.Computer Speech and Language, SpecialIssue on Spoken Language Generation,16(3-4):273?281.Walker, Marilyn A., Owen Rambow, andMonica Rogati.
2002.
Training a sentenceplanner for spoken dialogue usingboosting.
Computer Speech and Language,16(3-4):409?433.Walker, Marilyn A., Amanda Stent, Franc?oisMairesse, and Rashmi Prasad.
2007.Individual and domain adaptation insentence planning for dialogue.
Journalof Artificial Intelligence Research (JAIR),30:413?456.Wang, Ning, W. Lewis Johnson, Richard E.Mayer, Paola Rizzo, Erin Shaw, andHeather Collins.
2005.
The politenesseffect: Pedagogical agents and learninggains.
Frontiers in Artificial Intelligence andApplications, 125:686?693.White, Michael, Rajakrishnan Rajkumar,and Scott Martin.
2007.
Towards broadcoverage surface realization with CCG.In Proceedings of the Workshop on UsingCorpora for NLG: Language Generationand Machine Translation, pages 22?30,Copenhagen.Witten, Ian H. and Eibe Frank.
2005.
DataMining: Practical Machine Learning Toolsand Techniques.
Morgan Kaufmann,San Francisco, CA.488
