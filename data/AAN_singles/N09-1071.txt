Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 629?637,Boulder, Colorado, June 2009. c?2009 Association for Computational LinguisticsA Finite-State Turn-Taking Model for Spoken Dialog SystemsAntoine Raux?Honda Research Institute800 California StreetMountain View, CA 94041, USAaraux@hra.comMaxine EskenaziLanguage Technologies InstituteCarnegie Mellon UniversityPittsburgh, PA 15213, USAmax@cs.cmu.eduAbstractThis paper introduces the Finite-State Turn-Taking Machine (FSTTM), a new model tocontrol the turn-taking behavior of conversa-tional agents.
Based on a non-deterministicfinite-state machine, the FSTTM uses a costmatrix and decision theoretic principles to se-lect a turn-taking action at any time.
We showhow the model can be applied to the problemof end-of-turn detection.
Evaluation results ona deployed spoken dialog system show that theFSTTM provides significantly higher respon-siveness than previous approaches.1 IntroductionTurn-taking, the process by which participants in aconversation alternate speech and silence, is an es-sential component of spoken interaction.
In order tolead productive conversations, people need not onlyknow what to say but also when to say it.
Decadesof research on Conversation Analysis and psycholin-guistics (Duncan, 1972; Sacks et al, 1974; Ore-stro?m, 1983; Schegloff, 2000; Wesseling and vanSon, 2005) have shown that human turn-taking be-havior relies on a wide range of rules and signalsat many different levels of language, from prosodyto syntax, semantics, and discourse structure.
Incontrast, turn-taking in spoken dialog systems is of-ten reduced to ad hoc rules only based on very lowlevel features.
This simplistic approach leads to in-efficient, unnatural, and possibly confusing behavior(Porzel and Baudis, 2004; Ward et al, 2005).?
This research was conducted when the first author was astudent at the Language Technologies Institute.Recently, more complex models of turn-takinghave been proposed (Cassell et al, 2001; Thorisson,2002; Kronild, 2006).
Yet, these models still relyextensively on hand-coded expert knowledge anddo not lend themselves to data-driven optimization.Furthermore, to our knowledge, no such model hasbeen deployed in a widely used system outside of thelaboratory.
In this paper, we propose a flexible, prac-tical model of turn-taking behavior that builds uponprevious work on finite-state models of the conver-sational floor.
Because of its simplicity and gener-ality, this model can be applied to many turn-takingphenomena.
At the same time, being grounded indecision theory, it lends itself well to data-driven op-timization.
We illustrate our approach by applyingthe model to a specific turn-taking task: end-of-turndetection.2 Conversational Floor as a Finite-StateMachine2.1 6-state finite state models of turn-takingIn the 1960?s and early 1970?s, several researchersproposed models to explain the rhythmic turn-takingpatterns in human conversation.
In particular, Jaffeand Feldstein (1970) studied the mean duration ofpauses, switching pauses (when a different speakertakes the floor), simultaneous speech, and (single-speaker) vocalizations in recorded dyadic conversa-tions.
Based on their observation that these dura-tions follow exponential distributions, they proposedfirst-order Markov models to capture the alterna-tion of speech and silence in dialog.
Their initialmodel had four states: only participant A is speak-629Figure 1: Our six-state model of turn-taking, inspired byJaffe and Feldstein (1970) and Brady (1969).
See section3.1 for a description of the states.ing; only participant B is speaking; both participantsare speaking; and neither participant is speaking.However, such a model fails to distinguish switch-ing pauses from A to B from switching pauses fromB to A.
Based on this observation, they extend theirmodel to a six-state model which they found to bet-ter fit their data than the four-state model.
Aroundthe same time, Brady (1969) developed a very sim-ilar six-state model.
He trained the parameters on arecorded conversation and compared the generatedconversations to the original real one along severaldimensions (pause and speech segment durations,overlaps, etc), finding that his model generally pro-duced a good fit of the data.2.2 Finite-State Models for ControlWhile Jaffe, Feldstein and Brady were primarilyconcerned with the analysis of human-human con-versations, more recently, several researchers haveproposed finite-state machines to control conversa-tional agents.
For instance, Cassell et al (2001)model the conversational state of an embodied realestate agent as a 5-state machine.
Two states indi-cate whether a user is present or not, whereas theother three indicate who holds the floor between theuser and the agent, or whether the floor is open.Floor conflicts are not captured by this machine andare presumably resolved through simple rules (e.g.when the user speaks, the agent yields the floor).Kronild (2006) proposes a much more complexmodel, based on Harel statecharts, which are an ex-tension of finite-state machines for modeling and vi-sualizing abstract control (Harel, 1987).Thorisson?s Ymir architecture (Thorisson, 2002)is an attempt to model the cognitive processes in-volved in conversation.
It features dialog states, cap-turing, for example, who has the floor, and rules thatgovern the transition from one state to another basedon ?boolean conditions of perceptual features?.All these models are deterministic.
At any pointin time, the agent knows who owns the floor and usesfixed rules to take appropriate actions.
These ap-proaches assume 1) that the system can obtain per-fectly reliable information on the state of the world,and 2) that the state itself is unambiguous.3 The Finite-State Turn-Taking Machine3.1 Extending the 6-state model for controlOur model, the Finite-State Turn-Taking Machine(FSTTM), uses the same six states as Jaffe andFeldstein: USER and SY STEM represent stateswhere one and only one of the participants claimsthe floor, FREES and FREEU states where noparticipant claims the floor (following, resp., aSY STEM and USER state), and BOTHS andBOTHU states where both participants claim thefloor (following, resp.
a SY STEM and USERstate).
However, we apply this model to the controlof a conversational agent, with a goal similar to thatof Cassel, Thorisson, and Kronild.
One importantdistinction is that we define the states in terms of theparticipants?
intentions and obligations (in the senseof Traum and Allen (1994)) rather than the surfacelevel observation of speech vs silence.
For example,the state is USER when the user has the obligationto speak (to respond to a system question) or the in-tention to speak, while at the same time, the systemdoes not hold the floor.
This does not necessarilymean that the user is speaking, for example at pausesduring a user utterance.As can be seen in Figure 1, not all transitions arevalid.
First, there is no direct transition between anyof the intermediate states (the two FREE states andtwo BOTH states).
The assumption is that to gofrom any of these state to another, the model willfirst go to either SY STEM or USER.
This is an630approximation as there might be cases where, forexample, both the system and user start speakingat the exact same time, going from a FREE stateto a BOTH state.
However these cases are rareenough that they can be approximated using a tran-sition through either SY STEM or USER.
Sec-ond, because intermediate states are conditioned onwho had the floor previously, not all valid transitionsare bidirectional.
For example, there is no transi-tion from SY STEM to BOTHU .
We associatepairs of user/system actions to each transition.
Thefour possible actions are Grab the floor, Release thefloor, Wait while not claiming the floor, and Keepthe floor.
For example, transition from SY STEMto FREES corresponds to the user waiting silentlyand the system releasing the floor at the end of aprompt, noted (R,W ) (we always note the systemaction first and user action second).This representation allows us to formalize a widevariety of turn-taking phenomena in a unified frame-work.
Specifically, there are 4 types of 2-step transi-tions from a single-floor-holder state (SY STEM orUSER) to another (or the same) single-floor-holderstate, which represent typical turn-taking phenom-ena:Turn transitions with gap are the most commonway the floor goes from one participant to theother.
For example, at the end of a user utter-ance, once the user finishes speaking, the floorbecomes free, after which the system starts re-sponding, thus grabbing the floor.
The resultingstate sequence is:SY STEM (R,W )?
FREES(W,G)?
USERConversely, the transition with gap following asystem prompt corresponds to:USER (R,W )?
FREES(W,G?
USERTurn transitions with overlap happen when a par-ticipant grabs the floor while it still belongs tothe other.
For example, when a user barges inon a system prompt, both participants hold thefloor.
Then, the system recognizes the barge-in attempt and relinquishes the floor, which be-comes user?s.SY STEM (K,G)?
BOTHS(R,K?
USERAnd conversely, when the system interrupts theuser mid-utterance (which in dialog systems ismore often the result of an intentional cut-in,rather than intentional interruption), the statesequence is:USER (G,K)?
BOTHU(K,R)?
SY STEMFailed interruptions happen when a participantbarges in on the other and then withdraws be-fore the original floor holder releases the floor.For example, when the system interrupts theuser (often by mistake) but detects it and in-terrupts itself:USER (G,K)?
BOTHU(R,K?
USERThe converse is usually the result of the systemfailing to react fast enough to a user barge-in:SY STEM (K,G)?
BOTHS(K,R)?
SY STEMNote that backchannels seem to fit in this cat-egory too.
However, since backchannels, bydefinition, do not represent an attempt to grabthe floor, they are not captured by the modelas it is (for example, the floor should remainSY STEM when a user backchannels a sys-tem utterance).Time outs start like transitions with gap but the in-tended next speaker (e.g.
the user after a systemprompt) does not take the floor and the originalfloor holder grabs it back.
For instance, after asystem prompt, if the floor remains free for acertain amount of time, the system attempts tore-establish the communication with the user,as follows:SY STEM (R,W )?
FREES(G,W?
SY STEMThe opposite also happens when the system isto slow to respond to the user:USER (W,R)?
FREEU(W,G?
USERWhile all the transitions above were describedas deterministic, the actual state of the model isnot fully observable.
Specifically, while the system631knows whether its claiming the floor or not, it canonly believe with some degree of uncertainty thatthe user does so.
The system?s knowledge of its ownclaim to the floor splits the state space into two dis-joint subsets.
When the system claims the floor, thestate can be SY STEM , BOTHS , or BOTHU ).When the system does not claim the floor, the statecan be USER, FREEU , or FREES).
In eithercase, the system needs to recognize the user?s in-tention (i.e.
whether the user claims to the floor ornot) to maintain a probability distribution over thethree states.
Since the distinction between the twoBOTH states (resp.
the two FREE states) is basedon past history that can be known with a high levelof certainty, the uncertainty in state distribution isfully characterized by the probability that the user isclaiming the floor, which will have to be estimatedfrom observations, as we will see below.3.2 Cost of Turn-Taking ActionsThe problem we are facing is that of choosing thebest system action given the system?s belief aboutthe current state of the model.
That is achieved byapplying the probabilistic decision theory principleof selecting the action with lowest expected cost.The actions available to the system are the four de-scribed above (G,R,K,W ), although not all actionsare available in all states.
In fact, as can be seen inTable 1, there are always only two actions availablein each state, depending on whether the system isclaiming the floor or not.Each action in each state has a particular cost.While there are many possible ways of definingthese costs, we propose a simple cost structure thatderives from the principles laid out in Sacks et al(1974):Participants in a conversation attempt tominimize gaps and overlaps.From this general principle, we derive three rules todrive the design of a cost matrix:1.
The cost of an action that resolves either a gapor an overlap is zero2.
The cost of an action that creates unwanted gapor overlap is equal to a constant parameter (po-tentially different for each action/state pair)3.
The cost of an action that maintains a gap oroverlap is either a constant or an increasingfunction of the total time spent in that stateThe resulting cost matrix is shown in Table 1, where?
CS is the cost of interrupting a system promptbefore its end when the user is not claiming thefloor (false interruption)?
CO(? )
is the cost of remaining in an overlapthat is already ?
ms long?
CU is the cost of grabbing the floor when theuser is holding it (cut-in)?
CG(? )
is the cost of remaining in a gap that isalready ?
ms longThis cost structure makes a number of simplifyingassumptions and there are many other possible costmatrices.
For example, the cost of interrupting theuser might vary depending on what has already beensaid in the utterance, so does the cost of interrupt-ing a system prompt.
A more principled approachto setting the costs would be to estimate from per-ceptual experiments or user studies what the impactof remaining in gap or overlap is compared to thatof a cut-in or false interruption.
However, as a firstapproximation, the proposed cost structure offers asimple way to take into account some of the con-straints of interaction.3.3 Decision Theoretic Action SelectionGiven the state space and the cost matrix givenabove, the optimal decision at any point in time isthe one that yields the lowest expected cost, wherethe expected cost of action A is:C(A) = ?S?
?P (s = S|O) ?
C(A,S)where ?
is the set of states, O are the observablefeatures of the world, and C(A,S) is the cost of ac-tion A in state S, from the cost matrix in Table 1.In addition to the cost matrix?
four constants, whichwe will consider as parameters of the model, it isthus necessary to estimate P (s = S|O), which asseen above amounts to estimate the probability thatthe user is claiming the floor.
Key to applying theFSTTM to a practical turn-taking problem is thusthe construction of accurate estimates of the proba-bilities P (s = S|O).632PPPPPPPPStateAction K R W GSY STEM 0 CS - -BOTHS CO(?)
0 - -BOTHU CO(?)
0 - -USER - - 0 CUFREEU - - CG(?)
0FREES - - CG(?)
0Table 1: Cost of system actions in each state (K: keep the floor, R: release the floor, W : wait without the floor, G:grab the floor, ?
: time spent in current state, -: action unavailable).4 Endpointing with the FSTTM4.1 Problem formalizationIn our FSTTM formalism, endpointing is the prob-lem of selecting between the Wait and the Grab ac-tions during a user utterance.
We make the simplify-ing assumption that, once a user utterance has beendetected, the only states with non-zero probabilityare USER and FREEU .
While this does not cap-ture cases where the system erroneously detects userspeech (because there is, for example, backgroundnoise), it represents a working first approximationof the problem.The main issue is to estimate the probabilityP (s = FREEU |Ot) (hereafter abbreviated asP (F |Ot), P (s = USER|Ot) being abbreviated asP (U |Ot)) where Ot represents all observable fea-tures at time t. Given that probability, the expectedcost of grabbing the floor is:C(G|Ot) = P (U |Ot) ?
CU + P (F |Ot) ?
0= (1?
P (F |Ot)) ?
CUSimilarly, the expected cost of waiting is:C(W |Ot) = P (F |Ot) ?
CG(?
)The system endpoints whenever the expected costof grabbing the floor becomes higher than that ofwaiting.We consider two separate cases for computingboth P (F |Ot) and CG(?
): when a pause has beendetected by the voice activity detector (VAD), andwhen no pause has been detected (yet).
In the fol-lowing sections, we provide details on the approxi-mations and estimation methods for these two cases.4.2 At pausesIf a pause has been detected by the VAD, we setthe cost of waiting in the FREEU state to be pro-portional to the duration of the pause so far.
If theuser has released the floor, the duration of the currentpause corresponds to the time spent in the FREEUstate, i.e.
?
in the cost matrix of Table 1.
In this case,we set CG(?)
= CpG ?
?
as a simple application ofrule 3 from section 3.2.We decompose the observations at time t,Ot, intoobservations available at the start of the pause (O),and observations made during the pause.
With onlyaudio information available, the only informationavailable during the pause is its duration so far, i.e.?
.
Specifically, we know that d ?
?
, where d is thetotal duration of the pause (with d = ?
at the endof a turn1).
Consequently, P (F |Ot) can be rewrittenusing Bayes rule asP (F |Ot) = P (d ?
?
|O,F ) ?
P (F |O)P (d ?
?
|O)= P (F |O)P (d ?
?
|O)where P (F |O) is the probability that the user re-leased the floor without any knowledge of the dura-tion of the pause, and P (d ?
?
|O) is the probabilitythat the pause will last at least ?
ms. We further de-compose P (d ?
?
|O) intoP (d ?
?
|O) = P (d ?
?, U |O) + P (d ?
?, F |O)1Note that this is an approximation since the user could startspeaking again after releasing the floor to reestablish the chan-nel (e.g.
by saying ?Hello??).
However, in the vast majority ofcases, the time after which the user resumes speaking is signifi-cantly longer than the time the system takes to endpoint.633= P (d ?
?
|O,U) ?
P (U |O) +P (d ?
?
|O,F ) ?
P (F |O)= P (d ?
?
|O,U) ?
(1?
P (F |O))+P (F |O)Consequently, P (F |Ot) is a function of P (F |O)and P (d ?
?
|O,U).
We estimate P (F |O) by step-wise logistic regression on a training set of pauseslabeled for finality (whether the pause is turn-final orturn-internal), using a wide range of features avail-able from various components of the dialog system.Based on the well established observation that pausedurations follow an exponential distribution (Jaffeand Feldstein, 1970; Lennes and Anttila, 2002; Rauxet al, 2008), P (d ?
?
|O,U) is a function of meanpause duration, computed on the training set.4.3 In speechIn some cases, it is not necessary to wait for the VADto detect a pause to know with high confidence thatthe user has released the floor.
For example, after asimple yes/no question, if the user says ?YES?, theyare very likely to have released the floor, regardlessof how long they remain silent afterwards.
In orderto exploit this fact and improve the responsivenessof the system in these highly predictable cases, weuse a separate model to compute the expected costsof waiting and grabbing the floor before any pause isdetected by the VAD (specifically, whenever the du-ration of the current pause is between 0 and 200 ms).In this case, we set the cost of waiting to a constantCsG.
We train a logistic regression model to estimateP (F |Ot) each time a new partial hypothesis is pro-duced by the ASR during a user utterance.
We usethe same set of features as above.5 Evaluation5.1 Corpus and FeaturesWe evaluated the effectiveness of the FSTTM onan actual deployed spoken dialog system.
The sys-tem provides bus schedule information for a mid-size North American city.
It is actually used by thegeneral public and therefore constantly operates andcollects data.
In order to train the various proba-bility estimation models and evaluate the approachin batch, we first collected a corpus of 586 dialogsbetween May 4, and May 14, 2008 (the ?2008 cor-pus?
).All of the features we used can be automaticallyextracted at runtime, and most of them were readilyavailable in the system.
They include dialog state in-formation, turn-taking features, such as whether thecurrent user utterance is a barge-in, and semanticinformation derived from the dialog state and par-tial recognition hypotheses provided by the speechrecognizer.
Dialog state is abstracted to three high-level states, which correspond to the type of systemprompt directly preceding the user utterance: Openquestion (?What can I do for you??
); Closed ques-tion (e.g.
?Where do you want to go??
); and Confir-mation (e.g.
?Going to the airport.
Is this correct??
).To capture lexical cues correlated with the end ofturns, we created a new feature called the boundaryLM score.
To compute it, we used previously col-lected data to train dialog-state-dependent statisticallanguage models to estimate the probability that thehypothesis is complete.
Boundary LM score is de-fined as the ratio of the log likelihood of the hypoth-esis being complete by that of the hypothesis beingincomplete.5.2 Estimating P (F |Ot)We trained two logistic regression models usingstepwise regression and 10-fold cross-validation forevaluation.
The first model, whose performanceis given in Table 2, estimates P (F |O) at pauses.The model is unable to improve classification accu-racy over the majority baseline for each state, how-ever, the statistically significant improvement in av-erage log likelihood indicates that the probabilityestimates are improved by using the features.
Themost informative feature in all three states was theboundary LM score introduced in section 5.1.
Otherselected features included the average number ofwords per user utterance so far and whether the cur-rent utterance is a barge-in (for the Open and Closedquestion states), as well as whether the partial hy-pothesis contained a confirmation marker such as?YES?
or ?SURE?
(for the Confirmation state).The second model performs the same regression,this time on all partial hypotheses received duringspeech segments.
As seen in the ?S?
columns in Ta-ble 2, classification error was significantly reducedand the gain in average log likelihood were larger634Open question Closed question ConfirmationP S P S P SMajority Baseline 38% 20% 25% 32% 12% 36%Classification Error 35% 17% 26% 22% 12% 17%Baseline log likelihood -0.66 -0.50 -0.56 -0.63 -0.36 -0.65Log likelihood -0.61 -0.40 -0.50 -0.49 -0.30 -0.40Table 2: Performance of state-specific logistic regression for estimating P (F |O) at pauses (P) and in speech (S).
(a) In-pause evaluation on the 2007 corpus.
(a) Anytime evaluation on the 2008 corpus.Figure 2: Batch evaluation of FSTTM endpointing.than at pauses, particularly for the ?Closed ques-tion?
and ?Confirmation?
states.
Again, boundaryLM score was the most informative feature.
Theduration of the pause at the end of the partial hy-pothesis (between 0 and 200 ms) also proved wellcorrelated with finality.5.3 Batch Evaluation of the FSTTMWe performed two batch evaluations of the FSTTM.The first one aims at comparing in-pause-FSTTMwith a fixed-threshold baseline as well as previousdata-driven endpointing methods proposed in Ferreret al (2003) (reimplemented by us) and Raux et al(2008).
This evaluation was done on the corpus usedin Raux et al (2008) (the ?2007 corpus?).
As seenin Figure 2 (a), the FSTTM outperforms all other ap-proaches (albeit only slightly compared to Ferrer etal.
), improving over the fixed threshold baseline byup to 29.5%.Second, we compared the anytime-FSTTM within-pause-FSTTM and a fixed-threshold baseline (forreference) on the more recent 2008 corpus (since the2007 corpus did not contain all necessary featuresfor anytime-FSTTM).
We set CpG = 1 and set CsGto either 0, leading to an endpointer that never end-points during speech (in-pause-FSTTM), or 1000(anytime-FSTTM).
In both cases, we vary CU tocompute the latency / cut-in rate trade-off curve.The results are shown in Figure 2 (b).
Anytime-FSTTM endpointing is consistently better than in-pause-FSTTM.
For example, at a cut-in rate of 5%,anytime-FSTTM yields latencies that are on average17% shorter than in-pause-FSTTM, and 40% shorterthan the baseline.
Additionally, we found that, inanytime-FSTTM, 30 to 40% of the turns are end-pointed before the pause is detected by the VAD.5.4 Live EvaluationTo confirm the results of the batch evaluation, weimplemented our FSTTM model in the deployedsystem a let it run for ten days using either FSTTMor a fixed threshold for endpointing, resulting ina corpus of 171 FSTTM and 148 control dialogs.For FSTTM, we set CpG = 1, CsG = 500, andCU = 5000.
In the batch evaluation, these valuescorrespond to a cut-in rate of 6.3% and an averagelatency of 320 ms. For the control condition, weset the fixed endpointing threshold to 555 ms, whichalso corresponded to about 6.3% cut-ins.Figure 3 shows the average latency and cut-in rate635(a) Latency (b) Cut-in ratesFigure 3: Live evaluation results.
All confidence intervals for latency (not shown on the figure) fall within +/?
4ms.for both conditions.
The FSTTM improves over thebaseline on all metrics, reducing average latency by193 ms (p < 0.05), cut-in rate by 1.5% (althoughthis result is not statistically significant).6 DiscussionBoth batch and live evaluation results confirm theeffectiveness of the FSTTM approach in improv-ing system responsiveness.
This approach signif-icantly reduced endpointing latency over previousapproaches.
Boundary LM score got the highestweight in the regression, indicating that in a domainsuch as telephone-based information access, lexicalcues are very informative for endpointing.
The factthat boundary LMs can be computed without any hu-man transcription effort (since they are trained onASR output) makes them all the more appealing.Essentially, the FSTTM provides a simple, unifiedmodel of turn-taking that lends itself to data-drivenoptimization.
While we discussed specific coststructures and probability estimation techniques, theframework?s flexibility opens it to other choices atmany levels.
By formalizing the overall turn-takingprocess in a probabilistic, decision-theoretic frame-work, the FSTTM extends and generalizes previousclassification-based approaches to endpointing suchas those proposed by Sato et al (2002), Ferrer etal.
(2003), Takeuchi et al (2004), and our previouswork (Raux et al, 2008).Possible extensions of the approach include data-driven cost matrices to relax some of the assump-tions introduced in section 3.2, as well as more com-plex state structures to handle, for example, multi-party conversations.Finally, we plan to investigate more principled ap-proaches, such as Partially Observable Markov De-cision Processes or Dynamic Bayesian Networks, tomodel the different sources of uncertainty (detectionerrors and inherent ambiguity) and track the statedistribution over time.
Raux (2009) provides moredetails on all aspects of the approach and its possi-ble extensions.7 ConclusionIn this paper, motivated by existing finite-state mod-els of turn-taking in dyadic conversations, we pro-pose the Finite-State Turn-Taking Machine, an ap-proach to turn-taking that relies on three core ele-ments: a non-deterministic finite-state machine thatcaptures the conversational floor; a cost matrix thatmodels the impact of different system actions in dif-ferent states; and a decision-theoretic action selec-tion mechanism.
We describe the application of theFSTTM to the key turn-taking phenomenon of end-of-turn detection.
Evaluation both offline and byapplying the FSTTM to a deployed spoken dialogsystem system showed that it performs significantlybetter than a fixed-threshold baseline.AcknowledgmentsThis work is supported by the US National ScienceFoundation under grant number 0208835.
Any opin-ions, findings, and conclusions or recommendationsexpressed in this material are those of the authorsand do not necessarily reflect the views of the NSF.We would like to thank Alan Black for his manycomments and advice.636ReferencesP.
T. Brady.
1969.
A model for generating on-off speechpatterns in two-way conversation.
The Bell SystemTechnical Journal, 48:2445?2472.J.
Cassell, T. Bickmore, L. Campbell, H. Vilhjalmsson,and H. Yan.
2001.
More than just a pretty face: con-versational protocols and the affordances of embodi-ment.
Knowledge-Based Systems, 14:55?64.S.
Duncan.
1972.
Some signals and rules for takingspeaking turns in conversations.
Journal of Person-ality and Social Psychology, 23(2):283?292.L.
Ferrer, E. Shriberg, and A. Stolcke.
2003.
A prosody-based approach to end-of-utterance detection that doesnot require speech recognition.
In ICASSP, HongKong.D.
Harel.
1987.
Statecharts: A visual formalism forcomplex systems.
Science of Computer Programming,8:231?274.J.
Jaffe and S. Feldstein.
1970.
Rhythms of Dialogue.Academic Press.F.
Kronild.
2006.
Turn taking for artificial conversationalagents.
In Cooperative Information Agents X, Edin-burgh, UK.Mietta Lennes and Hanna Anttila.
2002.
Prosodic fea-tures associated with the distribution of turns in finnishinformal dialogues.
In Petri Korhonen, editor, ThePhonetics Symposium 2002, volume Report 67, pages149?158.
Laboratory of Acoustics and Audio SignalProcessing, Helsinki University of Technology.B.
Orestro?m.
1983.
Turn-Taking in English Conversa-tion.
CWK Gleerup, Lund.R.
Porzel and M. Baudis.
2004.
The tao of chi:Towards effective human-computer interaction.
InHLT/NAACL 2004, Boston, MA.A.
Raux, , and M. Eskenazi.
2008.
Optimizing endpoint-ing thresholds using dialogue features in a spoken dia-logue system.
In Proc.
SIGdial 2008, Columbus, OH,USA.A.
Raux.
2009.
Flexible Turn-Taking for Spoken DialogSystems.
Ph.D. thesis, Carnegie Mellon University.H.
Sacks, E. A. Schegloff, and G. Jefferson.
1974.A simplest systematics for the organization of turn-taking for conversation.
Language, 50(4):696?735.R.
Sato, R. Higashinaka, M. Tamoto, M. Nakano, andK.
Aikawa.
2002.
Learning decision trees to deter-mine turn-taking by spoken dialogue systems.
In IC-SLP 2002, Denver, CO.E.A.
Schegloff.
2000.
Overlapping talk and the orga-nization of turn-taking for conversation.
Language inSociety, 29:1?63.M.
Takeuchi, N. Kitaoka, and S. Nakagawa.
2004.Timing detection for realtime dialog systems usingprosodic and linguistic information.
In Proc.
SpeechProsody 04, Nara, Japan.K.
R. Thorisson, 2002.
Multimodality in Language andSpeech Systems, chapter Natural Turn-Taking NeedsNo Manual: Computational Theory and Model, FromPerception to Action, pages 173?207.
Kluwer Aca-demic Publishers.D.
R. Traum and J. F. Allen.
1994.
Discourse obligationsin dialogue.
In Proc.
ACL-94, pages 1?8.N.
Ward, A. Rivera, K. Ward, and D. Novick.
2005.
Rootcauses of lost time and user stress in a simple dialogsystem.
In Interspeech 2005, Lisbon, Portugal.W.
Wesseling and R.J.J.H.
van Son.
2005.
Timing ofexperimentally elicited minimal responses as quanti-tative evidence for the use of intonation in projectingTRPs.
In Interspeech 2005, pages 3389?3392, Lisbon,Portugal.637
