Coling 2010: Poster Volume, pages 1471?1479,Beijing, August 2010Chart Pruning for Fast Lexicalised-Grammar ParsingYue Zhanga?
Byung-Gyu Ahn b?
Stephen Clarka?
Curt Van Wyk cJames R. Currand Laura RimellaComputer Laboratorya Computer Scienceb Computer Sciencec School of ITdCambridge Johns Hopkins Northwestern College Sydney{yue.zhang,stephen.clark}@cl.cam.ac.uka?
bahn@jhu.edu b?AbstractGiven the increasing need to process mas-sive amounts of textual data, efficiency ofNLP tools is becoming a pressing concern.Parsers based on lexicalised grammar for-malisms, such as TAG and CCG, can bemade more efficient using supertagging,which for CCG is so effective that everyderivation consistent with the supertaggeroutput can be stored in a packed chart.However, wide-coverage CCG parsers stillproduce a very large number of deriva-tions for typical newspaper or Wikipediasentences.
In this paper we investigatetwo forms of chart pruning, and develop anovel method for pruning complete cellsin a parse chart.
The result is a wide-coverage CCG parser that can process al-most 100 sentences per second, with lit-tle or no loss in accuracy over the baselinewith no pruning.1 IntroductionMany NLP tasks and applications require the pro-cessing of massive amounts of textual data.
Forexample, knowledge acquisition efforts can in-volve processing billions of words of text (Cur-ran, 2004).
Also, the increasing need to processlarge amounts of web data places an efficiencydemand on existing NLP tools.
TextRunner, forexample, is a system that performs open infor-mation extraction on the web (Lin et al, 2009).However, the text processing that is performed byTextRunner, in particular the parsing, is rudimen-tary: finite-state shallow parsing technology thatis now decades old.
TextRunner uses this technol-ogy largely for efficiency reasons.Many of the popular wide-coverage parsersavailable today operate at around one newspa-per sentence per second (Collins, 1999; Charniak,2000; Petrov and Klein, 2007).
There are de-pendency parsers that operate orders of magni-tude faster, by exploiting the fact that accuratedependency parsing can be achieved by using ashift-reduce linear-time process which makes asingle decision at each point in the parsing pro-cess (Nivre and Scholz, 2004).In this paper we focus on the Combinatory Cat-egorial Grammar (CCG) parser of Clark and Cur-ran (2007).
One advantage of the CCG parser isthat it is able to assign rich structural descriptionsto sentences, from a variety of representations,e.g.
CCG derivations, CCG dependency structures,grammatical relations (Carroll et al, 1998), andfirst-order logical forms (Bos et al, 2004).
Oneof the properties of the grammar formalism isthat it is lexicalised, associating CCG lexical cate-gories, or CCG supertags, with the words in a sen-tence (Steedman, 2000).
Clark and Curran (2004)adapt the technique of supertagging (Bangaloreand Joshi, 1999) to CCG, using a standard max-imum entropy tagger to assign small sets of su-pertags to each word.
The reduction in ambiguityresulting from the supertagging stage results in asurprisingly efficient parser, given the rich struc-tural output, operating at tens of newspaper sen-tences per second.In this paper we demonstrate that the CCGparser can be made more than twice as fast, withlittle or no loss in accuracy.
A noteworthy featureof the CCG parser is that, after the supertagging1471stage, the parser builds a complete packed chart,storing all sentences consistent with the assignedsupertags and the parser?s CCG combinatory rules,with no chart pruning whatsoever.
The use ofchart pruning techniques, typically some form ofbeam search, is essential for practical parsing us-ing Penn Treebank parsers (Collins, 1999; Petrovand Klein, 2007; Charniak and Johnson, 2005), aswell as practical parsers based on linguistic for-malisms, such as HPSG (Ninomiya et al, 2005)and LFG (Kaplan et al, 2004).
However, in theCCG case, the use of the supertagger means thatenough ambiguity has already been resolved to al-low the complete chart to be represented.Despite the effectiveness of the supertaggingstage, the number of derivations stored in a packedchart can still be enormous for typical newspa-per sentences.
Hence it is an obvious questionwhether chart pruning techniques can be prof-itably applied to the CCG parser.
Some previouswork (Djordjevic et al, 2007) has investigated thisquestion but with little success.In this paper we investigate two types of chartpruning: a standard beam search, similar to thatused in the Collins parser (Collins, 1999), and amore aggressive strategy in which complete cellsare pruned, following Roark and Hollingshead(2009).
Roark and Hollingshead use a finite-statetagger to decide which words in a sentence canend or begin constituents, from which whole cellsin the chart can be removed.
We develop a novelextension to this approach, in which a tagger istrained to infer the maximum length constituentthat can begin or end at a particular word.
Theselengths can then be used in a more agressive prun-ing strategy which we show to be significantlymore effective than the basic approach.Both beam search and cell pruning are highlyeffective, with the resulting CCG parser able toprocess almost 100 sentences per second usinga single CPU, for both newspaper and Wikipediadata, with little or no loss in accuracy.2 The CCG ParserThe parser is described in detail in Clark and Cur-ran (2007).
It is based on CCGbank, a CCG ver-sion of the Penn Treebank developed by Hocken-maier and Steedman (2007).The stages in the parsing pipeline are as fol-lows.
First, a POS tagger assigns a single POS tagto each word in a sentence.
Second, a CCG su-pertagger assigns lexical categories to the wordsin the sentence.
Third, the parsing stage combinesthe categories, using CCG?s combinatory rules,and builds a packed chart representation contain-ing all the derivations which can be built fromthe lexical categories.
Finally, the Viterbi algo-rithm finds the highest scoring derivation fromthe packed chart, using the normal-form log-linearmodel described in Clark and Curran (2007).Sometimes the parser is unable to build an anal-ysis which spans the whole sentence.
When thishappens the parser and supertagger interact us-ing the adaptive supertagging strategy describedin Clark and Curran (2004): the parser effectivelyasks the supertagger to provide more lexical cate-gories for each word.
This potentially continuesfor a number of iterations until the parser doescreate a spanning analysis, or else it gives up andmoves to the next sentence.The parser uses the CKY algorithm (Kasami,1965; Younger, 1967) described in Steedman(2000) to create a packed chart.
The CKY al-gorithm applies naturally to CCG since the gram-mar is binary.
It builds the chart bottom-up, start-ing with two-word constituents (assuming the su-pertagging phase has been completed), incremen-tally increasing the span until the whole sentenceis covered.
The chart is packed in the standardsense that any two equivalent constituents createdduring the parsing process are placed in the sameequivalence class, with pointers to the childrenused in the creation.
Equivalence is defined interms of the category and head of the constituent,to enable the Viterbi algorithm to efficiently findthe highest scoring derivation.1 A textbook treat-ment of CKY applied to statistical parsing is givenin Jurafsky and Martin (2000).3 Data and Evaluation MetricsWe performed efficiency and accuracy tests onnewspaper and Wikipedia data.
For the newspa-per data, we used the standard test sections from1Use of the Viterbi algorithm in this way requires the fea-tures in the parser model to be local to a single rule applica-tion; Clark and Curran (2007) has more discussion.1472(ncmod num hundred 1 Seven 0)(conj and 2 sixty-one 3)(conj and 2 hundred 1)(dobj in 6 total 7)(ncmod made 5 in 6)(aux made 5 were 4)(ncsubj made 5 and 2 obj)(passive made 5)Seven hundred and sixty-one were made intotal.Figure 1: Example Wikipedia test sentence anno-tated with grammatical relations.CCGbank.
Following Clark and Curran (2007) weused the CCG dependencies for accuracy evalua-tion, comparing those output by the parser withthe gold-standard dependencies in CCGbank.
Un-like Clark and Curran, we calculated recall scoresover all sentences, including those for which theparser did not find an analysis.
For the WSJ datathe parser fails on a small number of sentences(less than 1%), but the chart pruning has the effectof reducing this failure rate further, and we feltthat this should be factored into the calculation ofrecall and hence F-score.In order to test the parser on Wikipedia text,we created two test sets.
The first, Wiki 300, fortesting accuracy, consists of 300 sentences man-ually annotated with grammatical relations (GRs)in the style of Briscoe and Carroll (2006).
Anexample sentence is given in Figure 1.
The datawas created by manually correcting the output ofthe parser on these sentences, with the annotationbeing performed by Clark and Rimell, includingchecks on a subset of these cases to ensure con-sistency across the two annotators.
For the ac-curacy evaluation, we calculated precision, recalland balanced F-measure over the GRs in the stan-dard way.For testing speed on Wikipedia, we used a cor-pus of 2500 randomly chosen sentences, Wiki2500.
For all speed tests we measured the num-ber of sentences per second, using a single CPUand standard hardware.4 Beam SearchThe beam search approach used in our exper-iments prunes all constituents in a cell havingscores below a multiple (?)
of the score of the?
Speed Gain F-score GainBaseline 43.0 85.550.001 48.6 13% 85.82 0.270.002 54.2 26% 85.88 0.330.005 59.0 37% 85.73 0.180.01 66.7 55% 85.53 -0.02Table 1: Accuracy and speed results using differ-ent beam values ?.?
Speed Gain F-score GainBaseline 43.0 85.5510 60.1 39% 85.55 0.0020 70.6 64% 85.66 0.1130 72.3 68% 85.65 0.1040 76.4 77% 85.63 0.0850 76.7 78% 85.62 0.0760 74.5 73% 85.71 0.1680 68.4 59% 85.71 0.16100 62.0 44% 85.73 0.18None 59.0 37% 85.73 0.18Table 2: Accuracy and speed results for differentvalues of ?
where ?
= 0.005.highest scoring constituent for that cell.2 Thescores for a constituent are calculated using thesame model used to find the highest scoringderivation.
We consider two scores: the Viterbiscore, which is the score of the highest scoringsub-derivation for that constituent; and the insidescore, which is the sum over all sub-derviationsfor that constituent.
We investigated the follow-ing: the trade-off between the aggressiveness ofthe beam search and accuracy; the comparison be-tween the Viterbi and inside scores; and whetherapplying the beam to only certain cells in the chartcan improve performance.Table 1 shows results on Section 00 of CCG-bank, using the Viterbi score to prune.
As ex-pected, the parsing speed increases as the valueof ?
increases, since more constituents are prunedwith a higher ?
value.
The pruning is effective,with a ?
value of 0.01 giving a 55% speed increasewith neglible loss in accuracy.32One restriction we apply in practice is that only con-stituents resulting from the application of a CCG binary rule,rather than a unary rule, are pruned.3The small accuracy increase for some ?
values could beattributable to two factors: one, the parser may select a lower1473Speed F-scoreDataset Baseline Beam Gain Baseline Beam GainWSJ 00 43.0 76.4 77% 85.55 85.63 0.08WSJ 02-21 53.4 99.4 86% 93.61 93.27 -0.34WSJ 23 55.0 107.0 94% 87.12 86.90 -0.22Wiki 300 35.5 80.3 126% 84.23 85.06 0.83Wiki 2500 47.6 90.3 89%Table 4: Beam search results on WSJ 00, 02-21, 23 and Wikipedia texts with ?
= 0.005 and ?
= 40.?
?
Speed F-scoreBaseline 24.7 85.55inside scores0.01 37.7 85.520.001 25.3 85.790.005 10 33.4 85.540.005 20 39.5 85.640.005 50 42.9 85.58Viterbi scores0.01 38.1 85.530.001 28.2 85.820.005 10 33.6 85.550.005 20 39.4 85.660.005 50 43.1 85.62Table 3: Comparison between using Viterbi scoresand inside scores as beam scores.We also studied the effect of the beam searchat different levels of the chart.
We applied a selec-tive beam in which pruning is only applied to con-stituents of length less than or equal to a threshold?.
For example, if ?
= 20, pruning is applied onlyto constituents spanning 20 words or less.
The re-sults are shown in Table 2.
The selective beamis also highly effective, showing speed gains overthe baseline (which does not use a beam) with noloss in F-score.
For a ?
value of 50 the speed in-crease is 78% with no loss in accuracy.Note that for ?
greater than 50, the speed re-duces.
We believe that this is due to the costof calculating the beam scores and the reducedeffectiveness of pruning for cells with longerspans (since pruning shorter constituents early inthe chart-parsing process prevents the creation ofmany larger, low-scoring constituents later).Table 3 shows the comparison between the in-scoring but more accurate derivation; and two, a possible in-crease in recall, discussed in Section 3, can lead to a higherF-score.side and Viterbi scores.
The results are similar,with Viterbi marginally outperforming the insidescore in most cases.
The interesting result fromthese experiments is that the summing used in cal-culating the inside score does not improve perfor-mance over the max operator used by Viterbi.Table 4 gives results on Wikipedia text, com-pared with a number of sections from CCGbank.
(Sections 02-21 provide the training data for theparser which explains the high accuracy resultson these sections.)
Despite the fact that the prun-ing model is derived from CCGbank and based onWSJ text, the speed improvements for Wikipediawere even greater than for WSJ text, with param-eters ?
= 0.005 and ?
= 40 leading to almost adoubling of speed on the Wiki 2500 set, with theparser operating at 90 sentences per second.5 Cell PruningWhole cells can be pruned from the chart by tag-ging words in a sentence.
Roark and Hollingshead(2009) used a binary tagging approach to prune aCFG CKY chart, where tags are assigned to inputwords to indicate whether they can be the start orend of multiple-word constituents.
We adapt theirmethod to CCG chart pruning.
We also show thelimitation of binary tagging, and propose a noveltagging method which leads to increased speedsand accuracies over the binary taggers.5.1 Binary taggingFollowing Roark and Hollingshead (2009), we as-sign the binary begin and end tags separately us-ing two independent taggers.
Given the input?We like playing cards together?, the pruning ef-fects of each type of tag on the CKY chart areshown in Figure 2.
In this chart, rows repre-1474XWe like playing cards together1 2 3 4 5124531 1 1 0 0X XXWe like playing cards together1 2 3 4 5124530 0 0 1 1Figure 2: The pruning effect of begin (top) andend (bottom) tags; X indicates a removed cell.sent consituent sizes and columns represent initialwords of constituents.
No cell in the first row ofthe chart is pruned, since these cells correspondto single words, and are necessary for finding aparse.
The begin tag for the input word ?cards?
is0, which means that it cannot begin a multi-wordconstituent.
Therefore, no cell in column 4 cancontain any constituent.
The pruning effect of abinary begin tag is to cross out a column of chartcells (ignoring the first row) when the tag valueis zero.
Similarly, the end tag of the word ?play-ing?
is 0, which means that it cannot be the endof a multi-word constituent.
Consequently cell (2,2), which contains constituents for ?like playing?,and cell (1, 3), which contains constituents for?We like playing?, must be empty.
The pruningeffect of a binary end tag is to cross out a diagonalof cells (ignoring the first row) when the tag valueis zero.We use a maximum entropy trigram tagger(Ratnaparkhi, 1996; Curran and Clark, 2003) toModel Speed F-scorebaseline 25.10 84.89begin only 27.49 84.71end only 30.33 84.56both 33.90 84.60oracle 33.60 85.67Table 5: Accuracy and speed results for the binarytaggers on Section 00 of CCGbank.assign the begin and end tags.
Features based onthe words and POS in a 5-word window, plus thetwo previously assigned tags, are extracted fromthe trigram ending with the current tag and thefive-word window with the current word in themiddle.
In our development experiments, both thebegin and the end taggers gave a per-word accu-racy of around 96%, similar to the accuracy re-ported in Roark and Hollingshead (2009).Table 5 shows accuracy and speed results forthe binary taggers.4 Using begin or end tags alone,the parser achieved speed increases with a smallloss in accuracy.
When both begin and end tagsare applied, the parser achieved further speed in-creases, with no loss in accuracy compared to theend tag alone.
Row ?oracle?
shows what happensusing the perfect begin and end taggers, by usinggold-standard constituent information from CCG-bank.
The F-score is higher, since the parser isbeing guided away from incorrect derivations, al-though the speed is no higher than when using au-tomatically assigned tags.5.2 Level taggingA binary tag cannot take effect when there is anychart cell in the corresponding column or diagonalthat contains constituents.
For example, the begintag for the word ?card?
in Figure 3 cannot be 0 be-cause ?card?
begins a two-word constituent ?cardgames?.
Hence none of the cells in the column canbe pruned using the binary begin tag, even thoughall the cells from the third row above are empty.We propose what we call a level tagging approachto address this problem.Instead of taking a binary value that indicates4The baseline differs slightly to the previous section be-cause gold-standard POS tags were used for the beam-searchexperiments.14751 2 3 4 512453Playing card games is funFigure 3: The limitation of binary begin tags.whether a whole column or diagonal of cells canbe pruned, a level tag (begin or end) takes an in-teger value which indicates the row from whicha column or diagonal can be pruned in the up-ward direction.
For example, a level begin tagwith value 2 allows the column of chart cells forthe word ?card?
in Figure 3 to be pruned from thethird row upwards.
A level tag (begin or end) withvalue 1 prunes the corresponding row or diago-nal from the second row upwards; it has the samepruning effect as a binary tag with value 0.
Forconvenience, value 0 for a level tag means that thecorresponding word can be the beginning or endof any constituent, which is the same as a binarytag value 1.A comparison of the pruning effect of binaryand level tags for the sentence ?Playing cardgames is fun?
is shown in Figure 4.
With a levelbegin tag, more cells can be pruned from the col-umn for ?card?.
Therefore, level tags are poten-tially more powerful for pruning.We now need a method for assigning level tagsto words in a sentence.
However, we cannotachieve this with a straighforward classifier sincelevel tags are related; for example, a level tag (be-gin or end) with value 2 implies level tags withvalues 3 and above.
We develop a novel methodfor calculating the probability of a level tag fora particular word.
Our mechanism for calculat-ing these probabilities uses what we call maxspantags, which can be assigned using a maximum en-tropy tagger.Maxspan tags take the same values as level tags.However, the meanings of maxspan tags and levelXXXX1 2 3 4 512453Playing card games is funXXXXPlaying card games is fun1 2 3 4 512453Figure 4: The pruning effect of binary (top) andlevel (bottom) tags.tags are different.
While a level tag indicates therow from which a column or diagonal of cells ispruned, a maxspan tag represents the size of thelargest constituent a word begins or ends.
For ex-ample, in Figure 3, the level end tag for the word?games?
has value 3, since the largest constituentthis words ends spans ?playing card games?.We use the standard maximum entropy trigramtagger for maxspan tagging, where features areextracted from tag trigrams and surrounding five-word windows, as for the binary taggers.
Parsetrees can be turned directly into training data fora maxspan tagger.
Since the level tag set is fi-nite, we a require a maximum value N that a leveltag can take.
We experimented with N = 2 andN = 4, which reflects the limited range of thefeatures used by the taggers.5During decoding, the maxspan tagger uses theforward-backward algorithm to compute the prob-ability of maxspan tag values for each word in the5Higher values of N did not lead to improvements duringdevelopment experiments.1476Model Speed F-scorebaseline 25.10 84.89binary 33.90 84.60binary oracle 33.60 85.67level N = 2 32.79 84.92level N = 4 34.91 84.95level N = 4 oracle 47.45 86.49Table 6: Accuracy and speed results for the leveltaggers on Section 00 of CCGbank.input.
Then for each word, the probability of itslevel tag tl having value x is the sum of the prob-abilities of its maxspan tm tag having values 1..x:P (tl = x) =x?i=1P (tm = i)Maxspan tag values i from 1 to x represent dis-joint events in which the largest constituent thatthe corresponding word begins or ends has size i.Summing the probabilities of these disjoint eventsgives the probability that the largest constituentthe word begins or ends has a size between 1 andx, inclusive.
That is also the probability that allthe constituents the word begins or ends are in therange of cells from rows 1 to row x in the corre-sponding column or diagonal.
And therefore thatis also the probability that the chart cells aboverow x in the corresponding column or diagonaldo not contain any constituents, which means thatthe column and diagonal can be pruned from rowx upward.
Therefore, it is also the probability of alevel tag with value x.The probability of a level tag having value xincreases as x increases from 1 to N .
We set aprobability threshold Q and choose the smallestlevel tag value x with probability P (tl = x) ?
Qas the level tag for a word.
If P (tl = N) < Q, weset the level tag to 0 and do not prune the columnor diagonal.
The threshold value determines a bal-ance between pruning power and accuracy, with ahigher value pruning more cells but increasing therisk of incorrectly pruning a cell.
During devel-opment we arrived at a threshold value of 0.8 asproviding a suitable compromise between pruningpower and accuracy.Table 6 shows accuracy and speed results forthe level tagger, using a threshold value of 0.8.Model Speed F-scorebaseline 36.64 84.23binary gold 49.59 84.36binary self 40K 48.79 83.64binary self 200K 51.51 83.71binary self 1M 47.78 83.75level gold 58.23 84.12level self 40K 54.76 83.83level self 200K 48.57 83.39level self 1M 52.54 83.71Table 7: Accuracy tests on Wiki 300 comparinggold training (gold) with self training (self) fordifferent sizes of parser output for self-training.We compare the effect of the binary tagger andlevel taggers with N = 2 and N = 4.
The accu-racies with the level taggers are higher than thosewith the binary tagger; they are also higher thanthe baseline parsing accuracy.
The parser achievesthe highest speed and accuracy when pruned withthe N = 4 level tagger.
Comparing the oraclescores, the level taggers lead to higher speeds thanthe binary tagger, reflecting the increased pruningpower of the level taggers compared with the bi-nary taggers.5.2.1 Final experiments using gold trainingand self trainingIn this section we report our final tests usingWikipedia data.
We used two methods to derivetraining data for the taggers.
The first is the stan-dard method, which is to transform gold-standardparse trees into begin and end tag sequences.
Thismethod is the method that we used for all previ-ous experiments, and we call it ?gold training?.In addition to gold training, we also investigatean alternative method, which is to obtain trainingdata for the taggers from the output of the parseritself, in a form of self-training (McClosky et al,2006).
The intuition is that the tagger will learnwhat constituents a trained parser will eventuallychoose, and as long as the constituents favouredby the parsing model are not pruned, no reductionin accuracy can occur.
There is the potential foran increase in speed, however, due to the pruningeffect.For gold training, we used sections 02-21 of1477Model Speedbaseline 47.6binary gold 80.8binary 40K 75.5binary 200K 77.4binary 1M 78.6level gold 93.7level 40K 92.8level 200K 92.5level 1M 96.6Table 8: Speed tests with gold and self-training onWiki 2500.CCGBank (which consists of about 40K trainingsentences) to derive training data.
For self train-ing, we trained the parser on sections 02-21 ofCCGBank, and used the parser to parse 40 thou-sand, 200 thousand and 1 million sentences fromWikipedia, respectively.
Then we derive three setsof self training data from the three sets of parseroutputs.
We then used our Wiki 300 set to test theaccuracy, and the Wiki 2500 set to test the speedof the parser.The results are shown in Tables 7 and 8, whereeach row represents a training data set.
Rows ?bi-nary gold?
and ?level gold?
represent binary andlevel taggers trained using gold training.
Rows?binary self X?
and ?level self X?
represent bi-nary and level taggers trained using self training,with the size of the training data being X sen-tences.It can be seen from the Tables that the accuracyloss with self-trained binary or level taggers wasnot large (in the worst case, the accuracy droppedfrom 84.23% to 83.39%), while the speed wassignificantly improved.
Using binary taggers, thelargest speed improvement was from 47.6 sen-tences per second to 80.8 sentences per second(a 69.7% relative increase).
Using level taggers,the largest speed improvement was from 47.6 sen-tences per second to 96.6 sentences per second (a103% relative increase).A potential advantage of self-training is theavailability of large amounts of training data.However, our results are somewhat negative inthis regard, in that we find training the tagger onmore than 40,000 parsed sentences (the size ofCCGbank) did not improve the self-training re-sults.
We did see the usual speed improvementsfrom using the self-trained taggers, however, overthe baseline parser with no pruning.6 ConclusionUsing our novel method of level tagging for prun-ing complete cells in a CKY chart, the CCG parserwas able to process almost 100 Wikipedia sen-tences per second, using both CCGbank and theoutput of the parser to train the taggers, with littleor no loss in accuracy.
This was a 103% increaseover the baseline with no pruning.We also demonstrated that standard beamsearch is highly effective in increasing the speedof the CCG parser, despite the fact that the su-pertagger has already had a significant pruningeffect.
In future work we plan to investigate thegains that can be achieved from combining thetwo pruning methods, as well as other pruningmethods such as the self-training technique de-scribed in Kummerfeld et al (2010) which re-duces the number of lexical categories assignedby the supertagger (leading to a speed increase).Since these methods are largely orthogonal, weexpect to achieve further gains, leading to a re-markably fast wide-coverage parser outputtingcomplex linguistic representations.AcknowledgementsThis work was largely carried out at the JohnsHopkins University Summer Workshop and (par-tially) supported by National Science Founda-tion Grant Number IIS-0833652.
Yue Zhang andStephen Clark are supported by the EuropeanUnion Seventh Framework Programme (FP7-ICT-2009-4) under grant agreement no.
247762.ReferencesBangalore, Srinivas and Aravind Joshi.
1999.
Su-pertagging: An approach to almost parsing.
Com-putational Linguistics, 25(2):237?265.Bos, Johan, Stephen Clark, Mark Steedman, James R.Curran, and Julia Hockenmaier.
2004.
Wide-coverage semantic representations from a CCGparser.
In Proceedings of COLING-04, pages 1240?1246, Geneva, Switzerland.1478Briscoe, Ted and John Carroll.
2006.
Evaluatingthe accuracy of an unlexicalized statistical parser onthe PARC DepBank.
In Proceedings of the PosterSession of COLING/ACL-06, pages 41?48, Sydney,Australia.Carroll, John, Ted Briscoe, and Antonio Sanfilippo.1998.
Parser evaluation: a survey and a new pro-posal.
In Proceedings of the 1st LREC Conference,pages 447?454, Granada, Spain.Charniak, Eugene and Mark Johnson.
2005.
Coarse-to-fine N-best parsing and maxent discriminativereranking.
In Proceedings of the 43rd Meeting ofthe ACL, pages 173?180, Michigan, Ann Arbor.Charniak, Eugene.
2000.
A maximum-entropy-inspired parser.
In Proceedings of the 1st Meetingof the NAACL, pages 132?139, Seattle, WA.Clark, Stephen and James R. Curran.
2004.
The im-portance of supertagging for wide-coverage CCGparsing.
In Proceedings of COLING-04, pages 282?288, Geneva, Switzerland.Clark, Stephen and James R. Curran.
2007.
Wide-coverage efficient statistical parsing with CCGand log-linear models.
Computational Linguistics,33(4):493?552.Collins, Michael.
1999.
Head-Driven Statistical Mod-els for Natural Language Parsing.
Ph.D. thesis,University of Pennsylvania.Curran, James R. and Stephen Clark.
2003.
Inves-tigating GIS and smoothing for maximum entropytaggers.
In Proceedings of the 10th Meeting of theEACL, pages 91?98, Budapest, Hungary.Curran, James R. 2004.
From Distributional to Se-mantic Similarity.
Ph.D. thesis, University of Edin-burgh.Djordjevic, Bojan, James R. Curran, and StephenClark.
2007.
Improving the efficiency of a wide-coverage CCG parser.
In Proceedings of IWPT-07,pages 39?47, Prague, Czech Republic.Hockenmaier, Julia and Mark Steedman.
2007.
CCG-bank: a corpus of CCG derivations and dependencystructures extracted from the Penn Treebank.
Com-putational Linguistics, 33(3):355?396.Jurafsky, Daniel and James H. Martin.
2000.
Speechand Language Processing: An Introduction to Nat-ural Language Processing, Computational Linguis-tics and Speech Recognition.
Prentice Hall, NewJersey.Kaplan, Ron, Stefan Riezler, Tracy H. King, JohnT.
Maxwell III, Alexander Vasserman, and RichardCrouch.
2004.
Speed and accuracy in shallow anddeep stochastic parsing.
In Proceedings of HLT-NAACL?04, Boston, MA.Kummerfeld, Jonathan K., Jessika Roesner, TimDawborn, James Haggerty, James R. Curran, andStephen Clark.
2010.
Faster parsing by supertag-ger adaptation.
In Proceedings of ACL-10, Uppsala,Sweden.Lin, Thomas, Oren Etzioni, and James Fogarty.
2009.Identifying interesting assertions from the web.
InProceedings of the 18th Conference on Informationand Knowledge Management (CIKM 2009), HongKong.McClosky, David, Eugene Charniak, and Mark John-son.
2006.
Effective self-training for parsing.
InProceedings of NAACL-06, pages 152?159, Brook-lyn, NY.Ninomiya, Takashi, Yoshimasa Tsuruoka, YusukeMiyao, and Jun?ichi Tsujii.
2005.
Efficacy of beamthresholding, unification filtering and hybrid pars-ing in probabilistic HPSG parsing.
In Proceedingsof IWPT-05, pages 103?114, Vancouver, Canada.Nivre, J. and M. Scholz.
2004.
Deterministic depen-dency parsing of English text.
In Proceedings ofCOLING-04, pages 64?70, Geneva, Switzerland.Petrov, Slav and Dan Klein.
2007.
Improved infer-ence for unlexicalized parsing.
In Proceedings ofthe HLT/NAACL conference, Rochester, NY.Ratnaparkhi, Adwait.
1996.
A maximum entropymodel for part-of-speech tagging.
In Proceedingsof EMNLP-96, pages 133?142, Somerset, New Jer-sey.Roark, Brian and Kristy Hollingshead.
2009.
Lin-ear complexity context-free parsing pipelines viachart constraints.
In Proceedings of HLT/NAACL-09, pages 647?655, Boulder, Colorado.Steedman, Mark.
2000.
The Syntactic Process.
TheMIT Press, Cambridge, MA.1479
