Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 612?621,Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational LinguisticsTrustRank: Inducing Trust in Automatic Translations via RankingRadu SoricutLanguage Weaver, Inc.6060 Center Drive, Suite 150Los Angeles, CA 90045rsoricut@languageweaver.comAbdessamad EchihabiLanguage Weaver, Inc.6060 Center Drive, Suite 150Los Angeles, CA 90045echihabi@languageweaver.comAbstractThe adoption of Machine Translation tech-nology for commercial applications ishampered by the lack of trust associatedwith machine-translated output.
In this pa-per, we describe TrustRank, an MT sys-tem enhanced with a capability to rank thequality of translation outputs from good tobad.
This enables the user to set a qualitythreshold, granting the user control overthe quality of the translations.We quantify the gains we obtain in trans-lation quality, and show that our solutionworks on a wide variety of domains andlanguage pairs.1 IntroductionThe accuracy of machine translation (MT) soft-ware has steadily increased over the last 20 yearsto achieve levels at which large-scale commercialapplications of the technology have become feasi-ble.
However, widespread adoption of MT tech-nology remains hampered by the lack of trust as-sociated with machine-translated output.
This lackof trust is a normal reaction to the erratic trans-lation quality delivered by current state-of-the-art MT systems.
Unfortunately, the lack of pre-dictable quality discourages the adoption of large-scale automatic translation solutions.Consider the case of a commercial enterprisethat hosts reviews written by travellers on its website.
These reviews contain useful informationabout hotels, restaurants, attractions, etc.
Thereis a large and continuous stream of reviews postedon this site, and the large majority is written in En-glish.
In addition, there is a large set of potentialcustomers who would prefer to have these reviewsavailable in their (non-English) native languages.As such, this enterprise presents the perfect oppor-tunity for the deployment of a large-volume MTsolution.
However, travel reviews present specificchallenges: the reviews tend to have poor spelling,loose grammar, and broad topics of discussion.The result is unpredictable levels of MT quality.This is undesirable for the commercial enterprise,who is not content to simply reach a broad audi-ence, but also wants to deliver a high-quality prod-uct to that audience.We propose the following solution.
We developTrustRank, an MT system enhanced with a ca-pability to rank the quality of translation outputsfrom good to bad.
This enables the user to set aquality threshold, granting the user control overthe quality of the translations that it employs inits product.
With this enhancement, MT adop-tion stops being a binary should-we-or-shouldn?t-we question.
Rather, each user can make a per-sonal trade-off between the scope and the qualityof their product.2 Related WorkWork on automatic MT evaluation started with theidea of comparing automatic translations againsthuman-produced references.
Such comparisonsare done either at lexical level (Papineni et al,2002; Doddington, 2002), or at linguistically-richer levels using paraphrases (Zhou et al, 2006;Kauchak and Barzilay, 2006), WordNet (Lavie andAgarwal, 2007), or syntax (Liu and Gildea, 2005;Owczarzak et al, 2007; Yang et al, 2008; Amigo?et al, 2009).
In contrast, we are interested in per-formingMT quality assessments on documents forwhich reference translations are not available.Reference-free approaches to automatic MTquality assessment, based on Machine Learningtechniques such as classification (Kulesza andShieber, 2004), regression (Albrecht and Hwa,2007), and ranking (Ye et al, 2007; Duh, 2008),have a different focus compared to ours.
Their ap-proach, which uses a test set that is held constantand against which various MT systems are mea-612sured, focuses on evaluating system performance.Similar proposals exist outside the MT field, forinstance in syntactic parsing (Ravi et al, 2008).
Inthis case, the authors focus on estimating perfor-mance over entire test sets, which in turn is usedfor evaluating system performance.
In contrast,we focus on evaluating the quality of the trans-lations themselves, while the MT system is keptconstant.A considerable amount of work has been donein the related area of confidence estimation forMT, for which Blatz et al (2004) provide a goodoverview.
The goal of this work is to identify smallunits of translated material (words and phrases)for which one can be confident in the quality ofthe translation.
Related to this goal, and closest toour proposal, is the work of Gamon et al (2005)and Specia et al (2009).
They describe Ma-chine Learning approaches (classification and re-gression, respectively) aimed at predicting whichsentences are likely to be well/poorly translated.Our work, however, departs from all these worksin several important aspects.First, we want to make the quality predic-tions at document-level, as opposed to sentence-level (Gamon et al, 2005; Specia et al, 2009), orword/phrase-level (Blatz et al, 2004; Ueffing andNey, 2005).
Document-level granularity is a re-quirement for large-scale commercial applicationsthat use fully-automated translation solutions.
Forthese applications, the need to make the distinctionbetween ?good translation?
and ?poor translation?must be done at document level.
Otherwise, it isnot actionable.
In contrast, quality-prediction orconfidence estimation at sentence- or word-levelfits best a scenario in which automated translationis only a part of a larger pipeline.
Such pipelinesusually involve human post-editing, and are usefulfor translation productivity (Lagarda et al, 2009).Such solutions, however, suffer from the inherentvolume bottleneck associated with human involve-ment.
Our fully-automated solution targets largevolume translation needs, on the order of 10,000documents/day or more.Second, we use automatically generated train-ing labels for the supervised Machine Learningapproach.
In the experiments presented in this pa-per, we use BLEU scores (Papineni et al, 2002)as training labels.
However, they can be substi-tuted with any of the proposedMTmetrics that usehuman-produced references to automatically as-sess translation quality (Doddington, 2002; Lavieand Agarwal, 2007).
In a similar manner, thework of (Specia et al, 2009) uses NIST scores,and the work of (Ravi et al, 2008) uses PARSE-VAL scores.
The main advantage of this approachis that we can generate quickly and cheaply asmany learning examples as needed.
Additionally,we can customize the prediction models on a largevariety of genres and domains, and quickly scaleto multiple language pairs.
In contrast, solutionsthat require training labels produced manually byhumans (Gamon et al, 2005; Albrecht and Hwa,2007) have difficulties producing prediction mod-els fast enough, trained on enough data, and cus-tomized for specific domains.Third, the main metric we use to assess the per-formance of our solution is targeted directly atmeasuring translation quality gains.
We are inter-ested in the extrinsic evaluation of the quantitativeimpact of the TrustRank solution, rather than inthe intrinsic evaluation of prediction errors (Raviet al, 2008; Specia et al, 2009).3 Experimental Framework3.1 DomainsWe are interested in measuring the impact ofTrustRank on a variety of genres, domains, andlanguage pairs.
Therefore, we set up the exper-imental framework accordingly.
We use threeproprietary data sets, taken from the domains ofTravel (consumer reviews), Consumer Electron-ics (customer support for computers, data storage,printers, etc.
), and HighTech (customer support forhigh-tech components).
All these data sets comein a variety of European and Asian language pairs.We also use the publicly available data set usedin the WMT09 task (Koehn and Haddow, 2009)(a combination of European parliament and newsdata).
Information regarding the sizes of these datasets is provided in Table 2.3.2 MetricsWe first present the experimental framework de-signed to answer the main question we want toaddress: can we automatically produce a rankingfor document translations (for which no human-produced references are available), such that thetranslation quality of the documents at the top ofthis ranking is higher than the average translationquality?
To this end, we use several metrics thatcan gauge how well we answer this question.613The first metric is Ranking Accuracy (rAcc),see (Gunawardana and Shani, 2009).
We are inter-ested in ranking N documents and assigning theminto n quantiles.
The formula is:rAcc[n] = Avgni=1TPiNn=1N?
?ni=1TPiwhere TPi (True-Positivei) is the number ofcorrectly-assigned documents in quantile i. Intu-itively, this formula is an average of the ratio ofdocuments correctly assigned in each quantile.The rAcc metric provides easy to understandlowerbounds and upperbounds.
For example, witha method that assigns random ranks, when using 4quantiles, the accuracy is 25% in any of the quan-tiles, hence an rAcc of 25%.
With an oracle-basedranking, the accuracy is 100% in any of the quan-tiles, hence an rAcc of 100%.
Therefore, the per-formance of any decent ranking method, when us-ing 4 quantiles, can be expected to fall somewherebetween these bounds.The second and main metric is the volume-weighted BLEU gain (vBLEU?)
metric.
It mea-sures the average BLEU gain when trading-offvolume for accuracy on a predefined scale.
Thegeneral formula, for n quantiles, isvBLEU?
[n] = ?n?1i=1 wi ?
(BLEU1...i ?
BLEU)with wi =in?n?1j=1jn= i?n?1j=1 j= 2in(n?1)where BLEU1...i is the BLEU score of the firsti quantiles, and BLEU is the score over all thequantiles.
Intuitively, this formula provides avolume-weighted average of the BLEU gain ob-tained while varying the threshold of acceptancefrom 1 to n-1.
(A threshold of acceptance set tothe n-th quantile means accepting all the transla-tions and therefore ignore the rankings, so we donot include it in the average.)
Without rankings(or with random ranks), the expected vBLEU?
[n]is zero, as the value BLEU1...i is expected to bethe same as the overall BLEU for any i.
With ora-cle ranking, the expected vBLEU?
[n] is a positivenumber representative of the upperbound on thequality of the translations that pass an acceptancethreshold.
We report the vBLEU?
[n] values assigned numbers, both within a domain and whencomputed as an average across domains.The choice regarding the number of quantilesis closely related to the choice of setting an ac-ceptance quality threshold.
Because we want thesolution to stay unchanged while the acceptancequality threshold can vary, we cannot treat this asa classification problem.
Instead, we need to pro-vide a complete ranking over an input set of doc-uments.
As already mentioned, TrustRank uses aregression method that is trained on BLEU scoresas training labels.
The regression functions arethen used to predict a BLEU-like number for eachdocument in the input set.
The rankings are de-rived trivially from the predicted BLEU numbers,by simply sorting from highest to lowest.
Ref-erence ranking is obtained similarly, using actualBLEU scores.Although we are mainly interested in the rank-ing problem here, it helps to look at the error pro-duced by the regression models to arrive at a morecomplete picture.
Besides the two metrics forranking described above, we use the well-knownregression metrics MAE (mean absolute error) andTE (test-level error):MAE =1N?
?Nk=1|predBLEUk ?
BLEUk|TE = predBLEU ?
BLEUwhere BLEUk is the BLEU score for documentk, predBLEUk is the predicted BLEU value, andpredBLEU is a weighted average of the predicteddocument-level BLEU numbers over the entire setof N documents.3.3 Experimental conditionsThe MT system used by TrustRank (TrustRank-MT) is a statistical phrase-based MT system sim-ilar to (Och and Ney, 2004).
As a reference pointregarding the performance of this system, we usethe official WMT09 parallel data, monolingualdata, and development tuning set (news-dev2009a)to train baseline TrustRank-MT systems for eachof the ten WMT09 language pairs.
Our systemproduces translations that are competitive withstate-of-the-art systems.
We show our baseline-system BLEU scores on the official developmenttest set (news-dev2009b) for the WMT09 task inTable 1, along with the BLEU scores reported forthe baseline Moses system (Koehn and Haddow,2009).For each of the domains we consider, we par-tition the data sets as follows.
We first set aside3000 documents, which we call the Regressionset 1.
The remaining data is called the training MT1For parallel data for which we do not have document614From Eng Fra Spa Ger Cze HunMoses 17.8 22.4 13.5 11.4 6.5TrustRank-MT 21.3 22.8 14.3 9.1 8.5Into Eng Fra Spa Ger Cze HunMoses 21.2 22.5 16.6 16.9 8.8TrustRank-MT 22.4 23.8 19.8 13.3 10.4Table 1: BLEU scores (uncased) for theTrustRank-MT system compared to Moses(WMT09 data).set, on which the MT system is trained.
From theRegression set, we set aside 1000 parallel docu-ments to be used as a blind test set (called Regres-sion Test) for our experiments.
An additional setof 1000 parallel documents is used as a develop-ment set, and the rest of 1000 parallel documentsis used as the regression-model training set.We have also performed learning-curve exper-iments using between 100 and 2000 documentsfor regression-model training.
We do not go intothe details of these experiments here for lack ofspace.
The conclusion derived from these exper-iments is that 1000 documents is the point wherethe learning-curves level off.In Table 2, we provide a few data points withrespect to the data size of these sets (tokenizedword-count on the source side).
We also report theBLEU performance of the TrustRank-MT systemon the Regression Test set.Note that the differences between the BLEUscores reported in Table 1 and the BLEU scoresunder the WMT09 label in Table 2 reflect dif-ferences in the genres of these sets.
The offi-cial development test set (news-dev2009b) for theWMT09 task is news only.
The regression Testsets have the same distribution between Europarldata and news as the corresponding training dataset for each language pair.4 The ranking algorithmAs mentioned before, TrustRank takes a super-vised Machine Learning approach.
We automat-ically generate the training labels by computingBLEU scores for every document in the Regres-sion training set.boundaries, we simply simulate document boundaries afterevery 10 consecutive sentences.LP MT set Regression setTrain Train Test BLEUWMT09Eng-Spa 41Mw 277Kw 281Kw 41.0Eng-Fra 41Mw 282Kw 283Kw 37.1Eng-Ger 41Mw 282Kw 280Kw 23.7Eng-Cze 1.2Mw 241Kw 242Kw 10.3Eng-Hun 30Mw 209Kw 206Kw 14.5Spa-Eng 42Mw 287Kw 293Kw 40.1Fra-Eng 44Mw 305Kw 308Kw 37.9Ger-Eng 39Mw 269Kw 267Kw 29.4Cze-Eng 1.0Mw 218Kw 219Kw 19.7Hun-Eng 26Mw 177Kw 176Kw 24.0TravelEng-Spa 4.3Mw 123Kw 121Kw 31.2Eng-Fra 3.5Mw 132Kw 126Kw 27.8Eng-Ita 3.4Mw 179Kw 183Kw 22.5Eng-Por 13.1Mw 83Kw 83Kw 41.9Eng-Ger 7.0Mw 69Kw 69Kw 27.6Eng-Dut 0.7Mw 89Kw 84Kw 41.9ElectronicsEng-Spa 7.0Mw 150Kw 149Kw 65.2Eng-Fra 6.5Mw 129Kw 129Kw 55.8Eng-Ger 5.9Mw 139Kw 140Kw 42.1Eng-Chi 7.1Mw 135Kw 136Kw 63.9Eng-Por 2.0Mw 124Kw 115Kw 47.9HiTechEng-Spa 2.8Mw 143Kw 148Kw 59.0Eng-Ger 5.1Mw 162Kw 155Kw 36.6Eng-Chi 5.6Mw 131Kw 129Kw 60.6Eng-Rus 2.8Mw 122Kw 117Kw 39.2Eng-Kor 4.2Mw 129Kw 140Kw 49.4Table 2: Data sizes and BLEU on Regression Test.4.1 The learning methodThe results we report here are obtained usingthe freely-available Weka engine 2.
We havecompared and contrasted results using all theregression packages offered by Weka, includ-ing regression functions based on simple andmultiple-feature Linear regression, Pace regres-sion, RBF networks, Isotonic regression, GaussianProcesses, Support Vector Machines (with SMOoptimization) with polynomial and RBF kernels,and regression trees such as REP trees and M5Ptrees.
Due to lack of space and the tangential im-pact on the message of this paper, we do not report2Weka software at http://www.cs.waikato.ac.nz/ml/weka/,version 3.6.1, June 2009.615these contrastive experiments here.The learning technique that consistentlyyields the best results is M5P regression trees(weka.classifiers.trees.M5P).
Therefore, we reportall the results in this paper using this learningmethod.
As an additional advantage, the decisiontrees and the regression models produced in train-ing are easy to read, understand, and interpret.One can get a good insight into what the impactof a certain feature on a final predicted value is bysimply inspecting these trees.4.2 The featuresIn contrast to most of the work on confidence es-timation (Blatz et al, 2004), the features we useare not internal features of the MT system.
There-fore, TrustRank can be applied for a large varietyof MT approaches, from statistical-based to rule-based approaches.The features we use can be divided into text-based, language-model?based, pseudo-reference?based, example-based, and training-data?basedfeature types.
These feature types can be com-puted either on the source-side (input documents)or on the target-side (translated documents).Text-based featuresThese features simply look at the length of the in-put in terms of (tokenized) number of words.
Theycan be applied on the input, where they induce acorrelation between the number of words in the in-put document and the expected BLEU score forthat document size.
They can also be applied onthe produced output, and learn a similar correla-tion for the produced translation.Language-model?based featuresThese features are among the ones that were firstproposed as possible differentiators between goodand bad translations (Gamon et al, 2005).
Theyare a measure of how likely a collection of stringsis under a language model trained on monolingualdata (either on the source or target side).The language-model?based feature values weuse here are computed as document-level per-plexity numbers using a 5-gram language modeltrained on the MT training set.Pseudo-reference?based featuresPrevious work has shown that, in the absenceof human-produced references, automatically-produced ones are still helpful in differentiatingbetween good and bad translations (Albrecht andHwa, 2008).
When computed on the target side,this type of features requires one or more sec-ondary MT systems, used to generate transla-tions starting from the same input.
These pseudo-references are useful in gauging translation con-vergence, using BLEU scores as feature values.In intuitive terms, their usefulness can be summa-rized as follows: ?if system X produced a trans-lation A and system Y produced a translation Bstarting from the same input, andA andB are sim-ilar, then A is probably a good translation?.An important property here is that systems Xand Y need to be as different as possible from eachother.
This property ensures that a convergence onsimilar translations is not just an artifact, but a trueindication that the translations are correct.
Thesecondary systems we use here are still phrase-based, but equipped with linguistically-orientedmodules similar with the ones proposed in (Collinset al, 2005; Xu et al, 2009).The source-side pseudo-reference?based fea-ture type is of a slightly different nature.
It still re-quires one or more secondary MT systems, but op-erating in the reverse direction.
A translated doc-ument produced by the main MT system is fed tothe secondary MT system(s), translated back intothe original source language, and used as pseudo-reference(s) when computing a BLEU score forthe original input.
In intuitive terms: ?if systemX takes document A and produces B, and systemX?1 takes B and produces C, and A and C aresimilar, then B is probably a good translation?.Example-based featuresFor example-based features, we use a develop-ment set of 1000 parallel documents, for which weproduce translations and compute document-levelBLEU scores.
We set aside the top-100 BLEUscoring documents and bottom-100 BLEU scoringdocuments.
They are used as positive examples(with better-than-average BLEU) and negative ex-amples (with worse-than-average BLEU), respec-tively.
We define a positive-example?based fea-ture function as a geometric mean of 1-to-4?gramprecision scores (i.e., BLEU score without lengthpenalty) between a document (on either sourceor target side) and the positive examples used asreferences (similarly for negative-example?basedfeatures).The intuition behind these features can be sum-marized as follows: ?if system X translated docu-616ment A well/poorly, and A and B are similar, thensystem X probably translates B well/poorly?.Training-data?based featuresIf the main MT system is trained on a parallel cor-pus, the data in this corpus can be exploited to-wards assessing translation quality (Specia et al,2009).
In our context, the documents that make upthis corpus can be used in a fashion similar withthe positive examples.
One type of training-data?based features operates by computing the numberof out-of-vocabulary (OOV) tokens with respect tothe training data (on either source or target side).A more powerful type of training-data?basedfeatures operates by computing a BLEU score be-tween a document (source or target side) and thetraining-data documents used as references.
Intu-itively, we assess the coverage with respect to thetraining data and correlate it with a BLEU score:?if the n-grams of input document A are well cov-ered by the source-side of the training data, thetranslation of A is probably good?
(on the sourceside); ?if the n-grams in the output translation Bare well covered by the target-side of the paralleltraining data, then B is probably a good transla-tion?
(on the target side).4.3 ResultsWe are interested in the best performance forTrustRank using the features described above.
Inthis section, we focus on reporting the results ob-tain for the English-Spanish language pair.
In thenext section, we report results obtained on all thelanguage pairs we considered.Before we discuss the results of TrustRank, letus anchor the numerical values using some lower-and upper-bounds.
As a baseline, we use a re-gression function that outputs a constant numberfor each document, equal to the BLEU score ofthe Regression Training set.
As an upperbound,we use an oracle regression function that outputs anumber for each document that is equal to the ac-tual BLEU score of that document.
In Table 4, wepresent the performance of these regression func-tions across all the domains considered.As already mentioned, the rAcc values arebounded by the 25% lowerbound and the 100%upperbound.
The vBLEU?
values are bounded by0 as lowerbound, and some positive BLEU gainvalue that varies among the domains we consid-ered from +6.4 (Travel) to +13.5 (HiTech).The best performance obtained by TrustRankDomain rAcc vBLEU?
[4] MAE TEBaselineWMT09 25% 0 9.9 +0.4Travel 25% 0 8.3 +2.0Electr.
25% 0 12.2 +2.6HiTech 25% 0 16.9 +2.4Dom.
avg.
25% 0 11.8 1.9OracleWMT09 100% +8.2 0 0Travel 100% +6.4 0 0Electr.
100% +9.2 0 0HiTech 100% +13.5 0 0Dom.
avg.
100% +9.3 0 0Table 4: Lower- and upper-bounds for ranking andregression accuracy (English-Spanish).for English-Spanish, using all the features de-scribed, is presented in Table 3.
The ranking ac-curacy numbers on a per-quantile basis revealsan important property for the approach we ad-vocate.
The ranking accuracy on the first quan-tile Q1 (identifying the best 25% of the transla-tions) is 52% on average across the domains.
Forthe last quantile Q4 (identifying the worst 25% ofthe translations), it is 56%.
This is much betterthan the ranking accuracy for the median-qualitytranslations (35-37% accuracy for the two middlequantiles).
This property fits well our scenario, inwhich we are interested in associating trust in thequality of the translations in the top quantile.The quality of the top quantile translations isquantifiable in terms of BLEU gain.
The 250 doc-ument translations in Q1 for Travel have a BLEUscore of 38.0, a +6.8 BLEU gain compared to theoverall BLEU of 31.2 (Q1?4).
The Q1 HiTechtranslations, with a BLEU of 77.9, have a +18.9BLEU gain compared to the overall BLEU of59.0.
The TrustRank algorithm allows us to trade-off quantity versus quality on any scale.
The re-sults under the BLEU heading in Table 3 repre-sent an instantiation of this ability to a 3-pointscale (Q1,Q1?2,Q1?3).
The vBLEU?
numbersreflect an average of the BLEU gains for this in-stantiation (e.g., a +11.6 volume-weighted averageBLEU gain for the HiTech domain).We are also interested in the best performanceunder more restricted conditions, such as timeconstraints.
The assumption we make here is thatthe translation time dwarfs the time needed for fea-617Domain Ranking Accuracy Translation Accuracy MAE TEBLEU vBLEU?
[4]Q1 Q2 Q3 Q4 rAcc Q1 Q1?2 Q1?3 Q1?4WMT09 34% 26% 29% 40% 32% 44.8 43.6 42.4 41.1 +2.1 9.6 -0.1Travel 50% 26% 29% 41% 36% 38.0 35.1 33.0 31.2 +3.4 7.4 -1.9Electronics 57% 38% 39% 68% 51% 76.1 72.7 69.6 65.2 +6.5 8.4 -2.6HiTech 65% 48% 49% 75% 59% 77.9 72.7 66.7 59.0 +11.6 8.6 -2.1Dom.
avg.
52% 35% 37% 56% 45% - +5.9 8.5 1.7Table 3: Detailed performance using all features (English-Spanish).ture and regression value computation.
Therefore,the most time-expensive feature is the source-sidepseudo-reference?based feature, which effectivelydoubles the translation time required.
Under the?time-constrained?
condition, we exclude this fea-ture and use all of the remaining features.
Table 5presents the results obtained for English-Spanish.Domain rAcc vBLEU?
[4] MAE TE?Time-constrained?
conditionWMT09 32% +2.1 9.6 -0.1Travel 35% +3.2 7.4 -1.8Electronics 50% +6.3 8.4 -2.2HiTech 59% +11.6 8.9 -2.1Dom.
avg.
44% +5.8 8.6 1.6Table 5: ?Time-constrained?
performance(English-Spanish).The results presented above allow us to draw aseries of conclusions.Benefits vary by domainEven with oracle rankings (Table 4), the benefitsvary from one domain to the next.
For Travel, withan overall BLEU score in the low 30s (31.2), westand to gain at most +6.4 BLEU points on average(+6.4 vBLEU?
upperbound).
For a domain suchas HiTech, even with a high overall BLEU scoreclose to 60 (59.0), we stand to gain twice as much(+13.5 vBLEU?
upperbound).Performance varies by domainAs the results in Table 3 show, the best perfor-mance we obtain also varies from one domain tothe next.
For instance, the ranking accuracy forthe WMT09 domain is only 32%, while for theHiTech domain is 59%.
Also, the BLEU gain forthe WMT09 domain is only +2.1 vBLEU?
(com-pared to the upperbound vBLEU?
of +8.2, it isonly 26% of the oracle performance).
In contrast,the BLEU gain for the HiTech domain is +11.6vBLEU?
(compared to the +13.5 vBLEU?
up-perbound, it is 86% of the oracle performance).Positive feature synergy and overlapThe features we described capture different infor-mation, and their combination achieves the bestperformance.
For instance, in the Electronics do-main, the best single feature is the target-side n-gram coverage feature, with +5.3 vBLEU?.
Thecombination of all features gives a +6.5 vBLEU?.The numbers in Table 3 also show that elimi-nating some of the features results in lower perfor-mance.
The rAcc drops from 45% to 44% in underthe ?time-constraint?
condition (Table 5).
The dif-ference in the rankings is statistically significant atp < 0.01 using the Wilcoxon test (Dems?ar, 2006).However, this drop is quantitatively small (1%rAcc drop, -0.1 in vBLEU?, averaged across do-mains).
This suggests that, even when eliminatingfeatures that by themselves have a good discrim-inatory power (the source-side pseudo-reference?based feature achieves a +5.0 vBLEU?
as a sin-gle feature in the Electronics domain), the otherfeatures compensate to a large degree.Poor regression performanceBy looking at the results of the regression metrics,we conclude that the predicted BLEU numbers arenot accurate in absolute value.
The aggregatedMean Absolute Error (MAE) is 8.5 when using allthe features.
This is less than the baseline MAE of11.8, but it is too high to allow us to confidentlyuse the document-level BLEU numbers as reliableindicators of translation accuracy.
The Test Error(TE) numbers are not encouraging either, as the1.7 TE of TrustRank is close to the baseline TE of1.9 (see Table 4 for baseline numbers).6185 Large-scale experimental resultsIn this section, we present the performance ofTrustRank on a variety of language pairs (Table 6).We report the BLEU score obtained on our 1000-document regression Test, as well as ranking andregression performance using the rAcc, vBLEU?,MAE, and TE metrics.As the numbers for the ranking and regres-sion metrics show, the same trends we observedfor English-Spanish hold for many other languagepairs as well.
Some domains, such as HiTech, areeasier to rank regardless of the language pair, andthe quality gains are consistently high (+9.9 av-erage vBLEU?
for the 5 language pairs consid-ered).
Other domains, such asWMT09 and Travel,are more difficult to rank.
However, the WMT09English-Hungarian data set appears to be bettersuited for ranking, as the vBLEU?
numbers arehigher compared to the rest of the language pairsfrom this domain (+4.3 vBLEU?
for Eng-Hun,+7.1 vBLEU?
for Hun-Eng).
For Travel, English-Dutch is also an outlier in terms of quality gains(+12.9 vBLEU?
).Overall, the results indicate that TrustRank ob-tains consistent performance across a large vari-ety of language pairs.
Similar with the conclusionfor English-Spanish, the regression performanceis currently too poor to allow us to confidentlyuse the absolute document-level predicted BLEUnumbers as indicators of translation accuracy.6 Examples and IllustrationsAs the experimental results in Table 6 show, theregression performance varies considerably acrossdomains.
Even within the same domain, the natureof the material used to perform the experimentscan influence considerably the results we obtain.In Figure 1, we plot ?BLEU,predBLEU?
points forthree of our language pairs presented in Table 6:Travel Eng-Fra, Travel Eng-Dut, and HiTech Eng-Rus.
These plots illustrate the tendency of the pre-dicted BLEU values to correlate with the actualBLEU scores.
The amount of correlation visible inthese plots matches the performance numbers pro-vided in Table 6, with Travel Eng-Fra at a lowerlevel of correlation compared to Travel Eng-Dutand HiTech Eng-Rus.
The ?BLEU,predBLEU?
pointstend to align along a line at an angle smaller than45?, an indication of the fact that the BLEU pre-dictions tend to be more conservative comparedto the actual BLEU scores.
For example, in theDomain BLEU rAcc vBLEU?
[4] MAE TEWMT09Eng-Spa 41.0 35% +2.4 9.2 -0.3Eng-Fra 37.1 37% +3.3 8.3 -0.5Eng-Ger 23.7 32% +1.9 5.8 -0.7Eng-Cze 10.3 38% +1.3 3.1 -0.6Eng-Hun 14.5 55% +4.3 3.7 -1.1Spa-Eng 40.1 37% +3.3 8.1 -0.2Fra-Eng 37.9 39% +3.8 10.1 -0.6Ger-Eng 29.4 36% +2.7 5.9 -0.9Cze-Eng 19.7 40% +2.4 4.3 -0.6Hun-Eng 24.0 61% +7.1 4.9 -1.8TravelEng-Spa 31.2 36% +3.4 7.4 -1.9Eng-Fra 27.8 39% +2.7 6.2 -0.9Eng-Ita 22.5 39% +2.4 5.1 +0.0Eng-Por 41.9 51% +5.6 8.6 +1.1Eng-Ger 27.6 37% +5.7 11.8 -0.4Eng-Dut 41.9 52% +12.9 12.9 -0.7ElectronicsEng-Spa 65.2 51% +6.5 8.4 -2.6Eng-Fra 55.8 49% +7.7 8.4 -2.3Eng-Ger 42.1 57% +8.9 7.4 -1.6Eng-Chi 63.9 48% +6.4 8.6 -0.8Eng-Por 47.9 49% +6.9 9.0 -1.8HiTechEng-Spa 59.0 59% +11.6 8.6 -2.1Eng-Ger 36.6 62% +9.2 7.1 -1.0Eng-Chi 60.3 54% +7.5 8.4 -1.0Eng-Rus 39.2 62% +10.7 8.7 -2.1Eng-Kor 49.4 61% +10.5 9.7 -3.2Table 6: Performance of TrustRank on a variety ofdomains and language pairs.Travel Eng-Fra case, the predicted BLEU numbersare spread across a narrower band (95% of the val-ues are in the [19-35] interval), compared to theactual BLEU scores (95% of the values are in the[11-47] interval).These intervals are also useful for gauging thelevel of difficulty stemming from the nature of thematerial used to perform the experiments.
In thecase of Travel Eng-Fra, the actual BLEU scoresare clustered in a narrower band (interval [11-47]covers 95% of the values), compared to the actualBLEU scores for Travel Eng-Dut (interval [11-92]covers 95% of the values) and HiTech Eng-Rus(interval [3-80] covers 95% of the values).
This619Figure 1: Examples of BLEU versus predBLEU.means that the documents in the latter cases areeasier to distinguish, compared to the documentsin Travel Eng-Fra.To provide an intuitive feel for the differencebetween the level of translation performance be-tween documents ranked close to the bottom anddocuments ranked close to the top, we presenthere two example translations.
They are docu-ments that we randomly picked from the bottom10% and top 10% of the Travel Eng-Fra documentset, and they correspond to points A and B in thefirst plot of Figure 1, respectively.
The A-Fra andB-Fra entries below are produced by our Eng-FraTrustRank-MT system, starting from A-Eng andB-Eng3, respectively.A-Eng This will be our 18th year,still love it.
Same hotel,room, staff, even other guests from other countries, itslovely to see everyone that you have gotten to knowover the years, even if ,you or they ,do not speak eachothers language.
We love the Island some much that,hopefuly, that is where we are retiring to, we do keeplooking for that affordable place.A-Fra Ce sera notre 18e`me anne?e, adore.
Me?me ho?tel,3We preserved the original writing style of the documentsin the source language.la chambre, le personnel, me?me d?autres clients dansd?autres pays, c?est tre`s agre?able de voir que tout lemonde vous aurais savoir au cours de ces dernie`resanne?es, me?me si, ou bien ils vous, ne parlent paschaque d?autres langues.
Nous adorons l??
?le des que,hopefuly, c?est l?endroit ou` nous avons retiring, nousne pour chercher un endroit abordable.B-Eng Stayed at the Intercontinental for 4 nights.
It is in anexcellent location, not far from the French Quarter.
Therooms are large, clean, and comfortable.
The staff isfriendly and helpful.
Parking is very expensive, around$29.
00 a day.
There is a garage next door which is alittle more reasonable.
I certainly suggest this hotel toothers.B-Fra J?ai se?journe?
a` l?Intercontinental pour 4 nuits.
Il esttre`s bien situe?, pas loin du Quartier Franc?ais.
Leschambres sont grandes, propres et confortables.
Le per-sonnel est sympa et serviable.
Le parking est tre`s cher,autour de 29 $ par jour.
Il y a un garage a` co?te?, cequi est un peu plus raisonnable.
Je conseille cet ho?tel a`d?autres.Document A-Fra is a poor translation, and isranked in the bottom 10%, while document B-Frais a nearly-perfect translation ranked in the top10%, out of a total of 1000 documents.7 Conclusions and Future WorkCommercial adoption of MT technology requirestrust in the translation quality.
Rather than delaythis adoption until MT attains a near-human levelof sophistication, we propose an interim approach.We present a mechanism that allows MT usersto trade quantity for quality, using automatically-determined translation quality rankings.The results we present in this paper show thatdocument-level translation quality rankings pro-vide quantitatively strong gains in translation qual-ity, as measured by BLEU.
A difference of +18.9BLEU, like the one we obtain for the English-Spanish HiTech domain (Table 3), is persuasiveevidence for inspiring trust in the quality of se-lected translations.
This approach enables us todevelop TrustRank, a complete MT solution thatenhances automatic translation with the ability toidentify document subsets containing translationsthat pass an acceptable quality threshold.When measuring the performance of our solu-tion across several domains, it becomes clear thatsome domains allow for more accurate quality pre-diction than others.
Given the immediate benefitthat can be derived from increasing the rankingaccuracy for translation quality, we plan to openup publicly available benchmark data that can beused to stimulate and rigorously monitor progressin this direction.620ReferencesJoshua Albrecht and Rebecca Hwa.
2007.
Regressionfor sentence-level MT evaluation with pseudo refer-ences.
In Proceedings of ACL.Joshua Albrecht and Rebecca Hwa.
2008.
The role ofpseudo references in MT evaluation.
In Proceedingsof ACL.Enrique Amigo?, Jesu?s Gime?nez, Julio Gonzalo, and Fe-lisa Verdejo.
2009.
The contribution of linguisticfeatures to automatic machine translation evaluation.In Proceedings of ACL.John Blatz, Erin Fitzgerald, GEorge Foster, SimonaGandrabur, Cyril Gouette, Alex Kulesza, AlbertoSanchis, and Nicola Ueffing.
2004.
Confidence es-timation for machine translation.
In Proceedings ofCOLING.Michael Collins, Philipp Koehn, and Ivona Kucerova.2005.
Clause restructuring for statistical machinetranslation.
In Proceedings of ACL.J.
Dems?ar.
2006.
Statistical comparisons of classifiersover multiple data sets.
Journal of Machine Learn-ing Research, 7.George Doddington.
2002.
Automatic evaluation ofmachine translation quality using n-gram coocur-rence statistics.
In Proceedings of HLT.Kevin Duh.
2008.
Ranking vs. regression in machinetranslation evaluation.
In Proceedings of the ACLThird Workshop on Statistical Machine Translation.Michael Gamon, Anthony Aue, and Martine Smets.2005.
Sentence-level MT evaluation without refer-ence translations: Beyond language modeling.
InProceedings of EAMT.Asela Gunawardana and Guy Shani.
2009.
A sur-vey of accuracy evaluation metrics of recommenda-tion tasks.
Journal of Machine Learning Research,10:2935?2962.David Kauchak and Regina Barzilay.
2006.
Para-phrasing for automatic evaluation.
In Proceedingsof HLT/NAACL.Philipp Koehn and Barry Haddow.
2009.
Edinburgh?ssubmission to all tracks of the WMT2009 sharedtask with reordering and speed improvements toMoses.
In Proceedings of EACL Workshop on Sta-tistical Machine Translation.Alex Kulesza and Stuart M. Shieber.
2004.
A learn-ing approach to improving sentence-level MT evalu-ation.
In Proceedings of the 10th International Con-ference on Theoretical and Methodological Issues inMachine Translation.A.-L. Lagarda, V. Alabau, F. Casacuberta, R. Silva, andE.
D?
?az de Lian?o.
2009.
Statistical post-editing of arule-based machine translation system.
In Proceed-ings of HLT/NAACL.A.
Lavie and A. Agarwal.
2007.
METEOR: An au-toamtic metric for mt evaluation with high levels ofcorrelation with human judgments.
In Proceedingsof ACL Workshop on Statistical Machine Transla-tion.Ding Liu and Daniel Gildea.
2005.
Syntactic fea-tures for evaluation of machine translations.
In Pro-ceedings of ACL Workshop on Intrinsic and Extrin-sic Evaluation Measures for Machine Translationand/or Summarization.Franz Joseph Och and Hermann Ney.
2004.
The align-ment template approach to statistical machine trans-lation.
Computational Linguistics, 30(4):417?449.Karolina Owczarzak, Josef Genabith, and Andy Way.2007.
Evaluating machine translation with LFGdependencies.
Machine Translation, 21(2):95?119,June.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
BLEU: a method for automaticevaluation of machine translation.
In Proceedingsof ACL.Sujith Ravi, Kevin Knight, and Radu Soricut.
2008.Automatic prediction of parsing accuracy.
In Pro-ceedings of EMNLP.Lucia Specia, Nicola Cancedda, Marc Dymetman,Marcho Turchi, and Nello Cristianini.
2009.
Esti-mating the sentence-level quality of machine trans-lation.
In Proceedings of EAMT.Nicola Ueffing and Hermann Ney.
2005.
Applica-tion of word-level confidence measures in interac-tive statistical machine translation.
In Proceedingsof EAMT.Peng Xu, Jaeho Kang, Michael Ringaard, and FranzOch.
2009.
Using a dependency parser to improveSMT for Subject-Object-Verb languages.
In Pro-ceedings of ACL.Muyun Yang, Shuqi Sun, Jufeng Li, Sheng Li, andZhao Tiejun.
2008.
A linguistically motivated MTevaluation system based on SVM regression.
InProceedings of AMTA.Yang Ye, Ming Zhou, and Chin-Yew Lin.
2007.
Sen-tence level machine translation evaluation as a rank-ing.
In Proceedings of the ACL Second Workshop onStatistical Machine Translation.Liang Zhou, Chin-Yew Lin, and Eduard Hovy.
2006.Re-evaluating machine translation results with para-phrase support.
In Proceedings of EMNLP.621
