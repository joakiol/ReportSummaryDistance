Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 503?513,Avignon, France, April 23 - 27 2012. c?2012 Association for Computational LinguisticsInstance-Driven Attachment of Semantic Annotationsover Conceptual HierarchiesJanara Christensen?University of WashingtonSeattle, Washington 98195janara@cs.washington.eduMarius Pas?caGoogle Inc.Mountain View, California 94043mars@google.comAbstractWhether automatically extracted or humangenerated, open-domain factual knowledgeis often available in the form of semanticannotations (e.g., composed-by) that takeone or more specific instances (e.g., rhap-sody in blue, george gershwin) as their ar-guments.
This paper introduces a methodfor converting flat sets of instance-levelannotations into hierarchically organized,concept-level annotations, which capturenot only the broad semantics of the desiredarguments (e.g., ?People?
rather than ?Loca-tions?
), but also the correct level of general-ity (e.g., ?Composers?
rather than ?People?,or ?Jazz Composers?).
The method refrainsfrom encoding features specific to a partic-ular domain or annotation, to ensure imme-diate applicability to new, previously un-seen annotations.
Over a gold standard ofsemantic annotations and concepts that bestcapture their arguments, the method sub-stantially outperforms three baselines, onaverage, computing concepts that are lessthan one step in the hierarchy away fromthe corresponding gold standard concepts.1 IntroductionBackground: Knowledge about the world canbe thought of as semantic assertions or anno-tations, at two levels of granularity: instancelevel (e.g., rhapsody in blue, tristan und isolde,george gershwin, richard wagner) and conceptlevel (e.g., ?Musical Compositions?, ?Works ofArt?, ?Composers?).
Instance-level annotationscorrespond to factual knowledge that can befound in repositories extracted automatically fromtext (Banko et al 2007; Wu and Weld, 2010)?Contributions made during an internship at Google.or manually created within encyclopedic re-sources (Remy, 2002).
Such facts could state, forinstance, that rhapsody in blue was composed-by george gershwin, or that tristan und isoldewas composed-by richard wagner.
In compar-ison, concept-level annotations more conciselyand effectively capture the underlying semanticsof the annotations by identifying the concepts cor-responding to the arguments, e.g., ?Musical Com-positions?
are composed-by ?Composers?.The frequent occurrence of instances, relativeto more abstract concepts, in Web documents andpopular Web search queries (Barr et al 2008;Li, 2010), is both an asset and a liability fromthe point of view of knowledge acquisition.
Onone hand, it makes instance-level annotations rel-atively easy to find, either from manually createdresources (Remy, 2002; Bollacker et al 2008),or extracted automatically from text (Banko etal., 2007).
On the other hand, it makes concept-level annotations more difficult to acquire di-rectly.
While ?Rhapsody in Blue was composedby George Gershwin [..]?
may occur in someform within Web documents, the more abstract?Musical compositions are composed by musi-cians [..]?
is unlikely to occur.
A more practicalapproach to collecting concept-level annotationsis to indirectly derive them from already plenti-ful instance-level annotations, effectively distill-ing factual knowledge into more abstract, conciseand generalizable knowledge.Contributions: This paper introduces a methodfor converting flat sets of specific, instance-level annotations into hierarchically organized,concept-level annotations.
As illustrated in Fig-ure 1, the resulting annotations must capture notjust the broad semantics of the desired arguments(e.g., ?People?
rather than ?Locations?
or ?Prod-503PeopleComposers MusiciansComposers by genre Cellists SingersBaroque Composers Jazz ComposersAnnotationsConceptual hierarchycomposed?by    lives?in    instrument?played    sung?byFigure 1: Hierarchical Semantic Annotations: Theattachment of semantic annotations (e.g., composed-by) into a conceptual hierarchy, a portion of which isshown in the diagram, requires the identification of thecorrect concept at the correct level of generality (e.g.,?Composers?
rather than ?Jazz Composers?
or ?Peo-ple?, for the right argument of composed-by).ucts?, as the right argument of the annotationcomposed-by), but actually identify the conceptsat the correct level of generality/specificity (e.g.,?Composers?
rather than ?Artists?
or ?Jazz Com-posers?)
in the underlying conceptual hierarchy.To ensure portability to new, previously unseenannotations, the proposed method avoids encod-ing features specific to a particular domain or an-notation.
In particular, the use of annotations?
la-bels (composed-by) as lexical features might betempting, but would anchor the annotation modelto that particular annotation.
Instead, the methodrelies only on features that generalize across an-notations.
Over a gold standard of semantic anno-tations and concepts that best capture their argu-ments, the method substantially outperforms threebaseline methods.
On average, the method com-putes concepts that are less than one step in thehierarchy away from the corresponding gold stan-dard concepts of the various annotations.2 Hierarchical Semantic Annotations2.1 Task DescriptionData Sources: The computation of hierarchicalsemantic annotations relies on the following datasources:?
a target annotation r (e.g., acted-in) that takesM arguments;?
N annotations I={<i1j , .
.
.
, iMj>}Nj=1 ofr at instance level, e.g., {<leonardo dicaprio,inception>, <milla jovovich, fifth element>} (inthis example, M=2);?
mappings {i?c} from instances to con-cepts to which they belong, e.g., milla jovovich?
?American Actors?, milla jovovich ?
?Peoplefrom Kiev?, milla jovovich?
?Models?;?
mappings {cs?cg} from more specific con-cepts to more general concepts, as encoded in ahierarchy H , e.g., ?American Actors??
?Actors?,?People from Kiev??
?People from Ukraine?,?Actors??
?Entertainers?.Thus, the main inputs are the conceptual hi-erarchy H , and the instance-level annotations I .The hierarchy contains instance-to-concept map-pings, as well as specific-to-general concept map-pings.
Via transitivity, instances (milla jovovich)and concepts (?American Actors?)
may be im-mediate children of more general concepts (?Ac-tors?
), or transitive descendants of more generalconcepts (?Entertainers?).
The hierarchy is not re-quired to be a tree; in particular, a concept mayhave multiple parent concepts.
The instance-levelannotations may be created collaboratively by hu-man contributors, or extracted automatically fromWeb documents or some other data source.Goal: Given the data sources, the goal is to de-termine to which concept c in the hierarchy H thearguments of the target concept-level annotationr should be attached.
While the left argument ofacted-in could attach to ?American Actors?, ?Peo-ple from Kiev?, ?Entertainers?
or ?People?, it isbest attached to the concept ?Actors?.
The goalis to select the concept c that most appropriatelygeneralizes across the instances.
Over the set Iof instance-level annotations, selecting a methodfor this goal can be thought of as a minimizationproblem.
The metric to be minimized is the sumof the distances between each predicted concept cand the correct concept cgold, where the distanceis the number of edges between c and cgold in H .Intuitions and Challenges: Given instances suchas milla jovovich that instantiate an argument ofan annotation like acted-in, the conceptual hierar-chy can be used to propagate the annotation up-wards, from instances to their concepts, then inturn further upwards to more general concepts.The best concept would be one of the many can-didate concepts reached during propagation.
In-tuitively, when compared to other candidate con-cepts, a higher proportion of the descendant in-stances of the best concept should instantiate (ormatch) the annotation.
At the same time, rela-tive to other candidate concepts, the best conceptshould have more descendant instances.While the intuitions seem clear, their inclu-sion in a working method faces a series of prac-tical challenges.
First, the data sources may benoisy.
One form of noise is missing or erroneous504Conceptual hierarchyEntitiesLocations PeopleSingers ActorsAmerican Actors English ActorsInstance-level annotationsacted-in(leonardo dicaprio, inception)acted-in(milla jovovich, fifth element)acted-in(judy dench, casino royale)acted-in(colin firth, the king?s speech)Instance to concept mappingsleonardo dicaprio: American Actorsmilla jovovich: American Actorsjudy dench: English Actorscolin firth: English ActorsCandidate conceptsEntitiesPeopleActorsAmerican ActorsEnglish ActorsRaw statisticsEntities, 4, 0.01 .
.
.People, 3, 0.1 .
.
.Actors, 2, 0.7 .
.
.American Actors, 1, 0.9 .
.
.English Actors, 1, 0.8 .
.
.Features Depth, Instance Percent .
.
.Query logsfifth element actorsfifth element costumesinception quotesout of africa actorsthe king?s speech oscarsClassified data0, People-Actors, 3/2, 0.1/0.7 .
.
.1, Actors-People, 2/3, 0.7/0.1 .
.
.1, Actors-American Actors, 2/1, 0.7/0.9 .
.
.0, American Actors-Actors, 1/2, 0.9/0.7 .
.
....Training/testing dataPeople-Actors, 3/2, 0.1/0.7 .
.
.Actors-People, 2/3, 0.7/0.1 .
.
.Actors-American Actors, 2/1, 0.7/0.9 .
.
.American Actors-Actors, 1/2, 0.9/0.7 .
.
....Ranked data (for Concept-level annotations)4, Actors3, People2, American Actors1, English Actors0, EntitiesConcept-level annotationsacted-in(Actors, ?
)Figure 2: Method Overview: Inferring concept-level annotations from instance-level annotations.instance-level annotations, which may artificiallyskew the distribution of matching instances to-wards a less than optimal region in the hierarchy.If the input annotations for acted-in are availablealmost exhaustively for all descendant instancesof ?American Actors?, and are available for only afew of the descendant instances of ?Belgian Ac-tors?, ?Italian Actors?
etc., then the distributionover the hierarchy may incorrectly suggest thatthe left argument of acted-in is ?American Actors?rather than the more general ?Actors?.
In anotherexample, if virtually all instances that instantiatethe left argument of the annotation won-award aremapped to the concept ?Award Winning Actors?,then it would be difficult to distinguish ?AwardWinning Actors?
from the more general ?Actors?or ?People?, as best concept to be computed forthe annotation.
Another type of noise is missingor erroneous edges in the hierarchy, which couldartificially direct propagation towards irrelevantregions of the hierarchy, or prevent propagationfrom even reaching relevant regions of the hier-archy.
For example, if the hierarchy incorrectlymaps ?Actors?
to ?Entertainment?, then ?Entertain-ment?
and its ancestor concepts incorrectly be-come candidate concepts during propagation forthe left argument of acted-in.
Conversely, if miss-ing edges caused ?Actors?
to not have any childrenin the hierarchy, then ?Actors?
would not even bereached and considered as a candidate conceptduring propagation.Second, to apply evidence collected from someannotations to a new annotation, the evidencemust generalize across annotations.
However,collected evidence or statistics may vary widelyacross annotations.
Observing that 90% of all de-scendant instances of the concept ?Actors?
matchan annotation acted-in constitutes strong evidencethat ?Actors?
is a good concept for acted-in.
Incontrast, observing that only 0.09% of all descen-dant instances of the concept ?Football Teams?match won-super-bowl should not be as strongnegative evidence as the percentage suggests.2.2 Inferring Concept-Level AnnotationsDetermining Candidate Concepts: As illus-trated in the left part of Figure 2, the first step to-wards inferring concept-level from instance-levelannotations is to propagate the instances that in-stantiate a particular argument of the annota-tion, upwards in the hierarchy.
Starting from theleft arguments of the annotation acted-in, namelyleonardo dicaprio, milla jovovich etc., the prop-agation reaches their parent concepts ?AmericanActors?, ?English Actors?, then their parent andancestor concepts ?Actors?, ?People?, ?Entities?etc.
The concepts reached during upward prop-agation become candidate concepts.
In subse-quent steps, the candidates are modeled, scoredand ranked such that ideally the best concept isranked at the top.Ranking Candidate Concepts: The identifica-505tion of a ranking function is cast as a semi-supervised learning problem.
Given the cor-rect (gold) concept of an annotation, it would betempting to employ binary classification directly,by marking the correct concept as a positive ex-ample, and all other candidate concepts as nega-tive examples.
Unfortunately, this would producea highly imbalanced training set, with thousandsof negative examples and, more importantly, withonly one positive example.
Another disadvan-tage of using binary classification directly is thatit is difficult to capture the preference for conceptscloser in the hierarchy to the correct concept, overconcepts many edges away.
Finally, the absolutevalues of the features that might be employed maybe comparable within an annotation, but incompa-rable across annotations, which reduces the porta-bility of the resulting model to new annotations.To address the above issues, the ranking func-tion proposed does not construct training exam-ples from raw features collected for each indi-vidual candidate concept.
Instead, it constructstraining examples from pairwise comparisons ofa candidate concept with another candidate con-cept.
Concretely, a pairwise comparison is la-beled as a positive example if the first concept iscloser to the correct concept than the second, or asnegative otherwise.
The pairwise formulation hasthree immediate advantages.
First, it accomodatesthe preference for concepts closer to the gold con-cept.
Second, the pairwise formulation producesa larger, more balanced training set.
Third, deci-sions of whether the first concept being comparedis more relevant than the second are more likely togeneralize across annotations, than absolute deci-sions of whether (and how much) a particular con-cept is relevant for a given annotation.Compiling Ranking Features: The features aregrouped into four categories: (A) annotation co-occurrence features, (B) concept features, (C) ar-gument co-occurrence features, and (D) combina-tion features, as described below.
(A) Annotation Co-occurrence Features: Theannotation co-occurrence features emphasize howwell an annotation applies to a concept.
Thesefeatures include (1) MATCHED INSTANCES thenumber of descendant instances of the conceptthat appear with the annotation, (2) INSTANCEPERCENT the percentage of matched instances inthe concept, (3) MORE THAN THREE MATCHINGINSTANCES and (4) MORE THAN TEN MATCH-ING INSTANCES, which indicate when the match-ing descendant instances might be noise.Also in this category are features that relay in-formation about the candidate concept?s childrenconcepts.
These features include (1) MATCHEDCHILDREN the number of child concepts con-taining at least one matching instance, (2) CHIL-DREN PERCENT the percentage of child conceptswith at least one matching instance, (3) AVG IN-STANCE PERCENT CHILDREN the average per-centage of matching descendant instances of thechild concepts, and (4) INSTANCE PERCENT TOINSTANCE PERCENT CHILDREN the ratio be-tween INSTANCE PERCENT and AVERAGE IN-STANCE PERCENT OF CHILDREN.
The last fea-ture is meant to capture dramatic changes in per-centages when moving in the hierarchy from childconcepts to the candidate concept in question.
(B) Concept Features: Concept features ap-proximate the generality of the concepts: (1)NUM INSTANCES the number of descendant in-stances of the concept, (2) NUM CHILDREN thenumber of child concepts, and (3) DEPTH the dis-tance to the concept?s farthest descendant.
(C) Argument Co-occurrence Features: The ar-gument co-occurrence features model the likeli-hood that an annotation applies to a concept bylooking at co-occurrences with another argumentof the same annotation.
Intuitively, if a con-cept representing one argument has a high co-occurrence with an instance that is some other ar-gument, a relationship more likely exists betweenmembers of the concept and the instance.
For ex-ample, given acted-in, ?Actors?
is likely to have ahigher co-occurrence with casablanca than ?Peo-ple?
is.
These features are generated from a set ofWeb queries.
Therefore, the collected values arelikely to be affected by different noise than thatpresent in the original dataset.
For every conceptand instance pair from the arguments of a givenannotation, they feature the number of times eachof the tokens in the concept appears in the samequery with each of the tokens in the instance,normalizing to the respective number of tokens.The procedure generates, for each candidate con-cept, an average co-occurrence score (AVG CO-OCCURRENCE) and a total co-occurrence score(TOTAL CO-OCCURRENCE) over all instances theconcept is paired with.
(D) Combination Features: The last groupof features are combinations of the above fea-tures: (1) DEPTH, INSTANCE PERCENT which isDEPTH multiplied by INSTANCE PERCENT, and506Concept Distance Match Total Match Total AvgInst Depth Avg TotalToCorrect Inst Inst Child Child PercOfChild Cooccur CooccurPeople 4 36512 879423 22 29 4% 14 0.67 33506Actors 0 29101 54420 6 10 32% 6 2.08 99971English Actors 2 3091 5922 3 4 37% 3 2.75 28378Labeled Concept Pair Annotation Co-occurrence Concept Arg Co-occurrence CombinationFeatures Features Features FeaturesConcept Label Match Inst Match Child AvgInst Num Num Depth Avg Total Depth DepthInstPair Inst Perc Child Perc PercChild Inst Child Cooccur Cooccur InstPerc PercChildPeople-Actors 0 1.25 0.08 3.67 1.26 0.13 1.25 3.67 2.33 0.32 0.34 0.18 0.66Actors-People 1 0.8 12.88 0.27 0.79 7.65 0.8 0.27 0.43 3.11 2.98 5.52 1.51Actors-English Actors 1 9.41 1.02 2.0 0.8 0.87 9.41 2.0 2.0 0.76 3.52 2.05 4.1English Actors-Actors 0 0.11 0.98 0.5 1.25 1.15 0.11 0.5 0.5 1.32 0.28 0.49 0.24English Actors-People 1 0.08 12.57 0.14 0.99 8.82 0.08 0.14 0.21 4.12 0.85 2.69 0.37People-English Actors 0 11.81 0.08 7.33 1.01 0.11 11.81 7.33 4.67 0.24 1.18 0.37 2.72Table 1: Training/Testing Examples: The top table shows examples of raw statistics gathered for three candidateconcepts for the left argument of the annotation acted-in.
The second table shows the training/testing examplesgenerated from these concepts and statistics.
Each example represents a pair of concepts which is labeled positiveif the first concept is closer to the correct concept than the second concept.
Features shown here are the ratiobetween a statistic for the first concept and a statistic for the second (e.g.
DEPTH for Actors-English Actors is 2as ?Actors?
has depth of 6 and ?English Actors?
has depth of 3).
Some features omitted due to space constraints.
(2) DEPTH, INSTANCE PERCENT, CHILDREN,which is the DEPTH multipled by the INSTANCEPERCENT multiplied by MATCHED CHILDREN.Both these features seek to balance the perceivedrelevance of an annotation to a candidate concept,with the generality of the candidate concept.Generating Learning Examples: For a givenannotation, the ranking features described so farare computed for each candidate concept (e.g.,?Movie Actors?, ?Models?, ?Actors?).
However,the actual training and testing examples are gener-ated for pairs of candidate concepts (e.g., <?FilmActors?, ?Models?>, <?Film Actors?, ?Actors?>,<?Models, ?Actors?>).
A training example rep-resents a comparison between two candidate con-cepts, and specifies which of the two is more rele-vant.
To create training and testing examples, thevalues of the features of the first concept in thepair are respectively combined with the values ofthe features of the second concept in the pair toproduce values corresponding to the entire pair.Following classification of testing examples,concepts are ranked according to the number ofother concepts which they are classified as morerelevant than.
Table 1 shows examples of train-ing/testing data.3 Experimental Setting3.1 Data SourcesConceptual Hierarchy: The experiments com-pute concept-level annotations relative to a con-ceptual hierarchy derived automatically from theWikipedia (Remy, 2002) category network, as de-scribed in (Ponzetto and Navigli, 2009).
The hi-erarchy filters out edges (e.g., from ?British FilmActors?
to ?Cinema of the United Kingdom?)
fromthe Wikipedia category network that do not corre-spond to IsA relations.
A concept in the hierarchyis a Wikipedia category (e.g., ?English Film Ac-tors?)
that has zero or more Wikipedia categoriesas child concepts, and zero or more Wikipediacategories (e.g., ?English People by Occupation?,?British Film Actors?)
as parent concepts.
Eachconcept in the hierarchy has zero or more in-stances, which are the Wikipedia articles listed (inWikipedia) under the respective categories (e.g.,colin firth is an instance of ?English Actors?
).Instance-Level Annotations: The experimentsexploit a set of binary instance-level annotations(e.g., acted-in, composed) among Wikipedia in-stances, as available in Freebase (Bollacker etal., 2008).
The annotation is a Freebase prop-erty (e.g., /music/composition/composer).
Inter-nally, the left and right arguments are Freebasetopic identifiers mapped to their correspondingWikipedia articles (e.g., /m/03f4k mapped to theWikipedia article on george gershwin).
In this pa-per, the derived annotations and instances are dis-played in a shorter, more readable form for con-ciseness and clarity.
As features do not use thelabel of the annotation, labels are never used inthe experiments and evaluation.507Web Search Queries: The argument co-occurrence features described above are com-puted over a set of around 100 millionanonymized Web search queries from 2010.3.2 Experimental RunsThe experimental runs exploit ranking featuresdescribed in the previous section, employing:?
one of three learning algorithms: naive Bayes(NAIVEBAYES), maximum entropy (MAXENT),or perceptron (PERCEPTRON) (Mitchell, 1997),chosen for their scalability to larger datasets viadistributed implementations.?
one of three ways of combining the valuesof features collected for individual candidate con-cepts into values of features for pairs of candidateconcepts: the raw ratio of the values of the re-spective features of the two concepts (0 when thedenominator is 0); the ratio scaled to the interval[0, 1]; or a binary value indicating which of thevalues is larger.For completeness, the experiments includethree additional, baseline runs.
Each baselinecomputes scores for all candidate concepts basedon the respective metric; then candidate conceptsare ranked in decreasing order of their scores.
Thebaselines metrics are:?
INSTPERCENT ranks candidate concepts bythe percentage of matched instances that are de-scendants of the concept.
It emphasizes conceptswhich are ?proven?
to belong to the annotation;?
ENTROPY ranks candidate concepts by theentropy (Shannon, 1948) of the proportion ofmatched descendant instances of the concept;?
AVGDEPTH ranks candidate concepts bytheir distances to half of the maximum hierarchyheight, emphasizing a balance of generality andspecificity.3.3 Evaluation ProcedureGold Standard of Concept-Level Annotations:A random, weighted sample of 200 annotation la-bels (e.g., corresponding to composed-by, play-instrument) is selected, out of the set of labelsof all instance-level annotations collected fromFreebase.
During sampling, the weights are thecounts of distinct instance-level annotations (e.g.,<rhapsody in blue, george gershwin>) avail-able for the label.
The arguments of the anno-tation labels are then manually annotated witha gold concept, which is the category from theWikipedia hierarchy that best captures their se-mantics.
The manual annotation is carried outindependently by two human judges, who thenverify each other?s work and discard inconsisten-cies.
For example, the gold concept of the leftargument of composed-by is annotated to be theWikipedia category ?Musical Compositions?.
Inthe process, some annotation labels are discarded,when (a) it is not clear what concept captures anargument (e.g., for the right argument of function-of-building), or (b) more than 5000 candidate con-cepts are available via propagation for one of thearguments, which would cause too many train-ing or testing examples to be generated via con-cept pairs, and slow down the experiments.
Theretained 139 annotation labels, whose argumentshave been labeled with their respective gold con-cepts, form the gold standard for the experiments.More precisely, an entry in the resulting gold stan-dard consists of an annotation label, one of itsarguments being considered (left or right), anda gold concept that best captures that argument.The set of annotation labels from the gold stan-dard is quite diverse and covers many domains ofpotential interest, e.g., has-company(?Industries?,?Companies?
), written-by(?Films?, ?Screenwrit-ers?
), member-of (?Politicians?,?Political Parties?
),or part-of-movement(?Artists?, ?Art Movements?
).Evaluation Metric: Following previous workon selectional preferences (Kozareva and Hovy,2010; Ritter et al 2010), each entry in the goldstandard, (i.e., each argument for a given annota-tion) is evaluated separately.
Experimental runscompute a ranked list of candidate concepts foreach entry in the gold standard.
In theory, a com-puted candidate concept is better if it is closersemantically to the gold concept.
In practice,the accuracy of a ranked list of candidate con-cepts, relative to the gold concept of the anno-tation label, is measured by two scoring metricsthat correspond to the mean reciprocal rank score(MRR) (Voorhees and Tice, 2000) and a modifi-cation of it (DRR) (Pas?ca and Alfonseca, 2009):MRR =1NN?i=1maxrank1rankiN is the number of annotations and ranki is therank of the gold concept in the returned list forMRR.
An annotation ai receives no credit forMRR if the gold concept does not appear in thecorresponding ranked list.DRR =1NN?i=1maxrank1ranki ?
(1 + Len)For DRR, ranki is the rank of a candidate con-cept in the returned list and Len is the length of508Annotation (Number of Candidate Concepts) Examples of Instances Top Ranked ConceptsComposers compose Musical Compositions (3038) aaron copland; black sabbath Music by Nationality; Composers; ClassicalComposersMusical Compositions composed-by Composers (1734) we are the champions; yor-ckscher marschMusical Compositions; Compositions byComposer; Classical MusicFoods contain Nutrients (1112) acca sellowiana; lasagna Foods; Edible Plants; Food IngredientsOrganizations has-boardmember People (3401) conocophillips; spence school Companies by Stock Exchange; CompaniesListed on the NYSE; CompaniesEducational Organizations has-graduate Alumni (4072) air force institute of technology;deering high schoolEducation by Country; Schools by Country;Universities and Colleges by CountryTelevision Actors guest-role Fictional Characters (4823) melanie griffith; patti laBelle Television Actors by Nationality; Actors;American ActorsMusical Groups has-member Musicians (2287) steroid maximus; u2 Musical Groups; Musical Groups by Genre;Musical Groups by NationalityRecord Labels represent Musician (920) columbia records; vandit Record Labels; Record Labels by Country;Record Labels by GenreAwards awarded-to People (458) academy award for best originalsong; erasmus prizeFilm Awards; Awards; Grammy AwardsFoods contain Nutrients (177) lycopene; glutamic acid Carboxylic Acids ; Acids; Essential NutrientsArchitects design Buildings and Structures (4811) 20 times square; berkeley build-ingBuildings and Structures; Buildings and Struc-tures by Architect; Houses by CountryPeople died-from Causes of Death (577) malaria; skiing Diseases; Infectious Diseases; Causes ofDeathArt Directors direct Films (1265) batman begins; the lion king Films; Films by Director; FilmEpisodes guest-star Television Actors (1067) amy poehler; david caruso Television Actors by Nationality; Actors;American ActorsTelevision Network has-tv-show Television Series (2492) george of the jungle; great expec-tationsTelevision Series by Network; Television Se-ries; Television Series by GenreMusicians play Musical Instruments (423) accordion; tubular bell Musical Instruments; Musical Instruments byNationality; Percussion InstrumentsPoliticians member-of Political Parties (938) independent moralizing front;national coalition partyPolitical Parties; Political Parties by Country;Political Parties by IdeologyTable 2: Concepts Computed for Gold-Standard Annotations: Examples of entries from the gold standard andcounts of candidate concepts (Wikipedia categories) reached from upward propagation of instances (Wikipediainstances).
The target gold concept is shown in bold.
Also shown are examples of Wikipedia instances, and thetop concepts computed by the best-performing learning algorithm for the respective gold concepts.the minimum path in the hierarchy between theconcept and the gold concept.
Len is minimum(0) if the candidate concept is the same as the goldstandard concept.
A given annotation ai receivesno credit for DRR if no path is found between thereturned concepts and the gold concept.As an illustration, for a single annotation, theright argument of composed-by, the ranked listof concepts returned by an experimental maybe [?Symphonies by Anton Bruckner?, ?Sym-phonies by Joseph Haydn?, ?Symphonies by Gus-tav Mahler?, ?Musical Compositions?, ..], with thegold concept being ?Musical Compositions?.
Thelength of the path between ?Symphonies by An-ton Bruckner?
etc.
and ?Musical Compositions?
is2 (via ?Symphonies?).
Therefore, the MRR scorewould be 0.25 (given by the fourth element ofthe ranked list), whereas the DRR score would be0.33 (given by the first element of the ranked list).MRR and DRR are computed in five-fold crossvalidation.
Concretely, the gold standard is splitinto five folds such that the sets of annotation la-bels in each fold are disjoint.
Thus, none ofthe annotation labels in testing appears in train-ing.
This restriction makes the evaluation morerigurous and conservative as it actually assessesthe extent the models learned are applicable tonew, previously unseen annotation labels.
Ifthis restriction were relaxed, the baselines wouldpreform equivalently as they do not depend onthe training data, but the learned methods wouldlikely do better.4 Evaluation Results4.1 Quantitative ResultsConceptual Hierarchy: The conceptual hierar-chy contains 108,810 Wikipedia categories, andits maximum depth, measured as the distancefrom a concept to its farthest descendant, is 16.Candidate Concepts: On average, for the goldstandard, the method propagates a given annota-tion from instances to 1,525 candidate concepts,from which the single best concept must be deter-mined.
The left part of Table 2 illustrates the num-ber of candidate concepts reached during propa-gation for a sample of annotations.509Experimental Run AccuracyN=1 N=20MRR DRR MRR DRR?With raw-ratio features:NAIVEBAYES 0.021 0.180 0.054 0.222MAXENT 0.029 0.168 0.045 0.208PERCEPTRON 0.029 0.176 0.045 0.216?With scaled-ratio features:NAIVEBAYES 0.050 0.170 0.112 0.243MAXENT 0.245 0.456 0.430 0.513PERCEPTRON 0.245 0.391 0.367 0.461?With binary features:NAIVEBAYES 0.115 0.297 0.224 0.361MAXENT 0.165 0.390 0.293 0.441PERCEPTRON 0.180 0.332 0.330 0.429?
For baselines:INSTPERCENT 0.029 0.173 0.045 0.224ENTROPY 0.000 0.110 0.007 0.136AVGDEPTH 0.007 0.018 0.028 0.045Table 3: Precision Results: Accuracy of ranked listsof concepts (Wikipedia categories) computed by var-ious runs, as an average over the gold standard ofconcept-level annotations, considering the top N can-didate concepts computed for each gold standard entry.4.2 Qualitative ResultsPrecision: Table 3 compares the precision of theranked lists of candidate concepts produced by theexperimental runs.
The MRR and DRR scores inthe table consider either at most 20 of the conceptsin the ranked list computed by a given experimen-tal run, or only the first, top ranked computed con-cept.
Note that, in the latter case, the MRR andDRR scores are equivalent to precision@1 scores.Several conclusions can be drawn from the re-sults.
First, as expected by definition of thescoring metrics, DRR scores are higher than thestricter MRR scores, as they give partial creditto concepts that, while not identical to the goldconcepts, are still close approximations.
This isparticularly noticeable for the runs MAXENT andPERCEPTRON with raw-ratio features (4.6 and4.8 times higher respectively).
Second, amongthe baselines, INSTPERCENT is the most accu-rate, with the computed concepts identifying thegold concept strictly at rank 22 on average (foran MRR score 0.045), and loosely at an aver-age of 4 steps away from the gold concept (fora DRR score of 0.224).
Third, the accuracy ofthe learning algorithms varies with how the pair-wise feature values are combined.
Overall, raw-ratio feature values perform the worst, and scaled-ratio the best, with binary in-between.
Fourth,the scores of the best experimental run, MAXENTwith scaled-ratio features, are 0.430 (MRR) and0.513 (DRR) over the top 20 computed concepts,and 0.245 (MRR) and 0.456 (DRR) when consid-ering only the first concept.
These scores corre-spond to the ranked list being less than one stepaway in the hierarchy.
The very first computedconcept exactly matches the gold concept in aboutone in four cases, and is slightly more than onestep away from it.
In comparison, the very firstconcept computed by the best baseline matchesthe gold concept in about one in 35 cases (0.029MRR), and is about 6 steps away (0.173 DRR).The accuracies of the various learning algorithms(not shown) were also measured and correlatedroughly with the MRR and DRR scores.Discussion: The baseline runs INSTPERCENTand ENTROPY produce categories that are fartoo specific.
For the gold annotation composed-by(?Composers?, ?Musical Compositions?
), INST-PERCENT produces ?Scottish Flautists?
for the leftargument and ?Operas by Ernest Reyer?
for theright.
AVGDEPTH does not suffer from over-specification, but often produces concepts thathave been reached via propagation, yet are notclose to the gold concept.
For composed-by,AVGDEPTH produces ?Film?
for the left argumentand ?History by Region?
for the right.4.3 Error AnalysisThe right part of Table 2 provides a more de-tailed view into the best performing experimentalrun, showing actual ranked lists of concepts pro-duced for a sample of the gold standard entriesby MAXENT with scaled-ratio.
A separate analy-sis of the results indicates that the most commoncause of errors is noise in the conceptual hier-archy, in the form of unbalanced instance-levelannotations and missing hierarchy edges.
Un-balanced annotations are annotations where cer-tain subtrees of the hierarchy are artificially morepopulated than other subtrees.
For the left argu-ment of the annotation has-profession, 0.05% of?New York Politicians?
are matched but 70% of?Bushrangers?
are matched.
Such imbalances maybe inherent to how annotations are added to Free-base: different human contributors may add newannotations to particular portions of Freebase, butmiss other relevant portions.The results are also affected by missing edgesin the hierarchy.
Of the more than 100K con-cepts in the hierarchy, 3479 are roots of subhier-archies that are mutually disconnected.
Exam-ples are ?People by Region?, ?Shades of Red?, and510?Members of the Parliament of Northern Ireland?,all of which should have parents in the hierarchy.If a few edges are missing in a particular regionof the hierarchy, the method can recover, but if somany edges are missing that a gold concept hasvery few descendants, then propagation can besubstantially affected.
In the worst case, the goldconcept becomes disconnected, and thus will bemissing from the set of candidate concepts com-piled during propagation.
For example, for theannotation team-color(?Sports Clubs?, ?Colors?
),the only descendant concept of ?Colors?
in the hi-erarchy is ?Horse Coat Colors?, meaning that thegold concept ?Colors?
is not reached during prop-agation from instances upwards in the hierarchy.5 Related WorkSimilar to the task of attaching a semantic anno-tation to the concept in a hierarchy that has thebest level of generality is the task of finding se-lectional preferences for relations.
Most relevantto this paper is work that seeks to find the appro-priate concept in a hierarchy for an argument ofa specific relation (Ribas, 1995; McCarthy, 1997;Li and Abe, 1998).
Li and Abe (1998) addressthis problem by attempting to identify the best treecut in a hierarchy for an argument of a given verb.They use the minimum description length princi-ple to select a set of concepts from a hierarchy torepresent the selectional preferences.
This workmakes several limiting assumptions including thatthe hierarchy is a tree, and every instance belongsto just one concept.
Clark and Weir (2002) inves-tigate the task of generalizing a single relation-concept pair.
A relation is propagated up a hier-archy until a chi-square test determines the differ-ence between the probability of the child and par-ent concepts to be significant where the probabili-ties are relation-concept frequencies.
This methodhas no direct translation to the task discussed here;it is unclear how to choose the correct concept ifinstances generalize to different concepts.In other research on selectional preferences,Pantel et al(2007), Kozareva and Hovy (2010)and Ritter et al(2010) focus on generating ad-missible arguments for relations, and Erk (2007)and Bergsma et al(2008) investigate classifyinga relation-instance pair as plausible or not.Important to this paper is the Wikipedia cate-gory network (Remy, 2002) and work on refin-ing it.
Ponzetto and Navigli (2009) disambiguateWikipedia categories by using WordNet synsetsand use this semantic information to construct ataxonomy.
The resulting taxonomy is the concep-tual hierarchy used in the evaluation.Another related area of work is the discovery ofrelations between concepts.
Nastase and Strube(2008) use Wikipedia category names and cate-gory structure to generate a set of relations be-tween concepts.
Yan et al(2009) discover re-lations between Wikipedia concepts via deep lin-guistic information and Web frequency informa-tion.
Mohamed et al(2011) generate candi-date relations by coclustering text contexts for ev-ery pair of concepts in a hierarchy.
In a sense,this area of research is complementary to that dis-cussed in this paper.
These methods induce newrelations, and the proposed method can be usedto find appropriate levels of generalization for thearguments of any given relation.6 ConclusionsThis paper introduces a method to convert flat setsof instance-level annotations to hierarchically or-ganized, concept-level annotations.
The methoddetermines the appropriate concept for a given se-mantic annotation in three stages.
First, it propa-gates annotations upwards in the hierarchy, form-ing a set of candidate concepts.
Second, it classi-fies each candidate concept as more or less appro-priate than each other candidate concept within anannotation.
Third, it ranks candidate concepts bythe number of other concepts relative to which itis classified as more appropriate.
Because the fea-tures are comparisons between concepts within asingle semantic annotation, rather than consider-ations of individual concepts, the method is ableto generalize across annotations, and can thus beapplied to new, previously unseen annotations.Experiments demonstrate that, on average, themethod is able to identify the concept of a givenannotation?s argument within one hierarchy edgeof the gold concept.The proposed method can take advantage ofexisting work on open-domain information ex-traction.
The output of such work is usuallyinstance-level annotations, although often at sur-face level (non-disambiguated arguments) ratherthan semantic level (disambiguated arguments).After argument disambiguation (e.g., (Dredze etal., 2010)), the annotations can be used as inputto determining concept-level annotations.
Thus,the method has the potential to generalize anyexisting database of instance-level annotations toconcept-level annotations.511ReferencesMichele Banko, Michael Cafarella, Stephen Soder-land, Matt Broadhead, and Oren Etzioni.
2007.Open information extraction from the Web.
In Pro-ceedings of the 20th International Joint Conferenceon Artificial Intelligence (IJCAI-07), pages 2670?2676, Hyderabad, India.Cory Barr, Rosie Jones, and Moira Regelson.
2008.The linguistic structure of English Web-searchqueries.
In Proceedings of the 2008 Conferenceon Empirical Methods in Natural Language Pro-cessing (EMNLP-08), pages 1021?1030, Honolulu,Hawaii.Shane Bergsma, Dekang Lin, and Randy Goebel.2008.
Discriminative learning of selectional pref-erence from unlabeled text.
In Proceedings of the2008 Conference on Empirical Methods in NaturalLanguage Processing (EMNLP-08), pages 59?68,Honolulu, Hawaii.Kurt Bollacker, Colin Evans, Praveen Paritosh, TimSturge, and Jamie Taylor.
2008.
Freebase: Acollaboratively created graph database for struc-turing human knowledge.
In Proceedings of the2008 International Conference on Management ofData (SIGMOD-08), pages 1247?1250, Vancouver,Canada.Stephen Clark and David Weir.
2002.
Class-basedprobability estimation using a semantic hierarchy.Computational Linguistics, 28(2):187?206.Mark Dredze, Paul McNamee, Delip Rao, Adam Ger-ber, and Tim Finin.
2010.
Entity disambiguationfor knowledge base population.
In Proceedingsof the 23rd International Conference on Compu-tational Linguistics (COLING-10), pages 277?285,Beijing, China.Katrin Erk.
2007.
A simple, similarity-based modelfor selectional preferences.
In Proceedings of the45th Annual Meeting of the Association for Com-putational Linguistics (ACL-07), pages 216?223,Prague, Czech Republic.Zornitsa Kozareva and Eduard Hovy.
2010.
Learningarguments and supertypes of semantic relations us-ing recursive patterns.
In Proceedings of the 48thAnnual Meeting of the Association for Computa-tional Linguistics (ACL-10), pages 1482?1491, Up-psala, Sweden.Hang Li and Naoki Abe.
1998.
Generalizing caseframes using a thesaurus and the mdl principle.
InProceedings of the ECAI-2000 Workshop on Ontol-ogy Learning, pages 217?244, Berlin, Germany.Xiao Li.
2010.
Understanding the semantic struc-ture of noun phrase queries.
In Proceedings of the48th Annual Meeting of the Association for Com-putational Linguistics (ACL-10), pages 1337?1345,Uppsala, Sweden.Diana McCarthy.
1997.
Word sense disambiguationfor acquisition of selectional preferences.
In Pro-ceedings of the ACL/EACL Workshop on AutomaticInformation Extraction and Building of Lexical Se-mantic Resources for NLP Applications, pages 52?60, Madrid, Spain.Tom Mitchell.
1997.
Machine Learing.
McGraw Hill.Thahir Mohamed, Estevam Hruschka, and TomMitchell.
2011.
Discovering relations betweennoun categories.
In Proceedings of the 2011 Con-ference on Empirical Methods in Natural LanguageProcessing (EMNLP-11), pages 1447?1455, Edin-burgh, United Kingdom.Vivi Nastase and Michael Strube.
2008.
DecodingWikipedia categories for knowledge acquisition.
InProceedings of the 23rd National Conference onArtificial Intelligence (AAAI-08), pages 1219?1224,Chicago, Illinois.M.
Pas?ca and E. Alfonseca.
2009.
Web-derivedresources for Web Information Retrieval: Fromconceptual hierarchies to attribute hierarchies.
InProceedings of the 32nd International Conferenceon Research and Development in Information Re-trieval (SIGIR-09), pages 596?603, Boston, Mas-sachusetts.Patrick Pantel, Rahul Bhagat, Timothy Chklovski, andEduard Hovy.
2007.
ISP: Learning inferential se-lectional preferences.
In Proceedings of the AnnualMeeting of the North American Chapter of the Asso-ciation for Computational Linguistics (NAACL-07),pages 564?571, Rochester, New York.Simone Paolo Ponzetto and Roberto Navigli.
2009.Large-scale taxonomy mapping for restructuringand integrating Wikipedia.
In Proceedings ofthe 21st International Joint Conference on Ar-tifical Intelligence (IJCAI-09), pages 2083?2088,Barcelona, Spain.Melanie Remy.
2002.
Wikipedia: The free encyclope-dia.
Online Information Review, 26(6):434.Francesc Ribas.
1995.
On learning more appropriateselectional restrictions.
In Proceedings of the 7thConference of the European Chapter of the Asso-ciation for Computational Linguistics (EACL-97),pages 112?118, Madrid, Spain.Alan Ritter, Mausam, and Oren Etzioni.
2010.
A la-tent dirichlet alcation method for selectional pref-erences.
In Proceedings of the 48th Annual Meet-ing of the Association for Computational Linguis-tics (ACL-10), pages 424?434, Uppsala, Sweden.Claude Shannon.
1948.
A mathematical theory ofcommunication.
Bell System Technical Journal,27:379?423,623?656.Ellen Voorhees and Dawn Tice.
2000.
Building aquestion-answering test collection.
In Proceedingsof the 23rd International Conference on Researchand Development in Information Retrieval (SIGIR-00), pages 200?207, Athens, Greece.512Fei Wu and Daniel S. Weld.
2010.
Open informationextraction using wikipedia.
In Proceedings of the48th Annual Meeting of the Association for Compu-tational Linguistics (ACL-10), pages 118?127, Up-psala, Sweden.Yulan Yan, Naoaki Okazaki, Yutaka Matsuo, ZhengluYang, and Mitsuru Ishizuka.
2009.
Unsupervisedrelation extraction by mining Wikipedia texts usinginformation from the Web.
In Proceedings of theJoint Conference of the 47th Annual Meeting of theACL and the 4th International Joint Conference onNatural Language Processing of the AFNLP (ACL-IJCNLP-09), pages 1021?1029, Suntec, Singapore.513
