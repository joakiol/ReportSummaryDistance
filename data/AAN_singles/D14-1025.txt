Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 202?206,October 25-29, 2014, Doha, Qatar.c?2014 Association for Computational LinguisticsFitting Sentence Level Translation Evaluationwith Many Dense FeaturesMilo?s Stanojevi?c and Khalil Sima?anInstitute for Logic, Language and ComputationUniversity of AmsterdamScience Park 107, 1098 XG Amsterdam, The Netherlands{m.stanojevic,k.simaan}@uva.nlAbstractSentence level evaluation in MT has turned outfar more difficult than corpus level evaluation.Existing sentence level metrics employ a lim-ited set of features, most of which are rathersparse at the sentence level, and their intricatemodels are rarely trained for ranking.
This pa-per presents a simple linear model exploiting33 relatively dense features, some of which arenovel while others are known but seldom used,and train it under the learning-to-rank frame-work.
We evaluate our metric on the stan-dard WMT12 data showing that it outperformsthe strong baseline METEOR.
We also ana-lyze the contribution of individual features andthe choice of training data, language-pair vs.target-language data, providing new insightsinto this task.1 IntroductionEvaluating machine translation (MT) output at the sen-tence/ segment level has turned out far more challeng-ing than corpus/ system level.
Yet, sentence levelevaluation can be useful because it allows fast, fine-grained analysis of system performance on individualsentences.It is instructive to contrast two widely used metrics,METEOR (Michael Denkowski and Alon Lavie, 2014)and BLEU (Papineni et al., 2002), on sentence levelevaluation.
METEOR constantly shows better corre-lation with human ranking than BLEU (Papineni etal., 2002).
Arguably, this shows that sentence levelevaluation demands finer grained and trainable modelsover less sparse features.
Ngrams, the core of BLEU,are sparse at the sentence level, and a mismatch forlonger ngrams implies that BLEU falls back on shorterngrams.
In contrast, METEOR has a trainable modeland incorporates a small, yet wider set of features thatare less sparse than ngrams.
We think that METEOR?sfeatures and its training approach only suggest that sen-tence level evaluation should be treated as a modellingchallenge.
This calls for questions such as what model,what features and what training objective are bettersuited for modelling sentence level evaluation.We start out by explicitly formulating sentence levelevaluation as the problem of ranking a set of compet-ing hypothesis.
Given data consisting of human rankedsystem outputs, the problem then is to formulate aneasy to train model for ranking.
One particular exist-ing approach (Ye et al., 2007) looks especially attrac-tive because we think it meshes well with a range ofeffective techniques for learning-to-rank (Li, 2011).We deliberately select a linear modelling approachinspired by RankSVM (Herbrich et al., 1999), which iseasily trainable for ranking and allows analysis of theindividual contributions of features.
Besides presentinga new metric and a set of known, but also a set of novelfeatures, we target three questions of interest to the MTcommunity:?
What kind of features are more helpful for sen-tence level evaluation??
How does a simple linear model trained for rank-ing compare to the well-developed metric ME-TEOR on sentence level evaluation??
Should we train the model for each language pairseparately or for a target language?Our new metric dubbed BEER1outperforms ME-TEOR on WMT12 data showing the effectiveness ofdense features in a learning-to-rank framework.
Themetric and the code are available as free software2.2 ModelOur model is a linear combination of features trainedfor ranking similar to RankSVM (Herbrich et al., 1999)or, to readers familiar with SMT system tuning, to PROtuning (Hopkins and May, 2011):score(sys) = ~w ?
~xsyswhere ~w represents a weight vector and ~xsysa vec-tor of feature values for system output sys.
Look-ing at evaluation as a ranking problem, we con-trast (at least) two system translations good andbad for the same source sentence.
Assuming thathumanRank(good) > humanRank(bad) as ranked1BEER participated on WMT14 evaluation metrics taskwhere it was the highest scoring sentence level evaluationmetric on average over all language pairs (Stanojevi?c andSima?an, 2014)2https://github.com/stanojevic/beer202by human judgement, we expect metric score(?)
to ful-fill score(good) > score(bad):~w ?
~xgood> ~w ?
~xbad?~w ?
~xgood?
~w ?
~xbad> 0 ?~w ?
(~xgood?
~xbad) > 0 ?~w ?
(~xbad?
~xgood) < 0The two feature vectors (~xgood?
~xbad) and (~xbad?~xgood) can be considered as positive and negative in-stances for training our linear classifier.
For trainingthis model, we use Logistic Regression from the Wekatoolkit (Hall et al., 2009).3 FeaturesGenerally speaking we identify adequacy and fluencyfeatures.
For both types we devise far less sparse fea-tures than word ngrams.Adequacy features We use precision P , recallR andF1-score F as follows:Pfunc, Rfunc, Ffuncon matched function wordsPcont, Rcont, Fconton matched content wordsPall, Rall, Fallon matched words of any typePchar, Rchar, Fcharmatching of the char ngramsBy differentiating between function and non-functionwords, our metric weighs each kind of words accord-ing to importance for evaluation.
Matching characterngrams, originally proposed in (Yang et al., 2013), re-wards certain translations even if they did not get themorphology completely right.
Existing metrics usestemmers for this, but using character ngrams is inde-pendent of the availability of a good quality stemmer.Higher-order character ngrams have less risk of sparsecounts than word ngrams.
In our experiments we usedchar ngrams for n up to 6, which makes the total num-ber of adequacy features 27.Fluency features To evaluate word order we follow(Isozaki et al., 2010; Birch and Osborne, 2010) in rep-resenting reordering as a permutation pi over [1..n] andthen measuring the distance to the ideal monotone per-mutation ?1, 2, ?
?
?
, n?.
We present a novel approachbased on factorization into permutation trees (PETs)(Zhang and Gildea, 2007), and contrast it with Kendall?
(Birch and Osborne, 2010; Isozaki et al., 2010).
PETsare factorizations of permutations, which allows for anabstract and less sparse view of word order as exempli-fied next.
Kendall score was regularly shown to havehigh correlation with human judgment on distant lan-guage pairs (Isozaki et al., 2010; Birch and Osborne,2010).Features based on PETs We informally reviewPETs in order to exploit them for novel ordering fea-tures.
We refer the reader to (Zhang and Gildea, 2007)and (Maillette de Buy Wenniger and Sima?an, 2011)for a formal treatment of PETs and efficient factoriza-tion algorithms.A PET of permutation pi is a tree organization of pi?sunique, atomic building blocks, called operators.
Ev-ery operator on a PET node is an atomic permutation(not factorizing any further),3and it stands for the per-mutation of the direct children of that node.
Figure 1ashows an example PET that has one 4-branching nodewith operator ?2, 4, 1, 3?, two binary branching nodesof which one decorated with the inverted operator ?2, 1?and another with the monotone ?1, 2?.PETs have two important properties making them at-tractive for measuring order difference: firstly, orderdifference is measured on the operators ?
the atomicreordering building blocks of the permutation, and sec-ondly, the operators on higher level nodes capture hid-den ordering patterns that cannot be observed withoutfactorization.
Statistics over ordering patterns in PETsare far less sparse than word or character ngram statis-tics.Intuitively, among the atomic permutations, the bi-nary monotone operator ?1, 2?
signifies no ordering dif-ference at all, whereas the binary inverted ?2, 1?
signi-fies the shortest unit of order difference.
Operators oflength four like ?2, 4, 1, 3?
(Wu, 1997) are presumablymore complex than ?2, 1?, whereas operators longerthan four signify even more complex order difference.Therefore, we devise possible branching feature func-tions over the operator length for the nodes in PETs:?
factor 2 - with two features: ?
[ ]and ?<>(thereare no nodes with factor 3 (Wu, 1997))?
factor 4 - feature ?=4?
factor bigger than 4 - feature ?>4Consider permutations ?2, 1, 4, 3?
and ?4, 3, 2, 1?, noneof which has exactly matching ngrams beyond uni-grams.
Their PETs are in Figures 1b and 1c.
Intuitively,?2, 1, 4, 3?
is somewhat less scrambled than ?4, 3, 2, 1?because it has at least some position in correct order.These ?abstract ngrams?
pertaining to correct order-ing of full phrases could be counted using ?
[ ]whichwould recognize that on top of the PET in 1b there isa binary monotone node, unlike the PET in Figure 1cwhich has no monotone nodes at all.Even though the set of operators that describe a per-mutation is unique for the given permutation, the waysin which operators are combined (the derivation tree)is not unique.
For example, for the fully monotone3For example ?2, 4, 1, 3?
is atomic whereas ?4, 3, 2, 1?
isnot.
The former does not contain any contiguous sub-rangesof integers whereas the latter contains sub-range {2, 3, 4} inreverse order ?4, 3, 2?, which factorizes into two binary in-verting nodes cf.
Fig.
1c.203?2, 4, 1, 3?2 ?2, 1?
?1, 2?5 641 3(a) Complex PET?1, 2?
?2, 1?2 1?2, 1?4 3(b) PET with inversions?2, 1?
?2, 1?
?2, 1?4 321(c) Canonical fullyinverted PET?2, 1?
?2, 1?4 ?2, 1?3 21(d) Alternative fullyinverted PET?2, 1?
?2, 1?4 3?2, 1?2 1(e) Alternative fullyinverted PET?2, 1?4 ?2, 1?
?2, 1?3 21(f) Alternative fullyinverted PET?2, 1?4 ?2, 1?3 ?2, 1?2 1(g) Alternative fullyinverted PETFigure 1: Examples of PETspermutation ?4, 3, 2, 1?
there are 5 possible derivations(PETs) presented in Figures 1c, 1d, 1e, 1f and 1g.
Thefeatures on PETs that we described so far look at theoperators independently (they treat a derivation as aset of operators) so differenct derivations do not influ-ence the score?whichever derviation we use we willget the same feature score.
However, the number ofderivations might say something about the goodness ofthe permutation.
Similar property of permutations wasfound to be helpful earlier in (Mylonakis and Sima?an,2008) as an ITG prior for learning translation rule prob-abilities.Permutations like ?3, 2, 1, 4?
and ?2, 4, 3, 1?
have thesame set of operators, but the former factorizes intomore PETs than the latter because ?4, 3?
must groupfirst before grouping it with 2 and then 1 in ?2, 4, 3, 1?.The ?freedom to bracket?
in different ways could be asignal of better grouping of words (even if they haveinverted word order).
Hence we exploit one more fea-ture:?countthe ratio between the number of alternativePETs for the given permutation, to the number ofPETs that could be built if permutation was per-fectly grouped (fully monotone or fully inverted).Finding the number of PETs that could be built doesnot require building all PETs or encoding them in thechart.
The number can be computed directly from thecanonical left-branching PET.
Since multiple differentPETs appear only in cases when there is a sequence ofmore than one node that is either ?1, 2?
or ?2, 1?
(Zhanget al., 2008), we can use these sequences to predict thenumber of PETs that could be built.
Let X represent aset of sequences of the canonical derivation.
The num-ber of PETs is computed in the following way:#PETs =?x?XCat(|x|) (1)Cat(n) =1n+ 1(2nn)(2)whereCat(?)
is a Catalan number.
The proof for thisformula is beyond the scope of this paper.
The readercan consider the example of the PET in Figure 1c.
Thatderivation has one sequence of monotone operators oflength 3.
So the number of PETs that could be built isCat(3) = 5.4 ExperimentsWe use human judgments from the WMT tasks:WMT13 is used for training whereas WMT12 for test-ing.
The baseline is METEOR?s latest version (MichaelDenkowski and Alon Lavie, 2014), one of the best met-rics on sentence level.
To avoid contaminating the re-sults with differences with METEOR due to resources,we use the same alignment, tokenization and lower-casing (-norm in METEOR) algorithms, and the sametables of function words, synonyms, paraphrases andstemmers.Kendall ?
correlation is borrowed from WMT12(Callison-Burch et al., 2012):?
=#concordant?#discordant?#ties#concordant+ #discordant+ #ties#concordant represents the number of pairs or-dered in the same way by metric and by human,#discordant the number of opposite orderings and#ties the number of tied rankings by metric.Beside testing our full metric BEER, we perform ex-periments where we remove one kind of the followingfeatures at a time:1. char n-gram features (P, R and F-score)2. all word features (P, R and F-score for all, functionand content words),3. all function and content words features4.
all F-scores (all words, function words, contentwords, char ngrams)204metric en-cs en-fr en-de en-es cs-en fr-en de-en es-en avg ?BEER without char features 0.124 0.178 0.168 0.149 0.121 0.17 0.179 0.078 0.146BEER without all word features 0.184 0.237 0.223 0.217 0.192 0.209 0.243 0.199 0.213BEER without all F-scores 0.197 0.243 0.219 0.22 0.177 0.227 0.254 0.211 0.219METEOR 0.156 0.252 0.173 0.202 0.208 0.249 0.273 0.246 0.22BEER without PET features 0.202 0.248 0.243 0.225 0.198 0.249 0.268 0.234 0.233BEER without function words 0.2 0.245 0.231 0.227 0.189 0.268 0.267 0.253 0.235BEER without fluency features 0.201 0.248 0.236 0.223 0.202 0.257 0.283 0.243 0.237BEER without Kendall ?
0.205 0.246 0.244 0.227 0.202 0.257 0.282 0.248 0.239BEER full 0.206 0.245 0.244 0.23 0.198 0.263 0.283 0.245 0.239Table 1: Kendall ?
scores on WMT12 data5.
PET features6.
Kendall ?
features7.
all fluency features (PET and Kendall ?
)Table 1 shows the results sorted by their averageKendall ?
correlation with human judgment.5 AnalysisGiven these experimental results, we are coming backto the questions we asked in the introduction.5.1 What kind of features are more helpful forsentence level evaluation?Fluency vs.
Adequacy The fluency features play asmaller role than adequacy features.
Apparently, manySMT systems participating in this task have rather sim-ilar reordering models, trained on similar data, whichmakes the fluency features not that discriminative rel-ative to adequacy features.
Perhaps in a different ap-plication, for example MT system tuning, the reorder-ing features would be far more relevant because ignor-ing them would basically imply disregarding the im-portance of the reordering model in MT.Character vs. Word features We observe that, pre-cision, recall and F-score on character ngrams are cru-cial.
We think that this shows that less sparse featuresare important for sentence level evaluation.
The sec-ond best features are word features.
Without wordfeatures, BEER scores just below METEOR, whichsuggests that word boundaries play a role as well.
Incontrast, differentiating between function and contentwords does not seem to be important.PETs vs. Kendall ?
Despite the smaller role forreordering features we can make a few observations.Firstly, while PETs and Kendall seem to have simi-lar effect on English-Foreign cases, in all four cases ofForeign-English PETs give better scores.
We hypoth-esize that the quality of the permutations (induced be-tween system output and reference) is better for Englishthan for the other target languages.
Discarding PETfeatures has far larger impact than discarding Kendall.Most interestingly, for de-en it makes the differencein outperforming METEOR.
In many cases discardingKendall ?
improves the BEER score, likely because itconflicts with the PET features that are found more ef-fective.5.2 Is a linear model sufficient?A further insight, from our perspective, is that F-scorefeatures constitute a crucial set of features, even whenthe corresponding precision and recall features are in-cluded.
Because our model merely allows for linear in-terpolation, whereas F-score is a non-linear function ofprecision and recall, we think this suggests that a non-linear interpolation of precision and recall is useful.4By formulating the evaluation as a ranking problem it isrelatively easy to ?upgrade?
for using non-linear mod-els while using the same (or larger) set of features.5.3 Train for the language pair or only for thetarget language?All our models were trained for each language pair.This is not the case with many other metrics whichtrain their models for each target language instead oflanguage pair.
We contrast these two settings in Table2.
Training for each language pair separately does notgive significant improvement over training for the tar-get language only.
A possible reason could be that bytraining for the target language we have more trainingdata (in this case four times more).Train for cs-en fr-en de-en es-en avg ?target lang 0.199 0.257 0.273 0.248 0.244lang pair 0.198 0.263 0.283 0.245 0.247Table 2: Kendall ?
scores on WMT12 for differenttraining data5.4 BEER vs. METEORThe results across individual language pairs are mostlyconsistent with the averages with a few exceptions.BEER outperforms METEOR in five out of eight lan-guage pairs, ties at one (the difference is only 0.001 ones-en) and loses in two (en-fr and cs-en).
In some casesBEER is better than METEOR by a large margin (see,e.g., en-cs, en-de).4Interestingly, METEOR tunes ?
in F?.2056 ConclusionIn this work we show that combining less sparse fea-tures at the sentence level into a linear model that istrained on ranking we can obtain state-of-the-art re-sults.
The analysis of the results shows that features oncharacter ngrams are crucial, besides the standard wordlevel features.
The reordering features, while ratherimportant, are less effective within this WMT task, al-beit the more abstract PET features have larger impactthan the often used Kendall.
Good performance of F-score features leads to the conclusion that linear modelsmight not be sufficient for modeling human sentencelevel ranking and to learn the right relation betweenprecision and recall it could be worthwhile exploringnon-linear models.AcknowledgmentsThis work is supported by STW grant nr.
12271 andNWO VICI grant nr.
277-89-002.
We also thank TAUSand the other DatAptor project User Board members.ReferencesAlexandra Birch and Miles Osborne.
2010.
LRscorefor Evaluating Lexical and Reordering Quality inMT.
In Proceedings of the Joint Fifth Workshop onStatistical Machine Translation and MetricsMATR,pages 327?332, Uppsala, Sweden, July.
Associationfor Computational Linguistics.Chris Callison-Burch, Philipp Koehn, Christof Monz,Matt Post, Radu Soricut, and Lucia Specia.
2012.Findings of the 2012 Workshop on Statistical Ma-chine Translation.
In Proceedings of the SeventhWorkshop on Statistical Machine Translation, pages10?51, Montr?eal, Canada, June.
Association forComputational Linguistics.Mark Hall, Eibe Frank, Geoffrey Holmes, BernhardPfahringer, Peter Reutemann, and Ian H. Witten.2009.
The WEKA Data Mining Software: AnUpdate.
SIGKDD Explor.
Newsl., 11(1):10?18,November.Ralf Herbrich, Thore Graepel, and Klaus Obermayer.1999.
Support Vector Learning for Ordinal Regres-sion.
In In International Conference on ArtificialNeural Networks, pages 97?102.Mark Hopkins and Jonathan May.
2011.
Tuning asRanking.
In Proceedings of the 2011 Conference onEmpirical Methods in Natural Language Processing,pages 1352?1362, Edinburgh, Scotland, UK., July.Association for Computational Linguistics.Hideki Isozaki, Tsutomu Hirao, Kevin Duh, KatsuhitoSudoh, and Hajime Tsukada.
2010.
AutomaticEvaluation of Translation Quality for Distant Lan-guage Pairs.
In Proceedings of the 2010 Conferenceon Empirical Methods in Natural Language Process-ing, EMNLP ?10, pages 944?952, Stroudsburg, PA,USA.
Association for Computational Linguistics.Hang Li.
2011.
Learning to Rank for Information Re-trieval and Natural Language Processing.
SynthesisLectures on Human Language Technologies.
Mor-gan & Claypool Publishers.Gideon Maillette de Buy Wenniger and Khalil Sima?an.2011.
Hierarchical Translation Equivalence overWord Alignments.
In ILLC Prepublication Series,PP-2011-38.
University of Amsterdam.Michael Denkowski and Alon Lavie.
2014.
MeteorUniversal: Language Specific Translation Evalua-tion for Any Target Language.
In Proceedings of theACL 2014 Workshop on Statistical Machine Transla-tion.Markos Mylonakis and Khalil Sima?an.
2008.Phrase Translation Probabilities with ITG Priors andSmoothing as Learning Objective.
In Proceedings ofthe 2008 Conference on Empirical Methods in Natu-ral Language Processing, pages 630?639, Honolulu,USA, October.
Association for Computational Lin-guistics.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
BLEU: A Method for AutomaticEvaluation of Machine Translation.
In Proceedingsof the 40th Annual Meeting on Association for Com-putational Linguistics, ACL ?02, pages 311?318,Stroudsburg, PA, USA.
Association for Computa-tional Linguistics.Milo?s Stanojevi?c and Khalil Sima?an.
2014.
BEER:BEtter Evaluation as Ranking.
In Proceedings of theNinth Workshop on Statistical Machine Translation,pages 414?419, Baltimore, Maryland, USA, June.Association for Computational Linguistics.Dekai Wu.
1997.
Stochastic inversion transductiongrammars and bilingual parsing of parallel corpora.Computational linguistics, 23(3):377?403.Muyun Yang, Junguo Zhu, Sheng Li, and Tiejun Zhao.2013.
Fusion of Word and Letter Based Metricsfor Automatic MT Evaluation.
In Proceedings ofthe Twenty-Third International Joint Conference onArtificial Intelligence, IJCAI?13, pages 2204?2210.AAAI Press.Yang Ye, Ming Zhou, and Chin-Yew Lin.
2007.
Sen-tence Level Machine Translation Evaluation As aRanking Problem: One Step Aside from BLEU.
InProceedings of the Second Workshop on StatisticalMachine Translation, StatMT ?07, pages 240?247,Stroudsburg, PA, USA.
Association for Computa-tional Linguistics.Hao Zhang and Daniel Gildea.
2007.
Factorization ofsynchronous context-free grammars in linear time.In In NAACL Workshop on Syntax and Structure inStatistical Translation (SSST.Hao Zhang, Daniel Gildea, and David Chiang.2008.
Extracting Synchronous Grammar RulesFrom Word-Level Alignments in Linear Time.
InProceedings of the 22nd International Conferenceon Computational Linguistics (COLING-08), pages1081?1088, Manchester, UK.206
