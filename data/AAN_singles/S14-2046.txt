Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 284?288,Dublin, Ireland, August 23-24, 2014.FBK-TR: Applying SVM with Multiple Linguistic Features forCross-Level Semantic SimilarityNgoc Phuoc An VoFondazione Bruno KesslerUniversity of TrentoTrento, Italyngoc@fbk.euTommaso CaselliTrentoRISETrento, Italyt.caselli@trentorise.euOctavian PopescuFondazione Bruno KesslerTrento, Italypopescu@fbk.euAbstractRecently, the task of measuring seman-tic similarity between given texts hasdrawn much attention from the NaturalLanguage Processing community.
Espe-cially, the task becomes more interestingwhen it comes to measuring the seman-tic similarity between different-sized texts,e.g paragraph-sentence, sentence-phrase,phrase-word, etc.
In this paper, we, theFBK-TR team, describe our system par-ticipating in Task 3 "Cross-Level Seman-tic Similarity", at SemEval 2014.
We alsoreport the results obtained by our system,compared to the baseline and other partic-ipating systems in this task.1 IntroductionMeasuring semantic text similarity has become ahot trend in NLP as it can be applied to othertasks, e.g.
Information Retrieval, Paraphrasing,Machine Translation Evaluation, Text Summariza-tion, Question and Answering, and others.
Severalapproaches proposed to measure the semantic sim-ilarity between given texts.
The first approach isbased on vector space models (VSMs) (Meadow,1992).
A VSM transforms given texts into "bag-of-words" and presents them as vectors.
Then, itdeploys different distance metrics to compute thecloseness between vectors, which will return asthe distance or similarity between given texts.
Thenext well-known approach is using text-alignment.By assuming that two given texts are semanticallysimilar, they could be aligned on word or phraselevels.
The alignment quality can serve as a simi-larity measure.
"It typically pairs words from thetwo texts by maximizing the summation of theThis work is licensed under a Creative Commons At-tribution 4.0 International Licence.
Page numbers and pro-ceedings footer are added by the organisers.
Licence details:http://creativecommons.org/licenses/by/4.0/word similarity of the resulting pairs" (Mihalceaet al., 2006).
In contrast, the third approach usesmachine learning techniques to learn models builtfrom different lexical, semantic and syntactic fea-tures and then give predictions on degree of simi-larity between given texts (?ari?c et al., 2012).At SemEval 2014, the Task 3 "Cross-Level Se-mantic Similarity" (Jurgens et al., 2014) is to eval-uate the semantic similarity across different sizesof texts, in particular, a larger-sized text is com-pared to a smaller-sized one.
The task consistsof four types of semantic similarity comparison:paragraph to sentence, sentence to phrase, phraseto word, and word to sense.
The degree of similar-ity ranges from 0 (different meanings) to 4 (simi-lar meanings).
For evaluation, systems were eval-uated, first, within comparison type and second,across all comparison types.
Two methods areused to evaluate between system outputs and goldstandard (human annotation), which are Pearsoncorrelation and Spearman?s rank correlation (rho).The FBK-TR team participated in this task withthree different runs.
In this paper, we present aclear and comprehensive description of our sys-tem which obtained competitive results.
Our mainapproach is using machine learning technique tolearn models from different lexical and semanticfeatures from train corpora to make prediction onthe test corpora.
We used support vector machine(SVM) regression model to solve the task.The remainder of the paper is organized as fol-lows.
Section 2 presents the system overview.Sections 3, 4 and 5 describe the Semantic WordSimilarity, String Similarity and other features, re-spectively.
Section 6 discusses about SVM ap-proach.
Section 7 presents the experiment settingsfor each subtask.
Finally, Sections 8 and 9 presentthe evaluation and conclusion.284Figure 1: System Overview.2 System OverviewOur system was built on different linguistic fea-tures as shown in Figure 1.
By constructing apipeline system, each linguistic feature can beused independently or together with others to mea-sure the semantic similarity of given texts as wellas to evaluate the significance of each feature tothe accuracy of system?s predictions.
On top ofthis, the system is expandable and scalable foradopting more useful features aiming for improv-ing the accuracy.3 Semantic Word Similarity MeasuresAt the lexical level, we built a simple, yet effec-tive Semantic Word Similarity model consisting ofthree components: WordNet similarity, Wikipediarelatedness and Latent Semantic Analysis (LSA).These components played important and compli-mentary roles to each other.3.1 Data ProcessingWe used the TreeTagger tool (Schmid, 1994) toextract Part-of-Speech (POS) from each giventext, then tokenize and lemmatize it.
On the basisof the POS tags, we only picked lemmas of con-tent words (Nouns and Verbs) from the given textsand then paired them up regarding to similar POStags.3.2 WordNet Similarity and LevenshteinDistanceWordNet (Fellbaum, 1999) is a lexical databasefor the English language in which words aregrouped into sets of synonyms (namely synsets,each expressing a distinct concept) to provideshort, general definitions, and record the vari-ous semantic relations between synsets.
We usedPerdersen?s package WordNet:Similarity (Peder-sen et al., 2004) to obtain similarity scores forthe lexical items covered in WordNet.
Similarityscores have been computed by means of the Linmeasure (Lin, 1998).
The Lin measure is built onResnik?s measure of similarity (Resnik, 1995):Simlin=2 ?
IC(LCS)IC(concept1) + IC(concept2)(1)where IC(LCS) is the information content (IC) ofthe least common subsumer (LCS) of two con-cepts.To overcome the limit in coverage of WordNet,we applied the Levenshtein distance (Levenshtein,1966).
The distance between two words is definedby the minimum number of operations (insertions,deletions and substitutions) needed to transformone word into the other.3.3 Wikipedia RelatednessWikipedia Miner (Milne and Witten, 2013) is aJava-based package developed for extracting se-mantic information from Wikipedia.
Through ourexperiments, we observed that Wikipedia related-ness plays an important role for providing extrainformation to measure the semantic similarity be-tween words.
We used the package WikipediaMiner from University of Waikato (New Zealand)to extract additional relatedness scores betweenwords.3.4 Latent Semantic Analysis (LSA)We also took advantage from corpus-based ap-proaches to measure the semantic similarity be-tween words by using Latent Semantic Analysis(LSA) technique (Landauer et al., 1998).
LSA as-sumes that similar and/or related words in termsof meaning will occur in similar text contexts.
Ingeneral, a LSA matrix is built from a large cor-pus.
Rows in the matrix represent unique wordsand columns represent paragraphs or documents.The content of the matrix corresponds to the wordcount per paragraph/document.
Matrix size is thenreduced by means of Single Value Decomposition(SVD) technique.
Once the matrix has been ob-tained, similarity and/or relatedness between thewords is computed by means of cosine values(scaled between 0 and 1) for each word vectorin the matrix.
Values close to 1 are assumed to285be very similar/related, otherwise dissimilar.
Wetrained our LSA model on the British NationalCorpus (BNC)1and Wikipedia2corpora.4 String Similarity MeasuresThe Longest Common Substring (LCS) is thelongest string in common between two or morestrings.
Two given texts are considered similar ifthey are overlapping/covering each other (e.g sen-tence 1 covers a part of sentence 2, or otherwise).We implemented a simple algorithm to extract theLCS between two given texts.
Then we divided theLCS length by the product of normalized lengthsof two given texts and used it as a feature.4.1 Analysis Before and After LCSAfter extracting the LCS between two given texts,we also considered the similarity for the parts be-fore and after the LCS.
The similarity between thetext portions before and after the LSC has been ob-tained by means of the Lin measure and the Lev-enshtein distance.5 Other FeaturesTo take into account other levels of analysis for se-mantic similarity between texts, we extended ourfeatures by means of topic modeling and NamedEntities.5.1 Topic Modeling (Latent DirichletAllocation - LDA)Topic modeling is a generative model of docu-ments which allows to discover topics embeddedin a document collection and their balance in eachdocument.
If two given texts are expressing thesame topic, they should be considered highly sim-ilar.
We applied topic modeling, particularly, La-tent Dirichlet allocation (LDA) (Blei et al., 2003)to predict the topics expressed by given texts.The MALLET topic model package (McCal-lum, 2002) is a Java-based tool used for inferringhidden "topics" in new document collections us-ing trained models.
We used Mallet topic model-ing tool to build different models using BNC andWikipedia corpora.We noticed that, in LDA, the number of top-ics plays an important role to fine grained predic-tions.
Hence, we built different models for differ-ent numbers of topics, from minimum 20 topics to1http://www.natcorp.ox.ac.uk2http://en.wikipedia.org/wiki/Wikipedia:Database_downloadmaximum 500 topics (20, 50, 100, 150, 200, 250,300, 350, 400, 450 and 500).
From the proportionvectors (distribution of documents over topics) ofgiven texts, we applied three different measures tocompute the distance between each pair of texts,which are Cosine similarity, Kullback-Leibler andJensen-Shannon divergences (Gella et al., 2013).5.2 Named-Entity Recognition (NER)NER aims at identifying and classifying entitiesin a text with respect to a predefined set of cate-gories such as person names, organizations, loca-tions, time expressions, quantities, monetary val-ues, percentages, etc.
By exploring the trainingset, we observed that there are lot of texts in thistask containing named entities.
We deployed theStanford Named Entity Recognizer tool (Finkel etal., 2005) to extract the similar and overlappingnamed entities between two given texts.
Then wedivided the number of similar/overlapping namedentities by the sum length of two given texts.6 Support Vector Machines (SVMs)Support vector machine (SVM) (Cortes and Vap-nik, 1995) is a type of supervised learning ap-proaches.
We used the LibSVM package (Changand Lin, 2011) to learn models from the differentlinguistic features described above.
However, inSVM the problem of finding optimal kernel pa-rameters is critical and important for the learningprocess.
Hence, we used practical advice (Hsu etal., 2003) for data scaling and a grid-search pro-cess for finding the optimal parameters (C andgamma) for building models.
We trained the SVMmodels in a regression framework.7 Experiment SettingsFor subtasks paragraph-to-sentence and sentence-to-phrase, since the length between two units iscompletely different, we decided, first to applytopic model to identify if two given texts are ex-pressing a same topic.
Furthermore, named enti-ties play an important role in these subtasks.
How-ever, as there are many named entities which arenot English words and cannot be identified by theNER tool, we developed a program to detect andidentify common words occurring in both giventexts.
Then we continued to extract other lexicaland semantic features to measure the similarity be-tween the two texts.286Team Para2Sent Para2Sent(Pearson) (Spearman)UNAL-NLP, run2 (ranked 1st) 0.837 0.820ECNU, run1(ranked 1st) 0.834 0.821FBK-TR, run2 0.77 0.775FBK-TR, run3 0.759 0.770FBK-TR, run1 0.751 0.759Baseline (LCS) 0.527 0.613Table 1: Results for paragraph-to-sentence.Team Sent2Phr Sent2Phr(Pearson) (Spearman)Meerkat_Mafia, 0.777 0.760SuperSaiyan (ranked 1st)FBK-TR, run3 0.702 0.695FBK-TR, run1 0.685 0.681FBK-TR, run2 0.648 0.642Baseline (LCS) 0.562 0.626Table 2: Results for sentence-to-phrase.For the subtask word-to-sense, we used the Se-mantic Word Similarity model which consists ofthree components: WordNet similarity, Wikipediarelatedness and LSA similarity (described in sec-tion 3).
For phrase-to-word, we extracted allglosses of the given word, then computed the simi-larity between the given phrase and each extractedgloss.
Finally, we selected the highest similarityscore for result.8 EvaluationsAs a result, we report our performance in the foursubtasks as follows.8.1 Subtasks: Paragraph-to-Sentence andSentence-to-PhraseThe evaluation results using Pearson and Spear-man correlations show the difference between oursystem and best system in these two subtasks inthe Tables 1 and 2.Team Para2Sent Sent2Phr Phr2Word Word2Sens SumSimCompass 0.811 0.742 0.415 0.356 2.324(ranked 1st)FBK-TR 0.759 0.702 0.305 0.155 1.95Baseline 0.527 0.562 0.165 0.109 1.363Table 3: Overall result using Pearson.Team Para2Sent Sent2Phr Phr2Word Word2Sens SumSimCompass 0.801 0.728 0.424 0.344 2.297(ranked 1st)FBK-TR 0.770 0.695 0.298 0.150 1.913Baseline 0.613 0.626 0.162 0.130 1.528Table 4: Overall result using Spearman.8.2 Subtasks: Phrase-to-Word andWord-to-SenseEven though we did not submit the results asthey looked very low, we report the scores forthe phrase-to-word and word-to-sense subtasks.
Inthe phrase-to-word subtask, we obtained a Pearsonscore of 0.305 and Spearman value of 0.298.
Asfor the word-to-sense subtask, we scored 0.155 forPearson and 0.150 for Spearman.Overall, with the submitted results for two sub-tasks described in Section 8.1, our system?s runsranked 20th, 21st and 22nd among 38 participat-ing systems.
However, by taking into account theun-submitted results for the two other subtasks,our best run obtained 1.95 (Pearson correlation)and 1.913 (Spearman correlation), which can beranked in the top 10 among 38 systems (figuresare reported in Table 3 and 4).9 Conclusions and Future WorkIn this paper, we describe our system participatingin the Task 3, at SemEval 2014.
We present a com-pact system using machine learning approach (par-ticularly, SVMs) to learn models from a set of lex-ical and semantic features to predict the degree ofsimilarity between different-sized texts.
Althoughwe only submitted the results for two out of foursubtasks, we obtained competitive results amongthe other participants.
For future work, we areplanning to increase the number of topics in LDA,as more fine-grained topics should allow predict-ing better similarity scores.
Finally, we will inves-tigate more on the use of syntactic features.ReferencesDavid M Blei, Andrew Y Ng, and Michael I Jordan.2003.
Latent Dirichlet Allocation.
The Journal ofMachine Learning research, 3:993?1022.Chih-Chung Chang and Chih-Jen Lin.
2011.
LIB-SVM: A Library for Support Vector Machines.ACM Transactions on Intelligent Systems and Tech-nology (TIST), 2(3):27.Corinna Cortes and Vladimir Vapnik.
1995.
Support-287Vector Networks.
Machine learning, 20(3):273?297.Christiane Fellbaum.
1999.
WordNet.
Wiley OnlineLibrary.Jenny Rose Finkel, Trond Grenager, and ChristopherManning.
2005.
Incorporating Non-local Informa-tion into Information Extraction Systems by GibbsSampling.
In Proceedings of the 43rd Annual Meet-ing on Association for Computational Linguistics,pages 363?370.Spandana Gella, Bahar Salehi, Marco Lui, KarlGrieser, Paul Cook, and Timothy Baldwin.
2013.Unimelb_nlp-core: Integrating predictions frommultiple domains and feature sets for estimating se-mantic textual similarity.
Atlanta, Georgia, USA,page 207.Chih-Wei Hsu, Chih-Chung Chang, Chih-Jen Lin, et al.2003.
A Practical Guide to Support Vector Classifi-cation.David Jurgens, Mohammad Taher Pilehvar, andRoberto Navigli.
2014.
Semeval-2014 Task 3:Cross-Level Semantic Similarity.
In Proceedings ofthe 8th International Workshop on Semantic Evalu-ation (SemEval-2014) August 23-24, 2014, Dublin,Ireland.Thomas K Landauer, Peter W Foltz, and Darrell La-ham.
1998.
An Introduction to Latent SemanticAnalysis.
Discourse processes, 25(2-3):259?284.Vladimir I Levenshtein.
1966.
Binary Codes Capableof Correcting Deletions, Insertions and Reversals.In Soviet physics doklady, volume 10, page 707.Dekang Lin.
1998.
An Information-Theoretic Defini-tion of Similarity.
In ICML, volume 98, pages 296?304.Andrew Kachites McCallum.
2002.
Mallet: A Ma-chine Learning for Language Toolkit.Charles T Meadow.
1992.
Text Information RetrievalSystems.
Academic Press, Inc., Orlando, FL, USA.Rada Mihalcea, Courtney Corley, and Carlo Strappa-rava.
2006.
Corpus-based and Knowledge-basedMeasures of Text Semantic Similarity.
In AAAI, vol-ume 6, pages 775?780.David Milne and Ian H Witten.
2013.
An Open-Source Toolkit for Mining Wikipedia.
Artificial In-telligence, 194:222?239.Ted Pedersen, Siddharth Patwardhan, and Jason Miche-lizzi.
2004.
Wordnet::similarity - Measuring the Re-latedness of Concepts.
In Demonstration Papers atHLT-NAACL 2004, pages 38?41.Philip Resnik.
1995.
Using Information Content toEvaluate Semantic Similarity in a Taxonomy.
arXivpreprint cmp-lg/9511007.Frane ?ari?c, Goran Glava?, Mladen Karan, Jan ?najder,and Bojana Dalbelo Ba?i?c.
2012.
Takelab: Sys-tems for Measuring Semantic Text Similarity.
InProceedings of the First Joint Conference on Lexicaland Computational Semantics-Volume 1: Proceed-ings of the main conference and the shared task, andVolume 2: Proceedings of the Sixth InternationalWorkshop on Semantic Evaluation, pages 441?448.Helmut Schmid.
1994.
Probabilistic Part-of-SpeechTagging Using Decision Trees.
In Proceedings ofinternational conference on new methods in lan-guage processing, volume 12, pages 44?49.
Manch-ester, UK.288
