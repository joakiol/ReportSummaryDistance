Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 1: Proceedings of the Main Conferenceand the Shared Task, pages 169?175, Atlanta, Georgia, June 13-14, 2013. c?2013 Association for Computational LinguisticsUNIBA-CORE: Combining Strategies for Semantic Textual SimilarityAnnalina Caputo Pierpaolo BasileDepartment of Computer ScienceUniversity of Bari Aldo MoroVia E. Orabona, 4 - 70125 Bari, Italy{annalina.caputo, pierpaolo.basile, giovanni.semeraro}@uniba.itGiovanni SemeraroAbstractThis paper describes the UNIBA participationin the Semantic Textual Similarity (STS) coretask 2013.
We exploited three different sys-tems for computing the similarity between twotexts.
A system is used as baseline, which rep-resents the best model emerged from our pre-vious participation in STS 2012.
Such systemis based on a distributional model of seman-tics capable of taking into account also syn-tactic structures that glue words together.
Inaddition, we investigated the use of two dif-ferent learning strategies exploiting both syn-tactic and semantic features.
The former usesensemble learning in order to combine thebest machine learning techniques trained on2012 training and test sets.
The latter tries toovercome the limit of working with differentdatasets with varying characteristics by select-ing only the more suitable dataset for the train-ing purpose.1 IntroductionSemantic Textual Similarity is the task of comput-ing the similarity between any two given texts.
Thetask, in its core formulation, aims at capturing thedifferent kinds of similarity that emerge from texts.Machine translation, paraphrasing, synonym substi-tution or text entailment are some fruitful methodsexploited for this purpose.
These techniques, alongwith other methods for estimating the text similar-ity, were successfully employed via machine learn-ing approaches during the 2012 task.However, the STS 2013 core task (Agirre et al2013) differs from the 2012 formulation in that itprovides a test set which is similar to the training,but not drawn from the same set of data.
Hence,in order to generalize the machine learning modelstrained on a group of datasets, we investigate the useof combination strategies.
The objective of combi-nation strategies, known under the name of ensem-ble learning, is that of reducing the bias-variancedecomposition through reducing the variance error.Hence, this class of methods should be more ro-bust with respect to previously unseen data.
Amongthe several ensemble learning alternatives, we ex-ploit the stacked generalization (STACKING) algo-rithm (Wolpert, 1992).
Moreover, we investigate theuse of a two-steps learning algorithm (2STEPSML).In this method the learning algorithm is trained us-ing only the dataset most similar to the instance tobe predicted.
The first step aims at predicting thedataset more similar to the given pair of texts.
Thenthe second step makes use of the previously trainedalgorithm to predict the similarity value.
The base-line for the evaluation is represented by our best sys-tem (DSM PERM) resulting from our participationin the 2012 task.
After introducing the general mod-els behind our systems in Section 2, Section 3 de-scribes the evaluation setting of our systems alongwith the experimental results.
Then, some conclu-sions and remarks close the paper.2 General Models2.1 Dependency Encoding via VectorPermutationsDistributional models are effective methods for rep-resenting word paradigmatic relations in a simple169way through vector spaces (Mitchell and Lapata,2008).
These spaces are built taking into accountthe word context, hence the resulting vector repre-sentation is such that the distance between vectorsreflects their similarity.
Although several definitionsof context are possible (e.g.
a sliding window oftext, the word order or syntactic dependencies), intheir plain definition these kinds of models accountfor just one type of context at a time.
To overcomethis limitation, we exploit a method to encode moredefinitions of context in the same vector exploitingthe vector permutations (Caputo et al 2012).
Thistechnique, which is based on Random Indexing asa means for computing the distributional model, isbased on the idea that when the components of ahighly sparse vector are shuffled, the resulting vec-tor is nearly orthogonal to the original one.
Hence,vector permutation represents a way for generat-ing new random vectors in a predetermined manner.Different word contexts can be encoded using dif-ferent types of permutations.
In our distributionalmodel system (DSM PERM), we encode the syn-tactic dependencies between words rather than themere co-occurrence information.
In this way, word-vector components bear the information about bothco-occurring and syntactically related words.
In thisdistributional space, a text can be easily representedas the superposition of its words.
Then, the vec-tor representation of a text is given by adding thevector representation of its words, and the similaritybetween texts come through the cosine of the anglebetween their vector representations.2.2 StackingStacking algorithms (Wolpert, 1992) are a way ofcombining different types of learning algorithms re-ducing the variance of the system.
In this model,the meta-learner tries to predict the real value ofan instance combining the outputs of other machinelearning methods.Figure 1 shows how the learning process takesplace.
The level-0 represents the ensemble of dif-ferent models to be trained on the same dataset.
Thelevel-0 outputs build up the level-1 dataset: an in-stance at this level is represented by the numericvalues predicted by each level-0 model along withthe gold standard value.
Then, the objective of thelevel-1 learning model is to learn how to combinethe level-0 outputs in order to provide the best pre-diction.level-0level-1model1 model2 ?
?
?
modelnmeta-learnerpredictionFigure 1: Stacking algorithm2.3 Two steps learning algorithmGiven an ensemble of datasets with different charac-teristics, this method is based on the idea that wheninstances come from a specific dataset, the learn-ing algorithm trained on that dataset outperforms thesame algorithm trained on the whole ensemble.Hence, the two steps algorithm tries to overcomethe problem of dealing with different datasets hav-ing different characteristics through a classificationmodel.step-1step-2dataset1 dataset2 ?
?
?
datasetnclassifierinputoutput: dataset classlearning algorithmpredicted datasetpredictionFigure 2: Two steps machine learning algorithmIn the first step (Figure 2), a different class is as-signed to each dataset.
The classifier is trained on170a set of instances whose classes correspond to thedataset numbers.
Then, given a new instance theoutput of this step will be the dataset to be usedfor training the learning algorithm in the step 2.
Inthe second step, the learning algorithm is trained onthe dataset choose in the first step.
The output ofthis step is the predicted similarity between the twotexts.
Through these steps, it is possible to selectthe dataset with the characteristics more similar toa given instance, and exploit just this set of data forlearning the algorithm.2.4 FeaturesBoth STACKING and 2STEPSML systems rely onseveral kinds of features, which vary from lexical tosemantic ones.
Features are grouped in seven mainclasses, as follows:1.
Character/string/annotation-based features:the length of the longest common contiguoussubstring between the texts; the Jaccard indexof both tokens and lemmas; the Levenshteindistance between texts; the normalized numberof common 2-grams, 3-grams and 4-grams; thetotal number of tokens and characters; the dif-ference in tokens and characters between texts;the normalized difference with respect to themax text length in tokens and characters be-tween texts.
Exploiting other linguistic anno-tations extracted by Stanford CoreNLP1, wecompute the Jaccard index between PoS-tagsand named entities.
Using WordNet we extractthe Jaccard index between the first sense and itssuper-sense tag.2.
Textual Similarity-based features: a set of fea-tures based on the textual similarity proposedby Mihalcea (Mihalcea et al 2006).
Given twotexts T1 and T2 the similarity is computed asfollows:sim(T1, T2) =12(?w?T1 maxSim(w, T2)?w?T1 idf(w)+?w?T2 maxSim(w, T1)?w?T2 idf(w))(1)1Available at: http://nlp.stanford.edu/software/corenlp.shtmlWe adopt several similarity measures usingsemantic distributional models (see Section2.5), the Resnik?s knowledge-based approach(Resnik, 1995) and the point-wise mutual infor-mation as suggested by Turney (Turney, 2001)computed on British National Corpus2.
For allthe features, the idf is computed relying onUKWaC corpus3 (Baroni et al 2009).3.
Head similarity-based features: this measuretakes into account the maximum similarity be-tween the roots of each text.
The roots are ex-tracted using the dependency parser providedby Stanford CoreNLP.
The similarity is com-puted according to the distributional semanticmodels proposed in Section 2.5.4.
ESA similarity: computes the similaritybetween texts using the Explicit SemanticAnalysis (ESA) approach (Gabrilovich andMarkovitch, 2007).
For each text we extract theESA vector built using the English Wikipedia,and then we compute the similarity as the co-sine similarity between the two ESA vectors.5.
Paraphrasing features: this is a very simplemeasure which counts the number of possi-ble paraphrasings belonging to the two texts.Given two texts T1 and T2, for each token in T1a list of paraphrasings is extracted using a dic-tionary4.
If T2 contains one of the paraphrasingin the list, the score is incremented by one.
Thefinal score is divided by the number of tokensin T1.
The same score is computed taking intoaccount T2.
Finally, the two score are addedand divided by 2.6.
Greedy Lemma Aligning Overlap features:this measure computes the similarity betweentexts using the semantic alignment of lemmasas proposed by S?aric?
et al(2012).
In orderto compute the similarity between lemmas, weexploit the distributional semantic models de-scribed in Section 2.5.2Available at: http://www.natcorp.ox.ac.uk/3Available at: http://wacky.sslmit.unibo.it/4English Thesaurus for StarDict available athttps://aur.archlinux.org/packages/stardict-thesaurus-ee/1717.
Compositional features: we build several simi-larity features using the distributional semanticmodels described in Section 2.5 and a compo-sitional operator based on sum.
This approachis thoroughly explained in Section 2.62.5 Distributional semantic modelsIn several features proposed in our approaches, thesimilarity between words is computed using Dis-tributional Semantic Models.
These models repre-sent word meanings through contexts: the differentmeanings of a word can be accounted for by look-ing at the different contexts wherein the word oc-curs.
This insight can beautifully be expressed bythe geometrical representation of words as vectorsin a semantic space.
Each term is represented as avector whose components are contexts surroundingthe term.
In this way, the meaning of a term acrossa corpus is thoroughly conveyed by the contexts itappears in, where a context may typically be the setof co-occurring words in a document, in a sentenceor in a window of surrounding terms.In particular, we take into account two mainclasses of models: Simple Distributional Spaces andStructured Semantic Spaces.
The former considersas context the co-occurring words, the latter takesinto account both co-occurrence and syntactic de-pendency between words.Simple Distributional Spaces rely on LatentSemantic Analysis (LSA) and Random Indexing(RI) in order to reduce the dimension of the co-occurrences matrix.
Moreover, we use an approachwhich applies LSA to the matrix produced by RI.Structured Semantic Spaces are based on twotechniques to encode syntactic information into thevector space.
The first approach uses the vector per-mutation of random vector in RI to encode the syn-tactic role (head or dependent) of a word.
The sec-ond method is based on Holographic Reduced Rep-resentation, in particular using convolution betweenvectors, to encode syntactic information.Adopting distributional semantic models, eachword can be represented as a vector in a geomet-ric space.
The similarity between two words can beeasily computed taking into account the cosine sim-ilarity between word vectors.All models are described in Basile et al(2012).2.6 Compositional featuresIn Distributional Semantic Models, given the vectorrepresentations of two words, it is always possibleto compute their similarity as the cosine of the anglebetween them.However, texts are composed by several terms,so in order to compute the similarity between themwe need a method to compose words occurring inthese texts.
It is possible to combine words throughthe vector addition (+).
This operator is similar tothe superposition defined in connectionist systems(Smolensky, 1990), and corresponds to the point-wise sum of components:p = u + v (2)where pi = ui + viThe addition is a commutative operator, whichmeans that it does not take into account any orderor underlying structures existing between words.
Inthis first study, we do not exploit more complexmethods to combine word vectors.
We plan to in-vestigate them in future work.Given a text p, we denote with p its vector repre-sentation obtained applying addition operator (+) tothe vector representation of terms it is composed of.Furthermore, it is possible to compute the similar-ity between two texts exploiting the cosine similaritybetween vectors.Formally, if a = a1, a2...an and b = b1, b2...bmare two texts, we build two vectors a and b whichrepresent respectively the two texts in a semanticspace.
Vector representations for the two texts arebuilt applying the addition operator to the vector rep-resentation of words belonging to them:a = a1 + a2 + .
.
.
+ anb = b1 + b2 .
.
.
+ bm(3)The similarity between a and b is computed as thecosine similarity between them.3 Experimental evaluationSemEval-2013 STS is the second attempt to providea ?unified framework for the evaluation of modularsemantic textual similarity and to characterize theirimpact on NLP applications?.
The task consistsin computing the similarity between pair of texts,172returning a similarity score.
The test set is com-posed by data coming from the following datasets:news headlines (headlines); mapping of lexical re-sources from Ontonotes to Wordnet (OnWN) andfrom FrameNet to WordNet (FNWN); and evalua-tion of machine translation (SMT).The training data for STS-2013 is made up bytraining and testing data from the previous editionof STS-2012 task.
During the 2012 edition, STSprovided participants with three training data: MSR-Paraphrase, MSR-Video, STMeuropar; and five test-ing data: MSR-Paraphrase, MSR-Video, STMeu-ropar, SMTnews and OnWN.
It is important to notethat part of 2012 test sets were made up from thesame sources of the training sets.
On the otherhand, STS-2013 training and testing are very differ-ent, making the prediction task a bit harder.Humans rated each pair of texts with values from0 to 5.
The evaluation is performed by compar-ing the humans scores against system performancethrough Pearson?s correlation with the gold standardfor the four datasets.3.1 System setupFor the evaluation, we built the distributional spacesusing the WaCkypedia EN corpus5.
WaCkype-dia EN is based on a 2009 dump of the EnglishWikipedia (about 800 million tokens) and includesinformation about: part-of-speech, lemma and a fulldependency parsing performed by MaltParser (Nivreet al 2007).
The structured spaces described inSubsections 2.1 and 2.5 are built exploiting infor-mation about term windows and dependency pars-ing supplied by WaCkypedia.
The total number ofdependencies amounts to about 200 million.The RI system is implemented in Java and re-lies on some portions of code publicly available inthe Semantic Vectors package (Widdows and Fer-raro, 2008), while for LSA we exploited the publiclyavailable C library SVDLIBC6.We restricted the vocabulary to the 50,000 mostfrequent terms, with stop words removal and forc-ing the system to include terms which occur in thedataset.Semantic space building involves some parame-5http://wacky.sslmit.unibo.it/doku.php?id=corpora6http://tedlab.mit.edu/ dr/SVDLIBC/ters.
In particular, each semantic space needs to setup the dimension k of the space.
All spaces use adimension of 500 (resulting in a 50,000?500 ma-trix).
The number of non-zero elements in the ran-dom vector is set to 10.
When we apply LSA to theoutput space generated by the Random Indexing wehold all the 500 dimensions, since during the tuningwe observed a drop in performance when a lowerdimension was set.
The co-occurrence distance wbetween terms was set up to 4.In order to compute the similarity betweenthe vector representations of text using UNIBA-DSM PERM, we used the cosine similarity, andthen we multiplied by 5 the obtained value.The two supervised methods, UNIBA-2STEPMLand UNIBA-STACKING, are developed in Javausing Weka7 to implement the learning algo-rithms.
Regarding the stacking approach (UNIBA-STACKING) we used for the level-0 the follow-ing models: Gaussian Process with polynomial ker-nel, Gaussian Process with RBF kernel, Linear Re-gression, Support Vector regression with polynomialkernel, and decision tree.
The level-1 model usesa Gaussian Process with RBF kernel.
In the firststep of UNIBA-2STEPML we adopt Support Vec-tor Machine, while in the second one we use Sup-port Vector Machine for regression.
In both steps,the RBF-Kernel is used.
Features are normalizedremoving non alphanumerics characters.
In all thelearning algorithms, we use the default parametersset by Weka.
As future work, we plan to perform atuning step in order to set the best parameters.The choice of the learning algorithms for bothUNIBA-STACKING and UNIBA-2STEPSML sys-tems was performed after a tuning phase where onlythe STS-2012 training datasets were exploited.
Ta-ble 1 reports the values obtained by our three sys-tems on the STS-2012 test sets.
After the tuning,we came up with the learning algorithms to employin the level-0 and level-1 of UNIBA-STACKINGand in step-1 and step-2 of UNIBA-2STEPSML.Then, the training of both UNIBA-STACKING andUNIBA-2STEPSML was performed on all STS-2012 datasets (training and test data).173MSRpar MSRvid SMTeuroparl OnWN SMTnews meanUNIBA-2STEPSML .6056 .8573 .6233 .5079 .4533 .7016UNIBA-DSM PERM .4349 .7592 .5324 .6593 .4559 .6172UNIBA-STACKING .6473 .8727 .5344 .6646 .4604 .7714Table 1: STS-2012 test results of Pearson?s correlation.headlines OnWN FNWN SMT mean rankUNIBA- 2STEPSML .4255 .4801 .1832 .2710 .3673 71UNIBA- DSM PERM .6319 4910 .2717 .3155 .4610 54UNIBA- STACKING .6275 .4658 .2111 .2588 .4293 61Table 2: Evaluation results of Pearson?s correlation for individual datasets.3.2 Evaluation resultsEvaluation results on the STS-2013 data are reportedin Table 2.
Among the three systems, UNIBA-DSM PERM obtained the best performances onboth individual datasets and in the overall evalua-tion metric (mean), which computes the Pearson?scorrelation considering all datasets combined in asingle one.
The best system ranked 54 over a to-tal of 90 submissions, while UNIBA-STACKINGand UNIBA-2STEPSML ranked 61 and 71 re-spectively.
These results are at odds with thosereported in Table 1.
During the test on 2012dataset, UNIBA-STACKING gave the best result,followed by UNIBA-2STEPSML, while UNIBA-DSM PERM gave the worst performance.
TheUNIBA-STACKING system corroborated our hy-pothesis giving also the best results on those datasetsnot exploited during the training phase of the sys-tem (OnWN, SMTnews).
Conversely, UNIBA-2STEPSML reported a different trend showing itsweakness with respect to a high variance in the data,and performing worse than UNIBA-DSM PERM onthe OnWN and SMTnews datasets.However, the evaluation results have refuted ourhypothesis, even with the use of the stacking sys-tem.
The independence from a training set makesthe UNIBA-DSM PERM system more robust thanother supervised algorithms, even though it is notable to give always the best performance on individ-ual datasets, as highlighted by results in Table 1.7http://www.cs.waikato.ac.nz/ml/weka/4 ConclusionsThis paper reports on UNIBA participation in Se-mantic Textual Similarity 2013 core task.
In thistask edition, we exploited both distributional mod-els and machine learning techniques to build threesystems.
A distributional model, which takes intoaccount the syntactic structure that relates words in acorpus, has been used as baseline.
Moreover, we in-vestigate the use of two machine learning techniquesas a means to make our systems more independentfrom the training data.
However, the evaluation re-sults have highlighted the higher robustness of thedistributional model with respect to these systems.AcknowledgmentsThis work fulfils the research objectives of the PON02 00563 3470993 project ?VINCENTE - A Virtualcollective INtelligenCe ENvironment to developsustainable Technology Entrepreneurship ecosys-tems?
funded by the Italian Ministry of Universityand Research (MIUR).ReferencesEneko Agirre, Daniel Cer, Mona Diab, Aitor Gonzalez-Agirre, and Weiwei Guo.
2013.
*sem 2013 sharedtask: Semantic textual similarity, including a pilot ontyped-similarity.
In *SEM 2013: The Second JointConference on Lexical and Computational Semantics.Association for Computational Linguistics.Marco Baroni, Silvia Bernardini, Adriano Ferraresi, andEros Zanchetta.
2009.
The WaCky Wide Web: acollection of very large linguistically processed web-crawled corpora.
Language Resources and Evalua-tion, 43(3):209?226.174Pierpaolo Basile, Annalina Caputo, and Giovanni Semer-aro.
2012.
A study on compositional semantics ofwords in distributional spaces.
In Sixth IEEE Inter-national Conference on Semantic Computing, ICSC2012, Palermo, Italy, September 19-21, 2012, pages154?161.
IEEE Computer Society.Annalina Caputo, Pierpaolo Basile, and Giovanni Semer-aro.
2012.
Uniba: Distributional semantics for tex-tual similarity.
In *SEM 2012: The First Joint Confer-ence on Lexical and Computational Semantics ?
Vol-ume 1: Proceedings of the main conference and theshared task, and Volume 2: Proceedings of the SixthInternational Workshop on Semantic Evaluation (Se-mEval 2012), pages 591?596, Montre?al, Canada, 7-8June.
Association for Computational Linguistics.Evgeniy Gabrilovich and Shaul Markovitch.
2007.
Com-puting semantic relatedness using Wikipedia-based ex-plicit semantic analysis.
In Proceedings of the 20th in-ternational joint conference on artificial intelligence,volume 6, page 12.Rada Mihalcea, Courtney Corley, and Carlo Strapparava.2006.
Corpus-based and knowledge-based measuresof text semantic similarity.
In Proceedings of the na-tional conference on artificial intelligence, volume 21,pages 775?780.
Menlo Park, CA; Cambridge, MA;London; AAAI Press; MIT Press.Jeff Mitchell and Mirella Lapata.
2008.
Vector-basedmodels of semantic composition.
In Kathleen McKe-own, Johanna D. Moore, Simone Teufel, James Allan,and Sadaoki Furui, editors, ACL 2008, Proceedings ofthe 46th Annual Meeting of the Association for Com-putational Linguistics, June 15-20, 2008, Columbus,Ohio, USA, pages 236?244.
The Association for Com-puter Linguistics.Joakim Nivre, Johan Hall, Jens Nilsson, Atanas Chanev,Gu?lsen Eryigit, Sandra Ku?bler, Svetoslav Marinov,and Erwin Marsi.
2007.
Maltparser: A language-independent system for data-driven dependency pars-ing.
Natural Language Engineering, 13(2):95?135.Philip Resnik.
1995.
Using information content to evalu-ate semantic similarity.
In Proceedings of the 14th In-ternational Joint Conference on Artificial Intelligence,pages 448?453.Paul Smolensky.
1990.
Tensor product variable bind-ing and the representation of symbolic structures inconnectionist systems.
Artificial Intelligence, 46(1-2):159?216, November.Peter Turney.
2001.
Mining the web for synonyms:PMI-IR versus LSA on TOEFL.
In Proceedings ofthe Twelfth European Conference on Machine Learn-ing (ECML-2001), pages 491?502.Frane S?aric?, Goran Glavas?, Mladen Karan, Jan S?najder,and Bojana Dalbelo Bas?ic?.
2012.
Takelab: Systemsfor measuring semantic text similarity.
In *SEM 2012:The First Joint Conference on Lexical and Computa-tional Semantics ?
Volume 1: Proceedings of the mainconference and the shared task, and Volume 2: Pro-ceedings of the Sixth International Workshop on Se-mantic Evaluation (SemEval 2012), pages 441?448,Montre?al, Canada, 7-8 June.
Association for Compu-tational Linguistics.Dominic Widdows and Kathleen Ferraro.
2008.
Se-mantic Vectors: A Scalable Open Source Packageand Online Technology Management Application.
InNicoletta Calzolari, Khalid Choukri, Bente Maegaard,Joseph Mariani, Jan Odjik, Stelios Piperidis, andDaniel Tapias, editors, Proceedings of the 6th Interna-tional Conference on Language Resources and Eval-uation (LREC2008), pages 1183?1190, Marrakech,Morocco.
European Language Resources Association(ELRA).David H. Wolpert.
1992.
Stacked generalization.
Neuralnetworks, 5(2):241?259, February.175
