Proceedings of SIGDIAL 2009: the 10th Annual Meeting of the Special Interest Group in Discourse and Dialogue, pages 357?366,Queen Mary University of London, September 2009. c?2009 Association for Computational LinguisticsContrasting the Interaction Structure of an Email and a TelephoneCorpus: A Machine Learning Approach to Annotation of DialogueFunction UnitsJun HuDepartment of Computer ScienceColumbia UniversityNew York, NY, USAjh2740@columbia.eduRebecca J. PassonneauCCLSColumbia UniversityNew York, NY, USAbecky@cs.columbia.eduOwen RambowCCLSColumbia UniversityNew York, NY, USArambow@ccls.columbia.eduAbstractWe present a dialogue annotation schemefor both spoken and written interaction,and use it in a telephone transaction cor-pus and an email corpus.
We train classi-fiers, comparing regular SVM and struc-tured SVM against a heuristic baseline.We provide a novel application of struc-tured SVM to predicting relations betweeninstance pairs.1 IntroductionWe present an annotation scheme for verbal inter-action which can be applied to corpora that varyacross many dimensions: modality of signal (oral,textual), medium (e.g., email, voice alone, voiceover electronic channel), register (such as infor-mal conversation versus formal legal interroga-tion), number of participants, immediacy (onlineversus offline), and so on.1 We test it by anno-tating transcribed phone conversations and emailthreads.
We then use three algorithms, two ofwhich use machine learning (including a novel ap-proach to using Structured SVM), to predict labelsand links (a generalization of adjacency pairs) onunseen data.
We conclude that we can indeed usea common annotation scheme, and that the emailmodality is easier to tag for dialogue acts, but thatit is harder in email to find the links.2 Related WorkAnnotation for dialogue acts (DAs), inspired bySearle and Austin?s work on speech acts, aroselargely as a means to understand, evaluate and1This research was supported in part by the National Sci-ence Foundation under grants IIS-0745369 and IIS-0713548,and by the Human Language Technology Center of Excel-lence.
Any opinions, findings, and conclusions or recom-mendations expressed in this material are those of the au-thors and do not necessarily reflect the views of the sponsors.We would like to thank three anonymous reviewers for theirthoughtful comments.model human-human and human-machine com-munication.
The need for the enterprise derivesfrom the fact that the relationship between lexico-grammatical form (including mood, e.g., interrog-ative) and communicative actions cannot be enu-merated; there are complex dependencies on thelinguistic and situational contexts of use.
ManyDA schemes exist: they can be hierarchical orflat (Popescu-Belis, 2008), can comprise a large(Devillers et al, 2002; Hardy et al, 2003) or smallrepertoire (Komatani et al, 2005), or can be ori-ented towards human-human dialogue (Allen andCore, 1997; Devillers et al, 2002; Thompson etal., 1993; Traum and Heeman, 1996; Stolcke etal., 2000) or multi-party interactions (Galley et al,2004), or human-computer interaction (Walkerand Passonneau, 2001; Hardy et al, 2003), in-cluding multimodal ones (Thompson et al, 1993;Kruijff-Korbayova?
et al, 2006).A major focus of the cited work is on how torecognize or generate speech acts for interactivesystems, or how to classify speech acts for dis-tributional analyses.
The focus can be on a spe-cific type of speech act (e.g., grounding and re-pairs (Traum and Heeman, 1996; Frampton andLemon, 2008)), or on more general comparisons,such as the contrast between human-human andhuman-computer dialogues (Doran et al, 2001).While there is a large degree of overlap acrossschemes, the set of DA types will differ due to dif-ferences in the nature of the communicative goals;thus information-seeking versus task-oriented di-alogues differ in the set of speech acts and theirrelative frequencies.Our motivation in providing a new DA annota-tion scheme is that our focus differs from muchof this prior work.
We aim for a relatively ab-stract annotation scheme in order to make compar-isons across interactions of widely differing prop-erties.
Our initial focus is less on speech act typesand more on the patterns of local alternation be-357tween an initiating speech act and a respondingone?the analog of adjacency pairs (Sacks et al,1974).
The most closely related effort is (Gal-ley et al, 2004), which aims to automaticallyidentify adjacency pairs in the ICSI Meeting cor-pus, a large corpus of 75 meetings, using a smalltagset.
Their maximum entropy ranking approachachieved 90% accuracy on the 4-way classifica-tion into agreement, disagreement, backchanneland other.
Using the switchboard corpus, (Stolckeet al, 2000) achieved good dialogue act labelingaccuracy (71% on manual transcriptions) for a setof 42 dialogue act types, and constructed proba-bilistic models of dialogue act sequencing in orderto test the hypothesis that dialogue act sequenceinformation could boost speech recognition per-formance.There has been far less work on developingmanual and automatic dialogue act annotationschemes for email.
We summarize some salientrecent work.
Carvalho and Cohen (2006) use wordn-grams (with extensive preprocssing) to classifyentire emails into a complex ontology of speechacts.
However, in their experiments, they con-centrate on detecting only a subset of speech acts,which is comparable in size to ours.
Speech actsare assigned for entire emails, but several speechacts can be assigned to one email.
Apparently,they develop separate binary classifiers for eachspeech act.
Corston-Oliver et al (2004) are in-terested in identifying tasks in email.
They labeleach sentence in email with tags from a set whichdescribes the type of content of the sentence (de-scribing a task, scheduling a meeting), but are lessinterested in the interactive aspect of email com-munication (creating an obligation to respond).There has been some work which relates to find-ing links, but limited to finding question-answerpairs.
Shrestha and McKeown (2004) first de-tect questions using lexical and part-of-speech fea-tures, and then find the paragraph that answers thequestion.
They use features related to the structureof the email thread, as well as lexical features.
Asdo we, they find that classifying is easier than link-ing.Ding et al (2008) argue that in order to dowell at finding answers to questions, one mustalso find the context of the question, since it of-ten contains the information needed to identify theanswer.
They use a corpus of online discussionforums, and use slip-CRFs and two-dimensionalCRFs, models related to those we use.
We willinvestigate their proposal to consider the questioncontext in future work.While they do not use dialogue act taggingto compare modalities, as we do, Murray andCarenini (2008) compare spoken conversationwith email by comparing a common summariza-tion architecture across both modalities.
They getsimilar performance, but the features differ.Table 1: DFU speech act labelsRequest-Information (R-I)Request-Action (R-A)Inform (Inf)Commit (Comm)Conventional (Conv)Perform (Perf)Backchannel (Bch) (+/- Grounding)Other3 Annotation SchemeFigure 1: Example DFU illustrating the relation ofextent (segmentation) to speech act typeM1.2 I have completed the invoices for April,May and JuneM1.3 and we owe Pasadena each month for a to-tal of $3,615,910.62.M1.4 I am waiting to hear back from Patti on Mayand June to make sure they are okay with her.
[Inform(1.2-1.4): status of Pasadena invoicing-completed & pending approval ?
versus amountdue]Sflink(1.2-1.4)M2.1 That?s fine.
[Inform(2.1): acknowledgement of status ofPasadena invoicing]Blink(1.2-1.4)The annotation scheme presented here consistsof Dialogue Function Units (DFUs), which areintended to represent abstract units of interac-tion.
The last two authors developed the annota-tion on three contrasting corpora: email threads,telephone conversations, and court transcripts.
Itbuilds on our previous work in intention-basedsegmentation (Passonneau and Litman, 1997),and on mixing a formal schema with natural lan-guage descriptions (Nenkova et al, 2007).
In this358paper, we investigate the modalities of telephonetwo-person conversation in a library setting, andmulti-party email in a workplace setting.
Our ini-tial focus is on the structure of turn-taking.
Byusing a relatively abstract annotation scheme, wecan compare and contrast this behavior across dif-ferent types of interaction.Our unit of annotation is the DFU.
DFUs havean extent, a dialogue act (DA) label along witha description, and possibly one or more forwardand/or backward links.
We explain each compo-nent of the annotation in turn.
We use the exam-ple in Figure 1; the example is drawn from actualmessages, but has been modified to yield a moresuccinct example.The extent of a DFU roughly corresponds to thatportion of a turn (conversational turn; email mes-sage; etc.)
that corresponds to a coherent com-municative intention.
Because we do not addressautomatic identification of the segmentation intoDFU units in this paper, we do not discuss howannotators are instructed to identify extent.As illustrated in Figure 1, the communicativefunction of a DFU is captured by a speech acttype, and a natural language description.
This issomewhat analogous to the natural language de-scriptions associated with Summary Content Units(SCUs) in pyramid annotation (Nenkova et al,2007), or with the intention-based segmentationof (Passonneau and Litman, 1997).
The pur-pose in all cases is to require annotators to artic-ulate briefly but specifically the unifying intention(Passonneau and Litman, 1997), semantic content(Nenkova et al, 2007), or speech act.
We use theeight dialogue act types listed in the upper left ofTable 1.
To accommodate discontinuous speechacts, due to the interruptions that are common toconversation, each speech act can have an oper-ator affix such as ?-Continue?.
We have previ-ously shown (Passonneau and Litman, 1997) thatintention-based segmentation can be done reliablyby multiple annotators.
For twenty narratives eachsegmented by the same seven annotators, usingCochran?sQ (Cochran, 1950), we found the prob-abilities associated with the null hypothesis thatthe observed distributions could have arisen bychance to be at or below p=0.1 ?10?6.
Partition-ingQ by number of annotators gave significant re-sults for all values of A ranging over the numberof annotators apart from A = 2.
We would expectsimilar patterns of agreement on DFU segmen-tation, but have not collected segmentation datafrom multiple annotators on the two corpora pre-sented here.DFU Links, or simply Links, correspond to ad-jacency pairs, but need not be adjacent.
A forwardlink (Flink) is the analog of a ?first pair-part?
ofan adjacency pair (Sacks et al, 1974), and is sim-ilarly restricted to specific speech act types.
AllRequest-Information and Request-Action DFUsare assigned Flinks.
The responses to such re-quests are assigned a backward link (Blink).
Inprinciple, a response can be any of the speech acttypes, thus it can be an answer to a question (In-form), a rejection of a Request-Action or a com-mitment to take the requested action (Commit),a request for clarification (Request-Information),and so on.
In most but not all cases, requests areresponded to, thus most Flinks and Blinks come inpairs.
We refer to Flinks with no matching Blinkas dangling links.
If an utterance can be inter-preted as a response to a preceding DFU, it willget a Blink even where the preceding DFU has noFlink.
The preceding DFU taken to be the ?firstpair-part?
of the Link will be assigned a secondaryforward link (Sflink).
All links except danglinglinks are annotated with the address of the DFUfrom which they originate.
Figure 1 illustrates anemail message (M2) containing a single sentence(?That?s fine?)
that is a response to a DFU in aprior email (M1), where the prior email had noFlink because it only contains Inform DAs; thusM1 gets an Sflink.4 CorporaThe Loqui corpus consists of 82 transcribed dia-logues from a larger set of 175 dialogues that wererecorded at New York City?s Andrew HeiskellBraille and Talking Book Library during the sum-mer of 2005.
All of the transcribed dialogues per-tain to one or more book requests.
Forty-eightdialogues were annotated; the annotators workedfrom a combination of the transcription and the au-dio.
Three annotators were trained together, anno-tated up to a dozen dialogues independently, thendiscussed, adjudicated and merged ten of them.During this phase, the annotation guidelines wererefined and revised.
One of the three annotatorssubsequently annotated 38 additional dialogues.We also annotated 122 email threads of the En-ron email corpus, consisting of email messagesin the inboxes and outboxes of Enron corporation359Table 2: Distributional Characteristics of DialogueActs in Enron and LoquiLoqui EnronWords 21097 17924DFUs 3845 1400Speech Act LabelsInform 1928 50% 853 61%Request-Inf.
761 20% 149 11%Request-Action 39 1% 37 3%Commit 338 9% 3 0%Conventional 254 7% 356 25%Backchannel 507 13% 0 0Other 18 0% 2 0%Total 3845 100% 1400 100%LinksPaired Links 1204 63% 193 28%Flink/Blink 702 58% 83 43%Sflink/Blink 502 42% 110 57%Dangling Links 90 2% 97 7%Mutliple Blinks 4 0% 4 0%Links by Speech Act LabelsInform 1003 83% 142 74%Request-Inf.
170 14% 44 23%Request-Action 1 0% 5 3%Commit 13 1% 2 1%Conventional 2 0% 0 0Backchannel 15 1% 0 01204 100% 193 100%employees.
Most of the emails are concerned withexchanging information, scheduling meetings, andsolving problems, but there are also purely socialemails.
We used a version of the corpus with somemissing messages restored from other emails inwhich they were quoted (Yeh and Harnly, 2006).The annotator of the majority of the Loqui corpusalso annotated the Enron corpus.
She received ad-ditional training and guidance based on our experi-ence with a pilot annotator who helped us developthe initial guidelines.Table 2 illustrates differences between the twocorpora.
The DFUs in the Loqui data are muchshorter, with 5.5 words on average compared with12.8 words in Enron.
The distribution of DFU la-bels shows a similarly high proportion of Informacts, comprising 50% of all Loqui DFUs and 61%of all Enron DFUs.
Otherwise, the distributionsare quite distinct.
The Loqui interactions are alltwo party telephone dialogues where the callers(library patrons) tend to have limited goals (re-questing books).
The Enron threads consist oftwo or more parties, and exhibit a much broaderrange of communicative goals.
In the Loqui data,backchannels are relatively frequent (13%) but donot occur in the email corpus for obvious reasons.There are some Commits (9%), typically reflect-ing cases where the librarian indicates she willsend requested items to the caller by mail, or placethem on reserve.
There are no Commits in theEnron data.
Neither corpus has many Request-Actions; the Loqui corpus has many more requestsfor information, which includes requests made bythe librarian, e.g., for the patrons?
identifying in-formation, or by the caller.The most striking differences between the twocorpora pertain to the distribution of DFU Links.In Loqui, 63% of the DFUs are the first pair-partor the second pair-part of a Link compared with28% in Enron.
In Loqui, the majority of Linksare initiated by overt requests (58% of Links areFlink/Blink pairs), whereas in Enron, the major-ity of Links involve SFlinks (57%).
There arerelatively few dangling Links in either dataset,with more than three times as many in Enron (7%versus 2% in Loqui).
Most of the DFU typesin the second pair-part of Links are Informs andRequest-Information, with a different proportionin each dataset.
In Loqui, 83% of DFUs that aresecond pair-part of a Link are Informs comparedwith 74% in Enron; correspondingly, only 14% ofDFUs in Links are Request-Information in Loquiversus 23% in Enron.5 Dialogue Act Tagging and LinkPredictionThere are two machine learning tasks in our prob-lem.
The first is Dialogue Act (DA) Tagging, inwhich we assign DAs to every Dialogue Func-tional Unit (DFU).
The second is Link predic-tion, in which we predict if two DFUs form a linkpair.
In this paper, we assume that the DFUs aregiven.
We propose three systems to tackle theproblem.
The first system is a non-strawman Base-line Heuristics system, which uses the structuralcharacteristics of dialogue.
The second is Regu-lar SVM.
The third is Structured SVM.
StructuredSVM is a discriminative method that can predictcomplex structured output.
Recently, discrimi-native Probabilistic Graphical Models have beenwidely applied in structural problems (Getoor and360Taskar, 2007) such as link prediction.
However,Structured SVM (Taskar et al, 2003; Tsochan-taridis et al, 2005) is also a compelling methodwhich has the potential to handle the interdepen-dence between labeling and sequencing, due to itsability to handle dependencies among features andprediction results within the structure.
sequencelabeling (Tsochantaridis et al, 2005).
We haveadapted Structured SVM to our problem, provideda novel method for link prediction, and shown thatit is superior in some aspects to Regular SVM.5.1 FeaturesWe have two sets of features.
DFU features are as-sociated with a particular DFU, and link featuresdescribe the relationship between two DFUs.
DFUfeatures are used in both tasks.
Link features areonly used in link prediction.
The feature vector ofa link contains two sets of DFU features and thelink features that are defined over the two DFUs.Table 3 gives the features we used, which are al-most identical for both corpora, so we could com-pare the performance.Because a lot of Flinks are questions, wechose some features that are tailored to Question-Answer detection, such as presence of a questionmark.
Dialogue fillers and acceptance words af-fect the accuracy of Part-Of-Speech tagging.
Onthe other hand, they are helpful indicators of dis-fluency or confirmation.
So we hand-picked a listof filler and acceptance words, removed them fromthe sentence, and added features counting their oc-currences.5.2 Baseline HeuristicsDialogue Act Tagging We use the most frequentDA as the heuristic for prediction.
In both Enronand Loqui, this DA is Inform.Link Prediction In link prediction, the heuris-tics for Enron and Loqui corpora are different dueto structural differences.
In Loqui, whenever wesee a DFU with a Forward Link (DA is Request-Information or Request-Action), we predict thatthe target of the link is the first following DFU thatis available and acceptable.
?Available?
meansthat the second DFU has not been assigned a Back-ward Link yet.
?Acceptable?
means that the sec-ond DFU has a DA that is very frequent in a Back-ward Link and it is of a different speaker to thefirst DFU.
We enforce similar constraints in Enroncorpus for link prediction, except that the secondTable 3: DFU features (E: Enron, L: Loqui)Structural for DA predictionE,L First three POSE,L Relative Position in the DialogueE Existence of Question MarkE,L Does the first POS start with ?w?
or ?v?E,L Length of the DFUE Head, body, tail of the MessageE,L Dialogue Act (Only used in link prediction)Lexical for DA predictionE,L Bag of WordsE,L Number of Content WordsL Number of Filler Words, as ?uh?, ?hmm?E,L Number of Acceptance Words, as ?yes?Structural for Link predictionE,L The distance between two DFUsLexical for Link predictionE,L Overlapping number of content wordsDFU not only has to be from a different author,but also has to be in a message which is a directdescendant in the reply chain of the message thatcontains the first DFU.
The baseline link predic-tion algorithm uses the DAs as predicted by theRegular SVM.
If we used the baseline DA predic-tion, the result would be too low to make a validcomparison against other systems in terms of linkprediction because all DAs would be identical.5.3 Regular SVMWe have used the Yamcha support vector machinepackage (chasen.org/?taku/software/yamcha/).The advantage of Yamcha is that it extends thetraditional SVM by enabling using dynamicallygenerated features such as preceding labels.Dialogue Act Tagging We use the feature vectorof the current DFU as well as the predicted DA ofthe preceding DFU as features to predict the DAof the current DFU.Link Prediction First, in order to limit searchspace, we specify a certain window size to producea space S of DFU pairs under consideration.
Fora particular DFU, we look at all succeeding DFUsand check if these two DFUs satisfy the follow-ing constraint: in Loqui, they must be of differentspeakers; in Email, one must be another?s ancestorand they must be of different authors.
We considerall valid pairs starting from the current DFU until361the number of considered valid pairs reaches thewindow size.
Then we proceed to the next DFUand collect more DFU pairs into our considerationspace.Second, we train a link binary classifier with allDFU pairs in this consideration space along with abinary classification correct/not correct as trainingdata.
This classifier takes the feature vectors of thetwo DFUs as well as the link features such as thedistance between these two DFUs as features.Third, we apply a greedy algorithm to gener-ate links in the test data with the binary classifier.The algorithm firstly uses the classifier to generatescores for all DFU pairs in the consideration spaceof the test data, then it scans the dialogue sequen-tially, checks all preceding DFUs that are allowedto link to the current DFU (i.e., the DFU pair is inthe consideration space), and assigns correspond-ing links to the most likely DFU pair.
We impose arestriction that there can be at most one Flink, oneSflink and one Blink for any given DFU.5.4 Structured SVMA Structured SVM is able to predict com-plex output instead of simply a binary resultas in a regular SVM.
There are several vari-ants.
We have followed the margin-rescaling ap-proach (Tsochantaridis et al, 2005), and im-plemented our systems using SVMpython, whichis a python interface to the SVMstruct package(svmlight.joachims.org/svm struct.html).
Gener-ally, Structured SVM learns a discriminant func-tion F : X?Y ?
R, which estimates a score ofhow likely the output y is given the input x. Cru-cially, y can be a complex structure.
Section A inthe appendix; here, we summarize the main intu-itions.Dialogue Act Tagging The input x is a sequenceof DFUs, and y is the corresponding sequence ofDAs to predict.
Compared to Regular SVM, in-stead of predicting yt one at a time, StructuredSVM optimizes the sequence as a whole and pre-dicts all labels simultaneously.
Due to the similar-ity to HMM, the maximization problem is solvedby the Viterbi algorithm (Tsochantaridis et al,2005).Link Prediction The input now contains the DFUsequence, a link consideration space, as well asa label sequence, which we get from the previ-ous stage.
The output structure chooses amongthe possible links in the link consideration space,such that there is at most one Flink/SFlink or Blinkfor any given DFU, and that there are no crossinglinks.
(Note that all the constraints are only en-forced in training and prediction; in testing, wecompare results against the complete manual an-notations which do not follow these constraints.
)Then the maximization problem can be solved by astraightforward dynamic programming algorithm.Table 4: Result of DA predictionBaseline Regular StructLoqui 50.14% 68.30% 70.26%Enron 60.93% 88.34% 88.71%Note: Structured SVM parameters for Loqui are C =300, ?
= 1; Structured SVM parameters for Enronare C = 1000, ?
= 1.6 ExperimentsWe have three hypotheses for our experiments:Hypothesis 1 Link prediction is harder than Dia-logue Act prediction.Hypothesis 2 Enron is harder than Loqui.Hypothesis 3 Structured SVM is better than Reg-ular SVM, and Baseline is the worst.We have applied the algorithm described in Sec-tion 5 to both the Enron and Loqui corpora.
Thedata set is annotated with DFUs; we focus on theDA labels and Links.
As discussed before, everysystem is a pipeline that would preprocess the datainto separate DFUs, predict the Dialogue Acts,and then feed the Dialogue Acts into the link pre-diction algorithm.
The size of the data set is shownin Table 2.
We do five-fold cross-validation.Table 4 shows the accuracy of three systems onEnron and Loqui.
Structured SVM has a clear leadto Regular SVM in Loqui; but the advantage is lessclear in Enron.
Tables 6 and 7 give detailed resultsof DA prediction.We do not show DAs that do notexist in the corpora, or that were not predicted bythe algorithms.
Both Regular SVM and StructuredSVM performed consistently for the two corpora.Table 5 gives Link prediction results.
Note thatwhen we compute the combined result for bothtypes of links, we are only concerned with theLink position.
The seperate results for Flink/Blinkand Sflink/Blink require us to identify the types oflinks first, so here we not only compare the posi-tion of predicted links against the gold, but alsorequire predicted DAs to indicate the link type(e.g., the DA of the first DFU must be Request-362Table 5: Link Prediction for Enron and LoquiBaseline Regular StructEnron R P F R P F R P RPaired Links 16.66% 40% 23.52% 18.75% 55.38% 28.01% 31.25% 39.47% 34.88%Flink/Blink 32.53% 33.75% 33.13% 26.50% 61.11% 36.97% 34.93% 47.54% 40.27%Sflink/Blink 0.0% 0.0% 0.0% 11.92% 44.82% 18.83% 22.93% 27.47% 25.00%LoquiPaired Links 30% 56.15% 39.11% 43.59% 60.60% 50.71% 44.15% 56.02% 49.38%Flink/Blink 43.30% 46.47% 44.83% 40.58% 57.73% 47.66% 43.55% 60.04% 50.48%Sflink/Blink 0.0% 0.0% 0.0% 21.76% 29.36% 25.00% 22.88% 26.24% 24.45%Note: Structured SVM parameters for Enron are C = 2000, ?
= 2., for Loqui C = 1000, ?
= 4.Information or Request-Action to qualify as aFlink/Blink).Table 6: Recall/Precision/F-measure of DA pre-diction for Loqui (in %)Regular StructP R F P R FR-A 50.0 51.7 50.9 43.3 43.3 43.3R-I 51.3 61.1 55.8 52.3 71.2 60.3Inf 73.9 73.0 73.5 76.9 74.1 75.5Bch 65.3 51.7 57.7 65.1 53.6 58.8Com 5.6 33.3 9.5 5.6 33.3 9.5Conv 81.2 84.0 82.6 83.7 83.3 83.5Table 7: Recall/Precision/F-measure of DA pre-diction for Enron (in %)Regular StructR P F R P FR-A 27.8 55.6 37.0 25.0 75.0 37.5R-I 77.9 82.3 80.0 77.2 83.3 80.1Inf 92.5 90.6 91.5 92.1 91.2 91.7Conv 90.5 87.3 88.9 93.4 85.6 89.37 DiscussionHypothesis 1 The result of DA prediction is dras-tically better than link prediction.
There are usu-ally indicators of DA types such as ?thank you?
forConventional, so learning algorithms could easilycapture them.
But in link prediction, we frequentlyneed to handle deep semantic inference and some-times useful information exists in the surroundingcontext rather than the DFU itself.
Both of thesescenarios imply that in order to predict links or re-lationships better, we need more sophisticated fea-tures.Hypothesis 2 This hypothesis turns out to be half-correct.
The DA prediction accuracy for Enronis better than that of Loqui.
The higher percent-age of Inform and less diversity of DAs in Enron(See Appendix for statistics) may be part of thereason.
Another possible explanation is that as aset of spoken dialogue data, Loqui is inherentlymore difficult to process than written form, sincesome common tasks such Part-Of-Speech tagginghave lower accuracy for spoken data.
On the otherhand, the result of link prediction did confirm ourhypothesis.
The first reason is that there are farfewer links in Enron than in Loqui, so we have lesstraining data.
The tree structure of the reply chainin the email threads also makes prediction moredifficult.
And the link distance is longer, becausein email, people can respond to a very early mes-sage, while in a phone conversation, people tendto respond to immediate requests.Hypothesis 3 Both SVM models perform betterthan the baseline.
Generally, Structured SVM per-forms better than Regular SVM, especially in linkprediction for Enron.
This confirms the advan-tage of using Structured SVM for output involv-ing inter-dependencies.
The only exception is theSflink prediction in Loqui, which in turn affectsthe overall accuracy of link prediction.ReferencesJames Allen and Mark Core.
1997.
Damsl:Dialogue act markup in several layers.http://www.cs.rochester.edu/research/cisd/resources/damsl.Vitor Carvalho and William Cohen.
2006.
Improving ?emailspeech acts?
analysis via n-gram selection.
In Proceed-ings of the Analyzing Conversations in Text and Speech.William G. Cochran.
1950.
The comparison of percentagesin matched samples.
Biometrika, 37:256?266.363Simon Corston-Oliver, Eric Ringger, Michael Gamon, andRichard Campbell.
2004.
Task-focused summarization ofemail.
In Stan Szpakowicz Marie-Francine Moens, edi-tor, Text Summarization Branches Out: Proceedings of theACL-04 Workshop.Laurence Devillers, Sophie Rosset, Bonneau-Helene May-nard, and Lamel Lori.
2002.
Annotations for dynamicdiagnosis of the dialog state.
In LREC.Shilin Ding, Gao Cong, Chin-Yew Lin, and Xiaoyan Zhu.2008.
Using conditional random fields to extract contextsand answers of questions from online forums.
In Proceed-ings of ACL-08: HLT, Columbus, Ohio.Christine Doran, John Aberdeen, Laurie Damianos, andLynette Hirschman.
2001.
Comparing several aspects ofhuman-computer and human-human dialogues.
In Pro-ceedings of the 2nd SIGDIAL Workshop on Discourse andDialogue.Matthew Frampton and Oliver Lemon.
2008.
Using dialogueacts to learn better repair strategies for spoken dialoguesystems.
In ICASSP.Michel Galley, Kathleen McKeown, Julia Hirschberg, andElizabeth Shriberg.
2004.
Identifying agreement and dis-agreement in conversational speech: use of Bayesian net-works to model pragmatic dependencies.
In Proceedingsof the 42nd Annual Meeting of the Association for Com-putational Linguistics, pages 669?676.Lise Getoor and Ben Taskar, editors.
2007.
Introduction toStatistical Relational Learning.
The MIT Press.Hilda Hardy, Kirk Baker, Bonneau-Helene Maynard, Lau-rence Devillers, Sophie Rosset, and Tomek Strza-lkowski.
2003.
Semantic and dialogic annotationfor automated multilingual customer service.
In Eu-rospeech/Interspeech.Kazunori Komatani, Nayouki Kanda, Tetsuya Ogata, and Hi-roshi G. Okuno.
2005.
Contextual constraints based ondialogue models in database search task for spoken dia-logue systems.
In Eurospeech.Ivana Kruijff-Korbayova?, Tilman Becker, Nate Blaylock,Ciprian Gerstenberger, Michael Kaisser, Peter Poller, Ver-ena Rieser, and Jan Schehl.
2006.
The Sammie corpus ofmultimodal dialogues with an mp3 player.
In LREC.Gabriel Murray and Giuseppe Carenini.
2008.
Summarizingspoken and written conversations.
In EMNLP.Ani Nenkova, Rebecca J. Passonneau, and Kathleen McKe-own.
2007.
The pyramid method: incorporating humancontent selection variation in summarization evaluation.ACM Transactions on Speech and Language Processing,4(2).Rebecca J. Passonneau and Diane J. Litman.
1997.
Dis-course segmentation by human and automated means.Computational Linguistics, 23(1).Andrei Popescu-Belis.
2008.
Dimensionality of dialogue acttagsets: An empirical analysis of large corpora.
LREC,42(1).Harvey Sacks, Emanuel A. Schegloff, and Gail Jefferson.1974.
A simplest systemics for the organization of turn-taking for conversation.
Language, 50(4).Lokesh Shrestha and Kathleen McKeown.
2004.
Detectionof question-answer pairs in email conversations.
In COL-ING.Andreas Stolcke, Klaus Ries, Noah Coccaro, ElizabethShriberg, Rebecca Bates, Daniel Jurafsky, Paul Taylor,Rachel Martin, Carol Van Ess-Dykena, and Meteer Marie.2000.
Dialogue act modeling for automatic tagging andrecognition of conversational speech.
International Jour-nal of Computational Linguistics, 26(3).Ben Taskar, Crlos Guestrin, and Daphne Koller.
2003.
Max-margin markov networks.
In NIPS.Henry S. Thompson, Anne H. Anderson, Ellen Gurman Bard,Gwyneth Doherty-Sneddon, Alison Newlands, and CathySotillo.
1993.
The HCRC map task corpus: Naturaldialogue for speech recognition.
In Proceedings of theDARPA Human Language Technology Workshop.David Traum and Peter Heeman.
1996.
Utterance units andgrounding in spoken dialogue.
In Interspeech/ICSLP.Ioannis Tsochantaridis, Thorsten Joachims, Thomas Hof-mann, and Yasemin Altun.
2005.
Large margin methodsfor structured and interdependent output variables.
JMLR,6.Marilyn A. Walker and Rebecca Passonneau.
2001.
Date:A dialogue act tagging scheme for evaluation of spokendialogue systems.
In HLT.Jen-Yuan Yeh and Aaron Harnly.
2006.
Email thread re-assembly using similarity matching.
In Conference onEmail and Anti-Spam.364A Appendix: Structured SVMThis section provides mathematical backgroundfor Secton 5.4.
The hypothesis function is givenby:f(x,w) = argmaxy?YF (x,y : w)And in addition, we assume F to be linear to ajoint feature map ?
(x,y).F (x,y : w) = ?w,?
(x,y)?We also define a loss function ?
(y,y) which de-fines the deviation of the predicted output y to thecorrect output.As a result, given a sequence of trainingexamples,(x1,y1) ?
?
?
(xn,yn) ?
X ?
Y, thefunction we need to optimize becomes:minw,?
12 ?w?2 + Cn?ni=1 ?is.t.
?i?y ?
Y\y(i) : ?w, ??i(y)?
>?(y(i),y)?
?i where,?w, ??i(y)?
=?w,?(x(i),y(i))??
(x(i),y)?w is optimized towards maximizing the marginbetween the true structured output y and anyother suboptimal configurations for all training in-stances.A cutting plane optimization algorithm is im-plemented in SVMstruct.
However, for any prob-lem, we need to implement the feature map?
(x,y), the loss function ?
(y,y), and a maxi-mization problem which enables the cutting planeoptimization, i.e.y = argmaxy?Y?(y(i),y)?
?w, ?
?i(y)?Only certain feature maps that would makesolving this maximization effectively, usually bydynamic programming, could be handled this way.For Dialogue Act Tagging, let x =(x1,x2 .
.
.xT)be the sequence of DFUs,and y =(y1,y2 .
.
.yT)the corresponding se-quence of dialogue acts.
?
(xt) represents the DFUfeatures and ?
(xt) ?
RD.
yt ?
L = {l1, .
.
.
, lK}where L contains the set of available DAs.
Thefeature map is (Tsochantaridis et al, 2005):?
(x,y) =( ?Tt=1 ?(xt)??(yt)?(yt?1)??
(yt))where ?
(yt) = [?
(l1,y), .
.
.
, ?
(lk,y)] and ?
isan indicator function that returns 1 if two parame-ters are equal.
?-operator is defined as:RD ?RK ?
RD?K , (a?
b)i+(j?1)D ?
ai ?
bjIn analogy to an HMM, the lower part in?
(x,y) encodes the histogram of adjacent DAtransitions in y ; the upper part encodes the DAemissions from a specific label to one dimensionin the DFU feature vector.
Hence, the total num-ber of dimensions in ?
(x,y) is K2 + DK.
Asa result, F (x,y : w) = ?w,?(x,y)?
gives aglobal score based on all transitions and emissionsin the sequence, which captures the dependeciesamong nearby labels and mimics the behaviour ofan HMM.
Figure 2 gives an example of how tocompute the feature map.The loss function is the sum of all zero-onelosses across the sequence, i.e.?
(y,y) = ?
?Tt=1 ?(yt,yt)?
denotes a cost assigned to every DA loss.For Link Prediction, the input contains theDFU sequence x, a link consideration spaces = {(i, j) :,DFU i and j is being considered},as well as label sequence y which we get fromthe previous stage.
?
(xi,xj) is the link featuredefined over two DFUs.
Let the dimension oflink feature be B.
The output structure u ={u1,u2 .
.
.uT}specifies the link plan.
ut denotesthat there is a link from DFU t ?
ut to t with theexception that ut being zero denotes there is nolink pointing to t. The setup of u constraints thatthere can be at most one Flink/SFlink or Blink forany given DFU.
In addition u is also subject to theconstraint that all specified links must be in thelink consideration space.The discriminant function becomes F : X ?Y?S?U?
R. Similar to structured DA predic-tion, the discriminant function should give a globalevaluation as to how likely is the link plan spec-ified by U with respect to all the input vectors.Our solution is to decompose the score, and cor-respondingly, the feature representation into twocomponents, link emission and no-link emission;the details can be found in Figure 3 in the appendixand an example is in Figure 2.Similarly, we could define the loss function asthe sum of all zero-one losses across the sequence, i.e.?
(u,u) = ?
?Tt=1 ?(ut,ut)?
denotes a cost assigned to every Link loss.365Figure 2: A full example of feature map for Structured SVMx1 = ?are you you sure?x2 = ?sure?y1 = ?Req-Info?y2 = ?Inform?u1 = 0u2 = 1?
(x1) = (1, 2, 1)?
(x2) = (0, 0, 1)?
(x1,x2) = (1, 1)?da =????????????????0001001121???????????????
?Inform to InformInform to Req-InfoReq-Info to InformReq-Info to InformInform with ?are?Inform with ?you?Inform with ?sure?Req-Info with ?are?Req-Info with ?you?Req-Info with ?sure?
?link =?????????????????????????????12101001101112101????????????????????????????
?1st link pair-part with?are?1st link pair-part with?you?1st link pair-part with?sure?1st link pair-part with Inform1st link pair-part with Req-Info2nd link pair-part with?are?2nd link pair-part with?you?2nd link pair-part with?sure?2nd link pair-part with Inform2nd link pair-part with Req-Infodistance of linkoverlap of linkNo-Link with?are?No-Link with?you?No-Link with?sure?No-Link with InformNo-Link with Req-InfoNote: In this example, ?
(xt) extracts the bag-of-words features from xt.
?are?,?you?,?sure?
are the 1st, 2ndand 3rd DFU feature respectively.
?
(xi,xj) extracts the distance and number of the overlap content, which arethe link features, from the 1st and 2nd pair-part in a DFU link pair.
There is a link from DFU 1 to DFU 2 asspecified by j ?
uj = i, but there is no link pointing to DFU 1.Figure 3: The feature map of link prediction forthe structured SVM?L =????????
?T?1i=1?Tj=i+1 ?(xi)?
(i, j ?
uj)?T?1i=1?Tj=i+1 ?(yi)?
(i, j ?
uj)?T?1i=1?Tj=i+1 ?(xj)?
(i, j ?
uj)?T?1i=1?Tj=i+1 ?(yj)?
(i, j ?
uj)?T?1i=1?Tj=i+1 ?(xi,xj)?
(i, j ?
uj)????????
?NL =( ?Ti=1 ?(xi)?
(0,ui)?Ti=1 ?(yi)?(0,ui))?
(x,y, s,u) =(?L?NL)Note: ?L and ?NL correspond to the link and no-link emissions in the feature map ?
(x,y, s,u) re-spectively as shown in the equations.
The total di-mension of the feature map is 3D + 3K +B.366
