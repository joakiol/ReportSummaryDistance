Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 385?393,Los Angeles, California, June 2010. c?2010 Association for Computational LinguisticsCoreference Resolution in a Modular, Entity-Centered ModelAria HaghighiComputer Science DivisionUniversity of California, Berkeleyaria42@cs.berkeley.eduDan KleinComputer Science DivisionUniversity of California, Berkeleyklein@cs.berkeley.eduAbstractCoreference resolution is governed by syntac-tic, semantic, and discourse constraints.
Wepresent a generative, model-based approach inwhich each of these factors is modularly en-capsulated and learned in a primarily unsu-pervised manner.
Our semantic representationfirst hypothesizes an underlying set of latententity types, which generate specific entitiesthat in turn render individual mentions.
Bysharing lexical statistics at the level of abstractentity types, our model is able to substantiallyreduce semantic compatibility errors, result-ing in the best results to date on the completeend-to-end coreference task.1 IntroductionCoreference systems exploit a variety of informa-tion sources, ranging from syntactic and discourseconstraints, which are highly configurational, to se-mantic constraints, which are highly contingent onlexical meaning and world knowledge.
Perhaps be-cause configurational features are inherently easierto learn from small data sets, past work has oftenemphasized them over semantic knowledge.Of course, all state-of-the-art coreference systemshave needed to capture semantic compatibility tosome degree.
As an example of nominal headwordcompatibility, a ?president?
can be a ?leader?
butcannot be not an ?increase.?
Past systems have of-ten computed the compatibility of specific headwordpairs, extracted either from lexical resources (Ng,2007; Bengston and Roth, 2008; Rahman and Ng,2009), web statistics (Yang et al, 2005), or sur-face syntactic patterns (Haghighi and Klein, 2009).While the pairwise approach has high precision, it isneither realistic nor scalable to explicitly enumerateall pairs of compatible word pairs.
A more compactapproach has been to rely on named-entity recog-nition (NER) systems to give coarse-grained entitytypes for each mention (Soon et al, 1999; Ng andCardie, 2002).
Unfortunately, current systems usesmall inventories of types and so provide little con-straint.
In general, coreference errors in state-of-the-art systems are frequently due to poor models of se-mantic compatibility (Haghighi and Klein, 2009).In this work, we take a primarily unsupervised ap-proach to coreference resolution, broadly similar toHaghighi and Klein (2007), which addresses this is-sue.
Our generative model exploits a large inven-tory of distributional entity types, including standardNER types like PERSON and ORG, as well as morerefined types like WEAPON and VEHICLE.
For eachtype, distributions over typical heads, modifiers, andgovernors are learned from large amounts of unla-beled data, capturing type-level semantic informa-tion (e.g.
?spokesman?
is a likely head for a PER-SON).
Each entity inherits from a type but capturesentity-level semantic information (e.g.
?giant?
maybe a likely head for the Microsoft entity but not allORGs).
Separately from the type-entity semanticmodule, a log-linear discourse model captures con-figurational effects.
Finally, a mention model assem-bles each textual mention by selecting semanticallyappropriate words from the entities and types.Despite being almost entirely unsupervised, ourmodel yields the best reported end-to-end results ona range of standard coreference data sets.2 Key AbstractionsThe key abstractions of our model are illustrated inFigure 1 and described here.Mentions: A mention is an observed textual ref-erence to a latent real-world entity.
Mentions are as-385Person[0: 0.30,1:0.25,2:0.20, ...]NOM-HEAD[1: 0.39,0:0.18,2:0.13, ...][Obama: 0.02,Smith:0.015,Jr.
: 0.01, ...][president: 0.14,painter:0.11,senator: 0.10,...]NAM-HEADr ?r frNOM-HEAD [president, leader]NAM-HEAD [Obama, Barack]r LrBarack ObamaNOM-HEAD [painter]NAM-HEAD [Picasso, Pablo]r LrPablo PicassoNN-MOD Mr.NAM-HEAD Obamar wrNOM-HEAD presidentr wrTypesEntitiesMentions(c)(b)(a)?Mr.
Obama?
?the president?...
...
...Figure 1: The key abstractions of our model (Section 2).
(a) Mentions map properties (r) to words (wr).
(b) Enti-ties map properties (r) to word lists (Lr).
(c) Types mapproperties (r) to distributions over property words (?r)and the fertilities of those distributions (fr).
For (b) and(c), we only illustrate a subset of the properties.sociated with nodes in a parse tree and are typicallyrealized as NPs.
There are three basic forms of men-tions: proper (denoted NAM), nominal (NOM), andpronominal (PRO).
We will often describe properand nominal mentions together as referring men-tions.We represent each mention M as a collection ofkey-value pairs.
The keys are called properties andthe values are words.
For example, the left mentionin Figure 1(a) has a proper head property, denotedNAM-HEAD, with value ?Obama.?
The set of prop-erties we consider, denoted R, includes several va-rieties of heads, modifiers, and governors (see Sec-tion 5.2 for details).
Not every mention has a valuefor every property.Entities: An entity is a specific individual or ob-ject in the world.
Entities are always latent in text.Where a mention has a single word for each prop-erty, an entity has a list of signature words.
For-mally, entities are mappings from properties r ?
Rto lists Lr of ?canonical?
words which that entityuses for that property.
For instance in Figure 1(b),the list of nominal heads for the Barack Obama en-tity includes ?president.
?Types: Coreference systems often make a men-tion / entity distinction.
We extend this hierarchyto include types, which represent classes of entities(PERSON, ORGANIZATION, and so on).
Types allowthe sharing of properties across entities and mediatethe generation of entities in our model (Section 3.1).See Figure 1(c) for a concrete example.We represent each type ?
as a mapping betweenproperties r and pairs of multinomials (?r, fr).
To-gether, these distributions control the lists Lr for en-tities of that type.
?r is a unigram distribution ofwords that are semantically licensed for property r.fr is a ?fertility?
distribution over the integers thatcharacterizes entity list lengths.
For example, for thetype PERSON, ?r for proper heads is quite flat (thereare many last names) but fr is peaked at 1 (peoplehave a single last name).3 Generative ModelWe now describe our generative model.
At the pa-rameter level, we have one parameter group for thetypes ?
= (?, ?1, .
.
.
, ?t), where ?
is a multinomialprior over a fixed number t of types and the {?i} arethe parameters for each individual type, described ingreater detail below.
A second group comprises log-linear parameters pi over discourse choices, also de-scribed below.
Together, these two groups are drawnaccording to P (?
|?
)P (pi|?2), where ?
and ?2 are asmall number of scalar hyper-parameters describedin Section 4.Conditioned on the parameters (?
,pi), a docu-ment is generated as follows: A semantic modulegenerates a sequence E of entities.
E is in prin-ciple infinite, though during inference only a finitenumber are ever instantiated.
A discourse modulegenerates a vector Z which assigns an entity in-dex Zi to each mention position i.
Finally, a men-tion generation module independently renders thesequence of mentions (M) from their underlying en-tities.
The syntactic position and structure of men-tions are treated as observed, including the mentionforms (pronominal, etc.).
We use X to refer to thisungenenerated information.
Our model decomposesas follows:P (E,Z,M|?
,pi,X) =P (E|? )
[Semantic, Section 3.1]P (Z|pi,X) [Discourse, Section 3.2]P (M|Z,E, ? )
[Mention, Section 3.3]We detail each of these components in subsequentsections.386TLr?fr ?rORG: 0.30PERS: 0.22GPE: 0.18LOC: 0.15WEA: 0.12VEH: 0.09...T = PERS0: 0.301: 0.252: 0.203: 0.18...PERSFor T = PERSpresident: 0.14painter: 0.11senator: 0.10minister: 0.09leader: 0.08official: 0.06executive: 0.05...presidentleaderofficialREFigure 2: Depiction of the entity generation process (Sec-tion 3.1).
Each entity draws a type (T ) from ?, and, foreach property r ?
R, forms a word list (Lr) by choosinga length from T ?s fr distribution and then independentlydrawing that many words from T ?s ?r distribution.
Ex-ample values are shown for the person type and the nom-inal head property (NOM-HEAD).3.1 Semantic ModuleThe semantic module is responsible for generatinga sequence of entities.
Each entity E is generatedindependently and consists of a type indicator T , aswell as a collection {Lr}r?R of word lists for eachproperty.
These elements are generated as follows:Entity GenerationDraw entity type T ?
?For each mention property r ?
R,Fetch {(fr, ?r)} for ?
TDraw word list length |Lr| ?
frDraw |Lr| words from w ?
?rSee Figure 2 for an illustration of this process.
Eachword list Lr is generated by first drawing a listlength from fr and then independently populatingthat list from the property?s word distribution ?r.1Past work has employed broadly similar distribu-tional models for unsupervised NER of proper men-1There is one exception: the sizes of the proper and nomi-nal head property lists are jointly generated, but their word listsare still independently populated.tions (Collins and Singer, 1999; Elsner et al, 2009).However, to our knowledge, this is the first workto incorporate such a model into an entity referenceprocess.3.2 Discourse ModuleThe discourse module is responsible for choosingan entity to evoke at each of the n mention posi-tions.
Formally, this module generates an entity as-signment vector Z = (Z1, .
.
.
, Zn), where Zi indi-cates the entity index for the ith mention position.Most linguistic inquiry characterizes NP anaphoraby the pairwise relations that hold between a men-tion and its antecedent (Hobbs, 1979; Kehler et al,2008).
Our discourse module utilizes this pairwiseperspective to define each Zi in terms of an interme-diate ?antecedent?
variable Ai.
Ai either points to aprevious antecedent mention position (Ai < i) and?steals?
its entity assignment or begins a new entity(Ai = i).
The choice ofAi is parametrized by affini-ties spi(i, j;X) between mention positions i and j.Formally, this process is described as:Entity AssignmentFor each mention position, i = 1, .
.
.
, n,Draw antecedent position Ai ?
{1, .
.
.
, i}:P (Ai = j|X) ?
spi(i, j;X)Zi ={ZAi , if Ai < iK + 1, otherwiseHere, K denotes the number of entities allocated inthe first i-1 mention positions.
This process is an in-stance of the sequential distance-dependent ChineseRestaurant Process (DD-CRP) of Blei and Frazier(2009).
During inference, we variously exploit boththe A and Z representations (Section 4).For nominal and pronoun mentions, there are sev-eral well-studied anaphora cues, including centering(Grosz et al, 1995), nearness (Hobbs, 1978), anddeterministic constraints, which have all been uti-lized in prior coreference work (Soon et al, 1999;Ng and Cardie, 2002).
In order to combine thesecues, we take a log-linear, feature-based approachand parametrize spi(i, j;X) = exp{pi>fX(i, j)},where fX(i, j) is a feature vector over mention po-sitions i and j, and pi is a parameter vector; the fea-tures may freely condition on X.
We utilize thefollowing features between a mention and an an-387tecedent: tree distance, sentence distance, and thesyntactic positions (subject, object, and oblique) ofthe mention and antecedent.
Features for starting anew entity include: a definiteness feature (extractedfrom the mention?s determiner), the top CFG ruleof the mention parse node, its syntactic role, and abias feature.
These features are conjoined with themention form (nominal or pronoun).
Additionally,we restrict pronoun antecedents to the current andlast two sentences, and the current and last three sen-tences for nominals.
Additionally, we disallow nom-inals from having direct pronoun antecedents.In addition to the above, if a mention is in a de-terministic coreference configuration, as defined inHaghighi and Klein (2009), we force it to take therequired antecedent.
In general, antecedent affini-ties learn to prefer close antecedents in prominentsyntactic positions.
We also learn that new entitynominals are typically indefinite or have SBAR com-plements (captured by the CFG feature).In contrast to nominals and pronouns, the choiceof entity for a proper mention is governed more byentity frequency than antecedent distance.
We cap-ture this by setting spi(i, j;X) in the proper case to1 for past positions and to a fixed ?
otherwise.
23.3 Mention ModuleOnce the semantic module has generated entities andthe discourse model selects entity assignments, eachmention Mi generates word values for a set of ob-served properties Ri:Mention GenerationFor each mention Mi, i = 1, .
.
.
, nFetch (T, {Lr}r?R) from EZiFetch {(fr, ?r)}r?R from ?
TFor r ?
Ri :w ?
(1?
?r)UNIFORM(Lr) + (?r)?rFor each property r, there is a hyper-parameter ?rwhich interpolates between selecting a word fromthe entity list Lr and drawing from the underlyingtype property distribution ?r.
Intuitively, a smallvalue of ?r indicates that an entity prefers to re-use2As Blei and Frazier (2009) notes, when marginalizing outtheAi in this trivial case, the DD-CRP reduces to the traditionalCRP (Pitman, 2002), so our discourse model roughly matchesHaghighi and Klein (2007) for proper mentions.?
1PersonOrganization[software]NN-NODORG[Microsoft][company,firm]NOM-HEADNAM-HEADT?
2E1 E2Z1 Z2 Z3M1 M2 M3[Steve,chief,Microsoft]NN-NODPERS[Ballmer,CEO][officer,executive]NOM-HEADNAM-HEADTjoinedGOV-SUBJBallmerSteveNN-HEADNAM-HEADjoinedGOV-DOBJMicrosoftNAM-HEADbecameGOV-DOBJCEONAM-HEAD?
?E, ?
E, ?
E, ?E,M E,ME2E1 E1M MFigure 3: Depiction of the discourse module (Sec-tion 3.2); each random variable is annotated with an ex-ample value.
For each mention position, an entity as-signment (Zi) is made.
Conditioned on entities (EZi ),mentions (Mi) are rendered (Section 3.3).
The Y sym-bol denotes that a random variable is the parent of all Yrandom variables.a small number of words for property r. This is typ-ically the case for proper and nominal heads as wellas modifiers.
At the other extreme, setting ?r to 1indicates the property isn?t particular to the entityitself, but rather only on its type.
We set ?r to 1for pronoun heads as well as for the governor of thehead properties.4 Learning and InferenceOur learning procedure involves finding parame-ters and assignments which are likely under ourmodel?s posterior distribution P (E,Z, ?
,pi|M,X).The model is modularized in such a way that run-ning EM on all variables simultaneously would bevery difficult.
Therefore, we adopt a variational ap-proach which optimizes various subgroups of thevariables in a round-robin fashion, holding approx-imations to the others fixed.
We first describe thevariable groups, then the updates which optimizethem in turn.Decomposition: We decompose the entity vari-388ables E into types, T, one for each entity, and wordlists, L, one for each entity and property.
We decom-pose the mentions M into referring mentions (prop-ers and nominals), Mr, and pronominal mentions,Mp (with sizes nr and np respectively).
The en-tity assignments Z are similarly divided into Zr andZp components.
For pronouns, rather than use Zp,we instead work with the corresponding antecedentvariables, denoted Ap, and marginalize over an-tecedents to obtain Zp.With these variable groups, we wouldlike to approximation our model posteriorP (T,L,Zr,Ap, ?
,pi|M,X) using a simple fac-tored representation.
Our variational approximationtakes the following form:Q(T,L,Zr,Ap, ?
,pi) = ?r(Zr,L)(n?k=1qk(Tk))( np?i=1ri(Api ))?s(?
)?d(pi)We use a mean field approach to update each of theRHS factors in turn to minimize the KL-divergencebetween the current variational posterior and thetrue model posterior.
The ?r, ?s, and ?d factorsplace point estimates on a single value, just as inhard EM.
Updating these factors involves finding thevalue which maximizes the model (expected) log-likelihood under the other factors.
For instance, the?s factor is a point estimate of the type parameters,and is updated with:3?s(?
)?
argmax?
EQ?
?s lnP (E,Z,M, ?
,pi) (1)where Q?
?s denotes all factors of the variationalapproximation except for the factor being updated.The ri (pronoun antecedents) and qk (type indica-tor) factors maintain a soft approximation and so areslightly more complex.
For example, the ri factorupdate takes the standard mean field form:ri(Api ) ?
exp{EQ?ri lnP (E,Z,M, ?
,pi)} (2)We briefly describe the update for each additionalfactor, omitting details for space.Updating type parameters ?s(?
): The type pa-rameters ?
consist of several multinomial distri-butions which can be updated by normalizing ex-pected counts as in the EM algorithm.
The prior3Of course during learning, the argmax is performed overthe entire document collection, rather than a single document.P (?
|?)
consists of several finite Dirichlet draws foreach multinomial, which are incorporated as pseu-docounts.4 Given the entity type variational poste-riors {qk(?
)}, as well as the point estimates of theL and Zr elements, we obtain expected counts fromeach entity?s attribute word lists and referring men-tion usages.Updating discourse parameters ?d(pi): Thelearned parameters for the discourse module rely onpairwise antecedent counts for assignments to nom-inal and pronominal mentions.5 Given these ex-pected counts, which can be easily obtained fromother factors, the update reduces to a weighted max-imum entropy problem, which we optimize usingLBFGS.
The prior P (pi|?2) is a zero-centered nor-mal distribution with shared diagonal variance ?2,which is incorporated via L2 regularization duringoptimization.Updating referring assignments and word lists?r(Zr,L): The word lists are usually concatena-tions of the words used in nominal and propermentions and so are updated together with theassignments for those mentions.
Updating the?r(Zr,L) factor involves finding the referring men-tion entity assignments, Zr, and property wordlists L for instantiated entities which maximizeEQ?
?r lnP (T,L,Zr,Ap,M, ?
,pi).
We actuallyonly need to optimize over Zr, since for any Zr, wecan compute the optimal set of property word listsL.
Essentially, for each entity we can compute theLr which optimizes the probability of the referringmentions assigned to the entity (indicated by Zr).
Inpractice, the optimal Lr is just the set of propertywords in the assigned mentions.
Of course enumer-ating and scoring all Zr hypotheses is intractable,so we instead utilize a left-to-right sequential beamsearch.
Each partial hypothesis is an assignment to aprefix of mention positions and is scored as thoughit were a complete hypothesis.
Hypotheses are ex-tended via adding a new mention to an existing en-tity or creating a new one.
For our experiments, welimited the number of hypotheses on the beam to thetop fifty and did not notice an improvement in modelscore from increasing beam size.4See software release for full hyper-parameter details.5Propers have no learned discourse parameters.389Updating pronominal antecedents ri(Api ) and en-tity types qk(Tk): These updates are straightfor-ward instantiations of the mean-field update (2).To produce our final coreference partitions, we as-sign each referring mention to the entity given by the?r factor and each pronoun to the most likely entitygiven by the ri.4.1 Factor StagingIn order to facilitate learning, some factors are ini-tially set to fixed heuristic values and only learnedin later iterations.
Initially, the assignment factors?r and {ri} are fixed.
For ?r, we use a determin-istic entity assignment Zr, similar to the Haghighiand Klein (2009)?s SYN-CONSTR setting: each re-ferring mention is coreferent with any past men-tion with the same head or in a deterministic syn-tactic configuration (appositives or predicative nom-inatives constructions).6 The {ri} factors are heuris-tically set to place most of their mass on the closestantecedent by tree distance.
During training, we pro-ceed in stages, each consisting of 5 iterations:Stage Learned Fixed B3All1 ?s, ?d, {qk} {ri},?r 74.62 ?s, ?d, {qk}, ?r {ri} 76.33 ?s, ?d, {qk}, ?r, {ri} ?
78.0We evaluate our system at the end of stage using theB3All metric on the A05CU development set (seeSection 5 for details).5 ExperimentsWe considered the challenging end-to-end systemmention setting, where in addition to predictingmention partitions, a system must identify the men-tions themselves and their boundaries automati-cally.
Our system deterministically extracts mentionboundaries from parse trees (Section 5.2).
We uti-lized no coreference annotation during training, butdid use minimal prototype information to prime thelearning of entity types (Section 5.3).5.1 DatasetsFor evaluation, we used standard coreference datasets derived from the ACE corpora:6Forcing appositive coreference is essential for tying properand nominal entity type vocabulary.?
A04CU: Train/dev/test split of the newswireportion of the ACE 2004 training set7 utilizedin Culotta et al (2007), Bengston and Roth(2008) and Stoyanov et al (2009).
Consists of90/68/38 documents respectively.?
A05ST: Train/test split of the newswire portionof the ACE 2005 training set utilized in Stoy-anov et al (2009).
Consists of 57/24 docu-ments respectively.?
A05RA: Train/test split of the ACE 2005 train-ing set utilized in Rahman and Ng (2009).
Con-sists of 482/117 documents respectively.For all experiments, we evaluated on the dev and testsets above.
To train, we included the text of all doc-uments above, though of course not looking at ei-ther their mention boundaries or reference annota-tions in any way.
We also trained on the followingmuch larger unlabeled datasets utilized in Haghighiand Klein (2009):?
BLLIP: 5k articles of newswire parsed with theCharniak (2000) parser.?
WIKI: 8k abstracts of English Wikipedia arti-cles parsed by the Berkeley parser (Petrov etal., 2006).
Articles were selected to have sub-jects amongst the frequent proper nouns in theevaluation datasets.5.2 Mention Detection and PropertiesMention boundaries were automatically detected asfollows: For each noun or pronoun (determined byparser POS tag), we associated a mention with themaximal NP projection of that head or that word it-self if no NP can be found.
This procedure recoversover 90% of annotated mentions on the A05CU devset, but also extracts many unannotated ?spurious?mentions (for instance events, times, dates, or ab-stract nouns) which are not deemed to be of interestby the ACE annotation conventions.Mention properties were obtained from parsetrees using the the Stanford typed dependency ex-tractor (de Marneffe et al, 2006).
The mention prop-erties we considered are the mention head (anno-tated with mention type), the typed modifiers of thehead, and the governor of the head (conjoined with7Due to licensing restriction, the formal ACE test sets arenot available to non-participants.390MUC B3All B3None Pairwise F1System P R F1 P R F1 P R F1 P R F1ACE2004-STOYANOV-TESTStoyanov et al (2009) - - 62.0 - - 76.5 - - 75.4 - - -Haghighi and Klein (2009) 67.5 61.6 64.4 77.4 69.4 73.2 77.4 67.1 71.3 58.3 44.5 50.5THIS WORK 67.4 66.6 67.0 81.2 73.3 77.0 80.6 75.2 77.3 59.2 50.3 54.4ACE2005-STOYANOV-TESTStoyanov et al (2009) - - 67.4 - - 73.7 - - 72.5 - - -Haghighi and Klein (2009) 73.1 58.8 65.2 82.1 63.9 71.8 81.2 61.6 70.1 66.1 37.9 48.1THIS WORK 74.6 62.7 68.1 83.2 68.4 75.1 82.7 66.3 73.6 64.3 41.4 50.4ACE2005-RAHMAN-TESTRahman and Ng (2009) 75.4 64.1 69.3 - - - 54.4 70.5 61.4 - - -Haghighi and Klein (2009) 72.9 60.2 67.0 53.2 73.1 61.6 52.0 72.6 60.6 57.0 44.6 50.0THIS WORK 77.0 66.9 71.6 55.4 74.8 63.8 54.0 74.7 62.7 60.1 47.7 53.0Table 1: Experimental results with system mentions.
All systems except Haghighi and Klein (2009) and current workare fully supervised.
The current work outperforms all other systems, supervised or unsupervised.
For comparison pur-poses, the B3None variant used on A05RA is calculated slightly differently than other B3None results; see Rahmanand Ng (2009).the mention?s syntactic position).
We discard deter-miners, but make use of them in the discourse com-ponent (Section 3.2) for NP definiteness.5.3 Prototyping Entity TypesWhile it is possible to learn type distributions in acompletely unsupervised fashion, we found it use-ful to prime the system with a handful of importanttypes.
Rather than relying on fully supervised data,we took the approach of Haghighi and Klein (2006).For each type of interest, we provided a (possibly-empty) prototype list of proper and nominal headwords, as well as a list of allowed pronouns.
Forinstance, for the PERSON type we might provide:NAM Bush, Gore, HusseinNOM president, minister, officialPRO he, his, she, him, her, you, ...The prototypes were used as follows: Any entitywith a prototype on any proper or nominal headword attribute list (Section 3.1) was constrained tohave the specified type; i.e.
the qk factor (Section 4)places probability one on that single type.
Simi-larly to Haghighi and Klein (2007) and Elsner et al(2009), we biased these types?
pronoun distributionsto the allowed set of pronouns.In general, the choice of entity types to primewith prototypes is a domain-specific question.
Forexperiments here, we utilized the types which areannotated in the ACE coreference data: person(PERS), organization (ORG), geo-political entity(GPE), weapon (WEA), vehicle (VEH), location(LOC), and facility (FAC).
Since the person typein ACE conflates individual persons with groupsof people (e.g., soldier vs. soldiers), we addedthe group (GROUP) type and generated a prototypespecification.We obtained our prototype list by extracting atmost four common proper and nominal head wordsfrom the newswire portions of the 2004 and 2005ACE training sets (A04CU and A05ST); we choseprototype words to be minimally ambiguous withrespect to type.8 When there are not at least threeproper heads for a type (WEA for instance), wedid not provide any proper prototypes and insteadstrongly biased the type fertility parameters to gen-erate empty NAM-HEAD lists.Because only certain semantic types were anno-tated under the arbitrary ACE guidelines, there aremany mentions which do not fall into those limitedcategories.
We therefore prototype (refinements of)the ACE types and then add an equal number of un-constrained ?other?
types which are automaticallyinduced.
A nice consequence of this approach isthat we can simply run our model on all mentions,discarding at evaluation time any which are of non-prototyped types.5.4 EvaluationWe evaluated on multiple coreference resolutionmetrics, as no single one is clearly superior, partic-8Meaning those headwords were assigned to the target typefor more than 75% of their usages.391ularly in dealing with the system mention setting.We utilized MUC (Vilain et al, 1995), B3All (Stoy-anov et al, 2009), B3None (Stoyanov et al, 2009),and Pairwise F1.
The B3All and B3None are B3variants (Bagga and Baldwin, 1998) that differ intheir treatment of spurious mentions.
For PairwiseF1, precision measures how often pairs of predictedcoreferent mentions are in the same annotated entity.We eliminated any mention pair from this calcula-tion where both mentions were spurious.95.5 ResultsTable 1 shows our results.
We compared to twostate-of-the-art supervised coreference systems.
TheStoyanov et al (2009) numbers represent theirTHRESHOLD ESTIMATION setting and the Rahmanand Ng (2009) numbers represent their highest-performing cluster ranking model.
We also com-pared to the strong deterministic system of Haghighiand Klein (2009).10 Across all data sets, our model,despite being largely unsupervised, consistently out-performs these systems, which are the best previ-ously reported results on end-to-end coreference res-olution (i.e.
including mention detection).
Perfor-mance on the A05RA dataset is generally lower be-cause it includes articles from blogs and web forumswhere parser quality is significantly degraded.While Bengston and Roth (2008) do not report onthe full system mention task, they do report on themore optimistic setting where mention detection isperformed but non-gold mentions are removed forevaluation using an oracle.
On this more lenient set-ting, they report 78.4B3 on the A04CU test set.
Ourmodel yields 80.3.6 AnalysisWe now discuss errors and improvements madeby our system.
One frequent source of error isthe merging of mentions with explicitly contrastingmodifiers, such as new president and old president.While it is not unusual for a single entity to admitmultiple modifiers, the particular modifiers new andold are incompatible in a way that new and popular9Note that we are still penalized for marking a spuriousmention coreferent with an annotated one.10Haghighi and Klein (2009) reports on true mentions; here,we report performance on automatically detected mentions.are not.
Our model does not represent the negativecovariance between these modifiers.We compared our output to the deterministic sys-tem of Haghighi and Klein (2009).
Many improve-ments arise from correctly identifying mentionswhich are semantically compatible but which donot explicitly appear in an appositive or predicate-nominative configuration in the data.
For example,analyst and it cannot corefer in our system becauseit is not a likely pronoun for the type PERSON.While the focus of our model is coreference res-olution, we can also isolate and evaluate the typecomponent of our model as an NER system.
Wetest this component by presenting our learned modelwith boundary-annotated non-pronominal entitiesfrom the A05ST dev set and querying their predictedtype variable T .
Doing so yields 83.2 entity clas-sification accuracy under the mapping between ourprototyped types and the coarse ACE types.
Notethat this task is substantially more difficult than theunsupervised NER in Elsner et al (2009) becausethe inventory of named entities is larger (7 vs. 3)and because we predict types over nominal mentionsthat are more difficult to judge from surface forms.In this task, the plurality of errors are confusions be-tween the GPE (geo-political entity) and ORG entitytypes, which have very similar distributions.7 ConclusionOur model is able to acquire and exploit knowledgeat either the level of individual entities (?Obama?
isa ?president?)
and entity types (?company?
can referto a corporation).
As a result, it leverages semanticconstraints more effectively than systems operatingat either level alone.
In conjunction with reasonable,but simple, factors capturing discourse and syntac-tic configurational preferences, our entity-centric se-mantic model lowers coreference error rate substan-tially, particularly on semantically disambiguatedreferences, giving a sizable improvement over thestate-of-the-art.11Acknowledgements: This project is funded inpart by the Office of Naval Research under MURIGrant No.
N000140911081.11See nlp.cs.berkeley.edu and aria42.com/software.html forsoftware release.392ReferencesA Bagga and B Baldwin.
1998.
Algorithms for scoringcoreference chains.
In Linguistic Coreference Work-shop (LREC).Eric Bengston and Dan Roth.
2008.
Understandingthe Value of Features for Corefernce Resolution.
InEmpirical Methods in Natural Language Processing(EMNLP).David Blei and Peter I. Frazier.
2009.
Dis-tance Dependent Chinese Restaurant Processes.http://arxiv.org/abs/0910.1022/.Eugene Charniak.
2000.
Maximum Entropy InspiredParser.
In North American Chapter of the Associationof Computational Linguistics (NAACL).Michael Collins and Yoram Singer.
1999.
UnsupervisedModels for Named Entity Classification.
In EmpiricalMethods in Natural Language Processing (EMNLP).Mike Collins.
1999.
Head-Driven Statistical Models forNatural Language Parsing.
Ph.D. thesis, University ofPennsylvania.A Culotta, M Wick, R Hall, and A McCallum.
2007.First-order Probabilistic Models for Coreference Res-olution.
In Proceedings of the conference on HumanLanguage Technology and Empirical Methods in Nat-ural Language Processing (NAACL-HLT).M.
C. de Marneffe, B. Maccartney, and C. D. Manning.2006.
Generating Typed Dependency Parses fromPhrase Structure Parses.
In LREC.M Elsner, E Charniak, and M Johnson.
2009.
Structuredgenerative models for unsupervised named-entity clus-tering.
In Proceedings of Human Language Technolo-gies: The 2009 Annual Conference of the North Ameri-can Chapter of the Association for Computational Lin-guistics, pages 164?172.Barbara J. Grosz, Aravind K. Joshi, and Scott Weinstein.1995.
Centering: A Framework for Modeling the Lo-cal Coherence of Discourse.
Computational Linguis-tics, 21(2):203?225.Aria Haghighi and Dan Klein.
2006.
Prototype-DrivenLearning for Sequence Models.
In HLT-NAACL.
As-sociation for Computational Linguistics.Aria Haghighi and Dan Klein.
2007.
UnsupervisedCoreference Resolution in a Nonparametric BayesianModel.
In Proceedings of the 45th Annual Meeting ofthe Association of Computational Linguistics.
Associ-ation for Computational Linguistics.Aria Haghighi and Dan Klein.
2009.
Simple CoreferenceResolution with Rich Syntactic and Semantic Features.In Proceedings of the 2009 Conference on EmpiricalConference in Natural Language Processing.J.
R. Hobbs.
1978.
Resolving Pronoun References.
Lin-gua, 44.J.
R. Hobbs.
1979.
Coherence and Coreference.
Cogni-tive Science, 3:67?90.Andrew Kehler, Laura Kertz, Hannah Rohde, and JeffreyElman.
2008.
Coherence and Coreference Revisited.Vincent Ng and Claire Cardie.
2002.
ImprovingMachine Learning Approaches to Coreference Res-olution.
In Association of Computational Linguists(ACL).Vincent Ng.
2005.
Machine Learning for Corefer-ence Resolution: From Local Classification to GlobalRanking.
In Association of Computational Linguists(ACL).Vincent Ng.
2007.
Shallow semantics for coreferenceresolution.
In IJCAI?07: Proceedings of the 20th in-ternational joint conference on Artifical intelligence,pages 1689?1694.Slav Petrov, Leon Barrett, Romain Thibaux, and DanKlein.
2006.
Learning Accurate, Compact, and Inter-pretable Tree Annotation.
In Proceedings of the 21stInternational Conference on Computational Linguis-tics and 44th Annual Meeting of the Association forComputational Linguistics, pages 433?440, Sydney,Australia, July.
Association for Computational Lin-guistics.J.
Pitman.
2002.
Combinatorial Stochastic Processes.
InLecture Notes for St. Flour Summer School.A Rahman and V Ng.
2009.
Supervised models forcoreference resolution.
In Proceedings of the 2009Conference on Empirical Conference in Natural Lan-guage Processing.W.H.
Soon, H. T. Ng, and D. C. Y. Lim.
1999.
A Ma-chine Learning Approach to Coreference Resolutionof Noun Phrases.V Stoyanov, N Gilbert, C Cardie, and E Riloff.
2009.Conundrums in Noun Phrase Coreference Resolution:Making Sense of the State-of-the-art.
In Associate ofComputational Linguistics (ACL).Marc Vilain, John Burger, John Aberdeen, Dennis Con-nolly, and Lynette Hirschman.
1995.
A model-theoretic coreference scoring scheme.
In MUC-6.X Yang, J Su, and CL Tan.
2005.
Improving pronounresolution using statistics-based semantic compatibil-ity information.
In Association of Computational Lin-guists (ACL).393
