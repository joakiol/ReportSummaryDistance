Proceedings of the TextGraphs-6 Workshop, pages 42?50,Portland, Oregon, USA, 19-24 June 2011. c?2011 Association for Computational LinguisticsSimultaneous Similarity Learning and Feature-Weight Learning forDocument ClusteringPradeep MuthukrishnanDept of CSE,University of Michiganmpradeep@umich.eduDragomir RadevSchool of Information,Dept of CSE,University of Michiganradev@umich.eduQiaozhu MeiSchool of Information,Dept of CSE,University of Michiganqmei@umich.eduAbstractA key problem in document classification andclustering is learning the similarity betweendocuments.
Traditional approaches includeestimating similarity between feature vectorsof documents where the vectors are computedusing TF-IDF in the bag-of-words model.However, these approaches do not work wellwhen either similar documents do not use thesame vocabulary or the feature vectors are notestimated correctly.In this paper, we represent documents andkeywords using multiple layers of connectedgraphs.
We pose the problem of simultane-ously learning similarity between documentsand keyword weights as an edge-weight regu-larization problem over the different layers ofgraphs.
Unlike most feature weight learningalgorithms, we propose an unsupervised algo-rithm in the proposed framework to simulta-neously optimize similarity and the keywordweights.
We extrinsically evaluate the perfor-mance of the proposed similarity measure ontwo different tasks, clustering and classifica-tion.
The proposed similarity measure out-performs the similarity measure proposed by(Muthukrishnan et al, 2010), a state-of-the-art classification algorithm (Zhou and Burges,2007) and three different baselines on a vari-ety of standard, large data sets.1 IntroductionThe recent upsurge in the amount of text availabledue to the widespread growth of the Internet has ledto the need for large scale, efficient Machine Learn-ing (ML), Information Retrieval (IR) tools for textmining.
At the heart of many of the ML, IR algo-rithms is the need for a good similarity measure be-tween documents.
For example, a better similaritymeasure almost always leads to better performancein tasks like document classification, clustering, etc.Traditional approaches represent documents withmany keywords using a simple feature vector de-scription.
Then, similarity between two documentsis estimated using the dot product between theircorresponding vectors.
However, such similaritymeasures do not use all the keywords together andhence, suffer from problems due to sparsity.
Thereare two major issues in computing similarity be-tween documents?
Similar documents may not use the same vo-cabulary.?
Estimating feature weights or weight of a key-word to the document it is contained in.For example, consider two publications, X andY , in the field of Machine Learning.
Let X be apaper on clustering while Y is on classification.
Al-though the two publications use very different vo-cabulary, they are semantically similar.
Keywordweights are mostly estimated using the frequency ofthe keyword in the document.
For example, TF-IDFbased scoring is the most commonly used approachto compute keyword weights to compute similaritybetween documents.
However, suppose publicationsX and Y mention the keyword ?
?Machine Learn-ing??
only once.
Although, they are mentioned onlyonce in the text of the document, for the purposesof computing semantic similarity between the docu-42ments, it would be beneficial to give it a high key-word weight.A commonly used approach to estimate seman-tic similarity between documents is to use an ex-ternal knowledge source like WordNet (Pedersenet al, 2004).
However, these approaches are do-main dependent and language dependent.
If docu-ment similarity can not be estimated accurately us-ing just the text, there have been approaches incor-porating multiple sources of similarity like link sim-ilarity, authorship similarity between publications(Bach et al, 2004; Cortes et al, 2009).
(Muthukr-ishnan et al, 2010) also uses multiple sources ofsimilarity.
In addition to improving similarity es-timates between documents, it also improves sim-ilarity estimates between keywords.
Co-clustering(Dhillon et al, 2003) based approaches are usedto alleviate problems due to the sparsity and high-dimensionality of the data.
In co-clustering, the key-words and the documents are simultaneously clus-tered by exploiting the duality between them.
How-ever, the approach relies solely on the keyword dis-tributions to cluster the documents and vice-versa.It does not take into account the inherent similar-ity between the keywords (documents) when cluster-ing the documents (keywords).
Also, co-clusteringtakes as input the weight of all keywords to corre-sponding documents.2 MotivationFirst, we explain how similarity learning and fea-ture weight learning can mutually benefit from eachother using an example.
For example, consider thefollowing three publications in the field of MachineTranslation, (Brown et al, 1990; Gale and Church,1991; Marcu and Wong, 2002)Clearly, all the papers belong to the field of Ma-chine Translation but (Gale and Church, 1991) con-tains the phrase ?
?Machine Translation??
only oncein the entire text.
However, we can learn to attributesome similarity between (Brown et al, 1990) andthe second publication using the text in (Marcu andWong, 2002).
The keywords ?
?Bilingual Corpora?
?and ?
?Machine Translation??
co-occur in the text in(Marcu andWong, 2002) which makes the keywordsthemselves similar.
Now we can attribute some sim-ilarity between the (Brown et al, 1990) and (MarcuandWong, 2002) publication since they contain sim-ilar keywords.
This shows how similarity learningcan benefit from important keywords.Now, assume that ?
?Machine Translation??
is animportant keyword (high keyword weight) for thefirst and third publication while ?
?Bilingual Cor-pora??
is an important keyword for the second pub-lication.
We explained how to infer similarity be-tween the first and second publication using the thirdpublication as a bridge.
Using the newly learnedsimilarity measure, we can infer that ??BilingualCorpora??
is an important keyword for the sec-ond publication since a similar keyword (??MachineTranslation??)
is an important keyword for similarpublications.Let documents Di and Dj contain keywords Kikand Kjl.
Then intuitively, the similarity betweentwo documents should be jointly proportional to?
The similarity between keywords Kik and Kjl?
The weights of Kik to Di and Kjl to Dj .Similarly the weight of a keyword Kik to docu-ment Di should be jointly proportional to?
The similarity between documents Di and Dj .?
The similarity between keyphrases Kik andKjl and weight of Kjl to Dj .The major contributions of the paper are given be-low,?
A rich representation model for representingdocuments with associated keywords for effi-ciently estimating document similarity..?
A regularization framework for joint featureweight (keyword weight) learning and similar-ity learning.?
An unsupervised algorithm in the proposedframework to efficiently learn similarity be-tween documents and the weights of keywordsfor each document in a set of documents.In the next two sections, we formalize and ex-ploit this observation to jointly optimize similaritybetween documents and weight of keywords to doc-uments in a principled way.433 Problem FormulationWe assume that a set of keywords have been ex-tracted for the set of documents to be analyzed.
Thesetup is very general: Documents are representedby the set of candidate keywords.
In addition tothat, we have crude initial similarities estimatedbetween documents and also between keywordsand the weights of keywords to documents.
Thesimilarities and keyword weights are representedusing two layers of graphs.
We formally define thenecessary concepts,Definition 1: Documents and correspondingkeywordsWe have a set of N documents D ={d1, d2, .
.
.
, dN}.
Each document, di has a setof mi keywords Ki = {ki1, ki2, .
.
.
, kimi}Definition 2: Document Similarity GraphThe document similarity graph, G1 = (V1, E1),consists of the set of documents as nodes and theedge weights represent the initial similarity betweenthe documents.Definition 3: Keyword Similarity GraphThe keyword similarity graph, G2 = (V2, E2), con-sists of the set of keywords as nodes and the edgeweights represent the initial similarity between thekeywords.The document similarity graph and the keywordsimilarity graph can be considered as two layers ofgraphs which are connected by the function definedbelowDefinition 4: keyword Weights (KW)There exists an edge between di and kij for 1 ?
j ?mi.
Let Z represent the keyword weighting func-tion, i.e, Zdi,kij represents the weight of keywordkij t document di.4 Regularization Framework?
(w,Z) = ?0 ?
ISC(w,w?)
+ ?1 ?
IKC(Z,Z?
)+?2 ?KS(w,Z) + ?3 ?
SK(Z,w) (1)where ?0 + ?1 + ?2 + ?3 = 1.ISC refers to Initial Similarity Criterion and IKCrefers to Initial Keyword weight Criterion.
They aredefined as followsISC(w,w?)
=?u,v?G1(wu,v ?
w?u,v)2 (2)IKC(Z,Z?)
=?u?G1,v?G2(Zu,v ?
Z?u,v)2 (3)KS refers toKeyword based Similarity and SK refersto Similarity induced Keyword weight.
They are de-fined as followsKS(w,Z) =?u1,v1?G1?u2,v2?G2Zu1,u2Zv1,v2(wu1,v1 ?
wu2,v2)2 (4)andSK(w,Z) =?u1,v1?G1?u2,v2?G2wu1,v1wu2,v2(Zu1,u2 ?
Zv1,v2)2 (5)Then the task is to minimize the objective function.The objective function consists of four parts.
Thefirst and second parts are initial similarity criterionand initial keyword criterion.
They ensure that theoptimized edge weights are close to the initial edgeweights.
The weights ?0 and ?1 ensure that the op-timized weights are close to the initial weights, inother words, they represent the confidence level ininitial weights.The significance of the third and the fourth partsof the objective function are best explained by a sim-ple example.
Consider two graphs, G1 and G2.
LetG1 be the graph containing publications as nodesand edge weights representing initial similarity val-ues.
Let G2 be the graph corresponding to keywordsand edge weights represent similarity between key-words.
There is an edge from a node u1 in G1 to anode v1 in G2 if the publication corresponding to u1contains the keyword corresponding to v1.According to this example, minimizing the key-word weight induced similarity part corresponds toupdating similarity values between keywords in pro-portion to weights of the keywords to the respectivedocuments they are contained in and the similaritybetween the documents.
keyword weight inducedsimilarity part also helps updating similarity values44between documents in proportion to weights of key-words they contain and the similarity between thecontained keywords.Minimizing the similarity induced keyword partcorresponds to updating keyword weights in propor-tion to the following?
Similarity between v1 and other keywords v2 ?G2?
Keyword weight of v2 to documents u2 ?
G1?
Similarity between u1 and u2Therefore, even if frequency of a keyword suchas ?
?Machine Translation??
in a publication is nothigh, it can achieve a high keyword weight if it con-tains many other similar keywords such as ?
?Bilin-gual Corpora??
and ?
?Word alignment?
?.5 Efficient AlgorithmWe seek to minimize the objective function usingAlternating Optimization (AO) (Bezdek and Hath-away, 2002), an approximate optimization method.Alternating optimization is an iterative procedure forminimizing (or maximizing) the function f(x) =f(X1, X2, .
.
.
, Xt) jointly over all variables by al-ternating restricted minimizations over the individ-ual subsets of variables X1, .
.
.
, Xt.In this optimization method, we partition the setof variables into a set of mutually exclusive, exhaus-tive subsets.
We iteratively perform minimizationsover each subset of variables while maintaining theother subsets of variables fixed.
Formally, let the setof real-valued variables be X = {X1, X2, .
.
.
, XN}be partitioned into m subsets, {Y1, Y2, .
.
.
, Ym}.Also, let si = |Yi|.
Then we begin with the ini-tial set of values {Y 01 , Y20, .
.
.
, Ym0} and make re-stricted minimizations of the following form,minYi?Rsi{f(Y1r+1, .
.
.
, Yi?1r+1, Yi, Yi+1r, .
.
.
, Ymr)}(6)where i = 1, 2, .
.
.
,m. The underline notation Yjindicates that the subset of variables Yj are fixedwith respect to Yi.
In the context of our prob-lem, we update each edge weight while maintainingother edge weights to be a constant.
Then the prob-lem boils down to the minimization problem over asingle edge weight.
For example, let us solve theminimization problem for edge weight correspond-ing to (ui, vj) where ui, vj ?
G1 (The case whereui, vj ?
G2 is analogous).
Clearly the objectivefunction is a convex function in w(ui, vj).
The par-tial derivative of the objective function with respectto the edge weight is given below,??
(w,Z)?wui,vj= 2?0(wui,vj ?
w?ui,vj )+2?2 ?
?u2,v2?G2(wui,vj ?
wu2,v2)Zu1,u2Zvj ,v2+?3 ?
?u2,v2?G2(Zui,u2 ?
Zvj ,v2)2wui,vjwu2,v2.
(7)To minimize the above function, we set the partialderivative to zero which gives us the following ex-pression,wuj ,vk =1C1(?0w?ui,vj +?2?u2,v2?G2Zui,u2 wu2,v2 Zvj ,v2)(8)whereC1 = ?0 + ?2?u2,v2?G2Zui,u2 Zvj ,v2+?32?u2,v2?G2(Zui,u2 ?
Zvj ,v2)2wu2,v2Similarly, we can derive the update equation forkeyword weights, Zui,uj as below,Zui,uj =1C2(?1Z?ui,uj +?3?v1?G1?v2?G2wui,v1 wuj ,v2 Zv1,v2)(9)where,C2 = ?1 + ?3?v1?G1?v2?G2wui,v1 wuj ,v2+?22?v1?G1?v2?G2(wui,v1 ?
wuj ,v2)2Zv1,v245The similarity score between two nodes is propor-tional to the similarity between nodes in the otherlayer.
For example, the similarity between two doc-uments (keywords) is proportional to the similaritybetween the keywords the documents they contain(the documents they are contained in).
C plays therole of a normalization constant.
Therefore, for sim-ilarity between two nodes to be high, it is requiredthat they not only contain a lot of similar nodes inthe other graph but the similar nodes need to be im-portant to the two target nodes.Similarly, a particular keyword will have a highweight to a document if similar keywords have highweights to similar documents.
Also, it is neces-sary that the similarity between the keywords andthe documents are high.It can be shown that equations 8 and 9 convergeq?
linearly since the minimization problem is con-vex in each of the variables individually and hencehas a global and unique minimizer (Bezdek andHathaway, 2002).5.1 Layered Random Walk InterpretationThe above algorithm has a very nice intuitive inter-preation in terms of random walks over the two dif-ferent graphs.
Assume the initial weights are transi-tion probability values after the graphs are normal-ized so that each row of the adjacency matrices sumsto 1.
Then the similarity between two nodes u and vin the same graph is computed as sum of two parts.The first part is ?0 times the initial similarity.
Thisis necessary so that the optimized similarity valuesare not too far away from the initial similarity val-ues.
The second part corresponds to the probabilityof a random walk of length 3 starting at u and reach-ing v through two intermediate nodes from the othergraph.The semantics of the random walk depends onwhether u, v are documents or keywords.
For exam-ple, if the two nodes are documents, then the simi-larity between two documents d1 and d2 is the prob-ability of random walk starting at document d1 andthen moving to a keyword k1 and then moving tokeyword k2 and then to document d2.
Note that key-words k1 and k2 can be the same keyword whichaccounts for similarity between documents becausethey contain the same keyword.Also, note that second and higher order depen-dencies are also taken into account by this algo-rithm.
That is, two papers may become similar be-cause they contain two keywords which are con-nected by a path in the keyword graph, whose lengthis greater than 1.
This is due to the iterative natureof the algorithm.
For example, keywords ??MachineTranslation??
and ?
?Bilingual corpora??
occur oftentogether and hence any co-occurrence based simi-larity measure will assign a high initial similarityvalue.
Hence two publications which contain thesewords will be assigned a non-zero similarity valueafter a single iteration.
Also, ?
?Bilingual corpora?
?and ??SMT??
(abbreviation for Statistical MachineTranslation) can have a high initial similarity valuewhich enables assiging a high similarity value be-tween ?
?Machine Translation??
and ??SMT??.
Thisleads to a chain effect as the number of iterations in-creases which helps assign non-zero similarity val-ues between semantically similar documents even ifthey do not contain common keywords.6 ExperimentsIt is very hard to evaluate similarity measures in iso-lation.
Thus, most of the algorithms to compute sim-ilarity scores are evaluated extrinsically, i.e, the sim-ilarity scores are used for an external task like clus-tering or classification and the performance in theexternal task is used as the performance measure forthe similarity scores.
This also helps demonstratethe different applications of the computed similar-ity measure.
Thus, we perform a variety of differ-ent experiments on standard data sets to illustratethe improved performance of the proposed similar-ity measure.
There are three natural variants of thealgorithm,?
Unified: We compare against the edge-weightregularization algorithm proposed in (Muthukr-ishnan et al, 2010).
The algorithm has thesame representation as our algorithm but theoptimization is strictly defined over the edgeweights in the two layers of the graph, wij?sand not on the keyword weights.
Therefore,Zij are maintained constant throughout the al-gorithm.?
Unified-binary: In this variant, we initialize thekeyword weights to 1, i.e, Zij = 1 wheneverdocument i contains the keyword j.46ACL-ID Paper Title Research TopicW05-0812 Improved HMM Alignment Models for Languages With ScarceResourcesMachine TranslationP07-1111 A Re-Examination of Machine Learning Approaches for Sentence-Level MT EvaluationMachine TranslationP03-1054 Accurate Unlexicalized Parsing Dependency ParsingP07-1050 K-Best Spanning Tree Parsing Dependency ParsingP88-1020 Planning Coherent Multi-Sentential Text SummarizationTable 1: Details of a few sample papers classified according to research topic?
Unified-TFIDF: We initialize the keywordweights to the TFIDF scores, Zij is set to theTFIDF score of keyword j for document i.Experiment Set I: We compare our similarity mea-sure against other similarity measures in the contextof classification.
We also compare against a stateof the art classification algorithm which uses differ-ent similarity measures due to different feature typeswithout integrating them into one single similaritymeasure.
Specifically, we compare our algorithmagainst three other similarity baselines in the contextof classification which are listed below.?
Content Similarity: Similarity is computed us-ing just the feature vector representation usingjust the text.
We use cosine similarity after pre-processing each document into a tf.idf vectorfor the AAN data set.
For all other data sets,we use the cosine similarity on the binary fea-ture vector representation that is available.?
Link Similarity: Similarity is computed usingonly the links (citations, in the case of publica-tions).
To compute link similarity, we use thenode similarity algorithm proposed by (Hareland Koren, 2001) using a random walk oflength 3 on the link graph.?
Linear combination: The content similarity(CS) and link similarity (LS) between docu-ments x and y are combined in a linear fashionas ?CS(x, y)+(1??
)LS(x, y).
We tried dif-ferent values of ?
and report only the best accu-racy that can be achieved using linear combina-tion of similarity measures.
Note that this is anupper bound on the accuracy of Multiple Ker-nel Learning with the restriction of the combi-nation being affine.We also compare our algorithm against the follow-ing algorithms SC-MV: We compare our algorithmagainst the spectral classification algorithm for datawith multiple views (Zhou and Burges, 2007).
Thealgorithm tries to classify data when multiple viewsof the data are available.
The multiple views are rep-resented using multiple homogeneous graphs with acommon vertex set.
In each graph, the edge weightsrepresent similarity between the nodes computed us-ing a single feature type.
For our experiments, weused the link similarity graph and the content simi-larity graph as described above as the two views ofthe same dataWe use a semi-supervised graph classification al-gorithm (Zhu et al, 2003) to perform the classifica-tion.Experiment Set II: We illustrate the improvedperformance of our similarity measure in the con-text of clustering.
We compare our similarity mea-sure against the three similarity baselines mentionedabove.
We use a spectral graph clustering algorithmproposed in (Dhillon et al, 2007) to perform theclustering.We performed our experiments on three differentdata sets.
The three data sets are explained below.?
AAN Data: The ACL Anthology is a collec-tion of papers from the Computational Lin-guistics journal as well as proceedings fromACL conferences and workshops and includes15, 160 papers.
To build the ACL AnthologyNetwork (AAN), (Radev et al, 2009) manu-ally performed some preprocessing tasks in-cluding parsing references and building the net-work metadata, the citation, and the author col-laboration networks.
The full AAN includesthe raw text of all the papers in addition to fullcitation and collaboration networks.We chose a subset of papers in 3 topics (Ma-470.450.50.550.60.650.70.750.80.850.90  10  20  30  40  50  60  70ContentLinkLinearSC-MVUnifiedUnified-binaryUnified-TFIDF(a) AAN0.450.50.550.60.650.70.750.810  15  20  25  30  35  40ContentLinkLinearSC-MVUnifiedUnified-binaryUnified-TFIDF(b) Cornell0.50.550.60.650.70.750.80.8510  15  20  25  30  35  40ContentLinkLinearSC-MVUnifiedUnified-binaryUnified-TFIDF(c) Texas0.50.550.60.650.70.750.80.8510  15  20  25  30  35  40  45  50ContentLinkLinearSC-MVUnifiedUnified-binaryUnified-TFIDF(d) Washington0.50.550.60.650.70.750.80.8510  15  20  25  30  35  40  45ContentLinkLinearSC-MVUnifiedUnified-binaryUnified-TFIDF(e) Wisconsin0.50.550.60.650.70.750.80.850.90.95150  100  150  200  250  300  350  400  450  500ContentLinkLinearSC-MVUnifiedUnified-binaryUnified-TFIDF(f) CoraFigure 1: Classification Accuracy on the different data sets.
The number of points labeled is plotted alongthe x-axis and the y-axis shows the classification accuracy on the unlabeled data.chine Translation, Dependency Parsing, Sum-marization) from the ACL anthology.
Thesetopics are three main research areas in NaturalLanguage Processing (NLP).
Specifically, wecollected all papers which were cited by pa-pers whose titles contain any of the followingphrases, ?
?Dependency Parsing?
?, ??MachineTranslation?
?, ??Summarization??.
From thislist, we removed all the papers which containedany of the above phrases in their title because48this would make the clustering task easy.
Thepruned list contains 1190 papers.
We manuallyclassified each paper into four classes (Depen-dency Parsing, Machine Translation, Summa-rization, Other) by considering the full text ofthe paper.
The manually cleaned data set con-sists of 275Machine Translation papers, 73 De-pendency Parsing papers and 32 Summariza-tion papers.
Table 1 lists a few sample papersfrom each class.WebKB(Sen et al, 2008): The data set con-sists of a subset of the original WebKB data set.The corpus consists of 877 web pages collectedfrom four different universities.
Each web pageis represented by a 0/1-valued word vector with1703 unique words after stemming and remov-ing stopwords.
All words with document fre-quency less than 10 were removed.Cora(Sen et al, 2008): The Cora dataset con-sists of 2708 scientific publications classifiedinto one of seven classes.
The citation networkconsists of 5429 links.
Each publication in thedataset is described by a 0/1-valued word vec-tor indicating the absence/presence of the cor-responding word from the dictionary.
The dic-tionary consists of 1433 unique words.For all the data sets, we constructed two graphs,the kewyord feature graph and the link similaritygraph.
The keyword feature layer graph, Gf =(Vf , Ef , wf ) is a weighted graph where Vf is theset of all features.
The edge weight between key-words fi and fj represents the similarity betweenthe features.
The edge weights are initialized to thecosine similarity between their corresponding doc-ument vectors.
The link similarity graph, Go =(Vo, Eo, wo) is another weighted graph where Vois the set of objects.
The edge weight representsthe similarity between the documents and is initial-ized to the similarity between the documents due tothe link structure.
The link similarity between twodocuments is computed using the similarity mea-sure proposed by (Harel and Koren, 2001) on thecitation graph.
We also performed experiments byinitializing the similarity between documents to thekeyword similarity.
Although, our algorithm stilloutperforms other algorithms and the baselines (notshown due to space restrictions), the accuracy usingcitation similarity is higher.7 Results and DiscussionFigure 1 shows the accuracy of the classification ob-tained using different similarity measures.
It can beseen that the proposed algorithm (both the variants)performs much better than other similarity measuresby a large margin.
The algorithm performs muchbetter when more information is provided in theform of TF-IDF scores.
We attribute this to therich representation of the data.
In our algorithm, thedata is represented as a set of heterogeneous graphs(layers) which are connected together instead of thenormal feature vector representation.
Thus, we canleverage on the similarity between the keywords andthe objects (documents) to iteratively improve sim-ilarity in both layers.
Whereas, in the case of thealgorithm in (Zhou and Burges, 2007) all the graphsare isolated homogeneous graphs.
Hence there is noinformation transfer across the different graphs.For the clustering task, we use Normalized Mu-tual Information (NMI) (Strehl and Ghosh, 2002)between the ground truth clusters and the outputtedclustering as the measure of clustering accuracy.Table 2 shows the Normalized Mutual Informa-tion scores obtained by the different similarity mea-sures on the different data sets.8 ConclusionIn this paper, we have proposed a novel approachto compute similarity between documents and key-words iteratively.
We formalized the problem ofsimilarity estimation as an optimization problem in-duced by a regularization framework over edges inmultiple graphs.
We propose an efficient, iterativealgorithm based on Alternating Optimization (AO)which has a neat, intuitive interpretation in termsof random walks over multiple graphs.
We demon-strated the improved performance of the proposedalgorithm over many different baselines and a state-of-the-art classifcation algorithm and a similaritymeasure which uses the same information as givento our algorithm.49Similarity Measure AAN Texas Wisconsin Washington Cornell CoraContent Similarity (Cosine) 0.66 0.34 0.42 0.59 0.63 0.48Link Similarity 0.45 0.49 0.39 0.52 0.56 0.52Linear Combination 0.69 0.54 0.46 0.54 0.68 0.54Unified Similarity 0.78 0.69 0.54 0.66 0.72 0.64Unified Similarity-Binary 0.80 0.68 0.56 0.69 0.74 0.66Unified Similarity-TFIDF 0.84 0.70 0.60 0.72 0.78 0.70Table 2: Normalized Mutual Information scores of the different similarity measures on the different datasetsReferencesFrancis R. Bach, Gert R. G. Lanckriet, and Michael I.Jordan.
2004.
Multiple kernel learning, conic duality,and the smo algorithm.
In Proceedings of the twenty-first international conference on Machine learning,ICML ?04, pages 6?, New York, NY, USA.
ACM.James Bezdek and Richard Hathaway.
2002.
Some noteson alternating optimization.
In Nikhil Pal and MichioSugeno, editors, Advances in Soft Computing AFSS2002, volume 2275 of Lecture Notes in Computer Sci-ence, pages 187?195.
Springer Berlin.Peter F. Brown, John Cocke, Stephen A. Della Pietra,Vincent J. Della Pietra, Fredrick Jelinek, John D. Laf-ferty, Robert L. Mercer, and Paul S. Roossin.
1990.
Astatistical approach to machine translation.
Computa-tional Linguistics.Corinna Cortes, Mehryar.
Mohri, and Afshin Ros-tamizadeh.
2009.
Learning non-linear combinationsof kernels.
In In NIPS.Inderjit S. Dhillon, Subramanyam Mallela, and Dhar-mendra S. Modha.
2003.
Information-theoretic co-clustering.
In Proceedings of the ninth ACM SIGKDDinternational conference on Knowledge discovery anddata mining, KDD ?03, pages 89?98, New York, NY,USA.
ACM.Inderjit S. Dhillon, Yuqiang Guan, and Brian Kulis.2007.
Weighted graph cuts without eigenvectorsa multilevel approach.
IEEE Transactions on Pat-tern Analysis and Machine Intelligence, 29(11):1944?1957, November.William A. Gale and Kenneth Ward Church.
1991.
Aprogram for aligning sentences in bilingual corpora.In In Proceedings of ACL.David Harel and Yehuda Koren.
2001.
On clustering us-ing random walks.
In Foundations of Software Tech-nology and Theoretical Computer Science 2245, pages18?41.
Springer-Verlag.Daniel Marcu andWilliamWong.
2002.
A phrase-based,joint probability model for statistical machine transla-tion.
In In Proceedings of EMNLP.Pradeep Muthukrishnan, Dragomir Radev, and QiaozhuMei.
2010.
Edge weight regularization over multiplegraphs for similarity learning.
In In ICDM.Ted Pedersen, Siddharth Patwardhan, and Jason Miche-lizzi.
2004.
Wordnet::similarity: measuring therelatedness of concepts.
In Demonstration Papersat HLT-NAACL 2004, HLT-NAACL?Demonstrations?04, pages 38?41, Stroudsburg, PA, USA.
Associationfor Computational Linguistics.Dragomir R. Radev, Pradeep Muthukrishnan, and VahedQazvinian.
2009.
The ACL Anthology Network cor-pus.
In In Proceedings of the ACL Workshop on Nat-ural Language Processing and Information Retrievalfor Digital Libraries.Prithviraj Sen, Galileo Mark Namata, Mustafa Bilgic,Lise Getoor, Brian Gallagher, and Tina Eliassi-Rad.2008.
Collective classification in network data.
AIMagazine, 29(3):93?106.Alexander Strehl and Joydeep Ghosh.
2002.
Cluster en-sembles: a knowledge reuse framework for combiningpartitionings.
In Eighteenth national conference onArtificial intelligence, pages 93?98, Menlo Park, CA,USA.
American Association for Artificial Intelligence.Dengyong Zhou and Christopher J. C. Burges.
2007.Spectral clustering and transductive learning with mul-tiple views.
In ICML ?07, pages 1159?1166, NewYork, NY, USA.Xiaojin Zhu, Zoubin Ghahramani, and John Lafferty.2003.
Semi-supervised learning using gaussian fieldsand harmonic functions.
In ICML 2003, pages 912?919.50
