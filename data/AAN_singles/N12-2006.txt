Proceedings of the NAACL HLT 2012 Student Research Workshop, pages 29?34,Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational LinguisticsChoosing an Evaluation Metric for Parser DesignWoodley Packardsweaglesw@sweaglesw.orgAbstractThis paper seeks to quantitatively evaluate thedegree to which a number of popular met-rics provide overlapping information to parserdesigners.
Two routine tasks are considered:optimizing a machine learning regularizationparameter and selecting an optimal machinelearning feature set.
The main result is that thechoice of evaluation metric used to optimizethese problems (with one exception amongpopular metrics) has little effect on the solu-tion to the optimization.1 IntroductionThe question of how best to evaluate the perfor-mance of a parser has received considerable atten-tion.
Numerous metrics have been proposed, andtheir relative merits have been debated.
In this pa-per, we seek to quantitatively evaluate the degree towhich a number of popular metrics provide overlap-ping information for two concrete subtasks of theparser design problem.The motivation for this study was to confirm oursuspicion that parsing models that performed wellunder one metric were likely to perform well un-der other metrics, thereby validating the widespreadpractice of using just a single metric when conduct-ing research on improving parser performance.
Ourresults are cautiously optimistic on this front.1We use the problem of selecting the best per-former from a large space of varied but related parse1Note that we are not suggesting that these metrics provideredundant information for other uses, e.g.
predicting utility forany particular downstream task.disambiguation models (?parsers?
henceforth) as thesetting for our study.
The parsers are all conditionallog-linear disambiguators with quadratic regulariza-tion, coupled to the English Resource Grammar(ERG) (Flickinger, 2000), a broad-coverage HPSG-based hand-built grammar of English.
Analysesfrom the ERG consist of a syntax tree together withan underspecified logical formula called an MRS(Copestake et al, 2005).The parsers differ from each other along two di-mensions: the feature templates employed, and thedegree of regularization used.
There are 57 differ-ent sets of traditional and novel feature templatescollecting a variety of syntactic and semantic dataabout candidate ERG analyses.
For each set of fea-ture templates, parsers were trained with 41 differentvalues for the quadratic regularization parameter, fora total of 2337 different parsers.TheWeScience Treebank of about 9100 sentences(Ytrest?l et al, 2009) was used both for training andtesting the parsers, with 10-fold cross validation.We break down the problem of selecting the bestparser into two tasks.
The first task is to identifythe optimal value for the regularization parameterfor each set of feature templates.
The second taskis to compare the different sets of feature templatesto each other, considering only the optimal value ofthe regularization parameter for each, and select theoverall best.
We attack each task with each of 14metrics, and discuss the results.2 Prior WorkComparisons of parser metrics have been under-taken in the past.
Carroll et al(1998) describe a29broad range of parser evaluation metrics, and com-ment on their advantages and disadvantages, but donot offer a quantitative comparison.
A number of pa-pers such as Clark and Curran (2007) have exploredthe difficulty of parser comparison across differentunderlying formalisms.Crouch et al(2002) compare two variantdependency-based metrics in some detail on a singleLFG-based parsing model, concluding that despitesome differences in the metrics?
strategies, they of-fer similar views on the performance of their parser.The literature specifically seeking to quantita-tively compare a broad range of metrics across alarge array of parsers is small.
Emms (2008) de-scribes the tree-distance metric and compares therankings induced by several variants of that met-ric and PARSEVAL on a collection of six statisti-cal parsers, finding broad compatibility, but observ-ing frequent disagreement about the relative ranksof two parsers whose scores were only marginallydifferent.3 MetricsIn our setup, the overall score a metric assigns toa parser is the average of the scores awarded forthe parser?s analyses of each sentence in the tree-bank (termed macro-averaging, in contrast to micro-averaging which is also common).
For sentenceswhere the parser selects several candidate analysesas tied best analyses, the actual metric score used isthe average value of the metric applied to the differ-ent tied best analyses.
Fourteen metrics are consid-ered:?
Exact Tree Match (ETM) (Toutanova et al,2005) - 100% if the returned tree is identicalto the gold tree, and 0% otherwise.?
Exact MRS Match (EMM) - 100% if the re-turned MRS is equivalent to the gold MRS, and0% otherwise.?
Average Crossing Brackets (AXB) - the num-ber of brackets (constituents) in the returnedtree that overlap incompatibly with somebracket in the gold tree.
Sign-inverted for com-parability to the other metrics.?
Zero Crossing Brackets (ZXB) - 100% if theAXB score is 0, and 0% otherwise.?
Labeled PARSEVAL (LP) (Abney et al, 1991)- the harmonic mean (F1) of the precision andrecall for comparing the set of labeled brack-ets in the returned tree with the set of labeledbrackets in the gold tree.
Labels are rule names.?
Unlabeled PARSEVAL (UP) - identical to LP,except ignoring the labels on the brackets.?
Labeled Syntactic Dependencies (LSD) (Buch-holz and Marsi, 2006) - the F1 for comparingthe sets of directed bilexical syntactic depen-dencies extracted from the returned and goldtrees, labeled by the rule name that joins thedependent to the dependee.?
Unlabeled Syntactic Dependencies (USD) -identical to LSD, except ignoring the labels.?
Labeled Elementary Dependencies (LED) - theF1 for comparing the sets of elementary depen-dency triples (Oepen and L?nning, 2006) ex-tracted from the returned and gold MRS. Theseannotations are similar in spirit to those used inthe PARC 700 Dependency Bank (King et al,2003) and other semantic dependency evalua-tion schemes.?
Unlabeled Elementary Dependencies (UED) -identical to LED, except ignoring all labelinginformation other than the input positions in-volved.?
Leaf Ancestor (LA) (Sampson and Babarczy,2003) - the average of the edit distances be-tween the paths through the returned and goldtrees from root to each leaf.?
Lexeme Name Match (LNM) - the percentageof input words parsed with the gold lexeme2.?
Part-of-Speech Match (POS) - the percentageof input words parsed with the gold part ofspeech.?
Node Count Match (NCM) - 100% if the goldand returned trees have exactly the same num-ber of nodes, and 0% otherwise.2In the ERG, lexemes are detailed descriptions of the syn-tactic and semantic properties of individual words.
There canbe multiple candidate lexemes for each word with the same partof speech.30242628303234363840420.001  0.01  0.1  1  10  100  1000ExactMatch Accuracy (%)Regularization Variance ParameterRegularized Performance of pcfg baselinepcfg baselineFigure 1: ETM for ?pcfg baseline?Note that the last three metrics are not commonlyused in parser evaluation, and we have no reasonto expect them to be particularly informative.
Theywere included for variety ?
in a sense serving as con-trols, to see how informative a very unsophisticatedmetric can be.4 Optimizing the RegularizationParameterThe first half of our problem is: given a set of fea-ture templates T , determine the optimal value for theregularization parameter ?.
We interpret the word?optimal?
relative to each of our 14 metrics.
This isquite straightforward: to optimize relative to metric?, we simply evaluate ?
(M(T, ?))
for each value of?, where M(T, ?)
is a parser trained using featuretemplates T and regularization parameter ?, and de-clare the value of ?
yielding the greatest value of?
the winner.
Figure 1 shows values of the ETMas a function of the regularization parameter ?
forT = ?pcfg baseline?3; as can easily be seen, the op-timal value is approximately ???
= 2.We are interested in how ???
varies with differentchoices of ?.
Figure 2 shows all 14 metrics as func-tions of ?
for the same T = ?pcfg baseline.?
Theactual scores from the metrics vary broadly, so thevertical axes of the superimposed plots have beenrescaled to allow for easier comparison.A priori we might expect the optimal ???
to be3Note that we are not actually considering a PCFG here; in-stead we are looking at a conditional log-linear model whosefeatures are shaped like PCFG configurations.-1.5-1-0.500.510.001  0.01  0.1  1  10  100  1000Z-ScoresRegularizationZ-Score Comparison of MetricsFigure 2: Z-scores for all metrics for ?pcfg baseline?quite different for different ?, but this does not turnout to be the case.
The curves for all of the met-rics peak in roughly the same place, with one no-ticeable outlier (AXB).
The actual peak4 regulariza-tion parameters for the 14 metrics were all in therange [1.8, 3.9] except for the outlier AXB, whichwas 14.8.Relative to the range under consideration, the op-timal regularization parameters can be seen by in-spection to depend very little on the metric.
Near theoptima, the graphs are all quite flat, and we calcu-lated that by choosing the optimal regularization pa-rameter according to any of the metrics (with the ex-ception of the outlier AXB), the maximum increasein error rate visible through the other metrics was1.6%.
If we ignore LNM, POS and NCM (the non-standard metrics we included for variety) in additionto AXB, the maximum increase in error rate result-ing from using an alternate metric to optimize theregularization parameter drops to 0.41%.
?pcfg baseline?
is just one of 57 sets of featuretemplates.
However, the situation is essentially thesame with each of the remaining 56.
The averagemaximum error rate increase observed across all ofthe sets of feature templates when optimizing on anymetric (including AXB, LNM, POS and NCM) was2.54%; on the worst single set of feature templates itwas 6.7%.
Excluding AXB, the average maximumerror rate increase was 1.7%.
Additionally exclud-4Due to noisiness near the tops of the graphs, the reportedoptimum regularization parameters are actually the averages ofthe best 3 values.
We attribute the noise to the limited size ofour corpus.31ing LNM, POS and NCM it was 0.81%.Given the size of the evaluation corpus we areusing, the significance of an error rate increase of0.81% is very marginal.
We conclude that, at leastin circumstances similar to ours, the choice of met-ric used to optimize regularization parameters is notimportant, provided we avoid AXB and the varietymetrics LNM, POS and NCM.5 Choosing a Set of Feature TemplatesThe second half of our problem is: given a col-lection T of different sets of feature templates, se-lect the optimal performer.
Again, we interpretthe word ?optimal?
relative to each of our 14 met-rics, and the selection is straightforward: givena metric ?, we first form a set of parsers P ={M(T, argmax?
?
(M(T, ?)))
: T ?
T } and thenselect argmaxp?P ?(p).
That is, we train parsersusing the ?-optimal regularization parameter foreach T ?
T , and then select the ?-optimal parserfrom that set.In our experiments, all 14 of the metrics rankedthe same set of feature templates as best.It is also interesting to inspect the order that eachmetric imposes on P .
There was some disagree-ment between the metrics about this order.
We com-puted pairwise Spearman rank correlations coeffi-cients5 for the different metrics.
As with the taskof choosing a regularization parameter, the metricsAXB, LNM, POS and NCM were outliers.
The av-erage pairwise Spearman rank correlation exclud-ing these metrics was 0.859 and the minimum was0.761.An alternate method of quantifying the degree ofagreement is described below.5.1 EpsilaConsider two metrics ?
: P 7?
R and ?
: P 7?
R.Assume for simplicity that for both ?
and ?, largervalues are better and 100 is perfect.
If x, y ?
Pthen the error rate reduction from y to x under ?is ??
(x, y) = ?(x)??(y)100??
(y) .
Let ?,?
be the smallestnumber such that ?x, y ?
P : ??
(x, y) > ?,?
?5The Spearman rank correlation coefficient of two metricsis defined as the Pearson correlation coefficient of the ranks themetrics assign to the elements of P .
It takes values between?1and 1, with larger values indicating higher ranking agreement.??
(x, y) > 0.
Informally, this says for all pairs ofparsers x and y, if x is at least ?,?
better than y whenevaluated under ?, then we are guaranteed that x isat least a tiny bit better than y when evaluated under?.
For an unrestricted domain of parsers, we are notguaranteed that such epsila exist or are small enoughto be interesting.
However, since our P is finite, wecan find an  that will provide the required propertyat least within P .?,?
serves as a measure of how similar ?
and ?are: if ?,?
is small, then small improvements seenunder ?
will be visible as improvements under ?,whereas if ?,?
is large, then small improvementsseen under ?
may in fact be regressions when evalu-ating with ?.We computed pairwise epsila for our 14 metrics.A large portion of pairwise epsila were around 5%,with some being considerably smaller or larger.5.2 ClusteringIn order to make sense of the idea that these ep-sila provide a similarity measure, we applied QualityThreshold clustering (Heyer et al, 1999) to discovermaximal clusters of metrics within which all pair-wise epsila are smaller than a given threshold.
Smallthresholds produce many small clusters, while largerthresholds produce fewer, larger clusters.At a 1% threshold, almost all of the metrics formsingleton clusters; that is, a 1% error rate reductionon any given metric is generally not enough to guar-antee that any other metrics will see any error reduc-tion at all.
The exceptions were that {ETM, EMM}formed a cluster, and {UED, LED} formed a cluster.Increasing the threshold to 3%, a new cluster{USD, LSD} forms (indicating that a 3% error ratereduction in USD always is visible as some levelof error rate reduction in LSD, and vice versa), andZXB joins the {ETM, EMM} cluster.By the time we reach a 5% threshold, the major-ity (7 out of 11) of the ?standard?
parser evaluationmetrics have merged into a single cluster, consistingof {ETM, EMM, ZXB, LA, LSD, UED, LED}.
ThePARSEVALmetrics form a cluster of their own {UP,LP}.Increasing the threshold even more to 10% causes10 out of 11 ?standard?
evaluation metrics to clustertogether; the only holdout is AXB (average numberof crossing brackets), which does not join the cluster32-0.200.20.40.60.811.21.41.61.822  4  6  8  10  12  14Z-ScoresMetricZ-Score Comparison of Feature SetsFigure 3: Z-scores for all feature sets on the Y axis (oneline per feature set); different metrics on the X axis.
The?control?
metrics and the outlier AXB are on the far rightend.even at a 20% threshold.5.3 VisualizationTo qualitatively illustrate the degree of variation inscores attributable to differences in metric as op-posed to differences in feature sets, and the extent ofthe metrics?
agreements in ranking the feature sets,we plotted linearly rescaled scores from the metrics(at their optimum regularization parameter value) intwo ways.In Figure 3, the scores of each feature set are plot-ted as a function of which metric is being used.
Tothe extent that the lines are horizontal, the metricsprovide identical information.
To the extent that thelines do not cross, the metrics agree about the rela-tive ordering of the feature sets.
Note that the threecontrol metrics and the outlier metric AXB are plot-ted on the far right of the figure, and show signifi-cantly more line crossings.In Figure 4, the score from each metric is plot-ted as a function of which feature set is being evalu-ated, sorted in increasing order of the LP metric.
Ascan be seen, the increasing trend of the LP metricis clearly mirrored in all the other metrics graphed,although there is a degree of variability.6 ConclusionsFrom both subtasks, we saw that the Average Cross-ing Brackets metric (AXB) is a serious outlier.
Wecannot say whether it provides complementary in--0.200.20.40.60.811.21.41.61.820  10  20  30  40  50  60Z-ScoresFeature SetZ-Score Comparison of MetricsFigure 4: Z-scores for all metrics except AXB, LNM,POS and NCM on the Y axis (one line per metric); dif-ferent feature sets on the X axis.formation or actually misleading information; in-deed, that might depend on the nature of the down-stream application.We can say with confidence that for the subtask ofoptimizing a regularization parameter, there is verylittle difference between the popular metrics {ETM,EMM, ZXB, LA, LP, UP, LSD, USD, LED, UED}.For the subtask of choosing the optimal set of fea-ture templates, there was even greater agreement: all14 metrics arrived at the same result.
Although theydid not impose the exact same rankings, the rankingswere similar.
It is interesting (and entertaining) thateven the three ?control?
metrics (LNM, POS andNCM) selected the same optimal feature set.
It isparticularly surprising that even the absurdly simpleNCM metric, which does nothing but check whethertwo trees have the same number of nodes, irrespec-tive of their structure or labels, when averaged overthousands of items, can identify the best feature set.Our findings agree with (Crouch et al, 2002)?ssuggestion that different metrics can offer similarviews on error rate reduction.Clustering based on epsila at the 5% and 10%thresholds showed interesting insights as well.
Wedemonstrated that a 5% error rate reduction as seenon any of {ETM, EMM, ZXB, LA, LSD, UED,LED} is also visible from the others (although thepopular PARSEVAL metrics were outliers at thisthreshold).
This has the encouraging implicationthat a decision made on the basis of strong evidencefrom just one metric is not likely to be contradicted33by evaluations by other metrics.
However, we mustpoint out that the precise values of these thresholdsare dependent on our setup.
They would likely belarger if a significantly larger number of parsers or asignificantly more varied group of parsers were con-sidered, and conversely would perhaps be smaller ifa larger evaluation corpus were used (reducing thenoise).Our data only directly apply to the tasks of se-lecting the value of the regularization parameter andselecting feature templates for a conditional log-likelihood model for parsing with the ERG.
How-ever, we expect the results to generalize at least tosimilar tasks with other precision grammars, andprobably treebank-derived parsers as well.
Explo-ration of how well these results hold for other tasksand for other types of parsers is an excellent subjectfor future research.ReferencesS.
Abney, D. Flickinger, C. Gdaniec, C. Grishman,P.
Harrison, D. Hindle, R. Ingria, F. Jelinek, J. Kla-vans, M. Liberman, et al 1991.
Procedure for quan-titatively comparing the syntactic coverage of Englishgrammars.
In Proceedings of the workshop on Speechand Natural Language, pages 306?311.
Associationfor Computational Linguistics.Sabine Buchholz and Erwin Marsi.
2006.
Conll-x sharedtask on multilingual dependency parsing.
In Proceed-ings of the Tenth Conference on Computational Nat-ural Language Learning (CoNLL-X), pages 149?164,New York City, June.
Association for ComputationalLinguistics.J.
Carroll, T. Briscoe, and A. Sanfilippo.
1998.
Parserevaluation: a survey and a new proposal.
In Proceed-ings of the 1st International Conference on LanguageResources and Evaluation, pages 447?454.S.
Clark and J. Curran.
2007.
Formalism-independentparser evaluation with CCG and DepBank.
In An-nual Meeting-Association for Computational Linguis-tics, volume 45, page 248.A.
Copestake, D. Flickinger, C. Pollard, and I.A.
Sag.2005.
Minimal recursion semantics: An introduction.Research on Language & Computation, 3(4):281?332.R.
Crouch, R.M.
Kaplan, T.H.
King, and S. Riezler.2002.
A comparison of evaluation metrics for a broad-coverage stochastic parser.
In Beyond PARSEVALworkshop at 3rd Int.
Conference on Language Re-sources an Evaluation (LREC 2002).Martin Emms.
2008.
Tree distance and some othervariants of evalb.
In Bente Maegaard Joseph Mari-ani Jan Odjik Stelios Piperidis Daniel Tapias NicolettaCalzolari (Conference Chair), Khalid Choukri, edi-tor, Proceedings of the Sixth International Conferenceon Language Resources and Evaluation (LREC?08),Marrakech, Morocco, may.
European LanguageResources Association (ELRA).
http://www.lrec-conf.org/proceedings/lrec2008/.Dan Flickinger.
2000.
On building a more efficientgrammar by exploiting types.
Natural Language En-gineering, 6(01):15?28.L.J.
Heyer, S. Kruglyak, and S. Yooseph.
1999.
Explor-ing expression data: identification and analysis of co-expressed genes.
Genome research, 9(11):1106.T.H.
King, R. Crouch, S. Riezler, M. Dalrymple, andR.
Kaplan.
2003.
The PARC 700 dependencybank.
In Proceedings of the EACL03: 4th Interna-tional Workshop on Linguistically Interpreted Corpora(LINC-03), pages 1?8.S.
Oepen and J.T.
L?nning.
2006.
Discriminant-basedMRS banking.
In Proceedings of the 5th InternationalConference on Language Resources and Evaluation(LREC 2006).G.
Sampson and A. Babarczy.
2003.
A test of the leaf-ancestor metric for parse accuracy.
Natural LanguageEngineering, 9(04):365?380.K.
Toutanova, C.D.
Manning, D. Flickinger, andS.
Oepen.
2005.
Stochastic HPSG parse disambigua-tion using the Redwoods corpus.
Research on Lan-guage & Computation, 3(1):83?105.Gisle Ytrest?l, Dan Flickinger, and Stephan Oepen.2009.
Extracting and Annotating Wikipedia Sub-Domains.
Towards a New eScience Community Re-source.
In Proceedings of the Seventh Interna-tional Workshop on Treebanks and Linguistic Theo-ries, Groningen, The Netherlands.34
