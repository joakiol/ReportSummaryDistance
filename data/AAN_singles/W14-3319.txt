Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 171?177,Baltimore, Maryland USA, June 26?27, 2014.c?2014 Association for Computational LinguisticsAbu-MaTran at WMT 2014 Translation Task:Two-step Data Selection and RBMT-Style Synthetic RulesRaphael Rubino?, Antonio Toral?, Victor M.
S?anchez-Cartagena?
?,Jorge Ferr?andez-Tordera?, Sergio Ortiz-Rojas?, Gema Ram?
?rez-S?anchez?,Felipe S?anchez-Mart?
?nez?, Andy Way?
?Prompsit Language Engineering, S.L., Elche, Spain{rrubino,vmsanchez,jferrandez,sortiz,gramirez}@prompsit.com?NCLT, School of Computing, Dublin City University, Ireland{atoral,away}@computing.dcu.ie?Dep.
Llenguatges i Sistemes Inform`atics, Universitat d?Alacant, Spainfsanchez@dlsi.ua.esAbstractThis paper presents the machine trans-lation systems submitted by the Abu-MaTran project to the WMT 2014 trans-lation task.
The language pair concernedis English?French with a focus on Frenchas the target language.
The French to En-glish translation direction is also consid-ered, based on the word alignment com-puted in the other direction.
Large lan-guage and translation models are built us-ing all the datasets provided by the sharedtask organisers, as well as the monolin-gual data from LDC.
To build the trans-lation models, we apply a two-step dataselection method based on bilingual cross-entropy difference and vocabulary satura-tion, considering each parallel corpus in-dividually.
Synthetic translation rules areextracted from the development sets andused to train another translation model.We then interpolate the translation mod-els, minimising the perplexity on the de-velopment sets, to obtain our final SMTsystem.
Our submission for the English toFrench translation task was ranked secondamongst nine teams and a total of twentysubmissions.1 IntroductionThis paper presents the systems submitted by theAbu-MaTran project (runs named DCU-Prompsit-UA) to the WMT 2014 translation task for theEnglish?French language pair.
Phrase-based sta-tistical machine translation (SMT) systems weresubmitted, considering the two translation direc-tions, with the focus on the English to French di-rection.
Language models (LMs) and translationmodels (TMs) are trained using all the data pro-vided by the shared task organisers, as well asthe Gigaword monolingual corpora distributed byLDC.To train the LMs, monolingual corpora and thetarget side of the parallel corpora are first usedindividually to train models.
Then the individ-ual models are interpolated according to perplex-ity minimisation on the development sets.To train the TMs, first a baseline is built us-ing the News Commentary parallel corpus.
Sec-ond, each remaining parallel corpus is processedindividually using bilingual cross-entropy differ-ence (Axelrod et al., 2011) in order to sepa-rate pseudo in-domain and out-of-domain sen-tence pairs, and filtering the pseudo out-of-domain instances with the vocabulary saturationapproach (Lewis and Eetemadi, 2013).
Third,synthetic translation rules are automatically ex-tracted from the development set and used to trainanother translation model following a novel ap-proach (S?anchez-Cartagena et al., 2014).
Finally,we interpolate the four translation models (base-line, in-domain, filtered out-of-domain and rules)by minimising the perplexity obtained on the de-velopment sets and investigate the best tuning anddecoding parameters.The reminder of this paper is organised as fol-lows: the datasets and tools used in our experi-ments are described in Section 2.
Then, detailsabout the LMs and TMs are given in Section 3 andSection 4 respectively.
Finally, we evaluate theperformance of the final SMT system according todifferent tuning and decoding parameters in Sec-tion 5 before presenting conclusions in Section 6.1712 Datasets and ToolsWe use all the monolingual and parallel datasetsin English and French provided by the shared taskorganisers, as well as the LDC Gigaword for thesame languages1.
For each language, a true-casemodel is trained using all the data, using the train-truecaser.perl script included in the MOSES tool-kit (Koehn et al., 2007).Punctuation marks of all the monolingual andparallel corpora are then normalised using thescript normalize-punctuation.perl provided by theorganisers, before being tokenised and true-casedusing the scripts distributed with the MOSES tool-kit.
The same pre-processing steps are applied tothe development and test sets.
As developmentsets, we used all the test sets from previous yearsof WMT, from 2008 to 2013 (newstest2008-2013).Finally, the training parallel corpora are cleanedusing the script clean-corpus-n.perl, keeping thesentences longer than 1 word, shorter than 80words, and with a length ratio between sentencepairs lower than 4.2The statistics about the cor-pora used in our experiments after pre-processingare presented in Table 1.For training LMs we use KENLM (Heafield etal., 2013) and the SRILM tool-kit (Stolcke et al.,2011).
For training TMs, we use MOSES (Koehnet al., 2007) version 2.1 with MGIZA++ (Och andNey, 2003; Gao and Vogel, 2008).
These tools areused with default parameters for our experimentsexcept when explicitly said.The decoder used to generate translations isMOSES using features weights optimised withMERT (Och, 2003).
As our approach relies ontraining individual TMs, one for each parallel cor-pus, our final TM is obtained by linearly interpo-lating the individual ones.
The interpolation ofTMs is performed using the script tmcombine.py,minimising the cross-entropy between the TMand the concatenated development sets from 2008to 2012 (noted newstest2008-2012), as describedin Sennrich (2012).
Finally, we make use of thefindings from WMT 2013 brought by the win-ning team (Durrani et al., 2013) and decide to usethe Operation Sequence Model (OSM), based onminimal translation units and Markov chains oversequences of operations, implemented in MOSES1LDC2011T07 English Gigaword Fifth Edition,LDC2011T10 French Gigaword Third Edition2This ratio was empirically chosen based on words fertil-ity between English and French.Corpus Sentences (k) Words (M)Monolingual Data ?
EnglishEuroparl v7 2,218.2 59.9News Commentary v8 304.2 7.4News Shuffled 2007 3,782.5 90.2News Shuffled 2008 12,954.5 308.1News Shuffled 2009 14,680.0 347.0News Shuffled 2010 6,797.2 157.8News Shuffled 2011 15,437.7 358.1News Shuffled 2012 14,869.7 345.5News Shuffled 2013 21,688.4 495.2LDC afp 7,184.9 869.5LDC apw 8,829.4 1,426.7LDC cna 618.4 45.7LDC ltw 986.9 321.1LDC nyt 5,327.7 1,723.9LDC wpb 108.8 20.8LDC xin 5,121.9 423.7Monolingual Data ?
FrenchEuroparl v7 2,190.6 63.5News Commentary v8 227.0 6.5News Shuffled 2007 119.0 2.7News Shuffled 2008 4,718.8 110.3News Shuffled 2009 4,366.7 105.3News Shuffled 2010 1,846.5 44.8News Shuffled 2011 6,030.1 146.1News Shuffled 2012 4,114.4 100.8News Shuffled 2013 9,256.3 220.2LDC afp 6,793.5 784.5LDC apw 2,525.1 271.3Parallel Data109Corpus21,327.1549.0 (EN)642.5 (FR)Common Crawl 3,168.576.0 (EN)82.7 (FR)Europarl v7 1,965.552.5 (EN)56.7 (FR)News Commentary v9 181.34.5 (EN)5.3 (FR)UN 12,354.7313.4 (EN)356.5 (FR)Table 1: Data statistics after pre-processing of themonolingual and parallel corpora used in our ex-periments.and introduced by Durrani et al.
(2011).3 Language ModelsThe LMs are trained in the same way for bothlanguages.
First, each monolingual and parallelcorpus is considered individually (except the par-allel version of Europarl and News Commentary)and used to train a 5-gram LM with the modifiedKneser-Ney smoothing method.
We then interpo-late the individual LMs using the script compute-best-mix available with the SRILM tool-kit (Stol-cke et al., 2011), based on their perplexity scoreson the concatenation of the development sets from2008 to 2012 (the 2013 version is held-out for thetuning of the TMs).172The final LM for French contains all the wordsequences from 1 to 5-grams contained in thetraining corpora without any pruning.
However,with the computing resources at our disposal, theEnglish LMs could not be interpolated withoutpruning non-frequent n-grams.
Thus, n-gramswith n ?
[3; 5] with a frequency lower than 2 wereremoved.
Details about the final LMs are given inTable 2.1-gram 2-gram 3-gram 4-gram 5-gramEnglish 13.4 198.6 381.2 776.3 1,068.7French 6.0 75.5 353.2 850.8 1,354.0Table 2: Statistics, in millions of n-grams, of theinterpolated LMs.4 Translation ModelsIn this Section, we describe the TMs trained forthe shared task.
First, we present the two-step dataselection process which aims to (i) separate in andout-of-domain parallel sentences and (ii) reducethe total amount of out-of-domain data.
Second,a novel approach for the automatic extraction oftranslation rules and their use to enrich the phrasetable is detailed.4.1 Parallel Data Filtering and VocabularySaturationAmongst the parallel corpora provided by theshared task organisers, only News Commentarycan be considered as in-domain regarding the de-velopment and test sets.
We use this trainingcorpus to build our baseline SMT system.
Theother parallel corpora are individually filtered us-ing bilingual cross-entropy difference (Moore andLewis, 2010; Axelrod et al., 2011).
This datafiltering method relies on four LMs, two in thesource and two in the target language, whichaim to model particular features of in and out-of-domain sentences.We build the in-domain LMs using the sourceand target sides of the News Commentary paral-lel corpus.
Out-of-domain LMs are trained on avocabulary-constrained subset of each remainingparallel corpus individually using the SRILM tool-kit, which leads to eight models (four in the sourcelanguage and four in the target language).33The subsets contain the same number of sentences andthe same vocabulary as News Commentary.Then, for each out-of-domain parallel corpus,we compute the bilingual cross-entropy differenceof each sentence pair as:[Hin(Ssrc)?Hout(Ssrc)] + [Hin(Strg)?Hout(Strg)] (1)where Ssrcand Strgare the source and the tar-get sides of a sentence pair, Hinand Houtarethe cross-entropies of the in and out-of-domainLMs given a sentence pair.
The sentence pairs arethen ranked and the lowest-scoring ones are takento train the pseudo in-domain TMs.
However,the cross-entropy difference threshold required tosplit a corpus in two parts (pseudo in and out-of-domain) is usually set empirically by testing sev-eral subset sizes of the top-ranked sentence pairs.This method is costly in our setup as it would leadto training and evaluating multiple SMT systemsfor each of the pseudo in-domain parallel corpora.In order to save time and computing power,we consider only pseudo in-domain sentence pairsthose with a bilingual cross-entropy difference be-low 0, i.e.
those deemed more similar to thein-domain LMs than to the out-of-domain LMs(Hin< Hout).
A sample of the distribution ofscores for the out-of-domain corpora is shown inFigure 1.
The resulting pseudo in-domain corporaare used to train individual TMs, as detailed in Ta-ble 3.-4-202468100 2k 4k 6k 8k 10kBilingualCross-Entropy DifferenceSentence PairsCommon CrawlEuroparl10^9UNFigure 1: Sample of ranked sentence-pairs (10k)of each of the out-of-domain parallel corpora withbilingual cross-entropy differenceThe results obtained using the pseudo in-domain data show BLEU (Papineni et al., 2002)scores superior or equal to the baseline score.Only the Europarl subset is slightly lower thanthe baseline, while the subset taken from the 109corpus reaches the highest BLEU compared to theother systems (30.29).
This is mainly due to the173size of this subset which is ten times larger thanthe one taken from Europarl.
The last row of Ta-ble 3 shows the BLEU score obtained after interpo-lating the four pseudo in-domain translation mod-els.
This system outperforms the best pseudo in-domain one by 0.5 absolute points.Corpus Sentences (k) BLEUdevBaseline 181.3 27.76Common Crawl 208.3 27.73Europarl 142.0 27.63109Corpus 1,442.4 30.29UN 642.4 28.91Interpolation - 30.78Table 3: Number of sentence pairs and BLEUscores reported by MERT on English?French new-stest2013 for the pseudo in-domain corpora ob-tained by filtering the out-of-domain corpora withbilingual cross-entropy difference.
The interpola-tion of pseudo in-domain models is evaluated inthe last row.After evaluating the pseudo in-domain paralleldata, the remaining sentence pairs for each cor-pora are considered out-of-domain according toour filtering approach.
However, they may stillcontain useful information, thus we make use ofthese corpora by building individual TMs for eachcorpus (in a similar way we built the pseudo in-domain models).
The total amount of remainingdata (more than 33 million sentence pairs) makesthe training process costly in terms of time andcomputing power.
In order to reduce these costs,sentence pairs with a bilingual cross-entropy dif-ference higher than 10 were filtered out, as we no-ticed that most of the sentences above this thresh-old contain noise (non-alphanumeric characters,foreign languages, etc.
).We also limit the size of the remaining data byapplying the vocabulary saturation method (Lewisand Eetemadi, 2013).
For the out-of-domain sub-set of each corpus, we traverse the sentence pairsin the order they are ranked by perplexity differ-ence and filter out those sentence pairs for whichwe have seen already each 1-gram at least 10times.
Each out-of-domain subset from each par-allel corpus is then used to train a TM before inter-polating them to create the pseudo out-of-domainTM.
The results reported by MERT obtained onthe newstest2013 development set are detailed inTable 4.Mainly due to the sizes of the pseudo out-of-Corpus Sentences (k) BLEUdevBaseline 181.3 27.76Common Crawl 1,598.7 29.84Europarl 461.9 28.87109Corpus 5,153.0 30.50UN 1,707.3 29.03Interpolation - 31.37Table 4: Number of sentence pairs and BLEUscores reported by MERT on English?Frenchnewstest2013 for the pseudo out-of-domain cor-pora obtained by filtering the out-of-domain cor-pora with bilingual cross-entropy difference, keep-ing sentence pairs below an entropy score of 10and applying vocabulary saturation.
The interpo-lation of pseudo out-of-domain models is evalu-ated in the last row.domain subsets, the reported BLEU scores arehigher than the baseline for the four individualSMT systems and the interpolated one.
This lattersystem outperforms the baseline by 3.61 absolutepoints.
Compared to the results obtained with thepseudo in-domain data, we observe a slight im-provement of the BLEU scores using the pseudoout-of-domain data.
However, despite the com-paratively larger sizes of the latter datasets, theBLEU scores reached are not that higher.
For in-stance with the 109corpus, the pseudo in and out-of-domain subsets contain 1.4 and 5.1 million sen-tence pairs respectively, and the two systems reach30.3 and 30.5 BLEU.
These scores indicate thatthe pseudo in-domain SMT systems are more ef-ficient on the English?French newstest2013 devel-opment set.4.2 Extraction of Translation RulesA synthetic phrase-table based on shallow-transferMT rules and dictionaries is built as follows.
First,a set of shallow-transfer rules is inferred from theconcatenation of the newstest2008-2012 develop-ment corpora exactly in the same way as in theUA-Prompsit submission to this translation sharedtask (S?anchez-Cartagena et al., 2014).
In sum-mary, rules are obtained from a set of bilingualphrases extracted from the parallel corpus afterits morphological analysis and part-of-speech dis-ambiguation with the tools in the Apertium rule-based MT platform (Forcada et al., 2011).The extraction algorithm commonly used inphrase-based SMT is followed with some addedheuristics which ensure that the bilingual phrases174extracted are compatible with the bilingual dic-tionary.
Then, many different rules are generatedfrom each bilingual phrase; each of them encodesa different degree of generalisation over the partic-ular example it has been extracted from.
Finally,the minimum set of rules which correctly repro-duces all the bilingual phrases is found based oninteger linear programming search (Garfinkel andNemhauser, 1972).Once the rules have been inferred, the phrasetable is built from them and the original rule-based MT dictionaries, following the methodby S?anchez-Cartagena et al.
(2011), which wasone of winning systems4(together with two on-line SMT systems) in the pairwise manual evalu-ation of the WMT11 English?Spanish translationtask (Callison-Burch et al., 2011).
This phrase-table is then interpolated with the baseline TM andthe results are presented in Table 5.
A slight im-provement over the baseline is observed, whichmotivates the use of synthetic rules in our final MTsystem.
This small improvement may be relatedto the small coverage of the Apertium dictionar-ies: the English?French bilingual dictionary has alow number of entries compared to more maturelanguage pairs in Apertium which have around 20times more bilingual entries.System BLEUdevBaseline 27.76Baseline+Rules 28.06Table 5: BLEU scores reported by MERT onEnglish?French newstest2013 for the baselineSMT system standalone and with automaticallyextracted translation rules.5 Tuning and DecodingWe present in this Section a short selection of ourexperiments, amongst 15+ different configura-tions, conducted on the interpolation of TMs, tun-ing and decoding parameters.
We first interpolatethe four TMs: the baseline, the pseudo in and out-of-domain, and the translation rules, minimisingthe perplexity obtained on the concatenated de-velopment sets from 2008 to 2012 (newstest2008-2012).
We investigate the use of OSM trained onpseudo in-domain data only or using all the paral-lel data available.
Finally, we make variations of4No other system was found statistically significantly bet-ter using the sign test at p ?
0.1.the number of n-bests used by MERT.Results obtained on the development set new-stest2013 are reported in Table 6.
These scoresshow that adding OSM to the interpolated trans-lation models slightly degrades BLEU.
However,by increasing the number of n-bests considered byMERT to 200-bests, the SMT system with OSMoutperforms the systems evaluated previously inour experiments.
Adding the synthetic translationrules degrades BLEU (as indicated by the last rowin the Table), thus we decide to submit two sys-tems to the shared task: one without and one withsynthetic rules.
By submitting a system withoutsynthetic rules, we also ensure that our SMT sys-tem is constrained according to the shared taskguidelines.System BLEUdevBaseline 27.76+ pseudo in + pseudo out 31.93+ OSM 31.90+ MERT 200-best 32.21+ Rules 32.10Table 6: BLEU scores reported by MERT onEnglish?French newstest2013 development set.As MERT is not suitable when a large numberof features are used (our system uses 19 fetures),we switch to the Margin Infused Relaxed Algo-rithm (MIRA) for our submitted systems (Watan-abe et al., 2007).
The development set used isnewstest2012, as we aim to select the best decod-ing parameters according to the scores obtainedwhen decoding the newstest2013 corpus, after de-truecasing and de-tokenising using the scripts dis-tributed with MOSES.
This setup allowed us tocompare our results with the participants of thetranslation shared task last year.
We pick the de-coding parameters leading to the best results interms of BLEU and decode the official test set ofWMT14 newstest2014.
The results are reported inTable 7.
Results on newstest2013 show that the de-coding parameters investigation leads to an over-all improvement of 0.1 BLEU absolute.
The re-sults on newstest2014 show that adding syntheticrules did not help improving BLEU and degradedslightly TER (Snover et al., 2006) scores.In addition to our English?French submission,we submitted a French?English translation.
OurFrench?English MT system is built on the align-ments obtained from the English?French direc-tion.
The training processes between the two sys-175System BLEU13A TERnewstest2013Best tuning 31.02 60.77cube-pruning (pop-limit 10000) 31.04 60.71increased table-limit (100) 31.06 60.77monotonic reordering 31.07 60.69Best decoding 31.14 60.66newstest2014Best decoding 34.90 54.70Best decoding + Rules 34.90 54.80Table 7: Case sensitive results obtained withour final English?French SMT system on new-stest2013 when experimenting with different de-coding parameters.
The best parameters are keptto translate the WMT14 test set (newstest2014)and official results are reported in the last tworows.tems are identical, except for the synthetic ruleswhich are not extracted for the French?Englishdirection.
Tuning and decoding parameters forthis latter translation direction are the best onesobtained in our previous experiments on thisshared task.
The case-sensitive scores obtainedfor French?English on newstest2014 are 35.0BLEU13A and 53.1 TER, which ranks us at thefifth position for this translation direction.6 ConclusionWe have presented the MT systems developed bythe Abu-MaTran project for the WMT14 trans-lation shared task.
We focused on the French?English language pair and particularly on theEnglish?French direction.
We have used a two-step data selection process based on bilingualcross-entropy difference and vocabulary satura-tion, as well as a novel approach for the extractionof synthetic translation rules and their use to en-rich the phrase table.
For the LMs and the TMs,we rely on training individual models per corpusbefore interpolating them by minimising perplex-ity according to the development set.
Finally, wemade use of the findings of WMT13 by includingan OSM model.Our English?French translation system wasranked second amongst nine teams and a total oftwenty submissions, while our French?Englishsubmission was ranked fifth.
As future work,we plan to investigate the effect of adding to thephrase table synthetic translation rules based onlarger dictionaries.
We also would like to study thelink between OSM and the different decoding pa-rameters implemented in MOSES, as we observedinconsistent results in our experiments.AcknowledgmentsThe research leading to these results has re-ceived funding from the European Union SeventhFramework Programme FP7/2007-2013 undergrant agreement PIAP-GA-2012-324414 (Abu-MaTran).ReferencesAmittai Axelrod, Xiaodong He, and Jianfeng Gao.2011.
Domain Adaptation Via Pseudo In-domainData Selection.
In Proceedings of EMNLP, pages355?362.Chris Callison-Burch, Philipp Koehn, Christof Monz,and Omar Zaidan.
2011.
Findings of the 2011 work-shop on statistical machine translation.
In Proceed-ings of WMT, pages 22?64.Nadir Durrani, Helmut Schmid, and Alexander Fraser.2011.
A Joint Sequence Translation Model with In-tegrated Reordering.
In Proceedings of ACL/HLT,pages 1045?1054.Nadir Durrani, Barry Haddow, Kenneth Heafield, andPhilipp Koehn.
2013.
Edinburgh?s Machine Trans-lation Systems for European Language Pairs.
InProceedings of WMT, pages 112?119.Mikel L Forcada, Mireia Ginest?
?-Rosell, Jacob Nord-falk, Jim O?Regan, Sergio Ortiz-Rojas, Juan An-tonio P?erez-Ortiz, Felipe S?anchez-Mart?
?nez, GemaRam?
?rez-S?anchez, and Francis M Tyers.
2011.Apertium: A Free/Open-source Platform for Rule-based Machine Translation.
Machine Translation,25(2):127?144.Qin Gao and Stephan Vogel.
2008.
Parallel Implemen-tations of Word Alignment Tool.
In Software Engi-neering, Testing, and Quality Assurance for NaturalLanguage Processing, pages 49?57.Robert S Garfinkel and George L Nemhauser.
1972.Integer Programming, volume 4.
Wiley New York.Kenneth Heafield, Ivan Pouzyrevsky, Jonathan H.Clark, and Philipp Koehn.
2013.
Scalable Mod-ified Kneser-Ney Language Model Estimation.
InProceedings of ACL, pages 690?696.Philipp Koehn, Hieu Hoang, Alexandra Birch, ChrisCallison-Burch, Marcello Federico, Nicola Bertoldi,Brooke Cowan, Wade Shen, Christine Moran,Richard Zens, et al.
2007.
Moses: Open SourceToolkit for Statistical Machine Translation.
In Pro-ceedings of ACL, Interactive Poster and Demonstra-tion Sessions, pages 177?180.176William D. Lewis and Sauleh Eetemadi.
2013.
Dra-matically Reducing Training Data Size Through Vo-cabulary Saturation.
In Proceedings of WMT, pages281?291.Robert C. Moore and William Lewis.
2010.
IntelligentSelection of Language Model Training Data.
In Pro-ceedings of ACL, pages 220?224.Franz Josef Och and Hermann Ney.
2003.
A System-atic Comparison of Various Statistical AlignmentModels.
Computational Linguistics, 29:19?51.Franz Josef Och.
2003.
Minimum Error Rate Trainingin Statistical Machine Translation.
In Proceedingsof ACL, volume 1, pages 160?167.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
BLEU: A Method for AutomaticEvaluation of Machine Translation.
In Proceedingsof ACL, pages 311?318.V?
?ctor M. S?anchez-Cartagena, Felipe S?anchez-Mart?
?nez, and Juan Antonio P?erez-Ortiz.
2011.
In-tegrating Shallow-transfer Rules into Phrase-basedStatistical Machine Translation.
In Proceedings ofMT Summit XIII, pages 562?569.V?
?ctor M. S?anchez-Cartagena, Juan Antonio P?erez-Ortiz, and Felipe S?anchez-Mart??nez.
2014.
TheUA-Prompsit Hybrid Machine Translation Systemfor the 2014 Workshop on Statistical MachineTranslation.
In Proceedings of WMT.Rico Sennrich.
2012.
Perplexity Minimization forTranslation Model Domain Adaptation in Statisti-cal Machine Translation.
In Proceedings of EACL,pages 539?549.Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-nea Micciulla, and John Makhoul.
2006.
A Studyof Translation Edit Rate with Targeted Human An-notation.
In Proceedings of AMTA, pages 223?231.Andreas Stolcke, Jing Zheng, Wen Wang, and VictorAbrash.
2011.
SRILM at Sixteen: Update and Out-look.
In Proceedings of ASRU.Taro Watanabe, Jun Suzuki, Hajime Tsukada, andHideki Isozaki.
2007.
Online Large-margin Train-ing for Statistical Machine Translation.
In Proceed-ings of EMNLP.177
