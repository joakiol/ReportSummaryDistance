Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 409?416Manchester, August 2008Textual Demand Analysis:Detection of Users?
Wants and Needs from OpinionsHiroshi Kanayama Tetsuya NasukawaTokyo Research Laboratory, IBM Japan, Ltd.1623-14 Shimotsuruma, Yamato-shi, Kanagawa-ken, 242-8502 Japan{hkana,nasukawa}@jp.ibm.comAbstractThis paper tackles textual demand analy-sis, the task of capturing what people wantor need, rather than identifying what theylike or dislike, on which much conven-tional work has focused.
It exploits syn-tactic patterns as clues to detect previouslyunknown demands, and requires domain-dependent knowledge to get high recall.
Tobuild such patterns we created an unsuper-vised pattern induction method relying onthe hypothesis that there are commonly de-sired aspects throughout a domain corpus.Experimental results show that the pro-posed method detects twice to four timesas many demand expressions in Japanesediscussion forums compared to a baselinemethod.1 IntroductionIncreasingly we can access many opinions towardsproducts, services, or companies through elec-tronic documents including questionnaires, calllogs, and other consumer-generated media (CGM)such as Internet discussion forums and blogs.
It isvery important for companies to get insights fromtheir customers?
opinions by analyzing such docu-ments in large numbers.The most popular way to utilize such data hasinvolved sentiment analysis (SA) (Nasukawa andYi, 2003; Yi et al, 2003), which is the task of rec-ognizing the writers?
feelings as expressed in pos-itive or negative comments.
Typically, a SA sys-tem focuses on expressions to identify the strongc?
2008.
Licensed under the Creative CommonsAttribution-Noncommercial-Share Alike 3.0 Unported li-cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).Some rights reserved.or weak points of the subjects as in (1) or in thewriters?
evaluations as in (2).
(1) I think the pictures are beautiful.
(2) I don?t like this camera very much.Here we call them polar expressions becausethey convey positive or negative polarities.
Bycounting the polar expressions related to productsor services, one can quantitatively compare thegoodness of competing services, find the draw-backs of specific products, and so on.In addition to polar expressions, there are othertypes of expressions that provide valuable infor-mation, especially for the supplier side rather thanthe consumer side.
Examples (3) and (4) expressthe demands of the writers.
(3) I?d be happy if it is equipped witha crisp LCD.
(4) I?m waiting for a single-lens reflex lessthan 30,000 yen to come on the market.We call such expressions ?demand expres-sions?, and the underlined phrases ?demand tar-gets.
?While sentiment analysis reveals evaluations ofexisting products or services, the task proposedhere, textual demand analysis1, gives more directsuggestions to companies: things they should doto attract customers.
For example, by investigatingdemand targets, companies can add new functionsto products on the market or plan new services tosatisfy customers.
These activities should lead topositive evaluations in the future.Interestingly, demand expressions may be noisein sentiment analysis, because the demand ex-pressions do not actually convey positive or neg-1Note that textual demand analysis is different from thedemand analysis in the field of marketing or software engi-neering.409ConsumersCompanyOpinions?TextualDemandAnalysis?........................Demands OutlinerSyntacticPatternsPatternExtractionFrequentDemandInstances-ff??
?Pattern InductionFigure 1: A demand analysis system and the flowof the pattern induction method.ative evaluations of existing products or services,even though these demand expressions often con-tain positive or negative words, as in Example (3)which contains the positive expressions ?happy?and ?crisp LCD?.The detection of novel demand targets requiresdeep syntactic information because such demandtargets themselves can not be predefined.
For ex-ample, to regard the underlined parts of (3) and(4) as demand targets, the non-underlined parts ofthese sentences have to be properly interpreted astriggers.
This is a major difference from sentimentanalysis where the polar expressions can be de-fined in the lexicon.The left parts of Figure 1 illustrate the conceptsof a system that visualizes the users?
demands de-scribed in the input opinion data, where the mainanalysis component processes the documents andextracts the demand targets.
The output of the sys-tem is created by a demand outliner, which thecompany uses to grasp the trends of consumers?demands.The syntactic patterns that can be used as cluesto demand expressions depend on the topic domainor the writing style.
To organize this linguisticknowledge we propose an unsupervised inductionmethod for syntactic patterns.
The right part ofFigure 1 shows the flow of pattern induction.In the next section, we review related work, andSection 3 defines our task more formally.
In Sec-tion 4 we describe a naive approach to the task andSection 5 shows a form of unsupervised pattern in-duction used to cover more demand expressions.Section 6 gives the experimental results and weconclude in Section 7.2 Related WorkSentiment analysis (SA) and related topics havebeen extensively studied in recent years.
The tex-tual demand analysis proposed in this paper sharessome properties with phrase-level SA, the detec-tion of sentiments and evaluations expressed inphrases, rather than document-level SA, the clas-sification of documents in terms of goodness ofreputation.
Yi et al (2003) pointed out that themultiple sentiment aspects in a document shouldbe extracted, and Nasukawa and Yi (2003) clar-ified the need for deep syntactic analysis for thephrase-level SA.The acquisition of clues is a key technology inthese research efforts, as seen in learning meth-ods for document-level SA (Hatzivassiloglou andMcKeown, 1997; Turney, 2002) and for phrase-level SA (Wilson et al, 2005; Kanayama and Na-sukawa, 2006).As well as the sentiment expressions leading toevaluations, there are many semantic aspects to beextracted from documents which contain writers?opinions, such as subjectivity (Wiebe and Mihal-cea, 2006), comparative sentences (Jindal and Liu,2006), or predictive expressions (Kim and Hovy,2007).
However, the extraction of the contents ofwriters?
demands which this paper handles is lessstudied while this type of information is very valu-able for commercial applications.For the tasks of information extraction and re-lation extraction, bootstrapping approaches havebeen proven successful (Yangarber, 2003; Panteland Pennacchiotti, 2006).
The pattern inductionmethod in this paper exploits their ideas, but theirapplication to the demand detection is not trivial,because some instances of demands are previouslyunknown and do not appear frequently, so theyhave to be abstracted effectively.The work by Inui et al (2003) handles seman-tics of a type similar to ours.
They aimed to detectthe requests in the responses to open-ended ques-tionnaires, seeking direct requests such as ?...?
(?
[I] would like you to ...?)
and other formswhich can be paraphrased as direct requests.
Theyclassified sentences into requests or non-requests,where their source documents were responses toquestionnaires, and where more than 60% of theutterances could be regarded as requests of somesort.
In contrast, our method detects the contentof the demands in the form of noun phrases, andhandles more general target documents including410CGM documents that contain much more diverseutterances.3 Task DefinitionAs shown in Section 1, our goal is to create a sys-tem to enumerate in an easily understandable waythe demand targets in the input text.
This sectiondescribes the definition of a demand target and itsrepresentation format.3.1 Demand targetsDemands or requests written in opinion texts canbe represented by verb phrases as in e.g.
?I wantto V.?
and ?I want you to V.?, or noun phrasesas in ?I want N.?2In this paper we focus on thelast type, i.e.
noun phrases which represent desiredobjects, because they are easier to aggregate andgrasp than verb phrases.
Another reason is thatsome demands represented with a verb phrase onlydescribe the objects that are desired.
For example,?I want to buy N?
and ?I want you to provide N?
canbe simply interpreted as meaning that N is what thewriter wants.
We call such a noun phrase a demandtarget, and these are the outputs of our system.For applications, the demand targets to be de-tected by the system depend on the type of in-put documents.
For example, from a consumers?forum on digital cameras, the underlined parts inExamples (3) and (4) from Section 1 apparentlydescribe the writer?s demands, so they are valu-able information for such users of demand anal-ysis such as the makers of digital camera.
How-ever, the request in Example (5) does not expressthe author?s demands about any digital camera, butrather it is written for other participants in the fo-rum.
This type should be excluded from the out-put.
(5) Please give me a good advice.In contrast, when the responses to a question-naire about the activities of an organization areprocessed, statements such as Example (5) shouldbe regarded as a demand target, since the writerwrote it as a request to the sponsor of the question-naire and the ?advice?
is indeed a thing that can beprovided by the sponsoring organization.3.2 Representation of demand targetsA demand target tends to be expressed by a nounphrase with modifiers, as seen in Examples (3) and2?V?
and ?N?
indicate a verb phrase and a noun phrase,respectively.7?happy??if?!?equip?-NOM2J?LCD??db?crisp??standpoint?F?I??want?H?sell?#-ACC-)?reflex??T!
?can buy??with?30,0003?-yen?15?single-lens?Figure 2: Syntactic trees for the sentences (6) and(7).
?*?
indicates the headword of the demand tar-gets.
(4), rather than by a single noun.
The headwordsof such phrases (e.g.
?LCD?
in (3) and ?reflex?
in(4)) represent the main categories of the demandedobjects, but they are not distinctive enough to rec-ognize as knowledge of the authors?
demands.Therefore the key task of this research was tofind ways to markup the headword of a nounphrase that represents the content of a demand inthe syntactic parse tree.
Examples (6) and (7) arethe original Japanese sentences corresponding toExamples (3) and (4).
(6) Fdb2J!7?I?d be happy if it is equipped with a crisp LCD.?
(7) Z3T!15-)#H?
[I?m] waiting for a single-lens reflexless than 30,000 yen to come on the market.
?Figure 2 represents the parse trees correspond-ing to sentences (6) and (7), where the demand tar-gets are identified by the mark ?
*?.This simple representation is advantageous forboth the collection of and the deeper investigationof the demand targets.
One can easily grasp thecontent of a demand if the application shows thewhole surface structure of the subtree under ?*?
inthe tree, e.g.
the underlined parts of Examples (6)and (7).
At the same time the tree structure sup-ports the further analysis of the trends of the de-mands by picking up the headwords or modifiersprominent in the subtrees that were detected as de-mand targets.4 Baseline Method of Textual DemandAnalysisThis section describes an algorithm to extract de-mand targets with high precision and describesa preliminary experiment to measure the perfor-mance.411N?-ACC]?want?(a)V?that?E?think?
(b)V?though?V(c)Figure 3: (a) is a demand pattern which indicatesthat the noun in N?is detected as a demand target.
(b) and (c) are auxiliary patterns, where V indicatesthe node matches any verb.4.1 Syntactic patterns and top-downmatchingA major purpose of textual demand analysis isto discover novel demands embedded in the text,thus the triggers of their detection should not bea predefined set of demand targets but should betheir surrounding syntactic information.
We usetwo types of syntactic patterns shown in Figure 3.Those patterns are compared with the syntactictree as the parsing result of the input sentence.The pattern (a) in Figure 3 is a demand pattern,which is used to search for demand targets.
Thenode with the ???
mark indicates the correspond-ing node will be the headword of a demand tar-get.
Hence we write the pattern (a) as ?N?--]?
for simplicity.
The patterns are applied ina top-down manner, that is, initially the top nodeof the input tree is examined to see whether ornot the node and its combination of children nodesmatch with one of the patterns in the pattern repos-itory.
This method supports higher precision in thedetection than the surface pattern matching.
Forexample, the expression ?WaV]K?
(?There is no one who wants low qualitygoods?)
should not be misunderstood to express ademand.The patterns (b) and (c) in Figure 3 are auxil-iary patterns.
These are used to apply the demandpatterns to nodes other than the root of the syn-tactic tree.
For example, by applying the patterns(b) and (c), the pattern (a) can then be applied tothe expressions ?N]E?
(?Ithink that I want N?)
and ?N];O ?
(?Though I want N, I don?t haveenough money?
), respectively, even though ?N?--]?
doesn?t appear at the top of the trees.In other words, the auxiliary patterns contribute togenerate variations of the demand patterns.In addition, simple rules can be applied to fil-ter out certain meaningless outputs.
When a nounphrase that matched to the ???
part of the demandTable 1: The result on the small gold standard withDP1.
PM signifies surface pattern matching, TMsignifies tree matching.
?+AP?
means that auxil-iary patterns are used.Method Precision RecallPM 39% (14/36) 25% (14/56)TM 92% (11/12) 20% (11/56)TM+AP 94% (17/18) 30% (17/56)pattern was a pronoun or very common noun (e.g.?camera?
in the camera domain) without any mod-ifier, it is not output as a demand target.4.2 Preliminary experimentWe conducted a preliminary experiment to assessthe feasibility of our approach.We prepared a small gold-standard datasetwhich consists of 1,152 sentences from a discus-sion forum on digital cameras, for which two hu-man annotators attached marks to the demand tar-gets.
There were 56 demand targets that at leastone of the annotators detected, and the sentence-level agreement value3was ?
= 0.73, which isregarded as a good level of agreement.
There wasno sentence in which the two annotators attachedmarks to different nouns.First, we made a minimum set of demand pat-terns DP1, which contained only one basic pattern?N?--]4?
(?I want N??
).To see the effect of the top-down matching andthe auxiliary patterns described in Section 4.1,demand targets in the gold-standard corpus wereautomatically detected using three methods: pat-tern matching with surface strings like ?]?
(PM), tree matching without the auxiliary pat-terns (TM), and tree matching with the auxiliarypatterns5(TM+AP).Table 1 shows the results.
The top-down match-ing on the syntactic tree resulted in much higherprecision than the surface pattern matching, andthe auxiliary patterns improved the recall.
Theonly misdetection in the tree matching method wasdue to an error in the sentence segmentation.However, all of them show low recall values,3The agreement on whether or not the sentence has a de-mand target.4Apparent character variations like ?]?
and ??,and alternative forms of particles were aggregated in the pars-ing process.5A total of 95 auxiliary patterns which Kanayama et al(2004) used for the sentiment analysis.412Table 2: The list of augmented demand patternsDPq.N?--] (I want N?
), N?-#-Y (I hope N?
),N?-#-	6-!
(Please [give] N?
), N?-#-6(I wish N?
), N?-#--4 (Please do N?
),N?-#-^ (I ask [you] N?
), N?--!-(N?should be), N?-#--P (Please do N?
)Table 3: The result with the minimum set of de-mand patterns DP1and the larger set DPq.Patterns Precision RecallDP194% (17/18) 30% (17/56)DPq78% (18/22) 32% (18/56)since only one demand pattern was used.
To makethe recall higher, we created another set of demandpatterns DPqlisted in Table 2, which are gener-ally used as clues for the request expressions inthe analysis of responses to open-ended question-naires.
They are derived from the previous workon request classification (Yamamoto et al, 2007).The result in Table 3 shows that the patternsnewly added in DPqdo not perform well.
This isbecause these patterns appear in responses to ques-tionnaires but are not suitable for the writing stylesused in discussion forums, as mentioned in Sec-tion 3.1.Therefore we used the TM+AP method with theminimum pattern set DP1as the baseline in thispaper rather than the pattern set DPq.5 Automatic Pattern InductionThe preliminary experiment in Section 4.2 showedthat high precision can be obtained by the top-down matching method, and at the same time, re-vealed the difficulty in building demand patterns toachieve high recall.
To overcome this problem, wedevised an automatic pattern induction algorithm.5.1 Frequent fragments of demand targetsHere we make an assumption that there are com-monly desired aspects or things throughout a do-main corpus.
Based on this assumption, we ex-tract the syntactic fragments which appear rela-tively frequently as the elements of demand targetsfrom the training corpus in a specific domain.First we obtain demand targets from the domaincorpus, in this case from the discussion forum ondigital cameras, by using the baseline method with-)?reflex??T!
?can buy??with?30,0003?-yen?15?single-lens?-) -)T!
15-)15-)T!-)T!NT!
15NT!NT!N15Figure 4: Sample extraction of demand instancesfrom a demand target detected by the system.
?N?denotes the wildcard for any nouns.the pattern set DP1.
Next, demand instances areextracted from each demand target.
A demand in-stance is a one-to-three-node subtree of a demandtarget, and shares the root node with the demandtarget.
The root node that is modified by one ormore nodes can be replaced with a wildcard.
Fig-ure 4 shows a sample extraction of demand in-stances from a demand target ?Z3T!15-)?
(?single-lens reflex less than 30,000 yen?
),where nine demand instances are extracted, andfour of them have a wildcard at the root node.The demand instances which appear more than?ftimes in the corpus are selected as frequent de-mand instances (FDIs), and each FDI is assignedthe following reliability value ri:ri=freqDT(i)freq(i)(8)where ?freq(i)?
denotes the frequency of the in-stance subtree in the whole corpus and ?freqDT(i)?means the i?s frequency in the demand targets.
Thenotion of an instance?s reliability is inspired bythe method of relation extraction (Pantel and Pen-nacchiotti, 2006), but our usage is different fromtheirs.
Here we use the reliability value only forthe relative comparison among demand instances,so normalization of the values is not needed.
Theintrinsic difference from the instance of relationextraction is that the demand instances are not thefinal outputs of the extraction, but are just triggersfor new demand patterns.When ?fwas set to 5, a total of 42 FDIswhich had reliabilities above 0.01 were pickedfrom 150,000 postings in the discussion forum ondigital cameras.
Table 4 shows examples of FDIs.5.2 Frequent patterns as the clueThe FDIs with high reliability correspond to as-pects which are likely to be demanded, thereforethe syntactic structures which often govern such413Table 4: Examples of frequent demand instances(FDIs) with ri> 0.01 in the digital camera do-main.
?-?
denotes the split of nodes.
"!-(&$+ (?digital camera - which can?
)--/' (?mm - lens?
),><9 (?newer model?
),a- (?good - thing?
),[!--/' (?sharp lens?
),db-N (?beautiful - N?
),*%.-N (?macro - N?
), ?
?
?
]?want?H?sell?#-ACC$+,?camera?!
?can?I?small?-ACCLG ?close-up?N?early?Figure 5: An example of extraction of a candidatedemand pattern.
From the sentence ?ILG!$+, #NH]?
(?I want a small camera with the close-up functionsold earlier?
), the CDP ?N?-#-H-]?
(the oval) is extracted, triggered by the FDI ?!-$+,?
(the square).FDIs are expected to be clues for detecting addi-tional demands.Following this expectation, the candidate de-mand patterns (hence CDPs) are extracted.
A CDPis a subtree that connects the head of an FDI andthe root of the syntactic tree, or auxiliary patternswhich cover the root of the tree.
A CDP forms anode sequence without a branch, and correspondsto a sentence-final expression in Japanese that usu-ally conveys modality information.
Figure 5 illus-trates the extraction of a CDP from a syntactic treetriggered by an FDI.For each CDP extracted in this way, the reliabil-ity is determined by Equation (9):rp=?i?FDIfreq(i, p) ?
rifreq(p)(9)where freq(i, p) denotes the frequency of the col-location of the instance i and the pattern p, andfreq(p) is the frequency of p in the entire corpus.These ratios are summed up over all of the FDIs,weighted by the reliability of the instance.
Alsorpis used only for the relative comparison amongCDPs, so it is not normalized to be in the range[0,1].Table 5 shows the CDPs which appeared 10times or more and their reliability values.
SomeTable 5: The extracted candidate demand patterns(CDPs) sorted by their reliability.Candidate Demand Pattern ReliabilityN?--"-a 1.70 ?10?2(be good if it includes N?
)N?-#-T-4 (please buy N?)
1.48 ?10?2N?--!-a 1.12 ?10?2(be good if it includes N?
)N?--!-X_ 4.81 ?10?3(be convenient if it includes N?
)N?-#-T	-E-!
3.32 ?10?3([I?m] going to buy N?
)N?--"-X_ 3.00 ?10?3(be convenient if it includes N?
)N?-#-H-] 1.88 ?10?3([I] want N?to be sold)N?--8Y- (N?
is longed for) 1.34 ?10?3N?-#-M!
([I] recommend N?)
1.06 ?10?3N?--AS-#-=R-!
8.92 ?10?4([I?m] thinking about buying N?)N?--WO!
(N?
is lacking) 3.53 ?10?4......N?-#-D (to use N?)
5.28 ?10?5N?-#-AS!
(to purchase N?)
4.31 ?10?5N?--C!
(to take [pictures] with N?)
6.06 ?10?6......of them apparently reflect the writing style of thediscussion forum and the digital camera domain.The effect of these patterns will be verified in Sec-tion 6.6 EvaluationWe conducted experiments to assess the contribu-tions of the candidate demand patterns acquired inSection 5.6.1 Experimental setupIn Section 4.2 we created a gold-standard datasetwith human annotations, however, the number ofannotation is not enough to fairly compare the sev-eral methods due to the sparseness of the demandtargets in the original corpus, and it would be verylaborious to prepare a larger annotated dataset.Therefore we used an unannotated corpus forthe evaluation in this section.
A total of 50,000postings in the digital camera forum6were pro-cessed by each method, and 100 demand targetswere randomly selected from the system output foreach trial and a human evaluator decided for eachdemand target whether or not it referred to any de-manded object related to the domain.6They are separate from the 150,000 postings used for thetraining.414Table 6: The evaluations when CDPs with reliabil-ity more than ?
were used.?
Precision Recall F1?
93% 30% 0.4510?291% 31% 0.4610?387% 37% 0.5210?468% 59% 0.6310?533% 57% 0.42In this way, the precision can be computed di-rectly, and the recall can be estimated as follows:Rec(T ) Num(T )Prec(T )Rec(B)Num(B)Prec(B)(10)where Rec(), Prec(), and Num() denote the recall,the precision, and the number of demand targetsdetected by the system in the entire test corpus, re-spectively, and T and B denote the tested methodand the baseline method, respectively.
Prec(B) isassumed to be 30% as observed in the preliminaryexperiment.6.2 Effect of new demand patternsThe CDFs for the digital camera domain that wereacquired with the method in Section 5 are testedby varying the threshold ?.
The CDFs which havereliability value more than ?
were added to the de-mand pattern set.
Table 6 shows the results.
Thebaseline method was without any newly acquireddemand patterns (i.e.
?
= ?
), thus it is the samecondition as the DP1in the preliminary experimentin Section 4.2.When ?
was set to 10?3, the recall increaseddrastically with little harm to the precision.
Thevalue of ?
= 10?5did not work well because theprecision was very low and the increase of the re-call was limited.
The value ?
= 10?4performedbest in terms of the F1 value.We observed the demand targets derived fromthe new demand patterns.
In most cases desir-able functions and features of the digital cameraswere successfully obtained from conditional pos-itive expressions such as ?N?--!-X_?
(?be convenient if it includes N??).
Also, preferredmachines were clarified by the expression ?N?-#-T-4?
(?please buy N??)
which is in apostings to recommend something to other users.On the other hand, the expression ?N?--WO!?
(?N?
is lacking?)
tend to result in noisy demandtargets.Table 7: The extracted CDPs and their reliabilityfor the domain of company?s questionnaire.Candidate Demand Pattern ReliabilityN?-#-Y ([I] hope N?)
3.10 ?10?2N?-#-6- 1.37 ?10?2([I] want to ask for N?
)N?-#-6 ([I] wish N?)
4.92 ?10?3N?--Y-"!
(N?
is hoped for) 1.45 ?10?3N?-#-Q:-] 1.04 ?10?3([I] want N?to be provided)N?--U\--@!
6.72 ?10?4([I] think N?is necessary)......N?--WO-!
(N?
is lacking) 1.02 ?10?4N?--0 (N?
is bad) 7.22 ?10?5......We also tried the iterative acquisition using thenewly acquired patterns, but the useful patternswere rarely acquired in the second step.
This isbecause FDIs cannot be definitive triggers, and thefirst seed pattern ?N?--]?
(?I want N??
)was a prominently reliable pattern compared withthe other demand patterns.6.3 Applicability to other demand targetsThe pattern induction method was expected tohave advantage that domain-dependent patternscan be acquired, and indeed some of the patternswere specific for the digital camera domain asshown in Table 5.
To see the applicability of ouralgorithm to other domains or other types of text,the pattern induction was tested on another corpus.The responses to a questionnaire about collabo-ration process in a company were used as the cor-pus.
Unlike the discussion forum on digital cam-eras, the writing style of direct request such as?Please provide N??
was observed frequently, andthe demand targets are much more dense in thecorpus.
Table 7 shows the CDPs acquired in thisdomain, and Table 8 shows the evaluation where25,000 and 5,000 sentences were used for the train-ing and the test, respectively.As a result, higher precision was achieved in thisdomain than in the digital camera domain, becausethe demands are stated more explicitly in the re-sponses to the questionnaires.
Unlike in the digitalcamera domain, the pattern ?N?--WO-!?
(?N?
is lacking?)
worked well because in manycases of this domain what are lacking equal to whatare needed.
For example, ??`acBWO!?
(?effective discussion is lack-415Table 8: The evaluations for the company ques-tionnaire domain.
The initial recall was estimatedas 15%.
DC10?4means that the CDPs for the dig-ital camera domain (?
= 104) were used.?
Precision Recall F1?
98% 15% 0.2610?296% 24% 0.3910?392% 30% 0.4510?485% 71% 0.7710?541% 73% 0.53DC10?472% 40% 0.51ing?)
implies that the effective discussion is a de-mand.When the demand patterns acquired in the dig-ital camera domain (DC10?4) were used, the in-crease of the recall was limited.
These results sup-port the value of the unsupervised pattern induc-tion method which works for any domain whenonly a raw domain corpus is provided.7 ConclusionWe formalized the task textual demand analysisand proposed a pattern induction method to in-crease the coverage of the automatic detection ofdemand targets.
The pattern induction proposedhere allows for the discovery of novel demandsthat can be represented by various forms of nounphrases, though they were triggered by frequentlyappeared syntactic fragments.
Beyond sentimentanalysis, textual demand analysis provides valu-able knowledge for industries, clarifying not onlythe favorable aspects in the current products, butalso the essential features in the future.ReferencesHatzivassiloglou, Vasileios and Kathleen R. McKeown.1997.
Predicting the semantic orientation of adjec-tives.
In Proceedings of the 35th ACL and the 8thEACL, pages 174?181.Inui, Hiroko, Masao Utiyama, and Hitoshi Isahara.2003.
Criterion for judging request intention in re-sponse texts of open-ended questionnaires.
In Pro-ceedings of the second international workshop onParaphrasing, pages 49?56.Jindal, Nitin and Bing Liu.
2006.
Mining compara-tive sentences and relations.
In Proceedings of the21st National Conference on Artificial Intelligence(AAAI2006).Kanayama, Hiroshi and Tetsuya Nasukawa.
2006.Fully automatic lexicon extraction for domain-oriented sentiment analysis.
In Proceedings of the2006 Conference on Empirical Methods in NaturalLanguage Processing (EMNLP), pages 355?363.Kanayama, Hiroshi, Tetsuya Nasukawa, and HideoWatanabe.
2004.
Deeper sentiment analysis us-ing machine translation technology.
In Proceedingsof 20th International Conference on ComputationalLinguistics (COLING), pages 494?500.Kim, Soo-Min and Eduard Hovy.
2007.
Crystal: An-alyzing predictive opinions on the Web.
In Pro-ceedings of the 2007 Joint Conference on EmpiricalMethods in Natural Language Processing and Com-putational Natural Language Learning (EMNLP-CoNLL), pages 1056?1064.Nasukawa, Tetsuya and Jeonghee Yi.
2003.
Senti-ment analysis: Capturing favorability using naturallanguage processing.
In Proceedings of the SecondInternational Conferences on Knowledge Capture,pages 70?77.Pantel, Patrick and Marco Pennacchiotti.
2006.Espresso: leveraging generic patterns for automat-ically harvesting semantic relations.
In ACL ?06:Proceedings of the 21st International Conferenceon Computational Linguistics and the 44th annualmeeting of the ACL, pages 113?120.Turney, Peter D. 2002.
Thumbs up or thumbs down?Semantic orientation applied to unsupervised classi-fication of reviews.
In Proc.
of the 40th ACL Conf.,pages 417?424.Wiebe, Janyce and Rada Mihalcea.
2006.
Wordsense and subjectivity.
In ACL ?06: Proceedings ofthe 21st International Conference on ComputationalLinguistics and the 44th annual meeting of the ACL,pages 1065?1072.Wilson, Theresa, Janyce Wiebe, and Paul Hoffmann.2005.
Recognizing contextual polarity in phrase-level sentiment analysis.
In Proceedings of HLTConference and Conference on EMNLP, pages 347?354, October.Yamamoto, Mizuki, Takashi Inui, Hiroya Takamura,Satoko Marumoto, Hiroko Otsuka, and ManabuOkumura.
2007.
Extracting demands and their rea-sons in answers to open-ended questionnaires.
InThe 13th Annual Meeting of The Association for Nat-ural Language Processing.
(in Japanese).Yangarber, Roman.
2003.
Counter-training in the dis-covery of semantic patterns.
In Proceedings of the41st annual meeting of the ACL, pages 343?350.Yi, Jeonghee, Tetsuya Nasukawa, Razvan Bunescu, andWayne Niblack.
2003.
Sentiment analyzer: Extract-ing sentiments about a given topic using natural lan-guage processing techniques.
In Proceedings of theThird IEEE International Conference on Data Min-ing, pages 427?434.416
