THE ROLE OF SYNTAX IN INFORMATION EXTRACTIONRalph GrishmanComputer  Science Depar tmentNew York Un ivers i ty715 Broadway,  7th F loorNew York, NY  10003gr i  shman?cs,  nyu .
eduINTRODUCTIONOur group at New York University has developed anumber of information extraction systems over thepast decade.
In particular, we have been participantsin the Message Understanding Conferences (MUCs)since MUC-1.
During this time, while experimentingwith many aspects of system design, we have retaineda basic approach in which information extraction in-volves a phase of full syntactic analysis, followed bya semantic analysis of the syntactic structure \[2\].
Be-cause we have a good, broad-coverage English gram-mar and a moderately effective method for recoveringfrom parse failures, this approach held us in fairlygood stead.However, we have recently found ourselves at a dis-advantage with respect to groups which performedmore local pattern matching, in three regards:1. our  sys tems were  qu i te  s lowIn processing the language as a whole, our systemis operating with only relatively weak semanticpreferences.
As a result, the process of buildinga global syntactic analysis involves a large andrelatively unconstrained search space and is con-sequently quite expensive.
In contrast, patternmatching systems assemble structure "bottom-up" and only in the face of compelling syntacticor semantic evidence, in a (nearly) deterministicmanner.Speed was particularly an issue for MUC-6 be-cause of the relatively short time frame (1 monthfor training).
With a slow system, which cananalyze only a few sentences per minute, it ispossible to perform only one or at best two runsper day over the full training corpus, severelylimiting debugging.2.
g loba l  pars ing  cons iderat ions  somet imesled  to  loca l  e r rorsOur system was designed to generate a full sen-tence parse if at all possible.
If not, it attempteda parse covering the largest substring of the sen-tence which it could.
This global goal sometimesled to incorrect local choices of analyses; an ana-lyzer which trusted local decisions could in manycases have done better.. add ing  syntact i c  const ructs  needed for anew scenar io  was hardHaving a broad-coverage,linguistically-principled grammar meant hat rel-atively few additions were needed when movingto a new scenario.
However, when specializedconstructs did have to be added, the task was rel-atively difficult, since these constructs had to beintegrated into a large and quite complex gram-mar.We considered carefully whether these difficultiesmight be readily overcome using an approach whichwas still based on a comprehensive syntactic gram-mar.
It appeared plausible, although not certain,that problems (1) and (2) could be overcome withinsuch an approach, by adopting a strategy of conser -vat ive  parsing.
A conservative parser would performa reduction only if there was strong (usually, local)syntactic evidence or strong semantic support.
Inparticular, chunking parsers, which built up smallchunks using syntactic riteria and then assembledlarger structures only if they were semantically li-censed, might provide a suitable candidate.In any case, problem (3) still loomed.
Our HolyGrail, like that of many groups, is to eventually getthe computational linguist out of the loop in adaptingan information extraction system for a new scenario.This will be difficult, however, if the scenario requiresthe addition of some grammatical construct, albeitminor.
It would require us to organize the grammarin such a way that limited additions could be made139by non-specialists without having to understand theentire grammar - -  again, not a simple task.In order to better understand the proper ole of syn-tax analysis, we decided to participate in the mostrecent MUC, MUC-6 (held in the fall of 1995), usinga quite different approach, often referred to as "pat-tern matching", which has become increasingly pop-ular among information extraction groups.
In par-ticular, we carefully studied the FASTUS system ofHobbs et al \[1\], who have clearly and eloquently setforth the advantages of this approach.
This approachcan be viewed as a form of conservative parsing, al-though the high-level structures which are created arenot explicitly syntactic.THE SYSTEMThe goal of information extraction is to analyze a text(an article / a message) and to fill a template withinformation about a specified type of event.
In thecase of MUC-6, the task (the "scenario") was to iden-tify instances of executives being hired or fired fromcorporations3Most of the stages of processing are performed onesentence at a time.
First, each word in a sentenceis looked up in a large English dictionary, ComlexSyntax, which provides syntactic information abouteach word.
The system then performs everal stagesof pattern matching.
The first stages deal primar-ily with name recognition - -  people's names, orga-nization names, geographic names, and names of ex-ecutive positions ("Executive Vice President for Re-call and Precision").
The next stages deal with nounand verb groups.
Basically, a noun group consists ofa noun and its left modifiers: "the first five para-graphs", "the yellow brick road"; such groupingscan generally be identified from syntactic informationalone.
A verb group consists of a verb and its relatedauxiliaries: "sleeps", "is sleeping", "has been sleep-ing", etc.
All of these stages are basically scenario-independent (except for the recognition of executivepositions, which was added for this scenario).Next come the scenario-specific patterns.
These in-clude, in particular, patterns to recognize the scenarioevents, such as "Smith became president of GeneralMotors", "Smith retired as president of General Mo-tors", and "Smith succeeded Jones as president ofGeneral Motors".
When such a pattern is matched, a1 For a description of MUC-6, see the papers "Design of theMUC-6 Evaluation" and "Overview of the Results of the MUC-6 Evaluation" in this volume; for a more detailed escription ofthe NYU system, see our paper "The NYU System for MUC-6 or Where's the Syntax?"
in Proc.
o\] the Sixth MessageUnderstanding Conference, Morgan Kaufmann, 1996.corresponding event structure is generated, recordingthe type of event (for this scenario, hiring or firing)and the people and companies involved.The next stage of processing is reference resolution.At this stage, pronouns and definite noun phraseswhich refer back to previously mentioned people ororganizations are linked to these antecedents.When all the sentences of an article have been an-alyzed, a final stage of processing assembles the in-formation and generates a template in the format re-quired for MUC.The resulting system did quite well.
With a limiteddevelopment time (four weeks for this MUC) we wereable to develop a system which obtained a recall of47% and a precision of 70% (with a combined F mea-sure of 56.4) on the test corpus.
This was the best Fscore on the scenario template task, although severalother systems (mostly with similar architectures) gotscores that were not significantly lower.THE ROLE OF SYNTAXAlthough our system, and systems like it, are char-acterized as "pattern matching" systems, they reallyare doing a form of parsing: they analyze the sen-tence into a nested constituent structure.
They differfrom more conventional parsing systems (such as ourearlier system) in?
not seeking a full-sentence analysis: they onlybuild as much structure as is needed for the infor-mation extraction task, including selected clausesrelevant o the scenarioparsing conservatively and deterministically:they only build structures which have a highchance of being correct, either because of syn-tactic clues (for noun groups) or semantic lues(for clause structures); as a result, they are muchfaster than traditional parsers* using semantic patterns for the final stage(s) ofanalysisOverall, we profited from the use of the pattern-matching approach; our analyzer was considerablyfaster, and we avoided some of the parsing errorswhich result from trying to obtain complete sen-tence analyses with a syntactic grammar.
On theother hand, we also experienced first-hand some of theshortcomings of the semantic pattern approach.
Syn-tax analysis provides two main benefits: it providesgeneralizations of linguistic structure across differentsemantic relations (for example, that the structure ofa main clause is basically the same whether the verb is140"to succeed" or "to fire"), and it captures paraphras-tic relations between different syntactic structures (forexample, between "X succeeds Y", "Y was succeededby X", and "Y, who succeeded X").
These benefits arelost when we encode individual semantic structures.In particular, in our system, we had to separately en-code the active, passive, relative, reduced relative, etc.patterns for each semantic structure.
These issues arehardly new; they have been well known at least sincethe syntactic grammar vs. semantic grammar contro-versies of the 1970's.How, then, to gain the benefits of clause-level syn-tax within the context of a partial parsing system?The approach we have adopted has been to intro-duce clause level patterns which are expanded bymetarules.
2As a simple example of a clause-le4el pattern, con-sider(defclausepattern runs"np-sem(C-person) vg(C-run)np-sem(C-company):person-at=l.attributes,verb-at=2.attributes,company-at=3.attributes"when-run)This specifies a clause with a subject of class C-person, a verb of class C-run (which includes "run"and "head"), and an object of class C-company.
3This is expanded into patterns for the active clause("Fred runs IBM"), the passive clause ("IBM is runby Fred.
"), relative clauses ("Fred, who runs IBM,..." and "IBM, which is headed by Fred, ..."), re-duced relative clauses ("IBM, headed by Fred, ...")and conjoined verb phrases ("... and runs IBM", "andis run by Fred").
The expanded patterns also includepattern elements for sentence modifiers, so that theycan analyze sentences uch as "Fred, who last yearran IBM, ...".Using de fc lansepat tern  reduced the number ofpatterns required and, at the same time, slightly im-proved coverage because - -  when we had been ex-panding patterns by hand - -  we had not included allexpansions in all cases.The defc lausepat tern  procedure performs a rudi-mentary syntactic analysis of the input.
In our exam-2These have some kinship to the metaru les  of GPSG,  whichexpand a smal l  set of product ions into a larger set involvingthe different clause-level structures.3It also specifies that the attr ibutes of these three con-st i tuents  are to be bound to the variables person-at ,  verb -a t ,and company-at ,  and that the procedure when-run is to be in-voked when this pattern is matched.pie, it determines that np-sem(C-person) is the sub-ject, vg(C-run) is the verb, and np-sem(C-company)is the object.
This is a prerequisite for generating thevarious restructurings, uch as the passive.
We in-tend in the near future to expand de : fc lausepat ternto handle (parse) a richer set of patterns, includingboth sentence modifiers and a wider range of comple-ments.
In this way the power of clause-level syntax isprovided to the pattern writer, without requiring thepattern writer to keep these details explicitly in mind.The use of clause-level syntax to generate syntacticvariants of a semantic pattern is even more importantif we look ahead to the time when such patterns will beentered by users rather than computational linguists.We can expect a computational linguist to consider allsyntactic variants, although it may be a small burden;we cannot expect the same of a typical user.We expect that users would enter patterns by ex-ample, and would answer queries to create variantsof the initial pattern.
We have just begun to createsuch an interface, which allows a user to begin theprocess of pattern creation by entering an exampleand the correspoding event structure to be generated.The example is analyzed using the low-level patterns(such as the name and noun group patterns) and thentranslated into a clause-level pattern.
The user canthen manipulate the pattern, generalizing pattern el-ements and dropping some pattern elements.
UsingdefclausepatVern, the resulting pattern is then an-alyzed and its clause-level syntactic variants are gen-erated.
Though our initial tests are promising, agreatdeal of work will still be required on this interface toprovide the full flexibility needed for creating a widerange of patterns.Our work has indicated the ways in which we cancontinue to obtain the benefits of syntax analysisalong with the performance benefits of the patternmatching approach.
While we no longer have a mono-lithic grammar, we are still able to take advantage ofthe syntactic regularities of both noun phrases andclauses.
Noun group syntax remains explicit, as onephase of pattern matching.
Clause syntax is now uti-lized in the metarules for defining patterns and inthe rules which analyze xample sentences to producepatterns.AcknowledgementsThe development of our language analysis softwareand our participation in the MUCs has been sup-ported by the Advanced Research Projects Agencyunder a series of contracts.
This work is currentlysupported under Contract 94-FI57900-000 from the141Office of Research and Development and under Con-tract DABT63-93-C-0058 from the Department oftheArmy.References[1] D. Appelt, J. Hobbs, J.
Bear, D. Israel,M.
Kameyama, and M. Tyson.
SRI: Descriptionof the JV-FASTUS System used for MUC-5.
InProc.
Fifth Message Understanding Conf.
(MUC-5), Baltimore, MD, August 1993.
Morgan Kauf-mann.
[2] Ralph Grishman and John Sterling.
New YorkUniversity: Description of the PROTEUS Systemas used for MUC-5.
In Proc.
Fifth Message Un-derstanding Conf.
(MUC-5), Baltimore, MD, Au-gust 1993.
Morgan Kaufmann.142
