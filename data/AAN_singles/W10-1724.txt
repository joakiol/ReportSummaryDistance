Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 167?171,Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational LinguisticsLinear Inversion Transduction Grammar Alignmentsas a Second Translation PathMarkus SAERS and Joakim NIVREComputational Linguistics GroupDept.
of Linguistics and PhilologyUppsala UniversitySwedenfirst.last@lingfil.uu.seDekai WUHuman Language Technology CenterDept.
of Computer Science and EngineeringHKUSTHong Kongdekai@cs.ust.hkAbstractWe explore the possibility of usingStochastic Bracketing Linear InversionTransduction Grammars for a full-scaleGerman?English translation task, both ontheir own and in conjunction with align-ments induced with GIZA++.
The ratio-nale for transduction grammars, the detailsof the system and some results are pre-sented.1 IntroductionLately, there has been some interest in using In-version Transduction Grammars (ITGs) for align-ment purposes.
The main problem with ITGs is thetime complexity, O(Gn6) doesn?t scale well.
Bylimiting the grammar to a bracketing ITG (BITG),the grammar constant (G) can be eliminated, butO(n6) is still prohibitive for large data sets.There has been some work on approximate in-ference of ITGs.
Zhang et al (2008) present amethod for evaluating spans in the sentence pairto determine whether they should be excluded ornot.
The algorithm has a best case time com-plexity of O(n3).
Saers, Nivre & Wu (2009) in-troduce a beam pruning scheme, which reducestime complexity to O(bn3).
They also showthat severe pruning is possible without significantdeterioration in alignment quality (as measuredby downstream translation quality).
Haghighi etal.
(2009) use a simpler aligner as guidance forpruning, which reduces the time complexity bytwo orders of magnitude.
Their work also par-tially implements the phrasal ITGs for translation-driven segmentation introduced in Wu (1997), al-though they only allow for one-to-many align-ments, rather than many-to-many alignments.
Amore extreme approach is taken in Saers, Nivre& Wu (2010).
Not only is the search severelypruned, but the grammar itself is limited to a lin-earized form, getting rid of branching within a sin-gle parse.
Although a small deterioration in down-stream translation quality is noted (compared toharshly pruned SBITGs), the grammar can be in-duced in linear time.In this paper we apply SBLITGs to a full sizeGerman?English WMT?10 translation task.
Wealso use differentiated translation paths to com-bine SBLITG translation models with a standardGIZA++ translation model.2 BackgroundA transduction grammar is a grammar that gener-ates a pair of languages.
In a transduction gram-mar, the terminal symbols consist of pairs of to-kens where the first is taken from the vocabularyof one of the languages, and the second from thevocabulary of the other.
Transduction grammarshave to our knowledge been restricted to trans-duce between languages no more complex thancontext-free languages (CFLs).
Transduction be-tween CFLs was first described in Lewis & Stearns(1968), and then further explored in Aho & Ull-man (1972).
The main motivation for explor-ing this was to build programming language com-pilers, which essentially translate between sourcecode and machine code.
There are two types oftransduction grammars between CFLs described inthe computer science literature: simple transduc-tion grammars (STGs) and syntax-directed trans-duction grammars (SDTGs).
The difference be-tween them is that STGs are monotone, whereasSDTGs allow unlimited reordering in rule produc-tions.
Both allow the use of singletons to insertand delete tokens from either language.
A sin-gleton is a biterminal where one of the tokens isthe empty string ().
Neither STGs nor SDTGsare intuitively useful in translating natural lan-guages, since STGs have no way to model reorder-ing, and SDTGs require exponential time to be in-duced from examples (parallel corpora).
Since167compilers in general work on well defined, manu-ally specified programming languages, there is noneed to induce them from examples, so the expo-nential complexity is not a problem in this setting?
SDTGs can transduce in O(n3) time, so once thegrammar is known they can be used to translateefficiently.In natural language translation, the grammar isgenerally not known, in fact, state-of-the art trans-lation systems rely heavily on machine learning.For transduction grammars, this means that theyhave to be induced from parallel corpora.An inversion transduction grammar (ITG)strikes a good balance between STGs and SDTGs,as it allows some reordering, while requiring onlypolynomial time to be induced from parallel cor-pora.
The allowed reordering is either the iden-tity permutation of the production, or the inver-sion permutation.
Restricting the permutations inthis way ensures that an ITG can be expressed intwo-normal form, which is the key property foravoiding exponential time complexity in biparsing(parsing of a sentence pair).An ITG in two-normal form (representing thetransduction between L1 and L2) is written withidentity productions in square brackets, and in-verted productions in angle brackets.
Each suchrule can be construed to represent two (one L1 andone L2) synchronized CFG rules:ITGL1,L2 CFGL1 CFGL2A?
[ B C ] A?
B C A?
B CA?
?
B C ?
A?
B C A?
C BA?
e/f A?
e A?
fInducing an ITG from a parallel corpus is still slow,as the time complexity is O(Gn6).
Several waysto get around this has been proposed (Zhang et al,2008; Haghighi et al, 2009; Saers et al, 2009;Saers et al, 2010).Taking a closer look at the linear ITGs (Saers etal., 2010), there are five rules in normal form.
De-composing these five rule types into monolingualrule types reveals that the monolingual grammarsare linear grammars (LGs):LITGL1,L2 LGL1 LGL2A?
[ e/f C ] A?
e C A?
f CA?
[ B e/f ] A?
B e A?
B fA?
?
e/f C ?
A?
e C A?
C fA?
?
B e/f ?
A?
B e A?
f BA?
/ A?
 A?
This means that LITGs are transduction grammarsthat transduce between linear languages.There is also a nice parallel in search time com-plexities between CFGs and ITGs on the one hand,and LGs and LITGs on the other.
Searching forall possible parses given a sentence is O(n3) forCFGs, and O(n2) for LGs.
Searching for all possi-ble biparses given a bisentence is O(n6) for ITGs,and O(n4) for LITGs.
This is consistent withthinking of biparsing as finding every L2 parse forevery L1 parse.
Biparsing consists of assigning ajoint structure to a sentence pair, rather than as-signing a structure to a sentence.In this paper, only stochastic bracketing gram-mars (SBITGs and SBLITGs) were used.
A brack-eting grammar has only one nonterminal symbol,denoted X .
A stochastic grammar is one whereeach rule is associated with a probability, such that?X???
?p(X ?
?)
= 1?
?While training a Stochastic Bracketing ITG(SBITG) or LITG (SBLITG) with EM, expectationsof probabilities over the biparse-forest are calcu-lated.
These expectations approach the true prob-abilities, and can be used as approximations.
Theprobabilities over the biparse-forest can be usedto select the one-best parse-tree, which in turnforces an alignment over the sentence pair.
Thealignments given by SBITGs and SBLITGs has beenshown to give better translation quality than bidi-rectional IBM-models, when applied to short sen-tence corpora (Saers and Wu, 2009; Saers et al,2009; Saers et al, 2010).
In this paper we ex-plore whether this hold for SBLITGs on standardsentence corpora.3 SetupThe baseline system for the shared task was aphrase based translation model based on bidi-rectional IBM- (Brown et al, 1993) and HMM-models (Vogel et al, 1996) combined with thegrow-diag-final-and heuristic.
This iscomputed with the GIZA++ tool (Och and Ney,2003) and the Moses toolkit (Koehn et al, 2007).The language model was a 5-gram SRILM (Stol-cke, 2002).
Parameters in the final translation sys-tem were determined with Minimum Error-RateTraining (Och, 2003), and translation quality wasassessed with the automatic measures BLEU (Pap-ineni et al, 2002) and NIST (Doddington, 2002).168Corpus Type SizeGerman?English Europarl out of domain 1,219,343 sentence pairsGerman?English news commentary in-domain 86,941 sentence pairsEnglish news commentary in-domain 48,653,884 sentencesGerman?English news commentary in-domain tuning data 2,051 sentence pairsGerman?English news commentary in-domain test data 2,489 sentence pairsTable 1: Corpora available for the German?English translation task after baseline cleaning.System BLEU NISTGIZA++ 17.88 5.9748SBLITG 17.61 5.8846SBLITG (only Europarl) 17.46 5.8491SBLITG (only news) 15.49 5.4987GIZA++ and SBLITG 17.66 5.9650GIZA++ and SBLITG (only Europarl) 17.58 5.9819GIZA++ and SBLITG (only news) 17.48 5.9693Table 2: Results for the German?English translation task.We chose to focus on the German?Englishtranslation task.
The corpora resources availablefor that task is summarized in Table 1.
We used theentire news commentary monolingual data con-catenated with the English side of the Europarlbilingual data to train the language model.
In ret-rospect, this was probably a bad choice, as othersseem to prefer the use of two language models in-stead.We contrasted the baseline system with pureSBLITG systems trained on different parts of thetraining data, as well as combined systems, wherethe SBLITG systems were combined with the base-line system.
The combination was done by addingthe SBLITG translation model as a second transla-tion path to the base line system.To train our SBLITG systems, we used the algo-rithm described in Saers et al (2010).
We set thebeam size parameter to 50, and ran expectation-maximization for 10 iterations or until the log-probability of the training corpus started deterio-rating.
After the grammar was induced we ob-tained the one-best parse for each sentence pair,which also dictates a word alignment over thatsentence pair, which we used instead of the wordalignments provided by GIZA++.
From that point,training did not differ from the baseline procedure.We trained a total of three pure SBLITG system,one with only the news commentary part of thecorpus, one with only the Europarl part, and onewith both.
We also combined all three SBLITGsystems with the baseline system to see whetherthe additional translation paths would help.The system we submitted corresponds to the?GIZA++ and SBLITG (only news)?
system, butwith RandLM (Talbot and Osborne, 2007) as lan-guage model rather than SRILM.
This was becausewe lacked the necessary RAM resources to calcu-late the full SRILM model before the system sub-mission deadline.4 ResultsThe results for the development test set are sum-marized in Table 2.
The submitted systemachieved a BLEU score of 0.1759 and a NISTscore of 5.9579 for cased output on this year?s testset (these numbers are not comparable to thosein Table 2).
To our surprise, adding the addi-tional phrases as a second translation path doesnot seem to help.
Instead a small deteriorationin BLEU is noted (0.22?0.40 points), whereas thedifferences in NIST are mixed (-0.0098?+0.0071points).
Over all the variations were very small.The pure SBLITG systems perform consistentlybelow baseline, which could indicate that thegrammar class is unable to capture the reorderingsfound in longer sentence pairs adequately in oneparse.
The variation between the pure SBLITG sys-tems can be explained by the size of the trainingdata: more data ?
better quality.1695 ConclusionsWe tried to use SBLITGs as word aligners on fullsize sentences, which has not been done to date,and noted that the formalism seems unable to ac-count for the full complexity of longer sentencepairs.
We also tried combining the translationmodels acquired with SBLITG alignments to thebaseline system, and noted very small differences,tending to a deterioration in quality.
The fact thatSBLITGs seem unable to capture the complex re-lationship between an English and a German sen-tence in one parse means that we need to find ei-ther some more complex model or some way touse the entire parse forest to arrive at the align-ment.AcknowledgmentsThis work was funded by the Swedish Na-tional Graduate School of Language Technol-ogy (GSLT), the Defense Advanced ResearchProjects Agency (DARPA) under GALE Con-tract No.
HR0011-06-C-0023, and the HongKong Research Grants Council (RGC) underresearch grants GRF621008, DAG03/04.EG09,RGC6256/00E, and RGC6083/99E.
Any opinions,findings and conclusions or recommendations ex-pressed in this material are those of the authors anddo not necessarily reflect the views of the DefenseAdvanced Research Projects Agency.
The com-putations were performed on UPPMAX resourcesunder project p2007020.ReferencesJeffrey D. Aho, Alfred V. Ullman.
1972.
The Theoryof Parsing, Translation, and Compiling.
Prentice-Halll, Englewood Cliffs, NJ.Peter F Brown, Stephen A Della Pietra, Vincent J DellaPietra, and Robert L Mercer.
1993.
The mathemat-ics of statistical machine translation: Parameter esti-mation.
Computational Linguistics, 19(2):263?311.George Doddington.
2002.
Automatic evaluationof machine translation quality using n-gram co-occurrence statistics.
In Proceedings of HumanLanguage Technology conference (HLT-2002), SanDiego, California.Aria Haghighi, John Blitzer, John DeNero, and DanKlein.
2009.
Better word alignments with super-vised ITG models.
In Proceedings of the Joint Con-ference of the 47th Annual Meeting of the ACL andthe 4th International Joint Conference on NaturalLanguage Processing of the AFNLP, pages 923?931, Suntec, Singapore, August.Philipp Koehn, Hieu Hoang, Alexandra Birch, ChrisCallison-Burch, Marcello Federico, Nicola Bertoldi,Brooke Cowan, Wade Shen, Christine Moran,Richard Zens, Chris Dyer, Ondrej Bojar, Alexan-dra Constantin, and Evan Herbst.
2007.
Moses:Open source toolkit for statistical machine transla-tion.
In Proceedings of the 45th Annual Meeting ofthe Association for Computational Linguistics Com-panion Volume Proceedings of the Demo and PosterSessions, pages 177?180, Prague, Czech Republic,June.Philip M. Lewis and Richard E. Stearns.
1968.
Syntax-directed transduction.
Journal of the Association forComputing Machinery, 15(3):465?488.Franz Josef Och and Hermann Ney.
2003.
A sys-tematic comparison of various statistical alignmentmodels.
Computational Linguistics, 29(1):19?51.Franz Josef Och.
2003.
Minimum error rate trainingin statistical machine translation.
In 41st AnnualMeeting of the Association for Computational Lin-guistics, pages 160?167, Sapporo, Japan, Jul.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
Bleu: a method for automaticevaluation of machine translation.
In Proceedingsof 40th Annual Meeting of the Association for Com-putational Linguistics, pages 311?318, Philadelphia,Pennsylvania, July.Markus Saers and Dekai Wu.
2009.
Improving phrase-based translation via word alignments from Stochas-tic Inversion Transduction Grammars.
In Proceed-ings of the Third Workshop on Syntax and Structurein Statistical Translation (SSST-3) at NAACL HLT2009, pages 28?36, Boulder, Colorado, June.Markus Saers, Joakim Nivre, and Dekai Wu.
2009.Learning Stochastic Bracketing Inversion Transduc-tion Grammars with a cubic time biparsing algo-rithm.
In Proceedings of the 11th International Con-ference on Parsing Technologies (IWPT?09), pages29?32, Paris, France, October.Markus Saers, Joakim Nivre, and Dekai Wu.
2010.Word alignment with Stochastic Bracketing LinearInversion Transduction Grammar.
In Proceedings ofHuman Language Technologies: The 11th AnnualConference of the North American Chapter of theAssociation for Computational Linguistics, Los An-geles, California, June.Andreas Stolcke.
2002.
SRILM ?
an extensible lan-guage modeling toolkit.
In International Confer-ence on Spoken Language Processing, Denver, Col-orado, September.David Talbot and Miles Osborne.
2007.
Randomisedlanguage modelling for statistical machine transla-tion.
In Proceedings of the 45th Annual Meeting ofthe Association of Computational Linguistics, pages512?519, Prague, Czech Republic, June.170Stephan Vogel, Hermann Ney, and Christoph Tillmann.1996.
Hmm-based word alignment in statisticaltranslation.
In Proceedings of the 16th conferenceon Computational linguistics, pages 836?841, Mor-ristown, New Jersey.Dekai Wu.
1997.
Stochastic inversion transductiongrammars and bilingual parsing of parallel corpora.Computational Linguistics, 23(3):377?403.Hao Zhang, Chris Quirk, Robert C. Moore, andDaniel Gildea.
2008.
Bayesian learning of non-compositional phrases with synchronous parsing.In Proceedings of ACL-08: HLT, pages 97?105,Columbus, Ohio, June.171
