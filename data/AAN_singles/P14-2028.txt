Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 168?173,Baltimore, Maryland, USA, June 23-25 2014.c?2014 Association for Computational LinguisticsImproved Iterative Correction for Distant Spelling ErrorsSergey Gubanov Irina Galinskaya Alexey BaytinYandex16 Leo Tolstoy St., Moscow, 119021 Russia{esgv,galinskaya,baytin}@yandex-team.ruAbstractNoisy channel models, widely used inmodern spellers, cope with typical mis-spellings, but do not work well with infre-quent and difficult spelling errors.
In thispaper, we have improved the noisy chan-nel approach by iterative stochastic searchfor the best correction.
The proposed al-gorithm allowed us to avoid local minimaproblem and improve the F1measure by6.6% on distant spelling errors.1 IntroductionA speller is an essential part of any program as-sociated with text input and processing ?
e-mailsystem, search engine, browser, form editor etc.To detect and correct spelling errors, the state ofthe art spelling correction systems use the noisychannel approach (Kernighan et al, 1990; Mayset al, 1991; Brill and Moore, 2000).
Its modelsare usually trained on large corpora and providehigh effectiveness in correction of typical errors(most of which consist of 1-2 wrong characters perword), but does not work well for complex (multi-character) and infrequent errors.In this paper, we improved effectiveness ofthe noisy channel for the correction of com-plex errors.
In most cases, these are cogni-tive errors in loan words (folsvagen ?
volkswa-gen), names of drugs (vobemzin ?
wobenzym),names of brands (scatcher?
skechers), scientificterms (heksagidron?
hexahedron) and last names(Shwartzneger ?
Schwarzenegger).
In all thesecases, the misspelled word contains many errorsand the corresponding error model penalty cannotbe compensated by the LM weight of its properform.
As a result, either the misspelled word it-self, or the other (less complicated, more frequent)misspelling of the same word wins the likelihoodrace.To compensate for this defect of the noisy chan-nel, the iterative approach (Cucerzan and Brill,2004) is typically used.
The search for the bestvariant is repeated several times, what allows cor-recting rather complex errors, but does not com-pletely solve the problem of falling into local min-ima.
To overcome this issue we suggest to con-sider more correction hypotheses.
For this pur-pose we used a method based on the simulatedannealing algorithm.
We experimentally demon-strate that the proposed method outperforms thebaseline noisy channel and iterative spellers.Many authors employ machine learning to buildrankers that compensate for the drawbacks of thenoisy channel model: (Whitelaw et al, 2009; Gaoet al, 2010).
These techniques can be combinedwith the proposed method by replacing posteriorprobability of single correction in our method withan estimate obtained via discriminative trainingmethod.In our work, we focus on isolated word-errorcorrection (Kukich, 1992), which, in a sense, is aharder task, than multi-word correction, becausethere is no context available for misspelled words.For experiments we used single-word queries to acommercial search engine.2 Baseline speller2.1 Noisy channel spelling correctionNoisy channel is a probabilistic model that definesposterior probability P (q0|q1) of q0being the in-tended word, given the observed word q1; for suchmodel, the optimal decision rule ?
is the follow-ing:?
(q1) = argmaxq0P (q0|q1);P (q0|q1) ?
Pdist(q0?
q1)PLM(q0),(1)where PLMis the source (language) model, andPdistis the error model.
Given P (q0|q1) defined,to correct the word q1we could iterate through168all ever-observed words, and choose the one, thatmaximizes the posterior probability.
However,the practical considerations demand that we donot rank the whole list of words, but insteadchoose between a limited number of hypothesesh1, ..., hK:1.
Given q1, generate a set of hypothesesh1, ..., hK, such thatK?k=1P (q0= hk|q1) ?
1; (2)2.
Choose the hypothesis hkthat maximizesP (q0= hk|q1).If hypotheses constitute a major part of the poste-rior probability mass, it is highly unlikely that theintended word is not among them.2.2 Baseline speller setupIn baseline speller we use a substring-based errormodel Pdist(q0?
q1) described in (Brill andMoore, 2000), the error model training methodand the hypotheses generator are similar to (Duanand Hsu, 2011).For building language (PLM?)
and error (Pdist?
)models, we use words collected from the 6-monthsquery log of a commercial search engine.Hypotheses generator is based on A* beamsearch in a trie of words, and yields K hy-potheses hk, for which the noisy channel scoresPdist(hk?
q1)PLM(hk) are highest possible.Hypotheses generator has high K-best recall (seeSection 4.2) ?
in 91.8% cases the correct hy-pothesis is found when K = 30, which confirmsthe assumption about covering almost all posteriorprobability mass (see Equation 2).3 Improvements for noisy channelspelling correctionWhile choosing argmax of the posterior probabil-ity is an optimal decision rule in theory, in practiceit might not be optimal, due to limitations of thelanguage and error modeling.
For example, vobe-mzin is corrected to more frequent misspellingvobenzin (instead of correct form wobenzym) bythe noisy channel, because Pdist(vobemzin ?wobenzym) is too low (see Table 1).There have been attempts (Cucerzan and Brill,2004) to apply other rules, which would over-come limitations of language and error modelswith compensating changes described further.c ?
logPdist?
logPLM?vobenzin 2.289 31.75 34.04wobenzym 12.52 26.02 38.54Table 1: Noisy-channel scores for two correctionsof vobemzin3.1 Iterative correctionIterative spelling correction with E iterations usesstandard noisy-channel to correct the query q re-peatedly E times.
It is motivated by the assump-tion, that we are more likely to successfully correctthe query if we take several short steps instead ofone big step (Cucerzan and Brill, 2004) .Iterative correction is hill climbing in the spaceof possible corrections: on each iteration we makea transition to the best point in the neighbourhood,i.e.
to correction, that has maximal posterior prob-ability P (c|q).
As any local search method, itera-tive correction is prone to local minima, stoppingbefore reaching the correct word.3.2 Stochastic iterative correctionA common method of avoiding local minima inoptimization is the simulated annealing algorithm,key ideas from which can be adapted for spellingcorrection task.
In this section we propose such anadaptation.
Consider: we do not always transitiondeterministically to the next best correction, butinstead transition randomly to a (potentially any)correction with transition probability being equalto the posterior P (ci|ci?1), where ci?1is the cor-rection we transition from, ciis the correction wetransition to, and P (?|?)
is defined by Equation 1.Iterative correction then turns into a random walk:we start at word c0= q and stop after E ran-dom steps at some word cE, which becomes ouranswer.To turn random walk into deterministic spellingcorrection algorithm, we de-randomize it, usingthe following transformation.
Described randomwalk defines, for each word w, a probabilityP (cE= w|q) of ending up in w after starting awalk from the initial query q.
With that probabilitydefined, our correction algorithm is the following:given query q, pick c = argmaxcEP (cE|q) as acorrection.Probability of getting from c0= q to somecE= c is a sum, over all possible paths, of prob-abilities of getting from q to c via specific path169q = c0?
c1?
...?
cE?1?
cE= c:P (cE|c0) =?c1?W...cE?1?WE?i=1P (ci|ci?1), (3)P (ci|ci?1) =Pdist(ci?
ci?1)PLM(ci)Pobserve(ci?1), (4)where W is the set of all possible words, andPobserve(w) is the probability of observing w asa query in the noisy-channel model.Example: if we start a random walk from vobe-mzin and make 3 steps, we most probably will endup in the correct form wobenzym with P = 0.361.A few of the most probable random walk pathsare shown in Table 2.
Note, that despite the factthat most probable path does not lead to the cor-rect word, many other paths to wobenzym sum upto 0.361, which is greater than probability of anyother word.
Also note, that the method works onlybecause multiple misspellings of the same wordare presented in our model; for related researchsee (Choudhury et al, 2007).c0?
c1?
c2?
c3Pvobemzin?vobenzin?vobenzin?vobenzin 0.074vobemzin?vobenzim?wobenzym?wobenzym 0.065vobemzin?vobenzin?vobenzim?vobenzim 0.052vobemzin?vobenzim?vobenzim?wobenzym 0.034vobemzin?wobenzym?wobenzym?wobenzym 0.031vobemzin?wobenzim?wobenzym?wobenzym 0.028vobemzin?wobenzyn?wobenzym?wobenzym 0.022Table 2: Most probable random walk paths start-ing from c0= q = vobemzin (the correct form isin bold).Also note, that while Equation 3 uses noisy-channel posteriors, the method can use an arbitrarydiscriminative model, for example the one from(Gao et al, 2010), and benefit from a more accu-rate posterior estimate.3.3 Additional heuristicsThis section describes some common heuristic im-provements, that, where possible, were appliedboth to the baseline methods and to the proposedalgorithm.Basic building block of every mentioned algo-rithm is one-step noisy-channel correction.
Eachbasic correction proceeds as described in Sec-tion 2.1: a small number of hypotheses h1, ..., hKis generated for the query q, hypotheses are scored,and scores are recomputed into normalized pos-terior probabilities (see Equation 5).
Posteriorprobabilities are then either used to pick the bestcorrection (in baseline and simple iterative cor-rection), or are accumulated to later compute thescore defined by Equation 3.score(hi) = Pdist(hi?
q)?PLM(hi)P (hi|q) = score(hi)/K?j=1score(hj)(5)A standard log-linear weighing trick was ap-plied to noisy-channel model components, see e.g.
(Whitelaw et al, 2009).
?
is the parameter thatcontrols the trade-off between precision and recall(see Section 4.2) by emphasizing the importanceof either the high frequency of the correction or itsproximity to the query.We have also found, that resulting posteriorprobabilities emphasize the best hypothesis toomuch: best hypothesis gets almost all probabilitymass and other hypotheses get none.
To compen-sate for that, posteriors were smoothed by raisingeach probability to some power ?
< 1 and re-normalizing them afterward:Psmooth(hi|q) = P (hi|q)?/K?j=1P (hj|q)?.
(6)In a sense, ?
is like temperature parameter in sim-ulated annealing ?
it controls the entropy of thewalk and the final probability distribution.
Unlikein simulated annealing, we fix ?
for all iterationsof the algorithm.Finally, if posterior probability of the best hy-pothesis was lower than threshold ?, then the orig-inal query q was used as the spell-checker output.
(Posterior is defined by Equation 6 for the baselineand simple iterative methods and by Equations 3and 6 for the proposed method).
Parameter ?
con-trols precision/recall trade-off (as well as ?
men-tioned above).4 Experiments4.1 DataTo evaluate the proposed algorithm we have col-lected two datasets.
Both datasets were randomlysampled from single-word user queries from the1-week query log of a commercial search en-gine.
We annotated them with the help of pro-fessional analyst.
The difference between datasets170is that one of them contained only queries withlow search performance: for which the numberof documents retrieved by the search engine wasless than a fixed threshold (we will address it asthe ?hard?
dataset), while the other dataset hadno such restrictions (we will call it ?common?
).Dataset statistics are shown in Table 3.Dataset Queries Misspelled Avg.
?
logPdistCommon 2240 224 (10%) 5.98Hard 2542 1484 (58%) 9.23Table 3: Evaluation datasets.Increased average error model score and er-ror rate of ?common?
dataset compared to ?hard?shows, that we have indeed managed to collecthard-to-correct queries in the ?hard?
dataset.4.2 Experimental resultsFirst of all, we evaluated the recall of hypothe-ses generator using K-best recall ?
the number ofcorrect spelling corrections for misspelled queriesamong K hypotheses divided by the total numberof misspelled queries in the test set.
Resulting re-call with K = 30 is 91.8% on ?hard?
and 98.6%on ?common?.Next, three spelling correction methods weretested: noisy channel, iterative correction and ourmethod (stochastic iterative correction).For evaluation of spelling correction quality, weuse the following metrics:?
Precision: The number of correct spellingcorrections for misspelled words generatedby the system divided by the total number ofcorrections generated by the system;?
Recall: The number of correct spelling cor-rections for misspelled words generated bythe system divided by the total number ofmisspelled words in the test set;For hypotheses generator, K = 30 was fixed: re-call of 91.8% was considered big enough.
Pre-cision/recall tradeoff parameters ?
and ?
(theyare applicable to each method, including baseline)were iterated by the grid (0.2, 0.25, 0.3, ..., 1.5)?
(0, 0.025, 0.05, ..., 1.0), and E (applicable to it-erative and our method) and ?
(just our method)were iterated by the grid (2, 3, 4, 5, 7, 10) ?
(0.1, 0.15, ...1.0); for each set of parameters, pre-cision and recall were measured on both datasets.Pareto frontiers for precision and recall are shownin Figures 1 and 2.Figure 1: Precision/recall Pareto frontiers on?hard?
datasetFigure 2: Precision/recall Pareto frontiers on?common?
datasetWe were not able to reproduce superior perfor-mance of the iterative method over the noisy chan-nel, reported by (Cucerzan and Brill, 2004).
Sup-posedly, it is because the iterative method bene-fits primarily from the sequential application ofsplit/join operations altering query decompositioninto words; since we are considering only one-word queries, such decomposition does not matter.On the ?hard?
dataset the performance of thenoisy channel and the iterative methods is infe-rior to our proposed method, see Figure 1.
Wetested all three methods on the ?common?
datasetas well to evaluate if our handling of hard casesaffects the performance of our approach on thecommon cases of spelling error.
Our method per-forms well on the common cases as well, as Fig-ure 2 shows.
The performance comparison forthe ?common?
dataset shows comparable perfor-mance for all considered methods.Noisy channel and iterative methods?
frontiers171are considerably inferior to the proposed methodon ?hard?
dataset, which means that our methodworks better.
The results on ?common?
datasetshow, that the proposed method doesn?t workworse than baseline.Next, we optimized parameters for each methodand each dataset separately to achieve the highestF1measure.
Results are shown in Tables 4 and 5.We can see, that, given the proper tuning, ourmethod can work better on any dataset (but it can-not achieve the best performance on both datasetsat once).
See Tables 4 and 5 for details.Method ?
?
?
E F1Noisy channel 0.6 0.1 - - 55.8Iterative 0.6 0.1 - 2 55.9Stochastic iterative 0.9 0.2 0.35 3 62.5Table 4: Best parameters and F1on ?hard?
datasetMethod ?
?
?
E F1Noisy channel 0.75 0.225 - - 62.06Iterative 0.8 0.275 - 2 63.15Stochastic iterative 1.2 0.4 0.35 3 63.9Table 5: Best parameters and F1on ?common?datasetNext, each parameter was separately iterated(by a coarser grid); initial parameters for eachmethod were taken from Table 4.
Such iterationserves two purposes: to show the influence of pa-rameters on algorithm performance, and to showdifferences between datasets: in such setup pa-rameters are virtually tuned using ?hard?
datasetand evaluated using ?common?
dataset.
Resultsare shown in Table 6.The proposed method is able to successfullycorrect distant spelling errors with edit distance of3 characters (see Table 7).However, if our method is applied to shorterand more frequent queries (as opposed to ?hard?dataset), it tends to suggest frequent words asfalse-positive corrections (for example, grid is cor-rected to creed ?
Assassin?s Creed is popular videogame).
As can be seen in Table 5, in order to fixthat, algorithm parameters need to be tuned moretowards precision.5 Conclusion and future workIn this paper we introduced the stochastic itera-tive correction method for spell check corrections.Our experimental evaluation showed that the pro-posed method improved the performance of popu-F1, common F1, hardN.ch.
It.
Our N.ch.
It.
Our?
= 0.5 45.3 45.9 37.5 54.9 54.8 50.00.6 49.9 50.5 41.5 55.8 55.9 56.60.7 50.4 50.4 44.1 54.5 55.1 59.60.8 52.7 52.7 46.0 52.6 53.0 61.50.9 53.5 53.5 49.3 50.3 50.6 62.51.0 55.4 55.0 50.9 47.0 47.3 61.81.1 53.7 53.4 52.7 44.3 44.6 60.81.2 52.5 52.5 53.7 41.9 42.3 58.81.3 52.2 52.6 54.6 39.5 39.9 56.61.4 51.4 51.8 55.0 36.8 37.3 53.6?
= 0 41.0 41.5 33.0 52.9 53.1 58.30.1 49.9 50.6 35.6 55.8 55.9 59.70.15 59.4 59.8 43.2 55.8 55.6 61.60.2 60.8 61.3 49.4 51.0 51.0 62.50.25 54.0 54.0 54.9 46.3 46.3 61.10.3 46.3 46.3 57.3 39.2 39.2 58.40.4 25.8 25.8 53.9 22.3 22.3 50.3E = 2 50.6 53.6 55.9 60.43 50.6 49.4 55.9 62.54 50.6 46.4 55.9 62.15 50.6 46.7 55.9 60.1?
= 0.1 10.1 6.00.2 49.4 51.50.3 51.4 61.40.35 49.4 62.50.4 47.5 62.00.45 45.8 60.80.5 45.2 60.3Table 6: Per-coordinate iteration of parametersfrom Table 4; per-method maximum is shown initalic, per-dataset in boldQuery Noisy channel Proposed methodakwamarin akvamarin aquamarinemaccartni maccartni mccartneyariflaim ariflaim oriflameepika epica replicagrid grid creedTable 7: Correction examples for the noisy chan-nel and the proposed method.lar spelling correction approach ?
the noisy chan-nel model ?
in the correction of difficult spellingerrors.
We showed how to eliminate the local min-ima issue of simulated annealing and proposed atechnique to make our algorithm deterministic.The experiments conducted on the specializeddatasets have shown that our method significantlyimproves the performance of the correction ofhard spelling errors (by 6.6% F1) while maintain-ing good performance on common spelling errors.In continuation of the work we are consideringto expand the method to correct errors in multi-word queries, extend the method to work with dis-criminative models, and use a query performanceprediction method, which tells for a query whetherour algorithm needs to be applied.172ReferencesEric Brill and Robert C Moore.
2000.
An improved er-ror model for noisy channel spelling correction.
InProceedings of the 38th Annual Meeting on Associa-tion for Computational Linguistics, pages 286?293.Association for Computational Linguistics.Monojit Choudhury, Markose Thomas, AnimeshMukherjee, Anupam Basu, and Niloy Ganguly.2007.
How difficult is it to develop a perfect spell-checker?
a cross-linguistic analysis through com-plex network approach.
In Proceedings of the sec-ond workshop on TextGraphs: Graph-based algo-rithms for natural language processing, pages 81?88.Silviu Cucerzan and Eric Brill.
2004.
Spelling correc-tion as an iterative process that exploits the collec-tive knowledge of web users.
In EMNLP, volume 4,pages 293?300.Huizhong Duan and Bo-June Paul Hsu.
2011.
On-line spelling correction for query completion.
InProceedings of the 20th international conference onWorld wide web, pages 117?126.
ACM.Jianfeng Gao, Xiaolong Li, Daniel Micol, Chris Quirk,and Xu Sun.
2010.
A large scale ranker-based sys-tem for search query spelling correction.
In Pro-ceedings of the 23rd International Conference onComputational Linguistics, pages 358?366.
Associ-ation for Computational Linguistics.Mark D Kernighan, Kenneth W Church, and William AGale.
1990.
A spelling correction program based ona noisy channel model.
In Proceedings of the 13thconference on Computational linguistics-Volume 2,pages 205?210.
Association for Computational Lin-guistics.Karen Kukich.
1992.
Techniques for automaticallycorrecting words in text.
ACM Computing Surveys(CSUR), 24(4):377?439.Eric Mays, Fred J Damerau, and Robert L Mercer.1991.
Context based spelling correction.
Informa-tion Processing & Management, 27(5):517?522.Casey Whitelaw, Ben Hutchinson, Grace Y Chung, andGerard Ellis.
2009.
Using the web for languageindependent spellchecking and autocorrection.
InProceedings of the 2009 Conference on EmpiricalMethods in Natural Language Processing: Volume2-Volume 2, pages 890?899.
Association for Com-putational Linguistics.173
