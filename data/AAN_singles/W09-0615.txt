Proceedings of the 12th European Workshop on Natural Language Generation, pages 98?101,Athens, Greece, 30 ?
31 March 2009. c?2009 Association for Computational LinguisticsA Hearer-oriented Evaluation of Referring Expression Generation ?Imtiaz H. Khan, Kees van Deemter, Graeme Ritchie, Albert Gatt, Alexandra A. ClelandUniversity of Aberdeen, Aberdeen, Scotland, United Kingdom{i.h.khan,k.vdeemter,g.ritchie,a.gatt,a.cleland}@abdn.ac.ukAbstractThis paper discusses the evaluation of aGeneration of Referring Expressions algo-rithm that takes structural ambiguity intoaccount.
We describe an ongoing studywith human readers.1 IntroductionIn recent years, the NLG community has seen asubstantial number of studies to evaluate Gener-ation of Referring Expressions (GRE) algorithms,but it is still far from clear what would constitutean optimal evaluation method.
Two limitationsstand out in the bulk of existing work.
Firstly,most existing evaluations are essentially speaker-oriented, focussing on the degree of ?human-likeness?
of the generated descriptions, disre-garding their effectiveness (e.g.
Mellish and Dale(1998), Gupta and Stent (2005), van Deemter et al(2006), Belz and Kilgarriff (2006), Belz and Re-iter (2006), Paris et al (2006), Viethen and Dale(2006), Gatt and Belz (2008)).
The limited num-ber of exceptions to this rule indicate that the dif-ferences between the two approaches to evaluationcan be substantial (Gatt and Belz, 2008).
Sec-ondly, most evaluations have focussed on the se-mantic content of the generated descriptions, asproduced by the Content Determination stage ofa GRE algorithm; this means that linguistic re-alisation (i.e.
the choice of words and linguisticconstructions) is usually not addressed (exceptionsare: Stone and Webber (1998), Krahmer and The-une (2002), Siddharthan and Copestake (2004)).Our aim is to build GRE algorithms that producereferring expressions that are of optimal benefit toa hearer.
That is, we are interested in generatingdescriptions that are easy to read and understand.But the readability and intelligibility of a descrip-tion can crucially depend on the way in which it is?
This work is supported by a University of AberdeenSixth Century Studentship, and EPSRC grant EP/E011764/1.worded.
This happens particularly when there ispotential for misunderstanding, as can happen inthe case of attachment and scope ambiguities.Suppose, for example, one wants to make itclear that all radical students and all radical teach-ers are in agreement with a certain idea.
It mightbe risky to express this as ?the radical students andteachers are agreed?, since the reader1 might beinclined to interpret this as pertaining to all teach-ers rather than only the radical ones.
For this rea-son, a GRE program might opt for the longer nounphrase ?the radical students and the radical teach-ers?.
But because this expression is lengthier, thechoice involves a compromise between compre-hensibiliity and brevity, a special case of a diffi-cult trade-off that is typical of generation as wellas interpretation of language (van Deemter, 2004).We previously reported the design of an algo-rithm (based on an earlier work on expressions re-ferring to sets (Gatt, 2007)), which was derivedfrom experiments in which readers were asked toexpress their preference between different descrip-tions and to respond to instructions which used avariety of phrasings (Khan et al, 2008).
Here wediscuss the issues that arise when such an algo-rithm is evaluated in terms of its benefits for read-ers.2 Summary of the algorithmIn order to study specific data, we have focussedon the construction illustrated in Section 1 above:potentially ambiguous Noun Phrases of the gen-eral form the Adj Nouni and Nounj .
For suchphrases, there are potentially two interpretations:wide scope (Adj modifies both Nouni and Nounj)or narrow scope (Adj modifies Nouni but notNounj).Our algorithm starts from an unambiguous set-theoretic formula over lexical items (i.e.
words1In this paper, we use the word reader and hearer inter-changeably.98have already been chosen), and thus has to choosebetween a number of different realisations.
Thepossible phrasings for the wide scope meaning are:(1) the Adj Noun1 and Noun2, (2) the Adj Noun2and Noun1, (3) the Adj Noun1 and the Adj Noun2,and (4) the Adj Noun2 and the Adj Noun1.
For nar-row scope, the possibilities are: (1) the Adj Noun1and Noun2, (2) the Noun2 and Adj Noun1, (3) theAdj Noun1 and the Noun2, and (4) the Noun2 andthe Adj Noun1.
For our purposes, (1) and (2) aredesignated as ?brief?, (3) and (4) as ?non-brief?
(that is, ?brevity?
has a specialised sense involv-ing the presence/absence of ?the?
and possibly Adjbefore the second Noun).
Importantly, the ?non-brief?
expressions are syntactically unambiguous,but the ?brief?
NPs are potentially ambiguous, andhence are the focus of attention in this work.Our algorithm is based on certain specific hy-potheses (from the earlier experiments) whichmake crucial use of corpus data concerning thefrequency of two types of collocations: the col-location between an adjective and a noun, and thecollocation between two nouns.
At a broader level,we hypothesise: the most likely reading of an NPcan be predicted using corpus data (Word Sketches(Kilgarriff, 2003)).
The more specific hypothesesderive from earlier work by Kilgarriff (2003) andChantree et al (2006), and were further developedand tested in our previous experiments.
The cen-tral idea is that this statistical information can beused to predict a ?most likely?
scoping (and henceinterpretation) for the adjective in the ?brief?
(i.e.potentially ambiguous) NPs.
We define an NP tobe predictable if our model predicts a single read-ing for it; otherwise it is unpredictable.
Hence, all?non-brief?
NPs are predictable (being unambigu-ous), but only some of the ?brief?
ones are pre-dictable.In a nutshell, the model underlying our algo-rithm prefers predictable expressions to unpre-dictable ones, but if several of the expressions arepredictable then brief expressions are preferredover non-brief.3 Aims of the studyWe want to find out whether our generatormakes the best possible choices (for hearers) fromamongst the different ways in which a given de-scription can be realised.
But although our al-gorithm uses sophisticated strategies for avoidingnoun phrases that it believes to be liable to mis-understanding, misunderstandings cannot be ruledout, and if a hearer misunderstands a noun phrasethen secondary aspects such as reading (and/orcomprehension) speed are of little consequence.We therefore plan first to find out the likelihood ofmisunderstanding.
For this reason, we will reporton the degree of accuracy, as a percentage of timesthat a participant?s understanding of an expressionthat we label as predictable fails to match the in-terpretation assigned by our model.
Additionally,we shall statistically test two hypotheses:Comprehension Accuracy 1: Predictable ex-pressions are more often interpreted inagreement than in disagreement with themodel.Comprehension Accuracy 2: There is moreagreement among participants on the inter-pretation of predictable expressions than ofunpredictable expressions.We will not only test the comprehensibility of theexpressions generated by our algorithm, but theirreadability and intelligibility as well.
This is nec-essary because the experiments which led to thealgorithm design considered only certain aspectsof the hearer?s reaction to NPs (e.g.
metalinguisticjudgements about a participant?s preferences) andwe wish to check these comprehensibility/brevityfacets from a different, perhaps psycholinguisti-cally more valid, perspective.
It is also necessarybecause avoidance of misunderstandings is not theonly decisive factor: if several of the expressionsare predictable then our algorithm chooses be-tween them by preferring brevity.
But why is briefbetter than non-brief?
Taking readability and intel-ligibility together as ?processing speed?, our thirdhypothesis is:Processing speed: Subjects processpredictable brief expressions morequickly than predictable non-brief ones.Confirmation of this hypothesis would be a strongindication that our algorithm is on the right track,particularly if the degree of accuracy (see above)turns out to be high.
Processing speed is a com-plex concept, but we could decompose it as ?read-ing speed?
and ?comprehension speed?, permittingus to examine reading and comprehension sepa-rately.
We intend to see what evidence there is forthe following additional propositions, which willbe tested solely to aid our understanding.99Reading Speed:RS1: Subjects read predictable brief NPs morequickly than unpredictable brief ones.RS2: Subjects read unpredictable brief NPs morequickly than predictable non-brief ones.RS3: Subjects read predictable brief NPs morequickly than predictable non-brief ones.Comprehension Speed:CS1: Subjects comprehend predictable brief NPsmore quickly than unpredictable brief ones.CS2: Subjects comprehend predictable non-briefNPs more quickly than unpredictable brief ones.CS3: Subjects do not comprehend predictablenon-brief NPs more quickly than predictable briefones.
(Remember that, in our restricted set of NPs, aphrase cannot be both ?unpredictable?
and ?non-brief?.)
Rejection of any of these statements willnot count against our algorithm.4 Sketch of experimental procedureParticipants will be presented with a sequence oftrials (on a computer screen), each of which con-sists of a lead-in sentence followed by a target sen-tence and a comprehension question that relates tothe two sentences together.
The target sentencemight for example say ?the radical students andteachers were waving their hands?.
The compre-hension question in this case could be ?Were themoderate teachers waving their hands??.
As boththe target sentence and the comprehension ques-tion make use of definite NPs (e.g.
?the moderateteachers?
), it is necessary to ensure any presuppo-sitions about the existence of the referent set aremet, without biasing the answer.
For this reason,the target sentence is preceded by a lead-in sen-tence to establish the existence of the sets withinthe discourse (here, ?there were radical and mod-erate people in a rally?
).Given this set-up we are confident that wecan identify, from a participant?s yes/no answer,whether the NP in the target sentence was assigneda narrow-scope or a wide-scope reading for the ad-jective.
The computer will record the participant?sresponse as well as the length of time that the par-ticipant took to answer the question.
We will useLinger2 for presentation of stimuli.
Pilots sug-gest that the complexity of the trials makes it ad-visable to use masked sentence-based self-paced2http://tedlab.mit.edu/?dr/Linger/reading, in which every press of the space bar re-veals the next sentence and the previous sentenceis replaced by dashes.The choice of nouns and adjectives (to constructNPs) is motivated by the fact that there is a bal-anced distribution of NPs in each of the follow-ing three classes.
Wide scope class is the one forwhich our model predicts a wide-scope reading;narrow scope class is the one for which our modelpredicts a narrow-scope reading; and ambiguousclass is the one for which our model fails to pre-dict a single reading (Khan et al, 2008).5 Issues emerging from this studyThe design of this experiment raised some difficultquestions, some quite unexpected:1.
The quality of the output of a generation al-gorithm might appear to be a simple and well-understood concept.
However, output quality ismulti-faceted, because an expression may be easyto read but difficult to process semantically, or theother way round.
A thorough output evaluationshould address both aspects of quality, in our view.2.
If both reading and understanding are ad-dressed, this raises the question of how thesetwo dimensions should be traded off against eachother.
If one algorithm?s output was read morequickly than that of another, but understood moreslowly than the second, which of the two should bepreferred?
Perhaps there is a legitimate role herefor metalinguistic judgments after all, in whichparticipants are asked to express their preferencebetween expressions (see Paraboni et al (2006) fordiscussion)?
An alternative point of view is thatthese questions are impossible to answer indepen-dent of a realistic setting in which participants ut-ter sentences with a concrete communicative pur-pose in mind.
If utterances were made in order toaccomplish a concrete task (e.g., to win a game)then task-based evaluation would be possible.3.
Even though this paper has not focussed on de-tails of experimental design and analysis, one diffi-culty is worth mentioning: given the grammaticaloptions between which the generator is choosing,only three types of situations are represented: a de-scription can be brief and predictable (e.g.
using?the old men and women?
to convey wide scope,since the adjective is predicted by our algorithmto have wide scope), brief and unpredictable (e.g.
?the rowing boats and ships?
for wide scope, given100a prediction of narrow scope), or non-brief andpredictable (e.g.
?the old men and the old women?for wide scope).
It might appear that there existsa fourth option: non-brief and unpredictable.
Butthis is ruled out by our technical sense of ?non-brief?
: as noted earlier, ?non-brief?
NPs do nothave the scope ambiguity.
Because of this ?miss-ing cell?, it will not be possible to analyse our datausing an ANOVA test, which would have automat-ically taken care of all possible interactions be-tween comprehensibility and brevity.
A numberof different tests will be used instead, with Bon-ferroni corrections where necessary.6 ConclusionHuman-based evaluation is gaining considerablepopularity in the NLG community.
Whereas eval-uation of GRE has mostly been speaker-oriented,the present paper has explored a plan for an ex-perimental hearer-oriented evaluation.
The mainconclusion is that hearer-based evaluation is diffi-cult because the quality of a generated expressioncan be measured in different ways, whose resultscannot be assumed to match.
One factor we havenot examined is the notion of fluency: it is possiblethat our algorithm will sometimes choose a wordorder (e.g.
?the women and old men?)
that is rela-tively infrequent, and therefore lacking in fluency.Such situations might lead to longer reading times.ReferencesA.
Belz and A. Kilgarriff.
2006.
Shared-task evalu-ations in HLT: Lessons for NLG.
In Proceedingsof the 4th International Conference on Natural Lan-guage Generation, pages 133?135.A.
Belz and E. Reiter.
2006.
Comparing automaticand human evaluation of NLG systems.
In Proceed-ings of the 11th Conference of the European Chap-ter of the Association for Computational Linguistics,pages 313?320, Trento, Italy, 3-7 April.F.
Chantree, B. Nuseibeh, A. de Roeck, and A. Willis.2006.
Identifying nocuous ambiguities in require-ments specifications.
In Proceedings of 14th IEEEInternational Requirements Engineering conference(RE?06), Minneapolis/St.
Paul, Minnesota, U.S.A.A.
Gatt and A. Belz.
2008.
Attribute selection for re-ferring expression generation: New algorithms andevaluation methods.
In Proceedings of the 5th Inter-national Conference on NLG.A.
Gatt.
2007.
Generating Coherent References toMultiple Entities.
Ph.D. thesis, University of Ab-erdeen, Aberdeen, Scotland.S.
Gupta and A. Stent.
2005.
Automatic evaluationof referring expression generation using corpora.
InProceedings of the Workshop on Using Corpora forNatural Language Generation, pages 1?6.I.
H. Khan, K. van Deemter, and G. Ritchie.
2008.Generation of referring expressions: Managingstructural ambiguities.
In Proceedings of the 22ndInternational Conference on Computational Lin-guistics (COLING-8), pages 433?440, Manchester.A.
Kilgarriff.
2003.
Thesauruses for natural languageprocessing.
In Proceedings of NLP-KE, pages 5?13,Beijing, China.E.
Krahmer and M. Theune.
2002.
Efficient context-sensitive generation of referring expressions.
InK.
van Deemter and R. Kibble, editors, InformationSharing: Reference and Presupposition in LanguageGeneration and Interpretation, CSLI Publications,pages 223?264.C.
Mellish and R. Dale.
1998.
Evaluation in thecontext of natural language generation.
ComputerSpeech and Language, 12(4):349?373.I.
Paraboni, J. Masthoff, and K. van Deemter.
2006.Overspecified reference in hierarchical domain:measuring the benefits for readers.
In Proceedingsof the Fourth International Conference on NaturalLanguage Generation(INLG), pages 55?62.C.
Paris, N. Colineau, and R. Wilkinson.
2006.
Eval-uations of NLG systems: Common corpus and tasksor common dimensions and metrics?
In Proceed-ings of the 4th International Conference on NaturalLanguage Generation, pages 127?129.A.
Siddharthan and A. Copestake.
2004.
Generatingreferring expressions in open domains.
In Proceed-ings of the 42nd Meeting of the Association for Com-putational Linguistics Annual Conference (ACL-04).M.
Stone and B. Webber.
1998.
Textual economythrough close coupling of syntax and semantics.
InProceedings of the Ninth International Workshop onNatural Language Generation, pages 178?187, NewBrunswick, New Jersey.K.
van Deemter, I. van der Sluis, and A. Gatt.
2006.Building a semantically transparent corpus for thegeneration of referring expressions.
In Proceedingsof the 4th International Conference on Natural Lan-guage Generation, pages 130?132.K.
van Deemter.
2004.
Towards a probabilistic versionof bidirectional OT syntax and semantics.
Journalof Semantics, 21(3):251?281.J.
Viethen and R. Dale.
2006.
Towards the evaluationof referring expression generation.
In Proceedingsof the 4th Australasian Language Technology Work-shop, pages 115?122, Sydney, Australia.101
