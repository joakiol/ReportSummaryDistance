Proceedings of the Ninth Workshop on Innovative Use of NLP for Building Educational Applications , pages 61?67,Baltimore, Maryland USA, June 26, 2014. c?2014 Association for Computational LinguisticsTowards Domain-Independent Assessment of ElementaryStudents?
Science Competency using Soft CardinalitySamuel P. Leeman-Munk, Angela Shelton, Eric N. Wiebe, James C. LesterNorth Carolina State UniversityRaleigh, North Carolina 27695{ spleeman, anshelto, wiebe, lester } @ ncsu.eduAbstractAutomated assessment of student learninghas become the subject of increasing atten-tion.
Students?
textual responses to shortanswer questions offer a rich source of datafor assessment.
However, automaticallyanalyzing textual constructed responsesposes significant computational challenges,exacerbated by the disfluencies that occurprominently in elementary students?
writ-ing.
With robust text analytics, there is thepotential to analyze a student?s text re-sponses and accurately predict his or herfuture success.
In this paper, we proposeapplying soft cardinality, a technique thathas shown success grading less disfluentstudent answers, on a corpus of fourth-grade responses to constructed responsequestions.
Based on decomposition ofwords into their constituent character sub-strings, soft cardinality?s evaluations of re-sponses written by fourth graders correlateswith summative analyses of their contentknowledge.1 IntroductionAs a tool for automated assessment, short answerquestions reveal cognitive processes and states instudents that are difficult to uncover in multiple-choice equivalents (Nicol, 2007).
Even when itseems that items could be designed to address thesame cognitive construct, success in devisingmultiple-choice and short answer items that be-have with psychometric equivalence has provento be limited (Kuechler & Simkin, 2010).
Be-cause standards-based STEM education in theUnited States explicitly promotes the develop-ment of writing skills for which constructed re-sponse items are ideally suited (NGSS LeadStates, 2013; Porter, McMaken, Hwang, & Yang,2011; Southavilay, Yacef, Reimann, & Calvo,2013), the prospect of designing text analyticstechniques for automatically assessing students?textual responses has become even more appeal-ing (Graesser, 2000; Jordan & Butcher, 2013;Labeke, Whitelock, & Field, 2013).An important family of short answer questionsis the constructed response question.
A con-structed response question is designed to elicit aresponse of no more than a few sentences andfeatures a relatively clear distinction betweenincorrect, partially correct, and correct answers.Ideally, a system designed for constructed re-sponse analysis (CRA) would be machine-learned from examples that include both gradedstudent answers and expert-constructed ?refer-ence?
answers (Dzikovska, Nielsen, & Brew,2012).The challenges of creating an accurate ma-chine-learning-based CRA system stem from thevariety of ways in which a student can express agiven concept.
In addition to lexical and syntac-tic variety, students often compose ill-formedtext replete with ungrammatical phrasings andmisspellings, which significantly complicateanalysis.
The task of automated grading also be-comes increasingly difficult as the material grad-ed comes from questions and domains more andmore distant from that of human graded respons-es on which the system is trained, leading to in-terest in domain-independent CRA systems de-signed to deal with this challenge (Dzikovska etal., 2013).In this paper we explore the applications of softcardinality (Jimenez, Becerra, & Gelbukh, 2013),an approach to constructed response analysis thathas shown prior success in domain-independentCRA.
We investigate whether soft cardinality isrobust to the disfluency common among elemen-tary students and whether its analyses of a stu-dent?s work as she progresses through a prob-lem-solving session can be used to roughly pre-dict the content knowledge she will have at theend.Because like other bag of words techniques,soft cardinality is independent of word order, it isrobust to grammatical disfluencies.
What distin-guishes soft cardinality, however, is its character-overlap technique, which allows it to evaluateword similarity across misspellings.
We evaluatesoft cardinality on a dataset of textual responsesto short-text science questions collected in a61study conducted at elementary schools in twostates.
Responders were in fourth grade and gen-erally aged between nine and ten.
We train oursystem on student responses to circuits questionsand test it on two domains in the physical scienc-es?circuits and magnetism.
The results indicatethat, soft cardinality shows promise as a first stepfor predicting a student?s future success withsimilar content even grading unseen domains inthe presence of high disfluency.This paper is structured as follows.
Section 2provides related work as a context for our re-search.
Section 3 introduces the corpus, collectedon tablet-based digital science notebook softwarefrom elementary students.
Section 4 describessoft cardinality and an evaluation thereof.
Sec-tion 6 discusses the findings and explores howsoft cardinality may serve as the basis for futureapproaches to real-time formative assessment.2 Related WorkShort answer assessment is a much-studied areathat has received increased attention in recentyears.
Disfluency and domain-independencehave been the beneficiaries of some of this atten-tion, but cutting edge systems seem to be de-signed first for correctly spelled in-domain text,and then have domain-independence and disflu-ency management added afterwards.For example, one system from EducationalTesting Services (ETS) uses an approach to do-main independence called ?domain adaptation?
(Heilman & Madnani, 2013).
Domain adaptationgenerates a copy of a given feature for gradinganswers to seen questions, answers to unseenquestions in seen domain, and answers to ques-tions in unseen domains, and each of these has aseparate weight.
An item represented in the train-ing data uses all three of these feature copies, andan item from another domain will only use thelatter, ?generic?
feature copy.Spell correction is also often treated as a sepa-rate issue, handled in the data-cleaning step of aCRA system.
The common approach at this stepis to mark words as misspelled if they do not ap-pear in a dictionary and replace them with theirmost likely alternative.
This technique only cor-rects non-word spelling errors (Leacock &Chodorow, 2003).
Another approach is to useSoundex hashes that translate every word into anormalized form based on its pronunciation (Ott,Ziai, Hahn, & Meurers, 2013).
This second ap-proach is generally featured alongside a moretraditional direct comparison.The primary limitation of CRA for elementaryschool education is that evaluations of state-of-the-art systems on raw elementary student re-sponse data are limited.
C-rater provides a smallevaluation on fourth-grade student math respons-es, but most evaluation is on seventh, eighth andeleventh grade students (Leacock & Chodorow,2003; Sukkarieh & Blackmore, 2009).
Further-more, the two datasets presented in SemEval?sshared task (Dzikovska et al., 2013) for testingand training featured relatively few spelling er-rors.
The BEETLE corpus was drawn from under-graduate volunteers with a relatively strongcommand of the English language, and the Sci-EntsBank corpus, which was drawn from 3-6thgraders, was originally intended for speech andas such was manually spell-corrected.
TheHewlett Foundation?s automated student assess-ment prize (ASAP) shared task for short answerscoring was drawn entirely from tenth grade stu-dents (Hewlett, 2012).3 CorpusWe have been exploring constructed responseassessment in the context of science educationfor upper elementary students with the LEONAR-DO CYBERPAD (Leeman-Munk, Wiebe, & Lester,2014).
Under development in our laboratory forthree years, the CYBERPAD is a digital sciencenotebook that runs on tablet and web based com-puting platforms.
The CYBERPAD integrates in-telligent tutoring systems technologies into a dig-ital science notebook that enables students tomodel science phenomena graphically.
With afocus on the physical and earth sciences, the LE-ONARDO PADMATE, a pedagogical agent, sup-ports students?
learning with real-time problem-solving advice.
The CYBERPAD?s curriculum isbased on that of the Full Option Science System(Foss Project, 2013).
As students progressthrough the curriculum, they utilize LEONARDO?svirtual notebook, complete virtual labs, and writeresponses to constructed response questions.
Todate, the LEONARDO CYBERPAD has been im-plemented in over 60 classrooms around theUnited States.The short answer and pre/post-test data used inthis investigation were gathered from fourthgrade students during implementations of TheCYBERPAD in public schools in California andNorth Carolina.
The data collection for eachclass took place over a minimum of five classperiods with students completing one or morenew investigations each day.
Students completed62investigations in one or both of two modules,?Energy and Circuits,?
and ?Magnetism.?
Mostquestions included ?starter text?
that studentswere expected to complete.
Students were able tomodify the starter text in any way including de-leting or replacing it entirely, although most stu-dents simply added to the starter text.
Exampleanswers can be found in a previous work on thesame dataset (Leeman-Munk et al., 2014).Two human graders scored students?
responsesfrom the circuits module on a science score ru-bric with three categories: incorrect, partiallycorrect, and correct.
The graders graded oneclass of data and then conferred on disagreeingresults.
They then graded other classes.
On asample of 10% of the responses of the classesgraded after conferring, graders achieved a Co-hen?s Kappa of 0.72.The graders dealt with considerable disfluencyin the student responses in the LEONARDO cor-pus.
An analysis of constructed responses in theEnergy and Circuits module reveals that 4.7% oftokens in all of student answers combined are notfound in a dictionary.
This number is higher inthe Magnetism module, 7.8%.
This is in contrastto other similar datasets, such as the BEETLEcorpus of undergraduate text answers to sciencequestions, which features a 0.8% rate of out-of-dictionary words (Dzikovska, Nielsen, & Brew,2012).
In each case, the numbers underestimateoverall spelling errors.
Misspellings such as ?bat-ter?
for ?battery?, are not counted as missing in adictionary test.
These real-word spelling errorsnevertheless misrepresent a student?s meaningand complicate analysis.
We describe how softcardinality addresses these issues in Section 4.4 Methodology and EvaluationSoft cardinality (Jimenez, Becerra, & Gelbukh,2013) uses decompositions of words into charac-ter sequences, known as q-grams, to gauge simi-larity between two words.
We use it here tobridge the gap between misspellings of the sameword.
Considering ?dcells?
in an example an-swer, ?mor dcells,?
and ?D-cells?
in the refer-ence answer, we can find overlaps in ?ce,?
?el,??ll,?
?ls,?
?ell,?
?lls,?
and so on up to and includ-ing ?cells.?
This technique functions equally wellfor real-word spelling errors such as if the stu-dent had forgotten the ?d?
and typed only?cells.?
Such overlaps signify a close match forboth of these words.
We evaluated the soft cardi-nality implementation of a generic short answergrading framework that we developed,WRITEEVAL, based on an answer grading systemdescribed in an earlier work (Leeman-Munk etal., 2014).
We used 100-fold cross-validation onthe ?Energy and Circuits?
module.
We compareWRITEEVAL using soft cardinality to the majorityclass baseline and to WRITEEVAL using Prece-dent Feature Collection (PFC), a latent semanticanalysis technique that performs competitivelywith the second highest-scoring system inSemeval Task 7 on unseen answers on the Sci-EntsBank corpus (Dzikovska et al., 2013).
Usinga Kruskal-Wallis test over one hundred folds,both systems significantly outperform the base-line (p<.001), which achieved an accuracy scoreof .61.
We could not evaluate the scores directlyon the Magnetism dataset as we did not have anyhuman-graded gold standard for comparison.To evaluate soft cardinality?s robustness to dis-fluency, we created a duplicate of the Energy andCircuits dataset and manually spell-corrected it.Table 1 and Figures 1 and 2 show our results.Using the Kruskal-Wallis Test, on the uncorrect-ed data PFC?s accuracy suffered with marginalsignificance (p = .054) while macro-averagedprecision and recall both suffered significantly (p< .01).
Soft cardinality suffered much less, with amarginally significant decrease in performance(p=.075) only in recall.
The decreases in accura-cy and precision had p=.88 and p=.25 respective-ly.To determine the usefulness of automatic grad-ing of science content in predicting the overalltrajectory of a student?s performance, we com-puted a running average of the grades given bysoft cardinality (converted to ?1?, ?2?, and ?3?
forincorrect, partially correct, correct) on students?answers as they progressed through the Energyand Circuits module and the Magnetism module.Because we would intend to be able to use thistechnique in a classroom on entirely new ques-tions and student answers, we use running aver-age instead of a regression, which would requireprior data on the questions to determine theweights.Students completed a multiple-choice test be-fore and after their interaction with the CYBER-PAD.
The Energy and Circuits module and theMagnetism module each had different tests ?there were ten questions on the Energy and Cir-cuits test and twenty on the Magnetism test.
Wecalculated the correlation of our running averageof formative assessments against the student?sscore on the final test.A critical assumption underlying the runningaverage is that students answered each question63in order.
Although WRITEEVAL does not preventstudents from answering questions out of order,it is organized to strongly encourage linear pro-gression.We excluded empty responses from the runningaverage because we did not want an artificialboost from simply noting what questions stu-dents did and did not answer.
Data from studentswho did not take the pre or post-test was exclud-ed, and students missing responses to more thantwenty out of twenty-nine questions in Mag-netism or fifteen out of twenty questions in En-ergy and Circuits were excluded from considera-tion.
After cleaning, our results include 85 stu-dents in Energy and Circuits and 61 in Mag-netism.Table 1.
Accuracy and Macro-Averaged Preci-sion and Recall for Soft-Cardinality and PFC onspell-corrected and uncorrected versions of theLEONARDO Energy and Circuits module.
*marginally significant decrease from spell-checked**significant decrease from spell-checkedFigure 1 depicts the correlation between therunning average of automatic scoring byWRITEEVAL soft cardinality, PFC, and humanscores with post-test score on the responses inthe Energy and Circuits module.
When spell-corrected, the correlation, as shown in Figure 2,surprisingly becomes worse.
We discuss a pos-sible reason for this in the discussion section.Figure 3 shows correlation of the running aver-age of Magnetism?s automatic scores with post-test.
For soft cardinality, significant correlationstarts five questions in and stays for the rest ofthe 29.
As it relies heavily on relevant trainingdata, PFC is less stable and does not achievenearly as high a correlation.5 DiscussionThe evaluation suggests that a relatively simpletechnique such as soft cardinality, despite per-forming less well than a domain specific tech-nique in the presence of relevant training data, ismore robust to spelling errors and can be farmore effective at grading questions and domainsnot present in the training data.Figure 1.
Correlation of grading systems onEnergy and Circuits with post-test score.
Dark-colored points indicate significant correlation(p<.05)Figure 2.
Correlation of grading systems onspell-corrected Energy and Circuits with post-test score.
Dark-colored points indicatesignificant correlation (p<.05)Figure 3.
Correlation of the Running Average ofWRITEEVAL with soft cardinality with post-testScores on the Magnetism module of the LEO-NARDO corpus.
Dark-colored points indicatesignificant correlation (p<.05)Soft cardinality is representative of the poten-tial of domain independent, disfluency-robustCRA systems.The improvement against the gold standard onspell-corrected data but loss of correlationagainst the post-test scores suggests that poorspelling is a predictor of poor post-test00.20.40.61 3 5 7 9 11 13 15 17 19CorrelationQuestions GradedHuman Soft CardinalityWriteEval PFC-0.200.20.40.61 3 5 7 9 11 13 15 17 19CorrelationQuestions GradedHuman Soft CardinalityWriteEval PFC0.10.30.55 7 9 11 13 15 17 19 21 23 25 27 29CorrelationQuestions GradedSoft Cardinality WriteEval PFCSp.Cr.
System Accuracy Precision RecallYes SoftCr .68 .55 .54No SoftCr .68 .52 .50*Yes PFC .78 .61 .58No PFC .74* .54** .52**64knowledge at the end of a task.
This could bebecause the students were less able to learn thematerial due to their poor language skills, theywere less able to complete the test effectivelydespite knowing the material again due to poorlanguage skills, or it could be a latent factor thataffects both the students use of language andtheir eventual circuits knowledge such as en-gagement.
This result shows the challenge ofseparating different skills in evaluating students.The significance of soft cardinality?s correla-tion over the running average for all but theeighth question as well as the generally high sig-nificant correlation achieved in the magnetismevaluation indicates the predictive potential ofsoft cardinality.
Soft cardinality?s performance inMagnetism suggests that with only a relativelylimited breadth of training examples it can effec-tively evaluate answers to questions in some un-seen domains.
It is important to note that Energyand Circuits and Magnetism are both subjects inthe physical sciences, and the questions and ref-erence answers themselves were authored by thesame individuals.
As such this result should notbe overstated, but is still a promising first steptowards the goal of domain-independence inconstructed response analysis.6 ConclusionThis paper presents a novel application of thesoft cardinality text analytics method to supportassessment of highly disfluent elementary schooltext.
Using q-gram overlap to evaluate word sim-ilarity across nonstandard spellings, soft cardi-nality was evaluated on highly disfluent con-structed response texts composed by fourth gradestudents interacting with a tablet-based digitalscience notebook.
The evaluation included an in-domain training corpus and another out-of-domain corpus.
The results of the evaluationsuggest that soft cardinality generates assess-ments that are predictive of students?
post-testperformance even in highly disfluent out-of-domain corpora.
It offers the potential to produceassessments in real-time that may serve as earlywarning indicators to help teachers support stu-dent learning.Soft cardinality?s current performance levelssuggest several promising directions for futurework.
First, it will be important to develop tech-niques to deal with widely varying student re-sponses without relying directly on training data.These techniques will take inspiration in partfrom bag-of-words techniques such as soft cardi-nality and Precedent Feature Collection, but willthemselves likely take word order into account asthere is a sizeable subset of answers whosemeaning is dependent on word order.
The use ofdistributional semantics will also be of help inresolving similarities between different words.Secondly, work should be done to consider an-swers in more detail than simple assessment ofcorrectness.
More detailed rubrics such as Task7?s 5-way rubric (Dzikovska et al., 2013) wouldallow for more detailed feedback from tutors.Further, detailed analysis of individual under-standings and misconceptions within answerswould be even more helpful, and will be the fo-cus of future work.
Third, it will be instructive toincorporate the WRITEEVAL framework into theLEONARDO CYBERPAD digital science notebookto investigate techniques for classroom-basedformative assessment that artfully utilize bothintelligent support by the PADMATE onboardintelligent tutor and personalized support by theteacher.
Finally, it will be important to to inves-tigate additional techniques to evaluate studentanswers more accurately using less training datafrom more distant domains.Reliable analysis of constructed response itemsnot only provides additional summative analysisof writing ability in science, but also gives theteacher a powerful formative assessment tool thatcan be used to guide instructional strategies ateither the individual student or whole class level.Given that time for science instruction is limitedat the elementary level, the use of real-time as-sessment to address student misconceptions ormissing knowledge immediately can be an inval-uable classroom tool.7 AcknowledgementsThe authors wish to thank our colleagues on theLEONARDO team for their contributions to thedesign, development, and classroom implementa-tions of LEONARDO: Courtney Behrle, MikeCarter, Bradford Mott, Peter Andrew Smith, andRobert Taylor.
This material is based upon worksupported by the National Science Foundationunder Grant No.
DRL1020229.
Any opinions,findings, and conclusions or recommendationsexpressed in this material are those of the authorsand do not necessarily reflect the views of theNational Science Foundation.65ReferencesDzikovska, M., Brew, C., Clark, P., Nielsen, R. D.,Leacock, C., Mcgraw-hill, C. T. B., &Bentivogli, L. (2013).
SemEval-2013 Task 7?
:The joint student response analysis and 8threcognizing textual entailment challenge.
InSecond Joint Conference on Lexical andComputational Semantics (*SEM), Volume 2:Proceedings of the Seventh InternationalWorkshop on Semantic Evaluation (SemEval2013) (Vol.
2, pp.
263?274).Dzikovska, M., Nielsen, R., & Brew, C. (2012).Towards effective tutorial feedback forexplanation questions: A dataset and baselines.In Proceedings of the 2012 Conference of theNorth American Chapter of the Association forComputational Linguistics: Human LanguageTechnologies (pp.
200?210).
Montreal, Canada.Retrieved fromhttp://dl.acm.org/citation.cfm?id=2382057Foss Project.
(2013).
Welcome to FossWeb.
RetrievedOctober 20, 2013, fromhttp://www.fossweb.com/Graesser, A.
(2000).
Using latent semantic analysis toevaluate the contributions of students inAutoTutor.
Interactive Learning Environments,8(2), 1?33.
Retrieved fromhttp://www.tandfonline.com/doi/full/10.1076/1049-4820(200008)8%3A2%3B1-B%3BFT129Heilman, M., & Madnani, N. (2013).
ETS?
: Domainadaptation and stacking for short answerscoring.
In Second Joint Conference on Lexicaland Computational Semantics (*SEM), Volume2: Proceedings of the Seventh InternationalWorkshop on Semantic Evaluation (SemEval2013) (Vol.
1, pp.
96?102).Hewlett, W. (2012).
The Hewlett Foundation: Shortanswer scoring.
Retrieved March 16, 2014,from https://www.kaggle.com/c/asap-sas/data?Data_Set_Descriptions.zipJimenez, S., Becerra, C., & Gelbukh, A.
(2013).SOFTCARDINALITY: hierarchical textoverlap for student response analysis.
In SecondJoint Conference on Lexical and ComputationalSemantics (*SEM), Volume 2: Proceedings ofthe Seventh International Workshop onSemantic Evaluation (SemEval 2013) (Vol.
2,pp.
280?284).
Retrieved fromhttp://www.gelbukh.com/CV/Publications/2013/SOFTCARDINALITY Hierarchical TextOverlap for Student Response Analysis.pdfJordan, S., & Butcher, P. (2013).
Does the Sun orbitthe Earth??
Challenges in using short free-textcomputer-marked questions .
In Proceedings ofHEA STEM Annual Learning and TeachingConference 2013: Where Practice andPedagogy Meet.
Birmingham, UK.Kuechler, W., & Simkin, M. (2010).
Why isperformance on multiple-choice tests andconstructed-response tests not more closelyrelated?
Theory and an empirical test.
DecisionSciences Journal of Innovative Education, 8(1),55?73.
Retrieved fromhttp://onlinelibrary.wiley.com/doi/10.1111/j.1540-4609.2009.00243.x/fullLabeke, N. Van, Whitelock, D., & Field, D. (2013).OpenEssayist: extractive summarisation andformative assessment of free-text essays.
InFirst International Workshop on Discourse-Centric Learning Analytics.
Leuven, Belgium.Retrieved from http://oro.open.ac.uk/37548/Leacock, C., & Chodorow, M. (2003).
C-rater:Automated scoring of short-answer questions.Computers and the Humanities, 37(4), 389?405.Retrieved fromhttp://link.springer.com/article/10.1023/A%3A1025779619903Leeman-Munk, S. P., Wiebe, E. N., & Lester, J. C.(2014).
Assessing Elementary Students?
ScienceCompetency with Text Analytics.
InProceedings of the Fourth InternationalConference on Learning Analytics &Knowledge.
Indianapolis, Indiana.NGSS Lead States.
(2013).
Next Generation ScienceStandards: For States, By States.
WashingtonDC: National Academic Press.Nicol, D. (2007).
E-assessment by design: Usingmultiple-choice tests to good effect.
Journal ofFurther and Higher Education, 31(1), 53?64.Retrieved fromhttp://www.tandfonline.com/doi/abs/10.1080/03098770601167922Ott, N., Ziai, R., Hahn, M., & Meurers, D.
(2013).CoMeT?
: Integrating different levels oflinguistic modeling for meaning assessment.
InSecond Joint Conference on Lexical andComputational Semantics (*SEM), Volume 2:Proceedings of the Seventh InternationalWorkshop on Semantic Evaluation (SemEval2013) (Vol.
2, pp.
608?616).Porter, A., McMaken, J., Hwang, J., & Yang, R.(2011).
Common core standards the new US66intended curriculum.
Educational Researcher,40(3), 103?116.
Retrieved fromhttp://edr.sagepub.com/content/40/3/103.shortSouthavilay, V., Yacef, K., Reimann, P., & Calvo, R.A. (2013).
Analysis of collaborative writingprocesses using revision maps and probabilistictopic models.
In Proceedings of the ThirdInternational Conference on Learning Analyticsand Knowledge - LAK ?13 (pp.
38?47).
NewYork, New York, USA: ACM Press.doi:10.1145/2460296.2460307Sukkarieh, J., & Blackmore, J.
(2009).
C-rater:Automatic content scoring for short constructedresponses.
Proceedings of the 22ndInternational FLAIRS Conference, 290?295.Retrieved fromhttp://www.aaai.org/ocs/index.php/FLAIRS/2009/paper/download/122/30267
