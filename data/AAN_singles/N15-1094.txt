Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 932?942,Denver, Colorado, May 31 ?
June 5, 2015.c?2015 Association for Computational LinguisticsPenalized Expectation Propagation for Graphical Models over Strings?Ryan Cotterell and Jason EisnerDepartment of Computer Science, Johns Hopkins University{ryan.cotterell,jason}@cs.jhu.eduAbstractWe present penalized expectation propaga-tion (PEP), a novel algorithm for approximateinference in graphical models.
Expectationpropagation is a variant of loopy belief prop-agation that keeps messages tractable by pro-jecting them back into a given family of func-tions.
Our extension, PEP, uses a structured-sparsity penalty to encourage simple mes-sages, thus balancing speed and accuracy.
Wespecifically show how to instantiate PEP in thecase of string-valued random variables, wherewe adaptively approximate finite-state distri-butions by variable-order n-gram models.
Onphonological inference problems, we obtainsubstantial speedup over previous related al-gorithms with no significant loss in accuracy.1 IntroductionGraphical models are well-suited to reasoning aboutlinguistic structure in the presence of uncertainty.Such models typically use discrete random vari-ables, where each variable ranges over a finite setof values such as words or tags.
But a variable canalso be allowed to range over an infinite space of dis-crete structures?in particular, the set of all strings,a case first explored by Bouchard-C?t?
et al (2007).This setting arises because human languagesmake use of many word forms.
These strings aresystematically related in their spellings due to lin-guistic processes such as morphology, phonology,abbreviation, copying error and historical change.To analyze or predict novel strings, we can modelthe joint distribution of many related strings at once.Under a graphical model, the joint probability of anassignment tuple is modeled as a product of poten-tials on sub-tuples, each of which is usually modeledin turn by a weighted finite-state machine.In general, we wish to infer the values of un-known strings in the graphical model.
Deterministic?This material is based upon work supported by the Na-tional Science Foundation under Grant No.
1423276, and by aFulbright Research Scholarship to the first author.approaches to this problem have focused on beliefpropagation (BP), a message-passing algorithm thatis exact on acyclic graphical models and approxi-mate on cyclic (?loopy?)
ones (Murphy et al, 1999).But in both cases, further heuristic approximationsof the BP messages are generally used for speed.In this paper, we develop a more principled andflexible way to approximate the messages, usingvariable-order n-gram models.We first develop a version of expectation propa-gation (EP) for string-valued variables.
EP offers aprincipled way to approximate BP messages by dis-tributions from a fixed family?e.g., by trigram mod-els.
Each message update is found by minimizing acertain KL-divergence (Minka, 2001a).Second, we generalize to variable-order models.To do this, we augment EP?s minimization prob-lem with a novel penalty term that keeps the num-ber of n-grams finite.
In general, we advocate pe-nalizing more ?complex?
messages (in our setting,large finite-state acceptors).
Complex messages areslower to construct, and slower to use in later steps.Our penalty term is formally similar to regulariz-ers that encourage structured sparsity (Bach et al,2011; Martins et al, 2011).
Like a regularizer, itlets us use a more expressive family of distribu-tions, secure in the knowledge that we will use onlyas many of the parameters as we really need for a?pretty good?
fit.
But why avoid using more param-eters?
Regularization seeks better generalization bynot overfitting the model to the data.
By contrast,we already have a model and are merely doing in-ference.
We seek better runtime by not over-fussingabout capturing the model?s marginal distributions.Our ?penalized EP?
(PEP) inference strategy isapplicable to any graphical model with complexmessages.
In this paper, we focus on strings, andshow how PEP speeds up inference on the computa-tional phonology model of Cotterell et al (2015).We provide further details, tutorial material, andresults in the appendices (supplementary material).9322 BackgroundGraphical models over strings are in fairly broad use.Linear-chain graphical models are equivalent to cas-cades of finite-state transducers, which have longbeen used to model stepwise derivational processessuch as speech production (Pereira and Riley, 1997)and transliteration (Knight and Graehl, 1998).
Tree-shaped graphical models have been used to modelthe evolution and speciation of word forms, in orderto reconstruct ancient languages (Bouchard-C?t?
etal., 2007; Bouchard-C?t?
et al, 2008) and discovercognates in related languages (Hall and Klein, 2010;Hall and Klein, 2011).
Cyclic graphical modelshave been used to model morphological paradigms(Dreyer and Eisner, 2009; Dreyer and Eisner, 2011)and to reconstruct phonological underlying forms(Cotterell et al, 2015).
All of these graphical mod-els, except Dreyer?s, happen to be directed ones.And all of these papers, except Bouchard-C?t?
?s, usedeterministic inference methods?based on BP.2.1 Graphical models over stringsA directed or undirected graphical model describesa joint probability distribution over a set of randomvariables.
To perform inference given a setting ofthe model parameters and observations of some vari-ables, it is convenient to construct a factor graph(Kschischang et al, 2001).
A factor graph is a fi-nite bipartite graph whose vertices are the randomvariables {V1, V2, .
.
.}
and the factors {F1, F2, .
.
.
}.Each factor F is a function of the variables that it isconnected to; it returns a non-negative real numberthat depends on the values of those variables.
We de-fine our factor graph so that the posterior probabilityp(V1= v1, V2= v2, .
.
.
| observations), as definedby the original graphical model, can be computed asproportional to the product of the numbers returnedby all the factors when V1= v1, V2= v2, .
.
..In a graphical model over strings, each randomvariable V is permitted to range over the strings ?
?where ?
is a fixed alphabet.
As in previous work, wewill assume that each factor F connected to d vari-ables is a d-way rational relation, i.e., a function thatcan be computed by a d-tape weighted finite-stateacceptor (Elgot and Mezei, 1965; Mohri et al, 2002;Kempe et al, 2004).
The weights fall in the semir-ing (R,+,?
): F ?s return value is the total weight ofall paths that accept the d-tuple of strings, where apath?s weight is the product of its arcs?
weights.
Soour model marginalizes over possible paths in F .2.2 Inference by (loopy) belief propagationInference seeks the posterior marginal probabilitiesp(Vi= v | observations), for each i. BP is an it-erative procedure whose ?normalized beliefs?
con-verge to exactly these marginals if the factor graph isacyclic (Pearl, 1988).
In the cyclic case, the normal-ized beliefs still typically converge and can be usedas approximate marginals (Murphy et al, 1999).A full presentation of BP for graphical modelsover strings can be found in Dreyer and Eisner(2009).
We largely follow their notation.
N (X) rep-resents the set of neighbors of X in the factor graph.For each edge in the factor graph, between a fac-tor F and a variable V , BP maintains two messages,?V?Fand ?F?V.
Each of these is a function overthe possible values v of variable V , mapping each vto a non-negative score.
BP also maintains anothersuch function, the belief bV, for each variable V .In general, each message or belief should be re-garded as giving only relative scores for the differ-ent v. Rescaling it by a positive constant would onlyresult in rescaling other messages and beliefs, whichwould not change the final normalized beliefs.
Thenormalized belief is the probability distribution?bVsuch that each?bV(v) is proportional to bV(v).The basic BP algorithm is just to repeatedly selectand update a function until convergence.
The rulesfor updating ?V?F, ?F?V, and bV, given the set of?neighboring?
messages in each case, can be foundas equations (2)?
(4) of Dreyer and Eisner (2009).
(We will give the EP variants in section 4.
)Importantly, that paper shows that for graphicalmodels over strings, each BP update can be imple-mented via standard finite-state operations of com-position, projection, and intersection.
Each messageor belief is represented as a weighted finite-state ac-ceptor (WFSA) that scores all strings v ?
?
?.2.3 The need for approximationBP is generally only used directly for short cascadesof finite-state transducers (Pereira and Riley, 1997;Knight and Graehl, 1998).
Alas, in other graphi-cal models over strings, the BP messages?whichare acceptors?become too large to be practical.933In cyclic factor graphs, where exact inference forstrings can be undecidable, the WFSAs can becomeunboundedly large as they are iteratively updatedaround a cycle (Dreyer and Eisner, 2009).
Even inan acyclic graph (where BP is exact), the finite-stateoperations quickly lead to large WFSAs.
Each inter-section or composition is a Cartesian product con-struction, whose output?s size (number of automatonstates) may be as large as the product of its inputs?sizes.
Combining many of these operations leads toexponential blowup.3 Variational Approximation of WFSAsTo address this difficulty through EP (section 4), wewill need the ability to approximate any probabilitydistribution p that is given by a WFSA, by choosinga ?simple?
distribution from a family Q.Take Q to be a family of log-linear distributionsq?
(v)def= exp(?
?
f(v)) /Z?
(?v ?
??)
(1)where ?
is a weight vector, f(v) is a feature vectorthat describes v, and Z?def=?v???exp(?
?
f(v))so that?vq?
(v) = 1.
Notice that the featurizationfunction f specifies the family Q, while the varia-tional parameters ?
specify a particular q ?
Q.1We project p into Q via inclusive KL divergence:?
= argmin?
D(p || q?)
(2)Now q?
approximates p, and has support everywherethat p does.
We can get finer-grained approxima-tions by expanding f to extract more features: how-ever, ?
is then larger to store and slower to find.3.1 Finding ?Solving (2) reduces to maximizing ?H(p, q?)
=Ev?p[log q?
(v)], the log-likelihood of q?
on an ?in-finite sample?
from p. This is similar to fitting alog-linear model to data (without any regularization:we want q?
to fit p as well as possible).
This objec-tive is concave and can be maximized by followingits gradient Ev?p[f(v)] ?
Ev?q?
[f(v)].
Often it isalso possible to optimize ?
in closed form, as we will1To be precise, we take Q = {q?
: Z?
is finite}.
For exam-ple, ?
= 0 is excluded because then Z?
=?v??
?exp 0 =?.
Aside from this restriction, ?
may be any vector overR?{??}.
We allow??
since it is a feature?s optimal weightif p(v) = 0 for all v with that feature: then q?
(v) = 0 for suchstrings as well.
(Provided that f(v) ?
0, as we will ensure.
)see later.
Either way, the optimal q?
matches p?s ex-pected feature vector: Ev?q?
[f(v)] = Ev?p[f(v)].This inspired the name ?expectation propagation.
?3.2 Working with ?Although p is defined by an arbitrary WFSA, we canrepresent q?
quite simply by just storing the parame-ter vector ?.
We will later take sums of such vectorsto construct product distributions: observe that un-der (1), q?1+?2(v) is proportional to q?1(v) ?
q?2(v).We will also need to construct WFSA versions ofthese distributions q?
?
Q, and of other log-linearfunctions (messages) that may not be normalizableinto distributions.
Let ENCODE(?)
denote a WFSAthat accepts each v ?
?
?with weight exp(?
?f(v)).3.3 Substring featuresTo obtain our family Q, we must design f .
Ourstrategy is to choose a set of ?interesting?
substringsW .
For each w ?
W , define a feature function?How many times does w appear as a substring ofv??
Thus, f(v) is simply a vector of counts (non-negative integers), indexed by the substrings inW .A natural choice ofW is the set of all n-grams forfixed n. In this case, Q turns out to be equivalent tothe family of n-gram language models.2Already inprevious work (?variational decoding?
), we used (2)with this family to approximate WFSAs or weightedhypergraphs that arose at runtime (Li et al, 2009).Yet a fixed n is not ideal.
If W is the set of bi-grams, one might do well to add the trigram the?perhaps because the is ?really?
a bigram (countingthe digraph th as a single consonant), or because thebigram model fails to capture how common the isunder p. Adding the toW ensures that q?will nowmatch p?s expected count for this trigram.
Doing thisshould not require adding all |?|3trigrams.By including strings of mixed lengths inW we getvariable-order Markov models (Ron et al, 1996).3.4 Arbitrary FSA-based featuresMore generally, letA be any unambiguous and com-plete finite-state acceptor: that is, any v ?
?
?hasexactly one accepting path inA.
For each arc or finalstate a in A, we can define a feature function ?How2Provided that we include special n-grams that match at theboundaries of v. See Appendix B.2 for details.934many times is a used when A accepts v??
Thus,f(v) is again a vector of non-negative counts.Section 6 gives algorithms for this general set-ting.
We implement the previous section as a spe-cial case, constructing A so that its arcs essentiallycorrespond to the substrings in W .
This encodes avariable-order Markov model as an FSA similarly to(Allauzen et al, 2003); see Appendix B.4 for details.In this general setting, ENCODE(?)
just returns aweighted version ofAwhere each arc or final state ahas weight exp ?ain the (+,?)
semiring.
Thus, thisWFSA accepts each v with weight exp(?
?
f(v)).3.5 Adaptive featurizationHow do we chooseW (orA)?
ExpandingW will al-low better approximations to p?but at greater com-putational cost.
We would like W to include justthe substrings needed to approximate a given p well.For instance, if p is concentrated on a few high-probability strings, then a good W might containthose full strings (with positive weights), plus someshorter substrings that help model the rest of p.To selectW at runtime in a way that adapts to p,let us say that ?
is actually an infinite vector withweights for all possible substrings, and defineW ={w ?
??
: ?w6= 0}.
Provided that W stays finite,we can store ?
as a map from substrings to nonzeroweights.
We keepW small by replacing (2) with?
= argmin?
D(p || q?)
+ ?
?
?(?)
(3)where ?(?)
measures the complexity of this W orthe corresponding A.
Small WFSAs ensure fastfinite-state operations, so ideally, ?(?)
should mea-sure the size of ENCODE(?).
Choosing ?
> 0 to belarge will then emphasize speed over accuracy.Section 6.1 will extend section 6?s algorithmsto approximately minimize the new objective (3).Formally this objective resembles regularized log-likelihood.
However, ?(?)
is not a regularizer?as section 1 noted, we have no statistical reason toavoid ?overfitting?
p?, only a computational one.4 Expectation PropagationRecall from section 2.2 that for each variable V , theBP algorithm maintains several nonnegative func-tions that score V ?s possible values v: the messages?V?Fand ?F?V(?F ?
N (V )), and the belief bV.FeaturecatcaatctWeight1.04.83.86.89.91-.96F1F2F3V3r in gue ?
ees e haV2V4V1Figure 1: Information flowing toward V2in EP (reverseflow not shown).
The factors work with purple ?
mes-sages represented by WFSAs, while the variables workwith green ?
messages represented by log-linear weightvectors.
The green table shows a ?
message: a sparseweight vector that puts high weight on the string cat.EP is a variant in which all of these are forcedto be log-linear functions from the same family,namely exp(?
?fV(v)).
Here fVis the featurizationfunction we?ve chosen for variable V .3We can rep-resent these functions by their parameter vectors?let us call those ?V?F, ?F?V, and ?Vrespectively.4.1 Passing messages through variablesWhat happens to BP?s update equations in this set-ting?
According to BP, the belief bVis the pointwiseproduct of all ?incoming?
messages to V .
But as wesaw in section 3.2, pointwise products are far eas-ier in EP?s restricted setting!
Instead of intersectingseveral WFSAs, we can simply add several vectors:?V=?F?
?N (V )?F?
?V(4)Similarly, the ?outgoing?
message from V to factorF is the pointwise product of all ?incoming?
mes-sages except the one from F .
This message ?V?Fcan be computed as ?V?
?F?V, which adjusts (4).4We never store this but just compute it on demand.3A single graphical model might mix categorical variables,continuous variables, orthographic strings over (say) the Romanalphabet, and phonological strings over the International Pho-netic Alphabet.
These different data types certainly require dif-ferent featurization functions.
Moreover, even when two vari-ables have the same type, we could choose to approximate theirmarginals differently, e.g., with bigram vs. trigram features.4If features can have ??
weight (footnote 1), this trickmight need to subtract ??
from ??
(the log-space versionof 0/0).
That gives an undefined result, but it turns out that anyresult will do?it makes no difference to the subsequent beliefs.9354.2 Passing messages through factorsOur factors are weighted finite-state machines, sotheir messages still require finite-state computations,as shown by the purple material in Figure 1.
Thesecomputations are just as in BP.
Concretely, let F bea factor of degree d, given as a d-tape machine.
Wecan compute a belief at this factor by joining F withd WFSAs that represent its d incoming messages,namely ENCODE(?V?
?F) for V??
N (F ).
Thisgives a new d-tape machine, bF.
We then obtaineach outgoing message ?F?Vby projecting bFontoits V tape, but removing (dividing out) the weightsthat were contributed (multiplied in) by ?V?F.54.3 Getting from factors back to variablesFinally, we reach the only tricky step.
Each resulting?F?Vis a possibly large WFSA, so we must force itback into our log-linear family to get an updated ap-proximation ?F?V.
One cannot directly employ themethods of section 3, because KL divergence is onlydefined between probability distributions.
(?F?Vmight not be normalizable into a distribution, nor isits best approximation necessarily normalizable.
)The EP trick is to use section 3 to instead approx-imate the belief at V , which is a distribution, andthen reconstruct the approximate message to V thatwould have produced this approximated belief.
The?unapproximated belief?
p?Vresembles (4): it multi-plies the unapproximated message ?F?Vby the cur-rent values of all other messages ?F??V.
We knowthe product of those other messages, ?V?F, sop?V:= ?F?V?V?F(5)where the pointwise product  is carried out byWFSA intersection and ?V?Fdef= ENCODE(?V?F).We now apply section 3 to choose ?Vsuch thatq?Vis a good approximation of the WFSA p?V.
Fi-nally, to preserve (4) as an invariant, we reconstruct?F?V:= ?V?
?V?F(6)5This is equivalent to computing each ?F?Vby ?general-ized composition?
of F with the d ?
1 messages to F fromits other neighbors V?.
The operations of join and generalizedcomposition were defined by Kempe et al (2004).In the simple case d = 2, F is just a weighted finite-statetransducer mapping V?to V , and computing ?F?Vreduces tocomposing ENCODE(?V?
?F) with F and projecting the resultonto the output tape.
In fact, one can assume WLOG that d ?
2,enabling the use of popular finite-state toolkits that handle atmost 2-tape machines.
See Appendix B.10 for the construction.In short, EP combines ?F?Vwith ?V?F, thenapproximates the result p?Vby ?Vbefore removing?V?Fagain.
Thus EP is approximating ?F?Vby?F?V:= argmin?D(?F?V?V?F?
??
?= p?V|| q?
?V?F?
??
?=?V)(7)in a way that updates not only ?F?Vbut also ?V.Wisely, this objective focuses on approximatingthe message?s scores for the plausible values v.Some values v may have p?V(v) ?
0, perhaps be-cause another incoming message ?F?
?Vrules themout.
It does not much harm the objective (7) if these?F?V(v) are poorly approximated by q?F?V(v),since the overall belief is still roughly correct.Our penalized EP simply adds ?
?
?(?)
into (7).4.4 The EP algorithm: Putting it all togetherTo run EP (or PEP), initialize all ?Vand ?F?Vto 0,and then loop repeatedly over the nodes of the factorgraph.
When visiting a factorF , ENCODE its incom-ing messages ?V?F(computed on demand) as WF-SAs, construct a belief bF, and update the outgoingWFSA messages ?F?V.
When visiting a variableV , iterate K ?
1 times over its incoming WFSAmessages: for each incoming ?F?V, compute theunapproximated belief p?Vvia (5), then update ?Vtoapproximate p?V, then update ?F?Vvia (6).For possibly faster convergence, one can alternate?forward?
and ?backward?
sweeps.
Visit the factorgraph?s nodes in a fixed order (given by an approx-imate topological sort).
At a factor, update the out-going WFSA messages to later variables only.
Ata variable, approximate only those incoming WFSAmessages from earlier factors (all the outgoing mes-sages ?V?Fwill be recomputed on demand).
Notethat both cases examine all incoming messages.
Af-ter each sweep, reverse the node ordering and repeat.If gradient ascent is used to find the ?Vthat ap-proximates p?V, it is wasteful to optimize to conver-gence.
After all, the optimization problem will keepchanging as the messages change.
Our implementa-tion improves ?Vby only a single gradient step oneach visit to V , since V will be visited repeatedly.See Appendix A for an alternative view of EP.5 Related Approximation MethodsWe have presented EP as a method for simplifying avariable?s incoming messages during BP.
The vari-936able?s outgoing messages are pointwise products ofthe incoming ones, so they become simple too.
Pastwork has used approximations with a similar flavor.Hall and Klein (2011) heuristically predeterminea short, fixed list of plausible values for V that wereobserved elsewhere in their dataset.
This list is anal-ogous to our ?V.
After updating ?F?V, they force?F?V(v) to 0 for all v outside the list, yielding afinite message that is analogous to our ?F?V.Our own past papers are similar, except theyadaptively set the ?plausible values?
list to?F?
?N (V )k-BEST(?F??V).
These strings are fa-vored by at least one of the current messages to V(Dreyer and Eisner, 2009; Dreyer and Eisner, 2011;Cotterell et al, 2015).
Thus, simplifying one of V ?sincoming messages considers all of them, as in EP.The above methods prune each message, so mayprune correct values.
Hall and Klein (2010) avoidthis: they fit a full bigram model by inclusive KLdivergence, which refuses to prune any values (seesection 3).
Specifically, they minimized D(?F?V?
|| q?
?
), where ?
was a simple fixed function (a0-gram model) included so that they were workingwith distributions (see section 4.3).
This is very sim-ilar to our (7).
Indeed, Hall and Klein (2010) foundtheir procedure ?reminiscent of EP,?
hinting that ?was a surrogate for a real ?V?Fterm.
Dreyer andEisner (2009) had also suggested EP as future work.EP has been applied only twice before in the NLPcommunity.
Daum?
III and Marcu (2006) used EPfor query summarization (following Minka and Laf-ferty (2003)?s application to an LDA model withfixed topics) and Hall and Klein (2012) used EP forrich parsing.
However, these papers inferred a singlestructured variable connected to all factors (as in thetraditional presentation of EP?see Appendix A),rather than inferring many structured variables con-nected in a sparse graphical model.We regard EP as a generalization of loopy BP forjust this setting: graphical models with large or un-bounded variable domains.
Of course, we are notthe first to use such a scheme; e.g., Qi (2005, chap-ter 2) applies EP to linear-chain models with bothcontinuous and discrete hidden states.
We believethat EP should also be broadly useful in NLP, sinceit naturally handles joint distributions over the kindsof structured variables that arise in NLP.6 Two Methods for Optimizing ?We now fill in details.
If the feature set is defined byan unambiguous FSA A (section 3.4), two methodsexist to maxEv?p[log q?
(v)] as section 3.1 requires.Closed-form.
Determine how often A would tra-verse each of its arcs, in expectation, when readinga random string drawn from p. We would obtain anoptimal ENCODE(?)
by, at each state of A, settingthe weights of the arcs from that state to be propor-tional to these counts while summing to 1.6Thus,the logs of these arc weights give an optimal ?.For example, in a trigram model, the probabilityof the c arc from the ab state is the expected count ofabc (according to p) divided by the expected countof ab.
Such expected substring counts can be foundby the method of Allauzen et al (2003).
For gen-eral A, we can use the method sketched by Li et al(2009, footnote 9): intersect the WFSA for p withthe unweighted FSA A, and then run the forward-backward algorithm to determine the posterior countof each arc in the result.
This tells us the expected to-tal number of traversals of each arc in A, if we havekept track of which arcs in the intersection of p withA were derived from which arcs in A.
That book-keeping can be handled with an expectation semir-ing (Eisner, 2002), or simply with backpointers.Gradient ascent.
For any given ?, we can usethe WFSAs p and ENCODE(?)
to exactly computeEv?p[log q?
(v)] = ?H(p, q?)
(Cortes et al, 2006).We can tune ?
to globally maximize this objective.The technique is to intersect p with ENCODE(?
),after lifting their weights into the expectation semir-ing via the mappings k 7?
?k, 0?
and k 7?
?0, log k?respectively.
Summing over all paths of this in-tersection via the forward algorithm yields ?Z, r?where Z is the normalizing constant for p. We alsosum over paths of ENCODE(?)
to get the normal-izing constant Z?.
Now the desired objective isr/Z ?
logZ?.
Its gradient with respect to ?
can befound by back-propagation, or equivalently by theforward-backward algorithm (Li and Eisner, 2009).An overlarge gradient step can leave the feasiblespace (footnote 1) by driving Z?Vto ?
and thusdriving (2) to ?
(Dreyer, 2011, section 2.8.2).
Inthis case, we try again with reduced stepsize.6This method always yields a probabilistic FSA, i.e., the arcweights are locally normalized probabilities.
This does not sac-rifice any expressiveness; see Appendix B.7 for discussion.9376.1 Optimizing ?
with a penaltyNow consider the penalized objective (3).
Ideally,?(?)
would count the number of nonzero weights in?
?or better, the number of arcs in ENCODE(?).
Butit is not known how to efficiently minimize the re-sulting discontinuous function.
We give two approx-imate methods, based on the two methods above.Proximal gradient.
Leaning on recent advancesin sparse estimation, we replace this ?(?)
with aconvex surrogate whose partial derivative with re-spect to each ?wis undefined at ?w= 0 (Bach et al,2011).
Such a penalty tends to create sparse optima.A popular surrogate is an `1penalty, ?(?)def=?w|?w|.
However, `1would not recognize that?
is simpler with the features {ab, abc, abd} thanwith the features {ab, pqr, xyz}.
The former leadsto a smaller WFSA encoding.
In other words, it ischeaper to add abd once abc is already present, asa state already exists that represents the context ab.We would thus like the penalty to be the numberof distinct prefixes in the set of nonzero features,|{u ?
??
: (?x ?
??)
?ux6= 0}|, (8)as this is the number of ordinary arcs in ENCODE(?
)(see Appendix B.4).
Its convex surrogate is?(?)def=?u?????x???
?2ux(9)This tree-structured group lasso (Nelakanti et al,2013) is an instance of group lasso (Yuan and Lin,2006) where the string w = abd belongs to fourgroups, corresponding to its prefixes u = , u =a, u = ab, u = abd.
Under group lasso, moving ?waway from 0 increases ?(?)
by ?|?w| (just as in `1)for each group in which w is the only nonzero fea-ture.
This penalizes for the new WFSA arcs neededfor these groups.
There are also increases due tow?s other groups, but these are smaller, especiallyfor groups with many strongly weighted features.Our objective (3) is now the sum of a differ-entiable convex function (2) and a particular non-differentiable convex function (9).
We minimize itby proximal gradient (Parikh and Boyd, 2013).
Ateach step, this algorithm first takes a gradient stepas in section 6 to improve the differentiable term,and then applies a ?proximal operator?
to jump to?-0.6a1.2b0aa0ab0ba0bb0Figure 2: Active set method, showing the infinite tree ofall features for the alphabet ?
= {a, b}.
The green nodescurrently have non-zero weights.
The yellow nodes areon the frontier and are allowed to become non-zero, butthe penalty function is still keeping them at 0.
The rednodes are not yet considered, forcing them to remain at 0.a nearby point that improves the non-differentiableterm.
The proximal operator for tree-structuredgroup lasso (9) can be implemented with an efficientrecursive procedure (Jenatton et al, 2011).What if ?
is?-dimensional because we allow alln-grams as features?
Paul and Eisner (2012) usedjust this feature set in a dual decomposition algo-rithm.
Like them, we rely on an active set method(Schmidt and Murphy, 2010).
We fix abcd?s weightat 0 until abc?s weight becomes nonzero (if ever);7only then does feature abc become ?active.?
Thus,at a given step, we only have to compute the gradientwith respect to the currently nonzero features (greennodes in Figure 2) and their immediate children (yel-low nodes).
This hierarchical inclusion techniqueensures that we only consider a small, finite subsetof all n-grams at any given iteration of optimization.Closed-form with greedy growing.
There areexisting methods for estimating variable-order n-gram language models from data, based on either?shrinking?
a high-order model (Stolcke, 1998) or?growing?
a low-order one (Siivola et al, 2007).We have designed a simple ?growing?
algorithmto estimate such a model from a WFSA p. It approx-imately minimizes the objective (3) where ?(?)
isgiven by (8).
We enumerate all n-grams w ?
?
?indecreasing order of expected count (this can be doneefficiently using a priority queue).
We addw toW ifwe estimate that it will decrease the objective.
Everyso often, we measure the actual objective (just as inthe gradient-based methods), and we stop if it is nolonger improving.
Algorithmic details are given inAppendices B.8?B.9.7Paul and Eisner (2012) also required bcd to have nonzeroweight, observing that abcd is a conjunction abc?bcd (Mc-Callum, 2003).
This added test would be wise for us too.938100 200 300 400 500102103104105Time(seconds,log-scale)Trigram EP (Gradient)BaselinePenalized EP (Gradient)Bigram EP (Gradient)Unigram EP (Gradient)100 200 300 400 500102103104105100 200 300 400 500101102103104105100 200 300 400 500German0.00.51.01.52.02.53.03.54.0Cross-Entropy(bits)100 200 300 400 500English0.00.51.01.52.02.53.03.54.0100 200 300 400 500Dutch0.00.51.01.52.02.53.03.54.0Figure 3: Inference on 15 factor graphs (3 languages ?
5 datasets of different sizes).
The first row shows the to-tal runtime (logscale) of each inference method.
The second row shows the accuracy, as measured by the negatedlog-probability that the inferred belief at a variable assigns to its gold-standard value, averaged over ?underlying mor-pheme?
variables.
At this penalty level (?
= 0.01), PEP [thick line] is faster than the pruning baseline of Cotterell etal.
(2015) [dashed line] and much faster than trigram EP, yet is about as accurate.
(For Dutch with sparse observations,it is considerably more accurate than baseline.)
Indeed, PEP is nearly as fast as bigram EP, which has terrible accuracy.An ideal implementation of PEP would be faster yet (see Appendix B.5).
Further graphs are in Appendix C.7 Experiments and ResultsOur experimental design aims to answer three ques-tions.
(1) Is our algorithm able to beat a strong base-line (adaptive pruning) in a non-trivial model?
(2)Is PEP actually better than ordinary EP, given thatthe structured sparsity penalty makes it more algo-rithmically complex?
(3) Does the ?
parameter suc-cessfully trade off between speed and accuracy?All experiments took place using the graphicalmodel over strings for the discovery of underly-ing phonological forms introduced in Cotterell etal.
(2015).
They write: ?Comparing cats ([k?ts]),dogs ([dOgz]), and quizzes ([kwIzIz]), we see theEnglish plural morpheme evidently has at least threepronunciations.?
Cotterell et al (2015) sought a uni-fying account of such variation in terms of phono-logical underlying forms for the morphemes.In their Bayes net, morpheme underlying formsare latent variables, while word surface forms areobserved variables.
The factors model underlying-to-surface phonological changes.
They learn the fac-tors by Expectation Maximization (EM).
Their firstE step presents the hardest inference problem be-cause the factors initially contribute no knowledgeof the language; so that is the setting we test on here.Their data are surface phonological forms fromthe CELEX database (Baayen et al, 1995).
For eachof 3 languages, we run 5 experiments, by observ-ing the surface forms of 100 to 500 words and run-ning EP to infer the underlying forms of their mor-phemes.
Each of the 15 factor graphs has?
150?700latent variables, joined by 500?2200 edges to 200?1200 factors of degree 1?3.
Variables representingsuffixes can have extremely high degree (> 100).We compare PEP with other approximate infer-ence methods.
As our main baseline, we take theapproximation scheme actually used by Cotterell etal.
(2015), which restricts the domain of a belief tothat of the union of 20-best strings of its incomingmessages (section 5).We also compare to unpenal-ized EP with unigram, bigram, and trigram features.We report both speed and accuracy for all meth-ods.
Speed is reported in seconds.
Judging accuracyis a bit trickier.
The best metric would to be to mea-sure our beliefs?
distance from the true marginals oreven from the beliefs computed by vanilla loopy BP.Obtaining these quantities, however, would be ex-tremely expensive?even Gibbs sampling is infeasi-ble in our setting, let alne 100-way WFSA intersec-tions.
Luckily, Cotterell et al (2015) provide gold-standard values for the latent variables (underlying939forms).
Figure 3 shows the negated log-probabilitiesof these gold strings according to our beliefs, aver-aged over variables in a given factor graph.
Our ac-curacy is weaker than Cotterell et al (2015) becausewe are doing inference with their initial (untrained)parameters, a more challenging problem.Each update to ?Vconsisted of a single step of(proximal) gradient descent: starting at the currentvalue, improve (2) with a gradient step of size ?
=0.05, then (in the adaptive case) apply the proximaloperator of (9) with ?
= 0.01.
We chose thesevalues by preliminary exploration, taking ?
smallenough to avoid backtracking (section 6.1).We repeatedly visit variables and factors (sec-tion 4.4) in the forward-backward order used by Cot-terell et al (2015).
For the first few iterations, whenwe visit a variable we make K = 20 passes over itsincoming messages, updating them iteratively to en-sure that the high probability strings in the initial ap-proximations are ?in the ballpark?.
For subsequentiterations of message passing we take K = 1.
Forsimilar reasons, we constrained PEP to use only un-igram features on the first iteration, when there arestill many viable candidates for each morph.7.1 ResultsThe results show that PEP is much faster than thebaseline pruning method, as described in Figure 3and its caption.
It mainly achieves better cross-entropy on English and Dutch, and even though itloses on German, it still places almost all of its prob-ability mass on the gold forms.
While EP with un-igram and bigram approximations are both fasterthan PEP, their accuracy is poor.
Trigram EP isnearly as accurate but even slower than the base-line.
The results support the claim that PEP hasachieved a ?Goldilocks number?
of n-grams in itsapproximation?just enough n-grams to approxi-mate the message well while retaining speed.Figure 4 shows the effect of ?
on the speed-accuracy tradeoff.
To compare apples to apples, thisexperiment fixed the set of ?F?Vmessages for eachvariable.
Thus, we held the set of beliefs fixed, butmeasured the size and accuracy of different approx-imations to these beliefs by varying ?.These figures show only the results from gradient-based approximation.
Closed-form approximation isfaster and comparably accurate: see Appendix C.102 103Number of Features (log-scale)0.00.51.01.52.02.53.03.5Cross-EntropyofGoldStandard(bits)?
= 0.01?
= 0.01?
= 0.5?
= 0.5?
= 1.0 ?
= 1.0Figure 4: Increasing ?
will greatly reduce the numberof selected features in a belief?initially without harmingaccuracy, and then accuracy degrades gracefully.
(Num-ber of features has 0.72 correlation with runtime, and isshown on a log scale on the x axis.
)Each point shows the result of using PEP to approxi-mate the belief at some latent variable V , using ?F?Vmessages from running the baseline method on German.Lighter points use larger ?.
Orange points are affixes(shorter strings), blue are stems (longer strings).
Largecircles are averages over all points for a given ?.8 Conclusion and Future WorkWe have presented penalized expectation propaga-tion (PEP), a novel approximate inference algo-rithm for graphical models, and developed specifictechniques for string-valued random variables.
Ourmethod integrates structured sparsity directly intoinference.
Our experiments show large speedupsover the strong baseline of Cotterell et al (2015).In future, instead of choosing ?, we plan to re-duce ?
as PEP runs.
This serves to gradually refinethe approximations, yielding an anytime algorithmwhose beliefs approach the BP beliefs.
Thanks to(7), the coarse messages from early iterations guidethe choice of finer-grained messages at later itera-tions.
In this regard, ?Anytime PEP?
resembles othercoarse-to-fine architectures such as generalized A*search (Felzenszwalb and McAllester, 2007).As NLP turns its attention to lower-resource lan-guages and social media, it is important to model therich phonological, morphological, and orthographicprocesses that interrelate words.
We hope that theintroduction of faster inference algorithms will in-crease the use of graphical models over strings.
Weare releasing our code package (see Appendix D).940ReferencesCyril Allauzen, Mehryar Mohri, and Brian Roark.
2003.Generalized algorithms for constructing statistical lan-guage models.
In Proceedings of ACL, pages 40?47.Cyril Allauzen, Michael Riley, Johan Schalkwyk, Woj-ciech Skut, and Mehryar Mohri.
2007.
OpenFst: Ageneral and efficient weighted finite-state transducerlibrary.
In Proceedings of the 12th International Con-ference on Implementation and Application of Au-tomata, volume 4783 of Lecture Notes in ComputerScience, pages 11?23.
Springer.R.
Harald Baayen, Richard Piepenbrock, and Leon Gu-likers.
1995.
The CELEX lexical database on CD-ROM.Francis Bach, Rodolphe Jenatton, Julien Mairal, Guil-laume Obozinski, et al 2011.
Convex optimizationwith sparsity-inducing norms.
In S. Sra, S. Nowozin,and S. J. Wright, editors, Optimization for MachineLearning.
MIT Press.Alexandre Bouchard-C?t?, Percy Liang, Thomas L Grif-fiths, and Dan Klein.
2007.
A probabilistic approachto diachronic phonology.
In Proceedings of EMNLP-CoNLL, pages 887?896.Alexandre Bouchard-C?t?, Percy Liang, Thomas Grif-fiths, and Dan Klein.
2008.
A probabilistic approachto language change.
In Proceedings of NIPS.Victor Chahuneau.
2013.
PyFST.
https://github.com/vchahun/pyfst.Corinna Cortes, Mehryar Mohri, Ashish Rastogi, andMichael D Riley.
2006.
Efficient computation ofthe relative entropy of probabilistic automata.
InLATIN 2006: Theoretical Informatics, pages 323?336.Springer.Ryan Cotterell, Nanyun Peng, and Jason Eisner.
2014.Stochastic contextual edit distance and probabilisticFSTs.
In Proceedings of ACL, pages 625?630.Ryan Cotterell, Nanyun Peng, and Jason Eisner.
2015.Modeling word forms using latent underlying morphsand phonology.
Transactions of the Association forComputational Linguistics.
To appear.Hal Daum?
III and Daniel Marcu.
2006.
Bayesian query-focused summarization.
In Proceedings of ACL, pages305?312.Markus Dreyer and Jason Eisner.
2009.
Graphical mod-els over multiple strings.
In Proceedings of EMNLP,pages 101?110, Singapore, August.Markus Dreyer and Jason Eisner.
2011.
Discover-ing morphological paradigms from plain text using aDirichlet process mixture model.
In Proceedings ofEMNLP, pages 616?627, Edinburgh, July.Markus Dreyer.
2011.
A Non-Parametric Model for theDiscovery of Inflectional Paradigms from Plain TextUsing Graphical Models over Strings.
Ph.D. thesis,Johns Hopkins University, Baltimore, MD, April.Jason Eisner.
2002.
Parameter estimation for probabilis-tic finite-state transducers.
In Proceedings of ACL,pages 1?8.C.C.
Elgot and J.E.
Mezei.
1965.
On relations defined bygeneralized finite automata.
IBM Journal of Researchand Development, 9(1):47?68.Gal Elidan, Ian Mcgraw, and Daphne Koller.
2006.Residual belief propagation: Informed scheduling forasynchronous message passing.
In Proceedings ofUAI.P.
F. Felzenszwalb and D. McAllester.
2007.
The gen-eralized A* architecture.
Journal of Artificial Intelli-gence Research, 29:153?190.David Hall and Dan Klein.
2010.
Finding cognategroups using phylogenies.
In Proceedings of ACL.David Hall and Dan Klein.
2011.
Large-scale cognaterecovery.
In Proceedings of EMNLP.David Hall and Dan Klein.
2012.
Training factoredPCFGs with expectation propagation.
In Proceedingsof EMNLP.Rodolphe Jenatton, Julien Mairal, Guillaume Obozinski,and Francis Bach.
2011.
Proximal methods for hierar-chical sparse coding.
The Journal of Machine Learn-ing Research, 12:2297?2334.Andr?
Kempe, Jean-Marc Champarnaud, and Jason Eis-ner.
2004.
A note on join and auto-intersectionof n-ary rational relations.
In Loek Cleophas andBruce Watson, editors, Proceedings of the EindhovenFASTAR Days (Computer Science Technical Report04-40), pages 64?78.
Department of Mathematicsand Computer Science, Technische Universiteit Eind-hoven, Netherlands, December.Kevin Knight and Jonathan Graehl.
1998.
Machinetransliteration.
Computational Linguistics, 24(4).F.
R. Kschischang, B. J. Frey, and H. A. Loeliger.
2001.Factor graphs and the sum-product algorithm.
IEEETransactions on Information Theory, 47(2):498?519,February.Zhifei Li and Jason Eisner.
2009.
First- and second-orderexpectation semirings with applications to minimum-risk training on translation forests.
In Proceedings ofEMNLP, pages 40?51.Zhifei Li, Jason Eisner, and Sanjeev Khudanpur.
2009.Variational decoding for statistical machine transla-tion.
In Proceedings of ACL, pages 593?601.Andr?
F. T. Martins, Noah A. Smith, Pedro M. Q. Aguiar,and M?rio A. T. Figueiredo.
2011.
Structured sparsityin structured prediction.
In Proceedings of EMNLP,pages 1500?1511.Andrew McCallum.
2003.
Efficiently inducing featuresof conditional random fields.
In Proceedings of UAI.941Thomas Minka and John Lafferty.
2003.
Expectation-propagation for the generative aspect model.
In Pro-ceedings of UAI.Thomas P Minka.
2001a.
Expectation propagation forapproximate Bayesian inference.
In Proceedings ofUAI, pages 362?369.Thomas P. Minka.
2001b.
A Family of Algorithms forApproximate Bayesian Inference.
Ph.D. thesis, Mas-sachusetts Institute of Technology, January.Thomas Minka.
2005.
Divergence measures and mes-sage passing.
Technical Report MSR-TR-2005-173,Microsoft Research, January.Mehryar Mohri, Fernando Pereira, and Michael Ri-ley.
2002.
Weighted finite-state transducers inspeech recognition.
Computer Speech & Language,16(1):69?88.Mehryar Mohri.
2000.
Minimization algorithms for se-quential transducers.
Theoretical Computer Science,324:177?201, March.Mehryar Mohri.
2005.
Local grammar algorithms.
InAntti Arppe, Lauri Carlson, Krister Lind?n, Jussi Pi-itulainen, Mickael Suominen, Martti Vainio, HannaWesterlund, and Anssi Yli-Jyr?, editors, Inquiries intoWords, Constraints, and Contexts: Festschrift in Hon-our of Kimmo Koskenniemi on his 60th Birthday, chap-ter 9, pages 84?93.
CSLI Publications, Stanford Uni-versity.Kevin P. Murphy, Yair Weiss, and Michael I. Jordan.1999.
Loopy belief propagation for approximate in-ference: An empirical study.
In Proceedings of UAI,pages 467?475.Anil Nelakanti, C?dric Archambeau, Julien Mairal, Fran-cis Bach, Guillaume Bouchard, et al 2013.
Structuredpenalties for log-linear language models.
In Proceed-ings of EMNLP, pages 233?243.Neal Parikh and Stephen Boyd.
2013.
Proximal al-gorithms.
Foundations and Trends in Optimization,1(3):123?231.Michael Paul and Jason Eisner.
2012.
Implicitly inter-secting weighted automata using dual decomposition.In Proceedings of NAACL-HLT, pages 232?242, Mon-treal, June.Judea Pearl.
1988.
Probabilistic Reasoning in Intelli-gent Systems: Networks of Plausible Inference.
Mor-gan Kaufmann, San Mateo, California.Fernando C. N. Pereira and Michael Riley.
1997.
Speechrecognition by composition of weighted finite au-tomata.
In Emmanuel Roche and Yves Schabes, ed-itors, Finite-State Language Processing.
MIT Press,Cambridge, MA.Slav Petrov, Leon Barrett, Romain Thibaux, and DanKlein.
2006.
Learning accurate, compact, and inter-pretable tree annotation.
In Proceedings of COLING-ACL, pages 433?440, July.Yuan Qi.
2005.
Extending Expectation Propagation forGraphical Models.
Ph.D. thesis, Massachusetts Insti-tute of Technology, February.Dana Ron, Yoram Singer, and Naftali Tishby.
1996.
Thepower of amnesia: Learning probabilistic automatawith variable memory length.
Machine Learning,25(2-3):117?149.Mark W Schmidt and Kevin P Murphy.
2010.
Convexstructure learning in log-linear models: Beyond pair-wise potentials.
In Proceedings of AISTATS, pages709?716.Vesa Siivola, Teemu Hirsim?ki, and Sami Virpioja.
2007.On growing and pruning Kneser-Ney smoothed n-gram models.
IEEE Transactions on Audio, Speech,and Language Processsing, 15(5):1617?1624.Andreas Stolcke.
1998.
Entropy-based pruning ofbackoff language models.
In Proceedings of theDARPA Broadcast News Transcription and Under-standing Workshop, pages 270?274.Ming Yuan and Yi Lin.
2006.
Model selection and esti-mation in regression with grouped variables.
Journalof the Royal Statistical Society: Series B (StatisticalMethodology), 68(1):49?67.942
