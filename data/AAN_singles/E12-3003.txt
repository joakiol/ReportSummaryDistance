Proceedings of the EACL 2012 Student Research Workshop, pages 22?31,Avignon, France, 26 April 2012. c?2012 Association for Computational LinguisticsA Comparative Study of Reinforcement Learning Techniques onDialogue ManagementAlexandros PapangelisNCSR ?Demokritos?,Institute of Informatics& TelecommunicationsandUniv.
of Texas at Arlington,Comp.
Science and Engineeringalexandros.papangelis@mavs.uta.eduAbstractAdaptive Dialogue Systems are rapidly be-coming part of our everyday lives.
As theyprogress and adopt new technologies theybecome more intelligent and able to adaptbetter and faster to their environment.
Re-search in this field is currently focused onhow to achieve adaptation, and particularlyon applying Reinforcement Learning (RL)techniques, so a comparative study of therelated methods, such as this, is necessary.In this work we compare several standardand state of the art online RL algorithmsthat are used to train the dialogue managerin a dynamic environment, aiming to aid re-searchers / developers choose the appropri-ate RL algorithm for their system.
This isthe first work, to the best of our knowledge,to evaluate online RL algorithms on the di-alogue problem and in a dynamic environ-ment.1 IntroductionDialogue Systems (DS) are systems that are ableto make natural conversation with their users.There are many types of DS that serve variousaims, from hotel and flight booking to provid-ing information or keeping company and forminglong term relationships with the users.
Other in-teresting types of DS are tutorial systems, whosegoal is to teach something new, persuasive sys-tems whose goal is to affect the user?s attitude to-wards something through casual conversation andrehabilitation systems that aim at engaging pa-tients to various activities that help their rehabili-tation process.
DS that incorporate adaptation totheir environment are called Adaptive DialogueSystems (ADS).
Over the past few years ADShave seen a lot of progress and have attracted theresearch community?s and industry?s interest.There is a number of available ADS, apply-ing state of the art techniques for adaptation andlearning, such as the one presented by Young etal., (2010), where the authors propose an ADSthat provides tourist information in a fictitioustown.
Their system is trained using RL and someclever state compression techniques to make itscalable, it is robust to noise and able to recoverfrom errors (misunderstandings).
Cuaya?huitl etal.
(2010) propose a travel planning ADS, that isable to learn dialogue policies using RL, buildingon top of existing handcrafted policies.
This en-ables the designers of the system to provide priorknowledge and the system can then learn the de-tails.
Konstantopoulos (2010) proposes an affec-tive ADS which serves as a museum guide.
It isable to adapt to each user?s personality by assess-ing his / her emotional state and current mood andalso adapt its output to the user?s expertise level.The system itself has an emotional state that is af-fected by the user and affects its output.An example ADS architecture is depicted inFigure 1, where we can see several componentstrying to understand the user?s utterance and sev-eral others trying to express the system?s re-sponse.
The system first attempts to convert spo-ken input to text using the Automatic SpeechRecognition (ASR) component and then tries toinfer the meaning using the Natural Language Un-derstanding (NLU) component.
At the core liesthe Dialogue Manager (DM), a component re-sponsible for understanding what the user?s utter-ance means and deciding which action to take thatwill lead to achieving his / her goals.
The DMmay also take into account contextual information22Figure 1: Example architecture of an ADS.or historical data before making a decision.
Afterthe system has decided what to say, it uses theReferring Expression Generation (REG) compo-nent to create appropriate referring expressions,the Natural Language Generation (NLG) compo-nent to create the textual form of the output andlast, the Text To Speech (TTS) component to con-vert the text to spoken output.Trying to make ADS as human-like as possi-ble researchers have focused on techniques thatachieve adaptation, i.e.
adjust to the current user?spersonality, behaviour, mood, needs and to theenvironment in general.
Examples include adap-tive or trainable NLG (Rieser and Lemon, 2009),where the authors formulate their problem as astatistical planning problem and use RL to finda policy according to which the system will de-cide how to present information.
Another exam-ple is adaptive REG (Janarthanam and Lemon,2009), where the authors again use RL to chooseone of three strategies (jargon, tutorial, descrip-tive) according to the user?s expertise level.
Anexample of adaptive TTS is the work of Boidinet al(2009), where the authors propose a modelthat sorts paraphrases with respect to predictionsof which sounds more natural.
Jurc??
?c?ek et al(2010) propose a RL algorithm to optimize ADSparameters in general.
Last, many researchershave used RL to achieve adaptive Dialogue Man-agement (Pietquin and Hastie, 2011; Gas?ic?
et al2010; Cuaya?huitl et al 2010).As the reader may have noticed, the currenttrend in training these components is the appli-cation of RL techniques.
RL is a well establishedfield of artificial intelligence and provides us withrobust frameworks that are able to deal with un-certainty and can scale to real world problems.One sub category of RL is Online RL where thesystem can be trained on the fly, as it interactswith its environment.
These techniques have re-cently begun to be applied to Dialogue Manage-ment and in this paper we perform an extensiveevaluation of several standard and state of the artOnline RL techniques on a generic dialogue prob-lem.
Our experiments were conducted with usersimulations, with or without noise and using amodel that is able to alter the user?s needs at anygiven point.
We were thus able to see how welleach algorithm adapted to minor (noise / uncer-tainty) or major (change in user needs) changes inthe environment.In general, RL algorithms fall in two cate-gories, planning and learning algorithms.
Plan-ning or model-based algorithms use training ex-amples from previous interactions with the envi-ronment as well as a model of the environmentthat simulates interactions.
Learning or model-free algorithms only use training examples fromprevious interactions with the environment andthat is the main difference of these two categories,according to Sutton and Barto, (1998).
The goalof an RL algorithm is to learn a good policy (orstrategy) that dictates how the system should in-teract with the environment.
An algorithm thencan follow a specific policy (i.e.
interact withthe environment in a specific, maybe predefined,way) while searching for a good policy.
This wayof learning is called ?off policy?
learning.
The op-posite is ?on policy?
learning, when the algorithmfollows the policy that it is trying to learn.
Thiswill become clear in section 2.2 where we pro-vide the basics of RL.
Last, these algorithms canbe categorized as policy iteration or value itera-tion algorithms, according to the way they evalu-ate and train a policy.Table 1 shows the algorithms we evaluatedalong with some of their characteristics.
We se-lected representative algorithms for each categoryand used the Dyna architecture (Sutton and Barto,1998) to implement model based algorithms.SARSA(?)
(Sutton and Barto, 1998), Q Learn-ing (Watkins, 1989), Q(?)
(Watkins, 1989; Pengand Williams, 1996) and AC-QV (Wiering andVan Hasselt, 2009) are well established RL al-gorithms, proven to work and simple to imple-ment.
A serious disadvantage though is the factthat they do not scale well (assuming we have23enough memory), as also supported by our resultsin section 5.
Least Squares SARSA(?)
(Chen andWei, 2008) is a variation of SARSA(?)
that usesthe least squares method to find the optimal pol-icy.
Incremental Actor Critic (IAC) (Bhatnagaret al 2007) and Natural Actor Critic (NAC) (Pe-ters et al 2005) are actor - critic algorithms thatfollow the expected rewards gradient and the nat-ural or Fisher Information gradient respectively(Szepesva?ri, 2010).An important attribute of many learning algo-rithms is function approximation which allowsthem to scale to real world problems.
Functionapproximation attempts to approximate a targetfunction by selecting from a class of functionsthat closely resembles the target.
Care must betaken however, when applying this method, be-cause many RL algorithms are not guaranteed toconverge when using function approximation.
Onthe other hand, policy gradient algorithms (algo-rithms that perform gradient ascend/descend ona performance surface), such as NAC or NaturalActor Belief Critic (Jurc??
?c?ek et al 2010) havegood guarantees for convergence, even if we usefunction approximation (Bhatnagar et al 2007).Algorithm Model Policy IterationSARSA(?)
No On ValueLS-SARSA(?)
No On PolicyQ Learning No Off ValueQ(?)
No Off ValueActor Critic - QV No On PolicyIAC No On PolicyNAC No On PolicyDynaSARSA(?)
Yes On ValueDynaQ Yes Off ValueDynaQ(?)
Yes Off ValueDynaAC-QV Yes On PolicyTable 1: Online RL algorithms used in ourevaluation.While there is a significant amount of work inevaluating RL algorithms, this is the first attempt,to the best of our knowledge, to evaluate onlinelearning RL algorithms on the dialogue manage-ment problem, in the presence of uncertainty andchanges in the environment.Atkeson and Santamaria (1997) evaluate modelbased and model free algorithms on the singlependulum swingup problem but their algorithmsare not the ones we have selected and the prob-lem on which they were evaluated differs fromours in many ways.
Ross et al(2008) com-pare many online planning algorithms for solvingPartially Observable Markov Decision Processes(POMDP).
It is a comprehensive study but not di-rectly related to ours, as we model our problemwith Markov Decision Processes (MDP) and eval-uate model-based and model-free algorithms on aspecific task.In the next section we provide some back-ground knowledge on MDPs and RL techniques,in section 3 we present our proposed formulationof the slot filling dialogue problem, in section 4we describe our experimental setup and results, insection 5 we discuss those results and in section 6we conclude this study.2 BackgroundIn order to fully understand the concepts dis-cussed in this work we will briefly introduce MDPand RL and explain how these techniques can beapplied to the dialogue policy learning problem.2.1 Markov Decision ProcessA MDP is defined as a triplet M = {X,A,P},where X is a non empty set of states, A is a nonempty set of actions and P is a transition probabil-ity kernel that assigns probability measures overX ?R for each state-action pair (x, a) ?
X ?A.We can also define the state transition probabil-ity kernel Pt that for each triplet (x1, a, x2) ?X ?
A ?
X would give us the probability ofmoving from state x1 to state x2 by taking actiona.
Each transition from a state to another is as-sociated with an immediate reward, the expectedvalue of which is called the reward function andis defined as R(x, a) = E[r(x, a)], where r(x, a)is the immediate reward the system receives aftertaking action a (Szepesva?ri, 2010).
An episodicMDP is defined as an MDP with terminal states,Xt+s = x, ?s > 1.
We consider an episode overwhen a terminal state is reached.2.2 Reinforcement LearningMotivation to use RL in the dialogue problemcame from the fact that it can easily tackle someof the challenges that arise when implementingdialogue systems.
One of those, for example, iserror recovery.
Hand crafted error recovery doesnot scale at all so we need an automated processto learn error-recovery strategies.
More than thiswe can automatically learn near optimal dialogue24policies and thus maximize user satisfaction.
An-other benefit of RL is that it can be trained usingeither real or simulated users and continue to learnand adapt with each interaction (in the case of on-line learning).
To use RL we need to model thedialogue system using MDPs, POMDPs or SemiMarkov Desicion Processes (SMDP).
POMDPstake uncertainty into account and model each statewith a distribution that represents our belief thatthe system is in a specific state.
SMDPs add tem-poral abstraction to the model and allow for timeconsuming operations.
We, however, do not dealwith either of those in an attempt to keep the prob-lem simple and focus on the task of comparing thealgorithms.More formally, RL tries to maximize an objec-tive function by learning how to control the ac-tions of a system.
A system in this setting is typ-ically formulated as an MDP.
As we discussed insection 2.1 for every MDP we can define a pol-icy pi, which is a mapping from states x ?
X andactions ?
?
A to a distribution pi(x, ?)
that repre-sents the probability of taking action ?
when thesystem is in state x.
This policy dictates the be-haviour of the system.
To estimate how good apolicy is we define the value function V :V pi(x) = E[?
?t=0?tRt+1|x0 = x], x ?
X (1)which gives us the expected cumulative rewardswhen beginning from state x and following policypi, discounted by a factor ?
?
[0, 1] that modelsthe importance of future rewards.
We define thereturn of a policy pi as:Jpi =?
?t=0?tRt(xt, pi(xt)) (2)A policy pi is optimal if Jpi(x) = V pi(x), ?x ?X .
We can also define the action-value functionQ:Qpi(x, ?)
= E[?
?t=0?tRt+1|x0 = x, a0 = ?]
(3)where x ?
X,?
?
A, which gives us the ex-pected cumulative discounted rewards when be-ginning from state x and taking action ?, againfollowing policy pi.
Note that Vmax = rmax1??
,where R(x) ?
[rmin, rmax].The goal of RL therefore is to find the optimalpolicy, which maximizes either of these functions(Szepesva?ri, 2010).3 Slot Filling ProblemWe formulated the problem as a generic slot fill-ing ADS, represented as an MDP.
This model hasbeen proposed in (Papangelis et al 2012), and weextend it here to account for uncertainty.
Formallythe problem is defined as: S =< s0, ..., sN >?M,M = M0?M1?
...?MN ,Mi = {1, ..., Ti},where S are the N slots to be filled, each slot sican take values from Mi and Ti is the number ofavailable values slot si can be filled with.
Dia-logue state is also defined as a vector d ?
M ,where each dimension corresponds to a slot andits value corresponds to the slot?s value.
We callthe set of all possible dialogue states D. Systemactions A ?
{1, ..., |S|} are defined as requestsfor slots to be filled and ai requests slot si.
Ateach dialogue state di we define a set of availableactions a?i ?
A.
A user query q ?
S is definedas the slots that need to be filled so that the sys-tem will be able to accurately provide an answer.We assume action aN always means Give Answer.The reward function is defined as:R(d, a) =?????
?1, if a 6= aN?100, if a = aN ,?qi|qi = ?0, if a = aN ,?
?qi|qi = ?
(4)Thus, the optimal reward for each problem is?|q|since |q| < |S|.Available actions for every state can be mod-elled as a matrix A?
?
{0, 1}|D|?|A|, where:A?ij ={1, if aj ?
a?i0, if aj 6?
a?i(5)When designing A?
one must keep in mind thatthe optimal solution depends on A?
?s structureand must take care not to create an unsolvableproblem, i.e.
a disconnected MDP.
This can beavoided by making sure that each action is avail-able at some state and that each state has at leastone available action.
We should now define thenecessary conditions for the slot filling problemto be solvable and the optimal reward be as de-fined before:??
?ij = 1, 1 ?
i < |D|,?j (6)25??
?ij = 1, 1 < j < |A|,?i (7)Note that j > 1 since d1 is our starting state.
Wealso allow Give Answer (which is aN ) to be avail-able from any state:A?i,N = 1, 1 ?
i ?
|D| (8)We define available action density to be the ra-tio of 1s over the number of elements of A?
:Density =|{(i, j)|A?ij = 1}||D| ?
|A|We can now incorporate uncertainty in ourmodel.
Rather than allowing deterministic transi-tions from a state to another we define a distribu-tion Pt(dj |di, am) which models the probabilityby which the system will go from state di to djwhen taking action am.
Consequently, when thesystem takes action am from state di, it transits tostate dk with probability:Pt(dk|di, am) =??
?Pt(dj |di, am), k = j1?Pt(dj |di,am)|D|?1 , k 6= j(9)assuming that under no noise conditions actionam would move the system from state di to statedj .
The probability of not transiting to state djis uniformly distributed among all other states.Pt(dj |di, am) is updated after each episode witha small additive noise ?, mainly to model unde-sirable or unforeseen effects of actions.
Anotherdistribution, Pc(sj = 1) ?
[0, 1], models our con-fidence level that slot sj is filled:sj ={1, Pc(sj = 1) ?
0.50, Pc(sj = 1) < 0.5(10)In our evaluation Pc(sj) is a random number be-tween [1 ?
, 1] where  models the level of un-certainty.
Last, we can slightly alter A?
after eachepisode to model changes or faults in the avail-able actions for each state, but we did not in ourexperiments.The algorithms selected for this evaluation arethen called to solve this problem online and findan optimal policy pi?
that will yield the highestpossible reward.Algorithm ?
?
?
?SARSA(?)
0.95 - 0.55 0.4LS-SARSA(?)
0.95 - 0.55 0.4Q Learning 0.8 - 0.8 -Q(?)
0.8 - 0.8 0.05Actor Critic - QV 0.9 0.25 0.75 -IAC 0.9 0.25 0.75 -NAC 0.9 0.25 0.75 -DynaSARSA(?)
0.95 - 0.25 0.25DynaQ 0.8 - 0.4 -DynaQ(?)
0.8 - 0.4 0.05DynaAC-QV 0.9 0.05 0.75 -Table 2: Optimized parameter values.4 Experimental SetupOur main goal was to evaluate how each algo-rithm behaves in the following situations:?
The system needs to adapt to a noise free en-vironment.?
The system needs to adapt to a noisy envi-ronment.?
There is a change in the environment and thesystem needs to adapt.To ensure each algorithm performed to the bestof its capabilities we tuned each one?s parametersin an exhaustive manner.
Table 2 shows the pa-rameter values selected for each algorithm.
Theparameter  in -greedy strategies was set to 0.01and model-based algorithms trained their modelfor 15 iterations after each interaction with theenvironment.
Learning rates ?
and ?
and explo-ration parameter  decayed as the episodes pro-gressed to allow better stability.At each episode the algorithms need enough it-erations to explore the state space.
At the initialstages of learning, though, it is possible that somealgorithms fall into loops and require a very largenumber of iterations before reaching a terminalstate.
It would not hurt then if we bound the num-ber of iterations to a reasonable limit, provided itallows enough ?negative?
rewards to be accumu-lated when following a ?bad?
direction.
In ourevaluation the algorithms were allowed 2|D| iter-ations, ensuring enough steps for exploration butnot allowing ?bad?
directions to be followed fortoo long.To assess each algorithm?s performance andconvergence speed, we run each algorithm 10026times on a slot filling problem with 6 slots, 6 ac-tions and 300 episodes.
The average reward overa high number of episodes indicates how stableeach algorithm is after convergence.
User query qwas set to be {s1, ..., s5} and there was no noisein the environment, meaning that the action ofquerying a slot deterministically gets the systeminto a state where that slot is filled.
This can beformulated as: Pt(dj |di, am) = 1, Pc(sj) = 1?j,?
= 0 and A?i,j = 1,?i, j.To evaluate the algorithms?
performance inthe presence of uncertainty we run each for 100times, on the same slot filling problem but withPt(dj |di, am) ?
[1 ?
, 1], with varying  andavailable action density values.
At each run, eachalgorithm was evaluated using the same transitionprobabilities and available actions.
To assess howthe algorithms respond to environmental changeswe conducted a similar but noise free experiment,where after a certain number of episodes the queryq was changed.
Remember that q models the re-quired information for the system to be able to an-swer with some degree of certainty, so changing qcorresponds to requiring different slots to be filledby the user.
For this experiment we randomly gen-erated two queries of approximately 65% of thenumber of slots.
The algorithms then needed tolearn a policy for the first query and then adaptto the second, when the change occurs.
Thiscould, for example, model scenarios where hotelbooking becomes unavailable or some airports areclosed, in a travel planning ADS.
Last, we evalu-ated each algorithm?s scalability, by running eachfor 100 times on various slot filling problems, be-ginning with a problem with 4 slots and 4 actionsup to a problem with 8 slots and 8 actions.
Wemeasured the return averaged over the 100 runseach algorithm achieved.Despite many notable efforts, a standardizedevaluation framework for ADS or DS is still con-sidered an open question by the research commu-nity.
The work in (Pietquin and Hastie, 2011)provides a very good survey of current techniquesthat evaluate several aspects of Dialogue Systems.When RL is applied, researchers typically usethe reward function as a metric of performance.This will be our evaluation metric as well, sinceit is common across all algorithms.
As definedin section 2.3, it penalizes attempts to answer theuser?s query with incomplete information as wellas lengthy dialogues.Algorithm Average RewardSARSA(?)
-10.5967LS-SARSA(?)
-14.3439Q Learning -14.8888Q(?)
-63.7588Actor Critic - QV -15.9245IAC -10.5000NAC -5.8273DynaSARSA(?)
-11.9758DynaQ -14.7270DynaQ(?)
-17.1964DynaAC-QV -58.4576Table 3: Average Total Reward without noise.As mentioned earlier in the text we opted foruser simulations for our evaluation experimentsinstead of real users.
This method has a number ofadvantages, for example the fact that we can veryquickly generate huge numbers of training exam-ples.
One might suggest that since the system istargeted to real users it might not perform as wellwhen trained using simulations.
However, as canbe seen from our results, there are online algo-rithms, such as NAC or SARSA(?
), that can adaptwell to environmental changes, so it is reasonableto expect such a system to adapt to a real user evenif trained using simulations.
We can now presentthe results of our evaluation, as described aboveand in the next section we will provide insight onthe algorithms?
behaviour on each experiment.Alg.
E1 E2 E3 E4S(?)
-7.998 -13.94 -23.68 -30.01LSS -9.385 -12.34 -25.67 -32.33Q -6.492 -15.71 -23.36 -30.56Q(?)
-22.44 -23.27 -27.04 -29.37AC -8.648 -17.91 -32.14 -38.46IAC -6.680 -18.58 -33.60 -35.39NAC -3.090 -9.142 -19.46 -21.33DS(?)
-8.108 -15.61 -38.22 -41.90DQ -6.390 -13.04 -23.64 -28.69DQ(?)
-16.04 -17.33 -39.20 -38.42DAC -28.39 -32.25 -44.26 -45.01Table 4: Average Total Reward with noise.4.1 Average reward without noiseTable 3 shows the average total reward each al-gorithm achieved (i.e.
the average of the sum ofrewards for each episode), over 100 runs, eachrun consisting of 300 episodes.
The problem had6 slots, 6 actions, a query q = {s1, ..., s5} andno noise.
In this scenario the algorithms need tolearn to request each slot only once and give the27answer when all slots are filled.
The optimal re-ward in this case was ?5.
Remember that duringthe early stages of training the algorithms receivesuboptimal rewards until they converge to the op-timal policy that yields Jpi?= ?5.
The sum of re-wards an algorithm received for each episode thencan give us a rough idea of how quickly it con-verged and how stable it is.
Clearly NAC outper-forms all other algorithms with an average rewardof ?5.8273 showing it converges early and is sta-ble from then on.
Note that the differences in per-formance are statistically significant except be-tween LS-SARSA(?
), DynaSARSA(?)
and Dy-naQ Learning.4.2 Average reward with noiseTable 4 shows results from four similar exper-iments (E1, E2, E3 and E4), with 4 slots, 4actions, q = {s1, s2, s3} and 100 episodesbut in the presence of noise.
For E1 we setPt(dj |di, am) = 1 and Density to 1, for E2 weset Pt(dj |di, am) = 0.8 and Density to 0.95, forE3 we set Pt(dj |di, am) = 0.6 and Density to0.9 and for E4 we set Pt(dj |di, am) = 0.4 andDensity to 0.8.
After each episode we added asmall noise ?
?
[?0.05, 0.05] to Pt(?).
Remem-ber that each algorithm run for 2|D| iterations(32 in this case) for each episode, so an aver-age lower than ?32 indicates slow convergenceor even that the algorithm oscillates.
In E1, sincethere are few slots and no uncertainty, most algo-rithms, except for IAC, NAC and Q(?)
convergequickly and have statistically insignificant differ-ences with each other.
In E2 we have less pairswith statistically insignificant differences, and inE3 and E4 we only have the ones mentioned inthe previous section.
As we can see, NAC han-dles uncertainty better, by a considerable margin,than the rest algorithms.
Note here that Q(?)
con-verges late while Q Learning, Dyna Q Learning,SARSA(?)
AC-QV and Dyna SARSA(?)
oscil-late a lot in the presence of noise.
The optimalreward is ?3, so it is evident that most algorithmscannot handle uncertainty well.4.3 Response to changeIn this experiment we let each algorithm run for500 episodes in a problem with 6 slots and 6actions.
We generated two queries, q1 and q2,consisting of 4 slots each, and begun the algo-rithms with q1.
After 300 episodes the querywas changed to q2 and the algorithms were al-lowed another 200 episodes to converge.
Table5 shows the episode at which, on average, eachalgorithm converged after the change (after the300th episode).
Note here that the learning rates?
and ?
were reset at the point of change.
Differ-ences in performance, with respect to the averagereward collected during this experiment are statis-tically significant, except between SARSA(?
), QLearning and DynaQ(?).
We can see that NACconverges only after 3 episodes on average, withIAC converging after 4.
All other algorithms re-quire many more episodes, from about 38 to 134.Algorithm EpisodeSARSA(?)
360.5LS-SARSA(?)
337.6Q Learning 362.8Q(?)
342.5Actor Critic - QV 348.7IAC 304.1NAC 302.9DynaSARSA(?)
402.6DynaQ 380.2DynaQ(?)
384.6DynaAC-QV 433.3Table 5: Average number of episodes requiredfor convergence after the change.4.4 Convergence SpeedTo assess the algorithms?
convergence speed werun each algorithm 100 times for problems of ?di-mension?
4 to 8 (i.e.
4 slots and 4 actions, 5 slotsand 5 actions and so on).
We then marked theepisode at which each algorithm had convergedand averaged it over the 100 runs.
Table 6 showsthe results.
It is important to note here that LS-SARSA, IAC and NAC use function approxima-tion while the rest algorithms do not.
We, how-ever, assume that we have enough memory forproblems up to 8 slots and 8 actions and are onlyinterested in how many episodes it takes eachalgorithm to converge, on average.
The resultsshow how scalable the algorithms are with respectto computational power.We can see that after dimension 7 many algo-rithms require much more episodes in order toconverge.
LS-SARSA(?
), IAC and NAC onceagain seem to behave better than the others, re-quiring only a few more episodes as the prob-lem dimension increases.
Note here however thatthese algorithms take much more absolute time to28converge compared to simpler algorithms (eg QLearning) who might require more episodes buteach episode is completed faster.Algorithm 4 5 6 7 8S(?)
5 23 29 42 101LSS(?)
10 22 27 38 51Q 11 29 47 212 816Q(?)
5 12 29 55 96AC 12 21 42 122 520IAC 7 14 29 32 39NAC 5 9 17 23 28DS(?)
5 11 22 35 217DQ 15 22 60 186 669DQ(?)
9 13 55 72 128DAC 13 32 57 208 738Table 6: Average number of episodes requiredfor convergence on various problem dimensions.5 DiscussionSARSA(?)
performed almost equally to IACat the experiment with deterministic transitionsbut did not react well to the change in q. Aswe can see in Table 6, SARSA(?)
generally con-verges at around episode 29 for a problem with6 slots and 6 actions, therefore the 61 episodes ittakes it to adapt to change are somewhat many.This could be due to the fact that SARSA(?)
useseligibility traces which means that past state - ac-tion pairs still contribute to the updates, so even ifthe learning rate ?
is reset immediately after thechange to allow faster convergence, it seems notenough.
It might be possible though to come upwith a strategy and deal with this type of situa-tion, for example zero out all traces as well as re-setting ?.
SARSA(?)
performs above average inthe presence of noise in this particular problem.LS-SARSA(?)
practically is SARSA(?)
withfunction approximation.
While this gives the ad-vantage of requiring less memory, it converges alittle slower than SARSA(?)
in the presence ofnoise or in noise free environments and it needsmore episodes to converge as the size of the prob-lem grows.
It does, however, react better tochanges in the user?s goals, since it requires 38episodes to converge after the change, comparedto 27 it normally needs as we can see in Table 6.Q Learning exhibits similar behaviour withthe only difference that it converges a little later.Again it takes many episodes to converge after thechange in the environment (compared to the 47that it needs initially).
This could be explained bythe fact that Q Learning only updates one row ofQ(x, a) at each iteration, thus needing more itera-tions forQ(x, a) to reflect expected rewards in thenew environment.
Like SARSA(?
), Q Learning isable to deal with uncertainty well enough on thedialogue task in the given time, but does not scalewell.Q(?)
, quite opposite from SARSA(?)
and QLearning, is the slowest to initially converge, buthandles changes in the environment much better.In Q(?)
the update of Q(x, a) is (very roughly)based on the difference of Q(x, a?)
?
Q(x, a?
)where a?
is the best possible action the algo-rithm can take, whereas in SARSA(?)
the updateis (again roughly) based on Q(x, a?)
?
Q(x, a).Also, in Q(?)
eligibility traces become zero if theselected action is not the best possible.
These tworeasons help obsolete information in Q(x, a) bequickly updated.
While it performs worse in thepresence of uncertainty, the average reward doesnot drop as steeply as for the rest algorithms.AC-QV converges better than average, com-pared to the other algorithms, and seems to copewell with changes in the environment.
Whileit needs 42 episodes, on average, to convergefor a problem of 6 slots and 6 actions, it onlyneeds around 49 episodes to converge again af-ter a change.
Unlike SARSA(?)
and Q(?)
it doesnot have eligibility traces to delay the update ofQ(x, a) (or P (x, a) for Preferences in this case,see (Wiering and Van Hasselt, 2009)) while it alsokeeps track of V (x).
The updates are then basedon the difference of P (x, a) and V (x) which,from our results, seems to make this algorithm be-have better in a dynamic environment.
AC-QValso cannot cope with uncertainty very well onthis problem.IAC is an actor - critic algorithm that fol-lows the gradient of cumulative discounted re-wards ?Jpi.
It always performs slightly worsethan NAC but in a consistent way, except in theexperiments with noise.
It only requires approx-imately 4 episodes to converge after a changebut cannot handle noise as well as other algo-rithms.
This can be in part explained by thepolicy gradient theorem (Sutton et al 2000) ac-cording to which changes in the policy do not29affect the distribution of state the system visits(IAC and NAC perform gradient ascend in thespace of policies rather than in parameter space(Szepesva?ri, 2010)).
Policy gradient methods ingeneral seem to converge rapidly, as supported byresults of Sutton et al(2000) or Konda and Tsit-siklis (2001) for example.NAC , as expected, performs better than anyother algorithm in all settings.
It not only con-verges in very few episodes but is also very robustto noise and changes in the environment.
Follow-ing the natural gradient has proven to be muchmore efficient than simply using the gradient ofthe expected rewards.
There are many positiveexamples of NAC performance (or following thenatural gradient in general), such as (Bagnell andSchneider, 2003; Peters et al 2005) and this workis one of them.Dyna Algorithms except for DynaSARSA(?
), seem to perform worse than av-erage on the deterministic problem.
In thepresence of changes, none of them seems toperform very well.
These algorithms use amodel of the environment to update Q(x, a) orP (x, a), meaning that after each interaction withthe environment they perform several iterationsusing simulated triplets (x, a, r).
In the presenceof changes this results in obsolete informationbeing reused again and again until sufficient realinteractions with the environment occur and themodel is updated as well.
This is possibly themain reason why each Dyna algorithm requiresmore episodes after the change than its corre-sponding learning algorithm.
Dyna Q Learningonly updates a single entry of Q(x, a) at eachsimulated iteration, which could explain whynoise does not corrupt Q(x, a) too much andwhy this algorithm performs well in the presenceof uncertainty.
Noise in this case is added at asingle entry of Q(x, a), rather than to the wholematrix, at each iteration.
Dyna SARSA(?)
andDyna Q(?)
handle noise slightly better than DynaAC-QV.6 Concluding RemarksNAC proved to be the best algorithm in our eval-uation.
It is, however, much more complex to im-plement and run and thus each episode takes more(absolute) time to complete.
One might suggestthen that a lighter algorithm such as SARSA(?
)will have the opportunity to run more iterationsin the same absolute time.
One should definitelytake this into account when designing a real worldsystem, when timely responses are necessary andresources are limited as, for example, in a mobilesystem.
Note that SARSA(?
), Q-Learning, Q(?
)and AC-QV are significantly faster than the restalgorithms.On the other hand, all algorithms except forNAC, IAC and LS-SARSA have the major draw-back of the size of the table representing Q(x, a)or P (x, a) that is needed to store state-action val-ues.
This is a disadvantage that practically pro-hibits the use of these algorithms in high dimen-sional or continuous problems.
Function approxi-mation might alleviate this problem, according toBertsekas (2007), if we reformulate the problemand reduce control space while increasing statespace.
In such a setting function approximationperforms well, while in general it cannot deal withlarge control spaces.
It becomes very expensiveas computation cost grows exponentially on thesize of the lookahead horizon.
Also, according toSutton and Barto (1998) and Sutton et al(2000),better convergence guarantees exist for online al-gorithms when combined with function approx-imation or for policy gradient methods (such asIAC or NAC) in general.
Finally, one must takegreat care when selecting features to approximateQ(x, a) or V (x) as they are important to con-vergence and speed of the algorithm (Allen andFritzsche, 2011; Bertsekas, 2007).To summarize, NAC outperforms the other al-gorithms in every experiment we conducted.
Itdoes require a lot of computational power thoughand might not be suitable if it is limited.
Onthe other hand, SARSA(?)
or Q Learning per-form well enough while requiring less computa-tional power but a lot more memory space.
Theresearcher / developer then must make his / herchoice between them taking into account suchpractical limitations.As future work we plan to implement these al-gorithms on the Olympus / RavenClaw (Bohusand Rudnicky, 2009) platform, using the resultsof this work as a guide.
Our aim will be to cre-ate a hybrid state of the art ADS that will com-bine advantages of existing state of the art tech-niques.
Moreover we plan to install our systemon a robotic platform and conduct real user trials.30ReferencesAllen, M., Fritzsche, P., 2011, Reinforcement Learn-ing with Adaptive Kanerva Encoding for XpilotGame AI, Annual Congress on Evolutionary Com-putation, pp 1521?1528.Atkeson, C.G., Santamaria, J.C., 1997, A comparisonof direct and model-based reinforcement learning,IEEE Robotics and Automation, pp 3557?3564.Bagnell, J., Schneider, J., 2003, Covariant pol-icy search, Proceedings of the Eighteenth Interna-tional Joint Conference on Artificial Intelligence, pp1019?1024.Bertsekas D.P., 2007, Dynamic Programming andOptimal Control, Athena Scientific, vol 2, 3rd edi-tion.Bhatnagar, S, Sutton, R.S., Ghavamzadeh, M., Lee,M.
2007, Incremental Natural Actor-Critic Algo-rithms, Neural Information Processing Systems, pp105?112.Bohus, D., Rudnicky, A.I., 2009, The RavenClaw di-alog management framework: Architecture and sys-tems, Computer Speech & Language, vol 23:3, pp332-361.Boidin, C., Rieser, V., Van Der Plas, L., Lemon, O.,and Chevelu, J.
2009, Predicting how it sounds:Re-ranking dialogue prompts based on TTS qual-ity for adaptive Spoken Dialogue Systems, Pro-ceedings of the Interspeech Special Session Ma-chine Learning for Adaptivity in Spoken Dialogue,pp 2487?2490.Chen, S-L., Wei, Y-M. 2008, Least-SquaresSARSA(Lambda) Algorithms for ReinforcementLearning, Natural Computation, 2008.
ICNC ?08,vol.2, pp 632?636.Cuaya?huitl, H., Renals, S., Lemon, O., Shimodaira,H.
2010, Evaluation of a hierarchical reinforce-ment learning spoken dialogue system, ComputerSpeech & Language, Academic Press Ltd., vol 24:2,pp 395?429.Gas?ic?, M., Jurc??
?c?ek, F., Keizer, S., Mairesse, F.and Thomson, B., Yu, K. and Young, S, 2010,Gaussian processes for fast policy optimisation ofPOMDP-based dialogue managers, Proceedingsof the 11th Annual Meeting of the Special InterestGroup on Discourse and Dialogue, pp 201?204.Geist, M., Pietquin, O., 2010, Kalman temporaldifferences, Journal of Artificial Intelligence Re-search, vol 39:1, pp 483?532.Janarthanam, S., Lemon, O.
2009, A Two-Tier UserSimulation Model for Reinforcement Learning ofAdaptive Referring Expression Generation Policies,SIGDIAL Conference?09, pp 120?123.Jurc??
?c?ek, F., Thomson, B., Keizer, S., Mairesse, F.,Gas?ic?, M., Yu, K., Young, S 2010, Natural Belief-Critic: A Reinforcement Algorithm for ParameterEstimation in Statistical Spoken Dialogue Systems,International Speech Communication Association,vol 7, pp 1?26.Konda, V.R., Tsitsiklis, J.N., 2001, Actor-Critic Al-gorithms, SIAM Journal on Control and Optimiza-tion, MIT Press, pp 1008?1014.Konstantopoulos S., 2010, An Embodied DialogueSystem with Personality and Emotions, Proceedingsof the 2010 Workshop on Companionable DialogueSystems, ACL 2010, pp 3136.Papangelis, A., Karkaletsis, V., Makedon, F., 2012,Evaluation of Online Dialogue Policy LearningTechniques, Proceedings of the 8th Conference onLanguage Resources and Evaluation (LREC) 2012,to appear.Peng, J., Williams, R., 1996, Incremental multi-stepQ-Learning, Machine Learning pp 283?290.Peters, J., Vijayakumar, S., Schaal, S. 2005, Naturalactor-critic , Machine Learning: ECML 2005, pp280?291.Pietquin, O., Hastie H. 2011, A survey on metrics forthe evaluation of user simulations, The KnowledgeEngineering Review, Cambridge University Press(to appear).Rieser, V., Lemon, O.
2009, Natural Language Gen-eration as Planning Under Uncertainty for SpokenDialogue Systems, Proceedings of the 12th Confer-ence of the European Chapter of the ACL (EACL2009), pp 683?691.Ross, S., Pineau, J., Paquet, S., Chaib-draa, B., 2008,Online planning algorithms for POMDPs, Journalof Artificial Intelligence Research, pp 663?704.Sutton R.S., Barto, A.G., 1998, Reinforcement Learn-ing: An Introduction, The MIT Press, Cambridge,MA.Sutton, R.S.,Mcallester, D., Singh, S., Mansour, Y.2000, Policy gradient methods for reinforcementlearning with function approximation, In Advancesin Neural Information Processing Systems 12, pp1057?1063.Szepesva?ri, C., 2010, Algorithms for ReinforcementLearning, Morgan & Claypool Publishers, Synthe-sis Lectures on Artificial Intelligence and MachineLearning, vol 4:1, pp 1?103.Watkins C.J.C.H., 1989, Learning from delayed re-wards, PhD Thesis, University of Cambridge, Eng-land.Wiering, M. A, Van Hasselt, H. 2009, The QVfamily compared to other reinforcement learningalgorithms, IEEE Symposium on Adaptive Dy-namic Programming and Reinforcement Learning,pp 101?108.Young S., Gas?ic?, M., Keizer S., Mairesse, F., Schatz-mann J., Thomson, B., Yu, K., 2010, The Hid-den Information State model: A practical frame-work for POMDP-based spoken dialogue manage-ment, Computer Speech & Language, vol 24:2, pp150?174.31
