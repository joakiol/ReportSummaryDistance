Cascaded Grammatical Relation AssignmentSab ine  Buchho lz  and Jo rn  Veenst ra  and Wal ter  Dae lemansILK, Computational Linguistics, Tilburg UniversityPO box 90153, 5000 LE Tilburg, The Netherlands\ [buchholz,  veenst ra ,  dae lemans\] 0kub.
n lAbst ractIn this paper we discuss cascaded Memory-Based grammatical relations assignment.
In thefirst stages of the cascade, we find chunks of sev-eral types (NP,VP,ADJP,ADVP,PP) and labelthem with their adverbial function (e.g.
local,temporal).
In the last stage, we assign gram-matical relations to pairs of chunks.
We stud-ied the effect of adding several levels to this cas-caded classifier and we found that even the lessperforming chunkers enhanced the performanceof the relation finder.1 In t roduct ionWhen dealing with large amounts of text, find-ing structure in sentences i  often a useful pre-processing step.
Traditionally, full parsing isused to find structure in sentences.
However,full parsing is a complex task and often pro-vides us with more information then we need.For many tasks detecting only shallow struc-tures in a sentence in a fast and reliable way isto be preferred over full parsing.
For example,in information retrieval it can be enough to findonly simple NPs and VPs in a sentence, for in-formation extraction we might also want to findrelations between constituents as for examplethe subject and object of a verb.In this paper we discuss ome Memory-Based(MB) shallow parsing techniques to find labeledchunks and grammatical relations in a sentence.Several MB modules have been developed inprevious work, such as: a POS tagger (Daele-mans et al, 1996), a chunker (Veenstra, 1998;Tjong Kim Sang and Veenstra, 1999) and agrammatical relation (GR) assigner (Buchholz,1998).
The questions we will answer in this pa-per are: Can we reuse these modules in a cas-cade of classifiers?
What is the effect of cascad-ing?
Will errors at a lower level percolate tohigher modules?Recently, many people have looked at cas-caded and/or shallow parsing and GR assign-ment.
Abney (1991) is one of the first who pro-posed to split up parsing into several cascades.He suggests to first find the chunks and thenthe dependecies between these chunks.
Grefen-stette (1996) describes a cascade of finite-statetransducers, which first finds noun and verbgroups, then their heads, and finally syntacticfunctions.
Brants and Skut (1998) describe apartially automated annotation tool which con-structs a complete parse of a sentence by recur-sively adding levels to the tree.
(Collins, 1997;Ratnaparkhi, 1997) use cascaded processing forfull parsing with good results.
Argamon et al(1998) applied Memory-Based Sequence Learn-ing (MBSL) to NP chunking and subject/objectidentification.
However, their subject and ob-ject finders are independent of their chunker(i.e.
not cascaded).Drawing from this previous work we willexplicitly study the effect of adding steps tothe grammatical relations assignment cascade.Through experiments with cascading severalclassifiers, we will show that even using im-perfect classifiers can improve overall perfor-mance of the cascaded classifier.
We illustratethis claim on the task of finding grammati-cal relations (e.g.
subject, object, locative) toverbs in text.
The GR assigner uses severalsources of information step by step such as sev-eral types of XP chunks (NP, VP, PP, ADJPand ADVP), and adverbial functions assignedto these chunks (e.g.
temporal, local).
Sincenot all of these entities are predicted reliably, itis the question whether each source leads to animprovement of the overall GR assignment.In the rest of this paper we will first briefly de-scribe Memory-Based Learning in Section 2.
In239Section 3.1, we discuss the chunking classifiersthat we later use as steps in the cascade.
Sec-tion 3.2 describes the basic GR classifier.
Sec-tion 3.3 presents the architecture and results ofthe cascaded GR assignment experiments.
Wediscuss the results in Section 4 and concludewith Section 5.2 Memory-Based  Learn ingMemory-Based Learning (MBL) keeps all train-ing data in memory and only abstracts at clas-sification time by extrapolating a class from themost similar item(s) in memory.
In recent workDaelemans et al (1999b) have shown that fortypical natural language processing tasks, thisapproach is at an advantage because it also"remembers" exceptional, low-frequency caseswhich are useful to extrapolate from.
More-over, automatic feature weighting in the similar-ity metric of an MB learner makes the approachwell-suited for domains with large numbers offeatures from heterogeneous sources, as it em-bodies a smoothing-by-similarity method whendata is sparse (Zavrel and Daelemans, 1997).We have used the following MBL algorithms1:IB1 : A variant of the k-nearest neighbor (k-NN) algorithm.
The distance between atest item and each memory item is definedas the number of features for which theyhave a different value (overlap metric).IB i - IG  : IB1 with information gain (aninformation-theoretic notion measuring thereduction of uncertainty about the class tobe predicted when knowing the value of afeature) to weight the cost of a feature valuemismatch during comparison.IGTree  : In.this variant, a decision tree is cre-ated with features as tests, and ordered ac-cording to the information gain of the fea-tures, as a heuristic approximation of thecomputationally more expensive IB1 vari-ants.For more references and information aboutthese algorithms we refer to (Daelemans et al,1998; Daelemans et al, 1999b).
For other1For the experiments described in this paper we haveused TiMBL, an MBL software package developed in theILK-group (Daelemans et al, 1998), TiMBL is availablefrom: http:// i lk.kub.nl/.memory-based approaches to parsing, see (Bod,1992) and (Sekine, 1998).3 Methods  and Resu l t sIn this section we describe the stages of the cas-cade.
The very first stage consists of a Memory-Based Part-of-Speech Tagger (MBT) for whichwe refer to (Daelemans et al, 1996).
Thenext three stages involve determining bound-aries and labels of chunks.
Chunks are non-recursive, non-overlapping constituent parts ofsentences (see (Abney, 1991)).
First, we si-multaneously chunk sentences into: NP-, VP-, Prep-, ADJP-  and APVP-chunks.
As thesechunks are non-overlapping, no words can be-long to more than one chunk, and thus no con-flicts can arise.
Prep-chunks are the preposi-tional part of PPs, thus excluding the nominalpart.
Then we join a Prep-chunk and one - -or more coordinated - -  NP-chunks into a PP-chunk.
Finally, we assign adverbial function(ADVFUNC) labels (e.g.
locative or temporal)to all chunks.In the last stage of the cascade, we labelseveral types of grammatical relations betweenpairs of words in the sentence.The data for all our experiments was ex-tracted from the Penn Treebank II Wall StreetJournal (WSJ) corpus (Marcus et al, 1993).For all experiments, we used sections 00-19 astraining material and 20-24 as test material.See Section 4 for results on other train/test setsplittings.For evaluation of our results we use the pre-cision and recall measures.
Precision is the per-centage of predicted chunks/relations that areactually correct, recall is the percentage of cor-rect chunks/relations that are actually found.For convenient comparisons of only one value,we also list the FZ=i value (C.J.van Rijsbergen,1979): (Z2+l)'prec'rec /~2.prec+rec , with/~ = 13.1 Chunk ingIn the first experiment described in this section,the task is to segment he sentence into chunksand to assign labels to these chunks.
This pro-cess of chunking and labeling is carried out byassigning a tag to each word in a sentence left-to-right.
Ramshaw and Marcus (1995) first as-signed a chunk tag to each word in the sentence:I for inside a chunk, O for outside a chunk, and240type precision B tbr inside a chunk, but tile preceding word isin another chunk.
As we want to find more thanone kind of chunk, we have to \[hrther differen-tiate tile IOB tags as to which kind of chunk(NP, VP, Prep, ADJP  or ADVP) the word isill.
With the extended IOB tag set at hand wecan tag the sentence:But/CO \[NP the/DT dollar/NN NP\]\[ADVP later/KB ADVP\]\[VP rebounded/VBD VP\] ,/,\[VP finishing/VBG VP\]\[ADJP slightly/KB higher/RBK ADJP\]\[Prep against/IN Prep\] \[NP the/DTyen/NNS NP\] \[ADJP although/IN ADJP\]\[ADJP slightly/RB lower/J JR ADJP\]\[Prep against/IN Prep\] \[NP the/DTmark/NN NP\] ./.as:But/CCo the/DTi_NP dollar/NNi_Nplater/KBi-ADVP rebounded/VBDi_vP ,/,0f inishing/VBGi_vP slightly/KBi-ADVPhigher/RBRi_ADVP against/INi_Prepthe/DTl_Np yen/NNSi_NPalthough/INi_ADJP slightly/KBB_ADJPlower/JJKl_ADJP against/IN\[_Prepthe/DTf_Np mark/NNi_Np ./.oAfter having found Prep-, NP- and otherchlmks, we collapse Preps and NPs to PPs ina second step.
While the GR assigner finds re-lations between VPs and other chunks (cf.
Sec-tion 3.2), the PP  chunker finds relations be-tween prepositions and NPs 2 in a way sim-ilar to GR assignment (see Section 3.2).
Inthe last chunking/labeling step, we assign ad-verbial functions to chunks.
The classes arethe adverbial function labels from the treebank:LOC (locative), TMP (temporal), DIR (direc-tional), PRP  (purpose and reason), MNR (man-ner), EXT (extension) or "-" for none of thefornmr.
Table 1 gives an overview of the resultsof the chunking-labeling experiments, using thefollowing algorithms, determined by validationon the train set: IBi- IG for XP-chunking andIGTree for PP-chunking and ADVFUNCs as-signment.3.2 Grammat ica l  Re lat ion Ass ignmentIn grammatical relation assignment we assigna GR to pairs of words in a sentence.
In our2pPs containing anything else than NPs (e.g.
withoutbringing his wife) are not searched for.NPchunksVPchunksADJPchunksADVPchunksPrepchunks92.591.968.478.095.5PPchunks 91.9ADVFUNCs 78.0recall92.291.765.077.996.792.269.5fl=l92.391.866.777.996.192.073.5Table h Results of chunking-labeling experi-ments.
NP-,VP-, ADJP-, ADVP- and Prep-chunks are found sinmltaneously, but for con-venience, precision and recall values are givenseparately for each type of chunk.experiments, one of these words is always a verb,since this yields the most important GRs.
Theother word is the head of the phrase which isannotated with this grammatical relation in thetreebank.
A preposition is the head of a PP,a noun of an NP and so on.
Defining relationsto hold between heads means that the algorithmcan, for example, find a subject relation betweena noun and a verb without necessarily having tomake decisions about the precise boundaries ofthe subject NP.Suppose we had the POS-tagged sentenceshown in Figure 1 and we wanted the algorithmto decide whether, and if so how, Miller (hence-forth: the focus) is related to the first verb or-ganized.
We then construct an instance for thispair of words by extracting a set of feature val-ues from the sentence.
The instance containsinformation about the verb  and the focus: afeature for the word form and a feature for thePOS of both.
It also has similar features for thelocal context  of the focus.
Experiments on thetraining data suggest an optimal context widthof two elements to the left and one to the right.In the present case, elements are words or punc-tuation signs.
In addition to the lexical and thelocal context information, we include superficialinformation about clause s t ruc ture :  The firstfeature indicates the distance from the verb tothe focus, counted in elements.
A negative dis-tance means that the focus is to the left of theverb.
The second feature contains the numberof other verbs between the verb and the focus.The third feature is the number of interveningcommas.
The features were chosen by manual241Not/RB surprisingly/RB ,/, Peter/NNP Miller/NNP ,/, who/WP organized/VBD the/DT con-ference/NN in~IN New/NNP York/NNP ,/, does/VBZ not/RB want/VB to~TO come/VB to~INParis /NNP without~IN bringing /VBG his /P RP$ wife /NN .Figure 1: An example sentence annotated with POS.Verb Context -2 Context -1word pos word posl word123  4 5 6 7 8 9-702-602-401 .-301-100surprisingly rbMiller nnporganized vbdorganized vbdorganized vbdorganized vbdorganized vbdnot rbPeter nnpFocus Context +1pos word pos10 11 12 13not rbsurprisingly rbPeter nnpMiller nnpwho wpsurprisingly rbMillet nnporganized vbdClassnp-sbjTable 2: The first five instances for the sentence in Figure 1.
Features 1-3 are the Features fordistance and intervening VPs and commas.
Features 4 and 5 show the verb and its POS.
Features6-7, 8-9 and 12-13 describe the context words, Features 10-11 the focus word.
Empty contextsare indicated by the value "-" for all features.
"feature engineering".
Table 2 shows the com-plete instance for Miller-organized in row 5, to-gether with the other first four instances for thesentence.
The class is mostly "-", to indicatethat the word does not have a direct grammati-cal relation to organized.
Other possible classesare those from a list of more than 100 differentlabels found in the treebank.
These are combi-nations of a syntactic category and zero, one ormore functions, e.g.
NP-SBJ for subject, NP-PRDfor predicative object, NP for (in)direct object 3,PP-LOC for locative PP adjunct, PP-LOC-CLR forsubcategorised locative PP, etcetera.
Accord-ing to their information gain values, features areordered with decreasing importance as follows:11, 13, 10, 1, 2, 8, 12, 9, 6 , 4 ,  7 ,  3 , 5.
In-tuitively,, this ordering makes sense.
The mostimportant feature is the POS of the focus, be-cause this determines whether it can have a GRto a verb at all (15unctuation cannot) and whatkind of relation is possible.
The POS of the fol-lowing word is important, because .g.
a nounfollowed by a noun is probably not the head ofan NP and will therefore not have a direct GRto the verb.
The word itself may be importantif it is e.g.
a preposition, a pronoun or a clearlytemporal/ local adverb.
Features 1 and 2 givesome indication of the complexity of the struc-ture intervening between the focus and the verb.aDirect and indirect object NPs have the same labelin the treebank annotation.
They can be differentiatedby their position.The more complex this structure, the lower theprobability that the focus and the verb are re-lated.
Context further away is less importantthan near context.To test the effects of the chunking steps fromSection 3.1 on this task, we will now constructinstances based on more structured input text,like that in Figure 2.
This time, the focus is de-scribed by five features instead of two, for theadditional information: which type of chunk itis in, what the preposition is if it is in a PPchunk, and what the adverbial function is, ifany.
We still have a context of two elementsleft, one right, but elements are now defined tobe either chunks, or words outside any chunk,or punctuation.
Each chunk in the context isrepresented by its last word (which is the se-mantically most important word in most cases),by the POS of the last word, and by the typeof chunk.
The distance feature is adapted tothe new definition of element, too, and insteadof counting intervening verbs, we now count in-tervening VP chunks.
Figure 3 shows the firstfive instances for the sentence in Figure 2.
Classvalue"-" again means "the focus is not directlyrelated to the verb" (but to some other verb ora non-verbal element).
According to their in-formation gain values, features are ordered indecreasing importance as follows: 16, 15, 12,14, 11, 2, 1, 19, 10, 9, 13, 18, 6, 17, 8, 4, 7, 3,5.
Comparing this to the earlier feature order-ing, we see that most of the new features are242\ [ADVP Not/RB suwri,singly/1RB ADVP\ ]  ,/, \ [NP Peter/NNP Miller/NNP NP\ ]  ,/, \ [NPwho/WP NP\ ]  \ [VP organized/VBD VP\]  \ [NP the/DT conference/NN P\ ]  {PP-LOC \ [P repin~IN Prep\ ]  \ [NP New/NNP York/NNP NP\ ]  PP -LOC} ,/, \ [VP does/VBZ not/RB want/VBto~TO eome/VB VP\ ]  {PP-D IR  \ [Prep to~IN Prep\ ]  \ [NP Paris/NNP NP\ ]  PP -D IR}  \ [P rep.,~ithout/IN Prep\ ]  \ [VP bringing/VBG VP\]  \ [NP his/PRP$ wife/NN NP\ ]  .Figure 2: An example sentence annotated with POS (after the slash), ctmnks (with square andcurly brackets) and adverbial functions (after the dash).Stru(:t. Verb Context -2 Context -1 Focus Context +1word pos cat word pos cat pr word pos cat adv word pos cat1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19org.
vbd vpconf.
nn npsurpris, rb advpMiller nnp npwho wp np -- conf.
nn npin York nnp pp locorg.
vbd vpi York nnp pporg.
vbdorg.
vbdorg.
vbdorg.
vbdorg.
vbd-502-301-100100200surpris, rb advpMiller nnp npwho wp nporg.
vbd vpClassnp-sbjnpTable 3: The first five instances for the sentence in Figure 2.
Features 1-3 are the features fordistance and intervening VPs and commas.
Features 4 and 5 show the verb and its POS.
Features6-8, 9-11 and 17 19 describe the context words/chunks, Features 12-16 the focus chunk.
Emptycontexts are indicated by the "-" for all features.very important, thereby justifying their intro-duction.
Relative to the other "old" features,the structural features 1 and 2 have gained im-portance, probably because more structure isavailable in the input to represent.In principle, we would have to construct oneinstance tbr each possible pair of a verb and afocus word in the sentence.
However, we re-strict instances to those where there is at mostone other verb/VP chunk between the verb andthe focus, in case the focus precedes the verb,and no other verb in case the verb precedes thefocus.
This restriction allows, for example, for arelative clause on the subject (as in our examplesentence).
In the training data, 97.9% of the re-lated pairs fulfill this condition (when countingVP chunks).
Experiments on the training datashowed that increasing the admitted number ofintervening VP chunks slightly increases recall,at the cost of precision.
Having constructed allinstances from the test data and from a trainingset with the same level of partial structure, wefirst train the IG'IYee algorithm, and then let itclassify the test instances.
Then, for each testinstance that was classified with a grammaticalrelation, we check whether the same verb-focus-pair appears with the same relation in the GRlist extracted irectly from the treebank.
Thisgives us the precision of the classifier.
Checkingthe treebank list versus the classified list yieldsrecall.3.3 Cascaded ExperimentsWe have already seen from the example that thelevel of structure in the input text can influencethe composition of the instances.
We are inter-ested in the effects of different sorts of partialstructure in the input data on the classificationperformance of the final classifier.Therefore, we ran a series of experiments.The classification task was always that of find-ing grammatical relations to verbs and perfor-mance was always measured by precision andrecall on those relations (the test set contained45825 relations).
The amount of structure inthe input data varied.
Table 4 shows the resultsof the experiments.
In the first experiment, onlyPOS tagged input is used.
Then, NP chunksare added.
Other sorts of chunks are insertedat each subsequent step.
Finally, the adverbialfunction labels are added.
We can see that themore structure we add, the better precision andrecall of the grammatical relations get: preci-sion increases from 60.7% to 74.8%, recall from41.3% to 67.9%.
This in spite of the fact thatthe added information is not always correct, be-cause it was predicted for the test material onthe basis of the training material by the classi-tiers described in Section 3.1.
As we have seenin Table 1, especially ADJP  and ADVP chunks243and adverbial function labels did not have veryhigh precision and recall.4 D iscuss ionThere are three ways how two cascaded modulescan interact.The first module can add information onwhich the later module can (partially) baseits decisions.
This is the case between theadverbial functions finder and the relationsfinder.
The former adds an extra informa-tive feature to the instances of the latter(Feature 16 in Table 3).
Cf.
column two ofTable 4.The first module can restrict the num-ber of decisions to be made by the sec-ond one.
This is the case in the combina-tion of the chunking steps and the relationsfinder.
Without the chunker, the relationsfinder would have to decide for every word,whether it is the head of a constituent thatbears a relation to the verb.
With the chun-ker., the relations finder has to make thisdecision for fewer words, namely only forthose which are the last word in a chunkresp.
the preposition of a PP  chunk.
Prac-tically, this reduction of the number of de-cisions (which translates into a reductionof instances) as can be seen in the thirdcolumn of Table 4.?
The first module can reduce the number ofelements used for the instances by count-ing one chunk as just one context element.We can see the effect in the feature thatindicates the distance in elements betweenthe focus and the verb.
The more chunksare used, the smaller the average absolutedistance (see column four Table 4).All three effects interact in the cascade wedescribe.
The PP  chunker reduces the numberof decisions for the relations finder (instead ofone instance for the preposition and one for theNP chunk, we get only one instance for the PPchunk), introduces an extra feature (Feature 12in Table 3), and changes the context (instead ofa preposition and an NP, context may now beone PP).As we already noted above, precision and re-call are monotonically increasing when addingmore structure.
However, we note large dif-ferences, such as NP chunks which increaseFZ=i by more than 10%, and VP chunks whichadd another 6.8%, whereas ADVPs and ADJPsyield hardly any improvement.
This may par-tially be explained by the fact that these chunksare less frequent han the former two.
Preps, onthe other hand, while hardly reducing the av-erage distance or the number of instances, im-prove F~=i by nearly 1%.
PPs yield another1.1%.
What may come as a surprise is that ad-verbial functions again increase FZ=i by nearly2%, despite the fact that FZ=i for this ADV-FUNC assignment step was not very high.
Thisresult shows that cascaded modules need not beperfect to be useful.Up to now, we only looked at the overall re-sults.
Table 4 also shows individual FZ=i val-ues for four selected common grammatical re-lations: subject NP, (in)direct object NP, loca-tive PP  adjunct and temporal PP  adjunct.
Notethat the steps have different effects on the dif-ferent relations: Adding NPs increases FZ=i by11.3% for subjects resp.
16.2% for objects, butonly 3.9% resp.
3.7% for locatives and tempo-rals.
Adverbial functions are more importantfor the two adjuncts (+6.3% resp.
+15%) thanfor the two complements (+0.2% resp.
+0.7%).Argamon et al (1998) report FZ=i for sub-ject and object identification of respectively86.5% and 83.0%, compared to 81.8% and81.0% in this paper.
Note however that Arg-amon et al (1998) do not identify the headof subjects, subjects in embedded clauses, orsubjects and objects related to the verb onlythrough a trace, which makes their task eas-ier.
For a detailed comparison of the two meth-ods on the same task see (Daelemans et al,1999a).
That paper also shows that the chunk-ing method proposed here performs about aswell as other methods, and that the influenceof tagging errors on (NP) chunking is less than1%.To study the effect of the errors in the lowermodules other than the tagger, we used "per-fect" test data in a last experiment, i.e.
data an-notated with partial information taken directlyfrom the treebank.
The results are shown inTable 5.
We see that later modules suffer fromerrors of earlier modules (as could be expected):Ff~=l of PP chunking is 92% but could have244Structure in inputwords and POS only+NP chunks+VP chunks+ADVP and ADJPchunks+Prep chunks+PP chunks+ADVFUNCsI All Subj.
Obj.
Loc.
Temp.
?p Feat.
i# Inst.
A Prec aec FZ=i FZ=i FZ=i FZ=i FZ=t13 350091 6.1 60.7 41.3 49.1 52.8 49.4 34.0 38.417 227995 4.2 65.9 55.7 60.4 64.1 75.6 37.9 42.117 186364 4.5 72.1 62.9 67.2 78.6 75.6 40.8 46.817 185005 4.4 72.1 63.0 67.3 78.8 75.8'40.41 46.517 184455 4.4 72.5 64.3 68.2 81.2 75.7 40.4!
47.118 149341,3.6 73.6 65.6 69.3 81.6 80.3 40.6 48.319 149341 3.6 i 74.8 67.9 71.2 81.8 81.0 46.9 63.3Table 4: Results of grammatical relation assignment with more and more structure in the test dataadded by earlier modules in the cascade.
Columns show the number of features in the instances,the mlmber of instances constructed front the test input, the average distance between the verband the tbcus element, precision, recall and FZ=i over all relations, and F?~=i over some selectedrelations.ExperimentPP chunkingPP on perfect est dataADVFUNC assigmnentADVFUNC on perfect est dataAll RelationsPrecision I Recalll F~=i91.9 92.2 92.098.5 97.4 97.978.0 q)9.5 73.5r3.4 77.065.6 69.380.8 73.9 77.274.8 67.986.3 80.8 83.580.9GR with all chunks, without ADV- 73.6FUNC labelGR with all chunks, without ADV-IFUNC label on perfect est dataGR with all chunks and ADVFUNC IlabelGR with all chunks and ADVFUNClabel on perfect est data71.2Table 5: Comparison of performance of several modules on realistic input structurally enriched byprevious modules in the cascade) vs. on "perfect" input (enriched with partial treebank annotation).For PPs, this means perfect POS tags and chunk labels/boundaries, for ADVFUNC additionallyperfect PP chunks, for GR assignment also perfect ADVFUNC labels.been 97.9% if all previous chunks would havebeen correct (+5.9%).
For adverbial functions,the difference is 3.5%.
For grammatical rela-tion assignment, the last module in the cascade,the difference is, not surprisingly, the largest:7.9% for chunks only, 12.3% for chunks and AD-VFUNCs.
The latter percentage shows whatcould maximally be gained by further improvingthe chunker and ADVFUNCs finder.
'On realis-tic data, a realistic ADVFUNCs finder improvesGR assigment by 1.9%.
On perfect data, a per-fect ADVFUNCs finder increases performanceby 6.3%.5 Conclus ion and Future ResearchIn this paper we studied cascaded grammaticalrelations assignment.
We showed that even theuse of imperfect modules improves the overallresult of the cascade.In future research we plan to also trainour classifiers on imperfectly chunked material.This enables the classifier to better cope withsystematic errors in train and test material.
Weexpect that especially an improvement of the245adverbial function assignment will lead to bet-ter GR assignment.Finally, since cascading proved effective forGR assignment we intend to study the effectof cascading different ypes of XP chunkers onchunking performance.
We might e.g.
first findADJP chunks, then use that chunker's outputas additional input for the NP chunker, then usethe combined output as input to the VP chunkerand so on.
Other chunker orderings are possible,too.
Likewise, it might be better to find differ-ent grammatical relations ubsequently, insteadof simultaneously.ReferencesS.
Abney.
1991.
Parsing by chunks.
InPrinciple-Based Parsing, pages 257-278.Kluwer Academic Publishers, Dordrecht.S.
Argamon, I. Dagan, and Y. Krymolowski.1998.
A memory-based approach to learningshallow natural language patterns.
In Proc.of 36th annual meeting of the A CL, pages 67-73, Montreal.R.
Bod.
1992.
A computational model oflanguage performance: Data oriented pars-ing.
In Proceedings of the l~th Interna-tional Conference on Computational Linguis-tics, COLING-92, Nantes, France, pages855-859.Thorsten, Brants and Wojciech Skut.
1998.
Au-tomation of treebank annotation.
In Proceed-ings of the Conference on New Methods inLanguage Processing (NeMLaP-3), Australia.Sabine Buchholz.
1998.
Distinguishing com-plements from adjuncts using memory-basedlearning.
In Proceedings of the ESSLLI-98Workshop on Automated Acquisition of Syn-tax and Parsing, Saarbriicken, Germany.C.J.van Rijsbergen.
1979.
Information Re-trieval.
Buttersworth, London.M.
Collins.
1997.
Three generative, lexicalisedmodels for statistical parsing.
In Proceedingsof the 35th A CL and the 8th EA CL, Madrid,Spain~ July.W.
Daelemans, J. Zavrel, P. Berck, and S. Gillis.1996.
MBT: A memory-based part of speechtagger generator.
In E. Ejerhed and I. Dagan,editors, Proc.
of Fourth Workshop on VeryLarge Corpora, pages 14-27.
ACL SIGDAT.W.
Daelemans, J. Zavrel, K. Van der Sloot, andA.
Van den Bosch.
1998.
TiMBL: TilburgMemory Based Learner, version 1.0, referencemanual.
Technical Report ILK-9803, ILK,Tilburg University.W.
Daelemans, S. Buchholz, and J. Veenstra.1999a.
Memory-based shallow parsing.
InProceedings of CoNLL, Bergen, Norway.W.
Daelemans, A.
Van den Bosch, and J. Za-vrel.
1999b.
Forgetting exceptions is harm-ful in language learning.
Machine Learning,Special issue on Natural Language Learning,34:11-41.Gregory Grefenstette.
1996.
Light parsing asfinite-state filtering.
In Wolfgang Wahlster,editor, Workshop on Extended Finite StateModels of Language, ECAI'96, Budapest,Hungary.
John Wiley & Sons, Ltd.M.
Marcus, B. Santorini, and M.A.Marcinkiewicz.
1993.
Building a large anno-tared corpus of english: The penn treebank.Computational Linguistics, 19(2):313-330.L.A.
Ramshaw and M.P.
Marcus.
1995.
Textchunking using transformation-based l arn-ing.
In Proceedings of the 3rd ACL/SIGDATWorkshop on Very Large Corpora, Cam-bridge, Massachusetts, USA, pages 82-94.A.
Ratnaparkhi.
1997.
A linear observed timestatistical parser based on maximum en-tropy models.
In Proceedings of the SecondConference on Empirical Methods in NaturalLanguage Processing, EMNLP-2, Providence,Rhode Island, pages 1-10.Satoshi Sekine.
1998.
Corpus-Based Parsingand Sublanguage Studies.
Ph.D. thesis, NewYork University.E.
Tjong Kim Sang and J.B. Veenstra.
1999.Representing text chunks.
In Proceedings ofthe EA CL, Bergen, N.J. B. Veenstra.
1998.
Fast np chunking usingmemory-based learning techniques.
In Pro-ceedings of BENELEARN'98, pages 71-78,Wageningen, The Netherlands.J.
Zavrel and W. Daelemans.
1997.
Memory-based learning: Using similarity for smooth-ing.
In Proc.
of 35th annual meeting of theA CL, Madrid.246
