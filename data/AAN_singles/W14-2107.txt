Proceedings of the First Workshop on Argumentation Mining, pages 49?58,Baltimore, Maryland USA, June 26, 2014.c?2014 Association for Computational LinguisticsBack up your Stance: Recognizing Arguments in Online DiscussionsFilip Boltu?zi?c and Jan?SnajderUniversity of ZagrebFaculty of Electrical Engineering and ComputingText Analysis and Knowledge Engineering LabUnska 3, 10000 Zagreb, Croatia{filip.boltuzic,jan.snajder}@fer.hrAbstractIn online discussions, users often back uptheir stance with arguments.
Their argu-ments are often vague, implicit, and poorlyworded, yet they provide valuable insightsinto reasons underpinning users?
opinions.In this paper, we make a first step towardsargument-based opinion mining from on-line discussions and introduce a new taskof argument recognition.
We match user-created comments to a set of predefinedtopic-based arguments, which can be eitherattacked or supported in the comment.
Wepresent a manually-annotated corpus for ar-gument recognition in online discussions.We describe a supervised model based oncomment-argument similarity and entail-ment features.
Depending on problem for-mulation, model performance ranges from70.5% to 81.8% F1-score, and decreasesonly marginally when applied to an unseentopic.1 IntroductionWhether about coffee preparation, music taste, orlegal cases in courtrooms, arguing has always beenthe dominant way of rationalizing opinions.
Anargument consists of one or more premises lead-ing to exactly one conclusion, while argumentationconnects together several arguments (Van Eemerenet al., 2013).
Across domains, argumentation dif-fers in vocabulary, style, and purpose, ranging fromlegal (Walton, 2005) and scientific argumentation(Jim?enez-Aleixandre and Erduran, 2007) to media(Walton, 2007) and social argumentation (Shum,2008).
When argumentation involves interactiveargument exchange with elements of persuasion,we talk about debating.
In the increasingly popularonline debates, such as VBATES,1users can en-1http://vbate.idebate.org/gage in debates over controversial topics, introducenew arguments or use existing ones.Early computational approaches to argumenta-tion have developed in two branches: logic-basedapproaches (Bos and Gabsdil, 2000; Lauriar et al.,2001) and argumentative zoning (Teufel and others,2000).
The latter aims to recognize argumentativesections of specific purpose in scientific papers,such as goals, related work, or conclusion.
Moenset al.
(2007) introduced argumentation mining asa research area involved with the automatic ex-traction of argumentation structure from free text,residing between NLP, argumentation theory, andinformation retrieval.Prior work in argumentation mining has focusedon official documents, such as legal cases (Palauand Moens, 2009), or moderated sources, such asdebates (Cabrio and Villata, 2012).
However, byfar the largest source of opinions are online userdiscussions: comments on newspaper articles, so-cial networks, blogs, and discussion forums ?
allargumentation arenas without strict rules.
Despitethe fact that the user-generated content is not mod-erated nor structured, one can often find an abun-dance of opinions, most of them backed up witharguments.
By analyzing such arguments, we cangain valuable insight into the reasons underpinningusers?
opinions.
Understanding the reasons hasobvious benefits in social opinion mining, with ap-plications ranging from brand analysis to politicalopinion mining.Inspired by this idea, in this paper we take onthe task of argument-based opinion mining.
In-stead of merely determining the general opinion orstance of users towards a given topic, in argument-based opinion mining we wish to determine thearguments on which the users base their stance.Unlike in argumentation mining, we are not ulti-mately interested in recovering the argumentationstructure.
Instead, we wish to recognize what ar-guments the user has used to back up her opinion.49As an example, consider a discussion on the topic?Should gay marriage be legal??
and the followingcomment:Gay marriages must be legal in all 50states.
A marriage is covenant between2 people regardless of their genders.
Dis-crimination against gay marriage is un-constitutional and biased.
Tolerance,education and social justice make ourworld a better place.This comment supports the argument ?It is discrim-inatory to refuse gay couples the right to marry?and denies the argument ?Marriage should be be-tween a man and a woman?.
The technical chal-lenge here lies in the fact that, unlike in debatesor other more formal argumentation sources, thearguments provided by the users, if any, are lessformal, ambiguous, vague, implicit, or often simplypoorly worded.In this paper, we make a first step towardsargument-based opinion mining from online dis-cussions and introduce the task of argument recog-nition.
We define this task as identifying whatarguments, from a predefined set of arguments,have been used in users?
comments, and how.
Weassume that a topic-dependent set of argumentshas been prepared in advance.
Each argument isdescribed with a single phrase or a sentence.
Toback up her stance, the user will typically use oneor more of the predefined arguments, but in theirown wording and with varying degree of explicit-ness.
The task of argument recognition amounts tomatching these arguments to the predefined argu-ments, which can be either attacked or supportedby the comment.
Note that the user?s commentmay by itself be a single argument.
However, werefer to it as comment to emphasize the fact that ingeneral it may contain several arguments as well asnon-argumentative text.The contribution of our work is twofold.
First,we present COMARG, a manually-annotated cor-pus for argument recognition from online discus-sions, which we make freely available.
Secondly,we describe a supervised model for argument recog-nition based on comment-argument comparison.To address the fact that the arguments expressed inuser comments are mostly vague and implicit, weuse a series of semantic comment-argument com-parison features based on semantic textual similar-ity (STS) and textual entailment (TE).
To this end,we rely on state-of-the-art off-the-shelf STS and TEsystems.
We consider different feature subsets andargument recognition tasks of varying difficulty.Depending on task formulation, their performanceranges from 70.5% to 81.8% micro-averaged F1-score.
Taking into account the difficulty of the task,we believe these results are promising.
In partic-ular, we show that TE features work best whenalso taking into account the stance of the argument,and that a classifier trained to recognize argumentsin one topic can be applied to another one with adecrease in performance of less than 3% F1-score.The rest of the paper is structured as follows.
Inthe next section we review the related work.
In Sec-tion 3 we describe the construction and annotationof the COMARG corpus.
Section 4 describes theargument recognition model.
In Section 5 we dis-cuss the experimental results.
Section 6 concludesthe paper and outlines future work.2 Related WorkArgument-based opinion mining is closely relatedto argumentation mining, stance classification, andopinion mining.Palau and Moens (2009) approach argumenta-tion mining in three steps: (1) argument identi-fication (determining whether a sentence is argu-mentative), (2) argument proposition classification(categorize argumentative sentences as premises orconclusions), and (3) detection of argumentationstructure or ?argumentative parsing?
(determiningthe relations between the arguments).
The focusof their work is on legal text: the Araucaria cor-pus (Reed et al., 2008) and documents from theEuropean Court of Human Rights.More recently, Cabrio and Villata (2012) ex-plored the use of textual entailment for buildingargumentation networks and determining the ac-ceptability of arguments.
Textual entailment (TE)is a generic NLP framework for recognizing in-ference relations between two natural languagetexts (Dagan et al., 2006).
Cabrio and Villata basetheir approach on Dung?s argumentation theory(Dung, 1995) and apply it to arguments from on-line debates.
After linking the arguments with sup-port/attack relations using TE, they are able to com-pute a set of acceptable arguments.
Their systemhelps the participants to get an overview of a debateand the accepted arguments.Our work differs from the above-described workin that we do not aim to extract the argumenta-50tion structure.
Similarly to Cabrio and Villata(2012), we use TE as one of the features of oursystem to recognize the well-established argumentsin user generated comments.
However, aiming atargument-based opinion mining from noisy com-ments, we address a more general problem in whicheach comment may contain several arguments aswell as non-argumentative text.
Thus, in contrastto Cabrio and Villata (2012) who framed the prob-lem as a binary yes/no entailment task, we tacklea more difficult five-class classification problem.We believe this is a more realistic task from theperspective of opinion mining.A task similar to argument recognition is thatof stance classification, which involves identifyinga subjective disposition towards a particular topic(Lin et al., 2006; Malouf and Mullen, 2008; So-masundaran and Wiebe, 2010; Anand et al., 2011;Hasan and Ng, 2013).
Anand et al.
(2011) classi-fied stance on a corpus of posts across a wide rangeof topics.
They analyzed the usefulness of meta-post features, contextual features, dependency fea-tures, and word-based features for signaling dis-agreement.
Their results range from 54% to 69%accuracy.
Murakami and Raymond (2010) iden-tify general user opinions in online debates.
Theydistinguish between global positions (opinions onthe topic) and local positions (opinions on previ-ous remarks).
By calculating user pairwise ratesof agreement and disagreement, users are groupedinto ?support?
and ?oppose?
sets.In contrast to stance classification, argumentrecognition aims to uncover the reasons underly-ing an opinion.
This relates to the well-establishedarea of opinion mining.
The main goal of opinionmining or sentiment analysis (Pang and Lee, 2008)is to analyze the opinions and emotions from (mostoften user-created) text.
Opinions are often asso-ciated with user reviews (Kobayashi et al., 2007),unlike stances, which are more common for de-bates.
Hasan and Ng (2013) characterize stancerecognition as a more difficult task than opinionmining.
Recently, however, there has been interest-ing work on combining argumentation mining andopinion mining (Ches?nevar et al., 2013; Grosse etal., 2012; Hogenboom et al., 2010).3 COMARG CorpusFor training and evaluating argument recognitionmodels, we have compiled a corpus of user com-ments, manually annotated with arguments, towhich we refer as COMARG.
The COMARG cor-pus is freely available for research purposes.23.1 Data DescriptionAs a source of data, we use two web sites: Pro-con.org3and Idebate.org.4The former is a discus-sion site covering ideological, social, political, andother topics.
Users express their personal opinionson a selected topic, taking either the pro or conside.
Idebate.org is a debating website containingonline debates and an archive of past debates.
Eacharchived topic contains a set of prominent argu-ments presented in the debate.
Each argument islabeled as either for or against the topic.
The argu-ments are moderated and edited to provide the bestquality of information.The two data sources are complementary to eachother: Procon.org contains user comments, whileIdebate.org contains the arguments.
We manuallyidentified near-identical topics covered by both websites.
From this set, we chose two topics: ?Un-der God in Pledge?
(UGIP) and ?Gay Marriage?(GM).
We chose these two topics because they havea larger-than-average number of comments (above300) and are well-balanced between pro and constances.
For these two topics, we then took thecorresponding comments and arguments from Pro-con.org and Idebate.org, respectively.
As the userscan post comments not relevant for the topic, weskim-read the comments and removed the spam.We end up with a set of 175 comments and 6 argu-ments for the UGIP topic, and 198 comments and7 arguments for the GM topic.
The comments areoften verbose: the average number of words percomment is 116.
This is in contrast to the less noisydataset from Cabrio and Villata (2012), where theaverage comment length is 50 words.Each comment has an associated stance (pro orcon), depending on how it was classified in Pro-con.org.
Similarly, each argument either attacks orsupports the claim of the topic, depending on howit was classified in Idebate.org.
To simplify the ex-position, we will refer to them as ?pro arguments?and ?con arguments?.
Table 1 shows the argumentsfor UGIP and GM topics.Users may attack or support both pro and conarguments.
We will refer to the way how the argu-ment is used (attacked or supported) as argument2Freely available under the CC BY-SA-NC license fromhttp://takelab.fer.hr/data/comarg3http://www.procon.org4http://idebate.org51?Under God in Pledge?
(UGIP): Should the words?under God?
be in the U.S.
Pledge of Allegiance?
(A1.1) Likely to be seen as a state sanctionedcondemnation of religionPro(A1.2) The principles of democracy regulate thatthe wishes of American Christians, whoare a majority are honoredPro(A1.3) Under God is part of American traditionand historyPro(A1.4) Implies ultimate power on the part of thestateCon(A1.5) Removing under god would promote reli-gious toleranceCon(A1.6) Separation of state and religion Con?Gay Marriage?
(GM): Should gay marriage be legal?
(A2.1) It is discriminatory to refuse gay couplesthe right to marryPro(A2.2) Gay couples should be able to take ad-vantage of the fiscal and legal benefits ofmarriagePro(A2.3) Marriage is about more than procreation,therefore gay couples should not be de-nied the right to marry due to their biol-ogyPro(A2.4) Gay couples can declare their union with-out resort to marriageCon(A2.5) Gay marriage undermines the institutionof marriage, leading to an increase in outof wedlock births and divorce ratesCon(A2.6) Major world religions are against gaymarriagesCon(A2.7) Marriage should be between a man and awomanConTable 1: Predefined arguments for the two topics inthe COMARG corpuspolarity.
Typically, but not necessarily, users whotake the pro stance do so by supporting one of thepro arguments, and perhaps attacking some of thecon arguments, while for users who take the constance it is the other way around.3.2 AnnotationThe next step was to annotate, for each comment,the arguments used in the comment as well as theirpolarity.
For each topic we paired all commentswith all possible arguments for that topic, resultingin 1,050 and 1,386 comment-argument pairs for theUGIP and GM topics, respectively.
We then askedthe annotators (not the authors) to annotate eachpair.
The alternative would have been to ask theannotators to assign arguments to comments, butwe believe annotating pairs reduces the annotationefforts and improves annotation quality.55We initially attempted to crowdsource the annotation, butthe task turned out to be too complex for the workers, resultingin unacceptably low interannotator agreement.Label Description: Comment.
.
.A .
.
.
explicitly attacks the argumenta .
.
.
vaguely/implicitly attacks the argumentN .
.
.
makes no use of the arguments .
.
.
vaguely/implicitly supports the argumentS .
.
.
explicitly supports the argumentTable 2: Labels for comment-argument pairs in theCOMARG corpusNo, of course not.
The original one was good enough.
Theinsertion of Under God?
between ?Our nation?
and ?indivis-ible?
is symbolic of how religion divides this country.
?The Pledge of Allegiance reflects our morals and values.
There-fore, it should reflect the ideas of all Americans not 80%.
Thiscountry has no national religion, so why should we promote agod.
Also, Thomas Jefferson, a founding father, was athiest.I believe that since this country was founded under God whyshould we take that out of the pledge?
Men and women havefought and gave their lives for this country, so that way wecan have freedom and be able to have God in our lives.
Andsince this country was founded under God and the Ten Com-mandments in mind, it needs to stay in.
If it offends you well Iam sorry but get out of this country!Table 3: Example comments with low IAA fromUGIPAcknowledging the fact that user-provided argu-ments are often vague or implicit, we decided toannotate each comment-argument pair using a five-point scale.
The labels are shown in Table 2.
Thelabels encode the presence/absence of an argumentin a comment, its polarity, as well as the degree ofexplicitness.The annotation was carried out by three trainedannotators, in two steps.
In the first step, each anno-tator independently annotated the complete datasetof 2,436 comment-argument pairs.
To improvethe annotation quality, we singled out the problem-atic comment-argument pairs.
We considered asproblematic all comment-argument pairs for which(1) there is no agreement among the three annota-tors or (2) the ordinal distance between any of thelabels assigned by the annotators is greater thanone.
Table 3 shows some examples of problematiccomments.
As for the arguments, the most prob-lematic ones are A1.3 and A1.5 for the UGIP topicand arguments A2.1 and A2.7 for the GM topic(cf.
Table 1).In the second step, we asked the annotators toindependently revise their decisions for the prob-lematic comment-argument pairs.
Each annotatorre-annotated 515 pairs, of which for 86 the anno-tations were revised.
In total, the annotation and52IAA UGIP GM UGIP+GMFleiss?
Kappa 0.46 0.51 0.49Cohen?s Kappa 0.46 0.51 0.49Weighted Kappa 0.45 0.51 0.50Pearson?s r 0.68 0.74 0.71Table 4: Interannotator agreement on theCOMARG corpusLabelsTopic A a N s S TotalUGIP 48 86 691 58 130 1,013GM 89 73 849 98 176 1,285UGIP+GM 137 159 1,540 156 306 2,298Table 5: Distribution of labels in the COMARGcorpussubsequent revision took about 30 person-hours.Table 4 shows the interannotator agreement(IAA).
We compute Fleiss?
multirater kappa, Co-hen?s kappa (averaged over three annotator pairs),Cohen?s linearly weighted kappa (also averaged),and Pearson?s r. The latter two reflect the fact thatthe five labels constitute an ordinal scale.
Accord-ing to standard interpretation (Landis and Koch,1977), these values indicate moderate agreement,proving that argument recognition is a difficult task.Finally, to obtain the the gold standard annota-tion, we took the majority label for each comment-argument pair, discarding the pairs for which thereare ties.
We ended up with a dataset of 2,249comment-argument pairs.
Table 6 shows examplesof annotated comment-argument pairs.3.3 Annotation AnalysisTable 5 shows the distribution of comment-argument pairs across labels.
Expectedly, themajority (67.0%) of comment-argument pairs arecases in which the argument is not used (label N).Attacked arguments (labels A or a) make up 12.9%,while supported arguments (labels S or s) make up20.1% of cases.
Among the cases not labeled as N,arguments are used explicitly in 58.4% (labels Aand S) and vague/implicit (labels a and s) in 41.5%of cases.
There is a marked difference between thetwo topics in this regard: in UGIP, arguments areexplicit in 55.3%, while in GM in 60.7% of cases.Note that this might be affected by the choice ofthe predefined arguments as well as how they areworded.The average number of arguments per commentis 1.9 (1.8 for UGIP and 2.0 for GM).
In GM,62.8% of arguments used are pro arguments, whilein UGIP pro arguments make up 52.2% of cases.4 Argument Recognition ModelWe cast the argument recognition task as a multi-class classification problem.
Given a comment-argument pair as input, the classifier should predictthe correct label from the set of five possible labels(cf.
Table 2).
The main idea is for the classifier torely on comment-argument comparison features,which in principle makes the model less domaindependent than if we were to use features extracteddirectly from the comment or the arguments.We use three kinds of features: textual entail-ment (TE) features, semantic text similarity (STS)features, and one ?stance alignment?
(SA) feature.The latter is a binary feature whose value is set toone if a pro comment is paired with a pro argumentor if a con comment is paired with a con argument.This SA feature presupposes that comment stanceis known a priori.
The TE and STS features aredescribed bellow.4.1 Textual EntailmentFollowing the work of Cabrio and Villata (2012),we use textual entailment (TE) to determinewhether the comment (the text) entails the argu-ment phrase (the hypothesis).
To this end weuse the Excitement Open Platform (EOP), a richsuite of textual entailment tools designed for mod-ular use (Pad?o et al., 2014).
From EOP weused seven pre-trained entailment decision algo-rithms (EDAs).
Some EDAs contain only syn-tactical features, whereas others rely on resourcessuch as WordNet (Fellbaum, 1998) and VerbOcean(Chklovski and Pantel, 2004).
Each EDA outputsa binary decision (Entailment or NonEntailment)along with the degree of confidence.
We use theoutputs (decisions and confidences) of all sevenEDAs as the features of our classifier (14 featuresin total).
We also experimented with using ad-ditional features (the disjunction of all classifierdecisions, the maximum confidence value, and themean confidence value), but using these did notimprove the performance.In principle, we expect the comment text (whichis usually longer) to entail the argument phrase(which is usually shorter).
This is also confirmedby the ratio of positive entailment decision acrosslabels (averaged over seven EDAs), shown in53Id Comment Argument Label2.23.4 All these arguments on my left are and have always been FALSE.
Marriageis between a MAN and a WOMAN by divine definition.
Sorry but, end ofstory.It is discriminatory to refusegay couples the right tomarry.s2.111.4 Marriage isn?t the joining of two people who have intentions of raisingand nurturing children.
It never has been.
There have been many marriedcouples whos have not had children.
(...) If straight couples can attempt towork out a marriage, why can?t homosexual couple have this same privilege?(...
)It is discriminatory to refusegay couples the right tomarrys2.114.2 (...) I truly believe that the powers behind the cause to re-define marriagestem from a stronger desire to attack a religious institution that does notsupport homosexuality, rather than a desire to achieve the same benefits asmarriage for same sex couples.
(...)?Gay couples should be ableto take advantage of the fis-cal and legal benefits of mar-riage.S2.101.2 (...) One part of marriage is getting benefits from the other.
Many marriedcouples never have children but still get the benefits of marriage, should wetake those benefits away because they don?t have children?
Another is thepromise to be with each other for an eternity?
etc.
Marriage is also aboutbeing able to celebrate having each other.
And last, marriage is about beingthere for each other.
(...)?Gay couples should be ableto take advantage of the fis-cal and legal benefits of mar-riage.S2.157.2 (...) There are no legal reasons why two homosexual people should not beallowed to marry, only religious ones (...)Gay couples should be ableto take advantage of the fis-cal and legal benefits of mar-riage.N1.45.2 I am not bothered by under God but by the highfalutin christians that donot realize that phrase was never in the original pledge - it was not addeduntil 1954.
So stop being so pompous and do not offend my parents andgrandparents who never used ?under God?
when they said the pledge.
Let itstay, but know the history of the Cold War and fear of communism.
?Under God?
is part ofAmerican tradition and his-tory.aTable 6: Example of comment-argument annotations from the COMARG corpusA a N s SLabel0.00.20.40.60.81.0Ratio ofpositiveentailmentdecisions (%)Figure 1: Ratio of positive entailment decisionsacross labels, scaled to a [0, 1] intervalFig.
1.
Pro arguments have a higher ratio ofpositive entailment decisions than con arguments.Also, vaguely/implicitly supported arguments havea lower rate of entailment decisions than explicitlysupported arguments.4.2 Semantic Textual SimilarityFormally speaking, the argument should either beentailed or not entailed from the comment.
Theformer case also includes a simple argument para-phrase.
In the latter case, the argument may becontradicted or it may simply be a non sequitur.While we might expect these relations to be rec-ognizable in texts from more formal genres, suchas legal documents and parliamentary debates, itis questionable to what extent these relations canbe detected in user-generated content, where thearguments are stated vaguely and implicitly.To account for this, we use a series of argument-comment comparison features based on semantictextual similarity (STS).
STS measures ?the degreeof semantic equivalence between two texts?
(Agirreet al., 2012).
It is a looser notion than TE and, un-like TE, it is a non-directional (symmetric) relation.We rely on the freely available TakeLab STS sys-tem by?Sari?c et al.
(2012).
Given a comment andan argument, the STS system outputs a continuoussimilarity score.
We also compute the similaritybetween the argument and each sentence from thecomment, which gives us a vector of similarities.The vector length equals the largest number of sen-tences in a comment, which in COMARG is 29.Additionally, we compute the maximum and the54A a N s SLabel0.00.20.40.60.81.0AveragescoreComment similaritySentence similarityFigure 2: Average similarity score on sentenceand comment level across labels, scaled to a [0, 1]intervalmean of sentence-level similarities.
In total, we use31 STS features.Fig.
2 shows the average comment- and sentence-level similarity scores across labels on COMARG,scaled to a [0, 1] interval.
Interestingly, attackedarguments on average receive a larger score thansupported arguments.5 Experimental Evaluation5.1 Experimental SetupWe consider three formulations of the argumentdetection task.
In the first setting (A-a-N-s-S), weconsider the classification of a comment-argumentinto one of the five labels, i.e., we wish to determinewhether an argument has been used, its polarity, aswell as the degree of explicitness.
In the secondsetting (As-N-sS), we conflate the two labels ofequal polarity, thus we only consider whether anargument has been used and with what polarity.In the third setting (A-N-S), we only consider thecomment-argument pairs in which arguments areeither not used or used explicitly.
This setting is notpractically relevant, but we include it for purposesof comparison.We compare to two baselines: (1) a majorityclass classifier (MCC), which assigns label N to ev-ery instance, and (2) a bag-of-words overlap classi-fier (BoWO), which uses the word overlap betweenthe comment and the argument as the only feature.For classification, we use the Support Vector Ma-chine (SVM) algorithm with a Radial Basis Func-tion kernel.
In each setting, we train and evalu-ate the model using nested 5?3 cross-validation.The hyperparameters C and ?
of the SVM are op-timized using grid search.
We rely on the well-A-a-N-s-S Aa-N-sS A-N-SModel UGIP GM UGIP GM UGIP GMMCC baseline 68.2 69.4 68.2 69.4 79.5 76.6BoWO baseline 68.2 69.4 67.8 69.5 79.6 76.9TE 69.1 81.1 69.6 72.3 80.1 73.4STS 67.8 68.7 67.3 69.9 79.2 75.8SA 68.2 69.4 68.2 69.4 79.5 76.6STS+SA 68.2 69.5 67.5 68.7 79.6 76.1TE+SA 68.9 72.4 71.0 73.7 81.8 80.3TE+STS+SA 70.5 72.5 68.9 73.4 81.4 79.7Table 7: Argument recognition F1-score (separatemodels for UGIP and GM topics)UGIP?
GM GM?
UGIPModel A-a-N-s-S Aa-N-sS A-a-N-s-S Aa-N-sSSTS+SA 69.4 69.4 68.2 68.2TE+SA 72.6 73.5 70.2 71.2STS+TE+SA 71.5 72.2 68.2 69.6Table 8: Argument recognition F1-score on UGIPand GM topics (cross-topic setting)known LibSVM implementation (Chang and Lin,2011).5.2 ResultsTable 7 shows the micro-averaged F1-score for thethree problem formulations, for models trained sep-arately on UGIP and GM topics.
The two baselinesperform similarly.
The models that use only theSTS or the SA features perform similar to the base-line.
The TE model outperforms the baselines inall but one setting and on both topics: the differ-ence ranges from 0.6 to 11.7 percentage points,depending on problem formulation, while the vari-ation between the two topics is negligible.
TheSTS model does not benefit from adding the SAfeature, while the TE model does so in simplersettings (Aa-N-sS and A-N-S), where the averageF1-scores increases by about 3 percentage points.This can be explained by referring to Fig.
1, whichshows that even for the attacked arguments (labelsA and a) entailment decisions are sometimes pos-itive.
In such cases, the stance alignment featurehelps to distinguish between entailment (supportedargument) and contradiction (attacked argument).Combining all three feature types gives the best re-sults for the A-a-N-s-S setting and the UGIP topic.The above evaluation was carried out in a within-topic setting.
To test how the models perform whenapplied to comments and arguments from unseentopics, we trained each model on one topic and55A-a-N-s-S Aa-N-sS A-N-SModel P R F1 micro-F1 P R F1 micro-F1 P R F1 micro-F1MCC baseline 13.8 20.0 16.3 68.9 23.0 33.3 27.2 68.9 26.0 33.3 29.2 77.9TE+SA 47.6 26.6 27.9 71.1 68.8 46.6 49.4 73.3 66.1 47.3 51.1 81.6STS+TE+SA 46.3 27.2 28.6 71.6 61.6 43.5 45.5 71.4 63.7 44.9 48.2 80.4Table 9: Argument recognition F1-score for TE+SA and STS+TE+SA models on UGIP+GM topicsevaluated on the other.
The results are shown inTable 8 (we show results only for the two prob-lem formulations of practical interest).
The dif-ference in performance is small (0.7 on average).The best-performing model (TE+SA) does not suf-fer a decrease in performance.
This suggests thatthe models are quite topic independent, but a moredetailed study is required to verify this finding.Finally, we trained and tested the TE+SA andSTS+TE+SA models on the complete COMARGdataset.
The results are shown in Table 9.
Wereport macro-averaged precision, recall, and F1-score, as well as micro-averaged F1-score.6Gen-erally, our models perform less well on smallerclasses (A, a, s, and S), hence the macro-averagedF1-scores are much lower than the micro-averagedF1-scores.
The recall is lower than the precision:the false negatives are mostly due to our modelswrongly classifying comment-argument pairs as N.The STS+TE+SA model slightly outperforms theTE+SA model on the A-a-N-s-S problem, while onthe other problem formulations the TE+SA modelperforms best.5.3 Error AnalysisThe vague/implicit arguments posed the greatestchallenge for all models.
A case in point is thecomment-argument pair 2.23.4 from Table 6.
Judg-ing solely from the comment text, it is unclear whatthe user actually meant.
Perhaps the user is attack-ing the argument, but there are certain additionalassumptions that would need to be met for the ar-gument to be entailed.The second major problem is distinguishing be-tween arguments that are mentioned and those thatare not.
Consider the comment-argument pairs2.111.4 and 2.114.2 from Table 6.
In the formercase, classifier mistakenly predicts S instead of s.The decision is likely due to the low differencein argument-comment similarities for these twoclasses.
In the latter example the classifier wrongly6We replace undefined values with zeros when computingthe macro-averages.predicts that the argument is used in the comment.The TE model in the majority of cases outper-forms the STS model.
Nonetheless, in case ofthe comment-argument pair 2.157.2 from Table 6,the STS-based model outperformed the entailmentmodel.
In this case, the word overlap between theargument and the comment in quite high, althoughthey completely differ in meaning.
Conversely,argument-comment 2.101.2 is a good example ofwhen entailment was correctly recognized, whereasthe STS model has failed.6 ConclusionIn this paper we addressed the argument recogni-tion task as a first step towards argument-basedopinion mining from online discussions.
We havepresented the COMARG corpus, which consists ofmanually annotated comment-argument pairs.
Onthis corpus we have trained a supervised modelfor three argument recognition tasks of varyingdifficulty.
The model uses textual entailment andsemantic textual similarity features.
The exper-iments as well as the inter-annotator agreementshow that argument recognition is a difficult task.Our best models outperform the baselines and per-form in a 70.5% to 81.8% micro-averaged F1-scorerange, depending on problem formulation.
Theoutputs of several entailment decision algorithms,combined with a stance alignment feature, provedto be the best features.
Additional semantic tex-tual similarity features seem to be useful in whenwe distinguish between vague/implicit and explicitarguments.
The model performance is marginallyaffected when applied to an unseen topic.This paper has only touched the surface of argu-ment recognition.
We plan to extend the COMARGcorpus with more topics and additional annotation,such as argument segments.
Besides experimentingwith different models and feature sets, we intendto investigate how argument interactions can be ex-ploited to improve argument recognition, as well ashow argument recognition can be used for stanceclassification.56ReferencesEneko Agirre, Mona Diab, Daniel Cer, and AitorGonzalez-Agirre.
2012.
Semeval-2012 task 6: A pi-lot on semantic textual similarity.
In Proceedings ofthe First Joint Conference on Lexical and Computa-tional Semantics-Volume 1: Proceedings of the mainconference and the shared task, and Volume 2: Pro-ceedings of the Sixth International Workshop on Se-mantic Evaluation, pages 385?393.
Association forComputational Linguistics.Pranav Anand, Marilyn Walker, Rob Abbott, JeanE Fox Tree, Robeson Bowmani, and Michael Minor.2011.
Cats rule and dogs drool!
: Classifying stancein online debate.
In Proceedings of the 2nd Work-shop on Computational Approaches to Subjectivityand Sentiment Analysis, pages 1?9.
Association forComputational Linguistics.Johan Bos and Malte Gabsdil.
2000.
First-order infer-ence and the interpretation of questions and answers.Proceedings of Gotelog, pages 43?50.Elena Cabrio and Serena Villata.
2012.
Combiningtextual entailment and argumentation theory for sup-porting online debates interactions.
In Proceedingsof the 50th Annual Meeting of the Association forComputational Linguistics: Short Papers-Volume 2,pages 208?212.
Association for Computational Lin-guistics.Chih-Chung Chang and Chih-Jen Lin.
2011.
Libsvm:a library for support vector machines.
ACM Transac-tions on Intelligent Systems and Technology (TIST),2(3):27.Carlos I Ches?nevar, Mar?
?a Paula Gonz?alez, KathrinGrosse, and Ana Gabriela Maguitman.
2013.
A firstapproach to mining opinions as multisets throughargumentation.
In Agreement Technologies, pages195?209.
Springer.Timothy Chklovski and Patrick Pantel.
2004.
Verbo-cean: Mining the web for fine-grained semantic verbrelations.
In EMNLP, volume 2004, pages 33?40.Ido Dagan, Oren Glickman, and Bernardo Magnini.2006.
The PASCAL recognising textual entailmentchallenge.
In Machine Learning Challenges.
Eval-uating Predictive Uncertainty, Visual Object Classi-fication, and Recognising Tectual Entailment, pages177?190.
Springer.Phan Minh Dung.
1995.
On the acceptability of ar-guments and its fundamental role in nonmonotonicreasoning, logic programming and n-person games.Artificial Intelligence, 77(2):321?357.Christiane Fellbaum.
1998.
WordNet: An ElectronicLexical Database.
MIT Press.Kathrin Grosse, Carlos Iv?an Ches?nevar, andAna Gabriela Maguitman.
2012.
An argument-based approach to mining opinions from Twitter.
InAT, pages 408?422.Kazi Saidul Hasan and Vincent Ng.
2013.
Extra-linguistic constraints on stance recognition in ideo-logical debates.
In Proceedings of the 51st AnnualMeeting of the Association for Computational Lin-guistics, pages 816?821.Alexander Hogenboom, Frederik Hogenboom, UzayKaymak, Paul Wouters, and Franciska De Jong.2010.
Mining economic sentiment using argumen-tation structures.
In Advances in Conceptual Model-ing ?
Applications and Challenges, pages 200?209.Springer.Mar?
?a Pilar Jim?enez-Aleixandre and Sibel Erduran.2007.
Argumentation in science education: Anoverview.
In Argumentation in Science Education,pages 3?27.
Springer.Nozomi Kobayashi, Kentaro Inui, and Yuji Matsumoto.2007.
Extracting aspect-evaluation and aspect-ofrelations in opinion mining.
In EMNLP-CoNLL,pages 1065?1074.J.
Richard Landis and Gary G. Koch.
1977.
The mea-surement of observer agreement for categorical data.Biometrics, 33(1):159?174.Stanislao Lauriar, Johan Bos, Ewan Klein, Guido Bug-mann, and Theocharis Kyriacou.
2001.
Trainingpersonal robots using natural language instruction.IEEE Intelligent Systems, 16(5):38?45.Wei-Hao Lin, Theresa Wilson, Janyce Wiebe, andAlexander Hauptmann.
2006.
Which side are youon?
: Identifying perspectives at the document andsentence levels.
In Proceedings of the Tenth Confer-ence on Computational Natural Language Learning,pages 109?116.
Association for Computational Lin-guistics.Robert Malouf and Tony Mullen.
2008.
Taking sides:User classification for informal online political dis-course.
Internet Research, 18(2):177?190.Marie-Francine Moens, Erik Boiy, Raquel MochalesPalau, and Chris Reed.
2007.
Automatic detec-tion of arguments in legal texts.
In Proceedings ofthe 11th International Conference on Artificial Intel-ligence and Law, pages 225?230.
ACM.Akiko Murakami and Rudy Raymond.
2010.
Supportor oppose?
: Classifying positions in online debatesfrom reply activities and opinion expressions.
InProceedings of the 23rd International Conference onComputational Linguistics: Posters, pages 869?875.Association for Computational Linguistics.Sebastian Pad?o, Tae-Gil Noh, Asher Stern, Rui Wang,and Roberto Zanoli.
2014.
Design and realization ofa modular architecture for textual entailment.
Natu-ral Language Engineering, FirstView:1?34, 2.Raquel Mochales Palau and Marie-Francine Moens.2009.
Argumentation mining: The detection, clas-sification and structure of arguments in text.
In Pro-ceedings of the 12th International Conference on Ar-tificial Intelligence and Law, pages 98?107.
ACM.57Bo Pang and Lillian Lee.
2008.
Opinion mining andsentiment analysis.
Foundations and trends in infor-mation retrieval, 2(1-2):1?135.Chris Reed, Raquel Mochales Palau, Glenn Rowe, andMarie-Francine Moens.
2008.
Language resourcesfor studying argument.
In Proceedings of the 6thConference on Language Resources and Evaluation(LREC 2008), pages 91?100.Frane?Sari?c, Goran Glava?s, Mladen Karan, Jan?Snajder,and Bojana Dalbelo Ba?si?c.
2012.
Takelab: Sys-tems for measuring semantic text similarity.
In Pro-ceedings of the Sixth International Workshop on Se-mantic Evaluation (SemEval 2012), pages 441?448,Montr?eal, Canada, 7-8 June.
Association for Com-putational Linguistics.Simon Buckingham Shum.
2008.
Cohere: Towardsweb 2.0 argumentation.
volume 8, pages 97?108.Swapna Somasundaran and Janyce Wiebe.
2010.
Rec-ognizing stances in ideological on-line debates.
InProceedings of the NAACL HLT 2010 Workshop onComputational Approaches to Analysis and Genera-tion of Emotion in Text, pages 116?124.
Associationfor Computational Linguistics.Simone Teufel et al.
2000.
Argumentative zoning: In-formation extraction from scientific text.
Ph.D. the-sis, University of Edinburgh.Frans H. Van Eemeren, Rob Grootendorst, Ralph H.Johnson, Christian Plantin, and Charles A. Willard.2013.
Fundamentals of argumentation theory: Ahandbook of historical backgrounds and contempo-rary developments.
Routledge.Douglas Walton.
2005.
Argumentation methods forartificial intelligence in law.
Springer.Douglas Walton.
2007.
Media argumentation: dialec-tic, persuasion and rhetoric.
Cambridge UniversityPress.58
