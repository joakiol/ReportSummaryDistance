Proceedings of the 53rd Annual Meeting of the Association for Computational Linguisticsand the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 112?118,Beijing, China, July 26-31, 2015.c?2015 Association for Computational LinguisticsLearning language through picturesGrzegorz Chrupa?ag.chrupala@uvt.nl?Akos K?ad?ara.kadar@uvt.nlTilburg Center for Cognition and CommunicationTilburg UniversityAfra Alishahia.alishahi@uvt.nlAbstractWe propose IMAGINET, a model of learn-ing visually grounded representations oflanguage from coupled textual and visualinput.
The model consists of two GatedRecurrent Unit networks with shared wordembeddings, and uses a multi-task objec-tive by receiving a textual description ofa scene and trying to concurrently predictits visual representation and the next wordin the sentence.
Mimicking an importantaspect of human language learning, it ac-quires meaning representations for indi-vidual words from descriptions of visualscenes.
Moreover, it learns to effectivelyuse sequential structure in semantic inter-pretation of multi-word phrases.1 IntroductionVision is the most important sense for humansand visual sensory input plays an important rolein language acquisition by grounding meanings ofwords and phrases in perception.
Similarly, inpractical applications processing multimodal datawhere text is accompanied by images or videos isincreasingly important.
In this paper we proposea novel model of learning visually-grounded rep-resentations of language from paired textual andvisual input.
The model learns language throughcomprehension and production, by receiving a tex-tual description of a scene and trying to ?imagine?a visual representation of it, while predicting thenext word at the same time.The full model, which we dub IMAGINET, con-sists of two Gated Recurrent Unit (GRU) networkscoupled via shared word embeddings.
IMAGINETuses a multi-task Caruana (1997) objective: bothnetworks read the sentence word-by-word in par-allel; one of them predicts the feature represen-tation of the image depicting the described sceneafter reading the whole sentence, while the otherone predicts the next word at each position in theword sequence.
The importance of the visual andtextual objectives can be traded off, and either ofthem can be switched off entirely, enabling us toinvestigate the impact of visual vs textual infor-mation on the learned language representations.Our approach to modeling human languagelearning has connections to recent models of im-age captioning (see Section 2).
Unlike in many ofthese models, in IMAGINET the image is the targetto predict rather then the input, and the model canbuild a visually-grounded representation of a sen-tence independently of an image.
We can directlycompare the performance of IMAGINET against asimple multivariate linear regression model withbag-of-words features and thus quantify the con-tribution of the added expressive power of a recur-rent neural network.We evaluate our model?s knowledge of wordmeaning and sentence structure through simulat-ing human judgments of word similarity, retriev-ing images corresponding to single words as wellas full sentences, and retrieving paraphrases of im-age captions.
In all these tasks the model outper-forms the baseline; the model significantly corre-lates with human ratings of word similarity, andpredicts appropriate visual interpretations of sin-gle and multi-word phrases.
The acquired knowl-edge of sentence structure boosts the model?s per-formance in both image and caption retrieval.2 Related workSeveral computational models have been proposedto study early language acquisition.
The acqui-sition of word meaning has been mainly mod-eled using connectionist networks that learn toassociate word forms with semantic or percep-tual features (e.g., Li et al, 2004; Coventry et al,2005; Regier, 2005), and rule-based or proba-bilistic implementations which use statistical reg-112ularities observed in the input to detect associa-tions between linguistic labels and visual featuresor concepts (e.g., Siskind, 1996; Yu, 2008; Fazlyet al, 2010).
These models either use toy lan-guages as input (e.g., Siskind, 1996), or child-directed utterances from the CHILDES database(MacWhinney, 2014) paired with artificially gen-erated semantic information.
Some models haveinvestigated the acquisition of terminology for vi-sual concepts from simple videos (Fleischmanand Roy, 2005; Skocaj et al, 2011).
Lazaridouet al (2015) adapt the skip-gram word-embeddingmodel (Mikolov et al, 2013) for learning wordrepresentations via a multi-task objective similarto ours, learning from a dataset where some wordsare individually aligned with corresponding im-ages.
All these models ignore sentence structureand treat inputs as bags of words.A few models have looked at the concurrent ac-quisition of words and some aspect of sentencestructure, such as lexical categories (Alishahi andChrupa?a, 2012) or syntactic properties (Howellet al, 2005; Kwiatkowski et al, 2012), from utter-ances paired with an artificially generated repre-sentation of their meaning.
To our knowledge, noexisting model has been proposed for concurrentlearning of grounded word meanings and sentencestructure from large scale data and realistic visualinput.Recently, the engineering task of generatingcaptions for images has received a lot of atten-tion (Karpathy and Fei-Fei, 2014; Mao et al,2014; Kiros et al, 2014; Donahue et al, 2014;Vinyals et al, 2014; Venugopalan et al, 2014;Chen and Zitnick, 2014; Fang et al, 2014).
Fromthe point of view of modeling, the research mostrelevant to our interests is that of Chen and Zitnick(2014).
They develop a model based on a context-dependent recurrent neural network (Mikolov andZweig, 2012) which simultaneously processes tex-tual and visual input and updates two parallel hid-den states.
Unlike theirs, our model receives thevisual target only at the end of the sentence and isthus encouraged to store in the final hidden stateof the visual pathway all aspects of the sentenceneeded to predict the image features successfully.Our setup is more suitable for the goal of learningrepresentations of complete sentences.3 ModelsIMAGINET consists of two parallel recurrent path-Figure 1: Structure of IMAGINETways coupled via shared word embeddings.
Bothpathways are composed of Gated Recurrent Units(GRU) first introduced by Cho et al (2014) andChung et al (2014).
GRUs are related to theLong Short-Term Memory units (Hochreiter andSchmidhuber, 1997), but do not employ a sepa-rate memory cell.
In a GRU, activation at time t isthe linear combination of previous activation, andcandidate activation:ht= (1?
zt)?
ht?1+ zt?
?ht(1)where ?
is elementwise multiplication.
The up-date gate determines how much the activation isupdated:zt= ?s(Wzxt+Uzht?1) (2)The candidate activation is computed as:?ht= ?(Wxt+U(rt?
ht?1)) (3)The reset gate is defined as:rt= ?s(Wrxt+Urht?1) (4)Our gated recurrent units use steep sigmoids forgate activations:?s(z) =11 + exp(?3.75z)and rectified linear units clipped between 0 and 5for the unit activations:?
(z) = clip(0.5(z + abs(z)), 0, 5)Figure 1 illustrates the structure of the network.The word embeddings is a matrix of learned pa-rametersWewith each column corresponding to avector for a particular word.
The input word sym-bol Stof sentence S at each step t indexes into theembeddings matrix and the vector xtforms inputto both GRU networks:xt= We[:, St] (5)113This input is mapped into two parallel hiddenstates, hVtalong the visual pathway, and hTtalongthe textual pathway:hVt= GRUV(hVt?1,xt) (6)hTt= GRUT(hTt?1,xt) (7)The final hidden state along the visual pathway hV?is then mapped to the predicted target image rep-resentation?i by the fully connected layer with pa-rameters V and the clipped rectifier activation:?i = ?(VhV?)
(8)Each hidden state along the textual pathway hTtisused to predict the next symbol in the sentence Svia a softmax layer with parameters L:p(St+1|S1:t) = softmax(LhTt) (9)The loss function whose gradient is backpropa-gated through time to the GRUs and the embed-dings is a composite objective with terms penaliz-ing error on the visual and the textual targets si-multaneously:L(?)
= ?LT(?)
+ (1?
?)LV(?)
(10)where ?
is the set of all IMAGINET parameters.
LTis the cross entropy function:LT(?)
= ?1??
?t=1log p(St|S1:t) (11)while LVis the mean squared error:LV(?)
=1KK?k=1(?ik?
ik)2(12)By setting ?
to 0 we can switch the whole textualpathway off and obtain the VISUAL model vari-ant.
Analogously, setting ?
to 1 gives the TEX-TUAL model.
Intermediate values of ?
(in the ex-periments below we use 0.1) give the full MUL-TITASK version.
Finally, as baseline for some ofthe tasks we use a simple linear regression modelLINREG with a bag-of-words representation of thesentence:?i = Ax+ b (13)where?i is the vector of the predicted image fea-tures, x is the vector of word counts for the in-put sentence and (A, b) the parameters of thelinear model estimated via L2-penalized sum-of-squared-errors loss.SimLex MEN 3KVISUAL 0.32 0.57MULTITASK 0.39 0.63TEXTUAL 0.31 0.53LINREG 0.18 0.23Table 1: Word similarity correlations with humanjudgments measured by Spearman?s ?
(all correla-tions are significant at level p < 0.01).4 ExperimentsSettings The model was implemented in Theano(Bastien et al, 2012; Bergstra et al, 2010) and op-timized by Adam (Kingma and Ba, 2014).1Thefixed 4096-dimensional target image representa-tion come from the pre-softmax layer of the 16-layer CNN (Simonyan and Zisserman, 2014).
Weused 1024 dimensions for the embeddings and forthe hidden states of each of the GRU networks.
Weran 8 iterations of training, and we report eitherfull learning curves, or the results for each modelafter iteration 7 (where they performed best for theimage retrieval task).
For training we use the stan-dard MS-COCO training data.
For validation andtest, we take a sample of 5000 images each fromthe validation data.4.1 Word representationsWe assess the quality of the learned embeddingsfor single words via two tasks: (i) we measuresimilarity between embeddings of word pairs andcompare them to elicited human ratings; (ii) weexamine how well the model learns visual repre-sentations of words by projecting word embed-dings into the visual space, and retrieving imagesof single concepts from ImageNet.Word similarity judgment For similarity judg-ment correlations, we selected two existing bench-marks that have the largest vocabulary overlapwith our data: MEN 3K (Bruni et al, 2014) andSimLex-999 (Hill et al, 2014).
We measure thesimilarity between word pairs by computing thecosine similarity between their embeddings fromthree versions of our model, VISUAL, MULTI-TASK and TEXTUAL, and the baseline LINREG.Table 1 summarizes the results.
All IMAGINETmodels significantly correlate with human simi-larity judgments, and outperform LINREG.
Ex-amples of word pairs for which MULTITASK cap-1Code available at github.com/gchrupala/imaginet.114VISUAL MULTITASK LINREG0.38 0.38 0.33Table 2: Accuracy@5 of retrieving images withcompatible labels from ImageNet.tures human similarity judgments better than VI-SUAL include antonyms (dusk, dawn), colloca-tions (sexy, smile), or related but not visually sim-ilar words (college, exhibition).Single-word image retrieval In order to visual-ize the acquired meaning for individual words, weuse images from the ILSVRC2012 subset of Im-ageNet (Russakovsky et al, 2014) as benchmark.Labels of the images in ImageNet are synsets fromWordNet, which identify a single concept in theimage rather than providing descriptions of itsfull content.
Since the synset labels in ImageNetare much more precise than the descriptions pro-vided in the captions in our training data (e.g.,elkhound), we use synset hypernyms from Word-Net as substitute labels when the original labelsare not in our vocabulary.We extracted the features from the 50,000 im-ages of the ImageNet validation set.
The labelsin this set result in 393 distinct (original or hyper-nym) words from our vocabulary.
Each word wasprojected to the visual space by feeding it throughthe model as a one-word sentence.
We rankedthe vectors corresponding to all 50,000 imagesbased on their similarity to the predicted vector,and measured the accuracy of retrieving an imagewith the correct label among the top 5 ranked im-ages (Accuracy@5).
Table 2 summarizes the re-sults: VISUAL and MULTITASK learn more accu-rate word meaning representations than LINREG.4.2 Sentence structureIn the following experiments, we examine theknowledge of sentence structure learned by IMAG-INET, and its impact on the model performance onimage and paraphrase retrieval.Image retrieval We retrieve images based onthe similarity of their vectors with those predictedby IMAGINET in two conditions: sentences are fedto the model in their original order, or scrambled.Figure 2 (left) shows the proportion of sentencesfor which the correct image was in the top 5 high-est ranked images for each model, as a function ofthe number of training iterations: both models out-Figure 2: Left: Accuracy@5 of image retrievalwith original versus scrambled captions.
Right:Recall@4 of paraphrase retrieval with originalvs scrambled captions.perform the baseline.
MULTITASK is initially bet-ter in retrieving the correct image, but eventuallythe gap disappears.
Both models perform substan-tially better when tested on the original captionscompared to the scrambled ones, indicating thatmodels learn to exploit aspects of sentence struc-ture.
This ability is to be expected for MULTI-TASK, but the VISUAL model shows a similar ef-fect to some extent.
In the case of VISUAL, thissensitivity to structural aspects of sentence mean-ing is entirely driven by how they are reflected inthe image, as this models only receives the visualsupervision signal.Qualitative analysis of the role of sequentialstructure suggests that the models are sensitiveto the fact that periods terminate a sentence, thatsentences tend not to start with conjunctions, thattopics appear in sentence-initial position, and thatwords have different importance as modifiers ver-sus heads.
Figure 3 shows an example; see supple-mentary material for more.IMAGINET vs captioning systems While it isnot our goal to engineer a state-of-the-art imageretrieval system, we want to situate IMAGINET?sperformance within the landscape of image re-trieval results on captioned images.
As most ofthese are on Flickr30K (Young et al, 2014), weran MULTITASK on it and got an accuracy@5 of32%, within the range of numbers reported in pre-vious work: 29.8% (Socher et al, 2014), 31.2%(Mao et al, 2014), 34% (Kiros et al, 2014) and37.7% (Karpathy and Fei-Fei, 2014).
Karpathyand Fei-Fei (2014) report 29.6% on MS-COCO,but with additional training data.115Original a couple of horses UNK their head over a rock pilerank 1 two brown horses hold their heads above a rocky wall .rank 2 two horses looking over a short stone wall .Scrambled rock couple their head pile a a UNK over of horsesrank 1 an image of a man on a couple of horsesrank 2 looking in to a straw lined pen of cowsOriginal a cute baby playing with a cell phonerank 1 small baby smiling at camera and talking on phone .rank 2 a smiling baby holding a cell phone up to ear .Scrambled phone playing cute cell a with baby arank 1 someone is using their phone to send a text or play a game .rank 2 a camera is placed next to a cellular phone .Table 3: Examples of two nearest neighbors retrieved by MULTITASK for original and scrambled cap-tions.?
a variety of kitchen utensils hanging from a UNK board .?
?kitchen of from hanging UNK variety a board utensils a .
?Figure 3: For the original caption MULTITASK un-derstands kitchen as a modifier of headword uten-sils, which is the topic.
For the scrambled sen-tence, the model thinks kitchen is the topic.Paraphrase retrieval In our dataset each imageis paired with five different captions, which canbe seen as paraphrases.
This affords us the op-portunity to test IMAGINET?s sentence represen-tations on a non-visual task.
Although all mod-els receive one caption-image pair at a time, theco-occurrence with the same image can lead themodel to learn structural similarities between cap-tions that are different on the surface.
We feedthe whole set of validation captions through thetrained model and record the final hidden visualstate hV?.
For each caption we rank all others ac-cording to cosine similarity and measure the pro-portion of the ones associated with the same imageamong the top four highest ranked.
For the scram-bled condition, we rank original captions againsta scrambled one.
Figure 2 (right) summarizes theresults: both models outperform the baseline onordered captions, but not on scrambled ones.
Asexpected, MULTITASK is more affected by manip-ulating word order, because it is more sensitive tostructure.
Table 3 shows concrete examples of theeffect of scrambling words in what sentences areretrieved.5 DiscussionIMAGINET is a novel model of grounded lan-guage acquisition which simultaneously learnsword meaning representations and knowledge ofsentence structure from captioned images.
Itacquires meaning representations for individualwords from descriptions of visual scenes, mim-icking an important aspect of human languagelearning, and can effectively use sentence structurein semantic interpretation of multi-word phrases.In future we plan to upgrade the current word-prediction pathway to a sentence reconstructionand/or sentence paraphrasing task in order to en-courage the formation of representations of fullsentences.
We also want to explore the acquiredstructure further, especially for generalizing thegrounded meanings to those words for which vi-sual data is not available.AcknowledgementsThe authors would like to thank Angeliki Lazari-dou and Marco Baroni for their many insightfulcomments on the research presented in this pa-per.ReferencesAfra Alishahi and Grzegorz Chrupa?a.
2012.
Concur-rent acquisition of word meaning and lexical cate-gories.
In Proceedings of the 2012 Joint Confer-ence on Empirical Methods in Natural LanguageProcessing and Computational Natural LanguageLearning, pages 643?654.
Association for Compu-tational Linguistics.Fr?ed?eric Bastien, Pascal Lamblin, Razvan Pascanu,James Bergstra, Ian J. Goodfellow, Arnaud Berg-116eron, Nicolas Bouchard, and Yoshua Bengio.
2012.Theano: new features and speed improvements.Deep Learning and Unsupervised Feature LearningNIPS 2012 Workshop.James Bergstra, Olivier Breuleux, Fr?ed?eric Bastien,Pascal Lamblin, Razvan Pascanu, Guillaume Des-jardins, Joseph Turian, David Warde-Farley, andYoshua Bengio.
2010.
Theano: a CPU andGPU math expression compiler.
In Proceedingsof the Python for Scientific Computing Conference(SciPy).
Oral Presentation.Elia Bruni, Nam-Khanh Tran, andMarco Baroni.
2014.Multimodal distributional semantics.
Journal of Ar-tificial Intelligence Research (JAIR), 49:1?47.Rich Caruana.
1997.
Multitask learning.
Machinelearning, 28(1):41?75.Xinlei Chen and C Lawrence Zitnick.
2014.
Learninga recurrent visual representation for image captiongeneration.
arXiv preprint arXiv:1411.5654.Kyunghyun Cho, Bart van Merri?enboer, Dzmitry Bah-danau, and Yoshua Bengio.
2014.
On the propertiesof neural machine translation: Encoder-decoder ap-proaches.
In Eighth Workshop on Syntax, Semanticsand Structure in Statistical Translation (SSST-8).Junyoung Chung, Caglar Gulcehre, KyungHyun Cho,and Yoshua Bengio.
2014.
Empirical evaluation ofgated recurrent neural networks on sequence model-ing.
In NIPS 2014 Deep Learning and Representa-tion Learning Workshop.Kenny R. Coventry, Angelo Cangelosi, Rohanna Ra-japakse, Alison Bacon, Stephen Newstead, DanJoyce, and Lynn V. Richards.
2005.
Spatial preposi-tions and vague quantifiers: Implementing the func-tional geometric framework.
In Christian Freksa,Markus Knauff, Bernd Krieg-Br?uckner, BernhardNebel, and Thomas Barkowsky, editors, SpatialCognition IV.
Reasoning, Action, Interaction, vol-ume 3343 of Lecture Notes in Computer Science,pages 98?110.
Springer Berlin Heidelberg.Jeff Donahue, Lisa Anne Hendricks, Sergio Guadar-rama, Marcus Rohrbach, Subhashini Venugopalan,Kate Saenko, and Trevor Darrell.
2014.
Long-term recurrent convolutional networks for vi-sual recognition and description.
arXiv preprintarXiv:1411.4389.Hao Fang, Saurabh Gupta, Forrest Iandola, Rupesh Sri-vastava, Li Deng, Piotr Doll?ar, Jianfeng Gao, Xi-aodong He, Margaret Mitchell, John Platt, et al2014.
From captions to visual concepts and back.arXiv preprint arXiv:1411.4952.Afsaneh Fazly, Afra Alishahi, and Suzanen Steven-son.
2010.
A probabilistic computational model ofcross-situational word learning.
Cognitive Science:A Multidisciplinary Journal, 34(6):1017?1063.Michael Fleischman and Deb Roy.
2005.
Intentionalcontext in situated natural language learning.
In Pro-ceedings of the Ninth Conference on ComputationalNatural Language Learning, pages 104?111.
Asso-ciation for Computational Linguistics.Felix Hill, Roi Reichart, and Anna Korhonen.
2014.Simlex-999: Evaluating semantic models with(genuine) similarity estimation.
arXiv preprintarXiv:1408.3456.Sepp Hochreiter and J?urgen Schmidhuber.
1997.Long short-term memory.
Neural computation,9(8):1735?1780.Steve R Howell, Damian Jankowicz, and SuzannaBecker.
2005.
A model of grounded language ac-quisition: Sensorimotor features improve lexical andgrammatical learning.
Journal of Memory and Lan-guage, 53(2):258?276.Andrej Karpathy and Li Fei-Fei.
2014.
Deep visual-semantic alignments for generating image descrip-tions.
arXiv preprint arXiv:1412.2306.Diederik P. Kingma and Jimmy Ba.
2014.
Adam:A method for stochastic optimization.
CoRR,abs/1412.6980.Ryan Kiros, Ruslan Salakhutdinov, and Richard SZemel.
2014.
Unifying visual-semantic embeddingswith multimodal neural language models.
arXivpreprint arXiv:1411.2539.Tom Kwiatkowski, Sharon Goldwater, Luke Zettle-moyer, and Mark Steedman.
2012.
A probabilis-tic model of syntactic and semantic acquisition fromchild-directed utterances and their meanings.
In Pro-ceedings of the 13th Conference of the EuropeanChapter of the Association for Computational Lin-guistics, pages 234?244.
Association for Computa-tional Linguistics.Angeliki Lazaridou, Nghia The Pham, and Marco Ba-roni.
2015.
Combining language and vision witha multimodal skip-gram model.
In Proceedings ofNAACL HLT 2015 (2015 Conference of the NorthAmerican Chapter of the Association for Computa-tional Linguistics - Human Language Technologies).Ping Li, Igor Farkas, and Brian MacWhinney.
2004.Early lexical development in a self-organizing neu-ral network.
Neural Networks, 17:1345?1362.Brian MacWhinney.
2014.
The CHILDES project:Tools for analyzing talk, Volume I: Transcription for-mat and programs.
Psychology Press.JunhuaMao, Wei Xu, Yi Yang, JiangWang, and Alan LYuille.
2014.
Explain images with multimodal recur-rent neural networks.
In NIPS 2014 Deep LearningWorkshop.Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-rado, and Jeff Dean.
2013.
Distributed representa-tions of words and phrases and their compositional-117ity.
In Advances in Neural Information ProcessingSystems, pages 3111?3119.Tomas Mikolov and Geoffrey Zweig.
2012.
Contextdependent recurrent neural network language model.In SLT, pages 234?239.Terry Regier.
2005.
The emergence of words: Atten-tional learning in form and meaning.
Cognitive Sci-ence: A Multidisciplinary Journal, 29:819?865.Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause,Sanjeev Satheesh, Sean Ma, Zhiheng Huang, An-drej Karpathy, Aditya Khosla, Michael Bernstein,Alexander C. Berg, and Li Fei-Fei.
2014.
ImageNetLarge Scale Visual Recognition Challenge.K.
Simonyan and A. Zisserman.
2014.
Very deep con-volutional networks for large-scale image recogni-tion.
CoRR, abs/1409.1556.Jeffrey M. Siskind.
1996.
A computational study ofcross-situational techniques for learning word-to-meaning mappings.
Cognition, 61(1-2):39?91.Danijel Skocaj, Matej Kristan, Alen Vrecko, MarkoMahnic, Miroslav Janicek, Geert-Jan M Krui-jff, Marc Hanheide, Nick Hawes, Thomas Keller,Michael Zillich, et al 2011.
A system for interac-tive learning in dialogue with a tutor.
In IntelligentRobots and Systems (IROS), 2011 IEEE/RSJ Inter-national Conference on, pages 3387?3394.
IEEE.Richard Socher, Andrej Karpathy, Quoc V Le, Christo-pher D Manning, and Andrew Y Ng.
2014.Grounded compositional semantics for finding anddescribing images with sentences.
Transactionsof the Association for Computational Linguistics,2:207?218.Subhashini Venugopalan, Huijuan Xu, Jeff Donahue,Marcus Rohrbach, Raymond Mooney, and KateSaenko.
2014.
Translating videos to natural lan-guage using deep recurrent neural networks.
arXivpreprint arXiv:1412.4729.Oriol Vinyals, Alexander Toshev, Samy Bengio,and Dumitru Erhan.
2014.
Show and tell: Aneural image caption generator.
arXiv preprintarXiv:1411.4555.Peter Young, Alice Lai, Micah Hodosh, and JuliaHockenmaier.
2014.
From image descriptions tovisual denotations: New similarity metrics for se-mantic inference over event descriptions.
Transac-tions of the Association for Computational Linguis-tics, 2:67?78.Chen Yu.
2008.
A statistical associative account of vo-cabulary growth in early word learning.
LanguageLearning and Development, 4(1):32?62.118
