Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 511?521,Baltimore, Maryland, USA, June 23-25 2014.c?2014 Association for Computational LinguisticsA Linear-Time Bottom-Up Discourse Parserwith Constraints and Post-EditingVanessa Wei FengDepartment of Computer ScienceUniversity of TorontoToronto, ON, Canadaweifeng@cs.toronto.eduGraeme HirstDepartment of Computer ScienceUniversity of TorontoToronto, ON, Canadagh@cs.toronto.eduAbstractText-level discourse parsing remains achallenge.
The current state-of-the-artoverall accuracy in relation assignment is55.73%, achieved by Joty et al (2013).However, their model has a high order oftime complexity, and thus cannot be ap-plied in practice.
In this work, we developa much faster model whose time complex-ity is linear in the number of sentences.Our model adopts a greedy bottom-up ap-proach, with two linear-chain CRFs ap-plied in cascade as local classifiers.
To en-hance the accuracy of the pipeline, we addadditional constraints in the Viterbi decod-ing of the first CRF.
In addition to effi-ciency, our parser also significantly out-performs the state of the art.
Moreover,our novel approach of post-editing, whichmodifies a fully-built tree by consideringinformation from constituents on upperlevels, can further improve the accuracy.1 IntroductionDiscourse parsing is the task of identifying thepresence and the type of the discourse relationsbetween discourse units.
While research in dis-course parsing can be partitioned into several di-rections according to different theories and frame-works, Rhetorical Structure Theory (RST) (Mannand Thompson, 1988) is probably the most am-bitious one, because it aims to identify not onlythe discourse relations in a small local context, butalso the hierarchical tree structure for the full text:from the relations relating the smallest discourseunits (called elementary discourse units, EDUs),to the ones connecting paragraphs.For example, Figure 1 shows a text fragmentconsisting of two sentences with four EDUs intotal (e1-e4).
Its discourse tree representation isshown below the text, following the notation con-vention of RST: the two EDUs e1and e2are re-lated by a mononuclear relation CONSEQUENCE,where e2is the more salient span (called nucleus,and e1is called satellite); e3and e4are related byanother mononuclear relation CIRCUMSTANCE,with e4as the nucleus; the two spans e1:2and e3:4are further related by a multi-nuclear relation SE-QUENCE, with both spans as the nucleus.Conventionally, there are two major sub-tasksrelated to text-level discourse parsing: (1) EDUsegmentation: to segment the raw text into EDUs,and (2) tree-building: to build a discourse treefrom EDUs, representing the discourse relations inthe text.
Since the first sub-task is considered rela-tively easy, with the state-of-art accuracy at above90% (Joty et al, 2012), the recent research focusis on the second sub-task, and often uses manualEDU segmentation.The current state-of-the-art overall accuracy ofthe tree-building sub-task, evaluated on the RSTDiscourse Treebank (RST-DT, to be introduced inSection 8), is 55.73% by Joty et al (2013).
How-ever, as an optimal discourse parser, Joty et al?smodel is highly inefficient in practice, with re-spect to both their DCRF-based local classifiers,and their CKY-like bottom-up parsing algorithm.DCRF (Dynamic Conditional Random Fields) isa generalization of linear-chain CRFs, in whicheach time slice contains a set of state variablesand edges (Sutton et al, 2007).
CKY parsing isa bottom-up parsing algorithm which searches allpossible parsing paths by dynamic programming.Therefore, despite its superior performance, theirmodel is infeasible in most realistic situations.The main objective of this work is to developa more efficient discourse parser, with similar oreven better performance with respect to Joty etal.
?s optimal parser, but able to produce parsing re-sults in real time.Our contribution is three-fold.
First, with a511[On Aug. 1, the state tore up its controls,]e1[and food prices leaped]e2[Without bufferstocks,]e3[inflation exploded.
]e4wsj 1146e1 e2consequencee1:4e3 e4circumstancesequencee1:2 e3:4Figure 1: An example text fragment composed oftwo sentences and four EDUs, with its RST dis-course tree representation shown below.greedy bottom-up strategy, we develop a discourseparser with a time complexity linear in the totalnumber of sentences in the document.
As a re-sult of successfully avoiding the expensive non-greedy parsing algorithms, our discourse parser isvery efficient in practice.
Second, by using twolinear-chain CRFs to label a sequence of discourseconstituents, we can incorporate contextual infor-mation in a more natural way, compared to us-ing traditional discriminative classifiers, such asSVMs.
Specifically, in the Viterbi decoding ofthe first CRF, we include additional constraintselicited from common sense, to make more ef-fective local decisions.
Third, after a discourse(sub)tree is fully built from bottom up, we performa novel post-editing process by considering infor-mation from the constituents on upper levels.
Weshow that this post-editing can further improve theoverall parsing performance.2 Related work2.1 HILDA discourse parserThe HILDA discourse parser by Hernault et al(2010) is the first attempt at RST-style text-leveldiscourse parsing.
It adopts a pipeline framework,and greedily builds the discourse tree from the bot-tom up.
In particular, starting from EDUs, at eachstep of the tree-building, a binary SVM classifieris first applied to determine which pair of adjacentdiscourse constituents should be merged to form alarger span, and another multi-class SVM classi-fier is then applied to assign the type of discourserelation that holds between the chosen pair.The strength of HILDA?s greedy tree-buildingstrategy is its efficiency in practice.
Also, the em-ployment of SVM classifiers allows the incorpora-tion of rich features for better data representation(Feng and Hirst, 2012).
However, HILDA?s ap-proach also has obvious weakness: the greedy al-gorithm may lead to poor performance due to localoptima, and more importantly, the SVM classifiersare not well-suited for solving structural problemsdue to the difficulty of taking context into account.2.2 Joty et al?s joint modelJoty et al (2013) approach the problem of text-level discourse parsing using a model trained byConditional Random Fields (CRF).
Their modelhas two distinct features.First, they decomposed the problem of text-level discourse parsing into two stages: intra-sentential parsing to produce a discourse tree foreach sentence, followed by multi-sentential pars-ing to combine the sentence-level discourse treesand produce the text-level discourse tree.
Specif-ically, they employed two separate models forintra- and multi-sentential parsing.
Their choiceof two-stage parsing is well motivated for two rea-sons: (1) it has been shown that sentence bound-aries correlate very well with discourse bound-aries, and (2) the scalability issue of their CRF-based models can be overcome by this decompo-sition.Second, they jointly modeled the structure andthe relation for a given pair of discourse units.For example, Figure 2 shows their intra-sententialmodel, in which they use the bottom layer to rep-resent discourse units; the middle layer of binarynodes to predict the connection of adjacent dis-course units; and the top layer of multi-class nodesto predict the type of the relation between twounits.
Their model assigns a probability to eachpossible constituent, and a CKY-like parsing al-gorithm finds the globally optimal discourse tree,given the computed probabilities.The strength of Joty et al?s model is their jointmodeling of the structure and the relation, suchthat information from each aspect can interact withthe other.
However, their model has a major defectin its inefficiency, or even infeasibility, for appli-cation in practice.
The inefficiency lies in boththeir DCRF-based joint model, on which infer-ence is usually slow, and their CKY-like parsingalgorithm, whose issue is more prominent.
Due tothe O(n3) time complexity, where n is the number512R2S2U2U1R3S3U3RjSjUjRt-1St-1Ut-1Relation sequenceStructure sequenceUnit sequence at level iFigure 2: Joty et al (2013)?s intra-sentential Con-dition Random Fields.of input discourse units, for large documents, theparsing simply takes too long1.3 Overall work flowFigure 3 demonstrates the overall work flow of ourdiscourse parser.
The general idea is that, similarto Joty et al (2013), we perform a sentence-levelparsing for each sentence first, followed by a text-level parsing to generate a full discourse tree forthe whole document.
However, in addition to effi-ciency (to be shown in Section 6), our discourseparser has a distinct feature, which is the post-editing component (to be introduced in Section 5),as outlined in dashes.Our discourse parser works as follows.
A doc-ument D is first segmented into a list of sen-tences.
Each sentence Si, after being segmentedinto EDUs (not shown in the figure), goes throughan intra-sentential bottom-up tree-building modelMintra, to form a sentence-level discourse tree TSi,with the EDUs as leaf nodes.
After that, we ap-ply the intra-sentential post-editing model Pintratomodify the generated tree TSito TpSi, by consideringupper-level information.We then combine all sentence-level discoursetree TpSi?s using our multi-sentential bottom-uptree-building model Mmultito generate the text-level discourse tree TD.
Similar to sentence-levelparsing, we also post-edit TDusing Pmultito pro-duce the final discourse tree TpD.1The largest document in the RST-DT contains over 180sentences, i.e., n > 180 for their multi-sentential CKY pars-ing.
Intuitively, suppose the average time to compute theprobability of each constituent is 0.01 second, then in total,the CKY-like parsing takes over 16 hours.
It is possible to op-timize Joty et al?s CKY-like parsing by replacing their CRF-based computation for upper-level constituents with some lo-cal computation based on the probabilities of lower-level con-stituents.
However, such optimization is beyond the scope ofthis paper.4 Bottom-up tree-buildingFor both intra- and multi-sentential parsing, ourbottom-up tree-building process adopts a similargreedy pipeline framework like the HILDA dis-course parser (discussed in Section 2.1), to guar-antee efficiency for large documents.
In partic-ular, starting from the constituents on the bot-tom level (EDUs for intra-sentential parsing andsentence-level discourse trees for multi-sententialparsing), at each step of the tree-building, wegreedily merge a pair of adjacent discourse con-stituents such that the merged constituent has thehighest probability as predicted by our structuremodel.
The relation model is then applied to as-sign the relation to the new constituent.4.1 Linear-chain CRFs as Local modelsNow we describe the local models we use to makedecisions for a given pair of adjacent discourseconstituents in the bottom-up tree-building.
Thereare two dimensions for our local models: (1) scopeof the model: intra- or multi-sentential, and (2)purpose of the model: for determining structuresor relations.
So we have four local models, Mstructintra,Mrelintra, Mstructmulti, and Mrelmulti.While our bottom-up tree-building shares thegreedy framework with HILDA, unlike HILDA,our local models are implemented using CRFs.In this way, we are able to take into account thesequential information from contextual discourseconstituents, which cannot be naturally repre-sented in HILDA with SVMs as local classifiers.Therefore, our model incorporates the strengthsof both HILDA and Joty et al?s model, i.e., theefficiency of a greedy parsing algorithm, and theability to incorporate sequential information withCRFs.As shown by Feng and Hirst (2012), for a pairof discourse constituents of interest, the sequentialinformation from contextual constituents is cru-cial for determining structures.
Therefore, it iswell motivated to use Conditional Random Fields(CRFs) (Lafferty et al, 2001), which is a discrimi-native probabilistic graphical model, to make pre-dictions for a sequence of constituents surround-ing the pair of interest.In this sense, our local models appear similarto Joty et al?s non-greedy parsing models.
How-ever, the major distinction between our modelsand theirs is that we do not jointly model the struc-ture and the relation; rather, we use two linear-513DS1SiSn......MintraMintraiMintraPintra...... ...PintraPintraPmultiMmulti......1ST iST nST pSnT pS iTpST 1 DT pDTFigure 3: The work flow of our proposed discourse parser.
In the figure, Mintraand Mmultistand for theintra- and multi-sentential bottom-up tree-building models, and Pintraand Pmultistand for the intra- andmulti-sentential post-editing models.chain CRFs to model the structure and the relationseparately.
Although joint modeling has shown tobe effective in various NLP and computer visionapplications (Sutton et al, 2007; Yang et al, 2009;Wojek and Schiele, 2008), our choice of using twoseparate models is for the following reasons:First, it is not entirely appropriate to model thestructure and the relation at the same time.
Forexample, with respect to Figure 2, it is unclearhow the relation node Rjis represented for a train-ing instance whose structure node Sj= 0, i.e., theunits Uj?1and Ujare disjoint.
Assume a specialrelation NO-REL is assigned for Rj.
Then, in thetree-building process, we will have to deal with thesituations where the joint model yields conflictingpredictions: it is possible that the model predictsSj= 1 and Rj= NO-REL, or vice versa, and wewill have to decide which node to trust (and thusin some sense, the structure and the relation is nolonger jointly modeled).Secondly, as a joint model, it is mandatory touse a dynamic CRF, for which exact inference isusually intractable or slow.
In contrast, for linear-chain CRFs, efficient algorithms and implementa-tions for exact inference exist.4.2 Structure models4.2.1 Intra-sentential structure modelFigure 4a shows our intra-sentential structuremodel Mstructintrain the form of a linear-chain CRF.Similar to Joty et al?s intra-sentential model, thefirst layer of the chain is composed of discourseconstituents Uj?s, and the second layer is com-posed of binary nodes Sj?s to indicate the proba-bility of merging adjacent discourse constituents.S2U2U1S3U3SjUjStUtStructuresequenceA ll units insentenceat level i(a) Intra-sentential structure model Mstructintra.Sj-1Uj-1Uj-2SjUjStructuresequenceA d jacentunits  atlevel iUj+ 1Sj+ 1Sj-1Uj-3Sj+ 2Uj+ 2C1 C2C3(b) Multi-sentential structure model Mstructmulti.
C1, C2, and C3denote the three chains for predicting Ujand Uj+1.Figure 4: Local structure models.At each step in the bottom-up tree-building pro-cess, we generate a single sequence E, consistingof U1,U2, .
.
.
,Uj, .
.
.
,Ut, which are all the currentdiscourse constituents in the sentence that needto be processed.
For instance, initially, we havethe sequence E1= {e1,e2, .
.
.
,em}, which are theEDUs of the sentence; after merging e1and e2onthe second level, we have E2= {e1:2,e3, .
.
.
,em};after merging e4and e5on the third level, we haveE3= {e1:2,e3,e4:5, .
.
.
,em}, and so on.Because the structure model is the first com-ponent in our pipeline of local models, its accu-racy is crucial.
Therefore, to improve its accuracy,we enforce additional commonsense constraints inits Viterbi decoding.
In particular, we disallow 1-1 transitions between adjacent labels (a discourseunit can be merged with at most one adjacent unit),and we disallow all-zero sequences (at least one514pair must be merged).Since the computation of Eidoes not dependon a particular pair of constituents, we can use thesame sequence Eito compute structural probabili-ties for all adjacent constituents.
In contrast, Jotyet al?s computation of intra-sentential sequencesdepends on the particular pair of constituents: thesequence is composed of the pair in question, withother EDUs in the sentence, even if those EDUshave already been merged.
Thus, different CRFchains have to be formed for different pairs of con-stituents.
In addition to efficiency, our use of asingle CRF chain for all constituents can bettercapture the sequential dependencies among con-text, by taking into account the information frompartially built discourse constituents, rather thanbottom-level EDUs only.4.2.2 Multi-sentential structure modelFor multi-sentential parsing, where the smallestdiscourse units are single sentences, as argued byJoty et al (2013), it is not feasible to use a longchain to represent all constituents, due to the factthat it takes O(TM2) time to perform the forward-backward exact inference on a chain with T unitsand an output vocabulary size of M, thus the over-all complexity for all possible sequences in theirmodel is O(M2n3)2.Instead, we choose to take a sliding-windowapproach to form CRF chains for a particular pairof constituents, as shown in Figure 4b.
For exam-ple, suppose we wish to compute the structuralprobability for the pairUj?1andUj, we form threechains, each of which contains two contextualconstituents: C1= {Uj?3,Uj?2,Uj?1,Uj},C2= {Uj?2,Uj?1,Uj,Uj+1}, and C3={Uj?1,Uj,Uj+1,Uj+2}.
We then find the chainCt,1 ?
t ?
3, with the highest joint probabilityover the entire sequence, and assign its marginalprobability P(Stj= 1) to P(Sj= 1).Similar to Mstructintra, for Mstructmulti, we also includeadditional constraints in the Viterbi decoding, bydisallowing transitions between two ones, and dis-allowing the sequence to be all zeros if it containsall the remaining constituents in the document.4.3 Relation models4.3.1 Intra-sentential relation modelThe intra-sentential relation model Mrelintra, shownin Figure 5a, works in a similar way to Mstructintra, as2The time complexity will be reduced to O(M2n2), if weuse the same chain for all constituents as in our Mstructintra.described in Section 4.2.1.
The linear-chain CRFcontains a first layer of all discourse constituentsUj?s in the sentence on level i, and a second layerof relation nodes Rj?s to represent the relation be-tween a pair of discourse constituents.However, unlike the structure model, adjacentrelation nodes do not share discourse constituentson the first layer.
Rather, each relation node Rjattempts to model the relation of one single con-stituent Uj, by taking Uj?s left and right subtreesUj,Land Uj,Ras its first-layer nodes; if Ujis a sin-gle EDU, then the first-layer node of Rjis simplyUj, and Rjis a special relation symbol LEAF3.Since we know, a priori, that the constituents in thechains are either leaf nodes or the ones that havebeen merged by our structure model, we neverneed to worry about the NO-REL issue as out-lined in Section 4.1.In the bottom-up tree-building process, aftermerging a pair of adjacent constituents usingMstructintrainto a new constituent, say Uj, we form achain consisting of all current constituents in thesentence to decide the relation label for Uj, i.e.,the Rjnode in the chain.
In fact, by perform-ing inference on this chain, we produce predic-tions not only for Rj, but also for all other R nodesin the chain, which correspond to all other con-stituents in the sentence.
Since those non-leaf con-stituents are already labeled in previous steps inthe tree-building, we can now re-assign their rela-tions if the model predicts differently in this step.Therefore, this re-labeling procedure can compen-sate for the loss of accuracy caused by our greedybottom-up strategy to some extent.4.3.2 Multi-sentential relation modelFigure 5b shows our multi-sentential relationmodel.
Like Mrelintra, the first layer consists of adja-cent discourse units, and the relation nodes on thesecond layer model the relation of each constituentseparately.Similar to Mstructmultiintroduced in Section 4.2.2,Mrelmultialso takes a sliding-window approach topredict labels for constituents in a local context.For a constituent Ujto be predicted, we form threechains, and use the chain with the highest jointprobability to assign or re-assign relations to con-stituents in that chain.3These leaf constituents are represented using a specialfeature vector is leaf = True; thus the CRF never labelsthem with relations other than LEAF.515Relation sequenceA ll units in sentence at level iR1U1, RU1, LR2U2RjUj, RUj, LRtUt, RUt, L(a) Intra-sentential relation model Mrelintra.Relation sequenceA d jacent units  at level iR1Uj-2, RUj-2, LRj-1Uj-1RjUj, RUj, LRj+ 1Uj+ 1, RUj+ 1, LRj+ 2Uj+ 2C1 C2C3(b) Multi-sentential relation model Mrelmulti.
C1, C2, and C3denote the three sliding windows for predictingUj,LandUj,R.Figure 5: Local relation models.5 Post-editingAfter an intra- or multi-sentential discourse treeis fully built, we perform a post-editing to con-sider possible modifications to the current tree, byconsidering useful information from the discourseconstituents on upper levels, which is unavailablein the bottom-up tree-building process.The motivation for post-editing is that, someparticular discourse relations, such as TEXTUAL-ORGANIZATION, tend to occur on the top levelsof the discourse tree; thus, information such as thedepth of the discourse constituent can be quite in-dicative.
However, the exact depth of a discourseconstituent is usually unknown in the bottom-uptree-building process; therefore, it might be ben-eficial to modify the tree by including top-downinformation after the tree is fully built.The process of post-editing is shown in Algo-rithm 1.
For each input discourse tree T , whichis already fully built by bottom-up tree-buildingmodels, we do the following:Lines 3 ?
9: Identify the lowest level of T onwhich the constituents can be modified accordingto the post-editing structure component, Pstruct.
Todo so, we maintain a list L to store the discourseconstituents that need to be examined.
Initially, Lconsists of all the bottom-level constituents in T .At each step of the loop, we consider merging thepair of adjacent units in L with the highest proba-bility predicted by Pstruct.
If the predicted pair isnot merged in the original tree T , then a possiblemodification is located; otherwise, we merge thepair, and proceed to the next iteration.Lines 10 ?
12: If modifications have been pro-posed in the previous step, we build a new treeAlgorithm 1 Post-editing algorithm.Input: A fully built discourse tree T .1: if |T |= 1 then2: return T .
Do nothing if it is a singleEDU.3: L?
[U1,U2, .
.
.
,Ut] .
The bottom-levelconstituents in T .4: while |L|> 2 do5: i?
PREDICTMERGING(L,Pstruct)6: p?
PARENT(L[i],L[i+1],T )7: if p = NULL then8: break9: Replace L[i] and L[i+1] with p10: if |L|= 2 then11: L?
[U1,U2, .
.
.
,Ut]12: Tp?
BUILDTREE(L,Pstruct,Prel,T )Output: TpTpusing Pstructas the structure model, and Prelas the relation model, from the constituents onwhich modifications are proposed.
Otherwise, Tpis built from the bottom-level constituents of T .The upper-level information, such as the depth ofa discourse constituent, is derived from the initialtree T .5.1 Local modelsThe local models, P{struct|rel}{intra|multi}, for post-editingis almost identical to their counterparts of thebottom-up tree-building, except that the linear-chain CRFs in post-editing includes additionalfeatures to represent information from constituentson higher levels (to be introduced in Section 7).6 Linear time complexityHere we analyze the time complexity of each com-ponent in our discourse parser, to quantitativelydemonstrate the time efficiency of our model.
Thefollowing analysis is focused on the bottom-uptree-building process, but a similar analysis can becarried out for the post-editing process.
Since thenumber of operations in the post-editing process isroughly the same (1.5 times in the worst case) asin the bottom-up tree-building, post-editing sharesthe same complexity as the tree-building.6.1 Intra-sentential parsingSuppose the input document is segmented inton sentences, and each sentence Skcontains mkEDUs.
For each sentence Skwith mkEDUs, the516overall time complexity to perform intra-sententialparsing is O(m2k).
The reason is the following.
Onlevel i of the bottom-up tree-building, we generatea single chain to represent the structure or relationfor all the mk?
i constituents that are currently inthe sentence.
The time complexity for performingforward-backward inference on the single chain isO((mk?
i)?M2) =O(mk?
i), where the constantM is the size of the output vocabulary.
Startingfrom the EDUs on the bottom level, we need toperform inference for one chain on each level dur-ing the bottom-up tree-building, and thus the totaltime complexity is ?mki=1O(mk?
i) = O(m2k).The total time to generate sentence-level dis-course trees for n sentences is ?nk=1O(m2k).
It isfairly safe to assume that each mkis a constant,in the sense that mkis independent of the totalnumber of sentences in the document.
There-fore, the total time complexity ?nk=1O(m2k) ?
n?O(max1?
j?n(m2j)) = n?O(1) = O(n), i.e., linearin the total number of sentences.6.2 Multi-sentential parsingFor multi-sentential models, Mstructmultiand Mrelmulti, asshown in Figures 4b and 5b, for a pair of con-stituents of interest, we generate multiple chainsto predict the structure or the relation.By including a constant number k of discourseunits in each chain, and considering a constantnumber l of such chains for computing each ad-jacent pair of discourse constituents (k = 4 forMstructmultiand k = 3 for Mrelmulti; l = 3), we have anoverall time complexity of O(n).
The reason isthat it takes l?O(kM2) =O(1) time, where l,k,Mare all constants, to perform exact inference for agiven pair of adjacent constituents, and we needto perform such computation for all n?1 pairs ofadjacent sentences on the first level of the tree-building.
Adopting a greedy approach, on an ar-bitrary level during the tree-building, once we de-cide to merge a certain pair of constituents, sayUjand Uj+1, we only need to recompute a smallnumber of chains, i.e., the chains which originallyinclude Ujor Uj+1, and inference on each chaintakes O(1).
Therefore, the total time complexityis (n?1)?O(1)+(n?1)?O(1) = O(n), wherethe first term in the summation is the complexityof computing all chains on the bottom level, andthe second term is the complexity of computingthe constant number of chains on higher levels.We have thus showed that the time complexityis linear in n, which is the number of sentences inthe document.
In fact, under the assumption thatthe number of EDUs in each sentence is indepen-dent of n, it can be shown that the time complexityis also linear in the total number of EDUs4.7 FeaturesIn our local models, to encode two adjacent units,Ujand Uj+1, within a CRF chain, we use the fol-lowing 10 sets of features, some of which are mod-ified from Joty et al?s model.Organization features: WhetherUj(orUj+1) isthe first (or last) constituent in the sentence (forintra-sentential models) or in the document (formulti-sentential models); whether Uj(or Uj+1) isa bottom-level constituent.Textual structure features: Whether Ujcon-tains more sentences (or paragraphs) than Uj+1.N-gram features: The beginning (or end) lexi-cal n-grams in each unit; the beginning (or end)POS n-grams in each unit, where n ?
{1,2,3}.Dominance features: The PoS tags of the headnode and the attachment node; the lexical heads ofthe head node and the attachment node; the domi-nance relationship between the two units.Contextual features: The feature vector of theprevious and the next constituent in the chain.Substructure features: The root node of the leftand right discourse subtrees of each unit.Syntactic features: whether each unit corre-sponds to a single syntactic subtree, and if so, thetop PoS tag of the subtree; the distance of eachunit to their lowest common ancestor in the syntaxtree (intra-sentential only).Entity transition features: The type and thenumber of entity transitions across the two units.We adopt Barzilay and Lapata (2008)?s entity-based local coherence model to represent a doc-ument by an entity grid, and extract local transi-tions among entities in continuous discourse con-stituents.
We use bigram and trigram transitionswith syntactic roles attached to each entity.4We implicitly made an assumption that the parsing timeis dominated by the time to perform inference on CRF chains.However, for complex features, the time required for fea-ture computation might be dominant.
Nevertheless, a care-ful caching strategy can accelerate feature computation, sincea large number of multi-sentential chains overlap with eachother.517Cue phrase features: Whether a cue phrase oc-curs in the first or last EDU of each unit.
The cuephrase list is based on the connectives collected byKnott and Dale (1994)Post-editing features: The depth of each unit inthe initial tree.8 ExperimentsFor pre-processing, we use the Stanford CoreNLP(Klein and Manning, 2003; de Marneffe et al,2006; Recasens et al, 2013) to syntactically parsethe texts and extract coreference relations, and weuse Penn2Malt5to lexicalize syntactic trees to ex-tract dominance features.For local models, our structure models aretrained using MALLET (McCallum, 2002) to in-clude constraints over transitions between adja-cent labels, and our relation models are trainedusing CRFSuite (Okazaki, 2007), which is a fastimplementation of linear-chain CRFs.The data that we use to develop and evaluateour discourse parser is the RST Discourse Tree-bank (RST-DT) (Carlson et al, 2001), which is alarge corpus annotated in the framework of RST.The RST-DT consists of 385 documents (347 fortraining and 38 for testing) from the Wall StreetJournal.
Following previous work on the RST-DT(Hernault et al, 2010; Feng and Hirst, 2012; Jotyet al, 2012; Joty et al, 2013), we use 18 coarse-grained relation classes, and with nuclearity at-tached, we have a total set of 41 distinct relations.Non-binary relations are converted into a cascadeof right-branching binary relations.9 Results and Discussion9.1 Parsing accuracyWe compare four different models using manualEDU segmentation.
In Table 1, the jCRF modelin the first row is the optimal CRF model proposedby Joty et al (2013).
gSVMFHin the second rowis our implementation of HILDA?s greedy parsingalgorithm using Feng and Hirst (2012)?s enhancedfeature set.
The third model, gCRF, represents ourgreedy CRF-based discourse parser, and the lastrow, gCRFPE, represents our parser with the post-editing component included.In order to conduct a direct comparison withJoty et al?s model, we use the same set of eval-5http://stp.lingfil.uu.se/?nivre/research/Penn2Malt.html.Model Span Nuc RelationAcc MAFSjCRF 82.5 68.4 55.7 N/AgSVMFH82.8 67.1 52.0 27.4/23.3gCRF 84.9?69.9?57.2?35.3/31.3gCRFPE85.7??71.0??58.2?
?36.2/32.3Human 88.7 77.7 65.8 N/A?
: significantly better than gSVMFH(p < .01)?
: significantly better than gCRF (p < .01)Table 1: Performance of different models usinggold-standard EDU segmentation, evaluated us-ing the constituent accuracy (%) for span, nucle-arity, and relation.
For relation, we also report themacro-averaged F1-score (MAFS) for correctlyretrieved constituents (before the slash) and forall constituents (after the slash).
Statistical sig-nificance is verified using Wilcoxon?s signed-ranktest.uation metrics, i.e., the unlabeled and labeled pre-cision, recall, and F-score6as defined by Marcu(2000).
For evaluating relations, since there is askewed distribution of different relation types inthe corpus, we also include the macro-averagedF1-score (MAFS)7as another metric, to empha-size the performance of infrequent relation types.We report the MAFS separately for the correctlyretrieved constituents (i.e., the span boundary iscorrect) and all constituents in the reference tree.As demonstrated by Table 1, our greedy CRFmodels perform significantly better than the othertwo models.
Since we do not have the actual out-put of Joty et al?s model, we are unable to con-duct significance testing between our models andtheirs.
But in terms of overall accuracy, our gCRFmodel outperforms their model by 1.5%.
More-over, with post-editing enabled, gCRFPEsignif-icantly (p < .01) outperforms our initial modelgCRF by another 1% in relation assignment, andthis overall accuracy of 58.2% is close to 90% ofhuman performance.
With respect to the macro-averaged F1-scores, adding the post-editing com-ponent also obtains about 1% improvement.However, the overall MAFS is still at the lower6For manual segmentation, precision, recall, and F-scoreare the same.7MAFS is the F1-score averaged among all relationclasses by equally weighting each class.
Therefore, we can-not conduct significance test between different MAFS.518Avg Min Max# of EDUs 61.74 4 304# of Sentences 26.11 2 187# of EDUs per sentence 2.36 1 10Table 2: Characteristics of the 38 documents in thetest data.end of 30% for all constituents.
Our error anal-ysis shows that, for two relation classes, TOPIC-CHANGE and TEXTUAL-ORGANIZATION, ourmodel fails to retrieve any instance, and forTOPIC-COMMENT and EVALUATION, our modelscores a class-wise F1score lower than 5%.
Thesefour relation classes, apart from their infrequencyin the corpus, are more abstractly defined, and thusare particularly challenging.9.2 Parsing efficiencyWe further illustrate the efficiency of our parser bydemonstrating the time consumption of differentmodels.First, as shown in Table 2, the average numberof sentences in a document is 26.11, which is al-ready too large for optimal parsing models, e.g.,the CKY-like parsing algorithm in jCRF, let alnethe fact that the largest document contains sev-eral hundred of EDUs and sentences.
Therefore,it should be seen that non-optimal models are re-quired in most cases.In Table 3, we report the parsing time8for thelast three models, since we do not know the time ofjCRF.
Note that the parsing time excludes the timecost for any necessary pre-processing.
As can beseen, our gCRF model is considerably faster thangSVMFH, because, on one hand, feature compu-tation is expensive in gSVMFH, since gSVMFHutilizes a rich set of features; on the other hand,in gCRF, we are able to accelerate decoding bymulti-threading MALLET (we use four threads).Even for the largest document with 187 sentences,gCRF is able to produce the final tree after about40 seconds, while jCRF would take over 16 hoursassuming each DCRF decoding takes only 0.01second.
Although enabling post-editing doublesthe time consumption, the overall time is still ac-ceptable in practice, and the loss of efficiency canbe compensated by the improvement in accuracy.8Tested on a Linux system with four duo-core 3.0GHzprocessors and 16G memory.Model Parsing Time (seconds)Avg Min MaxgSVMFH11.19 0.42 124.86gCRF 5.52 0.05 40.57gCRFPE10.71 0.12 84.72Table 3: The parsing time (in seconds) for the 38documents in the test set of RST-DT.
Time cost ofany pre-processing is excluded from the analysis.10 ConclusionsIn this paper, we presented an efficient text-leveldiscourse parser with time complexity linear inthe total number of sentences in the document.Our approach was to adopt a greedy bottom-up tree-building, with two linear-chain CRFs aslocal probabilistic models, and enforce reason-able constraints in the first CRF?s Viterbi decod-ing.
While significantly outperforming the state-of-the-art model by Joty et al (2013), our parseris much faster in practice.
In addition, we pro-pose a novel idea of post-editing, which modifies afully-built discourse tree by considering informa-tion from upper-level constituents.
We show that,although doubling the time consumption, post-editing can further boost the parsing performanceto close to 90% of human performance.In future work, we wish to further explore theidea of post-editing, since currently we use onlythe depth of the subtrees as upper-level informa-tion.
Moreover, we wish to study whether we canincorporate constraints into the relation models, aswe do to the structure models.
For example, itmight be helpful to train the relation models us-ing additional criteria, such as Generalized Ex-pectation (Mann and McCallum, 2008), to bettertake into account some prior knowledge about therelations.
Last but not least, as reflected by thelow MAFS in our experiments, some particularlydifficult relation types might need specifically de-signed features for better recognition.AcknowledgmentsWe thank Professor Gerald Penn and the review-ers for their valuable advice and comments.
Thiswork was financially supported by the NaturalSciences and Engineering Research Council ofCanada and by the University of Toronto.519ReferencesJason Baldridge and Alex Lascarides.
2005.
Proba-bilistic head-driven parsing for discourse structure.In Proceedings of the Ninth Conference on Compu-tational Natural Language Learning (CoNLL-2005),pages 96?103, Ann Arbor, Michigan, June.
Associ-ation for Computational Linguistics.Regina Barzilay and Mirella Lapata.
2008.
Modelinglocal coherence: an entity-based approach.
Compu-tational Linguistics, 34(1):1?34.Lynn Carlson, Daniel Marcu, and Mary EllenOkurowski.
2001.
Building a discourse-tagged cor-pus in the framework of Rhetorical Structure The-ory.
In Proceedings of Second SIGDial Workshopon Discourse and Dialogue (SIGDial 2001), pages1?10.Marie-Catherine de Marneffe, Bill MacCartney, andChristopher D. Manning.
2006.
Generating typeddependency parses from phrase structure parses.
InProceedings of the 5th International Conference onLanguage Resources and Evaluation (LREC 2006).Vanessa Wei Feng and Graeme Hirst.
2012.
Text-leveldiscourse parsing with rich linguistic features.
InProceedings of the 50th Annual Meeting of the Asso-ciation for Computational Linguistics: Human Lan-guage Technologies (ACL 2012), pages 60?68, Jeju,Korea.Hugo Hernault, Helmut Prendinger, David A. duVerle,and Mitsuru Ishizuka.
2010.
HILDA: A discourseparser using support vector machine classification.Dialogue and Discourse, 1(3):1?33.Shafiq Joty, Giuseppe Carenini, and Raymond T.Ng.
2012.
A novel discriminative frameworkfor sentence-level discourse analysis.
In Proceed-ings of the 2012 Joint Conference on EmpiricalMethods in Natural Language Processing and Com-putational Natural Language Learning, EMNLP-CoNLL 2012, pages 904?915.Shafiq Joty, Giuseppe Carenini, Raymond Ng, andYashar Mehdad.
2013.
Combining intra- and multi-sentential rhetorical parsing for document-level dis-course analysis.
In Proceedings of the 51st AnnualMeeting of the Association for Computational Lin-guistics (ACL 2013), pages 486?496, Sofia, Bul-garia, August.Dan Klein and Christopher D. Manning.
2003.
Ac-curate unlexicalized parsing.
In Proceedings of the41st Annual Meeting on Association for Computa-tional Linguistics (ACL 2003), ACL 2003, pages423?430, Stroudsburg, PA, USA.
Association forComputational Linguistics.Alistair Knott and Robert Dale.
1994.
Using linguisticphenomena to motivate a set of coherence relations.Discourse Processes, 18(1):35?64.John D. Lafferty, Andrew McCallum, and FernandoC.
N. Pereira.
2001.
Conditional random fields:Probabilistic models for segmenting and labeling se-quence data.
In Proceedings of the Eighteenth Inter-national Conference on Machine Learning, ICML2001, pages 282?289, San Francisco, CA, USA.Morgan Kaufmann Publishers Inc.Gideon S. Mann and Andrew McCallum.
2008.
Gen-eralized Expectation Criteria for semi-supervisedlearning of Conditional Random Fields.
In Proceed-ings of the 46th Annual Meeting of the Associationfor Computational Linguistics: Human LanguageTechnologies (ACL 2008), pages 870?878, Colum-bus, Ohio, June.
Association for Computational Lin-guistics.William Mann and Sandra Thompson.
1988.
Rhetor-ical structure theory: Toward a functional theory oftext organization.
Text, 8(3):243?281.Daniel Marcu.
2000.
The Theory and Practice of Dis-course Parsing and Summarization.
The MIT Press.Andrew Kachites McCallum.
2002.
MAL-LET: A machine learning for language toolkit.http://mallet.cs.umass.edu.Philippe Muller, Stergos Afantenos, Pascal Denis, andNicholas Asher.
2012.
Constrained decoding fortext-level discourse parsing.
In Proceedings ofCOLING 2012, pages 1883?1900, Mumbai, India,December.
The COLING 2012 Organizing Commit-tee.Naoaki Okazaki.
2007.
CRFsuite: a fast im-plementation of conditional random fields (CRFs).http://www.chokkan.org/software/crfsuite/.Marta Recasens, Marie-Catherine de Marneffe, andChristopher Potts.
2013.
The life and death of dis-course entities: Identifying singleton mentions.
InProceedings of the 2013 Conference of the NorthAmerican Chapter of the Association for Computa-tional Linguistics: Human Language Technologies,pages 627?633, Atlanta, Georgia, June.
Associationfor Computational Linguistics.Rajen Subba and Barbara Di Eugenio.
2009.
An effec-tive discourse parser that uses rich linguistic infor-mation.
In Proceedings of Human Language Tech-nologies: The 2009 Annual Conference of the NorthAmerican Chapter of the Association for Compu-tational Linguistics, pages 566?574, Boulder, Col-orado, June.
Association for Computational Linguis-tics.Charles Sutton, Andrew McCallum, and KhashayarRohanimanesh.
2007.
Dynamic conditional randomfields: Factorized probabilistic models for labelingand segmenting sequence data.
The Journal of Ma-chine Learning Research, 8:693?723, May.Christian Wojek and Bernt Schiele.
2008.
A dynamicconditional random field model for joint labeling ofobject and scene classes.
In European Conference520on Computer Vision (ECCV 2008), pages 733?747,Marseille, France.Dong Yang, Paul Dixon, Yi-Cheng Pan, Tasuku Oon-ishi, Masanobu Nakamura, and Sadaoki Furui.2009.
Combining a two-step conditional randomfield model and a joint source channel model formachine transliteration.
In Proceedings of the 2009Named Entities Workshop: Shared Task on Translit-eration (NEWS 2009), pages 72?75, Suntec, Singa-pore, August.
Association for Computational Lin-guistics.521
