Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1415?1424,Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational LinguisticsA Simple Measure to Assess Non-responseAnselmo Pen?as and Alvaro RodrigoUNED NLP & IR GroupJuan del Rosal, 1628040 Madrid, Spain{anselmo,alvarory@lsi.uned.es}AbstractThere are several tasks where is preferable notresponding than responding incorrectly.
Thisidea is not new, but despite several previous at-tempts there isn?t a commonly accepted mea-sure to assess non-response.
We study here anextension of accuracy measure with this fea-ture and a very easy to understand interpreta-tion.
The measure proposed (c@1) has a goodbalance of discrimination power, stability andsensitivity properties.
We show also how thismeasure is able to reward systems that main-tain the same number of correct answers andat the same time decrease the number of in-correct ones, by leaving some questions unan-swered.
This measure is well suited for taskssuch as Reading Comprehension tests, wheremultiple choices per question are given, butonly one is correct.1 IntroductionThere is some tendency to consider that an incorrectresult is simply the absence of a correct one.
This isparticularly true in the evaluation of Information Re-trieval systems where, in fact, the absence of resultssometimes is the worse output.However, there are scenarios where we shouldconsider the possibility of not responding, becausethis behavior has more value than responding incor-rectly.
For example, during the process of introduc-ing new features in a search engine it is importantto preserve users?
confidence in the system.
Thus,a system must decide whether it should give or nota result in the new fashion or keep on with the oldkind of output.
A similar example is the decisionabout showing or not ads related to the query.
Show-ing wrong ads harms the business model more thanshowing nothing.
A third example more related toNatural Language Processing is the Machine Read-ing evaluation through reading comprehension tests.In this case, where multiple choices for a questionare offered, choosing a wrong option should be pun-ished against leaving the question unanswered.In the latter case, the use of utility functions isa very common option.
However, utility functionsgive arbitrary value to not responding and ignorethe system?s behavior showed when it responds (seeSection 2).
To avoid this, we present c@1 measure(Section 2.2), as an extension of accuracy (the pro-portion of correctly answered questions).
In Sec-tion 3 we show that no other extension produces asensible measure.
In Section 4 we evaluate c@1 interms of stability, discrimination power and sensibil-ity, and some real examples of its behavior are givenin the context of Question Answering.
Related workis discussed in Section 5.2 Looking for the Value of Not RespondingLets take the scenario of Reading Comprehensiontests to argue about the development of the measure.Our scenario assumes the following:?
There are several questions.?
Each question has several options.?
One option is correct (and only one).The first step is to consider the possibility of notresponding.
If the system responds, then the assess-ment will be one of two: correct or wrong.
But if1415the system doesn?t respond there is no assessment.Since every question has a correct answer, non re-sponse is not correct but it is not incorrect either.This is represented in contingency Table 1, where:?
nac: number of questions for which the answeris correct?
naw: number of questions for which the answeris incorrect?
nu: number of questions not answered?
n: number of questions (n = nac + naw + nu)Correct (C) Incorrect (?C)Answered (A) nac nawUnanswered (?A) nuTable 1: Contingency table for our scenarioLet?s start studying a simple utility function ableto establish the preference order we want:?
-1 if question receives an incorrect response?
0 if question is left unanswered?
1 if question receives a correct responseLet U(i) be the utility function that returns one ofthe above values for a given question i.
Thus, if wewant to consider n questions in the evaluation, themeasure would be:UF = 1nn?i=1U(i) = nac ?
nawn(1)The rationale of this utility function is intuitive:not answering adds no value and wrong answers addnegative values.
Positive values of UF indicate morecorrect answers than incorrect ones, while negativevalues indicate the opposite.
However, the utilityfunction is giving an arbitrary value to the prefer-ences (-1, 0, 1).Now we want to interpret in some way the valuethat Formula (1) assigns to unanswered questions.For this purpose, we need to transform Formula (1)into a more meaningful measure with a parameterfor the number of unanswered questions (nu).
Amonotonic transformation of (1) permit us to pre-serve the ranking produced by the measure.
Letf(x)=0.5x+0.5 be the monotonic function to be usedfor the transformation.
Applying this function toFormula (1) results in Formula (2):0.5nac ?
nawn+ 0.5 = 0.5n[nac ?
naw + n] == 0.5n[nac ?
naw + nac + naw + nu]= 0.5n[2nac + nu] =nacn+ 0.5nun(2)Measure (2) provides the same ranking of sys-tems than measure (1).
The first summand of For-mula (2) corresponds to accuracy, while the secondis adding an arbitrary constant weight of 0.5 to theproportion of unanswered questions.
In other words,unanswered questions are receiving the same valueas if half of them had been answered correctly.This does not seem correct given that not answer-ing is being rewarded in the same proportion to allthe systems, without taking into account the per-formance they have shown with the answered ques-tions.
We need to propose a more sensible estima-tion for the weight of unanswered questions.2.1 A rationale for the Value of UnansweredQuestionsAccording to the utility function suggested, unan-swered questions would have value as if half of themhad been answered correctly.
Why half and not othervalue?
Even more, Why a constant value?
Let?s gen-eralize this idea and estate more clearly our hypoth-esis:Unanswered questions have the same value as if aproportion of them would have been answered cor-rectly.We can express this idea according to contingencyTable 1 in the following way:P (C) = P (C ?A) + P (C ?
?A) == P (C ?A) + P (C/?A) ?
P (?A)(3)P (C ?
A) can be estimated by nac/n, P (?A)can be estimated by nu/n, and we have to estimateP (C/?A).
Our hypothesis is saying that P (C/?A)1416is different from 0.
The utility measure (2) corre-sponds to P(C) in Formula (3) where P (C/?A) re-ceives a constant value of 0.5.
It is assuming arbi-trarily that P (C/?A) = P (C/A).Following this, our measure must consist of twoparts: The overall accuracy and a better estimationof correctness over the unanswered questions.2.2 The Measure Proposed: c@1From the answered questions we have already ob-served the proportion of questions that received acorrect answer (P (C ?A) = nac/n).
We can use thisobservation as our estimation for P (C/?A) insteadof the arbitrary value of 0.5.Thus, the measure we propose is c@1 (correct-ness at one) and is formally represented as follows:c@1 = nacn+ nacnnun= 1n(nac +nacnnu) (4)The most important features of c@1 are:1.
A system that answers all the questions will re-ceive a score equal to the traditional accuracymeasure: nu=0 and therefore c@1=nac/n.2.
Unanswered questions will add value to c@1as if they were answered with the accuracy al-ready shown.3.
A system that does not return any answer wouldreceive a score equal to 0 due to nac=0 in bothsummands.According to the reasoning above, we can inter-pret c@1 in terms of probability as P (C) whereP (C/?A) has been estimated with P (C ?
A).
Inthe following section we will show that there is noother estimation for P (C/?A) able to provide a rea-sonable evaluation measure.3 Other Estimations for P (C/?A)In this section we study whether other estimationsof P (C/?A) can provide a sensible measure for QAwhen unanswered questions are taken into account.They are:1.
P (C/?A) ?
02.
P (C/?A) ?
13.
P (C/?A) ?
P (?C/?A) ?
0.54.
P (C/?A) ?
P (C/A)5.
P (C/?A) ?
P (?C/A)3.1 P (C/?A) ?
0This estimation considers the absence of response asincorrect response and we have the traditional accu-racy (nac/n).Obviously, this is against our purposes.3.2 P (C/?A) ?
1This estimation considers all unanswered questionsas correctly answered.
This option is not reasonableand is given for completeness: systems giving noanswer would get maximum score.3.3 P (C/?A) ?
P (?C/?A) ?
0.5It could be argued that since we cannot have obser-vations of correctness for unanswered questions, weshould assume equiprobability between P (C/?A)and P (?C/?A).
In this case, P(C) correspondsto the expression (2) already discussed.
As previ-ously explained, in this case we are giving an arbi-trary constant value to unanswered questions inde-pendently of the system?s performance shown withanswered ones.
This seems unfair.
We should beaiming at rewarding those systems not respondinginstead of giving wrong answers, not reward the solefact that the system is not responding.3.4 P (C/?A) ?
P (C/A)An alternative is to estimate the probability of cor-rectness for the unanswered questions as the pre-cision observed over the answered ones: P(C/A)=nac/(nac+ naw).
In this case, our measure would belike the one shown in Formula (5):P (C) = P (C ?A) + P (C/?A) ?
P (?A) == P (C/A) ?
P (A) + P (C/A) ?
P (?A) == P (C/A) = nacnac + naw(5)The resulting measure is again the observed pre-cision over the answered ones.
This is not a sensiblemeasure, as it would reward a cheating system thatdecides to leave all questions unanswered except onefor which it is sure to have a correct answer.1417Furthermore, from the idea that P (C/?A) isequal to P (C/A) the underlying assumption is thatsystems choose to answer or not to answer ran-domly, whereas we want to reward the systems thatchoose not responding because they are able to de-cide that their candidate options are wrong or be-cause they are unable to decide which candidate iscorrect.3.5 P (C/?A) ?
P (?C/A)The last option to be considered explores the ideathat systems fail not responding in the same propor-tion that they fail when they give an answer (i.e.
pro-portion of incorrect answers).Estimating P (C/?A) as naw / (nac+ naw), themeasure would be:P (C) = P (C ?A) + P (C/?A) ?
P (?A) == P (C ?A) ?
P (?C/A) ?
P (?A) == nacn+ nawnac + naw?
nun(6)This measure is very easy to cheat.
It is possibleto obtain almost a perfect score just by answering in-correctly only one question and leaving unansweredthe rest of the questions.4 Evaluation of c@1When a new measure is proposed, it is importantto study the reliability of the results obtained us-ing that measure.
For this purpose, we have cho-sen the method described by Buckley and Voorhees(2000) for assessing the stability and discriminationpower, as well as the method described by Voorheesand Buckley (2002) for examining the sensitivity ofour measure.
These methods have been used forstudying IR metrics (showing similar results withthe methods based on statistics (Sakai, 2006)), aswell as for evaluating the reliability of other QAmeasures different to the ones studied here (Sakai,2007a; Voorhees, 2002; Voorhees, 2003).We have compared the results over c@1 with theones obtained using both accuracy and the utilityfunction (UF) defined in Formula (1).
This compari-son is useful to show how confident can a researcherbe with the results obtained using each evaluationmeasure.In the following subsections we will first show thedata used for our study.
Then, the experiments aboutstability and sensitivity will be described.4.1 Data setsWe used the test collections and runs from the Ques-tion Answering track at the Cross Language Evalu-ation Forum 2009 (CLEF) (Pen?as et al, 2010).
Thecollection has a set of 500 questions with their an-swers.
The 44 runs in different languages containthe human assessments for the answers given by ac-tual participants.
Systems could chose not to answera question.
In this case, they had the chance to sub-mit their best candidate in order to assess the perfor-mance of their validation module (the one that de-cides whether to give or not the answer).This data collection allows us to compare c@1and accuracy over the same runs.4.2 Stability vs.
Discrimination PowerThe more stable a measure is, the lower the probabil-ity of errors associated with the conclusion ?systemA is better than system B?
is.
Measures with a higherror must be used more carefully performing moreexperiments than in the case of using a measure withlower error.In order to study the stability of c@1 and to com-pare it with accuracy we used the method describedby Buckley and Voorhees (2000).
This method al-lows also to study the number of times systems aredeemed to be equivalent with respect to a certainmeasure, which reflects the discrimination power ofthat measure.
The less discriminative the measureis, the more ties between systems there will be.
Thismeans that longer difference in scores will be neededfor concluding which system is better (Buckley andVoorhees, 2000).The method works as follows: let S denote a setof runs.
Let x and y denote a pair of runs from S.Let Q denote the entire evaluation collection.
Let frepresents the fuzziness value, which is the percentdifference between scores such that if the differenceis smaller than f then the two scores are deemed tobe equivalent.
We apply the algorithm of Figure 1to obtain the information needed for computing theerror rate (Formula (7)).
Stability is inverse to thisvalue, the lower the error rate is, the more stablethe measure is.
The same algorithm gives us the1418proportion of ties (Formula (8)), which we use formeasuring discrimination power, that is the lowerthe proportion of ties is, the more discriminative themeasure is.for each pair of runs x,y ?
Sfor each trial from 1 to 100Qi = select at random subcol of size c from Q;margin = f * max (M(x,Qi),M(y,Qi));if(|M(x,Qi) - M(y,Qi)| < |margin|)EQM (x,y)++;else if(|M(x,Qi) > M(y,Qi)|)GTM (x,y)++;elseGTM (y,x)++;Figure 1: Algorithm for computing EQM (x,y),GTM (x,y) and GTM (y,x) in the stability methodWe assume that for each measure the correct de-cision about whether run x is better than run y hap-pens when there are more cases where the value ofx is better than the value of y.
Then, the number oftimes y is better than x is considered as the numberof times the test is misleading, while the number oftimes the values of x and y are equivalent is consid-ered the number of ties.On the other hand, it is clear that larger fuzzinessvalues decrease the error rate but also decrease thediscrimination power of a measure.
Since a fixedfuzziness value might imply different trade-offs fordifferent metrics, we decided to vary the fuzzinessvalue from 0.01 to 0.10 (following the work by Sakai(2007b)) and to draw for each measure a proportion-of-ties / error-rate curve.
Figure 2 shows thesecurves for the c@1, accuracy and UF measures.
Inthe Figure we can see how there is a consistent de-crease of the error rate of all measures when theproportion of ties increases (this corresponds to theincrease in the fuzziness value).
Figure 2 showsthat the curves of accuracy and c@1 are quite simi-lar (slightly better behavior of c@1) , which meansthat they have a similar stability and discriminationpower.The results suggest that the three measures arequite stable, having c@1 and accuracy a lower er-ror rate than UF when the proportion of ties grows.These curves are similar to the ones obtained forFigure 2: Error-rate / Proportion of ties curves for accu-racy, c@1 and UF with c = 250other QA evaluation measures (Sakai, 2007a).4.3 SensitivityThe swap-rate (Voorhees and Buckley, 2002) repre-sents the chance of obtaining a discrepancy betweentwo question sets (of the same size) as to whethera system is better than another given a certain dif-ference bin.
Looking at the swap-rates of all thedifference performance bins, the performance dif-ference required in order to conclude that a run isbetter than another for a given confidence value canbe estimated.
For example, if we want to know therequired difference for concluding that system A isbetter than system B with a confidence of 95%, thenwe select the difference that represents the first binwhere the swap-rate is lower or equal than 0.05.The sensitivity of the measure is the number oftimes among all the comparisons in the experi-ment where this performance difference is obtained(Sakai, 2007b).
That is, the more comparisons ac-complish the estimated performance difference, themore sensitive is the measure.
The more sensitivethe measure, the more useful it is for system dis-crimination.The swap method works as follows: let S denotea set of runs, let x and y denote a pair of runs from S.Let Q denote the entire evaluation collection.
Andlet d denote a performance difference between tworuns.
Then, we first define 21 performance differ-ence bins: the first bin represents performance dif-ferences between systems such that 0 ?
d < 0.01;the second bin represents differences such that 0.01?
d < 0.02; and the limits for the remaining bins in-crease by increments of 0.01, with the last bin con-taining all the differences equal or higher than 0.2.1419Error rateM =?x,y?S min(GTM (x, y), GTM (y, x))?x,y?S(GTM (x, y) + GTM (y, x) + EQM (x, y))(7)Prop T iesM =?x,y?S EQM (x, y)?x,y?S(GTM (x, y) + GTM (y, x) + EQM (x, y))(8)Let BIN(d) denote a mapping from a difference d toone of the 21 bins where it belongs.
Thus, algorithmin Figure 3 is applied for calculating the swap-rateof each bin.for each pair of runs x,y ?
Sfor each trial from 1 to 100select Qi , Q?i ?
Q, whereQi ?
Q?i == ?
and |Qi| == |Q?i| == c;dM (Qi) = M(x,Qi)?M(y,Qi);dM (Q?i) = M(x,Q?i)?M(y,Q?i);counter(BIN(|dM (Qi)|))++;if(dM (Qi) * dM (Q?i) < 0)swap counter(BIN(|dM (Qi)|))++;for each bin bswap rate(b) = swap counter(b)/counter(b);Figure 3: Algorithm for computing swap-rates(i) (ii) (iii) (iv)UF 0.17 0.48 35.12% 59.30%c@1 0.09 0.77 11.69% 58.40%accuracy 0.09 0.68 13.24% 55.00%Table 2: Results obtained applying the swap method toaccuracy, c@1 and UF at 95% of confidence, with c =250: (i) Absolute difference required; (ii) Highest valueobtained; (iii) Relative difference required ((i)/(ii)); (iv)percentage of comparisons that accomplish the requireddifference (sensitivity)Given that Qi and Q?i must be disjoint, their sizecan only be up to half of the size of the original col-lection.
Thus, we use the value c=250 for our exper-iment1.
Table 2 shows the results obtained by apply-ing the swap method to accuracy, c@1 and UF, withc = 250, swap-rate ?
5, and sensitivity given a con-fidence of 95% (Column (iv)).
The range of values1We use the same size for experiments in Section 4.2 forhomogeneity reasons.are similar to the ones obtained for other measuresaccording to (Sakai, 2007a).According to Column (i), a higher absolute dif-ference is required for concluding that a system isbetter than another using UF.
However, the relativedifference is similar to the one required by c@1.Thus, similar percentage of comparisons using c@1and UF accomplish the required difference (Column(iv)).
These results show that their sensitivity valuesare similar, and higher than the value for accuracy.4.4 Qualitative evaluationIn addition to the theoretical study, we undertook astudy to interpret the results obtained by real sys-tems in a real scenario.
The aim is to compare theresults of the proposed c@1 measure with accuracyin order to compare their behavior.
For this purposewe inspected the real systems runs in the data set.System c@1 accuracy (i) (ii) (iii)icia091ro 0.58 0.47 237 156 107uaic092ro 0.47 0.47 236 264 0loga092de 0.44 0.37 187 230 83base092de 0.38 0.38 189 311 0Table 3: Example of system results in QA@CLEF 2009.
(i) number of questions correctly answered; (ii) numberof questions incorrectly answered; (iii) number of unan-swered questions.Table 3 shows a couple of examples where twosystems have answered correctly a similar num-ber of questions.
For example, this is the case oficia091ro and uaic092ro that, therefore, obtain al-most the same accuracy value.
However, icia091rohas returned less incorrect answers by not respond-ing some questions.
This is the kind of behavior wewant to measure and reward.
Table 3 shows howaccuracy is sensitive only to the number of correctanswers whereas c@1 is able to distinguish when1420systems keep the number of correct answers but re-duce the number of incorrect ones by not respond-ing to some.
The same reasoning is applicable tologa092de compared to base092de for German.5 Related WorkThe decision of leaving a query without response isrelated to the system ability to measure accurately itsself-confidence about the correctness of their candi-date answers.
Although there have been one attemptto make the self-confidence score explicit and useit (Herrera et al, 2005), rankings are, usually, theimplicit way to evaluate this self-confidence.
MeanReciprocal Rank (MRR) has traditionally been usedto evaluate Question Answering systems when sev-eral answers per question were allowed and givenin order (Fukumoto et al, 2002; Voorhees and Tice,1999).
However, as it occurs with Accuracy (propor-tion of questions correctly answered), the risk of giv-ing a wrong answer is always preferred better thannot responding.The QA track at TREC 2001 was the first eval-uation campaign in which systems were allowedto leave a question unanswered (Voorhees, 2001).The main evaluation measure was MRR, but perfor-mance was also measured by means of the percent-age of answered questions and the portion of themthat were correctly answered.
However, no combi-nation of these two values into a unique measure wasproposed.TREC 2002 discarded the idea of including unan-swered questions in the evaluation.
Only one answerby question was allowed and all answers had to beranked according to the system?s self-confidence inthe correctness of the answer.
Systems were evalu-ated by means ofConfidence Weighted Score (CWS),rewarding those systems able to provide more cor-rect answers at the top of the ranking (Voorhees,2002).
The formulation of CWS is the following:CWS = 1nn?i=1C(i)i(9)Where n is the number of questions, and C(i) isthe number of correct answers up to the position i inthe ranking.
Formally:C(i) =i?j=1I(j) (10)where I(j) is a function that returns 1 if answer jis correct and 0 if it is not.
The formulation of CWSis inspired by the Average Precision (AP) over theranking for one question:AP = 1R?rI(r)C(r)r(11)where R is the number of known relevant resultsfor a topic, and r is a position in the ranking.
Sinceonly one answer per question is requested, R equalsto n (the number of questions) in CWS.
However,in AP formula the summands belong to the posi-tions of the ranking where there is a relevant result(product of I(r)), whereas in CWS every position ofthe ranking add value to the measure regardless ofwhether there is a relevant result or not in that po-sition.
Therefore, CWS gives much more value tosome questions over others: questions whose an-swers are at the top of the ranking are giving almostthe complete value to CWS, whereas those questionswhose answers are at the bottom of the ranking arealmost not counting in the evaluation.Although CWS was aimed at promoting the de-velopment of better self-confidence scores, it wasdiscussed as a measure for evaluating QA systemsperformance.
CWS was discarded in the followingcampaigns of TREC in favor of accuracy (Voorhees,2003).
Subsequently, accuracy was adopted by theQA track at the Cross-Language Evaluation Forumfrom the beginning (Magnini et al, 2005).There was an attempt to consider explicitly sys-tems confidence self-score (Herrera et al, 2005): theuse of the Pearson?s correlation coefficient and theproposal of measures K and K1 (see Formula 12).These measures are based in a utility function thatreturns -1 if the answer is incorrect and 1 if it iscorrect.
This positive or negative value is weightedwith the normalized confidence self-score given bythe system to each answer.
K is a variation of K1for being used in evaluations where more than ananswer per question is allowed.If the self-score is 0, then the answer is ignoredand thus, this measure is permitting to leave a ques-tion unanswered.
A system that always returns a1421K1 =?i?
{correctanswers}self score(i)??i?
{incorrectanswers}self score(i)n?
[?1, 1] (12)self-score equals to 0 (no answer) obtains a K1 valueof 0.
However, the final value of K1 is difficult tointerpret: a positive value does not indicate neces-sarily more correct answers than incorrect ones, butthat the sum of scores of correct answers is higherthan the sum resulting from the scores of incorrectanswers.
This could explain the little success of thismeasure for evaluating QA systems in favor, again,of accuracy measure.Accuracy is the simplest and most intuitive evalu-ation measure.
At the same time is able to rewardthose systems showing good performance.
How-ever, together with MRR belongs to the set of mea-sures that pushes in favor of giving always a re-sponse, even wrong, since there is no punishment forit.
Thus, the development of better validation tech-nologies (systems able to decide whether the can-didate answers are correct or not) is not promoted,despite new QA architectures require them.In effect, most QA systems during TREC andCLEF campaigns had an upper bound of accuracyaround 60%.
An explanation for this was the effectof error propagation in the most extended pipelinearchitecture: Passage Retrieval, Answer Extraction,Answer Ranking.
Even with performances higherthan 80% in each step, the overall performancedrops dramatically just because of the product ofpartial performances.
Thus, a way to break thepipeline architecture is the development of a mod-ule able to decide whether the QA system must con-tinue or not its searching for new candidate answers:the Answer Validation module.
This idea is behindthe architecture of IBM?s Watson (DeepQA project)that successfully participated at Jeopardy (Ferrucciet al, 2010).In 2006, the first Answer Validation Exercise(AVE) proposed an evaluation task to advance thestate of the art in Answer Validation technologies(Pen?as et al, 2007).
The starting point was the re-formulation of Answer Validation as a RecognizingTextual Entailment problem, under the assumptionthat hypotheses can be automatically generated bycombining the question with the candidate answer(Pen?as et al, 2008a).
Thus, validation was seen as abinary classification problem whose evaluation mustdeal with unbalanced collections (different propor-tion of positive and negative examples, correct andincorrect answers).
For this reason, AVE 2006 usedF-measure based on precision and recall for correctanswers selection (Pen?as et al, 2007).
Other op-tion is an evaluation based on the analysis of Re-ceiver Operating Characteristic (ROC) space, some-times preferred for classification tasks with unbal-anced collections.
A comparison of both approachesfor Answer Validation evaluation is provided in (Ro-drigo et al, 2011).AVE 2007 changed its evaluation methodologywith two objectives: the first one was to bring sys-tems based on Textual Entailment to the AutomaticHypothesis Generation problem which is not part it-self of the Recognising Textual Entailment (RTE)task but an Answer Validation need.
The secondone was an attempt to quantify the gain in QA per-formance when more sophisticated validation mod-ules are introduced (Pen?as et al, 2008b).
With thisaim, several measures were proposed to assess: thecorrect selection of candidate answers, the correctrejection of wrong answer and finally estimate thepotential gain (in terms of accuracy) that AnswerValidation modules can provide to QA (Rodrigo etal., 2008).
The idea was to give value to the cor-rectly rejected answers as if they could be correctlyanswered with the accuracy shown selecting the cor-rect answers.
This extension of accuracy in the An-swer Validation scenario inspired the initial develop-ment of c@1 considering non-response.6 ConclusionsThe central idea of this work is that not respond-ing has more value than responding incorrectly.
Thisidea is not new, but despite several attempts in TRECand CLEF there wasn?t a commonly accepted mea-1422sure to assess non-response.
We have studied herean extension of accuracy measure with this feature,and with a very easy to understand rationale: Unan-swered questions have the same value as if a pro-portion of them had been answered correctly, andthe value they add is related to the performance (ac-curacy) observed over the answered questions.
Wehave shown that no other estimation of this valueproduce a sensible measure.We have shown also that the proposed measurec@1 has a good balance of discrimination power,stability and sensitivity properties.
Finally, we haveshown how this measure rewards systems able tomaintain the same number of correct answers and atthe same time reduce the number of incorrect ones,by leaving some questions unanswered.Among other tasks, measure c@1 is well suitedfor evaluating Reading Comprehension tests, wheremultiple choices per question are given, but only oneis correct.
Non-response must be assessed if wewant to measure effective reading and not just theability to rank options.
This is clearly not enoughfor the development of reading technologies.AcknowledgmentsThis work has been partially supported by theResearch Network MA2VICMR (S2009/TIC-1542)and Holopedia project (TIN2010-21128-C02).ReferencesChris Buckley and Ellen M. Voorhees.
2000.
Evalu-ating evaluation measure stability.
In Proceedings ofthe 23rd annual international ACM SIGIR conferenceon Research and development in information retrieval,pages 33?40.
ACM.David Ferrucci, Eric Brown, Jennifer Chu-Carroll, JamesFan, David Gondek, Aditya A. Kalyanpur, AdamLally, J. William Murdock, Eric Nyberg, John Prager,Nico Schlaefer, and Chris Welty.
2010.
Building Wat-son: An Overview of the DeepQA Project.
AI Maga-zine, 31(3).Junichi Fukumoto, Tsuneaki Kato, and Fumito Masui.2002.
Question and Answering Challenge (QAC-1): Question Answering Evaluation at NTCIR Work-shop 3.
In Working Notes of the Third NTCIR Work-shop Meeting Part IV: Question Answering Challenge(QAC-1), pages 1-10.Jesu?s Herrera, Anselmo Pen?as, and Felisa Verdejo.
2005.Question Answering Pilot Task at CLEF 2004.
InMul-tilingual Information Access for Text, Speech and Im-ages, CLEF 2004, Revised Selected Papers., volume3491 of Lecture Notes in Computer Science, Springer,pages 581?590.Bernardo Magnini, Alessandro Vallin, Christelle Ayache,Gregor Erbach, Anselmo Pen?as, Maarten de Rijke,Paulo Rocha, Kiril Ivanov Simov, and Richard F. E.Sutcliffe.
2005.
Overview of the CLEF 2004 Multi-lingual Question Answering Track.
InMultilingual In-formation Access for Text, Speech and Images, CLEF2004, Revised Selected Papers., volume 3491 of Lec-ture Notes in Computer Science, Springer, pages 371?391.Anselmo Pen?as, A?lvaro Rodrigo, Valent?
?n Sama, and Fe-lisa Verdejo.
2007.
Overview of the Answer Valida-tion Exercise 2006.
In Evaluation of Multilingual andMulti-modal Information Retrieval, CLEF 2006, Re-vised Selected Papers, volume 4730 of Lecture Notesin Computer Science, Springer, pages 257?264.Anselmo Pen?as, A?lvaro Rodrigo, Valent?
?n Sama, and Fe-lisa Verdejo.
2008a.
Testing the Reasoning for Ques-tion Answering Validation.
In Journal of Logic andComputation.
18(3), pages 459?474.Anselmo Pen?as, A?lvaro Rodrigo, and Felisa Verdejo.2008b.
Overview of the Answer Validation Exercise2007.
In Advances in Multilingual and MultimodalInformation Retrieval, CLEF 2007, Revised SelectedPapers, volume 5152 of Lecture Notes in ComputerScience, Springer, pages 237?248.Anselmo Pen?as, Pamela Forner, Richard Sutcliffe, A?lvaroRodrigo, Corina Forascu, In?aki Alegria, Danilo Gi-ampiccolo, Nicolas Moreau, and Petya Osenova.2010.
Overview of ResPubliQA 2009: Question An-swering Evaluation over European Legislation.
InMultilingual Information Access Evaluation I.
Text Re-trieval Experiments, CLEF 2009, Revised Selected Pa-pers, volume 6241 of Lecture Notes in Computer Sci-ence, Springer.Alvaro Rodrigo, Anselmo Pen?as, and Felisa Verdejo.2008.
Evaluating Answer Validation in Multi-streamQuestion Answering.
In Proceedings of the Second In-ternational Workshop on Evaluating Information Ac-cess (EVIA 2008).Alvaro Rodrigo, Anselmo Pen?as, and Felisa Verdejo.2011.
Evaluating Question Answering Validation as aclassification problem.
Language Resources and Eval-uation, Springer Netherlands (In Press).Tetsuya Sakai.
2006.
Evaluating Evaluation Metricsbased on the Bootstrap.
In SIGIR 2006: Proceedingsof the 29th Annual International ACM SIGIR Confer-ence on Research and Development in Information Re-trieval, Seattle, Washington, USA, August 6-11, 2006,pages 525?532.1423Tetsuya Sakai.
2007a.
On the Reliability of FactoidQuestion Answering Evaluation.
ACM Trans.
AsianLang.
Inf.
Process., 6(1).Tetsuya Sakai.
2007b.
On the reliability of informationretrieval metrics based on graded relevance.
Inf.
Pro-cess.
Manage., 43(2):531?548.Ellen M. Voorhees and Chris Buckley.
2002.
The effectof Topic Set Size on Retrieval Experiment Error.
In SI-GIR ?02: Proceedings of the 25th annual internationalACM SIGIR conference on Research and developmentin information retrieval, pages 316?323.Ellen M. Voorhees and DawnM.
Tice.
1999.
The TREC-8 Question Answering Track Evaluation.
In Text Re-trieval Conference TREC-8, pages 83?105.Ellen M. Voorhees.
2001.
Overview of the TREC 2001Question Answering Track.
In E. M. voorhees, D. K.Harman, editors: Proceedings of the Tenth Text RE-trieval Conference (TREC 2001).
NIST Special Publi-cation 500-250.Ellen M. Voorhees.
2002.
Overview of TREC 2002Question Answering Track.
In E.M. Voorhees, L. P.Buckland, editors: Proceedings of the Eleventh TextREtrieval Conference (TREC 2002).
NIST Publication500-251.Ellen M. Voorhees.
2003.
Overview of the TREC 2003Question Answering Track.
In Proceedings of theTwelfth Text REtrieval Conference (TREC 2003).1424
