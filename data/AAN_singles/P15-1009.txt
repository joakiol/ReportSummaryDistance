Proceedings of the 53rd Annual Meeting of the Association for Computational Linguisticsand the 7th International Joint Conference on Natural Language Processing, pages 84?94,Beijing, China, July 26-31, 2015.c?2015 Association for Computational LinguisticsSemantically Smooth Knowledge Graph EmbeddingShu Guo?, Quan Wang?
?, Bin Wang?, Lihong Wang?, Li Guo?
?Institute of Information Engineering, Chinese Academy of Sciences, Beijing 100093, China{guoshu,wangquan,wangbin,guoli}@iie.ac.cn?National Computer Network Emergency Response Technical TeamCoordination Center of China, Beijing 100029, Chinawlh@isc.org.cnAbstractThis paper considers the problem of em-bedding Knowledge Graphs (KGs) con-sisting of entities and relations into low-dimensional vector spaces.
Most of theexisting methods perform this task basedsolely on observed facts.
The only re-quirement is that the learned embeddingsshould be compatible within each individ-ual fact.
In this paper, aiming at furtherdiscovering the intrinsic geometric struc-ture of the embedding space, we proposeSemantically Smooth Embedding (SSE).The key idea of SSE is to take full ad-vantage of additional semantic informa-tion and enforce the embedding space tobe semantically smooth, i.e., entities be-longing to the same semantic category willlie close to each other in the embedding s-pace.
Two manifold learning algorithmsLaplacian Eigenmaps and Locally LinearEmbedding are used to model the smooth-ness assumption.
Both are formulated asgeometrically based regularization termsto constrain the embedding task.
We em-pirically evaluate SSE in two benchmarktasks of link prediction and triple classi-fication, and achieve significant and con-sistent improvements over state-of-the-artmethods.
Furthermore, SSE is a generalframework.
The smoothness assumptioncan be imposed to a wide variety of em-bedding models, and it can also be con-structed using other information besidesentities?
semantic categories.1 IntroductionKnowledge Graphs (KGs) like WordNet (Miller,1995), Freebase (Bollacker et al, 2008), and DB-?Corresponding author: Quan Wang.pedia (Lehmann et al, 2014) have become ex-tremely useful resources for many NLP relat-ed applications, such as word sense disambigua-tion (Agirre et al, 2014), named entity recogni-tion (Magnini et al, 2002), and information ex-traction (Hoffmann et al, 2011).
A KG is a multi-relational directed graph composed of entities asnodes and relations as edges.
Each edge is repre-sented as a triple of fact ?ei, rk, ej?, indicating thathead entity eiand tail entity ejare connected by re-lation rk.
Although powerful in representing struc-tured data, the underlying symbolic nature makesKGs hard to manipulate.Recently a new research direction called knowl-edge graph embedding has attracted much atten-tion (Socher et al, 2013; Bordes et al, 2013; Bor-des et al, 2014; Lin et al, 2015).
It attempts toembed components of a KG into continuous vectorspaces, so as to simplify the manipulation whilepreserving the inherent structure of the originalgraph.
Specifically, given a KG, entities and re-lations are first represented in a low-dimensionalvector space, and for each triple, a scoring func-tion is defined to measure its plausibility in thatspace.
Then the representations of entities and re-lations (i.e.
embeddings) are learned by maximiz-ing the total plausibility of observed triples.
Thelearned embeddings can further be used to benefitall kinds of tasks, such as KG completion (Socheret al, 2013; Bordes et al, 2013), relation extrac-tion (Riedel et al, 2013; Weston et al, 2013), andentity resolution (Bordes et al, 2014).To our knowledge, most of existing KG embed-ding methods perform the embedding task basedsolely on observed facts.
The only requiremen-t is that the learned embeddings should be com-patible within each individual fact.
In this pa-per we propose Semantically Smooth Embedding(SSE), a new approach which further imposes con-straints on the geometric structure of the embed-ding space.
The key idea of SSE is to make ful-84l use of additional semantic information (i.e.
se-mantic categories of entities) and enforce the em-bedding space to be semantically smooth?entitiesbelonging to the same semantic category shouldlie close to each other in the embedding space.This smoothness assumption is closely related tothe local invariance assumption exploited in mani-fold learning theory, which requires nearby pointsto have similar embeddings or labels (Belkin andNiyogi, 2001).
Thus we employ two manifoldlearning algorithms Laplacian Eigenmaps (Belkinand Niyogi, 2001) and Locally Linear Embed-ding (Roweis and Saul, 2000) to model the s-moothness assumption.
The former requires anentity to lie close to every other entity in the samecategory, while the latter represents that entity asa linear combination of its nearest neighbors (i.e.entities within the same category).
Both are for-mulated as manifold regularization terms to con-strain the KG embedding objective function.
Assuch, SSE obtains an embedding space which issemantically smooth and at the same time com-patible with observed facts.The advantages of SSE are two-fold: 1) By im-posing the smoothness assumption, SSE success-fully captures the semantic correlation betweenentities, which exists intrinsically but is over-looked in previous work on KG embedding.
2)KGs are typically very sparse, containing a rela-tively small number of facts compared to the largenumber of entities and relations.
SSE can effec-tively deal with data sparsity by leveraging ad-ditional semantic information.
Both aspects leadto more accurate embeddings in SSE.
Moreover,our approach is quite general.
The smoothness as-sumption can actually be imposed to a wide va-riety of KG embedding models.
Besides seman-tic categories, other information (e.g.
entity sim-ilarities specified by users or derived from auxil-iary data sources) can also be used to construc-t the manifold regularization terms.
And besidesKG embedding, similar smoothness assumptionscan also be applied in other embedding tasks (e.g.word embedding and sentence embedding).Our main contributions can be summarized asfollows.
First, we devise a novel KG embeddingframework that naturally requires the embeddingspace to be semantically smooth.
As far as weknow, it is the first work that imposes constraintson the geometric structure of the embedding spaceduring KG embedding.
By leveraging addition-al semantic information, our approach can alsodeal with the data sparsity issue that commonlyexists in typical KGs.
Second, we evaluate ourapproach in two benchmark tasks of link predic-tion and triple classification, and achieve signif-icant and consistent improvements over state-of-the-art models.In the remainder of this paper, we first providea brief review of existing KG embedding model-s in Section 2, and then detail the proposed SSEframework in Section 3.
Experiments and resultsare reported in Section 4.
Then in Section 5 wediscuss related work, followed by the conclusionand future work in Section 6.2 A Brief Review of KG EmbeddingKG embedding aims to embed entities and rela-tions into a continuous vector space and model theplausibility of each fact in that space.
In general, itconsists of three steps: 1) representing entities andrelations, 2) specifying a scoring function, and 3)learning the latent representations.
In the first step,given a KG, entities are represented as points (i.e.vectors) in a continuous vector space, and relation-s as operators in that space, which can be charac-terized by vectors (Bordes et al, 2013; Bordes etal., 2014; Wang et al, 2014b), matrices (Bordes etal., 2011; Jenatton et al, 2012), or tensors (Socheret al, 2013).
In the second step, for each candi-date fact ?ei, rk, ej?, an energy function f (ei, rk, ej)is further defined to measure its plausibility, withthe corresponding entity and relation representa-tions as variables.
Plausible triples are assumed tohave low energies.
Then in the third step, to obtainthe entity and relation representations, a margin-based ranking loss, i.e.,L=?t+?O?t?
?Nt+[?+ f (ei, rk, ej)?
f (e?i, rk, e?j)]+, (1)is minimized.
Here, O is the set of observed (i.e.positive) triples, and t+= ?ei, rk, ej?
?
O; Nt+de-notes the set of negative triples constructed by re-placing entities in t+, and t?= ?e?i, rk, e?j?
?
Nt+;?
> 0 is a margin separating positive and nega-tive triples; and [x]+= max(0, x).
The rankingloss favors lower energies for positive triples thanfor negative ones.
Stochastic gradient descent (inmini-batch mode) is adopted to solve the mini-mization problem.
For details please refer to (Bor-des et al, 2013) and references therein.Different embedding models differ in the first t-wo steps: entity/relation representation and energy85Method Entity/Relation embeddings Energy functionTransE (Bordes et al, 2013) e, r ?
Rdf (ei, rk, ej) = ?ei+ rk?
ej?
?1/?2SME (lin) (Bordes et al, 2014) e, r ?
Rdf (ei, rk, ej) =(Wu1rk+Wu2ei+ bu)T(Wv1rk+Wv2ej+ bv)SME (bilin) (Bordes et al, 2014) e, r ?
Rdf (ei, rk, ej) =((Wu?
?3rk)ei+ bu)T((Wv?
?3rk)ej+ bv)SE (Bordes et al, 2011) e ?
Rd, Ru,Rv?
Rd?df (ei, rk, ej) = ?Rukei?
Rvkej?
?1Table 1: Existing KG embedding models.function definition.
Three state-of-the-art embed-ding models, namely TransE (Bordes et al, 2013),SME (Bordes et al, 2014), and SE (Bordes et al,2011), are detailed below.
Please refer to (Jenat-ton et al, 2012; Socher et al, 2013; Wang et al,2014b; Lin et al, 2015) for other methods.TransE (Bordes et al, 2013) represents both en-tities and relations as vectors in the embedding s-pace.
For a given triple ?ei, rk, ej?, the relation isinterpreted as a translation vector rkso that theembedded entities eiand ejcan be connected byrkwith low error.
The energy function is definedas f (ei, rk, ej) = ?ei+ rk?
ej?
?1/?2, where ???
?1/?2denotes the ?1-norm or ?2-norm.SME (Bordes et al, 2014) also represents enti-ties and relations as vectors, but models triples ina more expressive way.
Given a triple ?ei, rk, ej?,it first employs a function gu(?, ?
)to combine rkand ei, and gv(?, ?
)to combine rkand ej.
Then,the energy function is defined as matching gu(?, ?
)and gv(?, ?
)by their dot product, i.e., f (ei, rk, ej) =gu(rk, ei)Tgv(rk, ej).
There are two versions ofSME, linear and bilinear (denoted as SME (lin)and SME (bilin) respectively), obtained by defin-ing different gu(?, ?
)and gv(?, ?
).SE (Bordes et al, 2011) represents entities asvectors but relations as matrices.
Each relation ismodeled by a left matrix Rukand a right matrix Rvk,acting as independent projections to head and tailentities respectively.
If a triple ?ei, rk, ej?
holds,Rukeiand Rvkejshould be close to each other.
Theenergy function is f (ei, rk, ej) = ?Rukei?
Rvkej?
?1.Table 1 summarizes the entity/relation representa-tions and energy functions used in these models.3 Semantically Smooth EmbeddingThe methods introduced above perform the em-bedding task based solely on observed facts.
Theonly requirement is that the learned embeddingsshould be compatible within each individual fact.However, they fail to discover the intrinsic geo-metric structure of the embedding space.
To dealwith this limitation, we introduce Semantically S-mooth Embedding (SSE) which constrains the em-bedding task by incorporating geometrically basedregularization terms, constructed by using addi-tional semantic categories of entities.3.1 Problem FormulationSuppose we are given a KG consisting of n entitiesand m relations.
The facts observed are stored asa set of triples O ={?ei, rk, ej?}.
A triple ?ei, rk, ej?indicates that entity eiand entity ejare connectedby relation rk.
In addition, the entities are classi-fied into multiple semantic categories.
Each entitye is associated with a label ceindicating the cate-gory to which it belongs.
SSE aims to embed theentities and relations into a continuous vector s-pace which is compatible with the observed facts,and at the same time semantically smooth.To make the embedding space compatible withthe observed facts, we make use of the triple set Oand follow the same strategy adopted in previousmethods.
That is, we define an energy functionon each candidate triple (e.g.
the energy functionslisted in Table 1), and require observed triples tohave lower energies than unobserved ones (i.e.
themargin-based ranking loss defined in Eq.
(1)).To make the embedding space semantically s-mooth, we further leverage the entity category in-formation {ce}, and assume that entities within thesame semantic category should lie close to eachother in the embedding space.
This smoothnessassumption is similar to the local invariance as-sumption exploited in manifold learning theory(i.e.
nearby points are likely to have similar em-beddings or labels).
So we employ two manifoldlearning algorithms Laplacian Eigenmaps (Belkinand Niyogi, 2001) and Locally Linear Embed-ding (Roweis and Saul, 2000) to model such se-mantic smoothness, termed as LE and LLE forshort respectively.3.2 Modeling Semantic Smoothness by LELaplacian Eigenmaps (LE) is a manifold learningalgorithm that preserves local invariance between86each two data points (Belkin and Niyogi, 2001).We borrow the idea of LE and enforce semanticsmoothness by assuming:Smoothness Assumption 1 If two entities eiandejbelong to the same semantic category, they willhave embeddings eiand ejclose to each other.To encode the semantic information, we constructan adjacency matrix W1?
Rn?namong the enti-ties, with the i j-th entry defined as:w(1)i j=??????
?1, if cei= cej,0, otherwise,where cei/cejis the category label of entity ei/ej.Then, we use the following term to measure thesmoothness of the embedding space:R1=12n?i=1n?j=1?ei?
ej?22w(1)i j,where eiand ejare the embeddings of entities eiand ejrespectively.
By minimizing R1, we expectSmoothness Assumption 1: if two entities eiand ejbelong to the same semantic category (i.e.
w(1)i j=1), the distance between eiand ej(i.e.
?ei?
ej?22)should be small.We further incorporate R1as a regularizationterm into the margin-based ranking loss (i.e.
Eq.
(1)) adopted in previous KG embedding methods,and propose our first SSE model.
The new mod-el performs the embedding task by minimizing thefollowing objective function:L1=1N?t+?O?t??Nt+?
(t+, t?)+?12n?i=1n?j=1?ei?
ej?22w(1)i j,where ?
(t+, t?
)=[?+ f (ei, rk, ej)?
f (e?i, rk, e?j)]+isthe ranking loss on the positive-negative triple pair(t+, t?
), and N is the total number of such triplepairs.
The first term in L1enforces the resultantembedding space compatible with all the observedtriples, and the second term further requires thatspace to be semantically smooth.
Hyperparameter?1makes a trade-off between the two cases.The minimization is carried out by stochasticgradient descent.
Given a randomly sampled posi-tive triple t+= ?ei, rk, ej?
and the associated nega-tive triple t?= ?e?i, rk, e?j?,1the stochastic gradientw.r.t.
es(s ?
{i, j, i?, j?})
can be calculated as:?esL1= ?es?
(t+, t?
)+ 2?1E(D ?
W1)1s,1The negative triple is constructed by replacing one of theentities in the positive triple.where E = [e1, e2, ?
?
?
, en] ?
Rd?nis a matrix con-sisting of entity embeddings; D ?
Rn?nis a di-agonal matrix with the i-th entry on the diagonalbeing dii=?nj=1w(1)i j; and 1s?
Rnis a columnvector where the s-th entry is 1 and the others are0.
Other parameters are not included in R1, andtheir gradients remain the same as defined in pre-vious work.3.3 Modeling Semantic Smoothness by LLEAs opposed to LE which preserves local invari-ance within data pairs, Locally Linear Embedding(LLE) expects each data point to be roughly re-constructed by a linear combination of its nearestneighbors (Roweis and Saul, 2000).
We borrowthe idea of LLE and enforce semantic smoothnessby assuming:Smoothness Assumption 2 Each entity eican beroughly reconstructed by a linear combination ofits nearest neighbors in the embedding space, i.e.,ei??ej?N(ei)?jej.
Here nearest neighbors referto entities belonging to the same semantic catego-ry with ei.To model this assumption, for each entity ei, werandomly sample K entities uniformly from thecategory to which eibelongs, denoted as the n-earest neighbor set N(ei).
We construct a weightmatrix W2?
Rn?nby defining:w(2)i j=??????
?1, if ej?
N(ei),0, otherwise,and normalize the rows so that?nj=1w(2)i j= 1 foreach row i.
Note that W2is no longer a symmetricmatrix.
The smoothness of the embedding spacecan be measured by the reconstruction error:R2=n?i=1???????ei?
?ej?N(ei)w(2)i jej??????
?22.Minimizing R2results in Smoothness Assump-tion 2: each entity can be linearly reconstructedfrom its nearest neighbors with low error.By incorporating R2as a regularization term in-to the margin-based ranking loss defined in Eq.
(1), we obtain our second SSE model, which per-forms the embedding task by minimizing:L2=1N?t+?O?t??Nt+?
(t+, t?)+?2n?i=1???????ei?
?ej?N(ei)w(2)i jej??????
?22.87The resultant embedding space is also semanti-cally smooth and compatible with the observedtriples.
Hyperparameter ?2makes a trade-off be-tween the two cases.Similar to the first model, stochastic gradien-t descent is used to solve the minimization prob-lem.
Given a positive triple t+= ?ei, rk, ej?
andthe associated negative triple t?= ?e?i, rk, e?j?, thegradient w.r.t.
es(s ?
{i, j, i?, j?})
is calculated as:?esL2= ?es?
(t+, t?
)+2?2E(I ?
W2)T(I ?
W2)1s,where I ?
Rn?nis the identity matrix.
Other pa-rameters are not included in R2, and their gradi-ents remain the same as defined in previous work.To better capture the cohesion within each cate-gory, during each stochastic step we resample thenearest neighbors for each entity, uniformly fromthe category to which it belongs.3.4 Advantages and ExtensionsThe advantages of our approach can be summa-rized as follows: 1) By incorporating geometri-cally based regularization terms, the SSE mod-els are able to capture the semantic correlationbetween entities, which exists intrinsically but isoverlooked in previous work.
2) By leveraging ad-ditional entity category information, the SSEmod-els can deal with the data sparsity issue that com-monly exists in most KGs.
Both aspects lead tomore accurate embeddings.Entity category information has also been inves-tigated in (Nickel et al, 2012; Chang et al, 2014;Wang et al, 2015), but in different manners.
Nick-el et al (2012) take categories as pseudo entitiesand introduce a specific relation to link entitiesto categories.
Chang et al (2014) and Wang etal.
(2015) use entity categories to specify relation-s?
argument expectations, removing invalid triplesduring training and reasoning respectively.
Noneof them considers the intrinsic geometric structureof the embedding space.Actually, our approach is quite general.
1) Thesmoothness assumptions can be imposed to a widevariety of KG embedding models, not only theones introduced in Section 2, but also those basedon matrix/tensor factorization (Nickel et al, 2011;Chang et al, 2013).
2) Besides semantic cate-gories, other information (e.g.
entity similaritiesspecified by users or derived from auxiliary datasources) can also be used to construct the mani-fold regularization terms.
3) Besides KG embed-ding, similar smoothness assumptions can also beLocation SportCityCapitalOfCountry AthleteLedSportTeamCityLocatedInCountry AthletePlaysForTeamCityLocatedInGeopoliticallocation AthletePlaysInLeagueCityLocatedInState AthletePlaysSportCountryLocatedInGeopoliticallocation CoachesInLeagueStateHasCapital CoachesTeamStateLocatedInCountry TeamPlaysInLeagueStateLocatedInGeopoliticallocation TeamPlaysSportTable 2: Relations in Location and Sport.applied in other embedding tasks (e.g.
word em-bedding and sentence embedding).4 ExperimentsWe empirically evaluate the proposed SSE modelsin two tasks: link prediction (Bordes et al, 2013)and triple classification (Socher et al, 2013).4.1 Data SetsWe create three data sets with different sizes usingNELL (Carlson et al, 2010): Location, Sport, andNell186.
Location and Sport are two small-scaledata sets, both containing 8 relations on the topicsof ?location?
and ?sport?
respectively.
The corre-sponding relations are listed in Table 2.
Nell186 isa larger data set containing the most frequent 186relations.
On all the data sets, entities appearingonly once are removed.
We extract the entity cat-egory information from a specific relation calledGeneralization, and keep non-overlapping cat-egories.2Categories containing less than 5 entitieson Location and Sport as well as categories con-taining less than 50 entities on Nell186 are fur-ther removed.
Table 3 gives some statistics of thethree data sets, where # Rel./# Ent./# Trip./# Cat.denotes the number of relations/entities/observedtriples/categories respectively, and # c-Ent.
de-notes the number of entities that have category la-bels.
Note that our SSE models do not require ev-ery entity to have a category label.
From the statis-tics, we can see that all the three data sets sufferfrom the data sparsity issue, containing a relative-ly small number of observed triples compared tothe number of entities.On the two small-scale data sets Location andSport, triples are split into training/validation/testsets, with the ratio of 3:1:1.
The first set is usedfor modeling training, the second for hyperparam-eter tuning, and the third for evaluation.
All ex-periments are repeated 5 times by drawing new2If two categories overlap, the smaller one is discarded.88# Rel.
# Ent.
# Trip.
# Cat.
# c-Ent.Location 8 380 718 5 358Sport 8 1,520 3,826 4 1,506Nell186 186 14,463 41,134 35 8,590Table 3: Statistics of data sets.training/validation/test splits, and results averagedover the 5 rounds are reported.
On Nell186 ex-periments are conducted only once, using a train-ing/validation/test split with 31,134/5,000/5,000triples respectively.
We will release the data up-on request.4.2 Link PredictionThis task is to complete a triple ?ei, rk, ej?
with eior ejmissing, i.e., predict eigiven (rk, ej) or pre-dict ejgiven (ei, rk).Baseline methods.
We take TransE, SME (lin),SME (bilin), and SE as our baselines.
We then in-corporate manifold regularization terms into thesemethods to obtain the SSE models.
A modelwith the LE/LLE regularization term is denotedas TransE-LE/TransE-LLE for example.
We fur-ther compare our SSE models with the setting pro-posed by Nickel et al (2012), which also takes in-to account the entity category information, but ina more direct manner.
That is, given an entity ewith its category label ce, we create a new triple?e, Generalization, ce?
and add it into the train-ing set.
Such a method is denoted as TransE-Catfor example.Evaluation protocol.
For evaluation, we adoptthe same ranking procedure proposed by Bordes etal.
(2013).
For each test triple ?ei, rk, ej?, the headentity eiis replaced by every entity e?iin the KG,and the energy is calculated for the corrupted triple?e?i, rk, ej?.
Ranking the energies in ascending or-der, we get the rank of the correct entity ei.
Sim-ilarly, we can get another rank by corrupting thetail entity ej.
Aggregated over all test triples, wereport three metrics: 1) the averaged rank, denotedas Mean (the smaller, the better); 2) the median ofthe ranks, denoted as Median (the smaller, the bet-ter); and 3) the proportion of ranks no larger than10, denoted as Hits@10 (the higher, the better).Implementation details.
We implement themethods based on the code provided by Bordes etal.
(2013)3.
For all the methods, we create 100mini-batches on each data set.
On Location andSport, the dimension of the embedding space d is3https://github.com/glorotxa/SMEset in the range of {10, 20, 50, 100}, the margin ?is set in the range of {1, 2, 5, 10}, and the learningrate is fixed to 0.1.
On Nell186, the hyperparame-ters d and ?
are fixed to 50 and 1 respectively, andthe learning rate is fixed to 10.
In LE and LLE,the regularization hyperparameters ?1and ?2aretuned in {10?4, 10?5, 10?6, 10?7, 10?8}.
And thenumber of nearest neighbors K in LLE is tuned in{5, 10, 15, 20}.
The best model is selected by ear-ly stopping on the validation sets (by monitoringMean), with a total of at most 1000 iterations overthe training sets.Results.
Table 4 reports the results on the testsets of Location, Sport, and Nell186.
From theresults, we can see that: 1) SSE (regularized vi-a either LE or LLE) outperforms all the baselineson all the data sets and with all the metrics.
Theimprovements are usually quite significant.
Themetric Mean drops by about 10% to 65%, Medi-an drops by about 5% to 75%, and Hits@10 risesby about 5% to 190%.
This observation demon-strates the superiority and generality of our ap-proach.
2) Even if encoded in a direct way (e.g.TransE-Cat), the entity category information canstill help the baseline methods in the link predic-tion task.
This observation indicates that leverag-ing additional information is indeed useful in deal-ing with the data sparsity issue and hence leads tobetter performance.
3) Compared to the strategywhich incorporates the entity category informationdirectly, formulating such information as manifoldregularization terms results in better and more sta-ble results.
The *-Cat models sometimes perfor-m even worse than the baselines (e.g.
TransE-Caton Sport data), while the SSE models consistent-ly achieve better results.
This observation furtherdemonstrates the superiority of constraining thegeometric structure of the embedding space.We further visualize and compare the geometricstructures of the embedding spaces learned by tra-ditional embedding and semantically smooth em-bedding.
We select the 10 largest semantic cate-gories in Nell186 (specified in Figure 1) and the5,740 entities therein.
We take the embeddingsof these entities learned by TransE, TransE-Cat,TransE-LE, and TransE-LLE, with the optimal hy-perparameter settings determined in the link pre-diction task.
Then we create 2D plots using t-SNE (Van der Maaten and Hinton, 2008)4.
Theresults are shown in Figure 1, where a different4http://lvdmaaten.github.io/tsne/89Location Sport Nell186Mean Median Hits@10 (%) Mean Median Hits@10 (%) Mean Median Hits@10 (%)TransE 30.94 10.70 50.56 362.66 62.90 43.86 924.37 94.00 16.95TransE-Cat 28.48 8.90 52.43 320.30 86.40 37.46 657.53 80.50 19.14TransE-LE 28.59 8.90 53.06 183.10 23.20 45.83 573.55 79.00 20.26TransE-LLE 28.03 9.20 52.36 231.67 52.40 43.18 535.32 95.00 20.02SME (lin) 63.01 24.10 40.90 266.50 87.10 32.34 427.86 26.00 35.97SME (lin)-Cat 41.12 18.30 42.43 263.88 70.80 35.03 309.60 25.00 36.22SME (lin)-LE 36.19 16.10 43.75 237.38 50.80 38.35 276.94 25.00 37.14SME (lin)-LLE 38.22 15.60 43.96 241.70 63.70 36.54 252.87 25.00 37.14SME (bilin) 47.66 20.90 37.85 314.49 124.00 33.83 848.39 28.00 35.71SME (bilin)-Cat 40.75 16.20 42.71 298.09 103.80 35.86 560.76 24.00 37.83SME (bilin)-LE 33.41 14.00 44.24 297.90 116.10 38.95 448.31 24.00 37.80SME (bilin)-LLE 32.84 13.60 46.25 286.63 110.10 35.67 452.43 28.00 36.51SE 108.15 69.90 14.72 426.70 242.60 24.72 904.84 44.00 27.81SE-Cat 88.36 48.20 20.76 435.44 231.00 35.39 529.38 40.00 28.68SE-LE 36.43 16.00 42.92 252.30 90.50 37.19 456.20 43.00 30.89SE-LLE 38.47 17.50 42.08 235.44 105.40 37.83 447.05 37.00 31.55Table 4: Link prediction results on the test sets of Location, Sport, and Nell186.Athlete Politicianus Chemical City Clothing Country Sportsteam Journalist Televisionstation Room(a) TransE.
(b) TransE-Cat.
(c) TransE-LE.
(d) TransE-LLE.Figure 1: Embeddings of entities belonging to the 10 largest categories in Nell186 (best viewed in color).color is used for each category.
It is easy to seethat imposing the semantic smoothness assump-tions helps in capturing the semantic correlationbetween entities in the embedding space.
Entitieswithin the same category lie closer to each oth-er, while entities belonging to different categoriesare easily distinguished (see Figure 1(c) and Fig-ure 1(d)).
Incorporating the entity category infor-mation directly could also helps.
But it fails onsome ?hard?
entities (i.e., those belonging to d-ifferent categories but mixed together in the cen-ter of Figure 1(b)).
We have conducted the sameexperiments with the other methods and observedsimilar phenomena.4.3 Triple ClassificationThis task is to verify whether a given triple?ei, rk, ej?
is correct or not.
We test our SSE mod-els in this task, with the same comparison settingsas used in the link prediction task.Evaluation protocol.
We follow the same eval-uation protocol used in (Socher et al, 2013; Wanget al, 2014b).
To create labeled data for classifica-tion, for each triple in the test and validation sets,we construct a negative triple for it by randomlycorrupting the entities.
To corrupt a position (heador tail), only entities that have appeared in that po-sition are allowed.
During triple classification, atriple is predicted as positive if the energy is be-low a relation-specific threshold ?r; otherwise asnegative.
We report two metrics on the test sets:micro-averaged accuracy and macro-averaged ac-curacy, denoted as Micro-ACC and Macro-ACCrespectively.
The former is a per-triple average,while the latter is a per-relation average.Implementation details.
We use the same hy-perparameter settings as in the link prediction task.The relation-specific threshold ?ris determined bymaximizing Micro-ACC on the validation sets.
A-gain, training is limited to at most 1000 iterations,and the best model is selected by early stopping onthe validation sets (by monitoring Micro-ACC).Results.
Table 5 reports the results on the testsets of Location, Sport, and Nell186.
The resultsindicate that: 1) SSE (regularized via either LE orLLE) performs consistently better than the base-90Location Sport Nell186Micro-ACC Macro-ACC Micro-ACC Macro-ACC Micro-ACC Macro-ACCTransE 86.11 81.66 72.52 73.78 84.21 77.86TransE-Cat 82.50 77.81 75.09 74.23 87.34 81.27TransE-LE 86.39 81.50 79.88 77.34 90.32 84.61TransE-LLE 87.01 83.03 80.29 77.71 90.08 84.50SME (lin) 75.90 71.82 72.61 71.24 88.54 84.17SME (lin)-Cat 83.33 80.90 73.52 72.28 91.00 86.20SME (lin)-LE 84.65 79.33 79.25 74.95 92.44 88.07SME (lin)-LLE 84.58 79.60 79.45 75.61 92.99 88.68SME (bilin) 73.06 67.26 71.33 67.78 88.78 84.79SME (bilin)-Cat 79.38 74.35 75.12 72.41 91.67 86.48SME (bilin)-LE 83.75 79.66 79.23 76.18 93.37 89.29SME (bilin)-LLE 83.54 80.36 79.33 75.35 93.64 89.39SE 65.14 60.01 68.61 63.71 90.18 83.93SE-Cat 68.61 62.82 67.62 62.17 92.87 87.72SE-LE 81.67 77.52 81.46 74.72 93.94 88.62SE-LLE 82.01 77.45 80.25 76.07 93.95 88.54Table 5: Triple classification results (%) on the test sets of Location, Sport, and Nell186.line methods on all the data sets in both metric-s.
The improvements are usually quite substantial.The metric Micro-ACC rises by about 1% to 25%,and Macro-ACC by about 2% to 30%.
2) Incorpo-rating the entity category information directly canalso improve the baselines in the triple classifica-tion task, again demonstrating the effectiveness ofleveraging additional information to deal with thedata sparsity issue.
3) It is a better choice to in-corporate the entity category information as man-ifold regularization terms as opposed to encodingit directly.
The *-Cat models sometimes perfor-m even worse than the baselines (e.g.
TransE-Cat on Location data and SE-Cat on Sport data),while the SSE models consistently achieve betterresults.
The observations are similar to those ob-served during the link prediction task, and furtherdemonstrate the superiority and generality of ourapproach.5 Related WorkThis section reviews two lines of related work: KGembedding and manifold learning.KG embedding aims to embed a KG composedof entities and relations into a low-dimensionalvector space, and model the plausibility of eachfact in that space.
Yang et al (2014) categorizedthe literature into three major groups: 1) method-s based on neural networks, 2) methods based onmatrix/tensor factorization, and 3) methods basedon Bayesian clustering.
The first group perform-s the embedding task using neural network archi-tectures (Bordes et al, 2013; Bordes et al, 2014;Socher et al, 2013).
Several state-of-the-art neuralnetwork-based embedding models have been in-troduced in Section 2.
For other work please referto (Jenatton et al, 2012; Wang et al, 2014b; Lin etal., 2015).
In the second group, KGs are represent-ed as tensors, and embedding is performed via ten-sor factorization or collective matrix factorizationtechniques (Singh and Gordon, 2008; Nickel et al,2011; Chang et al, 2014).
The third group embedsfactorized representations of entities and relationsinto a nonparametric Bayesian clustering frame-work, so as to obtain more interpretable embed-dings (Kemp et al, 2006; Sutskever et al, 2009).Our work falls into the first group, but differs inthat it further imposes constraints on the geomet-ric structure of the embedding space, which existsintrinsically but is overlooked in previous work.Although this paper focuses on incorporating ge-ometrically based regularization terms into neuralnetwork architectures, it can be easily extended tomatrix/tensor factorization techniques.Manifold learning is a geometrically motivat-ed framework for machine learning, enforcing thelearning model to be smooth w.r.t.
the geometricstructure of data (Belkin et al, 2006).
Within thisframework, various manifold learning algorithm-s have been proposed, such as ISOMAP (Tenen-baum et al, 2000), Laplacian Eigenmaps (Belkinand Niyogi, 2001), and Locally Linear Embed-ding (Roweis and Saul, 2000).
All these algo-rithms are based on the so-called local invarianceassumption, i.e., nearby points are likely to havesimilar embeddings or labels.
Manifold learninghas been widely applied in many different areas,from dimensionality reduction (Belkin and Niyo-91gi, 2001; Cai et al, 2008) and semi-supervisedlearning (Zhou et al, 2004; Zhu and Niyogi,2005) to recommender systems (Ma et al, 2011)and community question answering (Wang et al,2014a).
This paper employs manifold learning al-gorithms to model the semantic smoothness as-sumptions in KG embedding.6 Conclusion and Future WorkIn this paper, we have proposed a novel approachto KG embedding, referred to as Semantically S-mooth Embedding (SSE).
The key idea of SSE isto impose constraints on the geometric structure ofthe embedding space and enforce it to be semanti-cally smooth.
The semantic smoothness assump-tions are constructed by using entities?
categoryinformation, and then formulated as geometrical-ly based regularization terms to constrain the em-bedding task.
The embeddings learned in this wayare capable of capturing the semantic correlationbetween entities.
By leveraging additional infor-mation besides observed triples, SSE can also dealwith the data sparsity issue that commonly existsin most KGs.
We empirically evaluate SSE in twobenchmark tasks of link prediction and triple clas-sification.
Experimental results show that by in-corporating the semantic smoothness assumption-s, SSE significantly and consistently outperformsstate-of-the-art embedding methods, demonstrat-ing the superiority of our approach.
In addition,our approach is quite general.
The smoothness as-sumptions can actually be imposed to a wide vari-ety of embedding models, and it can also be con-structed using other information besides entities?semantic categories.As future work, we would like to: 1) Constructthe manifold regularization terms using other da-ta sources.
The only information required to con-struct the manifold regularization terms is the sim-ilarity between entities (used to define the adja-cency matrix in LE and to select nearest neigh-bors for each entity in LLE).
We would try entitysimilarities derived in different ways, e.g., spec-ified by users or calculated from entities?
textualdescriptions.
2) Enhance the efficiency and scala-bility of SSE.
Processing the manifold regulariza-tion terms can be time- and space-consuming (e-specially the one induced by the LE algorithm).We would investigate how to address this prob-lem, e.g., via the efficient iterative algorithms in-troduced in (Saul and Roweis, 2003) or via paral-lel/distributed computing.
3) Impose the seman-tic smoothness assumptions on other KG embed-ding methods (e.g.
those based on matrix/tensorfactorization or Bayesian clustering), and even onother embedding tasks (e.g.
word embedding orsentence embedding).AcknowledgmentsWe would like to thank the anonymous reviewersfor their valuable comments and suggestions.
Thiswork is supported by the National Natural ScienceFoundation of China (grant No.
61402465), the S-trategic Priority Research Program of the ChineseAcademy of Sciences (grant No.
XDA06030200),and the National Key Technology R&D Program(grant No.
2012BAH46B03).ReferencesEneko Agirre, Oier Lopez de Lacalle, and Aitor Soroa.2014.
Random walks for knowledge-based wordsense disambiguation.
Computational Linguistics,40(1):57?84.Mikhail Belkin and Partha Niyogi.
2001.
Laplacianeigenmaps and spectral techniques for embeddingand clustering.
In Advances in Neural InformationProcessing Systems, pages 585?591.Mikhail Belkin, Partha Niyogi, and Vikas Sindhwani.2006.
Manifold regularization: A geometric frame-work for learning from labeled and unlabeled ex-amples.
Journal of Machine Learning Research,7:2399?2434.Kurt Bollacker, Colin Evans, Praveen Paritosh, Tim S-turge, and Jamie Taylor.
2008.
Freebase: A collab-oratively created graph database for structuring hu-man knowledge.
In Proceedings of the 2008 ACMSIGMOD International Conference on Managementof Data, pages 1247?1250.Antoine Bordes, Jason Weston, Ronan Collobert, andYoshua Bengio.
2011.
Learning structured em-beddings of knowledge bases.
In Proceedings ofthe 25th AAAI Conference on Artificial Intelligence,pages 301?306.Antoine Bordes, Nicolas Usunier, Alberto Garcia-Dur?an, Jason Weston, and Oksana Yakhnenko.2013.
Translating embeddings for modeling multi-relational data.
In Advances in Neural InformationProcessing Systems, pages 2787?2795.Antoine Bordes, Xavier Glorot, Jason Weston, andYoshua Bengio.
2014.
A semantic matching en-ergy function for learning with multi-relational data.Machine Learning, 94(2):233?259.92Deng Cai, Xiaofei He, Xiaoyun Wu, and Jiawei Han.2008.
Non-negative matrix factorization on mani-fold.
In Proceedings of the 8th IEEE InternationalConference on Data Mining, pages 63?72.Andrew Carlson, Justin Betteridge, Bryan Kisiel, Bur-r Settles, Estevam R. Hruschka Jr, and Tom M.Mitchell.
2010.
Toward an architecture for never-ending language learning.
In Proceedings of the24th AAAI Conference on Artificial Intelligence,pages 1306?1313.Kai-Wei Chang, Wen-tau Yih, and Christopher Meek.2013.
Multi-relational latent semantic analysis.
InProceedings of the 2013 Conference on EmpiricalMethods in Natural Language Processing, pages1602?1612.Kai-Wei Chang, Wen-tau Yih, Bishan Yang, andChristopher Meek.
2014.
Typed tensor decom-position of knowledge bases for relation extraction.In Proceedings of the 2014 Conference on Empiri-cal Methods in Natural Language Processing, pages1568?1579.Raphael Hoffmann, Congle Zhang, Xiao Ling,Luke Zettlemoyer, and Daniel S. Weld.
2011.Knowledge-based weak supervision for informationextraction of overlapping relations.
In Proceedingsof the 49th Annual Meeting of the Association forComputational Linguistics: Human Language Tech-nologies, pages 541?550.Rodolphe Jenatton, Nicolas L. Roux, Antoine Bordes,and Guillaume R. Obozinski.
2012.
A latent fac-tor model for highly multi-relational data.
In Ad-vances in Neural Information Processing Systems,pages 3167?3175.Charles Kemp, Joshua B. Tenenbaum, Thomas L. Grif-fiths, Takeshi Yamada, and Naonori Ueda.
2006.Learning systems of concepts with an infinite rela-tional model.
In Proceedings of the 21st AAAI Con-ference on Artificial Intelligence, pages 381?388.Jens Lehmann, Robert Isele, Max Jakob, AnjaJentzsch, Dimitris Kontokostas, Pablo N. Mendes,Sebastian Hellmann, Mohamed Morsey, Patrick vanKleef, S?oren Auer, et al 2014.
Dbpedia: A large-scale, multilingual knowledge base extracted fromwikipedia.
Semantic Web Journal.Yankai Lin, Zhiyuan Liu, Maosong Sun, Yang Liu, andXuan Zhu.
2015.
Learning entity and relation em-beddings for knowledge graph completion.
In Pro-ceedings of the 29th AAAI Conference on ArtificialIntelligence, pages 2181?2187.Hao Ma, Dengyong Zhou, Chao Liu, Michael R. Lyu,and Irwin King.
2011.
Recommender systems withsocial regularization.
In Proceedings of the 4th ACMInternational Conference on Web Search and DataMining, pages 287?296.Bernardo Magnini, Matteo Negri, Roberto Prevete, andHristo Tanev.
2002.
A wordnet-based approachto named entities recognition.
In Proceedings ofthe 2002 Workshop on Building and Using Seman-tic Networks, pages 1?7.George A. Miller.
1995.
Wordnet: A lexicaldatabase for english.
Communications of the ACM,38(11):39?41.Maximilian Nickel, Volker Tresp, and Hans-PeterKriegel.
2011.
A three-way model for collectivelearning on multi-relational data.
In Proceedingsof the 28th International Conference on MachineLearning, pages 809?816.Maximilian Nickel, Volker Tresp, and Hans-PeterKriegel.
2012.
Factorizing yago: Scalable machinelearning for linked data.
In Proceedings of the 21stInternational Conference on World Wide Web, pages271?280.Sebastian Riedel, Limin Yao, Andrew McCallum, andBenjamin M. Marlin.
2013.
Relation extractionwith matrix factorization and universal schemas.
InProceedings of the 2013 Conference on North Amer-ican Chapter of the Association for ComputationalLinguistics: Human Language Technologies, pages74?84.Sam T. Roweis and Lawrence K. Saul.
2000.
Nonlin-ear dimensionality reduction by locally linear em-bedding.
Science, 290(5500):2323?2326.Lawrence K. Saul and Sam T. Roweis.
2003.
Thinkglobally, fit locally: Unsupervised learning of lowdimensional manifolds.
Journal of Machine Learn-ing Research, 4:119?155.Geoffrey J. Singh and Ajit P. Gordon.
2008.
Relationallearning via collective matrix factorization.
In Pro-ceedings of the 14th ACM SIGKDD InternationalConference on Knowledge Discovery and Data Min-ing, pages 650?658.Richard Socher, Danqi Chen, Christopher D. Manning,and Andrew Y. Ng.
2013.
Reasoning with neuraltensor networks for knowledge base completion.
InAdvances in Neural Information Processing System-s, pages 926?934.Ilya Sutskever, Joshua B. Tenenbaum, and Ruslan R.Salakhutdinov.
2009.
Modelling relational data us-ing bayesian clustered tensor factorization.
In Ad-vances in Neural Information Processing Systems,pages 1821?1828.Joshua B. Tenenbaum, Vin De Silva, and John C.Langford.
2000.
A global geometric frameworkfor nonlinear dimensionality reduction.
Science,290(5500):2319?2323.Laurens Van der Maaten and Geoffrey Hinton.
2008.Visualizing data using t-sne.
Journal of MachineLearning Research, 9(85):2579?2605.93Quan Wang, Jing Liu, Bin Wang, and Li Guo.
2014a.A regularized competition model for question diffi-culty estimation in community question answeringservices.
In Proceedings of the 2014 Conference onEmpirical Methods in Natural Language Process-ing, pages 1115?1126.Zhen Wang, Jianwen Zhang, Jianlin Feng, and ZhengChen.
2014b.
Knowledge graph embedding bytranslating on hyperplanes.
In Proceedings of the28th AAAI Conference on Artificial Intelligence,pages 1112?1119.Quan Wang, Bin Wang, and Li Guo.
2015.
Knowl-edge base completion using embeddings and rules.In Proceedings of the 24th International Joint Con-ference on Artificial Intelligence.Jason Weston, Antoine Bordes, Oksana Yakhnenko,and Nicolas Usunier.
2013.
Connecting languageand knowledge bases with embedding models for re-lation extraction.
In Proceedings of the 2013 Con-ference on Empirical Methods in Natural LanguageProcessing, pages 1366?1371.Bishan Yang, Wen-tau Yih, Xiaodong He, JianfengGao, and Li Deng.
2014.
Learning multi-relationalsemantics using neural-embedding models.
arXivpreprint arXiv:1411.4072.Dengyong Zhou, Olivier Bousquet, Thomas NavinLal, Jason Weston, and Bernhard Sch?olkopf.
2004.Learning with local and global consistency.
In Ad-vances in Neural Information Processing Systems,pages 321?328.Xiaojin Zhu and Partha Niyogi.
2005.
Harmonic mix-tures: combining mixture models and graph-basedmethods for inductive and scalable semi-supervisedlearning.
In Proceedings of the 22nd Internation-al Conference on Machine Learning, pages 1052?1059.94
