Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 817?825,Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLPIncorporating Information Status into Generation RankingAoife Cahill and Arndt RiesterInstitut fu?r Maschinelle Sprachverarbeitung (IMS)University of Stuttgart70174 Stuttgart, Germany{aoife.cahill,arndt.riester}@ims.uni-stuttgart.deAbstractWe investigate the influence of informa-tion status (IS) on constituent order in Ger-man, and integrate our findings into a log-linear surface realisation ranking model.We show that the distribution of pairs of IScategories is strongly asymmetric.
More-over, each category is correlated with mor-phosyntactic features, which can be au-tomatically detected.
We build a log-linear model that incorporates these asym-metries for ranking German string reali-sations from input LFG F-structures.
Weshow that it achieves a statistically signif-icantly higher BLEU score than the base-line system without these features.1 IntroductionThere are many factors that influence word order,e.g.
humanness, definiteness, linear order of gram-matical functions, givenness, focus, constituentweight.
In some cases, it can be relatively straight-forward to automatically detect these features (i.e.in the case of definiteness, this is a syntactic prop-erty).
The more complex the feature, the more dif-ficult it is to automatically detect.
It is commonknowledge that information status1 (henceforth,IS) has a strong influence on syntax and word or-der; for instance, in inversions, where the subjectfollows some preposed element, Birner (1994) re-ports that the preposed element must not be newerin the discourse than the subject.
We would liketo be able to use information related to IS in theautomatic generation of German text.
Ideally, wewould automatically annotate text with IS labelsand learn from this data.
Unfortunately, however,to date, there has been little success in automati-cally annotating text with IS.1We take information status to be a subarea of informationstructure; the one dealing with varieties of givenness but notwith contrast and focus in the strictest sense.We believe, however, that despite this shortcom-ing, we can still take advantage of some of the in-sights gained from looking at the influence of ISon word order.
Specifically, we look at the prob-lem from a more general perspective by comput-ing an asymmetry ratio for each pair of IS cate-gories.
Results show that there are a large num-ber of pairs exhibiting clear ordering preferenceswhen co-occurring in the same clause.
The ques-tion then becomes, without being able to auto-matically detect these IS category pairs, can we,nevertheless, take advantage of these strong asym-metric patterns in generation.
We investigate the(automatically detectable) morphosyntactic char-acteristics of each asymmetric IS pair and inte-grate these syntactic asymmetric properties intothe generation process.The paper is structured as follows: Section 2outlines the underlying realisation ranking systemfor our experiments.
Section 3 introduces infor-mation status and Section 4 describes how we ex-tract and measure asymmetries in information sta-tus.
In Section 5, we examine the syntactic charac-teristics of the IS asymmetries.
Section 6 outlinesrealisation ranking experiments to test the integra-tion of IS into the system.
We discuss our findingsin Section 7 and finally we conclude in Section 8.2 Generation RankingThe task we are considering is generation rank-ing.
In generation (or more specifically, surfacerealisation) ranking, we take an abstract represen-tation of a sentence (for example, as produced bya machine translation or automatic summarisationsystem), produce a number of alternative stringrealisations corresponding to that input and usesome model to choose the most likely string.
Wetake the model outlined in Cahill et al (2007), alog-linear model based on the Lexical FunctionalGrammar (LFG) Framework (Kaplan and Bres-nan, 1982).
LFG has two main levels of represen-817CS 1: ROOT:1458CProot[std]:1451DP[std]:906DPx[std]:903D[std]:593die:34NP:738N[comm]:693Beh?rden:85Cbar:1448Cbar-flat:1436V[v,fin]:976Vx[v,fin]:973warnten:117PP[std]:2081PPx[std]:2072P[pre]:1013vor:154DP[std]:1894DPx[std]:1956NP:1952AP[std,+infl]:1946APx[std,+infl]:1928A[+infl]:1039m?glichen:185N[comm]:1252Nachbeben:263PERIOD:397.:389"Die Beh?rden warnten vor m?glichen Nachbeben.
"'warnen<[34:Beh?rde], [263:Nachbeben]>'PRED'Beh?rde'PRED'die'PREDDETSPECCASE nom, NUM pl, PERS 334SUBJ'vor<[263:Nachbeben]>'PRED'Nachbeben'PRED'm?glich<[263:Nachbeben]>'PRED [263:Nachbeben]SUBJattributiveATYPE185ADJUNCTCASE dat, NUM pl, PERS 3263OBJ154OBLMOOD indicative, TENSE pastTNS-ASP[34:Beh?rde]TOPIC117Figure 1: An example C(onstituent) and F(unctional) Structure pair for (1)tation, C(onstituent)-Structure and F(unctional)-Structure.
C-Structure is a context-free tree rep-resentation that captures characteristics of the sur-face string while F-Structure is an abstract repre-sentation of the basic predicate-argument structureof the string.
An example C- and F-Structure pairfor the sentence in (1) is given in Figure 1.
(1) DietheBeho?rdenauthoritieswarntenwarnedvorofmo?glichenpossibleNachbeben.aftershocks?The authorities warned of possible aftershocks.
?The input to the generation system is an F-Structure.
A hand-crafted, bi-directional LFG ofGerman (Rohrer and Forst, 2006) is used to gener-ate all possible strings (licensed by the grammar)for this input.
As the grammar is hand-crafted,it is designed only to parse (and therefore) gen-erate grammatical strings.2 The task of the reali-sation ranking system is then to choose the mostlikely string.
Cahill et al (2007) describe a log-linear model that uses linguistically motivated fea-tures and improves over a simple tri-gram lan-guage model baseline.
We take this log-linearmodel as our starting point.32There are some rare instances of the grammar parsingand therefore also generating ungrammatical output.3Forst (2007) presents a model for parse disambiguationthat incorporates features such as humanness, definiteness,linear order of grammatical functions, constituent weight.Many of these features are already present in the Cahill etal.
(2007) model.An error analysis of the output of that systemrevealed that sometimes ?unnatural?
outputs werebeing selected as most probable, and that ofteninformation structural effects were the cause ofsubtle differences in possible alternatives.
Forinstance, Example (3) appeared in the originalTIGER corpus with the 2 preceding sentences (2).
(2) Denn ausdru?cklich ist darin der rechtliche Ma?stabder Vorinstanz, des Sa?chsischen Oberverwaltungs-gerichtes, besta?tigt worden.
Und der besagt: DieBeteiligung am politischen Strafrecht der DDR, derMangel an kritischer Auseinandersetzung mit to-talita?ren U?berzeugungen rechtfertigen den Ausschlussvon der Dritten Gewalt.
?Because, the legal benchmark has explicitly been con-firmed by the lower instance, the Saxonian Higher Ad-ministrative Court.
And it indicates: the participationin the political criminal law of the GDR as well asdeficits regarding the critical debate on totalitarian con-victions justify an expulsion from the judiciary.?
(3) Manonehathasausout ofdertheVergangenheitsaufarbeitungcoming to terms with the pastgelernt.learnt?People have learnt from dealing with the past mis-takes.
?The five alternatives output by the grammar are:a.
Man hat aus der Vergangenheitsaufarbeitung gelernt.b.
Aus der Vergangenheitsaufarbeitung hat man gelernt.c.
Aus der Vergangenheitsaufarbeitung gelernt hat man.d.
Gelernt hat man aus der Vergangenheitsaufarbeitung.e.
Gelernt hat aus der Vergangenheitsaufarbeitung man.818The string chosen as most likely by the system ofCahill et al (2007) is Alternative (b).
No mat-ter whether the context in (2) is available or thesentence is presented without any context, thereseems to be a preference by native speakers forthe original string (a).
Alternative (e) is extremelymarked4 to the point of being ungrammatical.
Al-ternative (c) is also very marked and so is Alterna-tive (d), although less so than (c) and (e).
Alter-native (b) is a little more marked than the originalstring, but it is easier to imagine a preceding con-text where this sentence would be perfectly appro-priate.
Such a context would be, e.g.
(4).
(4) Vergangenheitsaufarbeitung und Abwiegeln sind zweisehr unterschiedliche Arten, mit dem Geschehenenumzugehen.
?Dealing with the mistakes or playing them down aretwo very different ways to handle the past.
?If we limit ourselves to single sentences, thetask for the model is then to choose the string thatis closest to the ?default?
expected word order (i.e.appropriate in the most number of contexts).
Inthis work, we concentrate on integrating insightsfrom work on information status into the realisa-tion ranking process.3 Information StatusThe concept of information status (Prince, 1981;Prince, 1992) involves classifying NP/PP/DP ex-pressions in texts according to various ways oftheir being given or new.
It replaces and specifiesmore clearly the often vaguely used term given-ness.
The process of labelling a corpus for IS canbe seen as a means of discourse analysis.
Differentclassification systems have been proposed in theliterature; see Riester (2008a) for a comparison ofseveral IS labelling schemes and Riester (2008b)for a new proposal based on criteria from presup-position theory.
In the work described here, weuse the scheme of Riester (2008b).
His main theo-retic assumption is that IS categories (for definites)should group expressions according to the contex-tual resources in which their presuppositions findan antecedent.
For definites, the set of main cate-gory labels found in Table 1 is assumed.The idea of resolution contexts derives fromthe concept of a presupposition trigger (e.g.
adefinite description) as potentially establishing an4By marked, we mean that there are relatively few or spe-cialised contexts in which this sentence is acceptable.Context resource IS labeldiscourse D-GIVENcontextencyclopedic/ ACCESSIBLE-GENERALknowledgecontextenvironment/ SITUATIVEsituativecontextbridging BRIDGINGcontext (scenario)accommodation ACCESSIBLE-(no context) DESCRIPTIONTable 1: IS classification for definitesanaphoric relation (van der Sandt, 1992) to an en-tity being available by some means or other.
Butthere are some expressions whose referent cannotbe identified and needs to be accommodated, com-pare (5).
(5) [die monatelange Fu?hrungskrise der HamburgerSozialdemokraten]ACC-DESC?the leadership crisis lasting for months among theHamburg Social Democrats?Examples like this one have been mentionedearly on in the literature (e.g.
Hawkins (1978),Clark and Marshall (1981)).
Nevertheless, label-ing schemes so far have neglected this issue, whichis explicitly incorporated in the system of Riester(2008b).The status of an expression is ACCESSIBLE-GENERAL (or unused, following Prince (1981))if it is not present in the previous discourse butrefers to an entity that is known to the intendedrecipent.
There is a further differentiation of theACCESSIBLE-GENERAL class into generic (TYPE)and non-generic (TOKEN) items.An expression is D-GIVEN (or textually evoked)if and only if an antecedent is available in thediscourse context.
D-GIVEN entities are subdi-vided according to whether they are repetitions oftheir antecedent, short forms thereof, pronouns orwhether they use new linguistic material to add in-formation about an already existing discourse ref-erent (label: EPITHET).
Examples representing aco-reference chain are shown in (6).
(6) [Angela Merkel]ACC-GEN (first mention) .
.
.
[An-gela Merkel]D-GIV-REPEATED (second mention) .
.
.
[Merkel]D-GIV-SHORT .
.
.
[she]D-GIV-PRONOUN .
.
.
[herself]D-GIV-REFLEXIVE .
.
.
[the Hamburg-bornpolitician]D-GIV-EPITHETIndexicals (referring to entities in the environ-ment context) are labeled as SITUATIVE.
Definite819items that can be identified within a scenario con-text evoked by a non-coreferential item receive thelabel BRIDGING; compare Example (7).
(7) IninSri LankaSri LankahabenhavetamilischeTamilRebellenrebelserstmalsfor the first timeeinenanLuftangriffairstrike[gegenagainstdietheStreitkra?fte]BRIDGarmed forcesgeflogen.flown.
?In Sri Lanka, Tamil rebels have, for the first time, car-ried out an airstrike against the armed forces.
?In the indefinite domain, a simple classificationalong the lines of Table 2 is proposed.Type IS labelunrelated to context NEWpart-whole relation PARTITIVEto previous entityother (unspecified) INDEF-RELrelation to contextTable 2: IS classification for indefinitesThere are a few more subdivisions.
Table 3,for instance, contains the labels BRIDGING-CON-TAINED and PARTITIVE-CONTAINED, going backto Prince?s (1981:236) ?containing inferrables?.The entire IS label inventory used in this studycomprises 19 (sub)classes in total.4 Asymmetries in ISIn order to find out whether IS categories are un-evenly distributed within German sentences weexamine a corpus of German radio news bulletinsthat has been manually annotated for IS (496 an-notated sentences in total) using the scheme ofRiester (2008b).5For each pair of IS labels X and Y we counthow often they co-occur in the corpus within a sin-gle clause.
In doing so, we distinguish the num-bers for ?X preceding Y ?
(=A) and ?Y precedingX?
(= B).
The larger group is referred to as thedominant order.
Subsequently, we compute a ratioindicating the degree of asymmetry between thetwo orders.
If, for instance, the dominant patternoccurs 20 times (A) and the reverse pattern only 5times (B), the asymmetry ratio B/A is 0.25.65The corpus was labeled by two independent annotatorsand the results were compared by a third person who tookthe final decision in case of disagreement.
An evaluation asregards inter-coder agreement is currently underway.6Even if some of the sentences we are learning from aremarked in terms of word order, the ratios allow us to still learnthe predominant order, since the marked order should occurmuch less frequently and the ratio will remain low.Dominant order (: ?before?)
B/A TotalD-GIV-PROINDEF-REL 0 19D-GIV-PROD-GIV-CAT 0.1 11D-GIV-RELNEW 0.11 31D-GIV-PROSIT 0.13 17ACC-DESCINDEF-REL 0.14 24ACC-DESCACC-GEN-TY 0.19 19D-GIV-EPIINDEF-REL 0.2 12D-GIV-REPNEW 0.21 23D-GIV-PROACC-GEN-TY 0.22 11ACC-GEN-TOACC-GEN-TY 0.24 42D-GIV-PROACC-DESC 0.24 46EXPLNEW 0.25 30D-GIV-RELD-GIV-EPI 0.25 15BRIDG-CONTPART-CONT 0.25 15ACC-DESCEXPL 0.29 27D-GIV-PROD-GIV-REP 0.29 18D-GIV-PRONEW 0.29 88D-GIV-RELACC-DESC 0.3 26SITEXPL 0.31 17D-GIV-PROBRIDG-CONT 0.31 21D-GIV-PROD-GIV-SHORT 0.32 29. .
.
.
.
.ACC-DESCACC-GEN-TO 0.91 201SITBRIDG 0.92 23EXPLACC-DESC 1 12Table 3: Asymmetric pairs of IS labelsTable 3 gives the top asymmetry pairs down toa ratio of about 1:3 as well as, down at the bottom,the pairs that are most evenly distributed.
Thismeans that the top pairs exhibit strong orderingpreferences and are, hence, unevenly distributedin German sentences.
For instance, the orderingD-GIVEN-PRONOUN before INDEF-REL (top line),shown in Example (8), occurs 19 times in the ex-amined corpus while there is no example in thecorpus for the reverse order.7(8) [Sie]D-GIV-PROshewu?rdewouldauchalso[beiatverringerterreducedAnzahl]INDEF-RELnumberjedeeveryvernu?nftigesensibleVerteidigungsplanungdefence planningsprengen.blast?Even if the numbers were reduced it would blow everysensible defence planning out of proportion.
?5 Syntactic IS AsymmetriesIt seems that IS could, in principle, be quite bene-ficial in the generation ranking task.
The problem,of course, is that we do not possess any reliablesystem of automatically assigning IS labels to un-known text and manual annotations are costly andtime-consuming.
As a substitute, we identify a list7Note that we are not claiming that the reverse pattern isungrammatical or impossible, we just observe that it is ex-tremely infrequent.820of morphosyntactic characteristics that the expres-sions can adopt and investigate how these are cor-related to our inventory of IS categories.For some IS labels there is a direct link betweenthe typical phrases that fall into that IS category,and the syntactic features that describe it.
Onesuch example is D-GIVEN-PRONOUN, which al-ways corresponds to a pronoun, or EXPL whichalways corresponds to expletive items.
Such syn-tactic markers can easily be identified in the LFGF-structures.
On the other hand, there are manyIS labels for which there is no clear cut syntac-tic class that describes its typical phrases.
Ex-amples include NEW, ACCESSIBLE-GENERAL orACCESSIBLE-DESCRIPTION.In order to determine whether we can ascertaina set of syntactic features that are representativeof a particular IS label, we design an inventory ofsyntactic features that are found in all types of ISphrases.
The complete inventory is given in Table5.
It is a much easier task to identify these syntac-tic characteristics than to try and automatically de-tect IS labels directly, which would require a deepsemantic understanding of the text.
We automati-cally mark up the news corpus with these syntacticcharacteristics, giving us a corpus both annotatedfor IS and syntactic features.We can now identify, for each IS label, what themost frequent syntactic characteristics of that la-bel are.
Some examples and their frequencies aregiven in Table 4.Syntactic feature CountD-GIVEN-PRONOUNPERS PRON 39DA PRON 25DEMON PRON 19GENERIC PRON 11NEWSIMPLE INDEF 113INDEF ATTR 53INDEF NUM 32INDEF PPADJ 26INDEF GEN 25. .
.Table 4: Syntactic characteristics of IS labelsCombining the most frequent syntactic charac-teristics with the asymmetries presented in Table 3gives us Table 6.88For reasons of space, we are only showing the very topof the table.6 Generation Ranking ExperimentsUsing the augmented set of IS asymmetries,we design new features to be included into theoriginal model of Cahill et al (2007).
For eachIS asymmetry, we extract all precedence patternsof the corresponding syntactic features.
Forexample, from the first asymmetry in Table 6, weextract the following features:PERS PRON precedes INDEF ATTRPERS PRON precedes SIMPLE INDEFDA PRON precedes INDEF ATTRDA PRON precedes SIMPLE INDEFDEMON PRON precedes INDEF ATTRDEMON PRON precedes SIMPLE INDEFGENERIC PRON precedes INDEF ATTRGENERIC PRON precedes SIMPLE INDEFWe extract these patterns for all of the asym-metric pairs in Table 3 (augmented with syntac-tic characteristics) that have a ratio >0.4.
Thepatterns we extract need to be checked for incon-sistencies because not all of them are valid.
Byinconsistencies, we mean patterns of the type Xprecedes X, Y precedes Y, and any pat-tern where the variant X precedes Y as wellas Y precedes X is present.
These are all auto-matically removed from the list of features to givea total of 130 new features for the log-linear rank-ing model.We train the log-linear ranking model on 7759F-structures from the TIGER treebank.
We gen-erate strings from each F-structure and take theoriginal treebank string to be the labelled exam-ple.
All other examples are viewed as unlabelled.We tune the parameters of the log-linear model ona small development set of 63 sentences, and carryout the final evaluation on 261 unseen sentences.The ranking results of the model with the addi-tional IS-inspired features are given in Table 7.ExactModel BLEU Match(%)Cahill et al (2007) 0.7366 52.49New Model (Model 1) 0.7534 54.40Table 7: Ranking Results for new model with IS-inspired syntactic asymmetry features.We evaluate the string chosen by the log-linearmodel against the original treebank string in termsof exact match and BLEU score (Papineni et al,821Syntactic feature TypeDefinitesDefinite descriptionsSIMPLE DEF simple definite descriptionsPOSS DEF simple definite descriptions with a possessive determiner(pronoun or possibly genitive name)DEF ATTR ADJ definite descriptions with adjectival modifierDEF GENARG definite descriptions with a genitive argumentDEF PPADJ definite descriptions with a PP adjunctDEF RELARG definite descriptions including a relative clauseDEF APP definite descriptions including a title or job descriptionas well as a proper name (e.g.
an apposition)NamesPROPER combinations of position/title and proper name (without article)BARE PROPER bare proper namesDemonstrative descriptionsSIMPLE DEMON simple demonstrative descriptionsMOD DEMON adjectivally modified demonstrative descriptionsPronounsPERS PRON personal pronounsEXPL PRON expletive pronounREFL PRON reflexive pronounDEMON PRON demonstrative pronouns (not: determiners)GENERIC PRON generic pronoun (man ?
one)DA PRON ?da?-pronouns (darauf, daru?ber, dazu, .
.
.
)LOC ADV location-referring pronounsTEMP ADV,YEAR Dates and timesIndefinitesSIMPLE INDEF simple indefinitesNEG INDEF negative indefinitesINDEF ATTR indefinites with adjectival modifiersINDEF CONTRAST indefinites with contrastive modifiers(einige ?
some, andere ?
other, weitere ?
further, .
.
.
)INDEF PPADJ indefinites with PP adjunctsINDEF REL indefinites with relative clause adjunctINDEF GEN indefinites with genitive adjunctsINDEF NUM measure/number phrasesINDEF QUANT quantified indefinitesTable 5: An inventory of interesting syntactic characteristics in IS phrasesLabel 1 (+ features) Label 2 (+ features) B/A TotalD-GIVEN-PRONOUN INDEF-REL 0 19PERS PRON 39 INDEF ATTR 23DA PRON 25 SIMPLE INDEF 17DEMON PRON 19GENERIC PRON 11D-GIVEN-PRONOUN D-GIVEN-CATAPHOR 0.1 11PERS PRON 39 SIMPLE DEF 13DA PRON 25 DA PRON 10DEMON PRON 19GENERIC PRON 11D-GIVEN-REFLEXIVE NEW 0.11 31REFL PRON 54 SIMPLE INDEF 113INDEF ATTR 53INDEF NUM 32INDEF PPADJ 26INDEF GEN 25...Table 6: IS asymmetric pairs augmented with syntactic characteristics8222002).
We achieve an improvement of 0.0168BLEU points and 1.91 percentage points in exactmatch.
The improvement in BLEU is statisticallysignificant (p < 0.01) using the paired bootstrapresampling significance test (Koehn, 2004).Going back to Example (3), the new modelchooses a ?better?
string than the Cahill et al(2007) model.
The new model chooses the orig-inal string.
While the string chosen by the Cahillet al (2007) system is also a perfectly valid sen-tence, our empirical findings from the news corpuswere that the default order of generic pronoun be-fore definite NP were more frequent.
The systemwith the new features helped to choose the originalstring, as it had learnt this asymmetry.Was it just the syntax?The results in Table 7 clearly show that the newmodel is beneficial.
However, we want to knowhow much of the improvement gained is due tothe IS asymmetries, and how much the syntacticasymmetries on their own can contribute.
To thisend, we carry out a further experiment where wecalculate syntactic asymmetries based on the au-tomatic markup of the corpus, and ignore the ISlabels completely.
Again we remove any incon-sistent asymmetries and only choose asymmetrieswith a ratio of higher than 0.4.
The top asymme-tries are given in Table 8.Dominant order (: ?before?)
B/A TotalBAREPROPERINDEF NUM 0 33DA PRONINDEF NUM 0 16DEF PPADJTEMP ADV 0 15SIMPLE INDEFINDEF QUANT 0 14PERS PRONINDEF ATTR 0 12DEF PPADJEXPL PRON 0 12GENERIC PRONINDEF ATTR 0 12REFL PRONYEAR 0 11INDEF PPADJINDEF NUM 0.02 57DEF APPBAREPROPER 0.03 34BAREPROPERTEMP ADV 0.04 26TEMP ADVINDEF NUM 0.04 25PROPERINDEF GEN 0.05 20DEF GENARGINDEF ATTR 0.06 18. .
.
.
.
.Table 8: Purely syntactic asymmetriesFor each asymmetry, we create a new feature Xprecedes Y.
This results in a total of 66 fea-tures.
Of these 30 overlap with the features usedin the above experiment.
We do not include thefeatures extracted in the first attempt in this exper-iment.
The same training procedure is carried outand we test on the same heldout test set of 261 sen-tences.
The results are given in Table 9.
Finally,we combine the two lists of features and evaluate,these results are also presented in Table 9.ExactModel BLEU Match(%)Cahill et al (2007) 0.7366 52.49Model 1 0.7534 54.40Synt.-asym.-based Model 0.7419 54.02Combination 0.7437 53.64Table 9: Results for ranking model with purelysyntactic asymmetry featuresThey show that although the syntactic asymme-tries alone contribute to an improvement over thebaseline, the gain is not as large as when the syn-tactic asymmetries are constrained to correspondto IS label asymmetries (Model 1).9 Interest-ingly, the combination of the lists of features doesnot result in an improvement over Model 1.
Thedifference in BLEU score between the model ofCahill et al (2007) and the model that only takessyntactic-based asymmetries into account is notstatistically significant, while the difference be-tween Model 1 and this model is statistically sig-nificant (p < 0.05).7 DiscussionIn the work described here, we concentrate only ontaking advantage of the information that is read-ily available to us.
Ideally, we would like to beable to use the IS asymmetries directly as features,however, without any means of automatically an-notating new text with these categories, this is im-possible.
Our experiments were designed to test,whether we can achieve an improvement in thegeneration of German text, without a fully labelledcorpus, using the insight that at least some IS cate-gories correspond to morphosyntactic characteris-tics that can be easily identified.
We do not claimto go beyond this level to the point where true ISlabels would be used, rather we attempt to pro-vide a crude approximation of IS using only mor-phosyntactic information.
To be able to fully auto-matically annotate text with IS labels, one wouldneed to supplement the morphosyntactic features9The difference may also be due to the fewer features usedin the second experiment.
However, this emphasises, thatthe asymmetries gleaned from syntactic information alone arenot strong enough to be able to determine the prevailing orderof constituents.
When we take the IS labels into account, weare honing in on a particular subset of interesting syntacticasymmetries.823with information about anaphora resolution, worldknowledge, ontologies, and possibly even builddynamic discourse representations.We would also like to emphasise that we areonly looking at one sentence at a time.
Of course,there are other inter-sentential factors (not relyingon external resources) that play a role in choosingthe optimal string realisation, for example paral-lelism or the position of the sentence in the para-graph or text.
Given that we only looked at IS fac-tors within a sentence, we think that such a sig-nificant improvement in BLEU and exact matchscores is very encouraging.
In future work, we willlook at what information can be automatically ac-quired to help generation ranking based on morethan one sentence.While the experiments presented this paper arelimited to a German realisation ranking system,there is nothing in the methodology that precludesit from being applied to another language.
The ISannotation scheme is language-independent, andso all one needs to be able to apply this to anotherlanguage is a corpus annotated with IS categories.We extracted our IS asymmetry patterns from asmall corpus of spoken news items.
This corpuscontains text of a similar domain to the TIGERtreebank.
Further experiments are required to de-termine how domain specific the asymmetries are.Much related work on incorporating informa-tion status (or information structure) into languagegeneration has been on spoken text, since infor-mation structure is often encoded by means ofprosody.
In a limited domain setting, Prevost(1996) describes a two-tiered information struc-ture representation.
During the high level plan-ning stage of generation, using a small knowl-edge base, elements in the discourse are automat-ically marked as new or given.
Contrast and fo-cus are also assigned automatically.
These mark-ings influence the final string generated.
We arefocusing on a broad-coverage system, and do notuse any external world-knowledge resources.
VanDeemter and Odijk (1997) annotate the syntac-tic component from which they are generatingwith information about givenness.
This informa-tion is determined by detecting contradictions andparallel sentences.
Pulman (1997) also uses in-formation about parallelism to predict word or-der.
In contrast, we only look at one sentencewhen we approximate information status, futurework will look at cross sentential factors.
Endrissand Klabunde (2000) describe a sentence plannerfor German that annotates the propositional in-put with discourse-related features in order to de-termine the focus, and thus influence word orderand accentuation.
Their system, again, is domain-specific (generating monologue describing a filmplot) and requires the existence of a knowledgebase.
The same holds for Yampolska (2007), whopresents suggestions for generating informationstructure in Russian and Ukrainian football re-ports, using rules to determine parallel structuresfor the placement of contrastive accent, followingsimilar work by Theune (1997).
While our paperdoes not address the generation of speech / accen-tuation, it is of course conceivable to employ theIS annotated radio news corpus from which we de-rived the label asymmetries (and which also existsin a spoken and prosodically annotated version) ina similar task of learning the correlations betweenIS labels and pitch accents.
Finally, Bresnan etal.
(2007) present work on predicting the dativealternation in English using 14 features relating toinformation status which were manually annotatedin their corpus.
In our work, we manually annotatea small corpus in order to learn generalisations.From these we learn features that approximate thegeneralisations, enabling us to apply them to largeamounts of unseen data without further manual an-notation.8 ConclusionsIn this paper we presented a novel method of in-cluding IS into the task of generation ranking.Since automatic annotation of IS labels them-selves is not currently possible, we approximatethe IS categories by their syntactic characteristics.By calculating strong asymmetries between pairsof IS labels, and establishing the most frequentsyntactic characteristics of these asymmetries, wedesigned a new set of features for a log-linearranking model.
In comparison to a baseline model,we achieve statistically significant improvement inBLEU score.
We showed that these improvementswere not only due to the effect of purely syntac-tic asymmetries, but that the IS asymmetries werewhat drove the improved model.AcknowledgmentsThis work was funded by the Collaborative Re-search Centre (SFB 732) at the University ofStuttgart.824ReferencesBetty J. Birner.
1994.
Information Status and WordOrder: an Analysis of English Inversion.
Language,70(2):233?259.Joan Bresnan, Anna Cueni, Tatiana Nikitina, andR.
Harald Baayen.
2007.
Predicting the Dative Al-ternation.
Cognitive Foundations of Interpretation,pages 69?94.Aoife Cahill, Martin Forst, and Christian Rohrer.
2007.Stochastic Realisation Ranking for a Free Word Or-der Language.
In Proceedings of the Eleventh Eu-ropean Workshop on Natural Language Generation,pages 17?24, Saarbru?cken, Germany.
DFKI GmbH.Herbert H. Clark and Catherine R. Marshall.
1981.Definite Reference and Mutual Knowledge.
In Ar-avind Joshi, Bonnie Webber, and Ivan Sag, editors,Elements of Discourse Understanding, pages 10?63.Cambridge University Press.Kees van Deemter and Jan Odijk.
1997.
ContextModeling and the Generation of Spoken Discourse.Speech Communication, 21(1-2):101?121.Cornelia Endriss and Ralf Klabunde.
2000.
PlanningWord-Order Dependent Focus Assignments.
In Pro-ceedings of the First International Conference onNatural Language Generation (INLG), pages 156?162, Morristown, NJ.
Association for Computa-tional Linguistics.Martin Forst.
2007.
Disambiguation for a Linguis-tically Precise German Parser.
Ph.D. thesis, Uni-versity of Stuttgart.
Arbeitspapiere des Institutsfu?r Maschinelle Sprachverarbeitung (AIMS), Vol.13(3).John A. Hawkins.
1978.
Definiteness and Indefinite-ness: A Study in Reference and Grammaticality Pre-diction.
Croom Helm, London.Ron Kaplan and Joan Bresnan.
1982.
Lexical Func-tional Grammar, a Formal System for GrammaticalRepresentation.
In Joan Bresnan, editor, The Men-tal Representation of Grammatical Relations, pages173?281.
MIT Press, Cambridge, MA.Philipp Koehn.
2004.
Statistical Significance Tests forMachine Translation Evaluation.
In Dekang Lin andDekai Wu, editors, Proceedings of the Conferenceon Empirical Methods in Natural Language Pro-cessing (EMNLP 2004), pages 388?395, Barcelona.Association for Computational Linguistics.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
BLEU: a Method for AutomaticEvaluation of Machine Translation.
In Proceedingsof the 40th Annual Meeting of the Association forComputational Linguistics (ACL 2002), pages 311?318, Philadelphia, PA.Scott Prevost.
1996.
An Information Structural Ap-proach to Spoken Language Generation.
In Pro-ceedings of the 34th Annual Meeting of the Asso-ciation for Computational Linguistics (ACL 1996),pages 294?301, Morristown, NJ.Ellen F. Prince.
1981.
Toward a Taxonomy of Given-New Information.
In P. Cole, editor, Radical Prag-matics, pages 233?255.
Academic Press, New York.Ellen F. Prince.
1992.
The ZPG Letter: Subjects, Def-initeness and Information Status.
In W. C. Mannand S. A. Thompson, editors, Discourse Descrip-tion: Diverse Linguistic Analyses of a Fund-RaisingText, pages 295?325.
Benjamins, Amsterdam.Stephen G. Pulman.
1997.
Higher Order Unificationand the Interpretation of Focus.
Linguistics and Phi-losophy, 20:73?115.Arndt Riester.
2008a.
A Semantic Explication of ?In-formation Status?
and the Underspecification of theRecipients?
Knowledge.
In Atle Gr?nn, editor, Pro-ceedings of Sinn und Bedeutung 12, University ofOslo.Arndt Riester.
2008b.
The Components of Focusand their Use in Annotating Information Struc-ture.
Ph.D. thesis, University of Stuttgart.
Ar-beitspapiere des Instituts fu?r Maschinelle Sprachver-arbeitung (AIMS), Vol.
14(2).Christian Rohrer and Martin Forst.
2006.
ImprovingCoverage and Parsing Quality of a Large-Scale LFGfor German.
In Proceedings of the Language Re-sources and Evaluation Conference (LREC 2006),Genoa, Italy.Rob van der Sandt.
1992.
Presupposition Projection asAnaphora Resolution.
Journal of Semantics, 9:333?377.Marie?t Theune.
1997.
Goalgetter: Predicting Con-trastive Accent in Data-to-Speech Generation.
InProceedings of the 35th Annual Meeting of the Asso-ciation for Computational Linguistics (ACL/EACL1997), pages 519?521, Madrid.
Student paper.Nadiya Yampolska.
2007.
Information Structure inNatural Language Generation: an Account for East-Slavic Languages.
Term paper.
Universita?t des Saar-landes.825
