Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 138?142,Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational LinguisticsThe Karlsruhe Institute for Technology Translation System for theACL-WMT 2010Jan Niehues, Teresa Herrmann, Mohammed Mediani and Alex WaibelKarlsruhe Instiute of TechnolgyKarlsruhe, Germanyfirstname.lastname@kit.eduAbstractThis paper describes our phrase-based Sta-tistical Machine Translation (SMT) sys-tem for the WMT10 Translation Task.
Wesubmitted translations for the German toEnglish and English to German transla-tion tasks.
Compared to state-of-the-artphrase-based systems we preformed addi-tional preprocessing and used a discrim-inative word alignment approach.
Theword reordering was modeled using POSinformation and we extended the transla-tion model with additional features.1 IntroductionIn this paper we describe the systems that webuilt for our participation in the Shared Trans-lation Task of the ACL 2010 Joint Fifth Work-shop on Statistical Machine Translation and Met-ricsMATR.
Our translations are generated usinga state-of-the-art phrase-based translation systemand applying different extensions and modifica-tions including Discriminative Word Alignment,a POS-based reordering model and bilingual lan-guage models using POS and stem information.Depending on the source and target languages,the proposed models differ in their benefit for thetranslation task and also expose different correl-ative effects.
The Sections 2 to 4 introduce thecharacteristics of the baseline system and the sup-plementary models.
In Section 5 we present theperformance of the system variants applying thedifferent models and chose the systems used forcreating the submissions for the English-Germanand German-English translation task.
Section 6draws conclusions and suggests directions for fu-ture work.2 Baseline SystemThe baseline systems for the translation directionsGerman-English and English-German are both de-veloped using Discriminative Word Alignment(Niehues and Vogel, 2008) and the Moses Toolkit(Koehn et al, 2007) for extracting phrase pairsand generating the phrase table from the discrimi-native word alignments.
The difficult reorderingbetween German and English was modeled us-ing POS-based reordering rules.
These rules werelearned using a word-aligned parallel corpus.
ThePOS tags for the reordering models are generatedusing the TreeTagger (Schmid, 1994) for all lan-guages.Translation is performed by the STTK Decoder(Vogel, 2003) and all systems are optimized to-wards BLEU using Minimum Error Rate Trainingas proposed in Venugopal et al (2005).2.1 Training, Development and Test DataWe used the data provided for the WMT for train-ing, optimizing and testing our systems: Ourtraining corpus consists of Europarl and NewsCommentary data, for optimization we use new-stest2008 as development set and newstest2009 astest set.The baseline language models are trained onthe target language part of the Europarl and NewsCommentary corpora.
Additional, bigger lan-guage models were trained on monolingual cor-pora.
For both systems the News corpus was usedwhile an English language model was also trainedon the even bigger Gigaword corpus.2.2 PreprocessingThe training data was preprocessed before used fortraining.
In this step different normalizations weredone like mapping different types of quotes.
Inthe end the first word of every sentence was smart-cased.138For the German text, additional preprocessingsteps were applied.
First, the older German datauses the old German orthography whereas thenewer parts of the corpus use the new Germanorthography.
We tried to normalize the text byconverting the whole text to the new German or-thography.
In a first step, we search for words thatare only correct according to the old writing rules.Therefore, we selected all words in the corpus, thatare correct according to the hunspell lexicon1 us-ing the old rules, but not correct according to thehunspell lexicon using the new rules.
In a secondstep we tried to find the correct spelling accordingto the new rules.
We first applied rules describinghow words changed from one spelling system tothe other, for example replacing ???
by ?ss?.
If thenew word is a correct word according to the hun-spell lexicon using the new spelling rules, we mapthe words.When translating from German to English, weapply compound splitting as described in Koehnand Knight (2003) to the German corpus.As a last preprocessing step we remove sen-tences that are too long and empty lines to obtainthe final training corpus.3 Word Reordering ModelReordering was applied on the source side priorto decoding through the generation of lattices en-coding possible reorderings of each source sen-tence that better match the word sequence in thetarget language.
These possible reorderings werelearned based on the POS of the source languagewords in the training corpus and the informationabout alignments between source and target lan-guage words in the corpus.
For short-range re-orderings, continuous reordering rules were ap-plied to the test sentences (Rottmann and Vogel,2007).
To model the long-range reorderings be-tween German and English, different types of non-continuous reordering rules were applied depend-ing on the translation direction.
(Niehues andKolss, 2009).
When translating from English toGerman, most of the changes in word order con-sist in a shift to the right while typical word shiftsin German to English translations take place in thereverse direction.1http://hunspell.sourceforge.net/4 Translation ModelThe translation model was trained on the parallelcorpus and the word alignment was generated bya discriminative word alignment model, which isdescribed below.
The phrase table was trained us-ing the Moses training scripts, but for the Germanto English system we used a different phrase ex-traction method described in detail in Section 4.2.In addition, we applied phrase table smoothing asdescribed in Foster et al (2006).
Furthermore, weextended the translation model by additional fea-tures for unaligned words and introduced bilinguallanguage models.4.1 Word AlignmentIn most phrase-based SMT systems the heuristicgrow-diag-final-and is used to combine the align-ments generated by GIZA++ from both direc-tions.
Then these alignments are used to extractthe phrase pairs.We used a discriminative word alignment model(DWA) to generate the alignments as described inNiehues and Vogel (2008) instead.
This model istrained on a small amount of hand-aligned dataand uses the lexical probability as well as the fer-tilities generated by the PGIZA++2 Toolkit andPOS information.
We used all local features, theGIZA and indicator fertility features as well asfirst order features for 6 directions.
The model wastrained in three steps, first using maximum likeli-hood optimization and afterwards it was optimizedtowards the alignment error rate.
For more detailssee Niehues and Vogel (2008).4.2 Lattice Phrase ExtractionIn translations from German to English, we oftenhave the case that the English verb is aligned toboth parts of the German verb.
Since this phrasepair is not continuous on the German side, it can-not be extracted.
The phrase could be extracted, ifwe also reorder the training corpus.For the test sentences the POS-based reorderingallows us to change the word order in the sourcesentence so that the sentence can be translatedmore easily.
If we apply this also to the train-ing sentences, we would be able to extract thephrase pairs for originally discontinuous phrasesand could apply them during translation of the re-ordered test sentences.2http://www.cs.cmu.edu/?qing/139Therefore, we build lattices that encode the dif-ferent reorderings for every training sentence, asdescribed in Niehues et al (2009).
Then we cannot only extract phrase pairs from the monotonesource path, but also from the reordered paths.
Soit would be possible to extract the example men-tioned before, if both parts of the verb were puttogether by a reordering rule.
To limit the num-ber of extracted phrase pairs, we extract a sourcephrase only once per sentence even if it may befound on different paths.
Furthermore, we do notuse the weights in the lattice.If we used the same rules as for reordering thetest sets, the lattice would be so big that the num-ber of extracted phrase pairs would be still toohigh.
As mentioned before, the word reorderingis mainly a problem at the phrase extraction stageif one word is aligned to two words which arefar away from each other in the sentence.
There-fore, the short-range reordering rules do not helpmuch in this case.
So, only the long-range reorder-ing rules were used to generate the lattices for thetraining corpus.4.3 Unaligned Word FeatureGuzman et al (2009) analyzed the role of the wordalignment in the phrase extraction process.
To bet-ter model the relation between word alignment andthe phrase extraction process, they introduced twonew features into the log-linear model.
One fea-ture counts the number of unaligned words on thesource side and the other one does the same for thetarget side.
Using these additional features theyshowed improvements on the Chinese to Englishtranslation task.
In order to investigate the impacton closer related languages like English and Ger-man, we incorporated those two features into oursystems.4.4 Bilingual Word language modelMotivated by the improvements in translationquality that could be achieved by using the n-grambased approach to statistical machine translation,for example by Allauzen et al (2009), we triedto integrate a bilingual language model into ourphrase-based translation system.To be able to integrate the approach easily into astandard phrase-based SMT system, a token in thebilingual language model is defined to consist ofa target word and all source words it is aligned to.The tokens are ordered according to the target lan-guage word order.
Then the additional tokens canbe introduced into the decoder as an additional tar-get factor.
Consequently, no additional implemen-tation work is needed to integrate this feature.If we have the German sentence Ich bin nachHause gegangen with the English translation Iwent home, the resulting bilingual text would looklike this: I Ich went bin gegangen home Hause.As shown in the example, one problem with thisapproach is that unaligned source words are ig-nored in the model.
One solution could be to havea second bilingual text ordered according to thesource side.
But since the target sentence and notthe source sentence is generated from left to rightduring decoding, the integration of a source sidelanguage model is more complex.
Therefore, asa first approach we only used a language modelbased on the target word order.4.5 Bilingual POS language modelThe main advantage of POS-based informationis that there are less data sparsity problems andtherefore a longer context can be considered.
Con-sequently, if we want to use this information in thetranslation model of a phrase-based SMT system,the POS-based phrase pairs should be longer thanthe word-based ones.
But this is not possible inmany decoders or it leads to additional computa-tion overhead.If we instead use a bilingual POS-based lan-guage model, the context length of the languagemodel is independent from the other models.
Con-sequently, a longer context can be considered forthe POS-based language model than for the word-based bilingual language model or the phrasepairs.Instead of using POS-based information, thisapproach can also be applied with other additionallinguistic word-level information like word stems.5 ResultsWe submitted translations for English-Germanand German-English for the Shared TranslationTask.
In the following we present the experimentswe conducted for both translation directions ap-plying the aforementioned models and extensionsto the baseline systems.
The performance of eachindividual system configuration was measured ap-plying the BLEU metric.
All BLEU scores are cal-culated on the lower-cased translation hypotheses.The individual systems that were used to create thesubmission are indicated in bold.1405.1 English-GermanThe baseline system for English-German appliesshort-range reordering rules and discriminativeword alignment.
The language model is trainedon the News corpus.
By expanding the coverageof the rules to enable long-range reordering, thescore on the test set could be slightly improved.We then combined the target language part of theEuroparl and News Commentary corpora with theNews corpus to build a bigger language modelwhich resulted in an increase of 0.11 BLEU pointson the development set and an increase of 0.25points on the test set.
Applying the bilingual lan-guage model as described above led to 0.04 pointsimprovement on the test set.Table 1: Translation results for English-German(BLEU Score)System Dev TestBaseline 15.30 15.40+ Long-range Reordering 15.25 15.44+ EPNC LM 15.36 15.69+ bilingual Word LM 15.37 15.73+ bilingual POS LM 15.42 15.67+ unaligned Word Feature 15.65 15.66+ bilingual Stem LM 15.57 15.74This system was used to create the submis-sion to the Shared Translation Task of the WMT2010.
After submission we performed additionalexperiments which only led to inconclusive re-sults.
Adding the bilingual POS language modeland introducing the unaligned word feature to thephrase table only improved on the developmentset, while the scores on the test set decreased.
Athird bilingual language model based on stem in-formation again only showed noteworthy effectson the development set.5.2 German-EnglishFor the German to English translation system,the baseline system already uses short-range re-ordering rules and the discriminative word align-ment.
This system applies only the languagemodel trained on the News corpus.
By adding thepossibility to model long-range reorderings withPOS-based rules, we could improve the system by0.6 BLEU points.
Adding the big language modelusing also the English Gigaword corpus we couldimprove by 0.3 BLEU points.
We got an addi-tional improvement by 0.1 BLEU points by addinglattice phrase extraction.Both the word-based and POS-based bilinguallanguage model could improve the translationquality measured in BLEU.
Together they im-proved the system performance by 0.2 BLEUpoints.The best results could be achieved by using alsothe unaligned word feature for source and targetwords leading to the best performance on the testset (22.09).Table 2: Translation results for German-English(BLEU Score)System Dev TestBaseline 20.94 20.83+ Long-range Reordering 21.52 21.43+ Gigaword LM 21.90 21.71+ Lattice Phrase Extraction 21.94 21.81+ bilingual Word LM 21.94 21.87+ bilingual POS LM 22.02 22.05+ unaligned Word Feature 22.09 22.096 ConclusionsFor our participation in the WMT 2010 we builttranslation systems for German to English and En-glish to German.
We addressed to the difficultword reordering when translating from or to Ger-man by using POS-based reordering rules duringdecoding and by using lattice-based phrase extrac-tion during training.
By applying those methodswe achieved substantially better results for bothtranslation directions.Furthermore, we tried to improve the translationquality by introducing additional features to thetranslation model.
On the one hand we includedbilingual language models based on different wordfactors into the log-linear model.
This led to veryslight improvements which differed also with re-spect to language and data set.
We will investigatein the future whether further improvements areachievable with this approach.
On the other handwe included the unaligned word feature which hasbeen applied successfully for other language pairs.The improvements we could gain with this methodare not as big as the ones reported for other lan-guages, but still the performance of our systemscould be improved using this feature.141AcknowledgmentsThis work was realized as part of the Quaero Pro-gramme, funded by OSEO, French State agencyfor innovation.ReferencesAlexandre Allauzen, Josep Crego, Aure?lien Max, andFranc?ois Yvon.
2009.
LIMSI?s statistical trans-lation system for WMT?09.
In Fourth Workshopon Statistical Machine Translation (WMT 2009),Athens, Greece.George Foster, Roland Kuhn, and Howard Johnson.2006.
Phrasetable Smoothing for Statistical Ma-chine Translation.
In Conference on EmpiricalMethods in Natural Language Processing (EMNLP2006), Sydney, Australia.Francisco Guzman, Qin Gao, and Stephan Vogel.2009.
Reassessment of the Role of Phrase Extrac-tion in PBSMT.
In MT Summit XII, Ottawa, Ontario,Canada.Philipp Koehn and Kevin Knight.
2003.
EmpiricalMethods for Compound Splitting.
In EACL, Bu-dapest, Hungary.Philipp Koehn, Hieu Hoang, Alexandra Birch, ChrisCallison-Burch, Marcello Federico, Nicola Bertoldi,Brooke Cowan, Wade Shen, Christine Moran,Richard Zens, Chris Dyer, Ondrej Bojar, Alexan-dra Constantin, and Evan Herbst.
2007.
Moses:Open Source Toolkit for Statistical Machine Trans-lation.
In ACL 2007, Demonstration Session,Prague, Czech Republic, June 23.Jan Niehues and Muntsin Kolss.
2009.
A POS-BasedModel for Long-Range Reorderings in SMT.
InFourth Workshop on Statistical Machine Translation(WMT 2009), Athens, Greece.Jan Niehues and Stephan Vogel.
2008.
DiscriminativeWord Alignment via Alignment Matrix Modeling.In Proc.
of Third ACL Workshop on Statistical Ma-chine Translation, Columbus, USA.Jan Niehues, Teresa Herrmann, Muntsin Kolss, andAlex Waibel.
2009.
The Universita?t KarlsruheTranslation System for the EACL-WMT 2009.
InFourth Workshop on Statistical Machine Translation(WMT 2009), Athens, Greece.Kay Rottmann and Stephan Vogel.
2007.
Word Re-ordering in Statistical Machine Translation with aPOS-Based Distortion Model.
In TMI, Sko?vde,Sweden.Helmut Schmid.
1994.
Probabilistic Part-of-SpeechTagging Using Decision Trees.
In InternationalConference on New Methods in Language Process-ing, Manchester, UK.Ashish Venugopal, Andreas Zollman, and Alex Waibel.2005.
Training and Evaluation Error MinimizationRules for Statistical Machine Translation.
In Work-shop on Data-drive Machine Translation and Be-yond (WPT-05), Ann Arbor, MI.Stephan Vogel.
2003.
SMT Decoder Dissected: WordReordering.
In Int.
Conf.
on Natural LanguageProcessing and Knowledge Engineering, Beijing,China.142
