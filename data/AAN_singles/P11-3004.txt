Proceedings of the ACL-HLT 2011 Student Session, pages 18?23,Portland, OR, USA 19-24 June 2011. c?2011 Association for Computational LinguisticsExploring Entity Relations for Named Entity DisambiguationDanuta PlochDAI-Labor, Technische Universita?t BerlinBerlin, Germanydanuta.ploch@dai-labor.deAbstractNamed entity disambiguation is the task oflinking an entity mention in a text to the cor-rect real-world referent predefined in a knowl-edge base, and is a crucial subtask in manyareas like information retrieval or topic detec-tion and tracking.
Named entity disambigua-tion is challenging because entity mentionscan be ambiguous and an entity can be refer-enced by different surface forms.
We presentan approach that exploits Wikipedia relationsbetween entities co-occurring with the am-biguous form to derive a range of novel fea-tures for classifying candidate referents.
Wefind that our features improve disambiguationresults significantly over a strong popularitybaseline, and are especially suitable for recog-nizing entities not contained in the knowledgebase.
Our system achieves state-of-the-art re-sults on the TAC-KBP 2009 dataset.1 IntroductionIdentifying the correct real-world referents of namedentities (NE) mentioned in text (such as people, or-ganizations, and geographic locations) plays an im-portant role in various natural language processingand information retrieval tasks.
The goal of NamedEntity Disambiguation (NED) is to label a surfaceform denoting an NE in text with one of multiplepredefined NEs from a knowledge base (KB), orto detect that the surface form refers to an out-of-KB entity, which is known as NIL detection.
NEDhas become a popular research field recently, asthe growth of large-scale publicly available encyclo-pedic knowledge resources such as Wikipedia hasstimulated research on linking NEs in text to theirentries in these KBs (Bunescu and Pasca, 2006; Mc-Namee and Dang, 2009).The disambiguation of named entities raises sev-eral challenges: Surface forms in text can be am-biguous, and the same entity can be referred to bydifferent surface forms.
For example, the surfaceform ?George Bush?
may denote either of two for-mer U.S. presidents, and the later president can bereferred to by ?George W. Bush?
or with his nick-name ?Dubya?.
Thus, a many-to-many mapping be-tween surface forms and entities has to be resolved.In addition, entity mentions may not have a match-ing entity in the KB, which is often the case for non-popular entities.Typical approaches to NED combine the use ofdocument context knowledge with entity informa-tion stored in the KB in order to disambiguate en-tities.
Many systems represent document contextand KB information as word or concept vectors,and rank entities using vector space similarity met-rics (Cucerzan, 2007).
Other authors employ su-pervised machine learning algorithms to classify orrank candidate entities (Bunescu and Pasca, 2006;Zhang et al, 2010).
Common features include pop-ularity metrics based on Wikipedia?s graph structureor on name mention frequency (Dredze et al, 2010;Han and Zhao, 2009), similarity metrics explor-ing Wikipedia?s concept relations (Han and Zhao,2009), and string similarity features.
Recent workalso addresses the task of NIL detection (Dredze etal., 2010).While previous research has largely focused ondisambiguating each entity mention in a document18separately (McNamee and Dang, 2009), we explorean approach that is driven by the observation thatentities normally co-occur in texts.
Documents of-ten discuss several different entities related to eachother, e.g.
a news article may report on a meetingof political leaders from different countries.
Analo-gously, entries in a KB such as Wikipedia are linkedto other, related entries.Our Contributions In this paper, we evaluate arange of novel disambiguation features that exploitthe relations between NEs identified in a documentand in the KB.
Our goal is to explore the usefulnessof Wikipedia?s link structure as source of relationsbetween entities.
We propose a method for candi-date selection that is based on an inverted index ofsurface forms and entities (Section 3.2).
Instead ofa bag-of-words approach we use co-occurring NEsin text for describing an ambiguous surface form.We introduce several different disambiguation fea-tures that exploit the relations between entities de-rived from the graph structure of Wikipedia (Section3.3).
Finally, we combine our disambiguation fea-tures and achieve state-of-the-art results with a Sup-port Vector Machine (SVM) classifier (Section 4).2 Problem statementThe task of NED is to assign a surface form s foundin a document d to a target NE t ?
E(s), whereE(s) ?
E is a set of candidate NEs from an entityKB that is defined by E = {e1, e2, ..., en}, or torecognize that the found surface form s refers to amissing target entity t /?
E(s).
For solving the task,three main challenges have to be addressed:Ambiguity Names of NEs may be ambiguous.Since the same surface form s may refer to morethan one NE e, the correct target entity t has to bedetermined from a set of candidates E(s)Name variants Often, name variants (e.g.
abbre-viations, acronyms or synonyms) are used in textsto refer to the same NE, which has to be consideredfor the determination of candidates E(s) for a givensurface form s.KB coverage KBs cover only a limited numberof NEs, mostly popular NEs.
Another challenge ofFigure 1: Ambiguity of Wikipedia surface forms.
Thedistribution follows a power law, as many surface formshave only a single meaning (i.e.
refer to a singleWikipedia concept), and some surface forms are highlyambiguous, referring to very many different concepts.NED is therefore to recognize missing NEs wheret /?
E(s), given a surface form s (NIL detection).3 Named Entity DisambiguationWe formulate NED as a supervised binary classifi-cation problem.
In this section we describe the con-struction and structure of the KB and the candidateselection scheme, followed by an overview of dis-ambiguation features and the candidate classifica-tion algorithm.3.1 Knowledge base constructionOur approach disambiguates named entities againsta KB constructed from Wikipedia.
To this end, weprocess Wikipedia to extract several types of infor-mation for each Wikipedia article describing a con-cept (i.e.
any article not being a redirect page, a dis-ambiguation page, or any other kind of meta page).We collect a set of name variants (surface forms)for each concept from article titles, redirect pages,disambiguation pages and the anchor texts of inter-nal Wikipedia links, following Cucerzan (2007).
Foreach concept, we also collect its set of incoming andoutgoing links to other Wikipedia pages.
Finally, weextract the article?s full text.
We store this informa-tion in an inverted index, which allows for very ef-ficient access and search during candidate selectionand feature computation.19The distribution of surface forms follows a powerlaw, where the majority of surface forms is unam-biguous, but some surface forms are very ambigu-ous (Figure 1).
This suggests that for a given set ofdistinct surface forms found in a document, many ofthese will unambiguously refer to a single Wikipediaentity.
These entities can then be used to disam-biguate surface forms referring to multiple entities.3.2 Candidate selectionGiven a surface form identified in a document, thetask of the candidate selection component is to re-trieve a set of candidate entities from the KB.
Tothis end, we execute a search on index fields storingarticle titles, redirect titles, and name variants.
Weimplement a weighted search to give high weightsto exact title matches, a lesser emphasis on redi-rect matches, and finally a low weight for all othername variants.
In addition, we implement a fuzzysearch on the title and redirect fields to select KBentries with approximate string similarity to the sur-face form.3.3 Disambiguation featuresIn this section, we describe the features that we usein our disambiguation approach.Entity Context (EC) The EC disambiguation fea-ture is calculated as the cosine similarity betweenthe document context d of a surface form s and theWikipedia article c of each candidate c ?
E(s).
Werepresent both contexts as vectors of URIs.
To created we extract all NEs from the text using the StanfordNE Recognizer (Finkel et al, 2005) and representeach NE by its Wikipedia URI.
If a surface form isambiguous, we choose the most popular NE with thepopularity metric described below.
Analogously, werepresent each c as a vector of the incoming and out-going URIs found on its Wikipedia page.Link Context (LC) The link context feature is anextension of the EC feature.
Since our observa-tions have shown that the entity context can be verysmall and consequently the overlap between d andc may be very low, we extend d by all incoming(LC-in) or by all incoming and outgoing (LC-all)Wikipedia URIs of the NEs from the entity context.We assume that Wikipedia pages that refer to otherWikipedia pages contain information on the refer-enced pages or at least are thematically related tothese pages.
With the extension of d to d?, we ex-pect a higher overlap between the context vectors, sothat cos(d?, c) ?
cos(d, c).Candidate Rank (CR) The features described sofar disambiguate every surface form s ?
S from adocument d separately, whereas our Candidate Rankfeature aims to disambiguate all surface forms Sfound in a document d at once.
We represent d asa graph D = (E(S), L(E(S))) where the nodesE(S) = ?s?SE(s) are all candidates of all surfaceforms in the document and L(E(S)) is the set oflinks between the candidates, as found in Wikipedia.Then, we compute the PageRank score (Brin andPage, 1998) of all c ?
E(S) and choose for eachs the candidate with the highest PageRank score inthe document graph D.Standard Features In addition to the previouslydescribed features we also implement a set of com-monly accepted features.
These include a featurebased on the cosine similarity between word vectorrepresentations of the document and the Wikipediaarticle of each candidate (BOW) (Bunescu, 2007).We perform stemming, remove stopwords, andweight words with tf.idf in both cases.
Another stan-dard feature we use is the popularity of a surfaceform (SFP).
We calculate how often a surface form sreferences a candidate c ?
E(s) in relation to the to-tal number of mentions of s in Wikipedia (Han andZhao, 2009).
Since we use an index for selectingcandidates (Section 3.2), we also exploit the candi-date selection score (CS) returned for each candidateas a disambiguation feature.3.4 Candidate classifier and NIL detectionWe cast NED as a supervised classification task anduse two binary SVM classifiers (Vapnik, 1995).
Thefirst classifier decides for each candidate c ?
E(s) ifit corresponds to the target entity.
Each candidate isrepresented as a vector x(c) of features.
For trainingthe classifier we label as a positive example at mostone x(c) from the set of candidates for a surface forms, and all others as negative.In addition, we train a separate classifier to detectNIL queries, i.e.
where all x(c) fromE(s) are labeledas negative examples.
This may e.g.
be the case20All queries KB NILBaseline features 0.7797 0.6246 0.8964All features 0.8391 0.6795 0.9592Best features 0.8422 0.6825 0.9623Dredze et al 0.7941 0.6639 0.8919Zheng et al 0.8494 0.7900 0.8941Best TAC 2009 0.8217 0.7725 0.8919Median TAC 2009 0.7108 0.6352 0.7891Table 1: Micro-averaged accuracy for TAC-KBP 2009data compared for different feature sets.
The best featureset contains all features except for LC-all and CR.
Oursystem outperforms previously reported results on NILqueries, and compares favorably on all queries.if the similarity values of all candidates c ?
E(s)are very low.
We calculate several different fea-tures, such as the maximum, mean and minimum,the difference between maximum and mean, and thedifference between maximum and minimum, of allatomic features, using the feature vectors of all can-didates in E(s).
Both classifier use a radial basisfunction kernel, with parameter settings of C = 32and ?
= 8.
We optimized these settings on a sepa-rate development dataset.4 EvaluationWe conduct our experiments on the 2009 Knowl-edge Base Population (KBP) dataset of the TextAnalysis Conference (TAC) (McNamee and Dang,2009).
The dataset consists of a KB derived from a2008 snapshot of the English Wikipedia, and a col-lection of newswire, weblog and newsgroup docu-ments.
A set of 3904 surface form-document pairs(queries) is constructed from these sources, encom-passing 560 unique entities.
The majority of queries(57%) are NIL queries, of the KB queries, 69%are for organizations and 15% each for persons andgeopolitical entities.
For each query the surfaceform appearing in the given document has to be dis-ambiguated against the KB.We randomly split the 3904 queries to perform10-fold cross-validation, and stratify the resultingfolds to ensure a similar distribution of KB and NILqueries in our training data.
After normalizing fea-ture values to be in [0, 1], we train a candidate anda NIL classifier on 90% of the queries in each it-eration, and test using the remaining 10%.
Resultsreported in this paper are then averaged across the0,00,10,20,30,40,50,60,70,80,91,0Baseline features Best features Dredze et al Zheng et alMicro-averagedaccuracyAll queries KB NILFigure 2: The micro-averaged accuracy for all types ofqueries on TAC-KBP 2009 data in comparison to othersystems.test folds.Table 1 compares the micro-averaged accuracy ofour approach on KB and NIL queries for differentfeature sets, and lists the results of two other state-of-the-art systems (Dredze et al, 2010; Zheng et al,2010), as well as the best and median reported per-formance of the 2009 TAC-KBP track (McNameeet al, 2010).
Micro-averaged accuracy is calculatedas the fraction of correct queries, and is the officialTAC-KBP evaluation measure.
As a baseline we usea feature set consisting of the BOW and SFP fea-tures.
The best feature set in our experiments com-prises all features except for the LC-all and CR fea-tures.Our best accuracy of 0.84 compares favorablywith other state-of-the-art systems on this dataset.Using the best feature set improves the disambigua-tion accuracy by 6.2% over the baseline feature set,which is significant at p = 0.05.
For KB queriesour system?s accuracy is higher than that of Dredzeet al, but lower than the accuracy reported by Zhenget al One striking result is the high accuracy for NILqueries, where our approach outperforms all previ-ously reported results (Figure 2).Figure 3 displays the performance of our ap-proach when iteratively adding features.
We cansee that the novel entity features contribute to ahigher overall accuracy.
Including the candidate se-lection score (CS) improves accuracy by 3.6% overthe baseline.
The Wikipedia link-based features pro-vide additional gains, however differences are quite210,750,800,85SFP + BOW CS + SFP + BOW CS + SFP +BOW + EC CS + SFP +BOW + EC + LC-inCS + SFP +BOW + EC +LC-allAllMicro-averaged accuracyFigure 3: Differences in micro-averaged accuracy forvarious feature combinations on TAC-KBP 2009 data.Adding Wikipedia link-based features significantly im-proves performance over the baseline feature set.small (1.0?
1.5%).
We find that there is hardly anydifference in performance between using the LC-all and LC-in features.
The Candidate Rank (CR)feature slightly decreases the overall accuracy.
Amanual inspection of the CR feature shows that of-ten candidates cannot be distinguished by the clas-sifier because they are assigned the same PageRankscores.
We assume this results from our use of uni-form priors for the edges and vertices of the docu-ment graphs.5 Conclusion and Future WorkWe presented a supervised approach for namedentity disambiguation that explores novel featuresbased on Wikipedia?s link structure.
These featuresuse NEs co-occurring with an ambiguous surfaceform in a document and their Wikipedia relations toscore the candidates.
Our system achieves state-of-the-art results on the TAC-KBP 2009 dataset.
Wefind that our features improve disambiguation resultsby 6.2% over the popularity baseline, and are espe-cially helpful for recognizing entities not containedin the KB.In future work we plan to explore multilin-gual data for NED.
Since non-English versions ofWikipedia often are less extensive than the Englishversion we find it promising to combine Wikipediaversions of different languages and to use them as asource for multilingual NED.
For multilingual NEDevaluation we are currently working on a Germandataset, following the TAC-KBP dataset creationguidelines.
In addition to Wikipedia, we also intendto exploit more dynamical information sources.
Forexample, when considering news articles, NEs of-ten occur for a certain period of time in consecutivenews dealing with the same topic.
This short-timecontext could be a useful source of information fordisambiguating novel entities.ReferencesSergey Brin and Lawrence Page.
1998.
The anatomyof a large-scale hypertextual web search engine.
InWWW7: Proceedings of the seventh international con-ference on World Wide Web 7, pages 107?117, Ams-terdam, The Netherlands.
Elsevier Science PublishersB.
V.Razvan Bunescu and Marius Pasca.
2006.
Using ency-clopedic knowledge for named entity disambiguation.In Proceedings of the 11th Conference of the EuropeanChapter of the Association for Computational Linguis-tics (EACL-06), pages 9?16, Trento, Italy.Razvan Constantin Bunescu.
2007.
Learning for Infor-mation Extraction: From Named Entity Recognitionand Disambiguation To Relation Extraction.
Ph.D.thesis, University of Texas at Austin, Department ofComputer Sciences.Silviu Cucerzan.
2007.
Large-Scale named entity dis-ambiguation based on Wikipedia data.
In Proceed-ings of the 2007 Joint Conference on Empirical Meth-ods in Natural Language Processing and Computa-tional Natural Language Learning (EMNLP-CoNLL),pages 708?716, Prague, Czech Republic.
Associationfor Computational Linguistics.Mark Dredze, Paul McNamee, Delip Rao, Adam Ger-ber, and Tim Finin.
2010.
Entity disambiguationfor knowledge base population.
In Proceedings ofthe 23rd International Conference on ComputationalLinguistics (Coling 2010), pages 277?285, Beijing,China.
Coling 2010 Organizing Committee.Jenny Rose Finkel, Trond Grenager, and ChristopherManning.
2005.
Incorporating non-local informationinto information extraction systems by gibbs sampling.In Proceedings of the 43rd Annual Meeting on Associ-ation for Computational Linguistics, pages 363?370,Ann Arbor, Michigan.
Association for ComputationalLinguistics.Xianpei Han and Jun Zhao.
2009.
Named entity dis-ambiguation by leveraging wikipedia semantic knowl-edge.
In Proceeding of the 18th ACM conference onInformation and knowledge management, pages 215?224, Hong Kong, China.
ACM.22Paul McNamee and Hoa Trang Dang.
2009.
Overviewof the tac 2009 knowledge base population track.
InText Analysis Conference (TAC).Paul McNamee, Hoa Trang Dang, Heather Simpson,Patrick Schone, and Stephanie M. Strassel.
2010.
Anevaluation of technologies for knowledge base pop-ulation.
In Proceedings of the Seventh conferenceon International Language Resources and Evaluation(LREC?10).
European Language Resources Associa-tion (ELRA).Vladimir N. Vapnik.
1995.
The nature of statisticallearning theory.
Springer-Verlag New York, Inc., NewYork, NY, USA.Wei Zhang, Jian Su, Chew Lim Tan, and Wen Ting Wang.2010.
Entity linking leveraging automatically gener-ated annotation.
In Proceedings of the 23rd Interna-tional Conference on Computational Linguistics (Col-ing 2010), pages 1290?1298, Beijing, China.
Coling2010 Organizing Committee.Zhicheng Zheng, Fangtao Li, Minlie Huang, and XiaoyanZhu.
2010.
Learning to link entities with knowledgebase.
In Human Language Technologies: The 2010Annual Conference of the North American Chapter ofthe Association for Computational Linguistics, HLT?10, pages 483?491, Stroudsburg, PA, USA.
Associ-ation for Computational Linguistics.23
