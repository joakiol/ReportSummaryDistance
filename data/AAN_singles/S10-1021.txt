Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 104?107,Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational LinguisticsBART: A Multilingual Anaphora Resolution SystemSamuel Broscheit?, Massimo Poesio?, Simone Paolo Ponzetto?, Kepa Joseba Rodriguez?,Lorenza Romano?, Olga Uryupina?, Yannick Versley?, Roberto Zanoli?
?Seminar fu?r Computerlinguistik, University of Heidelberg?CiMeC, University of Trento?Fondazione Bruno Kessler?SFB 833, University of Tu?bingenbroscheit@cl.uni-heidelberg.de, massimo.poesio@unitn.it,ponzetto@cl.uni-heidelberg.de, kepa.rodriguez@unitn.it,romano@fbk.eu, uryupina@gmail.com,versley@sfs.uni-tuebingen.de, zanoli@fbk.euAbstractBART (Versley et al, 2008) is a highly mod-ular toolkit for coreference resolution thatsupports state-of-the-art statistical approachesand enables efficient feature engineering.
Forthe SemEval task 1 on Coreference Resolu-tion, BART runs have been submitted for Ger-man, English, and Italian.BART relies on a maximum entropy-basedclassifier for pairs of mentions.
A novel entity-mention approach based on Semantic Trees isat the moment only supported for English.1 IntroductionThis paper presents a multilingual coreference reso-lution system based on BART (Versley et al, 2008).BART is a modular toolkit for coreference resolutionthat supports state-of-the-art statistical approachesto the task and enables efficient feature engineer-ing.
BART has originally been created and testedfor English, but its flexible modular architecture en-sures its portability to other languages and domains.In SemEval-2010 task 1 on Coreference Resolution,BART has shown reliable performance for English,German and Italian.In our SemEval experiments, we mainly focus onextending BART to cover multiple languages.
Givena corpus in a new language, one can re-train BARTto obtain baseline results.
Such a language-agnosticsystem, however, is only used as a starting point:substantial improvements can be achieved by incor-porating language-specific information with the helpof the Language Plugin.
This design provides ef-fective separation between linguistic and machinelearning aspects of the problem.2 BART ArchitectureThe BART toolkit has five main components: pre-processing pipeline, mention factory, feature extrac-tion module, decoder and encoder.
In addition, anindependent LanguagePlugin module handles all thelanguage specific information and is accessible fromany component.
The architecture is shown on Figure1.
Each module can be accessed independently andthus adjusted to leverage the system?s performanceon a particular language or domain.The preprocessing pipeline converts an input doc-ument into a set of lingustic layers, representedas separate XML files.
The mention factory usesthese layers to extract mentions and assign theirbasic properties (number, gender etc).
The fea-ture extraction module describes pairs of mentions{Mi,Mj}, i < j as a set of features.The decoder generates training examples througha process of sample selection and learns a pairwiseclassifier.
Finally, the encoder generates testing ex-amples through a (possibly distinct) process of sam-ple selection, runs the classifier and partitions thementions into coreference chains.3 Language-specific issuesBelow we briefly describe our language-specific ex-tensions to BART.
These issues are addressed inmore details in our recent papers (Broscheit et al,2010; Poesio et al, 2010).3.1 Mention DetectionRobust mention detection is an essential componentof any coreference resolution system.
BART sup-ports different pipelines for mention detection.
The104ParserDep-to-ConstConverterMorphologyPreprocessingMentionFactoryDecoderBasic featuresSyntactic featuresKnowledge-basedfeaturesMaxEntClassifierMention(with basicproperties):- Number- Gender- Mention Type- ModifiersUnannotatedTextCoreferenceChainsLanguagePluginFigure 1: BART architecturechoice of a pipeline depends crucially on the avail-ability of linguistic resources for a given language.For English and German, we use the ParsingPipeline and Mention Factory to extract mentions.The parse trees are used to identify minimal andmaximal noun projections, as well as additional fea-tures such as number, gender, and semantic class.For English, we use parses from a state-of-the-artconstituent parser (Petrov et al, 2006) and extractall base noun phrases as mentions.
For German,the SemEval dependency tree is transformed to aconstituent representation and minimal and maxi-mal phrases are extracted for all nominal elements(pronouns, common nouns, names), except when thenoun phrase is in a non-referring syntactic position(for example, expletive ?es?, predicates in copulaconstructions).For Italian, we use the EMD Pipeline and Men-tion Factory.
The Typhoon (Zanoli et al, 2009)and DEMention (Biggio et al, 2009) systems wereused to recognize mentions in the test set.
For eachmention, its head and extension were considered.The extension was learned by using the mention an-notation provided in the training set (13th column)whereas the head annotation was learned by exploit-ing the information produced by MaltParser (Nivreet al, 2007).
In addition to the features extractedfrom the training set, such as prefixes and suffixes(1-4 characters) and orthographic information (capi-talization and hyphenation), a number of features ex-tracted by using external resources were used: men-tions recognized by TextPro (http://textpro.fbk.eu),gazetteers of generic proper nouns extracted fromthe Italian phone-book and Wikipedia, and other fea-tures derived from WordNet.
Each of these featureswas extracted in a local context of ?2 words.3.2 FeaturesWe view coreference resolution as a binary classifi-cation problem.
Each classification instance consistsof two markables, i.e.
an anaphor and potential an-tecedent.
Instances are modeled as feature vectors(cf.
Table 1) and are handed over to a binary clas-sifier that decides, given the features, whether theanaphor and the candidate are coreferent or not.
Allthe feature values are computed automatically, with-out any manual intervention.Basic feature set.
We use the same set of rela-tively language-independent features as a backboneof our system, extending it with a few language-specific features for each subtask.
Most of them areused by virtually all the state-of-the-art coreferenceresolution systems.
A detailed description can befound, for example, in (Soon et al, 2001).English.
Our English system is based on a novelmodel of coreference.
The key concept of our modelis a Semantic Tree ?
a filecard associated with eachdiscourse entity containing the following fields:?
Types: the list of types for mentions of a givenentity.
For example, if an entity contains themention ?software from India?, the shallowpredicate ?software?
is added to the types.?
Attributes: this field collects the premodifiers.For instance, if one of the mentions is ?the ex-pensive software?
the shallow attribute ?expen-sive?
is added to the list of attributes.?
Relations: this field collects the prepositionalpostmodifiers.
If an entity contains the men-tion ?software from India?, the shallow relation?from(India)?
is added to the list of relations.105For each mention BART creates such a filecardusing syntactic information.
If the classifier decidesthat both mentions are corefering, the filecard ofthe anaphora is merged into the filecard of the an-tecedent (cf.
Section 3.3 below).The SemanticTreeCompatibility featureextractor checks whether individual slots of theanaphor?s filecard are compatible with those of theantecedent?s.The StrudelRelatedness feature relies onStrudel ?
a distributional semantic model (Baroni etal., 2010).
We compute Strudel vectors for the setsof types of the anaphor and the antecedent.
The re-latedness value is determined as the cosine betweenthe two.German.
We have tested extra features for Ger-man in our previous study (Broscheit et al, 2010).The NodeDistance feature measures the num-ber of clause nodes (SIMPX, R-SIMPX) and preposi-tional phrase nodes (PX) along the path between Mjand Miin the parse tree.The PartialMorphMatch feature is a sub-string match with a morphological extension forcommon nouns.
In German the frequent use ofnoun composition makes a simple string match forcommon nouns unfeasible.
The feature checks fora match between the noun stems of Miand Mj.We extract the morphology with SMOR/Morphisto(Schmid et al, 2004).The GermanetRelatedness feature uses thePathfinder library for GermaNet (Finthammer andCramer, 2008) that computes and discretizes rawscores into three categories of semantic relatedness.In our experiments we use the measure from Wu andPalmer (1994), which has been found to be the bestperforming on our development data.Italian.
We have designed a feature to cover Ital-ian aliasing patterns.
A list of company/person des-ignators (e.g., ?S.p.a?
or ?D.ssa?)
has been manuallycrafted.
We have collected patterns of name variantsfor locations.
Finally, we have relaxed abbreviationconstraints, allowing for lower-case characters in theabbreviations.
Our pilot experiments suggest that,although a universal aliasing algorithm is able to re-solve some coreference links between NEs, creatinga language-specific module boosts the system?s per-formance for Italian substantially.Basic feature setMentionType(Mi),MentionType(Mj)SemanticClass(Mi), SemanticClass(Mj)GenderAgreement(Mi,Mj)NumberAgreement(Mi,Mj)AnimacyAgreement(Mi,Mj)StringMatch(Mi,Mj)Distance(Mi,Mj)Basic features used for English and ItalianAlias(Mi,Mj)Apposition(Mi,Mj)FirstMention(Mi)EnglishIsSubject(Mi)SemanticTreeCompatibility(Mi,Mj)StrudelRelatedness(Mi,Mj)GermanInQuotedSpeech(Mi), InQuotedSpeech(Mj)NodeDistance(Mi,Mj)PartialMorphMatch(Mi,Mj)GermanetRelatedness(Mi,Mj)ItalianAliasItalian(Mi,Mj)Table 1: Features used by BART: each feature describesa pair of mentions {Mi,Mj}, i < j, where Miis a can-didate antecedent and Mjis a candidate anaphor3.3 Resolution AlgorithmThe BART toolkit supports several models of coref-erence (pairwise modeling, rankers, semantic trees),as well as different machine learning algorithms.Our final setting relies on a pairwise maximum en-tropy classifier for Italian and German.Our English system is based on an entity-mentionmodel of coreference.
The key concept of our modelis a Semantic Tree - a filecard associated to each dis-course entity (cf.
Section 3.2).
Semantic trees areused for both computing feature values and guidingthe resolution process.We start by creating a Semantic Tree for eachmention.
We process the document from left toright, trying to find an antecedent for each men-tion (candidate anaphor).
When the antecedent isfound, we extend its Semantic Tree with the types,attributes and relations of the anaphor, providedthey are mutually compatible.
Consider, for ex-106ample, a list of mentions, containing, among oth-ers, ?software from India?, ?the software?
and ?soft-ware from China?.
Initially, BART creates the fol-lowing semantic trees: ?
(type: software) (relation:from(India))?, ?
(type: software)?
and ?
(type: soft-ware) (relation: from(China))?.
When the secondmention gets resolved to the first one, their seman-tic trees are merged to ?
(type: software) (relation:from(India)?.
Therefore, when we attempt to resolvethe third mention, both candidate antecedents are re-jected, as their relation attributes are incompatiblewith ?from(China)?.
This approach helps us avoiderroneous links (such as the link between the secondand the third mentions in our example) by leveragingentity-level information.4 EvaluationThe system was evaluated on the SemEval task 1corpus by using the SemEval scorer.First, we have evaluated our mention detectionmodules: the system?s ability to recognize both themention extensions and the heads in the regular set-ting.
BART has achieved the best score for men-tion detection in German and has shown reliablefigures for English.
For Italian, the moderate per-formance level is due to the different algorithmsfor identifying the heads: the MaltParser (trainedon TUT: http://www.di.unito.it/?tutreeb) produces amore semantic representation, while the SemEvalscorer seems to adopt a more syntactic approach.Second, we have evaluated the quality of ourcoreference resolution modules.
For German, BARThas shown better performance than all the other sys-tems on the regular track.For English, the only language targeted by all sys-tems, BART shows good performance over all met-rics in the regular setting, usually only outperformedby systems that were tuned to a particular metric.Finally, the Italian version of BART shows re-liable figures for coreference resolution, given themention alignment problem discussed above.5 ConclusionWe have presented BART ?
a multilingual toolkitfor coreference resolution.
Due to its highly modu-lar architecture, BART allows for efficient language-specific feature engineering.
Our effort representsthe first steps towards building a freely availablecoreference resolution system for many languages.ReferencesMarco Baroni, Brian Murphy, Eduard Barbu, and Mas-simo Poesio.
2010.
Strudel: A corpus-based semanticmodel based on properties and types.
Cognitive Sci-ence, 34(2):222?254.Silvana Marianela Bernaola Biggio, Claudio Giuliano,Massimo Poesio, Yannick Versley, Olga Uryupina, andRoberto Zanoli.
2009.
Local entity detection andrecognition task.
In Proc.
of Evalita-09.Samuel Broscheit, Simone Paolo Ponzetto, Yannick Ver-sley, and Massimo Poesio.
2010.
Extending BART toprovide a coreference resolution system for German.In Proc.
of LREC ?10.Marc Finthammer and Irene Cramer.
2008.
Explor-ing and navigating: Tools for GermaNet.
In Proc.
ofLREC ?08.Joakim Nivre, Johan Hall, Jens Nilsson, Atanas Chanev,Gulsen Eryigit, Sandra Ku?bler, Svetoslav Marinov,and Erwin Marsi.
2007.
Maltparser: A language-independent system for data-driven dependency pars-ing.
Natural Language Engineering, 13(2):95?135.Slav Petrov, Leon Barett, Romain Thibaux, and DanKlein.
2006.
Learning accurate, compact, and inter-pretable tree annotation.
In Proc.
of COLING-ACL-06.Massimo Poesio, Olga Uryupina, and Yannick Versley.2010.
Creating a coreference resolution system forItalian.
In Proc.
of LREC ?10.Helmut Schmid, Arne Fitschen, and Ulrich Heid.
2004.SMOR: A German computational morphology cover-ing derivation, composition and inflection.
In Proc.
ofLREC ?04.Wee Meng Soon, Hwee Tou Ng, and Daniel Chung YongLim.
2001.
A machine learning approach to corefer-ence resolution of noun phrases.
Computational Lin-guistics (Special Issue on Computational AnaphoraResolution), 27(4):521?544.Yannick Versley, Simone Paolo Ponzetto, Massimo Poe-sio, Vladimir Eidelman, Alan Jern, Jason Smith,Xiaofeng Yang, and Alessandro Moschitti.
2008.BART: A modular toolkit for coreference resolution.In Proceedings of the Linguistic Coreference Work-shop at the International Conference on Language Re-sources and Evaluation (LREC-2008).Zhibiao Wu and Martha Palmer.
1994.
Verb semanticsand lexical selection.
In Proc.
of ACL-94, pages 133?138.Roberto Zanoli, Emiliano Pianta, and Claudio Giuliano.2009.
Named entity recognition through redundancydriven classifier.
In Proc.
of Evalita-09.107
