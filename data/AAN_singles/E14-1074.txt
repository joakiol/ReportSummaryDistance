Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 702?711,Gothenburg, Sweden, April 26-30 2014. c?2014 Association for Computational LinguisticsCluster-based Prediction of User Ratings for Stylistic Surface RealisationNina Dethlefs, Heriberto Cuaya?huitl, Helen Hastie, Verena Rieser and Oliver LemonHeriot-Watt University, Mathematical and Computer Sciences, Edinburghn.s.dethlefs@hw.ac.ukAbstractSurface realisations typically depend ontheir target style and audience.
A challengein estimating a stylistic realiser from data isthat humans vary significantly in their sub-jective perceptions of linguistic forms andstyles, leading to almost no correlation be-tween ratings of the same utterance.
We ad-dress this problem in two steps.
First, weestimate a mapping function between thelinguistic features of a corpus of utterancesand their human style ratings.
Users arepartitioned into clusters based on the sim-ilarity of their ratings, so that ratings fornew utterances can be estimated, even fornew, unknown users.
In a second step, theestimated model is used to re-rank the out-puts of a number of surface realisers to pro-duce stylistically adaptive output.
Resultsconfirm that the generated styles are recog-nisable to human judges and that predictivemodels based on clusters of users lead tobetter rating predictions than models basedon an average population of users.1 IntroductionStylistic surface realisation aims not only to findthe best realisation candidate for a semantic inputbased on some underlying trained model, but alsoaims to adapt its output to properties of the user,such as their age, social group, or location, amongothers.
One of the first systems to address stylis-tic variation in generation was Hovy (1988)?sPAULINE, which generated texts that reflect dif-ferent speaker attitudes towards events based onmultiple, adjustable features.
Stylistic variationin such contexts can often be modelled systemat-ically as a multidimensional variation space withseveral continuous dimensions, so that varyingstylistic scores indicate the strength of each di-mension in a realisation candidate.
Here, we fo-cus on the dimensions of colloquialism, politenessand naturalness.
Assuming a target score on oneor more dimensions, candidate outputs of a data-driven realiser can then be ranked according totheir predicted affinity with the target scores.In this paper, we aim for an approach to stylis-tic surface realisation which is on the one handbased on natural human data so as to reflect stylis-tic variation that is as natural as possible.
On theother hand, we aim to minimise the amount ofannotation and human engineering that informsthe design of the system.
To this end, we esti-mate a mapping function between automaticallyidentifiable shallow linguistic features character-istic of an utterance and its human-assigned styleratings.
In addition, we aim to address the highdegree of variability that is often encountered insubjective rating studies, such as assessments ofrecommender systems (O?Mahony et al., 2006;Amatriain et al., 2009), sentiment analysis (Pangand Lee, 2005), or surface realisations, where userratings have been shown to differ significantly(p<0.001) for the same utterance (Walker et al.,2007).
Such high variability can affect the per-formance of systems which are trained from anaverage population of user ratings.
However, weare not aware of any work that has addressed thisproblem principally by estimating ratings for bothknown users, for whom ratings exists, and un-known users, for whom no prior ratings exist.
Toachieve this, we propose to partition users intoclusters of individuals who assign similar ratingsto linguistically similar utterances, so that theirratings can be estimated more accurately than702based on an average population of users.
This issimilar to Janarthanam and Lemon (2014), whoshow that clustering users and adapting to theirlevel of domain expertise can significantly im-prove task success and user ratings.
Our resultingmodel is evaluated with realisers not originallybuilt to deal with stylistic variation, and producesnatural variation recognisable by humans.2 Architecture and DomainWe aim to with generating restaurant recommen-dations as part of an interactive system.
To dothis, we assume that a generator input is providedby a preceding module, e.g.
the interaction man-ager, and that the task of the surface realiser isto find a suitable stylistically appropriate realisa-tion.
An example input is inform(food=Italian,name=Roma), which could be expressed as Therestaurant Roma serves Italian food.
A furtheraspect is that users are initially unknown to thesystem, but that it should adapt to them over timeby discovering their stylistic preferences.
Fu-ture work involves integrating the surface realiserinto the PARLANCE1 (Hastie et al., 2013) spo-ken dialogue system with a method for triggeringthe different styles.
Here, we leave the questionof when different styles are appropriate as futurework and focus on being able to generate them.The architecture of our model is shown in Fig-ure 1.
Training of the regression model from sty-listically-rated human corpora is shown in the top-left box (grey).
Utterance ratings from humanjudges are used to extract shallow linguistic fea-tures as well as to estimate user clusters.
Bothtypes of information inform the resulting stylis-tic regression model.
For surface realisation (top-right box, blue), a semantic input from a preced-ing model is given as input to a surface realiser.Any realiser is suitable that returns a ranked list ofoutput candidates.
The resulting list is re-rankedaccording to stylistic scores estimated by the re-gressor, so that the utterance which most closelyreflects the target score is ranked highest.
The re-ranking process is shown in the lower box (red).3 Related Work3.1 Stylistic Variation in Surface RealisationOur approach is most closely related to work byPaiva and Evans (2005) and Mairesse and Walker1http://parlance-project.euUser ClustersRegressor Surface RealisationRanking +EvaluationFigure 1: Architecture of stylistic realisation model.Top left: user clusters are estimated from corpus ut-terances described by linguistic features and ratings.Top right: surface realisation ranks a list of output can-didates based on a semantic input.
These are rankedstylistically given a trained regressor.
(2011), discussed in turn here.
Paiva and Evans(2005) present an approach that uses multivari-ate linear regression to map individual linguisticfeatures to distinguishable styles of text.
The ap-proach works in three steps.
First, a factor anal-ysis is used to determine the relevant stylistic di-mensions from a corpus of human text using shal-low linguistic features.
Second, a hand-craftedgenerator is used to produce a large set of ut-terances, keeping traces of each generator deci-sion, and obtaining style scores for each outputbased on the estimated factor model.
The resultis a dataset of <generator decision, style score>pairs which can be used in a correlation analy-sis to identify the predictors of particular outputstyles.
During generation, the correlation equa-tions inform the generator at each choice point soas to best express the desired style.
Unfortunately,no human evaluation of the model is presented sothat it remains unclear to what extent the gener-ated styles are perceivable by humans.Closely related is work by Mairesse and Walker(2011) who present the PERSONAGE system,which aims to generate language reflecting par-ticular personalities.
Instead of choosing genera-tor decisions by considering their predicted stylescores, however, Mairesse and Walker (2011) di-rectly predict generator decisions based on tar-get personality scores.
To obtain the generator,the authors first generate a corpus of utteranceswhich differ randomly in their linguistic choices.All utterances are rated by humans indicating the703extent to which they reflect different personalitytraits.
The best predictive model is then chosen ina comparison of several classifiers and regressors.Mairesse and Walker (2011) are the first to evalu-ate their generator with humans and show that thegenerated personalities are indeed recognisable.Approaches on replicating personalities in re-alisations include Gill and Oberlander (2002) andIsard et al.
(2006).
Porayska-Pomsta and Mellish(2004) and Gupta et al.
(2007) are approaches topoliteness in generation, based on the notion offace and politeness theory, respectively.3.2 User Preferences in Surface RealisationTaking users?
individual content preferences intoaccount for training generation systems canpositively affect their performance (Jordan andWalker, 2005; Dale and Viethen, 2009).
We areinterested in individual user perceptions concern-ing the surface realisation of system output andthe way they relate to different stylistic dimen-sions.
Walker et al.
(2007) were the first to showthat individual preferences exist for the perceivedquality of realisations and that these can be mod-elled in trainable generation.
They train two ver-sions of a rank-and-boost generator, a first versionof which is trained on the average population ofuser ratings, whereas a second one is trained onthe ratings of individual users.
The authors showstatistically that ratings from different users aredrawn from different distributions (p<0.001) andthat significantly better performance is achievedwhen training and testing on data of individualusers.
In fact, training a model on one user?s rat-ings and testing it on another?s performs as badlyas a random baseline.
However, no previous workhas modelled the individual preferences of unseenusers?for whom no training data exists.4 Estimation of Style Prediction Models4.1 Corpora and Style DimensionsOur domain of interest is the automatic generationof restaurant recommendations that differ with re-spect to their colloquialism and politeness and areas natural as possible.
All three stylistic dimen-sion were identified from a qualitative analysis ofhuman domain data.
To estimate the strength ofeach of them in a single utterance, we collect userratings for three data sets that were collected un-der different conditions and are freely available.Corpus Colloquial Natural PoliteLIST 3.38 ?
1.5 4.06 ?
1.2 4.35 ?
0.8MAI 3.95 ?
1.2 4.32 ?
1.0 4.27 ?
0.8CLASSIC 4.29 ?
1.1 4.20 ?
1.2 3.64 ?
1.3Table 1: Average ratings with standard deviations.Ratings between datasets (except one) differ signifi-cantly at p<0.01, using the Wilcoxon signed-rank test.?
LIST is a corpus of restaurant recommenda-tions from the website The List.2 It consistsof professionally written reviews.
An exam-ple is ?Located in the heart of Barnwell, Bel-uga is an excellent restaurant with a smartmenu of modern Italian cuisine.??
MAI is a dataset collected by Mairesse etal.
(2010),3 using Amazon Mechanical Turk.Turkers typed in recommendations for vari-ous specified semantics; e.g.
?I recommendthe restaurant Beluga near the cathedral.??
CLASSIC is a dataset of transcribed spokenuser utterances from the CLASSiC project.4The utterances consist of user queries forrestaurants, such as ?I need an Italianrestaurant with a moderate price range.
?Our joint dataset consists of 1, 361 human ut-terances, 450 from the LIST, 334 from MAI,and 577 from CLASSIC.
We asked users on theCrowdFlower crowdsourcing platform5 to readutterances and rate their colloquialism, politenessand naturalness on a 1-5 scale (the higher the bet-ter).
The following questions were asked.?
Colloquialism: The utterance is colloquial,i.e.
could have been spoken.?
Politeness: The utterance is polite / friendly.?
Naturalness: The utterance is natural, i.e.could have been produced by a human.The question on naturalness can be seen as a gen-eral quality check for our training set.
We donot aim to generate unnatural utterances.
167users took part in our rating study leading to arated dataset of altogether 3, 849 utterances.
Allusers were from the USA.
The average ratings perdataset and stylistic dimension are summarisedin Table 1.
From this, we can see that LIST ut-terances were perceived as the least natural and2http://www.list.co.uk/3http://people.csail.mit.edu/francois/research/bagel/4http://www.classic-project.org/5http://crowdflower.com/704colloquial, but as the most polite.
CLASSIC ut-terances were perceived as the most colloquial,but the least polite, and MAI utterances were ratedas the most natural.
Differences between ratingsfor each dimension and dataset are significant atp<0.01, using the Wilcoxon signed-rank test, ex-cept the naturalness for MAI and CLASSIC.Since we are mainly interested in the lexicaland syntactic features of utterances here, the factthat CLASSIC utterances are spoken, whereas theother two corpora are written, should not affectthe quality of the resulting model.
Similarly, somestylistic categories may seem closely related, suchas colloquialism and naturalness, or orthogonalto each other, such as politeness and colloqui-alism.
However, while ratings for colloquialismand naturalness are very close for the CLASSICdataset, they vary significantly for the two otherdatasets (p<0.01).
Also, the ratings for colloqui-alim and politeness show a weak positive corre-lation of 0.23, i.e.
are not perceived as orthogo-nal by users.
These results suggest that all in allour three stylistic categories are perceived as suf-ficiently different from each other and suitable fortraining to predict a spectrum of different styles.Another interesting aspect is that individualuser ratings vary significantly, leading to a highdegree of variability for identical utterances.
Thiswill be the focus of the following sections.4.2 Feature EstimationTable 2 shows the feature set we will use in ourregression experiments.
We started from a largersubset including 45 lexical and syntactic featuresas well as unigrams and bigrams, all of whichcould be identified from the corpus without man-ual annotation.
The only analysis tool we usedwas the Stanford Parser,6 which identified certaintypes of words (pronouns, wh-words) or the depthof syntactic embedding.
A step-wise regressionanalysis was then carried out to identify thosefeatures that contributed significantly (at p<0.01)to the overall regression equation obtained perstylistic dimension.
Of all lexical features (uni-grams and bigrams), the word with was the onlycontributor.
A related feature was the average tf-idf score of the content words in an utterance.6http://nlp.stanford.edu/software/lex-parser.shtmlFeature TypeLength of utterance numPresence of personal pronouns boolPresence of WH words boolwith cue word boolPresence of negation boolAverage length of content words numAve tf-idf score of content words numDepth of syntactic embedding numTable 2: Features used for regression, which wereidentified as significant contributors (p<0.01) from alarger feature set in a step-wise regression analysis.4.3 Regression ExperimentsBased on the features identified in Section 4.2, wetrain a separate regressor for each stylistic dimen-sion.
The task of the regressor is to predict, basedon the extracted linguistic features of an utterance,a score in the range of 1-5 for colloquialism, po-liteness and naturalness.
We compare: (1) a mul-tivariate multiple regressor (MMR), (2) an M5Pdecision tree regressor, (3) a support vector ma-chine (SVM) with linear kernel, and (4) a ZeroRclassifier, which serves as a majority baseline.
Weused the R statistics toolkit7 for the MMR and theWeka toolkit8 for the remaining models.Average User Ratings The regressors were firsttrained to predict the average user ratings of an ut-terance and evaluated in a 10-fold cross validationexperiment.
Table 3 shows the results.
Here, rdenotes the Pearson correlation coefficient, whichindicates the correlation between the predictedand the actual user scores; R2 is the coefficient ofdetermination, which provides a measure of howwell the learnt model fits the data; and RMSErefers to the Root Mean Squared Error, the errorbetween the predicted and actual user ratings.We can observe that MMR achieves the bestperformance for predicting colloquialism and nat-uralness, whereas M5P best predicts politeness.Unfortunately, all regressors achieve at best amoderate correlation with human ratings.
Basedon these results, we ran a correlation analysis forall utterances for which more than 20 originaluser ratings were available.
The purpose was tofind out to what extent human raters agree witheach other.
The results showed that user agree-ment in fact ranges from a high positive corre-7http://www.r-project.org/8http://www.cs.waikato.ac.nz/ml/weka/705Model r R2 RMSEColloquialMMR 0.50 0.25 0.85SVM 0.47 0.22 0.86M5P 0.48 0.23 0.85ZeroR -0.08 0.006 0.97NaturalMMR 0.30 0.09 0.78SVM 0.24 0.06 0.81M5P 0.27 0.07 0.78ZeroR -0.09 0.008 0.81PoliteMMR 0.33 0.11 0.71SVM 0.31 0.09 0.73M5P 0.42 0.18 0.69ZeroR -0.09 0.008 0.76Table 3: Comparison of regression models per dimen-sion using average user ratings.
The best model isindicated in bold-face for the correlation coefficient.Model r R2 RMSEColloquialMMR 0.61 0.37 1.05SVM 0.36 0.13 1.3M5P 0.56 0.31 1.07ZeroR -0.06 0.004 1.3NaturalMMR 0.55 0.30 0.96SVM 0.36 0.13 1.13M5P 0.49 0.24 0.99ZeroR -0.08 0.06 1.13PoliteMMR 0.69 0.48 0.76SVM 0.54 0.30 0.92M5P 0.71 0.50 0.73ZeroR -0.04 0.002 1.04Table 4: Comparison of regression models per dimen-sion using individual user ratings.
The best model isindicated in bold-face for the correlation coefficient.lation of 0.79 to a moderate negative correlationof ?0.55.
The average is 0.04 (SD=0.95), i.e.indicating no correlation between user ratings,even for the same utterance.
This observation ispartially in line with related work that has foundhigh diversity in subjective user ratings.
Yeh andMellish (1997) report only 70% agreement of hu-man judges on the best choice of referring ex-pression.
Amatriain et al.
(2009) report incon-sistencies in user ratings in recommender systemswith an RMSE range of 0.55 to 0.81 and arguethat this constitutes a lower bound for system per-formance.
This inconsistency is exacerbated byraters recruited via crowdsourcing platforms asin our study (Koller et al., 2010; Rieser et al.,2011).
However, while crowdsourced data havebeen shown to contain substantially more noisethan data collected in a lab environment, they dotend to reflect the general tendency of their morecontrolled counterparts (Gosling et al., 2004).Individual User Ratings Given that individualpreferences exist for surface realisation (Walkeret al., 2007), we included the user?s ID as a re-gression feature and re-ran the experiments.
Thehypothesis was that if users differ in their pref-erences for realisation candidates, they may alsodiffer in terms of their perceptions of linguisticstyles.
The results shown in Table 4 support this:the obtained correlations are significantly higher(p<0.001, using the Fisher r-to-z transformation)than those without the user?s ID (though we arestill not able to model the full variation observedin ratings).
Importantly, this shows that user rat-ings are intrinsically coherent (not random) andthat variation exists mainly for inter-user agree-ment.
This model performs satisfactorily for aknown population of users.
However, it does notallow the prediction of ratings of unknown users,who we mostly encounter in generation.5 Clustering User Rating Behaviour5.1 Spectral ClusteringThe goal of this section is to find a number of kclusters which partition our data set of user rat-ings in a way that users in one cluster rate ut-terances with particular linguistic properties mostsimilarly to each other, while rating them mostdissimilarly to users in other clusters.
We as-sume a set of n data points x1.
.
.
xn, whichin our case correspond to an individual user orgroup of users, characterised in terms of wordbigrams, POS tag bigrams, and assigned rat-ings of the utterance they rated.
An exampleis Beluga NNP serves VBZ Italian JJ food NN;[col=5.0, nat=5.0, pol=4.0].
Features were cho-sen as a subset of relevant features from the largerset used for regression above.Using spectral clustering (von Luxburg, 2007),clusters can be identified from a set of eigenvec-tors of an affinity matrix S derived from pair-wisesimilarities between data points sij= s(xi, xj)using a symmetric and non-negative similarityfunction.
To do that, we use a cumulative simi-larity based on the Kullback-Leibler divergence,D(P,Q) =?ipilog2(piqi) +?jqjlog2(qjpj)2,where P is a distribution of words, POS tags orratings in data point xi; and Q a similar distribu-tion in data point xj.
The lower the cumulative di-7060.30.40.50.6Number of ClustersCorrelationCoefficient1 3 5 7 9 20 40 60 80 100 167IndividualClustersAverageFigure 2: Average correlation coefficient for differentnumbers of clusters.
For comparison, results from av-erage and individual user ratings are also shown.vergence between two data sets, the more similarthey are.
To find clusters of similar users from theaffinity matrix S, we use the algorithm describedin Ng et al.
(2001).
It derives clusters by choosingthe k largest eigenvectors u1, u2, .
.
.
, ukfrom theLaplacian matrix L = D1/2?SD1/2 (where D isa diagonal matrix), arranging them into columnsin a matrix U = [u1u2.
.
.
uk] and then normalis-ing them for length.
The result is a new matrix T ,obtained through tij= uij/(?ku2ik)1/2.
The setof clusters C1, .
.
.
Ckcan then be obtained from Tusing the K-means algorithm, where each row inT serves as an individual data point.
Finally, eachoriginal data point xi(row i of T ) is assigned to acluster Cj.
In comparison to other clustering algo-rithms, experiments by Ng et al.
(2001) show thatspectral clustering is robust for convex and non-convex data sets.
The authors also demonstratewhy using K-means only is often not sufficient.The main clusters obtained describe surfacerealisation preferences by particular groups ofusers.
An example is the realisation of the loca-tion of a restaurant as a prepositional phrase or asa relative clause as in restaurant in the city centrevs.
restaurant located in the city centre; or the re-alisation of the food type as an adjective, an Ital-ian restaurant, vs. a clause, this restaurant servesItalian food.
Clusters can then be characterised asdifferent combinations of such preferences.5.2 Results: Predicting Stylistic RatingsFigure 2 shows the average correlation coefficientr across dimensions in relation to the numberof clusters, in comparison to the results obtainedwith average and individual user ratings.
We cansee that the baseline without user information isoutperformed with as few as three clusters.
From30 clusters on, a medium correlation is obtaineduntil another performance jump occurs around 90clusters.
Evidently, the best performance wouldbe achieved by obtaining one cluster per user, i.e.167 clusters, but nothing would be gained in thisway, and we can see that useful generalisationscan be made from much fewer clusters.
Based onthe clusters found, we will now predict the ratingsof known and unknown users.Known Users For known users, first of all, Fig-ure 3 shows the correlations between the predictedand actual ratings for colloquialism, politenessand naturalness based on 90 user clusters.
Cor-relation coefficients were obtained using an MMRregressor.
We can see that a medium correlation isachieved for naturalness and (nearly) strong cor-relations are achieved for politeness and colloqui-alism.
This confirms that clustering users can helpto better predict their ratings than based on shal-low linguistic features alone, but that more gener-alisation is achieved than based on individual userratings that include the user?s ID as a regressionfeature.
The performance gain in comparison topredicting average ratings is significant (p<0.01)from as few as three clusters onwards.Unknown Users We initially sort unknownusers into the majority cluster and then aim tomake more accurate cluster allocations as moreinformation becomes available.
For example, af-ter a user has assigned their first rating, we cantake it into account to re-estimate their clustermore accurately.
Clusters are re-estimated witheach new rating, based on our trained regressionmodel.
While estimating a user cluster based onlinguistic features alone yields an average corre-lation of 0.38, an estimation based on linguisticfeatures and a single rating alone already yields anaverage correlation of 0.45.
From around 30 rat-ings, the average correlation coefficients achievedare as good as for known users.
More importantly,though, estimations based on a single rating alonesignificantly outperform ratings based on the av-707(a)1 2 3 4 52345Correlation: ColloquialismActual RatingsPredictedRatings(b)1 2 3 4 512345Correlation: NaturalnessActual RatingsPredictedRatings(c)1 2 3 4 512345Correlation: PolitenessActual RatingsPredictedRatingsFigure 3: Correlations per dimension between actual and predicted user ratings based on 90 user clusters: (a)Colloquialism (r = 0.57, p<0.001), (b) Naturalness (r = 0.49, p<0.001) and (c) Politeness (r = 0.59, p<0.001).erage population of users (p<0.001).
Fig.
4 showsthis process.
It shows the correlation between pre-dicted and actual user ratings for unknown usersover time.
This is useful in interactive scenarios,where system behaviour is refined as more infor-mation becomes available (Cuaya?huitl and Deth-lefs, 2011; Gas?ic?
et al., 2011), or for incrementalsystems (Skantze and Hjalmarsson, 2010; Deth-lefs et al., 2012b; Dethlefs et al., 2012a).0.30.40.50.6Number of RatingsCorrelationCoefficient1 2 3 4 5 6 7 8 9 10 15 20 3090 ClustersRatingsAverageFigure 4: Average correlation coefficient for unknownusers with an increasing number of ratings.
Resultsfrom 90 clusters and average ratings are also shown.6 Evaluation: Stylistically-AwareSurface RealisationTo evaluate the applicability of our regressionmodel for stylistically-adaptive surface realisa-tion, this section describes work that comparesfour different surface realisers, which were notoriginally developed to produce stylistic variation.To do that, we first obtain the cluster for each in-put sentence s: c?
= argminc?C?xD(Pxs|Qxc),where x refers to n-grams, POS tags or ratings(see Section 5.1); P refers to a discrete probabilitydistribution of sentence s; and Q refers to a dis-crete probability distribution of cluster c. The bestcluster is used to compute the style score of sen-tence s using: score(s) =?ni?ifi(s), c??
F ,where ?iare the weights estimated by the regres-sor, and fiare the features of sentence s; see Table2.
The idea is that if well-phrased utterances canbe generated, whose stylistic variation is recog-nisable to human judges, then our regressor canbe used in combination with any statistical sur-face realiser.
Note however that the stylistic vari-ation observed depends on the stylistic spectrumthat each realiser covers.
Here, our goal is mainlyto show that whatever stylistic variation exists ina realiser can be recognised by our model.6.1 Overview of Surface RealisersIn a human rating study, we compare four surfacerealisers (ordered alphabetically), all of whichare able to return a ranked list of candidate re-alisations for a semantic input.
Please refer tothe references given for details of each system.The BAGEL and SPaRKy realisers were comparedbased on published ranked output lists.9?
BAGEL is a surface realiser based on dy-namic Bayes Nets originally trained usingActive Learning by Mairesse et al.
(2010).It was shown to generate well-phrased utter-ances from unseen semantic inputs.?
CRF (global) treats surface realisation as a9Available from http://people.csail.mit.edu/francois/research/bagel and http://users.soe.ucsc.edu/?maw/downloads.html.708System UtteranceBAGEL Beluga is a moderately pricedrestaurant in the city centre area.Col = 4.0, Pol = 4.0, Nat = 4.0CRF (global) Set in the city centre, Beluga is amoderately priced location for thecelebration of the Italian spirit.Col = 2.0, Pol = 5.0, Nat = 2.0pCRU Beluga is located in the city centreand serves cheap Italian food.Col = 4.0, Pol = 3.0, Nat = 5.0SPaRKy Beluga has the best overall qualityamong the selected restaurantssince this Italian restaurant hasgood decor, with good service.Col = 3.0, Pol = 4.0, Nat = 5.0Table 5: Example utterances for the BAGEL, CRF(global), pCRU and SPaRKy realisers shown to users.Sample ratings from individual users are also shown.sequence labelling task: given a set of (ob-served) linguistic features, it aims to find thebest (hidden) sequence of phrases realising asemantic input (Dethlefs et al., 2013).?
pCRU is based on probabilistic context-free grammars and generation is done usingViterbi search, sampling (used here), or ran-dom search.
It is based on Belz (2008).?
SPaRKy is based on a rank-and-boost ap-proach.
It learns a mapping between the lin-guistic features of a target utterance and itspredicted user ratings and ranks candidatesaccordingly (Walker et al., 2007).6.2 Results: Recognising Stylistic Variation242 users from the USA took part in a rating studyon the CrowdFlower platform and rated altogether1, 702 utterances, from among the highest-rankedsurface realisations above.
For each utterancethey read, they rated the colloquialism, natura-less and politeness based on the same questionsas in Section 4.1, used to obtain the training data.Based on this, we compare the perceived strengthof each stylistic dimension in an utterance to theone predicted by the regressor.
Example utter-ances and ratings are shown in Table 5.
Resultsare shown in Table 6 and confirm our observa-tions: ratings for known users can be estimatedwith a medium (or high) correlation based onclusters of users who assign similar ratings to ut-terances with similar linguistic features.
We canalso see that such estimations do not depend on aparticular data set or realiser.System Colloquial Polite NaturalBAGEL 0.78 0.66 0.69CRF global 0.58 0.63 0.63pCRU 0.67 0.42 0.77SPaRKy 0.87 0.56 0.81Table 6: Correlation coefficients between subjectiveuser ratings and ratings predicted by the regressor forknown users across data-driven surface realisers.A novel aspect of our technique in compari-son to previous work on stylistic realisation isthat it does not depend on the time- and resource-intensive design of a hand-coded generator, as inPaiva and Evans (2005) and Mairesse and Walker(2011).
Instead, it can be applied in conjunc-tion with any system designer?s favourite realiserand preserves the realiser?s original features byre-ranking only its top n (e.g.
10) output candi-dates.
Our method is therefore able to strike abalance between highly-ranked and well-phrasedutterances and stylistic adaptation.
A current lim-itation of our model is that some ratings can stillnot be predicted with a high correlation with hu-man judgements.
However, even the medium cor-relations achieved have been shown to be signif-icantly better than estimations based on the aver-age population of users (Section 5.2).7 Conclusion and Future WorkWe have presented a model of stylistic realisationthat is able to adapt its output along several stylis-tic dimensions.
Results show that the variation isrecognisable by humans and that user ratings canbe predicted for known as well as unknown users.A model which clusters individual users basedon their ratings of linguistically similar utterancesachieves significantly higher performance than amodel trained on the average population of rat-ings.
These results may also play a role in otherdomains in which users display variability in theirsubjective ratings, e.g.
recommender systems,sentiment analysis, or emotion generation.
Futurework may explore the use of additional cluster-ing features as a more scalable alternative to re-ranking.
It also needs to determine how user feed-back can be obtained during an interaction, whereasking users for ratings may be disruptive.
Possi-bilities include to infer user ratings from their nextdialogue move, or from multimodal informationsuch as hesitations or eye-tracking.709Acknowledgements This research was fundedby the EC FP7 programme FP7/2011-14 undergrant agreements no.
270019 (SPACEBOOK)and no.
287615 (PARLANCE).ReferencesXavier Amatriain, Josep M. Pujol, and Nuria Oliver.2009.
I like It...
I Like It Not: Evaluating User Rat-ings Noise in Recommender Systems.
In In the 17thInternational Conference on User Modelling, Adap-tation, and Personalisation (UMAP), pages 247?258, Trento, Italy.
Springer-Verlag.Anja Belz.
2008.
Automatic Generation of WeatherForecast Texts Using Comprehensive ProbabilisticGeneration-Space Models.
Natural Language En-gineering, 14(4):431?455.Penelope Brown and Stephen Levinson.
1987.
SomeUniversals in Language Usage.
Cambridge Univer-sity Press, Cambridge, UK.Heriberto Cuaya?huitl and Nina Dethlefs.
2011.
Op-timizing Situated Dialogue Management in Un-known Environments.
In INTERSPEECH, pages1009?1012.Robert Dale and Jette Viethen.
2009.
ReferringExpression Generation Through Attribute-BasedHeuristics.
In Proceedings of the 12th Euro-pean Workshop on Natural Language Generation(ENLG), Athens, Greece.Nina Dethlefs, Helen Hastie, Verena Rieser, and OliverLemon.
2012a.
Optimising Incremental DialogueDecisions Using Information Density for Interac-tive Systems.
In Proceedings of the Conference onEmpirical Methods in Natural Language Process-ing (EMNLP-CoNLL), Jeju, South Korea.Nina Dethlefs, Helen Hastie, Verena Rieser, and OliverLemon.
2012b.
Optimising Incremental Genera-tion for Spoken Dialogue Systems: Reducing theNeed for Fillers.
In Proceedings of the Interna-tional Conference on Natural Language Generation(INLG), Chicago, Illinois, USA.Nina Dethlefs, Helen Hastie, Heriberto Cuaya?huitl,and Oliver Lemon.
2013.
Conditional RandomFields for Responsive Surface Realisation UsingGlobal Features.
In Proceedings of the 51st AnnualMeeting of the Association for Computational Lin-guistics (ACL), Sofia, Bulgaria.Michael Fleischman and Eduard Hovy.
2002.
Emo-tional Variation in Speech-Based Natural LanguageGeneration.
In Proceedings of the 2nd InternationalNatural Language Generation Conference.Milica Gas?ic?, Filip Jurc??
?c?ek, Blaise Thomson, Kai Yu,and Steve Young.
2011.
On-Line Policy Optimi-sation of Spoken Dialogue Systems via Interactionwith Human Subjects.
In Proceedings of the IEEEAutomatic Speech Recognition and Understanding(ASRU) Workshop.Alastair Gill and Jon Oberlander.
2002.
Taking Careof the Linguistic Features of Extraversion.
In Pro-ceedings of the 24th Annual Conference of the Cog-nitive Science Society, pages 363?368, Fairfax, VA.Samuel Gosling, Simine Vazire, Sanjay Srivastava,and Oliver John.
2004.
Should We Trust Web-Based Studies?
A Comparative Analysis of Six Pre-conceptions About Internet Questionnaires.
Ameri-can Psychologist, 59(2):93?104.Swati Gupta, Marilyn Walker, and Daniela Romano.2007.
How Rude Are You?
Evaluating Politenessand Affect in Interaction.
In Proceedings of the2nd International Conference on Affective Comput-ing and Intelligent Interaction.Helen Hastie, Marie-Aude Aufaure, Panos Alex-opoulos, Heriberto Cuayhuitl, Nina Dethlefs,James Henderson Milica Gasic, Oliver Lemon,Xingkun Liu, Peter Mika, Nesrine Ben Mustapha,Verena Rieser, Blaise Thomson, Pirros Tsiakoulis,Yves Vanrompay, Boris Villazon-Terrazas, andSteve Young.
2013.
Demonstration of the PAR-LANCE System: A Data-Driven, Incremental, Spo-ken Dialogue System for Interactive Search.
In Pro-ceedings of the 14th Annual Meeting of the SpecialInterest Group on Discourse and Dialogue (SIG-dial).Eduard Hovy.
1988.
Generating Natural Languageunder Pragmatic Constraints.
Lawrence ErlbaumAssociates, Hillsdale, NJ.Amy Isard, Carsten Brockmann, and Jon Oberlander.2006.
Individuality and Alignment in GeneratedDialogues.
In Proceedings of the 4th InternationalNatural Language Generation Conference (INLG),Sydney, Australia.Srini Janarthanam and Oliver Lemon.
2014.
Adaptivegeneration in dialogue systems using dynamic usermodeling.
Computational Linguistics.
(in press).Pamela Jordan and Marilyn Walker.
2005.
LearningContent Selection Rules for Generating Object De-scriptions in Dialogue.
Journal of Artificial Intelli-gence Research, 24:157?194.Alexander Koller, Kristina Striegnitz, Donna Byron,Justine Cassell, Robert Dale, and Johanna Moore.2010.
The First Challenge on Generating Instruc-tions in Virtual Environments.
In M. Theune andE.
Krahmer, editors, Empirical Methods in Natu-ral Language Generation, pages 337?361.
SpringerVerlag, Berlin/Heidelberg.Franc?ois Mairesse and Marilyn Walker.
2011.
Con-trolling User Perceptions of Linguistic Style: Train-able Generation of Personality Traits.
Computa-tional Linguistics, 37(3):455?488, September.Franc?ois Mairesse, Milica Gas?ic?, Filip Jurc??
?c?ek, Si-mon Keizer, Blaise Thomson, Kai Yu, and SteveYoung.
2010.
Phrase-based statistical languagegeneration using graphical models and active learn-ing.
In Proceedings of the Annual Meeting of the710Association for Computational Linguistics (ACL),pages 1552?1561.Andrew Ng, Michael Jordan, and Yair Weiss.
2001.On Spectral Clustering: Analysis and an Algorithm.In Advances in Neural Information Processing Sys-tems, pages 849?856.
MIT Press.Michael O?Mahony, Neil Hurley, and Gue?nole?
Sil-vestre.
2006.
Detecting Noise in RecommenderSystem Databases.
In Proceedings of the Inter-national Conference on Intelligent User Interfaces(IUI)s. ACM Press.Daniel Paiva and Roger Evans.
2005.
Empirically-Based Control of Natural Language Generation.
InProceedings of the 43rd Annual Meeting of theAssociation for Computational Linguistics (ACL),Ann Arbor, Michigan, USA.Bo Pang and Lillian Lee.
2005.
Seeing Stars: Exploit-ing Class Relationships for Sentiment Categoriza-tion with Respect to Rating Scales.
In Proceedingsof the 43rd Annual Meeting of the Association forComputational Linguistics (ACL).Kaska Porayska-Pomsta and Chris Mellish.
2004.Modelling Politness in Natural Language Gener-ation.
In Proceedings of the 3rd InternationalNatural Language Generation Conference (INLG),Brighton, UK.Verena Rieser, Simon Keizer, Xingkun Liu, and OliverLemon.
2011.
Adaptive Information Presentationfor Spoken Dialogue Systems: Evaluation with Hu-man Subjects.
In Proceedings of the 13th Euro-pean Workshop on Natural Language Generation(ENLG), Nancy, France.Gabriel Skantze and Anna Hjalmarsson.
2010.
To-wards Incremental Speech Generation in DialogueSystems.
In Proceedings of the 11th Annual Sig-Dial Meeting on Discourse and Dialogue, Tokyo,Japan.Ulrike von Luxburg.
2007.
A Tutorial on SpectralClustering.
Statistics and Computing, 17(4).Marilyn Walker, Amanda Stent, Franc?ois Mairesse,and Rashmi Prasad.
2007.
Individual and Do-main Adaptation in Sentence Planning for Dia-logue.
Journal of Artificial Intelligence Research,30(1):413?456.Ching-long Yeh and Chris Mellish.
1997.
An Empir-ical Study on the Generation of Anaphora in Chi-nese.
Computational Linguistics, 23:169?190.711
