Proceedings of the EACL 2012 Joint Workshop of LINGVIS & UNCLH, pages 16?24,Avignon, France, April 23 - 24 2012. c?2012 Association for Computational LinguisticsLooking at word meaning.An interactive visualization of Semantic Vector Spaces for Dutch synsetsKris Heylen, Dirk Speelman and Dirk GeeraertsQLVL, University of LeuvenBlijde-Inkomsstraat 21/3308, 3000 Leuven (Belgium){kris.heylen, dirk.speelman, dirk.geeraerts}@arts.kuleuven.beAbstractIn statistical NLP, Semantic Vector Spaces(SVS) are the standard technique for theautomatic modeling of lexical semantics.However, it is largely unclear how theseblack-box techniques exactly capture wordmeaning.
To explore the way an SVS struc-tures the individual occurrences of words,we use a non-parametric MDS solution ofa token-by-token similarity matrix.
TheMDS solution is visualized in an interac-tive plot with the Google Chart Tools.
Asa case study, we look at the occurrences of476 Dutch nouns grouped in 214 synsets.1 IntroductionIn the last twenty years, distributional models ofsemantics have become the standard way of mod-eling lexical semantics in statistical NLP.
Thesemodels, aka Semantic Vector Spaces (SVSs) orWord Spaces, capture word meaning in termsof frequency distributions of words over co-occurring context words in a large corpus.
Thebasic assumption of the approach is that wordsoccurring in similar contexts will have a simi-lar meaning.
Speficic implementations of thisgeneral idea have been developed for a wide va-riety of computational linguistic tasks, includ-ing Thesaurus extraction and Word Sense Dis-ambiguation, Question answering and the model-ing of human behavior in psycholinguistic experi-ments (see Turney and Pantel (2010) for a generaloverview of applications and speficic models).
Inrecent years, Semantic Vector Spaces have alsoseen applications in more traditional domains oflinguistics, like diachronic lexical studies (Sagi etal., 2009; Cook and Stevenson, 2010; Rohrdantzet al, 2011) , or the study of lexical variation(Peirsman et al, 2010).
In this paper, we want toshow how Semantic Vector Spaces can further aidthe linguistic analysis of lexical semantics, pro-vided that they are made accessible to lexicolo-gists and lexicographers through a visualizationof their output.Although all applications mentioned above as-sume that distributional models can capture wordmeaning to some extent, most of them use SVSsonly in an indirect, black-box way, without an-alyzing which semantic properties and relationsactually manifest themselves in the models.
Thisis mainly a consequence of the task-based evalu-ation paradigm prevalent in Computational Lin-guistics: the researchers address a specific taskfor which there is a pre-defined gold standard;they implement a model with some new features,that usually stem from a fairly intuitive, common-sense reasoning of why some feature might bene-fit the task at hand; the new model is then testedagainst the gold standard data and there is an eval-uation in terms of precision, recall and F-score.In rare cases, there is also an error analysis thatleads to hypotheses about semantic characteristicsthat are not yet properly modeled.
Yet hardly ever,there is in-depth analysis of which semantics thetested model actually captures.
Even though task-based evaluation and shared test data sets are vitalto the objective comparison of computational ap-proaches, they are, in our opinion, not sufficientto assess whether the phenomenon of lexical se-mantics is modeled adequately from a linguisticperspective.
This lack of linguistic insight intothe functioning of SVSs is also bemoaned in thecommunity itself.
For example, Baroni and Lenci(2011) say that ?To gain a real insight into the16abilities of DSMs (Distributional Semantic Mod-els, A/N) to address lexical semantics, existingbenchmarks must be complemented with a moreintrinsically oriented approach, to perform directtests on the specific aspects of lexical knowledgecaptured by the models?.
They go on to presenttheir own lexical database that is similar to Word-Net, but includes some additional semantic rela-tions.
They propose researchers test their modelagainst the database to find out which of the en-coded relations it can detect.
However, such ananalysis still boils down to checking whether amodel can replicate pre-defined structuralist se-mantic relations, which themselves represent aquite impoverished take on lexical semantics, atleast from a linguistic perspective.
In this pa-per, we want to argue that a more linguisticallyadequate investigation of how SVSs capture lex-ical semantics, should take a step back from theevalution-against-gold-standard paradigm and doa direct and unbiased analysis of the output ofSVS models.
Such an analysis should comparethe SVS way of structuring semantics to the richdescriptive and theoretic models of lexical se-mantics that have been developed in Linguisticsproper (see Geeraerts (2010b) for an overview ofdifferent research traditions).
Such an in-depth,manual analyis has to be done by skilled lexicolo-gists and lexicographers.
But would linguists, thatare traditionally seen as not very computation-ally oriented, be interested in doing what manyComputational Linguists consider to be tediousmanual analysis?
The answer, we think, is yes.The last decade has seen a clear empirical turnin Linguistics that has led linguists to embraceadvanced statistical analyses of large amounts ofcorpus data to substantiate their theoretical hy-potheses (see e.g.
Geeraerts (2010a) and othercontributions in Glynn and Fischer (2010) on re-search in semantics).
SVSs would be an idealaddition to those linguists?
methodological reper-toire.
This creates the potential for a win-win sit-uation: Computational linguists get an in-depthevaluation of their models, while theoretical lin-guists get a new tool for doing large scale empir-ical analyses of word meaning.
Of course, onecannot just hand over a large matrix of word sim-ilaties (the raw output of an SVS) and ask a lexi-cologist what kind of semantics is ?in there?.
In-stead, a linguist needs an intuitive interface to ex-plore the semantic structure captured by an SVS.In this paper, we aim to present exactly that: an in-teractive visualization of a Semantic Vector SpaceModel that allows a lexicologist or lexicographerto inspect how the model structures the uses ofwords.2 Token versus Type levelSVSs can model lexical semantics on two levels:1. the type level: aggregating over all occur-rences of a word, giving a representation ofa word?s general semantics.2.
the token level: representing the semanticsof each individual occurrence of a word.The type-level models are mostly used to retrievesemantic relations between words, e.g.
synonymsin the task of thesaurus extraction.
Token-levelmodels are typically used to distinguish betweenthe different meanings within the uses of oneword, notably in the task of Word Sense Disam-biguation or Word Sense Induction.
Lexicologicalstudies on the other hand, typically combine bothperspectives: their scope is often defined on thetype level as the different words of a lexical fieldor the set of near-synonyms referring to the sameconcept, but they then go on to do a fine-grainedanalysis on the token level of the uses of thesewords to find out how the semantic space is pre-cisely structured.
In our study, we will also takea concept-centered perspective and use as a start-ing point the 218 sets of Dutch near-synonymousnouns that Ruette et al (2012) generated withtheir type-level SVS.
For each synset, we then im-plement our own token-level SVS to model theindividual occurrences of the nouns.
The result-ing token-by-token similarity matrix is then visu-alized to show how the occurrences of the differ-ent nouns are distributed over the semantic spacethat is defined by the synset?s concept.
BecauseDutch has two national varieties (Belgium andthe Netherlands) that show considerable lexicalvariation, and because this is typically of inter-est to lexicologists, we will also differentiate theNetherlandic and Belgian tokens in our SVS mod-els and their visualization.The rest of this paper is structured as follows.In the next section we present the corpus andthe near-synonym sets we used for our study.Section 4 presents the token-level SVS imple-mented for modeling the occurrences of the nouns17in the synsets.
In section 5 we discuss the vi-sualization of the SVS?s token-by-token similar-ity matrices with Multi Dimensional Scaling andthe Google Visualization API.
Finally, section 6wraps up with conclusions and prospects for fu-ture research.3 Dutch corpus and synsetsThe corpus for our study consists of Dutch news-paper materials from 1999 to 2005.
For Nether-landic Dutch, we used the 500M words TwenteNieuws Corpus (Ordelman, 2002)1, and for Bel-gian Dutch, the Leuven Nieuws Corpus (aka Me-diargus corpus, 1.3 million words2).
The corporawere automatically lemmatized, part-of-speechtagged and syntactically parsed with the Alpinoparser (van Noord, 2006).Ruette et al (2012) used the same corporafor their semi-automatic generation of sets ofDutch near-synonymous nouns.
They used a so-called dependency-based model (Pado?
and Lap-ata, 2007), which is a type-level SVS that modelsthe semantics of a target word as the weighted co-occurrence frequencies with context words thatapear in a set of pre-defined dependency relationswith the target (a.o.
adjectives that modify thetarget noun, and verbs that have the target nounas their subject).
Ruette et al (2012) submittedthe output of their SVS to a clustering algorithmknown as Clustering by Committee (Pantel andLin, 2002).
After some further manual cleaning,this resulted in 218 synsets containing 476 nounsin total.
Table 1 gives some examples.CONCEPT nouns in synsetINFRINGEMENT inbreuk, overtredingGENOCIDE volkerenmoord, genocidePOLL peiling, opiniepeiling, rondvraagMARIHUANA cannabis, marihuanaCOUP staatsgreep, coupMENINGITIS hersenvliesontsteking, meningitisDEMONSTRATOR demonstrant, betogerAIRPORT vliegveld, luchthavenVICTORY zege, overwinningHOMOSEXUAL homo, homoseksueel, homofielRELIGION religie, godsdienstCOMPUTER SCREEN computerschem, beeldscherm, monitorTable 1: Dutch synsets (sample)1Publication years 1999 up to 2002 of Algemeen Dag-blad, NRC, Parool, Trouw and Volkskrant2Publication years 1999 up to 2005 of De Morgen, DeTijd, De Standaard, Het Laatste Nieuws, Het Nieuwsbladand Het Belang van Limburg4 Token-level SVSNext, we wanted the model the individual oc-currences of the nouns.
The token-level SVSwe used is an adaptation the approach proposedby Schu?tze (1998).
He models the semanticsof a token as the frequency distribution over itsso-called second order co-occurrences.
Thesesecond-order co-occurrences are the type-levelcontext features of the (first-order) context wordsco-occuring with the token.
This way, a token?smeaning is still modeled by the ?context?
it oc-curs in, but this context is now modeled itself bycombining the type vectors of the words in thecontext.
This higher order modeling is necessaryto avoid data-sparseness: any token only occurswith a handful of other words and a first-order co-occurrence vector would thus be too sparse to doany meaningful vector comparison.
Note that thisapproach first needs to construct a type-level SVSfor the first-order context words that can then beused to create a second-order token-vector.In our study, we therefore first constructed atype-level SVS for the 573,127 words in our cor-pus with a frequency higher than 2.
Since the fo-cus of this study is visualization rather than find-ing optimal SVS parameter settings, we chose set-tings that proved optimal in our previous studies(Peirsman et al, 2008; Heylen et al, 2008; Peirs-man et al, 2010).
For the context features of thisSVS, we used a bag-of-words approach with awindow of 4 to the left and right around the tar-gets.
The context feature set was restricted to the5430 words, that were the among the 7000 mostfrequent words in the corpus, (minus a stoplist of34 high-frequent function words) AND that oc-curred at least 50 times in both the Netherlandicand Belgian part of the corpus.
The latter wasdone to make sure that Netherlandic and Belgiantype vectors were not dissimilar just because oftopical bias from proper names, place names orwords relating to local events.
Raw co-occurrencefrequencies were weighted with Pointwise MutualInformation and negative PMI?s were set to zero.In a second step, we took a random sample of100 Netherlandic and a 100 Belgian newspaperissues from the corpus and extracted all occur-rences of each of the 476 nouns in the synsetsdescribed above.
For each occurrence, we builta token-vector by averaging over the type-vectorsof the words in a window of 5 words to the left18and right of the token.
We experimented with twoaveraging functions.
In a first version, we fol-lowed Schu?tze (1998) and just summed the typevectors of a token?s context words, normalizingby the number of context words for that token:~owi =?nj?Cwi~cjnwhere ~owi is the token vector for the ith occur-rence of noun w and Cwi is the set of n typevectors ~cj for the context words in the windowaround that ith occurrence of noun w. How-ever, this summation means that each first ordercontext word has an equal weight in determiningthe token vector.
Yet, not all first-order contextwords are equally informative for the meaning ofa token.
In a sentence like ?While walking towork, the teacher saw a dog barking and chasinga cat?, bark and cat are much more indicative ofthe meaning of dog than say teacher or work.In a second, weighted version, we therefore in-creased the contribution of these informative con-text words by using the first-order context words?PMI values with the noun in the synset.
PMI canbe regarded as a measure for informativeness andtarget-noun/context-word PMI-values were avail-able anyway from our large type-level SVS.
ThePMI of a noun w and a context word cj can nowbe seen as a weight pmiwcj .
In constructing the to-ken vector ~owi for the ith occurrence of noun w ,we now multiply the type vector ~cj of each con-text word with the PMI weight pmiwcj , and thennormalize by the sum of the pmi-weights:~owi =?nj?Cwipmiwcj ?
~cj?nj pmiwcjThe token vectors of all nouns from the samesynset were then combined in a token by second-order-context-feature matrix.
Note that this ma-trix has the same dimensionality as the underlyingtype-level SVS (5430).
By calculating the cosinebetween all pairs of token-vectors in the matrix,we get the final token-by-token similarity matrixfor each of the 218 synsets 3.3string operations on corpus text files were done withPython 2.7.
All matrix calculations were done in MatlabR2009a for Linux5 VisualizationThe token-by-token similarity matrices reflecthow the different synonyms carve up the ?seman-tic space?
of the synset?s concept among them-selves.
However, this information is hard to graspfrom a large matrix of decimal figures.
One pop-ular way of visualizing a similarity matrix forinterpretative purposes is Multidimensional Scal-ing (Cox and Cox, 2001).
MDS tries to give anoptimal 2 or 3 dimensional representation of thesimilarities (or distances) between objects in thematrix.
We applied Kruskal?s non-metric Multi-dimensional Scaling to the all the token-by-tokensimilarity matrices using the isoMDS function inthe MASS package of R. Our visualisation soft-ware package (see below) forced us to restrict our-selves to a 2 dimensional MDS solution for now,even tough stress levels were generally quite high(0.25 to 0.45).
Future implementation may use 3DMDS solutions.
Of course, other dimension re-duction techniques than MDS exist: PCA is usedin Latent Semantic Analysis (Landauer and Du-mais, 1997) and has been applied by Sagi et al(2009) for modeling token semantics.
Alterna-tively, Latent Dirichlect Allocation (LDA) is atthe heart of Topic Models (Griffiths et al, 2007)and was adapted by Brody and Lapata (2009) formodeling token semantics.
However, these tech-niques all aim at bringing out a latent structurethat abstracts away from the ?raw?
underlyingSVS similarities.
Our aim, on the other hand,is precisely to investigate how SVSs structure se-mantics based on contextual distribution proper-ties BEFORE additional latent structuring is ap-plied.
We therefore want a 2D representation ofthe token similarity matrix that is as faithful aspossible and that is what MDS delivers 4.In a next step we wanted to intergrate the 2dimensional MDS plots with different types ofmeta-data that might be of interest to the lexi-cologist.
Furthermore, we wanted the plots tobe interactive, so that a lexicologist can choosewhich information to visualize in the plot.
Weopted for the Motion Charts5 provided by Google4Stress is a measure for that faithfulness.
No such indi-cation is directly available for LSA or LDA.
However, we dothink LSA and LDA can be used to provide extra structure toour visualizations, see section 6.5To avoid dependence on commercial software, we alsomade an implementation based on the plotting options ofR and the Python Image Library( https://perswww.19Chart Tools6, which allows to plot objects with2D co-ordinates as color-codable and re-sizeablebubbles in an interactive chart.
If a time-variable is present, the charts can be made dy-namic to show the changing position of the ob-jects in the plot over time7.
We used the R-package googleVis (Gesmann and Castillo,2011), an interface between R and the GoogleVisualisation API, to convert our R datamatri-ces into Google Motion Charts.
The interac-tive charts, both those based on the weightedand unweighted token-level SVSs, can be ex-plored on our website ( https://perswww.kuleuven.be/?u0038536/googleVis).To illustrate the information that is avail-able through this visualization, we discuss theweighted chart for the concept COMPUTERSCREEN (Figure 1 shows a screen cap, but westrongly advise to look at the interactive versionon the website).
In Dutch, this concept can be ref-ered to with (at least) three near-synonyms, whichare color coded in the chart: beeldscherm (blue),computerscherm (green) and monitor (yellow).Each bubble in the chart is an occurrence (token)of one these nouns.
As Figure 2 shows, rolingover the bubbles makes the stretch of text visiblein which the noun occurs (These contexts are alsoavailable in the lower right side bar).
This usage-in-context allows the lexicologist to interpret theprecise meaning of the occurrence of the noun.The plot itself is a 2D representation of the seman-tic distances between all tokens (as measured witha token-level SVS) and reflects how the synonymsare distributed over the ?semantic space?.
As canbe expected with synonyms, they partially popu-late the same area of the space (the right hand sideof the plot).
Hovering over the bubbles and look-ing at the contexts, we can see that they indeedall refer to the concept COMPUTER SCREEN (Seeexample contexts 1 to 3 in Table 2).
However, wealso see that a considerable part on the left handside of the plot shows no overlap and is only popu-lated by tokens of monitor.
Looking more closelykuleuven.be/?u0038536/committees)6(http://code.google.com/apis/chart/interactive/docs/gallery/motionchart.html)7Since we worked with synchronic data, we did notuse this feature.
However, Motion Charts have been usedby Hilpert (http://omnibus.uni-freiburg.de/?mh608/motion.html) to visualize language change inMDS plots of hand coded diachronic linguistic data.at these occurrences, we see that they are instan-tiations of another meaning of monitor, viz.
?su-pervisor of youth leisure activities?
(See examplecontext 4 in Table 2).
Remember that our corpusis stratified for Belgian and Netherlandic Dutch.We can make this stratification visible by chang-ing the color coding of the bubbles to COUNTRYin the top right-hand drop-down menu.
Figure 3shows that the left-hand side, i.e.
monitor-onlyarea of the plot, is also an all-Belgian area (hov-ering over the BE value in the legend makes theBelgian tokens in the plot flash).
Changing thecolor coding to WORDBYCOUNTRY makes thiseven more clear.
Indeed the youth leader mean-ing of monitor is only familiar to speakers of Bel-gian Dutch.
Changing the color coding to thevariable NEWSPAPER shows that the youth leadermeaning is also typical for the popular, workingclass newspapers Het Laatste Nieuws (LN) andHet Nieuwsblad (NB) and is not prevelant in theBelgian high-brow newspapers.
In order to pro-vide more structure to the plot, we also experi-mented with including different K-means cluster-ing solutions (from 2 up to 6 clusters) as color-codable features, but these seem not very infor-mative yet (but see section 6).nr example context1 De analisten houden met e?e?n oog de computerschermenin de gatenThe analists keep one eye on the computer screen2 Met een digitale camera... kan je je eigen foto op hetbeeldscherm krijgenWith a digital camera, you can get your own photo on thecomputer screen3 Met een paar aanpassingen wordt het beeld op de moni-toren nog completerWith a few adjustments, the image on the screen becomeseven more complete4 Voor augustus zijn de speelpleinen nog op zoek naar mon-itorenFor August, the playgrounds are still looking for supervi-sorsTable 2: Contexts (shown in chart by mouse roll-over)On the whole, the token-level SVS succeedsfairly well in giving an interpretable semanticstructure to the tokens and the chart visualizesthis.
However, SVSs are fully automatic ways ofmodeling semantics and, not unexpectedly, sometokens are out of place.
For example, in the lowerleft corner of the yellow cluster with monitor to-kens referring to youth leader, there is also oneblue Netherlandic token of beeldscherm.
Thanksto the visualisation, such outliers can easily be20detected by the lexicologist who can then reportthem to the computational linguist.
The latter canthen try to come up with a model that gives a bet-ter fit.Finally, let us briefly look at the chart of anotherconcept, viz.
COLLISION with its near-synonymsaanrijding and botsing.
Here, we expect the lit-eral collissions (between cars), for which bothnouns can be used, to stand out form the figura-tive ones (differences in opinion between people),for which only botsing is apropriate in both vari-eties of Dutch.
Figure 4 indeed shows that theright side of the chart is almost exclusively popu-lated by botsing tokens.
Looking at their contextsreveals that they indeed overwhelmingly instan-tiate the metaphorical meaning og collision.
Yetalso here, there are some ?lost?
aanrijding tokenswith a literal meaning and the visualization showsthat the current SVS implementation is not yet afully adequate model for capturing the words?
se-mantics.6 General discussionAlthough Vector Spaces have become the main-stay of modeling lexical semantics in current sta-tistical NLP, they are mostly used in a black boxway, and how exactly they capture word meaningis not very clear.
By visualizing their output, wehope to have at least partially cracked open thisblack box.
Our aim is not just to make SVS out-put easier to analyze for computer linguists.
Wealso want to make SVSs accessible for lexicolo-gists and lexicographers with an interest in quanti-tative, empirical data analysis.
Such co-operationbrings mutual benefits: Computer linguists get ac-cess to expert evaluation of their models.
Lexicol-ogists and lexicographers can use SVSs to iden-tify preliminary semantic structure based on largequantities of corpus data, instead of heaving tosort through long lists of unstructured examplesof a word?s usage (the classical concordances).
Toour knowledge, this paper is one of the first at-tempts to visualize Semantic Vector Spaces andmake them accessible to a non-technical audi-ence.Of course, this is still largely work in progressand a number of improvements and extensions arestill possible.
First of all, the call-outs for thebubbles in the Google Motion Charts were notdesigned to contain large stretches of text.
Cur-rent corpus contexts are therefore to short to ana-lyze the precise meaning of the tokens.
One op-tion would be to have pop-up windows with largercontexts appear by clicking on the call-outs.Secondly, we didn?t use the motion feature thatgave the charts its name.
However, if we havediachronic data, we could e.g.
track the centroidof a word?s tokens in the semantic space throughtime and at the same time show the dispersion oftokens around that centroid8.Thirdly, in the current implementation, one im-portant aspect of the black-box quality of SVSsis not dealt with: it?s not clear which contextfeatures cause tokens to be similar in the SVSoutput, and, consequently, the interpreation ofthe distances in the MDS plot remains quite ob-scure.
One option would be to use the clustersolutions, that are already available as color cod-able variables, and indicate the highest scoringcontext features that the tokens in each clusterhave in common.
Another option for bringing outsense-distinguishing context words was proposedby Rohrdantz et al (2011) who use Latent Dirich-let Allocation to structure tokens.
The loadingson these latent topics could also be color-coded inthe chart.Fourthly, we already indicated that two dimen-sional MDS solutions have quite high stress val-ues and a three dimensional solution would bebetter to represent the token-by-token similari-ties.
This would require the 3D Charts, which arenot currently offered by the Google Chart Tools.However both R and Matlab do have interactive3D plotting functionality.Finally, and most importantly, the plots cur-rently do not allow any input from the user.
Ifwe want the plots to be the starting point of an in-depth semantic analysis, the lexicologist shouldbe able to annotate the occurrences with variablesof their own.
For example, they might want tocode whether the occurrence refers to a laptopscreen, a desktop screen or cell phone screen, tofind out whether their is a finer-grained division oflabor among the synonyms.
Additionally, an eval-uation of the SVS?s performance might includemoving wrongly positioned tokens in the plot andthus re-group tokens, based on the lexicologist?sinsights.
Tracking these corrective movementsmight then be valuable input for the computer lin-guists to improve their models.
Of course, this8This is basically the approach of Sagi et al (2009) butafter LSA and without interactive visualization21goes well beyond our rather opportunistic use ofthe Google Charts Tool.ReferencesMarco Baroni and Alessandro Lenci.
2011.
Howwe BLESSed distributional semantic evaluation.
InProceedings of the GEMS 2011 Workshop on GE-ometrical Models of Natural Language Semantics,pages 1?10, Edinburgh, UK.
Association for Com-putational Linguistics.Samuel Brody and Mirella Lapata.
2009.
BayesianWord Sense Induction.
In Proceedings of the 12thConference of the European Chapter of the ACL(EACL 2009), pages 103?111, Athens, Greece.
As-sociation for Computational Linguistics.Paul Cook and Suzanne Stevenson.
2010.
Automat-ically Identifying Changes in the Semantic Orien-tation of Words.
In Proceedings of the SeventhInternational Conference on Language Resourcesand Evaluation (LREC?10), pages 28?34, Valletta,Malta.
ELRA.Trevor Cox and Michael Cox.
2001.
Multidimen-sional Scaling.
Chapman & Hall, Boca Raton.Dirk Geeraerts.
2010a.
The doctor and the seman-tician.
In Dylan Glynn and Kerstin Fischer, edi-tors, Quantitative Methods in Cognitive Semantics:Corpus-Driven Approaches, pages 63?78.
Moutonde Gruyter, Berlin.Dirk Geeraerts.
2010b.
Theories of Lexical Semantics.Oxford University Press, Oxford.Markus Gesmann and Diego De Castillo.
2011.
Usingthe Google Visualisation API with R: googleVis-0.2.4 Package Vignette.Dylan Glynn and Kerstin Fischer.
2010.
Quanti-tative Methods in Cognitive Semantics: Corpus-driven Approaches, volume 46.
Mouton de Gruyter,Berlin.Thomas L. Griffiths, Mark Steyvers, and JoshuaTenenbaum.
2007.
Topics in Semantic Represen-tation.
Psychological Review, 114:211?244.Kris Heylen, Yves Peirsman, Dirk Geeraerts, and DirkSpeelman.
2008.
Modelling Word Similarity.
AnEvaluation of Automatic Synonymy Extraction Al-gorithms.
In Proceedings of the Language Re-sources and Evaluation Conference (LREC 2008),pages 3243?3249, Marrakech, Morocco.
ELRA.Thomas K Landauer and Susan T Dumais.
1997.
ASolution to Plato?s Problem: The Latent SemanticAnalysis Theory of Acquisition, Induction and Rep-resentation of Knowledge.
Psychological Review,104(2):240?411.Roeland J F Ordelman.
2002.
Twente Nieuws Cor-pus (TwNC).
Technical report, Parlevink LanguageTechonology Group.
University of Twente.Sebastian Pado?
and Mirella Lapata.
2007.Dependency-based construction of semanticspace models.
Computational Linguistics,33(2):161?199.Patrick Pantel and Dekang Lin.
2002.
Document clus-tering with committees.
In Proceedings of the 25thannual international ACM SIGIR conference on Re-search and development in information retrieval,SIGIR ?02, pages 199?206, New York, NY, USA.ACM.Yves Peirsman, Kris Heylen, and Dirk Geeraerts.2008.
Size matters: tight and loose context defini-tions in English word space models.
In Proceedingsof the ESSLLI Workshop on Distributional LexicalSemantics, pages 34?41, Hamburg, Germany.
ESS-LLI.Yves Peirsman, Dirk Geeraerts, and Dirk Speelman.2010.
The automatic identification of lexical varia-tion between language varieties.
Natural LanguageEngineering, 16(4):469?490.Christian Rohrdantz, Annette Hautli, Thomas Mayer,Miriam Butt, Daniel A Keim, and Frans Plank.2011.
Towards Tracking Semantic Change by Vi-sual Analytics.
In Proceedings of the 49th An-nual Meeting of the Association for ComputationalLinguistics: Human Language Technologies, pages305?310, Portland, Oregon, USA, June.
Associa-tion for Computational Linguistics.Tom Ruette, Dirk Geeraerts, Yves Peirsman, and DirkSpeelman.
2012.
Semantic weighting mechanismsin scalable lexical sociolectometry.
In BenediktSzmrecsanyi and Bernhard Wa?lchli, editors, Aggre-gating dialectology and typology: linguistic vari-ation in text and speech, within and across lan-guages.
Mouton de Gruyter, Berlin.Eyal Sagi, Stefan Kaufmann, and Brady Clark.
2009.Semantic Density Analysis: Comparing WordMeaning across Time and Phonetic Space.
In Pro-ceedings of the Workshop on Geometrical Mod-els of Natural Language Semantics, pages 104?111, Athens, Greece.
Association for Computa-tional Linguistics.Hinrich Schu?tze.
1998.
Automatic word sense dis-crimination.
Computational Linguistics, 24(1):97?124.Peter D. Turney and Patrick Pantel.
2010.
From Fre-quency to Meaning: Vector Space Models of Se-mantics.
Journal of Artificial Intelligence Research,37(1):141?188.Gertjan van Noord.
2006.
At Last Parsing Is NowOperational.
In Verbum Ex Machina.
Actes de la13e conference sur le traitement automatique deslangues naturelles (TALN06), pages 20?42, Leuven,Belgium.
Presses universitaires de Louvain.22Figure 1: Screencap of Motion Chart for COMPUTER SCREENFigure 2: token of beeldscherm with context23Figure 3: COMPUTER SCREEN tokens stratified by countryFigure 4: Screencap of Motion Chart for COLLISION24
