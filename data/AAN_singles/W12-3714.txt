Proceedings of the 3rd Workshop on Computational Approaches to Subjectivity and Sentiment Analysis, pages 89?98,Jeju, Republic of Korea, 12 July 2012. c?2012 Association for Computational LinguisticsAutomatically Annotating A Five-Billion-Word Corpus of Japanese Blogsfor Affect and Sentiment AnalysisMichal Ptaszynski ?
Rafal Rzepka ?
Kenji Araki ?
Yoshio Momouchi ??
JSPS Research Fellow / High-Tech Research Center, Hokkai-Gakuen Universityptaszynski@hgu.jp?
Graduate School of Information Science and Technology, Hokkaido University{kabura,araki}@media.eng.hokudai.ac.jp?
Department of Electronics and Information Engineering, Faculty of Engineering, Hokkai-Gakuen Universitymomouchi@eli.hokkai-s-u.ac.jpAbstractThis paper presents our research on automaticannotation of a five-billion-word corpus ofJapanese blogs with information on affect andsentiment.
We first perform a study in emotionblog corpora to discover that there has beenno large scale emotion corpus available forthe Japanese language.
We choose the largestblog corpus for the language and annotate itwith the use of two systems for affect anal-ysis: ML-Ask for word- and sentence-levelaffect analysis and CAO for detailed anal-ysis of emoticons.
The annotated informa-tion includes affective features like sentencesubjectivity (emotive/non-emotive) or emo-tion classes (joy, sadness, etc.
), useful in affectanalysis.
The annotations are also general-ized on a 2-dimensional model of affect to ob-tain information on sentence valence/polarity(positive/negative) useful in sentiment analy-sis.
The annotations are evaluated in severalways.
Firstly, on a test set of a thousand sen-tences extracted randomly and evaluated byover forty respondents.
Secondly, the statisticsof annotations are compared to other existingemotion blog corpora.
Finally, the corpus isapplied in several tasks, such as generation ofemotion object ontology or retrieval of emo-tional and moral consequences of actions.1 IntroductionThere is a lack of large corpora for Japanese ap-plicable in sentiment and affect analysis.
Althoughthere are large corpora of newspaper articles, likeMainichi Shinbun Corpus1, or corpora of classic lit-erature, like Aozora Bunko2, they are usually un-suitable for research on emotions since spontaneous1http://www.nichigai.co.jp/sales/mainichi/mainichi-data.html2http://www.aozora.gr.jp/emotive expressions either appear rarely in thesekinds of texts (newspapers), or the vocabulary is notup to date (classic literature).
Although there ex-ist speech corpora, such as Corpus of SpontaneousJapanese3, which could become suitable for thiskind of research, due to the difficulties with com-pilation of such corpora they are relatively small.In research such as the one by Abbasi and Chen(2007) it was proved that public Internet services,such as forums or blogs, are a good material for af-fect analysis because of their richness in evaluativeand emotive information.
One kind of these servicesare blogs, open diaries in which people encapsu-late their own experiences, opinions and feelings tobe read and commented by other people.
Recentlyblogs have come into the focus of opinion mining orsentiment and affect analysis (Aman and Szpakow-icz, 2007; Quan and Ren, 2010).
Therefore creatinga large blog-based emotion corpus could help over-come both problems: the lack in quantity of corporaand their applicability in sentiment and affect anal-ysis.
There have been only a few small Japaneseemotion corpora developed so far (Hashimoto et al,2011).
On the other hand, although there exist largeWeb-based corpora (Erjavec et al, 2008; Baroni andUeyama, 2006), access to them is usually allowedonly from the Web interface, which makes addi-tional annotations with affective information diffi-cult.
In this paper we present the first attempt to au-tomatically annotate affect on YACIS, a large scalecorpus of Japanese blogs.
To do that we use two sys-tems for affect analysis of Japanese, one for word-and sentence-level affect analysis and another espe-cially for detailed analysis of emoticons, to annotateon the corpus different kinds of affective informa-tion (emotive expressions, emotion classes, etc.
).3http://www.ninjal.ac.jp/products-k/katsudo/seika/corpus/public/89The outline of the paper is as follows.
Section2 describes the related research in emotion corpora.Section 3 presents our choice of the corpus for anno-tation of affect- and sentiment-related information.Section 4 describes tools used in annotation.
Sec-tion 5 presents detailed data and evaluation of theannotations.
Section 6 presents tasks in which thecorpus has already been applied.
Finally the paperis concluded and future applications are discussed.2 Emotion CorporaResearch on Affect Analysis has resulted in anumber of systems developed within several years(Aman and Szpakowicz, 2007; Ptaszynski et al,2009c; Matsumoto et al, 2011).
Unfortunately,most of such research ends in proposing and evaluat-ing a system.
The real world application that wouldbe desirable, such as annotating affective informa-tion on linguistic data is limited to processing a usu-ally small test sample in the evaluation.
The smallnumber of annotated emotion corpora that exist aremostly of limited scale and are annotated manually.Below we describe and compare some of the mostnotable emotion corpora.
Interestingly, six out ofeight emotion corpora described below are createdfrom blogs.
The comparison is summarized in Table1.
We also included information on the work de-scribed in this paper for better comparison (YACIS).Quan and Ren (2010) created a Chinese emotionblog corpus Ren-CECps1.0.
They collected 500blog articles from various Chinese blog services,such as sina blog (http://blog.sina.com.cn/), qq blog(http://blog.qq.com/), etc., and annotated them witha large variety of information, such as emotion class,emotive expressions or polarity level.
Although syn-tactic annotations were simplified to tokenizationand POS tagging, this corpus can be considered astate-of-the-art emotion blog corpus.
The motiva-tion for Quan and Ren is also similar to ours - deal-ing with the lack of large corpora for sentiment anal-ysis in Chinese (in our case - Japanese).Wiebe et al (2005) report on creating the MPQAcorpus of news articles.
The corpus contains 10,657sentences in 535 documents4.
The annotationschema includes a variety of emotion-related infor-4The new MPQA Opinion Corpus version 2.0 contains ad-ditional 157 documents, 692 documents in total.mation, such as emotive expressions, emotion va-lence, intensity, etc.
However, Wiebe et al focusedon detecting subjective (emotive) sentences, whichdo not necessarily convey emotions, and classifyingthem into positive and negative.
Thus their annota-tion schema, although one of the richest, does notinclude emotion classes.A corpus of Japanese blogs, called KNB, rich inthe amount and diversification of annotated informa-tion was developed by Hashimoto et al (2011).
Itcontains 67 thousand words in 249 blog articles.
Al-though it is not a small scale corpus, it developeda certain standard for preparing corpora, especiallyblog corpora for sentiment and affect-related stud-ies in Japan.
The corpus contains all relevant gram-matical annotations, including POS tagging, depen-dency parsing or Named Entity Recognition.
It alsocontains sentiment-related information.
Words andphrases expressing emotional attitude were anno-tated by laypeople as either positive or negative.One disadvantage of the corpus, apart from its smallscale, is the way it was created.
Eighty one studentswere employed to write blogs about different topicsespecially for the need of this research.
It could beargued that since the students knew their blogs willbe read mostly by their teachers, they selected theirwords more carefully than they would in private.Aman and Szpakowicz (2007) constructed asmall-scale English blog corpus.
They did not in-clude any grammatical information, but focused onaffect-related annotations.
As an interesting remark,they were some of the first to recognize the taskof distinguishing between emotive and non-emotivesentences.
This problem is usually one of the mostdifficult in text-based Affect Analysis and is there-fore often omitted in such research.
In our researchwe applied a system proved to deal with this taskwith high accuracy for Japanese.Das and Bandyopadhyay (2010) constructed anemotion annotated corpus of blogs in Bengali.
Thecorpus contains 12,149 sentences within 123 blogposts extracted from Bengali web blog archive(http://www.amarblog.com/).
It is annotated withface recognition annotation standard (Ekman, 1992).Matsumoto et al (2011) created Wakamono Ko-toba (Slang of the Youth) corpus.
It contains un-related sentences extracted manually from Yahoo!blogs (http://blog-search.yahoo.co.jp/).
Each sen-90Table 1: Comparison of emotion corpora ordered by the amount of annotations (abbreviations: T=tokenization,POS=part-of-speech tagging, L=lemmatization, DP=dependency parsing, NER=Named Entity Recognition).corpus scale language annotated affective information syntacticname (in senten-ces / docs)emotion classstandardemotiveexpressionsemotive/non-emot.valence/activationemotionintensityemotionobjectsannota-tionsYACIS 354 mil./13 mil.
Japanese10 (language andculture based) ?
?
?/?
?
?
T,POS,L,DP,NER;Ren-CECps1.0 12,724/500 Chinese 8 (Yahoo!
news) ?
?
?/?
?
?
T,POS;MPQA 10,657/535 English none (no standard) ?
?
?/?
?
?
T,POS;KNB 4,186/249 Japanese none (no standard) ?
?
?/?
?
?
T,POS,L,DP,NER;Minato et al 1,191sent.
Japanese 8 (chosen subjectively)?
?
?/?
?
?
POS;Aman&Szpak.
5,205/173 English 6 (face recognition) ?
?
?/?
?
?
?Das&Bandyo.
12,149/123 Bengali 6 (face recognition) ?
?
?/?
?
?
?Wakamono 4773sen- Japanese 9 (face recognition + ?
?
?/?
?
?
?Kotoba tences 3 added subjectively)Mishne ?/815,494 English 132 (LiveJournal) ?
?
?/?
?
?
?tence contains at least one word from a slang lexiconand one word from an emotion lexicon, with addi-tional emotion class tags added per sentence.
Theemotion class set used for annotation was chosensubjectively, by applying the 6 class face recogni-tion standard and adding 3 classes of their choice.Mishne (2005) collected a corpus of English blogsfrom LiveJournal (http://www.livejournal.com/)blogs.
The corpus contains 815,494 blog posts,from which many are annotated with emotions(moods) by the blog authors themselves.
TheLiveJournal service offers an option for its users toannotate their mood while writing the blog.
Thelist of 132 moods include words like ?amused?, or?angry?.
The LiveJournal mood annotation standardoffers a rich vocabulary to describe the writer?smood.
However, this richness has been consideredtroublesome to generalize the data in a meaningfulmanner (Quan and Ren, 2010).Finally, Minato et al (2006) collected a 14,195word, 1,191 sentence corpus.
The corpus was a col-lection of sentence examples from a dictionary ofemotional expressions (Hiejima, 1995).
The dictio-nary was created for the need of Japanese languagelearners.
Differently to the dictionary applied in ourresearch (Nakamura, 1993), in Hiejima (1995) sen-tence examples were mostly written by the author ofthe dictionary himself.
The dictionary also does notpropose any coherent emotion class list, but ratherthe emotion concepts are chosen subjectively.
Al-though the corpus by Minato et al is the smallestof all mentioned above, its statistics is described indetail.
Therefore in this paper we use it as one of theJapanese emotion corpora to compare our work to.All of the above corpora were annotated manu-ally or semi-automatically.
In this research we per-formed the first attempt to annotate a large scale blogcorpus (YACIS) with affective information fully au-tomatically.
We did this with systems based on pos-itively evaluated affect annotation schema, perfor-mance, and standardized emotion class typology.3 Choice of Blog CorpusAlthough Japanese is a well recognized and de-scribed world language, there have been only fewlarge corpora for this language.
For example, Er-javec et al (2008) gathered a 400-million-word scaleWeb corpus JpWaC, or Baroni and Ueyama (2006)developed a medium-sized corpus of Japanese blogsjBlogs containing 62 million words.
However, bothresearch faced several problems, such as characterencoding, or web page metadata extraction, such asthe page title or author which differ between do-mains.
Apart from the above mentioned mediumsized corpora at present the largest Web based blogcorpus available for Japanese is YACIS or YetAnother Corpus of Internet Sentences.
We chosethis corpus for the annotation of affective informa-tion for several reasons.
It was collected automati-cally by Maciejewski et al (2010) from the pages ofAmeba blog service.
It contains 5.6 billion wordswithin 350 million sentences.
Maciejewski et alwere able to extract only pages containing Japaneseposts (pages with legal disclaimers or written in lan-guages other than Japanese were omitted).
In theinitial phase they provided their crawler, optimizedto crawl only Ameba blog service, with 1000 links91Figure 1: The example of YACIS XML structure.Table 2: General Statistics of YACIS.# of web pages 12,938,606# of unique bloggers 60,658average # of pages/blogger 213.3# of pages with comments 6,421,577# of comments 50,560,024average # of comment/page 7.873# of words 5,600,597,095# of all sentences 354,288,529# of words per sentence (average) 15# of characters per sentence (average) 77taken from Google (response to one simple query:?site:ameblo.jp?).
They saved all pages to disk asraw HTML files (each page in a separate file) andafterward extracted all the posts and comments anddivided them into sentences.
The original structure(blog post and comments) was preserved, thanks towhich semantic relations between posts and com-ments were retained.
The blog service from whichthe corpus was extracted (Ameba) is encoded by de-fault in Unicode, thus there was no problem withcharacter encoding.
It also has a clear and stableHTML meta-structure, thanks to which they man-aged to extract metadata such as blog title and au-thor.
The corpus was first presented as an unanno-tated corpus.
Recently Ptaszynski et al (2012b) an-notated it with syntactic information, such as POS,dependency structure or named entity recognition.An example of the original blog structure in XMLis represented in Figure 1.
Some statistics about thecorpus are represented in Table 2.4 Affective Information Annotation ToolsEmotive Expression Dictionary (Nakamura, 1993)is a collection of over two thousand expressions de-scribing emotional states collected manually from awide range of literature.
It is not a tool per se, butFigure 2: Output examples for ML-Ask and CAO.Table 3: Distribution of separate expressions across emo-tion classes in Nakamura?s dictionary (overall 2100 ex.
).emotion nunber of emotion nunber ofclass expressions class expressionsdislike 532 fondness 197excitement 269 fear 147sadness 232 surprise 129joy 224 relief 106anger 199 shame 65sum 2100was converted into an emotive expression databaseby Ptaszynski et al (2009c).
Since YACIS is aJapanese language corpus, for the affect annotationwe needed the most appropriate lexicon for the lan-guage.
The dictionary, developed for over 20 yearsby Akira Nakamura, is a state-of-the art exampleof a hand-crafted emotive expression lexicon.
Italso proposes a classification of emotions that re-flects the Japanese culture: ki/yorokobi5 (joy),do?/ikari (anger), ai/aware (sorrow, sadness,gloom), fu/kowagari (fear), chi/haji (shame,shyness), ko?/suki (fondness), en/iya (dislike),ko?/takaburi (excitement), an/yasuragi (relief),and kyo?/odoroki (surprise).
All expressions in thedictionary are annotated with one emotion class ormore if applicable.
The distribution of expressionsacross all emotion classes is represented in Table 3.ML-Ask (Ptaszynski et al, 2009a; Ptaszynski et al,2009c) is a keyword-based language-dependent sys-tem for affect annotation on sentences in Japanese.It uses a two-step procedure: 1) specifying whetheran utterance is emotive, and 2) annotating the partic-ular emotion classes in utterances described as emo-tive.
The emotive sentences are detected on the ba-sis of emotemes, emotive features like: interjections,mimetic expressions, vulgar language, emoticons5Separation by ?/?
represents two possible readings of the character.92Table 4: Evaluation results of ML-Ask and CAO.emotive/ emotion 2D (valencenon-emotive classes and activation)ML-Ask 98.8% 73.4% 88.6%CAO 97.6% 80.2% 94.6%ML-Ask+CAO 100.0% 89.9% 97.5%and emotive markers.
The examples in Japaneseare respectively: sugee (great!
), wakuwaku (heartpounding), -yagaru (syntactic morpheme used inverb vulgarization), (?
?)
(emoticon expressing joy)and ?!
?, ????
(markers indicating emotive engage-ment).
Emotion class annotation is based on Naka-mura?s dictionary.
ML-Ask is also the only presentsystem for Japanese recognized to implement theidea of Contextual Valence Shifters (CVS) (Zaenenand Polanyi, 2005) (words and phrases like ?not?,or ?never?, which change the valence of an evalua-tive word).
The last distinguishable feature of ML-Ask is implementation of Russell?s two dimensionalaffect model (Russell, 1980), in which emotionsare represented in two dimensions: valence (posi-tive/negative) and activation (activated/deactivated).An example of negative-activated emotion couldbe ?anger?
; a positive-deactivated emotion is, e.g.,?relief?.
The mapping of Nakamura?s emotionclasses on Russell?s two dimensions was proved re-liable in several research (Ptaszynski et al, 2009b;Ptaszynski et al, 2009c; Ptaszynski et al, 2010b).With these settings ML-Ask detects emotive sen-tences with a high accuracy (90%) and annotates af-fect on utterances with a sufficiently high Precision(85.7%), but low Recall (54.7%).
Although low Re-call is a disadvantage, we assumed that in a corpusas big as YACIS there should still be plenty of data.CAO (Ptaszynski et al, 2010b) is a system foraffect analysis of Japanese emoticons, called kao-moji.
Emoticons are sets of symbols used to con-vey emotions in text-based online communication,such as blogs.
CAO extracts emoticons from in-put and determines specific emotions expressed bythem.
Firstly, it matches the input to a predeter-mined raw emoticon database (with over ten thou-sand emoticons).
The emoticons, which could not beestimated with this database are divided into seman-tic areas (representations of ?mouth?
or ?eyes?).
Theareas are automatically annotated according to theirTable 5: Statistics of emotive sentences.# of emotive sentences 233,591,502# of non-emotive sentence 120,408,023ratio (emotive/non-emotive) 1.94# of sentences containing emoteme class:- interjections 171,734,464- exclamative marks 89,626,215- emoticons 49,095,123- endearments 12,935,510- vulgarities 1,686,943ratio (emoteme classes in emotive sentence) 1.39co-occurrence in the database.
The performance ofCAO was evaluated as close to ideal (Ptaszynski etal., 2010b) (over 97%).
In this research we usedCAO as a supporting procedure in ML-Ask to im-prove the overall performance and add detailed in-formation about emoticons.5 Annotation Results and EvaluationIt is physically impossible to manually evaluate allannotations on the corpus6.
Therefore we appliedthree different types of evaluation.
First was basedon a sample of 1000 sentences randomly extractedfrom the corpus and annotated by laypeople.
In sec-ond we compared YACIS annotations to other emo-tion corpora.
The third evaluation was applicationbased and is be described in section 6.Evaluation of Affective Annotations: Firstly, weneeded to confirm the performance of affect anal-ysis systems on YACIS, since the performance isoften related to the type of test set used in evalu-ation.
ML-Ask was positively evaluated on sepa-rate sentences and on an online forum (Ptaszynskiet al, 2009c).
However, it was not yet evaluatedon blogs.
Moreover, the version of ML-Ask sup-ported by CAO has not been evaluated thoroughlyas well.
In the evaluation we used a test set cre-ated by Ptaszynski et al (2010b) for the evaluationof CAO.
It consists of thousand sentences randomlyextracted from YACIS and manually annotated withemotion classes by 42 layperson annotators in ananonymous survey.
There are 418 emotive and 582non-emotive sentences.
We compared the resultson those sentences for ML-Ask, CAO (described indetail by Ptaszynski et al (2010b)), and both sys-tems combined.
The results showing accuracy, cal-6Having one sec.
to evaluate one sentence, one evaluatorwould need 11.2 years to verify the whole corpus (354 mil.s.
).93Table 6: Emotion class annotations with percentage.emotionclass# ofsentences %emotionclass# ofsentences %joy 16,728,452 31% excitement 2,833,388 5%dislike 10,806,765 20% surprize 2,398,535 5%fondness 9,861,466 19% gloom 2,144,492 4%fear 3,308,288 6% anger 1,140,865 2%relief 3,104,774 6% shame 952,188 2%culated as a ratio of success to the overall numberof samples, are summarized in Table 4.
The perfor-mance of discrimination between emotive and non-emotive sentences of ML-Ask baseline was a high98.8%, which is much higher than in original eval-uation of ML-Ask (around 90%).
This could indi-cate that sentences with which the system was notable to deal with appear much less frequently onAmeblo.
As for CAO, it is capable of detecting thepresence of emoticons in a sentence, which is par-tially equivalent to detecting emotive sentences inML-Ask, since emoticons are one type of featuresdetermining sentence as emotive.
The performanceof CAO was also high, 97.6%.
This was due to thefact that grand majority of emotive sentences con-tained emoticons.
Finally, ML-Ask supported withCAO achieved remarkable 100% accuracy.
This wasa surprisingly good result, although it must be re-membered that the test sample contained only 1000sentences (less than 0.0003% of the whole corpus).Next we verified emotion class annotations on sen-tences.
The baseline of ML-Ask achieved slightlybetter results (73.4%) than in its primary evalua-tion (Ptaszynski et al, 2009c) (67% of balanced F-score with P=85.7% and R=54.7%).
CAO achieved80.2%.
Interestingly, this makes CAO a better affectanalysis system than ML-Ask.
However, the condi-tion is that a sentence must contain an emoticon.
Thebest result, close to 90%, was achieved by ML-Asksupported with CAO.
We also checked the resultswhen only the dimensions of valence and activationwere taken into account.
ML-Ask achieved 88.6%,CAO nearly 95%.
Support of CAO toML-Ask againresulted in the best score, 97.5%.Statistics of Affective Annotations: There werenearly twice as many emotive sentences than non-emotive (ratio 1.94).
This suggests that the cor-pus is biased in favor of emotive contents, whichcould be considered as a proof for the assumptionthat blogs make a good base for emotion related re-Table 7: Comparison of positive and negative sentencesbetween KNB and YACIS.positive negative ratioKNB* emotional 317 208 1.52attitudeopinion 489 289 1.69merit 449 264 1.70acceptation 125 41 3.05or rejectionevent 43 63 0.68sum 1,423 865 1.65YACIS** only 22,381,992 12,837,728 1.74only+mostly 23,753,762 13,605,514 1.75* p<.05, ** p<.01search.
When it comes to statistics of each emo-tive feature (emoteme), the most frequent class wereinterjections.
Second frequent was the exclamativemarks class, which includes punctuation marks sug-gesting emotive engagement (such as ?!
?, or ????
).Third frequent emoteme class was emoticons, fol-lowed by endearments.
As an interesting remark,emoteme class that was the least frequent were vul-garities.
As one possible interpretation of this re-sult we propose the following.
Blogs are socialspace, where people describe their experiences tobe read and commented by other people (friends,colleagues).
The use of vulgar language could dis-courage potential readers from further reading, mak-ing the blog less popular.
Next, we checked thestatistics of emotion classes annotated on emotivesentences.
The results are represented in Table 6.The most frequent emotions were joy (31%), dislike(20%) and fondness (19%), which covered over 70%of all annotations.
However, it could happen thatthe number of expressions included in each emotionclass database influenced the number of annotations(database containing many expressions has higherprobability to gather more annotations).
Thereforewe verified if there was a correlation between thenumber of annotations and the number of emotiveexpressions in each emotion class database.
Theverification was based on Spearman?s rank corre-lation test between the two sets of numbers.
Thetest revealed no statistically significant correlationbetween the two types of data, with ?=0.38.Comparison with Other Emotion Corpora:Firstly, we compared YACIS with KNB.
The KNBcorpus was annotated mostly for the need of sen-timent analysis and therefore does not contain any94Table 8: Comparison of number of emotive expressionsin three different corpora including ratio within this set ofemotions and results of Spearman?s rank correlation test.Minato et al YACIS Nakamuradislike 355 (26%) 14,184,697 (23%) 532 (32%)joy 295 (21%) 22,100,500 (36%) 224 (13%)fondness 205 (15%) 13,817,116 (22%) 197 (12%)sorrow 205 (15%) 2,881,166 (5%) 232 (14%)anger 160 (12%) 1,564,059 (3%) 199 (12%)fear 145 (10%) 4,496,250 (7%) 147 (9%)surprise 25 (2%) 3,108,017 (5%) 129 (8%)Minato et al Minato et al YACIS andand Nakamura and YACIS NakamuraSpearman?s ?
0.88 0.63 0.25information on specific emotion classes.
However,it is annotated with emotion valence for differentcategories valence is expressed in Japanese, suchas emotional attitude (e.g., ?to feel sad about X?
[NEG], ?to like X?
[POS]), opinion (e.g., ?X is won-derful?
[POS]), or positive/negative event (e.g., ?Xbroke down?
[NEG], ?X was awarded?
[POS]).
Wecompared the ratios of sentences expressing posi-tive to negative valence.
The comparison was madefor all KNB valence categories separately and as asum.
In our research we do not make additional sub-categorization of valence types, but used in the com-parison ratios of sentences in which the expressedemotions were of only positive/negative valence andincluding the sentences which were mostly (in ma-jority) positive/negative.
The comparison is pre-sented in table 7.
In KNB for all valence categoriesexcept one the ratio of positive to negative sentenceswas biased in favor of positive sentences.
Moreover,for most cases, including the ratio taken from thesums of sentences, the ratio was similar to the one inYACIS (around 1.7).
Although the numbers of com-pared sentences differ greatly, the fact that the ratioremains similar across the two different corpora sug-gests that the Japanese express in blogs more posi-tive than negative emotions.Next, we compared the corpus created by Minatoet al (2006).
This corpus was prepared on the ba-sis of an emotive expression dictionary.
Thereforewe compared its statistics not only to YACIS, butalso to the emotive lexicon used in our research (seesection 4 for details).
Emotion classes used in Mi-nato et al differ slightly to those used in our re-search (YACIS and Nakamura?s dictionary).
Forexample, they use class name ?hate?
to describewhat in YACIS is called ?dislike?.
Moreover, theyhave no classes such as excitement, relief or shame.To make the comparison possible we used only theemotion classes appearing in both cases and unifiedall class names.
The results are summarized in Ta-ble 8.
There was no correlation between YACIS andNakamura (?=0.25), which confirms the results cal-culated in previous paragraph.
A medium correla-tion was observed between YACIS and Minato et al(?=0.63).
Finally, a strong correlation was observedbetween Minato et al and Nakamura (?=0.88),which is the most interesting observation.
Both Mi-nato et al and Nakamura are in fact dictionaries ofemotive expressions.
However, the dictionaries werecollected in different times (difference of about 20years), by people with different background (lexi-cographer vs. language teacher), based on differ-ent data (literature vs. conversation) assumptionsand goals (creating a lexicon vs. Japanese languageteaching).
The only similarity is in the methodol-ogy.
In both cases the dictionary authors collectedexpressions considered to be emotion-related.
Thefact that they correlate so strongly suggests that forthe compared emotion classes there could be a ten-dency in language to create more expressions to de-scribe some emotions rather than the others (dislike,joy and fondness are often some of the most frequentemotion classes).
This phenomenon needs to be ver-ified more thoroughly in the future.6 Applications6.1 Extraction of Evaluation DatasetsIn evaluation of sentiment and affect analysis sys-tems it is very important to provide a statisticallyreliable random sample of sentences or documentsas a test set (to be further annotated by laypeople).The larger is the source, the more statistically reli-able is the test set.
Since YACIS contains 354 mil.sentences in 13 mil.
documents, it can be consideredsufficiently reliable for the task of test set extraction,as probability of extracting twice the same sentenceis close to zero.
Ptaszynski et al (2010b) alreadyused YACIS to randomly extract a 1000 sentencesample and used it in their evaluation of emoticonanalysis system.
The sample was also used in thisresearch and is described in more detail in section 5.956.2 Generation of Emotion Object OntologyOne of the applications of large corpora is toextract from them smaller sub-corpora for specifiedtasks.
Ptaszynski et al (2012a) applied YACISfor their task of generating an robust emotionobject ontology.
They used cross-reference ofannotations of emotional information describedin this paper and syntactic annotations doneby Ptaszynski et al (2012b) to extract onlysentences in which expression of emotion wasproceeded by its cause, like in the example below.Kanojo ni furareta kara kanashii...Girlfriend DAT dump PAS CAUS sad ...I?m sad becausemy girlfriend dumped me...The example can be analyzed in the following way.Emotive expression (kanashii, ?sad?)
is related withthe sentence contents (Kanojo ni furareta, ?mygirlfriend dumped me?)
with a causality morpheme(kara, ?because?).
In such situation the sentencecontents represent the object of emotion.
This canbe generalized to the following meta-structure,OE CAUS XE ,where OE=[Emotion object], CAUS=[causalform], and XE=[expression of emotion].The cause phrases were cleaned of irrelevantwords like stop words to leave only the objectphrases.
The evaluation showed they were able toextract nearly 20 mil.
object phrases, from which80% was extracted correctly with a reliable signifi-cance.
Thanks to rich annotations on YACIS corpusthe ontology included such features as emotion class(joy, anger, etc.
), dimensions (valence/activation),POS or semantic categories (hypernyms, etc.
).6.3 Retrieval of Moral Consequence of ActionsThird application of the YACIS corpus annotatedwith affect- and sentiment-related information hasbeen in a novel research on retrieval of moral con-sequences of actions, first proposed by Rzepka andAraki (2005) and recently developed by Komuda etal.
(2010)7.
The moral consequence retrieval agentwas based on the idea of Wisdom of Crowd.
Inparticular Komuda et al (2010) used a Web-mining7See also a mention in Scientific American, by Anderson andAnderson (2010).technique to gather consequences of actions apply-ing causality relations, like in the research describedin section 6.2, but with a reversed algorithm andlexicon containing not only emotional but also eth-ical notions.
They cross-referenced emotional andethical information about a certain phrase (such as?To kill a person.?)
to obtain statistical probabilityfor emotional (?feeling sad?, ?being in joy?, etc.
)and ethical consequences (?being punished?, ?beingpraised?, etc.).
Initially, the moral agent was basedon the whole Internet contents.
However, multiplequeries to search engine APIs made by the agentcaused constant blocking of IP address an in effecthindered the development of the agent.The agent was tested on over 100 ethically-significant real world problems, such as ?killing aman?, ?stealing money?, ?bribing someone?, ?help-ing people?
or ?saving environment?.
In result 86%of recognitions were correct.
Some examples of theresults are presented in the Appendix on the end ofthis paper.7 ConclusionsWe performed automatic annotation of a five-billion-word corpus of Japanese blogs with informa-tion on affect and sentiment.
A survey in emotionblog corpora showed there has been no large scaleemotion corpus available for the Japanese language.We chose YACIS, a large-scale blog corpus andannotated it using two systems for affect analysisfor word- and sentence-level affect analysis and foranalysis of emoticons.
The annotated informationincluded affective features like sentence subjectivity(emotive/non-emotive) or emotion classes (joy, sad-ness, etc.
), useful in affect analysis and informationon sentence valence/polarity (positive/negative) use-ful in sentiment analysis obtained as generalizationsof those features on a 2-dimensional model of af-fect.
We evaluated the annotations in several ways.Firstly, on a test set of thousand sentences extractedand evaluated by over forty respondents.
Secondly,we compared the statistics of annotations to otherexisting emotion corpora.
Finally, we showed sev-eral tasks the corpus has already been applied in,such as generation of emotion object ontology or re-trieval of emotional and moral consequences of ac-tions.96AcknowledgmentsThis research was supported by (JSPS) KAKENHIGrant-in-Aid for JSPS Fellows (Project Number: 22-00358).ReferencesAhmed Abbasi and Hsinchun Chen.
?Affect Intensity Analysisof Dark Web Forums?, Intelligence and Security Informatics2007, pp.
282-288, 2007Saima Aman and Stan Szpakowicz.
2007.
?Identifying Ex-pressions of Emotion in Text?.
In Proceedings of the 10thInternational Conference on Text, Speech, and Dialogue(TSD-2007), Lecture Notes in Computer Science (LNCS),Springer-Verlag.Michael Anderson and Susan Leigh Anderson.
2010.
?Robot beGood?, Scientific American, October, pp.
72-77.Dipankar Das, Sivaji Bandyopadhyay, ?Labeling Emotion inBengali Blog Corpus ?
A Fine Grained Tagging at SentenceLevel?, Proceedings of the 8thWorkshop on Asian LanguageResources, pages 47?55, 2010.Marco Baroni, Silvia Bernardini, Adriano Ferraresi, ErosZanchetta.
2008.
?The WaCky Wide Web: A Collectionof Very Large Linguistically Processed Web-Crawled Cor-pora?, Kluwer Academic Publishers, Netherlands.Marco Baroni and Motoko Ueyama.
2006.
?Building General-and Special-Purpose Corpora by Web Crawling?, In Pro-ceedings of the 13th NIJL International Symposium onLanguage Corpora: Their Compilation and Application,www.tokuteicorpus.jp/result/pdf/2006 004.pdfJu?rgen Broschart.
1997.
?Why Tongan does it differently: Cate-gorial Distinctions in a Language without Nouns and Verbs.
?Linguistic Typology, Vol.
1, No.
2, pp.
123-165.Eugene Charniak, Don Blaheta, Niyu Ge, Keith Hall, John Haleand Mark Johnson.
2000.
?BLLIP 1987-89 WSJ CorpusRelease 1?, Linguistic Data Consortium, Philadelphia,http://www.ldc.upenn.edu/Catalog/CatalogEntry.jsp?catalogId=LDC2000T43Paul Ekman.
1992.
?An Argument for Basic Emotions?.
Cogni-tion and Emotion, Vol.
6, pp.
169-200.Irena Srdanovic Erjavec, Tomaz Erjavec and Adam Kilgarriff.2008.
?A web corpus and word sketches for Japanese?, Infor-mation and Media Technologies, Vol.
3, No.
3, pp.529-551.Katarzyna G?owin?ska and Adam Przepio?rkowski.
2010.
?TheDesign of Syntactic Annotation Levels in the National Cor-pus of Polish?, In Proceedings of LREC 2010.Peter Halacsy, Andras Kornai, Laszlo Nemeth, Andras Rung,Istvan Szakadat and Vikto Tron.
2004.
?Creating open lan-guage resources for Hungarian?.
In Proceedings of theLREC, Lisbon, Portugal.Chikara Hashimoto, Sadao Kurohashi, Daisuke Kawahara,Keiji Shinzato andMasaaki Nagata.
2011.
?Construction of aBlog Corpus with Syntactic, Anaphoric, and Sentiment An-notations?
[in Japanese], Journal of Natural Language Pro-cessing, Vol 18, No.
2, pp.
175-201.Ichiro Hiejima.
1995.
A short dictionary of feelings and emo-tions in English and Japanese, Tokyodo Shuppan.Paul J. Hopper and Sandra A. Thompson.
1985.
?The Iconic-ity of the Universal Categories ?Noun?
and ?Verbs??.
In Ty-pological Studies in Language: Iconicity and Syntax.
JohnHaiman (ed.
), Vol.
6, pp.
151-183, Amsterdam: John Ben-jamins Publishing Company.Daisuke Kawahara and Sadao Kurohashi.
2006.
?A Fully-Lexicalized Probabilistic Model for Japanese Syntactic andCase Structure Analysis?, Proceedings of the Human Lan-guage Technology Conference of the North American Chap-ter of the ACL, pp.
176-183.Radoslaw Komuda, Michal Ptaszynski, Yoshio Momouchi,Rafal Rzepka, and Kenji Araki.
2010.
?Machine Moral De-velopment: Moral Reasoning Agent Based on Wisdom ofWeb-Crowd and Emotions?, Int.
Journal of ComputationalLinguistics Research, Vol.
1 , Issue 3, pp.
155-163.Taku Kudo and Hideto Kazawa.
2009.
?Japanese Web N-gramVersion 1?, Linguistic Data Consortium, Philadelphia,http://www.ldc.upenn.edu/Catalog/CatalogEntry.jsp?catalogId=LDC2009T08Vinci Liu and James R. Curran.
2006.
?Web Text Corpus forNatural Language Processing?, In Proceedings of the 11thMeeting of the European Chapter of the Association forComputational Linguistics (EACL), pp.
233-240.Maciejewski, J., Ptaszynski, M., Dybala, P. 2010.
?Developinga Large-Scale Corpus for Natural Language Processing andEmotion Processing Research in Japanese?, In Proceedingsof the International Workshop on Modern Science and Tech-nology (IWMST), pp.
192-195.Kazuyuki Matsumoto, Yusuke Konishi, Hidemichi Sayama,Fuji Ren.
2011.
?Analysis of Wakamono Kotoba EmotionCorpus and Its Application in Emotion Estimation?, Interna-tional Journal of Advanced Intelligence, Vol.3,No.1,pp.1-24.Junko Minato, David B. Bracewell, Fuji Ren and ShingoKuroiwa.
2006.
?Statistical Analysis of a Japanese EmotionCorpus for Natural Language Processing?, LNCS 4114.Gilad Mishne.
2005.
?Experiments with Mood Classification inBlog Posts?.
In The 1st Workshop on Stylistic Analysis ofText for Information Access, at SIGIR 2005, August 2005.Akira Nakamura.
1993.
?Kanjo hyogen jiten?
[Dictionary ofEmotive Expressions] (in Japanese), Tokyodo Publishing,Tokyo, 1993.Jan Pomika?lek, Pavel Rychly?
and Adam Kilgarriff.
2009.
?Scal-ing to Billion-plusWord Corpora?, In Advances in Computa-tional Linguistics, Research in Computing Science, Vol.
41,pp.
3-14.Michal Ptaszynski, Pawel Dybala, Wenhan Shi, Rafal Rzepkaand Kenji Araki.
2009.
?A System for Affect Analysis of Ut-terances in Japanese Supported with Web Mining?, Journalof Japan Society for Fuzzy Theory and Intelligent Informat-ics, Vol.
21, No.
2, pp.
30-49 (194-213).Michal Ptaszynski, Pawel Dybala, Wenhan Shi, Rafal Rzepkaand Kenji Araki.
2009.
?Towards Context Aware EmotionalIntelligence in Machines: Computing Contextual Appro-priateness of Affective States?.
In Proceedings of Twenty-first International Joint Conference on Artificial Intelligence(IJCAI-09), Pasadena, California, USA, pp.
1469-1474.Michal Ptaszynski, Pawel Dybala, Rafal Rzepka and KenjiAraki.
2009.
?Affecting Corpora: Experiments with Au-tomatic Affect Annotation System - A Case Study of97the 2channel Forum -?, In Proceedings of the Conferenceof the Pacific Association for Computational Linguistics(PACLING-09), pp.
223-228.Michal Ptaszynski, Rafal Rzepka and Kenji Araki.
2010a.
?Onthe Need for Context Processing in Affective Computing?,In Proceedings of Fuzzy System Symposium (FSS2010), Or-ganized Session on Emotions, September 13-15.Michal Ptaszynski, Jacek Maciejewski, Pawel Dybala, RafalRzepka and Kenji Araki.
2010b.
?CAO: Fully AutomaticEmoticon Analysis System?, In Proc.
of the 24th AAAI Con-ference on Artificial Intelligence (AAAI-10), pp.
1026-1032.Michal Ptaszynski, Rafal Rzepka, Kenji Araki and Yoshio Mo-mouchi.
2012a.
?A Robust Ontology of Emotion Objects?, InProceedings of The Eighteenth Annual Meeting of The Asso-ciation for Natural Language Processing (NLP-2012), pp.719-722.Michal Ptaszynski, Rafal Rzepka, Kenji Araki and Yoshio Mo-mouchi.
2012b.
?Annotating Syntactic Information on 5.5Billion Word Corpus of Japanese Blogs?, In Proceedingsof The 18th Annual Meeting of The Association for NaturalLanguage Processing (NLP-2012), pp.
385-388.Changqin Quan and Fuji Ren.
2010.
?A blog emotion corpusfor emotional expression analysis in Chinese?, ComputerSpeech & Language, Vol.
24, Issue 4, pp.
726-749.Rafal Rzepka, Kenji Araki.
2005.
?What Statistics Could Dofor Ethics?
- The Idea of Common Sense Processing BasedSafety Valve?, AAAI Fall Symposium on Machine Ethics,Technical Report FS-05-06, pp.
85-87.James A. Russell.
1980.
?A circumplex model of affect?.
J. ofPersonality and Social Psychology, Vol.
39, No.
6, pp.
1161-1178.Peter D. Turney and Michael L. Littman.
2002.
?UnsupervisedLearning of Semantic Orientation from a Hundred-Billion-Word Corpus?, National Research Council, Institute for In-formation Technology, Technical Report ERB-1094.
(NRC#44929).Masao Utiyama and Hitoshi Isahara.
2003.
?Reliable Mea-sures for Aligning Japanese-English News Articles and Sen-tences?.
ACL-2003, pp.
72-79.Janyce Wiebe, Theresa Wilson and Claire Cardie.
2005.
?An-notating expressions of opinions and emotions in language?.Language Resources and Evaluation, Vol.
39, Issue 2-3, pp.165-210.Theresa Wilson and Janyce Wiebe.
2005.
?Annotating Attribu-tions and Private States?, In Proceedings of the ACL Work-shop on Frontiers in Corpus Annotation II, pp.
53-60.Annie Zaenen and Livia Polanyi.
2006.
?Contextual ValenceShifters?.
In Computing Attitude and Affect in Text, J. G.Shanahan, Y. Qu, J. Wiebe (eds.
), Springer Verlag, Dor-drecht, The Netherlands, pp.
1-10.Appendix.
Examples of emotional andethical consequence retrieval.SUCCESS CASESemotionalconseq.
results scoreethicalconseq.
results score?To hurt somebody.
?anger 13.01/54.1 0.24 penalty/ 4.01/7.1 0.565fear 12.01/54.1 0.22 punishmentsadness 11.01/54.1 0.2?To kill one?s own mother.
?sadness 9.01/35.1 0.26 penalty/ 5.01/5.1 0.982surprise 6.01/35.1 0.17 punishmentanger 5.01/35.1 0.14?To steal an apple.
?surprise 2.01/6.1 0.33 reprimand/ 3.01/3.1 0.971anger 2.01/6.1 0.33 scold?To steal money.
?anger 3.01/9.1 0.33 penalty/punish.3.01/6.1 0.493sadness 2.01/9.1 0.22 reprimand/sco.
2.01/6.1 0.330?To kill an animal.
?dislike 7.01/23.1 0.3 penalty/ 36.01/45.1 0.798sadness 5.01/23.1 0.22 punishment?To drive after drinking.
?fear 6.01/19.1 0.31 penalty/punish.24.01/36.1 0.665?To cause a war.
?dislike 7.01/15.1 0.46 illegal 2.01/3.1 0.648fear 3.01/15.1 0.2?To stop a war.
?joy 6.01/13.1 0.46 forgiven 1.01/1.1 0.918surprise 2.01/13.1 0.15?To prostitute oneself.
?anger 6.01/19.1 0.31 illegal 12.01/19.1 0.629sadness 5.01/19.1 0.26?To have an affair.
?sadness 10,01/35.1 0.29 penalty/punish.8.01/11.1 0.722anger 9.01/35.1 0.26INCONSISTENCY BETWEEN EMOTIONS AND ETHICS?To kill a president.
?joy 2.01/4.1 0.49 penalty/ 2.01/2.1 0.957likeness 1.01/4.1 0.25 punishment?To kill a criminal.
?joy 8.01/39.1 0.2 penalty/ 556/561 0.991excite 8.01/39.1 0.2 punishmentanger 7.01/39.1 0.18CONTEXT DEPENDENT?To act violently.
?anger 4.01/11.1 0.36 penalty/punish.1.01/2.1 0.481fear 2.01/11.1 0.18 agreement 1.01/2.1 0.481NO ETHICAL CONSEQUENCES?Sky is blue.
?joy 51.01/110,1 0.46 none 0 0sadness 21.01/110,1 0.1998
