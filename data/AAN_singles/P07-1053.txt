Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 416?423,Prague, Czech Republic, June 2007. c?2007 Association for Computational LinguisticsOpinion Mining Using Econometrics: A Case Study on Reputation SystemsAnindya Ghose Panagiotis G. IpeirotisDepartment of Information, Operations, and Management SciencesLeonard N. Stern School of Business, New York University{aghose,panos,arun}@stern.nyu.eduArun SundararajanAbstractDeriving the polarity and strength of opinionsis an important research topic, attracting sig-nificant attention over the last few years.
Inthis work, to measure the strength and po-larity of an opinion, we consider the eco-nomic context in which the opinion is eval-uated, instead of using human annotators orlinguistic resources.
We rely on the fact thattext in on-line systems influences the behav-ior of humans and this effect can be observedusing some easy-to-measure economic vari-ables, such as revenues or product prices.
Byreversing the logic, we infer the semantic ori-entation and strength of an opinion by tracingthe changes in the associated economic vari-able.
In effect, we use econometrics to iden-tify the ?economic value of text?
and assign a?dollar value?
to each opinion phrase, measur-ing sentiment effectively and without the needfor manual labeling.
We argue that by inter-preting opinions using econometrics, we havethe first objective, quantifiable, and context-sensitive evaluation of opinions.
We make thediscussion concrete by presenting results onthe reputation system of Amazon.com.
Weshow that user feedback affects the pricingpower of merchants and by measuring theirpricing power we can infer the polarity andstrength of the underlying feedback postings.1 IntroductionA significant number of websites today allow users topost articles where they express opinions about prod-ucts, firms, people, and so on.
For example, userson Amazom.com post reviews about products theybought and users on eBay.com post feedback describ-ing their experiences with sellers.
The goal of opinionmining systems is to identify such pieces of the textthat express opinions (Breck et al, 2007; Ko?nig andBrill, 2006) and then measure the polarity and strengthof the expressed opinions.
While intuitively the taskseems straightforward, there are multiple challengesinvolved.?
What makes an opinion positive or negative?
Isthere an objective measure for this task??
How can we rank opinions according to theirstrength?
Can we define an objective measurefor ranking opinions??
How does the context change the polarity andstrength of an opinion and how can we take thecontext into consideration?To evaluate the polarity and strength of opinions,most of the existing approaches rely either on train-ing from human-annotated data (Hatzivassiloglou andMcKeown, 1997), or use linguistic resources (Hu andLiu, 2004; Kim and Hovy, 2004) like WordNet, orrely on co-occurrence statistics (Turney, 2002) be-tween words that are unambiguously positive (e.g.,?excellent?)
and unambiguously negative (e.g., ?hor-rible?).
Finally, other approaches rely on reviews withnumeric ratings from websites (Pang and Lee, 2002;Dave et al, 2003; Pang and Lee, 2004; Cui et al,2006) and train (semi-)supervised learning algorithmsto classify reviews as positive or negative, or in morefine-grained scales (Pang and Lee, 2005; Wilson et al,2006).
Implicitly, the supervised learning techniquesassume that numeric ratings fully encapsulate the sen-timent of the review.416In this paper, we take a different approach and in-stead consider the economic context in which an opin-ion is evaluated.
We observe that the text in on-linesystems influence the behavior of the readers.
Thiseffect can be measured by observing some easy-to-measure economic variable, such as product prices.For instance, online merchants on eBay with ?posi-tive?
feedback can sell products for higher prices thancompetitors with ?negative?
evaluations.
Therefore,each of these (positive or negative) evaluations hasa (positive or negative) effect on the prices that themerchant can charge.
For example, everything elsebeing equal, a seller with ?speedy?
delivery may beable to charge $10 more than a seller with ?slow?
de-livery.
Using this information, we can conclude that?speedy?
is better than ?slow?
when applied to ?deliv-ery?
and their difference is $10.
Thus, we can infer thesemantic orientation and the strength of an evaluationfrom the changes in the observed economic variable.Following this idea, we use techniques from econo-metrics to identify the ?economic value of text?
andassign a ?dollar value?
to each text snippet, measuringsentiment strength and polarity effectively and with-out the need for labeling or any other resource.We argue that by interpreting opinions within aneconometric framework, we have the first objectiveand context-sensitive evaluation of opinions.
Forexample, consider the comment ?good packaging,?posted by a buyer to evaluate a merchant.
Thiscomment would have been considered unambiguouslypositive by the existing opinion mining systems.
Weobserved, though, that within electronic markets, suchas eBay, a posting that contains the words ?good pack-aging?
has actually negative effect on the power of amerchant to charge higher prices.
This surprising ef-fect reflects the nature of the comments in online mar-ketplaces: buyers tend to use superlatives and highlyenthusiastic language to praise a good merchant, anda lukewarm ?good packaging?
is interpreted as neg-ative.
By introducing the econometric interpretationof opinions we can effortlessly capture such challeng-ing scenarios, something that is impossible to achievewith the existing approaches.We focus our paper on reputation systems in elec-tronic markets and we examine the effect of opinionson the pricing power of merchants in the marketplaceof Amazon.com.
(We discuss more applications inSection 7.)
We demonstrate the value of our techniqueusing a dataset with 9,500 transactions that took placeover 180 days.
We show that textual feedback affectsthe power of merchants to charge higher prices thanthe competition, for the same product, and still make asale.
We then reverse the logic and determine the con-tribution of each comment in the pricing power of amerchant.
Thus, we discover the polarity and strengthof each evaluation without the need for human anno-tation or any other form of linguistic resource.The structure of the rest of the paper is as fol-lows.
Section 2 gives the basic background on rep-utation systems.
Section 3 describes our methodol-ogy for constructing the data set that we use in ourexperiments.
Section 4 shows how we combine estab-lished techniques from econometrics with text miningtechniques to identify the strength and polarity of theposted feedback evaluations.
Section 5 presents theexperimental evaluations of our techniques.
Finally,Section 6 discusses related work and Section 7 dis-cusses further applications and concludes the paper.2 Reputation Systems and Price PremiumsWhen buyers purchase products in an electronic mar-ket, they assess and pay not only for the product theywish to purchase but for a set of fulfillment character-istics as well, e.g., packaging, delivery, and the extentto which the product description matches the actualproduct.
Electronic markets rely on reputation sys-tems to ensure the quality of these characteristics foreach merchant, and the importance of such systemsis widely recognized in the literature (Resnick et al,2000; Dellarocas, 2003).
Typically, merchants?
rep-utation in electronic markets is encoded by a ?repu-tation profile?
that includes: (a) the number of pasttransactions for the merchant, (b) a summary of nu-meric ratings from buyers who have completed trans-actions with the seller, and (c) a chronological list oftextual feedback provided by these buyers.Studies of online reputation, thus far, base a mer-chant?s reputation on the numeric rating that charac-terizes the seller (e.g., average number of stars andnumber of completed transactions) (Melnik and Alm,2002).
The general conclusion of these studies showthat merchants with higher (numeric) reputation cancharge higher prices than the competition, for thesame products, and still manage to make a sale.
Thisprice premium that the merchants can command overthe competition is a measure of their reputation.Definition 2.1 Consider a set of merchants s1, .
.
.
, snselling a product for prices p1, .
.
.
, pn.
If si makes417Figure 1: A set of merchants on Amazon.com sellingan identical product for different pricesthe sale for price pi, then si commands a price pre-mium equal to pi ?
pj over sj and a relative pricepremium equal to pi?pjpi .
Hence, a transaction that in-volves n competing merchants generates n ?
1 pricepremiums.1 The average price premium for the trans-action is?j 6=i(pi?pj)n?1 and the average relative pricepremium is?j 6=i(pi?pj)pi(n?1) .
2Example 2.1 Consider the case in Figure 1 wherethree merchants sell the same product for $631.95,$632.26, and $637.05, respectively.
If GameHog sellsthe product, then the price premium against XP Pass-port is $4.79 (= $637.05 ?
$632.26) and against themerchant BuyPCsoft is $5.10.
The relative price pre-mium is 0.75% and 0.8%, respectively.
Similarly, theaverage price premium for this transaction is $4.95and the average relative price premium 0.78%.
2Different sellers in these markets derive their repu-tation from different characteristics: some sellers havea reputation for fast delivery, while some others havea reputation of having the lowest price among theirpeers.
Similarly, while some sellers are praised fortheir packaging in the feedback, others get good com-ments for selling high-quality goods but are criticizedfor being rather slow with shipping.
Even though pre-vious studies have established the positive correlationbetween higher (numeric) reputation and higher pricepremiums, they ignored completely the role of the tex-tual feedback and, in turn, the multi-dimensional na-ture of reputation in electronic markets.
We show thatthe textual feedback adds significant additional valueto the numerical scores, and affects the pricing powerof the merchants.1As an alternative definition we can ignore the negative pricepremiums.
The experimental results are similar for both versions.3 DataWe compiled a data set using software resellers frompublicly available information on software productlistings at Amazon.com.
Our data set includes 280individual software titles.
The sellers?
reputation mat-ters when selling identical goods, and the price varia-tion observed can be attributed primarily to variationin the merchant?s reputation.
We collected the data us-ing Amazon Web Services over a period of 180 days,between October 2004 and March 2005.
We describebelow the two categories of data that we collected.Transaction Data: The first part of our data setcontains details of the transactions that took place onthe marketplace of Amazon.com for each of the soft-ware titles.
The Amazon Web Services associates aunique transaction ID for each unique product listedby a seller.
This transaction ID enables us to distin-guish between multiple or successive listings of iden-tical products sold by the same merchant.
Keepingwith the methodology in prior research (Ghose et al,2006), we crawl the Amazon?s XML listings every 8hours and when a transaction ID associated with aparticular listing is removed, we infer that the listedproduct was successfully sold in the prior 8 hour win-dow.2 For each transaction that takes place, we keepthe price at which the product was sold and the mer-chant?s reputation at the time of the transaction (moreon this later).
Additionally, for each of the competinglistings for identical products, we keep the listed pricealong with the competitors reputation.
Using the col-lected data, we compute the price premium variablesfor each transaction3 using Definition 2.1.
Overall,our data set contains 1,078 merchants, 9,484 uniquetransactions and 107,922 price premiums (recall thateach transaction generates multiple price premiums).Reputation Data: The second part of our data setcontains the reputation history of each merchant thathad a (monitored) product for sale during our 180-daywindow.
Each of these merchants has a feedback pro-file, which consists of numerical scores and text-basedfeedback, posted by buyers.
We had an average of4,932 postings per merchant.
The numerical ratings2Amazon indicates that their seller listings remain on the siteindefinitely until they are sold and sellers can change the price ofthe product without altering the transaction ID.3Ideally, we would also include the tax and shipping costcharged by each merchant in the computation of the price pre-miums.
Unfortunately, we could not capture these costs usingour methodology.
Assuming that the fees for shipping and taxare independent of the merchants?
reputation, our analysis is notaffected.418are provided on a scale of one to five stars.
These rat-ings are averaged to provide an overall score to theseller.
Note that we collect all feedback (both numeri-cal and textual) associated with a seller over the entirelifetime of the seller and we reconstruct each seller?sexact feedback profile at the time of each transaction.4 Econometrics-based Opinion MiningIn this section, we describe how we combine econo-metric techniques with NLP techniques to derive thesemantic orientation and strength of the feedbackevaluations.
Section 4.1 describes how we structurethe textual feedback and Section 4.2 shows how weuse econometrics to estimate the polarity and strengthof the evaluations.4.1 Retrieving the Dimensions of ReputationWe characterize a merchant using a vector of reputa-tion dimensions X = (X1, X2, ..., Xn), representingits ability on each of n dimensions.
We assume thateach of these n dimensions is expressed by a noun,noun phrase, verb, or a verb phrase chosen from theset of all feedback postings, and that a merchant isevaluated on these n dimensions.
For example, di-mension 1 might be ?shipping?, dimension 2 mightbe ?packaging?
and so on.
In our model, each of thesedimensions is assigned a numerical score.
Of course,when posting textual feedback, buyers do not assignexplicit numeric scores to any dimension.
Rather, theyuse modifiers (typically adjectives or adverbs) to eval-uate the seller along each of these dimensions (we de-scribe how we assign numeric scores to each modifierin Section 4.2).
Once we have identified the set of alldimensions, we can then parse each of the feedbackpostings, associate a modifier with each dimension,and represent a feedback posting as an n-dimensionalvector ?
of modifiers.Example 4.1 Suppose dimension 1 is ?delivery,?
di-mension 2 is ?packaging,?
and dimension 3 is ?ser-vice.?
The feedback posting ?I was impressed by thespeedy delivery!
Great service!?
is then encoded as?1 = [speedy ,NULL, great ], while the posting ?Theitem arrived in awful packaging, and the delivery wasslow?
is encoded as ?2 = [slow , awful ,NULL].
2Let M = {NULL, ?1, ..., ?M} be the set of modi-fiers and consider a seller si with p postings in its rep-utation profile.
We denote with ?ijk ?M the modifierthat appears in the j-th posting and is used to assessthe k-th reputation dimension.
We then structure themerchant?s feedback as an n?
p matrix M(si) whoserows are the p encoded vectors of modifiers associatedwith the seller.
We construct M(si) as follows:1.
Retrieve the postings associated with a merchant.2.
Parse the postings to identify the dimensionsacross which the buyer evaluates a seller, keep-ing4 the nouns, noun phrases, verbs, and verbalphrases as reputation characteristics.5.3.
Retrieve adjectives and adverbs that refer to6 di-mensions (Step 2) and construct the ?
vectors.We have implemented this algorithm on the feed-back postings of each of our sellers.
Our analysisyields 151 unique dimensions, and a total of 142 mod-ifiers (note that the same modifier can be used to eval-uate multiple dimensions).4.2 Scoring the Dimensions of ReputationAs discussed above, the textual feedback profile ofmerchant si is encoded as a n ?
p matrix M(si); theelements of this matrix belong to the set of modifiersM.
In our case, we are interested in computing the?score?
a(?, d, j) that a modifier ?
?
M assigns tothe dimension d, when it appears in the j-th posting.Since buyers tend to read only the first few pagesof text-based feedback, we weight higher the influ-ence of recent text postings.
We model this by as-suming that K is the number of postings that appearon each page (K = 25 on Amazon.com), and that cis the probability of clicking on the ?Next?
link andmoving the next page of evaluations.7 This assigns aposting-specific weight rj = cbjK c/?pq=1 cbqK c forthe jth posting, where j is the rank of the posting, Kis the number of postings per page, and p is the totalnumber of postings for the given seller.
Then, we seta(?, d, j) = rj ?
a(?, d) where a(?, d) is the ?global?score that modifier ?
assigns to dimension d.Finally, since each reputation dimension has poten-tially a different weight, we use a weight vector w to4We eliminate all dimensions appearing in the profiles of lessthan 50 (out of 1078) merchants, since we cannot extract statisti-cally meaningful results for such sparse dimensions5The technique as described in this paper, considers words like?shipping?
and ?
delivery?
as separate dimensions, although theyrefer to the same ?real-life?
dimension.
We can use Latent Dirich-let Allocation (Blei et al, 2003) to reduce the number of dimen-sions, but this is outside the scope of this paper.6To associate the adjectives and adverbs with the correct di-mensions, we use the Collins HeadFinder capability of the Stan-ford NLP Parser.7We report only results for c = 0.5.
We conducted experi-ments other values of c as well and the results are similar.419weight the contribution of each reputation dimensionto the overall ?reputation score?
?
(si) of seller si:?
(si) = rT ?A(M(si)) ?w (1)where rT = [r1, r2, ...rp] is the vector of the posting-specific weights and A(M(i)) is a matrix that con-tains as element the score a(?j , dk) where M(si) con-tains the modifier ?j in the column of the dimen-sion dk.
If we model the buyers?
preferences as inde-pendently distributed along each dimension and eachmodifier score a(?, dk) also as an independent ran-dom variable, then the random variable ?
(si) is a sumof random variables.
Specifically, we have:?
(si) =M?j=1n?k=1(wk ?
a(?j , dk))R(?j , dk) (2)where R(?j , dk) is equal to the sum of the ri weightsacross all postings in which the modifier ?j modifiesdimension dk.
We can easily compute the R(?j , dk)values by simply counting appearances and weightingeach appearance using the definition of ri.The question is, of course, how to estimate the val-ues of wk ?
a(?j , dk), which determine the polarityand intensity of the modifier ?j modifying the dimen-sion dk.
For this, we observe that the appearance ofsuch modifier-dimension opinion phrases has an ef-fect on the price premiums that a merchant can charge.Hence, there is a correlation between the reputationscores ?(?)
of the merchants and the price premi-ums observed for each transaction.
To discover thelevel of association, we use regression.
Since we aredealing with panel data, we estimate ordinary-least-squares (OLS) regression with fixed effects (Greene,2002), where the dependent variable is the price pre-mium variable, and the independent variables are thereputation scores ?(?)
of the merchants, together witha few other control variables.
Generally, we estimatemodels of the form:PricePremiumij =?
?c ?Xcij + fij + ?ij+?t1 ??
(merchant)ij + ?t2 ??
(competitor)ij (3)where PricePremiumij is one of the variations of pricepremium as given in Definition 2.1 for a seller siand product j, ?c, ?t1, and ?t2 are the regressor co-efficients, Xc are the control variables, ?(?)
are thetext reputation scores (see Equation 1), fij denotes thefixed effects and ?
is the error term.
In Section 5, wegive the details about the control variables and the re-gression settings.Interestingly, if we expand the ?(?)
variables ac-cording to Equation 2, we can run the regression us-ing the modifier-dimension pairs as independent vari-ables, whose values are equal to the R(?j , dk) val-ues.
After running the regression, the coefficients as-signed to each modifier-dimension pair correspond tothe value wk ?
a(?j , dk) for each modifier-dimensionpair.
Therefore, we can easily estimate in economicterms the ?value?
of a particular modifier when usedto evaluate a particular dimension.5 Experimental EvaluationIn this section, we first present the experimental set-tings (Section 5.1), and then we describe the results ofour experimental evaluation (Section 5.2).5.1 Regression SettingsIn Equation 3 we presented the general form of theregression for estimating the scores a(?j , dk).
Sincewe want to eliminate the effect of any other factorsthat may influence the price premiums, we also use aset of control variables.
After all the control factorsare taken into consideration, the modifier scores re-flect the additional value of the text opinions.
Specifi-cally, we used as control variables the product?s priceon Amazon, the average star rating of the merchant,the number of merchant?s past transactions, and thenumber of sellers for the product.First, we ran OLS regressions with product-sellerfixed effects controlling for unobserved heterogene-ity across sellers and products.
These fixed effectscontrol for average product quality and differencesin seller characteristics.
We run multiple variationsof our model, using different versions of the ?pricepremium?
variable as listed in Definition 2.1.
Wealso tested variations where we include as indepen-dent variable not the individual reputation scores butthe difference ?(merchant)??(competitor).
All re-gressions yielded qualitatively similar results, so dueto space restrictions we only report results for the re-gressions that include all the control variables and allthe text variables; we report results using the pricepremium as the dependent variable.
Our regressionsin this setting contain 107,922 observations, and a to-tal of 547 independent variables.5.2 Experimental ResultsRecall of Extraction: The first step of our experi-mental evaluation is to examine whether the opinionextraction technique of Section 4.1 indeed capturesall the reputation characteristics expressed in the feed-420Dimension Human Recall Computer RecallProduct Condition 0.76 0.76Price 0.91 0.61Package 0.96 0.66Overall Experience 0.65 0.55Delivery Speed 0.96 0.92Item Description 0.22 0.43Product Satisfaction 0.68 0.58Problem Response 0.30 0.37Customer Service 0.57 0.50Average 0.66 0.60Table 1: The recall of our technique compared to therecall of the human annotatorsback (recall) and whether the dimensions that we cap-ture are accurate (precision).
To examine the recallquestion, we used two human annotators.
The annota-tors read a random sample of 1,000 feedback postings,and identified the reputation dimensions mentioned inthe text.
Then, they examined the extracted modifier-dimension pairs for each posting and marked whetherthe modifier-dimension pairs captured the identifiedreal reputation dimensions mentioned in the postingand which pairs were spurious, non-opinion phrases.Both annotators identified nine reputation dimen-sions (see Table 1).
Since the annotators did not agreein all annotations, we computed the average humanrecall hRecd = agreeddalld for each dimension d, whereagreedd is the number of postings for which both an-notators identified the reputation dimension d, andalld is the number of postings in which at least oneannotator identified the dimension d. Based on theannotations, we computed the recall of our algorithmagainst each annotator.
We report the average recallfor each dimension, together with the human recall inTable 1.
The recall of our technique is only slightlyinferior to the performance of humans, indicating thatthe technique of Section 4.1 extracts the majority ofthe posted evaluations.8Interestingly, precision is not an issue in our setting.In our framework, if an particular modifier-dimensionpair is just noise, then it is almost impossible to have astatistically significant correlation with the price pre-miums.
The noisy opinion phrases are statisticallyguaranteed to be filtered out by the regression.Estimating Polarity and Strength: In Table 2,8In the case of ?Item Description,?
where the computer recallwas higher than the human recall, our technique identified almostall the phrases of one annotator, but the other annotator had amore liberal interpretation of ?Item Description?
dimension andannotated significantly more postings with the dimension ?ItemDescription?
than the other annotator, thus decreasing the humanrecall.we present the modifier-dimension pairs (positive andnegative) that had the strongest ?dollar value?
andwere statistically significant across all regressions.
(Due to space issues, we cannot list the values for allpairs.)
These values reflect changes in the merchants?spricing power after taking their average numericalscore and level of experience into account, and alsohighlight the additional the value contained in text-based reputation.
The examples that we list here il-lustrate that our technique generates a natural rankingof the opinion phrases, inferring the strength of eachmodifier within the context in which this opinion isevaluated.
This holds true even for misspelled evalua-tions that would break existing techniques based onannotation or on resources like WordNet.
Further-more, these values reflect the context in which theopinion is evaluated.
For example, the pair good pack-aging has a dollar value of -$0.58.
Even though thisseems counterintuitive, it actually reflects the natureof an online marketplace where most of the positiveevaluations contain superlatives, and a mere ?good?is actually interpreted by the buyers as a lukewarm,slightly negative evaluation.
Existing techniques can-not capture such phenomena.Price Premiums vs. Ratings: One of the naturalcomparisons is to examine whether we could reachsimilar results by just using the average star rating as-sociated with each feedback posting to infer the scoreof each opinion phrase.
The underlying assumptionbehind using the ratings is that the review is per-fectly summarized by the star rating, and hence thetext plays mainly an explanatory role and carries noextra information, given the star rating.
For this, weexamined the R2 fit of the regression, with and with-out the use of the text variables.
Without the use oftext variables, the R2 was 0.35, while when using onlythe text-based regressors, the R2 fit increased to 0.63.This result clearly indicates that the actual text con-tains significantly more information than the ratings.We also experimented with predicting which mer-chant will make a sale, if they simultaneously sellthe same product, based on their listed prices and ontheir numeric and text reputation.
Our C4.5 classi-fier (Quinlan, 1992) takes a pair of merchants and de-cides which of the two will make a sale.
We used astraining set the transactions that took place in the firstfour months and as test set the transactions in the lasttwo months of our data set.
Table 3 summarizes theresults for different sets of features used.
The 55%421Modifier Dimension Dollar Value[wonderful experience] $5.86[outstanding seller] $5.76[excellant service] $5.27[lightning delivery] $4.84[highly recommended] $4.15[best seller] $3.80[perfectly packaged] $3.74[excellent condition] $3.53[excellent purchase] $3.22[excellent seller] $2.70[excellent communication] $2.38[perfect item] $1.92[terrific condition] $1.87[top quality] $1.67[awesome service] $1.05[A+++ seller] $1.03[great merchant] $0.93[friendly service] $0.81[easy service] $0.78[never received] -$7.56[defective product] -$6.82[horible experience] -$6.79[never sent] -$6.69[never recieved] -$5.29[bad experience] -$5.26[cancelled order] -$5.01[never responded] -$4.87[wrong product] -$4.39[not as advertised] -$3.93[poor packaging] -$2.92[late shipping] -$2.89[wrong item] -$2.50[not yet received] -$2.35[still waiting] -$2.25[wrong address] -$1.54[never buy] -$1.48Table 2: The highest scoring opinion phrases, as de-termined by the product wk ?
a(?j , dk).accuracy when using only prices as features indicatesthat customers rarely choose a product based solely onprice.
Rather, as indicated by the 74% accuracy, theyalso consider the reputation of the merchants.
How-ever, the real value of the postings relies on the textand not on the numeric ratings: the accuracy is 87%-89% when using the textual reputation variables.
Infact, text subsumes the numeric variables but not viceversa, as indicated by the results in Table 3.6 Related WorkTo the best of our knowledge, our work is the first touse economics for measuring the effect of opinionsand deriving their polarity and strength in an econo-metric manner.
A few papers in the past tried tocombine text analysis with economics (Das and Chen,2006; Lewitt and Syverson, 2005), but the text anal-ysis was limited to token counting and did not useFeatures Accuracy on Test SetPrice 55%Price + Numeric Reputation 74%Price + Numeric Reputation 89%+ Text ReputationPrice + Text Reputation 87%Table 3: Predicting the merchant who makes the sale.any NLP techniques.
The technique of Section 4.1is based on existing research in sentiment analysis.For instance, (Hatzivassiloglou and McKeown, 1997;Nigam and Hurst, 2004) use annotated data to create asupervised learning technique to identify the semanticorientation of adjectives.
We follow the approach byTurney (2002), who note that the semantic orientationof an adjective depends on the noun that it modifiesand suggest using adjective-noun or adverb-verb pairsto extract semantic orientation.
However, we do notrely on linguistic resources (Kamps and Marx, 2002)or on search engines (Turney and Littman, 2003) todetermine the semantic orientation, but rather rely oneconometrics for this task.
Hu and Liu (2004), whosestudy is the closest to our work, use WordNet to com-pute the semantic orientation of product evaluationsand try to summarize user reviews by extracting thepositive and negative evaluations of the different prod-uct features.
Similarly, Snyder and Barzilay (2007)decompose an opinion across several dimensions andcapture the sentiment across each dimension.
Otherwork in this area includes (Lee, 2004; Popescu andEtzioni, 2005) which uses text mining in the contextproduct reviews, but none uses the economic contextto evaluate the opinions.7 Conclusion and Further ApplicationsWe demonstrated the value of using econometricsfor extracting a quantitative interpretation of opin-ions.
Our technique, additionally, takes into con-sideration the context within which these opinionsare evaluated.
Our experimental results show thatour techniques can capture the pragmatic mean-ing of the expressed opinions using simple eco-nomic variables as a form of training data.
Thesource code with our implementation together withthe data set used in this paper are available fromhttp://economining.stern.nyu.edu.There are many other applications beyond reputa-tion systems.
For example, using sales rank data fromAmazon.com, we can examine the effect of productreviews on product sales and detect the weight that422customers put on different product features; further-more, we can discover how customer evaluations onindividual product features affect product sales andextract the pragmatic meaning of these evaluations.Another application is the analysis of the effect ofnews stories on stock prices: we can examine whatnews topics are important for the stock market andsee how the views of different opinion holders and thewording that they use can cause the market to moveup or down.
In a slightly different twist, we can ana-lyze news stories and blogs in conjunction with resultsfrom prediction markets and extract the pragmatic ef-fect of news and blogs on elections or other politicalevents.
Another research direction is to examine theeffect of summarizing product descriptions on prod-uct sales: short descriptions reduce the cognitive loadof consumers but increase their uncertainty about theunderlying product characteristics; a longer descrip-tion has the opposite effect.
The optimum descriptionlength is the one that balances both effects and maxi-mizes product sales.Similar approaches can improve the state of art inboth economics and computational linguistics.
In eco-nomics and in social sciences in general, most re-searchers handle textual data manually or with sim-plistic token counting techniques; in the worst casethey ignore text data altogether.
In computationallinguistics, researchers often rely on human annota-tors to generate training data, a laborious and error-prone task.
We believe that cross-fertilization of ideasbetween the fields of computational linguistics andeconometrics can be beneficial for both fields.AcknowledgmentsThe authors would like to thank Elena Filatova forthe useful discussions and the pointers to related lit-erature.
We also thank Sanjeev Dewan, Alok Gupta,Bin Gu, and seminar participants at Carnegie Mel-lon University, Columbia University, Microsoft Re-search, New York University, Polytechnic University,and University of Florida for their comments andfeedback.
We thank Rhong Zheng for assistance indata collection.
This work was partially supported bya Microsoft Live Labs Search Award, a Microsoft Vir-tual Earth Award, and by NSF grants IIS-0643847 andIIS-0643846.
Any opinions, findings, and conclusionsexpressed in this material are those of the authors anddo not necessarily reflect the views of the MicrosoftCorporation or of the National Science Foundation.ReferencesD.M.
Blei, A.Y.
Ng, and M.I.
Jordan.
2003.
Latent Dirichletallocation.
JMLR, 3:993?1022.E.
Breck, Y. Choi, and C. Cardie.
2007.
Identifying expressionsof opinion in context.
In IJCAI-07, pages 2683?2688.H.
Cui, V. Mittal, and M. Datar.
2006.
Comparative experi-ments on sentiment classification for online product reviews.In AAAI-2006.S.
Ranjan Das and M. Chen.
2006.
Yahoo!
for Amazon: Senti-ment extraction from small talk on the web.
Working Paper,Santa Clara University.K.
Dave, S. Lawrence, and D.M.
Pennock.
2003.
Mining thepeanut gallery: Opinion extraction and semantic classificationof product reviews.
In WWW12, pages 519?528.C.
Dellarocas.
2003.
The digitization of word-of-mouth: Promiseand challenges of online reputation mechanisms.
ManagementScience, 49(10):1407?1424.A.
Ghose, M.D.
Smith, and R. Telang.
2006.
Internet exchangesfor used books: An empirical analysis for product cannibal-ization and social welfare.
Information Systems Research,17(1):3?19.W.H.
Greene.
2002.
Econometric Analysis.
5th edition.V.
Hatzivassiloglou and K.R.
McKeown.
1997.
Predicting thesemantic orientation of adjectives.
In ACL?97, pages 174?181.M.
Hu and B. Liu.
2004.
Mining and summarizing customerreviews.
In KDD-2004, pages 168?177.J.
Kamps and M. Marx.
2002.
Words with attitude.
In Proceed-ings of the First International Conference on Global WordNet.S.-M. Kim and E. Hovy.
2004.
Determining the sentiment ofopinions.
In COLING 2004, pages 1367?1373.A.C.
Ko?nig and E. Brill.
2006.
Reducing the human overhead intext categorization.
In KDD-2006, pages 598?603.T.
Lee.
2004.
Use-centric mining of customer reviews.
In WITS.S.
Lewitt and C. Syverson.
2005.
Market distortions when agentsare better informed: The value of information in real estatetransactions.
Working Paper, University of Chicago.M.I.
Melnik and J. Alm.
2002.
Does a seller?s reputation mat-ter?
Evidence from eBay auctions.
Journal of Industrial Eco-nomics, 50(3):337?350, September.K.
Nigam and M. Hurst.
2004.
Towards a robust metric of opin-ion.
In AAAI Spring Symposium on Exploring Attitude andAffect in Text, pages 598?603.B.
Pang and L. Lee.
2002.
Thumbs up?
Sentiment classificationusing machine learning techniques.
In EMNLP 2002.B.
Pang and L. Lee.
2004.
A sentimental education: Sentimentanalysis using subjectivity summarization based on minimumcuts.
In ACL 2004, pages 271?278.B.
Pang and L. Lee.
2005.
Seeing stars: Exploiting class relation-ships for sentiment categorization with respect to rating scales.In ACL 2005.A.-M. Popescu and O. Etzioni.
2005.
Extracting product featuresand opinions from reviews.
In HLT/EMNLP 2005.B.
Snyder and R. Barzilay.
2007.
Multiple aspect ranking usingthe good grief algorithm.
In HLT-NAACL 2007.J.R.
Quinlan.
1992.
C4.5: Programs for Machine Learning.Morgan Kaufmann Publishers, Inc.P.
Resnick, K. Kuwabara, R. Zeckhauser, and E. Friedman.
2000.Reputation systems.
CACM, 43(12):45?48, December.P.D.
Turney and M.L.
Littman.
2003.
Measuring praise andcriticism: Inference of semantic orientation from association.ACM Transactions on Information Systems, 21(4):315?346.P.D.
Turney.
2002.
Thumbs up or thumbs down?
Semantic ori-entation applied to unsupervised classification of reviews.
InACL 2002, pages 417?424.T.
Wilson, J. Wiebe, and R. Hwa.
2006.
Recognizing strong andweak opinion clauses.
Computational Intell., 22(2):73?99.423
