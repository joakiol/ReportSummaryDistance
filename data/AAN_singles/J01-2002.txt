Improving Accuracy in Word ClassTagging through the Combination ofMachine Learning SystemsHans  van  Halteren*TOSCA/Language & Speech,University of NijmegenWalter Daelemans~CNTS/Language Technology Group,University of AntwerpJakub Zavrel tTextkernel BV,University of AntwerpWe examine how differences in language models, learned by different data-driven systems per-forming the same NLP task, can be exploited to yield a higher accuracy than the best individualsystem.
We do this by means of experiments involving the task of morphosyntactic word classtagging, on the basis of three different tagged corpora.
Four well-known tagger generators (hiddenMarkov model, memory-based, transformation rules, and maximum entropy) are trained on thesame corpus data.
After comparison, their outputs are combined using several voting strategiesand second-stage classifiers.
All combination taggers outperform their best component.
The re-duction in error rate varies with the material in question, but can be as high as 24.3% with theLOB corpus.1.
IntroductionIn all natural anguage processing (NLP) systems, we find one or more languagemodels that are used to predict, classify, or interpret language-related observations.Because most real-world NLP tasks require something that approaches full languageunderstanding in order to be perfect, but automatic systems only have access to limited(and often superficial) information, as well as limited resources for reasoning with thatinformation, such language models tend to make errors when the system is tested onnew material.
The engineering task in NLP is to design systems that make as few errorsas possible with as little effort as possible.
Common ways to reduce the error rate are todevise better epresentations of the problem, to spend more time on encoding languageknowledge (in the case of hand-crafted systems), or to find more training data (in thecase of data-driven systems).
However, given limited resources, these options are notalways available.Rather than devising a new representation forour task, in this paper, we combinedifferent systems employing known representations.
The observation that suggeststhis approach is that systems that are designed ifferently, either because they use adifferent formalism or because they contain different knowledge, will typically producedifferent errors.
We hope to make use of this fact and reduce the number of errors with* P.O.
Box 9103, 6500 HD Nijmegen, The Netherlands.
E-mail: hvh@let.kun.nl.t Universiteitsplein 1, 2610 Wilrijk, Belgium.
E-mail: zavrel@textkerneLnl.
:~ Universiteitsplein 1, 2610 Wilrijk, Belgium.
E-mail: daelem@uia.ua.ac.be.Q 2001 Association for Computational LinguisticsComputational Linguistics Volume 27, Number 2very little additional effort by exploiting the disagreement between different languagemodels.
Although the approach is applicable to any type of language model, we focuson the case of statistical disambiguators that are trained on annotated corpora.
Theexamples of the task that are present in the corpus and its annotation are fed into alearning algorithm, which induces amodel of the desired input-output mapping in theform of a classifier.
We use a number of different learning algorithms simultaneouslyon the same training corpus.
Each type of learning method brings its own "inductivebias" to the task and will produce a classifier with slightly different characteristics, sothat different methods will tend to produce different errors.We investigate two ways of exploiting these differences.
First, we make use ofthe gang effect.
Simply by using more than one classifier, and voting between theiroutputs, we expect to eliminate the quirks, and hence errors, that are due to thebias of one particular learner.
However, there is also a way to make better use ofthe differences: we can create an arbiter effect.
We can train a second-level c assifierto select its output on the basis of the patterns of co-occurrence of the outputs ofthe various classifiers.
In this way, we not only counter the bias of each component,but actually exploit it in the identification of the correct output.
This method evenadmits the possibility of correcting collective rrors.
The hypothesis that both typesof approaches can yield a more accurate model from the same training data than themost accurate component of the combination, and that given enough training data thearbiter type of method will be able to outperform the gang type.
1In the machine learning literature there has been much interest recently in the the-oretical aspects of classifier combination, both of the gang effect ype and of the arbitertype (see Section 2).
In general, it has been shown that, when the errors are uncorre-lated to a sufficient degree, the resulting combined classifier will often perform betterthan any of the individual systems.
In this paper we wish to take a more empiricalapproach and examine whether these methods result in substantial ccuracy improve-ments in a situation typical for statistical NLP, namely, learning morphosyntactic wordclass tagging (also known as part-of-speech or POS tagging) from an annotated corpusof several hundred thousand words.Morphosyntactic word class tagging entails the classification (tagging) of eachtoken of a natural anguage text in terms of an element of a finite palette (tagset) ofword class descriptors (tags).
The reasons for this choice of task are several.
First ofall, tagging is a widely researched and well-understood task (see van Halteren \[1999\]).Second, current performance l vels on this task still leave room for improvement:"state-of-the-art" performance for data-driven automatic word class taggers on theusual type of material (e.g., tagging English text with single tags from a low-detailtagset) is at 96-97% correctly tagged words, but accuracy levels for specific classesof ambiguous words are much lower.
Finally, a number of rather different methodsthat automatically generate a fully functional tagging system from annotated text areavailable off-the-shelf.
First experiments (van Halteren, Zavrel, and Daelemans 1998;Brill and Wu 1998) demonstrated the basic validity of the approach for tagging, withthe error rate of the best combiner being 19.1% lower than that of the best individualtagger (van Halteren, Zavrel, and Daelemans 1998).
However, these experiments wererestricted to a single language, a single tagset and, more importantly, a limited amountof training data for the combiners.
This led us to perform further, more extensive,1 In previous work (van Halteren, Zavrel, and Daelemans 1998), we were unable to confirm the latterhalf of the hypothesis unequivocally.
As we judged this to be due to insufficient training data forproper training of the second-level classifiers, we greatly increase the amount of training data in thepresent work through the use of cross-validation.200van Halteren, Zavrel, and Daelemans Combination of Machine Learning Systemstagging experiments before moving on to other tasks.
Since then the method has alsobeen applied to other NLP tasks with good results (see Section 6).In the remaining sections, we first introduce classifier combination on the basis ofprevious work in the machine learning literature and present he combination meth-ods we use in our experiments (Section 2).
Then we explain our experimental setup(Section 3), also describing the corpora (3.1) and tagger generators (3.2) used in theexperiments.
In Section 4, we go on to report the overall results of the experiments,starting with a comparison between the component taggers (and hence between theunderlying tagger generators) and continuing with a comparison of the combinationmethods.
The results are examined in more detail in Section 5, where we discuss uchaspects as accuracy on specific words or tags, the influence of inconsistent trainingdata, training set size, the contribution of individual component taggers, and tagsetgranularity.
In Section 6, we discuss the results in the light of related work, afterwhich we conclude (Section 7) with a summary of the most important observationsand interesting directions for future research.2.
Combination MethodsIn recent years there has been an explosion of research in machine learning on findingways to improve the accuracy of supervised classifier learning methods.
An importantfinding is that a set of classifiers whose individual decisions are combined in someway (an ensemble) can be more accurate than any of its component classifiers if theerrors of the individual classifiers are sufficiently uncorrelated (see Dietterich \[1997\],Chan, Stolfo, and Wolpert \[1999\] for overviews).
There are several ways in which anensemble can be created, both in the selection of the individual classifiers and in theway they are combined.One way to create multiple classifiers is to use subsamples of the training exam-ples.
In bagging, the training set for each individual classifier is created by randomlydrawing training examples with replacement from the initial training set (Breiman1996a).
In boosting, the errors made by a classifier learned from a training set areused to construct a new training set in which the misclassified examples get moreweight.
By sequentially performing this operation, an ensemble is constructed (e.g.,ADABOOST, \[Freund and Schapire 1996\]).
This class of methods is also called arcing(for adaptive resampling and combining).
In general, boosting obtains better resultsthan bagging, except when the data is noisy (Dietterich 1997).
Another way to cre-ate multiple classifiers is to train classifiers on different sources of information aboutthe task by giving them access to different subsets of the available input features(Cherkauer 1996).
Still other ways are to represent the output classes as bit stringswhere each bit is predicted by a different component classifier (error correcting outputcoding \[Dietterich and Bakiri 1995\]) or to develop learning-method-specific methodsfor ensuring (random) variation in the way the different classifiers of an ensemble areconstructed (Dietterich 1997).In this paper we take a multistrategy approach, in which an ensemble is con-structed by classifiers resulting from training different learning methods on the samedata (see also Alpaydin \[1998\]).Methods to combine the outputs of component classifiers in an ensemble includesimple voting where each component classifier gets an equal vote, and weightedvoting, in which each component classifier's vote is weighted by its accuracy (see, forexample, Golding and Roth \[1999\]).
More sophisticated weighting methods have beendesigned as well.
Ali and Pazzani (1996) apply the Naive Bayes' algorithm to learnweights for classifiers.
Voting methods lead to the gang effect discussed earlier.
The201Computational Linguistics Volume 27, Number 2Let Ti be the component taggers, Si(tok) the most probable tag for a token tok as suggested by Ti, and letthe quality of tagger Ti be measured by?
the precision of Ti for tag tag: Prec(Ti, tag)?
the recall of T i for tag tag: Rec(Ti, tag)?
the overall precision of Ti: Prec(Ti)Then the vote V(tag, tok) for tagging token tok with tag tag is given by:?
Majority:?
TotPrecision:?
TagPrecision:?
Precision-Recall:~_IF  Si(tok)= tag THEN 1 ELSE 0i~_ IF  Si(tok)= tag THEN Prec(Ti) ELSE 0i~__IF Si(tok ) = tag THEN Prec(Ti, tag) ELSE 0i~_.IF Si(tok ) =tag THEN Prec(Ti, tag) ELSE 1-Rec(Ti, tag)iFigure 1Simple algorithms for voting between component taggers.most interesting approach to combination is stacking in which a classifier is trainedto predict the correct output class when given as input the outputs of the ensembleclassifiers, and possibly additional information (Wolpert 1992; Breiman 1996b; Ting andWitten 1997a, 1997b).
Stacking can lead to an arbiter effect.
In this paper we comparevoting and stacking approaches on the tagging problem.In the remainder of this section, we describe the combination methods we usein our experiments.
We start with variations based on weighted voting.
Then we goon to several types of stacked classifiers, which model the disagreement situationsobserved in the training data in more detail.
The input to the second-stage classifiercan be limited to the first-level outputs or can contain additional information from theoriginal input pattern.
We will consider a number of different second-level learners.Apart from using three well-known machine learning methods, memory-based learn-ing, maximum entropy, and decision trees, we also introduce a new method, based ongrouped voting.2.1 Simple VotingThe most straightforward method to combine the results of multiple taggers is to doan n-way vote.
Each tagger is allowed to vote for the tag of its choice, and the tag withthe highest number of votes is selected.
2 The question is how large a vote we alloweach tagger (Figure 1).
The most democratic option is to give each tagger one vote(Majority).
This does not require any tuning of the voting mechanism on training data.However, the component taggers can be distinguished by several figures of merit,and it appears more useful to give more weight to taggers that have proved theirquality.
For this purpose we use precision and recall, two well-known measures, which2 In all our experiments, any ties are resolved by a random selection from among the winning tags.202van Halteren, Zavrel, and Daelemans Combination of Machine Learning Systemscan be applied to the evaluation of tagger output as well.
For any tag X, precisionmeasures which percentage of the tokens tagged X by the tagger are also tagged Xin the benchmark.
Recall measures which percentage of the tokens tagged X in thebenchmark are also tagged X by the tagger.
When abstracting away from individualtags, precision and recall are equal (at least if the tagger produces exactly one tag pertoken) and measure how many tokens are tagged correctly; in this case we also usethe more generic term accuracy.
We will call the voting method where each tagger isweighted by its general quality TotPrecision, i.e., each tagger votes its overall precision.To allow for more detailed interactions, each tagger is weighted by the quality inrelation to the current situation, i.e., each tagger votes its precision on the tag it suggests(TagPrecision).
This way, taggers that are accurate for a particular type of ambiguitycan act as specialized experts.
The information about each tagger's quality is derivedfrom a cross-validation f its results on the combiner training set.
The precise setupfor deriving the training data is described in more detail below, in Section 3.We have access to even more information on how well the taggers perform.
Wenot only know whether we should believe what they propose (precision) but know aswell how often they fail to recognize the correct ag (1 - recall).
This information canbe used by forcing each tagger to add to the vote for tags suggested by the oppositiontoo, by an amount equal to 1 minus the recall on the opposing tag (Precision-Recall).As an example, suppose that the MXPOST tagger suggests DT and the HMM taggerTnT suggests CS (two possible tags in the LOB tagset for the token that).
If MXPOSThas a precision on DT of 0.9658 and a recall on CS of 0.8927, and TnT has a precision onCS of 0.9044 and a recall on DT of 0.9767, then DT receives a 0.9658 + 0.0233 = 0.9991vote and CS a 0.9044 + 0.1073 = 1.0117 vote.Note that simple voting combiners can never return a tag that was not suggestedby a (weighted) majority of the component taggers.
As a result, they are restrictedto the combination of taggers that all use the same tagset.
This is not the case forall the following (arbiter type) combination methods, a fact which we have recentlyexploited in bootstrapping a word class tagger for a new corpus from existing taggerswith completely different tagsets (Zavrel and Daelemans 2000).2.2 Stacked Probabilistic VotingOne of the best methods for tagger combination i (van Halteren, Zavrel, and Daele-mans 1998) is the TagPair method.
It looks at all situations where one tagger suggeststag 1 and the other tag 2 and estimates the probability that in this situation the tagshould actually be tag x.
Although it is presented as a variant of voting in that paper,it is in fact also a stacked classifier, because it does not necessarily select one of thetags suggested by the component taggers.
Taking the same example as in the votingsection above, if tagger MXPOST suggests DT and tagger TnT suggests CS, we findthat the probabilities for the appropriate tag are:CSCS22DTQLWPRsubordinating conjunction 0.4623second half of a two-token subordinating conjunction, e.g., so that 0.0171determiner 0.4966quantifier 0.0103wh-pronoun 0.0137When combining the taggers, every tagger pair is taken in turn and allowed tovote (with a weight equal to the probability P(tag x I tag1, tag2) as described above)for each possible tag (Figure 2).
If a tag pair tagl-tag 2 has never been observed in thetraining data, we fall back on information on the individual taggers, i.e., P(tag x I tag1)203Computational Linguistics Volume 27, Number 2Let Ti be the component taggers and Si(tok) the most probable tag for a token tok as suggested by Ti.
Thenthe vote V(tag, tok) for tagging token tok with tag tag is given by:V(tag, tok) = ~_~ Vi,j(tag , tok)i,jliKjwhere Vial(tag, tok) is given byIF frequency(Si(tokx) = Si(tok), Sj(tokx) = Sj(tok) ) > 0THEN Vial(tag, tok) = P(tag l Si(tok~) = Si(tok), Sj(tokx) = Sj(tok) )Vi,j(tag, tok) = ~P(tag \] Si(tokx) = Si(tok) ) + ~P(tag I Sj(tokx) = Sj(tok) ) ELSEFigure 2The TagPair algorithm for voting between component taggers.If the case to be classified corresponds to the feature-value pair setFcase = {0Cl = V l}  .
.
.
.
.
{fn = Vn}}then estimate the probability of each class Cx for Fcase as a weighted sum over all possible subsets Fsu b ofFcase:~(Cx) ~_, Wr~.bP(CK I Esub)Fsub C Fcasewith the weight WG, b for an Fsub containing n elements equal to n---L-r' where Wnorm is a normalizing Wnor m iconstant so that ~cx P(Cx) = 1.Figure 3The Weighted Probability Distribution Voting classification algorithm, as used in thecombination experiments.and P(tag x I tag2).
Note that with this method (and all of the following), a tag suggestedby a minority (or even none) of the taggers actually has a chance to win, although inpractice the chance to beat a majority is still very slight.Seeing the success of TagPair in the earlier experiments, we decided to try togeneralize this stacked probabilistic voting approach to combinations larger than pairs.Among other things, this would let us include word and context features here as well.The method that was eventually developed we have called Weighted ProbabilityDistribution Voting (henceforth WPDV).A WPDV classification model  is not l imited to pairs of features (such as the pairsof tagger outputs for TagPair), but can use the probabil ity distributions for all featurecombinations observed in the training data (Figure 3).
During voting, we do not use afallback strategy (as TagPair does) but use weights to prevent he lower-order combi-nations from excessively influencing the final results when a higher-order combination(i.e., more exact information) is present.
The original system, as used for this paper,weights a combination of order n with a factor n!, a number  based on the observationthat a combination of order m contains m combinations of order (m - 1) that have tobe competed with.
Its only parameter  is a threshold for the number  of times a combi-nation must  be observed in the training data in order to be used, which helps preventa combinatorial explosion when there are too many atomic features.
33 In our experiments, his parameter is always set to 5.
WPDV has since evolved, using more parametersand more involved weighting schemes, and also been tested on tasks other than tagger combination(van Halteren 2000a, 2000b).204van Halteren, Zavrel, and Daelemans Combination of Machine Learning Systems?
Tags uggested by the base taggers, used by all systems:TagTBL = JJ TagMBT = VBN TagMXP = VBD TagHMM = JJ?
The focus token, used by stacked classifiers at level Tags+Word:Word = restored?
Full form tags uggested by the base tagger for the previous and next oken, used by stackedclassifiers at level Tags+Context, except for WPDV:PrevTBL = JJ P revMBT = NN PrevMXP = NN PrevHMM = J\]NextTBL  = NN NextMBT = NN NextMXP = NN NextHMM = NN?
Compressed form of the context tags, used by WPDV(Tags+Context), because the system wasunable to cope with the large number of features:Prev ~- JJ + NN + NN + JJ Next  = NN + NN + NN + NN?
Target feature, used by all systems:Tag = VBDFigure 4Features used by the combination systems.
Examples are taken from the LOB material.In contrast o voting, stacking classifiers allows the combination of the outputs ofcomponent systems with additional information about the decision's context.
We in-vestigated several versions of this approach.
In the basic version (Tags), each trainingcase for the second-level learner consists of the tags suggested by the component tag-gers and the correct ag (Figure 4).
In the more advanced versions, we add informationabout the word in question (Tags+Word) and the tags suggested by all taggers for theprevious and the next position (Tags+Context).
These types of extended second-levelfeatures can be exploited by WPDV, as well as by a wide selection of other machinelearning algorithms.2.3 Memory-based CombinationOur first choice from these other algorithms is a memory-based second-level learner,implemented in TiMBL (Daelemans et al 1999), a package developed at Tilburg Uni-versity and Antwerp University.
4Memory-based learning is a learning method that is based on storing all examplesof a task in memory and then classifying new examples by similarity-based reasoningfrom these stored examples.
Each example is represented by a fixed-length vector offeature values, called a case.
If the case to be classified has been observed before,that is, if it is found among the stored cases (in the case base), the most frequentcorresponding output is used.
If the case is not found in the case base, k nearestneighbors are determined with some similarity metric, and the output is based on theobserved outputs for those neighbors.
Both the value of k and the similarity metricused can be selected by parameters of the system.
For the Tags version, the similaritymetric used is Overlap (a count of the number of matching feature values between atest and a training item) and k is kept at 1.
For the other two versions (Tags+Word andTags+Context), a value of k = 3 is used, and each overlapping feature is weighted byits Information Gain (Daelemans, Van den Bosch, and Weijters 1997).
The Information4 TiMBL is available from http://ilk.kub.nl/.205Computational Linguistics Volume 27, Number 2Gain of a feature is defined as the difference between the entropy of the a priori classdistribution and the conditional entropy of the classes given the value of the feature.
~2.4 Maximum Entropy CombinationThe second machine learning method, maximum entropy modeling, implemented inthe Maccent system (Dehaspe 1997), does the classification task by selecting the mostprobable class given a maximum entropy model.
6This type of model represents ex-amples of the task (Cases) as sets of binary indicator features, for the task at handconjunctions of a particular tag and a particular set of feature values.
The model hasthe form of an exponential model:1 e Y~i ~i~(ca~,~g) pA(tag l Case) -- Za(Case)where i indexes all the binary features, fi is a binary indicator function for feature i,ZA is a normalizing constant, and )~i is a weight for feature i.
The model is trained byiteratively adding binary features with the largest gain in the probability of the train-ing data, and estimating the weights using a numerical optimization method calledimproved iterafive scaling.
The model is constrained by the observed istribution ofthe features in the training data and has the property of having the maximum en-tropy of all models that fit the constraints, i.e., all distributions that are not directlyconstrained by the data are left as uniform as possible.
7The maximum entropy combiner takes the same information as the memory-basedlearner as input, but internally translates all multivalued features to binary indicatorfunctions.
The improved iterative scaling algorithm is then applied, with a maximumof one hundred iterations.
This algorithm is the same as the one used in the MXPOSTtagger described in Section 3.2.3, but without the beam search used in the taggingapplication.2.5 Decision Tree CombinationThe third machine learning method we used is c5.0 (Quinlan 1993), an example oftop-down induction of decision trees.
8 A decision tree is constructed by recursivelypartitioning the training set, selecting, at each step, the feature that most reduces theuncertainty about the class in each partition, and using it as a split, c5.0 uses GainRatio as an estimate of the utility of splitting on a feature.
Gain Ratio corresponds tothe Information Gain measure of a feature, as described above, except hat the measureis normalized for the number of values of the feature, by dividing by the entropy of thefeature's values.
After the decision tree is constructed, it is pruned to avoid overfitting,using a method described in detail in Quinlan (1993).
A classification for a test caseis made by traversing the tree until either a leaf node is found or all further branchesdo not match the test case, and returning the most frequent class at the last node.
Thecase representation uses exactly the same features as the memory-based learner.3.
Experimental SetupIn order to test the potential of system combination, we obviously need systems tocombine, i.e., a number of different aggers.
As we are primarily interested in the5 This is also sometimes referred to as mutual information i the computational linguistics literature.6 Maccent isavailable from http://www.cs.kuleuven.ac.be/~ldh.7 For a more detailed iscussion, see Berger, Della Pietra, and Della Pietra (1996) and Ratnaparkhi (1996).8 c5.0 is commercially available from http://www.rulequest.com/.
Its predecessor, c4.5, can bedownloaded from http://www.cse.unsw.edu.au/~quinlan/.206van Halteren, Zavrel, and Daelemans Combination of Machine Learning Systemscombination of classifiers trained on the same data sets, we are in fact looking fordata sets (in this case, tagged corpora) and systems that can automatically gener-ate a tagger on the basis of those data sets.
For the current experiments, we haveselected three tagged corpora and four tagger generators.
Before giving a detaileddescription of each of these, we first describe how the ingredients are used in theexperiments.Each corpus is used in the same way to test tagger and combiner performance.First of all, it is split into a 90% training set and a 10% test set.
We can evaluatethe base taggers by using the whole training set to train the tagger generators andthe test set to test the resulting tagger.
For the combiners, a more complex strategymust be followed, since combiner training must be done on material unseen by thebase taggers involved.
Rather than setting apart a fixed combiner training set, we usea ninefold training strategy?
The 90% trai1~ing set is split into nine equal parts.
Eachpart is tagged with component taggers that have been trained on the other eight parts.All results are then concatenated for use in combiner training, so that, in contrast oour earlier work, all of the training set is effectively available for the training of thecombiner.
Finally, the resulting combiners are tested on the test set.
Since the test setis identical for all methods, we can compute the statistical significance of the resultsusing McNemar's chi-squared test (Dietterich 1998).As we will see, the increase in combiner training set size (90% of the corpus versusthe fixed 10% tune set in the earlier experiments) indeed results in better performance.On the other hand, the increased amount of data also increases time and space require-ments for some systems to such a degree that we had to exclude them from (someparts of) the experiments.The data in the training set is the only information used in tagger and combinerconstruction: all components of all taggers and combiners (lexicon, context statistics,etc.)
are entirely data driven, and no manual adjustments are made.
If any tagger orcombiner construction method is parametrized, we use default settings where avail-able.
If there is no default, we choose intuitively appropriate values without prelimi-nary testing.
In these cases, we report such parameter settings in the introduction tothe system.3.1 DataIn the current experiments we make use of three corpora.
The first is the LOB corpus(Johansson 1986), which we used in the earlier experiments as well (van Halteren,Zavrel, and Daelemans 1998) and which has proved to be a good testing ground.
Wethen switch to Wall Street Journal material (WSJ), tagged with the Penn Treebank IItagset (Marcus, Santorini, and Marcinkiewicz 1993).
Like LOB, it consists of approx-imately 1M words, but unlike LOB, it is American English.
Furthermore, it is of adifferent structure (only newspaper text) and tagged with a rather different agset.The experiments with WSJ will also let us compare our results with those reported byBrill and Wu (1998), which show a much less pronounced accuracy increase than ourswith LOB.
The final corpus is the slightly smaller (750K words) Eindhoven corpus (Uitden Boogaart 1975) tagged with the Wotan tagset (Berghmans 1994).
This will let usexamine the tagging of a language other than English (namely, Dutch).
Furthermore,the Wotan tagset is a very detailed one, so that the error rate of the individual taggers9 Compare this to the "tune" set in van Halteren, Zavrel, and Daelemans (1998).
This consisted of 114Ktokens, but, because of a 92.5% agreement over all four taggers, ityielded less than 9K tokens of usefultraining material to resolve disagreements.
This was suspected tobe the main reason for the relativelack of performance by the more sophisticated combiners.207Computational Linguistics Volume 27, Number 2tends to be higher.
Moreover,  we can more easily use projections of the tagset andthus s tudy the effects of levels of granularity.3.1.1 LOB.
The first data set we use for our exper iments consists of the taggedLancaster-Oslo/Bergen corpus (LOB \[ Johansson 1986\]).
The corpus comprises aboutone mil l ion words  of British Engl ish text, d iv ided over 500 samples of 2,000 wordsf rom 15 text types.The tagging of the LOB corpus, wh ich  was manual ly  checked and corrected, isgeneral ly accepted to be quite accurate.
Here we use a slight adaptat ion of the tagset.The changes are main ly  cosmetic, e.g., nonalphabet ic  haracters uch as "$" in tagnames have been replaced.
However ,  there has also been some retokenization: genitivemarkers have been split off and the negative marker  n't has been reattached.An  example sentence tagged with the result ing tagset is:The ATI singular or plural articleLord NPT singular titular nounMajor NPT singular titular nounextended VBD past tense of verban AT singular articleinvitation NN singular common ounto IN prepositionall ABN pre-quantifierthe ATI singular or plural articleparliamentary JJ adjectivecandidates NNS plural common ounSPER periodThe tagset consists of 170 different tags ( including ditto tags), and has an averageambigui ty  of 2.82 tags per  word form over the corpus.
1?
An  impression of the difficultyof the tagging task can be gained f rom the two baseline measurements  in Table 2 (inSection 4.1 below), representing a completely random choice f rom the potential  tagsfor each token (Random) and selection of the lexically most  l ikely tag (LexProb)J 1The training/test  separat ion of the corpus is done at utterance boundar ies  (each 1stto 9th utterance is training and each 10th is test) and leads to a 1,046K token trainingset and a 115K token test set.
A round 2.14% of the test set are tokens unseen in thetraining set and a further 0.37% are known tokens but  with unseen tags.
123.1.2 WSJ.
The second data set consists of 1M words  of Wall Street Journal material.It differs f rom LOB in that it is Amer ican Engl ish and, more importantly,  in that it iscompletely made up of newspaper  text.
The material  is tagged with the Penn Treebanktagset (Marcus, Santorini, and Marcinkiewicz 1993), wh ich  is much smaller than theLOB one.
It consists of only 48 tags.
13 There is no attempt to annotate compoundwords,  so there are no ditto tags.10 Ditto tags are used for the components ofmultitoken units, e.g.
if as well as is taken to be a coordinatingconjunction, it is tagged "as_CC-1 well_CC-2 as_CC-3", using three related but different ditto tags.11 These numbers are calculated on the basis of a lexicon derived from the whole corpus.
An actualtagger will have to deal with unknown words in the test set, which will tend to increase the ambiguityand decrease Random and LexProb.
Note that all actual taggers and combiners in this paper do haveto cope with unknown words as their lexicons are based purely on their training sets.12 Because of the way in which the tagger generators treat their input, we do count okens as differenteven though they are the same underlying token, but differ in capitalization of one or more characters.13 In the material we have available, quotes are represented slightly differently, so that there are only 45different ags.
In addition, the corpus contains a limited number of instances of 38 "indeterminate"tags, e.g., JJ\]VBD indicates a choice between adjective and past participle which cannot be decided orabout which the annotator was unsure.208van Halteren, Zavrel, and Daelemans Combination of Machine Learning SystemsAn example sentence is:By IN preposition/subordinating conjunction10 CD cardinal numbera.m.
RB adverbTokyo NNP singular proper nountime NN singular common ouncommathe E)T determinerindex NN singular common ounwas VBD past tense verbup RB adverb435.11 CD cardinal numberpoints NNS plural common ouncommato  ,,to,,34903.80 CD cardinal numberas IN preposition/subordinating conjunctioninvestors NNS plural common ounhailed VBD past tense verbNew NNP singular proper nounYork NNP singular proper noun's POS possessive endingovernight JJ adjectiverally NN singular common ounsentence-final punctuationMostly because of the less detailed tagset, the average ambiguity of the tags islower than LOB's, at 2.34 tags per token in the corpus.
This means that the taggingtask should be an easier one than that for LOB.
This is supported by the values forRandom and LexProb in Table 2.
On the other hand, the less detailed tagset alo meansthat the taggers have less detailed information to base their decisions on.
Anotherfactor that influences the quality of automatic tagging is the consistency of the taggingover the corpus.
The WSJ material has not been checked as extensively as the LOBcorpus and is expected to have a much lower consistency level (see Section 5.3 belowfor a closer examination).The training/test separation of the corpus is again done at utterance boundariesand leads to a 1,160K token training set and a 129K token test set.
Around 1.86% ofthe test set are unseen tokens and a further 0.44% are known tokens with previouslyunseen tags.3.1.3 Eindhoven.
The final two data sets are both based on the Eindhoven corpus(Uit den Boogaart 1975).
This is slightly smaller than LOB and WSJ.
The written part,which we use in our experiments, consists of about 750K words, in samples rangingfrom 53 to 451 words.
In variety, it lies between LOB and WSJ, containing 150K wordseach of samples from Dutch newspapers (subcorpus CDB), weeklies (OBL), magazines(GBL), popular scientific writings (PWE), and novels (RNO).The tagging of the corpus, as used here, was created in 1994 as part of a mas-ter's thesis project (Berghmans 1994).
It employs the Wotan tagset for Dutch, newlydesigned uring the project.
It is based on the classification used in the most populardescriptive grammar of Dutch, the Algemene Nederlandse Spraakkunst (ANS \[Geerts etal.
1984\]).
The actual distinctions encoded in the tagset were selected on the basis oftheir importance to the potential users, as estimated from a number of in-depth inter-views with interested parties in the Netherlands.
The Wotan tagset is not only verylarge (233 base tags, leading to 341 tags when counting each ditto tag separately), butfurthermore contains distinctions that are very difficult for automatic taggers, such asverb transitivity, syntactic use of adjectives, and the recognition of multitoken units.It has an average ambiguity of 7.46 tags per token in the corpus.
For our experiments,209Computational Linguistics Volume 27, Number 2we also designed a simplif ication of the tagset, dubbed WotanLite, which no longercontains the most  problematic distinctions.
WotanLite has 129 tags (with a complementof ditto tags leading to a total of 173) and an average ambigui ty  of 3.46 tags per token.An  example of Wotan tagging is given below (only under l ined parts remain inWotanLite): 14Mr.
(Master, t i t le )  N(eigen,ev, neut):l/2Rijpstra N(eigen,ev, neut):2/2heeft (has) V(hulp,ott,3,ev)de (the) Art(bep,zijd-of-mv, neut)Commissarispost N(soort,ev, neut)(post of Commissioner)in (in) Prep(voor)Friesland N(eigen,ev, neut)geambieerd (aspired to) V(trans,verl-dw, onverv)en (and) Conj(neven)hij (he) Pron(per,3,ev, nom)moet (should) V(hulp,ott,3,ev)dus (therefore) Adv(gew, aanw)alle (a l l )  Pron(onbep,neut,attr)kans (opportunity) N_~(soort,ev, n ut)hebben (have) V__((trans,inf)er (there) Adv(pron,er)het (the) Art(bep,onzijd,neut)beste (best) Adj (zelfst,over tr,verv-neut)van (of) Adv(deel-adv)te (to) Prep(voor-inf)maken (make) V(trans,inf)Punc(punt)first part of singular neutral case proper nounsecond part of singular neutral case propernoun3rd person singular present tense auxiliary verbneutral case non-neuter or plural definite articlesingular neutral case common ounadposition used as prepositionsingular neutral case proper nounbase form of past participle of transitive verbcoordinating conjunction3rd person singular nominative personalpronoun3rd person singular present tense auxiliary verbdemonstrative non-pronominal adverbattributively used neutral case indefinitepronounsingular neutral case common ouninfinitive of transitive verbpronominal dverb "er"neutral case neuter definite articlenominally used inflected superlativeform of adjectiveparticle adverbinfinitival "te"infinitive of transitive verbperiodThe annotat ion of the corpus was realized by a semiautomat ic  upgrade  of thetagging inherited f rom an earlier project.
The result ing consistency has never beenexhaust ively measured for either the Wotan or the original tagging.The training/test  separat ion of the corpus is done at sample boundar ies  (each 1stto 9th sample is training and each 10th is test).
This is a much stricter separat ion thanappl ied for LOB and WSJ, as for those two corpora our test utterances are related tothe training ones by  being in the same samples.
Part ly as a result of this, but  also verymuch because of word  compound ing  in Dutch,  we  see a much higher percentage ofnew tokens--6.24% tokens unseen in the training set.
A further 1.45% known tokenshave new tags for Wotan, and 0.45% for WotanLite.
The training set consists of 640Ktokens and the test set of 72K tokens.3.2 Tagger GeneratorsThe second ingredient for our exper iments is a set of four tagger generator  systems,selected on the basis of var iety and availability, is Each of the systems represents a14 The example sentence could be rendered in English as Master Rijpstra has aspired to the post ofCommissioner in Friesland and he should therefore be given every opportunity to make the most of it.15 The systems have to differ as much as possible in their learning strategies and biases, as otherwisethere will be insufficient differences of opinion for the combiners to make use of.
This was shownclearly in early experiments in 1992, where only n-gram taggers were used, and which produced only avery limited improvement in accuracy (van Ha|teren 1996).210van Halteren, Zavrel, and Daelemans Combination of Machine Learning SystemsTable 1The features available to the four taggers in our study.
Except for MXPOST, all systems usedifferent models (and hence features) for known (k) and unknown (u) words.
However, Brill'stransformation-based learning system (TBL) applies its two models in sequence when facedwith unknown words, thus giving the unknown-word tagger access to the features used bythe known-word model as well.
The first five columns in the table show features of the focusword: capitalization (C), hyphen (H), or digit (D) present, and number of suffix (S) or prefix(P) letters of the word.
Brill's TBL system (for unknown words) also takes into accountwhether the addition or deletion of a suffix results in a known lexicon entry (indicated by anL).
The next three columns represent access to the actual word (W) and any range of words tothe left (Wleft) or  right (Wright).
The last three columns how access to tag information for theword itself (T) and any range of words left (Tleft) or right (Tright).
Note that the expressivepower of a method is not purely determined by the features it has access to, but also by itsalgorithm, and what combinations of the available features this allows it to consider.FeaturesSystem C D N S P W Wtedt Wright T Tteft ZrightTBL (k) x 1-2 1-2 x 1-3 1-3TBL (u) x x x 4,L 4,L 1-2 1-2 1-3 1-3MBT (k) x x 1-2 1-2MBT (u) x x x 3 1 1MXP (all) x x x 4 4 x 1-2 1-2 1-2TNT (k) x x x 1-2TNT (u) x 10 1-2popular type of learning method, each uses slightly different features of the text (seeTable 1), and each has a completely different representation for its language model.All publicly available systems are used with the default settings that are suggested intheir documentation.3.2.1 Error-driven Transformation-based Learning.
This learning method finds a setof rules that transforms the corpus from a baseline annotation so as to minimize thenumber of errors (we will refer to this system as TBL below).
A tagger generator usingthis learning method is described in Brill (1992, 1994).
The implementation that weuse is Eric Brill's publicly available set of C programs and Perl scripts.
16When training, this system starts with a baseline corpus annotation A0.
In A0,each known word is tagged with its most likely tag in the training set, and eachunknown word is tagged as a noun (or proper noun if capitalized).
The system thensearches through a space of transformation rules (defined by rule templates) in orderto reduce the discrepancy between its current annotation and the provided correctone.
There are separate templates for known words (mainly based on local wordand tag context), and for unknown words (based on suffix, prefix, and other lexicalinformation).
The exact features used by this tagger are shown in Table 1.
The learnerfor the unknown words is trained and applied first.
Based on its output, the rules forcontext disambiguation are learned.
In each learning step, all instantiations of the ruletemplates that are present in the corpus are generated and receive a score.
The rule thatcorrects the highest number of errors at step n is selected and applied to the corpus toyield an annotation A,, which is then used as the basis for step n + 1.
The process topswhen no rule reaches a score above a predefined threshold.
In our experiments thishas usually yielded several hundreds of rules.
Of the four systems, TBL has access to16 Brill's system can be downloaded fromftp : // ftp.cs.jhu.edu /pub /brill /Programs / R ULE_B ASED_TA GG ER_V.1.14.tar.Z.211Computational Linguistics Volume 27, Number 2the most features: contextual information (the words and tags in a window spanningthree positions before and after the focus word) as well as lexical information (theexistence of words formed by the addition or deletion of a suffix or prefix).
However,the conjunctions of these features are not all available in order to keep the search spacemanageable.
Even with this restriction, the search is computationally very costly.
Themost important rule templates are of the formi f  context = x change tag i to  tagjwhere context is some condition on the tags of the neighbouring words.
Hence learningspeed is roughly cubic in the tagset size.
17When tagging, the system again starts with a baseline annotation for the newtext, and then applies all rules that were derived during training, in the sequence inwhich they were derived.
This means that application of the rules is fully deterministic.Corpus statistics have been at the basis of selecting the rule sequence, but the resultingtagger does not explicitly use a probabilistic model.3.2.2 Memory-Based Learning.
Another learning method that does not explicitly ma-nipulate probabilities i  machine-based learning.
However, rather than extracting aconcise set of rules, memory-based learning focuses on storing all examples of a taskin memory in an efficient way (see Section 2.3).
New examples are then classified bysimilarity-based reasoning from these stored examples.
A tagger using this learningmethod, MBT, was proposed by Daelemans et al (1996).
TMDuring the training phase, the training corpus is transformed into two case bases,one which is to be used for known words and one for unknown words.
The cases arestored in an IGTree (a heuristically indexed version of a case memory \[Daelemans,Van den Bosch, and Weijters 1997\]), and during tagging, new cases are classified bymatching cases with those in memory going from the most important feature to theleast important.
The order of feature relevance is determined by Information Gain.For known words, the system used here has access to information about he focusword and its potential tags, the disambiguated tags in the two preceding positions,and the undisambiguated tags in the two following positions.
For unknown words,only one preceding and following position, three suffix letters and information aboutcapitalization and presence of a hyphen or a digit are used as features.
The case basefor unknown words is constructed from only those words in the training set that occurfive times or less.3.2.3 Maximum Entropy Modeling.
Tagging can also be done using maximum en-tropy modeling (see Section 2.4): a maximum entropy tagger, called MXPOST, wasdeveloped by Ratnaparkhi (1996) (we will refer to this tagger as MXP below).
19 Thissystem uses a number of word and context features rather similar to system MBT, andtrains a maximum entropy model using the improved iterative scaling algorithm forone hundred iterations.
The final model has a weighting parameter for each featurevalue that is relevant o the estimation of the probability P(tag I features), and com-bines the evidence from diverse features in an explicit probability model.
In contrastto the other taggers, both known and unknown words are processed by the same17 Because ofthe computational complexity, we have had to exclude the system from the experimentswith the very large Wotan tagset.18 An on-line version of the tagger isavailable at http://ilk.kub.nl/.19 Ratnaparkhi's Java implementation of this system isfreely available for noncommercial researchpurposes at ftp://ftp.cis.upenn.edu/pub/adwait/jmx/.212van Halteren, Zavrel, and Daelemans Combination of Machine Learning Systemsmodel.
Another striking difference is that this tagger does not have a separate storagemechanism for lexical information about the focus word (i.e., the possible tags).
Theword is merely another feature in the probability model.
As a result, no generaliza-tions over groups of words with the same set of potential tags are possible.
In thetagging phase, a beam search is used to find the highest probability tag sequence forthe whole sentence.3.2.4 Hidden Markov Models.
In a Hidden Markov Model, the tagging task is viewedas finding the maximum probability sequence of states in a stochastic finite-state ma-chine.
The transitions between states emit the words of a sentence with a probabilityP(w \[ St), the states St themselves model tags or sequences of tags.
The transitions arecontrolled by Markovian state transition probabilities P(Stl \] Sti_l ).
Because a sentencecould have been generated by a number of different state sequences, the states areconsidered to be "Hidden."
Although methods for unsupervised training of HMM'sdo exist, training is usually done in a supervised way by estimation of the above prob-abilities from relative frequencies in the training data.
The HMM approach to taggingis by far the most studied and applied (Church 1988; DeRose 1988; Charniak 1993).In van Halteren, Zavrel, and Daelemans (1998) we used a straightforward im-plementation of HMM's, which turned out to have the worst accuracy of the fourcompeting methods.
In the present work, we have replaced this by the TnT system(we will refer to this tagger as HMM below).
2?
TnT is a trigram tagger (Brants 2000),which means that it considers the previous two tags as features for deciding on thecurrent ag.
Moreover, it considers the capitalization of the previous word as well inits state representation.
The lexical probabilities depend on the identity of the currentword for known words and on a suffix tree smoothed with successive abstraction(Samuelsson 1996) for guessing the tags of unknown words.
As we will see below, itshows a surprisingly higher accuracy than our previous HMM implementation.
Whenwe compare it with the other taggers used in this paper, we see that a trigram HMMtagger uses a very limited set of features (Table 1).
on  the other hand, it is able toaccess ome information about the rest of the sentence indirectly, through its use ofthe Viterbi algorithm.4.
Overall ResultsThe first set of results from our experiments i  the measurement of overall accuracyfor the base taggers.
In addition, we can observe the agreement between the systems,from which we can estimate how much gain we can possibly expect from combination.The application of the various combination systems, finally, shows us how much ofthe projected gain is actually realized.4.1 Base Tagger QualityAn additional benefit of training four popular tagging systems under controlled con-ditions on several corpora is an experimental comparison of their accuracy.
Table 2lists the accuracies as measured on the test set.
21 We see that TBL achieves the lowestaccuracy on all data sets.
MBT is always better than TBL, but is outperformed by bothMXP and HMM.
On two data sets (LOB and Wotan) the Hidden Markov Model sys-tem (TnT) is better than the maximum entropy system (MXPOST).
On the other two20 The TnT system can be obtained from its author through ttp://www.coli.uni-sb.de/~thorsten/tnt/.21 In this and several following tables, the best performance is indicated with bold type.213Computational Linguistics Volume 27, Number 2Table 2Baseline and individual tagger test set accuracy for each of our four data sets.
The bottom fourrows show the accuracies of the four tagging systems on the various data sets.
In addition, welist two baselines: the selection of a completely random tag from among the potential tags forthe token (Random) and the selection of the lexically most likely tag (LexProb).LOB WSJ Wotan WotanLiteBaselineRandom 61.46 63.91 42.99 54.36LexProb 93.22 94.57 89.48 93.40Single TaggerTBL 96.37 96.28 -* 94.63MBT 97.06 96.41 89.78 94.92MXP 97.52 96.88 91.72 95.56HMM 97.55 96.63 92.06 95.26*The training of TBL on the large Wotan tagset wasaborted after several weeks of training failed to pro-duce any useful results.Table 3Pairwise agreement between the base taggers.
For each base tagger pair and data set, we listthe percentage of tokens in the test set on which the two taggers elect he same tag.Tagger PairMXP MXP MXP HMM HMM MBTData Set HMM MBT TBL MBT TBL TBLLOB 97.56 96.70 96.27 97.27 96.96 96.78WSJ 97.41 96.85 96.90 97.18 97.39 97.21Wotan 93.02 90.81 - 92.06 - -WotanLite 95.74 95.12 95.00 95.48 95.36 95.52(WSJ and WotanLite) MXPOST is the better system.
In all cases, except the differencebetween MXP and HMM on LOB, the differences are statistically significant (p K 0.05,McNemar 's  chi-squared test).We can also see from these results that WSJ, although it is about the same size asLOB, and has a smaller tagset, has a higher difficulty level than LOB.
We suspect hatan important reason for this is the inconsistency in the WSJ annotation (cf.
Ratnaparkhi1996).
We examine this effect in more detail below.
The Eindhoven corpus, both withWotan and WotanLite tagsets, is yet more difficult, but here the difficulty lies mainlyin the complexity of the tagset and the large percentage of unknown words in thetest sets.
We see that the reduction in the complexity of the tagset from Wotan toWotanLite leads to an enormous improvement  in accuracy.
This granularity effect isalso examined in more detail below.4.2 Base Tagger AgreementOn the basis of the output of the single taggers we can also examine the feasibilityof combination, as combination is dependent on different systems producing differenterrors.
As expected, a large part of the errors are indeed uncorrelated: the agreementbetween the systems (Table 3) is at about the same level as their agreement withthe benchmark tagging.
A more detailed view of intertagger agreement is shownin Table 4, which lists the (groups of) patterns of (dis)agreement for the four datasets.214van Halteren, Zavrel, and Daelemans Combination of Machine Learning SystemsTable 4The presence of various tagger (dis)agreeement patterns for the four data sets.
In addition tothe percentage of the test sets for which the pattern is observed (%), we list the cumulativepercentage (%Cum).LOB WSJ Wotan WotanLitePattern % %Cum % %Cure % %Cure % %CumAll taggers 93.93 93.93 93.80 93.80 85.68 85.68 90.50 90.50agree andare correct.A majority is 3.30 97.23 2.64 96.44 6.54 92.22 4.73 95.23correct.Correct tag is 1.08 98.31 1.07 97.51 0.82 93.04 1.59 96.82present but istied.A minority is 0.91 99.22 1.12 98.63 2.62 95.66 1.42 98.24correct.The taggers 0.21 99.43 0.26 98.89 1.53 97.19 0.46 98.70vary, butare all wrong.All taggers 0.57 100.00 1.11 100.00 2.81 100.00 1.30 100.00agree butare wrong.It is interest ing to see that a l though the genera l  accuracy for WSJ is lower  thanfor LOB, the inter tagger  agreement  for WSJ is on average higher.
It wou ld  seem thatthe less consistent  tagg ing for WSJ makes  it easier for all systems to fall into the sametraps.
This becomes even clearer when we examine the pat terns  of agreement  andsee, for example ,  that  the number  of tokens where  all taggers  agree on a wrong tag ispract ica l ly  doub led .The agreement  pat tern  d is t r ibut ion  enables us to determine  levels of combinat ionquality.
Table 5 lists both  the accuracies of several  ideal  combiners  (%) and  the errorreduct ion  in re lat ion to the best  base tagger  for the data set in quest ion (/~Err).
22For example ,  on LOB, "Al l  ties correct" p roduces  1,941 errors (cor respond ing  to anaccuracy of 98.31%), wh ich  is 31.3% less than HMM's  2,824 errors.
A min imal  level  ofcombinat ion  ach ievement  is that a major i ty  or better  wi l l  lead to the correct tag andthat ties are hand led  appropr ia te ly  about  50% of the t ime for the (2-2) pat tern  and25% for the (1-1-1-1)  pat tern  (or 33.3% for the (1-1-1) pat tern  for Wotan).
In moreopt imist ic  scenarios,  a combiner  is able to select the correct tag in all t ied cases, oreven in cases where  a two-  or three- tagger  major i ty  must  be overcome.
A l though theposs ib i l i ty  of overcoming  a major i ty  is present  w i th  the arbi ter  type  combiners ,  thes i tuat ion is rather  improbab le .
As a result,  we  ought  to be more  than sat isf ied if anycombiners  approach  the level  cor respond ing  to the pro jected combiner  wh ich  resolvesall ties correctly.
2322 We express the error reduction in the form of a percentage, i.e., a relative measure, instead of by anabsolute value, because we feel this is the more informative of the two.
After all, there is a vastdifference between an accuracy improvement of 0.5% from 50% to 50.5% (a /KEr r of 1%) and one of0.5% from 99% to 99.5% (a /KErr of 50%).23 The bottom rows of Table 5 might be viewed in the light of potential future extremely intelligentcombination systems.
For the moment, however, it is better to view them as containing recall values forn-best versions of the combination taggers, e.g., an n-best combination tagger for LOB, which simplyprovides all tags suggested by its four components, will have a recall score of 99.22%.215Computational Linguistics Volume 27, Number 2Table 5Projected accuracies for increasingly successful levels of combination achievement.
For eachlevel we list the accuracy (%) and the percentage of errors made by the best individual taggerthat can be corrected by combination (AEFr).LOB WSJ Wotan WotanLitePattern % AEr r % AEr r % AEr r % AEr rBest Single Tagger HMM MXP HMM MXP97.55 - 96.88 - 92.06 - 95.56 -Ties randomly 97.77 9.0 96.97 2.8 92.49 5.5 96.01 10.1correct.All ties correct.
98.31 31.3 97.50 19.9 93.04 12.4 96.82 28.3Minority vs. two-tagger 98.48 48.5 97.67 25.4 95.66 45.3 97.09 34.3correct.Minority vs three-tagger 99.22 68.4 98.63 56.0 - - 98.24 60.3correct.Table 6Accuracies of the combination systems on all four corpora.
For each system we list itsaccuracy (%) and the percentage of errors made by the best individual tagger that is correctedby the combination system (A~,).LOB WSJ Wotan WotanLite% AErr % AErr % AErr % AErrBest Single Tagger HMM MXP HMM MXP97.55 - 96.88 - 92.06 - 95.56VotingMajority 97.76 9.0 96.98 3.1 92.51 5.7 96.01 10.1TotPrecision 97.95 16.2 97.07 6.1 92.58 6.5 96.14 12.9TagPrecision 97.82 11.2 96.99 3.4 92.51 5.7 95.98 9.5Precision-Recall 97.94 16.1 97.05 5.6 92.50 5.6 96.22 14.8TagPair 97.98 17.8 97.11 7.2 92.72 8.4 96.28 16.2Stacked ClassifiersWPDV(Tags) 98.06 20.8 97.15 8.7 92.86 10.1 96.33 17.2WPDV(Tags+Word) 98.07 21.4 97.17 9.3 92.85 10.0 96.34 17.5WPDV(Tags+Context) 98.14 24.3 97.23 11.3 93.03 12.2 96.42 19.3MBL(Tags) 98.05 20.5 97.14 8.5 92.72 8.4 96.30 16.7MBL(Tags+Word) 98.02 19.2 97.12 7.6 92.45 5.0 96.30 16.6MBL(Tags+Context) 98.10 22.6 97.11 7.2 92.75 8.7 96.31 16.8DecTrees(Tags) 98.01 18.9 97.14 8.3 92.63 7.2 96.31 16.8DecTrees(Tags+Word) -* .
.
.
.
.
.
.DecTrees(Tags+Context) 98.03 19.7 97.12 7.7 - - 96.26 15.7Maccent(Tags) 98.03 19.6 97.10 7.1 92.76 8.9 96.29 16.4Maccent(Tags+Word) 98.02 19.3 97.09 6.6 92.63 7.2 96.27 16.0Maccent(Tags+Context) 98.12 23.5 97.10 7.0 93.25 15.0 96.37 18.2c5.0 was not able to cope with the large amount of data involved in all Tags+Wordexperiments and the Tags+Context experiment with Wotan.4.3 Results of Combinat ionIn Table 6 the results of our exper iments  wi th  the var ious combinat ion  methods  areshown.
Aga in  we list both  the accuracies of the combiners  (%) and  the error reduct ionin relat ion to the best base tagger (AEr~).
For example,  on LOB, TagPair produces2,321 errors (corresponding to an accuracy of 97.98%), which  is 17.8% less than HMM's2,824 errors.216van Halteren, Zavrel, and Daelemans Combination of Machine Learning SystemsAlthough the combiners generally fall short of the "All ties correct" level (cf.Table 5), even the most trivial voting system (Majority), significantly outperforms thebest individual tagger on all data sets.
Within the simple voting systems, it appearsthat use of more detailed voting weights does not necessarily lead to better esults.TagPrecision is clearly inferior to TotPrecision.
On closer examination, this could havebeen expected.
Looking at the actual tag precision values (see Table 9 below), wesee that the precision is generally more dependent on the tag than on the tagger, sothat TagPrecision always tends to select the easier tag.
In other words, it uses lessspecific rather than more specific information.
Precision-Recall is meant o correct hisbehavior by the involvement of recall values.
As intended, Precision-Recall generallyhas a higher accuracy than TagPrecision, but does not always improve on TotPrecision.Our previously unconfirmed hypothesis, that arbiter-type combiners would be ableto outperform the gang-type ones, is now confirmed.
With the exception of several ofthe Tags+Word versions and the Tags+Context version for WSJ, the more sophisticatedmodeling systems have a significantly better accuracy than the simple voting systemson all four data sets.
TagPair, being somewhere between simple voting and stacking,also falls in the middle where accuracy is concerned.
In general, it can at most besaid to stay close to the real stacking systems, except for the cleanest data set, LOB,where it is clearly being outperformed.
This is a fundamental change from our earlierexperiments, where TagPair was significantly better than MBL and Decision Trees.
Ourexplanation at the time, that the stacked systems uffered from a lack of training data,appears to be correct.
A closer investigation below shows at which amount of trainingdata the crossover point in quality occurs (for LOB).Another unresolved issue from the earlier experiments is the effect of making wordor context information available to the stacked classifiers.
With LOB and a single 114Ktune set (van Halteren, Zavrel, and Daelemans 1998), both MBL and Decision Treesdegraded significantly when adding context, and MBL degraded when adding theword .
24 With the increased amount of training material, addition of the context gener-ally leads to better esults.
For MBL, there is a degradation only for the WSJ data, andof a much less pronounced nature.
With the other data sets there is an improvement,significantly so for LOB.
For Decision Trees, there is also a limited degradation for WSJand WotanLite, and a slight improvement for LOB.
The other two systems appear to beable to use the context more effectively.
WPDV shows a relatively constant significantimprovement over all data sets.
Maccent shows more variation, with a comparableimprovement on LOB and WotanLite, a very slight degradation on WSJ, and a spec-tacular improvement on Wotan, where it even yields an accuracy higher than the "Allties correct" level.
25 Addition of the word is still generally counterproductive.
OnlyWPDV sometimes manages to translate the extra information i to an improvement inaccuracy, and even then a very small one.
It would seem that vastly larger amountsof training data are necessary if the word information is to become useful.5.
Combination in DetailThe observations about the overall accuracies, although the most important, are notthe only interesting ones.
We can also examine the results of the experiments abovein more detail, evaluating the results of combination for specific words and tags, and24 Just as in the current experiments, the Decision Tree system could not cope with the amount  of datawhen the word was added.25 We have no clear explanation for this exceptional behavior, but conjecture that Maccent is able to makeoptimal use of the tagging differences caused by the high error rate of all four taggers.217Computational Linguistics Volume 27, Number 2Table 7Error rates for the most confusing words.
For each word, we list the total number of instancesin the test set (n), the number of tags associated with the word (tags), and then, for each basetagger and WPDV(Tags+Context), the rank in the error list (rank), the absolute number oferrors (err), and the percentage of instances that is mistagged (%).MXP HMM MBT TBL WPDV(T+C)Word n/tags ra"k:err % rank:err % rank:err % rank:err % rank:err %as 719/17 1:102 14.19 1:130 18.08 3:120 16.69 1:167 23.23 1:82 11.40that 1,108/6 2:98 8.84 2:105 9.48 1:130 11.73 2:134 12.09 2:80 7.22to 2,645/9 3:81 2.76 3:59 2.23 2:122 4.61 3:131 4.27 3:40 1.51more 224/4 4:52 23.21 4:42 18.75 4:46 20.54 5:53 23.76 5:30 13.39so 247/10 6:32 12.96 6:40 16.19 6:40 16.19 4:63 25.51 4:31 12.55in 2,102/14 11:22 1.05 7:35 1.67 5:43 2.46 6:48 2.28 6:25 1.19about 177/3 5:37 20.90 5:41 23.16 7:30 16.95 17:23 12.99 7:22 12.43much 117/2 7:30 25.64 l?
:27 23.08 s:27 23.08 9:35 29.91 9:20 17.09her 373/3 lS:13 3.49 21:10 2.68 17:18 4.83 7:39 10.46 25:7 1.88trying to discover why such disappointing results are found for WSJ.
Furthermore, wecan run additional experiments, to determine the effects of the size of the training set,the number of base tagger components involved, and the granularity of the tagset.5.1 Specific WordsThe overall accuracy of the various tagging systems gives a good impression of relativeperformance, but it is also useful to have a more detailed look at the tagging results.Most importantly for this paper, the details give a better feel for the differences betweenthe base taggers and for how well a combiner can exploit these differences.
Moregenerally, users of taggers or tagged corpora are rarely interested in the whole corpus.They focus rather on specific words or word classes, for which the accuracy of taggingmay differ greatly from the overall accuracy.We start our detailed examination with the words that are most often mistagged.We use the LOB corpus for this evaluation, as it is the cleanest data set and hencethe best example.
For each base tagger, and for WPDV(Tags+Context), we list the topseven mistagged words, in terms of absolute numbers of errors, in Table 7.
Althoughthe base taggers have been shown (in Section 4.2) to produce different errors, we seethat they do tend to make errors on the same words, as the five top-sevens togethercontain only nine words.A high number of errors for a word is due to a combination of tagging difficultyand frequency.
Examples of primarily difficult words are much and more.
Even thoughthey have relatively low frequencies, they are ranked high on the error lists.
Wordswhose high error rate stems from their difficulty can be recognized by their higherror percentage scores.
Examples of words whose high error rate stems from theirfrequency are to and in.
The error percentages show that these two words are actuallytagged surprisingly well, as to is usually quoted as a tough case and for in the taggershave to choose between 14 possible tags.
The first place on the list is taken by as, whichhas both a high frequency and a high difficulty level (it is also the most ambiguousword with 17 possible tags in LOB).Table 7 shows yet again that there are clear differences between the base taggers,providing the opportunity for effective combination.
For all but one word, in, thecombiner manages to improve on the best tagger for that specific word.
If we compareto the overall best tagger, HMM, the improvements are sometimes spectacular.
This is218van Halteren, Zavrel, and Daelemans Combination of Machine Learning SystemsTable 8Confusion rates for the tag pairs most often confused.
For each pair (tagger, correct), we firsttake the two possible confusion directions eparately and list the corresponding error listranks (rank) and absolute number of errors (err) for the four base taggers and forWPDV(Tags+Context).
Then we list the same information for the pair as a whole, i~e., for thetwo directions together.MXP HMM MBT TBL WPDV(T+C)Tagger Correct rank err rank err rank err rank err rank errVBN VBD 6 92 1 154 1 205 1 236 3 102VBD VBN 3 118 3 117 3 152 3 149 4 100pair 210 271 357 385 202JJ NN 2 132 2 150 2 168 2 205 2 109NN JJ 1 153 6 75 4 148 4 148 1 110pair 285 225 316 353 219IN CS 4 105 4 93 5 122 s 97 5 79CS IN 10 55 7 70 10 64 6 122 8 48pair 160 163 186 219 127NN VB 5 98 5 78 6 116 5 132 6 59VB NN 25 28 14 45 12 60 7 100 15 35pair 126 123 176 232 94IN RP 7 59 10 61 7 99 12 83 7 50RP 1N 24 30 18 38 27 34 21 42 18 30pair 89 99 133 125 80of course especia l ly  the case where  HMM has par t icu lar  diff icult ies w i th  a word ,  e.g.,about with  a 46.3% reduct ion  in error rate, but  in other  cases as well ,  e.g., to with  a32.2% reduct ion,  wh ich  is stil l wel l  above  the overal l  error  rate reduct ion  of 24.3%.5.2 Specific TagsWe can also abstract  away f rom the words  and  s imply  look at common word  classconfusions,  e.g., a token that shou ld  be tagged VBD (past tense verb) is actua l ly  taggedVBN (past part ic ip le  verb).
Table 8 shows  the tag confus ions  that are present  in thetop seven confus ion list of at least  one of the systems (again the four  base taggersand  WPDV(Tags+Context)  used  on LOB).
The number  on the r ight  in each systemco lumn is the number  of t imes the error was  made and the number  on the left is thepos i t ion  in the confus ion list.
The rows marked  wi th  tag va lues  show the ind iv idua lerrors.
26 In add i t ion ,  the "pa i r "  rows show the combined  va lue  of the two inverseerrors preced ing  it.
27As w i th  the word  errors above,  we  see substant ia l  di f ferences between the basetaggers.
Unl ike the s i tuat ion wi th  words ,  there are now a number  of cases wherebase taggers  per fo rm better  than the combiner.
Partly, this is because the base taggeris outvoted  to such a degree that its qua l i ty  cannot  be mainta ined ,  e.g., NN ---, JJ.Fur thermore ,  it is p robab ly  unfa i r  to look at on ly  one hal f  of a pair.
Any  a t tempt  todecrease the number  of errors of type X --~ Y wi l l  tend to increase the number  of errorsof type  Y --* X.
The balance between the two is best  shown in the "pa i r "  rows,  and26 The tags are: CS = subordinat ing conjunction, IN = preposit ion, JJ = adjective, NN = singularcommon noun,  RP = adverbial particle, VB = base form of verb, VBD = past tense of verb, VBN =past participle.27 RP --~ IN is not  actually in any top seven, but  has been added to complete the last pair  of inverseerrors.219Computational Linguistics Volume 27, Number 2Table 9Precision and recall for tags involved in the tag pairs most often confused.
For each tag, welist the percentage of tokens in the test set that are tagged with that tag (%test), followed bythe precision (Prec) and recall (Rec) values for each of the systems.MXP HMM MBT TBL WPDV(T+C)Tag %test Prec/Rec Prec/Rec Prec/Rec Prec/Rec Prec/RecCS 1.48IN 10.57JJ 5.58NN 13.11RP 0.79VB 2.77VBD 2.17VBN 2.3092.69/90.6997.58/98.9594.52/94.5596.68/97.8595.74/91.8298.04/95.5594.20/95.2294.07/93.2990.14/91.1097.83/98.5994.07/95.6197.91/97.2494.78/92.2797.95/95.9994.23/93.0690.93/93.3789.46/89.0597.14/98.1792.79/94.3896.59/97.2295.26/88.8496.79/94.5592.48/90.2989.59/90.5484.85/91.51 93.11193.3897.33/97.62 98.37199.0390.66/94.06 95.64196.0096.00/96.31 97.66/98.2593.05/90.28 95.95/94.1495.09/93.36 98.13197.0691.74/87.40 95.26/95.1487.09/90.99 94.25194.50Table 10A comparison of benchmark consistency on a small sample of WSJ and LOB.
We list thereasons for differences between WPDV(Tags+Context) output and the benchmark tagging,both in terms of absolute numbers and percentages of the whole test set.WSJ LOBtokens % tokens %Tagger wrong, benchmark right 250 1.97 200 1.75Benchmark wrong, tagger ight 90 0.71 11 0.10Both wrong 7 0.06 1 0.01Benchmark left ambiguous, tagger ight 2 0.02 - -here the combiner is again performing excellently, in all cases improving on the bestbase tagger for the pair.For an additional point of view, we show the precision and recall values of thesystems on the same tags in Table 9, as well as the percentage of the test set thatshould be tagged with each specific tag.
The differences between the taggers are againpresent, and in all but two cases the combiner produces the best score for both preci-sion and recall.
Furthermore, as precision and recall form yet another balanced pair,that is, as improvements in recall tend to decrease precision and vice versa, the re-maining two cases (NN and VBD), can be considered to be handled quite adequatelyas well.5.3 Effects of InconsistencySeeing the rather bad overall performance of the combiners on WSJ, we feel the needto identify a property of the WSJ material that can explain this relative lack of success.A prime candidate for this property is the allegedly very low degree of consistencyof the WSJ material.
We can investigate the effects of the low consistency by way ofcomparison with the LOB data set, which is known to be very consistent.We have taken one-tenth of the test sets of both WSJ and LOB and manuallyexamined each token where the WPDV(Tags+Context) tagging differs from the bench-mark tagging.
The first indication that consistency is a major factor in performance isfound in the basic correctness information, given in Table 10.
For WSJ, there is a muchhigher percentage where the difference in tagging is due to an erroneous tag in thebenchmark.
This does not mean, however, that the tagger should be given a higheraccuracy score, as it may well be that the part of the benchmark where tagger and220van Halteren, Zavrel, and Daelemans Combination of Machine Learning Systemsbenchmark do agree contains a similar percentage of benchmark errors.
It does imply,though, that the WSJ tagging contains many more errors than the LOB tagging, whichis likely to be detrimental to the derivation of automatic taggers.The cases where the tagger is found to be wrong provide interesting informationas well.
Our examination shows that 109 of the 250 erroneous tags occur in situationsthat are handled rather inconsistently in the corpus.In some of these situations we only have to look at the word itself.
The mostnumerous type of problematic word (21 errors) is the proper noun ending in s. Itappears to be unclear whether such a word should be tagged NNP or NNPS.
Whentaking the words leading to errors in our 1% test set and examining them in thetraining data, we see a near even split for practically every word.
The most  frequentones are Securities (146 NNP vs. 160 NNPS) and Airlines (72 NNP vs. 83 NNPS).
Thereare only two very unbalanced cases: Times (78 NNP vs. 6 NNPS) and Savings (76 NNPvs.
21 NNPS).
A similar situation occurs, although less frequently, for common nouns,for example, headquarters gets 67 NN and 21 NNS tags.In other cases, difficult words are handled inconsistently in specific contexts.
Ex-amples here are about in cases such as about 20 (405 IN vs. 385 RB) or about $20 (243IN vs. 227 RB), ago in cases such as years ago (152 IN vs. 410 RB) and more in more than(558 JJR vs. 197 RBR).Finally, there are more general word class confusions, such as adjective/particleor noun/adject ive in noun premodify ing positions.
Here it is much harder to providenumerical examples, as the problematic situation must  first be recognized.
We thereforelimit ourselves to a few sample phrases.
The first is stock-index, which leads to severalerrors in combinations like stock-index futures or stock-index arbitrage.
In the training set,stock-index in premodify ing position is tagged JJ 64 times and NN 69 times.
The secondphrase chief executive officer has three words so that we have four choices of tagging:JJ-JJ-NN is chosen 90 times, J J -NN-NN 63 times, NN-J J -NN 33 times, and NN-NN-NN30 times.Admittedly, all of these are problematic ases and many other cases are han-dled quite consistently.
However,  the inconsistently handled cases do account for 44%of the errors found for our best tagging system.
Under the circumstances, we feelquite justified in assuming that inconsistency is the main cause of the low accuracyscores.
285.4 Size of the Training SetThe most important result that has undergone a change between van Halteren, Zavrel,and Daelemans (1998) and our current experiments i the relative accuracy of TagPairand stacked systems uch as MBL.
Where TagPair used to be significantly better thanMBL, the roles are now well reversed.
It appears that our hypothesis at the time, thatthe stacked systems were plagued by a lack of training data, is correct, since theycan now hold their own.
In order to see at which point TagPair is overtaken, wehave trained several systems on increasing amounts of training data from LOB.
29 Eachincrement is one of the 10% training corpus parts described above.
The results areshown in Figure 5.28 Another property that might contribute to the relatively low scores for the WSJ material is the use of avery small tagset.
This makes annotation easier for human annotators, but it provides much lessinformation to the automatic taggers and combiners.
It may well be that the remaining information isinsufficient for the systems to discover useful disambiguation patterns in.
Although we cannot measurethis effect for WSJ, because of the many differences with the LOB data set, we feel that it has much lessinfluence than the inconsistency of the WSJ material.29 Only combination uses a variable number of parts.
The base taggers are always trained on the full 90%.221Computational Linguistics Volume 27, Number 298.16098.14098,12098.10098.080 f ~ f J4 /,-J99.040 - "~/99.ooo97.94097,92097,900116K 231K 351K 468K 583K 697K 814K 931K 1045K1 2 3 4 5 6 7 8 9Size of Training SetFigure 5The accuracy of combiner methods on LOB as a function of the number of tokens of trainingmaterial.TagPair is only best when a single part is used (as in the earlier experiments).After that it is overtaken and quickly left behind, as it is increasingly unable to usethe additional training data to its advantage.The three systems using only base tagger outputs have comparable accuracygrowth curves, although the initial growth is much higher for WPDV.
The curvesfor WPDV and Maccent appear to be leveling out towards the right end of the graph.For MBL, this is much less clear.
However, it would seem that the accuracy level at1M words is a good approximation of the eventual ceiling.The advantage of the use of context information becomes clear at 500K words.Here the tags-only systems tart to level out, but WPDV(Tags+Context) keeps show-ing a constant growth.
Even at 1M words, there is no indication that the accuracy isapproaching a ceiling.
The model seems to be getting increasingly accurate in correct-ing very specific contexts of mistagging.5.5 Interaction of ComponentsAnother way in which the amount of input data can be varied is by taking subsetsof the set of component taggers.
The relation between the accuracy of combinationsfor LOB (using WPDV(Tags+Context)) and that of the individual taggers is shownin Table 11.
The first three columns show the combination, the accuracy, and theimprovement in relation to the best component.
The other four columns show thefurther improvement gained when adding yet another component.The most important observation is that every combination outperforms the com-bination of any strict subset of its components.
The difference is always significant, ex-cept in the cases MXP+HMM+MBT+TBL vs. MXP+HMM+MBT and HMM+MBT+TBLvs.
HMM+MBT.We can also recognize the quality of the best component as a major factor in thequality of the combination results.
HMM and MXP always add more gain than MBT,which always adds more gain than TBL.
Another major factor is the difference in222van Halteren, Zavrel, and Daelemans Combination of Machine Learning SystemsTable 11WPDV(Tags+Context) accuracy measurements forvarious component tagger combinations.For each combination, we list the tagging accuracy (Test), the error reduction expressed as apercentage of the error count for the best component base tagger (AErr(best)) and anysubsequent error reductions when adding further components (Gain).Gain Gain Gain GainCombination Test AErr(best) +TBL +MBT +MXP +HMMTBL 96.37 - - 29.1 40.2 38.9MBT 97.06 - 12.5 - 28.4 26.0MBT+TBL 97.43 12.5 (MBT) - - 20.6 17.2MXP 97.52 - 12.3 15.0 - 16.2HMM 97.55 - 9.5 11.3 15.3 -HMM+TBL 97.78 9.5 (HMM) - 4.0 11.8 -HMM+MBT 97.82 11.3 (HMM) 2.0 - 13.7 -MXP+TBL 97.83 12.3 (MXP) - 6.0 - 9.9HMM+MBT+TBL 97.87 13.1 (HMM) - - 12.9 -MXP+MBT 97.89 15.0 (MXP) 3.0 - - 10.8MXP+HMM 97.92 15.3 (HMM) 5.7 9.6 - -MXP+MBT+TBL 97.96 17.6 (MXP) - - - 9.1MXP+HMM+TBL 98.04 20.1 (HMM) - 5.2 - -MXP+HMM+MBT 98.12 23.4 (HMM) 1.1 - - -MXP+HMM+MBT+TBL 98.14 24.3 (HMM) .
.
.
.language model.
MXP, although having a lower accuracy by itself than HMM, yetleads to better combination results, again witnessed by the Gain columns.
In somecases, MXP is even able to outperform pairs of components in combination: bothMXP+MBT and MXP+HMM are better than HMM+MBT+TBL.5.6 Effects of GranularityThe final influence on combination that we measure is that of the granularity of thetagset, which can be examined with the highly structured Wotan tagset.
Part of theexamination has already taken place above, as we have added the WotanLite tagset, aless granular projection of Wotan.
As we have seen, the WotanLite taggers undeniablyhave a much higher accuracy than the Wotan ones.
However, this is hardly surprising,as they have a much easier task to perform.
In order to make a fair comparison, wenow measure them at their performance of the same task, namely, the prediction ofWotanLite tags.
We do this by projecting the output of the Wotan taggers (i.e., the basetaggers, WPDV(Tags), and WPDV(Tags+Context)) o WotanLite tags.
Additionally, wemeasure all taggers at the main word class level, i.e., after the removal of all attributesand ditto tag markers.All results are listed in Table 12.
The three major horizontal blocks each representa level at which the correctness of the final output is measured.
Within the lower twoblocks, the three rows represent the type of tags used by the base taggers.
The rowsfor Wotan and WotanLite represent the actual taggers, as described above.
The row forBestLite does not represent a real tagger, but rather a virtual tagger that correspondsto the best tagger from among Wotan (with its output projected to WotanLite format)and WotanLite.
This choice for the best granularity is taken once for each systemas a whole, not per individual token.
This leads to BestLite being always equal toWotanLite for TBL and MBT, and to projected Wotan for MXP and HMM.The three major vertical blocks represent combination strategies: no combination,combination using only the tags, and combination using tags and direct context.
Thetwo combination blocks are divided into three columns, representing the tag level223Computational Linguistics Volume 27, Number 2Table 12Accuracy for base taggers and different levels combiners, as measured at various levels ofgranularity.
The rows are divided into blocks, each listing accuracies for a different comparisongranularity.
Within a block, the individual rows list which base taggers are used as ingredientsin the combination.
The columns contain, from left to right, the accuracies for the base taggers,the combination accuracies when using only tags (WPDV(Tags)) at three different levels ofcombination granularity (Full, Lite, and Main) and the combination accuracies when addingcontext (WPDV(Tags+Context)), atthe same three levels of combination granularity.Base Taggers WPDV(Tags) WPDV(Tags+Context)TBL MBT MXP HMM Full Lite Main Full Lite MainMeasured as Wotan TagsWotan 89.78 91.72 92.06 I 92.83 - - I 93.03 -Measured as WotanLite TagsWotan - 94.56 95.71 95.98 96.50 96.49 - 96.53 96.54 -WotanLite 94.63 94.92 95.56 95.26 - 96.32 - - 96.42 -BestLite 94.63 94.92 95.71 95.98 - 96.58 - - 96.64 -.
mMeasured as Main Word Class TagsWotanWotanLiteBestLite- 96.55 97.23 97.5496.37 96.76 97.12 96.9696.37 96.76 97.23 97.5497.88 97.87 97.85- 97.69 97.71- 97.91 97.9097.88 97.89 97.91- 97.76 97.77- 97.94 97.93at which combination is performed, for example, for the Lite column the output ofthe base taggers is projected to WotanLite tags, which are then used as input for thecombiner.We hypothesized beforehand that, in general, the more information a system canuse, the better its results are.
Unfortunately, even for the base taggers, reality is not thatsimple.
For both MXP and HMM, the Wotan tagger indeed yields a better WotanLitetagging than the WotanLite tagger itself, thus supporting the hypothesis.
On the otherhand, the results for MBT do not confirm this, as here the WotanLite tagger is more ac-curate.
However, we have already seen that MBT has severe problems in dealing withthe complex Wotan data.
Furthermore, the lowered accuracy of the MBL combinerswhen provided with words (see Section 4.3) also indicate that memory-based learningsometimes has problems in coping with a surplus of information.
This means that wehave to adjust our hypothesis: more information is better, but only up to the pointwhere the wealth of information overwhelms the machine learning system.
Where thispoint is found obviously differs for each system.For the combiners, the situation is rather inconclusive.
In some cases, especiallyfor WPDV(Tags), combining at a higher granularity (i.e., using more information) pro-duces better results.
In others, combining at a lower granularity works better.
In allcases, the difference in scores between the columns is extremely small and hardlysupports any conclusions either way.
What is obviously much more important for thecombiners is the quality of the information they can work with.
Here, higher granular-ity on the part of the ingredients is preferable, as combiners based on Wotan taggersperform better than those based on WotanLite taggers, 3?and ingredient performanceseems to be even more useful, as BestLite yields yet better results in all cases.30 However, this comparison is not perfect, as the combination fWotan tags does not include TBL.
Onthe one hand, this means the combination has less information to go on and we should hence be evenmore impressed with the better performance.
Onthe other hand, TBL is the lowest scoring base tagger,so maybe the better performance is due to not having to cope with a flawed ingredient.224van Halteren, Zavrel, and Daelemans Combination of Machine Learning SystemsTable 13A comparison of our results for WSJ with those by Brill and Wu (1998).Brill and Wu Our ExperimentsTraining/Test Split 80/20 Training/Test Split 90/10Unigram 93.26 LexProb 94.57Trigram 96.36 TnT 96.63- MBT 96.41Transformation 96.61 Transformation 96.28Maximum Entropy 96.83 Maximum Entropy 96.88Transformation-based combination 97.16Error rate reduction 10.4%WPDV(Tags+Context) 97.23Error rate reduction 11.3%6.
Related ResearchCombination of ensembles of classifiers, although well-established in the machinelearning literature, has only recently been applied as a method for increasing accuracyin natural language processing tasks.
There has of course always been a lot of researchon the combination of different methods (e.g., knowledge-based and statistical) in hy-brid systems, or on the combination of different information sources.
Some of thatwork even explicitly uses voting and could therefore also be counted as an ensembleapproach.
For example, Rigau, Atserias, and Agirre (1997) combine different heuris-tics for word sense disambiguation by voting, and Agirre et al (1998) do the samefor spelling correction evaluation heuristics.
The difference between single classifierslearning to combine information sources, i.e., their input features (see Roth \[1998\] for ageneral framework), and the combination ofensembles ofclassifiers trained on subsetsof those features is not always very clear anyway.For part-of-speech tagging, a significant increase in accuracy through combiningthe output of different aggers was first demonstrated in van Halteren, Zavrel, andDaelemans (1998) and Brill and Wu (1998).
In both approaches, different agger gen-erators were applied to the same training data and their predictions combined usingdifferent combination methods, including stacking.
Yet the latter paper reported muchlower accuracy improvement figures.
As we now apply the methods of van Halteren,Zavrel, and Daelemans (1998) to WSJ as well, it is easier to make a comparison.
Anexact comparison is still impossible, as we have not used the exact same data prepara-tion and taggers, but we can put roughly corresponding fi ures ide by side (Table 13).As for base taggers, the first two differences are easily explained: Unigram has to dealwith unknown words, while LexProb does not, and TnT is a more advanced trigramsystem.
The slight difference for Maximum Entropy might be explained by the dif-ference in training/test plit.
What is more puzzling is the substantial difference forthe transformation-based tagger.
Possible explanations are that Brill and Wu used amuch better parametrization f this system or that they used a different version of theWSJ material.
Be that as it may, the final results are comparable and it is clear thatthe lower numbers in relation to LOB are caused by the choice of test material (WSJ)rather than by the methods used.In Tufi~ (1999), a single tagger generator is trained on different corpora repre-senting different language registers.
For the combination, a method called credibilityprofiles worked best.
In such a profile, for each component tagger, information iskept about its overall accuracy, its accuracy for each tag, etc.
In another ecent study,Marquez et al (1999) investigate several types of ensemble construction i a decisiontree learning framework for tagging specific classes of ambiguous words (as opposed225Computational Linguistics Volume 27, Number 2to tagging all words).
The construction of ensembles was based on bagging, selectionof different subsets of features (e.g., context and lexical features) in decision tree con-struction, and selection of different splitting criteria in decision tree construction.
Inall experiments, imple voting was used to combine component tagger decisions.
Allcombination approaches resulted in a better accuracy (an error reduction between 8%and 12% on average compared to the basic decision tree trained on the same data).
Butas these error reductions refer to only part of the tagging task (18 ambiguity classes),they are hard to compare with our own results.In Abney, Schapire, and Singer (1999), ADABOOST variants are used for taggingWSJ material.
Component classifiers here are based on different information sources(subsets of features), e.g., capitalization of current word, and the triple "string, cap-italization, and tag" of the word to the left of the current word are the basis forthe training of some of their component classifiers.
Resulting accuracy is comparableto, but not better than, that of the maximum entropy tagger.
Their approach is alsodemonstrated for prepositional phrase attachment, again with results comparable tobut not better than state-of-the-art single classifier systems.
High accuracy on the sametask is claimed by Alegre, Sopena, and Lloberas (1999) for combining ensembles ofneural networks.
ADABOOST has also been applied to text filtering (Schapire, Singer,and Singhal 1998) and text categorization (Schapire and Singer 1998).In Chen, Bangalore, and Vijay-Shanker (1999), classifier combination is used toovercome the sparse data problem when using more contextual information in super-tagging, an approach in which parsing is reduced to tagging with a complex tagset(consisting of partial parse trees associated with lexical items).
When using pairwisevoting on models trained using different contextual information, an error reductionof 5% is achieved over the best component model.
Parsing is also the task to whichHenderson and Brill (1999) apply combination methods with reductions of up to 30%precision error and 6% recall error compared to the best previously published resultsof single statistical parsers.This recent research shows that the combination approach is potentially useful formany NLP tasks apart from tagging.7.
ConclusionOur experiments have shown that, at least for the word class tagging task, combina-tion of several different systems enables us to raise the performance ceiling that can beobserved when using data-driven systems.
For all tested data sets, combination pro-vides a significant improvement over the accuracy of the best component tagger.
Theamount of improvement varies from 11.3% error reduction for WSJ to 24.3% for LOB.The data set that is used appears to be the primary factor in the variation, especiallythe data set's consistency.As for the type of combiner, all stacked systems using only the set of proposedtags as features reach about the same performance.
They are clearly better than sim-ple voting systems, at least as long as there is sufficient raining data.
In the absenceof sufficient data, one has to fall back to less sophisticated combination strategies.Addition of word information does not lead to improved accuracy, at least with thecurrent raining set size.
However, it might still be possible to get a positive effect byrestricting the word information to the most frequent and ambiguous words only.
Ad-dition of context information does lead to improvements for most systems.
WPDVand Maccent make the best use of the extra information, with WPDV having anedge for less consistent data (WSJ) and Maccent for material with a high error rate(Wotan).226van Halteren, Zavrel, and Daelemans Combination of Machine Learning SystemsAlthough the results reported in this paper are very positive, many directions forresearch remain to be explored in this area.
In particular, we have high expectations forthe following two directions.
First, there is reason to believe that better esults can beobtained by using the probability distributions generated by the component systems,rather than just their best guesses (see, for example, Ting and Witten \[1997a\]).
Second,in the present paper we have used disagreement between a fixed set of componentclassifiers.
However, there exist a number of dimensions of disagreement (inductivebias, feature set, data partitions, and target category encoding) that might fruitfullybe searched to yield large ensembles of modular components that are evolved tocooperate for optimal accuracy.Another open question is whether and, if so, when, combination is a worthwiletechnique in actual NLP applications.
After all, the natural language text at hand has tobe processed by each of the base systems, and then by the combiner.
Now none of theseis especially bothersome atrun-time (most of the computational difficulties being expe-rienced during training), but when combining N systems, the time needed to processthe text can be expected to be at least a factor N+ 1 more than when using a single sys-tem.
Whether this is worth the improvement that is achieved, which is as yet expressedin percents rather than in factors, will depend very much on the amount of text that hasto be processed and the use that is made of the results.
There are a few clear-cut cases,such as a corpus annotation project where the CPU time for tagging is negligible inrelation to the time needed for manual correction afterwards (i.e., do use combination),or information retrieval on very large text collections where the accuracy improvementdoes not have enough impact o justify the enormous amount of extra CPU time (i.e.,do not use combination).
However, most of the time, the choice between combining ornot combining will have to be based on evidence from carefully designed pilot experi-ments, for which this paper can only hope to provide suggestions and encouragement.AcknowledgmentsThe authors would like to thank thecreators of the tagger generators andclassification systems used here for makingtheir systems available, and ThorstenBrants, Guy De Pauw, Erik Tjong Kim Sang,Inge de M6nnink, the other members of theCNTS, ILK, and TOSCA research groups,and the anonymous reviewers forcomments and discussion.This research was done while the secondand third authors were at TilburgUniversity.
Their research was done in thecontext of the Induction of LinguisticKnowledge (ILK) research program,supported partially by the NetherlandsOrganization for Scientific Research (NWO).ReferencesAbney, S., R. E. Schapire, and Y. Singer.1999.
Boosting applied to tagging and PPattachment.
In Proceedings ofthe 1999 JointSIGDAT Conference on Empirical Methods inNatural Language Processing and Very LargeCorpora, pages 38-45.Agirre, E., K. Gojenola, K. Sarasola, and A.Voutilainen.
1998.
Towards a singleproposal in spelling correction.
InCOLING-ACL "98, pages 22-28.Alegre, M., J. Sopena, and A. Lloberas.
1999.PP-attachment: A committee machineapproach.
In Proceedings ofthe 1999 JointSIGDAT Conference on Empirical Methods inNatural Language Processing and Very LargeCorpora, pages 231-238.Ali, K. M., and M. J. Pazzani.
1996.
Errorreduction through learning multipledescriptions.
Machine Learning,24(3):173-202.Alpaydin, E. 1998.
Techniques forcombining multiple learners.
In E.Alpaydin, editor, Proceedings ofEngineeringof Intelligent Systems, pages 6-12.Berger, A., S. Della Pietra, and V. DellaPietra.
1996.
A maximum entropyapproach to natural language processing.Computational Linguistics, 22(1):39-71.Berghmans, J.
1994.
Wotan, eenautomatische grammatikale tagger voorhet Nederlands.
Master's thesis, Dept.
ofLanguage and Speech, University ofNijmegen.Brants, T. 2000.
TnT--A statisticalpart-of-speech tagger.
In Proceedings oftheSixth Applied Natural Language Processing227Computational Linguistics Volume 27, Number 2Conference, (ANLP-2000), pages 224-231,Seattle, WA.Breiman, L. 1996a.
Bagging predictors.Machine Learning, 24(2):123-140.Breiman, L. 1996b.
Stacked regressions.Machine Learning, 24(3):49-64.Brill, E. 1992.
A simple rule-basedpart-of-speech tagger.
In Proceedings oftheThird ACL Conference on Applied NLP,pages 152-155, Trento, Italy.Brill, E. 1994.
Some advances intransformation-based part-of-speechtagging.
In Proceedings ofthe TwelfthNational Conference on Artificial Intelligence(AAAI '94).Brill, E. and Jun Wu.
1998.
Classifiercombination for improved lexicaldisarnbiguation.
I  COLING-ACL '98: 36thAnnual Meeting of the Association forComputational Linguistics and 17thInternational Conference on ComputationalLinguistics, pages 191-195, Montreal,Quebec, Canada.Chan, P. K., S. J. Stolfo, and D. Wolpert.1999.
Guest editors' introduction.
SpecialIssue on Integrating Multiple LearnedModels for Improving and Scaling MachineLearning Algorithms.
Machine Learning,36(1-2):5-7.Charniak, E. 1993.
Statistical LanguageLearning.
MIT Press, Cambridge, MA.Chen, J., S. Bangalore, and K. Vijay-Shanker.1999.
New models for improving supertagdisambiguation.
I  Proceedings ofthe 1999Joint SIGDAT Conference on EmpiricalMethods in Natural Language Processing andVery Large Corpora, pages 188-195.Cherkauer, K. J.
1996.
Human expert-levelperformance on a scientific image analysistask by a system using combined artificialneural networks.
In P. Chan, editor,Working Notes of the AAAI Workshop onIntegrating Multiple Learned Models,pages 15-21.Church, K. W. 1988.
A stochastic partsprogram and noun phrase parser forunrestricted text.
In Proceedings oftheSecond Conference on Applied NaturalLanguage Processing.Daelemans, W., A.
Van den Bosch, and A.Weijters.
1997.
IGTree: Using trees forcompression and classification i lazylearning algorithms.
Artificial IntelligenceReview, 11:407-423.Daelemans, W., J. Zavrel, P. Berck, and S.Gillis.
1996.
MBT: A memory-based partof speech tagger generator.
In E. Ejerhedand I. Dagan, editors, Proceedings oftheFourth Workshop on Very Large Corpora,pages 14-27.
ACL SIGDAT.Daelemans, W., J. Zavrel, K. Van der Sloot,and A.
Van den Bosch.
1999.
TiMBL:Tilburg memory based learner, version2.0, reference manual.
Technical ReportILK-9901, ILK, Tilburg University.Dehaspe, L. 1997.
Maximum entropymodeling with clausal constraints.
InInductive Logic Programming: Proceedings ofthe 7th International Workshop (ILP-97),Lecture Notes in Artificial Intelligence, 1297,pages 109-124.
Springer Verlag.DeRose, S. 1988.
Grammatical categorydisambiguation by statisticaloptimization.
Computational Linguistics,14:31-39.Dietterich, T. G. 1997.
Machine learningresearch: Four current directions.
AIMagazine, 18(4):97-136.Dietterich, T. G. 1998.
Approximatestatistical tests for comparing supervisedclassification learning algorithms.
NeuralComputation, 10(7):1895-1924.Dietterich, T. G. and G. Bakiri.
1995.
Solvingmulticlass learning problems viaerror-correcting output codes.
Journal ofArtificial Intelligence Research, 2:263-286.Freund, Y. and R. E. Schapire.
1996.Experiments with a new boostingalgorithm.
In L. Saitta, editor, Proceedingsof the 13th International Conference onMachine Learning, ICML '96, pages 148-156,San Francisco, CA.
Morgan Kaufmann.Geerts, G., W. Haeseryn, J. de Rooij, and M.van der Toorn.
1984.
Algemene NederlandseSpraakkunst.
Wolters-Noordhoff,Groningen and Wolters, Leuven.Golding, A. R. and D. Roth.
1999.
Awinnow-based approach tocontext-sensitive spelling correction.Machine Learning, 34(1-3):107-130.Henderson, J. and E. Brill.
1999.
Exploitingdiversity in natural anguage processing:Combining parsers.
In Proceedings ofthe1999 Joint SIGDAT Conference on EmpiricalMethods in Natural Language Processing andVery Large Corpora, pages 187-194.Johansson, S. 1986.
The Tagged LOB Corpus:User's Manual.
Norwegian ComputingCentre for the Humanities, Bergen,Norway.Marcus, M., B. Santorini, and M. A.Marcinkiewicz.
1993.
Building a largeannotated corpus of English: The PennTreebank.
Computational Linguistics,19(2):313-330.Marquez, L., H. Rodrfguez, J. Carmona, andJ.
Montolio.
1999.
Improving POS taggingusing machine-learning techniques.
InProceedings ofthe 1999 Joint SIGDATConference on Empirical Methods in NaturalLanguage Processing and Very Large Corpora,pages 53-62.228van Halteren, Zavrel, and Daelemans Combination of Machine Learning SystemsQuinlan, J. R. 1993. c4.5: Programs forMachine Learning.
Morgan Kaufmann, SanMateo, CA.Ratnaparkhi, A.
1996.
A maximum entropymodel for part-of-speech tagging.
InProceedings ofthe Conference on EmpiricalMethods in Natural Language Processing,pages 133-142, University ofPennsylvania.Rigau, G., J. Atserias, and E. Agirre.
1997.Combining unsupervised lexicalknowledge methods for word sensedisambiguation.
In Proceedings ofACL/EACL "97, pages 48-55.Roth, D. 1998.
Learning to resolve naturallanguage ambiguities: A unifiedapproach.
In Proceedings ofAAAI '98,pages 806-813.Samuelsson, Christer.
1996.
Handling sparsedata by successive abstraction.
InProceedings ofthe 16th InternationalConference on Computational Linguistics,COLING-96, Copenhagen, Denmark.Schapire, R. E. and Y.
Singer.
1998.BoosTexter: A system for multiclassmulti-label text categorization.
TechnicalReport, AT&T Labs.
To appear in MachineLearning.Schapire, R. E., Y.
Singer, and A. Singhal.1998.
Boosting and Rocchio applied totext filtering.
In Proceedings ofthe 21stAnnual International Conference on Researchand Development in Information Retrieval,(SIGIR "98), pages 215-223.Ting, K. M. and I. H. Witten.
1997a.
Stackedgeneralization: When does it work?
InInternational Joint Conference on ArtificialIntelligence, Japan, pages 866-871.Ting, K. M. and I. H. Witten.
1997b.Stacking bagged and dagged models.
InInternational Conference on MachineLearning, Tennessee, pages 367-375.Tufi~, D. 1999.
Tiered tagging and combinedlanguage models classifiers.
In ProceedingsWorkshop on Text, Speech, and Dialogue.Uit den Boogaart, P. C. 1975.Woordfrequenties in geschreven  gesprokenNederlands.
Utrecht, Oosthoek, Scheltema& Holkema.van Halteren, H. 1996.
Comparison oftagging strategies, a prelude todemocratic tagging.
In S. Hockey andN.
Ide, editors, Research in HumanitiesComputing 4.
Selected Papers from theALLC/ACH Conference, Christ Church,Oxford, April 1992, pages 207-215, Oxford,England.
Clarendon Press.van Halteren, H., editor.
1999.
SyntacticWordclass Tagging.
Kluwer AcademicPublishers, Dordrecht, The Netherlands.van Halteren, H. 2000a.
A default first orderweight determination procedure forWPDV models.
In Proceedings ofCoNLL-2000 and LLL-2000, pages 119-122,Lisbon, Portugal.van Halteren, H. 2000b.
Chunking withWPDV models.
In Proceedings ofCoNLL-2000 and LLL-2000, pages 154-156,Lisbon, Portugal.van Halteren, H., J. Zavrel, and W.Daelemans.
1998.
Improving data-drivenwordclass tagging by systemcombination.
In COLING-ACL "98: 36thAnnual Meeting of the Association forComputational Linguistics and 17thInternational Conference on ComputationalLinguistics, pages 491--497, Montreal,Quebec, Canada.Wolpert, D. H. 1992.
Stacked generalization.Neural Networks, 5:241-259.Zavrel, J. and W. Daelemans.
2000.Bootstrapping a tagged corpus throughcombination of existing heterogeneoustaggers.
In Proceedings ofthe SecondInternational Conference on LanguageResources and Evaluation (LREC 2000),pages 17-20.229
