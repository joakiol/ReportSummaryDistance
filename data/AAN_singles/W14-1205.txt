Proceedings of the 3rd Workshop on Predicting and Improving Text Readability for Target Reader Populations (PITR) @ EACL 2014, pages 38?46,Gothenburg, Sweden, April 26-30 2014.c?2014 Association for Computational LinguisticsAn eye-tracking evaluation of some parser complexity metricsMatthew J. GreenUniversity of Aberdeen, UKmjgreen@abdn.ac.ukAbstractInformation theoretic measures of incre-mental parser load were generated from aphrase structure parser and a dependencyparser and then compared with incremen-tal eye movement metrics collected for thesame temporarily syntactically ambiguoussentences, focussing on the disambiguat-ing word.
The findings show that thesurprisal and entropy reduction metricscomputed over a phrase structure gram-mar make good candidates for predictorsof text readability for human comprehen-ders.
This leads to a suggestion for the useof such metrics in Natural Language Gen-eration (NLG).1 IntroductionThis work aims to predict automatically how dif-ficult a generated sentence will be for a per-son to read.
Temporarily syntactically ambigu-ous sentences were presented along with pre-disambiguated controls to people to read whiletheir eye movements were recorded.
The samematerials were given as input to two NLP parsers,trained on portions of the Wall Street Journal partof the Penn Treebank, that generate incrementalword by word metrics of parser load.
The met-rics of parser load were compared against a stan-dard measure of human sentence processing loadregression path duration.The purpose of the present article is to demon-strate that the parser metrics can predict humandifficulty for a certain syntactically-ambiguoussentence type (described in the next section).
Thearticle also proposes that, if future work shows thatthe parser metrics here also predict sentence pro-cessing difficulty more broadly, then this methodwould be a useful way for NLG systems to decideon a particular output from among several possibleoutputs that express the same information.2 Complement ambiguityThe sentences used in this article were represen-tative of complement ambiguity.
Sentences likethese are syntactically ambiguous until a disam-biguating word, which resolves the ambiguity ei-ther to no complement, direct object complement,or sentential complement.
This section gives thelinguistic aspects of this ambiguity type with ex-amples.
Material in parentheses indicates howthe unambiguous controls were constructed: bymeans of punctuation indicating the clause bound-ary in (1); and by means of an overt complemen-tiser establishing the sentential complement in (2).Phrase marker diagrams are given for the exam-ples in Figures (1) and (2).
(1) After the cadet saluted(,) the captain walkedto the gates of the enclosure.
SENTENCETYPE 1(2) The cadet noticed (that) the captain walkedto the gates of the enclosure.
SENTENCETYPE 2Sentential complement ambiguities exploit theproperties of ?complement?
verbs like noticed thatcan be followed either by a complement clause orby a direct object, or by no complement.
Whensuch verbs are followed by complements and anovert complementiser like that is used, no tem-porary syntactic ambiguity is present: however,when the complementiser is omitted, which maybe done without violating the grammar, temporarysyntactic ambiguity arises with respect to the firstfew words of the complement.
These words maybe taken as a direct object instead, and then whenthe complement verb appears, disambiguation en-sues as the words that were taken to be part of adirect object of the verb are revealed necessarilyto be part of a complement.
Another possibilityafforded by the multiple subcategorisation frame38of words like noticed is that the words immedi-ately following could properly be the start of amain clause where the clause containing noticedis properly a subordinate clause.
Such cases aresometimes referred to as reduced complements.In these cases only the presence of a main verbresolves the temporary syntactic ambiguity, andwhen it appears, some major restructuring is in-volved.
Complement ambiguities of both kindshave been used to investigate the parsing of am-biguous clauses (Holmes et al., 1987; Rayner andFrazier, 1987; Sturt et al., 1999; Ferreira andHenderson, 1991; Clifton Jr, 1993; Pickering andTraxler, 1998; Trueswell et al., 1993).Evidence from studies with human readers sup-port the notion that there is a processing difficultydifferential across the two forms such that dis-ambiguation in sentence type (1) is harder thanin sentence type (2).
This has been shown usinggrammaticality judgements (Ferreira and Hender-son, 1991), self-paced reading times (Sturt et al.,1999), and eye-tracking (Green, 2014).The current article presents an eye-trackingevaluation of the parser predictions for comple-ment ambiguity, and discusses applications of syn-tactic complexity metrics for evaluating test read-ability.3 Parser metricsThis section gives details of the surprisal, entropyreduction, and retrieval time metrics, and how theyare computed.3.1 SurprisalSurprisal was computed over a phase structureparser, and over a dependency parser.Surprisal is computed using two other quanti-ties.
These quantities are: (1) the probability of aderivation: a derivation is a set of weighted ruleproductions that result in the current partial stringof input words, such that a sentence fragment withtwo alternative parses is represented as two deriva-tions; (2) prefix probability: this is the probabilityof the parse of the fragment seen so far, which iscomposed of the sum of the probabilities of thetwo derivations if the fragment is syntactically am-biguous with two alternatives.Let G be a probabilistic context free grammar(PCFG).
Let d be a derivation composed of a se-quence of applications of grammar rules.
Let i in-dex these applications so that diis the ith applica-tion in d, and let j be the total number of applica-tions in the derivation.
Then the probability p ofa derivation d given a grammar G and the currentsentence fragment w1...kis given by the product ofthe probability of each rule applied in the deriva-tion, thus:p(d,G,w1...k) =j?i=1p(di, G,w1...k)Let D represent the set of all derivations d thatare present for the current sentence fragment ?when there are two alternative parses available forthe sentence fragment seen so far, D has two ele-ments.
Let w be the set of words in the sentencefragment seen so far.
Let wkbe the word thatthe parser encountered most recently at the currentstate.
Let wk+1be the first word of the rest of thesentence.
As the parser transitions from its state atwkto its state at wk+1we can derive a prefix prob-ability pp at wk+1that represents the sum proba-bility of the derivations of the string w1...k+1.
Sothe prefix probability of word wk+1with respectto a probabilistic context free grammar (PCFG) de-noted G is given by the sum of the probability ofall derivations of the string w1...k+1that the gram-mar generates.pp(wk+1, G,w1...k) =?d?Dp(d,G,w1...k)The conditional probability cp of the next wordwk+1is the ratio of the prefix probability of thenext word wk+1to the prefix probability of thecurrent word wk.cp(wk+1, G,w1...k) =pp(wk+1, G,w1...k)pp(wk, G,w1...k?1)The surprisal sp, measured in bits of informa-tion, associated with the next word wk+1is thenegative log of the conditional probability of thenext word wk+1sp(wk+1, G,w1...k) = ?log(cp(wk+1, G,w1...k))The TDPARSE top-down incremental parser pro-vided by Roark (2013) and described in Roark(2001) and Roark (2004) computes surprisal overa phrase structural grammar, incrementally foreach word in a sentence.
It is a parallel parser thatmaintains potentially very many parses at eachstate.
For details of how the beam width variesacross a sentence, see Roark (2001).39SADVP S'IN S'After NP VPthe cadet VsalutedNPthe captainSADVP S'IN S' NP VPAfter NP VP the captain walked ...the cadet VsalutedFigure 1: Phrase markers showing disambiguation in sentence type 1.
The left phrasemarker shows theinitial misattachment.
The right phrasemarker shows how the same initially misattached NP is attachedin the ultimately correct analysis.SNP VPThe cadet V NPnoticed the captainSNP VPthe cadet V S'noticed NP VPthe captain walked ...Figure 2: Phrase markers showing disambiguation in sentence type 2.
The left phrasemarker shows theinitial misattachment.
The right phrasemarker shows how the same initially misattached NP is attachedin the ultimately correct analysis.The HUMDEP parser provided by Boston(2013) and described in Boston and Hale (2007)and Boston (2012) computes surprisal over a de-pendency grammar transition system , incremen-tally for each word in a sentence.
It is a k-bestparser.
Here the value of k was set to 3, in linewith previous use of the parser to model humandisambiguation performance for garden-path sen-tences in Boston and Hale (2007).Hypothesis 1 Hale (2001), and also Levy(2008), gave the hypothesis that surprisal is lin-early related to the human effort of processing aparticular word in a sentence fragment.
This hy-pothesis casts disambiguation as the work incurredby disconfirming all parses of the fragment that areinconsistent with the fragment including the dis-ambiguating word.3.2 Entropy reductionEntropy reduction was computed over the outputof the phrase structure parser TDPARSE.
In gen-eral, the entropy (Shannon, 1948), denoted H, of arandom variable is the uncertainty associated withthat variable.
Specifically, for a discrete randomvariable X with outcomes x1, x2, .
.
.
with proba-bilities p1, p2, .
.
.H(X) = ?
?x?Xpxlog2pxPutting this in sentence processing terms, let Dbe a set of derivations for a sentence fragment Wand let X be the extended sentence fragment thatresults from adding a new word to the fragment.H(G, D,W ) = ?
?prp(G, X)log(prp(G, X))The quantity entropy reduction is defined with alower bound of zero so that this quantity is never40negative:ER = max(0,H(D|w1...k)?H(D|w1...k+1))Hypothesis 2 Hale (2004) and Hale (2006) gavethe entropy reduction hypothesis that the humaneffort of processing a particular word in a sentencefragment is the reduction in entropy from its valuegiven the fragment to its value given the fragmentincluding the disambiguating word.3.3 Retrieval timeParsing in retrieval theory (Lewis and Vasishth,2005) is accomplished by condition-action pairsgenerated with reference to a phrase structuregrammar.
A series of memory buffers stores el-ements in short-term and long-term buffers.
Par-allel associative retrieval (McElree et al., 2003),fluctuation of activation of elements already ina memory buffer, and retrieval interference as afunction of similarity are combined to predict theamount of time that it takes to read a word (Va-sishth et al., 2008).A word?s activation is based on two quantities:the baseline activation of the word, which is takento decay given the passage of time; and the amountof similarity based interference with other wordsthat have been parsed.
The baseline activation Bfor a word i is given here, taken from Lewis andVasishth (2005), and Patil et al.
(2009), where tris the time since the rth retrieval of the word, thesummation is over all n retrievals, and d is a decayfactor set to 0.5 as in other ACT-R models (Ander-son, 2005).Bi= ln(n?r=1tr?
d)The equation tracks the log odds that a word willneed to be retrieved, given its past usage history.It yields not a smoothly decaying activation frominitial encoding to the current time, but a ?seriesof spikes corresponding to the retrieval events?
(Lewis and Vasishth, 2005).The overall activationA for word i is given hereAi= Bi+?jWjSjifrom Lewis and Vasishth (2005).
In this equation,Biis the fluctuating baseline level of activation forword iwhich is subject to time-based decay.
In themodel, a goal buffer contains retrieval cues for in-tegrating the current word.
Overall activation Afor word i is found by adding to the baseline ac-tivation for word i an associative activation boostreceived from retrieval cues in the goal buffer thatare associated with i.
The variable j indexes thoseretrieval cues in the goal buffer.
Wjs are weightson the retrieval cues in the goal buffer.
The weighton a retrieval cue represents the proportion of thetotal activation available for the whole goal bufferthat is assigned to the particular retrieval cue j inthe goal buffer.
Sjis are the strengths of associ-ation from each retrieval cue j of the goal bufferto word i.
This equation is effectively adding tothe baseline activation an activation boost receivedfrom retrieval cues in the goal buffer.The amount of similarity based interference isestimated by the weighted strengths of associationbetween the word to be retrieved and retrieval cuesfrom other words already parsed and with a tracein memory.
In the following equation, word i isthe current word, and retrieval cue j is from aword that is similar to word i, with reference toits part of speech tag, so that nouns interfere withother nouns but not with verbs.
If retrieval cue jis similar to word i then the amount by which re-trieval cue j interferes with word i varies accord-ing to how many words have already been associ-ated with retrieval cue j.
The array of words thatis associated with retrieval cue j is considered toform a fan so that fanjgives the number of wordsin the fan for cue j.
The constant S refers to themaximum associative strength of 1.5 (Lewis andVasishth, 2005).Sji= S ?
ln(fanj)This equation is effectively reducing the maxi-mum associative strength S by the log of the ?fan?of cue j, that is, the number of items associatedwith j.The mapping from activation level to retrievaltime is given next.
F is a scaling constant setto 0.14 in Lewis and Vasishth (2005).
Aiis theword?s activation and e is Euler?s constant.
Tiisretrieval time for word i:Ti= FeAiThe retrieval time measure comes from Lewisand Vasishth (2005) where a theory of sentenceprocessing is expressed as set of processes cor-responding with skilled retrievals of linguisticcomponents from memory.
However in that pa-per it is computed over a phrase structure gram-41mar.
Boston provides a method to compute re-trieval time over a dependency grammar in theHUMDEP3.0 parser and Boston?s method (Boston,2013) is used here.Hypothesis 3 Retrieval time is related to humansentence processing difficulty.4 Eye movement metricsThis section gives the metrics used to index hu-man sentence processing load at disambiguation.Rayner et al.
(2012, p. 93) set out the most com-mon eye tracking measures.
These include the fol-lowing measures: First Fixation Duration (FFD);First Pass Reading Time (FPRT); Regression PathDuration (RPD).
These are defined next.
First fixa-tion duration (FFD) is the mean duration of the firstfixation on a word regardless of other possible fix-ations on the word.
It has traditionally been treatedas a measure of early processing.
First fixation du-ration is interpreted to index lexical access.
Firstpass reading time (FPRT): also known as gaze du-ration, is the sum of the durations of all fixationson the word that occur before leaving the word inany direction.
This still captures the early pro-cessing (FFD is a subset of FPRT) but FPRT alsoincludes any refixations that there might be on theword before a regression is launched from it.
Firstpass reading time is often interpreted to index lex-ical integration into the phrase marker.
Regressionpath duration (RPD) includes FPRT but adds to itthe durations of fixations on preceding words thatthe eyes regress to before leaving the word to theright to take in new material, as well as any refix-ations on the launch word that occur before newmaterial is taken in.
In this way RPD is sensitiveto integration difficulties that yield regressive eyemovements but it also includes early processing.Regression path duration is often interpreted to in-dex incremental syntactic integration of the newword into the sentence?s representation includingany semantic problems that arise from this.Since RPD is the measure most sensitive to syn-tactic disambiguation, it is used in this article asa measure that is representative of human parsingload at disambiguation.5 MethodThis section tells how the eye tracking experimentwas carried out.Participants were forty native speakers ofBritish English who were students of Psychologyat the University of Exeter and who participatedfor course credit.
All had normal or corrected-t-normal vision, were naive as to the purpose ofthe experiment, aged between eighteen and thirty-four.Apparatus used was an SR Research EyeLink IIhead-mounted eyetracker.
This recorded partici-pants?
eye movements with a sampling rate of 500Hz while they read sentences displayed on a 19inch Iiyama Vision Master Pro monitor at 1024 x768 resolution at a refresh rate of 60 Hz.
Viewingwas binocular but only the right eye was recorded.Participants sat in a dimly lit room in front of thecomputer at a viewing distance of approximately75 cm the average viewing distance was approx-imately 75 cm.
At this viewing distance, and as-suming that 1 character had 2 mm width on screen,a single character subtended 0.153 degrees of vi-sual angle, and approximately 6.5 characters sub-tended 1 degree of visual angle.
The font usedwas Courier New 12 point.
All sentences in thisexperiment were displayed on a single line with amaximum length of 100 characters.
A 9 point cali-bration procedure was used, on which participantswere required to achieve a score of ?good?.
Eachtrial started with a drift correction routine wherethe participant was required to fixate a target thatappeared in the same location as the first characterof the sentence would subsequently occupy, andthen required to press a button on the gamepadwhile fixating this point to start the trial.Participants were instructed to read silently forcomprehension at a comfortable speed.
The prac-tice trials and experimental trials were imple-mented as separate consecutive blocks.
The ex-perimental trials were randomised by ExperimentBuilder each time the experiment was run, i.e.,in a different order for each participant, withthe constraint that a maximum of two trials of agiven type could appear in a continuous sequence.There were four practice sentences, followed by adrift correction routine preceding the experimen-tal block containing 96 sentences, comprising 24in experimental conditions (6 in each of 4 con-ditions); 24 foils (sentences that contained com-plement ambiguities that resolved to NP) and 48fillers (sentences that did not contain complementambiguity).
Participants were rotated over oneof four lists, implementing a Latin square design.32 of the trials (including 8 of the experimentalconditions) were followed immediately by a com-42prehension question.
This was a simple questionabout the sentence immediately preceding that re-quired the participant to make a yes or no re-sponse using the appropriate trigger button on thegamepad.
The whole procedure took about 20 to40 minutes, depending on the participant.6 ResultsThis section shows how the comparisons weremade between patterns of differential processingload at disambiguation in the parser metrics andthe human metrics.
Per-condition means of allmetrics at the disambiguating word are given inFigure 3.6.1 Regression path duration (RPD)A linear mixed effects model (Bates et al., 2013)was constructed for regression path duration at thedisambiguating word i.e., walked in the examplesentences.
RPD was modeled as a function ofword length, word (unigram) frequency (Brantsand Franz, 2006), ambiguity, and sentence type(type 1 is exemplified in sentence 1 and type 2is exemplified in sentence 2), and the ambiguityx sentence type interaction; with random slopesfor the ambiguity x sentence type interaction overboth participant ID and over item ID.
Word lengthand word frequency both exerted non-significantinfluences.
There was a significant effect of am-biguity with the ambiguous conditions leading to146 ms more RPD than the disambiguated condi-tions (?
= 135.15, SE = 37.60, t = 3.56).There was a significant disadvantage for type 1sentences of 79 ms as a main effect (?
= ?68.59,SE = 30.66, t = ?2.27).
There was significantinteraction effect such that the effect of ambigu-ity in type 1 sentences was greater than the effectof ambiguity for type 2 sentences (?
= ?64.28,SE = 31.33, t = ?2.05).6.2 Phrase structure surprisalPhrase structure surprisal predicted that the am-biguous cases would be harder then the unambigu-ous cases; and that the disadvantage of sentencetype 1 in the ambiguous cases would turn aroundinto a disadvantage of sentence type 2 in the unam-biguous conditions.
Individual terms for ambigu-ity and sentence type were included at each levelof item.
Effects of ambiguity, sentence type andthe ambiguity x sentence type interaction were allsignificant in the model, and the shapes of theseeffects were broadly in line with the human data(?
= 0.65, SE = 0.05, t = 12.32, ?
= ?0.11,SE = 0.03, t = ?3.25, and ?
= ?0.35, SE =0.01, t = ?62.35 respectively).6.3 Phrase structure entropy reductionThe directions of the entropy reduction hypothesispredictions were the same as for phrase structuresurprisal, although there was a relatively greaterdifficulty with the type 2 cases versus surprisal.Effects of ambiguity, sentence type and the am-biguity x sentence type interaction were all sig-nificant in the model (?
= 0.32, SE = 0.02,t = 14.04, ?
= ?0.03, SE = 0.02, t = ?2.05,and ?
= ?0.17, SE = 0.002, t = ?55.79 respec-tively).
The shapes of these effects were broadlyin line with the human data.6.4 Dependency surprisalThe mean values of dependency surprisal at thedisambiguating word show that ambiguous sen-tence types 1 and 2 are predicted to be equal.
Forthe unambiguous cases, type 1 is predicted to bemore difficult than type 2.
Ambiguity did notexert a significant effect on dependency surprisal(?
= 0.0002, SE = 0.01, t = 0.01).
The ef-fect of sentence type was significant, with type1 causing more dependency surprisal than type 2(?
= ?0.09, SE = 0.01, t = ?6.26).
The am-biguity x sentence type interaction was significantin the model (?
= 0.09, SE = 0.002, t = 39.67)but the shape of the interaction did not match theshape of the human data: instead the model pre-dicted a large effect of sentence type in the unam-biguous conditions and a small effect of sentencetype in the unambiguous control sentences.6.5 Dependency retrieval timeThe mean values for retrieval predicted that bothof the ambiguous sentence types and unambigu-ous type 1 sentences should be equally difficult,with unambiguous type 1 predicted to cause themost difficulty.
Main effects of ambiguity andsentence type were significant in the model (?
=?17.7, SE = 0.60, t = ?29.72 and ?
= 17.7,SE = 0.6, t = 29.72 respectively).
There wasa significant ambiguity x sentence type interaction(?
= ?17.7, SE = 0.09, t = ?191.25).
Compar-ing these prediction with the human data, the pre-dictions are not in line with human performance atall.43lllllllllllllllllRPD TDP Surprisal TDP E.R.
DEP Surprisal DEP ACTR3003504004505008.59.09.510.01.82.02.22.42.61.11.21.31.4220240260280ambig control ambig control ambig control ambig control ambig controlllsentence type 1sentence type 2Figure 3: Per-condition means for each metric for the disambiguating word.
RPD is the human eyemovement measure regression path duration, see section 6.1.
TDP Surprisal is surprisal computed overa phrase structure grammar, see section 6.2.
TDP E.R.
is entropy reduction computed over a phrasestructure grammar, see section 6.3.
DEP Surprisal is surprisal computed over a dependency grammar,section 6.4; DEP ATCR is retrieval time computed over a dependency grammar, section 6.5.7 ConclusionsThis section lays out the the conclusions that canbe drawn from this work, paying attention to thequestion whether an information theoretic mea-sure can be used in the NLG process as a proxyfor human reading difficulty, as part of an effort togenerate more readable texts.For the metrics computed over a phrase struc-ture grammar (phrase structure surprisal and phasestructure entropy reduction), the comparison withhuman eye tracking metrics is relatively close.This suggests that phrase structure surprisal andphase structure entropy reduction are tracking hu-man reading difficulty at disambiguation well.Phrase structure surprisal and phase structure en-tropy reduction are good predictors of the sort ofhuman parsing difficulty that is measured by re-gression path duration, for these sentence types.Dependency surprisal computed over a depen-dency grammar using a k-best parser with k=3produces the wrong predictions for the comple-ment ambiguity sentence types in this article.There is some scope for improving the predic-tions of this parser, as follows.
Firstly settingk=3 may be restricting the beam width too muchsuch that the ultimately-correct analysis is prunedtoo early.
If so, simulations with increased val-ues of k might be worth exploring.
Secondly, oneof the sentence types in this article relies on dis-ambiguation by punctuation.
Punctuation is well-handled in phrase structural grammars because itserves as a clause boundary marker, and phrasestructure grammars natively express sentences asphrase combinations, whereas dependency gram-mars can only treat punctuation as a terminal in itsown right.
This might turn out to lead to an un-fair comparison between dependency parser per-formance and phrase structure performance for thesentence types examined here.
There is a clearcase for examining dependency parsing for disam-biguation types that use the sequence of words toeffect disambiguation.
Future work in this direc-tion could take advantage of previous work withdifferent ambiguities covered in e.g., Boston andHale (2007) and Boston (2012), and extendingit from using self-paced reading times to includeeye-tracking metrics.Dependency retrieval time did not show the in-teraction evident in the eye movement and phasegrammar parser data.
This suggests either that theLewis and Vasishth (2005) model does not coververy well the sentence types used in this experi-ment, or that whatever coverage the Lewis and Va-sishth (2005) model does have of the human data isobscured in the transformation from phrase struc-ture grammar to dependency grammar versions ofretrieval.Previous work aimed at broad-coverage parsingevaluated against human eye movement corpora(Demberg and Keller, 2008; Boston et al., 2011)indicates that, in those corpus-derived linguisticenvironments, phrase structure surprisal and phasestructure entropy reduction account for differentcomponents of variance in eye movement patterns.If future work continues to find that surprisal andentropy reduction predict human difficulty in psy-cholinguistic eye movement lab-based investiga-tions (and the present paper shows how that canbe done for one ambiguity type), then it will bereasonable to propose that a good model of sen-tence processing should use both surprisal and en-tropy reduction to predict (human) reading diffi-culty.
Such a model would need to consider care-44fully the nature of the relationship between thesedifferent types of parser complexity.
A startingpoint could be the observation that surprisal is es-sentially backwards-looking (seeks to disconfirmpast analyses) whereas entropy reduction is essen-tially forward-looking (seeks to establish the un-certainty that remains at the current word with re-spect to how the rest of the sentence might panout).For NLG, the importance of this proposal is thatsuch a model could be used to answer, algorith-mically, questions that have previously only beensatisfactorily answered in the laboratory.
For ex-ample, in NLG the question often arises ?For thisproposition P , which we want the generator to putin a surface form SF for some given natural lan-guage L, which of the many possible SF s that ex-press P in L should we produce??.
So far thisquestion has only been satisfactorily addressed bylaboratory studies, which are few in number, ex-pensive to run, and hard to generalise from.When such generators are faced with this ques-tion, a better way forward would be to generate(some finite subset of) all possible SF s that ex-press P in L, and then use surprisal and entropyreduction metrics as thresholds for pruning andranking the SF s. This would lead the generator toproduce only SF s that avoid syntactic complexityfor the benefit of human readers.
Different thresh-olds could produce texts tailor-made for groupswith different reading abilities, or texts aimed tomeet other constraints on acceptable human dif-ficulty, e.g., texts for beginners learning a givennatural language for the first time, or texts withdifferent forms aimed at novices and experts.Reiter and Belz (2009) discuss and evaluatesome metrics for automatic evaluation of NLG inthe context of generating weather forecasts.
How-ever these are designed to fit human measures atthe whole-document level of NLG, different fromthe sentence-level incremental predictions gener-ated and evaluated here.
Also the evaluations dis-cussed by those authors are done by fitting mea-sures from offline human ratings of text readabil-ity, again different from the fine-grained detailof online human processing provided by the eye-tracking experiment here.It seems clear that a combination of document-level and sentence-level predictors of human diffi-culty with generated text would be better than ei-ther alone for guiding NLG systems.
It is conceiv-able that surprisal and entropy reduction might be-come useful automatic metrics for sentence-levelevaluation of NLG texts, in the same way thatBLEU (Papineni et al., 2002) and similar metricsserve in Machine Translation, but incrementally,and at a finer-grained and level.ReferencesJ.R.
Anderson.
2005.
Human symbol manipulationwithin an integrated cognitive architecture.
Cogni-tive science, 29(3):313?341.Douglas Bates, Martin Maechler, Ben Bolker, andSteven Walker.
2013. lme4: Linear mixed-effectsmodels using Eigen and S4.
R package version 1.0-5.M.F.
Boston and J. Hale.
2007.
Garden-pathing in astatistical dependency parser.
In Proceedings of theMidwest Computational Linguistics Colloquium.M.F.
Boston, John T. Hale, Shravan Vasishth, and Rein-hold Kliegl.
2011.
Parallel processing and sentencecomprehension difficulty.
Language and CognitiveProcesses, 26(3):301?349.M.F.
Boston.
2012.
A Computational Model of Cogni-tive Constraints in Syntactic Locality.
Ph.D. thesis,Cornell University, January.M.F.
Boston.
2013.
Humdep3.0.
An incrementaldependency parser developed for human sentenceprocessing modeling.
http://conf.ling.cornell.edu/Marisa.Thorsten Brants and Alex Franz.
2006.
Web 1T 5-gram Version.
computed from Google, published byLinguistic Data Consortium.C.
Clifton Jr. 1993.
Thematic roles in sentence pars-ing.
Canadian Journal of Experimental Psychology,47(2):222?46.V.
Demberg and F. Keller.
2008.
Data from eye-tracking corpora as evidence for theories of syntacticprocessing complexity.
Cognition, 109(2):193?210.F.
Ferreira and J.M.
Henderson.
1991.
Recovery frommisanalyses of garden-path sentences.
Journal ofMemory and Language, 30(6):725?745.M.J.
Green.
2014.
On Repairing Sentences: An Ex-perimental and Computational Analysis of Recoveryfrom Unexpected Syntactic Disambiguation in Sen-tence Parsing.
Ph.D. thesis, Psychology, Exeter.J.
Hale.
2001.
A Probabilistic Earley Parser as a Psy-cholinguistic Model.
In Proceedings Of The SecondMeeting Of The North American Chapter Of The As-sociation For Computational Linguistics, pages 1?8, Morristown, NJ, USA.
Association for Computa-tional Linguistics.45J.
Hale.
2004.
The information-processing diffi-culty of incremental parsing.
In F. Keller, S. Clark,M Crocker, and M. Steedman, editors, ACL Work-shop Incremental Parsing: Bringing Engineeringand Cognition Together, pages 58?65.
Associationfor Computational Linguistics.J.
Hale.
2006.
Uncertainty about the rest of the sen-tence.
Cognitive Science, 30(4):643?672.V.M.
Holmes, A. Kennedy, and W.S.
Murray.
1987.Syntactic structure and the garden path.
The Quar-terly Journal of Experimental Psychology Section A:Human Experimental Psychology, 39(2):2 ?
277.R.
Levy.
2008.
Expectation-Based Syntactic Compre-hension.
Cognition, 106(3):1126?1177.R.L.
Lewis and S. Vasishth.
2005.
An activation-basedmodel of sentence processing as skilled memory re-trieval.
Cognitive Science, 29:1?45.B.
McElree, S. Foraker, and L. Dyer.
2003.
Mem-ory structures that subserve sentence comprehen-sion.
Journal of Memory and Language, 48(1):67?91.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
BLEU: a method for automaticevaluation of machine translation.
In Proceedings ofthe 40th annual meeting on association for compu-tational linguistics, pages 311?318.
Association forComputational Linguistics.Umesh Patil, Shravan Vasishth, and Reinhold Kliegl.2009.
Compound effect of probabilistic disam-biguation and memory retrievals on sentence pro-cessing: Evidence from an eyetracking corpus.
InProceedings of 9th International Conference onCognitive Modeling, Manchester.M.J.
Pickering and M.J. Traxler.
1998.
Plausibilityand recovery from garden paths: An eye-trackingstudy.
Journal of Experimental Psychology: Learn-ing, Memory, and Cognition, 24(4):940?961.K.
Rayner and L. Frazier.
1987.
Parsing temporarilyambiguous complements.
The Quarterly Journal ofExperimental Psychology Section A: Human Exper-imental Psychology, 39(4):657 ?
673.K.
Rayner, A. Pollatsek, J. Ashby, and C. Clifton Jr.2012.
Psychology of Reading.
Psychology Press,2nd edition.Ehud Reiter and Anja Belz.
2009.
An investiga-tion into the validity of some metrics for automat-ically evaluating natural language generation sys-tems.
Computational Linguistics, 35(4):529?558.Brian Roark.
2001.
Probabilistic top-down parsingand language modeling.
Computational linguistics,27(2):249?276.Brian Roark.
2004.
Robust garden path parsing.
Nat-ural language engineering, 10(1):1?24.B.
Roark.
2013. tdparse.
An incremental topdown parser.
http://code.google.com/p/incremental-top-down-parser/.Claude Shannon.
1948.
A mathematical theory ofcommunication.
Bell Systems Technical Journal,27:379?423.P.
Sturt, M.J. Pickering, and M.W.
Crocker.
1999.Structural Change and Reanalysis Difficulty in Lan-guage Comprehension.
Journal of Memory andLanguage, 40:136?150.J C Trueswell, M K Tanenhaus, and C Kello.
1993.Verb-specific constraints in sentence processing:separating effects of lexical preference from garden-paths.
J Exp Psychol Learn Mem Cogn, 19(3):528?53.S.
Vasishth, S. Br?ussow, R.L.
Lewis, and H. Drenhaus.2008.
Processing polarity: How the ungrammati-cal intrudes on the grammatical.
Cognitive Science,32(4):685?712.46
