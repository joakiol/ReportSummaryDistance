Generating Training Data for Medical DictationsSergey PakhomovUniversity of Minnesota, MNpakhomov.sergey@mayo.eduMichael SchonwetterLinguistech Consortium, NJMSchonwetter@qwest.netJoan BachenkoLinguistech Consortium,NJbachenko@mnic.netAbstractIn automatic speech recognition (ASR) enabledapplications for medical dictations, corpora ofliteral transcriptions of speech are critical fortraining both speaker independent and speakeradapted acoustic models.
Obtaining thesetranscriptions is both costly and time consuming.Non-literal transcriptions, on the other hand, areeasy to obtain because they are generated in thenormal course of a medical transcription operation.This paper presents a method of automaticallygenerating texts that can take the place of literaltranscriptions for training acoustic and languagemodels.
ATRS1 is an automatic transcriptionreconstruction system that can produce near-literaltranscriptions with almost no human labor.
We willshow that (i) adapted acoustic models trained onATRS data perform as well as or better thanadapted acoustic models trained on literaltranscriptions (as measured by recognitionaccuracy) and (ii) language models trained onATRS data have lower perplexity than languagemodels trained on non-literal data.IntroductionDictation applications of automatic speechrecognition (ASR) require literal transcriptions ofspeech in order to train both speaker independentand speaker adapted acoustic models.
Literaltranscriptions may also be used to train stochasticlanguage models that need to perform well onspontaneous or disfluent speech.
With theexception of personal desktop systems, however,obtaining these transcriptions is costly and timeconsuming since they must be produced manually1patent pending (Serial No.
: 09/487398)by humans educated for the task.
The high costmakes literal transcription unworkable for ASRapplications that require adapted acoustic modelsfor thousands of talkers as well as accuratelanguage models for idiosyncratic natural speech.Non-literal transcriptions, on the other hand, areeasy to obtain because they are generated in thenormal course of a medical transcription operation.It has been previously shown by Wightman andHarder (1999) that the non-literal transcriptions canbe successfully used in acoustic adaptation.However, non-literal transcriptions are incomplete.They exclude many utterances that commonlyoccur in medical dictation?filled pauses,repetitions, repairs, ungrammatical phrases,pleasantries, asides to the transcriptionist, etc.Depending on the talker, such material mayconstitute a significant portion of the dictation.We present a method of automatically generatingtexts that can take the place of literal transcriptionsfor training acoustic and language models.
ATRSis an automatic transcription reconstruction systemthat can produce near-literal transcriptions withalmost no human labor.The following sections will describe ATRS andpresent experimental results from language andacoustic modeling.
We will show that (i) adaptedacoustic models trained on ATRS data perform aswell as or better than adapted acoustic modelstrained on literal transcriptions (as measured byrecognition accuracy) and (ii) language modelstrained on ATRS data have lower perplexity thanlanguage models trained on non-literal data.
Dataused in the experiments comes from medicaldictations.
All of the dictations are telephonespeech.1  Dictation Applications of ASRThe application for our work is medical dictationover the telephone.
Medical dictation differs fromother telephony based ASR applications, e.g.
airlinereservation systems, because the talkers are repeatusers and utterances are long.
Dictations usuallyconsist of 1-30 minutes of speech.
The talkers callin 3-5 days per week and produce between 1 and 12dictations each day they call.
Hence a medicaldictation operation has access to hours of speechfor each talker.Spontaneous telephone speech presents additionalchallenges that are caused partly by a poor acousticsignal and partly by the disfluent nature ofspontaneous speech.
A number of researchers havenoted the effects of disfluencies on speechrecognition and have suggested various approachesto dealing with them at language modeling andpost-processing stages.
(Shriberg 1994, Shriberg1996, Stolcke and Shriberg 1996, Stolcke et al1998, Shriberg and Stolcke 1996, Siu andOstendorf 1996, Heeman et al 1996) Medical over-the-telephone dictations can be classified asspontaneous or quasi-spontaneous discourse(Pakhomov 1999, Pakhomov and Savova 1999).Most physicians do not read a script prepared inadvance, instead, they engage in spontaneousmonologues that display the full spectrum ofdisfluencies found in conversational dialogs inaddition to other "disfluencies" characteristic ofdictated speech.
An example of the latter is when aphysician gives instructions to the transcriptionistto modify something in the preceding discourse,sometimes as far as several paragraphs back.Most ASR dictation applications focus on desktopusers; for example, Dragon, IBM, Philips andLernout & Hauspie all sell desktop dictationrecognizers that work on high quality microphonespeech.
Typically, the desktop system builds anadapted acoustic model if the talker "enrolls", i.e.reads a prepared script that serves as a literaltranscription.
Forced alignment of the script andthe speech provides the input to acoustic modeladaptation.Enrollment makes it relatively easy to obtain literaltranscriptions for adaptation.
However, enrollmentis not feasible for dictation over the telephoneprimarily because most physicians will refuse totake the time to enroll.
The alternative is to hirehumans who will type literal transcriptions ofdictation until enough have been accumulated tobuild an adapted model, an impractical solution fora large scale operation that processes speech fromthousands of talkers.
ATRS is appealing because itcan generate an approximation of literaltranscription that can replace enrollment scripts andthe need for manually generated literaltranscriptions.2 Three Classes of Training DataIn this paper, training texts for language andacoustic models fall into three categories:Non-Literal:Non-literal transcripts present themeaning of what was spoken in a written formappropriate for the domain.
In a commercialmedical transcription operation, the non-literaltranscript will present the dictation in a formatappropriate for a medical record.
This typicallyinvolves (i.)
ignoring filled pauses, pleasantries,and repeats; (ii.)
acting on directions for repairs("delete the second paragraph and put this ininstead..."); (iii.)
adding non-dictated punctuation;(iv.)
correcting grammatical errors; and (v.) re-formatting certain phrases such as "Lung areClear", to a standard form such as "Lungs - Clear".Literal:Literal transcriptions are exacttranscriptions of what was spoken.
This includesany elements not found in the non-literal transcript,such as filled pauses (um's and ah's), pleasantriesand body noises ("thank you very much, just amoment, cough"), repeats, fragments, repairs anddirections for repairs, and asides ("make thatbold").
Literal transcriptions require significanthuman effort, and therefore are expensive toproduce.
Even though they are carefully prepared,some errors will be present in the result.In their study of how humans deal with transcribingspoken discourse, Lindsay and O'Connell (1995)have found that literal transcripts were "far fromverbatim."
(p.111) They find that the transcribers intheir study tended to have the most difficultytranscribing hesitation phenomena, followed bysentence fragments, adverbs and conjunctions and,finally, nouns, verbs, adjectives and prepositions.Our informal observations made from thetranscripts produced by highly trained medicaltranscriptionists suggest approximately 5% errormargin and a gradation of errors similar tothe one found by Lindsay and O'Connell.Semi-Literal: Semi-literal transcripts are derivedusing non-literal transcripts, the recognizer output,a set of grammars, a dictionary, and an interpreterto integrate the recognized material into the non-literal transcription.
Semi-literal transcripts willmore closely resemble the literal transcripts, asmany of the elements missing from the non-literaltranscripts will be restored.3 Model AdaptationIt is well known that ASR systems perform bestwhen acoustic models are adapted to a particulartalker?s speech.
This is why commercial desktopsystems use enrollment.
Although less widelyapplied, language model adaptation based on linearinterpolation is an effective technique for tailoringstochastic grammars to particular domains ofdiscourse and to particular speakers (Savova et al(2000), Weng et al (1997)).The training texts used in acoustic modeling comefrom recognizer-generated texts, literaltranscriptions or non-literal transcriptions.
Withinthe family of transformation and combinedapproaches to acoustic modeling (Digalakis andNeumeyer (1996), Strom (1996), Wightman andHarder (1999), Hazen and Glass (1997)) three basicadaptation methods can be identified: unsupervised,supervised, or semi-supervised.
Each adaptationmethod depends on a different type of training text.What follows will briefly introduce the threemethods.Unsupervised adaptation relies on therecognizer?s output as the text guiding theadaptation.
Efficacy of unsupervised adaptationfully depends on the recognition accuracy.
AsWightman and Harder (1999) pointed out,unsupervised adaptation works well in laboratoryconditions when the speech signal has largebandwidth and is relatively ?clean?
of backgroundnoise, throat clearings, and other disturbances.
Inlaboratory conditions, the errors introduced byunsupervised adaptation can be averaged out byusing more data (Zavaliagkos and Colthurst, 1997);however, in a telephony operation with degradedinput that is not feasible.Supervised adaptation is dependent on literaltranscription availability and is widely used inenrollment in most desktop ASR systems.
Aspeaker?s speech sample is transcribed verbatimand then the speech signal is aligned withpronunciations frame by frame for each individualword.
A speaker independent model is augmentedto include the observations resulting from thealignment.Semi-supervised adaptation rests on the idea thatthe speech signal can be partially aligned by usingof the recognition output and the non-literaltranscription.
A significant problem with semi-supervised adaptation is that only the speech thatthe recognizer already recognizes successfully endsup being used for adaptation.
This reinforces whatis already well represented in the model.Wightman and Harder (1999) report that semi-supervised adaptation has a positive side effect ofexcluding those segments of speech that were mis-recognized for reasons other than a poor acousticmodel.
They note that background noise andspeech disfluency are detrimental to theunsupervised adaptation.In addition to the two problems with semi-supervised adaptation pointed out by Wightmanand Harder, we find one more potential problem.As a result of matching the word labels producedby the recognizer and the non-literal transcription,some words may be skipped which may introduceunnatural phone transitions at word boundaries.Language model adaptation is not an appropriatedomain for acoustic adaptation methods.
However,adapted language models can be loosely describedas supervised or unsupervised, based on the typesof training texts?literal or non-literal?that wereused in building the model.In the following sections we will describe thesystem of generating data that is well suited foracoustic and language adaptation and presentresults of experimental evaluation of this system.3.2 Generating semi-literal dataATRS is based on reconstruction of non-literaltranscriptions to train utterance specific languagemodels.
First, a non-literal transcription is used totrain an augmented probabilistic finite state model(APFSM) which is, in turn, used by the recognizerto re-recognize the exact same utterance that thenon-literal transcription was generated from.
TheAPFSM is constructed by linear interpolation of afinite state model where all transitionalprobabilities are equal to 1 with two otherstochastic models.One of the two models is a background model thataccounts for expressions such as greetings,thanking, false starts and repairs.
A list of theseout-of-transcription expressions is derived bycomparing already existing literal transcriptionswith their non-literal transcription counterparts.The other model represents the same non-literaltranscription populated with filled pauses (FP)(?um?s and ah?s?)
using a stochastic FP modelderived from a relatively large corpus of literaltranscriptions (Pakhomov, 1999, Pakhomov andSavova, 1999).Interpolation weights are established empirically bycalculating the resulting model?s perplexity againstheld out data.
Out-of-vocabulary (OOV) items arehandled provisionally by generating on-the-flypronunciations based on the existing dictionaryspelling-pronunciation alignments.
The result ofinterpolating these two background models is thatsome of the transitional probabilities found in thefinite state model are no longer 1.The language model so derived can now be used toproduce a transcription that is likely to be more trueto what has actually been said than the non-literaltranscription that we started to work with.Further refinement of the new semi-literaltranscription is carried out by using dynamicprogramming alignment on the recognizer?shypothesis (HYP) and the non-literal transcriptionthat is used as reference (REF).
The alignmentresults in each HYP label being designated as aMATCH, a DELETION, a SUBSTITUTION or anINSERTION.
Those labels present in the HYPstream that do not align with anything in the REFstream are designated as insertions and are assumedto represent the out-of-transcription elements of thedictation.
Those labels that do align but do notmatch are designated as substitutions.
Finally, thelabels found in the REF stream that do not alignwith anything in the HYP stream are designated asdeletions.The final semi-literal transcription is constructeddifferently depending on the intended purpose ofFigure { SEQ Figure \* ARABIC } Percent improvement in true data representationof ATRS reconstruction vs. Non-Literal datathe transcription.
If the transcription will be usedfor acoustic modeling, then the MATCHES, theREF portion of SUBSTITUTIONS and the HYPportion of only those INSERTIONS that representpunctuation and filled pauses make it into the finalsemi-literal transcription.
It is important to filterout everything else because acoustic modeling isvery sensitive to misalignment errors.
Languagemodeling, on the other hand, is less sensitive toalignment errors; therefore, INSERTIONS andDELETIONS can be introduced into the semi-literal transcription.One method of ascertaining the quality of semi-literal reconstruction is to measure its alignmenterrors against literal data using a dynamicprogramming application.
By measuring thecorrectness spread between ATRS and literal data,as well as the correctness spread between non-literal and literal data, the ATRS alignmentcorrectness rate was observed to be 4.4% higherabsolute over 774 dictation files tested.
Chart 1summarizes the results.
The X axis represents thenumber of dictations in each bin displayed alongthe Y axis representing the % improvement overthe non-literal counterparts.
The results showednearly all ATRS files had better alignmentcorrectness than their non-literal counterparts.
Themajority of the reconstructed dictations resembleliteral transcriptions between 1% and 8% betterthan their non-literal counterparts.
These resultsare statistically significant as evidenced by a t-testat 0.05 confidence level.
Much of the increase inalignment can be attributed to the introduction offilled pauses by ATRS.
However, ignoring filledpauses, we have observed informally that thecorrectness still improves in ATRS files versusnon-literal.In the following sections we will address acousticand language modeling and show that semi-literaltraining data is a good substitute for literal data.4 Experimental resultsThe usefulness of semi-literal transcriptions wasevaluated in two ways: acoustic adaptation andlanguage modeling.4.1 Adapted acoustic model evaluationThree speaker adapted acoustic models weretrained for each of the 5 talkers in this study usingthe three types of label files and evaluated on thetalker?s testing data.4.1.1 SetupThe data collected for each talker were split intotesting and training.Training Data45-55 minutes of audio data was collected for eachof the six talkers in this experiment:A femaleB femaleC maleD maleF femaleAll talkers are native speakers of English, twomales and three females.Non-literal transcriptionsof this data wereobtained in the course of normal transcriptionoperation where trained medical transcriptionistsrecord the dictations while filtering out disfluency,asides and ungrammatical utterances.Literal transcriptionswere obtained by having 5medical transcriptionists specially trained not tofilter out disfluency and asides transcribe all thedictations used in this study.Semi-literal transcriptionswere obtained with thesystem described in section 5 of this paper.Testing DataThree dictations (0.5 ?
2 min) each were pulled outof the Literal transcriptions training set and setaside for each talker for testing.Recognition and evaluation software andformalismSoftware licensed from Entropic Laboratory wasused for performing recognition, evaluatingaccuracy and acoustic adaptation.
(Valtchev, et al(1998)).
Adapted models were trained using MLLRtechnique (Legetter and Woodland, (1996))available as part of the Entropic package.Recognition accuracy and correctness reported inthis study were calculated according to thefollowing formulas:(1) Acc = hits ?
insertions / total words(2) Correctness = hits / total words4.1.2 ExperimentThe following Acoustic Models were trained viaadaptation with a general SI model for each talkerusing all available data (except for the testing data).Each model?s name reflects the kind of label datathat was used for training.LITERALEach audio file was aligned with the correspondingliteral transcription.NON-LITERALEach audio file was recognized using SI acousticand language models.
The recognition output wasaligned with the non-literal transcription usingdynamic programming.
Only those portions ofaudio that corresponded to direct matches in thealignment were used to produce alignments foracoustic modeling.
This method was originally usedfor medical dictations by Wightman and Harder(1999).SEMI-LITERALEach audio file has been processed to produce asemi-literal transcription that was then aligned withrecognition output generated in the process ofcreating semi-literal transcriptions.
The portions ofthe audio corresponding to matching segments wereused for acoustic adaptation training.The SI model had been trained on all available atthe time (12 hours)2 similar medical dictations tothe ones used in this study.
The data for the2Although 50-100 hours of data for SI modeling is theindustry standard, the population we are dealing with ishighly homogeneous and reasonable results can beobtained with lesser amount of data.speakers in this study were not used in training theSI model.4.1.3 ResultsTable 1 shows the test results.
As expected, bothrecognition accuracy and correctness increase withany of the three kinds of adaptation.
Adaptationusing Literal transcriptions yields an overall10.84% absolute gain in correctness and 11.49% inaccuracy over the baseline.Adaptation using Non-literal transcriptions yieldsan overall 6.36 % absolute gain in correctness and5.23 % in accuracy over the baseline.
Adaptationwith Semi-literal transcriptions yields an overall11.39 % absolute gain in correctness and 11.05 %in accuracy over the baseline.
No statisticalsignificance tests were performed on this data.Table 1.
Recognition results for three adaptationmethods4.1.4 DiscussionThe results of this experiment provide additionalsupport for using automatically generated semi-literal transcriptions as a viable (and possiblysuperior) substitute for literal data.
The fact thatthree SEMI-LITERAL adapted AM?s out of 5performed better than their LITERAL counterpartsseems to indicate that there may be undesirablenoise either in the literal transcriptions or in thecorresponding audio.
It may also be due to therelatively small amount of training data used for SImodeling thus providing a baseline that can beimproved with little effort.
However, the resultsstill indicate that generating semi-literaltranscriptions may help eliminate the undesirablenoise and, at the same time, get the benefits ofbroader coverage that semi-literal transcripts canafford over NON-LITERAL transcriptions.Baseline (SI)%Literal%Semi-literal%Non-literal%Talker Cor Acc Cor Acc Cor  Acc Cor AccA 58.76 48.47 66.57 58.09 68 58.28 64.76 51.8B 41.28 32.2 58.36 49.46 64.59 56.22 55.87 44.66C 57.22 54.99 64.38 61.54 61.25 59.31 60.65 58.71D 56.86 51.47 68.69 63.3 65.91 59.13 64.69 58.26F 54.83 43.69 61.97 53.57 64.7 54.41 61.13 48.73AVG 52.49 44.81 63.33 56.3 63.81 55.86 58.85 50.044.2 Language Model EvaluationFor ASR applications where there are significantdiscrepancies between an utterance and its formaltranscription, the inclusion of literal data in thelanguage model can reduce language modelperplexity and improve recognition accuracy.
Inmedical transcription, the non-literal texts typicallydepart from what has actually been said.
Hence ifthe talker says "lungs are clear" or "lungs soundpretty clear", the typed transcription is likely tohave "Lungs - clear".
In addition, as we notedearlier, the non-literal transcription will omitdisfluencies and asides and will correctgrammatical errors.Literal and semi-literal texts can be added ontolanguage model training data or interpolated intoan existing language model.
Below we will presentresults of a language modeling experiment thatcompares language models built from literal, semi-literal and non-literal versions of the same trainingset.
The results substantiate our claim thatautomatically generated semi-literal transcriptioncan lead to a significant improvement in languagemodel quality.In order to test the proposed method?s suitabilityfor language modeling, we constructed threetrigram language models and used perplexity as themeasure of the models?
goodness.SetupThe following models were trained on threeversions of a 270,000-word corpus.
The size of thetraining corpus is dictated by availability of literaltranscriptions.
The vocabulary was derived from acombination of all three corpora to keep the OOVrate constant.LLM ?
language model built from a corpus ofliteral transcriptionsNLM ?
language model built from non-literaltranscriptionsSLM ?
language model built from semi-literaltranscriptionsApproximately 5,000-word literal transcriptionscorpus consisting of 24 dictations was set aside fortestingResultsThe results of perplexity tests of the three modelson the held-out data at 3-gram level aresummarized in Table 2.
The tests were carried outusing the Entropic Transcriber ToolkitIt is apparent that SLM yields considerably betterperplexity than NLM, which indicates that althoughsemi-literal transcriptions are not as good as actualliteral transcriptions, they are more suitable forTable 2.
Perplexity tests on LLM, NLM, SLMlanguage modeling than non-literal transcriptions.These results are obtained with 270,000 words oftraining data; however, the typical amount isdozens of million.
We would expect the differencesin perplexity to become smaller with largeramounts of training data.Conclusions and future workWe have described ATRS, a system forreconstructing semi-literal transcriptionsautomatically.
ATRS texts can be used as asubstitute for literal transcriptions when the costand time required for generating literaltranscriptions are infeasible, e.g.
in a telephonybased transcription operation that processesthousands of acoustic and language models.
Textsproduced with ATRS were used in training speakeradapted acoustic models, speaker independentacoustic models and language models.Experimental results show that models built fromATRS training data yield performance results thatare equivalent to those obtained with modelstrained on literal transcriptions.
In the future, wewill address the issue of the amount of training datafor the SI model.
Also, current ATRS system doesnot take advantage of various confidence scoresavailable in leading recognition engines.
Webelieve that using such confidence measures canimprove the generation of semi-literal transcriptionsconsiderably.
We would also like to investigate thepoint at which the size of the various kinds of dataPerplexity OOV rate (%)LLM 185 2.61NLM 613 2.61SLM 313 2.61used for adaptation stops making improvements inrecognition accuracy.AcknowledgementsWe would like to thank the anonymous reviewersof this paper for very helpful feedback.
We thankGuergana Savova for excellent suggestions andenthusiastic support.
We would also like to thankJim Wu for valuable input.ReferencesDigalakis, V and Neumyer, L. (1996).
SpeakerAdaptation Using Combined Transformationand Baysean Mehtods.
IEEE Trans.
Speech andAudio Processing.Hazen, T and Glass, J (1997).
A Comparison of NovelTechniques for Instantaneous SpeakerAdaptation.
In Proc.
Eurospeech ?97.Heeman, P., Loken-Kim, K and Allen J.
(1996).Combining the Detection and Correction ofSpeech Repairs.
In Proc.
ICSLP ?96.Huang, X.  and Lee, K (1993).
On Speaker ?Independent, Speaker-Dependent, and Speaker-Adaptive Speech Recognition.
In IEEETransactions on Speech and Audio processing,Vol.
1, No.
2, pp.
150 ?
157.Legetter, C. and Woodland, P. (1996).
MaximumLikelihood Linear Regression for SpeakerAdaptation of Continuous Density HMM?s.
InComputer Speech and Language , 9, (171-186).Pakhomov, S.  (1999).
Modeling Filled Pauses inMedical Transcriptions.
In Student Section ofProc.
ACL?99.Pakhomov, S and Savova, G.  (1999).
Filled PauseModeling in Quasi-Spontaneous Speech.
InProc.
Disfluency in Spontaneous SpeechWorkshop at ICPHIS ?99.Savova, G, Schonwetter, M. and Pakhomov, S. (2000).Improving language model perplexity andrecognition accuracy for medical dictations viawithin-domaininterpolation with literal andsemi-literal corpora " In Proc.
ICSLP ?00.Shriberg, E. 1994 Preliminaries to a Theory of SpeechDisfluencies.
Ph.
D. thesis, University ofCalifornia at Berkely.Shriberg, E. and Stolcke, A.
(1996).
Word Predictabilityafter Hesitations: A Corpus-based Study.
InProc.
ICSLP ?96.Siu, M and Ostendorf, M. (1996).
Modeling Disfluenciesin Conversational Speech.
In Proc.
ICSLP ?96.Stolcke, A. and Shriberg, E. (1996).
Statistical LanguageModeling for Speech Disfluencies.
In proc.ICASSP ?96.Stolcke A., Shriberg E., Bates R., Ostendorf M., HakkaniD., Plauche M., Tur G., and Lu  Y.
(1998).Automatic Detection of Sentence Boundariesand Disfluencies based on Recognized Words.Proc.
Intl.
Conf.
on Spoken LanguageProcessing.Str?m, N (1996): "Speaker Adaptation by Modeling theSpeaker Variation in a Continuous SpeechRecognition System," In Proc.
ICSLP '96,Philadelphia, pp.
989-992.Valtchev, V.  Kershaw, D.  and Odell, J.
(1998).
TheTruetalk Transcriber Book.
EntropicCambridge Research Laboratory, Cambridge,England.Wightman, C.  W.  and Harder T.  A.
(1999).
Semi-Supervised Adaptation of Acoustic Models forLarge-Volume Dictation?
In Proc.
Eurospeech?98.
pp 1371-1374.Weng, F.,  Stolcke, A., Sankar, A.
(1997).
Hub4Language Modeling Using DomainInterpolation and Data Clustering.
Proc.DARPA Speech Recognition Workshop, pp.147-151, Chantilly, VA.
