Col locat ion Map for Overcoming Data SparsenessMoonjoo Kim, Young S. Han, and Key-Sun ChoiDepartment of Computer ScienceKorea Advanced Institute of Science and TechnologyTaejon, 305-701, Koreamj0712~eve.kaist.ac.kr, yshan~csking.kaist.ac.kr, kschoi~csking.kai~t.ac.k~AbstractStatistical anguage models are usefulbecause they can provide probabilis-tic information upon uncertain decisionmaking.
The most common statistic isn-grams measuring word cooccurrencesin texts.
The method suffers from datashortage problem, however.
In this pa-per, we suggest Bayesian networks beused in approximating the statistics ofinsufficient occurrences and of those thatdo not occur in the sample texts withgraceful degradation.
Collocation mapis a sigmoid belief network that can beconstructed from bigrams.
We comparedthe conditional probabilities and mutualinformation computed from bigrams andCollocation map.
The results show thatthe variance of the values from Colloca-tion map is smaller than that from fre-quency measure for the infrequent pairsby 48%.
The predictive power of Col-location map for arbitrary associationsnot observed from sample texts is alsodemonstrated.1 IntroductionIn statistical language processing, n-grams are barsic to many probabilistic models including HiddenMarkov models that work on the limited depen-dency of linguistic events.
In this regard, Bayesianmodels (Bayesian etwork, Belief network, Infer-ence diagram to name a few) are not very differentfrom ItMMs.
Bayesian models capture the con-ditional independence among probabilistic vari-ables, and can compute the conditional distribu-tion of the variables, which is known as a prob-abilistic inferencing.
The pure n-gram statistic,however, is somewhat crude in that it cannot doanything about unobserved events and its approx-imation on infrequent events can be unreliable.In this paper we show by way of extensive x-periments that the Bayesian method that also canbe composed from bigrams can overcome the datasparseness problem that is inherent in frequencycounting methods.
According to the empirical re-sults, Collocation map that is a Bayesian modelfor lexical variables induced graceful approxima-tion over unobserved and infrequent events.There are two known methods to deal with thedata sparseness problem.
They are smoothing andclass based methods (Dagan 1992).
Smoothingmethods (Church and Gale 1991) readjust he dis-tribution of frequencies of word occurrences ob-tained from sample texts, and verify the distri-bution through held-out exts.
As Dagan (1992)pointed out, however, the values from the smooth-ing methods closely agree with the probability ofa bigram consisting of two independent words.Class based methods (Pereira et al 1993)approximate the likelihood of unobserved wordsbased on similar words.
Dagan and et al (1992)proposed a non-hierarchical class based method.The two approaches report limited successes ofpurely experimental nature.
This is so becausethey are based on strong assumptions.
In the caseof smoothing methods, frequency readjustment issomewhat arbitrary and will not be good for heav-ily dependent bigrams.
As to the class basedmethods, the notion of similar words differs acrossdifferent methods, and the association of proba-bilistic dependency with the similarity (class) ofwords is too strong to assume in generM.Collocation map that is first suggested in (Itan1993) is a sigmoid belief network with words asprobabilistic variables.
Sigmoid belief network isextensively studied by Neal (1992), and has an effi-cient inferencing algorithm.
Unlike other Bayesianmodels, the inferencing on sigmoid belief networkis not NP-hard, and inference methods by reduc-ing the network and sampling are discussed in(Han 1995).
Bayesian models constructed fromlocal dependencies provide formal approximationamong the variables, thus using Collocation mapdoes not require strong assumption or intuition tojustify the associations among words produced bythe map.The results of inferencing on Collocation mapare probabilities among any combinations ofwords represented in the map, which is not found53in other models.
One significant shortcoming ofBayesian models lies in the heavy cost of inferenc-ing.
Our implementation f Collocation map in-cludes 988 nodes, and takes 2 to 3 minutes to com-pute an association between words.
The purposeof experiments i to find out how gracefully Col-location map deals with the unobserved cooccur-rences in comparison with a naive bigram statistic.In the next section, Collocation map is reviewedfollowing the definition in (Flail 1993).
In section3, mutual information and conditional probabili-ties computed using bigrams and Collocation mapare compared.
Section 4 concludes the paper bysummarizing the good and bad points of the Col-location map and other methods.2 Collocation MapIn this section, we make a brief introduction onCollocation map, and refer to (ttan 1993) for morediscussion on the definition and to (ttan 1995) oninfi~rence methods.Bayesian model consists of a network and prob-ability tables defined on the nodes of the network.The nodes in the network repre.sent probabilis-tic variables of a problem domain.
The networkcan compute probabilistic dependency betweenan)" combination of the variables.
The model iswell documented as subjective probability theory(Pearl 1988).Collocation map is an application model of sig-mold belief network (Neal 1992) that belongs tobelief networks which in turn is a type of Bayesianmodel.
Unlike belief networks, Collocation mapdoes not have deterministic variables thus consistsonly of probabilistic variables that correspond towords in this case.Sigmoid belief network is different from otherbelief networks in that it does not have probabil-ity distribution table at each node but weights onthe edges between the nodes.
A node takes binaryoutcomes (1, -1) and the probability that a nodetakes an outcome given the vector of outcomes ofits preceding nodes is a sigmoid function of theoutcomes and the weights of associated edges.
Inthis regard, the sigmoid belief network resemblesartificial neural network.
Such probabilities usedto be stored at nodes in ordinary Bayesian mod-els, and this makes the inferencing very difficultbecause the probability table can be very big.
Sig-moid belief network does away with the NP-hardcomplexity by avoiding the tables at the loss ofexpressive generality of probability distributionsthat can be encoded in the tables.One who works with Collocation map has todeal with two problems.
The first is how to con-struct the network, and the other is how to com-pute the probabilities on the network.Network can be constructed irectly from a setof bigrams obtained from a training sample.
Be-cause Collocation map is a directed a~yclic graph,P( profit I investment ) = 0.644069P( risk-taking I investment ) = 0.549834P( stock } investment ) = 0.546001P( high-income I investment ) = 0.564798P( investment I high-income ) = 0.500000P( high-income I risk-taking profit ) = 0.720300P( investment I portfolio high-income risk-taking )= 0.495988P( portfolio I blue-chip ) = 0.500000P( portfolio stock I portfolio stock ) = 1.000000Figure 1: Example Collocation map and exampleinferences.
Graph reduction method (Hall 1995)is used in computing the probabilities.cycles are avoided by making additional node of aword when facing cycle due to the node.
No morethan two nodes for each word are needed to avoidthe cycle in any case (ltan 1993).
Once the net-work is setup, edges of the network are assignedwith weights that are normalized frequency of theedges at a node.The inferencing on Collocation map is not dif-ferent from that for sigmoid belief network.
Thetime complexity of inferencing by reducing raphon sigmoid belief networks is O(N a) given Nnodes (Han 1995).
It turned out that inferencingon networks containing more than a few hundrednodes was not practical using either node reduc-tion method or sampling method, thus we adoptedthe hybrid inferencing method that first reducesthe network and applies Gibbs sampling method(Hall 1995).
Using the hybrid inferencing method,computation of conditional probabilities took lessthan a second for a network with 50 nodes, twoseconds for a network with 100 nodes, about nineseconds for a network with 200 nodes, and abouttwo minutes for a network with about 1000 nodes.Conditional and marginal probabilities can beapproximated from Gibb's sampling.
Some con-ditional probabilities computed from a small net-work are shown in figure 1.
Though the networkmay not be big enough to model the domain of fi-nance, the resulting values from the small networkcomposed of 9 dependencies seem useful and intu-itive.542015Mutual  inInformation ~vooe~oo ooO~-~?o?
~,, oo.
~?
.
'~ :~.
"oo o w o o ~@ oee .~dr 'ee~0 ~ ?
0 Co ???
Po Oo ?
ieo?~'0average MI *variance oe ~oo g Do ~ ?
oq50 I00 150 200Frequency ofbigramsFigure 2: Average MI's and variances.
378,888 unique bigrams are classified according to frequency.55The computation i  figure 1 was done by usinggraph reduction method.
As it is shown in theexample inferences, the association between anycombination of variables can be measured.3 Exper imentsThe goal of our experiment is first to find how datasparseness is related with the frequency basedstatistics and to show Collocation map basedmethod gives more reliable approximations.
Inparticular, from the experiments we observed thevariances of statistics might suggest he level ofdata sparseness.
The less frequent data tended tohave higher variances though the values of statis-tics (mutual information for instance) did not dis-tinguish the level of occurrences.
The predictiveaccount of Collocation map is demonstrated byobserving the variances of approximations on theinfrequent events.The tagged Wall Street Journal articles of PennTree corpus were used that contain about 2.6 mil-lion word units.
In the experiments, about 1.2million of them was used.
Programs were codedin C language, and run on a Sun Spare 10 work-station.For the first 1.2 million words, the bigrams con-sisting of four types of categories (NN, NNS, IN,J J) were obtained, and mutual information of eachbigram (order insensitive) was computed.
The bi-grams were classified into 200 sets according totheir occurrences.
Figure 2 summarizes the theaverage MI value and the variance of each fre-quency range.
From figure 3 that shows the oc-currence distribution of 378,888 unique bigrams,about 70% of them occur only one time.
One in-teresting and important observation is that thoseof 1 to 3 frequency range that take about 90% ofthe population have very high MI values.
This re-sults also agree with Dunning's argument aboutoverestimation on the infrequent occurrences inwhich many infrequent pairs tend to get higherestimation (Dunning 1993).
The problem is dueto the assumption of normality in naive frequencybased statistics according to Dunning (1993).
Ap-proximated values, thus, do not indicate the levelof data quality.
Figure 3 shows variances can sug-gest the level of data sufficiency.
From this obser-vation we propose the following definition on thenotion of data sparseness.A set of units belonging to a sample ofordered word units (texts) is cz data-sparseif and only if the variance of measurementson the set is greater than ~.The definition sets the concept of sparsenesswithin the context of a focused set of linguisticunits.
For a set of units unoberved from a sam-ple, the given sample text is for sure data-sparse.The above definition then gives a way to judgewith respect o observed units.
The measurementof data sparseness can be a good issue to studywhere it may depend on the contexts of research.Here we suggest a simple method perhaps for thefirst time in the literature.Figure 4 compares the results from using Col-location map and simple frequency statistic.
Thevariances are smaller and the pairs in frequency1 class have non zero approximations.
Becausecomputation on Collocation map is very high, wehave chosen 2000 unique pairs at random.
Thenetwork consists of 988 nodes.
Computing an ap-proximation (inferencing) took about 3 minutes.The test size of 2000 pairs may not be sufficient,but it showed the consistent endency of grace-ful degradation of variances.
The overestimationproblem was not significant in the approximationsby Collocation map.
The average value of zero fre-quency class to which 50 unobserved pairs belongwas also on the line of smooth degradation, andfigure 4 shows only the variance.Table 1 summarizes the details of performancegain by using Collocation map.4 Conc lus ionCorpus based natural language processing hasbeen one of the central subjects gaining rapid at-tention from the research community.
The ma-jor virtue of statistical approaches i in evaluatinglinguistic events and determining the relative im-portance of the events to resolve ambiguities.
Theevaluation on the events (mostly cooccurrences) inmany cases, however, has been unreliable becauseof the lack of data.Data sparseness addresses the shortage of datain estimating probabilistic parameters.
As a re-sult, there are too many events unobserved, andeven if events have been found, the occurrence isnot sufficient enough for the estimation to be re-liable.In contrast with existing methods that arebased on strong assumptions, the method usingCollocation map promises a logical approximationsince it is built on a thorough formal argument ofBayesian probability theory.
The powerful featureof the framework is the ability to make use of theconditional independence among word units andto make associations about unseen cooccurrencesbased on observed ones.
This naturally inducesthe attributes required to deal with data sparse-ness.
Our experiments confirm that Collocationmap makes predictive approximation and avoidsoverestimation of infrequent occurrences.One critical drawback of Collocation map is thetime complexity, but it can be useful for applica-tions of limited scope.56Percentage0.80.60.40.200 2 4 6 8 10Frequency of bigramsFigure 3: The distribution of 378,888 unique bigrams.
First ten classes are shown.1 5.1 12.2 57%10 2.28 4.28 46%20 1.29 5.29 75%30 1.51 3.51 56%40 2.18 3.18 31%50 1.52 2.87 47%average 2.04 4.5 45%Table 1: Comparison of variances between frequency based and Collocation map based MI computations.571210fre, luency based ?Col\[ocatic n ma\[ oMIvariance?
q4 ?
?
O .
?
?
w ?0 0 ro  ?o ?o o 0 ?o :o  uO ?
oo  ~ oo o o o 0 o ~?o Ug0 ?0 0 0 0 00 0 o0 0 0 0 00 5 I0 15 20 25 30 35 40 45 50Frequency of bigramsFigure 4: Variances by frequency based and Collocation map based MI  computations for 2000 uniquebigrarns.58ReferencesKenneth W. Church, and William A. Gale.
1991.A comparison of the enhanced Good-Turingand deleted estimation methods for estimat-ing probabilities of English bigrams.
ComputerSpeech and Language.
5.
19-54.Ted Dunning.
1993.
Accurate methods for thestatistics of surprise and coincidence.
Compu-tational Linguistics.
19 (1).
61-74.Ido Dagan, Shaul Marcus, and Shaul Markovitch.1992.
Contextual word similarity and estima-tion from sparse data.
In Proceedings of AAAIfall symposium, Cambridge, MI.
164-171.Young S. Han, Young G. Han, and Key-sun Choi.1992.
Recursive Markov chain as a stochasticgrammar.
In Proceedings of a SIGLEX work-shop, Columbus, Ohio.
22-31.Young S. Han, Young C. Park, and Key-sun Choi.1995.
Efficient inferencing for sigmoid Bayesiannetworks, to appear in Applied Intelligence.Radford M. Neal.
1992.
Connectionist learning ofbelief networks.
J of Artificial Intelligence.
56.71-113.Judea Pearl.
1988.
Probabilistic Reasoning in In-telligent Systems.
Morgan Kaufmann Publish-ers.Fernando Pereira, Naftali Tishby, and LillianLee.
1993.
Distributional clustering of Englishwords.
In Proceedings of the Annual Meeting ofthe A CL.59
