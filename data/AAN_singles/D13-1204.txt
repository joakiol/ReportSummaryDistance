Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1983?1995,Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational LinguisticsBreaking Out of Local Optima with Count Transformsand Model Recombination: A Study in Grammar InductionValentin I. Spitkovskyvalentin@cs.stanford.eduHiyan Alshawihiyan@google.comDaniel Jurafskyjurafsky@stanford.eduAbstractMany statistical learning problems in NLP callfor local model search methods.
But accu-racy tends to suffer with current techniques,which often explore either too narrowly or toobroadly: hill-climbers can get stuck in localoptima, whereas samplers may be inefficient.We propose to arrange individual local opti-mizers into organized networks.
Our buildingblocks are operators of two types: (i) trans-form, which suggests new places to search, vianon-random restarts from already-found localoptima; and (ii) join, which merges candidatesolutions to find better optima.
Experimentson grammar induction show that pursuing dif-ferent transforms (e.g., discarding parts of alearned model or ignoring portions of train-ing data) results in improvements.
Groups oflocally-optimal solutions can be further per-turbed jointly, by constructing mixtures.
Us-ing these tools, we designed several modu-lar dependency grammar induction networksof increasing complexity.
Our complete sys-tem achieves 48.6% accuracy (directed depen-dency macro-average over all 19 languages inthe 2006/7 CoNLL data) ?
more than 5%higher than the previous state-of-the-art.1 IntroductionStatistical methods for grammar induction often boildown to solving non-convex optimization problems.Early work attempted to locally maximize the likeli-hood of a corpus, using EM to estimate probabilitiesof dependency arcs between word bigrams (Paskin2001a; 2001b).
That parsing model has since beenextended to make unsupervised learning more feasi-ble (Klein and Manning, 2004; Headden et al 2009;Spitkovsky et al 2012b).
But even the latest tech-niques can be quite error-prone and sensitive to ini-tialization, because of approximate, local search.In theory, global optima can be found by enumer-ating all parse forests that derive a corpus, thoughthis is usually prohibitively expensive in practice.
Apreferable brute force approach is sampling, as inMarkov-chain Monte Carlo (MCMC) and randomrestarts (Hu et al 1994), which hit exact solutionseventually.
Restarts can be giant steps in a parameterspace that undo all previous work.
At the other ex-treme, MCMC may cling to a neighborhood, reject-ing most proposed moves that would escape a localattractor.
Sampling methods thus take unboundedtime to solve a problem (and can?t certify optimal-ity) but are useful for finding approximate solutionsto grammar induction (Cohn et al 2011; Marec?ekand ?Zabokrtsky?, 2011; Naseem and Barzilay, 2011).We propose an alternative (deterministic) searchheuristic that combines local optimization via EMwith non-random restarts.
Its new starting places areinformed by previously found solutions, unlike con-ventional restarts, but may not resemble their prede-cessors, unlike typical MCMC moves.
We show thatone good way to construct such steps in a parame-ter space is by forgetting some aspects of a learnedmodel.
Another is by merging promising solutions,since even simple interpolation (Jelinek and Mercer,1980) of local optima may be superior to all of theoriginals.
Informed restarts can make it possible toexplore a combinatorial search space more rapidlyand thoroughly than with traditional methods alone.2 Abstract OperatorsLet C be a collection of counts ?
the sufficientstatistics from which a candidate solution to anoptimization problem could be computed, e.g., bysmoothing and normalizing to yield probabilities.The counts may be fractional and solutions couldtake the form of multinomial distributions.
A localoptimizer L will convert C into C?
= LD(C) ?
anupdated collection of counts, resulting in a proba-bilistic model that is no less (and hopefully more)consistent with a data set D than the original C:(1)LDC C?1983Unless C?
is a global optimum, we should be ableto make further improvements.
But if L is idempo-tent (and ran to convergence) then L(L(C)) = L(C).Given only C and LD, the single-node optimizationnetwork above would be the minimal search patternworth considering.
However, if we had another opti-mizer L?
?
or a fresh starting point C?
?
then morecomplicated networks could become useful.2.1 Transforms (Unary)New starts could be chosen by perturbing an existingsolution, as in MCMC, or independently of previousresults, as in random restarts.
We focus on interme-diate changes to C, without injecting randomness.All of our transforms involve selective forgettingor filtering.
For example, if the probabilistic modelthat is being estimated decomposes into independentconstituents (e.g., several multinomials) then a sub-set of them can be reset to uniform distributions, bydiscarding associated counts from C. In text classifi-cation, this could correspond to eliminating frequentor rare tokens from bags-of-words.
We use circularshapes to represent such model ablation operators:(2)CAn orthogonal approach might separate out vari-ous counts in C by their provenance.
For instance,if D consisted of several heterogeneous data sources,then the counts from some of them could be ignored:a classifier might be estimated from just news text.We will use squares to represent data-set filtering:(3)CFinally, if C represents a mixture of possible inter-pretations over D ?
e.g., because it captures the out-put of a ?soft?
EM algorithm ?
contributions fromless likely, noisier completions could also be sup-pressed (and their weights redistributed to the morelikely ones), as in ?hard?
EM.
Diamonds will repre-sent plain (single) steps of Viterbi training:(4)C2.2 Joins (Binary)Starting from different initializers, say C1 and C2,it may be possible for L to arrive at distinct localoptima, C?1 6= C?2 .
The better of the two solutions,according to likelihood LD of D, could then be se-lected ?
as is standard practice when sampling.Our joining technique could do better than eitherC?1 or C?2 , by entertaining also a third possibility,which combines the two candidates.
We constructa mixture model by adding together all counts fromC?1 and C?2 into C+ = C?1 + C?2 .
Original initializersC1, C2 will, this way, have equal pull on the mergedmodel,1 regardless of nominal size (because C?1 , C?2will have converged using a shared training set, D).We return the best of C?1 , C?2 and C?+ = L(C+).
Thisapproach may uncover more (and never returns less)likely solutions than choosing among C?1 , C?2 alone:(5)LDLDLD+argMAXLDC1C?1 = L(C1)C2C?2 = L(C2)C?1 + C?2 = C+We will use a short-hand notation to represent thecombiner network diagrammed above, less clutter:(6)LDC2C13 The Task and MethodologyWe apply transform and join paradigms to grammarinduction, an important problem of computationallinguistics that involves notoriously difficult objec-tives (Pereira and Schabes, 1992; de Marcken, 1995;Gimpel and Smith, 2012, inter alia).
The goal is toinduce grammars capable of parsing unseen text.
In-put, in both training and testing, is a sequence of to-kens labeled as: (i) a lexical item and its category,(w, cw); (ii) a punctuation mark; or (iii) a sentenceboundary.
Output is unlabeled dependency trees.3.1 Models and DataWe constrain all parse structures to be projective, viadependency-and-boundary grammars (Spitkovsky etal., 2012a; 2012b): DBMs 0?3 are head-outwardgenerative parsing models (Alshawi, 1996) that dis-tinguish complete sentences from incomplete frag-ments in a corpus D: Dcomp comprises inputs endingwith punctuation; Dfrag = D ?
Dcomp is everything1If desired, a scaling factor could be used to bias C+ towardseither C?1 or C?2 , for example based on their likelihood ratio.1984else.
The ?complete?
subset is further partitionedinto simple sentences, Dsimp ?
Dcomp, with no inter-nal punctuation, and others, which may be complex.As an example, consider the beginning of an arti-cle from (simple) Wikipedia: (i) Linguistics (ii) Lin-guistics (sometimes called philology) is the sciencethat studies language.
(iii) Scientists who study lan-guage are called linguists.
Since the title does notend with punctuation, it would be relegated to Dfrag.But two complete sentences would be in Dcomp, withthe last also filed under Dsimp, as it has only a trail-ing punctuation mark.
Spitkovsky et alsuggestedtwo curriculum learning strategies: (i) one in whichinduction begins with clean, simple data, Dsimp, anda basic model, DBM-1 (2012b); and (ii) an alterna-tive bootstrapping approach: starting with still more,simpler data ?
namely, short inter-punctuation frag-ments up to length l = 15, Dlsplit ?
Dlsimp ?
and abare-bones model, DBM-0 (2012a).
In our example,Dsplit would hold five text snippets: (i) Linguistics;(ii) Linguistics; (iii) sometimes called philology;(iv) is the science that studies language; and (v) Sci-entists who study language are called linguists.Only the last piece of text would still be consideredcomplete, isolating its contribution to sentence rootand boundary word distributions from those of in-complete fragments.
The sparse model, DBM-0, as-sumes a uniform distribution for roots of incompleteinputs and reduces conditioning contexts of stoppingprobabilities, which works well with split data.
Wewill exploit both DBM-0 and the full DBM,2 draw-ing also on split, simple and raw views of input text.All experiments prior to final multi-lingual eval-uation will use the Penn English Treebank?s WallStreet Journal (WSJ) portion (Marcus et al 1993) asthe underlying tokenized and sentence-broken cor-pus D. Instead of gold parts-of-speech, we pluggedin 200 context-sensitive unsupervised tags, fromSpitkovsky et al(2011c),3 for the word categories.3.2 Smoothing and LexicalizationAll unlexicalized instances of DBMs will be esti-mated with ?add one?
(a.k.a.
Laplace) smoothing,2We use the short-hand DBM to refer to DBM-3, which isequivalent to DBM-2 if D has no internally-punctuated sen-tences (D=Dsplit), and DBM-1 if all inputs also have trailingpunctuation (D=Dsimp); DBM0 is our short-hand for DBM-0.3http://nlp.stanford.edu/pubs/goldtags-data.tar.bz2using only the word category cw to represent a token.Fully-lexicalized grammars (L-DBM) are left un-smoothed, and represent each token as both a wordand its category, i.e., the whole pair (w, cw).
To eval-uate a lexicalized parsing model, we will always ob-tain a delexicalized-and-smoothed instance first.3.3 Optimization and Viterbi DecodingWe use ?early-switching lateen?
EM (Spitkovsky etal., 2011a, ?2.4) to train unlexicalized models, alter-nating between the objectives of ordinary (soft) andhard EM algorithms, until neither can improve itsown objective without harming the other?s.
This ap-proach does not require tuning termination thresh-olds, allowing optimizers to run to numerical con-vergence if necessary, and handles only our shorterinputs (l ?
15), starting with soft EM (L = SL, for?soft lateen?).
Lexicalized models will cover fulldata (l ?
45) and employ ?early-stopping lateen?EM (2011a, ?2.3), re-estimating via hard EM untilsoft EM?s objective suffers.
Alternating EMs wouldbe expensive here, since updates take (at least) O(l3)time, and hard EM?s objective (L = H) is the onebetter suited to long inputs (Spitkovsky et al 2010).Our decoders always force an inter-punctuationfragment to derive itself (Spitkovsky et al 2011b,?2.2).4 In evaluation, such (loose) constraints mayhelp attach sometimes and philology to called (andthe science... to is).
In training, stronger (strict)constraints also disallow attachment of fragments?heads by non-heads, to connect Linguistics, calledand is (assuming each piece got parsed correctly).3.4 Final Evaluation and MetricsEvaluation is against held-out CoNLL shared taskdata (Buchholz and Marsi, 2006; Nivre et al 2007),spanning 19 languages.
We compute performanceas directed dependency accuracies (DDA), fractionsof correct unlabeled arcs in parsed output (an extrin-sic metric).5 For most WSJ experiments we includealso sentence and parse tree cross-entropies (soft andhard EMs?
intrinsic metrics), in bits per token (bpt).4But these constraints do not impact training with shorterinputs, since there is no internal punctuation in Dsplit or Dsimp.5We converted gold labeled constituents in WSJ to unlabeledreference dependencies using deterministic ?head-percolation?rules (Collins, 1999); sentence root symbols, though not punc-tuation arcs, contribute to scores, as is standard (Paskin, 2001b).19854 Concrete OperatorsWe will now instantiate the operators sketched outin ?2 specifically for the grammar induction task.Throughout, we repeatedly employ single steps ofViterbi training to transfer information between sub-networks in a model-independent way: when a mod-ule?s output is a set of (Viterbi) parse trees, it neces-sarily contains sufficient information required to es-timate an arbitrarily-factored model down-stream.64.1 Transform #1: A Simple FilterGiven a model that was estimated from (and there-fore parses) a data set D, the simple filter (F ) at-tempts to extract a cleaner model, based on the sim-pler complete sentences of Dsimp.
It is implementedas a single (unlexicalized) step of Viterbi training:(7)C FThe idea here is to focus on sentences that are nottoo complicated yet grammatical.
This punctuation-sensitive heuristic may steer a learner towards easybut representative training text and, we showed, aidsgrammar induction (Spitkovsky et al 2012b, ?7.1).4.2 Transform #2: A SymmetrizerThe symmetrizer (S) reduces input models to sets ofword association scores.
It blurs all details of in-duced parses in a data set D, except the number oftimes each (ordered) word pair participates in a de-pendency relation.
We implemented symmetrizationalso as a single unlexicalized Viterbi training step,but now with proposed parse trees?
scores, for a sen-tence in D, proportional to a product over non-rootdependency arcs of one plus how often the left andright tokens (are expected to) appear connected:(8)C SThe idea behind the symmetrizer is to glean infor-mation from skeleton parses.
Grammar inducers cansometimes make good progress in resolving undi-rected parse structures despite being wrong aboutthe polarities of most arcs (Spitkovsky et al 2009,Figure 3: Uninformed).
Symmetrization offers anextra chance to make heads or tails of syntactic rela-tions, after learning which words tend to go together.6A related approach ?
initializing EM training with anM-step ?
was advocated by Klein and Manning (2004, ?3).At each instance where a word a?
attaches z?
on(say) the right, our implementation attributes half itsweight to the intended construction, ya?
z?, reservingthe other half for the symmetric structure, z?
attach-ing a?
to its left: xa?
z?.
For the desired effect, theseaggregated counts are left unnormalized, while allother counts (of word fertilities and sentence roots)get discarded.
To see why we don?t turn word attach-ment scores into probabilities, consider sentencesa?
z?
and c?
z?.
The fact that z?
co-occurs with a?introduces an asymmetry into z?
?s relation with c?
:P( z?
| c?)
= 1 differs from P( c?
| z?)
= 1/2.
Normal-izing might force the interpretation yc?
z?
(and alsoya?
z?
), not because there is evidence in the data, butas a side-effect of a model?s head-driven nature (i.e.,factored with dependents conditioned on heads).
Al-ways branching right would be a mistake, however,for example if z?
is a noun, since either of a?
or c?could be a determiner, with the other a verb.4.3 Join: A CombinerThe combiner must admit arbitrary inputs, includ-ing models not estimated from D, unlike the trans-forms.
Consequently, as a preliminary step, we con-vert each input Ci into parse trees of D, with countsC?i, via Viterbi-decoding with a smoothed, unlexical-ized version of the corresponding incoming model.Actual combination is then performed in a more pre-cise (unsmoothed) fashion: C?i are the (lexicalized)solutions starting from C?i; and C?+ is initialized withtheir sum,?iC?i .
Counts of the lexicalized modelwith lowest cross-entropy on D become the output:7(9)LDC2C15 Basic NetworksWe are ready to propose a non-trivial subnetwork forgrammar induction, based on the transform and joinoperators, which we will reuse in larger networks.5.1 Fork/Join (FJ)Given a model that parses a base data set D0, thefork/join subnetwork will output an adaptation ofthat model for D. It could facilitate a grammar in-duction process, e.g., by advancing it from smaller7In our diagrams, lexicalized modules are shaded black.1986to larger ?
or possibly more complex ?
data sets.We first fork off two variations of the incomingmodel based on D0: (i) a filtered view, which fo-cuses on cleaner, simpler data (transform #1); and(ii) a symmetrized view that backs off to word asso-ciations (transform #2).
Next is grammar inductionover D. We optimize a full DBM instance startingfrom the first fork, and bootstrap a reduced DBM0from the second.
Finally, the two new induced setsof parse trees, for D, are merged (lexicalized join):(10)HL?DBMDSLDBMDSLDBM0DCFSD0C1C2C?1C?2The idea here is to prepare for two scenarios: anincoming grammar that is either good or bad for D.If the model is good, DBM should be able to hangon to it and make improvements.
But if it is bad,DBM could get stuck fitting noise, whereas DBM0might be more likely to ramp up to a good alterna-tive.
Since we can?t know ahead of time which is thetrue case, we pursue both optimization paths simul-taneously and let a combiner later decide for us.Note that the forks start (and end) optimizing withsoft EM.
This is because soft EM integrates previ-ously unseen tokens into new grammars better thanhard EM, as evidenced by our failed attempt to re-produce the ?baby steps?
strategy with Viterbi train-ing (Spitkovsky et al 2010, Figure 4).
A combinerthen executes hard EM, and since outputs of trans-forms are trees, the end-to-end process is a chain oflateen alternations that starts and ends with hard EM.We will use a ?grammar inductor?
to representsubnetworks that transition from Dlsplit to Dl+1split, bytaking transformed parse trees of inter-punctuationfragments up to length l (base data set, D0) to ini-tialize training over fragments up to length l + 1:(11)C l+1The FJ network instantiates a grammar inductorwith l = 14, thus training on inter-punctuation frag-ments up to length 15, as in previous work, startingfrom an empty set of counts, C = ?.
Smoothingcauses initial parse trees to be chosen uniformly atrandom, as suggested by Cohen and Smith (2010):(12)?
155.2 Iterated Fork/Join (IFJ)Our second network daisy-chains grammar induc-tors, starting from the single-word inter-punctuationfragments in D1split, then retraining on D2split, and soforth, until finally stopping at D15split, as before:(13)1 2 14 15We diagrammed this system as not taking an input,since the first inductor?s output is fully determinedby unique parse trees of single-token strings.
Thisiterative approach to optimization is akin to deter-ministic annealing (Rose, 1998), and is patterned af-ter ?baby steps?
(Spitkovsky et al 2009, ?4.2).Unlike the basic FJ, where symmetrization was ano-op (since there were no counts in C = ?
), IFJmakes use of symmetrizers ?
e.g., in the third in-ductor, whose input is based on strings with up totwo tokens.
Although it should be easy to learnwords that go together from very short fragments,extracting correct polarities of their relations couldbe a challenge: to a large extent, outputs of early in-ductors may be artifacts of how our generative mod-els factor (see ?4.2) or how ties are broken in opti-mization (Spitkovsky et al 2012a, Appendix B).
Wetherefore expect symmetrization to be crucial in ear-lier stages but to weaken any high quality grammars,nearer the end; it will be up to combiners to handlesuch phase transitions correctly (or gracefully).5.3 Grounded Iterated Fork/Join (GIFJ)So far, our networks have been either purely itera-tive (IFJ) or static (FJ).
These two approaches canalso be combined, by injecting FJ?s solutions intoIFJ?s more dynamic stream.
Our new transition sub-network will join outputs of grammar inductors thateither (i) continue a previous solution (as in IFJ); or(ii) start over from scratch (?grounding?
to an FJ):(14)HL?DBMDl+1split?Cl Cl+1l+1l+1The full GIFJ network can then be obtained by un-rolling the above template from l = 14 back to one.1987WSJ15split WSJ15simpInstance Label Model hsents htrees DDA hsents htrees DDA TA DescriptionDBM 6.54 6.75 83.7 6.05 6.21 85.1 42.7 Supervised (MLE of WSJ45)?
= C ?
8.76 10.46 21.4 8.58 10.52 20.7 3.9 Random Projective ParsesSL(S(C)) = C2 DBM0 6.18 6.39 57.0 5.90 6.11 57.5 10.4 BA}UnlexicalizedBaselinesSL(F (C)) = C1 DBM 5.89 5.99 62.2 5.79 5.90 60.9 12.0H(C?2) = C?2 L-DBM 7.28 7.30 59.2 6.87 6.88 58.6 10.4Fork/Join????
?BaselineCombinationH(C?1) = C?1 L-DBM 7.07 7.08 62.3 6.72 6.73 60.8 12.0C?1 + C?2 = C+ L-DBM 7.20 7.27 64.0 6.82 6.88 62.5 12.3H(C+) = C?+ L-DBM 7.02 7.04 64.2 6.64 6.65 62.7 12.8L-DBM 6.95 6.96 70.5 6.55 6.56 68.2 14.9 Iterated Fork/Join (IFJ)L-DBM 6.91 6.92 71.4 6.52 6.52 69.2 15.6 Grounded Iterated Fork/JoinL-DBM 6.83 6.83 72.3 6.41 6.41 70.2 17.9 Grammar Transformer (GT)L-DBM 6.92 6.93 71.9 6.53 6.53 69.8 16.7 IFJGT}w/IteratedCombinersL-DBM 6.83 6.83 72.9 6.41 6.41 70.6 18.0Table 1: Sentence string and parse tree cross-entropies (in bpt), and accuracies (DDA), on inter-punctuation fragmentsup to length 15 (WSJ15split) and its subset of simple, complete sentences (WSJ15simp, with exact tree accuracies ?
TA).6 Performance of Basic NetworksWe compared our three networks?
performance ontheir final training sets, WSJ15split (see Table 1, whichalso tabulates results for a cleaner subset, WSJ15simp).The first network starts from C = ?, helping us es-tablish several straw-man baselines.
Its empty ini-tializer corresponds to guessing (projective) parsetrees uniformly at random, which has 21.4% accu-racy and sentence string cross-entropy of 8.76bpt.6.1 Fork/Join (FJ)FJ?s symmetrizer yields random parses of WSJ14split,which initialize training of DBM0.
This baseline (B)lowers cross-entropy to 6.18bpt and scores 57.0%.FJ?s filter starts from parse trees of WSJ14simp only, andtrains up a full DBM.
This choice makes a strongerbaseline (A), with 5.89bpt cross-entropy, at 62.2%.The join operator uses counts from A and B, C1and C2, to obtain parse trees whose own counts C?1and C?2 initialize lexicalized training.
From each C?i,an optimizer arrives at C?i .
Grammars correspondingto these counts have higher cross-entropies, becauseof vastly larger vocabularies, but also better accura-cies: 59.2 and 62.3%.
Their mixture C+ is a simplesum of counts in C?1 and C?2 : it is not expected to bean improvement but happens to be a good move, re-sulting in a grammar with higher accuracy (64.0%),though not better Viterbi cross-entropy (7.27 fallsbetween 7.08 and 7.30bpt) than both sources.
Thecombiner?s third alternative, a locally optimal C?+, isthen obtained by re-optimizing from C+.
This so-lution performs slightly better (64.2%) and will bethe local optimum returned by FJ?s join operator, be-cause it attains the lowest cross-entropy (7.04bpt).6.2 Iterated Fork/Join (IFJ)IFJ?s iterative approach results in an improvement:70.5% accuracy and 6.96bpt cross-entropy.
To testhow much of this performance could be obtained bya simpler iterated network, we experimented withablated systems that don?t fork or join, i.e., our clas-sic ?baby steps?
schema (chaining together 15 op-timizers), using both DBM and DBM0, with andwithout a transform in-between.
However, all such?linear?
networks scored well below 50%.
We con-clude from these results that an ability to branch outinto different promising regions of a solution space,and to merge solutions of varying quality into bettermodels, are important properties of FJ subnetworks.6.3 Grounded Iterated Fork/Join (GIFJ)Grounding improves GIFJ?s performance further, to71.4% accuracy and 6.92bpt cross-entropy.
This re-sult shows that fresh perspectives from optimizersthat start over can make search efforts more fruitful.7 Enhanced SubnetworksModularity and abstraction allow for compact repre-sentations of complex systems.
Another key benefitis that individual components can be understood andimproved in isolation, as we will demonstrate next.19887.1 An Iterative Combiner (IC)Our basic combiner introduced a third option, C?+,into a pool of candidate solutions, {C?1 , C?2}.
Thisnew entry may not be a simple mixture of the orig-inals, because of non-linear effects from applying Lto C?1 + C?2 , but could most likely still be improved.Rather than stop at C?+, when it is better than bothoriginals, we could recombine it with a next best so-lution, continuing until no further improvement ismade.
Iterating can?t harm a given combiner?s cross-entropy (e.g., it lowers FJ?s from 7.04 to 7.00bpt),and its advantages can be realized more fully in thelarger networks (albeit without any end-to-end guar-antees): upgrading all 15 combiners in IFJ wouldimprove performance (slightly) more than ground-ing (71.5 vs. 71.4%), and lower cross-entropy (from6.96 to 6.93bpt).
But this approach is still a bit timid.A more greedy way is to proceed so long as C?+is not worse than both predecessors.
We shall nowstate our most general iterative combiner (IC) algo-rithm: Start with a solution pool p = {C?i }ni=1.
Next,construct p?
by adding C?+ = L(?ni=1 C?i ) to p and re-moving the worst of n+ 1 candidates in the new set.Finally, if p = p?, return the best of the solutions in p;otherwise, repeat from p := p?.
At n = 2, one couldthink of taking L(C?1 + C?2 ) as performing a kind ofbisection search in some (strange) space.
With thesenew and improved combiners, the IFJ network per-forms better: 71.9% (up from 70.5 ?
see Table 1),lowering cross-entropy (down from 6.96 to 6.93bpt).We propose a distinguished notation for the ICs:(15)*C2C17.2 A Grammar Transformer (GT)The levels of our systems?
performance at grammarinduction thus far suggest that the space of possiblenetworks (say, with up to k components) may itselfbe worth exploring more thoroughly.
We leave thisexercise to future work, ending with two relativelystraight-forward extensions for grounded systems.Our static bootstrapping mechanism (?ground?
ofGIFJ) can be improved by pretraining with simplesentences first ?
as in the curriculum for learningDBM-1 (Spitkovsky et al 2012b, ?7.1), but nowwith a variable length cut-off l (much lower than theoriginal 45) ?
instead of starting from ?
directly:(16)SDBMDlsimp?l+1??
?lThe output of this subnetwork can then be refined,by reconciling it with a previous dynamic solution.We perform a mini-join of a new ground?s countswith Cl, using the filter transform (single steps oflexicalized Viterbi training on clean, simple data),ahead of the main join (over more training data):(17)HL?DBMDl+1splitCl Cl+1l+1FlThis template can be unrolled, as before, to obtainour last network (GT), which achieves 72.9% accu-racy and 6.83bpt cross-entropy (slightly less accu-rate with basic combiners, at 72.3% ?
see Table 1).8 Full Training and System CombinationAll systems that we described so far stop training atD15split.
We will use a two-stage adaptor network totransition their grammars to a full data set, D45:(18)HL?DBMD45split HL?DBMD45CThe first stage exposes grammar inducers to longerinputs (inter-punctuation fragments with up to 45tokens); the second stage, at last, reassembles textsnippets into actual sentences (also up to l = 45).8After full training, our IFJ and GT systems parseSection 23 of WSJ at 62.7 and 63.4% accuracy, bet-ter than the previous state-of-the-art (61.2% ?
seeTable 2).
To test the generalized IC algorithm, wemerged our implementations of these three stronggrammar induction pipelines into a combined sys-tem (CS).
It scored highest: 64.4%.
(19)HL?DBMD45(GT) #1(IFJ) #2#3CSThe quality of bracketings corresponding to (non-trivial) spans derived by heads of our dependencystructures is competitive with the state-of-the-art inunsupervised constituent parsing.
On the WSJ sen-tences up to length 40 in Section 23, CS attains sim-ilar F1-measure (54.2 vs. 54.6, with higher recall) to8Note that smoothing in the final (unlexicalized) Viterbi stepmasks the fact that model parts that could not be properly es-timated in the first stage (e.g., probabilities of punctuation-crossing arcs) are being initialized to uniform multinomials.1989System DDA (@10)(Gimpel and Smith, 2012) 53.1 (64.3)(Gillenwater et al 2010) 53.3 (64.3)(Bisk and Hockenmaier, 2012) 53.3 (71.5)(Blunsom and Cohn, 2010) 55.7 (67.7)(Tu and Honavar, 2012) 57.0 (71.4)(Spitkovsky et al 2011b) 58.4 (71.4)(Spitkovsky et al 2011c) 59.1 (71.4)#3 (Spitkovsky et al 2012a) 61.2 (71.4)#2w/Full Training{IFJGT62.7 (70.3)#1 63.4 (70.3)#1 + #2 + #3 System Combination CS 64.4 (72.0)Supervised DBM (also with loose decoding) 76.3 (85.4)Table 2: Directed dependency accuracies (DDA) on Sec-tion 23 of WSJ (all sentences and up to length ten) forrecent systems, our full networks (IFJ and GT), and three-way combination (CS) with the previous state-of-the-art.PRLG (Ponvert et al 2011), which is the strongestsystem of which we are aware (see Table 3).99 Multi-Lingual EvaluationLast, we checked how our algorithms generalize out-side English WSJ, by testing in 23 more set-ups: all2006/7 CoNLL test sets (Buchholz and Marsi, 2006;Nivre et al 2007), spanning 19 languages.
Most re-cent work evaluates against this multi-lingual data,with the unrealistic assumption of part-of-speechtags.
But since inducing high quality word clustersfor many languages would be beyond the scope ofour paper, here we too plugged in gold tags for wordcategories (instead of unsupervised tags, as in ?3?8).We compared to the two strongest systems weknew:10 MZ (Marec?ek and ?Zabokrtsky?, 2012) andSAJ (Spitkovsky et al 2012b), which report averageaccuracies of 40.0 and 42.9% for CoNLL data (seeTable 4).
Our fully-trained IFJ and GT systems score40.0 and 47.6%.
As before, combining these net-works with our own implementation of the best pre-vious state-of-the-art system (SAJ) yields a furtherimprovement, increasing final accuracy to 48.6%.9These numbers differ from Ponvert et als (2011, Table 6)for the full Section 23 because we restricted their eval-ps.pyscript to a maximum length of 40 words, in our evaluation, tomatch other previous work: Golland et als (2012, Figure 1) forCCM and LLCCM; Huang et als (2012, Table 2) for the rest.10During review, another strong system (Marec?ek and Straka,2013, scoring 48.7%) of possible interest to the reader came out,exploiting prior knowledge of stopping probabilities (estimatedfrom large POS-tagged corpora, via reducibility principles).System F1Binary-Branching Upper Bound 85.7Left-Branching Baseline 12.0CCM (Klein and Manning, 2002) 33.7Right-Branching Baseline 40.7F-CCM (Huang et al 2012) 45.1HMM (Ponvert et al 2011) 46.3LLCCM (Golland et al 2012) 47.6 P RCCL (Seginer, 2007) 52.8 54.6 51.1PRLG (Ponvert et al 2011) 54.6 60.4 49.8CS System Combination 54.2 55.6 52.8Supervised DBM Skyline 59.3 65.7 54.1Dependency-Based Upper Bound 87.2 100 77.3Table 3: Harmonic mean (F1) of precision (P) and re-call (R) for unlabeled constituent bracketings on Section23 of WSJ (sentences up to length 40) for our combinedsystem (CS), recent state-of-the-art and the baselines.10 DiscussionCoNLL training sets were intended for comparingsupervised systems, and aren?t all suitable for unsu-pervised learning: 12 languages have under 10,000sentences (with Arabic, Basque, Danish, Greek, Ital-ian, Slovenian, Spanish and Turkish particularlysmall), compared to WSJ?s nearly 50,000.
In sometreebanks sentences are very short (e.g., Chinese andJapanese, which appear to have been split on punc-tuation), and in others extremely long (e.g., Arabic).Even gold tags aren?t always helpful, as their num-ber is rarely ideal for grammar induction (e.g., 42 vs.200 for English).
These factors contribute to highvariances of our (and previous) results (see Table 4).Nevertheless, if we look at the more stable aver-age accuracies, we see a positive trend as we movefrom a simpler fully-trained system (IFJ, 40.0%),to a more complex system (GT, 47.6%), to systemcombination (CS, 48.6%).
Grounding seems to bemore important for the CoNLL sets, possibly be-cause of data sparsity or availability of gold tags.11 Related WorkThe surest way to avoid local optima is to craftan objective that doesn?t have them.
For example,Wang et al(2008) demonstrated a convex train-ing method for semi-supervised dependency pars-ing; Lashkari and Golland (2008) introduced a con-vex reformulation of likelihood functions for clus-tering tasks; and Corlett and Penn (2010) designed1990Directed Dependency Accuracies (DDA) (@10)CoNLL Data MZ SAJ IFJ GT CSArabic 2006 26.5 10.9 33.3 8.3 9.3 (30.2)?7 27.9 44.9 26.1 25.6 26.8 (45.6)Basque ?7 26.8 33.3 23.5 24.2 24.4 (32.8)Bulgarian ?7 46.0 65.2 35.8 64.2 63.4 (69.1)Catalan ?7 47.0 62.1 65.0 68.4 68.0 (79.2)Chinese ?6 ?
63.2 56.0 55.8 58.4 (60.8)?7 ?
57.0 49.0 48.6 52.5 (56.0)Czech ?6 49.5 55.1 44.5 43.9 44.0 (52.3)?7 48.0 54.2 42.9 24.5 34.3 (51.1)Danish ?6 38.6 22.2 37.8 17.1 21.4 (29.8)Dutch ?6 44.2 46.6 40.8 51.3 48.0 (48.7)English ?7 49.2 29.6 39.3 57.6 58.2 (75.0)German ?6 44.8 39.1 34.1 54.5 56.2 (71.2)Greek ?6 20.2 26.9 23.7 45.0 45.4 (52.2)Hungarian ?7 51.8 58.2 24.8 52.9 58.3 (67.6)Italian ?7 43.3 40.7 56.8 31.1 34.9 (44.9)Japanese ?6 50.8 22.7 32.6 63.7 63.0 (68.9)Portuguese ?6 50.6 72.4 38.0 72.7 74.5 (81.1)Slovenian ?6 18.1 35.2 42.1 50.8 50.9 (57.3)Spanish ?6 51.9 28.2 57.0 61.7 61.4 (73.2)Swedish ?6 48.2 50.7 46.6 48.6 49.7 (62.1)Turkish ?6 ?
34.4 28.0 32.9 29.2 (33.2)?7 15.7 44.8 42.1 41.7 37.9 (42.4)Average: 40.0 42.9 40.0 47.6 48.6 (57.8)Table 4: Blind evaluation on 2006/7 CoNLL test sets (allsentences) for our full networks (IFJ and GT), previousstate-of-the-art systems of Spitkovsky et al(2012b) andMarec?ek and ?Zabokrtsky?
(2012), and three-way combi-nation with SAJ (CS, including results up to length ten).a search algorithm for encoding decipherment prob-lems that guarantees to quickly converge on optimalsolutions.
Convexity can be ideal for comparativeanalyses, by eliminating dependence on initial con-ditions.
But for many NLP tasks, including grammarinduction, the most relevant known objective func-tions are still riddled with local optima.
Renewed ef-forts to find exact solutions (Eisner, 2012; Gormleyand Eisner, 2013) may be a good fit for the smallerand simpler, earlier stages of our iterative networks.Multi-start methods (Solis and Wets, 1981) canrecover certain global extrema almost surely (i.e.,with probability approaching one).
Moreover, ran-dom restarts via uniform probability measures canbe optimal, in a worst-case-analysis sense, with par-allel processing sometimes leading to exponentialspeed-ups (Hu et al 1994).
This approach is rarelyemphasized in NLP literature.
For instance, Mooreand Quirk (2008) demonstrated consistent, substan-tial gains from random restarts in statistical machinetranslation (but also suggested better and faster re-placements ?
see below); Ravi and Knight (2009,?5, Figure 8) found random restarts for EM to becrucial in parts-of-speech disambiguation.
However,other reviews are few and generally negative (Kimand Mooney, 2010; Martin-Brualla et al 2010).Iterated local search methods (Hoos and Stu?tzle,2004; Johnson et al 1988, inter alia) escape lo-cal basins of attraction by perturbing candidate so-lutions, without undoing all previous work.
?Large-step?
moves can come from jittering (Hinton andRoweis, 2003), dithering (Price et al 2005, Ch.
2)or smoothing (Bhargava and Kondrak, 2009).
Non-improving ?sideways?
moves offer substantial helpwith hard satisfiability problems (Selman et al1992); and injecting non-random noise (Selman etal., 1994), by introducing ?uphill?
moves via mix-tures of random walks and greedy search strate-gies, does better than random noise alone or simu-lated annealing (Kirkpatrick et al 1983).
In NLP,Moore and Quirk?s (2008) random walks from pre-vious local optima were faster than uniform sam-pling and also increased BLEU scores; Elsner andSchudy (2009) showed that local search can outper-form greedy solutions for document clustering andchat disentanglement tasks; and Mei et al(2001)incorporated tabu search (Glover, 1989; Glover andLaguna, 1993, Ch.
3) into HMM training for ASR.Genetic algorithms are a fusion of what?s best inlocal search and multi-start methods (Houck et al1996), exploiting a problem?s structure to combinevalid parts of any partial solutions (Holland, 1975;Goldberg, 1989).
Evolutionary heuristics proveduseful in the induction of phonotactics (Belz, 1998),text planning (Mellish et al 1998), factored mod-eling of morphologically-rich languages (Duh andKirchhoff, 2004) and plot induction for story gener-ation (McIntyre and Lapata, 2010).
Multi-objectivegenetic algorithms (Fonseca and Fleming, 1993) canhandle problems with equally important but con-flicting criteria (Stadler, 1988), using Pareto-optimalensembles.
They are especially well-suited to lan-guage, which evolves under pressures from compet-ing (e.g., speaker, listener and learner) constraints,and have been used to model configurations of vow-els and tone systems (Ke et al 2003).
Our transformand join mechanisms also exhibit some features ofgenetic search, and make use of competing objec-1991tives: good sets of parse trees must make sense bothlexicalized and with word categories, to rich and im-poverished models of grammar, and for both long,complex sentences and short, simple text fragments.This selection of text filters is a specialized caseof more general ?data perturbation?
techniques ?even cycling over randomly chosen mini-batchesthat partition a data set helps avoid some local op-tima (Liang and Klein, 2009).
Elidan et al(2002)suggested how example-reweighing could cause ?in-formed?
changes, rather than arbitrary damage, toa hypothesis.
Their (adversarial) training schemeguided learning toward improved generalizations,robust against input fluctuations.
Language learn-ing has a rich history of reweighing data via (co-operative) ?starting small?
strategies (Elman, 1993),beginning from simpler or more certain cases.
Thisfamily of techniques has met with success in semi-supervised named entity classification (Collins andSinger, 1999; Yarowsky, 1995),11 parts-of-speechinduction (Clark, 2000; 2003), and language model-ing (Krueger and Dayan, 2009; Bengio et al 2009),in addition to unsupervised parsing (Spitkovsky etal., 2009; Tu and Honavar, 2011; Cohn et al 2011).12 ConclusionWe proposed several simple algorithms for combin-ing grammars and showed their usefulness in merg-ing the outputs of iterative and static grammar in-duction systems.
Unlike conventional system com-bination methods, e.g., in machine translation (Xiaoet al 2010), ours do not require incoming mod-els to be of similar quality to make improvements.We exploited these properties of the combiners toreconcile grammars induced by different views ofdata (Blum and Mitchell, 1998).
One such view re-tains just the simple sentences, making it easier torecognize root words.
Another splits text into manyinter-punctuation fragments, helping learn word as-sociations.
The induced dependency trees can them-selves also be viewed not only as directed structuresbut also as skeleton parses, facilitating the recoveryof correct polarities for unlabeled dependency arcs.By reusing templates, as in dynamic Bayesiannetwork (DBN) frameworks (Koller and Friedman,11The so-called Yarowsky-cautious modification of the orig-inal algorithm for unsupervised word-sense disambiguation.2009, ?6.2.2), we managed to specify relatively?deep?
learning architectures without sacrificing(too much) clarity or simplicity.
On a still morespeculative note, we see two (admittedly, tenuous)connections to human cognition.
First, the benefitsof not normalizing probabilities, when symmetriz-ing, might be related to human language process-ing through the base-rate fallacy (Bar-Hillel, 1980;Kahneman and Tversky, 1982) and the availabilityheuristic (Chapman, 1967; Tversky and Kahneman,1973), since people are notoriously bad at probabil-ity (Attneave, 1953; Kahneman and Tversky, 1972;Kahneman and Tversky, 1973).
And second, inter-mittent ?unlearning?
?
though perhaps not of thekind that takes place inside of our transforms ?is an adaptation that can be essential to cognitivedevelopment in general, as evidenced by neuronalpruning in mammals (Craik and Bialystok, 2006;Low and Cheng, 2006).
?Forgetful EM?
strategiesthat reset subsets of parameters may thus, possibly,be no less relevant to unsupervised learning than is?partial EM,?
which only suppresses updates, otherEM variants (Neal and Hinton, 1999), or ?dropouttraining?
(Hinton et al 2012; Wang and Manning,2013), which is important in supervised settings.Future parsing models, in grammar induction,may benefit by modeling head-dependent relationsseparately from direction.
As frequently employedin tasks like semantic role labeling (Carreras andMa`rquez, 2005) and relation extraction (Sun et al2011), it may be easier to first establish existence,before trying to understand its nature.
Other keynext steps may include exploring more intelligentways of combining systems (Surdeanu and Man-ning, 2010; Petrov, 2010) and automating the op-erator discovery process.
Furthermore, we are opti-mistic that both count transforms and model recom-bination could be usefully incorporated into sam-pling methods: although symmetrized models mayhave higher cross-entropies, hence prone to rejectionin vanilla MCMC, they could work well as seedsin multi-chain designs; existing algorithms, such asMCMCMC (Geyer, 1991), which switch contentsof adjacent chains running at different temperatures,may also benefit from introducing the option to com-bine solutions, in addition to just swapping them.1992AcknowledgmentsWe thank Yun-Hsuan Sung, for early-stage discussionson ways of extending ?baby steps,?
Elias Ponvert, forsharing all of the relevant experimental results and eval-uation scripts from his work with Jason Baldridge andKatrin Erk, and the anonymous reviewers, for theirhelpful comments on the draft version of this paper.Funded, in part, by Defense Advanced Research ProjectsAgency (DARPA) Deep Exploration and Filtering ofText (DEFT) Program, under Air Force Research Lab-oratory (AFRL) prime contract no.
FA8750-13-2-0040.Any opinions, findings, and conclusion or recommen-dations expressed in this material are those of the au-thors and do not necessarily reflect the view of theDARPA, AFRL, or the US government.
Once again, thefirst author thanks Moofus.ReferencesH.
Alshawi.
1996.
Head automata for speech translation.
InICSLP.F.
Attneave.
1953.
Psychological probability as a function ofexperienced frequency.
Experimental Psychology, 46.M.
Bar-Hillel.
1980.
The base-rate fallacy in probability judg-ments.
Acta Psychologica, 44.A.
Belz.
1998.
Discovering phonotactic finite-state automataby genetic search.
In COLING-ACL.Y.
Bengio, J. Louradour, R. Collobert, and J. Weston.
2009.Curriculum learning.
In ICML.A.
Bhargava and G. Kondrak.
2009.
Multiple word alignmentwith profile hidden Markov models.
In NAACL-HLT: Stu-dent Research and Doctoral Consortium.Y.
Bisk and J. Hockenmaier.
2012.
Simple robust grammarinduction with combinatory categorial grammars.
In AAAI.A.
Blum and T. Mitchell.
1998.
Combining labeled and unla-beled data with co-training.
In COLT.P.
Blunsom and T. Cohn.
2010.
Unsupervised induction of treesubstitution grammars for dependency parsing.
In EMNLP.S.
Buchholz and E. Marsi.
2006.
CoNLL-X shared task onmultilingual dependency parsing.
In CoNLL.X.
Carreras and L. Ma`rquez.
2005.
Introduction to the CoNLL-2005 shared task: Semantic role labeling.
In CoNLL.L.
J. Chapman.
1967.
Illusory correlation in observational re-port.
Verbal Learning and Verbal Behavior, 6.A.
Clark.
2000.
Inducing syntactic categories by context distri-bution clustering.
In CoNLL-LLL.A.
Clark.
2003.
Combining distributional and morphologicalinformation for part of speech induction.
In EACL.S.
B. Cohen and N. A. Smith.
2010.
Viterbi training for PCFGs:Hardness results and competitiveness of uniform initializa-tion.
In ACL.T.
Cohn, P. Blunsom, and S. Goldwater.
2011.
Inducing tree-substitution grammars.
JMLR.M.
Collins and Y.
Singer.
1999.
Unsupervised models fornamed entity classification.
In EMNLP.M.
Collins.
1999.
Head-Driven Statistical Models for NaturalLanguage Parsing.
Ph.D. thesis, University of Pennsylvania.E.
Corlett and G. Penn.
2010.
An exact A?
method for deci-phering letter-substitution ciphers.
In ACL.F.
I. M. Craik and E. Bialystok.
2006.
Cognition through thelifespan: mechanisms of change.
TRENDS in Cognitive Sci-ences, 10.C.
de Marcken.
1995.
Lexical heads, phrase structure and theinduction of grammar.
In WVLC.K.
Duh and K. Kirchhoff.
2004.
Automatic learning of lan-guage model structure.
In COLING.J.
Eisner.
2012.
Grammar induction: Beyond local search.
InICGI.G.
Elidan, M. Ninio, N. Friedman, and D. Schuurmans.
2002.Data perturbation for escaping local maxima in learning.
InAAAI.J.
L. Elman.
1993.
Learning and development in neural net-works: The importance of starting small.
Cognition, 48.M.
Elsner and W. Schudy.
2009.
Bounding and comparingmethods for correlation clustering beyond ILP.
In NAACL-HLT: Integer Linear Programming for NLP.C.
M. Fonseca and P. J. Fleming.
1993.
Genetic algorithms formultiobjective optimization: Formulation, discussion andgeneralization.
In ICGA.C.
J. Geyer.
1991.
Markov chain Monte Carlo maximum like-lihood.
In Interface Symposium.J.
Gillenwater, K. Ganchev, J. Grac?a, F. Pereira, and B. Taskar.2010.
Posterior sparsity in unsupervised dependency pars-ing.
Technical report, University of Pennsylvania.K.
Gimpel and N. A. Smith.
2012.
Concavity and initializationfor unsupervised dependency parsing.
In NAACL-HLT.F.
Glover and M. Laguna.
1993.
Tabu search.
In C. R.Reeves, editor, Modern Heuristic Techniques for Combina-torial Problems.
Blackwell Scientific Publications.F.
Glover.
1989.
Tabu search ?
Part I. ORSA Journal onComputing, 1.D.
E. Goldberg.
1989.
Genetic Algorithms in Search, Opti-mization & Machine Learning.
Addison-Wesley.D.
Golland, J. DeNero, and J. Uszkoreit.
2012.
A feature-rich constituent context model for grammar induction.
InEMNLP-CoNLL.M.
R. Gormley and J. Eisner.
2013.
Nonconvex global opti-mization for latent-variable models.
In ACL.W.
P. Headden, III, M. Johnson, and D. McClosky.
2009.
Im-proving unsupervised dependency parsing with richer con-texts and smoothing.
In NAACL-HLT.G.
Hinton and S. Roweis.
2003.
Stochastic neighbor embed-ding.
In NIPS.G.
E. Hinton, N. Srivastava, A. Krizhevsky, I. Sutskever, andR.
R. Salakhutdinov.
2012.
Improving neural networks bypreventing co-adaptation of feature detectors.
In ArXiv.J.
H. Holland.
1975.
Adaptation in Natural and Artificial Sys-tems: An Introductory Analysis with Applications to Biology,Control, and Artificial Intelligence.
University of MichiganPress.H.
H. Hoos and T. Stu?tzle.
2004.
Stochastic Local Search:Foundations and Applications.
Morgan Kaufmann.1993C.
R. Houck, J.
A. Joines, and M. G. Kay.
1996.
Comparisonof genetic algorithms, random restart, and two-opt switchingfor solving large location-allocation problems.
Computers& Operations Research, 23.X.
Hu, R. Shonkwiler, and M. C. Spruill.
1994.
Randomrestarts in global optimization.
Technical report, GT.Y.
Huang, M. Zhang, and C. L. Tan.
2012.
Improved con-stituent context model with features.
In PACLIC.F.
Jelinek and R. L. Mercer.
1980.
Interpolated estimationof Markov source parameters from sparse data.
In PatternRecognition in Practice.D.
S. Johnson, C. H. Papadimitriou, and M. Yannakakis.
1988.How easy is local search?
Journal of Computer and SystemSciences, 37.D.
Kahneman and A. Tversky.
1972.
Subjective probability: Ajudgment of representativeness.
Cognitive Psychology, 3.D.
Kahneman and A. Tversky.
1973.
On the psychology ofprediction.
Psychological Review, 80.D.
Kahneman and A. Tversky.
1982.
Evidential impact of baserates.
In D. Kahneman, P. Slovic, and A. Tversky, editors,Judgment under uncertainty: Heuristics and biases.
Cam-bridge University Press.J.
Ke, M. Ogura, and W. S.-Y.
Wang.
2003.
Optimization mod-els of sound systems using genetic algorithms.
Computa-tional Linguistics, 29.J.
Kim and R. J. Mooney.
2010.
Generative alignment andsemantic parsing for learning from ambiguous supervision.In COLING.S.
Kirkpatrick, C. D. Gelatt, Jr., and M. P. Vecchi.
1983.
Opti-mization by simulated annealing.
Science, 220.D.
Klein and C. D. Manning.
2002.
A generative constituent-context model for improved grammar induction.
In ACL.D.
Klein and C. D. Manning.
2004.
Corpus-based induction ofsyntactic structure: Models of dependency and constituency.In ACL.D.
Koller and N. Friedman.
2009.
Probabilistic GraphicalModels: Principles and Techniques.
MIT Press.K.
A. Krueger and P. Dayan.
2009.
Flexible shaping: Howlearning in small steps helps.
Cognition, 110.D.
Lashkari and P. Golland.
2008.
Convex clustering withexemplar-based models.
In NIPS.P.
Liang and D. Klein.
2009.
Online EM for unsupervisedmodels.
In NAACL-HLT.L.
K. Low and H.-J.
Cheng.
2006.
Axon pruning: an essen-tial step underlying the developmental plasticity of neuronalconnections.
Royal Society of London Philosophical Trans-actions Series B, 361.M.
P. Marcus, B. Santorini, and M. A. Marcinkiewicz.
1993.Building a large annotated corpus of English: The PennTreebank.
Computational Linguistics, 19.D.
Marec?ek and M. Straka.
2013.
Stop-probability estimatescomputed on a large corpus improve unsupervised depen-dency parsing.
In ACL.D.
Marec?ek and Z.
?Zabokrtsky?.
2011.
Gibbs sampling withtreeness constraint in unsupervised dependency parsing.
InROBUS.D.
Marec?ek and Z.
?Zabokrtsky?.
2012.
Exploiting reducibilityin unsupervised dependency parsing.
In EMNLP-CoNLL.R.
Martin-Brualla, E. Alfonseca, M. Pasca, K. Hall, E. Robledo-Arnuncio, and M. Ciaramita.
2010.
Instance sense inductionfrom attribute sets.
In COLING.N.
McIntyre and M. Lapata.
2010.
Plot induction and evolu-tionary search for story generation.
In ACL.X.-d. Mei, S.-h. Sun, J.-s. Pan, and T.-Y.
Chen.
2001.
Op-timization of HMM by the tabu search algorithm.
In RO-CLING.C.
Mellish, A. Knott, J. Oberlander, and M. O?Donnell.
1998.Experiments using stochastic search for text planning.
InINLG.R.
C. Moore and C. Quirk.
2008.
Random restarts in min-imum error rate training for statistical machine translation.In COLING.T.
Naseem and R. Barzilay.
2011.
Using semantic cues to learnsyntax.
In AAAI.R.
M. Neal and G. E. Hinton.
1999.
A view of the EM al-gorithm that justifies incremental, sparse, and other variants.In M. I. Jordan, editor, Learning in Graphical Models.
MITPress.J.
Nivre, J.
Hall, S. Ku?bler, R. McDonald, J. Nilsson, S. Riedel,and D. Yuret.
2007.
The CoNLL 2007 shared task on de-pendency parsing.
In EMNLP-CoNLL.M.
A. Paskin.
2001a.
Cubic-time parsing and learning algo-rithms for grammatical bigram models.
Technical report,UCB.M.
A. Paskin.
2001b.
Grammatical bigrams.
In NIPS.F.
Pereira and Y. Schabes.
1992.
Inside-outside reestimationfrom partially bracketed corpora.
In ACL.S.
Petrov.
2010.
Products of random latent variable grammars.In NAACL-HLT.E.
Ponvert, J. Baldridge, and K. Erk.
2011.
Simple unsuper-vised grammar induction from raw text with cascaded finitestate models.
In ACL-HLT.K.
V. Price, R. M. Storn, and J.
A. Lampinen.
2005.
Differ-ential Evolution: A Practical Approach to Global Optimiza-tion.
Springer.S.
Ravi and K. Knight.
2009.
Minimized models for unsuper-vised part-of-speech tagging.
In ACL-IJCNLP.K.
Rose.
1998.
Deterministic annealing for clustering, com-pression, classification, regression and related optmizationproblems.
Proceedings of the IEEE, 86.Y.
Seginer.
2007.
Fast unsupervised incremental parsing.
InACL.B.
Selman, H. Levesque, and D. Mitchell.
1992.
A new methodfor solving hard satisfiability problems.
In AAAI.B.
Selman, H. A. Kautz, and B. Cohen.
1994.
Noise strategiesfor improving local search.
In AAAI.F.
J. Solis and R. J.-B.
Wets.
1981.
Minimization by randomsearch techniques.
Mathematics of Operations Research, 6.V.
I. Spitkovsky, H. Alshawi, and D. Jurafsky.
2009.
BabySteps: How ?Less is More?
in unsupervised dependencyparsing.
In GRLL.V.
I. Spitkovsky, H. Alshawi, D. Jurafsky, and C. D. Manning.2010.
Viterbi training improves unsupervised dependencyparsing.
In CoNLL.1994V.
I. Spitkovsky, H. Alshawi, and D. Jurafsky.
2011a.
LateenEM: Unsupervised training with multiple objectives, appliedto dependency grammar induction.
In EMNLP.V.
I. Spitkovsky, H. Alshawi, and D. Jurafsky.
2011b.
Punctu-ation: Making a point in unsupervised dependency parsing.In CoNLL.V.
I. Spitkovsky, A. X. Chang, H. Alshawi, and D. Jurafsky.2011c.
Unsupervised dependency parsing without gold part-of-speech tags.
In EMNLP.V.
I. Spitkovsky, H. Alshawi, and D. Jurafsky.
2012a.
Boot-strapping dependency grammar inducers from incompletesentence fragments via austere models.
In ICGI.V.
I. Spitkovsky, H. Alshawi, and D. Jurafsky.
2012b.
Threedependency-and-boundary models for grammar induction.In EMNLP-CoNLL.W.
Stadler, editor.
1988.
Multicriteria Optimization in Engi-neering and in the Sciences.
Plenum Press.A.
Sun, R. Grishman, and S. Sekine.
2011.
Semi-supervisedrelation extraction with large-scale word clustering.
In ACL.M.
Surdeanu and C. D. Manning.
2010.
Ensemble models fordependency parsing: Cheap and good?
In NAACL-HLT.K.
Tu and V. Honavar.
2011.
On the utility of curricula inunsupervised learning of probabilistic grammars.
In IJCAI.K.
Tu and V. Honavar.
2012.
Unambiguity regularizationfor unsupervised learning of probabilistic grammars.
InEMNLP-CoNLL.A.
Tversky and D. Kahneman.
1973.
Availability: A heuristicfor judging frequency and probability.
Cognitive Psychol-ogy, 5.S.
I. Wang and C. D. Manning.
2013.
Fast dropout training.
InICML.Q.
I. Wang, D. Schuurmans, and D. Lin.
2008.
Semi-supervised convex training for dependency parsing.
In HLT-ACL.T.
Xiao, J. Zhu, M. Zhu, and H. Wang.
2010.
Boosting-basedsystem combination for machine translation.
In ACL.D.
Yarowsky.
1995.
Unsupervised word sense disambiguationrivaling supervised methods.
In ACL.1995
