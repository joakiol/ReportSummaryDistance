Proceedings of the Fifth SIGHAN Workshop on Chinese Language Processing, pages 126?129,Sydney, July 2006. c?2006 Association for Computational LinguisticsVoting between Dictionary-based and Subword Tagging Models forChinese Word SegmentationDong Song and Anoop SarkarSchool of Computing Science, Simon Fraser UniversityBurnaby, BC, Canada V5A1S6{dsong,anoop}@cs.sfu.caAbstractThis paper describes a Chinese word seg-mentation system that is based on ma-jority voting among three models: a for-ward maximum matching model, a con-ditional random field (CRF) model us-ing maximum subword-based tagging, anda CRF model using minimum subword-based tagging.
In addition, it contains apost-processing component to deal withinconsistencies.
Testing on the closedtrack of CityU, MSRA and UPUC corporain the third SIGHAN Chinese Word Seg-mentation Bakeoff, the system achieves aF-score of 0.961, 0.953 and 0.919, respec-tively.1 IntroductionTokenizing input text into words is the first step ofany text analysis task.
In Chinese, a sentence iswritten as a string of characters, to which we shallrefer by their traditional name of hanzi, withoutseparations between words.
As a result, before anytext analysis on Chinese, word segmentation taskhas to be completed so that each word is ?isolated?by the word-boundary information.Participating in the third SIGHAN ChineseWord Segmentation Bakeoff in 2006, our systemis tested on the closed track of CityU, MSRA andUPUC corpora.
The sections below provide a de-tailed description of the system and our experi-mental results.2 System DescriptionIn our segmentation system, a hybrid strategyis applied (Figure 1): First, forward maximummatching (Chen and Liu, 1992), which is adictionary-based method, is used to generate asegmentation result.
Also, the CRF model us-ing maximum subword-based tagging (Zhang etal., 2006) and the CRF model using minimumsubword-based tagging, both of which are statis-tical methods, are used individually to solve theproblem.
In the next step, the solutions fromthese three methods are combined via the hanzi-level majority voting algorithm.
Then, a post-processing procedure is applied in order to to getthe final output.
This procedure merges adjoin-ing words to match the dictionary entries and thensplits words which are inconsistent with entries inthe training corpus.ForwardPost?processingMajority VotingResultInput SentenceTaggingSubword?basedMinimumCRF withTaggingSubword?basedMaximumCRF withMatchingMaximumFigure 1: Outline of the segmentation process2.1 Forward Maximum MatchingThe maximum matching algorithm is a greedysegmentation approach.
It proceeds through thesentence, mapping the longest word at each pointwith an entry in the dictionary.
In our system,the well-known forward maximum matching algo-rithm (Chen and Liu, 1992) is implemented.The maximum matching approach is simple andefficient, and it results in high in-vocabulary ac-curacy; However, the small size of the dictionary,which is obtained only from the training data, isa major bottleneck for this approach to be appliedby itself.1262.2 CRF Model with MaximumSubword-based TaggingConditional random fields (CRF), a statistical se-quence modeling approach (Lafferty et al, 2001),has been widely applied in various sequencelearning tasks including Chinese word segmen-tation.
In this approach, most existing methodsuse the character-based IOB tagging.
For ex-ample, ??
(all) ??(extremely important)?is labeled as ??
(all)/O ?
(until)/B (close)/I?
(heavy)/I(demand)/I?.Recently (Zhang et al, 2006) proposed a maxi-mum subword-based IOB tagger for Chinese wordsegmentation, and our system applies their ap-proach which obtains a very high accuracy on theshared task data from previous SIGHAN compe-titions.
In this method, all single-hanzi words andthe top frequently occurring multi-hanzi words areextracted from the training corpus to form the lexi-con subset.
Then, each word in the training corpusis segmented for IOB tagging, with the forwardmaximum matching algorithm, using the formedlexicon subset as the dictionary.
In the aboveexample, the tagging labels become ??(all)/O?
(until)/B (close)/I ?(important)/I?, as-suming that ??(important)?
is the longest sub-word in this word, and it is one of the top fre-quently occurring words in the training corpus.After tagging the training corpus, we use thepackage CRF++1 to train the CRF model.
Sup-pose w0 represents the current word, w?1 is thefirst word to the left, w?2 is the second word tothe left, w1 is the first word to the right, and w2is the second word to the right, then in our experi-ments, the types of unigram features used includew0, w?1, w1, w?2, w2, w0w?1, w0w1, w?1w1,w?2w?1, and w2w0.
In addition, only combina-tions of previous observation and current observa-tion are exploited as bigram features.2.3 CRF Model with MinimumSubword-based TaggingIn our third model, we applies a similar approachas in the previous section.
However, instead offinding the maximum subwords, we explore theminimum subwords.
At the beginning, we buildthe dictionary using the whole training corpus.Then, for each word in the training data, a forwardshortest matching is used to get the sequence ofminimum-length subwords, and this sequence is1available from http://www/chasen.org/?taku/softwaretagged in the same IOB format as before.
Suppose?a?, ?ac?, ?de?
and ?acde?
are the only entries inthe dictionary.
Then, for the word ?acde?, the se-quence of subwords is ?a?, ?c?
and ?de?, and thetags assigned to ?acde?
are ?a/B c/I de/I?.After tagging the training data set, CRF++package is executed again to train this type ofmodel, using the identical unigram and bigramfeature sets that are used in the previous model.Meanwhile, the unsegmented test data is seg-mented by the forward shortest matching algo-rithm.
After this initial segmentation process, theresult is fed into the trained CRF model for re-segmentation by assigning IOB tags.2.4 Majority VotingHaving the segmentation results from the abovethree models in hand, in this next step, we adoptthe hanzi-level majority voting algorithm.
First,for each hanzi in a segmented sentence, we tag iteither as ?B?
if it is the first hanzi of a word ora single-hanzi word, or as ?I?
otherwise.
Then,for a given hanzi in the results from those threemodels, if at least two of the models provide theidentical tag, it will be assigned that tag.
For in-stance, suppose ?a c de?
is the segmentation resultvia forward maximum matching, and it is also theresult from CRF model with maximum subword-based tagging, and ?ac d e?
is the result from thethird model.
Then, for ?a?, since all of them as-sign ?B?
to it, ?a?
is given the ?B?
tag; for ?c?,because two of segmentations tag it as ?B?, ?c?
isgiven the ?B?
tag as well.
Similarly, the tag foreach remaining hanzi is determined by this major-ity voting process, and we get ?a c de?
as the resultfor this example.To test the performance of each of the threemodels and that of the majority voting, we di-vide the MSRA corpus into training set and held-out set.
Throughout all the experiments we con-ducted, we discover that those two CRF modelsperform much better than the pure hanzi-basedCRF method, and that the voting process improvesthe performance further.2.5 Post-processingWhile analyzing errors with the segmentation re-sult from the held-out set, we find two incon-sistency problems: First, the inconsistency be-tween the dictionary and the result: that is, certainwords that appear in the dictionary are separatedinto consecutive words in the test result; Second,127the inconsistency among words in the dictionary;For instance, both ?)????
(scientific research)and ?)?(science)??(research)?
appear in thetraining corpus.To deal with the first phenomena, for the seg-mented result, we try to merge adjoining words tomatch the dictionary entries.
Suppose ?a b c de?are the original voting result, and ?ab?, ?abc?
and?cd?
form the dictionary.
Then, we merge ?a?, ?b?and ?c?
together to get the longest match with thedictionary.
Therefore, the output is ?abc de?.For the second problem, we introduce the splitprocedure.
In our system, we only consider twoconsecutive words.
First, all bigrams are extractedfrom the training corpus, and their frequencies arecounted.
After that, for example, if ?a b?
appearsmore often than ?ab?, then whenever in the testresult we encounter ?ab?, we split it into ?a b?.The post-processing steps detailed above at-tempt to maximize the value of known words inthe training data as well as attempting to deal withthe word segmentation inconsistencies in the train-ing data.3 Experiments and AnalysisThe third International Chinese LanguageProcessing Bakeoff includes four different cor-pora, Academia Sinica (CKIP), City Universityof Hong Kong (CityU), Microsoft Research(MSRA), and University of Pennsylvania andUniversity of Colorado, Boulder (UPUC), for theword segmentation task.In this bakeoff, we test our system in CityU,MSRA and UPUC corpora, and follow the closedtrack.
That is, we only use training material fromthe training data for the particular corpus we aretesting on.
No other material or any type of ex-ternal knowledge is used, including part-of-speechinformation, externally generated word-frequencycounts, Arabic and Chinese numbers, feature char-acters for place names and common Chinese sur-names.3.1 Results on SIGHAN Bakeoff 2006To observe the result of majority voting and thecontribution of the post-processing step, the ex-periment is ran for each corpus by first producingthe outcome of majority voting and then producingthe output from the post-processing.
In each ex-periment, the precision (P ), recall (R), F-measure(F ), Out-of-Vocabulary rate (OOV ), OOV recallrate (ROOV ), and In-Vocabulary rate (RIV ) arerecorded.
Table 1,2,3 show the scores for theCityU corpus, for the MSRA corpus, and for theUPUC corpus, respectively.Majority Voting Post-processingP 0.956 0.958R 0.962 0.963F 0.959 0.961OOV 0.04 0.04ROOV 0.689 0.689RIV 0.974 0.974Table 1: Scores for CityU corpusMajority Voting Post-processingP 0.952 0.954R 0.952 0.952F 0.952 0.953OOV 0.034 0.034ROOV 0.604 0.604RIV 0.964 0.964Table 2: Scores for MSRA corpusMajority Voting Post-processingP 0.908 0.909R 0.927 0.929F 0.918 0.919OOV 0.088 0.088ROOV 0.628 0.628RIV 0.956 0.958Table 3: Scores for UPUC corpusFrom those tables, we can see that a simple ma-jority voting algorithm produces accuracy that ishigher than each individual system and reason-ably high F-scores overall.
In addition, the post-processing step indeed helps to improve the per-formance.3.2 Error analysisThe errors that occur in our system are mainly dueto the following three factors:First, there is inconsistency between the goldsegmentation and the training corpus.
Althoughthe inconsistency problem within the training cor-pus is intended to be tackled in the post-processingstep, we cannot conclude that the segmentation128for certain words in the gold test set alays fol-lows the convention in the training data set.
Forexample, in the MSRA training corpus, ??)u?
(Chinese government) is usually consideredas a single word; while in the gold test set, it isseparated as two words ??)?
(Chinese) and ?u?(government).
This inconsistency issue lowersthe system performance.
This problem, of course,affects all competing systems.Second, we don?t have specific steps to dealwith words with postfixes such as ?V?
(person).Compared to our system, (Zhang, 2005) proposeda segmentation system that contains morpholog-ically derived word recognition post-processingcomponent to solve this problem.
Lacking of sucha step prevents us from identifying certain typesof words such as ???V?
(worker) to be a singleword.In addition, the unknown words are still trou-blesome because of the limited size of the trainingcorpora.
In the class of unknown words, we en-counter person names, numbers, dates, organiza-tion names and words translated from languagesother than Chinese.
For example, in the producedCityU test result, the translated person name ?
?-b???
(Mihajlovic) is incorrectly separatedas ?
?-b?
and ????.
Moreover, in cer-tain cases, person names can also create ambigu-ity.
Take the name ?B?0?
(Qiu, Beifang) inUPUC test set for example, without understand-ing the meaning of the whole sentence, it is dif-ficult even for human to determine whether it isa person name or it represents ?B?
(autumn), ??0?
(north), with the meaning of ?the autumn in thenorth?.4 Alternative to Majority VotingIn designing the voting procedure, we also attemptto develop and use a segmentation lattice, whichproceeds using a similar underlying principle asthe one applied in (Xu et al, 2005).In our approach, for an input sentence, the seg-mentation result using each of our three models istransformed into an individual lattice.
Also, eachedge in the lattice is assigned a particular weight,according to certain features such as whether ornot the output word from that edge is in the dictio-nary.
After building the three lattices, one for eachmodel, we merge them together.
Then, the shortestpath, referring to the path that has the minimumweight, is extracted from the merged lattice, andtherefore, the segmentation result is determined bythis shortest path.However, in the time we had to run our experi-ments on the test data, we were unable to optimizethe edge weights to obtain high accuracy on someheld-out set from the training corpora.
So instead,we tried a simple method for finding edge weightsby uniformly distributing the weight for each fea-ture; Nevertheless, by testing on the shared taskdata from the 2005 SIGHAN bakeoff, the perfor-mance is not competitive, compared to our simplemajority voting method described above.
As a re-sult, we decide to abandon this approach for thisyear?s SIGHAN bakeoff.5 ConclusionOur Chinese word segmentation system is basedon majority voting among the initial outputs fromforward maximum matching, from a CRF modelwith maximum subword-based tagging, and froma CRF model with minimum subword-based tag-ging.
In addition, we experimented with varioussteps in post-processing which effectively boostedthe overall performance.In future research, we shall explore more so-phisticated ways of voting, including the contin-uing investigation on the segmentation lattice ap-proach.
Also, more powerful methods on howto accurately deal with unknown words, includingperson and place names, without external knowl-edge, will be studied as well.ReferencesKeh-jiann Chen, and Shing-Huan Liu.
1992.
Word Identi-fication for Mandarin Chinese Sentences.
In Fifth Inter-national Conference on Computational Linguistics, pages101?107.John Lafferty, Andrew McCallum, and Fernando Pereira.2001.
Conditional Random Fields: Probabilistic Modelsfor Segmenting and Labeling Sequence Data.
In Proc.
ofICML-2001, pages 591?598.Jia Xu, Evgeny Matusov, Richard Zens, and Hermann Ney.2005.
Integrated Chinese Word Segmentation in Statisti-cal Machine Translation.
In Proc.
of IWSLT-2005.Huipeng Zhang, Ting Liu, Jinshan Ma, and Xiantao Liu.2005.
Chinese Word Segmentation with Multiple Post-processors in HIT-IRLab.
In Proceedings of the FourthSIGHAN Workshop on Chinese Language Processing,pages 172?175.Ruiqiang Zhang, Genichiro Kikui, and Eiichiro Sumita.2006.
Subword-based Tagging by Conditional RandomFields for Chinese Word Segmentation.
In Proc.
of HLT-NAACL 2006.129
