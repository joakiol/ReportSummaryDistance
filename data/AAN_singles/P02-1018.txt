A simple pattern-matching algorithm for recovering empty nodesand their antecedents?Mark JohnsonBrown Laboratory for Linguistic Information ProcessingBrown UniversityMark Johnson@Brown.eduAbstractThis paper describes a simple pattern-matching algorithm for recovering emptynodes and identifying their co-indexed an-tecedents in phrase structure trees that donot contain this information.
The pat-terns are minimal connected tree frag-ments containing an empty node and allother nodes co-indexed with it.
This pa-per also proposes an evaluation proce-dure for empty node recovery procedureswhich is independent of most of the de-tails of phrase structure, which makes itpossible to compare the performance ofempty node recovery on parser outputwith the empty node annotations in a gold-standard corpus.
Evaluating the algorithmon the output of Charniak?s parser (Char-niak, 2000) and the Penn treebank (Mar-cus et al, 1993) shows that the pattern-matching algorithm does surprisingly wellon the most frequently occuring types ofempty nodes given its simplicity.1 IntroductionOne of the main motivations for research on pars-ing is that syntactic structure provides important in-formation for semantic interpretation; hence syntac-tic parsing is an important rst step in a variety of?
I would like to thank my colleages in the Brown Labora-tory for Linguistic Information Processing (BLLIP) as well asMichael Collins for their advice.
This research was supportedby NSF awards DMS 0074276 and ITR IIS 0085940.useful tasks.
Broad coverage syntactic parsers withgood performance have recently become available(Charniak, 2000; Collins, 2000), but these typicallyproduce as output a parse tree that only encodes lo-cal syntactic information, i.e., a tree that does notinclude any ?empty nodes?.
(Collins (1997) dis-cusses the recovery of one kind of empty node, viz.,WH-traces).
This paper describes a simple pattern-matching algorithm for post-processing the outputof such parsers to add a wide variety of empty nodesto its parse trees.Empty nodes encode additional information aboutnon-local dependencies between words and phraseswhich is important for the interpretation of construc-tions such as WH-questions, relative clauses, etc.1For example, in the noun phrase the man Sam likesthe fact the man is interpreted as the direct object ofthe verb likes is indicated in Penn treebank notationby empty nodes and coindexation as shown in Fig-ure 1 (see the next section for an explanation of whylikes is tagged VBZ t rather than the standard VBZ).The broad-coverage statistical parsers just men-tioned produce a simpler tree structure for such a rel-ative clause that contains neither of the empty nodesjust indicated.
Rather, they produce trees of the kindshown in Figure 2.
Unlike the tree depicted in Fig-ure 1, this type of tree does not explicitly representthe relationship between likes and the man.This paper presents an algorithm that takes as itsinput a tree without empty nodes of the kind shown1There are other ways to represent this information that donot require empty nodes; however, information about non-localdependencies must be represented somehow in order to interpretthese constructions.Computational Linguistics (ACL), Philadelphia, July 2002, pp.
136-143.Proceedings of the 40th Annual Meeting of the Association forNPNPDTtheNNmanSBARWHNP-1-NONE-0SNPNNPSamVPVBZ tlikesNP-NONE-*T*-1Figure 1: A tree containing empty nodes.in Figure 2 and modies it by inserting empty nodesand coindexation to produce a the tree shown in Fig-ure 1.
The algorithm is described in detail in sec-tion 2.
The standard Parseval precision and recallmeasures for evaluating parse accuracy do not mea-sure the accuracy of empty node and antecedent re-covery, but there is a fairly straightforward extensionof them that can evaluate empty node and antecedentrecovery, as described in section 3.
The rest of thissection provides a brief introduction to empty nodes,especially as they are used in the Penn Treebank.Non-local dependencies and displacement phe-nomena, such as Passive and WH-movement, havebeen a central topic of generative linguistics sinceits inception half a century ago.
However, currentlinguistic research focuses on explaining the pos-sible non-local dependencies, and has little to sayabout how likely different kinds of dependenciesare.
Many current linguistic theories of non-localdependencies are extremely complex, and would bedifcult to apply with the kind of broad coverage de-scribed here.
Psycholinguists have also investigatedcertain kinds of non-local dependencies, and theirtheories of parsing preferences might serve as thebasis for specialized algorithms for recovering cer-tain kinds of non-local dependencies, such as WHdependencies.
All of these approaches require con-siderably more specialized linguitic knowledge thanthe pattern-matching algorithm described here.
Thisalgorithm is both simple and general, and can serveas a benchmark against which more complex ap-proaches can be evaluated.NPNPDTtheNNmanSBARSNPNNPSamVPVBZ tlikesFigure 2: A typical parse tree produced by broad-coverage statistical parser lacking empty nodes.The pattern-matching approach is not tied to anyparticular linguistic theory, but it does require a tree-bank training corpus from which the algorithm ex-tracts its patterns.
We used sections 2?21 of thePenn Treebank as the training corpus; section 24was used as the development corpus for experimen-tation and tuning, while the test corpus (section 23)was used exactly once (to obtain the results in sec-tion 3).
Chapter 4 of the Penn Treebank taggingguidelines (Bies et al, 1995) contains an extensivedescription of the kinds of empty nodes and the useof co-indexation in the Penn Treebank.
Table 1contains summary statistics on the distribution ofempty nodes in the Penn Treebank.
The entry withPOS SBAR and no label refers to a ?compound?type of empty structure labelled SBAR consisting ofan empty complementizer and an empty (moved) S(thus SBAR is really a nonterminal label rather thana part of speech); a typical example is shown inFigure 3.
As might be expected the distribution ishighly skewed, with most of the empty node tokensbelonging to just a few types.
Because of this, a sys-tem can provide good average performance on allempty nodes if it performs well on the most frequenttypes of empty nodes, and conversely, a system willperform poorly on average if it does not perform atleast moderately well on the most common types ofempty nodes, irrespective of how well it performs onmore esoteric constructions.2 A pattern-matching algorithmThis section describes the pattern-matching algo-rithm in detail.
In broad outline the algorithm canAntecedent POS Label Count DescriptionNP NP * 18,334 NP trace (e.g., Sam was seen *)NP * 9,812 NP PRO (e.g., * to sleep is nice)WHNP NP *T* 8,620 WH trace (e.g., the woman who you saw *T*)*U* 7,478 Empty units (e.g., $ 25 *U*)0 5,635 Empty complementizers (e.g., Sam said 0 Sasha snores)S S *T* 4,063 Moved clauses (e.g., Sam had to go, Sasha explained *T*)WHADVP ADVP *T* 2,492 WH-trace (e.g., Sam explained how to leave *T*)SBAR 2,033 Empty clauses (e.g., Sam had to go, Sasha explained (SBAR))WHNP 0 1,759 Empty relative pronouns (e.g., the woman 0 we saw)WHADVP 0 575 Empty relative pronouns (e.g., no reason 0 to leave)Table 1: The distribution of the 10 most frequent types of empty nodes and their antecedents in sections 2?21 of the Penn Treebank (there are approximately 64,000 empty nodes in total).
The ?label?
column givesthe terminal label of the empty node, the ?POS?
column gives its preterminal label and the ?Antecedent?column gives the label of its antecedent.
The entry with an SBAR POS and empty label corresponds to anempty compound SBAR subtree, as explained in the text and Figure 3.SINVS-1NPNNSchangesVPVBDoccured,,VPVBDsaidSBAR-NONE-0S-NONE-*T*-1NPNNPSamFigure 3: A parse tree containing an empty com-pound SBAR subtree.be regarded as an instance of the Memory-BasedLearning approach, where both the pattern extrac-tion and pattern matching involve recursively visit-ing all of the subtrees of the tree concerned.
It canalso be regarded as a kind of tree transformation, sothe overall system architecture (including the parser)is an instance of the ?transform-detransform?
ap-proach advocated by Johnson (1998).
The algorithmhas two phases.
The rst phase of the algorithmextracts the patterns from the trees in the trainingcorpus.
The second phase of the algorithm usesthese extracted patterns to insert empty nodes andindex their antecedents in trees that do not containempty nodes.
Before the trees are used in the train-ing and insertion phases they are passed through acommon preproccessing step, which relabels preter-minal nodes dominating auxiliary verbs and transi-tive verbs.2.1 Auxiliary and transitivity annotationThe preprocessing step relabels auxiliary verbs andtransitive verbs in all trees seen by the algorithm.This relabelling is deterministic and depends only onthe terminal (i.e., the word) and its preterminal label.Auxiliary verbs such as is and being are relabelled aseither a AUX or AUXG respectively.
The relabellingof auxiliary verbs was performed primarily becauseCharniak?s parser (which produced one of the testcorpora) produces trees with such labels; experi-ments (on the development section) show that aux-iliary relabelling has little effect on the algorithm?sperformance.The transitive verb relabelling sufxes the preter-minal labels of transitive verbs with ?
t?.
For ex-ample, in Figure 1 the verb likes is relabelled VBZ tin this step.
A verb is deemed transitive if its stemis followed by an NP without any grammatical func-tion annotation at least 50% of the time in the train-ing corpus; all such verbs are relabelled whether ornot any particular instance is followed by an NP.Intuitively, transitivity would seem to be a power-ful cue that there is an empty node following a verb.Experiments on the development corpus showed thattransitivity annotation provides a small but usefulimprovement to the algorithm?s performance.
TheSBARWHNP-1-NONE-0SNP VPVBZ t NP-NONE-*T*-1Figure 4: A pattern extracted from the tree displayedin Figure 1.accuracy of transitivity labelling was not systemati-cally evaluated here.2.2 Patterns and matchingsInformally, patterns are minimal connected treefragments containing an empty node and all nodesco-indexed with it.
The intuition is that the pathfrom the empty node to its antecedents species im-portant aspects of the context in which the emptynode can appear.There are many different possible ways of realiz-ing this intuition, but all of the ones tried gave ap-proximately similar results so we present the sim-plest one here.
The results given below were gener-ated where the pattern for an empty node is the min-imal tree fragment (i.e., connected set of local trees)required to connect the empty node with all of thenodes coindexed with it.
Any indices occuring onnodes in the pattern are systematically renumberedbeginning with 1.
If an empty node does not bearan index, its pattern is just the local tree containingit.
Figure 4 displays the single pattern that would beextracted corresponding to the two empty nodes inthe tree depicted in Figure 1.For this kind of pattern we dene pattern match-ing informally as follows.
If p is a pattern and t isa tree, then p matches t iff t is an extension of p ig-noring empty nodes in p. For example, the patterndisplayed in Figure 4 matches the subtree rooted un-der SBAR depicted in Figure 2.If a pattern p matches a tree t, then it is possibleto substitute p for the fragment of t that it matches.For example, the result of substituting the patternshown in Figure 4 for the subtree rooted under SBARdepicted in Figure 2 is the tree shown in Figure 1.Note that the substitution process must ?standardizeapart?
or renumber indices appropriately in order toavoid accidentally labelling empty nodes inserted bytwo independent patterns with the same index.Pattern matching and substitution can be denedmore rigorously using tree automata (G?ecseg andSteinby, 1984), but for reasons of space these def-initions are not given here.In fact, the actual implementation of patternmatching and substitution used here is considerablymore complex than just described.
It goes to somelengths to handle complex cases such as adjunctionand where two or more empty nodes?
paths cross(in these cases the pattern extracted consists of theunion of the local trees that constitute the patternsfor each of the empty nodes).
However, given thelow frequency of these constructions, there is prob-ably only one case where this extra complexity isjustied: viz., the empty compound SBAR subtreeshown in Figure 3.2.3 Empty node insertionSuppose we have a rank-ordered list of patterns (thenext subsection describes how to obtain such a list).The procedure that uses these to insert empty nodesinto a tree t not containing empty nodes is as fol-lows.
We perform a pre-order traversal of the sub-trees of t (i.e., visit parents before their children),and at each subtree we nd the set of patterns thatmatch the subtree.
If this set is non-empty we sub-stitute the highest ranked pattern in the set into thesubtree, inserting an empty node and (if required)co-indexing it with its antecedents.Note that the use of a pre-order traversal effec-tively biases the procedure toward ?deeper?, moreembedded patterns.
Since empty nodes are typi-cally located in the most embedded local trees ofpatterns (i.e., movement is usually ?upward?
in atree), if two different patterns (corresponding to dif-ferent non-local dependencies) could potentially in-sert empty nodes into the same tree fragment in t,the deeper pattern will match at a higher node in t,and hence will be substituted.
Since the substitu-tion of one pattern typically destroys the context fora match of another pattern, the shallower patternsno longer match.
On the other hand, since shal-lower patterns contain less structure they are likelyto match a greater variety of trees than the deeperpatterns, they still have ample opportunity to apply.Finally, the pattern matching process can bespeeded considerably by indexing patterns appropri-ately, since the number of patterns involved is quitelarge (approximately 11,000).
For patterns of thekind described here, patterns can be indexed on theirtopmost local tree (i.e., the pattern?s root node labeland the sequence of node labels of its children).2.4 Pattern extractionAfter relabelling preterminals as described above,patterns are extracted during a traversal of each ofthe trees in the training corpus.
Table 2 lists themost frequent patterns extracted from the Penn Tree-bank training corpus.
The algorithm also recordshow often each pattern was seen; this is shown inthe ?count?
column of Table 2.The next step of the algorithm determines approx-imately how many times each pattern can matchsome subtree of a version of the training corpus fromwhich all empty nodes have been removed (regard-less of whether or not the corresponding substitu-tions would insert empty nodes correctly).
This in-formation is shown under the ?match?
column in Ta-ble 2, and is used to lter patterns which would mostoften be incorrect to apply even though they match.If c is the count value for a pattern and m is its matchvalue, then the algorithm discards that pattern whenthe lower bound of a 67% condence interval for itssuccess probability (given c successes out of m tri-als) is less than 1/2.
This is a standard techniquefor ?discounting?
success probabilities from smallsample size data (Witten and Frank, 2000).
(As ex-plained immediately below, the estimates of c and mgiven in Table 2 are inaccurate, so whenever the es-timate of m is less than c we replace m by c in thiscalculation).
This pruning removes approximately2,000 patterns, leaving 9,000 patterns.The match value is obtained by making a secondpre-order traversal through a version of the train-ing data from which empty nodes are removed.
Itturns out that subtle differences in how the matchvalue is obtained make a large difference to the algo-rithm?s performance.
Initially we dened the matchvalue of a pattern to be the number of subtrees thatmatch that pattern in the training corpus.
But as ex-plained above, the earlier substitution of a deeperpattern may prevent smaller patterns from applying,so this simple denition of match value undoubt-edly over-estimates the number of times shallow pat-terns might apply.
To avoid this over-estimation, af-ter we have matched all patterns against a node ofa training corpus tree we determine the correct pat-tern (if any) to apply in order to recover the emptynodes that were originally present, and reinsert therelevant empty nodes.
This blocks the matching ofshallower patterns, reducing their match values andhence raising their success probability.
(Undoubt-edly the ?count?
values are also over-estimated inthe same way; however, experiments showed that es-timating count values in a similar manner to the wayin which match values are estimated reduces the al-gorithm?s performance).Finally, we rank all of the remaining patterns.
Weexperimented with several different ranking crite-ria, including pattern depth, success probability (i.e.,c/m) and discounted success probability.
Perhapssurprisingly, all produced similiar results on the de-velopment corpus.
We used pattern depth as theranking criterion to produce the results reported be-low because it ensures that ?deep?
patterns receivea chance to apply.
For example, this ensures thatthe pattern inserting an empty NP * and WHNP canapply before the pattern inserting an empty comple-mentizer 0.3 Empty node recovery evaluationThe previous section described an algorithm forrestoring empty nodes and co-indexing their an-tecedents.
This section describes two evaluationprocedures for such algorithms.
The rst, whichmeasures the accuracy of empty node recovery butnot co-indexation, is just the standard Parseval eval-uation applied to empty nodes only, viz., precisionand recall and scores derived from these.
In thisevaluation, each node is represented by a triple con-sisting of its category and its left and right string po-sitions.
(Note that because empty nodes dominatethe empty string, their left and right string positionsof empty nodes are always identical).Let G be the set of such empty node represen-tations derived from the ?gold standard?
evaluationcorpus and T the set of empty node representationsCount Match Pattern5816 6223 (S (NP (-NONE- *)) VP)5605 7895 (SBAR (-NONE- 0) S)5312 5338 (SBAR WHNP-1 (S (NP (-NONE- *T*-1)) VP))4434 5217 (NP QP (-NONE- *U*))1682 1682 (NP $ CD (-NONE- *U*))1327 1593 (VP VBN t (NP (-NONE- *)) PP)700 700 (ADJP QP (-NONE- *U*))662 1219 (SBAR (WHNP-1 (-NONE- 0)) (S (NP (-NONE- *T*-1)) VP))618 635 (S S-1 , NP (VP VBD (SBAR (-NONE- 0) (S (-NONE- *T*-1)))) .
)499 512 (SINV ??
S-1 , ??
(VP VBZ (S (-NONE- *T*-1))) NP .
)361 369 (SINV ??
S-1 , ??
(VP VBD (S (-NONE- *T*-1))) NP .
)352 320 (S NP-1 (VP VBZ (S (NP (-NONE- *-1)) VP)))346 273 (S NP-1 (VP AUX (VP VBN t (NP (-NONE- *-1)) PP)))322 467 (VP VBD t (NP (-NONE- *)) PP)269 275 (S ??
S-1 , ??
NP (VP VBD (S (-NONE- *T*-1))) .
)Table 2: The most common empty node patterns found in the Penn Treebank training corpus.
The Countcolumn is the number of times the pattern was found, and the Match column is an estimate of the number oftimes that this pattern matches some subtree in the training corpus during empty node recovery, as explainedin the text.derived from the corpus to be evaluated.
Then as isstandard, the precision P , recall R and f-score f arecalculated as follows:P = |G ?
T ||T |R = |G ?
T ||G|f = 2P RP + RTable 3 provides these measures for two differenttest corpora: (i) a version of section 23 of thePenn Treebank from which empty nodes, indicesand unary branching chains consisting of nodes ofthe same category were removed, and (ii) the treesproduced by Charniak?s parser on the strings of sec-tion 23 (Charniak, 2000).To evaluate co-indexation of empty nodes andtheir antecedents, we augment the representation ofempty nodes as follows.
The augmented represen-tation for empty nodes consists of the triple of cat-egory plus string positions as above, together withthe set of triples of all of the non-empty nodes theempty node is co-indexed with.
(Usually this setof antecedents is either empty or contains a singlenode).
Precision, recall and f-score are dened forthese augmented representations as before.Note that this is a particularly stringent evalua-tion measure for a system including a parser, sinceit is necessary for the parser to produce a non-emptynode of the correct category in the correct location toserve as an antecedent for the empty node.
Table 4provides these measures for the same two corporadescribed earlier.In an attempt to devise an evaluation measure forempty node co-indexation that depends less on syn-tactic structure we experimented with a modiedaugmented empty node representation in which eachantecedent is represented by its head?s category andlocation.
(The intuition behind this is that we donot want to penalize the empty node antecedent-nding algorithm if the parser misattaches modi-ers to the antecedent).
In fact this head-based an-tecedent representation yields scores very similiarto those obtained using the phrase-based represen-tation.
It seems that in the cases where the parserdoes not construct a phrase in the appropriate loca-tion to serve as the antecedent for an empty node,the syntactic structure is typically so distorted thateither the pattern-matcher fails or the head-ndingalgorithm does not return the ?correct?
head either.Empty node Section 23 Parser outputPOS Label P R f P R f(Overall) 0.93 0.83 0.88 0.85 0.74 0.79NP * 0.95 0.87 0.91 0.86 0.79 0.82NP *T* 0.93 0.88 0.91 0.85 0.77 0.810 0.94 0.99 0.96 0.86 0.89 0.88*U* 0.92 0.98 0.95 0.87 0.96 0.92S *T* 0.98 0.83 0.90 0.97 0.81 0.88ADVP *T* 0.91 0.52 0.66 0.84 0.42 0.56SBAR 0.90 0.63 0.74 0.88 0.58 0.70WHNP 0 0.75 0.79 0.77 0.48 0.46 0.47Table 3: Evaluation of the empty node restoration procedure ignoring antecedents.
Individual results arereported for all types of empty node that occured more than 100 times in the ?gold standard?
corpus (sec-tion 23 of the Penn Treebank); these are ordered by frequency of occurence in the gold standard.
Section 23is a test corpus consisting of a version of section 23 from which all empty nodes and indices were removed.The parser output was produced by Charniak?s parser (Charniak, 2000).Empty node Section 23 Parser outputAntecedant POS Label P R f P R f(Overall) 0.80 0.70 0.75 0.73 0.63 0.68NP NP * 0.86 0.50 0.63 0.81 0.48 0.60WHNP NP *T* 0.93 0.88 0.90 0.85 0.77 0.80NP * 0.45 0.77 0.57 0.40 0.67 0.500 0.94 0.99 0.96 0.86 0.89 0.88*U* 0.92 0.98 0.95 0.87 0.96 0.92S S *T* 0.98 0.83 0.90 0.96 0.79 0.87WHADVP ADVP *T* 0.91 0.52 0.66 0.82 0.42 0.56SBAR 0.90 0.63 0.74 0.88 0.58 0.70WHNP 0 0.75 0.79 0.77 0.48 0.46 0.47Table 4: Evaluation of the empty node restoration procedure including antecedent indexing, using the mea-sure explained in the text.
Other details are the same as in Table 4.4 ConclusionThis paper described a simple pattern-matching al-gorithm for restoring empty nodes in parse treesthat do not contain them, and appropriately index-ing these nodes with their antecedents.
The pattern-matching algorithm combines both simplicity andreasonable performance over the frequently occur-ing types of empty nodes.Performance drops considerably when using treesproduced by the parser, even though this parser?sprecision and recall is around 0.9.
Presumably thisis because the pattern matching technique requiresthat the parser correctly identify large tree fragmentsthat encode long-range dependencies not capturedby the parser.
If the parser makes a single parsingerror anywhere in the tree fragment matched by apattern, the pattern will no longer match.
This isnot unlikely since the statistical model used by theparser does not model these larger tree fragments.It suggests that one might improve performance byintegrating parsing, empty node recovery and an-tecedent nding in a single system, in which case thecurrent algorithm might serve as a useful baseline.Alternatively, one might try to design a ?sloppy?
pat-tern matching algorithm which in effect recognizesand corrects common parser errors in these construc-tions.Also, it is undoubtedly possible to build pro-grams that can do better than this algorithm onspecial cases.
For example, we constructed aBoosting classier which does recover *U* andempty complementizers 0 more accurately thanthe pattern-matcher described here (although thepattern-matching algorithm does quite well on theseconstructions), but this classier?s performance av-eraged over all empty node types was approximatelythe same as the pattern-matching algorithm.As a comparison of tables 3 and 4 shows, thepattern-matching algorithm?s biggest weakness is itsinability to correctly distinguish co-indexed NP *(i.e., NP PRO) from free (i.e., unindexed) NP *.This seems to be a hard problem, and lexical infor-mation (especially the class of the governing verb)seems relevant.
We experimented with specializedclassiers for determining if an NP * is co-indexed,but they did not perform much better than the algo-rithm presented here.
(Also, while we did not sys-tematically investigate this, there seems to be a num-ber of errors in the annotation of free vs. co-indexedNP * in the treebank).There are modications and variations on this al-gorithm that are worth exploring in future work.We experimented with lexicalizing patterns, butthe simple method we tried did not improve re-sults.
Inspired by results suggesting that the pattern-matching algorithm suffers from over-learning (e.g.,testing on the training corpus), we experimentedwith more abstract ?skeletal?
patterns, which im-proved performance on some types of empty nodesbut hurt performance on others, leaving overall per-formance approximately unchanged.
Possibly thereis a way to use both skeletal and the original kind ofpatterns in a single system.ReferencesAnn Bies, Mark Ferguson, Karen Katz, and Robert Mac-Intyre, 1995.
Bracketting Guideliness for Treebank IIstyle Penn Treebank Project.
Linguistic Data Consor-tium.Eugene Charniak.
2000.
A maximum-entropy-inspiredparser.
In The Proceedings of the North AmericanChapter of the Association for Computational Linguis-tics, pages 132?139.Michael Collins.
1997.
Three generative, lexicalisedmodels for statistical parsing.
In The Proceedings ofthe 35th Annual Meeting of the Association for Com-putational Linguistics, San Francisco.
Morgan Kauf-mann.Michael Collins.
2000.
Discriminative reranking for nat-ural language parsing.
In Machine Learning: Pro-ceedings of the Seventeenth International Conference(ICML 2000), pages 175?182, Stanford, California.Ferenc Ge?cseg and Magnus Steinby.
1984.
Tree Au-tomata.
Akade?miai Kiado?, Budapest.Mark Johnson.
1998.
PCFG models of linguis-tic tree representations.
Computational Linguistics,24(4):613?632.Michell P. Marcus, Beatrice Santorini, and Mary AnnMarcinkiewicz.
1993.
Building a large annotated cor-pus of English: The Penn Treebank.
ComputationalLinguistics, 19(2):313?330.Ian H. Witten and Eibe Frank.
2000.
Data mining: prac-tical machine learning tools and techniques with Javaimplementations.
Morgan Kaufmann, San Francisco.
