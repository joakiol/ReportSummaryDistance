Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 96?106,Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational LinguisticsA Generate and Rank Approach to Sentence ParaphrasingProdromos Malakasiotis?
and Ion Androutsopoulos?+?Department of Informatics, Athens University of Economics and Business, Greece+Digital Curation Unit ?
IMIS, Research Centre ?Athena?, GreeceAbstractWe present a method that paraphrases a givensentence by first generating candidate para-phrases and then ranking (or classifying)them.
The candidates are generated by ap-plying existing paraphrasing rules extractedfrom parallel corpora.
The ranking compo-nent considers not only the overall quality ofthe rules that produced each candidate, butalso the extent to which they preserve gram-maticality and meaning in the particular con-text of the input sentence, as well as the de-gree to which the candidate differs from theinput.
We experimented with both a Max-imum Entropy classifier and an SVR ranker.Experimental results show that incorporatingfeatures from an existing paraphrase recog-nizer in the ranking component improves per-formance, and that our overall method com-pares well against a state of the art paraphrasegenerator, when paraphrasing rules apply tothe input sentences.
We also propose a newmethodology to evaluate the ranking compo-nents of generate-and-rank paraphrase gener-ators, which evaluates them across differentcombinations of weights for grammaticality,meaning preservation, and diversity.
The pa-per is accompanied by a paraphrasing datasetwe constructed for evaluations of this kind.1 IntroductionIn recent years, significant effort has been devotedto research on paraphrasing (Androutsopoulos andMalakasiotis, 2010; Madnani and Dorr, 2010).
Themethods that have been proposed can be roughlyclassified into three categories: (i) recognition meth-ods, i.e., methods that detect whether or not two in-put sentences or other texts are paraphrases; (ii) gen-eration methods, where the aim is to produce para-phrases of a given input sentence; and (iii) extractionmethods, which aim to extract paraphrasing rules(e.g., ?X wrote Y ?
??
Y was authored by X?)
orsimilar patterns from corpora.
Most of the methodsthat have been proposed belong in the first category,possibly because of the thrust provided by relatedresearch on textual entailment recognition (Dagan etal., 2009), where the goal is to decide whether or notthe information of a given text is entailed by that ofanother.
Significant progress has also been made inparaphrase extraction, where most recent methodsproduce large numbers of paraphrasing rules frommultilingual parallel corpora (Bannard and Callison-Burch, 2005; Callison-Burch, 2008; Zhao et al,2008; Zhao et al, 2009a; Zhao et al, 2009b; Kokand Brockett, 2010).
In this paper, we are concernedwith paraphrase generation, which has received lessattention than the other two categories.There are currently two main approaches to para-phrase generation.
The first one treats paraphrasegeneration as a machine translation problem, withthe peculiarity that the target language is the same asthe source one.
To bypass the lack of large monolin-gual parallel corpora, which are needed to train sta-tistical machine translation (SMT) systems for para-phrasing, monolingual clusters of news articles re-ferring to the same event (Quirk et al, 2004) orother similar monolingual comparable corpora canbe used, though sentence alignment methods for par-allel corpora may perform poorly on comparablecorpora (Nelken and Shieber, 2006); alternatively,large collections of paraphrasing rules obtained viaparaphrase extraction from multilingual parallel cor-pora can be used as monolingual phrase tables in a96phrase-based SMT systems (Zhao et al, 2008; Zhaoet al, 2009a); in both cases, paraphrases can thenbe generated by invoking an SMT system?s decoder(Koehn, 2009).
A second paraphrase generation ap-proach is to treat existing machine translation en-gines as black boxes, and translate each input sen-tence to a pivot language and then back to the orig-inal language (Duboue and Chu-Carroll, 2006).
Anextension of this approach uses multiple translationengines and pivot languages (Zhao et al, 2010).In this paper, we investigate a different paraphrasegeneration approach, which does not produce para-phrases by invoking machine translation system(s).We use an existing collection of monolingual para-phrasing rules extracted from multilingual parallelcorpora (Zhao et al, 2009b); each rule is accompa-nied by one or more scores, intended to indicate therule?s overall quality without considering particularcontexts where the rule may be applied.
Instead ofusing the rules as a monolingual phrase table and in-voking an SMT system?s decoder, we follow a gen-erate and rank approach, which is increasingly com-mon in several language processing tasks.1 Givenan input sentence, we use the paraphrasing rules togenerate a large number of candidate paraphrases.The candidates are then represented as feature vec-tors, and a ranker (or classifier) selects the best ones;we experimented with a Maximum Entropy classi-fier and a Support Vector Regression (SVR) ranker.The vector of each candidate paraphrase includesfeatures indicating the overall quality of the rulesthat produced the candidate, the extent to which therules preserve grammaticality and meaning in theparticular context of the input sentence, and the de-gree to which the candidate?s surface form differsfrom that of the input; we call the latter factor di-versity.
The intuition is that a good paraphrase isgrammatical, preserves the meaning of the originalsentence, while also being as different as possible.Experimental results show that including in theranking (or classification) component features froman existing paraphrase recognizer leads to improvedresults.
We also propose a new methodology to eval-uate the ranking components of generate-and-rankparaphrase generators, which evaluates them acrossdifferent combinations of weights for grammatical-1See, for example, Collins and Koo (2005).ity, meaning preservation, and diversity.
The paperis accompanied by a new publicly available para-phrasing dataset we constructed for evaluations ofthis kind.
Further experiments indicate that whenparaphrasing rules apply to the input sentences, ourparaphrasing method is competitive to a state of theart paraphrase generator that uses multiple transla-tion engines and pivot languages (Zhao et al, 2010).We note that paraphrase generation is useful inseveral language processing tasks.
In question an-swering, for example, paraphrase generators can beused to paraphrase the user?s queries (Duboue andChu-Carroll, 2006; Riezler and Liu, 2010); andin machine translation, paraphrase generation canhelp improve the translations (Callison-Burch et al,2006; Marton et al, 2009; Mirkin et al, 2009; Mad-nani et al, 2007), or it can be used when evaluat-ing machine translation systems (Lepage and De-noual, 2005; Zhou et al, 2006; Kauchak and Barzi-lay, 2006; Pado?
et al, 2009).The remainder of this paper is structured as fol-lows: Section 2 explains how our method gener-ates candidate paraphrases; Section 3 introduces thedataset we constructed, which is also used in sub-sequent sections; Section 4 discusses how candi-date paraphrases are ranked; Section 5 compares ouroverall method to a state of the art paraphrase gen-erator; and Section 6 concludes.2 Generating candidate paraphrasesWe use the approximately one million English para-phrasing rules of Zhao et al (2009b).
Roughlyspeaking, the rules were extracted from a parallelEnglish-Chinese corpus, based on the assumptionthat two English phrases e1 and e2 that are oftenaligned to the same Chinese phrase c are likely tobe paraphrases and, hence, they can be treated as aparaphrasing rule e1 ?
e2.2 Zhao et al?s method ac-tually operates on slotted English phrases, obtainedfrom parse trees, where slots correspond to part ofspeech (POS) tags.
Hence, rules like the followingthree may be obtained, where NNi indicates a nounslot and NNPi a proper name slot.2This pivot-based paraphrase extraction approach was firstproposed by Bannard and Callison-Burch (2005).
It under-lies several other paraphrase extraction methods (Riezler et al,2007; Callison-Burch, 2008; Kok and Brockett, 2010).97(1) a lot of NN1?
plenty of NN1(2) NNP1 area?
NNP1 region(3) NNP1 wrote NNP2?
NNP2 was written by NNP1In the basic form of their method, called Model1, Zhao et al (2009b) use a log-linear ranker to as-sign scores to candidate English paraphrase pairs?e1, e2?
; the ranker uses the alignment probabilitiesP (c|e1) and P (e2|c) as features, along with featuresthat assess the quality of the corresponding align-ments.
In an extension of their method, Model 2,Zhao et al consider two English phrases e1 and e2 asparaphrases, if they are often aligned to two Chinesephrases c1 and c2, which are themselves paraphrasesaccording to Model 1 (with English used as the pivotlanguage).
Again, a log-linear ranker assigns a scoreto each ?e1, e2?
pair, now with P (c1|e1), P (c2|c1),and P (e2|c1) as features, along with similar featuresfor alignment quality.
In a further extension, Model3, all the candidate phrase pairs ?e1, e2?
are collec-tively treated as a monolingual parallel corpus.
Thephrases of the corpus are aligned, as when aligninga bilingual parallel corpus, and additional features,based on the alignment, are added to the log-linearranker, which again assigns a score to each ?e1, e2?.The resulting paraphrasing rules e1 ?
e2 typi-cally contain short phrases (up to four or five wordsexcluding slots) on each side; hence, they can beused to rewrite only parts of longer sentences.
Givenan input (source) sentence S, we generate candidateparaphrases by applying rules whose left or righthand side matches any part of S. For example, rule(1) matches the source sentence (4); hence, (4) canbe rewritten as the candidate paraphrase (5).3(4) S: He had a lot of [NN 1admiration] for his job.
(5) C: He had plenty of [NN 1admiration] for his job.Several rules may apply to S; for example, they mayrewrite different parts of S, or they may replace thesame parts of S by different phrases.
We allow allpossible combinations of applicable rules to apply toS, excluding combinations that include rules rewrit-ing overlapping parts of S.4 To avoid generating toomany candidates (C), we use only the 20 rules (that3We use Stanford?s POS tagger, MaxEnt classifier, and de-pendency parser; see http://nlp.stanford.edu/.4A possible extension, which we have not explored, wouldbe to recursively apply the same process to the resulting Cs.apply to S) with the highest scores.
Zhao et al actu-ally associate each rule with three scores.
The firstone, hereafter called r1, is the Model 1 score, and theother two, r2 and r3, are the forward and backwardalignment probabilities of Model 3; see Zhao et al(2009b) for details.
We use the average of the threescores, hereafter r4, when generating candidates.Unfortunately, Zhao et al?s scores reflect the over-all quality of each rule, without considering the con-text of the particular S where the rule is applied.Szpektor et al (2008) point out that, for example,a rule like ?X acquire Y ??
?X buy Y ?
may workwell in many contexts, but not in ?Children acquirelanguage quickly?.
Similarly, ?X charged Y with??
?X accused Y of?
should not be applied to sen-tences about charging batteries.
Szpektor et al pro-pose, roughly speaking, to associate each rule witha model of the contexts where the rule is applicable,as well as models of the expressions that typicallyfill its slots, in order to be able to assess the applica-bility of each rule in specific contexts.
The rules thatwe use do not have associated models of this kind,but we follow Szpektor et al?s idea of assessing theapplicability of each rule in each particular context,when ranking candidates, as discussed below.3 A dataset of candidate paraphrasesOur generate and rank method relies on existinglarge collections of paraphrasing rules to generatecandidate paraphrases.
Our main contribution is inthe ranking of the candidates.
To be able to evalu-ate the performance of different rankers in the taskwe are concerned with, we first constructed an eval-uation dataset that contains pairs ?S,C?
of source(input) sentences and candidate paraphrases, and weasked human judges to assess the degree to whichthe C of each pair was a good paraphrase of S.We selected randomly 75 source (S) sentencesfrom the AQUAINT corpus, such that at least oneof the paraphrasing rules applied to each S.5 Foreach S, we generated candidate Cs using Zhao etal.
?s rules, as discussed in Section 2.
This led to1,935 ?S,C?
pairs, approx.
26 pairs for each S. Thepairs were given to 13 judges other than the authors.6Each judge evaluated approx.
148 (different) ?S,C?5The corpus is available from the LDC (LDC2002T31).6The judges were fluent, but not native English speakers.98Figure 1: Distribution of overall quality scores in theevaluation dataset (1 = totally unacceptable, 4 = perfect).pairs; each of the 1,935 pairs was evaluated by onejudge.
The judges were asked to provide grammati-cality, meaning preservation, and overall paraphrasequality scores for each ?S,C?
pair, each score on a1?4 scale (1 for totally unacceptable, 4 for perfect);guidelines and examples were also provided.Figure 1 shows the distribution of the overall qual-ity scores in the 1,935 ?S,C?
pairs of the evalua-tion dataset; the distributions of the grammaticalityand meaning preservation scores are similar.
No-tice that although we used only the 20 applicableparaphrasing rules with the highest scores to gen-erate the ?S,C?
pairs, less than half of the candidateparaphrases (C) were considered good, and approx-imately only 20% perfect.
In other words, apply-ing paraphrasing rules (even only those with the 20best scores) to each input sentence S and randomlypicking one of the resulting candidate paraphrasesC, without any further filtering (or ranking) of thecandidates, would on average produce unacceptableparaphrases more frequently than acceptable ones.Hence, the role of the ranking component is crucial.We also measured inter-annotator agreement byconstructing, in the same way, 100 additional ?S,C?pairs (other than the 1,935) and asking 3 of the 13judges to evaluate all of them.
We measured themean absolute error, i.e., the mean absolute differ-ence in the judges?
scores (averaged over all pairsof judges) and the mean (over all pairs of judges)K statistic (Carletta, 1996).
In the overall scores,K was 0.64, which is in the range often taken toindicate substantial agreement (0.61?0.80).7 Agree-ment was higher for grammaticality (K = 0.81),7It is also close to 0.67, which is sometimes taken to be acutoff for substantial agreement in computational linguistics.mean abs.
diff.
K-statisticgrammaticality 0.20 0.81meaning preserv.
0.26 0.59overall quality 0.22 0.64Table 1: Inter-annotator agreement when manually eval-uating candidate paraphrases.and lower (K = 0.59) for meaning preservation.
Ta-ble 1 shows that the mean absolute difference in theannotators?
scores was 15 to 14 of a point.Several judges commented that they had troubledeciding to what extent the overall quality scoreshould reflect grammaticality or meaning preserva-tion.
They also wondered if it was fair to consider asperfect candidate paraphrases that differed in onlyone or two words from the source sentences, i.e.,candidates with low diversity.
These comments ledus to ignore the judges?
overall quality scores insome experiments, and to use a weighted averageof grammaticality, meaning preservation, and (auto-matically measured) diversity instead, with differentweight combinations corresponding to different ap-plication requirements, as discussed further below.In the same way, 1,500 more ?S,C?
pairs (otherthan the 1,935 and the 100, not involving previouslyseen Ss) were constructed, and they were evaluatedby the first author.
The 1,500 pairs were used asa training dataset in experiments discussed below.Both the 1,500 training and the 1,935 evaluation(test) pairs are publicly available.8 We occasionallyrefer to the training and evaluation datasets as a sin-gle dataset, but they are clearly separated.4 Ranking candidate paraphrasesWe now discuss the ranking component of ourmethod, which assesses the candidate paraphrases.4.1 Features of the ranking componentEach ?S,C?
pair is represented as a feature vector.To allow the ranking component to assess the degreeto which a candidate C is grammatical, or at leastas grammatical as the source S, we include in thefeature vectors the language model scores of S, C,and the difference between the two scores.
We usea 3-gram language model trained on approximately8See the paper?s supplementary material.996.5 million sentences of the AQUAINT corpus.9 Toallow the ranker to consider the (context-insensitive)quality scores of the rules that generated C from S,we also include as features the highest, lowest, andaverage r1, r2, r3, and r4 scores (Section 2) of theserules, 12 features in total.The features discussed so far are similar to thoseemployed by Zhao et al (2009a) in the only compa-rable paraphrase generation method we are aware ofthat uses paraphrasing rules.
That method, hereaftercalled ZHAO-RUL, uses the language model scoreof C and scores similar to r1, r2, r3 in a log-linearmodel.10 The log-linear model of ZHAO-RUL is usedby an SMT-like decoder to identify the transforma-tions (applications of rules) that produce the (hope-fully) best paraphrase.
By contrast, we first gen-erate a large number of candidates using the para-phrasing rules, and we then rank them.
Unfortu-nately, we did not have access to an implementa-tion of ZHAO-RUL to compare against, but belowwe compare against another paraphraser proposedby Zhao et al (2010), hereafter called ZHAO-ENG,which uses multiple machine translation engines andpivot languages, instead of paraphrasing rules, andwhich Zhao et al found to outperform ZHAO-RUL.To further help the ranking component assess thedegree to which C preserves the meaning of S, wealso optionally include in the vectors of the ?S,C?pairs the features of an existing paraphrase recog-nizer (Malakasiotis, 2009) that obtained the bestpublished results (Androutsopoulos and Malakasio-tis, 2010) on the widely used MSR paraphrasing cor-pus.11 Most of the recognizer?s features are com-puted by using nine similarity measures: Leven-shtein, Jaro-Winkler, Manhattan, Euclidean, and n-gram (n = 3) distance, cosine similarity, Dice, Jac-card, and matching coefficients, all computed on to-kens; consult Malakasiotis (2009) for details.
Foreach ?S,C?
pair, the nine similarity measures are ap-9We use SRILM; see http://www-speech.sri.com/.10Application-specific features are also included, which canbe used, for example, to favor paraphrases that are shorter thanthe input in sentence compression (Knight and Marcu, 2002;Clarke and Lapata, 2008).
Similar features could also be addedto application-specific versions of our method.11The MSR corpus contains pairs that are paraphrases or not.It is a benchmark for paraphrase recognizers, not generators.
Itprovides only one paraphrase (true or false) of each source, andfew of the true paraphrases can be obtained by the rules we use.plied to ten different forms ?s1, c1?
, .
.
.
, ?s10, c10?of ?S,C?, described below, leading to 90 features.
?s1, c1?
: The original forms of S and C.?s2, c2?
: S and C with tokens replaced by stems.
?s3, c3?
: S and C, with tokens replaced by POS tags.
?s4, c4?
: S and C, tokens replaced by soundex codes.12?s5, c5?
: S and C, but having removed non-nouns.
?s6, c6?
: As previously, but nouns replaced by stems.
?s7, c7?
: As previously, nouns replaced by soundex.
?s8, c8?
: S and C, but having removed non-verbs.
?s9, c9?
: As previously, but verbs replaced by stems.
?s10, c10?
: As previously, verbs replaced by soundex.When constructing all ten forms ?si, ci?
of ?S,C?,synonyms (in any WordNet synset) are treated asidentical words.
Additional variants of some of the90 features compare a sliding window of some ofthe si forms to the corresponding ci forms (or viceversa), adding 40 more features; see Malakasiotis(2009).
Two more Boolean features indicate the ex-istence or absence of negation in S or C, respec-tively; and another feature computes the ratio of thelengths of S and C, measured in tokens.
Finally,three additional features compare the dependencytrees of S and C:RS =|common dependencies of S,C||dependencies of S|RC =|common dependencies of S,C||dependencies of C|F?=1 =2 ?RS ?RCRS +RCThe recognizer?s features are 136 in total.13Hence, the full feature set of our paraphraser?s rank-ing component comprises 151 features.12The Soundex algorithm maps English words to alphanu-meric codes, so that words with the same pronunciationsreceive the same codes, despite spelling differences; seehttp://en.wikipedia.org/wiki/Soundex.13Malakasiotis (2009) shows that although there is a lot of re-dundancy in the recognizer?s feature set, the full feature set stillleads to better paraphrase recognition results, compared to sub-sets constructed via feature selection with hill-climbing or beamsearch.
The same paper reports that the recognizer performs al-most as well without the last three features, which may not beavailable in languages with no reliable dependency parsers.
No-tice, also, that the recognizer does not use paraphrasing rules.1004.2 Learning rate with a MaxEnt classifierTo obtain a first indication of whether or not a rank-ing component equipped with the features discussedabove could learn to distinguish good from bad can-didate paraphrases, and to investigate if our train-ing dataset is sufficiently large, we initially experi-mented with a Maximum Entropy classifier (with the151 features) as the ranking component.
This initialversion of the ranking component, called ME-REC,was trained on increasingly larger parts of the train-ing dataset of Section 3, and it was always evaluatedon the entire test dataset of that section.
For simplic-ity, we used only the judges?
overall quality scoresin these experiments, and we treated the problem asone of binary classification; overall quality scores of1 and 2 where conflated to a negative category, andscores of 3 and 4 to a positive category.Figure 2 plots the error rate of ME-REC, com-puted both on the test set and the encountered train-ing subset.
The error rate on the training instancesa learner has encountered is typically lower than theerror rate on the test set (unseen instances); hence,the former error rate can be seen as a lower boundof the latter.
ME-REC shows signs of having reachedits lower bound when the entire training dataset isused, suggesting that the training dataset is suffi-ciently large.
The baseline (BASE) of Figure 2 usesonly a threshold on the average r4 (Section 2) of therules that turned S into C. If the average r4 is higherthan the threshold, the ?S,C?
pair is classified in thepositive class, otherwise in the negative one.
Thethreshold was tuned by experimenting on a sepa-rate tuning dataset.
Clearly, ME-REC outperformsthe baseline, which uses only the average (context-insensitive) scores of the applied paraphrasing rules.4.3 Experiments with an SVR rankerAs already noted, when our dataset were constructedthe judges felt it was not always clear to what ex-tent the overall quality scores should reflect meaningpreservation or grammaticality; and they also won-dered if the overall quality scores should have alsotaken into consideration diversity.
To address theseconcerns, in the experiments described in this sec-tion (and the remainder of the paper) we ignored thejudges?
overall scores, and we used a weighted av-erage of the grammaticality, meaning preservation,15%20%25%30%35%40%45%50%75 1502253003754505256006757508259009751050112512001275135014251500ErrorrateTraining instances usedME-REC.TRAINME-REC.TESTBASEFigure 2: Learning curves of a Maximum Entropy classi-fier used as the ranking component of our method.and diversity scores instead; the grammaticality andmeaning preservation scores were those provided bythe judges, while diversity was automatically com-puted as the edit distance (Levenshtein, computedon tokens) between S and C. Stated otherwise, thecorrect score y(xi) of each training or test instancexi (i.e., of each feature vector of an ?S,C?
pair) wastaken to be a linear combination of the grammati-cality score g(xi), the meaning preservation scorem(xi), and the diversity d(xi), as in Equation (6),where ?3 = 1?
?1 ?
?2.y(xi) = ?1 ?
g(xi) + ?2 ?m(xi) + ?3 ?
d(xi) (6)We believe that the ?i weights should in prac-tice be application-dependent.
For example, whenparaphrasing user queries to a search engine thatturns them into bags of words, diversity and meaningpreservation may be more important than grammati-cality; by contrast, when paraphrasing the sentencesof a generated text to avoid repeating the same ex-pressions, grammaticality is very important.
Hence,generic paraphrase generators, like ours, intended tobe useful in many different applications, should beevaluated for many different combinations of the ?iweights.
Consequently, in the experiments of thissection we trained and evaluated the ranking com-ponent of our method (on the training and evalua-tion part, respectively, of the dataset of Section 3)several times, each time with a different combina-tion of ?1, ?2, ?3 values, with the values of each ?iranging from 0 to 1 with a step of 0.2.We employed a Support Vector Regression (SVR)model in the experiments of this section, instead of1010,00%10,00%20,00%30,00%40,00%50,00%60,00%70,00%?1=0.0 ?2=0.0?1=0.0 ?2=0.2?1=0.0 ?2=0.4?1=0.0 ?2=0.6?1=0.0 ?2=0.8?1=0.0 ?2=1.0?1=0.2 ?2=0.0?1=0.2 ?2=0.2?1=0.2 ?2=0.4?1=0.2 ?2=0.6?1=0.2 ?2=0.8?1=0.4 ?2=0.0?1=0.4 ?2=0.2?1=0.4 ?2=0.4?1=0.4 ?2=0.6?1=0.6 ?2=0.0?1=0.6 ?2=0.2?1=0.6 ?2=0.4?1=0.8 ?2=0.0?1=0.8 ?2=0.2?1=1.0 ?2=0.0SVR-RECSVR-BASE?2Figure 3: Performance of our method?s SVR ranking com-ponent with (SVR-REC) and without (SVR-BASE) the ad-ditional features of the paraphrase recognizer.a classifier, given that the y(xi) scores that we wantto predict are real values.14 An SVR is very similarto a Support Vector Machine (Vapnik, 1998; Cris-tianini and Shawe-Taylor, 2000; Joachims, 2002),but it is trained on examples of the form ?xi, y(xi)?,where xi ?
Rn and y(xi) ?
R, and it learns a rank-ing function f : Rn ?
R that is intended to returnf(xi) values as close as possible to the correct onesy(xi), given feature vectors xi.
In our case, the cor-rect y(xi) values were those of Equation (6).
We callSVR-REC the SVR ranker with all the 151 features ofSection 4.2, and SVR-BASE the SVR ranker withoutthe 136 features of the paraphrase recognizer.We used the squared correlation coefficient ?2 toevaluate SVR-REC against SVR-BASE.15 The ?2 co-efficient shows how well the scores returned by theSVR are correlated with the desired scores y(xi); thehigher the ?2 the higher the agreement.
Figure 314Additional experiments confirmed that the SVR per-forms better than ME-REC as the ranking component.
Weuse the SVR implementation of LIBSVM, available fromhttp://www.csie.ntu.edu.tw/?cjlin/libsvm/,with an RBF kernel and default settings.
All the features arenormalized in [?1, 1], when using SVR or ME-REC.15If n is the number of test pairs, f(xi) the score returned bythe SVR for the i-th pair, and y(xi) the correct score, then ?2 is:(n?ni=1 f(xi)yi ?
?ni=1 f(xi)?ni=1 y(xi))2(n?ni=1 f(xi)2 ?
(?ni=1 f(xi))2)(n?ni=1 y2i ?
(?ni=1 y(xi))2)shows the experimental results.
Each line from thediagram?s center represents a different experimentalsetting, i.e., a different combination of ?1 and ?2;recall that ?3 = 1 ?
?1 ?
?2.
The distance of amethod?s curve from the center is the method?s ?2for that setting.
The farther a point is from the centerthe higher ?2 is; hence, methods whose curves arecloser to the diagram?s outmost perimeter are better.Clearly, SVR-REC (which includes the recognizer?sfeatures) outperforms SVR-BASE (which relies onlyon the language model and the scores of the rules).The two peaks of SVR-REC?s curve are when ?3is very high (1 or 0.8), i.e., when y(xi) is dominatedby the diversity score; in these cases, SVR-REC isat a clear advantage, since it includes features forsurface string similarity (e.g., Levenshtein distancemeasured on ?s1, c1?
), which in effect measure di-versity, unlike SVR-BASE.
Even when ?1 is veryhigh (1 or 0.8), i.e., when all or most of the weightis placed on grammaticality, SVR-REC outperformsSVR-BASE, indicating that the extra features in SVR-REC also contribute towards assessing grammatical-ity; by contrast SVR-BASE relies exclusively on thelanguage model for grammaticality.
Unfortunately,when ?2 is very high (1 or 0.8), i.e., when all ormost of the weight is placed on meaning preserva-tion, there is no or very small difference betweenSVR-REC and SVR-BASE, suggesting that the extrafeatures of the paraphrase recognizer are not as use-ful to the SVR, when assessing meaning preserva-tion, as we would have hoped.
Nevertheless, SVR-REC is overall better than SVR-BASE.We believe that the dataset of Section 3 and theevaluation methodology summarized by Figure 3will prove useful to other researchers, who may wishto evaluate other ranking components of generate-and-rank paraphrasing methods against ours, for ex-ample with different ranking algorithms or features.Similar datasets of candidate paraphrases can alsobe created using different collections of paraphras-ing rules.16 The same methodology can then be usedto evaluate ranking components on those datasets.5 Comparison to the state of the artHaving established that SVR-REC is a better config-uration of our method?s ranker than SVR-BASE, we16See Androutsopoulos and Malakasiotis (2010) for pointers.102proceed to investigate how well our overall generate-and-rank method (with SVR-REC) compares againsta state of the art paraphrase generator.As already mentioned, Zhao et al (2010) recentlypresented a method (we call it ZHAO-ENG) that out-performs their previous method (Zhao et al, 2009a),which used paraphrasing rules and an SMT-like de-coder (we call that previous method ZHAO-RUL).Given an input sentence S, ZHAO-ENG producescandidate paraphrases by translating S to 6 pivotlanguages via 3 different commercial machine trans-lation engines (treated as black boxes) and then backto the original language, again via 3 machine transla-tion engines (54 combinations).
Roughly speaking,ZHAO-ENG then ranks the candidate paraphrases bytheir average distance from all the other candidates,selecting the candidate(s) with the smallest distance;distance is measured as BLEU score (Papineni etal., 2002).17 Hence, ZHAO-ENG is also, in effect,a generate-and-rank paraphraser, but the candidatesare generated by invoking multiple machine transla-tion engines instead of applying paraphrasing rules,and they are ranked by the average distance measurerather than using an SVR.An obvious practical advantage of ZHAO-ENG isthat it exploits the vast resources of existing com-mercial machine translation engines when generat-ing candidate paraphrases, which allows it to alwaysobtain large numbers of candidate paraphrases.
Bycontrast, the collection of paraphrasing rules that wecurrently use does not manage to produce any can-didate paraphrases in 40% of the sentences of theNew York Times part of AQUAINT, because no ruleapplies.
Hence, in terms of ability to always para-phrase the input, ZHAO-ENG is clearly better, thoughit should be possible to improve our methods?s per-formance in that respect by using larger collectionsof paraphrasing rules.18 A further interesting ques-tion, however, is how good the paraphrases of thetwo methods are, when both methods manage toparaphrase the input, i.e., when at least one para-17We use the version of ZHAO-ENG that Zhao et al (2010)call ?selection-based?, since they reported it performs overallbetter than an alternative decoding-based version.18Recall that the paraphrasing rules we use were extractedfrom an English-Chinese parallel corpus.
Additional rulescould be extracted from other parallel corpora, like Europarl(http://www.statmt.org/europarl/).phrasing rule applies to S. This scenario can be seenas an emulation of the case where the collection ofparaphrasing rules is sufficiently large to guaranteethat at least one rule applies to any source sentence.To answer the latter question, we re-implementedZHAO-ENG, with the same machine translation en-gines and languages used by Zhao et al (2010).We also trained our paraphraser (with SVR-REC) onthe training part of the dataset of Section 3.
Wethen selected 300 random source sentences S fromAQUAINT that matched at least one of the paraphras-ing rules, excluding sentences that had been used be-fore.
Then, for each one of the 300 S sentences, wekept the single best candidate paraphraseC1 andC2,respectively, returned by our paraphraser and ZHAO-ENG.
The resulting ?S,C1?
and ?S,C2?
pairs weregiven to 10 human judges.
This time the judgesassigned only grammaticality and meaning preser-vation scores (on a 1?4 scale); diversity was againcomputed as edit distance.
Each pair was evaluatedby one judge, who was given an equal number ofpairs from the two methods, without knowing whichmethod each pair came from.
The same judge neverrated two pairs with the same S. Since we had noway to make ZHAO-ENG sensitive to ?1, ?2, ?3, wetrained SVR-REC with ?1 = ?2 = 1/3, as the mostneutral combination of weights.Table 2 lists the average grammaticality, meaningpreservation, and diversity scores of the two meth-ods.
All scores were normalized in [0, 1], but thereader should keep in mind that diversity was com-puted as edit distance, whereas the other two scoreswere provided by human judges on a 1?4 scale.
Thegrammaticality score of our method was better thanZHAO-ENG?s, and the difference was statisticallysignificant.19 In meaning preservation, ZHAO-ENGwas slightly better, but the difference was not statis-tically significant.
The difference in diversity waslarger and statistically significant, with the diversityscores indicating that it takes approximately twice asmany edit operations (insert, delete, replace) to turneach source sentence to ZHAO-ENG?s paraphrase,compared to the paraphrase of our method.We note that our method can be tuned, by ad-justing the ?i weights, to produce paraphrases with19We used Analysis of Variance (ANOVA) (Fisher, 1925), fol-lowed by post-hoc Tukey tests to check whether the scores ofthe two methods differ significantly (p < 0.05).103score (%) our method ZHAO-ENGgrammaticality 90.89 85.33meaning preserv.
76.67 78.56diversity 6.50 14.58Table 2: Evaluation of our paraphrasing method (withSVR-REC) against ZHAO-ENG, using human judges.
Re-sults in bold indicate statistically significant differences.higher grammaticality, meaning preservation, or di-versity scores; for example, we could increase ?3and decrease ?1 to obtain higher diversity at the costof lower grammaticality in the results of Table 2.20 Itis unclear how ZHAO-ENG could be tuned that way.Overall, our method seems to perform wellagainst ZHAO-ENG, despite the vastly larger re-sources of ZHAO-ENG, provided of course that welimit ourselves to source sentences to which para-phrasing rules apply.
It would be interesting to in-vestigate in future work if our method?s coverage(sentences it can paraphrase) can increase to ZHAO-ENG?s level by using larger collections of paraphras-ing rules.
It would also be interesting to combine thetwo methods, perhaps by using SVR-REC (withoutfeatures for the quality scores of the rules) to rankcandidate paraphrases generated by ZHAO-ENG.6 Conclusions and future workWe presented a generate-and-rank method to para-phrase sentences.
The method first produces can-didate paraphrases by applying existing paraphras-ing rules extracted from parallel corpora, and it thenranks (or classifies) the candidates to keep the bestones.
The ranking component considers not only thecontext-insensitive quality scores of the paraphras-ing rules that produced each candidate, but also fea-tures intended to measure the extent to which therule applications preserve grammaticality and mean-ing in the particular context of the input sentence, aswell as the degree to which the resulting candidatediffers from the input sentence (diversity).Initial experiments with a Maximum Entropyclassifier confirmed that the features we use can helpa ranking component select better candidate para-phrases than a baseline ranker that considers only20Additional application-specific experiments confirm thatthis tuning is possible (Malakasiotis, 2011).the average context-insensitive quality scores of theapplied rules.
Further experiments with an SVRranker indicated that our full feature set, which in-cludes features from an existing paraphrase recog-nizer, leads to improved performance, compared toa smaller feature set that includes only the context-insensitive scores of the rules and language model-ing scores.
We also propose a new methodology toevaluate the ranking components of generate-and-rank paraphrase generators, which evaluates themacross different combinations of weights for gram-maticality, meaning preservation, and diversity.
Thepaper is accompanied by a paraphrasing dataset weconstructed for evaluations of this kind.Finally, we evaluated our overall method againsta state of the art sentence paraphraser, whichgenerates candidates by using several commercialmachine translation systems and pivot languages.Overall, our method performed well, despite the vastresources of the machine translation systems em-ployed by the system we compared against.
Ourmethod performed better in terms of grammaticality,equally well in meaning preservation, and worse indiversity, but it could be tuned to obtain higher diver-sity at the cost of lower grammaticality, whereas itis unclear how the system we compare against couldbe tuned this way.
On the other hand, an advantageof the paraphraser we compared against is that it al-ways produces paraphrases; by contast, our systemdoes not produce paraphrases when no paraphrasingrule applies to the source sentence.
Larger collec-tions of paraphrasing rules would be needed to im-prove our method in that respect.Apart from obtaining and experimenting withlarger collections of paraphrasing rules, it would beinteresting to evaluate our method in vivo, for ex-ample by embedding it in question answering sys-tems (to paraphrase the questions), in informationextraction systems (to paraphrase extraction tem-plates), or in natural language generators (to para-phrase template-like sentence plans).
We also planto investigate the possibility of embedding our SVRranker in the sentence paraphraser we comparedagainst, i.e., to rank candidates produced by usingseveral machine translation systems and pivot lan-guages, as in ZHAO-ENG.104AcknowledgmentsThis work was partly carried out during INDIGO, anFP6 IST project funded by the European Union, withadditional funding from the Greek General Secre-tariat of Research and Technology.21ReferencesI.
Androutsopoulos and P. Malakasiotis.
2010.
A surveyof paraphrasing and textual entailment methods.
Jour-nal of Artificial Intelligence Research, 38:135?187.C.
Bannard and C. Callison-Burch.
2005.
Paraphrasingwith bilingual parallel corpora.
In Proc.
of the 43rdACL, pages 597?604, Ann Arbor, MI.C.
Callison-Burch, P. Koehn, and M. Osborne.
2006.Improved statistical machine translation using para-phrases.
In Proc.
of HLT-NAACL, pages 17?24, NewYork, NY.C.
Callison-Burch.
2008.
Syntactic constraints on para-phrases extracted from parallel corpora.
In Proc.
ofEMNLP, pages 196?205, Honolulu, HI, October.J.
Carletta.
1996.
Assessing agreement on classificationtasks: The kappa statistic.
Computational Linguistics,22:249?254.J.
Clarke and M. Lapata.
2008.
Global inference forsentence compression: An integer linear programmingapproach.
Journal of Artificial Intelligence Research,1(31):399?429.M.
Collins and T. Koo.
2005.
Discriminative rerankingfor natural language parsing.
Computational Linguis-tics, 31(1):25?69.N.
Cristianini and J. Shawe-Taylor.
2000.
An In-troduction to Support Vector Machines and OtherKernel-based Learning Methods.
Cambridge Univer-sity Press.I.
Dagan, B. Dolan, B. Magnini, and D. Roth.
2009.
Rec-ognizing textual entailment: Rational, evaluation andapproaches.
Natural Lang.
Engineering, 15(4):i?xvii.Editorial of the special issue on Textual Entailment.P.
A. Duboue and J. Chu-Carroll.
2006.
Answering thequestion you wish they had asked: The impact of para-phrasing for question answering.
In Proc.
of HLT-NAACL, pages 33?36, New York, NY.Ronald A. Fisher.
1925.
Statistical Methods for Re-search Workers.
Oliver and Boyd.T.
Joachims.
2002.
Learning to Classify Text Using Sup-port Vector Machines: Methods, Theory, Algorithms.Kluwer.D.
Kauchak and R. Barzilay.
2006.
Paraphrasing forautomatic evaluation.
In Proc.
of HLT-NAACL, pages455?462, New York, NY.21Consult http://www.ics.forth.gr/indigo/.K.
Knight and D. Marcu.
2002.
Summarization be-yond sentence extraction: A probalistic approach tosentence compression.
Artif.
Intelligence, 139(1):91?107.P.
Koehn.
2009.
Statistical Machine Translation.
Cam-bridge University Press.S.
Kok and C. Brockett.
2010.
Hitting the right para-phrases in good time.
In Proc.
of HLT-NAACL, pages145?153, Los Angeles, CA.Y.
Lepage and E. Denoual.
2005.
Automatic genera-tion of paraphrases to be used as translation referencesin objective evaluation measures of machine transla-tion.
In Proc.
of the 3rd Int.
Workshop on Paraphras-ing, pages 57?64, Jesu Island, Korea.N.
Madnani and B.J.
Dorr.
2010.
Generating phrasal andsentential paraphrases: A survey of data-driven meth-ods.
Computational Linguistics, 36(3):341?387.N.
Madnani, F. Ayan, P. Resnik, and B. J. Dorr.
2007.Using paraphrases for parameter tuning in statisticalmachine translation.
In Proc.
of 2nd Workshop on Sta-tistical Machine Translation, pages 120?127, Prague,Czech Republic.P.
Malakasiotis.
2009.
Paraphrase recognition us-ing machine learning to combine similarity measures.In Proc.
of the Student Research Workshop of ACL-AFNLP, Singapore.P.
Malakasiotis.
2011.
Paraphrase and Textual Entail-ment Recognition and Generation.
Ph.D. thesis, De-partment of Informatics, Athens University of Eco-nomics and Business, Greece.Y.
Marton, C. Callison-Burch, and P. Resnik.
2009.Improved statistical machine translation usingmonolingually-derived paraphrases.
In Proc.
ofEMNLP, pages 381?390, Singapore.S.
Mirkin, L. Specia, N. Cancedda, I. Dagan, M. Dymet-man, and I. Szpektor.
2009.
Source-language en-tailment modeling for translating unknown terms.
InProc.
of ACL-AFNLP, pages 791?799, Singapore.R.
Nelken and S. M. Shieber.
2006.
Towards robustcontext-sensitive sentence alignment for monolingualcorpora.
In Proc.
of the 11th EACL, pages 161?168,Trento, Italy.S.
Pado?, M. Galley, D. Jurafsky, and C. D. Manning.2009.
Robust machine translation evaluation with en-tailment features.
In Proc.
of ACL-AFNLP, pages 297?305, Singapore.K.
Papineni, S. Roukos, T. Ward, and W. J. Zhu.
2002.BLEU: a method for automatic evaluation of machinetranslation.
In Proc.
of the 40th ACL, pages 311?318,Philadelphia, PA.C.
Quirk, C. Brockett, and W. B. Dolan.
2004.
Mono-lingual machine translation for paraphrase generation.In Proc.
of the Conf.
on EMNLP, pages 142?149,Barcelona, Spain.105S.
Riezler and Y. Liu.
2010.
Query rewriting usingmonolingual statistical machine translation.
Compu-tational Linguistics, 36(3):569?582.S.
Riezler, A. Vasserman, I. Tsochantaridis, V. Mittal, andY.
Liu.
2007.
Statistical machine translation for queryexpansion in answer retrieval.
In Proc.
of the 45thACL, pages 464?471, Prague, Czech Republic.I.
Szpektor, I. Dagan, R. Bar-Haim, and J. Goldberger.2008.
Contextual preferences.
In Proc.
of ACL-HLT,pages 683?691, Columbus, OH.V.
Vapnik.
1998.
Statistical learning theory.
John Wiley.S.
Zhao, H. Wang, T. Liu, and S. Li.
2008.
Pivot ap-proach for extracting paraphrase patterns from bilin-gual corpora.
In Proc.
of ACL-HLT, pages 780?788,Columbus, OH.S.
Zhao, X. Lan, T. Liu, and S. Li.
2009a.
Application-driven statistical paraphrase generation.
In Proc.
ofACL-AFNLP, pages 834?842, Singapore.S.
Zhao, H. Wang, T. Liu, and Li.
S. 2009b.
Extract-ing paraphrase patterns from bilingual parallel cor-pora.
Natural Language Engineering, 15(4):503?526.S.
Zhao, H. Wang, X. Lan, and T. Liu.
2010.
Leverag-ing multiple MT engines for paraphrase generation.
InProceedings of the 23rd COLING, pages 1326?1334,Beijing, China.L.
Zhou, C.-Y.
Lin, and Eduard Hovy.
2006.
Re-evaluating machine translation results with paraphrasesupport.
In Proc.
of the Conf.
on EMNLP, pages 77?84.106
