Summarizing Short StoriesAnna Kazantseva?University of OttawaStan Szpakowicz?
?University of OttawaPolish Academy of SciencesWe present an approach to the automatic creation of extractive summaries of literary shortstories.
The summaries are produced with a specific objective in mind: to help a reader decidewhether she would be interested in reading the complete story.
To this end, the summaries givethe user relevant information about the setting of the story without revealing its plot.
The systemrelies on assorted surface indicators about clauses in the short story, the most important of whichare those related to the aspectual type of a clause and to the main entities in a story.
Fifteen judgesevaluated the summaries on a number of extrinsic and intrinsic measures.
The outcome of thisevaluation suggests that the summaries are helpful in achieving the original objective.1.
IntroductionIn the last decade, automatic text summarization has become a popular research topicwith a curiously restricted scope of applications.
A few innovative research directionshave emerged, including headline generation (Soricut and Marcu 2007), summarizationof books (Mihalcea and Ceylan 2007), personalized summarization (D?
?az and Gerva?s2007), generation of tables-of-contents (Branavan, Deshpande, and Barzilay 2007), sum-marization of speech (Fuentes et al 2005), dialogues (Zechner 2002), evaluative text(Carenini, Ng, and Pauls 2006), and biomedical documents (Reeve, Han, and Brooks2007).
In addition, more researchers have been venturing past purely extractive sum-marization (Krahmer, Marsi, and van Pelt 2008; Nomoto 2007; McDonald 2006).
Byand large, however, most research in text summarization still revolves around textscharacterized by rigid structure.
The better explored among such texts are news articles(Barzilay and McKeown 2005), medical documents (Elhadad et al 2005), legal docu-ments (Moens 2007), and papers in the area of computer science (Teufel and Moens2002; Mei and Zhai 2008).
Although summarizing these genres is a formidable challengein itself, it excludes a continually increasing number of informal documents availableelectronically.
Such documents, ranging from novels to personal Web pages, offer awealth of information that merits the attention of the text summarization community.?
School of Information Technology and Engineering, University of Ottawa, 800 King Edward Ave.,Ottawa, Ontario K1N 6N5, Canada.
E-mail: ankazant@site.uottawa.ca.??
School of Information Technology and Engineering, University of Ottawa, 800 King Edward Ave.,Ottawa, Ontario K1N 6N5, Canada.
E-mail: szpak@site.uottawa.ca.Submission received: 3 April 2007; revised submission received: 20 January 2009; accepted for publication:29 July 2009.?
2010 Association for Computational LinguisticsComputational Linguistics Volume 36, Number 1We attempt to make a step in this direction by devising an approach to summarizing arelatively unexplored genre: literary short stories.Well-structured documents, such as news articles, exhibit a number of character-istics that help identify some of the important passages without performing in-depthsemantic analysis.
These characteristics include predictable location of typical items ina document and in its well-delineated parts, cue words, and template-like structurethat often characterizes a genre (e.g., scientific papers).
This is not the case in literature.Quite the contrary?to write fiction in accordance with a template is a sure way to writepoor prose.
One also cannot expect to find portions of text that summarize the mainidea behind a story, and even less so to find them in the same location.
In addition, thevariety of literary devices (the widespread use of metaphor and figurative language,leaving things unsaid and relying on the reader?s skill of reading between the lines,frequent use of dialogue, etc.)
makes summarizing fiction a very distinct task.
It isa contribution of this work to demonstrate that summarizing short fiction is feasibleusing state-of-the-art tools in natural language technology.
In the case of our corpus,this is also done without deep semantic resources or knowledge bases, although suchresources would be of great help.
We leverage syntactic information and shallow se-mantics (provided by a gazetteer) to produce indicative summaries of short stories thatpeople find helpful and that outperform naive baselines and two state-of-the-art genericsummarizers.We have restricted the scope of this potentially vast project in several ways.
Inthe course of this work we concentrate on producing summaries of short stories suit-able for a particular purpose: to help a reader form adequate expectations about thecomplete story and decide whether she would be interested in reading it.
To this end,the summary includes important elements of the setting of a story, such as the placeand the main characters, presented as excerpts from the complete story.
The assump-tion behind this definition is this: If a reader knows when and where the story takesplace and who its main characters are, she should be able to make informed decisionsabout it.With such a definition of short story summaries, re-telling the plot in the summaryis not among the objectives of this work; in fact, doing so is undesirable.
We haveintroduced this limitation for two reasons.
There is an ?ideological?
side of the decision:Not many people want to know what happens in a story before reading it, even if thismay help them decide that the story is worth reading.
There also is a practical side,namely the complexity of the problem: Summarizing the plot would be considerablymore difficult (see Section 2 for a review of related work).
We hope to tackle thisissue in the future.
For now, creating indicative summaries of short stories is challengeenough.The summaries in Figures 1?3 illustrate our approach in the context of a naivelead baseline and a ceiling.
Figure 1 shows an example of an automatically producedsummary that meets the aforementioned criteria.
A reader can see that the story is set ina restaurant where the customers are tended to by two waitresses: the fair Aileen who?wins hearts?
and ?the-bag-o?-meal?
plain-faced Tildy.
If the reader chooses to pursuethe story, she will find the description of an accident of paramount importance to Tildy:One day she is kissed by a customer in public!
The event is more than flattering tousually under-appreciated Tildy.
It causes a complete change in how she views herself.The story then unfolds to reveal that the customer was drunk on the day in questionand that he returned to apologize several days later.
This apology is a severe blow toTildy and an abrupt end of many a dream that the incident had spurred in her head.
Thestory ends with Aileen trying to comfort her crying friend by saying ?He ain?t anything72Kazantseva and Szpakowicz Summarizing Short StoriesFigure 1Example of a summary produced by the system.of a gentleman or he wouldn?t ever of apologized.?
Yet, the summary in Figure 1 doesnot reveal these facts.
For comparison, Figure 2 shows a summary obtained by takingthe same number of sentences from the beginning of the story.
As the reader can see,such a trivial approach is not sufficient to create a useful summary.
Figure 3 shows amanually created ?ideal?
summary.We experimented with a corpus of 47 stories from the 19th and early 20th centurywritten by renowned writers, including O. Henry, Jerome K. Jerome, Anton Chekhov,and Guy de Maupassant.
The stories, with the exception of a few fairy tales, are clas-sical examples of short social fiction.
The corpus was collected from Project Gutenberg(www.gutenberg.org) and only contains stories in English.
The average length of a storyis 3,333 tokens and the target compression rate expressed in the number of sentencesis 94%.In order to create summaries of short stories that satisfy our stated criteria (hence-forth indicative summaries), the system searches each story for sentences that focuson important entities and relate the background of the story (as opposed to events).Correspondingly, processing has two stages.
Initially, the summarizer identifies twotypes of important entities: main characters and locations.
This is achieved using agazetteer, resolving anaphoric expressions and then identifying frequently mentionedFigure 2Example of a lead baseline summary.73Computational Linguistics Volume 36, Number 1Figure 3Example of a manual summary.entities.
Next, the system selects sentences that set out the background of the storyand focus on one of the important entities.
In order to separate the background of astory from the plot (i.e., events), we rely on the notion of aspect.1 We approximate theaspectual type of a clause using either machine learning or manually produced rules.This is achieved by relying on an array of verb-related features, such as tense, lexicalaspect of the main verb, presence of temporal expressions, and so on.
Finally, the systemcomposes a summary out of the selected sentences.Our task remains a significant challenge despite its limited scope.
To produce suchindicative summaries successfully, one needs to consider many facets of the problem.An informative data representation, computational complexity, and usability of the finalproduct are only some of them.
Because the project is at the stage of an advancedfeasibility study, it has not been possible to do justice to all aspects of the problem.Therefore, we concentrated on several specific issues and left many more to future workand to fellow researchers.Firstly, we sought to identify characteristics of short stories that could be helpful increating summaries.
We devised an informative and practical data representation thatcould be reproduced without too much cost or effort.
Secondly, we restricted ourselvesto identifying the most informative portions of the stories and paid much less attentionto readability and coherence of the resulting summaries.
Even though readability is animportant property, we hypothesized that informativeness is yet more important.
Oncethe task of identifying informative passages has been accomplished, one can work onachieving coherence and readability.
In the end, the emphasis was on the creation ofextractive summaries using established tools and methods and on the identification ofgenre-specific properties that can help summarization.The novelty of the task and the absence of agreed-upon measures for evaluatingsummaries of literary prose call for a thorough evaluation using a variety of metrics.That is why we conduct three distinct evaluation experiments.
The summaries are1 The term aspect is defined and explained in detail in Section 4.
For now it suffices to say that by aspectwe mean a characteristic of a clause that gives readers an idea about the temporal flow of an event or astate described in it.
For example, the aspectual type of the sentence He likes to run is a state.
The aspectualtype of He has run a marathon is an event.74Kazantseva and Szpakowicz Summarizing Short Storiesevaluated both extrinsically and intrinsically.
They are also compared with two naivebaselines (lead and random) and two state-of-the-art summarization systems designedfor summarizing newswire (henceforth baseline summarizers).2In the first experiment, 15 people read a mix of machine-made, random, and manualsummaries, and answer questions about them.
Some questions are factual in nature(e.g., list the main characters of the story), and others are subjective (e.g., rate the readabilityof the summary).
The results show that the machine-made summaries are significantlybetter than the random baseline but they fall far short of the quality of the manualsummaries.During the second evaluative experiment, the machine-made summaries are com-pared against extracts created by people using sentence co-selection measures (pre-cision, recall, and F-score).
By sentence co-selection we mean measuring how manysentences found in manually created extracts are selected for inclusion in automaticallyproduced summaries.
The results suggest that our system outperforms all baselines,including state-of-the-art summarizers.The third part of the evaluation uses two ROUGE metrics (Lin 2004) to comparethe machine-made and the baseline summaries with the model abstracts.
The resultssuggest that these measures are not well suited for evaluating extractive indicativesummaries of short stories.This paper is organized in the following manner.
Section 2 gives a brief overview ofthe body of research in automatic story comprehension.
Section 3 describes the processof identifying important entities in short stories.
Section 4 introduces the notion ofaspect, gives an overview of the system?s design, and discusses the linguistic motivationbehind it.
Section 5 describes the classification procedures (the use of machine learningand manual rule creation) that distinguish between the descriptive elements of a storyand the passages that describe events.
Section 6 reports on the evaluation of summarieswhich our system produces.
Section 7 draws conclusions and outlines directions forfuture work.2.
Related WorkSummarization of literary prose is a relatively unexplored topic.
There exists, however,a substantial body of research tackling the problem of story comprehension.
Duringthe 1970s and 1980s, a number of researchers in artificial intelligence built story-understanding systems that relied in one way or another on contemporary researchin psychology and discourse processing.Much of that line of research relied on an assumption that stories exhibit globalcognitive structure (known as macrostructure [van Dijk 1980] or schema [Bartlett 1932])and that they can be decomposed into a finite number of cognitive units.
Accordingto this view, diversity in stories is not due to an infinite number of plots, but to aninfinite number of combinations of a (relatively) small number of cognitive units.
Thisdirection was pioneered in 1928 by Vladimir Propp (1968) with his detailed analysis of100 Russian folk tales.
After a period of oblivion, these ideas regained popularity: vanDijk and Kintsch (1978) demonstrated the existence of macrostructures and their rolein story comprehension and recall; Rumelhart (1975), Thorndyke (1975), and Mandler2 These systems are GISTexter (Harabagiu, Hickl, and Lacatusu 2007) and CLASSY (Schlesinger, O?Leary,and Conroy 2008; Conroy, Schlesinger, and O?Leary 2007).
See Section 6 for details.75Computational Linguistics Volume 36, Number 1(1987) developed sets of cognitive schemas (story grammars) that could be applied tocertain types of stories; and Lehnert (1982) proposed to represent action-based storiesin terms of a finite number of plot units?configurations of affect (or emotional) statesof its characters.The developments in psychology and discourse processing had gone hand in handwith those in artificial intelligence and resulted in a score of story-understandingand question-answering systems.
Many of these relied on a set of manually encodedschemas and chose the most appropriate one for a given story (Cullingford 1978; Dyer1983; Leake 1989).
For example, a system called BORIS (Dyer 1983) processed storiesword-by-word to create a very rich semantic representation of them using MemoryOrganization Packets (MOPs) and Thematic Affect Units (TAUs).
These knowledgestructures were activated by means of a very detailed lexicon where each lexeme wasassociated with MOPs and TAUs it could invoke.Systems such as BORIS could not process stories that did not conform to schemasalready at their disposal.
Charniak and Goldman (1988) and Norvig (1989) at-tempted to circumvent this problem by learning to recognize more general structures.FAUSTUS (Norvig 1989) recognized six general classes of inferences by finding patternsof connectivity in a semantic network.
It could be adapted to new kinds of documents byextending its knowledge base and not the underlying algorithm or patterns.
Research inautomatic story comprehension offered a number of important solutions for subsequentdevelopments in artificial intelligence.
No less important, it pointed out a number ofchallenges.
All these systems required a formidable amount of semantic knowledge anda robust and efficient way of building a semantic representation of texts.
In addition,systems such as BORIS or SAM (Cullingford 1978) also needed a set of schemas orschema-like scenarios.
With such labor intensity, these requirements prohibit usingschema-based approaches for real-life stories (e.g., fiction) and only allow the processingof artificially created examples.In this historical context, our current approach to summarization of short fictionappears rather modest: Our system does not ?understand?
stories, nor does it retelltheir plot.
Instead, it offers the reader hints?important information about the story?ssetting?which should help her guess what type of story is to come.
This assumptionappears reasonable because it has been shown that comprehension and recall of dis-course are strongly influenced by the reader?s familiarity with the type of schema (vanDijk and Kintsch 1978).
Because our system is tailored to work with classics of the genre,it was our expectation that the gist of the story?s setting offered to the reader in thewording of the original would give her an idea about the story?s themes and likely plotdevelopments.
The results of our experiments appear to back this assumption.In addition, given the original objective, it seems reasonable that elements otherthan the plot would have a strong influence on the reader?s decision to read or not toread the story.
The setting of the story, its characters, and style are some of the importantfactors.
Many outstanding literary works differ not so much in plot as in their setting ormoral.3 Our system attempts to capture important elements of the setting explicitly andwe expect that some elements of style may be captured implicitly due to the extractivenature of the summaries.3 Consider, for example, Goethe?s Faust and Bulgakov?s Master and Margarita.
Although they both revolvearound the protagonist entering into a pact with the devil?albeit for different reasons?the latter takesplace in Moscow around 1930s and the two works are dramatically different.76Kazantseva and Szpakowicz Summarizing Short Stories3.
Identifying Important EntitiesDuring the first stage of summary production the system identifies important entitiesin stories.
Initially, we planned to identify three types of entities: people, locations, andtime stamps.
During a preliminary exploration of the corpus, we analyzed 14 stories forthe presence of surface indicators of characters, locations, and temporal anchors.4 Weemployed the GATE Gazetteer (Cunningham et al 2002), and only considered entitiesit recognized automatically.The experiment revealed that the stories in the corpus contained multiple mentionsof characters (on average, 64 mentions per story, excluding pronouns).
On the otherhand, the 14 stories contained only 22 location markers, mostly street names.
Fourstories had no identifiable location markers.
Finally, merely four temporal anchorswere identified in all 14 stories: two absolute (such as year) and two relative (e.g.,Christmas).
These findings support the intuitive idea that short stories revolve aroundtheir characters, even if the ultimate goal is to show a larger social phenomenon.
Theyalso suggest that looking for time stamps in short stories is unlikely to prove productive,because such information is not included in these texts explicitly.
That is why our systemdoes not attempt to identify them.Because characters appear to be central to short stories, we designed our systemto maximize the amount of information available about them.
It contains an anaphoraresolution module that resolves pronominal and noun phrase anaphoric references toanimate entities.
The term anaphora, as used in this work, can be explained as a wayof mentioning a previously encountered entity without naming it explicitly.
ConsiderExamples 1a, 1b, and 1c from ?A Matter of Mean Elevation?
by O. Henry.
The nounphrase Mlle.
Giraud from Example 1a is an antecedent and the pronouns her and she fromExample 1c are anaphoric expressions or referents.
Example 1c illustrates pronominalanaphora, and Example 1b illustrates noun phrase anaphora.
Here the noun phrasethe woman is the anaphoric expression which refers to the antecedent Mlle.
Giraud fromExample 1a.
(1a) John Armstrongent1 and Mlle.
Giraudent2 rode among the Andean peaks, envelopedin their greatness and sublimity.
(1b) To Armstrongent1 the womanent2 seemed almost a holy thing.
(1c) Never yet since herent2 rescue had sheent2 smiled.The anaphora resolution module only handles first and third person singular per-sonal pronouns (I, me, my, he, his ...) and singular definite noun phrases that denoteanimate entities (e.g., the man, but not men).
It is implemented in Java, within the GATEframework, using the Connexor Machinese Syntax parser (Tapanainen and Ja?rvinen1997).The system resolves anaphoric expressions in the following manner.
Initially, thedocuments are parsed with the Connexor Machinese Syntax parser.
The parsed data arethen forwarded to the Gazetteer in GATE, which recognizes nouns denoting locationsand persons.
The original version of the Gazetteer only recognizes named entities andprofessions, but we extended it to include 137 common animate nouns such as man,woman, soldier, or baby.
During the next stage, pronominal anaphoric expressions are4 The stories used in this exploration were later included in the training part of the data.
They were neverused for testing.77Computational Linguistics Volume 36, Number 1Table 1Results of anaphora resolution.Type of anaphora All Correct Incorrect Error rate, %Pronominal 597 507 90 15.07Nominal 152 96 56 36.84Both 749 603 146 19.49resolved using an implementation of the algorithm proposed by Lappin and Leass(1994).5 Subsequently, anaphoric noun phrases are identified using the rules outlined byVieira and Poesio (2000).
Finally, anaphoric noun phrases are resolved using a modifiedversion of the Lappin and Leass algorithm, adjusted to finding antecedents of nouns.The implementation is described in detail in Kazantseva (2006).A thorough evaluation of the anaphora resolution module would be prohibitivelylabor-intensive.
We estimated the performance of the module by manually verifyingthe results it achieved on two short stories of the training set (Table 1).
The error ratesfor pronominal anaphora resolution are significantly lower than those for noun phraseanaphora resolution (15.07% vs. 36.84%).
This is not unexpected because resolving nounphrase anaphora is known to be a very challenging task (Vieira and Poesio 2000).
Theresults also reveal that referring to characters by pronouns is much more frequentthan by noun phrases?in our case, the ratio of pronominal to nominal expressions isalmost 4:1.
This suggests that resolving pronominal anaphoric expressions is crucial tosummarizing short stories.The GATE Gazetteer, part of this module, also annotates the stories for the pres-ence of expressions denoting locations.
After resolving anaphoric expressions, char-acters central to each story are selected based on normalized frequency counts takinganaphoric expressions into account.
The output of this module consists of short storiesannotated for the presence of location markers and main character mentions.4.
Selecting Descriptive Sentences Using Aspectual Information4.1 Linguistic Definition of AspectWe rely on aspect to select salient sentences that set out the background of a story.
Inthis paper, the term aspect denotes the same concept as what Huddleston and Pullum(2002, page 118) call the situation type.
The term refers to ?different ways of viewingthe internal temporal consistency of a situation?
(Comrie 1976, page 3).
Informally, theaspect of a clause suggests the temporal flow of an event or a state and the speaker?sposition with respect to it.A general aspectual classification based on Huddleston and Pullum (2002) appearsin Figure 4, with examples for each type.5 Lappin and Leass (1994) present a rule-based algorithm for resolving pronominal anaphora.
Thealgorithm suggests the most likely antecedent after taking into account the candidates?
syntactic function,recency, and absence or presence of parallelism and cataphora with the referent.
It also enforcesagreement between referent?antecedent pairs.78Kazantseva and Szpakowicz Summarizing Short StoriesFigure 4Aspectual hierarchy after Huddleston and Pullum (2002).The first distinction is between states and events.
Events are processes that go on intime and consist of successive phases (Vendler 1967, page 99).
For instance, an event ofwriting an essay consists of writing separate words, correcting, pausing between words,and so on.
A state of understanding each other, on the other hand, does not imply suchcompositionality: It remains unchanged throughout the whole period when it is true.In other words, the meaning of events exhibits a dynamic component, whereas that ofstates does not.Events are further categorized by whether a particular situation lasts for some timeor occurs momentarily.
The latter type of events are referred to as achievements, andevents that imply duration are known as processes.
For example, the nature of eventssuch as dropping, stumbling, or recognizing is that they occur instantaneously and,therefore, are achievements.
On the other hand, events such as playing golf or writingan essay last for some time, so they are processes.Processes are classified into accomplishments and activities depending on whethera situation implies an ending (Vendler 1967, page 100).
This property is known astelicity.
Reading a book in the context of Example 2a implies that the person finishedreading it: the overall situation is telic.
We cannot say that she has read the book in thefirst 15 minutes of doing so because the implied ending was not achieved (i.e., the bookhas not been read).
Such situations are referred to as accomplishments.
On the otherhand, playing golf or talking on the phone does not imply that the process must endwith a specific conclusion and the situation is atelic.
Such situations are called activities.In addition, the aspectual type of a clause may be altered by multiplicity, for exam-ple, repetitions.
Consider Examples 2a and 2b.
(2a) She read a book.
(2b) She read a book a day.Example 2b is referred to as a serial situation (Huddleston and Pullum 2002, page 123).It is considered to be a state, even though a single act of reading a book would constitutean event.79Computational Linguistics Volume 36, Number 1Intuitively, stative?and especially serial?situations are more likely to be associ-ated with descriptions, that is to say, with things that are, or things that were happeningfor an extended period (consider He was a tall man vs.
He opened the window).
Theremainder of Section 4 describes how we identify single and serial stative clauses anduse them to construct summaries.4.2 Overall System DesignSeveral system components are responsible for selecting salient background sentences.A story, annotated for the presence of important entities (as outlined in Section 3), isparsed with the Connexor Machinese Syntax parser.
The sentences are then recursivelysplit into clauses based on the results of parsing.
For the purposes of this project, aclause is defined as a main verb as identified by the parser (whether finite or non-finite)with all its complements, including subject, modifiers, and their constituents.Next, each clause is represented as a vector of features describing its characteristics.The system offers a choice: a fine-grained or coarse-grained representation.
The maindifference between the two is in the level of detail at which each clause is represented.For instance, a fine-grained feature vector has three different features with seven possi-ble values to carry tense-related information: tense, is progressive, and is perfect, whereasa coarse-grained vector carries only one binary feature, is simple past or present.6Finally, the system selects salient descriptive sentences.
Regardless of the granular-ity of the representation, one may choose between two different procedures for sentenceselection.
The first procedure employs machine learning techniques, namely the C5.0decision tree induction (Quinlan 1992).
The second procedure applies a set of manuallycreated rules that guide the classification process.
Section 4.3 gives a motivation for fea-tures used in each data set.
Sections 5.1?5.3 describe the experimental setting.
Section 6presents the results.The part of the system that selects descriptive sentences is implemented in Python.4.3 Feature Selection: Description and MotivationThere are two criteria for the selection of features for both representations:(Criterion 1) a clause should ?talk?
about important things, such as characters orlocations(Criterion 2) a clause should contain background descriptions rather then eventsWe hypothesize that sentences which satisfy both criteria are good candidates for inclu-sion in indicative summaries.
In other words, a summary that consists of such sentenceswould familiarize the reader with important elements of the setting of the story, butwould not reveal the plot.The features that contribute towards Criterion 1 can be divided into character-related and location-related.
We have designed character-related features to help iden-tify sentences that focus on characters, not just mention them in passing.
These featuresare modeled so as to help identify sentences that contain at least one mention of animportant character with a salient grammatical function (e.g., subject).
Location-related6 Furthermore, in this article we refer to a data set annotated with the fine-grained features as thefine-grained data set, and to the one annotated with the coarse-grained features as the coarse-graineddata set.80Kazantseva and Szpakowicz Summarizing Short StoriesTable 2Description of the features in both data sets.Fine-grained data set Coarse-grained data setType of Number of Number of Number of Number offeatures features values features valuesCharacter-related 10 18 4 6Aspect-related 14 48 6 15Location-related 2 4 2 4Other 3 7 3 4All 29 77 15 29features are intended to help identify sentences where named entities tagged as loca-tions by the Gazetteer indeed refer to location names.Criterion 2 has been introduced to ensure that the selected sentences are back-ground sentences (as opposed to those relating events) and are therefore suitable forinclusion in indicative summaries.
To this end, the features that contribute towardsCriterion 2 are designed to identify stative clauses and clauses that describe serial situa-tions.
A single unambiguous indicator of aspectual type does not exist, but a number ofverb-related characteristics of the clause may signal or limit its possible aspectual type.These characteristics include the lexical aspect of the main verb, tense, the presence oftemporal expressions, voice, and certain properties of the direct object.
The verb-relatedfeatures capture this information in our representation.7The remainder of this section contains a detailed description of the various types offeatures and motivates their inclusion.
Table 2 shows how many features contributeto each criterion, and how many discrete values they have.
Appendix A contains acomplete list of features used in both representations, explains how they are computed,and shows the cardinality of the sets of possible values.Character-related features.Character-related features help ensure that selected sen-tences are about one of the important characters in the story.
So, this group of featuresdescribes whether a clause contains a character mention and what its grammaticalfunction is (subject, object, indirect object, or other).
Mentions of characters early in thetext tend to contain more salient background information.
That is why character-relatedfeatures reflect the position of a parent sentence8 relative to the sentence where thecharacter is introduced.
In addition, these features capture the presence of a charactermention that is premodified by a noun phrase.
The interest in such mentions is inspiredby the fact that these constructions?appositions?often introduce new entities into thediscourse (Vieira and Poesio 2000).
For the same reasons, the system also establisheswhether a character mention is nominal or pronominal (e.g., Jack vs. he), whether it isused in the genitive case (e.g., Jack?s) and, for common nouns, whether the mention isaccompanied by an indefinite article.7 It must be mentioned that several researchers have looked into automatically determining varioussemantic properties of verbs (Siegel 1998b; Merlo et al 2002).
These approaches, however, attempt todetermine properties of verbs viewed in isolation and do not deal with particular usages in the context ofconcrete sentences.
That is why we cannot directly re-apply that research in determining the aspectualtype of clauses.8 By parent sentence we mean the sentence from which the clause is taken.81Computational Linguistics Volume 36, Number 1Table 3Privative featural identification of aspectual classes after Dorr and Olsen (1997).Aspectual class Telic Dynamic Durative ExamplesState + know, believeActivity + + paint, walkAccomplishment + + + destroyAchievement + + notice, winLocation-related features.
In discourse such as fiction, not all tokens that theGazetteer recognizes as markers of location denote locations.
Location-related featureshelp identify mentions of locations in each clause and verify that these mentions indeeddenote a place.
These features describe whether a clause contains a mention of a locationand whether it is embedded in a prepositional phrase.
The rationale for these features isthat true location mentions are more likely to occur inside prepositional phrases, suchas from Chicago or to China.Verb-related features.
Verb-related features model the characteristics of a clausethat help determine its aspectual type.Lexical aspect of a verb.
Lexical aspect refers to a property of a verb when viewedin isolation, without regard to the context provided by a particular clause.
Just as forclauses, a verb may be a state (or stative) verb (e.g., believe), or an event verb (e.g., run).Event verbs are further subdivided into verbs of activity (e.g., read), accomplishment(e.g., take a test), and achievement (e.g., drop).The relation between the lexical aspect of a verb and the aspect of a clause has beendiscussed by Vendler (1967), Dorr and Olsen (1997), and Huddleston and Pullum (2002,pages 118?123).
Dorr and Olsen have proposed a privative model of this relation?seeTable 3.
The model states that verbs are categorized into aspectual classes based onwhether they exhibit one or more of the following properties: dynamicity, durativity,and telicity.
Dorr and Olsen speculate that, depending on the context of usage, verbsmay form clauses that have more of these properties than the main verb viewed inisolation, but that it is impossible for a verb to ?shed?
one of its properties.
We illustratethis in Examples 3a and 3b.
In Example 3a the state verb know participates in anaccomplishment clause; the clause is telic, although the verb by itself is not.
On the otherhand, an attempt to deprive the accomplishment verb destroy of its telic meaning whenconstructing a clause of type activity fails to create an acceptable clause (Example 3b).
(3a) He knew it that very moment.
(accomplishment)(3b) *He was destroying it for an hour.
(activity) 9The lexical aspect of a verb influences the aspect of a clause.
Several features in oursystem capture this information.
The fine-grained data set contains three features withsix possible values that show whether the main verb of a clause is durative, dynamic, ortelic.
The coarse-grained data set contains a single feature with four possible values (thelexical aspect of a verb according to the model in Table 3).
We derive this informationfrom a manually compiled database of Lexical Conceptual Structures (Dorr and Olsen1997), which contains these properties for 4,432 English verbs.9 Throughout this paper, the asterisk (*) denotes incorrect or marginally correct usage.82Kazantseva and Szpakowicz Summarizing Short StoriesGrammatical tense.
The grammatical tense used in a particular clause places anumber of constraints on its aspectual type.
For instance, simple tenses are morelikely to be used in stative or habitual situations than progressive or perfect tenses.It is also commonly accepted (Dowty 1979; Huddleston and Pullum 2002, page 119)that stative clauses cannot be realized using progressive tenses (see Examples 4a and4b).
Huddleston and Pullum (2002, page 121) stipulate that it is also the case withachievement clauses (see Example 4c).
(4a) John is running.
(event, activity)(4b) *John is knowing the answer.
(state)(4c) *John was recognizing her.
(event, accomplishment)Among the constraints that grammatical tense imposes there is the special relationbetween simple present tense and event clauses.
As a rule, clauses realized in simplepresent tense cannot denote events, but only states (Huddleston and Pullum 2002,page 119).
The matter is illustrated in Examples 5a through 7b.
(5a) She knew history well.
(state)(5b) She knows history well.
(state)(6a) She fell off a chair.
(event)(6b) *She falls off a chair.
(event)(7a) She danced (last night).
(event)(7b) She dances.
(state)In the fine-grained data set the information related to tense is expressed using threefeatures with seven possible values (whether a clause is in present, past, or future tense;whether it is progressive; and whether it is perfective).
In the coarse-grained data set,this information is expressed using one binary feature: whether a clause is in simple,past, or present tense.Temporal expressions.
Temporal markers (often referred to as temporal adverbials),such as usually, never, suddenly, at that moment, and many others, are widely employedto mark the aspectual type of a sentence (Dowty 1979; Harkness 1987; By 2002).
Suchmarkers provide a wealth of information and often unambiguously signal aspectualtype.
For example:(8a) She read a lot tonight.
(8b) She always read a lot.
(or She used to read a lot.
)Such expressions are not easy to capture automatically, however.
In order to use theinformation expressed in temporal adverbials, we analyzed the training part of thecorpus for the presence of such expressions.
There were 295 occurrences in 10 stories.It turns out that this set can be reduced to 95 templates.
For example, the expressionsthis year, next year, that long year can all be reduced to a template ?some expression year?.Possible values of ?time expression?
are further restricted to allow only valid modifiers(e.g., last, next, but not yellow).
The system captures temporal expressions using a cas-cade of regular expression.
It first identifies the least ambiguous unit (in this example83Computational Linguistics Volume 36, Number 1year) and then attempts to find the boundaries of the expression.
The complete list ofregular expressions used appears in Kazantseva (2006).Three features characterize each template: the type of the temporal expression(location, duration, frequency, or enactment) (Harkness 1987); magnitude (year, day,etc.
); and plurality (year vs. years).
The fine-grained data set contains three such fea-tures with 14 possible values (type of expression, its magnitude, and plurality).
Thecoarse-grained data set contains one binary feature (whether a clause contains an ex-pression that denotes a long period of time).Voice.
Usually, clauses in passive voice only occur with events (Siegel 1998b,page 51).
Both data sets contain one binary feature that describes this information.Properties of the direct object.
For some verbs, properties of the direct object helpdetermine whether a given clause is stative or dynamic.
(9a) She wrote a book.
(event)(9b) She wrote books.
(state)It is of particular interest whether the direct object follows a definite or indefinitedeterminer and whether it is used in a singular or plural form.
Two binary featuresthat describe this information are included in the fine-grained data set.Several additional features in both data sets describe the overall characteristics ofa clause and its parent sentence, such as whether these were affirmative statements,exclamations, or questions; their index in the text; and a few others.
The fine-graineddata set contains three such features with seven possible values.
The coarse-graineddata set contains three features with four values.4.4 Handling Clauses with the Verb HaveThe preceding section notes that the same verb may form clauses of different aspectualtypes depending on its context.
A verb with a particularly ambiguous aspect is theverb have (when used as the main verb and not an auxiliary).
Its meaning is stronglyinfluenced by what kind of direct object it takes.
That is why determining its aspectualtype is a very challenging task.
This issue is illustrated in Examples 10a?10c.
(10a) She had lunch.
(event).
(10b) She had a friend.
(state).
(10c) She had an accident.
(event).Due to the high degree of ambiguity, our system handles clauses with have as themain verb in a manner different from all other clauses.
This machinery remains thesame regardless of what options are used for the granularity of representation and forsentence selection procedures.In order to handle have-clauses, our system contains an implementation of an ap-proach proposed by Siegel (1998a).
The solution relies on WordNet (Fellbaum 1998) andcontains a set of rules that determine the aspectual type of a have-clause based on thetop WordNet category of the direct object.
For instance, the direct object lunch fromExample 11a belongs to the category food and, according to rules from Siegel (1998a),the aspectual type of a clause is event.
The direct object friend from Example 11b belongsto the category person, so the aspectual type of the clause is state.
Siegel (1998a) usedWordNet 1.6, whereas we work with a newer version, WordNet 2.0.
The structure of84Kazantseva and Szpakowicz Summarizing Short StoriesFigure 5Pseudo-code for determining the type of have-clauses based on the WordNet category of directobject (Siegel 1998b).this newer ontology is different from that of version 1.6.
For this reason, we considerall parent categories in the hypernym tree, not only the top category.
For the sake ofcompleteness, Figure 5 shows the pseudo-code for this procedure.
The system judges ahave-clause to be summary-worthy if two conditions are fulfilled: the clause contains amention of one or more important characters and it is a state clause.5.
Experiments5.1 Experimental SettingThe final version of the summarizer proceeds as follows.
First of all, the stories areparsed with the Connexor parser and named entities are recognized using the GATEGazetteer.
Then the system resolves anaphoric references and identifies important char-acters and locations.
During the next stage, the summarizer splits all source sentencesinto clauses and creates coarse- and fine-grained representations for each clause.
Aclause is modeled as a vector of character-, location- and verb-related features.
Finally,the system employs two alternative procedures to select summary-worthy sentences:manually designed rules and machine learning.We performed a number of experiments to find out how successful our system isin creating summaries of short stories.
The experimental corpus consisted of 47 shortstories split into a training set of 27 stories and a test set of 20 stories.
The average lengthof a story in the corpus was 3,333 tokens, 244 sentences, or approximately 4.5 U.S.-letter-sized pages.
The corpus contains stories written by 17 different authors.
It was splitmanually so that its training and test portions contained approximately an equal num-ber of stories by the same writer.
The first author of this paper annotated each clause ofevery story for summary-worthiness and achieved the compression rate of 6%, countedin sentences.
This rate was the target compression rate in all further experiments.The training data set consisted of 10,525 clauses, 506 of which were annotated assummary-worthy and all others as not summary-worthy.
The test data set contained7,890 clauses, 406 of them summary-worthy.We fine-tuned the system and used the training portion of the data set to iden-tify the best settings.
Then we ran two sets of experiments on the test portion.
In thefirst set of experiments, we applied a manually designed set of rules that select sen-tences for possible inclusion in summaries.
These experiments are described in Sec-tion 5.2.
The second set of experiments relied on using machine-learning techniquesto create summaries.
It is described in Section 5.3.
After the completion of the experi-ments, the summaries were evaluated by six judges.
They were also compared against85Computational Linguistics Volume 36, Number 1Figure 6Examples of manually designed rules.extractive summaries produced by three people.
Section 6 discusses the evaluationprocedures in detail and reports the results.5.2 Experiments with Manually Designed RulesThe first classification procedure applies manually designed rules to a clause-levelrepresentation of the original stories to produce descriptive summaries.
The rules aredesigned using the same features as those used for machine learning and described inSection 4.3 and in Appendix A.The first author created two sets of rules to guide the sentence classification process:one for the coarse-grained and another for the fine-grained representation.
The rulesoperate at clause level.
If a clause is deemed summary-worthy, the complete parentsentence is included in the summary.
Figure 6 displays a few examples of rules for thefine-grained data set (a clause is considered to be summary-worthy if a rule returnsTrue).
The first rule attempts to select clauses that talk about one of the main charactersand contain temporal expressions of type enactment.
The rationale for this rule is thatsuch clauses are likely to describe habitual activities of protagonists (e.g., He alwayssmoked.)
The second rule follows the same rationale but the stativity of the situationis signaled by the main stative verb.
The third rule rejects clauses in progressive tensebecause such clauses are unlikely to contain background information.The set of rules for the fine-grained representation has a tree-like structure.
Itprocesses the features of a clause and outputs a binary prediction.
The rules for thecoarse-grained representation function differently.
Each clause is assigned a score basedon the values of its features.
The system then selects 6% of sentences that contain clauseswith the highest scores.
The scores attributed to the particular feature values wereassigned and fine-tuned manually using linguistic knowledge described in Section 4.3.The reasons why the procedures for the two data sets differ are as follows.
Assigningand fine-tuning the scores is a more flexible process and it is easier to perform manu-ally.
Ideally, we would apply score-based rules to both representations, but assigningand fine-tuning the scores manually for the fine-grained data set is excessively labor-intensive: there are too many features with too many values.
For instance, one maywant to reward clauses in simple past or present tenses, reflecting the fact that suchclauses are more likely to be descriptive than those in perfect or progressive tenses.This information is expressed in the coarse-grained data set using one binary featuresimple past present and fine-tuning the score is trivial.
On the other hand, the same86Kazantseva and Szpakowicz Summarizing Short StoriesFigure 7High-level overview of the rules for the fine-grained data set.information in the fine-grained data set is distributed over three features with a totalof seven values: is perf (yes, no), is progressive (yes, no), and tense (past, present.
future).Distributing the ?reward?
among three independent features is far less obvious.The rules in both data sets, as well as the set of weights used for the coarse-grainedrepresentation, were selected and fine-tuned empirically using the training portion ofthe corpus as a guide.
Once the parameters had been adjusted, the system producedtwo sets of summaries for the test portion of the corpus (one for each representation).The detailed algorithms for both data sets are too long for inclusion in this article.Figures 7 and 8 show the rationale for the algorithms.
The interested reader is referredto Kazantseva (2006) for pseudo-code.5.3 Experiments with Machine LearningAs an alternative to rule construction, in the second set of experiments we performeddecision tree induction with C5.0 (Quinlan 1992) to select salient descriptive sentences.C5.0 was our choice mainly because of the readability of its output.The training and test data sets exhibited an almost 1:17 class imbalance (i.e., only 6%of all annotated clauses belonged to the positive class).
Because the corpus was rathersmall, we applied a number of techniques to correct class imbalance in the training dataset.
These techniques included classification costs, undersampling (randomly removinginstances of the majority class), oversampling (randomly duplicating instances of theminority class), and synthetic example generation (Chawlar et al 2002).
Using tenfoldcross-validation on the training data set and original annotations by the first author,Figure 8High-level overview of the rules for the coarse-grained data set.87Computational Linguistics Volume 36, Number 1we selected the best class-imbalance correction techniques for each representation andalso fine-tuned the learning parameters available in C5.0.
These experiments broughtthe best results when using classification costs for the coarse-grained data set andundersampling for the fine-grained data set.In order to see what features were most informative in each data set, we conducteda small experiment.
We removed one feature at a time from the training set and used thedecrease in F-score as a measure of informativeness.
The experiment showed that in thecoarse-grained data set the following features were the most informative: the presenceof a character in a clause, the difference between the index of the current sentence andthe sentence where the character was first mentioned, syntactic function of a charactermention, index of the sentence, and tense.
In the fine-grained data set the findings aresimilar: the index of the sentence, whether a character mention is a subject, the presenceof a character mention in the clause, and whether the character mention is a pronounare more important than the other features.After selecting the best parameters on the training data set using tenfold cross-validation, the system produced two sets of summaries for the test data set.6.
Evaluation6.1 OverviewWe are not aware of any agreed-upon metrics for evaluating summaries of short fiction.In fact, it is not wholly clear what makes one summary better than another even formanual ones.
That is why we evaluate our summaries using a variety of metrics andbaselines, hoping to obtain a stereoscopic view of their quality.The first evaluation experiment aims to measure the informativeness and the use-fulness of the summaries.
It is designed so as to compare the machine-made summarieswith a random baseline and also with a ?ceiling??
manual abstracts (henceforth modelsummaries).
To achieve this, we engaged 15 evaluators to read the summaries andthe stories of the test set and to answer two types of questions about them: factual(e.g., list main characters of the story) and subjective (e.g., rank readability of the summary).Such experimental design allowed us to evaluate extrinsically the informativeness ofthe summaries and intrinsically their usefulness.
Both types of questions were askedfirst after reading the summary alone and then after reading the complete story.
Thesummaries are an anonymous mix of random, machine-made, and model ones (i.e., theevaluators did not know whether the summaries were produced by programs or bypeople).
Section 6.2 describes the experiment in detail.The second round of evaluation aimed to evaluate the summaries by measuringsentence co-selection with the manually created extracts.
It was designed to allowthe comparison of machine-made summaries with two naive baselines and with twostate-of-the-art generic summarizers (baseline summarizers).
Section 6.3 contains thedescription of this experiment.The third evaluation experiment compared the machine-made and the baselinesummaries with the manually created abstracts using ROUGE (Lin 2004)?a packagefor automatically evaluating summaries.
This experiment is described in Section 6.4.6.2 Evaluating Informativeness and Usefulness of the SummariesWe define the objectives of this experiment as measuring the informativeness, theusefulness and, to some extent, the linguistic quality of the summaries that our system88Kazantseva and Szpakowicz Summarizing Short Storiesproduces.
The informativeness is measured indirectly?by asking people factualquestions about the story.
The linguistic quality and the usefulness are evaluatedintrinsically?by asking people to rank specific characteristics of the summaries.Fifteen unbiased evaluators answer both types of questions twice, first after readingthe summary alone and then again after reading the complete story?repeating theprocedure for all 20 test stories.
Asking the questions after reading the summaryalone measures the informativeness and the usefulness of the summaries in a realisticsituation: To an evaluator, the summary is the only source of information about theoriginal story.
Repeating the procedure after reading the complete story evaluates thesummaries in a situation where the evaluator has the complete information aboutthe source.
Each evaluator works with a mix of machine-made, random, and modelsummaries with six or seven summaries of each kind.
This allows comparing theperformance of our summarizer with a baseline and a ceiling.Our summarizer produces four different flavors of summaries.10 The labor intensityof the process prohibits asking the subjects to evaluate all four summary types.
That isalso why it is not possible to use more than one baseline or the summaries createdby the baseline systems.11 Restricted to evaluating only one type of the machine-madesummaries, we opt for the coarse-grained rule-based ones, mainly because the coarse-grained representation and the rules make it easier to trace why the system selectsspecific sentences.Conditions to be tested.
We evaluate three factual and four subjective characteristicsof the summaries.FactualHow well the reader can name the main characters the location the timewhen the summary is the only source of information about the story.Subjective How readable the summaries are. How much irrelevant information they contain. How complete they are. How useful they are for deciding whether to read the complete story.Evaluating these facets of the summaries reveals whether we achieve the objective ofproducing informative summaries.
The focus of the system was not on readability.
Still,we evaluate how readable the summaries are, because severe lack of coherence mayprevent people from correctly interpreting the available information.
We have provided10 The options are as follows: either the coarse-grained or the fine-grained representation, and selectingsentences using either rules or machine learning.11 Each additional summary type would have required approximately 67 extra man-hours.89Computational Linguistics Volume 36, Number 1no definitions or criteria to the subjects apart from the questions shown in Tables 4and 5.Baselines.
We compare the machine-made summaries with a baseline and a ceiling.The baseline consists of randomly selected sentences.
Both the machine-made and therandom summaries contain the same number of sentences.
The ceiling consists of thesummaries written by two human subjects.
The summary writers were instructed towrite 20 summaries of short stories in a way that does not reveal all of the plot.
Theyreceived one example summary and were allowed to reuse sentences from the stories,to employ metaphor and any other literary devices they found useful.Metrics.
The evaluators answered the factual questions about the main charactersand the location of the story in their own words.
The first author rated the answerson the scale of ?1 to 3.
A score of 3 means that the answer is complete and correct,2 = slightly incomplete, 1 = very incomplete, 0 = the subject cannot find the answerin the text, and ?1 = the answer is incorrect.
The question asking to identify the timeframe of the story is a multiple-choice one: select the century when the story takes place.The answers to this question are rated on a binary scale (1 if the answer is correct,0 if it is not or if the subject cannot infer time from the text).
We calculate the meananswers for each question and compare them across summary types using the Kruskal?Wallis test and the Mann?Whitney test (also known as the Wilcoxon Rank?Sum test).The tests are appropriate when the response variable is ordinal and the dependentvariable is categorical.
Both tests are based on assigning ranks to the available datapoints.The Kruskal?Wallis test is a nonparametric test used to determine whether severalsamples come from the same population.
It is based on calculating the K statistic whichfollows ?2 distribution for sample sizes of five or larger.
Given i samples containingti data points each with Ri being the sum of ranks of all data points in sample ti, K iscalculated as follows (Leach 1979, page 150):K = 12n(n+ 1)?ti(Riti?
n+ 12)2 (1)In order to make pairwise comparisons between samples and to establish the locusof the difference, we rely on the Mann?Whitney test.
The test is based on calculating theS statistic.
For large sample sizes the distribution of S can be approximated using thenormal distribution.S = 2R?
t2(n+ 1) (2)where t2 is the size of the smaller sample, n is the size of both samples together, and Ris the sum of ranks in the smaller sample.
We use the Kruskal?Wallis test with 0.01 con-fidence level.
In order to avoid increasing the chance of Type I error when performingpairwise comparisons, we set per-comparison confidence level for the Mann?Whitneytest at ?
= ?/c where ?
is the desired per-experiment confidence level and c is thenumber of comparisons (Leach 1979, page 161).
In our case ?
= 0.0033.All subjective questions are multiple-choice questions.
An evaluator has to select ascore of 1 to 6, with 1 indicating a strong negative property and 6 indicating a strongpositive property.
We opt for a scale with an even number of available values so asto avoid the evaluators?
giving excessive preference to the middle rank.
We measurethe mean ranks for each question and compare them across summary types using theKruskal?Wallis and Mann?Whitney tests.
The inter-annotator agreement is computed90Kazantseva and Szpakowicz Summarizing Short Storiesusing Krippendorff?s ?
(Krippendorff 2004, pp.
221?236) (henceforth ?).
?
measuresdisagreement between annotators corrected for chance disagreement.?
= 1 ?DisagreementobservedDisagreementexpected= 1 ?Average metric ?2 within all categoriesAverage metric ?2 within all items(3)Unlike other coefficients of inter-coder agreement, ?
allows taking into account themagnitude of disagreement by specifying a distance metric ?2.
This property is crucialin our case: a situation when raters disagree whether to give a summary a rank of 1 or 6should be penalized more heavily than a situation when they do not agree betweenthe ranks of 5 and 6.
When computing ?, we use the distance metric suggested byKrippendorff for ordinal data (Krippendorff 2004, page 223):ordinal ?2 = (nc2+g<k?g>cng +nk2)2 (4)where c and k, c < k, are the two ranks.For all questions, the computation of ?
is based on the following parameters: N =300, n = 15, and c = 6, where N is the total number of items (i.e., summary?story pairsranked), n is the number of raters, and c is the number of available categories.Subjects.
The participants for the experiment were recruited by the means of adver-tising at the Department of Linguistics at the University of Ottawa.
Most of them arethird- and fourth-year undergraduate students of linguistics.
The only requirement forparticipation was to be a native speaker of English.
We hired two people to create modelsummaries for the 20 stories of the test set.
The summary writers worked approximately15?20 hours each.
Fifteen people were hired to evaluate the summaries (i.e., to readthe summary?story pairs and answer the questions).
The task of evaluating a sum-mary required approximately 12?15 hours of labor per person.
All participants werepaid.
The instructions for summary writers are available at www.site.uottawa.ca/?ankazant/instructions-writers.zip.
The instructions for evaluators can be foundat www.site.uottawa.ca/?ankazant/instructions eval.zip.Material.
Each evaluator received 20 summary?story pairs.
Because some questionssought to measure the informativeness of the summary, every evaluator worked on 20distinct stories of the test set and no one worked with the same story more than once.The summaries were a randomly selected mix of random, machine-made, and modelsummaries.Procedure.
The experiment was conducted remotely.
The summary writers receivedthe test set of stories and the instructions and had seven working days to submit theirabstracts.
A week later, we sent randomly generated packages of summary?story pairsto the evaluators.
The packages contained between six and seven summaries of eachkind (random, machine-made, and model).
Each evaluator worked with exactly onesummary for each story, reading a total of 20 pairs.
Every summary was evaluated byfive subjects.
The evaluators had seven working days to complete the task.Results.
Informativeness.
Table 4 shows the results of comparing the mean an-swers between the machine-made, the baseline, and the model summaries using theKruskal?Wallis and Mann?Whitney tests.
The column Groups shows homogeneousgroups, identified using the Mann?Whitney test with 99.67% confidence (recall thatper-comparison confidence level ?
= 0.0033).
The groups are denoted using distinctliterals (e.g., A, B, C).91Computational Linguistics Volume 36, Number 1Table 4Answers to factual questions.Summary type After reading the summaries only After reading the storiesMean rank Groups Std.
Dev Mean rank Groups Std.
DevQuestion: Name three main characters in the order of importance.Model 2.24 A 0.73 2.73 A 0.49Machine 2.21 A 0.69 2.71 A 0.56Random 1.42 B 1.04 2.67 A 0.62Question: Name the location of the story.Model 2.1 A 1.25 2.62 A 0.93Machine 1.39 B 1.33 2.79 A 0.62Random 0.71 C 1.18 2.43 A 0.98Question: Select the century when the story takes place.Model 0.5 A 0.5 0.69 A 0.46Machine 0.29 B 0.46 0.76 A 0.43Random 0.19 B 0.39 0.7 A 0.54The differences between the machine-made summaries and the random onesare significant for the questions about characters and the location of the story.
Thisshows that in these respects the machine-made summaries are?rather predictably?consistently more informative than the random ones.
The difference between themachine-made and the random summaries is not statistically significant for the questionasking to name the time of the story.
Keeping in mind how rare absolute temporalanchors are in short stories, this is not surprising.
The manual summaries, however,are ranked higher with statistical significance.
This may suggest that the machine-madesummaries are not as coherent as the model ones, which prevents the reader fromfinding implicit cues about timeframe available in the summaries.The differences between the machine-made and the model summaries are signifi-cant for the questions about the time and the place of the story, but not for the questionsabout the main characters.
This suggests that the machine-made summaries are almostas informative as the model ones when it comes to informing the reader whom the storyis about.
They cannot, however, give the reader as good an idea about the time and theplace of the story as the model summaries can.All summary types are less informative than the complete story; that is, the differ-ences between answers obtained after reading the summary alone and after reading thecomplete story are significant in all cases.Usefulness and linguistic quality.
Table 5 shows mean ranks for the three summarytypes, along with the homogeneous groups identified using the Mann?Whitney test,with 99.67% confidence.
The request to rank readability was made only once?afterreading the summary; the request to evaluate the completeness was made only afterreading the complete story.
(The corresponding cells in Table 5 are empty.
)The results reveal that the evaluators consistently rank the model summaries asbest, the machine-made summaries as second-best, and the random ones as worst.
Thedifferences between summary types are significant in all cases.The readability of the machine-made summaries is ranked as slightly better thanaverage (3.28 on the scale of 1 to 6).
For all other questions (relevance, completeness, and92Kazantseva and Szpakowicz Summarizing Short StoriesTable 5Subjective rankings.Summary type After reading the summaries only After reading the storiesMean rank Groups Alpha Mean rank Groups AlphaQuestion: How readable do you find the summary?
(Is it coherent and easy to read, or confusing and does not make sense?)
(scale: 1 to 6)Model 5.44 A 0.74Machine 3.28 BRandom 1.89 CQuestion: How much irrelevant information does the summary contain?
(useless, confusing information, fragments that do not make sense) (scale: 1 to 6)Model 5.10 A 0.61 5.24 A 0.62Machine 2.83 B 2.82 BRandom 1.93 C 1.85 CHaving read the story, do you find that a lot of important information is missing?Rate how complete you find the summary.
(scale: 1 to 6)Model 5.18 A 0.69Machine 2.81 BRandom 1.65 CImagine that this summary should help you decide whether you would like to read the completestory.
How helpful was the summary for this purpose?
(scale: 1 to 6)Model 5.22 A 0.60 5.11 A 0.63Machine 2.81 B 2.81 BRandom 1.88 C 1.65 Cusefulness), the machine-made summaries are ranked as slightly worse than average(around 2.81).
This shows that even though the summaries are somewhat useful andconsistently outperform the random baseline, they fall short of the quality of the manualabstracts.
This is hardly surprising given the inherent difficulty of summarizing fictionand the exploratory nature of this work.
It is worth remarking that even the modelsummaries do not appear to be perfect: The evaluators ranked them around 5.20, eventhough they had significantly worse summaries to compare against.
This may suggestthat the task is not easy even for people, let alne for a computer program.The column labelled Alpha in Table 5 shows the results of measuring the extent towhich the evaluators agree when answering the subjective questions.12 The agreementis measured using Krippendorff?s ?.
The results show substantial agreement but fallshort of the reliability cut-off point of 0.8 suggested by Krippendorff.
The failure toreach such high agreement is hardly surprising: the task of ranking the quality ofthe summaries is highly subjective.
Instead of asking the subjects to bin items intoa predefined number of categories, the task calls for discretizing a concept which is12 We have not measured the agreement for factual questions because those questions were answered in theevaluators?
own words and the answers were scored by the first author.
To give the reader an idea ofvariability of the answers, Table 4 reports standard deviation from the mean.93Computational Linguistics Volume 36, Number 1continuous in nature: the quality of a summary.
That is why we interpret the level ofagreement as sufficient for the purpose of evaluating the quality of the summaries.6.3 Comparing the Machine-Made Summaries and the Manually Created ExtractsMeasuring sentence co-selection between extractive summaries created by humans andthose created by automatic summarizers has a long tradition in the text summarizationcommunity (Lin and Hovy 2000; Marcu 2000), but this family of measures has a numberof well-known shortcomings.
As many have remarked on previous occasions (Mani2001; Radev et al 2003), co-selection measures do not provide a complete assessment ofthe quality of a summary.
First of all, when a summary in question contains sentencesthat do not appear in any of the model extracts, one may not be sure that those sentencesare uninformative or inappropriate for inclusion in a summary.
In addition, documentshave internal discourse structure and sentences are often inter-dependent.
Therefore,even if a summary contains sentences found in one or more reference summaries, itdoes not always mean that it is advisable to include those sentences in the summary inquestion.Sentence co-selection does not explicitly measure the quality of a summary.
It does,however, measure a quality that is objective and easy to pin down: how many sentencesthat humans judge summary-worthy are included in the machine-made summary.Such a metric is a useful complement to the results reported in Section 6.2.
It has theadvantage of being easy to interpret and comprehend.
It also has a long tradition ofusage which allows us to compare our summarizer?on a familiar scale?with othersummarization systems.
That is why we chose co-selection as the basis for comparingthe summaries that our system produces with manually created extracts.13Overview.
The experiment involves six annotators divided into two groups of three.Each annotator is asked to read 10 short stories and to select 6% of sentences that, in theiropinion, constitute a good indicative summary.
In this manner three people annotateeach story of the test set for summary-worthy sentences.
We used their annotations as agold standard and compared the machine-made summaries against them.
In addition,we used the same gold standard and metrics to evaluate the quality of two baselinesummaries and of two summaries produced by state-of-the art summarization systems.Conditions to be tested.
The purpose of the experiment is defined as measuring howmany sentences found in our system?s summaries and the baseline summaries occur inthe extractive summaries created by the human annotators.
We are also interested infinding out whether our summarizer outperforms the trivial baseline algorithms andthe existing state-of-the-art summarizers fine-tuned to summarizing newswire.Baselines.
In order to evaluate our summarizer comparatively, we defined two naivebaselines and a ceiling.
Intuitively, when a person wishes to decide whether to reada book, she opens it and flips through several pages at the beginning.
Imitating thisprocess, we computed a simple lead baseline consisting of the first 6% of the sentences ina story.
The second baseline consists of 6% of sentences of the story selected at random.The ceiling consists of all sentences deemed summary-worthy by one of the humanannotators.13 We decided against using deeper approaches, such as the Pyramid method (Nenkova and Passonneau2004), factoids (van Halteren and Teufel 2003), and relative utility (Radev and Tam 2003).
The reason ispractical: These approaches have an unfortunate disadvantage of being considerably morelabor-intensive than the measures based on sentence co-selection.94Kazantseva and Szpakowicz Summarizing Short StoriesIt is also necessary to see whether our genre-specific approach shows any improve-ments over the existing generic state-of-the-art systems put to work on fiction.
To thisend, we compared our summarizer with two systems that were top performers in theDocument Understanding Conference (henceforth DUC) 2007, the annual ?competi-tion?
for automatic summarizers.
In DUC competitions the summarization systemsare evaluated on a variety of metrics: manually assigned scores (ranking readability,grammaticality, non-redundancy, referential clarity, focus, and coherence), the pyramidmethod (Nenkova and Passonneau 2004), and ROUGE scores (Lin 2004).
There is nounified ranking of the systems?
performance, and selecting the best summarizer isnot straightforward.
We chose two systems among the top performers in DUC 2007?GISTexter (Harabagiu, Hickl, and Lacatusu 2007) and CLASSY (Schlesinger, O?Leary,and Conroy 2008; Conroy, Schlesinger, and O?Leary 2007).
GISTexter appears to bethe best summarizer according to the scores assigned by the human judges.
Apartfrom baselines, it is consistently ranked as the best or the second-best system on mostcharacteristics evaluated by the judges (the only exception is non-redundancy whereGISTexter is ranked eighth).
CLASSY, on the other hand, is one of the four top systemsaccording to ROUGE scores.
The scores it received from the human judges are alsoquite good.The main task in DUC 2007 called for creating 250-word summaries from a collec-tion of newswire articles on a specific topic.
Each input collection was accompanied bya topic statement that briefly explained what the summaries should cover.
Therefore,both CLASSY and GISTexter are geared towards multi-document query-based sum-marization of newswire?a task dramatically different from that of summarizing shortfiction.14 No adjustments were made to either system to make them more suitable forsummarizing stories.
Therefore, the comparison is not wholly fair, but?in the absenceof systems similar to ours?it was the only possibility to compare our summarizer withthe state-of-the-art in the community and to verify whether genre-specific methods areuseful in summarizing fiction.Evaluation metrics.
By combining summaries created by the annotators in severalways we create three distinct gold-standard summaries.
The majority gold-standardsummary contains all sentences selected by at least two judges.
It is best suited to givean overall picture of how similar the machine-made summaries are to the man-madeones.
The union gold standard is obtained by considering all sentences that are judgedsummary-worthy by at least one judge.
Union summaries provide a more relaxedmeasurement.
Precision computed on the basis of the union gold standard gives an ideaof how many irrelevant sentences a given summary contains (sentences not selected byany of the three judges are more likely to prove irrelevant).
The intersection summariesare obtained by combining sentences that all three judges deemed to be important.Recall measured on the basis of the intersection gold standard says how many of themost important sentences are included in summaries produced by the system (sentencesselected by all three judges are likely to be the most important ones).
All summariesare compared against each gold-standard summary using precision (P), recall (R), andequally-weighted F-score (F).Recall = TPTP+ FN(5)14 GISTexter in particular relies on extensive syntactic and semantic query decomposition and, thus, is at adisadvantage when no informative query is provided.95Computational Linguistics Volume 36, Number 1Precision = TPTP+ FP(6)F = 2TP2TP+ FP+ FN(7)TP denotes true positives, FP = false positives, TN = true negatives, and FN = falsenegatives.The statistical significance of the differences between various types of summariesis established using the one-way Analysis Of Variance test (ANOVA) and the TukeyHonestly Significant Differences test (henceforth Tukey HSD).ANOVA tests whether the differences between sample means are significant bycomparing variance between samples with total variance within samples:f =S 2between/(p?
1)S 2within/(n?
1)(8)where p is the number of samples, n is the total number of observations, Sbetween is thesum of squared deviations between sample means and the total mean, and Swithin isthe total sum of squared deviations within samples.
The f statistic is distributed as Fwhen the null hypothesis is true (i.e., the differences between sample means are notsignificant).The power of ANOVA extends as far as verifying whether the differences betweensample means are significant overall, but the test does not say anything about differ-ences between particular pairs of sample means.
Tukey HSD is a test that does just that.It measures q, the studentized range statistic:q =Ml ?MsSE(9)where Ml is the larger of the sample means, Ms is the smaller one, and SE is the standarderror of the data in question.
In accordance with our interpretation of the three types ofgold standards, we use the most meaningful measurement for each standard: F-scorefor the majority, precision for the union, and recall for the intersection.
We also use thesame measurement to set the ceiling for each standard (i.e., by choosing the manuallycreated extract that compares best on that scale).Before combining the extracts created by the annotators into gold-standard sum-maries, we measure how well these people agree among themselves.
We estimatethe agreement using Scott?s ?
(Scott 1955).15 This coefficient measures the observedagreement between judges corrected for chance agreement.?
=Agreementobserved ?
Agreementexpected1 ?
Agreementexpected(10)15 The coefficient is also known as Siegel and Castellan?s ?
(Siegel and Castellan 1988).96Kazantseva and Szpakowicz Summarizing Short StoriesTable 6Inter-annotator agreement on selecting summary-worthy sentences.Statistic Group 1 Group 2 Average?
(4) 0.52 0.34 0.43?
(3) 0.50 0.34 0.42Agreementobserved =1ic(c?
1)?i?I?k?Knik(nik ?
1) (11)Agreementexpected =1(ic)2?k?Kn2k (12)where i is the number of items to be classified in set I, k is the number of availablecategories in set K, c is the number of coders, nik is the number of coders who assignitem i to category k, and nk is the total number of items assigned to category k by allannotators (Artstein and Poesio 2008, pp.
562?563).Subjects.
Six human subjects participated in annotating the test set of stories for thepresence of summary-worthy sentences.
These people are colleagues and acquaintancesof the first author.
At the time of the experiment none of them was familiar with thedesign of the system.
Four annotators are native speakers of English and the remainingtwo have a very good command of the language.Materials.
The material for the experiment consisted of the 20 stories of the test set.Three annotators created extractive summaries for each story.
In addition, there wereeight distinct automatically produced summaries per story: four summaries producedby our system, two baseline summaries, and two summaries created by the baselinesystems from DUC.Procedure.
The experiment was conducted by e-mail.
The annotators received thestories and had two weeks to annotate them.
The participants reported having taken10?20 hours to complete the task.Results.
Agreement.
Table 6 reports the agreement between the judges within eachgroup and with the first author of this article.
The agreement with the first author isreported because she created the initial training and test data for experiments.
The num-bers 3 and 4 state whether the statistic is computed only for three subjects participatingin the evaluation or for four subjects (including the first author).
As can be seen fromTable 6, the agreement statistics are computed for each group separately.
This is becausethe sets of stories that they annotated are disjoint.
The ?Average?
column shows anaverage of these figures, to give a better overall idea.The agreement values in Table 6 are rather low.
They fall well below the 0.8 cut-offpoint specified by Krippendorff (2004).
On a less demanding scale, Landis and Koch(1977) interpret values in the range of 0.21?0.4 as fair agreement and in the range of0.41?0.6 as moderate agreement.16 Weak agreement is not surprising: Many researchers16 Krippendorff?s suggestion refers to ?, rather than Scott?s ?, and Landis and Koch?s scale was created forCohen?s ?
(Cohen 1960).
In our setting, however, the values of ?,?
and ?
are almost the same.97Computational Linguistics Volume 36, Number 1report that people do not agree well on what sentences constitute a good summary ofa document (Rath, Resnick, and Savage 1961; Salton et al 1997; Lin and Hovy 2003).
Inmost cases the agreement corresponding to ?
of 0.42 would not be sufficient for creatinga resource, but we interpret this level of agreement as acceptable for evaluating a singlefacet of the summaries that are also evaluated in other ways.Co-selection.
Tables 7?9 show the results of comparing eight different versions of thecomputer-made summaries against the gold-standard summaries produced by people.In each table, the entry HUMAN corresponds to the summaries created by the annotatorwho achieves the highest scores for the corresponding standard.
The ?Groups (metric)?column reports homogeneous groups identified using Tukey HSD with 95% confidencefor the specified metric.Our system outperforms both baseline algorithms and the baseline summarizers,but it always falls short of the performance of the best human summary.
The improve-ment margins between the random and the baseline systems?
summaries and thoseproduced by our system are rather wide.
The weaker performance of the baselinesummarizers strongly suggests the need for genre-specific methods when summarizingshort fiction.The differences between the lead summaries and the system-made ones are alsostatistically significant, yet they are much narrower.
We interpret this as an indicationthat the lead baseline is more demanding than the random one when creating indicativesummaries of short fiction.Table 7Sentence co-selection between computer- and human-made summaries.
Majority gold standard.Data set Precision Recall F Groups (F)HUMAN 64.95 84.18 72.41 ARules, fine-grained 39.20 53.35 44.47 BMachine-learning, fine-grained 36.36 49.44 41.26 BRules, coarse-grained 35.42 44.39 38.94 BMachine learning, coarse-grained 35.31 41.81 36.90 BLEAD 25.50 31.18 27.57 CCLASSY 7.14 9.08 7.70 DGISTexter 9.67 6.30 6.91 DRANDOM 4.10 5.40 4.57 DTable 8Sentence co-selection between computer- and human-made summaries.
Union gold standard.Data set Precision Recall F Groups (P)HUMAN 1 55.61 71.05 ARules, fine-grained 56.10 32.11 40.56 BCRules, coarse-grained 53.98 29.77 38.04 BCMachine-learning, fine-grained 52.39 30.66 38.35 CMachine learning, coarse-grained 49.62 25.06 32.78 CLEAD 36.76 18.53 24.50 DEFGISTexter 22.70 6.18 9.28 EFGCLASSY 19.74 10.23 13.05 FGRANDOM 12.41 6.40 8.40 G98Kazantseva and Szpakowicz Summarizing Short StoriesTable 9Sentence co-selection between computer- and human-made summaries.
Intersection goldstandard.Data set Precision Recall F Groups (R)HUMAN 31.32 1.00 45.21 ARules, fine-grained 23.11 79.25 34.02 ABRules, coarse-grained 21.70 65.42 30.55 BCMachine learning, coarse-grained 19.66 57.29 26.80 BCMachine-learning, fine-grained 16.41 56.19 24.00 BCLEAD 12.37 38.83 17.84 DECLASSY 3.58 8.04 4.89 FGISTexter 3.20 6.06 3.83 FRANDOM 0.79 1.67 1.06 FThe results also suggest that automatically produced summaries bear some resem-blance to manual ones.
There is no straightforward way to interpret these results asgood or bad in the context of other summarization systems.
Firstly, the task is new andno comparable results exist.
Secondly, even though sentence co-selection metrics havebeen widely used for evaluating summaries of other genres, different compression rates,different gold standards, and availability of naturally occurring competitive baselines(e.g., lead baseline in newswire summarization) make fair comparison difficult.
Forexample, Marcu (2000, page 214) reports achieving F-score of 76.04 when creating sum-maries of newswire articles at 10% of their original length.
The lead baseline achievesF-score of 71.89.
When summarizing dialogues, Zechner (2002, page 479) reportsweighted accuracy of 0.614 compared to the lead baseline?s performance of 0.438 (thenumbers are averages over five different summary sizes of 5%, 10%, 15%, 20%, and25%).
In this context we interpret the results in Tables 7?9 as suggesting that our genre-specific system outperforms the naive baselines and two generic summarizers.6.4 Evaluating Summaries using Lexical OverlapROUGE (Lin 2004) is a package for automatically evaluating summaries.
Given oneor more gold-standard summaries (usually written by people), ROUGE offers severalmetrics for evaluating the summary in question.
The metrics reward lexical overlapbetween the model summaries and the candidate one.
Depending on the metric, thelexical units taken into consideration are n-grams, word sequences, and word pairs.Since 2004, ROUGE scores have been among the measures used for evaluating au-tomatic summarizers at DUC.
Following this tradition, we ran ROUGE to evaluate oursummaries and to compare them to the baselines (including CLASSY and GISTexter).Conditions to be tested.
The objective of the experiment was to establish howmuch lexical overlap exists between the machine-made and the model summaries.
Weachieved this by computing ROUGE-2 and ROUGE-SU4 scores.17Baselines and material.
We evaluated eight types of summaries: four types createdby our summarizer, the lead and the random baselines, and the summaries created by17 ROUGE-2 and ROUGE-SU4 are two measures that were used at DUC 2007.99Computational Linguistics Volume 36, Number 1GISTexter and CLASSY.
In addition, we included a ceiling by computing ROUGE scoresfor the model summaries.Metrics.
ROUGE-2 score measures the bigram recall between the reference summaryand the candidate one.
It is computed according to the following formula:ROUGE-2 =?s?S?b?s Countmatch(b)?s?S?b?s Count(b)(13)where S is the set of reference summaries, b is a bigram in the reference summary s,Countmatch(b) is the number of bigrams that both summaries share, and Count(b) is thetotal number of bigrams in the reference summary s.ROUGE-S measures the similarity of a pair of summaries based on how manyskip-bigrams they have in common.
A skip-bigram is any pair of words in a sentence,allowing for arbitrary gaps.ROUGE-S =SKIP2(X,Y)C(m, 2)(14)where X is the reference summary of length m, Y is the candidate summary, SKIP2(X,Y)is the number of skip-bigram matches between X and Y, and C is the combinationfunction.ROUGE-SU4 is an extension of ROUGE-S that also rewards matching unigrams.The maximum gap allowed by skip-bigrams is 4 (hence SU4).In order to compare the automatically produced summaries with those created byhumans we implemented the following leave-one-out procedure.
At first, we computedROUGE scores by comparing all automatically produced summaries (i.e., those createdby our system and the baseline ones) and one of the model summaries against thesecond available model summary.
Next, the procedure was repeated but the modelsummaries were switched.
The significance of the differences was tested using ANOVAand Tukey HSD for 95% confidence level.
When calculating ANOVA and Tukey HSD,we used the scores obtained from both runs.Results.
Tables 10 and 11 show ROUGE-2 and ROUGE-SU4 scores for all automati-cally produced and model summaries.
The results are inconclusive.When using ROUGE-2 as a guide, the only summaries consistently different fromthe rest with 95% confidence are the randomly generated ones.
The scores of all othersummaries are too close to reject the hypothesis that the differences are due to chance.This is the case even with the differences between the model and the automaticallyproduced summaries.
A possible interpretation could be that all summaries are ofvery high quality that is indistinguishable from that of the model summaries.
Thishypothesis, however, can be easily dismissed: The results reported in Sections 6.2 and6.3 clearly show that the quality of the summaries produced by our system is well belowthe ceiling.The situation is similar with ROUGE-SU4 scores, if not so dramatic.
There are threedistinct groups of summaries.
Group A includes the rule-based fine-grained summariesand those produced by CLASSY.
The second group includes the lead baseline, threetypes of summaries created by our summarizer, the model summaries, and those cre-ated by GISTexter.
The last group contains the random and the lead baselines.
Eventhough ROUGE-SU4 measurement seems to have more discriminative power, it is atleast puzzling that it cannot distinguish between the model and the automatically100Kazantseva and Szpakowicz Summarizing Short StoriesTable 10ROUGE-2 recall scores.System ROUGE-2 GroupsHUMAN-1 0.0874 AHUMAN-2 0.0807 ARules, fine-grained 0.0981 AMachine learning, fine-grained 0.0905 AGISTexter 0.0829 ACLASSY 0.0826 ARules, coarse-grained 0.0816 AMachine learning, coarse-grained 0.0808 ALEAD 0.0572 ABRANDOM 0.038 BTable 11ROUGE-SU4 recall scores.System ROUGE-2 GroupsRules, fine-grained 0.1684 ACLASSY 0.1654 AGISTexter 0.1607 ABRules, coarse-grained 0.1564 ABHUMAN-1 0.1540 ABMachine learning, coarse-grained 0.1468 ABHUMAN-2 0.1426 ABMachine learning, fine-grained 0.1584 ABLEAD 0.127 BCRANDOM 0.0956 Cproduced summaries.
In particular, placing the rule-based coarse-grained summariesand the model ones in the same group directly contradicts the results reported inSection 6.2?that people find the model summaries far superior to this particular typeof summary produced by our summarizer.We interpret these results as suggesting that the ROUGE-2 and ROUGE-SU4 scoresare not well suited for evaluating indicative summaries of short stories.
An explana-tion could be that when people summarize fiction?rather than newswire or scientificpapers?they seem to use fewer sentences and clauses verbatim and, by and large, in-troduce more generalization and abstraction.
(We have made this informal observationwhen processing the model summaries used in this experiment.)
This results in littlelexical overlap with the source text and hence with extractive summaries of any flavor.This hypothesis, however, is only preliminary and requires further investigation.1818 It is possible that significantly increasing the number of model summaries would alleviate the problem.Unfortunately, obtaining so many model summaries was prohibitively expensive in our case.
To move inthis direction, we ran ROUGE without jackknifing to enable the use of two model summaries forcomparison.
The results were similar to those reported in Tables 10 and 11: Only the random summariesare consistently significantly worse than the rest.101Computational Linguistics Volume 36, Number 17.
ConclusionsWe presented an approach to summarizing literary short stories.
The text sum-marization community has not yet seriously explored this genre, except for early sem-inal work on story understanding.
In contrast with the story-understanding systemsproposed in the 1970s and 1980s, our system does not require labor-intensive semanticresources?knowledge-bases and schemas?and it works on real-life stories, namely,short fiction.The summaries that the system produces, limited in scope, are intended to helpreaders form adequate expectations about the original story.
We have demonstratedthat such summaries can be produced without deep semantic resources, only relyingon syntax and the information about important entities in the story.
According to thejudges who evaluated the summaries, our summaries are somewhat useful for theiroriginal purpose, even if their quality falls far short of the quality of manual abstracts.Our summaries appear better than the naive baselines and than two state-of-the-artsummarizers fine-tuned for working with newswire.In the course of this work we have made a number of observations about automaticsummarization of short stories.
First of all, we confirmed informally that characterstend to be a central element of short fiction.
Character mentions provide a wealthof information that can be leveraged in automatic summarization.
This finding wasalso reflected in the approach proposed by Lehnert (1982).
In addition, it appears thatposition in text is important, as can be seen from the analysis of the usefulness offeatures in Section 5.3.
Besides, relatively high performance of the lead baselines alsosuggests that position in text is a good indicator of salience, even though it plays alesser role than in more structured documents.We view this work as a small step towards creating tools for searching, summa-rizing, and otherwise processing fiction available electronically.
The current systemaccomplishes with some success a limited task of producing indicative summaries orshort stories, but much more work is needed to create high-quality flexible summaries ofliterary works suitable for more than one purpose.
Perhaps the most obvious extensionto the current system would be summarizing the plot of short stories.
Although this isnot useful given our original criterion (forming adequate expectations about the story,without ?spoilers?
), the ability to handle plot would allow the creation of differenttypes of summaries.
We also hope to explore the possibility of establishing structurewithin stories: Knowing that certain portions of a story lay out the setting whileothers describe events or the culmination would be a significant step towards bettersummarization.Evaluation of summaries of literary work is yet another dimension of the task thatneeds to be considered.
We have concentrated thus far on summary production ratherthan on establishing the criteria that define the quality of the summary.
Evaluationof summaries remains an issue even where well-structured factual documents areconcerned.
In fiction, it is far less clear what contributes towards the quality of thesummary: The facts, for instance, are likely to be less important than in scientific papersor news items.
Other candidate qualities may include closeness to the language or thetone of the original story, the information about the author, the time period, or ideologybehind a certain work of fiction.
This remains an open question, the answer to whichmay well lie outside the field of computational linguistics.102Kazantseva and Szpakowicz Summarizing Short StoriesAppendix A: Features Used in the Coarse- and the Fine-GrainedClause RepresentationsThe appendix lists features computed to represent a clause in the fine-grained data set(Table 12) and in the coarse-grained data set (Table 13).
Prior to constructing featurevectors, the stories are parsed with the Connexor Machinese Parser.
All syntactic infor-mation is computed on the basis of the parser output.
The ?Category?
column showswhether a feature is character-related (C), location-related (L), aspect-related (A ), orother (O).
LCS refers to the database of Lexical Conceptual Structures (Dorr and Olsen1997).Features representing a clause in the fine-grained data set.Table 12Name Category Possible Description Defaultvalues valuechar if ind obj C yes, no yes if the clause contains a mentionof a character and its grammaticalfunction is indirect objectnochar if obj C yes, no yes if the clause contains a mentionof a character and its grammaticalfunction is direct objectnochar if subj C yes, no yes if the clause contains a mentionof a character and its grammaticalfunction is subjectnochar in sent C yes, no yes if the parent sentence contains amention of a characternochar indef C def, indef def if the clause contains a mentionof a character and a) it is a propername or b) it is modified by adefinite determiner or a pronoun;indef if the mention is modified byan indefinite determinern/achar is attr C yes, no yes if the mention of a character is inthe genitive casen/achar mention C yes, no yes if the clause contains a mentionof a characternochar modified C yes, no yes if the mention of a character ismodified by a noun phrasen/achar pronoun C 1st, 3rd 1st if the clause contains apronominal mention of a characterand it is in 1st person (e.g., I); 3rd ifthe pronominal mention is in 3rdperson (e.g., he)n/anbr afterfirst mentionC continuous an integer that reflects the differencebetween the index of the currentsentence and the sentence where thecharacter is first mentioned (it isonly defined for clauses containingmentions of characters)-1loc in prep L yes, no yes if the clause contains a mentionof a location and is embedded in aprepositional clauseno103Computational Linguistics Volume 36, Number 1Table 12(continued)Name Category Possible Description Defaultvalues valueloc present L yes, no yes if the clause contains a mentionof a locationnodurative A yes, no yes if the main verb of the clause isdurative; this information iscomputed using LCSnodynamic A yes, no yes if the main verb of the clause isdynamic; this information iscomputed using LCSnomodal A can, could,shall, should,would, must,may, might,dare, need, will,ought, cansta modal verb from the list, if itappears in the clausen/aneg A yes, no yes if the main verb of the clause isnegatednoobj def A yes, no no if the direct object of the mainverb is modified by an indefinitedeterminer; yes in all other caseswhere a direct object is presentn/aobj plur A yes, no yes if the direct object of the verb isin plural; no in all other cases wherea direct object is presentn/apassive A yes, no yes if the clause is realized in passivevoicenoperf A yes, no yes if the clause is realized in aperfect tensenoprogr A yes, no yes if the clause is realized in aprogressive tensenotelic A yes, no yes if the main verb of the clause istelic; this information is computedusing LCSnotense A past, present,futurethe tense used in the clause n/atmp magn A min, hour, day,week, month,year, year plusthe magnitude of the core temporalunit in the expression (defined forclauses containing temporalexpressions and assigned using a setof manually designed templates):min if the core unit denotes a periodof no more than a minute (e.g., in afew seconds, that moment); hour if itdenotes a period of no more than anhour (e.g., during those hours, at10 am); the values day through yearare assigned analogously, andyear plus denotes periods longerthan a year (e.g., for decades)n/atmp plur A yes, no yes if the core temporal unit in theexpression is in plural (e.g., duringthose years), no if it is singular(e.g., that day); defined for clausescontaining temporal expressionsn/a104Kazantseva and Szpakowicz Summarizing Short StoriesTable 12(continued)Name Category Possible Description Defaultvalues valuetmp type A location,duration,frequency,enactment,temporalmannerthe type of the expression (definedfor clauses containing temporalexpressions, and assigned using aset of manually designed templates):all values except temporal manner areassigned according to theclassification of temporalexpressions available in thelinguistic literature (Harkness 1987),for example today (location), duringthose hours (duration), every day(frequency), never (enactment);temporal manner is a separatepseudo-category defined to includeexpressions such as immediately,instantly etc.n/aclause type O assertive,imperative,infinitive,subjunctivethe form of the main verb in theclause as output by the parser:imperative for clauses realized in theimperative mood, subjunctive forthose realized in subjunctive,infinitive for infinitival clauses (e.g.,He decided to go), assertive otherwiseassertivenbr of sent O continuous the index of the parent sentence intext-1sent type O exclaim,question,assertexclaim for clauses that areexclamations, question for thosethat are questions, and assert for allothersassertFeatures representing a clause in the coarse-grained data set.Table 13Name Category Possible Description Defaultvalues valuechar in clause C yes, no yes if the clause contains a mentionof a characternois subj obj C yes, no yes if the clause contains a mentionof a character and its grammaticalfunction is subject or direct objectnomodified by np C yes, no yes if the mention of a character ispresent in the clause and it ismodified by a noun phrasen/anbr afterfirst mentionC continuous an integer that reflects the differencebetween the index of the currentsentence and the sentence where thecharacter is first mentioned (onlydefined for clauses containingmentions of characters)-1105Computational Linguistics Volume 36, Number 1Table 13(continued)Name Category Possible Description Defaultvalues valueloc in prep L yes, no yes if the clause contains a mentionof a location embedded in aprepositional clausenoloc present L yes, no yes if the clause contains a mentionof a locationnodefault aspect A state, activity,accomp, achievedefault lexical aspect of the mainverb in the clause; computedaccording to the privative modeldefined in (Dorr and Olsen 1997)n/ahas modal A yes, no yes if the clause contains a modalverbnopast perfect A yes, no yes if the clause is realized in pastperfect tensenopolitenesswith beA yes, no yes if the clause contains one of thefollowing expressions: to be sorry, tobe delighted, to be glad, to be sad; thefeature is designed to help capturepoliteness expressions (e.g., I am gladto see you)nosimple pastpresentA yes, no yes if the clause is realized in simplepresent or past tensenotmp explong durationA no, long, short long if the clause contains a temporalexpression denoting a long period oftime, short if it contains anexpression denoting a short periodof time and no otherwisenois assertiveclauseO yes, no no if the clause is not an assertion yesis assertivesentO yes, no no if the parent sentence is notan assertionyesnbr of sent O continuous the index of the parent sentencein text-1AcknowledgmentsWe are grateful to Connexor Oy andespecially to Atro Voutilainen for permissionto use the Connexor Machinese Syntaxparser free of charge for research purposes.We thank John Conroy and JudithSchlesinger for running CLASSY on our testset, and Andrew Hickl for doing it withGISTexter.
Ana Arregui helped us recruitstudents for the evaluation.
Many thanks tothe annotators, summary writers, and raters,who helped evaluate our summarizer.
Aspecial thank-you goes to the anonymousreviewers for Computational Linguistics for alltheir incisive, insightful, and immenselyhelpful comments.
Support for this workcomes from the Natural Sciences andEngineering Research Council of Canada.ReferencesArtstein, Ron and Massimo Poesio.
2008.Inter-coder agreement for computationallinguistics (survey article).
ComputationalLinguistics, 34(4):555?596.Bartlett, Frederic C. 1932.
Remembering: AStudy in Experimental and Social Psychology.Cambridge University Press, London.Barzilay, Regina and Kathleen R. McKeown.2005.
Sentence fusion for multidocumentnews summarization.
ComputationalLinguistics, 31(3):297?239.Branavan, S. R. K., Pawan Deshpande, andRegina Barzilay.
2007.
Generating atable-of-contents.
In Proceedings of the 45thAnnual Meeting of the Association ofComputational Linguistics, pages 544?551,Prague.106Kazantseva and Szpakowicz Summarizing Short StoriesBy, Thomas.
2002.
Tears in the Rain.
Ph.D.thesis, University of Sheffield.Carenini, Giuseppe, Raymond Ng, andAdam Pauls.
2006.
Multi-documentsummarization of evaluative text.
InProceedings of the 11th Conference of theEuropean Chapter of the Association forComputational Linguistics, pages 305?313,Trento.Charniak, Eugene and Robert Goldman.1988.
A logic for semantic interpretation.In Proceedings of the 26th Annual Meeting ofthe Association for Computational Linguistics,pages 87?94, State University of New Yorkat Buffalo, Buffalo, NY.Chawlar, Nitesh V., Kevin W. Bowyer,Lawrence O.
Hall, and W. PhilipKegelmeyer.
2002.
SMOTE: syntheticminority over-sampling techniques.Journal of Artificial Intelligence Research,16:321?357.Cohen, Jacob.
1960.
A coefficient ofagreement for nominal scales.
Educationaland Psychological Measurement, 20:37?46.Comrie, Bernard.
1976.
Aspect.
CambridgeUniversity Press, London.Conroy, John M., Judith D. Schlesinger, andDiane O. O?Leary.
2007.
CLASSY 2007 atDUC 2007.
In Proceedings of the DocumentUnderstanding Conference 2007, New York.Available at http://duc.nist.gov/pubs/2007papers/ida-umd.final.pdf.Cullingford, R. E. 1978.
Script Application:Computer Understanding of NewspaperStories.
Ph.D. thesis, Department ofComputer Science, Yale University.Cunningham, Hamish, Diana Maynard,Kalina Bontcheva, and Valentin Tablan.2002.
GATE: an Architecture forDevelopment of Robust HLTapplications.
In Proceedings of the 40thAnniversary Meeting of the Association forComputational Linguistics, pages 168?175,Philadelphia, PA.D?
?az, Alberto and Pablo Gerva?s.
2007.User-model based personalizedsummarization.
Information Processing andManagement, 43(6):1715?1734.Dorr, Bonnie J. and Mari Broman Olsen.1997.
Deriving verbal and compositonallexical aspect for NLP applications.
InProceedings of the 35th Annual Meeting of theAssociation for Computational Linguistics and8th Conference of the European Chapter of theAssociation for Computational Linguistics,pages 151?158, Madrid.Dowty, David.
1979.
Word Meaning andMontague Grammar.
D. Reidel PublishingCompany, Dordrecht.Dyer, Michael G. 1983.
In-DepthUnderstanding: A Computer Model ofIntegrated Processing for NarrativeComprehension.
MIT Press,Cambridge, MA.Elhadad, Noemie, Min-Yen Kan, JudithKlavans, and Kathleen McKeown.
2005.Customization in a unified framework forsummarizing medical literature.
Journal ofArtificial Intelligence in Medicine,33(2):179?198.Fellbaum, Christiane.
1998.
WordNet: AnElectronic Lexical Database.
MIT Press,Cambridge, MA.Fuentes, Maria, Edgar Gonza`lez, HoracioRodr?
?gue, Jordi Turmo, and Laura Alonsoi Alemany.
2005.
Summarizingspontaneous speech using general textproperties.
In Proceedings of InternationalWorkshop Crossing Barriers in TextSummarization Research, at Recent Advancesin Natural Language Processing 2005,pages 10?18, Borovetz.Harabagiu, Sandra, Andrew Hickl, andFinley Lacatusu.
2007.
Satisfyinginformation needs with multi-documentsummaries.
Information Processing andManagement, 43(6):1619?1642.Harkness, Janet.
1987.
Time adverbials inEnglish and reference time.
In AlfredSchopf, editor, Essays on Tensing in English,Vol.
I: Reference Time, Tense and Adverbs.Max Niemeyer, Tu?bingen, pages 71?110.Huddleston, Rodney D. and Geoffrey K.Pullum.
2002.
The Cambridge Grammar ofthe English Language.
CambridgeUniversity Press, New York.Kazantseva, Anna.
2006.
Automaticsummarization of short stories.
Master?sthesis, University of Ottawa.
Available atwww.site.uottawa.ca/?ankazant/pubs/thesis.tar.gz.Krahmer, Emiel, Erwin Marsi, and Paul vanPelt.
2008.
Query-based sentence fusion isbetter defined and leads to more preferredresults than generic sentence fusion.
InProceedings of ACL-08: HLT, Short Papers,pages 193?196, Columbus, OH.Krippendorff, Klaus.
2004.
Content Analysis.An Introduction to Its Methodology.
SagePublications, Thousand Oaks, CA.Landis, J. Richards and Garry G. Koch.
1977.The measurement of observer agreementfor categorical data.
Biometrics,33(1):159?174.Lappin, Shalom and Herbert Leass.
1994.An algorithm for pronominal anaphoraresolution.
Computational Linguistics,20(4):535?561.107Computational Linguistics Volume 36, Number 1Leach, Chris.
1979.
Introduction to Statistics.A Nonparametric Approach for the SocialSciences.
John Wiley and Sons, New York.Leake, David.
1989.
Anomaly detectionstrategies for schema-based storyunderstanding.
In Proceedings of theEleventh Annual Conference of theCognitive Science Society, pages 490?497,Ann Arbor, MI.Lehnert, Wendy G. 1982.
Plot units: Anarrative summarization strategy.
InWendy G. Lehnert and Martin H. Ringle,editors, Strategies for Natural LanguageProcessing.
Erlbaum, Hillsdale, NJ,pages 375?414.Lin, Chin-Yew.
2004.
ROUGE: A packagefor automatic evaluation of summaries.In Marie-Francine Moens and StanSzpakowicz, editors, Text SummarizationBranches Out: Proceedings of the ACL-04Workshop, pages 74?81, Barcelona.Lin, Chin-Yew and Eduard Hovy.
2000.
Theautomated acquisition of topic signaturesfor text summarization.
In Proceedings ofthe 18th Conference on ComputationalLinguistics, pages 495?501, Morristown, NJ.Lin, Chin-Yew and Eduard Hovy.
2003.Automatic evaluation of summaries usingn-gram co-occurrence statistics.
In MartiHearst and Mari Ostendorf, editors,HLT-NAACL 2003: Main Proceedings,pages 150?157, Edmonton.Mandler, George.
1987.
Determinants ofrecognition.
In E. van Der Meer andJ.
Hoffman, editors, Knowledge-AidedInformation Processing.
North Holland,Amsterdam.Mani, Indejeet.
2001.
AutomaticSummarization.
John Benjamins B.V.,Amsterdam.Marcu, Daniel.
2000.
The Theory and Practiceof Discourse Parsing and Summarization.The MIT Press, Cambridge, MA.McDonald, Ryan.
2006.
Discriminativesentence compression with soft syntacticevidence.
In Proceedings of the 11thConference of the European Chapter of theAssociation for Computational Linguistics,pages 297?305, Trento.Mei, Qiaozhu and ChengXiang Zhai.
2008.Generating impact-based summariesfor scientific literature.
In Proceedingsof ACL-08: HLT, pages 816?824,Columbus, OH.Merlo, Paola, Suzanne Stevenson, VivianTsang, and Gianluca Allaria.
2002.
Amultilingual paradigm for automaticverb classification.
In Proceedingsof the 40th Annual Meeting of theAssociation for Computational Linguistics,pages 207?214, University ofPennsylvania, Philadelphia, PA.Mihalcea, Rada and Hakan Ceylan.
2007.Explorations in automatic booksummarization.
In Proceedings of the2007 Joint Conference on EmpiricalMethods in Natural Language Processingand Computational Natural LanguageLearning (EMNLP-CoNLL), pages 380?389,Prague.Moens, Marie-Francine.
2007.
Summarizingcourt decisions.
Information Processing andManagement, 43(6):1748?1764.Nenkova, Ani and Rebecca Passonneau.2004.
Evaluating content selection insummarization: The Pyramid Method.In Proceedings of the Human LanguageTechnology Conference and North AmericanChapter of the Association for ComputationalLinguistics Annual Meeting, pages 145?152,Boston, MA.Nomoto, Tadashi.
2007.
Discriminativesentence compression with randomconditional fields.
Information Processingand Management, 43(6):1571?1587.Norvig, Peter.
1989.
Marker passing as aweak method for text inferencing.Cognitive Science, 13(4):569?620.Propp, Vladimir.
1968.
Morphology of theFolk Tale.
Indiana University Press,Bloomington, IN, 2nd edition.Quinlan, J. Ross.
1992.
C4.
5: Programs forMachine Learning.
Morgan Kaufmann,San Mateo, CA.Radev, Dragomir and Daniel Tam.
2003.Summarization evaluation using relativeutility.
In Proceedings of the 12thInternational Conference on Information andKnowledge Management, pages 508?511,New York, NY.Radev, Dragomir R., Simone Teufel, HoracioSaggion, Wai Lam, John Blitzer, Hong Qi,Arda C?elebi, Danyu Liu, and ElliottDrabek.
2003.
Evaluation challenges inlarge-scale document summarization.
InErhard Hinrichs and Dan Roth, editors,Proceedings of the 41st Annual Meeting of theAssociation for Computational Linguistics,pages 375?382, Sapporo.Rath, G. J., A. Resnick, and T. R. Savage.1961.
The formation of abstracts by theselection of sentences.
AmericanDocumentation, 2(12):139?143.Reeve, Lawrence H., Hyoil Han, and Ari D.Brooks.
2007.
The use of domain-specificconcepts in biomedical textsummarization.
Information Processing andManagement, 43(6):1765?1776.108Kazantseva and Szpakowicz Summarizing Short StoriesRumelhart, David E. 1975.
Notes on a schemafor stories.
In Daniel G. Bobrow and AllanCollins, editors, Representation andUnderstanding.
Studies in Cognitive Science,pages 221?237.
Academic Press, New York.Salton, Gerard, Amit Singhal, Mandar Mitra,and Chris Buckley.
1997.
Automatic textstructuring and summarization.Information Processing and Management,33(2):193?207.Schlesinger, Judith D., Dianne P. O?Leary,and John M. Conroy.
2008.
Arabic/Englishmulti-document summarization withCLASSY - The past and the future.
InComputational Linguistics and Intelligent TextProcessing, 9th International Conference,CICLing 2008, pages 568?581, Haifa.Scott, William.
1955.
Reliability of contentanalysis: The case of nominal scale coding.Public Opinion Quarterly, 19(3):321?325.Siegel, Eric V. 1998a.
Disambiguating verbswith the WordNet category of the directobject.
In Usage of WordNet in NaturalLanguage Processing Systems Workshop,pages 9?15, Universite?
de Montre?al.Siegel, Eric V. 1998b.
Linguistic Indicators forLanguage Understanding: Using MachineLearning Methods to Combine Corpus-BasedIndicators for Aspectual Classification ofClauses.
Ph.D. thesis, Columbia University,New York.Siegel, Sidney and John.
N. Castellan, Jr.1988.
Nonparametric Statistics for theBehavioral Sciences.
McGraw Hill,Boston, MA.Soricut, Radu and Daniel Marcu.
2007.Abstractive headline generation usingwild-expressions.
Information Processingand Management, 43(6):1536?1548.Tapanainen, Pasi and Timo Ja?rvinen.
1997.A non-projective dependency parser.Proceedings of the 5th Conference on AppliedNatural Language Processing, pages 64?71,Washington, DC.Teufel, Simone and Marc Moens.
2002.Summarizing scientific articles:Experiments with relevance and rhetoricalstatus.
Computational Linguistics,28(4):409?445.Thorndyke, Perry W. 1975.
CognitiveStructures in Human Story Comprehensionand Memory.
Ph.D. thesis, StanfordUniversity.van Dijk, Teun A.
1980.
Macrostructures.
AnInterdisciplinary Study of Global Structuresin Discourse, Interaction, and Cognition.Laurence Erlbaum Associates,Hillsdale, NJ.van Dijk, Teun A. and Walter Kintsch.
1978.Cognitive psychology and discourse:Recalling and summarizing stories.
InWolfgang U. Dressler, editor, CurrentTrends in Textlinguistics.
Walter de Gruyter,New York, pages 61?79.van Halteren, Hans and Simone Teufel.2003.
Examining the consensus betweenhuman summaries: initial experimentswith factoid analysis.
In DragomirRadev and Simone Teufel, editors,HLT?NAACL 2003 Workshop: TextSummarization (DUC03), pages 57?64,Edmonton.Vendler, Zeno.
1967.
Linguistics in Philosophy.Cornell University Press, Ithaca, NY.Vieira, Renata and Massimo Poesio.
2000.
Anempirically based system for processingdefinite descriptions.
ComputationalLinguistics, 26(4):539?593.Zechner, Klaus.
2002.
Automaticsummarization of open-domainmultiparty dialogues in diverse genres.Computational Linguistics, 28(4):447?485.109
