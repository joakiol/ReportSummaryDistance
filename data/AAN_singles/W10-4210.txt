Natural Reference to Objects in a Visual DomainMargaret MitchellComputing Science Dept.University of AberdeenScotland, U.K.Kees van DeemterComputing Science Dept.University of AberdeenScotland, U.K.{m.mitchell, k.vdeemter, e.reiter}@abdn.ac.ukEhud ReiterComputing Science Dept.University of AberdeenScotland, U.K.AbstractThis paper discusses the basic structuresnecessary for the generation of referenceto objects in a visual scene.
We constructa study designed to elicit naturalistic re-ferring expressions to relatively complexobjects, and find aspects of reference thathave not been accounted for in work onReferring Expression Generation (REG).This includes reference to object parts,size comparisons without crisp measure-ments, and the use of analogies.
By draw-ing on research in cognitive science, neu-rophysiology, and psycholinguistics, webegin developing the input structure andbackground knowledge necessary for analgorithm capable of generating the kindsof reference we observe.1 IntroductionOne of the dominating tasks in Natural LanguageGeneration (NLG) is the generation of expressionsto pick out a referent.
In recent years there hasbeen increased interest in generating referentialexpressions that are natural, e.g., like those pro-duced by people.
Although research on the gener-ation of referring expressions has examined differ-ent aspects of how people generate reference, therehas been surprisingly little research on how peoplerefer to objects in a real-world setting.
This paperaddresses this issue, and we begin formulating therequirements for an REG algorithm that refers tovisible three-dimensional objects in the real world.Reference to objects in a visual domain pro-vides a straightforward extension of the sorts ofreference REG research already tends to consider.Toy examples outline reference to objects, peo-ple, and animals that are perceptually availablebefore the speaker begins generating an utterance(Dale and Reiter, 1995; Krahmer et al, 2003; vanDeemter et al, 2006; Areces et al, 2008).
Exam-ple referents may be referred to by their color, size,type (?dog?
or ?cup?
), whether or not they have abeard, etc.Typically, the reference process proceeds bycomparing the properties of the referent with theproperties of all the other items in the set.
Thefinal expression roughly conforms to the Griceanmaxims (Grice, 1975).However, when the goal is to generate naturalreference, this framework is too simple.
The formreference takes is profoundly affected by modality,task, and audience (Chapanis et al, 1977; Cohen,1984; Clark and Wilkes-Gibbs, 1986), and evenwhen these aspects are controlled, different peoplewill refer differently to the same object (Mitchell,2008).
In light of this, we isolate one kind of nat-ural reference and begin building the algorithmicframework necessary to generate the observed lan-guage.Psycholinguistic research has examined refer-ence in a variety of settings, which may informresearch on natural REG, but it is not always clearhow to extend this work to a computational model.This is true in part because these studies favor ananalysis of reference in the context of collabora-tion; reference is embedded within language, andlanguage is often a joint activity.
However, mostresearch on referring expression generation sup-poses a solitary generating agent.1 This tacitlyassumes that reference will be taking place in amonologue setting, rather than a dialogue or groupsetting.
Indeed, the goal of most REG algorithmsis to produce uniquely distinguishing, one-shot re-ferring expressions.Studies on natural reference usually use atwo person (speaker-listener) communication task(e.g., Flavell et al, 1968; Krauss and Glucksberg,1969; Ford and Olson, 1975).
This research has1A notable exception is Heeman and Hirst (1995).shown that reference is more accurate and efficientwhen it incorporates things like gesture and gaze(Clark and Krych, 2004).
There is a trade-off ineffort between initiating a noun phrase and refash-ioning it so that both speakers understand the ref-erent (Clark and Wilkes-Gibbs, 1986), and speak-ers communicate to form lexical pacts on howto refer to an object (Sacks and Schegloff, 1979;Brennan and Clark, 1996).
Mutual understandingof referents is achieved in part by referring withina subset of potential referents (Clark et al, 1983;Beun and Cremers, 1998).
A few studies havecompared monologue to dialogue reference, andhave shown that monologue references tend to beharder for a later listener to disambiguate (Clarkand Krych, 2004) and that subsequent referencestend to be longer than those in dialogues (Kraussand Weinheimer, 1967).Aiming to generate natural reference in a mono-logue setting raises questions about what an algo-rithm should use to produce utterances like thoseproduced by people.
In a monologue setting, thespeaker (or algorithm) gets no feedback from thelistener; the speaker?s reference is not tied to in-teractions with other participants.
The speakeris therefore in a difficult position, attempting toclearly convey a referent without being able tocheck if the reference is understood along the way.Recent studies that have focused on monologuereference do so rather explicitly, which may af-fect participant responses.
These studies utilize2D graphical depictions of simple 3D objects (vanDeemter et al, 2006; Viethen and Dale, 2008),where a small set of properties can be used to dis-tinguish one item from another.
The expressionsare elicited in isolation, typed and then submitted,which may hide some of the underlying referen-tial processes.
None of these studies utilize actualobjects.
It is therefore difficult to use these datato draw conclusions about how reference works innaturalistic settings.
It is unclear if these experi-mental settings are natural enough, i.e., if they getat reference as it may occur every day.The study in this paper attempts to bring out in-formation about reference in a number of ways.First, we conduct the study in-person, using real-world objects.
This design invites referential phe-nomena that may not have been previously ob-served in simpler domains.
Second, the refer-ring expressions are produced orally.
This allowsus access to reference as it is generated, withoutthe participants revising and so potentially obscur-ing information about their reference.
Third, weuse a relatively complicated task, where partici-pants must explain how to use pieces to put to-gether a picture of a face.
The fact that we arelooking at reference is not made explicit, whichlessens any experimental effects caused by sub-jects guessing the purpose of the study.
This ap-proach also situates reference within a larger task,which may draw out aspects of reference not usu-ally seen in experiments that elicit reference in iso-lation.
Fourth, the objects used display a varietyof different features: texture, material, color, sizealong several dimensions, etc.
This brings the dataset closer to objects that people interact with everyday.
A monologue setting offers a picture of thephenomena at play during a single individual?s re-ferring expression generation.The referring expressions gathered in this studyexhibit several aspects of reference that have notyet been addressed in REG.
This includes (1) part-whole modularity; (2) size comparisons acrossthree dimensions; and (3) analogies.
Work in cog-nitive sciences suggests that these phenomena areinterrelated, and may be possible to represent in acomputational framework.
This research also of-fers connections to further aspects of natural refer-ence that were not directly observed in the study,but will need to be accounted for in future work onnaturalistic referring expression generation.
Us-ing these ideas, we begin formulating the struc-tures that an REG algorithm would need in orderto produce reference to real-world objects in a vi-sual setting.Approaching REG in this way allows us to tieresearch in the generation of referring expressionsto computational models of visual perception andcognitively-motivated computer vision.
Moving inthis direction offers the prospect of eventually de-veloping an application for the generation of nat-ural reference to objects automatically recognizedby a computer vision system.In the next section, we describe our study.
InSection 3, we analyze the results and discuss whatthey tell us about natural reference.
In Section 4,we draw on our results and cognitive models of ob-ject recognition to begin building the frameworkfor a referring expression algorithm that generatesnaturalistic reference to objects in a visual scene.In Section 5, we offer concluding remarks and out-line areas for further study.Figure 1: Object Board.2 Method2.1 SubjectsThe subjects were 20 residents of Aberdeen, Scot-land, and included undergraduates, graduates, andprofessionals.
All were native speakers of English,had normal or corrected vision, and had no otherknown visual issues (such as color-blindness).Subjects were paid for their participation.
Tworecordings were left out of the analysis: one par-ticipant?s session was not fully recorded due to asoftware error, and one participant did not pick outmany objects in each face and so was not included.The final set of participants included 18 people, 10female and 8 male.2.2 MaterialsA board was prepared with 51 craft objects.
Theobjects were chosen from various craft sets, andincluded pom-poms, pipe-cleaners, beads, andfeathers (see Table 1).
The motley group of objectshad different colors, textures, shapes, patterns, andwere made of different materials.
Similar objectswere grouped together on the board, with a labelplaced underneath.
This was done to control thehead noun used in each reference.
The objectswere used to make up 5 different craft ?face?
pic-tures.
Subjects sat at a desk facing the board andthe stack of pictures.
A picture of the board isshown in Figure 1.Subjects were recorded on a head-mounted mi-crophone, which fed directly into a laptop placedon the left of the desk.
The open-source audio-recording program Audacity (Mazzoni, 2010) wasused to record the audio signal and export it towave format.2.3 ProcedureSubjects were told to give instructions on how toconstruct each face using the craft supplies on theboard.
They were instructed to be clear enough fora listener to be able to reconstruct each face with-out the pictures, with only the board items in frontof them.
A pilot study revealed that such open-ended instructions left some subjects spending aninordinate amount of time on the exact placementof each piece, and so in the current study sub-jects were told that each face should take ?a cou-ple?
minutes, and that the instructions should beas clear as possible for a listener to use the sameobjects in reconstructing the pictures without be-ing ?overly concerned?
with the details of exactlyhow each piece is angled in relation to the other.Subjects were first given a practice face to de-scribe.
This face was the same face for all subjects.They were then allowed to voice any concerns orask questions, but the experimenter only repeatedportions of the original instructions; no new infor-mation was given.
The subject could then proceedto the next four faces, which were in a random or-der for each subject.
A transcript of a single facefrom a session is provided in Figure 2.2.4 AnalysisThe recordings of each monologue were tran-scribed, including disfluencies, and each face sec-tion (?eyes?, ?chin?, etc.)
was marked.
First refer-ence to items on the board were annotated withtheir corresponding item numbers, yielding 722references.2 Initial references to single objectswere extracted, creating a final data set with 505references to single objects.3 ResultsEach reference was annotated in terms of the prop-erties used to pick out the referent.
For exam-ple, ?the red feather?
was annotated as contain-ing the <ATTRIBUTE:value> pairs <COLOR:red,TYPE:feather>.
Discerning properties from themodifiers used in reference is generally straight-forward, and all of the references produced maybe partially deconstructed using such properties.2This corpus is available athttp://www.csd.abdn.ac.uk/?mitchema/craft corpus.14 foam shapes 2 large red hearts 2 small red hearts 2 small neon green hearts2 small blue hearts 1 small green heart 1 green triangle 1 red circle1 red square 1 red rectangle 1 white rectangle11 beads 4 large round wooden beads 2 small white plastic beads 2 brown patterned beads1 gold patterned bead 1 shiny gold patterned heart 1 red patterned heart9 pom poms 2 big green pom-poms 2 small neon green pom-poms 2 small silver pom-poms1 small metallic green pom-pom 1 large white pom-pom 1 medium white pom-pom8 pipe cleaners 1 gold pipe-cleaner 1 gold pipe-cleaner in half 1 silver pipe-cleaner1 circular neon yellow soft pipe-cleaner 1 neon orange puffy pipe-cleaner 1 grey puffy pipe-cleaner1 purple/yellow striped pipe-cleaner 1 brown/grey striped pipe-cleaner5 feathers 2 purple feathers 2 red feathers 1 yellow feather3 ribbons 1 gold sequined wavy ribbon 1 silver wavy ribbon 1 small silver wavy ribbon1 star 1 gold starTable 1: Board items.<CHIN> Okay so this face again um this face has um uhfor the chin, it uses (10 a gold pipe-cleaner in a V shape)where the bottom of the V is the chin.
</CHIN><MOUTH> The mouth is made up of (9 a purple feather).And the mouth is slightly squint, um as if the the personis smiling or even smirking.
So this this smile is almostoff to one side.
</MOUTH><NOSE> The nose is uh (5 a wooden bead, a medium-sized wooden bead with a hole in the center).
</NOSE><EYES> And the eyes are made of (2,3 white pom-poms),em just uh em evenly spaced in the center of the face.</EYES><FOREHEAD> Em it?s see the person?s em top of the per-son?s head is made out of (1 another, thicker pipe-cleanerthat?s uh a grey color, it?s kind of uh a knotted blue-typepipe-cleaner).
So that that acts as the top of the person?shead.
</FOREHEAD><HAIR> And down the side of the person?s face, there are(7,8 two ribbons) on each side.
(7,8 And those are silverribbons).
Um and they just hang down the side of the faceand they join up the the grey pipe-cleaner and the top umof the person?s head to the to the chin and then hang downeither side of the chin.
</HAIR><EARS> And the person?s ears are made up of (4,6 twobeads, which are um love-heart-shaped beads), where thepoints of the love-hearts are facing outwards.
And thoseare just placed um around same em same em horizontalline as the nose of the person?s face is.
</EARS>Figure 2: Excerpt Transcript.Using sets of properties to distinguish referentsis nothing new in REG.
Algorithms for the genera-tion of referring expressions commonly use this asa starting point, proposing that properties are orga-nized in some linear order (Dale and Reiter, 1995)or weighted order (Krahmer et al, 2003) as input.However, we find evidence that more is at play.
Abreakdown of our findings is listed in Table 2.3.1 Spatial ReferenceIn addition to properties that pick out referents,throughout the data we see reference to objectsas they exist in space.
Size is compared acrossdifferent dimensions of different objects, and ref-erence is made to different parts of the objects,picking out pieces within the whole.
These twophenomena ?
relative size comparisons and part-whole modularity ?
point to an underlying spatialobject representation that may be utilized duringreference.3.1.1 Relative Size ComparisonsA total of 122 (24.2%) references mention sizewith a vague modifier (e.g., ?big?, ?wide?).
Thisincludes comparative (e.g, ?larger?)
and superla-tive (e.g., ?largest?)
size modifiers, which occur 40(7.9%) times in the data set.
Examples are givenbelow.
(1) ?the bigger pom-pom?
(2) ?the green largest pom-pom?
(3) ?the smallest long ribbon?
(4) ?the large orange pipe-cleaner?Of the references that mention size, 35 (6.9%)use a vague modifier that applies to one or two di-mensions.
This includes modifiers for height (?theshort silver ribbon?
), width (?quite a fat rectan-gle?
), and depth (?the thick grey pipe-cleaner?
).87 (17.2%) use a modifier that applies to the over-all size of the object (e.g., ?big?
or ?small?).
Table3 lists these values.
Crisp measurements (such as?1 centimeter?)
occur only twice (0.4%), with bothproduced by the same participant.Comparative/Superlative: 40 (7.9%)Base: 82 (16.2%)Height/Width/Depth: 35 (6.9%)Overall size: 87 (17.2%)Table 3: Size Modifier Breakdown.Part-whole modularity Relative size Analogies?a green pom-pom.
.
.
?a red foam-piece.
.
.
?a natural-looking piecewith the tinsel on the outside?
which is more square of pipe-cleaner, it looks?your gold twisty ribbon.
.
.
in shape rather than a bit like a rope?with sequins on it?
the longer rectangle?
?a pipe-cleaner that?a wooden bead.
.
.
?the grey pipe-cleaner.
.
.
looks a bit like.
.
.with a hole in the center?
which is the thicker one.
.
.
a fluffy caterpillar?
?one of the green pom-poms.
.
.
?the slightly larger one?
?the silver ribbonwith the sort of strands ?the smaller silver ribbon?
that?s almost likecoming out from it.?
?the short silver ribbon?
a big S shape.?
?the silver ribbon.
.
.
with the chainmail ?quite a fat rectangle?
?a.
.
.
pipe-cleanerdetail down through the middle of it.?
?thick grey pipe-cleaner?
that looks like tinsel.
?11 References 122 References 16 ReferencesTable 2: Examples of Observed Reference.Participants produce such modifiers withoutsizes or measurements explicitly given; with aninput of a visual object presentation, the outputincludes size modifiers.
Such data suggests thatnatural reference in a visual domain utilizes pro-cesses comparing the length, width, and height ofa target object with other objects in the set.
Indeed,5 references (1.0%) in our data set include explicitcomparison with the size of other objects.
(5) ?a red foam-piece.
.
.
which is more square inshape rather than the longer rectangle?
(6) ?the grey pipe-cleaner.
.
.
which is the thickerone.
.
.
of the selection?
(7) ?the shorter of the two silver ribbons?
(8) ?the longer one of the ribbons?
(9) ?the longer of the two silver ribbons?In Example (5), height and width across twodifferent objects are compared, distinguishing asquare from a rectangle.
In (6) ?thicker?
marksthe referent as having a larger circumference thanother items of the same type.
(7) (8) and (9) com-pare the height of the target referent to the heightof similar items.The use of size modifiers in a domain withoutspecified measurements suggests that when peo-ple refer to an object in a visual domain, theyare sensitive to its size and structure within a di-mensional, real-world space.
Without access tocrisp measurements, people compare relative sizeacross different objects, and this is reflected in theexpressions they generate.
These comparisons arenot only limited to overall size, but include sizein each dimension.
This suggests that objects?structures within a real-world space are relevantto REG in a visual domain.3.1.2 Part-Whole ModularityThe role that a spatial object understanding haswithin reference is further detailed by utterancesthat pick out the target object by mentioning an ob-ject part.
11 utterances (2.2%) in our data includemention of an object part within reference to thewhole object.
This is spread across participants,such that half of the participants make referenceto an object part at least once.
(10) ?a green pom-pom, which is with the tinselon the outside?
(11) ?your gold twisty ribbon...with sequins onit?
(12) ?a wooden bead...with a hole in the center?In (10), pieces of tinsel are isolated from thewhole object and specified as being on the outside.In (11), smaller pieces that lay on top of the ribbonare picked out.
And in (12), a hole within the beadis isolated.The use of part-whole modularity suggests anunderstanding that parts of the object take up theirown space within the object.
An object is not onlyviewed as a whole during reference, but parts in,on, and around it may be considered as well.
Foran REG algorithm to generate these kinds of ref-erences, it must be provided with a representationthat details the structure of each object.3.2 ANALOGIESThe data from this study also provide informationon what can be expected from a knowledge basein an algorithm that aims to generate naturalisticreference.
Reference is made 16 times (3.2%) toobjects not on the board, where the intended refer-ent is compared against something it is like.
Someexamples are given below.
(13) ?a gold.
.
.
pipe-cleaner.
.
.
completelystraight, like a ruler?
(14) ?a natural-looking piece of pipe-cleaner, itlooks a bit like a rope?
(15) ?a pipe-cleaner that looks a bit like.
.
.
afluffy caterpillar.
.
.
?In (13), a participant makes reference to aSHAPE property of an object not on the board.
In(14) and (15), participants refer to objects that mayshare a variety of properties with the referent, butare also not on the board.Reference to these other items do not pick outsingle objects, but types of objects (e.g., an objecttype, not token).
They correspond to some pro-totypical idea of an object with properties similarto those of the referent.
Work by Rosch (1975)has examined this tendency, introducing the ideaof prototype theory, which proposes that there maybe some central, ?prototypical?
notions of items.
Aknowledge base with stored prototypes could beutilized by an REG algorithm to compare the tar-get referent to item prototypes.
Such representa-tions would help guide the generation of referenceto items not in the scene, but similar to the targetreferent.4 DiscussionWe have discussed several different aspects of ref-erence in a study where referring expressions areelicited for objects in a spatial, visual scene.
Ref-erence in this domain draws on object forms asthey exist in a three-dimensional space and uti-lizes background knowledge to describe referentsby analogy to items outside of the scene.
Thisis undoubtedly not an exhaustive account of thephenomena at play in such a domain, but offerssome initial conclusions that may be drawn fromexploratory work of this kind.Before continuing with the discussion, it isworthwhile to consider whether some of our datamight be seen as going beyond reference.
Perhapsthe participants are doing something else, whichcould be called describing.
How to draw the linebetween a distinguishing reference and a descrip-tion, and whether such a line can be drawn at all, isan interesting question.
If the two are clearly dis-tinct, then both are interesting to NLG research.If the two are one in the same, then this shedssome light on how REG algorithms should treatreference.
We leave a more detailed discussion ofthis for future work, but note recent psycholinguis-tic work suggesting that referring establishes (1)an individual as the referent; (2) a conceptualiza-tion or perspective on that individual (Clark andBangerter, 2004).
Schematically, referring = indi-cating + describing.We now turn to a discussion of how the ob-served phenomena may be best represented in anREG algorithm.
We propose that an algorithm ca-pable of generating natural reference to objects ina visual scene should utilize (1) a spatial objectrepresentation; (2) a non-spatial feature-based rep-resentation; and (3) a knowledge base of objectprototypes.4.1 Spatial and Visual PropertiesIt is perhaps unsurprising to find reference that ex-hibits spatial knowledge in a study where objectsare presented in three-dimensional space.
Hu-man behavior is anchored in space, and spatial in-formation is essential for our ability to navigatethe world we live in.
However, referring expres-sion generation algorithms geared towards spa-tial representations have oversimplified this ten-dency, keeping objects within the realm of two-dimensions and only looking at the spatial rela-tions between objects.For example, Funakoshi et al (2004) and Gatt(2006) focus on how objects should be clusteredtogether to form groups.
This utilizes some ofthe spatial information between objects, but doesnot address the spatial, three-dimensional natureof objects themselves.
Rather, objects exist as en-tities that may be grouped with other entities in aset or singled out as individual objects; they do nothave their own spatial characteristics.
Similarly,one of the strengths of the Graph-Based Algorithm(Krahmer et al, 2003) is its ability to generate ex-pressions that involve relations between objects,and these include spatial ones (?next to?, ?on topof?, etc.).
In all these approaches, however, ob-jects are essentially one-dimensional, representedas individual nodes.Work that does look at the spatial informationof different objects is provided by Kelleher et al(2005).
In this approach, the overall volume ofeach object is calculated to assign salience rank-ings, which then allow the Incremental Algorithm(Dale and Reiter, 1995) to produce otherwise ?un-derspecified?
reference.
The spatial properties ofthe objects are kept relatively simple.
They arenot used in constructing the referring expression,but one aspect of the object?s three-dimensionalshape (volume) affects the referring expression?sfinal form.
To the authors?
knowledge, the cur-rent work is the first to suggest that objects them-selves should have their spatial properties repre-sented during reference.Research in cognitive modelling supports theidea that we attend to the spatial properties of ob-jects when we view them (Blaser et al, 2000), andthat we have purely spatial attentional mechanismsoperating alongside non-spatial, feature-based at-tentional mechanisms (Treue and Trujillo, 1999).These feature-based attentional mechanisms pickout properties commonly utilized in REG, such astexture, orientation, and color.
They also pick outedges and corners, contrast, and brightness.
Spa-tial attentional mechanisms provide informationabout where the non-spatial features are located inrelation to one another, size, and the spatial inter-relations between component parts.Applying these findings to our study, an REGalgorithm that generates natural reference shouldutilize a visual, feature-based representation of ob-jects alongside a structural, spatial representationof objects.
A feature-based representation is al-ready common to REG, and could be representedas a series of <ATTRIBUTE:value> pairs.
A spa-tial representation is necessary to define how theobject is situated within a dimensional space, pro-viding information about the relative distances be-tween object components, edges, and corners.With such information provided by a spatialrepresentation, the generation of part-whole ex-pressions, such as ?the pom-pom with the tinsel onthe outside?, is possible.
This also allows for thegeneration of size modifiers (?big?, ?small?)
with-out the need for crisp measurements, for example,by comparing the difference in overall height ofthe target object with other objects in the scene, oragainst a stored prototype (discussed below).
Rel-ative size comparisons across different dimensionswould also be possible, used to generate size mod-ifiers such as ?wide?
and ?thick?
that refer to onedimensional axis.4.2 AnalogiesA feature-based and a spatial representation mayalso play a role in analogies.
When we use analo-gies, as in ?the pipe-cleaner that looks like a cater-pillar?, we use world knowledge about items thatare not themselves visible.
Such an expressiondraws on similarity that does not link the referentwith a particular object, but with a general type ofobject: the pipe-cleaner is caterpillar-like.To generate these kinds of expressions, an REGalgorithm would first need a knowledge base withprototypes listing prototypical values of attributes.For example, a banana prototype might have a pro-totypical COLOR of yellow.
With prototypes in theknowledge base, the REG algorithm would needto calculate similarity of a target referent to otherknown items.
This would allow a piece of yellowcloth, for example, to be described as being thecolor of a banana.Implementing such similarity measures in anREG algorithm will be challenging.
One difficultyis that prototype values may be different depend-ing on what is known about an item; a prototypicalunripe banana may be green, or a prototypical rot-ten banana brown.
Another difficulty will be indetermining when a referent is similar enough toa prototype to warrant an analogy.
Additional re-search is needed to explore how these propertiescan be reasoned about.4.3 Further ImplicationsA knowledge base containing prototypes opens upthe possibility of generating many other kinds ofnatural references.
In particular, such knowledgewould allow the algorithm to compute which prop-erties a given kind of referent may be expectedto have, and which properties may be unexpected.Unexpected properties may therefore stand out asparticularly salient.For example, a dog missing a leg may be de-scribed as a ?three-legged dog?
because the pro-totypical dog has four legs.
We believe that thisperspective, which hinges on the unexpectednessof a property, suggests a new approach to at-tribute selection.
Unlike the Incremental Algo-rithm, the Preference Order that determines the or-der in which attributes are examined would not befixed, but would depend on the nature of the refer-ent and what is known about it.Approaching REG in this way follows work incognitive science and neurophysiology that sug-gests that expectations about objects?
visual andspatial characteristics are derived from stored rep-resentations of object ?prototypes?
in the infe-rior temporal lobe of the brain (Logothetis and- A spatial representation (depicting size, inter-relations between component parts)- A non-spatial, propositional representation(describing color, texture, orientation, etc.
)- A knowledge base with stored prototypical ob-ject propositional and spatial representationsTable 4: Requirements for an REG algorithm thatgenerates natural reference to visual objects.Sheinberg, 1996; Riesenhuber and Poggio, 2000;Palmeri and Gauthier, 2004).
Most formal theo-ries of object perception posit some sort of cate-gory activation system (Kosslyn, 1994), a systemthat matches input properties of objects to thoseof stored prototypes, which then helps guide ex-pectations about objects in a top-down fashion.3This appears to be a neurological correlate of theknowledge base we propose to underlie analogies.Such a system contains information about pro-totypical objects?
component parts and where theyare placed relative to one another, as well as rele-vant values for material, color, etc.
This suggeststhat the spatial and non-spatial feature-based rep-resentations proposed for visible objects could beused to represent prototype objects as well.
In-deed, how we view and refer to objects appears tobe influenced by the interaction of these structures:Expectations about an object?s spatial propertiesguide our attention towards expected object partsand non-spatial, feature-based properties through-out the scene (Kosslyn, 1994; Itti and Koch, 2001).This affects the kinds of things we are most likelyto generate language about (Itti and Arbib, 2005).We can now outline some general requirementsfor an algorithm capable of generating naturalis-tic reference to objects in a visual scene: Input tosuch an algorithm should include a feature-basedrepresentation, which we will call a propositionalrepresentation, with values for color, texture, etc.,and a spatial representation, with symbolic infor-mation about objects?
size and the spatial relation-ships between components.
A system that gener-ates naturalistic reference must also use a knowl-edge base storing information about object proto-types, which may be represented in terms of theirown propositional/spatial representations.3Note that this is not the only proposed matching structurein the brain ?
an exemplar activation system matches input tostored exemplars.5 Conclusions and Future WorkWe have explored the interaction between view-ing objects in a three-dimensional, spatial domainand referring expression generation.
This has ledus to propose structures that may be used to con-nect vision in a spatial modality to naturalistic ref-erence.
The proposed structures include a spatialrepresentation, a propositional representation, anda knowledge base with representations for objectprototypes.
Using structures that define the propo-sitional and spatial content of objects fits well withwork in psycholinguistics, cognitive science andneurophysiology, and may provide the basis togenerate a variety of natural-sounding referencesfrom a system that recognizes objects.It is important to note that any naturalistic ex-perimental design limits the kinds of conclusionsthat can be drawn about reference.
A study thatelicits reference to objects in a visual scene pro-vides insight into reference to objects in a visualscene; these conclusions cannot easily be extendedto reference to other kinds of phenomena, such asreference to people in a novel.
We therefore makeno claims about reference as a whole in this paper;generalizations from this research can provide hy-potheses for further testing in different modalitiesand with different sorts of referents.Our data leave open many areas for furtherstudy, and we hope to address these in future work.Experiments designed specifically to elicit relativesize modifiers, reference to object components,and reference to objects that are like other thingswould help further detail the form our proposedstructures take.What is clear from our data is that both a spa-tial understanding and a non-spatial feature-basedunderstanding appear to play a role in referenceto objects in a visual scene, and further, refer-ence in such a setting is bolstered by a knowl-edge base with stored prototypical object repre-sentations.
Utilizing structures representative ofthese phenomena, we may be able to extend ob-ject recognition research into object reference re-search, generating natural-sounding reference ineveryday settings.AcknowledgementsThanks to Advaith Siddarthan for thought-provoking discussions and to the anonymous re-viewers for useful suggestions.ReferencesCarlos Areces, Alexander Koller, and Kristina Strieg-nitz.
2008.
Referring expressions as formulas ofdescription logic.
Proceedings of the Fifth Inter-national Natural Language Generation Conference,pages 42?29.Robbert-Jan Beun and Anita H. M. Cremers.
1998.Object reference in a shared domain of conversation.Pragmatics and Cognition, 6:121?52.Erik Blaser, Zenon W. Pylyshyn, and Alex O. Hol-combe.
2000.
Tracking an object through featurespace.
Nature, 408:196?199.Susan E. Brennan and Herbert H. Clark.
1996.
Con-ceptual pacts and lexical choice in conversation.Journal of Experimental Psychology: Learning,Memory, and Cognition, 22:1482?93.Alphonse Chapanis, Robert N. Parrish, Robert B.Ochsman, and Gerald D. Weeks.
1977.
Studiesin interactive communication: II.
the effects of fourcommunication modes on the linguistic performanceof teams during cooperative problem solving.
Hu-man Factors, 19:101?125.Herbert H. Clark and Adrian Bangerter.
2004.
Chang-ing ideas about reference.
In Ira A. Noveck and DanSperber, editors, Experimental pragmatics, pages25?49.
Palgrave Macmillan, Basingstoke, England.Herbert H. Clark and Meredyth A. Krych.
2004.Speaking while monitoring addressees for under-standing.
Journal of Memory and Language, 50:62?81.Herbert H. Clark and Deanna Wilkes-Gibbs.
1986.
Re-ferring as a collaborative process.
Cognition, 22:1?39.Herbert H. Clark, Robert Schreuder, and Samuel But-trick.
1983.
Common ground and the understand-ing of demonstrative reference.
Journal of VerbalLearning and Verbal Behavior, 22:1?39.Philip R. Cohen.
1984.
The pragmatics of referringand the modality of communication.
ComputationalLinguistics, 10(2):97?146.Robert Dale and Ehud Reiter.
1995.
Computationalinterpretations of the gricean maxims in the gener-ation of referring expressions.
Cognitive Science,18:233?263.J.
H. Flavell, P. T. Botkin, D. L. Fry Jr., J. W. Wright,and P. E. Jarvice.
1968.
The Development of Role-Taking and Communication Skills in Children.
JohnWiley, New York.William Ford and David Olson.
1975.
The elaborationof the noun phrase in children?s description of ob-jects.
The Journal of Experimental Child Psychol-ogy, 19:371?382.Kotaro Funakoshi, Satoru Watanabe, Naoko Kuriyama,and Takenobu Tokunaga.
2004.
Generating refer-ring expressions using perceptual groups.
In Pro-ceedings of the 3rd International Conference on Nat-ural Language Generation, pages 51?60.Albert Gatt.
2006.
Structuring knowledge for refer-ence generation: A clustering algorithm.
Proceed-ings of the 11th Conference of the European Chap-ter of the Association for Computational Linguistics(EACL-06), pages 321?328.Paul H. Grice.
1975.
Logic and conversation.
Syntaxand Semantics, 3:41?58.Peter A. Heeman and Graeme Hirst.
1995.
Collabo-rating on referring expressions.
Computational Lin-guistics, 21.Laurent Itti and Michael A. Arbib.
2005.
Attention andthe minimal subscene.
In Michael A. Arbib, editor,Action to Language via the Mirror Neuron System.Cambridge University Press.Laurent Itti and Christof Koch.
2001.
Computationalmodelling of visual attention.
Nature Reviews Neu-roscience.J.
Kelleher, F. Costello, and J. van Genabith.
2005.Dynamically structuring, updating and interrelatingrepresentations of visual and linguistic discoursecontext.
Artificial Intelligence, 167:62?102.Stephen M. Kosslyn.
1994.
Image and Brain: TheResolution of the Imagery Debate.
MIT Press, Cam-bridge, MA.Emiel Krahmer, Sebastiaan van Erk, and Andre?
Verleg.2003.
Graph-based generation of referring expres-sions.
Computational Linguistics, 29(1):53?72.Robert M. Krauss and Sam Glucksberg.
1969.
Thedevelopment of communication: Competence as afunction of age.
Child Development, 40:255?266.Robert M. Krauss and Sidney Weinheimer.
1967.
Ef-fect of referent similarity and communication modeon verbal encoding.
Journal of Verbal Learning andVerbal Behavior, 6:359?363.Nikos K. Logothetis and David L. Sheinberg.
1996.Visual object recognition.
Annual Review Neuro-science, 19:577?621.Dominic Mazzoni.
2010.
Audacity.Margaret Mitchell.
2008.
Towards the generationof natural reference.
Master?s thesis, University ofWashington.Thomas J. Palmeri and Isabel Gauthier.
2004.
Vi-sual object understanding.
Nature Reviews Neuro-science, 5:291?303.Maximilian Riesenhuber and Tomaso Poggio.
2000.Models of object recognition.
Nature NeuroscienceSupplement, 3:1199?1204.Eleanor Rosch.
1975.
Cognitive representation ofsemantic categories.
Journal of Experimental Psy-chology, 104:192?233.Harvey Sacks and Emanuel A. Schegloff.
1979.
Twopreferences in the organization of reference to per-sons in conversation and their interaction.
In GeorgePsathas, editor, Everyday Language: Studies in Eth-nomethodology, pages 15?21.
Irvington Publishers,New York.Stegan Treue and Julio C. Martinez Trujillo.
1999.Feature-based attention influences motion process-ing gain in macaque visual cortex.
Nature, 399:575?579.Kees van Deemter, Ielka van der Sluis, and Albert Gatt.2006.
Building a semantically transparent corpusfor the generation of referring expressions.
In Pro-ceedings of the 4th International Conference on Nat-ural Language Generation, Sydney, Australia.
ACL.Jette Viethen and Robert Dale.
2008.
The use of spatialdescriptions in referring expressions.
In Proceed-ings of the 5th International Conference on NaturalLanguage Generation, INLG-08, Salt Fork, Ohio.ACL.
