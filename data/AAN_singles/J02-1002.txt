A Critique and Improvement of anEvaluation Metric for Text SegmentationLev Pevzner Marti A. HearstyHarvard University University of California, BerkeleyThe Pk evaluation metric, initially proposed by Beeferman, Berger, and Lafferty (1997), is be-coming the standard measure for assessing text segmentation algorithms.
However, a theoreticalanalysis of the metric finds several problems: the metric penalizes false negatives more heavilythan false positives, overpenalizes near misses, and is affected by variation in segment size dis-tribution.
We propose a simple modification to the Pk metric that remedies these problems.
Thisnew metric?called WindowDiff?moves a fixed-sized window across the text and penalizes thealgorithm whenever the number of boundaries within the window does not match the true numberof boundaries for that window of text.1.
IntroductionText segmentation is the task of determining the positions at which topics change ina stream of text.
Interest in automatic text segmentation has blossomed over the lastfew years, with applications ranging from information retrieval to text summariza-tion to story segmentation of video feeds.
Early work in multiparagraph discoursesegmentation examined the problem of subdividing texts into multiparagraph unitsthat represent passages or subtopics.
An example, drawn from Hearst (1997), is a 21-paragraph science news article, called ?Stargazers,?
whose main topic is the existenceof life on earth and other planets.
Its contents can be described as consisting of thefollowing subtopic discussions (numbers indicate paragraphs):1?3 Introduction: The search for life in space4?5 The moon?s chemical composition6?8 How early earth-moon proximity shaped the moon9?12 How the moon helped life evolve on earth13 Improbability of the earth-moon system14?16 Binary/trinary star systems make life unlikely17?18 The low probability of nonbinary/trinary systems19?20 Properties of earth?s sun that facilitate life21 SummaryThe TextTiling algorithm (Hearst 1993, 1994, 1997) attempts to recognize thesesubtopic changes by making use of patterns of lexical co-occurrence and distribution;subtopic boundaries are assumed to occur at the point in the documents at which largeshifts in vocabulary occur.
Many others have used this technique, or slight variations Harvard University, 380 Leverett Mail Center, Cambridge, MA 02138.
E-mail:pevzner@post.harvard.eduy University of California, Berkeley, 102 South Hall #4600, Berkeley, CA 94720.
E-mail:hearst@sims.berkeley.educ?
2002 Association for Computational LinguisticsComputational Linguistics Volume 28, Number 1of it, for subtopic segmentation (Nomoto and Nitta 1994; Hasnah 1996; Richmond,Smith, and Amitay 1997; Heinonen 1998; Boguraev and Neff 2000).
Other techniquesuse clustering and/or similarity matrices based on word co-occurrences (Reynar 1994;Yaari 1997; Choi 2000), and still others use machine learning techniques to detect cuewords, or hand-selected cue words to detect segment boundaries (Passonneau andLitman 1993; Beeferman, Berger, and Lafferty 1997; Manning 1998).Researchers have explored the use of this kind of document segmentation to im-prove automated summarization (Salton et al 1994; Barzilay and Elhadad 1997; Kan,Klavans, and McKeown 1998; Mittal et al 1999; Boguraev and Neff 2000) and auto-mated genre detection (Karlgren 1996).
Text segmentation issues are also importantfor passage retrieval, a subproblem of information retrieval (Hearst and Plaunt 1993;Salton, Allan, and Buckley 1993; Callan 1994; Kaszkiel and Zobel 1997).
More recently,a great deal of interest has arisen in using automatic segmentation for the detectionof topic and story boundaries in news feeds (Mani et al 1997; Merlino, Morey, andMaybury 1997; Ponte and Croft 1997; Hauptmann and Witbrock 1998; Allan et al1998; Beeferman, Berger, and Lafferty 1997, 1999).
Sometimes segmentation is doneat the clause level, for the purposes of detecting nuances of dialogue structure or formore sophisticated discourse-processing purposes (Morris and Hirst 1991; Passonneauand Litman 1993; Litman and Passonneau 1995; Hirschberg and Nakatani 1996; Marcu2000).
Some of these algorithms produce hierarchical dialogue segmentations whoseevaluation is outside the scope of this discussion.1.1 Evaluating Segmentation AlgorithmsThere are two major difficulties associated with evaluating algorithms for text seg-mentation.
The first is that since human judges do not always agree where boundariesshould be placed and how fine grained an analysis should be, it is difficult to choosea reference segmentation for comparison.
Some evaluations circumvent this difficultyby detecting boundaries in sets of concatenated documents, where there can be no dis-agreements about the fact of the matter (Reynar 1994; Choi 2000); others have severalhuman judges make ratings to produce a ?gold standard.
?The second difficulty with evaluating these algorithms is that for different applica-tions of text segmentation, different kinds of errors become important.
For instance, forinformation retrieval, it can be acceptable for boundaries to be off by a few sentences?a condition called a near miss?but for news boundary detection, accurate placementis crucial.
For this reason, some researchers prefer not to measure the segmentationalgorithm directly, but consider its impact on the end application (Manning 1998; Kan,Klavans, and McKeown 1998).
Our approach to these two difficulties is to evaluate al-gorithms on real segmentations using a ?gold standard?
and to develop an evaluationalgorithm that suits all applications reasonably well.Precision and recall are standard evaluation measures for information retrievaltasks and are often applied to evaluation of text segmentation algorithms as well.Precision is the percentage of boundaries identified by an algorithm that are indeedtrue boundaries; recall is the percentage of true boundaries that are identified bythe algorithm.
However, precision and recall are problematic for two reasons.
Thefirst is that there is an inherent trade-off between precision and recall; improvingone tends to cause the score for the other to decline.
In the segmentation example,positing more boundaries will tend to improve the recall but at the same time reducethe precision.
Some evaluators use a weighted combination of the two known asthe F-measure (Baeza-Yates and Ribeiro-Neto 1999), but this is difficult to interpret(Beeferman, Berger, and Lafferty 1999).
Another approach is to plot a precision-recallcurve, showing the scores for precision at different levels of recall.20Pevzner and Hearst An Evaluation Metric for Text SegmentationFigure 1Two hypothetical segmentations of the same reference (ground truth) document segmentation.The boxes indicate sentences or other units of subdivision, and spaces between boxes indicatepotential boundary locations.
Algorithm A-0 makes two near misses, while Algorithm A-1misses both boundaries by a wide margin and introduces three false positives.
Bothalgorithms would receive scores of 0 for both precision and recall.Another problem with precision and recall is that they are not sensitive to nearmisses.
Consider, for example, a reference segmentation and the results obtained bytwo different text segmentation algorithms, as depicted in Figure 1.
In both cases, thealgorithms fail to match any boundary precisely; both receive scores of 0 for precisionand recall.
However, Algorithm A-0 is close to correct in almost all cases, whereasAlgorithm A-1 is entirely off, adding extraneous boundaries and missing importantboundaries entirely.
In some circumstances, it would be useful to have an evaluationmetric that penalizes A-0 less harshly than A-1.1.2 The Pk Evaluation MetricBeeferman, Berger, and Lafferty (1997) introduce a new evaluation metric that attemptsto resolve the problems with precision and recall, including assigning partial credit tonear misses.
They justify their metric as follows (page 43):Segmentation : : : is about identifying boundaries between successiveunits of information in a text corpus.
Two such units are either relatedor unrelated by the intent of the document author.
A natural wayto reason about developing a segmentation algorithm is therefore tooptimize the likelihood that two such units are correctly labeled asbeing related or being unrelated.
Our error metric P is simply theprobability that two sentences drawn randomly from the corpus are correctlyidentified as belonging to the same document or not belonging to the samedocument.The derivation of P is rather involved, and a much simpler version is adoptedin the later work (Beeferman, Berger, and Lafferty 1999) and by others.
This version,referred to as Pk, is calculated by setting k to half of the average true segment sizeand then computing penalties via a moving window of length k. At each location, thealgorithm determines whether the two ends of the probe are in the same or differ-ent segments in the reference segmentation and increases a counter if the algorithm?ssegmentation disagrees.
The resulting count is scaled between 0 and 1 by dividingby the number of measurements taken.
An algorithm that assigns all boundaries cor-rectly receives a score of 0.
Beeferman, Berger, and Lafferty (1999) state as part of21Computational Linguistics Volume 28, Number 1Figure 2An illustration of how the Pk metric handles false negatives.
The arrowed lines indicate thetwo poles of the probe as it moves from left to right; the boxes indicate sentences or otherunits of subdivision; and the width of the window (k) is four, meaning four potentialboundaries fall between the two ends of the probe.
Solid lines indicate no penalty is assigned;dashed lines indicate a penalty is assigned.
Total penalty is always k for false negatives.the justification for this metric that, to discourage ?cheating?
of the metric, degener-ate algorithms?those that place boundaries at every position, or place no boundariesat all?are assigned (approximately) the same score.
Additionally, the authors definea false negative (also referred to as a miss) as a case when a boundary is present inthe reference segmentation but missing in the algorithm?s hypothesized segmentation,and a false positive as an assignment of a boundary that does not exist in the referencesegmentation.2.
Analysis of the Pk Error MetricThe Pk metric is fast becoming the standard among researchers working in text seg-mentation (Allan et al 1998; Dharanipragada et al 1999; Eichmann et al 1999; vanMulbregt et al 1999; Choi 2000).
However, we have reservations about this metric.We claim that the fundamental premise behind it is flawed; additionally, it has sev-eral significant drawbacks, which we identify in this section.
In the remainder of thepaper, we suggest modifications to resolve these problems, and we report the resultsof simulations that validate the analysis and suggest that the modified metric is animprovement over the original.2.1 Problem 1: False Negatives Penalized More Than False PositivesAssume a text with segments of average size 2k, where k is the distance betweenthe two ends of the Pk probe.
If the algorithm misses a boundary?produces a falsenegative?it receives k penalties.
To see why, suppose S1 and S2 are two segmentsof length 2k, and the algorithm misses the transition from S1 to S2.
When Pk sweepsacross S1, if both ends of the probe point to sentences that are inside S1, the twosentences are in the same segment in both the reference and the hypothesis, and nopenalty is incurred.
When the right end of the probe crosses the reference boundarybetween S1 and S2, it will start recording nonmatches, since the algorithm assigns thetwo sentences to the same segment, while the reference does not.
This circumstancehappens k times, until both ends of the probe point to sentences that are inside S2.
(See Figure 2.)
This analysis assumes average size segments; variation in segment sizeis discussed below, but does not have a large effect on this result.22Pevzner and Hearst An Evaluation Metric for Text SegmentationFigure 3An illustration of how the Pk metric handles false positives.
Notation is as in Figure 2.
Totalpenalty depends on the distance between the false positive and the relevant correctboundaries; on average, it is k2 , assuming a uniform distribution of boundaries across thedocument.
This example shows the consequences of two different locations of false positives:on the left, the penalty is k2 ; on the right, it is k.Now, consider false positives.
A false positive occurs when the algorithm placesa boundary at some position where there is no boundary in the reference segmenta-tion.
The number of times that this false positive is noted by Pk depends on whereexactly inside S2 the false positive occurs.
(See Figure 3.)
If it occurs in the middleof the segment, the false positive is noted k times (as seen on the right-hand side ofFigure 3).
If it occurs j < k sentences from the beginning or the end of the segment, thesegmentation is penalized j times.
Assuming uniformly distributed false positives, onaverage a false positive is noted k2 times by the metric?half the rate for false negatives.This average increases with segment size, as we will discuss later, and changes if oneassumes different distributions of false positives throughout the document.
However,this does not change the fact that in most cases, false positives are penalized someamount less than false negatives.This is not an entirely undesirable side effect.
This metric was devised to take intoaccount how close an assigned boundary is to the true one, rather than just markingit as correct or incorrect.
This method of penalizing false positives achieves this goal:the closer the algorithm?s boundary is to the actual boundary, the less it is penalized.However, overpenalizing false negatives to do this is not desirable.One way to fix the problem of penalizing false negatives more than false positivesis to double the false positive penalty (or halve the false negative penalty).
However,this would undermine the probabilistic nature of the metric.
In addition, doubling thepenalty may not always be the correct solution, since segment size will vary from theaverage, and false positives are not necessarily uniformly distributed throughout thedocument.2.2 Problem 2: Number of Boundaries IgnoredAnother important problem with the Pk metric is that it allows some errors to go unpe-nalized.
In particular, it does not take into account the number of segment boundariesbetween the two ends of the probe.
(See Figure 4.)
Let ri indicate the number of bound-aries between the ends of the probe according to the reference segmentation, and let aiindicate the number of boundaries proposed by some text segmentation algorithm forthe same stretch of text.
If ri = 1 (the reference segmentation indicates one boundary)and ai = 2 (the algorithm marks two boundaries within this range), then the algorithmmakes at least one false positive (spurious boundary) error.
However, the evaluationmetric Pk does not assign a penalty in this situation.
Similarly, if ri = 2 and ai = 1, the23Computational Linguistics Volume 28, Number 1Figure 4An illustration of the fact that the Pk metric fails to penalize false positives that fall within ksentences of a true boundary.
Notation is as in Figure 2.algorithm has made at least one false negative (missing boundary) error, but it is notpenalized for this error under Pk.2.3 Problem 3: Sensitivity to Variations in Segment SizeThe size of the segment plays a role in the amount that a false positive within thesegment or a false negative at its boundary is penalized.
Let us consider false negatives(missing boundaries) first.
As seen above, with average size segments, the penalty fora false negative is k. For larger segments, it remains at k?it cannot be any largerthan that, since for a given position i there can be at most k intervals of length kthat include that position.
As segment size gets smaller, however, the false negativepenalty changes.
Suppose we have two segments, A and B, and the algorithm missesthe boundary between them.
Then the algorithm will be penalized k times if Size(A)+Size(B) > 2k, that is, as long as each segment is about half the average size or larger.The penalty will then decrease linearly with Size(A)+Size(B) so long as k < Size(A)+Size(B) < 2k.
To be more exact, the penalty actually decreases linearly as the size ofeither segment decreases below k. This is intuitively clear from the simple observationthat in order to incur a penalty at any range ri for a false negative, it has to bethe case that ri > ai.
In order for this to be true, both the segment to the left andthe segment to the right of the missed boundary have to be of size greater thank; otherwise, the penalty can only be equal to the size of the smaller segment.
WhenSize(A)+Size(B) < k, the penalty disappears completely, since then the probe?s intervalis larger than the combined size of both segments, making it not sensitive enough todetect the false negative.
It should be noted that fixing Problem 2 would at leastpartially fix this bias as well.Now, consider false positives (extraneous boundaries).
For average segment sizeand a uniform distribution of false positives, the average penalty is k2 , as describedearlier.
In general, in large enough segments, the penalty when the false positive isa distance d < k from a boundary is d, and the penalty when the false positive is adistance d > k from a boundary is k. Thus, for larger segments, the average penaltyassuming a uniform distribution becomes larger, because there are more places in thesegment that are at least k positions away from a boundary.
The behavior at the edgesof the segments remains the same, though, so the average penalty never reaches k.Now, consider what happens with smaller segments.
Suppose we have a false positivein Segment A.
As Size(A) decreases from 2k to k, the average false positive penaltydecreases linearly with it, because when Size(A) decreases below 2k, the maximumdistance any sentence can be from a boundary becomes less than k. Therefore, the24Pevzner and Hearst An Evaluation Metric for Text SegmentationFigure 5A reference segmentation and five different hypothesized segmentations with differentproperties.maximum possible penalty for a false positive in A is less than k, and this numbercontinues to decrease as Size(A) decreases.
When Size(A) < k, the false positive penaltydisappears, for the same reason that the false negative penalty disappears for smallersegments.
Again, fixing Problem 2 would go a long way toward eliminating this bias.Thus, errors in larger-than-average segments increase the penalty slightly (for falsepositives) or not at all (for false negatives) as compared to average size segments, whileerrors in smaller-than-average segments decrease the penalty significantly for bothtypes of error.
This means that as the variation of segment size increases, the metricbecomes more lenient, since it severely underpenalizes errors in smaller segments,while not making up for this by overpenalizing errors in larger segments.2.4 Problem 4: Near-Miss Error Penalized Too MuchReconsider the segmentation made by Algorithm A-0 in Figure 1.
In both cases ofboundary assignment, Algorithm A-0 makes both a false positive and a false negativeerror, but places the boundary very close to the actual one.
We will call this kind oferror a near-miss error, distinct from a false positive or false negative error.
Distinguish-ing this type of error from ?pure?
false positives better reflects the goal of creating ametric different from precision and recall, since it can be penalized less than a falsenegative or a false positive.Now, consider the algorithm segmentations shown in Figure 5.
Each of the fivealgorithms makes a mistake either on the boundary between the first and second seg-ment of the reference segmentation, or within the second segment.
How should thesevarious segmentations be penalized?
In the analysis below, we assume an applicationfor which it is important not to introduce spurious boundaries.
These comparisonswill most likely vary depending on the goals of the target application.Algorithm A-4 is arguably the worst of the examples, since it has a false positiveand a false negative simultaneously.
Algorithms A-0 and A-2 follow: they contain apure false negative and false positive, respectively.
Comparing Algorithms A-1 andA-3, we see that Algorithm A-3 is arguably better, because it recognizes that only oneboundary is present rather than two.
Algorithm A-1 does not recognize this, and insertsan extra segment.
Even though Algorithm A-1 actually places a correct boundary, italso places an erroneous boundary, which, although close to the actual one, is stilla false positive?in fact, a pure false positive.
For this reason, Algorithm A-3 can beconsidered better than Algorithm A-1.25Computational Linguistics Volume 28, Number 1Now, consider how Pk treats the five types of mistakes above.
Again, assume thefirst and second segments in the reference segmentation are average size segments.Algorithm A-4 is penalized the most, as it should be.
The penalty is as much as 2k if thefalse positive falls in the middle of Segment C, and it is > k as long as the false positiveis a distance > k2 from the actual boundary between the first and second referencesegments.
The penalty is large because the metric catches both the false negativeand the false positive errors.
The segmentations assigned by Algorithms A-0 and A-2are treated as discussed earlier in conjunction with Problem 1: the one assigned byAlgorithm A-0 has a false negative and thus incurs a penalty of k, and the one assignedby Algorithm A-2 has a false positive, and thus incurs a penalty of k. Finally, considerthe segmentations assigned by Algorithms A-1 and A-3, and suppose that both containan incorrect boundary some small distance e from the actual one.
Then the penalty forAlgorithm A-1 is e, while the penalty for Algorithm A-3 is 2e.
This should not be thecase; Algorithm A-1 should be penalized more than Algorithm A-3, since a near-misserror is better than a pure false positive, even if it is close to the boundary.2.5 Problem 5: What Do the Numbers Mean?Pk is nonintuitive because it measures the probability that two sentences k units apartare incorrectly labeled as being in different segments, rather than directly reflectingthe competence of the algorithm.
Although perfect algorithms score 0, and various de-generate ones score 0:5, numerical interpretation and comparison are difficult becauseit is not clear how the scores are scaled.3.
A SolutionIt turns out that a simple change to the error metric algorithm remedies most of theproblems described above, while retaining the desirable characteristic of penalizingnear misses less than pure false positives and pure false negatives.
The amendedmetric, which we call WindowDiff, works as follows: for each position of the probe,simply compare the number of reference segmentation boundaries that fall in thisinterval (ri) with the number of boundaries that are assigned by the algorithm (ai).The algorithm is penalized if ri 6= ai (which is computed as jri ?
aij > 0).More formally,WindowDi (ref , hyp) =1N ?
kN?kXi=1(jb(ref i, ref i+k)?
b(hyp i, hyp i+k)j > 0),where b(i, j) represents the number of boundaries between positions i and j in the textand N represents the number of sentences in the text.This approach clearly eliminates the asymmetry between the false positive andfalse negative penalties seen in the Pk metric.
It also catches false positives and falsenegatives within segments of length less than k.To understand the behavior of WindowDiff with respect to the other problems,consider again the examples in Figure 5.
This metric penalizes Algorithm A-4 (whichcontains both a false positive and a false negative) the most, assigning it a penaltyof about 2k.
Algorithms A-0, A-1, and A-2 receive the same penalty (about k), andAlgorithm A-3 receives the smallest penalty (2e, where e is the offset from the actualboundary, presumed to be much smaller than k).
Thus, although it makes the mistakeof penalizing Algorithm A-1 as much as Algorithms A-0 and A-2, it correctly recog-nizes that the error made by Algorithm A-3 is a near miss and assigns it a smallerpenalty than Algorithm A-1 or any of the others.
We argue that this kind of error isless detrimental than the errors made by Pk.
WindowDiff successfully distinguishes26Pevzner and Hearst An Evaluation Metric for Text Segmentationthe near-miss error as a separate kind of error and penalizes it a different amount,something that Pk is unable to do.We explored a weighted version of WindowDiff, in which the penalty is weightedby the difference jri?
aij.
However, the results of the simulations were nearly identicalwith those of the nonweighted version of this metric, so we do not consider theweighted version further.4.
Validation via SimulationsThis section describes a set of simulations that verify the theoretical analysis of thePk metric presented above.
It also reports the results of simulating two alternatives,including the proposed solution just described.For the simulation runs described below, three metrics were implemented: the Pk metric; the Pk metric modified to double the false positive penalty (henceforthP0k); and our proposed alternative, WindowDiff (henceforth WD), which countsthe number of segment boundaries between the two ends of the probeand assigns a penalty if this number is different for the experimental andreference segmentations.In these studies, a single trial consists of generating a reference segmentation of1,000 segments with some distribution, generating different experimental segmenta-tions of a specific type 100 times, computing the metric based on the comparison ofthe reference and experimental segmentations, and averaging the 100 results.
For ex-ample, we might generate a reference segmentation R, then generate 100 experimentalsegmentations that have false negatives with probability 0.5, and then compute theaverage of their Pk penalties.
We carried out 10 such trials for each experiment andaveraged the average penalties over these trials.4.1 Variation in the Segment SizesThe first set of tests was designed to test the metric?s performance on texts withdifferent segment size distributions (Problem 3).
We generated four sets of referencesegmentations with segment size uniformly distributed between two numbers.
Notethat the units of segmentation are deliberately left unspecified.
So a segment of size 25can refer to 25 words, clauses, or sentences?whichever is applicable to the task underconsideration.
Also note that the same tests were run using larger segment sizes thanthose reported here, with the results remaining nearly identical.For these tests, the mean segment size was held constant at 25 for each set ofreference segments, in order to produce distributions of segment size with the samemeans but different variances.
The four ranges of segment sizes were (20, 30), (15, 35),(10, 40), and (5, 45).
The results of these tests are shown in Table 1.
The tests used thefollowing types of experimental segmentations: FN: segmentation with false negative probability 0.5 at each boundary; FP: segmentation with false positive probability 0.5 in each segment,with the probability uniformly distributed within each segment; and FNP: segmentation with false positive probability 0.5 (uniformlydistributed), and false negative probability 0.5.27Computational Linguistics Volume 28, Number 1Table 1Average error score for Pk, P0k, and WD over 10 trials of 100measurements each, shown by segment size distribution range.
(a) False negatives were placed with probability 0.5 at eachboundary; (b) false positives were placed with probability 0.5,uniformly distributed within each segment; and (c) both falsenegatives and false positives were placed with probability 0.5.
(a) False negatives, p = 0:5(20, 30) (15, 35) (10, 40) (5, 45)Pk 0.245 0.245 0.240 0.223P0k 0.245 0.245 0.240 0.223WD 0.245 0.245 0.242 0.237(b) False positives, p = 0:5(20, 30) (15, 35) (10, 40) (5, 45)Pk 0.128 0.122 0.112 0.107P0k 0.256 0.245 0.225 0.213WD 0.240 0.241 0.238 0.236(c) False positives and false negatives, p = 0:5(20, 30) (15, 35) (10, 40) (5, 45)Pk 0.317 0.309 0.290 0.268P0k 0.446 0.432 0.403 0.375WD 0.376 0.370 0.357 0.343The results indicate that variation in segment size does make a difference, but nota very big one.
(As we will show, the differences are similar when we use a smallerprobability of false negative/positive occurrence.)
The Pk value for the (20, 30) rangewith FN segmentation is on average 0.245, and it decreases to 0.223 for the (5, 45)range.
Similarly, the FP segmentation decreases from 0.128 for the (20, 30) range to0.107 for the (5, 45) range, and the FNP segmentation decreases from 0.317 for the (20,30) range to 0.268 for the (5, 45) range.
Thus, variation in segment size has an effecton Pk, as predicted.Note that for false negatives, the Pk value for the (20, 30) range is not muchdifferent than for the (15, 35) range.
This is expected since there are no segments ofsize less than k (12.5) in these conditions.
For the (10, 40) range, the Pk value is slightlysmaller; and for the (5, 45) range, it is smaller still.
These results are to be expected,since more segments in these ranges will be of length less than k.For the FP segmentations, on the other hand, the decrease in Pk value is morepronounced, falling from 0.128 to 0.107 as the segment size range changes from (20,30) to (5, 45).
This is also consistent with our earlier analysis of the behavior of themetric on false positives as segment size decreases.
Notice that the difference in Pkvalues between (15, 35) and (10, 40) is slightly larger than the other two differences.This happens because for segment sizes < k, the false positive penalty disappearscompletely.
The results for the FNP segmentation are consistent with what one wouldexpect of a mix of the FN and FP segmentations.Several other observations can be made from Table 1.
We can begin to make somejudgments about how the metric performs on algorithms prone to different kinds oferrors.
First, Pk penalizes false negatives about twice as much as false positives, as28Pevzner and Hearst An Evaluation Metric for Text Segmentationpredicted by our analysis.
The experimental segmentations in Table 1a contain onaverage 500 false negatives, while the ones in Table 1b contain on average 500 falsepositives, but the penalty for the Table 1b segmentations is consistently about halfthat for those in Table 1a.
Thus, algorithms prone to false positives are penalized lessharshly than those prone to false negatives.The table also shows the performance of the two other metrics.
P0k simply doublesthe false positive penalty, while WD counts and compares the number of boundariesbetween the two ends of the probe, as described earlier.
Both P0k and WD appear tosolve the problem of underpenalizing false positives, but WD has the added benefit ofbeing more stable across variations in segment size distribution.
Thus, WD essentiallysolves Problems 1, 2, and 3.Table 1c shows that for the FNP segmentation (in which both false positives andfalse negatives occur), there is a disparity between the performances of P0k and WD.It appears that P0k is harsher in this situation.
From the above discussion, we knowthat WD is more lenient in situations where a false negative and a false positive occurnear each other (where ?near?
means within a distance of k2 ) than P0k is.
However,P0k is more lenient for pure false positives that occur close to boundaries.
Thus, it isnot immediately clear why P0k is harsher in this situation, but a more detailed lookprovides the answer.Let us begin the analysis by trying to explain why Pk scores for the FNP seg-mentation make sense.
The FNP segmentation places both false negatives and falsepositives with probability 0.5.
Since we are working with reference segmentations of1,000 segments, this means 500 missed boundaries and 500 incorrect boundaries.
Sincethe probabilities are uniformly distributed across all segments and all boundaries, onaverage one would expect the following distribution of errors: 250 false positives with no false negative within k sentences of them(Type A); 250 false negatives with no false positive within k sentences of them(Type B); and 250 ?joint?
errors, where a false positive and a false negative occurwithin k sentences of each other (Type C).A Type A error is a standard false positive, so the average penalty is k2 .
A Type Berror is a standard false negative, so the average penalty is k. It remains to fig-ure out what the average penalty is for a Type C error.
Modeling the behavior ofthe metric, a Type C error occurrence in which a false positive and a false nega-tive are some distance e < k from each other incurs a penalty of 2e, where e is as-signed for the false positive and another e is assigned for the false negative.
Thismay range from 0 to 2k, and since error distribution is uniform, the penalty is kon average?the same as for a regular false negative.
To translate this into actualvalues, we assume the metric is linear with respect to the number of errors (a rea-sonable assumption, supported by our experiments).
Thus, if Pk outputs a penaltyof p for 500 false negatives, it would have a penalty of p2 for 250 false negatives.Let a be the penalty for 500 Type A errors, b the penalty for 500 Type B errors,and c the penalty for 500 Type C errors; then the penalty for the FNP segmenta-tion is p = a2 +b2 +c2 .
Assuming the metric is linear, we know that c = b = 2a(because Pk penalized false negatives twice as much as false positives on average).We can thus substitute either b or 2a for c. We choose to substitute 2a, because Pk isstrongly affected by segment size variation for Type A and Type C errors, but not for29Computational Linguistics Volume 28, Number 1Type B errors.
Thus, replacing c with 2a is more accurate.
Performing the substitu-tion, we have p = 3  a2 + b2 .
We have a and b from the FP and FN data, respectively,so we can compute p. The results, arranged by segment size variation, are as fol-lows:(20, 30) (15, 35) (10, 40) (5, 45)Estimate 0.315 0.306 0.288 0.272Actual 0.317 0.309 0.290 0.268As can easily be seen, the estimate produced using this method is very similar to theactual Pk value.The same sort of analysis applies for P0k and WD.
In P0k, Type A errors are penalizedk on average, since the false positive penalty is doubled.
Type B errors have an averagepenalty of k, as for Pk.
Type C errors have an average penalty of 3e, where 2e isassigned for the false positive and e is assigned for the false negative.
This means thatthe average penalty for a Type C error is 3  k2 .
Since we know that c = 1:5a by thelinear metric assumption, we have p = a2 +b2 + 1:5  a2 = 5  a4 + b2 (the choice of 1:5aover 1:5b was made for the same reason as the choice of 2a over b in the calculationsfor Pk).
The results, arranged by segment size variation, are as follows:(20, 30) (15, 35) (10, 40) (5, 45)Estimate 0.443 0.429 0.401 0.378Actual 0.446 0.432 0.403 0.375Finally, WD incurs an average penalty of k for both Type A and Type B errors.
ForType C errors, the penalty is 2e, so it is also k on average.
Thus, we get p = a2 +b2 +a2 =a + b2 .
The results, arranged by segment size variation, are as follows:(20, 30) (15, 35) (10, 40) (5, 45)Estimate 0.363 0.364 0.359 0.355Actual 0.376 0.370 0.357 0.343These estimates do not correspond to the actual results quite as closely as the estimatesfor Pk and P0k did, but they are still very close.
One reason why these estimates area little less accurate is that for WD, Type C errors are more affected by variation insegment size than either Type A or Type B errors.
This is clear from the fact that thedecrease is greater in the actual data than in the estimate.Table 2 shows data similar to those of Table 1, but using two different probabilityvalues for error occurrence: 0:05 and 0:25.
These results have the same tendencies asthose shown above for p = 0:5.4.2 Variation in the Error DistributionsThe second set of tests was designed to assess the performance of the metrics on algo-rithms prone to different kinds of errors.
This would determine whether the metricsare consistent in applying penalties, or whether they favor certain kinds of errors overothers.
For these trials, we generated the reference segmentation using a uniform dis-tribution of segment sizes in the (15, 35) range.
We picked this range because it hasreasonably high segment size variation, but segment size does not dip below k. For the30Pevzner and Hearst An Evaluation Metric for Text SegmentationTable 2Average error score for Pk, P0k, and WD over 10 trialsof 100 measurements each, shown by segment sizedistribution range.
(a) False negatives were placedwith probability 0.05 at each boundary; (b) falsepositives were placed with probability 0.05,uniformly distributed within each segment; and (c)both false negatives and false positives were placedwith probability 0.05.
(d) False negatives were placedwith probability 0.25 at each boundary; (e) falsepositives were placed with probability 0.25,uniformly distributed within each segment; and (f)both false negatives and false positives were placedwith probability 0.25.
(a) False negatives, p = 0:05(20, 30) (15, 35) (10, 40) (5, 45)Pk 0.025 0.025 0.024 0.022P0k 0.025 0.025 0.024 0.022WD 0.025 0.025 0.024 0.024(b) False positives, p = 0:05(20, 30) (15, 35) (10, 40) (5, 45)Pk 0.013 0.012 0.011 0.011P0k 0.026 0.025 0.023 0.021WD 0.024 0.024 0.024 0.024(c) False positives and false negatives, p = 0:05(20, 30) (15, 35) (10, 40) (5, 45)Pk 0.037 0.036 0.035 0.032P0k 0.050 0.048 0.046 0.042WD 0.048 0.048 0.048 0.047(d) False negatives, p = 0:25(20, 30) (15, 35) (10, 40) (5, 45)Pk 0.122 0.122 0.121 0.110P0k 0.122 0.122 0.121 0.110WD 0.122 0.122 0.122 0.121(e) False positives, p = 0:25(20, 30) (15, 35) (10, 40) (5, 45)Pk 0.064 0.061 0.056 0.053P0k 0.129 0.123 0.112 0.106WD 0.121 0.121 0.121 0.120(f) False positives and false negatives, p = 0:25(20, 30) (15, 35) (10, 40) (5, 45)Pk 0.172 0.168 0.161 0.147P0k 0.236 0.229 0.217 0.200WD 0.215 0.213 0.211 0.20531Computational Linguistics Volume 28, Number 1Table 3Average error score for Pk, P0k, and WD over 10 trials of100 measurements each over the segment distributionrange (15, 35) and with error probabilities of 0.5.
Theaverage penalties computed by the three metrics areshown for seven different error distributions.FN FP1 FP2 FP3 FNP1 FNP2 FNP3Pk .245 .122 .091 .112 .308 .267 .304P0k .245 .244 .182 .224 .431 .354 .416WD .245 .240 .236 .211 .370 .341 .363reasons described above, this means the results will not be skewed by the sensitivityof Pk and P0k to segment size variations.The tests analyzed below were performed using the high error occurrence proba-bilities of 0.5, but similar results were obtained using probabilities of 0.25 and 0.05 aswell.
The following error distributions were used:1 FN: false negatives, probability p = 0:5; FP1: false positives uniformly distributed in each segment, probabilityp = 0:5; FP2: false positives normally distributed around each boundary withstandard deviation equal to 14 the segment size, probability p = 0:5; FP3: false positives uniformly distributed throughout the document,occurring at each point with probability p = number of segmentslength2 (thiscorresponds to a 0.5 probability value for each individual segment); FNP1: FN and FP1 combined; FNP2: FN and FP2 combined; FNP3: FN and FP3 combined.The results are shown in Table 3.
Pk penalizes FP2 less than FP1 and FP3, andFNP2 less than FNP1 and FNP3.
This result is as expected.
FP2 and FNP2 have falsepositives normally distributed around each boundary, which means that more of thefalse positives are close to the boundaries and thus are penalized less.
If we made thestandard deviation smaller, we would expect this difference to be even more apparent.P0k penalized FP2 and FNP2 the least in their respective categories, and FP1 andFNP1 the most, with FP3 and FNP3 falling in between.
These results are as expected,for the same reasons as for Pk.
The difference in the penalty for FP1 and FP3 (andFNP1 vs. FNP3)?for both Pk and P0k, but especially apparent for P0k?is interesting.
InFP/FNP1, false positive probability is uniformly distributed throughout each segment,whereas in FP/FNP3, false positive probability is uniformly distributed throughout theentire document.
Thus, the FP/FNP3 segmentations are more likely to have boundariesthat are very close to each other, since they are not segment dependent, while FP/FNP11 Normal distributions were calculated using the gaussrand() function from Box and Muller (1958),found online at http://www.eskimo.com/scs/C-faq/q13.20.html.32Pevzner and Hearst An Evaluation Metric for Text Segmentationare limited to at most one false positive per segment.
This results in P0k assigningsmaller penalties for FP/FNP3, since groups of false positives close together (to bemore exact, within k sentences of each other) would be underpenalized.
This differenceis also present in the Pk results, but is about half for obvious reasons.WD penalized FP1 the most and FP3 the least among the FP segmentations.
Amongthe FNP segmentations, FNP1 was penalized the most and FNP2 the least.
To see why,we examine the results for the FP segmentations.
WD penalizes pure false positivesthe same amount regardless of how close they are to a boundary; the only way falsepositives are underpenalized is if they occur in bunches.
As mentioned earlier, this ismost likely to happen in FP3.
It is least likely to happen in FP1, since in FP1 there is amaximum of one false positive per segment, and this false positive is not necessarilyclose to a boundary.
In FP2, false positives are also limited to one per segment, butthey are also more likely to be close to boundaries.
This increases the likelihood that2 false positives will be within k sentences of each other and thus makes WD give aslightly lower score to the FP2 segmentation than to the FP1 segmentation.Now let us look at the FNP segmentations.
FNP3 is penalized less than FNP1for the same reason described above, and FNP2 is penalized even less than FNP3.The closer a Type C error is to the boundary, the lower the penalty.
FNP2 has moreerrors distributed near the boundaries than the others: thus, the FNP2 segmentationis penalized less than either FNP1 or FNP3.The same tests were run for different error occurrence probabilities (p = 0:05 andp = 0:25), achieving results similar to those for p = 0:5 just described.
There is a slightdifference for the case of p = 0:05 because the error probability is too small for some ofthe trends to manifest themselves.
In particular, the differences in the way WD treatsthe different segmentations disappear when the error probability is this small.4.3 Variation in the Error TypesWe also performed a small set of tests to verify the theoretical finding that Pk and P0koverpenalize near-miss errors as compared with pure false positives, and that WD doesthe opposite, overpenalizing the pure false positives.
Space limitations prevent detailedreporting of these results, but the simulations did indeed verify these expectations.5.
ConclusionsWe have found that the Pk error metric for text segmentation algorithms is affectedby the variation of segment size distribution, becoming slightly more lenient as thevariance increases.
It penalizes false positives significantly less than false negatives,particularly if the false positives are uniformly distributed throughout the document.
Itpenalizes near-miss errors more than pure false positives of equal magnitude.
Finally,it fails to take into account situations in which multiple boundaries occur betweenthe two sides of the probe, and it often misses or underpenalizes mistakes in smallsegments.We proposed two modifications to tackle these problems.
The first, which we callP0k, simply doubles the false positive penalty.
This solves the problem of overpenalizingfalse negatives, but it is not effective at dealing with the other problems.
The second,which we call WindowDiff (WD), counts the number of boundaries between the twoends of a fixed-length probe, and compares this number with the number of boundariesfound in the same window of text for the reference segmentation.
This modificationaddresses all of the problems listed above.
WD is only slightly affected by variation ofsegment size distribution, gives equal weight to the false positive penalty and the falsenegative penalty, is able to catch mistakes in small segments just as well as mistakes in33Computational Linguistics Volume 28, Number 1large segments, and penalizes near-miss errors less than pure false positives of equalmagnitude.
However, it has some problems of its own.
WD penalizes all pure falsepositives the same amount regardless of how close they are to an actual boundary.It is not clear whether this is a good thing or not, but it seems to be preferable tooverpenalizing near misses.The discussion above addresses Problems 1 through 4 but does not address Prob-lem 5: how does one interpret the values produced by the metric?
From the tests wehave run, it appears that the WD metric grows in a roughly linear fashion with thedifference between the reference and the experimental segmentations.
In addition, wefeel that WD is a more meaningful metric than Pk.
Comparing two stretches of text tosee how many discrepancies occur between the reference and the algorithm?s resultseems more intuitive than determining how often two text units are incorrectly labeledas being in different segments.AcknowledgmentsThis work was completed while the secondauthor was a visiting professor at HarvardUniversity.
Both authors thank BarbaraGrosz and Stuart Shieber, without whomthis work would not have happened, andFreddy Choi for some helpful explanations.They would also like to thank theanonymous reviewers for their valuablecomments.Partial support for the research reportedin this paper was provided by NationalScience Foundation Grants IRI-9618848 andCDA-94-01024.ReferencesAllan, James, Jaime Carbonell, GeorgeDoddington, Jonathan Yamron, andYiming Yang.
1998.
Topic detection andtracking pilot study: Final report.
InProceedings of the DARPA Broadcast NewsTranscription and Understanding Workshop,pages 194?218, Lansdowne, VA.Baeza-Yates, Ricardo and BerthierRibeiro-Neto.
1999.
Modern InformationRetrieval.
Addison-Wesley Longman.Barzilay, Regina and Michael Elhadad.
1997.Using lexical chains for textsummarization.
In Proceedings of the ACLIntelligent Scalable Text SummarizationWorkshop (ISTS?97), Madrid, Spain.Beeferman, Douglas, Adam Berger, andJohn Lafferty.
1997.
Text segmentationusing exponential models.
In Proceedingsof the 2nd Conference on Empirical Methods inNatural Language Processing, pages 35?46,Providence, RI.Beeferman, Douglas, Adam Berger, andJohn Lafferty.
1999.
Statistical models oftext segmentation.
Machine Learning,34(1?3):177?210.Boguraev, Branimir and Mary Neff.
2000.Discourse segmentation in aid ofdocument summarization.
In Proceedingsof the 33rd Hawaii International Conference onSystem Sciences, Maui, HI.Box, G. E. P. and M. E. Muller.
1958.
A noteon the generation of random normaldeviates.
Annals of Mathematical Statistics,29:610?611.Callan, James P. 1994.
Passage-levelevidence in document retrieval.
InProceedings of the 17th Annual InternationalACM/SIGIR Conference, pages 302?310,Dublin, Ireland.Choi, Freddy.
2000.
Advances in domainindependent linear text segmentation.
InProceedings of the 1st Meeting of the NorthAmerican Chapter of the Association forComputational Linguistics, pages 26?33,Seattle, WA.Dharanipragada, S., M. Franz, Jeffrey S.McCarley, S. Roukos, and Todd Ward.1999.
Story segmentation and topicdetection in the broadcast news domain.In Proceedings of the DARPA Broadcast NewsWorkshop, Herndon, VA.Eichmann, David, Miguel Ruiz, PadminiSrinivasan, Nick Street, Chris Culy, andFilippo Menczer.
1999.
A cluster-basedapproach to tracking, detection andsegmentation of broadcast news.
InProceedings of the DARPA Broadcast NewsWorkshop, Herndon, VA.Hasnah, Ahmad.
1996.
Full Text Processingand Retrieval: Weight Ranking, TextStructuring, and Passage Retrieval for ArabicDocuments.
Ph.D. thesis, Illinois Instituteof Technology.Hauptmann, Alexander G. and Michael J.Witbrock.
1998.
Story segmentation anddetection of commercials in broadcastnews video.
In Proceedings of the Advancesin Digital Libraries Conference,pages 168?179, Santa Barbara, CA.Hearst, Marti A.
1993.
TextTiling: Aquantitative approach to discourse34Pevzner and Hearst An Evaluation Metric for Text Segmentationsegmentation.
Technical Report Sequoia93/24, Computer Science Division,University of California, Berkeley.Hearst, Marti A.
1994.
Multi-paragraphsegmentation of expository text.
InProceedings of the 32nd Annual Meeting of theAssociation for Computational Linguistics,pages 9?16, Las Cruces, NM.Hearst, Marti A.
1997.
TextTiling:Segmenting text into multi-paragraphsubtopic passages.
ComputationalLinguistics, 23(1):33?64.Hearst, Marti A. and Christian Plaunt.
1993.Subtopic structuring for full-lengthdocument access.
In Proceedings of the 16thAnnual International ACM/SIGIRConference, pages 59?68, Pittsburgh, PA.Heinonen, Oskari.
1998.
Optimalmulti-paragraph text segmentation bydynamic programming.
In Proceedings ofthe 17th International Conference onComputational Linguistics and the 36thAnnual Meeting of the Association forComputational Linguistics (ACL-COLING?98), pages 1484?1486, Montreal.Hirschberg, Julia and Christine H. Nakatani.1996.
A prosodic analysis of discoursesegments in direction-giving monologues.In Proceedings of the 34th Annual Meeting ofthe Association for Computational Linguistics,pages 286?293, Santa Cruz, CA.Kan, Min-Yen, Judith L. Klavans, andKathleen R. McKeown.
1998.
Linearsegmentation and segment relevance.
InProceedings of the Sixth Workshop on VeryLarge Corpora (WVLC 6), pages 197?205,Montreal.Karlgren, Jussi.
1996.
Stylistic variation inan information retrieval experiment.
InProceedings of the 2nd InternationalConference on New Methods in LanguageProcessing (NeMLaP 2), Ankara, Turkey.Kaszkiel, Marcin and Justin Zobel.
1997.Passage retrieval revisited.
In Proceedingsof the 20th International Conference onResearch and Development in InformationAccess (ACM SIGIR), pages 178?185,Philadelphia, PA.Litman, Diane J. and Rebecca J. Passonneau.1995.
Combining multiple knowledgesources for discourse segmentation.
InProceedings of the 33rd Annual Meeting of theAssociation for Computational Linguistics,pages 108?115, Cambridge, MA.Mani, Inderjeet, David House, MarkMaybury, and Morgan Green.
1997.Towards content-based browsing ofbroadcast news video.
In Mark Maybury,editor, Intelligent Multimedia InformationRetrieval.
AAAI/MIT Press,pages 241?258.Manning, Christopher D. 1998.
Rethinkingtext segmentation models: An informationextraction case study.
Technical ReportSULTRY-98-07-01, University of Sydney.Marcu, Daniel.
2000.
The Theory and Practiceof Discourse Parsing and Summarization.MIT Press.Merlino, Andy, Daryl Morey, and MarkMaybury.
1997.
Broadcast newsnavigation using story segmentation.
InProceedings of the ACM InternationalMultimedia Conference, pages 157?164,Seattle, WA.Mittal, Vibhu, Mark Kantrowitz, JadeGoldstein, and Jaime Carbonell.
1999.Selecting text spans for documentsummaries: Heuristics and metrics.
InProceedings of the 16th Annual Conference onArtificial Intelligence (AAAI ?99),pages 467?473, Orlando, FL.Morris, Jane and Graeme Hirst.
1991.Lexical cohesion computed by thesauralrelations as an indicator of the structure oftext.
Computational Linguistics, 17(1):21?48.Nomoto, Tadashi and Yoshihiko Nitta.
1994.A grammatico-statistical approach todiscourse partitioning.
In Proceedings of the15th International Conference onComputational Linguistics (COLING?94),pages 1145?1150, Kyoto, Japan.Passonneau, Rebecca J. and Diane J. Litman.1993.
Intention-based segmentation:Human reliability and correlation withlinguistic cues.
In Proceedings of the 31stAnnual Meeting of the Association forComputational Linguistics, pages 148?155,Columbus, OH.Ponte, Jay and Bruce Croft.
1997.
Textsegmentation by topic.
In Proceedings of the1st European Conference on Research andAdvanced Technology for Digital Libraries,pages 113?125.Reynar, Jeffrey C. 1994.
An automaticmethod of finding topic boundaries.
InProceedings of the Student Session of the 32ndAnnual Meeting of the Association forComputational Linguistics, pages 331?333,Las Cruces, NM.Richmond, Korin, Andrew Smith, and EinatAmitay.
1997.
Detecting subjectboundaries within text: A languageindependent statistical approach.
InProceedings of the Second Conference onEmpirical Methods in Natural LanguageProcessing, pages 47?54.
Association forComputational Linguistics.Salton, Gerard, James Allan, and ChrisBuckley.
1993.
Approaches to passageretrieval in full text information systems.In Proceedings of the 16th AnnualInternational ACM/SIGIR Conference, pages35Computational Linguistics Volume 28, Number 149?58, Pittsburgh, PA.Salton, Gerard, James Allan, Chris Buckley,and Amit Singhal.
1994.
Automaticanalysis, theme generation, andsummarization of machine-readable texts.Science, 264(5164):1421?1426.van Mulbregt, P., Ira Carp, Larry Gillick,Stephen Lowe, and Jonathan Yamron.1999.
Segmentation of automaticallytranscribed broadcast news text.
InProceedings of the DARPA Broadcast NewsWorkshop, Herndon, VA.Yaari, Yaakov.
1997.
Segmentation ofexpository text by hierarchicalagglomerative clustering.
In RecentAdvances in NLP (RANLP?97), Bulgaria.36
