Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 894?902,Los Angeles, California, June 2010. c?2010 Association for Computational LinguisticsInterpretation and Transformation for Abstracting ConversationsGabriel Murraygabrielm@cs.ubc.caGiuseppe Careninicarenini@cs.ubc.caDepartment of Computer Science, University of British ColumbiaVancouver, CanadaRaymond Ngrng@cs.ubc.caAbstractWe address the challenge of automatically ab-stracting conversations such as face-to-facemeetings and emails.
We focus here onthe stages of interpretation, where sentencesare mapped to a conversation ontology, andtransformation, where the summary contentis selected.
Our approach is fully developedand tested on meeting speech, and we subse-quently explore its application to email con-versations.1 IntroductionThe dominant approach to the challenge of auto-matic summarization has been extraction, where in-formative sentences in a document are identified andconcatenated to form a condensed version of theoriginal document.
Extractive summarization hasbeen popular at least in part because it is a binaryclassification task that lends itself well to machinelearning techniques, and does not require a naturallanguage generation (NLG) component.
There is ev-idence that human abstractors at times use sentencesfrom the source documents nearly verbatim in theirown summaries, justifying this approach to some ex-tent (Kupiec et al, 1995).
Extrinsic evaluations havealso shown that, while extractive summaries may beless coherent than human abstracts, users still findthem to be valuable tools for browsing documents(He et al, 1999; McKeown et al, 2005; Murray etal., 2008).However, these same evaluations also indicatethat concise abstracts are generally preferred byusers and lead to higher objective task scores.
Thelimitation of a cut-and-paste summary is that theend-user does not know why the selected sentencesare important; this can often only be discerned byexploring the context in which each sentence origi-nally appeared.
One possible improvement is to cre-ate structured extracts that represent an increasedlevel of abstraction, where selected sentences aregrouped according to phenomena such as decisions,action items and problems, thereby giving the usermore information on why the sentences are beinghighlighted.
For example, the sentence Let?s go witha simple chip represents a decision.
An even higherlevel of abstraction can be provided by generatingnew text that synthesizes or extrapolates on the in-formation contained in the structured summary.
Forexample, the sentence Sandra and Sue expressednegative opinions about the remote control designcan be coupled with extracted sentences containingthese negative opinions, forming a hybrid summary.Our summarization system ultimately performs bothtypes of abstraction, grouping sentences accordingto various sentence-level phenomena, and generat-ing novel text that describes this content at a higherlevel.In this work we describe the first two componentsof our abstractive summarization system.
In the in-terpretation stage, sentences are mapped to nodesin a conversation ontology by utilizing classifiersrelating to a variety of sentence-level phenomenasuch as decisions, action items and subjective sen-tences.
These classifiers achieve high accuracy byusing a very large feature set integrating conversa-tion structure, lexical patterns, part-of-speech (POS)tags and character n-grams.
In the transformationstage, we select the most informative sentences bymaximizing a function based on the derived ontol-ogy mapping and the coverage of weighted enti-ties mentioned in the conversation.
This transforma-tion component utilizes integer linear programming(ILP) and we compare its performance with severalgreedy selection algorithms.We do not discuss the generation compo-nent of our summarization system in this pa-per.
The transformation component is still ex-894tractive in nature, but the sentences that are se-lected in the transformation stage correspond toobjects in the ontology and the properties link-ing them.
Specifically, these are triples of theform < participant, relation, entity > where aparticipant is a person in the conversation, anentity is an item under discussion, and a relationsuch as positive opinion or action item links the two.This intermediate output enables us to create struc-tured extracts as described above, with the triplesalso acting as input to the downstream NLG com-ponent.We have tested our approach in summarizationexperiments on both meeting and email conversa-tions, where the quality of a sentence is measuredby how effectively it conveys information in a modelabstract summary according to human annotators.On meetings the ILP approach consistently outper-forms several greedy summarization methods.
Akey finding is that emails exhibit markedly varyingconversation structures, and the email threads yield-ing the best summarization results are those that arestructured similarly to meetings.
Other email con-versation structures are less amenable to the currenttreatment and require further investigation and pos-sibly domain adaptation.2 Related ResearchThe view that summarization consists of stages ofinterpretation, transformation and generation waslaid out by Sparck-Jones (1999).
Popular ap-proaches to text extraction essentially collapse inter-pretation and transformation into one step, with gen-eration either being ignored or consisting of post-processing techniques such as sentence compres-sion (Knight and Marcu, 2000; Clarke and Lapata,2006) or sentence merging (Barzilay and McKeown,2005).
In contrast, in this work we clearly separateinterpretation from transformation.The most relevant research to ours is by Klein-bauer et al (2007), similarly focused on meet-ing abstraction.
They create an ontology for theAMI scenario meeting corpus (Carletta et al, 2005),described in Section 5.1.
The system uses topicsegments and topic labels, and for each topic seg-ment in the meeting a sentence is generated that de-scribes the most frequently mentioned content itemsin that topic.
Our systems differ in two major re-spects: their summarization process uses humangold-standard annotations of topic segments, topiclabels and content items from the ontology, whileour summarizer is fully automatic; and the ontologyused by Kleinbauer et al is specific not just to meet-ings but to the AMI scenario meetings, while ourontology applies to conversations in general.While the work by Kleinbauer et al is amongthe earliest research on abstracting multi-party dia-logues, much attention in recent years has been paidto extractive summarization of such conversations,including meetings (Galley, 2006), emails (Rambowet al, 2004; Carenini et al, 2007), telephone con-versations (Zhu and Penn, 2006) and internet relaychats (Zhou and Hovy, 2005).Recent research has addressed the challenges ofdetecting decisions (Hsueh et al, 2007), action items(Purver et al, 2007; Murray and Renals, 2008) andsubjective sentences (Raaijmakers et al, 2008).
Inour work we perform all of these tasks but rely ongeneral conversational features without recourse tomeeting-specific or email-specific features.Our approach to transformation is an adaptationof an ILP sentence selection algorithm described byXie et al (2009).
We describe both ILP approachesin Section 4.3 Interpretation - Ontology MappingSource document interpretation in our system re-lies on a simple conversation ontology.
The ontol-ogy is written in OWL/RDF and contains two coreupper-level classes: Participant and Entity.
Whenadditional information is available about participantroles in a given domain, Participant subclasses suchas ProjectManager can be utilized.
The ontologyalso contains six properties that express relations be-tween the participants and the entities.
For example,the following snippet of the ontology indicates thathasActionItem is a relationship between a meetingparticipant (the property domain) and a discussedentity (the property range).<owl:ObjectProperty rdf:ID="hasActionItem"><rdfs:domain rdf:resource="#Participant"/><rdfs:range rdf:resource="#Entity"/></owl:ObjectProperty>Similar properties exist for decisions, actions,problems, positive-subjective sentences, negative-895subjective sentences and general extractive sen-tences (important sentences that may not match theother categories), all connecting conversation par-ticipants and entities.
The goal is to populate theontology with participant and entity instances froma given conversation and determine their relation-ships.
This involves identifying the important en-tities and classifying the sentences in which theyoccur as being decision sentences, action item sen-tences, etc.Our current definition of entity is simple.
The en-tities in a conversation are noun phrases with mid-range document frequency.
This is similar to thedefinition of concept as defined by Xie et al (Xieet al, 2009), where n-grams are weighted by tf.idfscores, except that we use noun phrases rather thanany n-grams because we want to refer to the enti-ties in the generated text.
We use mid-range doc-ument frequency instead of idf (Church and Gale,1995), where the entities occur in between 10% and90% of the documents in the collection.
In Section 4we describe how we use the entity?s term frequencyto detect the most informative entities.
We do notcurrently attempt coreference resolution for entities;recent work has investigated coreference resolutionfor multi-party dialogues (Muller, 2007; Gupta etal., 2007), but the challenge of resolution on suchnoisy data is highlighted by low accuracy (e.g.
F-measure of 21.21) compared with using well-formedtext (e.g.
monologues).We map sentences to our ontology?s object prop-erties by building numerous supervised classifierstrained on labeled decision sentences, action sen-tences, etc.
A general extractive classifier is alsotrained on sentences simply labeled as important.After predicting these sentence-level properties, weconsider a participant to be linked to an entity ifthe participant mentioned the entity in a sentence inwhich one of these properties is predicted.
We give aspecific example of the ontology mapping using thisexcerpt from the AMI corpus:1.
A: And you two are going to work together ona prototype using modelling clay.2.
A: You?ll get specific instructions from yourpersonal coach.3.
C: Cool.4.
A: Um did we decide on a chip?5.
A: Let?s go with a simple chip.Example entities are italicized.
Sentences 1 and2 are classified as action items.
Sentence 3 is clas-sified as positive-subjective, but because it containsno entities, no < participant, relation, entity >triple can be added to the ontology.
Sentence4 is classified as a decision sentence, and Sen-tence 5 is both a decision sentence and a positive-subjective sentence (because the participant is advo-cating a particular position).
The ontology is pop-ulated by adding all of the sentence entities as in-stances of the Entity class, all of the participantsas instances of the Participant class, and adding< participant, relation, entity > triples for Sen-tences 1, 2, 4 and 5.
For example, Sentence 5 resultsin the following two triples being added to the on-tology:<ProjectManager rdf:ID="participant-A"><hasDecision rdf:resource="#simple-chip"/></ProjectManager><ProjectManager rdf:ID="participant-A"><hasPos rdf:resource="#simple-chip"/></ProjectManager>Elements in the ontology are associated with lin-guistic annotations used by the generation compo-nent of our system; since we do not discuss the gen-eration task here, we presently skip the details of thisaspect of the ontology.
In the following section wedescribe the features used for the ontology mapping.3.1 Feature SetThe interpretation component uses general featuresthat are applicable to any conversation domain.
Thefirst set of features we use for ontology mapping arefeatures relating to conversational structure.
Theseare listed and briefly described in Table 1.
TheSprob and Tprob features measure how terms clus-ter between conversation participants and conver-sation turns.
There are simple features measur-ing sentence length (SLEN, SLEN2) and position(TLOC, CLOC).
Pause-style features indicate howmuch time transpires between the previous turn, thecurrent turn and the subsequent turn (PPAU, SPAU).For email conversations, pause features are based onthe timestamps between consecutive emails.
Lexicalfeatures capture cohesion (CWS) and cosine sim-ilarity between the sentence and the conversation(CENT1, CENT2).
All structural features are nor-malized by document length.896Feature ID DescriptionMXS max Sprob scoreMNS mean Sprob scoreSMS sum of Sprob scoresMXT max Tprob scoreMNT mean Tprob scoreSMT sum of Tprob scoresTLOC position in turnCLOC position in conv.SLEN word count, globally normalizedSLEN2 word count, locally normalizedTPOS1 time from beg.
of conv.
to turnTPOS2 time from turn to end of conv.DOM participant dominance in wordsCOS1 cos. of conv.
splits, w/ SprobCOS2 cos. of conv.
splits, w/ TprobPENT entro.
of conv.
up to sentenceSENT entro.
of conv.
after the sentenceTHISENT entropy of current sentencePPAU time btwn.
current and prior turnSPAU time btwn.
current and next turnBEGAUTH is first participant (0/1)CWS rough ClueWordScoreCENT1 cos. of sentence & conv., w/ SprobCENT2 cos. of sentence & conv., w/ TprobTable 1: Features KeyWhile these features have been found to workwell for generic extractive summarization, we useadditional features for capturing the more specificsentence-level phenomena of this research.?
Character trigrams We derive all of the char-acter trigrams in the collected corpora and in-clude features indicating the presence or ab-sence of each trigram in a given sentence.?
Word bigrams We similarly derive all of theword bigrams in the collected corpora.?
POS bigrams We similarly derive all of thePOS-tag bigrams in the collected corpora.?
Word pairs We consider w1, w2 to be a wordpair if they occur in the same sentence and w1precedes w2.
We derive all of the word pairsin the collected corpora and includes featuresindicating the presence or absence of each wordpair in the given sentence.
This is essentially askip bigram where any amount of interveningmaterial is allowed as long as the words occurin the same sentence.?
POS pairs We calculate POS pairs in the samemanner as word pairs, above.
These are essen-tially skip bigrams for POS tags.?
Varying instantiation ngrams We derive asimplified set of VIN features for these exper-iments.
For each word bigram w1, w2, we fur-ther represent the bigram as p1, w2 and w1, p2so that each pattern consists of a word and aPOS tag.
We include a feature indicating thepresence or absence of each of these varyinginstantiation bigrams.After removing features that occur fewer than fivetimes, we end up with 218,957 total features.4 Transformation - ILP Content SelectionIn the previous section we described how weidentify sentences that link participants and enti-ties through a variety of sentence-level phenom-ena.
Having populated our ontology with thesetriples to form a source representation, we now turnto the task of transforming the source representa-tion to a summary representation, identifying the <participant, relation, entity > triples for whichwe want to generate text.
We adapt a method pro-posed by Xie et al (2009) for extractive sentenceselection.
They propose an ILP approach that cre-ates a summary by maximizing a global objectivefunction:maximize (1 ?
?)
?
?iwici + ?
?
?jujsj (1)subject to?jljsj < L (2)where wi is the tf.idf score for concept i, uj is theweight for sentence j using the cosine similarity tothe entire document, ci is a binary variable indicat-ing whether concept i is selected (with the conceptrepresented by a unique weighted n-gram), sj is abinary variable indicating whether sentence j is se-lected, lj is the length of sentence j and L is thedesired summary length.
The ?
term is used to bal-ance concept and sentence weights.
This method se-lects sentences that are weighted strongly and whichcover as many important concepts as possible.
Asdescribed by Gillick et al (2009), concepts andsentences are tied together by two additional con-straints:?jsjoij ?
ci ?i (3)sjoij ?
ci ?i,j (4)897where oij is the occurence of concept i in sentencej.
These constraints state that a concept can only beselected if it occurs in a sentence that is selected,and that a sentence can only be selected if all of itsconcepts have been selected.We adapt their method in several ways.
As men-tioned in the previous section, we use weighted nounphrases as our entities instead of n-grams.
In ourversion of Equation 1, wi is the tf score of en-tity i (the idf was already used to identify entitiesas described previously).
More importantly, oursentence weight uj is the sum of all the posteriorprobabilities for sentence j derived from the varioussentence-level classifiers.
In other words, sentencesare weighted highly if they correspond to multipleobject properties in the ontology.
To continue theexample from Section 3, the sentence Let?s go withthe simple chip may be selected because it representsboth a decision and a positive-subjective opinion, aswell as containing the entity simple chip which ismentioned frequently in the conversation.We include constraint 3 but not 4; it is possi-ble for a sentence to be extracted even if not allof its entities are.
We know that all the sentencesunder consideration will contain at least one en-tity because sentences with no entities would nothave been mapped to the ontology in the form of< participant, relation, entity > triples in thefirst place.
To begin with, we set the ?
term at 0.75as we are mostly concerned with identifying impor-tant sentences containing multiple links to the on-tology.
In our case L is 20% of the total documentword count.5 Experimental SetupIn this section we describe our conversation cor-pora, the statistical classifiers used, and the evalu-ation metrics employed.5.1 CorporaThese experiments are conducted on both meetingand email conversations, which we describe in turn.5.1.1 The AMI Meetings CorpusFor our meeting summarization experiments, weuse the scenario portion of the AMI corpus (Carlettaet al, 2005), where groups of four participants takepart in a series of four meetings and play roles withina fictitious company.
There are 140 of these meet-ings in total, including a 20 meeting test set contain-ing multiple human summary annotations per meet-ing (the others are annotated by a single individual).We report results on both manual and ASR tran-scripts.
The word error rate for the ASR transcriptsis 38.9%.For the summary annotation, annotators wrote ab-stract summaries of each meeting and extracted sen-tences that best conveyed or supported the informa-tion in the abstracts.
The human-authored abstractseach contain a general abstract summary and threesubsections for ?decisions,?
?actions?
and ?prob-lems?
from the meeting.
A many-to-many mappingbetween transcript sentences and sentences from thehuman abstract was obtained for each annotator.
Ap-proximately 13% of the total transcript sentences areultimately labeled as extracted sentences.
A sen-tence is considered a decision item if it is linked tothe decision portion of the abstract, and action andproblem sentences are derived similarly.For the subjectivity annotation, we use annota-tions of positive-subjective and negative-subjectiveutterances on a subset of 20 AMI meetings (Wil-son, 2008).
Such subjective utterances involvethe expression of a private state, such as a pos-itive/negative opinion, positive/negative argument,and agreement/disagreement.
Of the roughly 20,000total sentences in the 20 AMI meetings, nearly 4000are labeled as positive-subjective and nearly 1300 asnegative-subjective.5.1.2 The BC3 Email CorpusWhile our main experiments focus on the AMImeeting corpus, we follow these up with an inves-tigation into applying our abstractive techniques toemail data.
The BC3 corpus (Ulrich et al, 2008)contains email threads from the World Wide WebConsortium (W3C) mailing list.
The threads fea-ture a variety of topics such as web accessibility andplanning face-to-face meetings.
The annotated por-tion of the mailing list consists of 40 threads.
Thethreads are annotated in the same manner as the AMIcorpus, with three human annotators per thread firstauthoring abstracts and then linking email threadsentences to the abstract sentences.
The corpus alsocontains speech act annotations.
Unlike the AMIcorpus, however, there are no annotations for deci-898sions, actions and problems, an issue addressed later.5.2 ClassifiersFor these experiments we use a maximum entropyclassifier using the liblinear toolkit1 (Fan et al,2008).
For each of the AMI and BC3 corpora, weperform 10-fold cross-validation on the data.
In allexperiments we apply a 20% compression rate interms of the total document word count.5.3 EvaluationWe evaluate the various classifiers described in Sec-tion 3 using the ROC curve and the area under thecurve (AUROC), where a baseline AUROC is 0.5and an ideal classifier approaches 1.To evaluate the content selection in the transfor-mation stage, we use weighted recall.This evaluationmetric is based on the links between extracted sen-tences and the human gold-standard abstracts, withthe underlying motivation being that sentences withmore links to the human abstract are generally moreinformative, as they provide the content on which aneffective abstract summary should be built.
If M isthe number of sentences selected in the transforma-tion step, O is the total number of sentences in thedocument, and N is the number of annotators, thenWeighted Recall is given byrecall =?Mi=1?Nj=1 L(si, aj)?Oi=1?Nj=1 L(si, aj)where L(si, aj) is the number of links for a sen-tence si according to annotator aj .
We can com-pare machine performance with human performancein the following way.
For each annotator, we ranktheir sentences from most-linked to least-linked andselect the best sentences until we reach the sameword count as our selections.
We then calculate theirweighted recall score by using the other N-1 annota-tions, and then average over all N annotators to getan average human performance.
We report all trans-formation scores normalized by human performancefor that dataset.6 ResultsIn this section we present results for our interpreta-tion and transformation components.1http://www.csie.ntu.edu.tw/ cjlin/liblinear/6.1 Interpretation: MeetingsFigure 1 shows the ROC curves for the sentence-level classifiers applied to manual transcripts.
Onboth manual and ASR transcripts, the classifierswith the largest AUROCs are the action item andgeneral extractive classifiers.
Action item sentencescan be detected very well with this feature set, withthe classifier having an AUROC of 0.92 on man-ual transcripts and 0.93 on ASR, a result compa-rable to previous findings of 0.91 and 0.93 (Mur-ray and Renals, 2008) obtained using a speech-specific feature set.
General extractive classificationis also similar to other state-of-the-art extraction ap-proaches on spoken data using speech features (Zhuand Penn, 2006)2 with an AUROC of 0.87 on man-ual and 0.85 on ASR.
Decision sentences can alsobe detected quite well, with AUROCs of 0.81 and0.77.
Positive-subjective, negative-subjective andproblem sentences are the most difficult to detect,but the classifiers still give credible performancewith AUROCs of approximately 0.76 for manualand 0.70-0.72 for ASR.00.20.40.60.810  0.2  0.4  0.6  0.8  1TPFPactionsdecisionsproblemspositive-subjectivenegative-subjectiveextractiverandomFigure 1: ROC Curves for Ontology Mapping Classifiers(Manual Transcripts)6.2 Transformation: MeetingsIn this section we present the weighted recall scoresfor the sentences selected using the ILP method de-scribed in Section 4.
Remember, weighted recallmeasures how useful these sentences would be ingenerating sentences for an abstract summary.
Wealso assess the performance of three baseline sum-marizers operating at the same compression level.2Based on visual inspection of their reported best ROC curve899The simplest baseline (GREEDY) selects sentencesby ranking the posterior probabilites output by thegeneral extractive classifier.
The second baseline(CLASS COMBO) averages the posterior proba-bilites output by all the classifiers and ranks sen-tences from best to worst.
The third baseline (RE-TRAIN) uses the posterior probability outputs of allthe classifiers (except for the extractive classifier) asnew feature inputs for the general extractive classi-fier.0.50.60.70.80.91GreedyClass ComboRetrainILPWeightedRecall,NormalizedmanualASRFigure 2: Weighted Recall Scores for AMI MeetingsFigure 2 shows the weighted recall scores, nor-malized by human performance, for all approacheson both manual and ASR transcripts.
On man-ual transcripts, the ILP approach (0.76) is betterthan GREEDY (0.71) with a marginally significantdifference (p=0.07) and is significantly better thanCLASS COMBO and RETRAIN (both 0.68) ac-cording to t-test (p < 0.05) .
For ASR transcripts,the ILP approach is significantly better than all otherapproaches (p < 0.05).
Xie et al (2009) reportedROUGE-1 F-measures on a different meeting cor-pus, and our ROUGE-1 scores are in the same rangeof 0.64-0.69 (they used 18% compression ratio).6.3 Interpretation: EmailsWe applied the same summarization method to the40 BC3 email threads, with contrasting results.
Be-cause the BC3 corpus does not currently contain an-notations for decisions, actions and problems, wesimply ran the AMI-trained models over the datafor those three phenomena.
We can assess theperformance of the extractive, positive-subjectiveand negative-subjective classifiers by examining theROC curves displayed in Figure 3.
Both the generalextractive and negative-subjective classifiers haveAUROCs of around 0.75.
The positive-subjectiveclassifier initially has the worst performance withan AUROC of 0.66, but we found that positive-subjective performance increased dramatically to anAUROC of 0.77 when we used only conversationalfeatures and not word bigrams, character trigrams orPOS tags.00.20.40.60.810  0.2  0.4  0.6  0.8  1TPFPpositive-subjectivenegative-subjectiveextractiverandomFigure 3: ROC Curves for Ontology Mapping Classifiers(BC3 Corpus)6.4 Transformation: EmailsIf we examine the weighted recall scores in Fig-ure 4 we see that the ILP approach is worse thanthe greedy summarizers on the BC3 dataset.
How-ever, the differences are not significant between ILPand COMBO CLASS (p=0.15) and only marginallysignificant compared with RETRAIN and GREEDY(both p=0.08).
The performance of the ILP approachvaries greatly across email threads.
The top 15threads (out of 40) yield ILP weighted recall scoresthat are on par with human performance, while theworst 15 are half that.6.4.1 Email Corpus AnalysisDue to the large discrepancy in performance onBC3 emails, we conducted additional experimentsfor error analysis.
We first explored whether wecould build a classifier that could discriminate thebest 15 emails from the worst 15 emails in terms ofweighted recall scores with the ILP approach, to de-termine whether there are certain features that cor-relate with good performance.
Using the same fea-9000.50.60.70.80.91GreedyClass ComboRetrainILPWeightedRecall,NormalizedFigure 4: Weighted Recall Scores for BC3 Threadstures described in Section 3.1, we built a logistic re-gression classifier on the two classes and found thatthey can be discriminated quite well (80% accuracyon an approximately balanced dataset) and that theconversation structure features are the most usefulfor discerning them.
Table 2 shows the weightedrecall scores and several conversation features thatwere weighted most highly by the logistic regres-sion model.
In particular, we found that the emailthreads that yielded good performance tended to fea-ture more active participants (# Participants), werenot dominated by a single individual (BEGAUTH),and featured a higher number of turns (# Turns)that followed each other in quick succession withoutlong pauses (PPAU, pause as percentage of conver-sation length).
In other words, these emails werestructured more similarly to meetings.
Note thatsince we normalize weighted recall by human per-formance, it is possible to have a weighted recallscore higher than 1.
On the 15 best threads, our sys-tem achieves human-level performance.
Because weused AMI-trained models for detecting decisions,actions and problems in the BC3 data, it is not sur-prising that performance was better on those emailsstructured similarly to meetings.
All of this indicatesthat there are many different types of emails and thatwe will have to focus on improving performance onemails that differ markedly in structure.7 ConclusionWe have presented two components of an abstractiveconversation summarization system.
The interpreta-tion component is used to populate a simple conver-Metric Worst 15 Best 15Weighted Recall 0.49 1.05# Turns 6.27 6.73# Participants 4.67 5.4PPAU 0.18 0.12BEGAUTH 0.31 0.18Table 2: Selected Email Features, Averagedsation ontology where conversation participants andentities are linked by object properties such as deci-sions, actions and subjective opinions.
For this stepwe show that highly accurate classifiers can be builtusing a large set of features not specific to any con-versation modality.In the transformation step, a summary is cre-ated by maximizing a function relating sentenceweights and entity weights, with the sentenceweights determined by the sentence-ontology map-ping.
Our evaluation shows that the sentences weselect are highly informative to generate abstractsummaries, and that our content selection methodoutperforms several greedy selection approaches.The system described thus far may appear extrac-tive in nature, as the transformation step is iden-tifying informative sentences in the conversation.However, these selected sentences correspond to< participant, relation, entity > triples in theontology, for which we can subsequently gener-ate novel text by creating linguistic annotations ofthe conversation ontology (Galanis and Androut-sopolous, 2007).
Even without the generation step,the approach described above allows us to createstructured extracts by grouping sentences accordingto specific phenomena such as action items and de-cisions.
The knowledge represented by the ontologyenables us to significantly improve sentence selec-tion according to intrinsic measures and to generatestructured output that we hypothesize will be moreuseful to an end user compared with a generic un-structured extract.Future work will focus on the generation compo-nent and on applying the summarization system toconversations in other modalities such as blogs andinstant messages.
Based on the email error analysis,we plan to pursue domain adaptation techniques toimprove performance on different types of emails.901ReferencesR.
Barzilay and K. McKeown.
2005.
Sentence fusion formultidocument news summarization.
ComputationalLinguistics, 31(3):297?328.G.
Carenini, R. Ng, and X. Zhou.
2007.
Summarizingemail conversations with clue words.
In Proc.
of ACMWWW 07, Banff, Canada.J.
Carletta, S. Ashby, S. Bourban, M. Flynn, M. Guille-mot, T. Hain, J. Kadlec, V. Karaiskos, W. Kraaij,M.
Kronenthal, G. Lathoud, M. Lincoln, A. Lisowska,I.
McCowan, W. Post, D. Reidsma, and P. Well-ner.
2005.
The AMI meeting corpus: A pre-announcement.
In Proc.
of MLMI 2005, Edinburgh,UK, pages 28?39.K.
Church and W. Gale.
1995.
Inverse document fre-quency IDF: A measure of deviation from poisson.
InProc.
of the Third Workshop on Very Large Corpora,pages 121?130.J.
Clarke and M. Lapata.
2006.
Constraint-basedsentence compression: An integer programming ap-proach.
In Proc.
of COLING/ACL 2006, pages 144?151.R.-E.
Fan, K.-W. Chang, C.-J.
Hsieh, X.-R. Wang, andC.-J.
Lin.
2008.
Liblinear: A library for large linearclassification.
Journal of Machine Learning Research,9:1871?1874.D.
Galanis and I. Androutsopolous.
2007.
Generatingmultilingual descriptions from linguistically annotatedowl ontologies: the naturalowl system.
In Proc.
ofENLG 2007, Schloss Dagstuhl, Germany.M.
Galley.
2006.
A skip-chain conditional randomfield for ranking meeting utterances by importance.
InProc.
of EMNLP 2006, Sydney, Australia, pages 364?372.D.
Gillick, K. Riedhammer, B. Favre, and D. Hakkani-Tu?r.
2009.
A global optimization framework for meet-ing summarization.
In Proc.
of ICASSP 2009, Taipei,Taiwan.S.
Gupta, J. Niekrasz, M. Purver, and D. Jurafsky.
2007.Resolving ?You?
in multi-party dialog.
In Proc.
ofSIGdial 2007, Antwerp, Belgium.L.
He, E. Sanocki, A. Gupta, and J. Grudin.
1999.
Auto-summarization of audio-video presentations.
In Proc.of ACM MULTIMEDIA ?99, Orlando, FL, USA, pages489?498.P-Y.
Hsueh, J. Kilgour, J. Carletta, J. Moore, and S. Re-nals.
2007.
Automatic decision detection in meetingspeech.
In Proc.
of MLMI 2007, Brno, Czech Repub-lic.K.
Spa?rck Jones.
1999.
Automatic summarizing: Factorsand directions.
In I. Mani and M. Maybury, editors,Advances in Automatic Text Summarization, pages 1?12.
MITP.T.
Kleinbauer, S. Becker, and T. Becker.
2007.
Com-bining multiple information layers for the automaticgeneration of indicative meeting abstracts.
In Proc.
ofENLG 2007, Dagstuhl, Germany.K.
Knight and D. Marcu.
2000.
Statistics-based summa-rization - step one: Sentence compression.
In Proc.
ofAAAI 2000, Austin, Texas, USA, pages 703?710.J.
Kupiec, J. Pederson, and F. Chen.
1995.
A trainabledocument summarizer.
In Proc.
of the 18th Annual In-ternational ACM SIGIR Conference on Research andDevelopment in Information Retrieval.
Seattle, Wash-ington, USA, pages 68?73.K.
McKeown, J. Hirschberg, M. Galley, and S. Maskey.2005.
From text to speech summarization.
In Proc.
ofICASSP 2005, Philadelphia, USA, pages 997?1000.C.
Muller.
2007.
Resolving It, This and That in un-restricted multi-party dialog.
In Proc.
of ACL 2007,Prague, Czech Republic.G.
Murray and S. Renals.
2008.
Detecting action itemsin meetings.
In Proc.
of MLMI 2008, Utrecht, theNetherlands.G.
Murray, T. Kleinbauer, P. Poller, S. Renals, T. Becker,and J. Kilgour.
2008.
Extrinsic summarization evalu-ation: A decision audit task.
In Proc.
of MLMI 2008,Utrecht, the Netherlands.M.
Purver, J. Dowding, J. Niekrasz, P. Ehlen, andS.
Noorbaloochi.
2007.
Detecting and summariz-ing action items in multi-party dialogue.
In Proc.
ofthe 9th SIGdial Workshop on Discourse and Dialogue,Antwerp, Belgium.S.
Raaijmakers, K. Truong, and T. Wilson.
2008.
Multi-modal subjectivity analysis of multiparty conversation.In Proc.
of EMNLP 2008, Honolulu, HI, USA.O.
Rambow, L. Shrestha, J. Chen, and C. Lauridsen.2004.
Summarizing email threads.
In Proc.
of HLT-NAACL 2004, Boston, USA.J.
Ulrich, G. Murray, and G. Carenini.
2008.
A publiclyavailable annotated corpus for supervised email sum-marization.
In Proc.
of AAAI EMAIL-2008 Workshop,Chicago, USA.T.
Wilson.
2008.
Annotating subjective content in meet-ings.
In Proc.
of LREC 2008, Marrakech, Morocco.S.
Xie, B. Favre, D. Hakkani-Tu?r, and Y. Liu.
2009.Leveraging sentence weights in a concept-based op-timization framework for extractive meeting summa-rization.
In Proc.
of Interspeech 2009, Brighton, Eng-land.L.
Zhou and E. Hovy.
2005.
Digesting virtual ?geek?culture: The summarization of technical internet relaychats.
In Proc.
of ACL 2005, Ann Arbor, MI, USA.X.
Zhu and G. Penn.
2006.
Summarization of spon-taneous conversations.
In Proc.
of Interspeech 2006,Pittsburgh, USA, pages 1531?1534.902
