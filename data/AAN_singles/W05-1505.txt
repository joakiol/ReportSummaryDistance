Proceedings of the Ninth International Workshop on Parsing Technologies (IWPT), pages 42?52,Vancouver, October 2005. c?2005 Association for Computational LinguisticsCorrective Modeling for Non-Projective Dependency ParsingKeith HallCenter for Language and Speech ProcessingJohns Hopkins UniversityBaltimore, MD 21218keith hall@jhu.eduVa?clav Nova?kInstitute of Formal and Applied LinguisticsCharles UniversityPrague, Czech Republicnovak@ufal.mff.cuni.czAbstractWe present a corrective model for recov-ering non-projective dependency struc-tures from trees generated by state-of-the-art constituency-based parsers.
The con-tinuity constraint of these constituency-based parsers makes it impossible forthem to posit non-projective dependencytrees.
Analysis of the types of depen-dency errors made by these parsers on aCzech corpus show that the correct gov-ernor is likely to be found within a localneighborhood of the governor proposedby the parser.
Our model, based on aMaxEnt classifier, improves overall de-pendency accuracy by .7% (a 4.5% reduc-tion in error) with over 50% accuracy fornon-projective structures.1 IntroductionStatistical parsing models have been shown tobe successful in recovering labeled constituencies(Collins, 2003; Charniak and Johnson, 2005; Roarkand Collins, 2004) and have also been shown tobe adequate in recovering dependency relationships(Collins et al, 1999; Levy and Manning, 2004;Dubey and Keller, 2003).
The most successful mod-els are based on lexicalized probabilistic contextfree grammars (PCFGs) induced from constituency-based treebanks.
The linear-precedence constraintof these grammars restricts the types of dependencystructures that can be encoded in such trees.1 Ashortcoming of the constituency-based paradigm forparsing is that it is inherently incapable of repre-senting non-projective dependencies trees (we de-fine non-projectivity in the following section).
Thisis particularly problematic when parsing free word-order languages, such as Czech, due to the frequencyof sentences with non-projective constructions.In this work, we explore a corrective model whichrecovers non-projective dependency structures bytraining a classifier to select correct dependencypairs from a set of candidates based on parses gen-erated by a constituency-based parser.
We chose touse this model due to the observations that the de-pendency errors made by the parsers are generallylocal errors.
For the nodes with incorrect depen-dency links in the parser output, the correct gov-ernor of a node is often found within a local con-text of the proposed governor.
By considering al-ternative dependencies based on local deviations ofthe parser output we constrain the set of candidategovernors for each node during the corrective proce-dure.
We examine two state-of-the-art constituency-based parsers in this work: the Collins Czech parser(1999) and a version of the Charniak parser (2001)that was modified to parse Czech.Alternative efforts to recover dependency struc-ture from English are based on reconstructing themovement traces encoded in constituency trees(Collins, 2003; Levy and Manning, 2004; Johnson,2002; Dubey and Keller, 2003).
In fact, the fea-1In order to correctly capture the dependency structure, co-indexed movement traces are used in a form similar to govern-ment and Binding theory, GPSG, etc.42wcwawbb cawcb cawa wbwcwawbb caFigure 1: Examples of projective and non-projective trees.
The trees on the left and center are both projec-tive.
The tree on the right is non-projective.tures we use in the current model are similar to thoseproposed by Levy and Manning (2004).
However,the approach we propose discards the constituencystructure prior to the modeling phase; we model cor-rective transformations of dependency trees.The technique proposed in this paper is similar tothat of recent parser reranking approaches (Collins,2000; Charniak and Johnson, 2005); however, whilereranking approaches allow a parser to generate alikely candidate set according to a generative model,we consider a set of candidates based on local per-turbations of the single most likely tree generated.The primary reason for such an approach is that weallow dependency structures which would never behypothesized by the parser.
Specifically, we allowfor non-projective dependencies.The corrective algorithm proposed in this papershares the motivation of the transformation-basedlearning work (Brill, 1995).
We do consider localtransformations of the dependency trees; however,the technique presented here is based on a generativemodel that maximizes the likelihood of good depen-dents.
We consider a finite set of local perturbationsof the tree and use a fixed model to select the besttree by independently choosing optimal dependencylinks.In the remainder of the paper we provide a defini-tion of a dependency tree and the motivation for us-ing such trees as well as a description of the particu-lar dataset that we use in our experiments, the PragueDependency Treebank (PDT).
In Section 3 we de-scribe the techniques used to adapt constituency-based parsers to train from and generate dependencytrees.
Section 4 describes corrective modeling asused in this work and Section 4.2 describes the par-ticular features with which we have experimented.Section 5 presents the results of a set of experimentswe performed on data from the PDT.2 Syntactic Dependency Trees and thePrague Dependency TreebankA dependency tree is a set of nodes ?
={w0, w1, .
.
.
, wk} where w0 is the imaginary rootnode2 and a set of dependency links G ={g1, .
.
.
, gk} where gi is an index into ?
represent-ing the governor of wi.
In other words g3 = 1 in-dicates that the governor of w3is w1.
Finally, everynode has exactly one governor except for w0, whichhas no governor (the tree constraints).3 The index ofthe nodes represents the surface order of the nodesin the sequence (i.e., wi precedes wj in the sentenceif i < j).A tree is projective if for every three nodes: wa,wb, and wc where a < b < c; if wa is governed bywc then wb is transitively governed by wc or if wcis governed by wa then wb is transitively governedby wa.4 Figure 1 shows examples of projective andnon-projective trees.
The rightmost tree, which isnon-projective, contains a subtree consisting of waand wc but not wb; however, wb occurs between waand wc in the linear ordering of the nodes.
Projec-tivity in a dependency tree is akin to the continuityconstraint in a constituency tree; such a constraint is2The imaginary root node simplifies notation.3The dependency structures here are very similar to thosedescribed by Mel?c?uk (1988); however the nodes of the depen-dency trees discussed in this paper are limited to the words ofthe sentence and are always ordered according to the surfaceword-order.4Node wais said to transitively govern node wbif wbis adescendant of wain the dependency tree.43implicitly imposed by trees generated from contextfree grammars (CFGs).Strict word-order languages, such as English, ex-hibit non-projective dependency structures in a rel-atively constrained set of syntactic configurations(e.g., right-node raising).
Traditionally, these move-ments are encoded in syntactic analyses as traces.In languages with free word-order, such as Czech,constituency-based representations are overly con-strained (Sgall et al, 1986).
Syntactic dependencytrees encode syntactic subordination relationshipsallowing the structure to be non-specific about theunderlying deep representation.
The relationshipbetween a node and its subordinates expresses asense of syntactic (functional) entailment.In this work we explore the dependency struc-tures encoded in the Prague Dependency Treebank(Hajic?, 1998; Bo?hmova?
et al, 2002).
The PDT 1.0analytical layer is a set of Czech syntactic depen-dency trees; the nodes of which contain the wordforms, morphological features, and syntactic anno-tations.
These trees were annotated by hand andare intended as an intermediate stage in the annota-tion of the Tectogrammatical Representation (TR),a deep-syntactic or syntacto-semantic theory of lan-guage (Sgall et al, 1986).
All current automatictechniques for generating TR structures are based onsyntactic dependency parsing.When evaluating the correctness of dependencytrees, we only consider the structural relationshipsbetween the words of the sentence (unlabeled depen-dencies).
However, the model we propose containsfeatures that are considered part of the dependencyrather than the nodes in isolation (e.g., agreementfeatures).
We do not propose a model for correctlylabeling dependency structures in this work.3 Constituency Parsing for DependencyTreesA pragmatic justification for using constituency-based parsers in order to predict dependency struc-tures is that currently the best Czech dependency-tree parser is a constituency-based parser (Collins etal., 1999; Zeman, 2004).
In fact both Charniak?sand Collins?
generative probabilistic models con-tain lexical dependency features.5 From a gener-ative modeling perspective, we use the constraintsimposed by constituents (i.e., projectivity) to enablethe encapsulation of syntactic substructures.
This di-rectly leads to efficient parsing algorithms such asthe CKY algorithm and related agenda-based pars-ing algorithms (Manning and Schu?tze, 1999).
Addi-tionally, this allows for the efficient computation ofthe scores for the dynamic-programming state vari-ables (i.e., the inside and outside probabilities) thatare used in efficient statistical parsers.
The computa-tional complexity advantages of dynamic program-ming techniques along with efficient search tech-niques (Caraballo and Charniak, 1998; Klein andManning, 2003) allow for richer predictive modelswhich include local contextual information.In an attempt to extend a constituency-based pars-ing model to train on dependency trees, Collinstransforms the PDT dependency trees into con-stituency trees (Collins et al, 1999).
In order toaccomplish this task, he first normalizes the treesto remove non-projectivities.
Then, he creates ar-tificial constituents based on the parts-of-speech ofthe words associated with each dependency node.The mapping from dependency tree to constituencytree is not one-to-one.
Collins describes a heuristicfor choosing trees that work well with his parsingmodel.3.1 Training a Constituency-based ParserWe consider two approaches to creating projec-tive trees from dependency trees exhibiting non-projectivities.
The first is based on word-reorderingand is the model that was used with the Collinsparser.
This algorithm identifies non-projectivestructures and deterministically reorders the wordsof the sentence to create projective trees.
An alter-native method, used by Charniak in the adaptationof his parser for Czech6 and used by Nivre and Nils-son (2005), alters the dependency links by raisingthe governor to a higher node in the tree whenever5Bilexical dependencies are components of both the Collinsand Charniak parsers and effectively model the types of syntac-tic subordination that we wish to extract in a dependency tree.
(Bilexical models were also proposed by Eisner (Eisner, 1996)).In the absence of lexicalization, both parsers have dependencyfeatures that are encoded as head-constituent to sibling features.6This information was provided by Eugene Charniak in apersonal communication.44Density0.00.20.40.60.81.00.005 0.006 0.020.8430.0840.023 0.009 0.005 0.004less ?2 ?1 1 2 3 4 5 moreDensity0.00.20.40.60.81.00.005 0.006 0.0220.8240.0920.029 0.012 0.005 0.005less ?2 ?1 1 2 3 4 5 more(a)Charniak (b)CollinsFigure 2: Statistical distribution of correct governor positions in the Charniak (left) and Collins (right) parser output of parsedPDT development data.a non-projectivity is observed.
The trees are thentransformed into Penn Treebank style constituen-cies using the technique described in (Collins et al,1999).Both of these techniques have advantages and dis-advantages which we briefly outline here:Reordering The dependency structure is preserved,but the training procedure will learn statisticsfor structures over word-strings that may not bepart of the language.
The parser, however, maybe capable of constructing parses for any stringof words if a smoothed grammar is being used.Governor?Raising The dependency structure iscorrupted leading the parser to incorporate ar-bitrary dependency statistics into the model.However, the parser is trained on true sen-tences, the words of which are in the correctlinear order.
We expect the parser to predictsimilar incorrect dependencies when sentencessimilar to the training data are observed.Although the results presented in (Collins et al,1999) used the reordering technique, we have exper-imented with his parser using the governor?raisingtechnique and observe an increase in dependency ac-curacy.
For the remainder of the paper, we assumethe governor?raising technique.The process of generating dependency trees fromparsed constituency trees is relatively straight-forward.
Both the Collins and Charniak parsers pro-vide head-word annotation on each constituent.
Thisis precisely the information that we encode in an un-labeled dependency tree, so the dependency struc-ture can simply be extracted from the parsed con-stituency trees.
Furthermore, the constituency labelscan be used to identify the dependency labels; how-ever, we do not attempt to identify correct depen-dency labels in this work.3.2 Constituency-based errorsWe now discuss a quantitative measure for the typesof dependency errors made by constituency-basedparsing techniques.
For node wi and the correct gov-ernor wg?ithe distance between the two nodes in thehypothesized dependency tree is:dist(wi, wg?i)=????
?d(wi, wg?i) iff wg?iis ancestor of wid(wi, wg?i) iff wg?iis sibling/cousin of wi?d(wi, wg?i) iff wg?iis descendant of wiAncestor, sibling, cousin, and descendant have thestandard interpretation in the context of a tree.
Thedependency distance d(wi, wg?i) is the minimumnumber of dependency links traversed on the undi-rected path from wi to wg?iin the hypothesized de-pendency tree.
The definition of the dist functionmakes a distinction between paths through the par-ent of wi (positive values) and paths through chil-45CORRECT(W )1 Parse sentence W using the constituency-based parser2 Generate a dependency structure from the constituency tree3 for wi ?
W4 do for wc ?
N (wghi) // Local neighborhood of proposed governor5 do l(c) ?
P (g?i = c|wi,N (wghi))6 g?i ?
arg maxc l(c) // Pick the governor in which we are most confidentTable 1: Corrective Modeling Proceduredren of wi (negative values).
We found that a vastmajority of the correct governors were actually hy-pothesized as siblings or grandparents (a dist valuesof 2) ?
an extreme local error.Figure 2 shows a histogram of the fraction ofnodes whose correct governor was within a particu-lar dist in the hypothesized tree.
A dist of 1 indicatesthe correct governor was selected by the parser; inthese graphs, the density at dist = 1 (on the x axis)shows the baseline dependency accuracy of eachparser.
Note that if we repaired only the nodes thatare within a dist of 2 (grandparents and siblings),we can recover more than 50% of the incorrect de-pendency links (a raw accuracy improvement of upto 9%).
We believe this distribution to be indirectlycaused by the governor raising projectivization rou-tine.
In the cases where non-projective structurescan be repaired by raising the node?s governor to itsparent, the correct governor becomes a sibling of thenode.4 Corrective ModelingThe error analysis of the previous section suggeststhat by looking only at a local neighborhood of theproposed governor in the hypothesized trees, we cancorrect many of the incorrect dependencies.
Thisfact motivates the corrective modeling procedureemployed here.Table 1 presents the pseudo-code for the correc-tive procedure.
The set gh contains the indices ofgovernors as predicted by the parser.
The set of gov-ernors predicted by the corrective procedure is de-noted as g?
.
The procedure independently correctseach node of the parsed trees meaning that thereis potential for inconsistent governor relationshipsto exist in the proposed set; specifically, the result-ing dependency graph may have cycles.
We em-ploy a greedy search to remove cycles when they arepresent in the output graph.The final line of the algorithm picks the governorin which we are most confident.
We use the correct-governor classification likelihood,P (g?i = j|wi,N (wghi)), as a measure of the confi-dence that wc is the correct governor of wi wherethe parser had proposed wghias the governor.
In ef-fect, we create a decision list using the most likelydecision if we can (i.e., there are no cycles).
If thedependency graph resulting from the most likely de-cisions does not result in a tree, we use the decisionlists to greedily select the tree for which the productof the independent decisions is maximal.Training the corrective model requires pairs ofdependency trees; each pair contains a manually-annotated tree (i.e., the gold standard tree) and a treegenerated by the parser.
This data is trivially trans-formed into per-node samples.
For each node wi inthe tree, there are |N (wghi)| samples; one for eachgovernor candidate in the local neighborhood.One advantage to the type of corrective algorithmpresented here is that it is completely disconnectedfrom the parser used to generate the tree hypotheses.This means that the original parser need not be sta-tistical or even constituency based.
What is criticalfor this technique to work is that the distribution ofdependency errors be relatively local as is the casewith the errors made by the Charniak and Collinsparsers.
This can be determined via data analysisusing the dist metric.
Determining the size of the lo-cal neighborhood is data dependent.
If subordinatenodes are considered as candidate governors, then amore robust cycle removal technique is be required.464.1 MaxEnt EstimationWe have chosen a MaxEnt model to estimate thegovernor distributions, P (g?i = j|wi,N (wghi)).
Inthe next section we outline the feature set with whichwe have experimented, noting that the features areselected based on linguistic intuition (specificallyfor Czech).
We choose not to factor the feature vec-tor as it is not clear what constitutes a reasonablefactorization of these features.
For this reason weuse the MaxEnt estimator which provides us withthe flexibility to incorporate interdependent featuresindependently while still optimizing for likelihood.The maximum entropy principle states that wewish to find an estimate of p(y|x) ?
C that maxi-mizes the entropy over a sample set X for some setof observations Y , where x ?
X is an observationand y ?
Y is a outcome label assigned to that obser-vation,H(p) ?
??x?X,y?Yp?
(x)p(y|x) log p(y|x)The set C is the candidate set of distributions fromwhich we wish to select p(y|x).
We define this setas the p(y|x) that meets a feature-based expectationconstraint.
Specifically, we want the expected countof a feature, f(x, y), to be equivalent under the dis-tribution p(y|x) and under the observed distributionp?(y|x).?x?X,y?Yp?
(x)p(y|x)fi(x, y)=?x?X,y?Yp?(x)p?
(y|x)fi(x, y)fi(x, y) is a feature of our model with which wecapture correlations between observations and out-comes.
In the following section, we describe a set offeatures with which we have experimented to deter-mine when a word is likely to be the correct governorof another word.We incorporate the expected feature-count con-straints into the maximum entropy objective usingLagrange multipliers (additionally, constraints areadded to ensure the distributions p(y|x) are consis-tent probability distributions):H(p)+?i?i?x?X,y?Y(p?
(x)p(y|x)fi(x, y)?p?(x)p?
(y|x)fi(x, y))+ ?
?y?Yp(y|x) ?
1Holding the ?i?s constant, we compute the uncon-strained maximum of the above Lagrangian form:p?
(y|x) =1Z?
(x)exp(?i?ifi(x, y))Z?
(x) =?y?Yexp(?i?ifi(x, y))giving us the log-linear form of the distributionsp(y|x) in C (Z is a normalization constant).
Finally,we compute the ?i?s that maximize the objectivefunction:??x?Xp?
(x) log Z?
(x) +?i?ip?
(x, y)fi(x, y)A number of algorithms have been proposed to ef-ficiently compute the optimization described in thisderivation.
For a more detailed introduction to max-imum entropy estimation see (Berger et al, 1996).4.2 Proposed ModelGiven the above formulation of the MaxEnt estima-tion procedure, we define features over pairs of ob-servations and outcomes.
In our case, the observa-tions are simply wi, wc, and N (wghi) and the out-come is a binary variable indicating whether c = g?i(i.e., wc is the correct governor).
In order to limitthe dimensionality of the feature space, we considerfeature functions over the outcome, the current nodewi, the candidate governor node wc and the nodeproposed as the governor by the parser wghi.Table 2 describes the general classes of featuresused.
We write Fi to indicate the form of the currentchild node, Fc for the form of the candidate, and Fgas the form of the governor proposed by the parser.A combined feature is denoted as LiTc and indicateswe observed a particular lemma for the current nodewith a particular tag of the candidate.47Feature Type Id DescriptionForm F the fully inflected word form as it appears in the dataLemma L the morphologically reduced lemmaMTag T a subset of the morphological tag as described in (Collins et al, 1999)POS P major part-of-speech tag (first field of the morphological tag)ParserGov G true if candidate was proposed as governor by parserChildCount C the number of childrenAgreement A(x, y) check for case/number agreement between word x and yTable 2: Description of the classes of features usedIn all models, we include features containing theform, the lemma, the morphological tag, and theParserGov feature.
We have experimented with dif-ferent sets of feature combinations.
Each combina-tion set is intended to capture some intuitive linguis-tic correlation.
For example, the feature componentLiTc will fire if a particular child?s lemma Li is ob-served with a particular candidate?s morphologicaltag Tc.
This feature is intended to capture phenom-ena surrounding particles; for example, in Czech,the governor of the reflexive particle se will likelybe a verb.4.3 Related WorkRecent work by Nivre and Nilsson introduces a tech-nique where the projectivization transformation isencoded in the non-terminals of constituents dur-ing parsing (Nivre and Nilsson, 2005).
This al-lows for a deterministic procedure that undoes theprojectivization in the generated parse trees, creat-ing non-projective structures.
This technique couldbe incorporated into a statistical parsing frame-work, however we believe the sparsity of such non-projective configurations may be problematic whenusing smoothed backed-off grammars.
We suspectthat the deterministic procedure employed by Nivreand Nilsson enables their parser to greedily considernon-projective constructions when possible.
Thismay also explain the relatively low overall perfor-mance of their parser.A primary difference between the Nivre and Nils-son approach and what we propose in this paper isthat of determining the projectivization procedure.While we exploit particular side-effects of the pro-jectivization procedure, we do not assume any par-ticular algorithm.
Additionally, we consider trans-formations for all dependency errors where theirtechnique explicitly addresses non-projectivity er-rors.We mentioned above that our approach appears tobe similar to that of reranking for statistical parsing(Collins, 2000; Charniak and Johnson, 2005).
Whileit is true that we are improving upon the output of theautomatic parser, we are not considering multiple al-ternate parses.
Instead, we consider a complete setof alternate trees that are minimal perturbations ofthe best tree generated by the parser.
In the contextof dependency parsing, we do this in order to gen-erate structures that constituency-based parsers areincapable of generating (i.e., non-projectivities).Recent work by Smith and Eisner (2005) on con-trastive estimation suggests similar techniques togenerate local neighborhoods of a parse; however,the purpose in their work is to define an approxi-mation to the partition function for log-linear esti-mation (i.e., the normalization factor in a MaxEntmodel).5 Empirical ResultsIn this section we report results from experiments onthe PDT Czech dataset.
Approximately 1.9% of thewords?
dependencies are non-projective in version1.0 of this corpus and these occur in 23.2% of thesentences (Hajic?ova?
et al, 2004).
We used the stan-dard training, development, and evaluation datasetsdefined in the PDT documentation for all experi-ments.7 We use Zhang Lee?s implementation of the7We have used PDT 1.0 (2002) data for the Charniak experi-ments and PDT 2.0 (2005) data for the Collins experiments.
Weuse the most recent version of each parser; however we do nothave a training program for the Charniak parser and have usedthe pretrained parser provided by Charniak; this was trained onthe training section of the PDT 1.0.
We train our model on the48Model Features DescriptionCount ChildCount count of children for the three nodesMTagL TiTc, LiLc, LiTc, TiLc, TiPg conjunctions of MTag and LemmasMTagF TiTc, FiFc, FiTc, TiFc, TiPg conjunctions of MTag and FormsPOSL Pi, Pc, Pg, PiPcPg, PiPg, PcLc conjunctions of POS and LemmaTTT TiTcTg conjunction of tags for each of the three nodesAgr A(Ti, Tc), A(Ti, Tg) binary feature if case/number agreeTrig LiLgTc, TiLgTc, LiLgLc trigrams of Lemma/TagTable 3: Model feature descriptions.Model Charniak Parse Trees Collins Parse TreesDevel.
Accuracy NonP Accuracy Devel.
Accuracy NonP AccuracyBaseline 84.3% 15.9% 82.4% 12.0%Simple 84.3% 16.0% 82.5% 12.2%Simple + Count 84.3% 16.7% 82.5% 13.8%Simple + MtagL 84.8% 43.5% 83.2% 44.1%Simple + MtagF 84.8% 42.2% 83.2% 43.2%Simple + POS 84.3% 16.0% 82.4% 12.1%Simple + TTT 84.3% 16.0% 82.5% 12.2%Simple + Agr 84.3% 16.2% 82.5% 12.2%Simple + Trig 84.9% 47.9% 83.1% 47.7%All Features 85.0% 51.9% 83.5% 57.5%Table 4: Comparative results for different versions of our model on the Charniak and Collins parse trees forthe PDT development data.MaxEnt estimator using the L-BFGS optimizationalgorithms and Gaussian smoothing.8Table 4 presents results on development data forthe correction model with different feature sets.
Thefeatures of the Simple model are the form (F),lemma (L), and morphological tag (M) for the eachnode, the parser-proposed governor node, and thecandidate node; this model also contains the Parser-Gov feature.
In the table?s following rows, we showthe results for the simple model augmented with fea-ture sets of the categories described in Table 2.
Ta-ble 3 provides a short description of each of the mod-els.
As we believe the Simple model provides theminimum information needed to perform this task,Collins trees via a 20-fold Jackknife training procedure.8Using held-out development data, we determined a Gaus-sian prior parameter setting of 4 worked best.
The optimal num-ber of training iterations was chosen on held-out data for eachexperiment.
This was generally in the order of a couple hun-dred iterations of L-BFGS.
The MaxEnt modeling implemen-tation can be found at http://homepages.inf.ed.ac.uk/s0450736/maxent_toolkit.html.we experimented with the feature-classes as addi-tions to it.
The final row of Table 4 contains resultsfor the model which includes all features from allother models.We define NonP Accuracy as the accuracy forthe nodes which were non-projective in the originaltrees.
Although both the Charniak and the Collinsparser can never produce non-projective trees, thebaseline NonP accuracy is greater than zero.
Thisis due to the parser making mistakes in the tree suchthat the originally non-projective node?s dependencyis projective.Alternatively, we report the Non-Projective Preci-sion and Recall for our experiment suite in Table 5.Here the numerator of the precision is the numberof nodes that are non-projective in the correct treeand end up in a non-projective configuration; how-ever, this new configuration may be based on incor-rect dependencies.
Recall is the obvious counterpartto precision.
These values correspond to the NonP49Model Charniak Parse Trees Collins Parse TreesPrecision Recall F-measure Precision Recall F-measureBaseline N/A 0.0% 0.000 N/A 0.0% 0.000Simple 22.6% 0.3% 0.592 5.0% 0.2% 0.385Simple + Count 37.3% 1.1% 2.137 16.8% 2.0% 3.574Simple + MtagL 78.0% 29.7% 43.020 62.4% 35.0% 44.846Simple + MtagF 78.7% 28.6% 41.953 62.0% 34.3% 44.166Simple + POS 23.3% 0.3% 0.592 2.5% 0.1% 0.192Simple + TTT 20.7% 0.3% 0.591 6.1% 0.2% 0.387Simple + Agr 40.0% 0.5% 0.988 5.7% 0.2% 0.386Simple + Trig 74.6% 35.0% 47.646 52.3% 40.2% 45.459All Features 75.7% 39.0% 51.479 48.1% 51.6% 49.789Table 5: Alternative non-projectivity scores for different versions of our model on the Charniak and Collinsparse trees.accuracy results reported in Table 4.
From these ta-bles, we see that the most effective features (whenused in isolation) are the conjunctive MTag/Lemma,MTag/Form, and Trigram MTag/Lemma features.Model Dependency NonPAccuracy AccuracyCollins 81.6% N/ACollins + Corrective 82.8% 53.1%Charniak 84.4% N/ACharniak + Corrective 85.1% 53.9%Table 6: Final results on PDT evaluation datasetsfor Collins?
and Charniak?s trees with and withoutthe corrective modelFinally, Table 6 shows the results of the full modelrun on the evaluation data for the Collins and Char-niak parse trees.
It appears that the Charniak parserfares better on the evaluation data than does theCollins parser.
However, the corrective model isstill successful at recovering non-projective struc-tures.
Overall, we see a significant improvement inthe dependency accuracy.We have performed a review of the errors thatthe corrective process makes and observed that themodel does a poor job dealing with punctuation.This is shown in Table 7 along with other types ofnodes on which we performed well and poorly, re-spectively.
Collins (1999) explicitly added featuresto his parser to improve punctuation dependencyparsing accuracy.
The PARSEVAL evaluation met-Top Five Good/Bad RepairsWell repaired child se i si az?
jenWell repaired false governor v vs?ak li na oWell repaired real governor a je sta?t ba ,Poorly repaired child , se na z?e -Poorly repaired false governor a , vs?ak mus??
liPoorly repaired real governor root sklo , je -Table 7: Categorization of corrections and errorsmade by our model on trees from the Charniakparser.
root is the artificial root node of the PDTtree.
For each node position (child, proposed parent,and correct parent), the top five words are reported(based on absolute count of occurrences).
The par-ticle ?se?
occurs frequently explaining why it occursin the top five good and top five bad repairs.Charniak CollinsCorrect to incorrect 13.0% 20.0%Incorrect to incorrect 21.6% 25.8%Incorrect to correct 65.5% 54.1%Table 8: Categorization of corrections made by ourmodel on Charniak and Collins trees.ric for constituency-based parsing explicitly ignorespunctuation in determining the correct boundaries ofconstituents (Harrison et al, 1991) and so should thedependency evaluation.
However, the reported re-sults include punctuation for comparative purposes.Finally, we show in Table 8 a coarse analysis of thecorrective performance of our model.
We are repair-50ing more dependencies than we are corrupting.6 ConclusionWe have presented a Maximum Entropy-based cor-rective model for dependency parsing.
The goal isto recover non-projective dependency structures thatare lost when using state-of-the-art constituency-based parsers; we show that our technique recoversover 50% of these dependencies.
Our algorithm pro-vides a simple framework for corrective modelingof dependency trees, making no prior assumptionsabout the trees.
However, in the current model, wefocus on trees with local errors.
Overall, our tech-nique improves dependency parsing and providesthe necessary mechanism to recover non-projectivestructures.ReferencesAdam L. Berger, Stephen A. Della Pietra, and VincentJ.
Della Pietra.
1996.
A maximum entropy approachto natural language processing.
Computational Lin-guistics, 22(1):39?71.Alena Bo?hmova?, Jan Hajic?, Eva Hajic?ova?, andBarbora Vidova?
Hladka?.
2002.
The prague depen-dency treebank: Three-level annotation scenario.
InAnne Abeille, editor, In Treebanks: Building andUsing Syntactically Annotated Corpora.
Dordrecht,Kluwer Academic Publishers, The Neterlands.Eric Brill.
1995.
Transformation-based error-drivenlearning and natural language processing: A casestudy in part of speech tagging.
Computational Lin-guistics, 21(4):543?565, December.Sharon Caraballo and Eugene Charniak.
1998.
New fig-ures of merit for best-first probabilistic chart parsing.Computational Linguistics, 24(2):275?298, June.Eugene Charniak and Mark Johnson.
2005.
Coarse-to-fine n-best parsing and maxent discriminative rerank-ing.
In Proceedings of the 43rd Annual Meeting of theAssociation for Computational Linguistics.Eugene Charniak.
2001.
Immediate-head parsing forlanguage models.
In Proceedings of the 39th AnnualMeeting of the Association for Computational Linguis-tics.Michael Collins, Lance Ramshaw, Jan Hajic?, andChristoph Tillmann.
1999.
A statistical parser forczech.
In Proceedings of the 37th annual meeting ofthe Association for Computational Linguistics, pages505?512.Michael Collins.
2000.
Discriminative reranking for nat-ural language parsing.
In Proceedings of the 17th In-ternational Conference on Machine Learning 2000.Michael Collins.
2003.
Head-driven statistical modelsfor natural language processing.
Computational Lin-guistics, 29(4):589?637.Amit Dubey and Frank Keller.
2003.
Probabilistic pars-ing for German using sister-head dependencies.
InProceedings of the 41st Annual Meeting of the Asso-ciation for Computational Linguistics, pages 96?103,Sapporo.Jason Eisner.
1996.
Three new probabilistic modelsfor dependency parsing: An exploration.
In Proceed-ings of the 16th International Conference on Compu-tational Linguistics (COLING), pages 340?345.Jan Hajic?, Eva Hajic?ova?, Petr Pajas, JarmilaPanevova?, Petr Sgall, and Barbora Vidova?
Hladka?.2005.
The prague dependency treebank 2.0.http://ufal.mff.cuni.cz/pdt2.0.51Jan Hajic?.
1998.
Building a syntactically annotatedcorpus: The prague dependency treebank.
In Issuesof Valency and Meaning, pages 106?132.
Karolinum,Praha.Eva Hajic?ova?, Jir???
Havelka, Petr Sgall, Kater?ina Vesela?,and Daniel Zeman.
2004.
Issues of projectivity inthe prague dependency treebank.
Prague Bulletin ofMathematical Linguistics, 81:5?22.P.
Harrison, S. Abney, D. Fleckenger, C. Gdaniec, R. Gr-ishman, D. Hindle, B. Ingria, M. Marcus, B. Santorini,, and T. Strzalkowski.
1991.
Evaluating syntax perfor-mance of parser/grammars of english.
In Proceedingsof the Workshop on Evaluating Natural Language Pro-cessing Systems, ACL.Mark Johnson.
2002.
A simple pattern-matching al-gorithm for recovering empty nodes and their an-tecedents.
In Proceedings of the 40th Annual Meetingof the Association for Computational Linguistics.Dan Klein and Christopher D. Manning.
2003.
FactoredA* search for models over sequences and trees.
InProceedings of IJCAI 2003.Roger Levy and Christopher Manning.
2004.
Deep de-pendencies from context-free statistical parsers: Cor-recting the surface dependency approximation.
In Pro-ceedings of the 42nd Annual Meeting of the Associ-ation for Computational Linguistics, pages 327?334,Barcelona, Spain.Christopher D. Manning and Hinrich Schu?tze.
1999.Foundations of statistical natural language process-ing.
MIT Press.Igor Mel?c?uk.
1988.
Dependency Syntax: Theory andPractice.
SUNY Press, Albany, NY.Joakim Nivre and Jens Nilsson.
2005.
Pseudo-projectivedependency parsing.
In Proceedings of the 43rd An-nual Meeting of the Association for ComputationalLinguistics, pages 99?106, Ann Arbor.Brian Roark and Michael Collins.
2004.
Incrementalparsing with the perceptron algorithm.
In Proceed-ings of the 42nd Annual Meeting of the Association forComputational Linguistics, Barcelona.Petr Sgall, Eva Hajic?ova?, and Jarmila Panevova?.
1986.The Meaning of the Sentence in Its Semantic and Prag-matic Aspects.
Kluwer Academic, Boston.Noah A. Smith and Jason Eisner.
2005.
Contrastive esti-mation: Training log-linear models on unlabeled data.In Proceedings of the Association for ComputationalLinguistics (ACL 2005), Ann Arbor, Michigan.Daniel Zeman.
2004.
Parsing with a statistical de-pendency model.
Ph.D. thesis, Charles University inPrague.52
