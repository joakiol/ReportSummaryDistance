Proceedings of the NAACL HLT Workshop on Semantic Evaluations: Recent Achievements and Future Directions, pages 138?145,Boulder, Colorado, June 2009. c?2009 Association for Computational LinguisticsError Analysis of the TempEval Temporal Relation Identification TaskChong Min LeeLinguistics DepartmentGeorgetown UniversityWashington, DC 20057, USAcml54@georgetown.eduGraham KatzLinguistics DepartmentGeorgetown UniversityWashington, DC 20057, USAegk7@georgetown.eduAbstractThe task to classify a temporal relation be-tween temporal entities has proven to be dif-ficult with unsatisfactory results of previousresearch.
In TempEval07 that was a first at-tempt to standardize the task, six teams com-peted with each other for three simple relation-identification tasks and their results were com-parably poor.
In this paper we provide an anal-ysis of the TempEval07 competition results,identifying aspects of the tasks which pre-sented the systems with particular challengesand those that were accomplished with relativeease.1 IntroductionThe automatic temporal interpretation of a text haslong been an important area computational linguis-tics research (Bennett and Partee, 1972; Kamp andReyle, 1993).
In recent years, with the advent ofthe TimeML markup language (Pustejovsky et al,2003) and the creation of the TimeBank resource(Pustejovsky et al, 2003) interest has focussed onthe application of a variety of automatic techniquesto this task (Boguraev and Ando, 2005; Mani et al,2006; Bramsen et al, 2006; Chambers et al, 2007;Lee and Katz, 2008).
The task of identifying theevents and times described in a text and classifyingthe relations that hold among them has proven to bedifficult, however, with reported results for relationclassification tasks ranging in F-score from 0.52 to0.60.Variation in the specifics has made comparisonamong research methods difficult, however.
A firstattempt to standardize this task was the 2007 Tem-pEval competition(Verhagen et al, 2007).
Thiscompetition provided a standardized training andevaluation scheme for automatic temporal interpre-tation systems.
Systems were pitted against one an-other on three simple relation-identification tasks.The competing systems made use of a variety oftechniques but their results were comparable, butpoor, with average system performance on the tasksranging in F-score from 0.74 on the easiest task to0.51 on the most difficult.
In this paper we providean analysis of the TempEval 07 competition, identi-fying aspects of the tasks which presented the sys-tems with particular challenges and those that wereaccomplished with relative ease.2 TempEvalThe TempEval competition consisted of three tasks,each attempting to model an important subpart of thetask of general temporal interpretation of texts.
Eachof these tasks involved identifying in running textthe temporal relationships that hold among eventsand times referred to in the text.?
Task A was to identify the temporal relationholding between an event expressions and atemporal expression occurring in the same sen-tence.?
Task B was to identify the temporal relationsholding between an event expressions and theDocument Creation Time (DCT) for the text.?
Task C was to identify which temporal relationheld between main events of described by sen-138tences adjacent in text.For the competition, training and developmentdata?newswire files from the TimeBank corpus(Pustejovsky et al, 2003) ?was made available inwhich the events and temporal expressions of in-terest were identified, and the gold-standard tempo-ral relation was specified (a simplified set of tem-poral relations was used: BEFORE, AFTER, OVER-LAP, OVERLAP-OR-BEFORE,AFTER-OR-OVERLAPand VAGUE.1).
For evaluation, a set of newswiretexts was provided in which the event and temporalexpressions to be related were identified (with fulland annotated in TimeML markup) but the temporalrelations holding among them withheld.
The task inwas to identify these relations.The text below allows illustrates the features ofthe TimeML markup that were made available aspart of the training texts and which will serve as thebasis for our analysis below:<TIMEX3 tid="t13" type="DATE"value="1989-11-02"temporalFunction="false"functionInDocument="CREATION TIME">11/02/89</TIMEX3> <s> Italian chemical giantMontedison S.p.A. <TIMEX3 tid="t19"type="DATE" value="1989-11-01"temporalFunction="true"functionInDocument="NONE"anchorTimeID="t13">yesterday</TIMEX3<EVENT eid="e2" class="OCCURRENCE"stem="offer" aspect="NONE"tense="PAST" polarity="POS"pos="NOUN">offered</EVENT>$37-a-share for all the common sharesoutstanding of Erbamont N.V.</s><s>Montedison <TIMEX3 tid="t17"type="DATE" value="PRESENT REF"temporalFunction="true"functionInDocument="NONE"anchorTimeID="t13">currently</TIMEX3><EVENT eid="e20" class="STATE"stem="own" aspect="NONE"tense="PRESENT" polarity="POS"pos="VERB">owns</EVENT> about72%of Erbamont?s common sharesoutstanding.</s>TimeML annotation associates with temporal ex-pression and event expression identifiers (tid andeid, respectively).
Task A was to identify the tem-poral relationships holding between time t19 andevent e2 and between t17 and e20 (OVERLAP was1This contrasts with the 13 temporal relations supported byTimeML.
The full TimeML markup of event and temporal ex-pressions was maintained.Task A Task B Task CCU-TMP 60.9 75.2 53.5LCC-TE 57.4 71.3 54.7NAIST 60.9 74.9 49.2TimeBandits 58.6 72.5 54.3WVALI 61.5 79.5 53.9XRCE-T 24.9 57.4 42.2average 54.0 71.8 51.3Table 2: TempEval Accuracy (%)the gold-standard answer for both).
Task B was toidentify the relationship between the events and thedocument creation time t13 (BEFORE for e2 andOVERLAP for e20).
Task C was to identify therelationship between e2 and e20 (OVERLAP-OR-BEFORE).
The TempEval07 training data consistedof a total of 162 document.
This amounted to a totalof 1490 total relations for Task A, 2556 for task B,and 1744 for Task C. The 20 documents of testingdata had 169 Task A relations, 337 Task B relations,and 258 Task C relations.
The distribution of itemsby relation type in the training and test data is givenin Table 1.Six teams participated in the TempEval compe-tition.
They made use of a variety of techniques,from the application of off-the shelf machine learn-ing tools to ?deep?
NLP.
As indicated in Table 22,while the tasks varied in difficulty, within each taskthe results of the teams were, for the most part, com-parable.3The systems (other than XRCE-T) did somewhatto quite a bit better than baseline on the tasks.Our focus here is on identifying features of thetask that gave rise to difficult, using overall per-formance of the different systems as a metric.
Ofthe 764 test items, a large portion were either?easy?
?meaning that all the systems provided cor-rect output?or ?hard?
?meaning none did.Task A Task B Task CAll systems correct 24 (14%) 160 (45%) 35 (14%)No systems correct 33 (20%) 36 (11%) 40 (16%)In task A, the cases (24/14%) that all participantsmake correct prediction are when the target relationis overlap.
And, the part-of-speeches of most events2TempEval was scored in a number of ways; we report accu-racy of relation identification here as we will use this measure,and ones related to it below3The XRCE-T team, which made use of the deep analysisengine XIP lightly modified for the competition, was a clearoutlier.139Task A Task B Task CBEFORE 276(19%)/21(12%) 1588(62%)/186(56%) 434(25%)/59(23%)AFTER 369(25%)/30(18%) 360(14%)/48(15%) 306(18%)/42(16%)OVERLAP 742(50%)/97(57%) 487(19%)/81(25%) 732(42%)/122(47%)BEFORE-OR-OVERLAP 32(2%)/2(1%) 47(2%)/8(2%) 66(4%)/12(5%)OVERLAP-OR-AFTER 35(2%)/5(3%) 35(1%)/2(1%) 54(3%)/7(3%)VAGUE 36(2%)/14(8%) 39(2%)/5(2%) 152(9%)/16(6%)Table 1: Relation distribution of training/test setsin the cases are verbs (19 cases), and their tenses arepast (13 cases).
In task B, among 160 cases for thatevery participant predicts correct temporal relation,159 cases are verbs, 122 cases have before as targetrelation, and 112 cases are simple past tenses.
Intask C, we find that 22 cases among 35 cases arereporting:reporting with overlap as target relation.In what follows we will identify aspects of the tasksthat make some items difficult and some not so muchso.3 AnalysisIn order to make fine-grained distinctions and tocompare arbitrary classes of items, our analysis willbe stated in terms of a summary statistic: the successmeasure (SM).
(1) Success measure?k=0 6kCk6(?k=0 6Ck )where Ck is the number of items k systems gotcorrect.
This simply the proportion of total correctresponses to items in a class (for all systems) dividedby the total number of items in that class (a successmeasure of 1.0 is easy and of 0.0 is hard).
For exam-ple, let?s suppose before relation have 10 instances.Among the instances, three cases are correct by allteams, four by three teams, two by two teams, andone by no teams.
Then, SM of before relation is0.567 ( (3?6)+(4?3)+(2?2)+(1?0)6?
(1+2+4+3) ).In addition, we would like to keep track of howimportant each class of errors is to the total evalu-ation.
To indicate this, we compute the error pro-portion (ER) for each class: the proportion of totalerrors attributable to that class.
(2) Error proportion?k=0 6 (6?
k)CkAllErrorsInTask ?NumberOfTeamsTaskA TaskB TaskCBEFORE 0.26/21% 0.89/23% 0.47/25%AFTER 0.42/24% 0.56/23% 0.48/17%OVERLAP 0.75/33% 0.56/39% 0.68/31%BEFORE-OR-OVERLAP 0.08/9% 0/3% 0.06/9%OVERLAP-OR-AFTER 0.03/2% 0/1% 0.10/5%VAGUE 0/19% 0/5% 0.02/12%Table 3: Overall performance by relation type (SM/ER)When a case shows high SM and high ER, we canguess that the case has lots of instances.
With lowSM and low ER, it says there is little instances.
Withhigh SM and low ER, we don?t need to focus on thecase because the case show very good performance.Of particular interest are classes in which the SM islow and the ER is high because it has a room for theimprovement.3.1 Overall analysisTable 3 provides the overall analysis by relationtype.
This shows that (as might be expected) thesystems did best on the relations that were the ma-jority class for each task: overlap in Task A, beforein Task B, and overlap in Task C.Furthermore systems do poorly on all of the dis-junctive classes, with this accounting for between1% and 9% of the task error.
In what follows we willignore the disjunctive relations.
Performance on thebefore relation is low for Task A but very good forTask B and moderate for Task C. For more detailedanalysis we treat each task separately.3.2 Task AFor Task A we analyze the results with respect to theattribute information of the EVENT and TIMEX3TimeML tags.
These are the event class (aspectual,i action, i state, occurrence, perception, reporting,and state)4 part-of-speech (basically noun and verb),4The detailed explanations on the event classescan be found in the TimeML annotation guideline at140NOUN VERBBEFORE 0/5% 0.324/15%AFTER 0.119/8% 0.507/15%OVERLAP 0.771/7% 0.747/24%VAGUE 0/8% 0/10%Table 4: POS of EVENT in Task Aand tense&aspect marking for event expressions.
In-formation about the temporal expression turned outnot to be a relevant dimension of analysis.As we seen in Table 4, verbal event expressionsmake for easier classification for before and after(there is a 75%/25% verb/noun split in the data).When the target relation is overlap, nouns and verbshave similar SMs.One reason for this difference, of course, isthat verbal event expressions have tense and aspectmarking (the tense and aspect marking for nouns issimply none).In Table 5 we show the detailed error analy-sis with respect to tense and aspect values of theevent expression.
The combination of tense andaspect values of verbs generates 10 possible val-ues: future, infinitive, past, past-perfective, past-progressive (pastprog), past-participle (pastpart),present, present-perfective (presperf), present-progressive (presprog), and present-participle (pres-part).
Among them, only five cases (infinitive, past,present, presperf, and prespart) have more than 2examples in test data.
Past takes the biggest por-tions (40%) in test data and in errors (33%).
Over-lap seems less influenced with the values of tenseand aspect than before and after when the five casesare considered.
Before and after show 0.444 and0.278 differences between infinitive and present andbetween infinitive and present.
But, overlap scores0.136 differences between present and past.
And aproblem case is before with past tense that shows0.317 SM and 9% EP.When we consider simultaneously SM and EP ofthe semantic class of events in Table 6, we can findthree noticeable cases: occurrence and reporting ofbefore, and occurrence of after.
All of them haveover 5% EP and under 0.4 SM.
In case of reportingof after, its SM is over 0.5 but its EP shows someroom for the improvement.http://www.timeml.org/.BEFORE AFTER OVERLAP VAGUEFUTURE 0/0% 0.333/1% 0.833/0% 0/0%INFINITIVE 0/3% 0.333/3% 0.667/2% 0/1%NONE 0/5% 0.119/8% 0.765/7% 0/8%PAST 0.317/9% 0.544/9% 0.782/10% 0/5%PASTPERF 0/0% 0.333/1% 0.833/0% 0/0%PASTPROG 0/0% 0/0% 0.500/1% 0/0%PRESENT 0.444/2% 0.611/2% 0.646/4% 0/1%PRESPERF 0.833/0% 0/0% 0.690/3% 0/0%PRESPROG 0/0% 0/0% 0.833/0% 0/0%PRESPART 0/0% 0/0% 0.774/4% 0/1%Table 5: Tense & Aspect of EVENT in Task A?
4 ?
16 > 16BEFORE 0/1% 0.322/13% 0.133/6%AFTER 0.306/5% 0.422/13% 0.500/5%OVERLAP 0.846/10% 0.654/17% 0.619/3%VAGUE 0/0% 0/5% 0/13%Table 7: Distance in Task ABoguraev and Ando (2005) report a slight in-crease in performance in relation identificationbased on proximity of the event expression to thetemporal expression.
We investigated this in Table 7,looking at the distance in word tokens.We can see noticeable cases in before and after of?
16 row.
Both cases show over 13% EP and under0.5 SM.
The participants show good SM in overlapof ?
4.
Overlap of ?
16 has the biggest EP (17%).When its less satisfactory SM (0.654) is considered,it seems to have a room for the improvement.
One ofthe cases that have 13% EP is vague of ?
16.
It saysthat it is difficult even for humans to make a decisionon a temporal relation when the distance between anevent and a temporal expression is greater than andequal to 16 words.3.3 Task BTask B is to identify a temporal relation between anEVENT and DCT.
We analyze the participants per-formance with part-of-speech.
This analysis showshow poor the participants are on after and overlap ofnouns (0.167 and 0.115 SM).
And the EM of over-lap of verbs (26%) shows that the improvement isneeded on it.In test data, occurrence and reporting have simi-lar number of examples: 135 (41%) and 106 (32%)in 330 examples.
In spite of the similar distribu-tion, their error rates show difference.
It suggeststhat reporting is easier than occurrence.
Moreover,141ASPECTUAL I ACTION I STATE OCCURRENCE PERCEPTION REPORTING STATEBEFORE 0.167/1% 0/0% 0.333/3% 0.067/6% 0/0% 0.364/9% 0/1%AFTER 0.111/3% 0/0% 0/0% 0.317/9% 0/0% 0.578/8% 0.167/2%OVERLAP 0.917/0% 0.778/1% 0.583/3% 0.787/15% 0.750/1% 0.667/9% 0.815/2%VAGUE 0/1% 0/1% 0/0% 0/9% 0/0% 0/6% 0/0%Table 6: EVENT Class in Task AASPECTUAL I ACTION I STATE OCCURRENCE PERCEPTION REPORTING STATEBEFORE 1/0% 0.905/1% 0.875/1% 0.818/13% 0.556/1% 0.949/5% 0.750/1%AFTER 0.500/3% 0.500/1% 0/0% 0.578/15% 0.778/1% 0.333/1% 0.444/2%OVERLAP 0.625/2% 0.405/5% 0.927/1% 0.367/17% 0.500/1% 0.542/6% 0.567/7%VAGUE 0/1% 0/0% 0/0% 0/4% 0/0% 0/0% 0/0%Table 9: EVENT Class in Task BNOUN VERBBEFORE 0.735/6% 0.908/16%AFTER 0.167/8% 0.667/14%OVERLAP 0.115/13% 0.645/26%VAGUE 0/4% 0/1%Table 8: POS of EVENT in Task BTable 9 shows most errors in after occur with oc-currence class 65% (15%/23%) when we consider23% EP in Table 3.
Occurrence and reporting of be-fore show noticeably good performance (0.818 and0.949).
And occurrence of overlap has the biggesterror rate (17%) with 0.367 of SM.In case of state, it has 22 examples (7%) but takes10% of errors.
And it is interesting that the mosterrors are concentrated in state.
In our intuition, itis not a difficult task to identify overlap relation ofstate class.Table 9 does not clearly show what causes thepoor performance of nouns in after and overlap.In the additional analysis of nouns with class in-formation, occurrence shows poor performance inafter and overlap: 0.111/6% and 0.083/8%.
Andother noticeable case in nouns is state of overlap:0.125/4%.
We can see the low performance of nounsin overlap is due to the poor performance of stateand occurrence, but only occurrence is a cause ofthe poor performance in after.DCT can be considered as speech time.
Then,tense and aspect of verb events can be a cue in pre-dicting temporal relations between verb events andDCT.
The better performance of the participants inverbs can be an indirect evidence.
The analysis withtense & aspect can tell us which tense & aspect in-formation is more useful.
A problem with the in-formation is sparsity.
Most cases appear less than3 times.
The cases that have more than or equalto three instances are 13 cases among the possiblecombinations of 7 tenses and 4 aspects in TimeML.Moreover, only two cases are over 5% of the wholedata: past with before (45%) and present with over-lap (15%).
In Table 10, tense and aspect informationseems valuable in judging a relation between a verbevent and DCT.
The participants show good perfor-mances in the cases that seem easy intuitively: pastwith before, future with after, and present with over-lap.
Among intuitively obvious cases that are past,present, or future tense, present tense makes largeerrors (20% of verb errors).
And present shows 7%EP in before.When events has no cue to infer a relation likeinfinitive, none, pastpart, and prespart, their SMsare lower than 0.500 except infinitive and none ofafter.
infinitive of overlap shows poor performancewith the biggest error rate (0.125/12%).3.4 Task CThe task is to identify the relation between consec-utive main events.
There are four part-of-speechesin Task C: adjective, noun, other, and verb.
Amongeight possible pairs of part-of-speeches, only threepairs have over 1% in 258 TLINKs: noun and verb(4%), verb and noun (4%), and verb and verb (85%).When we see the distribution of verb and verb bythree relations (before, after, and overlap), the rela-tions show 19%, 14%, and 41% distribution each.In Table 11, the best SM is verb:verb of overlap(0.690).
And verb:verb shows around 0.5 SM in be-fore and after.Tense & aspect pairs of main event pairs show142BEFORE AFTER OVERLAP VAGUEFUTURE 0/0% 0.963/1% 0.333/2% 0/0%FUTURE-PROGRESSIVE 0/0% 0/0% 0.167/1% 0/0%INFINITIVE 0.367/5% 0.621/7% 0.125/12% 0/2%NONE 0/0% 0.653/7% 0/2% 0/0%PAST 0.984/3% 0.333/1% 0.083/3% 0/0%PASTPERF 1.000/0% 0/0% 0/0% 0/0%PASTPROG 1.000/0% 0/0% 0/0% 0/0%PASTPART 0.583/1% 0/0% 0/0% 0/0%PRESENT 0.429/7% 0.167/3% 0.850/10% 0/0%PRESPERP 0.861/3% 0/0% 0/2% 0/0%PRESENT-PROGRESIVE 0/0% 0/0% 0.967/0% 0/0%PRESPART 0/0% 0.444/3% 0.310/8% 0/0%Table 10: Tense & Aspect of EVENT in Task BBEFORE AFTER OVERLAP VAGUENOUN:VERB 0.250/2% 0/0% 0.625/1% 0/0%VERB:NOUN 0.583/1% 0.500/2% 0.333/1% 0/1%VERB:VERB 0.500/20% 0.491/15% 0.690/26% 0.220/12%Table 11: POS pairs in Task Cskewed distribution, too.
The cases that haveover 1% data are eight: past:none, past:past,past:present, present:past, present:present,present:past, presperf:present, and pres-perf:presperf.
Among them, past tense pairsshow the biggest portion (40%).
The performanceof the eight cases is reported in Table 12.
As we canguess with the distribution of tense&aspect, mosterrors are from past:past (40%).
When the target re-lation of past:past is overlap, the participants showreasonable SM (0.723).
But, their performances areunsatisfactory in before and after.When we consider cases over 1% of test data inmain event class pairs, we can see eleven cases asTable 13.
Among the eleven cases, four pairs haveover 5% data: occurrence:occurrence (13%), occur-rence:reporting (14%), reporting:occurrence (9%),and reporting:reporting (17%).
Reporting:reportingshows the best performance (0.934/2%) in over-lap.
Two class pairs have over 10% EP: occur-rence:occurrence (15%), and occurrence:reporting(14%).
In addition, occurrence pairs seem difficulttasks when target relations are before and after be-cause they show low SMs (0.317 and 0.200) with5% and 3% error rates.4 Discussion and ConclusionOur analysis shows that the participants have the dif-ficulty in predicting a relation of a noun event whenits target relation is before and after in Task A, andafter and overlap in Task B.
When the distance is inthe range from 5 to 16 in Task A, more effort seemsto be needed.In Task B, tense and aspect information seemsvaluable.
Six teams show good performance whensimple tenses such as past, present, and future ap-pear with intuitively relevant target relations such asbefore, overlap, and after.
Their poor performancewith none and infinitive tenses, and nouns can be an-other indirect evidence.A difficulty in analyzing Task C is sparsity.
So,this analysis is focused on verb:verb pair.
When wecan see in (12), past pairs still show the margin forthe improvement.
But, a lot of reporting events areused as main events.
When we consider that im-portant events in news paper are cited, the currentTempEval task can miss useful information.Six participants make very little correct predic-tions on before-or-overlap, overlap-or-after, andvague.
A reason on the poor prediction can be smalldistribution in the training data as we can see in Ta-ble 1.
Data sparsity problem is a bottleneck in nat-ural language processing.
The addition of the dis-junctive relations and vague to the target labels canmake the sparsity problem worse.
When we con-sider the participants?
poor performance on the la-bels, we suggest to use three labels (before, overlap,and after) as the target labels.143BEFORE AFTER OVERLAP VAGUEPAST:NONE 0.750/1% 0.167/1% 0.167/3% 0/0%PAST:PAST 0.451/12% 0.429/10% 0.723/11% 0.037/7%PAST:PRESENT 0.667/1% 0/0% 0.708/2% 0/0%PRESENT:PAST 0/0% 0.292/2% 0.619/2% 0/1%PRESENT:PRESENT 0.056/2% 0/0% 0.939/1% 0/1%PRESPERF:PAST 0.500/0% 0/0% 0.542/1% 0/0%PRESPERF:PRESENT 0/1% 0/0% 0.583/1% 0/0%PRESPERF:PRESPERF 0/0% 0/0% 0.600/2% 0/0%Table 12: Tense&Aspect Performance in Task CBEFORE AFTER OVERLAP VAGUEI ACTION:OCCURRENCE 0.524/1% 0.400/2% 0.500/1% 0/0%I STATE:OCCURRENCE 0.250/1% 0.500/1% 0.833/0% 0/0%I STATE:ASPECTUAL 0/0% 0.333/1% 0.500/0% 0/0%OCCURRENCE:I ACTION 0.583/1% 0.417/1% 0.300/3% 0/0%OCCURRENCE:OCCURRENCE 0.317/5% 0.200/3% 0.600/5% 0/2%OCCURRENCE:REPORTING 0.569/4% 0.367/3% 0.594/5% 0.111/2%OCCURRENCE:STATE 0.333/1% 0/0% 0.583/1% 0/0%REPORTING:I STATE 0.167/1% 0.583/1% 0.867/1% 0/0%REPORTING:OCCURRENCE 0.625/1% 0.611/3% 0.542/3 0/2%REPORTING:REPORTING 0.167/1% 0.167/2% 0.934/2% 0/4%Table 13: Event class in Task COur analysis can be used as a cue in adding anadditional module for weak points.
When a pair ofa noun event and a temporal expression appears in asentence, a module can be added based on our study.ReferencesBranimir Boguraev and Rie Kubota Ando.
2005.TimeML-Compliant Text Analysis for Temporal Rea-soning.
Preceedings of IJCAI-05, 997?1003.James Pustejovsky, Patrick Hanks, Roser Sauri, AndrewSee, David Day, Lisa Ferro, Robert Gaizauska, MarciaLazo, Andrea Setzer, and Beth Sundheim.
2003.
TheTIMEBANK corpus.
Proceedings of Corpus Linguis-tics 2003, 647?656.Michael Bennett and Barbara Partee.
1972.
Toward thelogic of tense and aspect in English.
Technical report,System Development Corporation.
Santa Monica, CAPhilip Bramsen, Pawan Deshpande, Yoong Keok Lee,and Regina Barzilay.
2006.
Inducing TemporalGraphs Proceedings of EMNLP 2006, 189?198.Nathanael Chambers, Shan Wang, and Dan Jurafsky.2007.
Classifying Temporal Relations BetweenEvents Proceedings of ACL 2007, 173?176.Inderjeet Mani, Marc Verhagen, Ben Wellner, Chong MinLee, and James Pustejovsky.
2006.
Machine Learn-ing of Temporal Relations.
Proceedings of ACL-2006,753?760.Chong Min Lee and Graham Katz.
2008.
Toward anAutomated Time-Event Anchoring System.
The FifthMidwest Computational Linguistics Colloquium.Hans Kamp and Uwe Reyle.
1993.
From Discourseto Logic: Introduction to modeltheoretic semantics ofnatural language.
Kluwer Academic, Boston.James Pustejovsky, Jose?
Castan?o, Robert Ingria, RoserSaur?
?, Robert Gaizauskas, Andrea Setzer, and GrahamKatz.
2003.
TimeML: Robust Specification of Eventand Temporal Expressions in Text.
IWCS-5, Fifth In-ternational Workshop on Computational Semantics.Marc Verhagen, Robert Gaizauskas, Frank Schilder,Mark Hepple, Graham Katz, James Pustejovsky.2007.
SemEval-2007 Task 15: TempEval Tempo-ral Relation Identification.
Proceedings of SemEval-2007, 75?80.Caroline Hage`ge and Xavier Tannier.
2007 XRCE-T:XIP Temporal Module for TempEval campaign.
Pro-ceedings of the Fourth International Workshop on Se-mantic Evaluations (SemEval-2007), 492?495.Steven Bethard and James H. Martin.
2007.
CU-TMP:Temporal Relation Classification Using Syntactic andSemantic Features.
Proceedings of the Fourth Interna-tional Workshop on Semantic Evaluations (SemEval-2007), 129?132.Congmin Min, Munirathnam Srikanth, and AbrahamFowler.
2007.
LCC-TE: A Hybrid Approach to Tem-poral Relation Identification in News Text.
Proceed-ings of the Fourth International Workshop on SemanticEvaluations (SemEval-2007), 219?222.Yuchang Cheng, Masayuki Asahara, and Yuji Mat-sumoto.
2007.
NAIST.Japan: Temporal Relation144Identification Using Dependency Parsed Tree.
Pro-ceedings of the Fourth International Workshop on Se-mantic Evaluations (SemEval-2007), 245?248.Georgiana Pus?cas?u.
2007.
WVALI: Temporal RelationIdentification by Syntactico-Semantic Analysis Pro-ceedings of the Fourth International Workshop on Se-mantic Evaluations (SemEval-2007), 484?487.Mark Hepple, Andrea Setzer, and Robert Gaizauskas.2007.
USFD: Preliminary Exploration of Featuresand Classifiers for the TempEval-2007 Task.
Proceed-ings of the Fourth International Workshop on SemanticEvaluations (SemEval-2007), 438?441.145
