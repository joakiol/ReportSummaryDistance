REASONING ON A H IGHL IGHTED USER MODELTO RESPOND TO MISCONCEPT IONSKath leen  F.  McCoyDepartment  of Computer  and Information SciencesUniversity of DelawareNewark,  DE 19716Responses to misconceptions given by human conversational partners very often contain informationrefuting possible reasoning which may have led to the misconceptions.
Surprisingly there is a great dealof regularity in these responses across different domains of discourse.
For instance, one reason a usermight have given an object a property it does not have is that the user confused the object with anothersimilar object.
In correcting such a misconception, a human conversational partner is likely to point outthis possible confusion.This work describes a method for generating responses like the one just described by reasoning on ahighlighted model of the user to identify possible sources of the error.
Through a transcript study anumber of response strategies were abstracted.
Each strategy was associated with a structuralconfiguration of the user model.
For example, the above mentioned strategy of pointing out a similarconfused object is associated with a configuration of the user model that indicates the user believes thereis an important similar object that has the property involved in the misconception.
Upon finding thatconfiguration in the highlighted user model, the system can respond with the associated strategy.Notice that the reasoning must be done on a highlighted user model since the perception of both anobject's importance and its similarity with another object change with the perspective being taken on thedomain.
This paper investigates how domain perspective can be modeled to provide the neededhighlighting and introduces a similarity metric that is sensitive to the highlighting provided by thedomain perspective.
Finally, the paper shows how the highlighting affects misconception responses.1 INTRODUCTIONWhen people interact with a database or expert system,it is reasonable to expect that they might reveal amisconception about an object modeled by the system.Since a human conversational partner would correctsuch a misconception if it was important to the currentgoals of the conversation, our database and expertsystems hould also be equipped with this ability.In order to investigate how the process of correctingmisconceptions might be automated, a study of tran-scripts of both humans .interacting with what theythought were expert systems (Malhotra 1975, Malhotraand Sheridan 1976, Schuster 1982), and humans inter-acting with other humans to achieve some goal (Pollack,Hirschberg, and Webber 1982) was undertaken.
Thetranscripts, which varied greatly in their domains ofdiscourse, were analyzed to determine if there was anyregularity in the content and rhetorical force of re-sponses given to misconceptions.
The intention of thisanalysis was not to mimic the actual behavior found inthe transcripts, but to use them as a source of intuitionsabout the context and textual shape of responses as wellas the process of generating them.The study revealed that a response to a misconcep-tion important o the current discourse goals of theparticipants can be viewed as consisting of three parts:1. a denial of the incorrect information; 2. a statement ofthe correct information; and 3. justification for thedenial and correction given.
For a particular type ofmisconception (i.e., one involving a particular kind ofknowledge), variations in responses could be found inthe form of the justification given.
The justificationoften seemed to refute support that might have led tothe misconception.
While the kind of support someonemight have for a misconception seems unrestricted, theform of the justification was limited for misconceptionsCopyright 1988 by the Association for Computational Linguistics.
Permission tocopy without fee all or part of this material isgranted providedthat the copies are not made for direct commercial dvantage and the CL reference and this copyright notice are included on the first page.
Tocopy otherwise, or to republish, requires afee and/or specific permission.0362-613X/88/0100e-e$03.0052 Computational Linguistics, Volume 14, Number 3, September 1988Katlfleen F. McCoy Reasoning on a Highlighted User Model to Respond to Misconceptionsinvolving a particular kind of knowledge.
A large num-ber of responses found could be accounted for by asmall number of correction strategies based on the kindof justification given (and hence the faulty reasoningrefuted).If a principled reason for using one strategy overanother could be developed, these strategies could beused by a natural anguage generation system for re-sponding to a misconception.
In this work the faultyreasoning refuted by several of the found correctionstrategies is characterized in a domain-independentfashion in terms of the user's beliefs about he domain.Therefore, given a highlighted model of the user'sbeliefs about he domain, a generation system can lookfor possible support for the misconception.
The re-sponse strategy that refuted the kind of support foundcould then be instantiated.
Notice that the domain-independent characterization f the faulty reasoningenables the same strategies and same method for choos-ing a strategy to be used given a highlighted user modelfor any domain.It is crucial that the user model given to the miscon-ception corrector be highlighted by previous discoursesince the kind of response given by a human conversa-tion partner is apparently not only dependent on thebeliefs about he person being corrected, but also on thecontext in which the misconception occurred.
For in-stance, we could imagine the following dialog where theuser exhibits the misconception that T-bills have apenalty.
A reasonable response is shown.U: I am interested in investing in some securities touse as savings instruments.
I want somethingshort-term and I don't have a lot of money toinvest, so the instrument must have small denom-inations.
I am a bit concerned about he penaltiesfor early withdrawal.
What is the penalty on aT-Bill?R: T-Bills don't have a penalty.
Were you thinkingof Money Market Certificates?This response might be prompted by R thinking that Ucame to the misconception by confusing T-Bills withMoney Market Certificates.On the other hand, it is reasonable that the responsemight be different given a different preceding dialog.For example:U: I am interested in investing in some securities.Safety is very important o me, so I wouldprobably like to get something from the govern-ment.
I am a bit concerned about he penalties forearly withdrawal.
What is the penalty on a T-Bill?R: T-Bills don't have a penalty.
Were you thinkingof T-Bonds?This response may have been prompted by R thinkingthat a confusion between T-Bills and T-Bonds was thesource of the misconception.
The two different re-sponses to the same misconception suggest that the usermodel should be influenced by previous discourse.
I willshow how part of this influence can be achieved by ahighlighting due to the perspective being taken on thedomain.Currently, when a user model containing what thesystem takes to be the user's model for the domain isaccessed, all user knowledge has equal importance.When people engage in a conversation, however, cer-tain aspects of their domain model become more impor-ant than others.
This importance is more than just ahighlighting of those things that have been explicitlymentioned.
Rather, certain things that are somehowrelated to those things explicitly mentioned in previousdiscourse are also highlighted.
In fact, an orientation onthe domain is usually established.In section 8 I will introduce a notion of objectperspective that will enable this highlighting effect ofprevious discourse to be incorporated into the usermodel.
Thus when the user model is accessed, certainthings in it will be highlighted while other things will besuppressed.
I will show how this variable highlighting ofthe user model can explain why the response to aparticular misconception by a particular user may vary.2 RELATED MISCONCEPTION WORKThe method of correcting misconceptions outlinedabove should be contrasted with the way that miscon-ceptions are handled by AI systems today.
For the mostpart misconceptions have been left to the IntelligentComputer Aided Instruction systems, which basicallyuse an a priori listing of misconception-response pairs(see, e.g., Brown and Burton 1978; Stevens, Collins,and Goldin 1979; Stevens and Collins 1980; Woolf andMcDonald 1983; Woolf 1984).
The major problem withthese systems is due to their inability to reason aboutthe misconception itself, they are completely at a losswhen faced with a misconception absent from their apriori listing.The work of Sleeman (1982) on inferring defectivealgebra rules (mal-rules) is based on the observationthat the a priori listing of misconceptions is a difficult, ifnot impossible, task.
Sleeman proposes on-line infer-ence of mal-rules based on the answer the student hasgiven to a particular problem.
Although Sleeman's workis a major improvement over the a priori listing ap-proach, it still has several problems.
First, there is nomeasure of how reasonable or likely a particular mal-rule is.
In addition, once a mal-rule has been inferred,no indication is given concerning how the misconcep-tion should be corrected.The work of Kaplan (1979) and Mays (1980) is closerto the work described here in that they were concernedwith handling and reasoning about whole classes ofmisconceptions, thereby giving the system the ability tohandle a potentially infinite number of misconceptions.Kaplan and Mays were concerned with responding toComputational Linguistics, Volume 14, Number 3, September 1988 53Kathleen F. McCoy Reasoning on a Highlighted User Model to Respond to Misconceptionscertain types of misconceptions in the context of anatural anguage interface to a database system.
Theyworked on detecting and correcting such misconcep-tions based on domain independent linguistic ues fromthe user and an enhanced model of the domain.
Forinstance, the query "Which faculty take courses?
"indicates a presumption failure.
A truthful response of"none" to this query would confirm the user's errone-ous belief that faculty can take courses.
Mays suggestscorrecting a query like the one above by 1. denying thatthe "take" relation can hold between faculty andcourses, and 2. describing all correct alternatives whichcan be reached by abstracting on each of the objects andthe relation involved.
This method would produce thefollowing kind of response:R: I don't believe that faculty can take courses.Faculty teach courses.
Students take courses.Although responses uch as this would probably behelpful to the user, they have the potential (given acomplicated domain) for being overly verbose and con-taining information that the user does not care about.One goal of this work is to provide a more pointed andnatural response to these same kinds of errors.3 KNOWLEDGE AVAILABLEThe work being done here is in the context of a naturallanguage interface to a database or expert system.
It isan attempt to define a module of an interface that couldgenerate a cooperative response to a misconception--the kind of response that would be generated by ahelpful human conversational partner.
In this work, amisconception is defined to be some discrepancy be-tween system beliefs and user beliefs (as exhibitedthrough the conversation).
Upon encountering such amisconception, the assumption is that the systemknowledge is correct, and therefore the job of themisconception corrector module is to attempt to bringthe user's knowledge into line with the system's knowl-edge.The scope of this work is limited by several assump-tions about the kind of knowledge available.?
The system's model of the world contains an objecttaxonomy with attribute/value pairs attached to theobjects.?
The system has available to it a user model thatincludes the user's beliefs about the world.'
This iswhat the system takes to be the user's model of thedomain.
Although the content of the user model andthe system's model of the world may differ greatly,the user model is in the same form as the system'smodel of the world.
Thus while both of these modelscontain an object axonomy with attribute/value pairsattached to the objects, the set of objects in thetaxonomies and the way these objects are classified,as well as the particular attribute/value pairs associ-ated with an object, may vary.
This model of the usermay be updated as the conversation progresses.?
The .,;ystem has available to it certain pieces ofcontextual and discourse information that serve tohighlight he user model.
This highlighting (exploredbelow) is gained from a new notion of object perspec-tive and from a record of items and attributes whichhave been explicitly focused on in the discourse.Given the kind of information assumed in the system'sand user's models of the world, there are two kinds ofmisconceptions that may occur: misclassifications (auser may classify an object wrong) and misattributions(a user may give an object an attribute/value pair it doesnot have).
For each of these kinds of misconceptions, asmall number of response strategies 2 were found in thetranscript study.
These were abstracted into responseschemas.
In section 4, examples of the response strat-egies found for misclassifications will be examined.
Foreach strategy an abstract specification of the contentwill be given.
Next we will look at what beliefs aboutthe user might have prompted the use of each strategy,and characterize these beliefs in terms of the structureof a highlighted user model.
With this pairing of usermodel structures and response schemas, a misconcep-tion can be responded to by looking in the highlighteduser model for one of the user model structures andinstantiating the associated schema.
Section 5 examinesresponse strategies for misattributions.
The sectionsfollowing that will concentrate on the highlighting fromobject perspective.
A new notion of object perspectivewill be defined and it will be shown how object perspec-tive aids in generating context sensitive responses tomisconceptions.This paper is concerned with reasoning on the usermodel to decide how to respond to a misconception.
Itis not concerned with inferring the user model; it isassumed that the user model is already available.
Theemphasis in this work is on using the user model, in adomain-independent fashion, to respond to a miscon-ception in a manner similar to a human conversationalpartner's response.The methods described here have been implementedin the ROMPER system (Responding to Object-relatedMisconceptions using PERspective).
The system takesas input a specification of the information that is incon-sistent with the system's model of the world, the currentperspective (described below), and a record of pastfocus.
It produces a formal specification of the re-sponse.
This response specification is passed into theMumble system (McDonald 1980), which, using a gram-mar and dictionary written by Robin Karlin (1985),produces an actual English response.The implemented system works on the financialsecurities domain.
In order to show the generality ofthis approach, two different domains will be used in thispaper.
The motivation for the system's method ofchoosing aresponse strategy will use examples from thedomain containing whales and fish.
In the last sections54 Computatiional Linguistics, Volume 14, Number 3, September 1988Kathleen F. McCoy Reasoning on a Highlighted User Model to Respond to Misconceptionsof the paper (those pertaining to the new notion ofobject perspective), the examples will be taken from thedomain of financial securities, since the ideas can bebetter motivated with a more complex domain.4 MISCLASSIFICATIONSOne kind of misconception concentrated on in thetranscript study was that involving the classification ofan object.
From that study three major correctionstrategies, which could be supported with the kind ofknowledge available in our knowledge base, were ab-stracted.
These are used to illustrate the type of analysisadvocated; no claim is being made about he complete-ness of this set.
The first strategy is exemplified by thefollowing dialog.U: I thought whales were fish.R: No, they are mammals.
You may have thoughtthey were fish because they are fin-bearing andlive in the water.
However, they are mammalssince, (while fish have gills) whales breathethrough lungs and feed their young with milk.Let us first examine the content of this response.
It canbe seen as consisting of three parts.
In the first sentencewe have the denial of the incorrect information and thestatement of the corresponding correct information.The remaining sentences comprise the justification forthe denial and correction given.The content of this response can be abstracted intothe following:MISCONCEPTION = X is-a YRESPONSE =I.
X is-NOT-a Y2.
X is-a Type-of(X)3.
X is like Y because both share attributes-of(X) Nattributes-of(Y)4.
BUT X has attributes-of(X) - -  attributes-of(Y)5.
WHILE Y has attributes-of(Y) - - attributes-of(X)The justification is contained in (3-5) of the rule.
Noticethat it can be seen as a concede/override pair.
First,information is conceded that actually supports the mis-conception.
This consists of attributes that whales andfish have in common.
These attributes are a potentialsource of the misconception since an object hat sharesmany attributes with a superordinate is likely to belongto that superordinate class.
One way to view thisconceding of intersecting attributes i as R acknowledg-ing the apparent support for the misconception lest Uthink that that part of his/her domain model is wrong.The response goes on to override the concededinformation by offering attributes that differentiatewhales and fish.
These are attributes that whales havebut fish cannot, and attributes that fish have that whalesdo not.By analyzing the justification i this way, not only dowe get an abstract representation that can be used by ageneration system, but we get some insight into when itwould be reasonable to use such a response.
It makessense to use a response that first concedes a similaritybetween the misclassified object and the object hat waswrongly given as its superordinate (from now called theposited superordinate) and then overrides that con-ceded information, when such a similarity is a possiblesource of the misconception.
This might be the casewhen the user model shows that the user believes thatsuch a similarity exists.
Thus a generation system maygenerate its response by instantiating the above rule ifthe user has misclassified the object X as a Y, and ananalysis of the user model shows that the user believesX is similar to Y.
Because of this reason for using thisresponse strategy, it has been termed the like superresponse strategy.A second kind of strategy is exemplified by thefollowing dialog.
Imagine a recent conversation cen-tered on sharks being large fish that people are typicallyafraid of.
Following that, the conversation turns toclassifying whales.
The user says:U: I thought whales were fish, too.R: No, they are mammals.
You may have thoughtthey were fish since they are like the fish, sharks,in that both are large aquatic reatures and bothscare people.
However, whales are mammalssince, (while fish have gills) whales breathethrough lungs and feed their young with milk.Again, this response can be seen as consisting of adenial, a correction, and a justification.
The denial andcorrection are contained in the first sentence.
As in theprevious example, the remaining sentences comprisethe justification i  the form of a concede/override pair.The conceded information, while dependent on at-tributes, differs from that given above.
The abstractcontent rule can perhaps best illustrate this difference:MISCONCEPTION = X is-a YRESPONSE =I.
X is-NOT-a Y2.
X is-a Type-of(X)3.
3Z (Z is-a Y /~ X is like Z because both haveattributes-of(X) A attributes-of(Z))4.
HOWEVER X is-a Type-of(X) because X has at-tributes-of(Type-of(X)) - -  attributes-of(Y)The major difference between this response and the likesuper response is that instead of conceding a similaritybetween the misclassified object and the posited super-ordinate itself, a similarity between the misclassifiedobject and a descendant of the posited superordinate isconceded.
This response is reasonable if the user cameto the misconception by reasoning: "I  don't know howto classify whales, but I do know how to classify sharks.Computational Linguistics, Volume 14, Number 3, September 1988 55Kathleen F. McCoy Reasoning on a Highlighted User Model to Respond to MisconceptionsSharks are fish and whales are very similar to sharks.Therefore it is reasonable to believe that they areclassified the same."
Evidence for this kind of reason-ing might be a similarity found in the user modelbetween the misclassified object and some descendantof the posited superordinate.
Finding such a similaritywould justify the use of this like-some-super strategy.A final strategy found for misclassifications is exem-plified by the following:U: I thought whales were fish.R: No, they are mammals.This is the degenerate no support strategy, which con-tains no justification for the correction.
At first glancethe use of this strategy seems rather puzzling.
Since Ihave assumed that the misconception is important, onewould expect a full response to be given.
However, nojustification was included in the response.This last response is an example of a response in asituation where no evidence was found in the high-lighted user model for any of the preidentified kinds offaulty reasoning.
If R could not figure out what led tothe misconception, then R could not include justifica-tion to refute the support!
The most that R could do inthis situation was deny the incorrect information andstate the corresponding correct information.
This isexactly what was done in the response above.
For thisreason the response has been termed the no supportresponse, and it can be used when the user modelcontains none of the evidence that would key off any ofthe other correction strategies.5 MISATTRIBUTIONSMisattributions are the second kind of misconceptionsthat may arise given the kind of knowledge assumed.
Amisattribution occurs when the user gives an object anattribute/value that the object does not have in thesystem's domain model.The transcript study revealed that a misattributionresponse could also be broken into three parts.
In thiscase, however, the justification took the form of offer-ing another statement that would change what the usersaid in a small way in order to make it a correctstatement.
This is illustrated in the following dialog,which is an example of the first kind of correctionstrategy found.
Imagine that we are discussing seaanimals and have previously discussed sharks and theirattributes.
We move to whales.U: I thought whales had gills.R: No, they breathe through lungs.
Were you think-ing of sharks?The response might go on to discuss the similarities anddifferences between whales and sharks.Notice what was done in this response.
We have thedenial and the correction as before.
This time thejustification offers another object that is similar to theobject involved in the misconception but for which themisattributed attribute/value pair is true.
This content iscaptured in the following rule:MISC, ONCEPTION = X has attribute Y with value Vthen RESPONSE =1.
X NOT have attribute Y with value V2.
X has attribute Y with value W3.
3 Z (Z has attribute Y with value V), did you meanZ?This kind of strategy would be effective if the user'smisconception results from either confusing the objectbeing discussed with some other object with the misat-tributed property, or through an incorrect analogy withsuch an object.
Mention of the object in the case of aconfusion prevents continuation of the confusion.
In thecase of an incorrect analogy, mention of the other objectalerts the user to the inappropriateness of analogy andalso prevents the continued usage of the analogy.
No-tice that in either case, a failure to mention the secondobject will probably bring protest on the part of theuser.
For instance, suppose that the preceding miscon-ception was arrived at because the user was confusingwhales and sharks.
If the expert just responds, "No,they breathe through lungs", it is likely that the userwill counter, "But you just told us that they had gills!
"By mentioning the confused object, the user becomesaware of the confusion.It makes sense to use this wrong object strategywhen a possible source of the misconception is either anobject confusion or the usage of a bad analogy.
Evi-dence for this would be the existence of some similarobject from the highlighted user model that has themisattributed property.A second kind of correction strategy used for misat-tributions is shown here:U: I thought whales had gills.R: No, whales breathe through lungs.
Did you meanyou thought hey had fins?This is very similar to the last response xcept, insteadof offering another object, the strategy offers a similarattribute which makes the user's statement correct.
Thecontent rule for this response follows:MISCONCEPTION = X has attribute Y with value Vthen RESPONSE =I.
X does not have attribute Y with value V2.
X has attribute Y with value W3.
3Q=IU(X has attribute Q with value U /k simi-lar(Y,Q), did you mean X has attribute Q with valueu'?
)This strategy, termed the wrong attribute strategy, isused when there is a similarity of attributes within the56 Computational Linguistics, Volume 14, Number 3, September 1988Kathleen F. McCoy Reasoning on a Highlighted User Model to Respond to Misconceptionshighlighted user model.
(The no support strategy alsooccurs in the case of misattributions.
)6 USER MODEL ANALYSISGiven the association of user model configurations withresponse strategies given in the previous sections, wecan come up with a method for deciding how to respondbased on the type of misconception along with ananalysis of the highlighted user model.
Basically theuser model analysis looks for one of the preidentifiedconfigurations and, if one is found, suggests instantiat-ing the associated strategy.
The following rule capturesthe way that the ROMPER system implements what hasbeen discussed so far.IF misconception = "X is-a Y"THENIF similar(X,Y)THEN instantiate like super schemaELSEIF 3Z (Z is-a Y)/~ similar(X,Z)THEN instantiate like-some-super schemaELSE instantiate no support schemaELSEIF misconception = "X has attribute Y withvalue V"THENIF 3Z ((Z has attribute Y with value V)/~similar(X,Z))THEN instantiate wrong object schemaELSEIF 3Q3U ((X has attribute Q with value U)/~similar(Y,Q))THEN instantiate wrong attribute schemaELSE instantiate no support schemaNotice that each of the tests for instantiating a schemahinges on the similarity assessment of two objects.These assessments must be context dependent.
Be-cause of this, it is crucial that the user model analysis bedone on a user model highlighted by previous discourseand that the similarity metric take advantage of thishighlighting.
The highlighting and similarity metric usedby ROMPER will be discussed below.This method for correcting misconceptions suggestsa model of natural language generation that is similar tothat put forth by McKeown (1982) but which differsfrom McKeown's model in several ways.Both McKeown and this work concentrate on deter-mining the content and textual shape of a response.McKeown is concerned with responses to questionsabout the structure of a data base.
Upon encounteringsuch a question McKeown first delimits a relevantknowledge pool using fairly simple mechanisms.
Thisrelevant knowledge pool contains that information fromthe knowledge base that could possibly be included inthe response; the actual generated response need notexhaust his pool.
Next, based on the goal of thediscourse (as determined by the question type) and acharacterization f the relevant knowledge pool (again,a simple test) a response schema is chosen for theresponse.
The response schema dictates the textualstructure of the response.
This schema is filled bystepping through it and matching its predicates againstthe relevant knowledge pool.
A focusing mechanism isused to mediate between choices arising during thisprocess.The schemas used by the ROMPER system are morecomplicated than those advocated by McKeown.
Ineffect, ROMPER's schemas are responsible both fordetermining the textual shape of a response and fordetermining what information from the knowledge baseto include in the response.
Thus they are applicable to amuch more restricted generation problem (e.g., re-sponding to a misconception of a particular type).Because of this, the test for determining which schemato use can be much more specific than the tests em-ployed by McKeown.7 HIGHLIGHTING AND OBJECT SIMILARITYWe claim that in order for the above strategy forcorrecting misconceptions to work, the similarity metricthat is used to assess object similarity must be affectedby the preceding discourse.To date, most AI systems do not assess objectsimilarity in a way that is context dependent.
Severalsystems that do assess object similarity (Rumelhart andAbrahamson 1973, McKeown 1982, Carberry 1984,Weiner 1984) use a metric based on distance in somespace.
Most often, this space is the generalizationhierarchy.
Basically, two objects that have a commonimmediate superordinate (i.e., are siblings in the hier-archy) are seen as very similar, while objects whoselowest common ancestor is several evels up in thehierarchy are seen as quite different.One problem with this metric arises when objects canbe classified in more than one way and there are severallowest common ancestors of the objects being com-pared.
A decision must be made about which of theselowest common ancestors hould be considered sincethe similarity assessment of the objects might varywidely as a result.
For instance, a treasury bond and acorporate bond may be assessed as being very similarsince they have a common immediate superordinate ofbonds.
On the other hand, both of these objects can beclassified along other dimensions.
If we look at atreasury bond as being a treasury issue, which is a typeof US Government security, it seems quite differentfrom a corporate bond, which is a corporate security.A second major problem with a similarity metricbased on distance in the generalization hierarchy is thatit is context invariant; contextual information has noway of affecting the assessments.
As shown by Tversky(1977) and others, human judgments of object similarityhave been found to shift both when the set of objectsunder discussion are altered (e.g., a violin and anelectric guitar may be judged quite similar when in agroup with a clarinet and an oboe, and may be judgedquite different when the other members of the group areComputational Linguistics, Volume 14, Number 3, September 1988 57Kathleen F. McCoy Reasoning on a Highlighted User Model to Respond to Misconceptionsa cello and an electric bass), and when the salience ofattributes are altered (e.g., in a group containing a redtriangle, a blue triangle, and a red square, the redtriangle might be judged similar to the blue trianglewhen attribute shape is stressed, but may be judgedsimilar to the red square when attribute color isstressed).One metric that avoids these problems was intro-duced by Tversky (1977).
Tversky's metric, rather thanrelying on distance in some space, is based on thecommon and disjoint features of the objects involved.The metric, termed a contrast model, allows context obe taken into account in several places.Suppose we have two objects a and b where A is theset of properties associated with object a and B is theset of properties associated with object b. Tversky'smeasure can be expressed as:s (a ,b )  = Of(A fq B)  ~ a f (A  - -  B )  ~ /3f(B ~ A)for some 0, a, and/3 -> 0.In the above equation 0, a, and /3 are parameterswhich represent the importance of each piece of theequation.
The function f maps over the features andyields a salience rating for each.
In essence, the contrastmodel states that the similarity of two objects is somefunction of their common features minus some functionof their disjoint features.
The importance of each par-ticular feature involved (determined by the function j')and the importance of each piece of the equation(determined by 0, a, and/3) may change with context.Although Tversky discusses in general terms howthese functions might be set, he gives no concretemethods for doing so.
For instance to set 0, a, and/3 heturns to the relative prominence ofobjects a and b in thediscourse.
The more prominent an object is, the moreits attributes hould have an impact on the similarityrating.
Thus, finding the relative prominence of objectsa and b in the discourse would help set these values.
Ifa is relatively more important, hen functions 0 and ashould be greater than/3 resulting in the attributes of themore prominent object having a greater influence overthe similarity assessment.
While I would conjecture thatinformation about the focus of the discourse (Grosz1981, Sidner 1983, Grosz, Joshi, and Weinstein 1983)might give an indication of an object's prominence andwould therefore be useful in setting the values of 0, a,and/3, in this work I have assumed a value of 1 for the0, a, and 13 and have concentrated on setting the ffunction.We turn, then, to the problem of finding a value fortheffunction: the measure of salience for each propertyof the objects involved.
Other work, such as Carbonnelland Collins (1970) and Weiner (1984), has hand-encodedsalience values for attributes of individual objects di-rectly into the knowledge base, permanently setting theffunction.
This approach is not sufficient for setting thef function for Tversky's metric, since it is crucial thattheffunction be able to change with context.
In order tomake this happen, the salience values computed by fmust change with context.To see this, consider our ability to explicitly mentionan attribute to increase its salience.
In the exampleearlier with the red and blue triangles and the redsquare, if the request for a similarity judgment had beenpreceded by "Look at the pretty colors of theseobjects;", the red triangle and red square would haveprobably been judged to be more similar than the redtriangle and the blue triangle.
Thus we see that explicitmention of an attribute in a discourse isone way that thef function might be affected by previous discourse.Explicit mention of an attribute is only one way inwhich the salience of the attributes may change dynam-ically.
Another aspect of dynamic salience comes fromthe point of view, or perspective, applied to the domain.For instance, a building can be referred to as being anarchitectural work, for example, or as being someone'shome.
The two different views of the building causedifferent sets of attributes to become salient.
Noticethat this set of attributes i in addition to the attributesthat have been explicitly mentioned in the discourse.From an architectural work point of view, attributes likethe architect's name, date of building, and particulararchitectural features become salient.
On the otherhand, from the home point of view, attributes like thekitchen size, number of bedrooms, and living spacebecome important.
If we can find a way of modelinghow these "precompiled" sets of attributes becomehighlighted in a discourse, we will have a principledmethod for setting the f function needed for Tversky'ssimilarity metric.
The next section discusses how thishighlighting can be modeled by a computer system.8 OBJECT PERSPECTIVEThe notion of point of view or object perspective hasbeen noted by other esearchers in artificial intelligence.Perspective's ability to explain the changing attributesalience has been attributed to a limited inheritancemechanism (see, e.g., Grosz 1977; Bobrow and Wino-grad 1977; Tou et al 1982).
An object viewed from aparticular perspective is seen as having one particularsuperordinate, although in fact it may have many.
Theobject inherits properties only from the superordinate inperspective.
Therefore different perspectives on thesame object cause different properties to be inherited(and therefore highlighted).While explaining object perspective via a limitedinheritance mechanism is intuitively appealing, it isunable to handle some effects which intuitively shouldbe handled by object perspective.
The first has to dowith the availability of object attributes.
A limitedinheritance mechanism akes attributes inherited fromsuperordinates other than the one in perspective un-available.
This seems a bit too strong.
When we discussa building as an architectural work, I may comment onthe number of bedrooms the building has.
While you58 ComputatJional Linguistics, Volume 14, Number 3, September 1988Kathleen F. McCoy Reasoning on a Highlighted User Model to Respond to Misconceptionsmay think my comment irrelevant to the current con-versation, you would still be able to understand it andeven evaluate its truth or falsity.
In the limited inheri-tance account of object perspective, however, thiswould not be possible.
As far as the system would beconcerned the number of bedrooms would not even bean attribute to the building.A second problem with the limited inheritance ac-count of object perspective has to do with deciding whatattributes the superordinate in perspective has.
Thesuperordinate itself may have multiple classificationsand thus potentially multiple perspectives.
Thus, inorder to figure out what attributes a particular conceptshould inherit, we must figure out not only what per-spective it is being viewed from, but also what perspec-tive the perspective superordinate is being viewed from,and so on.
But this seems to be much more work than isnecessary.Another problem is that a limited inheritance mech-anism explains the perspective for a single object only.However, during the course of a conversation it isusually the case that more than one object will bediscussed.
When this happens, usually the same kindsof things are discussed about the objects.
In essence, aparticular highlighting of attributes (or point of view)seems to be in force during the conversation.
Yet, thishighlighting is applied to different objects--some ofwhich may not even have the same superordinates.What seems to be happening is that the conversationalpartners are viewing an entire group of objects from thesame perspective.
A limited inheritance mechanismcannot account for this unless each of the objects underdiscussion can be said to have the same (immediate)superordinate.A final effect hat is not accounted for by the limitedinheritance mechanism, yet seems to hinge on the viewbeing taken on the domain, has to do with the height-ened importance of some objects during a discourse.Like the importance of attributes, the relative impor-tance of some objects in the discourse cannot solely beaccounted for by explicit mention.
Some objects aremore likely to be mentioned and discussed in a dis-course than others.
For instance, when discussing aparticular building as an architectural work, I mightreasonably mention the library down the street hat wasdesigned by the same architect.
On the other hand, Iwill probably not mention my apartment.
Along thesame lines, in discussing that building as a home, myapartment is a likely candidate for mention in theconversation.
Although this effect seems to be in someway tied to the notion of object perspective, the limitedinheritance mechanism does not address this issue.We want to retain the dynamic highlighting of "pre-compiled" groups of attributes.
Instead of the limitedinheritance mechanism, we propose that the followingaccount be used:1.
Instead of tying perspective into the generaliza-tion hierarchy of objects as has been done in thepast, the new notion of perspective is independentof that hierarchy.
Perspectives that can be takenon the objects in the domain will be defined andwill sit in a structure that is orthogonal to thegeneralization hierarchy.2.
A number of perspectives are available for anydomain of discourse and any given domain objectmay be viewed from any one of several perspec-tives for that domain.3.
Each perspective comprises a set of attributeswith associated salience values.
It is these sa-lience values that dictate which attributes arehighlighted and which are suppressed.4.
One such perspective is designated active at anyparticular point in the discourse)Our solution is that any object that is accessed by thesystem is viewed through the current active perspec-tive.
However, instead of dictating which attributes anobject inherits, the active perspective affects the sa-lience values of the attributes that an object possesses(either directly or inherited through the generalizationhierarchy).
The active perspective essentially acts as afilter on an object's attributes.
By raising the salience ofthe attributes, it highlights those attributes which have ahigh salience rating in the active perspective.
By low-ering the salience of the attributes, it suppresses thoseattributes that are either given a low salience value or donot appear in the active perspective.By defining object perspective in this way, we haveretained the desirable results of the limited inheritanceaccount of object perspective while avoiding its prob-lems.
In addition, since any object accessed by thesystem is viewed through the active perspective, wegain the feeling of perspective on the entire domain.
Theobject importance aspect of perspective is gotten bysaying that those objects that contribute attributeswhich are highly salient o a perspective are importantwhile that perspective is active.We propose that theffunction i Tversky's metric beset by taking into account he salience values derivedfrom the active perspective.
This would yield an ffunction that is context dependent and would help thesimilarity metric exhibit many desirable properties.9 MODELING A DOMAIN WITH PERSPECTIVESIn some natural language systems, a model of a partic-ular domain includes a usual object taxonomy contain-ing all of the objects in the domain and all of theattributes associated with those objects.
We will showan example of building a domain model with perspec-tives.
In order to do this, one must build the domainmodel as usual.
In addition, the perspectives that can betaken on the domain objects must be defined.
The resultof viewing the domain model through the perspectiveswill be shown.Computational Linguistics, Volume 14, Number 3, September 1988 59Katldeen F. McCoy Reasoning on a Highlighted User Model to Respond to MisconceptionsMoney Market CertificatesMaturity: 3 monthsDenominations: $1,000Issuer: Commercial BankPenalty for Early Withdrawal: 10%Purchase Place: Commercial BankSafety: MediumTreasury BillsMaturity: 3 monthsDenominations: $1,000Issuer: US GovernmentPurchase Place: Federal ReserveSafety: HighSavings InstrumentsMaturitywl.0denominations--1.0safety---0.5yield---0.5Issuing Companyissuer--l.0safety--1.0purchase-place--0.5yield--4).5tax--0.5Figure 2.
Sample Perspectives.Treasury BondMaturity: 7 yearsDenominations: $500Issuer: US GovernmentPenalty for Early Withdrawal: 20%Purchase Place: Federal ReserveSafety: HighFigure 1.
Objects in "Flat" Domain Model.Figure I shows a small piece of a typical domainmodel.
The domain is that of financial securities.
Threeof the objects from this domain, Money Market Certif-icates, Treasury Bills, and Treasury Bonds, are shownwith the attributes they possess.
In systems as they aredefined today, a group of objects defined in this way andarranged in a generalization hierarchy would constitutethe domain model.
I fa system were to access any one ofthe objects in the domain it would be given the objectwith the attributes as listed in the figure.In order to get a dynamic highlighting of the domainmodel, we must build, in addition to the object taxon-omy, a separate structure containing the perspectivesthat can be taken on the domain objects.
This meansthat we must think about the different points of viewthat can be taken on the objects in the domain andcompile sets of attributes from our model which capturethe important domain concepts in that point of view.There will be attributes in each perspective that do notoccur with all of the objects in the domain.
At the sametime, there will be attributes of individual objects thatdo not appear in a particular perspective.
The perspec-tive simply captures the attributes that are important ina particular point of view.Figure 2 contains two perspectives that might bereasonable for this domain (here we are assumingsalience values from low salience of 0 to high salience of1).
The perspective of Savings Instruments highlightsmaturity and denominations, and somewhat highlightssafety and yield.
This indicates that when people arediscussing securities as savings instruments, they aremost interested in how long their money will be tied upand in what denominations they can save their money.The perspective ofIssuing Company, on the other hand,highlights different attributes.
When securities are dis-cussed from this perspective, things like the name of thecompany and the stability of an investment in thecompany become important.
Other attributes of thesecurities are ignored (recall that attributes not men-tioned in the perspective get assigned a low saliencerating).Notice how the objects look when accessed, depend-ing on which of the two different perspectives areactive.
For instance, through the savings instrumentsperspective the objects' attributes take on the saliencevalues shown in Figure 3.
Attributes are not shown inthe figure that have 0 salience.
When no static salienceis inchlded in the domain model, the attributes of theobjects derive their salience directly from values givenin the active perspective.
Attributes in the perspectivethat the objects do not have (e.g., yield) are ignored.Attributes of the objects not occurring in the perspec-tive are given the lowest salience rating.
The very sameobjects look different when viewed through the otherperspective.
This is shown in Figure 4.The salience values derived from perspective can beused for many tasks.
Section 11 will show how they canbe used in conjunction with Tversky's imilarity metricto generate context sensitive responses to misconcep-tions.10 CHOOSING THE ACTIVE PERSPECTIVEIn order for the notion of object perspective to be trulybeneficial, there must be a mechanism for choosing theactive perspective based on previous discourse.
Whilethis topic is still very much open to investigation, somepreliminary research as revealed several factors thatmight influence the choice of active perspective.Perhaps one of the most influential pieces of infor-mation useful in choosing a perspective is the user'scurrent goal.
In (McKeown, Wish, and Matthews 1985)the user's goal completely determines which perspec-tive is active.
In their work each perspective that can betaken on the domain objects is indexed by potentialgoals.
Thus once the system has determined what theuser's goal probably is, it has also determined what60 Computational Linguistics, Volume 14, Number 3, September 1988Kathleen F. McCoy Reasoning on a Highlighted User Model to Respond to MisconceptionsMoney Market CertificartesMaturity: 3 months---1Denominations: $1,000~1Safety: Medium---0.5Treasury BillsMaturity: 3 months--1Denominations: $1,000--1Safety: High---0.5Treasury BondMaturity: 7 years--1Denominations: $500---1Safety: High--0.5Figure 3.
Objects Through Savings InstrumentsPerspective.perspective the user has probably taken on the domainobjects.While the user's goal is a good source of informationto use to determine the probable perspective, otherfactors may also influence this choice.
These includethe attributes and objects mentioned so far in the dialog.The mentioned attributes are obviously thought o beimportant and one would therefore xpect hem to begiven a fairly high salience rating in the active perspec-tive.
Thus the choice of active perspective can benarrowed down to those in which the mentioned at-tributes appear with high salience.By the same token, the objects mentioned so far inthe dialog can also give a clue concerning the activeperspective.
One would expect hat the active perspec-tive would deem these objects important.
Therefore thesystem might look for perspectives that give high sa-lience ratings to many of the attributes associated withobjects that have been mentioned in the discourse.In this section I have identified several factors thatinfluence the choice of active perspective.
While thesuccess of other systems (McKeown, Wish, and Mat-thews 1985) has shown that a reasonable choice ofMonrey Market CertificatesIssuer: Commercial Bank--IPurchase Place: Commercial Bank--0.5Safety: Medium--ITreasury BillsIssuer: US Government--IPurchase Place: Federal Reserve---0.5Safety: High--1Treasury BondIssuer: US Government--1Purchase Place: Federal Reserve---0.5Safety: High--1Figure 4.
Objects Through Issuing Company Perspective.perspective can be made based on discourse goals, thenature of establishing and perhaps "shifting" perspec-tive during a discourse must still be investigated.
Stillunanswered are questions uch as: When does a per-spective change?
How long is a perspective active?
Isthere a relationship between a discourse unit (Grosz andSidner 1985) and perspective?
Is there any structure tothe space of perspectives that would put constraints onmoving from one active perspective to another?
Thesequestions must be taken up in future research onperspective.l l  PERSPECTIVE'S INFLUENCE ON RESPONSESIn this section we will show how using the activeperspective to highlight he user model, the misconcep-tion correcting algorithm from Section 6, and theTversky similarity metric can account for context sen-sitive corrections to misconceptions.
Recall that incorrecting a misattribution one of the correction sche-mas used by ROMPER called for a similar object to beoffered as a possible object of confusion.
A study oftranscripts reveals, however, that this schema may beinstantiated in different ways depending on the context.Consider once again the following dialogs that were firstseen in the introduction.U: I am interested in investing in some securities touse as savings instruments.
I want somethingshort-term and I don't have a lot of money toinvest so the instrument must have small denom-inations.
I am a bit concerned about he penaltiesfor early withdrawal.
What is the penalty on aT-Bill?R: T-Bills don't have a penalty.
Were you thinkingof Money Market Certificates?In this case money market certificates were seen asbeing similar to T-bills and therefore included in theresponse.
A different object might be used in a differentcontext.
Consider:U: I am interested in investing in some securities.Safety is very important o me, so I wouldprobably like to get something from the govern-ment.
I am a bit concerned about he penalties forearly withdrawal.
What is the penalty on a T-Bill?R: T-Bills don't have a penalty.
Were you thinkingof T-Bonds?The difference in these two responses can be explainedby different perspectives being taken on the objects.Suppose we are given the objects, attributes, and per-spectives from Section 9.
The dialog preceding the firstexample could lead to the establishment of savingsinstruments a  the active perspective 4 since it mentionsthe perspective by name and explicitly mentions everalof the attributes made important by the perspective.If ROMPER were given this information it wouldComputational Linguistics, Volume 14, Number 3, September 1988 61Kathleen F. McCoy Reasoning on a Highlighted User Model to Respond to Misconceptionsproceed by attempting to instantiate the wrong objectschema described in section 5.
Recall that this schema isapplicable when there is a similar object that has theproperty involved in the misconception.
The systemwould collect all objects having the attribute in questionand then test their similarity with the object involved inthe misconception.
In our knowledge base there are twoobjects that have the attribute involved in the miscon-ception: Money Market Certificates and T-Bonds.Assume that the f funct ion  in the metric is set solelyon the basis of the salience values given by the perspec-tive.
Also assume that we have decided that two objectsare highly similar if Tversky 's  metric returns a numbergreater than 0, and not similar otherwise.
Applying theTversky metric using the salience values attached bythe savings instrument perspective (and assuming avalue of 1 for 0, a, and/3) we get:s(T-BiU, MM-Cert) -- f(maturity, dehorn) - f(safety)= 2 - .5 = 1.5 ===> high similaritys(T-Bill, T-Bond) = f(safety) - f(maturity, denom)= .5 - 2 = -1 .5  ===> low similarityWith these calculations the system would choose theMoney Market Certificate as the possible object ofconfusion and respond:R: Treasury Bills don't  have a penalty.
Were youthinking of Money Market Certificates?Contrast the above calculations with calculations thatmight occur given a different active perspective.
Thediscourse preceding the misconception utterance in thesecond example suggests the active perspective of " Is -suing Company" .
Using the salience values attached bythis perspective the similarity metric would produce thefollowing calculations:s(T-Bill, MM-Cert)= f0 - f(issuer, safety, purchase)= 0 - 2.5 = -2 .5  ===> low similaritys(T-BiU, T-Bond)= f(issuer, safety, purchase) - f0= 2.5 - 0 = 2.5 ===> high similarityIn this case a reasonable response by the system wouldbe:R: Treasury Bills don't  have a penalty.
Were youthinking of a Treasury Bond?As the examples how, changes in the active perspec-tive can account for the same misconception beingresponded to in two different ways.12 CONCLUSIONSThis paper has described a new method for respondingto misconceptions that relies on an analysis of a high-lighted user model to generate a response that is mostlikely to benefit the user.
A number of strategies wereabstracted from a study of transcripts.
Each strategywas associated with a distinguished structure in the usermodel which could explain its use in a given situation.
Asystem can use this pairing to decide how to respond bylooking in the highlighted user model for evidence ofone of the distinguished structures.
The correspondingstrategy can be used to respond.In addition, the paper has described a new notion ofobject perspective that is able to model one aspect ofthe dynamic highlighting of the user model due toprevious discourse.
It was shown how perspectivecould account for certain contextual affects on re-sponses to misconceptions.ACKNOWLEDGEMENTSI would like to thank Sandee Carberry, Julia Hirschberg, AravindJoshi, Martha Pollack, Bonnie Webber, and the participants of theUser Model Workshop for their helpful comments and discussions onvarious aspects of this work.
Many thanks also to the anonymousreviewers for their constructive input concerning the style and contentof this paper.Some of this work was done while the author was at the Universityof Pennsylvania nd was partially supported by the ARO grantDAA20-84-K-0061 and the NFS grant MCS81-07290.REFERENCESBobrow, D.G.
and Winograd, T. 1977 An overview of krl, a knowl-edge representation language.
Cognitive Science 1(1): 3-46.Brown, J.S.
and Burton, R.R.
1978 Diagnostic models for proceduralbugs in basic mathematical skills.
Cognitive Science 2(2): 155-192,1978.Carberry.
Sandra M. 1984 Understanding pragmatically ill-formedinput.
In lOth International Conference on Computational Lin-guistics and 22nd Annual Meeting of the Association of Compu-tational Linguistics, Stanford University, CA: 200-206.Carbonnell, Jaime R. and Collins, Allan M. 1970 Mixed-InitiativeSystems For Training and Decision-Aid Applications.
TechnicalReport ESD-TR-70-373, Electronics Systems Division, LaurenceG.
Hanscom Field, U.S. Air Force, Bedford, MA.Grosz, B.
1977 The Representation and Use of Focus in DialogUnderstanding.
Technical Report 151, SRI International, MenloPark, CA,Grosz, B.; Joshi, A.K.
; and Weinstein S. 1983 Providing a unifiedaccount of definite noun phrases in discourse.
In Proceedings ofthe 21st Annual Meeting of the Association for ComputationalLinguistics, Cambridge, MA: 44-50.Grosz, B.
1981 Focusing and description i natural language dialog.
InWebber B, Joshi, A, and Sag, I.
(ed.
), Elements of DiscourseUnderstanding, Cambridge University Press, Cambridge, En-gland: 85-105.Grosz, B. and Sidner, C. 1985 Discourse structure and the propertreatment of interruptions.
In Proceedings of the 1985 JointConference on Artificial Intelligence, IJCAI85, Los Angeles, CA.Kaplan, S.J.
1979 Cooperative Responses From a Portable NaturalLanguage Database Query System.
Ph.D. thesis, University ofPennsylvania, Philadelphia, PA.Karlin, Robin 1985 Romper Mumbles.
Technical Report, Universityof Pennsylvania, Philadelphia, PA.Malhotra, A.
1975 Design Criteria for a Knowledge-Based EnglishLanguage System for Management.
Technical Report TR-146,Project MAC, MIT, Cambridge, MA.Malhotra, A. and Sheridan, P. 1976 Experimental Determination ofDesign Requirements for a Program Explanation System.
Tech-62 Computational Linguistics, Volume 14, Number 3, September 1988Kathleen F. McCoy Reasoning on a Highlighted User Model to Respond to Misconceptionsnical Report RC 5831, IBM Research Center, Yorktown Heights,NY.Mays, E. 1980 Correcting misconceptions about database structure.In Proceedings of the 3rd CSCS1 Biennial Meeting, Victoria, BC,Canada.McDonald, D.D.
1980 Natural Language Production as a Process ofDecision Making Under Constraint.
Ph.D. thesis, MIT, Cam-bridge, MA.McKeown, K. 1982 Generating Natural Language Text in Responseto Questions About Database Structure.
Ph.D. thesis, Universityof Pennsylvania, Philadelphia, PA.McKeown, K.; Wish, M.; and Matthews, K. 1985 Tailoring explana-tions for the user.
In Proceedings of the 1985 International JointConference on Artificial Intelligence, Los Angeles, CA.Pollack, M.; Hirschberg, J.; and Webber, B.
1982 User participationin the reasoning processes of expert systems.
In Proceedings ofthe 1982 National Conference on Artificial Intelligence, AAAI,Pittsburgh, PA.Rumelhart, D.E.
and Abrahamson, A.A. 1973 A model for analogicalreasoning.
Cognitive Psychology 5: 1-28.Schuster, E. 1982 Explaining and Expounding.
Technical ReportMS-CIS-82-49, University of Pennsylvania, Philadelphia, PA.Sidner, C.L.
1983 Focusing in the comprehension of definite ana-phora.
In Brady, Michael and Berwick, Robert (eds.
), Computa-tional Models of Discourse, MIT Press, Cambridge, MA: 267-330.Sleeman, D. 1982 Inferring (mal) rules from pupil's protocols.
InProceedings ofECAI-82, Orsay, France: 160-164.Stevens, A.L.
and Collins, A.
1980 Multiple conceptual models of acomplex system.
In Federico, Pat-Anthony; Snow, Richard E.;and Montague, William E.
(eds.
), Aptitude, Learning, andlnstruc-tion, Erlbaum, Hillsdale, NJ: 177-197.Stevens, A.; Collins, A.; and Goldin, S.E.
1979 Misconceptions instudent's understanding.
International Journal of Man-MachineStudies 11: 145-156.Tou, F.; Williams, M.; Fikes, R.; Henderson, A.; and Malone, T.1982 Rabbit: an intelligent database assistant.
In Proceedings ofAAAl-82, Carnegie-Mellon University, Pittsburgh, PA: 314-317.Tversky, A.
1977 Features of similarity.
Psychological Review 84:327-352.Weiner, E. Judith 1984 A knowledge representation approach tounderstanding metaphors.
Computational Linguistics 19(1):1-14.Woolf, Beverly P. 1984 Context Dependent Planning in a MachineTutor.
Ph.D. thesis, University of Massachusetts, Amherst, MA.Woolf, B. and McDonald, D. 1983 Human-computer discourse in thedesign of a pascal tutor.
In Janda, Ann (ed.
), CHl'83 ConferenceProceedings--Human Factors in Computing Systems, Boston,MA: 230-234.NOTES1.
While in general it may be necessary for the user model to containmore information about he user (e.g., goals and plans), these arenot required for the restricted task set out in this paper.2.
The dialogs contained in this paper were not taken directly fromthe transcripts.
They are used to illustrate the kinds of responsesfound.3.
Saying that exactly one perspective is active is actually a simpli-fication.
It may be the case that a number of perspectives can beactive.
In this case the resulting "active perspective" will besome function of the individual active perspectives.
The exactnature of this function is an open research question.4.
Note that the ROMPER system does not choose the activeperspective--it is given as input to the system.
This example issimply used to illustrate perspective's influence on misconceptionresponses.Computational Linguistics, Volume 14, Number 3, September 1988 63
