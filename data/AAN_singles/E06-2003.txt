LinguaStream: An Integrated Environmentfor Computational Linguistics ExperimentationFre?de?rik BilhautGREYC-CNRSUniversity of Caenfbilhaut@info.unicaen.frAntoine Widlo?cherGREYC-CNRSUniversity of Caenawidloch@info.unicaen.frAbstractBy presenting the LinguaStream plat-form, we introduce different methodolog-ical principles and analysis models, whichmake it possible to build hybrid experi-mental NLP systems by articulating cor-pus processing tasks.1 IntroductionSeveral important tendencies have been emergingrecently in the NLP community.
First of all, workon corpora tends to become the norm, which con-stitutes a fruitful convergence area between task-driven, computational approaches and descriptivelinguistic ones.
On corpora validation becomesmore and more important for theoretical models,and the accuracy of these models can be evalu-ated either with regard to their ability to accountfor the reality of a given corpus (pursuing descrip-tive aims), either with regard to their ability toanalyse it accurately (pursuing operational aims).From this point of view, important questions haveto be considered regarding which methods shouldbe used in order to project efficiently and accu-rately linguistic models on corpora.It is indeed less and less appropriate to considercorpora as raw materials to which models and pro-cesses could be immediately applicable.
On thecontrary, the multiplicity of approaches, wouldthey be lexical, syntactical, semantic, rhetoricalor pragmatical, would they focus on one of thesedimensions or cross them, raises questions abouthow these different levels can be articulated withinoperational models, and how the related process-ing systems can be assembled, applied on a cor-pus, and evaluated within an experimental process.New NLP concerns confirm these needs: re-cent works on automatic discourse structure anal-ysis, for example regarding thematic structures orrhetorical ones (Bilhaut, 2005; Widlo?cher, 2004),show that the results obtained from lower-grainedanalysers (such as part-of-speech taggers or lo-cal semantics analysers) can be successfully ex-ploited to perform higher-grained analyses.
In-deed, such works rely on non-trivial process-ing streams, where several modules collaboratebasing on the principles of incremental enrich-ment of documents and progressive abstractionfrom surface forms.
The LinguaStream plat-form (Widlo?cher and Bilhaut, 2005; Ferrari et al,2005), which is presented here, promotes and fa-cilitates such practices.
It allows complex pro-cessing streams to be designed and evaluated, as-sembling analysis components of various typesand levels: part-of-speech, syntax, semantics, dis-course or statistical.
Each stage of the processingstream discovers and produces new information,on which the subsequent steps can rely.
At the endof the stream, various tools allow analysed docu-ments and their annotations to be conveniently vi-sualised.
The uses of the platform range from cor-pora exploration to the development of fully oper-ational automatic analysers.Other platform or tools pursue similar goals.We share some principles with GATE (Cunning-ham et al, 2002), HoG (Callmeier et al, 2004)and NOOJ1 (Muller et al, 2004), but one impor-tant difference is that the LinguaStream platformpromotes the combination of purely declarativeformalisms (when GATE is mostly based on theJAPE language and NOOJ focuses on a uniqueformalism), and allows processing streams to bedesigned graphically as complex graphs (whenGATE relies on the pipeline paradigm).
Also, the1Formerly known as INTEX.95Figure 1: LinguaStream Integrated Environmentlow-level architecture of LinguaStream is compa-rable to the HoG middleware, but we are moreinterested in higher-level aspects such as analy-sis models and methodological concerns.
Finally,when other platforms usually enforce the use of adedicated document format, LinguaStream is ableto process any XML document.
On the other hand,LinguaStream is more targeted to experimentationtasks on low amounts of data, when tools such asGATE or NOOJ allow to process larger ones.2 The LinguaStream PlatformLinguaStream is an integrated experimentation en-vironment targeted to researchers in NLP.
It al-lows complex experiments on corpora to be re-alised conveniently, using various declarative for-malisms.
Without appropriate tools, the devel-opment costs that are induced by each new ex-periment become a considerable obstacle to theexperimental approach.
In order to address thisproblem, LinguaStream facilitates the realisationof complex processes while calling for minimaltechnical skills.Its integrated environment allows processingstreams to be assembled visually, picking individ-ual components in a ?palette?
(the standard setcontains about fifty components, and is easily ex-tensible using a Java API, a macro-component sys-tem, and templates).
Some components are specif-ically targeted to NLP, while others solve variousissues related to document engineering (especiallyto XML processing).
Other components are tobe used in order to perform computations on theannotations produced by the analysers, to visu-alise annotated documents, to generate charts, etc.Each component has a set of parameters that al-low their behaviour to be adapted, and a set of in-put and/or output sockets, that are to be connectedusing pipes in order to obtain the desired process-ing stream (see figure 2).
Annotations made on asingle document are organised in independent lay-ers and may overlap.
Thus, concurrent and am-biguous annotations may be represented in orderto be solved afterwards, by subsequent analysers.The platform is systematically based on XML rec-ommendations and tools, and is able to processany file in this format while preserving its originalstructure.
When running a processing stream, theplatform takes care of the scheduling of sub-tasks,and various tools allow the results to be visualisedconveniently.Fundamental principlesFirst of all, the platform makes use of declarativerepresentations, as often as possible, in order todefine processing modules as well as their connec-tions.
Thus, available formalisms allow linguisticknowledge to be directly ?transcribed?
and used.Involved procedural mechanisms, committed tothe platform, can be ignored.
In this way, givenrules are both descriptive (they provide a formalrepresentation for a linguistic phenomenon) andoperative (they can be considered as instructionsto drive a computational process).Moreover, the platform takes advantage of thecomplementarity of analysis models, rather thanconsidering one of them as ?omnipotent?, thatis to say, as able to express all constraint types.We indeed rely on the assumption that a complexanalyser can successively adopt several points ofview on the same linguistic data.
Different for-malisms and analysis models allow these differ-ent points of view.
In a same processing stream,we can successively make use of regular expres-sions at the morphologic level, a local unificationgrammar at the phrasal level, finite state trans-ducer at sentential level and constraint grammarfor discourse level analysis.
The interoperabil-ity between analysis models and the communica-tion between components are ensured by a unifiedrepresentation of markups and annotations.
Thelatter are uniformly represented by feature sets,which are commonly used in linguistics and NLP,and allow rich and structured information repre-sentation.
Every component can produce its ownmarkup using preliminary markups and annota-96tions.
Available formalisms make it possible to ex-press constraints on these annotations by means ofunification.
Thereby, the platform promotes pro-gressive abstraction from surface forms.
Inso-far as each step can access to annotations producedupstream, high level analysers often only use theseannotations, ignoring raw textual data.Another fundamental aspect consists in thevariability of analysis grain between differentanalysis steps.
Many analysis models require aminimal grain to be defined, called token.
For ex-ample, formalisms such as grammar or transduc-ers need a textual unit (such as character or word)to which patterns are applied.
When a componentrequires such a minimal grain, the platform allowsto define locally the unit types which have to beconsidered as tokens.
Any previously marked unitcan be used as such: usual tokenisation in wordsor any other beforehand analysed elements (syn-tagms, sentences, paragraphs...).
The minimal unitmay differ from an analysis step to another and thescope of the available analysis models is conse-quently increased.
In addition, each analysis mod-ule indicates antecedent markups to which it refersand considers as relevant.
Other markups can beignored and it makes it possible to partially riseabove textual linearity.
Combining these function-alities, it is possible to define different points ofview on the document for each analysis step.The modularity of processing streams pro-motes the reusability of components in variouscontexts: a given module, developed for a firstprocessing stream may be used in other ones.
Inaddition, every stream may be used as a singlecomponent, called macro-component, in a higherlevel stream.
Moreover, for a given stream, eachcomponent may be replaced by any other func-tionally equivalent component.
For a given sub-task, a rudimentary prototype may in fine be re-placed by an equivalent, fully operational, compo-nent.
Thus, it is possible to compare processingresults in rigourously similar contexts, which is anecessary condition for relevant comparisons.Figure 2: A Simple Processing StreamAnalysis modelsWe indicated above some of the componentswhich may be used in a processing stream.
Amongthose which are especially dedicated to NLP, twocategories have to be distinguished.
Some of themconsist in ready-made analysers linked to a spe-cific task.
For example, morpho-syntactic tag-ging (an interface with TreeTagger is provided bydefault) consists in such a task.
Although someparameters allow to adapt the associated compo-nents to the task (tag set for a given language...),it is impossible to fundamentally modify their be-haviour.
Others, on the contrary, provide an anal-ysis model, that is to say, firstly, a formalismfor representing linguistic constraints by meansof which the user can express expected process-ing.
This formalism will usually rely on a spe-cific operational model.
These analysis modelsallow constraints to be expressed, on surface formas well as on annotations produced by the prece-dent analysers.
All annotations are represented byfeature sets and the constraints are encoded by uni-fication on these structures.
Some of the availablesystems follow.?
A system called EDCG (Extended-DCG) al-lows local unification grammars to be writ-ten, using the DCG (Definite Clause Gram-mars) syntax of Prolog.
Such a grammarcan be described in a pure declarative mannereven if the features of the logical languagemay be accessed by expert users.?
A system called MRE (Macro-Regular-Expressions) allows patterns to be describedusing finite state transducers on surfaceforms and previously computed annotations.Its syntax is similar to regular expressionscommonly used in NLP.
However, this for-malism not only considers characters andwords, but may apply to any previously de-limited textual unit.?
Another descriptive, prescriptive and declar-ative formalism called CDML (Constraint-Based Discourse Modelling Language) al-lows a constraint-based approach of formaldescription and computation of discoursestructure.
It considers both textual segmentsand discourse relations, and relies on expres-sion and satisfaction of a set of primitive con-straints (presence, size, boundaries...) on pre-viously computed annotations.97?
A semantic lexicon marker, a configurabletokenizer (using regular expressions at thecharacter level), a system allowing linguisticunits to be delimited relying on the XML tagsthat are available in the original document,etc.3 ConclusionLinguaStream is used in several research and edu-cational projects:?
Works on discourse semantics: discourseframing (Ho-Dac and Laignelet, 2005; Bil-haut et al, 2003b), thematic (Bilhaut, 2005;Bilhaut and Enjalbert, 2005) and rhetorical(Widlo?cher, 2004) structures with a view toinformation retrieval and theoretical linguis-tics.?
Works on Geographical Information, as inthe GeoSem project (Bilhaut et al, 2003a;Widlo?cher et al, 2004), or in another researchproject (Marquesuza` et al, 2005).?
TCAN project: Temporal intervals and appli-cations to text linguistics, CNRS interdisci-plinary project.?
The platform is also used for other researchor teaching purposes in several French lab-oratories (including GREYC, ERSS and LI-UPPA) in the fields of corpus linguistics, nat-ural language processing and text mining.More information can be obtained from the ded-icated web site2.ReferencesFre?de?rik Bilhaut and Patrice Enjalbert.
2005.
Dis-course thematic organisation reveals domain knowl-edge structure.
In Proceedings of the ConferenceRecent Advances in Natural Language Processing,Pune, India.Fre?de?rik Bilhaut, Thierry Charnois, Patrice Enjalbert,and Yann Mathet.
2003a.
Passage extraction in geo-graphical documents.
In Proceedings of New Trendsin Intelligent Information Processing and Web Min-ing, Zakopane, Poland.Fre?de?rik Bilhaut, Lydia-Mai Ho-Dac, Andre?e Bo-rillo, Thierry Charnois, Patrice Enjalbert, Anne LeDraoulec, Yann Mathet, He?le`ne Miguet, Marie-Paule Pe?ry-Woodley, and Laure Sarda.
2003b.2http://www.linguastream.orgIndexation discursive pour la navigation intradoc-umentaire : cadres temporels et spatiaux dansl?information ge?ographique.
In Actes de TraitementAutomatique du Langage Naturel (TALN), Batz-sur-Mer, France.Fre?de?rik Bilhaut.
2005.
Composite topics in discourse.In Proceedings of the Conference Recent Advancesin Natural Language Processing, Borovets, Bul-garia.Ulrich Callmeier, Andreas Eisele, Ulrich Scha?fer, andMelanie Siegel.
2004.
The DeepThought Core Ar-chitecture Framework.
In Proceedings of the 4th In-ternational Conference on Language Resources andEvaluation, Lisbon, Portugal.Hamish Cunningham, Diana Maynard, KalinaBontcheva, and Valentin Tablan.
2002.
GATE:A framework and graphical development environ-ment for robust NLP tools and applications.
InProceedings of the 40th Anniversary Meeting of theAssociation for Computational Linguistics.Ste?phane Ferrari, Fre?fe?rik Bilhaut, Antoine Widlo?cher,and Marion Laignelet.
2005.
Une plate-formelogicielle et une de?marche pour la validation deressources linguistiques sur corpus : application a`l?e?valuation de la de?tection automatique de cadrestemporels.
In Actes des 4e`mes Journe?es de Linguis-tique de Corpus (JLC), Lorient, France.Lydia-Mai Ho-Dac and Marion Laignelet.
2005.
Tem-poral structure and thematic progression: A casestudy on french corpora.
In Symposium on theExploration and Modelling of Meaning (SEM?O5),Biarritz, France.Christophe Marquesuza`, Patrick Etcheverry, and JulienLesbegueries.
2005.
Exploiting geospatial mark-ers to explore and resocialize localized documents.In Proceedings of the 1st Conference on GeoSpatialSemantics (GEOS), Mexico City.ClaudeMuller, Jean Royaute, andMax Silberztein, edi-tors.
2004.
INTEX pour la Linguistique et le Traite-ment Automatique des Langues.
Presses Universi-taires de Franche-Comte?.Antoine Widlo?cher and Fre?de?rik Bilhaut.
2005.
Laplate-forme linguastream : un outil d?explorationlinguistique sur corpus.
In Actes de la 12eConfe?rence Traitement Automatique du LangageNaturel (TALN), Dourdan, France.Antoine Widlo?cher, Eric Faurot, and Fre?de?rik Bilhaut.2004.
Multimodal indexation of contrastive struc-tures in geographical documents.
In Proceedingsof Recherche d?Information Assiste?e par Ordinateur(RIAO), Avignon, France.Antoine Widlo?cher.
2004.
Analyse macro-se?mantique: vers une analyse rhe?torique du dis-cours.
In Actes de RECITAL?04, Fe`s, Maroc.98
