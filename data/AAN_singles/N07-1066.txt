Proceedings of NAACL HLT 2007, pages 524?531,Rochester, NY, April 2007. c?2007 Association for Computational LinguisticsA Probabilistic Framework for Answer Selection in Question AnsweringJeongwoo Ko1, Luo Si2, Eric Nyberg11Language Technologies Institute, Carnegie Mellon, Pittsburgh, PA 152132Department of Computer Science, Purdue University, West Lafayette, IN 47907jko@cs.cmu.edu, lsi@cs.purdue.edu, ehn@cs.cmu.eduAbstractThis paper describes a probabilistic an-swer selection framework for question an-swering.
In contrast with previous workusing individual resources such as ontolo-gies and the Web to validate answer can-didates, our work focuses on developinga unified framework that not only usesmultiple resources for validating answercandidates, but also considers evidence ofsimilarity among answer candidates in or-der to boost the ranking of the correct an-swer.
This framework has been used to se-lect answers from candidates generated byfour different answer extraction methods.An extensive set of empirical results basedon TREC factoid questions demonstratesthe effectiveness of the unified framework.1 IntroductionQuestion answering aims at finding exact answersto a user?s natural language question from a largecollection of documents.
Most QA systems com-bine information retrieval with extraction techniquesto identify a set of likely candidates and then uti-lize some selection strategy to generate the finalanswers (Prager et al, 2000; Clarke et al, 2001;Harabagiu et al, 2001).
Since answer extractorsmay be based on imprecise empirical methods, theselection process can be very challenging, as it oftenentails identifying correct answer(s) amongst manyincorrect ones.Question Question AnalysisQuery DocumentRetrieval CorpusDocsAnswerExtractionAnswer candidates Answer Selection AnswerShanghaiFT942-20160.5TaiwanFBIS3-453200.4ShanghaiFBIS3-580.64ShanghaiWSJ920110-00130.65HongKongAP880603-02680.7BeijingDocumentextractedScoreAnswercandidatesWhich cityin Chinahas thelargest numberof foreignfinancial companies?Figure 1: A traditional QA pipeline architectureFigure 1 shows a traditional QA architecture withan example question.
Given the question ?Whichcity in China has the largest number of foreign fi-nancial companies?
?, the answer extraction com-ponent produces a ranked list of five answer can-didates.
Due to imprecision in answer extraction,an incorrect answer (?Beijing?)
was ranked at thetop position.
The correct answer (?Shanghai?)
wasextracted from two documents with different confi-dence scores and ranked at the third and the fifth po-sitions.
In order to select ?Shanghai?
as the finalanswer, we need to address two issues:?
Answer Validation.
How do we identify correctanswer(s) amongst incorrect ones?
Validatingan answer may involve searching for facts ina knowledge base, e.g.
IS-A(Shanghai,city), IS-IN(Shanghai, China).?
Answer Similarity.
How do we exploit evi-dence of similarity among answer candidates?524For example, when there are redundant an-swers (?Shanghai?, as above) or several an-swers which represent a single instance (e.g.
?Clinton, Bill?
and ?William Jefferson Clin-ton?)
in the candidate list, how much should weboost the answer candidate scores?To address the first issue, several answer selec-tion approaches have used semantic resources.
Oneof the most common approaches relies on Word-Net, CYC and gazetteers for answer validation oranswer reranking; answer candidates are prunedor discounted if they are not found within a re-source?s hierarchy corresponding to the expected an-swer type (Xu et al, 2003; Moldovan et al, 2003;Prager et al, 2004).
In addition, the Web has beenused for answer reranking by exploiting search en-gine results produced by queries containing the an-swer candidate and question keywords (Magnini etal., 2002), and Wikipedia?s structured informationhas been used for answer type checking (Buscaldiand Rosso, 2006).To use more than one resource for answertype checking of location questions, Schlobachet al (2004) combined WordNet with geographi-cal databases.
However, in their experiments thecombination actually hurt performance because ofthe increased semantic ambiguity that accompaniesbroader coverage of location names.
This demon-strates that the method used to combine potentialanswers may matter as much as the choice of re-sources.To address the second issue we must determinehow to detect and exploit answer similarity.
As an-swer candidates are extracted from different docu-ments, they may contain identical, similar or com-plementary text snippets.
For example, the UnitedStates may be represented by the strings ?U.S.
?,?United States?
or ?USA?
in different documents.
Itis important to detect this type of similarity and ex-ploit it to boost answer confidence, especially for listquestions that require a set of unique answers.
Oneapproach is to incorporate answer clustering (Kwoket al, 2001; Nyberg et al, 2003; Jijkoun et al,2006).
For example, we might merge ?April 1912?and ?14 Apr 1912?
into a cluster and then chooseone answer as the cluster head.
However, clusteringraises new issues: how to choose the cluster headand how to calculate the scores of the clustered an-swers.Although many QA systems individually addressthese issues in answer selection, there has been lit-tle research on generating a generalized probabilisticframework that allows any validation and similarityfeatures to be easily incorporated.In this paper we describe a probabilistic answerselection framework to address the two issues.
Theframework uses logistic regression to estimate theprobability that an answer candidate is correct givenmultiple answer validation features and answer sim-ilarity features.
Experimental results on TRECfactoid questions (Voorhees, 2004) show that ourframework significantly improved answer selectionperformance for four different extraction techniques,when compared to default selection using the indi-vidual candidate scores produced by each extractor.This paper is organized as follows: Section 2 de-scribes our answer selection framework and Section3 lists the features that generate similarity and va-lidity scores for factoid questions.
In Section 4, wedescribe the experimental methodology and the re-sults.
Section 5 describes how we intend to extendour framework to handle complex questions.
FinallySection 6 concludes with suggestions for future re-search.2 MethodAnswer validation is based on an estimate of theprobability P (correct(Ai)|Ai, Q), where Q is aquestion and Ai is an answer candidate to the ques-tion.
Answer similarity is is based on an estimateof the probability P (correct(Ai)|Ai, Aj), where Ajis similar to Ai.
Since both probabilities influ-ence answer selection performance, it is importantto combine them in a unified framework and es-timate the probability of an answer candidate as:P (correct(Ai)|Q,A1, ..., An).In this paper, we propose a proba-bilistic framework that directly estimatesP (correct(Ai)|Q,A1, ..., An) using multipleanswer validation features and answer similarityfeatures.
The framework was implemented withlogistic regression, which is a statistical machinelearning technique used to predict the probabilityof a binary variable from input variables.
Logistic525P (correct(Ai)|Q,A1, ..., An) (1)?
P (correct(Ai)|val1(Ai), ..., valK1(Ai), sim1(Ai), ..., simK2(Ai))=exp(?0 +K1?k=1?kvalk(Ai) +K2?k=1?ksimk(Ai))1 + exp(?0 +K1?k=1?kvalk(Ai) +K2?k=1?ksimk(Ai))where, simk(Ai) =N?j=1(j 6=i)sim?k(Ai, Aj).~?, ~?,~?
= argmax~?,~?,~?R?j=1Nj?i=1logP (correct(Ai)|val1(Ai), ..., valK1(Ai), sim1(Ai), ..., simK2(Ai)) (2)regression has been successfully employed in manyapplications including multilingual document merg-ing (Si and Callan, 2005).
In our previous work (Koet al, 2006), we showed that logistic regressionperformed well in merging three resources to vali-date answers to location and proper name questions.We extended this approach to combine multiplesimilarity features with multiple answer validationfeatures.
The extended framework estimates theprobability that an answer candidate is correct giventhe degree of answer correctness and the amountof supporting evidence provided in a set of answercandidates (Equation 1).In Equation 1, each valk(Ai) is a feature functionused to produce an answer validity score for an an-swer candidate Ai.
Each sim?k(Ai, Aj) is a similar-ity function used to calculate an answer similaritybetween Ai and Aj .
K1 and K2 are the number ofanswer validation and answer similarity features, re-spectively.
N is the number of answer candidates.To incorporate multiple similarity features, eachsimk(Ai) is obtained from an individual similaritymetric.
For example, if Levenshtein distance is usedas one similarity metric, simk(Ai) is calculated bysumming N-1 Levenshtein distances between oneanswer candidate and all other candidates.
As somestring similarity metrics (e.g.
Levenshtein distance)produce a number between 0 and 1 (where 1 meanstwo strings are identical and 0 means they are differ-ent), similarity scores less than some threshold valueare ignored.The parameters ?, ?, ?
were estimated from train-ing data by maximizing the log likelihood as shownin Equation 2, where R is the number of trainingquestions and Nj is the number of answer candidatesfor each question Qj .
For parameter estimation, weused the Quasi-Newton algorithm (Minka, 2003).To select correct answers, the initial answer candi-date set is reranked according to the estimated prob-ability of each candidate.
For factoid questions, thetop answer is selected as the final answer to the ques-tion.
As logistic regression can be used for binaryclassification with a default threshold of 0.5, we canalso use the framework to classify incorrect answers:if the probability of an answer candidate is lowerthan 0.5, it is considered to be a wrong answer andis filtered out of the answer list.
This is useful indeciding whether or not a valid answer exists in thecorpus, an important aspect of the TREC QA evalu-ation (Voorhees, 2004).3 Feature RepresentationThis section details the features used to generate an-swer validity scores and answer similarity scores forour answer selection framework.5263.1 Answer Validation FeaturesEach answer validation feature produces a validityscore which predicts whether or not an answer can-didate is a correct answer for the question.
This taskcan be done by exploiting external QA resourcessuch as the Web, databases, and ontologies.
For fac-toid questions, we used gazetteers and WordNet in aknowledge-based approach; we also used Wikipediaand Google in a data-driven approach.3.1.1 Knowledge-based FeaturesIn order to generate answer validity scores usinggazetteers and WordNet, we reused the algorithmsdescribed in our previous work (Ko et al, 2006).Gazetteers: Gazetteers provide geographicinformation, which allows us to identifystrings as instances of countries, their cities,continents, capitals, etc.
For answer selec-tion, we used three gazetteer resources: theTipster Gazetteer, the CIA World Factbook(https://www.cia.gov/cia/publications/factbook/index.html) and information about the US states pro-vided by 50states.com (http://www.50states.com).These resources were used to assign an answervalidity score between -1 and 1 to each candidate(Figure 2).
A score of 0 means the gazetteers didnot contribute to the answer selection process forthat candidate.
For some numeric questions, rangechecking was added to validate numeric questionssimilarly to Prager et al (2004).
For example, giventhe question ?How many people live in Chile?
?,if an answer candidate is within ?
10% of thepopulation stated in the CIA World Factbook, itreceives a score of 1.0.
If it is in the range of 20%,its score is 0.5.
If it significantly differs by morethan 20%, it receives a score of -1.0.
The thresholdmay vary based on when the document was writtenand when the census was taken1.WordNet: The WordNet lexical database includesEnglish words organized in synonym sets, calledsynsets (Fellbaum, 1998).
We used WordNet in or-der to produce an answer validity score between -1and 1, following the algorithm in Figure 3.
A score1The ranges used here were found to work effectively, butwere not explicitly validated or tuned.1)  If the answer candidate directly matches the gazetteeranswer for the question, its gazetteer score is 1.0.
(e.g.Given the question ?What continent is Togo on?
?, thecandidate ?Africa?
receives a score of 1.0.
)2)  If the answer candidate occurs in the gazetteer withinthe subcategory of the expected answer type, its scoreis 0.5.
(e.g., Given the question ?Which city in Chinahas the largest number of foreign financialcompanies?
?, the candidates ?Shanghai?
and ?Boston?receive a score of 0.5 because they are both cities.
)3)  If the answer candidate is not the correct semantictype, its score is -1.
(e.g., Given the question ?Whichcity in China has the largest number of foreignfinancial companies?
?, the candidate ?Taiwan?receives a score of -1 because it is not a city.
)4) Otherwise, the score is 0.0.Figure 2: Validity scoring with gazetteers.1)  If the answer candidate directly matches WordNet, itsWordNet score is 1.0.
(e.g.
Given the question ?What isthe capital of Uruguay?
?, the candidate ?Montevideo?receives a score of 1.0.
)2)  If the answer candidate?s hypernyms include asubcategory of the expected answer type, its score is0.5.
(e.g., Given the question ?Who wrote the book?Song of Solomon??
", the candidate ?Mark Twain?receives a score of 0.5 because its hypernyms include?writer?.
)3)  If the answer candidate is not the correct semantictype, this candidate receives a score of -1.
(e.g., Giventhe question ?What state is Niagara Falls located in?
?,the candidate ?Toronto?
gets a score of -1 because it isnot a state.
)4) Otherwise, the score is 0.0.Figure 3: Validity scoring with WordNet.of 0 means that WordNet does not contribute to theanswer selection process for a candidate.3.1.2 Data-driven FeaturesWikipedia and Google were used in a data-drivenapproach to generate answer validity scores.Wikipedia: Wikipedia (http://www.wikipedia.org)is a multilingual free on-line encyclopedia.
Fig-ure 4 shows the algorithm used to generate ananswer validity score from Wikipedia.
If thereis a Wikipedia document whose title matches ananswer candidate, the document is analyzed toobtain the term frequency (tf) and the inverse term527Foreach answer candidateA i,1.
Initialize the Wikipediascore:ws(A i)= 02.
Search for aWikipedia document whosetitle isA i3.
If adocumentis found, calculatetf.idfscoreofA iin theretrieved Wikipediadocumentws(A i)= (1+log(tf)) ?(1+log(idf))4.
If not, for each questionkeyword Kj,4.1.Searchfora Wikipediadocumentthatincludes Kj4.2.If adocumentis found, calculatetf.idfscoreofA iws(A i)+=(1+log(tf)) ?
(1+log(idf))Figure 4: Validity scoring with Wikipedia1 )1( 2)()(?+?=dscsscsForeach answer candidateA i,1.
Initialize the Google score: gs(Ai) =02.
For eachsnippet s:2.1.Initializethesnippetco-occurrencescore:cs(s) =12.2.For each questionkeyword kin s:2.2.1 Compute distance d, the minimumnumberofwords betweenkand the answer candidate2.2.2 Update the snippet co-occurrence score:2.3.gs(A i)= gs(Ai) +cs(s)3.
Normalize the Google score (dividingit by aconstant C)Figure 5: Validity scoring with Googlefrequency (idf) of the candidate, from which atf.idf score is calculated.
When there is no matcheddocument, each question keyword is also processedas a back-off strategy, and the answer validity scoreis calculated by summing the tf.idf scores.
Tocalculate word frequency, the TREC Web Corpus(http://ir.dcs.gla.ac.uk/test collections/wt10g.html)was used as a large background corpus.Google: Following Magnini et al (2002), we usedGoogle to generate a numeric score.
A query con-sisting of an answer candidate and question key-words was sent to the Google search engine.
Tocalculate a score, the top 10 text snippets returnedby Google were then analyzed using the algorithmin Figure 5.3.2 Answer Similarity FeaturesWe calculate the similarity between two answer can-didates using multiple string distance metrics and alist of synonyms.3.2.1 String Distance MetricsThere are several different string distance metricsto calculate the similarity of short strings.
We usedfive popular string distance metrics: Levenshtein,Jaccard, Jaro, Jaro-Winkler, and Cosine similarity.3.2.2 SynonymsSynonyms can be used as another metric to calcu-late answer similarity.
We defined a binary similar-ity score for synonyms.sim(Ai, Aj) ={1, if Ai is a synonym of Aj0, otherwiseTo get a list of synonyms, we used three knowl-edge bases: WordNet, Wikipedia and the CIA WorldFactbook.
WordNet includes synonyms for Englishwords.
Wikipedia redirection is used to obtain an-other set of synonyms.
For example, ?Calif.?
is redi-rected to ?California?
in Wikipedia, and ?WilliamJefferson Clinton?
is redirected to ?Bill Clinton?.The CIA World Factbook includes five differentnames for a country: conventional long form, con-ventional short form, local long form, local shortform and former name.
For example, the conven-tional long form of Egypt is ?Arab Republic ofEgypt?, the conventional short form is ?Egypt?, thelocal short form is ?Misr?, the local long form is?Jumhuriyat Misr al-Arabiyah?
and the former nameis ?United Arab Republic (with Syria)?.
All are con-sidered to be synonyms of ?Egypt?.In addition, manually generated rules are used toobtain synonyms for different types of answer can-didates (Nyberg et al, 2003):?
Dates are converted into the ISO 8601 date for-mat (YYYY-MM-DD) (e.g., ?April 12 1914?and ?12th Apr.
1914?
are converted into ?1914-04-12?
and considered as synonyms).?
Temporal expressions are converted into theHH:MM:SS format (e.g., ?six thirty five p.m.?and ?6:35 pm?
are converted into ?18:35:xx?and considered as synonyms).?
Numeric expression are converted into sci-entific notation (e.g, ?one million?
and?1,000,000?
are converted into ?1e+06?
andconsidered as synonyms).528?
Representative entities are converted into therepresented entity when the expected answertype is COUNTRY (e.g., ?the Egyptian govern-ment?
is changed to ?Egypt?
and ?Clinton ad-ministration?
is changed to ?U.S.?
).4 ExperimentThis section describes the experiments we usedto evaluate our answer selection framework.
TheJAVELIN QA system (Nyberg et al, 2006) was usedas a testbed for the evaluation.4.1 Experimental SetupA total of 1760 factoid questions from the TREC8-12 QA evaluations served as a dataset, with 5-foldcross validation.To better understand how the performance of ourframework varies for different extraction techniques,we tested it with four JAVELIN answer extractionmodules: FST, LIGHTv1, LIGHTv2 and SVM (Ny-berg et al, 2006).
FST is an answer extractor basedon finite state transducers that incorporate a set ofextraction patterns (both manually-created and gen-eralized patterns).
LIGHTv1 is an extractor that se-lects answer candidates using a non-linear distanceheuristic between the keywords and an answer can-didate.
LIGHTv2 is another extractor based on adifferent distance heuristic, originally developed aspart of a multilingual QA system.
SVM is an extrac-tor that uses Support Vector Machines to discrimi-nate between correct and incorrect answers.Answer selection performance was measured byaverage accuracy: the number of correct top answersdivided by the number of questions where at leastone correct answer exists in the candidate list pro-vided by an extractor.
The baseline was calculatedwith the answer candidate scores provided by eachindividual extractor; the answer with the best extrac-tor score was chosen, and no validation or similarityprocessing was performed.
For Wikipedia, we useda version downloaded in Nov. 2005, which con-tained 1,811,554 articles.4.2 Results and AnalysisWe first analyzed the average accuracy when us-ing individual validation features.
Figure 6 showsthe effect of the individual answer validation fea-tures on different extraction outputs.
The combina-0.00.10.20.30.40.50.60.70.80.91.0ALLGLWIKIWNGZBaselineAverage AccuracyFSTLightV1LightV2SVMFigure 6: Average accuracy of individual answervalidation features (GZ: gazetteers, WN: WordNet,WIKI: Wikipedia, GL: Google, ALL: combinationof all features).tion of all features significantly improved the per-formance when compared to answer selection usinga single feature.
Comparing the data-driven featureswith the knowledge-based features, the data-drivenfeatures (such as Wikipedia and Google) increasedperformance more than the knowledge-based fea-tures (such as gazetteers and WordNet); our intuitionis that the knowledge-based features covered fewerquestions.
The biggest improvement was found withcandidates produced by the SVM extractor: a 242%improvement over the baseline.
It was mostly be-cause SVM tended to produce several answer can-didates with the same or very similar confidencescores, but our framework could select the correctanswer among many incorrect ones by exploitinganswer validation features.Table 1 shows the effect of individual similarityfeatures on different extractors when using 0.3 and0.5 as a similarity threshold, respectively.
Whencomparing five different string similarity features(Levenshtein, Jaro, Jaro-Winkler, Jaccard and Co-sine similarity), Levenshtein and Jaccard tended toperform better than the others.
When comparingsynonym features with string similarity features,synonyms performed slightly better.We also analyzed answer selection performancewhen combining all six similarity features (?All?
inTable 1).
Combining all similarity features did notimprove the performance except for the FST extrac-tor, because including five string similarity features529Similarity FST LIGHTv1 LIGHTv2 SVMfeature 0.3 0.5 0.3 0.5 0.3 0.5 0.3 0.5Levenshtein 0.728 0.728 0.471 0.455 0.399 0.400 0.381 0.383Jaro 0.708 0.705 0.422 0.440 0.373 0.378 0.274 0.282Jaro-Winkler 0.701 0.705 0.426 0.442 0.374 0.379 0.277 0.275Jaccard 0.738 0.738 0.438 0.448 0.452 0.448 0.382 0.390Cosine 0.738 0.738 0.436 0.435 0.418 0.422 0.380 0.378Synonyms 0.745 0.745 0.458 0.458 0.442 0.442 0.412 0.412Lev+Syn 0.748 0.751 0.460 0.466 0.445 0.448 0.420 0.412Jac+Syn 0.742 0.742 0.456 0.465 0.440 0.445 0.396 0.396All 0.755 0.755 0.405 0.425 0.435 0.431 0.303 0.302Table 1: Average accuracy using individual similarity features under different thresholds: 0.3 and 0.5(?Lev+Syn?
: the combination of Levenshtein with synonyms, ?Jac+Syn?
: the combination of Jaccard andsynonyms, ?All?
: the combination of all similarity metrics)Baseline Sim Val AllFST 0.658 0.751 0.855 0.877LIGHTv1 0.394 0.466 0.612 0.628LIGHTv2 0.343 0.448 0.578 0.582SVM 0.169 0.420 0.578 0.586Table 2: Average accuracy of individual features(Sim: merging similarity features, Val: merging val-idation features, ALL: combination of all features).provided too much redundancy to the logistic regres-sion.
We also compared the combination of Leven-shtein with synonyms and the combination of Jac-card with synonyms, and then chose Levenshteinand synonyms as the two best similarity features inour framework.We also analyzed the degree to which the averageaccuracy was affected by answer similarity and val-idation features.
Table 2 compares the average ac-curacy using the baseline, the answer similarity fea-tures, the answer validation features and all featurecombinations.
As can be seen, the similarity fea-tures significantly improved performance, so we canconclude that exploiting answer similarity improvesanswer selection performance.
The validation fea-tures also significantly improved the performance.When combining both sets of features together,the answer selection performance increased for allfour extractors: an average of 102% over the base-line, 30% over the similarity features and 1.82%over the validation features.
Adding the similarityfeatures to the validation features generated smallbut consistent improvement in all configurations.We expect more performance gain from similar-ity features when merging similar answers returnedfrom all four extractors.5 Extensions for Complex QuestionsAlthough we conducted our experiments on fac-toid questions, our framework can be easily ex-tended to handle complex questions, which requirelonger answers representing facts or relations (e.g.,?What is the relationship between Alan Greenspanand Robert Rubin??).
As answer candidates arelong text snippets, different features should be usedfor answer selection.
Possible validation featuresinclude question keyword inclusion and predicatestructure match (Nyberg et al, 2005).
For exam-ple, given the question ?Did Egypt sell Scud mis-siles to Syria?
?, the key predicate from the ques-tion is Sell(Egypt, Syria, Scud missile).
If there isa sentence which contains the predicate structureBuy(Syria, Scud missile, Egypt), we can calculatethe predicate structure distance and use it as a val-idation feature.
For answer similarity, we intend toexplore novelty detection approaches evaluated inAllan et al (2003).6 ConclusionIn this paper, we described our answer selectionframework for estimating the probability that an an-swer candidate is correct given multiple answer vali-530dation and similarity features.
We conducted a seriesof experiments to evaluate the performance of theframework and analyzed the effect of individual val-idation and similarity features.
Empirical results onTREC questions show that our framework improvedanswer selection performance in the JAVELIN QAsystem by an average of 102% over the baseline,30% over the similarity features alone and 1.82%over the validation features alone.We plan to improve our framework by adding reg-ularization and selecting the final answers amongcandidates returned from all extractors.
As ourcurrent framework is based on the assumption thateach answer is independent, we are building anotherprobabilistic framework which does not require anyindependence assumption, and uses an undirectedgraphical model to estimate the joint probability ofall answer candidates.7 AcknowledgmentsThis work was supported in part by ARDA/DTOAdvanced Question Answering for Intelli-gence (AQUAINT) program award numberNBCHC040164.ReferencesJ.
Allan, C. Wade, and A. Bolivar.
2003.
Retrieval andnovelty detection at the sentence level.
In Proceedingsof SIGIR.D.
Buscaldi and P. Rosso.
2006.
Mining Knowledgefrom Wikipedia for the Question Answering task.
InProceedings of the International Conference on Lan-guage Resources and Evaluation.C.
Clarke, G. Cormack, and T. Lynam.
2001.
Exploitingredundancy in question answering.
In Proceedings ofSIGIR.C.
Fellbaum.
1998.
WordNet: An Electronic LexicalDatabase.
MIT Press.S.
Harabagiu, D. Moldovan, M. Pasca, R. Mihalcea,M.
Surdeanu, R. Bunescu, R. Grju, V. Rus, andP.
Morarescu.
2001.
FALCON: Boosting knowledgefor answer engines.
In Proceedings of TREC.V.
Jijkoun, J. van Rantwijk, D. Ahn, E. Tjong Kim Sang,and M. de Rijke.
2006.
The University of Amsterdamat CLEF@QA 2006.
In Working Notes CLEF.J.
Ko, L. Hiyakumoto, and E. Nyberg.
2006.
Exploit-ing semantic resources for answer selection.
In Pro-ceedings of the International Conference on LanguageResources and Evaluation.C.
Kwok, O. Etzioni, and D. S. Weld.
2001.
Scal-ing question answering to the web.
In Proceedings ofWWW10 Conference.B.
Magnini, M. Negri, R. Pervete, and H. Tanev.
2002.Comparing statistical and content-based techniques foranswer validation on the web.
In Proceedings of theVIII Convegno AI*IA.T.
Minka.
2003.
A Comparison of Numerical Optimizersfor Logistic Regression.
Unpublished draft.D.
Moldovan, D. Clark, S. Harabagiu, and S. Maiorano.2003.
Cogex: A logic prover for question answering.In Proceedings of HLT-NAACL.E.
Nyberg, T. Mitamura, J. Carbonell, J. Callan,K.
Collins-Thompson, K. Czuba, M. Duggan,L.
Hiyakumoto, N. Hu, Y. Huang, J. Ko, L. Lita,S.
Murtagh, V. Pedro, and D. Svoboda.
2003.
TheJAVELIN Question-Answering System at TREC 2002.In Proceedings of the Text REtrieval Conference.E.
Nyberg, T. Mitamura, R. Frederking, M. Bilotti,K.
Hannan, L. Hiyakumoto, J. Ko, F. Lin, V. Pedro,and A. Schlaikjer.
2006.
JAVELIN I and II Systems atTREC 2005.
In Proceedings of TREC.E.
Nyberg, T. Mitamura, R. Frederking, V. Pedro,M.
Bilotti, A. Schlaikjer, and K. Hannan.
2005.
Ex-tending the javelin qa system with domain semantics.In Proceedings of AAAI-05 Workshop on Question An-swering in Restricted Domains.J.
Prager, E. Brown, A. Coden, and D. Radev.
2000.Question answering by predictive annotation.
In Pro-ceedings of SIGIR.J.
Prager, J. Chu-Carroll, K. Czuba, C. Welty, A. Itty-cheriah, and R. Mahindru.
2004 IBM?s Piquant inTrec2003.
In Proceedings of TREC.S.
Schlobach, M. Olsthoorn, and M. de Rijke.
2004.Type checking in open-domain question answering.
InProceedings of European Conference on Artificial In-telligence.L.
Si and J. Callan.
2005 CLEF2005: Multilingualretrieval by combining multiple multilingual rankedlists.
In Proceedings of Cross-Language EvaluationForum.E.
Voorhees.
2004.
Overview of the TREC 2003 ques-tion answering track.
In Proceedings of TREC.J.
Xu, A. Licuanan, J.
May, S. Miller, and R. Weischedel.2003.
TREC 2002 QA at BBN: Answer Selection andConfidence Estimation.
In Proceedings of TREC.531
