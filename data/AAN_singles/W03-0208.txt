Automatic Evaluation of Students?
Answers usingSyntactically Enhanced LSADharmendra Kanejiya?
and Arun Kumar?
and Surendra Prasad?
?Department of Electrical Engineering?Centre for Applied Research in ElectronicsIndian Institute of TechnologyNew Delhi 110016 INDIAkanejiya@hotmail.com arunkm@care.iitd.ernet.in sprasad@ee.iitd.ernet.inAbstractLatent semantic analysis (LSA) has been usedin several intelligent tutoring systems(ITS?s)for assessing students?
learning by evaluat-ing their answers to questions in the tutoringdomain.
It is based on word-document co-occurrence statistics in the training corpus anda dimensionality reduction technique.
How-ever, it doesn?t consider the word-order orsyntactic information, which can improve theknowledge representation and therefore lead tobetter performance of an ITS.
We present herean approach called Syntactically EnhancedLSA (SELSA) which generalizes LSA by con-sidering a word along with its syntactic neigh-borhood given by the part-of-speech tag of itspreceding word, as a unit of knowledge repre-sentation.
The experimental results on Auto-Tutor task to evaluate students?
answers to ba-sic computer science questions by SELSA andits comparison with LSA are presented in termsof several cognitive measures.
SELSA is ableto correctly evaluate a few more answers thanLSA but is having less correlation with humanevaluators than LSA has.
It also provides bet-ter discrimination of syntactic-semantic knowl-edge representation than LSA.1 IntroductionComputer based education systems are useful in dis-tance learning as well as for class-room learning envi-ronment.
These systems are based on intelligent tutor-ing systems(ITS?s) which provide an interactive learningenvironment to students.
These systems first familiarizea student with a topic and then ask questions to assessher knowledge.
Automatic evaluation of students?
an-swers is thus central to design of an ITS that can func-tion without the need of continuous monitoring by a hu-man.
Examples of ITS?s that use natural language pro-cessing to understand students?
contribution are CIRC-SIM (Glass, 2001), Atlas (Freedman et al, 2000), PACT(Aleven et al, 2001) etc.
These systems use a parser toderive various levels of syntactic and semantic informa-tion and rules to determine the next dialog move.
Theyperform quite well with short answers in a limited do-main, but are limited to take arbitrarily long free-text in-put and are difficult to port across domains.
These limi-tations can be alleviated by using latent semantic analy-sis(LSA), a recently developed technique for informationretrieval (Deerwester et al, 1990), knowledge represen-tation (Landauer et al, 1998), natural language under-standing and cognitive modeling (Graesser et al, 1999;Graesser et al, 2000) etc.
LSA has been used in vari-ous ITS?s like AutoTutor (Wiemer-Hastings et al, 1998),Intelligent Essay Assessor (Foltz et al, 1999), SummaryStreet (Kintsch et al, 2000), Apex (Dessus et al, 2000)etc.LSA is a statistical corpus-based natural language un-derstanding technique that supports semantic similaritymeasurement between texts.
Given a set of documentsin the tutoring domain, LSA uses the frequency of oc-currence of each word in each document to construct aword-document co-occurrence matrix.
After preprocess-ing, singular value decomposition is performed to repre-sent the domain knowledge into a 200 to 400 dimensionalspace.
This space is then used for evaluating the semanticsimilarity between any two text units.In an ITS, LSA is used to evaluate students?
answerswith respect to the ideal answers to questions in the do-main (Graesser et al, 2000).
This is done by finding thematch between a student?s answer and the ideal answer bycalculating the cosine similarity measure between theirprojections in LSA space.
This information is used toprovide interactive response to the student in terms ofhint, prompt,question etc.It has been found that LSA performs as good as anintermediate expert human evaluator but not so well as anaccomplished expert of the domain.
This may be becauseLSA is a ?bag-of-words?
approach and so lacks the word-order or syntactic information in a text document.
Butfor correct automatic evaluation of students?
answers, amodel should consider both syntax and semantics in theanswer.
So, one obvious way to improve the performanceof LSA is to incorporate some syntactic information in it.In order to add syntactic information to LSA, recentlythere has been an effort in (Wiemer-Hastings and Zipitria,2001), where a word along with its part-of-speech (POS)tag was used to construct the LSA matrix, thus capturingmultiple syntactic senses of a word.
But this approach,called tagged LSA, deteriorated the performance.
In an-other attempt (Wiemer-Hastings and Zipitria, 2001), sim-ilarity between two sentences was calculated by averag-ing the LSA based similarity of sub-sentence structureslike noun phrase, verb phrase, object phrase etc.
Thisapproach, called as structured LSA (SLSA), could im-prove the performance in terms of sentence-pair similar-ity judgment.
But its performance in terms of evaluatingstudents?
answers was poorer than that of LSA(Wiemer-Hastings, 2000).We propose here a model called Syntactically En-hanced LSA (SELSA), where we augment each word withthe part-of-speech (POS) tag of the preceding word.
Thusinstead of word-document co-occurence matrix, we gen-erate a matrix in which rows correspond to all possibleword - POS tag combinations and columns correspond todocuments.
A preceding tag indicates some kind of syn-tactic neighbourhood around the focus word.
Dependingon the preceding tag, the syntactic-semantic sense of aword can vary.
Thus SELSA captures finer resolution ofsyntactic-semantic information compared to mere seman-tics of LSA.
This finer information can therefore be usedto evaluate a student?s answer more accurately than LSA.We compare the performance of SELSA with LSAfor the AutoTutor cognitive modeling task (Graesser etal., 1999).
This involves evaluating students?
answers toquestions in three areas of computer science viz.
hard-ware, operating system and networking.
The perfor-mance is measured in terms of various criteria like cor-relation, mean absolute difference and number of correct/emphvs false evaluations by humans and by computer.SELSA is found better than LSA in terms of robustnessacross thresholds as well as in terms of evaluating moreanswers correctly, but it is having less correlation mea-sure with human than LSA.The organization of this paper is as follows.
The nextsection describes LSA and its applications in ITS?s.
Insection 3, we describe the proposed SELSA model.
Theexperimental details are given in section 4 followed bydiscussion on results in section 5.2 LSA in Intelligent Tutoring Systems2.1 A Brief Introduction to LSALSA is a statistical-algebraic technique for extracting andinferring contextual usage of words in documents (Lan-dauer et al, 1998).
A document can be a sentence, a para-graph or even a larger unit of text.
It consists of first con-structing a word-document co-occurrence matrix, scalingand normalizing it with a view to discriminate the impor-tance of words across documents and then approximatingit using singular value decomposition(SVD) in R dimen-sions (Bellegarda, 2000).
It is this dimensionality reduc-tion step through SVD that captures mutual implicationsof words and documents and allows us to project any textunit whether a word, a sentence or a paragraph as a vectoron the latent ?semantic?
space.
Then any two documentscan be compared by calculating the cosine measure be-tween their projection vectors in this space.LSA has been applied to model various ITS relatedphenomena in cognitive science e.g.
judgment of es-say quality scores (Landauer et al, 1998), assessing stu-dent knowledge by evaluating their answers to questionsetc (Graesser et al, 2000), deciding tutoring strategy(Lemaire, 1999).
It has been also used to derive a sta-tistical language model for large vocabulary continuousspeech recognition task (Bellegarda, 2000).2.2 LSA based ITS?sResearchers have long been attempting to develop a com-puter tutor that can interact naturally with students tohelp them understand a particular subject.
Unfortunately,however, language and discourse have constituted a seri-ous barrier in these efforts.
But recent technological ad-vances in the areas of latent semantic processing of natu-ral language, world knowledge representation, multime-dia interfaces etc have made it possible for various teamsof researchers to develop ITS?s that approach human per-formance.
Some of these are briefly reviewed below.2.2.1 AutoTutorAutoTutor task (Graesser et al, 1999) was developedat Tutoring Research Group of University of Memphis.AutoTutor is a fully automated computer tutor that as-sists students in learning about hardware, operating sys-tems and the Internet in an introductory computer literacycourse.
AutoTutor presents questions and problems froma curriculum script, attempts to comprehend learner con-tributions that are entered by keyboard, formulates dia-log moves that are sensitive to the learner?s contributions(such as prompts, elaborations, corrections and hints),and delivers the dialog moves with a talking head.
LSA isa major component of the mechanism that evaluates thequality of student contributions in the tutorial dialog.
Itwas found that the performance of LSA in terms of evalu-ating answers from college students was equivalent to anintermediate expert human evaluator.2.2.2 Intelligent Essay AssessorIntelligent essay assessor (Foltz et al, 1999) uses LSAfor automatic scoring of short essays that would be usedin any kind of content-based courses.
Student essays arecharacterized by LSA representations of the meaning oftheir contained words and compared with pre-graded es-says on degree of conceptual relevance and amount ofrelevant content by means of two kinds of scores: (1) theholistic score, the score of the closest pre-graded essayand (2) the gold standard, the LSA proximity betweenthe student essay and a standard essay.2.2.3 Summary StreetSummary Street (Kintsch et al, 2000) is also built ontop of LSA.
It helps students to write good summaries.First of all, a student is provided with a general advice onhow to write a summary, then the student selects a topic,reads the text and writes out a summary.
LSA proceduresare then applied to give a holistic grade to the summary.2.2.4 ApexApex (Dessus et al, 2000) is a web-based learning en-vironment which manages student productions, assess-ments and courses.
Once connected to the system, a stu-dent selects a topic or a question that he or she wishes towork on.
The student then types a text about this topicinto a text editor.
At any time, she can get a three-partevaluation of the essay based on content, outline and co-herence.
At the content level, the system identifies howwell the notions are covered by requesting LSA to mea-sure a semantic similarity between the student text andeach notion of the selected topic and correspondinglyprovides a message to the student.3 Syntactically Enhanced LSA (SELSA)LSA is based on word-document co-occurrence, alsocalled a ?bag-of-words?
approach.
It is therefore blindto word-order or syntactic information.
This puts limita-tions on LSA?s ability to capture the meaning of a sen-tence which depends upon both syntax and semantics.The syntactic information in a text can be characterized invarious ways like a full parse tree, a shallow parse, POStag sequence etc.
In an effort to generalize the LSA, wepresent here a concept of word-tag-document structure,which captures the behavior of a word within each syn-tactic context across various semantic contexts.
The ideabehind this is that the syntactic-semantic sense of a wordis specified by the syntactic neighborhood in which it oc-curs.
So representation of each such variation in an LSA-like space gives us a finer resolution in a word?s behaviorcompared to an average behavior captured by LSA.
Thisthen allows to compare two text documents based on theirsyntactic-semantic regularity and not based on semantics-only.
So it can be used in high quality text evaluationapplications.This approach is quite similar to the tagged LSA(Wiemer-Hastings and Zipitria, 2001) which considered aword along with its POS tag to discriminate multiple syn-tactic senses of a word.
But our approach is an extensionof this work towards a more general framework wherea word along with the syntactic context specified by itsadjacent words is considered as a unit of knowledge rep-resentation.
We define the syntactic context as the POStag information around a focus word.
In particular, welook at the POS tag of the preceding word also calledprevtag for convenience.
The motivation for this comesfrom statistical language modeling and left-to-right pars-ing literature where a word is predicted or tagged using itspreceding words and their POS tags.
Moreover, prevtagis used as an approximation to the notion of a precedingparse tree characterizing the word sequence before thefocus word.
But in general, we can also use the syntacticinformation from the words following the current word,e.g.
posttag, the POS tag of the next word.
However, oneof the concerns while incorporating syntactic informationin LSA is that of sparse data estimation problem.
So it isvery important to choose a robust characterization of syn-tactic neighbourhood as well as apply smoothing either atthe matrix formation level or at the time of projecting adocument in the latent space.The approach consists of first identifying a sufficientlylarge corpus representing the domain of tutoring.
Then aPOS tagger is used to convert it to a POS tagged corpus.The next step is to construct a matrix whose rows corre-spond to word-prevtag pairs and columns correspond todocuments in the corpus.
Again, a document can be asentence, a paragraph or a larger unit of text.
If the vo-cabulary size is I , POS tag vocabulary size is J and num-ber of documents in corpus is K, then the matrix willbe IJ ?
K. Let ci j,k denote the frequency of word wiwith prevtag pj in the document dk.
The notation i j (iunderscore j) in subscript is used for convenience and in-dicates word wi with prevtag pj i.e., (i ?
1)J + jth rowof the matrix.
Then as in LSA (Bellegarda, 2000), wefind entropy ?i j of each word-prevtag pair and scale thecorresponding row of the matrix by (1?
?i j).
The doc-ument length normalization to each column of the matrixis also applied by dividing the entries of kth document bynk, the number of words in document dk.
Let ti j be thefrequency of i jth word-prevtag pair in the whole corpusi.e.
ti j =?Kk=1 ci j,k.
Then ?i j and the matrix elementxi j,k are given as:?i j = ?1logKK?k=1ci j,kti jlogci j,kti j(1)xi j,k = (1?
?i j)ci j,knk(2)Once the matrix X is obtained, we perform its singularvalue decomposition (SVD) and approximate it by keep-ing the largest R singular values and setting the rest tozero.
Thus,X ?
X?
= USVT (3)where, U(IJ ?R) and V(K ?R) are orthonormal ma-trices and S(R?R) is a diagonal matrix.
It is this dimen-sionality reduction step through SVD that captures majorstructural associations between words-prevtags and docu-ments, removes ?noisy?
observations and allows the samedimensional representation of words-prevtags and docu-ments (albeit, in different bases).
This R-dimensionalspace can be called either syntactically enhanced latentsemantic space or latent syntactic-semantic space.After the knowledge is represented in the latentsyntactic-semantic space, we can project any new docu-ment as aR dimensional vector d?L in this space.
Let d bethe IJ ?
1 vector representing this document whose ele-ments di j are the frequency counts i.e.
number of timesword wi occurs with prevtag pj , weighted by its corre-sponding entropy measure (1 ?
?i j).
It can be thoughtof as an additional column in the matrix X, and thereforecan be thought of as having its corresponding vector v inthe matrix V. Then, d = USvT andd?L = SvT = UTd (4)which is a R?1 dimensional vector representation of thedocument in the latent space.We can also define a syntactic-semantic similaritymeasure between any two text documents as the cosineof the angle between their projection vectors in the latentsyntactic-semantic space.
With this measure we can ad-dress the problems that LSA has been applied to, namelynatural language understanding, cognitive modeling, sta-tistical language modeling etc.4 Experiment - Evaluating Students?AnswersWe have studied the performance of SELSA and com-pared it with LSA in the AutoTutor task (section 2.2.1)for natural language understanding and cognitive model-ing performance.
The details of the experiment are pre-sented below.4.1 CorpusThe tutoring research group at the University of Memphishas developed the training as well as testing corpus forthe AutoTutor task.
The training corpus consisted of twocomplete computer literacy textbooks, and ten articles oneach of the tutoring topics viz.
hardware, operating sys-tem and the Internet.
The test corpus was formed in thefollowing manner : eight questions from each of the threetopics were asked to a number of students.
Then eightanswers per question, 192 in total, were selected as testdatabase.
There were also around 20 good answers perquestion which were used in training and testing.
Usingthis corpus, we have implemented LSA and SELSA.4.2 Human Evaluation of AnswersFor comparing the performance of SELSA and LSA withhumans, we selected four human evaluators from com-puter related areas.
Three of them were doctorate candi-dates and one had completed it, thus they were expert hu-man evaluators.
Each of them were given the 192 student-answers and a set of good answers to each of the question.They were asked to evaluate the answers on the basis ofcompatibility score i.e.
the fraction of the number of sen-tences in a student-answer that matches any of the goodanswers.
Thus, the score for each answer ranged between0 to 1.
They were not told what constitutes a ?match?, butwere to decide themselves.4.3 Syntactic InformationWe approximated the syntactic neighborhood by the POStag of preceding word.
POS tagging was performedby the LTPOS software from the Language TechnologyGroup of University of Edinburgh1.
We also mapped the45 tags from Penn tree-bank tagset to 12 tags so as to con-sider major syntactic categories and also to keep the sizeof resulting matrix manageable.4.4 LSA and SELSA TrainingWe considered a paragraph as a unit of document.
Af-ter removing very small documents consisting less thanfour words, we had 5596 documents.
The vocabularysize, after removing words with frequency less than twoand some stopwords, was 9194.
The density of LSA andSELSA matrices were 0.27% and 0.025% respectively.SVD was performed using the MATLAB sparse matrixtoolbox.
We performed SVD with dimensions R varyingfrom 200 to 400 in steps of 50.4.5 Evaluation MeasureIn order to evaluate the performance of SELSA and LSAon AutoTutor task, we need to define an appropriate mea-sure.
The earlier studies on this task used a correlationcoefficient measure between the LSA?s rating and humanrating of the 192 answers.
We have also used this as oneof the three measures for comparison.
But for a task hav-ing small sample size, the correlation coefficient is notreliably estimated, so we defined two new performance1http://www.ltg.ed.ac.ukmeasures.
The first one was the mean absolute differencebetween the human and SELSA (correspondingly LSA)evaluations.
In the other measure we used the compari-son of how many answers were correctly evaluated versushow many were falsely evaluated by SELSA (LSA) ascompared to human evaluations.
A detailed explanationof these measures is given in the following section.5 Results and DiscussionsWe calculated the compatibility score evaluation usingSELSA (LSA) in an analogous way to the human evalua-tion.
Thus SELSA (LSA) would evaluate the answers inthe following manner.
It would first break each student-answer into a number of sentences and then evaluate eachsentence against the good answers for that question.
Ifthe cosine measure between the SELSA (LSA) represen-tation of the sentence and any good answer exceeded apredefined threshold then that part was considered cor-rect.
Thus it would find the fraction of the number ofsentences in a student-answer that exceeded the thresh-old.
We performed the experiments by varying thresholdbetween 0.05 to 0.95 with a step of 0.05.
We also variedthe number of singular values R from 200 to 400 with astep of 50.
In the following, we present our results usingthe three evaluation measures.5.1 Correlation AnalysisFor each of the five SVD dimensions R and each valueof the thresholds, we calculated the correlation coeffi-cient between the SELSA (LSA) evaluation and each hu-man rater?s evaluation.
Then we averaged this across thefour human evaluators.
The resulting average correlationcurves for SELSA and LSA are shown in figs.
(1) and (2)respectively.From these two figures we observe that maximum cor-relation between SELSA and human raters is 0.47 andthat between LSA and human is 0.51 while the averageinter-human correlation was 0.59.
Thus LSA seems to becloser to human than SELSA in this particular tutoringtask.
This seems to support the arguments from (Lan-dauer et al, 1997) that syntax plays little role, if any, insemantic similarity judgments and text comprehension.But the likely reason behind this could be that the corpus,particularly the student answers, contained very poor syn-tactic structure and also that human evaluators might nothave paid attention to grammatical inaccuracies in thistechnical domain of computer literacy.But it is also worth noting that SELSA is closer toLSA than a previous approach of adding syntactic in-formation to LSA (Wiemer-Hastings, 2000), which hada correlation of 0.40 compared to 0.49 of LSA on thesame task of evaluating students?
answers, where aver-age inter-human correlation was 0.78 between the expertraters and 0.51 between the intermediate experts.
SELSA0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 10.150.20.250.30.350.40.450.5 Correlation between evaluations by Human and SELSAThresholdCorrelationR 200R 250R 300R 350R 400Figure 1: Correlation between SELSA and human evalu-ators0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 10.10.150.20.250.30.350.40.450.50.55 Correlation between evaluations by Human and LSAThresholdCorrelationR 200R 250R 300R 350R 400Figure 2: Correlation between LSA and human evalua-torsis also comparable to tagged LSA (Wiemer-Hastings andZipitria, 2001), which used the current POS tag insteadof prevtag.
It had a correlation of 0.27 compared to 0.36of LSA in a modified evaluation task of judging similar-ity between two sentences where the correlation betweenskilled raters was 0.45 and that between non-proficientraters was 0.35.If we look at these curves more carefully, especially,their behavior across thresholds, then it is interesting tonote that SELSA has wider threshold-widths(TW) thanLSA across all the cases of SVD dimension R. In ta-ble (1) and (2) we have shown the 10% and 20% TWof SELSA and LSA respectively.
This is calculated byfinding the range over thresholds for which the correla-tion is within 10% and 20% of the maximum correlation.This observation shows that SELSA is much more robustacross thresholds than LSA in the sense that semantic in-formation is discriminated better in SELSA space than inLSA space.R Cormax Tmax 10% TW 20% TW200 0.46 0.45 0.48 0.63250 0.47 0.40 0.42 0.61300 0.47 0.40 0.41 0.62350 0.45 0.50 0.55 0.65400 0.46 0.50 0.45 0.64Table 1: Threshold Width of SELSAR Cormax Tmax 10% TW 20% TW200 0.49 0.65 0.33 0.44250 0.51 0.65 0.29 0.44300 0.51 0.60 0.26 0.41350 0.50 0.60 0.32 0.44400 0.50 0.50 0.32 0.50Table 2: Threshold Width of LSA0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 10.150.20.250.30.350.40.450.50.55CorrelationThresholdCorrelation for LSA and SELSA at R=250LSA250SELSA250Figure 3: LSA vs SELSA for SVD dimensions 250Another interesting observation occurs when we plotthe two curves simultaneously as shown in fig.
(3).
Herewe plotted the SELSA and LSA performances for 250 di-mensions of latent space.
We can easily see that SELSAperforms better than LSA for thresholds less than 0.5 andviceversa.
This observation along with the previous ob-servation about TW can be understood in the followingmanner.
When comparing two document vectors for acosine measure exceeding a threshold, we can considerone of the vectors to be the axis of a right circular conewith a semi-vertical angle decided by the threshold.
If theother vector falls within this cone, we say the two docu-ments are matching.
Now if the human raters emphasizedsemantic similarity, which is most likely the case, thenthis means that LSA could best capture the same infor-mation in a narrower cone while SELSA required a widercone.
This is quite intuitive in the sense that SELSA haszoomed the document similarity measure axis by puttingfiner resolution of syntactic information.
Thus mere se-mantically similar documents are placed wider apart inSELSA space than syntactic-semantically similar docu-ments.
This concept can be best used in a language mod-eling task where a word is to be predicted from the his-tory.
It is observed in (Kanejiya et al, 2003) that SELSAassigns better probabilities to syntactic-semantically reg-ular words than LSA, although the overall perplexity re-duction over a bi-gram language model was less than thatby LSA.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 10.20.250.30.350.40.450.50.550.60.65 Mean Absolute Difference between evaluations by Human and SELSAThresholdmeanabsolutedifferenceR 200R 250R 300R 350R 400Figure 4: Mean absolute difference between SELSA andhuman evaluators0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 10.20.250.30.350.40.450.50.550.60.65 Mean Absolute Difference between evaluations by Human and LSAThresholdmeanabsolutedifferenceR 200R 250R 300R 350R 400Figure 5: Mean absolute difference between LSA and hu-man evaluators5.2 Mean Absolute Difference AnalysisHere we calculated the mean absolute difference(MAD)between a human rater?s evaluation and SELSA (LSA)evaluations as follow:MAD =1192192?i=1|hi ?
li| (5)where, hi and li correspond to human and SELSA(LSA)evaluation of ith answer.
This was then averaged acrosshuman evaluators.
These results are plotted in figs.
(4)and (5).
These two curves show that SELSA and LSA arealmost equal to each other.
Again SELSA has the advan-tage of more robustness and in most cases it is even betterthan LSA in terms of minimum MAD with human.
Ta-bles (3) and (4) show values of minimum MAD at variousvalues of SVD dimensions R. The best minimum MADfor SELSA is 0.2412 at 250 dimensional space while thatfor LSA is 0.2475 at 400 dimensions.
The average MADamong human evaluators is 0.2050.R minMAD maxCorrect minFalse200 0.2449 125 31250 0.2412 125 30300 0.2422 126 30350 0.2484 125 31400 0.2504 124 32Table 3: SELSA - MAD, correct and false evaluationR minMAD maxCorrect minFalse200 0.2497 122 29250 0.2523 120 31300 0.2555 121 32350 0.2525 122 32400 0.2475 123 30Table 4: LSA - MAD, correct and false evaluation5.3 Correct vs False Evaluations AnalysisWe define an evaluation li by SELSA (LSA) to be corrector false as below:li CORRECT if |li ?
hi| < CTli FALSE if |li ?
hi| > FTwhere CT and FT are correctness and falsehood thresh-olds which were set to 0.05 and 0.95 respectively for strictmeasures.
Number of such correct as well as false eval-uations were then averaged across the four human evalu-ators.
They are plotted in figs.
(6) and (7) for SELSAand LSA respectively (the upper curves correspondingto correct and the lower ones to false evaluations).
Themaximum number of correct (maxCorrect) and the min-imum number of false (minFalse) evaluations across thethresholds for each value of SVD dimensions are calcu-lated and shown in tables (3) and (4).
We observe that thebest performance for SELSA is achieved at 300 dimen-sions with 126 correct and 30 false evaluations, while forLSA it is at 400 dimensions with 123 correct and 30 falseevaluations.
The average correct and false evaluationsamong all human-human evaluator pairs were 132 and 23respectively.
Thus here also SELSA is closer to humanevaluators than LSA.
In fact, for the cognitive task likeAutoTutor, this is a more appealing and explicit measurethan the previous two.
Apart from these three measures,one can also calculate precision, recall and F-measure(Burstein et al, 2003) to evaluate the performance.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 130405060708090100110120130 Correct and False evaluations using SELSAThresholdNumber of CorrectandFalseEvaluationsR 200R 250R 300R 350R 400Figure 6: Correct and false evaluations by SELSA ascompared to human evaluators0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 130405060708090100110120130 Correct and False evaluations using LSAThresholdNumber of CorrectandFalseEvaluationsR 200R 250R 300R 350R 400Figure 7: Correct and False evaluations by LSA as com-pared to human evaluators6 ConclusionAutomatic evaluation of students?
answers in an intelli-gent tutoring system can be performed using LSA.
ButLSA lacks syntactic information which can be also use-ful for meaning representation of a text document.
So, wehave developed and implemented a model called syntacti-cally enhanced LSA which generalizes LSA by augment-ing a word with the POS tag of the preceding word to de-rive a latent syntactic-semantic information.
Experimen-tal results on the AutoTutor task of evaluating students?answers to computer science questions show a range ofperformance comparison between SELSA and LSA.
Interms of the correlation measure with human raters, LSAis slightly better than SELSA.
But SELSA is at least asgood as LSA in terms of the mean absolute differencemeasure.
On the other end, SELSA is able to correctlyevaluate a few more answers than LSA is.
SELSA cando better if the training and testing corpora have a goodsyntactic structure.From the correlation performance analysis, it is ob-served that SELSA is more robust in discriminating thesemantic information across a wider threshold width thanLSA.
It is also found that SELSA uses the syntactic infor-mation to expand the document similarity measure i.e.,mere semantically similar documents are placed widerapart than syntactic-semantically similar documents inSELSA space.These initial results are part of an ongoing research to-wards an overall improvement of natural language under-standing and modeling.
Although the present version ofSELSA has limited improvements over LSA, it leads tofuture experiments with robust characterization of syn-tactic neighbourhood in terms of headwords or phrasestructure as well as applying smoothing across syntax totackle the problem of sparse data estimation.ReferencesV.
Aleven, O. Popescu, and K. R. Koedinger.
2001.
Atutorial dialogue system with knowledge-based under-standing and classification of student explanations.
InWorking notes of the 2nd IJCAI Workshop on Knowl-edge and Reasoning in Practical Dialogue Systems,Seattle.J.
R. Bellegarda.
2000.
Exploiting latent semantic infor-mation in statistical language modeling.
Proceedingsof the IEEE, 88(8):1279?1296.J.
Burstein, C. Leacock, and M. Chodorow.
2003.
Crite-rion: Online essay evaluation: An application for au-tomated evaluation of student essays.
In Proc.
of theFifteenth Annual Conf.
on Innovative Applications ofArtificial Intelligence, Acapulco, Mexico.
(in press).S.
Deerwester, S. T. Dumais, G. W. Furnas, T. K. Lan-dauer, and R. Harshmann.
1990.
Indexing by latentsemantic analysis.
Journal of the American Society forInformation Science, 41(6):391?407.P.
Dessus, B. Lemaire, and A. Vernier.
2000.
Free-textassessment in a virtual campus.
In Proc.
3rd Int.
Conf.on Human Systems Learning (CAPS?2000), Paris.P.
W. Foltz, D. Laham, and T. K. Landauer.
1999.
Auto-mated essay scoring: applications to educational tech-nology.
In Proc.
of the ED-MEDIA?99 conference,Charlottesville.
AACE.R.
Freedman, C. P. Ros, M. A. Ringenberg, and K. Van-Lehn.
2000.
ITS tools for natural language dialogue:A domain-independent parser and planner.
In Fifth In-ternational Conference on Intelligent Tutoring Systems(ITS 2000), Montreal.
Springer-Verlag.M.
Glass.
2001.
Processing language input in theCIRCSIM-tutor intelligent tutoring system.
In J. D.Moore, C. L. Redfield, and W. L. Johnson, editors, Ar-tificial Intelligence in Education, pages 210?221.
IOSPress, San Antonio.A.
C. Graesser, K. Wiemer-Hastings, P. Wiemer-Hastings, R. Kreuz, and Tutoring Research Group.1999.
Autotutor: A simulation of a human tutor.
Jour-nal of Cognitive Systems Research, 1:35?51.A.
C. Graesser, P. Wiemer-Hastings, K. Wiemer-Hastings, D. Harter, Tutoring Research Group, andN.
Person.
2000.
Using latent semantic analysis toevaluate the contributions of students in autotutor.
In-teractive Learning Environments, 8(2):129?147.D.
P. Kanejiya, A. Kumar, and S. Prasad.
2003.
Statis-tical language modeling using syntactically enhancedLSA.
In Proc.
TIFR Workshop on Spoken LanguageProcessing, pages 93?100, Mumbai, India.E.
Kintsch, D. Steinhart, G. Stahl, and the LSA Re-search Group.
2000.
Developing summarization skillsthrough the use of lsa-based feedback.
InteractiveLearning Environments, 8(2):87?109.T.
K. Landauer, D. Laham, B. Rehder, and M. E.Schreiner.
1997.
How well can passage meaning bederived without using word order?
a comparison oflatent semantic analysis and humans.
In Proc.
9th an-nual meeting of the Cognitive Science Society, pages412?417, Mawhwah, NJ.
Erlbaum.T.
K. Landauer, P. W. Foltz, and D. Laham.
1998.
In-troduction to latent semantic analysis.
Discourse Pro-cesses, 25:259?284.B.
Lemaire.
1999.
Tutoring systems based on latent se-mantic analysis.
In S. Lajoie and M. Vivet, editors, Ar-tificial Intelligence in Education, pages 527?537.
IOSPress, Amsterdam.P.
Wiemer-Hastings and I. Zipitria.
2001.
Rules for syn-tax, vectors for semantics.
In Proc.
23rd Annual Conf.of the Cognitive Science Society, Mawhwah, NJ.
Erl-baum.P.
Wiemer-Hastings, A. C. Graesser, D. Harter, and Tu-toring Research Group.
1998.
The foundation and ar-chitecture of autotutor.
In Proc.
4th Int.
Conf.
on Intel-ligent Tutoring Systems, Berlin.
Springer-Verlag.P.
Wiemer-Hastings.
2000.
Adding syntactic informa-tion to lsa.
In Proc.
22nd Annual Conf.
of the Cog-nitive Science Society, pages 988?993, Mawhwah, NJ.Erlbaum.
