Everyday Language is HighlyIntensionalAllan RamsayUniversity of Manchester (UK)email: allan.ramsay@manchester.ac.ukDebora FieldUniversity of Sheffield (UK)email: D.Field@sheffield.ac.ukAbstractThere has recently been a great deal of work aimed at trying to extractinformation from substantial texts for tasks such as question answering.Much of this work has dealt with texts which are reasonably large, butwhich are known to contain reliable relevant information, e.g.
FAQ lists,on-line encyclopaedias, rather than looking at huge unorganised resourcessuch as the web.
We believe, however, that even this work underestimatesthe complexity and subtlety of language, and hence will inevitably berestricted in what it can cope with.
In particular, everyday use of lan-guage involves considerable amounts of reasoning over intensional ob-jects (properties and propositions).
In order to respond appropriately tosimple-seeming questions such as ?Is going for a walk good for me?
?, forinstance, you have to be able to talk about event-types, which are intrinsi-cally intensional.
We discuss the issues involved in handling such items,and shows the kind of background knowledge that is required for drawingthe appropriate conclusions about them.193194 Ramsay and Field1 IntroductionThe work reported here aims to allow users to interact with a health information sys-tem via natural language.
In this context, allowing a user to make simple statementsabout their condition and then ask questions about what they can or should do, as in(1), seems to be a minimal requirement.
(1) My doctor says I am allergic to eggs.
Is it safe for me to eat cake?Understanding such utterances requires the use of a highly intensional representa-tion language, and responding to them requires a surprising amount of backgroundknowledge.
We will consider below the problems that such everyday utterances bringfor formal paraphrases of natural language, and we will look at the kind of back-ground knowledge that is required for producing the right kinds of response.
In orderto produce a system that carries out the required inference we need access to an in-ference engine for carrying out proofs in a representation language with the requiredexpressive power.
The details of the engine we use are beyond the scope of this paper.
(Ramsay, 2001; Ramsay and Field, 2008).
For the purposes of the current paper wewill simply show the results that can be obtained by using it.The work reported here is complementary to work on corpus-based approachessuch as textual entailment: approaches that ignore the intensionality of everyday lan-guage will inevitably fail to capture important inference patterns, but on the other handthe work reported here cannot deal with large amounts of information provided as freetext.
Ideally, the two approaches will be combined.
The aim of the current paper isto provide a reminder of the prevalence of intensionality in everyday language, andto demonstrate that modern theorem proving techniques can cope with this kind ofknowledge without introducing undue processing delays.2 BackgroundThe general idea behind the work reported here is that users will input statementsabout their health, either spontaneously or in response to prompts from the system,and will ask questions about what they can and should do, and the system will providethem with appropriate guidance.
The overall architecture is completely classical:1.
The user?s input is translated into a meaning representation (logical form, LF)in some suitable representation language.2.
This LF contains a specification of the illocutionary force of the input (is it astatement, or a question, or a command, or .
.
.
?).3.
If the utterance is classified as a statement, its propositional content is added tothe system?s view of the user?s beliefs, and if it is classified as a question, thesystem will attempt to use its background knowledge of the domain to answerit.
We are not currently attempting to make the system do anything in responseto a command from the user, since users do not generally issue commands inour chosen domain, but clearly if this did happen then we would want to makethe system construct a plan to carry out the required action.Everyday Language is Highly Intensional 195This part of the system?s activity requires it be able to access and exploit relevantbackground knowledge.
This is obvious in the case of questions, but in the givendomain it is also important to be able to spot situations where the user?s beliefsare incomplete or are in conflict with the system?s beliefs, since most people?sunderstanding about medical topics is flawed.
The ability to reason about whathas been said, then, is crucial to the construction of appropriate responses.This architecture is entirely orthodox.
What is unusual about the current work isthe emphasis on intensionality, so the first thing to do is examine why we believe thatthis is such a significant problem.1.
Doctors and patients make extensive use of generic NPs and bare plurals: ?Ifyou follow this diet you should manage to control them without drugs?, ?Do younormally have snacks?
?, ?When I started chemotherapy, on the 2nd of August,glycaemia was still rather high?
.
.
.Such NPs are not, in fact, all that much more prevalent in this domain than ingeneral language.
Across the BNC, for instance, it turns out that 27% of NPshave ?the?
as their determiner, 19% are bare plurals, 29% are bare singulars,11% have ?a?
or ?an?
as their determiner, and the remainder have a variety ofother determiners1.Thus bare plural and generic singular NPs occur about as frequently as ?the?
and?a?, and substantially more freqently than ?some?, ?all?
and ?every?
(less than1% each).
They have, however, been much less widely discussed by formalsemanticists, and there are a number of serious problems with the analyses thathave been proposed (Carlson, 1989; Ramsay, 1992; Cohen, 1994).2.
Everyday language is littered with words that can be used either as nouns orverbs, and many of the apparently verbal uses of such words occur in essentiallynominal contexts.
Table 1 shows the pattern of usage for three common words2,but it should be noted that about 25% of the instances that are classified asverbs are present participle forms, many of which are actually nominal or verbalgerunds and hence should be regarded as nouns.Table 1: Uses of common words in the BNCVerb Noun Otherwalk 75% 22% 3%run 70% 24% 6%kick 63% 35% 2%Axiomatisation of the semantics of such words requires considerable care, sincewe need to ensure that all the examples in (2) have very similar consequences.
(2) a.
Swimming is good for you.1The count of bare singulars is in fact a slight overestimate, since it includes some uses of singular nounsas modifiers.2The classification is taken directly from the BNC tags.196 Ramsay and Fieldb.
Going for a swim is good for you.c.
It is good for you to go swimming.3.
The goal of the project is to produce appropriate responses to simple statementsand queries about a patient?s health.
To do this, we need to be able to specifya body of background knowledge in this area.
We believe that for applicationssuch as medical information provision it is important that the information pro-vided be as accurate as possible, and hence that it may be necessary to providethe required background knowledge from scratch.
This is, of course, a verytime-consuming and challenging activity, and it would be nice to be able toside-step it by extracting the required information from existing texts.
Unfortu-nately, it seems likely that any such existing text will contain gaps which willlead to the generation of partial, or wrong, answers.
As noted above, ideally wewould want to link special purpose knowledge of the kind outlined here withinformation extracted from existing texts, but for the current paper we are justlooking at what is involved in providing the required knowledge from scratch.It turns out, as will be seen below, that much of this knowledge involves quantifica-tion over situation types (of roughly the kind discussed by (Barwise and Perry, 1983)),and in particular it involves statements about whether one situation type is a subset ofanother, or is incompatible with it.
This kind of knowledge is intrinsically intensional,but it is hard to see how it can be avoided in this domain.3 Logical formsThe logical forms that we use are fairly orthodox.?
We assume that events are first-class objects, as suggested by Davidson David-son (1967, 1980).?
We allow other entities to play named roles with respect to these events, wherewe denote that some item X is, for instance, the agent of some event E by writing?
(E,agent,X): using this notation, rather than writing agent(E,X), allows usto quantify over thematic roles, which in turn allows us to state generalisationsthat would otherwise be awkward.?
We treat tense as a relation between speech time and ?reference time?, and aspectas a relation between reference time and event time, as suggested by Reichen-bach Reichenbach (1947, 1956).?
We use ?reference terms?
to denote referring expressions, so that re f (?Xman(X))is used to denote ?the man?.
Reference terms are similar to ?anchors?
from (Bar-wise and Perry, 1983), though the treatment is essentially proof-theoretic (sim-ilar to the discussion of presupposition in (Gazdar, 1979; van der Sandt, 1992))rather than model theoretic.?
Given that we are particularly concerned with the intensional nature of naturallanguage, we need to use a formal language that supports intensionaly.
TheEveryday Language is Highly Intensional 197language we choose is a constructive version of property theory (Turner, 1987;Ramsay, 2001).
We have extended the theorem prover described in (Ramsay,2001) to cope with reasoning about knowledge and belief, and we have shownhow this can be used to carry out interesting inferences in cooperative and non-cooperative situations (Ramsay and Field, 2008).We also include the surface illocutionary force in the LF, since this is part of themeaning of the utterance and hence it seems sensible to include it in the LF.
In partic-ular, there are interactions between surface illocutionary force and other aspects of themeaning which are hard to capture if you treat them independently.
This is slightlyless standard than the other aspects of our LFs, but it does have the advantage thatthese LFs keep all the information that we can obtain by inspecting the form of theutterance in one place.A typical example of an LF for a simple sentence is given in Figure 13.
(3) The man loves a woman.claim(?B : {woman(B)}?C : {past(now,C)}?D : {aspect(C,simplePast,D)}?
(D, agent, ref (?E(man(E))))&?
(D,object,B)&event(D, love))Figure 1: Logical form for (3)If you want to reason about utterances in natural language, e.g.
in order to answerquestions on the basis of things you have been told, then there seems to be no alterna-tive to constructing LFs of the kind in Figure 1, axiomatising the relevant backgroundknowledge, and then invoking your favourite theorem prover.
Shallow semantic anal-ysis simply does not provide the necessary detail, and it is very hard to link textualentailment algorithms (Dagan et al, 2005) to complex domain knowledge.
The crit-ical issue in connecting NLP systems to rich axiomatisations of domain knowledgeseems likely to be that existing frameworks for constructing meaning representationsare not rich enough, not that they are too rich.
In the remainder of this paper we willexplore three specific issues that have arisen in our attempt to use natural languageas a means for accessing medical knowledge.
We have beoome sensitised to theseissues because of their importance for our application, but we believe that they are ac-tually widespread, and they will need to be solved for any system which links naturallanguage to complex domain knowledge.4 Bare NPsConsider (4):3All the formal paraphrases in this paper are obtained from the target sentences by parsing the text andusing the standard techniques of compositional semantics.198 Ramsay and Field(4) a. I am eating eggs.b.
I eat eggs.c.
I am allergic to eggs.What is the status of ?eggs?
in these sentences?It is clear that in (4a) there are some eggs that I am eating, so that (4a) means some-thing quite like ?I am eating some eggs.?.
(4b), on the other hand, means somethingfairly different from ?There are some eggs that I eat?, since it does not seem to committhe speaker to the existence of any specific set of eggs.
The use of the simple aspectwith a non-stative verb gives (4b) a habitual/repeated interpretation, saying that thereare numerous eating events, each of which involves at least one egg.It seems, then, that it is possible to treat ?eggs?
in (4a) and (4b) as a narrow scopeexistential, with the simple aspect introducing a set of eating events of the requiredkind.You would not, however, want to paraphrase (4c) by saying that there are some eggsto which I am allergic.
(4b) says that there is a relationship between me and situationswhere there is an egg present, namely that if I eat something which has been madeout of some part of an egg then I am likely to have an allergic reaction.
The bareplural ?eggs?
in (4c) seems to have some of the force of a universal quantifier.
This isproblematic: does the bare plural ?eggs?
induce an existential or a universal reading,or something entirely different?Note that the word ?eggs?
can appear as a free-standingNP (as in (4a)) or as the headnoun of an NP with an explicit determiner (as in ?He was cooking some eggs.?).
In thelatter context, the meaning of ?eggs?
is normally taken be the property ?X(egg(X)),to be combined with the determiner ?some?
to produce an existentially quantified ex-pression which can be used as part of the interpretation of the entire sentence.It is clear that there are constructions that involve allowing prepositions to takenouns rather than NPs as their complements, in examples like ?For example, cockerelsgenerally have more decorative plumage than hens?, where ?example?
is evidently anoun rather than an NP.
If we allow the adjective ?allergic?
to select for a PP with anoun complement rather than an NP complement, we can obtain an interpretation of(4c) which says that my allergy is a relation between me and the property of being anegg (= the set of eggs) (Figure 2).utt(claim,?Bstate(B,allergic(to,?C(egg(C))),ref (?D(speaker(D)))!0)&aspect(now,simple,B))Figure 2: Logical form for (4c)Thus we can distinguish between cases where ?eggs?
is being used as an NP, whereit introduces a narrow scope existential quantifier, and ones where it is being used asan NN, where it denotes, as usual, the property ?
(X ,egg(X)).
We still have to workEveryday Language is Highly Intensional 199out saying that the relationship ?allergic?
holds between me and the property of beingan egg, but at least we have escaped the trap of saying that it holds between me andsome eggs (or indeed all eggs).
We will return ton this in ?65 Nominalisations and paraphrasesAs noted above, there are often numerous ways of saying very much the same thing,and these often involve using combinations of nominal and verbal forms of the sameroot.
To cope with these, we have to do two things: we have to construct appropriatelogical forms, and we have to spot cases where we believe that there is no significantdifference between the various natural language forms and introduce appropriate rulesfor treating one as canonical.Gerunds and gerundives occur in very much the same places as bare NPs, and havevery much the same feeling of being about types of entity.
(5) a.
Exercise is good for you.b.
Swimming is good for you.
(6) a. I like watching old movies.b.
I like old movies.It therefore seems natural to treat them in much the same way, as descriptions ofevent types, as in Figure 3utt(claim,?Bstate(B,?C(?Devent(D,swim) & ?
(D,agent,C)),?E(good(E)))&for(B,ref (?F(hearer(F)))!4)&aspect(now,simple,B))Figure 3: Logical form for (5b)The logical form in Figure 3 says that there is a state of affairs relating events wheresomeone does some swimming and the property of being good, and that this state ofaffairs concerns the speaker.
This does at least have the benefit of exposing the keyconcepts mentioned in (5b), and of doing so in such a way that it is possible to writerules that support appropriate chains of inference.The kind of inferencewe are interested in concerns patterns like the ones in Figure 4Exercise is good for you if you are overweightSwimming is a form of exerciseI am obeseShould I go swimming?Figure 4: A simple(!)
pattern of natural reasoning200 Ramsay and FieldWe will discuss the rules and inference engine that are required in order to supportthis kind of reasoning in ?6 and ?7.
For now we are concerned with the fact that thelast line in Figure 4 could have been replaced by a number of alternative forms suchas ?Is swimming good for me??
or ?Is it good for me to go swimming?
without anysubstantial change of meaning.In general, we believe that determining the relationships between sentences re-quires inference based on background rules which describe the relationships betweenterms.
However, when we have forms which are essentially paraphrases of one an-other, these rules will tend to be bi-equivalences?rules of the form P ?
Q.
Such rulesare awkward for any theorem prover, since they potentially introduce infinite loops:in order to prove P you can try proving Q, where one of the possible ways of provingQ is by proving P, .
.
.
It is possible to catch such loops, and our inference engine doesmonitor for various straightforward loops of this kind, but they do introduce an extraoverhead.
Equivalences of this kind are, in any case, not really facts about the world somuch as facts about the way natural language describes the world.
It seems thereforemore sensible to capture them at the point when we construct our logical forms, whenthey can be dealt with by straightforward pattern matching and substitution on logicalforms, rather than by embodying them as bi-directional rules to be used as required bythe inference engine.
We use rules of the kind given in Figure 5 to canonical versionsof logical forms for sentences which we regard as mutual paraphrases.
These rulesare matched against elements of the logical form, and the required substitutions aremade.
This process is applied iteratively, so that multiple rules can be applied whennecessary.
?B : {allergy(B,C)}?Devent(D,have) & ?
(D,object,B)& ?
(D,agent,E) & aspect(X,Y ,D)?
?Fstate(F,E,?G(allergic(G)),to(C))& aspect(X,Y ,F)event(B,go)&?(B,event,?C(event(C,D)))&?(B,agent,E)?
event(B,D) &?
(B,agent,E)Figure 5: Canonical form rulesThe first of the rules in Figure 5 captures the equivalences between ?I have an al-lergy to eggs?
and ?I am allergic to eggs?, ?having an allergy to milk is bad news?
and?being allergic to milk is bad news?, and so on, and the second captures the equiv-alences between ?I like walking?
and ?I like going walking?, ?Swimming is good foryou?
and ?Going for a swim is good for you?, and so on.
These equivalences have tobe captured somewhere, and we believe that canonical forms of this kind arte a goodway to do it.
We will return to where the rules in Figure 5 come from in ?8.Everyday Language is Highly Intensional 2016 Intensional predicatesThe material we are interested in, like all natural language, makes extensive use ofintensional predicates.
The adjective ?good?
in ?Going swimming is good for you?
ex-presses a relationship between an event type (?going swimming?)
and an individual;the verb ?make?
in ?Eating raw meat will make you feel sick?
expresses a relation-ship between an event type (?eating raw meat?)
and a state of affairs (?you are ill?
).Constructions like these are widespread, and are inherently intensional.
To draw con-clusions about sentences involving them, you have to be able to reason about whetherone event type or one parameterised state of affairs is a subset of another, which is theessence of intensionality.Once you recognise that examples like these involve event types and propositions,it is fairly straightforward to construct appropriate logical forms.
We simply use thenotation of the ?-calculus to depict abstractions (e.g.
event types), and we allow propo-sitions to appear in argument positions, and standard techniques from comppsitionalsemantics do the rest.
?C : {future(now,C)}?Bevent(B,make)&?
(B,scomp,?D(event(D,feel)& ?
(D,object,?E(sick(E)))& ?
(D,agent,ref (?F(hearer(F)))!5)))&?
(B,cause,?G ?Hevent(H,eat)&?I : {raw(I) & meat(I)}?(H,object,I)&?
(H,agent,G))&aspect(C,simple,B)Figure 6: Eating raw meat will make you feel sickFigure 6 describes a relationship between situations where you eat raw meat andones where you feel sick.
This is entirely correct: what else could this sentence de-note?Constructing formal paraphrases for sentences involving intensional predicates isthus both straightforward (so long as you can parse them) and essential.
Formal lan-guages that support such paraphrases are, however, potentially problematic.
The keyproblem is that such languages tend to permit paradoxical constructions such as theLiar Paradox and Ruessll?s set which introduce sentences which are true if and only ifthey are false.
It is difficult to provide semantics for languages which allow paradoxesto be stated, but there are a number of ways out of this dilemma, either by puttingsyntactic restrictions on what can be said (Whitehead and Russell, 1925; Jech, 1971)or by devising appropriate interpretations (Turner, 1987; Aczel, 1988).
We choose toemploy a constructive variant of property theory, because it allows us a comparativelystraightforward and implemetable proof theory, but it does not really matter what youchoose.
What does matter is that if you choose a language with less expressive power202 Ramsay and Fieldthan natural language, such as description logic, your paraphrases must fail to supportsome of the distinctions that are expressible in natural language, and as a consequenceyou will inevitably draw incorrect conclusions from the texts you are processing.7 InferenceConsider (7):(7) a.
Eating eggs will make you ill if you are allergic to eggs.b.
I am allergic to eggs.c.
Will eating fried-egg sandwiches make me ill?It is pretty obvious that the answer to (7c), given (7a) and (7b), must be ?Yes?.
Thereasoning that is required to arrive at this answer turns out to be suprisingly complex.The problem is, as noted above, that we need to reason about relationships betweenevent types.
We need to be able to spot that events where someone eats a fried-eggsandwich involve situations where they eat an egg.
It is clearly quite easy, if tedious,to write rules that say that if someone eats something which contains an egg then theymust eat an egg, and that fried-egg sandwiches contain eggs.
The trouble is that wehave to be able invoke this rule in order to determine whether the arguments of ?make?are of the right kind.
Because we are (correctly) allowing event types as argumentsin intensional predicates, we have to be able to invoke arbitrary and unpredictableamounts of inference even to determine whether the arguments of a predicate are ad-missible.
Roughly speaking, we have to be prepared to carry out arbitrary amounts ofinference at the point where first-order theorem provers invoke unification.There is nothing to stop us doing this.
Sorted logics, for instance, use an extendednotion of unification to try to ensure that items that are being considered as argumentshave specific properties (Cohn, 1987).
We can, indeed, do any computation we like inorder to verify the suitability of arguments.
The more complex the computations weperform, of course, the longer it may take to come to a decision.
The key is thus totry to bound the potential costs without compromising what we can do too much.
Weexploit a notion of ?guarded?
axioms, where we allow arbitrary amounts of reasoningto be performed to verify that some item fits a fully specified description, but we donot allow such reasoning to be used for generating candidates.
We do, of course, haveto put a bound on the amount of work that will be done at any point, as indeed anyinference engine for a language as expressive as first-order logic must do.
In general,however, using guarded intensionality in this way allows us to cover a wide rangeof cases which are simply inexpressible using first-order logic (or any fragment offirst-order logic, such as description logic) comparatively inexpensively.8 ConclusionsWe have argued that in order to cope properly with even quite straightforward usesof language, you need large amounts of background knowledge, much of which hasto be couched in some highly intensional framework, and you need inference engineswhich can manipulate this knowledge.
In the body of the paper we have shown anumber of examples which we believe illustrate this argument, and have looked at therepresentations and rules that we employ for dealing with these cases.
The naturalEveryday Language is Highly Intensional 203question that arises at this point is: that?s all very well, but can the approach outlinedhere be extended to cover a more substantial set of cases?There are two key issues here.
How difficult is it to capture a reasonably substantialbody of knowledge within the framework we have outlined, and what will happen tothe inference engine when we do?Writing rules in property theory is very hard work.
Writing rules in property theorywhich will mesh nicely with logical forms obtained from natural language sentencesis extremely hard work.
If we had to hand-code the rules we want directly in propertytheory (or indeed in any formal language) then the approach discussed here would,clearly, be impossible to extend to cover more than a handful of cases.
Fortunately,however, we have a much easier way of constructing rules.
We have, after all, amechanism for converting natural language sentences into logical forms.
So if westate the rules we want in natural language we will obtain logical forms of those rules,and furthermore those paraphrases will automatically be couched in terms which meshnicely with logical forms obtained from other natural language sentences.
Thus (8)produces the rule in Figure 7(8) Eating Y will make X ill if X is allergic to Y.?C?D?Estate(E,C,?F(allergic(F))) & to(E,D)& aspect(now,simple,E)?
?G : {future(now,G)}?Bevent(B,make)&?(B,object,C)&?(B,object1,?H(ill(H)))&?
(B,agent,?I ?Jevent(J,eat)& ?
(J,object,D)& ?
(J,agent,I))&aspect(G,simple,B)Figure 7: Logical form for (8)Writing rules like (8) is clearly easier than producing formulae like Figure 7 byhand.
Writing down all the knowledge you need in order to cope with a non-trivialdomain is still a very substantial task, but doing it in English is at least feasible in away that doing it directly in a formal language is not.How will the inference engine cope when confronted with thousands of rules?
Verylarge parts of everyday knowledge can, in fact, be expressed pretty much as Hornclauses.
Our inference engine converts Horn clauses into (almost) pure Prolog, andthere is certainly no problem in using very large sets of Horn clauses converted tothis form (a modern Prolog system will cope comfortably with sets of several hun-dred thousand Horn clauses, and will carry out substantial inference chains involvingsuch sets in small fractions of a second).
The only concern here relates to non-Hornclauses (which do not tend to occur all that frequently in rules explaining the rela-tionships between natural language terms) and intensional rules.
The fact that most204 Ramsay and Fieldintensional rules are guarded has certainly meant that so far we have not encounteredany problems when using them, and we are hopeful that this will remain the case.In any case, there is an alternative question to be answered: what will happen ifyou don?t take the approach outlined here?
All the phenomena we have discussed arewidespread?bare plurals, mutual paraphrases, intensional attitudes all occur all overthe place.
It is extremely hard to see that systems that rely on surface patterns (eitherdirectly, as in textual entailment, or indirectly through shallow parsing/informationextraction) can support the kind of reasoning required for getting from ?I have anallergy to eggs.?
to ?It is dangerous for me to eat pancakes?, so at some point inferencebased on background knowledgewill have to be invoked.
There seems little alternativeto constructing formal paraphrases that capture the subtleties of natural language in allits glory.
If you don?t, then you will by definition lose some of the information thatwas expressed in the text, and that will inevitably mean that you get things wrong.There is no way round it: either you bite the bullet, construct formal paraphrases thatcapture the content of the input and use them to carry out inference, or you will getsome things wrong.ReferencesAczel, P. (1988).
Non-Well-Founded-Sets.
Stanford: CSLI Publications.Barwise, J. and J. Perry (1983).
Situations and Attitudes.
Cambridge, MA: BradfordBooks.Carlson, G. (1989).
On the semantic composition of English generic sentences.
InG.
Chierchia, B. H. Partee, and R. Turner (Eds.
), Properties, Types and MeaningII: Semantic Issues, Dordrecht, pp.
167?192.
Kluwer Academic Press.Cohen, A.
(1994).
Reasoning with generics.
In H. C. Bunt (Ed.
), 1st InternationalWorkshop on Computational Semantics, University of Tilburg, pp.
263?270.Cohn, A. G. (1987).
A more expressive formulation of many sorted logic.
Journal ofAutomated Reasoning 3, 113?200.Dagan, I., B. Magnini, and O. Glickman (2005).
The PASCAL recognising textual en-tailment challenge.
In Proceedings of Pascal Challenge Workshop on RecognizingTextual Entailment.Davidson, D. (1967).
The logical form of action sentences.
In N. Rescher (Ed.
), TheLogic of Decision and Action, Pittsburgh.
University of Pittsburgh Press.Davidson, D. (1980).
Essays on actions and events.
Oxford: Clarendon Press.Gazdar, G. (1979).
Pragmatics: Implicature, Presupposition and Logical Form.
NewYork: Academic Press.Jech, T. J.
(1971).
Lectures in Set Theory, with Particular Emphasis on the Method ofForcing.
Berlin: Springer Verlag (Lecture Notes in Mathematics 217).Everyday Language is Highly Intensional 205Ramsay, A. M. (1992).
Bare plural NPs and habitual VPs.
In Proceedings of the 14thInternational Conference on Computational Linguistics (COLING-92), Nantes, pp.226?231.Ramsay, A. M. (2001).
Theorem proving for untyped constructive ?-calculus: imple-mentation and application.
Logic Journal of the Interest Group in Pure and AppliedLogics 9(1), 89?106.Ramsay, A. M. and D. G. Field (2008).
Speech acts, epistemic planning and Grice?smaxims.
Journal of Logic and Computation 18(3), 431?457.Reichenbach, H. (1947).
Elements of Symbolic Logic.
New York: The Free Press.Reichenbach, H. (1956).
The Direction of Time.
Berkeley: University of CaliforniaPress.Turner, R. (1987).
A theory of properties.
Journal of Symbolic Logic 52(2), 455?472.van der Sandt, R. (1992).
Presupposition projection as anaphora resolution.
Journalof Semantics 9, 333?377.Whitehead, A. N. and B. Russell (1925).
Principia Mathematica.
Cambridge: Cam-bridge University Press.
