Proceedings of the NAACL HLT 2010 Student Research Workshop, pages 34?39,Los Angeles, California, June 2010. c?2010 Association for Computational LinguisticsA Learning-based Sampling Approach to Extractive SummarizationVishal Juneja and Sebastian Germesin and Thomas KleinbauerGerman Research Center for Artificial IntelligenceCampus D3.266123 Saarbu?cken, Germany{firstname.lastname}@dfki.deAbstractIn this paper we present a novel resamplingmodel for extractive meeting summarization.With resampling based on the output of a base-line classifier, our method outperforms previ-ous research in the field.
Further, we com-pare an existing resampling technique withour model.
We report on an extensive se-ries of experiments on a large meeting corpuswhich leads to classification improvement inweighted precision and f-score.1 IntroductionFeature-based machine learning approaches havebecome a standard technique in the field of extrac-tive summarization wherein the most important sec-tions within a meeting transcripts need to be iden-tified.
We perceive the problem as recognizing themost extract-worthy meeting dialog acts (DAs) in abinary classification framework.In this paper, firstly, in section 4 we create a goldstandard to train the classifier, by improvising uponthe existing annotations in our meeting corpus.
Thenin section 5 we present actual numbers which dis-play a very skewed class distribution to learn forthe binary classifier.
This skewness is attributed tothe less number of actual extract-worthy and im-portant DAs (positive examples) compared to ordi-nary chit-chat, backchannel noises etc (negative ex-amples) spoken during the course of the meeting.We tackle this data skewness with a novel resam-pling approach which reselects the data set to createa more comparable class distribution between thesepostive and negative instances.Resampling methods have been found effective incatering to the data imbalance problem mentionedabove.
(Corbett and Copestake, 2008) used a re-sampling module for chemical named entity recog-nition.
The pre-classifier, based on n-gram characterfeatures, assigned a probability of being a chemicalword, to each token.
Only tokens having probabilitygreater than a predefined threshold were preservedand the output of the first stage classification alongwith word suffix were used as features in furtherclassification steps.
(Hinrichs et al, 2005) used ahybrid approach for Computational Anaphora Res-olution (CAR) combining rule based filtering withMemory based learning to reduce the huge popu-lation of anaphora/candidate-antecedent pairs.
(Xieet al, 2008), in their experimentation on the ICSImeeting corpus, employ the salience scores gener-ated by a TFIDF classifier in the resampling task.We discuss the actual technique and our resamplingmodule further in section 6.We compare its performance with the TFIDFmodel of (Xie et al, 2008) in section 8.2 and observea general improvement in summary scores throughresampling.2 DataWe use the scenario meetings of the AMI corpusfor our experiments in this paper which compriseabout two thirds of around 100 hours of recordedand annotated meetings.
The scenario meetings eachhave four participants who play different roles in afictitious company for designing a remote control.The AMI corpus has a standard training set of 9434meetings1 and 20 meetings each for developmentand testing.Annotators wrote abstractive summaries for eachmeeting and then linked summary sentences tothose DA segments from the meeting transcriptswhich best conveyed the information in the ab-stracts.
There was no limit on the number of links anannotator could create and a many-to-many mappingexists between the meeting DA segments and humanabstracts.
Here, DA segments are used in analogyto sentences in document summarization becausethe spontaneously spoken material in meeting tran-scripts rarely contains actual grammatical sentences.3 Pre-processing and Feature ExtractionTo the feature set of (Murray, 2008) listed in table1 we add some high level features.
Since the mainfocus of this paper is to deal with the data imbal-anace issue hence for the sake of completeness andreproducibility of our work we briefly mention thebasic features used.
In section 8.3 we explicitly re-port the performance rise over the baseline due tothe added features.3.1 Lexical and Structural featuresThe list of added features include the number ofcontent words (nouns and adjectives) in a DA.
(Ed-mundson, 1969) looked at cue-phrases, keywordstitle and location of a sentence as features indica-tive of important sections in a document.
We usea handpicked list of cue words like ?for example?,?gonna have?
etc as binary features.
We also addseveral keywords like ?remote?,?plastic?
etc basedupon manual scrutiny, as binary features into theclassifier.
Further we use DA labels of current andfour adjacent DAs as features.3.2 DisfluencyThe role of disfluencies in summarization has beeninvestigated by (Zhu and Penn, 2006) before.
Theyfound that disfluencies improve summarization per-formance when used as an additional feature.
Wecount the number of disfluent words in a DA usingan automatic disfluency detector.1Three of the meetings were missing some required features.3.3 ProsodicWe employ all the signal level features described by(Murray, 2008) which include mean, max and stan-dard deviation of energy and pitch values normal-ized by both speaker and meeting.
The duration ofthe DA in terms of time and number of words spo-ken.
The subsequent, precedent pauses and rate ofspeech feature.DA Featuresmean energymean pitchmaximum energy valuemaximum pitch valuestandard deviation of pitchprecedent pausesubsequent pauseuninterrupted lengthnumber of wordsposition in the meetingposition in the speaker turnDA time durationspeaker dominance in DAspeaker dominance in timerate of speechSUIDF scoreTFIDF scoreTable 1: Features used in baseline classifier4 Gold StandardIn supervised frameworks, the creation of gold-standard annotations for training (and testing) isknown to be a difficult task, since (a) what shouldgo into a summary can be a matter of opinion and(b) multiple sentences from the original documentmay express similar content, making each of themequally good candidates for selection.
The hypoth-esis is well supported by the low kappa value (Co-hen, 1960) of 0.48 reported by (Murray, 2008) onthe AMI corpus.We describe the procedure for creating the goldstandard for our experimentation in this paper.Firstly we join all annotations and rank the DAsfrom most number of links to least number of linksto create a sorted list of DAs.
Depending on a pre-defined variable percentage as gold standard cut-off35or threshold we preserve the corresponding numberof highest ranked DAs in the above list.
For evalu-ation, (Murray, 2008) uses gold standard summariesobtained using similar procedure.
For training, how-ever, he uses all DA segments with at least one linkas positive examples.As the term gold standard for the data set, cre-ated above, is misleading.
We call the set of DAsso obtained by using this ranking and resamplingprocedure as Weighted-Resampled Gold Standard(WRGS).
Henceforth in this paper, for a resamplingrate of say 35% we will name the set of DAs so ob-tained as WRGS(35%) or simply WRGS for someundefined, arbitrary threshold.5 Data SkewnessIn this section we focus on the skewed data setwhich arises because of creating WRGS for trainingour classifiers.
Consider the set of DAs with at leastone link to the abstractive or human summaries.
Letus call it DAl?1.
This set accounts for 20.9% of allDAs in the training set.set size%WRGS(25%) 5.22%DAl?1 20.9%Table 2: Set sizes in % of all training DAsAgain consider set of DAs for WRGS(25%).
Thisset, by definition, contains 25% of all DAs in theset DAl?1.
Hence the set WRGS(25%) constitute5.22% of all DAs in the training set.
Note that this isa skewed class distribution as also visible in table 2.Our system employs resampling architectureshown in figure 1.
The first classifier is similar inspirit to the one developed in (Murray, 2008) withthe additional features listed in section 3.
The out-put we use is not the discrete classification result butrather the probability for each DA segment to be ex-tracted.These probabilities are used in two ways for train-ing the second classifier: firstly, to create the resam-pled training set and secondly, as an additional fea-ture for the second classifier.
The procedure for re-sampling is explained in the section 6.First Classifier /ResamplerSecondClassifierTraining Set ResampledTraining SetprobabiltiesFigure 1: A two-step classification architecture for ex-tractive meeting summarization.6 ResamplingAs explained in previous section our model obtainsresampled data for second stage classification usingthe probabilistic outcomes of a first stage classifier.The resampling is done similar to (Xie et al, 2008)to cater to the data skewness problem.
To do theresampling, firstly, the DAs are ranked on decreasingprobabilities.
In the next step, depending on someresampling rate, a percentage of highest ranked DAsis used in further classification steps, while rest ofDA segments are neglected.
(Xie et al, 2008) obtained the resampled set byranking the DAs on TFIDF weights.
Data resam-pling benefits the model in two ways a) by improv-ing the positive/negative example ratio during thetraining phase b) by discarding noisy utterances inthe test phase as they usually attain low scores fromthe first classifier.In testing, the first classifier is run on the test data,its output is used, as in training, to create the resam-pled test set and the probability features.
Finally,the summary is created from the probabilities pro-duced by the second classifier by selecting the high-est ranked DA segments for the specified summarylength.As the data for resampling is derived by alearning-based classifier, we call our approachLearning-Based Sampling (LBS).In this paper, we compare our LBS model withthe TFIDF sampling approach adopted by (Xie etal., 2008) and present the results of resampling onboth models in section 8.2.For comparison, we use Murray?s (2008) state ofart extractive summarization model.367 Evaluation MetricThe main metric we use for evaluating the sum-maries is the extension of the weighted precisionevaluation scheme introduced by (Murray, 2008).The measure relies on having multiple annotationsfor a meeting and a many-to-many mapping dis-cussed in section 2.
To calculate weighted precision,the number of times that each extractive summaryDA was linked by each annotator is counted and av-eraged to get a single DA score.
The DA scores arethen averaged over all DAs in the summary to getthe weighted precision score for the entire summary.The total number of links in an extractive summarydivided by the total number of links to the abstract asa whole gives the weighted recall score.
By this def-inition, weighted recall can have a maximum scoreof 1 since it is a fraction of the total links for the en-tire summary.
Also, there is no theoretical maximumfor weighted precision as annotators were allowed tocreate any number of links for a single DA.Both weighted precision and recall share the samenumerator: num = ?d Ld/N where Ld is the num-ber of links for a DA d in the extractive summary,and N is the number of annotators.
Weighted pre-cision is equal to wp = num/Ds where Ds is thenumber of DAs in the extractive summary.
Weightedrecall is given by recall = num/(Lt/N) where Ltis the total number of links made between DAs andabstract sentences by all annotators, and N is thenumber of annotators.
The f-score is calculated as:(2?
wp?
recall)/(wp + recall).In simple terms a DA which might be discussingan important meeting topic e.g.
selling price of theremote control etc is more likely to be linked bymore than one annotator and possibly more thanonce by an annotator.
Therefore the high scoringDAs are in a way indicative of quintessential topicsand agenda points of the meeting.
Hence, weightedprecision which is number of links per annotatoraveraged over all the meeting DAs is a figure thataligns itself with average information content perDA in the summary.
Low scoring meeting chit-chatswill tend to bring the precision score down.
We re-port a weighted precision of 1.33 for 700 word sum-mary extracted using the procedure described in 2for obtaining gold standard.
This is hence a ceil-ing to the weighted precision score that can be ob-tained by any summary corresponding to this com-pression rate.
Weighted Recall on the other handsignifies total information content of the meeting.For intelligent systems in general the recall rate in-creases with increasing summary compression rateswhile weighted precision decreases2.Since we experiment with short summaries thathave at most 700 words, we do most of the com-parisons in terms of weighted precision values.
Inthe final system evaluation in section 8.3, we includeweighted recall and f-score values.8 Experimental Results and Discussion8.1 Training on gold standardFigure 2 shows the weighted precision results ontraining an SVM classifier with different gold stan-dard thresholds.
For example, at a threshold of 60%,the top 60% of the linked DA segments are definedas the gold standard positive examples, all other DAsegments of the meeting are defined as negative,non-extraction worthy.
The tests are performed ona single stage classifier similar to (Murray, 2008).In addition, the curves show the behavior of thesystem at three different summary compression rates(i.e., number of words in the summary).
A gen-eral tendency that can be observed is the increasein summary scores with decreasing threshold.
For700 word summaries the peak weighted precisionscore is observed at 35% threshold.
The recall rateremains constant as seen by comparing the first tworows of table 5.We believe that low inter annotator agreement isthe major factor responsible for these results.
Thisshows that a reduced subset classification approachwill generally improve results when multiple anno-tations are available.8.2 ResamplingIn this section we compare two resampling models.The TFIDF model explained in section 6 selects bestDAs based on their TFIDF scores.
As discussed2An important point to notice is that, a high recall rate doesnot ensure a good content coverage by the summary.
As anexample, the summary might pick up DAs pertaining to only afew very important points discussed during the meeting whichwill lead to a high recall rate although lesser important conceptsmay still be exclusive.37Figure 2: SVM at different compression rates.previously all sentences above a resampling thresh-old are preserved while rest are discarded.
In 8.2.2resampling is done from the probabilities of a firststage classifier.
SVM model is used for both firstand second stage classification.8.2.1 TFIDF ResamplingTable 3 reports weighted precision and f-scores attwo compression rates.
The highest f-scores for 700,1000 word summaries are obtained at 85% and 55%respectively.
Plots of figure 3 compare weightedprecision scores for LBS and TFIDF models.# words: 700 1000resampl.
% wp f-score wp f-score15 .631 .217 .600 .27425 .670 .227 .610 .28235 .673 .227 .630 .29655 .685 .231 .641 .30575 .689 .232 .632 .30285 .692 .233 .631 .299100 .686 .231 .637 .302Table 3: TFIDF weighted Precision, f-score for 700 and1000 word summaries8.2.2 LBSThe peak performance of the LBS model is ob-served at resampling rate of 35% for both 700 and1000 word summaries as seen in table 4.
The maxi-mum f-scores, 0.248 and 0.319 (table 4) obtained forLBS outperforms maximum f-scores of 0.233 and0.305 (table 3) for TFIDF.# words: 700 1000resampl.
% wp f-score wp f-score15 .684 .236 .662 .30925 .706 .244 .664 .31735 .710 .248 .664 .31955 .707 .245 .652 .31375 .702 .239 .650 .31085 .702 .239 .642 .307100 .692 .236 .639 .306Table 4: weighted precision, f-scores on LBS modelFigure 3: LBS and TFIDF wp values at different com-pression rates.From figure 4 which shows positive example re-tention against sampling rate for TFIDF and LBS itis clear that for all sampling rates, LBS provides ahigher rate of positive examples.Also as discussed above, using a learning-basedfirst classifier produces probability values that canbe leveraged as features for the second classifier.
Wespeculate that this also contributes to the differencesin overall performance.8.3 Overall System PerformanceIn this section we report weighted precision, recalland f-score for 700-word summaries, comparing re-sults of the new model with the initial baseline sys-tem.As shown in table 5, training the system on38Figure 4: LBS and TFIDF retention rates.WRGS, with a threshold of 35% increases the pre-cision score from 0.61 to 0.64 while maintaining therecall rate.
This is corresponding to the weightedprecision score for 35% data point in figure 2.The last row in table 5 correspond to results ob-tained with using the LBS proposed in this paper.The scores at 35% resampling are same as the boldfaced observations in table 4 for 700 word sum-maries.
We observe that the LBS architecture alonebrings about an absolute improvement of 4.41% and8.69% in weighted precision and f-score.System wp recall f-scorebaseline 0.61 0.13 0.20+ gold standard 0.64 0.13 0.20+ new features 0.68 0.15 0.23+ resampling(LBS 35)% 0.71 0.16 0.25Table 5: Results on the AMI corpus.9 Conclusions and Future WorkThrough our experimental results in this pa-per, we firstly observed that training the classifieron WRGS (weighted-resampled gold standard) in-stances, rather than all the annotated DAs improvedthe weighted precision scores of our summarizer.We further addressed the problem of skewed classdistribution in our data set and introduced a learning-based resampling approach where we resample fromthe probabilistic outcomes of a first stage classifier.We noted that resampling the data set increased per-formance, peaking at around 35% sampling rate.
Wecompared the LBS model with the TFIDF resamplerobtaining better f-scores from our proposed machinelearning based architecture.
We conclude in generalthat resampling techniques for resolving data imbal-ance problem in extractive meeting summarizationdomain, results in enhanced system performance.We are currently working on multiple extensionsof this work, including investigating how the resultscan be applied to other corpora, adding additionalfeatures, and finally methods for post-processing ex-tractive summaries.Acknowledgments This work is supported by the Eu-ropean IST Programme Project AMIDA [FP6-0033812].This paper only reflects the authors views and fundingagencies are not liable for any use that may be made ofthe information contained herein.ReferencesJacob Cohen.
1960.
A coefficient of agreement for nom-inal scales.
In Educational and Psychological Mea-surement.Peter Corbett and Ann Copestake.
2008.
Cascadedclassifiers for confidence-based chemical named entityrecognition.
In Current Trends in Biomedical NaturalLanguage Processing.H.
P. Edmundson.
1969.
New methods in automatic ex-tracting.
In J. ACM, 16(2).Erhard W. Hinrichs, Katja Filippova, and HolgerWunsch.2005.
A data-driven approach to pronominal anaphoraresolution for german.
In In Proceedings of RecentAdvances in Natural Language Processing.Gabriel Murray.
2008.
Using Speech-Specific Charac-teristics for Automatic Speech Summarization.
Ph.D.thesis, University of Edinburgh.Shasha Xie, Yang Liu, and Hui Lin.
2008.
Evaluatingthe effectiveness of features and sampling in extractivemeeting summarization.
In IEEE Spoken LanguageTechnology Workshop (SLT), pages 157?160.Xiaodan Zhu and Gerald Penn.
2006.
Summarizationof spontaneous conversations.
In Proceedings of the2006 ACM Conference on Computer Supported Coop-erative Work (CSCW 2006),.39
