Proceedings of SIGDIAL 2009: the 10th Annual Meeting of the Special Interest Group in Discourse and Dialogue, pages 21?28,Queen Mary University of London, September 2009. c?2009 Association for Computational LinguisticsAre You Being Addressed?
- real-time addressee detection to supportremote participants in hybrid meetingsHarm op den AkkerRoessingh Research and DevelopmentEnschedethe Netherlandsh.opdenakker@rrd.nlRieks op den AkkerHuman Media Interaction TwenteEnschedethe Netherlandsinfrieks@cs.utwente.nlAbstractIn this paper, we describe the developmentof a meeting assistant agent that helpsremote meeting participants by notifyingthem when they are being addressed.
Wepresent experiments that have been con-ducted to develop machine classifiers todecide whether ?you are being addressed?where ?you?
refers to a fixed (remote) par-ticipant in a meeting.
The experimental re-sults back up the choices made regardingthe selection of data, features, and classifi-cation methods.
We discuss variations ofthe addressee classification problem thathave been considered in the literature andhow suitable they are for addressee detec-tion in a system that plays a role in a livemeeting.1 IntroductionIn order to understand what is going on in a meet-ing, it is important to know who is talking, whatis being said, and who is being addressed (talkedto).
Here, we focus on the question of whom thespeech is addressed to.
We present results ob-tained in developing a classifier for real-time ad-dressee prediction to be used in an assistant for aremote participant in a hybrid meeting, a meetingwhere a number of participants share a commonmeeting room and one or more others take part viateleconferencing software.It is obvious that in order to effectively par-ticipate in a meeting, participants need to knowwho is being addressed at all times.
For remoteparticipants in hybrid meetings, understanding thecourse of the conversation can be difficult due tothe fact that it is hard to figure out who is beingaddressed.
But it is not only meeting participantswho are interested in addressees.
The questionwho is being addressed has long been of interestfor science: group therapists (Bales, 1950), smallgroup research, or outside observers who analyserecorded meetings.How speakers address listeners, what kind ofprocedures speakers use to designate their audi-ence and to make clear whom they address hasbeen the focus of conversational analysis, socio-linguistics and ethnomethodology for quite sometime.
An analysis of addressee selection is pre-sented in (Lerner, 1996).
Addressing as a specialtype of multi-modal interactional referring expres-sion generation behavior is considered in (op denAkker and Theune, 2008).The problem of automatic addressee detectionis one of the problems that come up when technol-ogy makes the move from two-party man-machinenatural dialogue systems to systems for multi-party conversations.
In this context the addressingproblem was raised by Traum (2004).Since Jovanovic?
(2004), presented her researchon addressee prediction in meetings at SigDial,quite a few publications on the topic appeared.
Jo-vanovic?
used a number of multi-modal meetingcorpora developed in the European projects M4and AMI.
In (Jovanovic?
et al, 2006b) the firstmulti-modal multi-party corpus containing handlabeled addressee annotations was presented.
Thepublic release of the multi-modal AMI meetingcorpus (Carletta, 2007; McCowan et al, 2005), a100 hour annotated corpus of small group meet-ings has already shown to be an important achieve-ment for research; not only for conversationalspeech recognition and tracking of visual elements21but also for automatic multi-modal conversationalscene analysis.
The M4 and AMI corpora are theonly multi-modal meeting corpora (partly) anno-tated with addressee labels.
Addressee detectionin robot-human interaction is studied in (Katzen-maier et al, 2004) and in multi-party dialoguesystems in (Knott and Vlugter, 2008; van Turn-hout et al, 2005; Bakx et al, 2003; Rickel et al,2002).
Addressing in face-to-face conversations isachieved by multi-modal behavior and addresseedetection is thus a multi-modal recognition task.This task requires not only speech recognition butalso gaze and gesture recognition, the recognitionof deictic references, and, ideally, the understand-ing of the ?what?s going on?
in the meeting.
Itrequires the detection of who is involved in cur-rent (parallel) activities.
Speakers show explicitaddressing behavior when they are not confidentthat the participants they want to address are pay-ing attention to their words.
Analysis of the re-mote meetings recorded in the EC project AMIDAreinforces our experiences that this happens morein remote meetings than in small group face-to-face meetings.In AMIDA, the European follow-up project ofAMI, the two new research goals are: (1) real-timeprocessing (real-time speech recognition (Hainet al, 2008), focus of attention recognition (Baand Odobez, 2009), real-time dialogue act label-ing (Germesin et al, 2008) and addressee detec-tion); and (2) technology for (remote) meetingsupport.
Technology based on the analysis ofhow people behave and converse in meetings isnow going to re-shape the meetings, and hopefullymake them more effective and more engaging.
So-cial interaction graphs that show who is talking towhom and how frequently in a meeting may helpthe group by mirroring its interpersonal relations,dominance, and group dynamics, and understandsocial mechanisms as possible causes of ineffec-tiveness.
Although, feedback about the social in-teractions may also be useful during meetings, itdoesn?t require the prediction of the speaker?s ad-dressees in real-time.
A participant in a meeting,however, needs to know who is being addressed bythe speaker at ?the time of speaking?.
This holdsfor humans as well as for an artificial partner, arobot or a virtual Embodied Conversational Agentin a multi-party conversation.The problem of addressee prediction comes indifferent flavors, depending on the relations thatthe subject who is in need of an answer, has withthe event itself.
Time is one of the aspects that playa role here: whether the subject needs to knowthe addressee of an utterance in real-time or off-line.
But it is not only time that plays a role.
Theaddressing problem is an interactional problem,meaning that it is determined by the role that thesubject has in the interaction itself; if and how thespeaker and others communicate with each otherand with the subject.
Is he himself a possibleaddressee of the speaker or is he an outside ob-server?
What type of communication channelsare available to the subject and which channels ofcommunication are available to the conversationalpartners in the meeting?
It is often harder to fol-low a face-to-face discussion on the radio than tofollow a radio broadcasted multi-party discussionthat was held via a point-to-point telephone con-nection.What speakers do to make clear whom they areaddressing depends on the status and capacities ofthe communication lines with their interlocutors.Discussion leaders in TV shows are aware of theirTV audience.
Every now and then, they explicitlyaddress their virtual audience at home.
They alsodesign their questions so as to make clear to theTV viewer whom their questions are addressed to.Outside observers in the form of a video camerawill, however, not affect the way speakers makeclear whom they address as long as the camerais not considered as a participant interested in thespeaker?s intention.
Because remote participantsare often out of sight, speakers in the meetingroom do not take them into account when theyconverse to others in the meeting room.
Remoteparticipants become a kind of outside observersand share the same problems that annotators havewhen they watch video recordings of meetings tosee what is happening in the meeting and who isbeing addressed by the speaker.In section 2 we will specify the particular typeof addressing problem that we are trying to tacklehere.
We make clear how our problem and ap-proach differ from those of other researchers andwhat this means for the applicability of previousresults and available data.
In section 3 we presentthe data we used for testing and training.
We seta baseline for the performance of our classifiers as22well as a hypothesized maximum value, or ceiling,based on the complexity of the task at hand.
Insection 4 we discuss the experiments, for selectingthe optimal features, classifiers, and parameters.In section 5 we present the experimental results.In section 6 we discuss how the currently imple-mented addressing module works in the meetingassistant and what is required to use all the featuresof the addressee predictor in a hybrid meeting.2 The Addressing Problem ConsideredHereJovanovic?
et al (2004) and Jovanovic?
et al(2006a) describe the classifiers that have beentrained and tested on the M4 and AMI corpora.The classification problem is to assign an ad-dressee label to a dialogue act, a hand-labeled andhand-segmented sequence of words, which is ob-tained by manual transcription of a speaker?s utter-ance.
The output of the classifier is one of a set ofpossible addressee labels: Group, or P0,P1,P2,P3,which are the four fixed positions around the ta-ble of the four participants in the meeting.
Sincethe AMI data contains several meetings of differ-ent groups of four people, the class value cannot bethe name of a participant, as that is not an invari-ant of the meeting setting.
Positions at the rect-angular table are invariant.
This implies that theclassifiers can only be used for meetings with thissetting and four participants.
A comparison of thestatistical classifier of Jovanovic?
with a rule-basedmethod using the same part of the AMI corpus ispresented in (op den Akker and Traum, 2009).
Thesame data is also used by Gupta et al (2007) intheir study of a related problem: finding the personthe speaker refers to when he uses a second personpronoun (e.g.
?you?
or ?your?)
as a deictic referringexpression.
Their class values are not positions atthe table but ?virtual positions?
in the speaking or-der (e.g.
next speaker, previous speaker), a solu-tion that generalises to a broader class of conversa-tions than four participants in a face-to-face meet-ing.
In a more recent study, Frampton et al (2009)use positions at the table relative to the positionof the speaker as class values: L1, L2, L3.
Thereason for this is to alleviate the problem of classimbalance in the corpus.We will also use the AMI corpus but we willlook at a different variant of the addressing prob-lem.
This is motivated by our application: to sup-port a remote participant in a hybrid meeting.
Thequestion that we will try to answer is ?are youbeing addressed?
?, where ?you?
refers to an in-dividual participant in a conversation.
The possi-ble answers we consider are ?yes?
or ?no?1.
Theaddressing classifier that solves this problem isthus dedicated to a personal buddy.
Note that thismakes the method useable for any type of conver-sational setting.
Note also that the addressing pre-diction problem ?are you being addressed??
fora meeting assistant who is not himself participat-ing in the meeting is different from the problem?am I being addressed??
that a participant himselfmay have to solve.
The meeting assistant does nothave direct ?internal?
knowledge about the pro-cesses or attentiveness of his buddy participant; hehas to rely on outside observations.
Our view onthe problem implies that we have to take anotherlook at the AMI data and that we will analyse anduse it in a different way for training, testing andperformance measuring.
It also implies that wecannot rely for our binary classification problemon the results of Jovanovic?
(2007) with (dynamic)Bayesian networks.3 The Data and How Complex Our TaskIsWe use a subset of the AMI corpus, containingthose fourteen meetings that have not only beenannotated with dialogue acts, but where dialogueacts are also attributed an addressee label, tellingif the speaker addresses the Group, or the personsitting at position P0,P1,P2 or P32.
They have alsobeen annotated with visual focus of attention: atany time it is known for each partner where he islooking and during what time frame.
Annotatedgaze targets are persons in the meeting, white-board, laptop, table or some other object.Another level of annotations that we use con-cerns the topic being discussed during a topic seg-ment of the meeting.
Participants in the AMI cor-pus play a role following a scenario, the group hasto design a remote TV control and team memberseach have one of four roles in the design project:PM - project manager; UI - user interface de-signer; ID - industrial designer; or ME - market-ing expert.
For details on the meeting scenario see1A ?yes?
means that the dialogue act is addressed to ?you?only.
Group-addressed dialogue acts are considered to be?no?
(not addressed to you only).2Annotators could also use label Unknown in case theycould not decide the addressee of the speaker, this is treatedas Group-addressed or ?no?.23(Post et al, 2004).
In training and testing the clas-sifiers we alternately take up the position in themeeting of one of the participants, who is treatedas the target for addressee prediction.3.1 Base-line and Ceiling-valueBecause most of the dialogue acts are not specif-ically addressed to one and the same meetingparticipant, the baseline for the binary classifica-tion task is already quite high: 89.20%, beingthe percentage of all dialogue acts annotated withaddressing information ?not addressed to You?,which is 5962 out of a total of 6648 dialogue acts.The performance of a supervised machinelearning method depends on (1) the selection offeatures (2) the type of classifier including thesettings of the hyper-parameters of the classi-fiers (Daelemans et al, 2003), and (3) the qualityand the amount of training data (Reidsma, 2008;Reidsma and Carletta, 2008).
Since we measurethe classifier?s performance with a part of the an-notated data it is interesting to see how human an-notators (or, ?human classifiers?)
perform on thistask.One of the AMI meetings3 has been annotatedwith addressing information by four different an-notators.
We will use this to measure how am-biguous the task of addressee labeling is.
Table1 shows the confusion matrix for two annotators:s95 and vka.
This shows the (dis-)agreements forlabelling the 412 dialogue acts as addressed to A,B, C, D or to the Group.
4 However, because weuse our data differently, we will look at the con-fusion matrices in a different way.
We split it upinto 4 matrices, each from the view of one of thefour meeting participants.
Table 2 is an example ofthis, taking the view of participant A (i.e.
for thebinary decision task ?is Participant A being ad-dressed?
?, and having annotator s95 as gold stan-dard.Table 2 shows that when taking annotator s95as gold standard, and considering annotator vkaas the classifier, he achieves an accuracy of 92.23(380 out of 412 instances classified correctly).3IS1003d4Note that the annotators first independently segmentedthe speaker?s turns into dialogue act segments; then labeledthem with a dialogue act type label and then labeled the dia-logue acts with an addressee label.
The 412 dialogues acts arethose segments that both annotators identified as a dialogueact segment.A B C D Group TotalA 29 10 39B 14 8 22C 32 7 39D 1 1 49 18 69Group 21 10 19 22 171 243Total 51 24 52 71 214 412Table 1: Confusion matrix for one pair of annota-tors ( = 0.55).A ?A TotalA 29 10 39?A 22 351 373Total 51 361 412Table 2: Confusion matrix for one pair of anno-tators, considering addressed to A or not (derivedfrom the matrix in Table 1).We can argue that we can use these human an-notators/classifiers scores as a measure of ?max-imum performance?, because it indicates a levelof task ambiguity.
Classifiers can achieve higherscores, because they can learn through noise in thedata.
Thus, the inter-annotator confusion value isnot an absolute limit of actual performance, butcases in which the classifier is ?right?
and the test-set ?wrong?
would not be reflected in the results.Since the inter-annotator confusion does also saysomething about the inherent task ambiguity, itcan be used as a measure to compare a classifierscore with.
Table 3 contains the overall scores(taken over all 4 individual participants) for the6 annotator pairs.
The average values for Recall,Precision, F-Measure and Accuracy in Table 3 areconsidered as ceiling values for the performancemeasures for this binary classification task5.
TheHypothesized Maximum Score (HMS) is the aver-age accuracy value: 92.47.Pair Rec Prec F Accs-v 73.37 62.63 67.58 92.78m-s 59.75 70.59 64.72 91.87m-v 69.92 74.78 72.27 93.11m-d 37.77 81.61 51.64 91.79v-d 42.04 80.49 55.23 92.22s-d 43.68 77.55 55.88 93.02Average: 54.42 74.61 61.22 92.47Table 3: Recall, Precision, F-measure and Accu-racy values for the 6 pairs of annotators.5Inter-changing the roles of the two annotators, i.e.
con-sider vka as ?gold standard?
in Table 2, means inter-changingthe Recall and Precision values.
The F-value remains thesame, though.24The baseline (89.20 for all dialogue acts anno-tated with addressing) and the HMS (92.47) accu-racy values will be used for comparison with theperformance of our classifiers.4 The Methods and Their FeaturesIn the experiments, four different classifiers werecreated:1.
Lexical and Context Classifier2.
Visual Focus of Attention Classifier3.
Combined Classifier4.
Topic and Role Extended ClassifierFor each of these classifiers a large number ofexperiments were performed with a varying num-ber of 15 to 30 different machine learning meth-ods -using Weka (Witten and Frank, 1999)- to se-lect optimal feature sets.
In this section we sum-marize the most important findings.
For a moredetailed analysis refer to (op den Akker, 2009).Because of the large number of features and clas-sifiers used, the various classifier hyper parame-ters have largely been kept to their default val-ues.
Where it was deemed critical (Neural Net-work training epochs and number of trees in Ran-domForest classifier) these parameters were variedafterwards to make sure that the performance didnot deviate too much from using the default val-ues.
It didn?t.4.1 Lexical and Context ClassifierThe lexical and context based classifier uses fea-tures that can be derived from words and dialogueacts only.
A total of 14 features were defined,7 of which say something about the dialogue act(type, number of words, contains 1st person sin-gular personal pronoun, and so on) and 7 of whichsay something about the context of the dialogueact (how often was I addressed in the previous 6 di-alogue acts, how often did I speak in the previous5 dialogue acts, and so on).
Of these 14 features,the optimal feature subset was selected by tryingout all the subsets.
This was repeated using 15different classifiers from the WEKA toolkit.
Thebest result was achieved with a subset of 10 fea-tures, by the MultiLayerPerceptron classifier.
Inthis way an accuracy of 90.93 was reached.
Giventhe baseline of the used train and test set of 89.20and the HMS of 92.47, this can be seen as 53% ofwhat ?can?
be achieved.4.2 Visual Focus of Attention ClassifierThe VFOA classifier uses features derived from ameeting participant?s visual focus of attention.
Atotal of 8 features were defined, such as: the totaltime that the speaker looks at me, the total timeeveryone is looking at me, and so on.
The optimaltime interval in which to measure who is lookingat you was extensively researched by trying outdifferent intervals around the start of a dialogueact, and training and testing a classifier on the fea-ture.
These optimal interval values differ for everyfeature, but is usually somewhere between a fewseconds before the start of the dialogue act, to 1second into the dialogue act.
The difference in per-formance for using the optimal interval comparedto using the start- and end times of the dialogueact is sometimes as much as 0.93 accuracy (whichis a lot given a base score of 89.20 and HMS of92.47).
This shows, that when looking at VFOAinformation, one should take into account the par-ticipant?s gaze before the dialogue act, instead oflooking at the utterance duration as in (Jovanovic?,2007; Frampton et al, 2009)6.
The representationof feature values was also varied by either nor-malizing to the duration of the window or usingthe raw values.
Again the optimal feature subsetwas calculated using brute-force.
Because of thereduced time complexity for 28 possible featuresubsets, 30 different classifiers from the WEKAtoolkit were trained and tested.
One of the best re-sults was achieved with a feature set of 4 featuresagain with the MultiLayerPerceptron: 90.80 accu-racy.
The train and test sets used for this classifierare slightly smaller than those used for the Lex-Cont classifier because not all dialogue acts areannotated with VFOA.
The base score for the datahere is 89.24, and given the HMS of 92.47, this re-sult can be seen as 48% of what can be achieved.4.3 Combined ClassifierThe third classifier is a combination of the firsttwo.
We tried three different methods of combin-ing the results of the LexCont and VFOA classi-fiers.
First we tried to train a classifier using allthe features (14 lexical, 8 vfoa) which explodedthe feature subset search space to over 4 millionpossibilities.
A second approach was to combinethe output of the LexCont and VFOA classifiersusing a simple rule-based approach.
The OR-rule6Note that a dialogue act segment can be preceded by another utterance unit of the same speaker.25(if either of the two classifiers thinks the DA is ad-dressed to you, the outcome is ?yes?)
performedthe best (91.19% accuracy).
But the best resultswere achieved by training a rule based (Ridor)classifier on the output of the first two.
For theseexperiments the test-set of the previous two clas-sifiers was split again into a new train (3080 in-stances) and test set (1540 instances).
The featuresare the outputs of the VFOA and LexCont classi-fiers (both class and class-probabilities).
For thistask, 35 classifiers have been trained with the bestresults coming from the Ridor classifier: 92.53 ac-curacy.
The results of all the different techniquesfor combining the classifiers can be seen in Table4.
The baseline score for this smaller test set is89.87, so given the HMS of 92.47, this result canbe seen as 102% of what can be achieved.
Notethat this is not ?impossible?, because the Hypoth-esized Maximum Score is merely an indication ofhow humans perform on the task, not an absoluteceiling.4.4 Topic and Role Extended ClassifierAs a final attempt to improve the results we usedtopic and role information as features to our com-bined classifier.
In the AMI corpus, every meet-ing participant has a certain role (project manager,interface designer, etc.
.
. )
and the meetings weresegmented into broad topic (opening, discussion,industrial designer presentation).
Now the idea isthat participants with certain roles are more likelyto be addressed during certain topics.
As an illus-tration of how much these a-priori chances of be-ing addressed can change, take the example of anindustrial designer during an ?industrial designerpresentation?.
The a-priori probability of you be-ing addressed as industrial designer in the entirecorpus is 13%.
This probability, given also thefact that the current topic is ?industrial designerpresentation?
becomes 46%.
This is a huge differ-ence, and this information can be exploited.
For allcombinations of topic and role, the a-priori prob-ability of you being addressed as having that roleand during that topic, have been calculated.
Thesevalues have been added as features to the featuresused in the Combined Classifier, and the experi-ments have been repeated.
This time, the best per-forming classifier is Logistic Model Trees with anaccuracy of 92.99%.
Given the baseline of 89.87and HMS of 92.47, this can be seen as 120% ofwhat ?can?
be achieved, which is better by a fairlylarge margin than the results of the inter-annotatoragreement values.5 Summary of ResultsTable 4 summarizes the results for the variousclassifiers.
The LexCont and VFOA classifiers in-dividually achieve only about 50% of what canbe achieved, but if combined in a clever way,their performance seems to reach the limit of whatis possible based on the comparison with inter-annotator agreement.
The fact that the topic-roleextended classifier achieves so much more than100% can be ascribed to the fact that it is cheating.It uses pre-calculated a-priori chances of ?you?being addressed given the circumstances.
Thisknowledge could be calculated by the machinelearner by feeding it the topic and role features,and letting it learn these a-priori probabilities foritself.
But the classifier that uses these types offeatures can not easily be deployed in any differ-ent setting, where participants have different rolesand where different topics are being discussed.Method Acc Rec Prec F PoMHMS 92.47 54.42 74.61 61.22 -LexCont 90.93 33.10 66.02 44.09 53VFoA 90.80 27.77 67.65 39.38 48CombinedFeat 91.56 36.62 70.82 48.28 72ClassOfResults 43.68 77.55 55.88 93.02 102LogComb(AND) 90.24 9.86 94.23 17.85 31LogComb(OR) 91.19 47.08 61.90 53.48 60TopicRoleExt 92.99 41.03 80.00 54.24 120Table 4: Performance values of the Methods dis-cussed in this paper: Accuracy, Recall, Precision,F-measure and Percentage of Hypothezised Maxi-mum Score (PoM).6 How Does The Assistant Work?At the time of writing, the assistant that has beenimplemented is based on the simple visual focusof attention classifier.
The focus of attention isinferred from the head pose and head movementsof a participant in the meeting room who is beingobserved by a close-up camera.
The real-time fo-cus of attention module sends the coordinates ofthe head pose to a central database 15 times persecond (Ba and Odobez, 2009).
The coordinatesare translated into targets: objects and personsin the meeting room.
For the addressing modulemost important are the persons and in particularthe screen in the meeting room where the remote26participant is visible.
The addressing module isnotified of updates of who is speaking and decideswhether the remote participant is being looked atby the speaker.If the remote participant (RP) is not attentive(which can be detected automatically based on hisrecent activity) he is called when he is addressedor when the real-time keyword spotter has de-tected a word or phrase that occurs on the list oftopics of interest to the RP.
For a detailed descrip-tion of the remote meeting assistant demonstratordeveloped in the AMIDA project refer to (op denAkker et al, 2009).The meeting assistant allows the RP to dis-tribute his attention over various tasks.
The systemcan give a transcript of the fragment of the meet-ing that is of interest to the RP, so he can catchup with the meeting if he was not following.
Thesimple focus of attention based addressing moduleworks fine.
The question is now if an addressingmodule that uses the output of the real-time dia-logue act recognizer, which in turn uses the out-put of the real-time speech recognizer will outper-form the visual focus of attention based addresseedetector.
Experiments make us rather pessimisticabout this: the performance drop of state of the artreal-time dialogue segmentation and labeling tech-nology based on real-time ASR output is too largein comparison with those based on hand-annotatedtranscripts (Jovanovic?, 2007).
For real-time au-tomatic addressee detection more superficial fea-tures need to be used, such as: speech/non-speech,who is speaking, some prosodic information andvisual focus of attention, by means of head orien-tation.The most explicit way of addressing is by usinga vocative, the proper name of the addressed per-son.
In small group face-to-face meetings, wherepeople constantly pay attention and keep track ofothers?
attentiveness to what is being said anddone, this method of addressing hardly ever oc-curs.
In remote meetings where it is often not clearto the speaker if others are paying attention, peoplecall other?s names when they are addressing them.Other properties of the participant relevant for ad-dressee detection include his role and his topicsof interest.
These can either be obtained directlyfrom the participant when he subscribes for themeeting, or they can be recognized during an in-troduction round that most business meetings startwith.
For automatic topic detection further anal-ysis of the meeting will be needed (Purver et al,2007).
Probability tables for the conditional prob-abilities of the chance that someone with a givenrole is being addressed when the talk is about agiven topic, can be obtained from previous data,and could be updated on the fly during the meet-ing.
Only when that has been achieved will itbe possible for our extended topic/role addresseeclassifier to be fully exploited by a live meetingassistant.AcknowledgementsThe research of the first author was performedwhen he was a Master?s student at the Human Me-dia Interaction group of the University of Twente.This work is supported by the European IST Pro-gramme Project FP6-0033812 (AMIDA).
We aregratefull to the reviewers of SigDial 2009 for theirencouraging comments, and to Lynn Packwoodfor correcting our English.ReferencesSileye Ba and Jean-Marc Odobez.
2009.
Recognizinghuman visual focus of attention from head pose inmeetings.
In IEEE Transaction on Systems, Man,and Cybernetics, Part B (Trans.
SMC-B), volume 39,pages 16?33.I.
Bakx, K. van Turnhout, and J. Terken.
2003.
Facialorientation during multi-party interaction with infor-mation kiosks.
In Proceedings of 9th IFIP TC13 In-ternational Conference on Human-Computer Inter-action (INTERACT), Zurich, Switzerland.Robert Freed Bales.
1950.
Interaction Process Analy-sis; A Method for the Study of Small Groups.
Addi-son Wesley, Reading, Mass.Jean C. Carletta.
2007.
Unleashing the killer corpus:experiences in creating the multi-everything AMImeeting corpus.
Language Resources and Evalua-tion, 41(2):181?190, May.Walter Daelemans, Ve?ronique Hoste, Fien De Meul-der, and Bart Naudts.
2003.
Combined opti-mization of feature selection and algorithm param-eter interaction in machine learning of language.In Proceedings of the 14th European Conferenceon Machine Learning (ECML-2003), Lecture Notesin Computer Science 2837, pages 84?95, Cavtat-Dubrovnik, Croatia.
Springer-Verlag.Matthew Frampton, Raquel Fernndez, Patrick Ehlen,Mario Christoudias, Trevor Darrell, and Stanley Pe-ters.
2009. Who is you?
combining linguistic andgaze features to resolve second-person references in27dialogue.
In Proceedings of the 12th Conference ofthe EACL.Sebastian Germesin, Tilman Becker, and Peter Poller.2008.
Determining latency for on-line dialog actclassification.
In Poster Session for the 5th Inter-national Workshop on Machine Learning for Multi-modal Interaction, volume 5237.Surabhi Gupta, John Niekrasz, Matthew Purver, andDaniel Jurafsky.
2007.
Resolving ?you?
in multi-party dialog.
In Proceedings of the 8th SIGdialWorkshop on Discourse and Dialogue, Antwerp,Belgium, September.Thomas Hain, Asmaa El Hannani, Stuart N. Wrigley,and Vincent Wan.
2008.
Automatic speech recogni-tion for scientific purposes - webasr.
In Proceedingsof the international conference on spoken languageprocessing (Interspeech 2008).Natasa Jovanovic?
and Rieks op den Akker.
2004.
To-wards automatic addressee identification in multi-party dialogues.
In Proceedings of the 5th SIGdialWorkshop on Discourse and Dialogue, pages 89?92, Cambridge, Massachusetts, USA.
Associationfor Computational Linguistics.Natasa Jovanovic?, Rieks op den Akker, and Anton Ni-jholt.
2006a.
Addressee identification in face-to-face meetings.
In Proceedings of 11th Conferenceof the European Chapter of the Association for Com-putational Linguistics (EACL), Trento, Italy.Natasa Jovanovic?, Rieks op den Akker, and Anton Ni-jholt.
2006b.
A corpus for studying addressingbehaviour in multi-party dialogues.
Language Re-sources and Evaluation Journal, 40(1):5?23.Natasa Jovanovic?.
2007.
To whom it may con-cern: adressee identification in face-to-face meet-ings.
Ph.D. thesis, University of Twente.M.
Katzenmaier, R. Stiefelhagen, and T. Schultz.
2004.Identifying the addressee in human-human-robot in-teractions based on head pose and speech.
In Pro-ceedings of International Conference on MultimodalInterfaces (ICMI), pages 144?151, State College,PA.A.
Knott and P. Vlugter.
2008.
Multi-agent human-machine dialogue: issues in dialogue managementand referring expression semantics.
Artificial Intel-ligence, 172:69?102.Gene H. Lerner.
1996.
On the place of linguisticresources in the organization of talk-in interaction:?Second person?
reference in multi-party conversa-tion.
Pragmatics, 6(3):281?294.I.
McCowan, J. Carletta, W. Kraaij, S. Ashby, S. Bour-ban, M. Flynn, M. Guillemot, T. Hain, J. Kadlec,V.
Karaiskos, M. Kronenthal, G. Lathoud, M. Lin-coln, A. Lisowska, W. Post, D. Reidsma, andP.
Wellner.
2005.
The AMI meeting corpus.
InProceedings of the 5th International Conference onMethods and Techniques in Behavioral Research.Rieks op den Akker and Mariet Theune.
2008.
Howdo I address you?
- modelling addressing behaviorbased on an analysis of a multi-modal corpus of con-versational discourse.
In Proceedings of the AISB2008 Symposium on Multimodal Output Generation(MOG 2008), Aberdeen, UK, pages 10?17.Rieks op den Akker and David Traum.
2009.
A com-parison of addressee detection methods for multi-party conversations.
In Proceedings of DiaHolmia,13th Workshop on the Semantics and Pragmatics ofDialogue.Rieks op den Akker, Dennis Hofs, Hendri Hondorp,Harm op den Akker, Job Zwiers, and Anton Nijholt.2009.
Engagement and floor control in hybrid meet-ings.
In Proceedings COST Action Prague 2008 (toappear), LNCS.
Springer Verlag.Harm op den Akker.
2009.
On addressee detectionfor remote hybrid meeting settings.
Master?s thesis,University of Twente.W.M.
Post, A.H. Cremers, and O.B.
Henkemans.
2004.A research environment for meeting behavior.
InA.
Nijholt, T. Nishida, R. Fruchter, and D. Rosen-berg, editors, Social Intelligence Design, Enschede,The Netherlands.Matthew Purver, John Dowding, John Niekrasz,Patrick Ehlen, Sharareh Noorbaloochi, and StanleyPeters.
2007.
Detecting and summarizing actionitems in multi-party dialogue.
In Proceedings of the8th SIGdial Workshop on Discourse and Dialogue,Antwerp, Belgium, September.Dennis Reidsma and Jean C. Carletta.
2008.
Relia-bility measurement without limits.
ComputationalLinguistics, 34(3):319?326, September.Dennis Reidsma.
2008.
Annotations and SubjectiveMachines.
Ph.D. thesis, University of Twente.J.
Rickel, S. Marsella, J. Gratch, R. Hill, D. Traum, andW.
Swartout.
2002.
Towards a new generation ofvirtual humans for interactive experiences.
Intelli-gent Systems, 17:32?36.David Traum.
2004.
Issues in multiparty dialogues.
InAdvances in Agent Communication, pages 201?211.K.
van Turnhout, J. Terken, I. Bakx, and B. Eggen.2005.
Identifying the intended addressee inmixed human-human and human-computer interac-tion from non-verbal features.
In Proceedings of 7thInternational Conference on Multimodal Interfaces(ICMI?05), Trento, Italy.Ian H. Witten and Eibe Frank.
1999.
Data Mining:Practical Machine Learning Tools and Techniqueswith Java Implementations.
Morgan Kaufmann, 1stedition, October.28
