Coling 2010: Poster Volume, pages 972?978,Beijing, August 2010Improving Name Origin Recognition with Context Features andUnlabelled DataVladimir Pervouchine, Min Zhang, Ming Liu and Haizhou LiInstitute for Infocomm Research, A-STARvpervouchine@gmail.com,{mzhang,mliu,hli}@i2r.a-star.edu.sgAbstractWe demonstrate the use of context fea-tures, namely, names of places, and un-labelled data for the detection of per-sonal name language of origin.While some early work used eitherrule-based methods or n-gram statisti-cal models to determine the name lan-guage of origin, we use the discrimi-native classification maximum entropymodel and view the task as a classifica-tion task.
We perform bootstrapping ofthe learning using list of names out ofcontext but with known origin and thenusing expectation-maximisation algo-rithm to further train the model ona large corpus of names of unknownorigin but with context features.
Us-ing a relatively small unlabelled cor-pus we improve the accuracy of nameorigin recognition for names writtenin Chinese from 82.7% to 85.8%, asignificant reduction in the error rate.The improvement in F -score for infre-quent Japanese names is even greater:from 77.4% without context features to82.8% with context features.1 IntroductionTransliteration is a process of rewriting aword from a source language to a target lan-guage in a different writing system using theword?s phonological equivalent.
Many techni-cal terms and proper nouns, such as personalnames, names of places and organisations aretransliterated during translation of a text fromone language to another.
A process reverseto the transliteration, which is recovering aword in its native language from its translit-eration in a foreign language, is called back-transliteration (Knight and Graehl, 1998).
Inmany natural language processing (NLP) taskssuch as machine translation and cross-lingualinformation retrieval, transliteration is an im-portant component.Name origin refers to the language of ori-gin of a name.
For example, the origin of En-glish name ?Smith?
and its Chinese transliter-ation ????
(Shi-Mi-Si)?
is English, whileboth ?Tokyo?
and ???
(Dong-Jing)?
are ofJapanese origin.For machine transliteration the name originsdictate the way we re-write a foreign name.For example, given a name written in Chi-nese for which we do not have a translationin an English-Chinese dictionary, we first haveto decide whether the name is of Chinese,Japanese, Korean, English or another origin.Then we follow the transliteration rules im-plied by the origin of the name.
Althoughall English personal names are rendered in26 letters, they may come from different ro-manization systems.
Each romanisation sys-972tem has its own rewriting rules.
English name?Smith?
could be directly transliterated intoChinese as ????
(Shi-Mi-Si)?
since it fol-lows the English phonetic rules, while the Chi-nese translation of Japanese name ?Koizumi?becomes ???
(Xiao-Quan)?
following theJapanese phonetic rules.
The name originsare equally important in back-transliteration.Li et al (2007b) demonstrated that incorpo-rating name origin recognition (NOR) into atransliteration system greatly improves the per-formance of personal name transliteration.
Be-sides multilingual processing, the name originalso provides useful semantic information (re-gional and language information) for commonNLP tasks, such as co-reference resolution andname entity recognition.Unfortunately, not much attention has beengiven to name origin recognition (NOR) so farin the literature.
In this paper, we are inter-ested in recognition of the origins of nameswritten in Chinese, which names can be ofthree origins: Chinese, Japanese or English,where ?English?
is a rather broad category thatincludes other West European and Americannames written natively in Latin script.Unlike previous work (Qu and Grefenstette,2004; Li et al, 2007a; Li et al, 2007b),where NOR was formulated with a genera-tive model, we follow the approach of Zhanget al (2008) and regard the NOR task as aclassification problem, using a discriminativelearning algorithm for classification.
Further-more, in the training data with names labelledwith their origin is rather limited, whereasthere is vast data from news articles that con-tains many personal names without any labelsof their origins.
In this research we proposea method to harness the power of the unla-belled noisy news data by bootstrapping thelearning process with labelled data and thenusing the personal name context in the unla-belled data to improve the NOR model.
Weachieve that by using the maximum entropymodel and the expectation-maximisation train-ing, and demonstrate that our method can sig-nificantly improve the accuracy of NOR com-pared to the baseline model trained only fromthe labelled data.The rest of the paper is organised as follows:in Section 2 we review the previous research.In Section 3 we present our approach, and inSection 4 we describe our experimental setup,the data used and the evaluation method.
Weconclude in Section 5.2 Related researchMost the research up to date focuses primar-ily on recognition of origin of names writtenin Latin script, called English NOR (ENOR),although the same methods can be extended tonames in Chinese script (CNOR).
We noticethat there are two informative clues that usedin previous work in ENOR.
One is the lexi-cal structure of a romanisation system, for ex-ample, Hanyu Pinyin, Mandarin Wade-Giles,Japanese Hepbrun or Korean Yale, each hasa finite set of syllable inventory (Li et al,2007a).
Another is the phonetic and phono-tactic structure of a language, such as phoneticcomposition, syllable structure.
For example,English has unique consonant clusters suchas ?str?
and ?ks?
which Chinese, Japaneseand Korean (CJK) do not have.
Consider-ing the NOR solutions by the use of thesetwo clues, we can roughly group them intotwo categories: rule-based methods (for solu-tions based on lexical structures) and statisti-cal methods (for solutions based on phonotac-tic structures).Rule-based method Kuo et al (2007) pro-posed using a rule-based method to recog-nise different romanisation system for Chineseonly.
The left-to-right longest match-basedlexical segmentation was used to parse a testword.
The romanisation system is confirmed973if it gives rise to a successful parse of the testword.
This kind of approach (Qu and Grefen-stette, 2004) is suitable for romanisation sys-tems that have a finite set of discriminativesyllable inventory, such as Pinyin for ChineseMandarin.
For the general tasks of identifyingthe language origin and romanisation system,rule based approach sounds less attractive be-cause not all languages have a finite set of dis-criminative syllable inventory.N-gram statistics methodsN-gram sum method Qu and Grefenstette(2004) proposed a NOR identifier us-ing a trigram language model (Cavnarand Trenkle, 1994) to distinguish per-sonal names of three language origins,namely Chinese, Japanese and English.In their work the training set includes11,416 Chinese, 83,295 Japanese and88,000 English name entries.
How-ever, the trigram is defined as the jointprobability p(cici?1ci?2) rather than thecommonly used conditional probabilityp(ci|ci?1ci?2).
Therefore it is basicallya substring unigram probability.
For ori-gin recognition of Japanese names, thismethod works well with an accuracy of92%.
However, for English and Chinese,the results are far behind with a reportedaccuracy of 87% and 70% respectively.N-gram perplexity method Li et al (2007a)proposed a method of NOR using n-gramcharacter perplexity PPc to identify theorigin of names written in Latin script.Using bigrams, the perplexity is definedasPPc = 2?1Nc?Nci=1 log p(ci|ci?1)whereNc is the total number of charactersin a given name, ci is the i-th characterin the name and p(ci|ci?1) is the bigramprobability learned from a list of namesof the same origin.
Therefore, PPc canbe used to measure how well a new namefits the model learned from the trainingset of names.
The origin is assigned ac-cording to the model that gives the lowestperplexity value.
Li et al (2007a) demon-strated that using PPc gives much betterperformance than with the substring uni-gram method.Classification method Zhang et al (2008)proposed using a discriminative classificationapproach and extract features from the names.They use Maximum Entropy (MaxEnt) modeland a number of features based on n-grams,character positions, word length as well assome rule-based phonetic features.
They per-formed both ENOR and CNOR and demon-strated that their method indeed leads to betterperformance in name origin recognition thenthe n-gram statistics method.
They attributethat to the fact their model incorporates morerobust features than the n-gram statistics basedmodels.In this paper we too follow the discriminat-ing classification approach, but we add fea-tures based on the context of a personal name.These features require the original text with thenames to be available.
Our approach closelymodels the real-life situation when large cor-pora of articles with personal names is read-ily available in the Web, yet the origins of thenames are unknown.3 Model and training methods3.1 Maximum entropy model for NORThe principle of maximum entropy is thatgiven a collection of facts we should choosea model that is consistent with all the facts butotherwise as uniform as possible (Berger et al,1996).
maximum entropy model (MaxEnt) isknown to easily combine diverse features and974has been used widely in natural language pro-cessing research.
Given an observation x theprobability of outcome label ci, i = 1 .
.
.
Ngiven x is given byp(ci|x) =1Z exp?
?n?j=1?jfj (x, ci)??
(1)where N is the number of the outcome labels,which is the number of name origins in ourcase, n is the number of features, fj are thefeature functions and ?j are the model param-eters.
Each parameter corresponds to exactlyone feature and can be viewed as a ?weight?for the corresponding feature.
Z is the normal-isation factor given byZ =N?i=1p(ci|x) (2)In the problem at hand x is a personal nameand all the features are binary.
The features,also known as contextual predicates, are in theformfi(x, c) ={1 if c = ci and cp(x) = true0 otherwise(3)where cp is the contextual predicate that mapsa pair (ci, x) to {true, false}.In our experiments we use Zhang?s maxi-mum entropy library1.3.2 Initial training with labelled data andn-gram featuresFor the initial training of MaxEnt model weuse labelled data: personal names of Chinese,Japanese or English origin written in Chinese.The origin of each name is known.
Followingpaper by Zhang et al (2008) and their findings1http://homepages.inf.ed.ac.uk/lzhang10/maxent toolkit.htmlregarding the contribution value of each fea-ture that they studied, we extract unigram, po-sitional unigram and word length features.
Forexample, Chinese name ?????
has the fol-lowing features:???
(?,0) (?,1) (?,2) 3We restrict the n-gram features to unigramonly to avoid the data sparseness, because ourdata contains a number of Chinese surnamesand given names, which have a length of oneor two characters.3.3 Further training with unlabelled dataand context featuresFor further training of MaxEnt model we useunlabelled data collected from news articles.The name origin is not known but each per-sonal name is in a context and is often sur-rounded by names of places that may give ahint about the personal name origin.
For eachpersonal name we extract all names of placesin the same paragraph and use them as fea-tures.
If a place name is repeated many timesin the same paragraph we only include it oncein the feature list.For example, paragraph containing passage?The U.S. President Barack Obama ...?
willresult in two personal names ?Barack?
and?Obama?
having ?U.S.?
as their context fea-ture.
Due to the diversity of place names wealso attempt to map the names of the placesinto the country names.
In this case, featureslike ?U.S.
?, ?USA?, ?America?
are manuallysubstituted with ?USA?.
In our experiments wealso try to narrow the place name extractionto windows of different sizes surrounding thepersonal name.
The rationale here is that thecloser a place name is to the personal name,the more likely it has a connection to the ori-gin of the personal name.In summary, our algorithm includes twosteps.975First, we use the boostrap data and n-gram,positional n-gram and name length features todo the initial training (the 0-th iteration) ofMaxEnt model with L-BFGS method (Byrd etal., 1995).
After that we use the model to as-sign origin labels to names of the training setof the unlabelled data.Next, we use both the bootstrap data andthe training set of the unlabelled data, labelledin the previous step, and add the context fea-tures to the already used n-gram, positional n-gram and name length features.
Since there isno context available for the bootstrap data, thecontext features for it are missing, which canbe handled by the MaxEnt model.
We performthe Expectation-Maximisation (EM) iterationsby using the mixed data to train the i-th itera-tion of the MaxEnt model, then use the modelto re-label the training set of the unlabelleddata and repeat the training of the model forthe (i + 1)-st iteration.
We stop the iterationswhen the ratio of patterns that change the ori-gin labels becomes less than 0.01%.4 Experiments4.1 CorporaThe corpora consists of two datasets.
Onedataset, called the ?bootstrap data?, is a set ofChinese, Japanese and English names writtenin Chinese following the respective translitera-tion rules according to the name origins.
Thenames are a mixture of full names, first (given)names and surnames.
Table 1 shows the num-ber of names of each origin.
This is the la-belled data; the origin of each name is known.The data is used to start the MaxEnt modeltraining.The second dataset, called the ?unlabelleddata?, is Chinese, Japanese and English per-sonal names written in Chinese, which havebeen extracted from the news articles col-lected over 6 months from Xinhua news web-site.
The articles have been processed by anOrigin Number of namesChinese 52,342Japanese 26,171English 26,171Table 1: Number of names of each origin inthe bootstrap dataset.automatic part-of-speech (POS) tagger, afterwhich personal names and names of placeshave been manually identified (the latter forextracting the context features).
Normally thefirst (given) name and surnames are identi-fied as two separate personal names.
The datais split into a training set of 27,882 nameswith unknown origin and a testing set of 1,476names whose origin was manually assigned.We split data in such a way that there is nooverlap between patterns in the training andtesting sets, although there may be overlap be-tween names.
For example, if a name maybe present in both training and testing sets butin a different context, making the two namestwo distinct patterns.
The number of namesof each origin in the testing set is shown inTable 2.
As seen from the table, the numberOrigin Number of namesChinese 738Japanese 369English 422Table 2: Number of names of each origin inthe testing dataset.of Chinese names exceeds the number of En-glish or Japanese names.
This is an expectedconsequence of using articles from a Chinesenews agency because many of the articles arereporting on local affairs.
We have manuallyremoved a number of Chinese name patternsfrom the testing set, since the original percent-age of Chinese names in the articles is about83%.9764.2 Evaluation methodFollowing Zhang et al (2008) to makeour results comparable to theirs, we eval-uate our system using precision Po, recallRo and F -score Fo for each origin o ?
{?Chinese ??
?Japanese ??
?English ??}.
Letthe number of correctly recognised names ofa given origin o be ko, and the total number ofnames recognised as being of origin o be mo,while the actual number of names of origin obe no.
Then the precision, recall and F -scoreare given as:Po =komoRo =konoFo =2?
Po ?RoPo +RoWe also report the overall accuracy of the sys-tem (or, rather the overall recall), which is theratio of the total number of correctly recog-nised names to the number of all names:Acc = kChinese + kJapanese + kEnglishnChinese + nJapanese + nEnglish4.3 ResultsAfter each iteration of our MaxEnt-based EMalgorithm, we record the number of patterns inthe training set that changed their origin labels,as well as calculate the precision, recall andF -score for each origin as well as the overallaccuracy.
The results are reported in Tables 3and 4, where for the sake of brevity the originsubscripts are ?C?, ?J?
and ?W?
for Chinese,Japanese and English name origin respectively.Compared to the 0-th iteration there is ansignificant improvement in accuracy, particu-larly in recognition of Japanese names, whichare relatively infrequent compared to Chineseand English ones in the unlabelled trainingdata.
This clearly shows the effectiveness ofour proposed method.Iteration PC PJ PW RC RJ RW0 0.887 0.724 0.857 0.823 0.911 0.7611 0.914 0.736 0.875 0.823 0.968 0.7752 0.910 0.736 0.874 0.823 0.968 0.7673 0.914 0.737 0.874 0.824 0.973 0.7674 0.913 0.742 0.875 0.825 0.968 0.778Table 3: Results of running EM iterations,original names of the places are kept.Iteration Acc FC FJ FW0 0.829 0.854 0.807 0.8061 0.847 0.866 0.836 0.8222 0.845 0.864 0.836 0.8173 0.847 0.867 0.839 0.8174 0.849 0.867 0.840 0.824Table 4: Results of running EM iterations,original names of the places are kept.5 ConclusionsWe propose extension of MaxEnt model forNOR task by using two types of data for train-ing: origin-labelled names alone and origin-unlabelled names in their context surrounding.We show how to apply a simple EM method tomake use of the contextual words as features,and improve the NOR accuracy from 82.9%to 84.9% overall, while for rare names suchas Japanese the effect of using unlabelled datawith context features is even greater.The purpose of this research is to demon-strate how the unlabelled data can be used.
Inthe future we hope to investigate the use ofother context features, as well as to study theeffect of data size on the NOR accuracy im-provement.The feature of names?
places normally ex-hibit great variation: one country name may bespelled in many different ways, and often thereare names of cities etc that surround personalnames.
We will explore to normalise namesof places by substituting each name with nameof the country where the place is in the futurework.977References[Berger et al1996] Berger, A., Stephen A.Della Pietra, and V. Della Pietra.
1996.
Amaximum entropy approach to natural lan-guage processing.
Computational Linguistics,22(1):39?71.
[Byrd et al1995] Byrd, R. H., P. Lu, and J. Nocedal.1995.
A limited memory algorithm for boundconstrained optimization.
SIAM Journal of Sci-entific and Statistical Computing, 16(5):1190?1208.
[Cavnar and Trenkle1994] Cavnar, William B. andJohn M. Trenkle.
1994.
Ngram based text cat-egorization.
In Proc.
3rd Annual Symposium onDocument Analysis and Information Retrieval,pages 275?282.
[Knight and Graehl1998] Knight, Kevin andJonathan Graehl.
1998.
Machine translitera-tion.
Computational Linguistics, 24(4).
[Kuo et al2007] Kuo, Jin-Shea, Haizhou Li, andYing-Kuei Yang.
2007.
A phonetic similaritymodel for automatic extraction of transliterationpairs.
ACM Transactions on Asian Language In-formation Processing, 6(2).
[Li et al2007a] Li, Haizhou, Shuanhu Bai, and Jin-Shea Kuo.
2007a.
Transliteration.
In Advancesin Chinese Spoken Language Processing, chap-ter 15, pages 341?364.
World Scientific.
[Li et al2007b] Li, Haizhou, Khe Chai Sim, Jin-Shea Kuo, and Minghui Dong.
2007b.
Semantictransliteration of personal names.
In Proc.
45thAnnual Meeting of the ACL, pages 120?127.
[Qu and Grefenstette2004] Qu, Yan and GregoryGrefenstette.
2004.
Finding ideographic rep-resentations of Japanese names written in Latinscript via language identification and corpus val-idation.
In Proc.
42nd ACL Annual Meeting,pages 183?190, Barcelona, Spain.
[Zhang et al2008] Zhang, Min, Chengjie Sun,Haizhou Li, Aiti Aw, Chew Lim Tan, and Xi-aolong Wang.
2008.
Name origin recognitionusing maximum entropy model and diverse fea-tures.
In Proc.
3rd Int?l Conf.
NLP, pages 56?63.978
