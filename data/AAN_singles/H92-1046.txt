Lexical Disambiguation using Simulated AnnealingJim Cowie, Joe Guthrie, Louise GuthrieComputing Research LaboratoryBox 30001New Mexico State UniversityLas Cruces, NM 88003-0001ABSTRACTThe resolution of lexical ambiguity is important for most nat-ural language processing tasks, and a range of computationaltechniques have been proposed for its solution.
None of thesehas yet proven effective on a large scale.
In this paper, we de-scribe a method for lexical disambiguation of text using thedefinitions in a machine-readable dictionary together withthe technique of simulated annealing.
The method operateson complete sentences and attempts to select the optimalcombinations ofword senses for all the words in the sentencesimultaneously.
The words in the sentences may be any ofthe 28,000 headwords in Longman's Dictionary of Contempo-rary English (LDOCE) and are disambiguated relative to thesenses given in LDOCE.
Our initial results on a sample setof 50 sentences are comparable to those of other researchers,and the fully automatic method requires no hand coding oflexical entries, or hand tagging of text.1.
Introduct ionThe problem of word-sense disambiguation is centralto text processing.
Recently, promising computat ionalmethods have been suggested \[Lesk, 1987; McDonald etal., 1990; Wilks et al, 1990; Zernik and Jacobs, 1990;Guthrie et al, 1991; Hearst, 1991\] which attempt o usethe local context of the word to be disambiguated to-gether with information about each of its word senses tosolve this problem.Lesk \[1987\] described a technique which measured theamount of overlap between a dictionary sense definitionand the local context of the word to be disambiguated tosuccessfully disambiguate he word "cone" in the phrases"pine cone" and "ice cream cone".
Later researchershave extended this basic idea in various ways.
Wilks etal., \[1990\] identified neighborhoods of the 2,187 controlvocabulary words in Longman's Dictionary of Contem-porary English (LDOCE) \[Procter, 1978\] based on theco-occurrence ofwords in LDOCE dictionary definitions.These neighborhoods were then used to expand the wordsense definitions of the word to be disambiguated, andthe'overlap between the expanded efinitions and the lo-cdl context was used to select he correct sense of a word.
?/~ similar method reported by Guthrie et al, \[1991\] de-fined subject-specific neighborhoods of words, using thesubject area markings in the machine readable version ofLDOCE.
Hearst \[1991\] suggests using syntactic informa-tion and part-of-speech tagging to aid in the disambigua-tion.
She gathers co-occurrence information from man-ually sense-tagged text.
Zernik and Jacobs \[1990\] alsoderive their neighborhoods from a training text whichhas been sense-tagged by hand.
This method incorpo-rates other clues as to the sense of the word in questionfound in the morphology or by first tagging the text asto part of speech.Although each of these techniques looks somewhatpromising for disambiguation, the techniques have onlybeen applied to several words, and the results have beenbased on experiments which repeatedly disambiguate asingle word (or in \[Zernik and Jacobs, 1990\], one of threewords) in a large number of sentences.
In the cases wherea success rate for the technique is reported, the resultsvary from 35% to 80%, depending on whether the cor-rect dictionary sense is desired, or some coarser graineddistinction is considered acceptable.For even the most successful of these techniques, process-ing of text is limited because of the amount of computa-tion necessary to disambiguate each word in a sentence.A sentence which has ten words, several of which havemultiple senses, can easily generate a million possiblecombinations of senses.
The following figure ??
illus-trates the number of combinations of word senses in theexample sentences used in our experiment described be-low.
Furthermore, if only one sense is computed at atime, as is the case in all of the numerically based workon disambiguation, the question arises as to whether andhow to incorporate the fact that a sense has been chosenfor a word when attempting to disambiguate the next.Should this first choice be changed in light of how otherword senses are selected?
These problems have not yetbeen addressed.In contrast o the somewhat numerical techniques de-scribed above, more principled methods based on lin-guistic information such as semantic preferences \[Wilks,1975a; 1975b; Wilks and Fass, 1991\] have also been usedfor lexical disambiguation.
These methods require exten-238Combinationsle+093Ic.o.053le44)43Ic+033- , ?le401 \[20.00Combinat ions  o f  Word  SensesB?
Or ?
?'
?
m%,  | ?40.00 60.00 110.00 100,00 120.00~t  0Figure 1:Total Sensessive hand crafting by specialists of lexical items: assign-ing semantic categories to nouns, preferences to verbsand adjectives, etc.
Maintaining consistency in thesecategories and preferences i  a problem, and these meth-ods are also susceptible to the combinatorial explosiondescribed above.In this paper we suggest the application of a compu-tational method called simulated annealing to this gen-eral class of methods (including some of the numericalmethods referenced above) to allow all senses to be de-termined at once in a computationally effective way.
Wedescribe the application of simulated annealing to a ba-sic method similar to that of Lesk \[1987\] which doesn'tmake use of any of the features uch as part of speechtagging, subject area, or the use of morphology to de-termine part of speech.
The simplicity of the techniquemakes it fully automatic, and it requires no hand-taggingof text or hand-crafting ofneighborhoods.
When this ba-sic method operates under the guidance of the simulatedannealing algorithm, sense selections are made concur-rently for all ambiguous words in the sentence in a waydesigned to optimize their choice.
The system's perfor-mance on a set of test sentences was encouraging andcan be expected to improve when some of the refine-ments mentioned above are incorporated.2.
Simulated AnnealingThe method of simulated annealing \[Metropolis et al,1953; Kirkpatrick et al, 1983\] is a technique for solv-ing large scale problems of combinatorial minimization.It has been successfully applied to the famous travel-ing Salesman problem of finding the shoitest route fora salesman who must visit a number of cities in turn,and is now a standard method for optimizing the place-ment of circuit elements on large scale integrated cir-cuits.
Simulated annealing was applied to parsing bySampson \[1986\], but since the method has not yet beenwidely applied to Computational Linguistics or NaturalLanguage Processing, we describe it briefly.The name of the algorithm is an analogy to the processby which metals cool and anneal.
A feature of this phe-nomenon is that slow cooling usually allows the metalto reach a uniform composition and a minimum energystate, while fast cooling leads to an amorphous tatewith higher energy.
In simulated annealing, a parameterT which corresponds to temperature is decreased slowlyenough to allow the system to find its minimum.The process requires a function E of configurations ofthe system which corresponds to the energy.
It is E thatwe seek to minimize.
From a starting point, a new con-figuration is randomly chosen, and a new value of E iscomputed.
If the new E is less than the old one, the newconfiguration is chosen to replace the older.
An essentialfeature of simulated annealing is that even if the new Eis larger than the old (indicating that this configurationis farther away from the desired minimum than the lastchoice), the new configuration may be chosen.
The de-cision of whether or not to replace the old configurationwith the new inferior one is made probabilistieally.
Thisfeature of allowing the algorithm to "go up hill" helpsit to avoid settling on a local minimum which is not theactual minimum.
In succeeding trials, it becomes moredifficult for configurations which increase E to be chosen,and finally, when the method has retained the same con-figuration for long enough, that configuration is chosenas the solution.
In the traveling salesman example, theconfigurations are the different paths through the cities,and E is the total length of his trip.
The final configu-ration is an approximation to the shortest path throughthe cities.
The next section describes how the algorithmmay be applied to word-sense disambiguation.3.
Word-Sense  D isambiguat ionGiven a sentence with N words, we may represent thesenses of the ith word as si l ,s i2, .
.
.s ik, ,  where ki isthe number of senses of the ith word which appear inLDOCE.
A configuration of the system is obtained bychoosing a sense for each word in the sentence.
Our goalis to choose that configuration which a human disam-biguator would choose.
To that end, we must define afunction E whose minimum we may reasonable xpectto correspond to the correct choice of the word senses.The value of E for a given configuration is calculated in239terms of the definitions of the N senses which make itup.
All words in these definitions are stemmed, and theresults stored in a list.
The redundancy R is computedby giving a stemmed word form which appears n timesa score of n - 1 and adding up the scores.
Finally, E isdefined to be 1 I+R"The rationale behind this choice of E is that word senseswhich belong together in a sentence will have more wordsin common in their definitions (larger values of R) thansenses which do not belong together.
Minimizing E willmax imize/ /and etermine our choice of word senses.The starting configuration C is chosen to be that inwhich sense number one of each word is chosen.
Sincethe senses in LDOCE are generally listed with the mostfrequently used sense first, this is a likely starting point.The value of E is computed for this configuration.
Thenext step is to choose at random a word number i anda sense Sij of that ith word.
The configuration C ~ is isconstructed by replacing the old sense of the ith wordby the sense Sij.
Let zXE be the change from E to thevalue computed for C ~.
If zkE < 0, then C ~ replaces C,and we make a new random change in Cq If AE  >= 0,we change to C ~ with probability P = e--mr.
In thisexpression, T is a constant whose initial value is 1, andthe decision of whether or not to adopt C ~ is made bycalling a random number generator.
If the number gen-erated is less than P, C is replaced by Cq Otherwise,C is retained.
This process of generating new configura-tions and checking to see whether or not to choose themis repeated on the order of 1000 times, T is replaced by0.9 T, and the loop entered again.
Once the loop is ex-ecuted with no change in the configuration, the routineends, and this final configuration tells which word sensesare to be selected.these choices of word senses with the output of the pro-gram.
Using the human choices as the standard, thealgorithm correctly disambiguated 47% of the words tothe sense level, and 72% to the homograph level.Direct comparisons of these success rates with those ofother methods is difficult.
None of the other methodswas used to disambiguate the same text, and while wehave attempted to tag every ambiguous word in a sen-tence, other methods were applied to one, or at most afew, highly ambiguous words.
It appears that in somecases the fact that our success rates include not onlyhighly ambiguous words, but some words with only a fewsenses is offset by the fact that other researchers haveused a broader definition of word sense.
For example,the four senses of "interest" used by Zernick and Jacobs\[1990\] may correspond more closely to our two homo-graphs and not our ten senses of "interest."
Their successrate in tagging the three words "interest", "stock", and"bond" was 70%.
Thus it appears that the method wepropose is comparable in effectiveness to the other com-putational methods of word-sense disambiguation, andhas the advantages of being automatically applicable toall the 28,000 words in LDOCE and of being computa-tionally practical.Below we give two examples of the results of the tech-nique.
The words following the arrow are the stemmedwords selected from the definitions and used to calculatethe redundancy.
The headword and sense numbers arethose used in the machine readable version of LDOCE.Finally, we show two graphs (figure ??)
which illustratethe convergence of the simulated annealing technique tothe minimum energy (E) level.
The second graph is aclose-up of the final cycles of the complete process hownin the first graph.4.
An  ExperimentThe algorithm described above was used to disambiguate50 example sentences from LDOCE.
A stop list of verycommon words such as "the", "as", and "of' was re-moved from each sentence.
The sentences then containedfrom two to fifteen words, with an average of 5.5 am-biguous words per sentence.
Definitions in LDOCE arebroken down first into broad senses which we call "ho-mographs", and then into individual senses which dis-tinguish among the various meanings.
For example, onehomograph of "bank" means roughly "something piledup."
There are five senses in this homograph which dis-tinguish whether the thing piled up is snow, clouds, earthby a river, etc.Results of the algorithm were evaluated by having a lit-erate human disambiguate he sentences and comparing5.
ConclusionThis paper describes a method for word-sense disam-biguation based on the simple technique of choosingsenses of the words in a sentence so that their defini-tions in a machine readable dictionary have the mostwords in common.
The amount of computation eces-sary to find this optimal choice exactly quickly becomesprohibitive as the number of ambiguous words and thenumber of senses increase.
The computational techniqueof simulated annealing allows a good approximation tobe computed quickly.
Advantages of this technique overprevious work are that all the words in a sentence aredisambiguated simultaneously, in a reasonable time, andautomatically (with no hand disambiguation of trainingtext).
Results using this technique are comparable toother computational techniques and enhancements in-240SENTENCEThe fish f loundered on the river ban.k,struggling to breatheDISAMBIGUATIONf ish hw 1 sense I :DEF -> fish creature whose blood change temperatureaccording around live water use its FIN tail swimriver hw 0 sense I :DEF -> river wide nature stream water flow betweenbank lake another seabank hw 1 sense I :DEF -> bank land along side river lakestruggle hw I sense 0 :DEF -> struggle violent move fight against thingbreathe hw 0 sense 2 :DEF -> breathe light liveE,~' rgy  Leve l  x I1.1.3Annealing Process.
.
.
.
.
.
.
.
.
.
.
.
?LTLi 21~l (x ,  - -  ,~- -  .
.
.
.
.
: ?\] ~l1111 - -  1" ....... - - f -  i .
.
.
.
.
.
.
.
.
.
.
.
.
....... ?-1 .
.
.
.
.
.
.
.
.
: : : :Z  11 .,.,i,,,-I .
.
.
.
.
.
.
.
.
'.
.
.
.
.
.
.
.
.
.
.
L .
.
.
.
.
.
.I IX).IX} --'g() ~X) - -6,tl,~ ) - -40 ~1 - -21) IXIO.
(~l -?
- .2~?i .....~i """~T.
- "~5?t3 -~-et~ "' Se t8  "%~i"9" .....%~i\]b'"~&\] f -'~&'t2""~=,7 ~-'~' : t  T4  "~f i lg"?
~t.
.
.
.
.
.
.
.
"Set 2,1'~ii3""I I I I , ^.ore, ..... ~,,1.3X) I.IX) 2.tEl 3.00 4.
(YJ 5,rioSENTENCEThe interest on my bank account accrued overthe yearsDISAMBIGUATIONinterest hw 1 sense 6 :DEF -> interest money paid usebank hw 4 sense I :DEF -> bank place money keep paid demandwhere related activity goaccount hw I sense 5 :DEF -> account record state money receive paidbank busy part icular period dateaccrue hw 0 sense 0 :DEF -> accrue become big more additionyear hw 0 sense 3 :DEF -> year period 365 day measure any pointTable 1: Sample Disambiguat ionsE.c rgy  Leve l  x 10 .3I ' l l  (N)JXO ix ,1711 IX}151,.
(X} - -141).00I I l l .
IN)12,0.111I I I I .
(XI .
.
.
.
.
.I l l ) iX )  .
.
.
.
.
.
.AiInellJhlg ProcessI(~)AX)5(}l ~) ?
.
- ,~ .
,~ ,  - - -  , = .~ ,~40,(X,~ctO~Ti"~'4-'~,'3 "- I I )  (X)5 .00  5.211 5 .40  5.
( .0corporat ing co-occurrence, part-of-speech, and subjectcode information,  which have been exploited in one-word-at -a- t ime techniques, may be expected to improvethe performance.References1.
Lesk, Michael E. (1986).
"Automatic Sense Disambigua-tion Using Machine Readable Dictionaries: How to Tella Pine Cone from an Ice Cream Cone".
Proceedings o.fthe A CM SIGDOC Conference, Toronto, Ontario.2.
McDonald, J. E., Plate, T, and Schvaneveldt, R. W.(1990).
"Using Pathfinder to Extract Semantic Informa-Figure 2:tion from Text".
In Schvaneveldt, R. W.
(ed.)
PathfinderAssociative Networks: Studies in Knowledge Organ,sa-t,on, Norwood, N J: Ablex.3.
Wilks, Yorick A., Dan C. Fass, Cheng-Ming Guo, JamesE.
McDonald, Tony Plate, and Brian M. Slator (1990).
"Providing Machine Tractable Dictionary Tools".
Com-puters and Translation 2.
Also to appear in Theoret-ical and Computational Issues in Lexical Semantics(TCILS).
Edited by James Pustejovsky.
Cambridge,MA: MIT Press.2414.
Zernik, Uri, and Paul Jacobs (1990).
"Tagging for learn-ing: Collecting thematic relations from corpus".
Pro-ceedi.ngs of the 13th International Conference on Com-putational Linguistics (COLING-90), Helsinki, Finland,1, pp.
34-37.5.
Guthrie, 3, Guthrie, L., Wilks, Y., and Aidinejad, H.(1991).
"Subject-Dependent Co-Occurrence and WordSense Disambiguation", Proceedings of the ~9th An-nual Meeting of the Association for Computational Lin-guistics, Berkeley, CA.
June 1991. pp.
146-152.
AlsoMemoranda in Computer and Cognitive Science MCCS-91-206, Computing Research Laboratory, New MexicoState University.6.
Hearst, M. (1991).
"Toward Noun Homonym Disam-biguation - Using Local Context in Large Text Cor-pora", Proceedings of the Seventh Annual Conference ofthe UW Centre for the New OED and Text Research,Using Corpora pp.
1-22.7.
Procter, P., R. Ilson, J. Ayto, et al (1978) Longman Dic-tionary of Contemporary English.
Haxlow, UK: LongmanGroup Limited.8.
Wilks, Yorick A.
(1975a).
"An Intelligent Analyzer andUnderstander ofEngfish".
Communications of the A CM,18, 5, pp.
264-274.
Reprinted in Readings in Natu-ral Language Processing, Edited by Barbara J. Grosz,Kaxen Sparck-Jones and Bonnie Lynn Webber, Los Al-tos: Morgan Kaufmann, 1986, pp.
193-203.9.
Wilks, Yorick A.
(1975b).
"A Preferential Pattern-Seeking Semantics for Natural Language Inference".
Ar-tificial Intelligence, 6, pp.
53-74.10.
Wilks, Y. and Fass.
D. (1991).
"Preference Semantics:a family history", To appear in Computing and Math-ematics with Applications (in press).
A shorter versionin the second edition of the Encyclopedia of ArtificialIntelligence .11.
Metropolis, N., Rosenbluth, A., Rosenbluth, M., Teller,A., and Teller, E. (1953) J. Chem.
Phys.
vol.
21, p.1087.12.
Kirkpatrick, S., Gelatt, C. D., and Vecchi, M. P. (1983).Science vol.
220, pp.
671-680.13.
Sampson, G. (1986).
"A Stochastic Approach to Pars-ing".
l l th International Conference on ComputationalLinguistics (GOLING-86).
pp.
151-155.242
