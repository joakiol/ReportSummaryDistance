Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 545?550,Baltimore, Maryland, USA, June 23-25 2014.c?2014 Association for Computational LinguisticsImproving Lexical Embeddings with Semantic KnowledgeMo Yu?Machine Translation LabHarbin Institute of TechnologyHarbin, Chinagflfof@gmail.comMark DredzeHuman Language Technology Center of ExcellenceCenter for Language and Speech ProcessingJohns Hopkins UniversityBaltimore, MD 21218mdredze@cs.jhu.eduAbstractWord embeddings learned on unlabeleddata are a popular tool in semantics, butmay not capture the desired semantics.
Wepropose a new learning objective that in-corporates both a neural language modelobjective (Mikolov et al, 2013) and priorknowledge from semantic resources tolearn improved lexical semantic embed-dings.
We demonstrate that our embed-dings improve over those learned solely onraw text in three settings: language mod-eling, measuring semantic similarity, andpredicting human judgements.1 IntroductionWord embeddings are popular representations forsyntax (Turian et al, 2010; Collobert and We-ston, 2008; Mnih and Hinton, 2007), semantics(Huang et al, 2012; Socher et al, 2013), morphol-ogy (Luong et al, 2013) and other areas.
A longline of embeddings work, such as LSA and ran-domized embeddings (Ravichandran et al, 2005;Van Durme and Lall, 2010), has recently turnedto neural language models (Bengio et al, 2006;Collobert and Weston, 2008; Turian et al, 2010).Unsupervised learning can take advantage of largecorpora, which can produce impressive results.However, the main drawback of unsupervisedlearning is that the learned embeddings may notbe suited for the task of interest.
Consider se-mantic embeddings, which may capture a notionof semantics that improves one semantic task butharms another.
Controlling this behavior is chal-lenging with an unsupervised objective.
However,rich prior knowledge exists for many tasks, andthere are numerous such semantic resources.We propose a new training objective for learn-ing word embeddings that incorporates prior?This work was done while the author was visiting JHU.knowledge.
Our model builds on word2vec(Mikolov et al, 2013), a neural network basedlanguage model that learns word embeddings bymaximizing the probability of raw text.
We extendthe objective to include prior knowledge aboutsynonyms from semantic resources; we considerboth the Paraphrase Database (Ganitkevitch et al,2013) and WordNet (Fellbaum, 1999), which an-notate semantic relatedness between words.
Thelatter was also used in (Bordes et al, 2012) fortraining a network for predicting synset relation.The combined objective maximizes both the prob-ability of the raw corpus and encourages embed-dings to capture semantic relations from the re-sources.
We demonstrate improvements in ourembeddings on three tasks: language modeling,measuring word similarity, and predicting humanjudgements on word pairs.2 Learning EmbeddingsWe present a general model for learning word em-beddings that incorporates prior knowledge avail-able for a domain.
While in this work we con-sider semantics, our model could incorporate priorknowledge from many types of resources.
We be-gin by reviewing the word2vec objective and thenpresent augmentations of the objective for priorknowledge, including different training strategies.2.1 Word2vecWord2vec (Mikolov et al, 2013) is an algorithmfor learning embeddings using a neural languagemodel.
Embeddings are represented by a set oflatent (hidden) variables, and each word is rep-resented by a specific instantiation of these vari-ables.
Training learns these representations foreach word wt(the tth word in a corpus of size T )so as to maximize the log likelihood of each tokengiven its context: words within a window sized c:max1TT?t=1log p(wt|wt+ct?c), (1)545where wt+ct?cis the set of words in the window ofsize c centered at wt(wtexcluded).Word2vec offers two choices for modeling ofEq.
(1): a skip-gram model and a continuous bag-of-words model (cbow).
The latter worked betterin our experiments so we focus on it in our presen-tation.
cbow defines p(wt|wt+ct?c) as:exp(e?wt>??
?c?j?c,j 6=0ewt+j)?wexp(e?w>??
?c?j?c,j 6=0ewt+j), (2)where ewand e?wrepresent the input and outputembeddings respectively, i.e., the assignments tothe latent variables for word w. While some learna single representation for each word (e?w, ew),our results improved when we used a separate em-bedding for input and output in cbow.2.2 Relation Constrained ModelSuppose we have a resource that indicates rela-tions between words.
In the case of semantics,we could have a resource that encodes semanticsimilarity between words.
Based on this resource,we learn embeddings that predict one word fromanother related word.
We defineR as a set of rela-tions between two words w and w?.
R can containtyped relations (e.g., w is related to w?througha specific type of semantic relation), and rela-tions can have associated scores indicating theirstrength.
We assume a single relation type of uni-form strength, though it is straightforward to in-clude additional characteristics into the objective.Define Rwto be the subset of relations in Rwhich involve word w. Our objective maximizesthe (log) probability of all relations by summingover all words N in the vocabulary:1NN?i=1?w?Rwilog p (w|wi) , (3)p(w|wi) = exp(e?wTewi)/?w?exp(e?w?Tewi)takes a form similar to Eq.
(2) but without thecontext: e and e?are again the input and outputembeddings.
For our semantic relations e?wandeware symmetrical, so we use a single embedding.Embeddings are learned such that they are predic-tive of related words in the resource.
We call thisthe Relation Constrained Model (RCM).2.3 Joint ModelThe cbow and RCM objectives use separate datafor learning.
While RCM learns embeddingssuited to specific tasks based on knowledge re-sources, cbow learns embeddings for words not in-cluded in the resource but appear in a corpus.
Weform a joint model through a linear combinationof the two (weighted by C):1TT?t=1log p(wt|wt+ct?c)+CNN?i=1?w?Rwilog p (w|wi)Based on our initial experiments, RCM uses theoutput embeddings of cbow.We learn embeddings using stochastic gradientascent.
Updates for the first term for e?and e are:e?w?
?cbow(?(f(w))?
I[w=wt])?t+c?j=t?cewjewj?
?cbow?w(?(f(w))?
I[w=wt])?
e?w,where ?
(x) = exp{x}/(1 + exp{x}), I[x]is 1when x is true, f(w) = e?w>?t+cj=t?cewj.
Secondterm updates are:e?w?
?RCM(?(f?(w))?
I[w?Rwi])?
e?wie?wi?
?RCM?w(?(f?(w))?
I[w?Rwi])?
e?w,where f?
(w) = e?w>e?wi.
We use two learningrates: ?cbowand ?RCM.2.4 Parameter EstimationAll three models (cbow, RCM and joint) use thesame training scheme based on Mikolov et al(2013).
There are several choices to make in pa-rameter estimation; we present the best perform-ing choices used in our results.We use noise contrastive estimation (NCE)(Mnih and Teh, 2012), which approximately max-imizes the log probability of the softmax objec-tive (Eq.
2).
For each objective (cbow or RCM),we sample 15 words as negative samples for eachtraining instance according to their frequencies inraw texts (i.e.
training data of cbow).
Suppose whas frequency u(w), then the probability of sam-pling w is p(w) ?
u(w)3/4.We use distributed training, where shared em-beddings are updated by each thread based ontraining data within the thread, i.e., asynchronousstochastic gradient ascent.
For the joint model,we assign threads to the cbow or RCM objectivewith a balance of 12:1(i.e.
C is approximately112).We allow the cbow threads to control convergence;training stops when these threads finish process-ing the data.
We found this an effective method546for balancing the two objectives.
We trained eachcbow objective using a single pass over the data set(except for those in Section 4.1), which we empir-ically verified was sufficient to ensure stable per-formances on semantic tasks.Model pre-training is critical in deep learning(Bengio et al, 2007; Erhan et al, 2010).
We eval-uate two strategies: random initialization, and pre-training the embeddings.
For pre-training, we firstlearn using cbow with a random initialization.
Theresulting trained model is then used to initializethe RCM model.
This enables the RCM model tobenefit from the unlabeled data, but refine the em-beddings constrained by the given relations.Finally, we consider a final model for trainingembeddings that uses a specific training regime.While the joint model balances between fitting thetext and learning relations, modeling the text atthe expense of the relations may negatively impactthe final embeddings for tasks that use the embed-dings outside of the context of word2vec.
There-fore, we use the embeddings from a trained jointmodel to pre-train an RCM model.
We call thissetting Joint?RCM.3 EvaluationFor training cbow we use the New York Times(NYT) 1994-97 subset from Gigaword v5.0(Parker et al, 2011).
We select 1,000 paragraphseach for dev and test data from the December 2010portion of the NYT.
Sentences are tokenized usingOpenNLP1, yielding 518,103,942 tokens for train-ing, 42,953 tokens for dev and 41,344 for test.We consider two resources for training theRCM term: the Paraphrase Database (PPDB)(Ganitkevitch et al, 2013) and WordNet (Fell-baum, 1999).
For each semantic pair extractedfrom these resources, we add a relation to theRCM objective.
Since we use both resources forevaluation, we divide each into train, dev and test.PPDB is an automatically extracted dataset con-taining tens of millions of paraphrase pairs, in-cluding words and phrases.
We used the ?lexi-cal?
version of PPDB (no phrases) and filtered toinclude pairs that contained words found in the200,000 most frequent words in the NYT corpus,which ensures each word in the relations had sup-port in the text corpus.
Next, we removed dupli-cate pairs: if <A,B> occurred in PPDB, we re-moved relations of <B,A>.
PPDB is organized1https://opennlp.apache.org/PPDB Relations WordNet RelationsTrain XL 115,041 Train 68,372XXL 587,439 (not used inXXXL 2,647,105 this work)Dev 1,582 Dev 1,500Test 1,583 Test 1,500Table 1: Sizes of semantic resources datasets.into 6 parts, ranging from S (small) to XXXL.Division into these sets is based on an automat-ically derived accuracy metric.
Since S containsthe most accurate paraphrases, we used these forevaluation.
We divided S into a dev set (1582pairs) and test set (1583 pairs).
Training was basedon one of the other sets minus relations from S.We created similar splits using WordNet, ex-tracting synonyms using the 100,000 most fre-quent NYT words.
We divide the vocabulary intothree sets: the most frequent 10,000 words, wordswith ranks between 10,001-30,000 and 30,001-100,000.
We sample 500 words from each set toconstruct a dev and test set.
For each word wesample one synonym to form a pair.
The remain-ing words and their synonyms are used for train-ing.
However we did not use the training data be-cause it is too small to affect the results.
Table 1summarizes the datasets.4 ExperimentsThe goal of our experiments is to demonstrate thevalue of learning semantic embeddings with infor-mation from semantic resources.
In each setting,we will compare the word2vec baseline embed-ding trained with cbow against RCM alone, thejoint model and Joint?RCM.
We consider threeevaluation tasks: language modeling, measuringsemantic similarity, and predicting human judge-ments on semantic relatedness.
In all of our ex-periments, we conducted model development andtuned model parameters (C, ?cbow, ?RCM, PPDBdataset, etc.)
on development data, and evaluatethe best performing model on test data.
The mod-els are notated as follows: word2vec for the base-line objective (cbow or skip-gram), RCM-r/p andJoint-r/p for random and pre-trained initializationsof the RCM and Joint objectives, and Joint?RCMfor pre-training RCM with Joint embeddings.
Un-less otherwise notes, we train using PPDB XXL.We initially created WordNet training data, butfound it too small to affect results.
Therefore,we include only RCM results trained on PPDB,but show evaluations on both PPDB and WordNet.547Model NCE HSword2vec (cbow) 8.75 6.90RCM-p 8.55 7.07Joint-r (?RCM= 1?
10?2) 8.33 6.87Joint-r (?RCM= 1?
10?3) 8.20 6.75Joint?RCM 8.40 6.92Table 2: LM evaluation on held out NYT data.We trained 200-dimensional embeddings and usedoutput embeddings for measuring similarity.
Dur-ing the training of cbow objectives we remove allwords with frequencies less than 5, which is thedefault setting of word2vec.4.1 Language ModelingWord2vec is fundamentally a language model,which allows us to compute standard evaluationmetrics on a held out dataset.
After obtainingtrained embeddings from any of our objectives,we use the embeddings in the word2vec modelto measure perplexity of the test set.
Measuringperplexity means computing the exact probabilityof each word, which requires summation over allwords in the vocabulary in the denominator of thesoftmax.
Therefore, we also trained the languagemodels with hierarchical classification (Mikolovet al, 2013) strategy (HS).
The averaged perplexi-ties are reported on the NYT test set.While word2vec and joint are trained as lan-guage models, RCM is not.
In fact, RCM does noteven observe all the words that appear in the train-ing set, so it makes little sense to use the RCM em-beddings directly for language modeling.
There-fore, in order to make fair comparison, for everyset of trained embeddings, we fix them as inputembedding for word2vec, then learn the remain-ing input embeddings (words not in the relations)and all the output embeddings using cbow.
Sincethis involves running cbow on NYT data for 2 it-erations (one iteration for word2vec-training/pre-training/joint-modeling and the other for tuningthe language model), we use Joint-r (random ini-tialization) for a fair comparison.Table 2 shows the results for language mod-eling on test data.
All of our proposed modelsimprove over the baseline in terms of perplexitywhen NCE is used for training LMs.
When HS isused, the perplexities are greatly improved.
How-ever in this situation only the joint models improvethe results; and Joint?RCM performs similar tothe baseline, although it is not designed for lan-guage modeling.
We include the optimal ?RCMin the table; while set ?cbow= 0.025 (the defaultsetting of word2vec).
Even when our goal is tostrictly model the raw text corpus, we obtain im-provements by injecting semantic information intothe objective.
RCM can effectively shift learningto obtain more informative embeddings.4.2 Measuring Semantic SimilarityOur next task is to find semantically related wordsusing the embeddings, evaluating on relationsfrom PPDB and WordNet.
For each of the wordpairs in the evaluation set <A,B>, we use the co-sine distance between the embeddings to score Awith a candidate word B?.
We use a large sampleof candidate words (10k, 30k or 100k) and rank allcandidate words for pairs where B appears in thecandidates.
We then measure the rank of the cor-rect B to compute mean reciprocal rank (MRR).Our goal is to use word A to select word B asthe closest matching word from the large set ofcandidates.
Using this strategy, we evaluate theembeddings from all of our objectives and mea-sure which embedding most accurately selectedthe true correct word.Table 3 shows MRR results for both PPDBand WordNet dev and test datasets for all models.All of our methods improve over the baselines innearly every test set result.
In nearly every case,Joint?RCM obtained the largest improvements.Clearly, our embeddings are much more effectiveat capturing semantic similarity.4.3 Human JudgementsOur final evaluation is to predict human judge-ments of semantic relatedness.
We have pairs ofwords from PPDB scored by annotators on a scaleof 1 to 5 for quality of similarity.
Our data arethe judgements used by Ganitkevitch et al (2013),which we filtered to include only those pairs forwhich we learned embeddings, yielding 868 pairs.We assign a score using the dot product betweenthe output embeddings of each word in the pair,then order all 868 pairs according to this score.Using the human judgements, we compute theswapped pairs rate: the ratio between the numberof swapped pairs and the number of all pairs.
Forpair p scored ypby the embeddings and judged y?pby an annotator, the swapped pair rate is:?p1,p2?DI[(yp1?
yp2) (y?p2?
y?p1) < 0]?p1,p2?DI[yp16= yp2](4)where I[x] is 1 when x is true.548PPDB WordNetModelDev Test Dev Test10k 30k 100k 10k 30k 100k 10k 30k 100k 10k 30k 100kword2vec (cbow) 49.68 39.26 29.15 49.31 42.53 30.28 10.24 8.64 5.14 10.04 7.90 4.97word2vec (skip-gram) 48.70 37.14 26.20 - - - 8.61 8.10 4.62 - - -RCM-r 55.03 42.52 26.05 - - - 13.33 9.05 5.29 - - -RCM-p 61.79 53.83 40.95 65.42 55.82 41.20 15.25 12.13 7.46 14.13 11.23 7.39Joint-r 59.91 50.87 36.81 - - - 15.73 11.36 7.14 13.97 10.51 7.44Joint-p 59.75 50.93 37.73 64.30 53.27 38.97 15.61 11.20 6.96 - - -Joint?RCM 64.22 54.99 41.34 68.20 57.87 42.64 16.81 11.67 7.55 16.16 11.21 7.56Table 3: MRR for semantic similarity on PPDB and WordNet dev and test data.
Higher is better.
AllRCM objectives are trained with PPDB XXL.
To preserve test data integrity, only the best performingsetting of each model is evaluated on the test data.Model Swapped Pairs Rateword2vec (cbow) 17.81RCM-p 16.66Joint-r 16.85Joint-p 16.96Joint?RCM 16.62Table 4: Results for ranking the quality of PPDBpairs as compared to human judgements.PPDB DevModel Relations 10k 30k 100kRCM-r XL 24.02 15.26 9.55RCM-p XL 54.97 45.35 32.95RCM-r XXL 55.03 42.52 26.05RCM-p XXL 61.79 53.83 40.95RCM-r XXXL 51.00 44.61 28.42RCM-p XXXL 53.01 46.35 34.19Table 5: MRR on PPDB dev data for training onan increasing number of relations.Table 4 shows that all of our models obtainreductions in error as compared to the baseline(cbow), with Joint?RCM obtaining the largest re-duction.
This suggests that our embeddings arebetter suited for semantic tasks, in this case judgedby human annotations.PPDB DevModel ?RCM10k 30k 100kJoint-p 1?
10?147.17 36.74 24.505?
10?254.31 44.52 33.071?
10?259.75 50.93 37.731?
10?357.00 46.84 34.45Table 6: Effect of learning rate ?RCMon MRR forthe RCM objective in Joint models.4.4 AnalysisWe conclude our experiments with an analysis ofmodeling choices.
First, pre-training RCM modelsgives significant improvements in both measuringsemantic similarity and capturing human judge-ments (compare ?p?
vs. ?r?
results.)
Second, thenumber of relations used for RCM training is animportant factor.
Table 5 shows the effect on devdata of using various numbers of relations.
Whilewe see improvements from XL to XXL (5 times asmany relations), we get worse results on XXXL,likely because this set contains the lowest qualityrelations in PPDB.
Finally, Table 6 shows differentlearning rates ?RCMfor the RCM objective.The baseline word2vec and the joint model havenearly the same averaged running times (2,577sand 2,644s respectively), since they have samenumber of threads for the CBOW objective and thejoint model uses additional threads for the RCMobjective.
The RCM models are trained with sin-gle thread for 100 epochs.
When trained on thePPDB-XXL data, it spends 2,931s on average.5 ConclusionWe have presented a new learning objective forneural language models that incorporates priorknowledge contained in resources to improvelearned word embeddings.
We demonstrated thatthe Relation Constrained Model can lead to bettersemantic embeddings by incorporating resourceslike PPDB, leading to better language modeling,semantic similarity metrics, and predicting hu-man semantic judgements.
Our implementation isbased on the word2vec package and we made itavailable for general use2.We believe that our techniques have implica-tions beyond those considered in this work.
Weplan to explore the embeddings suitability forother semantics tasks, including the use of re-sources with both typed and scored relations.
Ad-ditionally, we see opportunities for jointly learn-ing embeddings across many tasks with many re-sources, and plan to extend our model accordingly.Acknowledgements Yu is supported by ChinaScholarship Council and by NSFC 61173073.2https://github.com/Gorov/JointRCM549ReferencesYoshua Bengio, Holger Schwenk, Jean-S?ebastienSen?ecal, Fr?ederic Morin, and Jean-Luc Gauvain.2006.
Neural probabilistic language models.
InInnovations in Machine Learning, pages 137?186.Springer.Yoshua Bengio, Pascal Lamblin, Dan Popovici, HugoLarochelle, et al 2007.
Greedy layer-wise trainingof deep networks.
In Neural Information ProcessingSystems (NIPS).Antoine Bordes, Xavier Glorot, Jason Weston, andYoshua Bengio.
2012.
Joint learning of wordsand meaning representations for open-text semanticparsing.
In International Conference on ArtificialIntelligence and Statistics, pages 127?135.Ronan Collobert and Jason Weston.
2008.
A unifiedarchitecture for natural language processing: Deepneural networks with multitask learning.
In Interna-tional Conference on Machine Learning (ICML).Dumitru Erhan, Yoshua Bengio, Aaron Courville,Pierre-Antoine Manzagol, Pascal Vincent, and SamyBengio.
2010.
Why does unsupervised pre-traininghelp deep learning?
Journal of Machine LearningResearch (JMLR), 11:625?660.Christiane Fellbaum.
1999.
WordNet.
Wiley OnlineLibrary.Juri Ganitkevitch, Benjamin Van Durme, and ChrisCallison-Burch.
2013.
PPDB: The paraphrasedatabase.
In North American Chapter of the Asso-ciation for Computational Linguistics (NAACL).Eric H Huang, Richard Socher, Christopher D Man-ning, and Andrew Y Ng.
2012.
Improving wordrepresentations via global context and multiple wordprototypes.
In Association for Computational Lin-guistics (ACL), pages 873?882.Minh-Thang Luong, Richard Socher, and Christo-pher D Manning.
2013.
Better word representa-tions with recursive neural networks for morphol-ogy.
In Conference on Natural Language Learning(CoNLL).Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Cor-rado, and Jeffrey Dean.
2013.
Distributed represen-tations of words and phrases and their composition-ality.
arXiv preprint arXiv:1310.4546.Andriy Mnih and Geoffrey Hinton.
2007.
Three newgraphical models for statistical language modelling.In International Conference on Machine Learning(ICML).Andriy Mnih and Yee Whye Teh.
2012.
A fast andsimple algorithm for training neural probabilisticlanguage models.
arXiv preprint arXiv:1206.6426.Robert Parker, David Graff, Junbo Kong, Ke Chen, andKazuaki Maeda.
2011.
English gigaword fifth edi-tion.
Technical report, Linguistic Data Consortium.Deepak Ravichandran, Patrick Pantel, and EduardHovy.
2005.
Randomized algorithms and nlp: us-ing locality sensitive hash function for high speednoun clustering.
In Association for ComputationalLinguistics (ACL).Richard Socher, Alex Perelygin, Jean Wu, JasonChuang, Christopher D. Manning, Andrew Ng, andChristopher Potts.
2013.
Recursive deep modelsfor semantic compositionality over a sentiment tree-bank.
In Empirical Methods in Natural LanguageProcessing (EMNLP), pages 1631?1642.Joseph Turian, Lev Ratinov, and Yoshua Bengio.
2010.Word representations: a simple and general methodfor semi-supervised learning.
In Association forComputational Linguistics (ACL).Benjamin Van Durme and Ashwin Lall.
2010.
On-line generation of locality sensitive hash signatures.In Association for Computational Linguistics (ACL),pages 231?235.550
