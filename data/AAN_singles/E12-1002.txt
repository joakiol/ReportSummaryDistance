Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 2?11,Avignon, France, April 23 - 27 2012. c?2012 Association for Computational LinguisticsPower-Law Distributions for Paraphrases Extracted from BilingualCorporaSpyros Martzoukos Christof MonzInformatics Institute, University of AmsterdamScience Park 904, 1098 XH Amsterdam, The Netherlands{s.martzoukos, c.monz}@uva.nlAbstractWe describe a novel method that extractsparaphrases from a bitext, for both thesource and target languages.
In orderto reduce the search space, we decom-pose the phrase-table into sub-phrase-tablesand construct separate clusters for sourceand target phrases.
We convert the clus-ters into graphs, add smoothing/syntactic-information-carrier vertices, and computethe similarity between phrases with a ran-dom walk-based measure, the commutetime.
The resulting phrase-paraphraseprobabilities are built upon the conversionof the commute times into artificial co-occurrence counts with a novel technique.The co-occurrence count distribution be-longs to the power-law family.1 IntroductionParaphrase extraction has emerged as an impor-tant problem in NLP.
Currently, there exists anabundance of methods for extracting paraphrasesfrom monolingual, comparable and bilingual cor-pora (Madnani and Dorr, 2010; Androutsopou-los and Malakasiotis, 2010); we focus on the lat-ter and specifically on the phrase-table that is ex-tracted from a bitext during the training stage ofStatistical Machine Translation (SMT).
Bannardand Callison-Burch (2005) introduced the pivot-ing approach, which relies on a 2-step transitionfrom a phrase, via its translations, to a paraphrasecandidate.
By incorporating the syntactic struc-ture of phrases (Callison-Burch, 2005), the qual-ity of the paraphrases extracted with pivoting canbe improved.
Kok and Brockett (2010) (hence-forth KB) used a random walk framework to de-termine the similarity between phrases, whichwas shown to outperform pivoting with syntac-tic information, when multiple phrase-tables areused.
In SMT, extracted paraphrases with asso-ciated pivot-based (Callison-Burch et al 2006;Onishi et al 2010) and cluster-based (Kuhn etal., 2010) probabilities have been found to im-prove the quality of translation.
Pivoting has alsobeen employed in the extraction of syntactic para-phrases, which are a mixture of phrases and non-terminals (Zhao et al 2008; Ganitkevitch et al2011).We develop a method for extracting para-phrases from a bitext for both the source and tar-get languages.
Emphasis is placed on the qual-ity of the phrase-paraphrase probabilities as wellas on providing a stepping stone for extractingsyntactic paraphrases with equally reliable prob-abilities.
In line with previous work, our methoddepends on the connectivity of the phrase-table,but the resulting construction treats each side sep-arately, which can potentially be benefited fromadditional monolingual data.The initial problem in harvesting paraphrasesfrom a phrase-table is the identification of thesearch space.
Previous work has relied on breadthfirst search from the query phrase with a depthof 2 (pivoting) and 6 (KB).
The former can betoo restrictive and the latter can lead to excessivenoise contamination when taking shallow syntac-tic information features into account.
Instead, wechoose to cluster the phrase-table into separatesource and target clusters and in order to make thistask computationally feasible, we decompose thephrase-table into sub-phrase-tables.
We proposea novel heuristic algorithm for the decompositionof the phrase-table (Section 2.1), and use a well-established co-clustering algorithm for clustering2each sub-phrase-table (Section 2.2).The underlying connectivity of the sourceand target clusters gives rise to a natural graphrepresentation for each cluster (Section 3.1).The vertices of the graphs consist of phrasesand features with a dual smoothing/syntactic-information-carrier role.
The latter allow (a) re-distribution of the mass for phrases with no appro-priate paraphrases and (b) the extraction of syn-tactic paraphrases.
The proximity among verticesof a graph is measured by means of a randomwalkdistance measure, the commute time (Aldous andFill, 2001).
This measure is known to performwell in identifying similar words on the graph ofWordNet (Rao et al 2008) and a related measure,the hitting time is known to perform well in har-vesting paraphrases on a graph constructed frommultiple phrase-tables (KB).Generally in NLP, power-law distributions aretypically encountered in the collection of countsduring the training stage.
The distances of Sec-tion 3.1 are converted into artificial co-occurrencecounts with a novel technique (Section 3.2).
Al-though they need not be integers, the main chal-lenge is the type of the underlying distributions;it should ideally emulate the resulting count dis-tributions from the phrase extraction stage of amonolingual parallel corpus (Dolan et al 2004).These counts give rise to the desired probabilitydistributions by means of relative frequencies.2 Sub-phrase-tables & Clustering2.1 Extracting Connected ComponentsFor the decomposition of the phrase-table intosub-phrase-tables it is convenient to view thephrase-table as an undirected, unweighted graphP with the vertex set being the source and targetphrases and the edge set being the phrase-table en-tries.
For the rest of this section, we do not distin-guish between source and target phrases, i.e.
bothtypes are treated equally as vertices of P .
Whenreferring to the size of a graph, we mean the num-ber of vertices it contains.A trivial initial decomposition of P is achievedby identifying all its connected components (com-ponents for brevity), i.e.
the mutually disjointconnected subgraphs, {P0, P1, ..., Pn}.
It turnsout (see Section 4.1) that the largest component,say P0, is of significant size.
We call P0 giantand it needs to be further decomposed.
This isdone by identifying all vertices such that, uponremoval, the component becomes disconnected.Such vertices are called articulation points or cut-vertices.
Cut-vertices of high connectivity degreeare removed from the giant component (see Sec-tion 4.1).
For the remaining vertices of the giantcomponent, new components are identified andwe proceed iteratively, while keeping track of thecut-vertices that are removed at each iteration, un-til the size of the largest component is less than acertain threshold ?
(see Section 4.1).Note that at each iteration, when removing cut-vertices from a giant component, the resulting col-lection of components may include graphs con-sisting of a single vertex.
We refer to such ver-tices as residues.
They are excluded from the re-sulting collection and are considered for separatetreatment, as explained later in this section.The cut-vertices need to be inserted appropri-ately back to the components: Starting from thelast iteration step, the respective cut-vertices areadded to all the components of P0 which theyused to ?glue?
together; this process is performediteratively, until there are no more cut-vertices toadd.
By ?addition?
of a cut-vertex to a component,we mean the re-establishment of edges betweenthe former and other vertices of the latter.
Theresult is a collection of components whose totalnumber of unique vertices is less than the numberof vertices of the initial giant component P0.These remaining vertices are the residues.
Wethen construct the graph R which consists ofthe residues together with all their translations(even those that are included in components ofthe above collection) and then identify its compo-nents {R0, ..., Rm}.
It turns out, that the largestcomponent, say R0, is giant and we repeat the de-composition process that was performed on P0.This results in a new collection of componentsas well as new residues: The components needto be pruned (see Section 4.1) and the residuesgive rise to a new graph R?
which is constructedin the same way asR.
We proceed iteratively untilthe number of residues stops changing.
For eachremaining residue u, we identify its translations,and for each translation v we identify the largestcomponent of which v is a member and add u tothat component.The final result is a collection C = D ?
F ,where D is the collection of components emerg-ing from the entire iterative decomposition of P03and R, and F = {P1, ..., Pn}.
Figure 1 showsthe decomposition of a connected graph G0; forsimplicity we assume that only one cut-vertex isremoved at each iteration and ties are resolved ar-bitrarily.
In Figure 2 the residue graph is con-structed and its two components are identified.The iterative insertion of the cut vertices is alsodepicted.
The resulting two components togetherwith those from R form the collection D for G0.The addition of cut-vertices into multiple com-ponents, as well as the construction method of theresidue-based graph R, can yield the occurrencesof a vertex in multiple components in D. We ex-ploit this property in two ways:(a) In order to mitigate the risk of excessive de-composition (which implies greater risk of goodparaphrases being in different components), aswell as to reduce the size of D, a conserva-tive merging algorithm of components is em-ployed.
Suppose that the elements of D areranked according to size in ascending order asD = {D1, ..., Dk, Dk+1, ..., D|D|}, where |Di| ?
?, for i = 1, ..., k, and some threshold ?
(see Sec-tion 4.1).
Each component Di with i ?
{1, ..., k}is examined as follows: For each vertex of Di thenumber of its occurrences inD is inspected; this isdone in order to identify an appropriate vertex b toact as a bridge between Di and other componentsof which b is a member.
Note that translations ofa vertex b with smaller number of occurrences inD are less likely to capture their full spectrum ofparaphrases.
We thus choose a vertex b from Diwith the smallest number of occurrences in D ,resolving ties arbitrarily, and proceed with merg-ing Di with the largest component, say Dj withj ?
{1, ..., |D| ?
1}, of which b is also a member.The resulting merged component Dj?
contains allvertices and edges of Di and Dj and new edges,which are formed according to the rule: if u is avertex of Di and v is a vertex of Dj and (u, v) isa phrase-table entry, then (u, v) is an edge in Dj?
.As long as no connected component has identi-fied Di as the component with which it should bemerged, then Di is deleted from the collection D.(b) We define an idf -inspired measure for eachphrase pair (x, x?)
of the same type (source or tar-get) asidf(x, x?)
=1log |D|log(2c(x, x?
)|D|c(x) + c(x?
)), (1)where c(x, x?)
is the number of components inwhich the phrases x and x?
co-occur, and equiv-alently for c(?).
The purpose of this measure isfor pruning paraphrase candidates and its use isexplained in Section 3.1.
Note that idf(x, x?)
?
[0, 1].The merging process and the idf measure areirrelevant for phrases belonging to the compo-nents of F , since the vertex set of each compo-nent of F is mutually disjoint with the vertex setof any other component in C.G0 s1s2s3s4 t1t 2t3 c0={s2 } G11r={t 2 }s1s4 t1 G12s3s4G12 G21s3 t 4c1={t3} r?
r?
{s4 }t 4 s3 t3t 4t3t 4Figure 1: The decomposition of G0 with verticessi and tj : The cut-vertex of the ith iteration is de-noted by ci, and r collects the residues after eachiteration.
The task is completed in Figure 2.G s0s1 t 0t2 s0s1 t 0t2s2 t 1 =c3 s3 t3 =c4=c4s2 t 1t2 s2 t2t 1s3s0 t3 s0s2 t2t 1Figure 2: Top: Residue graph with its components(no further decomposition is required).
Bottom:Adding cut-vertices back to their components.2.2 Clustering Connected ComponentsThe aim of this subsection is to generate sep-arate clusters for the source and target phrasesof each sub-phrase-table (component) C ?
C.For this purpose the Information-Theoretic Co-Clustering (ITC) algorithm (Dhillon et al 2003)is employed, which is a general principled cluster-ing algorithm that generates hard clusters (i.e.
ev-4ery element belongs to exactly one cluster) of twointerdependent quantities and is known to per-form well on high-dimensional and sparse data.In our case, the interdependent quantities are thesource and target phrases and the sparse data isthe phrase-table.ITC is a search algorithm similar to K-means,in the sense that a cost function, is minimized ateach iteration step and the number of clusters forboth quantities are meta-parameters.
The numberof clusters is set to the most conservative initial-ization for both source and target phrases, namelyto as many clusters as there are phrases.
At eachiteration, new clusters are constructed based onthe identification of the argmin of the cost func-tion for each phrase, which gradually reduces thenumber of clusters.We observe that conservative choices for themeta-parameters often result in good paraphrasesbeing in different clusters.
To overcome this prob-lem, the hard clusters are converted into soft (i.e.an element may belong to several clusters): Onestep before the stopping criterion is met, we mod-ify the algorithm so that instead of assigning aphrase to the cluster with the smallest cost we se-lect the bottom-X clusters ranked by cost.
Addi-tionally, only a certain number of phrases is cho-sen for soft clustering.
Both selections are doneconservatively with criteria based on the proper-ties of the cost functions.The formation of clusters leads to a natural re-finement of the idf measure defined in eqn.
(1):The quantity c(x, x?)
is redefined as the numberof components in which the phrases x and x?
co-occur in at least one cluster.3 Monolingual Graphs & CountsWe proceed with converting the clusters into di-rected, weighted graphs and then extract para-phrases for both the source and target side.
Forbrevity we explain the process restricted to thesource clusters of a sub-phrase-table, but the samemethod applies for the target side and for all sub-phrase-tables in the collection C.3.1 Monolingual graphsEach source cluster is converted into a graph G asfollows: The vertex set consists of the phrases ofthe cluster and an edge between s and s?
exists, if(a) s and s?
have at least one translation from thesame target cluster, and (b) idf(s, s?)
is greaterthan some threshold ?
(see Section 4.1).
If twophrases that satisfy condition (b) and have trans-lations in more than one common target cluster,a distinct such edge is established.
All edges arebi-directional with distinct weights for both direc-tions.Figure 3 depicts an example of such a construc-tion; a link between a phrase si and a target clusterimplies the existence of at least one translation forsi in that cluster.
We are not interested in the tar-get phrases and they are thus not shown.
For sim-plicity we assume that condition (b) is always sat-isfied and the extracted graph contains the maxi-mum possible edges.
Observe that phrases s3 ands4 have two edges connecting them, (due to tar-get clusters Tc and Td) and that the target clusterTa is irrelevant to the construction of the graph,since s1 is the only phrase with translations in it.This conversion of a source cluster into a graph Gs1 s2 s4 s5s3 s8s7s6Ta Tb Tc Td Te Tfs2s1 s3 s4s5 s6s7 s8Figure 3: Top: A source cluster containingphrases s1,..., s8 and the associated target clustersTa,..., Tf .
Bottom: The extracted graph from thesource cluster.
All edges are bi-directional.results in the formation of subgraphs in G, whereeach subgraph is generated by a target cluster.
Ingeneral, if condition (b) is not always satisfied,then G need not be connected and each connectedcomponent is treated as a distinct graph.Analogous to KB, we introduce feature verticesto G: For each phrase vertex s, its part-of-speech(POS) tag sequence and stem sequence are iden-tified and inserted into G as new vertices withbi-directional weighted edges connected to s. Ifphrase vertices s and s?
have the same POS tag se-quence, then they are connected to the same POStag feature vertex.
Similarly for stem feature ver-tices.
See Figure 4 for an example.
Note that wedo not allow edges between POS tag and stem fea-5s124534876Tab Tabcd8ef?53????cd8e????87?f?53???
?87?Figure 4: Adding feature vertices to the extractedgraph (has) ??
(owns) ??
(i have) ??
(i had).Phrase, POS tag feature and stem feature ver-tices are drawn in circles, dotted rectangles andsolid rectangles respectively.
All edges are bi-directional.ture vertices.
The purpose of the feature vertices,unlike KB, is primarily for smoothing and secon-darily for identifying paraphrases with the samesyntactic information and this will become clearin the description of the computation of weights.The set of all phrase vertices that are adja-cent to s is written as ?
(s), and referred toas the neighborhood of s. Let n(s, t) denotethe co-occurrence count of a phrase-table entry(s, t) (Koehn, 2009).
We define the strength ofs in the subgraph generated by cluster T asn(s;T ) =?t?Tn(s, t), (2)which is simply a partial occurrence count for s.We proceed with computing weights for all edgesof G:Phrase?
?phrase weights: Inspired by thenotion of preferential attachment (Yule, 1925),which is known to produce power-law weight dis-tributions for evolving weighted networks (Barratet al 2004), we set the weight of a directededge from s to s?
to be proportional to thestrengths of s?
in all subgraphs in which boths and s?
are members.
Thus, in the randomwalk framework, s is more likely to visita stronger (more reliable) neighbor.
If Ts,s?
={T |s and s?
coexist in subgraph generated by T},then the weight w(s ?
s?)
of the directed edgefrom s to s?
is given byw(s ?
s?)
=?T?Ts,s?n(s?
;T ), (3)if s?
?
?
(s) and 0 otherwise.Phrase?
?feature weights: As mentionedabove, feature vertices have the dual role of car-rying syntactic information and smoothing.
Fromeqn.
(3) it can be deduced that, if for a phrases, the amount of its outgoing weights is close tothe amount of its incoming weights, then this isan indication that at least a significant part of itsneighborhood is reliable; the larger the strengths,the more certain the indication.
Otherwise, eithers or a significant part of its neighborhood isunreliable.
The amount of weight from s to itsfeature vertices should depend on this observationand we thus letnet(s) =???????s???
(s)(w(s ?
s?)?
w(s?
?
s))?????
?+ ,(4)where  prevents net(s) from becoming 0 (seeSection 4.1).
The net weight of a phrase vertexs is distributed over its feature vertices asw(s ?
fX) =< w(s ?
s?)
> +net(s), (5)where the first summand is the average weightfrom s to its neighboring phrase vertices andX = POS,STEM.
If s has multiple POS tagsequences, we distribute the weight of eqn.
(5)relatively to the co-occurrences of s with the re-spective POS tag feature vertices.
The quantity< w(s ?
s?)
> accounts for the basic smoothingand is augmented by a value net(s) that measuresthe reliability of s?s neighborhood; the more unre-liable the neighborhood, the larger the net weightand thus larger the overall weights to the featurevertices.The choice for the opposite direction is trivial:w(fX ?
s) =1|{s?
: (fX , s?)
is an edge }|, (6)where X = POS,STEM.
Note the effect ofeqns.
(4)?
(6) in the case where the neighborhoodof s has unreliable strengths: In a random walkthe feature vertices of s will be preferred and theresulting similarities between s and other phrasevertices will be small, as desired.
Nonetheless,if the syntactic information is the same with anyother phrase vertex inG, then the paraphrases willbe captured.The transition probability from any vertex u toany other vertex v in G, i.e., the probability of6hopping from u to v in one step, is given byp(u ?
v) =w(u ?
v)?v?
w(u ?
v?
), (7)where we sum over all vertices adjacent to u inG.We can thus compute the similarity between anytwo vertices u and v in G by their commute time,i.e., the expected number of steps in a round trip,in a random walk from u to v and then back to u,which is denoted by ?
(u, v) (see Section 4.1 forthe method of computation of ?).
Since ?
(u, v) isa distance measure, the smaller its value, the moresimilar u and v are.3.2 CountsWe convert the distance ?
(u, v) of a vertex pairu, v in a graph G into a co-occurrence countnG(u, v) with a novel technique: In order to as-sess the quality of the pair u, v with respect to Gwe compare ?
(u, v) with ?
(u, x) and ?
(v, x) forall other vertices x in G. We thus consider the av-erage distance of u with the other vertices of Gother than v, and similarly for v. This quantity isdenoted by ?
(u; v) and ?
(v;u) respectively, andby definition it is given by?
(i; j) =?x?Gx 6=j?
(i, x)pG(x|i) (8)where pG(x|i) ?
p(x|G, i) is a yet unknownprobability distribution with respect to G. Thequantity (?
(u; v)+?
(v;u))/2 can then be viewedas the average distance of the pair u, v to the restof the graph G. The co-occurrence count of u andv in G is thus defined bynG(u, v) =?
(u; v) + ?(v;u)2?
(u, v).
(9)In order to calculate the probabilities pG(?|?)
weemploy the following heuristic: Starting with auniform distribution p(0)G (?|?)
at timestep t = 0,we iterate?
(t)(i; j) =?x?Gx 6=j?
(i, x)p(t)G (x|i) (10)n(t)G (u, v) =?
(t)(u; v) + ?(t)(v;u)2?
(u, v)(11)p(t+1)G (v|u) =n(t)G (u, v)?x?G n(t)G (u, v)(12)for all pairs of vertices u, v in G until conver-gence.
Experimentally, we find that convergenceis always achieved.
After the execution of this it-erative process we divide each count by the small-est count in order to achieve a lower bound of 1.A pair u, v may appear in multiple graphs in thesame sub-phrase-tableC.
The total co-occurrencecount of u and v in C and the associated condi-tional probabilities are thus given bynC(u, v) =?G?CnG(u, v) (13)pC(v|u) =nC(u, v)?x?C nC(u, x).
(14)A pair u, v may appear in multiple sub-phrase-tables and for the calculation of the final countn(u, v) we need to average over the associatedcounts from all sub-phrase-tables.
Moreover, wehave to take into account the type of the vertices:For the simplest case where both u and v repre-sent phrase vertices, their expected count is, bydefinition, given byn(s, s?)
=?CnC(s, s?
)p(C|s, s?).
(15)On the other hand, if at least one of u or v isa feature vertex, then we have to consider thephrase vertex that generates this feature: Supposethat u is the phrase vertex s=?acquire?
and v thePOS tag vertex f=?NN?
and they co-occur in twosub-phrase-tables C and C ?
with positive countsnC(s, f) and nC?
(s, f) respectively; the featurevertex f is generated by the phrase vertices ?own-ership?
in C and by ?possession?
in C ?.
In thatcase, an interpolation of the counts nC(s, f) andnC?
(s, f) as in eqn.
(15) would be incorrect anda direct sum nC(s, f) + nC?
(s, f) would providethe true count.
As a result we haven(s, f) =?s?
?CnC(s, f(s?
))p(C|s, f(s?
)),(16)where the first summation is over all phrase ver-tices s?
such that f(s?)
= f .
With a similar argu-ment we can writen(f, f ?)
=?s,s?
?CnC(f(s), f(s?))??
p(C|f(s), f(s?)).
(17)7For the interpolants, from standard probability wefindp(C|u, v) =pC(v|u)p(C|u)?C?
pC?
(v|u)p(C?|u), (18)where the probabilities p(C|u) can be computedby considering the likelihood function`(u) =N?i=1p(xi|u) =N?i=1?CpC(xi|u)p(C|u)and by maximizing the average log-likelihood1N log `(u), where N is the total number of ver-tices with which u co-occurs with positive countsin all sub-phrase-tables.Finally, the desired probability distributions aregiven by the relative frequenciesp(v|u) =n(u, v)?x n(u, x), (19)for all pairs of vertices u, v.4 Experiments4.1 SetupThe data for building the phrase-table Pis drawn from DE-EN bitexts crawled fromwww.project-syndicate.org, which isa standard resource provider for the WMTcampaigns (News Commentary bitexts, see,e.g.
(Callison-Burch et al 2007) ).
The filteredbitext consists of 125K sentences; word align-ment was performed running GIZA++ in both di-rections and generating the symmetric alignmentsusing the ?grow-diag-final-and?
heuristics.
Theresulting P has 7.7M entries, 30% of which are?1-1?, i.e.
entries (s, t) that satisfy p(s|t) =p(t|s) = 1.
These entries are irrelevant for para-phrase harvesting for both the baseline and ourmethod, and are thus excluded from the process.The initial giant component P0 contains 1.7Mvertices (Figure 5), of which 30% becomeresidues and are used to construct R. At each it-eration of the decomposition of a giant compo-nent, we remove the top 0.5% ?
size cut-verticesranked by degree of connectivity, where size isthe number of vertices of the giant component andset ?
= 2500 as the stopping criterion.
The latterchoice is appropriate for the subsequent step ofco-clustering the components, for both time com-plexity and performance of the ITC algorithm.100 102 104 106100101102103104105106107ranksize 100 102 104 106100105P0Figure 5: Log-log plot of ranked components ac-cording to their size (number of source and targetphrases) for: (a) Components extracted from P .?1-1?
components are not shown.
(b) Componentsextracted from the decomposition of P0.In the components emerging from the decompo-sition of R0, we observe an excessive numberof cut-vertices.
Note that vertices that consistthese components can be of two types: i) for-mer residues, i.e., residues that emerged from thedecomposition of P0, and ii) other vertices ofP0.
Cut-vertices can be of either type.
For eachcomponent, we remove cut-vertices that are nottranslations of the former residues of that com-ponent.
Following this pruning strategy, the de-generacy of excessive cut-vertices does not reap-pear in the subsequent iterations of decompos-ing components generated by new residues, butthe emergence of two giant components was ob-served: One consisting mostly of source type ver-tices and one of target type vertices.
Without go-ing into further details, the algorithm can extendto multiple giant components straightforwardly.For the merging process of the collection D weset ?
= 5000, to avoid the emergence of a giantcomponent.
The sizes of the resulting sub-phrase-tables are shown in Figure 6.
For the ITC algo-rithm we use the smoothing technique discussedin (Dhillon and Guan, 2003) with ?
= 106.For the monolingual graphs, we set ?
= 0.65and discard graphs with more than 20 phrase ver-tices, as they contain mostly noise.
Thus, the sizesof the graphs allow us to use analytical methodsto compute the commute times: For a graph G,we form the transition matrix P , whose entriesP (u, v) are given by eqn.
(7), and the fundamen-8100 102 104 106100101102103104105106ranksizebefore mergingafter mergingFigure 6: Log-log plot of ranked sub-phrase-tables according to their size (number of sourceand target phrases).tal matrix (Grinstead and Snell, 2006; Boley et al2011) Z = (I?P +1piT )?1, where I is the iden-tity matrix, 1 denotes the vector of all ones and piis the vector of stationary probabilities (Aldousand Fill, 2001) which is such that piTP = piTand piT1 = 1 and can be computed as in (Hunter,2000).
The commute time between any vertices uand v in G is then given by (Grinstead and Snell,2006)?
(u, v) = (Z(v, v)?
Z(u, v))/pi(v) ++ (Z(u, u)?
Z(v, u))/pi(u).
(20)For the parameter of eqn.
(4), an appropriatechoice is  = |?
(s)| + 1; for reliable neighbor-hoods, this quantity is insignificant.
POS tags andlemmata are generated with TreeTagger1.Figure 7 depicts the most basic type of graphthat can be extracted from a cluster; it includestwo source phrase vertices a, b, of different syn-tactic information.
Suppose that both a andb are highly reliable with strengths n(a;T ) =n(b;T ) = 40, for some target cluster T .
The re-sulting conditional probabilities adequately repre-sent the proximity of the involved vertices.
Onthe other hand, the range of the co-occurrencecounts is not compatible with that of the strengths.This is because i) there are no phrase vertices withsmall strength in the graph, and ii) eqn.
(9) is es-sentially a comparison between a pair of verticesand the rest of the graph.
To overcome this prob-lem inflation vertices ia and ib of strength 1 withaccompanying feature vertices are introduced to1http://www.ims.uni-stuttgart.de/projekte/corplex/TreeTagger/the graph.
Figure 8 depicts the new graph, wherethe lengths of the edges represent the magnitudeof commute times.
Observe that the quality ofthe probabilities is preserved but the counts areinflated, as required.In general, if a source phrase vertex s has atleast one translation t such that n(s, t) ?
3, then atriplet (is, f(is), g(is)) is added to the graph as inFigure 8.
The inflation vertex is establishes edgeswith all other phrase and inflation vertices in thegraph and weights are computed as in Section 3.1.The pipeline remains the same up to eqn.
(13),where all counts that include inflation vertices areignored.G st =G{c=G{t =s {c=s {r =s}G{ 0 123r =t =G{}G{ 0 124r=c =G{}G{ 0 124r =t =s {}G{ 0 1?r=c=s {}G{ 0 1??
=G?
s { 0 213?
=G?
t =G{{ 0 21??
=G?
c=G{{ 0 21??
=G?
t =s {{ 0 ???
=G?
c =s {{ 0 ?
?Figure 7: Top: A graph with source phrase ver-tices a and b, both of strength 40, with accom-panying distinct POS sequence vertices f(?)
andstem sequence vertices g(?).
Bottom: The result-ing co-occurrence counts and conditional proba-bilities for a.G=s{t } 0 122G=c=t }{t } 0 123G=r =t }{t } 0 123G=c=s }{t } 0 14?G=r =s }{t } 0 14?tsc=t }c=s }r =t }r =s }?
t?
s c=?
t }r =?
t }c=?
s }r =?
s }?
=t ?
s } 0 441??
=t ?
c=t }} 0 4?1??
=t ?
r =t }} 0 4?1??
=t ?
c=s }} 0 31??
=t ?
r =s }} 0 31?Figure 8: The inflated version of Figure 7.94.2 ResultsOur method generates conditional probabilitiesfor any pair chosen from {phrase, POS sequence,stem sequence}, but for this evaluation we restrictourselves to phrase pairs.
For a phrase s, the qual-ity of a paraphrase s?
is assessed byP (s?|s) ?
p(s?|s) + p(f1(s?
)|s) + p(f2(s?
)|s),(21)where f1(s?)
and f2(s?)
denote the POS tag se-quence and stem sequence of s?
respectively.
Allthree summands of eqn.
(21) are computed fromeqn.
(19).
The baseline is given by pivoting (Ban-nard and Callison-Burch, 2005),P (s?|s) =?tp(t|s)p(s?|t), (22)where p(t|s) and p(s?|t) are the phrase-based rel-ative frequencies of the translation model.We select 150 phrases (an equal number forunigrams, bigrams and trigrams), for which weexpect to see paraphrases, and keep the top-10paraphrases for each phrase, ranked by the abovemeasures.
We follow (Kok and Brockett, 2010;Metzler et al 2011) in the evaluation of the ex-tracted paraphrases: Each phrase-paraphrase pairis manually annotated with the following options:0) Different meaning; 1) (i) Same meaning, butpotential replacement of the phrase with the para-phrase in a sentence ruins the grammatical struc-ture of the sentence.
(ii) Tokens of the paraphraseare morphological inflections of the phrase?s to-kens.
2) Samemeaning.
Although useful for SMTpurposes, ?super/substrings of?
are annotated with0 to achieve an objective evaluation.Both methods are evaluated in terms of theMean Expected Precision (MEP) at k; the Ex-pected Precision for each selected phrase s atrank k is computed by Es[p@k] = 1k?ki=1 pi,where pi is the proportion of positive annotationsfor item i.
The desired metric is thus given byMEP@k = 1150?s Es[p@k].
The contributionto pi can be restricted to perfect paraphrases only,which leads to a strict strategy for harvesting para-phrases.
Table 1 summarizes the results of ourevaluation andwe deduce that our method can lead to improve-ments over the baseline.An important accomplishment of our methodis that the distribution of counts n(u, v), (as givenMethodLenient MEP Strict MEP@1 @5 @10 @1 @5 @10Baseline .58 .47 .41 .43 .33 .28Graphs .72 .61 .52 .53 .40 .33Table 1: Mean Expected Precision (MEP) at k un-der lenient and strict evaluation criteria.by eqns.
(15)?
(17)) for all vertices u and v, be-longs to the power-law family (Figure 9).
This isevidence that the monolingual graphs can simu-late the phrase extraction process of a monolin-gual parallel corpus.
Intuitively, we may think ofthe German side of the DE?EN parallel corpus asthe ?English?
approximation to a ?EN?
?EN par-allel corpus, and the monolingual graphs as theword alignment process.100 102 104 106 108100101102103104105rankco?occurrence countFigure 9: Log-log plot of ranked pairs of Englishvertices according to their counts5 Conclusions & Future WorkWe have described a new method that harvestsparaphrases from a bitext, generates artificialco-occurrence counts for any pair chosen from{phrase, POS sequence, stem sequence}, and po-tentially identifies patterns for the syntactic infor-mation of the phrases.
The quality of the para-phrases?
ranked lists outperforms that of a stan-dard baseline.
The quality of the resulting condi-tional probabilities is promising and will be eval-uated implicitly via an application to SMT.This research was funded by the EuropeanCommission through the CoSyne project FP7-ICT- 4-248531.10ReferencesDavid Aldous and James A.
Fill.
2001.
ReversibleMarkov Chains and Random Walks on Graphs.http://www.stat.berkeley.edu/?aldous/RWG/book.htmlIon Androutsopoulos and Prodromos Malakasiotis.2010.
A Survey of Paraphrasing and Textual En-tailment Methods.
Journal of Artificial IntelligenceResearch, 38:135?187.Colin Bannard and Chris Callison-Burch.
2005.
Para-phrasing with Bilingual Parallel Corpora.
Proc.ACL, pp.
597?604.Alain Barrat, Marc Barthlemy, and Alessandro Vespig-nani.
2004.
Modeling the Evolution of WeightedNetworks.
Phys.
Rev.
Lett., 92.Daniel Boley, Gyan Ranjan, and Zhi-Li Zhang.
2011.Commute Times for a Directed Graph using anAsymmetric Laplacian.
Linear Algebra and its Ap-plications, Issue 2, pp.
224?242.Chris Callison-Burch.
2008.
Syntactic Constraintson Paraphrases Extracted from Parallel Corpora.Proc.
EMNLP, pp.
196?205.Chris Callison-Burch, Cameron Fordyce, PhilippKoehn, Christof Monz, and Josh Schroeder.
2007(Meta-) Evaluation of Machine Translation.
Proc.Workshop on Statistical Machine Translation, pp.136?158.Chris Callison-Burch, Philipp Koehn, and Miles Os-borne.
2006 Improved statistical machine trans-lation using paraphrases.
Proc.
HLT/NAACL, pp.17?24.Inderjit S. Dhillon and Yuqiang Guan.
2003.
Informa-tion Theoretic Clustering of Sparse Co-OccurrenceData.
Proc.
IEEE Int?l Conf.
Data Mining, pp.
517?520.Inderjit S. Dhillon, Subramanyam Mallela, and Dhar-mendra S. Modha.
2003.
Information-TheoreticCoclustering.
Proc.
ACM SIGKDD Int?l Conf.Knowledge Discovery and Data Mining, pp.
89?98.William Dolan, Chris Quirk, and Chris Brockett.2004.
Unsupervised construction of large para-phrase corpora: Exploiting massively parallel newssources.
Proc.
COLING, pp.
350-356.Juri Ganitkevitch, Chris Callison-Burch, CourtneyNapoles, and Benjamin Van Durme 2011.
Learn-ing Sentential Paraphrases from Bilingual Paral-lel Corpora for Text-to-Text Generation.
Proc.EMNLP, pp.
1168?1179.Charles Grinstead and Laurie Snell.
2006.
Introduc-tion to Probability.
Second ed., American Mathe-matical Society.Jeffrey J.
Hunter.
2000.
A Survey of Generalized In-verses and their Use in Stochastic Modelling.
Res.Lett.
Inf.
Math.
Sci., Vol.
1, pp.
25?36.Philipp Koehn.
2009.
Statistical Machine Translation.Cambridge University Press, Cambridge, UK.Stanley Kok and Chris Brockett.
2010.
Hitting theRight Paraphrases in Good Time.
Proc.
NAACL,pp.145?153.Roland Kuhn, Boxing Chen, George Foster, and EvanStratford.
2010.
Phrase Clustering for SmoothingTM Probabilities: or, how to Extract Paraphrasesfrom Phrase Tables.
Proc.
COLING, pp.608?616.Nitin Madnani and Bonnie Dorr.
2010.
GeneratingPhrasal and Sentential Paraphrases: A Survey ofData-Driven Methods.
Computational Linguistics,36(3):341?388.Donald Metzler, Eduard Hovy, and ChunliangZhang.
2011.
An Empirical Evaluation of Data-Driven Paraphrase Generation Techniques.
Proc.ACL:Short Papers, pp.
546?551.Takashi Onishi, Masao Utiyama, and Eiichiro Sumita.2010.
Paraphrase Lattice for Statistical MachineTranslation.
Proc.
ACL:Short Papers, pp.
1?5.Delip Rao, David Yarowsky, and Chris Callison-Burch.
2008.
Affinity Measures based on the GraphLaplacian.
Proc.
Textgraphs Workshop on Graph-based Algorithms for NLP at COLING, pp.
41?48.George U. Yule.
1925.
A Mathematical Theory ofEvolution, based on the Conclusions of Dr. J. C.Willis, F.R.S.
Philos.
Trans.
R. Soc.
London, B 213,pp.
21?87.Shiqi Zhao, Haifeng Wang, Ting Liu, and Sheng Li.2008.
Pivot Approach for Extracting ParaphrasePatterns from Bilingual Corpora.
Proc.
ACL, pp.780?788.11
