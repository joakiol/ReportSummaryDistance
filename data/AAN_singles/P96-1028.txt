Evaluating the Portability of Revision Rulesfor Incremental Summary GenerationJacques Robinht tp : / /www.d i .u fpe .br /~ j rjr@di.ufpe.brDepar tamento  de Inform?tica,  Universidade Federal de PernambucoCaixa Postal  7851, Cidade Universit?riaRecife, PE  50732-970 BrazilAbst rac tThis paper presents a quantitative evalu-ation of the portability to the stock mar-ket domain of the revision rule hierarchyused by the system STREAK to incremen-tally generate newswire sports summaries.The evaluation consists of searching a testcorpus of stock market reports for sentencepairs whose (semantic and syntactic) struc-tures respectively match the triggering con-dition and application result of each revi-sion rule.
The results show that at least59% of all rule classes are fully portable,with at least another 7% partially portable.1 In t roduct ionThe project STREAK 1 focuses on the specific issuesinvolved in generating short, newswire style, naturallanguage texts that summarize vast amount of in-put tabular data in their historical context.
A seriesof previous publications presented complementaryaspects of this project: motivating corpus analysisin (Robin and McKeown, 1993), new revision-basedtext generation model in (Robin, 1993), system im-plementation and rule base in (Robin, 1994a) andempirical evaluation of the robustness and scalabil-ity of this new model as compared to the traditionalsingle pass pipeline model in (Robin and McKeown,1995).
The present paper completes this series bydescribing a second, empirical, corpus-based evalu-ation, this time quantifying the portability to an-other domain (the stock market) of the revision rulehierarchy acquired in the sports domain and imple-mented in STREAK.
The goal of this paper is twofold:(1) assessing the generality of this particular rule hi-erarchy and (2) providing a general, semi-automatic1 Surface Text Reviser Expressing AdditionalKnowledge.methodology for evaluating the portability of seman-tic and syntactic knowledge structures used for nat-ural language generation.
The results reveal that atleast 59% of the revision rule hierarchy abstractedfrom the sports domain could also be used to incre-mentally generate the complex sentences observed ina corpus of stock market reports.I start by providing the context of the evalua-tion with a brief overview of STREAK's revision-basedgeneration model, followed by some details about heempirical acquisition of its revision rules from cor-pus data.
I then present he methodology of thisevaluation, followed by a discussion of its quantita-tive results.
Finally, I compare this evaluation withother empirical evaluations in text generation andconclude by discussing future directions.2 An  overv iew o f  STREAKThe project STREAK was initially motivated by an-alyzing a corpus of newswire summaries written byprofessional sportswriters 2.
This analysis revealedfour characteristics of summaries that challenge thecapabilities of previous text generators: concise lin-guistic forms, complex sentences, optional and back-ground facts opportunistically slipped as modifiersof obligatory facts and high paraphrasing power.
Bygreatly increasing the number of content planningand linguistic realization options that the genera-tor must consider, as well as the mutual constraintsamong them, these characteristics make generatingsummaries in a single pass impractical.The example run given in Fig.
1 illustrates howSTREAK overcomes these difficulties.
It first gener-ates a simple draft sentence that contains only theobligatory facts to include in any game report (lo-cation, date, game result and key player statistic).It then applies a series of revision rules 3, each one2This 800,000 word corpus covers a whole NBA (Na-tional Basketball Association) season.3In Fig.
1, the nile used is indicated above ach re-2051.
In i t ia l  draft  (bas ic  sentence pattern):"Dallas, TX - Charles Barkley scored 42 points Sunday as the Phoenix Suns defeated the Dallas Mavericks123-97."2.
Adjunctization of Created into Instrument:"Dallas, TX - Charles Barkley tied a season high wlth 42 points Sunday as the Phoenix Suns defeated theDallas Mavericks 123-97."3.
Coordinative Conjoin of Clause:"Dallas, TX  - Charles Barkley tied a season high with 42 points and Danny  A|nge added 21 Sunday as thePhoenix Suns defeated the Dallas Mavericks 123-97."4.
Absorb of Clause in Clause as Result with Agent Control:"Dallas, TX  - Charles Barkley tied a season high with 42 points and Danny Ainge came oIT the benchto add 21 Sunday as the Phoenix Suns defeated the Dallas Mavericks 123-97."5.
l~ominalization with 0rdinal Adjoin:"Dallas, TX - Charles Barldey tied a season high with 42 points and Danny Ainge came off the bench to add21 Sunday as the Phoenix Suns handed the Dallas Mavericks their 13th straight home defeat 123-97."6.
Adjoin of Classifier to NP:"Dallas, TX - Charles Barkley tied a season high with 42 points and Danny Ainge came off the bench to add21 Sunday as the Phoenix Suns handed the Dallas Mavericks their league worst 13th straight home defeat123-97.
"Figure 1: Complex sentence generation through incremental revisions in STREAKopportunistically adding a new fact 4 that either:?
Complements an Mready included fact (e.g., re-vision of sentence 2 into 3).?
Justifies its relevance by providing its historicalbackground (e.g., revision of sentence 1 into 2).Some of these revisions are non-monotonic,rewording 5 a draft fact to more concisely accommo-date the additional fact (e.g., revision of sentence1 into 2).
Popping additional facts from a prior-ity stack, STREAK stops revising when the summaryvised sentence.4Highlighted in bold in Fig.
1.5In Fig.
1, words that get deleted are italicized andwords that get modified are underlined.Charles Barldey scored 42 points.
Those 42 points equalhis best scoring performance ofthe season.
Danny Aingeis a teammate of Barkley.
They play for the PhoenixSuns.
Ainge is a reserve player.
Yet he scored 21 points.The high scoring performances by Barkley and Aingehelped the Suns defeat he Dallas Mavericks.
The Mav-ericks played on their homecourt in Texas.
They hadalready lost their 12 previous games there.
No otherteam in the league has lost so many games in a row athome.
The final score was 123-97.
The game was playedSunday.Figure 2: Paragraph of simple sentencesparaphrasing a single complex sentencesentence reaches linguistic complexity limits empir-icMly observed in the corpus (e.g., 50 word long orparse tree of depth 10).While STREAK generates only single sentences,those complex sentences convey as much informationas whole paragraphs made of simple sentences, onlyfar more fluently and concisely.
This is illustratedby the 12 sentence paragraph 6 of Fig.
2, which para-phrases entence 6 of Fig.
1.
Because they expressfacts essentially independently of one another, suchmulti-sentence paragraphs are much easier to gener-ate than the complex single sentences generated bySTREAK.3 Acquiring revision rules fromcorpus dataThe rules driving the revision process in STREAKwere acquired by reverse ngineering 7 about 300 cor-pus sentences.
These sentences were initially classi-fied in terms of:?
The combination of domain concepts they ex-pressed.?
The thematic role and top-level syntactic ate-gory used for each of these concepts.6This paragraph was not generated by STREAK, it isshown here only for contrastive purposes.v i.e., analyzing how they could be incrementally gen-erated through gradual revisions.206The resulting classes, called realization patterns,abstract the mapping from semantic to syntacticstructure by factoring out lexical material and syn-tactic details.
Two examples of realization patternsare given in Fig.
3.
Realization patterns were thengrouped into surface decrement pairs consisting of:?
A more complex pattern (called the target pat-tern).?
A simpler pattern (called the source pattern)that is structurally the closest o the target pat-tern among patterns with one less concept s .The structural transformations from source to tar-get pattern in each surface decrement pair werethen hierarchically classified, resulting in the revi-sion rule hierarchy shown in Fig.
4-10.
For ex-ample, the surface decrement pair < R~, R 1 >,shown in Fig.
3, is one of the pairs from whichthe revision rule Adjunctization of Range intoInstrument, shown in Fig.
10 was abstracted.It involves displacing the Range argument of thesource clause as an Instrument adjunct to accom-modate a new verb and its argument.
This revi-sion rule is a sibling of the rule Adjunctization ofCreated into Instrument used to revise sentencei into 2 in STREAK'S run shown in Fig.
1 (where theCreated argument role "42 points" of the verb "toscore" in I becomes an Instrument adjunct in 2).The bottom level of the revision rule hierarchyspecifies the side revisions that are orthogonal andsometimes accompany the restructuring revisionsdiscussed up to this point.
Side revisions do notmake the draft more informative, but instead im-prove its style, conciseness and unambiguity.
For ex-ample, when STREAK revises sentence (3) into (4) inthe example run of Fig.
1, the Agent of the absorbedclause "Danny Ainge added 21 points" becomes con-trolled by the new embedding clause "Danny Aingecame off the bench" to avoid the verbose form:?
"Danny Ainge came off the bench for Danny Aingeto add 21 points".4 Eva luat ion  methodo logyIn the spectrum of possible evaluations, the eval-uation presented in this paper is characterized asfollows:?
Its object is the revision rule hierarchy acquiredfrom the sports summary corpus.
It thus doesnot directly evaluate the output of STREAK, butrather the special knowledge structures requiredby its underlying revision-based model.s i.e., the source pattern expresses the same conceptcombination than the target pattern minus one concept.The particular property of this revision rule hi-erarchy that is evaluated is cross-domain porta-bility: how much of it could be re-used to gener-ate summaries in another domain, namely thestock market?The basis for this evaluation is corpus data 9.The original sports summary corpus from whichthe revision rules were acquired is used as the'training' (or acquisition) corpus and a cor-pus of stock market reports taken from severalnewswires is used as the 'test' corpus.
This testcorpus comprises over 18,000 sentences.The evaluation procedure is quantitative, mea-suring percentages ofrevision rules whose targetand source realization patterns are observablein the test corpus.
It is also semi-automatedthrough the use of the corpus search tool CREP(Duford, 1993) (as explained below).Basic pr inc ip le  As explained in section 3, a re-vision rule is associated with a list of surface decre-ment pairs, each one consisting of:A source pattern whose content and linguisticform match the triggering conditions of the rule(e.g., R~ in Fig.
3 for the rule Ad junct i za t ionof Range into Instrument) .A target pattern whose content and linguis-tic form can be derived from the source pat-tern by applying the rule (e.g., R 2 in Fig.
3for the rule Adjunctization of Range intoInstrument).This list of decrement pairs can thus be used asthe signature of the revision rule to detect its usagein the test corpus.
The needed evidence is the simul-taneous presence of two test corpus sentences 1?, eachone respectively matching the source and target pat-terns of at least one element in this list.
Requiringoccurrence of the source pattern in the test corpus isnecessary for the computation of conservative porta-bility estimates: while it may seem that one targetpattern alone is enough evidence, without the pres-ence of the corresponding source pattern, one cannotrule out the possibility that, in the test domain, thistarget pattern is either a basic pattern or derivedfrom another source pattern using another evisionrule.9Only the corpus analysis was performed for both do-mains.
The implementation was not actually ported tothe stock market domain.1?In general, not from the same report.207Realization pattern R~:?
Expresses the concept pair:<game-result(winner,loser,score), str ak(winner,aspect,result-type,lengt h) >.?
Is a target pattern of the revision rule Adjunctization of Range into Instrument.winner aspect l type l streak length \[agent action affected/located locationproper verb NP PPdet \ ]  classifier I noun prep \]Utah extended its win streak to 6 games withBoston stretching its winning spree to 9 outings with\[ score \]game-result \[ loserinstrumentPPNPL_J n u m b e ~ J  PPa 99-84 triumph - over enl3-d-~V-~a 118-94 rout of UtahRealization pattern R~:* Expresses the single concept <game-result(winner,loser,score)>.?
Is a source pattern of the revision rule Adjunctization of Range into Instrument.?
Is a surface decrement of pattern R~ above.winneragent actionproper support-verbI score \] game-result I loserrangeNPdet I number I nounChicago claimed a YOrlando recorded a 101-95  triumphI PPover New Jerseyagainst New YorkFigure 3: Realization pattern examplesPar t ia l l y  automat ing  the  eva luat ion  The soft-ware tool CREP 11 was  developed to partially auto-mate detection of realization patterns in a text cor-pus.
The basic idea behind CREP is to approximatea realization pattern by a regular expression whoseterminals are words or parts-of-speech tags (POS-tags).
CR~.P will then automatically retrieve the cor-pus sentences matching those expressions.
For ex-ample, the CREP expression C~1 below approximatesthe realization pattern R~ shown in Fig.
3:(61) TEAM Of (claimed\[recorded)@VBD I- SCORE O=(victory\[triumph)@NN O= (over\[against)@IN O= TEAMIn the expression above, 'VBD', 'NN' and 'IN' are thePOS-tags for past verb, singular noun and preposi-tion (respectively), and the sub-expressions 'TEAH'and 'SCORE' (whose recursive definitions are notshown here) match the team names and possible fi-nal scores (respectively) in the NBA.
The CREP op-erators 'N=' and 'N-' (N being an arbitrary integer)respectively specify exact and minimal distance ofN words, and ' l '  encodes disjunction.l lcREP was implemented (on top of FLEX, GNUS' ver-sion of LEX) and to a large extent also designed by Du-ford.
It uses Ken Church's POS tagger.Because a realization pattern abstracts away fromlexical items to capture the mapping from conceptsto syntactic structure, approximating such a patternby a regular expression of words and POS-tags in-volves encoding each concept of the pattern by thedisjunction of its alternative lexicalizations.
In agiven domain, there are therefore two sources of in-accuracy for such an approximation:?
Lexical ambiguity resulting in false positives byover-generalization.?
Incomplete vocabulary resulting in false nega-tives by over-specialization 12.Lexical ambiguities can be alleviated by writingmore context-sensitive expressions.
The vocabu-lary can be acquired through additional exploratoryCREP runs with expressions containing wild-cardsfor some concept slots.
Although automated corpussearch using CREP expressions considerably speeds-up corpus analysis, manual intervention remains12This is the case for example of C1 above, which is asimplification of the actual expression that was used tosearch occurrences of R~ in the test corpus (e.g., Cz ismissing "win" and "rout" as alternatives for "victory").208necessary to filter out incorrect matches resultingfrom imperfect approximations.Cross -domain  d iscrepanc ies  Basic similaritiesbetween the finance and sports domains form thebasis for the portability of the revision rules.
Inboth domains, the core facts reported are statis-tics compiled within a standard temporal unit (insports, one ballgame; in finance, one stock marketsession) together with streaks 13 and records com-piled across several such units.
This correspondenceis, however, imperfect.
Consequently, before theycan track down usage of a revision rule in the test do-main, the CREP expressions approximating the sig-nature of the rule in the acquisition domain must beadjusted for cross-domain discrepancies to preventfalse negatives.
Two major types of adjustments arenecessary: lexical and thematic.Lexical adjustments handle cases of partial mis-match between the respective vocabularies used tolexicalize matching conceptual structures in each do-main.
(e.g.,, the verb "to rebound from" expressesthe interruption of a streak in the stock market do-main, while in the basketball domain "to break" or"to snap" are preferred since "to rebound" is used toexpress a different concept).Thematic adjustments handle cases of partial dif-ferences between corresponding conceptual struc-tures in the acquisition and test domains.
For ex-ample, while in sports garae- resu l t  involves an-tagonistic teams, its financial domain counterpartsession-result concerns only a single indicator.Consequently, the sub-expression for the loser  rolein the example CI:tEP expression (~1 shown before,and which approximates realization pattern /~ forgame-resull ;  (shown in Fig.
3), needs to becomeoptional in order to also approximate patterns forsession-resul~.
This is done using the CREP op-erator ?
as shown below:(C1/): TEAM O= (claimedlrecorded)@VBD 1-SCORE O= (victoryltriumph) @NN O=( (over\] against)@IN 0= TEAM) ?Note that it is the CREP expressions used to auto-matically retrieve test corpus sentence pairs attest-ing usage of a revision rule that require this typeof adjustment and not the revision rule itself 14.
Forexample, the Adjoin of Frequency PP to  Clauserevision rule attaches a streak to a sess ion- resu l tclause without l oser  role in exactly the same waythan it attaches a streak to a game-resu l t  with13i.e., series of events with similar outcome.14Some revision rules do require adjustment, but ofanother type (cfl Sect.
5).l oser  role.
This is illustrated by the two corpussentences below:P~: "The Chicago Bulls beat the Phoenix Suns 9991 for  the i r  3rd s t ra ight  win"pt: "The Amex Market Value Index inched up 0.16to 481.94 for its s ix th  s t ra ight  advance"Deta i led  eva luat ion  procedure  The overallprocedure to test portability of a revision rule con-sists of considering the surface decrement pairs in therule signature in order, and repeating the followingsteps:1.
Write a CREP expression for the acquisition tar-get pattern.2.
Iteratively delete, replace or generalize sub-expressions in the CREP expression - to glossover thematic and lexical discrepancies betweenthe acquisition and test domains, and preventfalse negatives - until it matches ome test cor-pus sentence(s).3.
Post-edit he file containing these matched sen-tences.
If it contains only false positives of thesought target pattern, go back to step 2.
Oth-erwise, proceed to  step 4.4.
Repeat step (1-3) with the source pattern of thepair under consideration.
If a valid match canalso be found for this source pattern, stop: therevision rule is portable.
Otherwise, start overfrom step 1 with the next surface decrement pairin the revision rule signature.
If there is no nextpair left, stop: the revision rule is considerednon-portable.Steps (2,3) constitute a general, generate-and-testprocedure to detect realization patterns usage in acorpus 15.
Changing one CKEP sub-expression mayresult in going from too specific an expression withno valid match to either: (1) a well-adjusted ex-pression with a valid match, (2) still too specific anexpression with no valid match, or (3) already toogeneral an expression with too many matches to bemanually post-edited.It is in fact always possible to write more context-sensitive expressions, to manually edit larger no-match files, or even to consider larger test corpora inthe hope of finding a match.
At some point however,one has to estimate, guided by the results of previ-ous runs, that the likelihood of finding a match is too15And since most generators ely on knowledge struc-tures equivalent to realization patterns, this procedurecan probably be adapted to semi-automatically evaluatethe portability of virtually any corpus-based generator.209small to justify the cost of further attempts.
This iswhy the last line in the algorithm reads "considerednon-portable" as opposed to "non-portable".
Thealgorithm guarantees the validity of positive (i.e.,portable) results only.
Therefore, the figures pre-sented in the next section constitute in fact a lower-bound estimate of the actual revision rule portability.5 Eva luat ion  resu l t sThe results of the evaluation are summarized inFig.
4-10.
They show the revision rule hierarchy,with portable classes highlighted in bold.
The fre-quency of occurrence of each rule in the acquisitioncorpus is given below the leaves of the hierarchy.Some rules are same-concept portable: they areused to attach corresponding concepts in each do-main (e.g., Adjoin of Frequency PP to Clause,as explained in Sect.
4).
They could be re-used "asis" in the financial domain.
Other rules, however,are only different-concept portable: they are used toattach altogether different concepts in each domain.This is the case for example of Adjoin Finite TimeClause to Clause, as illustrated by the two corpussentences below, where the added temporal adjunct(in bold) conveys a streak in the sports sentence, buta complementary statistics in the financial one:T~: "to lead Utah to a 119-89 trouncing of Denveras the Jazz defeated the Nuggets for the 12thstraight ime at home.
"T~: "Volume amounted to a solid 349 million sharesas advances out-paced eclines 299 to 218.
".For different-concept portable rules, the left-handside field specifying the concepts incorporable to thedraft using this rule will need to be changed whenporting the rule to the stock market domain.
InFig.
4-10, the arcs leading same-concept ortableclasses are full and thick, those leading to different-concept portable classes are dotted, and those lead-ing to a non-portable classes are full but thin.59% of all revision rule classes turned out to besame-concept ortable, with another 7% different-concept portable.
Remarkably, all eight top-levelclasses identified in the sports domain had instancessame-concept ortable to the financial domain, eventhose involving the most complex non-monotonic re-visions, or those with only a few instances in thesports corpus.
Among the bottom-level classes thatdistinguish between revision rule applications inveryspecific semantic and syntactic ontexts, 42% aresame-concept ortable with another 10% different-concept portable.
Finally, the correlation betweenhigh usage frequency in the acquisition corpus andportability to the test corpus is not statistically sig-nificant (i.e., the hypothesis that the more commona rule, the more likely it is to be portable could notbe confirmed on the analyzed sample).
See (Robin,1994b) for further details on the evMuation results.There are two main stumbling blocks to porta-bility: thematic role mismatch and side revisions.Thematic role mismatches are cases where the se-mantic label or syntactic sub-category of a con-stituent added or displaced by the rule differ ineach domain (e.g., Adjunct izat ion of Createdinto Instrument vs.
Adjoin of Affected intoInstrument).
They push portability from 92% downto 71%.
Their effect could be reduced by allowingSTREAK'S reviser to manipulate the draft down tothe surface syntactic role level (e.g., in both cor-pora Created and Affected surface as object).
Cur-rently, the reviser stops at the thematic role level toallow STREAK to take full advantage of the syntac-tic processing front-end SURGE (Elhadad and Robin,1996), which accepts such thematic structures as in-put.Accompanying side revisions push portabilityfrom 71% to 52%.
This suggests that the design ofSTREAK could be improved by keeping side revisionsseparate from re-structuring revisions and interleav-ing the applications of the two.
Currently, they areintegrated together at the bottom of the revision rulehierarchy.6 Re la ted  workApart from STREAK, only three generation projectsfeature an empirical and quantitative valuation:ANA (Kukich, 1983), KNIGHT (Lester, 1993) and IM-AGENE (Van der Linden, 1993).ANA generates short, newswire style summaries ofthe daily fluctuations ofseveral stock market indexesfrom half-hourly updates of their values.
For eval-uation, Kukich measures both the conceptual andlinguistic (lexical and syntactic) coverages of ANAby comparing the number of concepts and realiza-tion patterns identified uring a corpus analysis withthose actually implemented in the system.KNIGHT generates natural anguage concept defi-nitions from a large biological knowledge base, rely-ing on SURGE for syntactic realization.
For evalua-tion, Lester performs a Turing test in which a panelof human judges rates 120 sample definitions by as-signing grades (from A to F) for:?
Semantic accuracy (defined as "Is the definitionadequate, providing correct information and fo-cusing on what's important?"
in the instruc-tions provided to the judges).?
Stylistic accuracy (defined as "Does the defini-tion use good prose and is the information it210Monotonic Revisions Non-monotonic RevisionsA d l ~ j o i n  Recast Adjunctization Nomlnalization Demotion PromotionFigure 4: Revision rule hierarchy: top-levelsAbsorbof NP ofclauseinsidc-NP ~?~'~ ....... ~d~lause inslde!lanseas querier i t  ...... " ? "
' ~  " ' ' "~  - as- ns rument as-affected-apposition as-mean as-co-event1 2 l 1 3Figure 5: Absorb revision rule sub-hierarchyRecastof NP of clausefrom classifier from location from rangeto qualifier to instrument to time to instrument10 9 1 1Nominalization+ordinal +ordinal +ordinal+classifier +qualifier1 2 2Figure 6: Recast and Nominalize revision rule sub-hierarchyDemotionf r o m ~ f f e c t e d\] to qualifleJ(affeeted) I to score(co-event) to determiner(affected)1 2 1Coordination Promotionsimple w/Adjunctization1 1Figure 7: Demotion and Promotion revision rule sub-hierarchy211Adjointo NT to clause1 25 aSfi use4~non.fi?
.
?
?
rel ve-cla rote clauseabridged /~de le ted  ~ +reorder ref ref full abridged2ref/''- ref~ 3 ref 9 2 2 10f r e q u e n ~ n tI I i IPP non-finite finite non-finiteda e clause clause 2 ,full I ~ abridged abridg~  deletedrefl~ ref re f~ ref J \ ref13 3 1 12 4Figure 8: Adjoin revision rule sub-hierarchyConjoinNPsby apposition/ % % % % %abridged ~de le tedre~ f ref I \ ref15 5 4 1clausesI by coordination by coordination"+scope full ~bridged full Aabridgedl~mark refl ~ ref refl ~ ref1 2 1 5 1 1Figure 9: Conjoin revision rule sub-hierarchyinto opposition into instrumentof affected of range of created of location1 ~ +agent 1 \demotionabridged / full,deleted \ full abridged ref// refJ\ ref \ ref/~ ref7 27 3 1 14 5 4Figure 10: Adjunctization revision rule sub-hierarchy212ANAKNIGHTObject of Evaluationknowledge structuresoutput textIMAGENE output textknowledge structuresSTREAKEvaluated Propertiesconceptual coveragelinguistic coveragesemantic accuracystylistic accuracystylistic accuracystylistic robustnesscross-domain portabi l i tysame-domain robustnesssame-domain scalabilityEmpirical Basistextual corpushuman judgestextual corpustextual corpusEvaluation Proceduremanua lmal lualmanua lsemi-automaticFigure 11: Empirical evaluations in language generationconveys well organized" in the instructions pro-vided to the judges).The judges did not know that half the definitionswere computer-generated while the other half werewritten by four human domain experts.
Impres-sively, the results show that:?
With respect o semantic accuracy, the humanjudges could not tell KNIGHT apart from the hu-man writers.
* While as a group, humans got statistically sig-nificantly better grades for stylistic accuracythan KNIGHT, the best human writer was single-handly responsible for this difference.IMAGENE generates instructions on how to oper-ate household evices relying on NIGEL (Mann andMatthiessen, 1983) for syntactic realization.
Theimplementation focuses on a very limited aspect oftext generation: the realization of purpose relations.Taking as input the description of a pair <operation,purpose of the operation>, augmented by a set offeatures simulating the communicative context ofgeneration, IMAGENE selects, among the many real-izations of purpose generable by NIGEL (e.g., frontedto-infinitive clause vs. trailing for-gerund clauses),the one that is most appropriate for the simulatedcontext (e.g., in the context of several operationssharing the same purpose, the latter is preferentiallyexpressed before those actions than after them).
IM-AGENE's contextual preference rules were abstractedby analyzing an acquisition corpus of about 300 pur-pose clauses from cordless telephone manuMs.
Forevaluation, Van der Linden compares the purposerealizations picked by IMAGENE to the one in thecorresponding corpus text, first on the acquisitioncorpus and then on a test corpus of about 300 otherpurpose clauses from manuals for other devices thancordless telephones (ranging from clock radio to au-tomobile).
The results show a 71% match on theacquisition corpus 16 and a 52% match on the testcorpus.The table of Fig.
11 summarizes the differenceon both goal and methodology between the eval-uations carried out in the projects ANA, KNIGHT,IMAGENE and STREAK.
In terms of goals, whileKukich and Lester evaluate the coverage or accu-racy of a particular implementation, I instead fo-cus on three properties inherent to the use of therevision-based generation model underlying STREAK:robustness (how much of other text samples from thesame domain can be generated without acquiringnew knowledge?)
and scalability (how much morenew knowledge is needed to fully cover these othersamples?)
discussed in (Robin and McKeown, 1995),and portability to another domain in the present pa-per.
Van der Linden does a little bit of both by firstmeasuring the stylistic accuracy of his system for avery restricted sub-domain, and then measuring howit degrades for a more general domain.In itself, measuring the accuracy and coverage ofa particular implementation i  the sub-domain forwhich it was designed brings little insights aboutwhat generation approach should be adopted in fu-ture work.
Indeed, even a system using mere cannedtext can be very accurate and attain substantial cov-erage if enough hand-coding effort is put into it.However, all this effort will have to be entirely du-plicated each time the system is scaled up or portedto a new domain.
Measuring how much of this effortduplication can be avoided when relying on revision-based generation was the very object of the threeevaluations carried in the STREAK project.16This imperfect match on the acquisition corpusseems to result from the heuristic nature of IMAGENE'sstylistic preferences: individually, none of them needs toapply to the whole corpus.213In terms of methodology, the main originality ofthese three evaluations i the use of CREP to par-tially automate reverse engineering of corpus sen-tences.
Beyond evaluation, CREP is a simple, butgeneral and very handy tool that should prove use-ful to speed-up a wide range of corpora analyses.7 Conc lus ionIn this paper, I presented a quantitative evaluationof the portability to the stock market domain of therevision rule hierarchy used by the system STREAK toincrementally generate newswire sports summaries.The evaluation procedure consists of searching a testcorpus of stock market reports for sentence pairswhose (semantic and syntactic) structures respec-tively match the triggering condition and applicationresult of each revision rule.
The results how that atleast 59% of all rule classes are fully portable, withat least another 7% partially portable.Since the sports domain is not closer to the finan-cial domain than to other quantitative domains uchas meteorology, demography, business auditing orcomputer surveillance, these results are very encour-aging with respect o the general cross-domain re-usability potential of the knowledge structures usedin revision-based generation.
However, the presentevaluation concerned only one type of such knowl-edge structures: revision rules.
In future work, sim-ilar evaluations will be needed for the other types ofknowledge structures: content selection rules, phraseplanning rules and lexicalization rules.AcknowledgementsMany thanks to Kathy McKeown for stressing the im-portance of empirically evaluating STREAK.
The re-search presented in this paper is currently supported byCNPq (Brazilian Research Council) under post-doctoralresearch grant 150130-95.3.
It started out while I wasat Columbia University supported by of a joint grantfrom the Office of Naval Research, by the AdvancedResearch Projects Agency under contract N00014-89-J-1782, by National Science Foundation Grants IRT-84-51438 and GER-90-2406, and by the New York StateScience and Technology Foundation under this auspicesof the Columbia University CAT in High PerformanceComputing and Communications i  Health Care, a NewYork State Center for Advanced Technology.Re ferencesDuford, D. 1993. caEP: a regular expression-matching textual corpus tool.
Technical ReportCU-CS-005-93.
Computer Science Department,Columbia University, New York.Elhadad, M. and Robin, J.
1996.
An overviewof SURGE: a re-usable comprehensive syntacticrealization component.
Technical Report 96-03.Computer Science and Mathematics Department,Ben Gurion University, Beer Sheva, Israel.Kukich, K. 1983.
Knowledge-based report genera-tion: a knowledge engineering approach to naturallanguage report generation.
PhD.
Thesis.
Depart-ment of Information Science.
University of Pitts-burgh.Lester, J.C. 1993.
Generating natural languageexplanations from large-scale knowledge bases.PhD.
Thesis.
Computer Science Department,University of Texas at Austin.Mann, W.C. and Matthiessen, C. M. 1983.
NIGEL:a systemic grammar for text generation.
ResearchReport RR-83-105.
ISI.
Marina Del Rey, CA.Robin, J. and McKeown, K.R.
1993.
Corpus anal-ysis for revision-based generation of complex sen-tences.
In Proceedings of the 11th National Con-ference on Artificial Intelligence, Washington DC.
(AAAI'93).Robin, J. and McKeown, K.R.
1995.
Empiricallydesigning and evaluating a new revision-basedmodel for summary generation.
Artificial Intel-ligence.
Vol 85.
Special Issue on Empirical Meth-ods.
North-Holland.Robin, J.
1993.
A revision-based generation archi-tecture for reporting facts in their historical con-text.
In New Concepts in Natural Language Gen-eration: Planning, Realization and System.
Ho-racek, H. and Zock, M., Eds.
Frances Pinter.Robin, J.
1994a.
Automatic generation and revisionof natural anguage summaries providing histori-cal background In Proceedings of the 11th Brazil-ian Symposium on Artificial Intelligence, Fort-aleza, Brazil.
(SBIA'94).Robin, J.
1994b.
Revision-based generation ofnatu-ral language summaries providing historical back-ground: corpus-based analysis, design, implemen-tation and evaluation.
PhD.
Thesis.
Availableas Technical Report CU-CS-034-94.
ComputerScience Department, Columbia University, NewYork.Van der Linden, K. and Martin, J.H.
1995.
Ex-pressing rhetorical relations in instructional texts:a case study of the purpose relation.
Computa-tional Linguistics, 21(1).
MIT Press.214
