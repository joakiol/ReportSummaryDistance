Proceedings of the 5th Workshop on Important Unresolved Matters, pages 65?72,Prague, Czech Republic, June 2007. c?2007 Association for Computational LinguisticsArabic Tokenization SystemMohammed A. AttiaSchool of Informatics / The University of Manchester, PO Box88, Sackville Street, Manchester M60 1QD, UKmohammed.attia@postgrad.manchester.ac.ukAbstractTokenization is a necessary and non-trivialstep in natural language processing.
In thecase of Arabic, where a single word cancomprise up to four independent tokens,morphological knowledge needs to be in-corporated into the tokenizer.
In this paperwe describe a rule-based tokenizer thathandles tokenization as a full-roundedprocess with a preprocessing stage (whitespace normalizer), and a post-processingstage (token filter).
We also show how ithandles multiword expressions, and howambiguity is resolved.1 IntroductionTokenization is a non-trivial problem as it is?closely related to the morphological analysis?
(Chanod and Tapanainen 1994).
This is even morethe case with languages with rich and complexmorphology such as Arabic.
The function of a to-kenizer is to split a running text into tokens, so thatthey can be fed into a morphological transducer orPOS tagger for further processing.
The tokenizer isresponsible for defining word boundaries, demar-cating clitics, multiword expressions, abbreviationsand numbers.Clitics are syntactic units that do not have freeforms but are instead attached to other words.
De-ciding whether a morpheme is an affix or a cliticcan be confusing.
However, we can generally saythat affixes carry morpho-syntactic features (suchas tense, person, gender or number), while cliticsserve syntactic functions (such as negation, defini-tion, conjunction or preposition) that would other-wise be served by an independent lexical item.Therefore tokenization is a crucial step for a syn-tactic parser that needs to build a tree from syntac-tic units.
An example of clitics in English is thegenitive suffix ??s?
in the student?s book.Arabic clitics, however, are not as easily recog-nizable.
Clitics use the same alphabet as that ofwords with no demarcating mark as the Englishapostrophe, and they can be concatenated one afterthe other.
Without sufficient morphological knowl-edge, it is impossible to detect and mark clitics.
Inthis paper we will show different levels of imple-mentation of the Arabic tokenizer, according to thelevels of linguistic depth involved.Arabic Tokenization has been described in vari-ous researches and implemented in many solutionsas it is a required preliminary stage for furtherprocessing.
These solutions include morphologicalanalysis (Beesley 2001; Buckwalter 2002), diacri-tization (Nelken and Shieber 2005), InformationRetrieval (Larkey and Connell 2002), and POSTagging (Diab et al2004; Habash and Rambow2005).
None of these projects, however, show howmultiword expressions are treated, or how ambigu-ity is filtered out.In our research, tokenization is handled in arule-based system as an independent process.
Weshow how the tokenizer interacts with other trans-ducers, and how multiword expressions are identi-fied and delimited.
We also show how incorrecttokenizations are filtered out, and how undesiredtokenizations are marked.
All tools in this researchare developed in Finite State Technology (Beesleyand Karttunen 2003).
These tools have been devel-oped to serve an Arabic Lexical Functional Gram-mar parser using XLE (Xerox Linguistics Envi-ronment) platform as part of the ParGram Project(Butt et al2002).652 Arabic TokensA token is the minimal syntactic unit; it can be aword, a part of a word (or a clitic), a multiwordexpression, or a punctuation mark.
A tokenizerneeds to know a list of all word boundaries, suchas white spaces and punctuation marks, and alsoinformation about the token boundaries insidewords when a word is composed of a stem and cli-tics.
Throughout this research full form words, i.e.stems with or without clitics, as well as numberswill be termed main tokens.
All main tokens aredelimited either by a white space or a punctuationmark.
Full form words can then be divided intosub-tokens, where clitics and stems are separated.2.1 Main TokensA tokenizer relies mainly on white spaces andpunctuation marks as delimiters of word bounda-ries (or main tokens).
Additional punctuationmarks are used in Arabic such as the comma ??
?,question mark ???
and semicolon ???.
Numbers arealso considered as main tokens.
A few Arab coun-tries use the Arabic numerals as in English, whilemost Arab countries use the Hindi numerals suchas ?2?
(2) and ?3?
(3).
Therefore a list of allpunctuation marks and number characters must befed to the system to allow it to demarcate maintokens in the text.2.2 Sub-TokensArabic morphotactics allow words to be pre-fixed or suffixed with clitics (Attia 2006b).
Cliticsthemselves can be concatenated one after the other.Furthermore, clitics undergo assimilation withword stems and with each other, which makesthem even harder to handle in any superficial way.A verb can comprise up four sub-tokens (a con-junction, a complementizer, a verb stem and anobject pronoun) as illustrated by Figure 1.Figure 1: Possible sub-tokens in Arabic verbsSimilarly a noun can comprise up to four sub-tokens.
Although Figure 2 shows five sub-tokensbut we must note that the definite article and thegenitive pronoun are mutually exclusive.Figure 2: Possible sub-tokens in Arabic nounsMoreover there are various rules that govern thecombination of words with affixes and clitics.These rules are called grammar-lexis specifications(Abb?s et al2004; Dichy 2001; Dichy and Fargaly2003).
An example of these specifications is a rulethat states that adjectives and proper nouns do notcombine with possessive pronouns.3 Development in Finite State TechnologyFinite state technology has successfully beenused in developing morphologies and text process-ing tools for many natural languages, includingSemitic languages.
We will explain briefly howfinite state technology works, then we will proceedinto showing how different tokenization modelsare implemented.
(1) LEXICON Proclitical@U.Def.On@ Root;Root;LEXICON Rootkitab Enclitic;LEXICON Suffixan Enclitic;Enclitic;LEXICON Enclitichi@U.Def.Off@ #;In a standard finite state system, lexical entriesalong with all possible affixes and clitics are en-coded in the lexc language which is a right recur-sive phrase structure grammar (Beesley and Kart-tunen 2003).
A lexc file contains a number of lexi-cons connected through what is known as ?con-tinuation classes?
which determine the path ofconcatenation.
In example (1) above the lexiconProclitic has a lexical form al, which is linked to a66continuation class named Root.
This means that theforms in Root will be appended to the right of al.The lexicon Proclitic also has an empty string,which means that Proclitic itself is optional andthat the path can proceed without it.
The bulk of alllexical entries are presumably listed under Root inthe example.Sometimes an affix or a clitic requires or for-bids the existence of another affix or clitic.
This iswhat is termed ?long distance dependencies?
(Beesley and Karttunen 2003).
So Flag Diacriticsare introduced to serve as filters on possible con-catenations to a stem.
As we want to prevent Pro-clitic and Enclitic from co-occurring, for the defi-nite article and the possessive pronoun are mutu-ally excusive, we add a Flag Diacritic to each ofthem with the same feature name ?U.Def?, butwith different value ?On/Off?, as shown in (1)above.
At the end we have a transducer with a bi-nary relation between two sets of strings: the lowerlanguage that contains the surface forms, and theupper language that contains the analysis, as shownin (2) for the noun ??????
kitaban (two books).
(2) Lower Language: ?????
?Upper Language: ???
?+noun+dual+sg4 Tokenization SolutionsThere are different levels at which an Arabictokenizer can be developed, depending on thedepth of the linguistic analysis involved.
Duringour work with the Arabic grammar we developedthree different solutions, or three models, for Ara-bic tokenization.
These models vary greatly in theirrobustness, compliance with the concept of modu-larity, and the ability to avoid unnecessary ambi-guities.The tokenizer relies on white spaces and punc-tuation marks to demarcate main tokens.
In demar-cating sub-tokens, however, the tokenizer needsmore morphological information.
This informationis provided either deterministically by a morpho-logical transducer, or indeterministically by a to-ken guesser.
Eventually both main tokens and sub-tokens are marked by the same token boundary,which is the sign ?@?
throughout this paper.
Theclassification into main and sub-tokens is a con-ceptual idea that helps in assigning the task ofidentification to different components.Identifying main tokens is considered a straight-forward process that looks for white spaces andpunctuation marks and divides the text accord-ingly.
No further details of main tokens are givenbeyond this point.
The three models described be-low are different ways to identify and divide sub-tokens, or clitics and stems within a full formword.4.1 Model 1: Tokenization Combined withMorphological AnalysisIn this implementation the tokenizer and themorphological analyzer are one and the same.
Asingle transducer provides both morphologicalanalysis and tokenization.
Examples of the token-izer/analyser output are shown in (3).
The ?+?
signprecedes morphological features, while the ?
@?sign indicates token boundaries.
(3) ??????
(waliyashkur: and to thank)?+conj@?+comp@+verb+pres+sg???
@This sort of implementation is the most linguis-tically motivated.
This is also the most commonform of implementation for Arabic tokenization(Habash and Rambow 2005).
However, it violatesthe design concept of modularity which requiressystems to have separate modules for undertakingseparate tasks.
For a syntactic parser that requiresthe existence of a tokenizer besides a morphologi-cal analyzer, this implementation is not workable,and either Model 2 or Model 3 is used instead.4.2 Model 2: Tokenization GuesserIn this model tokenization is separated frommorphological analysis.
The tokenizer only detectsand demarcates clitic boundaries.
Yet informationon what may constitute a clitic is still needed.
Thisis why two additional components are required: aclitics guesser to be integrated with the tokenizer,and a clitics transducer to be integrated with themorphological transducer.Clitics Guesser.
We developed a guesser forArabic words with all possible clitics and all possi-ble assimilations.
Please refer to (Beesley andKarttunen 2003) on how to create a basic guesser.The core idea of a guesser is to assume that a stemis composed of any arbitrary sequence of Arabicalphabets, and this stem can be prefixed or/andsuffixed with a limited set of tokens.
This guesseris then used by the tokenizer to mark clitic bounda-67ries.
Due to the nondeterministic nature of aguesser, there will be increased tokenization ambi-guities.
(4) ??????
(and to the man)???@??@?@?@?????@?@?@?????@?@??????
@Clitics Transducer.
We must note that Arabicclitics do not occur individually in natural texts.They are always attached to words.
Therefore aspecialized small-scale morphological transducer isneeded to handle these newly separated forms.
Wedeveloped a lexc transducer for clitics only, treat-ing them as separate words.
The purpose of thistransducer is to provide analysis for morphemesthat do not occur independently.
(5) ?+conj?+prep?
?+art+defThis small-scale specialized transducer is thenunioned (or integrated) with the main morphologi-cal transducer.
Before making the union it is nec-essary to remove all paths that contain any cliticsin the main morphological transducer to eliminateredundancies.In our opinion this is the best model, the advan-tages are robustness as it is able to deal with anywords whether they are known to the morphologi-cal transducer or not, and abiding by the concept ofmodularity as it separates the process of tokeniza-tion from morphological analysis.There are disadvantages, however, for thismodel, and among them is that the morphologicalanalyzer and the syntactic parser have to deal withincreased tokenization ambiguities.
The tokenizeris highly non-deterministic as it depends on aguesser which, by definition, is non-deterministic.For a simple sentence of three words, we are facedwith eight different tokenization solutions.
None-theless, this can be handled as explained in subsec-tion 5.1 on discarding spurious ambiguities.4.3 Model 3: Tokenization Dependent on theMorphological AnalyserIn the above solution, the tokenizer defines thepossible Arabic stem as any arbitrary sequence ofArabic letters.
In this solution, however, the wordstem is not guessed, but taken as a list of actualwords.
A possible word in the tokenizer in thismodel is any word found in the morphologicaltransducer.
The morphological transducer here isthe same as the one described in subsection 4.1 butwith one difference, that is the output does not in-clude any morphological features, but only tokenboundaries between clitics and stems.This is a relatively deterministic tokenizer thathandles clitics properly.
The main downfall is thatonly words found in the morphological transducerare tokenized.
It is not robust, yet it may be moreconvenient during grammar debugging, as it pro-vides much fewer analyses than model 2.
Herespurious ambiguities are successfully avoided.
(6) ??????
(and to the man)???@??@?@?
@One advantage of this implementation is thatthe tool becomes more deterministic and moremanageable in debugging.
Its lack of robustness,however, makes it mostly inapplicable as no singlemorphological transducer can claim to comprise allthe words in a language.
In our XLE grammar, thismodel is only 0.05% faster than Model 2.
This isnot statistically significant advantage compared toits limitations.4.4 Tokenizing Multiword ExpressionsMultiword Expressions (MWEs) are two ormore words that behave like a single word syntac-tically and semantically.
They are defined, moreformally, as ?idiosyncratic interpretations thatcross word boundaries?
(Sag et al2001).
MWEscover expressions that are traditionally classified asidioms (e.g.
down the drain), prepositional verbs(e.g.
rely on), verbs with particles (e.g.
give up),compound nouns (e.g.
traffic lights) and colloca-tions (e.g.
do a favour).With regard to syntactic and morphologicalflexibility, MWEs are classified into three types:fixed, semi-fixed and syntactically flexible expres-sions (Baldwin 2004; Oflazer et al2004; Sag et al2001).a.
Fixed Expressions.
These expressions arelexically, syntactically and morphologically rigid.An expression of this type is considered as a wordwith spaces (a single word that happens to contain68spaces), such as ???????
????
al-sharq al-awsat (theMiddle East) and ???
???
bait lahem (Bethlehem).b.
Semi-Fixed Expressions.
These expressionscan undergo variations, but still the components ofthe expression are adjacent.
The variations are oftwo types, morphological variations where lexicalitems can express person, number, tense, gender,etc., such as the examples in (7), and lexical varia-tions, where one word can be replaced by anotheras in (8).
(7.a) ????
???????
?fatratah intiqaliyyahtranslational.sg.fem period.sg.fem(7.b) ??????
?????????
?fatratan intiqaliyyatantranslational.dual.fem period.dual.fem(8) ???????/???
?????/???
??
?ala zahr/wajh al-ard/al-basitahon the face/surface of the land/earth(on the face of the earth)c. Syntactically Flexible Expressions.
Theseare the expressions that can either undergo reorder-ing, such as passivization (e.g.
the cat was let outof the bag), or allow external elements to intervenebetween the components such as (9.b), where theadjacency of the MWE is disrupted.
(9.a) ?????
????
?darrajah nariyyahbike fiery (motorbike)(9.b) ?????
?????
??????
?darrajat al-walad al-nariyyahthe-bike the-boy the-fiery (the boy?s motorbike)Fixed and semi-fixed expressions are identifiedand marked by the tokenizer, while syntacticallyflexible expressions can only be handled by a syn-tactic parser (Attia 2006a).The tokenizer is responsible for treating MWEsin a special way.
They should be marked as singletokens with the inner space(s) preserved.
For thispurpose, as well as for the purpose of morphologi-cal analysis, a specialized transducer is developedfor MWEs that lists all variations of MWEs andprovides analyses for them (Attia 2006a).One way to allow the tokenizer to handleMWEs is to embed the MWEs in the Tokenizer(Beesley and Karttunen 2003).
Yet a better ap-proach, described by (Karttunen et al1996), is todevelop one or several multiword transducers or?staplers?
that are composed on the tokenizer.
Wewill explain here how this is implemented in oursolution, where the list of MWEs is extracted fromthe MWE transducer and composed on the token-izer.
Let?s look at the composition regular expres-sion:(10) 1    singleTokens.i2    .o.
?
* 0:"[[[" (MweTokens.l) 0:"]]]" ?
*3    .o.
"@" -> " " || "[[[" [Alphabet* | "@"*]  _4    .o.
"[[[" -> [] .o. "]]]"
-> []].i;Single words separated by the ?@?
sign are definedin the variable singleTokens and the MWE trans-ducer is defined in MweTokens.
In the MWEtransducer all spaces in the lower language are re-placed by ?@?
so that the lower language can bematched against singleTokens.
In line 1 the single-Tokens is inverted (the upper language is shifteddown) by the operator ?.i?
so that compositiongoes on the side that contains the relevant strings.From the MWE transducer we take only the lowerlanguage (or the surface form) by the operator ?.l?in line 2.
Single words are searched and if theycontain any MWEs, the expressions will (option-ally) be enclosed by three brackets on either side.Line 3 replaces all ?@?
signs with spaces in sideMWEs only.
The two compositions in line 4 re-move the intermediary brackets.Let?s now show this with a working example.For the phrase in (11), the tokenizer first gives theoutput in (12).
Then after the MWEs are composedwith the tokenizer, we obtain the result in (13) withthe MWE identified as a single token.
(11) ??????
???????
?wa-liwazir kharijiyatihaand-to-foreign minister-its(and to its foreign minister)(12) ??@??????@????@?@?@(approx.
and@to@foreign@minister@its@)(13)  ??@????
??????@?@?(approx.
and@to@foreign minister@its@)4.5 Normalizing White SpacesWhite space normalization is a preliminarystage to tokenization where redundant and mis-placed white spaces are corrected, to enable thetokenizer to work on a clean and predictable text.69In real-life data spaces may not be as regularly andconsistently used as expected.
There may be two ormore spaces, or even tabs, instead of a singlespace.
Spaces might even be added before or afterpunctuation marks in the wrong manner.
There-fore, there is a need for a tool that eliminates in-consistency in using white spaces, so that when thetext is fed into a tokenizer or morphological ana-lyzer, words and expressions can be correctly iden-tified and analyzed.
Table 1 shows where spacesare not expected before or after some punctuationmarks.No Space Before No Space After) (} {] [?
?Table 1.
Space distribution with some punctuationmarksWe have developed a white space normalizerwhose function is to go through real-life texts andcorrect mistakes related to the placement of whitespaces.
When it is fed an input such as the one in(14.a) in which additional spaces are inserted andsome spaces are misplaced, it corrects the errorsand gives the output in (14.b):(14.a)  ??????
?????
???
)???????????
( ???.
(14.b)  ?????
???
??????)???????????(???
.5 Resolving AmbiguityThere are different types of ambiguity.
Thereare spurious ambiguities created by the guesser.There are also ambiguities which do not exist inthe text before tokenization but are only createdduring the tokenization process.
Finally there arereal ambiguities, where a form can be read as asingle word or two sub-tokens, or where an MWEhas a compositional reading.
These three types aretreated by the following three subsections respec-tively.5.1 Discarding Spurious AmbiguitiesTokenization Model 2 discussed above in subsec-tion 4.2 is chosen as the optimal implementationdue to its efficiency and robustness, yet it is highlynondeterministic and produces a large number ofspurious ambiguities.
Therefore, a morphologicaltransducer is needed to filter out the tokenizationpaths that contain incorrect sub-tokens.
Recall ex-ample (4) which contained the output of the nonde-terministic tokenizer.
In (15) below, after the out-put is fed into a morphological transducer, onlyone solution is accepted and the rest are discarded,as underlined words do not constitute valid stems.
(15) ??????
(and to the man)???@??@?@?
@ - Passed.@?@?@?????
- Discarded.@?@?????
- Discarded.@??????
- Discarded.5.2 Handling Tokenization AmbiguitiesAmong the function of a tokenizer is separateclitics from stems.
Some clitics, however, whenseparated, become ambiguous with other cliticsand also with other free forms.
For example theword ??????
kitabahum has only one morphologicalreading (meaning their book), but after tokeniza-tion ??@????
there are three different readings, asthe second token ??
can either be a clitic genitivepronoun (the intended reading) or a free pronounthey (a book, they) or a noun meaning worry(forming the compound book of worry).This problem is solved by inserting a mark thatprecedes enclitics and follows proclitics to distin-guish them from each other as well as from freeforms (Ron M. Kaplan and Martin Forst, personalcommunications, Oxford, UK, 20 September2006).
The mark we choose is the Arabic elonga-tion short line called cashida which is originallyused for graphical decorative purposes and looksnatural with most clitics.
To illustrate the usage, atwo-word string (16.a) will be rendered withoutcashidas as in (16.b), and a single-word string thatcontains clitics (17.a) will be rendered with a dis-tinctive cashida before the enclitic pronoun as in(17.b).
This indicates that the pronoun is attachedto the preceding word and not standing alone.
(16.a) ????
?
?kitab hum/hamm (book of worry/a book, they)(16.b) ??
@ ????
(17.a) ??????
kitabuhum (their book)(17.b) ???@???
?70This implementation will also resolve a similarambiguity, that is ambiguity arising between pro-clitics and enclitics.
The proclitic preposition ?
ka(as) always occurs initially.
There is a homo-graphic enclitic object pronoun ?
ka (you) thatalways occurs in the final position.
This can createambiguity in instances such as the made-up sen-tence in (18.a).
The sentence has the initial tokeni-zation of (18.b) without a cashida, and thereforethe central token becomes ambiguous as it can nowbe attached either to the preceding or followingword leading either to the readings in (18.a) or(18.c).
The cashida placement, however, resolvesthis ambiguity as in (18.d).
The cashida is addedafter the token, indicating that it is attached to thefollowing word and now only the reading in (18.a)is possible.
(18.a) ?????
??????
?a?taitu ka-lamir (I gave like a prince)(18.b) ??????@?@?????
(18.c) ??????
?????
?a?taitu-ka alamir (I gave you the prince)(18.d) ??????@??@????
?5.3 Handling Real AmbiguitiesSome tokenization readings are legal, yet highlyinfrequent and undesired in real-life data.
Theseundesired readings create onerous ambiguities, asthey are confused with more common and moreacceptable forms.
For example the Arabic preposi-tion ???
ba?d (after) has the possible remote readingof being split into two tokens ??@??
, which is madeof two elements: ??
bi (with) and ??
?add (counting).Similarly ???
baina (between) has the possible re-mote reading ??@??
, which is made of two tokensas well: ??
bi (with) and ??
yin (Yen).The same problem occurs with MWEs.
The op-timal handling of MWEs is to treat them as singletokens and leave internal spaces intact.
Yet a non-deterministic tokenizer allows MWEs to be ana-lysed compositionally as individual words.
So theMWE ???
??????
hazr al-tajawwul (curfew) hastwo analyses, as in (19), although the composi-tional reading in (19.b) is undesired.
(19.a) ???
??????
@  hazr al-tajawwul (curfew)(19.b) ??????@??
?hazr (forbidding) al-tajawwul (walking)The solution to this problem is to mark the un-desired readings.
This is implemented by develop-ing a filter, or a finite state transducer that containsall possible undesired tokenization possibilities andattaches the ?+undesired?
tag to each one of them.Undesired tokens, such as ??@??
and ??@??
,explained above, can be included in a custom listin the token filter.
As for MWEs, the token filterimports a list from the MWE transducer and re-places the spaces with the token delimiter ?@?
todenote the undesired tokenization solutions.
Thetoken filter then matches the lists against the outputof the tokenizer.
If the output contains a matchingstring a mark is added, giving the output in (20).Notice how (20.b) is marked with the ?+undesired?tag.
(20.a) ???
??????
@ [hazr al-tajawwul (curfew)](20.b) ??????@???
+undesiredThis transducer or filter is composed on top ofthe core tokenizer.
The overall design of the token-izer and its interaction with other finite state com-ponents is shown in Figure 3.
WE must note thatthe tokenizer, in its interaction with the morpho-logical transducer and the MWE transducer, doesnot seek morpho-syntactic information, but it que-ries for lists and possible combinations.Figure 3: Design of the Arabic Tokenizer6 ConclusionTokenization is a process that is closely connectedto and dependent on morphological analysis.
In ourresearch we show how different models of tokeni-zation are implemented at different levels of lin-guistic depth.
We also explain how the tokenizer71interacts with other components1, and how it re-solves complexity and filters ambiguity.
By apply-ing token filters we gain control over the tokeniza-tion output.ReferencesAbb?s R, Dichy J, Hassoun M (2004): The Architectureof a Standard Arabic lexical database: some figures,ratios and categories from the DIINAR.1 source pro-gram, The Workshop on Computational Approachesto Arabic Script-based Languages, COLING 2004.Geneva, Switzerland.Attia M (2006a): Accommodating Multiword Expres-sions in an Arabic LFG Grammar.
In Salakoski T,Ginter F, Pyysalo S, Pahikkala T (eds), Advances inNatural Language Processing, 5th International Con-ference on NLP, FinTAL 2006, Turku, Finland, Vol4139.
Turku, Finland: Springer-Verlag Berlin Hei-delberg, pp 87-98.Attia M (2006b): An Ambiguity-Controlled Morpho-logical Analyzer for Modern Standard Arabic Model-ling Finite State Networks, The Challenge of Arabicfor NLP/MT Conference.
The British Computer So-ciety, London, UK.Baldwin T (2004): Multiword Expressions, an Ad-vanced Course, The Australasian Language Technol-ogy Summer School (ALTSS 2004).
Sydney, Austra-lia.Beesley KR (2001): Finite-State Morphological Analy-sis and Generation of Arabic at Xerox Research:Status and Plans in 2001, Proceedings of the ArabicLanguage Processing: Status and Prospect--39th An-nual Meeting of the Association for ComputationalLinguistics.
Toulouse, France.Beesley KR, Karttunen L (2003): Finite State Morphol-ogy.
Stanford, Calif.: CSLI.Buckwalter T (2002): Buckwalter Arabic Morphologi-cal Analyzer Version 1.0., Linguistic Data Consor-tium.
Catalog number LDC2002L49, and ISBN 1-58563-257-0.Butt M, Dyvik H, King TH, Masuichi H, Rohrer C(2002): The Parallel Grammar Project, COLING-2002 Workshop on Grammar Engineering andEvaluation.
Taipei, Taiwan.1 The tokenizer along with a number of other Arabicfinite state tools are made available for evaluation onthe website: www.attiapace.comChanod J-P, Tapanainen P (1994): A Non-DeterministicTokenizer for Finite-State Parsing, ECAI'96.
Buda-pest, Hungary.Diab M, Hacioglu K, Jurafsky D (2004): AutomaticTagging of Arabic Text: From Raw Text to BasePhrase Chunks, Proceedings of NAACL-HLT 2004.Boston.Dichy J (2001): On lemmatization in Arabic.
A formaldefinition of the Arabic entries of multilingual lexicaldatabases, ACL 39th Annual Meeting.
Workshop onArabic Language Processing; Status and Prospect.Toulouse, pp 23-30.Dichy J, Fargaly A (2003): Roots & Patterns vs. Stemsplus Grammar-Lexis Specifications: on what basisshould a multilingual lexical database centred onArabic be built?, Proceedings of the MT-Summit IXworkshop on Machine Translation for Semitic Lan-guages.
New-Orleans.Habash N, Rambow O (2005): Arabic Tokenization,Part-of-Speech Tagging and Morphological Disam-biguation in One Fell Swoop, Proceedings of ACL2005.
Michigan.Karttunen L, Chanod J-P, Grefenstette G, Schiller A(1996): Regular expressions for language engineer-ing.
Natural Language Engineering 2:305-328.Larkey LS, Connell ME (2002): Arabic InformationRetrieval at UMass.
In Voorhees EM, Harman DK(eds), The Tenth Text Retrieval Conference, TREC2001.
Maryland: NIST Special Publication, pp 562-570.Nelken R, Shieber SM (2005): Arabic DiacritizationUsing Weighted Finite-State Transducers, Proceed-ings of the 2005 ACL Workshop on ComputationalApproaches to Semitic Languages.
Michigan.Oflazer K, Uglu ?
?, Say B (2004): Integrating Mor-phology with Multi-word Expression Processing inTurkish, Second ACL Workshop on Multiword Ex-pressions: Integrating Processing.
Spain, pp 64-71.Sag IA, Baldwin T, Bond F, Copestake A, Flickinger D(2001): Multi-word Expressions: A Pain in the Neckfor NLP, LinGO Working Papers.
Stanford Univer-sity, CA.72
