Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 941?948,Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLPA Comparative Study of Hypothesis Alignment and its Improvementfor Machine Translation System CombinationBoxing Chen*, Min Zhang, Haizhou Li and Aiti AwInstitute for Infocomm Research1 Fusionopolis Way, 138632 Singapore{bxchen, mzhang, hli, aaiti}@i2r.a-star.edu.sgAbstractRecently confusion network decoding showsthe best performance in combining outputsfrom multiple machine translation (MT) sys-tems.
However, overcoming different wordorders presented in multiple MT systems dur-ing hypothesis alignment still remains thebiggest challenge to confusion network-basedMT system combination.
In this paper, wecompare four commonly used word align-ment methods, namely GIZA++, TER, CLAand IHMM, for hypothesis alignment.
Thenwe propose a method to build the confusionnetwork from intersection word alignment,which utilizes both direct and inverse wordalignment between the backbone and hypo-thesis to improve the reliability of hypothesisalignment.
Experimental results demonstratethat the intersection word alignment yieldsconsistent performance improvement for allfour word alignment methods on both Chi-nese-to-English spoken and written languagetasks.1 IntroductionMachine translation (MT) system combinationtechnique leverages on multiple MT systems toachieve better performance by combining theiroutputs.
Confusion network based system com-bination for machine translation has shownpromising advantage compared with other tech-niques based system combination, such as sen-tence level hypothesis selection by voting andsource sentence re-decoding using the phrases ortranslation models that are learned from thesource sentences and target hypotheses pairs(Rosti et al, 2007a; Huang and Papineni, 2007).In general, the confusion network based sys-tem combination method for MT consists of foursteps: 1) Backbone selection: to select a back-bone (also called ?skeleton?)
from all hypotheses.The backbone defines the word orders of the fi-nal translation.
2) Hypothesis alignment: to buildword-alignment between backbone and each hy-pothesis.
3) Confusion network construction: tobuild a confusion network based on hypothesisalignments.
4) Confusion network decoding: todecode the best translation from a confusionnetwork.
Among the four steps, the hypothesisalignment presents the biggest challenge to themethod due to the varying word orders betweenoutputs from different MT systems (Rosti et al2007).
Many techniques have been studied toaddress this issue.
Bangalore et al (2001) usedthe edit distance alignment algorithm which isextended to multiple strings to build confusionnetwork, it only allows monotonic alignment.Jayaraman and Lavie (2005) proposed a heuris-tic-based matching algorithm which allows non-monotonic alignments to align the words be-tween the hypotheses.
More recently, Matusov etal.
(2006, 2008) used GIZA++ to produce wordalignment for hypotheses pairs.
Sim et al (2007),Rosti et al (2007a), and Rosti et al (2007b) usedminimum Translation Error Rate (TER) (Snoveret al, 2006) alignment to build the confusionnetwork.
Rosti et al (2008) extended TER algo-rithm which allows a confusion network as thereference to compute word alignment.
Karakos etal.
(2008) used ITG-based method for hypothesisalignment.
Chen et al (2008) used CompetitiveLinking Algorithm (CLA) (Melamed, 2000) toalign the words to construct confusion network.Ayan et al (2008) proposed to improve align-ment of hypotheses using synonyms as found inWordNet (Fellbaum, 1998) and a two-passalignment strategy based on TER word align-ment approach.
He et al (2008) proposed anIHMM-based word alignment method which theparameters are estimated indirectly from a varie-ty of sources.Although many methods have been attempted,no systematic comparison among them has beenreported.
A through and fair comparison amongthem would be of great meaning to the MT sys-941tem combination research.
In this paper, we im-plement a confusion network-based decoder.Based on this decoder, we compare four com-monly used word alignment methods (GIZA++,TER, CLA and IHMM) for hypothesis alignmentusing the same experimental data and the samemultiple MT system outputs with similar featuresin terms of translation performance.
We conductthe comparison study and other experiments inthis paper on both spoken and newswire do-mains: Chinese-to-English spoken and writtenlanguage translation tasks.
Our comparisonshows that although the performance differencesbetween the four methods are not significant,IHMM consistently show slightly better perfor-mance than other methods.
This is mainly due tothe fact the IHMM is able to explore more know-ledge sources and Viterbi decoding used inIHMM allows more thorough search for the bestalignment while other methods has to use lessoptimal greedy search.In addition, for better performance, instead ofonly using one direction word alignment (n-to-1from hypothesis to backbone) as in previouswork, we propose to use more reliable wordalignments which are derived from the intersec-tion of two-direction hypothesis alignment toconstruct confusion network.
Experimental re-sults show that the intersection word alignment-based method consistently improves the perfor-mance for all four methods on both spoken andwritten language tasks.This paper is organized as follows.
Section 2presents a standard framework of confusion net-work based machine translation system combina-tion.
Section 3 introduces four word alignmentmethods, and the algorithm of computing inter-section word alignment for all four word align-ment methods.
Section 4 describes the experi-ments setting and results on two translation tasks.Section 5 concludes the paper.2 Confusion network based systemcombinationIn order to compare different hypothesis align-ment methods, we implement a confusion net-work decoding system as follows:Backbone selection: in the previous work,Matusov et al (2006, 2008) let every hypothesisplay the role of the backbone (also called ?skele-ton?
or ?alignment reference?)
once.
We followthe work of (Sim et al, 2007; Rosti et al, 2007a;Rosti et al, 2007b; He et al, 2008) and choosethe hypothesis that best agrees with other hypo-theses on average as the backbone by applyingMinimum Bayes Risk (MBR) decoding (Kumarand Byrne, 2004).
TER score (Snover et al2006) is used as the loss function in MBR decod-ing.
Given a hypothesis set H, the backbone canbe computed using the following equation, where( , )TER ?
?
returns the TER score of two hypothes-es.?
?arg min ( , )bE H E HE TER E E?
?= ?
(1)Hypothesis alignment: all hypotheses areword-aligned to the corresponding backbone in amany-to-one manner.
We apply four wordalignment methods: GIZA++-based, TER-based,CLA-based, and IHMM-based word alignmentalgorithm.
For each method, we will give detailsin the next section.Confusion network construction: confusionnetwork is built from one-to-one word alignment;therefore, we need to normalize the word align-ment before constructing the confusion network.The first normalization operation is removingduplicated links, since GIZA++ and IHMM-based word alignments could be n-to-1 mappingsbetween the hypothesis and backbone.
Similar tothe work of (He et al, 2008), we keep the linkwhich has the highest similarity measure( , )j iS e e?
based on surface matching score, suchas the length of maximum common subsequence(MCS) of the considered word pair.2 ( ( , ))( , )( ) ( )j ij ij ilen MCS e eS e elen e len e???
=?
+(2)where ( , )j iMCS e e?
is the maximum commonsubsequence of word je?
and ie ; (.
)len  is afunction to compute the length of letter sequence.The other hypothesis words are set to align to thenull word.
For example, in Figure 1, 1e?
and 3e?are aligned to the same backbone word2e , weremove the link between2e  and 3e?
if3 2 1 2( , ) ( , )S e e S e e?
?< , as shown in Figure 1 (b).The second normalization operation is reorder-ing the hypothesis words to match the word orderof the backbone.
The aligned words are reor-dered according to their alignment indices.
Toreorder the null-aligned words, we need to firstinsert the null words into the proper position inthe backbone and then reorder the null-alignedhypothesis words to match the nulls on the back-bone side.
Reordering null-aligned words variesbased to the word alignment method in the pre-942vious work.
We reorder the null-aligned wordfollowing the approach of Chen et al (2008)with some extension.
The null-aligned words arereordered with its adjacent word: moving with itsleft word (as Figure 1 (c)) or right word (as Fig-ure 1 (d)).
However, to reduce the possibility ofbreaking a syntactic phrase, we extend to chooseone of the two above operations depending onwhich one has the higher likelihood with the cur-rent null-aligned word.
It is implemented bycomparing two association scores based on co-occurrence frequencies.
They are associationscore of the null-aligned word and its left word,or the null-aligned word and its right word.
Weuse point-wise mutual information (MI) as Equa-tion 3 to estimate the likelihood.111( )( , ) log( ) ( )i ii ii ip e eMI e ep e p e+++?
??
?
=?
?
(3)where 1( )i ip e e +?
?
is the occurrence probability ofbigram 1i ie e +?
?
observed in the hypothesis list;( )ip e?
and 1( )ip e +?
are probabilities of hypothe-sis word ie?
and 1ie +?
respectively.In example of Figure 1, we choose (c)if 2 3 3 4( , ) ( , )MI e e MI e e?
?
?
?> , otherwise, word isreordered as (d).a1e  2e  3e1e?
2e?
3e?
4e?b1e  2e  3e1e?
2e?
3e?
4e?c1e  2e  3e4e?
1e?
2e?
3e?d1e  2e  3e3e?
4e?
1e?
2e?Figure 1: Example of alignment normalization.Confusion network decoding: the outputtranslations for a given source sentence are ex-tracted from the confusion network through abeam-search algorithm with a log-linear combi-nation of a set of feature functions.
The featurefunctions which are employed in the searchprocess are:?
Language model(s),?
Direct and inverse IBM model-1,?
Position-based word posterior probabili-ties (arc scores of the confusion network),?
Word penalty,?
N-gram frequencies (Chen et al, 2005),?
N-gram posterior probabilities (Zens andNey, 2006).The n-grams used in the last two feature func-tions are collected from the original hypotheseslist from each single system.
The weights of fea-ture functions are optimized to maximize thescoring measure (Och, 2003).3 Word alignment algorithmsWe compare four word alignment methodswhich are widely used in confusion networkbased system combination or bilingual parallelcorpora word alignment.3.1 Hypothesis-to-backbone word align-mentGIZA++: Matusov et al (2006, 2008) proposedusing GIZA++ (Och and Ney, 2003) to alignwords between the backbone and hypothesis.This method uses enhanced HMM model boot-strapped from IBM Model-1 to estimate thealignment model.
All hypotheses of the wholetest set are collected to create sentence pairs forGIZA++ training.
GIZA++ produces hypothesis-backbone many-to-1 word alignments.TER-based: TER-based word alignmentmethod (Sim et al, 2007; Rosti et al, 2007a;Rosti et al, 2007b) is an extension of multiplestring matching algorithm based on Levenshteinedit distance (Bangalore et al, 2001).
The TER(translation error rate) score (Snover et al, 2006)measures the ratio of minimum number of stringedits between a hypothesis and reference wherethe edits include insertions, deletions, substitu-tions and phrase shifts.
The hypothesis is modi-fied to match the reference, where a greedysearch is used to select the set of shifts becausean optimal sequence of edits (with shifts) is veryexpensive to find.
The best alignment is the onethat gives the minimum number of translationedits.
TER-based method produces 1-to-1 wordalignments.CLA-based: Chen et al (2008) used competi-tive linking algorithm (CLA) (Melamed, 2000)to build confusion network for hypothesis rege-neration.
Firstly, an association score is com-puted for every possible word pair from thebackbone and hypothesis to be aligned.
Then agreedy algorithm is applied to select the bestword alignment.
We compute the associationscore from a linear combination of two clues:943surface similarity computed as Equation (2) andposition difference based distortion score by fol-lowing (He et al, 2008).
CLA works under a 1-to-1 assumption, so it produces 1-to-1 wordalignments.IHMM-based: He et al (2008) propose anindirect hidden Markov model (IHMM) for hy-pothesis alignment.
Different from traditionalHMM, this model estimates the parameters indi-rectly from various sources, such as word seman-tic similarity, surface similarity and distortionpenalty, etc.
For fair comparison reason, we alsouse the surface similarity computed as Equation(2) and position difference based distortion scorewhich are used for CLA-based word alignment.IHMM-based method produces many-to-1 wordalignments.3.2 Intersection word alignment and its ex-pansionIn previous work, Matusov et al (2006, 2008)used both direction word alignments to computeso-called state occupation probabilities and thencompute the final word alignment.
The otherwork usually used only one direction wordalignment (many/1-to-1 from hypothesis tobackbone).
In this paper, we use more reliableword alignments which are derived from the in-tersection of both direct (hypothesis-to-backbone)and inverse (backbone-to-hypothesis) wordalignments with heuristic-based expansion whichis widely used in bilingual word alignment.
Thealgorithm includes two steps:1) Generate bi-directional word alignments.
Itis straightforward for GIZA++ and IHMM togenerate bi-directional word alignments.
This issimply achieved by switching the parameters ofsource and target sentences.
Due to the nature ofgreedy search in TER, the bi-directional TER-based word alignments by switching the parame-ters of source and target sentences are not neces-sary exactly the same.
For example, in Figure 2,the word ?shot?
can be aligned to either ?shoot?or ?the?
as the edit cost of word pair (shot, shoot)and (shot, the) are the same when compute theminimum-edit-distance for TER score.I shot  killerI shoot the killeraI shoot the killerI  shot killerbFigure 2: Example of two directions TER-basedword alignments.For CLA word alignment, if we use the sameassociation score, direct and inverse CLA wordalignments should be exactly the same.
There-fore, we use different functions to compute thesurface similarities, such as using maximumcommon subsequence (MCS) to compute inverseword alignment, and using longest matched pre-fix (LMP) for computing direct word alignment,as in Equation (4).2 ( ( , ))( , )( ) ( )j ij ij ilen LMP e eS e elen e len e???
=?
+(4)2) When two word alignments are ready, westart from the intersection of the two wordalignments, and then continuously add new linksbetween backbone and hypothesis if and only ifboth of the two words of the new link are un-aligned and this link exists in the union of twoword alignments.
If there are more than two linksshare a same hypothesis or backbone word andalso satisfy the constraints, we choose the linkthat with the highest similarity score.
For exam-ple, in Figure 2, since MCS-based similarityscores ( , ) ( , )S shot shoot S shot the> , wechoose alignment (a).4  Experiments and results4.1 Tasks and single systemsExperiments are carried out in two domains.
Oneis in spoken language domain while the other ison newswire corpus.
Both experiments are onChinese-to-English translation.Experiments on spoken language domain werecarried out on the Basic Traveling ExpressionCorpus (BTEC) (Takezawa et al, 2002) Chi-nese- to-English data augmented with HIT-corpus1.
BTEC is a multilingual speech corpuswhich contains sentences spoken by tourists.40K sentence-pairs are used in our experiment.HIT-corpus is a balanced corpus and has 500Ksentence-pairs in total.
We selected 360K sen-tence-pairs that are more similar to BTEC dataaccording to its sub-topic.
Additionally, the Eng-lish sentences of Tanaka corpus2 were also usedto train our language model.
We ran experimentson an IWSLT challenge task which uses IWSLT-20063 DEV clean text set as development set andIWSLT-2006 TEST clean text as test set.1 http://mitlab.hit.edu.cn/2 http://www.csse.monash.edu.au/~jwb/tanakacorpus.html3 http:// www.slc.atr.jp/IWSLT2006/944Experiments on newswire domain were car-ried out on the FBIS4 corpus.
We used NIST52002 MT evaluation test set as our developmentset, and the NIST 2005 test set as our test set.Table 1 summarizes the statistics of the train-ing, dev and test data for IWSLT and NIST tasks.task data Ch EnIWSLTTrain Sent.
406KWords 4.4M 4.6MDev Sent.
489 489?7Words 5,896 45,449Test Sent.
500 500?7Words 6,296 51,227Add.
Words - 1.7MNISTTrain Sent.
238KWords 7.0M 8.9MDev2002Sent.
878 878?4Words 23,248 108,616Test2005Sent.
1,082 1,082?4Words 30,544 141,915Add.
Words - 61.5MTable 1: Statistics of training, dev and test datafor IWSLT and NIST tasks.In both experiments, we used four systems, aslisted in Table 2,  they are phrase-based systemMoses (Koehn et al, 2007), hierarchical phrase-based system (Chiang, 2007), BTG-based lexica-lized reordering phrase-based system (Xiong etal., 2006) and a tree sequence alignment-basedtree-to-tree translation system (Zhang et al,2008).
Each system for the same task is trainedon the same data set.4.2 Experiments settingFor each system, we used the top 10 scored hy-potheses to build the confusion network.
Similarto (Rosti et al, 2007a), each word in the hypo-thesis is assigned with a rank-based score of1/ (1 )r+ , where r is the rank of the hypothesis.And we assign the same weights to each system.For selecting the backbone, only the top hypo-thesis from each system is considered as a candi-date for the backbone.Concerning the four alignment methods, weuse the default setting for GIZA++; and use tool-kit TERCOM (Snover et al, 2006) to computethe TER-based word alignment, and also use thedefault setting.
For fair comparison reason, we4 LDC2003E145 http://www.nist.gov/speech/tests/mt/decide to do not use any additional resource,such as target language synonym list, IBM modellexicon; therefore, only surface similarity is ap-plied in IHMM-based and CLA-based methods.We compute the distortion model by following(He et al, 2008) for IHMM and CLA-based me-thods.
The weights for each model are optimizedon held-out data.System Dev TestIWSLTSys1 30.75 27.58Sys2 30.74 28.54Sys3 29.99 26.91Sys4 31.32 27.48NISTSys1 25.64 23.59Sys2 24.70 23.57Sys3 25.89 22.02Sys4 26.11 21.62Table 2: Results (BLEU% score) of single sys-tems involved to system combination.4.3 Experiments resultsOur evaluation metric is BLEU (Papineni et al,2002), which are to perform case-insensitivematching of n-grams up to n = 4.Performance comparison of four methods:the results based on direct word alignments arereported in Table 3, row Best is the best singlesystems?
scores; row MBR is the scores of back-bone; GIZA++, TER, CLA, IHMM stand forscores of systems for four word alignment me-thods.z MBR decoding slightly improves the per-formance over the best single system for bothtasks.
This suggests that the simple voting strate-gy to select backbone is workable.z For both tasks, all methods improve the per-formance over the backbone.
For IWSLT test set,the improvements are from 2.06 (CLA, 30.88-28.82) to 2.52 BLEU-score (IHMM, 31.34-28.82).
For NIST test set, the improvements arefrom 0.63 (TER, 24.31-23.68) to 1.40 BLEU-score (IHMM, 25.08-23.68).
This verifies thatthe confusion network decoding is effective incombining outputs from multiple MT systemsand the four word-alignment methods are alsoworkable for hypothesis-to-backbone alignment.z For IWSLT task where source sentences areshorter (12-13 words per sentence in average),the four word alignment methods achieve similarperformance on both dev and test set.
The big-gest difference is only 0.46 BLEU score (30.88for CLA, vs. 31.34 for IHMM).
For NIST task945where source sentences are longer (26-28 wordsper sentence in average), the difference is moresignificant.
Here IHMM method achieves thebest performance, followed by GIZA++, CLAand TER.
IHMM is significantly better than TERby 0.77 BLEU-score (from 24.31 to 25.08,p<0.05).
This is mainly because IHMM exploitsmore knowledge source and Viterbi decodingallows more thorough search for the best align-ment while other methods use less optimal gree-dy search.
Another reason is that TER uses hardmatching in computing edit distance.method Dev TestIWSLTBest 31.32 28.54MBR 31.40 28.82GIZA++ 34.16 31.06TER 33.92 30.96CLA 33.85 30.88IHMM 34.35 31.34NISTBest 26.11 23.59MBR 26.36 23.68GIZA++ 27.58 24.88TER 27.15 24.31CLA 27.44 24.51IHMM 27.76 25.08Table 3: Results (BLEU% score) of combinedsystems based on direct word alignments.Performance improvement by intersectionword alignment: Table 4 reports the perfor-mance of the system combinations based on in-tersection word alignments.
It shows that:z Comparing Tables 3 and 4, we can see thatthe intersection word alignment-based expansionmethod improves the performance in all the devand test sets for both tasks by 0.2-0.57 BLEU-score and the improvements are consistent underall conditions.
This suggests that the intersectionword alignment-based expansion method is moreeffective than the commonly used direct word-alignment-based hypothesis alignment method inconfusion network-based MT system combina-tion.
This is because intersection word align-ments are more reliable compared with directword alignments, and so for heuristic-based ex-pansion which is based on the aligned wordswith higher scores.z TER-based method achieves the biggestperformance improvement by 0.4 BLEU-score inIWSLT and 0.57 in NIST.
Our statistics showsthat the TER-based word alignment generatesmore inconsistent links between the two-directional word alignments than other methods.This may give the intersection with heuristic-based expansion method more room to improveperformance.z On the contrast, CLA-based method obtainsrelatively small improvement of 0.26 BLEU-score in IWSLT and 0.21 in NIST.
The reasoncould be that the similarity functions used in thetwo directions are more similar.
Therefore, thereare not so many inconsistent links between thetwo directions.z Table 5 shows the number of links modifiedby intersection operation and the BLEU-scoreimprovement.
We can see that the more the mod-ified links, the bigger the improvement.method Dev TestIWSLTMBR 31.40 28.82GIZA++ 34.38 31.40TER 34.17 31.36CLA 34.03 31.14IHMM 34.59 31.74NISTMBR 26.36 23.68GIZA++ 27.80 25.11TER 27.58 24.88CLA 27.64 24.72IHMM 27.96 25.37Table 4: Results (BLEU% score) of combinedsystems based on intersection word alignments.systemIWSLT NISTInc.
Imp.
Inc. Imp.CLA 1.2K 0.26 9.2K 0.21GIZA++ 3.2K 0.36 25.5K 0.23IHMM 3.7K 0.40 21.7K 0.29TER 4.3K 0.40 40.2K 0.57#total links 284K 1,390KTable 5: Number of modified links and absoluteBLEU(%) score improvement on test sets.Effect of fuzzy matching in TER: the pre-vious work on TER-based word alignment useshard match in counting edits distance.
Therefore,it is not able to handle cognate words match,such as in Figure 2, original TER script count theedit cost of (shoot, shot) equals to word pair(shot, the).
Following (Leusch et al, 2006), wemodified the TER script to allow fuzzy matching:change the substitution cost from 1 for any wordpair to946( , ) 1 ( , )sub j i j iCOST e e S e e?
?= ?
(5)which ( , )j iS e e?
is the similarity score based onthe length of longest matched prefix (LMP)computed as in Equation (4).
As a result, thefuzzy matching reports( , ) 1 (2 3) /(5 4) 1/ 3SubCost shoot shot = ?
?
+ =  and( , ) 1 (2 0) /(5 3) 1SubCost shoot the = ?
?
+ =  while inoriginal TER, both of the two scores are equal to1.
Since cost of word pair (shoot, shot) is smallerthan that of word pair (shot, the), word ?shot?has higher chance to be aligned to ?shoot?
(Fig-ure 2 (a)) instead of ?the?
(Figure 2 (b)).
Thisfuzzy matching mechanism is very useful to suchkind of monolingual alignment task as in hypo-thesis-to-backbone word alignment since it canwell model word variances and morphologicalchanges.Table 6 summaries the results of TER-basedsystems with or without fuzzy matching.
We cansee that the fuzzy matching improves the per-formance for all cases.
This verifies the effect offuzzy matching for TER in monolingual wordalignment.
In addition, the improvement in NISTtest set (0.36 BLEU-score for direct alignmentand 0.21 BLEU-score for intersection one) aremore than that in IWSLT test set (0.15 BLEU-score for direct alignment and 0.11 BLEU-scorefor intersection one).
This is because the sen-tences of IWSLT test set are much shorter thanthat of NIST test set.TER-basedsystemsIWSLT NISTDev Test Dev TestDirect align+fuzzy match33.9234.1430.9631.1127.1527.5324.3124.67Intersect align+fuzzy match34.1734.4031.3631.4727.5827.7924.8825.09Table 6: Results (BLEU% score) of TER-basedcombined systems with or without fuzzy match.5 ConclusionConfusion-network-based system combinationshows better performance than other methods incombining multiple MT systems?
outputs, andhypothesis alignment is a key step.
In this paper,we first compare four word alignment methodsfor hypothesis alignment under the confusionnetwork framework.
We verify that the confu-sion network framework is very effective in MTsystem combination and IHMM achieves the bestperformance.
Moreover, we propose an intersec-tion word alignment-based expansion method forhypothesis alignment, which is more reliable as itleverages on both direct and inverse word align-ment.
Experimental results on Chinese-to-English spoken and newswire domains show thatthe intersection word alignment-based methodyields consistent improvements across all fourword alignment methods.
Finally, we evaluatethe effect of fuzzy matching for TER.Theoretically, confusion network decoding isstill a word-level voting algorithm although it ismore complicated than other sentence-level vot-ing algorithms.
It changes lexical selection byconsidering the posterior probabilities of wordsin hypothesis lists.
Therefore, like other votingalgorithms, its performance strongly depends onthe quality of the n-best hypotheses of each sin-gle system.
In some extreme cases, it may not beable to improve BLEU-score (Mauser et al,2006; Sim et al, 2007).ReferencesN.
F. Ayan.
J. Zheng and W. Wang.
2008.
ImprovingAlignments for Better Confusion Networks forCombining Machine Translation Systems.
In Pro-ceedings of COLING 2008, pp.
33?40.
Manchester,Aug.S.
Bangalore, G. Bordel, and G. Riccardi.
2001.Computing consensus translation from multiplemachine translation systems.
In Proceeding ofIEEE workshop on Automatic Speech Recognitionand Understanding, pp.
351?354.
Madonna diCampiglio, Italy.B.
Chen, R. Cattoni, N. Bertoldi, M. Cettolo and M.Federico.
2005.
The ITC-irst SMT System forIWSLT-2005.
In Proceeding of IWSLT-2005,pp.98-104, Pittsburgh, USA, October.B.
Chen, M. Zhang, A. Aw and H. Li.
2008.
Regene-rating Hypotheses for Statistical Machine Transla-tion.
In: Proceeding of COLING 2008. pp105-112.Manchester, UK.
Aug.D.
Chiang.
2007.
Hierarchical phrase-based transla-tion.
Computational Linguistics, 33(2):201?228.C.
Fellbaum.
editor.
1998.
WordNet: An ElectronicLexical Database.
MIT Press.X.
He, M. Yang, J. Gao, P. Nguyen, R. Moore, 2008.Indirect-HMM-based Hypothesis Alignment forCombining Outputs from Machine TranslationSystems.
In Proceeding of EMNLP.
Hawaii, US,Oct.F.
Huang and K. Papinent.
2007.
Hierarchical SystemCombination for Machine Translation.
In Proceed-ings of the 2007 Joint Conference on EmpiricalMethods in Natural Language Processing and947Computational Natural Language Learning(EMNLP-CoNLL?2007), pp.
277 ?
286, Prague,Czech Republic, June.S.
Jayaraman and A. Lavie.
2005.
Multi-engine ma-chine translation guided by explicit word matching.In Proceeding of EAMT.
pp.143?152.D.
Karakos, J. Eisner, S. Khudanpur, and M. Dreyer.2008.
Machine Translation System Combinationusing ITG-based Alignments.
In Proceeding ofACL-HLT 2008, pp.
81?84.O.
Kraif, B. Chen.
2004.
Combining clues for lexicallevel aligning using the Null hypothesis approach.In: Proceedings of COLING 2004, Geneva, Au-gust, pp.
1261-1264.P.
Koehn, H. Hoang, A. Birch, C. Callison-Burch, M.Federico, N. Bertoldi, B. Cowan, W. Shen, C. Mo-ran, R. Zens, C. Dyer, O. Bojar, A. Constantin andE.
Herbst.
2007.
Moses: Open Source Toolkit forStatistical Machine Translation.
In Proceedings ofACL-2007.
pp.
177-180, Prague, Czech Republic.S.
Kumar and W. Byrne.
2004.
Minimum Bayes RiskDecoding for Statistical Machine Translation.
InProceedings of HLT-NAACL 2004, May 2004,Boston, MA, USA.G.
Leusch, N. Ueffing and H. Ney.
2006.
CDER: Ef-ficient MT Evaluation Using Block Movements.
InProceedings of EACL.
pp.
241-248.
Trento Italy.E.
Matusov, N. Ueffing, and H. Ney.
2006.
Compu-ting consensus translation from multiple machinetranslation systems using enhanced hypothesesalignment.
In Proceeding of EACL, pp.
33-40,Trento, Italy, April.E.
Matusov, G. Leusch, R. E. Banchs, N. Bertoldi, D.Dechelotte, M. Federico, M. Kolss, Y. Lee, J. B.Marino, M. Paulik, S. Roukos, H. Schwenk, and H.Ney.
System Combination for Machine Translationof Spoken and Written Language.
IEEE Transac-tions on Audio, Speech and Language Processing,volume 16, number 7, pp.
1222-1237, September.A.
Mauser, R. Zens, E. Matusov, S. Hasan, and H.Ney.
2006.
The RWTH Statistical Machine Trans-lation System for the IWSLT 2006 Evaluation.
InProceeding of IWSLT 2006, pp.
103-110, Kyoto,Japan, November.I.
D. Melamed.
2000.
Models of translational equiva-lence among words.
Computational Linguistics,26(2), pp.
221-249.F.
J. Och.
2003.
Minimum error rate training in statis-tical machine translation.
In Proceedings of ACL-2003.
Sapporo, Japan.F.
J. Och and H. Ney.
2003.
A systematic comparisonof various statistical alignment models.
Computa-tional Linguistics, 29(1):19-51.K.
Papineni, S. Roukos, T. Ward, and W.-J.
Zhu.2002.
BLEU: a method for automatic evaluation ofmachine translation.
In Proceeding of ACL-2002,pp.
311-318.A.
I. Rosti, N. F. Ayan, B. Xiang, S. Matsoukas, R.Schwartz and B. Dorr.
2007a.
Combining Outputsfrom Multiple Machine Translation Systems.
InProceeding of NAACL-HLT-2007, pp.
228-235.Rochester, NY.A.
I. Rosti, S. Matsoukas and R. Schwartz.
2007b.Improved Word-Level System Combination forMa-chine Translation.
In Proceeding of ACL-2007,Prague.A.
I. Rosti, B. Zhang, S. Matsoukas, and R. Schwartz.2008.
Incremental Hypothesis Alignment forBuilding Confusion Networks with Application toMachine Translation System Combination, In Pro-ceeding of the Third ACL Workshop on StatisticalMachine Translation, pp.
183-186.K.
C. Sim, W. J. Byrne, M. J.F.
Gales, H. Sahbi, andP.
C. Woodland.
2007.
Consensus network decod-ing for statistical machine translation system com-bination.
In Proceeding of  ICASSP-2007.M.
Snover, B. Dorr, R. Schwartz, L. Micciulla, and J.Makhoul.
2006.
A study of translation edit ratewith targeted human annotation.
In Proceeding ofAMTA.T.
Takezawa, E. Sumita, F. Sugaya, H. Yamamoto,and S. Yamamoto.
2002.
Toward a broad-coveragebilingual corpus for speech translation of travelconversations in the real world.
In Proceeding ofLREC-2002, Las Palmas de Gran Canaria, Spain.D.
Xiong, Q. Liu and S. Lin.
2006.
Maximum Entro-py Based Phrase Reordering Model for StatisticalMachine Translation.
In Proceeding of ACL-2006.pp.521-528.R.
Zens and H. Ney.
2006.
N-gram Posterior Prob-abilities for Statistical Machine Translation.
InProceeding of HLT-NAACL Workshop on SMT, pp.72-77, NY.M.
Zhang, H. Jiang, A. Aw, H. Li, C. L. Tan, and S.Li.
2008.
A Tree Sequence Alignment-based Tree-to-Tree Translation Model.
In Proceeding of ACL-2008.
Columbus, US.
June.Y.
Zhang, S. Vogel, and A. Waibel 2004.
InterpretingBLEU/NIST scores: How much improvement dowe need to have a better system?
In Proceedings ofLREC 2004, pp.
2051-2054.
*  The first author has moved to National ResearchCouncil, Canada.
His current email address is: Box-ing.Chen@nrc.ca.948
