Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 382?391,Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational LinguisticsIs a 204 cm Man Tall or Small ?Acquisition of Numerical Common Sense from the WebKatsuma Narisawa1 Yotaro Watanabe1 Junta Mizuno2Naoaki Okazaki1,3 Kentaro Inui11Graduate School of Information Sciences, Tohoku University2National Institute of Information and Communications Technology (NICT)3Japan Science and Technology Agency (JST){katsuma,yotaro-w,junta-m,okazaki,inui}@ecei.tohoku.ac.jpAbstractThis paper presents novel methods formodeling numerical common sense: theability to infer whether a given number(e.g., three billion) is large, small, or nor-mal for a given context (e.g., number ofpeople facing a water shortage).
We firstdiscuss the necessity of numerical com-mon sense in solving textual entailmentproblems.
We explore two approaches foracquiring numerical common sense.
Bothapproaches start with extracting numeri-cal expressions and their context from theWeb.
One approach estimates the distribu-tion of numbers co-occurring within a con-text and examines whether a given value islarge, small, or normal, based on the distri-bution.
Another approach utilizes textualpatterns with which speakers explicitly ex-presses their judgment about the value ofa numerical expression.
Experimental re-sults demonstrate the effectiveness of bothapproaches.1 IntroductionTextual entailment recognition (RTE) involves awide range of semantic inferences to determinewhether the meaning of a hypothesis sentence (h)can be inferred from another text (t) (Dagan etal., 2006).
Although several evaluation campaigns(e.g., PASCAL/TAC RTE challenges) have madesignificant progress, the RTE community recog-nizes the necessity of a deeper understanding ofthe core phenomena involved in textual inference.Such recognition comes from the ideas that cru-cial progress may derive from decomposing thecomplex RTE task into basic phenomena and fromsolving each basic phenomenon separately (Ben-tivogli et al, 2010; Sammons et al, 2010; Cabrioand Magnini, 2011; Toledo et al, 2012).Given this background, we focus on solving oneof the basic phenomena in RTE: semantic infer-ence related to numerical expressions.
The spe-cific problem we address is acquisition of numeri-cal common sense.
For example,(1) t : Before long, 3b people will face a watershortage in the world.h : Before long, a serious water shortagewill occur in the world.Although recognizing the entailment relation be-tween t and h is frustratingly difficult, we assumethis inference is decomposable into three phases:3b people face a water shortage.?
3,000,000,000 people face a water shortage.|= many people face a water shortage.|= a serious water shortage.In the first phase, it is necessary to recognize 3bas a numerical expression and to resolve the ex-pression 3b into the exact amount 3,000,000,000.The second phase is much more difficult becausewe need subjective but common-sense knowledgethat 3,000,000,000 people is a large number.In this paper, we address the first and sec-ond phases of inference as an initial step towardssemantic processing with numerical expressions.The contributions of this paper are four-fold.1.
We examine instances in existing RTE cor-pora, categorize them into groups in terms ofthe necessary semantic inferences, and dis-cuss the impact of this study for solving RTEproblems with numerical expressions.2.
We describe a method of normalizing numer-ical expressions referring to the same amountin text into a unified semantic representation.3.
We present approaches for aggregating nu-merical common sense from examples of nu-merical expressions and for judging whethera given amount is large, small, or normal.3824.
We demonstrate the effectiveness of this ap-proach, reporting experimental results andanalyses in detail.
Although it would be idealto evaluate the impact of this study on theoverall RTE task, we evaluate each phase sep-arately.
We do this because the existing RTEdata sets tend to exhibit very diverse linguis-tic phenomena, and it is difficult to employsuch data for evaluating the real impact ofthis study.2 Related workSurprisingly, NLP research has paid little atten-tion to semantic processing of numerical expres-sions.
This is evident when we compare with tem-poral expressions, for which corpora (e.g., ACE-20051, TimeBank2) were developed with annota-tion schemes (e.g., TIMEX3, TimeML4).Several studies deal with numerical expressionsin the context of information extraction (Bakalovet al, 2011), information retrieval (Fontoura et al,2006; Yoshida et al, 2010), and question answer-ing (Moriceau, 2006).
Numbers such as prod-uct prices and weights have been common targetsof information extraction.
Fontoura et al (2006)and Yoshida et al (2010) presented algorithms anddata structures that allow number-range queriesfor searching documents.
However, these studiesdo not interpret the quantity (e.g., 3,000,000,000)of a numerical expression (e.g., 3b people), butrather treat numerical expressions as strings.Banerjee et al (2009) focused on quantity con-sensus queries, in which there is uncertainty aboutthe quantity (e.g., weight airbus A380 pounds).Given a query, their approach retrieves documentsrelevant to the query and identifies the quantitiesof numerical expressions in the retrieved docu-ments.
They also proposed methods for enumer-ating and ranking the candidates for the consen-sus quantity intervals.
Even though our studyshares a similar spirit (modeling of consensus forquantities) with Banerjee et al (2009), their goalis different: to determine ground-truth values forqueries.In question answering, to help ?sanity check?answers with numerical values that were1http://www.itl.nist.gov/iad/mig/tests/ace/ace05/2http://www.timeml.org/site/timebank/timebank.html3http://timex2.mitre.org/4http://timeml.org/site/index.htmlway out of common-sense ranges, IBM?s PI-QUANT (Prager et al, 2003; Chu-Carroll et al,2003) used information in Cyc (Lenat, 1995).For example, their question-answering systemrejects 200 miles as a candidate answer for theheight of Mt.
Everest, since Cyc knows mountainsare between 1,000 and 30,000 ft. high.
Theyalso consider the problem of variations in theprecision of numbers (e.g., 5 million, 5.1 million,5,200,390) and unit conversions (e.g., squarekilometers and acres).Some recent studies delve deeper into the se-mantic interpretation of numerical expressions.Aramaki et al (2007) focused on the physical sizeof an entity to predict the semantic relation be-tween entities.
For example, knowing that a bookhas a physical size of 20 cm ?
25 cm and that a li-brary has a size of 10 m ?
10 m, we can estimatethat a library contains a book (content-containerrelation).
Their method acquires knowledge aboutentity size from the Web (by issuing queries like?book (*cm x *cm)?
), and integrates the knowl-edge as features for the classification of relations.Davidov and Rappoport (2010) presented amethod for the extraction from the Web and ap-proximation of numerical object attributes such asheight and weight.
Given an object-attribute pair,the study expands the object into a set of compa-rable objects and then approximates the numericalvalues even when no exact value can be found in atext.
Aramaki et al (2007) and Davidov and Rap-poport (2010) rely on hand-crafted patterns (e.g.,?Object is * [unit] tall?
), focusing on a specific setof numerical attributes (e.g., height, weight, size).In contrast, this study can handle any kind of targetand situation that is quantified by numbers, e.g.,number of people facing a water shortage.Recently, the RTE community has started topay some attention to the appropriate processingof numerical expressions.
Iftene (2010) presentedan approach for matching numerical ranges ex-pressed by a set of phrases (e.g., more than and atleast).
Tsuboi et al (2011) designed hand-craftedrules for matching intervals expressed by temporalexpressions.
However, these studies do not nec-essarily focus on semantic processing of numeri-cal expressions; thus, these studies do not normal-ize units of numerical expressions nor make infer-ences with numerical common sense.Sammons et al (2010) reported that most sys-tems submitted to RTE-5 failed on examples383where numeric reasoning was necessary.
They ar-gued the importance of aligning numerical quanti-ties and performing numerical reasoning in RTE.LoBue and Yates (2011) identified 20 categoriesof common-sense knowledge that are prevalent inRTE.
One of the categories comprises arithmeticknowledge (including computations, comparisons,and rounding).
They concluded that many kindsof the common-sense knowledge have receivedscarce attention from researchers even though theknowledge is essential to RTE.
These studies pro-vided a closer look at the phenomena involved inRTE, but they did not propose a solution for han-dling numerical expressions.3 Investigation of textual-entailmentpairs with numerical expressionsIn this section, we investigate textual entailment(TE) pairs in existing corpora in order to studythe core phenomena that establish an entailmentrelation.
We used two Japanese TE corpora:RITE (Shima et al, 2011) and Odani et al (2008).RITE is an evaluation workshop of textual entail-ment organized by NTCIR-9, and it targets theEnglish, Japanese, and Chinese languages.
Weused the Japanese portions of the developmentand training data.
Odani et al (2008) is anotherJapanese corpus that was manually created.
Thetotal numbers of text-hypothesis (T -H) pairs are1,880 (RITE) and 2,471 (Odani).We manually selected sentence pairs in whichone or both of the sentences contained a numericalexpression.
Here, we define the term numericalexpression as an expression containing a numberor quantity represented by a numeral and a unit.For example, 3 kilometers is a numerical expres-sion with the numeral 3 and the unit kilometer.Note that intensity of 4 is not a numerical expres-sion because intensity is not a unit.We obtained 371 pairs from the 4,351 T -Hpairs.
We determined the inferences needed toprove ENTAILMENT or CONTRADICTION of thehypotheses, and classified the 371 pairs into 11categories.
Note that we ignored T -H pairs inwhich numerical expressions were unnecessaryto prove the entailment relation (e.g., Socrateswas sentenced to death by 500 jury members andSocrates was sentenced to death).
Out of 371pairs, we identified 114 pairs in which numericalexpressions played a central role in the entailmentrelation.Table 1 summarizes the categories of TE phe-nomena we found in the data set.
The largest cate-gory is numerical matching (32 pairs).
We can in-fer an entailment relation in this category by align-ing two numerical expressions, e.g., 2.2 million|= over 800 thousand.
This is the most funda-mental task in numerical reasoning, interpretingthe amount (number, unit, and range) in a numer-ical expression.
We address this task in Section4.1.
The second largest category requires com-mon sense about numerical amounts.
In order torecognize textual entailment of pairs in this cat-egory, we need common-sense knowledge abouthumans?
subjective judgment of numbers.
Weconsider this problem in Section 5.To summarize, this study covers 37.9% of theinstances in Table 1, focusing on the first and sec-ond categories.
Due to space limitations, we omitthe explanations for the other phenomena, whichrequire such things as lexical knowledge, arith-metic operations, and counting.
The coverage ofthis study might seem small, but it is difficult tohandle varied phenomena with a unified approach.We believe that this study forms the basis for in-vestigating other phenomena of numerical expres-sions in the future.4 Collecting numerical expressions fromthe WebIn this paper, we explore two approaches to acquir-ing numerical common sense.
Both approachesstart with extracting numerical expressions andtheir context from the Web.
We define a contextas the verb and its arguments that appear around anumerical expression.For instance, the context of 3b people in the sen-tence 3b people face a water shortage is ?face?and ?water shortage.?
In order to extract andaggregate numerical expressions in various doc-uments, we converted the numerical expressionsinto semantic representations (to be described inSection 4.1), and extracted their context (to be de-scribed in Section 4.2).The first approach for acquiring numerical com-mon sense estimates the distribution of numbersthat co-occur within a context, and examineswhether a given value is large, small, or normalbased on that distribution (to be described in Sec-tion 5.1).
The second approach utilizes textualpatterns with which speakers explicitly expressestheir judgment about the value of a numerical ex-384Category Definition Example #Numerical matchingAligning numerical expres-sions in T and H, consideringdifferences in unit, range, etc.t: It is said that there are about 2.2 million alcoholics in the whole country.h: It is estimated that there are over 800 thousand people who are alcoholics.
32Numerical common senseInferring by interpreting thenumerical amount (large orsmall).t: In the middle of the 21st century, 7 billion people, corresponding to 70% of theglobal population, will face a water shortage.h: It is concerning that a serious water shortage will spread around the world in thenear future.12Lexical knowledge Inferring by using numericalaspects of word meanings.t: Mr. and Ms. Sato celebrated their 25th wedding anniversary.h: Mr. and Ms. Sato celebrated their silver wedding anniversary.
12Arithmetic Arithmetic operations includ-ing addition and subtraction.t: The number of 2,000-yen bills in circulation has increased to 450 million, incontrast with 440 million 5,000-yen bills.h: The number of 2,000-yen bills in circulation exceeds the number of 5,000-yenbills by 10 million bills.11Numeric-range expressionof verbsNumerical ranges expressed byverbs (e.g., exceed).t: It is recorded that the maximum wave height reached 13.8 meters during the Seaof Japan Earthquake Tsunami in May 1983.h: During the Sea of Japan Earthquake, the height of the tsunami exceeded 10meters.9Simple Rewrite Rule This includes various simplerules for rewriting.t: The strength of Taro?s grip is No.
1 in his class.h: Taro?s grip is the strongest in his class.
7State change Expressing the change of avalue by a multiplier or ratio.t: Consumption of pickled plums is 1.5 times the rate of 20 years ago.h: Consumption of pickled plums has increased.
6Ordinal numbers Inference by interpreting ordi-nal numbers.t: Many precious lives were sacrificed in the Third World War.h: So far, there have been at least three World Wars.
6Temporal expressionInference by interpreting tem-poral expressions such as an-niversary, age, and ordinalnumbers.t: Mr. and Ms. Sato celebrate their 25th wedding anniversary.h: Mr. and Ms. Sato got married 25 years ago.
3Count Counting up the number of var-ious entities.t: In Japan, there are the Asian Triopsidae, the American Triopsidae, and the Euro-pean Triopsidae.h: In Japan, there are 3 types of Triopsidae.3Others 15All 116Table 1: Frequency and simple definitions for each category of the entailment phenomena in the survey.Numerical Semantic representationExpression Value Unit Mod.about seven grams 7 g aboutroughly 7 kg 7000 g aboutas heavy as 7 tons 7?
106 g largeas cheap as $1 1 $ small30?40 people [30, 40] nin (people)more than 30 cars 30 dai (cars) over7 km per hour 7000 m/hTable 2: Normalized representation examplespression (to be explained in Section 5.2).In this study, we acquired numerical commonsense from a collection of 8 billion sentences in100 million Japanese Web pages (Shinzato et al,2012).
For this reason, we originally designedtext patterns specialized for Japanese dependencytrees.
For the sake of the readers?
understand-ing, this paper uses examples with English trans-lations for explaining language-independent con-cepts, and both Japanese and English translationsfor explaining language-dependent concepts.4.1 Extracting and normalizing numericalexpressionsThe first step for collecting numerical expres-sions is to recognize when a numerical expressionis mentioned and then to normalize it into a seman-tic representation.
This is the most fundamentalString Operationgram(s) set-unit: ?g?kilogram(s) set-unit: ?g?
; multiply-value: 1,000kg set-unit: ?g?
; multiply-value: 1,000ton(s) set-unit: ?g?
; multiply-value: 1,000,000nin (people) set-unit: ?nin?
(person)about set-modifier: ?about?as many as set-modifier: ?large?as little as set-modifier: ?small?Table 3: An example of unit/modifier dictionarystep in numerical reasoning and has a number ofapplications.
For example, this step handles casesof numerical matching, as in Table 1.The semantic representation of a numerical ex-pression consists of three fields: the value or rangeof the real number(s)5, the unit (a string), and theoptional modifiers.
Table 2 shows some exam-ples of numerical expressions and their semanticrepresentations.
During normalization, we identi-fied spelling variants (e.g., kilometer and km) andtransformed auxiliary units into their correspond-ing canonical units (e.g., 2 tons and 2,000 kg to2,000,000 grams).
When a numerical expressionis accompanied by a modifier such as over, about,or more than, we updated the value and modifierfields appropriately.5Internally, all values are represented by ranges (e.g., 75is represented by the range [75, 75]).385We developed an extractor and a normalizer forJapanese numerical expressions6.
We will outlinethe algorithm used in the normalizer with an exam-ple sentence: ?Roughly three thousand kilogramsof meats have been provided every day.?1.
Find numbers in the text by using regular ex-pressions and convert the non-Arabic num-bers into their corresponding Arabic num-bers.
For example, we find three thousand7and represent it as 3, 000.2.
Check whether the words that precede or fol-low the number are units that are registered inthe dictionary.
Transform any auxiliary units.In the example, we find that kilograms8 is aunit.
We multiply the value 3, 000 by 1, 000,and obtain the value 3, 000, 000 with the unitg.3.
Check whether the words that precede or fol-low the number have a modifier that is regis-tered in the dictionary.
Update the value andmodifier fields if necessary.
In the example,we find roughly and set about in the modifierfield.We used a dictionary9 to perform procedures 2and 3 (Table 3).
If the words that precede or fol-low an extracted number match an entry in the dic-tionary, we change the semantic representation asdescribed in the operation.The modifiers ?large?
and ?small?
require elab-oration because the method in Section 5.2 reliesheavily on these modifiers.
We activated the mod-ifier ?large?
when a numerical expression occurredwith the Japanese word mo, which roughly cor-responds to as many as, as large as, or as heavyas in English10.
Similarly, we activated the modi-fier ?small?
when a numerical expression occurredwith the word shika, which roughly correspondsto as little as, as small as, or as light as11.
Thesemodifiers are important for this study, reflectingthe writer?s judgment about the amount.6The software is available at http://www.cl.ecei.tohoku.ac.jp/?katsuma/software/normalizeNumexp/7In Japanese 3, 000 is denoted by the Chinese symbols ???
?.8We write kilograms as ???????
in Japanese.9The dictionary is bundled with the tool.
See Footnote 6.10In Japanese, we can use the word mo with a numericalexpression to state that the amount is ?large?
regardless ofhow large it is (e.g., large, big, many, heavy).11Similarly, we can use the word shika with any adjective.??
???
???
?????
???
?He gave to a friend$300 at the bank.Japanese:English:nsubj dobj prep_toprep_atNumber: {value: 300; unit: ?$?
}Context: {verb: ?give?
; nsubj: ?he?
;prep_to: ?friend?
; prep_at: ?bank?
}Figure 1: Example of context extraction4.2 Extraction of contextThe next step in acquiring numerical commonsense is to capture the context of numerical ex-pressions.
Later, we will aggregate numbers thatshare the same context (see Section 5).
The con-text of a numerical expression should provide suf-ficient information to determine what it measures.For example, given the sentence, ?He gave $300 toa friend at the bank,?
it would be better if we couldgeneralize the context to someone gives money toa friend for the numerical expression $300.
How-ever, it is a nontrivial task to design an appropriaterepresentation of varying contexts.
For this rea-son, we employ a simple rule to capture the con-text of numerical expressions: we represent thecontext with the verb that governs the numericalexpression and its typed arguments.Figure 1 illustrates the procedure for extractingthe context of a numerical expression12.
The com-ponent in Section 4.1 recognizes $300 as a numer-ical expression, then normalizes it into a semanticrepresentation.
Because the numerical expressionis a dependent of the verb gave, we extract the verband its arguments (except for the numerical ex-pression itself) as the context.
After removing in-flections and function words from the arguments,we obtain the context representation of Figure 1.5 Acquiring numerical common senseIn this section, we present two approaches for ac-quiring numerical common sense from a collec-tion of numerical expressions and their contexts.Both approaches start with collecting the numbers(in semantic representation) and contexts of nu-merical expressions from a large number of sen-tences (Shinzato et al, 2012), and storing them12The English dependency tree might look peculiar be-cause it is translated from the Japanese dependency tree.386in a database.
When a context and a value aregiven for a prediction (hereinafter called the querycontext and query value, respectively), these ap-proaches judge whether the query value is large,small, or normal.5.1 Distribution-based approachGiven a query context and query value, thisapproach retrieves numbers associated with thequery context and draws a distribution of normal-ized numbers.
This approach considers the dis-tribution estimated for the query context and de-termines if the value is within the top 5 percent(large), within the bottom 5 percent (small), or islocated in between these regions (normal).The underlying assumption of this approach isthat the real distribution of a query (e.g., moneygiven to a friend) can be approximated by the dis-tribution of numbers co-occurring with the context(e.g., give and friend) on the Web.
However, thecontext space generated in Section 4.2 may be toosparse to find numbers in the database, especiallywhen a query context is fine-grained.
Therefore,when no item is retrieved for the query context,we employ a backoff strategy to drop some of theuninformative elements in the query context: ele-ments are dropped from the context based on thetype of argument, in this order: he (prep to), kara(prep from), ha (nsubj), yori (prep from), made(prep to), nite (prep at), de (prep at, prep by), ni(prep at), wo (dobj), ga (nsubj), and verb.5.2 Clue-based approachThis approach utilizes textual clues with which aspeaker explicitly expresses his or her judgmentabout the amount of a numerical expression.
Weutilize large and small modifiers (described in Sec-tion 4.1), which correspond to textual clues mo(as many as, as large as) and shika (only, asfew as), respectively, for detecting humans?
judg-ments.
For example, we can guess that $300 islarge if we find an evidential sentence13, He gaveas much as $100 to a friend.Similarly to the distribution-based approach,this approach retrieves numbers associated withthe query context.
This approach computes the13Although the sentence states a judgment about $100, wecan infer that $300 is also large because $300 > $100.largeness L(x) of a value x:L(x) = pl(x)ps(x) + pl(x), (1)pl(x) =??
{r|rv < x ?
rm 3 large}????
{r|rm 3 large}??
, (2)ps(x) =??
{r|rv > x ?
rm 3 small}????
{r|rm 3 small}??
.
(3)In these equations, r denotes a retrieved item forthe query context, and rv and rm represent the nor-malized value and modifier flags, respectively, ofthe item r. The numerator of Equation 2 countsthe number of numerical expressions that supportthe judgment that x is large14, and its denominatorcounts the total number of numerical expressionswith large as a modifier.
Therefore, pl(x) com-putes the ratio of times there is textual evidencethat says that x is large, to the total number oftimes there is evidences with large as a modifier.In an analogous way, ps(x) is defined to be the ra-tio for evidence that says x is small.
Hence, L(x)approaches 1 if everyone on the Web claims thatx is large, and approaches 0 if everyone claimsthat x is small.
This approach predicts large ifL(x) > 0.95, small if L(x) < 0.05, and normalotherwise.6 Experiments6.1 Normalizing numerical expressionsWe evaluated the method that we described in Sec-tion 4.1 for extracting and normalizing numericalexpressions.
In order to prepare a gold-standarddata set, we obtained 1,041 sentences by randomlysampling about 1% of the sentences containingnumbers (Arabic digits and/or Chinese numericalcharacters) in a Japanese Web corpus (100 millionpages) (Shinzato et al, 2012).
For every numer-ical expression in these sentences, we manuallydetermined a tuple of the normalized value, unit,and modifier.
Here, non-numerical expressionssuch as temporal expressions, telephone numbers,and postal addresses, which were very common,were beyond the scope of the project15.
We ob-tained 329 numerical expressions from the 1,041sentences.We evaluated the correctness of the extractionand normalization by measuring the precision and14This corresponds to the events where we find an evidenceexpression ?as many as rv?, where rv < x.15If a tuple was extracted from a non-numerical expres-sion, we regarded this as a false positive387recall using the gold-standard data set16.
Ourmethod performed with a precision of 0.78 and arecall of 0.92.
Most of the false negatives werecaused by the incompleteness of the unit dictio-nary.
For example, the proposed method could notidentify 1Ghz as a numerical expression becausethe unit dictionary did not register Ghz but GHz.It is trivial to improve the recall of the method byenriching the unit dictionary.The major cause of false positives was the se-mantic ambiguity of expressions.
For example, theproposed method identified Seven Hills as a nu-merical expression although it denotes a locationname.
In order to reduce false positives, it maybe necessary to utilize broader contexts when lo-cating numerical expressions; this could be doneby using, for example, a named entity recognizer.This is the next step to pursue in future work.However, these errors do not have a large effecton the estimation of the distribution of the numer-ical values that occur with specific named entitiesand idiomatic phrases.
Moreover, as explained inSection 5, we draw distributions for fine-grainedcontexts of numerical expressions.
For these rea-sons, we think that the current performance is suf-ficient for acquiring numerical common sense.6.2 Acquisition of numerical common sense6.2.1 Preparing an evaluation setWe built a gold-standard data set for numericalcommon sense.
We applied the method in Sec-tion 4.1 to sentences sampled at random from theJapanese Web corpus (Shinzato et al, 2012), andwe extracted 2,000 numerical expressions.
Weasked three human judges to annotate every nu-merical expression with one of six labels, small,relatively small, normal, relatively large, large,and unsure.
The label relatively small could beapplied to a numerical expression when the judgefelt that the amount was rather small (below thenormal) but hesitated to label it small.
The la-bel relatively large was defined analogously.
Wegave the following criteria for labeling an item asunsure: when the judgment was highly dependenton the context; when the sentence was incompre-hensible; and when it was a non-numerical expres-sions (false positives of the method are discussedin Section 4.1).Table 4 reports the inter-annotator agreement.16All fields (value, unit, modifier) of the extracted tuplemust match the gold-standard data set.Agreement # expressions3 annotators 735 (36.7%)2 annotators 963 (48.2%)no agreement 302 (15.1%)Total 2000 (100.0%)Table 4: Inter-annotator agreement0"100"200"300"400"500"0"100"200"130" 140" 150" 160" 170" 180" 190" 200" 210"[cm]distribu7on:based"clue:based(large)"clue:based(small)"[#"extrac7on]"(distribu7on:based)[#"extrac7on]"(clue:based)Figure 2: Distributions of numbers with large andsmall modifiers for the context human?s height.For the evaluation of numerical expressions in thedata set, we used those for which at least two anno-tators assigned the same label.
After removing theunsure instances, we obtained 640 numerical ex-pressions (20 small, 35 relatively small, 152 nor-mal, 263 relatively large, and 170 large) as theevaluation set.6.2.2 ResultsThe proposed method extracted about 23 millionpairs of numerical expressions and their contextfrom the corpus (with 100 million Web pages).About 15% of the extracted pairs were accom-panied by either a large or small modifier.
Fig-ure 2 depicts the distributions of the context hu-man?s height produced by the distribution-basedand clue-based approaches.
These distributionsare quite reasonable as common-sense knowledge:we can interpret that numbers under 150 cm areperceived as small and those above 180 cm aslarge.We measured the correctness of the proposedmethods on the gold-standard data.
For thisevaluation, we employed two criteria for correct-ness: strict and lenient.
With the strict crite-rion, the method must predict a label identical tothat in the gold-standard.
With the lenient crite-rion, the method was also allowed to predict eitherlarge/small or normal when the gold-standard la-bel was relatively large/small.Table 5 reports the precision (P), recall (R), F1(F1), and accuracy (Acc) of the proposed methods.388No.
System Gold Sentence Remark1 small smallI think that three men cancreate such a great thing inthe world.Correct2 normal normal I have two cats.
Correct3 large large It?s above 32 centigrade.
Correct4 large large I earned 10 million yen fromhorse racing.
Correct5 small normal There are 2 reasons.
Difficulty in judging small.
Since a few people say, ?There areonly 2 reasons,?
our approach predicted a small label.6 small largeTen or more people came,and my eight-mat room waspacked.Difficulty in modeling the context because this sentence omitsthe locational argument for the verb came.
We should extractthe context as the number of people who came to my eight-matroom instead of the number of people who came.7 small normalI have two friends whohave broken up with theirboyfriends recently.Difficulty in modeling the context.
We should extract context asthe number of friends who have broken up with their boyfriendsrecently instead of the number of friends.8 small largeLack of knowledge.
We extract the context as the number ofheads of a turtle, but no corresponding information was foundon the Web.Table 6: Output example and error analysis.
We present translations of the sentences, which were origi-nally in Japanese.Approach Label P R F1 Acclarge+ 0.892 0.498 0.695Distribution normal+ 0.753 0.935 0.844 0.760small+ 0.273 0.250 0.262large 0.861 0.365 0.613Distribution normal 0.529 0.908 0.719 0.590small 0.222 0.100 0.161large+ 0.923 0.778 0.851Clue normal+ 0.814 0.765 0.790 0.770small+ 0.228 0.700 0.464large 0.896 0.659 0.778Clue normal 0.593 0.586 0.590 0.620small 0.164 0.550 0.357Table 5: Precision (P), recall (R), F1 score (F1),and accuracy (Acc) of the acquisition of numericalcommon sense.Labels with the suffix ?+?
correspond to the lenientcriterion.
The clue-based approach achieved 0.851F1 (for large), 0.790 F1 (for normal), and 0.464(for small) with the lenient criterion.
The perfor-mance is surprisingly good, considering the sub-jective nature of this task.The clue-based approach was slightly betterthan the distribution-based approach.
In particu-lar, the clue-based approach is good at predictinglarge and small labels, whereas the distribution-based approach is good at predicting normal la-bels.
We found some targets for which the distri-bution on the Web is skewed from the ?real?
dis-tribution.
For example, let us consider the distri-bution of the context ?the amount of money that aperson wins in a lottery?.
We can find a numberof sentences like if you won the 10-million-dollarlottery, ....
In other words, people talk about alarge amount of money even if they did not winany money at all.
In order to remedy this problem,we may need to enrich the context representationby introducing, for example, the factuality of anevent.6.2.3 DiscussionTable 6 shows some examples of predictions fromthe clue-based approach.
Because of space limita-tions, we mention only the false instances of thisapproach.The clue-based approach tends to predict smalleven if the gold-standard label is normal.
Abouthalf of the errors of the clue-based approach wereof this type; this is why the precision for small andthe recall for normal are low.
The cause of this er-ror is exemplified by the sentence, ?there are tworeasons.?
Human judges label normal to the nu-merical expression two reasons, but the methodpredicts small.
This is because a few people saythere are only two reasons, but no one says thereare as many as two reasons.
In order to handlethese cases, we may need to incorporate the distri-bution information with the clue-based approach.We found a number of examples for whichmodeling the context is difficult.
Our approachrepresents the context of a numerical expressionwith the verb that governs the numerical expres-sion and its typed arguments.
However, this ap-proach sometimes misses important information,especially when an argument of the verb is omit-ted (Example 6).
The approach also suffers fromthe relative clause in Example 7, which conveys anessential context of the number.
These are similarto the scope-ambiguity problem such as encoun-389tered with negation and quantification; it is diffi-cult to model the scope when a numerical expres-sion refers to a situation.Furthermore, we encountered some false exam-ples even when we were able to precisely modelthe context.
In Example 8, the proposed methodwas unable to predict the label correctly becauseno corresponding information was found on theWeb.
The proposed method might more easily pre-dict a label if we could generalize the word turtleas animal.
It may be worth considering using lan-guage resources (e.g., WordNet) to generalize thecontext.7 ConclusionsWe proposed novel approaches for acquiring nu-merical common sense from a collection of texts.The approaches collect numerical expressions andtheir contexts from the Web, and acquire numeri-cal common sense by considering the distributionsof normalized numbers and textual clues such asmo (as many as) and shika (only, as few as).
Theexperimental results showed that our approachescan successfully judge whether a given amountis large, small, or normal.
The implementationsand data sets used in this study are available onthe Web17.
We believe that acquisition of numer-ical common sense is an important step towards adeeper understanding of inferences with numbers.There are three important future directions forthis research.
One is to explore a more sophis-ticated approach for precisely modeling the con-texts of numbers.
Because we confirmed in thispaper that these two approaches have differentcharacteristics, it would be interesting to incorpo-rate textual clues into the distribution-based ap-proach by using, for example, machine learningtechniques.
Finally, we are planning to address the?third phase?
of the example explained in Section1: associating many people face a water shortagewith a serious water shortage.AcknowledgmentsThis research was partly supported by JST,PRESTO.
This research was partly supported byJSPS KAKENHI Grant Numbers 23240018 and23700159.17http://www.cl.ecei.tohoku.ac.jp/?katsuma/resource/numerical common sense/ReferencesEiji Aramaki, Takeshi Imai, Kengo Miyo, andKazuhiko Ohe.
2007.
Uth: Svm-based semanticrelation classification using physical sizes.
In Pro-ceedings of the 4th International Workshop on Se-mantic Evaluations, pages 464?467.Anton Bakalov, Ariel Fuxman, Partha Pratim Talukdar,and Soumen Chakrabarti.
2011.
SCAD: collectivediscovery of attribute values.
In Proceedings of the20th international conference on World wide web,WWW ?11, pages 447?456.Somnath Banerjee, Soumen Chakrabarti, and GaneshRamakrishnan.
2009.
Learning to rank for quantityconsensus queries.
In Proceedings of the 32nd inter-national ACM SIGIR conference on Research anddevelopment in information retrieval, SIGIR ?09,pages 243?250.Luisa Bentivogli, Elena Cabrio, Ido Dagan, DaniloGiampiccolo, Medea Lo Leggio, and BernardoMagnini.
2010.
Building textual entailment special-ized data sets: a methodology for isolating linguis-tic phenomena relevant to inference.
Proceedings ofthe Seventh International Conference on LanguageResources and Evaluation, pages 3542?3549.Elena Cabrio and Bernardo Magnini.
2011.
Towardscomponent-based textual entailment.
In Proceed-ings of the Ninth International Conference on Com-putational Semantics, IWCS ?11, pages 320?324.Jennifer Chu-Carroll, David A. Ferrucci, John M.Prager, and Christopher A. Welty.
2003.
Hybridiza-tion in question answering systems.
In New Direc-tions in Question Answering?03, pages 116?121.Ido Dagan, Oren Glickman, and Bernardo Magnini.2006.
The pascal recognising textual entailmentchallenge.
In Machine Learning Challenges.
Eval-uating Predictive Uncertainty, Visual Object Classi-fication, and Recognising Tectual Entailment, pages177?190.Dmitry Davidov and Ari Rappoport.
2010.
Extrac-tion and approximation of numerical attributes fromthe web.
In Proceedings of the 48th Annual Meet-ing of the Association for Computational Linguis-tics, pages 1308?1317.Marcus Fontoura, Ronny Lempel, Runping Qi, and Ja-son Zien.
2006.
Inverted index support for numericsearch.
Internet Mathematics, 3(2):153?185.Adrian Iftene and Mihai-Alex Moruz.
2010.
UAICparticipation at RTE-6.
In Proceedings of the ThirdText Analysis Conference (TAC 2010) November.Douglas B Lenat.
1995.
Cyc: A large-scale investmentin knowledge infrastructure.
Communications of theACM, 38(11):33?38.Peter LoBue and Alexander.
Yates.
2011.
Types ofcommon-sense knowledge needed for recognizing390textual entailment.
In Proceedings of the 49th An-nual Meeting of the Association for ComputationalLinguistics: Human Language Technologies: shortpapers-Volume 2, pages 329?334.Ve?ronique Moriceau.
2006.
Generating intelligentnumerical answers in a question-answering system.In Proceedings of the Fourth International NaturalLanguage Generation Conference, INLG ?06, pages103?110.Michitaka Odani, Tomohide Shibata, Sadao Kurohashi,and Takayuki Nakata.
2008.
Building data ofjapanese text entailment and recognition of infer-encing relation based on automatic achieved similarexpression.
In Proceeding of 14th Annual Meetingof the Association for ?atural Language Processing,pages 1140?1143.John M. Prager, Jennifer Chu-Carroll, KrzysztofCzuba, Christopher A. Welty, Abraham Ittycheriah,and Ruchi Mahindru.
2003.
IBM?s PIQUANT inTREC2003.
In TREC, pages 283?292.Mark Sammons, Vinod V.G.
Vydiswaran, and DanRoth.
2010.
Ask not what textual entailment can dofor you...
In Proceedings of the 48th Annual Meet-ing of the Association for Computational Linguis-tics, pages 1199?1208.Hideki Shima, Hiroshi Kanayama, Cheng-Wei Lee,Chuan-Jie Lin, Teruko Mitamura, Yusuke Miyao,Shuming Shi, and Koichi Takeda.
2011.
Overviewof ntcir-9 rite: Recognizing inference in text.
In Pro-ceeding of NTCIR-9 Workshop Meeting, pages 291?301.Keiji Shinzato, Tomohide Shibata, Daisuke Kawahara,and Sadao Kurohashi.
2012.
Tsubaki: An opensearch engine infrastructure for developing informa-tion access methodology.
Journal of InformationProcessing, 20(1):216?227.Assaf Toledo, Sophia Katrenko, Stavroula Alexan-dropoulou, Heidi Klockmann, Asher Stern, Ido Da-gan, and Yoad Winter.
2012.
Semantic annotationfor textual entailment recognition.
In Proceedings ofthe 11th Mexican International Conference on Arti-ficial Intelligence, MICAI ?12.Yuta Tsuboi, Hiroshi Kanayama, Masaki Ohno, andYuya Unno.
2011.
Syntactic difference based ap-proach for ntcir-9 rite task.
In Proceedings of the9th NTCIR Workshop, pages 404?411.Minoru Yoshida, Issei Sato, Hiroshi Nakagawa, andAkira Terada.
2010.
Mining numbers in text usingsuffix arrays and clustering based on dirichlet pro-cess mixture models.
Advances in Knowledge Dis-covery and Data Mining, pages 230?237.391
