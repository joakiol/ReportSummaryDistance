I-IFEASIBILITYThis paper discusses the feasibility of applying amodel of language use based on a modification and extension(to be discussed below) of the generative semantic(transformational) theory of language competence recentlydeveloped by Paul Postal, George Lakoff, John Robert Ross,~ames D. McCawley, and others, to problems of computation-al linguistics.The theory of generative semantics, to be discussedin section II, is an outgrowth of, and reaction to,Chomsky's 1965 theory of transformational linguistics.It is a radical theory which deals with a very greatrange of problems with very abstract methods.
Troseworking in this paradigm hold that there is a linguist-ic level reflecting conceptual or semantic structurewhich is directly convertible into surface syntax by asingle set of garden-variety transformations, with noO significant intermediary level, that is, no deepstructure".
These of us working in generative semanticsbelieve that methods substantially those long familiarin linguistics can achieve very absract , very generalresults which treat semantics in a more serious and en-lightening way than ever before.
I do not, I think,support this very strong claim very well in section II,but I provide summaries of several studies and a lengthybibliogrpahy of works which when consulted will hopefullygive some feeling for what is being attempted, I thinknot without results.But generative semantics is a model, or rather, atheory, of competence, like most serious theories oflanguage now held to by American linguists.
~ven if,as might be claimed, our semantic structures are to bemerely variants of the structures long familiar fromformal logic, so that if our assumptions are correct,we will ultimately be able to directly transformsurface structures into underlying semantic structures,the majority of actual sentences, as well as all hyper-sentential structures, the treatment of which has beenswept under the rug of "performance", will remain unhandle-able.Accordingly, I propose initially cert@in extensionsand modifications of the theory to make it in some sense1-2a model of performance.
But if we are to apply it to thecomputer, a major component must still be added.
The impe-tus to this application is ~he possibility of creatingan understanding machine, dewcribed in section IV below.Since the actual human interpretation of language dependson past knowledge (consider which of these sentences isgood and why:As for Albuquerque, the ~iffel Tower is pretty.As for Paris, the Eiffel ~ower is pretty.And the se :ShirLey is a blonde and Susan is Nordic-looking too.Shirley is a linguist and Susan is Nordic-lloking too.
)the old split between semantics, syntax, and pragmaticsmust be revised, and our model closely linked with amemory and possibly a logic component as well.Obviously this defines a very difficult task, but insofaras such goals as HT, artificial intelligence, and machinereading of handwritten material or writing of spokenmaterial involve comprehension on the part of the machinejo~ which there seems to be no doubt, these importantgoals will continue to ~lude us until such time as wecan devise such an understanding machine as I have ~escribedbelow.I believe that generative semantics lays thefoundation for studies relevant to such a development,and it is in this context that my proposals are made.In section II I will d~scuss generative semantics.In section III I will discuss the body of my proposalshere.In section IV I will discuss what should be requiredof a generalized "understanding" machine.. .
.
.
tl I I I  \] \] .
.
.
.
.
.
I \]\]Part II.
The theory ofGenerative Semantics.~ae theory of ~enerative semantics is an out-growth and reaction to the theory of transformationalgrammar as represented in Chomsky's 1965 book, As ~ctso f  the Theor~ of S~tax  (MIT Press).
To a very ~- I -~extent, this theory has been the development of asmall group of former students of Chomsky,s or theirclose colleagues.
John (HaJ) Ross has said that thetheory is really Just an attempt to explicate Pa~lPostal's work of five years ago to date.
If Postalwas the founder of this school, if you can call it that,its main workers have been HaJ Ross and George Lakoff,who between 1965 and 1968 swept aside most of transforma-tional linguistics as it then was.
But perhaps bestknown of the group is J~mes McCawley, who graduatedfrom MIT in 1965 with a Ph.D. based on work in ubono-logy, not syntax or semantics.
He promptly amazed Lakoff and 2oss bysome very substantive work in the latter areas as well as phonology.s~udent of McCawley's I will be emphasizing hiscontributions here, and those of my co l lea~s  atChicago, Jerry L. Morgan and Georgia M. Green, butit should be kept in mind that people like Ross,Lakoff, Postal, Arnold Zwicky, David Perlmutter,Emmon Bach, Robin Lakoff, and several others, havemade the current theory possible, and that many others,such as Robert Wall, Lauri Kartunnen, Ronald Langacker,and others, have contributed as well.
It should alsobe kept in mind that the Case Grammar of Fillmoreand the work done by Gruber, while differing fromgenerative semantics, have contributed a great dealto it.~ae basic theory of generative semantics isbuilt upon an attempt to relate the underlyingsemantic structure of language to the surface, phoneticmanifestation of that underlying structure.
That is,a phonetic reality is recognized, and a semanticreality is recognized.
But unlike other versions oftransformational grammar, this theory assigns no specialstatus to syntax; syntax is subsumed in the semantics.McCawley has Jokingly referred to his theor~ as beingone of either "semantax" or "synantics".~11e name generative semantics is not a particularlygood one, since it implies that the ~oal of the theoryis, as with the work of Chomsky, to "separate thegrsumuatical sequences" of a language"from the ~E~__ammaticalSequences."
(Chomsky, S~_~ct ic  Structures~ I~.)
InSAs aII-2other words, to generate all and only grammatical sent-ences of a language.
~his is not at all the goal ofgenerative semantics.
Rather, what we want to do isin some rigorous way specify the correlations of under-lying semantic entities and surface phonetic entities:to specify for any underlying semantic structure whatits possible phonetic realizations in some language are,and for some phonetic structure what underlying semanticstructures it can represent.
Naturally, so~e descript-ive ability is predicated as well, that is, we want tobe able to define ambiguity in some algorithmic fashion,we want to be able to define levels or classes ofill-correlation between structures on different levels,etc.
Chomsky would say that a sentence~like "Golf playsJohn" is eminently deserving of a star; we would say(I) if it's supposed to mean'John plays golf', it doesn'tsucceed in conveying the message; (2) if it's suppesedto mean 'John loves Marsha', then it's really bad; and(3) if Golf is a man.s name and Gohn the name of agame or role, it's a good sentence --- indeed, onecan very well imagine arcane circumstances under whichone might utter that sentence with the intent o fsaying that the game plays John, that the tail wags thedog~ as it were.
Suppose, for example, that John'swife were tired of him spending all his free timeplaying golf and she grumbled to a heighbor aboutit, and the neighbor rather unfeelingly replied,"Oh well, John plays golf."
I can ~ery well imagineJohn's wife complaining bitterly, "Oh no, golf playsJohn."
In any case, it is for hus unimaginativeapproach to language that Chomsky has been Jokinglycalled a "bourgeois formalist"o Even when we usestars, we try to keep in mind that Just aboutany valid phonological string of a language conveysone or more meanings in some context, and that it isartificial to take a string out of context and declareit good or bad.
So "generative semantics" is a badns~e.The following diagram of the components of thetheory is based on McCawley's paper in the proceedingsof the 4th Regional Feeting of the Chicago LinguisticSociety (1968).
A theory very similar is discussedin Ronald Langacker's book LAnguage and its Structure(Harbrace, 1968), pp.
114-34.II-3WHATEVERINITIATESTHO UG HTsemantic ITRANSFORMA-  | surface-->represent- --~TIONAL ~repre -ation |COMPONENT |senta -.
.
.
.
.
tion $phonetic .
|PH0 NOLOGIC-represent a-~--~ALtion ~COMPONENT_The above diagram comes from a report prepared by myself,Jerry Morgan, and Georgia Green, called the Uamelot~ o '  which attempted to describe the cur ren-~te  ofrmational re, search in the Sum~uer cf 1968, particu-larly in reference to the LSA Summer Linguistic Instituteat the University of lllinois, where HaJ Ross, GeorgeLakcff, and Jim McCawley had lectured to large groupson a huge number of very '~airy" (i.e., difficult andtickleishly novel) topics.In that report (which was prepared for Victor Yngve ),we raised several questions concerning the above repre-sentation.
We asked:i.
~hat will an adequate semantic representation have toinclude?
What form will it have?2.
~hat can a transformation do?
What does one locklike ?stage3.
At what A and in what manner are semanticrepres@~tations converted into words of reallanguages?~ese  were by no means all of the questions asked.Needless to say, the answering cf these questionshas hardly begun and will undoubtedly guaranteelinguists a few gocd centuries of work at least.
Itis only in the last decade that syntax has been thesubject of serious work, and we are still onlydiscovering how ignorant we are.
Semantics is evennewer, less than a decade old.
If anyone doubtsthat this is true, consider a) what the above 3questions would have meant to a linguist in (say)1955, and b)why he would have been wrong in his(lack of) comprehension of them.
One of thegreat contributions of Postal and Ross has beenII-4their constant critical look at transformational grammar.One of the things they saw was that our transformationswere (and are) extremely powerful devices, with practical-ly no constraints placed on their formulation.~at  I will do here is summarize some of theattempts at partial answers to the three above questions.In this way I can delimit and explicate generative seman-tics best.i will start by abstracting parts of two papersby McCawley that deal with the nature of semantic representa-tion.
In a paper in the Japanese Journal ~otoba no Uchu(World of Language) in 1967, McCawley argued tha~ semanticrepresentation would be similar to syntactic representationas familiar from ~- type  grammar, but that it wouldalso be quite similar to symbolic logic as familiar fromthe tons of work that have followed Principia and suchstudies.
That semantic representation should resemblesyntactic representation makes sense if only becausewe are arguing for a single set of rules that transforms(i.e., reEates) the underlying structure into (to)the surface structures.
There will be more about thatlater.McCawley argues as follows: the following deviceshave all had a role in symbolic logic:I. propositional connectives" 'and', 'or', 'not'.2.
constants denoting individuals.3.
predicates, denoting properties and relationships.4.
set symbols and the quantifiers 'all' and 'thereexists '.5.
descriptions of sets and individuals.?
x The following devices play a role in natural languages :I. all igs.
have  words for 'and', 'or', and 'not'.
(he notes however that these words in natural igs.may connect more than sentences )2.
"indices" denoting individuals; John loves Johnmight be represented as x I loves x2, but John lovesII-5himself is x I loves x I.3.
predicates are expressed in natural Igs.
(by verbs,adjectives, nouns, etc.
)expressioms such as4.
"Words such as all and&at least one are two membersof a rather larg--e--clasB of expressions which areused to indicate not only the existence of anindividual or a set but the absolute or relativenumber of members in that set."5.
sets and individuals can be expressed as descriptionsusing modified noun phrases.McCawley then gives further reasons for supposing symboliclogic representation to be proper for semantic representa-tion.
(See the bibliography to this section where t~isand other papers that can be consulted for these argumentsin detail are listed.
)In a paper prepared for the symposium on "CognitiveStudies and Artificial Intelligence Research" held by theWenner-~ren Foundation at the University of Chicago inMarch of this year, McCaWley discussed semantic representa-tion at length.
Some of what he had to say there shouldbe noted.
He claimed, "semantic representation must indicatethe immediate constituent structure of the elements invol~edin it {i.e.
examples showing that different meanings cancomsist of the same semantic elements combined in differentways \ [  are easy to come b~)" {p.l) He gave the example ofJohn doesn't beat his wife because he loves her.If the negation applies to John beats his wife, these'~tence means 'the reason ~at  John doesn't beat his wi~eis that he loves her', whereas if it applies to theJohn heats his wife because he loves  her., the mg. is 'thereason that John beats his wi~e -~not -  ~Hat he loves her.
'Notice that here a surface form represents at least twodifferent underlying structures which nonetheless containprecisely the same semantic elements-- grouped differently,however.Another point made is that "semantic representationsmust include .., some indication of presupposed coref-erence."
(p.2) That is, the fol lowingsentence in neutral(i.e.
null) context is ambiguous three-ways:II-6John told Harry that his wife was pretty.Whose wife?
John's?
Harry's?
or a third's?
It could beany.
However, if we know who his refers to, thereis no such ambiguity.
This may--seem trivial, butit is a point often ignored.McCawley then gives an argument for referentialindices being different from expressions used to de~ribe.The sentenceMax d~bied that he kissed the girl Be kissed.
/is not contradictory if "the girl he kissed" is thespeaker's description.Another notion is that of presupposed set memberoship.
lhMax is more intelligent than most Americans.said with primary stress ~n most, the sentence is goodif and only if Max is presupp--~a to be American, thatis, the sentence implies Max is American.
With primarystress on Americans, however, Max is presupposed notto be American.
Presupposition is in general a veryhairy topic which was recently the subject of an entireconference (at the Ohio State University).
We knowvery little about the nuamces of implication andare only beginning even to identify the problems.
Butif a machine is ever to rea~ Ga_tcher i~ the Rye catchingall the nuances of the italicized words, we had betterfind out how stress is used to alter the presuppositlon-al set of a sentence.
I need not be so unsubtle as tosuggest the extreme value of such researches to psycholo-gy.
Perhaps they already know about all this, for allI know.
In any case I cannot restrain myself frominclucing McCawley's beautiful exampleCIA Agents are more stupid than mostAmericans.He had primary streos on the ~ but I prefer to thinkof it as going on the Americans.Z would like ~o interject at this point a minorapology.
I have been rather fan-clubish here andhave waved my hand a lot.
Frankly I see no value811-7in rehearsing here all the arguments available elsewhere.But I would like the rea~r  to bear in mind my skimpyresume in no way reflects~the quality of the original.Let me also note, lest I seem u~uduly credulous towardstjheDthoughts of.
C~i rman Quang 4mild-maunered linguist?
.
mc~awley Is In reality Q. p. Dong, Chairman ofUnamerican Studies at an unknown universityJ, that mostof us working within the paradigm of generativesemantics would be the first to admit that our theorieshaven't a pra~er of being right, that is, t h e y ~ ~approach even a partially realistic and naturalistic "Jtheory of language.
If we like it better than otherparadigms it is because we believe that noother cureent theory is any better and that this oneat least has a good chance of self-improvement.
(Endof apologia.
)If semantic representation looks much likelogical representation, it also differs from it.In the Kotoba no Uchu paper McCawley noted the follow-ing differences :I.
"It is necessary to admit predicates which assertproperties not only of individuals but also ofsets and propositions."2.
"In mathematics one enumerates certain objectswhich ~one~will talk about, defines other obJecSsin te~ms o?
these objects, and co~Ifines\[onesel~to a discussion of objects which\[oneS has eitherpostulated or defined ....
However, one does notbegin a conversation by giving a list of postu-lates and def init ions.
.
.
.
.
?..people oftentalk about things which either do not exist orwhich they have identified incorrectly?
indicesexist in the minds of the speaker rather thanin the real world; they are conceptual entitieswhich the individual speaker creates in interpret-ing his experience.
"In the Wenner-Gren symposium, McCawley had more to say aboutthe difference between logic and language.1.
Immediate constituent structure (trees)rather thanparentheses are basic.
First, "semantic representationsare to form the input to a system of ~ransformationsthat relate meaning to superficial form; to theII-8extent that these transformations have been formulatedand Justified, they appear to be stateable only interms of constituent structure and constituent type,rather than in terms of configurations of parenthesesand terminal symbols."
Secondly, "it may be necessa-ry to operate in terms of semantic representations inwhich symbols have no left-to-right ordering .... "2.
There will have to be more 'logi~al operators', such asmost, almost all, and m~.3.
"And and ... or ... cannot be regarded as Just binaryoperators but-~ust be allowed to take an arbitrarynumber of operands."4.
The quantifiers must be restricted rather thanunrestricted as in most logical systems.
Somequantiflers imply existence: All dogs like tobite postmen, involves the presupposition thatdogs exist, whereas the unrestricted quantifierslogicians use have no such presupposition.5.
"Adequate semantic representation of sentencesinvolving'shifters' (Jakobson, 1957) such asI, YOu~_ ~ now, ..., gestures and deictic~6rds like this--and that, and tenses, will haveto include re~rence-K~-the speech act.
The mostpromising approach to this aspect of semanticrepresentation ... is Rose's (1969)elaborationof Austin's (1962) notion of 'performative verb'.
"(See now too Searle's book, Speech A~ts, CUP, 1969---RIB)6.
"The range of indices will ~ave to be enormous.In particular, it will have to include not onlyindices that purport to refer to physical objects,but also indices corresponding to mythical orliterary objects, so that one can represent themeaning of sentences such asThe Trobriand Islanders believe in Santa61aus, but they call him Ubu Ubu."7.
McC.
rejects "the traditional distinction between'predicate' and 'logical operator' and trea~s~such 'logical operators' as quantifiers, conjunctions,and negation as predicates...."10II-9To clarify the relationship of semantic to syntact-ic representations let me quote here from McCawley'sKotoba no Uchu paper:Since the rules for combining items into largerunits in symbolic logic formulas must be stated in termsof categories such as 'preposition', Ipredicatel, and'index'~ these categories can be regarded as labels onthe nodes of these trees.
And since ... these categoriesall appear to correspond to syntactic categories, thesame symbols (S, V, NP, etc. )
may be used as node labelsin semantic representations as are used in syntactic re-presentations.
Accordingly, semantic representationsappear to be extremely close in formal nature to syntacticrepresentations, so close in fact that it becomes possibleto catalogue the conceivable formal differences and determinewhether those differences are real or apparent?Among such differences he lists:I.
"The items in a s#ntactic representation must beassigned a linear order, whereas it is not obviousthat linear ordering of items in a semantic repre-sentation makes shy sense."2.
"Syntactic representations inwolve lexical items fromthe language as their terminal nodes, whereas theterminal nodes in a semantic representation aresemantic units rather than lexical units.
""There are many syntactic categories which appear toplay no role in semantic representation, for ex.,verb-phrase, preposition, and prepositional phrase.
"(At the 5th Regional Meeting of the CLS, April ofthis year, A. L. Becket of the University of Michiganpresented a paper in which he argued prepositions areunderlying predicates; prepositional phrases areaccordingly verb-phrases.
)McCawEey concluded nonetheless that these differencesdo not provide an argument that semantic ~epresentationsare different in formal nature from syntactic representa-tions.
Again, I will omit his reasons for that conclusion.I might summarize all this by saying:i. Semantic representatio~ is a modification of therepresentations long familiar from ~ormal logic.B.11II-lO2.
Such representations do not radically differ fromthe surface syntactic representations of Aspects-type grammar.Let me close by posing more problems.
McCawley asksthe  following questions at the end of his Eotoba noUchu paper.
While they do not specifically reEateto semantic structure, I include them to give some ideaof what we believe to be the sort of questions thata serious theory of language should prowide Justifiableanswers for :I.
How do the mgs.
of words change as a language evolves?2.
How does a child learn rags.
in learning to speak hisnative language?3.
~at  mechanisms are involved in phenomena such asmetaphor .... ?
(Dorothy Lambert has written aPh.D.
thesis at Michigan on the subject ofmetaphor within the paradigm of Case Grammar.This 500 page dissertation is probably one ofthe best studies of the subject to date from alinguistic point of view.S--RIB)4.
To what extent are the units of semantic representatiomqunivers el ?5.
To what extent does the lexicon of a language havea structure?6.
Can all languages express the same ideas?7.
To what extent doe's one's language affect his thinking?8.
To what extent is one's ability to learn lexical itemsconditioned by his knowledge of the world?I will now turn to the second question raisedabove on p. II-3o This question has as yet receivedlittle study.
It is a very difficult topic, but avery important one.
I will confine myself here to afew brief comments and a few references.12II -IiOne of the important studies underway now is aboutsyntactic variables.
This was the subject of Ross' 1967dissertation.
Variables such as X and Y are famil iarfrom transformational grammars, but no one had attemptedbefore to specify in general what the notion of syntacticvariable entailed.
While Ross' study was important, andhe came up with several important constraints on theform of transformations, much work remains.
Lakoff an~Postal are also working on related questions.
Let mellst here some of the constraints Ross gave in his thesis:I) The complex NP constraint.No element contained in a sentence dominatedby a mounphrase  withxa lexical head nounmay be moved out of that noun phrase by atransformation.
(p, 127)2) The o~oss-over condition.No NPment ioned  in the structural index ofa transformation may be reordered by thatrule in such a way as to cross over acoreferential NP.
(p. 132)3)T~e coordinate structure constraint.In a coordinate structure, no conjunct may bemoved, nor may any element ~ontained in aconjunct be moved out of that conjunct.(p.
161 )~) The pied piping convention.Any transformation which is stated in such a wayas to effect the reordering of some specifiednode NP, where this node is preceded and followedby variables in the structural index of therule, may apply to this NP or to any non-coordinate NP which dominates it, as long asthere are no occurences of any coordinate node,nor of the node S, on the branch connectingthe higher node and the specified node.
(That is,:.
.
.any NP above some specif ied one may be reorder-e~, instead of the specif ied one, but there areenvironments where the lower NP ~ay not be moved,and only some higher one can, consonant with theconditions imposed ~rn the convention.~7) (p.206)i311-125) The sen~entia l  subject constraint.No element dominated b~ an S ~ay be movedout of that S if that node S is dominatedhy an NP which itself is immediatelydominated by S. (p. 243)6) The frozen structure constraint.If a clause has been, extraposed from a nounphrase whose head noun is lexical, this nounphrase may not be moved, nor may any elementof the clause be moved out of that clause.(p.
295)7) Definition of identity.Constituents are identical if they have thesame constituent structure and are identicalmorpheme-for-morpheme, or if they differ onlyas to pronouns, where the pronouns in each ofthe identical constituents are commanded byantecedents-in the non-identical portionsof the phrase-marker.
(p.348)A very important constraint occurs on p. 480 of thethesis, but I omit it here because it contains manyterms I would not care to define here.
I reccomendRoss' dissertation for anyone with doubts aboutany deep principles of language organization emergingfrom our studies in transformational grammar.
He willbe cured.Recently George Lakoff has studied the notion of"derivational constraint".
This study is quite recentand still very very hairy, but hints in his 1969 CLSpaper, and comments by Postal on it suggest that ruleodering is merely a special case or manifestationof a deeper principle of grammar organization.
Thenext revolution effected by generative semantics maywell be to drop rule ordering from our canons.For various reasons (partly that it interestsme mere ) I will have much more to say here aboutlexlcal insertion than I will about constraints ontransformations, although undoubtedly the .
latteris ultimately o~ much greater importance.Until 1965 or so, it was assumed that the terminalsymbols of a P-marker are lexical items; the lexicon merelyassigns properties to these items.
Bruber in his 196514If-13dissertation argued that certain transformations had tooccur before lexical items entered trees: that is, thatthere were pre-lex?cal transformations.Before Gruber, the system of semantics was onein which T-rules generated from deep structures surfacestr%~ctures and P-rules generated semantic representationsfor those deep structures.
?
T~lis was the theory ofintepretive semantics (as in Katz and Postal, for ex.
)Gr~ber proposed a derivational semantics.
Gruberintended to "show va-~ consistently recurrent semanticrelationships among parts of the sentence and amongdifferent sentences, which can best be explained by theexistence of some underlying pattern of which thesyntactic structure is a particular manifestation."
(p.l)He concluded that "a level at which semantic interpretationw~ll be relevant will ... be deeper than the levelof 'deep structure' in syntax."
(p.2) Later Lakoff showedevidence that in fact the level of semantic interpretationwas that of deep structure, but argued that (as Grubersaid) "syntax and semantics will have the same representa-tion at the prelexical level"(p. 3): a single set of ruleswould transform semantic structures containing no lexicalitems into surface syntactic representations containingthem.The s~udy of lexical insertion, the process bywhich the underlying semantic elements are grouped intounits replaceable ~y surface lexical items has led toa large literature containing a great many questions,and some positive answers.
An important paper wasMcCawley's 1968 paper, "Lexical insertion in a transforma-tional grammar without deep structure.
"There he started by assuming various pointsconcluded in other papers of his.
He very clearlypresents some of the tehots of generative semantics,so with some repetition from above I quote thesepoints here :I. Syntactic and semantic representations are of thesame formal nature....2.
There is a single system of rules ... which relatessemantic representation ~o surface structure throughintermediate stages.3.
In the earlier stages of the conversion from semantic1511-14representation to surface structure, terminal nodeamay have for labels 'referential indices' such aswere ~ntroduced in Chomsky 1965 ....
In semanticrepresentation, only indices and 'predicates' a~eterminal node labels ....McCawley then defined tdictionary entry' as atransformation which replaced part of a tree by asurface lexical item.
He expressed doubt these rulescould be ordered internally or external, since itwould hardly be possible, for example, that some questionwould arise as to the relative ordering of the transforma-tion introducing the word horse and that extraposing NP'sin two dialects, that is, ~he ~rdering could not possiblymatter.He then raised several possibilities as to the re-lative ordering of the lexical rules v~s-a-vis otherrules.
Are the lexical rules last, first, or where?McCawley argued for the lexical rules applying Justbefore the post-cyclic rules, and adduced evidence forseveral rules, predicate-raising, equi-NP deletion, etc.,being pre-lexlcal.In his 1968 LSA paper, Jerry L. Morgan of theU;~Iversity of Chicago added to this.
He pointed out'the rather strong assumptlon that lexical_items only'replace' constituents."
(P.3) He wrote, "~he processof syntactic derivation begins with semantic representa-tion in terms of trees containing very highly abstractsemantic terms, operating upon this by means of rulespermuting, deleting, and collapsing parts of the representa-tion, finally deriving a structure whose constituents arereplaced by lexical items."
(p. 3) He then s~ated avery strong claim of the theory:Given the set of universal pre-~exicalrules, the set of universal semantloprimitives, and the set of universalconstraints on the operation of rules,such as those described by Ross 1967,these define the universal set of possiblelexical items in their semantic aspect;that is, they rule out as impossible amin f in i t~ classof a priori possible"meanings" a lexlcal item could have.
(P.4)16If-19A second very strong claim of the theory is:Insofar as the selection from, and detailsof implementation of, the universal set ofrules is language-specific, the idiosyncraciesof a given language in this respect will alsobe reflected by systematic gaps in the lexicon.The same is true for the set of semanticprimitives and the se~ of constraints on rules..... (P.4-5)Morgan came up with some restrictions on lexical items :onlyI) "lexical items Jan replace a constituent which,!
is not labelled S. (p.6)2) "verbs cannot incorporate referential indi~es.
"(p.6)One l~Lher  point to be made is that lexical itemscan only replace well-formed sub~rees.My own work has been concerned with specifyingclasses of possible lexical items and accounting forthe syntact ic properties of verbs in terms of theirsemantics, thereby attempting to capture the intuitionlong familiar from traditional grammar that certain~emantic classes of verbs, such as "verbs of givingand taking" or "verbs of motion" also form syntactic classesand hence their syntactic properties can be regarded asderived from their semantics.Georgia Green of the University of Chicago haspresented a paper (1969) which is also interestingin terms of lexical insertion.
She tends to regardlexical insertion hs fairly ~ivorced frommorphology, and views lexical insertion as the replace-ment of an entire sub-tree by a surface lexical itemwhich may contain more than one morpheme as ~lassicallydefined.
This position is somewhat different from myown, as I regard lexical insertion as primarily involvingthe replacement of items on a i-I basis.
However, thisis an empirical question and only future research willdecide which of us is more nearly correct.So far I have discussed lexical ~nsertion interms of sweeping, general p~inciples of the organizationof the grammar.
I~ order to more clearly specify what17II-16lexical insertion is all about, I ought to presentsome of the kinds of problems which have generated811 of this interest in the subject.fAt the Texas Conference on Universals in 1967, the proceedings of whioh werep~lished in 1968 as Universal_s in Linguistic Theory, McCawley raised the~e~0sa ls  in L, nguistic Theory, McCawley ralsed theques~o~ dictionary organiza'tion anew.
He opted fo~a "Weinreichian" lexicon in which lexical items werecombinations of semantic, syntactic~ and phonologicalinformation.
McCawley supported this with this evidence:the reason John is sadder than that book.
is bad is thatthe two sads in the underlying structure of the sentenceare d~erent  lexical items.
They therefore cannotparticipate in comparison:*John is as sad as that book he read yesterday.
*He exploits his employees more than the oppur-tunity to please.
*Is Brazil as independent as the continuumhypothesis?
(exx.
of Chomsky's.
)McCawley called for a theory of "implioaticnal relations",since in cases such as the ambiguity of warm the ambiguityis not a property of the item itself but-B'~--a class ofitems, and therefore such an ambiguity must be specifiedin terms of general principles.
NcCawley was not clearabout the nature of these implicational r~lations, sothat the nature of the relationship of the various sadswas more or less left open.
I have discussed the no-~onof systematic ambiguity, where the ambiguities of an entireclass of verbs is specified in terms of the derivationalprocess underlying them all, not Just in termso~ a descriptive statement.
Thus we are seeking toexplain lexical gaps in terms of statements such as"The reason some language L lacks a verb ?glossing the verb W in the language M is thatM, but not L, has the transformation T."Anyone familiar with the lexicons of French, ~n~lish,and German, for example, knows that there are certain kindsof verb which are not typical of one or another of theselanguages which nonetheless readily occur in the others.Such verbs are derived by processes occuring in one butnot another language, and our task is to discover anddescribe such processes.
Thus we may ultimately be ableto tell how the class o~ French verbs, say, differs from18II-17the class of all possible verbs.I have attempted in these few pages to presenta digest of some works in the paradigm of generativesemantics.
I have not really attempted to provideeven an elementary guide to the methods of generatives~mantics or to its conclusions, its findings, but Ihope I have explicated somewhat its goals and givensome insight into the direction in which it is moving.Some very strong claims are forthcoming on the nature ofgrammars and languages and hence of language itself.
Atremendous amount of work needs to be done, but one cansee clearly that one possible end point of this work willbe a very comprehensive, very strong theory of languagecompetence that has a great deal to say about humanbe ings.One perhaps minor point, though, looms up largehere: generative semantics relates semantic structuresto stu~face sentences by a single Eet of r~les.
Thereare s-~veral versions of transformational grammar that dothis, but generative semantics is perhaps the most-Cevelop-ed of these.
But as the saying goes, what goes up mustcome down: we may paraphrase this as: what can be generated,can be analyzed.
T~e theory permits, idsally, analgorithmic translation of a surface string into one ormore underlying semantic structures.
For computationallinguistics, that may be its most appealing feature.19II-18BIBLIOGRAPHYA short, select blblio~raphy of recent works in andon gener~ ~ive semantics.Austin, J. L.1962.
How to  do things with words.London: 0UP.
(1965 paper.)ed.
J.0.
Urmson.Bach, Emmon.1964.
"Have and be in English syntax."
Lg.
~3.462-85.196~.
"Problomlnalization I-II."
Mimeo.196~.
"Nouns and nounphrases."
in Bach & Harms.1969.
"Anti-pronomlnalization."
Mimeo.Forthcoming.
"Binding.
"Bach, E~on,  and Peters, Stanley.1968.
"Pseudd-cleft sentences."
Mimeo.Bach, E~m~n, and Harms, Robert, edd.1968.
Universals in Linguistic Theory.
NYC: Halt,Rinehart,Winston.Becket, A.
L.1969.
"Prepositions as Predicates.
"in Binnick et alBenwick, Launcelot de, the Green Knight, and Morgan le Faye.(:R.
Binnick, G. M. @teen, J. L. Morgan.
)1968.
Camelot 1968.
Internal memo., MT Group, UC, mimeo.Bierwlsch, Manfred, and Heidolph (edd)to appear.
Recent Advances in Linguistics.
l~uton.Binnick,1967.1967.1968.1968.1968.1968.1969.Robert I.
"Semantic and syntactic classes of verbs."
Mimeo.
"The lexicon in a derivational semantic theory~of J rna~Sf~i~?~ lslnlg~i~tics 1 In Chicago i i ab from Univer-sity Microfilms, AnnA~bor, Michigan, as ser~ls-372.
"On the nature of the 'lexical item'",in Darden etal.
"On transforma~ionally derived verbs in a~rammar of English", .Ditto, read at LSAoThe characterization of abstract lexicalentities", ~Ditto, read at ACL.
"Transitive verbs and lexical insertion",dittoed, read at Kansas and CLS.
"Predicative structure."
Unpublished Ph.D. diss.20II-19Bin_nick, R., G. Green, J. Morgan, & A. Davison, edd,1969.
Papers from the 5th Regional Meeting,Chicago Linguistic Society.
Chicag6:Department of Linguistics, University ofChicago.
(Advt.
: dirt cheap at $5.
)Camelot.
see Benwick.J~arden, B. J., C. J. Bailey, A. Davison, odd.1968.
Papers from the 4th Regional Meeting,Chicago Linguistic Society.
Chicago:Department of Linguistics, University ofChicago.
(Advt.
: still dirt cheap at $3.
)OeRiJk~ Rudolph.19~J.
"A note on prelexical predicate raising.
"Dittoed.Donnellen, Keith.1966.
"Reference and definite descriptions.
"Philosophical Review 75.
281-304.Green, Georgia M.1968.
"0n too and either, and not Just tooan~ eith-e-r, either-~-7-in Darden et a1~----1969.
"Some---~-eoretical implications of the lexicalexpression of emphatic conjunction."
UnpublisheoM.A.
thesis, dittoed.1969.
"On the notion 'related lexical entry. '"
inBinnick et al~orthcoming.
Review of R. Lakoff, Abstract Syntaxand Latin Complementation.
To appear in L~.Gruber, Jeffrey S.1965.
Studies in lexical relations.
Unpublishe~ diss.1967.
"Look and se~', in L~K.
43.9~7-47.1967.
"The functions of t~-6 lexicon in formaldescriptive grammars."
Systems DevelopmentCorporation doctuuent TM-3770/000/00.Jakobovits and Steinberg, edd.to appear.
Semantics: an interdisciplinary reader ....Jakobson, Roman1957.
"Shifters,verbal categories and the Russian verb.
"Slavic Dept., Harvard Univ.i21II-20Kartunnen.
Lauri.1968.
~Co-reference and discourse."
Read at LSA.1968.
"~hat do referential indices refer to?
"~ RANDCorporation Rgport.1969.
"Migs and pilots. "
Mimeo1969.
"Pronouns and variables.
~' In Binnick et alKatz, J. and Postal, Paul.1964.
An integrated theory of linguistic descriptions.HIT Press.Kiparsky, Paul.1968.
"Linguistic Universals and Linguistic Change.
"in Bach and Harms.Kiparsky, P. & C.1968.
"Fact. "
To appear in Bierwisch and Heidolph.Lakoff, G~orge.1965.
"On the nature of syntactic irregularity.
"Indiana Univ.
diss.
=NSF-16 report, ed.A.
0ettinger.
Available from :Clearing House for Federal Scientificand Technical Information, Springfield, Va.,as document PB 169 252 ($3).
See also0ettinger below.
(Alias NSF#~7).1966.
"Stative adjectives and verbs in English.
""A note on negation.
"Both in NSF-17.
~ 1967.
"Pronominalization, negation, and the analysis of adverbs."
ms.k1966.
"Deep and surface gray,nat."
ms. Alsoseveral of these papers currently availablefrom Linguistics Club, Linguistics Department,Indiana University.1968."Counterparts."
Read at LSA.
Mimeo.nd.
"Pronoun~ and reference", ms.1969.
"Some semantic considerations in syntax.
"In Binnick and al.add:1968."Repartee".
To appear in Foundations of Language.1968.
"Instrumenta~ adverbs and the Concept of Deepstructure."
in Foundations of Language.1969.
"Presuppositions and relative grammaticality.
"Read at LSA in 1968.to appear.
"On Generative Semantics" in Jakobovits andSt einberg.2211-21Lakoff, George, and Peters, Stanley.1966.
"Phrasal conjunction and s~numetric predicates",in NSF-17.Lakoff, George, and Ross, John R.1966.
"A criterion for verb phrase constituency.
"I n NSF-17.Lakoff, Rob-n T.1968.
Abstract syntax and Latin complementation.
MIT Press.1968.
"Some reasons why there can't be a some-any rifle.
"Read at LSA, mimeo.1969.
"Syntac%ic arguments for not-transportation.
"In Binnick et al /to appear.
Review of Grammaire G~n~rale et Raisonnee,1660.
To appear in L~.Langacker, Ronald.1968.
Language and its Structure.
Harbraceto appear.
"On pronominalization and the chain ofcommand", mlmeo 1966, to appear in an anthologyby Reibel and Schane to appear.>~ Cawley,nd.
~ 1968.1966.1968,1969.1969.1969.add:1967.~ 1969.19?8.James Do"The annotated respective."
F~Imeo.
"On the role of semantics in a grammar."
InBach & Harms.Review of Cooper's Set Theory and SyntacticOescription.
in Foundations of Language.
"Concerning the base component of a transforma-tional grammar."
in Foundations of Lg.
"A note on multiple negations."
mimeo.
"English as a VS0 Lg."
mimeo, read at LSA 1968.
"Tense an~ time reference in English."
Readat Ohio State Semantics funfest; to appear inWorking Papers in Linguistics 4 of 0SU Ling.
dept.
;~eo,"Meaning and the description of ig."
Kotoba noUchu, Tokyo, nos.
9,10,Ii.
"Semantic representation."
Read at Symposiumon Cognition etc.
"On lexical insertion in a ~ransformational gram-mar without d~ep structure."
In Darden et alMorgan, J. L.1968.
"Some strange aspects o?
"it"."
In Da~den et al1968.
"On the notion 'possible lexical item'".
Readat LSA.
Ditto.196~.
"Irving."
Dittoed.23II-221968.
"Three notes on Irving and other matters."
Ditto.1969.
"On arguing about semantics."
Ditto, read at SECOL.1969.
"On the memantic representation of lexicalitems."
In Binnick et al0ettinger, A. ed.1966.
NSF-17.
Available from Clearimghouse asdocument PB 173 630 ($3).Perlmutter, David.1968.
~e  two verbs 'begin'."
to appear in ananthology by Jacohs and Rosenbaum to appear.1968.
"aeep and Surface Structure Constraints inSyntax. "
MIT diss.Postal, Paul.1966.
"On so-called 'pronouns, in English."
inGeorgetown University Monography Serieson Languages... 19, 177-206.1968.
"Cross-over constraints."
ms.to appear-a.
"On coreferential complement subjectdeletion" in Jakobovits and Steinberg.to appear-b.
"On the derivation of surface nouns."
inL~ngui_stic In u q,u~_~ /a new Journa!
J1969.
"On 'remind'", read at Ohio State.1969.
"Anaphoric Islands."
in Binnick et alQuang Fnuc Dong.1968.
"English sentences without overt grammaticalsubject."
Mimeo.1968.
"A note on conjoined NP.s", mimeo.Ross, John R.~196).
"A p~ogsed rule of tree pruning.
""Relativization in gxtraposed Clauses.
"_~ Both in NSF-17.~1967.
"Auxiliaries as main verbs."
~.z~c~ ,~\1966.
"On the cyclic nattu~e of English pronominalization.\ in Jakobson Festschrift, Mouton.~1966.
"Adjectives as NP's".
5 ,~,~ 1967.
"Gapping and the order of constituents."
PEGS.
1968.
"O n declarative sentences."
0;~%~1967.
"Constraints on variables in syntax. "
Dies.1969~ several forthcoming papers.24II-23Vendler, Zeno.19o7.
Linguistics in Philosophy.
Cornell Univ.
press.Wall, Robert.1967.
"Selectional restrictions on subjects andobjects of transitive verbs."Mimeo.Zwicky.
Arnold.1968.
"Naturalness argt~uents in Syntax."
In Darden et al25III-iEXTENSION AND MODIFICATION O~TH~ THEORYPaul Postal, in a 196~ paper, "Underlying and super-ficial l inguistic structure", seemed to rule out anyprincipled approach to the study of performance.
But itseems clear to me that performance has me, ely been acatch-all term used by linguists with a lot of nastyfacts on their hands they had no way of handling.
Insection I I I  mentioned the treatment of semi-grammaticalsentences as they used to be called.
Now I think weshould be able to treat so-called sentence fragmentsas being part of language proper.
I see no reason,once we get over our hang-ups with sharp categorizationof grammatical ity and Judgments of grammatical ity innull context, why we cannot have a principled treatmentof sentence fragments.Another area ~sually relegated to Never-never landis that of the structure of discourse.
Obviously thesentence pairsHarry is a fool.
He voted for Richard Nixon.Ha voted for Richard Nixon.
Harry is a vote.are not equivalent.
Imagine if we take every other sentenceon a page, say, the beginning of Matthew 2.
The result ishardly a well-formed discourse.Now the birth of Jesus came about in this way.But her husband, Joseph, was an upright manand did not wish to disgrace her, and he decidedto break off the engagement privately.
"Joseph,descendent of David, do not fear ~o take Mary,your wife, to your home, for it is through thein/'luence of the holy Spirit that she is to be-come a mother."
All this happened in fulfi l l-ment of what the Lord said through the prophet ....But he did not live with her as a husband untilshe had had a son, and he named the child Jesus.
"Where is the newly born king of the Jews?
"To now, it has general ly been held that the structure ofdiscourse is linear, that is, sentences are strung to-gether one after the other and well-formedness is basedon kow well these sentences string.
But the contextis vital to the form of a sentence.
Similarly, whether26I I I-2two clauses are united or put into separate sentences de-pends on context: by context we cannot mean m~rely thetwo sentences on either side of the sentence in question,nor can we mean the n sentences to either side.
~ is  isquite as mad as the fol l~ of the early 50's that syntaxwas a matter of which words had what probabil ity ofoccurir~ n words to either side of a given word.
~at  weneed is a grammar, a generative grammar, a transforma-tional grammar, of discourse, based on the same methodsthat have been developed in syntax over the last decade.This worm was pioneered by George Lakoff's 196~ studyof Russian folk-tales, in which he revised Propp'sphrase structure grammar of the "morphology" of Ruszian~olk-tales.
I subsequently re-modif ied Lakoff-s workand programmed it in COMIT for a 7090-7094 machine togenerate plot outl ines of Russian folktales.
The resultswere partly abominable and partly amusing, but the point isthat while hardly any discourse is as steretyped asRussian folktales or US patents, that certain structuresnonetheless occur which are larger than the sentence.
Thenotions of subordination and coordination of sentences andeven whole discourses are quite val id and quite amenableto inve st~gation.A third class of problems concern logic.
The implica-tions of a sentence may be quite as important as the state-ments made by it.
We l inguists are only beginning toinvestigate presupposition, implication, insinuation,assertion, etc., but ph i losophershave  been aware of theseproblems for a long time ano a large l iterature exists.We want a machine to get as much information out of asentence as a human would.A fourth class of problems concern memory.
Any programmust involve knowledge.
H~mans do not use language invacuo.
Suppose I know that Sherlock Holmes is a tall,thin man.
Suppose further that a fat, short man comesup to me and tells me he is Sherlock Holmes.
If mymemory and logic components are going full blast Iimmediately suggest to the gentleman that a)he iseither lying, or b)could use a good psychiatrist,or c)he has a bad sense of humor.
W~ would notlike the computer to read a sarcastic sentence, such as"Surely they have a right to do unto others what theywould not want others to do unto them" and file it awayneatly.
We need to give the computer a &ertain amountof l inguistic sophistocation as far as irony, insinuation,27III-3and such go.
This might seem overly optimistic, sincemost human beings lack this ability, but let me suggestthat the goal of computational l inguistics is to understandhuman capabilities, not reproduce them, something whichcan be done far cheaper by producing new human beingst~ru natural means than producing software in our labs.The only thing keeping us from programming ~omputersto~ for example, have a sense of human, is our peculiardelusion that we can't do it.So these are the problems that have not been thesubject of serious research.
Note that I do not meanby this that no one has ewr  looked at them andfound anything out.
~en Newton was Platonian enoughto realize that nothing new is ever discovered underthe sun.
But no linguist operating in terms of aformalized or quasi-formalized system has studied theseproblems very much.
This is not to say that certainconclusions about the future construction of a theory oflanguage use cannot be drawn from ou~ present ignorance.T~e rest of this section will be devoted to how wewith our Neanderthalic knowledge of language can outlinea decent formal theory of 'la parole', something thatwe would want to do, I think, even had the computernever been invented.
(~nd of sermon.
)One question which arises ~ere is what the nature ofunderlying semantic structures is.
Do people think intrees?
McCawley in his article on the base rejected thenotion of derivation.
Instead he instituted a system of"node-sdmisslbil ity conditions".
These are actuallyconditions on the well-formedness of trees.
Any objectmeeting these requirements is a w l l-formed tree, other-wise it is not (although I have yet to settle inmy own mind whether an i l l-formed tree is still a tree,Just as I have been confused about whether an Il l-formedsentence of English is still a sentence of gngllsh atal~l.)
Each NAC has the form<a; BC>which is read, "a node A is admissible if it immediate-ly and exclusively dominates a node labelled B and anode labelled C." NAC's generate trees directly, as opposedto rewriting rules which, in Choms~y's system, firstgo through a derivation, from which trees are then construct-ed.
But the important point here is "Grammars are writtenby fools llke me, but only God can make a tree": meaning28iii-~that l inguists need not concern themselves with the or iginof trees to discover their properties.Of course, if we are to be manipulat ing semantic struct-ures, we are going to have to be concerned with wheretrees come from.
A more basic question is whether thekinds of trees generative semantics claims to be semanticare reasonable semantic structures, that is, whether theinvestigator in artif icial intell igence, for example,could live with them.
I think there is a very goodchance that this is the case.
The basic elements of thesetrees are as follows.
We have referential indices referencingindividuals.
I think that in any system we will need adevice such as this.
Both these indices and larger entitiescal led senetences or S,s can be dominated by the categoryN.
I think again that any system will heed to considersentences recursive in this way.
Then we will need predicatesof arbitrary "weight", tho' in natural language the numberof N's associated with any predicate V will undoubtedly berather small.
One possible counter to this is obviatedif we assure that we have ways of referr ing to sets.
Thenwe can define S as a V and associated N's.
This is notrea l ly  a bad scheme.Where it does fall down is in its failure to reflect~yper-proposit ional  relations.
The conceptual universeof a person is not a bunch of unrelated trees orsentences (propositions).
We wil l  want ways to connectthe Napoleon of "Napoleon ate cheese" with that of "NapoEeonhated Elba".
Thus the conceptual universe is a network,with a far more complex structure than our underlying semantictrees.
We therefore need some set of rules for isolatingpart of this network to serve as the underlying tree forsome surface sentence or set of sentences, since it may turnout from our study of the structure of discourse thatthe unit of generat ion is larger than the sentence.More will be said on these matters in section IV.29IV-IPART IV.
The understandingmachine.One basic goal of research into computational linguist-ics might be to investigate how information is extractedfrom linguistic source data.
(ultimately this ties intosuch questions as that of automated abstracting.)
Thatcomponent of our projected understanding machine whichwill model the information abstracting process let usdub the "info grabber~.The info grabber of course is not isolated.
It willhave to be connected with a logic component and a memorywith which it will interact.Nor is this the whole picture.
As shown below one needsalso a way of encoding the semantic output of the_logiccomponent for later output as linguistic data.
Thereforethe whole system will look like:I~i nguist lc\[~INFO "' ~LoGIc  ource I1"~1 ~RAB- ata (LSD)~ BER ~output If SPEW- ~ata  (LOD)II ~ERFNotice that I have dubbed that component ~ahich synthesizesthe LOD the "infox spewer".W e can regard the above as a reasonable model not onlyof an understanding machine, but of the speaker.
The abovemodel would certainly be of use in the study of the useo f a natural language as a computer input-output languageboth for programming and for other applications, such asinteraction between student and teaching machine in aneducational program.
I have made some study of such a sys-tem, which I called EASIOL (English as an Input-outputLanguage), taking into account the results of the twostudies I know of which approximated what I was after,namely Daniel Bobrow's STUDINT program, reported on in"A Question Answering System for High School AlgebraWord Problems" Proc.
FJCC 25, !9641 and in ScientificAmerican S~ptember 198b, pp.
252-260, which was BobroW's30IV-2research for his doctorate.
Bobrow modified LISPo_in thedirection of COMIT, walling the hybrid METEOR.
Resystem he evolved has a fair amount of flexibdlityand generality, and can doal with many kindsof problems expressed in st~llzed language.
I mightcriticize Bobrow for his naivete over natural language,but since I am even more naive about informationprocessing I will not do so.A second system which I have heard later evolved intoa more general system, is the BASEBALL program reportedon by Green, Wolf, Carol 0homsky, and Laughery in theFeigenbaum-Feldman voltume, Computers and Thou~t.
Thissystem bases itself on a ra~-h-er stylized t~e of datastructure.
I have not followed the prc~ress ofeither of these projects, but both Betray inherentfaults that made it unlikely t11at either could formthe basis for a more general system operating onactual discourse.
Nonetheless, these systems are veryconvincing for those Who think that language is thesacrosanct birthright of human Beings and that computerswill never Be able to hahdle such tasks as writing abstractsof articles.The above model is also a reasonable model of humanspeakers (if we forget that people differ from machinesin essential ways -- vive la dlff@rence.T )The first part of the "info grab" is the read-ln.
Hope-fully this will someday Be done by the machine itself, viaoptical reader or speech analyzer.
I think thatresearch on readers and a~alyzers has in general beenunhappy because of a failure to realize how complexrecognition b~ humans is.
Recognition is not simplyan ootlcal or auditory problem.
All levels of languagemust  interact in the process."
it is well-known that realspeech is more easily handled than ~pproxlmantsto speech, x~this can only be d~e to the recognitionprocess being cyclic and operating slm~ltaneously onall levels.
The slmpllst recognition routines wouldinvolve something lik~ :$1IV-3SIGNAL ---~ ~o ise  Filter~(LSO) -----=' ~ Segm~ter-- |IIMORPHOLOGICAL I ~I ~i ANALYZER I ~ 2~ ~"Indeed, we have to connect up t~e logic and the memoryto this system.
Below is a real sample of my hand-writing when writing rapidly.No recognition routine, not even my own human onecan at all times decipher this garbage.
Redundancy ispretty near nil and such words as '!of", "as", "a", and"or" tend to be homologous.
What a human reader can'tdo, we can hardly .
expeet a machine to do.But humans can gmess from context what a word must be,and then see if the squiggle on the page is closeenough.
This involves both syntactic and semanticrecognition, and if we ever want machine reading ofhandwriting, we must give the machine this capability.But suppose the reader still can'~ handle thewriting?
I suppose then we want ~t to get the logiccomponent to intiate a question such as," What is that?
"$2Iv-4That is, we want the computer to be able to gothru the whole set of levels.
This will necessitatea much more complicated program than those aroundtoday, incorporating a greater amount of linguisticexpertise, but undoubtedly it is necessary.Let us assume that the info grabber has grabbed theinfo, it ~_ll have (1) to store this information in thememory, and (2) ~et the logic component examine theinformation.
Suppose I know that Richard Daley is themayor of Chicago, and I read in a Chicago newspaper thatthe ~ayor of Chicago is the greatest man in the world.The LSD m~st somehow be so stored that I can retrievefrom my memory the fact that RichardDa__~l~ is thot~tto be the greatest man in the w- - -~~t  newspaper.This raises .
the question of how to convertunderlying semantic trees into subnets of the semanticnetwork of which memory probably consists, hmny of thefeatures incorporated into ?
Sidney Lamb's conceptu-al networks will, I think, be incorporatable into the moddl.In particular, all occurences of a particular entity (con-cept) will have to be linked in some way or identified.In a sense info grabbing starts by analyzing the LSDinto semantic structures, and ends by synthesizing thesestructures and those already in memory into a new memorynetwork.One point that should be made clear is that all informa-tion will have to be represented on the same level.
Thatis, both the program and the data will reside in the samememory net, as in a computer.
Reading an algorithm ina book, the machine will store this in its memory Justas it stores part of its own program, and it will beable to either quote the algorithm later as linguisticmaterial as part of info retrieval, or use that algorithmas part of its own logical operations.
There is somequestion the as to ~hether this quite ideal machine couldactually function in this way.
But human beings are likethis in some ways, and it is part of their language capa-bil ity that they should or could.The process of info spewing is a reverse of theinfo grab.
The logic component will initiate thespew, using part of the memory net and selectingone or more underlying trees to spew out.
It will then3SIV-5go through the derivational process and ultimately genera@ean actual string of sentences.
Perhaps feedback willenter here, so that the machine can utilize part of itsown spewings as immediate LSD, although it is hardto see why the machine would need to do so, altho humansare constantly correcting themselves mid-sentence.An obvious question is what the role of generativesemantics in all t~Is.
I think the experience of CLhas been ~ in general that ad hoc programs don'twork.
W e need a basic linguistic theory.
I thinkgenerative semantics is the best bet.
But asI noted, it is a theory of cometence.
We will needto modify it.
I think we need tol) admit rules of non-recoverable deletion,2) admit rules for hypersentential constructs,and3) build strong interactions with lo~ic and memorycomponents.In particular, the relationship of underlying semanticstructures to conceptual networks will have to heinvestigated in depth.If the hypotheses of the GS linguists are correct, thenwe have a simple but powerful basis for programs d i rect lytransforming language source materials into semanticinformation usuable by programs.
For example, ifthe semantic structures turn out to be universal, theycan servq as a pivot or intermediary for the currentlyout of fashion goal of MT.$4
