Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 596?605,MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational LinguisticsA Fast Fertility Hidden Markov Model for Word Alignment Using MCMCShaojun Zhao and Daniel GildeaDepartment of Computer ScienceUniversity of RochesterAbstractA word in one language can be translated tozero, one, or several words in other languages.Using word fertility features has been shownto be useful in building word alignment mod-els for statistical machine translation.
We builta fertility hidden Markov model by adding fer-tility to the hidden Markov model.
This modelnot only achieves lower alignment error ratethan the hidden Markov model, but also runsfaster.
It is similar in some ways to IBMModel 4, but is much easier to understand.
Weuse Gibbs sampling for parameter estimation,which is more principled than the neighbor-hood method used in IBM Model 4.1 IntroductionIBM models and the hidden Markov model (HMM)for word alignment are the most influential statisticalword alignment models (Brown et al, 1993; Vogel etal., 1996; Och and Ney, 2003).
There are three kindsof important information for word alignment mod-els: lexicality, locality and fertility.
IBM Model 1uses only lexical information; IBM Model 2 and thehidden Markov model take advantage of both lexi-cal and locality information; IBM Models 4 and 5use all three kinds of information, and they remainthe state of the art despite the fact that they were de-veloped almost two decades ago.Recent experiments on large datasets have shownthat the performance of the hidden Markov model isvery close to IBM Model 4.
Nevertheless, we be-lieve that IBM Model 4 is essentially a better modelbecause it exploits the fertility of words in the tar-get language.
However, IBM Model 4 is so com-plex that most researches use the GIZA++ softwarepackage (Och and Ney, 2003), and IBM Model 4 it-self is treated as a black box.
The complexity in IBMModel 4 makes it hard to understand and to improve.Our goal is to build a model that includes lexicality,locality, and fertility; and, at the same time, to makeit easy to understand.
We also want it to be accurateand computationally efficient.There have been many years of research on wordalignment.
Our work is different from others inessential ways.
Most other researchers take eitherthe HMM alignments (Liang et al, 2006) or IBMModel 4 alignments (Cherry and Lin, 2003) as in-put and perform post-processing, whereas our modelis a potential replacement for the HMM and IBMModel 4.
Directly modeling fertility makes ourmodel fundamentally different from others.
Mostmodels have limited ability to model fertility.
Lianget al (2006) learn the alignment in both translationdirections jointly, essentially pushing the fertility to-wards 1.
ITG models (Wu, 1997) assume the fer-tility to be either zero or one.
It can model phrases,but the phrase has to be contiguous.
There have beenworks that try to simulate fertility using the hiddenMarkov model (Toutanova et al, 2002; Deng andByrne, 2005), but we prefer to model fertility di-rectly.Our model is a coherent generative model thatcombines the HMM and IBM Model 4.
It is easier tounderstand than IBM Model 4 (see Section 3).
Ourmodel also removes several undesired properties inIBM Model 4.
We use Gibbs sampling instead of aheuristic-based neighborhood method for parameter596estimation.
Our distortion parameters are similar toIBM Model 2 and the HMM, while IBM Model 4uses inverse distortion (Brown et al, 1993).
Ourmodel assumes that fertility follows a Poisson distri-bution, while IBM Model 4 assumes a multinomialdistribution, and has to learn a much larger numberof parameters, which makes it slower and less reli-able.
Our model is much faster than IBM Model 4.In fact, we will show that it is also faster than theHMM, and has lower alignment error rate than theHMM.Parameter estimation for word alignment modelsthat model fertility is more difficult than for mod-els without fertility.
Brown et al (1993) and Ochand Ney (2003) first compute the Viterbi alignmentsfor simpler models, then consider only some neigh-bors of the Viterbi alignments for modeling fertil-ity.
If the optimal alignment is not in those neigh-bors, this method will not be able find the opti-mal alignment.
We use the Markov Chain MonteCarlo (MCMC) method for training and decoding,which has nice probabilistic guarantees.
DeNero etal.
(2008) applied the Markov Chain Monte Carlomethod to word alignment for machine translation;they do not model word fertility.2 Statistical Word Alignment Models2.1 Alignment and FertilityGiven a source sentence fJ1 = f1, f2, .
.
.
, fJ and atarget sentence eI1 = e1, e2, .
.
.
, eI , we define thealignments between the two sentences as a subset ofthe Cartesian product of the word positions.
Fol-lowing Brown et al (1993), we assume that eachsource word is aligned to exactly one target word.We denote as aJ1 = a1, a2, .
.
.
, aJ the alignmentsbetween fJ1 and eI1.
When a word fj is not alignedwith any word e, aj is 0.
For convenience, we addan empty word ?
to the target sentence at position 0(i.e., e0 = ?).
However, as we will see, we haveto add more than one empty word for the HMM.In order to compute the ?jump probability?
in theHMM model, we need to know the position of thealigned target word for the previous source word.
Ifthe previous source word aligns to an empty word,we could use the position of the empty word to indi-cate the nearest previous source word that does notalign to an empty word.
For this reason, we use atotal of I + 1 empty words for the HMM model1.Moore (2004) also suggested adding multiple emptywords to the target sentence for IBM Model 1.
Afterwe add I+1 empty words to the target sentence, thealignment is a mapping from source to target wordpositions:a : j ?
i, i = ajwhere j = 1, 2, .
.
.
, J and i = 1, 2, .
.
.
, 2I + 1.Words from position I + 1 to 2I + 1 in the targetsentence are all empty words.We allow each source word to align with exactlyone target word, but each target word may align withmultiple source words.The fertility ?i of a word ei at position i is definedas the number of aligned source words:?i =J?j=1?
(aj , i)where ?
is the Kronecker delta function:?
(x, y) ={1 if x = y0 otherwiseIn particular, the fertility of all empty words inthe target sentence is?2I+1i=I+1 ?i.
We define ??
?
?2I+1i=I+1 ?i.
For a bilingual sentence pair e2I+11 andfJ1 , we have?Ii=1 ?i + ??
= J .The inverted alignments for position i in the tar-get sentence are a set Bi, such that each element inBi is aligned with i, and all alignments of i are inBi.
Inverted alignments are explicitly used in IBMModels 3, 4 and 5, but not in our model, which isone reason that our model is easier to understand.2.2 IBM Model 1 and HMMIBM Model 1 and the HMM are both generativemodels, and both start by defining the probabil-ity of alignments and source sentence given thetarget sentence: P (aJ1 , fJ1 |e2I+11 ); the data likeli-hood can be computed by summing over alignments:1If fj?1 does not align with an empty word and fj alignswith an empty word, we want to record the position of the targetword that fj?1 aligns with.
There are I + 1 possibilities: fj isthe first word in the source sentence, or fj?1 aligns with one ofthe target word.597P (fJ1 |e2I+11 ) =?aJ1P (aJ1 , fJ1 |e2I+11 ).
The align-ments aJ1 are the hidden variables.
The expectationmaximization algorithm is used to learn the parame-ters such that the data likelihood is maximized.Without loss of generality, P (aJ1 , fJ1 |e2I+11 ) canbe decomposed into length probabilities, distor-tion probabilities (also called alignment probabil-ities), and lexical probabilities (also called transla-tion probabilities):P (aJ1 , fJ1 |e2I+11 )= P (J |e2I+11 )J?j=1P (aj , fj |f j?11 , aj?11 , e2I+11 )= P (J |e2I+11 )J?j=1(P (aj |f j?11 , aj?11 , e2I+11 ) ?P (fj |f j?11 , aj1, e2I+11 ))where P (J |e2I+11 ) is a length probability,P (aj |f j?11 , aj?11 , e2I+11 ) is a distortion prob-ability and P (fj |f j?11 , aj1, e2I+11 ) is a lexicalprobability.IBM Model 1 assumes a uniform distortion prob-ability, a length probability that depends only on thelength of the target sentence, and a lexical probabil-ity that depends only on the aligned target word:P (aJ1 , fJ1 |e2I+11 ) =P (J |I)(2I + 1)JJ?j=1P (fj |eaj )The hidden Markov model assumes a length prob-ability that depends only on the length of the targetsentence, a distortion probability that depends onlyon the previous alignment and the length of the tar-get sentence, and a lexical probability that dependsonly on the aligned target word:P (aJ1 , fJ1 |e2I+11 ) =P (J |I)J?j=1P (aj |aj?1, I)P (fj |eaj )In order to make the HMM work correctly, we en-force the following constraints (Och and Ney, 2003):P (i+ I + 1|i?, I) = p0?
(i, i?
)P (i+ I + 1|i?
+ I + 1, I) = p0?
(i, i?
)P (i|i?
+ I + 1, I) = P (i|i?, I)where the first two equations imply that the proba-bility of jumping to an empty word is either 0 or p0,and the third equation implies that the probability ofjumping from a non-empty word is the same as theprobability of jumping from the corespondent emptyword.The absolute position in the HMM is not impor-tant, because we re-parametrize the distortion prob-ability in terms of the distance between adjacentalignment points (Vogel et al, 1996; Och and Ney,2003):P (i|i?, I) = c(i?
i?)?i??
c(i??
?
i?
)where c( ) is the count of jumps of a given distance.In IBM Model 1, the word order does not mat-ter.
The HMM is more likely to align a sourceword to a target word that is adjacent to the previ-ous aligned target word, which is more suitable thanIBM Model 1 because adjacent words tend to formphrases.For these two models, in theory, the fertility fora target word can be as large as the length of thesource sentence.
In practice, the fertility for a targetword in IBM Model 1 is not very big except for raretarget words, which can become a garbage collector,and align to many source words (Brown et al, 1993;Och and Ney, 2003; Moore, 2004).
The HMM isless likely to have this garbage collector problem be-cause of the alignment probability constraint.
How-ever, fertility is an inherent cross-language propertyand these two models cannot assign consistent fer-tility to words.
This is our motivation for adding fer-tility to these two models, and we expect that the re-sulting models will perform better than the baselinemodels.
Because the HMM performs much betterthan IBM Model 1, we expect that the fertility hid-den Markov model will perform much better thanthe fertility IBM Model 1.
Throughout the paper,?our model?
refers to the fertility hidden Markovmodel.Due to space constraints, we are unable to pro-vide details for IBM Models 3, 4 and 5; see Brownet al (1993) and Och and Ney (2003).
But we wantto point out that the locality property modeled in theHMM is missing in IBM Model 3, and is modeledinvertedly in IBM Model 4.
IBM Model 5 removesdeficiency (Brown et al, 1993; Och and Ney, 2003)598from IBM Model 4, but it is computationally veryexpensive due to the larger number of parametersthan IBM Model 4, and IBM Model 5 often providesno improvement on alignment accuracy.3 Fertility Hidden Markov ModelOur fertility IBM Model 1 and fertility HMMare both generative models and start by defin-ing the probability of fertilities (for eachnon-empty target word and all empty words),alignments, and the source sentence giventhe target sentence: P (?I1, ?
?,aJ1 , fJ1 |e2I+11 );the data likelihood can be computed bysumming over fertilities and alignments:P (fJ1 |e2I+11 ) =??I1,?
?,aJ1P (?I1, ?
?,aJ1 , fJ1 |e2I+11 ).The fertility for a non-empty word ei is a randomvariable ?i, and we assume ?i follows a Poisson dis-tribution Poisson(?i;?(ei)).
The sum of the fer-tilities of all the empty words (??)
grows with thelength of the target sentence.
Therefore, we assumethat ??
follows a Poisson distribution with parameterI?(?
).Now P (?I1, ?
?,aJ1 , fJ1 |e2I+11 ) can be decomposedin the following way:P (?I1, ?
?,aJ1 , fJ1 |e2I+11 )= P (?I1|e2I+11 )P (?
?|?I1, e2I+11 )?J?j=1P (aj , fj |f j?11 , aj?11 , e2I+11 , ?I1, ??)=I?i=1?(ei)?ie??(ei)?i!?(I?(?))??
e?I?(?)??!
?J?j=1(P (aj |f j?11 , aj?11 , e2I+11 , ?I1, ??)
?P (fj |f j?11 , aj1, e2I+11 , ?I1, ??
))Superficially, we only try to model the lengthprobability more accurately.
However, we also en-force the fertility for the same target word across thecorpus to be consistent.
The expected fertility for anon-empty word ei is ?
(ei), and the expected fertil-ity for all empty words is I?(?).
Any fertility valuehas a non-zero probability, but fertility values thatare further away from the mean have low probabil-ity.
IBM Models 3, 4, and 5 use a multinomial distri-bution for fertility, which has a much larger numberof parameters to learn.
Our model has only one pa-rameter for each target word, which can be learnedmore reliably.In the fertility IBM Model 1, we assume thatthe distortion probability is uniform, and the lexicalprobability depends only on the aligned target word:P (?I1, ?
?,aJ1 , fJ1 |e2I+11 )=I?i=1?(ei)?ie??(ei)?i!?(I?(?))??
e?(I?(?))??!
?1(2I + 1)JJ?j=1P (fj |eaj ) (1)In the fertility HMM, we assume that the distor-tion probability depends only on the previous align-ment and the length of the target sentence, and thatthe lexical probability depends only on the alignedtarget word:P (?I1, ?
?,aJ1 , fJ1 |e2I+11 )=I?i=1?(ei)?ie??(ei)?i!?(I?(?))??
e?(I?(?))??!
?J?j=1P (aj |aj?1, I)P (fj |eaj ) (2)When we compute P (fJ1 |e2I+11 ), we only sumover fertilities that agree with the alignments:P (fJ1 |e2I+11 ) =?aJ1P (aJ1 , fJ1 |e2I+11 )599whereP (aJ1 , fJ1 |e2I+11 )=??I1,?
?P (?I1, ?
?,aJ1 , fJ1 |e2I+11 )?
P (?I1, ?
?,aJ1 , fJ1 |e2I+11 )?I?i=1???J?j=1?
(aj , i), ?i??????2I+1?i=I+1J?j=1?
(aj , i), ????
(3)In the last two lines of Equation 3, ??
and each?i are not free variables, but are determined bythe alignments.
Because we only sum over fer-tilities that are consistent with the alignments, wehave?fJ1P (fJ1 |e2I+11 ) < 1, and our model is de-ficient, similar to IBM Models 3 and 4 (Brown etal., 1993).
We can remove the deficiency for fertil-ity IBM Model 1 by assuming a different distortionprobability: the distortion probability is 0 if fertilityis not consistent with alignments, and uniform oth-erwise.
The total number of consistent fertility andalignments is J !??!
?Jj=1 ?i!.
Replacing 1(2I+1)J with??!
?Jj=1 ?i!J !
, we have:P (?I1, ?
?,aJ1 , fJ1 |e2I+11 )=I?i=1?(ei)?ie??
(ei) ?(I?(?))??
e?(I?(?))
?1J !J?j=1P (fj |eaj )In our experiments, we did not find a noticeablechange in terms of alignment accuracy by removingthe deficiency.4 Expectation Maximization AlgorithmWe estimate the parameters by maximizingP (fJ1 |e2I+11 ) using the expectation maximization(EM) algorithm (Dempster et al, 1977).
Theauxiliary function is:L(P (f |e), P (a|a?
), ?
(e), ?1(e), ?2(a?))=?aJ1P?
(aJ1 |e2I+11 , fJ1 ) logP (aJ1 , fJ1 |e2I+11 )?
?e?1(e)(?fP (f |e)?
1)??a??2(a?
)(?aP (a|a?)?
1)Because P (aJ1 , fJ1 |e2I+11 ) is in the exponentialfamily, we get a closed form for the parameters fromexpected counts:P (f |e) =?s c(f |e; f (s), e(s))?f?s c(f |e; f (s), e(s))(4)P (a|a?)
=?s c(a|a?
; f (s), e(s))?a?s c(a|a?
; f (s), e(s))(5)?
(e) =?s c(?|e; f (s), e(s))?s c(k|e; f (s), e(s))(6)where s is the number of bilingual sentences, andc(f |e; fJ1 , e2I+11 ) =?aJ1P?
(aJ1 |fJ1 , e2I+11 )??j?
(fj , f)?
(ei, e)c(a|a?
; fJ1 , e2I+11 ) =?aJ1P?
(aJ1 |fJ1 , e2I+11 )??j?
(aj , a)?
(aj?1, a?
)c(?|e; fJ1 , e2I+11 ) =?aJ1P?
(aJ1 |fJ1 , e2I+11 )??i?i?
(ei, e)c(k|e; fJ1 , e2I+11 ) =?ik(ei)?
(ei, e)These equations are for the fertility hiddenMarkov model.
For the fertility IBM Model 1, wedo not need to estimate the distortion probability.5 Gibbs Sampling for Fertility HMMAlthough we can estimate the parameters by usingthe EM algorithm, in order to compute the expected600counts, we have to sum over all possible alignmentsaJ1 , which is, unfortunately, exponential.
We devel-oped a Gibbs sampling algorithm (Geman and Ge-man, 1984) to compute the expected counts.For each target sentence e2I+11 and source sen-tence fJ1 , we initialize the alignment aj for eachsource word fj using the Viterbi alignments fromIBM Model 1.
During the training stage, we try all2I + 1 possible alignments for aj but fix all otheralignments.2 We choose alignment aj with probabil-ity P (aj |a1, ?
?
?
aj?1, aj+1 ?
?
?
aJ , fJ1 , e2I+11 ), whichcan be computed in the following way:P (aj |a1, ?
?
?
, aj?1, aj+1, ?
?
?
, aJ , fJ1 , e2I+11 )=P (aJ1 , fJ1 |e2I+11 )?aj P (aJ1 , fJ1 |e2I+11 )(7)For each alignment variable aj , we choose t sam-ples.
We scan through the corpus many times untilwe are satisfied with the parameters we learned us-ing Equations 4, 5, and 6.
This Gibbs samplingmethod updates parameters constantly, so it is an?online learning?
algorithm.
However, this samplingmethod needs a large amount of communication be-tween machines in order to keep the parameters upto date if we compute the expected counts in parallel.Instead, we do ?batch learning?
: we fix the parame-ters, scan through the entire corpus and compute ex-pected counts in parallel (E-step); then combine allthe counts together and update the parameters (M-step).
This is analogous to what IBM models andthe HMM do in the EM algorithms.
The algorithmfor the E-step on one machine (all machines are in-dependent) is in Algorithm 1.For the fertility hidden Markov model, updatingP (aJ1 , fJ1 |e2I+11 ) whenever we change the alignmentaj can be done in constant time, so the complexityof choosing t samples for all aj (j = 1, 2, .
.
.
, J) isO(tIJ).
This is the same complexity as the HMMif t is O(I), and it has lower complexity if t is aconstant.
Surprisingly, we can achieve better resultsthan the HMM by computing as few as 1 samplefor each alignment, so the fertility hidden Markovmodel is much faster than the HMM.
Even whenchoosing t such that our model is 5 times faster thanthe HMM, we achieve better results.2For fertility IBM Model 1, we only need to compute I + 1values because e2I+1I+1 are identical empty words.Algorithm 1: One iteration of E-step: drawt samples for each aj for each sentence pair(fJ1 , e2I+11 ) in the corpusfor (fJ1 , e2I+11 ) in the corpus doInitialize aJ1 with IBM Model 1;for t dofor j dofor i doaj = i;Compute P (aJ1 , fJ1 |e2I+11 );endDraw a sample for aj usingEquation 7;Update counts;endendendWe also consider initializing the alignments usingthe HMM Viterbi algorithm in the E-step.
In thiscase, the fertility hidden Markov model is not fasterthan the HMM.
Fortunately, initializing using IBMModel 1 Viterbi does not decrease the accuracy inany noticeable way, and reduces the complexity ofthe Gibbs sampling algorithm.In the testing stage, the sampling algorithm is thesame as above except that we keep the alignmentsaJ1 that maximize P (aJ1 , fJ1 |e2I+11 ).
We need moresamples in the testing stage because it is unlikelyto get to the optimal alignments by sampling a fewtimes for each alignment.
On the contrary, in theabove training stage, although the samples are notaccurate enough to represent the distribution definedby Equation 7 for each alignment aj , it is accurateenough for computing the expected counts, whichare defined at the corpus level.
Interestingly, wefound that throwing away the fertility and using theHMM Viterbi decoding achieves same results as thesampling approach (we can ignore the difference be-cause it is tiny), but is faster.
Therefore, we useGibbs sampling for learning and the HMM Viterbidecoder for testing.Gibbs sampling for the fertility IBM Model 1 issimilar but simpler.
We omit the details here.601Alignment Model P R AERen ?
cnIBM1 49.6 55.3 47.8IBM1F 55.4 57.1 43.8HMM 62.6 59.5 39.0HMMF-1 65.4 59.1 37.9HMMF-5 66.8 60.8 36.2HMMF-30 67.8 62.3 34.9IBM4 66.8 64.1 34.5cn ?
enIBM1 52.6 53.7 46.9IBM1F 55.9 56.4 43.9HMM 66.1 62.1 35.9HMMF-1 68.6 60.2 35.7HMMF-5 71.1 62.2 33.5HMMF-30 71.1 62.7 33.2IBM4 69.3 68.5 31.1Table 1: AER results.
IBM1F refers to the fertility IBM1 and HMMF refers to the fertility HMM.
We choose t = 1,5, and 30 for the fertility HMM.0.320.340.360.380.40.420.440.460.48AERIBM1IBM1FHMMHMMF-1HMMF-5HMMF-30IBM4Figure 1: AER comparison (en?cn)6020.320.340.360.380.40.420.440.460.48AERIBM1IBM1FHMMHMMF-1HMMF-5HMMF-30IBM4Figure 2: AER comparison (cn ?en)010002000300040005000trainingtime[seconds]IBM1IBM1FHMMHMMF-1HMMF-5HMMF-30IBM4Figure 3: Training time comparison.
The training time for each model is calculated from scratch.
For example, thetraining time of IBM Model 4 includes the training time of IBM Model 1, the HMM, and IBM Model 3.6036 ExperimentsWe evaluated our model by computing the wordalignment and machine translation quality.
We usethe alignment error rate (AER) as the word align-ment evaluation criterion.
Let A be the alignmentsoutput by word alignment system, P be a set of pos-sible alignments, and S be a set of sure alignmentsboth labeled by human beings.
S is a subset of P .Precision, recall, and AER are defined as follows:recall =|A ?
S||S|precision =|A ?
P ||A|AER(S, P,A) = 1?
|A ?
S|+ |A ?
P ||A|+ |S|AER is an extension to F-score.
Lower AER is bet-ter.We evaluate our fertility models on a Chinese-English corpus.
The Chinese-English data takenfrom FBIS newswire data, and has 380K sentencepairs, and we use the first 100K sentence pairs asour training data.
We used hand-aligned data as ref-erence.
The Chinese-English data has 491 sentencepairs.We initialize IBM Model 1 and the fertility IBMModel 1 with a uniform distribution.
We smoothall parameters (?
(e) and P (f |e)) by adding a smallvalue (10?8), so they never become too small.
Werun both models for 5 iterations.
AER results arecomputed using the IBM Model 1 Viterbi align-ments, and the Viterbi alignments obtained from theGibbs sampling algorithm.We initialize the HMM and the fertility HMMwith the parameters learned in the 5th iteration ofIBM Model 1.
We smooth all parameters (?
(e),P (a|a?)
and P (f |e)) by adding a small value (10?8).We run both models for 5 iterations.
AER results arecomputed using traditional HMM Viterbi decodingfor both models.It is always difficult to determine how many sam-ples are enough for sampling algorithms.
However,both fertility models achieve better results than theirbaseline models using a small amount of samples.For the fertility IBM Model 1, we sample 10 timesfor each aj , and restart 3 times in the training stage;we sample 100 times and restart 12 times in the test-ing stage.
For the fertility HMM, we sample 30times for each aj with no restarting in the trainingstage; no sampling in the testing stage because weuse traditional HMM Viterbi decoding for testing.More samples give no further improvement.Initially, the fertility IBM Model 1 and fertilityHMM did not perform well.
If a target word eonly appeared a few times in the training corpus, ourmodel cannot reliably estimate the parameter ?
(e).Hence, smoothing is needed.
One may try to solveit by forcing all these words to share a same pa-rameter ?(einfrequent).
Unfortunately, this does notsolve the problem because all infrequent words tendto have larger fertility than they should.
We solvethe problem in the following way: estimate the pa-rameter ?
(enon empty) for all non-empty words, allinfrequent words share this parameter.
We considerwords that appear less than 10 times as infrequentwords.Table 1, Figure 1, and Figure 2 shows the AERresults for different models.
We can see that the fer-tility IBM Model 1 consistently outperforms IBMModel 1, and the fertility HMM consistently outper-forms the HMM.The fertility HMM not only has lower AER thanthe HMM, it also runs faster than the HMM.
Fig-ure 3 show the training time for different models.In fact, with just 1 sample for each alignment, ourmodel archives lower AER than the HMM, and runsmore than 5 times faster than the HMM.
It is pos-sible to use sampling instead of dynamic program-ming in the HMM to reduce the training time withno decrease in AER (often an increase).
We con-clude that the fertility HMM not only has better AERresults, but also runs faster than the hidden Markovmodel.We also evaluate our model by computing themachine translation BLEU score (Papineni et al,2002) using the Moses system (Koehn et al, 2007).The training data is the same as the above wordalignment evaluation bitexts, with alignments foreach model symmetrized using the grow-diag-finalheuristic.
Our test is 633 sentences of up to length50, with four references.
Results are shown in Ta-ble 2; we see that better word alignment results donot lead to better translations.604Model BLEUHMM 19.55HMMF-30 19.26IBM4 18.77Table 2: BLEU results7 ConclusionWe developed a fertility hidden Markov modelthat runs faster and has lower AER than theHMM.
Our model is thus much faster than IBMModel 4.
Our model is also easier to understandthan IBM Model 4.
The Markov Chain Monte Carlomethod used in our model is more principled thanthe heuristic-based neighborhood method in IBMModel 4.
While better word alignment results do notnecessarily correspond to better translation quality,our translation results are comparable in translationquality to both the HMM and IBM Model 4.Acknowledgments We would like to thank Tagy-oung Chung, Matt Post, and the anonymous review-ers for helpful comments.
This work was supportedby NSF grants IIS-0546554 and IIS-0910611.ReferencesPeter F. Brown, Stephen A. Della Pietra, Vincent J. DellaPietra, and Robert L. Mercer.
1993.
The mathematicsof statistical machine translation: Parameter estima-tion.
Computational Linguistics, 19(2):263?311.Colin Cherry and Dekang Lin.
2003.
A probabilitymodel to improve word alignment.
In Proceedings ofACL-03.A.
P. Dempster, N. M. Laird, and D. B. Rubin.
1977.Maximum likelihood from incomplete data via the EMalgorithm.
Journal of the Royal Statistical Society,39(1):1?21.John DeNero, Alexandre Bouchard-Cote, and Dan Klein.2008.
Sampling alignment structure under a Bayesiantranslation model.
In Proceedings of EMNLP.Yonggang Deng and William Byrne.
2005.
HMM wordand phrase alignment for statistical machine transla-tion.
In Proceedings of Human Language TechnologyConference and Conference on Empirical Methods inNatural Language Processing, pages 169?176, Van-couver, British Columbia, Canada, October.
Associa-tion for Computational Linguistics.Stuart Geman and Donald Geman.
1984.
Stochastic re-laxation, Gibbs distribution, and the Bayesian restora-tion of images.
IEEE Transactions on Pattern Anal-ysis and Machine Intelligence, PAMI-6(6):721?741,November.Philipp Koehn, Hieu Hoang, Alexandra Birch, ChrisCallison-Burch, Marcello Federico, Nicola Bertoldi,Brooke Cowan, Wade Shen, Christine Moran, RichardZens, Chris Dyer, Ondrej Bojar, Alexandra Con-stantin, and Evan Herbst.
2007.
Moses: Open sourcetoolkit for statistical machine translation.
In Proceed-ings of ACL, Demonstration Session, pages 177?180.P.
Liang, B. Taskar, and D. Klein.
2006.
Alignment byagreement.
In North American Association for Com-putational Linguistics (NAACL), pages 104?111.Robert C. Moore.
2004.
Improving IBM word alignmentModel 1.
In Proceedings of the 42nd Meeting of theAssociation for Computational Linguistics (ACL?04),Main Volume, pages 518?525, Barcelona, Spain, July.Franz Josef Och and Hermann Ney.
2003.
A system-atic comparison of various statistical alignment mod-els.
Computational Linguistics, 29(1):19?51.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
BLEU: A method for automatic eval-uation of machine translation.
In Proceedings of ACL-02.Kristina Toutanova, H. Tolga Ilhan, and Christopher D.Manning.
2002.
Extensions to HMM-based statis-tical word alignment models.
In Proceedings of the2002 Conference on Empirical Methods in NaturalLanguage Processing (EMNLP 2002), pages 87?94.Stephan Vogel, Hermann Ney, and Christoph Tillmann.1996.
HMM-based word alignment in statistical trans-lation.
In COLING-96, pages 836?841.Dekai Wu.
1997.
Stochastic inversion transductiongrammars and bilingual parsing of parallel corpora.Computational Linguistics, 23(3):377?403.605
