Proceedings of the 6th Workshop on Building and Using Comparable Corpora, pages 112?120,Sofia, Bulgaria, August 8, 2013. c?2013 Association for Computational LinguisticsMining for Domain-specific Parallel Text from WikipediaMagdalena Plamada?, Martin VolkInstitute of Computational Linguistics, University of ZurichBinzmu?hlestrasse 14, 8050 Zurich{plamada, volk}@cl.uzh.chAbstractPrevious attempts in extracting paralleldata from Wikipedia were restricted by themonotonicity constraint of the alignmentalgorithm used for matching possible can-didates.
This paper proposes a method forexploiting Wikipedia articles without wor-rying about the position of the sentences inthe text.
The algorithm ranks the candidatesentence pairs by means of a customizedmetric, which combines different similar-ity criteria.
Moreover, we limit the searchspace to a specific topical domain, sinceour final goal is to use the extracted datain a domain-specific Statistical MachineTranslation (SMT) setting.
The precisionestimates show that the extracted sentencepairs are clearly semantically equivalent.The SMT experiments, however, show thatthe extracted data is not refined enough toimprove a strong in-domain SMT system.Nevertheless, it is good enough to boostthe performance of an out-of-domain sys-tem trained on sizable amounts of data.1 IntroductionA high-quality Statistical Machine Translation(SMT) system can only be built with large quan-tities of parallel texts.
Moreover, systems special-ized in specific domains require in-domain train-ing data.
A well-known problem of SMT systemsis that existing parallel corpora cover a small per-centage of the possible language pairs and veryfew domains.
We therefore need a language-independent approach for discovering parallel sen-tences in the available multilingual resources.This idea was explored intensively in the lastdecade with different text sources, genericallycalled comparable corpora, such as news feeds,encyclopedias or even the entire Web.
The firstapproaches focused merely on news corpora andwere either based on IBM alignment models (Zhaoand Vogel, 2002; Fung and Cheung, 2004) or em-ploying machine learning techniques (Munteanuand Marcu, 2005; Abdul Rauf and Schwenk,2011).The multilingual Wikipedia is another sourceof comparable texts, not yet thoroughly explored.Adafre and de Rijke (2006) describe two meth-ods for identifying parallel sentences across itbased on monolingual sentence similarity (MTand respectively, lexicon based).
Fung et al(2010) approach the problem by combining recall-and precision-oriented methods for sentence align-ment, such as the DK-vec algorithm or algorithmsbased on cosine similarities.
Both approacheshave achieved good results in terms of precisionand recall.However, we are interested in real applica-tion scenarios, such as SMT systems.
The fol-lowing approaches report significant performanceimprovements when using the extracted data astraining material for SMT: Smith et al(2010)use a maximum entropy-based classifier withvarious feature functions (e.g.
alignment cover-age, word fertility, translation probability, distor-tion).
S?tefa?nescu et al(2012) propose an algo-rithm based on cross-lingual information retrieval,which also considers similarity features equivalentto the ones mentioned in the previous paper.The presented approaches extract general pur-pose sentences, but we are interested in a spe-cific topical domain.
We have previously tackledthe problem (Plamada and Volk, 2012) and en-countered two major bottlenecks: the alignmentalgorithm for matching possible candidates andthe similarity metric used to compare them.
Toour knowledge, existing sentence alignment al-gorithms (including the one we have employedin the first place) have a monotonic order con-straint, meaning that crossing alignments are not112allowed.
But this phenomenon occurs often inWikipedia, because its articles in different lan-guages are edited independently, without respect-ing any guidelines.
Moreover, the string-basedcomparison metric proved to be unreliable foridentifying parallel sentences.In this paper we propose an improved approachfor selecting parallel sentences in Wikipedia arti-cles which considers all possible sentence pairs,regardless of their position in the text.
The selec-tion will be made by means of a more informedsimilarity metric, which rates different aspectsconcerning the correspondences between two sen-tences.
Although the approach is language anddomain-independent, the present paper reports re-sults obtained through querying the German andFrench Wikipedia for Alpine texts (i.e.
moun-taineering reports, hiking recommendations, arti-cles on the biology and the geology of mountain-ous regions).
Moreover, we report preliminary re-sults regarding the use of the extracted corpus forSMT training.2 Finding candidate articlesThe general architecture of our parallel sentenceextraction process is shown in Figure 1.
Weapplied the approach only to the language pairGerman-French, as these are the languages wehave expertise in.
In the project Domain-specificStatistical Machine Translation1 we have built anSMT system for the Alpine domain and for thislanguage pair.
The training data comes from theText+Berg corpus2, which contains the digitizedpublications of the Swiss Alpine Club (SAC) from1864 until 2011, in German and French.
ThisSMT system will generate the automatic transla-tions required in the extraction process.The input consists of German and FrenchWikipedia dumps3, available in the MediaWikiformat 4.
Therefore our workflow requires a pre-processing step, where the MediaWiki contentsare transformed to XML and then to raw text.Preprocessing is based on existing tools, such asWikiPrep5, but further customization is needed inorder to correctly convert localized MediaWiki el-ements (namespaces, templates, date and numberformats etc.).
We then identify Wikipedia articles1http://www.cl.uzh.ch/research en.html2See www.textberg.ch3Accessed in September 20114http://www.mediawiki.org/wiki/MediaWiki5http://sourceforge.net/projects/wikiprep/Figure 1: The extraction workflowavailable in both languages by means of the inter-language links provided by Wikipedia itself.
Thisreliable information is a good basis for the extrac-tion workflow, since we do not have to worry aboutthe document alignment.Upon completion of this step, we have extracteda bilingual corpus of approximately 400 000 ar-ticles per language.
The corpus is subsequentlyused for information retrieval (IR) queries aimingto identify the articles belonging to the Alpine do-main.
The input queries contain the 100 most fre-quent mountaineering keywords in the Text+Bergcorpus (e.g.
Alp, Gipfel, Berg, Route in Germanand montagne, sommet, voie, cabane in French).This filter reduces the search space to 40 000articles.
Although we have refined our searchterms by discarding the ones occurring frequentlyin other text types (e.g.
meter, day, year, end),we were not able to avoid a small percentage offalse positives.
Once we extract the Alpine com-parable corpus, we proceed to the extraction of113parallel sentences, which will be thoroughly dis-cussed in the following section.
See (Plamada andVolk, 2012) for more details about the extractionpipeline.3 Finding parallel segments in WikipediaarticlesThe analysis of our previous results brought intoattention many ?parallel?
sentence pairs of dif-ferent lengths, meaning that the shared trans-lated content does not span over the whole sen-tence.
As an example, consider the following sen-tences which have been retrieved by the extractionpipeline.
Although they both contain informationabout the valleys connected by the Turini pass, theGerman sentence contains a fragment about its po-sition, which has not been translated into French.DE: Der Pass liegt in der a?usseren, besiedeltenZone des Nationalpark Mercantour und stellt denU?bergang zwischen dem Tal der Be?ve?ra und demTal der Ve?subie dar.FR: Le col de Turini relie la valle?e de la Ve?subiea` la valle?e de la Be?ve?ra.If this sentence pair would be used for MTtraining, it would most probably confuse the sys-tem, because noisy word alignments are to be ex-pected.
Our solution to this problem is to splitthe sentences into smaller entities (e.g.
clauses)and then to find the alignments on this granular-ity level.
The clause boundary detection is per-formed independently for German and French, re-spectively, following the approach developed byVolk (2001).
The general idea is to split the sen-tences into clauses containing a single full verb.Our alignment algorithm, unlike previous ap-proaches, ignores the position of the clauses in thetexts.
Although Wikipedia articles are divided intosections, their structure is not synchronized acrossthe language variants, since articles are edited in-dependently.
We have encountered, for example,cases where one section in the French article wasincluded in the general introduction of the Ger-man article.
If we would have considered sec-tions boundaries as anchor points, we would havemissed useful clause pairs.
We therefore decidedto use an exhaustive matching algorithm, in orderto cover all possible combinations.For the sake of simplicity, we reduce the prob-lem to a monolingual alignment task by using anintermediary machine translation of the source ar-ticle.
We decided that German articles should al-ways be considered the source because we expecta better automatic translation quality from Germanto French.
The translation is performed by our in-house SMT system trained on Alpine texts.
Thealgorithm generates all possible clause pairs be-tween the automatic translation and the targetedarticle and computes for each of them a similarityscore.
Subsequently it reduces the search space bykeeping only the 3 best-scoring alignment candi-dates for each clause.
Finally the algorithm returnsthe alignment pair which maximizes the similarityscore and complies with the injectivity constraint.In the end we filter the results by allowing onlyclause pairs above the set threshold.We defined our similarity measure as aweighted sum of feature functions, which returnsvalues in the range [0,1].
The similarity scoremodels two comparison criteria:?
METEOR scoreWe used the METEOR similarity metric be-cause, unlike other string-based metrics (e.g.BLEU (Papineni et al 2002)), it considersnot only exact matches, but also word stems,synonyms, and paraphrases (Denkowski andLavie, 2011).
Suppose that we compute thesimilarity between the following sentences inFrench: j?
aimerais bien vous voir and jevoudrais vous voir (both meaning I wouldlike to see you).
BLEU, which is a string-based metric, would assign a similarity scoreof 52.5.
This value could hardly be con-sidered reliable, given that the sentence tavoiture vous voir (paired with the first sen-tence) would get the same BLEU score, al-though the latter sentence (EN: your car seeyou) is obviously nonsense.
On the otherhand, METEOR would return a score of 90.3for the original sentence pair, since it canappreciate that the two pronouns (je and j?
)are both variations of the first person singularin French and that the predicates convey thesame meaning.?
Number of aligned content wordsHowever, METEOR scores can also be mis-leading, since they rely on automatic wordalignments.
Two sentences are likely to re-ceive a high similarity score when they sharemany aligned words.
However, the align-ments are not always reliable.
We often saw114sentence pairs with a decent Meteor scorewhere only some determiners, punctuationsigns or simple word collocations (e.g.
dela montagne (EN: of the mountain)) werematched.
As an illustration, consider the fol-lowing sentence pair and its correspondingalignment:Hyp: les armoiries , le de?sir de la ville debreslau par ferdinand i. le 12 mars 1530 aRef: le 19 juin 1990 , le conseil municipalre?tablit le blason original de la ville2-4 3-5 5-12 6-13 7-14 13-0Although the sentences are obviously not se-mantically equivalent (a fact also suggestedby the sparse word alignments), the pair re-ceives a METEOR score of 0.23.
We decidedto compensate for this by counting only thealigned pairs which link content words anddividing them by the total number of wordsin the longest sentence from the consideredpair.
In the example above, only one validalignment (7-14) can be identified, thereforethe sentence pair will get a partial score of1/18.
In this manner we can ensure the de-crease of the initial similarity score.Additionally, we have defined a token ratio fea-ture to penalize the sentence length differences.Although a length penalty is already included inthe METEOR score, we still found false candidatepairs with exceedingly different lengths.
There-fore we decided to use this criterion as a selec-tion filter rather than including it in the similarityfunction, in order to increase the chances of othercandidates with similar length.
Even if no othercandidate will pass all the filters, at least we ex-pect the precision to increase, since we will haveone false positive less.The final formula for the similarity score be-tween two clauses src in the source language and,respectively trg in the target language is:score(src, trg) = w1 ?
s1 + (1?
w1) ?
s2 (1)where s1 represents the METEOR score and s2 thealignment score.The weights, as well as the final threshold aretuned to maximize the correlation with humanjudgments.
We modeled the task as a minimiza-tion problem, where the function value increasesby 1 for each correctly selected clause pair anddecreases by 1 for each wrong pair.
The solu-tion (consisting of the individual weights and thethreshold) is found using a brute force approach,for which we employed the scipy.optimizepackage from Python.
The training set consistsof an article with 1300 clause pairs, 25 of whichare parallel and the rest non-parallel.
We chosethis distribution of the useful/not useful clausesbecause this corresponds to the real distributionobserved in Wikipedia articles.
In the best con-figuration, we retrieve 23 good clause pairs and 1wrong.
This corresponds to a precision of 95%and a recall of 92% on this small test set.However, we can influence the quantity of ex-tracted parallel clauses by manually adjusting thefinal filter threshold.
Figure 2 depicts the size vari-ations of the resulting corpus at different thresh-olds, where the relative frequency is representedon a logarithmic scale.
We notice that the rate ofdecrease is linear in the log scale of the numberof extracted clause pairs.
We start at a similarityscore of 0.2 because the pairs below this thresh-old are too noisy.
The data between 0.2 and 0.3is already mixed, as it will be shown in the fol-lowing sections.
However, since this data segmentcontains approximately twice as much data as thesummed superior ones, we decided to include it inthe corpus.Figure 2: The distribution of the extracted clausepairs at different thresholdsTable 1 presents German-French clause pairswith their corresponding similarity scores.
Onthe top we can find rather short clauses (up to10 words) with perfectly aligned words.
One ex-pects that the decrease of the values implies that115Nr.
French clause German clause Score1 mcnish e?crit dans son journal : mcnish schrieb in sein tagebuch : 1.02 son journal n?
a pas e?te?
retrouve?
sein tagebuch wurde nie gefunden 0.9503 elle travailla pendant plusieurs se-maines avec luiwa?hrend mehrerer wochen arbeitete siemit ihm zusammen0.8404 en 1783, il fait une premie`re tentativeinfructueuse avec marc the?odore bourritpaccard startete 1783 zusammen mit marctheodore bourrit einen ersten, erfolglosenbesteigungsversuch0.7175 en 1962, les bavarois toni kinshofer,siegfried lo?w et anderl mannhardtre?ussirent pour la premie`re fois l?
as-cension par la face du diamir1962 durchstiegen die bayern tonikinshofer, siegfried lo?w und anderlmannhardt erstmals die diamir-flanke0.6236 le 19 aou?t 1828 il tenta, avec les deuxguides jakob leuthold et johann wahrenl?
ascension du finsteraarhornaugust 1828 versuchte er zusammen mitden beiden bergfu?hrern jakob leutholdund johann wa?hren das finsteraarhorn zubesteigen0.5197 le parc prote`ge le mont robson, leplus haut sommet des rocheuses cana-diennesdas 2248 km2 gro?e schutzgebiet er-streckt sich um den 3954 m hohen mountrobson, dem ho?chsten berg der kanadis-chen rocky mountains0.4708 la plupart des e?difices volcaniques duhaut eifel sont des do?mes isole?s plus oumoins aplatisdie meisten der vulkanbauten derhocheifel sind als isolierte kuppenvereinzelt oder in reihen der mehr oderminder flachen hochfla?che aufgesetzt0.3799 le site, candidat au patrimoine mondial,se compose d?
esplanades-autels faitsde pierresdie sta?tte, ein kandidat fu?r das unesco-welterbe, besteht aus altarplattformen aussteinen und erde, gestu?tzt auf einer un-terirdischen konstruktion aus bemaltenton-pfeilern0.25910 qu?
un cas mineur ayant un effet limite?sur la sante?wie sich diese substanzen auf die gesund-heit auswirken,0.200Table 1: Examples of extracted clause pairsthe clauses contain less or even no translated frag-ments.
A manual inspection of the extracted pairsshowed that this is not always the case.
We havefound clause pairs with almost perfect 1-1 wordcorrespondences and a similarity score of only0.51.
The ?low?
score is due to the fact that weare comparing human language to automatic trans-lations, which are not perfect.On the other hand, a comparable score can beachieved by a pair in which one of the clausescontains some extra information (e.g.
pair num-ber 7).
The extra parts in the German variant(2248 km2 gro?e - EN: with an area of 2248 km2;3954 m hohen - EN: 3954 m high) cannot beseparated by means of clause boundary detection,since they don?t contain any verbs.
This findingwould motivate the idea of splitting the phrasesinto subsentential segments (linguistically moti-vated or not) and aligning the segments, similarto what Munteanu (2006) proposed.
Nevertheless,we consider this pair a good candidate for the par-allel corpus.Pair number 8 has the same coordinates (i.e.
anextra tail in the German variant), yet it receives alower score, which might disqualify it for the finallist, if we only look at the numbers.
In this case,the low score is caused by the German compounds(Vulkanbauten, Hocheifel), which are unknown tothe SMT system, therefore they are left untrans-lated and cannot be aligned.
However, we arguethat this clause pair should also be part of the ex-tracted corpus.116Score Average sentence lengthrange German French[0.9?
1.0] 4 4.26[0.8?
0.9) 4.87 5.04[0.7?
0.8) 6.47 6.65[0.6?
0.7) 10.78 10.71[0.5?
0.6) 12.09 11.51[0.4?
0.5) 11.91 11.80[0.3?
0.4) 11.28 11.22[0.2?
0.3) 11.22 11.01Table 2: The average sentence length for differentscore rangesThe last pair is definitely a bad candidate for aparallel corpus, since the clauses do not convey thesame meaning, although they share many words(avoir un effet - auswirken, sur la sante?
- auf dieGesundheit).
A subsentential approach would al-low us to extract the useful segments in this case,as well.
There are, of course, pairs with similarscores and poorer quality, therefore 0.2 is the low-est threshold which can provide useful candidatepairs.
At the other end of the scale, we considerpairs above 0.4 as parallel and everything belowas comparable.
As a general rule, a high thresholdensures a high accuracy of the extraction pipeline.Table 2 presents the average length (numberof tokens) of the extracted clauses for differentranges of the similarity score.
We notice that thebest ranked clauses tend to be very short, whereasthe last ranked are longer, as the examples in ta-ble 1 confirm.
However, the average length overthe whole extracted corpus is below 10 words, asmall value compared to the results reported onWikipedia articles by S?tefa?nescu and Ion (2013).This finding is due to the fact that we are aligningclauses instead of whole sentences.We expected the German sentences to be usu-ally shorter than the French ones (or at least havea similar number of words), since they are morelikely to contain compounds.
This fact is con-firmed by the first part of the table.
A turnaroundoccurs in the range (0.5,0.6), where the Germansentences become slightly longer than the Frenchones, since they tend to contain extra information(see also table 1).4 Experiments and ResultsThe conducted experiments have focused only onthe extraction of parallel clauses and their use in aSMT scenario.
For this purpose, we have used asinput the articles selected and preprocessed in theprevious development phase (Plamada and Volk,2012).
Specifically, the data set consists of 39 000parallel articles with approximately 6 million Ger-man clauses and 2.7 million French ones.
We wereable to extract 225 000 parallel clause pairs out ofthem, by setting the final filter threshold to 0.2.This means that roughly 4% of the German clauseshave an French equivalent (and 8% when reportingto the French clauses), figures comparable to ourprevious results on a different sized data set.
How-ever, the quality of the extracted data is higher thanin our previous approaches.To evaluate the quality of the parallel data ex-tracted, we manually checked a set of 200 au-tomatically aligned clauses with similarity scoresabove 0.25.
For this test set, 39% of the ex-tracted data represent perfect translations, 26% aretranslations with an extra segment (e.g.
a nounphrase) on one side and 35% represent misalign-ments.
However, given the high degree of paral-lelism between the clauses from the middle class,we consider them as true positives, achieving aprecision of 65%.
Furthermore, 40% of the falsepositives have been introduced by matching propernames, 32% contain matching subsentential seg-ments (word sequences longer than 3 words) and27% represent failures in the alignment process.4.1 SMT ExperimentsIn addition to the manual evaluation discussed inthe previous subsection, we have run preliminaryinvestigations with regard to the usefulness of theextracted corpus for SMT.
In this evaluation sce-nario, we use only pairs with a similarity scoreabove 0.35.
The results discussed in this sec-tion refer only to the translation direction German-French.
The SMT systems are trained with theMoses toolkit (Koehn et al 2007), according tothe WMT 2011 guidelines6.
The translation per-formance was measured using the BLEU evalua-tion metric on a single reference translation.
Wealso report statistical significance scores, in orderto indicate the validity of the comparisons betweenthe MT systems (Riezler and Maxwell, 2005).
Weconsider the BLEU score difference significant ifthe computed p-value is below 0.05.We compare two baseline MT systems and sev-eral systems with different model mixtures (trans-6http://www.statmt.org/wmt11/baseline.html117lation models, language models or both).
The firstbaseline system is an in-domain one, trained on theText+Berg corpus and is the same used for the au-tomatic translations required in the extraction step(see section 3).
The second system is purely out-of-domain and it is trained on Europarl, a collec-tion of parliamentary proceedings (Koehn, 2005).The development set and the test set contain in-domain data, held out from the Text+Berg corpus.Table 3 lists the sizes of the data sets used for theSMT experiments.Data set Sentences DE Words FR WordsSAC 220 000 4 200 000 4 700 000Europarl 1 680 000 37 000 000 43 000 000Wikipedia 120 000 1 000 000 1 000 000Dev set 1424 30 000 33 000Test set 991 19 000 21 000Table 3: The size of the German-French data setsOur first intuition was to add the extracted sen-tences to the existing in-domain training corpusand to evaluate the performance of the system.
Inthe second scenario, we added the extracted datato an SMT system for which no in-domain paral-lel data was available.
For this purpose, we exper-imented with different combinations of the mod-els involved in the translation process, namely theGerman-French translation model (responsible forthe translation variants) and the French languagemodel (ensures the fluency of the output).
Besidesof the models trained on the parallel data availablein each of the data sets, we also built combinedmodels with optimized weights for each of the in-volved data sets.
The optimization was performedwith the tools provided by Sennrich (2012) as partof the Moses toolkit.
We also want to compareseveral language models, some trained on the indi-vidual data sets, others obtained by linearly inter-polating different data sets, all optimized for min-imal perplexity on the in-domain development set.The results are summarized in table 4.A first remark is that an out-of-domain lan-guage model (LM) adapted with in-domain data(extracted from Wikipedia and/or SAC data) sig-nificantly improves on top of a baseline systemtrained with out-of-domain texts (Europarl, EP)with up to 1.7 BLEU points.
And this improve-ment can be achieved with only a small quantityof additional data compared to the size of the orig-inal training data (120k or 220k versus 1680k sen-tence pairs).
When replacing the out-of-domainTranslation Language model BLEUmodel scoreEuroparl TM EP LM 9.45Europarl TM EP+Wiki LM 10.39EP+Wiki TM EP+Wiki LM 10.37Europarl TM EP+Wiki+SAC LM 11.22EP+Wiki TM EP+Wiki+SAC LM 11.74EP+WMix TM EP+Wiki+SAC LM 10.40SAC TM SAC LM 16.71SAC+Wiki TM SAC+Wiki LM 16.51SAC+WMix TM SAC+Wiki LM 16.37Table 4: SMT results for German-Frenchtranslation model with a combined one (includ-ing the Wikipedia data set) and keeping only theadapted language models, we can observe two ten-dencies.
In the first case (using a combinationof out-of-domain and Wikipedia-data for the lan-guage model), the BLEU score remains approxi-mately at the same level (10.37-10.39), the differ-ence not being statistically significant (p-value =0.387).The addition of quality in-domain data for theLM from the previous configuration brings an im-provement of 0.5 BLEU points on top of the bestEuroparl system (11.22 BLEU points).
Given thatall other factors are kept constant, this improve-ment can be attributed to the additional transla-tion model (TM) trained on Wikipedia data.
More-over, the statistical significance tests confirm thatthe improved system performs better than the pre-vious one (p-value = 0.005).
To demonstratethat these results are not accidental, we replacedthe Wikipedia extracted sentences with a randomcombination thereof (referred to as WMix) and re-trained the system.
Under these circumstances,the performance of the system dropped to 10.40BLEU points.
These findings demonstrate the ef-fect of a small in-domain data set on the perfor-mance of an out-of-domain system trained on bigamounts of data.
If the data is of good quality, itcan improve the performance of the system, other-wise it significantly deteriorates it.We notice that the performance of a strong in-domain baseline system (SAC) cannot be heav-ily influenced (either positively or negatively) bytranslation and language model mixtures combin-ing existing in-domain data with Wikipedia data.In terms of BLEU points, the mixture modelstrained with ?good?
Wikipedia data cause a perfor-118mance drop of 0.2, but the significance test showsthat the difference is not statistically significant (p-value = 0.08).
On the other hand, the TM includ-ing shuffled Wikipedia sentences causes a perfor-mance drop of 0.34 BLEU points, which is statis-tically significant (p-value = 0.013).
We can con-clude that the quantity of the data is not the deci-sive factor for the performance change, but ratherthe quality of the data.
The Wikipedia extracteddata set maintains the good performance, whereasa random mixture of the Wikipedia data set causesa performance decrease.
Therefore the focus offuture work should be on obtaining high qualitydata, regardless of its amount.5 Conclusions and OutlookIn this paper we presented a method for extract-ing domain-specific parallel data from Wikipediaarticles.
Based on previous experiments, we fo-cus on clause level alignments rather than on full-sentence extraction methods.
Moreover, the rank-ing of the candidates is based on a metric com-bining different similarity criteria, which we de-fined ourselves.
The precision estimates show thatthe extracted sentence pairs are clearly semanti-cally equivalent.
The SMT experiments, however,show that the extracted data is not refined enoughto improve a strong in-domain SMT system.
Nev-ertheless, it is good enough to overtake an out-of-domain system trained on 10 times bigger amountsof data.Since our extraction system is merely a proto-type, there are several ways to improve its per-formance, including better filtering for in-domainarticles, finer grained alignment and more so-phisticated similarity metrics.
For example, theselection of domain-specific articles can be im-proved by means of an additional filter based onWikipedia categories.
The accuracy of the extrac-tion procedure can be improved by means of amore informed similarity metric, weighting morefeature functions.
Moreover, we can bypass themanual choice of thresholds by employing a clas-sifier (e.g.
SVMlight (Joachims, 2002)).
Addi-tionally, we could try to align even shorter sen-tence fragments (not necessarily linguistically mo-tivated).We are confident that Wikipedia can be seen asa useful resource for SMT, but further investiga-tion is needed in order to find the best method toexploit the extracted data in a SMT scenario.
Forthis purpose, quality data should be preferred oversizable data.
We would therefore like to experi-ment with different ratio combinations of the datasets (Wikipedia extracted and in-domain data) un-til we find a combination which outperforms ourin-domain baseline system.ReferencesSadaf Abdul Rauf and Holger Schwenk.
2011.
Paral-lel sentence generation from comparable corpora forimproved SMT.
Machine Translation, 25:341?375.Sisay Fissaha Adafre and Maarten de Rijke.
2006.Finding similar sentences across multiple languagesin Wikipedia.
Proceedings of the 11th Conference ofthe European Chapter of the Association for Compu-tational Linguistics, pages 62?69.Michael Denkowski and Alon Lavie.
2011.
Meteor1.3: Automatic metric for reliable optimization andevaluation of machine translation systems.
In Pro-ceedings of the EMNLP 2011 Workshop on Statisti-cal Machine Translation.Pascale Fung and Percy Cheung.
2004.
Mining very-non-parallel corpora: Parallel sentence and lexiconextraction via bootstrapping and EM.
In Proceed-ings of EMNLP.Pascale Fung, Emmanuel Prochasson, and Simon Shi.2010.
Trillions of comparable documents.
In Pro-ceedings of the the 3rd workshop on Building andUsing Comparable Corpora (BUCC?10), Malta.Thorsten Joachims.
2002.
Learning to classify textusing Support Vector Machines.
Kluwer AcademicPublishers.Philipp Koehn, Hieu Hoang, Alexandra Birch, ChrisCallison-Burch, Marcello Federico, Nicola Bertoldi,Brooke Cowan, Wade Shen, Christine Moran,Richard Zens, Chris Dyer, Ondr?ej Bojar, AlexandraConstantin, and Evan Herbst.
2007.
Moses: opensource toolkit for statistical machine translation.
InProceedings of the 45th Annual Meeting of the ACLon Interactive Poster and Demonstration Sessions,ACL ?07, pages 177?180.Philipp Koehn.
2005.
Europarl: A parallel corpus forstatistical machine translation.
In Machine Transla-tion Summit X, pages 79?86.Dragos Stefan Munteanu and Daniel Marcu.
2005.
Im-proving machine translation performance by exploit-ing non-parallel corpora.
Computational Linguis-tics, 31:477?504, December.Dragos Stefan Munteanu.
2006.
Exploiting compara-ble corpora.
Ph.D. thesis, University Of SouthernCalifornia.119Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
BLEU: a method for automaticevaluation of machine translation.
In Proceedingsof the 40th Annual Meeting on Association for Com-putational Linguistics, ACL ?02, pages 311?318,Stroudsburg, PA, USA.
Association for Computa-tional Linguistics.Magdalena Plamada and Martin Volk.
2012.
Towardsa Wikipedia-extracted alpine corpus.
In Proceed-ings of the Fifth Workshop on Building and UsingComparable Corpora, Istanbul, May.Stefan Riezler and John T. Maxwell.
2005.
On somepitfalls in automatic evaluation and significance test-ing for MT.
In Proceedings of the ACL Workshopon Intrinsic and Extrinsic Evaluation Measures forMachine Translation and/or Summarization, pages57?64, Ann Arbor, Michigan, June.
Association forComputational Linguistics.Rico Sennrich.
2012.
Perplexity minimization fortranslation model domain adaptation in statisticalmachine translation.
In Proceedings of the 13thConference of the European Chapter of the Asso-ciation for Computational Linguistics, pages 539?549, Avignon, France.
Association For Computa-tional Linguistics.Jason Smith, Chris Quirk, and Kristina Toutanova.2010.
Extracting parallel sentences from compa-rable corpora using document level alignment.
InHuman Language Technologies: The 2010 AnnualConference of the North American Chapter of theAssociation for Computational Linguistics, pages403?411, Stroudsburg, PA, USA.
Association forComputational Linguistics.Dan S?tefa?nescu and Radu Ion.
2013.
Parallel-Wiki:A collection of parallel sentences extracted fromWikipedia.
In Proceedings of the 14th Conferenceon Intelligent Text Processing and ComputationalLinguistics (CICLing 2013).Dan S?tefa?nescu, Radu Ion, and Sabine Hunsicker.2012.
Hybrid parallel sentence mining from com-parable corpora.
In Mauro Cettolo, Marcello Fed-erico, Lucia Specia, and AndyEditors Way, editors,Proceedings of the 16th Conference of the EuropeanAssociation for Machine Translation EAMT 2012,pages 137?144.Martin Volk.
2001.
The automatic resolution of prepo-sitional phrase - attachment ambiguities in German.Habilitation thesis, University of Zurich.Bing Zhao and Stephan Vogel.
2002.
Adaptive parallelsentences mining from web bilingual news collec-tion.
In Proceedings of the 2002 IEEE InternationalConference on Data Mining, ICDM ?02, pages 745?748, Washington, DC, USA.
IEEE Computer Soci-ety.120
