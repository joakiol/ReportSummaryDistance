Maximum Entropy Model Learning of the Translation RulesKengo Sato and Masakazu NakanishiDepartment ofComputer ScienceKeio University3-14-1, Hiyoshi, Kohoku, Yokohama 223-8522, Japane-mail: {satoken, czl}@nak, ics.
keio.
ac.
jpAbstractThis paper proposes a learning method oftranslation rules from parallel corpora.
Thismethod applies the maximum entropy prin-ciple to a probabilistic model of translationrules.
First, we define feature functionswhich express statistical properties of thismodel.
Next, in order to optimize the model,the system iterates following steps: (1) se-lects a feature function which maximizes log-likelihood, and (2) adds this function to themodel incrementally.
As computational costassociated with this model is too expensive,we propose several methods to suppress theoverhead in order to realize the system.
Theresult shows that it attained 69.54% recallrate.1 IntroductionA statistical natural anguage modeling canbe viewed as estimating a combinational dis-tribution X x Y -+ \[0, 1\] using training data(xl, yl>,..., <XT, YT> 6 X. x Y observed incorpora.
For this topic, Baum (1972) pro-posed EM algorithm, which was basis ofForward-Backward algorithm for the hiddenMarkov model (HMM) and Inside-Outsidealgorithm (Lafferty, 1993) for the pr0babilis-tic context free grammar (PCFG).
However,these methods have problems such as in-creasing optimization costs which is due toa lot of parameters.
Therefore, estimating anatural language model based on the max-imum entropy (ME) method (Pietra et al,1995; Berger et al, 1996) has been high-lighted recently.On the other hand, dictionaries for multi-lingual natural anguage processing such asthe machine translation has been made byhuman hand usually.
However, since thiswork requires a great deal of labor and itis difficult to keep description of dictionar-ies consistent, he researches of automaticaldictionaries making for machine translation(translation rules) from corpora become ac-tive recently (Kay and RSschesen, 1993; Kajiand Aizono, 1996).In this paper, we notice that estimatinga language model based on ME method issuitable for learning the translation rules,and propose several methods to resolve prob-lems in adapting ME method to learning thetranslation rules.2 Problem SettingIf there exist (xl, Yl>,..., {XT, YT) 6 X ?
Ysuch that each xi is translated into Yi inthe parallel corpora X,Y, then its empiri-cal probability distribution/5 obtained fromobserved training data is defined by:p(x,y) - c(x,y) (1)Ex, c(x,y)where c(x, y) is the number of times that xis translated into y in the training data.However, since it is difficult to observetranslating between words actually, c(x, y) isapproximated with equation (2) for sentencealigned parallel corpora.<(x,y)c(x, y) =T(2)where X~ is i-th sentence in X.
We denotethat sentence Xi is translated into sentenceY/ in aligned parallel corpora.
And c~(x, y)1171is the number of times that x and y appearin the i-th sentence.Our task is to learn the translation rulesby estimating probability distribution p(yI x)that x E X is translated into y E Y from15(x, y) given above.3 Maximum Entropy Method3.1 Feature  Funct ionWe define binary-valued indicator functionf : X ?
Y -+ {0,1} which divide X x Yinto two subsets.
This is called feature func-tion, which expresses statistical properties ofa language model.The expected value of f with respected toiS(x, y) is defined such as:p(f) =  p(x,y)f(x,y) (z)x,yThus training data are summarized as theexpected value of feature function f.The expected value of a feature functionf with respected to p(yl x) which we wouldlike to estimate is defined such as:p(f)  = y~fi(x)p(ylx)f(x,y ) (4)x,ywhere 15(x) is the empirical probability dis-tribution on X.
Then, the model which wewould like to estimate is under constraint tosatisfy an equation such as:p(f) =iS(f) (5)This is called the constraint equation.3.2 Max imum Ent ropy  Pr inc ip leWhen there are feature functions fi(i E{1, 2 , .
.
.
,  n}) which are important to model-ing processes, the distribution p we estimateshould be included in a set of distributionsdefined such as:C = {p E 7 9 I P(fi) =16(fi) for i E {1,2,...,n}}(6)where P is a set of all possible distributionsonX?Y.For the distribution p, there is no assump-tion except equation (6), so it is reason-able that the most uniform distribution isthe most suitable for the training corpora.The conditional entropy defined in equa-tion (7) is used as the mathematical measureof the uniformity of a conditional probabilityp(ylx).H(p) = - y~(x)p(y lx  ) logp(ylx ) (7)x,yThat is, the model p. which maximizes theentropy H should be selected from C.p.
-- argmax H(p) (S)petThis heuristic is called the maximum entropyprinciple.3.3 Parameter  Es t imat ionIn simple cases, we can find the solutionto the equation (8) analytically.
Unfortu-nately, there is no analytical solution in gen-eral cases, and we need a numerical algo-rithm to find the solution.By applying the Lagrange multiplier toequation (7), we can introduce the paramet-ric form of p.1Px(YIx)- Z>,(x) exp hifi(x,y) (9)Z,x(x) = y~ exp (~,~i f i (x ,y ) )Ywhere each hi is the parameter for the fea-ture fi.
P~ is known as Gibbs distribution.Then, to solve p. E C in equation (8) isequivalent to solve h .
that maximize the log-likelihood:= -   (x)log z j , ( z )  +x i(10)h.  = argmax kV(h)Such h.  can be solved by one of the nu-merical algorithm called the Improved Itera-tire Scaling Algorithm (Berger et al, 1996).1.
Start with hi = 0 for al l i  E {1 ,2 , .
.
.
,n}2.
Do for each i E {1,2 , .
.
.
,n}:1172(a) Let AAi be the solution to~-~(x)p(ylx)$i(x,y)exp (AAif#(x,y)) = P(fi)x~y(11)where f#(x,y) = Ei~=t f~(x,y)(b) Update the value of Ai according to:Ai ~- A~ + AAi4 Maximum Entropy ModelLearning of the TranslationRulesThe art of modeling with the maximum en-tropy method is to define an informativeset of computationally feasible feature func-tions.
In this section, we define two modelsof feature functions for learning the transla-tion rules.3.
Go to step 2 if not all the Ai have con-vergedTo solve AAi in the step (2a), the Newton'smethod is applied to equation (11).3.4 Feature  Select ionIn general cases, there exist a large collec-tion ~" of candidate features, and becauseof the limit of machine resources, we can-not expect to obtain all iS(f) estimated inreal-life.
However, the Maximum EntropyPrinciple does not explicitly state how to se-lect those particular constraints.
We build asubset S C ~" incrementally by iterating toadjoin a feature f E ~" which maximizes log-likelihood of the model to S. This algorithmis called the Basic Feature Selection (Bergeret al, 1996).Model  1: Co-occurrence In fo rmat ionThe first model is defined with co-occurrenceinformation between words appeared in thecorpus X.
{ 1 (x e W(d,w)) (12)fw(x,y) = 0 (otherwise)where W(d,w) is a set of words which ap-peared within d words from w E X (in ourexperiments, d = 5).
fw(x,y) expresses theinformation on w for predicting that x istranslated into y (Figure 1).. .
.
.
.
W .
.
.
.
.
.
.
.
.
.
.
.
X .
.
.
.
.
.
.
.
.
.
~ ' Xpred~ct i~power" "/translation role.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
y .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
~ Y1.
Start with S = O Figure 1: co-occurance information.
Do for each candidate feature f E ~':Compute the model Psus using ImproveIterative Scaling Algorithm and thegain in the log-likelihood from addingthis featureModel  2: Morphologica l  In fo rmat ionThe second model is defined with morpho-logical information such as part-of-speech.3.
Check the termination condition4.
Select the feature \] with maximal gain5.
Adjoin f to S6.
Compute Ps using Improve Iterative Al-gorithm7.
Go to Step 2{ l osxtlft,s(x, Y) = 1 and POS(y) s0 (otherwise)(13)where POS(x) is a part-of-speech tag for x.ft,u(x, y) expresses the information on part-of-speech t, s for predicting that x is trans-lated into y (Figure 2).
If part-of-speech tag-1173t - eos.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
predictive ~" /x  .
.
.
.
.
.
.
.
.
.
~'-Xpower _ " l~'~'Jtranslation mle.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
y .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
, .
yFigure 2: morphological informationgers for each language work extremely ac-curate, then these feature functions can begenerated automatically.5 Implementat ionComputational cost associated with themodel described above is too expensive torealize the system for learning the transla-tion rules.
We propose several methods tosuppress the overhead.An estimated probability p~(yI x) for a pairof (x,y) E X x Y which has not been ob-served as the sample data in the parallelcorpora X ,Y  should be kept lower.
Ac-cording to equation (9), we can allow to letfi(x,y) = 0 (for all i E {1, .
.
.
,n})  for non-observed (x, y).
Therefore, we will acceptobserved (x, y) only instead of all possible(x, y) in summation i  equation (11), so thatp~(ylx) can be calculated much more effi-ciently.Suppose that a set of (x, y) such that eachmember activates a feature function f is de-fined by:D( f )= {(x,y) eX?r l f (x ,y )= 1} (14)Shirai et al (1996) showed that if D(fi) andD(fj) were exclusive to each other, that isD(fi) fq D(fj) = O, then Ai and Xj couldbe estimated independently.
Therefore, wecan split a set of candidate feature functions.T" into several exclusive subsets, and calcu-late Px(YlX) more efficiently by estimating oneach subset independently.6 Exper iments  and ResultsAs the training corpora, we used 6,057 pairsof sentences included in Kodansya Japanese-English Dictionary, a machine-readable dic-tionary made by the Electrotechnical Lab-oratory.
By applying morphological anal-ysis for the corpora, each word was trans-formed to the infinitive form.
We excludedwords which appeared below 3 times or over1,000 times from the target of learning.
Con-sequently, our target for the experimentsincluded 1,375 English words and 1,195Japanese words, and we prepared 1,375 fea-ture functions for model 1 and 2,744 formodel 2 (56 part-of-speech for English and49 part-of-speech for Japanese).We tried to learn the translation rulesfrom English to Japanese.
We had two ex-periments: one of model 1 as the set of fea-ture functions, and one of model 1 + 2.
Foreach experiment, 500 feature functions wereselected according to the feature selectionalgorithm described in section 3.4, and wecalculated p(yI x) in equation (9), that is,the probability that English word x is trans-lated into Japanese word y.
For each Englishword, all Japanese word were ordered by es-timated probability p(yix), and we evaluatedthe recall rates by comparing the dictionary.Table 1 shows the recall rates for each ex-periment.
The numbers for 15(x,y) are theTa )le 1: rec1sty) 44.55%model 1 41.58%model 1 +2 58.29%dl rates-~ 3rd53.47%63.37%69.54%,-~ 10th58.42%76.24%80.13%recall rates when the empirical probabilitydefined by equation (1) was used instead ofthe estimated probability.
It is showed thatthe model 1 + 2 attains higher recall ratesthan the model 1 and ~(x, y).Figure 3 shows the log-likelihood for eachmodel plotted by the number of feature func-tions in the feature selection algorithm.
No-tice that the log-likelihood for the model 1+2is always higher than the model 1.Thus, the model 1 + 2 is more'effectivethan the model 1 for learning the translationrules.However, the result shows that the recall1174+9.02-9.04.11.06*&08- I k12-&14-9.14.9 .16  I I I I I I I I I50 100 1~0 290 2~ ~0 350 400 ~ 500Ihe nun~ od ~t~l lFigure 3: log-likelihoodrates of the '1st' for all models are not fa-vorable.
We consider that it is the reasonfor this to assume word-to-word translationrules implicitly.7 ConclusionsWe have described an approach to learn thetranslation rules from parallel corpora basedon the maximum entropy method.
As fea-ture functions, we have defined two mod-els, one with co-occurrence information andthe other with morphological information.As computational cost associated with thismethod is too expensive, we have proposedseveral methods to suppress the overhead inorder to realize the system.
We had experi-ments for each model of features, and the re-sult showed the effectiveness of this method,especially for the model of features with co-occurrence and morphological information.AcknowledgmentsWe would like to thank the Electrotechni-cal Laboratory for giving us the machine-readable dictionary which was used as thetraining data.ReferencesL.
E. Baum.
1972.
An inequality and associ-ated maximumization technique in statis-tical estimation of probabilistic functionsof a markov process.
Inequalities, 3:1-8.Adam L. Berger, Stephen A. Della Pietra,and Vincent J. Della Pietra.
1996.
A max-imum entropy approach to natural lan-guage processing.
Computational Linguis-tics, 22(1):39-71.Hiroyuki Kaji and Toshiko Aizono.
1996.Extracting word correspondences frombilingual corpora based on word co-occurrence information.
In Proceedingsof the 16th International Conference onComputational Linguistics, pages 23-28.M.
Kay and M. RSschesen.
1993.
Texttranslation alignment.
ComputationalLinguistics, 19(1):121-142.J.
D. Lafferty.
1993.
A derivation of theinside-outside algorithm from the EM al-gorithm.
IBM Research Report.
IBM T.J.Watson Research Center.Stephen Della Pietra, Vincent Della Pietra,and John Lafferty.
1995.
Inducing fea-tures of random fields.
Technical ReportCMU-CS-95-144, Carnegie Mellon Univer-sity, May.Adwait Ratnaparkhi.
1997.
A linear ob-served time statistical parser based onmaximum entropy models.
In Proceedingsof Second Conference On Empirical Meth-ods in Natural Language Processing.Jeffrey C. Reynar and Adwait Ratnaparkhi.1997.
A maximum entropy approach toidentifying sentence boundaries.
In Pro-ceedings of the 5th Applied Natural Lan-guage Processing Conference.Ronald Rosenfeld.
1996.
A maximum en-tropy approach to adaptive statistical lan-guage modeling.
Computer, Speech andLanguage, (10):187-228.Kiyoaki Shirai, Kentaro Inui, TakenobuTokunaga, and Hozumi Tanaka.
1996.A maximum entropy model for estimat-ing lexical bigrams (in Japanese).
In SIGNotes of the Information Processing Soci-ety of Japan, number 96-NL-116.Takehito Utsuro, Takashi Miyata, and YujiMatsumoto.
1997.
Maximum entropymodel learning of subcategorizatoin pref-erence.
In Proceedings of the 5th Work-shop on Very Large Corpora, pages 246-260, August.1175
