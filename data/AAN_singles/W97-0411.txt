Trans la t ion  Methodo logy  in the  Spoken Language Trans la tor :An  Eva luat ionDavid CarterRalph BecketManny RaynerRobert Eklund Sabine Kirchmeier-AndersenCatriona MacDermid Christina PhilpMats Wir4nSRI InternationalSuite 23, Millers YardCambridge CB2 1RQUnited Kingdomdmc@cam.sr i .
comrvab l@cam,  s r i .
com Cat r iona .
I .Macdermid@te l ia .
semanny@cam.sr i .
com Mats .G .Wi ren@te l ia .
seTelia Research AB Handelshcjskolen i Kc~benhavnSpoken Language Processing Institut for DatalingvistikS-13680 Haninge Dalgas Have 15Sweden DK-2000 FrederiksbergDenmarkRober t .
H. Ek lund@te l ia .
sesab ine .
id@cbs .dkcp .
id@cbs .dkAbstractIn this paper we describe how thetranslation methodology adopted for theSpoken Language Translator (SLT) ad-dresses the characteristics of the speechtranslation task in a context where it isessential to achieve easy customizationto new languages and new domains.
Wethen discuss the issues that arise in anyattempt to evaluate a speech translator,and present the results of such an evalu-ation carried out on SLT for several lan-guage pairs.1 The nature of the speechtranslation taskSpeech translation is in many  respects a particu-larly difficult version of the translation task.
Highquality output is essential: the speech producedmust sound natural if it is to be easily compre-hensible.
The quality of the translation itself mustalso be high, in spite of the fact that, by the natureof the problem, no post-editing is possible.
Thingsare equally difficult on the input side: pre-editing,too, is difficult or impossible, yet ill-formed inputand recognition errors are both likely to be quitecommon.
Thus robust analysis and translation arealso required.
Furthermore, any attempted solu-tions to these problems must be capable of oper-ating at a speed close enough to real time thatusers are not faced with unacceptable delays.Together, these factors mean that speech trans-lation is currently only practical for limited do-mains, typically involving a vocabulary of a fewthousand words.
Because of this, it is desir-able that a speech translator should be easilyportable to new domains.
Portability to new lan-guages, involving the acquisition of both monolin-gual and cross-linguistic information, should alsobe as straightforward as possible.
These endscan be achieved by using general-purpose com-ponents for both speech and language processingand training them on domain-specific speech andtext corpora.
The training should be automatedwhenever possible, and where human interventionis required, the process should be deskilled to thelevel where, ideally, it can be carried out by peo-ple who are familiar with the domain but are notexperts in the systems themselves.These points will be discussed in the context ofthe Spoken Language Translator (SLT) (Rayner,Alshawi eta/, 1993; Agn~s et al, 1994; Raynerand Carter, 1997), a customizable speech trans-lator built as a pipelined sequence of general-purpose components.
These components are: aversion of the Decipher (TM) speech recognizer(Murveit eta/, 1993) for the source language; acopy of the Core Language Engine (CLE) (Al-shawi (ed), 1992) for the source language; anothercopy of the CLE  for the target language; and atarget language text-to-speech synthesizer.The current SLT  system carries out multi-lingual speech translation in near real time in theATIS domain (Hemphill et al, 1990) for severallanguage pairs.
Good demonstration versions ex-ist for the four pairs English ~ Swedish, EnglishFrench, Swedish ~ English and SwedishDanish.
Preliminary versions exist for five morepairs: Swedish ~ French, French --~ English, En-glish ~ Danish, French --d. Spanish and English--~ Spanish.We describe the methodology used to build theSLT system itself, particularly in the areas of cus-tomization (Section 2), robustness (Section 3),and multilinguality (Section 4).
For further de-tails on the topics of customization and multilin-73guality, see (Rayner, Bretan et al 1996; Rayner,Carter et al 1997); and on robustness, ee (Raynerand Carter, 1997).
We then discuss the evalu-ation of speech translation systems.
This is anarea that deserves more attention than it has re-ceived to date; indeed, it is not obvious how bestto perform such an evaluation so as to measuremeaningfully the performance both of the overallsystem and of each of its components.
In Sec-tions 5 and 6 of this paper, we therefore considerthe characteristics an evaluation should have, anddescribe one we have carried out, discussing theextent to which it meets the desired criteria.2 Customizat ion to languages anddomainsIn the Core Language Engine, the languag e pro-cessing component of the Spoken Language Trans-lator system, we address the requirement of porta-bility by maintaining a clear separation between(I) the system code; (2) linguistic rules, includinglexicon entries, to generate possible analyses andtranslations non-deterministically; and (3) statis-tical information, to choose between these possi-bilities.
The practical advantage of this architec-ture is that most of the work involved in portingthe system to a new domain is concerned with theparts of the system that can be modified by non-experts: the central activities are addition of newlexicon entries, and supervised training to derivethe statistical preference information.
Porting tonew languages is a more complex task, but stillonly involves modifications to a relatively smallsubset of the whole system.
In more detail:(I) The system code is completely general-purposeand does not need any changes for new domainsor, other than in exceptional cases, I for new lan-guages.
(2) The more complex of the linguistic rules fora given language are the grammar, the func-tion word lexicon, and the macros defining com-mon content word behaviours (count noun, tran-sitive verb, etc).
These are defined using explicitfeature-value equations which must be written bya skilled grammarian.
For a given language pair,the more complex transfer rules, which tend to befor function .words and other commonly-occurring,idiosyncratic words, can also involve arbitrarilylarge, recursive structures.
However, nearly all ofthese monolingual and bilingual rules are domain-independent.On  the other side of the coin, the main domain-dependent aspects of a linguistic description aret E.g.
in our initial extension from English to lan-guages with more complicated morphology, which ne-cessitated the development of a morphological pro-cessor based on the two-level formalism (see (Carter,1995)).lexicon entries defining content words in termsof existing behaviours, and simple (atomic-to-atomic) transfer rules.
These do need to be cre-ated manually for each new domain, but they aresimple enough to be defined by non-experts withthe help of relatively simple graphical tools.
SeeFigures i and 2 for some examples of these twokinds of rule (the details of the formalism areunimportant here, we intend simply to illustratethe differences in complexity).When moving to a new language, more expertintervention is typically required than for a newdomain, because many of the complex rules doneed some modifications.
However, we have foundthat the amount of work involved in developingnew grammars for Swedish, French, Spanish andmost recently Danish has always been at least anorder of magnitude less than the effort requiredfor the original grammar (Gambgck and Rayner,1992; Rayner, Carter and Bouillon, 1996; Rayner,Carter et al 1997).
(3) The statistical information used in analy-sis is entirely derived from the results of super-vised training on corpora carried out using theTreeBanker (Carter, 1997), a graphical tool thatpresents a non-expert user with a display of thesalient differences between alternative analyses inorder that the correct one may be identified.
Oncea user has become accustomed to the system,around two hundred sentences per hour may beprocessed in this way.
This, together with theuse of representative subcorpora (Rayner, Bouil-lon and Carter, 1995) to allow structurally equiv-alent sentences to be represented by a single ex-ample, means that a corpus of many  thousandsof sentences can be judged in just a few personweeks.
The principal information extracted auto-matically from a judged corpus is:?
Constituent pruning rules, which allow thedetection and removal, at intermediate stagesof parsing, of syntactic constituents occur-ring in contexts where they are unlikely tocontribute to the correct parse.
Removingthese constituents significantly constrains thesearch space and speeds up parsing (Raynerand Carter, 1997).
* An  automatic tuning of the grammar to thedomain using the technique of Explanation-Based Learning (van Harmelen and Bundy,1988; Rayner, 1988; Samuelsson and Rayner,1991; Rayner and Carter, 1996).
Thisrewrites it to a form where only commonly-occurring rule combinations are represented,thus reducing the search space still furtherand giving an additional significant speedup.?
Preference information attached to certaincharacteristics of full analyses of sentences -the most important being semantic triples ofhead, relationship and modifier - which allow74Syntax rule ~r S- -~NP VP:syn(s_np_vp_Normal, core,\[s:\[@s_np_feats(MMM), @vp_feats(MM),~sententialsubj=SS,sai=Aux, hascomp=n,conjoined=n\],np:\[@snp_feats(MMM),vform=(fin\/to), relational=_,temporal=_,agr=Ag,sentential=SS, wh=_, whmoved=_,pron=_,nform=Sfm\],vp:\[?vp_feats(MM),vform=(\(en)),agr=Ag,sai=Aux, modifiable=_,mainv=_,beadfinal=_,subjform=Sfm~\] ).Macro definition ~rsyntaxoftransitive verb:macro(v_subj_obj,\[v:\[vform=base,mhdfl=A,passive=A,gaps=B,conjoined=n,subcat=\[np:\[relational=_,passive=A,wh=_,gap=_,gaps=B,temporal=_,pron=_,case=nonsubj\]\]\]\]).Trans~r rule relating English a~ective "early" and French PP "de bonne heure':trule(\[eng,fre\],semi_lex(early-debonne_heure),\[early_NotLate,~r(arg)\]?form(prep('de bonne beure_Early'),_,P" \[P,tr(arg),@term (ref (pro, de _bonne _heure, sing, _),V,W" \[time,W\] )+_\] )).Figure i: Complex, domain-independent linguistic rulesa selection to be made between competing fullanalyses.
See (Alshawi and Carter, 1994) and(Carter, 1997) for details.A similar mechanism has been developed to al-low users to specify appropriate translations, giv-ing rise to preferences on outcomes of the transferprocess.
Work on this continues.3 RobustnessRobustness in the face of ill-formed input andrecognition errors is tackled by means of a "multi-engine" strategy (Frederking and Nirenburg, 1994;Rayner and Carter, 1997), combining two differ-ent translation methods.
The main translationmethod uses transfer at the level of QLF  (Alshawiet al, 1991; Rayner and Bouillon, 1995); this issupplemented by a simpler, glossary-based trans-lation method.
Processing is carried out bottom-up.
Roughly speaking, the QLF  transfer methodis used to translate as much as possible of the in-put utterance, any remaining gaps being filled byapplication of the glossary-based method.In more detail, source-language parsing goesthrough successive stages of lexical (morpholog-ical) analysis, low-level phrasal parsing to iden-tify constituents such as simple noun phrases, andfinally full sentential parsing using a version ofthe original grammar tuned to the domain usingexplanation-based learning (see Section 2 above).Parsing is carried out in a bottom-up mode.
Af-ter each parsing stage, a corresponding translationoperation takes place on the resulting constituentlattice.
Translation is performed by using theglossary-based method at the early stages of pro-cessing, before parsing is initiated, and by usingthe QLF-transfer method during and after pars-ing.
Each successful transfer attempt results ina target language string being added to a target-side lattice.
Metrics are then applied to choosea path through this lattice.
The criteria used toselect the path involve preferences for sequencesthat have been encountered in a target-languagecorpus; for the use of more sophisticated trans-fer methods over less sophisticated; and for largerover smaller chunks.The bottom-up approach contributes to robust-ness in the obvious way: if a single analysis can-not be found for the whole utterance, then trans-lations can be produced for partial analyses thathave already been found.
It also contributes tosystem response in that the earlier, more local,shallower methods of analysis and transfer usu-ally operate very quickly to produce an attempt attranslation.
The target-language user may inter-rupt processing before the more global methodshave finished if the translation (assuming it canbe viewed on a screen) is adequate, or the sys-tem itself may abandon a sentence, and presentits current best translation, if a specified time haselapsed.Figure 3 exemplifies the operation of the multi-engine strategy as well as of the preferences ap-plied to analysis and transfer.
2 The N-best list2The example chosen was the most interesting ofthe dozen or so in our most recent demonstration ses-sion, and the intermediate results have been repro-75Lexicon entry, using transitive verb macro, for "serve"asin "Does Continentalserve Atlanta?
":ir(serve,v_subj_obj,serve_F1yTo).Trans~r rule relating that sense of "serve" to one sense of French "desservir":t ru le ( \ [eng , f re \ ]  ,lex(simple),serve_FlyTo==desservir_ServeCity).Figure 2: Simple, domain-dependent linguistic rulesdelivered by the speech recognizer contains thesentence actually uttered, "Could you show me anearly flight please?
", but only in fourth position.?
Before any linguistic processing is carried out,the word sequence at the top of the N-best listis the most preferred one, as only recognitionpreferences (shown by position in the list) areavailable.
This sequence is translated word-for-word using the glossary method, givingresult C a) in the figure.?
After lexical analysis, which effectively in-cludes part-of-speech tagging, it is deter-mined that the word "a" is unlikely to precede"are", and so "a" is dropped from the trans-lated sequence (b) - thus translating recog-nizer hypothesis 2, using the glossary-basedmethod.?
Phrasal parsing identifies "an early flight" asa likely noun phrase, so that this is for thefirst time selected for translation, in (c).
Notethat the system has now settled on the correctEnglish word sequence.
QLF-bascd transferis used for the first time, and the transferrule in Figure 1 is used to translate "early" as"de bonne heure" which, because it is a PP,is placed after "vol" (flight) by the Frenchgrammar.?
Finally, as shown in (d), an analysis anda QLF-based translation are found for thewhole sentence, allowing the inadequateword-for-word translation of "could you showme" as "*pourriez vous montrez moi" to beimproved to a more grammatical "pourriez-vous m'indiquer".We thus see the results of translation becomingsteadily more accurate and comprehensible as pro-cessing proceeds.4 Multillnguality, interlinguas andthe "N-squared problem"While using an interlingual representation wouldseem to be the obvious way to avoid the "N-squared problem" (translating between N lan-guages involves order N 2 transfer pairs), we aresceptical about interlinguas for the following rea-sons.duced from the system log file without any changesother than reformatting.Firstly, doing good translation is a mixture oftwo tasks: semantics (getting the meaning right)and collocation (getting the appearance of thetranslation right).
Defining an interlingua, evenif it is possible to do so for an increasing num-ber N of languages, really only addresses the firsttask.
Interlingual representations also tend to bcless portable to new domains, since they if theyare to be truly interlingual they normally needto be based on domain concepts, which have tobe redefined for each new domain -  a task thatinvolves considerable human intervention, muchof it at an expert level.
In contrast, a transfer-based representation can be shallower Cat the levelof linguistic predicates) while still abstracting farenough away from surface form to make most ofthe transfer rules simple atomic substitutions.Secondly, systems based on formal representa-tions are brittle: a fully interlingual system firstneeds to translate its input into a formal repre-sentation, and then realise the representation as atarget-language string.
An  interlingual system isthus inherently more brittle than a transfer sys-tem, which can produce an output without everidentifying a "deep" formal representation of theinput.
For these reasons, we prefer to stay with afundamentally transfer-based methodology; nonethe less, we include some aspects of the inter-lingual approach, by regularizing the intermedi-ate QLF  representation to make it as language-independent as possible consonant with the re-quirement that it also be independent of domain.Regularizing the representation has the positiveeffect of making the transfer rules simpler (in thelimiting case, a fully interlingual system, they be-come trivial).We tackle the N-squared problem by means oftransfer composition (Rayner, Carter and Bouil-lon, 1996; Rayner, Carter et al 1997).
If we al-ready have transfer rules for mapping from lan-guage A to language B and from language B tolanguage C, we can compose them to generate aset to translate directly from A to C. The firststage of this composition can be done automat-ically, and then the results can be manually ad-justed by adding new rules and by introducingdeclarations to disallow the creation of implausi-ble rules: these typically arise because the con-texts in which a E A can correctly be translatedto ~ E B are disjoint from those in which ~ canbe translated into 7 E C. As with the other cus-76N-best list (N=5) delivered by speech recognizer:I could you show me a are the flight please2 could you show me are  the  flight please3 could you show me in order a flight please4 could you show me an early flight pleaseS could you show meals are the flight please(a) Selected input sequence and translation after surface phase:could \[ you I show I me \[ a I are ) the \[ ftight I please \]pourriez vous montrez moi un sont les vol s'il vous plait(b) Selected input sequence and translation after lexical phase:could I you I show I me I are Ithe lflight I please t pourriez vous montrez moi sont les vol s'il vous plait(c) Setected input sequence and translation after phrasal phase:could I you I show Ime l  an early flight please Ipourriez vous montrez moi un vol de bonne heure s'il vous plait(d) Selected input sequence and translation after full parsing phase:I could you show me an early flight please Ipourriez-vous m'indiquer un vol de bonne heure s'il vous plaitFigure 3: N-best list and translation results for "Could you show me an early flight please?
"tomization tasks described here, the amount ofhuman intervention required to adjust a composedset of transfer rules is vastly less, and less special-ized, than what would be required to write themfrom scratch.In the current version of SLT, transfer ruleswere written directly for neighbouring languagesin the sequence Spanish - French - English -Swedish - Danish (most of these neighbours beingrelatively closely related), with other pairs beingderived by transfer composition.
Further detailscan be found in (Rayner, Carter et al 1997).5 Evaluation of speech translationsystems: methodological issuesThere is still no real consensus on how to evalu-ate speech translation systems.
The most com-mon approach is some version of the following.The system is run on a set of previously unseenspeech data; the results are stored in text form;someone judges them as acceptable or unaccept-able translations; and finally the system's perfor-mance is quoted as the proportion that are ac-ceptable.
This is clearly much better than noth-ing, but still contains some serious methodologicalproblems.
In particular:i.
There is poor agreement on what constitutesan "acceptable translation".
Some judges re-gard a translation as unacceptable if a singleword-choice is suboptimal.
At the other endof the scale, there are judges who will acceptany translation which conveys the approxi-mate meaning of the sentence, irrespective ofhow many grammatical or stylistic mistakesit contains.
Without specifying more closelywhat is meant by "acceptable", it is difficultto compare evaluations.2.
Speech translation is normally an interactiveprocess, and it is natural that it should beless than completely automatic.
At a min-imum, it is clearly reasonable in many con-texts to feed back to the source-language userthe words the recognizer believed it heard,and permit them to abort translation if recog-nition was unacceptably bad.
Evaluationshould take account of this possibility.3.
Evaluating a speech-to-speech system asthough it were a speech-to-text system intro-duces a certain measure of distortion.
Speechand text are in some ways very different me-dia: a poorly translated sentence in writ-ten form can normally be re-examined sev-eral times if necessary, but a spoken utter-ance may only be heard once.
In this re-spect, speech output places heavier demandson translation quality.
On  the other hand, itcan also be the case that constructions whichwould be regarded as unacceptably sloppy inwritten text pass unnoticed in speech.We are in the process of redesigning our trans-lation evaluation methodology to take account ofall of the above points.
Currently, most of our7?empirical work still treats the system as thoughit produced text output; we describe this mode ofevaluation in Section 5.1.
A novel method whichevaluates the system's actual spoken output is cur-rently undergoing initial testing, and is describedin Section 5.2.
Section 6 presents results of exper-iments using both evaluation methods.5.1 Eva luat ion  o f  speech to textt rans la t ionIn speech-to-text mode, evaluation of the system'sperformance on a given utterance proceeds as fol-lows.
The judge is first shown a text version of thecorrect source utterance (what the user actuallysaid), followed by the selected recognition hypoth-esis (what the system thought he user said).
Thejudge is then asked to decide whether the recog-nition hypothesis is acceptable.
Judges are toldto assume that they have the option of abortingtranslation if recognition is of insufficient quality;judging a recognition hypothesis as unacceptablecorresponds to pushing the 'abort'  button.When the judge has determined the acceptabil-ity of the recognition hypothesis, the text versionof the translation is presented.
(Note that it isnot presented earlier, as this might bias the deci-sion about recognition acceptability.)
The judge isnow asked to classify the quality of the translationalong a seven-point scale; the points on the scalehave been chosen to reflect the distinctions judgesmost frequently have been observed to make inpractice.
When selecting the appropriate cate-gory, judges are instructed only to take into ac-count the actual spoken source utterance and thetranslation produced, and ignore the recognitionhypothesis.
The possible judgement categories arethe following; the headings are those used in Ta-bles 1 and 2 below.Ful ly acceptab le .
Fully acceptable translation.Unnatural style.
Fully acceptable, except thatstyle is not completely natural.
This is mostcommonly due to over-literal translation.Minor syntactic errors.
One or two minorsyntactic or word-choice rrors, otherwise ac-ceptable.
Typical examples are bad choicesof determiners or prepositions.Major syntactic errors.
At least one major orseveral minor syntactic or word-choice er-rors, but the sense of the utterance is pre-served.
The most common example is an er-ror in word-order produced when the systemis forced to back up to the robust translationmethod.Partial translation.
At least half of the utter-ance has been acceptably translated, and therest is nonsense.
A typical example is whenmost of the utterance has been correctly rec-ognized and translated, but there is a short'false start' at the beginning which has re-sulted in a word or two of junk at the startof the translation.Nonsense .
The translation makes no sense.
Themost common reason is gross misrecognition,but translation problems can sometimes bethe cause as well.Bad  t rans la t ion .
The translation makes somesense, but fails to convey the sense of thesource utterance.
The most common reasonis again a serious recognition error.Results are presented by simply counting thenumber of translations in a run which fall intoeach category.
By taking account of the "unac-ceptable hypothesis" judgements, it is possible toevaluate the performance of the system either ina fully automatic mode, or in a mode where thesource-language user has the option of abortingmisrecognized utterances.5.2 Evaluation of  speech to speechtranslationOur intuitive impression, based on many eval-uation runs in several different language-pairs,is that the "fine-grained" style of speech-to-text evaluation described in the preceding sec-tion gives a much more informative picture ofthe system's performance than the simple accept-able/unacceptable dichotomy.
However, it raisesan obvious question: how important, in objec-tive terms, are the distinctions drawn by the fine-grained scale?
The preliminary work we now goon to describe attempts to provide an empiricallyjustifiable answer, in terms of the relationship be-tween translation quality and comprehensibilityof output speech.
Our goal, in other words, isto measure objectively the ability of subjects tounderstand the content of speech output.
Thismust be the key criterion for evaluating a candi-date translation: if apparent deficiencies in syntaxor word-choice fail to affect subject's ability to un-derstand content, then it is hard to say that theyrepresent real loss of quality.The programme sketched above is difficult or,arguably, impossible to implement in a generalsetting.
In a limited domain, however, it ap-pears quite feasible to construct a domain-specificform-based questionnaire designed to test a sub-ject's understanding of a given utterance.
In theSLT system's current domain of air travel plan-ning (ATIS), a simple form containing about 20questions extracts enough content from most ut-terances that it can be used as a reliable measureof a subject's understanding.
The assumption isthat a normal domain utterance can be regardedas a database query involving a limited numberof possible categories: in the ATIS domain, theseare concepts like flight origin and destination, de-parture and arrival times, choice of airline, and78so on.
A detailed description of the evaluationmethod follows.The judging interface is structured as a hyper-text document that can be accessed through a.web-browser.
Each utterance is represented byone web page.
On  entering the page for a givenutterance, the judge first clicks a button that playsan audio file, and then fills in an HTML form de-scribing what they heard.
Judges are allowed tostart by writing down as much as they can of theutterance, so as to keep it clear ir; their memoryas they fill in the form.The form is divided into four major sections.The first deals with the linguistic form of the en-quiry, for example, whether it is a command (im-perative), a yes/no-question or a wh-question.
Inthe second section the judge is asked to write downthe principal '!object" of the utterance.
For exam-ple, in the utterance "Show flights from Boston toAtlanta", the principal object would be "flights".The third section lists some 15 constraints on theobject explicitly mentioned in the enquiry, like"...one-way from New York to Boston on Sun-day".
Initial testing proved that these three sec-tions covered the form and content of most en-quiries within the domain, but to account for un-foreseen material the judge is also presented witha "miscellaneous" category.
Depending on thecharacter of the options, form entries are eithermultiple-choice or free-text.
All form entries maybe negated ("No stopovers") and disjunctive en-quiries are indicated by dint of indexing ("Deltaon Thursday or American on Friday").
When thepage is exited, the contents of the completed formare stored for further use.Each translated utterance is judged in three ver-sions, by different judges.
The first two versionsare the source and target speech files; the thirdtime, the form is filled in from the tezt versionof the source utterance.
(The judging tool allowsa mode in which the text version is displayed in-stead of an audio file being played.)
The intentionis that the source text version of the utteranceshould act as a baseline with which the source andtarget speech versions can respectively be com-pared.
Comparison is carried out by a fourthjudge.
Here, the contents of the form entries fortwo versions of the utterance are compared.
Thejudge has to decide whether the contents of eachfield in the form are compatible between the twoversions.When the forms for two versions of an utterancehave been filled in and compared, the results canbe examined for comprehensibility in terms of thestandard notions of precision and recall.
We saythat the recall of version 2 of the utterance withrespect to version I is the proportion of the fieldsfilled in version 1 that are filled in compatibly inversion 2.
Conversely, the precision is the propor-tion of the fields filled in in version 2 that are filledin compatibly in version i.The recall and precision scores together definea two-element vector which we will call the com-prehensibility of version 2 with respect to versioni.
We can now define C,o~,ce to be the compre-hensibility of the source speech with respect tothe source text, and Ct~,get to be the comprehen-sibility of the target speech with respect to thesource text.
Finally, we define the quality of thetranslation to be I - (C,~,ce - Cta,get), whereCm~rce - Cta~get in a natural way can be inter-preted as the extent to which comprehensibilityhas degraded as a result of the translation process.At the end of the following section, we describe anexperiment in which we use this measure to eval-uate the quality of translation in the English --~French version of SLT.6 An  eva luat ion  o f  the  SpokenLanguage Trans la torWe begin by presenting the results of tests run inspeech-to-text mode on versions of the SLT systemdeveloped for six different language-pairs: EnglishSwedish, English ~ French, Swedish --+ En-glish, Swedish ~ French, Swedish -+ Danish, andEnglish ~ Danish.
Before going any further, itmust be stressed that the various versions of thesystem differ in important ways; some language-pairs are intrinsically much easier than others, andsome versions of the system have received far moreeffort than others.In terms of diffculty, Swedish --~ Danish isclearly the easiest language-pair, and SwedishFrench is clearly the hardest.
English ~ French iseasier than Swedish ~ French, but substantiallymore diffcult than any of the others.
English --~Swedish, Swedish ~ English and English --~ Dan-ish are all of comparable difficulty.
We presentapproximate figures for the amounts of effort de-voted to each language pair in conjunction withthe other results.We evaluated performance on each language-pair in the manner described in Section 5.1 above,taking as input two sets of 200 recorded speechutterances each (one for English and one forSwedish) which had not previously been used forsystem development.
Judging was done by sub-jects who had not participated in system develop-ment, were native speakers of the target language,and were fluent in the source language.
Resultsare presented both for a fully automatic versionof the system (Table i), and for a version with asimulated 'abort' button (Table 2).Finally, we turn to a preliminary experi-ment which used the speech-to-speech evaluationmethodology from Section 5.2 above.
A set of 200previously unseen English utterances were trans-lated by the system into French speech, usingthe same kind of subjects as in the previous ex-periments.
Source-language and target-language79speech was synthesized using commercially avail-able, state-of-the-art synthesizers (TrueTalk fromEntropies and CNETVOX from ELAN Informa-tique, respectively).
The subjects were only al-lowed to hear each utterance once.
The resultswere evaluated in the manner described, to pro-duce figures for comprehensibility of source andtarget speech respectively.
The figures are pre-sented in Table 3; we expect o be able to presenta more detailed iscussion of their ~ignificance bythe time of the workshop.In summary, we have improved the standardevaluation method for speech translation by de-veloping a feasible alternative with a more fine-grained taxonomy of acceptability.
In order tomake the task of evaluation more realistic, we havealso created a method in which instead of textualtranslations it is the spoken form that is judged.This method is currently in embryonic form, butthe pilot experiment described here leads us tothink that the method shows promise for furtherdevelopment.An interesting future task would be to in-vestigate the significance of various kinds ofwritten-language translation errors in terms of re-ducing comprehensibility of the spoken output.This would amount o systematically comparingCta,#et with results obtained in speech-to-textevaluations, divided up according to error cate-gories such as those in our taxonomy.AcknowledgementsThe Danish-related work reported here wasfunded by SRI International and Handels-hc~jskolen i Kebenhsvn.
Other work was fundedby Telia Research AB under the SLT-2 project.We would like to thank Beats Forsmark, NathalieKirchmeyer, Carin Lindberg, Thierry Reynier andJennifer Spenader for carrying out judging tasks.ReferencesAgn~, M-S., Alshawi, H., Bretan, I., Carter, D.,Coder, K., Collins, M., Crouch, R., Di-galakis, V., Ekholm, B., Gamb~ck, B., Kaja, J.,Karlgren, J., Lyberg, B., Price, P., Pulman, S.,Rayner, M., Ssmuelsson, C. and Svensson, T.1994.
Spoken Language Translator: First YearReport.
SRI Cambridge Technical report CRC-043.
sAlshawi, H.
(ed.)
1992.
The Core Language En-gine.
MIT Press.Alshawi, H., D. Carter, B. Gamb?ck andM.
Rayner.
1991.
Transfer through Quasi Log-ical Form.
Proceedings of ACL-91.
Also SRICambridge Technical report CRC-021.~All SRI Cambridge technical reports are availablethrough WWW from http ://www.
cam.
sri .
conAlshawi, H., and D. Carter.
1994.
Training andScaling Preference Functions for Disambigua-tion Computational Linguistics 20:4, 635-648.Also SRI Cambridge Technical report CRC-041.Carter, D. 1995.
Rapid Development of Mor-phological Descriptions for Full Language Pro-cessing Systems.
Proceedings of 7th EuropeanACL.
Also SRI Cambridge Technical ReportCRC-047.Carter, D. 1997.
The TreeBanker: a Tool forSupervised Training of Parsed Corpora.
Proc.ACL/EACL workshop "Computational Envi-ronments for Grammar Development and Lin-guistic Engineering", Madrid.
Also SRI Cam-bridge Technical report CRC-068.Frederking, R. and S. Nirenburg.
1994.
Threeheads are better than one.
Proc.
4th ANLP,Stuttgart, Germany, pp 95-100.Gamb~ck, B., and M. Rayner.
The Swedish CoreLanguage Engine.
Proc.
3rd Nordic Conferenceon Text Comprehension in Man and Machine,LinkSping, Sweden.
Also SRI Cambridge Tech-nical Report CRC-025.van Harmelen, F., and A. Bundy.
1988.Explanation-Based Generalization = PartialEvaluation (Research Note).
Artificial Intelli-gence 36, pp.
401-412.Hemphill, C.T., J.J. Godfrey and G.R.
Dodding-ton.
1990.
The ATIS Spoken Language Sys-tems pilot corpus.
Proc.
DARPA Speech andNatural Language Workshop, Hidden Valley,Pa., pp.
96-101.Murveit, H., Butzberger, J., Digalakis, V. andWeintraub, M. 1993.
Large Vocabulary Dic-tation using SRI's DECIPHER(TM) SpeechRecognition System: Progressive Search Tech-niques.
Proc.
Inter.
Conf.
on Acoust., Speechand Signal, Minneapolis, Mn.Pulman, S. 1992.
Unification-Based SyntacticAnalysis.
In (Alshswi (ed), 1992).Rayner, M. 1988.
Applying Explanation-BasedGeneralization to Natural-Language Process-ing.
Proc.
the International Conference onFifth Generation Computer Systems, Kyoto,pp.
1267-1274.Rayner, M., Alshawi, H., Bretan, I., Carter, D.M.,Digalskis, V., Gamb~ck, B., Kaja, J., Karl-gren, J., Lyberg, B., Price, P., Pulman, S. andSamuelsson, C. 1993.
A Speech to SpeechTranslation System Built From Standard Com-ponents.
Proc.
ist ARPA workshop on HumanLanguage Technology, Princeton, NJ.
MorganKaufmsnn.
Also SRI Technical Report CRC-031.Rayner, M. and Bouillon, P. 1995.
HybridTransfer in an English-French Spoken LanguageTranslator.
Proceedings of IA '95, Montpellier,80France.
Also SRI Cambridge Technical ReportCRC-056.Rayner, M., P. Bouillon and D. Carter.
1995.Using Corpora to Develop Limited-DomainSpeech Translation Systems.
Proc.
Translat-ing and the Computer 17.
Also SRI CambridgeTechnical Report CRC-059.Rayner, M., I. Bretan, M. Wirdn, S. Rydin andE.
Beshai.
1996.
Composition of Transfer Rulesin a Multi-Lingual MT System.- Proc.
Work-shop on Future Issues for Multilingual Text Pro-cessing.
Also SRI Cambridge Technical ReportCRC-063.Rayner, M., D. Carter and P. Bouillon.
1996.Adapting the Core Language Engine to Frenchand Spanish.
Proc.
NLP-IA, Moncton, NewBrunswick.
Also SRI Cambridge Technical Re-port CRC-061.Rayner, M. and D. Carter.
1996.
Fast Pars-ing using Pruning and Grammar Specialization.Proc.
ACL-96, Santa Cruz, Ca.
Also SRI Cam-bridge Technical Report CRC-060.Rayner, M., and D. Carter.
1997 Hybrid languageprocessing in the Spoken Language Translator.Proc.
ICASSP-97, Munich, Germany.
Also SRICambridge Technical Report CRC-064.Rayner, M., D. Carter, I. Bretan, R. Eklund,M.
Wirdn, S. Hansen, S. Kirchmeier-Andersen,C.
Philp, F. Scrensen and H. Erdman Thorn-sen. 1997.
Recycling Lingware in a Multilin-gum MT System Proc.
ACL /EACL  workshop"From Research to Commercial Applications",Madrid.
Also SRI Cambridge Technical ReportCRC-067.Samuelsson, C., and M. Rayner.
1991.
Quanti-tative Evaluation of Explanation-Based Learn-ing as an Optimization Tool for a Large-ScaleNatural Language System.
Proc.
12th IJCAI,Sydney, pp.
609-615.81Table i: Translation results for six language pairs on 200 unseen utterances, all utterances in test setcounted.
"Note that in both tables on this page, the "effort" figures refer specifically to translationwork for the language pair in question, and exclude work on grammar and lexicon development for theindividual languages.Source language EnglishTarget language Swedish\[ Effort (person-months)Fully acceptableUnnatural styleMinor syntactic errors\[ Clearly usefulMajor syntactic errorsPartial translationBorderlineNonsenseBad translationNo translationClearly useless8-1046.0%14.0%12.0%72.0%7.0%6.5%13.5%7.5%5.0%2.0%14.5%EnglishFrenchI 3-552.0%10.5%3.5%66.0%2.5%11.5%\]14.o%13.0%5.5%1.5%20.0%SwedishEnglish3-545.0%4.5%12.0%61.5%7.5%14.5%22.0%10.5%4.5%1.5%16.5%SwedishFrench1-219.0%15.0%13.0%47.0%13.0%17.5%30.5%18.0%3.5%1.0%22.5%SwedishDanishI 0.536.5%0.0%37.5%74.0%0.0%1.5%1.5%13.0%9.0%2.5%24.5%EnglishDanish<0.527.0%0.0%28.0%55.0%0.0%1.5%1.5%30.5%10.5%2.5%43.5%Table 2: Translation results for six language pairs on 200 unseen utterances, ignoring utterances judgedas recognition failures.Source languageTarget languageEffort (person-months)Fully acceptableUnnatural styleMinor syntactic errorsClearly usefulMajor syntactic errorsPartial translationBorderlineNonsenseBad translationNo translationClearly uselessEnglishSwedish8-1055.8%15.8%12.1%83.7%7.9%2.4%10.3%3.0%1.2%1.8%6.0%(Utterances ignored) I 35EnglishFrench3-565.8%12.9%3.2%81.9%2.6%5.8%8.4%4.5%3.2%1.9%9.6%45SwedishEnglish3-560.7%6.4%11.4%\[ 76.5%10.0%5.0%SwedishFrench1-223.1%19.2%15.4%57.7%12.8%14.1%SwedishDanish0.549.0%0.0%38.1%87.1%0.0%0.7%Danish<0.535.9%0.0%35.9%71.8%0.0%2.1%15.0% 26.9% 0.7% 2.1%2.9% 11.5% 4.8% 12.4%2.1% 2.6% 5.4% 11.7%1.4% 1.3% 2.0% 2.1%6.4% 15.4% 12.2% 26.2%60 44 53 55Table 3: Relative comprehensibility of source and target speech for English ~ French test on 200 unseenutterances.\[ \[Source I Target \] Difference I Quality I\[ Precision 97.6% 86.0% 11.6% t 88.4% IRecall 97.5% 84.0% 13 .5% 86.5%82
