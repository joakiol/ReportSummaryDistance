In: Proceedings o/CoNLL-2000 and LLL-2000, pages 49-54, Lisbon, Portugal, 2000.Overfitting Avoidance for Stochastic Modeling ofAttribute-Value GrammarsTony  Mu l lenAlfa-InformaticaUniversity of Groningenmullen@let, rug.
nlMi les  OsborneDivision of InformaticsUniversity of Edinburghosborne@cogsci, ed.
ac.
ukAbst rac tWe present a novel approach to the problemof overfitting in the training of stochastic mod-els for selecting parses generated by attribute-valued grammars.
In this approach, statisticalfeatures are merged according to the frequencyof linguistic elements within the features.
Theresulting models are more general than the orig-inal models, and contain fewer parameters.
Em-pirical results from the task of parse selectionsuggest hat the improvement in performanceover repeated iterations of iterative scaling ismore reliable with such generalized models thanwith ungeneralized models.1 In t roduct ionThe maximum entropy technique of statisticalmodeling using random fields has proved to bean effective way of dealing with a variety of lin-guistic phenomena, in particular where mod-eling of attribute-valued grammars (AVG's) isconcerned (Abney, 1997).
This is largely be-cause its capacity for considering overlappinginformation sources allows the most to be madeof situations where data is sparse.
Neverthe-less, it is important that the statistical featuresemployed be appropriate to the job.
If the infor-mation contributed by the features is too spe-cific to the training data, overfitting becomes aproblem (Chen and Rosenfeld, 1999; Osborne,2000).
In this event, a peak in model perfor-mance will be reached early on, and continuedtraining yields progressive deterioration i per-formance.
From a theoretical standpoint, over-fitting indicates that the model distribution isunrepresentative of the actual probabilities.
Inpractice, it makes the performance of the modeldependent upon early stopping of training.
Thepoint at which this must be done is not alwaysreliably predictable.This paper describes an approach to featureselection for maximum entropy models whichreduces the effects of overfitting.
Candidate fea-tures are built up from basic grammatical el-ements found in the corpus.
This "composi-tional" quality of the features is exploited forthe purpose of overfitting reduction by meansof \]eature merging.
In this process, featureswhich are similar to each other, save for cer-tain elements, are merged; i.e, their disjunc-tion is considered as a feature in itself, thusreducing the number of features in the model.The motivation behind this methodology is sim-ilar to that behind that of Kohavi and John(1997), but rather than seeking a proper subsetof the candidate feature set, the merging pro-cedure attempts to compress the feature set,diminishing both noise and redundancy.
Themethod iffers from a simple feature cutoff, suchas that described in Ratnaparkhi (1998), inthat the feature cutoff eliminates statistical fea-tures directly, whereas the merging procedureattempts to generalize them.
The method em-ployed here also derives inspiration from the no-tion of Bayesian model merging introduced byStolcke and Omohundro (1994).Section 2 describes parse selection and dis-cusses the "compositional" statistical featuresemployed in a maximum entropy approach tothe task.
Section 3 introduces the notion of fea-ture merging and discusses its relationship withoverfitting reduction.
Sections 4 and 5 describethe experimental models built and the results ofmerging on their performance.
Finally, section6 sums up briefly and indicates ome further di-rections for inquiry on the subject.492 Max imum ent ropy-based  parsese lec t ionThe task of parse selection involves electing thebest possible parse for a sentence from a setof possible parses produced by an AVG.
In thepresent approach, parses are ranked accordingto their goodness by a statistical model built us-ing the maximum entropy technique, which in-volves building a distribution over events whichis the most uniform possible, given constraintsderived from training data.
Events are com-posed of features, the fundamental statisticalunits whose distribution is modeled.
The modelis characterized by constraints upon the dis-tributions of features, derived from the fea-tures' empirical frequencies.
An untrained (thusunconstrained) max ent model is by defini-tion characterized by the uniform distribution.The constraints which characterize the modelare expressed as weights on individual features.Training the model involves deriving the bestweights from the training data by means ofan algorithm such as Improved Iterative Scaling(IIS) (Della Pietra et al, 1995).IIS assigns weights to features which reflecttheir distribution and significance.
With eachiteration, these weights reflect he empirical dis-tribution of the features in the training datawith increasing accuracy.
In ideal circum-stances, where the distribution of features inthe training data accurately represents he trueprobability of the features, the performance ofthe model should increase asymptotically witheach iteration of training until it eventually con-verges.
If the training data is corrupt, or noisy,or if it contains features which are too sparselydistributed to accurately represent their proba-bility, then overfitting arises.2.1 The s t ructure  of the featuresThe statistical features used for parse selectionshould contain information pertinent o sen-tence structure, as it is the information encodedin these features which will be brought o bearin prefering one parse over another.
Informationregarding constituent heads, POS tags, and lex-ical information is pertinent, as is informationon constituent ordering and other grammaticalinformation present in the data.
Most or allof these factors are considered in some formor another by current state-of-the-art statisti-cal parsers such as those of Charniak (1997),Magerman (1995) and Collins (1996).In the present approach, each feature in thefeature set corresponds to a depth-one treestructure in the data, i.e.
a mother node andall of its daughters.
Within this general struc-ture various schemata may be used to derive ac-tual features, where the information about eachnode employed in the feature is determined bywhich schema is used.
For example, one schemamight call for POS information from all nodesand lexical information only from head nodes.Another might call for lexical information onlyfrom nodes which also contain the POS tag forprepositions.
The term compositional is usedin this context o describe features built up ac-cording to some such schema from basic linguis-tic elements uch as these.
Thus each compo-sitional feature is an ordered sequence of ele-ments, where the order reflects the position inthe tree of the elements.
Instantiations of theseschemata in the data are used as the statisticalfeatures.
The first step is to run a given schemaover the data, collecting a set of features.
Thenext step is to characterize all events in the datain terms of those features.This general structure for features allows con-siderable versatility; models of widely varyingquality may be constructed.
This structure forstatistical features might be compared with theData-Oriented Parsing (DOP) of Bod (1998) inthat it considers ubtrees of parses as the struc-tural units from which statistical informationis taken.
The present approach differs sharplyfrom DOP in that its trees are limited to a depthof one node below the mother and, more impor-tantly, in the fact that the maximum entropyframework allows modeling without the inde-pendence assumptions made in DOP.Since maximum entropy allows for overlap-ping information sources, features derived usingdifferent schemata (that is, collecting differentpieces of node-specific information) may be col-lected from the same subtrees, and used simul-taneously in a single model.3 Feature  merging and over f i t t ingreductionThe idea behind feature merging is to reduceoverfitting through changes made directly tothe model.
This is done by combining highly50VVD\]\ [~VD\ ]  \[bIPl\] \[~IN1\]offered\]U\[VVD\]V+VD \[?qP 1\] \[IqN 1\]allow J\[oVffVerDed V allow\] \[IqP1\] \[~qN1\]Figure 1: An example of feature merging.
Thetop two features are merged in the form of thebottom feature, where the lexical elements havebeen replaced by their disjunction.
The mergedfeature represents he union of the sets of tokensdescribed by the unmerged feature types.
Allinstances of the original two features would nowbe replaced in the data by the merged feature.specific features which occur rarely to producemore general features which occur more often,resulting in fewer total features used.
Even ifthe events are not noisy or inaccurate in actualfact, they may still contribute to overfitting iftheir features occur too infrequently in the datato give accurate frequencies.
The merging pro-cedure seeks to address overfitting at the level ofthe features themselves and remain true to thespirit of the maximum entropy approach, whichseeks to represent what is unknown about thedata with uniformity of the distribution, ratherthan by making adjustments on the model dis-tribution itself, such as the Gaussian prior ofOsborne (2000).Each feature, as described above, is made upof discrete elements, which may include suchobjects as lexical items, POS tags, and gram-matical attribute information, depending on theschema being used.
The rarity of the featurein the data is largely--although not entirely--determined by the rarity of elements withinit.
In the present merging scheme, a set ofelements is collected whose empirical frequen-cies are below some predetermined cutoff point.Note that the use of the term "cutoff" hererefers to the empirical frequency of elements offeatures rather than of features themselves, asin Ratnaparkhi (1998).
All features containingelements in this set will be altered such thatthe cutoff element is replaced by a uniform dis-junctive element, effectively merging all simi-larly structured features into one, with the dis-parate elements replaced by the disjunctive l-ement.
An example may be seen in figure 1,where the union of the two features at top of thefigure is represented as the feature below them.The merged elements in this case are the lexicalitems offered and allow.
Such a merge wouldtake place on the condition that the empiricalfrequencies of both elements are below a certaincutoff point.
If so, the elements are replaced bya new element representing the disjunction ofthe original elements, creating a single feature.This feature then replaces all instances of bothof the original features.
If both of the originalfeatures appear once each together in an event,then two instances of the merged feature willappear in that event in the new model.4 ExperimentsThe experiments described here were conductedusing the Wall Street Journal Penn Treebankcorpus (Marcus et al, 1993).
The gram-mar used was a manually written broad cov-erage DCG style grammar (Briscoe and Car-roll, 1997).
Parses of WSJ sentences producedby the grammar were ranked empirically usingthe treebank parse as a gold standard accord-ing to a weighted linear combination of cross-ing brackets, precision, and recall.
If more thanfifty parses were produced for a sentence, the51best fifty were used and the rest discarded.
Forthe training data, the empirical rankings of allparses for each sentence were normalized so thetotal parse scores for each sentence added to aconstant.
The events of the training data con-sisted of parses and their corresponding nor-malized score.
These scores were furthermoretreated as frequencies.
Thus, high ranked parseswould be treated as events occurring more fre-quently in the training data, and low rankedparses would be treated as occurring rarely.The features of the unmerged model consistedof depth-one trees carrying node information ac-cording to the following schema: the POS tag ofthe mother, POS tags of all daughters orderedleft to right, HEAD+ information for the headdaughter, and lexical information for all daugh-ters carrying a verbal or prepositional POS tag.The features themselves were culled using thisschema on 2290 sentences from the trainingdata.
The feature set consisted of 38,056 fea-tures in total, of which 6561 were active in themodel (assigned non-zero weights) after the fi-nal iteration of IIS.
Two models using this fea-ture set were trained, one on only 498 trainingsentences, a subset of the 2290 sentences usedto collect the features, and the other on nearlyten times that number, 4600 training sentences,a superset of the same set of sentences.Several merged models were made based oneach of these unmerged models, using variouscutoff numbers.
Cutoffs were set at empiricalfrequencies of 100, 500, 1000, 1250, and 1500elements.
For each model merge, all elementswhich occurred in the training data fewer timesthan the cutoff number were replaced in eachfeature they appeared in by the uniform dis-junctive element, and the merged features thentook the place of the unmerged features.Iterative scaling was performed for 150 iter-ations on each model.
This number was cho-sen arbitrarily as a generous but not gratuitousnumber of iterations, allowing general trends tobe observed.The models were tested on approximately5,000 unseen sentences from other parts ofthe corpus.
The performance of each modelwas measured at each iteration by binary bestmatch.
The model chose a single top parse andif this parse's empirical rank was the highest(or equal to the highest) of all the parses for thesentence, the model was awarded a point for thematch, otherwise the model was awarded zero.The performance rating reflects the percentageof times that the model chose the best parse ofall possible parses, averaged over all test sen-tences.5 Resu l ts5.1 Per fo rmance  of unmerged modelsOf the unmerged models, as expected, the onetrained on the smaller set shows the worst per-formance and most drastic overfitting.
Its peakat approximately 42.5% performance comesearly, at around 20 iterations of IIS, and sub-sequently drops to 40.5% at around 50 itera-tions.
At around 80 iterations, it plunges toabout 39%, where it remains.
This model's per-formance may be seen in figure 2 represented bythe solid black line.In figure 3, the solid black line represents heoriginal model trained on 4600 sentences.
Thefeature set is the same, although in this case allof the 38,057 features are active.
The advantageof having so much more training data is evident.The performance peaks at a much higher leveland overfitting, although present, is much lessdrastic at the end of 150 iterations.
Neverthe-less, the curve still reaches a maximum pointfairly early on, at about 40 iterations, and theperformance diminishes from there.5.2 Per fo rmance  of merged modelsDifferent cutoffs yielded varying degrees of im-provement.
A cutoff of 100 elements seemed tomake no meaningful difference ither way witheither model.
Increasing the cutoff for the 498sentence-trained model both lowered the peakbefore 40 iterations and raised the dip after 80in a fairly regular fashion.
The best balanceseemed to be struck with a cutoff of 1250.
Inthis case the number of active features in themodel was reduced to 4801.As can be seen from figure 2, the mergedmodel, represented by the dotted line, shows amuch more predictable improvement, i s curvemuch closer to the optimal asymptotic mprove-ment.
In terms of actual performance, the earlypeak of the unmerged model is not present at allin the merged model, which catches up betweenaround 40 and 80 iterations.
After 80 iterations,the merged model begins to outperform the un-merged model, which has begun to suffer from5243 , ,.
.
.
.
4241.5~ " ~'~.... //.///..',,i .... ~ .. ,x r "~'~'i; 2o 2o ;o ' ' ,,o 100 120iteraUons"unmerged" - -"merged" - .
.
.
.464544~ 43~.
42414o39"unmerge?
- -"merged"i20 40  60  80  100 120 t40  160i te rat ionsFigure 2: For the model trained on 498 sen-tences, features containing elements appearingfewer than 1250 times are merged.
The earlypeak of the unmerged model gives way to dras-tic overfitting.
The merged model, on the otherhand, does not reach this peak, but overfittingis not present.severe overfitting.
The merged model, on theother hand, shows no evidence of overfitting.Likewise, the merged model represented bythe dotted line in figure 3 shows no overfittingeither, an improvement in that regard over itsunmerged counterpart.
For this model, the bestcutoff of those tried appeared to be 500, andthe number of active features was reduced to77,286.
Higher cutoffs led to slower rates of im-provement and lower levels of performance.Both merging operations may be viewed asyielding improvements over the unmerged mod-els, as the accuracy of the model should ide-ally increase with each iteration of the IIS al-gorithm until it converges.
It is likely that fur-ther iterations would yield even more clear im-provement, although it is also possible that themerged models themselves would begin to ex-hibit overfitting after some point.
The rate ofincrease in performance and the point of onsetof overfitting varies from model to model.
Ingeneral, predictable improvement, even if grad-ual, is preferable to sporadic peaking and dras-tic overfitting.
This may not always be the casein practice.6 Conc lus ionThe feature merging strategy described in thispaper may be employed to reduce overfitting inFigure 3: For the model trained on 4600 sen-tences, features containing elements appearingfewer than 500 times are merged.
The overfit-ting in the unmerged model, represented by thesolid line, is less drastic due to more extensivetraining material, but an improvement can stillbe seen in the curve of the merged model.situations where statistical features are built upcompositionally from basic elements.
As men-tioned, the merging strategy bears certain simi-larities with other methods of overfitting reduc-tion, such as standard feature cutoffs where en-tire features appearing less than some numberof times are ignored (Ratnaparkhi, 1998).
Intu-itively, it seems that in a sparse data situation,it would be beneficial to retain the general in-formation in features, rather than ignoring rarefeatures entirely.
It would be worthwhile to ver-ify this suspicion by comparing the present ap-proach directly with a simple feature cutoff, andfurthermore comparing a simple cutoff to onewhere the low-frequency features were mergedaccording to the present scheme, rather thansimply discarded.
It is to be expected that acombination of both approaches would be likelyto outperform either individual approach.
Howmuch improvement may be gained remains tobe seen.ReferencesSteven P. Abney.
1997.
Stochastic attribute-valuegrammars.
Computational Linguistics, 23(4):597-618, December.Rens Bod and Ronald M. Kaplan.
1998.
A proba-bilistic corpus-driven model for lexical-functionalanalysis.
In Proceedings o/ A CL/COLING '98,53Montreal.Ted Briscoe and John Carroll.
1997.
Automaticextraction of subcategorization from corpora.
InProceedings of the 5 th Conference on AppliedNLP, pages 356-363, Washington, DC.Eugene Charniak.
1997.
Statistical parsing with acontext-free grammar and word statistics.
Tech-nical Report CS-95-28, Department of ComputerScience, Brown University.Stanley F. Chen and Ronald Rosenfeld.
1999.A gaussian prior for smoothing maximum en-tropy models.
Technical Report CMU-CS-99-108,Carnegie Mellon University, School of ComputerScience.Michael John Collins.
1996.
A new statistical parserbased on bigram lexical dependencies.
In ArivindJoshi and Martha Palmer, editors, Proceedingsof the 3~th Annual Meeting of the ACL, pages184-191, San Francisco.
Association for Compu-tational Linguistics, Morgan Kaufmann Publish-ers.Stephen Della Pietra, Vincent Della Pietra, andJohn Lafferty.
1995.
Inducing features of ran-dom fields.
Technical Report CS-95-144, CarnegieMellon University, School of Computer Science.Ron Kohavi and George H. John.
1997.
Wrappersfor feature subset selection.
Artificial Intelligence:special issue on relevance, 97:273-324.David M. Magerman.
1995.
Statistical decision-treemodels for parsing.
In Proceedings of the 33rd An-nual Meeting of the A CL, pages 276-283.M.P.
Marcus, B. Santorini, and M.A.
Marcinkievicz.1993.
Building a large annotated corpus of En-glish: the Penn Treebank.
Computational Lin-guistics, 19:313-330.Miles Osborne.
2000.
Estimation of stochas-tic attribute-value grammars using an informa-tive sample.
In Proceedings of Coling 2000,Saarbrficken.Adwait Ratnaparkhi.
1998.
Maximum EntropyModels for Natural Language Ambiguity Resolu-tion.
Ph.D. thesis, University of Pennsylvania.Andreas Stolcke and Stephen Omohundro.
1994.
In-ducing probabilistic grammars by bayesian modelmerging.
In R.C.
Carrasco and J. Oncina, editors,Grammatical Inference and Applications, pages106-118.
Springer.54
