A Bag of Useful Techniques for Efficient and Robust  ParsingBernd  K ie fer  t, Hans -U l r i ch  Kr ieger t ,  J ohn  Car ro l l  $, and Rob  Ma louf fIGerman Research Center for Artificial Intell igence (DFKI)Stuhlsatzenhausweg 3, D-66123 Saarbriicken$Cognitive and Comput ing  Sciences, University of SussexFalmer, Br ighton BN1 9QH, UK*Center for the Study of Language and Information, Stanford UniversityVentura Hall, Stanford, CA 94305-4115, USA{kiefer, krieger}@dfki, de, j ohnca@cogs, susx.
ac.
uk, malouf@csli, stanford, eduAbst rac tThis paper describes new and improved tech-niques which help a unification-based parser toprocess input efficiently and robustly.
In com-bination these methods result in a speed-up inparsing time of more than an order of magni-tude.
The methods are correct in the sense thatnone of them rule out legal rule applications.1 Int roduct ionThis paper describes several generally-applicable techniques which help a unification-based parser to process input efficiently androbustly.
As well as presenting a number of newmethods, we also report significant improve-ments we have made to existing techniques.The methods preserve correctness in the sensethey do not rule out legal rule applications.In particular, none of the techniques involvestatistical or approximate processing.
We alsoclaim that these methods are independentof the concrete parser and neutral with re-spect to a given unification-based grammartheory/formalism.How can we gain reasonable efficiency in pars-ing when using large integrated grammars withseveral thousands of huge lexicon entries?
Ourbelief is that there is no single method whichachieves this goal alone.
Instead, we have todevelop and use a set of "cheap" filters whichare correct in the above sense.
As we indicatein section 10, combining these methods leadsto a speed-up in parsing time (and reduction ofspace consumption) of more than an order ofmagnitude when applied to a mature, well en-gineered unification-based parsing system.We have implemented our methods as exten-sions to a HPSG grammar development environ-ment (Uszkoreit et al, 1994) which employs asophisticated typed feature formalism (Kriegerand Sch~ifer, 1994; Krieger and Sch~ifer, 1995)and an advanced agenda-based bottom-up chartparser (Kiefer and Scherf, 1996).
A special-ized runtime version of this system is currentlyused in VERBMOBIL as the primary deep anal-ysis component.
IIn the next three sections, we report on trans-formations we have applied to the knowledgebase (grammar/lexicon) and on modificationsin the core formalism (unifier, type system).
InSection 5-8, we describe how a given parser canbe extended to filter out possible rule applica-tions efficiently before performing "expensive"unification.
Section 9 shows how to computebest partial analyses in order to gain a certainlevel of robustness.
Finally, we present empir-ical results to demonstrate he efficiency gains,and speculate on extensions we intend to workon in the near future.
Within the different sec-tions, we refer to three corpora we have used tomeasure the effects of our methods.
The refer-ence corpora for English, German, and Japaneseconsist of 1200-5000 samples.2 P recompi l ing  the  Lex iconLexicon entries in the development system aresmall templates that are loaded and expandedon demand by the typed feature structure sys-tem.
Thereafter, all lexical rules are applied tothe expanded feature structures.
The results ofthese two computations form the input of theanalysis tage.1VERBMOBIL (Wahlster, 1993) deals with the trans-lation of spontaneously spoken dialogues, where only aminor part consists of "sentences" in a linguistic sense.Current languages are English, German, and Japanese.Some of the methods were originally developed in thecontext of another HPSG environment, he LKB (Copes-take, 1998).
This lends support o our claims of their in-dependence from a particular parser or grammar engine.473In order to save space and time in the run-time system, the expansion and the applicationof lexical rules is now done off-line.
In addi-tion, certain parts of the feature structure aredeleted, since they are only needed to restrictthe application of lexical rules (see also section7 for a similar approach).
For each stem, allresults are stored in compact form as one com-piled LISP file, which allows to access and loada requested entry rapidly with almost no restric-tion on the size of the lexicon.
Although loadtime is small (see figure 1), the most frequentlyused entries are cached in main memory, reduc-ing effort in the lexicon stage to a minimum.We continue to compute morphological infor-mation online, due to the significant increase ofentries (a factor of 10 to 20 for German), whichis not justifiable considering the minimal com-putation time for this operation.German English Japanese# stems 4269 3754 1875space 10.3 KB 10.8 KB 5.4 KBentries 6 2.2 2.1load time 25.8 msec 29.5 msec 7.5 msecFigure 1: Space and time requirements, pace,entries and load time values are per stem3 Improvements  in un i f i ca t ionUnification is the single most expensive oper-ation performed in the course of parsing.
Upto 90% of the CPU time expended in parsinga sentence using a large-scale unification basedgrammar can go into feature structure and typeunification.
Therefore, any improvements in theefficiency of unification would have direct conse-quences for the overall performance of the sys-tem.One key to reducing the cost of unification isto find the simplest set of operations that meetthe needs of grammar writers but still can beefficiently implemented.
The unifier which waspart of the original HPSG grammar develop-ment system mentioned in the introduction (de-scribed by (Backofen and Krieger, 1993)) pro-vided a number of advanced features, includingdistributed (or named) disjunctions (D6rre andEisele, 1990) and support for full backtracking.While these operations were sometimes useful,they also made the unifier much more complexthan was really necessary.The unification algorithm used by the cur-rent system is a modification of Tomabechi's(Tomabechi, 1991) "quasi-destructive" unifica-tion algorithm.
Tomabechi's algorithm is basedon the insight that unification often fails, andcopying should only be performed when the uni-fication is going to succeed.
This makes it par-ticularly well suited to chart-based parsing.During parsing, each edge must be built with-out modifying the edges that contribute to it.With a non-backtracking unifier, one option isto copy the daughter feature structures beforeperforming a destructive unification operation,while the other is to use a non-destructive al-gorithm that produces a copy of the result upto the point a failure occurs.
Either approachwill result in some structures being built in thecourse of an unsuccessful nification, wastingspace and reducing the overall throughput ofthe system.
Tomabechi avoids these problemsby simulating non-destructiveness without in-curring the overhead necessary to support back-tracking.
First, it performs a destructive (butreversible) check that the two structures arecompatible, and only when that succeeds doesit produce an output structure.
Thus, no out-put structures are built until it is certain thatthe unification will ultimately succeed.While an improvement over simple destruc-tive unification, Tomabechi's approach still suf-fers from what Kogure (Kogure, 1990) calls re-dundant copying.
The new feature structuresproduced in the second phase of unification in-clude copies of all the substructures of the in-put graphs, even when these structures are un-changed.
This can be avoided by reusing partsof the input structures in the output structure(Carroll and Malouf, 1999) without introducingsignificant bookkeeping overhead.To keep things as simple and efficient as pos-sible, the improved unifier also only supportsconjunctive feature structures.
While disjunc-tions can be a convenient descriptive tool forwriting grammars, they are not absolutely nec-essary.
When using a typed grammar formal-ism, most disjunctions can be easily put into thetype hierarchy.
Any disjunctions which cannotbe removed by introducing new supertypes canbe eliminated by translating the grammar into474disjunctive normal form (DNF).
Of course, theratio of the number of rules and lexical entries inthe original grammar and the DNFed grammardepends on the 'style' of the grammar writer,the particular grammatical theory used, thenumber of disjunction alternatives, and so on.However, context management for distributeddisjunctions requires enormous overhead whencompared to simple conjunctive unification, sothe benefits of using a simplified unifier out-weigh the cost of moving to DNF.
For the Ger-man and Japanese VERBMOBIL grammars, wegot 1.4-3?
more rules and lexical entries, butby moving to a sophisticated conjunctive unifierwe obtained an overall speed-up of 2-5.4 P recompi l ing  Type  Un i f i ca t ionAfter changing the unification engine, type uni-fication now became a big factor in processing:nearly 50% of the overall unification and copy-ing time was taken up by the computation ofthe greatest lower bounds (GLBs).
Althoughwe have in the past computed GLBs online effi-ciently with bit vectors, off-line computation isof course superior.The feasibility of the latter method dependson the number of types T of a grammar.
TheEnglish grammar employs 6000 types which re-sults in 36,000,000 possible GLBs.
Our exper-iments have shown, however, that only 0.5%-2% of the type unifications were successful andonly these GLBs need to be entered into theGLB table.
In our implementation, accessingan arbitrary GLB takes less than 0.002 msec,compared to 15 msec of 'expensive' bit vectorcomputation (following (A'/t-Kaci et al, 1989))which also produces a lot of memory garbage.Our method, however, does not consume anymemory and works as follows.
We first assigna unique code (an integer) to every type t E 7-.After that, the GLB of s and t is assignedthe following code (again an integer, in fact afixnum): code(s) ?
ITI + code(t).
This array-like encoding uarantees that a specific code isgiven away to a GLB at most once.
Finally, thiscode together with the GLB is stored in a hashtable.
Hence, type unification costs are mini-mized: two symbol table lookups, one addition,one multiplication, and a hash table lookup.In order to access a unique maximal lowerbound (= GLB), we must require that the typehierarchy is a lower semilattice (or boundedcomplete partial order).
This is often not thecase, but this deficiency can be overcome itherby pre-computing the missing types (an efficientimplementation of this takes approximately 25seconds for the English grammar) or by makingthe online table lookup more complex.A naive implementation f the off-line compu-tation (compute the GLBs for T ?
T) only worksfor small grammars.
Since type unification isa commutative operation (glb(s,t) = glb(t, s);s,t  E 7"), we can improve the algorithm bycomputing only glb(s,t).
A second improve-ment is due to the following fact: if the GLBof s and t is bottom, we do not have to com-pute the GLBs of the subtypes of both s andt, since they guarantee to fail.
Even with theseimprovements, the GLB computation of a spe-cific grammar took more than 50 CPU hours,due to the special 'topology' of the type hierar-chy.
However, not even the failing GLBs needto be computed (which take much of the time).When starting with the leaves of the type hi-erarchy, we can compute maximal componentsw.r.t, the supertype relation: by following thesubsumption links upwards, we obtain sets oftypes, s.t.
for a given component C, we canguarantee that glb(s,t) ~ _k, for all s,t E C.This last technique has helped us to drop theoff-line computation time to less than one CPUhour.Overall when using the off-line GLBs, we ob-tained a parsing speed-up of 1.5, compared tothe bit vector computation.
25 P recompi l ing  Ru le  F i l te rsThe aim of the methods described in this andthe next section is to avoid failing unificationsby applying cheap 'filters' (i.e., methods thatare cheaper than unification).
The first filterwe want to describe is a rule application filter.We have used this method for quite a while, andit has proven both efficient and easy to employ.Our rule application filter is a function that2An alternative approach to improving the speed oftype unification would be to implement the GLB tableas a cache, rather than pre-computing the table's con-tents exhaustively.
Whether this works well in practiceor not depends on the efficiency of the primitive glb(s, t)computation; if the latter were relatively slow then theparser itself would run slowly until the cache was suffi-ciently full that cache hits became predominant.475takes two rules and an argument position andreturns a boolean value that specifies if the sec-ond rule can be unified into the given argumentposition of the first rule.Take for example the binary filler-head rulein the HPSG grammar for German.
Sincethis grammar allows not more than one el-ement on the SLASH list, the left hand sideof the rule specifies an empty list as SLASHvalue.
In the second (head) argument of therule, SLASH has to be a list of length one.Consequently, a passive chart item whose top-most rule is a filler-head rule, and so has anempty SLASH, can not be a valid second ar-gument for another filler-head rule application.The filter function, when called with argu-ments (filler-head-rule-nr, filler-head-rule-nr, 2 )for mother ule, topmost rule of the daughterand argument position respectively, will returnfalse and no unification attempt will be made.The conjunctive grammars have between 20and 120 unary and binary rule schemata.
Sinceall rule schemata in our system bear a uniquenumber, this filter can be realized as a three di-mensional boolean array.
Thus, access costs areminimized and no additional memory is used atrun-time.
The filters for the three languages arecomputed off-line in less than one minute andrule out 50% to 60% of the failing unificationsduring parsing, saving about 45% of the parsingtime.6 Dynamic Unif ication Fi ltering( 'Qu ick  Check ' )Our second filter (which we have dubbed the'quick check') exploits the fact that unificationfails more often at certain points in featurestructures than at others.
For example, syn-tactic features uch as CAW(egory) are very fre-quent points of failure, whereas unification al-most never fails on semantic features which areused merely to accumulate pieces of the logicalform.
Since all substructures are typed, uni-fication failure is manifested by a type clashwhen attempting a type unification.
The quickcheck is invoked before each unification attemptto check the most frequent failure points, eachstored as a feature path.The technique works as follows.
First, thereis an off-line stage, in which a modified unifi-cation engine is used that does not return im-mediately after a single type unification failure,but instead records in a global data structurethe paths at which all such failures occurred.Using this modified system a set of sentences iparsed, and the n paths with the highest failurecounts are saved.
It is exactly these paths thatare used later in filtering.During parsing, when an active chart item(i.e., a rule schema or a partly instantiated ruleschema) and a passive chart item (a lexical entryor previously-built constituent) are combined,the parser has to unify the feature structure ofthe passive item into the substructure ofthe ac-tive item that corresponds to the argument tobe filled.
If either of the two structures has notbeen seen before, the parser associates with ita vector of length n containing the types at theend of the previously determined paths.
Thefirst position of the vector contains the type cor-responding to the most frequently failing path,the second position the second most frequentlyfailing path, and so on.
Otherwise, the existingvectors of types are retrieved.
Correspondingelements in the vectors are then type-unified,and full unification of the feature structures iperformed only if all the type unifications uc-ceed.Clearly, when considering the number ofpaths n used for this technique, there is a trade-off between the time savings from filtered uni-fications and the effort required to create thevectors and compare them.
The main factorsinvolved are the speed of type unification andthe percentage of unification attempts filteredout (the 'filter rate') with a given set of paths.The optimum number of paths cannot be de-termined analytically.
Our English, Germanand Japanese grammars use between 13 to 22paths for quick check filtering, the precise num-ber having been established by experimenta-tion.
The paths derived for these grammars aresomewhat surprising, and in many cases do notfit in with the intuitions of the grammar-writers.In particular, some of the paths are very long(of length ten or more).
Optimal sets of pathsfor grammars of this complexity could not beproduced manually.The technique will only be of benefit if typeunification is computationally cheap--as indeedit is in our implementation (section 4)--and ifthe filter rate is high (otherwise the extra work476performed essentially just duplicates work car-ried out later in unification).
There is also over-lap between the quick check and the rule filter(previous ection) since they are applied at thesame point in processing.
We have found that(given a reasonable number of paths) the quickcheck is the more powerful filter of the two be-cause it functions dynamically, taking into ac-count feature instantiations that occur duringthe parsing process, but that the rule filter isstill valuable if executed first since it is a single,very fast table lookup.
Applying both filters,the filter rate ranges from 95% to over 98%.Thus almost all failing unifications are avoided.Compared to the system with only rule applica-tion filtering, parse time is reduced by approxi-mately 75% 3 .7 Reducing Feature Structure Sizevia Rest r i c to rsThe 'category' information that is attached toeach chart item of the parser consists of a singlefeature structure.
Thus a rule is implementedby a feature structure where the daughters haveto be unified into predetermined substructures.Although this implementation is along the linesof HPSG, it has the drawback that the treestructure that is already present in the chartitems is duplicated in the feature structures.Since HPSG requires all relevant informa-tion to be contained in the SYNSEM feature ofthe mother structure, the unnecessary daugh-ters only increase the size of the overall featurestructure without constraining the search space.Due to the Locality Principle of HPSG (Pollardand Sag, 1987, p. 145ff), they can therefore belegally removed in fully instantiated items.
Thesituation is different for active chart items sincedaughters can affect their siblings.To be independent from a-certain grammati-cal theory or implementation, we use restrictorssimilar to (Shieber, 1985) as a flexible and easy-to-use specification to perform this deletion.
Apositive restrictor is an automaton describingthe paths in a feature structure that will re-main after restriction (the deletion operation),3There are refinements of the technique which wehave implemented and which in practice produce ad-ditional benefits; we will report these in a subsequentpaper.
Briefly, they involve an improvement to th e pathcollection method, and the storage of other informationbesides types in the vectors.whereas a negative restrictor specifies the partsto be deleted.
Both kinds of restrictors can beused in our system.In addition to the removal of the tree struc-ture, the grammar writer can specify the re-strictor further to remove features that are onlyused locally and do not play a role in furtherderivation.
It is worth noting that this methodis only correct if the specified restrictor does notremove paths that would lead to future unifica-tion failures.
The reduction in size results in aspeed-up in unification itself, but also in copy-ing and memory management.As already mentioned in section 2, there ex-ists a second restrictor to get rid of unnecessaryparts of the lexical entries after lexicon process-ing.
The speed gain using the restrictors inparsing ranges from 30% for the German sys-tem to 45% for English.8 Limiting the Number of InitialChart ItemsSince the number of lexical entries per stem hasa direct impact on the number of parsing hy-potheses (in the worst case leads to an expo-nential increase), it would be a good idea tohave a cheap mechanism at hand that helps tolimit these initial items.
The technique we haveimplemented is based on the following observa-tion: in order to contribute to a reading, certainitems (concrete lexicon entries, but also classesof entries) require the existence of other itemssuch that the non-existence of one allows a safedeletion of the other (and vice versa).
In Ger-man, for instance, prefix verbs require the rightseparable prefixes to be present in the chart, butalso a potential prefix requires its prefix verb.Note that such a technique operates in a muchlarger context (in fact, the whole chart) than alocal rule application filter or the quick-checkmethod.
The method works as follows.
In apreprocessing step, we first separate the chartitems which encode prefix verbs from thoseitems which represent separable prefixes.
Sinceboth specify the morphological form of the pre-fix, a set-exclusive-or peration yields exactlythe items which can be safely deleted from thechart.Let us give some examples to see the useful-ness of this method.
In the sentence Ich kommemo,'ge,~ (I (will) come tomorrow), komme maps477onto 97 lexical entries--remember, kommemight encode prefix verbs such as ankommen(arrive), zuriickkommen (come back), etc.
al-though here, none of the prefix verb readingsare valid, since a prefix is missing.
Using theabove method, only 8 of 97 lexical entries willremain in the chart.
The sentence Ich kommemorgen an ( I  (will) arrive tomorrow) results in8+7 entries for komme (8 entries for the comereading together with 7 entries for the arrivereading of komme) and 3 prepositional read-ings plus 1 prefix entry for an.
However in DerMann wartet an der Tiir (The man is waitingat the door), only the three prepositional read-ings for an come into play, since no prefix verbanwartet exists.
Although there are no Englishprefix verbs, the method also works for verbsrequiring certain particles, such as come, comealong, come back, come up, etc.The parsing time for the second example goesdown by a factor of 2.4; overall savings w.r.t, ourreference corpus is 17% of the parsing time (i.e.,speed-up factor of 1.2).9 Comput ing  Best  Par t ia l  Ana lysesGiven deficient, ungrammatical, or spontaneousinput, a traditional parser is not able to de-liver a useful result.
To overcome this disadvan-tage, our approach focuses on partial analyseswhich are combined in a later stage to form to-tal analyses without giving up the correctnessof the overall deep grammar.
But what can beconsidered good partial analyses?
Obviously a(sub)tree licensed by the grammar which coversa continuous part of the input (i.e., a passiveparser edge).
But not every passive edge is agood candidate since otherwise we would end upwith perhaps thousands of them.
Instead, ourapproach computes an 'optimal' connected se-quence of partial analyses which cover the wholeinput.
The idea here is to view the set of pas-sive edges as a directed graph and to computeshortest paths w.r.t, a user-defined estimationfunction.Since this graph is acyclic and topologicallysorted, we have chosen the DAG-shortest-pathalgorithm (Cormen et al, 1990) which runs inO(V + E).
We have modified this algorithmto cope with the needs we have encountered inspeech parsing: (i) one can use several start and~nd vertices (e.g., in case of n-best chains orword graphs); (ii) all best shortest paths arereturned (i.e., we obtain a shortest-path sub-graph); (iii) estimation and selection of the bestedges is done incrementally when parsing n-best chains (i.e., only new passive dges enteredinto the chart are estimated and perhaps se-lected).
This approach as one important prop-erty: even if certain parts of the input have notundergone at least one rule application, thereare still lexical edges which help to form a bestpath through the passive edges.
This meansthat we can interrupt parsing at any time, butstill obtain a useful result.Let us give an example to see how the estima-tion function on edges (-- trees) might look like(this estimation is actually used in the Germangrammar):?
n-ary tree (n > 1) with utterance status(e.g., NPs, PPs): value 1?
lexical items: value 2?
otherwise: value c~Th is  approach does not always favor pathswith longest edges as the example in figure 2shows--instead it prefers paths containing nolexical edges (where this is possible) and theremight be several such paths having the samecost.
Longest (sub)paths, however, can be ob-tained by employing an exponential estimationfunction.
Other properties, such as prosodicinformation or probabilistic scores could alsobe utilized in the estimation function.
A de-tailed description of the approach can be foundin (Kasper et al, 1999).P RSFigure 2: Computing best partial analyses.Note that the paths PR and QR are chosen,but not ST, although S is the longest edge.47810 Conc lus ions  and  Fur ther  WorkThe collection of methods described in this pa-per has enabled us to unite deep linguistic anal-ysis with speech processing.
The overall speed-up compared to the original system is about afactor of 10 up to 25.
Below we present someabsolute timings to give an impression of thecurrent systems' performance.German English Japanese# sentences 5106 1261 1917# words 7 6.7 7.2# lex.
entries 40.9 25.6 69.8# chart items 1024 234 565# results 5.8 12.4 53.6time first 1.46 s 0.24 s 0.9 stime overall 4.53 s 1.38 s 4.42 sIn the table, the last six rows are average val-ues per sentence, time first and time overallare the mean CPU times to compute the firstresult and the whole search space respectively.# lex.
entries and # chart items give an im-pression of the lexical and syntactic ambiguityof the respective grammars 4The German and Japanese corpora and halfof the English corpus consist of transliterationsof spoken dialogues used in the VEI:tBMOBILproject.
These dialogues are real world dia-logues about appointment scheduling and va-cation planning.
They contain a variety of syn-tactic as well as spontaneous speech phenom-ena.
The remaining half of the English corpusis taken from a manually constructed test suite,which may explain some of the differences inabsolute parse time.Most of the methods are corpus independent,except for the quick check filter, which requiresa training corpus, and the use of a purely con-junctive grammar, which will do worse in casesof great amounts of syntactic ambiguity becausethere is currently no ambiguity packing in theparser.
For the quick check, we have observedthat a random subset of the corpora with aboutone to two hundred sentences i enough to ob-tain a filter with nearly optimal filter rate.Although the actual efficiency gain will varyfor differently implemented grammars, we are4The computations were made using a 300MHz SUNUltrasparc 2 with Solaris 2.5.
The whole system is pro-grammed in Franz Allegro Common Lisp.certain that these techniques will lead to sub-stantial improvements in almost every unifica-tion based system.
It is, for example, quite un-likely that unification failures are equally dis-tributed over the different nodes of the gram-mar's feature structure, which is the most im-portant prerequisite for the quick check filter towork.
Avoiding disjunctions usually requires areworking of the grammar which will pay off inthe end.We have shown that the combination of al-gorithmic methods together with some disci-pline in grammar writing can lead to a practi-cal high performance analysis ystem even withlarge general grammars for different languages.There is, however, room for further improve-ments.
We intend to generalize to other casesthe technique for removing unnecessary lexicalitems.
A detailed investigation of the quick-check method and its interaction with the ruleapplication filter is planned for the near future.Since almost all failing unifications are avoidedthrough the use of filtering techniques, we willnow focus on methods to reduce the number ofchart items that do not contribute to any anal-ysis; for instance, by computing context-free orregular approximations of the HPSG grammars(e.g., (Nederhof, 1997)).AcknowledgmentsThe research described in this paper has greatlybenefited from a very fruitful collaboration withthe HPSG group of CSLI at Stanford University.This cooperation is part of the deep linguis-tic processing effort within the BMBF projectVERBMOBIL.
Special thanks are due to StefemMiiller for discussing the topic of German prefixverbs.
Thanks to Dan Flickinger who providedus with several English phenomena.
We alsowant to thank Nicolas Nicolov for reading aver-sion of this paper.
Stephan Oepen's and Mark-Jan Nederhof's fruitful comments have helpedus a lot.
Finally, we want to thank the anony-mous ACL reviewers for their comments.
Thisresearch was supported by the German FederalMinistry for Education, Science, Research andTechnology under grant no.
01 IV 701 V0, andby a UK EPSRC Advanced Fellowship to thethird author, and also is in part based uponwork supported by the National Science Foun-dation under grant number IRL9612682.479ReferencesHassan Ait-Kaci, Robert Boyer, Patrick Lin-coln, and Roger Nasr.
1989.
Efficient imple-mentation of lattice operations.
ACM Trans-actions on Programming Languages and Sys-tems, 11(1):115-146, January.Rolf Backofen and Hans-Ulrich Krieger.
1993.The TD?///D/A/'e system.
In R. Backofen, H.-U.
Krieger, S.P.
Spackman, and H. Uszkor-eit, editors, Report of the EAGLES Work-shop on Implemented Formalisms at DFKI,Saarbriicken, pages 67-74.
DFKI ResearchReport D-93-27.John Carroll and Robert Malouf.
1999.
Effi-cient graph unification for parsing feature-based grammars.
University of Sussex andStanford University.Ann Copestake.
1998.
The (new) LKB system.Ms, Stanford University,http ://~n~-csli.
stanford, edu/~aac/newdoc, pdf.Thomas H. Cormen, Charles E. Leiserson, andRonald L. Rivest.
1990.
Introduction to Al-gorithms.
MIT Press, Cambridge, MA.Jochen DSrre and Andreas Eisele.
1990.Feature logic with disjunctive unification.In Proceedings of the 13th InternationalConference on Computational Linguistics,COLING-90, pages Vol.
3, 100-105.Walter Kasper, Bernd Kiefer, Hans-UlrichKrieger, C.J.
Rupp, and Karsten L. Worm.1999.
Charting the depths of robust speechparsing.
In Proceedings of the ACL-99 The-matic Session on Robust Sentence-Level In-terpretation.Bernd Kiefer and Oliver Scherf.
1996.
Gimmemore HQ parsers.
The generic parser class ofDISCO.
Unpublished raft.
German ResearchCenter for Artificial Intelligence (DFKI),Saarbr/icken, Germany.Kiyoshi Kogure.
1990.
Strategic lazy incremen-tal copy graph unification.
In Proceedings ofthe 13th International Conference on Com-putational Linguistics (COLING '90), pages223-228, Helsinki.Hans-Ulrich Krieger and Ulrich Sch~ifer.
1994.7"DE--a type description language forconstraint-based grammars.
In Proceedingsof the 15th International Conference onComputational Linguistics, COLING-94,pages 893-899.
An enlarged version of thispaper is available as DFKI Research ReportRR-94-37.Hans-Ulrich Krieger and Ulrich Sch~ifer.
1995.Efficient parameterizable type expansion fortyped feature formalisms.
In Proceedings ofthe l~th International Joint Conference onArtificial Intelligence, IJCAI-gS, pages 1428-1434.
DFKI Research Report RR-95-18.Mark Jan Nederhof.
1997.
Regular approxima-tions of cfls: A grammatical view.
In Pro-ceedings of the 5th International Workshop onParsing Technologies, IWPT'97, pages 159-170.Carl Pollard and Ivan A.
Sag.
1987.Information-Based Syntax and Seman-tics.
Vol.
I: Fundamentals.
CSLI LectureNotes, Number 13.
Center for the Study ofLanguage and Information, Stanford.Stuart M. Shieber.
1985.
Using restrictionto extend parsing algorithms for complex-feature-based formalisms.
In Proceedings ofthe 23rd Annual Meeting of the Associa-tion for Computational Linguistics, ACL-85,pages 145-152.Hideto Tomabechi.
1991.
Quasi-destructivegraph unification.
In Proceedings of the 29thAnnual Meeting of the Association for Com-putational Linguistics, volume 29, pages 315-322.Hans Uszkoreit, Rolf Backofen, Stephan Buse-mann, Abdel Kader Diagne, Elizabeth A.Hinkelman, Walter Kasper, Bernd Kiefer,Hans-Ulrich Krieger, Klaus Netter, G/interNeumann, Stephan Oepen, and Stephen P.Spackman.
1994.
DISCO--an HPSG-basedNLP system and its application for appoint-ment scheduling.
In Proceedings of COLING-94, pages 436-440.
DFKI Research ReportRR-94-38.Wolfgang Wahlster.
1993.
VERBMOBIL--translation of face-to-face dialogs.
Re-search Report RR-93-34, German ResearchCenter for Artificial Intelligence (DFKI),Saarbr/icken, Germany.
Also in Proc.
MTSummit IV, 127-135, Kobe, Japan, July1993.480
