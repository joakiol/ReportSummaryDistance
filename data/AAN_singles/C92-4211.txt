KNOWLEDGE ACQUIS IT ION AND CHINESE PARSING BASEDON CORPUSYuan Chunfa, Huaug Chaugning and Pan ShimeiDept.
of Computer ScienceTsinghua University, Beijing, ChinaFax: 861-256-2768ABSTRACTIn Natural Language Processing (NLP), one key problem is how to design a ro-bust and effective parsing system.
In this paper, we will introduce a corpm- basedChinese parsing system.
Our efforts are coucetrated on: (1) knowledge acquisitionand representation; and (2) the parsing scheme.
The knowledge of this system is prin-cipally extracted from analyzed corpus, others are a few grammatical principles, i.e.the four axioms of the Dependency Grammar (DG).
In addition, we also propose thefifth axiom of DG to support he parsing of Chinese sentences.1.
IntroductionThe traditional approaches of natural anguage parsing are based on rewriting rules.
Weknow that when the number of rules have already increased to a certain level, the performanceof parsing will be improved little by increasing the number of rules further.
So usingcorpus-based approach, i.e.
extracting linguistic knowledge with t'me grain size from corpusdirectly to support natural language parsing is more impressive.In this paper we will introduce the work on Knowledge acquisition and Chinese parsingbased on corpus.
Our work includeds:?
Take out a total of 500 sentences from geography text book of middle school to form asmall Chinese corpus.?
Because Dependency Grammar (DG) directly describes the functional relations betweenwords, and s dependency tree has not any non-terminal nodes, DG is suitable for ourCorpus-Bused Chinese Parser (CBCP) particularly.
We marked the dependency relations ofevery sentence in our corpus manually.?
Input the analyzed corpus into the computer and form u matrix f'de for every sentence inthe corpus.?
Extract he knowledge from the matrix f'de and form a knowledge base.?
Implement the CBCP system for parsing input sentences and assigning dependency treesto them.2.
Construction of the knowledge base(I) Thl.
project is supported by National Science Fundation of China under grant No.
69073063AcrEs DE COLING-92, NArerES, 23-28 AOL-r 1992 1 3 0 0 Proc.
OF COLING-92, NANTES, AUG. 23-28, 1992At first, we marked the dependency relations of every sentences in our corpus manuaUy.An example of analyzed sentence i.s as follows :SUBJDETA CDE ATRA / ADVA~ OBJ(each) (river) (of) (middle and low reaches) (mostly) (ate) (flatlands)Most of the middle and low reaches of each river are fiatlauds.Fig 2.1Here: DETA(DETerminntive Adjunct), CDE(Complement of ~(DE) ' ) ,  ATRA(ATtRibuteAdjunct), SUBJ(SUBJeet) ,  ADVA(ADVerhial Adjunct), OBJ (OBJect ) .Then we run a program to input the dependency relations of every sentence to the comput-er and form a matrix file as bellow:M(0 1)=DETA M(1 2) :CDE M(2 3)=ATRA M(3 5)~SUBJM(4 5)=ADVA M(6 5)=ORJIn order to expound the knowledge representation, we give some definitions as below.
Ifthere are four words wl,  w2, w3 and w4 with dependency relations RI ,  R2 and R3:RI R2 R3Fig 2.2Then for the word ~w3", its d-relation Ls R2; its g-relat inn is R1; and its s-relat ion is R3.We extract he knowledge from the matrix file to form a frame as below :word-name ::= \[ < govfreq >,  < govlLst > ,  < linklLst > ,  <: patlLst > \]The slots of the frame are:governor frequency (govfreq): It indicates that wltether the given word can be a governor of asentence and how many times it has been in our corpus.governor list (govlLst): It indicates which word can be the parent node of thc given word, andwhat is the dependency relation between the word and its parent node.
In other words,what is the word's d-relat ion and how many times it has occurred in the corpus, i.e.govlist :: = \[{ < governor-name > {\[ < d-relation :>, <frcqncncy > \]} * } * \]dependency link list 0inkli~t): The d-relation and g-reintion of the given words can form apair of relations described as d-relat ion < .
.
.
.
~-relatiou.
The information on iinklist in-cludes: how many kinds of dependency links the given word have in our corpus?
And whatare they?
how many times it has occurred?
what Ls the position of the word's parent node (to the right or to the left of the word) ia a sentence?
i.e.AttirEs DE COLING-92, NAtCrEs, 23-28 AOt}I 1992 l 3 0 1 PROC.
OF COL,ING-92.
NAtCr~S, Ann.
23-28, 1992llnklist :: = \[{ < d-relat inn > {\[ < g-relat ion >,  < position >,  < frequency >\]} * } * Ipattern list (patlist): The given word and its s-relat ions constitute a pattern of the word as:(s-relation1 s--relation2 s-relation3 ...).
This pattren information describes the rationalityof the syntactic structure in a dependency tree.
The patlist knowledge xtracted from thecorpus includes: how many patterns can the word act in our corpus?
What is each pattern?how many times has it occurred?
What Ls the position (to the right or left of the word) ofthe children node in a sentence in our corpus?
i.e.patlist :: = \[{ \[pattern \[ < frequency >,  {\[ < s-relat ion >,  < positinn > \]} * \]\]} * \](notes: the content inside the "{ } * " can be repeated n times, where n > 1)3.
The parserIn our CBCP system, the knowledge base will first be searched for all the possible linklistinformation of each word pair, according to the words in the input sentence.
We use this infor-mation to construct a Specific Matrix of the Sentence (SMS).
Sccond, remove impossiblelinks in the SMS, and form a network.
Third, we search all the possible depcndcncy trees in thenetwork, using the pruning algorithm.
Finally, the solutions will be selected by evaluating thedependency trees.
The process of removing and pruning is based on the knowledge base and thefour axioms of Dependency Grammar (Robinson, J .
J .1970).
The four axioms are:I .
There is only one independent element (governor) in a sentence.\]\].
Other elements must directly depend on one certain clement in the sentence.l\[I.
There should not be any element which depends on two or more elememts.IV.
If the element A directly depends on element B, and clement C is located between Aand B in a sentence, element C must be either directly dependent on A or B or an elementwhich is between A and B in the sentence.According to our Dcpendcncy Grammar practice in Chinese, we populate the fifth axiomas follows:V .
There is no direct dependent relation between two elements which one is on the lefthand side and the other is on the right hand side of a governor.3.1 Comtruet a specifieal matrix of a sentenceSuppose there are k words in a sentence marked as S=(wl  w2 w3 ... wi... wk), CBCPsearches the linklist information of every word in the sentence.
For example, ff one link of wi isATRA<- - - -OBJ ,  and the link of wj is OB J<- - - -GOV (GOVernor)  in the knowledgebase, CBCP can construct the link between wi and wj as ATRA < - - - -OB J .
The SMS willbe constructed by searching all the links of words in the input sentence.3.2 Remove impossible governors and linksSince an input sentence may form a large number of dependency trees based on the SMS,it is necessary to remove the impossible links before connecting every node to a network.
Sup-pose in a SMS,  the word A is dependent on the word B and the link between them isACIF~ DE COLING-92, N^rzff~s, 23-28 ^ o(rr 1992 1 3 0 2 l))~oc.
OF COL1NG-92, NANTES, AUG. 23-28, 1992Ra<- -Rb .
I f  there exists a (RI R2 ...Ra...Rk) in B's patlist, the dependent relation ofRa<- -Rb  is reasonable.
Otherwise, the Ra<- -Rb  relation is impossible, and should beremovcdoThe CBCP system looks for the govfreq information of each word in an Input sentence.
I fthe govfreq of a word is greater than zero, the word can be a governor.
The rules of removingimpossible governors arc:?
I fa  word has no parent node in SMS, the word must be the governor (based on axiom ~\[).
Other words which can also act as a governor must be removed.?
If a word A has only one rink to word B with the link Ra <- -GOV,  and the word B cannot he a governor, the word A will not depend on any word in the dependency tree?
Accordingto axiom I\] this is impossible, therefore word B must he the governor.
Other words which alsocan act as a governor must be removed.?
When n word A has only one link to word B with the link Ra <- -Rb  (Rb < > GOV) ,and the d-relatinn of the word B is not Rb, the word A will not depend on any words in the de-pendency tree.
According to axiom \]\] this is impossible.
So the d-rclatinu of the word B mustnot be the governor.
Then this kind of link in which the word B is used as a governor must he~movcd.
After removing all the impossible governors and links, the SMS of the sentence inFig-2.
i  is as follows:M(0 1)~ DETA <- -CDE M(0 5) = ADVA < - -GOV M( I  2) = CDE < - -ATRAM(I  3) = ATRA <- -SUBJ '  M( I  5) = SUBJ < ~-GOV M(2 3) = ATRA <- -SUBJM(3 5) ~ SUBJ < - -GOV M(4 5) = ADVA < - -GOV M(6 5) = OBJ  < - - -GOV3?3 Search the possible integrated tree from the specific treeLet the governor be the root node, connecting nil the nodes in order.
If a node have n (n >1) parent nodes, we can sprit this node to n same nodes.
Let these n same nodes depend on the nparent nodes respectirely.
Thus Specific Tree (ST) will be constructed.
The ST of the sentencein Fig-2.1 Ls as bellow:ADVA- -~f r  w0UBJ- ~--~.4n'~j" wl ATRAGov I su.
J  y - - - -~  wlw S ~ t \ [ l ' ~ .
~  w3,~ ATRA., CDE DETAw2- - - - -~  wl-----------~ w0Fig-3.1A(zrf, s DE COLING-92, NANTEs.
23-28 ^ o(;r 1992 1 3 0 3 PROC.
of: COLING-92, NANTES, At;6.23-28, 1992I fa  node appears m times in the ST, we may say the degree of freedom of this node is 2m.
If  there is only one word, whose ~ equala to m in a ST, then m dependency trees may beconstructed.
If the degree of freedom of the word- i  equals to n ,  the degree of freedom of theword- j  equals to m then the n * m dependency trees will be constructed.
If there are manywords with ~ greater than one, the number of dependency trees being formed will be very large.Therefore, in the process of seaching an integrated ependency tree, the pruning technologymust be taken.
The pruning technology derives from axiom V.After the integrated dependency trees have been produced, we use the numericalevaluation to produce the parsing result \[1\].4.
Experimental result and future workWhen CBCP analyzed Chinese sentences in a closed corpus, it has an approximately 90%success rate (comparing with the result of manual parsing).
If each word in a sentence can befound in our corpus and the corresponding dependence r lation can also be found in our know-tcdge base, it is also feasible for CBCP to perform syntactic parsing in an open corpus.As our research is advancing, we will enlarge the scale of our corpus and make it work onopen corpus more effectively.
On the other hand, we have great interests in how to retrievemore information from different aspects.
For example, we want to acquire grammatical cate-gory information and semantic features for our system or equip complex feature set for eachword to support corpus-based as well as rule-based system.
We want to add a few rules to oursystem, in order to replace the frames of the words which frequently appear in our corpus.
Theframe of such a word is very large, but it is easy to describe its dependency relations by rules.We plan to do further esearch in this field.In addition, our work can be easily expanded to set up a Chinese Collocation Dictionary.It is very difficult to make this kind of dictionary by man power, beacuase it is impossible toseek all the possible collocations of a particular word just by thinking.
But it is easy to achievethis with corpus-based approach like our work.
The more refined analyzing of the texts in thecorpus, the more knowledge can be acquired from the corpus.References|1\] van Zuillco, Job M. (1990): ~Notes on a Probabilistic Parsing Experiment' .BSO / Language Systems, Utrecht, The Netherlands.\[2\] van Zuljlcn, Job M.(1989) : "The Application of Simulated Annealing inDependency Grammar Pars ing' .
BSO / Language Systems, Utrecht, The Netherlands.\[3\] \]~l~'-~ (1991) : ( (4 , '~t \ [ t~' -~i~SO)  , ~{~t~._~..I41 ~,~ (1987): ( ( ~ o ~ ,  ~?~R~?.ACTES DE COLING-92, NANFES.
23-28 hOt':I' 1992 1 3 0 4 PROC.
Ol: COIANG-92.
NANTES, AUG. 23-28.
1992
