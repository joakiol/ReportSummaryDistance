Minimizing Word Error Rate inTextual Summaries of Spoken LanguageKlaus Zechner and A lex WaibelLanguage Technologies InstituteCarnegie Mellon University5000 Forbes AvenuePittsburgh, PA 15213, USA(zechner ,waibel}@cs.
cmu.
eduAbstractAutomatic generation of text summaries for spokenlanguage faces the problem of containing incorrectwords and passages due to speech recognition er-rors.
This paper describes comparative experimentswhere passages with higher speech recognizer confi-dence scores are favored in the ranking process.
Re-sults show that a relative word error rate reductionof over 10% can be achieved while at the same timethe accuracy of the summary improves markedly.1 IntroductionThe amount of audio data on-line has been grow-ing rapidly in recent years, and so methods for ef-ficiently indexing and retrieving non-textual infor-mation have become increasingly important (see,e.g., the TREC-7 branch for "Spoken Document Re-trieval" (Garofolo et al, 1999)).One way of compressing audio information is theautomatic creation of textual summaries which canbe skimmed much faster and stored much more effi-ciently than the audio itself.
There has been plentyof research in the area of summarizing written lan-guage (see (Mani and Maybury, 1999) for a compre-hensive overview).
So far, however, very little atten-tion has been given to the question how to createand evaluate a summary of spoken audio based onautomatically generated transcripts from a speechrecognizer.
One fundamental problem with thosesummaries i that they contain incorrectly recog-nized words, i.e., the original text is to some extent"distorted".Several research groups have developed interac-tive "browsing" tools, where audio (and possiblyvideo) can be accessed together with various typesof textual information (transcripts, ummaries) via agraphical user interface (Waibel et al, 1998; Valenzaet al, 1999; Hirschberg et al, 1999).
With thesetools, the problem of misrecognitions is alleviatedin the sense that the user can always easily listento the audio recording corresponding to a passagein a textual summary.
In some instances, however,this approach may not be feasible or too expensiveto pursue, and a short, stand-alone textual repre-sentation of the spoken audio may be preferred oreven required.
This paper addresses in particularthis latter case and (a) explores means of makingtextual summaries less distorted (i.e., reducing theirword error rate (WElt)), and (b) assesses how theaccuracy of the summaries changes when methodsfor word error rate reduction ar e applied.
Summaryaccuracy will be a function of how much relevantinformation is present in the sun'mary.Our results from experiments on four televisionshows with multiple speakers how that it is possi-ble to reduce word error rate while at the same timealso improving the accuracy of the summary.
Fur-thermore, this paper presents a novel method forevaluation of textual summaries from spoken lan-guage data.
.
.
.
.The paper is organized as follows: In the next :section, we review related work on spoken languagesummarization.
In section 3 we describe our sum:marizer.
Next, we present and discuss our proposalfor an audio summarization evaluation metric (sec-tion 4).
In section 5 we describe the Corpus that weuse for our experiments and how i t  was annotated.Sections 6 and 7 describe xperixnents on both hu ....man and machine generated transcripts of the audiodata.
Finally, we discuss and summarize the resultsin sections 8 and 9.2 Related work ?
"(Waibel et al, 1998) report results of their sum-marization system on automatically transcribedSWITCHBOARD data (Godfrey et al, 1992), the worderror rate being about 30%.
In a question-answertest with summaries offive dialogues, ubjects couldidentify most of the key concepts using a summarysize of only five turns.
However, the results varywidely across five different dialogues tested in thisexperiment (between 20% and 90% accuracy).
(Valenza et al, 1999) went one step further andreport that they were able to reduce the word errorrate in summaries (as opposed to full texts) by usingspeech recognizer confidence scores.
They combinedinverse frequency weights with confidence scores foreach recognized word.
Using summaries composed of186one 30-gram per minute (approximately 15% lengthof the full text), the WER dropped from 25% forthe full text to 10% for these summaries.
They alsoconducted a qualitative study where human subjectswere given summaries of n-grams of different lengthand also summaries with speaker utterances as min-imal units, either giving a high weight o the inversefrequency scores or to the confidence scores.
Theutterance summaries were considered best, followedclosely by 30-gram summaries, both using high con-fidence score weights.
This suggests that not onlydoes the WER drop by extracting passages that aremore likely to be correctly recognized but also dosummaries seem to be "better" which are generatedthat way.While the results of (Valenza et al, 1999) are in-dicative for their approach, we want to investigatethe benefits of using speech recognizer confidencescores in more detail and particularly find out aboutthe trade-off between WER and summarization ac-curacy when we vary the influence of the confidencescores.
To our knowledge, this paper addresses thistrade-off for the first time in a clear, numerically de-scribable way.
To be able to obtain numerical valuesfor summary accuracy, we had our corpus annotatedfor relevance (section 5) and devised an evaluationscheme that allows the calculation of summary ac-curacy for both human and machine generated tran-scripts (section 4).3 Summar izat ion  sys temPrior to summarizing, the input text is cleaned upfor disfluencies, such as hesitations, filled pauses,and repetitions.
I In the context of multi-topicalrecordings we use for our experiments, summariesare generated for each topical segment separately.The segment boundaries were determined to be atthose places where the majority Cat least half) of thehuman annotators agreed (see section 5).
Intercoderagreement for topical boundaries i fairly good (andhigher than the agreement on relevant words or pas-sages).2To determine the content of the summaries, weuse a "maximal marginal relevance" (MMR) basedsummarizer with speaker turns as minimal units (cf.
(Carbonell and Goldstein, 1998)).The MMR formula is given in equation 1.
It gen-erates a list of turns ranked by their relevance andstates that the next turn to be put in this rankedlist will be taken from the turns which were not yetranked (tar) and has the following properties: it is(a) maximally similar to a "query" and (b) max-imally dissimilar to the turns which were already1 More details about this component and other parts of thesummarization system can be found in (Zechner and Walbei,20oo).2For details ee (Zechner, 2000).ranked (tr).
As "query" we use a frequency vectorfor all content words within a topical segment.
TheA-parameter (0.0 < A < 1.0) is used to trade off theinfluence of C a) vs. (b).Both similarity metrics (sire1, sire2) are innervector products of (_stemmed) term frequencies (seeequations 2 to 4); tft is a vector of stem frequenciesin a turn; f ,  are in-segment frequencies of a stem;f ,  rna= are maximal segment frequencies of any stemin the topical segment, sirnl can be normalized ornot.
The formulae for tfa (equation 4) are inspiredfrom Cornell's SMART system (Salton, 1971); wewill call these parameters "smax', "log", and ?
'freq",respectively.neztturn = argmax(Asima(tn,j,query)tnr,~- (1 - A) maxsim2 (tnrd, tr,t~)) (1)tr ,  ksiml : tf~tft or \[tf=tit~ (2)I / I  -Itftztft~I I I  Itfi,, = 0.5 + 0.5 /~" or 1 + logfi.,or ~,, (4)Using the MMR algorithm, we obtain a list ofranked turns for each topical segment.
We com-pute this both for human and machine generatedtranscripts of the audio files ("reference text" vs."hypothesis text") .34 Eva luat ion  metr i csThe challenge of devising a meaningful evaluationmetric for the task of audio summarization is thatit has to be applicable to both the reference (hu-man transcript) and the hypothesis transcripts (au-tomatic speech recognizer (ASR) transcripts).
Wewant to be able to assess the quality of the sum-mary with respect to the relevance markings of thehuman annotators (see section 5), as well as to re-late this "summary accuracy" to the word error ratepresent in the ASR transcripts.The approach we take is to align the words in thesummary with the words in the reference transcript(wa).
For ASR transcripts, word substitutions arealigned with their "true original" and word inser-tions are aligned with a NIL dummy.
That way,3The human reference is cons idered  to be an "optimal" or"ideal" rendering of the words which were actually said in aconversation.
Human transcription errors do occur, but aremarginal and hence ignored in the context of this paper.187we can determine for each individual word wa inthe summary (a) whether it occurs in a "relevantphrase" and (b) whether it is correctly recognizedor a recognition error (for ASR transcripts).We define word error rate as WER = (S +I + D) / (S  + I + C) (I=insertion, D=deletion,S=substitution, C=correct).Each word's relevance score r is the average num-ber it occurs in the human annotators' relevantphrases (0.0 < r <_.
1.0).
Relevance scores for in-sertions and substitutions are always 0.0.We choose to define the summary accuracy sa("relevance") as the sum of relevance scores of alln aligned words ~--~?
r~, divided by the maximumachievable relevance score with the same number ofn words somewhere in the text (i.e., 0.0 < sa <_ 1.0).Word deletions obviously do not show up in the sum-mary, but are accounted for, as well, to make theWER computation sound.To better illustrate how these metrics work, wedemonstrate them on a simplified example of onlytwo speaker turns (Figure 1).
The first line repre-sents the relevance score r for each word (the numberthis word was within a "relevant phrase" divided bythe number of annotators for that text).
In turn 1,"this is to illustrate" was only marked relevant bytwo annotators, whereas "the idea" by 3 out of 4annotators.
The second line provides the referencetranscript, the third line the ASB.
transcript.
Line4 gives the type of word error, and line 5 the con-fidence score of the speech recognizer (between 0.0and 1.0, 1.0 meaning maximal confidence).Now let us assume that turn 2 shows up in thesummary.
The scores are computed as follows:?
When summarizing the reference: Here, theword error rate is trivially 0.0; the summaryaccuracy sa is the sum of all relevance scores(-6.0) divided by the maximal achievable scorewith the same number of words (n = 7).
l"hrn 2has 6 words which were marked relevant by allcoders (r -- 1.0), turn l 's highest score is r =0.75.
Therefore: sa2 = 6.0/(6.0 + 0.75) = 0.89.This is higher than the summary accuracy forturn 1: sal = 3.5/6.0 = 0.58(n = 6).?
When summarizing the ASR transcript ("hy-pothesis"): Selecting turn 2 will give sa2 =0.02.25 = 0.0 (n = 5) .
For turn 1, sal =2.25/(0.75 + 0.5 + 0.5 + 0.5 + 0.0 + 0.0) = 1.0(n = 6; the sum in the denominator can only userelevance scores based on the aligned words wawhich were correctly recognized, therefore the1.0-scores in turn 2 cannot be used).
Turn 2 hasWER=6\ [5=l .2 ,  turn 1 has WER=3/6=0.5 .Obviously, when summarizing the ASB.
output, wewould rather have turn 1 showing up in the summarythan turn 2, because turn 2 is completely off fromthe truth and turn 1 only partially.
The fact thatturn 2 was considered to be more relevant by humancoders cannot, in our opinion, be used to favor itsinclusion in the summary.
An exception would bea situation where the user has immediate access tothe audio as well and is able to listen to selectedpassages from the summary (see section 1).
In ourcase, where we focus on text-only summaries to beused stand-alone, we have to minimize their worderror rate.Given that, turn 1 has to be favored over turn 2,both because of its lower WEB, and because of itshigher accuracy with respect o the relevance anno-tations.In order to increase the likelihood that turnswith lower WEB, are selected over turns with higherWEB., we make use of the speech recognizer's con-fidence scores which are attached to every word hy-pothesis and can be viewed as probabilities: they arein \[0.0,1.0\], high values reflecting a high confidencein the correctness of the respective word.
4 Follow-ing (Valenza et al, 1999) we conjecture that we canuse these confidence scores to increase the probabil-ity of passages with lower WEB, to show up in thesummary.
To test how far this assumption is justi-fied, we correlated the WEB.
with various metrics ofconfidence scores: (i) sum of scores, (ii) average ofscores, (iii) number of scores above a threshold, (iv)the latter normalized by the number of all scores,and (v) the geometric mean of scores.
Table 1 showsthe correlation coefficients (Pearson r) for the fourASK transcripts we used in our experiments ( ee sec-tion 5).
To prevent he influence of large differencesin turn length, those computations were done forsubsequent "buckets" of 50 words each.Since in most cases we achieve the highest corre-lation coefficient (absolute value) for method (iv =avgth) (average number of words whose confidencescore is greater than a threshold of 0.95), we applythis metric to the computation ofturn-query similar-ities (sire1 in equation 1).
We use the two followingformulae to adjust the similarity,scores.
(We shallcall these adjustments MULT and EXP in the follow-ins.
)\[mutt\] sirn~ = Siml (1 + aavgth) (5)\[ezp\] sim'l' = s imlavgth ~ (6)For both equations it holds that if a = 0.0, thescores don't change, whereas if c~ > 0.0, we en-hance the weights of turns with many high confi-dence scores ("boosting") and hence increase theirlikelihood of showing up earlier in the summary.
5Even though our evaluation method looks like itwould "guarantee" an increase in summary accu-4The speech recognizer computes  these scores based on theacoustic stability of words during lattice rescoring.5For EXP, we define 0 ?
---- O.188TURNre1:REF:HYP:e r r :con:TURNre l  :REF:HYP:err:con:1:0.5 0.5 0.5 0.5 0.75 0.75 ***this is to illustrate the idea ***this is to  ILLUMINATE *** ideaC C C S D C I1 1 1 0 .9  - 0 .8  0 .82:0 1 1 1 1 1 1and here we have very  re levant  in fo rmat ionand HE ** BEHAVES **** IRREVERENT FORMATIONC S D S D S S0.8  0 .7  - 0 .8  - 0 .8  0 .9Figure 1: Simplified example of two turns (for score computation)BACK(i} sum -0.43(ii) average -0.53Off) scores > 0.95 -0.55( iv) normal i zed  (ii i) -0.58(v) geometric mean -0.5319CENT BUCHANAN-0.51 -0.12-0.52 -0.43-0.48 -0.35-0.48 -0.48-0.53 -0.42GRAY-0.03-0.42-0.25-0.44-0.38Table 1: Pearson r correlation between WER and confidence scoresracy when the word error rate is reduced, this isnot necessarily the case.
For example, it could turnout that while we can reduce WER by "boosting"passages with higher confidence scores, those pas-sages might have (much) fewer words marked rele-vant than those being present in the summary with- ~out boosting.
This way, it would be conceivable tocreate low word error summaries that contain alsovery few relevant pieces of information.
However, aswe will see later, WER reduction goes hand in handwith an increase of summary accuracy.5 Data  character i s t i cs  andannotat ionTable 2 describes the main features of the corpus weused for our experiments: we selected four audio ex-cerpts from four television shows, together with hu-man generated textual transcripts.
All these showsare conversations between multiple speakers.
Theaudio was sampled at 16kHz and then also automat-ically transcribed using a gender independent, vo-cal tract length normalized, large vocabulary speechrecognizer which was trained on about 80 hours ofBroadcast News data (Yu et al, 1999).
The averageword error rates for our 4 recordings ranged from25% to 50%.The reference transcripts of the four recordingswere given to six human annotators who had to seg-ment them into topically coherent regions and to de-cide on the "most relevant phrases" to be includedin a summary for each topical region.
Those phrasesusually do not coincide exactly with speaker turnsand the annotators were encouraged to mark sec-tions of text freely such that they would form mean-ingful, concise, and informative phrases.
Three an- ?notators could listen to the audio while annotat-ing the corpus, the other three only had the hu-man generated transcripts available.
2 of the 6 an-notators only finished the NewsHour data, so wehave the opinion of 4 annotators for the recordingsBUCHANAN and GRAY and of 6 annotators for BACKand 19CENT.6 Exper iments  on  human generatedt ranscr ip tsWe created summaries of the reference transcriptsusing different parameters for the MMR computa-tion: For tf  we used "freq", "log", and "smax"; fur-ther, we did or did not normalize these weights; fi-nally, we varied the MMR-A from 0.85 to 1.0.
Sum-marization accuracy was determined at 5%, 10%,15%, 20%, and 25% of the text length of each sum-marized topical segment and then averaged over allsample points in all segments.
Since these wereword-based lengths, words were added incrementallyto the summary in the order of the turns ranked viaMMR; turns were cut off when the length limit wasreached.
As explained in the example in section 4,the accuracy score is defined as the fraction of thesum of all individual word relevance scores (as de-189TV shownumber of speakersspeaker turnswords in transcriptlength in minutestopical segmentsword error rate (in %)BACK 19CENT BUCHANAN GR.AYNewsHour52412168.6425.6NewsHour22712818.6432.6Crossfire469325217.3432.5Table 2: Characteristics of the corpusCrossfire57O220511.9349.8i 119CE T I BUCH*NAN I O A* I av0*a'?
I 0,0.533 0.596 0.513 0.443 0.522Table 3: Reference summarization accuracy of MMR o~summariestermined by human annotators) over the maximumpossible score given the current number of words inthe summary.Table 3 shows the summary accuracy results forthe best parameter setting (if=log, no normaliza-tion) ~.7 Exper iments  on automat ica l lygenerated  t ranscr ip tsUsing the same summarizer as before, we now cre-ated summaries from ASR transcripts.
Addition-ally to the summary accuracy, we evaluate now alsothe WER for each evaluation point.
Again, we rana series of experiments for different parameters ofthe MMR formula (if=log, smax, freq; with/withoutnormalization).
As before, we achieved the best re-sults for non normalized scores and tf=log.
We var-ied a from 0.0 to 10.0 to see how much of an effect wewould get from the "boosting" of turns with manyhigh confidence scores (see equations 5 and 6).The ExP formula yielded better esults than MULl?
(Table 4), the optimum for ExP was reached for= 3.0 with a WER of 26.6%, an absolute improve-ment of over 8% over the average of WER=35.1%for the complete ASR transcripts (non-summarized).The summarization accuracy peaks at 0.47, a 9%absolute improvement over the a = 0.0-baseline andonly about 5% absolute lower than for reference sum-maries (Table 4 and Figure 2).When we compare the baseline of ~ = 0.0 (i.e., no"boosting" of high confidence turns) to the best re-sult (a = 3.0), we see that the WER drops markedlyby about 12% relative from 30.1 to 26.6%.
At thesame time, the summarization accuracy increases byabout 18% relative form 0.401 to 0.472.?
I f  we use non-normal ized scores, the value of the MMR-Xdoes not  have any measurab le  effect; we assigned it to be 0.95for all subsequent  experiments.0.$0.25Summary accuracy vL Word mror rateFgXP Ioenula' ' i 0"2.35 0.4 0.45 0 5 0.65=unvnan/accuracyFigure 2: Summary accuracy vs. word error rateswith ~.xP boosting (0 < a < 7)Results for the MULT formula confirm this trend,but it is considerably weaker: approximately 6%WER reduction and 14% accuracy improvement forc~ = 10.0 over the c~ = 0.0 baseline.An appendix (section 11) provides an example ofactual summaries generated by our system for thefirst topical segment of the BACK conversation.
Itillustrates how WER reduction and summary ac-curacy improvement can be achieved by using ourconfidence boosting method.8 D iscuss ionThe most significant result of our experiments i ,in our opinion, the fact that the trade-off betweenword and summary accuracy indeed leads to an op-timal parameter setting for the creation of textualsummaries for spoken language (Figure 2).
Usinga formula which emphasizes turns containing manyhigh confidence scores leads to an average WER re-duction of over 10% and to an average improvementin summary accuracy of over 15%, compared to thebaseline of a standard MMR-based summary.Comparing our results to those reported in(Valenza et al, 1999), we find that their relative190a = 0~0P.xP (o = 3.0)MOLT (0 = 10.0)BACK 19CENTacc I WER acc I WER0.411 26 .2  0.501 26.70.648 18.8 0.501 26.70.575 21.5 0.501 26.7BUCHANAN G RAY averageacc WER acc WER ace \] WER0.412 30 .6  0.280 36.9 0.401 30.10.444 26.9 0.296 34.0 0.472 26.60.429 29.6 0.317 35.7 0.456 28.3Table 4: Effect of a on summary accuracy vs. WER (in %) transcripts with ExP and MULl" boosting methodsI IBm?K\[ 19C NT BUOHA ANIO AYIavgth -0.79 -0.11 -0.43 -0.03Table 5: Correlation between WER and confidencescores on a turn basisWER reduction for summaries over full texts wasconsiderably arger than ours (60% vs. 24%).
Weconjecture that reasons for this may be due to thedifferent nature and quality of the confidence scores,and (not unrelated), to the different absolute WERof the two corpora (25% vs. 35%): in transcriptswith higher WER, the confidence scores are usuallyless reliable (eft Table 1).Looking at the four audio recordings individually,we see that the improvements vary strongly acrossdifferent recordings.
We conjecture that one reasonfor this fact may be due to the high variation inthe correlation between WER and confidence scoreson a turn basis (Table 5).
This would explain why,e.g., BACK'S improvements are much stronger thanthose of the BUCHANAN recording or why there areno improvements for the 19CENT recording.
How-ever, GRAY does improve despite its very low abso-lute correlation.9 SummaryIn this paper, we presented experiments on sum-maries of both human and machine generated tran-scripts from four recordings of spoken language.
Weexplored the trade-off of word accuracy vs. summaryaccuracy (relevance) using speech recognizer confi-dence scores to rank passages with lower word errorrate higher in the summarization process.Results comparing our approach to a simple MMRranking show that while the WER can be reduced byover 10%, summarization accuracy improves by over15% as measured against ranscripts with relevanceannotations.10 AcknowledgementsWe thank the six human annotators for their tediouswork of annotating the corpus with topical segmentboundaries and relevance information.
We also wantto thank Alon Lavie and the three anonymous re-viewers for useful feedback and comments on earlierdrafts of this paper.This work was funded in part by ATR - Inter-preting Telecommunications Re earch Laboratoriesof Japan, and the US Department ofDefense.Re ferencesJaime Carbonell and Jade Goldstein.
1998.
The useof MMR, diversity-based reranking for reorderingdocuments and producing summaries.
In Proceed-ings of the ~Ist A CM-SIGIR International Con-ference on Research and Development in Informa-tion Retrieval, Melbourne, Australia.John S. Garofolo, Ellen M. Voorhees, Cedric G. P.Auzanue, and Vincent M. Stanford.
1999.
Spokendocument retrieval: 1998 evaluation and investi-gation of new metrics.
In Proceedings of the ESCAworkshop: Accessing information in spoken audio,pages 1-7.
Cambridge, OK, April.J.
J. Godfrey, E. C. Holliman, and J. McDaniel.1992.
SWITCHBOARD: telephone speech corpusfor research and development.
In Proceedings ofthe ICASSP-9~, volume 1, pages 517-520.Julia Hirschberg, Steve Whittaker, Don Hindle, Fer-nando Pereira, and Amit Singhal.
1999.
Findinginformation i  audio: A new paradigm for audiobrowsing/retrieval.
In Proceedings of the ESCAworkshop: Accessing information in spoken audio,pages 117-122.
Cambridge, OK, April.Inderjeet Mani and Mark T. Maybury, editors.
1999.Advances in automatic text summarization.
MITPress, Cambridge, MA.Gerard Salton, editor.
1971.
The SMART RetrievalSystem -- Experiments in Automatic Text Pro-cessing.
Prentice Hall, Englewood Cliffs, New Jer-sey.Robin Valenza, Tony Robinson, Marianne Hickey,and Roger Tucker.
1999.
Summarisation f spo-ken audio through information extraction.
In Pro-ceedings of the ESCA workshop: Accessing in-formation in spoken audio, pages 111-116.
Cam-bridge, OK, April.Alex Waibel, Michael Bett, and Michael Finke.1998.
Meeting browser: Tracking and summa-rizing meetings.
In Proceedings of the DARPABroadcast News Workshop.Hua Yu, Michael Finke, and Alex Waibel.
1999.Progress in automatic meeting transcription.In Proceedings of EUROSPEECH-99, Budapest,Hungary, September.191a relative sa WEB.
in % turns in summary0.0 0.428 29.2 2, 1 \[beginning\]3.0 0.885 11.8 1, 5\[beginning\]Table 6: Relative summary accuracy, WER, and se-lected turns by the summarizer for (a) no boostingand (b) P.XP boosting.higher WER scores, case (b) (0 = 3.0) successfullyranks turn 1 first due to its higher confidence scoresand hence both summary accuracy and WElt scoresimprove.turn avg.
relevance score1 0.6632 0.3693 0.1494 0.2125 0.274WERin % avgth.9.5 0.8427.5 0.4026.9 0.3911.1 0.0827.7 0.17Table 7: Average relevance scores, WER, and confi-dence values for the five turns of BACK'S first topicalsegment.Klaus Zechner and Alex Waibel.
2000.
Dia-summ: Flexible summarization of spontaneousdialogues in unrestricted domains.
Available fromhttp://www.cs.cmu.edu/?zechner/publications.html.Klaus Zechner.
2000.
A word-based annota-tion and evaluation scheme for summariza-tion of spontaneous speech.
Available fromhttp://www .cs.cmu.edu/~zechner/publications.html.11 Append ix :  Example  summar iesThis appendix provides summaries for the first topi-cal segment of the BACK conversation.
The contentsof this conversation revolves around former Illinoiscongressman Dan Rostenkowski who had been re-leased from prison and was ready to re-enter publiclife.Figure 3 shows the human transcript of this seg-ment which is about two minutes long and con-sists of 5 speaker turns.
Figure 4 contrasts themachine generated summaries for this segment (a)without confidence boosting (a -- 0.0) and (b) usingthe optimal confidence boosting (c~ = 3.0, methodExP).
Insertions and substitutions are capitalizedand marked with I- or S- prefixes.
Table 6 comparesthe relative summary accuracies ( a) and word errorrates (WER in %) for these two summaries (aver-age over the 5 sample points from 5% to 25% sum-mary length).
Additionally, the turns that show upin the summaries are listed in their ranking order.Table 7 provides the average relevance scores, worderror rates, and confidence scores ("avgth") for eachturn of this topical segment.We observe that the most relevant urn is turn1 which has, incidentally, also the lowest WER.Whereas in case (a) (o = 0.0), turn 2 is rankedfirst and therefore dominates the lower relevance and1921 e l i zabeth :  i t  has been e ight  months s ince  dan roetenkowsk i  ua lked  out  o f  a u i scons in  federa lp r i son  n ix  months s ince  he le f t  a ha l fuay  house in  ch icago  the  fo rmer  chai rman of  the  house ways and meanscommittee i s  ready  to  s tep  back in to  the  pub l i c  eye2 e l i zabeth :  the  recept ion  was-warm the  banquet  ha l l  packed w i th  the  c i ty ' s  movers and shakersthe  th i r ty  f i ve  do l la rs  a p la te  inv i ta t ion  re fer red  to  ros tenkowsk i  an mr. chai rman ros tenkowsk imade no re ference  to h i s  conv ic t ion  fo r  misus ing  federa l  funds  on ly  a br ie f  re fe rence  to  h i s  f i f teenmonths of  p r i son  t ime3 dan:  i g raduated  from oxford  and i rea l ly  had a rhodes scho larsh ip  the  past  th ree  years  have been aconstant ly  cha l leng ing  t ime fo r  me change never  comes eas i ly  and g iven  the  c i rcumstanceso f  my s i tuat ion  that  has par t i cu le~r ly  t rue  fo r  me at  t imes  th ings  have been dosnr ight  b leak  and iuou ldn ' t  want to  wish my exper ience  on my uors t  enemy but  there  were some s i l ver  l in ings  i ' ve  had anoppor tun i ty  to  read and re f lec t  in  a bay that  uasn ' t  poss ib le  when i wasin  constant  moment in  theseremarks  today  i 'd  l i ke  to  share  some of  my conc lus ions4 e l i zabeth :  the  conc lus ions  d id 'not  due l l  on the  demise of  dan ros tenkuwsk i ' s  career  butthe  demise o f  par ty  po l i t i cs5 dan:  those  who say that  the  pres ident ' s  po l i t i ca l  poser  has been ueakened by scanda l  have t ru ly  shor tmemories the  sad fac t  i s  that  p res ident  c l in ton  has never  had a democrat i c  base in  congress  a groupof  peop le  uhom one cou ld  suppor t  the  wh i te  house on any g iven  i ssue  are  not  thereFigure 3: Human transcript of first topical segment (BACK)1 e l i zabeth :  hen been e ight  months s ince  dan ros tenkoesk i  ba lked  out  o fu i scons in  federa l  p r i son  I -~YBE2 e l i zabeth :  wan S-ALARMED the  banquet  ha l l  packed s i th  the  c i ty ' sS-CDM~O~S S-IH S-CHAHBERSS-UHICH th i r ty  f i ve  S-DOLLAR a p la teS - IN ITAT IO| re fer red  to  routenkoeek i  an S-MR. chai rman I-LET 1-HE I-ASKros tenkousk imade no re ference  to  h i s  conv ic t ion  fo r  I -HIS S-USIHG federa lfunds  on ly  a br ie f  re fe rence  to  S- IS  f i f teen  months o f  p r i son  t ime1 e l i zabeth :  has been e ight  months s ince  dan routenkoesk iea lked  out  o f  s i scons in  federa l  p r i son  I-SAYBE s ix  months s ince  he le f tS-THE ha l fuay  house in  ch icago  the  fo rmer  chairman of  the  house says  andmeans committee ready  to  s tep  back in to  the  pub l i c  eye5 dan:  S-ALSO say  that  the  pres ident ' s  po l i t i ca lpower  has been eeakened byscanda l  S-RIGHT S-ESPECIALLY shor t  S-MEMORY S-THAT S-DISSATISFACTI0| thatp res ident  c l in ton  has neverFigure 4: Machine generated summaries for (a) ~ = 0.0 and (b) a = 3.0 (25% of text length)193
