The SAU Report for the 1st CIPS-SIGHAN-ParsEval-2010Qiaoli Zhou WenjingLangYingyingWangYan Wang Dongfeng CaiKnowledge Engineering Research Center,Shenyang AerospaceUniversity,Shenyang,ChinaQiaoli_z@yahoo.com.cnAbstractThis paper presents our work forparticipation in the 2010 CIPS-SIGHANevaluation on two tasks which are EventDescription Sub-sentence (EDSs)Analysis and Complete Sentence (CS)Parsing in Chinese Parsing.
The paperdescribes the implementation of oursystem as well as the results we haveachieved and the analysis.1 IntroductionThe paper describes the parsing system of SAUin 1st CLPS-SIGHAN evaluation task 2.
Weparticipate in two tasks - EDS Analysis and CSParsing.
The testing set only providessegmentation results, therefore, we divide oursystem into the following subsystems: (1) Part-of-Speech (POS) tagging system, we mainlymake use of Conditional Random Fields (CRFs)model for POS tagging; (2) parsing system, thepaper adopts divide-and-conquer strategy toparsing, which uses CCRFs model for parsingand adopts searching algorithm to build trees indecoding; (3) head recognition system, whichalso makes use of CCRFs model.The rest of the paper is organized as follows:Section 2 describes the POS tagging system;Section 3 describes the structure of our parsingsystem; Section 4 describes head recognitionsystem in parsing tree; Section 5 presents theresults of our system and the analysis; Section 6concludes the paper.2 Part-of-Speech TaggingWe use CRFs model and post-processingmethod for POS tagging.
In the first step, we tagPOS based on CRFs.
The second step is thepost-processing after tagging, which iscorrecting by using dictionary drawn fromtraining set.
The system architecture of POStagging is shown in Figure 1.2.1 FeaturesFeature selection significantly influences theperformance of CRFs.
We use the followingfeatures in our system.Atom Templateword(-2) , word(-1) , word(0) , word(1) , word(2)prefix( word (0) ) ,suffix( word(0) )includeDot1(word ( 0 ))includeDot2(word ( 0 ))Complex Templateword(-1)& word(0) ?
word(0)& word(1)word(0)& prefix( word (0) )word(0)& suffix( word(0) )word(0)& includeDot1(word ( 0 ))word(0)& includeDot2(word ( 0 ))Table 1: Feature templates used in POS tagger.word(i) represents the ith word, prefix( word (i) )represents the first character of the ith word,suffix( word (i) ) represents the last character ofthe ith word, ncludeDot1(word ( i)) representsthe ith word containing ??
?
or not, andincludeDot2(word ( i)) represnts the ith wordcontaining ?.?
or not.2.2 Post-processingThe post-processing module adopts thefollowing processing by analyzing the errorsfrom tagging result based on CRFs.
We firstlyneed to build two dictionaries which are singleclass word dictionary and ambiguity worddictionary before the post-processing.
Thesingle class word dictionary and ambiguityword dictionary are built by drawing fromtraining set.The single class word is the word havingsingle POS in training set, and the ambiguityword is the word having multi POS in trainingset.
Besides, we build rules for words withdistinctive features aiming at correcting errors,such as ??
?, numbers and English characters,etc.Figure 2 shows the post-processing step afterPOS tagging by CRFs model.
As shown inFigure 2, we respectively post-process singleclass words and ambiguity words according toCRF score.
(1) Single class word processing moduleThe post-processing of single class wordsconsults the single class word dictionary andCRFs score.
When the score from CRFs ishigher than 0.9, we take the POS from CRFs asthe final POS; otherwise, POS of the word iscorrected by the POS in the single class worddictionary.231NCRF Primary resultWord class?Ambiguity wordSingle class wordUnknown wordEndRule baseFigure2: Post-processing architecture after CRF labelingSingle class wordprocessing moduleAmbiguity wordprocessing moduleUnknown wordprocessing moduleTrainingcorpusFeatures selectionParameter estimationCRF modelTestingcorpusPOS tagger basedon CRFPrimary reco-gnition resultPost-processingPOSResultFigure 1: System architecture of POS tagging(2) Ambiguity word processing moduleThe post-processing of ambiguity wordsconsults the ambiguity word dictionary andCRFs score.
When the POS from CRFs belongsto the POS of the word in the ambiguity worddictionary, we take the POS from CRFs as thefinal POS; otherwise, we examine the score ofCRF, if the score is less than 0.4, the final POSof the word is the POS who has the highestscore (has highest frequency), or else takingPOS from CRF as the final POS.
(3) Unknown word processing moduleThe unknown words are the words not intraining set.
By analyzing the examples, we findthat there are great deals of person names,location names, organization names andnumbers, etc.
And the words havecharacteristics when building word, therefore,we set up rules for processing.2.3 Experiment resultsTable 2 shows the comparative experimentalresults of POS tagging using two methods.Table 2: Comparative POS tagging results3 Parsing systemThe paper uses divide-and-conquer strategy(Shiuan 1996 et al, Braun 2000 et al, Lyon1997 et al)for parsing.
Firstly, we recognizeMNP for an input sentence, which divide thesentence into two kinds of parts.
One kind isMNPs, and the other one is frame which is anew sentence generating by replacing MNPusing its head word.
Secondly, we use parsingapproach based on chunking (Abney, 1991, ErikTjong and Kim Sang, 2001) and a searchingalgorithm in decoding.
Thirdly, we combine theparsing trees of MNPs and frame, which obtainsthe full parsing tree of the original sentence.Figure 3 shows the architecture of paringsystem.3.1 MNP recognitionMaximal Noun Phrase (MNP) is the nounphrase which is not contained by any other nounphrases.
We use Berkeley parser (2009 1.0) forMNP recognition.
We first use Berkeley parserto parse sentences after POS tagging, and thenwe tag MNPs from the parsing results.
As thefollowing example:Berkeley parser result: dj[ ?
?/nS vp[ ?
?/vvp[ ?
?/v np[ pp[ ?/p np[ ?
?/nS ?
?/n ] ] ?/uJDE ?
?/n ] ] ] ]MNP recognition result: ?
?/nS ?
?/v ?
?/v np[ ?/p ?
?/nS ?
?/n ?/uJDE ?
?/n ]The results of MNP recognition EDSsanalysis and CS parsing are as table3:P R FEDSs 85.3202% 85.998% 85.6578%CS 77.7102% 79.2782% 78.4864%Table 3: Results of MNP recognition3.2 Head recognition of MNP andgeneration of frameIn this paper, the new sentence in which MNPsare replaced by their head word is defined as thesentence?s frame.
The head of MNPs isidentified after MNP recognition and then theyare used to replace the original MNP, andfinally the sentence?s frame is formed.
We usethe rules to recognize the head of MNP.
Usually,the last word of MNP is the head of the phrase,which can represent the MNP in function.
Forexample: ?
[?/r ?
?/n] ?
?/ad ?
?/v ??/v[??
/v ??
/v ?
/u ??
/n]?
?
In thissentence?
?
/r??
/n?
and ?
?
?/v ?
?/v?/u ??/n?
are MNPs.
If we omit themodifier in MNP, for example ?[?
?/n] ?
?/ad ?
?/v ?
?/v [??/n]?
?, the meaning ofthe sentence will not be changed.
Because thehead can represent the syntax function of MNP,we can use the head for parsing, which canavoid the effect of the modifier of MNP onparsing and reduce the complexity of parsing.Method EDSs precisionCSprecisionCRF 92.83% 89.42%CRF +post-processing 93.96% 91.05%However, the components of MNP arecomplicated, not all of the last word of MNPcan be the head of MNP.
The paper shows thatif MNP has parentheses, we can use the lastword before parentheses as the head.
When thelast word of MNP is ??
?, we use the second lastword as the head.3.3 Chunking with CRFsThe accuracy of chunk parsing is highlydependent on the accuracy of each level ofchunking.
This section describes our approachto the chunking task.
A common approach tothe chunking problem is to convert the probleminto a sequence tagging task by using the?BIEO?
(B for beginning, I for inside, E forending, and O for outside) representation.This representation enables us to use thelinear chain CRF model to perform chunking,since the task is simply assigning appropriatelabels to sequence.3.3.1 FeaturesTable 4 shows feature templates used in thewhole levels of chunking.
In the whole levels ofchunking, we can use a rich set of featuresbecause the chunker has access to theinformation about the partial trees that havebeen already created (Yoshimasa et al, 2009).
Ituses the words and POS tags around the edgesof the covered by the current non-terminalsymbol.Table 4: Feature templates used in parsing system.W represents a word, P represents the part-of-speechof the word, C represents the sum of the chunkcontaining the word, F represents the first word ofthe chunk containing the word, L represents the lastword of the chunk containing the word, S representsthat the word is a non-terminal symbol or not.
Wj isthe current word; Wj-1 is the word preceding Wj, Wj+1is the word following Wj.3.4 Searching for the Best ParseThe probability for an entire parsing tree iscomputed as the product of the probabilitiesoutput by the individual CRF chunkers:0(y / )hi iiscore p x==?We use a searching algorithm to find the highestprobability derivation.
CRF can score eachchunker result by A* search algorithm,therefore, we use the score as the probability ofeach chunker.
We do not give pseudo code, butthe basic idea is as figure 4.1: inti parser(sent)2: Parse(sent, 1, 0)3:4: function Parse(sent, m, n)5:  if sent is chunked as a complete sentence6:     return m7:  H = Chunking(sent, m/n)8:   for h?H do9:    r = m * h.probability10:     if r?n then11:        sent2 = Update(sent, h)12:        s = Parse(sent2, r, n)13:        if s?n then n = s14:    return n15: function Chunking(sent, t)16: perform chunking with a CRF chunker andreturn a set of chunking hypotheses whose17: probabilities are greater than t.18: function Update(sent, h)19:  update sequence sent according to chunkinghypothesis h and return the updated sequence.Figure 4: Searching algorithm for the best parseIt is straightforward to introduce beam searchin this search algorithm?we simply limit thenumber of hypotheses generated by the CRFchunker.
We examine how the width of thebeam affects the parsing performance in theWord Unigrams W-2 , W-1, W0, W1, W2,Word Bigrams W-2W-1, W-1W0, W0W1,W1W2, W0W-2, W0W2,Word Trigrams W0W-1W-2, W0W1W2POS Unigrams P-3, P-2 , P-1 , P0 , P1, P2, P3,POS Bigrams P-3P-2, P-2P-1, P-1P0, P0P1,P1P2, P2P3, P0P-2, P0P2,POS Trigrams P-3P-2P-1, P-2P-1P0, P-1P0P1,P0P1P2, P1P2P3Word & POS W0P0, W0P-1, W0P1,Word & WordCount W0C0Word & FirstWord W0F0 , W-1F0Word & LastWord W0L0, W1L0Word & Symbol W0S0Chunk ModelframeMNPssentenceMNP Recognition parsing treeSearchCRF ChunkerFigure3: Parsing system architectureexperiments.
We experiment beam width andwe adopt the beam width of 4 at last.3.5 Head FindingHead finding is a post process after parsing inour system.
The paper uses method combiningstatistics and rules to find head.
The selectedstatistical method is CRF model.
The first stepis to train a CRF classifier to classify eachcontext-free production into several categories.Then a rule-based method is used to postprocess the identification results and gets thefinal recognition results.
The rule-based post-processing module mainly uses rule base andcase base to carry out post-processing.3.6 Head finding based on CRFsThe head finding procedure proceeds in thebottom-up fashion, so that the head words ofproductions in lower layers could be used asfeatures for the productions of higher layers(Xiao chen et al 2009).Atom template DefinitionCurPhraseTag The label of the current wordLCh_Word The left most childRCh_Word The right most childLCh_Pos The POS of the left most childMCh_Pos The POS of the middle childRCh_Pos The POS of the right most childNumCh The number of childrenCurPhraseTag 1 ?
The labels of the former phrase and the latterTable 5: Atom templates for Head findingTable 6: Complex templates for Head findingThe atom templates are not sufficient forlabeling context; therefore, we use somecomplex templates by combining the upperatom templates for more effectively describingcontext.
When the feature function is fixed, theatom templates in complex templates areinstantiated, which will generate features.The final feature templates are composed ofthe atom templates and the complex templates.The feature templates of the head recognition inphrases contain 24 types.3.7 Head Finding based on rulesThrough the analysis of error examples, wefound that some CRFs recognition results areclearly inconsistent with the actual situation; wecan use rules to correct these errors, thusforming a rule base.
Example-base is a chunk-based library built through analysis andprocessing on the training corpus.
TheExample-base is composed of all the bottomchunk and high-level chunk in training corpus.High-level phrases are the bottom chunkreplaced by heads.3.8 Experiment results of head findingTable 7 shows the comparative experimentresults of head recognition.Table7: Comparative results of head recognition4 Experiment of parsing systemWe perform experiments on the training set andtesting set of Tsinghua Treebank provided byCIPS-SIGHAN-ParsEval-2010.
For the directfluence of parsing result by the length ofsentence, we count the length distribution ofcorpus.inTable 8 shows that the length of training setand testing set of EDSs is mostly less than 20words.
The length of training set of CS is evenlydistributed, while the length of testing set isbetween 30 and 40 words.Complex TemplateCurPhraseTag/ NumCh, CurPhraseTag/ LCh_Word,CurPhraseTag/LCh_Pos,CurPhraseTag/LCh_Pos/RCh_Pos,CurPhraseTag/NumCh/LCh_Pos/ RCh_Pos,CurPhraseTag/NumCh/LCh_Word/LCh_Pos/MCh_Pos/RCh_Word/RCh_Pos,LCh_Word/LCh_Pos, CurPhraseTag/MCh_Pos,NumCh/LCh_Pos/ MCh_Pos/ RCh_Pos,CurPhraseTag/ NumCh/ MCh_Pos,CurPhraseTag/LCh_Word/LCh_Pos/MCh_Pos/RCh_Word/RCh_Pos,LCh_Word/ LCh_Pos, LCh_Pos/ MCh_Pos,CurPhraseTag/NumCh, RCh_Word/RCh_Pos,NumCh/LCh_Word/LCh_Pos/MCh_Pos/RCh_Word/RCh_PosTotal NumWrongNum PrecisionCRFs 7035 93 98.68%CRFs +rule-base+case-base7035 74 98.95%The paper adopts divide-and-conquer strategyto parsing; therefore, we conduct theframe whose length is less than 5 words, the framelength distribution of training set is 9.17% higherthan the testing set; for the frame whose length ismore than 5 words and less than 10 words, thetraining set is 7.65% lower than testing; and for theframe whose length is between 10 words and 20words, the testing set is 20.09% higher comparedwith the training set.
From another aspect, intesting set, CS is 46.2% lower compared withEDSs for frame whose length is less than 5.Therefore, the complexity of frame in CS is higherthan in EDSs.comparative experiment of MNP parsing andframe parsing.
In addition, the results of MNPparsing and frame parsing depend on the lengthlargely, so we list the length distribution ofMNP and frame of EDSs and CS as table 9 andtable 10.As shown in Table 8, 9 and 10, the lengthdistribution of testing set shows that the paring unitlength of EDSs is reduced to less than 10 from lessthan 20 in original sentence and CS is reduced toless than 20 from between 30 and 40 after dividingan original sentence into MNPs parts and framepart.
The above data indicate the divide-and-conquer strategy reduces the complexity ofsentences significantly.Table 8: Length distribution of EDSs and CSEDSs CSlength training settestingsettrainingsettestingset[0, 10) 50.68% 64.30% 10.59% 0[10,20) 37.27% 29.50% 27.55% 0[20,30) 8.64% 5.40% 26.37% 79.9%[30,40) 2.31% 0.60% 16.63% 20.1%40?
1.10% 0.20% 18.86% 0We define Simple MNP (SMNP) whoselength is less than 5 words and Complete MNP(CMNP) whose length is more than 5 words.We can conclude that the parsing result of CSis lower than EDSs from Table 11, which is dueto the higher complexity of MNP and frame in CScompared with EDSs from the results of Table 9and Table 10.
In addition, we obtain about 1%improvement compared with Berkeley parser inMNP and Frame parsing result in EDSs fromTable 11 and Table 12, which indicates that ourmethod is effective for short length parsing units.
Inparticular, Table 12 shows that our result is 1.8%higher than Berkeley parser in the frame parsing ofCS.
Due to the non-consistent frame lengthdistribution of training set and testing set in CSfrom Table 10, we find that Berkeley parser largelydepends on training set compared with our method.Table 9: Length distribution of MNPEDSs CSlength training settestingsettrainingsettestingset[0,5) 55.30% 62.46% 55.42% 59.45%[5,10) 32.66% 29.69% 32.57% 30.77%[10,20) 10.03% 6.75% 10.03% 8.65%20?
2.00% 1.09% 1.98% 1.12%Table 9 shows the length distribution of MNPin training set and testing set of sub-sentence isconsistent in basic, but the SMNP distributionof EDSs is 3.01% less than CS, whichilluminates the complexity of MNP in CS ishigher than in EDSs.EDSs CSlength training settestingsettrainingsettestingset[0,5) 45.84% 47.20% 10.17% 1.00%[5,10) 43.58% 44.00% 24.14% 10.80%[10,20) 9.98% 8.70% 41.31% 62.20%20?
0.60% 0.10% 24.38% 26.00%To more fairly compare the performance ofour proposed method, the comparative resultsare shown as Table 13, the first one (Model01)is combination method of MNP pre-processingand chunk-based, and the chunk-based resultwhich adopts CCRFs method with searchingalgorithm; the second one (Berkeley) is theparsing result of Berkeley parser; the third one(Model02) also is combination method of MNPpre-processing and chunk-based, and the chunk-based result which adopts CCRFs method only;and the lase one (Model03) is the chunk-basedresult which adopts CCRFs method withsearching algorithm.Table 10: Length distribution of frameTable 10 shows the length distribution of framein training set and testing set of EDSs is consistentin basic, while the CS is non-consistent.
For themethod P R FBerkeley 87.5746% 87.8365% 87.7053%EDSsProposed Method 88.5752% 88.6341% 88.6047%Berkeley 84.4755% 84.9182% 84.6963% CSProposed Method 84.7535% 85.046% 84.8995%Table 11: Comparative results of MNP parsingmethod P R FBerkeley 91.3411% 91.1823% 91.2617%EDSsProposed Method 92.4669% 92.0765% 92.2713%Berkeley 85.4388% 85.3023% 85.3705%CSProposed Method 87.3357% 87.0357% 87.1854%Table12: Comparative results of Frame parsingP R FModel 01 85.42% 85.35% 85.39%Berkeley 84.56% 84.62% 84.59%Models 02 85.31% 85.30% 85.31%Models 03 83.99% 83.77% 83.88%Table13: Comparative results of EDSsdj constituent fj constituent overall FP R P R F F FModel 01 78.64% 78.73% 78.69% 70.22% 71.62% 70.91% 74.80%Berkeley 78.37% 78.16% 78.26% 69.43% 72.42% 70.89% 74.58%Models 02 78.18% 78.30% 78.24% 70.20% 70.98% 70.59% 74.41%Models 03 77.38% 77.41% 77.39% 70.39% 70.01% 70.24% 73.82%Table14: Comparative results of CSFrom Table 13, we can see that Model01performance in EDSs is improved by 0.08%than Model02, and the searching algorithmhelps little in EDSs analysis.
From Table 14, wecan see that Model01 performance in CS isimproved by 0.4% than Model02, better thanBerkeley parser result with search algorism.Overall, in EDSs analysis, Model01performance is improved by 0.8% thanBerkeley parser, and in overall F-measure of CS,Model01 performance is 0.22% higher thanBerkeley parser.
From Table 13 and 14, We cansee that Model01 performance in EDSs isimproved by 1.51% than Model03 and theModel01 in CS is improved by 0.98% thanModel03, and the MNP pre-processing helps.5 ConclusionsWe participate in two tasks - EDS Analysisand CS Parsing in CLPS-SIGHAN- ParsEval-2010.
We use divide-and-conquer strategy forparsing and a chunking-based discriminativeapproach to full parsing by using CRF forchunking.
As we all know, CRF is effective forchunking task.
However, the chunking result inthe current level is based on the upper level inthe chunking-based parsing approach, whichwill enhance ambiguity problems when theinput of the current level contains non-terminalsymbols, therefore, the features used inchunking is crucial.
This paper, for effectivelyusing the information of partial trees that havebeen already created, keeps the terminalsymbols in the node containing non-terminalsymbols for features.
Our experiments showthat these features are effective for ambiguityproblems.We suppose that MNP pre-processing beforestatistical model can significantly simplify theanalysis of complex sentences, which will havemore satisfatory results compared with usingstatistical model singly.
The current resultsshow that the MNP pre-processing doessimplify the complex sentences.
However, theperformance of MNP recognition and theparsing of MNP need to be improved, whichwill be our next work.ReferencesYoshimasa Tsuruoka, Jun?ichi Tsujii, SophiaAnaiakou.
2009.
Fast Full Parsing by Linear-Chain Conditional Random Fields.
InProceedings of EACL?09, pages 790-798.Xiao chen, Changning Huang, Mu li, Chunyu Kit.2009.
Better Parser Combination.
In CIPS-ParsEval-2009, pages 81-90.Abney, S.. 1991.
Parsing by chunks, Principle-BasedParsing, Kluwer Academic Publishers.Erik Tjong, Kim Sang.
2000.
Transforming achunker to a parser.
In J.Veenstra W.daelemans,K Sima?
an and J. Zavrek, editors, ComputationalLinguistics in the Netherlands 2000, Rodopi, page177-188.P.L.
Shiuan, C.T.H.
Ann.
1996.
A Divided-and-Conquer Strategy for Parsing.
In Proc.
of theACL/SIGPARSE 5th International Workshop onParsing Technologies.
Santa Cruz, USA, 1996,pages 57-66C.
Braun, G. Neumann, J, Piskorski.
2000.
A Divide-and-Conquer Strategy for Shallow Parsing ofGerman Free Texts.
In Proc.
of ANLP-2000.Seattle, Washington, 2000, pages 239-246.C.Lyon, B.Dickerson.
1997.
Reducing theComplexity of Parsing by a Method ofDecomposition International Workshop onParsing Technology, 1997, pages 215-222.Qiaoli Zhou, Xin Liu, Xiaona Ren, Wenjing Lang,Dongfeng Cai.
2009.
Statistical parsing based onMaximal Noun Phrase pre-processing.
In CIPS-ParsEval-2209.P.L.
Shiuan, C.T.H.
Ann.
A Divide-and-ConquerStrategy for Parsing.
In: Proc.
of theACL/SIGPARSE 5th International Workshop onParsing Technologies.
Santa Cruz, USA, 1996.57-66.C.
Braun, G. Neumann, J. Piskorski.
A Divide-and-Conquer Strategy for Shallow Parsing of GermanFree Texts.
In: Proc.
of ANLP-2000.
Seattle,Washington, 2000.
239-246.C.
Lyon, B. Dickerson.
Reducing the Complexity ofParsing by a Method of Decomposition.International Workshop on Parsing Technology.1997.
215-222.
