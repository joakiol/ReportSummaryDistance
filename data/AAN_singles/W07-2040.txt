Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 191?194,Prague, June 2007. c?2007 Association for Computational LinguisticsIRST-BP: Preposition Disambiguationbased onChain Clarifying Relationships ContextsOctavian PopescuFBK-IRST, Trento (Italy)popescu@itc.itSara TonelliFBK-IRST, Trento (Italy)satonelli@itc.itEmanuele PiantaFBK-IRST, Trento (Italy)pianta@itc.itAbstractWe are going to present a technique ofpreposition disambiguation based onsense discriminative patterns, which areacquired using a variant of Angluin?s al-gorithm.
They represent the essential in-formation extracted from a particulartype of local contexts we call ChainClarifying Relationship contexts.
Thedata set and the results we present arefrom the Semeval task, WSD of Preposi-tion (Litkowski 2007).1 IntroductionWord Sense Disambiguation (WSD) is a prob-lem of finding the relevant clues in a surround-ing context.
Context is used with a wide scope inthe NLP literature.
However, there is a dichot-omy among two types of contexts, local andtopical contexts (Leacock et.
all 1993), that isgeneral enough to encompass the whole notionand at the same to represent a relevant distinc-tion.The local context is formed by information onword order, distance and syntactic structure andit is not restricted to open-class words.
A topicalcontext is formed by the list of those words thatare likely to co-occur with a particular sense of aword.
Generally, the WSD methods have amarked predilection for topical context, with theconsequence that structural clues are rarely, ifever, taken into account.
However, it has beensuggested (Stetina&Nagao 1997, Dekang 1997)that structural words, especially prepositions andparticles, play an important role in computingthe lexical preferences considered to be the mostimportant clues for disambiguation.Closed class words, prepositions in particular,are ambiguous (Litkowski&Hargraves2006).Their disambiguation is essential for the correctprocessing of the meaning of a whole phrase.
Awrong PP-attachment may render the sense ofthe whole sentence unintelligible.
Consider forexample:(1) Joe heard the gossip about you and me.
(2) Bob rowed about his old car and hismother.A probabilistic context free grammar mostlikely will parse both (1) and (2) wrongly1.
Itwould attach ?about?
to ?to hear?
in (1) andwould consider the ?his old car and his mother?the object of ?about?
in (2).The information needed for disambiguation ofopen class words is spread at all linguistics lev-els, from lexicon to pragmatics, and can be lo-cated within all discourse levels, from immedi-ate collocation to paragraphs (Stevenson&Wilks1999).
Intuitively, prepositions have a differentbehavior.
Most likely, their senses are deter-mined within the government category of their1Indeed, Charniak?s parser, considered to be amongthe most accurate ones for English, parses wronglyboth of them.191heads.
We expect the local context to play themost important role in the disambiguation ofprepositions.We are going to present a technique of prepo-sition disambiguation based on sense discrimina-tive patterns, which are acquired using a variantof Angluin?s algorithm.
These patterns representthe essential information extracted from a par-ticular type of local contexts we call ChainClarifying Relationship contexts.
The data setand the results we present are from the Semevaltask, WSD of Preposition (Litkowski 2007).In Section 2 we introduce the Chain Clarify-ing Relationships, which represent particulartypes of local contexts.
In Section 3 we presentthe main ideas of the Angluin algorithm.
Weshow in Section 4 how it can be adapted to ac-commodate the preposition disambiguation task.Section 5 is dedicated to further research.2 Chain Clarifying RelationshipsWe think of ambiguity of natural language as anet - like relationship.
Under certain circum-stances, a string of words represents a uniquecollection of senses.
If a different sense for oneof these words is chosen, the result is an un-grammatical sentence.
Consider (3) below:(3) Most people do not live in a state ofhigh intellectual awareness about theirevery action.Suppose one chooses the sense of ?to live?
tobe ?to populate?.
Then, its complement, ?state?,should be synonym with location.
The analysiscrashes when ?awareness?
is considered.
Thereare two things we notice here: (a) the relation-ship between ?live?
and ?state?
?
the only twoacceptable sense combination out of four are(populate, location) and (experience, entity) ?and (b) the chain like relationship between?awareness?, ?state?, ?live?
where the sense ofany of them determines the sense of all the oth-ers in a cascade effect, or results in ungrammati-cality.
A third thing, not directly observable in(3) is that the syntactic configuration is crucial inorder for (a) and (b) to arise.
Example (4) showsthat in a different syntactic configuration theabove sense relationship simply disappears:(4) The awareness of people about the state insti-tutions is arguably the first condition to livein a democratic state.We call the relationship between ?live?,?state?, ?awareness?
a Chain Clarifying Rela-tionship (CCR).
In that specific syntactic con-figuration their senses are interdependent andindependent of the rest of the sentence.
To eachCCR corresponds a sense discriminative pattern.Our goal is to learn which local contexts areCCRs.
Each CCR is a pattern of words on a syn-tactic configuration.
Each slot can be filled onlyby words defined by certain lexical features.
Tolearn a CCR means to discover the syntacticconfiguration and the respective features.
Forexample consider (5) and (6) with their CCRs in(CCR5) and (CCR6) respectively:(5) Some people lived in the same state ofdisappointment/ optimism/ happiness.
(CCR5) (vb=live_sense_2, prep1=in_1,prep1_obj=state_sense_1,prep2=of_sense_1a,prep2_obj=[State_of_Spirit])(6) Some people lived in the same state ofAfrica/ Latin America/ Asia.
(CCR6) (vb=live_sense_1, prep1=in_1,prep1_obj=state_sense_1,prep2=of_1b,prep2_obj = [Location])The lexical features of the open class words ina specific syntactic configuration trigger thesenses of each word, if the context is a CCR.
In(CCR5) any word that has the same lexical traitas the one required by prep2_obj slot will deter-mine a unique sense for all the other words, in-cluding the preposition.
The same holds for(CCR6).
The difference between (CCR5) and(CCR6) is part of the linguistic knowledge(which can be clearly shown: ?how?
(5) vs.?where?
(6)).The CCR approach proposes a deterministicapproach to WSD.
There are two features ofCCRs which are interesting from a strictly prac-tical point of view.
Firstly, CCR proposal is away to determine the size of the window wherethe disambiguation clues are searched for (manyWSD algorithms arbitrarily set it apriori).
Sec-ondly, within a CCR, by construction, the senseof one word determines the senses of all the oth-ers.1923 Angluin Learning AlgorithmOur working hypothesis is that we can learn theCCRs contexts by inferring differences via aregular language learning algorithm.
What wewant to learn is which features fulfil each syn-tactic slot.
First we introduce the original An-gluin?s algorithm and then we mention a variantof it admitting unspecified values.Angluin proved that a regular set can belearned in polynomial time by assuming the ex-istence of an oracle which can gives ?yes/no?answers and counterexamples to two types ofqueries: membership queries and conjecture que-ries (queries about the form of the regular lan-guage) (Angluin 1998).The algorithm employs an observation tablebuilt on prefix /suffix closed classes.
To eachword a {1, 0} value is associated, ?1?
meaningthat the word belongs to the target regular lan-guage.
Initially the table is empty and is filledincrementally.
The table is closed if all prefixesof the already seen examples are in the table andis consistent if two rows dominated by the sameprefix have the same value, ?0?
or ?1?.If the table is not consistent or closed then aset of membership queries is made.
If the table isconsistent and closed then a conjecture query ismade.
If the oracle responds ?no?, it has to pro-vide a counterexample and the previous steps arecycled till ?yes?
is obtained.The role of the oracle for conjecture questionscan be substituted by a stochastic process.
Ifstrict equality is not requested, then a probablyapproximately correct identification of languagecan be obtained (PAC identification), whichguarantees that the two languages (the identifiedone, Li, and the target one, Lt) are equal up to acertain extent.
The approximation is constrainedby two parameters ?
?
accuracy and ?
?
confi-dence, and the constraint is P(d(Li, Lt) ?
?)
?
?
),where the distance between two languages is theprobability to see a word in just one of them.The algorithm can be further generalized towork with unspecified values.
The examplesmay have three values (?yes?, ?no?, ???
), as inmany domains one has to deal with partialknowledge The main result is that a variant ofthe above algorithm successfully halts if thenumber of counterexamples provided by the ora-cle have O(log n) missing attributes, where n isthe number of attributes (Goldmann et al 2003).4 Preposition Disambiguation TaskThe CCR extraction algorithm is supervised.Consider that you have a sense annotated cor-pora.
Extract the dependency paths and filter outthe ones which are not sense discriminative.
Tryto generalize each slot and retain the minimalones.
What is left are CCRs.Unfortunately, for the preposition disam-biguation task the training set is sense annotatedonly for prepositions.
We have undertaken a dif-ferent strategy.
The training corpus can be usedas an oracle.
The main idea is to start with a setof few examples for each sense from the trainingset which are considered to be the most repre-sentative ones.
We try to generalize each ofthem independently and to tackle down the bor-der cases (the cases that may correspond to twodifferent senses) which are considered unspeci-fied examples.
The process stops when the ora-cle does not bring any new information (thetraining cases have been learned).
Below weexplain this process step by step.Step 1.
Get the seed examples.
For eachpreposition and sense get the seed examples.This operation is performed by a human expert.It may be the case that the glosses or the diction-ary definition are a good starting point (with theadvantage that the intervention of a human is nomore required).
However, we preferred do to itmanually for better precision.Besides the most frequent sense, we have con-sidered, in average, another two senses.
There isa practical reason for this limitation: the numberof examples for the rest of the senses is insuffi-cient.
In total we have considered 149 senses outof the 241 senses present in the training set.
Foreach an average of three examples has been cho-sen.Step 2.
Get the CCRs.
For each example weread the lex units associated with its frame fromFrameNet.
Our goal is to identify the relevantsyntactic and lexical features associated witheach slot.
We have undertaken two simplifyingassumptions.
Firstly, only the government cate-gory of the head of the PP is considered (whichcan be a verb, a noun or an adjective).
Secondly,193the lexical features are identified with synsetsfrom WordNet.We have used the Charniak?s parser to extractthe structure of the PP-phrases and further wehave used Collin?s algorithm to implement ahead recogniser.A head can have many synsets.
In order tounderstand which sense the word has in the re-spective construction we look for the synsetcommon to the elements extracted from lex.
Ifthe proposed synset uniquely identifies just onesense then it is considered a CCR.
If not, we arelooking for the next synset.
This step corre-sponds to membership queries in Angluin?s al-gorithm.Step 3.
Generalize the CCRs.
At the end ofstep 2 we have a set of CCRs for each sense.
Weobtained 395 initial CCRs.
We tried to extendthe coverage by taking into account the hypero-nyms of each synsets.
Only approximately 10%of these new patterns have received an answerfrom the oracle.
Consequently, for our ap-proach ,a part of the training corpus has not beenused.
It serves only 15 examples in average toget a correct CCR.
All the instances of the sameCCR do not bring any new information to ourapproach.Posteriori, we have noticed that the initial pat-terns have an almost 50% (48.57%) coverage inthe test data.
The generalized patterns obtainedafter the third step have 82% test corpus cover-age.
For the rest 18%, which are totally un-known cases, we have chosen the most frequentsense.In table 1 we present the performances of oursystem.
It achieves 0.65 (FF-score), which com-pares favourably against baseline ?
the most fre-quent -of 0.53.
On the first column of Table 1we write the FF score interval - more than 0.75,between 0.75 and 0.5, and less than 0.5 respec-tively, - on the second column we present thenumber of cases within that interval the systemsolved and on the third column we include thecorresponding number for baseline.Table 1Interval System Baseline1.00 - 0.75 18 80.75 - 0.50 15 60.00 ?
0.50 2 205 Conclusion and Further ResearchOur system did not perform very well (third po-sition out of three).
Analyzing the errors, wehave noticed that our system systematically con-found two senses in some cases (for example?by?
5(2) vs. 15(3), for ?on?
4(1c) vs. 1(1) etc.
).We would like to see whether these errors aredue to a misclassification in training.ReferencesAngluin, D. (1987): ?Learning Regular Setsfrom Queries and Counterexamples?, Infor-mation and Computation Volume 75 ,  Issue 2Goldman, S., Kwek, S., Scott, S. (2003): ?Learn-ing from examples with unspecified attributevalues?, Information and Computation, Vol-ume 180Leacock, C., Towell, G., Voorhes, E. (1993):?Towards Building Contextual Representa-tions of Word Senses Using Statistical Mod-els?, In Proceedings, SIGLEX workshop: Ac-quisition of Lexical Knowledge from TextLin, D. (1997): ?Using syntactic dependency aslocal context to resolve word sense ambigu-ity?.ACL/EACL-97,  MadridLitkowski, K. C. (2007):?Word Sense Disam-biguation of Prepositions?
, The Semeval2007 WePS Track.
In Proceedings of Semeval2007, ACLLitkowski, K. C., Hargraves O.
(2006): ?Cover-age and Inheritance in the Preposition Project",Proceedings of the Third ACL-SIGSEMWorkshop on Prepositions, Trento,Stetina J, Nagao M (1997): ?Corpus based PPattachment ambiguity resolution with a se-mantic dictionary.
?, Proc.
of the 5th Work-shop on very large corpora, Beijing andHongkong, pp 66-80Stevenson K., Wilks, Y.,(2001): ?The interactionof knowledge sources in word sense disam-biguation?, Computational Linguistics,27(3):321?349.194
