Proceedings of the NAACL HLT Student Research Workshop and Doctoral Consortium, pages 13?18,Boulder, Colorado, June 2009. c?2009 Association for Computational LinguisticsExploring Topic Continuation Follow-up Questions using Machine LearningManuel KirschnerKRDB CenterFaculty of Computer ScienceFree University of Bozen-Bolzano, Italykirschner@inf.unibz.itRaffaella BernardiKRDB CenterFaculty of Computer ScienceFree University of Bozen-Bolzano, Italybernardi@inf.unibz.itAbstractSome of the Follow-Up Questions (FU Q) thatan Interactive Question Answering (IQA) sys-tem receives are not topic shifts, but rathercontinuations of the previous topic.
In this pa-per, we propose an empirical framework to ex-plore such questions, with two related goals inmind: (1) modeling the different relations thathold between the FU Q?s answer and either theFU Q or the preceding dialogue, and (2) show-ing how this model can be used to identify thecorrect answer among several answer candi-dates.
For both cases, we use Logistic Regres-sion Models that we learn from real IQA datacollected through a live system.
We show thatby adding dialogue context features and fea-tures based on sequences of domain-specificactions that represent the questions and an-swers, we obtain important additional predic-tors for the model, and improve the accuracywith which our system finds correct answers.1 IntroductionInteractive Question Answering (IQA) can be de-scribed as a fusion of the QA paradigm with di-alogue system capabilities.
While classical QA isconcerned with questions posed in isolation, its in-teractive variant is intended to support the user infinding the correct answer via natural-language dia-logue.
In an IQA setting, both the system and theuser can pose Follow-Up Questions (FU Q).
In thesecond case, whenever an IQA system receives anadditional user question (note that this is what wecall a Follow-Up Question throughout this work), itcan either interpret it as being thematically related toa previous dialogue segment (topic continuation), oras a shift to some new, unrelated topic (topic shift).A definition of thematic relatedness of FU Qs mightrely on the elements of the attentional state, i.e., onthe objects, properties and relations that are salientbefore and after processing the user question.
Topiccontinuation FU Qs should be interpreted within thecontext, whereas topic shift FU Qs have to be treatedas first questions and can thus be processed withstandard QA technologies.
Therefore, a first taskin IQA is to detect whether a FU Q is a topic shift ora topic continuation (Yang et al, 2006).To help answering topic continuation FU Qs, anIQA system would need to fuse the FU Q with cer-tain information from the dialogue context (cf.
(vanSchooten et al, 2009)).
Thus, a second task in IQAis to understand which turns in the dialogue contextare possible locations of such information, and ex-actly what kind of information should be considered.Knowing that a FU Q concerns the same topic as theprevious question or answer, we thus want to studyin more detail the way the informational content ofquestions and answers evolves before/after the FU Qis asked.
A model of these so-called informationaltransitions would provide insights into what a user islikely to ask about next in natural coherent human-machine dialogue.In order to tackle any of the two IQA tasks men-tioned above we need IQA dialogues.
Most currentwork on IQA uses the TREC QA data; the TRECQA tracks in 2001 and 2004 included series of con-text questions, where FU Qs always depended on thecontext set by an earlier question from the same se-ries.
However, these data were constructed artifi-cially and are not representative of actual dialoguesfrom an IQA system (for instance, system answersare not considered at all).
Real IQA data yield chal-13lenges for an automatic processing approach (Yanget al, 2006).
Our work is based on collecting andanalyzing IQA dialogues from users of a deployedsystem.In this paper, we address the second task intro-duced above, namely the study of common relationsbetween the answer to a topic continuation FU Q andother turns in the dialogue context.
Our collected di-alogue data are from the ?library help desk?
domain.In many of the dialogues, library users request in-formation about a specific library-related action; weare thus dealing with task-oriented dialogues.
Thiswork is based on two hypotheses regarding relationsholding between the FU Q?s answer and the dialoguecontext.
For studying such relations, we want to ex-plore the usefulness of (1) a representation of thelibrary-related action underlying questions and an-swers, and (2) a representation of the dialogue con-text of the FU Q.2 BackgroundIn order to understand what part of the history ofthe dialogue is important for processing FU Qs,significant results come from Wizard-of-Oz stud-ies, like (Dahlba?ck and Jo?nsson, 1989; Bertomeuet al, 2006; Kirschner and Bernardi, 2007), fromwhich it seems that the immediate linguistic context(i.e., the last user initiative plus the last system re-sponse) provides the most information for resolvingany context-dependency of the FU Qs.
These studiesanalyzed one particular case of topic continuationFU Q, namely those questions containing reference-related discourse phenomena (ellipsis, definite de-scription or anaphoric pronoun); we assume that theresults could be extended to fully specified ques-tions, too.Insights about the informational transitions withina dialogue come from Natural Language Genera-tion research.
(McCoy and Cheng, 1991) providea list of informational transitions (they call them fo-cus shifts) that we can interpret as transitions basedon certain thematic relations.
Depending on the con-versation?s current focus type, they list specific focusshift candidates, i.e., the items that should get focusas a coherent conversation moves along.
Since weare interested in methods for interpreting FU Qs au-tomatically, we decided to restrict ourselves to useNode type Informational transition targetsAction Actor, object, etc., of the action ?any participant (Fillmore) role; pur-pose (goal) of action, next action insome sequence, subactions, special-izations of the actionTable 1: Possible informational transition targets for ?ac-tion?
node type (McCoy and Cheng, 1991)only the ?action?
focus type to represent the focusof questions and answers in IQA dialogues.
We con-jecture that actions form a suitable and robust basisfor describing the (informational) meaning of utter-ances in our class of task-based ?help desk?
IQA di-alogues.
Table 1 shows the focus shift candidatesfor a current focus of type ?action?.
In this workwe concentrate on the informational transitions in-volving two actions (i.e., including one of the focustargets listed in bold face in the table).3 Exploring topic continuation FU Qsusing Machine LearningWe base our study of topic continuation FU Qs onthe two main results described in Section 2: Westudy snippets of dialogues consisting of four turns,viz.
a user question (Q?1), the corresponding sys-tem answer (A?1), the FU Q and its system answer(A0); we use Logistic Regression Models to learnfrom these snippets (1) which informational (action-action) transitions hold between A0 and the FU Qor the preceding dialogue, and (2) how to predictwhether a specific answer candidate A0 is correct fora given dialogue snippet.3.1 Machine learning framework: LogisticRegressionLogistic regression models (Agresti, 2002) are gen-eralized linear models that describe the relationshipbetween features (predictors) and a binary outcome(in our case: answer correctness).
We estimate themodel parameters (the beta coefficients ?1, .
.
.
, ?k)that represent the contribution of each feature to thetotal answer correctness score using maximum like-lihood estimation.
Note that there is a close rela-tionship to Maximum Entropy models, which haveperformed well in many tasks.
A major advantageof using logistic regression as a supervised machine14learning framework (as opposed to other, possiblybetter performing approaches) is that the learned co-efficients are easy to interpret.
The logistic regres-sion equation which predicts the probability for aparticular answer candidate A0 being correct, de-pending on the learned intercept ?0, the other betacoefficients and the feature values x1, .
.
.
, xk (whichthemselves depend on a combination of Q?1, A?1,FU Q or A0) is:Prob{answerCorrect} = 11 + exp(?X??)
, whereX??
= ?0 + (?1x1 + .
.
.+ ?kxk)3.2 Dialogue data collectionWe have been collecting English human-computerdialogues using BoB, an IQA system which is pub-licly accessible on the Library?s web-site of ouruniversity1.
We see the availability of dialoguedata from genuinely motivated visitors of the libraryweb-site as an interesting detail of our approach; ourdata are less constrained and potentially more dif-ficult to interpret than synthesized dialogues (e.g.,TREC context track data), but should on the otherhand provide insights into the structure of actualIQA dialogues that IQA systems might encounter.We designed BoB as a simple chatbot-inspired ap-plication that robustly matches user questions usingregular expression-based question patterns, and re-turns an associated canned-text answer from a repos-itory of 529.
The question patterns and answershave been developed by a team of librarians, andcover a wide range of library information topics,e.g., opening time, lending procedures and differentlibrary services.
In the context of this work, we useBoB merely as a device for collecting real human-computer IQA dialogues.As a preliminary step towards automatically mod-eling action-based informational transitions trig-gered by FU Qs, we annotated each of the 529 an-swers in our IQA system?s repository with the ?li-brary action?
that we considered to best representits (informational) meaning.
For this, we had de-vised a (flat) list of 25 library-related actions by an-alyzing the answer repository (e.g.
: access, borrow,change, deliver).
We also added synonymous verbs1www.unibz.it/libraryto our action list, like ?obtain?
for ?borrow?.
If wedid not find any action to represent a system an-swer, we assigned it a special ?generic-information?tag, e.g.
for answers to questions like ?What are theopening times?
?.We base our current study on the dialogues col-lected during the first four months of the IQA sys-tem being accessible via the Library?s web site.
Af-ter a first pass of manually filtering out dialoguesthat consisted only of a single question, or where thequestion topics were only non-library-related, thecollected corpus consists of 948 user questions (firstor FU Qs) in 262 dialogue sessions (i.e., from differ-ent web sessions).
We hand-annotated the user FUQs in these dialogues as either ?topic continuation?
(248 questions), or ?topic shift?
(150 questions).The remaining FU Qs are user replies to system-initiative clarification questions, which we do notconsider here.
For each user question, we markedwhether the answer given by the IQA system wascorrect; in the case of wrong answers, we asked ourlibrary domain experts to provide the correct answerthat BoB should have returned.
However, we onlycorrected the system answer in those cases wherethe user did not ask a further FU Q afterwards, aswe must not change on-going dialogues.To get the actual training/test data, we had to fur-ther constrain the set of 248 topic continuation FUQs.
We removed all FU Qs that immediately followa system answer that we considered incorrect; this isbecause any further FU Q is then uttered in a situa-tion where the user is trying to react to the problem-atic answer, which clearly influences the topic of theFU Q.
Of the then remaining 76 FU Qs, we keep thefollowing representation of the dialogue context: theprevious user question Q?1 and the previous systemanswer A?1.
We also keep the FU Q itself, and itscorresponding correct answer A0.Finally, we automatically annotated each questionwith one or more action tags.
This was done by sim-ply searching the stemmed question string for anyverb stem from our list of 25 actions (or one of theirsynonyms); if no action stem is found, we assignedthe ?generic-information?
tag to the question.
Notethat this simple action detection algorithm for ques-tions fails in case of context-dependent questionswhere the verb is elided or if the question containsstill unknown action synonyms.153.3 FeaturesIn the machine learning framework introducedabove, the model is intended to predict the correct-ness of a given system answer candidate, harnessinginformation from the local dialogue context: Q?1,A?1, FU Q and the particular answer candidate A0.We now introduce different features that relate A0 toeither the FU Q or some other preceding turn of thedialogue.
The features describe specific aspects ofhow the answer candidate relates to the current dia-logue.
Note that we do not list features relating Q?1and A0, since our experiments showed no evidencefor including them in our models.tfIdfSimilarityQA, tfIdfSimilarityAA: TF/IDF-based proximity scores (ranging from 0 to 1) be-tween two strings, namely FU Q and A0, or A?1and A0, respectively.
Based on vector similarity (us-ing the cosine measure of angular similarity) overdampened and discriminatively weighted term fre-quencies.
Definition of the TF/IDF distance: twostrings are more similar if they contain many of thesame tokens with the same relative number of occur-rences of each.
Tokens are weighted more heavily ifthey occur in few documents2, hence we used a sub-set of the UK English version of the Web-as-Corpusdata3 to train the IDF scores.Features based on action sequences.
To describethe action-related informational transitions we ob-serve between the FU Q and A0 and between A?1and A0, we use two sets of features, both of whichare based on hand-annotated actions for answersand automatically assigned actions for questions.actionContinuityQA, actionContinuityAA: sim-ple binary features indicating whether the same li-brary action (or one of its synonyms) was identi-fied between the FU Q and A0, or A?1 and A0, re-spectively.
lmProbQA, lmProbAA: encode Statis-tical Language Model probabilities for action tag se-quences, i.e., the probability for A0 having a certainaction, given the action associated with FU Q, or theaction of A?1, respectively.
The underlying Statis-tical Language Models are probability distributions2Cf.
Alias-i?s LingPipe documentation http://alias-i.com/lingpipe/demos/tutorial/stringCompare/read-me.html3http://wacky.sslmit.unibo.itover action-action sequences that reflect how likelycertain action sequences occur in our IQA dialogues,thus capturing properties of salient action sequences.More technically, we use Witten-Bell smoothed 2-gram statistical language models, which we trainedon our action-tagged FU Q data.4 ResultsFor the evaluation of the logistic regression model,we proceed as follows.
Applying a cross-validationscheme, we split our 76 FU Q training examplesrandomly into five non-intersecting partitions of 15(or 16) FU Q (with corresponding Q?1, A?1, andcorrect A0) each.
To train the logistic regressionmodel, we need training data consisting of a vec-tor of independent variables (the various feature val-ues), along with the binary dependent variable, i.e.,?answer correct?
or ?answer false?.
We generatethese training data by ?multiplying out?
each train-ing partition?s 61 FU Qs (76 minus the held-out testset of 15) with all 529 answer candidates; for eachFU Q dialogue snippet used for training, this resultsin one positive training example (where A0 is the 1correct out 529 answer candidates), and 528 nega-tive training examples (for all other answer candi-dates).For each of the five training/test partitions, wetrain a different model.
We then evaluate each ofthese models on their corresponding held-out testset.
Following the cross-validation idea through, wealso train separate Statistical Language Models onsequences of action tags for each of the five trainingsplits; this ensures that the language model proba-bilities were never trained on test data.
We performthe evaluation in terms of the mean rank that the cor-rect answer A0 is assigned after ranking all 529 an-swer candidates (by evaluating the logistic regres-sion equation to yield answer scores).In the following, we give details of different lo-gistic regression models we experimented with.
Ini-tially, we chose a subset from the list of featuresintroduced above.
Our goal was to retain as fewfeatures as needed to explore our two hypotheses,i.e., whether we can make use of (1) a representa-tion of the FU Q?s underlying library action, and/or(2) a representation of the immediate dialogue con-text.
By dropping uninformative features, the result-16ing models become simpler and easier to interpret.With this goal in mind, we applied a fast backwardselimination routine that drops uninformative predic-tors (cf.
(Baayen, 2008, p.204)) on the five trainingdata splits.
In all five splits, both TF/IDF featuresturned out to be important predictors; in four of thesplits, also lmProbQA was retained.
lmProbAA wasdropped as superfluous in all but two splits, and ac-tionSimilarityAA was retained only in one.
Withthese results, the set of features we retain for ourmodeling experiments is: tfIdfSimilarityQA, tfIdf-SimilarityAA and lmProbQA.?Complete?
model: tfIdfSimilarityQA, tfIdfSim-ilarityAA and lmProbQA We estimated logisticregression models on the five cross evaluation train-ing sets using all three features as predictors.
Table 2shows the mean ranks of the correct answer for thefive evaluation runs, and an overall mean rank withthe average across the five splits.To illustrate the contribution of each of the threepredictors towards the score of an answer candi-date, we provide the (relevant linear part of) thelearned logistic regression equation for the ?com-plete?
model (trained on split 1 of the data).
Notethat the ?answer ranker?
evaluates this equation toget a score for an answer candidate A0.X??
= ?8.4 + (9.5 ?
tfIdfSimilarityQA +4.6 ?
tfIdfSimilarityAA +1.7 ?
lmProbQA)Reduced model 1: No representation of dialoguecontext Only the features concerning the FU Qand the answer A0 (tfIdfSimilarityQA, lmProbQA)are used as predictors in building the logistic re-gression model.
The result is a model that treatsevery FU Q as a stand-alone question.
Across thefive models, the coefficient for tfIdfSimilarityQA isroughly five times the size of that for lmProbQA.Reduced model 2: No action sequences Wekeep only the two TF/IDF features (tfIdfSimilari-tyQA, tfIdfSimilarityAA).
This model thus does notuse any features that depend on human annotation,but only fully automatic features.
The coefficientlearned for tfIdfSimilarityQA is generally twice aslarge as that for tfIdfSimilarityAA.Reduced model 3: No dialogue context, no actionsequences Considered as a baseline, this modeluses a single feature (tfIdfSimilarityQA) to predictanswer correctness, favoring those answer candi-dates that have the highest lexical similarity wrt.
theFU Q.5 DiscussionIn order to better understand the relatively highmean ranks of the correct answer candidates acrossTable 2, we scrutinized the results of the answerranker (based on all tests on the ?complete?
model).The distribution of the ranks of correct answers isclearly skewed; in around half of the 76 cases, thecorrect answer was actually ranked among the top20 of the 529 answer candidates.
However, the meancorrect rank deteriorates badly due to the lowest-ranking third of cases.
Analyzing these lowest-ranking cases, it appears that they are often instancesof two sub-classes of topic continuation FU Qs: (i)the FU Q is context-dependent, i.e., underspecifiedor exhibiting reference-related discourse phenom-ena; (ii) the FU Q is a slight variation of the pre-vious question (e.g.
only the wh-phrase changes, oronly the object changes).
This error analysis seemsto suggest that it should be worthwhile to distin-guish between sub-classes of topic-continuation FUQs, and to improve specifically how answers for the?difficult?
sub-classes are ranked.The relatively high mean ranks are also due to thefact that in our approach of acquiring dialogue data,for each FU Q we marked only one answer from thewhole repository as ?correct?.
Again for the ?com-plete?
model, we checked the top 20 answer can-didates that ranked higher than the actual ?correct?one.
We found that in over half of the cases an an-swer that could be considered correct was among thetop 20.Looking at the ranking results across the differ-ent models in Table 2, the fact that the ?complete?model seems to outperform each of the three re-duced models (although no statistical significancecould be attained from comparing the rank num-bers) confirms our two hypotheses proposed earlier.Firstly, identifying the underlying actions of ques-tions/answers and modeling action-based sequencesyield important information for identifying correct17Reduced m. 3 Reduced m. 2 Reduced m. 1 Complete modelPredictors tfIdfSimilarityQA tfIdfSimilarityQA, tfIdfSimilarityQA, tfIdfSimilarityQA,in model tfIdfSimilarityAA tfIdfSimilarityAA,lmProbQA lmProbQASplit 1 141.2 108.4 112.5 96.2Split 2 102.7 97.4 53.8 57.7Split 3 56.7 63.7 50.5 52.7Split 4 40.5 26.2 37.9 35.7Split 5 153.1 105.3 129.6 89.1Mean 98.8 80.2 76.7 66.3Table 2: Mean ranks of correct A0 out of 529 answer candidates, across models and training/test splitsanswers to topic continuation FU Qs.
Secondly, asfor the role of the immediate dialogue context forproviding additional clues for identifying good an-swers to FU Qs, our data show that a high lexicalsimilarity score between A?1 and A0 indicates a cor-rect answer candidate.
While (Yang et al, 2006)point out the importance of Q?1 to provide contextinformation, in our experiments it was generally su-perseded by A?1.As for the two features relating the underlyingactions of A?1 and A0 (actionContinuityAA, lm-ProbAA), the picture seems less clear; in our currentmodeling experiments, we had not enough evidenceto keep these features.
However, we plan to explorethe underlying idea of action-action sequences in thefuture, and conjecture that such information shouldcome into its own for context-dependent FU Qs.6 Future workBesides annotating and using more dialogue data asmore people talk to our IQA system, we plan toimplement a state-of-the-art topic-shift detection al-gorithm as proposed in (Yang et al, 2006), train-ing and testing it on our own FU Q data.
We willattempt to improve this system by adding action-based features, and then extend it to distinguishthree classes: topic shifts, (topic continuation) FUQs that are fully specified, and (topic continuation)context-dependent FU Qs.
We then plan to builddedicated logistic regression models for the differ-ent sub-classes of topic continuation FU Qs.
If eachmodel uses a specific set of predictors, we hope toimprove the overall rank of correct answers acrossthe different classes of FU Qs.
Also, from compar-ing the different models, we are interested in study-ing the specific properties of different FU Q types.References[Agresti2002] Alan Agresti.
2002.
Categorical DataAnalysis.
Wiley-Interscience, New York.
[Baayen2008] R. Harald Baayen.
2008.
Analyzing Lin-guistic Data.
Cambridge University Press.
[Bertomeu et al2006] Nu?ria Bertomeu, Hans Uszkoreit,Anette Frank, Hans-Ulrich Krieger, and Brigitte Jo?rg.2006.
Contextual phenomena and thematic relationsin database QA dialogues: results from a wizard-of-ozexperiment.
In Proc.
of the Interactive Question An-swering Workshop at HLT-NAACL 2006, pages 1?8,New York, NY.
[Dahlba?ck and Jo?nsson1989] Nils Dahlba?ck and ArneJo?nsson.
1989.
Empirical studies of discourse repre-sentations for natural language interfaces.
In Proc.
ofthe 4th Conference of the European Chapter of theACL (EACL?89), pages 291?298, Manchester, UK.
[Kirschner and Bernardi2007] Manuel Kirschner andRaffaella Bernardi.
2007.
An empirical view oniqa follow-up questions.
In Proc.
of the 8th SIGdialWorkshop on Discourse and Dialogue, Antwerp,Belgium.
[McCoy and Cheng1991] Kathleen F. McCoy and Jean-nette Cheng.
1991.
Focus of attention: Constrainingwhat can be said next.
In Cecile L. Paris, William R.Swartout, and William C. Mann, editors, Natural Lan-guage Generation in Artificial Intelligence and Com-putational Linguistics, pages 103?124.
Kluwer Aca-demic Publishers, Norwell, MA.
[van Schooten et al2009] Boris van Schooten, R. op denAkker, R. Rosset, O. Galibert, A. Max, and G. Illouz.2009.
Follow-up question handling in the IMIX andRitel systems: A comparative study.
Journal of Natu-ral Language Engineering, 15(1):97?118.
[Yang et al2006] Fan Yang, Junlan Feng, and GiuseppeDi Fabbrizio.
2006.
A data driven approach to rele-vancy recognition for contextual question answering.In Proc.
of the Interactive Question Answering Work-shop at HLT-NAACL 2006, pages 33?40, New YorkCity, NY.18
