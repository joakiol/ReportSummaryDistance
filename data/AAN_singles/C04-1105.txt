Bilingual-Dictionary Adaptation to DomainsHiroyuki KajiCentral Research Laboratory, Hitachi, Ltd.1-280 Higashi-Koigakubo, Kokubunji-shi, Tokyo 185-8601, Japankaji@crl.hitachi.co.jpAbstractTwo methods using comparable corpora to se-lect translation equivalents appropriate to a do-main were devised and evaluated.
The firstmethod ranks translation equivalents of a targetword according to similarity of their contexts tothat of the target word.
The second methodranks translation equivalents according to theratio of associated words that suggest them.
Anexperiment using the EDR bilingual dictionarytogether with Wall Street Journal and NihonKeizai Shimbun corpora proved that the methodusing the ratio of associated words outperformsthe method based on contextual similarity.Namely, in a quantitative evaluation usingpseudo words, the maximum F-measure of theformer method was 86%, while that of the lattermethod was 82%.1 IntroductionIt is well known that appropriate translations for aword vary with domains, and bilingual-dictionaryadaptation to domains is an effective way to improvethe performance of, for example, machine translationand cross-language information retrieval.
However,bilingual dictionaries have commonly been adaptedto domains on the basis of lexicographers?
intuition.It is thus desirable to develop an automated methodfor bilingual-dictionary adaptation.Technologies for extracting pairs of translationequivalents from parallel corpora have been estab-lished (Gale and Church 1991; Dagan, et al 1993;Fung 1995; Kitamura and Matsumoto 1996;Melamed 1997).
They can, naturally, be used toadapt a bilingual dictionary to domains, that is, toselect corpus-relevant translation equivalents fromamong those provided by an existing bilingual dic-tionary.
However, their applicability is limited be-cause of the limited availability of large parallelcorpora.
Methods of bilingual-dictionary adaptationusing weakly comparable corpora, i.e., a pair of twolanguage corpora of the same domain, are thereforerequired.There are a number of previous works related tobilingual-dictionary adaptation using comparablecorpora.
Tanaka and Iwasaki?s (1996) optimizationmethod for a translation-probability matrix mainlyaims at adapting a bilingual dictionary to domains.However, it is hampered by a huge amount of com-putation, and was only demonstrated in a small-scaleexperiment.
Several researchers have developed acontextual-similarity-based method for extractingpairs of translation equivalents (Kaji and Aizono1996; Fung and McKeown 1997; Fung and Yee1998; Rapp 1999).
It is computationally efficientcompared to Tanaka and Iwasaki?s method, but theprecision of extracted translation equivalents is stillnot acceptable.In the light of these works, the author proposestwo methods for bilingual-dictionary adaptation.
Thefirst one is a variant of the contex-tual-similarity-based method for extracting pairs oftranslation equivalents; it focuses on selecting cor-pus-relevant translation equivalents from amongthose provided by a bilingual dictionary.
This select-ing may be easier than finding new pairs of transla-tion equivalents.
The second one is a newly devisedmethod using the ratio of associated words that sug-gest each translation equivalent; it was inspired by aresearch on word-sense disambiguation using bilin-gual comparable corpora (Kaji and Morimoto 2002).The two methods were evaluated and compared byusing the EDR (Japan Electronic Dictionary Re-search Institute) bilingual dictionary together withWall Street Journal and Nihon Keizai Shimbun cor-pora.2 Method based on contextual similarityThis method is based on the assumption that aword in a language and its translation equivalent inanother language occur in similar contexts, albeittheir contexts are represented by words in their re-spective languages.
In the case of the present task(i.e., bilingual-dictionary adaptation), a bilingualdictionary provides a set of candidate translationequivalents for each target word1.
The contextualsimilarity of each of the candidate translationequivalents to the target word is thus evaluated withthe assistance of the bilingual dictionary, and a pre-determined number of translation equivalents areselected in descending order of contextual similarity.Note that it is difficult to preset a threshold for con-textual similarity since the distribution of contextualsimilarity values varies with target words.1 In this paper, ?target word?
is used to indicate the word forwhich translation equivalents are to be selected.A flow diagram of the proposed method is shownin Figure 1.
The essential issues regarding thismethod are described in the following.Word associations are extracted by setting athreshold for mutual information between words inthe same language.
The mutual information of a pairof words is defined in terms of their co-occurrencefrequency and respective occurrence frequencies(Church and Hanks 1990).
A medium-sized window,i.e., a window including a few-dozen words, is usedto count co-occurrence frequencies.
Only word asso-ciations consisting of content words are extracted.This is because function words neither have do-main-dependent translation equivalents nor representcontexts.Both a target word and each of its candidatetranslation equivalents are characterized by contextvectors.
A context vector consists of associatedwords weighted with mutual information.Similarity of a candidate translation equivalent toa target word is defined as the cosine coefficient be-tween the context vector characterizing the targetword and the translated context vector characterizingthe candidate translation equivalent as follows.
Un-der the assumption that target word x and candidatetranslation equivalent y are characterized byfirst-language context vector a(x) = (a1(x), a2(x), ?,am(x)) and second-language context vector b(y) =(b1(y), b2(y), ?, bn(y)), respectively, b(y) is translatedinto a first-language vector denoted as a'(y) = (a'1(y),a'2(y), ?, a'm(y)).
That is,m),1,2,()()( ,n,1,2, LL =?= = iybmaxy'a jjiji ?
,where ?i,j=1 if the j-th element of b(y) is a translationof the i-th element of a(x); otherwise, ?i,j=0.
Ele-ments of b(y) that cannot be translated into elementsof a'(y) constitute a residual second-language vector,denoted as b'(y) = (b'1(y), b'2(y), ?, b'n(y)).
That is,.jybyb'mijijj n),1,2,(otherwise00)()( 1, LLL =?????
== ?= ?The similarity of candidate translation equivalent yto target word x is then defined as))()(),(()( yyxcosy,xSim b'a'a += .Note that a'(y)+b'(y) is a concatenation of a'(y) andb'(y) since they have no elements in common.3 Method using the ratio of associatedwords3.1 OutlineThis method is based on the assumption that eachword associated with a target word suggests a spe-cific sense of the target word, in other words, spe-cific translation equivalents of the target word.
It isalso assumed that dominance of a translationequivalent in a domain correlates with how manyassociated words suggesting it occur in a corpus ofthe domain.
It is thus necessary to identify whichassociated words suggest which translation equiva-lents.
This can be done by using the sense-vs.-cluecorrelation algorithm that the author developed forunsupervised word-sense disambiguation (Kaji andMorimoto 2002).
The algorithm works with a set ofsenses of a target word, each of which is defined as aset of synonymous translation equivalents, and itresults in a correlation matrix of senses vs. clues (i.e.,associated words).
It is used here with a set of trans-lation equivalents instead of a set of senses, resultingin a correlation matrix of translation equivalents vs.associated words.The proposed method consists of the followingsteps (as shown in Figure 2).First, word associations are extracted from a cor-pus of each language.
The first step is the same asthat of the contextual-similarity-based method de-scribed in Section 2.Second, word associations are aligned translin-gually by consulting a bilingual dictionary, andpairwise correlation between translation equivalentsof a target word and its associated words is calcu-lated iteratively.
A detailed description of this step isgiven in the following subsection.Third, each associated word is assigned to thetranslation equivalent having the highest correlationwith it.
This procedure may be problematic, since anassociated word often suggests two or more transla-tion equivalents that represent the same sense.
How-ever, it is difficult to separate translation equivalentssuggested by an associated word from others.
Eachassociated word is therefore assigned to thetranslation equivalent it suggests most strongly.Finally, a translation equivalent is selected when1st-language corpusExtract word associationsExtract word associations1st-language word associations 2nd-language word associations2nd-language corpusConstruct context vector Construct context vectors Original bilingual dictionaryTranslate context vectorsTranslated context vectorsCalculate contextual similarity of candidate translation equivalents to target wordSelect N most-similar translation equivalentsAdapted bilingual dictionaryContext vectors characterizing candidate translation equivalents Context vector characterizing target wordFigure 1: Bilingual-dictionary adaptationbased on contextual similaritythe ratio of associated words assigned to it exceeds acertain threshold.
In addition, representative associ-ated words are selected for each selected translationequivalent.
A representativeness measure was de-vised under the assumption that representative asso-ciated words are near the centroid of a cluster con-sisting of associated words assigned to a translationequivalent.
The representative associated words helplexicographers validate the selected translationequivalents.3.2 Calculation of correlation between translationequivalents and associated wordsThe iterative algorithm described below has twomain features.
First, it overcomes the problem offailure in word-association alignment due to incom-pleteness of the bilingual dictionary and disparity intopical coverage between the corpora of the twolanguages.
Second, it overcomes the problem of am-biguity in word-association alignment.3.2.1 Alignment of word associationsFor a first-language word association (x,x?
(j))?where a target word is given as x and its j-thassociated word is given as x?
(j)?a set consisting ofsecond-language word associations alignable with it,denoted as Y(x, x?
(j)), is constructed.
That is,Y(x, x?
(j))= {(y, y?)
| (y, y?
)?R2, (x, y)?D, (x?
(j), y?
)?D},where R2 is the collection of word associations ex-tracted from a corpus of the second language, and Dis a bilingual dictionary to be adapted.Each first-language word association (x, x?
(j)) ischaracterized by a set consisting of accompanyingassociated words, denoted as Z(x, x?(j)).
An accom-panying associated word is a word that is associatedwith both words making up the word association inquestion.
That is,Z(x, x?
(j)) = {x?
| (x, x?
)?R1, (x?
(j), x?
)?R1},where R1 is the collection of word associations ex-tracted from a corpus of the first language.In addition, alignment of a first-language wordassociation (x, x?
(j)) with a second-language wordassociation (y, y?)
(?Y(x, x?
(j))) is characterized by aset consisting of translingually alignable accompa-nying associated words, denoted as W((x, x?
(j)), (y,y?)).
A translingually alignable accompanying asso-ciated word is a word that is an accompanying asso-ciated word of the first-language word associationmaking up the alignment in question and, at the sametime, is alignable with an accompanying associatedword of the second-language word association mak-ing up the alignment in question.
That is,W((x, x?
(j)), (y, y?
))= Z(x, x?
(j)) ?
{x?
| ?
y?
(?V(y, y?))
(x?, y?
)?D},where V(y, y?)
= {y?
| (y, y?
)?R2, (y?, y?
)?R2}.3.2.2 Iterative calculation of correlationThe correlation between the i-th translationequivalent of target word x, denoted as y(i), and thej-th associated word x?
(j) is defined as( ) ( ) ( )( ) ,jx',kyPLmaxjx',iyPLjx',xMIjx',iyCk)()()()()()()( ?=where MI(x, x?
(j)) is the mutual information betweenx and x?
(j), and PL(y(i), x?
(j)) is the plausibility fac-tor for y(i) given by x?(j).
The mutual informationbetween the target word and the associated word isthe base of the correlation between each translationequivalent of the target word and the associatedword; it is multiplied by the normalized plausibilityfactor.
The plausibility factor is defined as theweighted sum of two component plausibility factors.That is,( ) ( ) ( ),jx',iyPLjx',iyPLjx',iyPL )()(?
)()()()( 21 ?+=where ?
is a parameter adjusting the relative weightsof the component plausibility factors.The first component plausibility factor, PL1, is de-fined as the sum of correlations between the transla-tion equivalent and the accompanying associatedwords.
That is,( ) ( ).x",iyCjx',iyPLjx',xx"?
?=))(Z(1 )()()(This is based on the assumption that an associatedTarget wordExtract word associations1st-language corpusOriginal bilingual dictionary2nd-language corpusExtract word associations2nd-language  word associations 1st-language  word associationsCalculate correlation between translation equivalents and associated wordsAssign each associated word to the translation equivalent having the highest correlation with itBinary matrix of translation equivalents vs. associated wordsRepresentative associated wordsAligned word associationsCorrelation matrix of translation equivalents vs. associated wordsAlign word associationsCandidate translationequivalentsAdapted bilingual dictionarySelect representative associated wordsSelect translation equivalents to which more than a certain percentage of associated words are assignedFigure 2: Bilingual-dictionary adaptation usingthe ratio of associated wordsword usually correlates closely with the translationequivalent that correlates closely with a majority ofits accompanying associated words.The second component plausibility factor, PL2, isdefined as the maximum plausibility of alignmentinvolving the translation equivalent, where the plau-sibility of alignment of a first-language word asso-ciation with a second-language word association isdefined as the mutual information of the sec-ond-language word association multiplied by thesum of correlations between the translation equiva-lent and the translingually alignable accompanyingassociated words.
That is,( )( ) .x"iyCy'iyMImaxjx'iyPLy'iyjx'xWx"jx'xYy'iy????????
?= ???
))),(()),(,(())(,()),((2),()),(()(),(This is based on the assumption that correct align-ment of word associations is usually accompanied bymany associated words that are alignable with eachother as well as the assumption that alignment with astrong word association is preferable to alignmentwith a weak word association.The above definition of the correlations betweentranslation equivalents and associated words is re-cursive, so they can be calculated iteratively.
Initialvalues are set asC0(y(i), x?
(j)) = MI(x, x?
(j)).That is, the mutual information between the targetword and an associated word is used as the initialvalue for the correlations between all translationequivalents of the target word and the associatedword.It was proved experimentally that the algorithmworks well for a wide range of values of parameter ?and that the correlation values converge rapidly.
Pa-rameter ?
and the number of iterations were set tofive and six, respectively, in the experiments de-scribed in Section 4.4 Experiments4.1 Material and preparationThe experiment focused on nouns, whose appro-priate translations often vary with domains.
Awide-coverage bilingual noun dictionary was con-structed by collecting pairs of nouns from the EDREnglish-to-Japanese and Japanese-to-English dic-tionaries.
The resulting dictionary consists of633,000 pairs of 269,000 English nouns and 276,000Japanese nouns.An English corpus consisting of Wall Street Jour-nal articles (July 1994 to December 1995; 189MB)and a Japanese corpus consisting of Nihon KeizaiShimbun articles (December 1993 to November1994; 275MB) were used as the comparable corpora.English nouns occurring 10 or more times in theEnglish corpus were selected as the target words.The total number of selected target words was12,848.
For each target word, initial candidate trans-lation equivalents were selected from the bilingualdictionary in descending order of frequency in theJapanese corpus; the maximum number of candi-dates was set at 20, and the minimum frequency wasset at 10.
The average number of candidate transla-tion equivalents per target word was 3.3, and 1,251target words had 10 or more candidate translationequivalents.Extraction of word associations, which is the firststep common to the method based on contextualsimilarity (abbreviated as the CS method hereinafter)and the method using the ratio of associated words(abbreviated as the RAW method hereinafter), wasdone as follows.
Co-occurrence frequencies of nounpairs were counted by using a window of 13 words,excluding function words, and then noun pairs hav-ing mutual information larger than zero were ex-tracted.Table 1: Example translation equivalents selected bythe method based on contextual similarityTarget word[Freq.]
# Translation equivalent*) [Freq.]
Similarity1 ????
(administration organ) [137] 0.1272 ??
(reign) [32] 0.1193 ??
(direction of domestic affairs) [2366] 0.1164 ??
(political power) [2370] 0.111admini-stration[2027]5 ??
(operation) [453] 0.1111 ????
(election campaign) [71] 0.0672 ??
(competition) [2608] 0.0503 ??????
(aggressive activities) [561] 0.0494 ??
(movement) [947] 0.049campaign[1656]5 ????
(military activities) [89] 0.0401 ??
(management) [4810] 0.1162 ??
(enterprise) [8735] 0.0913 ??
(conduct) [1431] 0.0764 ??
(tactics) [528] 0.074operation[3469]5 ??
(function) [2721] 0.0741 ?????
(energy) [913] 0.1032 ?
(force) [6276] 0.1013 ??
(majority) [1036] 0.1014 ??
(electric power) [1208] 0.079power[2826]5 ??
(ability) [1254] 0.0741 ?
(husk) [135] 0.0822 ?
(ball) [137] 0.0703 ??
(cannonball) [32] 0.0704 ?
(ball) [1370] 0.062shell[102]6 ???
(case) [4851] 0.0601 ?
(voice) [13536] 0.1032 ??
(target) [4676] 0.0963 ??
(business) [7163] 0.0874 ??
(indication) [215] 0.087sign[4064]5 ???
(mark) [297] 0.084*) English translations other than target words are given in parentheses.4.2 Experimental resultsResults of the CS and RAW methods for six targetwords are listed in Tables 1 and 2, respectively.
Table1 lists the top-five translation equivalents in de-scending order of contextual similarity.
Table 2 liststranslation equivalents with a ratio of associatedwords larger than 4% along with their top-four rep-resentative associated words.
In these tables, the oc-currence frequencies in the test corpora are appendedto both the target words and the translation equiva-lents.
These indicate the weak comparability be-tween the Wall Street Journal and Nihon KeizaiShimbun corpora.
Moreover, it is clear that neitherthe CS method nor the RAW method relies on theoccurrence frequencies of words.Tables 1 and 2 clearly show that the two methodsproduce significantly different lists of translationequivalents.
It is difficult to judge the appropriate-ness of the results of the CS method without exam-ining the comparable corpora.
However, it seemsthat inappropriate translation equivalents were oftenranked high by the CS method.
In contrast, referringto the representative associated words enables theresults of the RAW method to be judged as appropri-ate or inappropriate.
More than 90% of the selectedtranslation equivalents were judged as definitely ap-propriate.Table 2 also includes the orders of translationequivalents determined by a conventional bilingualdictionary (remarks column).
They are quite differ-ent from the orders determined by the RAW method.This shows the necessity and effectiveness of rank-ing translation equivalents according to relevancy toa domain.Processing times were measured by separatingboth the CS and RAW methods into two parts.
Theprocessing time of the first part shared by the twomethods, i.e., extracting word associations fromcorpora, is roughly proportional to the corpus size.For example, it took 2.80 hours on a Windows PC(CPU clock: 2.40 GHz; memory: 1 GB) to extractword associations from the 275 MB Japanese corpus.The second part, i.e., selecting translation equiva-lents for target words, is specific to each method, andthe processing time of it is proportional to the num-ber of target words.
It took 11.5 minutes and 2.40hours on another Windows PC (CPU clock: 2.40GHz; memory: 512 MB) for the CS and RAWmethods, respectively, to process the 12,848 targetwords.
It was thus proved that both the CS and RAWmethods are computationally feasible.4.3 Quantitative evaluation using pseudo targetwords4.3.1 Evaluation methodA method for bilingual-dictionary adaptation us-ing comparable corpora should be evaluated by us-Table 2: Example translation equivalents selected by the method using the ratio of associated wordsTarget word[Freq.]
# Translation equivalent*) [Freq.]
Ratio Representative associated words Remarks **)1 ??
(cabinet) [1067] 0.419 House, Clinton, White House, Republican 3a2 ??
(political power) [2370] 0.236 U.S. official, Haiti, Haitian, Clinton admini-stration3a3 ??
(operation) [453] 0.147 GATT, fast-track, trade pact, Trade 4aadministration[2027]4 ??
(control) [84] 0.058 China, U.S., import, Japan -1 ????
(election campaign) [71] 0.612 Republican, candidate, GOP, Democrat 2a campaign[1656] 2 ??????
(aggressive activities) [561] 0.371 ad, advertise, brand, advertising2a1 ??
(management) [4810] 0.788 Stock Exchange, last year, profit, loss 2b operation[3469] 2 ??
(enterprise) [8735] 0.144 quarter, net, income, plant -1 ??
(electric power) [1208] 0.434 electricity, power plant, utility, megawatt 8b2 ??
(influence) [826] 0.425 military, leader, President, Haiti 3 power [2826] 3 ??
(authority) [909] 0.062 reform, law, Ukraine, amendment 5a1 ??
(cannonball) [32] 0.560 Serb, U.N., Sarajevo, NATO 4a2 ?
(shellfish) [100] 0.168 crab, fish, hermit crab, Mr. Soifer 1a3 ?
(ball) [137] 0.112 rupture, bacterium, implant, brain -shell[102]4 ??
(external appearance) [267] 0.064 tape, camera, video, building 3a1 ??
(indication) [215] 0.568 inflation, interest rate, rate, economy 4a2 ??
(signboard) [566] 0.099 tourist, billboard,  airport, exit 3b3 ??
(target) [4676] 0.086 accord, agreement, pact, treaty -4 ??
(indication) [2396] 0.086 last year, month, demand, order -sign[4064]5 ??
(signal) [231] 0.062 driver, accident, highway, motorist 2a*) English translations other than target words are given in parentheses.
**) This column shows the orders of translation equivalents determined by a conventional dictionary ?Kenkyusha?s New Collegiate English-JapaneseDictionary, 5th edition.?
For example, ?3a?
indicates that a translation equivalent belongs to the subgroup ?a?
in the third group of translations.
A hy-phen indicates that a translation equivalent is not contained in the dictionary.ing recall and precision measures defined as,TTSecisionPrandSTScallRe ?=?=where S is a set consisting of pairs of translationequivalents contained in the test comparable corpora,and T is a set consisting of pairs of translationequivalents selected by the method.
To calculatethese measures, it is necessary to know all pairs oftranslation equivalents contained in the test corpora.This is almost impossible in the case that the testcorpora are large.To avoid this difficulty, an automated evaluationscheme using pseudo target words was devised.
Apseudo word is formed by three real words, and ithas three distinctive pseudo senses corresponding tothe three constituent words.
Translation equivalentsof a constituent word are regarded as candidatetranslation equivalents of the pseudo word that rep-resent the pseudo sense corresponding to the con-stituent word.
For example, a pseudo word ?ac-tion/address/application?
has three pseudo sensescorresponding to ?action,?
?address,?
and ?applica-tion.?
It has candidate translation equivalents such as???<SOSHOU>?
and ???<KETSUGI>?
originatingfrom ?action,?
???
<ENZETSU>?
and ???<SEIGAN>?
originating from ?address,?
and ???<OUYOU>?
and ???<OUBO>?
originating from?application.?
Furthermore, pseudo word associa-tions are produced by combining a pseudo word witheach of the associated words of the first two con-stituent words.
It is thus assumed that first twopseudo senses occur in the corpora but the third onedoes not.
For example, the pseudo word ?ac-tion/address/application?
has associated wordsincluding ?court?
and ?vote,?
which are associatedwith ?action,?
as well as ?President?
and ?legisla-tion,?
which are associated with ?address.
?Using the pseudo word associations, a bilin-gual-dictionary-adaptation method selects translationequivalents for the pseudo target word.
On the onehand, when at least one of the translation equivalentsoriginating from the first (second) constituent wordis selected, it means that the first (second) pseudosense is successfully selected.
For example, when?
??
<SOSHOU>?
is selected as a translationequivalent for the pseudo target word ?ac-tion/address/application,?
it means that the pseudosense corresponding to ?action?
is successfully se-lected.
On the other hand, when at least one oftranslation equivalents originating from the thirdconstituent word is selected, it means that the thirdpseudo sense is erroneously selected.
For example,when ???<OUYOU>?
is selected as a translationequivalent for the pseudo target word ?ac-tion/address/application,?
it means that the pseudosense corresponding to ?application?
is erroneouslyselected.
The method is thus evaluated by recall andprecision of selecting pseudo senses.
That is,,'T'T'SecisionPrand'S'T'ScallRe ?=?=where S?
is a set consisting of pseudo senses corre-sponding to the first two constituent words, and T?
isa set consisting of pseudo senses relevant to transla-tion equivalents selected by the method.4.3.2 Evaluation resultsA total of 1,000 pseudo target words wereformed by using randomly selected words that oc-cur more than 100 times in the Wall Street Journalcorpus.
Using these pseudo target words, both theCS and RAW methods were evaluated.
As for theCS method, the recall and precision of selectingpseudo senses were calculated in the case that Nmost-similar translation equivalents are selected(N=2, 3,?).
As for the RAW method, the recall andprecision of selecting pseudo senses were calcu-lated in the case that the threshold for the ratio ofassociated words is set from 20% down to 1% in1% intervals.Recall vs. precision curves for the two methodsare shown in Figure 3.
These curves clearly showthat the RAW method outperforms the CS method.The RAW method maximizes the F-measure, i.e.,harmonic means of recall and precision, when thethreshold for the ratio of associated words is set at4%; the recall, precision, and F-measure are 92%,80%, and 86%, respectively.
In contrast, the CSmethod maximizes the F-measure when N is set atnine; the recall, precision, and F-measure are 96%,72%, and 82%, respectively.It should be mentioned that the above evaluationwas done under strict conditions.
That is, two out ofthree pseudo senses of each pseudo target word wereassumed to occur in the corpus, while many real tar-get words have only one sense in a specific domain.Target words with only one sense occurring in acorpus are generally easier to cope with than thosewith multiple senses occurring in a corpus.
Accord-ingly, recall and precision for real target wordswould be higher than the above ones for the pseudotarget words.0.50.60.70.80.91.00.5 0.6 0.7 0.8 0.9 1.0RecallPrecisionCS RAWFigure 3: Recall and precision ofselecting pseudo senses5 DiscussionThe reasons for the superior performance of theRAW method to the CS method are discussed in thefollowing.?
The RAW method overcomes both the sparsenessof word-association data and the topical disparitybetween corpora of two languages.
This is due tothe smoothing effects of the iterative algorithm forcalculating correlation between translation equiva-lents and associated words; namely, associatedwords are correlated with translation equivalentseven if they fail to be aligned with their counter-part.
In contrast, the CS method is much affectedby the above-mentioned difficulties.
All low valuesof contextual similarity (see Table 1) support thisfact.?
The RAW method assumes that a target word hasmore than one sense, and, therefore, it is effectivefor polysemous target words.
In contrast, contex-tual similarity is ineffective for a target word withtwo or more senses occurring in a corpus.
Thecontext vector characterizing such a word is acomposite of context vectors characterizing re-spective senses; therefore, the context vector char-acterizing any candidate translation equivalentdoes not show very high similarity.?
The RAW method can select an appropriatenumber of translation equivalents for each targetword by setting a threshold for the ratio of associ-ated words.
In contrast, the CS method is forced toselect a fixed number of translation equivalents forall target words; it is difficult to predetermine athreshold for the contextual similarity, since therange of its values varies with target words (seeTable 1).Finally, from a practical point of view, advantagesof the RAW method are discussed in the following.?
The RAW method selects translation equivalentscontained in the comparable corpora of a domaintogether with evidence, i.e., representative associ-ated words that suggest the selected translationequivalents.
Accordingly, it allows lexicographersto check the appropriateness of selected translationequivalents efficiently.?
The ratio of associated words can be regarded asa rough approximation of a translation probability.Accordingly, a translation equivalent can be fixedfor a word, when the particular translation equiva-lent has an exceedingly large ratio of associatedwords.
A sophisticated procedure for word-sensedisambiguation or translation-word selection needsto be applied only to words whose two or moretranslation equivalents have significant ratios ofassociated words.6 ConclusionThe method using the ratio of associated wordswas proved to be effective, while the method basedon contextual similarity was not.
The former methodhas the following features that make it practical.
First,is uses weakly comparable corpora, which are avail-able in many domains.
Second, it selects translationequivalents together with representative associatedwords that suggest them, enabling the translationequivalents to be validated.
The method will be ap-plied to several domains, and its effect on the per-formance of application systems will be evaluated.7 AcknowledgmentsThis research was supported by the New Energyand Industrial Technology Development Organiza-tion of Japan (NEDO).ReferencesChurch, Kenneth W. and Patrick Hanks.
1990.
Wordassociation norms, mutual information, and lexicog-raphy.
Computational Linguistics, 16(1): 22-29.Dagan, Ido, Kenneth W. Church, and William A. Gale.1993.
Robust bilingual word alignment for machineaided translation.
In Proc.
Workshop on Very LargeCorpora, pages 1-8.Fung, Pascale.
1995.
A pattern matching method forfinding noun and proper noun translations fromnoisy parallel corpora.
In Proc.
33rd Annual Meetingof the ACL, pages 236-243.Fung, Pascale and Kathleen McKeown.
1997.
Findingterminology translations from non-parallel corpora.In Proc.
5th Annual Workshop on Very Large Cor-pora, pages 192-202.Fung, Pascale and Lo Yuen Yee.
1998.
An IR approachfor translating new words from nonparallel, compa-rable texts.
In Proc.
36th Annual Meeting of the ACL/ 17th COLING, pages 414-420.Gale, William A. and Kenneth W. Church.
1991.
Iden-tifying word correspondences in parallel texts.
InProc.
4th DARPA Speech and Natural LanguageWorkshop, pages 152-157.Kaji, Hiroyuki and Toshiko Aizono.
1996.
Extractingword correspondences from bilingual corpora basedon word co-occurrence information.
In Proc.
16thCOLING, pages 23-28.Kaji, Hiroyuki and Yasutsugu Morimoto.
2002.
Unsu-pervised word sense disambiguation using bilingualcomparable corpora.
In Proc.
19th COLING, pages411-417.Kitamura, Mihoko and Yuji Matsumoto.
1996.
Auto-matic extraction of word sequence correspondencesin parallel corpora, In Proc.
4th Workshop on VeryLarge Corpora, pages 79-87.Melamed, I. Dan.
1997.
A word-for-word model oftranslational equivalence.
In Proc.
35th AnnualMeeting of the ACL / 8th Conference of the EACL,pages 490-497.Rapp, Reinhard.
1999.
Automatic identification ofword translations from unrelated English and Ger-man corpora.
In Proc.
37th Annual Meeting of theACL, pages 320-322.Tanaka, Kumiko and Hideya Iwasaki.
1996.
Extractionof lexical translations from non-aligned corpora, InProc.
16th COLING, pages 580-585.
