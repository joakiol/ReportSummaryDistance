Proceedings of the ACL Student Research Workshop, pages 96?102,Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational LinguisticsQuestion Analysis for Polish Question AnsweringPiotr Przyby?aInstitute of Computer Science, Polish Academy of Sciences,ul.
Jana Kazimierza 5, 01-248 Warszawa, Poland,P.Przybyla@phd.ipipan.waw.plAbstractThis study is devoted to the problem ofquestion analysis for a Polish question an-swering system.
The goal of the questionanalysis is to determine its general struc-ture, type of an expected answer and cre-ate a search query for finding relevant doc-uments in a textual knowledge base.
Thepaper contains an overview of availablesolutions of these problems, description oftheir implementation and presents an eval-uation based on a set of 1137 questionsfrom a Polish quiz TV show.
The resultshelp to understand how an environmentof a Slavonic language affects the perfor-mance of methods created for English.1 IntroductionThe main motivation for building Question An-swering (QA) systems is that they relieve a userof a need to translate his problem to a machine-readable form.
To make it possible, we need toequip a computer system with an ability to under-stand requests in a natural language, find answersin a knowledge base and formulate them in the nat-ural language.
The aim of this paper is to deal withthe first of these steps, i.e.
question analysis mod-ule.
It accepts the question as an input and returnsa data structure containing relevant information,herein called question model.
It consists of twoelements: a question type and a search query.The question type classifies a question to oneof the categories based on its structure.
A gen-eral question type takes one of the following val-ues: verification (Czy Lee Oswald zabi?
JohnaKennedy?ego?, Eng.
Did Lee Oswald kill JohnKennedy?
), option choosing (Kt?ry z nich zabi?Johna Kennedy?ego: Lance Oswald czy Lee Os-wald?, Eng.
Which one killed John Kennedy:Lance Oswald or Lee Oswald?
), named entity(Kto zabi?
Johna Kennedy?ego?, Eng.
Who killedJohn Kennedy?
), unnamed entity (Czego uz?y?Lee Oswald, z?eby zabic?
Johna Kennedy?ego?,Eng.
What did Lee Oswald use to kill JohnKennedy?
), other name for a given named en-tity (Jakiego pseudonimu uz?ywa?
John Kennedy wtrakcie s?uz?by wojskowej?, Eng.
What nicknamedid John Kennedy use during his military service?
)and multiple entities (Kt?rzy prezydenci Stan?wZjednoczonych zostali zabici w trakcie kadencji?,Eng.
Which U.S. presidents were assassinated inoffice?).
There are many others possible, such asdefinition or explanation questions, but they re-quire specific techniques for answer finding andremain beyond the scope of this work.
For exam-ple, the Question Answering for Machine Read-ing Evaluation (QA4MRE) competition (Pe?as etal., 2012) included these complex questions (e.g.What caused X?, How did X happen?, Why did Xhappen?).
In case of named entity questions, itis also useful to find its named entity type, cor-responding to a type of an entity which could beprovided as an answer.
A list of possible options,suited to questions about general knowledge, isgiven in Table 1.
As some of the categories in-clude others (e.g.
CITY is a PLACE), the goal ofa classifier is to find the narrowest available.The need for a search query is motivated byperformance reasons.
A linguistic analysis ap-plied to a source text to find the expected answeris usually resource-consuming, so it cannot be per-formed on the whole corpus (in case of this exper-iment 839,269 articles).
To avoid it, we transformthe question into the search query, which is sub-sequently used in a search engine, incorporating afull-text index of the corpus.
As a result we get alist of documents, possibly related to the question.Although the query generation plays an auxiliaryrole, failure at this stage may lead both to too longprocessing times (in case of excessive number ofreturned documents) and lack of a final answer (in96Question type OccurrencesNAMED_ENTITY 657OPTION 28VERIFICATION 25MULTIPLE 28UNNAMED_ENTITY 377OTHER_NAME 22PLACE 33CONTINENT 4RIVER 11LAKE 9MOUNTAIN 4RANGE 2ISLAND 5ARCHIPELAGO 2SEA 2CELESTIAL_BODY 8COUNTRY 52STATE 7CITY 52NATIONALITY 12PERSON 260NAME 11SURNAME 10BAND 6DYNASTY 6ORGANISATION 20COMPANY 2EVENT 7TIME 2CENTURY 9YEAR 34PERIOD 1COUNT 31QUANTITY 6VEHICLE 10ANIMAL 1TITLE 38Table 1: The 6 general question types and the 31named entity types and numbers of their occur-rences in the test set.case of not returning a relevant document).2 Related workThe problem of determination of the general ques-tion type is not frequent in existing QA solutions,as most of the public evaluation tasks, such asthe TREC question answering track (Dang et al2007) either provide it explicitly or focus on oneselected type.
However, when it comes to namedentity type determination, a proper classificationis indispensable for finding an answer of a desiredtype.
Some of the interrogative pronouns, such asgdzie (Eng.
where) or kiedy (Eng.
when) uniquelydefine this type, so the most obvious approach usesa list of manually defined patterns.
For example,Lee et al(2005) base solely on such rules, butneed to have 1273 of them.
Unfortunately, somepronouns (i.e.
jaki, Eng.
what, and kt?ry, Eng.which) may refer to different types of entities.
Inquestions created with them, such as Kt?ry znanymalarz twierdzi?, z?e obcia??
sobie ucho?
(Eng.Which famous painter claimed to have cut hisear?)
the question focus (znany malarz, Eng.
fa-mous painter), following the pronoun, should beanalysed, as its type corresponds to a named en-tity type (a PERSON in this case).
Such approachis applied in a paper by Harabagiu et al(2001),where the Princeton WordNet (Fellbaum, 1998)serves as an ontology to determine foci types.
Fi-nally, one could use a machine learning (ML) ap-proach, treating the task as a classification prob-lem.
To do that, a set of features (such as occur-rences of words, beginning pronouns, etc.)
shouldbe defined and extracted from every question.
Liand Roth (2002) implemented this solution, usingas much as 200,000 features, and also evaluatedan influence of taking into account hierarchy ofclass labels.
C?eh and Ojster?ek (2009) used thisapproach in a Slovene QA system for closed do-main (students?
faculty-related questions) with aSVM (support vector machines) classifier.The presented problem of question classifica-tion for Polish question answering is studied in apaper by Przyby?a (2013).
The type determinationpart presented here bases on that solution, but in-cludes several improvements.To find relevant documents, existing QA solu-tions usually employ one of the widely availablegeneral-purpose search engines, such as Lucene.Words of the question are interpreted as keywordsand form a boolean query, where all the con-stituents are considered required.
This proceduresuffices only in case of a web-based QA, wherewe can rely on a high redundancy of the WWW,which makes finding a similar expression proba-ble enough.
Such an approach, using the Googlesearch engine is presented by Brill et al(2002).When working with smaller corpora, one needsto take into account different formulations of thedesired information.
Therefore, an initial queryis subject to some modifications.
First, some ofthe keywords may be dropped from the query;Moldovan et al(2000) present 8 different heuris-tics of selecting them, based on quotation marks,parts of speech, detected named entities and otherfeatures, whereas Katz et al(2003) drop terms inorder of increasing IDF.
C?eh and Ojster?ek (2009)start term removal from the end of the sentence.Apart from simplifying the query, its expansion is97also possible.
For example, Hovy et al(2000) addsynonyms for each keyword, extracted from Word-Net while Katz et al(2003) introduce their inflec-tional and derivational morphological forms.3 Question analysisFor the purpose of building an open-domaincorpus-based Polish question answering system, aquestion analysis module, based on some of thesolutions presented above, has been implemented.The module accepts a single question in Polishand outputs a data structure, called a questionmodel.
It includes a general question type, a setof named entity types (if the general type equalsNAMED_ENTITY) and a Lucene search query.
Aset of named entity types, instead of a single one,is possible as some of the question constructionsare ambiguous, e.g.
a Kto?
(Eng.
Who?)
ques-tion may be answered by a PERSON, COUNTRY,BAND, etc.3.1 Question type classificationFor the question type classification all the tech-niques presented above are implemented.
Pat-tern matching stage bases on a list of 176 regu-lar expressions and sets of corresponding questiontypes.
If any of the expressions matches the ques-tion, its corresponding set of types may be imme-diately returned at this stage.
These expressionscover only the most obvious cases and have beencreated using general linguistic knowledge.
Thelength of the list arises from some of the featuresof Polish, typical for Slavonic languages, i.e.
rel-atively free word order and rich nominal inflec-tion (Przepi?rkowski, 2007).
For example one En-glish pattern Whose .
.
.
?
corresponds to 11 Polishpatterns (Czyj .
.
.
?, Czyjego .
.
.
?, Czyjemu .
.
.
?,Czyim .
.
.
?, Czyja .
.
.
?,Czyjej .
.
.
?, Czyja?
.
.
.
?,Czyje .
.
.
?, Czyi .
.
.
?, Czyich .
.
.
?, Czyimi .
.
.
?
).However, in case of ambiguous interrogativepronouns, such as jaki (Eng.
what) or kt?ry(Eng.
which), a further analysis gets necessaryto determine a question focus type.
The ques-tion is annotated using the morphological anal-yser Morfeusz (Wolin?ski, 2006), the tagger PAN-TERA (Acedan?ski, 2010) and the shallow parserSpejd (Przepi?rkowski, 2008).
The first nomi-nal group after the pronoun is assumed to be aquestion focus.
The Polish WordNet databaseplWordNet (Maziarz et al 2012) is used to findits corresponding lexeme.
If nothing is found,the procedure repeats with the current group?ssemantic head until a single segment remains.Failure at that stage results in returning an UN-NAMED_ENTITY label, whereas success leadsus to a synset in WordNet.
Then, we checkwhether its direct and indirect parents (i.e.
synsetsconnected via hypernymy relations) include oneof the predefined synsets, corresponding to theavailable named entity types.
The whole proce-dure is outlined in Figure 1.
The error analysisof this procedure performed in (Przyby?a, 2013)shows a high number of errors caused by a lackof a word sense disambiguation.
A lexeme maybe connected to many synsets, each correspond-ing to a specific word sense and having a differ-ent parent list.
Among the possible ways to com-bine them are: intersection (corresponding to us-ing only the parents common for all word senses),union (the parents of any word sense), voting (theparents common for the majority of word senses)and selecting only the first word sense (which usu-ally is the most common in the language).
Theexperiments have shown a better precision of clas-sification using the first word sense (84.35%) thanother techniques (intersection - 72.00%, union -80.95%, voting - 79.07%).
Experimental detailsare provided in the next section.As an alternative, a machine learning approachhas been implemented.
After annotation using thesame tools, we extract the features as a set of rootforms appearing in the question.
Only the lem-mas appearing in at least 3 sentences are used forfurther processing.
In this way, each sentence isdescribed with a set of boolean features (420 forthe evaluation set described in next section), de-noting the appearance of a particular root form.Additionally, morphological interpretations of thefirst five words in the question are also extractedas features.
Two classifiers, implemented in the Rstatistical environment, were used: a decision tree(for human-readable results) and a random forest(for high accuracy).3.2 Query formationThe basic procedure for creating a query treatseach segment from the question (apart from thewords included in a matched regular expression)as a keyword of an OR boolean query.
No termweighting or stop-words removal is implementedas Lucene uses TF/IDF statistic, which penalizesomnipresent tokens.
However, several other im-98Which russian  submarine         sank       in 2000        with its whole crew?Kt?ra  rosyjska ??d?
podwodna zaton???
w 2000 roku wraz z  ca??
za?og?
?first nominal group(rosyjska (??d?
podwodna))semantic head(??d?
podwodna)No synsetWordNet search{??d?
podwodna 1}WordNet searchhypernymInterrogativepronoun Questionfocus{statek podwodny 1}hypernymsubmersible ship{statek 1}hypernymship{?rodek lokomocji 1, ?rodek transportu 1}vehicleNAMED_ENTITYVEHICLEFigure 1: Outline of the disambiguation procedure, used to determine named entity type in case ofambiguous interrogative pronouns (see explanation in text).provements are used.
First, we start with a restric-tive AND query and fall back into OR only in caseit provides no results.
A question focus removal(applied by Moldovan et al(2000)) requires spe-cial attention.
For example, let us consider againthe question Kt?ry znany malarz twierdzi?, z?e ob-cia??
sobie ucho?.
The words of the question fo-cus znany malarz are not absolutely necessary ina source document, but their appearance may bea helpful clue.
The query could also be expandedby replacing each keyword by a nested OR query,containing synonyms of the keyword, extractedfrom plWordNet.
Both the focus removal and syn-onym expansion have been implemented as op-tions of the presented query formation mechanism.Finally, one needs to remember about animportant feature of Polish, typical for aSlavonic language, namely rich nominal inflection(Przepi?rkowski, 2007).
It means that the ortho-graphic forms of nouns change as they appear indifferent roles in a sentence.
We could either ig-nore this fact and look for exact matches betweenwords in the question and a document or allowsome modifications.
These could be done by stem-ming (available for Polish in Lucene, see the de-scription in (Galambos, 2001)), fuzzy queries (al-lowing a difference between the keyword and adocument word restricted by a specified Leven-shtein distance) or a full morphological analysisand tagging of the source corpus and the query.
Allthe enumerated possibilities are evaluated in thisstudy, apart from the last one, requiring a sizeableamount of computing resources.
This problem isless acute in case of English; most authors (e.g.Hovy et al(2000)) use simple (such as Porter?s)stemmers or do not address the problem at all.4 EvaluationFor the purpose of evaluation, a set of 1137 ques-tions from a Polish quiz TV show "Jeden z dziesie?-ciu", published in (Karzewski, 1997), has beenmanually reviewed and updated.
A general ques-tion type and a named entity type has been as-signed to each of the questions.
Table 1 presentsthe number of question types occurrences in thetest set.
As a source corpus, a textual version of thePolish Wikipedia has been used.
To evaluate querygeneration an article name has been assigned tothose questions (1057), for which a single articlein Wikipedia containing an answer exists.Outputs of type classifiers have been gathered99Classifier Classified Precision Overallpattern matching 36.15% 95.37% 34.48%WordNet-aided 98.33% 84.35% 82.94%decision tree 100% 67.02% 67.02%random forest 100% 72.91% 72.91%Table 2: Accuracy of the four question type classi-fiers: numbers of questions classified, percentagesof correct answers and products of these two.and compared to the expected ones.
The machinelearning classifiers have been evaluated using 100-fold cross-validation1.Four of the presented improvements of querygeneration tested here include: basic OR query,AND query with fallback to OR, focus segmentsremoval and expansion with synonyms.
For eachof those, three types of segment matching strate-gies have been applied: exact, stemming-basedand fuzzy.
The recorded results include recall(percentage of result lists including the desired ar-ticle among the first 100) and average position ofthe article in the list.5 ResultsThe result of evaluation of classifiers is presentedin Table 2.
The pattern matching stage behavesas expected: accepts only a small part of ques-tions, but yields a high precision.
The WordNet-aided focus analysis is able to handle almost allquestions with an acceptable precision.
Unfortu-nately, the accuracy of ML classifiers is not sat-isfactory, which could be easily explained usingTable 1: there are many categories represented byvery few cases.
An expansion of training set ordropping the least frequent categories (dependingon a particular application) is necessary for betterclassification.Results of considered query generation tech-niques are shown in Table 3.
It turns out that thebasic technique generally yields the best result.Starting with an AND query and using OR onlyin case of a failure leads to an improvement of theexpected article ranking position but the recall ra-tio drops significantly, which means that quite of-ten the results of a restrictive query do not includethe relevant article.
The removal of the questionfocus from the list of keywords also has a nega-tive impact on performance.
The most surprising1I.e.
the whole test set has been divided into 100 nearlyequal subsets and each of them has been classified using theclassifier trained on the remaining 99 subsets.XXXXXXXXQueryMatch Exact Stemming Fuzzybasic 69.97% 80.08% 82.19%OR query 14.32 12.90 12.36priority for 57.94% 57.07% 34.84%AND query 11.36 8.80 7.07with focus 62.75% 71.99% 73.34%segments removed 14.65 14.00 12.84with synonyms 47.06% 65.64% 58.71%21.42 15.47 16.00Table 3: Results of the four considered query gen-eration techniques, each with the three types ofmatching strategy.
For each combination a recall(measured by the presence of a given source docu-ment in the first 100 returned) and an average po-sition on the ranked list is given.results are those of expanding a query with syn-onyms - the number of matching articles growsabruptly and Lucene ranking mechanism does notlead to satisfying selection of the best 100.
Oneneeds to remember that only one article has beenselected for each test question, whereas probablythere are many relevant Wikipedia entries in mostcases.
Unfortunately, finding all of them manuallywould require a massive amount of time.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.86567697173757779818385Relative Absolute Fixed prefixQuery fuzzinessRecall(%)Figure 2: Impact of the fuzziness of queries onthe recall using three types of fuzzy queries.
Toshow the relative and absolute fuzziness on oneplot, a word-length of 10 letters is assumed.
See adescription in text.We can also notice a questionable impact of thestemming.
As expected, taking into account in-flection is necessary (cf.
results of exact match-ing), but fuzzy queries provide more accurate re-100sults, although they use no linguistic knowledge.As the fuzzy queries yield the best results, anadditional experiment becomes necessary to findan optimal fuzziness, i.e.
a maximal Levenshteindistance between the matched words.
This param-eter needs tuning for particular language of im-plementation (in this case Polish) as it reflects amutability of its words, caused by inflection andderivation.
Three strategies for specifying the dis-tance have been used: relative (with distance be-ing a fraction of a keyword?s length), absolute (thesame distance for all keywords) and with prefix(same as absolute, but with changes limited to theend of a keyword; with fixed prefix).
In Figure2 the results are shown - it seems that allowing 3changes at the end of the keyword is enough.
Thisoption reflects the Polish inflection schemes and isalso very fast thanks to the fixedness of the prefix.6 ConclusionIn this paper a set of techniques used to build aquestion model has been presented.
They havebeen implemented as a question analysis modulefor the Polish question answering task.
Several ex-periments using Polish questions and knowledgebase have been performed to evaluate their per-formance in the environment of the Slavonic lan-guage.
They have led to the following conclu-sions: firstly, the best technique to find a correctquestion type is to combine pattern matching withthe WordNet-aided focus analysis.
Secondly, itdoes not suffice to process the first 100 article, re-turned by the search engine using the default rank-ing procedure, as they may not contain desiredinformation.
Thirdly, the stemmer of Polish pro-vided by the Lucene is not reliable enough - prob-ably it would be best to include a full morpholog-ical analysis and tagging process in the documentindexing process.This study is part of an effort to build an open-domain corpus-based question answering systemfor Polish.
The obvious next step is to create a sen-tence similarity measure to select the best answerin the source document.
There exist a variety oftechniques for that purpose, but their performancein case of Polish needs to be carefully examined.AcknowledgementsCritical reading of the manuscript by AgnieszkaMykowiecka is gratefully acknowledged.
Studywas supported by research fellowship within "In-formation technologies: research and their in-terdisciplinary applications" agreement numberPOKL.04.01.01-00-051/10-00.ReferencesSzymon Acedan?ski.
2010.
A morphosyntactic BrillTagger for inflectional languages.
In Proceedingsof the 7th international conference on Advances inNatural Language Processing (IceTAL?10 ), pages3?14.Eric Brill, Susan Dumais, and Michele Banko.
2002.An analysis of the AskMSR question-answering sys-tem.
In Proceedings of the ACL-02 conference onEmpirical methods in natural language processing- EMNLP ?02, volume 10, pages 257?264, Morris-town, NJ, USA, July.
Association for ComputationalLinguistics.Hoa Trang Dang, Diane Kelly, and Jimmy Lin.
2007.Overview of the TREC 2007 Question Answeringtrack.
In Proceedings of The Sixteenth Text RE-trieval Conference, TREC 2007.Christiane Fellbaum.
1998.
WordNet: An ElectronicLexical Database.
The MIT Press.Leo Galambos.
2001.
Lemmatizer for DocumentInformation Retrieval Systems in JAVA.
In Pro-ceedings of the 28th Conference on Current Trendsin Theory and Practice of Informatics (SOFSEM2001), pages 243?252.Sanda Harabagiu, Dan Moldovan, Marius Pasca, RadaMihalcea, Mihai Surdeanu, Razvan Bunescu, Rox-ana G?rju, Vasile Rus, and Paul Morarescu.
2001.The role of lexico-semantic feedback in open-domain textual question-answering.
In Proceedingsof the 39th Annual Meeting on Association for Com-putational Linguistics - ACL ?01, pages 282?289.Eduard Hovy, Laurie Gerber, Ulf Hermjakob, MichaelJunk, and Chin-Yew Lin.
2000.
Question Answer-ing in Webclopedia.
In Proceedings of The NinthText REtrieval Conference (TREC 2000).Marek Karzewski.
1997.
Jeden z dziesie?ciu - pytania iodpowiedzi.
Muza SA.Boris Katz, Jimmy Lin, Daniel Loreto, Wesley Hilde-brandt, Matthew Bilotti, Sue Felshin, Aaron Fernan-des, Gregory Marton, and Federico Mora.
2003.Integrating Web-based and corpus-based techniquesfor question answering.
In Proceedings of theTwelfth Text REtrieval Conference (TREC 2003).Changki Lee, Ji-Hyun Wang, Hyeon-Jin Kim, andMyung-Gil Jang.
2005.
Extracting Template forKnowledge-based Question-Answering Using Con-ditional Random Fields.
In Proceedings of the28th Annual International ACM SIGIR Workshop onMFIR, pages 428?434.101Xin Li and Dan Roth.
2002.
Learning Question Classi-fiers.
In Proceedings of the 19th International Con-ference on Computational Linguistics (COLING-2002), volume 1 of COLING ?02.Marek Maziarz, Maciej Piasecki, and Stanis?aw Sz-pakowicz.
2012.
Approaching plWordNet 2.0.
InProceedings of the 6th Global Wordnet Conference.Dan Moldovan, Sanda Harabagiu, Marius Pas?ca, RadaMihalcea, Roxana G?rju, Richard Goodrum, andVasile Rus.
2000.
The structure and performanceof an open-domain question answering system.
InProceedings of the 38th Annual Meeting on Associa-tion for Computational Linguistics - ACL ?00, pages563?570, Morristown, NJ, USA, October.
Associa-tion for Computational Linguistics.Anselmo Pe?as, Eduard H. Hovy, Pamela Forner, ?l-varo Rodrigo, Richard F. E. Sutcliffe, CarolineSporleder, Corina Forascu, Yassine Benajiba, andPetya Osenova.
2012.
QA4MRE: Question An-swering for Machine Reading Evaluation at CLEF2012.
In CLEF 2012 Evaluation Labs and Work-shop Online Working Notes.Adam Przepi?rkowski.
2007.
Slavonic informationextraction and partial parsing.
In Proceedings of theWorkshop on Balto-Slavonic Natural Language Pro-cessing Information Extraction and Enabling Tech-nologies - ACL ?07.Adam Przepi?rkowski.
2008.
Powierzchnioweprzetwarzanie je?zyka polskiego.
Akademicka Ofi-cyna Wydawnicza EXIT, Warszawa.Piotr Przyby?a.
2013.
Question classification for Pol-ish question answering.
In Proceedings of the 20thInternational Conference of Language Processingand Intelligent Information Systems (LP&IIS 2013).Ines C?eh and Milan Ojster?ek.
2009.
Developing aquestion answering system for the slovene language.WSEAS Transactions on Information Science andApplications, 6(9):1533?1543.Marcin Wolin?ski.
2006.
Morfeusz ?
a PracticalTool for the Morphological Analysis of Polish.
InMieczys?aw K?opotek, S?awomir Wierzchon?, andKrzysztof Trojanowski, editors, Intelligent Informa-tion Processing and Web Mining, pages 511?520.102
