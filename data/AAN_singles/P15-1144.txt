Proceedings of the 53rd Annual Meeting of the Association for Computational Linguisticsand the 7th International Joint Conference on Natural Language Processing, pages 1491?1500,Beijing, China, July 26-31, 2015.c?2015 Association for Computational LinguisticsSparse Overcomplete Word Vector RepresentationsManaal Faruqui Yulia Tsvetkov Dani Yogatama Chris Dyer Noah A. SmithLanguage Technologies InstituteCarnegie Mellon UniversityPittsburgh, PA, 15213, USA{mfaruqui,ytsvetko,dyogatama,cdyer,nasmith}@cs.cmu.eduAbstractCurrent distributed representations ofwords show little resemblance to theo-ries of lexical semantics.
The formerare dense and uninterpretable, the lat-ter largely based on familiar, discreteclasses (e.g., supersenses) and relations(e.g., synonymy and hypernymy).
We pro-pose methods that transform word vec-tors into sparse (and optionally binary)vectors.
The resulting representations aremore similar to the interpretable featurestypically used in NLP, though they are dis-covered automatically from raw corpora.Because the vectors are highly sparse, theyare computationally easy to work with.Most importantly, we find that they out-perform the original vectors on benchmarktasks.1 IntroductionDistributed representations of words have beenshown to benefit NLP tasks like parsing (Lazari-dou et al, 2013; Bansal et al, 2014), named en-tity recognition (Guo et al, 2014), and sentimentanalysis (Socher et al, 2013).
The attraction ofword vectors is that they can be derived directlyfrom raw, unannotated corpora.
Intrinsic evalua-tions on various tasks are guiding methods towarddiscovery of a representation that captures manyfacts about lexical semantics (Turney, 2001; Tur-ney and Pantel, 2010).Yet word vectors do not look anything like therepresentations described in most lexical seman-tic theories, which focus on identifying classes ofwords (Levin, 1993; Baker et al, 1998; Schuler,2005) and relationships among word meanings(Miller, 1995).
Though expensive to construct,conceptualizing word meanings symbolically isimportant for theoretical understanding and alsowhen we incorporate lexical semantics into com-putational models where interpretability is de-sired.
On the surface, discrete theories seem in-commensurate with the distributed approach, aproblem now receiving much attention in compu-tational linguistics (Lewis and Steedman, 2013;Kiela and Clark, 2013; Vecchi et al, 2013; Grefen-stette, 2013; Lewis and Steedman, 2014; Papernoet al, 2014).Our contribution to this discussion is a new,principled sparse coding method that transformsany distributed representation of words into sparsevectors, which can then be transformed into binaryvectors (?2).
Unlike recent approaches of incorpo-rating semantics in distributional word vectors (Yuand Dredze, 2014; Xu et al, 2014; Faruqui et al,2015), the method does not rely on any externalinformation source.
The transformation results inlonger, sparser vectors, sometimes called an ?over-complete?
representation (Olshausen and Field,1997).
Sparse, overcomplete representations havebeen motivated in other domains as a way to in-crease separability and interpretability, with eachinstance (here, a word) having a small numberof active dimensions (Olshausen and Field, 1997;Lewicki and Sejnowski, 2000), and to increasestability in the presence of noise (Donoho et al,2006).Our work builds on recent explorations of spar-sity as a useful form of inductive bias in NLP andmachine learning more broadly (Kazama and Tsu-jii, 2003; Goodman, 2004; Friedman et al, 2008;Glorot et al, 2011; Yogatama and Smith, 2014,inter alia).
Introducing sparsity in word vector di-mensions has been shown to improve dimensioninterpretability (Murphy et al, 2012; Fyshe et al,2014) and usability of word vectors as features indownstream tasks (Guo et al, 2014).
The wordvectors we produce are more than 90% sparse; wealso consider binarizing transformations that bringthem closer to the categories and relations of lex-1491ical semantic theories.
Using a number of state-of-the-art word vectors as input, we find consis-tent benefits of our method on a suite of standardbenchmark evaluation tasks (?3).
We also evalu-ate our word vectors in a word intrusion experi-ment with humans (Chang et al, 2009) and findthat our sparse vectors are more interpretable thanthe original vectors (?4).We anticipate that sparse, binary vectors canplay an important role as features in statisticalNLP models, which still rely predominantly ondiscrete, sparse features whose interpretability en-ables error analysis and continued development.We have made an implementation of our methodpublicly available.12 Sparse Overcomplete Word VectorsWe consider methods for transforming dense wordvectors to sparse, binary overcomplete word vec-tors.
Fig.
1 shows two approaches.
The one on thetop, method A, converts dense vectors to sparseovercomplete vectors (?2.1).
The one beneath,method B, converts dense vectors to sparse and bi-nary overcomplete vectors (?2.2 and ?2.4).Let V be the vocabulary size.
In the following,X ?
RL?Vis the matrix constructed by stack-ing V non-sparse ?input?
word vectors of lengthL (produced by an arbitrary word vector estima-tor).
We will refer to these as initializing vectors.A ?
RK?Vcontains V sparse overcomplete wordvectors of length K. ?Overcomplete?
representa-tion learning implies that K > L.2.1 Sparse CodingIn sparse coding (Lee et al, 2006), the goal is torepresent each input vector xias a sparse linearcombination of basis vectors, ai.
Our experimentsconsider four initializing methods for these vec-tors, discussed in Appendix A.
Given X, we seekto solvearg minD,A?X?DA?22+ ??
(A) + ?
?D?22, (1)where D ?
RL?Kis the dictionary of basis vec-tors.
?
is a regularization hyperparameter, and ?
isthe regularizer.
Here, we use the squared loss forthe reconstruction error, but other loss functionscould also be used (Lee et al, 2009).
To obtainsparse word representations we will impose an `11https://github.com/mfaruqui/sparse-codingpenalty on A. Eq.
1 can be broken down into lossfor each word vector which can be optimized sep-arately in parallel (?2.3):arg minD,AV?i=1?xi?Dai?22+??ai?1+?
?D?22(2)where midenotes the ith column vector of matrixM.
Note that this problem is not convex.
We referto this approach as method A.2.2 Sparse Nonnegative VectorsNonnegativity in the feature space has often beenshown to correspond to interpretability (Lee andSeung, 1999; Cichocki et al, 2009; Murphy et al,2012; Fyshe et al, 2014; Fyshe et al, 2015).
Toobtain nonnegative sparse word vectors, we use avariation of the nonnegative sparse coding method(Hoyer, 2002).
Nonnegative sparse coding furtherconstrains the problem in Eq.
2 so that D and aiare nonnegative.
Here, we apply this constraintonly to the representation vectors {ai}.
Thus, thenew objective for nonnegative sparse vectors be-comes:arg minD?RL?K?0,A?RK?V?0V?i=1?xi?Dai?22+??ai?1+?
?D?22(3)This problem will play a role in our second ap-proach, method B, to which we will return shortly.This nonnegativity constraint can be easily incor-porated during optimization, as explained next.2.3 OptimizationWe use online adaptive gradient descent (Ada-Grad; Duchi et al, 2010) for solving the optimiza-tion problems in Eqs.
2?3 by updating A and D.In order to speed up training we use asynchronousupdates to the parameters of the model in parallelfor every word vector (Duchi et al, 2012; Heigoldet al, 2014).However, directly applying stochastic subgradi-ent descent to an `1-regularized objective fails toproduce sparse solutions in bounded time, whichhas motivated several specialized algorithms thattarget such objectives.
We use the AdaGrad vari-ant of one such learning algorithm, the regular-ized dual averaging algorithm (Xiao, 2009), whichkeeps track of the online average gradient at timet: g?t=1t?tt?=1gt?Here, the subgradients do notinclude terms for the regularizer; they are deriva-tives of the unregularized objective (?
= 0, ?
= 0)1492XLVKxVD A KKxVDVKBSparse overcomplete vectorsSparse, binary overcomplete vectorsProjectionSparse codingNon-negative sparse codingInitial dense vectorsFigure 1: Methods for obtaining sparse overcomplete vectors (top, method A, ?2.1) and sparse, binaryovercomplete word vectors (bottom, method B, ?2.2 and ?2.4).
Observed dense vectors of length L (left)are converted to sparse non-negative vectors (center) of lengthK which are then projected into the binaryvector space (right), where L  K. X is dense, A is sparse, and B is the binary word vector matrix.Strength of colors signify the magnitude of values; negative is red, positive is blue, and zero is white.with respect to ai.
We define?
= ?sign(g?t,i,j)?t?Gt,i,j(|g?t,i,j| ?
?
),where Gt,i,j=?tt?=1g2t?,i,j.
Now, using the av-erage gradient, the `1-regularized objective is op-timized as follows:at+1,i,j={0, if |g?t,i,j| ?
?
?, otherwise(4)where, at+1,i,jis the jth element of sparse vectoraiat the tth update and g?t,i,jis the correspond-ing average gradient.
For obtaining nonnegativesparse vectors we take projection of the updated aionto RK?0by choosing the closest point in RK?0ac-cording to Euclidean distance (which correspondsto zeroing out the negative elements):at+1,i,j=????
?0, if |g?t,i,j| ?
?0, if ?
< 0?, otherwise(5)2.4 Binarizing TransformationOur aim with method B is to obtain word rep-resentations that can emulate the binary-featureX L ?
?
K % SparseGlove 300 1.0 10?53000 91SG 300 0.5 10?53000 92GC 50 1.0 10?5500 98Multi 48 0.1 10?5960 93Table 1: Hyperparameters for learning sparseovercomplete vectors tuned on the WS-353 task.Tasks are explained in ?B.
The four initial vectorrepresentations X are explained in ?A.hot, fresh, fish, 1/2, wine, saltseries, tv, appearances, episodes1975, 1976, 1968, 1970, 1977, 1969dress, shirt, ivory, shirts, pantsupscale, affluent, catering, clienteleTable 2: Highest frequency words in randomlypicked word clusters of binary sparse overcom-plete Glove vectors.space designed for various NLP tasks.
We could1493state this as an optimization problem:arg minD?RL?KB?
{0,1}K?VV?i=1?xi?Dbi?22+ ?
?bi?11+ ?
?D?22(6)where B denotes the binary (and also sparse) rep-resentation.
This is an mixed integer bilinear pro-gram, which is NP-hard (Al-Khayyal and Falk,1983).
Unfortunately, the number of variables inthe problem is ?
KV which reaches 100 millionwhen V = 100, 000 and K = 1, 000, which isintractable to solve using standard techniques.A more tractable relaxation to this hard prob-lem is to first constrain the continuous represen-tation A to be nonnegative (i.e, ai?
RK?0; ?2.2).Then, in order to avoid an expensive computation,we take the nonnegative word vectors obtained us-ing Eq.
3 and project nonzero values to 1, preserv-ing the 0 values.
Table 2 shows a random set ofword clusters obtained by (i) applying our methodto Glove initial vectors and (ii) applying k-meansclustering (k = 100).
In ?3 we will find that thesevectors perform well quantitatively.2.5 Hyperparameter TuningMethods A and B have three hyperparameters: the`1-regularization penalty ?, the `2-regularizationpenalty ?
, and the length of the overcomplete wordvector representationK.
We perform a grid searchon ?
?
{0.1, 0.5, 1.0} and K ?
{10L, 20L}, se-lecting values that maximizes performance on one?development?
word similarity task (WS-353, dis-cussed in ?B) while achieving at least 90% sparsityin overcomplete vectors.
?
was tuned on one col-lection of initializing vectors (Glove, discussed in?A) so that the vectors in D are near unit norm.The four vector representations and their corre-sponding hyperparameters selected by this proce-dure are summarized in Table 1.
There hyperpa-rameters were chosen for method A and retainedfor method B.3 ExperimentsUsing methods A and B, we constructed sparseovercomplete vector representations A, startingfrom four initial vector representations X; theseare explained in Appendix A.
We used one bench-mark evaluation (WS-353) to tune hyperparame-ters, resulting in the settings shown in Table 1;seven other tasks were used to evaluate the qualityof the sparse overcomplete representations.
Thefirst of these is a word similarity task, where thescore is correlation with human judgments, andthe others are classification accuracies of an `2-regularized logistic regression model trained usingthe word vectors.
These tasks are described in de-tail in Appendix B.3.1 Effects of Transforming VectorsFirst, we quantify the effects of our transforma-tions by comparing their output to the initial (X)vectors.
Table 3 shows consistent improvementsof sparsifying vectors (method A).
The exceptionsare on the SimLex task, where our sparse vectorsare worse than the skip-gram initializer and on parwith the multilingual initializer.
Sparsification isbeneficial across all of the text classification tasks,for all initial vector representations.
On averageacross all vector types and all tasks, sparse over-complete vectors outperform their correspondinginitializers by 4.2 points.2Binarized vectors (from method B) are also usu-ally better than the initial vectors (also shown inTable 3), and tend to outperform the sparsifiedvariants, except when initializing with Glove.
Onaverage across all vector types and all tasks, bina-rized overcomplete vectors outperform their cor-responding initializers by 4.8 points and the con-tinuous, sparse intermediate vectors by 0.6 points.From here on, we explore more deeply thesparse overcomplete vectors from method A (de-noted by A), leaving binarization and method Baside.3.2 Effect of Vector LengthHow does the length of the overcomplete vector(K) affect performance?
We focus here on theGlove vectors, where L = 300, and report av-erage performance across all tasks.
We considerK = ?L where ?
?
{2, 3, 5, 10, 15, 20}.
Figure 2plots the average performance across tasks against?.
The earlier selection of K = 3, 000 (?
= 10)gives the best result; gains are monotonic in ?
tothat point and then begin to diminish.3.3 Alternative TransformationsWe consider two alternative transformations.
Thefirst preserves the original vector length but2We report correlation on a 100 point scale, so that theaverage which includes accuracuies and correlation is equallyrepresentatitve of both.1494VectorsSimLex Senti.
TREC Sports Comp.
Relig.
NPAverageCorr.
Acc.
Acc.
Acc.
Acc.
Acc.
Acc.GloveX 36.9 77.7 76.2 95.9 79.7 86.7 77.9 76.2A 38.9 81.4 81.5 96.3 87.0 88.8 82.3 79.4B 39.7 81.0 81.2 95.7 84.6 87.4 81.6 78.7SGX 43.6 81.5 77.8 97.1 80.2 85.9 80.1 78.0A 41.7 82.7 81.2 98.2 84.5 86.5 81.6 79.4B 42.8 81.6 81.6 95.2 86.5 88.0 82.9 79.8GCX 9.7 68.3 64.6 75.1 60.5 76.0 79.4 61.9A 12.0 73.3 77.6 77.0 68.3 81.0 81.2 67.2B 18.7 73.6 79.2 79.7 70.5 79.6 79.4 68.6MultiX 28.7 75.5 63.8 83.6 64.3 81.8 79.2 68.1A 28.1 78.6 79.2 93.9 78.2 84.5 81.1 74.8B 28.7 77.6 82.0 94.7 81.4 85.6 81.9 75.9Table 3: Performance comparison of transformed vectors to initial vectors X.
We show sparse over-complete representations A and also binarized representations B.
Initial vectors are discussed in ?A andtasks in ?B.Figure 2: Average performace across all tasksfor sparse overcomplete vectors (A) produced byGlove initial vectors, as a function of the ratio ofK to L.achieves a binary, sparse vector (B) by applying:bi,j={1 if xi,j> 00 otherwise(7)The second transformation was proposed byGuo et al (2014).
Here, the original vector lengthis also preserved, but sparsity is achieved through:ai,j=??
?1 if xi,j?M+?1 if xi,j?M?0 otherwise(8)where M+(M?)
is the mean of positive-valued(negative-valued) elements of X.
These vectorsare, obviously, not binary.We find that on average, across initializing vec-tors and across all tasks that our sparse overcom-plete (A) vectors lead to better performance thaneither of the alternative transformations.4 InterpretabilityOur hypothesis is that the dimensions of sparseovercomplete vectors are more interpretable thanthose of dense word vectors.
Following Murphyet al (2012), we use a word intrusion experiment(Chang et al, 2009) to corroborate this hypothesis.In addition, we conduct qualitative analysis of in-terpretability, focusing on individual dimensions.4.1 Word IntrusionWord intrusion experiments seek to quantify theextent to which dimensions of a learned word rep-resentation are coherent to humans.
In one in-stance of the experiment, a human judge is pre-sented with five words in random order and askedto select the ?intruder.?
The words are selected bythe experimenter by choosing one dimension j ofthe learned representation, then ranking the wordson that dimension alone.
The dimensions are cho-sen in decreasing order of the variance of theirvalues across the vocabulary.
Four of the wordsare the top-ranked words according to j, and the?true?
intruder is a word from the bottom half ofthe list, chosen to be a word that appears in the top10% of some other dimension.
An example of aninstance is:naval, industrial, technological, marine, identity1495X: Glove SG GC Multi AverageX 76.2 78.0 61.9 68.1 71.0Eq.
7 75.7 75.8 60.5 64.1 69.0Eq.
8 (Guo et al, 2014) 75.8 76.9 60.5 66.2 69.8A 79.4 79.4 67.2 74.8 75.2Table 4: Average performance across all tasks and vector models using different transformations.Vectors A1 A2 A3 Avg.
IAA ?X 61 53 56 57 70 0.40A 71 70 72 71 77 0.45Table 5: Accuracy of three human annotators onthe word intrusion task, along with the averageinter-annotator agreement (Artstein and Poesio,2008) and Fleiss?
?
(Davies and Fleiss, 1982).
(The last word is the intruder.
)We formed instances from initializing vectorsand from our sparse overcomplete vectors (A).Each of these two combines the four different ini-tializers X.
We selected the 25 dimensions d ineach case.
Each of the 100 instances per condition(initial vs. sparse overcomplete) was given to threejudges.Results in Table 5 confirm that the sparse over-complete vectors are more interpretable than thedense vectors.
The inter-annotator agreement onthe sparse vectors increases substantially, from57% to 71%, and the Fleiss?
?
increases from?fair?
to ?moderate?
agreement (Landis and Koch,1977).4.2 Qualitative Evaluation of InterpretabilityIf a vector dimension is interpretable, the top-ranking words for that dimension should displaysemantic or syntactic groupings.
To verify thisqualitatively, we select five dimensions with thehighest variance of values in initial and sparsi-fied GC vectors.
We compare top-ranked words inthe dimensions extracted from the two representa-tions.
The words are listed in Table 6, a dimensionper row.
Subjectively, we find the semantic group-ings better in the sparse vectors than in the initialvectors.Figure 3 visualizes the sparsified GC vectors forsix words.
The dimensions are sorted by the aver-age value across the three ?animal?
vectors.
Theanimal-related words use many of the same di-mensions (102 common active dimensions out of500 total); in constrast, the three city names useXcombat, guard, honor, bow, trim, naval?ll, could, faced, lacking, seriously, scoredsee, n?t, recommended, depending, partdue, positive, equal, focus, respect, bettersergeant, comments, critics, she, videosAfracture, breathing, wound, tissue, reliefrelationships, connections, identity, relationsfiles, bills, titles, collections, poems, songsnaval, industrial, technological, marinestadium, belt, championship, toll, ride, coachTable 6: Top-ranked words per dimension for ini-tial and sparsified GC representations.
Each lineshows words from a different dimension.mostly distinct vectors.5 Related WorkTo the best of our knowledge, there has been noprior work on obtaining overcomplete word vec-tor representations that are sparse and categorical.However, overcomplete features have been widelyused in image processing, computer vision (Ol-shausen and Field, 1997; Lewicki and Sejnowski,2000) and signal processing (Donoho et al, 2006).Nonnegative matrix factorization is often used forinterpretable coding of information (Lee and Se-ung, 1999; Liu et al, 2003; Cichocki et al, 2009).Sparsity constraints are in general useful in NLPproblems (Kazama and Tsujii, 2003; Friedmanet al, 2008; Goodman, 2004), like POS tagging(Ganchev et al, 2009), dependency parsing (Mar-tins et al, 2011), text classification (Yogatama andSmith, 2014), and representation learning (Ben-gio et al, 2013).
Including sparsity constraintsin Bayesian models of lexical semantics like LDAin the form of sparse Dirichlet priors has beenshown to be useful for downstream tasks like POS-tagging (Toutanova and Johnson, 2007), and im-proving interpretation (Paul and Dredze, 2012;Zhu and Xing, 2012).1496V379V353 V76 V186V339V177V114V342V332V270V222 V91 V303V473V355V358V164V348V324V192 V24 V281 V82 V46 V277V466V465V128 V11 V413 V98 V131V445V199V475V208V431V299V357V149 V80 V247V231 V42 V44 V376V152 V74 V254V141V341V349V234 V55 V477V272V217V457 V57 V159V223V310V436V325V211V117V360V483V363V439V403V119V329 V83 V371V424V179V214V268 V38 V102 V93 V89 V12 V172V173V285V344 V78 V227V426V430V241V384V460V347V171V289V380 V8 V2 V3 V5 V6 V7 V10 V14 V15 V16 V17 V18 V19 V20 V21 V22 V25 V26 V28 V29 V30 V31 V32 V33 V35 V36 V37 V39 V40 V41 V43 V45 V47 V49 V50 V51 V52 V54 V56 V58 V59 V60 V63 V64 V65 V67 V68 V69 V70 V72 V75 V77 V81 V87 V90 V92 V94 V99 V101V103V105V106V108V110V111V116V118V122V123V125V130V132V133V136V137V138V139V140V143V144V147V148V150V155V158V160V162V165V166V167V168V169V170V174V175V178V180V181V182V183V185V188V189V190V191V193V194V195V196V202V203V204V205V212V213V215V218V220V224V226V228V232V233V235V236V238V239V240V242V243V244V248V249V250V251V252V253V255V258V259V260V261V262V263V264V265V266V271V273V274V278V282V284V287V288V290V292V293V294V296V300V302V304V307V308V311V312V313V314V316V317V318V319V320V321V322V323V327V330V331V333V334V336V338V340V343V345V346V352V356V361V362V366V368V369V370V372V373V375V377V378V381V382V383V385V386V387V388V389V390V391V392V394V395V396V398V399V400V401V402V404V405V406V407V408V409V410V412V414V415V416V417V418V419V420V422V423V425V427V428V429V433V434V435V437V441V442V444V446V449V450V451V452V453V455V456V458V459V461V462V463V464V467V468V469V471V472V478V479V480V481V482V484V485V486V488V489V490V491V492V493V494V495V497V499V500V501V487V200V326 V4 V121V267V230V438V134 V97 V104V351V219 V13 V88 V129V286V229V350 V96 V107V153V145V154 V34 V301V374V109V397V156V161V297V115V151V245V447 V53 V337 V79 V448V283V443V201V393V365 V48 V126V257V246V295V120V367 V27 V184V209V306V269V124V470V112V187 V62 V474V354V454V279V146V275V221V207 V71 V335 V73 V85 V440 V95 V23 V225V411V328V305V198V163 V9 V135V315V142V498V291 V86 V476V210V359 V84 V100V309V176V216V432V206V421V276V237 V61 V157V364V127 V66 V256V280V113V298V197V496bostonseattlechicagodoghorsefishFigure 3: Visualization of sparsified GC vectors.
Negative values are red, positive values are blue, zeroesare white.6 ConclusionWe have presented a method that converts wordvectors obtained using any state-of-the-art wordvector model into sparse and optionally binaryword vectors.
These transformed vectors appear tocome closer to features used in NLP tasks and out-perform the original vectors from which they arederived on a suite of semantics and syntactic eval-uation benchmarks.
We also find that the sparsevectors are more interpretable than the dense vec-tors by humans according to a word intrusion de-tection test.AcknowledgmentsWe thank Alona Fyshe for discussions on vec-tor interpretability and three anonymous review-ers for their feedback.
This research was sup-ported in part by the National Science Foundationthrough grant IIS-1251131 and the Defense Ad-vanced Research Projects Agency through grantFA87501420244.
This work was supported in partby the U.S. Army Research Laboratory and theU.S.
Army Research Office under contract/grantnumber W911NF-10-1-0533.A Initial Vector Representations (X)Our experiments consider four publicly availablecollections of pre-trained word vectors.
They varyin the amount of data used and the estimationmethod.Glove.
Global vectors for word representations(Pennington et al, 2014) are trained on aggregatedglobal word-word co-occurrence statistics from acorpus.
These vectors were trained on 6 billionwords from Wikipedia and English Gigaword andare of length 300.33http://www-nlp.stanford.edu/projects/glove/Skip-Gram (SG).
The word2vec tool (Mikolovet al, 2013) is fast and widely-used.
In this model,each word?s Huffman code is used as an input toa log-linear classifier with a continuous projectionlayer and words within a given context window arepredicted.
These vectors were trained on 100 bil-lion words of Google news data and are of length300.4Global Context (GC).
These vectors arelearned using a recursive neural network thatincorporates both local and global (document-level) context features (Huang et al, 2012).
Thesevectors were trained on the first 1 billion words ofEnglish Wikipedia and are of length 50.5Multilingual (Multi).
Faruqui and Dyer (2014)learned vectors by first performing SVD on textin different languages, then applying canonicalcorrelation analysis on pairs of vectors for wordsthat align in parallel corpora.
These vectors weretrained on WMT-2011 news corpus containing360 million words and are of length 48.6B Evaluation BenchmarksOur comparisons of word vector quality considerfive benchmark tasks.
We now describe the differ-ent evaluation benchmarks for word vectors.Word Similarity.
We evaluate our word repre-sentations on two word similarity tasks.
The firstis the WS-353 dataset (Finkelstein et al, 2001),which contains 353 pairs of English words thathave been assigned similarity ratings by humans.This dataset is used to tune sparse vector learninghyperparameters (?2.5), while the remaining of thetasks discussed in this section are completely heldout.4https://code.google.com/p/word2vec5http://nlp.stanford.edu/?socherr/ACL2012_wordVectorsTextFile.zip6http://cs.cmu.edu/?mfaruqui/soft.html1497A more recent dataset, SimLex-999 (Hill et al,2014), has been constructed to specifically focuson similarity (rather than relatedness).
It con-tains a balanced set of noun, verb, and adjectivepairs.
We calculate cosine similarity between thevectors of two words forming a test item and re-port Spearman?s rank correlation coefficient (My-ers and Well, 1995) between the rankings pro-duced by our model against the human rankings.Sentiment Analysis (Senti).
Socher et al(2013) created a treebank of sentences anno-tated with fine-grained sentiment labels on phrasesand sentences from movie review excerpts.
Thecoarse-grained treebank of positive and negativeclasses has been split into training, development,and test datasets containing 6,920, 872, and 1,821sentences, respectively.
We use average of theword vectors of a given sentence as feature forclassification.
The classifier is tuned on thedev.
set and accuracy is reported on the test set.Question Classification (TREC).
As an aid toquestion answering, a question may be classi-fied as belonging to one of many question types.The TREC questions dataset involves six differ-ent question types, e.g., whether the question isabout a location, about a person, or about some nu-meric information (Li and Roth, 2002).
The train-ing dataset consists of 5,452 labeled questions, andthe test dataset consists of 500 questions.
An av-erage of the word vectors of the input question isused as features and accuracy is reported on thetest set.20 Newsgroup Dataset.
We consider three bi-nary categorization tasks from the 20 News-groups dataset.7Each task involves categoriz-ing a document according to two related cate-gories with training/dev./test split in accordancewith Yogatama and Smith (2014): (1) Sports:baseball vs. hockey (958/239/796) (2) Comp.
:IBM vs. Mac (929/239/777) (3) Religion: atheismvs.
christian (870/209/717).
We use average of theword vectors of a given sentence as features.
Theclassifier is tuned on the dev.
set and accuracy isreported on the test set.NP bracketing (NP).
Lazaridou et al (2013)constructed a dataset from the Penn Treebank(Marcus et al, 1993) of noun phrases (NP) of7http://qwone.com/?jason/20Newsgroupslength three words, where the first can be an ad-jective or a noun and the other two are nouns.
Thetask is to predict the correct bracketing in the parsetree for a given noun phrase.
For example, local(phone company) and (blood pressure) medicineexhibit right and left bracketing, respectively.
Weappend the word vectors of the three words in theNP in order and use them as features for binaryclassification.
The dataset contains 2,227 nounphrases split into 10 folds.
The classifier is tunedon the first fold and cross-validation accuracy isreported on the remaining nine folds.ReferencesFaiz A. Al-Khayyal and James E. Falk.
1983.
Jointlyconstrained biconvex programming.
Mathematics ofOperations Research, pages 273?286.Ron Artstein and Massimo Poesio.
2008.
Inter-coderagreement for computational linguistics.
Computa-tional Linguistics, 34(4):555?596.Collin F. Baker, Charles J. Fillmore, and John B. Lowe.1998.
The Berkeley FrameNet project.
In Proc.
ofACL.Mohit Bansal, Kevin Gimpel, and Karen Livescu.2014.
Tailoring continuous word representations fordependency parsing.
In Proc.
of ACL.Yoshua Bengio, Aaron Courville, and Pascal Vincent.2013.
Representation learning: A review and newperspectives.
IEEE Transactions on Pattern Analy-sis and Machine Intelligence, 35(8):1798?1828.Jonathan Chang, Sean Gerrish, Chong Wang, Jordan L.Boyd-Graber, and David M. Blei.
2009.
Readingtea leaves: How humans interpret topic models.
InNIPS.Andrzej Cichocki, Rafal Zdunek, Anh Huy Phan, andShun-ichi Amari.
2009.
Nonnegative Matrix andTensor Factorizations: Applications to ExploratoryMulti-way Data Analysis and Blind Source Separa-tion.
John Wiley & Sons.Mark Davies and Joseph L Fleiss.
1982.
Measuringagreement for multinomial data.
Biometrics, pages1047?1051.David L. Donoho, Michael Elad, and Vladimir N.Temlyakov.
2006.
Stable recovery of sparse over-complete representations in the presence of noise.IEEE Transactions on Information Theory, 52(1).John Duchi, Elad Hazan, and Yoram Singer.
2010.Adaptive subgradient methods for online learn-ing and stochastic optimization.
Technical ReportEECS-2010-24, University of California Berkeley.1498John C. Duchi, Alekh Agarwal, and Martin J. Wain-wright.
2012.
Dual averaging for distributed opti-mization: Convergence analysis and network scal-ing.
IEEE Transactions on Automatic Control,57(3):592?606.Manaal Faruqui and Chris Dyer.
2014.
Improvingvector space word representations using multilingualcorrelation.
In Proc.
of EACL.Manaal Faruqui, Jesse Dodge, Sujay K. Jauhar, ChrisDyer, Eduard Hovy, and Noah A. Smith.
2015.Retrofitting word vectors to semantic lexicons.
InProc.
of NAACL.Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias,Ehud Rivlin, Zach Solan, Gadi Wolfman, and Ey-tan Ruppin.
2001.
Placing search in context: theconcept revisited.
In Proc.
of WWW.Jerome Friedman, Trevor Hastie, and Robert Tibshi-rani.
2008.
Sparse inverse covariance estimationwith the graphical lasso.
Biostatistics, 9(3):432?441.Alona Fyshe, Partha P. Talukdar, Brian Murphy, andTom M. Mitchell.
2014.
Interpretable semantic vec-tors from a joint model of brain- and text- basedmeaning.
In Proc.
of ACL.Alona Fyshe, Leila Wehbe, Partha P. Talukdar, BrianMurphy, and Tom M. Mitchell.
2015.
A composi-tional and interpretable semantic space.
In Proc.
ofNAACL.Kuzman Ganchev, Ben Taskar, Fernando Pereira, andJo?ao Gama.
2009.
Posterior vs. parameter sparsityin latent variable models.
In NIPS.Xavier Glorot, Antoine Bordes, and Yoshua Bengio.2011.
Domain adaptation for large-scale sentimentclassification: A deep learning approach.
In Proc.
ofICML.Joshua Goodman.
2004.
Exponential priors for maxi-mum entropy models.
In Proc.
of NAACL.E.
Grefenstette.
2013.
Towards a formal distributionalsemantics: Simulating logical calculi with tensors.arXiv:1304.5823.Jiang Guo, Wanxiang Che, Haifeng Wang, and TingLiu.
2014.
Revisiting embedding features for sim-ple semi-supervised learning.
In Proc.
of EMNLP.Georg Heigold, Erik McDermott, Vincent Vanhoucke,Andrew Senior, and Michiel Bacchiani.
2014.Asynchronous stochastic optimization for sequencetraining of deep neural networks.
In Proc.
ofICASSP.Felix Hill, Roi Reichart, and Anna Korhonen.
2014.Simlex-999: Evaluating semantic models with (gen-uine) similarity estimation.
CoRR, abs/1408.3456.Patrik O. Hoyer.
2002.
Non-negative sparse coding.
InNeural Networks for Signal Processing, 2002.
Proc.of IEEE Workshop on.Eric H. Huang, Richard Socher, Christopher D. Man-ning, and Andrew Y. Ng.
2012.
Improving wordrepresentations via global context and multiple wordprototypes.
In Proc.
of ACL.Jun?ichi Kazama and Jun?ichi Tsujii.
2003.
Evaluationand extension of maximum entropy models with in-equality constraints.
In Proc.
of EMNLP.Douwe Kiela and Stephen Clark.
2013.
Detectingcompositionality of multi-word expressions usingnearest neighbours in vector space models.
In Proc.of EMNLP.J.
Richard Landis and Gary G. Koch.
1977.
The mea-surement of observer agreement for categorical data.Biometrics, 33(1):159?174.Angeliki Lazaridou, Eva Maria Vecchi, and MarcoBaroni.
2013.
Fish transporters and miraclehomes: How compositional distributional semanticscan help NP parsing.
In Proc.
of EMNLP.Daniel D. Lee and H. Sebastian Seung.
1999.
Learningthe parts of objects by non-negative matrix factoriza-tion.
Nature, 401(6755):788?791.Honglak Lee, Alexis Battle, Rajat Raina, and An-drew Y. Ng.
2006.
Efficient sparse coding algo-rithms.
In NIPS.Honglak Lee, Rajat Raina, Alex Teichman, and An-drew Y. Ng.
2009.
Exponential family sparse cod-ing with application to self-taught learning.
In Proc.of IJCAI.Beth Levin.
1993.
English Verb Classes and Alter-nations: A Preliminary Investigation.
University ofChicago Press.Michael Lewicki and Terrence Sejnowski.
2000.Learning overcomplete representations.
NeuralComputation, 12(2):337?365.Mike Lewis and Mark Steedman.
2013.
Combineddistributional and logical semantics.
Transactionsof the ACL, 1:179?192.Mike Lewis and Mark Steedman.
2014.
Combiningformal and distributional models of temporal and in-tensional semantics.
In Proc.
of ACL.Xin Li and Dan Roth.
2002.
Learning question classi-fiers.
In Proc.
of COLING.Weixiang Liu, Nanning Zheng, and Xiaofeng Lu.2003.
Non-negative matrix factorization for visualcoding.
In Proc.
of ICASSP.Mitchell P. Marcus, Mary Ann Marcinkiewicz, andBeatrice Santorini.
1993.
Building a large anno-tated corpus of english: The penn treebank.
Compu-tational Linguistics, 19(2):313?330.1499Andr?e F. T. Martins, Noah A. Smith, Pedro M. Q.Aguiar, and M?ario A. T. Figueiredo.
2011.
Struc-tured sparsity in structured prediction.
In Proc.
ofEMNLP.Tomas Mikolov, Kai Chen, Greg Corrado, and JeffreyDean.
2013.
Efficient estimation of word represen-tations in vector space.George A. Miller.
1995.
WordNet: a lexicaldatabase for English.
Communications of the ACM,38(11):39?41.Brian Murphy, Partha Talukdar, and Tom Mitchell.2012.
Learning effective and interpretable seman-tic models using non-negative sparse embedding.
InProc.
of COLING.Jerome L. Myers and Arnold D. Well.
1995.
ResearchDesign & Statistical Analysis.
Routledge.Bruno A. Olshausen and David J.
Field.
1997.
Sparsecoding with an overcomplete basis set: A strategyemployed by v1?
Vision Research, 37(23):3311 ?3325.Denis Paperno, Nghia The Pham, and Marco Baroni.2014.
A practical and linguistically-motivated ap-proach to compositional distributional semantics.
InProc.
of ACL.Michael Paul and Mark Dredze.
2012.
Factorial LDA:Sparse multi-dimensional text models.
In NIPS.Jeffrey Pennington, Richard Socher, and Christo-pher D. Manning.
2014.
Glove: Global vectors forword representation.
In Proc.
of EMNLP.Karin Kipper Schuler.
2005.
Verbnet: A Broad-coverage, Comprehensive Verb Lexicon.
Ph.D. the-sis, University of Pennsylvania.Richard Socher, Alex Perelygin, Jean Wu, JasonChuang, Christopher D. Manning, Andrew Y. Ng,and Christopher Potts.
2013.
Recursive deep mod-els for semantic compositionality over a sentimenttreebank.
In Proc.
of EMNLP.Kristina Toutanova and Mark Johnson.
2007.
Abayesian lda-based model for semi-supervised part-of-speech tagging.
In NIPS.Peter D. Turney and Patrick Pantel.
2010.
From fre-quency to meaning : Vector space models of seman-tics.
JAIR, 37(1):141?188.Peter D. Turney.
2001.
Mining the web for synonyms:PMI-IR versus LSA on TOEFL.
In Proc.
of ECML.Eva Maria Vecchi, Roberto Zamparelli, and Marco Ba-roni.
2013.
Studying the recursive behaviour ofadjectival modification with compositional distribu-tional semantics.
In Proc.
of EMNLP.Lin Xiao.
2009.
Dual averaging methods for regular-ized stochastic learning and online optimization.
InNIPS.Chang Xu, Yalong Bai, Jiang Bian, Bin Gao, GangWang, Xiaoguang Liu, and Tie-Yan Liu.
2014.
Rc-net: A general framework for incorporating knowl-edge into word representations.
In Proc.
of CIKM.Dani Yogatama and Noah A Smith.
2014.
Linguisticstructured sparsity in text categorization.
In Proc.
ofACL.Mo Yu and Mark Dredze.
2014.
Improving lexicalembeddings with semantic knowledge.
In Proc.
ofACL.Jun Zhu and Eric P Xing.
2012.
Sparse topical coding.arXiv:1202.3778.1500
