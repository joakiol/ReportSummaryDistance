Algebraic Approaches to Compositional Distributional SemanticsDaoud ClarkeUniversity of Hertfordshiredaoud@metrica.netDavid WeirUniversity of Sussexdavidw@sussex.ac.ukRudi LutzUniversity of Sussexrudil@sussex.ac.ukAbstractThe question of how to compose meaning in distributional representations of meaning has re-cently been recognised as a central issue in computational linguistics.
In this paper we describethree general and powerful tools that can be used to describe composition in distributional seman-tics: quotient algebras, learning of finite dimensional algebras, and the construction of algebras fromsemigroups.1 IntroductionVector based representations of meaning have wide application in natural language processing.
Whilethese techniques work well at the word level, for longer strings, data becomes extremely sparse.
Thequestion of how the principle of compositionality might apply for such representations has thus beenrecognised as an important one (Widdows, 2008; Clark et al, 2008).Context-theoretic semantics (Clarke, 2007) is a framework for composing meanings in vector basedsemantics, in which the composition of the meaning of strings is described by a multiplication on a realvector space A that is bilinear with respect to the addition of the vector space, i.e.x(y + z) = xy + xz (x + y)z = xz + yz (?x)(?y) = ?
?xywhere x, y, z ?
A and ?, ?
?
R. It is assumed that the multiplication is associative, but not commutative.The resulting structure is an associative algebra over a field ?
or simply an algebra when there is noambiguity.
Clarke (2007) gives a mathematical model of meaning as context, and shows that under thismodel, the meaning of natural language expressions can be described by an algebra.
The frameworkis also applied to models of textual entailment, and logical and ontological representations of naturallanguage meaning.In this paper, we identify three general techniques for constructing algebras.?
Using quotient algebras to impose relations on a free algebra, as described in (Clarke et al, 2010).?
Defining finite-dimensional algebras using matrices.
Any finite-dimensional algebra can be de-scribed in this way; we have investigated the possibility of learning such algebras using leastsquares regression.?
Constructing algebras from a semigroup to give it vector space properties.
We sketch a possiblemethod of using this technique, identified by Clarke (2007), to endow logical semantics with avector space nature.This paper presents a preliminary consideration of these general techniques, and our goal is simply toshow that they are worthy of further exploration.325applebigappleredapplecitybigcityredcitybookbigbookredbookapple 1.0 0.26 0.24 0.52 0.13 0.12 0.33 0.086 0.080big apple 1.0 0.33 0.13 0.52 0.17 0.086 0.33 0.11red apple 1.0 0.12 0.17 0.52 0.080 0.11 0.33city 1.0 0.26 0.24 0.0 0.0 0.0big city 1.0 0.33 0.0 0.0 0.0red city 1.0 0.0 0.0 0.0book 1.0 0.26 0.24big book 1.0 0.33red book 1.0Figure 1: Cosine similarity values between phrasessee red apple see big citybuy apple visit big appleread big book modernise citythrow old small red book see modern citybuy large new bookFigure 2: The corpus used to compute the vectorsthat formed the generating set for the ideal.2 Quotient AlgebrasOne commonly used bilinear multiplication operator on vector spaces is the tensor product (denoted ?
),whose use as a method of combining meaning was first proposed by Smolensky (1990), and has beenconsidered more recently by Clark and Pulman (2007) and Widdows (2008), who also looked at thedirect sum (which Widdows calls the direct product, denoted ?
).The tensor algebra on a vector space V (where V is a space of context features) is defined as:T (V ) = R?
V ?
(V ?
V )?
(V ?
V ?
V )?
?
?
?Any element of T (V ) can be described as a sum of components with each in a different tensor powerof V .
Multiplication is defined as the tensor product on these components, and extended linearly to thewhole of T (V ).Previous work has not made full use of the tensor product space; only tensor products are used,not sums of tensor products, giving us the equivalent of the product states of quantum mechanics.
Ourapproach imposes relations on the vectors of the tensor product space that causes some product statesto become equivalent to entangled states, containing sums of tensor products of different degrees.
Thisallows strings of different lengths to share components.
We achieve this by constructing a quotientalgebra.An ideal I of an algebra A is a sub-vector space of A such that xa ?
I and ax ?
I for all a ?
Aand all x ?
I .
An ideal introduces a congruence ?
on A defined by x ?
y if and only if x?
y ?
I .
Forany set of elements ?
?
A there is a unique minimal ideal I?
containing all elements of ?
; this is calledthe ideal generated by ?.
The quotient algebra A/I is the set of all equivalence classes defined by thiscongruence.
Multiplication is defined on A/I by the multiplication on A, since ?
is a congruence.Elements that are congruent with respect to the ideal have equivalence classes that are equal in thequotient algebra.
The construction is thus a way of imposing relations between vector elements: wesimply choose a set of pairs that we wish to be equal, and put their difference in the generating set ?.Clarke et al (2010), showed how an inner product can be computed for elements of the quotientalgebra by taking the quotient of a finite dimensional subspace of the ideal and how a treebank could beused to identify suitable elements to put into the generating set for the ideal in such a way that strings ofdifferent lengths become comparable.
Figure 1 shows similarities between adjective phrases computedusing vectors derived from the corpus in figure 2.
The construction allows many properties of the tensorproduct to carry over into the quotient algebra, for example the similarity of red book to red apple is thesame as the similarity of book to apple, as we would expect from the tensor product.
Unlike the tensorproduct, strings of different length are comparable, so for example, the similarity of apple to red appleis non-zero.
The benefit of using quotient algebras for compositional distributional semantics lies inthis ability to extend the favourable properties of the tensor product by imposing linguistically plausiblerelations between vectors.3263 Learning Finite-dimensional AlgebrasQuotient algebras are useful constructions when we have a small number of relations which we wishto impose on the tensor algebra.
In highly lexicalised grammars, the number of relations we wish toimpose may become so large that the ideal generates the whole vector space, and is thus useless, sincethe resulting quotient space will be trivial.
An alternative to this is to restrict the space of exploration tofinite-dimensional algebras.
In this case, we can explore the space of possible products in relation to theset of relations we wish to hold; in other words, we can view this as an optimisation problem in whichwe want to find the best possible product given the required relations.We apply this to the situation where we obtain a vector x?
for each individual word and pair ofwords in sequence.
We then find the product that best fits these observed vectors.
Given a set W ={w1, w2 .
.
.
wm} of words, we want to define a product  to minimise the difference between w?i  w?jand w?iwj , for 1 ?
i, j ?
m. Specifically, we can define this as minimising?i,j?w?iwj ?
w?i  w?j?If word vectors have n dimensions, then  is defined by an n3 dimensional vector, which we denote frstfor 1 ?
r, s, t ?
n, where (er  es)t = frst and e is the vector with 1 in every component, and vt is thetth component of v.We can view this as a linear model:(w?iwj)t = ijt +n?r,s=1(w?i)r(w?j)sfrstwhere we have m2 statistical units to learn n2 parameters relating to the tth component of the vectorspace.
Since these parameters are independent for each value of t, each set of n2 parameters can be learntin parallel.
We are currently exploring ways of learning these parameters.
The form of the equationabove suggests the use of least squares, and we have performed some experiments using this methodusing a corpus extracted from the ukWaC corpus (Ferraresi et al, 2008).
We extracted a list of verbadjective?noun sequences, and used latent semantic analysis (Deerwester et al, 1990) to generate n-dimensional vectors for the 160 most common adjectives and nouns, and pairs of these adjectives andnouns.
Our initial results indicate that the learnt parameters tend to get very large when using leastsquares to find the parameters, leading to poor results; we plan to investigate other methods such aslinear optimisation.Guevara (2010) proposed a related method of learning composition which used linear regression tolearn how components compose.
His model is however much more restrictive than ours in that the valueof a component in the product depends only on that same component in the composed vectors, whereasin our model, the value of the component can depend on all components in the composed vectors.Baroni and Zamparelli (2010) took a similar approach, in which adjectives are modelled as matricesacting on the space of nouns, and the matrices are learnt using least squares regression.
The algebraproducts we propose learning are more general than matrix products; in addition we do not need todistinguish between words which are represented as matrices and words which are represented as vectors.4 Constructing Algebras from SemigroupsWhilst the previous two techniques we have discussed are very general, and allow corpus data to be easilyincorporated into the composition definition, our implementations are currently a long way from beingable to represent the complexities of natural language semantics that is currently possible with logicalsemantics.
This has become the standard method of representing natural language meaning, originatingin the work of Montague (1973), however there is currently no way to incorporate statistical features ofmeaning that are described by the distributional hypothesis of Harris (1968).327Term Context vectorfish (0, 0, 1)big (1, 2, 0)Figure 3: Example context vectors for terms.ni = (N , ?x nouni(x))ai = (N /N , ?p?y adji(y) ?
p.y)Figure 4: Equations describing syntax and semanticsof adjectives and nouns.In related work, Clark et al (2008) described a method of composing meanings which they notedwas a generalisation of Montague semantics.
However, their version of Montague semantics assumed aparticular model, and thus effectively mapped sentences to truth values.
This omits much of the powerof Montague semantics in which sentences are mapped to logical forms which then provide restrictionson the set of allowable models, allowing, for example, entailments to be computed between sentences.We will sketch a method by which Montague semantics can be described within the context-theoreticframework.
We follow a standard method of representing logic in language, but instead of representingwords using logic, we represent an individual dimension of meaning of a word by a logical form ?
wecall this dimension a ?aspect?.
The general scheme is to represent aspects as elements of a semigroup,from which we form an algebra.
Words are then represented as weighted sums over individual aspects.We define a set S of all aspects as the set of pairs (s, ?
), where s is the syntactic type of an aspect(for example in the Lambek calculus) and ?
is the semantics of the aspect (for example described inthe lambda calculus).
We can extend S by defining a product on such pairs reducing each element to anormal form.
This defines a semigroup: the Lambek calculus can be described in terms of a residuatedlattice, which is a partially ordered semigroup (Lambek, 1958), and the lambda calculus is equivalent to aCartesian closed category under ?-equivalence (Lambek, 1985), which can be considered as a semigroupwith additional structure.Given any semigroup S we can construct an algebra L1(S) of real-valued functions on S which arefinite under the L1 norm with multiplication defined by convolution:(u ?
v)(x) =?y,z?S:yz=xu(y)v(z).For example, suppose we have context vectors for the terms big and fish as described in Figure3.
We represent the syntax and semantics of adjectives and nouns by elements ai and ni respectivelyof a semigroup S (Figure 4), where we assume equivalence under ?-reduction is accounted for.
Thepredicates adji and nounj correspond to aspects, in this case each dimension i of the three dimensionsin the context vectors has a corresponding adji and nouni.
We may then represent the vectors for theseterms as elements of the algebra b?ig = a1 + 2a2 and ?
= n3, where we equate an element u of thesemigroup with the function in the algebra L1(S) which maps u to 1 and every other element to zero.Then b?ig ?
= a1n3 + 2a2n3, whereainj = (N,?x(nounj(x) ?
adji(x))).Note that the elements ai form a commutative, idempotent subsemigroup of S, so they have a semilatticestructure.
In order for this structure to carry over to the vector structure in the algebra, we would needa more sophisticated construction, such as a C?
enveloping algebra; we leave the investigation of thispossibility to further work.5 DiscussionWe have presented our initial investigations into the application of three powerful methods of construct-ing algebras to representing natural language semantics.
Each of these approaches has potential use inrepresenting meaning; here we have only touched the surface of what is possible with each technique.
We328hope that with further work, these methods will lead to a true synthesis between logical and distributionalapproaches to natural language semantics.6 AcknowledgmentsWe are grateful to Peter Hines, Stephen Clark and Peter Lane for useful discussions.
The first author alsowishes to thank Metrica for supporting this research.ReferencesBaroni, M. and R. Zamparelli (2010).
Nouns are vectors, adjectives are matrices: Representing adjective-noun constructions in semantic space.
In Proceedings of the Conference on Empirical Methods inNatural Language Processing (EMNLP 2010), East Stroudsburg PA: ACL, pp.
1183?1193.Clark, S., B. Coecke, and M. Sadrzadeh (2008).
A compositional distributional model of meaning.
InProceedings of the Second Quantum Interaction Symposium (QI-2008), Oxford, UK, pp.
133?140.Clark, S. and S. Pulman (2007).
Combining symbolic and distributional models of meaning.
In Proceed-ings of the AAAI Spring Symposium on Quantum Interaction, Stanford, CA, pp.
52?55.Clarke, D. (2007).
Context-theoretic Semantics for Natural Language: an Algebraic Framework.
Ph.
D.thesis, Department of Informatics, University of Sussex.Clarke, D., R. Lutz, and D. Weir (2010, July).
Semantic composition with quotient algebras.
In Proceed-ings of the 2010 Workshop on GEometrical Models of Natural Language Semantics, Uppsala, Sweden,pp.
38?44.
Association for Computational Linguistics.Deerwester, S., S. Dumais, G. Furnas, T. Landauer, and R. Harshman (1990).
Indexing by latent semanticanalysis.
Journal of the American Society for Information Science 41(6), 391?407.Ferraresi, A., E. Zanchetta, M. Baroni, and S. Bernardini (2008).
Introducing and evaluating ukwac, avery large web-derived corpus of english.
In Workshop Programme, pp.
47.Guevara, E. (2010).
A Regression Model of Adjective-Noun Compositionality in Distributional Seman-tics.
ACL 2010, 33.Harris, Z.
(1968).
Mathematical Structures of Language.
Wiley, New York.Lambek, J.
(1958).
The mathematics of sentence structure.
American Mathematical Monthly 65, 154?169.Lambek, J.
(1985, May).
Cartesian closed categories and typed lambda-calculi.
In G. Cousineau, P.-L.Curien, and B. Robinet (Eds.
), Combinators and Functional Programming Languages, Lecture Notesin Computer Science.
Springer-Verlag.Montague, R. (1973).
The proper treatment of quantification in ordinary English.
Dordrecht, Holland:D. Reidel Publishing Co.Smolensky, P. (1990, November).
Tensor product variable binding and the representation of symbolicstructures in connectionist systems.
Artificial Intelligence 46(1-2), 159?216.Widdows, D. (2008).
Semantic vector products: Some initial investigations.
In Proceedings of theSecond Symposium on Quantum Interaction, Oxford, UK.329
