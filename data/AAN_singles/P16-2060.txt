Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 369?373,Berlin, Germany, August 7-12, 2016.c?2016 Association for Computational LinguisticsEvent Nugget Detection with Forward-Backward Recurrent NeuralNetworksReza Ghaeini, Xiaoli Z. Fern, Liang Huang, Prasad TadepalliSchool of Electrical Engineering and Computer Science, Oregon State University1148 Kelley Engineering Center, Corvallis, OR 97331-5501, USA{ghaeinim, xfern, huanlian, tadepall}@eecs.oregonstate.eduAbstractTraditional event detection methods heav-ily rely on manually engineered rich fea-tures.
Recent deep learning approaches al-leviate this problem by automatic featureengineering.
But such efforts, like tra-dition methods, have so far only focusedon single-token event mentions, whereasin practice events can also be a phrase.We instead use forward-backward recur-rent neural networks (FBRNNs) to detectevents that can be either words or phrases.To the best our knowledge, this is oneof the first efforts to handle multi-wordevents and also the first attempt to useRNNs for event detection.
Experimentalresults demonstrate that FBRNN is com-petitive with the state-of-the-art methodson the ACE 2005 and the Rich ERE 2015event detection tasks.1 IntroductionAutomatic event extraction from natural text isan important and challenging task for natural lan-guage understanding.
Given a set of ontologizedevent types, the goal of event extraction is to iden-tify the mentions of different event types and theirarguments from natural texts.
In this paper we fo-cus on the problem of extracting event mentions,which can be in the form of a single word or mul-tiple words.
In the current literature, events havebeen annotated in two different forms:?
Event trigger: a single token that is consid-ered to signify the occurrence of an event.Here a token is not necessarily a word, forexample, in order to capture a death event,the phrase ?kick the bucket?
is concatenatedinto a single token ?kick the bucket?.
Thisscheme has been used in the ACE and LightERE data and has been followed in most stud-ies on event extraction.?
Event nugget: a word or a phrase of multi-ple words that most clearly expresses the oc-currence of an event.
This scheme is recentlyintroduced to remove the limitation of single-token event triggers and has been adopted bythe rich ERE data for event annotation.Existing event extraction work often heavily relieson a rich set of hand-designed features and utilizesexisting NLP toolkits and resources (Ji and Grish-man, 2008; Patwardhan and Riloff, 2009; Liao andGrishman, 2010; McClosky et al, 2011; Huangand Riloff, 2012; Li et al, 2013a; Li et al, 2013b;Li et al, 2014).
Consequently, it is often challeng-ing to adapt prior methods to multi-lingual or non-English settings since they require extensive lin-guistic knowledge for feature engineering and ma-ture NLP toolkits for extracting the features with-out severe error propagation.By contrast, deep learning has recently emergedas a compelling solution to avoid the afore-mentioned problems by automatically extractingmeaningful features from raw text without relyingon existing NLP toolkits.
There have been somelimited attempts in using deep learning for eventdetection (Nguyen and Grishman, 2015; Chen etal., 2015) which apply Convolutional Neural Net-works (CNNs) to a window of text around poten-tial triggers to identify events.
These efforts out-perform traditional methods, but there remain twomajor limitations:?
So far they have, like traditional methods,only focused on the oversimplified scenarioof single-token event detection.?
Such CNN-based approaches require a fixedsize window.
In practice it is often unclear369Figure 1: The Proposed Forward-Backward Recurrent Neural Network (FBRNN) Model, with the ex-ample sentence ?an unknown man had [broken into] a house last November?
and event nugget candidate?broken into?how large this window needs to be in order tocapture necessary context to make decisionfor an event candidate.Recurrent Neural Networks (RNNs), by con-trast, is a natural solution to both problems abovebecause it can be applied to inputs of variablelength which eliminates both the requirement ofsingle-token event trigger and the need for afixed window size.
Using recurrent nodes withLong Short Term Memory (LSTM) (Hochreiterand Schmidhuber, 1997) or Gated Recurrent Units(GRU) (Cho et al, 2014), RNN is potentially ca-pable of selectively deciding the relevant contextto consider for detecting events.In this paper we present a forward-backward re-current neural network (FBRNN) to extract (pos-sibly multi-word) event mentions from raw text.Although RNNs have been studied extensively inother NLP tasks (Cross and Huang, 2016; Taiet al, 2015; Socher et al, 2014; Paulus et al,2014), to the best of our knowledge, this is thefirst work to use RNNs for event detection.
Thisis also one of the first efforts to handle multi-wordevent nuggets.
Experimental results confirm thatFBRNN is competitive compared to the state-of-the-art on the ACE 2005 dataset and the Rich ERE2015 event detection task.2 Proposed ModelLet x = [w0, w1, ..., wn] be a sentence.
We first goover each word and phrase and heuristically ex-tract a set of event candidates.
The task is thento predict for each candidate given the sentencewhether it is an event and, if so, its type.
Figure 1demonstrates our proposed model for this task.For each event candidate, which consists ofa continuous span of texts [wi, ..., wj], we splitthe sentence into three parts: the left con-text [w0, ..., wi?1], the event nugget candidate[wi, ..., wj] and the right context [wj+1, ..., wn].For instance, for event candidate ?broken into?and given sentence ?an unknown man had brokeninto a house last November?
; [an, unknown, man,had], [broken, into] and [a, house, last, Novem-ber] are the left context, the event nugget candidateand the right context respectively.
For each part,we learn a separate RNN to produce a represen-tation.
Before feeding the data into the network,each word is represented as a real-valued vectorthat is formed by concatenating a word embeddingwith a branch embedding, which we describe be-low:?
Word embedding: Several studies have in-vestigated methods for representing words asreal-valued vectors in order to capture thehidden semantic and syntactic properties ofwords (Collobert and Weston, 2008; Mikolovet al, 2013).
Such embeddings are typicallylearned from large unlabeled text corpora,consequently can serve as good initializa-tions.
In our work, we initialize the word em-bedding with the pretrained 300-diemensionword2vec (Mikolov et al, 2013).370?
Branch embedding: The relative positionof a word to the current event nugget candi-date may contain useful information towardhow the word should be used or interpretedin identifying events.
It is thus a commonpractice to include an additional embeddingfor each word that characterizes its relativeposition to the event nugget candidate.
Inthis work, to reduce the complexity of ourmodel and avoid overfitting, we only learnembeddings for three different positions: theleft branch, the nugget branch and the rightbranch respectively.
This is illustrated usingthree different colors in Figure 1.Now each word is represented as a real-valuedvector, formed by concatenating its word andbranch embeddings.
The sequence of words inthe left, nugget and right branches will each passthrough a separate Recurrent Neural Network.
Forthe left and nugget branches, we process the wordsfrom left to right, and use the opposite direction(from right to left) for the right context, thus thename Forward-Backward RNN (FBRNN).The output of each recurrent neural network is afixed size representation of its input.
We concate-nate the representations from the three branchesand pass it through a fully connected neural net-work with a softmax output node that classifieseach event candidate as an event of specific typeor a non-event.
Note that in cases where an eventcandidate can potentially belong to multiple eventtypes, one can replace the softmax output nodewith a set of binary output nodes or a sigmoid toallow for multi-label prediction for each event can-didate.To avoid overfitting, we use dropout (Hintonet al, 2012; Srivastava et al, 2014) with rateof 0.5 for regularization.
The weights of the re-current neural networks as well as the fully con-nected neural network are learned by minimizingthe log-loss on the training data via the Adamoptimizer (Kingma and Ba, 2015) which per-forms better that other optimization methods likeAdaDelta (Zeiler, 2012), AdaGrad (Duchi et al,2011), RMSprop and SGD.
During training, theword and branch embeddings are updated to learneffective representations for this specific task.3 ExperimentsIn this section, we first empirically examine somedesign choices for our model and then comparethe proposed model to the current state-of-the-arton two different event detection datasets.3.1 Datasets, candidate generation andhyper-parametersWe experiment on two different corpora, ACE2005 and Rich ERE 2015.?
ACE 2005: The ACE 2005 corpus is anno-tated with single-token event triggers and haseight event types and 33 event subtypes that,along with the ?non-event?
class, constitutesa 34-class classification problem.
In our ex-periments we used the same train, develop-ment and test sets as the previous studies onthis dataset (Nguyen and Grishman, 2015; Liet al, 2013b).
Candidate generation for thiscorpus is based on a list of candidate eventtrigger words created from the training dataand the PPDB paraphrase database.
Given asentence, we go over each token and extractthe tokens that appear in this high-recall listas event candidates, which we then classifywith our proposed FBRNN model.?
Rich ERE 2015: The Rich ERE 2015 cor-pus was released in the TAC 2015 competi-tion and annotated at the nugget level, thusaddressing phrasal event mentions.
The RichERE 2015 corpus has nine event types and38 event subtypes, forming a 39-class clas-sification problem (considering ?non-event?as an additional class).
We utilized the sametrain and test sets that have been used in theTAC 2015 event nugget detection competi-tion.
A subset of the provided train set wasset aside as our development set.
To gener-ate event nugget candidates, we first followedthe same strategy that we used for the ACE2005 dataset experiment to identify single-token event candidates.
We then expand thesingle-token event candidates using a heuris-tic rule based on POS tags.There are a number of hyper-parameters for ourmodel, including the dimension of the branch em-bedding, the number of recurrent layers in eachRNN, the size of the RNN outputs, the dropoutrates for training the networks.
We tune these pa-rameters using the development set.3.2 Exploration of different design choicesWe first design some experiments to evaluate theimpact of the following design choices:371Configurations P R F1LSTM+branch 59.82 48.39 53.50-branch 58.50 44.82 50.76GRU+branch 63.72 47.68 54.55-branch 64.56 43.93 52.28Table 1: Performance on the development set withdifferent configurations on Rich ERE 2015.Methods P R F1Sentence level in Ji and- - 59.7Grishman (2008)MaxEnt with local- - 64.7features in Li et al (2013b)Joint beam search with local- - 63.7features in Li et al (2013b)Joint beam search with- - 65.6local and global features inLi et al (2013b)CNN (Nguyen, 2015) 71.9 63.8 67.6FBRNN 66.8 68.0 67.4Table 2: Comparison with reported performanceby event detection systems without using gold en-tity mentions and types on the ACE 2005 corpus.i) Different RNN structures: LSTM and GRUare two popular recurrent network structuresthat are capable of extracting long-term de-pendencies in different ways.
Here we com-pare their performance for event detection.ii) The effect of branch embedding: A word canpresent different role and concept when it isin a nugget branch or other branches.
Herewe would examine the effect of includingbranch embedding.Table 1 shows the results of our model with dif-ferent design choices on the development set ofthe Rich ERE 2015 corpus.
We note that the per-formance of GRU is slightly better than that ofLSTM.
We believe this is because GRU is a lesscomplex structure compared to LSTM, thus lessprone to overfitting given the limited training datafor our task.
From the results we can also see thatthe branch embedding performs a crucial role forour model, producing significant improvement forboth LSTM and GRU.Based on the results presented above, for the re-maining experiments we will focus on GRU struc-ture with branch embeddings.3.3 Results on ACE 2005Many prior studies employ gold-standard en-tity mentions and types from manual annotation,Methods P R F11st75.23 47.74 58.412nd73.95 46.61 57.183th73.68 44.94 55.834th73.73 44.57 55.565th71.06 43.50 53.97FBRNN 71.58 48.19 57.61Table 3: Performance of FBRNN compared withreported top results in TAC competition (Mita-mura et al, 2015) on Rich ERE 2015.which would not be available in reality duringtesting.
Nguyen and Grishman (2015) examinedthe performance of a number of traditional sys-tems (Li et al, 2013b) in a more realistic setting,where entity mentions and types are acquired froman automatic high-performing name tagger and in-formation extraction system.
In Table 2 we com-pare the performance of our system with these re-sults reported by Nguyen and Grishman (2015).We first note that the deep learning methods(CNN and FBRNN) achieve significantly better F1performance compared to traditional methods us-ing manually engineered features (both local andglobal).
Compared to CNN, our FBRNN modelachieved better recall but the precision is lower.For the overall F1 measure, our model is compa-rable with the CNN model.3.4 Results on Rich ERE 2015Table 3 reports the test performance of our modeland shows that it is competitive with the top-ranked results obtained in the TAC 2015 eventnugget detection competition.
It is interesting tonote that FBRNN is again winning in recall, butlosing in precision, a phenomenon that is consis-tently observed in both corpora and a topic wortha closer look for future work.Finally, in Rich ERE test data, approximately9% of the events are actually multi-labeled.
Ourcurrent model uses softmax output layer and isthus innately incapable of making multi-label pre-dictions.
Despite this limitation, FBRNN achievedcompetitive result on Rich ERE with only 0.8%difference from the best reported system in theTAC 2015 competition.4 ConclusionsThis paper proposes a novel language-independentevent detection method based on RNNs which canautomatically extract effective features from raw372text to detect event nuggets.
We conducted twoexperiments to compare FBRNN with the state-of-the-art event detection systems on the ACE 2005and Rich ERE 2015 corpora.
These experimentsdemonstrate that FBRNN achieves competitive re-sults compared to the current state-of-the-art.ReferencesYubo Chen, Liheng Xu, Kang Liu, Daojian Zeng, andJun Zhao.
2015.
Event Extraction via DynamicMulti-Pooling Convolutional Neural Networks.
As-sociation for Computational Linguistics, 1:167?176.Kyunghyun Cho, Bart van Merrienboer, C?aglarG?ulc?ehre, Dzmitry Bahdanau, Fethi Bougares, Hol-ger Schwenk, and Yoshua Bengio.
2014.
Learn-ing Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation.
Em-pirical Methods in Natural Language Processing,pages 1724?1734.Ronan Collobert and Jason Weston.
2008.
A unifiedarchitecture for natural language processing: deepneural networks with multitask learning.
ICML,pages 160?167.James Cross and Liang Huang.
2016.
IncrementalParsing with Minimal Features Using Bi-DirectionalLSTM.
Association for Computational Linguistics.John Duchi, Elad Hazan, and Yoram Singer.
2011.Adaptive subgradient methods for online learningand stochastic optimization.
The Journal of Ma-chine Learning Research, 12:2121?2159.Geoffrey E. Hinton, Nitish Srivastava, AlexKrizhevsky, Ilya Sutskever, and Ruslan Salakhut-dinov.
2012.
Improving neural networks bypreventing co-adaptation of feature detectors.CoRR, abs/1207.0580.Sepp Hochreiter and J?urgen Schmidhuber.
1997.Long Short-Term Memory.
Neural Computation,9(8):1735?1780.Ruihong Huang and Ellen Riloff.
2012.
ModelingTextual Cohesion for Event Extraction.
AAAI.Heng Ji and Ralph Grishman.
2008.
Refining EventExtraction through Cross-Document Inference.
As-sociation for Computational Linguistics, pages 254?262.Diederik Kingma and Jimmy Ba.
2015.Adam: A method for stochastic optimization.arXiv:1412.6980.Peifeng Li, Qiaoming Zhu, and Guodong Zhou.
2013a.Argument Inference from Relevant Event Mentionsin Chinese Argument Extraction.
Association forComputational Linguistics, 1:1477?1487.Qi Li, Heng Ji, and Liang Huang.
2013b.
Joint EventExtraction via Structured Prediction with GlobalFeatures.
Association for Computational Linguis-tics, 1:73?82.Qi Li, Heng Ji, Yu Hong, and Sujian Li.
2014.
Con-structing Information Networks Using One SingleModel.
Empirical Methods in Natural LanguageProcessing, pages 1846?1851.Shasha Liao and Ralph Grishman.
2010.
Using Docu-ment Level Cross-Event Inference to Improve EventExtraction.
Association for Computational Linguis-tics, pages 789?797.David McClosky, Mihai Surdeanu, and Christopher D.Manning.
2011.
Event Extraction as DependencyParsing.
Association for Computational Linguistics,pages 1626?1635.Tomas Mikolov, Ilya Sutskever, Kai Chen, Gregory S.Corrado, and Jeffrey Dean.
2013.
Distributed Rep-resentations of Words and Phrases and their Compo-sitionality.
Neural Information Processing Systems,pages 3111?3119.Teruko Mitamura, Zhengzhong Liu, and Eduard Hovy.2015.
Overview of TAC KBP 2015 Event NuggetTrack.
Text Analysis Conference.Thien Huu Nguyen and Ralph Grishman.
2015.
EventDetection and Domain Adaptation with Convolu-tional Neural Networks.
Association for Computa-tional Linguistics, 2:365?371.Siddharth Patwardhan and Ellen Riloff.
2009.
A Uni-fied Model of Phrasal and Sentential Evidence forInformation Extraction.
Empirical Methods in Nat-ural Language Processing, pages 151?160.Romain Paulus, Richard Socher, and Christopher D.Manning.
2014.
Global Belief Recursive NeuralNetworks.
Neural Information Processing Systems,pages 2888?2896.Richard Socher, Andrej Karpathy, Quoc V. Le, Christo-pher D. Manning, and Andrew Y. Ng.
2014.Grounded Compositional Semantics for Finding andDescribing Images with Sentences.
Transactionsof the Association for Computational Linguistics,2:207?218.Nitish Srivastava, Geoffrey E. Hinton, AlexKrizhevsky, Ilya Sutskever, and Ruslan Salakhutdi-nov. 2014.
Dropout: a simple way to prevent neuralnetworks from overfitting.
Journal of MachineLearning Research, 15(1):1929?1958.Kai Sheng Tai, Richard Socher, and Christopher D.Manning.
2015.
Improved Semantic Representa-tions From Tree-Structured Long Short-Term Mem-ory Networks.
Association for Computational Lin-guistics, 1:1556?1566.Matthew D Zeiler.
2012.
ADADELTA: anadaptive learning rate method.
arXiv preprintarXiv:1212.5701.373
