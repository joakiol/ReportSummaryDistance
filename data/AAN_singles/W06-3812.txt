Workshop on TextGraphs, at HLT-NAACL 2006, pages 73?80,New York City, June 2006. c?2006 Association for Computational LinguisticsChinese Whispers - an Efficient Graph Clustering Algorithmand its Application to Natural Language Processing ProblemsChris BiemannUniversity of Leipzig, NLP DepartmentAugustusplatz 10/1104109 Leipzig, Germanybiem@informatik.uni-leipzig.deAbstractWe introduce Chinese Whispers, arandomized graph-clustering algorithm,which is time-linear in the number ofedges.
After a detailed definition of thealgorithm and a discussion of its strengthsand weaknesses, the performance ofChinese Whispers is measured on NaturalLanguage Processing (NLP) problems asdiverse as language separation,acquisition of syntactic word classes andword sense disambiguation.
At this, thefact is employed that the small-worldproperty holds for many graphs in NLP.1 IntroductionClustering is the process of grouping togetherobjects based on their similarity to each other.
Inthe field of Natural Language Processing (NLP),there are a variety of applications for clustering.The most popular ones are document clustering inapplications related to retrieval and word clusteringfor finding sets of similar words or concepthierarchies.Traditionally, language objects arecharacterized by a feature vector.
These featurevectors can be interpreted as points in amultidimensional space.
The clustering uses adistance metric, e.g.
the cosine of the anglebetween two such vectors.
As in NLP there areoften several thousand features, of which only afew correlate with each other at a time ?
thinkabout the number of different words as opposed tothe number of words occurring in a sentence ?dimensionality reduction techniques can greatlyreduce complexity without considerably losingaccuracy.An alternative representation that does not dealwith dimensions in space is the graphrepresentation.
A graph represents objects (asnodes) and their relations (as edges).
In NLP, thereare a variety of structures that can be naturallyrepresented as graphs, e.g.
lexical-semantic wordnets, dependency trees, co-occurrence graphs andhyperlinked documents, just to name a few.Clustering graphs is a somewhat different taskthan clustering objects in a multidimensionalspace: There is no distance metric; the similaritybetween objects is encoded in the edges.
Objectsthat do not share an edge cannot be compared,which gives rise to optimization techniques.
Thereis no centroid or ?average cluster member?
in agraph, permitting centroid-based techniques.As data sets in NLP are usually large, there is astrong need for efficient methods, i.e.
of lowcomputational complexities.
In this paper, a veryefficient graph-clustering algorithm is introducedthat is capable of partitioning very large graphs incomparatively short time.
Especially for small-world graphs (Watts, 1999), high performance isreached in quality and speed.
After explaining thealgorithm in the next section, experiments withsynthetic graphs are reported in section 3.
Thesegive an insight about the algorithm?s performance.In section 4, experiments on three NLP tasks arereported, section 5 concludes by discussingextensions and further application areas.2 Chinese Whispers AlgorithmIn this section, the Chinese Whispers (CW)algorithm is outlined.
After recalling importantconcepts from Graph Theory (cf.
Bollob?s 1998),we describe two views on the algorithm.
The73second view is used to relate CW to another graphclustering algorithm, namely MCL (van Dongen,2000).We use the following notation throughout thispaper: Let G=(V,E) be a weighted graph withnodes (vi)?V and weighted edges (vi, vj, wij) ?Ewith weight wij.
If (vi, vj, wij)?E implies (vj, vi,wij)?E, then the graph is undirected.
If all weightsare 1, G is called unweighted.The degree of a node is the number of edges anode takes part in.
The neighborhood of a node vis defined by the set of all nodes v?
such that(v,v?,w)?E or (v?,v,w)?E; it consists of all nodesthat are connected to v.The adjacency matrix AG of a graph G with nnodes is an n?n matrix where the entry aij denotesthe weight of the edge between vi and vj , 0otherwise.The class matrix DG of a Graph G with n nodes isan n?n matrix where rows represent nodes andcolumns represent classes (ci)?C.
The value dij atrow i and column j represents the amount of vi asbelonging to a class cj.
For convention, classmatrices are row-normalized; the i-th row denotesa distribution of vi over C. If all rows have exactlyone non-zero entry with value 1, DG denotes a hardpartitioning of V, soft partitioning otherwise.2.1 Chinese Whispers algorithmCW is a very basic ?
yet effective ?
algorithm topartition the nodes of weighted, undirected graphs.It is motivated by the eponymous children?s game,where children whisper words to each other.
Whilethe game?s goal is to arrive at some funnyderivative of the original message by passing itthrough several noisy channels, the CW algorithmaims at finding groups of nodes that broadcast thesame message to their neighbors.
It can be viewedas a simulation of an agent-based social network;for an overview of this field, see (Amblard 2002).The algorithm is outlined in figure 1:initialize:forall vi in V: class(vi)=i;while changes:forall v in V, randomized order:class(v)=highest ranked classin neighborhood of v;Figure 1: The Chinese Whispers algorithmIntuitively, the algorithm works as follows in abottom-up fashion: First, all nodes get differentclasses.
Then the nodes are processed for a smallnumber of iterations and inherit the strongest classin the local neighborhood.
This is the class whosesum of edge weights to the current node ismaximal.
In case of multiple strongest classes, oneis chosen randomly.
Regions of the same classstabilize during the iteration and grow until theyreach the border of a stable region of another class.Note that classes are updated immediately: a nodecan obtain classes from the neighborhood that wereintroduced there in the same iteration.Figure 2 illustrates how a small unweightedgraph is clustered into two regions in threeiterations.
Different classes are symbolized bydifferent shades of grey.Figure 2: Clustering an 11-nodes graph with CW intwo iterationsIt is possible to introduce a random mutationrate that assigns new classes with a probabilitydecreasing in the number of iterations as describedin (Biemann & Teresniak 2005).
This showedhaving positive effects for small graphs because ofslower convergence in early iterations.The CW algorithm cannot cross componentboundaries, because there are no edges betweennodes belonging to different components.
Further,nodes that are not connected by any edge arediscarded from the clustering process, whichpossibly leaves a portion of nodes unclustered.Formally, CW does not converge, as figure 3exemplifies: here, the middle node?s neighborhood0.1.2.74consists of a tie which can be decided in assigningthe class of the left or the class of the right nodes inany iteration all over again.
Ties, however, do notplay a major role in weighted graphs.Figure 3: The middle node gets the grey or theblack class.
Small numbers denote edge weights.Apart from ties, the classes usually do notchange any more after a handful of iterations.
Thenumber of iterations depends on the diameter ofthe graph: the larger the distance between twonodes is, the more iterations it takes to percolateinformation from one to another.The result of CW is a hard partitioning of thegiven graph into a number of partitions thatemerges in the process ?
CW is parameter-free.
Itis possible to obtain a soft partitioning by assigninga class distribution to each node, based on theweighted distribution of (hard) classes in itsneighborhood in a final step.The outcomes of CW resemble those of Min-Cut (Wu & Leahy 1993): Dense regions in thegraph are grouped into one cluster while sparselyconnected regions are separated.
In contrast toMin-Cut, CW does not find an optimal hierarchicalclustering but yields a non-hierarchical (flat)partition.
Furthermore, it does not require anythreshold as input parameter and is more efficient.Another algorithm that uses only local contextsfor time-linear clustering is DBSCAN as, describedin (Ester et al 1996), needing two input parameters(although the authors propose an interactiveapproach to determine them).
DBSCAN isespecially suited for graphs with a geometricalinterpretation, i.e.
the objects have coordinates in amultidimensional space.
A quite similar algorithmto CW is MAJORCLUST (Stein & Niggemann1996), which is based on a comparable idea butconverges slower.2.2 Chinese Whispers as matrix operationAs CW is a special case of Markov-Chain-Clustering (MCL) (van Dongen, 2000), we spend afew words on explaining it.
MCL is the parallelsimulation of all possible random walks up to afinite length on a graph G. The idea is that randomwalkers are more likely to end up in the samecluster where they started than walking acrossclusters.
MCL simulates flow on a graph byrepeatedly updating transition probabilitiesbetween all nodes, eventually converging to atransition matrix after k steps that can beinterpreted as a clustering of G. This is achieved byalternating an expansion step and an inflation step.The expansion step is a matrix multiplication ofMG with the current transition matrix.
The inflationstep is a column-wise non-linear operator thatincreases the contrast between small and largetransition probabilities and normalizes the column-wise sums to 1.
The k matrix multiplications of theexpansion step of MCL lead to its time-complexityof O(k?n?
).It has been observed in (van Dongen, 2000),that only the first couple of iterations operate ondense matrices ?
when using a strong inflationoperator, matrices in the later steps tend to besparse.
The author further discusses pruningschemes that keep only some of the largest entriesper column, leading to drastic optimizationpossibilities.
But the most aggressive sort ofpruning is not considered: only keeping one singlelargest entry.
Exactly this is conducted in the basicCW process.
Let maxrow(.)
be an operator thatoperates row-wise on a matrix and sets all entriesof a row to zero except the largest entry, which isset to 1.
Then the algorithm is denoted as simple asthis:D0 = Infor t=1 to iterationsDt-1 = maxrow(Dt-1)Dt  = Dt-1AGFigure 4: Matrix Chinese Whispers process.
t istime step, In is the identity matrix of size n?n, AG isthe adjacency matrix of graph G.By applying maxrow(.
), Dt-1 has exactly nnon-zero entries.
This causes the time-complexityto be dependent on the number of edges, namelyO(k?|E|).
In the worst case of a fully connectedgraph, this equals the time-complexity of MCL.A problem with the matrix CW process is that itdoes not necessarily converge to an iteration-invariant class matrix D, but rather to a pair ofoscillating class matrices.
Figure 5 shows anexample.11  112275Figure 5: oscillating states in matrix CW for anunweighted graphThis is caused by the stepwise update of theclass matrix.
As opposed to this, the CW algorithmas outlined in figure 1 continuously updates D afterthe processing of each node.
To avoid theseoscillations, one of the following measures can betaken:?
Random mutation: with some probability, themaxrow-operator places the 1 for an otherwiseunused class?
Keep class: with some probability, the row iscopied from Dt-1 to Dt?
Continuous update (equivalent to CW asdescribed in section 2.1.
)While converging to the same limits, thecontinuous update strategy converges the fastestbecause prominent classes are spread much fasterin early iterations.3 Experiments with synthetic graphsThe analysis of the CW process is difficult due toits nonlinear nature.
Its run-time complexityindicates that it cannot directly optimize mostglobal graph cluster measures because of their NP-completeness (?
?ma and Schaeffer, 2005).Therefore we perform experiments on syntheticgraphs to empirically arrive at an impression of ouralgorithm's abilities.
All experiments wereconducted with an implementation followingfigure 1.
For experiments with synthetic graphs,we restrict ourselves to unweighted graphs, if notstated explicitly.3.1 Bi-partite cliquesA cluster algorithm should keep dense regionstogether while cutting apart regions that aresparsely connected.
The highest density is reachedin fully connected sub-graphs of n nodes, a.k.a.
n-cliques.
We define an n-bipartite-clique as a graphof two n-cliques, which are connected such thateach node has exactly one edge going to the cliqueit, does not belong to.Figures 5 and 6 are n-partite cliques for n=4,10.Figure 6: The 10-bipartite clique.We clearly expect a clustering algorithm to cutthe two cliques apart.
As we operate onunweighted graphs, however, CW is left with twochoices: producing two clusters or grouping allnodes into one cluster.
This is largely dependent onthe random choices in very early iterations - if thesame class is assigned to several nodes in bothcliques, it will finally cover the whole graph.Figure 7 illustrates on what rate this happens on n-bipartite-cliques for varying n.Figure 7: Percentage of obtaining two clusterswhen applying CW on n-bipartite cliquesIt is clearly a drawback that the outcome of CWis non-deterministic.
Only half of the experimentswith 4-bipartite cliques resulted in separation.However, the problem is most dramatic on smallgraphs and ceases to exist for larger graphs asdemonstrated in figure 7.3.2 Small world graphsA structure that has been reported to occur in anenormous number of natural systems is the smallworld (SW) graph.
Space prohibits an in-depthdiscussion, which can be found in (Watts 1999).Here, we restrict ourselves to SW-graphs inlanguage data.
In (Ferrer-i-Cancho and Sole,2001), co-occurrence graphs as used in theexperiment section are reported to possess thesmall world property, i.e.
a high clustering co-efficient and short average path length between76arbitrary nodes.
Steyvers and Tenenbaum (2005)show that association networks as well as semanticresources are scale-free SW-graphs: their degreedistribution follows a power law.
A generativemodel is provided that generates undirected, scale-free SW-graphs in the following way: We startwith a small number of fully connected nodes.When adding a new node, an existing node v ischosen with a probability according to its degree.The new node is connected to M nodes in theneighborhood of v. The generative model isparameterized by the number of nodes n and thenetwork's mean connectivity, which approaches2M for large n.Let us assume that we deal with natural systemsthat can be characterized by small world graphs.
Iftwo or more of those systems interfere, theirgraphs are joined by merging some nodes,retaining their edges.
A graph-clustering algorithmshould split up the resulting graph in its previousparts, at least if not too many nodes were merged.We conducted experiments to measure CW'sperformance on SW-graph mixtures: We generatedgraphs of various sizes, merged them by twos to avarious extent and measured the amount of caseswhere clustering with CW leads to thereconstruction of the original parts.
Whengenerating SW-graphs with the Steyvers-Tenenbaum model, we fixed M to 10 and varied nand the merge rate r, which is the fraction of nodesof the smaller graph that is merged with nodes ofthe larger graph.Figure 8: Rate of obtaining two clusters for mix-tures of SW-graphs dependent on merge rate r.Figure 8 summarizes the results for equisizedmixtures of 300, 3,000 and 30,000 nodes andmixtures of 300 with 30,000 nodes.It is not surprising that separating the two partsis more difficult for higher r. Results are not verysensitive to size and size ratio, indicating that CWis able to identify clusters even if they differconsiderably in size ?
it even performs best at theskewed mixtures.
At merge rates between 20% and30%, still more then half of the mixtures areseparated correctly and can be found whenaveraging CW?s outcome over several runs.3.3 Speed issuesAs formally, the algorithm does not converge, it isimportant to define a stop criterion or to set thenumber of iterations.
To show that only a fewiterations are needed until almost-convergence, wemeasured the normalized Mutual Information(MI)1 between the clustering in the 50th iterationand the clusterings of earlier iterations.
This wasconducted for two unweighted SW-graphs with1,000 (1K) and 10,000 (10K) nodes, M=5 and aweighted 7-lingual co-occurrence graph (cf.section 4.1) with 22,805 nodes and 232,875 edges.Table 1 indicates that for unweighted graphs,changes are only small after 20-30 iterations.
Initerations 40-50, the normalized MI-values do notimprove any more.
The weighted graph convergesmuch faster due to fewer ties and reaches a stableplateau after only 6 iterations.Iter 1 2 3 5 10 20 30 40 491K 1 8 13 20 37 58 90 90 9110K 6 27 46 64 79 90 93 95 967ling 29 66 90 97 99.5 99.5 99.5 99.5 99.5Table 1: normalized Mutual Information values forthree graphs and different iterations in %.4 NLP ExperimentsIn this section, some experiments with graphsoriginating from natural language data arepresented.
First, we define the notion of co-occurrence graphs, which are used in sections 4.1and 4.3: Two words co-occur if they can both befound in a certain unit of text, here a sentence.Employing a significance measure, we determinewhether their co-occurrences are significant orrandom.
In this case, we use the log-likelihoodmeasure as described in (Dunning 1993).
We usethe words as nodes in the graph.
The weight of an1defined for two random variables X and Y as (H(X)+H(Y)-H(X,Y))/max(H(X),H(Y)) with H(X) entropy.
A value of 0denotes indepenence, 1 is perfect congruence.77edge between two words is set to the significancevalue of their co-occurrence, if it exceeds a certainthreshold.
In the experiments, we used sig-nificances from 15 on.
The entirety of words thatare involved in at least one edge together withthese edges is called co-occurrence graph (cf.Biemann et al 2004).In general, CW produces a large number ofclusters on real-world graphs, of which themajority is very small.
For most applications, itmight be advisable to define a minimum clustersize or something alike.4.1 Language SeparationThis section shortly reviews the results of(Biemann and Teresniak, 2005), where CW wasfirst described.
The task was to separate amultilingual corpus by languages, assuming itstokenization in sentences.The co-occurrence graph of a multilingualcorpus resembles the synthetic SW-graphs: Everylanguage forms a separate co-occurrence graph,some words that are used in more than onelanguage are members of several graphs,connecting them.
By CW-partitioning, the graph issplit into its monolingual parts.
These parts areused as word lists for word-based languageidentification.
(Biemann and Teresniak, 2005)report almost perfect performance on getting 7-lingual corpora with equisized parts sorted apart aswell as highly skewed mixtures of two languages.In the process, language-ambiguous words areassigned to only one language, which did not hurtperformance due to the high redundancy of thetask.
However, it would have been possible to usethe soft partitioning to acquire a distribution overlanguages for each word.4.2 Acquisition of Word ClassesFor the acquisition of word classes, we use adifferent graph: the second-order graph onneighboring co-occurrences.
To set up the graph, aco-occurrence calculation is performed whichyields significant word pairs based on theiroccurrence as immediate neighbors.
This can beperceived as a bipartite graph, figure 9a gives a toyexample.
Note that if similar words occur in bothparts, they form two distinct nodes.This graph is transformed into a second-ordergraph by comparing the number of common rightand left neighbors for two words.
The similarity(edge weight) between two words is the sum ofcommon neighbors.
Figure 9b depicts the second-order graph derived from figure 9a and itspartitioning by CW.
The word-class-ambiguousword ?drink?
(to drink the drink) is responsible forall intra-cluster edges.
The hypothesis here is thatwords sharing many neighbors should usually beobserved with the same part-of-speech and gethigh weights in the second order graph.
In figure 9,three clusters are obtained that correspond todifferent parts-of-speech (POS).
(a)             (b)Figure 9: Bi-partite neighboring co-occurrencegraph (a) and second-order graph on neighboringco-occurrences (b) clustered with CW.To test this on a large scale, we computed thesecond-order similarity graph for the BritishNational Corpus (BNC), excluding the mostfrequent 2000 words and drawing edges betweenwords if they shared at least four left and rightneighbors.
The clusters are checked against alexicon that contains the most frequent tag for eachword in the BNC.
The largest clusters arepresented in table 2 .size tags:count sample words18432 NN:17120AJ: 631secret, officials, transport,unemployment, farm, county,wood, procedure, grounds, ...4916 AJ: 4208V: 343busy, grey, tiny, thin, sufficient,attractive, vital, ...4192 V: 3784AJ: 286filled, revealed,  experienced,learned, pushed, occurred, ...3515 NP: 3198NN: 255White, Green, Jones, Hill, Brown,Lee, Lewis, Young, ...2211 NP: 1980NN: 174Ian, Alan, Martin, Tony, Prince,Chris, Brian, Harry, Andrew,111  122422111111left right78Christ, Steve, ...1855 NP: 1670NN: 148Central, Leeds, Manchester,Australia,  Yorkshire, Belfast,Glasgow, Middlesbrough,  ...Table 2: the largest clusters from partitioning thesecond order graph with CW.In total, CW produced 282 clusters, of which 26exceed a size of 100.
The weighted average ofcluster purity (i.e.
the number of predominant tagsdivided by cluster size) was measured at 88.8%,which exceeds significantly the precision of 53%on word type as reported by Sch?tze (1995) on arelated task.
How to use this kind of word clustersto improve the accuracy of POS-taggers is outlinedin (Ushioda, 1996).4.3 Word Sense InductionThe task of word sense induction (WSI) is to findthe different senses of a word.
The number ofsenses is not known in advance, therefore has to bedetermined by the method.Similar to the approach as presented in (Dorowand Widdows, 2003) we construct a word graph.While there, edges between words are drawn iffwords co-occur in enumerations, we use the co-occurrence graph.
Dorow and Widdows construct agraph for a target word w by taking the sub-graphinduced by the neighborhood of w (without w) andclustering it with MCL.
We replace MCL by CW.The clusters are interpreted as representations ofword senses.To judge results, the methodology of (Bordag,2006) is adopted: To evaluate word senseinduction, two sub-graphs induced by theneighborhood of different words are merged.
Thealgorithm's ability to separate the merged graphinto its previous parts can be measured in anunsupervised way.
Bordag defines four measures:?
retrieval precision (rP): similarity of thefound sense with the gold standard sense?
retrieval recall (rR): amount of words thathave been correctly assigned to the goldstandard sense?
precision (P): fraction of correctly founddisambiguations?
recall (R): fraction of correctly foundsensesWe used the same program to compute co-occurrences on the same corpus (the BNC).Therefore it is possible to directly compare ourresults to Bordag?s, who uses a triplet-basedhierarchical graph clustering approach.
Themethod was chosen because of its appropriatenessfor unlabelled data: without linguistic pre-processing like tagging or parsing, only thedisambiguation mechanism is measured and notthe quality of the preprocessing steps.
We providescores for his test 1 (word classes separately) andtest 3 (words of different frequency bands).
Datawas obtained from BNC's raw text; evaluation wasperformed for 45 test words.% (Bordag, 2006) Chinese WhispersPOS P R rP rR P R rP rRN 87.0 86.7 90.9 64.2 90.0 79.5 94.8 71.3V 78.3 64.3 80.2 55.2 77.6 67.1 87.3 57.9A 88.6 71.0 88.0 65.4 92.2 61.9 89.3 71.9Table 3: Disambiguation results in % dependent onword class (nouns, verbs, adjectives)% (Bordag, 2006) Chinese Whispersfreq P R rP rR P R rP rRhigh 93.7 78.1 90.3 80.7 93.7 72.9 95.0 73.8med 84.6 85.2 89.9 54.6 80.7 83.8 91.0 55.7low 74.8 49.5 71.0 41.7 74.1 51.4 72.9 56.2Table 4: Disambiguation results in % dependent onfrequencyResults (tables 3 and 4) suggest that bothalgorithms arrive at about equal overallperformance (P and R).
Chinese Whispersclustering is able to capture the same informationas a specialized graph-clustering algorithm forWSI, given the same input.
The slightly superiorperformance on rR and rP indicates that CW leavesfewer words unclustered, which can beadvantageous when using the clusters as clues inword sense disambiguation.5 ConclusionChinese Whispers, an efficient graph-clusteringalgorithm was presented and described in theoryand practice.
Experiments with synthetic graphsshowed that for small graphs, results can beinconclusive due to its non-deterministic nature.But while there exist plethora of clusteringapproaches that can deal well with small graphs,the power of CW lies in its capability of handlingvery large graphs in reasonable time.
The79application field of CW rather lies in size regions,where other approaches?
solutions are intractable.On the NLP data discussed, CW performsequally or better than other clustering algorithms.As CW ?
like other graph clustering algorithms ?chooses the number of classes on its own and canhandle clusters of different sizes, it is especiallysuited for NLP problems, where class distributionsare often highly skewed and the number of classes(e.g.
in WSI) is not known beforehand.To relate the partitions, it is possible to set up ahierarchical version of CW in the following way:The nodes of equal class are joined to hyper-nodes.Edge weights between hyper-nodes are setaccording to the number of inter-class edgesbetween the corresponding nodes.
This results inflat hierarchies.In further works it is planned to apply CW toother graphs, such as the co-citation graph ofCiteseer, the co-citation graph of web pages andthe link structure of Wikipedia.AcknowledgementsThanks go to Stefan Bordag for kindlyproviding his WSI evaluation framework.
Further,the author would like to thank Sebastian Gottwaldand Rocco Gwizdziel for a platform-independentGUI implementation of CW, which is available fordownload from the author?s homepage.ReferencesF.
Amblard.
2002.
Which ties to choose?
A survey ofsocial networks models for agent-based socialsimulations.
In Proc.
of the 2002 SCS InternationalConference On Artificial Intelligence, Simulationand Planning in High Autonomy Systems, pp.253-258, Lisbon, Portugal.C.
Biemann, S. Bordag, G. Heyer, U. Quasthoff,  C.Wolff.
2004.
Language-independent Methods forCompiling Monolingual Lexical Data, Proceedingsof CicLING 2004, Seoul, Korea and Springer LNCS2945, pp.
215-228, Springer,  Berlin HeidelbergB.
Bollob?s.
1998.
Modern graph theory, GraduateTexts in Mathematics, vol.
184, Springer, New YorkS.
Bordag.
2006.
Word Sense Induction: Triplet-BasedClustering and Automatic Evaluation.
Proceedings ofEACL-06.
TrentoC.
Biemann and S. Teresniak.
2005.
Disentangling fromBabylonian Confusion ?
Unsupervised LanguageIdentification.
Proceedings of CICLing-2005,Mexico City, Mexico and Springer LNCS 3406, pp.762-773S.
van Dongen.
2000.
A cluster algorithm for graphs.Technical Report INS-R0010, National ResearchInstitute for Mathematics and Computer Science inthe Netherlands, Amsterdam.T.
Dunning.
1993.
Accurate Methods for the Statisticsof Surprise and Coincidence, ComputationalLinguistics 19(1), pp.
61-74M.
Ester, H.-P. Kriegel, J. Sander and X. Xu.
1996.
ADensity-Based Algorithm for Discovering Clusters inLarge Spatial Databases with Noise.
In Proceedingsof the 2nd Int.
Conf.
on Knowledge Discovery andDatamining (KDD'96) Portland, USA, pp.
291-316.B.
Dorow and D. Widdows.
2003.
Discovering Corpus-Specific Word Senses.
In EACL-2003 ConferenceCompanion (research notes and demos), pp.
79-82,Budapest, HungaryR.
Ferrer-i-Cancho and R.V.
Sole.
2001.
The smallworld of human language.
Proceedings of The RoyalSociety of London.
Series B, Biological Sciences,268(1482):2261-2265H.
Sch?tze.
1995.
Distributional part-of-speechtagging.
In EACL 7, pages 141?148J.
?
?ma and S.E.
Schaeffer.
2005.
On the np-completeness of some graph cluster measures.Technical Report cs.CC/0506100, arXiv.org e-Printarchive, http://arxiv.org/.B.
Stein and O. Niggemann.
1999.
On the Nature ofStructure and Its Identification.
Proceedings ofWG'99, Springer LNCS 1665, pp.
122-134, SpringerVerlag HeidelbergM.
Steyvers, J.
B. Tenenbaum.
2005.
The large-scalestructure of semantic networks: statistical analysesand a model of semantic growth.
Cognitive Science,29(1).Ushioda, A.
(1996).
Hierarchical clustering of wordsand applications to NLP tasks.
In Proceedings of theFourth Workshop on Very Large Corpora, pp.
28-41.Somerset, NJ, USAD.
J. Watts.
1999.
Small Worlds: The Dynamics ofNetworks Between Order and Randomness, PrincetonUniv.
Press, Princeton, USAZ.
Wu and R. Leahy (1993): An optimal graph theoreticapproach to data clustering: Theory and itsapplication to image segmentation.
IEEETransactions on Pattern Analysis and MachineIntelligence80
