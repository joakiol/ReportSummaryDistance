Proceedings of Third Workshop on Semantic Web and Information Extraction, pages 9?16,Dublin, Ireland, 24 August, 2014.Information Extraction for Social MediaMena B. HabibChair DatabasesUniversity of Twentem.b.habib@ewi.utwente.nlMaurice van KeulenChair DatabasesUniversity of Twentem.vankeulen@utwente.nlAbstractThe rapid growth in IT in the last two decades has led to a growth in the amount of informationavailable online.
A new style for sharing information is social media.
Social media is a contin-uously instantly updated source of information.
In this position paper, we propose a frameworkfor Information Extraction (IE) from unstructured user generated contents on social media.
Theframework proposes solutions to overcome the IE challenges in this domain such as the shortcontext, the noisy sparse contents and the uncertain contents.
To overcome the challenges facingIE from social media, State-Of-The-Art approaches need to be adapted to suit the nature of socialmedia posts.
The key components and aspects of our proposed framework are noisy text filtering,named entity extraction, named entity disambiguation, feedback loops, and uncertainty handling.1 IntroductionThe rapid growth in IT in the last two decades has led to a growth in the amount of information availableon the World Wide Web.
A new style for exchanging and sharing information is social media.
Socialmedia refers to the means of interaction among people in which they create, share, and exchange infor-mation and ideas in virtual communities and networks (like Twitter and Facebook).
According to CNN1,more Americans get their news from the Internet than from newspapers or radio, and three-fourths saythey hear of news via e-mail or updates on social media sites.
Social media, in many cases, provide moreup-to-date information than conventional sources like online news.
To make use of this vast amountof information, it is required to extract structured information out of these heterogeneous unstructuredinformation.
Information Extraction (IE) is the research field that enables the use of such a vast amountof unstructured distributed information in a structured way.
IE systems analyse human language textin order to extract information about different types of events, entities, or relationships.
Structured in-formation could be stored in Knowledge-bases (KB) which hold facts and relations extracted from thefree style text.
A KB is an information repository that provides a means for information to be collected,organized, shared, searched and utilized.
It can be either machine-readable or intended for human use.In this paper, we introduce a framework for IE from unstructured user generated contents on socialmedia.
Although IE is a field of research that has been studied for long time, there is very few workdone on that field for social media contents.
(Bontcheva et al., 2013) proposed TwitIE, an open-sourceNLP pipeline customised to microblog text.
However, TwitIE doesn?t provide mechanisms for messagesfiltering or named entity disambiguation or relation/fact extraction.
All efforts on IE field focus on factsextraction from encyclopaedias like Wikipedia (Suchanek et al., 2007; Auer and Lehmann, 2007), orfrom web pages (Nakashole et al., 2011; Carlson et al., 2010; Kushmerick et al., 1997; Crescenzi et al.,2001).IE from text is an important task in text mining.
The general goal of information extraction is todiscover structured information from unstructured or semi-structured text.
For example, given the tweetsshown in figure 1, we can extract the following information:1http://edition.cnn.com/2010/TECH/03/01/social.network.news/index.html9(a) Example 1.
(b) Example 2.
(c) Example 3.Figure 1: Tweets examplesExample (1):Called(U.S. state of Ohio, Buckeye State),FoundedIn(The Ohio State University, 1870).Example (2):SignedFor(Nacer Chadli (the football player), Tottenham HotspurFootball Club).Example (3):Fire(1600 Belmont Avenue, Fort Worth, TX),Fire(2900 Avenue G., Fort Worth, TX).As we can see in the examples, IE can be applied for open or closed domain.
Open IE is to extractall possible relations and facts stated in a post as in examples 1 and 2.
Closed domain IE is to extractfacts for a specific target domain or fill in predefined templates like example 3.
Other meta data couldbe extracted like the time or the source of the extracted fact.
This could help in improving the precisionof the extraction process.
For instance, in the 3rd example, it is not stated where exactly is the ?1600Belmont Avenue?
or ?2900 Avenue G.?.
We could infer this extra knowledge from the sourceof the tweet ?Fort Worth Fire Dept?.
Same with example 2, the word ?Tottenham?
is am-biguous.
Further information about the entity ?Nacer Chadli?
should help to link ?Tottenham?
to?Tottenham Hotspur Football Club?.2 ChallengesApplication of the State-Of-The-Art approaches on social media is not reasonable for the followingchallenges:?
Informal language: Posted texts are noisy and written in an informal setting, include misspellings,lack punctuation and capitalisation, use non-standard abbreviations, and do not contain grammati-cally correct sentences.
Traditional KB construction approaches rely mostly on capitalization and10Part-Of-Speech tags to extract the named entities.
The lack of such features in social media postsmakes the IE task more challenging.?
Short context: There is a post length limit on some social media networks like Twitter.
Thislimit forces the users to use more abbreviations to express more information in their posts.
Theshortness of the posts makes it more challenging to disambiguate mentioned entities and to resolveco-references among tweets.?
Noisy sparse contents: The users?
posts on social media are not always important nor containuseful information.
Around 40% of twitter messages content are pointless babble2.
Filtering is apre-processing step that is required to purify the input posts stream.?
Information about non-famous entities: The IE State-Of-The-Art approaches link the entitiesinvolved in the extracted information to a KB.
However, people normally use social media to expressinformation about themselves or about some small local events (street festival or accident) and thusthe involved entities are not contained in a KB.
New ways of entity linkage need to be introducedto suit IE from social media posts.?
Uncertain contents: Of course not every available information is trustworthy.
In addition to errorsthat may take place during the IE process, information contained in users?
contributions is oftenpartial, subject to evolution over time, in conflict with other sources, and sometimes untrustworthy.It is required to handle the uncertainty involved in the extracted facts.3 The State-Of-The-ArtIn order to extract information from text, a set of subtasks has to be applied on the input text.
Figure 2shows the subtasks modules of a traditional IE system.
Those modules are described according to theState-Of-The-Art IE approaches as follows:?
Named Entity Extraction: A named entity is a sequence of words that designates some real worldentity (e.g.
?California?, ?Steve Jobs?
and ?Apple Inc.?).
The task of named entity extraction (NEE),is to identify named entities from free-form text.
This task cannot be simply accomplished by stringmatching against pre-compiled gazetteers because named entities of a given entity type usually donot form a closed set and therefore any gazetteer would be incomplete.
NEE approaches mainlyuse capitalization features and Part-Of-Speech tags for recognizing named entities.
Part-Of-Speech(POS) tagging is the process of marking up a word in a text (corpus) as corresponding to a particularPart-Of-Speech, based on both its definition, as well as its context (i.e.
relationship with adjacentand related words in a phrase, sentence, or paragraph).
A simplified form of this is commonly taughtto school-age children, in the identification of words as nouns, verbs, adjectives, adverbs, etc.?
Named Entity Disambiguation: In natural language processing, named entity disambiguation(NED) or entity linking is the task of determining the identity of entities mentioned in text.
Forexample, to link the mention ?California?
to the Wikipedia article ?http://en.wikipedia.org/wiki/California?.
It is distinct from named entity extraction (NEE) in that it identifiesnot the occurrence of names but their reference.
NED needs a KB of entities to which names canbe linked.
A popular choice for entity linking on open domain text is Wikipedia (Cucerzan, 2007;Hoffart et al., 2011).?
Fact Extraction: In open IE, the goal of the fact extraction (FE) module is to detect and characterizethe semantic relations between entities in text or relations between entities and values.
In closeddomain IE, the goal is to fill in a predefined template using the extracted named entities.2http://web.archive.org/web/20110715062407/www.pearanalytics.com/blog/wp-content/uploads/2010/05/Twitter-Study-August-2009.pdf11Figure 2: Traditional IE framework versus our proposed IE framework.4 Proposed FrameworkTo overcome the challenges facing IE from social media, State-Of-The-Art approaches need to beadapted to suit the nature of social media posts.
Here, we describe the key components and aspectsof our proposed framework (see figure 2) and show how it would overcome the challenges.?
Noisy Text Filtering: There are millions of social media posts every day.
For example, the averagenumber of tweets exceeds 140 million tweet per day sent by over 200 million users around theworld.
These numbers are growing exponentially3.
This huge number of posts not always containsuseful information about users, locations, events, etc.
It is required to filter non-informative posts.Filtering could be done based on domain or language or other criteria to make sure to keep onlyrelevant posts that contains information about the domain need to be processed.
For example, ifwe want to extract the results of all the football World Cup matches from tweets, we need to filtermillions of tweets to get only the subset of tweets that contain information about results of matches,note that even this subset may contains predicted results or results changing during the matches.?
Named Entity Extraction: With the lack of formal writing style, we need new approaches for NEEthat don?t rely heavily on syntactic features like capitalization and POS.
In (Habib et al., 2013), weparticipated in a challenge to extract named entities from microposts of tweets, we proposed a newapproach that combines State-Of-The-Art techniques with clues derived from disambiguation stepto detect named entities.
Our system named to be the best among all the challenge participants(Basave et al., 2013).?
Named Entity Disambiguation: As stated in the State-Of-The-Art section, researchers normallylink entities to Wikipedia articles or to KB entries.
For social media posts, sometimes this is not3http://www.marketinggum.com/twitter-statistics-2011-updated-stats/12(a) Example 4.
(b) Example 5.Figure 3: Tweets examplespossible as many of the mentioned entities cannot be linked to Wikipedia articles or a KB entries.However, normally users have home pages or profiles on a social media network.
Furthermore,festivals and local events also commonly have home pages representing these events.
In (Habib andvan Keulen, 2013), we proposed an open world approach for NED for tweets.
Named entities aredisambiguated by linking them to a home page or a social network profile page in case they don?thave a Wikipedia article.
Target tweets (tweets revolving around same event) are used to enrich thetweet context and hence to improve the effectiveness of finding the correct entity page.
Other metadata from users profiles could also be used to improve the disambiguation process.?
Feedback Loops: In figure 2, we can see, in the traditional IE framework, the pipeline of thesubtasks.
Each subtask processes the input and generates an output and passes this output to thenext subtask.
There is no possibility of modifying or refining the output of one subtask once it isalready generated.
In our framework, feedback plays a key role in the system.
Every subtask givesa feedback to the preceding subtask which allows for possibility of iterations of refinement (Habiband van Keulen, 2012).
For example, if the NEE module extracted the mention ?Apple?.
Andwhen NED module tries to disambiguate the extracted mention, it finds that it could not be linkedto any entity.
This means that most probably this mention ?Apple?
refers to the fruit rather thanthe company.
In traditional approaches, such feedback cannot be passed, and the NED has to finda page to link the extracted mention anyway.
Furthermore, as ?Apple?
is not considered a namedentity anymore this may affect the decision made that this piece of text in non-informative and thusshould be filtered.
This is typically how human beings interpret text.
In (Habib et al., 2014), weapplied the proposed feedback loop on the #Microposts 2014 Named Entity Extraction and LinkingChallenge.
Our system is ranked second among all the challenge participants (Cano Basave et al.,2014).Similarly, the feedback loop takes place between the FE and the NED modules.
This feedback helpsresolving errors that took place earlier in the disambiguation step.
For example in figure 3a, onemight interpret that the tweet refers to a match of ?FC Twente?
versus ?Ajax Amsterdam?
in theDutch football league.
Unfortunately, this turns to be a wrong assumption after checking the tweetin figure 3b which shows that the match was between ?FC Tygerberg?
and ?Ajax Cape Town?
in theSouth African second division football league.
A feedback from the FE module should trigger andcorrect the wrong decision made earlier in the NED module.
It is also possible that the FE modulesends a feedback message to the noisy text filtering module that the message is non-informative ifit failed to extract the required information or if the extracted fact contradicts other facts or rules.For example, if we want to extract facts about the football World Cup, and we found a tweet thecontains a fact about football club (not national team) then a feedback message is sent back to thenoisy text filtering module to mark this tweet as irrelevant one.?
Uncertainty Handling: As mentioned in the challenges, the information contained in the socialmedia posts involves high degree uncertainty due to many reason.
We envision an approach thatfundamentally treats annotations and extracted information as uncertain throughout the process.13(Goujon, 2009) models this uncertainty in a fuzzy way, however we believe that a probabilistic ap-proach would be a better solution to handle such uncertainty.
Probabilistic knowledge-bases (PKB)are KBs where each fact is associated with a probability indicating how trustworthy is this fact.Probabilities are updated according to many factors like time, users, contradiction or compatibilitywith other facts, etc.Using the same example (figure 3a) mentioned above, the mention ?FCT?
is linked to ?FC Twente?with some certainty confidence.
This probability should be adjusted after processing the secondtweet shown in figure 3b which holds a contradicting fact about the mention ?FCT?.
Furthermore, anew fact is added to the KB indicating that ?FCT?
is linked to ?FC Tygerberg?.
The benefit of usinga PKB is that we can keep both interpretations ?FC Twente?
and ?FC Tygerberg?
with differentprobabilities assigned to them.
Using a PKB, all information is preserved.Another source of uncertainty is the knowledge updating.
One true fact at certain point of time maybe wrong at a later point of time.
Scores of sport games change over time.
Twitter users normallytweet about the score during and after the game.
They may also write their predictions on the gameprior to the game itself.
A probabilistic model should be developed to handle those uncertaintiesusing evidences like number of tweets with the same extracted result, number of re-tweets, time ofthe tweets, last extracted result about the game, etc.?
Modules Portability: Each module from our proposed framework could be customized and reusedindividually or embedded inside other frameworks.
For example, NEE and NED modules could beused in a sentiment analysis system that measures the users opinions towards some product.
Noisytext filtering could be embedded inside a search engine for social media posts.5 Knowledge exchange and impactThe aim of this position paper is to propose a framework for information extraction from unstructureduser generated contents on social media.
IE systems analyse human language text in order to extractinformation about different types of events, entities, or relationships.
Structured information could bestored in KB which hold facts and relations extracted from the free style text.
A KB is a special kind ofdatabase for knowledge management.
A KB is an information repository that provides a means for infor-mation to be collected, organized, shared, searched and utilized.
Information extraction has applicationsin a wide range of domains.
There is many stakeholders that would benefit from such framework.
Here,we give some examples for applications of information extraction:?
Financial experts always look for specific information to help their decision making.
Social mediais a very important source of information about shareholders attitudes and behaviours.
For example,a finance company may need to know the shareholders reaction towards some political action.
Au-tomatically finding such information from users posts on social media requires special informationextraction technologies to analyse social media streams and capture such information at runtime.?
Security agencies normally analyse large amounts of text manually to search for information aboutpeople involved in criminal or terrorism activities.
Social media is a continuously instantly updatedsource of information.
Football hooligans sometimes start their fight electronically on social medianetworks even before the sport event.
This information could be helpful to take actions to preventsuch violent, and destructive behaviours.?
With the fast growth of the Web, search engines have become an integral part of people?s dailylives, and users?
search behaviours are much better understood now.
Search based on bag-of-wordrepresentation of documents provides less satisfactory results for the new challenges and demands.More advanced search problems such as entity search, and question answering can provide userswith better search experience.
To facilitate these search capabilities, information extraction is oftenneeded as a pre-processing step to enrich document representation or to populate an underlyingdatabase.14Our main goal of this proposal is to provide an open source set of portable and customizable modules thatcan be used by different stakeholders with different application needs on social media contents.
Opensource software is a computer software with its source code made available and licensed with a license inwhich the copyright holder provides the rights to study, change and distribute the software to anyone andfor any purpose.
This enables the ICT community from not only using but also developing and extendingthe system according to their needs.
Individuals and organizations always choose open source softwarefor their zero cost, and its adaptability.Reusability would be a key feature in our framework design.
In software industry, reusability is thelikelihood that a part of a system can be used again to add new functionalities with slight or no modifica-tion.
Reusable modules reduce implementation time and effort.
As an example for possible contributionto the society, we contribute to the TEC4SE project4.
The aim of the project is to improve the operationaldecision-making within the security domain by gathering as much information available from differentsources (like cameras, police officers on field, or social media posts).
Then these information is linkedand relationships between different information streams are found.
The result is a good overview of whatis happening in the field of security in the region.
Our contribution to this project to filter twitter streammessages and enrich it by extracting named entities at run time.
It will be more valuable to this projectto complete the whole IE process by building a complete KB from the extracted information for furtheror later investigations.6 ConclusionIE for social media is an emerging field of research.
The noisy contents, shortness of posts, informalityof used language, and the uncertainty involved, add more challenges to IE for social media over those offormal news articles.
In this paper we propose a framework to cope with those challenges through setof portable modules.
Messages filtering, feedback loops, and uncertainty handling are the key aspects ofour framework.ReferencesS?oren Auer and Jens Lehmann.
2007.
What have innsbruck and leipzig in common?
extracting semantics fromwiki content.
In Proceedings of the 4th European Conference on The Semantic Web: Research and Applications,ESWC ?07, pages 503?517, Berlin, Heidelberg.
Springer-Verlag.Amparo E. Cano Basave, Matthew Rowe, Milan Stankovic, and Aba-Sah Dadzie, editors.
2013.
Proceedings,Concept Extraction Challenge at the 3rd Workshop on Making Sense of Microposts (#MSM2013): Big thingscome in small packages, Rio de Janeiro, Brazil, 13 May 2013, May.Kalina Bontcheva, Leon Derczynski, Adam Funk, Mark Greenwood, Diana Maynard, and Niraj Aswani.
2013.Twitie: An open-source information extraction pipeline for microblog text.
In In Proceedings of the Interna-tional Conference on Recent Advances in Natural Language Processing.
Association for Computational Lin-guistics.Amparo Elizabeth Cano Basave, Giuseppe Rizzo, Andrea Varga, Matthew Rowe, Milan Stankovic, and Aba-SahDadzie.
2014.
Making sense of microposts (#microposts2014) named entity extraction & linking challenge.
In4th Workshop on Making Sense of Microposts (#Microposts2014), pages 54?60.Andrew Carlson, Justin Betteridge, Richard C. Wang, Estevam R. Hruschka, Jr., and Tom M. Mitchell.
2010.Coupled semi-supervised learning for information extraction.
In Proceedings of the Third ACM InternationalConference on Web Search and Data Mining, WSDM ?10, pages 101?110, New York, NY, USA.
ACM.Valter Crescenzi, Giansalvatore Mecca, and Paolo Merialdo.
2001.
Roadrunner: Towards automatic data extrac-tion from large web sites.
In Proceedings of the 27th International Conference on Very Large Data Bases,VLDB ?01, pages 109?118, San Francisco, CA, USA.
Morgan Kaufmann Publishers Inc.Silviu Cucerzan.
2007.
Large-scale named entity disambiguation based on Wikipedia data.
In Proceedings ofthe 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational NaturalLanguage Learning (EMNLP-CoNLL), pages 708?716, Prague, Czech Republic, June.
Association for Compu-tational Linguistics.4http://www.tec4se.nl/15B?en?edicte Goujon.
2009.
Uncertainty detection for information extraction.
In RANLP, pages 118?122.Mena B. Habib and Maurice van Keulen.
2012.
Improving toponym disambiguation by iteratively enhancingcertainty of extraction.
In Proceedings of the 4th International Conference on Knowledge Discovery and Infor-mation Retrieval, KDIR 2012, Barcelona, Spain, pages 399?410, Spain, October.
SciTePress.Mena B. Habib and Maurice van Keulen.
2013.
A generic open world named entity disambiguation approach fortweets.
In Proceedings of the 5th International Conference on Knowledge Discovery and Information Retrieval,KDIR 2013, Vilamoura, Portugal, pages 267?276, Portugal, September.
SciTePress.Mena B. Habib, Maurice Van Keulen, and Zhemin Zhu.
2013.
Concept extraction challenge: University of Twenteat #msm2013.
In Basave et al.
(Basave et al., 2013), pages 17?20.Mena B. Habib, Maurice van Keule, and Zhemin Zhu.
2014.
Named entity extraction and linking challenge:University of twente at #microposts2014.
In 4th Workshop on Making Sense of Microposts (#Microposts2014),pages 64?65.Johannes Hoffart, Mohamed Amir Yosef, Ilaria Bordino, Hagen F?urstenau, Manfred Pinkal, Marc Spaniol, BilyanaTaneva, Stefan Thater, and Gerhard Weikum.
2011.
Robust disambiguation of named entities in text.
InProceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP ?11, pages782?792, Stroudsburg, PA, USA.
Association for Computational Linguistics.Nicholas Kushmerick, Daniel S. Weld, and Robert Doorenbos.
1997.
Wrapper induction for information extrac-tion.
In Proc.
IJCAI-97.Ndapandula Nakashole, Martin Theobald, and Gerhard Weikum.
2011.
Scalable knowledge harvesting with highprecision and high recall.
In Proceedings of the Fourth ACM International Conference on Web Search and DataMining, WSDM ?11, pages 227?236, New York, NY, USA.
ACM.Fabian M. Suchanek, Gjergji Kasneci, and Gerhard Weikum.
2007.
Yago: A core of semantic knowledge.
InProceedings of the 16th International Conference on World Wide Web, WWW ?07, pages 697?706, New York,NY, USA.
ACM.16
