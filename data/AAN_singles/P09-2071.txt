Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 281?284,Suntec, Singapore, 4 August 2009.c?2009 ACL and AFNLPEfficient Inference of CRFs for Large-Scale Natural Language DataMinwoo Jeong?
?Chin-Yew Lin?Gary Geunbae Lee?
?Pohang University of Science & Technology, Pohang, Korea?Microsoft Research Asia, Beijing, China?
{stardust,gblee}@postech.ac.kr?cyl@microsoft.comAbstractThis paper presents an efficient inference algo-rithm of conditional random fields (CRFs) forlarge-scale data.
Our key idea is to decomposethe output label state into an active set and aninactive set in which most unsupported tran-sitions become a constant.
Our method uni-fies two previous methods for efficient infer-ence of CRFs, and also derives a simple butrobust special case that performs faster thanexact inference when the active sets are suffi-ciently small.
We demonstrate that our methodachieves dramatic speedup on six standard nat-ural language processing problems.1 IntroductionConditional random fields (CRFs) are widely used innatural language processing, but extending them tolarge-scale problems remains a significant challenge.For simple graphical structures (e.g.
linear-chain), anexact inference can be obtained efficiently if the num-ber of output labels is not large.
However, for largenumber of output labels, the inference is often pro-hibitively expensive.To alleviate this problem, researchers have begun tostudy the methods of increasing inference speeds ofCRFs.
Pal et al (2006) proposed a Sparse Forward-Backward (SFB) algorithm, in which marginal distribu-tion is compressed by approximating the true marginalsusing Kullback-Leibler (KL) divergence.
Cohn (2006)proposed a Tied Potential (TP) algorithm which con-strains the labeling considered in each feature function,such that the functions can detect only a relatively smallset of labels.
Both of these techniques efficiently com-pute the marginals with a significantly reduced runtime,resulting in faster training and decoding of CRFs.This paper presents an efficient inference algorithmof CRFs which unifies the SFB and TP approaches.
Wefirst decompose output labels states into active and in-active sets.
Then, the active set is selected by feasibleheuristics and the parameters of the inactive set are helda constant.
The idea behind our method is that not allof the states contribute to the marginals, that is, only a?Parts of this work were conducted during the author?sinternship at Microsoft Research Asia.small group of the labeling states has sufficient statis-tics.
We show that the SFB and the TP are special casesof our method because they derive from our unified al-gorithm with a different setting of parameters.
We alsopresent a simple but robust variant algorithm in whichCRFs efficiently learn and predict large-scale naturallanguage data.2 Linear-chain CRFsMany versions of CRFs have been developed for usein natural language processing, computer vision, andmachine learning.
For simplicity, we concentrate onlinear-chain CRFs (Lafferty et al, 2001; Sutton andMcCallum, 2006), but the generic idea described herecan be extended to CRFs of any structure.Linear-chain CRFs are conditional probability dis-tributions over label sequences which are conditionedon input sequences (Lafferty et al, 2001).
Formally,x = {xt}Tt=1and y = {yt}Tt=1are sequences of in-put and output variables.
Respectively, where T is thelength of sequence, xt?
X and yt?
Y where X is thefinite set of the input observations and Y is that of theoutput label state space.
Then, a first-order linear-chainCRF is defined as:p?
(y|x) =1Z(x)T?t=1?t(yt, yt?1,x), (1)where ?tis the local potential that denotes the factorat time t, and ?
is the parameter vector.
Z(x) is apartition function which ensures the probabilities of allstate sequences sum to one.
We assume that the poten-tials factorize according to a set of observation features{?1k} and transition features {?2k}, as follows:?t(yt, yt?1,x) =?1t(yt,x) ?
?2t(yt, yt?1), (2)?1t(yt,x) =e?k?1k?1k(yt,x), (3)?2t(yt, yt?1) =e?k?2k?2k(yt,yt?1), (4)where {?1k} and {?2k} are weight parameters which wewish to learn from data.Inference is significantly challenging both in learn-ing and decoding CRFs.
Time complexity is O(T |Y|2)for exact inference (i.e., forward-backward and Viterbialgorithm) of linear-chain CRFs (Lafferty et al, 2001).The inference process is often prohibitively expensive281when |Y| is large, as is common in large-scale tasks.This problem can be alleviated by introducing approx-imate inference methods based on reduction of thesearch spaces to be explored.3 Efficient Inference Algorithm3.1 MethodThe key idea of our proposed efficient inferencemethod is that the output label state Y can be decom-posed to an active set A and an inactive set Ac.
Intu-itively, many of the possible transitions (yt?1?
yt) donot occur, or are unsupported, that is, only a small partof the possible labeling set is informative.
The infer-ence algorithm need not precisely calculate marginalsor maximums (more generally, messages) for unsup-ported transitions.
Our efficient inference algorithmapproximates the unsupported transitions by assigningthem a constant value.
When |A| < |Y|, both train-ing and decoding times are remarkably reduced by thisapproach.We first define the notation for our algorithm.
LetAibe the active set and Acibe the inactive set of outputlabel i where Yi= Ai?
Aci.
We define Aias:Ai= {j|?
(yt= i, yt?1= j) > ?}
(5)where ?
is a criterion function of transitions (yt?1?yt) and ?
is a hyperparameter.
For clarity, we define thelocal factors as:?1t,i, ?1t(yt= i,x), (6)?2j,i, ?2t(yt?1= j, yt= i).
(7)Note that we can ignore the subscript t at ?2t(yt?1=j, yt= i) by defining an HMM-like model, that is,transition matrix ?2j,iis independent of t.As exact inference, we use the forward-backwardprocedure to calculate marginals (Sutton and McCal-lum, 2006).
We formally describe here an efficientcalculation of ?
and ?
recursions for the forward-backward procedure.
The forward value ?t(i) is thesum of the unnormalized scores for all partial paths thatstart at t = 0 and converge at yt= i at time t. Thebackward value ?t(i) similarly defines the sum of un-normalized scores for all partial paths that start at timet + 1 with state yt+1= j and continue until the endof the sequences, t = T + 1.
Then, we decompose theequations of exact ?
and ?
recursions as follows:?t(i) = ?1t,i???j?Ai(?2j,i?
?
)?t?1(j) + ??
?, (8)?t?1(j) =?i?Aj?1t,i(?2j,i?
?
)?t(i) + ?
?i?Y?1t,i?t(i),(9)where ?
is a shared transition parameter value for setAci, that is, ?2j,i= ?
if j ?
Aci.
Note that?i?t(i) = 1(Sutton and McCallum, 2006).
Because all unsup-ported transitions in Aciare calculated simultaneously,the complexities of Eq.
(8) and (9) are approximatelyO(T |Aavg||Y|) where |Aavg| is the average number ofstates in the active set, i.e.,1T?Tt=1|Ai|.
The worstcase complexity of our ?
and ?
equations is O(T |Y|2).Similarly, we decompose a ?
recursion for theViterbi algorithm as follows:?t(i) = ?1t,i{max(maxj?Ai?2j,i?t?1(j),maxj?Y?
?t?1(j))},(10)where ?t(i) is the sum of unnormalized scores for thebest-scored partial path that starts at time t = 0 andconverges at yt= i at time t. Because ?
is constant,maxj?Y?t?1(j) can be pre-calculated at time t ?
1.By analogy with Eq.
(8) and (9), the complexity is ap-proximately O(T |Aavg||Y|).3.2 Setting ?
and ?To implement our inference algorithm, we need amethod of choosing appropriate values for the settingfunction ?
of the active set and for the constant value?
of the inactive set.
These two problems are closelyrelated.
The size of the active set affects both the com-plexity of inference algorithm and the quality of themodel.
Therefore, our goal for selecting ?
and ?
isto make a plausible assumption that does not sacrificemuch accuracy but speeds up when applying large statetasks.
We describe four variant special case algorithms.Method 1: We set ?
(i, j) = Z(L) and ?
= 0 whereL is a beam set, L = {l1, l2, .
.
.
, lm} and the sub-partition function Z(L) is approximated by Z(L) ??t?1(j).
In this method, all sub-marginals in the inac-tive set are totally excluded from calculation of the cur-rent marginal.
?
and ?
in the inactive sets are set to 0by default.
Therefore, at each time step t the algorithmprunes all states i in which ?t(i) < ?.
It also generatesa subset L of output labels that will be exploited in nexttime step t + 1.1This method has been derived the-oretically from the process of selecting a compressedmarginal distribution within a fixed KL divergence ofthe true marginal (Pal et al, 2006).
This method mostclosely resembles SFB algorithm; hence we refer an al-ternative of SFB.Method 2: We define ?
(i, j) = |?2j,i?1| and ?
= 1.In practice, unsupported transition features are not pa-rameterized2; this means that ?k= 0 and ?2j,i= 1if j ?
Aci.
Thus, this method estimates nearly-exact1In practice, dynamically selecting L increases the num-ber of computations, and this is the main disadvantage ofMethod 1.
However, in inactive sets ?t?1(j) = 0 by de-fault; hence, we need not calculate ?t?1(j).
Therefore, itcounterbalances the extra computations in ?
recursion.2This is a common practice in implementation of inputand output joint feature functions for large-scale problems.This scheme uses only supported features that are used atleast once in the training examples.
We call it the sparsemodel.
While a complete and dense feature model may per-282CRFs if the hyperparameter is ?
= 0; hence this cri-terion does not change the parameter.
Although thismethod is simple, it is sufficiently efficient for trainingand decoding CRFs in real data.Method 3: We define ?
(i, j) = Ep??
?2k(i, j)?
whereEp??z?
is an empirical count of event z in training data.We also assign a real value for the inactive set, i.e.,?
= c ?
R, c 6= 0, 1.
The value c is estimated in thetraining phase; hence, c is a shared parameter for theinactive set.
This method is equivalent to TP (Cohn,2006).
By setting ?
larger, we can achieve faster infer-ence, a tradeoff exists between efficiency and accuracy.Method 4: We define the shared parameter as a func-tion of output label y in the inactive set, i.e., c(y).
As inMethod 3, c(y) is estimated during the training phase.When the problem expects different aspects of unsup-ported transitions, this method would be better than us-ing only one parameter c for all labels in inactive set.4 ExperimentWe evaluated our method on six large-scale natu-ral language data sets (Table 1): Penn Treebank3for part-of-speech tagging (PTB), phrase chunk-ing data4(CoNLL00), named entity recognitiondata5(CoNLL03), grapheme-to-phoneme conversiondata6(NetTalk), spoken language understanding data(Communicator) (Jeong and Lee, 2006), and fine-grained named entity recognition data (Encyclopedia)(Lee et al, 2007).
The active set is sufficiently small inCommunicator and Encyclopedia despite their largenumbers of output labels.
In all data sets, we selectedthe current word, ?2 context words, bigrams, trigrams,and prefix and suffix features as basic feature templates.A template of part-of-speech tag features was added forCoNLL00, CoNLL03, and Encyclopedia.
In particu-lar, all tasks except PTB and NetTalk require assigninga label to a phrase rather than to a word; hence, we usedstandard ?BIO?
encoding.
We used un-normalized log-likelihood, accuracy and training/decoding times as ourevaluation measures.
We did not use cross validationand development set for tuning the parameter becauseour goal is to evaluate the efficiency of inference algo-rithms.
Moreover, using the previous state-of-the-artfeatures we expect the achievement of better accuracy.All our models were trained until parameter estima-tion converged with a Gaussian prior variance of 4.During training, a pseudo-likelihood parameter estima-tion (Sutton and McCallum, 2006) was used as an ini-tial weight (estimated in 30 iterations).
We used com-plete and dense input/output joint features for densemodel (Dense), and only supported features that areused at least once in the training examples for sparseform better, the sparse model performs well in practice with-out significant loss of accuracy (Sha and Pereira, 2003).3Penn Treebank3: Catalog No.
LDC99T424http://www.cnts.ua.ac.be/conll2000/chunking/5http://www.cnts.ua.ac.be/conll2003/ner/6http://archive.ics.uci.edu/ml/Table 1: Data sets: number of sentences in the train-ing (#Train) and the test data sets (#Test), and numberof output labels (#Label).
|A?=1avg| denotes the averagenumber of active set when ?
= 1, i.e., the supportedtransitions that are used at least once in the training set.Set #Train #Test #Label |A?=1avg|PTB 38,219 5462 45 30.01CoNLL00 8,936 2,012 22 6.59CoNLL03 14,987 3,684 8 4.13NetTalk 18,008 2,000 51 22.18Communicator 13,111 1,193 120 3.67Encyclopedia 25,348 6,336 279 3.27model (Sparse).
All of our model variants were basedon Sparse model.
For the hyper parameter ?, we empir-ically selected 0.001 for Method 1 (this preserves 99%of probability density), 0 for Method 2, and 4 for Meth-ods 3 and 4.
Note that ?
for Methods 2, 3, and 4 indi-cates an empirical count of features in training set.
Allexperiments were implemented in C++ and executed inWindows 2003 with XEON 2.33 GHz Quad-Core pro-cessor and 8.0 Gbyte of main memory.We first show that our method is efficient for learningCRFs (Figure 1).
In all learning curves, Dense gener-ally has a higher training log-likelihood than Sparse.For PTB and Encyclopedia, results for Dense are notavailable because training in a single machine faileddue to out-of-memory errors.
For both Dense andSparse, we executed the exact inference method.
Ourproposed method (Method 1?4) performs faster thanSparse.
In most results, Method 1 was the fastest, be-cause it was terminated after fewer iterations.
How-ever, Method 1 sometimes failed to converge, for ex-ample, in Encyclopedia.
Similarly, Method 3 and 4could not find the optimal solution in the NetTalk dataset.
Method 2 showed stable results.Second, we evaluated the accuracy and decodingtime of our methods (Table 2).
Most results obtainedusing our method were as accurate as those of Denseand Sparse.
However, some results of Method 1, 3,and 4 were significantly inferior to those of Dense andSparse for one of two reasons: 1) parameter estimationfailed (NetTalk and Encyclopedia), or 2) approximateinference caused search errors (CoNLL00 and Com-municator).
The improvements of decoding time onCommunicator and Encyclopedia were remarkable.Finally, we compared our method with two open-source implementations of CRFs: MALLET7andCRF++8.
MALLET can support the Sparse model, andthe CRF++ toolkit implements only the Dense model.We compared them with Method 2 on the Commu-nicator data set.
In the accuracy measure, the re-sults were 91.56 (MALLET), 91.87 (CRF++), and 91.92(ours).
Our method performs 5?50 times faster fortraining (1,774 s for MALLET, 18,134 s for CRF++,7Ver.
2.0 RC3, http://mallet.cs.umass.edu/8Ver.
0.51, http://crfpp.sourceforge.net/2830 10000 20000 30000 40000?140000?100000Training time (sec)Log?likelihoodSparseMethod 1Method 2Method 3Method 4(a) PTB0 500 1500 2500?10000?6000Training time (sec)Log?likelihoodDenseSparseMethod 1Method 2Method 3Method 4(b) CoNLL000 500 1000 1500?14000?10000?6000?2000Training time (sec)Log?likelihoodDenseSparseMethod 1Method 2Method 3Method 4(c) CoNLL030 1000 3000 5000?41000?39000Training time (sec)Log?likelihoodDenseSparseMethod 1Method 2Method 3Method 4(d) NetTalk0 1000 3000 5000?7500?6500?5500?4500Training time (sec)Log?likelihoodDenseSparseMethod 1Method 2Method 3Method 4(e) Communicator0 100000 200000 300000?30000?20000?10000Training time (sec)Log?likelihoodSparseMethod 1Method 2Method 3Method 4(f) EncyclopediaFigure 1: Result of training linear-chain CRFs: Un-normalized training log-likelihood and training times arecompared.
Dashed lines denote the termination of training step.Table 2: Decoding result; columns are percent accuracy (Acc), and decoding time in milliseconds (Time) measuredper testing example.
???
indicates that the result is significantly different from the Sparse model.
N/A indicatesfailure due to out-of-memory error.MethodPTB CoNLL00 CoNLL03 NetTalk Communicator EncyclopediaAcc Time Acc Time Acc Time Acc Time Acc Time Acc TimeDense N/A N/A 96.1 0.89 95.8 0.26 88.4 0.49 91.6 0.94 N/A N/ASparse 96.6 1.12 95.9 0.62 95.9 0.21 88.4 0.44 91.9 0.83 93.6 34.75Method 1 96.8 0.74 95.9 0.55?94.0 0.24?88.3 0.34 91.7 0.73?69.2 15.77Method 2 96.6 0.92?95.7 0.52 95.9 0.21?87.4 0.32 91.9 0.30 93.6 4.99Method 3 96.5 0.84?94.2 0.51 95.9 0.24?78.2 0.29?86.7 0.30 93.7 6.14Method 4 96.6 0.85?92.1 0.51 95.9 0.24?77.9 0.30 91.9 0.29 93.3 4.88and 368 s for ours) and 7?12 times faster for decod-ing (2.881 ms for MALLET, 5.028 ms for CRF++, and0.418 ms for ours).
This result demonstrates that learn-ing and decoding CRFs for large-scale natural languageproblems can be efficiently solved using our method.5 ConclusionWe have demonstrated empirically that our efficient in-ference method can function successfully, allowing fora significant speedup of computation.
Our method linkstwo previous algorithms, the SFB and the TP.
We havealso showed that a simple and robust variant method(Method 2) is effective in large-scale problems.9Theempirical results show a significant improvement inthe training and decoding speeds especially when theproblem has a large state space of output labels.
Fu-ture work will consider applications to other large-scaleproblems, and more-general graph topologies.9Code used in this work is available athttp://argmax.sourceforge.net/.ReferencesT.
Cohn.
2006.
Efficient inference in large conditional ran-dom fields.
In Proc.
ECML, pages 606?613.M.
Jeong and G. G. Lee.
2006.
Exploiting non-local fea-tures for spoken language understanding.
In Proc.
of COL-ING/ACL, pages 412?419, Sydney, Australia, July.J.
Lafferty, A. McCallum, and F. Pereira.
2001.
Conditionalrandom fields: Probabilistic models for segmenting andlabeling sequence data.
In Proc.
ICML, pages 282?289.C.
Lee, Y. Hwang, and M. Jang.
2007.
Fine-grained namedentity recognition and relation extraction for question an-swering.
In Proc.
SIGIR Poster, pages 799?800.C.
Pal, C. Sutton, and A. McCallum.
2006.
Sparse forward-backward using minimum divergence beams for fast train-ing of conditional random fields.
In Proc.
ICASSP.F.
Sha and F. Pereira.
2003.
Shallow parsing with conditionalrandom fields.
In Proc.
of NAACL/HLT, pages 134?141.C.
Sutton and A. McCallum.
2006.
An introduction to condi-tional random fields for relational learning.
In Lise Getoorand Ben Taskar, editors, Introduction to Statistical Rela-tional Learning.
MIT Press, Cambridge, MA.284
