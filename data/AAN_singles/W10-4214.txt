Towards an Extrinsic Evaluation of Referring Expressionsin Situated DialogsPhilipp SPANGER IIDA Ryu TOKUNAGA Takenobu{philipp,ryu-i,take}@cl.cs.titech.ac.jpTERAI Asuka KURIYAMA Naokoasuka@nm.hum.titech.ac.jp kuriyama@hum.titech.ac.jpTokyo Institute of TechnologyAbstractIn the field of referring expression gener-ation, while in the static domain both in-trinsic and extrinsic evaluations have beenconsidered, extrinsic evaluation in the dy-namic domain, such as in a situated col-laborative dialog, has not been discussedin depth.
In a dynamic domain, a cru-cial problem is that referring expressionsdo not make sense without an appropriatepreceding dialog context.
It is unrealisticfor an evaluation to simply show a humanevaluator the whole period from the be-ginning of a dialog up to the time pointat which a referring expression is used.Hence, to make evaluation feasible it isindispensable to determine an appropriateshorter context.
In order to investigate thecontext necessary to understand a referringexpression in a situated collaborative dia-log, we carried out an experiment with 33evaluators and a Japanese referring expres-sion corpus.
The results contribute to find-ing the proper contexts for extrinsic evalu-tion in dynamic domains.1 IntroductionIn recent years, the NLG community has paid sig-nificant attention to the task of generating referringexpressions, reflected in the seting-up of severalcompetitive events such as the TUNA and GIVE-Challenges at ENLG 2009 (Gatt et al, 2009; By-ron et al, 2009).With the development of increasingly complexgeneration systems, there has been heightened in-terest in and an ongoing significant discussion ondifferent evaluation measures for referring expres-sions.
This discussion is carried out broadly in thefield of generation, including in the multi-modaldomain, e.g.
(Stent et al, 2005; Foster, 2008).!"#$%&!"#$%&'()$)&''($)*$+%"&,#-+."/*+),&#(&'&#),&#(&'-$,$./#&0!"#$%123445678$#0!"#$%10234496:$#0!*,0;<=&(0!"#$%12344>6;)/&$0!"#$%1234456?/,!$#0@A$<B*,2344C6D*<E0@0F$))02344G6H8&(0I$I*,234J46K/()*,#!
"#$%&#234496Figure 1: Overview of recent work on evaluationof referring expressionsFigure 1 shows a schematic overview of recentwork on evaluation of referring expressions alongthe two axes of evaluation method and domain inwhich referring expressions are used.There are two different evaluation methods cor-responding to the bottom and the top of the verti-cal axis in Figure 1: intrinsic and extrinsic eval-uations (Sparck Jones and Galliers, 1996).
In-trinsic methods often measure similarity betweenthe system output and the gold standard corporausing metrics such as tree similarity, string-edit-distance and BLEU (Papineni et al, 2002).
Intrin-sic methods have recently become popular in theNLG community.
In contrast, extrinsic methodsevaluate generated expressions based on an exter-nal metric, such as its impact on human task per-formance.While intrinsic evaluations have been widelyused in NLG, e.g.
(Reiter et al, 2005), (Cahilland van Genabith, 2006) and the competitive 2009TUNA-Challenge, there have been a number ofcriticisms against this type of evaluation.
(Reiterand Sripada, 2002) argue, for example, that gener-ated text might be very different from a corpus butstill achieve the specific communicative goal.An additional problem is that corpus-similaritymetrics measure how well a system reproduceswhat speakers (or writers) do, while for most NLGsystems ultimately the most important considera-tion is its effect on the human user (i.e.
listeneror reader).
Thus (Khan et al, 2009) argues that?measuring human-likeness disregards effective-ness of these expressions?.Furthermore, as (Belz and Gatt, 2008) state?there are no significant correlations between in-trinsic and extrinsic evaluation measures?, con-cluding that ?similarity to human-produced refer-ence texts is not necessarily indicative of qualityas measured by human task performance?.From early on in the NLG community, task-based extrinsic evaluations have been consideredas the most meaningful evaluation, especiallywhen having to convince people in other commu-nities of the usefulness of a system (Reiter andBelz, 2009).
Task performance evaluation is rec-ognized as the ?only known way to measure the ef-fectiveness of NLG systems with real users?
(Re-iter et al, 2003).
Following this direction, theGIVE-Challenges (Koller et al, 2009) at INLG2010 (instruction generation) also include a task-performance evaluation.In contrast to the vertical axis of Figure 1, thereis the horizontal axis of the domain in which refer-ring expressions are used.
Referring expressionscan thus be distinguished according to whetherthey are used in a static or a dynamic domain, cor-responding to the left and right of the horizontalaxis of Figure 1.
A static domain is one such as theTUNA corpus (van Deemter, 2007), which col-lects referring expressions based on a motionlessimage.
In contrast, a dynamic domain comprises aconstantly changing situation where humans needcontext information to identify the referent of a re-ferring expression.Referring expressions in the static domain havebeen evaluated relatively extensively.
A recent ex-ample of an intrinsic evaluation is (van der Sluiset al, 2007), who employed the Dice-coefficientmeasuring corpus-similarity.
There have been anumber of extrinsic evaluations as well, such as(Paraboni et al, 2006) and (Khan et al, 2009), re-spectively measuring the effect of overspecifica-tion on task performance and the impact of gener-ated text on accuracy as well as processing speed.They belong thus in the top-left quadrant of Fig-ure 1.Over a recent period, research in the generationof referring expressions has moved to dynamic do-mains such as situated dialog, e.g.
(Jordan andWalker, 2005) and (Stoia et al, 2006).
However,both of them carried out an intrinsic evaluationmeasuring corpus-similarity or asking evaluatorsto compare system output to expressions used byhuman (the right bottom quadrant in Figure 1).The construction of effective generation sys-tems in the dynamic domain requires the imple-mentation of an extrinsic task performance evalu-ation.
There has been work on extrinsic evalua-tion of instructions in the dynamic domain on theGIVE-2 challenge (Byron et al, 2009), which is atask to generate instructions in a virtual world.
It isbased on the GIVE-corpus (Gargett et al, 2010),which is collected through keyboard interaction.The evaluation measures used are e.g.
the numberof successfully completed trials, completion timeas well as the numbers of instructions the systemsent to the user.
As part of the JAST project, aJoint Construction Task (JCT) puzzle constructioncorpus (Foster et al, 2008) was created which issimilar in some ways in its set-up to the REX-J corpus which we use in the current research.There has been some work on evaluating gener-ation strategies of instructions for a collaborativeconstruction task on this corpus, both consideringintrinsic as well as extrinsic measures (Foster etal., 2009).
Their main concern is, however, the in-teraction between the text structure and usage ofreferring expressions.
Therefore, their ?context?was given a priori.However, as can be seen from Figure 1, in thefield of referring expression generation, while inthe static domain both intrinsic and extrinsic eval-uations have been considered, the question of re-alizing an extrinsic evaluation in the dynamic do-main has not been dealt with in depth by previouswork.
This paper addresses this shortcoming ofprevious work and contributes to ?filling in?
themissing quadrant of Figure 1 (the top-right).The realization of such an extrinsic evaluationfaces one key difficulty.
In a static domain, an ex-trinsic evaluation can be realized relatively easilyby showing evaluators the static context (e.g.
anyimage) and a referring expression, even thoughthis is still costly in comparison to intrinsic meth-ods (Belz and Gatt, 2008).In contrast, an extrinsic evaluation in the dy-namic domain needs to present an evaluator withthe dynamic context (e.g.
a certain length of therecorded dialog) preceding a referring expression.It is clearly not feasible to simply show the wholepreceding dialog; this would make even a verysmall-scale evaluation much too costly.
Thus, inorder to realize a cost-effective extrinsic evalua-tion in a dynamic domain we have to deal with theadditional parameter of time length and content ofthe context shown to evaluators.This paper investigates the context necessary forhumans to understand different types of referringexpressions in a situated domain.
This work thuscharts new territory and contributes to developinga extrinsic evaluation in a dynamic domain.
Sig-nificantly, we consider not only linguistic but alsoextra-linguistic information as part of the context,such as the actions that have been carried out in thepreceding interaction.
Our results indicate that, atleast in this domain, extrinsic evaluation resultsin dynamic domains can depend on the specificamount of context shown to the evaluator.
Basedon the results from our evaluation experiments, wediscuss the broader conclusions to be drawn anddirections for future work.2 Referring Expressions in the REX-JCorpusWe utilize the REX-J corpus, a Japanese corpusof referring expressions in a situated collaborativetask (Spanger et al, 2009a).
It was collected byrecording the interaction of a pair of dialog partic-ipants solving the Tangram puzzle cooperatively.The goal of the Tangram puzzle is to construct agiven shape by arranging seven pieces of simplefigures as shown in Figure 2!"#$%&'#()*"+,-.
!%#+)#Figure 2: Screenshot of the Tangram simulatorIn order to record the precise position of everypiece and every action by the participants, we im-plemented a simulator.
The simulator displays twoareas: a goal shape area, and a working area wherepieces are shown and can be manipulated.We assigned different roles to the two partici-pants of a pair: solver and operator.
The solvercan see the goal shape but cannot manipulate thepieces and hence gives instructions to the opera-tor; by contrast, the operator can manipulate thepieces but can not see the goal shape.
The twoparticipants collaboratively solve the puzzle shar-ing the working area in Figure 2.In contrast to other recent corpora of refer-ring expressions in situated collaborative tasks(e.g.
COCONUT corpus (Di Eugenio et al, 2000)and SCARE corpora (Byron et al, 2005)), inthe REX-J corpus we allowed comparatively largereal-world flexibility in the actions necessary toachieve the task (such as flipping, turning andmoving of puzzle pieces at different degrees), rel-ative to the task complexity.
The REX-J corpusthus allows us to investigate the interaction of lin-guistic and extra-linguistic information.
Interest-ingly, the GIVE-2 challenge at INLG 2010 notesits ?main novelty?
is allowing ?continuous movesrather than discrete steps as in GIVE-1?.
Our workis in line with the broader research trend in theNLG community of trying to get away from sim-ple ?discrete?
worlds to more realistic settings.The REX-J corpus contains a total of 1,444 to-kens of referring expressions in 24 dialogs with atotal time of about 4 hours and 17 minutes.
Theaverage length of each dialog is 10 minutes 43seconds.
The asymmetric data-collection settingencouraged referring expressions from the solver(solver: 1,244 tokens, operator: 200 tokens).
Weexclude from consideration 203 expressions refer-ring to either groups of pieces or whose referentcannot be determined due to ambiguity, thus leav-ing us 1,241 expressions.We identified syntactic/semantic features in thecollected referring expressions as listed in Table 1:(a) demonstratives (adjectives and pronouns), (b)object attribute-values, (c) spatial relations and (d)actions on an object.
The underlined part of theexamples denotes the feature in question.3 Design of Evaluation ExperimentThe aim of our experiment is to investigate the?context?
(content of the time span of the recordedTable 1: Syntactic and semantic features of refer-ring expressions in the REX-J corpusFeature Tokens Example(a) demonstrative 742 ano migigawa no sankakkei(that triangle at the right side)(b) attribute 795 tittyai sankakkei(the small triangle)(c) spatial relations 147 hidari no okkii sankakkei(the small triangle on the left)(d) action-mentioning 85 migi ue ni doketa sankakkei(the triangle you put away tothe top right)interaction prior to the uttering of the referring ex-pression) necessary to enable successful identifi-cation of the referent of a referring expression.Our method is to vary the context presented toevaluators and then to study the impact on humanreferent identification.
In order to realize this, foreach instance of a referring expression, we varythe length of the video shown to the evaluator.!
"#$ %&'()*+,!-./0#!1#$ 234*&,&5,678+*4,9&+:3(;,8+*8,3(,)7*,63<'=8)&+!%#$ >))*+8(?
*,3(?='43(;,)7*,+*5*++3(;,*@A+*663&(,)&,*B8='8)*,!67&9(,3(,+*4#!C#$ D)8+)E+*A*8),F'))&(!G#$ D*=*?)3&(,F'))&(6,!-.H#,8(4,IJ,4&(K),:(&9I.F'))&(,,,,,,,,,,!
"#!1#!%#!C#!G#Figure 3: The interface presented to evaluatorsThe basic procedure of our evaluation experi-ment is as follows:(1) present human evaluators with speech andvideo from a dialog that captures sharedworking area of a certain length previous tothe uttering of a referring expression,(2) stop the video and display as text the nextsolver?s utterance including the referring ex-pression (shown in red),(3) ask the evaluator to identify the referentof the presented referring expression (if theevaluator wishes, he/she can replay the videoas many times as he likes),(4) proceed to the next referring expression (goto (1)).Figure 3 shows a screenshot of the interface pre-pared for this experiment.The test data consists of three types of referringexpressions: DPs (demonstrative pronouns),AMEs (action-mentioning expressions), andOTHERs (any other expression that is neither aDP nor AME, e.g intrinsic attributes and spatialrelations).
DPs are the most frequent type ofreferring expression in the corpus.
AMEs areexpressions that utilize an action on the referentsuch as ?the triangle you put away to the topright?
(see Table 1)1.
As we pointed out in ourprevious paper (Spanger et al, 2009a), they arealso a fundamental type of referring expression inthis domain.The basic question in investigating a suitablecontext is what information to consider about thepreceding interaction; i.e.
over what parameters tovary the context.
In previous work on the gener-ation of demonstrative pronouns in a situated do-main (Spanger et al, 2009b), we investigated therole of linguistic and extra-linguistic information,and found that time distance from the last action(LA) on the referent as well as the last mention(LM) to the referent had a significant influence onthe usage of referring expressions.
Based on thoseresults, we focus on the information on the refer-ent, namely LA and LM.For both AMEs and OTHERs, we only considertwo possibilities of the order in which LM and LAappear before a referring expression (REX), de-pending on which comes first.
These are shown inFigure 4, context patterns (a) LA-LM and (b) LM-LA.
Towards the very beginning of a dialog, somereferring expressions have no LM and LA; thoseexpressions are not considered in this research.All instances of AMEs and OTHERs in our testdata belong to either the LA-LM or the LM-LA1An action on the referent is usually described by a verbas in this example.
However, there are cases with a verb el-lipsis.
While this would be difficult in English, it is naturaland grammatical in Japanese.!
"#$%&'()&*+'()&,($%-./%+ !"#$%&'"()!"#$%&'%($)"**+,-!
"#*+'()&$%&'()&,($%-./%+ !"#$%*'"()!.#$%('%&$)"**+,-!
"#$%&'()&,($%-./%+%*'"()!/#$%('%&0$)"**+,-*+'()&-./%+-./%+Figure 4: Schematic overview of the three contextPatternspattern.
For each of these two context patterns,there are three possible contexts2: Both (includingboth LA and LM), LA/LM (including either LA orLM) and None (including neither).
Depending onthe order of LA and LM prior to an expression,only one of the variations of LA/LM is possible(see Figure 4 (a) and (b)).In contrast, DPs tend to be utilized in a deic-tic way in such situated dialogs (Piwek, 2007).We further noted in (Spanger et al, 2009b), thatDPs in a collaborative task are also frequently usedwhen the referent is under operation.
While theybelong neither to the LA-LM nor the LM-LA pat-tern, it would be inappropriate to exclude thosecases.
Hence, for DPs we consider another situa-tion where the last action on the referent overlapswith the utterance of the DP (Figure 4 (c) LM-LA?pattern).
In this case, we consider an ongoing op-eration on the referent as a ?last action?.
Anotherpeculiarity of the LM-LA?
pattern is that we haveno None context in this case, since there is no wayto show a video without showing LA (the currentoperation).Given the three basic variations of context, werecruited 33 university students as evaluators and2To be more precise, we set a margin at the beginning ofcontexts as shown in Figure 4.divided them equally into three groups, i.e.
11evaluators per group.
As for the referring ex-pressions to evaluate, we selected 60 referring ex-pressions used by the solver from the REX-J cor-pus (20 from each category), ensuring all werecorrectly understood by the operator during therecorded dialog.
We selected those 60 instancesfrom expressions where both LM and LA ap-peared within the last 30 secs previous to the re-ferring expression.
This selection excludes initialmentions, as well as expressions where only LAor only LM exists or they do not appear within 30secs.
Hence the data utilized for this experimentis limited in this sense.
We need further experi-ments to investigate the relation between the timelength of contexts and the accuarcy of evaluators.We will return to this issue in the conclusion.We combined 60 referring expressions and thethree contexts to make the test instances.
Follow-ing the Latin square design, we divided these testinstances into three groups, distributing each ofthe three contexts for every referring expressionto each group.
The number of contexts was uni-formly distributed over the groups.
Each instancegroup was assigned to each evaluator group.For each referring expression instance, werecord whether the evaluator was able to correctlyidentify the referent, how long it took them toidentify it and whether they repeated the video(and if so how many times).Reflecting the distribution of the data availablein our corpus, the number of instances per contextpattern differs for each type of referring expres-sion.
For AMEs, overwhelmingly the last actionon the referent was more recent than the last men-tion.
Hence we have only two LA-LM patternsamong the 20 AMEs in our data.
For OTHERs, thebalance is 8 to 12, with a slight majority of LM-LA patterns.
For DPs, there is a strong tendency touse a DP when a piece is under operation (Spangeret al, 2009b).
Of the 20 DPs in the data, 2 wereLA-LM, 5 were LM-LA pattern while 13 were ofthe LM-LA?
pattern (i.e.
their referents were underoperation at the time of the utterance).
For these13 instances of LM-LA?
we do not have a Nonecontext.The average stimulus times, i.e.
time period ofpresented context, were 7.48 secs for None, 11.04secs for LM/LA and 18.10 secs for Both.Table 2: Accuracy of referring expression identification per type and contextType context pattern\Context None LM/LA Both Increase [None ?
Both](LA-LM) 0.909 0.955 0.955 0.046DP (20/22) (21/22) (21/22)(LM-LA) 0.455 0.783 0.843 0.388(25/55) (155/198) (167/198)Total 0.584 0.800 0.855 0.271(LA-LM) 0.227 0.455 0.682 0.455AME (5/22) (10/22) (15/22)(LM-LA) 0.530 0.859 0.879 0.349(105/198) (170/198) (174/198)Total 0.500 0.818 0.859 0.359(LA-LM) 0.784 0.852 0.943 0.159OTHER (69/88) (75/88) (83/88)(LM-LA) 0.765 0.788 0.879 0.114(101/132) (104/132) (116/132)Total 0.773 0.814 0.905 0.132Overall 0.629 0.811 0.903 0.274(325/517) (535/660) (576/638)4 Results and AnalysisIn this section we discuss the results of our evalua-tion experiment.
In total 33 evaluators participatedin our experiment, each solving 60 problems ofreferent identification.
Taking into account the ab-sence of the None context for the DPs of the LM-LA?
pattern (see (c) in Figure 4), we have 1,815responses to analyze.
We focus on the impact ofthe three contexts on the three types of referringexpressions, considering the two context patternsLA-MA and LM-LA.4.1 Overview of ResultsTable 2 shows the micro averages of the accura-cies of referent identification of all evaluators overdifferent types of referring expressions with differ-ent contexts.
Accuracies increase with an increasein the amount of information in the context; fromNone to Both by between 13.2% (OTHERs) and35.9% (AMEs).
The average increase of accuracyis 27.4%.Overall, for AMEs the impact of the context isthe greatest, while for OTHERs it is the smallest.This is not surprising given that OTHERs tend toinclude intrinsic attributes of the piece and its spa-tial relations, which are independent of the pre-ceding context.
We conducted ANOVA with thecontext as the independent variable, testing its ef-fect on identification accuracy.
The main effectof the context was significant on accuracy withF (2, 1320) = 9.17, p < 0.01.
Given that forDPs we did not have an even distribution betweencontexts, we only utilized the results of AMEs andOTHERs.There are differences between expression typesin terms of the impact of addition of LM/LA intothe context, which underlines that when studyingcontext, the relative role and contribution of LAand LM (and their interaction) must be looked at indetail for different types of referring expressions.Over all referring expressions, the addition intoa None context of LM yields an average increasein accuracy of 9.1% for all referring expressiontypes, while for the same conditions the additionof LA yields an average increase of 21.3%.
Hence,interestingly for our test data, the addition of LAto the context has a positive impact on accuracy bymore than two times over the addition of LM.It is also notable that even with neither LA norLM present (i.e.
the None context), the evaluatorswere still able to correctly identify referents in be-tween 50?68.6% (average: 62.9%) of the cases.While this accuracy would be insufficient for theevaluation of machine generated referring expres-sions, it is still higher than one might expect andfurther investigation of this case is necessary.4.2 Demonstrative PronounsFor DPs, there is a very clear difference betweenthe two patterns (LM-LA and LA-LM) in terms ofthe increase of accuracy with a change of context.While accuracy for the LA-LM pattern remains ata high level (over 90%) for all three contexts (andthere is only a very small increase from None toBoth), for the LM-LA pattern there is a strong in-crease from None to Both of 38.8%.The difference in accuracy between the twocontext patterns of DPs in the None context mightcome from the mouse cursor effect.
The two ex-pressions of LA-LM pattern happened to have amouse cursor on the referent, when they wereused, resulting in high accuracy.
On the otherhand, 4 out of 5 expressions of LM-LA pattern didnot have a mouse cursor on the referent.
We havecurrently no explanation for the relation betweencontext patterns and the mouse position.
Whilewe have only 7 expressions in the None contextfor DPs and hence cannot draw any decisive con-clusions, we note that the impact of the mouse po-sition is a likely factor.For the LM-LA pattern, there is an increasein accuracy of 32.8% from None to the LA-context.
Overwhelmingly, this represents in-stances in which the referents are being operatedat the point in time when the solver utters a DP(this is in fact the LM-LA?
pattern, which has noNone context).
For those instances, the currentoperation information is sufficient to identify thereferents.
In contrast, addition of LM leads onlyto a small increase in accuracy of 5.6%.
This re-sult is in accordance with our previous work on thegeneration of DPs, which stressed the importanceof extra-linguistic information in the framework ofconsidering the interaction between linguistic andextra-linguistic information.4.3 Action-mentioning ExpressionsWhile for AMEs the number of instances is veryuneven between patterns (similar to the distribu-tion for DPs), there is a strong increase in accuracyfrom the None context to the Both context for bothpatterns (between 30% to almost 50%).
However,there is a difference between the two patterns interms of the relative contribution of LM and LA tothis increase.While for the LA-LM pattern the impact ofadding LM and LA is very similar, for the LM-LApattern the major increase in accuracy is due toadding LA into the None context.
This indicatesthat for AMEs, LA has a stronger impact on ac-curacy than LM, as is to be expected.
The strongincrease for AMEs of the LM-LA pattern whenadding LA into the context is not surprising, giventhat the evaluators were able to see the action men-tioned in the AME.For the opposite reason, it is not surprising thatAMEs show the lowest accuracy in the None con-text, given that the last action on the referent isnot seen by the evaluators.
However, accuracywas still slightly over 50% in the LM-LA pattern.Overall, of the 18 instances of AMEs of the LM-LA pattern, in the None context a majority of eval-uators correctly identified 9 and erred on the other9.
Further analysis of the difference between cor-rectly and incorrectly identified AMEs led us tonote again the important role of the mouse cursoralso for AMEs.Comparing to the LM-LA pattern, we had verylow accuracy even with the Both context.
As wementioned in the previous section, we had veryskewed test instances for AME, i.e.
18 LM-LApatterns vs. 2 LA-LM patterns.
We need furtherinvestigation on the LA-LM pattern of AME withmore large number of instances.Of the 18 LM-LA instances of AMEs, there are14 instances that mention a verb describing an ac-tion on the referent.
The referents of 6 of those14 AMEs were correctly determined by the evalu-ators and in all cases the mouse cursor played animportant role in enabling the evaluator to deter-mine the referent.
The evaluators seem to utilizethe mouse position at the time of the uttering of thereferring expression as well as mouse movementsin the video shown.
In contrast, for 8 out of the9 incorrectly determined AMEs no such informa-tion from the mouse was available.
There was avery similar pattern for AMEs that did not includea verb.
These points indicate that movements andthe position of the mouse both during the video aswell as the time point of the uttering of the refer-ring expression give important clues to evaluators.4.4 Other ExpressionsThere is a relatively even gain in identification ac-curacy from None to Both of between about 10?15% for both patterns.
However, there is a simi-lar tendency as for AMEs, since there is a differ-ence between the two patterns in terms of the rel-ative contribution of LM and LA to this increase.While for the LA-LM pattern the impact of addingLM and LA is roughly equivalent, for the LM-LApattern the major increase in accuracy is due toadding LM into the LA-context.For this pattern of OTHERs, LM has a strongerimpact on accuracy than LA, which is exactly theopposite tendency to AMEs.
For OTHERs (e.g.use of attributes for object identification), seeingthe last action on the target has a less positive im-pact than listening to the last linguistic mention.Furthermore, we note the relatively high accuracyin the None context for OTHERs, underlining thecontext-independence of expressions utilizing at-tributes and spatial relations of the pieces.4.5 Error AnalysisWe analyzed those instances whose referents werenot correctly identified by a majority of evalua-tors in the Both context.
Among the three expres-sion types, there were about 13?16% of wrong an-swers.
In total for 7 of the 60 expressions a ma-jority of evaluators gave wrong answers (4 DPs, 2AMEs and 1 OTHER).
Analysis of these instancesindicates that some improvements of our concep-tion of ?context?
is needed.For 3 out of the 4 DPs, the mouse was not overthe referent or was closer to another piece.
In addi-tion, these DPs included expressions that pointedto the role of a piece in the overall construction ofthe goal shape, e.g.
?soitu ga atama (that is thehead)?, or where a DP is used as part of a morecomplex referring expression, e.g.
?sore to onazikatati .
.
.
(the same shape as this)?, intended toidentify a different piece.
For a non-participantof the task, such expressions might be difficult tounderstand in any context.
This phenomenon isrelated to the ?overhearer-effect?
(Schober et al,1989).The two AMEs that the majority of evaluatorsfailed to identify in the Both context were alsomisidentified in the LA context.
Both AMEs weremissing a verb describing an action on the referent.While for AMEs including a verb the accuracy in-creased from None to Both by 50%, for AMEswithout a verb there was an increase by slightlyover 30%, indicating that in the case where anAME lacks a verb, the context has a smaller pos-itive impact on accuracy than for AMEs that in-clude a verb.
In order to account for those cases,further work is necessary, such as investigatinghow to account for the information on the distrac-tors.5 Conclusions and Future WorkIn order to address the task of designing a flexi-ble experiment set-up with relatively low cost forextrinsic evaluations of referring expressions, weinvestigated the context that needs to be shown toevaluators in order to correctly determine the ref-erent of an expression.The analysis of our results showed that the con-text had a significant impact on referent identifi-cation.
The impact was strongest for AMEs andDPs and less so for OTHERs.
Interestingly, wefound for both DPs and AMEs that including LAin the context had a stronger positive impact thanincluding LM.
This emphasizes the importance oftaking into account extra-linguistic information ina situated domain, as considered in this study.Our analysis of those expressions whose refer-ent was incorrectly identified in the Both contextindicated some directions for improving the ?con-text?
used in our experiments, for example look-ing further into AMEs without a verb describingan action on the referent.
Generally, there is anecessity to account for mouse movements duringthe video shown to evaluators as well as the prob-lem for extrinsic evaluations of how to address the?overhearer?s effect?.While likely differing in the specifics of the set-up, the methodology in the experiment design dis-cussed in this paper is applicable to other domains,in that it allows a low-cost flexible design of eval-uating referring expressions in a dynamic domain.In order to avoid the additional effort of analyzingcases in relation to LM and LA, in the future it willbe desirable to simply set a certain time period andbase an evaluation on such a set-up.However, we cannot simply assume that alonger context would yield a higher identificationaccuracy, given that evaluators in our set-up arenot actively participating in the interaction.
Thusthere is a possibility that identification accuracyactually decreases with longer video segments,due to a loss of the evaluator?s concentration.
Fur-ther investigation of this question is indicated.Based on the work reported in this paper, weplan to implement an extrinsic task-performanceevaluation in the dynamic domain.
Even withthe large potential cost-savings based on the re-sults reported in this paper, extrinsic evaluationswill remain costly.
Thus one important future taskfor extrinsic evaluations will be to investigate thecorrelation between extrinsic and intrinsic evalua-tion metrics.
This in turn will enable the use ofcost-effective intrinsic evaluations whose resultsare strongly correlated to task-performance eval-uations.
This paper made an important contribu-tion by pointing the direction for further researchin extrinsic evaluations in the dynamic domain.ReferencesAnja Belz and Albert Gatt.
2008.
Intrinsic vs. extrinsicevaluation measures for referring expression genera-tion.
In Proceedings of ACL-08: HLT, Short Papers,pages 197?200.Donna Byron, Thomas Mampilly, Vinay Sharma, andTianfang Xu.
2005.
Utilizing visual attention forcross-modal coreference interpretation.
In CON-TEXT 2005, pages 83?96.Donna Byron, Alexander Koller, Kristina Striegnitz,Justine Cassell, Robert Dale, Johanna Moore, andJon Oberlander.
2009.
Report on the First NLGChallenge on Generating Instructions in Virtual En-vironments (GIVE).
In Proceedings of the 12th Eu-ropean Workshop on Natural Language Generation(ENLG 2009), pages 165?173.Aoife Cahill and Josef van Genabith.
2006.
Ro-bust PCFG-based generation using automaticallyacquired lfg approximations.
In Proceedings ofthe 21st International Conference on ComputationalLinguistics and 44th Annual Meeting of the Asso-ciation for Computational Linguistics, pages 1033?1040.Barbara Di Eugenio, Pamela.
W. Jordan, Richmond H.Thomason, and Johanna.
D Moore.
2000.
Theagreement process: An empirical investigation ofhuman-human computer-mediated collaborative di-alogues.
International Journal of Human-ComputerStudies, 53(6):1017?1076.Mary Ellen Foster, Ellen Gurman Bard, Markus Guhe,Robin L. Hill, Jon Oberlander, and Alois Knoll.2008.
The roles of haptic-ostensive referring expres-sions in cooperative, task-based human-robot dia-logue.
In Proceedings of 3rd Human-Robot Inter-action, pages 295?302.Mary Ellen Foster, Manuel Giuliani, Amy Isard, ColinMatheson, Jon Oberlander, and Alois Knoll.
2009.Evaluating description and reference strategies in acooperative human-robot dialogue system.
In Pro-ceedings of the 21st international jont conferenceon Artifical intelligence (IJCAI 2009), pages 1818?1823.Mary Ellen Foster.
2008.
Automated metrics thatagree with human judgements on generated outputfor an embodied conversational agent.
In Proceed-ings of the 5th International Natural Language Gen-eration Conference (INLG 2008), pages 95?103.Andrew Gargett, Konstantina Garoufi, AlexanderKoller, and Kristina Striegnitz.
2010.
The give-2 corpus of giving instructions in virtual environ-ments.
In Proceedings of the Seventh conference onInternational Language Resources and Evaluation(LREC 2010), pages 2401?2406.Albert Gatt, Anja Belz, and Eric Kow.
2009.
TheTUNA-REG Challenge 2009: Overview and eval-uation results.
In Proceedings of the 12th EuropeanWorkshop on Natural Language Generation (ENLG2009), pages 174?182.Pamela W. Jordan and Marilyn A. Walker.
2005.Learning content selection rules for generating ob-ject descriptions in dialogue.
Journal of ArtificialIntelligence Research, 24:157?194.Imtiaz Hussain Khan, Kees van Deemter, GraemeRitchie, Albert Gatt, and Alexandra A. Cleland.2009.
A hearer-oriented evaluation of referring ex-pression generation.
In Proceedings of the 12th Eu-ropean Workshop on Natural Language Generation(ENLG 2009), pages 98?101.Alexander Koller, Kristina Striegnitz, Donna Byron,Justine Cassell, Robert Dale, Sara Dalzel-Job, JonOberlander, and Johanna Moore.
2009.
Validatingthe web-based evaluation of nlg systems.
In Pro-ceedings of the ACL-IJCNLP 2009 Conference ShortPapers, pages 301?304.Kishore Papineni, Salim Roukos, Todd Ward, and Weijing Zhu.
2002.
BLEU: a method for automaticevaluation of machine translation.
In Proceedingsof the 40th Annual Meeting on Association for Com-putational Linguistics (ACL 2002), pages 311?318.Ivandre?
Paraboni, Judith Masthoff, and Kees vanDeemter.
2006.
Overspecified reference in hierar-chical domains: Measuring the benefits for readers.In Proceedings of the 4th International Natural Lan-guage Generation Conference (INLG 2006), pages55?62.Paul L.A. Piwek.
2007.
Modality choise for generationof referring acts.
In Proceedings of the Workshop onMultimodal Output Generation (MOG 2007), pages129?139.Ehud Reiter and Anja Belz.
2009.
An investiga-tion into the validity of some metrics for automat-ically evaluating natural language generation sys-tems.
Computational Linguistics, 35(4):529?558.Ehud Reiter and Somayajulu Sripada.
2002.
Shouldcorpora texts be gold standards for NLG?
In Pro-ceesings of 2nd International Natural LanguageGeneration Conference (INLG 2002), pages 97?104.Ehud Reiter, Roma Robertson, and Liesl M. Osman.2003.
Lessons from a failure: generating tailoredsmoking cessation letters.
Artificial Intelligence,144(1-2):41?58.Ehud Reiter, Somayajulu Sripada, Jim Hunter, Jin Yu,and Ian Davy.
2005.
Choosing words in computer-generated weather forecasts.
Artificial Intelligence,167(1-2):137?169.Michael F. Schober, Herbert, and H. Clark.
1989.
Un-derstanding by addressees and overhearers.
Cogni-tive Psychology, 21:211?232.Philipp Spanger, Masaaki Yasuhara, Ryu Iida, andTakenobu Tokunaga.
2009a.
A Japanese corpusof referring expressions used in a situated collabo-ration task.
In Proceedings of the 12th EuropeanWorkshop on Natural Language Generation (ENLG2009), pages 110 ?
113.Philipp Spanger, Masaaki Yasuhara, Iida Ryu, andTokunaga Takenobu.
2009b.
Using extra linguisticinformation for generating demonstrative pronounsin a situated collaboration task.
In Proceedings ofPreCogSci 2009: Production of Referring Expres-sions: Bridging the gap between computational andempirical approaches to reference.Karen Sparck Jones and Julia R. Galliers.
1996.
Eval-uating Natural Language Processing Systems: AnAnalysis and Review.
Springer-Verlag.Amanda Stent, Matthew Marge, and Mohit Singhai.2005.
Evaluating evaluation methods for generationin the presence of variation.
In Linguistics and In-telligent Text Processing, pages 341?351.
Springer-Verlag.Laura Stoia, Darla Magdalene Shockley, Donna K. By-ron, and Eric Fosler-Lussier.
2006.
Noun phrasegeneration for situated dialogs.
In Proceedings ofthe 4th International Natural Language GenerationConference (INLG 2006), pages 81?88.Kees van Deemter.
2007.
TUNA: Towards a unifiedalgorithm for the generation of referring expres-sions.
Technical report, Aberdeen University.www.csd.abdn.ac.uk/research/tuna/pubs/TUNA-final-report.pdf.Ielka van der Sluis, Albert Gatt, and Kees van Deemter.2007.
Evaluating algorithms for the generation ofreferring expressions: Going beyond toy domains.In Proceedings of Recent Advances in Natural Lan-guae Processing (RANLP 2007).
