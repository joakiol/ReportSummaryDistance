Proceedings of the NAACL HLT Student Research Workshop and Doctoral Consortium, pages 72?77,Boulder, Colorado, June 2009. c?2009 Association for Computational LinguisticsTowards Building a Competitive Opinion Summarization System:Challenges and KeysElena Lloret*, Alexandra Balahur, Manuel Palomar and Andr?s MontoyoDepartment of Software and Computing SystemsUniversity of AlicanteApartado de Correos 99, E-03080, Alicante, Spain{elloret, abalahur, mpalomar, montoyo}@dlsi.ua.esAbstractThis paper presents an overview of our participation inthe TAC 2008 Opinion Pilot Summarization task, aswell as the proposed and evaluated post-competitionimprovements.
We first describe our opinionsummarization system and the results obtained.
Furtheron, we identify the system?s weak points and suggestseveral improvements, focused both on informationcontent, as well as linguistic and readability aspects.
Weobtain encouraging results, especially as far as F-measure is concerned, outperforming the competitionresults by approximately 80%.1 IntroductionThe Opinion Summarization Pilot (OSP) taskwithin the TAC 2008 competition consisted ingenerating summaries from answers to opinionquestions retrieved from blogs (the Blog061collection).
The questions were organized around25 targets ?
persons, events, organizations etc.Additionally, a set of text snippets that containedthe answers to the questions were provided by theorganizers, their use being optional.
An example oftarget, question and provided snippet is given inFigure 1.Figure 1.
Examples of target, question and snippet*Elena Lloret is funded by the FPI program (BES-2007-16268) from the Spanish Ministry of Science and Innovation,under the project TEXT-MESS (TIN-2006-15265)1http://ir.dcs.gla.ac.uk/test_collections/access_to_data.htmlThe techniques employed by the participants weremainly based on the already existingsummarization systems.
While most participantsadded new features (sentiment, pos/neg sentiment,pos/neg opinion) to account for the presence ofpositive opinions or negative ones - CLASSY(Conroy and Schlessinger, 2008); CCNU (He etal.,2008);  LIPN (Bossard et al, 2008);  IIITSum08(Varma et al, 2008) -, efficient methods wereproposed focusing on the retrieval and filteringstage, based on polarity ?
DLSIUAES (Balahur etal., 2008) - or on separating information richclauses - italica (Cruz et al, 2008).
In general,previous work in opinion mining includesdocument level sentiment classification usingsupervised (Chaovalit and Zhou, 2005) andunsupervised methods (Turney, 2002), machinelearning techniques and sentiment classificationconsidering rating scales (Pang, Lee andVaithyanathan, 2002), and scoring of features(Dave, Lawrence and Pennock, 2003).
Otherresearch has been conducted in analysingsentiment at a sentence level using bootstrappingtechniques (Riloff and Wiebe, 2003), findingstrength of opinions (Wilson, Wiebe and Hwa,2004), summing up orientations of opinion wordsin a sentence (Kim and Hovy, 2004), andidentifying opinion holders (Stoyanov and Cardie,2006).
Finally, fine grained, feature-based opinionsummarization is defined in (Hu and Liu, 2004).2 Opinion Summarization SystemIn order to tackle the OSP task, we considered theuse of two different methods for opinion miningand summarization, differing mainly with respectto the use of the optional text snippets provided.Our first approach (the Snippet-driven Approach)Target : George ClooneyQuestion: Why do people like George Clooney?Snippet 1: 1050 BLOG06-20060125-015-0025581509 he is a great actor72used these snippets, whereas the second one (Blog-driven Approach) found the answers directly in thecorresponding blogs.
A general overview of thesystem?s architecture is shown in Figure 2, wherethree main parts can be distinguished: the questionprocessing stage, the snippets processing stage(only carried out for the first approach), and thefinal summary generation module.
Next, the mainsteps involved in each process will be explained inmore detail.Figure 2.
System architectureThe first step was to determine the polarity of eachquestion, extract the keywords from each of themand finally, build some patterns of reformulation.The latter were defined in order to give the finalsummary an abstract nature, rather than a simplejoining of sentences.
The polarity of the questionwas determined using a set of created patterns,whose goal was to extract for further classificationthe nouns, verbs, adverbs or adjectives indicatingsome kind of polarity (positive or negative).
Theseextracted words, together with their determiners,were classified using the emotions lists inWordNet Affect (Strapparava and Valitutti, 2005),jointly with the emotions lists of attitudes, triggersresource (Balahur and Montoyo, 2008 [1]), fourcreated lists of attitudes, expressing criticism,support, admiration and rejection and twocategories for value (good and bad), taking for theopinion mining systems in (Balahur and Montoyo,2008 [2]).
Moreover, the focus of each questionwas automatically extracted using the Freeling2Named Entity Recognizer module.
Thisinformation was used to determine whether or notall the questions within the same topic had thesame focus, as well as be able to decide later onwhich text snippet belonged to which question.Regarding the given text snippets, we alsocomputed their polarity and their focus.
The2http://garraf.epsevg.upc.es/freeling/polarity was calculated as a vector similaritybetween the snippets and vectors constructed fromthe list of sentences contained in the ISEAR corpus(Scherer and Wallbot, 1997), WordNet Affectemotion lists of anger, sadness, disgust and joy andthe emotion triggers resource, using Pedersen'sText Similarity Package.3Concerning the blogs, our opinion mining andsummarization system is focused only on plaintext; therefore, as pre processing stage, weremoved all unnecessary tags and irrelevantinformation, such as links, images etc.
Further on,we split the remaining text into individualsentences.
A matching between blogs' sentencesand text snippets was performed so that apreliminary set of potential meaningful sentenceswas recorded for further processing.
To achievethis, snippets not literally contained in the blogswere tokenized and stemmed using Porter'sStemmer,4 and stop words were removed in orderto find the most similar possible sentenceassociated with it.
Subsequently, by means of thesame Pedersen Text Similarity Package as forcomputing the snippets' polarity, we computed thesimilarity between the given snippets and thiscreated set of potential sentences.
We extracted thecomplete blog sentences to which each snippet wasrelated.
Further on, we extracted the focus for eachblog phrase sentence as well.
Then, we filteredredundant sentences using a na?ve similarity basedapproach.
Once we obtained the possible answers,we used Minipar5 to filter out incompletesentences.Having computed the polarity for the questions andsnippets, and set out the final set of sentences toproduce the summary, we bound each sentence toits corresponding question, and we grouped allsentences which were related to the same questiontogether, so that we could generate the languagefor this group, according to the patterns ofreformulation previously mentioned.
Finally, thespeech style was changed to an impersonal one, inorder to avoid directly expressed opinionsentences.
A POS-tagger tool (TreeTagger6) wasused to identify third person verbs and changethem to a neutral style.
A set of rules to identify3http://www.d.umn.edu/~tpederse/text-similarity.html4http://tartarus.org/~martin/PorterStemmer/5http://www.cs.ualberta.ca/~lindek/minipar.htm6http://www.ims.uni-tuttgart.de/projekte/corplex/TreeTagger/73pronouns was created, and they were also changedto the more general pronoun ?they?
and itscorresponding forms, to avoid personal opinions.3 EvaluationTable 1 shows the final results obtained by ourapproaches in the TAC 2008 Opinion Pilot (therank among the 36 participating systems is shownin brackets for each evaluation measure).
Both ofour approaches were totally automatic, and theonly difference between them was the use of thegiven snippets in the first one (A1) and not in thesecond (A2).
The column numbers stand for thefollowing average scores: summarizerID (1);pyramid F-score (Beta=1) (2), grammaticality (3);non-redundancy (4); structure/coherence(including focus and referential clarity) (5); overallfluency/readability (6); overall responsiveness (7).1 2 3 4 5 6 7A1 0.357(7)4.727(8)5.364(28)3.409(4)3.636(16)5.045(5)A2 0.155(23)3.545(36)4.364(36)3.091(13)2.636(36)2.227(28)Table 1.
Evaluation resultsAs it can be noticed from Table 1, our systemperformed well regarding F-measure, the first runbeing classified 7th among the 36 evaluated.
As faras the structure and coherence are concerned, theresults were also good, placing the first approachin the fourth.
Also worth mentioning is the goodperformance obtained regarding the overallresponsiveness, where A1 ranked 5th.
Generallyspeaking, the results for A1 showed well-balancedamong all the criteria evaluated, except for nonredundancy and grammaticality.
For the secondapproach, results were not as good, due to thedifficulty in selecting the appropriate opinion blogsentence by only taking into account the keywordsof the question.4 Post-competition tests, experimentsand improvementsWhen an exhaustive examination of the nuggetsused for evaluating the summaries was done, wefound some problems that are worth mentioning.a) Some nuggets with high score did not exist inthe snippet list (e.g.
?When buying fromCARMAX, got a better than blue book trade-inon old car?
(0.9)).b) Some nuggets for the same target express thesame idea, despite their not being identical(e.g.
?NAFTA needs to be renegotiated toprotect Canadian sovereignty?
and ?GreenParty: Renegotiate NAFTA to protectCanadian Sovereignty?
).c) The meaning of one nugget can be deducedfrom another's (e.g.
?reasonably healthy food?and ?sandwiches are healthy?
).d) Some nuggets are not very clear in meaning(e.g.
?hot?, ?fun?
).e) A snippet can be covered by several nuggets(e.g.
both nuggets ?it is an honest book?
and?it is a great book?
correspond to the samesnippet ?It was such a great book- honest andhard to read (content not languagedifficulty)?
).On the other hand, regarding the use of theoptional snippets, the main problem to address is toremove redundancy, because many of them arerepeated for the same target, and we have todetermine which snippet represents better the ideafor the final summary, in order to avoid noisyirrelevant information.4.1 Measuring the Performance of aGeneric Summarization SystemSeveral participants in the TAC 2008 editionperformed the OSP task by using genericsummarization systems.
Most were adjusted byintegrating an opinion classifier module so that thetask could be fulfilled, but some were not (Bossardet al, 2008), (Hendrickx and Bosma, 2008).
Thisfact made us realize that a generic summarizercould be used to achieve this task.
We wanted toanalyze the effects of such a kind of summarizer toproduce opinion summaries.
We followed theapproach described in (Lloret et al, 2008).
Themain idea employed is to score sentences of adocument with regard to the word frequency count(WF), which can be combined with a TextualEntailment (TE) module.Although the first approach suggested for opinionsummarization obtained much better results in theevaluation than the second one (see Section 3.1),we decided to run the generic system over bothapproaches, with and without applying TE, to74provide a more extent analysis and conclusions.After preprocessing the blogs and having all thepossible candidate sentences grouped together, weconsidered these as the input for the genericsummarizer.
The goal of these experiments was todetermine whether the techniques used for ageneric summarizer would have a positiveinfluence in selecting the main relevantinformation to become part of the final summary.4.2 Results and DiscussionWe re-evaluated the summaries generated by thegeneric system following the nuggets?
list providedby the TAC 2008 organization, and countingmanually the number of nuggets that were coveredin the summaries.
This was a tedious task, but itcould not be automatically performed because ofthe fact that many of the provided nuggets werenot found in the original blog collection.
After themanual matching of nuggets and sentences, wecomputed the average Recall, Precision and F-measure (Beta =1) in the same way as in the TAC2008 was done, according to the number andweight of the nuggets that were also covered in thesummary.
Each nugget had a weight ranging from0 to 1 reflecting its importance, and it was countedonly once, even though the information wasrepeated within the summary.The average for each value was calculated takinginto account the results for all the summaries ineach approach.
Unfortunately, we could notmeasure criteria such as readability or coherence asthey were manually evaluated by human experts.Table 2 points out the results for all the approachesreported.
We have also considered the resultsderived from our participation in the TAC 2008conference (OpSum-1 and OpSum-2), in order toanalyze whether they have been improved or not.From these results it can be stated that the TEmodule in conjunction with the WF counts, havebeen very appropriate in selecting the mostimportant information of a document.
Although itcan be thought that applying TE can remove somemeaningful sentences which contained importantinformation, results show the opposite.
It benefitsthe Precision value, because a shorter summarycontains greater ratio of relevant information.
Onthe other hand, taking into consideration the F-measure value only, it can be seen that theapproach combining TE and WF, for the sentencesin the first approach, has beaten significantly thebest F-measure result among the participants ofTAC 2008 (please see Table 3), increasing itsperformance by 20% (with respect to WF only),and improving by approximately 80% with respectto our first approach submitted to TAC 2008.However, a simple generic summarization systemlike the one we have used here is not enough toproduce opinion oriented summaries, sincesemantic coherence given by the grouping ofpositive and negative opinions is not taken intoaccount.
Therefore, the opinion classification stagemust be added in the same manner as used in thecompetition.SYSTEM RECALL PRECISION F-MEASUREOpSum-1 0.592 0.272 0.357OpSum-2 0.251 0.141 0.155WF-1 0.705 0.392 0.486TE+WF -1  0.684 0.630  0.639WF -2 0.322 0.234  0.241TE+WF-2 0.292 0.282 0.262Table 2.
Comparison of the results4.3 Improving the quality of summariesIn the evaluation performed by the TACorganization, a manual quality evaluation was alsocarried out.
In this evaluation the important aspectswere grammaticality, non-redundancy, structureand coherence, readability, and overallresponsiveness.
Although our participating systemsobtained good F-measure values, in other scores,especially in grammaticality and non-redundancy,the results achieved were very low.
Focusing allour efforts in improving the first approach,OpSum-1, non-redundancy and grammaticalityverification had to be performed.
In this approach,we wanted to test how much of the redundantinformation would be possible to remove by usinga Textual Entailment system similar to (Iftene andBalahur-Dobrescu, 2007), without it affecting thequality of the remaining data.
As input for the TEsystem, we considered the snippets retrieved fromthe original blog posts.
We applied the entailmentverification on each of the possible pairs, taking inturn all snippets as Text and Hypothesis with allother snippets as Hypothesis and Text,respectively.
Thus, as output, we obtained the listof snippets from which we eliminated those that75are entailed by any of the other snippets.
Wefurther eliminated those snippets which had a highentailment score with any of the remainingsnippets.SYSTEM F-MEASUREBest system  0.534Second best system 0.490OpSum-1 + TE  0.530OpSum-1 0.357Table 3.
F-measure results after improving the systemTable 3 shows that applying TE before generatingthe final summary leads to very good resultsincreasing the F-measure by 48.50% with respectto the original first approach.
Moreover, it can beseen form Table 3 that our improved approachwould have ranked in the second place among allthe participants, regarding F-measure.
The mainproblem with this approach is the long processingtime.
We can apply Textual Entailment in themanner described within the genericsummarization system presented, successivelytesting the relation as Snippet1 entails Snippet2?,Snippet1+Snippet2 entails Snippet3?
and so on.The problem then becomes the fact that thisapproach is random, since different snippets comefrom different sources, so there is no order amongthem.
Further on, we have seen that manyproblems arise from the fact that extractinginformation from blogs introduces a lot of noise.
Inmany cases, we had examples such as:At 4:00 PM John said Starbucks coffee tastes greatJohn said Starbucks coffee tastes great, always get onewhen reading New York Times.To the final summary, the important informationthat should be added is ?Starbucks coffee tastesgreat?.
Our TE system contains a rule specifyingthat the existence or not of a Named Entity in thehypothesis and its not being mentioned in the textleads to the decision of ?NO?
entailment.
For theexample given, both snippets are maintained,although they contain the same data.Another issue to be addressed is the extrainformation contained in final summaries that isnot scored as nugget.
As we have seen from ourdata, much of this information is also valid andcorrectly answers the questions.
Therefore, whatmethods can be employed to give more weight tosome and penalize others automatically?Regarding the grammaticality criteria, once we hada summary generated we used the moduleLanguage Tool7 as a post-processing step.
Theerrors that we needed correcting included thenumber matching between nouns and determinersas well as among subject and predicate, upper casefor sentence start, repeated words or punctuationmarks and lack of punctuation marks.
The rulespresent in the module and that we ?switched off?,due to the fact that they produced more errors,were those concerning the limit in the number ofconsecutive nouns and the need for an articlebefore a noun (since it always seemed to want tocorrect ?Vista?
for ?the Vista?
a.o.).
We evaluatedby observing the mistakes that the texts contained,and counting the number of remaining orintroduced errors in the output.
The resultsobtained can be seen in Table 4.Problem Rightly correctedWronglycorrectedMatch S-P 90% 10%Noun-det 75% 25%Upper case 80% 20%Repeated words 100% 0%Repeated ?.?
80% 20%Spelling mistakes 60% 40%Unpaired ?
?/() 100% 0%Table 4.
Grammaticality analysisThe greatest problem encountered was the fact thatbigrams are not detected and agreement is notmade in cases in which the noun does not appearexactly after the determiner.
All in all, using thismodule, the grammaticality of our texts wasgreatly improved.5 Conclusions and future workThe Opinion Pilot in the TAC 2008 competitionwas a difficult task, involving the development ofsystems including components for QA, IR, polarityclassification and summarization.
Our contributionpresented in this paper resides in proposing anopinion mining and summarization method usingdifferent approaches and resources, evaluatingeach of them in turn.
We have shown that using ageneric summarization system, we obtain 80%improvement over the results obtained in thecompetition, with coherence being maintained byusing the same polarity classification mechanisms.7http://community.languagetool.org/76Using redundancy removal with TE, as opposed toour initial polarity strength based sentence filteringimproved the system performance by almost 50%.Finally, we showed that grammaticality can bechecked and improved using an independentsolution given by Language Tool.Further work includes the improvement of thepolarity classification component by usingmachine learning over annotated corpora and othertechniques, such as anaphora resolution.
As wecould see, the well functioning of this componentensures logic, structure and coherence to theproduced summaries.
Moreover, we plan to studythe manner in which opinion sentences ofblogs/bloggers can be coherently combined.ReferencesBalahur, A., Lloret, E., Ferr?ndez, ?., Montoyo, A.,Palomar, M., Mu?oz, R., The DLSIUAES Team?sParticipation in the TAC 2008 Tracks.
InProceedings of the Text Analysis Conference (TAC),2008.Balahur, A. and Montoyo, A.
[1].
An IncrementalMultilingual Approach to Forming a CultureDependent Emotion Triggers Database.
InProceedings of the 8th International Conference onTerminology and Knowledge Engineering, 2008.Balahur, A. and Montoyo, A.
[2].
Multilingual Feature--driven Opinion Mining and Summarization fromCustomer Reviews.
In Lecture Notes in ComputerScience 5039, pg.
345-346.Bossard, A., G?n?reux, M. and  Poibeau, T.. Descriptionof the LIPN systems at TAC 2008: Summarizinginformation and opinions.
In Proceedings of the TextAnalysis Conference (TAC), 2008.Chaovalit, P., Zhou, L. 2005.
Movie Review Mining: aComparison between Supervised and UnsupervisedClassification Approaches.
In Proceedings of HICSS-05, the 38th Hawaii International Conference onSystem Sciences.Cruz, F., Troyani, J.A., Ortega, J., Enr?quez, F. TheItalica System at TAC 2008 Opinion SummarizationTask.
In Proceedings of the Text AnalysisConference (TAC), 2008.Cui, H., Mittal, V., Datar, M. 2006.
ComparativeExperiments on Sentiment Classification for OnlineProduct Reviews.
In Proceedings of the 21st NationalConference on Artificial Intelligence AAAI 2006.Dave, K., Lawrence, S., Pennock, D. 2003.
Mining thePeanut Gallery: Opinion Extraction and SemanticClassification of Product Reviews.
In Proceedings ofWWW-03.Lloret, E., Ferr?
?ndez, O., Mu?oz, R. and Palomar, M. AText Summarization Approach under the Influence ofTextual Entailment.
In Proceedings of the 5thInternational Workshop on Natural LanguageProcessing and Cognitive Science (NLPCS 2008),pages 22?31, 2008.Gamon, M., Aue, S., Corston-Oliver, S., Ringger, E.2005.
Mining Customer Opinions from Free Text.Lecture Notes in Computer Science.He, T., Chen, J., Gui, Z., Li, F. CCNU at TAC 2008:Proceeding on Using Semantic Method forAutomated Summarization Yield.
In Proceedings ofthe Text Analysis Conference (TAC), 2008.Hendrickx, I. and Bosma, W..
Using coreference linksand sentence compression in graph-basedsummarization.
In Proceedings of the Text AnalysisConference (TAC), 2008.Hu, M., Liu, B.
2004.
Mining Opinion Features inCustomer Reviews.
In Proceedings of 19th NationalConference on Artificial Intelligence AAAI.Iftene, A., Balahur-Dobrescu, A. HypothesisTransformation and Semantic Variability Rules forRecognizing Textual Entailment.
In Proceedings ofthe ACL 2007 Workshop on Textual Entailment andParaphrasis, 2007.Kim, S.M., Hovy, E. 2004.
Determining the Sentimentof Opinions.
In Proceedings of COLING 2004.Pang, B., Lee, L., Vaithyanathan, S. 2002.
Thumbs up?Sentiment classification using machine learningtechniques.
In Proceedings of EMNLP-02, theConference on Empirical Methods in NaturalLanguage Processing.Riloff, E., Wiebe, J.
2003 Learning Extraction Patternsfor Subjective Expressions.
In Proceedings of the2003 Conference on Empirical Methods in NaturalLanguage Processing.Scherer, K. and Wallbott, H.G.
The ISEARQuestionnaire and Codebook, 1997.Stoyanov, V., Cardie, C. 2006.
Toward OpinionSummarization: Linking the Sources.
In: COLING-ACL 2006 Workshop on Sentiment and Subjectivityin Text.Strapparava, C. and Valitutti, A.
"WordNet-Affect: anaffective extension of WordNet".
In Proceedingsofthe 4th International Conference on LanguageResources and Evaluation, 2004, pp.
1083-1086.Turney, P., 2002.
Thumbs up or thumbs down?Semantic orientation applied to unsupervisedclassification of reviews.
In Proceedings of the 40thAnnual Meeting of the ACLVarma, V., Pingali, P., Katragadda, R., Krisha, S.,Ganesh, S., Sarvabhotla, K., Garapati, H., Gopisetty,H.,, Reddy, V.B., Bysani, P., Bharadwaj, R. IITHyderabad at TAC 2008.
In Proceedings of the TextAnalysis Conference (TAC), 2008.Wilson, T., Wiebe, J., Hwa, R. 2004.
Just how mad areyou?
Finding strong and weak opinion clauses.
In:Proceedings of AAAI 2004.77
