Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 1: Proceedings of the Main Conferenceand the Shared Task, pages 148?154, Atlanta, Georgia, June 13-14, 2013. c?2013 Association for Computational LinguisticsMayoClinicNLP?CORE: Semantic representations for textual similarityStephen WuMayo ClinicRochester, MN 55905wu.stephen@mayo.eduDongqing Zhu & Ben CarteretteUniversity of DelawareNewark, DE 19716{zhu,carteret}@cis.udel.eduHongfang LiuMayo ClinicRochester, MN 55905liu.hongfang@mayo.eduAbstractThe Semantic Textual Similarity (STS) taskexamines semantic similarity at a sentence-level.
We explored three representations ofsemantics (implicit or explicit): named enti-ties, semantic vectors, and structured vectorialsemantics.
From a DKPro baseline, we alsoperformed feature selection and used source-specific linear regression models to combineour features.
Our systems placed 5th, 6th, and8th among 90 submitted systems.1 IntroductionThe Semantic Textual Similarity (STS) task (Agirreet al 2012; Agirre et al 2013) examines semanticsimilarity at a sentence-level.
While much work hascompared the semantics of terms, concepts, or doc-uments, this space has been relatively unexplored.The 2013 STS task provided sentence pairs and a0?5 human rating of their similarity, with trainingdata from 5 sources and test data from 4 sources.We sought to explore and evaluate the usefulnessof several semantic representations that have hadrecent significance in research or practice.
First,information extraction (IE) methods often implic-itly consider named entities as ad hoc semantic rep-resentations, for example, in the clinical domain.Therefore, we sought to evaluate similarity based onnamed entity-based features.
Second, in many appli-cations, an effective means of incorporating distri-butional semantics is Random Indexing (RI).
Thuswe consider three different representations possi-ble within Random Indexing (Kanerva et al 2000;Sahlgren, 2005).
Finally, because compositionaldistributional semantics is an important researchtopic (Mitchell and Lapata, 2008; Erk and Pado?,2008), we sought to evaluate a principled compo-sition strategy: structured vectorial semantics (Wuand Schuler, 2011).The remainder of this paper proceeds as follows.Section 2 overviews our similarity metrics, and Sec-tion 3 overviews the systems that were defined onthese metrics.
Competition results and additionalanalyses are in Section 4.
We end with discussionon the results in Section 5.2 Similarity measuresBecause we expect semantic similarity to be multi-layered, we expect that we will need many similar-ity measures to approximate human similarity judg-ments.
Rather than reinvent the wheel, we have cho-sen to introduce features that complement existingsuccessful feature sets.
We utilized 17 features fromDKPro Similarity and 21 features from TakeLab,i.e., the two top-performing systems in the 2012 STStask, as a solid baseline.These are summarized in Table 1.
We introduce 3categories of new similarity metrics, 9 metrics in all.2.1 Named entity measuresNamed entity recognition provides a common ap-proximation of semantic content for the informa-tion extraction perspective.
We define three simplesimilarity metrics based on named entities.
First,we computed the named entity overlap (exact stringmatches) between the two sentences, where NEkwas the set of named entities found in sentenceSk.
This is the harmonic mean of how closely S1148Table 1: Full feature pool in MayoClinicNLP systems.
The proposed MayoClinicNLP metrics are meant to comple-ment DKPro (Ba?r et al 2012) and TakeLab (?Saric?
et al 2012) metrics.DKPro metrics (17) TakeLab metrics (21) Custom MayoClinicNLP metrics (9)n-grams/WordNGramContainmentMeasure 1 stopword-filtered t ngram/UnigramOverlapn-grams/WordNGramContainmentMeasure 2 stopword-filtered t ngram/BigramOverlapn-grams/WordNGramJaccardMeasure 1 t ngram/TrigramOverlapn-grams/WordNGramJaccardMeasure 2 stopword-filtered t ngram/ContentUnigramOverlapn-grams/WordNGramJaccardMeasure 3 t ngram/ContentBigramOverlapn-grams/WordNGramJaccardMeasure 4 t ngram/ContentTrigramOverlapn-grams/WordNGramJaccardMeasure 4 stopword-filteredt words/WeightedWordOverlap custom/StanfordNerMeasure overlap.txtt words/GreedyLemmaAligningOverlap custom/StanfordNerMeasure aligngst.txtt words/WordNetAugmentedWordOverlap custom/StanfordNerMeasure alignlcs.txtesa/ESA Wiktionary t vec/LSAWordSimilarity NYT custom/SVSePhrSimilarityMeasure.txtesa/ESA WordNet t vec/LSAWordSimilarity weighted NYT custom/SVSeTopSimilarityMeasure.txtt vec/LSAWordSimilarity weighted Wiki custom/SemanticVectorsSimilarityMeasure d200 wr0.txtcustom/SemanticVectorsSimilarityMeasure d200 wr6b.txtcustom/SemanticVectorsSimilarityMeasure d200 wr6d.txtcustom/SemanticVectorsSimilarityMeasure d200 wr6p.txtn-grams/CharacterNGramMeasure 2 t other/RelativeLengthDifferencen-grams/CharacterNGramMeasure 3 t other/RelativeInfoContentDifferencen-grams/CharacterNGramMeasure 4 t other/NumbersSizestring/GreedyStringTiling 3 t other/NumbersOverlapstring/LongestCommonSubsequenceComparator t other/NumbersSubsetstring/LongestCommonSubsequenceNormComparator t other/SentenceSizestring/LongestCommonSubstringComparator t other/CaseMatchest other/StocksSizet other/StocksOverlapmatches S2, and how closely S2 matches S1:simneo(S1, S2) = 2 ?
?NE1 ?NE2??NE1?
+ ?NE2?
(1)Additionally, we relax the constraint of requiringexact string matches between the two sentences byusing the longest common subsequence (Allison andDix, 1986) and greedy string tiling (Wise, 1996) al-gorithms.
These metrics give similarities betweentwo strings, rather than two sets of strings as wehave with NE1 and NE2.
Thus, we follow previ-ous work in greedily aligning these named entities(Lavie and Denkowski, 2009; ?Saric?
et al 2012) intopairs.
Namely, we compare each pair (nei,1, nej,2)of named entity strings in NE1 and NE2.
Thehighest-scoring pair is entered into a set of pairs, P .Then, the next highest pair is added to P if neithernamed entity is already in P , and discarded other-wise; this continues until there are no more namedentities in either NE1 or NE2.We then define two named entity aligning mea-sures that use the longest common subsequence(LCS) and greedy string tiling (GST) fuzzy stringmatching algorithms:simnea(S1, S2) =?
(ne1,ne2)?Pf(ne1, ne2)max (?NE1?, ?NE2?
)(2)where f(?)
is either the LCS or GST algorithm.In our experiments, we performed named entityrecognition with the Stanford NER tool using thestandard English model (Finkel et al 2005).
Also,we used UKP?s existing implementation of LCS andGST (?Saric?
et al 2012) for the latter two measures.2.2 Random indexing measuresRandom indexing (Kanerva et al 2000; Sahlgren,2005) is another distributional semantics frameworkfor representing terms as vectors.
Similar to LSA(Deerwester et al 1990), an index is created thatrepresents each term as a semantic vector.
Butin random indexing, each term is represented byan elemental vector et with a small number ofrandomly-generated non-zero components.
The in-tuition for this means of dimensionality reduction isthat these randomly-generated elemental vectors arelike quasi-orthogonal bases in a traditional geomet-ric semantic space, rather than, e.g., 300 fully or-thogonal dimensions from singular value decompo-sition (Landauer and Dumais, 1997).
For a standardmodel with random indexing, a contextual term vec-tor ct,std is the the sum of the elemental vectors cor-responding to tokens in the document.
All contextsfor a particular term are summed and normalized toproduce a final term vector vt,std.Other notions of context can be incorporated into149this model.
Local co-occurrence context can be ac-counted for in a basic sliding-window model by con-sidering words within some window radius r (in-stead of a whole document).
Each instance of theterm t will have a contextual vector ct,win = et?r +?
+ et?1 + et+1 +?
+ et+r; context vectors for eachinstance (in a large corpus) would again be addedand normalized to create the overall vector vt,win.A directional model doubles the dimensionality ofthe vector and considers left- and right-context sepa-rately (half the indices for left-context, half for right-context), using a permutation to achieve one of thetwo contexts.
A permutated positional model uses aposition-specific permutation function to encode therelative word positions (rather than just left- or right-context) separately.
Again, vt would be summedand normalized over all instances of ct.Sentence vectors from any of these 4 RandomIndexing-based models (standard, windowed, direc-tional, positional) are just the sum of the vectors foreach term vS = ?t?S vt. We define 4 separate simi-larity metrics for STS as:simRI(S1, S2) = cos(vS1,vS2) (3)We used the semantic vectors package (Widdowsand Ferraro, 2008; Widdows and Cohen, 2010) inthe default configuration for the standard model.
Forthe windowed, directional, and positional models,we used a 6-word window radius with 200 dimen-sions and a seed length of 5.
All models weretrained on the raw text of the Penn Treebank WallStreet Journal corpus and a 100,075-article subset ofWikipedia.2.3 Semantic vectorial semantics measuresStructured vectorial semantics (SVS) composes dis-tributional semantic representations in syntacticcontext (Wu and Schuler, 2011).
Similarity met-rics defined with SVS inherently explore the quali-ties of a fully interactive syntax?semantics interface.While previous work evaluated the syntactic contri-butions of this model, the STS task allows us to eval-uate the phrase-level semantic validity of the model.We summarize SVS here as bottom-up vector com-position and parsing, then continue on to define theassociated similarity metrics.Each token in a sentence is modeled generativelyas a vector e?
of latent referents i?
in syntactic con-text c?
; each element in the vector is defined as:e?[i?]
= P(x?
?
lci?
), for preterm ?
(4)where l?
is a constant for preterminals.We write SVS vector composition between twoword (or phrase) vectors in linear algebra form,1 as-suming that we are composing the semantics of twochildren e?
and e?
in a binary syntactic tree intotheir parent e?
:e?
= M?
(L???
?
e?)?
(L???
?
e?)
?
1 (5)M is a diagonal matrix that encapsulates probabilis-tic syntactic information; the L matrices are lineartransformations that capture how semantically rele-vant child vectors are to the resulting vector (e.g.,L???
defines the the relevance of e?
to e?).
Thesematrices are defined such that the resulting e?
is asemantic vector of consistent P(x?
?
lci?)
probabil-ities.
Further detail is in our previous work (Wu,2010; Wu and Schuler, 2011).Similarity metrics can be defined in the SVSspace by comparing the distributions of the com-posed e?
vectors ?
i.e., our similarity metric isa comparison of the vector semantics at differentphrasal nodes.
We define two measures, one cor-responding to the top node c?
(e.g., with a syntacticconstituent c?
= ?S?
), and one corresponding to theleft and right largest child nodes (e.g.,, c?
= ?NP?and c ?= ?VP?
for a canonical subject?verb?objectsentence in English).simsvs-top(S1, S2) = cos(e?(S1),e?
(S2)) (6)simsvs-phr(S1, S2) =max(avgsim(e?(S1),e?
(S2);e ?
(S1),e ?(S2)),avgsim(e?
(S1),e ?
(S2);e ?(S1),e?
(S2))) (7)where avgsim() is the harmonic mean of the co-sine similarities between the two pairs of arguments.Top-level similarity comparisons in (6) amounts tocomparing the semantics of a whole sentence.
Thephrasal similarity function simsvs-phr(S1, S2) in (7)thus seeks to semantically align the two largest sub-trees, and weight them.
Compared to simsvs-top,1We define the operator ?
as point-by-point multiplicationof two diagonal matrices and 1 as a column vector of ones, col-lapsing a diagonal matrix onto a column vector.150the phrasal similarity function simsvs-phr(S1, S2) as-sumes there might be some information captured inthe child nodes that could be lost in the final compo-sition to the top node.In our experiments, we used the parser describedin Wu and Schuler (2011) with 1,000 headwordsand 10 relational clusters, trained on the Wall StreetJournal treebank.3 Feature combination frameworkThe similarity metrics of Section 2 were calculatedfor each of the sentence pairs in the training set, andlater the test set.
In combining these metrics, we ex-tended a DKPro Similarity baseline (3.1) with fea-ture selection (3.2) and source-specific models andclassification (3.3).3.1 Linear regression via DKPro SimilarityFor our baseline (MayoClinicNLPr1wtCDT), weused the UIMA-based DKPro Similarity systemfrom STS 2012 (Ba?r et al 2012).
Aside from thelarge number of sound similarity measures, this pro-vided linear regression through the WEKA package(Hall et al 2009) to combine all of the disparatesimilarity metrics into a single one, and some pre-processing.
Regression weights were determined onthe whole training set for each source.3.2 Feature selectionNot every feature was included in the final linear re-gression models.
To determine the best of the 47(DKPro?17, TakeLab?21, MayoClinicNLP?9) fea-tures, we performed a full forward-search on thespace of similarity measures.
In forward-search, weperform 10-fold cross-validation on the training setfor each measure, and pick the best one; in the nextround, that best metric is retained, and the remainingmetrics are considered for addition.
Rounds con-tinue until all the features are exhausted, though astopping-point is noted when performance no longerincreases.3.3 Subdomain source models andclassificationThere were 5 sources of data in the training set:paraphrase sentence pairs (MSRpar), sentence pairsfrom video descriptions (MSRvid), MT evaluationsentence pairs (MTnews and MTeuroparl) and glosspairs (OnWN).
In our submitted runs, we traineda separate, feature-selected model based on cross-validation for each of these data sources.
In train-ing data on cross-validation tests, training domain-specific models outperformed training a single con-glomerate model.In the test data, there were 4 sources, with 2appearing in training data (OnWN, SMT) and 2that were novel (FrameNet/Wordnet sense defini-tions (FNWN), European news headlines (head-lines)).
We examined two different strategies for ap-plying the 5-source trained models on these 4 testsets.
Both of these strategies rely on a multiclassrandom forest classifier, which we trained on the 47similarity metrics.First, for each sentence pair, we considered thefinal similarity score to be a weighted combinationof the similarity score from each of the 5 source-specific similarity models.
The combination weightswere determined by utilizing the classifier?s confi-dence scores.
Second, the final similarity was cho-sen as the single source-specific similarity score cor-responding to the classifier?s output class.4 EvaluationThe MayoClinicNLP team submitted three systemsto the STS-Core task.
We also include here a post-hoc run that was considered as a possible submis-sion.r1wtCDT This run used the 47 metrics fromDKPro, TakeLab, and MayoClinicNLP as afeature pool for feature selection.
Source-specific similarity metrics were combined withclassifier-confidence-score weights.r2CDT Same feature pool as run 1.
Best-match (asdetermined by classifier) source-specific simi-larity metric was used rather than a weightedcombination.r3wtCD TakeLab features were removed from thefeature pool (before feature selection).
Samesource combination as run 1.r4ALL Post-hoc run using all 47 metrics, but train-ing a single linear regression model rather thansource-specific models.151Table 2: Performance comparison.TEAM NAME headlines rank OnWNrank FNWNrank SMT rank mean rankUMBC EBIQUITY-ParingWords 0.7642 0.7529 0.5818 0.3804 0.6181 1UMBC EBIQUITY-galactus 0.7428 0.7053 0.5444 0.3705 0.5927 2deft-baseline 0.6532 0.8431 0.5083 0.3265 0.5795 3MayoClinicNLP-r4ALL 0.7275 0.7618 0.4359 0.3048 0.5707UMBC EBIQUITY-saiyan 0.7838 0.5593 0.5815 0.3563 0.5683 4MayoClinicNLP-r3wtCD 0.6440 43 0.8295 2 0.3202 47 0.3561 17 0.5671 5MayoClinicNLP-r1wtCDT 0.6584 33 0.7775 4 0.3735 26 0.3605 13 0.5649 6CLaC-RUN2 0.6921 0.7366 0.3793 0.3375 0.5587 7MayoClinicNLP-r2CDT 0.6827 23 0.6612 20 0.396 17 0.3946 5 0.5572 8NTNU-RUN1 0.7279 0.5952 0.3215 0.4015 0.5519 9CLaC-RUN1 0.6774 0.7667 0.3793 0.3068 0.5511 104.1 Competition performanceTable 2 shows the top 10 runs of 90 submitted inthe STS-Core task are shown, with our three sys-tems placing 5th, 6th, and 8th.
Additionally, we cansee that run 4 would have placed 4th.
Notice thatthere are significant source-specific differences be-tween the runs.
For example, while run 4 is betteroverall, runs 1?3 outperform it on all but the head-lines and FNWN datasets, i.e., the test datasets thatwere not present in the training data.
Thus, it isclear that the source-specific models are beneficialwhen the training data is in-domain, but a combinedmodel is more beneficial when no such training datais available.4.2 Feature selection analysis0 10 20 30 400.600.650.700.750.800.850.90StepPearson?sCorrelationCoefficientMSRparMSRvidSMTeuroparlOnWNSMTnewsALLFigure 1: Performance curve of feature selection forr1wtCDT, r2CDT, and r4ALLDue to the source-specific variability among theruns, it is important to know whether the forward-search feature selection performed as expected.
Forsource specific models (runs 1 and 3) and a com-bined model (run 4), Figure 1 shows the 10-foldcross-validation scores on the training set as the nextfeature is added to the model.
As we would ex-pect, there is an initial growth region where the firstfeatures truly complement one another and improveperformance significantly.
A plateau is reached foreach of the models, and some (e.g., SMTnews) evendecay if too many noisy features are added.The feature selection curves are as expected.
Be-cause the plateau regions are large, feature selectioncould be cut off at about 10 features, with gains inefficiency and perhaps little effect on accuracy.The resulting selected features for some of thetrained models are shown in Table 3.4.3 Contribution of MayoClinicNLP metricsWe determined whether including MayoClinicNLPfeatures was any benefit over a feature-selectedDKPro baseline.
Table 4 analyzes this questionby adding each of our measures in turn to a base-line feature-selected DKPro (dkselected).
Note thatthis baseline was extremely effective; it would haveranked 4th in the STS competition, outperformingour run 4.
Thus, metrics that improve this baselinemust truly be complementary metrics.
Here, we seethat only the phrasal SVSmeasure is able to improveperformance overall, largely by its contributions tothe most difficult categories, FNWN and SMT.
Infact, that system (dkselected + SVSePhrSimilari-tyMeasure) represents the best-performing run ofany that was produced in our framework.152Table 3: Top retained features for several linear regression models.OnWN - r1wtCDT and r2CDT (15 shown/19 selected) SMTnews - r1wtCDT and r2CDT (15 shown/17 selected) All - r4ALL (29 shown/29 selected)t ngram/ContentUnigramOverlap t other/RelativeInfoContentDifference t vec/LSAWordSimilarity weighted NYTt other/RelativeInfoContentDifference n-grams/CharacterNGramMeasure 2 n-grams/CharacterNGramMeasure 2t vec/LSAWordSimilarity weighted NYT t other/CaseMatches string/LongestCommonSubstringComparatoresa/ESA Wiktionary string/GreedyStringTiling 3 t other/NumbersOverlapt ngram/ContentBigramOverlap custom/RandomIndexingMeasure d200 wr6p t words/WordNetAugmentedWordOverlapn-grams/CharacterNGramMeasure 2 custom/StanfordNerMeasure overlap n-grams/WordNGramJaccardMeasure 1t words/WordNetAugmentedWordOverlap t vec/LSAWordSimilarity weighted NYT n-grams/CharacterNGramMeasure 3t ngram/BigramOverlap t other/SentenceSize t other/SentenceSizestring/GreedyStringTiling 3 custom/RandomIndexingMeasure d200 wr0 t other/RelativeInfoContentDifferencestring/LongestCommonSubsequenceNormComparator custom/SVSePhrSimilarityMeasure t ngram/ContentBigramOverlapcustom/RandomIndexingMeasure d200 wr0 esa/ESA Wiktionary n-grams/WordNGramJaccardMeasure 4custom/StanfordNerMeasure aligngst string/LongestCommonSubstringComparator t other/NumbersSizecustom/StanfordNerMeasure alignlcs t other/NumbersSize t other/NumbersSubsetcustom/StanfordNerMeasure overlap n-grams/WordNGramContainmentMeasure 2 stopword-filtered custom/SVSePhrSimilarityMeasurecustom/SVSePhrSimilarityMeasure custom/SVSeTopSimilarityMeasure custom/SemanticVectorsSimilarityMeasure d200 wr6pesa/ESA WordNetOnWN - r3wtCD (7 shown/7 selected) SMTnews - r3wtCD (15 shown/23 selected) esa/ESA Wiktionaryesa/ESA Wiktionary string/GreedyStringTiling 3 string/LongestCommonSubsequenceComparatorstring/LongestCommonSubsequenceComparator custom/StanfordNerMeasure overlap string/LongestCommonSubsequenceNormComparatorstring/GreedyStringTiling 3 n-grams/CharacterNGramMeasure 2 n-grams/WordNGramContainmentMeasure 1 stopword-filteredstring/LongestCommonSubsequenceNormComparator custom/RandomIndexingMeasure d200 wr6p word-sim/MCS06 Resnik WordNetstring/LongestCommonSubstringComparator n-grams/CharacterNGramMeasure 3 t ngram/ContentUnigramOverlapword-sim/MCS06 Resnik WordNet string/LongestCommonSubsequenceComparator n-grams/WordNGramContainmentMeasure 2 stopword-filteredn-grams/WordNGramContainmentMeasure 2 stopword-filtered custom/StanfordNerMeasure aligngst n-grams/WordNGramJaccardMeasure 2 stopword-filteredcustom/SVSePhrSimilarityMeasure t ngram/UnigramOverlapesa/ESA Wiktionary t ngram/BigramOverlapesa/ESA WordNet t other/StocksSizen-grams/WordNGramContainmentMeasure 2 stopword-filtered t words/GreedyLemmaAligningOverlapn-grams/WordNGramJaccardMeasure 1 t other/StocksOverlapstring/LongestCommonSubstringComparatorcustom/RandomIndexingMeasure d200 wr6dcustom/RandomIndexingMeasure d200 wr0Table 4: Adding customized features one at a time into optimized DKPro feature set.
Models are trained across allsources.headlines OnWN FNWN SMT meandkselected 0.70331 0.79752 0.38358 0.31744 0.571319dkselected + SVSePhrSimilarityMeasure 0.70178 0.79644 0.38685 0.32332 0.572774dkselected + RandomIndexingMeasure d200 wr0 0.70054 0.79752 0.38432 0.31615 0.570028dkselected + SVSeTopSimilarityMeasure 0.69873 0.79522 0.38815 0.31723 0.569533dkselected + RandomIndexingMeasure d200 wr6d 0.69944 0.79836 0.38416 0.31397 0.569131dkselected + RandomIndexingMeasure d200 wr6b 0.69992 0.79788 0.38435 0.31328 0.568957dkselected + RandomIndexingMeasure d200 wr6p 0.69878 0.79848 0.37876 0.31436 0.568617dkselected + StanfordNerMeasure aligngst 0.69446 0.79502 0.38703 0.31497 0.567212dkselected + StanfordNerMeasure overlap 0.69468 0.79509 0.38703 0.31466 0.567200dkselected + StanfordNerMeasure alignlcs 0.69451 0.79486 0.38657 0.31394 0.566807(dk + all custom) selected 0.70311 0.79887 0.37477 0.31665 0.570586Also, we see some source-specific behavior.
Noneof our introduced measures are able to improve theheadlines similarities.
However, random indexingimproves OnWN scores, several strategies improvethe FNWN metric, and simsvs-phr is the only viableperformance improvement on the SMT corpus.5 DiscussionMayo Clinic?s submissions to Semantic TextualSimilarity 2013 performed well, placing 5th, 6th,and 8th among 90 submitted systems.
We intro-duced similarity metrics that used different meansto do compositional distributional semantics alongwith some named entity-based measures, findingsome improvement especially for phrasal similar-ity from structured vectorial semantics.
Through-out, we utilized forward-search feature selection,which enhanced the performance of the models.
Wealso used source-based linear regression models andconsidered unseen sources as mixtures of existingsources; we found that in-domain data is neces-sary for smaller, source-based models to outperformlarger, conglomerate models.AcknowledgmentsThanks to the developers of the UKP DKPro sys-tem and the TakeLab system for making their codeavailable.
Also, thanks to James Masanz for initialimplementations of some similarity measures.153ReferencesEneko Agirre, Mona Diab, Daniel Cer, and AitorGonzalez-Agirre.
2012.
Semeval-2012 task 6: A piloton semantic textual similarity.
In Proceedings of theFirst Joint Conference on Lexical and ComputationalSemantics-Volume 1: Proceedings of the main confer-ence and the shared task, and Volume 2: Proceedingsof the Sixth International Workshop on Semantic Eval-uation, pages 385?393.
Association for ComputationalLinguistics.Eneko Agirre, Daniel Cer, Mona Diab, Aitor Gonzalez-Agirre, and Weiwei Guo.
2013.
*sem 2013 sharedtask: Semantic textual similarity, including a pilot ontyped-similarity.
In *SEM 2013: The Second JointConference on Lexical and Computational Semantics.Association for Computational Linguistics.Lloyd Allison and Trevor I Dix.
1986.
A bit-stringlongest-common-subsequence algorithm.
InformationProcessing Letters, 23(5):305?310.Daniel Ba?r, Chris Biemann, Iryna Gurevych, and TorstenZesch.
2012.
Ukp: Computing semantic textual sim-ilarity by combining multiple content similarity mea-sures.
In Proceedings of the First Joint Conferenceon Lexical and Computational Semantics-Volume 1:Proceedings of the main conference and the sharedtask, and Volume 2: Proceedings of the Sixth Interna-tional Workshop on Semantic Evaluation, pages 435?440.
Association for Computational Linguistics.Scott Deerwester, Susan Dumais, George W. Furnas,Thomas K. Landauer, and Richard Harshman.
1990.Indexing by latent semantic analysis.
Journal of theAmerican Society for Information Science, 41(6):391?407.Katrin Erk and Sebastian Pado?.
2008.
A structured vec-tor space model for word meaning in context.
In Pro-ceedings of EMNLP 2008.Jenny Rose Finkel, Trond Grenager, and ChristopherManning.
2005.
Incorporating non-local informationinto information extraction systems by gibbs sampling.In Proceedings of the 43rd Annual Meeting on Associ-ation for Computational Linguistics, pages 363?370.Association for Computational Linguistics.Mark Hall, Eibe Frank, Geoffrey Holmes, BernhardPfahringer, Peter Reutemann, and Ian H. Witten.2009.
The weka data mining software: an update.SIGKDD Explor.
Newsl., 11(1):10?18, November.Pentti Kanerva, Jan Kristofersson, and Anders Holst.2000.
Random indexing of text samples for latent se-mantic analysis.
In Proceedings of the 22nd annualconference of the cognitive science society, volume1036.
Citeseer.T.K.
Landauer and S.T.
Dumais.
1997.
A Solution toPlato?s Problem: The Latent Semantic Analysis The-ory of Acquisition, Induction, and Representation ofKnowledge.
Psychological Review, 104:211?240.Alon Lavie and Michael J Denkowski.
2009.
The meteormetric for automatic evaluation of machine translation.Machine translation, 23(2-3):105?115.Jeff Mitchell and Mirella Lapata.
2008.
Vector-basedmodels of semantic composition.
In Proceedings ofACL-08: HLT, pages 236?244, Columbus, OH.M.
Sahlgren.
2005.
An introduction to random index-ing.
In Methods and Applications of Semantic Index-ing Workshop at the 7th International Conference onTerminology and Knowledge Engineering, TKE, vol-ume 5.Frane ?Saric?, Goran Glavas?, Mladen Karan, Jan ?Snajder,and Bojana Dalbelo Bas?ic?.
2012.
Takelab: Sys-tems for measuring semantic text similarity.
In Pro-ceedings of the Sixth International Workshop on Se-mantic Evaluation (SemEval 2012), pages 441?448,Montre?al, Canada, 7-8 June.
Association for Compu-tational Linguistics.Dominic Widdows and Trevor Cohen.
2010.
The seman-tic vectors package: New algorithms and public toolsfor distributional semantics.
In Semantic Computing(ICSC), 2010 IEEE Fourth International Conferenceon, pages 9?15.
IEEE.D.
Widdows and K. Ferraro.
2008.
Semantic vec-tors: a scalable open source package and online tech-nology management application.
Proceedings of theSixth International Language Resources and Evalua-tion (LREC?08), pages 1183?1190.Michael J Wise.
1996.
Yap3: Improved detection of sim-ilarities in computer program and other texts.
In ACMSIGCSE Bulletin, volume 28, pages 130?134.
ACM.StephenWu andWilliam Schuler.
2011.
Structured com-position of semantic vectors.
In Proceedings of the In-ternational Conference on Computational Semantics.Stephen Tze-Inn Wu.
2010.
Vectorial Representationsof Meaning for a Computational Model of LanguageComprehension.
Ph.D. thesis, Department of Com-puter Science and Engineering, University of Min-nesota.154
