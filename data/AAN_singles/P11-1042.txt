Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 409?419,Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational LinguisticsUnsupervised Word Alignment with Arbitrary FeaturesChris Dyer Jonathan Clark Alon Lavie Noah A. SmithLanguage Technologies InstituteCarnegie Mellon UniversityPittsburgh, PA 15213, USA{cdyer,jhclark,alavie,nasmith}@cs.cmu.eduAbstractWe introduce a discriminatively trained, glob-ally normalized, log-linear variant of the lex-ical translation models proposed by Brownet al (1993).
In our model, arbitrary, non-independent features may be freely incorpo-rated, thereby overcoming the inherent limita-tion of generative models, which require thatfeatures be sensitive to the conditional inde-pendencies of the generative process.
How-ever, unlike previous work on discriminativemodeling of word alignment (which also per-mits the use of arbitrary features), the param-eters in our models are learned from unanno-tated parallel sentences, rather than from su-pervised word alignments.
Using a varietyof intrinsic and extrinsic measures, includingtranslation performance, we show our modelyields better alignments than generative base-lines in a number of language pairs.1 IntroductionWord alignment is an important subtask in statis-tical machine translation which is typically solvedin one of two ways.
The more common approachuses a generative translation model that relates bilin-gual string pairs using a latent alignment variable todesignate which source words (or phrases) generatewhich target words.
The parameters in these modelscan be learned straightforwardly from parallel sen-tences using EM, and standard inference techniquescan recover most probable alignments (Brown et al,1993).
This approach is attractive because it onlyrequires parallel training data.
An alternative to thegenerative approach uses a discriminatively trainedalignment model to predict word alignments in theparallel corpus.
Discriminative models are attractivebecause they can incorporate arbitrary, overlappingfeatures, meaning that errors observed in the predic-tions made by the model can be addressed by engi-neering new and better features.
Unfortunately, bothapproaches are problematic, but in different ways.In the case of discriminative alignment mod-els, manual alignment data is required for train-ing, which is problematic for at least three reasons.Manual alignments are notoriously difficult to cre-ate and are available only for a handful of languagepairs.
Second, manual alignments impose a commit-ment to a particular preprocessing regime; this canbe problematic since the optimal segmentation fortranslation often depends on characteristics of thetest set or size of the available training data (Habashand Sadat, 2006) or may be constrained by require-ments of other processing components, such parsers.Third, the ?correct?
alignment annotation for differ-ent tasks may vary: for example, relatively denser orsparser alignments may be optimal for different ap-proaches to (downstream) translation model induc-tion (Lopez, 2008; Fraser, 2007).Generative models have a different limitation: thejoint probability of a particular setting of the ran-dom variables must factorize according to steps in aprocess that successively ?generates?
the values ofthe variables.
At each step, the probability of somevalue being generated may depend only on the gen-eration history (or a subset thereof), and the possiblevalues a variable will take must form a locally nor-malized conditional probability distribution (CPD).While these locally normalized CPDs may be pa-409rameterized so as to make use of multiple, overlap-ping features (Berg-Kirkpatrick et al, 2010), the re-quirement that models factorize according to a par-ticular generative process imposes a considerable re-striction on the kinds of features that can be incor-porated.
When Brown et al (1993) wanted to in-corporate a fertility model to create their Models 3through 5, the generative process used in Models 1and 2 (where target words were generated one byone from source words independently of each other)had to be abandoned in favor of one in which eachsource word had to first decide how many targets itwould generate.1In this paper, we introduce a discriminativelytrained, globally normalized log-linear model of lex-ical translation that can incorporate arbitrary, over-lapping features, and use it to infer word alignments.Our model enjoys the usual benefits of discrimina-tive modeling (e.g., parameter regularization, well-understood learning algorithms), but is trained en-tirely from parallel sentences without gold-standardword alignments.
Thus, it addresses the two limita-tions of current word alignment approaches.This paper is structured as follows.
We begin byintroducing our model (?2), and follow this with adiscussion of tractability, parameter estimation, andinference using finite-state techniques (?3).
We thendescribe the specific features we used (?4) and pro-vide experimental evaluation of the model, showingsubstantial improvements in three diverse languagepairs (?5).
We conclude with an analysis of relatedprior work (?6) and a general discussion (?8).2 ModelIn this section, we develop a conditional modelp(t | s) that, given a source language sentence s withlength m = |s|, assigns probabilities to a target sen-tence t with length n, where each word tj is an el-ement in the finite target vocabulary ?.
We beginby using the chain rule to factor this probability intotwo components, a translation model and a lengthmodel.p(t | s) = p(t, n | s) = p(t | s, n)?
??
?translation model?
p(n | s)?
??
?length model1Moore (2005) likewise uses this example to motivate theneed for models that support arbitrary, overlapping features.In the translation model, we then assume that eachword tj is a translation of one source word, or aspecial null token.
We therefore introduce a latentalignment variable a = ?a1, a2, .
.
.
, an?
?
[0,m]n,where aj = 0 represents a special null token.p(t | s, n) =?ap(t, a | s, n)So far, our model is identical to that of (Brown etal., 1993); however, we part ways here.
Rather thanusing the chain rule to further decompose this prob-ability and motivate opportunities to make indepen-dence assumptions, we use a log-linear model withparameters ?
?
Rk and feature vector function Hthat maps each tuple ?a, s, t, n?
into Rk to modelp(t, a | s, n) directly:p?
(t, a | s, n) =exp?>H(t, a, s, n)Z?
(s, n), whereZ?
(s, n) =?t??
?n?a?exp?>H(t?, a?, s, n)Under some reasonable assumptions (a finite targetvocabulary ?
and that all ?k < ?
), the partitionfunction Z?
(s, n) will always take on finite values,guaranteeing that p(t, a | s, n) is a proper probabilitydistribution.So far, we have said little about the length model.Since our intent here is to use the model for align-ment, where both the target length and target stringare observed, it will not be necessary to commit toany length model, even during training.3 Tractability, Learning, and InferenceThe model introduced in the previous section isextremely general, and it can incorporate featuressensitive to any imaginable aspects of a sentencepair and their alignment, from linguistically in-spired (e.g., an indicator feature for whether boththe source and target sentences contain a verb), tothe mundane (e.g., the probability of the sentencepair and alignment under Model 1), to the absurd(e.g., an indicator if s and t are palindromes of eachother).However, while our model can make use of arbi-trary, overlapping features, when designing featurefunctions it is necessary to balance expressivenessand the computational complexity of the inference410algorithms used to reason under models that incor-porate these features.2 To understand this tradeoff,we assume that the random variables being modeled(t, a) are arranged into an undirected graph G suchthat the vertices represent the variables and the edgesare specified so that the feature function H decom-poses linearly over all the cliques C in G,H(t, a, s, n) =?Ch(tC , aC , s, n) ,where tC and aC are the components associated withsubgraph C and h(?)
is a local feature vector func-tion.
In general, exact inference is exponential inthe width of tree-decomposition of G, but, given afixed width, they can be solved in polynomial timeusing dynamic programming.
For example, whenthe graph has a sequential structure, exact infer-ence can be carried out using the familiar forward-backward algorithm (Lafferty et al, 2001).
Al-though our features look at more structure than this,they are designed to keep treewidth low, meaningexact inference is still possible with dynamic pro-gramming.
Figure 1 gives a graphical representationof our model as well as the more familiar genera-tive (directed) variants.
The edge set in the depictedgraph is determined by the features that we use (?4).3.1 Parameter LearningTo learn the parameters of our model, we select the??
that minimizes the `1 regularized conditional log-likelihood of a set of training data T :L(?)
=???s,t??Tlog?ap?
(t, a | s, n) + ?
?k|?k| .Because of the `1 penalty, this objective is not every-where differentiable, but the gradient with respect tothe parameters of the log-likelihood term is as fol-lows.?L??=??s,t??TEp?(a|s,t,n)[H(?)]?
Ep?(t,a|s,n)[H(?
)](1)To optimize L, we employ an online method thatapproximates `1 regularization and only depends on2One way to understand expressiveness is in terms of inde-pendence assumptions, of course.
Research in graphical modelshas done much to relate independence assumptions to the com-plexity of inference algorithms (Koller and Friedman, 2009).the gradient of the unregularized objective (Tsu-ruoka et al, 2009).
This method is quite attrac-tive since it is only necessary to represent the activefeatures, meaning impractically large feature spacescan be searched provided the regularization strengthis sufficiently high.
Additionally, not only has thistechnique been shown to be very effective for opti-mizing convex objectives, but evidence suggests thatthe stochasticity of online algorithms often resultsin better solutions than batch optimizers for non-convex objectives (Liang and Klein, 2009).
On ac-count of the latent alignment variable in our model,L is non-convex (as is the likelihood objective of thegenerative variant).To choose the regularization strength ?
and theinitial learning rate ?0,3 we trained several mod-els on a 10,000-sentence-pair subset of the French-English Hansards, and chose values that minimizedthe alignment error rate, as evaluated on a 447 sen-tence set of manually created alignments (Mihalceaand Pedersen, 2003).
For the remainder of the ex-periments, we use the values we obtained, ?
= 0.4and ?0 = 0.3.3.2 Inference with WFSAsWe now describe how to use weighted finite-stateautomata (WFSAs) to compute the quantities neces-sary for training.
We begin by describing the idealWFSA representing the full translation search space,which we call the discriminative neighborhood, andthen discuss strategies for reducing its size in thenext section, since the full model is prohibitivelylarge, even with small data sets.For each training instance ?s, t?, the contributionto the gradient (Equation 1) is the difference in twovectors of expectations.
The first term is the ex-pected value of H(?)
when observing ?s, n, t?
andletting a range over all possible alignments.
Thesecond is the expectation of the same function, butobserving only ?s, n?
and letting t?
and a take onany possible values (i.e., all possible translationsof length n and all their possible alignments to s).To compute these expectations, we can constructa WFSA representing the discriminative neighbor-hood, the set ?n?
[0,m]n, such that every path fromthe start state to goal yields a pair ?t?, a?
with weight3For the other free parameters of the algorithm, we use thedefault values recommended by Tsuruoka et al (2009).411a1a2a3ant1t2t3tnsnFully directed model (Brown et al, 1993;Vogel et al, 1996; Berg-Kirkpatrick et al, 2010)Our model...a1a2a3ant1t2t3tnsn.........ss s s sss sFigure 1: A graphical representation of a conventional generative lexical translation model (left) and our model withan undirected translation model.
For clarity, the observed node s (representing the full source sentence) is drawn inmultiple locations.
The dashed lines indicate a dependency on a deterministic mapping of tj (not its complete value).H(t?, a, s, n).
With our feature set (?4), number ofstates in this WFSA isO(m?n) since at each targetindex j, there is a different state for each possible in-dex of the source word translated at position j ?
1.4Once the WFSA representing the discriminativeneighborhood is built, we use the forward-backwardalgorithm to compute the second expectation term.We then intersect the WFSA with an unweightedFSA representing the target sentence t (because ofthe restricted structure of our WFSA, this amountsto removing edges), and finally run the forward-backward algorithm on the resulting WFSA to com-pute the first expectation.3.3 Shrinking the DiscriminativeNeighborhoodThe WFSA we constructed requires m?
|?| transi-tions between all adjacent states, which is impracti-cally large.
We can reduce the number of edges byrestricting the set of words that each source word cantranslate into.
Thus, the model will not discriminate4States contain a bit more information than the index of theprevious source word, for example, there is some additional in-formation about the previous translation decision that is passedforward.
However, the concept of splitting states to guaranteedistinct paths for different values of non-local features is wellunderstood by NLP and machine translation researchers, andthe necessary state structure should be obvious from the featuredescription.among all candidate target strings in ?n, but ratherin ?ns , where ?s =?mi=1 ?si , and where ?s is theset of target words that s may translate into.5We consider four different definitions of ?s: (1)the baseline of the full target vocabulary, (2) the setof all target words that co-occur in sentence pairscontaining s, (3) the most probable words underIBM Model 1 that are above a threshold, and (4) thesame Model 1, except we add a sparse symmetricDirichlet prior (?
= 0.01) on the translation distri-butions and use the empirical Bayes (EB) method toinfer a point estimate, using variational inference.Table 1: Comparison of alternative definitions ?s (arrowsindicate whether higher or lower is better).
?s time (s) ?
?s |?s| ?
AER ?= ?
22.4 86.0M 0.0co-occ.
8.9 0.68M 0.0Model 1 0.2 0.38M 6.2EB-Model 1 1.0 0.15M 2.9Table 1 compares the average per-sentence timerequired to run the inference algorithm described5Future work will explore alternative formulations of thediscriminative neighborhood with the goal of further improvinginference efficiency.
Smith and Eisner (2005) show that goodperformance on unsupervised syntax learning is possible evenwhen learning from very small discriminative neighborhoods,and we posit that the same holds here.412above under these four different definitions of ?s ona 10,000 sentence subset of the Hansards French-English corpus that includes manual word align-ments.
While our constructions guarantee that allreferences are reachable even in the reduced neigh-borhoods, not all alignments between source and tar-get are possible.
The last column is the oracle AER.Although EB variant of Model 1 neighborhood isslightly more expensive to do inference with thanregular Model 1, we use it because it has a loweroracle AER.6During alignment prediction (rather than duringtraining) for a sentence pair ?s, t?, it is possible tofurther restrict ?s to be just the set of words occur-ring in t, making extremely fast inference possible(comparable to that of the generative HMM align-ment model).4 FeaturesFeature engineering lets us encode knowledge aboutwhat aspects of a translation derivation are useful inpredicting whether it is good or not.
In this sectionwe discuss the features we used in our model.
Manyof these were taken from the discriminative align-ment modeling literature, but we also note that ourfeatures can be much more fine-grained than thoseused in supervised alignment modeling, since welearn our models from a large amount of paralleldata, rather than a small number of manual align-ments.Word association features.
Word association fea-tures are at the heart of all lexical translation models,whether generative or discriminative.
In addition tofine-grained boolean indicator features ?saj , tj?
forpair types, we have several orthographic features:identity, prefix identity, and an orthographic simi-larity measure designed to be informative for pre-dicting the translation of named entities in languagesthat use similar alphabets.7 It has the property thatsource-target pairs of long words that are similar aregiven a higher score than word pairs that are shortand similar (dissimilar pairs have a score near zero,6We included all translations whose probability was withina factor of 10?4 of the highest probability translation.7In experiments with Urdu, which uses an Arabic-derivedscript, the orthographic feature was computed after first ap-plying a heuristic Romanization, which made the orthographicforms somewhat comparable.regardless of length).
We also include ?global?
asso-ciation scores that are precomputed by looking at thefull training data: Dice?s coefficient (discretized),which we use to measure association strength be-tween pairs of source and target word types acrosssentence pairs (Dice, 1945), IBM Model 1 forwardand reverse probabilities, and the geometric mean ofthe Model 1 forward and reverse probabilities.
Fi-nally, we also cluster the source and target vocab-ularies (Och, 1999) and include class pair indicatorfeatures, which can learn generalizations that, e.g.,?nouns tend to translate into nouns but not modalverbs.
?Positional features.
Following Blunsom andCohn (2006), we include features indicatingcloseness to the alignment matrix diagonal,h(aj , j,m, n) =??
?ajm ?jn???.
We also conjoin thisfeature with the source word class type indicator toenable the model to learn that certain word typesare more or less likely to favor a location on thediagonal (e.g.
Urdu?s sentence-final verbs).Source features.
Some words are functional el-ements that fulfill purely grammatical roles andshould not be the ?source?
of a translation.
For ex-ample, Romance languages require a preposition inthe formation of what could be a noun-noun com-pound in English, thus, it may be useful to learn notto translate certain words (i.e.
they should not par-ticipate in alignment links), or to have a bias to trans-late others.
To capture this intuition we include anindicator feature that fires each time a source vocab-ulary item (and source word class) participates in analignment link.Source path features.
One class of particularlyuseful features assesses the goodness of the align-ment ?path?
through the source sentence (Vogel etal., 1996).
Although assessing the predicted pathrequires using nonlocal features, since each aj ?
[0,m] and m is relatively small, features can be sen-sitive to a wider context than is often practical.We use many overlapping source path features,some of which are sensitive to the distance and di-rection of the jump between aj?1 and aj , and oth-ers which are sensitive to the word pair these twopoints define, and others that combine all three el-ements.
The features we use include a discretized413jump distance, the discretized jump conjoined withan indicator feature for the target length n, the dis-cretized jump feature conjoined with the class of saj ,and the discretized jump feature conjoined with theclass of saj and saj?1 .
To discretize the features wetake a log transform (base 1.3) of the jump width andlet an indicator feature fire for the closest integer.In addition to these distance-dependent features, wealso include indicator features that fire on bigrams?saj?1 , saj ?
and their word classes.
Thus, this fea-ture can capture our intuition that, e.g., adjectivesare more likely to come before or after a noun indifferent languages.Target string features.
Features sensitive to mul-tiple values in the predicted target string or latentalignment variable must be handled carefully for thesake of computational tractability.
While featuresthat look at multiple source words can be computedlinearly in the number of source words considered(since the source string is always observable), fea-tures that look at multiple target words require ex-ponential time and space!8 However, by groupingthe tj?s into coarse equivalence classes and lookingat small numbers of variables, it is possible to incor-porate such features.
We include a feature that fireswhen a word translates as itself (for example, a nameor a date, which occurs in languages that share thesame alphabet) in position j, but then is translatedagain (as something else) in position j ?
1 or j + 1.5 ExperimentsWe now turn to an empirical assessment of ourmodel.
Using various datasets, we evaluate theperformance of the models?
intrinsic quality andtheirtheir alignments?
contribution to a standard ma-chine translation system.
We make use of parallelcorpora from languages with very different typolo-gies: a small (0.8M words) Chinese-English corpusfrom the tourism and travel domain (Takezawa et al,2002), a corpus of Czech-English news commen-tary (3.1M words),9 and an Urdu-English corpus(2M words) provided by NIST for the 2009 OpenMT Evaluation.
These pairs were selected sinceeach poses different alignment challenges (word or-8This is of course what makes history-based language modelintegration an inference challenge in translation.9http://statmt.org/wmt10der in Chinese and Urdu, morphological complex-ity in Czech, and a non-alphabetic writing system inChinese), and confining ourselves to these relativelysmall corpora reduced the engineering overhead ofgetting an implementation up and running.
Futurework will explore the scalability characteristics andlimits of the model.5.1 MethodologyFor each language pair, we train two log-lineartranslation models as described above (?3), oncewith English as the source and once with Englishas the target language.
For a baseline, we usethe Giza++ toolkit (Och and Ney, 2003) to learnModel 4, again in both directions.
We symmetrizethe alignments from both model types using thegrow-diag-final-and heuristic (Koehn et al,2003) producing, in total, six alignment sets.
Weevaluate them both intrinsically and in terms of theirperformance in a translation system.Since we only have gold alignments for Czech-English (Bojar and Prokopova?, 2006), we can re-port alignment error rate (AER; Och and Ney, 2003)only for this pair.
However, we offer two furthermeasures that we believe are suggestive and thatdo not require gold alignments.
One is the aver-age alignment ?fertility?
of source words that occuronly a single time in the training data (so-called ha-pax legomena).
This assesses the impact of a typicalalignment problem observed in generative modelstrained to maximize likelihood: infrequent sourcewords act as ?garbage collectors?, with many targetwords aligned to them (the word dislike in the Model4 alignment in Figure 2 is an example).
Thus, we ex-pect lower values of this measure to correlate withbetter alignments.
The second measure is the num-ber of rule types learned in the grammar inductionprocess used for translation that match the transla-tion test sets.10 While neither a decrease in the aver-age singleton fertility nor an increase in the numberof rules induced guarantees better alignment quality,we believe it is reasonable to assume that they arepositively correlated.For the translation experiments in each languagepair, we make use of the cdec decoder (Dyer et al,10This measure does not assess whether the rule types aregood or bad, but it does suggest that the system?s coverage isgreater.4142010), inducing a hierarchical phrase based trans-lation grammar from two sets of symmetrized align-ments using the method described by Chiang (2007).Additionally, recent work that has demonstrated thatextracting rules from n-best alignments has value(Liu et al, 2009; Venugopal et al, 2008).
Wetherefore define a third condition where rules areextracted from the corpus under both the Model 4and discriminative alignments and merged to forma single grammar.
We incorporate a 3-gram lan-guage model learned from the target side of thetraining data as well as 50M supplemental wordsof monolingual training data consisting of sentencesrandomly sampled from the English Gigaword, ver-sion 4.
In the small Chinese-English travel domainexperiment, we just use the LM estimated from thebitext.
The parameters of the translation model weretuned using ?hypergraph?
minimum error rate train-ing (MERT) to maximize BLEU on a held-out de-velopment set (Kumar et al, 2009).
Results arereported using case-insensitive BLEU (Papineni etal., 2002), METEOR11 (Lavie and Denkowski, 2009),and TER (Snover et al, 2006), with the number ofreferences varying by task.
Since MERT is a non-deterministic optimization algorithm and results canvary considerably between runs, we follow Clark etal.
(2011) and report the average score and stan-dard deviation of 5 independent runs, 30 in the caseof Chinese-English, since observed variance washigher.5.2 Experimental ResultsCzech-English.
Czech-English poses problemsfor word alignment models since, unlike English,Czech words have a complex inflectional morphol-ogy, and the syntax permits relatively free word or-der.
For this language pair, we evaluate alignmenterror rate using the manual alignment corpus de-scribed by Bojar and Prokopova?
(2006).
Table 2summarizes the results.Chinese-English.
Chinese-English poses a differ-ent set of problems for alignment.
While Chinesewords have rather simple morphology, the Chinesewriting system renders our orthographic featuresuseless.
Despite these challenges, the Chinese re-11Meteor 1.0 with exact, stem, synonymy, and paraphrasemodules and HTER parameters.Table 2: Czech-English experimental results.
??sing.
is theaverage fertility of singleton source words.AER ?
??sing.
?
# rules ?Model 4 e | f 24.8 4.1f | e 33.6 6.6sym.
23.4 2.7 993,953Our model e | f 21.9 2.3f | e 29.3 3.8sym.
20.5 1.6 1,146,677Alignment BLEU ?
METEOR ?
TER ?Model 4 16.3?0.2 46.1?0.1 67.4?0.3Our model 16.5?0.1 46.8?0.1 67.0?0.2Both 17.4?0.1 47.7?0.1 66.3?0.5sults in Table 3 show the same pattern of results asseen in Czech-English.Table 3: Chinese-English experimental results.??sing.
?
# rules ?Model 4 e | f 4.4f | e 3.9sym.
3.6 52,323Our model e | f 3.5f | e 2.6sym.
3.1 54,077Alignment BLEU ?
METEOR ?
TER ?Model 4 56.5?0.3 73.0?0.4 29.1?0.3Our model 57.2?0.8 73.8?0.4 29.3?1.1Both 59.1?0.6 74.8?0.7 27.6?0.5Urdu-English.
Urdu-English is a more challeng-ing language pair for word alignment than the pre-vious two we have considered.
The parallel data isdrawn from numerous genres, and much of it was ac-quired automatically, making it quite noisy.
So ourmodels must not only predict good translations, theymust cope with bad ones as well.
Second, there hasbeen no previous work on discriminative modelingof Urdu, since, to our knowledge, no manual align-ments have been created.
Finally, unlike English,Urdu is a head-final language: not only does it haveSOV word order, but rather than prepositions, it haspost-positions, which follow the nouns they modify,meaning its large scale word order is substantially415different from that of English.
Table 4 demonstratesthe same pattern of improving results with our align-ment model.Table 4: Urdu-English experimental results.??sing.
?
# rules ?Model 4 e | f 6.5f | e 8.0sym.
3.2 244,570Our model e | f 4.8f | e 8.3sym.
2.3 260,953Alignment BLEU ?
METEOR ?
TER ?Model 4 23.3?0.2 49.3?0.2 68.8?0.8Our model 23.4?0.2 49.7?0.1 67.7?0.2Both 24.1?0.2 50.6?0.1 66.8?0.55.3 AnalysisThe quantitative results presented in this sectionstrongly suggest that our modeling approach pro-duces better alignments.
In this section, we try tocharacterize how the model is doing what it doesand what it has learned.
Because of the `1 regular-ization, the number of active (non-zero) features inthe inferred models is small, relative to the numberof features considered during training.
The num-ber of active features ranged from about 300k forthe small Chinese-English corpus to 800k for Urdu-English, which is less than one tenth of the availablefeatures in both cases.
In all models, the coarse fea-tures (Model 1 probabilities, Dice coefficient, coarsepositional features, etc.)
typically received weightswith large magnitudes, but finer features also playedan important role.Language pair differences manifested themselvesin many ways in the models that were learned.For example, orthographic features were (unsurpris-ingly) more valuable in Czech-English, with theirlargely overlapping alphabets, than in Chinese orUrdu.
Examining the more fine-grained features isalso illuminating.
Table 5 shows the most highlyweighted source path bigram features on the threemodels where English was the source language, andin each, we may observe some interesting character-istics of the target language.
Left-most is English-Czech.
At first it may be surprising that words likesince and that have a highly weighted feature fortransitioning to themselves.
However, Czech punc-tuation rules require that relative clauses and sub-ordinating conjunctions be preceded by a comma(which is only optional or outright forbidden in En-glish), therefore our model translates these wordstwice, once to produce the comma, and a secondtime to produce the lexical item.
The middle col-umn is the English-Chinese model.
In the trainingdata, many of the sentences are questions directed toa second person, you.
However, Chinese questionsdo not invert and the subject remains in the canon-ical first position, thus the transition from the startof sentence to you is highly weighted.
Finally, Fig-ure 2 illustrates how Model 4 (left) and our discrimi-native model (right) align an English-Urdu sentencepair (the English side is being conditioned on in bothmodels).
A reflex of Urdu?s head-final word orderis seen in the list of most highly weighted bigrams,where a path through the English source where verbsthat transition to end-of-sentence periods are predic-tive of good translations into Urdu.Table 5: The most highly weighted source path bigramfeatures in the English-Czech, -Chinese, and -Urdu mod-els.Bigram ?k.
?/s?
3.08like like 1.19one of 1.06?
.
0.95that that 0.92is but 0.92since since 0.84?s?
when 0.83, how 0.83, not 0.83Bigram ?k.
?/s?
2.67?
?
2.25?s?
please 2.01much ?
1.61?s?
if 1.58thank you 1.47?s?
sorry 1.46?s?
you 1.45please like 1.24?s?
this 1.19Bigram ?k.
?/s?
1.87?s?
this 1.24will .
1.17are .
1.16is .
1.09is that 1.00have .
0.97has .
0.96was .
0.91will ?/s?
0.886 Related WorkThe literature contains numerous descriptions of dis-criminative approaches to word alignment motivatedby the desire to be able to incorporate multiple,overlapping knowledge sources (Ayan et al, 2005;Moore, 2005; Taskar et al, 2005; Blunsom andCohn, 2006; Haghighi et al, 2009; Liu et al, 2010;DeNero and Klein, 2010; Setiawan et al, 2010).This body of work has been an invaluable sourceof useful features.
Several authors have dealt withthe problem training log-linear models in an unsu-416IBM Model 4 alignment Our model's alignmentFigure 2: Example English-Urdu alignment under IBM Model 4 (left) and our discriminative model (right).
Model4 displays two characteristic errors: garbage collection and an overly-strong monotonicity bias.
Whereas our modeldoes not exhibit these problems, and in fact, makes no mistakes in the alignment.pervised setting.
The contrastive estimation tech-nique proposed by Smith and Eisner (2005) is glob-ally normalized (and thus capable of dealing with ar-bitrary features), and closely related to the model wedeveloped; however, they do not discuss the problemof word alignment.
Berg-Kirkpatrick et al (2010)learn locally normalized log-linear models in a gen-erative setting.
Globally normalized discriminativemodels with latent variables (Quattoni et al, 2004)have been used for a number of language processingproblems, including MT (Dyer and Resnik, 2010;Blunsom et al, 2008a).
However, this previouswork relied on translation grammars constructed us-ing standard generative word alignment processes.7 Future WorkWhile we have demonstrated that this model can besubstantially useful, it is limited in some importantways which are being addressed in ongoing work.First, training is expensive, and we are exploring al-ternatives to the conditional likelihood objective thatis currently used, such as contrastive neighborhoodsadvocated by (Smith and Eisner, 2005).
Addition-ally, there is much evidence that non-local featureslike the source word fertility are (cf.
IBM Model 3)useful for translation and alignment modeling.
To betruly general, it must be possible to utilize such fea-tures.
Unfortunately, features like this that dependon global properties of the alignment vector, a, makethe inference problem NP-hard, and approximationsare necessary.
Fortunately, there is much recentwork on approximate inference techniques for incor-porating nonlocal features (Blunsom et al, 2008b;Gimpel and Smith, 2009; Cromie`res and Kurohashi,2009; Weiss and Taskar, 2010), suggesting that thisproblem too can be solved using established tech-niques.8 ConclusionWe have introduced a globally normalized, log-linear lexical translation model that can be traineddiscriminatively using only parallel sentences,which we apply to the problem of word alignment.Our approach addresses two important shortcomingsof previous work: (1) that local normalization ofgenerative models constrains the features that can beused, and (2) that previous discriminatively trainedword alignment models required supervised align-ments.
According to a variety of measures in a vari-ety of translation tasks, this model produces superioralignments to generative approaches.
Furthermore,the features learned by our model reveal interestingcharacteristics of the language pairs being modeled.AcknowledgmentsThis work was supported in part by the DARPA GALEprogram; the U. S. Army Research Laboratory and theU.
S. Army Research Office under contract/grant num-417ber W911NF-10-1-0533; and the National Science Foun-dation through grants IIS-0844507, IIS-0915187, IIS-0713402, and IIS-0915327 and through TeraGrid re-sources provided by the Pittsburgh Supercomputing Cen-ter under grant number TG-DBS110003.
We thankOndr?ej Bojar for providing the Czech-English alignmentdata, and three anonymous reviewers for their detailedsuggestions and comments on an earlier draft of this pa-per.ReferencesN.
F. Ayan, B. J. Dorr, and C. Monz.
2005.
NeurAlign:combining word alignments using neural networks.
InProc.
of HLT-EMNLP.T.
Berg-Kirkpatrick, A.
Bouchard-Co?te?, J. DeNero, andD.
Klein.
2010.
Painless unsupervised learning withfeatures.
In Proc.
of NAACL.P.
Blunsom and T. Cohn.
2006.
Discriminative wordalignment with conditional random fields.
In Proc.
ofACL.P.
Blunsom, T. Cohn, and M. Osborne.
2008a.
A dis-criminative latent variable model for statistical ma-chine translation.
In Proc.
of ACL-HLT.P.
Blunsom, T. Cohn, and M. Osborne.
2008b.
Proba-bilistic inference for machine translation.
In Proc.
ofEMNLP 2008.O.
Bojar and M. Prokopova?.
2006.
Czech-English wordalignment.
In Proc.
of LREC.P.
F. Brown, V. J. Della Pietra, S. A. Della Pietra, andR.
L. Mercer.
1993.
The mathematics of statisticalmachine translation: parameter estimation.
Computa-tional Linguistics, 19(2):263?311.D.
Chiang.
2007.
Hierarchical phrase-based translation.Computational Linguistics, 33(2):201?228.J.
Clark, C. Dyer, A. Lavie, and N. A. Smith.
2011.
Bet-ter hypothesis testing for statistical machine transla-tion: Controlling for optimizer instability.
In Proc.
ofACL.F.
Cromie`res and S. Kurohashi.
2009.
An alignment al-gorithm using belief propagation and a structure-baseddistortion model.
In Proc.
of EACL.J.
DeNero and D. Klein.
2010.
Discriminative modelingof extraction sets for machine translation.
In Proc.
ofACL.L.
R. Dice.
1945.
Measures of the amount of eco-logic association between species.
Journal of Ecology,26:297?302.C.
Dyer and P. Resnik.
2010.
Context-free reordering,finite-state translation.
In Proc.
of NAACL.C.
Dyer, A. Lopez, J. Ganitkevitch, J. Weese, F. Ture,P.
Blunsom, H. Setiawan, V. Eidelman, and P. Resnik.2010.
cdec: A decoder, alignment, and learningframework for finite-state and context-free translationmodels.
In Proc.
of ACL (demonstration session).A.
Fraser.
2007.
Improved Word Alignments for Statis-tical Machine Translation.
Ph.D. thesis, University ofSouthern California.K.
Gimpel and N. A. Smith.
2009.
Cube summing, ap-proximate inference with non-local features, and dy-namic programming without semirings.
In Proc.
ofEACL.N.
Habash and F. Sadat.
2006.
Arabic preprocessingschemes for statistical machine translation.
In Proc.
ofNAACL, New York.A.
Haghighi, J. Blitzer, J. DeNero, and D. Klein.
2009.Better word alignments with supervised ITG models.In Proc.
of ACL-IJCNLP.P.
Koehn, F. J. Och, and D. Marcu.
2003.
Statisticalphrase-based translation.
In Proc.
of NAACL.D.
Koller and N. Friedman.
2009.
Probabilistic Graphi-cal Models: Principles and Techniques.
MIT Press.S.
Kumar, W. Macherey, C. Dyer, and F. Och.
2009.
Effi-cient minimum error rate training and minimum bayes-risk decoding for translation hypergraphs and lattices.In Proc.
of ACL-IJCNLP.J.
Lafferty, A. McCallum, and F. Pereira.
2001.
Con-ditional random fields: Probabilistic models for seg-menting and labeling sequence data.
In Proc.
of ICML.A.
Lavie and M. Denkowski.
2009.
The METEOR metricfor automatic evaluation of machine translation.
Ma-chine Translation Journal, 23(2?3):105?115.P.
Liang and D. Klein.
2009.
Online EM for unsuper-vised models.
In Proc.
of NAACL.Y.
Liu, T. Xia, X. Xiao, and Q. Liu.
2009.
Weightedalignment matrices for statistical machine translation.In Proc.
of EMNLP.Y.
Liu, Q. Liu, and S. Lin.
2010.
Discriminative wordalignment by linear modeling.
Computational Lin-guistics, 36(3):303?339.A.
Lopez.
2008.
Tera-scale translation models via pat-tern matching.
In Proc.
of COLING.R.
Mihalcea and T. Pedersen.
2003.
An evaluation exer-cise for word alignment.
In Proc.
of the Workshop onBuilding and Using Parallel Texts.R.
C. Moore.
2005.
A discriminative framework forbilingual word alignment.
In Proc.
of HLT-EMNLP.F.
Och and H. Ney.
2003.
A systematic comparison ofvarious statistical alignment models.
ComputationalLinguistics, 29(1):19?51.F.
Och.
1999.
An efficient method for determining bilin-gual word classes.
In Proc.
of EACL.K.
Papineni, S. Roukos, T. Ward, and W.-J.
Zhu.
2002.BLEU: a method for automatic evaluation of machinetranslation.
In Proc.
of ACL.418A.
Quattoni, M. Collins, and T. Darrell.
2004.
Condi-tional random fields for object recognition.
In NIPS17.H.
Setiawan, C. Dyer, and P. Resnik.
2010.
Discrimina-tive word alignment with a function word reorderingmodel.
In Proc.
of EMNLP.N.
A. Smith and J. Eisner.
2005.
Contrastive estimation:training log-linear models on unlabeled data.
In Proc.of ACL.M.
Snover, B. J. Dorr, R. Schwartz, L. Micciulla, andJ.
Makhoul.
2006.
A study of translation edit ratewith targeted human annotation.
In Proc.
of AMTA.T.
Takezawa, E. Sumita, F. Sugaya, H. Yamamoto, andS.
Yamamoto.
2002.
Toward a broad-coverage bilin-gual corpus for speech translation of travel conversa-tions in the real world.
In Proc.
of LREC.B.
Taskar, S. Lacoste-Julien, and D. Klein.
2005.
A dis-criminative matching approach to word alignment.
InProc.
of HLT-EMNLP.Y.
Tsuruoka, J. Tsujii, and S. Ananiadou.
2009.
Stochas-tic gradient descent training for l1-regularized log-linear models with cumulative penalty.
In Proc.
ofACL-IJCNLP.A.
Venugopal, A. Zollmann, N. A. Smith, and S. Vogel.2008.
Wider pipelines: n-best alignments and parsesin MT training.
In Proc.
of AMTA.S.
Vogel, H. Ney, and C. Tillmann.
1996.
HMM-basedword alignment in statistical translation.
In Proc.
ofCOLING.D.
Weiss and B. Taskar.
2010.
Structured prediction cas-cades.
In Proc.
of AISTATS.419
