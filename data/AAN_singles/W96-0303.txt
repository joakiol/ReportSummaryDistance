Controlling the application of lexical rulesTed Briscoeejb@cl, cam.
ac  .ukComputer Laboratory, University of CambridgeAnn Copestakeaac@cs l i ,  stanford ,  eduCSLI, Stanford UniversityAbstractIn this paper, we describe an item-familiarity account of the semi-productivity of morphologicaland lexical rules, and illustrate how it can be applied to practical issues which arise when buildinglarge scale lexical knowledge bases which utilize lexical rules.
Our approach assumes that attesteduses of derived words and senses are explicitly recorded, but that productive use of lexical rules isalso possible, though controlled by probabilities associated with rule application.
We discuss howthe necessary probabilities and estimates of lexical rule productivity may be acquired from corpora.1 IntroductionLexicalist linguistic theories, such as HPSG, LFG and categorial grammar, rely heavily on lexicalrules.
Recently, techniques have been described which address the efficiency issues that this raisesfor fully productive rules, such as inflectional rules and 'syntactic rules' (such as the HPSG com-plement extraction lexical rule).
For example, Bouma & van Noord (1994) and Johnson & Dorre(1995) propose techniques for delayed evaluation of lexical rules so that they apply 'on demand' atparse time.
Meurers ~ Minnen (1995) present a covariation approach, in which a finite-state ma-chine for the application of lexical rules is derived by computing possible follow relations betweenthe set of rules and then pruned FSMs are associated with classes of actual lexical entries repre-senting the restricted set of rules which can apply to those entries.
Finally, entries themselves areextended with information common to all their derived variants.
These techniques achieve most ofthe advantages of lexicon expansion in the face of recursive rules and cyclic rule interactions whichpreclude a full off-line expansion.Although these treatments allow for the efficient use of productive lexical rules, they do not ad-dress the issue of semi-productivity of derivational morphological nd sense extension rules, whichcauses considerable problems in construction of broad coverage lexical knowledge bases (LKBs)(see, for example, Climent and Mart/(1995), Pirelli et al, 1994).
The standard formalization oflexical rules entails that derived entries will exist without exception for any basic entry which iscompatible with the lexical rule input description.
Formal accounts of some classes of exceptions,such as preemption by synonomy, have been developed (e.g.
Briscoe et al 1995), but these sufferfrom the disadvantage that detailed lexical semantic information must be available to detect po-tential synonyms.
The search for a fully productive statement of verb alternations has led to anincreasingly semantic perspective on such rules.
Pinker (1989) argues that so-called broad seman-tic classes (e.g.
creation or transfer verbs) provide necessary conditions for lexical rule application,but that narrow class lexical rules should be specified, breaking down such rules into a number offully-productive subcases.
But, in the attempt o define such subcases, Pinker is forced to makesubtle and often unintuitive distinctions.
Similarly, Levin (1992) delimits classes of verbs to whichparticular sets of alternations apply, but some of her classes are very small and do not have straight-forward semantic riteria for membership.
Thus, even if the narrow class approach is correct, itsimplementation is problematic.From a computational perspective, an equally acute problem is the proliferation of senses thatresults when lexical rules are encoded as fully productive.
For instance, the result of applying thevehicle-name -> verb-of-motion lexical rule can be input to several other lexical rules.
Theforms which would arise if the alternations given by Levin (1992:267) are applied to helicopter areillustrated in (1):(1) a The pilot helicopteredb The pilot helicoptered over the forestsc Mrs Clinton was helicoptered to the based The pilot helicoptered the forestse The pilot helicoptered his passengers sickJudgements of the grammaticality of such examples differ (though (lc) is an attested example) buteven when such senses are plausible and attested, they are rare for the great majority of nounswhich could in principle undergo the conversion.Jackendoff (1975) and others have proposed that lexical rules be interpreted as redundancystatements which abbreviate the statement of the lexicon but which are not applied generatively.This conception of lexical rules has been utilized in computational lexical knowledge bases, forexample by Sanfilippo (1993).
However, this approach cannot account for the semi-productivenature of such rules, illustrated with respect o the dative alternation in (2):(2) John faxed / xeroxed / emailed his colleagues a copy of the reportAnd for practical LKB building, there is a problem acquiring the information about which lexicalentries a rule applies to.
Machine readable dictionaries (MRDs) were used for this purpose bySanfilippo, but the absence of a sense in an MRD does not mean it is unknown to the lexicographer:dictionaries have space limitations and senses may be omitted if they are rare or specialized, and alsoif they are 'obvious' - -  i.e.
the result of a highly productive process (Kilgarriff, 1992).
Furthermore,if broad-coverage is attempted, the polysemy problem is still acute.
Finally, theories of the lexiconin which the consequences of lexical rules are precomputed, cannot be correct in the limit becauseof the presence of recursive lexical rules such as re-, anti- or great- prefixation (e.g.
rereprogram,anti-anti-missile or great-great-grandfather).Thus neither the interpretation of lexical rules as fully generative or as purely abbreviatoryis adequate linguistically or as the basis for LKBs.
Although many lexical rules are subject toexceptions, gaps and variable degrees of conventionalization, most are semi-productive in the sensethat they play a role in the production and interpretation of nonce forms and errors.
In theremainder of this paper, we illustrate how the linguistically-motivated probablistic framework forlexical rule application described in Copestake and Briscoe (1995) and Briscoe and Copestake(1995) might be utilized to address these practical problems.2 Probabi l ist ic  lexical rulesCopestake and Briscoe (1995) and Briscoe and Copestake (1995) argue that lexical rules, are sensitiveto both type and token frequency effects which determine language users' assessments of the degreeof acceptability of a given derived form and also their willingness to apply a rule in producingior interpreting a novel form.
Arguments for a treatment of semi-productivity along these lineshave been advanced by Goldberg (1995) and Bauer (1983) (though not with respect to lexicalrules).
We regard our use of probabilities as being consistent with Bauer's claim that accountingfor semi-productivity is an issue of performance, not competence (Bauer 1983:71f).The frequency with which a given word form is associated with a particular lexical entry (i.e.sense or grammatical realization) is often highly skewed; Church (1988) points out that a modelof part-of-speech assignment in context will be 90% accurate (for English) if it simply choosesthe lexically most frequent part-of-speech for a given word.
Briscoe and Carroll (1995) foundin one corpus that there were about 18 times as many instances of believe in the most commonsubcategorizati0n class as in the 4 least common classes combined.
In the absence of other factors,it seems very likely that language users utilize frequency information to resolve indeterminaciesin both generation and interpretation.
Such a strategy is compatible with and may well underliethe Gricean Maxim of Manner, in that ambiguities in language will be more easily interpretable ifthere is a tacit agreement not to utilize abnormal or rare means of conveying particular messages.We can model this aspect of language use as a conditional probability that a word form will beassociated to a specific lexical entry:freq(lexical-entry with word-form)Prob(lexical-entry \[ word-form)-- freq(word-form)This proposal is not novel and is the analogue of proposals to associate probabilities with initialtrees in a Lexicalized Tree Adjoining Grammar (Resnik, 1992; Schabes, 1992).
The derivationprobability which gives the probability of a particular sentence interpretation will depend on theproduct of the lexical probabilities (rule probabilities might also play a role, but can be ignored inthe categorial framework we adopt here).Lexical probabilities are acquired for both basic and derived lexical entries independently of thelexical rules used to create derived entries, so a derived entry might be more frequent han a basicone.
Basic entries are augmented with a representation f the attested lexical rules which haveapplied to them and any such derived chains, where both the basic entry and these 'abbreviated'derived entries are associated with a probability.
One way of implementing this approach is to adoptthe covariation technique of Meurers & Minnen (1995) discussed above.
If we assume a precompiledrepresentation f this form, conditional probabilities that a word form will be associated with aparticular (basic or derived) entry can be associated with states in the FSM, as illustrated inFigure 1.
(The feature structure itself is based on the verb representation scheme developed bySanfilippo (1993), though the details are unimportant for current purposes.
)In this representation, the states of the FSM, which have been given mnemonic names corre-sponding to their types, are each associated with a probability representing the relative likelihoodthat fax will be associated with the derived entry which results from applying the rule to the sourceentry (the probabilities shown here are purely for illustrative purposes).
We call this representationthe lexeme for a given word.
Figure 2 shows part of the corresponding FSM explicitly.
Note thatthere are states with no associated probabilities, reflecting possible but unattested usages.
Thetopology of the FSM associated with a given word may be shared with other words, but the specificprobabilities associated with the states representing lexical entries will be idiosyncratic so that theeach lexeme representation must minimally encode the unique name of the relevant FSM and aprobability for each attested state / lexical entry as shown in Figure 1.
If the derived form isirregular in some way, then the exceptional information can be stipulated at the relevant state, andthe feature structure calculated by default-unifying the specified information with the productiveoutput of the lexical rule.
For example, if beggar is treated as derived by the agentive -er rule (which- trans-causative-verbORTH : faxrSYN : \] RESULTlk ACTIVE\[ fax-relSEM : < \[EVENT\[RESULT: ssig.n \ ] \ ]ACTIVE : nps lgn \ [ \ ]nps ign \ [ \ ]\] \[ p-agt-caus \] \[ p-pat-aff-obj \].
EVENT: e |EVENT : e |e ARG:  \[\] L ARG:  \[\] Jcreate/transfer-lexeme-fsm(trans(0.2), to-ditrans(0.3)..
.
)Figure h Lexeme for faxdat ive- l r  @t rans f  ?
r -  l r ~ t o .
d i t r a n  screate-dat ive- i  ~ r 0.3@ ?
?trails benef-dative0.2Figure 2: FSM for fax,@rec ip -dat iveis reasonable synchronically), then the irregular morphology can be stipulated and will override thepredicted begger.The resulting FSM is not a Markov model because probabilities on states represent outputprobabilities and not transition probabilities in the machine.
In addition, since the probabilitiesencode the relative likelihood that a given word form will associate with a particular lexical entry,the set of probabilities on states of a FSM will not be globally normalized.
One FSM will representthe application of both rules of conversion (zero affixation) and rules of derivation to a given lexemeand the latter will change the form of the word, and thus participate in a different distribution.See for example, Figure 3, which is intended to cover the noun and verb lacquer, plus the derivedform, lacquerer (with agentive and instrument readings taken as distinct).One problem with the acquisition of reliable estimates of such probabilities is that many pos-sibilities will remain unseen and will, therefore, be unattested.
There are a variety of well-knowntechniques for smoothing probability distributions which avoid assigning zero probability to unseenevents.
Church ~ Gale (1994) discuss the applicability of these to linguistic problems and emphasizethe need for differential estimation of the probability of different unseen events in typical linguisticapplications.
For instance, one standard approach to smoothing involves assigning a hypotheticalsingle observation to each unseen event in a distribution before normalizing frequencies to obtainprobabilities.
This captures the intuition that the more frequent he observation of some eventsin a distribution, the less likely it is that the unseen possibilities will occur.
Thus, a rare wordwith only a few observations may be more likely to be seen in an alternative realization than avery frequent word which has been observed many times in some subset of the possible realizationslicensed by the grammar.
However, all unseen events will be assigned the same probability withineach distinct distribution and this is at best a gross estimate of the actual distribution.For unattested erived lexical entries for a given word form, the relative productivity of thelexical rule(s) required to produce the derived entry can be used to allow differential estimation10Isubstance0.84agent ire- e _~o~use-subst-lr ~ instrument-er' 0  "0t r  ns  ins t rumentresult-lr resu l tat lve0.01Figure 3: Lexeme for lacquerthe probability of an unattested erived entry given a word form.
We can estimate the relativeproductivity of each lexical rule by calculating the ratio of possible to attested outputs for eachrule (cf Aronoff, 1976):MProd(lexical-rule) = T-(where N is the number of attested lexical entries which match the lexical rule input and M is thenumber of attested output entries).
We discuss ome more elaborate measurements for productivityin section 4.This information concerning degree of productivity of a rule can be combined with a smoothingtechnique to obtain a variant enhanced smoothing method of the type discussed by Church & Gale(1994) capable of assigning distinct probabilities to unseen events within the same distribution.This can be achieved by estimating the held back probability mass to be distributed between theunseen entries using the basic smoothing method and then distributing this mass differentially bymultiplying the total  mass for unseen entries (expressed as a ratio of the total observations for agiven word) by a different ratio for each lexical rule.
This ratio is obtained by dividing the ratiorepresenting the productivity of the lexical rule(s) by the sum of the ratios of the lexical rulesrequired to construct all the unseen entries.number-of-unattested-entries (word-form)Unseen-pr-mass(word-form) = freq(word-form) -q-number-of-unattested-entries(word-form)Est-freq(lex-entryi w th word-formj) = Unseen-pr-mass(word-formj) ?
Prod(lri)E Prod(lrl),..., Prod(Ira)(where lr l .
.
.
lrn are the n lexical rules needed to derive the n unattested entries for word-form j)This will yield revised ratios for each given word which can then be normalized to probabilities.To make this clearer, consider the use of the probabilities to drive interpretation i the case ofa nonce usage.
Consider the lexical entry for the verb fax given in Figure 1 and assume the verbis unattested in a dative construction, such as fax me the minutes of the last meeting.
But it mayundergo either the benefactive-dative or recipient-dative rules to yield a dative realization.
Theserules would produce either a deputive reading where although the speaker is a beneficiary of the11action the recipient is unspecified or a reading where the speaker is also the recipient of the transferaction.
Choosing between these rules in the absence of clear contextual information could beachieved by choosing the derivation (and thus interpretation) with highest probability.
This woulddepend solely on the relative probability of the unseen derived entries created by applying thesetwo rules to fax.
This would be (pre)computed by applying the formulae above to a representationof the lexeme for fax in which ratios represent the number of observations of an entry for a givenword form over the total number of observations of that word form, and unattested entries arenoted and assigned one observation each20 30 1 1create/transfer-lexeme-fsm (t rans(1~),  for-ditrans(1~), recip-dative (1~)  , benef-dative (1~) , .
.
.
)Now if we assume that the recipient dative rule can apply to 100 source entries and the resultingderived entries are attested in 60 cases, whilst the benefactive dative can apply to 1000 entriesand the derived entries are attested in 100 cases, we can compute the revised estimates of theprobabilities for the unseen entries for fax by instantiating the formula for estimated frequency asfollows:Est-freq(fax with recipient-dative) -- 10--O2 ?
(~(  1~0,1 10~0) ?
1~0)and similarly for the benefactive-dative case.
The resulting ratios can then be converted to prob-abilities by normalizing them along with those for the attested entries for .fax.
In this case, therecipient reading will be preferred as the recipient dative rule is more productive.This general approach andles the possibility of specialized subcases of more general rules.
Forexample, we could factor the computation of productivity between subtypes of the input type ofa rule and derive more fine-grained measures of productivity for each narrow class a rule appliesto.
In the case of specialized subcases of lexical rules which apply to a narrower ange of lexicalitems but yield a more specific interpretation (such as the rules of Meat or Fur grinding as opposedto Grinding proposed in Copestake & Briscoe, 1995), the relative productivity of each rule willbe estimated in the manner described above, but the more specialized rule is likely to be moreproductive since it will apply to fewer entries than the more general rule.
Similarly, in Figure 3, weassumed a use-substance  l xical rule, but a more accurate stimation of probabilities i  obtainedby considering specialized subclasses, as we will see in the next section.3 Acquiring probabilitiesIn order to implement the approach described, it is necessary to acquire probabilities for attestedsenses, and to derive appropriate stimates of lexical rule productivity.
Probabilities of differentword senses can be learned by a running analyzer, to the extent hat lexical ambiguities are resolvedeither during processing or by an external oracle, and for limited domains this may well be thebest approach.
We are more interested in incorporating probabilities in a large, reusable, lexicalknowledge base.
Recent developments in corpus processing techniques have made this more feasible.For instance, work on word sense disambiguation i corpora (e.g.
Resnik 1995), could lead toan estimate of frequencies for word senses in general, with rule-derived senses simply being aspecial case.
Many lexical rules involve changes in subcategorization, a d automatic techniques forextracting subcategorization from corpora (e.g.
Briscoe and Carroll, 1995; Manning, 1993) couldeventually be exploited to give frequency information.In some cases, a combination of large corpora and sense taxonomies can be used to providea rough estimate of lexical rule productivity suitable for instantiating the formulae given in the12previous section.
For example, we examined verbs derived from several classes of noun from the90 million word written portion of the British National Corpus, using the wordlists compiled byAdam Kilgarriff.
We looked at four classes of nouns: vehicles, dances, hitting weapons (e.g.
club,whip) and decOrative coatings (e.g.
lacquer, varnish).
For the sake of this experiment, we assumedthat these undergo four different lexical rules1:?
vehicle -> go using vehicle (Levin, 1992 : 51.4.1)?
dance -> perform dance ((Levin : 51.5)?
hitting weapon -> hit with weapon (subclass of Levin : 18.3)?
paint-like substance -> apply paint-like substance (Levin : 24)The first problem is isolating the nouns which can be input to the lexical rules.
For thepurposes of dei-iving a productivity measurement for the rule as a whole, it does not matter muchif the set is incomplete, as long as there are no systematic differences in productivity between theincluded and the excluded cases.
There are several potential sources for semantically coherent nounclasses.
The list of vehicle nouns was derived from a taxonomy constructed semi-automatically fromLongman Dictionary of Contemporary English (Procter, 1978), as described by Copestake (1990).This taxonomy only included land vehicles, not boats or airplanes.
The other three classes werederived manually from a combination of Roget's and WordNet, since the relevant axonomies werenot available.
For the 'hitting weapon' and 'paint-like substance' classes, this involved combiningseveral Roget categories and WordNet synsets.
We excluded entries made up of more than oneword, such as square dance and also pruned the set of nouns to exclude cases where a non-derivedverb form would confuse the results (e.g.
taxi).Initially we :used the automatically assigned part of speech tags to identify verbs, but these gavea large number of false positives, because of errors in the tagging process.
Therefore we lookedinstead for forms ending in -ed and -ing which had been tagged as verbs.
There is still the potentialfor false positives if an adjectival -ed form (e.g.
bearded) was mistakenly tagged as a verb, but thisdid not appear to cause a problem for these experiments.
Only considering inflected forms meansthat we are systematically underestimating frequencies, but since the main aim is to acquire thecorrect relative ordering of lexical rules, this is not too problematic.
Figure 4 shows some rawfrequencies of noun and verb (-ed, and -ing form) from the BNC.
We also show frequencies of the-er nominal, which we assume is derived from the verb form.
For comparison, we show whether theword is found in the Cambridge International Dictionary of English (CIDE), a modern learner'sdictionary.
A more sophisticated system for acquisition of accurate frequencies for each word wouldhave to be capable of sense-disambiguation.
For example, according to Figure 4, distemper wasfound as a noun 37 times, but many of these uses actually referred to the disease, rather than thepaint.We assumed that a unique conversion rule applied to each noun and calculated the productivitiesof the lexical rules as the ratio of the number of words for Which verbs were found over the totalnumber of words in the class which were found in the corpus.
The results are summarized inFigure 5.
The results for vehicle nouns were manually checked to ensure that the unusual verbforms were genuine.
This resulted in one putative example of the conversion rule being discarded:trailered and trailering were found in one section of the corpus, but turned out to refer to gettinglit is irrelevant here exactly how these rules axe to be formalized, though see references in Levin (1992) and alsoKiparsky (1996).
It is also not essential to our approach that these rules be treated as distinct, from the viewpoint oftheir representation astyped feature structures, ince it would be possible to attach probabilities to subrules whichonly differed in the semantic type of their input.13wordacryliccalciminedistemperdyeemulsionenamelgouachejapanlacquerpaintprimersemiglossshellactemperatintundercoatvarnishveneerwatercolourBNCnoun verb -er104 0 00 0 037 4 0291 153 19117 2 0287 25 185 0 017 1 0132 25 11783 2170 965236 0 00 0 035 1 029 0 060 46 044 0 0231 37 1156 12 0272 0 0CIDEnoun verb -er?
?+?+++++++++++++ -+ -+ ++ -+ -Figure 4: Raw frequencies for some paint nouns.class total alternating productivityvehicle 75 12 0.16dance 41 9 0.22paint 23 15 0.65club 35 18 0.51Figure 5: Productivity estimates14Ia horse into a trailer, rather than transporting by trailer.
In other words, trailer here is beingregarded as a container or location, rather than as a vehicle.
Manual checking of the rare derivedforms is not particularly time-consuming, so a semi-automatic approach, where high frequencyforms which are found in an MRD are assumed to be genuine, but where low frequency examplesare manually checked, should be adequate.As expected, some very frequent nouns such as car and vehicle had no corresponding verbs.
Ofcourse we could hypothesize that verb formation is preempted by synonymy (e.g.
by drive).
But,whatever the cause, blocking is allowed for automatically by the approach proposed here, since theprobability calculated for unseen entries of high frequency words will be very low (see section 4).Similarly, it should not be necessary to explicitly encode the fact that the conversion rule does notapply to an already derived form such as primer.Even with a 90 million word corpus, some words occurred very infrequently, and others whichwere found in Roget's and/or WordNet were absent completely.
For example calcimine is definedin WordNet as a type of water-based paint, and is also found in Roget's, but does not occur in theBNC.
Even the relative estimates for productivity of rules will be inaccurate if there is a systematicdifference between the frequency of words in one input class as compared to another, since infre-quently occurring words are less likely to have attested erived forms.
We discuss modifications tothe formulae which would allow for this in the next section.
This effect might have accounted forthe relatively 10w productivity observed for the dance rule.
However, there might also be phonolog-ical effects since many dance names are taken from languages other than English.
The results forproductivity are only strictly comparable within a particular corpus.
It should be apparent fromthe frequencies that large corpora are needed to find instances of some words.4 Utilizing probabilistic lexical rulesThe majority of implemented NLP systems have either simply listed derived forms and extendedsenses, or treated them using lexical rules as redundancy statements.
In the introductory section,we argued that this approach cannot be correct in principle, because of the problem of noncesenses.
But it is also demonstrably inadequate, at least for systems which are not limited to anarrow domain.
In an experiment with a wide-coverage parsing system (Alvey NL Tools, ANLT)Briscoe and Carroll (1993) observed that half of the parse failures were caused by inaccuratesubcategorization information in the lexicon.
The ANLT lexicon was derived semi-automaticallyfrom a machine readable dictionary (LDOCE), and although the COMLEX syntax dictionary(Grishman et al, 1994), which was derived with much greater amounts of human effort, has aslightly better performance, the difference is not great.
Automatic acquisition of information fromcorpora is a partial answer to this problem, and one which is in many respects complementary to theapproach assumed here, but successful acquisition of a broad-coverage lexicon from a really largecorpus would lead to a similar problem of massive ambiguity as we see in the case of productivelexical rules.
Control of syntactic ambiguity by the use of lexical and other probabilities has beendemonstrated by several authors (e.g.
Black et al, 1993; Schabes, 1992; Resnik, 1992), but thedifficulty of acquisition means that the validity of utilizing lexical probabilities of the type assumedhere has not yet been demonstrated on a large scale.This approach fits in most naturally with systems where probabilistic information is incorpo-rated systematically.
However it could be useful with more traditional systems.
Different appli-cations could utilize probabilistic information in different ways.
For word choice in generation, itwould be appropriate to take the highest-probability suitable entry, and, if none are attested, toconstruct a phrase, rather than apply a semi-productive lexical rule to produce a nonce form.
For15analysis, the most likely rules can be applied first, in the case of known senses, and since noncesenses are (by definition) rarer, rules will be applied productively only when this fails.
This im-proves on the control principle suggested in Copestake (1992), that lexical rules should only beapplied if no interpretation was applicable which did not involve a lexical rule, since it allows forcases such as turkey, where the derived (meat) use is more frequent han the non-derived (animal)use in the corpora which we have examined.
The two other control effects uggested in Copestake(1992) are both also superseded by the current proposal.
One of these was to allow for blocking,which is discussed below.
The other was that more specific lexical rules should be preferred over'more general ones.
We would expect that, in general, the more specialized rule will be more pro-ductive, as a natural consequence of applying to a smaller class, but the earlier proposal would havehad the undesirable consequence that this was a fixed consequence, which could not be adjustedfor cases where the generalization did not hold.
Thus the grammar writer was, in effect, requiredto consider both competence and performance when stipulating a rule.The general claim we make here is that if we assume that speakers choose well-attested high-frequency forms to realize particular senses and listeners choose well-attested high-frequency senseswhen faced with ambiguity, then much of the 'semi-productivity' of lexical rules is predicted.Blocking can be treated as a special case of this principle: if speakers use higher frequency formsto convey a given meaning, an extended meaning will not become conventionalized if a commonsynonym exists.
This means that we do not have to stipulate a separate blocking principle ininterpretation, since the blocked senses will not be attested or will have a very low frequency.
Andin generation, we assume that higher probability forms are preferred as a way of conveying a givenmeaning.
Practically, this has considerable advantages over the earlier proposal, that blockingshould be detected by looking for synonyms, since the the state of the art in acquisition andrepresentation f lexical semantic information makes it difficult to detect synonymy accurately.
Wecan assume, for example, that a verbal use of car will not be postulated by a generator, because itis unattested, and will only be possible for an analyzer when forced by context.
It is necessary toallow for the possibility of unblocking, because of examples uch as the following:(3) a There were five thousand extremely loud people on the floor eagerto tear into roast cow with both hands and wash it down withbourbon whiskey.
(Tom Wolfe, 1979.
The Right StuJ~)b In the case of at least one county primary school .
.
.
they  wereoffered (with perfect iming) saute potatoes, carrots, runner beansand roast cow.
(Guardian newspaper, May 16th 1990, in a story about mad cowdisease.
)However, this is not the complete story, since we have not accounted formally for the extra impli-catures that the use of a blocked form conveys, nor have we allowed for the generation of blockedforms (apart from in the circumstances where the generator's lexicon omits the synonym).
Boththese problems require an account of the interface with pragmatics, though the latter is perhapsnot serious for computational pplications, since we are unlikely to want to generate blocked forms.The treatment proposed here is one of many possible schemes for estimating the productivityof lexical rules and integrating these estimates with the estimation of the probabilities of unseenentries for given word forms.
Other more complex schemes could be developed, which,, for example,took account of the average probability of the output of a lexical rule.
This might be necessary,for example, to model the relative frequencies of -er vs -ee suffixation, since although the latter16Iis more productive (by Baayen and Lieber's (1991) definition), tokens of the former are morefrequent overall (Barker, 1996).
However, we will assume the simple approach ere, since acquiringthe average probability of lexical rule output raises some additional difficulties, and we currentlyhave no evidence that the more complex approach is justified, given that our main aim is to rankunseen senses by plausibility.
Another problem, mentioned above, is the need to ensure that classeshave comparable frequency distributions.
This could matter if there were competing lexical rules,defined on different but overlapping classes, and if one class has a very high percentage of lowfrequency words compared to the other, the estimate of its productivity will tend to be lower.
The ?productivity figure could be adjusted to allow for item frequency within classes, but we will notdiscuss this further here.5 Conc lus ionIn this paper, we have described a possible approach to application-independent t chnique for con-trolling lexical rule application.
We have concentrated on sense-extension, but the same machinerycould be used for derivational morphology, with the advantage that acquiring frequencies fromcorpora is easier, at least for unambiguous affixes.
Our approach requires ome lexical semanticinformation, to identify possible inputs to rules, but the need for detailed definitions of narrowclasses for which rules can be treated as fully productive is reduced (since failure to identify anarrow class will lead to less accurate prediction of probabilities, rather than over-generation) asis the requirement to identify synonyms to predict blocking.
The probabilistic approach to lexi-cal rules integrates neatly with existing proposals to control application of lexical rules efficientlywithin a constraint-based framework, such as those of Meurers & Minnen (1995).
Thus we believethat this approach could provide a linguistically motivated and practical solution to the problemof semi-productivity.
However, further work remains to be done on acquiring sense frequencies andproductivity measurements, before evaluation in a full system is feasible.ReferencesAronoff, M. (1976) Word Formation in Generative Grammar, Linguistic Inquiry Monograph 1.MIT Press, Cambridge, Mass.Baayen, H. and R. Lieber (1991) 'Productivity and English Derivation: A Corpus-Based Study',Linguistics, voi.29, 801-843.Barker, C. (1996) 'Episodic -ee in English: Thematic relations and new word formation' in M. Si-mons and T. Galloway (eds.
), Semantics and Linguistic Theory V, Cornell University, Ithaca, NY,pp.
1-18.Bauer, L. (1983) English word-formation, Cambridge University Press, Cambridge, England.Black, E., F. Jelinek, J. Lafferty, D.M.
Magerman, R. Mercer, S. Roukos (1993) 'Towards history-based grammars: using richer models for probabilistic parsers', Proceedings of the 31st AnnualMeeting of the Association of Computational Linguistics, Ohio State University, Columbus, Ohio,pp.
31-37.Bouma, G. and G. van Noord (1994) 'Constraint-based categorial grammar', Proceedings of the32nd Annual Meeting of the Association of Computational Linguistics, Las Cruces, NM.Briscoe, E.J.
and J. Carroll (1993) 'Generalized probabilistic LR parsing for unification-basedgrammars', Computational Linguistics, vol.
19.1, 25-60.17Briscoe, E.J.
and J. Carroll (1995) Towards automatic extraction of argument structure from cor-pora, ACQUILEX II Working Paper.Briscoe, E.J., A. Copestake and A. Lascarides (1995) 'Blocking' in P. St. Dizier and E.
Viegas(eds.
), Computational lexical semantics, Cambridge University Press, pp.
273-302.Briscoe, E.J.
and A. Copestake (1995) Dative constructions a lexical rules in the TDFS framework,ACQUILEX II Working Paper.Church, K. (1988) 'A stochastic parts program and noun phrase parser for unrestricted text',Proceedings of the Second Conference on Applied Natural Language Processing (ANLP-88), Austin,Texas, pp.
136-143.Church, K. and W. Gale (1994) 'Enhanced Good-Turing and Cat-Cal estimation', Speech andLanguage Processing, vol.lO,Climent, S. and M. A. Martl (1995) On using lexical rules with semantic effects, Paper presentedat ACQUILEX Workshop on lexical rules.Copestake, A.
(1990) 'An approach to building the hierarchical element of a lexical knowledgebase from a machine readable dictionary', Proceedings of the First International Workshop onInheritance in Natural Language Processing, Tilburg, The Netherlands, pp.
19-29.Copestake, A. and E.J.
Briscoe (1995) 'Semi-Productive Polysemy and Sense Extension', Journalof Semantics, vol.
12, 15-67.Goldberg, A.
(1995) Constructions, Chicago University Press.Grishman, R., Macleod, C. & Meyers, A.
(1994) 'Comlex syntax: building a computational lexicon',Proceedings of the International Conference on Computational Linguistics, COLING-94, Kyoto,Japan, pp.
268-272.Jackendoff, R. (1975) 'Morphological nd semantic regularities in the lexicon', Language, vol.51(3),639-71.Johnson, M. and J. Dorre (1995) 'Memoization of coroutined constraints', Proceedings of the 33rdAnnual Meeting of the Association of Computational Linguistics, Cambridge, MA, pp.
100-107.Kilgarriff, A.
(1992) Polysemy, D.Phil.
thesis, Cognitive Science Research Paper 261, University ofSussex, UK.Kiparsky, P. (1996) Remarks on Denominal Verbs, Stanford University.Levin, Beth (1992) Towards a lexical organization of English verbs, Chicago University Press,Chicago.Manning, C. (1993) 'Automatic acquisition of a large subcategorisation dictionary from corpora',Proceedings of the 31st Annual Meeting of the Association for Computational Linguistics (A CL-93),Columbus, Ohio.Meurers, D. and G. Minnen (1995) 'A computational treatment ofHPSG lexical rules as covariationlexical entries', Proceedings of the 5th Int.
Workshop on Natural Language Understanding and LogicProgramming, Lisbon.Pinker, S (1989) Learnability and Cognition: The Acquisition of Argument Structure, MIT Press,Cambridge, MA.Pirelli, V., N. Ruimy and S. Montemagni (1994) Lexical regularities and lexicon compilation,Acquilex-II Working Paper 36.Procter, P. (ed) (1978) Longman Dictionary of Contemporary English, Longman, London.18Resnik, P. (1992) 'Probabilistic Lexicalised Tree Adjoining Grammar', Proceedings of the Coling92,Nantes, France.Resnik, Philip (1995) 'Disambiguating oun groupings with respect to WordNet senses', Proceedingsof the 3rd workshop on very large corpora, MIT, Cambridge, Mass..Sanfilippo, A.
(1993) 'LKB encoding of lexical knowledge from machine-readable dictionaries' inE.
J. Briscoe, A. Copestake and V. de Paiva (eds.
), Inheritance, defaults and the lexicon, CambridgeUniversity Press, Cambridge, England, pp.
190-222.Schabes, Y.
(1992) 'Stochastic Lexicalized Tree Adjoining Grammar', Proceedings of the Coling92,Nantes, France,19
