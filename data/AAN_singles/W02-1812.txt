A Word Segmentation Method With Dynamic Adapting To TextUsing Inductive LearningZhongjian WangHokuto System Co.,LTDOyachi Higashi1-3-23, Atubetuku, Sapporo, 004-0041 Japanwang@hscnet.co.jpKenji ArakiGraduate School of Engineering, Hokkaido UniversityN13-W8, Kita-ku, Sapporo, 060-8628 Japanaraki@media.eng.hokudai.ac.jpKoji TochinaiGraduate School of Business Administration, Hokkai-Gakuen University,Asahimachi 4-1-40, Toyohira-ku, Sapporo, 062-8605 JapanAbstractWe have proposed a method of word segmen-tation for non-segmented language using Induc-tive Learning.
This method uses only surfaceinformation of a text, so that it has an ad-vantage that is entirely not dependent on anyspecific language.
In this method, we considerthat a character string of appearing frequentlyin a text has a high possibility as a word.
Themethod predicts unknown words by recursivelyextracting common character strings.
With theproposed method, the segmentation results canadapt to different users and fields.
To evaluateeffectivety for Chinese word segmentation andadaptability for different fields, we have donethe evaluation experiment with Chinese text ofthe two fields.1 InstructionIn NLP applications, word segmentation of non-segmented language is a very necessary initialstage(Sun et al, 1998).
In the other hands,with the development of the Internet and popu-larization of computers, a large amount of textinformation in different languages on the In-ternet are increasing explosively, so it is nec-essary to develop a common method to dealwith multi-language(Yamasita and Matsumoto,2000).
Furthermore, the standard of word seg-mentation is dependent on a user and destina-tion of use(Sproat et al, 1996), so that it is nec-essary that word segmentation can adapt users,can deal with multi languages.In our method, we extract recursively acommon character string that occur frequentlyin text and call it a common part.
Whensome common parts contain still same charac-ter strings, furthermore we extract the samecharacter string as high dimensional commonparts and the remain parts is called differentparts.
The high dimensional common partsmaybe have higher possibility as words becauseit is extracted by multi steps.
Those extractedcommon parts and different parts are calledWS(Word Segment), and classified into someranks according to extracting condition.
Theproposed method segments a non-segmentedsentence into words using the ranks of WS inorder of the higher value of the certainty de-grees as words.
When there are multiple seg-mentation candidates, the system gets a list ofsegmentable candidates, and picks a correct seg-mentation candidate from the list by using avalue of LEF (Likelihood Evaluation Function,Section 2.1) and so on.
In addition, it is notnecessary to prepare a dictionary and any wordsegmentation rules beforehand.
A dictionary ofadapting to the user or the field is generatedwith increasing of processed text.
Because onlysurface information of a text is used, it is possi-ble the method is used to deal with general non-segmented language.
Here Inductive Learningis the procedure to extract recursively WS bymulti steps(Araki et al, 1995).2 AlgrithmFig.
1 shows the outline of the proposedmethod.
(1) Input sentences are segmented by wordcandidates that were acquired in the dictionaryso far.Segment by a update dictionaryPrediction overNoYesInput sentencesPredict unknown words usingInductive LearningSegment by known wordsOutput segmentation resultsProofreadingFeedback processingSegmentationresultsCorrected resultsRecursivelypredictionDictionaryFigure 1: Outline(2) For the remaining part of the charac-ter strings that are unsegmented by the knownwords, the system predicts unknown words byextracting WS using Inductive Learning.The system extracts WS as word candidates.This process is based on the supposition that acommon character string of appearing repeat-edly in text has high probability as a word.
(3) The user judges whether the results of theword segmentation is correct or not.
If there areerrors in the result, the user will correct errors.
(4) The system compares the proofread re-sults with the segmentation result to updatethe information in the dictionary.
Through thisprocedure, the certainty of WS as a word is con-firmed and increased.Here, the WS those are used in correct seg-mentation are called CW (Correct Word).2.1 Segmentation by Known WordsInput sentence and then the system segmentsit into words by registered CW and WS thatthe system has got by using Inductive Learninguntil that time.
(1) In the first step, the system compares theregistered CW or WS in the dictionary withthe character string in the input sentence fromthe beginning of the sentence, and finds outthe same character strings with the registeredwords.
The system repeats this comparison pro-cess until the end of the sentence is reached.A list of segmentation candidate is established.Then the system segments the sentence intowords.
(2) In the second step, however, for the char-acter strings of multiple segmentations, we usethe registered candidates in order of their ranksin the dictionary(Section 2.3).
When there aremore than one word candidate with the samerank, we decide the correct segmentation fromthe list of segmentation candidates by the valueof LEF.
We define LEF as follows:LEF = FR + ?CS ?
?ES + ?LEFR +CS ?
ES + LE(1)Where: FR, CS, ES and LE are the fre-quency of CW or WS appearing in the text, thefrequency of the correct segmentation, the fre-quency of the erroneous segmentation and thelength of CW orWS respectively.
?, ?
and ?
arecoefficients.
The optimum coefficients of LEFare decided by the preliminary experiments us-ing Greedy method, ?=10, ?=1 and ?=5.The word that has the maximum value ofLEF is decided as the correct segmentation can-didate.
(3) When LEF value of the set of possiblesegmentations is equal to each other, the cor-rect segmentation candidate is decided by theword candidate that the value of ES is mini-mum, the value of CS is maximum, the value ofFR is maximum, the value of LE is the longestor the location of segmentation is the leftmostin a sentence in turn.2.2 Prediction for Unknown WordsFig.
2 shows an example of a non-segmentedsentence.
In this example, every character rep-resents a Chinese character, so we use this ex-ample to express a general sentence of non-segmented language to present the proposedmethod.
Those words that are not registered inthe dictionary are predicted by using InductiveLearning.
After the sentences were segmentedby known words, which have been registered inthe dictionary, the unsegmented part of char-acter string will be used to extract WS.
Theprediction method is to find the common char-acter string in text.
The extraction procedureis carried out as Fig.
3 shows: the extractionof common parts, sift out the common part ofthe most possibility as a word, the re-extractionof common parts and the extraction of differentparts.??????????????????????????????????
?.Figure 2: An example of non-segmented sen-tence.2.2.1 Extraction of a Common PartA common part in non-segmented text is ex-tracted by two steps:(1) When a character string appears in textfrequently, we call it a common character string.If the common character string consists of morethan two characters, we extract it as a word can-didate and call it common part and represent itby S1(Segment one).
Here, we use length, fre-quency and location of S1 in the sentence to siftout it, to get the S1 of the most possible as aword.
At this step, we acquired S1 from thesentence that is shown in Fig.
2: ??????,???
?and ????????.
(2) When the character string appears in thesentence only one times but meanwhile it isincluded in other extracted common part andmade up by more than two characters, we alsoextract it as a word candidate.
For example inFig.
2: ?????
is included in ????????.
There-fore ?????
is extracted and belongs to S1.2.2.2 Extraction of a High DimensionalCommon Part and a DifferentPartThe extracted S1 at 2.2.1 may still include acommon character string.
At this situation, thecommon character string can be re-extractedmoreover from the extracted S1.
We considerit has a higher probability as a word that re-extracted common parts at this procedure.
Theconditions of re-extraction are presented as fol-lows:(1) The common part can be re-extractedfrom the extracted S1 when it includes a com-mon character string that is more than twocharacters.
For example, ????????
contains?????
which can be extracted from ???????
?,so ???????(S1)?
is equal to ????(S2)?
+????
(S3)?.The part of re-extraction is called high di-mensional common part and represented by S2(Segment two).
The part of remain is calleddifferent part and represented by S3 (Segmentthree).
The S1 is deleted from the dictionaryExtraction of high dimensioncommon parts and different partsSift out common parts of themost possibility as wordsExtraction of common partsGet S1Get S2 and S3Unsegmented Character StringFigure 3: WS extraction procedurewhen it is divided into S2 and S3.
(2) Furthermore one character can also be ex-tracted as a word candidate when both sidesof it are extracted as a word candidate orboth sides were segmented by known words.Like ???
in ?????????????
is surrounded by????????
and ?????
?, and ???
is extractedas a word candidate belonging to S2.The extraction procedure is carried out re-peatedly until the new WS can not be extractedand the input can not be segmented.2.3 Segmentation by a UpdateDictionaryThe extracted WS are classified to ?S1?, ?S2?,and ?S3?.
Those WS that are confirmedas a word by proofreading process are called?CW?
(Correct Word).
Furthermore, theFR(appearing FRequency), CS(Correct Seg-mentation frequency), ES(Erroneous Segmen-tation frequency), LE(LEength) and rank of aword candidate are rigestered simultaneously.Word Segmentation is carried out by the up-date dictionary as 2.1.2.4 Feedback ProcessAfter the system segments the sentence intowords, the results are judged whether they arecorrect or not by the user.
Then the user cor-rects the errors if there are errors in the results.The system updates the rank of the registeredCW andWS in the dictionary by comparing thecorrected results with the segmentation results.And the system increases the priority degree ofthe words that were used in correct segmenta-tion and decreases the priority degree of wordsthat were used in erroneous segmentations.
TheTable 1: Experimental resultsFields Economics Engineering Averagewords 92,085 70,017 162,102CSR[%] 87.50 90.80 89.44ESR[%] 5.40 5.60 5.45USR[%] 7.10 3.60 5.11feedback process is described in detail as follows:(1) For the Correct Segmentation Results: When the result of segmentation is correct,the value of FR and CS of a word that isused to segment are added one. If the rank of the words does not belong toCW, it is changed to CW.
(2) For the Erroneous Segmentation Results: If the dictionary does not has the correctwords, the system registers the words in thedictionary.
In this case, their FRs are 1,their ranks are CW. If the dictionary has the correct words, thesystem adds one to the value of FR for aword and changes the value of CL to CWif it does not belong to CW. If the reason of erroneous segmentation isthat the erroneous word was used, then theES of erroneous word is added one.
(3) For the Unsegmented Parts: The system registers the words in the dic-tionary, as FR of the words equal 1 andrank equal CW.3 Evaluation Experiments3.1 Experimental Data And ProcedureTo evaluate the adaptability of the proposedmethod for different fields and the effectivity forChinese word segmentation.
We use the Chi-nese text of two specialized fields from SinicaCorpus1: the economics contains 92,085 wordsand the engineering contains 70,017 words.
To-tal words is 162,102.
The economics consists ofthe text of economic system, economic policyand economic theory.
The engineering consistsof the text of electronics, communication engi-neering, machine engineering and nuclear indus-try.1http : //www.sinica.edu.tw/ftms ?
bin/kiwi.sh01020304050607080901000 2 4 6 8 10 12 14 16SegmentationRate(%)Number of WordsEconomics EngineeringCorrect Segmentation RateUnsegmentation RateErroneous Segmentation Rate(x10,000)Figure 4: The changes of segmentation ratesIn order to confirm the adaptability of pro-posed method to user, we let the initial dic-tionary empty.
We input a paragraph abouthundred words one times and two fields text inturns.3.2 Experimental ResultsThe results of experiment are shown in Table1.
Fig.
4 shows the change of CSR, ESR andUSR.
In our method, the correct segmentationnumber is the number of correct segmentationthat is judged by a user.
The unsegmentationnumber is the number when all unsegmentedstrings are segmented correctly.
The erroneoussegmentation number is the number that sub-tracts the number of correct segmentation andunsegmentation from the number of all wordsin the input text.
To evaluate the experimentresult, we use these formulas of CSR (CorrectSegmentation Rate), ESR (Erroneous Segmen-tation Rate) and USR (Unsegmented Rate) asfollows:CSR[%] = Correct segmentationnumberTotal number of words?
100 (2)ESR[%] = Erroneous segmentation numberTotal number of words ?
100 (3)USR[%] = UnsegmentationnumberTotal number of words ?
100 (4)4 Discussion4.1 Adaptability To Different FieldsFig.
4 shows the experimental results of twofields.
When the text is changed to differentdomain, because appearance of some new wordsof different fields, the correct segmentation rate01020304050607080900 0.5 1 1.5 2 2.5 3 3.5 4 4.5 5Precision&RecallNumber of words (x10,000)PrecosionRecallFigure 5: The ability to predict unknown wordsis fall down temporary.
However with increasingof processed sentence, the correct segmentationrate goes on increasing quickly.We may consider that the proposed methodhas adaptability for different fields.
Sometimesthe correct segmentation rate is a little lowerbecause the domain of text is a little difference,for example: the economics consists of the textof economic system, economic policy and eco-nomic theory and so on.4.2 Evaluation of Ability for PredictingUnknown WordsWe use 50,000 words to discuss the predictingability of proposed method for unknown words.Precision[%] = CWNTWN?
100 (5)Recall[%] = CWNTUN?
100 (6)Where, CWN is the number of words that arepredicted correctly.
TWN is the total numberof words that are predicted.
TUN is the totalnumber of unknown words.The precision and recall are shown in Fig.
5.The average precision is 26.0%.
The averagerecall is 31.0%.
With increasing of registeredwords in the dictionary, prediction effect forunknown words is becoming well, after 40,000words are processed the precision and the recallare 85.0%, 40.0% respectively.4.3 Analysis of ErroneousSegmentationWe select 1,000 words from the beginning of theexperimental date and the end of the experi-mental date respectively, to analysis the reasonof an erroneous segmentation.
At the begin-ning, ESR that is because of unregistered wordsis 18.0%, but after 16,000 words are processed,ESR that is because of unregistered words is0.9%.
However ESR that is caused by ambigu-ity goes on increasing from 1.6% to 7.0%.
ESRcaused by ambiguity is increasing with increas-ing of registered word in the dictionary.
Am-biguous segmentation is still a difficult problem,so that it is necessary to improve the ability todeal with ambiguity.5 ConclusionThe experiment results show the predictionability for unknown words by using InductiveLearning.
The experiment results of two fieldsshown the proposed method can adapt to dif-ferent fields text.
In this paper, the emphasis isto evaluate the adaptivity of the method to dif-ferent user and fields.
About comparison withother existed methods will be done in the future.The proposed method may be used tocomputer-aided acquisition of language re-source.
The experimental results show ourproposed method has ability of learning, pre-dictability for unknown words and effectivityfor Chinese word segmentation.
For the futureworks, we plan to improve the ability of dealingwith segmentation ambiguity, and use this pro-posed method for Chinese morphological anal-ysis.ReferencesKenji Araki, Yoshio Momouchi, and Koji Tochi-nai.
1995.
Evaluation for adaptability ofkana-kanji translation of non-segmentationjapanese kana sentences using inductivelearning.
PACLING-II, pages 1?7.Richard Sproat, Chilin Shih, William Gale,and Nancy Chang.
1996.
A stochastic finite-state word-segmentation algorithm for Chi-nese.
Association for Computational Linguis-tics, 22(3):377?404.Maosong Sun, Dayang Shen, and Benjamin KTsou.
1998.
Chinese word segmentationwithout using lexicon and hand-crafted train-ing data.
17th International Conference onComputational Linguistics, pages 1265?1271.T.
Yamasita and Y. Matsumoto.
2000.
Journalof natural language processing(in japanese).Framework for Language Independent Mor-phological Analysis, 7(3):39?56.
