Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 181?186,Baltimore, Maryland, USA, June 23-25 2014.c?2014 Association for Computational LinguisticsI?m a Belieber:Social Roles via Self-identification and Conceptual AttributesCharley Beller, Rebecca Knowles, Craig HarmanShane Bergsma?, Margaret Mitchell?, Benjamin Van DurmeHuman Language Technology Center of ExcellenceJohns Hopkins University, Baltimore, MD USA?University of Saskatchewan, Saskatoon, Saskatchewan Canada?Microsoft Research, Redmond, Washington USAcharleybeller@jhu.edu, rknowles@jhu.edu, craig@craigharman.net,shane.a.bergsma@gmail.com, memitc@microsoft.com, vandurme@cs.jhu.eduAbstractMotivated by work predicting coarse-grained author categories in social me-dia, such as gender or political preference,we explore whether Twitter contains infor-mation to support the prediction of fine-grained categories, or social roles.
Wefind that the simple self-identification pat-tern ?I am a ?
supports significantlyricher classification than previously ex-plored, successfully retrieving a variety offine-grained roles.
For a given role (e.g.,writer), we can further identify character-istic attributes using a simple possessiveconstruction (e.g., writer?s ).
Tweetsthat incorporate the attribute terms in firstperson possessives (my ) are confirmedto be an indicator that the author holds theassociated social role.1 IntroductionWith the rise of social media, researchers havesought to induce models for predicting latent au-thor attributes such as gender, age, and politi-cal preferences (Garera and Yarowsky, 2009; Raoet al, 2010; Burger et al, 2011; Van Durme,2012b; Zamal et al, 2012).
Such models areclearly in line with the goals of both computa-tional advertising (Wortman, 2008) and the grow-ing area of computational social science (Conoveret al, 2011; Nguyen et al, 2011; Paul and Dredze,2011; Pennacchiotti and Popescu, 2011; Moham-mad et al, 2013) where big data and computa-tion supplement methods based on, e.g., direct hu-man surveys.
For example, Eisenstein et al (2010)demonstrated a model that predicted where an au-thor was located in order to analyze regional dis-tinctions in communication.
While some users ex-plicitly share their GPS coordinates through theirTwitter clients, having a larger collection of au-tomatically identified users within a region waspreferable even though the predictions for anygiven user were uncertain.We show that media such as Twitter can sup-port classification that is more fine-grained thangender or general location.
Predicting social rolessuch as doctor, teacher, vegetarian, christian,may open the door to large-scale passive surveysof public discourse that dwarf what has been pre-viously available to social scientists.
For exam-ple, work on tracking the spread of flu infectionsacross Twitter (Lamb et al, 2013) might be en-hanced with a factor based on aggregate predic-tions of author occupation.We present two studies showing that first-person social content (tweets) contains intuitivesignals for such fine-grained roles.
We argue thatnon-trivial classifiers may be constructed basedpurely on leveraging simple linguistic patterns.These baselines suggest a wide range of authorcategories to be explored further in future work.Study 1 In the first study, we seek to determinewhether such a signal exists in self-identification:we rely on variants of a single pattern, ?I am a ?,to bootstrap data for training balanced-class binaryclassifiers using unigrams observed in tweet con-tent.
As compared to prior research that requiredactively polling users for ground truth in order toconstruct predictive models for demographic in-formation (Kosinski et al, 2013), we demonstratethat some users specify such properties publiclythrough direct natural language.Many of the resultant models show intuitivestrongly-weighted features, such as a writer be-ing likely to tweet about a story, or an ath-lete discussing a game.
This demonstrates self-identification as a viable signal in building predic-tive models of social roles.181Role Tweetartist I?m an Artist..... the last of a dying breedbelieber @justinbieber I will support you in ev-erything you do because I am a belieberplease follow me I love you 30vegetarian So glad I?m a vegetarian.Table 1: Examples of self-identifying tweets.# Role # Role # Role29,924 little 5,694 man 564 champion21,822 big ... ... 559 teacher18,957 good 4,007 belieber 556 writer13,069 huge 3,997 celebrity 556 awful13,020 bit 3,737 virgin ... ...12,816 fan 3,682 pretty 100 cashier10,832 bad ... ... 100 bro10,604 girl 2,915 woman ... ...9,981 very 2,851 beast 10 linguist... ... ... ... ... ...Table 2: Number of self-identifying users per ?role?.
Whilerich in interesting labels, cases such as very highlight the pur-poseful simplicity of the current approach.Study 2 In the second study we exploit a com-plementary signal based on characteristic con-ceptual attributes of a social role, or conceptclass (Schubert, 2002; Almuhareb and Poesio,2004; Pas?ca and Van Durme, 2008).
We identifytypical attributes of a given social role by collect-ing terms in the Google n-gram corpus that occurfrequently in a possessive construction with thatrole.
For example, with the role doctor we extractterms matching the simple pattern ?doctor?s ?.2 Self-identificationAll role-representative users were drawn fromthe free public 1% sample of the Twitter Fire-hose, over the period 2011-2013, from the sub-set that selected English as their native language(85,387,204 unique users).
To identify users ofa particular role, we performed a case-agnosticsearch of variants of a single pattern: I am a(n), and I?m a(n) , where all single tokens fillingthe slot were taken as evidence of the author self-reporting for the given ?role?.
Example tweets canbe seen in Table 1, examples of frequency per rolein Table 2.
This resulted in 63,858 unique rolesidentified, of which 44,260 appeared only once.1We manually selected a set of roles for fur-ther exploration, aiming for a diverse sampleacross: occupation (e.g., doctor, teacher), family(mother), disposition (pessimist), religion (chris-1Future work should consider identifying multi-word rolelabels (e.g., Doctor Who fan, or dog walker).0.600.650.700.750.80llllll lllllllllllllllllllllll llllllldirectionerbelieberoptimistsoldiersophomorepessimistrandom.0dancerhipsterrandom.2singerfreshmanmotherrandom.1cheerleaderrapperchristianartistsmoker actorvegetarianwomanathletegeek engineerwaitressnursemanstudent doctor poet writeratheistgrandmalawyerteacherRoleChanceof SuccessFigure 1: Success rate for querying a user.
Random.0,1,2are background draws from the population, with the mean ofthose three samples drawn horizontally.
Tails capture 95%confidence intervals.tian), and ?followers?
(belieber, directioner).2We filtered users via language ID (Bergsma et al,2012) to better ensure English content.3For each selected role, we randomly sampled upto 500 unique self-reporting users and then queriedTwitter for up to 200 of their recent publiclyposted tweets.4These tweets served as represen-tative content for that role, with any tweet match-ing the self-reporting patterns filtered.
Three setsof background populations were extracted basedon randomly sampling users that self-reported En-glish (post-filtered via LID).Twitter users are empowered to at any timedelete, rename or make private their accounts.Any given user taken to be representative based ona previously posted tweet may no longer be avail-able to query on.
As a hint of the sort of user stud-ies one might explore given access to social roleprediction, we see in Figure 1 a correlation be-tween self-reported role and the chance of an ac-count still being publicly visible, with roles suchas belieber and directioner on the one hand, anddoctor and teacher on the other.The authors examined the self-identifying tweetof 20 random users per role.
The accuracy of theself-identification pattern varied across roles andis attributable to various factors including quotes,e.g.
@StarTrek Jim, I?m a DOCTOR not a down-load!.
While these samples are small (and thusestimates of quality come with wide variance), it2Those that follow the music/life of the singer JustinBieber and the band One Direction, respectively.3This removes users that selected English as their primarylanguage, used a self-identification phrase, e.g.
I am a be-lieber, but otherwise tended to communicate in non-English.4Roughly half of the classes had less than 500 self-reporting users in total, in those cases we used all matches.182actorartistatheistathletebeliebercheerleaderchristiandancerdirectionerdoctorengineerfreshmangeekgrandmahipsterlawyermanmothernurseoptimistpessimistpoetrappersingersmokersoldiersophomorestudentteachervegetarianwaitresswomanwriter0 5 10 15Figure 2: Valid self-identifying tweets from sample of 20.is noteworthy that a non-trivial number for eachwere judged as actually self-identifying.Indicative Language Most work in user clas-sification relies on featurizing language use,most simply through binary indicators recordingwhether a user did or did not use a particular wordin a history of n tweets.
To explore whether lan-guage provides signal for future work in fine-grainsocial role prediction, we constructed a set of ex-periments, one per role, where training and testsets were balanced between users from a randombackground sample and self-reported users.
Base-line accuracy in these experiments was thus 50%.Each training set had a target of 600 users (300background, 300 self-identified); for those roleswith less than 300 users self-identifying, all userswere used, with an equal number background.
Weused the Jerboa (Van Durme, 2012a) platformto convert data to binary feature vectors over a un-igram vocabulary filtered such that the minimumfrequency was 5 (across unique users).
Trainingand testing was done with a log-linear model viaLibLinear (Fan et al, 2008).
We used the pos-itively annotated data to form test sets, balancedwith data from the background set.
Each test sethad a theoretical maximum size of 40, but for sev-eral classes it was in the single digits (see Fig-ure 2).
Despite the varied noisiness of our simplepattern-bootstrapped training data, and the smallsize of our annotated test set, we see in Figure 3that we are able to successfully achieve statisti-cally significant predictions of social role for themajority of our selected examples.Table 3 highlights examples of language indica-tive of role, as determined by the most positivelyweighted unigrams in the classification experi-0.20.40.60.81.0ll l lllll llll lllll llllll lllll l llllsoldierwomanpessimistchristiangrandmanurserapperman poetcheerleaderstudentengineer actorteachervegetarianmother singerlawyeroptimistwaitresssmoker hipster doctordancerartistfreshmandirectionergeeksophomoreatheistathletewriterbelieberRoleAccuracyFigure 3: Accuracy in classifying social roles.Role :: Feature ( Rank)artist morning, summer, life, most, amp, studioatheist fuck, fucking, shit, makes, dead, ..., religion19athlete lol, game, probably, life, into, ..., team9belieber justin, justinbeiber, believe, beliebers, biebercheerleader cheer, best, excited, hate, mom, ..., prom16christian lol, ..., god12, pray13, ..., bless17, ..., jesus20dancer dance, since, hey, never, beendirectioner harry, d, follow, direction, never, liam, nialldoctor sweet, oh, or, life, nothingengineer (, then, since, may, ), test9, -17, =18freshman summer, homework, na, ..., party19, school20geek trying, oh, different, dead, beengrandma morning, baby, around, night, excitedhipster fucking, actually, thing, fuck, songlawyer did, never, his, may, pretty, law, even, officeman man, away, ai, young, sincemother morning, take, fuck, fucking, tryingnurse lol, been, morning, ..., night10, nursing11, shift13optimist morning, enough, those, everything, neverpoet feel, song, even, say, yorapper fuck, morning, lol, ..., mixtape8, songs15singer sing, song, music, lol, neversmoker fuck, shit, fucking, since, ass, smoke, weed20solider ai, beautiful, lol, wan, tryingsophmore summer, >, ..., school11, homework12student anything, summer, morning, since, actuallyteacher teacher, morning, teach, ..., students7, ..., school20vegetarian actually, dead, summer, oh, morningwaitress man, try, goes, hate, fatwoman lol, into, woman, morning, neverwriter write, story, sweet, very, workingTable 3: Most-positively weighted features per role, alongwith select features within the top 20.
Surprising motherfeatures come from ambigious self-identification, as seen intweets such as: I?m a mother f!cking starrrrr.ment.
These results qualitatively suggest manyroles under consideration may be teased out from abackground population by focussing on languagethat follows expected use patterns.
For examplethe use of the term game by athletes, studio byartists, mixtape by rappers, or jesus by Christians.3 Characteristic AttributesBergsma and Van Durme (2013) showed that the183task of mining attributes for conceptual classes canrelate straightforwardly to author attribute predic-tion.
If one views a role, in their case gender, astwo conceptual classes, male and female, then ex-isting attribute extraction methods for third-personcontent (e.g., news articles) can be cheaply used tocreate a set of bootstrapping features for buildingclassifiers over first-person content (e.g., tweets).For example, if we learn from news corpora that:a man may have a wife, then a tweet saying: ...mywife... can be taken as potential evidence of mem-bership in the male conceptual class.In our second study, we test whether this ideaextends to our wider set of fine-grained roles.
Forexample, we aimed to discover that a doctor mayhave a patient, while a hairdresser may have asalon; these properties can be expressed in first-person content as possessives like my patient or mysalon.
We approached this task by selecting targetroles from the first experiment and ranking charac-teristic attributes for each using pointwise mutualinformation (PMI) (Church and Hanks, 1990).First, we counted all terms matching a targetsocial role?s possessive pattern (e.g., doctor?s )in the web-scale n-gram corpus Google V2 (Linet al, 2010)5.
We ranked the collected termsby computing PMI between classes and attributeterms.
Probabilities were estimated from counts ofthe class-attribute pairs along with counts match-ing the generic possessive patterns his andher which serve as general background cate-gories.
Following suggestions by Bergsma andVan Durme, we manually filtered the ranked list.6We removed attributes that were either (a) notnominal, or (b) not indicative of the social role.This left fewer than 30 attribute terms per role,with many roles having fewer than 10.We next performed a precision test to identifypotentially useful attributes in these lists.
We ex-amined tweets with a first person possessive pat-tern for each attribute term from a small corpusof tweets collected over a single month in 2013,discarding those attribute terms with no positivematches.
This precision test is useful regardlessof how attribute lists are generated.
The attribute5In this corpus, follower-type roles like belieber and di-rectioner are not at all prevalent.
We therefore focused onoccupational and habitual roles (e.g., doctor, smoker).6Evidence from cognitive work on memory-dependenttasks suggests that such relevance based filtering (recogni-tion) involves less cognitive effort than generating relevantattributes (recall) see (Jacoby et al, 1979).
Indeed, this filter-ing step generally took less than a minute per class.term chart, for example, had high PMI with doc-tor; but a precision test on the phrase my chartyielded a single tweet which referred not to a med-ical chart but to a top ten list (prompting removalof this attribute).
Using this smaller high-precisionset of attribute terms, we collected tweets from theTwitter Firehose over the period 2011-2013.4 Attribute-based ClassificationAttribute terms are less indicative overall thanself-ID, e.g., the phrase I?m a barber is a clearersignal than my scissors.
We therefore include arole verification step in curating a collection ofpositively identified users.
We use the crowd-sourcing platform Mechanical Turk7to judgewhether the person tweeting held a given roleTweets were judged 5-way redundantly.
Me-chanical Turk judges (?Turkers?)
were presentedwith a tweet and the prompt: Based on thistweet, would you think this person is a BAR-BER/HAIRDRESSER?
along with four responseoptions: Yes, Maybe, Hard to tell, and No.We piloted this labeling task on 10 tweets perattribute term over a variety of classes.
Each an-swer was associated with a score (Yes = 1, Maybe= .5, Hard to tell = No = 0) and aggregated acrossthe five judges.
We found in development that anaggregate score of 4.0 (out of 5.0) led to an ac-ceptable agreement rate between the Turkers andthe experimenters, when the tweets were randomlysampled and judged internally.
We found thatmaking conceptual class assignments based on asingle tweet was often a subtle task.
The results ofthis labeling study are shown in Figure 4, whichgives the percent of tweets per attribute that were4.0 or above.
Attribute terms shown in red weremanually discarded as being inaccurate (low onthe y-axis) or non-prevalent (small shape).From the remaining attribute terms, we identi-fied users with tweets scoring 4.0 or better as posi-tive examples of the associated roles.
Tweets fromthose users were scraped via the Twitter API toconstruct corpora for each role.
These were splitintro train and test, balanced with data from thesame background set used in the self-ID study.Test sets were usually of size 40 (20 positive, 20background), with a few classes being sparse (thesmallest had only 16 instances).
Results are shownin Figure 5.
Several classes in this balanced setupcan be predicted with accuracies in the 70-90%7https://www.mturk.com/mturk/184l l l l ll lll l l ll l l l l lll l llll l l l lll l lActor/Actress Athlete Barber/Hairdresser Bartender Blogger CheerleaderChristian College Student Dancer Doctor/Nurse Drummer HunterJew Mom Musician Photographer Professor Rapper/SongwriterReporter Sailor Skier Smoker Soldier StudentSwimmer Tattoo Artist Waiter/Waitress Writer0.00.20.40.60.80.00.20.40.60.80.00.20.40.60.80.00.20.40.60.80.00.20.40.60.8rehearsal theater director linesconcussionplayingprotein sport squadconditioningjerseypositioncoach calves clientscissorsshears salon bar blog blogging pomhope testimonychurch bible scholarshipsyllabusadvisortuition campusuniversitycollege tutuscrub patient stethoscope drum standshul angel delivery kid parenting set alum guitar piano shoot shutter lecture faculty student lyricscoverage editor article shipgoggles pipe smokingtobaccosmoke cigarettebillet combat duffel orders bunk deploymentbarracks stats cap lab philosophypool ink station tip apron script memoir poemKeywordAboveThresholdlog10(Count)l l l l1 2 3 4Keepl FALSE TRUEFigure 4: Turker judged quality of attributes selected ascandidate features for bootstrapping positive instances of thegiven social role.0.50.60.70.8actorathletebarberbloggercheerleaderchristian doctordrummermommusicianphotographerprofessorreportersmokersoldierstudentwaiterwriterAccuracyFigure 5: Classifier accuracy on balanced set contrastingagreed upon Twitter users of a given role against users pulledat random from the 1% stream.range, supporting our claim that there is discrimi-nating content for a variety of these social roles.Conditional Classification How accurately wecan predict membership in a given class when aTwitter user sends a tweet matching one of the tar-geted attributes?
For example, if one sends a tweetsaying my coach, then how likely is it that authorFigure 6: Results of positive vs negative by attribute term.Given that a user tweets .
.
.
my lines .
.
.
we are nearly 80%accurate in identifying whether or not the user is an actor.is an athlete?Using the same collection as the previous ex-periment, we trained classifiers conditioned on agiven attribute term.
Positive instances were takento be those with a score of 4.0 or higher, with neg-ative instances taken to be those with scores of 1.0or lower (strong agreement by judges that the orig-inal tweet did not provide evidence of the givenrole).
Classification results are shown in Figure 6.5 ConclusionWe have shown that Twitter contains sufficientlyrobust signal to support more fine-grained au-thor attribute prediction tasks than have previouslybeen attempted.
Our results are based on simple,intuitive search patterns with minimal additionalfiltering: this establishes the feasibility of the task,but leaves wide room for future work, both in thesophistication in methodology as well as the diver-sity of roles to be targeted.
We exploited two com-plementary types of indicators: self-identificationand self-possession of conceptual class (role) at-tributes.
Those interested in identifying latent de-mographics can extend and improve these indica-tors in developing ways to identify groups of inter-est within the general population of Twitter users.Acknowledgements This material is partiallybased on research sponsored by the NSF un-der grants DGE-123285 and IIS-1249516 and byDARPA under agreement number FA8750-13-2-0017 (the DEFT program).185ReferencesAbdulrahman Almuhareb and Massimo Poesio.
2004.Attribute-based and value-based clustering: an eval-uation.
In Proceedings of EMNLP.Shane Bergsma and Benjamin Van Durme.
2013.
Us-ing Conceptual Class Attributes to Characterize So-cial Media Users.
In Proceedings of ACL.Shane Bergsma, Paul McNamee, Mossaab Bagdouri,Clay Fink, and Theresa Wilson.
2012.
Languageidentification for creating language-specific twittercollections.
In Proceedings of the NAACL Workshopon Language and Social Media.John D. Burger, John Henderson, George Kim, andGuido Zarrella.
2011.
Discriminating gender ontwitter.
In Proceedings of EMNLP.Kenneth Ward Church and Patrick Hanks.
1990.
Wordassociation norms, mutual information, and lexicog-raphy.
Computational linguistics, 16(1):22?29.Michael Conover, Jacob Ratkiewicz, Matthew Fran-cisco, Bruno Gonc?alves, Filippo Menczer, andAlessandro Flammini.
2011.
Political polarizationon twitter.
In ICWSM.Jacob Eisenstein, Brendan O?Connor, Noah Smith, andEric P. Xing.
2010.
A latent variable model ofgeographical lexical variation.
In Proceedings ofEMNLP.Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsief, Xiang-Rui Wang, and Chih-Jen Lin.
2008.
Liblinear: Alibrary for large linear classification.
Journal of Ma-chine Learning Research, (9).Nikesh Garera and David Yarowsky.
2009.
Modelinglatent biographic attributes in conversational genres.In Proceedings of ACL.Larry L Jacoby, Fergus IM Craik, and Ian Begg.
1979.Effects of decision difficulty on recognition and re-call.
Journal of Verbal Learning and Verbal Behav-ior, 18(5):585?600.Michal Kosinski, David Stillwell, and Thore Graepel.2013.
Private traits and attributes are predictablefrom digital records of human behavior.
Proceed-ings of the National Academy of Sciences.Alex Lamb, Michael J. Paul, and Mark Dredze.
2013.Separating fact from fear: Tracking flu infections ontwitter.
In Proceedings of NAACL.Dekang Lin, Kenneth Church, Heng Ji, Satoshi Sekine,David Yarowsky, Shane Bergsma, Kailash Patil,Emily Pitler, Rachel Lathbury, Vikram Rao, KapilDalwani, and Sushant Narsale.
2010.
New tools forweb-scale n-grams.
In Proc.
LREC, pages 2221?2227.Saif M. Mohammad, Svetlana Kiritchenko, and JoelMartin.
2013.
Identifying purpose behind elec-toral tweets.
In Proceedings of the Second Interna-tional Workshop on Issues of Sentiment Discoveryand Opinion Mining, WISDOM ?13, pages 1?9.Dong Nguyen, Noah A Smith, and Carolyn P Ros?e.2011.
Author age prediction from text using lin-ear regression.
In Proceedings of the 5th ACL-HLT Workshop on Language Technology for Cul-tural Heritage, Social Sciences, and Humanities,pages 115?123.
Association for Computational Lin-guistics.Marius Pas?ca and Benjamin Van Durme.
2008.Weakly-Supervised Acquisition of Open-DomainClasses and Class Attributes from Web Documentsand Query Logs.
In Proceedings of ACL.Michael J Paul and Mark Dredze.
2011.
You are whatyou tweet: Analyzing twitter for public health.
InICWSM.Marco Pennacchiotti and Ana-Maria Popescu.
2011.Democrats, Republicans and Starbucks afficionados:User classification in Twitter.
In Proceedings ofthe 17th ACM SIGKDD International Conference onKnowledge Discovery and Data mining, pages 430?438.
ACM.Delip Rao, David Yarowsky, Abhishek Shreevats, andManaswi Gupta.
2010.
Classifying latent user at-tributes in twitter.
In Proceedings of the Work-shop on Search and Mining User-generated Con-tents (SMUC).Lenhart K. Schubert.
2002.
Can we derive generalworld knowledge from texts?
In Proceedings ofHLT.Benjamin Van Durme.
2012a.
Jerboa: A toolkit forrandomized and streaming algorithms.
TechnicalReport 7, Human Language Technology Center ofExcellence, Johns Hopkins University.Benjamin Van Durme.
2012b.
Streaming analysis ofdiscourse participants.
In Proceedings of EMNLP.Jennifer Wortman.
2008.
Viral marketing and thediffusion of trends on social networks.
TechnicalReport MS-CIS-08-19, University of Pennsylvania,May.Faiyaz Al Zamal, Wendy Liu, and Derek Ruths.
2012.Homophily and latent attribute inference: Inferringlatent attributes of Twitter users from neighbors.
InProceedings of ICWSM.186
