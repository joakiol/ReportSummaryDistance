Proceedings of Ninth Meeting of the ACL Special Interest Group in Computational Morphology and Phonology, pages 48?56,Prague, June 2007. c?2007 Association for Computational LinguisticsInducing Sound Segment Differences using Pair Hidden Markov ModelsMartijn WielingAlfa-InformaticaUniversity of Groningenwieling@gmail.comTherese LeinonenAlfa-InformaticaUniversity of Groningent.leinonen@rug.nlJohn NerbonneAlfa-InformaticaUniversity of Groningenj.nerbonne@rug.nlAbstractPair Hidden Markov Models (PairHMMs)are trained to align the pronunciation tran-scriptions of a large contemporary collec-tion of Dutch dialect material, the Goeman-Taeldeman-Van Reenen-Project (GTRP, col-lected 1980?1995).
We focus on the ques-tion of how to incorporate information aboutsound segment distances to improve se-quence distance measures for use in di-alect comparison.
PairHMMs induce seg-ment distances via expectation maximisa-tion (EM).
Our analysis uses a phonologi-cally comparable subset of 562 items for all424 localities in the Netherlands.
We evalu-ate the work first via comparison to analysesobtained using the Levenshtein distance onthe same dataset and second, by comparingthe quality of the induced vowel distances toacoustic differences.1 IntroductionDialectology catalogues the geographic distributionof the linguistic variation that is a necessary condi-tion for language change (Wolfram and Schilling-Estes, 2003), and is sometimes successful in iden-tifying geographic correlates of historical develop-ments (Labov, 2001).
Computational methods forstudying dialect pronunciation variation have beensuccessful using various edit distance and relatedstring distance measures, but unsuccessful in us-ing segment differences to improve these (Heeringa,2004).
The most successful techniques distinguishconsonants and vowels, but treat e.g.
all the voweldifferences as the same.
Ignoring the special treat-ment of vowels vs. consonants, the techniques re-gard segments in a binary fashion?as alike ordifferent?in spite of the overwhelming consensusthat some sounds are much more alike than others.There have been many attempts to incorporate moresensitive segment differences, which do not neces-sarily perform worse in validation, but they fail toshow significant improvement (Heeringa, 2004).Instead of using segment distances as these are(incompletely) suggested by phonetic or phonolog-ical theory, we can also attempt to acquire theseautomatically.
Mackay and Kondrak (2005) in-troduce Pair Hidden Markov Models (PairHMMs)to language studies, applying them to the problemof recognising ?cognates?
in the sense of machinetranslation, i.e.
pairs of words in different languagesthat are similar enough in sound and meaning toserve as translation equivalents.
Such words maybe cognate in the sense of historical linguistics, butthey may also be borrowings from a third language.We apply PairHMMs to dialect data for the first timein this paper.
Like Mackay and Kondrak (2005) weevaluate the results both on a specific task, in ourcase, dialect classification, and also via examinationof the segment substitution probabilities induced bythe PairHMM training procedures.
We suggest us-ing the acoustic distances between vowels as a probeto explore the segment substitution probabilities in-duced by the PairHMMs.Naturally, this validation procedure only makessense if dialects are using acoustically more similarsounds in their variation, rather than, for example,48randomly varied sounds.
But why should linguisticand geographic proximity be mirrored by frequencyof correspondence?
Historical linguistics suggeststhat sound changes propagate geographically, whichmeans that nearby localities should on average sharethe most changes.
In addition some changes are con-vergent to local varieties, increasing the tendencytoward local similarity.
The overall effect in bothcases strengthens the similarity of nearby varieties.Correspondences among more distant varieties aremore easily disturbed by intervening changes anddecreasing strength of propagation.2 MaterialIn this study the most recent Dutch dialect datasource is used: data from the Goeman-Taeldeman-Van Reenen-project (GTRP; Goeman and Taelde-man, 1996).
The GTRP consists of digital tran-scriptions for 613 dialect varieties in the Netherlands(424 varieties) and Belgium (189 varieties), gath-ered during the period 1980?1995.
For every vari-ety, a maximum of 1876 items was narrowly tran-scribed according to the International Phonetic Al-phabet.
The items consisted of separate words andword groups, including pronominals, adjectives andnouns.
A more detailed overview of the data collec-tion is given in Taeldeman and Verleyen (1999).Since the GTRP was compiled with a view todocumenting both phonological and morphologicalvariation (De Schutter et al, 2005) and our pur-pose here is the analysis of variation in pronunci-ation, many items of the GTRP are ignored.
Weuse the same 562 item subset as introduced and dis-cussed in depth by Wieling et al (2007).
In short,the 1876 item word list was filtered by selectingonly single word items, plural nouns (the singularform was preceded by an article and therefore not in-cluded), base forms of adjectives instead of compar-ative forms and the first-person plural verb insteadof other forms.
We omit words whose variation isprimarily morphological as we wish to focus on pro-nunciation.Because the GTRP transcriptions of Belgian vari-eties are fundamentally different from transcriptionsof Netherlandic varieties (Wieling et al, 2007), wewill focus our analysis on the 424 varieties in theNetherlands.
The geographic distribution of theseLeeuwardenVeendamAalsmeerUtrechtDelftUrkPuttenCoevordenOldenzaalMiddelburg GoirleVenloFigure 1.
Distribution of GTRP localities.varieties is shown in Figure 1.
Furthermore, notethat we will not look at diacritics, but only at thephonetic symbols (82 in total).3 The Pair Hidden Markov ModelIn this study we will use a Pair Hidden MarkovModel (PairHMM), which is essentially a HiddenMarkov Model (HMM) adapted to assign similar-ity scores to word pairs and to use these similarityscores to compute string distances.
In general anHMM generates an observation sequence (output)by starting in one of the available states based on theinitial probabilities, going from state to state basedon the transition probabilities while emitting an out-put symbol in each state based on the emission prob-ability of that output symbol in that state.
The prob-ability of an observation sequence given the HMMcan be calculated by using well known HMM algo-rithms such as the Forward algorithm and the Viterbialgorithms (e.g., see Rabiner, 1989).The only difference between the PairHMM andthe HMM is that it outputs a pair of symbols in-stead of only one symbol.
Hence it generates two(aligned) observation streams instead of one.
ThePairHMM was originally proposed by Durbin et al(1998) and has successfully been used for aligning49Figure 2.
Pair Hidden Markov Model.
Image cour-tesy of Mackay and Kondrak (2005).biological sequences.
Mackay and Kondrak (2005)adapted the algorithm to calculate similarity scoresfor word pairs in orthographic form, focusing onidentifying translation equivalents in bilingual cor-pora.Their modified PairHMM has three states repre-senting the basic edit operations: a substitution state(M), a deletion state (X) and an insertion state (Y).
Inthe substitution state two symbols are emitted, whilein the other two states a gap and a symbol are emit-ted, corresponding with a deletion and an insertion,respectively.
The model is shown in Figure 2.
Thefour transition parameters are specified by ?, ?, ?and ?
.
There is no explicit start state; the proba-bility of starting in one of the three states is equal tothe probability of going from the substitution state tothat state.
In our case we use the PairHMM to alignphonetically transcribed words.
A possible align-ment (including the state sequence) for the two ob-servation streams [mO@lk@] and [mEl@k] (Dutch di-alectal variants of the word ?milk?)
is given by:m O @ l k @m E l @ kM M X M Y M XWe have several ways to calculate the similarityscore for a given word pair when the transition andemission probabilities are known.
First, we can usethe Viterbi algorithm to calculate the probability ofthe best alignment and use this probability as a sim-ilarity score (after correcting for length; see Mackayand Kondrak, 2005).
Second, we can use the For-ward algorithm, which takes all possible alignmentsinto account, to calculate the probability of the ob-servation sequence given the PairHMM and use thisprobability as a similarity score (again corrected forlength; see Mackay, 2004 for the adapted PairHMMViterbi and Forward algorithms).A third method to calculate the similarity score isusing the log-odds algorithm (Durbin et al, 1998).The log-odds algorithm uses a random model to rep-resent how likely it is that a pair of words occur to-gether while they have no underlying relationship.Because we are looking at word alignments, thismeans an alignment consisting of no substitutionsbut only insertions and deletions.
Mackay and Kon-drak (2005) propose a randommodel which has onlyinsertion and deletion states and generates one wordcompletely before the other, e.g.m O @ l k @m E l @ kX X X X X X Y Y Y Y YThe model is described by the transition proba-bility ?
and is displayed in Figure 3.
The emis-sion probabilities can be either set equal to the inser-tion and deletion probabilities of the word similaritymodel (Durbin et al, 1998) or can be specified sepa-rately based on the token frequencies in the data set(Mackay and Kondrak, 2005).The final log-odds similarity score of a word pairis calculated by dividing the Viterbi or Forwardprobability by the probability generated by the ran-dom model, and subsequently taking the logarithmof this value.
When using the Viterbi algorithmthe regular log-odds score is obtained, while usingthe Forward algorithm yields the Forward log-oddsscore (Mackay, 2004).
Note that there is no needfor additional normalisation; by dividing two mod-els we are already implicitly normalising.Before we are able to use the algorithms de-scribed above, we have to estimate the emissionprobabilities (i.e.
insertion, substitution and dele-tion probabilities) and transition probabilities of themodel.
These probabilities can be estimated by us-ing the Baum-Welch expectation maximisation al-gorithm (Baum et al, 1970).
The Baum-Welch algo-50Figure 3.
Random Pair Hidden Markov Model.
Im-age courtesy of Mackay and Kondrak (2005).rithm iteratively reestimates the transition and emis-sion probabilities until a local optimum is found andhas time complexity O(TN2), where N is the num-ber of states and T is the length of the observa-tion sequence.
The Baum-Welch algorithm for thePairHMM is described in detail in Mackay (2004).3.1 Calculating dialect distancesWhen the parameters of the complete model havebeen determined, the model can be used to calculatethe alignment probability for every word pair.
As inMackay and Kondrak (2005) and described above,we use the Forward and Viterbi algorithms in boththeir regular (normalised for length) and log-oddsform to calculate similarity scores for every wordpair.
Subsequently, the distance between two dialec-tal varieties can be obtained by calculating all wordpair scores and averaging them.4 The Levenshtein distanceThe Levenshtein distance was introduced by Kessler(1995) as a tool for measuring linguistic distancesbetween language varieties and has been success-fully applied in dialect comparison (Nerbonne et al,1996; Heeringa, 2004).
For this comparison we usea slightly modified version of the Levenshtein dis-tance algorithm, which enforces a linguistic syllab-icity constraint: only vowels may match with vow-els, and consonants with consonants.
The specificdetails of this modification are described in more de-tail in Wieling et al (2007).We do not normalise the Levenshtein distancemeasurement for length, because Heeringa et al(2006) showed that results based on raw Levenshteindistances are a better approximation of dialect dif-ferences as perceived by the dialect speakers thanresults based on the normalised Levenshtein dis-tances.
Finally, all substitutions, insertions and dele-tions have the same weight.5 ResultsTo obtain the best model probabilities, we trainedthe PairHMM with all data available from the 424Netherlandic localities.
For every locality there wereon average 540 words with an average length of 5tokens.
To prevent order effects in training, everyword pair was considered twice (e.g., wa ?
wb andwb?wa).
Therefore, in one training iteration almost100 million word pairs had to be considered.
To beable to train with these large amounts of data, a par-allel implementation of the PairHMM software wasimplemented.
After starting with more than 6700uniform initial substitution probabilities, 82 inser-tion and deletion probabilities and 5 transition prob-abilities, convergence was reached after nearly 1500iterations, taking 10 parallel processors each morethan 10 hours of computation time.In the following paragraphs we will discuss thequality of the trained substitution probabilities aswell as comment on the dialectological results ob-tained with the trained model.5.1 Trained substitution probabilitiesWe are interested both in how well the overall se-quence distances assigned by the trained PairHMMsreveal the dialectological landscape of the Nether-lands, and also in how well segment distances in-duced by the Baum-Welch training (i.e.
based onthe substitution probabilities) reflect linguistic real-ity.
A first inspection of the latter is a simple checkon howwell standard classifications are respected bythe segment distances induced.Intuitively, the probabilities of substituting avowel with a vowel or a consonant with a conso-nant (i.e.
same-type substitution) should be higherthan the probabilities of substituting a vowel with aconsonant or vice versa (i.e.
different-type substitu-tion).
Also the probability of substituting a phonetic51symbol with itself (i.e.
identity substitution) shouldbe higher than the probability of a substitution withany other phonetic symbol.
To test this assumption,we compared the means of the above three substi-tution groups for vowels, consonants and both typestogether.In line with our intuition, we found a higher prob-ability for an identity substitution as opposed tosame-type and different-type non-identity substitu-tions, as well as a higher probability for a same-typesubstitution as compared to a different-type substitu-tion.
This result was highly significant in all cases:vowels (all p?s ?
0.020), consonants (all p?s <0.001) and both types together (all p?s < 0.001).5.2 Vowel substitution scores compared toacoustic distancesPairHMMs assign high probabilities (and scores)to the emission of segment pairs that are morelikely to be found in training data.
Thus we expectfrequent dialect correspondences to acquire highscores.
Since phonetic similarity effects alignmentand segment correspondences, we hypothesise thatphonetically similar segment correspondences willbe more usual than phonetically remote ones, morespecifically that there should be a negative correla-tion between PairHMM-induced segment substitu-tion probabilities presented above and phonetic dis-tances.We focus on segment distances among vowels,because it is straightforward to suggest a measureof distance for these (but not for consonants).
Pho-neticians and dialectologists use the two first for-mants (the resonant frequencies created by differentforms of the vocal cavity during pronunciation) asthe defining physical characteristics of vowel qual-ity.
The first two formants correspond to the ar-ticulatory vowel features height and advancement.We follow variationist practice in ignoring third andhigher formants.
Using formant frequencies we cancalculate the acoustic distances between vowels.Because the occurrence frequency of the pho-netic symbols influences substitution probability, wedo not compare substitution probabilities directly toacoustic distances.
To obtain comparable scores, thesubstitution probabilities are divided by the productof the relative frequencies of the two phonetic sym-bols used in the substitution.
Since substitutions in-volving similar infrequent segments now get a muchhigher score than substitutions involving similar, butfrequent segments, the logarithm of the score is usedto bring the respective scores into a comparablescale.In the program PRAAT we find Hertz valuesof the first three formants for Dutch vowels pro-nounced by 50 male (Pols et al, 1973) and 25 fe-male (Van Nierop et al, 1973) speakers of stan-dard Dutch.
The vowels were pronounced in a /hVt/context, and the quality of the phonemes for whichwe have formant information should be close to thevowel quality used in the GTRP transcriptions.
Byaveraging over 75 speakers we reduce the effect ofpersonal variation.
For comparison we chose onlyvowels that are pronounced as monophthongs instandard Dutch, in order to exclude interference ofchanging diphthong vowel quality with the results.Nine vowels were used: /i, I, y, Y, E, a, A, O, u/.We calculated the acoustic distances between allvowel pairs as a Euclidean distance of the formantvalues.
Since our perception of frequency is non-linear, using Hertz values of the formants when cal-culating the Euclidean distances would not weighF1 heavily enough.
We therefore transform frequen-cies to Bark scale, in better keeping with human per-ception.
The correlation between the acoustic voweldistances based on two formants in Bark and the log-arithmical and frequency corrected PairHMM sub-stitution scores is r = ?0.65 (p < 0.01).
ButLobanov (1971) and Adank (2003) suggested usingstandardised z-scores, where the normalisation isapplied over the entire vowel set produced by a givenspeaker (one normalisation per speaker).
This helpsin smoothing the voice differences between men andwomen.
Normalising frequencies in this way re-sulted in a correlation of r = ?0.72 (p < 0.001)with the PairHMM substitution scores.
Figure 4 vi-sualises this result.
Both Bark scale and z-valuesgave somewhat lower correlations when the thirdformant was included in the measures.The strong correlation demonstrates that thePairHMM scores reflect phonetic (dis)similarity.The higher the probability that vowels are alignedin PairHMM training, the smaller the acoustic dis-tance between two segments.
We conclude thereforethat the PairHMM indeed aligns linguistically corre-sponding segments in accord with phonetic similar-52Figure 4.
Predicting acoustic distances based onPairHMM scores.
Acoustic vowel distances are cal-culated via Euclidean distance based on the first twoformants measured in Hertz, normalised for speaker.r = ?0.72ity.
This likewise confirms that dialect differencestend to be acoustically slight rather than large, andsuggests that PairHMMs are attuned to the slightdifferences which accumulate locally during lan-guage change.
Also we can be more optimisticabout combining segment distances and sequencedistance techniques, in spite of Heeringa (2004,Ch.
4) who combined formant track segment dis-tances with Levenshtein distances without obtainingimproved results.5.3 Dialectological resultsTo see how well the PairHMM results reveal the di-alectological landscape of the Netherlands, we cal-culated the dialect distances with the Viterbi andForward algorithms (in both their normalised andlog-odds version) using the trained model parame-ters.To assess the quality of the PairHMM results,we used the LOCAL INCOHERENCE measurementwhich measures the degree to which geographicallyclose varieties also represent linguistically similarvarieties (Nerbonne and Kleiweg, 2005).
Just asMackay and Kondrak (2005), we found the over-all best performance was obtained using the log-odds version of Viterbi algorithm (with insertion anddeletion probabilities based on the token frequen-cies).Following Mackay and Kondrak (2005), we alsoexperimented with a modified PairHMM obtainedby setting non-substitution parameters constant.Rather than using the transition, insertion and dele-tion parameters (see Figure 2) of the trained model,we set these to a constant value as we are mostinterested in the effects of the substitution param-eters.
We indeed found slightly increased perfor-mance (in terms of LOCAL INCOHERENCE) for thesimplified model with constant transition parame-ters.
However, since there was a very high corre-lation (r = 0.98) between the full and the simplifiedmodel and the resulting clustering was also highlysimilar, we will use the Viterbi log-odds algorithmusing all trained parameters to represent the resultsobtained with the PairHMM method.5.4 PairHMM vs. Levenshtein resultsThe PairHMM yielded dialectological results quitesimilar to those of Levenshtein distance.
The LOCALINCOHERENCE of the two methods was similar, andthe dialect distance matrices obtained from the twotechniques correlated highly (r = 0.89).
Given thatthe Levenshtein distance has been shown to yield re-sults that are consistent (Cronbach?s ?
= 0.99) andvalid when compared to dialect speakers judgementsof similarity (r ?
0.7), this means in particular thatthe PairHMMs are detecting dialectal variation quitewell.Figure 5 shows the dialectal maps for the resultsobtained using the Levenshtein algorithm (top) andthe PairHMM algorithm (bottom).
The maps on theleft show a clustering in ten groups based on UP-GMA (Unweighted Pair Group Method with Arith-metic mean; see Heeringa, 2004 for a detailed expla-nation).
In these maps phonetically close dialectalvarieties are marked with the same symbol.
How-ever note that the symbols can only be comparedwithin a map, not between the two maps (e.g., a di-alectal variety indicated by a square in the top mapdoes not need to have a relationship with a dialec-tal variety indicated by a square in the bottom map).Because clustering is unstable, in that small differ-ences in input data can lead to large differences inthe classifications derived, we repeatedly added ran-dom small amounts of noise to the data and iter-atively generated the cluster borders based on the53Figure 5.
Dialect distances for Levenshtein method (top) and PairHMM method (bottom).
The maps onthe left show the ten main clusters for both methods, indicated by distinct symbols.
Note that the shape ofthese symbols can only be compared within a map, not between the top and bottom maps.
The maps in themiddle show robust cluster borders (darker lines indicate more robust cluster borders) obtained by repeatedclustering using random small amounts of noise.
The maps on the right show for each locality a vectortowards the region which is phonetically most similar.
See section 5.4 for further explanation.54noisy input data.
Only borders which showed upduring most of the 100 iterations are shown in themap.
The maps in the middle show the most ro-bust cluster borders; darker lines indicate more ro-bust borders.
The maps on the right show a vector ateach locality pointing in the direction of the regionit is phonetically most similar to.A number of observations can be made on thebasis of these maps.
The most important observa-tion is that the maps show very similar results.
Forinstance, in both methods a clear distinction canbe seen between the Frisian varieties (north) andtheir surroundings as well as the Limburg varieties(south) and their surroundings.
Some differencescan also be observed.
For instance, at first glancethe Frisian cities among the Frisian varieties are sep-arate clusters in the PairHMM method, while thisis not the case for the Levenshtein method.
Sincethe Frisian cities differ from their surroundings agreat deal, this point favours the PairHMM.
How-ever, when looking at the deviating vectors for theFrisian cities in the two vector maps, it is clear thatthe techniques again yield similar results.
Note thata more detailed description of the results using theLevenshtein distance on the GTRP data can be foundin Wieling et al (2007).Although the PairHMM method is much more so-phisticated than the Levenshtein method, it yieldsvery similar results.
This may be due to the factthat the data sets are large enough to compensate forthe lack of sensitivity in the Levenshtein technique,and the fact that we are evaluating the techniques ata high level of aggregation (average differences in540-word samples).6 DiscussionThe present study confirms Mackay and Kondrak?s(2004) work showing that PairHMMs align linguis-tic material well and that they induce reasonable seg-ment distances at the same time.
We have extendedthat work by applying PairHMMs to dialectal data,and by evaluating the induced segment distances viatheir correlation with acoustic differences.
We notedabove that it is not clear whether the dialectologicalresults improve on the simple Levenshtein measures,and that this may be due to the level of aggregationand the large sample sizes.
But we would also liketo test PairHMMs on a data set for which more sen-sitive validation is possible, e.g.
the Norwegian setfor which dialect speakers judgements of proximityis available (Heeringa et al, 2006); this is clearly apoint at which further work would be rewarding.At a more abstract level, we emphasise that thecorrelation between acoustic distances on the onehand and the segment distances induced by thePairHMMs on the other confirm both that align-ments created by the PairHMMs are linguisticallyresponsible, and also that this linguistic structure in-fluences the range of variation.
The segment dis-tances induced by the PairHMMs reflect the fre-quency with which such segments need to be alignedin Baum-Welch training.
It would be conceivablethat dialect speakers used all sorts of correspon-dences to signal their linguistic provenance, but theydo not.
Instead, they tend to use variants which arelinguistically close at the segment level.Finally, we note that the view of diachronicchange as on the one hand the accumulation ofchanges propagating geographically, and on theother hand as the result of a tendency toward localconvergence suggests that we should find linguis-tically similar varieties nearby rather than furtheraway.
The segment correspondences PairHMMs in-duce correspond to those found closer geographi-cally.We have assumed a dialectological perspectivehere, focusing on local variation (Dutch), and usingsimilarity of pronunciation as the organising varia-tionist principle.
For the analysis of relations amonglanguages that are further away from each other?temporally and spatially?there is substantial con-sensus that one needs to go beyond similarity as abasis for postulating grouping.
Thus phylogenetictechniques often use a model of relatedness aimednot at similarity-based grouping, but rather at creat-ing a minimal genealogical tree.
Nonetheless sim-ilarity is a satisfying basis of comparison at morelocal levels.AcknowledgementsWe are thankful to Greg Kondrak for providing thesource code of the PairHMM training and testing al-gorithms.
We thank the Meertens Instituut for mak-ing the GTRP data available for research and espe-55cially Boudewijn van den Berg for answering ourquestions regarding this data.
We would also like tothank Vincent van Heuven for phonetic advice andPeter Kleiweg for providing support and the soft-ware we used to create the maps.ReferencesPatti Adank.
2003.
Vowel normalization - a perceptual-acoustic study of Dutch vowels.
Wageningen: Ponsen& Looijen.Leonard E. Baum, Ted Petrie, George Soules, and Nor-man Weiss.
1970.
A maximization technique oc-curring in the statistical analysis of probabilistic func-tions of Markov Chains.
The Annals of MathematicalStatistics, 41(1):164?171.Georges De Schutter, Boudewijn van den Berg, Ton Goe-man, and Thera de Jong.
2005.
Morfologische at-las van de Nederlandse dialecten - deel 1.
Amster-dam University Press, Meertens Instituut - KNAW,Koninklijke Academie voor Nederlandse Taal- en Let-terkunde.Richard Durbin, Sean R. Eddy, Anders Krogh, andGraeme Mitchison.
1998.
Biological Sequence Anal-ysis : Probabilistic Models of Proteins and NucleicAcids.
Cambridge University Press, July.Ton Goeman and Johan Taeldeman.
1996.
Fonologie enmorfologie van de Nederlandse dialecten.
een nieuwemateriaalverzameling en twee nieuwe atlasprojecten.Taal en Tongval, 48:38?59.Wilbert Heeringa, Peter Kleiweg, Charlotte Gooskens,and John Nerbonne.
2006.
Evaluation of string dis-tance algorithms for dialectology.
In John Nerbonneand Erhard Hinrichs, editors, Linguistic Distances,pages 51?62, Shroudsburg, PA. ACL.Wilbert Heeringa.
2004.
Measuring Dialect Pronunci-ation Differences using Levenshtein Distance.
Ph.D.thesis, Rijksuniversiteit Groningen.Brett Kessler.
1995.
Computational dialectology in IrishGaelic.
In Proceedings of the seventh conference onEuropean chapter of the Association for Computa-tional Linguistics, pages 60?66, San Francisco, CA,USA.
Morgan Kaufmann Publishers Inc.William Labov.
2001.
Principles of Linguistic Change.Vol.2: Social Factors.
Blackwell, Malden, Mass.Boris M. Lobanov.
1971.
Classification of Russian vow-els spoken by different speakers.
Journal of the Acous-tical Society of America, 49:606?608.Wesley Mackay and Grzegorz Kondrak.
2005.
Com-puting word similarity and identifying cognates withPair Hidden Markov Models.
In Proceedings of the9th Conference on Computational Natural LanguageLearning (CoNLL), pages 40?47, Morristown, NJ,USA.
Association for Computational Linguistics.Wesley Mackay.
2004.
Word similarity using Pair Hid-den Markov Models.
Master?s thesis, University ofAlberta.John Nerbonne and Peter Kleiweg.
2005.
Toward a di-alectological yardstick.
Accepted for publication inJournal of Quantitative Linguistics.John Nerbonne, Wilbert Heeringa, Erik van den Hout, Pe-ter van der Kooi, Simone Otten, and Willem van deVis.
1996.
Phonetic distance between Dutch dialects.In Gert Durieux, Walter Daelemans, and Steven Gillis,editors, CLIN VI: Proc.
from the Sixth CLIN Meet-ing, pages 185?202.
Center for Dutch Language andSpeech, University of Antwerpen (UIA), Antwerpen.Louis C. W. Pols, H. R. C. Tromp, and R. Plomp.
1973.Frequency analysis of Dutch vowels from 50 malespeakers.
The Journal of the Acoustical Society ofAmerica, 43:1093?1101.Lawrence R. Rabiner.
1989.
A tutorial on HiddenMarkov Models and selected applications in speechrecognition.
Proceedings of the IEEE, 77(2):257?286.Johan Taeldeman and Geert Verleyen.
1999.
De FAND:een kind van zijn tijd.
Taal en Tongval, 51:217?240.D.
J. P. J.
Van Nierop, Louis C. W. Pols, and R. Plomp.1973.
Frequency analysis of Dutch vowels from 25female speakers.
Acoustica, 29:110?118.Martijn Wieling, Wilbert Heeringa, and John Nerbonne.2007.
An aggregate analysis of pronunciation in theGoeman-Taeldeman-Van Reenen-Project data.
Taalen Tongval.
submitted, 12/2006.Walt Wolfram and Natalie Schilling-Estes.
2003.
Dialec-tology and linguistic diffusion.
In Brian D. Joseph andRichard D. Janda, editors, The Handbook of HistoricalLinguistics, pages 713?735.
Blackwell, Malden, Mas-sachusetts.56
