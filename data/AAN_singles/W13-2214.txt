Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 128?133,Sofia, Bulgaria, August 8-9, 2013 c?2013 Association for Computational LinguisticsTowards Efficient Large-Scale Feature-Rich Statistical MachineTranslationVladimir Eidelman1, Ke Wu1, Ferhan Ture1, Philip Resnik2, Jimmy Lin31 Dept.
of Computer Science 2 Dept.
of Linguistics 3 The iSchoolInstitute for Advanced Computer StudiesUniversity of Maryland{vlad,wuke,fture,resnik,jimmylin}@umiacs.umd.eduAbstractWe present the system we developed toprovide efficient large-scale feature-richdiscriminative training for machine trans-lation.
We describe how we integrate withMapReduce using Hadoop streaming toallow arbitrarily scaling the tuning set andutilizing a sparse feature set.
We report ourfindings on German-English and Russian-English translation, and discuss benefits,as well as obstacles, to tuning on largerdevelopment sets drawn from the paralleltraining data.1 IntroductionThe adoption of discriminative learning methodsfor SMT that scale easily to handle sparse and lex-icalized features has been increasing in the lastseveral years (Chiang, 2012; Hopkins and May,2011).
However, relatively few systems take fulladvantage of the opportunity.
With some excep-tions (Simianer et al 2012), most still rely ontuning a handful of common dense features, alongwith at most a few thousand others, on a relativelysmall development set (Cherry and Foster, 2012;Chiang et al 2009).
While more features tunedon more data usually results in better performancefor other NLP tasks, this has not necessarily beenthe case for SMT.Thus, our main focus in this paper is to improveunderstanding into the effective use of sparse fea-tures, and understand the benefits and shortcom-ings of large-scale discriminative training.
Tothis end, we conducted experiments for the sharedtranslation task of the 2013 Workshop on Statis-tical Machine Translation for the German-Englishand Russian-English language pairs.2 Baseline systemWe use a hierarchical phrase-based decoder im-plemented in the open source translation systemcdec1 (Dyer et al 2010).
For tuning, we useMr.
MIRA2 (Eidelman et al 2013), an opensource decoder agnostic implementation of onlinelarge-margin learning in Hadoop MapReduce.
Mr.MIRA separates learning from the decoder, allow-ing the flexibility to specify the desired inferenceprocedure through a simple text communicationprotocol.
The decoder receives input sentencesand weight updates from the learner, while thelearner receives k-best output with feature vectorsfrom the decoder.Hadoop MapReduce (Dean and Ghemawat,2004) is a popular distributed processing frame-work that has gained widespread adoption, withthe advantage of providing scalable parallelizationin a manageable framework, taking care of datadistribution, synchronization, fault tolerance, aswell as other features.
Thus, while we could oth-erwise achieve the same level of parallelization, itwould be in a more ad-hoc manner.The advantage of online methods lies in theirability to deal with large training sets and high-dimensional input representations while remain-ing simple and offering fast convergence.
WithHadoop streaming, our system can take advantageof commodity clusters to handle parallel large-scale training while also being capable of runningon a single machine or PBS-managed batch clus-ter.System design To efficiently encode the infor-mation that the learner and decoder require (sourcesentence, reference translation, grammar rules) ina manner amenable to MapReduce, i.e.
avoidingdependencies on ?side data?
and large transfersacross the network, we append the reference and1http://cdec-decoder.org2https://github.com/kho/mr-mira128per-sentence grammar to each input source sen-tence.
Although this file?s size is substantial, it isnot a problem since after the initial transfer, it re-sides on Hadoop distributed file system, and Map-Reduce optimizes for data locality when schedul-ing mappers.A single iteration of training is performed asa Hadoop streaming job.
Each begins with amap phase, with every parallel mapper loading thesame initial weights and decoding and updatingparameters on a shard of the data.
This is followedby a reduce phase, with a single reducer collect-ing final weights from all mappers and computinga weighted average to distribute as initial weightsfor the next iteration.Parameter Settings We tune our system towardapproximate sentence-level BLEU (Papineni et al2002),3 and the decoder is configured to use cubepruning (Huang and Chiang, 2007) with a limitof 200 candidates at each node.
For optimiza-tion, we use a learning rate of ?=1, regularizationstrength of C=0.01, and a 500-best list for hopeand fear selection (Chiang, 2012) with a singlepassive-aggressive update for each sentence (Ei-delman, 2012).Baseline Features We used a set of 16 stan-dard baseline features: rule translation relativefrequency P (e|f), lexical translation probabilitiesPlex(e|f) and Plex(f |e), target n-gram languagemodel P (e), penalties for source and target words,passing an untranslated source word to the tar-get side, singleton rule and source side, as wellas counts for arity-0,1, or 2 SCFG rules, the totalnumber of rules used, and the number of times theglue rule is used.2.1 Data preparationFor both languages, we used the provided Eu-roparl and News Commentary parallel trainingdata to create the translation grammar neces-sary for our model.
For Russian, we addi-tionally used the Common Crawl and Yandexdata.
The data were lowercased and tokenized,then filtered for length and aligned using theGIZA++ implementation of IBM Model 4 (Ochand Ney, 2003) to obtain one-to-many align-ments in both directions and symmetrized sing thegrow-diag-final-and method (Koehn et al 2003).3We approximate corpus BLEU by scoring sentences us-ing a pseudo-document of previous 1-best translations (Chi-ang et al 2009).We constructed a 5-gram language model us-ing SRILM (Stolcke, 2002) from the providedEnglish monolingual training data and paralleldata with modified Kneser-Ney smoothing (Chenand Goodman, 1996), which was binarized usingKenLM (Heafield, 2011).
The sentence-specifictranslation grammars were extracted using a suffixarray rule extractor (Lopez, 2007).For German, we used the 3,003 sentences innewstest2011 as our Dev set, and report resultson the 3,003 sentences of the newstest2012 Testset using BLEU and TER (Snover et al 2006).For Russian, we took the first 2,000 sentences ofnewstest2012 for Dev, and report results on the re-maining 1,003.
For both languages, we selected1,000 sentences from the bitext to be used as anadditional testing set (Test2).Compound segmentation lattices As Germanis a morphologically rich language with produc-tive compounding, we use word segmentation lat-tices as input for the German translation task.These lattices encode alternative segmentations ofcompound words, allowing the decoder to auto-matically choose which segmentation is best.
Weuse a maximum entropy model with recommendedsettings to create lattices for the dev and test sets,as well as for obtaining the 1-best segmentation ofthe training data (Dyer, 2009).3 EvaluationThis section describes the experiments we con-ducted in moving towards a better understandingof the benefits and challenges posed by large-scalehigh-dimensional discriminative tuning.3.1 Sparse FeaturesThe ability to incorporate sparse features is the pri-mary reason for the recent move away from Min-imum Error Rate Training (Och, 2003), as well asfor performing large-scale discriminative training.We include the following sparse Boolean featuretemplates in our system in addition to the afore-mentioned baseline features: rule identity (for ev-ery unique rule in the grammar), rule shape (map-ping rules to sequences of terminals and nontermi-nals), target bigrams, lexical insertions and dele-tions (for the top 150 unaligned words from thetraining data), context-dependent word pairs (forthe top 300 word pairs in the training data), andstructural distortion (Chiang et al 2008).129Dev Test Test2 5k 10k 25k 50ken 75k 74k 27k 132k 255k 634k 1258kde 74k 73k 26k 133k 256k 639k 1272kTable 1: Corpus statistics in tokens for German.Dev Test Test2 15kru 46k 24k 24k 350ken 50k 27k 25k 371kTable 2: Corpus statistics in tokens forRussian.Set # features Tune Test?BLEU ?BLEU ?TERde-en 16 22.38 22.69 60.61+sparse 108k 23.86 23.01 59.89ru-en 16 30.18 29.89 49.05+sparse 77k 32.40 30.81 48.40Table 3: Results with the addition of sparse fea-tures for German and Russian.All of these features are generated from thetranslation rules on the fly, and thus do not haveto be stored as part of the grammar.
To allow formemory efficiency while scaling the training data,we hash all the lexical features from their stringrepresentation into a 64-bit integer.Altogether, these templates result in millions ofpotential features, thus how to select appropriatefeatures, and how to properly learn their weightscan have a large impact on the potential benefit.3.2 Adaptive Learning RateThe passive-aggressive update used in MIRA has asingle learning rate ?
for all features, which alongwith ?
limits the amount each feature weight canchange at each update.
However, since the typicaldense features (e.g., language model) are observedfar more frequently than sparse features (e.g., ruleidentity), it has been shown to be advantageousto use an adaptive per-feature learning rate thatallows larger steps for features that do not havemuch support (Green et al 2013; Duchi et al2011).
Essentially, instead of having a single pa-rameter ?,??
min(C, cost(y?)?w>(f(y+)?
f(y?))?f(y+)?
f(y?)?2)w?
w + ??(f(y+)?
f(y?
))we instead have a vector ?
with one entry for eachfeature weight:?
?1 ?
?
?1 + ?diag(ww>)w?
w + ??1/2(f(y+)?
f(y?
))?=1?=0.01?=0.122.222.422.622.82323.223.423.623.824BLEUIterationFigure 1: Learning curves for tuning when usinga single step size (?)
versus different per-featurelearning rates.In practice, this update is very similar to that ofAROW (Crammer et al 2009; Chiang, 2012).Figure 1 shows learning curves for sparse mod-els with a single learning rate, and adaptive learn-ing with ?=0.01 and ?=0.1, with associated re-sults on Test in Table 4.4 As can be seen, usinga single ?
produces almost no gain on Dev.
How-ever, while both settings using an adaptive rate farebetter, the proper setting of ?
is important.
With?=0.01 we observe 0.5 BLEU gain over ?=0.1 intuning, which translates to a small gain on Test.Henceforth, we use an adaptive learning rate with?=0.01 for all experiments.Table 3 presents baseline results for both lan-guages.
With the addition of sparse features, tun-ing scores increase by 1.5 BLEU for German, lead-ing to a 0.3 BLEU increase on Test, and 2.2 BLEUfor Russian, with 1 BLEU increase on Test.
Themajority of active features for both languages arerule id (74%), followed by target bigrams (14%)and context-dependent word pairs (11%).3.3 Feature SelectionAs the tuning set size increases, so do the num-ber of active features.
This may cause practi-cal problems, such as reduced speed of computa-tion and memory issues.
Furthermore, while some4All sparse models are initialized with the same tunedbaseline weights.
Learning rates are local to each mapper.130Adaptive # feat.
Tune Test?BLEU ?BLEU ?TERnone 74k 22.75 22.87 60.19?=0.01 108k 23.86 23.01 59.89?=0.1 62k 23.32 22.92 60.09Table 4: Results with different ?
settings for using a per-feature learning rate with sparse features.Set # feat.
Tune Test?BLEU ?BLEU ?TERall 510k 32.99 22.36 59.26top 200k 200k 32.96 22.35 59.29all 373k 34.26 28.84 49.29top 200k 200k 34.45 28.98 49.30Table 5: Comparison of using all features versustop k selection.sparse features will generalize well, others maynot, thereby incurring practical costs with no per-formance benefit.
Simianer et al(2012) recentlyexplored `1/`2 regularization for joint feature se-lection for SMT in order to improve efficiency andcounter overfitting effects.
When performing par-allel learning, this allows for selecting a reducedset of the top k features at each iteration that areeffective across all learners.Table 5 compares selecting the top 200k fea-tures versus no selection for a larger German andRussian tuning set (?3.4).
As can be seen, weachieve the same performance with the top 200kfeatures as we do when using double that amount,while the latter becomes increasing cumbersometo manage.
Therefore, we use a top 200k selectionfor the remainder of this work.3.4 Large-Scale TrainingIn the previous section, we saw that learningsparse features on the small development set leadsto substantial gains in performance.
Next, wewanted to evaluate if we can obtain further gainsby scaling the tuning data to learn parameters di-rectly on a portion of the training bitext.
Since thebitext is used to learn rules for translation, usingthe same parallel sentences for grammar extrac-tion as well as for tuning feature weights can leadto severe overfitting (Flanigan et al 2013).
Toavoid this issue, we used a jackknifing method tosplit the training data into n = 10 folds, and builta translation system on n?1 folds, while samplingsentences from the News Commentary portion ofthe held-out fold to obtain tuning sets from 5,000to 50,000 sentences for German, and 15,000 sen-tences for Russian.Results for large-scale training for German arepresented in Table 6.
Although we cannot com-pare the tuning scores across different size sets,we can see that tuning scores for all sets improvesubstantially with sparse features.
Unfortunately,with increasing tuning set size, we see very littleimprovement in Test BLEU and TER with eitherfeature set.
Similar findings for Russian are pre-sented in Table 7.
Introducing sparse features im-proves performance on each set, respectively, butDev always performs better on Test.While tuning on Dev data results in better BLEUon Test than when tuning on the larger sets, it isimportant to note that although we are able to tunemore features on the larger bitext tuning sets, theyare not composed of the same genre as the Tuneand Test sets, resulting in a domain mismatch.This phenomenon is further evident in Germanwhen testing each model on Test2, which is se-lected from the bitext, and is thus closer matchedto the larger tuning sets, but is separate from boththe parallel data used to build the translation modeland the tuning sets.
Results on Test2 clearly showsignificant improvement using any of the largertuning sets versus Dev for both the baseline andsparse features.
The 50k sparse setting achievesalmost 1 BLEU and 2 TER improvement, showingthat there are significant differences between theDev/Test sets and sets drawn from the bitext.For Russian, we amplified the effects by select-ing Test2 from the portion of the bitext that is sepa-rate from the tuning set, but is among the sentencesused to create the translation model.
The effects ofoverfitting are markedly more visible here, as thereis almost a 7 BLEU difference between tuning onDev and the 15k set with sparse features.
Further-more, it is interesting to note when looking at Devthat using sparse features has a significant nega-tive impact, as the baseline tuned Dev performs131Tuning Test?BLEU ?TER5k 22.81 59.9010k 22.77 59.7825k 22.88 59.7750k 22.86 59.76Table 8: Results for German with 2 iterations oftuning on Dev after tuning on larger set.reasonably well, while the introduction of sparsefeatures leads to overfitting the specificities of theDev/Test genre, which are not present in the bitext.We attempted two strategies to mitigate thisproblem: combining the Dev set with the largerbitext tuning set from the beginning, and tuningon a larger set to completion, and then running 2additional iterations of tuning on the Dev set usingthe learned model.
Results for tuning on Dev and alarger set together are presented in Table 7 for Rus-sian and Table 6 for German.
As can be seen, theresulting model improves somewhat on the othergenre and strikes a middle ground, although it isworse on Test than Dev.Table 8 presents results for tuning several ad-ditional iterations after learning a model on thelarger sets.
Although this leads to gains of around0.5 BLEU on Test, none of the models outperformsimply tuning on Dev.
Thus, neither of these twostrategies seem to help.
In future work, we planto forgo randomly sampling the tuning set fromthe bitext, and instead actively select the tuningset based on similarity to the test set.4 ConclusionWe explored strategies for scaling learning forSMT to large tuning sets with sparse features.While incorporating an adaptive per-feature learn-ing rate and feature selection, we were able touse Hadoop to efficiently take advantage of largeamounts of data.
Although discriminative trainingon larger sets still remains problematic, having thecapability to do so remains highly desirable, andwe plan to continue exploring methods by whichto leverage the power of the bitext effectively.AcknowledgmentsThis research was supported in part by the DARPABOLT program, Contract No.
HR0011-12-C-0015; NSF under awards IIS-0916043 and IIS-1144034.
Vladimir Eidelman is supported by aNDSEG Fellowship.ReferencesS.
Chen and J. Goodman.
1996.
An empirical studyof smoothing techniques for language modeling.
InACL.Colin Cherry and George Foster.
2012.
Batch tun-ing strategies for statistical machine translation.
InNAACL.David Chiang, Yuval Marton, and Philip Resnik.
2008.Online large-margin training of syntactic and struc-tural translation features.
In EMNLP.D.
Chiang, K. Knight, and W. Wang.
2009.
11,001new features for statistical machine translation.
InNAACL-HLT.D.
Chiang.
2012.
Hope and fear for discriminativetraining of statistical translation models.
JMLR,13:1159?1187.K.
Crammer, A. Kulesza, and M. Dredze.
2009.
Adap-tive regularization of weight vectors.
In NIPS.J.
Dean and S. Ghemawat.
2004.
MapReduce: Simpli-fied data processing on large clusters.
In OSDI.J.
Duchi, E. Hazan, and Y.
Singer.
2011.
Adaptive sub-gradient methods for online learning and stochasticoptimization.
JMLR, 12:2121?2159.C.
Dyer, A. Lopez, J. Ganitkevitch, J. Weese,F.
Ture, P. Blunsom, H. Setiawan, V. Eidelman, andP.
Resnik.
2010. cdec: A decoder, alignment, andlearning framework for finite-state and context-freetranslation models.
In ACL System Demonstrations.Chris Dyer.
2009.
Using a maximum entropy model tobuild segmentation lattices for mt.
In Proceedingsof NAACL-HLT.Vladimir Eidelman, Ke Wu, Ferhan Ture, PhilipResnik, and Jimmy Lin.
2013.
Mr. MIRA: Open-source large-margin structured learning on map-reduce.
In ACL System Demonstrations.Vladimir Eidelman.
2012.
Optimization strategies foronline large-margin learning in machine translation.In WMT.Jeffrey Flanigan, Chris Dyer, and Jaime Carbonell.2013.
Large-scale discriminative training for statis-tical machine translation using held-out line search.In NAACL.S.
Green, S. Wang, D. Cer, and C. Manning.
2013.Fast and adaptive online training of feature-richtranslation models.
In ACL.Kenneth Heafield.
2011.
KenLM: faster and smallerlanguage model queries.
In WMT.Mark Hopkins and Jonathan May.
2011.
Tuning asranking.
In EMNLP.132Tuning # mappers # features Tune Test Test2?BLEU ?BLEU ?TER ?BLEU ?TERDev 120 16 22.38 22.69 60.61 29.31 54.265k 120 16 32.60 22.14 59.60 29.69 52.9610k 120 16 33.16 22.06 59.43 29.93 52.37Dev+10k 120 16 19.40 22.32 59.37 30.17 52.4525k 300 16 32.48 22.21 59.54 30.03 51.7150k 600 16 32.21 22.21 59.39 29.94 52.55Dev 120 108k 23.86 23.01 59.89 29.65 53.865k 120 159k 33.70 22.26 59.26 30.53 51.8410k 120 200k 34.00 22.12 59.24 30.51 51.71Dev+10k 120 200k 19.62 22.42 59.17 30.26 52.2125k 300 200k 32.96 22.35 59.29 30.39 52.1450k 600 200k 32.86 22.40 59.15 30.54 51.88Table 6: German evaluation with large-scale tuning, showing numbers of mappers employed, number ofactive features for best model, and test scores on Test and bitext Test2 domains.Tuning # mappers # features Tune Test Test2?BLEU ?BLEU ?TER ?BLEU ?TERDev 120 16 30.18 29.89 49.05 57.14 32.5615k 200 16 34.65 28.60 49.63 59.64 30.65Dev+15k 200 16 33.97 28.88 49.37 58.24 31.81Dev 120 77k 32.40 30.81 48.40 52.90 36.8515k 200 200k 35.05 28.34 49.69 59.81 30.59Dev+15k 200 200k 34.45 28.98 49.30 57.61 32.71Table 7: Russian evaluation with large-scale tuning, showing numbers of mappers employed, number ofactive features for best model, and test scores on Test and bitext Test2 domains.Liang Huang and David Chiang.
2007.
Forest rescor-ing: Faster decoding with integrated language mod-els.
In ACL.Philipp Koehn, Franz Josef Och, and Daniel Marcu.2003.
Statistical phrase-based translation.
InNAACL.Adam Lopez.
2007.
Hierarchical phrase-based trans-lation with suffix arrays.
In EMNLP.Franz Och and Hermann Ney.
2003.
A systematiccomparison of various statistical alignment models.In Computational Linguistics.Franz Josef Och.
2003.
Minimum error rate training instatistical machine translation.
In ACL.K.
Papineni, S. Roukos, T. Ward, and W.-J.
Zhu.
2002.BLEU: a method for automatic evaluation of ma-chine translation.
In ACL.P.
Simianer, S. Riezler, and C. Dyer.
2012.
Joint fea-ture selection in distributed stochastic learning forlarge-scale discriminative training in SMT.
In ACL.M.
Snover, B. Dorr, R. Schwartz, L. Micciulla, andJ.
Makhoul.
2006.
A study of translation edit ratewith targeted human annotation.
In AMTA.Andreas Stolcke.
2002.
SRILM - an extensible lan-guage modeling toolkit.
In ICSLP.133
