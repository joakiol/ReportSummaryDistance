Probabilistic Parsing StrategiesMark-Jan NederhofFaculty of ArtsUniversity of GroningenP.O.
Box 716NL-9700 AS GroningenThe Netherlandsmarkjan@let.rug.nlGiorgio SattaDept.
of Information EngineeringUniversity of Paduavia Gradenigo, 6/AI-35131 PadovaItalysatta@dei.unipd.itAbstractWe present new results on the relation betweencontext-free parsing strategies and their probabilis-tic counter-parts.
We provide a necessary conditionand a sufficient condition for the probabilistic exten-sion of parsing strategies.
These results generalizeexisting results in the literature that were obtainedby considering parsing strategies in isolation.1 IntroductionContext-free grammars (CFGs) are standardly usedin computational linguistics as formal models of thesyntax of natural language, associating sentenceswith all their possible derivations.
Other computa-tional models with the same generative capacity asCFGs are also adopted, as for instance push-downautomata (PDAs).
One of the advantages of the useof PDAs is that these devices provide an operationalspecification that determines which steps must beperformed when parsing an input string, somethingthat is not offered by CFGs.
In other words, PDAscan be associated to parsing strategies for context-free languages.
More precisely, parsing strategiesare traditionally specified as constructions that mapCFGs to language-equivalent PDAs.
Popular ex-amples of parsing strategies are the standard con-structions of top-down PDAs (Harrison, 1978), left-corner PDAs (Rosenkrantz and Lewis II, 1970),shift-reduce PDAs (Aho and Ullman, 1972) and LRPDAs (Sippu and Soisalon-Soininen, 1990).CFGs and PDAs have probabilistic counterparts,called probabilistic CFGs (PCFGs) and probabilis-tic PDAs (PPDAs).
These models are very popularin natural language processing applications, wherethey are used to define a probability distributionfunction on the domain of all derivations for sen-tences in the language of interest.
In PCFGs andPPDAs, probabilities are assigned to rules or tran-sitions, respectively.
However, these probabilitiescannot be chosen entirely arbitrarily.
For example,for a given nonterminalA in a PCFG, the sum of theprobabilities of all rules rewritingAmust be 1.
Thismeans that, out of a total of saym rules rewritingA,only m?
1 rules represent ?free?
parameters.Depending on the choice of the parsing strategy,the constructed PDA may allow different probabil-ity distributions than the underlying CFG, since theset of free parameters may differ between the CFGand the PDA, both quantitatively and qualitatively.For example, (Sornlertlamvanich et al, 1999) and(Roark and Johnson, 1999) have shown that a prob-ability distribution that can be obtained by trainingthe probabilities of a CFG on the basis of a corpuscan be less accurate than the probability distributionobtained by training the probabilities of a PDA con-structed by a particular parsing strategy, on the basisof the same corpus.
Also the results from (Chitraoand Grishman, 1990), (Charniak and Carroll, 1994)and (Manning and Carpenter, 2000) could be seenin this light.The question arises of whether parsing strate-gies can be extended probabilistically, i.e., whethera given construction of PDAs from CFGs can be?augmented?
with a function defining the probabili-ties for the target PDA, given the probabilities asso-ciated with the input CFG, in such a way that the ob-tained probabilistic distributions on the CFG deriva-tions and the corresponding PDA computations areequivalent.
Some first results on this issue have beenpresented by (Tendeau, 1995), who shows that thealready mentioned left-corner parsing strategy canbe extended probabilistically, and later by (Abney etal., 1999) who show that the pure top-down parsingstrategy and a specific type of shift-reduce parsingstrategy can be probabilistically extended.One might think that any ?practical?
parsingstrategy can be probabilistically extended, but thisturns out not to be the case.
We briefly discusshere a counter-example, in order to motivate the ap-proach we have taken in this paper.
ProbabilisticLR parsing has been investigated in the literature(Wright and Wrigley, 1991; Briscoe and Carroll,1993; Inui et al, 2000) under the assumption thatit would allow more fine-grained probability distri-butions than the underlying PCFGs.
However, thisis not the case in general.
Consider a PCFG withrule/probability pairs:S ?
AB , 1 B ?
bC , 23A?
aC , 13 B ?
bD ,13A?
aD , 23 C ?
xc, 1D ?
xd , 1There are two key transitions in the associated LRautomaton, which represent shift actions over c andd (we denote LR states by their sets of kernel itemsand encode these states into stack symbols):?c : {C ?
x ?
c,D ?
x ?
d}c7?
{C ?
x ?
c,D ?
x ?
d} {C ?
xc ?
}?d : {C ?
x ?
c,D ?
x ?
d}d7?
{C ?
x ?
c,D ?
x ?
d} {D ?
xd ?
}Assume a proper assignment of probabilities to thetransitions of the LR automaton, i.e., the sum oftransition probabilities for a given LR state is 1.
Itcan be easily seen that we must assign probabil-ity 1 to all transitions except ?c and ?d, since thisis the only pair of distinct transitions that can be ap-plied for one and the same top-of-stack symbol, viz.
{C ?
x ?
c,D ?
x ?
d}.
However, in the PCFGmodel we havePr(axcbxd)Pr(axdbxc) =Pr(A?aC )?Pr(B?bD)Pr(A?aD)?Pr(B?bC ) =13 ?1323 ?23= 14whereas in the LR PPDA model we havePr(axcbxd)Pr(axdbxc) =Pr(?c)?Pr(?d)Pr(?d)?Pr(?c)= 1 6= 14 .Thus we conclude that there is no proper assignmentof probabilities to the transitions of the LR automa-ton that would result in a distribution on the gener-ated language that is equivalent to the one inducedby the source PCFG.
Therefore the LR strategy doesnot allow probabilistic extension.One may seemingly solve this problem by drop-ping the constraint of properness, letting each tran-sition that outputs a rule have the same probabilityas that rule in the PCFG, and letting other transitionshave probability 1.
However, the properness condi-tion for PDAs has been heavily exploited in pars-ing applications, in doing incremental left-to-rightprobability computation for beam search (Roarkand Johnson, 1999; Manning and Carpenter, 2000),and more generally in integration with other lin-ear probabilistic models.
Furthermore, commonlyused training algorithms for PCFGS/PPDAs alwaysproduce proper probability assignments, and manydesired mathematical properties of these methodsare based on such an assumption (Chi and Geman,1998; Sa?nchez and Bened?
?, 1997).
We may there-fore discard non-proper probability assignments inthe current study.However, such probability assignments are out-side the reach of the usual training algorithms forPDAs, which always produce proper PDAs.
There-fore, we may discard such assignments in the cur-rent study, which investigates aspects of the poten-tial of training algorithms for CFGs and PDAs.What has been lacking in the literature is a theo-retical framework to relate the parameter space of aCFG to that of a PDA constructed from the CFG bya particular parsing strategy, in terms of the set ofallowable probability distributions over derivations.Note that the number of free parameters alone isnot a satisfactory characterization of the parameterspace.
In fact, if the ?nature?
of the parameters isill-chosen, then an increase in the number of param-eters may lead to a deterioration of the accuracy ofthe model, due to sparseness of data.In this paper we extend previous results, whereonly a few specific parsing strategies were consid-ered in isolation, and provide some general char-acterization of parsing strategies that can be prob-abilistically extended.
Our main contribution canbe stated as follows.?
We define a theoretical framework to relate theparameter space defined by a CFG and that de-fined by a PDA constructed from the CFG by aparticular parsing strategy.?
We provide a necessary condition and a suffi-cient condition for the probabilistic extensionof parsing strategies.We use the above findings to establish new resultsabout probabilistic extensions of parsing strategiesthat are used in standard practice in computationallinguistics, as well as to provide simpler proofs ofalready known results.We introduce our framework in Section 3 and re-port our main results in Sections 4 and 5.
We discussapplications of our results in Section 6.2 PreliminariesIn this paper we assume some familiarity with def-initions of (P)CFGs and (P)PDAs.
We refer thereader to standard textbooks and publications as forinstance (Harrison, 1978; Booth and Thompson,1973; Santos, 1972).A CFG G is a tuple (?, N, S, R), with ?
and Nthe sets of terminals and nonterminals, respectively,S the start symbol and R the set of rules.
In thispaper we only consider left-most derivations, repre-sented as strings d ?
R?
and simply called deriva-tions.
For ?, ?
?
(?
?N)?, we write ?
?d ?
withthe usual meaning.
If ?
= S and ?
= w ?
?
?, wecall d a complete derivation of w. We say a CFG isreduced if each rule in R occurs in some completederivation.A PCFG is a pair (G, p) consisting of a CFG Gand a probability function p from R to real num-bers in the interval [0, 1].
A PCFG is properif?pi=(A??
)?R p(pi) = 1 for each A ?
N .The probability of a (left-most) derivation d =pi1 ?
?
?pim, pii ?
R for 1 ?
i ?
m, is p(d) =?mi=1 p(pii).
The probability of a string w ?
?
?is p(w) =?S?dw p(d).
A PCFG is consistent if?w???
p(w) = 1.
A PCFG (G, p) is reduced if G isreduced.In this paper we will mainly consider push-downtransducers rather than push-down automata.
Push-down transducers not only compute derivations ofthe grammar while processing an input string, butthey also explicitly produce output strings fromwhich these derivations can be obtained.
We usetransducers for two reasons.
First, constraints onthe output strings allow us to restrict our attentionto ?reasonable?
parsing strategies.
Those strategiesthat cannot be formalized within these constraintsare unlikely to be of practical interest.
Secondly,mappings from input strings to derivations, such asthose realized by push-down transducers, turn outto be a very powerful abstraction and allow directproofs of several general results.Contrary to many textbooks, our push-down de-vices do not possess states next to stack symbols.This is without loss of generality, since states canbe encoded into the stack symbols, given the typesof transitions that we allow.
Thus, a PDT A is a6-tuple (?
?, ?
?, Q, Xin, Xfin, ?
), with ??
and??
the input and output alphabets, respectively, Qthe set of stack symbols, including the initial and fi-nal stack symbols Xin and Xfin, respectively, and?
the set of transitions.
Each transition has oneof the following three forms: X 7?
XY , called apush transition, YX 7?
Z, called a pop transition,or Xx,y7?
Y , called a swap transition; here X , Y ,Z ?
Q, x ?
??
?
{?}
is the input read by the tran-sition and y ?
???
is the written output.
Note thatin our notation, stacks grow from left to right, i.e.,the top-most stack symbol will be found at the rightend.
A configuration of a PDT is a triple (?,w, v),where ?
?
Q?
is a stack, w ?
???
is the remain-ing input, and v ?
???
is the output generated sofar.
Computations are represented as strings c ???.
For configurations (?,w, v) and (?,w?, v?
), wewrite (?,w, v) `c (?,w?, v?)
with the usual mean-ing, and write (?,w, v) `?
(?,w?, v?)
when c is ofno importance.
If (Xin, w, ?)
`c (Xfin, ?, v), thenc is a complete computation of w, and the outputstring v is denoted out(c).
A PDT is reduced ifeach transition in ?
occurs in some complete com-putation.Without loss of generality, we assume that com-binations of different types of transitions are not al-lowed for a given stack symbol.
More precisely,for each stack symbol X 6= Xfin, the PDA canonly take transitions of a single type (push, pop orswap).
A PDT can easily be brought in this formby introducing for each X three new stack symbolsXpush , Xpop and Xswap and new swap transitionsX?,?7?
Xpush , X?,?7?
Xpop and X?,?7?
Xswap .
Ineach existing transition that operates on top-of-stackX , we then replace X by one from Xpush , Xpop orXswap , depending on the type of that transition.
Wealso assume that Xfin does not occur in the left-hand side of a transition, again without loss of gen-erality.A PPDT is a pair (A, p) consisting of a PDT Aand a probability function p from?
to real numbersin the interval [0, 1].
A PPDT is proper if?
?
?=(X 7?XY )??
p(?)
= 1 for each X ?
Qsuch that there is at least one transition X 7?XY , Y ?
Q;?
?
?=(Xx,y7?Y )??p(?)
= 1 for each X ?
Q suchthat there is at least one transition X x,y7?
Y ,x ?
??
?
{?
}, y ?
???
, Y ?
Q; and?
?
?=(Y X 7?Z)??
p(?)
= 1, for each X,Y ?
Qsuch that there is at least one transition Y X 7?Z, Z ?
Q.The probability of a computation c = ?1 ?
?
?
?m,?i ?
?
for 1 ?
i ?
m, is p(c) =?mi=1 p(?i).
The probability of a stringw is p(w) =?(Xin,w,?
)`c(Xfin,?,v) p(c).
A PPDT is consistentif ?w???
p(w) = 1.
A PPDT (A, p) is reduced ifA is reduced.3 Parsing StrategiesThe term ?parsing strategy?
is often used informallyto refer to a class of parsing algorithms that behavesimilarly in some way.
In this paper, we assign aformal meaning to this term, relying on the obser-vation by (Lang, 1974) and (Billot and Lang, 1989)that many parsing algorithms for CFGs can be de-scribed in two steps.
The first is a construction ofpush-down devices from CFGs, and the second isa method for handling nondeterminism (e.g.
back-tracking or dynamic programming).
Parsing algo-rithms that handle nondeterminism in different waysbut apply the same construction of push-down de-vices from CFGs are seen as realizations of the sameparsing strategy.Thus, we define a parsing strategy to be a func-tion S that maps a reduced CFG G = (?
?, N, S,R) to a pair S(G) = (A, f) consisting of a reducedPDT A = (?
?, ?
?, Q, Xin, Xfin, ?
), and a func-tion f that maps a subset of ???
to a subset of R?,with the following properties:?
R ?
??.?
For each string w ?
???
and each completecomputation c on w, f(out(c)) = d is a (left-most) derivation of w. Furthermore, each sym-bol from R occurs as often in out(c) as it oc-curs in d.?
Conversely, for each string w ?
???
andeach derivation d of w, there is preciselyone complete computation c on w such thatf(out(c)) = d.If c is a complete computation, we will write f(c)to denote f(out(c)).
The conditions above then im-ply that f is a bijection from complete computationsto complete derivations.
Note that output strings of(complete) computations may contain symbols thatare not in R, and the symbols that are in R mayoccur in a different order in v than in f(v) = d.The purpose of the symbols in ??
?
R is to helpthis process of reordering of symbols from R in v,as needed for instance in the case of the left-cornerparsing strategy (see (Nijholt, 1980, pp.
22?23) fordiscussion).A probabilistic parsing strategy is defined tobe a function S that maps a reduced, proper andconsistent PCFG (G, pG) to a triple S(G, pG) =(A, pA, f), where (A, pA) is a reduced, proper andconsistent PPDT, with the same properties as a(non-probabilistic) parsing strategy, and in addition:?
For each complete derivation d and each com-plete computation c such that f(c) = d, pG(d)equals pA(c).In other words, a complete computation has thesame probability as the complete derivation that itis mapped to by function f .
An implication ofthis property is that for each string w ?
???
, theprobabilities assigned to that string by (G, pG) and(A, pA) are equal.We say that probabilistic parsing strategy S ?
is anextension of parsing strategy S if for each reducedCFG G and probability function pG we have S(G) =(A, f) if and only if S ?
(G, pG) = (A, pA, f) forsome pA.4 Correct-Prefix PropertyIn this section we present a necessary condition forthe probabilistic extension of a parsing strategy.
Fora given PDT, we say a computation c is dead if(Xin, w1, ?)
`c (?, ?, v1), for some ?
?
Q?, w1 ????
and v1 ?
???
, and there are no w2 ?
???
andv2 ?
???
such that (?,w2, ?)
`?
(Xfin, ?, v2).
In-formally, a dead computation is a computation thatcannot be continued to become a complete compu-tation.
We say that a PDT has the correct-prefixproperty (CPP) if it does not allow any dead com-putations.
We also say that a parsing strategy hasthe CPP if it maps each reduced CFG to a PDT thathas the CPP.Lemma 1 For each reduced CFG G, there is aprobability function pG such that PCFG (G, pG) isproper and consistent, and pG(d) > 0 for all com-plete derivations d.Proof.
Since G is reduced, there is a finite set Dconsisting of complete derivations d, such that foreach rule pi in G there is at least one d ?
D inwhich pi occurs.
Let npi,d be the number of occur-rences of rule pi in derivation d ?
D, and let npi be?d?D npi,d, the total number of occurrences of pi inD.
Let nA be the sum of npi for all rules pi withA inthe left-hand side.
A probability function pG can bedefined through ?maximum-likelihood estimation?such that pG(pi) = npinA for each rule pi = A?
?.For all nonterminals A, ?pi=A??
pG(pi) =?pi=A??
npinA =nAnA= 1, which means that thePCFG (G, pG) is proper.
Furthermore, it has beenshown in (Chi and Geman, 1998; Sa?nchez andBened?
?, 1997) that a PCFG (G, pG) is consistent ifpG was obtained by maximum-likelihood estimationusing a set of derivations.
Finally, since npi > 0 foreach pi, also pG(pi) > 0 for each pi, and pG(d) > 0for all complete derivations d.We say a computation is a shortest dead compu-tation if it is dead and none of its proper prefixes isdead.
Note that each dead computation has a uniqueprefix that is a shortest dead computation.
For aPDT A, let TA be the union of the set of all com-plete computations and the set of all shortest deadcomputations.Lemma 2 For each proper PPDT (A, pA),?c?TA pA(c) ?
1.Proof.
The proof is a trivial variant of the proofthat for a proper PCFG (G, pG), the sum of pG(d) forall derivations d cannot exceed 1, which is shown by(Booth and Thompson, 1973).From this, the main result of this section follows.Theorem 3 A parsing strategy that lacks the CPPcannot be extended to become a probabilistic pars-ing strategy.Proof.
Take a parsing strategy S that does not havethe CPP.
Then there is a reduced CFG G = (?
?, N,S, R), with S(G) = (A, f) for some A and f , anda shortest dead computation c allowed by A.It follows from Lemma 1 that there is a proba-bility function pG such that (G, pG) is a proper andconsistent PCFG and pG(d) > 0 for all completederivations d. Assume we also have a probabilityfunction pA such that (A, pA) is a proper and con-sistent PPDT and pA(c?)
= pG(f(c?))
for each com-plete computation c?.
SinceA is reduced, each tran-sition ?
must occur in some complete computationc?.
Furthermore, for each complete computation c?there is a complete derivation d such that f(c?)
= d,and pA(c?)
= pG(d) > 0.
Therefore, pA(?)
> 0for each transition ?
, and pA(c) > 0, where c is theabove-mentioned dead computation.Due to Lemma 2, 1 ?
?c?
?TA pA(c?)
??w????
pA(w) + pA(c) > ?w????
pA(w) =?w????
pG(w).
This is in contradiction with the con-sistency of (G, pG).
Hence, a probability functionpA with the properties we required above cannot ex-ist, and therefore S cannot be extended to become aprobabilistic parsing strategy.5 Strong PredictivenessIn this section we present our main result, which is asufficient condition allowing the probabilistic exten-sion of a parsing strategy.
We start with a technicalresult that was proven in (Abney et al, 1999; Chi,1999; Nederhof and Satta, 2003).Lemma 4 Given a non-proper PCFG (G, pG), G =(?,N, S,R), there is a probability function p?G suchthat PCFG (G, p?G) is proper and, for every com-plete derivation d, p?G(d) = 1C ?
pG(d), where C =?S?d?w,w???
pG(d?
).Note that if PCFG (G, pG) in the above lemma isconsistent, then C = 1 and (G, p?G) and (G, pG) de-fine the same distribution on derivations.
The nor-malization procedure underlying Lemma 4 makesuse of quantities?A?dw,w???
pG(d) for each A ?N .
These quantities can be computed to any degreeof precision, as discussed for instance in (Booth andThompson, 1973) and (Stolcke, 1995).
Thus nor-malization of a PCFG can be effectively computed.For a fixed PDT, we define the binary relation ;on stack symbols by: Y ; Y ?
if and only if(Y,w, ?)
`?
(Y ?, ?, v) for some w ?
???
andv ?
???
.
In words, some subcomputation of thePDT may start with stack Y and end with stack Y ?.Note that all stacks that occur in such a subcompu-tation must have height of 1 or more.
We say that a(P)PDA or a (P)PDT has the strong predictivenessproperty (SPP) if the existence of three transitionsX 7?
XY , XY1 7?
Z1 and XY2 7?
Z2 such thatY ; Y1 and Y ; Y2 implies Z1 = Z2.
Infor-mally, this means that when a subcomputation startswith some stack ?
and some push transition ?
, thensolely on the basis of ?
we can uniquely determinewhat stack symbol Z1 = Z2 will be on top of thestack in the firstly reached configuration with stackheight equal to |?|.
Another way of looking at it isthat no information may flow from higher stack el-ements to lower stack elements that was not alreadypredicted before these higher stack elements cameinto being, hence the term ?strong predictiveness?.We say that a parsing strategy has the SPP if it mapseach reduced CFG to a PDT with the SPP.Theorem 5 Any parsing strategy that has the CPPand the SPP can be extended to become a proba-bilistic parsing strategy.Proof.
Consider a parsing strategy S that has theCPP and the SPP, and a proper, consistent and re-duced PCFG (G, pG), G = (?
?, N, S, R).
LetS(G) = (A, f), A = (?
?, ?
?, Q, Xin, Xfin, ?
).We will show that there is a probability function pAsuch that (A, pA) is a proper and consistent PPDT,and pA(c) = pG(f(c)) for all complete computa-tions c.We first construct a PPDT (A, p?A) as follows.For each scan transition ?
= X x,y7?
Y in ?, letp?A(?)
= pG(y) in case y ?
R, and p?A(?)
= 1otherwise.
For all remaining transitions ?
?
?, letp?A(?)
= 1.
Note that (A, p?A) may be non-proper.Still, from the definition of f it follows that, for eachcomplete computation c, we havep?A(c) = pG(f(c)), (1)and so our PPDT is consistent.We now map (A, p?A) to a language-equivalentPCFG (G?, pG?
), G?
= (?
?, Q,Xin, R?
), where R?contains the following rules with the specified asso-ciated probabilities:?
X ?
YZ with pG?
(X ?
YZ ) = p ?A(X 7?XY ), for each X 7?
XY ?
?
with Z theunique stack symbol such that there is at leastone transition XY ?
7?
Z with Y ; Y ?;?
X ?
xY with pG?
(X ?
xY ) = p ?A(Xx7?Y ), for each transition X x7?
Y ?
?;?
Y ?
?
with pG?
(X ?
?)
= 1, for each stacksymbol Y such that there is at least one transi-tion XY 7?
Z ?
?
or such that Y = Xfin.It is not difficult to see that there exists a bijectionf ?
from complete computations of A to completederivations of G?, and that we havepG?(f?
(c)) = p?A(c), (2)for each complete computation c. Thus (G?, pG?
)is consistent.
However, note that (G?, pG?)
is notproper.By Lemma 4, we can construct a new PCFG(G?, p?G?)
that is proper and consistent, and such thatpG?
(d) = p?G?
(d), for each complete derivation d ofG?.
Thus, for each complete computation c ofA, wehavep?G?(f?
(c)) = pG?(f?(c)).
(3)We now transfer back the probabilities of rules of(G?, p?G?)
to the transitions ofA.
Formally, we definea new probability function pA such that, for each?
?
?, pA(?)
= p?G?
(pi), where pi is the rule in R?that has been constructed from ?
as specified above.It is easy to see that PPDT (A, pA) is now proper.Furthermore, for each complete computation c ofAwe havepA(c) = p?G?(f?
(c)), (4)and so (A, pA) is also consistent.
By combiningequations (1) to (4) we conclude that, for each com-plete computation c of A, pA(c) = p?G?
(f ?
(c)) =pG?
(f ?
(c)) = p?A(c) = pG(f(c)).
Thus our parsingstrategy S can be probabilistically extended.Note that the construction in the proof above canbe effectively computed (see discussion in Section 4for effective computation of normalized PCFGs).The definition of p?A in the proof of Theorem 5relies on the strings output by A.
This is the mainreason why we needed to consider PDTs ratherthan PDAs.
Now assume an appropriate probabil-ity function pA has been computed, such that thesource PCFG and (A, pA) define equivalent dis-tributions on derivations/computations.
Then theprobabilities assigned to strings over the input al-phabet are also equal.
We may subsequently ignorethe output strings if the application at hand merelyrequires probabilistic recognition rather than proba-bilistic transduction, or in other words, we may sim-plify PDTs to PDAs.The proof of Theorem 5 also leads to the obser-vation that parsing strategies with the CPP and theSPP as well as their probabilistic extensions can bedescribed as grammar transformations, as follows.A given (P)CFG is mapped to an equivalent (P)PDTby a (probabilistic) parsing strategy.
By ignoringthe output components of swap transitions we ob-tain a (P)PDA, which can be mapped to an equiva-lent (P)CFG as shown above.
This observation givesrise to an extension with probabilities of the work oncovers by (Nijholt, 1980; Leermakers, 1989).6 ApplicationsMany well-known parsing strategies with the CPPalso have the SPP.
This is for instance the casefor top-down parsing and left-corner parsing.
Asdiscussed in the introduction, it has already beenshown that for any PCFG G, there are equiva-lent PPDTs implementing these strategies, as re-ported in (Abney et al, 1999) and (Tendeau, 1995),respectively.
Those results more simply follownow from our general characterization.
Further-more, PLR parsing (Soisalon-Soininen and Ukko-nen, 1979; Nederhof, 1994) can be expressed in ourframework as a parsing strategy with the CPP andthe SPP, and thus we obtain as a new result that thisstrategy allows probabilistic extension.The above strategies are in contrast to the LRparsing strategy, which has the CPP but lacks theSPP, and therefore falls outside our sufficient condi-tion.
As we have already seen in the introduction, itturns out that LR parsing cannot be extended to be-come a probabilistic parsing strategy.
Related to LRparsing is ELR parsing (Purdom and Brown, 1981;Nederhof, 1994), which also lacks the SPP.
By anargument similar to the one provided for LR, we canshow that also ELR parsing cannot be extended tobecome a probabilistic parsing strategy.
(See (Ten-deau, 1997) for earlier observations related to this.
)These two cases might suggest that the sufficientcondition in Theorem 5 is tight in practice.Decidability of the CPP and the SPP obviouslydepends on how a parsing strategy is specified.
Asfar as we know, in all practical cases of parsingstrategies these properties can be easily decided.Also, observe that our results do not depend on thegeneral behaviour of a parsing strategy S, but juston its ?point-wise?
behaviour on each input CFG.Specifically, if S does not have the CPP and theSPP, but for some fixed CFG G of interest we ob-tain a PDT A that has the CPP and the SPP, thenwe can still apply the construction in Theorem 5.In this way, any probability function pG associatedwith G can be converted into a probability functionpA, such that the resulting PCFG and PPDT induceequivalent distributions.
We point out that decid-ability of the CPP and the SPP for a fixed PDT canbe efficiently decided using dynamic programming.One more consequence of our results is this.
Asdiscussed in the introduction, the properness condi-tion reduces the number of parameters of a PPDT.However, our results show that if the PPDT has theCPP and the SPP then the properness assumption isnot restrictive, i.e., by lifting properness we do notgain new distributions with respect to those inducedby the underlying PCFG.7 ConclusionsWe have formalized the notion of CFG parsing strat-egy as a mapping from CFGs to PDTs, and have in-vestigated the extension to probabilities.
We haveshown that the question of which parsing strategiescan be extended to become probabilistic heavily re-lies on two properties, the correct-prefix propertyand the strong predictiveness property.
As far as weknow, this is the first general characterization thathas been provided in the literature for probabilisticextension of CFG parsing strategies.
We have alsoshown that there is at least one strategy of practicalinterest with the CPP but without the SPP, namelyLR parsing, that cannot be extended to become aprobabilistic parsing strategy.AcknowledgementsThe first author is supported by the PIO-NIER Project Algorithms for Linguistic Process-ing, funded by NWO (Dutch Organization forScientific Research).
The second author is par-tially supported by MIUR under project PRIN No.2003091149 005.ReferencesS.
Abney, D. McAllester, and F. Pereira.
1999.
Re-lating probabilistic grammars and automata.
In37th Annual Meeting of the Association for Com-putational Linguistics, Proceedings of the Con-ference, pages 542?549, Maryland, USA, June.A.V.
Aho and J.D.
Ullman.
1972.
Parsing, vol-ume 1 of The Theory of Parsing, Translation andCompiling.
Prentice-Hall.S.
Billot and B. Lang.
1989.
The structure ofshared forests in ambiguous parsing.
In 27thAnnual Meeting of the Association for Com-putational Linguistics, Proceedings of the Con-ference, pages 143?151, Vancouver, BritishColumbia, Canada, June.T.L.
Booth and R.A. Thompson.
1973.
Apply-ing probabilistic measures to abstract languages.IEEE Transactions on Computers, C-22(5):442?450, May.T.
Briscoe and J. Carroll.
1993.
Generalized prob-abilistic LR parsing of natural language (cor-pora) with unification-based grammars.
Compu-tational Linguistics, 19(1):25?59.E.
Charniak and G. Carroll.
1994.
Context-sensitive statistics for improved grammatical lan-guage models.
In Proceedings Twelfth NationalConference on Artificial Intelligence, volume 1,pages 728?733, Seattle, Washington.Z.
Chi and S. Geman.
1998.
Estimation of prob-abilistic context-free grammars.
ComputationalLinguistics, 24(2):299?305.Z.
Chi.
1999.
Statistical properties of probabilisticcontext-free grammars.
Computational Linguis-tics, 25(1):131?160.M.V.
Chitrao and R. Grishman.
1990.
Statisticalparsing of messages.
In Speech and Natural Lan-guage, Proceedings, pages 263?266, Hidden Val-ley, Pennsylvania, June.M.A.
Harrison.
1978.
Introduction to Formal Lan-guage Theory.
Addison-Wesley.K.
Inui, V. Sornlertlamvanich, H. Tanaka, andT.
Tokunaga.
2000.
Probabilistic GLR parsing.In H. Bunt and A. Nijholt, editors, Advancesin Probabilistic and other Parsing Technologies,chapter 5, pages 85?104.
Kluwer Academic Pub-lishers.B.
Lang.
1974.
Deterministic techniques for ef-ficient non-deterministic parsers.
In Automata,Languages and Programming, 2nd Colloquium,volume 14 of Lecture Notes in Computer Science,pages 255?269, Saarbru?cken.
Springer-Verlag.R.
Leermakers.
1989.
How to cover a grammar.In 27th Annual Meeting of the Association forComputational Linguistics, Proceedings of theConference, pages 135?142, Vancouver, BritishColumbia, Canada, June.C.D.
Manning and B. Carpenter.
2000.
Proba-bilistic parsing using left corner language mod-els.
In H. Bunt and A. Nijholt, editors, Ad-vances in Probabilistic and other Parsing Tech-nologies, chapter 6, pages 105?124.
Kluwer Aca-demic Publishers.M.-J.
Nederhof and G. Satta.
2003.
Probabilis-tic parsing as intersection.
In 8th InternationalWorkshop on Parsing Technologies, pages 137?148, LORIA, Nancy, France, April.M.-J.
Nederhof.
1994.
An optimal tabular parsingalgorithm.
In 32nd Annual Meeting of the Associ-ation for Computational Linguistics, Proceedingsof the Conference, pages 117?124, Las Cruces,New Mexico, USA, June.A.
Nijholt.
1980.
Context-Free Grammars: Cov-ers, Normal Forms, and Parsing, volume 93 ofLecture Notes in Computer Science.
Springer-Verlag.P.W.
Purdom, Jr. and C.A.
Brown.
1981.
Pars-ing extended LR(k) grammars.
Acta Informatica,15:115?127.B.
Roark and M. Johnson.
1999.
Efficient proba-bilistic top-down and left-corner parsing.
In 37thAnnual Meeting of the Association for Compu-tational Linguistics, Proceedings of the Confer-ence, pages 421?428, Maryland, USA, June.D.J.
Rosenkrantz and P.M. Lewis II.
1970.
Deter-ministic left corner parsing.
In IEEE ConferenceRecord of the 11th Annual Symposium on Switch-ing and Automata Theory, pages 139?152.J.-A.
Sa?nchez and J.-M.
Bened??.
1997.
Consis-tency of stochastic context-free grammars fromprobabilistic estimation based on growth trans-formations.
IEEE Transactions on Pattern Anal-ysis and Machine Intelligence, 19(9):1052?1055,September.E.S.
Santos.
1972.
Probabilistic grammars and au-tomata.
Information and Control, 21:27?47.S.
Sippu and E. Soisalon-Soininen.
1990.
ParsingTheory, Vol.
II: LR(k) and LL(k) Parsing, vol-ume 20 of EATCS Monographs on TheoreticalComputer Science.
Springer-Verlag.E.
Soisalon-Soininen and E. Ukkonen.
1979.
Amethod for transforming grammars into LL(k)form.
Acta Informatica, 12:339?369.V.
Sornlertlamvanich, K. Inui, H. Tanaka, T. Toku-naga, and T. Takezawa.
1999.
Empirical sup-port for new probabilistic generalized LR pars-ing.
Journal of Natural Language Processing,6(3):3?22.A.
Stolcke.
1995.
An efficient probabilisticcontext-free parsing algorithm that computesprefix probabilities.
Computational Linguistics,21(2):167?201.F.
Tendeau.
1995.
Stochastic parse-tree recognitionby a pushdown automaton.
In Fourth Interna-tional Workshop on Parsing Technologies, pages234?249, Prague and Karlovy Vary, Czech Re-public, September.F.
Tendeau.
1997.
Analyse syntaxique etse?mantique avec e?valuation d?attributs dansun demi-anneau.
Ph.D. thesis, University ofOrle?ans.J.H.
Wright and E.N.
Wrigley.
1991.
GLR pars-ing with probability.
In M. Tomita, editor, Gen-eralized LR Parsing, chapter 8, pages 113?128.Kluwer Academic Publishers.
