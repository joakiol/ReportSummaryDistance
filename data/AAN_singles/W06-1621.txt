Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 172?179,Sydney, July 2006. c?2006 Association for Computational LinguisticsLexical Reference: a Semantic Matching SubtaskOren Glickman and Eyal Shnarch and Ido DaganComputer Science DepartmentBar Ilan UniversityRamat Gan, Israel{glikmao, dagan}@cs.biu.ac.ilAbstractSemantic lexical matching is a prominentsubtask within text understanding applica-tions.
Yet, it is rarely evaluated in a di-rect manner.
This paper proposes a def-inition for lexical reference which cap-tures the common goals of lexical match-ing.
Based on this definition we createdand analyzed a test dataset that was uti-lized to directly evaluate, compare and im-prove lexical matching models.
We sug-gest that such decomposition of the globalsemantic matching task is critical in orderto fully understand and improve individualcomponents.1 IntroductionA fundamental task for text understanding ap-plications is to identify semantically equivalentpieces of text.
For example, Question Answer-ing (QA) systems need to match correspondingparts in the question and in the answer passage,even though such parts may be expressed in dif-ferent terms.
Summarization systems need to rec-ognize (redundant) semantically matching partsin multiple sentences that are phrased differently.Other applications, such as information extractionand retrieval, face pretty much the same seman-tic matching task.
The degree of semantic match-ing found is typically factored into systems?
scor-ing and ranking mechanisms.
The recently pro-posed framework of textual entailment (Dagan etal., 2006) attempts to formulate the generic seman-tic matching problem in an application indepen-dent manner.The most commonly implemented semanticmatching component addresses the lexical level.At this level the goal is to identify whether themeaning of a lexical item of one text is expressedalso within the other text.
Typically, lexical match-ing models measure the degree of literal lexicaloverlap, augmented with lexical substitution cri-teria based on resources such as Wordnet or theoutput of statistical similarity methods (see Sec-tion 2).
Many systems apply semantic matchingonly at the lexical level, which is used to approx-imate the overall degree of semantic matching be-tween texts.
Other systems incorporate lexicalmatching as a component within more complexmodels that examine matching at higher syntacticand semantic levels.While lexical matching models are so promi-nent within semantic systems they are rarely eval-uated in a direct manner.
Typically, improve-ments to a lexical matching model are evaluated bytheir marginal contribution to overall system per-formance.
Yet, such global and indirect evaluationdoes not indicate the absolute performance of themodel relative to the sheer lexical matching taskfor which it was designed.
Furthermore, the indi-rect application-dependent evaluation mode doesnot facilitate improving lexical matching modelsin an application dependent manner, and does notallow proper comparison of such models whichwere developed (and evaluated) by different re-searchers within different systems.This paper proposes a generic definition for thelexical matching task, which we term lexical ref-erence.
This definition is application indepen-dent and enables annotating test datasets that eval-uate directly lexical matching models.
Conse-quently, we created a dataset annotated for lexicalreference, using a sample of sentence pairs (text-hypothesis) from the 1st Recognising Textual En-tailment dataset.
Further analysis identified sev-172eral sub-types of lexical reference, pointing at themany interesting cases where lexical reference isderived from a complete context rather than froma particular matching lexical item.Next, we used the lexical reference dataset toevaluate and compare several state-of-the-art ap-proaches for lexical matching.
Having a directevaluation task enabled us to capture the actualperformance level of these models, to reveal theirrelative strengths and weaknesses, and even toconstruct a simple combination of two models thatoutperforms all the original ones.
Overall, we sug-gest that it is essential to decompose global se-mantic matching and textual entailment tasks intoproper subtasks, like lexical reference.
Such de-composition is needed in order to fully understandthe behavior of individual system components andto guide their future improvements.2 Background2.1 Term MatchingThesaurus-based term expansion is a commonlyused technique for enhancing the recall of NLPsystems and coping with lexical variability.
Ex-pansion consists of altering a given text (usu-ally a query) by adding terms of similar meaning.WordNet is commonly used as a source of relatedwords for expansion.
For example, many QA sys-tems perform expansion in the retrieval phase us-ing query related words based on WordNet?s lexi-cal relations such as synonymy or hyponymy (e.g(Harabagiu et al, 2000; Hovy et al, 2001)).
Lex-ical similarity measures (e.g.
(Lin, 1998)) havealso been suggested to measure semantic similar-ity.
They are based on the distributional hypothe-sis, suggesting that words that occur within similarcontexts are semantically similar.2.2 Textual EntailmentThe Recognising Textual Entailment (RTE-1) chal-lenge (Dagan et al, 2006) is an attempt to promotean abstract generic task that captures major seman-tic inference needs across applications.
The taskrequires to recognize, given two text fragments,whether the meaning of one text can be inferred(entailed) from another text.
Different techniquesand heuristics were applied on the RTE-1 datasetto specifically model textual entailment.
Interest-ingly, a number of works (e.g.
(Bos and Mark-ert, 2005; Corley and Mihalcea, 2005; Jijkoun andde Rijke, 2005; Glickman et al, 2006)) applied orutilized lexical based word overlap measures.
Var-ious word-to-word similarity measures where ap-plied, including distributional similarity (such as(Lin, 1998)), web-based co-occurrence statisticsand WordNet based similarity measures (such as(Leacock et al, 1998)).2.3 Paraphrase AcquisitionA substantial body of work has been dedicated tolearning patterns of semantic equivalency betweendifferent language expressions, typically consid-ered as paraphrases.
Recently, several works ad-dressed the task of acquiring paraphrases (semi-)automatically from corpora.
Most attempts werebased on identifying corresponding sentences inparallel or ?comparable?
corpora, where each cor-pus is known to include texts that largely corre-spond to texts in another corpus (e.g.
(Barzilayand McKeown, 2001)).
Distributional Similaritywas also used to identify paraphrase patterns froma single corpus rather than from a comparableset of corpora (Lin and Pantel, 2001).
Similarly,(Glickman and Dagan, 2004) developed statisticalmethods that match verb paraphrases within a reg-ular corpus.3 The Lexical Reference Dataset3.1 Motivation and DefinitionOne of the major observations of the 1st Recog-nizing Textual Entailment (RTE-1) challenge re-ferred to the rich structure of entailment modelingsystems and the need to evaluate and optimize in-dividual components within them.
When buildingsuch a compound system it is valuable to test eachcomponent directly during its development, ratherthan indirectly evaluating the component?s perfor-mance via the behavior of the entire system.
Ifgiven tools to evaluate each component indepen-dently researchers can target and perfect the per-formance of the subcomponents without the needof building and evaluating the entire end-to-endsystem.A common subtask, addressed by practically allparticipating systems in RTE-1, was to recognizewhether each lexical meaning in the hypothesis isreferenced by some meaning in the correspondingtext.
We suggest that this common goal can becaptured through the following definition:Definition 1 A word w is lexically referenced bya text t if there is an explicit or implied reference173from a set of words in t to a possible meaning ofw.Lexical reference may be viewed as a natural ex-tension of textual entailment for sub-sentential hy-potheses such as words.
In this work we fo-cus on words meanings, however this work canbe directly generalized to word compounds andphrases.
A concrete version of detailed annotationguidelines for lexical reference is presented in thenext section.1 Lexical Reference is, in some sense,a more general notion than paraphrases.
If the textincludes a paraphrase for w then naturally it doesrefer to w?s meaning.
However, a text need notinclude a paraphrase for the concrete meaning ofthe referenced word w, but only an implied refer-ence.
Accordingly, the referring part might be alarge segment of the text, which captures informa-tion different than w?s meaning, but still implies areference to w as part of the text?s meaning.It is typically a necessary, but not sufficient,condition for textual entailment that the lexicalconcepts in a hypothesis h are referred in a giventext t. For example, in order to infer from a textthe hypothesis ?a dog bit a man,?
it is a neces-sary that the concepts of dog, bite and man mustbe referenced by the text, either directly or in animplied manner.
However, for proper entailmentit is further needed that the right relations wouldhold between these concepts2.
Therefore lexicalentailment should typically be a component withina more complex entailment modeling (or semanticmatching) system.3.2 Dataset Creation and Annotation ProcessWe created a lexical reference dataset derivedfrom the RTE-1 development set by randomlychoosing 400 out of the 567 text-hypothesis exam-ples.
We then created sentence-word examples forall content words in the hypotheses which do notappear in the corresponding sentence and are nota morphological derivation of a word in it (since asimple morphologic module could easily identifythese cases).
This resulted in a total of 708 lexi-cal reference examples.
Two annotators annotatedthese examples as described in the next section.1These terms should not be confused with the use of lex-ical entailment in WordNet, which is used to describe an en-tailment relationship between verb lexical types, nor with therelated notion of reference in classical linguistics, generallydescribing the relation between nouns or pronouns and ob-jects that are named by them (Frege, 1892)2or quoting the known journalism saying ?
?Dog bitesman?
isn?t news, but ?Man bites dog?
is.Taking the same approach as of the RTE-1 datasetcreation (Dagan et al, 2006), we limited our ex-periments to the resulting 580 examples that thetwo annotators agreed upon3.3.2.1 Annotation guidelinesWe asked two annotators to annotate thesentence-word examples according to the follow-ing guidelines.
Given a sentence and a target wordthe annotators were asked to decide whether thetarget word is referred by the sentence (true) ornot (false).
Annotators were guided to mark thepair as true in the following cases:Word: if there is a word in the sentence which,in the context of the sentence, implies a meaningof the target word (e.g.
a synonym or hyponym),or which implies a reference to the target word?smeaning (e.g.
blind?see, sight).
See examples 1-2 in Table 1 where the word that implies the refer-ence is emphasized in the text.
Note that in exam-ple 2 murder is not a synonym of died nor does itshare the same meaning of died; however it is clearfrom its presence in the sentence that it refers to adeath.
Also note that in example 8 although homeis a possible synonym for house, in the context ofthe text it does not appear in that meaning and theexample should be annotated as false.Phrase: if there is a multi-word independent ex-pression in the sentence that implies the target (im-plication in the same sense that a Word does).
Seeexamples 3-4 in Table 1.Context: if there is a clear reference to the mean-ing of the target word by the overall meaning ofsome part(s) of the sentence (possibly all the sen-tence), though it is not referenced by any singleword or phrase.
The reference is derived from thecomplete context of the relevant sentence part.
Seeexamples 5-7 in Table 1.If there is no reference from the sentence tothe target word the annotators were instructed tochoose false.
In example 9 in Table 1 the targetword ?HIV-positive?
should be considered as oneword that cannot be broken down from its unit andalthough both the general term ?HIV status?
andthe more specific term ?HIV negative?
are referredto, the target word cannot be understood or derivedfrom the text.
In example 10 although the year1945 may refer to a specific war, there is no ?war?either specifically or generally understood by thetext.3dataset avaiable at http://ir-srv.cs.biu.ac.il:64080/emnlp06_dataset.zip174ID TEXT TARGET VALUE1 Oracle had fought to keep the forms from being released.
document word2 The court found two men guilty of murdering Shapour Bakhtiar.
died word3 The new information prompted them to call off the search.
cancelled phrase4 Milan, home of the famed La Scala opera house,.
.
.
located phrase5 Successful plaintiffs recovered punitive damages in Texas discrimination cases 53 legal context6 Recreational marijuana smokers are no more likely to develop oral cancer than nonusers.
risk context7 A bus ticket cost nowadays 5.2 NIS whereas last year it cost 4.9. increase context8 Pakistani officials announced that two South African men in their custody had confessed toplanning attacks at popular tourist spots in their home country.house false9 For women who are HIV negative or who do not know their HIV status, breastfeeding shouldbe promoted for six months.HIV-positive false10 On Feb. 1, 1945, the Polish government made Warsaw its capital, and an office for urbanreconstruction was set up.war falseTable 1: Lexical Reference Annotation Examples3.2.2 Annotation resultsWemeasured the agreement on the lexical refer-ence binary task (in which Word, Phrase and Con-text are conflated to true).
The resulting kappastatistic of 0.63 is regarded as substantial agree-ment (Landis and Koch, 1997).
The resultingdataset is not balanced in terms of true and falseexamples and a straw-baseline for accuracy is0.61, representing a system which predicts all ex-amples as true.3.3 Dataset AnalysisIn a similar manner to (Bar-Haim et al, 2005; Van-derwende et al, 2005) we investigated the rela-tionship between lexical reference and textual en-tailment.
We checked the performance of a textualentailment system which relies solely on an ideallexical reference component which makes no mis-takes and asserts that a hypothesis is entailed froma text if and only if all content words in the hypoth-esis are referred in the text.
Based on the lexicalreference dataset annotations, such an ?ideal?
sys-tem would obtain an accuracy of 74% on the cor-responding subset of the textual entailment task.The corresponding precision is 68% and a recallof 82%.
This is significantly higher than the re-sults of the best performing systems that partici-pated in the challenge on the RTE-1 test set.
Thissuggests that lexical reference is a valuable sub-task for entailment.
Interestingly, a similar entail-ment system based on a lexical reference compo-nent which doesn?t account for the contextual lex-ical reference (i.e.
all Context annotations are re-garded as false) would achieve an accuracy of only63% with 41% precision and a recall of 63%.
Thissuggests that lexical reference in general and con-textual entailment in particular, play an important(though not sufficient) role in entailment recogni-tion.Further, we wanted to investigate the validityof the assumption that for entailment relationshipto hold all content words in the hypothesis mustbe referred by the text.
We examined the exam-ples in our dataset which were derived from text-hypothesis pairs that were annotated as true (en-tailing) in the RTE dataset.
Out of 257 such exam-ples only 34 were annotated as false by both anno-tators.
Table 2 lists a few such examples in whichentailment at whole holds, however, there exists aword in the hypothesis (highlighted in the table)which is not lexically referenced by the text.
Inmany cases, the target word was part of a non com-positional compound in the hypothesis, and there-fore should not be expected to be referenced bythe text (see examples 1-2).
This finding indicatesthat the basic assumption is a reasonable approxi-mation for entailment.
We could not have revealedthis fact without the dataset for the subtask of lex-ical reference.4 Lexical Reference ModelsThe lexical reference dataset facilitates qualita-tive and quantitative comparison of various lexicalmodels.
This section describes four state-of-the-art models that can be applied to the lexical refer-ence task.
The performance of these models wastested and analyzed, as described in the next sec-tion, using the lexical reference dataset.
All mod-els assign a [0, 1] score to a given pair of text tand target word u which can be interpreted as theconfidence that u is lexically referenced in t.175ID TEXT HYPOTHESIS ENTAIL-MENTREFER-ENCE1 Iran is said to give up al Qaeda members.
Iran hands over al Qaeda members.
true false2 It would help the economy by putting peopleback to work and more money in the hands ofconsumers.More money in the hands of consumersmeans more money can be spent to get theeconomy going.true false3 The Securities and Exchange Commission?snew rule to beef up the independence of mutualfund boards represents an industry defeat.The SEC?s new rule will give boards inde-pendence.true false4 Texas Data Recovery is also successful at re-trieving lost data from notebooks and laptops,regardless of age, make or model.In the event of a disaster you could use TexasData Recovery and you will have the capabil-ity to restore lost data.true falseTable 2: examples demonstrating cases when lexical entailment does not correlate with entailment.
Tar-get word is shown in bold.4.1 WordNetFollowing the common practice in NLP applica-tions (see Section 2.1) we evaluated the perfor-mance of a straight-forward utilization of Word-Net?s lexical information.
Our wordnet model firstlemmatizes the text and target word.
It then as-signs a score of 1 if the text contains a synonym,hyponym or derived form of the target word and ascore of 0 otherwise.4.2 SimilarityAs a second measure we used the distributionalsimilarity measure of (Lin, 1998).
For a text t anda word u we assign the max similarity score as fol-lows:similarity(t, u) = maxv?tsim(u, v) (1)where sim(u, v) is the similarity score for u andv4.4.3 Alignment model(Glickman et al, 2006) was among the top scor-ing systems on the RTE-1 challenge and supplies aprobabilistically motivated lexical measure basedon word co-occurrence statistics.
It is defined fora text t and a word u as follows:align(t, u) = maxv?tP(u|v) (2)where P(u|v) is simply the co-occurrence proba-bility ?
the probability that a sentence containing valso contains u.
The co-occurrence statistics werecollected from the Reuters Corpus Volume 1.4the scores were obtained from the following online re-source: http://www.cs.ualberta.ca/?lindek/downloads.htm4.4 Baysean model(Glickman et al, 2005) provide a contextual mea-sure which takes into account the whole contextof the text rather than from a single word in thetext as do the previous models.
This model isthe only model which addresses contextual refer-ence rather than just word-to-word matching.
Themodel is based on a Na?
?ve Bayes text classificationapproach in which corpus sentences serve as doc-uments and the class is the reference of the targetword u. Sentences containing the word u are usedas positive examples while all other sentences areconsidered as negative examples.
It is defined fora text t and a word u as follows:bayes(t, u) =P(u)?v?t P(v|u)n(v,t)P(?u)?v?t P(v|?u)n(v,t)+P(u)?v?t P(v|u)n(v,t)(3)where n(w, t) is the number of times word w ap-pears in t, P(u) is the probability that a sentencecontains the word u andP(v|?u) is the probabilitythat a sentence NOT containing u contains v. Inorder to reduce data size and to account for zeroprobabilities we applied smoothing and informa-tion gain based feature selection on the data priorto running the model.
The co-occurrence prob-abilities were collected from sentences from theReuters corpus in a similar manner to the align-ment model.4.5 Combined ModelThe WordNet and Bayesian models are derivedfrom quite different motivations.
One would ex-pect the WordNet model to be better in identify-ing the word-to-word explicit reference exampleswhile the Bayesian model is expected to model thecontextualy implied references.
For this reason wetried to combine forces by evaluating a na?
?ve linear176interpolation of the two models (by simply averag-ing the score of the two models).
This model havenot been previously suggested and to the best ofour knowledge this type of combination is novel.5 Empirical Evaluation and Analysis5.1 ResultsIn order to evaluate the scores produced by thevarious models as a potential component in an en-tailment system we compared the recall-precisiongraphs.
In addition we compared the average pre-cision which is a single number measure equiv-alent to the area under an uninterpolated recall-precision curve and is commonly used to evaluatea systems ranking ability (Voorhees and Harman,1999).
On our dataset an average precision greaterthan 0.65 is better than chance at the 0.05 leveland an average precision greater than 0.66 is sig-nificant at the 0.01 level.Figure 1 compares the average precision andrecall-precision results for the various models.
Ascan be seen, the combined wordnet+bayes modelperforms best.
In terms of average precision,the similarity and wordnet models are comparableand are slightly better than bayes.
The alignmentmodel, however, is not significantly better thanrandom guessing.
The recall-precision figure indi-cates that the baysian model succeeds to rank quitewell both within the the positively scored wordnetexamples and within the negatively scored word-net examples and thus resulting in improved av-erage precision of the combined model.
A betterunderstanding of the systems?
performance is evi-dent from the following analysis.5.2 AnalysisTable 3 lists a few examples from the lexical refer-ence dataset alng with their gold-standard anno-tation and the Bayesian model score.
Manual in-spection of the data shows that the Bayesian modelcommonly assigns a low score to correct exampleswhich have an entailing trigger word or phrase inthe sentence but yet the context of the sentence as awhole is not typical for the target hypothesized en-tailed word.
For example, in example 5 the entail-ing phrase ?set in place?
and in example 6 the en-tailing word ?founder?
do appear in the text how-ever the contexts of the sentences are not typicalnews domain contexts of issued or founded.
An in-teresting future work would be to change the gen-erative story and model to account for such cases.The WordNet model identified a matching wordin the text for 99 out of the 580 examples.
Thiscorresponds to a somewhat low recall of 25% anda quite high precision of 90%.
Table 4 lists typicalmistakes of the wordnet model.
Examples 1-3 arefalse positive examples in which there is a wordin the text (emphasized in the table) which is asynonym or hyponym of the target word for somesense in WordNet, however in the context of thetext it is not of such a sense.
Examples 4-6 showfalse negative examples, in which the annotatorsidentified a trigger word in the text (emphasizedin the table) but yet it or no other word in the textis a synonym or hyponym of the target word.5.3 Subcategory analysisword phrase context falseword 178 16 59 32phrase 4 12 9 4context 15 5 56 25false 24 5 38 226Table 5: inter-annotator confusion matrix for theauxiliary annotation.As seen above, the combined model outper-forms the others since it identifies both word-to-word lexical reference as well as context-to-word lexical reference.
These are quite differentcases.
We asked the annotators to state the sub-category when they annotated an example as true(as described in the annotation guidelines in Sec-tion 3.2.1).
The Word subcategory correspondsto a word-to-word match and Phrase and Contextsubcategories correspond to more than one wordto word match.
As can be expected, the agreementon such a task resulted in a lower Kappa of 0.5which corresponds to moderate agreement (Landisand Koch, 1997).
the confusion matrix betweenthe two annotators is presented in Table 5.
This de-composition enables the evaluation of the strengthand weakness of different lexical reference mod-ules, free from the context of the bigger entailmentsystem.We used the subcategories dataset to test theperformances of the different models.
Table 6lists for each subcategory the recall of correctlyidentified examples for each model?s 25% recalllevel.
The table shows that the wordnet and simi-larity models?
strength is in identifying exampleswhere lexical reference is triggered by a dominantword in the sentence.
The bayes model, however,177Figure 1: comparison of average precision (left) and recall-precision (right) results for the various modelsid text token annotation score1 QNX Software Systems Ltd., a leading provider of real-time software and ser-vices to the embedded computing market, is pleased to announce the appoint-ment of Mr. Sachin Lawande to the position of vice president, engineering ser-vices.named PHRASE 0.982 NIH?s FY05 budget request of $28.8 billion includes $2 billion for the NationalInstitute of General Medical Sciences, a 3.4-percent increase, and $1.1 billionfor the National Center for Research Resources, and a 7.2-percent decrease fromFY04 levels.reduced WORD 0.913 Pakistani officials announced that two South African men in their custody hadconfessed to planning attacks at popular tourist spots in their home country.security CONTEXT 0.804 With $549 million in cash as of June 30, Google can easily afford to makeamends.shares FALSE 0.035 In the year 538, Cyrus set in place a policy which demanded the return of thevarious gods to their proper places.issued PHRASE 7e-46 The black Muslim activist said that he had relieved Muhammad of his duties?until he demonstrates that he is willing to conform to the manner of representingAllah and the honorable Elijah Muhammad (founder of the Nation of Islam)?.founded WORD 3e-6Table 3: A sample from the lexical reference dataset alng with the Bayesian model?s scoreid text token annotation1 Kerry hit Bush hard on his conduct on the war in Iraq shot FALSE2 Pakistani officials announced that two South African men in their custody had confessed toplanning attacks at popular tourist spots in their home countryforces FALSE3 It would help the economy by putting people back to work and more money in the hands ofconsumersget FALSE4 Eating lots of foods that are a good source of fiber may keep your blood glucose from risingtoo fast after you eatsugar WORD5 Hippos do come into conflict with people quite often human WORD6 Weinstock painstakingly reviewed dozens of studies for evidence of any link between sun-screen use and either an increase or decrease in melanomacancer WORDTable 4: A few erroneous examples of WordNet modelis better at identifying phrase and context exam-ples.
The combined WordNet and Bayesian mod-els?
strength can be explained by the quite dif-ferent behaviors of the two models - the Word-Net model seems to be better in identifying theword-to-word explicit reference examples whilethe Bayesian model is better in modeling the con-textual implied references.6 ConclusionsThis paper proposed an explicit task definition forlexical reference.
This task captures directly thegoal of common lexical matching models, whichtypically operate within more complex systems178method word disagreement phrase/contextwordnet 38% 9% 17%similarity 39% 7% 17%bayes 22% 21% 37%Table 6: Breakdown of recall of correctly identi-fied example types at an overall system?s recall of25%.
Disagreement refers to examples for whichthe annotators did not agree on the subcategory an-notation (word vs. phrase/context).that address more complex tasks.
This defini-tion enabled us to create an annotated dataset forthe lexical reference task, which provided insightsinto interesting sub-classes that require differenttypes of modeling.
The dataset enabled us tomake a direct evaluation and comparison of lexicalmatching models, reveal insightful differences be-tween them, and create a simple improved modelcombination.
In the long run, we believe thatthe availability of such datasets will facilitate im-proved models that consider the various sub-casesof lexical reference, as well as applying supervisedlearning to optimize model combination and per-formance.References[Bar-Haim et al2005] Roy Bar-Haim, Idan Szpecktor,and Oren Glickman.
2005.
Definition and analysisof intermediate entailment levels.
In Proceedingsof the ACL Workshop on Empirical Modeling of Se-mantic Equivalence and Entailment, pages 55?60,Ann Arbor, Michigan, June.
Association for Com-putational Linguistics.
[Barzilay and McKeown2001] Regina Barzilay andKathleen McKeown.
2001.
Extracting paraphrasesfrom a parallel corpus.
In ACL, pages 50?57.
[Bos and Markert2005] Johan Bos and Katja Markert.2005.
Recognising textual entailment with logicalinference techniques.
In EMNLP.
[Corley and Mihalcea2005] Courtney Corley and RadaMihalcea.
2005.
Measuring the semantic similarityof texts.
In Proceedings of the ACL Workshop onEmpirical Modeling of Semantic Equivalence andEntailment, pages 13?18.
[Dagan et al2006] Ido Dagan, Oren Glickman, andBernardo Magnini, editors.
2006.
The PASCALRecognising Textual Entailment Challenge, volume3944.
Lecture Notes in Computer Science.
[Frege1892] Gottlob Frege.
1892.
On sense andreference.
Reprinted in P. Geach and M.
Black,eds., Translations from the Philosophical Writingsof Gottlob Frege.
1960.
[Glickman and Dagan2004] Oren Glickman and IdoDagan, 2004.
Recent Advances in Natural Lan-guage Processing III, chapter Acquiring lexicalparaphrases from a single corpus, pages 81?90.John Benjamins.
[Glickman et al2005] Oren Glickman, Ido Dagan, andMoshe Koppel.
2005.
A probabilistic classificationapproach for lexical textual entailment.
In AAAI,pages 1050?1055.
[Glickman et al2006] Oren Glickman, Ido Dagan, andMoshe Koppel.
2006.
A lexical alignment modelfor probabilistic textual entailment, volume 3944.In Lecture Notes in Computer Science, pages 287 ?298.
Springer.
[Harabagiu et al2000] Sanda M. Harabagiu, Dan I.Moldovan, Marius Pasca, Rada Mihalcea, MihaiSurdeanu, Razvan C. Bunescu, Roxana Girju, VasileRus, and Paul Morarescu.
2000.
Falcon: Boostingknowledge for answer engines.
In TREC.
[Hovy et al2001] Eduard H. Hovy, Ulf Hermjakob, andChin-Yew Lin.
2001.
The use of external knowl-edge of factoid QA.
In Text REtrieval Conference.
[Jijkoun and de Rijke2005] Valentin Jijkoun andMaarten de Rijke.
2005.
Recognizing textualentailment using lexical similarity.
Proceedings ofthe PASCAL Challenges Workshop on RecognisingTextual Entailment (and forthcoming LNAI bookchapter).
[Landis and Koch1997] J. R. Landis and G. G. Koch.1997.
The measurements of observer agreement forcategorical data.
Biometrics, 33:159?174.
[Leacock et al1998] Claudia Leacock, George A.Miller, and Martin Chodorow.
1998.
Usingcorpus statistics and wordnet relations for senseidentification.
Comput.
Linguist., 24(1):147?165.
[Lin and Pantel2001] Dekang Lin and Patrik Pantel.2001.
Discovery of inference rules for question an-swering.
Natural Language Engineering, 4(7):343?360.
[Lin1998] Dekang Lin.
1998.
Automatic retrieval andclustering of similar words.
In Proceedings of the17th international conference on Computational lin-guistics, pages 768?774, Morristown, NJ, USA.
As-sociation for Computational Linguistics.
[Vanderwende et al2005] Lucy Vanderwende, DeborahCoughlin, and Bill Dolan.
2005.
What syntaxcan contribute in entailment task.
Proceedings ofthe PASCAL Challenges Workshop on RecognisingTextual Entailment.
[Voorhees and Harman1999] Ellen M. Voorhees andDonna Harman.
1999.
Overview of the seventh textretrieval conference.
In Proceedings of the SeventhText REtrieval Conference (TREC-7).
NIST SpecialPublication.179
