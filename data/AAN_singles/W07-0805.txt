Proceedings of the 5th Workshop on Important Unresolved Matters, pages 33?40,Prague, Czech Republic, June 2007. c?2007 Association for Computational LinguisticsSyllable-Based Speech Recognition for AmharicSolomon Teferra Abatesolomon_teferra_7@yahoo.comWolfgang Menzelmenzel@informatik.uni-hamburg.deUniformity of Hamburg, Department of Informatik Natural Language Systems GroupsVogt-K?lln-Strasse.
30, D-22527 Hamburg, GermanyAbstractAmharic  is  the  Semitic  language  that  has  thesecond  large  number  of  speakers  after  Arabic(Hayward and Richard 1999).
Its writing system issyllabic  with  Consonant-Vowel  (CV)  syllablestructure.
Amharic orthography has more or less aone  to  one correspondence with  syllabic  sounds.We have used this feature of Amharic to develop aCV syllable-based speech recognizer,  using  Hid-den  Markov  Modeling  (HMM),  and  achieved90.43% word recognition accuracy.1 IntroductionMost of the Semitic languages are technologicallyunfavored.
Amharic is one of these languages thatare looking for technological considerations of re-searchers and developers in the area of natural lan-guage  processing  (NLP).
Automatic  Speech  Re-cognition (ASR) is one of the major areas of NLPthat is understudied in Amharic.
Only few attempts(Solomon,  2001;  Kinfe,  2002;  Zegaye,  2003;Martha,  2003;  Hussien  and  Gamb?ck,  2005;Solomon et al, 2005; Solomon, 2006)  have beenmade.We have  developed an  ASR for  the  languageusing  CV  syllables  as  recognition  units.
In  thispaper  we  present  the  development  and  therecognition  performance  of  the  recognizerfollowing  a  brief  description  of  the  Amhariclanguage and speech recognition technology.2 The Amharic LanguageAmharic,  which belongs to the Semitic  languagefamily, is the official language of Ethiopia.
In thisfamily,  Amharic  stands  second in  its  number  ofspeakers  after  Arabic  (Hayward  and  Richard1999).
Amharic has five dialectical variations (Ad-dis  Ababa,  Gojjam,  Gonder,  Wollo,  and  Menz)spoken in different regions of the country (Cowley,et.al.
1976).
The  speech  of  Addis  Ababa  hasemerged as the standard dialect and has wide cur-rency  across  all  Amharic-speaking  communities(Hayward and Richard 1999).As with all of the other languages, Amharic hasits own characterizing phonetic, phonological andmorphological properties.
For example, it has a setof  speech sounds that  is  not  found in  other lan-guages.
For example the following sounds are notfound in English: [p`], [t?`], [s`], [t`], and [q].Amharic also has its  own inventory of  speechsounds.
It  has  thirty  one  consonants  and  sevenvowels.
The consonants are generally classified asstops, fricatives,  nasals,  liquids, and semi-vowels(Leslau 2000).
Tables 1 and 2 show the classifica-tion of Amharic consonants and vowels1.ManofArtVoicingPlace of ArticulationLab Den Pal Vel GlotStops Vs  [p] [t] [t?]
[k] [?
]Vd [b] [d] [d?]
[g]Glott [p`] [t`] [t?`] [q]Rd       [kw][gw][qw]Fric Vs [f] [s] [?]
[h]Vd   [z] [?
]Glott   [s`]Rd         [hw]Nas-alsVd [m] [n] [?
]Liq   Vd   [l][r]Sv Vd [w]     [j]Table 1: Amharic ConsonantsKey: Lab = Labials; Den = Dentals; Pal = Palat-als; Vel = Velars; Glot = Glottal; Vs = Voiceless;1International  Phonetic  Association's  (IPA)  standard  hasbeen used for representation.33Vd = Voiced; Rd = Rounded; Fric = Fricatives; Liq= Liquids; Sv = Semi-Vowels.Positions front center backhigh            [i]    [u]mid             [e]    [?]
[o]low [a]Table 2: Amharic VowelsAmharic is one of the languages that have theirown writing system, which is used across all Am-haric dialects.
Getachew (1967) stated that the Am-haric writing system is phonetic.
It allows any oneto write Amharic texts if s/he can speak Amharicand has knowledge of the Amharic alphabet.
Un-like most known languages, no one needs to learnhow  to  spell  Amharic  words.
In  support  of  theabove  point,  Leslaw  (1995)  noted  that  no  realproblems exist in Amharic orthography, as there ismore or less, a one-to-one correspondence betweenthe sounds and the graphic symbols, except for thegemination  of  consonants and  some  redundantsymbols.Many (Bender 1976; Cowley 1976; Baye 1986)have claimed the Amharic orthography as a syllab-ary for a relatively long period of time.
Recently,however, Taddesse (1994) and Baye (1997), whoapparently modified his view, have argued it is not.Both of these arguments are based on the specialfeature of the orthography; the possibility of rep-resenting  speech  using  either  isolated  phonemesymbols or concatenated symbols.In the concatenated feature, commonly known tomost of the population, each orthographic symbolrepresents a consonant and a vowel, except for thesixth order2, which is sometimes realized as a con-sonant without a vowel and at other times a con-sonant with a vowel.
This representation of concat-enated speech sounds by a single symbol has beenthe basis for the claim made of the writing system,as syllabary.Amharic  orthography  does  not  indicategemination,  but  since  there  are  relatively  few2An order in Amharic writing system is a combination of aconsonant with a vowel represented by a symbol.
A consonanthas therefore, 7 orders or different symbols that represent itscombination with 7 Amharic vowels.minimal pairs of geminations, Amharic readers donot find this to be a problem.
This property of thewriting  system  is  analogous  to  the  vowels  ofArabic  and  Hebrew,  which  are  not  normallyindicated in writing.The Amharic orthography, as represented in theAmharic  Character  set  -  also called [fid?lI]  con-sists of 276 distinct symbols.
In addition, there aretwenty numerals and eight  punctuation marks.
Asample  of  the  orthographic  symbols  is  given  inTable 3.?
u i a e oh ?
?
?
?
?
?
?l ?
?
?
?
?
?
?m ?
?
?
?
?
?
?r ?
?
?
?
?
?
?Table 3: Some Orthographic Symbols of AmharicHowever, research in speech recognition shouldonly consider distinct sounds instead of all the or-thographic symbols, unless there is a need to de-velop a dictation machine that includes all of theorthographic symbols.
Therefore, redundant ortho-graphic  symbols  that  represent  the  same syllabicsounds can be eliminated.
Thus, by eliminating re-dundant graphemes, we are left with a total of 233distinct  CV syllable  characters.
In  our  work,  anHMM model has been developed for each of theseCV syllables.3 HMM-Based Speech RecognitionThe  most  well  known  and  well  performing  ap-proach for speech recognition are Hidden MarkovModels (HMM).
An HMM can be classified on thebasis  of  the type of its  observation  distributions,the structure in its transition matrix and the numberof states.The observation distributions of HMMs can beeither discrete, or continuous.
In discrete HMMs,distributions are defined on finite spaces while incontinuous  HMMs,  distributions  are  defined  asprobability  densities  on  continuous  observationspaces,  usually  as  a  mixture  of  several  Gaussiandistributions.The model topology that is generally adopted forspeech recognition is a left-to-right or Bakis model34because the speech signal varies in time from leftto right (Deller, Proakis and Hansen 1993).An HMM is flexible in its size, type, or architec-ture to model words as well as any sub-word unit.3.1 Sub-word Units of Speech RecognitionLarge Vocabulary Automatic Speech RecognitionSystems (LVASRSs)  require modeling of  speechin smaller  units  than words  because the acousticsamples of most words will never be seen duringtraining,  and  therefore,  can  not  be  trained.Moreover,  in  LVASRSs  there  are  thousands  ofwords  and most  of  them occur very rarely,  con-sequently  training of  models  for  whole  words  isgenerally impractical.
That  is  why LVASRSs re-quire a segmentation of each word in the vocabu-lary into sub-word units that occur more frequentlyand can be trained more robustly than words.
Us-ing sub-word based models enables us to deal withwords  which have not  been seen during trainingsince they can just  be decomposed into the sub-word units.
As a word can be decomposed in sub-word units of different granularities, there is a needto choose the most suitable sub-word unit that fitsthe purpose of the system.Lee et al (1992) pointed out that there are twoalternatives  for  choosing  the  fundamental  sub-word units, namely acoustically-based and linguist-ically-based units .
The acoustic units are the labelsassigned  to  acoustic  segment  models,  which  aredefined on the basis of procuring a set of segmentmodels that spans the acoustic space determined bythe given, unlabeled training data.
The linguistic-ally-based  units  include  the  linguistic  units,  e.g.phones, demi-syllables, syllables and morphemes.It should be clear that there is no ideal (perfect)set  of  sub-word units.
Although phones  are verysmall in number and relatively easy to train, theyare much more sensitive to contextual influencesthan larger units.
The use of triphones, which mod-el both the right and left context of a phone, hasbecome the dominant  solution to  the problem ofthe context sensitivity of phones.Triphones  are  also  relatively  inefficient  sub-word units due to their large number.
Moreover,since a triphone unit spans a short time-interval, itis  not  suitable for the integration of spectral andtemporal dependencies.An other alternative is the syllable.
Syllables arelonger and less context sensitive than phones andcapable of exploiting both the spectral and tempor-al  characteristics  of  continuous  speech(Ganapathiraju et al 1997).
Moreover, the syllablehas  a  close  connection  to  articulation,  integratessome co-articulation phenomena, and has the po-tential  for  a  relatively  compact  representation  ofconversational speech.Therefore, different attempts have been made touse syllables as a unit of recognition for the devel-opment of ASR.
To mention a few: Ganapathirajuet al (1997) have explored techniques to accentu-ate the strengths of syllable-based modeling with aprimary interest of integrating finite-duration mod-eling and monosyllabic word modeling.
Wu et al(1998) tried to extract the features of speech overthe syllabic duration (250ms), considering syllable-length interval to be 100-250ms.
Hu et al (1996)used  a  pronunciation  dictionary  of  syllable-likeunits that are created from sequences of phones forwhich the boundary is difficult to detect.
Kanok-phara  (2003)  used  syllable-structure-based  tri-phones as speech recognition units for Thai.However, syllables are too many in a number oflanguages, such as English, to be trained properly.Thus  ASR researchers  in  languages  like  Englishare led to choose phones where as for Amharic itseems promising to consider syllables as an altern-ative, because Amharic has only 233 distinct CVsyllables.4 Syllable-Based Speech  Recognition  forAmharicIn  the  development  of  syllable-based  LVASRSsfor Amharic we need to deal with a  language mod-el,  pronunciation  dictionary,  initialization  andtraining of the HMM models, and identification ofthe proper HMM topologies that can be properlytrained  with  the  available  data.
This  sectionpresents the development and the performance ofsyllable based speech recognizers.4.1 The Language ModelOne of the required elements in the development ofLVASRSs is the language model.
As there is nousable  language  model  for  Amharic,  we  havetrained  bigram language  models  using  the  HTKstatistical  language  model  development  modules.Due to  the  inflectional  and derivativational  mor-phological feature of Amharic our language mod-els have relatively high perplexities.354.2 The Pronunciation DictionaryThe development of a large vocabulary speaker in-dependent recognition system requires the availab-ility of an appropriate pronunciation dictionary.
Itspecifies the finite set of words that may be outputby the speech recognizer and gives, at  least,  onepronunciation for each.
A pronunciation dictionarycan be classified as a canonical or alternative onthe basis of the pronunciations it includes.A  canonical  pronunciation  dictionary  includesonly  the  standard  phone  (or  other  sub-word)  se-quence assumed to be pronounced in read speech.It does not consider pronunciation variations suchas speaker variability, dialect, or co-articulation inconversational  speech.
On the other hand,  an al-ternative pronunciation dictionary uses the actualphone (or other sub-word) sequences pronouncedin speech.
In an alternative pronunciation diction-ary,  various  pronunciation  variations  can  be  in-cluded (Fukada et al 1999).We have used the pronunciation dictionary thathas been developed by Solomon et al (2005).
Theyhave developed a canonical and an alternative pro-nunciation dictionaries.
Their canonical dictionarytranscribes  50,000 words  and the  alternative  onetranscribes 25,000 words in terms of CV syllables.Both  these  pronunciation  dictionaries  do  nothandle the difference between geminated and non-geminated consonants; the variation of the pronun-ciation  of  the  sixth  order  grapheme,  with  orwithout vowel; and the absence or presence of theglottal  stop  consonant.
Gemination  of  Amharicconsonants  range  from  a  slight  lengthening  tomuch  more  than  doubling.
In  the  dictionary,however, they are represented with the same tran-scription symbols.The sixth order grapheme may be realized withor without vowel but the pronunciation dictionariesdo not  indicate  this  difference.
For  example,  thedictionaries used the same symbol for the syllable[rI]  in  the  word  [d?
?m?rInI]  'we  started',  whosevowel part may not be realized, and in the word[b?rIzo] 'he diluted with water' that is always real-ized with its vowel sound.
That forces a syllablemodel to capture two different sounds: a sound of aconsonant followed by a vowel, and a sound of theconsonant only.
A similar problem occurs with theglottal stop consonant [?]
which may be uttered ornot.A sample of pronunciations in the canonical andalternative  pronunciation  dictionaries  is  given  inTable 43.
The alternative pronunciation dictionarycontains up to 25 pronunciation variants per wordform.
Table  5  illustrates  some cases of  the vari-ation.WordsCanonical  Pro-nunciationAlternative Pronun-ciationCAmACA mA spCA mA spCa mA spHitey-oPeyAHi te yo Pe yAspHi te yo Pe yA spHi te yo Pi yA spHi to Pe yA spte yo Pe yA spto Pe yA spTable 4: Canonical and Alternative PronunciationWords   Number of pronun-ciation variantsHiteyoPeyAweyAne       25HiheHadEge       16yaHiteyoPeyAne       7miniseteru       7yaganezabe       6HegeziHabehEre       6yehenene       5Table 5: Number of Pronunciation variantsAlthough it does not handle gemination and pro-nunciation  variabilities,  the  canonical  pronunci-ation dictionary contains all  233 distinct CV syl-lables of Amharic, which is 100% syllable cover-age.Pronunciation  dictionaries  of  development  andevaluation test sets have been extracted from thecanonical pronunciation dictionary.
These test dic-tionaries have 5,000 and 20,000 words each.4.3  The Acoustic ModelFor training and evaluation of our recognizers, wehave used the Amharic read speech corpus that hasbeen developed by  Solomon et al (2005).The speech corpus consists of a training set, aspeaker adaptation set, development test sets (for5,000 and 20,000 vocabularies), and evaluation testsets (for 5,000 and 20,000 vocabularies).
It  is  amedium size speech corpus of 20 hours of trainingspeech that has been read by 100 training speakerswho  read  a  total  of  10850  different  sentences.Eighty of the training speakers are from the Addis3In tables 4 and 5, we used our own transcription36Ababa dialect while the other twenty are from theother four dialects.Test and speaker adaptation sets were read bytwenty other speakers of the Addis Ababa dialectand four speakers of the other four dialects.
Eachspeaker read 18 different sentences for the 5,000vocabulary (development and evaluation sets each)and 20 different sentences for the 20,000 vocabu-lary  (development  and  evaluation  sets  each)  testsets.
For the adaptation set al of these readers read53 adaptation sentences that consist of all AmharicCV syllables.Initialization:  Training  HMM  models  startswith initialization.
Initialization of the model for aset of sub-word HMMs prior to re-estimation canbe achieved in two different ways: bootstrappingand flat start.
The latter implies that during the firstcycle of embedded re-estimation, each training ut-terance will be uniformly segmented.
The hope ofusing such a procedure is that in the second andsubsequent iterations, the models align as intended.We have initialized HMMs with both methodsand trained them in the same way.
The HMMs thathave been initialized with the flat start method per-formed better (40% word recognition accuracy) ondevelopment test set of 5,000 words.The problem with the bootstrapping approach isthat  any  error  of  the  labeler  strongly  affects  theperformance of the resulting model because con-secutive training steps are influenced by the initialvalue of the model.
As a result, we did not benefitfrom the use of the segmented speech, which hasbeen transcribed with a speech recognizer that haslow word recognition accuracy, and edited by non-linguist  listeners.
We  have,  therefore,  continuedour subsequent experiments with the flat start ini-tialization method.Training: We have used the Baum-Welch re-es-timation procedure for the training.
In training sub-word HMMs that are initialized using the flat-startprocedure,  this  re-estimation  procedure  uses  theparameters of continuously spoken utterances as aninput source.
A transcription, in terms of sub-wordunits, is also needed for each input utterance.
Us-ing the speech parameters and their transcription,the complete set of sub-word HMMs are re-estim-ated  simultaneously.
Then  all  of  the  sub-wordHMMs  corresponding  to  the  sub-word  list  arejoined together to make a single composite HMM.It is important to emphasize that in this process thetranscriptions are only needed to identify the se-quence  of  sub-word  units  in  each  utterance.
Noboundary  information  is  required  (Young  et  al.2002).The major problem with HMM training is that itrequires a great amount of speech data.
To over-come  the  problem  of  training  with  insufficientspeech data, a variety of sharing mechanisms canbe  implemented.
For  example,  HMM parametersare tied together so that the training data is pooledand more robust estimates result.
It is also possibleto restrict the model to a variance vector for the de-scription of output probabilities,  instead of a fullcovariance matrix.
Rabiner and Juang(1993) poin-ted out that for the continuous HMM models, it ispreferable to use diagonal covariance matrices withseveral mixtures, rather than fewer mixtures withfull covariance matrices to perform reliable re-es-timation of the components of the model from lim-ited  training  data.
The  diagonal  covariancematrices have been used in our work.HMM Topologies:  To our knowledge, there isno topology of HMM model that can be taken as arule of thumb for modeling syllable HMMs, espe-cially, for Amharic CV syllables.
To have a goodHMM model for Amharic CV syllables, one needsto conduct experiments to select the optimal modeltopology.
Designing  an HMM topology  has to bedone with proper consideration of the size of theunit of recognition and the amount of the trainingspeech data.
This is because as the size of the re-cognition unit increases and the size of the model(in terms of the number of parameters to be re-es-timated) grows, the model requires more trainingdata.We,  therefore,  carried  out  a  series  of  experi-ments using a left-to-right HMM with and withoutjumps and skips, with a different number of emit-ting states (3, 5, 6, 7, 8, 9, 10 and 11) and differentnumber of Gaussian mixtures (from 2 to 98).
Byjump we mean skips  from the first  non-emittingstate  to  the  middle  state  and/or  from the  middlestate to the last non-emitting state.
Figure 1 showsa  left-to-right  HMM  of  5  emitting  states  withjumps and skips.Figure 1: An example of HMM topologies37We have assumed that the problem of gemina-tion  may  be  compensated  by  the  looping  statetransitions of the HMM.
Accordingly, CV syllablescontaining  geminated  consonants  should  have  ahigher  loop  probability  than those  with the  non-geminated consonants.To develop a solution for the problem of the ir-regularities  in  the  realization  of  the  sixth  ordervowel [I] and the glottal stop consonant [?
], HMMtopologies with jumps have been used.We conducted an experiment using HMMs witha jump from the middle state to the last (non-emit-ting) state for all of the CV syllables with the sixthorder  vowel,  and  a  jump from the  first  emittingstate to the middle state for all of the CV syllableswith the glottal stop consonant.
The CV syllablewith the  glottal  stop consonant  and the  6th ordervowel  have  both  jumps.
These  topologies  havebeen chosen so that the models recognize the ab-sence of the vowel and the glottal stop consonantof  CV syllables.
This assumption was confirmedby  the  observation  that  the  trained models  favorsuch a jump.
A model, which has 5 emitting states,of the glottal stop consonant with the sixth ordervowel tends to start emitting with the 3rd emittingstate with a probability of 0.72.
The model also hasaccumulated  a  considerable  probability  (0.38)  tojump from the 3rd emitting state to the last (non-emitting) state.A similar model of this consonant with the othervowels (our example is the 5th order vowel) tend tostart  emitting  with  the  3rd emitting  state  with  aprobability of 0.68.
This is two times the probabil-ity (0.32) of its transition from the starting (non-emitting state) to the 1st emitting state.The  models  of  the  other  consonants  with  thesixth order vowel,  which are  exemplified by  themodel of the syllable [jI], tend to jump from the 3rdemitting state to the last (non-emitting) state with aprobability of 0.39, which is considerably greaterthan that of continuing with the next state (0.09).Since the amount of available training speech isnot enough to train transition probabilities for skip-ping two or more states, the number of states to beskipped have been limited to one.To determine the  optimal  number of  Gaussianmixtures for the syllable models, we have conduc-ted a series of experiments by adding two Gaussianmixtures for all the models until the performanceof the model starts to degrade.
Considering the dif-ference in the frequency of the CV syllables,  a hy-brid number of Gaussian mixtures has been tried.By hybrid, we mean that Gaussian mixtures are as-signed  to  different  syllables  based  on  their  fre-quency.
For example:  the frequent  syllables,  like[nI], are assigned up to fifty-eight while rare syl-lables, like [p`i], are assigned not more than twoGaussian mixtures.4.4 Performance of the RecognizersWe present recognition results of only those recog-nizers which have competitive performance to thebest performing models.
For example: the perform-ance  of  the  model  with  11  emitting  states  withskips and hybrid Gaussian mixtures is more com-petitive  than those with 7,  8,  9,  and 10 emittingstates.
We have also systematically left out test res-ults which are worse than those presented in Table6.
Table 64 shows evaluation results made on the5k development test set.States TransitionTopolo-giesMix.
ModelsAM AM +LMAM +LM +SA3 No skipand jump18 62.85 88.82Hy 60.87 87.63 88.50skip 12 69.20jump 12 43.74 79.945 No skipand jump12 69.29 88.99 89.80Hy 60.04skip 12 85.77jump 12 54.53 84.6011 skip 12 55.04Hy 71.83 89.21 89.04Table 6: Recognition Performance on 5k Develop-ment test setFrom Table 6, we can see that the models withfive  emitting  states,  with  twelve  Gaussian  mix-tures,  without  skips  and  jumps  has  the  best(89.80%)  word  recognition  accuracy.
It  has87.69% word recognition accuracy on the 20k de-velopment test set.Since  the  most  commonly  used  number  ofHMM states for phone-based speech recognizers isthree emitting states, one may expect a model ofsix emitting states to be the best for an HMM of4In tables 6 and 7, States refers to the number of emittingstates;  Mix  refers  to  the  number  of  Gaussian  mixtures  perstate; Hy refers to hybrid; AM refers to acoustic model; LMrefers to language model; and SA refers to speaker adaptation.38concatenated consonant and vowel.
But the resultof our experiment shows that a CV syllable-basedrecognizer with only five emitting states performedbetter than all the other recognizers.As we can see from Table 6, models with threeemitting states do have a competitive performancewith 18 and hybrid Gaussian mixtures.
They havethe least number of states of all our models.
Never-theless,  they  require  more  storage  space  (33MBwith 18 Gaussian mixtures and 34MB with hybridGaussian mixtures) than the best performing mod-els (32MB).
Models with three emitting states alsohave  larger  number  of  total  Gaussian  mixtures5(30,401  with  18  Gaussian  mixtures  and  31,384with hybrid Gaussian mixtures) than the best per-forming models (13,626 Gaussian mixtures).The other model topology that is competitive inword  recognition  performance is  the  model  witheleven emitting states, with skip and hybrid Gaus-sian mixtures, which has a word recognition accur-acy  of  89.21%.
It  requires  the  biggest  memoryspace (40MB) and uses the largest number of totalGaussian mixtures (36,619) of all the models wehave developed.We have evaluated the top two models with re-gard  to  their  word  recognition  accuracy  on  theevaluation test sets.
Their performance is presentedin Table 7.
As it can be seen from the table, themodels with the better performance on the devel-opment test sets also showed better results with theevaluation test sets.
We can, therefore, say that themodel with five emitting states without skips andtwelve  Gaussian  mixtures  is  preferable  not  onlywith regard to its word recognition accuracy, butalso with regard to its memory requirements.StatesMix.
ModelsAM + LM AM + LM + SA5k 20k 5k 20k5 12 90.43 87.2611 Hy 89.36 87.13Table 7:  Recognition Performance on 5k and 20kEvaluation test setsFor a comparison purpose, we have developed abaseline  word-internal  triphone-based  recognizerusing the same corpus.
The models of 3 emittingstates, 12 Gaussian mixtures, with skips have the5We  counted  the  Gaussian  mixtures  that  are  physicallysaved, instead of what should actually be.best word recognition accuracy (91.31%) of all theother triphone-based recognizers that we have de-veloped.
This recognizer  also has better word re-cognition accuracy than that of our syllable-basedrecognizer (90.43%).
But tying is applied only forthe triphone-based recognizers.However the triphone-based recognizer requiresmuch  more  storage  space  (38MB)  than  the  syl-lable-based  recognizer  that  requires  only  15MBspace.
With regard to their speed of processing, thesyllable-based  model  was  37%  faster  than  tri-phone-based one.These  are  encouraging  results  as  compared  tothe performance  reported by Afify et al (2005) forArabic speech recognition (14.2% word error rate).They have used a trigram language model with alexicon of 60k vocabulary.4.5 Conclusions  and  Research  Areas  in  theFutureWe conclude  that  the  use  of  CV  syllables  is  apromising  alternative  in  the  development  ofASRSs for Amharic.
Although there are still pos-sibilities  of  performance  improvement,  we  havegot  an  encouraging  word  recognition  accuracy(90.43%).
Some of the possibilities of performanceimprovement are:?
The pronunciation dictionary that we have useddoes not  handle the  problem of gemination ofconsonants  and  the  irregular  realization  of  thesixth order vowel and the glottal stop consonant,which has a direct effect on the quality of thesub-word transcriptions.
Proper editing (use ofphonetic transcription) of the pronunciation dic-tionaries  which,  however,  requires  a  consider-able amount of work, certainly will  result  in ahigher  quality  of  sub-word  transcription  andconsequently in the improvement of the recog-nizers'  performance.
By  switching  from  thegrapheme-based  recognizer  to  phonetic-basedrecognizer in Arabic, Afif et  al.
(2005) gainedrelative  word  error  rate  reduction  of  10%  to14%.?
Since tying is one way of minimizing the prob-lem of shortage of training speech, tying the syl-lable-based  models  would  possibly  result  in  again of  some degree  of  performance improve-ment.395 ReferencesAfif, Mohamed, Long Nguyen, Bing Xiang, Sherif Ab-dou, and John Makhoul.
2005.
Recent progress in Ar-abic broadcast news transcription at BBN.
In INTER?SPEECH?2005, 1637-1640Baye  Yimam and  TEAM 503  students.
1997.
"????????"
Ethiopian Journal of Languages and Literat-ure 7(1997): 1-32.Baye Yimam.
1986.
"?????
????".
Addis  Ababa.?.?.?.?.
?.Bender,  L.M.
and  Ferguson  C.  1976.
The  EthiopianWriting System.
In Language in Ethiopia.
Edited byM.L.
Bender,  J.D.
Bowen,  R.L.
Cooper,  and  C.A.Ferguson.
London: Oxford University press.Cowley, Roger, Marvin L. Bender and Charles A. Fer-gusone.
1976.
The Amharic Language-Description.In  Language  in  Ethiopia.
Edited  by  M.L.
Bender,J.D.
Bowen, R.L.
Cooper, and C.A.
Ferguson.
Lon-don: Oxford University press.Deller, J.R. Jr., Hansen, J.H.L.
and Proakis, J.G., Dis-crete-time Processing of Speech Signals.
MacmillanPublishing Company, New York, 2000.Fukada, Toshiaki, Takayoshi Yoshimura and YoshinoriSagisa.
1999.
Automatic generation of multiple pro-nunciations based on neural networks.
Speech Com-munication  27:63?73http://citeseer.ist.psu.edu/fukada99automatic.html.Ganapathiraju,  Aravind; Jonathan Hamaker;  Mark Or-dowski; and George R. Doddington.
1997.
Joseph Pi-cone.
Syllable-based  Large  Vocabulary  ContinuousSpeech Recognition.Getachew Haile.
1967.
The Problems of the AmharicWriting System.
A paper presented in advance for theinterdisciplinary seminar of the Faculty of Arts andEducation.
HSIU.Hayward, Katrina and Richard J. Hayward.
1999.
Am-haric.
In Handbook of the International Phonetic As-sociation:  A  guide  to  the  use  of  the  InternationalPhonetic Alphabet.
Cambridge: the University Press.Hu, Zhihong; Johan Schalkwyk; Etienne Barnard; andRonald Cole.
1996.
Speech recognition using syllablelike units.
Proc.
Int'l Conf.
on Spoken Language Pro-cessing (ICSLP), 2:426-429.Kanokphara,  Supphanat;  Virongrong  Tesprasit  andRachod Thongprasirt.
2003.
Pronunciation VariationSpeech  Recognition  Without  Dictionary  Modifica-tion on Sparse Database, IEEE International Confer-ence  on  Acoustics,  Speech,  and  Signal  Processing(ICASSP 2003, Hong Kong).Kinfe Tadesse.
2002.
Sub-word Based Amharic WordRecognition: An Experiment  Using Hidden MarkovModel  (HMM),  M.Sc  Thesis.
Addis  Ababa  Uni-versity Faculty of Informatics.
Addis Ababa.Lee, C-H., Gauvain, J-L., Pieraccini, R. and Rabiner, L.R.. 1992.
Large vocabulary speech recognition usingsubword units.
Proc.
ICSST-92, Brisbane, Australia,pp.
342-353.Leslau,  W.  2000.
Introductory  Grammar  of  Amharic,Wiesbaden: Harrassowitz.Martha Yifiru.
2003.
Application of Amharic speech re-cognition system  to command and control computer:An experiment with Microsoft  Word, M.Sc Thesis.Addis Ababa University Faculty of Informatics.
Ad-dis Ababa.Rabiner, L. and Juang, B.
1993.
Fundamentals of speechrecognition.
Englewood Cliffs, NJ.Hussien Seid and Bj?rn.
Gamb?ck  2005.
A Speaker In-dependent  Continuous  Speech  Recognizer  for  Am-haric.
In: INTERSPEECH 2005,  9th European Con-ference on Speech Communication and Technology.Lisbon, September 4-9.Solomon Birihanu.
2001.
Isolated Amharic Consonant-Vowel (CV) Syllable Recognition, M.Sc Thesis.
Ad-dis Ababa University Faculty of Informatics.
AddisAbaba.Solomon Teferra Abate.
2006.
Automatic  Speech Re-cognition for  Amharic.
Ph.D. Thesis.
University  ofHamburg.
Hamburg.Solomon Teferra  Abate,  Wolfgang Menzel  and  BairuTafla.
2005.
An Amharic Speech Corpus for LargeVocabulary Continuous Speech Recognition.
In: IN-TERSPEECH  2005,  9th  European  Conference  onSpeech  Communication  and  Technology.
Lisbon,September 4-9.Tadesse Beyene.
1994.
The Ethiopian Writing System.Paper presented at the 12th International Conferenceof Ethiopian Studies, Michigan State University.Wu, Su-Lin.
1998.
Incorporating Information from Syl-lable-length Time Scales into Automatic Speech Re-cognition.
PhD  thesis,  University  of  California,Berkeley, CA.Young, Steve; Dan Kershaw; Julian Odell and Dave Ol-lason.
2002.
The HTK Book.Zegaye Seyifu.
2003.
Large vocabulary, speaker inde-pendent, continuous  Amharic speech recognition,M.Sc Thesis.
Addis Ababa University Faculty of In-formatics.
Addis Ababa.40
