c?
2004 Association for Computational LinguisticsLearning Subjective LanguageJanyce Wiebe?
Theresa Wilson?University of Pittsburgh University of PittsburghRebecca Bruce?
Matthew Bell?University of North Carolina University of Pittsburghat AshevilleMelanie Martin?New Mexico State UniversitySubjectivity in natural language refers to aspects of language used to express opinions, evalua-tions, and speculations.
There are numerous natural language processing applications for whichsubjectivity analysis is relevant, including information extraction and text categorization.
Thegoal of this work is learning subjective language from corpora.
Clues of subjectivity are gener-ated and tested, including low-frequency words, collocations, and adjectives and verbs identifiedusing distributional similarity.
The features are also examined working together in concert.
Thefeatures, generated from different data sets using different procedures, exhibit consistency inperformance in that they all do better and worse on the same data sets.
In addition, this articleshows that the density of subjectivity clues in the surrounding context strongly affects how likelyit is that a word is subjective, and it provides the results of an annotation study assessing thesubjectivity of sentences with high-density features.
Finally, the clues are used to perform opinionpiece recognition (a type of text categorization and genre detection) to demonstrate the utility ofthe knowledge acquired in this article.1.
IntroductionSubjectivity in natural language refers to aspects of language used to express opin-ions, evaluations, and speculations (Banfield 1982; Wiebe 1994).
Many natural lan-guage processing (NLP) applications could benefit from being able to distinguishsubjective language from language used to objectively present factual information.Current extraction and retrieval technology focuses almost exclusively on the sub-ject matter of documents.
However, additional aspects of a document influence itsrelevance, including evidential status and attitude (Kessler, Nunberg, Schu?tze 1997).Information extraction systems should be able to distinguish between factual infor-mation (which should be extracted) and nonfactual information (which should be?
Department of Computer Science, University of Pittsburgh, Pittsburgh, PA 15260.E-mail{wiebe,mbell}@cs.pitt.edu.?
Intelligent Systems Program, University of Pittsburgh, Pittsburgh, PA 15260.
Email: twilson@cs.pitt.edu.?
Department of Computer Science, University of North Carolina at Asheville, Asheville, NC 28804.E-mail: bruce@cs.unca.edu?
Department of Computer Science, New Mexico State University, Las Cruces, NM 88003.
E-mail:mmartin@cs.nmsu.edu.Submission received: 20 March 2002; Revised submission received: 30 September 2003; Accepted forpublication: 23 January 2004278Computational Linguistics Volume 30, Number 3discarded or labeled as uncertain).
Question-answering systems should distinguishbetween factual and speculative answers.
Multi-perspective question answering aimsto present multiple answers to the user based upon speculation or opinions derivedfrom different sources (Carbonell 1979; Wiebe et al 2003).
Multidocument summa-rization systems should summarize different opinions and perspectives.
Automaticsubjectivity analysis would also be useful to perform flame recognition (Spertus 1997;Kaufer 2000), e-mail classification (Aone, Ramos-Santacruze, and Niehaus 2000), intel-lectual attribution in text (Teufel and Moens 2000), recognition of speaker role in radiobroadcasts (Barzialy et al 2000), review mining (Terveen et al 1997), review classifi-cation (Turney 2002; Pang, Lee, and Vaithyanathan 2002), style in generation (Hovy1987), and clustering documents by ideological point of view (Sack 1995).
In general,nearly any information-seeking system could benefit from knowledge of how opin-ionated a text is and whether or not the writer purports to objectively present factualmaterial.To perform automatic subjectivity analysis, good clues must be found.
A hugevariety of words and phrases have subjective usages, and while some manually de-veloped resources exist, such as dictionaries of affective language (General-Inquirer2000; Heise 2000) and subjective features in general-purpose lexicons (e.g., the atti-tude adverb features in Comlex [Macleod, Grishman, and Meyers 1998]), there is nocomprehensive dictionary of subjective language.
In addition, many expressions withsubjective usages have objective usages as well, so a dictionary alone would not suffice.An NLP system must disambiguate these expressions in context.The goal of our work is learning subjective language from corpora.
In this article,we generate and test subjectivity clues and contextual features and use the knowledgewe gain to recognize subjective sentences and opinionated documents.Two kinds of data are available to us: a relatively small amount of data manuallyannotated at the expression level (i.e., labels on individual words and phrases) of WallStreet Journal and newsgroup data and a large amount of data with existing document-level annotations from the Wall Street Journal (opinion pieces, such as editorials andreviews, versus nonopinion pieces).
Both are used as training data to identify cluesof subjectivity.
In addition, we cross-validate the results between the two types ofannotation: The clues learned from the expression-level data are evaluated against thedocument-level annotations, and those learned using the document-level annotationsare evaluated against the expression-level annotations.There were a number of motivations behind our decision to use document-levelannotations, in addition to our manual annotations, to identify and evaluate cluesof subjectivity.
The document-level annotations were not produced according to ourannotation scheme and were not produced for the purpose of training and evaluatingan NLP system.
Thus, they are an external influence from outside the laboratory.
Inaddition, there are a great number of these data, enabling us to evaluate the resultson a larger scale, using multiple large test sets.
This and cross-training between thetwo types of annotations allows us to assess consistency in performance of the variousidentification procedures.
Good performance in cross-validation experiments betweendifferent types of annotations is evidence that the results are not brittle.We focus on three types of subjectivity clues.
The first are hapax legomena, the setof words that appear just once in the corpus.
We refer to them here as unique words.The set of all unique words is a feature with high frequency and significantly higherprecision than baseline (Section 3.2).The second are collocations (Section 3.3).
We demonstrate a straightforward methodfor automatically identifying collocational clues of subjectivity in texts.
The method isfirst used to identify fixed n-grams, such as of the century and get out of here.
Interest-279Wiebe, Wilson, Bruce, Bell, and Martin Learning Subjective Languageingly, many include noncontent words that are typically on stop lists of NLP systems(e.g., of, the, get, out, here in the above examples).
The method is then used to identifyan unusual form of collocation: One or more positions in the collocation may be filledby any word (of an appropriate part of speech) that is unique in the test data.The third type of subjectivity clue we examine here are adjective and verb fea-tures identified using the results of a method for clustering words according to dis-tributional similarity (Lin 1998) (Section 3.4).
We hypothesized that two words maybe distributionally similar because they are both potentially subjective (e.g., tragic, sad,and poignant are identified from bizarre).
In addition, we use distributional similarityto improve estimates of unseen events: A word is selected or discarded based on theprecision of it together with its n most similar neighbors.We show that the various subjectivity clues perform better and worse on the samedata sets, exhibiting an important consistency in performance (Section 4.2).In addition to learning and evaluating clues associated with subjectivity, we ad-dress disambiguating them in context, that is, identifying instances of clues that aresubjective in context (Sections 4.3 and 4.4).
We find that the density of clues in thesurrounding context is an important influence.
Using two types of annotations servesus well here, too.
It enables us to use manual judgments to identify parameters fordisambiguating instances of automatically identified clues.
High-density clues are highprecision in both the expression-level and document-level data.
In addition, we givethe results of a new annotation study showing that most high-density clues are in sub-jective text spans (Section 4.5).
Finally, we use the clues together to perform document-level classification, to further demonstrate the utility of the acquired knowledge (Sec-tion 4.6).At the end of the article, we discuss related work (Section 5) and conclusions(Section 6).2.
SubjectivitySubjective language is language used to express private states in the context of atext or conversation.
Private state is a general covering term for opinions, evaluations,emotions, and speculations (Quirk et al 1985).
The following are examples of subjectivesentences from a variety of document types.The first two examples are from Usenet newsgroup messages:(1) I had in mind your facts, buddy, not hers.
(2) Nice touch.
?Alleges?
whenever facts posted are not in your persona ofwhat is ?real.
?The next one is from an editorial:(3) We stand in awe of the Woodstock generation?s ability to be unceasinglyfascinated by the subject of itself.
(?Bad Acid,?
Wall Street Journal,August 17, 1989)The next example is from a book review:(4) At several different layers, it?s a fascinating tale.
(George Melloan,?Whose Spying on Our Computers??
Wall Street Journal, November 1,1989)280Computational Linguistics Volume 30, Number 3The last one is from a news story:(5) ?The cost of health care is eroding our standard of living and sappingindustrial strength,?
complains Walter Maher, a Chryslerhealth-and-benefits specialist.
(Kenneth H. Bacon, ?Business and LaborReach a Consensus on Need to Overhaul Health-Care System,?
WallStreet Journal, November 1, 1989)In contrast, the following are examples of objective sentences, sentences without sig-nificant expressions of subjectivity:(6) Bell Industries Inc. increased its quarterly to 10 cents from 7 cents ashare.
(7) Northwest Airlines settled the remaining lawsuits filed on behalf of 156people killed in a 1987 crash, but claims against the jetliner?s maker arebeing pursued, a federal judge said.
(?Northwest Airlines Settles Rest ofSuits,?
Wall Street Journal, November 1, 1989)A particular model of linguistic subjectivity underlies the current and past re-search in this area by Wiebe and colleagues.
It is most fully presented in Wiebe andRapaport (1986, 1988, 1991) and Wiebe (1990, 1994).
It was developed to support NLPresearch and combines ideas from several sources in fields outside NLP, especiallylinguistics and literary theory.
The most direct influences on the model were Dolezel(1973) (types of subjectivity clues), Uspensky (1973) (types of point of view), Kuroda(1973, 1976) (pragmatics of point of view), Chatman (1978) (story versus discourse),Cohn (1978) (linguistic styles for presenting consciousness), Fodor (1979) (linguisticdescription of opaque contexts), and especially Banfield (1982) (theory of subjectivityversus communication).1The remainder of this section sketches our conceptualization of subjectivity anddescribes the annotation projects it underlies.Subjective elements are linguistic expressions of private states in context.
Subjec-tive elements are often lexical (examples are stand in awe, unceasingly, fascinated in (3)and eroding, sapping, and complains in (5)).
They may be single words (e.g., complains)or more complex expressions (e.g., stand in awe, what a NP).
Purely syntactic or mor-phological devices may also be subjective elements (e.g., fronting, parallelism, changesin aspect).A subjective element expresses the subjectivity of a source, who may be the writeror someone mentioned in the text.
For example, the source of fascinating in (4) isthe writer, while the source of the subjective elements in (5) is Maher (according tothe writer).
In addition, a subjective element usually has a target, that is, what thesubjectivity is about or directed toward.
In (4), the target is a tale; in (5), the target ofMaher?s subjectivity is the cost of health care.Note our parenthetical above?
?according to the writer?
?concerning Maher?ssubjectivity.
Maher is not directly speaking to us but is being quoted by the writer.Thus, the source is a nested source, which we notate (writer, Maher); this representsthe fact that the subjectivity is being attributed to Maher by the writer.
Since sources1 For additional citations to relevant work from outside NLP, please see Banfield (1982), Fludernik (1993),Wiebe (1994), and Stein and Wright (1995).281Wiebe, Wilson, Bruce, Bell, and Martin Learning Subjective Languageare not directly addressed by the experiments presented in this article, we merelyillustrate the idea here with an example, to give the reader an idea:The Foreign Ministry said Thursday that it was ?surprised, to put itmildly?
by the U.S. State Department?s criticism of Russia?s humanrights record and objected in particular to the ?odious?
section onChechnya.
(Moscow Times, March 8, 2002]Let us consider some of the subjective elements in this sentence, along with theirsources:surprised, to put it mildly: (writer, Foreign Ministry, Foreign Ministry)to put it mildly: (writer, Foreign Ministry)criticism: (writer, Foreign Ministry, Foreign Ministry, U.S. State Department)objected: (writer, Foreign Ministry)odious: (writer, Foreign Ministry)Consider surprised, to put it mildly.
This refers to a private state of the Foreign Ministry(i.e., it is very surprised).
This is in the context of The Foreign Ministry said, which is ina sentence written by the writer.
This gives us the three-level source (writer, ForeignMinistry, Foreign Ministry).
The phrase to put it mildly, which expresses sarcasm, isattributed to the Foreign Ministry by the writer (i.e., according to the writer, the ForeignMinistry said this).
So its source is (writer, Foreign Ministry).
The subjective elementcriticism has a deeply nested source: According to the writer, the Foreign Ministry saidit is surprised by the U.S. State Department?s criticism.The nested-source representation allows us to pinpoint the subjectivity in a sen-tence.
For example, there is no subjectivity attributed directly to the writer in theabove sentence: At the level of the writer, the sentence merely says that someonesaid something and objected to something (without evaluating or questioning this).If the sentence started The magnificent Foreign Ministry said.
.
.
, then we would have anadditional subjective element, magnificent, with source (writer).Note that subjective does not mean not true.
Consider the sentence John criticizedMary for smoking.
The verb criticized is a subjective element, expressing negative eval-uation, with nested source (writer, John).
But this does not mean that John does notbelieve that Mary smokes.
(In addition, the fact that John criticized Mary is beingpresented as true by the writer.
)Similarly, objective does not mean true.
A sentence is objective if the language usedto convey the information suggests that facts are being presented; in the context ofthe discourse, material is objectively presented as if it were true.
Whether or not thesource truly believes the information, and whether or not the information is in facttrue, are considerations outside the purview of a theory of linguistic subjectivity.An aspect of subjectivity highlighted when we are working with NLP applicationsis ambiguity.
Many words with subjective usages may be used objectively.
Examplesare sapping and eroding.
In (5), they are used subjectively, but one can easily imagineobjective usages, in a scientific domain, for example.
Thus, an NLP system may notmerely consult a list of lexical items to accurately identify subjective language butmust disambiguate words, phrases, and sentences in context.
In our terminology, apotential subjective element (PSE) is a linguistic element that may be used to express282Computational Linguistics Volume 30, Number 3Table 1Data Sets and Annotations used in Experiments.
Annotators M, MM, and T areco-authors of this paper.
D and R are not.Name Source Number of Words Annotators Type ofannotationWSJ-SE Wall Street Journal 18,341 D,M Subjective elementsNG-SE Newsgroup 15,413 M Subjective elementsNG-FE Newsgroup 88,210 MM,R Flame elementsOP1 Wall Street Journal 640,975 M,T DocumentsComposed of 4 data sets: W9-4,W9-10,W9-22,W-33OP2 Wall Street Journal 629,690 M,T DocumentsComposed of 4 data sets: W9-2,W9-20,W9-21,W-23subjectivity.
A subjective element is an instance of a potential subjective element, in aparticular context, that is indeed subjective in that context (Wiebe 1994).In this article, we focus on learning lexical items that are associated with subjec-tivity (i.e., PSEs) and then using them in concert to disambiguate instances of them(i.e., to determine whether the instances are subjective elements).2.1 Manual AnnotationsIn our subjectivity annotation projects, we do not give the annotators lists of particularwords and phrases to look for.
Rather, we ask them to label sentences according totheir interpretations in context.
As a result, the annotators consider a large variety ofexpressions when performing annotations.We use data that have been manually annotated at the expression level, the sen-tence level, and the document level.
For diversity, we use data from the Wall StreetJournal Treebank as well as data from a corpus of Usenet newsgroup messages.
Table1 summarizes the data sets and annotations used in this article.
None of the datasetsoverlap.
The annotation types listed in the table are those used in the experimentspresented in this article.In our first subjectivity annotation project (Wiebe, Bruce, and O?Hara 1999; Bruceand Wiebe 1999), a corpus of sentences from the Wall Street Journal Treebank Corpus(Marcus, Santorini, and Marcinkiewicz 1993) (corpus WSJ-SE in Table 1) was annotatedat the sentence level by multiple judges.
The judges were instructed to classify a sen-tence as subjective if it contained any significant expressions of subjectivity, attributedto either the writer or someone mentioned in the text, and to classify the sentence asobjective, otherwise.
After multiple rounds of training, the annotators independentlyannotated a fresh test set of 500 sentences from WSJ-SE.
They achieved an averagepairwise kappa score of 0.70 over the entire test set, an average pairwise kappa scoreof 0.80 for the 85% of the test set for which the annotators were somewhat sure oftheir judgments, and an average pairwise kappa score of 0.88 for the 70% of the testset for which the annotators were very sure of their judgments.We later asked the same annotators to identify the subjective elements in WSJ-SE.
Specifically, each annotator was given the subjective sentences he identified in283Wiebe, Wilson, Bruce, Bell, and Martin Learning Subjective Languagethe previous study and asked to put brackets around the words he believed causedthe sentence to be classified as subjective.2 For example (subjective elements are inparentheses):They paid (yet) more for (really good stuff).
(Perhaps you?ll forgive me) for reposting his response.No other instructions were given to the annotators and no training was performed forthe expression-level task.
A single round of tagging was performed, with no commu-nication between annotators.
There are techniques for analyzing agreement when an-notations involve segment boundaries (Litman and Passonneau 1995; Marcu, Romera,and Amorortu 1999), but our focus in this article is on words.
Thus, our analyses areat the word level: Each word is classified as either appearing in a subjective elementor not.
Punctuation and numbers are excluded from the analyses.
The kappa value forword agreement in this study is 0.42.Another two-level annotation project was performed in Wiebe et al (2001), thistime involving document-level and expression-level annotations of newsgroup data(NG-FE in Table 1).
In that project, we were interested in annotating flames, inflam-matory messages in newsgroups or listservs.
Note that inflammatory language is akind of subjective language.
The annotators were instructed to mark a message asa flame if the main intention of the message is a personal attack and the messagecontains insulting or abusive language.After multiple rounds of training, three annotators independently annotated afresh test set of 88 messages from NG-FE.
The average pairwise percentage agreementis 92% and the average pairwise kappa value is 0.78.
These results are comparable tothose of Spertus (1997), who reports 98% agreement on noninflammatory messagesand 64% agreement on inflammatory messages.Two of the annotators were then asked to identify the flame elements in the entirecorpus NG-FE.
Flame elements are the subset of subjective elements that are perceivedto be inflammatory.
The two annotators were asked to do this in the entire corpus, eventhose messages not identified as flames, because messages that were not judged to beflames at the document level may contain some individual inflammatory phrases.
Asabove, no training was performed for the expression-level task, and a single round oftagging was performed, without communication between annotators.
Agreement wasmeasured in the same way as in the subjective-element study above.
The kappa valuefor flame element annotations in corpus NG-FE is 0.46.An additional annotation project involved a single annotator, who performedsubjective-element annotations on the newsgroup corpus NG-SE.The agreement results above suggest that good levels of agreement can be achievedat higher levels of classification (sentence and document), but agreement at the expres-sion level is more challenging.
The agreement values are lower for the expression-levelannotations but are still much higher than that expected by chance.Note that our word-based analysis of agreement is a tough measure, because itrequires that exactly the same words be identified by both annotators.
Consider thefollowing example from WSJ-SE:D: (played the role well) (obligatory ragged jeans a thicket of long hairand rejection of all things conventional)2 We are grateful to Aravind Joshi for suggesting this level of annotation.284Computational Linguistics Volume 30, Number 3M: played the role (well) (obligatory) (ragged) jeans a (thicket) of longhair and (rejection) of (all things conventional)Judge D in the example consistently identifies entire phrases as subjective, while judgeM prefers to select discrete lexical items.Despite such differences between annotators, the expression-level annotationsproved very useful for exploring hypotheses and generating features, as describedbelow.Since this article was written, a new annotation project has been completed.
A10,000-sentence corpus of English-language versions of world news articles has beenannotated with detailed subjectivity information as part of a project investigatingmultiple-perspective question answering (Wiebe et al 2003).
These annotations aremuch more detailed than the annotations used in this article (including, for example,the source of each private state).
The interannotator agreement scores for the newcorpus are high and are improvements over the results of the studies described above(Wilson and Wiebe 2003).The current article uses existing document-level subjective classes, namely edito-rials, letters to the editor, Arts & Leisure reviews, and Viewpoints in the Wall StreetJournal.
These are subjective classes in the sense that they are text categories for whichsubjectivity is a key aspect.
We refer to them collectively as opinion pieces.
All othertypes of documents in the Wall Street Journal are collectively referred to as nonopinionpieces.Note that opinion pieces are not 100% subjective.
For example, editorials containobjective sentences presenting facts supporting the writer?s argument, and reviewscontain sentences objectively presenting facts about the product beign reviewed.
Sim-ilarly, nonopinion pieces are not 100% objective.
News reports present opinions andreactions to reported events (van Dijk 1988); they often contain segments starting withexpressions such as critics claim and supporters argue.
In addition, quoted-speech sen-tences in which individuals express their subjectivity are often included (Barzilay etal.
2000).
For concreteness, let us consider WSJ-SE, which, recall, has been manuallyannotated at the sentence level.
In WSJ-SE, 70% of the sentences in opinion piecesare subjective and 30% are objective.
In nonopinion pieces, 44% of the sentences aresubjective and only 56% are objective.
Thus, while there is a higher concentration ofsubjective sentences in opinion versus nonopinion pieces, there are many subjectivesentences in nonopinion pieces and objective sentences in opinion pieces.An inspection of some data reveals that some editorial and review articles are notmarked as such by the Wall Street Journal.
For example, there are articles whose purposeis to present an argument rather than cover a news story, but they are not explicitlylabeled as editorials by the Wall Street Journal.
Thus, the opinion piece annotations ofdata sets OP1 and OP2 in Table 1 have been manually refined.
The annotation instruc-tions were simply to identify any additional opinion pieces that were not marked assuch.
To test the reliability of this annotation, two judges independently annotatedtwo Wall Street Journal files, W9-22 and W9-33, each containing approximately 160,000words.
This is an ?annotation lite?
task: With no training, the annotators achievedkappa values of 0.94 and 0.95, and each spent an average of three hours per WallStreet Journal file.3.
Generating and Testing Subjective Features3.1 IntroductionThe goal in this section is to learn lexical subjectivity clues of various types, singlewords as well as collocations.
Some require no training data, some are learned us-285Wiebe, Wilson, Bruce, Bell, and Martin Learning Subjective Languageing the expression-level subjective-element annotations as training data, and someare learned using the document-level opinion piece annotations as training data (i.e.,opinion piece versus nonopinion piece).
All of the clues are evaluated with respect tothe document-level opinion piece annotations.
While these evaluations are our focus,because many more opinion piece than subjective-element data exist, we do evaluatethe clues learned from the opinion piece data on the subjective-element data as well.Thus, we cross-validate the results both ways between the two types of annotations.Throughout this section, we evaluate sets of clues directly, by measuring the pro-portion of clues that appear in subjective documents or expressions, seeking those thatappear more often than expected.
In later sections, the clues are used together to findsubjective sentences and to perform text categorization.The following paragraphs give details of the evaluation and experimental designused in this section.The proportion of clues in subjective documents or expressions is their precision.Specifically, the precision of a set S with respect to opinion pieces isprec(S) =number of instances of members of S in opinion piecestotal number of instances of members of S in the dataThe precision of a set S with respect to subjective elements isprec(S) =number of instances of members of S in subjective elementstotal number of instances of members of S in the dataIn the above, S is a set of types (not tokens).
The counts are of tokens (i.e., instancesor occurrences) of members of S.Why use a set rather than individual items?
Many good clues of subjectivity occurwith low frequency (Wiebe, McKeever, and Bruce 1998).
In fact, as we shall see below,uniqueness in the corpus is an informative feature for subjectivity classification.
Thus,we do not want to discard low-frequency clues, because they are a valuable source ofinformation, and we do not want to evaluate individual low-frequency lexical items,because the results would be unreliable.
Our strategy is thus to identify and evaluatesets of words and phrases, rather than individual items.What kinds of results may we expect?
We cannot expect absolutely high precisionwith respect to the opinion piece classifications, even for strong clues, for three reasons.First, for our purposes, the data are noisy.
As mentioned above, while the proportionof subjective sentences is higher in opinion than in nonopinion pieces, the proportionsare not 100 and 0: Opinion pieces contain objective sentences, and nonopinion piecescontain subjective sentences.Second, we are trying to learn lexical items associated with subjectivity, that is,PSEs.
As discussed above, many words and phrases with subjective usages have ob-jective usages as well.
Thus, even in perfect data with no noise, we would not expect100% precision.
(This is the motivation for the work on density presented in section4.4.
)Third, the distribution of opinions and nonopinions is highly skewed in favor ofnonopinions: Only 9% of the articles in the combination of OP1 and OP2 are opinionpieces.In this work, increases in precision over a baseline precision are used as evidencethat promising sets of PSEs have been found.
Our main baseline for comparison isthe number of word instances in opinion pieces, divided by the total number of wordinstances:Baseline Precision =number of word instances in opinion piecestotal number of word instances286Computational Linguistics Volume 30, Number 3Table 2Frequencies and increases in precision of uniquewords in subjective-element data.
Baselinefrequency is the total number of words, andbaseline precision is the proportion of words insubjective elements.WSJ-SED Mfreq +prec +precUnique words 2,615 +.07 +.12Baseline 18,341 .07 .08Words and phrases with higher proportions than this appear more than expected inopinion pieces.To further evaluate the quality of a set of PSEs, we also perform the followingsignificance test.
For a set of PSEs in a given data set, we test the significance of thedifference between (1) the proportion of words in opinion pieces that are PSEs and (2)the proportion of words in nonopinion pieces that are PSEs, using the z-significancetest for two proportions.Before we continue, there are a few more technical items to mention concerningthe data preparation and experimental design:?
All of the data sets are stemmed using Karp?s morphological analyzer(Karp et al 1994) and part-of-speech tagged using Brill?s (1992) tagger.?
When the opinion piece classifications are used for training, the existingclassifications, assigned by the Wall Street Journal, are used.
Thus, theprocesses using them as training data may be applied to more data tolearn more clues, without requiring additional manual annotation.?
When the opinion piece data are used for testing, the manually refinedclassifications (described at the end of Section 2.1) are used.?
OP1 and OP2 together comprise eight treebank files.
Below, we oftengive results separately for the component files, allowing us to assess theconsistency of results for the various types of clues.3.2 Unique WordsIn this section, we show that low-frequency words are associated with subjectivity inboth the subjective-element and opinion piece data.
Apparently, people are creativewhen they are being opinionated.Table 2 gives results for unique words in subjective-element data.
Recall thatunique words are those that appear just once in the corpus, that is, hapax legomena.The first row of Table 2 gives the frequency of unique words in WSJ-SE, followedby the percentage-point improvements in precision over baseline for unique words insubjective elements marked by two annotators (denoted as D and M in the table).
Thesecond row gives baseline frequency and precisions.
Baseline frequency is the totalnumber of words in WSJ-SE.
Baseline precision for an annotator is the proportion ofwords included in subjective elements by that annotator.
Specifically, consider anno-tator M. The baseline precision of words in subjective elements marked by M is 0.08,287Wiebe, Wilson, Bruce, Bell, and Martin Learning Subjective LanguageTable 3Frequencies and increases in precision for words that appear exactly once in the data setscomposing OP1.
For each data set, baseline frequency is the total number of words, andbaseline precision is the proportion of words in opinion pieces.W9-04 W9-10 W9-22 W9-33freq +prec freq +prec freq +prec freq +precUnique words 4,794 +.15 4,763 +.16 4,274 +.11 4,567 +.11Baseline 156,421 .19 156,334 .18 155,135 .13 153,634 .14but the precision of unique words in these same annotations is 0.20, 0.12 points higherthan the baseline.
This is a 150% improvement over the baseline.The number of unique words in opinion pieces is also higher than expected.
Table3 compares the precision of the set of unique words to the baseline precision (i.e.,the precision of the set of all words that appear in the corpus) in the four WSJ filescomposing OP1.
Before this analysis was performed, numbers were removed from thedata (we are not interested in the fact that, say, the number 163,213.01 appears just oncein the corpus).
The number of words in each data set and baseline precisions are listedat the bottom of the table.
The freq columns give total frequencies.
The +prec columnsshow the percentage-point improvements in precision over baseline.
For example, inW9-10, unique words have precision 0.34: 0.18 baseline plus an improvement overbaseline of 0.16.
The difference in the proportion of words that are unique in opinionpieces and the proportion of words that are unique in nonopinion pieces is highlysignificant, with p < 0.001 (z ?
22) for all of the data sets.
Note that not only does theset of unique words have higher than baseline precision, the set is a frequent feature.The question arises, how does corpus size affect the precision of the set of uniquewords?
Presumably, uniqueness in a larger corpus is more meaningful than uniquenessin a smaller one.
The results in Figure 1 provide evidence that it is.
The y-axis in Figure1 represents increase in precision over baseline and the x-axis represents corpus size.Five graphs are plotted, one for the set of words that appear exactly once (uniques),one for the set of words that appear exactly twice ( freq2), one for the set of words thatappear exactly three times ( freq3), etc.In Figure 1, increases in precision are given for corpora of size n, where n =20, 40, .
.
.
, 2420, 2440 documents.
Each data point is an average over 25 sample corporaof size n. The sample corpora were chosen from the concatenation of OP1 and OP2, inwhich 9% of the documents are opinion pieces.
The sample corpora were created byrandomly selecting documents from the large corpus, preserving the 9% distributionof opinion pieces.
At the smallest corpus size (containing 20 documents), the averagenumber of words is 9,617.
At the largest corpus size (containing 2440 documents), theaverage is 1,225,186 words.As can be seen in the figure, the precision of unique and other low-frequencywords increases with corpus size, with increases tapering off at the largest corpus sizetested.
Words with frequency 2 also realize a nice increase, although one that is not asdramatic, in precision over baseline.
Even words of frequency 3, 4, and 5 show modestincreases.To help us understand the importance of low-frequency words in large as opposedto small data sets, we can consider the following analogy.
With collectible tradingcards, rare cards are the most valuable.
However, if we have some cards and aretrying to determine thier value, looking in only a few packs of cards will not tell us if288Computational Linguistics Volume 30, Number 30.000.020.040.060.080.100.120.140.160.180.2020 620 1220 1820 2420Corpus Size (documents)Increase inPrecisionuniques freq2 freq3 freq4 freq5Figure 1Precision of low-frequency words as corpus size increases.any of our cards are valuable.
Only by looking at many packs of cards can we makea determination as to which are the rare ones.
Only in samples of sufficient size isuniqueness informative.The results in this section suggest that an NLP system using uniqueness featuresto recognize subjectivity should determine uniqueness with respect to the test dataaugmented with an additional store of (unannotated) data.3.3 Identifying Potentially Subjective Collocations from Subjective-Element andFlame-Element AnnotationsIn this section, we describe experiments in identifying potentially subjective colloca-tions.Collocations are selected from the subjective-element data (i.e., NG-SE, NG-FE, andWSJ-SE), using the union of the annotators?
tags for the data sets tagged by multipletaggers.
The results are then evaluated on opinion piece data.The selection procedure is as follows.
First, all 1-grams, 2-grams, 3-grams, and4-grams are extracted from the data.
In this work, each constituent of an n-gram isa word-stem, part-of-speech pair.
For example, (in-prep the-det can-noun) is a 3-gramthat matches trigrams consisting of preposition in, followed by determiner the, andending with noun can.A subset of the n-grams are then selected based on precision.
The precision of ann-gram is the number of subjective instances of that n-gram in the data divided bythe total number of instances of that n-gram in the data.
An instance of an n-gram issubjective if each word occurs in a subjective element in the data.n-grams are selected based on two criteria.
First, the precision of the n-gram mustbe greater than the baseline precision (i.e., the proportion of all word instances that289Wiebe, Wilson, Bruce, Bell, and Martin Learning Subjective Languageare in subjective elements).
Second, the precision of the n-gram must be greater thanthe maximum precision of its constituents.
This criterion is used to avoid selectingunnecessarily long collocations.
For example, scumbag is a strongly subjective clue.
Ifbe a scumbag does not have higher precision than scumbag alone, we do not want toselect it.Specifically, let (W1, W2) be a bigram consisting of consecutive words W1 and W2.
(W1,W2) is identified as a potential subjective element if prec(W1, W2) ?
0.1 and:prec(W1, W2) > max(prec(W1), prec(W2))For trigrams, we extend the second condition as follows.
Let (W1, W2, W3) be a trigramconsisting of consecutive words W1, W2, and W3.
The condition is thenprec(W1, W2, W3) > max(prec(W1, W2), prec(W3))orprec(W1, W2, W3) > max(prec(W1), prec(W2, W3))The selection of 4-grams is similar to the selection of 3-grams, comparing the 4-gramfirst with the maximum of the precisions of word W1 and trigram (W2, W3, W4) andthen with the maximum of the precisions of trigram (W1,W2,W3) and word W4.
Wecall the n-gram collocations identified as above fixed-n-grams.We also define a type of collocation called a unique generalized n-gram (ugen-n-gram).
Such collocations have placeholders for unique words.
As will be seen below,these are our highest-precision features.To find and select such generalized collocations, we first find every word thatappears just once in the corpus and replace it with a new word, UNIQUE (but re-membering the part of speech of the original word).
In essence, we treat the set ofsingle-instance words as a single, frequently occurring word (which occurs with var-ious parts of speech).
Precisely the same method used for extracting and selectingn-grams above is used to obtain the potentially subjective collocations with one ormore positions filled by a UNIQUE, part-of-speech pair.To test the ugen-n-grams extracted from the subjective-element training data usingthe method outlined above, we assess their precision with respect to opinion piecedata.
As with the training data, all unique words in the test data are replaced byUNIQUE.
When a ugen-n-gram is matched against the test data, the UNIQUE fillersmatch words (of the appropriate parts of speech) that are unique in the test data.Table 4 shows the results of testing the fixed-n-gram and the ugen-n-gram patternsidentified as described above on the four data sets composing OP1.
The freq columnsgive total frequencies, and the +prec columns show the improvements in precisionfrom the baseline.
The number of words in each data set and baseline precisions aregiven at the bottom of the table.
For all n-gram features besides the fixed-4-grams andugen-4-grams, the proportion of features in opinion pieces is significantly greater thanthe proportion of features in nonopinion pieces.3The question arises, how much overlap is there between instances of fixed-n-gramsand instances of ugen-n-grams?
In the test data of Table 4, there are a total of 8,577fixed-n-grams instances.
Only 59 of these, fewer than 1% are contained (wholly or inpart) in ugen-n-gram instances.
This small intersection set shows that two differenttypes of potentially subjective collocations are being recognized.3 Specifically, the difference between (1) the number of feature instances in opinion pieces divided by thenumber of words in opinion pieces and (2) the number of feature instances in nonopinion piecesdivided by the number of words in nonopinion pieces is significant (p < 0.05) for all data sets.290Computational Linguistics Volume 30, Number 3Table 4Frequencies and increases in precision of fixed-n-gram and ugen-n-gram collocationslearned from the subjective-element data.
For each data set, baseline frequency is the totalnumber of words, and baseline precision is the proportion of words in opinion pieces.W9-04 W9-10 W9-22 W9-33freq +prec freq +prec freq +prec freq +precfixed-2-grams 1,840 +.07 1,972 +.07 1,933 +.04 1,839 +.05ugen-2-grams 281 +.21 256 +.26 261 +.17 254 +.17fixed-3-grams 213 +.08 243 +.09 214 +.05 238 +.05ugen-3-grams 148 +.29 133 +.27 147 +.16 133 +.15fixed-4-grams 18 +.15 17 +.06 12 +.29 14 ?.07ugen-4-grams 13 +.12 3 +.82 15 +.27 13 +.25baseline 156,421 .19 156,334 .18 155,135 .13 153,634 .14Randomly selected examples of our learned collocations that appear in the testdata are given in Tables 5 and 6.
It is interesting to note that the unique generalizedcollocations were learned from the training data by their matching different uniquewords from the ones they match in the test data.3.4 Generating Features from Document-Level Annotations Using DistributionalSimilarityIn this section, we identify adjective and verb PSEs using distributional similarity.Opinion-piece data are used for training, and (a different set of) opinion-piece dataand the subjective-element data are used for testing.With distributional similarity, words are judged to be more or less similar basedon their distributional patterning in text (Lee 1999; Lee and Pereira 1999).
OurTable 5Random sample of fixed-3-gram collocations in OP1.one-noun of-prep his-det worst-adj of-prep all-detquality-noun of-prep the-det to-prep do-verb so-adverbin-prep the-det company-noun you-pronoun and-conj your-pronounhave-verb taken-verb the-det rest-noun of-prep us-pronounare-verb at-prep least-adj but-conj if-prep you-pronounas-prep a-det weapon-noun continue-verb to-to do-verbpurpose-noun of-prep the-det could-modal have-verb be-verbit-pronoun seem-verb to-prep to-pronoun continue-verb to-prephave-verb be-verb the-det do-verb something-noun about-prepcause-verb you-pronoun to-to evidence-noun to-to back-adverbthat-prep you-pronoun are-verb i-pronoun be-verb not-adverbof-prep the-det century-noun of-prep money-noun be-prep291Wiebe, Wilson, Bruce, Bell, and Martin Learning Subjective LanguageTable 6Random sample of unique generalized collocations in OP1.
U: UNIQUE.Pattern InstancesU-adj as-prep: drastic as; perverse as; predatory asU-adj in-prep: perk in; unsatisfying in; unwise inU-adverb U-verb: adroitly dodge; crossly butter; unceasingly fascinateU-noun back-adverb: cutting back; hearken backU-verb U-adverb: coexist harmoniously; flouncing tiresomelyad-noun U-noun: ad hoc; ad valoremany-det U-noun: any over-payment; any tapings; any write-offare-verb U-noun: are escapist; are lowbrow; are resonancebut-conj U-noun: but belch; but cirrus; but ssadifferent-adj U-noun: different ambience; different subconferenceslike-prep U-noun: like hoffmann; like manute; like woodchucknational-adj U-noun: national commonplace; national yonhapparticularly-adverb U-adj: particularly galling; particularly noteworthyso-adverb U-adj: so monochromatic; so overbroad; so permissivethis-det U-adj: this biennial; this inexcusable; this scurrilousyour-pronoun U-noun: your forehead; your manuscript; your popcornU-adj and-conj U-adj: arduous and raucous; obstreperous and abstemiousU-noun be-verb a-det: acyclovir be a; siberia be aU-noun of-prep its-pronoun: outgrowth of its; repulsion of itsU-verb and-conj U-verb: wax and brushed; womanize and boozeU-verb to-to a-det: cling to a; trek to aare-verb U-adj to-to: are opaque to; are subject toa-det U-noun and-conj: a blindfold and; a rhododendron anda-det U-verb U-noun: a jaundice ipo; a smoulder sofait-pronoun be-verb U-adverb: it be humanly; it be sooothan-prep a-det U-noun: than a boob; than a menacethe-det U-adj and-conj: the convoluted and; the secretive andthe-det U-noun that-prep: the baloney that; the cachet thatto-to a-det U-adj: to a gory; to a trappistto-to their-pronoun U-noun: to their arsenal; to their subsistencewith-prep an-det U-noun: with an alias; with an avalanche292Computational Linguistics Volume 30, Number 3trainingPrec(s) is the precision of s in the training datavalidationPrec(s) is the precision of s in the validation datatestPrec(s) is the precision of s in the test data(similarly for trainingFreq, validationFreq, and testFreq)S = the set of all adjectives (verbs) in the training datafor T in [0.01,0.04,. .
.,0.70]:for n in [2,3,. .
.,40]:retained = {}For si in S:if trainingPrec({si} ?
Ci,n) > T:retained = retained ?
{si} ?
Ci,nRT,n = retainedADJpses = {} (VERBpses = {})for T in [0.01,0.04,. .
.,0.70]:for n in [2,3,. .
.,40]:if validationPrec(RT,n) ?
0.28 (0.23 for verbs)and validationFreq(RT,n) ?
100:ADJpses = ADJpses ?
RT,n (VERBpses = VERBpses ?
RT,n)Results in Table 7 show testPrec(ADJpses) and testFreq(ADJpses).Figure 2Algorithm for selecting adjective and verb features using distributional similarity.motivation for experimenting with it to identify PSEs was twofold.
First, we hypoth-esized that words might be distributionally similar because they share pragmatic us-ages, such as expressing subjectivity, even if they are not close synonyms.
Second,as shown above, low-frequency words appear more often in subjective texts than ex-pected.
We did not want to discard all low-frequency words from consideration butcannot effectively judge the suitability of individual words.
Thus, to decide whetherto retain a word as a PSE, we consider the precision not of the individual word, butof the word together with a cluster of words similar to it.Many variants of distributional similarity have been used in NLP (Lee 1999; Leeand Pereira 1999).
Dekang Lin?s (1998) method is used here.
In contrast to manyimplementations, which focus exclusively on verb-noun relationships, Lin?s methodincorporates a variety of syntactic relations.
This is important for subjectivity recogni-tion, because PSEs are not limited to verb-noun relationships.
In addition, Lin?s resultsare freely available.A set of seed words begins the process.
For each seed si, the precision of the set{si}?Ci,n in the training data is calculated, where Ci,n is the set of n words most similarto si, according to Lin?s (1998) method.
If the precision of {si} ?
Ci,n is greater than athreshold T, then the words in this set are retained as PSEs.
If it is not, neither si northe words in Ci,n are retained.
The union of the retained sets will be denoted RT,n, thatis, the union of all sets {si} ?
Ci,n with precision on the training set > T.In Wiebe (2000), the seeds (the sis) were extracted from the subjective-elementannotations in corpus WSJ-SE.
Specifically, the seeds were the adjectives that appearat least once in a subjective element in WSJ-SE.
In this article, the opinion piece corpusis used to move beyond the manual annotations and small corpus of the earlier work,and a much looser criterion is used to choose the initial seeds: All of the adjectives(verbs) in the training data are used.The algorithm for the process is given in Figure 2.
There is one small differencefor adjectives and verbs noted in the figure, that is, the precision threshold of 0.28 for293Wiebe, Wilson, Bruce, Bell, and Martin Learning Subjective LanguageTable 7Frequencies and increases in precision for adjective and verb features identifiedusing distributional similarity with filtering.
For each test data set, baselinefrequency is the total number of words, and baseline precision is the proportion ofwords in opinion pieces.Baseline ADJpses VERBpsesTraining Validation Test freq prec freq +prec freq +precW9-10 W9-22W9-22 W9-10 W9-33 153,634 .14 1,576 +.12 1,490 +.11W9-10 W9-33W9-33 W9-10 W9-22 155,135 .13 859 +.15 535 +.11W9-22 W9-33W9-33 W9-22 W9-10 156,334 .18 249 +.22 224 +.10All pairings of W9-10,W9-22,W9-33 W9-4 156,421 .19 1,872 +.17 1,777 +.15adjectives versus 0.23 for verbs.
These thresholds were determined using validationdata.Seeds and their clusters are assessed on a training set for many parameter settings(cluster size n from 2 through 40, and precision threshold T from 0.01 through 0.70by .03).
As mentioned above, each (n, T) parameter pair yields a set of adjectives RT,n,that is, the union of all sets {si}?Ci,n with precision on the training set > T. A subset,ADJpses, of those sets is chosen based on precision and frequency in a validation set.Finally, the ADJpses are tested on the test set.Table 7 shows the results for four opinion piece test sets.
Multiple training-validation data set pairs are used for each test set, as given in Table 7.
The resultsare for the union of the adjectives (verbs) chosen for each pair.
The freq columns givetotal frequencies, and the +prec columns show the improvements in precision fromthe baseline.
For each data set, the difference between the proportion of instancesof ADJpses in opinion pieces and the proportion in nonopinion pieces is significant(p < 0.001, z ?
9.2).
The same is true for VERBpses (p < 0.001, z ?
4.1).In the interests of testing consistency, Table 8 shows the results of assessing theadjective and verb features generated from opinion piece data (ADJpses and VERBpsesTable 8Average frequencies and increases in precision in subjective-element dataof the sets tested in Table 7.
The baselines are the precisions ofadjectives/verbs that appear in subjective elements in thesubjective-element data.Adj baseline Verb baseline ADJpses VERBpsesfreq prec freq prec freq +prec freq +precWSJ-SE-D 1,632 .13 2,980 .15 136 +.16 151 +.10WSJ-SE-M 1,632 .19 2,980 .12 136 +.24 151 +.13NG-SE 1,104 .37 2,629 .15 185 +.25 275 +.08294Computational Linguistics Volume 30, Number 3Table 9Frequencies and increases in precision for all features.
For each data set, baselinefrequency is the total number of words, and baseline precision is the proportion ofwords in opinion pieces.
freq: total frequency; +prec: increase in precision over baseline.W9-04 W9-10 W9-22 W9-33freq +prec freq +prec freq +prec freq +precUnique words 4794 +.15 4763 +.16 4274 +.11 4567 +.11Fixed-2-grams 1840 +.07 1972 +.07 1933 +.04 1839 +.05ugen-2-grams 281 +.21 256 +.26 261 +.17 254 +.17Fixed-3-grams 213 +.08 243 +.09 214 +.05 238 +.05ugen-3-grams 148 +.29 133 +.27 147 +.16 133 +.15Fixed-4-grams 18 +.15 17 +.06 12 +.29 14 ?.07ugen-4-grams 13 +.12 3 +.82 15 +.27 13 +.25Adjectives 1872 +.17 249 +.22 859 +.15 1576 +.12Verbs 1777 +.15 224 +.10 535 +.11 1490 +.11Baseline 156421 .19 156334 .18 155135 .13 153634 .14in Table 7) on the subjective-element data.
The left side of the table gives baselinefigures for each set of subjective-element annotations.
The right side of the table givesthe average frequencies and increases in precision over baseline for the ADJpses andVERBpses sets on the subjective-element data.
The baseline figures in the table are thefrequencies and precisions of the sets of adjectives and verbs that appear at least oncein a subjective element.
Since these sets include words that appear just once in thecorpus (and thus have 100% precision), the baseline precision is a challenging one.Testing the VERBpses and ADJpses on the subjective-element data reveals some inter-esting consistencies for these subjectivity clues.
The precision increases of the VERBpseson the subjective-element data are comparable to their increases on the opinion piecedata.
Similarly, the precision increases of the ADJpses on the subjective-element dataare as good as or better than the performance of this set of PSEs on the opinion piecedata.
Finally, the precisions increases for the ADJpses are higher than for the VERBpseson all data sets.
This is again consistent with the higher performance of the ADJpsessets in the opinion piece data sets.4.
Features Used in Concert4.1 IntroductionIn this section, we examine the various types of clues used together.
In preparation forthis work, all instances in OP1 and OP2 of all of the PSEs identified as described inSection 3 have been automatically identified.
All training to define the PSE instancesin OP1 was performed on data separate from OP1, and all training to define the PSEinstances in OP2 was performed on data separate from OP2.4.2 Consistency in Precision among Data SetsTable 9 summarizes the results from previous sections in which the opinion piece dataare used for testing.
The performance of the various features is consistently good orbad on the same data sets: the performance is better for all features on W9-10 andW9-04 than on W9-22 and W9-33 (except for the ugen-4-grams, which occur with verylow frequency, and the verbs, which have low frequency in W9-10).
This is so despitethe fact that the features were generated using different procedures and data: The295Wiebe, Wilson, Bruce, Bell, and Martin Learning Subjective Language0.
PSEs = all adjs, verbs, modals, nouns, and adverbs that appear at leastonce in an SE (except not, will, be, have).1.
PSEinsts = the set of all instances of PSEs2.
HiDensity = {}3.
For P in PSEinsts:4. leftWin(P) = the W words before P5.
rightWin(P) = the W words after P6.
density(P) = number of SEs whose first or lastword is in leftWin(P) or rightWin(P)7. if density(P) ?
T:HiDensity = HiDensity ?
{P}8. prec(PSEinsts) =number of PSEinsts in subject elements|PSEinsts|9.
prec(HiDensity) =number of HiDensity in subject elements|HiDensity|Figure 3Algorithm for calculating density in subjective-element data.adjectives and verbs were generated from WSJ document-level opinion piece classifi-cations; the n-gram features were generated from newsgroup and WSJ expression-levelsubjective-element classifications; and the unique unigram feature requires no training.This consistency in performance suggests that the results are not brittle.4.3 Choosing Density Parameters from Subjective-Element DataIn Wiebe (1994), whether a PSE is interpreted to be subjective depends, in part, onhow subjective the surrounding context is.
We explore this idea in the current work,assessing whether PSEs are more likely to be subjective if they are surrounded by sub-jective elements.
In particular, we experiment with a density feature to decide whetheror not a PSE instance is subjective: If a sufficient number of subjective elements arenearby, then the PSE instance is considered to be subjective; otherwise, it is discarded.The density parameters are a window size W and a frequency threshold T.In this section, we explore the density of manually annotated PSEs in subjective-element data and choose density parameters to use in Section 4.4, in which we applythem to automatically identified PSEs in opinion piece data.The process for calculating density in the subjective-element data is given in Fig-ure 3.
The PSEs are defined to be all adjectives, verbs, modals, nouns, and adverbs thatappear at least once in a subjective element, with the exception of some stop words(line 0 of Figure 3).
Note that these PSEs depend only on the subjective-element man-ual annotations, not on the automatically identified features used elsewhere in thearticle or on the document-level opinion piece classes.
PSEinsts is the set of PSEinstances to be disambiguated (line 1).
HiDensity (initialized on line 2) will be thesubset of PSEinsts that are retained.
In the loop, the density of each PSE instanceP is calculated.
This is the number of subjective elements that begin or end in theW words preceding or following P (line 6).
P is retained if its density is at least T(line 7).Lines 8?9 of the algorithm assess the precision of the original (PSEinsts) and new(HiDensity) sets of PSE instances.
If prec(HiDensity) is greater than prec(PSEinsts), then296Computational Linguistics Volume 30, Number 3Table 10Most frequent entry in the top three precision intervals for eachsubjective-element data set.WSJ-SE1-M WSJ-SE1-D WSJ-SE2-M WSJ-SE2-D NG-SEBaseline freq 1,566 1,245 1,167 1,108 3,303Baseline prec .49 .47 .41 .36 .51Range .87?.92 .95?1.0 .95?1.0 .95?1.0 .95?1.0T, W 10, 20 12, 50 20, 50 14, 100 10, 10freq 76 12 1 1 3prec .89 1.0 1.0 1.0 1.0Range .82?.87 .90?.95 .73?.78 .51?.56 .67?.72T, W 6, 10 12, 60 46, 190 22, 370 26, 90freq 63 22 53 221 664prec .84 .91 .78 .51 .67Range .77?.82 .84?.89 .66?.71 .46?.51 .63?.67T, W 12, 40 12, 80 18, 60 16, 310 8, 30freq 292 42 53 358 1504prec .78 .88 .68 .47 .63there is evidence that the number of subjective elements near a PSE instance is relatedto its subjectivity in context.To create more data points for this analysis, WSJ-SE was split into two (WSJ-SE1and WSJ-SE2) and annotations of the two judges are considered separately.
WSJ-SE2-D,for example, refers to D?s annotations of WSJ-SE2.
The process in Figure 3 was repeatedfor different parameter settings (T in [1, 2, 4, .
.
.
, 48] and W in [1, 10, 20, .
.
.
, 490]) on eachof the SE data sets.
To find good parameter settings, the results for each data set weresorted into five-point precision intervals and then sorted by frequency within eachinterval.
Information for the top three precision intervals for each data set are shownin Table 10, specifically, the parameter values (i.e., T and W) and the frequency andprecision of the most frequent result in each interval.
The intervals are in the rowslabeled Range.
For example, the top three precision intervals for WSJ-SE1-M, 0.87-0.92,0.82-0.87, and 0.77-0.82 (no parameter values yield higher precision than 0.92).
Thetop of Table 10 gives baseline frequencies and precisions, which are |PSEinsts| andprec(PSEinsts), respectively, in line 8 of Figure 3.The parameter values exhibit a range of frequencies and precisions, with the ex-pected trade-off between precision and frequency.
We choose the following parametersto test in Section 4.4: For each data set, for each precision interval whose lower boundis at least 10 percentage points higher than the baseline for that data set, the toptwo (T, W) pairs yielding the highest frequencies in that interval are chosen.
Amongthe five data sets, a total of 45 parameter pairs were so selected.
This exercise wascompleted once, without experimenting with different parameter settings.4.4 Density for DisambiguationIn this section, density is exploited to find subjective instances of automatically iden-tified PSEs.
The process is shown in Figure 4.
There are only two differences betweenthe algorithms in Figures 3 and 4.
First, in Figure 3, density is defined in terms ofthe number of subjective elements nearby.
However, subjective-element annotationsare not available in test data.
Thus in Figure 4, density is defined in terms of the297Wiebe, Wilson, Bruce, Bell, and Martin Learning Subjective Language0.
PSEinsts = the set of instances in the testdata of all PSEs described in Section 31.
HiDensity = {}2.
For P in PSEinsts:3. leftWin(P) = the W words before P4.
rightWin(P) = the W words after P5.
density(P) = number of PSEinsts whose first or lastword is in leftWin(P) or rightWin(P)6. if density(P) ?
T:HiDensity = HiDensity ?
{P}7. prec(PSEinsts) = # of PSEinsts in OPs|PSEinsts|8.
prec(HiDensity) = # of HiDensity in OPs|HiDensity|Figure 4Algorithm for calculating density in opinion piece (OP) datanumber of other PSE instances nearby, where PSEinsts consists of all instances of theautomatically identified PSEs described in Section 3, for which results are given inTable 9.Second, in Figure 4, we assess precision with respect to the document-level classes(lines 7?8).
The test data are OP1.An interesting question arose when we were defining the PSE instances: Whatshould be done with words that are identified to be PSEs (or parts of PSEs) accordingto multiple criteria?
For example, sunny, radiant, and exhilarating are all unique incorpus OP1, and are all members of the adjective PSE feature defined for testingon OP1.
Collocations add additional complexity.
For example, consider the sequenceand splendidly, which appears in the test data.
The sequence and splendidly matchesthe ugen-2-gram (and-conj U-adj), and the word splendidly is unique.
In addition, asequence may match more than one n-gram feature.
For example, is it that matchesthree fixed-n-gram features: is it, is it that, and it that.In the current experiments, the more PSEs a word matches, the more weight itis given.
The hypothesis behind this treatment is that additional matches representadditional evidence that a PSE instance is subjective.
This hypothesis is realized asfollows: Each match of each member of each type of PSE is considered to be a PSEinstance.
Thus, among them, there are 11 members in PSEinsts for the five phrasessunny, radiant, exhilarating, and splendidly, and is it that, one for each of the matchesmentioned above.The process in Figure 4 was conducted with the 45 parameter pair values (T andW) chosen from the subjective-element data as described in Section 4.3.
Table 11 showsresults for a subset of the 45 parameters, namely, the most frequent parameter pairchosen from the top three precision intervals for each training set.
The bottom of thetable gives a baseline frequency and a baseline precision in OP1, defined as |PSEinsts|and prec(PSEinsts), respectively, in line 7 of Figure 4.The density features result in substantial increases in precision.
Of the 45 parameterpairs, the minimum percentage increase over baseline is 22%.
Fully 24% of the 45parameter pairs yield increases of 200% or more; 38% yield increases between 100%298Computational Linguistics Volume 30, Number 3Table 11Results for high-density PSEs in test data OP1 using parameters chosenfrom subjective-element data.WSJ-SE1-M WSJ-SE1-D WSJ-SE2-M WSJ-SE2-D NG-SET, W 10, 20 12, 50 20, 50 14, 100 10, 10freq 237 3,176 170 10,510 8prec .87 .72 .97 .57 1.0T, W 6, 10 12, 60 46, 190 22, 370 26, 90freq 459 5,289 1,323 21,916 787prec .68 .68 .95 .37 .92T, W 12, 40 12, 80 18, 60 16, 310 8, 30freq 1,398 9,662 906 24,454 3,239prec .79 .58 .87 .34 .67PSE baseline: freq = 30,938, prec = .28and 199%, and 38% yield increases between 22% and 99%.
In addition, the increasesare significant.
Using the set of high-density PSEs defined by the parameter pair withthe least increase over baseline, we tested the difference in the proportion of PSEsin opinion pieces that are high-density and the proportion of PSEs in nonopinionpieces that are high-density.
The difference between these two proportions is highlysignificant (z = 46.2, p < 0.0001).Notice that, except for one blip (T, W = 6, 10 under WSJ-SE-M), the precisionsdecrease and the frequencies increase as we go down each column in Table 11.
Thesame pattern can be observed with all 45 parameter pairs (results not included herebecause of space considerations).
But the parameter pairs are ordered in Table 11based on performance in the manually annotated subjective-element data, not basedon performance in the test data.
For example, the entry in the first row, first column(T, W = 10, 20) is the parameter pair giving the highest frequency in the top precisioninterval of WSJ-SE-M (frequency and precision in WSJ-SE-M, using the process ofFigure 3).
Thus, the relative precisions and frequencies of the parameter pairs arecarried over from the training to the test data.
This is quite a strong result, given thatthe PSEs in the training data are from manual annotations, while the PSEs in the testdata are our automatically identified features.4.5 High-Density Sentence AnnotationsTo assess the subjectivity of sentences with high-density PSEs, we extracted the 133sentences in corpus OP2 that contain at least one high-density PSE and manuallyannotated them.
We refer to these sentences as the system-identified sentences.We chose the density-parameter pair (T, W = 12, 30), based on its precision andfrequency in OP1.
This parameter setting yields results that have relatively high pre-cision and low frequency.
We chose a low-frequency setting to make the annotationstudy feasible.The extracted sentences were independently annotated by two judges.
One is acoauthor of this article (judge 1), and the other has performed subjectivity annota-tion before, but is not otherwise involved in this research (judge 2).
Sentences wereannotated according to the coding instructions of Wiebe, Bruce, and O?Hara (1999)which, recall, are to classify a sentence as subjective if there is a significant expressionof subjectivity of either the writer or someone mentioned in the text, in the sentence.299Wiebe, Wilson, Bruce, Bell, and Martin Learning Subjective LanguageTable 12Examples of system-identified sentences.
(1) The outburst of shooting came nearly two weeks after clashes between Moslem worshippers and ooSomali soldiers.
(2.a) But now the refugees are streaming across the border and alarming the world.
ss(2.b) In the middle of the crisis, Erich Honecker was hospitalized with a gall stone operation.
oo(2.c) It is becoming more and more obvious that his gallstone-age communism is dying with him: .
.
.
ss(3.a) Not brilliantly, because, after all, this was a performer who was collecting paychecks from lounges ssat Hiltons and Holiday Inns, but creditably and with the air of someone for whom?Ten Cents a Dance?
was more than a bit autobiographical.
(3.b) ?It was an exercise of blending Michelle?s singing with Susie?s singing,?
explained Ms. Stevens.
oo(4) Enlisted men and lower-grade officers were meat thrown into a grinder.
ss(5) ?If you believe in God and you believe in miracles, there?s nothing particularly crazy about that.?
ss(6) He was much too eager to create ?something very weird and dynamic,?
ss?catastrophic and jolly?
like ?this great and coily thing?
?Lolita.?
(7) The Bush approach of mixing confrontation with conciliation strikes some people as sensible, perhaps sseven inevitable, because Mr. Bush faces a Congress firmly in the hands of the opposition.
(8) Still, despite their efforts to convince the world that we are indeed alone, the visitors do seem to keep sscoming and, like the recent sightings, there?s often a detail or two that suggests they mayactually be a little on the dumb side.
(9) As for the women, they?re pathetic.
ss(10) At this point, the truce between feminism and sensationalism gets might uneasy.
ss(11) MMPI?s publishers say the test shouldn?t be used alone to diagnose sspsychological problems or in hiring; it should be given in conjunction with other tests.
(12) While recognizing that professional environmentalists may feel threatened, ssI intend to urge that UV-B be monitored whenever I can.Table 13Sentence annotation contingency table; judge 1 counts are in rows andjudge 2 counts are in columns.Subjective Objective UnsureSubjective 98 2 3Objective 2 14 0Unsure 2 11 1In addition to the subjective and objective classes, a judge can tag a sentence as unsureif he or she is unsure of his or her rating or considers the sentence to be borderline.An equal number (133) of other sentences were randomly selected from the corpusto serve as controls.
The 133 system-identified sentences and the 133 control sentenceswere randomly mixed together.
The judges were asked to annotate all 266 sentences,not knowing which were system-identified and which were control.
Each sentencewas presented with the sentence that precedes it and the sentence that follows it inthe corpus, to provide some context for interpretation.Table 12 shows examples of the system-identified sentences.
Sentences classifiedby both judges as objective are marked oo and those classified by both judges assubjective are marked ss.300Computational Linguistics Volume 30, Number 3Table 14Examples of subjective sentences adjacent to system-identified sentences.Bathed in cold sweat, I watched these Dantesque scenes, holding tightly thedamp hand of Edek or Waldeck who, like me, were convinced that there was no God.
?The Japanese are amazed that a company like this exists in Japan,?
says KimindoKusaka, head of the Softnomics Center, a Japanese management-research organization.And even if drugs were legal, what evidence do you have that the habitual drug userwouldn?t continue to rob and steal to get money for clothes, food or shelter?The moral cost of legalizing drugs is great, but it is a cost that apparently liesoutside the narrow scope of libertarian policy prescriptions.I doubt that one exists.They were upset at his committee?s attempt to pacify the program critics bycutting the surtax paid by the more affluent elderly and making up the loss byshifting more of the burden to the elderly poor and by delaying some benefits by a year.Judge 1 classified 103 of the system-identified sentences as subjective, 16 as ob-jective, and 14 as unsure.
Judge 2 classified 102 of the system-identified sentences assubjective, 27 as objective; and 4 as unsure.
The contingency table is given in Table 13.4The kappa value using all three classes is 0.60, reflecting the highly skewed distri-bution in favor of subjective sentences, and the disagreement on the lower-frequencyclasses (unsure and objective).
Consistent with the findings in Wiebe, Bruce, andO?Hara (1999), the kappa value for agreement on the sentences for which neitherjudge is unsure is very high: 0.86.A different breakdown of the sentences is illuminating.
For 98 of the sentences (callthem SS), judges 1 and 2 tag the sentence as subjective.
Among the other sentences, 20appear in a block of contiguous system-identified sentences that includes a member ofSS.
For example, in Table 12, (2.a) and (2.c) are in SS and (2.b) is in the same block ofsubjective sentences as they are.
Similarly, (3.a) is in SS and (3.b) is in the same block.Among the remaining 15 sentences, 6 are adjacent to subjective sentences thatwere not identified by our system (so were not annotated by the judges).
All of thosesentences contain significant expressions of subjectivity of the writer or someone men-tioned in the text, the criterion used in this work for classifying a sentence as subjective.Samples are shown in Table 14.Thus, 93% of the sentences identified by the system are subjective or are nearsubjective sentences.
All the sentences, together with their tags and the sentencesadjacent to them, are available on the Web at www.cs.pitt.edu/?
wiebe.4.6 Using Features for Opinion Piece RecognitionIn this section, we assess the usefulness of the PSEs identified in Section 3 and listedin Table 9 by using them to perform document-level classification of opinion pieces.Opinion-piece classification is a difficult task for two reasons.
First, as discussed in Sec-tion 2.1, both opinionated and factual documents tend to be composed of a mixture ofsubjective and objective language.
Second, the natural distribution of documents in ourdata is heavily skewed toward nonopinion pieces.
Despite these hurdles, using only4 In contrast, Judge 1 classified only 53 (45%) of the control sentences as subjective, and Judge 2classified only 47 (36%) of them as subjective.301Wiebe, Wilson, Bruce, Bell, and Martin Learning Subjective Languageour PSEs, we achieve positive results in opinion-piece classification using the basic k-nearest-neighbor (KNN) algorithm with leave-one-out cross-validation (Mitchell 1997).Given a document, the basic KNN algorithm classifies the document according tothe majority classification of the document?s k closest neighbors.
For our purposes, eachdocument is characterized by one feature, the count of all PSE instances (regardlessof type) in the document, normalized by document length in words.
The distancebetween two documents is simply the absolute value of the difference between thenormalized PSE counts for the two documents.With leave-one-out cross-validation, the set of n documents to be classified isdivided into a training set of size n?1 and a validation set of size 1.
The one documentin the validation set is then classified according to the majority classification of its kclosest-neighbor documents in the training set.
This process is repeated until everydocument is classified.Which value to use for k is chosen during a preprocessing phase.
During the pre-processing phase, we run the KNN algorithm with leave-one-out cross-validation ona separate training set, for odd values of k from 1 to 15.
The value of k that results inthe best classification during the preprocessing phase is the one used for later KNNclassification.For the classification experiment, the data set OP1 was used in the preprocess-ing phase to select the value of k, and then classification was performed on the 1,222documents in OP2.
During training on OP1, k equal to 15 resulted in the best classifi-cation.
On the test set, OP2, we achieved a classification accuracy of 0.939; the baselineaccuracy for choosing the most frequent class (nonopinion pieces) was 0.915.
Our clas-sification accuracy represents a 28% reduction in error and is significantly better thanbaseline according to McNemar?s test (Everitt 1997).The positive results from the opinion piece classification show the usefulness ofthe various PSE features when used together.5.
Relation to Other WorkThere has been much work in other fields, including linguistics, literary theory, psy-chology, philosophy, and content analysis, involving subjective language.
As men-tioned in Section 2, the conceptualization underlying our manual annotations is basedon work in literary theory and linguistics, most directly Dolez?el (1973), Uspensky(1973), Kuroda (1973, 1976), Chatman (1978), Cohn (1978), Fodor (1979), and Banfield(1982).
We also mentioned existing knowledge resources such as affective lexicons(General-Inquirer 2000; Heise 2000) and annotations in more general-purpose lexicons(e.g., the attitude adverb features in Comlex [Macleod, Grishman, and Meyers 1998]).Such knowledge may be used in future work to complement the work presented inthis article, for example, to seed the distributional-similarity process described in Sec-tion 3.4.There is also work in fields such as content analysis and psychology on statisti-cally characterizing texts in terms of word lists manually developed for distinctionsrelated to subjectivity.
For example, Hart (1984) performs counts on a manually de-veloped list of words and rhetorical devices (e.g., ?sacred?
terms such as freedom)in political speeches to explore potential reasons for public reactions.
Anderson andMcMaster (1998) use fixed sets of high-frequency words to assign connotative scoresto documents and sections of documents along dimensions such as how pleasant,acrimonious, pious, or confident, the text is.What distinguishes our work from work on subjectivity in other fields is thatwe focus on (1) automatically learning knowledge from corpora, (2) automatically302Computational Linguistics Volume 30, Number 3performing contextual disambiguation, and (3) using knowledge of subjectivity inNLP applications.
This article expands and integrates the work reported in Wiebe andWilson (2002), Wiebe, Wilson, and Bell (2001), Wiebe et al (2001) and Wiebe (2000).Previous work in NLP on the same or related tasks includes sentence-level anddocument-level subjectivity classifications.
At the sentence level, Wiebe, Bruce, andO?Hara (1999) developed a machine learning system to classify sentences as subjec-tive or objective.
The accuracy of the system was more than 20 percentage pointshigher than a baseline accuracy.
Five part-of-speech features, two lexical features, anda paragraph feature were used.
These results suggested to us that there are clues tosubjectivity that might be learned automatically from text and motivated the workreported in the current article.
The system was tested in 10-fold cross validation ex-periments using corpus WSJ-SE, a small corpus of only 1,001 sentences.
As discussedin Section 1, a main goal of our current work is to exploit existing document-levelannotations, because they enable us to use much larger data sets, they were createdoutside our research group, and they allow us to assess consistency of performanceby cross-validating between our manual annotations and the existing document-levelannotations.
Because the document-level data are not annotated at the sentence level,sentence-level classification is not highlighted in this article.
The new sentence annota-tion study to evaluate sentences with high-density features (Section 4.5) uses differentdata from WSJ-SE, because some of the features (n-grams and density parameters)were identified using WSJ-SE as training data.Other previous work in NLP has addressed related document-level classifications.Spertus (1997) developed a system for recognizing inflammatory messages.
As men-tioned earlier in the article, inflammatory language is a type of subjective language,so the task she addresses is closely related to ours.
She uses machine learning toselect among manually developed features.
In contrast, the focus in our work is onautomatically identifying features from the data.A number of projects investigating genre detection include editorials as one of thetargeted genres.
For example, in Karlgren and Cutting (1994), editorials are one of fif-teen categories, and in Kessler, Nunberg, and Schu?tze (1997), editorials are one of six.Given the goal of these works to perform genre detection in general, they use low-levelfeatures that are not specific to editorials.
Neither shows significant improvements foreditorial recognition.
Argamon, Koppel, and Avneri (1998) address a slightly differenttask, though it does involve editorials.
Their goal is to distinguish not only, for ex-ample, news from editorials, but also these categories in different publications.
Theirbest results are distinguishing among the news categories of different publications;their lowest results involve editorials.
Because we focus specifically on distinguishingopinion pieces from nonopinion pieces, our results are better than theirs for thosecategories.
In addition, in contrast to the above studies, the focus of our work is onlearning features of subjectivity.
We perform opinion piece recognition in order toassess the usefulness of the various features when used together.Other previous NLP research has used features similar to ours for other NLP tasks.Low-frequency words have been used as features in information extraction (Weeber,Vos, and Baayen 2000) and text categorization (Copeck et al 2000).
A number ofresearchers have worked on mining collocations from text to extend lexicographicresources for machine translation and word sense disambiguation (e.g., Smajda 1993;Lin 1999; Biber 1993).In Samuel, Carberry, and Vijay-Shanker?s (1998) work on identifying collocationsfor dialog-act recognition, a filter similar to ours was used to eliminate redundantn-gram features: n-grams were eliminated if they contained substrings with the sameentropy score as or a better entropy score than the n-gram.303Wiebe, Wilson, Bruce, Bell, and Martin Learning Subjective LanguageWhile it is common in studies of collocations to omit low-frequency words andexpressions from analysis, because they give rise to invalid or unrealistic statisticalmeasures (Church and Hanks, 1990), we are able to identify higher-precision colloca-tions by including placeholders for unique words (i.e., the ugen-n-grams).
We are notaware of other work that uses such collocations as we do.Features identified using distributional similarity have previously been used forsyntactic and semantic disambiguation (Hindle 1990; Dagan, Pereira, and Lee 1994)and to develop lexical resources from corpora (Lin 1998; Riloff and Jones 1999).We are not aware of other work identifying and using density parameters asdescribed in this article.Since our experiments, other related work in NLP has been performed.
Some ofthis work addresses related but different classification tasks.
Three studies classifyreviews as positive or negative (Turney 2002; Pang, Lee, and Vaithyanathan 2002;Dave, Lawrence, Pennock 2003).
The input is assumed to be a review, so this taskdoes not include finding subjective documents in the first place.
The first study listedabove (Turney 2002) uses a variation of the semantic similarity procedure presentedin Wiebe (2000) (Section 3.4).
The third (Dave, Lawrence, and Pennock 2003) uses n-gram features identified with a variation of the procedure presented in Wiebe, Wilson,and Bell (2001) (Section 3.3).
Tong (2001) addresses finding sentiment timelines, thatis, tracking sentiments over time in multiple documents.
For clues of subjectivity, heuses manually developed lexical rules, rather than automatically learning them fromcorpora.
Similarly, Gordon et al (2003) use manually developed grammars to detectsome types of subjective language.
Agrawal et al (2003) partition newsgroup authorsinto camps based on quotation links.
They do not attempt to recognize subjectivelanguage.The most closely related new work is Riloff, Wiebe, and Wilson (2003), Riloffand Wiebe (2003) and Yu and Hatzivassiloglou (2003).
The first two focus on findingadditional types of subjective clues (nouns and extraction patterns identified usingextraction pattern bootstrapping).
Yu and Hatzivassiloglou (2003) perform opinion textclassification.
They also use existing WSJ document classes for training and testing,but they do not include the entire corpus in their experiments, as we do.
Their opinionpiece class consists only of editorials and letters to the editor, and their nonopinionclass consists only of business and news.
They report an average F-measure of 96.5%.Our result of 94% accuracy on document level classification is almost comparable.They also perform sentence-level classification.We anticipate that knowledge of subjective language may be usefully exploited ina number of NLP application areas and hope that the work presented in this article willencourage others to experiment with subjective language in their applications.
Moregenerally, there are many types of artificial intelligence systems for which state-of-affairs types such as beliefs and desires are central, including systems that perform planrecognition for understanding narratives (Dyer 1982; Lehnert et al 1983), for argumentunderstanding (Alvarado, Dyer, and Flowers 1986), for understanding stories fromdifferent perspectives (Carbonell 1979), and for generating language under differentpragmatic constraints (Hovy 1987).
Knowledge of linguistic subjectivity could enhancethe abilities of such systems to recognize and generate expressions referring to suchstates of affairs in natural text.6.
ConclusionsKnowledge of subjective language promises to be beneficial for many NLP applica-tions including information extraction, question answering, text categorization, and304Computational Linguistics Volume 30, Number 3summarization.
This article has presented the results of an empirical study in ac-quiring knowledge of subjective language from corpora in which a number of fea-ture types were learned and evaluated on different types of data with positive re-sults.We showed that unique words are subjective more often than expected and thatunique words are valuable clues to subjectivity.
We also presented a procedure for au-tomatically identifying potentially subjective collocations, including fixed collocationsand collocations with placeholders for unique words.
In addition, we used the resultsof a method for clustering words according to distributional similarity (Lin 1998) toidentify adjectival and verbal clues of subjectivity.Table 9 summarizes the results of testing all of the above types of PSEs.
All showincreased precision in the evaluations.
Together, they show consistency in performance.In almost all cases they perform better or worse on the same data sets, despite thefact that different kinds of data and procedures are used to learn them.
In addition,PSEs learned using expression-level subjective-element data have precisions higherthan baseline on document-level opinion piece data, and vice versa.Having a large stable of PSEs, it was important to disambiguate whether or notPSE instances are subjective in the contexts in which they appear.
We discovered thatthe density of other potentially subjective expressions in the surrounding context isimportant.
If a clue is surrounded by a sufficient number of other clues, then it ismore likely to be subjective than if there were not.
Parameter values were selectedusing training data manually annotated at the expression level for subjective elementsand then tested on data annotated at the document level for opinion pieces.
All ofthe selected parameters led to increases in precision on the test data, and most lead toincreases over 100%.
Once again we found consistency between expression-level anddocument-level annotations.
PSE sets defined by density have high precision in boththe subjective-element data and the opinion piece data.
The large differences betweentraining and testing suggest that our results are not brittle.Using a density feature selected from a training set, sentences containing high-density PSEs were extracted from a separate test set, and manually annotated by twojudges.
Fully 93% of the sentences extracted were found to be subjective or to be nearsubjective sentences.
Admittedly, the chosen density feature is a high-precision, low-frequency one.
But since the process is fully automatic, the feature could be applied tomore unannotated text to identify regions containing subjective sentences.
In addition,because the precision and frequency of the density features are stable across data sets,lower-precision but higher-frequency options are available.Finally, the value of the various types of PSEs was demonstrated with the taskof opinion piece classification.
Using the k-nearest-neighbor classification algorithmwith leave-one-out cross-validation, a classification accuracy of 94% was achieved ona large test set, with a reduction in error of 28% from the baseline.Future work is required to determine how to exploit density features to improvethe performance of text categorization algorithms.
Another area of future work issearching for clues to objectivity, such as the politeness features used by Spertus (1997).Still another is identifying the type of a subjective expression (e.g., positive or neg-ative evaluative), extending work such as Hatzivassiloglou and McKeown (1997) onclassifying lexemes to the classification of instances in context (compare, e.g., ?great!
?and ?oh great.?
)In addition, it would be illuminating to apply our system to data annotated withdiscourse trees (Carlson, Marcu, and Okurowski 2001).
We hypothesize that most ob-jective sentences identified by our system are dominated in the discourse by subjectivesentences and that we are moving toward identifying subjective discourse segments.305Wiebe, Wilson, Bruce, Bell, and Martin Learning Subjective LanguageAcknowledgmentsWe thank the anonymous reviewers fortheir helpful and constructive comments.This research was supported in part by theOffice of Naval Research under grantsN00014-95-1-0776 and N00014-01-1-0381.ReferencesAgrawal, Rakesh, Sridhar Rajagopalan,Ramakrishnan Srikant, and Yirong Xu.2003.
Mining newsgroups using networksarising from social behavior.
InProceedings of the 12th International WorldWide Web Conference (WWW2003),Budapest, May 20-24.Alvarado, Sergio J., Michael G. Dyer, andMargot Flowers.
1986.
Editorialcomprehension in oped throughargument units.
In Proceedings of the FifthNational Conference on Artificial Intelligence(AAAI-86), Philadelphia, August 11?15,pages 250?256.Anderson, Clifford W. and George C.McMaster.
1989.
Quantification ofrewriting by the Brothers Grimm: Acomparison of successive versions ofthree tales.
Computers and the Humanities,23(4?5):341?346.Aone, Chinatsu, Mila Ramos-Santacruz, andWilliam J. Niehaus.
2000.
Assentor: AnNLP-based solution to e-mail monitoring.In Proceedings of the 12th InnovativeApplications of Artificial IntelligenceConference (IAAI-2000), Austin, TX,August 1?3, pages 945?950.Argamon, Shlomo, Moshe Koppel, andGalit Avneri.
1998.
Routing documentsaccording to style.
In Proceedings of theFirst International Workshop on InnovativeInternet Information Systems (IIIS-98), Pisa,Italy, June 8?9.Banfield, Ann.
1982.
Unspeakable Sentences.Routledge and Kegan Paul, Boston.Barzilay, Regina, Michael Collins, JuliaHirschberg, and Steve Whittaker.
2000.The rules behind roles: Identifyingspeaker role in radio broadcasts.
InProceedings of the 17th National Conference onArtificial Intelligence (AAAI-2000), Austin,TX, July 30?August 3, pages 679?684.Biber, Douglas.
1993.
Co-occurrrencepatterns among collocations: A tool forcorpus-based lexical knowledgeacquisition.
Computational Linguistics,19(3):531?538.Brill, Eric.
1992.
A simple rule-based part ofspeech tagger.
In Proceedings of the 3rdConference on Applied Natural LanguageProcessing (ANLP-92), Trenton, Italy, April1?3 pages 152?155.Bruce, Rebecca and Janyce Wiebe.
1999.Recognizing subjectivity: A case study ofmanual tagging.
Natural LanguageEngineering, 5(2):187?205.Carbonell, Jaime G. 1979.
SubjectiveUnderstanding: Computer Models of BeliefSystems.
Ph.D. thesis, and TechnicalReport no.
150, Department of ComputerScience, Yale University, New Haven, CT.Carlson, Lynn, Daniel Marcu, andMary Ellen Okurowski.
2001.
Building adiscourse-tagged corpus in theframework of rhetorical structure theory.In Proceedings of the Second SIG dialWorkshop on Discourse and Dialogue(SIGdial-2001), Aalborg, Denmark,September 1?2, pages 30?39.Chatman, Seymour.
1978.
Story andDiscourse: Narrative Structure in Fiction andFilm.
Cornell University Press, Ithaca, NY.Church, Kenneth W. and Patrick Hanks.1990.
Word association norms, mutualinformation, and lexicography.Computational Linguistics, 16:22?29.Cohn, Dorrit.
1978.
Transparent Minds:Narrative Modes for RepresentingConsciousness in Fiction.
PrincetonUniversity Press, Princeton, NJ.Copeck, Terry, Kim Barker, Sylvain Delisle,and Stan Szpakowicz.
2000.
Automatingthe measurement of linguistic features tohelp classify texts as technical.
InProceedings of the Seventh Conference onAutomatic NLP (TALN-2000), Lausanne,Switzerland, October 16?18, pages101?110.Dagan, Ido, Fernando Pereira, and LillianLee.
1994.
Similarity-based estimation ofword cooccurrence probabilities.
InProceedings of the 32nd Annual Meeting of theAssociation for Computational Linguistics(ACL-94), Las Cruces, NM, June 27?30,pages 272?278.Dave, Kushal, Steve Lawrence, andDavid M. Pennock.
2003.
Mining thepeanut gallery: Opinion extraction andsemantic classification of producereviews.
In Proceedings of the 12thInternational World Wide Web Conference(WWW2003), Budapest, May 20?24.Dolez?el, Lubomir.
1973.
Narrative Modes inCzech Literature.
University of TorontoPress, Toronto, Ontario, Canada.Dyer, Michael G. 1982.
Affect processing fornarratives.
In Proceedings of the SecondNational Conference on Artificial Intelligence(AAAI-82), Pittsburgh, August 18?20,pages 265?268.Everitt, Brian S. 1977.
The Analysis ofContingency Tables.
Chapman and Hall,London.306Computational Linguistics Volume 30, Number 3Fludernik, Monika.
1993.
The Fictions ofLanguage and the Languages of Fiction.Routledge, London.Fodor, Janet Dean.
1979.
The LinguisticDescription of Opaque Contexts, volume 13of Outstanding Dissertations in Linguistics.Garland, New York and London.General-Inquirer, The.
2000.
Available athttp://www.wjh.harvard.edu/?inquirer/spreadsheet guide.htm.Gordon, Andrew, Abe Kazemzadeh, AnishNair, and Milena Petrova.
2003.Recognizing expressions of commonsensepsychology in English text.
In Proceedingsof the 41st Annual Meeting of the Associationfor Computational Linguistics (ACL-03),Sapporo, Japan, July 7?12, pages 208?215.Hart, Roderick P. 1984.
Systematic analysisof political discourse: The development ofdiction.
In K. Sanders et al, editors,Political Communication Yearbook: 1984.Southern Illinois University Press,Carbondale, pages 97?134.Hatzivassiloglou, Vasileios and KathyMcKeown.
1997.
Predicting the semanticorientation of adjectives.
In Proceedings ofthe 35th Annual Meeting of the Association forComputational Linguistics (ACL-97),Madrid, July 12, pages 174?181.Heise, David.
2000.
Affect control theory.Available athttp://www.indiana.edu/socpsy/ACT/index.htm.Hindle, Don.
1990.
Noun classification frompredicate-argument structures.
InProceedings of the 28th Annual Meeting of theAssociation for Computational Linguistics(ACL-90), Pittsburgh, June 6?9, pages268?275.Hovy, Eduard.
1987.
Generating NaturalLanguage under Pragmatic Constraints.
Ph.D.thesis, Yale University, New Haven, CT.Karlgren, Jussi and Douglass Cutting.
1994.Recognizing text genres with simplemetrics using discriminant analysis.
InProceedings of the Fifteenth InternationalConference on Computational Linguistics(COLING-94), pages 1071?1075.Karp, Daniel, Yves Schabes, Martin Zaidel,and Dania Egedi.
1994.
A freely availablewide coverage morphological analyzer forEnglish.
In Proceedings of the 15thInternational Conference on ComputationalLinguistics (COLING-94), Nantes, Francepages 922?928.Kaufer, David.
2000.
Flaming: A white paper.Available at www.eudora.com.Kessler, Brett, Geoffrey Nunberg, andHinrich Schu?tze.
1997.
Automaticdetection of text genre.
In Proceedings ofthe 35th Annual Meeting of the Association forComputational Linguistics (ACL-97),Madrid, July 7?12, pages 32?38.Kuroda, S.-Y.
1973.
Where epistemology,style and grammar meet: A case studyfrom the Japanese.
In P. Kiparsky andS.
Anderson, editors, A Festschrift forMorris Halle.
Holt, Rinehart & Winston,New York, pages 377?391.Kuroda, S.-Y.
1976.
Reflections on thefoundations of narrative theory?from alinguistic point of view.
In T. A. van Dijk,editor, Pragmatics of Language andLiterature.
North-Holland, Amsterdam,pages 107?140.Lee, Lillian.
1999.
Measures of distributionalsimilarity.
In Proceedings of the 37th AnnualMeeting of the Association for ComputationalLinguistics (ACL-99), College Park, MD,pages 25?32.Lee, Lillian and Fernando Pereira.
1999.Distributional similarity models:Clustering vs. nearest neighbors.
InProceedings of the 37th Annual Meeting of theAssociation for Computational Linguistics(ACL-99), College Park, MD,pages 33?40.Lehnert, Wendy G., Michael Dyer, PeterJohnson, C. J. Yang, and Steve Harley.1983.
BORIS: An Experiment in In-DepthUnderstanding of Narratives.
ArtificialIntelligence, 20:15?62.Lin, Dekang.
1998.
Automatic retrieval andclustering of similar words.
In Proceedingsof the 36th Annual Meeting of the Associationfor Computational Linguistics (ACL-98),Montreal, August 10?14, pages 768?773.Lin, Dekang.
1999.
Automatic identificationof non-compositional phrases.
InProceedings of the 37th Annual Meeting of theAssociation for Computational Linguistics(ACL-99), College Park, MD, pages317?324.Litman, Diane J. and Rebecca J. Passonneau.1995.
Combining multiple knowledgesources for discourse segmentation.
InProceedings of the 33rd Annual Meeting of theAssociation for Computational Linguistics(ACL-95), Cambridge, MA, June 26?30,pages 108?115.Macleod, Catherine, Ralph Grishman, andAdam Meyers.
1998.
Complex syntaxreference manual.
Technical report, NewYork University.Marcu, Daniel, Magdalena Romera, andEstibaliz Amorrortu.
1999.
Experiments inconstructing a corpus of discourse trees:Problems, annotation choices, issues.
InProceedings of the International Workshop onLevels of Representation in Discourse(LORID-99), Edinburgh, July 6?9 pages71?78.307Wiebe, Wilson, Bruce, Bell, and Martin Learning Subjective LanguageMarcus, Mitch, Beatrice Santorini, andMary Ann Marcinkiewicz.
1993.
Buildinga large annotated corpus of English: ThePenn Treebank.
Computational Linguistics,19(2):313?330.Mitchell, Tom.
1997.
Machine Learning.McGraw-Hill, Boston.Pang, Bo, Lillian Lee, and ShivakumarVaithyanathan.
2002.
Thumbs up?Sentiment classification using machinelearning techniques.
In Proceedings of theConference on Empirical Methods in NaturalLanguage Processing (EMNLP-2002),Philadelphia, July 6?7, pages 79?86.Quirk, Randolph, Sidney Greenbaum,Geoffry Leech, and Jan Svartvik.
1985.
AComprehensive Grammar of the EnglishLanguage.
Longman, New York.Riloff, Ellen and Rosie Jones.
1999.
Learningdictionaries for information extraction bymulti-level Bootstrapping.
In Proceedingsof the 16th National Conference on ArtificialIntelligence (AAAI-1999), Orlando, FL, July18?22, pages 474?479.Riloff, Ellen and Janyce Wiebe.
2003.Learning extraction patterns for subjectiveexpressions.
In Proceedings of the Conferenceon Empirical Methods in Natural LanguageProcessing (EMNLP-2003), Sapporo, Japan,July 11?12, pages 105?112.Riloff, Ellen, Janyce Wiebe, and TheresaWilson.
2003.
Learning subjective nounsusing extraction pattern bootstrapping.
InProceedings of the Seventh Conference onNatural Language Learning (CoNLL-2003),Edmonton, Alberta, Canada, May 31?June1, pages 25?32.Sack, Warren.
1995.
Representing andrecognizing point of view.
In Proceedingsof the AAAI Fall Symposium on AIApplications in Knowledge Navigation andRetrieval, Cambridge, MA, page 152.Samuel, Ken, Sandra Carberry, andK.
Vijay-Shanker.
1998.
Dialogue acttagging with transformation-basedlearning.
In Proceedings of the 36th AnnualMeeting of the Association for ComputationalLinguistics (ACL-98), Montreal, August10?14, pages 1150?1156.Smajda, Frank.
1993.
Retrieving collocationsfrom text: Xtract.
Computational Linguistics,19:143?177.Spertus, Ellen.
1997.
Smokey: Automaticrecognition of hostile messages.
InProceedings of the Ninth Annual Conferenceon Innovative Applications of ArtificialIntelligence (IAAI-97), Providence, RI, July27?31, pages 1058?1065.Stein, Dieter and Susan Wright, editors.1995.
Subjectivity and Subjectivisation.Cambridge University Press, Cambridge.Terveen, Loren, Will Hill, Brian Amento,David McDonald, and Josh Creter.
1997.Building task-specific interfaces to highvolume conversational data.
InProceedings of the Conference on HumanFactors in Computing Systems (CHI-97), LosAngeles, April 18?23, pages 226?233.Teufel, Simone and Marc Moens.
2000.What?s yours and what?s mine:Determining intellectual attribution inscientific texts.
In Proceedings of theConference on Empirical Methods in NaturalLanguage Processing and the Workshop onVery Large Corpora (EMNLP/VLC-2000),Hong Kong, October 7?8, pages 9?17.Tong, Richard.
2001.
An operational systemfor detecting and tracking opinions inon-line discussions.
In Working Notes of theSIGIR Workshop on Operational TextClassification, New Orleans, September9?13, pages 1?6.Turney, Peter.
2002.
Thumbs up or thumbsdown?
Semantic orientation applied tounsupervised classification of reviews.
InProceedings of the 40th Annual Meeting of theAssociation for Computational Linguistics(ACL-2000), Philadelphia, July 7?12, pages417?424.Uspensky, Boris.
1973.
A Poetics ofComposition.
University of CaliforniaPress, Berkeley, and Los Angeles.van Dijk, Teun A.
1988.
News as Discourse.Erlbaum, Hillsdale, NJ.Weeber, Marc, Rein Vos, and R. HaraldBaayen.
2000.
Extracting thelowest-frequency words: Pitfalls andpossibilities.
Computational Linguistics,26(3):301?317.Wiebe, Janyce and Theresa Wilson.
2002.Learning to disambiguate potentiallysubjective expressions.
In Proceedings of theSixth Conference on Natural LanguageLearning (CoNLL-2002), Taipei, Taiwan,pages 112?118.Wiebe, Janyce.
1990.
Recognizing SubjectiveSentences: A Computational Investigation ofNarrative Text.
Ph.D. thesis, StateUniversity of New York at Buffalo.Wiebe, Janyce.
1994.
Tracking point of viewin narrative.
Computational Linguistics,20(2):233?287.Wiebe, Janyce.
2000.
Learning subjectiveadjectives from corpora.
In Proceedings ofthe 17th National Conference on ArtificialIntelligence (AAAI-2000), Austin, TX, July30?August 3, pages 735?740.Wiebe, Janyce, Eric Breck, Chris Buckley,Claire Cardie, Paul Davis, Bruce Fraser,Diane Litman, David Pierce, Ellen Riloff,Theresa Wilson, David Day, and MarkMaybury.
2003.
Recognizing and308Computational Linguistics Volume 30, Number 3organizing opinions expressed in theworld press.
In Working Notes of the AAAISpring Symposium in New Directions inQuestion Answering, Palo Alto, CA, pages12?19.Wiebe, Janyce, Rebecca Bruce, Matthew Bell,Melanie Martin, and Theresa Wilson.2001.
A corpus study of evaluative andspeculative language.
In Proceedings of theSecond ACL SIGdial Workshop on Discourseand Dialogue (SIGdial-2001), Aalborg,Denmark, September 1?2, pages 186?195.Wiebe, Janyce, Rebecca Bruce, and ThomasO?Hara.
1999.
Development and use of agold standard data set for subjectivityclassifications.
In Proceedings of the 37thAnnual Meeting of the Association forComputational Linguistics (ACL-99), CollegePark, MD, pages 246?253.Wiebe, Janyce, Kenneth McKeever, andRebecca Bruce.
1998.
Mappingcollocational properties into machinelearning features.
In Proceedings of the SixthWorkshop on Very Large Corpora (WVLC-98),Montreal, August 15?16, pages 225?233.Wiebe, Janyce and William J. Rapaport.1986.
Representing de re and de dicto beliefreports in discourse and narrative.Proceedings of the IEEE, 74:1405?1413.Wiebe, Janyce and William J. Rapaport.1988.
A computational theory ofperspective and reference in narrative.
InProceedings of the 26th Annual Meeting of theAssociation for Computational Linguistics(ACL-88), Buffalo, NY, pages 131?138.Wiebe, Janyce M. and William J. Rapaport.1991.
References in narrative text.
Nou?s,25(4):457?486.Wiebe, Janyce, Theresa Wilson, andMatthew Bell.
2001.
Identifyingcollocations for recognizing opinions.
InProceedings of the ACL-01 Workshop onCollocation: Computational Extraction,Analysis, and Exploitation, Toulouse,France, July 7, pages 24?31.Wilson, Theresa and Janyce Wiebe.
2003.Annotating opinions in the world press.In Proceedings of the Fourth SIGdial Workshopon Discourse and Dialogue (SIGdial-2003),Sapporo, Japan, July 5?6, pages 13?22.Yu, Hong and Vasileios Hatzivassiloglou.2003.
Towards answering opinionquestions: Separating facts from opinionsand identifying the polarity of opinionsentences.
In Proceedings of the Conferenceon Empirical Methods in Natural LanguageProcessing (EMNLP-2003), Sapporo, Japan,July 11?12, pages 129?136.
