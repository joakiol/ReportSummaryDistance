Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 204?212,Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational LinguisticsDiscriminative Strategies to Integrate Multiword Expression Recognitionand ParsingMatthieu ConstantUniversite?
Paris-EstLIGM, CNRSFrancemconstan@univ-mlv.frAnthony SigogneUniversite?
Paris-EstLIGM, CNRSFrancesigogne@univ-mlv.frPatrick WatrinUniversite?
de LouvainCENTALBelgiumpatrick.watrin@uclouvain.beAbstractThe integration of multiword expressions in aparsing procedure has been shown to improveaccuracy in an artificial context where suchexpressions have been perfectly pre-identified.This paper evaluates two empirical strategiesto integrate multiword units in a real con-stituency parsing context and shows that theresults are not as promising as has sometimesbeen suggested.
Firstly, we show that pre-grouping multiword expressions before pars-ing with a state-of-the-art recognizer improvesmultiword recognition accuracy and unlabeledattachment score.
However, it has no statis-tically significant impact in terms of F-scoreas incorrect multiword expression recognitionhas important side effects on parsing.
Sec-ondly, integrating multiword expressions inthe parser grammar followed by a rerankerspecific to such expressions slightly improvesall evaluation metrics.1 IntroductionThe integration of Multiword Expressions (MWE)in real-life applications is crucial because such ex-pressions have the particularity of having a certainlevel of idiomaticity.
They form complex lexicalunits which, if they are considered, should signifi-cantly help parsing.From a theoretical point of view, the integra-tion of multiword expressions in the parsing pro-cedure has been studied for different formalisms:Head-Driven Phrase Structure Grammar (Copestakeet al, 2002), Tree Adjoining Grammars (Schulerand Joshi, 2011), etc.
From an empirical point ofview, their incorporation has also been consideredsuch as in (Nivre and Nilsson, 2004) for depen-dency parsing and in (Arun and Keller, 2005) in con-stituency parsing.
Although experiments always re-lied on a corpus where the MWEs were perfectlypre-identified, they showed that pre-grouping suchexpressions could significantly improve parsing ac-curacy.
Recently, Green et al (2011) proposed in-tegrating the multiword expressions directly in thegrammar without pre-recognizing them.
The gram-mar was trained with a reference treebank whereMWEs were annotated with a specific non-terminalnode.Our proposal is to evaluate two discriminativestrategies in a real constituency parsing context:(a) pre-grouping MWE before parsing; this wouldbe done with a state-of-the-art recognizer basedon Conditional Random Fields; (b) parsing witha grammar including MWE identification and thenreranking the output parses thanks to a Maxi-mum Entropy model integrating MWE-dedicatedfeatures.
(a) is the direct realistic implementation ofthe standard approach that was shown to reach thebest results (Arun and Keller, 2005).
We will evalu-ate if real MWE recognition (MWER) still positivelyimpacts parsing, i.e., whether incorrect MWER doesnot negatively impact the overall parsing system.
(b) is a more innovative approach to MWER (de-spite not being new in parsing): we select the finalMWE segmentation after parsing in order to exploreas many parses as possible (as opposed to method(a)).
The experiments were carried out on the FrenchTreebank (Abeille?
et al, 2003) where MWEs are an-notated.204The paper is organized as follows: section 2 isan overview of the multiword expressions and theiridentification in texts; section 3 presents the two dif-ferent strategies and their associated models; sec-tion 4 describes the resources used for our exper-iments (the corpus and the lexical resources); sec-tion 5 details the features that are incorporated in themodels; section 6 reports on the results obtained.2 Multiword expressions2.1 OverviewMultiword expressions are lexical items made upof multiple lexemes that undergo idiosyncratic con-straints and therefore offer a certain degree of id-iomaticity.
They cover a wide range of linguisticphenomena: fixed and semi-fixed expressions, lightverb constructions, phrasal verbs, named entities,etc.
They may be contiguous (e.g.
traffic light) ordiscontinuous (e.g.
John took your argument intoaccount).
They are often divided into two mainclasses: multiword expressions defined through lin-guistic idiomaticity criteria (lexicalized phrases inthe terminology of Sag et al (2002)) and those de-fined by statistical ones (i.e.
simple collocations).Most linguistic criteria used to determine whether acombination of words is a MWE are based on syn-tactic and semantic tests such as the ones describedin (Gross, 1986).
For instance, the utterance at nightis a MWE because it does display a strict lexicalrestriction (*at day, *at afternoon) and it does notaccept any inserting material (*at cold night, *atpresent night).
Such linguistically defined expres-sions may overlap with collocations which are thecombinations of two or more words that cooccurmore often than by chance.
Collocations are usu-ally identified through statistical association mea-sures.
A detailed description of MWEs can be foundin (Baldwin and Nam, 2010).In this paper, we focus on contiguous MWEs thatform a lexical unit which can be marked by a part-of-speech tag (e.g.
at night is an adverb, because of is apreposition).
They can undergo limited morphologi-cal and lexical variations ?
e.g.
traffic (light+lights),(apple+orange+...) juice ?
and usually do not al-low syntactic variations1 such as inserts (e.g.
*at1Such MWEs may very rarely accept inserts, often limitedto single word modifiers: e.g.
in the short term, in the very shortcold night).
Such expressions can be analyzed at thelexical level.
In what follows, we use the term com-pounds to denote such expressions.2.2 IdentificationThe idiomaticity property of MWEs makes themboth crucial for Natural Language Processing appli-cations and difficult to predict.
Their actual iden-tification in texts is therefore fundamental.
Thereare different ways for achieving this objective.
Thesimpler approach is lexicon-driven and consists inlooking the MWEs up in an existing lexicon, suchas in (Silberztein, 2000).
The main drawback isthat this procedure entirely relies on a lexicon andis unable to discover unknown MWEs.
The useof collocation statistics is therefore useful.
For in-stance, for each candidate in the text, Watrin andFranc?ois (2011) compute on the fly its associationscore from an external ngram base learnt from alarge raw corpus, and tag it as MWE if the associa-tion score is greater than a threshold.
They reach ex-cellent scores in the framework of a keyword extrac-tion task.
Within a validation framework (i.e.
withthe use of a reference corpus annotated in MWEs),Ramisch et al (2010) developped a Support VectorMachine classifier integrating features correspond-ing to different collocation association measures.The results were rather low on the Genia corpusand Green et al (2011) confirmed these bad resultson the French Treebank.
This can be explained bythe fact that such a method does not make any dis-tinctions between the different types of MWEs andthe reference corpora are usually limited to certaintypes of MWEs.
Furthermore, the lexicon-drivenand collocation-driven approaches do not take thecontext into account, and therefore cannot discardsome of the incorrect candidates.
A recent trend isto couple MWE recognition with a linguistic ana-lyzer: a POS tagger (Constant and Sigogne, 2011)or a parser (Green et al, 2011).
Constant and Si-gogne (2011) trained a unified Conditional RandomFields model integrating different standard taggingfeatures and features based on external lexical re-sources.
They show a general tagging accuracy of94% on the French Treebank.
In terms of Multi-word expression recognition, the accuracy was notterm.205clearly evaluated, but seemed to reach around 70-80% F-score.
Green et al (2011) proposed to in-clude the MWER in the grammar of the parser.
Todo so, the MWEs in the training treebank were anno-tated with specific non-terminal nodes.
They used aTree Substitution Grammar instead of a Probabilis-tic Context-free Grammar (PCFG) with latent anno-tations in order to capture lexicalized rules as wellas general rules.
They showed that this formalismwas more relevant to MWER than PCFG (71% F-score vs. 69.5%).
Both methods have the advantageof being able to discover new MWEs on the basisof lexical and syntactic contexts.
In this paper, wewill take advantage of the methods described in thissection by integrating them as features of a MWERmodel.3 Two strategies, two discriminativemodels3.1 Pre-grouping Multiword ExpressionsMWER can be seen as a sequence labelling task(like chunking) by using an IOB-like annotationscheme (Ramshaw and Marcus, 1995).
This impliesa theoretical limitation: recognized MWEs must becontiguous.
The proposed annotation scheme istherefore theoretically weaker than the one proposedby Green et al (2011) that integrates the MWER inthe grammar and allows for discontinuous MWEs.Nevertheless, in practice, the compounds we aredealing with are very rarely discontinuous and if so,they solely contain a single word insert that can beeasily integrated in the MWE sequence.
Constantand Sigogne (2011) proposed to combine MWE seg-mentation and part-of-speech tagging into a singlesequence labelling task by assigning to each token atag of the form TAG+X where TAG is the part-of-speech (POS) of the lexical unit the token belongs toand X is either B (i.e.
the token is at the beginningof the lexical unit) or I (i.e.
for the remaining posi-tions): John/N+B hates/V+B traffic/N+B jams/N+I.In this paper, as our task consists in jointly locatingand tagging MWEs, we limited the POS tagging toMWEs only (TAG+B/TAG+I), simple words beingtagged by O (outside): John/O hates/O traffic/N+Bjams/N+I.For such a task, we used Linear chain ConditionalRamdom Fields (CRF) that are discriminative prob-abilistic models introduced by Lafferty et al (2001)for sequential labelling.
Given an input sequence oftokens x = (x1, x2, ..., xN ) and an output sequenceof labels y = (y1, y2, ..., yN ), the model is definedas follows:P?
(y|x) =1Z(x).N?tK?klog?k.fk(t, yt, yt?1, x)where Z(x) is a normalization factor dependingon x.
It is based on K features each of them be-ing defined by a binary function fk depending onthe current position t in x, the current label yt, thepreceding one yt?1 and the whole input sequencex.
The tokens xi of x integrate the lexical valueof this token but can also integrate basic propertieswhich are computable from this value (for example:whether it begins with an upper case, it contains anumber, its tags in an external lexicon, etc.).
Thefeature is activated if a given configuration betweent, yt, yt?1 and x is satisfied (i.e.
fk(t, yt, yt?1, x) =1).
Each feature fk is associated with a weight ?k.The weights are the parameters of the model, to beestimated.
The features used for MWER will be de-scribed in section 5.3.2 RerankingDiscriminative reranking consists in reranking the n-best parses of a baseline parser with a discriminativemodel, hence integrating features associated witheach node of the candidate parses.
Charniak andJohnson (2005) introduced different features thatshowed significant improvement in general parsingaccuracy (e.g.
around +1 point in English).
For-mally, given a sentence s, the reranker selects thebest candidate parse p among a set of candidatesP (s) with respect to a scoring function V?:p?
= argmaxp?P (s)V?
(p)The set of candidates P (s) corresponds to the n-bestparses generated by the baseline parser.
The scor-ing function V?
is the scalar product of a parametervector ?
and a feature vector f :V?
(p) = ?.f(p) =m?j=1?j .fj(p)where fj(p) corresponds to the number of occur-rences of the feature fj in the parse p. According to206Charniak and Johnson (2005), the first feature f1 isthe probability of p provided by the baseline parser.The vector ?
is estimated during the training stagefrom a reference treebank and the baseline parserouputs.In this paper, we slightly deviate from the originalreranker usage, by focusing on improving MWERin the context of parsing.
Given the n-best parses,we want to select the one with the best MWE seg-mentation by keeping the overall parsing accuracy ashigh as possible.
We therefore used MWE-dedicatedfeatures that we describe in section 5.
The trainingstage was performed by using a Maximum entropyalgorithm as in (Charniak and Johnson, 2005).4 Resources4.1 CorpusThe French Treebank2 [FTB] (Abeille?
et al, 2003)is a syntactically annotated corpus made up of jour-nalistic articles from Le Monde newspaper.
Weused the latest edition of the corpus (June 2010)that we preprocessed with the Stanford Parser pre-processing tools (Green et al, 2011).
It contains473,904 tokens and 15,917 sentences.
One benefit ofthis corpus is that its compounds are marked.
Theirannotation was driven by linguistic criteria such asthe ones in (Gross, 1986).
Compounds are identifiedwith a specific non-terminal symbol ?MWX?
whereX is the part-of-speech of the expression.
They havea flat structure made of the part-of-speech of theircomponents as shown in figure 1.MWNHHHNpartPdeNmarche?Figure 1: Subtree of MWE part de marche?
(marketshare): The MWN node indicates that it is a multiwordnoun; it has a flat internal structure N P N (noun ?
pre-prosition ?
noun)The French Treebank is composed of 435,860 lex-ical units (34,178 types).
Among them, 5.3% arecompounds (20.8% for types).
In addition, 12.9%2http://www.llf.cnrs.fr/Gens/Abeille/French-Treebank-fr.phpof the tokens belong to a MWE, which, on average,has 2.7 tokens.
The non-terminal tagset is composedof 14 part-of-speech labels and 24 phrasal ones (in-cluding 11 MWE labels).
The train/dev/test split isthe same as in (Green et al, 2011): 1,235 sentencesfor test, 1,235 for development and 13,347 for train-ing.
The development and test sections are the sameas those generally used for experiments in French,e.g.
(Candito and Crabbe?, 2009).4.2 Lexical resourcesFrench is a resource-rich language as attested bythe existing morphological dictionaries which in-clude compounds.
In this paper, we use two large-coverage general-purpose dictionaries: Dela (Cour-tois, 1990; Courtois et al, 1997) and Lefff (Sagot,2010).
The Dela was manually developed in the90?s by a team of linguists.
We used the distributionfreely available in the platform Unitex3 (Paumier,2011).
It is composed of 840,813 lexical entries in-cluding 104,350 multiword ones (91,030 multiwordnouns).
The compounds present in the resources re-spect the linguistic criteria defined in (Gross, 1986).The lefff is a freely available dictionary4 that hasbeen automatically compiled by drawing from dif-ferent sources and that has been manually validated.We used a version with 553,138 lexical entries in-cluding 26,311 multiword ones (22,673 multiwordnouns).
Their different modes of acquisition makesthose two resources complementary.
In both, lexicalentries are composed of a inflected form, a lemma,a part-of-speech and morphological features.
TheDela has an additional feature for most of the mul-tiword entries: their syntactic surface form.
For in-stance, eau de vie (brandy) has the feature NDN be-cause it has the internal flat structure noun ?
prepo-sition de ?
noun.In order to compare compounds in these lexicalresources with the ones in the French Treebank, weapplied on the development corpus the dictionar-ies and the lexicon extracted from the training cor-pus.
By a simple look-up, we obtained a prelimi-nary lexicon-based MWE segmentation.
The resultsare provided in table 1.
They show that the use ofexternal resources may improve recall, but they lead3http://igm.univ-mlv.fr/?unitex4http://atoll.inria.fr/?sagot/lefff.html207to a decrease in precision as numerous MWEs in thedictionaries are not encoded as such in the referencecorpus; in addition, the FTB suffers from some in-consistency in the MWE annotations.T L D T+L T+D T+L+Drecall 75.9 31.7 59.0 77.3 83.4 84.0precision 61.2 52.0 55.6 58.7 51.2 49.9f-score 67.8 39.4 57.2 66.8 63.4 62.6Table 1: Simple context-free application of the lexicalresources on the development corpus: T is the MWE lex-icon of the training corpus, L is the lefff, D is the Dela.The given scores solely evaluate MWE segmentation andnot tagging.In terms of statistical collocations, Watrin andFranc?ois (2011) described a system that lists all thepotential nominal collocations of a given sentencealong with their association measure.
The authorsprovided us with a list of 17,315 candidate nominalcollocations occurring in the French treebank withtheir log-likelihood and their internal flat structure.5 MWE-dedicated FeaturesThe two discriminative models described in sec-tion 3 require MWE-dedicated features.
In order tomake these models comparable, we use two compa-rable sets of feature templates: one adapted to se-quence labelling (CRF-based MWER) and the otherone adapted to reranking (MaxEnt-based reranker).The MWER templates are instantiated at each posi-tion of the input sequence.
The reranker templatesare instantiated only for the nodes of the candidateparse tree, which are leaves dominated by a MWEnode (i.e.
the node has a MWE ancestor).
We definea template T as follows:?
MWER: for each position n in the input se-quence x,T = f(x, n)/yn?
RERANKER: for each leaf (in position n)dominated by a MWE node m in the currentparse tree p,T = f(p, n)/label(m)/pos(p, n)where f is a function to be defined; yn is the out-put label at position n; label(m) is the label of nodem and pos(p, n) indicates the position of the wordcorresponding to n in the MWE sequence: B (start-ing position), I (remaining positions).5.1 Endogenous FeaturesEndogenous features are features directly extractedfrom properties of the words themselves or from atool learnt from the training corpus (e.g.
a tagger).Word n-grams.
We use word unigrams and bigramsin order to capture multiwords present in the trainingsection and to extract lexical cues to discover newMWEs.
For instance, the bigram coup de is oftenthe prefix of compounds such as coup de pied (kick),coup de foudre (love at first sight), coup de main(help).POS n-grams.
We use part-of-speech unigramsand bigrams in order to capture MWEs with irreg-ular syntactic structures that might indicate the id-iomacity of a word sequence.
For instance, the POSsequence preposition ?
adverb associated with thecompound depuis peu (recently) is very unusual inFrench.
We also integrated mixed bigrams made upof a word and a part-of-speech.Specific features.
Due to their different use, eachmodel integrates some specific features.
In order todeal with unknown words and special tokens, we in-corporate standard tagging features in the CRF: low-ercase forms of the words, word prefixes of length 1to 4, word suffice of length 1 to 4, whether the wordis capitalized, whether the token has a digit, whetherit is an hyphen.
We also add label bigrams.
Thereranker models integrate features associated witheach MWE node, the value of which is the com-pound itself.5.2 Exogenous FeaturesExogenous features are features that are not entirelyderived from the (reference) corpus itself.
They arecomputed from external data (in our case, our lexicalresources).
The lexical resources might be useful todiscover new expressions: usually, expressions thathave standard syntax like nominal compounds andare difficult to predict from the endogenous features.The resources are applied to the corpus through alexical analysis that generates, for each sentence, afinite-state automaton TFSA which represents all thepossible analyses.
The features are computed fromthe automaton TFSA.Lexicon-based features.
We associate each wordwith its part-of-speech tags found in our externalmorphological lexicon.
All tags of a word constitute208an ambiguity class ac.
If the word belongs to a com-pound, the compound tag is also incorporated in theambiguity class.
For instance, the word night (eithera simple noun or a simple adjective) in the context atnight, is associated with the class adj noun adv+I asit is located inside a compound adverb.
This featureis directly computed from TFSA.
The lexical anal-ysis can lead to a preliminary MWE segmentationby using a shortest path algorithm that gives priorityto compound analyses.
This segmentation is also asource of features: a word belonging to a compoundsegment is assigned different properties such as thesegment part-of-speech mwt and its syntactic struc-turemws encoded in the lexical resource, its relativeposition mwpos in the segment (?B?
or ?I?
).Collocation-based features.
In our collocation re-source, each candidate collocation of the Frenchtreebank is associated with its internal syntacticstructure and its association score (log-likelihood).We divided these candidates into two classes: thosewhose score is greater than a threshold and the otherones.
Therefore, a given word in the corpus can beassociated with different properties whether it be-longs to a potential collocation: the class c and theinternal structure cs of the collocation it belongs to,its position cpos in the collocation (B: beginning; I:remaining positions; O: outside).
We manually setthe threshold to 150 after some tuning on the devel-opment corpus.All feature templates are given in table 2.Endogenous Featuresw(n+ i), i ?
{?2,?1, 0, 1, 2}w(n+ i)/w(n+ i+ 1), i ?
{?2,?1, 0, 1}t(n+ i), i ?
{?2,?1, 0, 1, 2}t(n+ i)/t(n+ i+ 1), i ?
{?2,?1, 0, 1}w(n+ i)/t(n+ j), (i, j) ?
{(1, 0), (0, 1), (?1, 0), (0,?1)}Exogenous Featuresac(n)mwt(n)/mwpos(n)mws(n)/mwpos(n)c(n)/cs(n)/cpos(n)Table 2: Feature templates (f ) used both in the MWERand the reranker models: n is the current position in thesentence, w(i) is the word at position i; t(i) is the part-of-speech tag of w(i); if the word at absolute position iis part of a compound in the Shortest Path Segmentation,mwt(i) and mws(i) are respectively the part-of-speechtag and the internal structure of the compound,mwpos(i)indicates its relative position in the compound (B or I).6 Evaluation6.1 Experiment SetupWe carried out 3 different experiments.
We firsttested a standalone MWE recognizer based on CRF.We then combined MWE pregrouping based onthis recognizer and the Berkeley parser5 (Petrovet al, 2006) trained on the FTB where the com-pounds were concatenated (BKYc).
Finally, wecombined the Berkeley parser trained on the FTBwhere the compounds are annotated with specificnon-terminals (BKY), and the reranker.
In all exper-iments, we varied the set of features: endo are all en-dogenous features; coll and lex include all endoge-nous features plus collocation-based features andlexicon-based ones, respectively; all is composed ofboth endogenous and exogenous features.
The CRFrecognizer relies on the software Wapiti6 (Lavergneet al, 2010) to train and apply the model, and onthe software Unitex (Paumier, 2011) to apply lexicalresources.
The part-of-speech tagger used to extractPOS features was lgtagger7 (Constant and Sigogne,2011).
To train the reranker, we used a MaxEnt al-gorithm8 as in (Charniak and Johnson, 2005).Results are reported using several standard mea-sures, the F1score, unlabeled attachment and LeafAncestor scores.
The labeled F1score [F1]9, de-fined by the standard protocol called PARSEVAL(Black et al, 1991), takes into account the brack-eting and labeling of nodes.
The unlabeled attache-ment score [UAS] evaluates the quality of unlabeled5We used the version adapted to French inthe software Bonsai (Candito and Crabbe?, 2009):http://alpage.inria.fr/statgram/frdep/fr stat dep parsing.html.The original version is available at:http://code.google.com/p/berkeleyparser/.
We trained theparser as follows: right binarization, no parent annotation, sixsplit-merge cycles and default random seed initialisation (8).6Wapiti can be found at http://wapiti.limsi.fr/.
It was con-figured as follows: rprop algorithm, default L1-penalty value(0.5), default L2-penalty value (0.00001), default stopping cri-terion value (0.02%).7Available at http://igm.univ-mlv.fr/?mconstan/research/software/.8We used the following mathematical libraries PETSc etTAO, freely available at http://www.mcs.anl.gov/petsc/ andhttp://www.mcs.anl.gov/research/projects/tao/9Evalb tool available at http://nlp.cs.nyu.edu/evalb/.
Wealso used the evaluation by category implemented in the classEvalbByCat in the Stanford Parser.209dependencies between words of the sentence10.
Andfinally, the Leaf-Ancestor score [LA]11 (Sampson,2003) computes the similarity between all paths (se-quence of nodes) from each terminal node to the rootnode of the tree.
The global score of a generatedparse is equal to the average score of all terminalnodes.
Punctuation tokens are ignored in all met-rics.
The quality of MWE identification was evalu-ated by computing the F1 score on MWE nodes.
Wealso evaluated the MWE segmentation by using theunlabeled F1 score (U).
In order to compare both ap-proaches, parse trees generated by BKYc were auto-matically transformed in trees with the same MWEannotation scheme as the trees generated by BKY.In order to establish the statistical significance ofresults between two parsing experiments in terms ofF1 and UAS, we used a unidirectional t-test for twoindependent samples12.
The statistical significancebetween two MWE identification experiments wasestablished by using the McNemar-s test (Gillickand Cox, 1989).
The results of the two experimentsare considered statistically significant with the com-puted value p < 0.01.6.2 Standalone Multiword recognitionThe results of the standalone MWE recognizer aregiven in table 3.
They show that the lexicon-basedsystem (lex) reaches the best score.
Accuracy is im-proved by an absolute gain of +6.7 points as com-pared with BKY parser.
The strictly endogenoussystem has a +4.9 point absolute gain, +5.4 pointswhen collocations are added.
That shows that mostof the work is done by fully automatically acquiredfeatures (as opposed to features coming from a man-ually constructed lexicon).
As expected, lexicon-based features lead to a 5.3 point recall improve-ment (with respect to non-lexicon based features)whereas precision is stable.
The more precise sys-tem is the base one because it almost solely detectscompounds present in the training corpus; neverthe-less, it is unable to capture new MWEs (it has the10This score is computed by using the tool available athttp://ilk.uvt.nl/conll/software.html.
The constituent trees areautomatically converted into dependency trees with the toolBonsai.11Leaf-ancestor assessment tool available athttp://www.grsampson.net/Resources.html12Dan Bikel?s tool available athttp://www.cis.upenn.edu/?dbikel/software.html.lowest recall).
BKY parser has the best recall amongthe non lexicon-based systems, i.e.
it is the best oneto discover new compounds as it is able to preciselydetect irregular syntactic structures that are likely tobe MWEs.
Nevertheless, as it does not have a lex-icalized strategy, it is not able to filter out incorrectcandidates; the precision is therefore very low (theworst).P R F1 F1 ?
40 Ubase 78.0 68.3 72.8 71.2 74.3endo 75.5 74.5 75.0 74.0 76.3coll 76.6 74.4 75.5 74.9 77.0lex 76.0 79.8 77.8 77.8 79.0all 76.2 79.2 77.7 77.3 78.8BKY 67.6 75.1 71.1 70.7 72.5Stanford* - - - 70.1 -DP-TSG* - - - 71.1 -Table 3: MWE identification with CRF: base are thefeatures corresponding to token properties and word n-grams.
The differences between all systems are statisti-cally significant with respect to McNemar?s test (Gillickand Cox, 1989), except lex/all and all/coll;lex/coll is ?border-line?.
The results of the systemsbased on the Stanford Parser and the Tree SubstitutionParser (DP-TSG) are reported from (Green et al, 2011).6.3 Combination of Multiword ExpressionRecognition and ParsingWe tested and compared the two proposed dis-criminative strategies by varying the sets of MWE-dedicated features.
The results are reported in ta-ble 4.
Table 5 compares the parsing systems, byshowing the score differences between each of thetested system and the BKY parser.Strat.
Feat.
Parser F1 LA UAS F1(MWE)- - BKY 80.61 92.91 82.99 71.1pre - BKYc 75.47 91.10 76.74 0.0pre endo BKYc 80.23 92.69 83.62 74.9pre coll BKYc 80.32 92.73 83.77 75.5pre lex BKYc 80.66 92.81 84.16 77.4pre all BKYc 80.51 92.77 84.05 77.2post endo BKY 80.87 92.94 83.49 72.9post coll BKY 80.71 92.85 83.16 71.2post lex BKY 81.08 92.98 83.98 74.5post all BKY 81.03 92.96 83.97 74.3pre gold BKYc 83.73 93.77 90.08 95.8Table 4: Parsing evaluation: pre indicates a MWE pre-grouping strategy, whereas post is a reranking strategywith n = 50.
The feature gold means that we have ap-plied the parser on a gold MWE segmentation.210?F1 ?UAS ?F1(MWE)pre post pre post pre postendo -0.38 +0.26 +0.63 +0.50 +3.8 +1.8coll -0.29 +0.10 +0.78 +0.17 +4.4 +0.1lex +0.05 +0.47 +1.17 +0.99 +6.3 +3.4Table 5: Comparison of the strategies with respect toBKY parser.Firstly, we note that the accuracy of the best re-alistic parsers is much lower than that of a parserwith a golden MWE segmentation13 (-2.65 and -5.92respectively in terms of F-score and UAS), whichshows the importance of not neglecting MWE recog-nition in the framework of parsing.
Furthermore,pre-grouping has no statistically significant impacton the F-score14, whereas reranking leads to a sta-tistically significant improvement (except for col-locations).
Both strategies also lead to a statisti-cally significant UAS increase.
Whereas both strate-gies improve the MWE recognition, pre-groupingis much more accurate (+2-4%); this might be dueto the fact that an unlexicalized parser is limited interms of compound identification, even within n-best analyses (cf.
Oracle in table 6).
The benefits oflexicon-based features are confirmed in this experi-ment, whereas the use of collocations in the rerank-ing strategy seems to be rejected.endo coll lex all oraclen=1 80.61(71.1)n=5 80.74 80.88 81.03 81.05 83.17(71.5) (71.7) (73.4) (73.3) (74.6)n=20 80.98 80.72 81.09 81.01 84.76(72.9) (70.6) (73.6) (73.0) (75.5)n=50 80.87 80.71 81.08 81.03 85.21(72.9) (71.2) (74.5) (74.3) (76.4)n=100 80.69 80.53 81.12 80.93 85.54(72.0) (70.0) (74.4) (73.7) (76.4)Table 6: Reranker F1 evaluation with respect to n and thetypes of features.
The F1(MWE) is given in parenthesis.Table 7 shows the results by category.
It indi-cates that both discriminative strategies are of in-terest in locating multiword adjectives, determinersand prepositions; the pre-grouping method appearsto be particularly relevant for multiword nouns and13The F1(MWE) is not 100% with a golden segmentation be-cause of tagging errors by the parser.14Note that we observe an increase of +0.5 in F1 on the de-velopment corpus with lexicon-based features.adverbs.
However, it performs very poorly in multi-word verb recognition.
In terms of standard parsingaccuracy, the pre-grouping approach has a very het-erogeneous impact: Adverbial and Adjective Modi-fier phrases tend to be more accurate; verbal kernelsand higher level constituents such as relative andsubordinate clauses see their accuracy level drop,which shows that pre-recognition of MWE can havea negative impact on general parsing accuracy asMWE errors propagate to higher level constituents.cat #gold BKY endo lex endo lex(pre) (pre) (post) (post)MWET 4 0.0 N/A N/A N/A N/AMWA 22 37.2 +15.2 +21.3 +0.9 +4.7MWV 47 62.1 -9.7 -13.2 +1.7 +2.5MWD 24 62.1 +7.3 +10.2 0.0 +1.2MWN 860 68.2 +4.0 +7.0 +1.7 +4.2MWADV 357 72.1 +3.8 +6.4 +3.4 +4.1MWPRO 31 84.2 -3.5 -0.9 0.0 0.0MWP 294 79.1 +4.3 +5.8 +0.4 +1.1MWC 86 85.7 +0.9 +3.7 +0.2 +1.0Sint 209 47.2 -7.7 -8.7 +0.1 -0.2AdP 86 48.8 +1.2 +3.0 +3.4 +5.1Ssub 406 60.8 -1.1 -1.1 -0.3 -0.5VPpart 541 63.2 -2.8 -2.1 -0.5 -1.6Srel 408 74.8 -3.4 -3.5 -0.3 -0.6VPinf 781 75.2 0.0 -0.1 -0.3 -0.3COORD 904 75.2 +0.2 +0.4 -0.3 -0.4PP 4906 76.7 -0.8 -0.3 +0.5 +0.7AP 1482 74.5 +3.2 +3.9 +0.7 +1.6NP 9023 79.8 -1.1 -0.8 +0.1 +0.2VN 3089 94.0 -2.0 -1.0 0.0 0.0Table 7: Evaluation by category with respect to BKYparser.
The BKY column indicates the F1 of BKY parser.7 Conclusions and Future WorkIn this paper, we evaluated two discriminative strate-gies to integrate Multiword Expression Recognitionin probabilistic parsing: (a) pre-grouping MWEswith a state-of-the-art recognizer and (b) MWEidentification with a reranker after parsing.
Weshowed that MWE pre-grouping significantly im-proves compound recognition and unlabeled depen-dency annotation, which implies that this strategycould be useful for dependency parsing.
The rerank-ing procedure evenly improves all evaluation scores.Future work could consist in combining both strate-gies: pre-grouping could suggest a set of potentialMWE segmentations in order to make it more flexi-ble for a parser; final decisions would then be madeby the reranker.211AcknowlegmentsThe authors are very grateful to Spence Green for hisuseful help on the treebank, and to Jennifer Thewis-sen for her careful proof-reading.ReferencesA.
Abeille?
and L. Cle?ment and F. Toussenel.
2003.Building a treebank for French.
Treebanks.
In A.Abeille?
(Ed.).
Kluwer.
Dordrecht.A.
Arun and F. Keller.
2005.
Lexicalization in crosslin-guistic probabilistic parsing: The case of French.
InACL.E.
Black, S. Abney, D. Flickinger, C. Gdaniec, R. Gr-ishman, P. Harrison, D. Hindle, R. Ingria, F. Jelinek,J.
Klavans, M. Liberman, M. Marcus, S. Roukos, B.Santorini and T. Strzalkowski.
1991.
A procedure forquantitatively comparing the syntactic coverage of En-glish grammars.
In Proceedings of the DARPA Speechand Natural Language Workshop.T.
Baldwin and K.S.
Nam.
2010.
Multiword Ex-pressions.
Handbook of Natural Language Process-ing, Second Edition.
CRC Press, Taylor and FrancisGroup.M.
-H. Candito and B. Crabbe?.
2009.
Improving gen-erative statistical parsing with semi-supervised wordclustering.
Proceedings of IWPT 2009.E.
Charniak and M. Johnson.
2005.
Coarse-to-Fine n-Best Parsing and MaxEnt Discriminative Reranking.Proceedings of the 43rd Annual Meeting of the Asso-ciation for Computational Linguistics (ACL?05).M.
Constant and A. Sigogne.
2011.
MWU-aware Part-of-Speech Tagging with a CRF model and lexical re-sources.
In Proceedings of the Workshop on Multi-word Expressions: from Parsing and Generation to theReal World (MWE?11).A.
Copestake, F. Lambeau, A. Villavicencio, F. Bond,T.
Baldwin, I.
Sag, D. Flickinger.
2002.
Multi-word Expressions: Linguistic Precision and Reusabil-ity.
Proceedings of the Third International Conferenceon Language Resources and Evaluation (LREC 2002).B.
Courtois.
1990.
Un syste`me de dictionnairese?lectroniques pour les mots simples du franc?ais.Langue Franc?aise.
Vol.
87.B.
Courtois, M. Garrigues, G. Gross, M. Gross, R.Jung, M. Mathieu-Colas, A. Monceaux, A. Poncet-Montange, M. Silberztein and R. Vive?s.
1997.
Dic-tionnaire e?lectronique DELAC : les mots compose?s bi-naires.
Technical Report.
n. 56.
LADL, UniversityParis 7.L.
Gillick and S. Cox.
1989.
Some statistical issues inthe comparison of speech recognition algorithms.
InProceedings of ICASSP?89.S.
Green, M.-C. de Marneffe, J. Bauer and C. D. Man-ning.
2011.
Multiword Expression Identification withTree Substitution Grammars: A Parsing tour de forcewith French.
In Empirical Method for Natural Lan-guage Processing (EMNLP?11).M.
Gross.
1986.
Lexicon Grammar.
The Representa-tion of Compound Words.
In Proceedings of Compu-tational Linguistics (COLING?86).J.
Lafferty and A. McCallum and F. Pereira.
2001.
Con-ditional random Fields: Probabilistic models for seg-menting and labeling sequence data.
In Proceedings ofthe Eighteenth International Conference on MachineLearning (ICML 2001).T.
Lavergne, O. Cappe?
and F. Yvon.
2010.
Practical VeryLarge Scale CRFs.
In ACL.J.
Nivre and J. Nilsson.
2004.
Multiword units in syntac-tic parsing.
In Methodologies and Evaluation of Mul-tiword Units in Real-World Applications (MEMURA).S.
Paumier.
2011.
Unitex 3.9 documentation.http://igm.univ-mlv.fr/?unitex.S.
Petrov, L. Barrett, R. Thibaux and D. Klein.
2006.Learning accurate, compact and interpretable tree an-notation.
In ACL.C.
Ramisch, A. Villavicencio and C. Boitet.
2010. mwe-toolkit: a framework for multiword expression identi-fication.
In LREC.L.
A. Ramshaw and M. P. Marcus.
1995.
Text chunkingusing transformation-based learning.
In Proceedingsof the 3rd Workshop on Very Large Corpora.I.
A.
Sag, T. Baldwin, F. Bond, A. Copestake and D.Flickinger.
2002.
Multiword Expressions: A Pain inthe Neck for NLP.
In CICLING 2002.
Springer.B.
Sagot.
2010.
The Lefff, a freely available, accurateand large-coverage lexicon for French.
In Proceed-ings of the 7th International Conference on LanguageResources and Evaluation (LREC?10).G.
Sampson and A. Babarczy.
2003.
A test of the leaf-ancestor metric for parsing accuracy.
Natural Lan-guage Engineering.
Vol.
9 (4).Seddah D., Candito M.-H. and Crabb B.
2009.
Cross-parser evaluation and tagset variation: a French tree-bank study.
Proceedings of International Workshopon Parsing Technologies (IWPT?09).W.
Schuler, A. Joshi.
2011.
Tree-rewriting models ofmulti-word expressions.
Proceedings of the Workshopon Multiword Expressions: from Parsing and Genera-tion to the Real World (MWE?11).M.
Silberztein.
2000.
INTEX: an FST toolbox.
Theoret-ical Computer Science, vol.
231(1).P.
Watrin and T. Franc?ois.
2011.
N-gram frequencydatabase reference to handle MWE extraction in NLPapplications.
In Proceedings of the 2011 Workshop onMultiWord Expressions.212
