A reliable approach to automatic assessmentof short answer free responsesLyle F Bachman, Nathan Carr, Greg Kamei, Mikyung Kim, Michael J Pan, Chris Salvador, Yasuyo SawakiWebLAS, Applied Linguistics & TESL, UCLALos Angeles, CA 90095{bachman, carr, kamei, kimmi, mjpan, chriss, ysawaki}@WebLAS.ucla.eduAbstractThis paper discusses an innovative approach tothe computer assisted scoring of studentresponses in WebLAS (web-based languageassessment system)- a language assessmentsystem delivered entirely over the web.
Expectedstudent responses are limited production freeresponse questions.The portions of WebLAS with which we areconcerned are the task creation and scoringmodules.
Within the task creation module,instructors and language experts do not onlyprovide the task input and prompt.
Moreimportantly, they interactively inform the systemhow and how much to score student responses.This interaction consists of WebLAS?
naturallanguage processing (NLP) modules searchingfor alternatives of the provided ?gold standard?
(Hirschman et al 2000) answer and asking forconfirmation of score assignment.
WebLASprocesses and stores all this information withinits database, to be used in the task delivery andscoring phases.1 IntroductionMost assessment for placement, diagnosis,progress and achievement in our languageprograms are presently administered in paper andpencil (P&P) format.
This format carries anumber of administrative costs and inefficiencies.It requires new hard copy forms of assessmentsfor each course and class, incurring costsassociated with copying, handling, distributing,and collecting test booklets and answer sheets totest takers.
Although some of the assessmentscan be scored by machine, teachers score thosewith free responses, such as open-endedquestions and cloze (gap filling) tests.WebLAS addresses the problems of a P&Pformat.
It provides an integrated approach toassessing language ability for the purpose ofmaking decisions about placement, diagnosis,progress and achievement in the East Asianlanguage programs, as the content specificationsof the assessment system for these languages arebased directly on the course content, as specifiedin scope and sequence charts, and utilize tasks thatare similar to those used in classroom instruction.WebLAS is thus being designed with thefollowing expected advantages as objectives:1.
Greater administrative efficiency2.
More authentic, interactive and validassessments of language ability such asintegration of course and assessmentcontent and incorporation of cutting edgeand multimedia technology for assessmentNested within these objectives is the ability toautomatically assess limited production freeresponses.
Existing systems such as e-Rater(Burstein et al focus on holistic essay scoring.Even so, systems such as PEG (Page 1966)disregard content and simply perform surfacefeature analysis, such as a tabulation of syntacticalusage.
Others like LSA (Foltz et al1998) requirea large corpora as basis for comparison.
Lately,there has been more interested in approaching theshort answer scoring problem.
These few such asMITRE (Hirschman et al 2000) and ATM(Callear et al 2001) are extraordinarilyprogramming intensive however, andincomprehensible to educators.
Additionally, theydo not permit a partial credit scoring system,thereby introducing subjectivity into the scoring(Bachman 1990).
None are truly suited for shortanswer scoring in an educational context, since thescores produced are neither easily explanable norjustifiable to testtakers.WebLAS is developed in response to theneeds of the language assessors.
Currentmethods for scoring P&P tests require the testcreators to construct a scoring rubrid, by whichhuman scorers reference as they score studentresponses.
Weblas imitates this process byprompting the test creator for the scoring rubrid.It tags and parses the model answer, extractsrelevant elements from within the model answerand proposes possible alternatives interactivelywith the educator.
It also tags, parses, andextracts the same from the student responses.Elements are then pattern matched and scored.2 Using WebLASJust as a scoring rubric for short answerscoring cannot be created in a vacuum, it wouldbe difficult for us to discuss the scoring processwithout describing the task creation process.Task development consists of all the effortsthat lead to the test administration.
The taskdevelopment portion of WebLAS consists ofthree modules- task creation, task modification,and lexicon modification.
These are explainedbelow.2.1 Using WebLASWebLAS is written mostly in Perl.
Itscapacity for regular expressions (regex) make itwell suited for natural language processing (NLP)tasks, and its scripting abilities enable dynamicand interactive content deliverable over the web.There is also a complete repository of opensource Perl modules available, eliminating thenecessity to reinvent the wheel.One of the tools WebLAS incorporates isWordnet, an English lexicon under developmentat Princeton with foundations in cognitivepsychology (Fellbaum 1998).
A second toolWebLAS uses is the Link Grammar Parser, aresearch prototype available from CarnegieMellon University (Grinberg et al1995).
BothWordnet and Link Grammar are written inC/C++.
To interface with the systems, we makeuse of 2 Perl modules developed by Dan Brian1.Linguana::Wordnet converts to Berkeley DB1  http://www.brians.orgformat2 for fast access, and allows formodifications to the lexicon.Linguana::LinkGrammar interfaces with the LinkGrammar for parts of speech (POS) tagging andsyntactic parsing.
For our web server we use theApache Advanced Extranet web server.
To runperl scripts via the web, we use mod_perl, whichenables us to run unmodified scripts.
Ourdatabase is MySQL server3.2.2 Task DevelopmentWebLAS is organized into four majorcomponents relative to the test event itself.
Theseare test development, test delivery, responsescoring, and test analysis.
Two of these arerelevant to NLP- task development and testscoring.Figure 1.
Task Creation Flowchart2  http://www.sleepycat.com3  http://www.mysql.org2.2.1 Task CreationThe task creation module is somewhat of amisnomer.
At the time of using this module, thetask has already been specified according tolanguage assessment requirements.
The moduleactually facilitates the instructor with the processof storing into the database and preprocessing thetask for automatic scoring, rather than creatingthe task itself.
This process is shown in theflowchart in Figure 1.a.
The module requests from the instructor thetask name, task type, input prompt, responseprompt, and model answer for the task.
Thisinformation is stored within the database forretrieval.b.
WebLAS sends Link Grammar the modelanswer, which returns the answer aftertagging the POS and parsing it.
WebLASthen finds important elements of the modelanswer which are necessary to receive fullcredit from the parsed answer and confirmseach one with the instructor.
Elements aregenerally phrases, such as ?the sushirestaurant?
or ?next to the post office?
butcould be singletons as well, such as?Yamada-san?
as well.c.
After each element is confirmed, WebLASsearches Wordnet for possible alternatives ofthe elements and their individual words.
Forexample, it may deem ?near the post office?as a possible alternate to ?next to the postoffice.?
Once found, it asks for confirmationfrom the instructor again.
Additionally, theeducator is prompted for other possibilitiesthat were not found.d.
The task creator dictates a ratings scale.Point values assigned to elements derivingdirectly from the model answer are assumedto be maximum values, i.e.
full credit for thegiven element.
Alternatives to the modelanswer elements found can be assignedscoring less than or equal to the maximumvalue.
Thus an answer with numerouselements can be scored with a partial creditschema.e.
WebLAS takes the input (model answer,elements, alternatives, score assignments) tocreate a scoring key.
The scoring keyemploys regular expressions for patternmatching.
For example, ?(next|near)?
indicatethat either ?next?
or ?near?
are acceptableanswers.
Along with each regex is a pointvalue, which is added to a test taker?s finalscore if the regex is matched with the studentresponse.2.2.2 Task ModificationThe task modification module allows forinstructors to go back and modify tasks they havecreated, as well as tasks others created.
Thedatabase tracks information relevant to thechanges, including information on the modifier,date and time of the modification, evolvingchanges to the tasks, and any comments on thereasons for the change.
The database supportsdata synchronization, so that two instructorscannot and do not change tasks simultaneously.Should the model answer be changed, thescoring key creation of the task creation module isactivated and the instructor is guided through theprocess again.2.2.3 Lexicon ModificationThe WebLAS lexicon is based on Wordnet.Wordnet is by no means complete, however, and itmay be possible that instructors may find the needto add to its knowledge.
The lexicon isautomatically updated given the input given duringscoring key creation.One can also manually modify the lexiconthrough a guided process.
The system prompts forthe word and its parts of speech and returns allpossible word senses.
The user chooses a wordsense, and is then given a choice of the relationtype to modify (i.e.
synonyms, antonyms,hyponyms, etc.).
The final step is the modificationand confirmation of the change to the relationtype.2.3      Test ScoringOnce the task creation module creates theregexes, task scoring becomes trivial.
WebLASsimply needs to pattern match the regexes to scoreeach element.
Additionally, WebLAS can be quiteflexible in its scoring.
It is tolerant of a widerange of answers on the part of test takers,incorporating adapted soundex, edit distances,and word stemming algorithms, for phonetic,typographic, and morphological deviations frommodel answers.3 Lexicon ModificationThere are advantages to the WebLAS system.The first is a computational efficiency factor.The system is not a learning system (yet).
Theautomatic scoring section, if it did not usepreprocessed regexes, would perform the samesearch for each student response.
This searchbecomes redundant and unnecessary.
Bypreprocessing the search, we reduce the lineartime complexity- O(n), to a constant- O(1), withrespect to the number of student responses.Second, partial scoring eliminatesarbitrariness of scoring.
Rather than a simplecredit/no credit schema, each elementindividually contributes to the final scoretabulation.Reliability also increases.
Since the scoresproduced are repeatable, and do not change witheach scoring, WebLAS has perfect intra-raterreliability.
Because the instructor confirms allscoring decisions beforehand, the scores are alsoexplainable and justifiable, and can withstandcriticism.4 ConclusionOur approach towards automatic computerscoring of open ended responses show promisingpotential for reasons of its reliability androbustness.
Future plans include making use ofadditional NLP algorithms such as inference andpronoun resolution, as well as inclusion ofadditional task types such as summarization,outline, and gap-fill tasks.
We should also like tobring the scoring online and provide the studentwith instantaneous feedback.
Pilot testing withinthe campus is scheduled for Winter and Spring2003 quarters, with full campus rollout in Fall2003.ReferencesBachman, Lyle F. (1990) FundamentalConsiderations in Language Testing.
OxfordUniversity Press: Oxford.Bachman, Lyle F.; Palmer, Adrian S.  (1996)Language Testing in Practice.
OxfordUniversity Press: Oxford.Brian, Daniel.
(2001)  Linguana: Perl as alanguage for conceptual representation in NLPsystems.
Proceedings of the O?Reilly PerlConference 2001.
24-31.Burstein, Jill; Leacock, Claudia; Swartz, Richard.
(2001)  Automated evaluation of essays andshort answers.
Proceedings of the 5thInternational Computer Assisted AssessmentConference (CAA 01).Callear, David; Jerrams-Smith, Jenny; Soh, David.
(2001)  CAA of short non-MCQ answers.Proceedings of the 5th International ComputerAssisted Assessment Conference (CAA 01).Chung, Gregory K.W.K; O?Neil, Harold F., Jr.(1997)  Methodological approaches to onlinescoring of essays.
University of California LosAngeles, Center for Research on Evaluation,Standards, and Student Testing technical report461.Grinberg, Dennis; Lafferty, John; Sleator, Daniel.
(1995)  A robust parsing algorithm for linkgrammars, Carnegie Mellon UniversityComputer Science technical report CMU-CS-95-125, and Proceedings of the FourthInternational Workshop on ParsingTechnologies, Prague.Hirschman, Lynette; Breck, Eric; Light, Marc;Burger, John D.; Ferro, Lisa.
(2000)  Automatedgrading of short answer tests.
IEEE IntelligentSystems.
15(5):31-35.Fellbaum, Christiane (editor).
(1998)  Wordnet:An electronic lexical database.
MIT Press,Cambridge, MA.Foltz P; Kintsch W; Landauer T.  (1998)  ?Themeasurement of textual coherence with latentsemantic analysis.?
Discourse Processes.25(23):285-307.Page, E.B.
(1966)  ?The imminence of gradingessays by computer.?
Phi Delta Kappan.47:238-243.
