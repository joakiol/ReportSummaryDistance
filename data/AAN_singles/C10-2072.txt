Coling 2010: Poster Volume, pages 630?638,Beijing, August 2010Enhancing Multi-lingual Information Extractionvia Cross-Media Inference and FusionAdam Lee, Marissa Passantino, Heng JiComputer Science DepartmentQueens College and Graduate CenterCity University of New Yorkhengji@cs.qc.cuny.eduGuojun Qi, Thomas HuangDepartment of Electrical and ComputerEngineering & Beckman InstituteUniversity of Illinois at Urbana-Champaignhuang@ifp.uiuc.eduAbstractWe describe a new information fusionapproach to integrate facts extractedfrom cross-media objects (videos andtexts) into a coherent common represen-tation including multi-level knowledge(concepts, relations and events).
Beyondstandard information fusion, we ex-ploited video extraction results and sig-nificantly improved text Information Ex-traction.
We further extended our meth-ods to multi-lingual environment (Eng-lish, Arabic and Chinese) by presentinga case study on cross-lingual comparablecorpora acquisition based on video com-parison.1 IntroductionAn enormous amount of information is widelyavailable in various data modalities (e.g.
speech,text, image and video).
For example, a Webnews page about ?Health Care Reform inAmerica?
is composed with texts describingsome events (e.g., Final Senate vote for thereform plans, Obama signs the reformagreement), images (e.g., images about variousgovernment involvements over decades) andvideos/speech (e.g.
Obama?s speech video aboutthe decisions) containing additional informationregarding the real extent of the events orproviding evidence corroborating the text part.These cross-media objects exist in redundantand complementary structures, and therefore itis beneficial to fuse information from variousdata modalities.
The goal of our paper is toinvestigate this task from both mono-lingual andcross-lingual perspectives.The processing methods of texts andimages/videos are typically organized into twoseparate pipelines.
Each pipeline has beenstudied separately and quite intensively over thepast decade.
It is critical to move away fromsingle media processing, and instead towardmethods that make multiple decisions jointlyusing cross-media inference.
For example, videoanalysis allows us to find both entities andevents in videos, but it?s very challenging tospecify some fine-grained semantic types suchas proper names (e.g.
?Obama Barack?)
andrelations among concepts; while the speechembedded and the texts surrounding thesevideos can significantly enrich such analysis.
Onthe other hand, image/video features canenhance text extraction.
For example, entitygender detection from speech recognition outputis challenging because of entity mentionrecognition errors.
However, gender detectionfrom corresponding images and videos canachieve above 90% accuracy (Baluja and Rowley,2006).
In this paper, we present a case study ongender detection to demonstrate how text andvideo extractions can boost each other.We can further extend the benefit of cross-media inference to cross-lingual informationextraction (CLIE).
Hakkani-Tur et al (2007)found that CLIE performed notably worse thanmonolingual IE, and indicated that a majorcause was the low quality of machine translation(MT).
Current statistical MT methods requirelarge and manually aligned parallel corpora asinput for each language pair of interest.
Somerecent work (e.g.
Munteanu and Marcu, 2005; Ji,2009) found that MT can benefit from multi-lingual comparable corpora (Cheung and Fung,2004), but it is time-consuming to identify pairsof comparable texts; especially when there is630lack of parallel information such as news releasedates and topics.
However, the images/videosembedded in the same documents can provideadditional clues for similarity computationbecause they are ?language-independent?.
Wewill show how a video-based comparisonapproach can reliably build large comparabletext corpora for three languages: English,Chinese and Arabic.2 Baseline SystemsWe apply the following state-of-the-art text andvideo information extraction systems as ourbaselines.
Each system can produce reliableconfidence values based on statistical models.2.1 Video Concept ExtractionThe video concept extraction system wasdeveloped by IBM for the TREC VideoRetrieval Evaluation (TRECVID-2005)(Naphade et al, 2005).
This system can extract2617 concepts defined by TRECVID, such as"Hospital", "Airplane" and "Female-Person".
Ituses support vector machines to learn themapping between low level features extractedfrom visual modality as well as from transcriptsand production related meta-features.
It alsoexploits a Correlative Multi-label Learner (Qi etal., 2007), a Multi-Layer Multi-Instance Kernel(Gu et al, 2007) and Label Propagation throughLinear Neighborhoods (Wang et al, 2006) toextract all other high-level features.
For eachclassifier, different models are trained on a setof different modalities (e.g., the color moments,wavelet textures, and edge histograms), and thepredictions made by these classifiers arecombined together with a hierarchical linearly-weighted fusion strategy across differentmodalities and classifiers.2.2 Text Information ExtractionWe use a state-of-the-art IE system (Ji andGrishman, 2008) developed for the AutomaticContent Extraction (ACE) program1 to processtexts and automatic speech recognition output.The pipeline includes name tagging, nominalmention tagging, coreference resolution, timeexpression extraction and normalization, rela-tion extraction and event extraction.
Entities1 http://www.nist.gov/speech/tests/ace/include coreferred persons, geo-political entities(GPE), locations, organizations, facilities, vehi-cles and weapons; relations include 18 types(e.g.
?a town some 50 miles south of Salzburg?indicates a located relation.
); events include the33 distinct event types defined in ACE 2005(e.g.
?Barry Diller on Wednesday quit as chiefof Vivendi Universal Entertainment.?
indicates a?personnel-start?
event).
Names are identifiedand classified using an HMM-based name tag-ger.
Nominals are identified using a maximumentropy-based chunker and then semanticallyclassified using statistics from ACE trainingcorpora.
Relation extraction and event extractionare also based on maximum entropy models,incorporating diverse lexical, syntactic, semanticand ontological knowledge.3 Mono-lingual Information Fusionand Inference3.1 Mono-lingual System OverviewFigure 1.
Mono-lingual Cross-MediaInformation Fusion and Inference PipelineFigure 1 depicts the general procedure of ourmono-lingual information fusion and inferenceMulti-mediaDocumentEnhancedConcepts/EntitiesRelations/EventsTextsText InformationExtractionVideo ConceptExtractionEntities/Relations/EventsConceptsPartitioningMulti-level Concept FusionASRGlobal InferenceSpeech Videos/Images631approach.
After we apply two baseline systemsto the multi-media documents, we use a novelmulti-level concept fusion approach to extract acommon knowledge representation across textsand videos (section 3.2), and then apply a globalinference approach to enhance fusion results(section 3.3).3.2 Cross-media Information Fusion?
Concept MappingFor each input video, we apply automatic speechrecognition to obtain background texts.
Then weuse the baseline IE systems described in section2 to extract concepts from texts and videos.
Weconstruct mappings on the overlapped factsacross TRECVID and ACE.
For example,?LOC.Water-Body?
in ACE is mapped to?Beach, Lakes, Oceans, River, River_Bank?
inTRECVID.Due to different characteristics of video clipsand texts, these two tasks have quite differentgranularities and focus.
For example,?PER.Individual?
in ACE is an open set includ-ing arbitrary names, while TRECVID only cov-ers some famous proper names such as?Hu_Jintao?
and ?John_Edwards?.
Geopoliticalentities appear very rarely in TRECVID becausethey are more explicitly presented in back-ground texts.
On the other hand, TRECVID de-fined much more fine-grained nominals thanACE, for example, ?FAC.Building-Grounds?
inACE can be divided into 52 possible concepttypes such as ?Conference_Buildings?
and?Golf_Course?
because they can be more easilydetected based on video features.
We also noticethat TRECVID concepts can include multiplelevels of ACE facts, for example?WEA_Shooting?
concept can be separated into?weapon?
entities and ?attack?
events in ACE.These different definitions bring challenges tocross-media fusion but also opportunities to ex-ploit complementary facts to refine both pipe-lines.
We manually resolved these issues andobtained 20 fused concept sets.?
Time-stamp based Multi-level ProjectionAfter extracting facts from videos and texts, weconduct information fusion at all possible levels:name, nominal, coreference link, relation orevent mention.
We rely on the timestamp infor-mation associated with video keyframes or shots(sequential keyframes) and background speechto align concepts.
During this fusion process, wecompare the normalized confidence values pro-duced from two pipelines to resolve the follow-ing three types of cases:?
Contradiction ?
A video fact contradicts atext fact; we only keep the fact with higherconfidence.?
Redundancy ?
A video fact conveys thesame content as (or entails, or is entailed by)a text fact; we only keep the unique parts ofthe facts.?
Complementary ?
A video fact and a textfact are complementary; we merge thesetwo to form more complete fact sets.?
A Common RepresentationIn order to effectively extract compact informa-tion from large amounts of heterogeneous data,we design an integrated XML format to repre-sent the facts extracted from the above multi-level fusion.
We can view this representation asa set of directed ?information graphs?
G={Gi(Vi, Ei)}, where Vi is the collection of conceptsfrom both texts and videos, and Ei is the collec-tion of edges linking one concept to the other,labeled by relation or event attributes.
An exam-ple is presented in Figure 2.
This common rep-resentation is applied in both mono-lingual andmulti-lingual information fusion tasks describedin next sections.Figure 2.
An example for cross-media commonfact representation3.3 Cross-media Information Inference?
Uncertainty Problem in Cross-Media Fu-sionHowever, such a simple merging approach usu-ally leads to unsatisfying results due to uncer-tainty.
Uncertainty in multimedia is inducedfrom noise in the data acquisition procedureChildPLOBritishMandate ofPalestineSafedMahmoudAbbasLeaderBirth-PlaceLocatedAmina AbbasSpouseYasserAbbasPLOElected/2008-11-23632(e.g., noise in automatic speech recognition re-sults and low-quality camera surveillance vid-eos) as well as human errors and subjectivity.Unstructured texts, especially those translatedfrom foreign languages, are difficult to interpret.In addition, automatic IE systems for both vid-eos and texts tend to produce errors.?
Case Study on Mention Gender DetectionWe employ cross-media inference methods toreduce uncertainty.
We will demonstrate thisapproach on a case study of gender detection forpersons.
Automatic gender detection is crucialto many natural language processing tasks suchas pronoun reference resolution (Bergsma,2005).
Gender detection for last names hasproved challenging; Gender for nominals can behighly ambiguous in various contexts.
Unfortu-nately most state-of-the-art approaches discovergender information without considering specificcontexts in the document.
The results werestored either as a knowledge base with prob-abilities (e.g.
Ji and Lin, 2009) or as a staticgazetteer (e.g.
census data).
Furthermore, speechrecognition normally performs poorly on names,which brings more challenges to gender detec-tion for mis-spelled names.We consider two approaches as our baselines.The first baseline is to discover gender knowl-edge from Google N-grams using specific lexi-cal patterns (e.g.
?
[mention] andhis/her/its/their?)
(Ji and Lin, 2009).
The otherbaseline is a gazetteer matching approach basedon census data including person names and gen-der information, as used in typical text IE sys-tems.We introduce the third method based onmale/female concept extraction from associatedbackground videos.
These concepts are detectedfrom context-dependent features (e.g.
face rec-ognition).
If there are multiple persons in onesnippet associated with one shot, we propagategender information to all instances.We then linearly combine these three methodsbased on confidence values.
For example, theconfidence of predicting a name mention n as amale (M) can be computed by combining prob-abilities P(n, M, method):confidence(n,male)=?1*P(n,M,ngram)+?2*P(n,M,census) +?3*P(n,M,video)In this paper we used?1=0.1, ?2=0.1and?3=0.8 which are optimized from a devel-opment set.4 Cross-lingual Comparable CorporaAcquisitionIn this section we extend the information fusionapproach to a task of discovering comparablecorpora.4.1 Comparable DocumentsFigure 3 presents an example of cross-lingualcomparable documents.
They are both about therescue activities for the Haiti earthquake.Figure 3.
An example for cross-lingualmulti-media comparable documentsFigure 4.
Cross-lingual Comparable TextCorpora Acquisition based onVideo Similarity ComputationMulti-mediaDocument inLanguage jMulti-mediaDocument inLanguage iFacts-ViSimilarity ComputationText T i Video V iConceptExtractionFacts-VjVideo V iConceptExtractionText T jSimilarity>?
?Comparable Docu-ments <Ti, Tj>633Traditional text translation based methods tendto miss such pairs due to poor translation qualityof informative words (Ji et al, 2009).
However,the background videos and images are language-independent and thus can be exploited to iden-tify such comparable documents.
This providesa cross-media approach to break language bar-rier.4.2 Cross-lingual System OverviewFigure 4 presents the general pipeline of discov-ering cross-lingual comparable documents basedon background video comparison.
The detailedvideo similarity computation method is pre-sented in next section.4.3 Video Concept Similarity ComputationMost document clustering systems use represen-tations built out of the lexical and syntactic at-tributes.
These attributes may involve stringmatching, agreement, syntactic distance, anddocument release dates.
Although gains havebeen made with such methods, there are clearlycases where shallow information will not be suf-ficient to resolve clustering correctly.
Therefore,we should therefore expect a successful docu-ment comparison approach to exploit worldknowledge, inference, and other forms of se-mantic information in order to resolve hardcases.
For example, if two documents includeconcepts referring to male-people, earthquakeevent, rescue activities, and facility-groundswith similar frequency information, we can de-termine they are likely to be comparable.
In thispaper we represent each video as a vector ofsemantic concepts extracted from videos andthen use standard vector space model to com-pute similarity.Let A=(a1, ?a|?|) and B=(b1, ?b|?|) be suchvectors for a pair of videos, then we use cosinesimilarity to compute similarity:| |1| | | |2 21 1cos( , ) i iii ii ia bA Ba b== =?=?
???
?,where |?
| contains all possible concepts.
Weuse traditional TF-IDF (Term Frequency-InverseDocument Frequency) weights for the vectorelements ai and bi.
Let C be a unique concept, Vis a video consisting of a series of k shots V ={S1, ?, Sk}, then:1( , ) ( , )kiitf C V tf C S k==?Let p(C, Si) denote the probability that C is ex-tracted from Si, we define two different ways tocompute term frequency tf (C, Si):(1) ( , ) ( , )i itf C S confidence C S=and(2) ( , )( , ) iconfidence C Sitf C S ?=Where Confidence (C, Si) denotes the probabil-ity of detecting a concept C in a shot Si:( , ) ( , )i iconfidence C S p C S= if ( , )ip C S ?> ,otherwise 0.Let: ( , ) 1idf C S = if ( , )ip C S ?> , otherwise 0,assuming there are j shots in the entire corpus,we calculate idf as follows:1( , ) log / ( , )jiiidf C V j df C S=?
?= ?
??
?
?5 Experimental ResultsThis section presents experimental results of allthe three tasks described above.5.1 DataWe used 244 videos from TRECVID 2005 dataset as our test set.
This data set includes 133,918keyframes, with corresponding automaticspeech recognition and translation results (forforeign languages) provided by LDC.5.2 Information Fusion ResultsTable 1 shows information fusion results forEnglish, Arabic and Chinese on multiple levels.It indicates that video and text extraction pipe-lines are complementary ?
almost all of thevideo concepts are about nominals and events;while text extraction output contains a largeamount of names and relations.
Therefore theresults after information fusion produced muchricher knowledge.634Annotation Lev-elsEnglish Chinese Arabic# of videos 104 84 56Video Concept 250880 221898 197233Name 17350 22154 20057Nominal 31528 21852 16253Relation 9645 20880 16584TextEvent 31132 10348 7148Table 1.
Information Fusion ResultsIt?s also worth noting that the number of con-cepts extracted from videos is similar acrosslanguages, while much fewer events are ex-tracted from Chinese or Arabic because ofspeech recognition and machine translation er-rors.
We took out 1% of the results to measureaccuracy against ground-truth in TRECVID andACE training data respectively; the mean aver-age precision for video concept extraction isabout 33.6%.
On English ASR output the text-IE system achieved about 82.7% F-measure onlabeling names, 80.5% F-measure on nominals(regardless of ASR errors), 66% on relationsand 64% on events.5.3 Information Inference ResultsFrom the test set, we chose 650 persons (492males and 158 females) to evaluate gender dis-covery.
For baselines, we used Google n-gram(n=5) corpus Version II including 1.2 billion 5-grams extracted from about 9.7 billion sentences(Lin et al, 2010) and census data including5,014 person names with gender information.Since we only have gold-standard gender in-formation on shot-level (corresponding to asnippet in ASR output), we asked a human an-notator to associate ground-truth with individualpersons.
Table 2 presents overall precision (P),recall (R) and F-measure (F).Methods P R FGoogle N-gram 89.1% 70.2% 78.5%Census 96.2% 19.4% 32.4%Video Extraction 88.9% 73.8% 80.6%Combined 89.3% 80.4% 84.6%Table 2.
Gender Discovery PerformanceTable 2 shows that video extraction based ap-proach can achieve the highest recall among allthree methods.
The combined approachachieved statistically significant improvementon recall.Table 3 presents some examples (?F?
for fe-male and ?M?
for male).
We found that mostspeech name recognition errors are propagatedto gender detection in the baseline methods, forexample, ?Sala Zhang?
is mis-spelled in speechrecognition output (the correct spelling shouldbe ?Sarah Chang?)
and thus Google N-gramapproach mistakenly predicted it as a male.Many rare names such as ?Wu Ficzek?,?Karami?
cannot be predicted by the baselines,Error analysis on video extraction based ap-proach showed that most errors occur on thoseshots including multiple people (males and fe-males).
In addition, since the data set is fromnews domain, there were many shots includingreporters and target persons at the same time.For example, ?Jiang Zemin?
was mistakenlyassociated with a ?female?
gender because thereporter is a female in that corresponding shot.5.4 Comparable Corpora Acquisition Re-sultsFor comparable corpora acquisition, we meas-ured accuracy for the top 50 document pairs.Due to lack of answer-keys, we asked a bi-lingual human annotator to judge results manu-ally.
The evaluation guideline generally fol-lowed the definitions in (Cheung and Fung,2004).
A pair of documents is judged as compa-rable if they share a certain amount of informa-tion (e.g.
entities, events and topics).Without using IDF, for different parameter ?and ?
in the similarity metrics, the results aresummarized in Figure 5.
For comparison wepresent the results for mono-lingual and cross-lingual separately.
Figure 5 indicates that as thethreshold and normalization values increase, theaccuracy generally improves.
It?s not surprisingthat mono-lingual results are better than cross-lingual results, because generally more videoswith comparable topics are in the same language.635MentionGoogleN-gramCensusVideoExtractionCorrectAnswerContext SentenceZhangSalaM: 1F: 0-F: 0.699M: 0.301FWorld famous meaning violin soloistZhang Sala recently again to Toronto sym-phony orchestra...PeterM: .979F: 0.021M: 1M: 0.699F: 0.301MIraq, there are in Lebanon Paris pass Peterafter 10 five Dar exile without peace...WuFiczek-M: 0.699F: 0.301MIf you want to do a good job indeed WuFiczekPresidentM: .953F: 0.047-M: 0.704F: 0.296MLabor union of Arab heritage publisherspresident to call for the opening of theArab Book Exhibition.JiangZeminM: 1F: 0-F: 0.787M: 0.213MIt has never stopped the including the for-mer CPC General Secretary Jiang Zemin?KaramiM: 1F: 0-M: 0.694F: 0.306Mall the Gamal Ismail introduced the needsof the Akkar region, referring to the desireon the issue of the President Karami togive priority disadvantaged areasTable 3.
Examples for Mention Gender DetectionFigure 5.
Comparable Corpora Acquisitionwithout IDFWe then added IDF to the optimized thresholdand obtained results in Figure 6.
The accuracyfor both languages was further enhanced.
We cansee that under any conditions our approach candiscover comparable documents reliably.
In or-der to measure the impact of concept extractionerrors, we also evaluated the results for usingground-truth concepts as shown in Figure 6.
Sur-prisingly it didn?t provide much higher accuracythan automatic concept extraction, mainly be-cause the similarity can be captured by somedominant video concepts.Figure 6.
Comparable Corpora Acquisition withIDF (?=0.6)6 Related WorkA large body of prior work has focused on multi-media information retrieval and document classi-fication (e.g.
Iria and Magalhaes, 2009).
State-of-the-art information fusion approaches can bedivided into two groups: formal ?top-down?methods from the generic knowledge fusioncommunity and quantitative ?bottom-up?
tech-niques from the Semantic Web community (Ap-priou et al, 2001; Gregoire, 2006).
However,very limited research methods have been ex-636plored to fuse automatically extracted facts fromtexts and videos/images.
Our idea of conductinginformation fusion on multiple semantic levels issimilar to the kernel method described in (Gu etal., 2007).Most previous work on cross-media informationextraction focused on one single domain (e.g.
e-Government (Amato et al, 2010); soccer game(Pazouki and Rahmati, 2009)) and struc-tured/semi-structured texts (e.g.
product cata-logues (Labsky et al, 2005)).
Saggion et al(2004) described a multimedia extraction ap-proach to create composite index from multipleand multi-lingual sources.
We expand the task tothe more general news domain including unstruc-tured texts and use cross-media inference to en-hance extraction performance.Some recent work has exploited analysis of as-sociated texts to improve image annotation (e.g.Deschacht and Moens, 2007; Feng and Lapata,2008).
Some recent research demonstrated cross-modal integration can provide significant gainsin improving the richness of information.
Forexample, Oviatt et al (1997) showed that speechand pen-based gestures can provide complemen-tary capabilities because basic subject, verb, andobject constituents almost always are spoken,whereas those describing locative informationinvariably are written or gestured.
However, notmuch work demonstrated an effective method ofusing video/image annotation to improve textextraction.
Our experiments provide some casestudies in this new direction.
Our work can alsobe considered as an extension of global back-ground inference (e.g.
Ji and Grishman, 2008) tocross-media paradigm.Extensive research has been done on video clus-tering.
For example, Cheung and Zakhor (2000)used meta-data extracted from textual and hyper-link information to detect similar videos on theweb; Magalhaes et al (2008) described a seman-tic similarity metric based on key word vectorsfor multi-media fusion.
We extend such videosimilarity computing approaches to a multi-lingual environment.7 Conclusion and Future WorkTraditional Information Extraction (IE) ap-proaches focused on single media (e.g.
texts),with very limited use of knowledge from otherdata modalities in the background.
In this paperwe propose a new approach to integrate informa-tion extracted from videos and texts into a coher-ent common representation including multi-levelknowledge (concepts, relations and events).
Be-yond standard information fusion, we attemptedglobal inference methods to incorporate videoextraction and significantly enhanced the per-formance of text extraction.
Finally, we extendour methods to multi-lingual environment (Eng-lish, Arabic and Chinese) by presenting a casestudy on cross-lingual comparable corpora acqui-sition.We used a dataset which includes videos andassociated speech recognition output (texts), butour approach is applicable to any cases in whichtexts and videos appear together (from associatedtexts, captions etc.).
The proposed common rep-resentation will provide a framework for manybyproducts.
For example, the monolingual fusedinformation graphs can be used to generate ab-stractive summaries.
Given the fused informationwe can also visualize the facts from backgroundtexts effectively.
We are also interested in usingvideo information to discover novel relations andevents which are missed in the text IE task.AcknowledgementThis work was supported by the U.S. Army Re-search Laboratory under Cooperative AgreementNumber W911NF-09-2-0053, the U.S. NSFCAREER Award under Grant IIS-0953149,Google, Inc., DARPA GALE Program, CUNYResearch Enhancement Program, PSC-CUNYResearch Program, Faculty Publication Programand GRTI Program.
The views and conclusionscontained in this document are those of the au-thors and should not be interpreted as represent-ing the official policies, either expressed or im-plied, of the Army Research Laboratory or theU.S.
Government.
The U.S. Government is au-thorized to reproduce and distribute reprints forGovernment purposes notwithstanding any copy-right notation here on.ReferencesAmato, F., Mazzeo, A.,  Moscato, V. and Picariello,A.
2010.
Information Extraction from MultimediaDocuments for e-Government Applications.Information Systems: People, Organizations, Insti-tutions, and Technologies.
pp.
101-108.637Appriou A., A. Ayoun, Benferhat, S., Besnard, P.,Cholvy, L., Cooke, R., Cuppens, F., Dubois, D.,Fargier, H., Grabisch, M., Kruse, R., Lang, J.Moral, S., Prade, H., Saffiotti, A., Smets, P., Sos-sai, C. 2001.
Fusion: General concepts and charac-teristics.
International Journal of Intelligent Sys-tems 16(10).Baluja, S. and Rowley, H. 2006.
Boosting Sex Identi-fication Performance.
International Journal ofComputer Vision.Bergsma, S. 2005.
Automatic Acquisition of GenderInformation for Anaphora Resolution.
Proc.
Cana-dian AI 2005.Cheung, P. and Fung P. 2004.
Sentence Alignment inParallel, Comparable, and Quasi-comparable Cor-pora.
Proc.
LREC 2004.Cheung, S.-C.  and Zakhor, A.
2000.
Efficient videosimilarity measurement and search.
Proc.
IEEE In-ternational Conference on Image Processing.Deschacht K. and Moens M. 2007.
Text Analysis forAutomatic Image Annotation.
Proc.
ACL 2007.Feng, Y. and Lapata, M. 2008.
Automatic Image An-notation Using Auxiliary Text Information.
Proc.ACL 2008.Gregoire, E. 2006.
An unbiased approach to iteratedfusion by weakening.
Information Fusion.
7(1).Gu, Z., Mei, T., Hua, X., Tang, J., Wu, X.
2007.Multi-Layer Multi-Instance Kernel for Video Con-cept Detection.
Proc.
ACM Multimedia 2007.Hakkani-Tur, D., Ji, H. and Grishman, R. 2007.
UsingInformation Extraction to Improve Cross-lingualDocument Retrieval.
Proc.
RANLP 2007 Workshopon Multi-Source Multi-lingual Information Extrac-tion and Summarization.Iria, J. and Magalhaes, J.
2009.
Exploiting Cross-Media Correlations in the Categorization of Mul-timedia Web Documents.
Proc.
CIAM 2009.Ji, H. and Grishman, R. 2008.
Refining Event Extrac-tion Through Cross-document Inference.
Proc.ACL 2008.Ji, H. 2009.
Mining Name Translations from Compa-rable Corpora by Creating Bilingual InformationNetworks.
Proc.
ACL-IJCNLP 2009 workshop onBuilding and Using Comparable Corpora (BUCC2009): from parallel to non-parallel corpora.Ji, H., Grishman, R., Freitag, D., Blume, M., Wang,J., Khadivi, S., Zens, R., and Ney, H. 2009.
NameTranslation for Distillation.
Handbook of NaturalLanguage Processing and Machine Translation:DARPA Global Autonomous Language Exploita-tion.
Springer.Ji, H. and Lin, D. 2009.
Gender and Animacy Knowl-edge Discovery from Web-Scale N-Grams for Un-supervised Person Mention Detection.
Proc.
PA-CLIC 2009.Oviatt, S. L., DeAngeli, A., & Kuhn, K. 1997.
Inte-gration and synchronization of input modes duringmultimodal human-computer interaction.
Proceed-ings of Conference on Human Factors in Comput-ing Systems (CHI?97), 415-422.
New York: ACMPress.Labsky, M., Praks, P., Sv?atek1, V., and Svab, O.2005.
Multimedia Information Extraction fromHTML Product Catalogues.
Proc.
2005IEEE/WIC/ACM International Conference on WebIntelligence.
pp.
401 ?
404.Lin, D., Church, K.,  Ji, H., Sekine, S., Yarowsky, D.,Bergsma, S., Patil, K., Pitler, E., Lathbury, R., Rao,V., Dalwani, K. and Narsale, S. 2010.
New Data,Tags and Tools for Web-Scale N-grams.
Proc.LREC 2010.Magalhaes, J., Ciravegna, F. and Ruger, S. 2008.
Ex-ploring Multimedia in a Keyword Space.
Proc.ACM Multimedia 2008.Munteanu, D. S. and Marcu D. 2005.
Improving Ma-chine Translation Performance by Exploiting Non-Parallel Corpora.
Computational Linguistics.
Vol-ume 31, Issue 4. pp.
477-504.Naphade, M. R., Kennedy, L., Kender, J. R., Chang,S.-F., Smith, J. R., Over, P., and Hauptmann, A. Alight scale concept ontology for multimedia under-standing for TRECVID 2005.
Technical report,IBM, 2005.Pazouki, E. and Rahmati, M. 2009.
A novel multime-dia data mining framework for information extrac-tion of a soccer video stream.
Intelligent DataAnalysis.
pp.
833-857.Qi,G.-J., Hua,X.-S., Rui, Y., Tang, J., Mei, T., andZhang,H.-J.
2007.
Correlative Multi-label VideoAnnotation.
Proc.
ACM Multimedia 2007.Saggion, H., Cunningham, H., Bontcheva, K.,Maynard, D., Hamza, O., and Wilks, Y.
2004.
Mul-timedia indexing through multi-source and multi-language information extraction: the MUMIS pro-ject.
Data Knowlege Engineering, 48, 2, pp.
247-264.Wang, F. and Zhang, C. 2006.
Label propagationthrough linear neighborhoods.
Proc.
ICML 2006.638
