Proceedings of the MultiLing 2013 Workshop on Multilingual Multi-document Summarization, pages 72?76,Sofia, Bulgaria, August 9 2013. c?2013 Association for Computational LinguisticsMultilingual summarization system based on analyzing the discoursestructure at MultiLing 2013Daniel Alexandru Anechitei?Al.
I. Cuza?
University of Iasi,Faculty of Computer Science16,General Berthelot St., 700483, Iasi,Romaniadaniel.anechitei@info.uaic.roEugen Ignat?Al.
I. Cuza?
University of Iasi,Faculty of Computer Science16,General Berthelot St., 700483, Iasi,Romaniaeugen.ignat@info.uaic.roAbstractThis paper describes the architecture ofUAIC1?s Summarization system participatingat MultiLing ?
2013.
The architecture includeslanguage independent text processing mod-ules, but also modules that are adapted for onelanguage or another.
In our experiments, thelanguages under consideration are Bulgarian,German, Greek, English, and Romanian.
Ourmethod exploits the cohesion and coherenceproperties of texts to build discourse struc-tures.
The output of the parsing process is usedto extract general summaries.1 IntroductionAutomatic text summarization is a well studiedresearch area and has been active for many years.In this paper, we describe the automatic textsummarization system implemented by UAIC forparticipation at MultiLing 2013 single documenttrack.
Our approach to summarization followsthe one presented in (Anechitei et al 2013).
Thesummarization architecture that this system usesincludes two main parts that can be viewed inFigure 1.
The text is passed to the language pro-cessing chain (LPC) which processes the data.As revealed from the figure each language has itsown LPC.
The LPC?s, acts as a prerequisite forthe summarization meta tool (SMT).
In this pa-per we will focus more on the SMT engine,which is composed of four modules: anaphoraresolution (AR), clause splitter (CS), discourseparser (DP) and the proper summarizer (SUM).The intermediate format between the modulesconsists of XML files.
The summary of a text is1 University ?Al.
I. Cuza?
of Iasi, Romaniaobtained as a sequence of discourse clauses ex-tracted from the original text, after obtaining thediscourse structure of the text and exploiting thecohesion and coherence properties.Figure 1: Summarization system architecture2 Language Processing ChainsEvery document is analyzed by the LPC in thefollowing consecutive steps: sentence splitter,tokenizer, Part of Speech tagger, lemmatizer,Noun phrase extractor and Named entity recog-nizer.
All tools are self-contained and designed72to work in a chain, i.e.
the output of the previouscomponent is the input for the next component.3 Anaphora ResolutionAnaphora resolution is one of the key steps ofthe discourse parser, by resolving anaphoric pro-nouns, automatically generated summaries maybe more cohesive and, thus, more coherent.
Cal-culating scores for references and transitionswould be impossible without the proper identifi-cation of the co-referential chains.Anaphora resolution is defined in (Or?san et.al, 2008) as the process of resolving an anaphoricexpression to the expression it refers to.
The toolused for the anaphora resolution named RARE(Robust Anaphora Resolution Engine) uses thework done in (Cristea and Dima, 2001), wherethe process implies three layers (Figure 2):?
The text layer, containing referential ex-pressions(RE) as they appear in the dis-course;?
An intermediate layer (projection layer)that contains any specific informationthat can be extracted from the corre-sponding referential expressions.?
A semantic layer that contains descrip-tions of the discourse entities (DE).
Herethe information contributed by chains ofreferential expressions is accumulated.Figure 2: Three layers representation of co-referencing REs (Cristea and Dima, 2001)The core of the system is language independ-ent, but in order to localize it to one language oranother it requires specific resources.
These spe-cific resources are as follows:?
constraints ?
containing the rules thatmatch the conditions between anaphorand antecedent;?
stopwords ?
containing a list ofstopwords;?
tagset ?
implies a mapping from thetagset used in the input file to a moresimplified tagset used by the system.?
window ?
here is defined the length ofthe window where the antecedent shouldbe looked for by the system.The process of anaphora resolution runs as fol-lows: The text is ?read?
from the left to right.When a new NP is found, a new RE is createdand contains the morphological, syntactic andsemantic features.
All the features are tested us-ing the constraints and it is decided whether theRE introduces a new discourse entity, not men-tioned before, or it revokes one already men-tioned.4 Clause SplitterNumerous techniques are used to recognizeclause boundaries for different languages, wheresome are rule based (Leffa, 1988), and others arehybrid methods, like in (Parven et al 2011) and(Or?san, 2000), where the results of a machinelearning algorithm, trained on an annotated cor-pus, are processed by a shallow rule-based mod-ule in order to improve the accuracy of the meth-od.
Our approach to discourse segmentationstarts from the assumption that a clause is headedby a main verb, like ?go?
or a verbal compound,like ?like to swim?
(Ex.1).
Verbs and verb com-pounds are considered pivots and clause bounda-ries are looked for in-between them.Ex.
1 <When I go to river>< I like to swim with myfriends.>Verb compounds are sequences of more thanone verb in which one is the main verb and theothers are auxiliaries, infinitives, conjunctivesthat complement the main verb and the semanticsof the main verb in context obliges to take thewhole construction together.
The CS modulesegments the input by applying a machine learn-ing algorithm, to classify pairs of verbs as beingor not compound verbs and, after that, applyingrules and heuristics based on pattern matching ormachine learning algorithms to identify theclause boundary.
The exact place of a clauseboundary between verbal phrases is best indicat-ed by discourse markers.
A discourse marker,like ?because?
(Ex.1), or, simply, marker, is aword or a group of words having the function tosignal a clause boundary and/or to signal a rhe-torical relation between two text spans.Ex.
1 <Markers are good><because they can giveinformation on boundaries and discourse structure.>73When markers are missing, boundaries are foundby statistical methods, which are trained on ex-plicit annotations given in manually built files.Based on the manually annotated files, a trainingmodule extracts two models (one for the CSmodule and one for the DP module).
These mod-els incorporate patterns of use of markers used todecide the segmentation boundaries and also toidentify rhetorical relations between spans oftext.
The clauses act as terminal nodes in theprocess of discourse parsing which is describedbelow.5 Discourse ParserDiscourse parsing is the process of building ahierarchical model of a discourse from its basicelements (sentences or clauses), as one wouldbuild a parse of a sentence from its words (Ban-galore and Stent, 2009).
Rhetorical StructureTheory (Mann and Thompson, 1988) is one ofthe most popular discourse theories.
In RST atext segment assumes one of two roles in a rela-tionship: the nucleus (N) or satellite (S).
Nucleiexpress what is more essential to the understand-ing of the narrative than the satellites.
Our Dis-course Parser uses a symbolic approach and pro-duces discourse trees, which include nuclearity,but lacking rhetorical relation names: intermedi-ate nodes in the discourse tree have no name andterminal nodes are elementary discourse units,mainly clauses.
It adopts an incremental policy indeveloping the trees, on three levels (paragraphs,sentences and clauses) by consuming, recursive-ly, one entire structure of an inferior level, byattaching the elementary discourse tree (edt) ofthe last structure to the already developed tree onthe right frontier (Cristea and Webber, 1997).First, an edt of each sentence is produced usingincremental parsing, by consuming each clausewithin the sentence.
Secondly, the edt of the par-agraph is produced by consuming each sentencewithin the paragraph.
The same approach is usedat discourse level by attaching the paragraph treeof each paragraph to the already developed tree.The criterion to guide the discourse parsing isrepresented by the principle of sequentiality(Marcu, 2000).
The incremental discourse pars-ing approach borrows the two operations used in(L)TAG (lexicalized tree-adjoining grammar)(Joshi and Schabes, 1997): adjunction and sub-stitution.Adjunction operation (Figure 3) occurs onlyon the right frontier and it takes an initial or de-veloping tree (D-treei-1), creating a new develop-ing tree (D-treei) by combining D-treei-1 with anauxiliary tree (A-tree), by replacing the foot nodewith the cropped tree.
This is done for each nodeon the right frontier resulting in multiple D-trees.Figure 3 depicts this idea.Figure 3: Adjunction operationSubstitution operation (Figure 4) replaces aplaced node on a terminal frontier, called substi-tution node, with an auxiliary tree (Figure 14).Figure 4: Substitution operationThe uses of different types of auxiliary trees(Figure 5) are determined by two factors:?
the type of operation in which are used:alpha and beta are used only for adjunc-tion operations and gamma and delta forsubstitution operations;?
the auxiliary tree introduces or not anexpectation: beta and gamma are auxilia-ry trees that raise an expectation and al-pha an delta are auxiliary trees which donot raise an expectation.Figure 5: Types of auxiliary trees74At each parsing step there is a module whichdecides the type of the auxiliary tree betweenalpha, gamma, beta, delta (Anechitei et al2013) together with the relations type (R1 and R2,which can be N_N, N_S or S_N; the notationexpress the nuclearity of the child nodes: left oneand the right one) by analyzing the structurewhich is processed (clause, sentence or para-graph).
This module uses the compiled modeldescribed in previous section and doesn?t pro-duce a unique auxiliary tree for each structurebut rather a set of trees.At each level, the parser goes on with a forestof developing trees in parallel, ranking them by aglobal score (Figure 6) based on heuristics thatare suggested by both Veins Theory (Cristea etal., 1998) and Centering Theory (Grosz et al1995).
After normalizing the score for each heu-ristic, the global score is computed by summingthe score of one heuristic with the correspondingweight.
The weights were established after a cal-ibration process.Figure 6: Global score for each discourse treeThe trees used at the next step are only thebest ranked trees.
The aim of this filtering step isto reduce the exponential explosion of the ob-tained trees.
For this task the threshold was set tofive best trees from iteration to another and six(N=6) heuristics chosen in a way to maximizethe coherence of the discourse structure and im-plicitly the coherence of the summary.6 The SummarizerThe mentioned system produces excerpt typesummaries, which are summaries that copy con-tiguous sequences of tokens from the originaltext.The structure of a discourse as a complete treegives more information than properly needed (atleast for summarization purpose).
By exploitingthe discourse structure, we expect to add cohe-sion and coherence to our summaries.
From thediscourse structure we can extract three types ofsummaries: general summaries, entity focusedsummaries and clause focused summaries.
Forthe summarization task we only extracted thegeneral summary.
The module that extracts thesummaries (SUM) takes the tree of a discoursestructure and produces a general summary, of acertain length, depending on the length of thecomputed vein (Cristea et al 1998).
As the tasksupposed summaries containing a maximum of250 words and the summaries the system wasproviding were always bigger, a new scoring sys-tem was needed.
This scoring system needed toshorten the summaries to under 250 words, yetkeep as much coherence and cohesion as the sys-tem provided.
For this end the scoring systemtook all the clauses from the vein and scoredthem as follows: in each clause the noun phraseswere found, for each noun phrase a coreferentialscore was given.
These scores are added andcomputed for each clause.
The clauses were sort-ed and only the first N clauses were selectedsuch as the maximum coherence was retained,where N is the number of the clauses so that thefinal summaries are below the word countthreshold.
The score for each noun phrase is giv-en taking into account how big the coreferencechain is.7 Conclusion and ResultsThis year, the evaluation at MultiLing 2013was performed automatically using N-gramgraph methods, which were interchangeable inthe single document setting.
Below we providethe results based on average NPowER  grades.Lang UAIC Mary-land (I)Mary-land (II)Mary-land (II)BaselineBG 1.538 1.600 1.593 1.600 1.310DE 1.537 1.64 1.612 1.617 1.289EL 1.560 1.501 1.513 1.494 1.314EN 1.646 1.641 1.661 1.656 1.367RO 1.627 1.655 1.679 1.680 1.3461.582 1.607 1.611 1.609 1.325Table 1: Table with resultsTable 1 shows the comparison betweenUAIC?s system and Maryland?s system, as it wasthe only other system, besides the baseline, thatran on the same 5 languages.
Generally the re-sults of both systems are close as the averagefigure shows.
For our first participation the re-sults are encouraging for this complex system,which has the possibility of running on multiplelanguages.
Our future work should reside in thescorer of the summarizer, as the approach usuallycreates summaries bigger than 250 words.75ReferencesAnechitei A. Daniel, Cristea Dan, Dimosthenis Ioan-nidis, Ignat Eugen, Karagiozov Diman, KoevaSvetla, Kope?
Mateusz and Vertan Cristina.
2013.Summarizing Short Texts Through a Discourse-Centered Approach in a Multilingual Context.
InNeustein, A., Markowitz, J.A.
(eds.
), Where Hu-mans Meet Machines: Innovative Solutions toKnotty Natural Language Problems.
SpringerVerlag, Heidelberg/New York.Bangalore Srinivas and Stent J. Amanda.
2009.
In-cremental parsing models for dialog task structure,in Proceedings of the 12th Conference of the Euro-pean Chapter of the Association for ComputationalLinguistics.Cristea Dan and Webber Bonnie.
1997.
Expectationsin incremental discourse processing.
In Proceed-ings of the 8th Conference of the European Chapterof the Association for Computational Linguistics.Cristea Dan, Ide Nancy and Romary Laurent.
1998:Veins Theory: A Model of Global Discourse Cohe-sion and Coherence, in Proceedings of the 17th in-ternational conference on Computational linguis-tics.Cristea Dan and Dima E. Gabriela.
2001.
An integrat-ing framework for anaphora resolution.
In Infor-mation Science and Technology, Romanian Acad-emy Publishing House, Bucharest, vol.
4, no.
3-4, p273-291.Grosz J. Barbara, Joshi K. Arvind and WeinsteinScott.
1995.
Centering: A Framework for Model-ling the Local Coherence of Discourse.
Computa-tional Linguistics, 21/2: 203-25.Joshi K. Aravind and Schabes Yves.
1997: Tree-Adjoining Grammars.
In G. Rozenberg andA.Salomaa, editors, Handbook of Formal lan-guages.Leffa J. Vilson.
1988.
Clause processing in complexsentences.
In Proceedings of the First InternationalConference on Language Resource and Evaluation,volume 1, pages 937 ?
943, May 1998.Mann C. William and Thompson A. Sandra.
1988.Rhetorical structure theory: a theory of text organ-ization.
Text 8(3):243?281.Marcu Daniel.
2000.
The Theory and Practice of Dis-course Parsing and Summarization.
The MITPress, November 2000.Orasan Constantin.
2000.
A hybrid method for clausesplitting in unrestricted English texts.
In Proceed-ings of ACIDCA, Corpora and Natural LanguageProcessing, March 22-24, Monastir, Tunisia, pp.129 ?
134.Or?san Constantin, Cristea Dan, Mitkov Ruslan andBranco Antonio.
2008.
Anaphora Resolution Exer-cise ?
An Overview.
In Proceedings of LREC-2008,Marrakech, Morocco.Parveen Daraksha, Sanyal Ratna and Ansari Afreen.2011.
Clause Boundary Identification using Classi-fier and Clause Markers in Urdu Language.Polibits Research Journal on Computer Science,43, pp.
61-65.76
