Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 301?307,Sydney, July 2006. c?2006 Association for Computational LinguisticsLearning Phrasal CategoriesWilliam P. Headden III, Eugene Charniak and Mark JohnsonBrown Laboratory for Linguistic Information Processing (BLLIP)Brown UniversityProvidence, RI 02912{headdenw|ec|mj}@cs.brown.eduAbstractIn this work we learn clusters of contex-tual annotations for non-terminals in thePenn Treebank.
Perhaps the best wayto think about this problem is to contrastour work with that of Klein and Man-ning (2003).
That research used tree-transformations to create various gram-mars with different contextual annotationson the non-terminals.
These grammarswere then used in conjunction with a CKYparser.
The authors explored the spaceof different annotation combinations byhand.
Here we try to automate the process?
to learn the ?right?
combination auto-matically.
Our results are not quite as goodas those carefully created by hand, but theyare close (84.8 vs 85.7).1 Introduction and Previous ResearchIt is by now commonplace knowledge that accu-rate syntactic parsing is not possible given onlya context-free grammar with standard Penn Tree-bank (Marcus et al, 1993) labels (e.g., S, NP ,etc.)
(Charniak, 1996).
Instead researcherscondition parsing decisions on many other fea-tures, such as parent phrase-marker, and, fa-mously, the lexical-head of the phrase (Mager-man, 1995; Collins, 1996; Collins, 1997; Johnson,1998; Charniak, 2000; Henderson, 2003; Kleinand Manning, 2003; Matsuzaki et al, 2005) (andothers).One particularly perspicuous way to view theuse of extra conditioning information is that oftree-transformation (Johnson, 1998; Klein andManning, 2003).
Rather than imagining the parserroaming around the tree for picking up the infor-mation it needs, we rather relabel the nodes to di-rectly encode this information.
Thus rather thanhave the parser ?look?
to find out that, say, theparent of some NP is an S, we simply relabel theNP as an NP [S].This viewpoint is even more compelling if onedoes not intend to smooth the probabilities.
Forexample, consider p(NP ?
PRN | NP [S]) Ifwe have no intention of backing off this probabil-ity to p(NP ?
PRN | NP ) we can treat NP [S]as an uninterpreted phrasal category and run allof the standard PCFG algorithms without change.The result is a vastly simplified parser.
This is ex-actly what is done by Klein and Manning (2003).Thus the ?phrasal categories?
of our title referto these new, hybrid categories, such as NP [S].We hope to learn which of these categories workbest given that they cannot be made too specificbecause that would create sparse data problems.The Klein and Manning (2003) parser is an un-lexicalized PCFG with various carefully selectedcontext annotations.
Their model uses some par-ent annotations, and marks nodes which initiate orin certain cases conclude unary productions.
Theyalso propose linguistically motivated annotationsfor several tags, including V P , IN , CC ,NP andS.
This results in a reasonably accurate unlexical-ized PCFG parser.The downside of this approach is that their fea-tures are very specific, applying different annota-tions to different treebank nonterminals.
For in-stance, they mark right-recursive NP s and notV P s (i.e., an NP which is the right-most childof another NP ).
This is because data sparsity is-sues preclude annotating the nodes in the treebanktoo liberally.
The goal of our work is to automatethe process a bit, by annotating with more generalfeatures that apply broadly, and by learning clus-301ters of these annotations.Mohri and Roark (2006) tackle this problem bysearching for what they call ?structural zeros?orsets of events which are individually very likely,but are unlikely to coincide.
This is to be con-trasted with sets of events that do not appear to-gether simply because of sparse data.
They con-sider a variety of statistical tests to decide whethera joint event is a structural zero.
They mark thehighest scoring nonterminals that are part of thesejoint events in the treebank, and use the resultingPCFG.Coming to this problem from the standpoint oftree transformation, we naturally view our workas a descendent of Johnson (1998) and Klein andManning (2003).
In retrospect, however, there areperhaps even greater similarities to that of (Mager-man, 1995; Henderson, 2003; Matsuzaki et al,2005).
Consider the approach of Matsuzaki et al(2005).
They posit a series of latent annotationsfor each nonterminal, and learn a grammar usingan EM algorithm similar to the inside-outside al-gorithm.
Their approach, however, requires thenumber of annotations to be specified ahead oftime, and assigns the same number of annotationsto each treebank nonterminal.
We would like toinfer the number of annotations for each nonter-minal automatically.However, again in retrospect, it is in the work ofMagerman (1995) that we see the greatest similar-ity.
Rather than talking about clustering nodes, aswe do, Magerman creates a decision tree, but thedifferences between clustering and decision treesare small.
Perhaps a more substantial differenceis that by not casting his problem as one of learn-ing phrasal categories Magerman loses all of thefree PCFG technology that we can leverage.
Forinstance, Magerman must use heuristic search tofind his parses and incurs search errors because ofit.
We use an efficient CKY algorithm to do ex-haustive search in reasonable time.Belz (2002) considers the problem in a man-ner more similar to our approach.
Beginning withboth a non-annotated grammar and a parent anno-tated grammar, using a beam search they searchthe space of grammars which can be attained viamerging nonterminals.
They guide the search us-ing the performance on parsing (and several othertasks) of the grammar at each stage in the search.In contrast, our approach explores the space ofgrammars by starting with few nonterminals andsplitting them.
We also consider a much widerrange of contextual information than just parentphrase-markers.2 BackgroundA PCFG is a tuple (V,M,?0, R, q : R ?
[0, 1]),where V is a set of terminal symbols; M = {?i}is a set of nonterminal symbols; ?0 is a start orroot symbol; R is a set of productions of the form?i ?
?, where ?
is a sequence of terminals andnonterminals; and q is a family of probability dis-tributions over rules conditioned on each rule?sleft-hand side.As in (Johnson, 1998) and (Klein and Man-ning, 2003), we annotate the Penn treebank non-terminals with various context information.
Sup-pose ?
is a Treebank non-terminal.
Let ?
= ?[?
]denote the non-terminal category annotated with avector of context features ?.
A PCFG is derivedfrom the trees in the usual manner, with produc-tion rules taken directly from the annotated trees,and the probability of an annotated rule q(?
??)
= C(???)C(?)
where C(?
?
?)
and C(?)
are thenumber of observations of the production and itsleft hand side, respectively.We refer to the grammar resulting from extract-ing annotated productions directly out of the tree-bank as the base grammar.Our goal is to partition the set of annotated non-terminals into clusters ?
= {?i}.
Each possibleclustering corresponds to a PCFG, with the set ofnon-terminals corresponding to the set of clusters.The probability of a production under this PCFGisp(?i ?
?j?k) =C(?i ?
?j?k)C(?i)where ?s ?
?
are clusters of annotated non-terminals and where:C(?i ?
?j?k .
.
.)
=?
(?i,?j ,?k...)??i??j?
?k...C(?i ?
?j?k .
.
.
)We refer to the PCFG of some clustering as theclustered grammar.2.1 FeaturesMost of the features we use are fairly standard.These include the label of the parent and grand-parent of a node, its lexical head, and the part ofspeech of the head.Klein and Manning (2003) find marking non-terminals which have unary rewrites to be helpful.302They also find useful annotating two preterminals(DT ,RB) if they are the product of a unary pro-duction.
We generalize this via two width features:the first marking a node with the number of non-terminals to which it rewrites; the second markingeach preterminal with the width of its parent.Another feature is the span of a nonterminal, orthe number of terminals it dominates, which wenormalize by dividing by the length of the sen-tence.
Hence preterminals have normalized spansof 1/(length of the sentence), while the root has anormalized span of 1.Extending on the notion of a Base NP, intro-duced by Collins (1996), we mark any nonter-minal that dominates only preterminals as Base.Collins inserts a unary NP over any base NPs with-out NP parents.
However, Klein and Manning(2003) find that this hurts performance relative tojust marking the NPs, and so our Base feature doesnot insert.We have two features describing a node?s posi-tion in the expansion of its parent.
The first, whichwe call the inside position, specifies the nonter-minal?s position relative to the heir of its parent?shead, (to the left or right) or whether the nontermi-nal is the heir.
(By ?heir?
we mean the constituentdonates its head, e.g.
the heir of an S is typicallythe V P under the S.) The second feature, outsideposition, specifies the nonterminal?s position rel-ative to the boundary of the constituent: it is theleftmost child, the rightmost child, or neither.Related to this, we further noticed that severalof Klein & Manning?s (2003) features, such asmarking NP s as right recursive or possessive havethe property of annotating with the label of therightmost child (when they are NP and POS re-spectively).
We generalize this by marking allnodes both with their rightmost child and (an anal-ogous feature) leftmost child.We also mark whether or not a node bordersthe end of a sentence, save for ending punctuation.
(For instance, in this sentence, all the constituentswith the second ?marked?
rightmost in their spanwould be marked).Another Klein and Manning (2003) feature wetry includes the temporal NP feature, where TMPmarkings in the treebank are retained, and propa-gated down the head inheritance path of the tree.It is worth mentioning that all the features herecome directly from the treebank.
For instance, thepart of speech of the head feature has values onlyfrom the raw treebank tag set.
When a preterminalcluster is split, this assignment does not change thevalue of this feature.3 ClusteringThe input to the clusterer is a set of annotatedgrammar productions and counts.
Our clusteringalgorithm is a divisive one reminiscent of (Martinet al, 1995).
We start with a single cluster for eachTreebank nonterminal and one additional clusterfor intermediate nodes, which are described in sec-tion 3.2.The clustering method has two interleavedparts: one in which candidate splits are generated,and one in which we choose a candidate split toenact.For each of the initial clusters, we generate acandidate split, and place that split in a prior-ity queue.
The priority queue is ordered by theBayesian Information Criterion (BIC), e.g.
(Hastieet al, 2003).The BIC of a model M is defined as -2*(loglikelihood of the data according to M ) +dM*(lognumber of observations).
dM is the number of de-grees of freedom in the model, which for a PCFGis the number of productions minus the numberof nonterminals.
Thus in this context BIC can bethought of as optimizing the likelihood, but with apenalty against grammars with many rules.While the queue is nonempty, we remove a can-didate split to reevaluate.
Reevaluation is neces-sary because, if there is a delay between when asplit is proposed and when a split is enacted, thegrammar used to score the split will have changed.However, we suppose that the old score is closeenough to be a reasonable ordering measure forthe priority queue.
If the reevaluated candidate isno longer better than the second candidate on thequeue, we reinsert it and continue.
However, if itis still the best on the queue, and it improves themodel, we enact the split; otherwise it is discarded.When a split is enacted, the old cluster is re-moved from the set of nonterminals, and is re-placed with the two new nonterminals of the split.A candidate split for each of the two new clustersis generated, and placed on the priority queue.This process of reevaluation, enacting splits,and generating new candidates continues until thepriority queue is empty of potential splits.We select a candidate split of a particular clusteras follows.
For each context feature we generate303S^ROOTNP^SNNP^NPRexCC^NPandNNP^NPGingerVP^SVBD^VPranNP^VPNNhomeFigure 1: A Parent annotated tree.a potential nominee split.
To do this we first par-tition randomly the values for the feature into twobuckets.
We then repeatedly try to move valuesfrom one bucket to the other.
If doing so resultsin an improvement to the likelihood of the trainingdata, we keep the change, otherwise we reject it.The swapping continues until moving no individ-ual value results in an improvement in likelihood.Suppose we have a grammar derived from a cor-pus of a single tree, whose nodes have been anno-tated with their parent as in Figure 1.
The baseproductions for this corpus are:S[ROOT ] ?
NP [S] V P [S] 1/1V P [S] ?
V BD[V P ] NP [V P ] 1/1NP [S] ?
NP [NP ] CC[NP ] NP [NP ] 1/1NP [V P ] ?
NN [NP ] 1/1NP [NP ] ?
NNP [NP ] 2/2Suppose we are in the initial state, with a singlecluster for each treebank nonterminal.
Considera potential split of the NP cluster on the par-ent feature, which in this example has three val-ues: S, V P , and NP .
If the S and V P val-ues are grouped together in the left bucket, andthe NP value is alone in the right bucket, we getcluster nonterminals NPL = {NP [S], NP [V P ]}and NPR = {NP [NP ]}.
The resulting grammarrules and their probabilities are:S ?
NPL V P 1/1V P ?
V BD NPL 1/1NPL ?
NPR CC NPR 1/2NPL ?
NN 1/2NPR ?
NNP 2/2If however, V P is swapped to the right bucketwith NP , the rules become:S ?
NPL V P 1/1V P ?
V BD NPR 1/1NPL ?
NPR CC NPR 1/1NPR ?
NN 1/3NPR ?
NNP 2/3The likelihood of the tree in Figure 1 is 1/4 underthe first grammar, but only 4/27 under the second.Hence in this case we would reject the swap of V Pfrom the right to the left buckets.The process of swapping continues until no im-provement can be made by swapping a singlevalue.The likelihood of the training data according tothe clustered grammar is?r?Rp(r)C(r)for R the set of observed productions r = ?i ?
?j .
.
.
in the clustered grammar.
Notice that whenwe are looking to split a cluster ?, only produc-tions that contain the nonterminal ?
will haveprobabilities that change.
To evaluate whether achange increases the likelihood, we consider theratio between the likelihood of the new model, andthe likelihood of the old model.Furthermore, when we move a value from onebucket to another, only a fraction of the rules willhave their counts change.
Suppose we are mov-ing value x from the left bucket to the right whensplitting ?i.
Let ?x ?
?i be the set of base nonter-minals in ?i that have value x for the feature beingsplit upon.
Only clustered rules that contain basegrammar rules which use nonterminals in ?x willhave their probability change.
These observationsallow us to process only a relatively small numberof base grammar rules.Once we have generated a potential nomineesplit for each feature, we select the partitioningwhich leads to the greatest improvement in theBIC as the candidate split of this cluster.
This can-didate is placed on the priority queue.One odd thing about the above is that in the lo-cal search phase of the clustering we use likeli-hood, while in the candidate selection phase weuse BIC.
We tried both measures in each phase,but found that this hybrid measure outperformedusing only one or the other.3.1 Model SelectionUnfortunately, the grammar that results at the endof the clustering process seems to overfit the train-ing data.
We resolve this by simply noting period-ically the intermediate state of the grammar, andusing this grammar to parse a small tuning set (weuse the first 400 sentences of WSJ section 24, andparse this every 50 times we enact a split).
At theconclusion of clustering, we select the grammar304AB C <D> E F(a)AB [C,<D>,E,F]C [<D>,E,F][<D>,E]D EF(b)Figure 2: (a) A production.
(b) The production,binarized.with the highest f-score on this tuning set as thefinal model.3.2 BinarizationSince our experiments make use of a CKY(Kasami, 1965) parser 1 we must modify the tree-bank derived rules so that each expands to at mosttwo labels.
We perform this in a manner simi-lar to Klein and Manning (2003) and Matsuzakiet al (2005) through the creation of intermediatenodes, as in Figure 2.
In this example, the nonter-minal heir of A?s head is D, indicated in the figureby marking D with angled brackets.
The squarebrackets indicate an intermediate node, and the la-bels inside the brackets indicate that the node willeventually be expanded into those labels.Klein and Manning (2003) employ Collins?
(1999) horizontal markovization to desparsifytheir intermediate nodes.
This means that givenan intermediate node such as [C ?D?EF ] in Fig-ure 2, we forget those labels which will not be ex-panded past a certain horizon.
Klein and Manning(2003) use a horizon of two (or less, in some cases)which means only the next two labels to be ex-panded are retained.
For instance in in this exam-ple [C ?D?EF ] is markovized to [C ?D?
.
.
.
F ],since C and F are the next two non-intermediatelabels.Our mechanism lays out the unmarkovized in-termediate rules in the same way, but we mostlyuse our clustering scheme to reduce sparsity.
Wedo so by aligning the labels contained in the in-termediate nodes in the order in which they wouldbe added when increasing the markovization hori-1The implementation we use was created by Mark John-son and used for the research in (Johnson, 1998).
It is avail-able at his homepage.zon from zero to three.
We also always keepthe heir label as a feature, following Klein andManning (2003).
So for instance, [C ?D?EF ]is represented as having Treebank label ?IN-TERMEDIATE?, and would have feature vector(D,C,F,E,D),while [?D?EF ] would have fea-ture vector (D,F,E,D,?
), where the first itemis the heir of the parent?s head.
The ?-?
in-dicates that the fourth item to be expanded ishere non-existent.
The clusterer would considereach of these five features as for a single pos-sible split.
We also incorporate our other fea-tures into the intermediate nodes in two ways.Some features, such as the parent or grandpar-ent, will be the same for all the labels in the in-termediate node, and hence only need to be in-cluded once.
Others, such as the part of speechof the head, may be different for each label.
Thesefeatures we align with those of corresponding la-bel in the Markov ordering.
In our running ex-ample, suppose each child node N has part ofspeech of its head PN , and we have a parent fea-ture.
Our aligned intermediate feature vectors thenbecome (A,D,C, PC , F, PF , E, PE ,D, PD) and(A,D,F, PF , E, PE ,D, PD,?,?).
As these aresomewhat complicated, let us explain them by un-packing the first, the vector for [C ?D?EF ].
Con-sulting Figure 2 we see that its parent is A. Wehave chosen to put parents first in the vector, thusexplaining (A, ...).
Next comes the heir of theconstituent, D. This is followed by the first con-stituent that is to be unpacked from the binarizedversion, C , which in turn is followed by its headpart-of-speech PC , giving us (A,D,C, PC , ...).We follow with the next non-terminal to be un-packed from the binarized node and its head part-of-speech, etc.It might be fairly objected that this formulationof binarization loses the information of whether alabel is to the left, right, or is the heir of the par-ent?s head.
This is solved by the inside positionfeature, described in Section 2.1 which containsexactly this information.3.3 SmoothingIn order to ease comparison between our workand that of Klein and Manning (2003), we followtheir lead in smoothing no production probabilitiessave those going from preterminal to nonterminal.Our smoothing mechanism runs roughly along thelines of theirs.305LP LR F1 CB 0CBKlein & Manning 86.3 85.1 85.7 1.31 57.2Matsuzaki et al 86.1 86.0 86.1 1.39 58.3This paper 84.8 84.8 84.8 1.47 57.1Table 1: Parsing results on final test set (Section23).Run LP LR F1 CB 0CB1 85.3 85.6 85.5 1.29 59.52 85.8 85.9 85.9 1.29 59.43 85.1 85.5 85.3 1.36 58.04 85.3 85.7 85.5 1.30 59.9Table 2: Parsing results for grammars generatedusing clusterer with different random seeds.
Allnumbers here are on the development test set (Sec-tion 22).Preterminal rules are smoothed as follows.
Weconsider several classes of unknown words, basedon capitalization, the presence of digits or hy-phens, and the suffix.
We estimate the probabil-ity of a tag T given a word (or unknown class)W , as p(T | W ) = C(T,W )+hp(T |unk)C(W )+h , wherep(T | unk) = C(T, unk)/C(unk) is the prob-ability of the tag given any unknown word class.In order to estimate counts of unknown classes,welet the clusterer see every tree twice: once un-modified, and once with the unknown class re-placing each word seen less than five times.
Theproduction probability p(W | T ) is then p(T |W )p(W )/p(T ) where p(W ) and p(T ) are the re-spective empirical distributions.The clusterer does not use smoothed probabil-ities in allocating annotated preterminals to clus-ters, but simply the maximum likelihood estimatesas it does elsewhere.
Smoothing is only used in theparser.4 ExperimentsWe trained our model on sections 2-21 of the PennWall Street Journal Treebank.
We used the first400 sentences of section 24 for model selection.Section 22 was used for testing during develop-ment, while section 23 was used for the final eval-uation.5 DiscussionOur results are shown in Table 1.
The first threecolumns show the labeled precision, recall and f-measure, respectively.
The remaining two showthe number of crossing brackets per sentence,and the percentage of sentences with no crossingbrackets.Unfortunately, our model does not performquite as well as those of Klein and Manning (2003)or Matsuzaki et al (2005).
It is worth noting thatMatsuzaki?s grammar uses a different parse evalu-ation scheme than Klein & Manning or we do.We select the parse with the highest probabilityaccording to the annotated grammar.
Matsuzaki,on the other hand, argues that the proper thing todo is to find the most likely unannotated parse.The probability of this parse is the sum over theprobabilities of all annotated parses that reduceto that unannotated parse.
Since calculating theparse that maximizes this quantity is NP hard, theytry several approximations.
One is what Klein &Manning and we do.
However, they have a betterperforming approximation which is used in theirreported score.
They do not report their scoreon section 23 using the most-probable-annotated-parse method.
They do however compare the per-formance of different methods using developmentdata, and find that their better approximation givesan absolute improvement in f-measure in the .5-1percent range.
Hence it is probable that even withtheir better method our grammar would not out-perform theirs.Table 2 shows the results on the developmenttest set (Section 22) for four different initial ran-dom seeds.
Recall that when splitting a cluster, theinitial partition of the base grammar nonterminalsis made randomly.
The model from the second runwas used for parsing the final test set (Section 23)in Table 1.One interesting thing our method allows is forus to examine which features turn out to be usefulin which contexts.
We noted for each trereebanknonterminal, and for each feature, how many timesthat nonterminal was split on that feature, for thegrammar selected in the model selection stage.
Weran the clustering with these four different randomseeds.We find that in particular, the clusterer onlyfound the head feature to be useful in very spe-cific circumstances.
It was used quite a bit tosplit preterminals; but for phrasals it was onlyused to split ADJP ,ADV P ,NP ,PP ,V P ,QP ,and SBAR.
The part of speech of the head wasonly used to split NP and V P .Furthermore, the grandparent tag appears to beof importance primarily for V P and PP nonter-306minals, though it is used once out of the four runsfor NP s.This indicates that perhaps lexical parsers mightbe able to make do by only using lexical head andgrandparent information in very specific instances,thereby shrinking the sizes of their models, andspeeding parsing.
This warrants further investiga-tion.6 ConclusionWe have presented a scheme for automaticallydiscovering phrasal categories for parsing with astandard CKY parser.
The parser achieves 84.8%precision-recall f-measure on the standard test-section of the Penn WSJ-Treebank (section 23).While this is not as accurate as the hand-tailoredgrammar of Klein and Manning (2003), it is close,and we believe there is room for improvement.For starters, the particular clustering scheme isonly one of many.
Our algorithm splits clus-ters along particular features (e.g., parent, head-part-of-speech, etc.).
One alternative would be tocluster simultaneously on all the features.
It isnot obvious which scheme should be better, andthey could be quite different.
Decisions like thisabound, and are worth exploring.More radically, it is also possible to grow manydecision trees, and thus many alternative gram-mars.
We have been impressed by the success ofrandom-forest methods in language modeling (Xuand Jelinek, 2004).
In these methods many trees(the forest) are grown, each trying to predict thenext word.
The multiple trees together are muchmore powerful than any one individually.
Thesame might be true for grammars.AcknowledgementThe research presented here was funded in part byDARPA GALE contract HR 0011-06-20001.ReferencesAnja Belz.
2002.
Learning grammars for differentparsing tasks by partition search.
In Proceedings ofthe 19th international conference on ComputationalLinguistics, pages 1?7.Eugene Charniak.
1996.
Tree-bank grammars.
InProceedings of the Thirteenth National Conferenceon Artificial Intelligence, pages 1031?1036.
AAAIPress/MIT Press.Eugene Charniak.
2000.
A maximum-entropy-inspired parser.
In Proceedings of NAACL, pages132?139.Michael J. Collins.
1996.
A new statistical parserbased on bigram lexical dependencies.
In The Pro-ceedings of the 34th Annual Meeting of the Associa-tion for Computational Linguistics, pages 184?191.Michael Collins.
1997.
Three generative, lexicalisedmodels for statistical parsing.
In The Proceedingsof the 35th Annual Meeting of the Association forComputational Linguistics.Michael Collins.
1999.
Head-driven Statistical Mod-els for Natural Language Parsing.
Ph.D. thesis, TheUniversity of Pennsylvania.Trevor Hastie, Robert Tibshirani, and Jerome Fried-man.
2003.
The Elements of Statistical Learning.Springer, New York.James Henderson.
2003.
Inducing history representa-tions for broad coverage statistical parsing.
In Pro-ceedings of HLT-NAACL 2003, pages 25?31.Mark Johnson.
1998.
PCFG models of linguis-tic tree representations.
Computational Linguistics,24(4):613?632.Tadao Kasami.
1965.
An efficient recognition and syn-tax algorithm for context-free languages.
TechnicalReport AF-CRL-65-758, Air Force Cambridge Re-search Laboratory.Dan Klein and Christopher Manning.
2003.
Accu-rate unlexicalized parsing.
In Proceedings of the41st Annual Meeting of the Association for Compu-tational Linguistics.David M. Magerman.
1995.
Statistical decision-treemodels for parsing.
In The Proceedings of the 33rdAnnual Meeting of the Association for Computa-tional Linguistics, pages 276?283.Michell P. Marcus, Beatrice Santorini, and Mary AnnMarcinkiewicz.
1993.
Building a large annotatedcorpus of English: The Penn Treebank.
Computa-tional Linguistics, 19(2):313?330.Sven Martin, Jo?rg Liermann, and Hermann Ney.
1995.Algorithms for bigram and trigram word cluster-ing.
In Proceedings of the European Conferenceon Speech, Communication and Technology, pages1253?1256.Takuya Matsuzaki, Yusuke Miyao, and Jun?ichi Tsujii.2005.
Probabilistic CFG with latent annotations.
InProceedings of the 2005 Meeting of the Associationfor Computational Linguistics.Mehryar Mohri and Brian Roark.
2006.
Effective self-training for parsing.
In Proceedings of HLT-NAACL2006.Peng Xu and Fred Jelinek.
2004.
Random forestsin language modeling.
In Proceedings of EMNLP2004.307
