Pseudo-Projectivity: A Polynomially Parsable Non-ProjectiveDependency GrammarSy lva in  Kahane*  and A lex is  Nasr  t and Owen Rambowt?
TALANA Universit@ Paris 7 (sk0ccr .
juss ieu .
f r )t LIA Universit@ d'Avignon (a lex is .
nasr?l ia ,  un iv -av ignon,  f r ):~CoGenTex, Inc. (owenOcogentex.com)1 In t roduct ionDependency grammar has a long traditionin syntactic theory, dating back to at leastTesni~re's work from the thirties3 Recently, ithas gained renewed attention as empirical meth-ods in parsing are discovering the importanceof relations between words (see, e.g., (Collins,1997)), which is what dependency grammarsmodel explicitly do, but context-free phrase-structure grammars do not.
One problem thathas posed an impediment to more wide-spreadacceptance of dependency grammars i the factthat there is no computationally tractable ver-sion of dependency grammar which is not re-stricted to projective analyses.
However, it iswell known that there are some syntactic phe-nomena (such as wh-movement in English orclitic climbing in Romance) that require non-projective analyses.
In this paper, we presenta form of projectivity which we call pseudo-projectivity, and we present a generative string-rewriting formalism that can generate pseudo-projective analyses and which is polynomiallyparsable.The paper is structured as follows.
In Sec-tion 2, we introduce our notion of pseudo-projectivity.
We briefly review a previously pro-posed formalization of projective dependencygrammars in Section 3.
In Section 4, we extendthis formalism to handle pseudo-projectivity.We informally present a parser in Section 5.2 L inear  and  Syntact i c  Order  of  aSentence2.1 Some Notat ion  and Termino logyWe will use the following terminology and no-tation in this paper.
The hierarchical  o rdertThe work presented in this paper is collective andthe order of authors is alphabetical.
(dominance) between the nodes of a t ree T willbe represented with the symbol _~T and T .Whenever they are unambiguous, the notations-< and _ will be used.
When x -~ y, we will saythat x is a descendent  ofy and y an ancestorof x.
The pro ject ion  of a node x, belonging toa tree T, is the set of the nodes y of T such thaty _T X.
An arc between two nodes y and x of atree T, directed from y to x will be noted either(y, x) or ~-.
The node x will be referred to asthe dependent  and y as the governor.
Thelatter will be noted, when convenient, x +T (x +when unambiguous).
The notations ~2- and x +are unambiguous because a node x has at mostone governor in a tree.
As usual, an orderedt ree is a tree enriched with a linear order overthe set of its nodes.
Finally, if l is an arc ofan ordered tree T, then Supp(1) represents hesuppor t  of l, i.e.
the set of the nodes of Tsituated between the extremities of l, extremi-ties included.
We will say that the elements ofSupp(1) are covered by I.2.2 P ro jec t iv i tyThe notion of projectivity was introduced by(Lecerf, 1960) and has received several differentdefinitions ince then.
The definition given hereis borrowed from (Marcus, 1965) and (Robin-son, 1970):Definit ion: An arc ~- is pro ject ive if andonly if for every y covered by ~2-, y ~ x +.
Atree T is pro ject ive  if and only if every arc ofT is projectiveA projective tree has been represented in Fig-ure 1.A projective dependency tree can be associ-ated with a phrase structure tree whose con-stituents are the projections of the nodes ofthe dependency tree.
Projectivity is thereforeequivalent, in phrase structure markers, to con-646The big cat sometimes eats white miceFigure 1: A projective sub-categorization treetinuity of constituent.The strong constraints introduced by the pro-jectivity property on the relationship betweenhierarchical order and linear order allow us todescribe word order of a projective dependencytree at a local level: in order to describe thelinear position of a node, it is sufficient o de-scribe its position towards its governor and sis-ter nodes.
The domain of locality of the linearorder rules is therefore limited to a subtree ofdepth equal to one.
It can be noted that this do-main of locality is equal to the domain of local-ity of sub-categorization rules.
Both rules cantherefore be represented together as in (Gaif-man, 1965) or separately as will be proposedin 3.2.3 Pseudo-Pro jec t iv i tyAlthough most linguistic structures can berepresented as projective trees, it is well knownthat projectivity is too strong a constraint fordependency trees, as shown by the example ofFigure 2, which includes a non-projective arc(marked with a star).Who do you think she invited ?Figure 2: A non projective sub-categorizationtreeThe non projective structures found inlinguistics represent a small subset of thepotential non projective structures.
We willdefine a property (more exactly a family ofproperties), weaker than projectivity, calledpseudo-pro jec t iv i ty ,  which describes asubset of the set of ordered dependency trees,containing the non-projective linguistic struc-tures.In order to define pseudo-projectivity, we in-troduce an operation on dependency trees calledlifting.
When applied to a tree, this operationleads to the creation of a second tree, a lift ofthe first one.
An ordered tree T' is a lift ofthe ordered tree T if and only if T and T' havethe same nodes in the same order and for ev-ery node x, x +T ..<T x+T'.
We will say that thenode x has been lifted from x +T (its syntact icgovernor)  to x +T' (its l inear governor) .Recall that the linear position of a node ina projective tree can be defined relative to itsgovernor and its sisters.
In order to define thelinear order in a non projective tree, we willuse a projective lift of the tree.
In this case,the position of a node can be defined only withregards to its governor and sisters in the lift,i.e., its linear governor and sisters.Def in i t ion:  An ordered tree T is saidpseudo-pro jec t ive  if there exists a lift T' oftree T which is projective.If there is no restriction on the lifting, theprevious definition is not very interesting sincewe can in fact take any non-projective tree andlift all nodes to the root node and obtain a pro-jective tree.We will therefore constrain the lifting by aset of rules, called lifting rules.
Consider a setof (syntactic) categories.
The following defini-tions make sense only for trees whose nodes arelabeled with categories.
2The lifting rules are of the following form(LD, SG and LG are categories and w is a reg-ular expression on the set of categories):LD $ SG w LG (1)This rule says that a node of category LDcan be lifted from its syntactic governor of cat-egory SG to its linear governor of category LGthrough a path consisting of nodes of categoryC1, .
.
.
,  Ca, where the string C1.
.
.
Cn belongsto L(w).
Every set of lifting rules defines a par-ticular property of pseudo-projectivity b im-posing particular constraints on the lifting.
Asit is possible to define pseudo-projectivity purelystructurally (i.e.
without referring to the labeling).
Forexample, we can impose that each node x is lifted tothe highest ancestor of x covered by ~2" ((Nasr, 1996)).The resulting pseudo-projectivity s a fairly weak exten-sion to projectivity, which nevertheless covers major non-projective linguistic structures.
However, we do not pur-sue a purely structural definition of pseudo-projectivityin this paper.647linguistic example of lifting rule is given in Sec-tion 4.The idea of building a projective tree bymeans of lifting appears in (Kunze, 1968) andis used by (Hudson, 1990) and (Hudson, un-published).
This idea can also be compared tothe notion of word order domain (Reape, 1990;BrSker and Neuhaus, 1997), to the Slash featureof GPSG and HPSG, to the functional uncer-tainty of LFG, and to the Move-a of GB theory.3 P ro jec t ive  Dependency  GrammarsRev is i tedWe (informally) define a projective DependencyGrammar as a string-rewriting system 3 by giv-ing a set of categories uch as N, V and Adv, 4a set of distinguished start categories (the rootcategories of well-formed trees), a mapping fromstrings to categories, and two types of rules: de-pendency  ru les  which state hierarchical order(dominance) and LP  ru les  which state linearorder.
The dependency rules are further sub-divided into subcategorization rules (or s-rules)and modification rules (or m-rules).
Here aresome sample s-rules:dl : Vtrans ) gnom, Nobj, (2)d2 : Yclause ~ gnom, YHere is a sample m-rule.
(3)d3 : V ~ Adv (4)LP rules are represented as regular expressions(actually, only a limited form of regular expres-sions) associated with each category.
We usethe hash sign (#) to denote the position of thegovernor (head).
For example:pl:Yt .
.
.
.
= (Adv)Nnom(Aux)Adv*#YobjAdv*Yt .
.
.
.
(5)3We follow (Gaifman, 1965) throughout this paper bymodeling adependency grammar with a string-rewritingsystem.
However, we will identify a derivation with itsrepresentation as a tree, and we will sometimes referto symbols introduced in a rewrite step as "dependentnodes".
For a model of a DG based on tree-rewriting(in the spirit of Tree Adjoining Grammar (Joshi et al,1975)), see (Nasr, 1995).4In this paper, we will allow finite feature structureson categories, which we will notate using subscripts; e.g.,Vtrans.
Since the feature structures are finite, this is sim-ply a notational variant of a system defined only withsimple category labels.~clauseAdv Nnom thought Vtransyesterday Fernando thought Vtrans==~ yesterday Fernando thought Nnom eats Nob jA dvyesterday Fernando thought Carlos eats beans slowlyVclauseAdv Nnom thought Vtransyesterday FernandoNnom eats Nobj  AdvI f JCarlos beans slowlyFigure 3: A sample GDG derivationWe will call this system generat ive  depen-dency  grammar  or GDG for short.Derivations in GDG are defined as follows.In a rewrite step, we choose a multiset of de-pendency rules (i.e., a set of instances of de-pendency rules) which contains exactly one s-rule and zero or more m-rules.
The left-handside nonterminal is the same as that we want torewrite.
Call this multiset he rewrite-multiset.In the rewriting operation, we introduce a mul-tiset of new nonterminals and exactly one termi-nal symbol (the head).
The rewriting operationthen must meet the following three conditions:?
There is a bijection between the set of de-pendents of the instances of rules in therewrite-multiset and the set of newly intro-duced dependents.?
The order of the newly introduced epen-dents is consistent with the LP rule associ-ated with the governor.?
The introduced terminal string (head) ismapped to the rewritten category.As an example, consider a grammar contain-ing the three dependency rules dl (rule 2), d2(rule 3), and d3 (rule 4), as well as the LP rule Pl(rule 5).
In addition, we have some lexical map-pings (they are obvious from the example), andthe start symbol is Yfinite: +.
A sample deriva-tion is shown in Figure 3, with the sententialform representation top and the correspond-ing tree representation below.Using this kind of representation, we canderive a bottom-up parser in the following648straightforward manner.
5 Since syntactic andlinear governors coincide, we can derive de-terministic finite-state machines which captureboth the dependency and the LP rules for agiven governor category.
We will refer to theseFSMs as ru le -FSMs,  and if the governor is ofcategory C, we will refer to a C-rule-FSM.
Ina rule-FSM, the transitions are labeled by cate-gories, and the transition corresponding to thegovernor labeled by its category and a specialmark (such as #).
This transition is called the"head transition".The entries in the parse matrix M are of theform (m, q), where rn is a rule-FSM and q a stateof it, except for the entries in squares M(i, i),1 <: i < n, which also contain category labels.Let wo'"wn be the input word.
We initializethe parse matrix as follows.
Let C be a categoryof word wi.
First, we add C to M(i, i) .
Then,we add to M(i, i) every pair (m, q) such that mis a rule-FSM with a transition labeled C froma start state and q the state reached after thattransition.
6Embedded in the usual three loops on i, j ,  k,we add an entry (ml,q) to M(i , j )  if (rnl,ql) isin M(k, j ) ,  (m2, q2) is in M(i, k-t-l), q2 is a finalstate of m2, m2 is a C-rule-FSM, and ml transi-tions from ql to q on C (a non-head transition).There is a special case for the head transitionsin ml: i f k  = i - 1, C is in M(i, i) ,  ml is a C-rule-FSM, and there is a head transition fromql to q in ml,  then we add (ml, q) to M(i, j).The time complexity of the algorithm isO(n3\[GIQmax), where G is the number of rule-FSMs derived from the dependency and LPrules in the grammar and Qmax is the maximumnumber of states in any of the rule-FSMs.4 A Formal izat ion ofPP-Dependency  GrammarsRecall that in a pseudo-projective tr e, we makea distinction between a syntactic governor anda linear governor.
A node can be "lifted" alonga lifting path from being a dependent of its syn-tactic governor to being a dependent of its linear5This type of parser has been proposed previously.See for example (Lombardi, 1996; Eisner, 1996), whoalso discuss Early-style parsers for projective depen-dency grammars.6We can use pre-computed top-down prediction tolimit the number of pairs added.649governor, which must be an ancestor of the gov-ernor.
In defining a formal rewriting system forpseudo-projective trees, we will not attempt omodel the "lifting" as a transformational step inthe derivation.
Rather, we will directly derivethe "lifted" version of the tree, where a nodeis dependent of its linear governor.
Thus, thederived structure resembles more a unistrataldependency representation like those used by(Hudson, 1990) than the multistratal represen-tations of, for example, (Mel'~uk, 1988).
How-ever, from a formal point of view, the distinctionis not significant.In order to capture pseudo-projectivity, wewill interpret rules of the form (2) (for subcate-gorization of arguments by a head) and (4) (forselection of a head by an adjunct) as introducingsyntactic dependents which may lift to a higherlinear governor.
An LP rule of the form (5) or-ders all linear dependents of the linear governor,no matter whose syntactic dependents hey are.In addition, we need a third type of rule,namely a lifting rule, or l-rule (see 2.3).
The1-rule (1) can be rewrited on the following form:ll : LG > LD {LG.w SG LD} (6)This rule resembles normal dependency rulesbut instead of introducing syntactic dependentsof a category, it introduces a lifted dependent.Besides introducing a linear dependent LD, a1-rule should make sure that the syntactic gov-ernor of LD will be introduced at a later stage ofthe derivation, and prevent it to introduce LDas its syntactic dependent, otherwise non pro-jective nodes would be introduced twice, a firsttime by their linear governor and a second timeby their syntactic governor.
This condition isrepresented in the rule by means of a constrainton the categories found along the lifting path.This condition, which we call the lifting con-dition, is represented by the regular expressionLG.
w SG.
The regular expression representingthe lifting condition is enriched with a dot sep-arating, on its left, the part of the lifting pathwhich has already been introduced uring therewriting and on its right the part which is stillto be introduced for the rewriting to be valid.The dot is an unperfect way of representing thecurrent state in a finite state automaton equiv-alent to the regular expression.
We can furthernotice that the lifting condition ends with a rep-etition of LD for reasons which will be madeclear when discussing the rewriting process.A sentential form contains terminal stringsand categories paired with a multiset of liftingconditions, called the lift multiset.
The lift mul-tiset associated to a category C contains 'tran-siting' lifting conditions: introduced by ances-tors of C and passing across C.Three cases must be distinguished whenrewriting a category C and its lifting multisetLM:?
LM contains a single lifting condi-tion which dot is situated to its right:LGw SG C .
.
In such acase, Cmust  berewritten by the empty string.
The situ-ation of the dot at the right of the liftingcondition indicates that C has been intro-duced by its syntactic governor although ithas already been introduced by its lineargovernor earlier in the rewriting process.This is the reason why C has been addedat the end of the lifting condition.?
LM contains everal ifting conditions oneof which has its dot to the right.
In sucha case, the rewriting fails since, in accor-dance with the preceding case, C must berewritten by the empty string.
Therefore,the other lifting conditions of LM will notbe satisfied.
Furthermore, a single instanceof a category cannot anchor more than onelifting condition.?
LM contains everal lifting conditions noneof which having the dot to their right.
Inthis case, a rewrite multiset of dependencyrules and lifting rules, both having C astheir left hand side, is selected.
The resultof the rewriting then must meet the follow-ing conditions:1.
The order of the newly introduced e-pendents i consistent with the LP ruleassociated with C.2.
The union 7 of the lift multisets asso-ciated with all the newly introduced(instances of) categories i equal to theunion of the lift multiset of C and themultiset composed of the lift condition7When discussing set operations on multisets, we ofcourse mean the corresponding multiset operations.of the 1-rules used in the rewriting op-eration.3.
The lifting conditions contained in thelift multiset of all the newly introduceddependents D should be compatiblewith D, with the dot advanced appro-priately.In addition, we require that, when we rewritea category as a terminal, the lift multiset isempty.Let us consider an example.
Suppose we havehave a grammar containing the dependencyrules dl (rule 2), d2 (rule 3), and d3 (rule 4);the LP rule Pl (rule 5) and p2:p2:Vclause : (Ntop: + INwh:+)(Adv)Nnom(Aux)Adv* #Adv* Vt .
.
.
.Furthermore, we have the following 1-rule:II :Vbridge:+---~Nc .
.
.
.
.
bj top:+ {'V~ridge:+VNc .
.
.
.
.
bj top:+ }This rule says that an objective wh-noun withfeature top:+ which depends on a verb with nofurther restrictions (the third V in the liftingpath) can raise to any verb that dominates itsimmediate governor as long as the raising pathscontains only verb with feature bridge:+, i.e.,bridge verbs.VclauseNobj Nnom thought Adv Y{'Y~ridge: + Y Ncase:obj top:+}beans Fernando thought yesterdayV{.V~ridge: + V Nc .
.
.
.
bj top:+}beans Fernando thought yesterday Nnom claimsV{.V~ridge: + V Nc .
.
.
.
bj top:+}=~ beans Fernando thought yesterday Milagro claimsV{-V~ridge: + Y Nc ..... bj top:+}beans yesterday Fernando thought yesterday Milagroclaims Nnom eats N { Y~ridge:+ V Ycase:obj top:+'} Adv:=~ beans Fernando thought yesterday Milagro claims Carloseats slowlyVcl~us?N ~ a u * ebeans Fernando yesterNno m claims VtransMilagroNnom eats AdvI I Carlos slowlyFigure 4: A sample PP-GDG derivationA sample derivation is shown in Figure 4,with the sentential form representation  top650and the corresponding tree representation be-low.
We start our derivation with the startsymbol Vclause and rewrite it using dependencyrules d2 and d3, and the lifting rule ll whichintroduces an objective NP argument.
The lift-ing condition of I1 is passed to the V dependentbut the dot remains at the left of V'bridge:.
{.
be-cause of the Kleene star.
When we rewrite theembedded V, we choose to rewrite again withYclause , and the lifting condition is passed on tothe next verb.
This verb is a Ytrans which re-quires a Yobj.
The lifting condition is passed toNob j and the dot is moved to the right of theregular expression, therefore Nob j is rewrittenas the empty string.5 A Po lynomia l  Parser  for  PP -GDGIn this section, we show that pseudo-projectivedependency grammars as defined in Section 2.3are polynomially parsable.We can extend the bottom-up arser for GDGto a parser for PP-GDG in the following man-ner.
In PP-GDG, syntactic and linear governorsdo not necessarily coincide, and we must keeptrack separately of linear precedence and of lift-ing (i.e., "long distance" syntactic dependence).The entries in the parse matrix M are ofthe form (m,q, LM), where m is a rule-FSM,q a state of m, and LM is a multiset of lift-ing conditions as defined in Section 4.
An entry(m, q, LM) in a square M(i, j) of the parse ma-trix means that the sub-word wi...wj of theentry can be analyzed by m up to state q (i.e.,it matches the beginning of an LP rule), butthat nodes corresponding to the lifting rules inLM are being lifted from the subtrees span-ning wi...wj.
Put differently, in this bottom-up view LM represents the set of nodes whichhave a syntactic governor in the subtree span-ning wi...wj and a lifting rule, but are stilllooking for a linear governor.Suppose we have an entry in the parse matrixM of the form (m, q, L).
As we traverse the C-rule-FSM m, we recognize one by one the lineardependents of a node of category C. Call thisgovernor ~?.
The action of adding a new entry tothe parse matrix corresponds to adding a singlenew linear dependent to 77.
(While we are work-ing on the C-rule-FSM m and are not yet in afinal state, we have not yet recognized ~?
itself.
)Each new dependent ~?'
brings with it a multiset651of nodes being lifted from the subtree it is theroot of.
Call this multiset LM'.
The new entrywill be (m, q', LM U LM') (where q' is the state!
,that m transitions to when ~?
is recognized asthe next linear dependent.When we have reached a final state q of therule-FSM m, we have recognized a completesubtree rooted in the new governor, ~?.
Someof the dependent nodes of ~?
will be both syn-tactic and linear dependents of ~?, and the otherswill be linear dependents of ~?, but lifted from adescendent of 7.
In addition, 77 may have syn-tactic dependents which are not realized as itsown linear dependent and are lifted away.
(Noother options are possible.)
Therefore, when wehave reached the final state of a rule-FSM, wemust connect up all nodes and lifting conditionsbefore we can proceed to put an entry (m, q, L)in the parse matrix.
This involves these steps:1.
For every lifting condition in LM, we en-sure that it is compatible with the categoryof ~?.
This is done by moving the dot left-wards in accordance with the category of77.
(The dot is moved leftwards since weare doing bottom-up recognition.
)The obvious special provisions deal withthe Kleene star and optional elements.If the category matches a catgeory withKleene start in the lifting condition, we donot move the dot.
If the category matchesa category which is to the left of an op-tional category, or to the left of categorywith Kleene star, then we can move the dotto the left of that category.If the dot cannot be placed in accordancewith the category of 77, then no new entryis made in the parse matrix for ~?.2.
We then choose a multiset of s-, m-, and 1-rules whose left-hand side is the category of~?.
For every dependent of 77 introduced byan 1-rule, the dependent must be compati-ble with an instance of a lifting condition inLM (whose dot must be at its beginning, orseperated from the beginning by optionalor categories only); the lifting condition isthen removed from L.3.
If, after the above repositioning of the dotand the linking up of all linear dependentsto lifting conditions, there are still lifting.conditions in LM such that the dot is atthe beginning of the lifting condition, thenno new entry is made in the parse matrixfor ~?.For every syntactic dependent of ?, we de-termine if it is a linear dependent of~ whichhas not yet been identified as lifted.
Foreach syntactic dependents which is not alsoa linear dependent, we check whether thereis an applicable lifting rule.
If not, no entryis made in the parse matrix for 77.
If yes,we add the lifting rule to LM.This procedure determines a new multisetLM so we can add entry (m, q, LM) in the parsematrix.
(In fact, it may determine several pos-sible new multisets, resulting in multiple newentries.)
The parse is complete if there is anentry (m, qrn, O) in square M(n, 1) of the parsematrix, where m is a C-rule-FSM for a startcategory and qm is a final state of m. If we keepbackpointers at each step in the algorithm, wehave a compact representation f the parse for-est.The maximum number of entries in eachsquare of the parse matrix is O(GQnL), whereG is the number of rule-FSMs corresponding toLP rules in the grammar, Q is the maximumnumber of states in any of the rule-FSMs, andL is the maximum number of states that thelifting rules can be in (i.e., the number of lift-ing conditions in the grammar multiplied by themaximum number of dot positions of any liftingcondition).
Note that the exponent is a gram-mar constant, but this number can be rathersmall since the lifting rules are not lexicalized- they are construction-specific, not lexeme-specific.
The time complexity of the algorithmis therefore O(GQn3+21L\[).ReferencesNorbert BrSker and Peter Neuhaus.
1997.
Thecomplexity of recognition of linguistically ad-equate dependency grammars.
In 35th Meet-ing of the Association for Computational Lin-guistics (ACL'97), Madrid, Spain.
ACL.M.
Collins.
1997.
Three generative, lexicalisedmodels for statistical parsing.
In Proceedingsof the 35th Annual Meeting of the Associa-tion for Computational Linguistics, Madrid,Spain, July.652Jason M. Eisner.
1996.
Three new probabilis-tic models for dependency parsing: An ex-ploration.
In Proceedings of the 16th Inter-national Conference on Computational Lin-guistics (COLING'96), Copenhagen.Haim Galfman.
1965.
Dependency systems andphrase-structure systems.
Information andControl, 8:304-337.Richard Hudson.
1990.
English Word Gram-mar.
Basil Blackwell, Oxford, RU.Richard Hudson.
unpublished.
Discontinuity.e-preprint (ftp.phon.ucl.ac.uk).Aravind K. Joshi, Leon Levy, and M Takahashi.1975.
Tree adjunct grammars.
J. Comput.Syst.
Sci., 10:136-163.Jiirgen Kunze.
1968.
The treatment of non-projective structures in the syntactic analysisand synthesis of english and german.
Com-putational Linguistics, 7:67-77.Yves Lecerf.
1960.
Programme des conflits,module des conflits.
Bulletin bimestriel deI'ATALA, 4,5.Vicenzo Lombardi.
1996.
An Earley-styleparser for dependency grammars.
In Pro-ceedings of the 16th International Conferenceon Computational Linguistics (COLING'96),Copenhagen.Solomon Marcus.
1965.
Sur la notion de projec-tivit6.
Zeitschr.
f. math.
Logik und Grundla-gen d.
Math., 11:181-192.Igor A. Mel'6uk.
1988.
Dependency Syntax:Theory and Practice.
State University of NewYork Press, New York.Alexis Nasr.
1995.
A formalism and a parser forlexicalised ependency grammars.
In 4th In-ternational Workshop on Parsing Technolo-gies, pages 186-195, Prague.Alexis Nasr.
1996.
Un syst~me de reformu-lation automatique de phrases fondd sur laThdorie Sens-Texte : application aux languescontr61des.
Ph.D. thesis, Universit6 Paris 7.Michael Reape.
1990.
Getting things in order.In Proceedings of the Symposium on Discon-tinuous Constituents, Tilburg, Holland.Jane J. Robinson.
1970.
Dependency struc-tures and transformational rules.
Language,46(2):259-285.
