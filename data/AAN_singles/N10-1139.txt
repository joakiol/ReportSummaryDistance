Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 957?965,Los Angeles, California, June 2010. c?2010 Association for Computational LinguisticsExpected Sequence Similarity MaximizationCyril Allauzen1, Shankar Kumar1, Wolfgang Macherey1, Mehryar Mohri2,1 and Michael Riley11Google Research, 76 Ninth Avenue, New York, NY 100112Courant Institute of Mathematical Sciences, 251 Mercer Street, New York, NY 10012AbstractThis paper presents efficient algorithms forexpected similarity maximization, which co-incides with minimum Bayes decoding for asimilarity-based loss function.
Our algorithmsare designed for similarity functions that aresequence kernels in a general class of posi-tive definite symmetric kernels.
We discussboth a general algorithm and a more efficientalgorithm applicable in a common unambigu-ous scenario.
We also describe the applica-tion of our algorithms to machine translationand report the results of experiments with sev-eral translation data sets which demonstrate asubstantial speed-up.
In particular, our resultsshow a speed-up by two orders of magnitudewith respect to the original method of Trombleet al (2008) and by a factor of 3 or moreeven with respect to an approximate algorithmspecifically designed for that task.
These re-sults open the path for the exploration of moreappropriate or optimal kernels for the specifictasks considered.1 IntroductionThe output of many complex natural language pro-cessing systems such as information extraction,speech recognition, or machine translation systemsis a probabilistic automaton.
Exploiting the full in-formation provided by this probabilistic automatoncan lead to more accurate results than just using theone-best sequence.Different techniques have been explored in thepast to take advantage of the full lattice, some basedon the use of a more complex model applied tothe automaton as in rescoring, others using addi-tional data or information for reranking the hypothe-ses represented by the automaton.
One method forusing these probabilistic automata that has been suc-cessful in large-vocabulary speech recognition (Goeland Byrne, 2000) and machine translation (Kumarand Byrne, 2004; Tromble et al, 2008) applicationsand that requires no additional data or other com-plex models is the minimum Bayes risk (MBR) de-coding technique.
This returns that sequence of theautomaton having the minimum expected loss withrespect to all sequences accepted by the automaton(Bickel and Doksum, 2001).
Often, minimizing theloss function L can be equivalently viewed as max-imizing a similarity function K between sequences,which corresponds to a kernel function when it ispositive definite symmetric (Berg et al, 1984).
Thetechnique can then be thought of as an expected se-quence similarity maximization.This paper considers this expected similarity max-imization view.
Since different similarity functionscan be used within this framework, one may wish toselect the one that is the most appropriate or relevantto the task considered.
However, a crucial require-ment for this choice to be realistic is to ensure thatfor the family of similarity functions considered theexpected similarity maximization is efficiently com-putable.
Thus, we primarily focus on this algorith-mic problem in this paper, leaving it to future workto study the question of determining how to selectthe similarity function and report on the benefits ofthis choice.A general family of sequence kernels includingthe sequence kernels used in computational biology,text categorization, spoken-dialog classification, andmany other tasks is that of rational kernels (Corteset al, 2004).
We show how the expected similaritymaximization can be efficiently computed for thesekernels.
In section 3, we describe more specificallythe framework of expected similarity maximizationin the case of rational kernels and the correspond-ing algorithmic problem.
In Section 4, we describeboth a general method for the computation of the ex-pected similarity maximization, and a more efficientmethod that can be used with a broad sub-familyof rational kernels that verify a condition of non-ambiguity.
This latter family includes the class ofn-gram kernels which have been previously used to957apply MBR to machine translation (Tromble et al,2008).
We examine in more detail the use and ap-plication of our algorithms to machine translationin Section 5.
Section 6 reports the results of ex-periments applying our algorithms in several largedata sets in machine translation.
These experimentsdemonstrate the efficiency of our algorithm whichis shown empirically to be two orders of magnitudefaster than Tromble et al (2008) and more than 3times faster than even an approximation algorithmspecifically designed for this problem (Kumar et al,2009).
We start with some preliminary definitionsand algorithms related to weighted automata andtransducers, following the definitions and terminol-ogy of Cortes et al (2004).2 PreliminariesWeighted transducers are finite-state transducers inwhich each transition carries some weight in addi-tion to the input and output labels.
The weight sethas the structure of a semiring.A semiring (K,?,?, 0, 1) verifies all the axiomsof a ring except from the existence of a negative el-ement ?x for each x ?
K, which it may verify ornot.
Thus, roughly speaking, a semiring is a ringthat may lack negation.
It is specified by a set ofvalues K, two binary operations ?
and ?, and twodesignated values 0 and 1.
When ?
is commutative,the semiring is said to be commutative.The real semiring (R+,+,?, 0, 1) is used whenthe weights represent probabilities.
The logsemiring (R ?
{??,+?
},?log,+,?, 0) is iso-morphic to the real semiring via the negative-log mapping and is often used in practicefor numerical stability.
The tropical semiring(R?, {??,+?
},min,+,?, 0) is derived fromthe log semiring via the Viterbi approximation andis often used in shortest-path applications.Figure 1(a) shows an example of a weightedfinite-state transducer over the real semiring(R+,+,?, 0, 1).
In this figure, the input and out-put labels of a transition are separated by a colondelimiter and the weight is indicated after the slashseparator.
A weighted transducer has a set of initialstates represented in the figure by a bold circle anda set of final states, represented by double circles.
Apath from an initial state to a final state is an accept-ing path.The weight of an accepting path is obtained byfirst ?-multiplying the weights of its constituent0                          a:b/11a:b/22/1a:b/43/8b:a/6b:a/3b:a/50                        b/11b/22/1b/43/8a/6a/3a/5(a) (b)Figure 1: (a) Example of weighted transducer T over thereal semiring (R+,+,?, 0, 1).
(b) Example of weightedautomaton A.
A can be obtained from T by projection onthe output and T (aab, bba) = A(bba) = 1?
2?
6?
8+2?
4?
5?
8.transitions and?-multiplying this product on the leftby the weight of the initial state of the path (whichequals 1 in our work) and on the right by the weightof the final state of the path (displayed after the slashin the figure).
The weight associated by a weightedtransducer T to a pair of strings (x, y) ?
??
???
isdenoted by T (x, y) and is obtained by ?-summingthe weights of all accepting paths with input label xand output label y.For any transducer T , T?1 denotes its inverse,that is the transducer obtained from T by swappingthe input and output labels of each transition.
For allx, y ?
?
?, we have T?1(x, y) = T (y, x).The composition of two weighted transducers T1and T2 with matching input and output alphabets ?,is a weighted transducer denoted by T1 ?
T2 whenthe semiring is commutative and the sum:(T1 ?
T2)(x, y) =?z??
?T1(x, z)?
T2(z, y) (1)is well-defined and in K for all x, y (Salomaa andSoittola, 1978).Weighted automata can be defined as weightedtransducers A with identical input and output labels,for any transition.
Since only pairs of the form (x, x)can have a non-zero weight associated to them byA, we denote the weight associated by A to (x, x)by A(x) and call it the weight associated by A tox.
Similarly, in the graph representation of weightedautomata, the output (or input) label is omitted.
Fig-ure 1(b) shows an example of a weighted automa-ton.
When A and B are weighted automata, A ?
Bis called the intersection of A and B. Omitting theinput labels of a weighted transducer T results in aweighted automaton which is said to be the outputprojection of T .9583 General FrameworkLet X be a probabilistic automaton representing theoutput of a complex model for a specific query input.The model may be for example a speech recognitionsystem, an information extraction system, or a ma-chine translation system (which originally motivatedour study).
For machine translation, the sequencesaccepted by X are the potential translations of theinput sentence, each with some probability given byX .Let ?
be the alphabet for the task considered, e.g.,words of the target language in machine translation,and let L : ??
?
??
?
R denote a loss functiondefined over the sequences on that alphabet.
Givena reference or hypothesis set H ?
?
?, minimumBayes risk (MBR) decoding consists of selecting ahypothesis x ?
H with minimum expected loss withrespect to the probability distribution X (Bickel andDoksum, 2001; Tromble et al, 2008):x?
= argminx?HEx?
?X[L(x, x?)].
(2)Here, we shall consider the case, frequent in prac-tice, where minimizing the loss L is equivalent tomaximizing a similarity measure K : ?????
?
R.When K is a sequence kernel that can be repre-sented by weighted transducers, it is a rational ker-nel (Cortes et al, 2004).
The problem is then equiv-alent to the following expected similarity maximiza-tion:x?
= argmaxx?HEx?
?X[K(x, x?)].
(3)When K is a positive definite symmetric rationalkernel, it can often be rewritten as K(x, y) = (T ?T?1)(x, y), where T is a weighted transducer overthe semiring (R+?{+?
},+,?, 0, 1).
Equation (3)can then be rewritten asx?
= argmaxx?HEx?
?X[(T ?
T?1)(x, x?)]
(4)= argmaxx?H?A(x) ?
T ?
T?1 ?X?, (5)where we denote by A(x) an automaton accepting(only) the string x and by ???
the sum of the weightsof all accepted paths of a transducer.4 Algorithms4.1 General methodEquation (5) could suggest computing A(x) ?
T ?T?1 ?
X for each possible x ?
H .
Instead, wecan compute a composition based on an automa-ton accepting all sequences in H , A(H).
This leadsto a straightforward method for determining the se-quence maximizing the expected similarity havingthe following steps:1. compute the composition X ?
T , project onthe output and optimize (epsilon-remove, de-terminize, minimize (Mohri, 2009)) and let Y2be the result;12. compute the composition Y1 = A(H) ?
T ;3. compute Y1 ?
Y2 and project on the input, let Zbe the result;24. determinize Z;5. find the maximum weight path with the label ofthat path giving x?.While this method can be efficient in various scenar-ios, in some instances the weighted determinizationyielding Z can be both space- and time-consuming,even though the input is acyclic.
The next two sec-tions describe more efficient algorithms.Note that in practice, for numerical stability, allof these computations are done in the log semiringwhich is isomorphic to (R+?{+?
},+,?, 0, 1).
Inparticular, the maximum weight path in the last stepis then obtained by using a standard single-sourceshortest-path algorithm.4.2 Efficient method for n-gram kernelsA common family of rational kernels is the familyof n-gram kernels.
These kernels are widely use asa similarity measure in natural language processingand computational biology applications, see (Leslieet al, 2002; Lodhi et al, 2002) for instance.The n-gram kernel Kn of order n is defined asKn(x, y) =?|z|=ncx(z)cy(z), (6)where cx(z) is the number of occurrences of z inx.
Kn is a positive definite symmetric rational ker-nel since it corresponds to the weighted transducerTn ?
T?1n where the transducer Tn is defined suchthat Tn(x, z) = cx(z) for all x, z ?
??
with |z| = n.1Equivalent to computing T?1 ?
X and projecting on theinput.2Z is then the projection on the input of A(H)?T ?T?1?X .9590a:?b:?1a:ab:b2a:ab:ba:?b:?01a/0.52b/0.53b/14b/15a/16a/17a/0.48b/0.6b/19/1b/1a/1(a) (b)01a2b3b4b5a6a7a8bb9ba01a/12b/1 3/1a/0.2b/1.5a/1.8b/0.5(c) (d)?a/0a/0b/0b/0a/0.2b/1.5a/1.8b/0.501a/02b/03b/1.54b/0.55a/1.86a/1.87a/0.28b/1.5b/0.59/0b/1.5a/1.8(e) (f)Figure 2: Efficient method for bigram kernel: (a) Counting transducer T2 for ?
= {a, b} (over the real semiring).
(b)Probabilistic automaton X (over the real semiring).
(c) The hypothesis automaton A(H) (unweighted).
(d) AutomatonY2 representing the expected bigram counts in X (over the real semiring).
(e) Automaton Y1: the context dependencymodel derived from Y2 (over the tropical semiring).
(f) The composition A(H) ?
Y1 (over the tropical semiring).The transducer T2 for ?
= {a, b} is shown in Fig-ure 2(a).Taking advantage of the special structure of n-gram kernels and of the fact that A(H) is an un-weighted automaton, we can devise a new and sig-nificantly more efficient method for computing x?based on the following steps.1.
Compute the expected n-gram counts in X: Wecompute the composition X ?T , project on out-put and optimize (epsilon-remove, determinize,minimize) and let Y2 be the result.
Observe thatthe weighted automaton Y2 is a compact repre-sentation of the expected n-gram counts in X ,i.e.
for an n-gram w (i.e.
|w| = n):Y2(w) =?x??
?X(x)cx(w)= Ex?X[cx(w)] = cX(w).(7)2.
Construct a context-dependency model: Wecompute the weighted automaton Y1 over thetropical semiring as follow: the set of states isQ = {w ?
?
?| |w| ?
n and w occurs in X},the initial state being ?
and every state being fi-nal; the set of transitions E contains all 4-tuple(origin, label, weight, destination) of the form:?
(w, a, 0, wa) with wa ?
Q and |w| ?
n?2 and?
(aw, b, Y2(awb), wb) with Y2(awb) 6= 0and |w| = n?
2where a, b ?
?
and w ?
??.
Observe thatw ?
Q when wa ?
Q and that aw,wb ?
Qwhen Y2(awb) 6= 0.
Given a string x, we haveY1(x) =?|w|=ncX(w)cx(w).
(8)Observe that Y1 is a deterministic automaton,hence Y1(x) can be computed in O(|x|) time.3.
Compute x?
: We compute the compositionA(H) ?
Y1.
x?
is then the label of the acceptingpath with the largest weight in this transducerand can be obtained by applying a shortest-pathalgorithm to ?A(H) ?
Y1 in the tropical semir-ing.The main computational advantage of this methodis that it avoids the determinization of Z in the9600 1a/1 2/1a/c1b/c20 1a2/c1a3/c2b0b1a2/c1a3/c2bbaba0b/01a/02a/03b/02?/0b/0 a/0?
/c13?/0?
/c2b/0a/0(a) (b) (c) (d)Figure 3: Illustration of the construction of Y1 in the unambiguous case.
(a) Weighted automaton Y2 (over the realsemiring).
(b) Deterministic tree automaton Y ?2 accepting {aa, ab} (over the tropical semiring).
(c) Result of deter-minization of ?
?Y ?2 (over the tropical semiring).
(d) Weighted automaton Y1 (over the tropical semiring).(+,?)
semiring, which can sometimes be costly.The method has also been shown empirically to besignificantly faster than the one described in the pre-vious section.The algorithm is illustrated in Figure 2.
The al-phabet is ?
= {a, b} and the counting transducercorresponding to the bigram kernel is given in Fig-ure 2(a).
The evidence probabilistic automaton Xis given in Figure 2(b) and we use as hypothesisset the set of strings that were assigned a non-zeroprobability by X; this set is represented by the deter-ministic finite automaton A(H) given in Figure 2(c).The result of step 1 of the algorithm is the weightedautomaton Y2 over the real semiring given in Fig-ure 2(d).
The result of step 2 is the weighted au-tomaton Y1 over the tropical semiring is given inFigure 2(e).
Finally, the result of the compositionA(H) ?
Y1 (step 3) is the weighted automaton overthe tropical semiring given in Figure 2(f).
The re-sult of the expected similarity maximization is thestring x?
= ababa, which is obtained by applyinga shortest-path algorithm to ?A(H) ?
Y1.
Observethat the string x with the largest probability in X isx = bbaba and is hence different from x?
= ababa inthis example.4.3 Efficient method for the unambiguous caseThe algorithm presented in the previous section forn-gram kernels can be generalized to handle a widevariety of rational kernels.Let K be an arbitrary rational kernel defined by aweighted transducer T .
Let XT denote the regularlanguage of the strings output by T .
We shall as-sume that XT is a finite language, though the resultsof this section generalize to the infinite case.
Let?
denote a new alphabet defined by ?
= {#x : x ?XT } and consider the simple grammar G of context-dependent batch rules:?
?
#x/x ?.
(9)Each such rule inserts the symbol #x immediatelyafter an occurrence x in the input string.
For batchcontext-dependent rules, the context of the applica-tion for all rules is determined at once before theirapplication (Kaplan and Kay, 1994).
Assume thatthis grammar is unambiguous for a parallel applica-tion of the rules.
This condition means that there isa unique way of parsing an input string using thestrings of XT .
The assumption holds for n-gramsequences, for example, since the rules applicableare uniquely determined by the n-grams (making theprevious section a special case).Given an acyclic weighted automaton Y2 over thetropical semiring accepting a subset of XT , we canconstruct a deterministic weighted automaton Y1 for?
?L(Y2) when this grammar is unambiguous.
Theweight assigned by Y1 to an input string is then thesum of the weights of the substrings accepted by Y2.This can be achieved using weighted determiniza-tion.This suggests a new method for generalizing Step2 of the algorithm described in the previous sectionas follows (see illustration in Figure 3):(i) use Y2 to construct a deterministic weightedtree Y ?2 defined on the tropical semiring ac-cepting the same strings as Y2 with the sameweights, with the final weights equal to the to-tal weight given by Y2 to the string ending atthat leaf;(ii) let Y1 be the weighted automaton obtained byfirst adding self-loops labeled with all elementsof ?
at the initial state of Y ?2 and then deter-minizing it, and then inserting new transitionsleaving final states as described in (Mohri andSproat, 1996).961Step (ii) consists of computing a deterministicweighted automaton for ?
?Y ?2 .
This step corre-sponds to the Aho-Corasick construction (Aho andCorasick, 1975) and can be done in time linear inthe size of Y ?2 .This approach assumes that the grammar G ofbatch context-dependent rules inferred by XT is un-ambiguous.
This can be tested by constructing thefinite automaton corresponding to all rules in G. Thegrammar G is unambiguous iff the resulting automa-ton is unambiguous (which can be tested using aclassical algorithm).
An alternative and more ef-ficient test consists of checking the presence of afailure or default transition to a final state duringthe Aho-Corasick construction, which occurs if andonly if there is ambiguity.5 Application to Machine TranslationIn machine translation, the BLEU score (Papineni etal., 2001) is typically used as an evaluation metric.In (Tromble et al, 2008), a Minimum Bayes-Riskdecoding approach for MT lattices was introduced.3The loss function used in that approach was an ap-proximation of the log-BLEU score by a linear func-tion of n-gram matches and candidate length.
Thisloss function corresponds to the following similaritymeasure:KLB(x, x?)
= ?0|x?|+?|w|?n?|w|cx(w)1x?(w).
(10)where 1x(w) is 1 if w occurs in x and 0 otherwise.
(Tromble et al, 2008) implements the MBR de-coder using weighted automata operations.
First,the set of n-grams is extracted from the lat-tice.
Next, the posterior probability p(w|X) ofeach n-gram is computed.
Starting with the un-weighted lattice A(H), the contribution of each n-gram w to (10) is applied by iteratively compos-ing with the weighted automaton corresponding tow(w/(?|w|p(w|X))w)?
where w = ??
\ (??w??
).Finally, the MBR hypothesis is extracted as the bestpath in the automaton.
The above steps are carriedout one n-gram at a time.
For a moderately large lat-tice, there can be several thousands of n-grams andthe procedure becomes expensive.
This leads us toinvestigate methods that do not require processingthe n-grams one at a time in order to achieve greaterefficiency.3Related approaches were presented in (DeNero et al, 2009;Kumar et al, 2009; Li et al, 2009).01?:?2?:?b:?3a:aa:?
b:ba:?b:?Figure 4: Transducer T 1 over the real semiring for thealphabet {a, b}.The first idea is to approximate the KLB similar-ity measure using a weighted sum of n-gram ker-nels.
This corresponds to approximating 1x?
(w) bycx?
(w) in (10).
This leads us to the following simi-larity measure:KNG(x, x?)
= ?0|x?|+?|w|?n?|w|cx(w)cx?
(w)= ?0|x?|+?1?i?n?iKi(x, x?
)(11)Intuitively, the larger the length of w the less likelyit is that cx(w) 6= 1x(w), which suggests comput-ing the contribution to KLB(x, x?)
of lower-ordern-grams (|w| ?
k) exactly, but using the approxima-tion by n-gram kernels for the higher-order n-grams(|w| > k).
This gives the following similarity mea-sure:KkNG(x, x?)
= ?0|x?|+?1?|w|?k?|w|cx(w)1x?(w)+?k<|w|?n?|w|cx(w)cx?
(w)(12)Observe that K0NG = KNG and KnNG = KLB .All these similarity measures can still be com-puted using the framework described in Section 4.Indeed, there exists a transducer Tn over the realsemiring such that Tn(x, z) = 1x(z) for all x ?
?
?and z ?
?n.
The transducer T 1 for ?
= {a, b} isgiven by Figure 4.
Let us define the similarity mea-sure Kn as:Kn(x, x?)
= (Tn?T?1n )(x, x?)
=?|w|=ncx(w)1x?(w).
(13)Observe that the framework described in Section 4can still be applied even though Kn is not symmet-ric.
The similarity measures KLB , KNG and KkNG962zhen arennist02 nist04 nist05 nist06 nist08 nist02 nist04 nist05 nist06 nist08no mbr 38.7 39.2 38.3 33.5 26.5 64.0 51.8 57.3 45.5 43.8exact 37.0 39.2 38.6 34.3 27.5 65.2 51.4 58.1 45.2 45.0approx 39.0 39.9 38.6 34.4 27.4 65.2 52.5 58.1 46.2 45.0ngram 36.6 39.1 38.1 34.4 27.7 64.3 50.1 56.7 44.1 42.8ngram1 37.1 39.2 38.5 34.4 27.5 65.2 51.4 58.0 45.2 44.8Table 1: BLEU score (%)zhen arennist02 nist04 nist05 nist06 nist08 nist02 nist04 nist05 nist06 nist08exact 3560 7863 5553 6313 5738 12341 23266 11152 11417 11405approx 168 422 279 335 328 504 1296 528 619 808ngram 28 72 34 70 43 85 368 105 63 66ngram1 58 175 96 99 89 368 943 308 167 191Table 2: MBR Time (in seconds)can then be expressed as the relevant linear combi-nation of Ki and Ki.6 Experimental ResultsLattices were generated using a phrase-based MTsystem similar to the alignment template system de-scribed in (Och and Ney, 2004).
Given a source sen-tence, the system produces a word lattice A that is acompact representation of a very large N -best list oftranslation hypotheses for that source sentence andtheir likelihoods.
The lattice A is converted into alattice X that represents a probability distribution(i.e.
the posterior probability distribution given thesource sentence) following:X(x) = exp(?A(x))?y???
exp(?A(y))(14)where the scaling factor ?
?
[0,?)
flattens the dis-tribution when ?
< 1 and sharpens it when ?
> 1.We then applied the methods described in Section 5to the lattice X using as hypothesis set H the un-weighted lattice obtained from X .The following parameters for the n-gram factorswere used:?0 =?1T and ?n =14Tprn?1 for n ?
1.
(15)Experiments were conducted on two languagepairs Arabic-English (aren) and Chinese-English(zhen) and for a variety of datasets from the NISTOpen Machine Translation (OpenMT) Evaluation.4The values of ?, p and r used for each pair are given4http://www.nist.gov/speech/tests/mt?
p raren 0.2 0.85 0.72zhen 0.1 0.80 0.62Table 3: Parameters used for performing MBR.in Table 3.
We used the IBM implementation of theBLEU score (Papineni et al, 2001).We implemented the following methods using theOpenFst library (Allauzen et al, 2007):?
exact: uses the similarity measure KLB basedon the linearized log-BLEU, implemented asdescribed in (Tromble et al, 2008);?
approx: uses the approximation to KLB from(Kumar et al, 2009) and described in the ap-pendix;?
ngram: uses the similarity measure KNG im-plemented using the algorithm of Section 4.2;?
ngram1: uses the similarity measure K1NGalso implemented using the algorithm of Sec-tion 4.2.The results from Tables 1-2 show that ngram1performs as well as exact on all datasets5 while be-ing two orders of magnitude faster than exact andoverall more than 3 times faster than approx.7 ConclusionWe showed that for broad families of transducersT and thus rational kernels, the expected similar-5We consider BLEU score differences of less than 0.4% notsignificant (Koehn, 2004).963ity maximization problem can be solved efficiently.This opens up the option of seeking the most appro-priate rational kernel or transducer T for the spe-cific task considered.
In particular, the kernel Kused in our machine translation applications mightnot be optimal.
One may well imagine for exam-ple that some n-grams should be further emphasizedand others de-emphasized in the definition of thesimilarity.
This can be easily accommodated in theframework of rational kernels by modifying the tran-sition weights of T .
But, ideally, one would wishto select those weights in an optimal fashion.
Asmentioned earlier, we leave this question to futurework.
However, we can offer a brief look at howone could tackle this question.
One method for de-termining an optimal kernel for the expected sim-ilarity maximization problem consists of solving aproblem similar to that of learning kernels in classi-fication or regression.
Let X1, .
.
.
, Xm be m latticeswith Ref(X1), .
.
.
,Ref(Xm) the associated refer-ences and let x?
(K,Xi) be the solution of the ex-pected similarity maximization for lattice Xi whenusing kernel K. Then, the kernel learning optimiza-tion problem can be formulated as follows:minK?K1mm?i=1L(x?(K,Xi),Ref(Xi))s.
t. K = T ?
T?1 ?
Tr[K] ?
C,where K is a convex family of rational kernels andTr[K] denotes the trace of the kernel matrix.
Inparticular, we could choose K as a family of linearcombinations of base rational kernels.
Techniquesand ideas similar to those discussed by Cortes et al(2008) for learning sequence kernels could be di-rectly relevant to this problem.A AppendixWe describe here the approximation of the KLBsimilarity measure from Kumar et al (2009).
Weassume in this section that the lattice X is determin-istic in order to simplify the notations.
The posteriorprobability of n-gram w in the lattice X can be for-mulated as:p(w|X) =?x??
?1x(w)P (x|s) =?x??
?1x(w)X(x)(16)where s denotes the source sentence.
When usingthe similarity measure KLB defined Equation (10),Equation (3) can then be reformulated as:x?
= argmaxx??H?0|x?|+?w?|w|cx?(w)p(w|X).
(17)The key idea behind this new approximation algo-rithm is to rewrite the n-gram posterior probability(Equation 16) as follows:p(w|X) =?x???
?e?EXf(e, w, ?x)X(x) (18)where EX is the set of transitions of X , ?x isthe unique accepting path labeled by x in X andf(e, w, ?)
is a score assigned to transition e on path?
containing n-gram w:f(e, w, ?)
=??
?1 if w ?
e, p(e|X) > p(e?|X),and e?
precedes e on ?0 otherwise.
(19)In other words, for each path ?, we count the tran-sition that contributes n-gram w and has the highesttransition posterior probability relative to its prede-cessors on the path ?
; there is exactly one such tran-sition on each lattice path ?.We note that f(e, w, ?)
relies on the full path ?which means that it cannot be computed based onlocal statistics.
We therefore approximate the quan-tity f(e, w, ?)
with f?
(e, w,X) that counts the tran-sition e with n-gram w that has the highest arc poste-rior probability relative to predecessors in the entirelattice X .
f?
(e, w,X) can be computed locally, andthe n-gram posterior probability based on f?
can bedetermined as follows:p(w|G) =?x????e?EXf?
(e, w,X)X(x)=?e?Ex1w?ef?
(e, w,X)?x???1pix(e)X(x)=?e?EX1w?ef?
(e, w,X)P (e|X),(20)where P (e|X) is the posterior probability of a lat-tice transition e ?
EX .
The algorithm to performLattice MBR is given in Algorithm 1.
For each statet in the lattice, we maintain a quantity Score(w, t)for each n-gram w that lies on a path from the initialstate to t. Score(w, t) is the highest posterior prob-ability among all transitions on the paths that termi-nate on t and contain n-gram w. The forward passrequires computing the n-grams introduced by eachtransition; to do this, we propagate n-grams (up tomaximum order ?1) terminating on each state.964Algorithm 1 MBR Decoding on Lattices1: Sort the lattice states topologically.2: Compute backward probabilities of each state.3: Compute posterior prob.
of each n-gram:4: for each transition e do5: Compute transition posterior probability P (e|X).6: Compute n-gram posterior probs.
P (w|X):7: for each n-gram w introduced by e do8: Propagate n?
1 gram suffix to he.9: if p(e|X) > Score(w, T (e)) then10: Update posterior probs.
and scores:p(w|X) += p(e|X) ?
Score(w, T (e)).Score(w, he) = p(e|X).11: else12: Score(w, he) = Score(w, T (e)).13: end if14: end for15: end for16: Assign scores to transitions (given by Equation 17).17: Find best path in the lattice (Equation 17).ReferencesAlfred V. Aho and Margaret J. Corasick.
1975.
EfficientString Matching: An Aid to Bibliographic Search.Communications of the ACM, 18(6):333?340.Cyril Allauzen, Michael Riley, Johan Schalkwyk, Woj-ciech Skut, and Mehryar Mohri.
2007.
OpenFst: ageneral and efficient weighted finite-state transducerlibrary.
In CIAA 2007, volume 4783 of LNCS, pages11?23.
Springer.
http://www.openfst.org.Christian Berg, Jens Peter Reus Christensen, and PaulRessel.
1984.
Harmonic Analysis on Semigroups.Springer-Verlag: Berlin-New York.Peter J. Bickel and Kjell A. Doksum.
2001.
Mathemati-cal Statistics, vol.
I. Prentice Hall.Corinna Cortes, Patrick Haffner, and Mehryar Mohri.2004.
Rational Kernels: Theory and Algorithms.Journal of Machine Learning Research, 5:1035?1062.Corinna Cortes, Mehryar Mohri, and Afshin Ros-tamizadeh.
2008.
Learning sequence kernels.
In Pro-ceedings of MLSP 2008, October.John DeNero, David Chiang, and Kevin Knight.
2009.Fast consensus decoding over translation forests.
InProceedings of ACL and IJCNLP, pages 567?575.Vaibhava Goel and William J. Byrne.
2000.
MinimumBayes-risk automatic speech recognition.
ComputerSpeech and Language, 14(2):115?135.Ronald M. Kaplan and Martin Kay.
1994.
Regular mod-els of phonological rule systems.
Computational Lin-guistics, 20(3).Philipp Koehn.
2004.
Statistical Significance Testsfor Machine Translation Evaluation.
In EMNLP,Barcelona, Spain.Shankar Kumar and William J. Byrne.
2004.
MinimumBayes-risk decoding for statistical machine transla-tion.
In HLT-NAACL, Boston, MA, USA.Shankar Kumar, Wolfgang Macherey, Chris Dyer, andFranz Och.
2009.
Efficient minimum error rate train-ing and minimum bayes-risk decoding for translationhypergraphs and lattices.
In Proceedings of the Asso-ciation for Computational Linguistics and IJCNLP.Christina S. Leslie, Eleazar Eskin, and William StaffordNoble.
2002.
The Spectrum Kernel: A String Kernelfor SVM Protein Classification.
In Pacific Symposiumon Biocomputing, pages 566?575.Zhifei Li, Jason Eisner, and Sanjeev Khudanpur.
2009.Variational decoding for statistical machine transla-tion.
In Proceedings of ACL and IJCNLP, pages 593?601.Huma Lodhi, Craig Saunders, John Shawe-Taylor, NelloCristianini, and Chris Watskins.
2002.
Text classifica-tion using string kernels.
Journal of Machine LearningResearch, 2:419?44.Mehryar Mohri and Richard Sproat.
1996.
An EfficientCompiler for Weighted Rewrite Rules.
In Proceedingsof ACL ?96, Santa Cruz, California.Mehryar Mohri.
2009.
Weighted automata algorithms.In Manfred Droste, Werner Kuich, and Heiko Vogler,editors, Handbook of Weighted Automata, chapter 6,pages 213?254.
Springer.Franz J. Och and Hermann Ney.
2004.
The align-ment template approach to statistical mchine transla-tion.
Computational Linguistics, 30(4):417?449.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2001.
Bleu: a Method for AutomaticEvaluation of Machine Translation.
Technical ReportRC22176 (W0109-022), IBM Research Division.Arto Salomaa and Matti Soittola.
1978.
Automata-Theoretic Aspects of Formal Power Series.
Springer.Roy W. Tromble, Shankar Kumar, Franz J. Och, andWolfgang Macherey.
2008.
Lattice minimum Bayes-risk decoding for statistical machine translation.
InProceedings of the 2008 Conference on EmpiricalMethods in Natural Language Processing, pages 620?629.965
