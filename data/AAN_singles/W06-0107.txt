Proceedings of the Fifth SIGHAN Workshop on Chinese Language Processing, pages 48?55,Sydney, July 2006. c?2006 Association for Computational LinguisticsLatent Features in Automatic Tense Translation between Chinese andEnglishYang Ye?, Victoria Li Fossum?, Steven Abney ?
??
Department of Linguistics?
Department of Electrical Engineering and Computer ScienceUniversity of MichiganAbstractOn the task of determining the tense to usewhen translating a Chinese verb into En-glish, current systems do not perform aswell as human translators.
The main focusof the present paper is to identify featuresthat human translators use, but which arenot currently automatically extractable.The goal is twofold: to test a particu-lar hypothesis about what additional infor-mation human translators might be using,and as a pilot to determine where to focuseffort on developing automatic extractionmethods for features that are somewhat be-yond the reach of current feature extrac-tion.
The paper shows that incorporatingseveral latent features into the tense clas-sifier boosts the tense classifier?s perfor-mance, and a tense classifier using only thelatent features outperforms one using onlythe surface features.
Our findings confirmthe utility of the latent features in auto-matic tense classification, explaining thegap between automatic classification sys-tems and the human brain.1 IntroductionLanguage speakers make two types of distinctionsabout temporal relations: the first type of relationis based on precedence between events and can beexpanded into a finer grained taxonomy as pro-posed by (Allen, 1981).
The second type of re-lation is based on the relative positioning betweenthe following three time parameters proposed by(Reichenbach, 1947): speech time (S), event time(E) and reference time (R).
In the past couple ofdecades, the NLP community has seen an emer-gent interest in the first type of temporal relation.In the cross-lingual context, while the first type ofrelationship can be easily projected across a lan-guage pair, the second type of relationship is of-ten hard to be projected across a language pair.
Incontrast to this challenge, cross-lingual temporalreference distinction has been poorly explored.Languages vary in the granularity of theirtense and aspect representations; some have finer-grained tenses or aspects than others.
Tense gener-ation and tense understanding in natural languagetexts are highly dynamic and context-dependentprocesses, since any previously established timepoint or interval, whether explicitly mentioned inthe context or not, could potentially serve as thereference time for the event in question.
(Bruce,1972) captures this nature of temporal referenceorganization in discourse through a multiple tem-poral reference model.
He defines a set (S1, S2, ...,Sn) that is an element of tense.
S1 corresponds tothe speech time, Sn is the event time, and (Si, i=2,..., n-1) stand for a sequence of time referencesfrom which the reference time of a particular eventcould come.
Given the elusive nature of referencetime shift, it is extremely hard to model the ref-erence time point directly in temporal informationprocessing.
The above reasons motivate classify-ing temporal reference distinction automatically,using machine learning algorithms such as Con-ditional Random Fields (CRFs).Many researchers in Natural Language Process-ing seem to believe that an automatic system doesnot have to follow the mechanism of human brainin order to optimize its performance, for example,the feature space for an automatic classificationsystem does not have to replicate the knowledgesources that human beings utilize.
There has beenvery little research that pursues to testify this faith.The current work attempts to identify whichfeatures are most important for tense generationin Chinese to English translation scenario, whichcan point to direction of future research effort forautomatic tense translation between Chinese andEnglish.48The remaining part of the paper is organizedas follows: Section 2 summarizes the significantrelated works in temporal information annotationand points out how this study relates to yet dif-fers from them.
Section 3 formally defines theproblem, tense taxonomy and introduces the data.Section 4 discusses the feature space and proposesthe latent features for the tense classification task.Section 5 presents the classification experimentsin Conditional Random Fields as well as Classifi-cation Tree and reports the evaluation results.
Sec-tion 6 concludes the paper and section 7 points outdirections for future research.2 Related WorkThere is an extensive literature on temporal infor-mation processing.
(Mani, et al, 2005) providesa survey of works in this area.
Here, we high-light several works that are closely related to Chi-nese temporal information processing.
(Li, 2001)describes a model of mining and organizing tem-poral relations embedded in Chinese sentences,in which a set of heuristic rules are developed tomap linguistic patterns to temporal relations basedon Allen?s thirteen relations.
Their work showspromising results via combining machine learningtechniques and linguistic features for successfultemporal relation classification, but their work isconcerned with another type of temporal relation-ship, namely, the precedence-based temporal rela-tion between a pair of events explicitly mentionedin text.A significant work worth mentioning is (Olsenet.
al.
2001)?s paper, where the authors exam-ine the determination of tense for English verbsin Chinese-to-English translation.
In addition tothe surface features such as the presence of aspectmarkers and certain adverbials, their work makesuse of the telicity information encoded in the lexi-cons through the use of Lexical Conceptual Struc-tures (LCS).
Based on the dichotomy of grammat-ical aspect and lexical aspect, they propose thatpast tense corresponds to the telic (either inher-ently or derived) LCS.
They propose a heuristicalgorithm in which grammatical aspect markingssupersede the LCS, and in the absence of gram-matical aspect marking, verbs that have telic LCSare translated into past tense and present tense oth-erwise.
They report a significant performance im-provement in tense resolution from adding a verbtelicity feature.
They also achieve better perfor-mance than the baseline system using the telic-ity feature alone.
This work, while alerting re-searchers to the importance of lexical aspectualfeature in determination of tense for English verbsin Chinese-to-English machine translation, is sub-ject to the risk of adopting a one-to-one mappingbetween grammatical aspect markings and tenseshence oversimplifies the temporal reference prob-lem in Chinese text.
Additionally, their binarytense taxonomy is too coarse for the rich tempo-ral reference system in Chinese.
(Ye, et al 2005) reported a tense tagging casestudy of training Conditional Random Fields ona set of shallow surface features.
The low inter-annotator agreement rate reported in the paper il-lustrates the difficulty of tense tagging.
Neverthe-less, the corpora size utilized is too small with only52 news articles and none of the latent features wasexplored, so the evaluation result reported in thepaper leaves room for improvement.3 Problem Definition3.1 Problem FormulationThe problem we are interested in can be formal-ized as a standard classification or labeling prob-lem, in which we try to learn a classifierC : V ?
T (1)where V is a set of verbs (each described by afeature vector), and T is the set of possible tensetags.Tense and aspect are morphologically mergedin English and coarsely defined, there can betwelve combinations of the simple tripartite tenses(present, past and future) with the progressive andperfect grammatical aspects.
For our classificationexperiments, in order to combat sparseness, we ig-nore the aspects and only deal with the three sim-ple tenses: present, past and future.3.2 DataWe use 152 pairs of parallel Chinese-English arti-cles from LDC release.
The Chinese articles comefrom two news sources: Xinhua News Service andZaobao News Service, consisting of 59882 Chi-nese characters in total with roughly 350 charac-ters per article.
The English parallel articles arefrom Multiple-Translation Chinese (MTC) Corpusfrom LDC with catalog number LDC2002T01.We chose to use the best human translation out49of 9 translation teams as our gold-standard par-allel English data.
The verb tenses are obtainedthrough manual alignment between the Chinesesource articles and the English translations.
In or-der to avoid the noise brought by errors and be fo-cused on the central question we try to answer inthe paper, we did not use automatic tools such asGIZA++ to obtain the verb alignments, which typ-ically comes with significant amount of errors.
Weignore Chinese verbs that are not translated intoEnglish as verbs because of ?nominalization?
(bywhich verbal expressions in Chinese are translatedinto nominal phrases in English).
This exclusion isbased on the rationale that another choice of syn-tactic structure might retain the verbal status in thetarget English sentence, but the tense of those po-tential English verbs would be left to the joint de-cision of a set of disparate features.
Those tensesare unknown in our training data.
This preprocess-ing yields us a total of 2500 verb tokens in our dataset.4 Feature Space4.1 Surface FeaturesThere are many heterogeneous features that con-tribute to the process of tense generation for Chi-nese verbs in the cross-lingual situation.
Tenses inEnglish, while manifesting a distinction in tempo-ral reference, do not always reflect this distinctionat the semantic level, as is shown in the sentence ?Iwill leave when he comes.?
(Hornstein, 1990) ac-counts for this phenomenon by proposing the Con-straints on Derived Tense Structures.
Therefore,the feature space we use includes the features thatcontribute to the semantic level temporal referenceconstruction as well as those contributing to tensegeneration from that semantic level.
The follow-ing is a list of the surface features that are directlyextractable from the training data:1.
Feature 1: Whether the verb is in quotedspeech or not.2.
Feature 2: The syntactic structure in whichthe current verb is embedded.
Possible struc-tures include sentential complements, rel-ative clauses, adverbial clauses, appositiveclauses, and null embedding structure.3.
Feature 3: Which of the following signaladverbs occur between the current verband the previous verb: yi3jing1(already),ceng2jing1(once), jiang1(future tensemarker), zheng4zai4(progressive aspectmarker), yi4zhi2(have always been).4.
Feature 4: Which of the following aspectmarkers occur between the current verb andthe subsequent verb: le0, zhe0, guo4.5.
Feature 5: The distance in characters betweenthe current verb and the previously taggedverb (We descretize the continuous distanceinto three ranges: 0 < distance < 5, 5 ?distance < 10, or 10 ?
distance <?).6.
Feature 6: Whether the current verb is in thesame clause as the previous verb.Feature 1 and feature 2 are used to capture thediscrepancy between semantic tense and syntactictense.
Feature 3 and feature 4 are clues or triggersof certain aspectual properties of the verbs.
Fea-ture 5 and feature 6 try to capture the dependencybetween tenses of adjacent verbs.4.2 Latent FeaturesThe bottleneck in Artificial Intelligence is the un-balanced knowledge sources shared by human be-ings and a computer system.
Only a subset ofthe knowledge sources used by human beings canbe formalized, extracted and fed into a computersystem.
The rest are less accessible and are veryhard to be shared with a computer system.
De-spite their importance in human language process-ing, latent features have received little attention infeature space exploration in most NLP tasks be-cause they are impractical to extract.
Althoughthere have not yet been rigorous psycholinguis-tic studies demonstrating the extent to which theabove knowledge types are used in human tempo-ral relation processing, we hypothesize that theyare very significant in assisting human?s temporalrelation decision.
Nevertheless, a quantitative as-sessment of the utility of the latent features in NLPtasks has yet to be explored.
(Olsen, et al, 2001)illustrates the value of latent features by showinghow the telicity feature alone can help with tenseresolution in Chinese to English machine transla-tion.
Given the prevalence of latent features in hu-man language processing, in order to emulate hu-man beings performance of the disambiguation, itis crucial to experiment with the latent features inautomatic tense classification.
(Pustejovsky, 2004) discusses the four basicproblems in event-temporal identification:50???????????????????????????????????????????
?He said that Henan Province not only possesses the hardwares necessary for foreigninvestment, but also has, on the basis of the State policies and Henan's specificconditions, formulated its own preferential policies.?
??
????
?
?N/A include subsume??
?
?Figure 1: Temporal Relations between Adjacent Events1.
Time-stamping of events (identifying anevent and anchoring it in time)2.
Ordering events with respect to one another3.
Reasoning with contextually under-specifiedtemporal expressions4.
Reasoning about the persistence of events(how long does an event or the outcome ofan event last?
)While time-stamping of the events and reason-ing with contextually under-specified temporal ex-pressions might be too informative to be featuresin tense classification, information concerning or-derings between events and persistence of eventsare relatively easier to be encoded as features ina tense classification task.
Therefore, we exper-iment with these two latent knowledge sources,both of which are heavily utilized by human be-ings in tense resolution.4.3 Telicity and Punctuality FeaturesFollowing (Vendler, 1947), temporal informationencoded in verbs is largely captured by some in-nate properties of verbs, of which telicity andpunctuality are two very important ones.
Telic-ity specifies a verb?s ability to be bound in a cer-tain time span, while punctuality specifies whetheror not a verb is associated with a point event intime.
Telicity and punctuality prepare verbs to beassigned different tenses when they enter the con-text in the discourse.
While it is true that isolatedverbs are typically associated with certain telicityand punctuality features, such features are contex-tually volatile.
In reaction to the volatility exhib-ited in verb telicity and punctuality features, wepropose that verb telicity and punctuality featuresshould be evaluated only at the clausal or senten-tial level for the tense classification task.
We man-ually obtained these two features for both the En-glish and the Chinese verbs.
All verbs in our dataset were manually tagged as ?telic?
or ?atelic?, and?punctual?
or ?apunctual?, according to context.4.4 Temporal Ordering Feature(Allen, 1981) defines thirteen relations that couldpossibly hold between any pair of situations.
Weexperiment with six temporal relations which wethink represent the most typical temporal relation-ships between two events.
We did not adopt allof the thirteen temporal relationships proposed byAllen for the reason that some of them would re-quire excessive deliberation from the annotatorsand hard to implement.
The six relationships weexplore are as follows:1. event A precedes event B2.
event A succeeds event B3.
event A includes event B4.
event A subsumes event B5.
event A overlaps with event B6.
no temporal relations between event A andevent BFor each Chinese verb in the source Chinesetexts, we annotate the temporal relation betweenthe verb and the previously tagged verb as belong-ing to one of the above classes.
The annotationof the temporal relation classes mimics a deepersemantic analysis of the Chinese source text.
Fig-ure 1 illustrates a sentence in which each verb istagged by the temporal relation class that holds be-tween it and the previous verb.515 Experiments and Evaluation5.1 CRF learning algorithmsConditional Random Fields (CRFs) are a formal-ism well-suited for learning and prediction on se-quential data in many NLP tasks.
It is a prob-abilistic framework proposed by (Lafferty et al,2001) for labeling and segmenting structured data,such as sequences, trees and lattices.
The condi-tional nature of CRFs relaxes the independence as-sumptions required by traditional Hidden MarkovModels (HMMs).
This is because the conditionalmodel makes it unnecessary to explicitly representand model the dependencies among the input vari-ables, thus making it feasible to use interacting andglobal features from the input.
CRFs also avoidthe label bias problem exhibited by maximum en-tropy Markov models (MEMMs) and other con-ditional Markov models based on directed graph-ical models.
CRFs have been shown to performwell on a number of NLP problems such as shal-low parsing (Sha and Pereira, 2003), table extrac-tion (Pinto et al, 2003), and named entity recog-nition (McCallum and Li, 2003).
For our exper-iments, we use the MALLET implementation ofCRF?s (McCallum, 2002).5.2 Experiments5.2.1 Human Inter-Annotator AgreementAll supervised learning algorithms require acertain amount of training data, and the reliabilityof the computational solutions is intricately tiedto the accuracy of the annotated data.
Human an-notations typically suffer from errors, subjectivity,and the expertise effect.
Therefore, researchersuse consistency checking to validate human an-notation experiments.
The Kappa Statistic (Co-hen, 1960) is a standard measurement of inter-annotator agreement for categorical data annota-tion.
The Kappa score is defined by the followingformula, where P(A) is the observed agreementrate from multiple annotators and P(E) is the ex-pected rate of agreement due to pure chance:k = P (A)?
P (E)1?
P (E)(2)Since tense annotation requires disambiguatinggrammatical meaning, which is more abstract thanlexical meaning, one would expect the challengeposed by human annotators in a tense annota-tion experiment to be even greater than for wordsense disambiguation.
Nevertheless, the tense an-notation experiment carried as a precursor to ourtense classification task showed a kappa Statisticof 0.723 on the full taxonomy, with an observedagreement of 0.798.
In those experiments, weasked three bilingual English native speakers whoare fluent in Chinese to annotate the English verbtenses for the first 25 Chinese and English parallelnews articles from our training data.We could also obtain a measurement of reliabil-ity by taking one annotator as the gold standardat one time, then averaging over the precisions ofthe different annotators across different gold stan-dards.
While it is true that numerically, precisionwould be higher than Kappa score and seems tobe inflating Kappa score, we argue that the dif-ference between Kappa score and precision is notlimited to one measure being more aggressive thanthe other.
Rather, the policies of these two mea-surements are different.
The Kappa score carespurely about agreement without any considerationof trueness or falseness, while the procedure wedescribed above gives equal weight to each anno-tator being the gold standard, and therefore con-siders both agreement and truthness of the annota-tion.
The advantage of the precision-based agree-ment measurement is that it makes comparison ofthe system performance accuracy to the humanperformance accuracy more direct.
The precisionunder such a scheme for the three annotators is80% on the full tense taxonomy.5.2.2 CRF Learning ExperimentsWe train a tense classifier on our data set in twostages: first on the surface features, and then onthe combined space of both surface features (dis-cussed in 4.1) and latent features (discussed in 4.2-4.4).
It is conceivable that the granularity of se-quences may matter in learning from data with se-quential relationship, and in the context of verbtense tagging, it naturally maps to the granularityof discourse.
(Ye, et al, 2005) shows that thereis no significant difference between sentence-levelsequences and paragraph-level sequences.
There-fore, we experiment with only sentence-level se-quences.5.2.3 Classification Tree LearningExperimentsTo verify the stability of the utility of the la-tent features, we also experiment with classifica-tion tree learning on the same features space as52Tense Precision Recall FPresent tense 0.662 0.661 0.627Past tense 0.882 0.915 0.896Future tense 0.758 0.487 0.572Table 1: Evaluation Results for CRFs Classifier in Precision, Recall and F Using All FeaturesSurface Features Latent Features Surface and Latent FeaturesAccuracy for Training Data 79.3% 82.9% 85.9%Table 2: Apparent Accuracy for the Training Data of the Classification Tree Classifiersdiscussed above.
Classification Trees are usedto predict membership of cases or objects in theclasses of a categorical dependent variable fromtheir measurements on one or more predictor vari-ables.
The main idea of Classification Tree is todo a recursive partitioning of the variable spaceto achieve good separation of the classes in thetraining dataset.
We use the Recursive Partition-ing and Regression Trees(Rpart) package providedby R statistical computing software for the imple-mentation of classification trees.
In order to avoidover-fitting, we prune the tree by setting the min-imum number of objects in a node to attempt asplit and the minimum number of objects in anyterminal node to be 10 and 3 respectively.
In theconstructed classification tree when we use all fea-tures including both surface and latent features,the top split at the root node in the tree is basedon telicity feature of the English verb, indicatingthe importance of telicity feature for English verbamong all of the features.5.3 Evaluation ResultsAll results are obtained by 5-fold cross validation.The classifier?s performance is evaluated againstthe tenses from the best-ranked human-generatedEnglish translation.
To evaluate the performanceof the CRFs tense classifier, we compute the pre-cision, recall, general accuracy and F, which aredefined as follow.Accuracy =npredictionNprediction(3)Recall = nhitS(4)Precision = nhitNhit(5)F = 2?
Precision ?RecallPrecision + Recall(6)where1.
Nprediction: Total number of predictions;2. nprediction: Number of correct predictions;3.
Nhit: Total number of hits;4. nhit: Number of correct hits;5.
S: Size of perfect hitlist;From Table 1, we see that past tense, which oc-curs most frequently in the training data, has thehighest precision, recall and F. Future tense, whichoccurs least frequently, has the lowest F. Precisionand recall do not show clear pattern across differ-ent tense classes.Table 2 presents the apparent classification ac-curacies for the training data, we see that latentfeatures still outperform the surface features.
Ta-ble 3 summarizes the general accuracies of thetense classification systems for CRFs and Classifi-cation Trees.
The CRFs classifier and the Classifi-cation Tree classifier demonstrate similar scales ofimprovement from surface features, latent featuresto both surface and latent features.53Methodology Surface Features Latent Features Surface and Latent FeaturesCRFs 75.8% 80% 83.4%Classification Tree 74.1% 81% 84.5%Table 3: Evaluations in General Accuracy5.4 Baseline SystemsTo better evaluate our tense classifiers, we providetwo baseline systems here.
The first baseline sys-tem is the tense resolution from the best rankedmachine translation system?s translation results inthe MTC corpus mentioned above.
When evalu-ated against the reference tense tags from the bestranked human translation team, the best MT sys-tem yields a accuracy of 47%.
The second base-line system is a naive system that assigns the mostfrequent tense in the training data set, which in ourcase is past tense, to all verbs in the test data set.Given the fact that we are deadling with newswiredata, this baseline system yields a high baselinesystem with an accuracy of 69.5%.6 Discussion and ConclusionsTo the best of our knowledge, the current paperis the first work investigating the utility of latentfeatures in the task of machine-learning based au-tomatic tense classification.
We significantly out-perform the two baseline systems as well as theautomatic tense classifier performance reported by(Ye, et al, 2005) by 15% in general accuracy.
Acrucial finding of our experiments is that utility ofonly three latent features, i.e.
verb telicity, verbpunctuality and temporal ordering between adja-cent events, outperforms that of all the surfacelinguistic features we discussed earlier in the pa-per.
While one might think that the lack of exist-ing techonology of latent feature extraction woulddiscount research effort on latent features?
utili-ties, we believe that such efforts guide the researchcommunity to determine where to focus effort ondeveloping automatic extraction methods for fea-tures that are beyond the reach of current tech-nologies.
Such research effort will also help toshed light on the enigmatic research question ofwhether automatic NLP systems should take ef-fort to make use of the features employed by hu-man beings to optimize the system performanceand shorten the gap between the system and hu-man brain.
The results of the current paper pointto the fact that bottleneck of cross-linguistic tenseclassification is acquisition and modeling of themore latent linguistic knowledge.
To our surprise,CRF tense classifier performance is consistentlytied with classification tree tense classifier perfor-mance in all of our experiments.
One might expectthat CRFs would accurately capture sequential de-pendencies among verbs.
Reflecting upon the sim-ilar evaluation results of the CRFs classifier andthe Classification Tree classifier, it is unlikely forthis to be due to the over-fitting of the Classifi-cation Tree because of the pruning we did to theClassification Trees.
Therefore, we speculate thatthe dependencies between the tense tags of verbsin the texts may not be strong enough for CRFsto outperform Classification Tree.
This might alsobe contributable to the built-in variable selectionprocedures of Classification Trees, which makesit more robust to interacting and interdependentfeatures.
A confirmative explanation towards theequal performances between the CRFs and theClassification Tree classifiers requires more exper-iments with other machine learning algorithms.In conclusion, this paper makes the followingcontributions:1.
It demonstrates that an accurate tense classi-fier can be constructed automatically by com-bining off-the-shelf machine learning tech-niques and inexpensive linguistic features.2.
It shows that latent features (such as verbtelicity, verb punctuality and temporal order-ing between adjacent events) have higher util-ity in tense classification than the surface lin-guistic features.3.
It reveals that the sequential dependency be-tween tenses of adjacent verbs in the dis-course may be rather weak.547 Future WorkTemporal reference is a complicated semantic do-main with rich connections among the disparatefeatures.
We investigate three latent features:telicity, punctuality, and temporal ordering be-tween adjacent verbs.
We summarize several in-teresting questions for future research in this sec-tion.
First, besides the latent features we examinedin the current paper, there are other interestinglatent features to be investigated under the sametheme, e.g.
classes of temporal expression associ-ated with the verbs and causal relationships amongdisparate events.
Second, currently, the latent fea-tures are obtained through manual annotation bya single annotator.
In an ideal situation, multi-ple annotators are desired to provide the reliabil-ity of the annotations as well as reduce the noisein annotations.
Thirdly, it would be interesting toexamine the utility of the same latent features forclassification in the opposite direction, namely, as-pect marker classification for Chinese verbs in theEnglish-to-Chinese translation scenario.
Finally,following our discussion of the degree of depen-dencies among verb tenses in the texts, it is desir-able to study rigorously the dependencies amongtenses and aspect markers for verbs in extensionsof the current research.ReferencesJames Allen.
1981.
Towards a General Theory of Ac-tion and Time.
Artificial Intelligence, 23(2): 123-160.Hans Reichenbach.
1947.
Elements of Symbolic Logic.Macmillan, New York, N.Y.B.
Bruce.
1972.
A Model for Temporal Reference andits Application in a Question Answering System, Ar-tificial Intelligence.
Vol.
3, No.
1, 1-25.Inderjeet Mani, James Pustejovsky and RobertGaizauskas.
2005.
The Language of Time, OxfordPress.Wenjie Li, Kam-Fai Wong, Caogui Hong, ChunfaYuan.
2004.
Applying Machine Learning to ChineseTemporal Relation Resolution.
Proceedings of the42nd Annual Meeting of the Association for Com-putational Linguistics, 582-588.Mari Olson, David Traum, Carol Van-ess Dykema,and AmyWeinberg.
2001.
Implicit Cues for ExplicitGeneration: Using Telicity as a Cue for Tense Struc-ture in a Chinese to English MT System.
Proceed-ings Machine Translation Summit VIII, Santiago deCompostela, Spain.Yang Ye, Zhu Zhang.
2005.
Tense Tagging for Verbs inCross-Lingual Context: a Case Study.
Proceedingsof IJCNLP 2005, 885-895Norbert Hornstein.
1990.
As Time Goes By: Tense andUniversal Grammar.
The MIT Press.James Pustejovsky, Robert Ingria, Roser Sauri, JoseCastano, Jessica Littman, Rob Gaizauskas, AndreaSetzer, Graham Katz, and Inderjeet Mani.
2004.
TheSpecification Language TimeML.
The Language ofTime: A Reader.
Oxford, 185-96.Zeno Vendler.
1967.
Verbs and Times.
Linguistics inPhilosophy, 97-121.Lafferty, J., McCallum, A. and Pereira, F. 2001.
Con-ditional random fields: Probabilistic models for seg-menting and labeling sequence data.
In Proceedingsof ICML-01, 282-289.Sha, F. and Pereira, F. 2003.
Shallow Parsing withConditional Random Fields.
Proceedings of the2003 Human Language Technology Conference andNorth American Chapter of the Association forComputational Linguistics (HLT/NAACL-03)Pinto, D., McCallum, A., Lee, X. and Croft, W. B.2003.
Table Extraction Using Conditional RandomFields.
Proceedings of the 26th Annual InternationalACM SIGIR Conference on Research and Develop-ment in Information Retrieval (SIGIR 2003)McCallum, A. and Li, W. 2003.
Early Results forNamed Entity Recognition with Conditional Ran-dom Fields, Feature Induction and Web-EnhancedLexicons.
Proceedings of the Seventh Conference onNatural Language Learning (CoNLL)McCallum, A. K. 2002.
MALLET: A Ma-chine Learning for Language Toolkit,http://mallet.cs.umass.edu.Jacob Cohen, 1960.
A Coefficient of Agreement forNominal Scales, Educational and PsychologicalMeasurement, 20, 37-46.Ross Ihaka and Robert Gentleman.
1996.
R: A Lan-guage for Data Analysis and Graphics, Journalof Computational and Graphical Statistics, Vol.
5.299?14.55
