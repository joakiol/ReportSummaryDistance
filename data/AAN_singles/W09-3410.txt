Proceedings of the 7th Workshop on Asian Language Resources, ACL-IJCNLP 2009, pages 70?75,Suntec, Singapore, 6-7 August 2009. c?2009 ACL and AFNLPConstruction of Chinese Segmented and POS-tagged  Conversational Corporaand Their Evaluations on Spontaneous Speech RecognitionsXinhui Hu, Ryosuke Isotani, Satoshi NakamuraNational Institute of Information and Communications Technology, Japan{xinhui.hu, ryosuke.isotani, satoshi.nakamura}@nict.go.jpAbstractThe performance of a corpus-based language andspeech processing system depends heavily on thequantity and quality of the training corpora.
Althoughseveral famous Chinese corpora have been developed,most of them are mainly written text.
Even for someexisting corpora that contain spoken data, the quantityis insufficient and the domain is limited.
In this paper,we describe the development of Chinese conversationalannotated textual corpora currently being used in theNICT/ATR speech-to-speech translation system.
A totalof 510K manually checked utterances provide 3.5Mwords of Chinese corpora.
As far as we know, this isthe largest conversational textual corpora in thedomain of travel.
A set of three parallel corpora isobtained with the corresponding pairs of Japanese andEnglish words from which the Chinese words aretranslated.
Evaluation experiments on these corporawere conducted by comparing the parameters of thelanguage models, perplexities of test sets, and speechrecognition performance with Japanese and English.The characteristics of the Chinese corpora, theirlimitations, and solutions to these limitations areanalyzed and discussed.1.
IntroductionIn corpus-based machine translation and speechrecognition, the performance of the language modeldepends heavily on the size and quality of the corpora.Therefore, the corpora are indispensable for thesestudies and applications.
In recent decades, corpusdevelopment has seen rapid growth for manylanguages such as English, Japanese, and Chinese.
ForChinese, since there are no plain delimiters among thewords, the creation of a segmented and part-of-speech(POS)-tagged corpus is the initial step for moststatistical language processes.
Several such Chinesecorpora have been developed since the 1990s.
The twomost typical are People?s Daily corpus (referred to asPKU), jointly developed by the Institute ofComputational Linguistics of Peking University andthe Fujitsu Research & Development Center [1], andthe Sinica Corpus (referred to as Sinica) developed bythe Institute of Information Science and the CKIPGroup in Academia Sinica of Taiwan [2].
The formeris based on the People?s Daily newspaper in 1998.
It!uses standard articles of news reports.
The latter is abalanced corpus collected from different areas andclassified according to five criteria: genre, style, mode,topic, and source.
Although conversational text is alsocontained in this corpus, it has only 75K of utterancesand the domains are limited to a few fields, such asacademia and economics, and the style is mostly inaddress and seldom in conversation.Since the features of conversation differ from writtentext, especially in news articles, the development of asegmented and POS-tagged corpus of conversationallanguage is promising work for spontaneous speechrecognition and speech-to-speech translation.In the Spoken Communication Group of NICT, inorder to study corpus-based speech translationtechnologies for the real world, a set of corpora ontravel conversation has been built for Japanese,English, and Chinese [3].
These corpora are elaboratelydesigned and constructed on the basis of the concept ofvariety in samples, situations, and expressions.
Nowthese corpora have been used in the NICT speech-to-speech translation (S2ST) system [8] and otherservices.In this paper, we introduce our work on this Chinesecorpora development and applications in S2ST speechrecognition using these corpora.
In Section 2, weprovide a brief description of the contents of the NICTcorpora, then describe how the Chinese data wereobtained.
In Section 3, we illustrate the specificationsfor the segmentation and POS tagging designed forthese corpora.
Here, we explain the guidelines ofsegmentation and POS tagging, placing particularemphasis on the features of conversation and speechrecognition application.
In Section 4, we outline thedevelopment procedures and explain our methods ofhow to get the segmented and POS-tagged data.
Somestatistical characteristics of the corpora will be shownhere.
In Section 5, evaluation experiments of speechrecognition utilizing these corpora are reported bycomparing the results using the same data sets of70Japanese and English.
Finally, in Section 6, we discussthe performance of the corpora, the problems thatremain in the corpora, and give our ideas concerningfuture work.2.
Current NICT Chinese Corpora onTravel Dialog DomainAt NICT, in order to deal with various conversationalcases of S2ST research, several kinds of corpora wereelaborately designed and constructed [3].
Table 1 givesa brief description of the data sets related to thedevelopment of the Chinese corpora.
Each corpusshown in this table was collected using differentmethods, for different application purposes, and wascategorized into different domains.Table 1.
NICT Corpora Used for Chinese ProcessingName Collecting Method Uttr.
DomainSLDBBilingual conversationevolved byinterpreters.16KDialogueswith thefront deskclerk at ahotelMADBilingual conversationevolved by a machinetranslation system.19KGeneraldialogueson travelBTEC Text in guidebooks for overseas travelers 475KGeneraldialogueson travelThe SLDB (Spoken Language Database) is acollection of transcriptions of spoken languagebetween two people speaking different languages andmediated by a professional interpreter.In comparison, the MAD (Machine Translation AidDialogue) is a similar collection, but it uses our S2STsystem instead of an interpreter.The BTEC (Basic Travel Expression Corpus) is acollection of Japanese sentences and their Englishtranslations written by bilingual travel experts.
Thiscorpus covers such topics related to travel as shopping,hotel or restaurant reservations, airports, lost andfound, and so on.The original data of the above corpora weredeveloped in the form of English-to-Japanesetranslation pairs.
The Chinese versions are mainlytranslated from the Japanese, but a small portion ofBTEC (namely, BTEC4, about 70K of utterances) wastranslated from English.
Every sentence in thesecorpora has an equivalent in the other two languages,and they share a common header (ID), except for thelanguage mark.
All the data in these three languagesconstitute a set of parallel corpora.
The followingshows examples of sentences in the three languages:Chn.
: BTEC1\jpn067\03870\zh\\\\???????Eng.
: BTEC1\jpn067\03870\en\\\\I'd like to have some strongcoffee.Jap.:BTEC1\jpn067\03870\ja\\\\????????????3.
Specifications of Segmentation and Part-of-Speech TaggingBy mainly referring to the PKU and taking intoaccount the characteristics of conversational data, wemade our definitions for segmentation units and POStags.
Here, we explain the outlines of these definitions,then illustrate the segmentation and POS-tagging itemsrelating to those considerations on conversations.3.1.
Guidelines of the Definitions(1) Compatibility with the PKU and Taking intoaccount the Demand of Speech Recognition ofS2STSince the specification of segmentations and POS-tagging proposed by the PKU [4] has its palpabilityand maneuverability and is close to China?s nationalstandard [5] on segmentation and close to thespecification on POS tags recommended by theNational Program of China, namely, the 973-project[6], we mainly followed PKU?s specification.
Weadopted the concept of ?segmentation unit,?
i.e., wordswith disyllable, trisyllable, some compound words, andsome phrases were regarded as segmentation units.
Themorpheme character (word) was also regarded as anindependent unit.However, we made some adjustments to thesespecifications.
In the speech recognition phase of S2STto deal with data sparseness, the word for ?training?needed to be shortened.
So a numeral  was divided intosyllabic units, while both the PKU and the Sinica tookthe whole number as a unit.
For the same reason, thedirectional verbs (????
), such as ?
?, ?????????
and ??
,?
which generally followanother verb and express action directions, weredivided from the preceding verb.
The modal auxiliaryverbs (????
), such as ????
?and ?,?
whichoften precede another verb were separated and taggedwith an individual tag set.
Because the numeral can beeasily reunited as an integrated unit, such a processingmethod for numerals does not harm the translationphase of S2ST.
Moreover, if the directional verb andthe modal auxiliary verb can be identified, they willhelp the syntactic analysis and improve the translationphrase.
These two kinds of verbs, together with ??(be)?
and ??
(have)?
are more frequently used in71colloquial conversations than in written text, so wetook them as an individual segmentation unit andassigned a POS tag to each.
The special processes forthese kinds of words aim at reflecting the features ofspoken language and improve the performance of theS2ST system.
(2) Ability for Future ExpansionAlthough the corpora were developed for speechrecognition in S2ST system, it is desirable that theycan be used in other fields when necessary.
Thisreflects in both segmentation and POS-tagging.
Insegmentation, the compound words with definitivesuffix or prefix are divided, so they can be combinedeasily when necessary.
In POS-tagging, the nouns andverbs are mainly further categorized into several sub-tags.
We selected about 40 POS tags for our corpora,as shown in Table 1 in the Appendix.
With such scaleof tag sets, it is regarded to be suitable forlanguage model of ASR.
When necessary, it isalso easy to choice an adequate tag set from it tomeet the needs of other tasks.
(3) Relation with the Corpora of Other Languagesin NICTThe original data of the corpora are in Japanese orEnglish.
It is meaningful to build connections at themorphological level among these trilingual parallelcorpora at least for ?named entities.?
For example, weadopted the same segmentation units as in Japanese,and we subcategorized these words into personalnames, organization names, location names, and drinkand food names and assigned them each an individualtag.
Personal names were further divided into familynames and first names for Chinese, Japanese, andWestern names.
These subclasses are useful inlanguage modeling, especially in the travel domain.3.2.
Some Explanations on Segmentation andPOS-tagging(1) About SegmentationIn our definition of a segmentation unit, words longerthan 4 Hanzis (Chinese characters) were generallydivided into their syntactic units.
Idioms and somegreeting phrases were also regarded as segmentationunits.
For example: ???/?????/???/???
/.?
Semantic information was also used to judgesegmentation unit.
For example:?
?/ ?/ ?/ ?/ ?/ ?/ ?
?/ ?/ (Tell me the bestrestaurant around here.)?
?
?/ ?/ ?
?/ ?
?/ ?/ ?/ ?
?/ ?/ (I'd like ahotel that is not too expensive.
)For segmenting compound words with differentstructures, we constituted detailed items to deal withthem.
These structures include ?coordinated (??
)?modifying (??
), verb-object (??
), subject-predicate (??
), and verb-complement (??).?
Themain consideration for these was to divide themwithout changing the original meaning.
For thosewords that have a strong ability to combine withothers, we generally separated them from the others.This was due to the consideration that if it were done inanother way, it would result in too many words.
Forexample, in the verb-object (?? )
structure, ??(buy)?
can combine with many nouns to getmeaningful words or phrases, such as ???
(buybook), ??
(buy meat)???
(buy ticket)?and ???
(buy clothes).?
We prescribed separating suchactive characters or words, no matter how frequentlythey are used in the real world, to ensure that themeaning did not change and ambiguity did not arise.So the above phrases should be separated in followingforms: ?
?/ ?/ (buy book), ?/ ?/ (buy meat)?
?/ ?/(buy ticket)?and ?/ ??
/buy clothes).
?For the directional verbs, we generally separatedthem from their preceding verbs.
For example:?/ ?
?/ ?/ ?/ ?
?/ ?
?/ ?/ ?/ (Is it all right to moveto another seat?
)?/ ?/ ?/ ?/ ??
?/ ?
?/ ?/ ??
?/ ?/ (Please keepthis suitcase until one o'clock.
)Prefix and appendix were commonly separated fromthe root words.
For example:?
?/ ?/ ?/ ?/ ?
?/ ?/ ?/ (Are all students going toKyoto?
)?/ ?/ ?
?/ ?
?/ ?/.
(I do free-lance work.
)(2) About POS-TaggingThe POS tag sets are shown in Table 1 in the Appendix.The POS tagging was conducted by the grammarfunction based on how the segmentation unit behavesin a sentence.4.
Procedure of Developing the ChineseCorporaThe segmented and POS-tagged data were obtained intwo steps.
The first step was to get the raw segmentedand POS-tagged data automatically by computer.
Thesecond was to check the raw segmented and POS-tagged data manually.
(1) Getting Raw Segmented and POS-Tagged DataThe text data were segmented and POS tagged byusing the language model shown in formula (1).
)|()|()1()ww|(w)( 2-i1-iiii2-i1-ii cccPcwPPLP ??
?+=    (1)72Here iw  denotes the word at the ith position of asentence, and ic  stands for the class to which the wordiw  belongs.
The class we used here is a POS-tag set,and  ?
is set 0.9.The initial data for training the model were from theSinica due to their balanced characteristics.
Theannotated data were added to the training data whenproducing new data.
When the annotated data reacheda given quantity (here, the BTEC1 was finished, andthe total words in the corpora exceeded 1M), the Sinicadata were not used for training.
We have conducted anexperiment with this model for an open test text of 510utterances from BTEC, and the segmentation and POS-tagging accuracy was more than 95%.
Furthermore,proper noun information was extracted from Japanesecorpora and marked in the corresponding lines of theChinese segmented and POS-tagged data.
(2) Manual AnnotationThe manual annotations were divided into two phases.The first was a line-by-line check of the raw segmentedand POS-tagged data.
The second was to check theconsistency.
The consistency check was conducted inthe following manner:?
Find the candidates having differences between themanually checked data and the automaticallysegmented and POS-tagged data.?
Pick up the candidates having a high frequency ofupdating in the above step, and build aninconsistency table.
The candidates in this table arethe main objects of the later checks.?
Check the same sentences with differentsegmentations and POS tags.?
List all words having multiple POS tags and theirfrequencies.
Determine the infrequent ones asdistrustful candidates and add them into theinconsistency tables.The released annotated data were appended with aheader ID for each token (pair of word entry and POStag) in an utterance including a start marker and endmarker, shown as follows:BTEC1\jpn067\03870\zh\\\00010\||||UTT-START||||BTEC1\jpn067\03870\zh\\\00020\?|?||?|r||||BTEC1\jpn067\03870\zh\\\00030\?|?||?|vw||||BTEC1\jpn067\03870\zh\\\00040\?|?||?|v||||BTEC1\jpn067\03870\zh\\\00050\?|?||?|a||||BTEC1\jpn067\03870\zh\\\00060\??|??||?
?|n||||BTEC1\jpn067\03870\zh\\\00070\?||||UTT-END||||Table 2 shows some of the statistics for the 510Kutterances in Table 1 for different languages.Table 2.
Some Statistics of Each Corpora in NICTUtter.Ave.words/Uttr.Words Vocab.Chinese 510K 6.95 3.50M 47.3KJapanese 510K 8.60 4.30M 45.5KEnglish 510K 7.74 3.80M 32.9KFigure 1 shows the distributions of utterance length(words in an utterance) for 3 languages among the510K annotated data.
From Figure 1, we know that theChinese has the fewest words in an utterance, followedby English, with the Japanese having the most.Figure 1.
Distribution of utterance length5.
Evaluation ExperimentsTo verify the effectiveness of the developed Chinesetextual corpora, we built a language model for speechrecognition using these corpora.
For comparisons withother languages, including Japanese and English, wealso built language models for these two languagesusing the same training sets.
Meanwhile, the same testset of each language was selected for speechrecognition.5.1.
Data Sets for Language Models andSpeech RecognitionsFor simplicity, we adopted word 2-gram and word 3-gram for evaluating perplexities and speechrecognition performance.
The training data wereselected from the 510K utterances in Table 1, while thetest sets were also extracted from them, but they areguaranteed not to exist in the training sets.
Inevaluations of perplexity, 1524 utterances (a total ofthree sets) were chosen as the test set.
In evaluation ofrecognition, 510 utterances were chosen as test set.
ForJapanese and English, the same data sets were alsochosen for comparisons.Distribution of Utterance Length0%2%4%6%8%10%12%14%16%18%2 4 6 8 10 12 14>15length(words)PercentageJapaneseEnglishChinese73Figure 2.
Ratio of 2-gram items with low occurrence5.2.
Comparisons of Language ModelsUsing the above utterances in the training sets, a word2-gram and a 3-gram were built respectively for eachlanguage.
The distributions of items inside thesemodels were investigated.
Figure 2 shows the ratios of2-gram?s items which have low occurrences (from 1 to6) in the 2-gram model.Compared with the other two languages, the Chinesehas the biggest vocabulary.
Moreover, it also has alarge amount of low-frequency 1-gram, 2-gram, and 3-gram items.
For example, more than 60% of its 2-gramentries appear only once.
This can be regarded that theChinese has more varieties when expressing a samemeaning than the other two languages.
It is also partlydue to bias occurred in the translation process,compared to the original languages.
So the probabilitycomputations in 2 or 3-gram related to these entrieswere estimated by using a smoothing approach, so theaccuracy is not high.Table 3 shows average sentence entropies (ASE) ofthe test sets to the 3-gram models.
The ASE is obtainedas follows: (1) first to get the  product of average wordentropy and the total word count in test set.
(2) thendivide the product by the total sentences in the test set.From the table, we know the Chinese has the maximalsentence entropy (or maximal perplexity) among thethree languages.
This means that when predicate asentence in the recognition process, Chinese requires amuch bigger search space than the other two languages.Table 3.
Average Sentence Entropy of the Test Sets to 3-gram ModelsChinese Japanese EnglishVocab.
of Test Set 10,030 12,344 10,840Ave.
Sen. Entropy  294.58 165.80 202.92Word Perplexity 45.0 20.1 28.55.3.
Comparison of Speech RecognitionPerformancesFigure 3.
Word recognition accuracies of 3 languagesThe 2-gram language model was used for decodingrecognition lattice, while the 3-gram model was usedfor rescoring process.
The recognition results areshown in  Figure 3.
Here, WordID  refers to the word?souter layer (entry) together with its POS tag, otherinformation like conjugation of verbs, declension ofnouns, etc., while the surface word contains only itsouter layer, no POS tag is contained  in this case .The difference in word accuracy of speechrecognition between these two forms is about 2% forChinese, and 1% for English and Japanese.6.
SummaryThis paper described the development of Chineseconversational segmented and POS-tagged corpora thatare used for spontaneous speech recognition in S2STsystem.
While referring mainly to the PKU?sspecifications, we defined ours by taking into accountthe needs of S2ST.
About 510K utterances, or about3.5M words of conversational Chinese data, arecontained in these corpora.
As far as we know, they arepresently the biggest ones in the domain of travel, witha style of conversations.
Moreover, a parallel corpuswas obtained using these 510K pairs of utterances ofChinese, Japanese, and English.
These corpora nowplay a big role in spontaneous language and speechprocessing, and are used in the NICT/ATR Chinese-Japanese-English Speech-to-Speech TranslationSystem [8] and other communication services.However, according to our evaluations in this paper,there are still some difference in performance amongChinese and other languages, especially Japanese.There is still some room to improve the quality of thesecorpora mainly because the Chinese text data weretranslated from other languages, mainly Japanese, witha few words from English.
There is some bias inexpression, especially for the transliterations of propernouns.
For examples, ?Los Angles?
is translated as ????,???,??
?, and ???.
?also, someutterances are not like those spoken by native speakers,0102030405060701 2 3 4 5 6ChineseJapaneseEnglishRatio of 2-gram Item [%]occurrence8187.9693.8782.8989.2994.67707580859095100Chinese English JapaneseWordAccuracyWord Accuracy for Each LanguageWordIDSurface Word74like sentence of ?????????
?
whichcorresponds to the original sentence of  ??????????
(I appreciate your kindness).
?For future work, while continuing to improve theconsistency of the corpora, we will expand the Chinesecorpora from external data resource, such as Web sitesand LDC databases, to extract original Chinesespontaneous text data.7.
References[1] H.M. Duan, J.
Song, G.W.
Xu, G.X.
Hu and S.W.
Yu,?The Development of a Large-scale Tagged ChineseCorpus and Its Applications.?
http://icl.pku.edu.cn/icl_tr[2] C.R.
Huang, and K.J.
Chen, ?Introduction to SinicaCorpus,?
CKIP Technical Report 95-02/98-04,http://www.sinica.edu.tw/SinicaCorpus[3] G. Kikui, E. Sumita, T. Takezawa, S. Yamamoto,?Creating Corpora for Speech-to-Speech Translation.?
8thEuropean Conference on Speech Communication andTechnology, Vol.1, pp.381-384, Sep., 2003[4] S.W.
Yu, X.F.
Zhu, and H.M. Duan, ?The Guideline forSegmentation and Part-Of-Speech Tagging on Very LargeScale Corpus of Contemporary Chinese.
?http://icl.pku.edu.cn/icl_tr[5] The National Standard of PRC, ?Standardization ofSegmentation for Contemporary Chinese.?
GB13715,1992.
[6] Institute of Applied Linguistics of the Ministry ofEducation, China, ?Specification on Part-of-SpeechTagging of Contemporary Chinese for InformationProcessing (Draft).?
2002.
[7] H. Yamamoto, S. Isogai, and Y. Sagisaka, ?Multi-classComposite N-gram Language Model,?
SpeechCommunication, 2003, Vol.41, pp369-379.
[8] T. Shimizu, Y. Ashikari, E. Sumita, J.S.
Zhang, S.Nakamura, ?NICT/ATR Chinese-Japanese-EnglishSpeech-to-Speech Translation System.?
Tsinghua Scienceand Technology, Vol.13, No.4, pp540-544, Aug. 2008.Appendix Table 1.
Chinese POS Tag TablePOS TagDescriptionPOS TagDescriptionChinese English Chinese Englisha ???
Adjectivennppx ?????
Chinese  family nameb ???
Non-predicate  adjective nppm ????
?Chinesefirst namec ??
Conjunction nppxj ?????
Japanese  family named ??
Adverb nppmj ?????
Japanese  first namede ????
Attributive nppxw ???????
Western  family namee ??
Interjection nppmw ???????
Western  first nameg ???
Morpheme Word npl ??
Placeh ???
Prefix npo ???
Organizationi??,???
Idiom npfd ????
Drink and foodj ???
Abbreviation o ???
Onomatopoeiak ???
Suffix p ??
Prepositionmm ??
Numeral q ??
Quantifierma ????
Numeral Classifier r ??
Pronounmb ???
Approximate  numeral u ??
Auxiliarynn ????
Nounvv ????
Verbnd ???
Directional locality v1 ??????
?Auxiliaryverbns ???
Space word v2 ?????
Verb ?Have?nt ???
Time word vt ????
Directional verbnx???,?
?Numeric,character string vw ????
Modal verbnp ????
Proper noun w ????
Punctuationnpp ??
Personal name y ????
Modal particle75
