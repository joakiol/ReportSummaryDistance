EXPERIMENTS AND PROSPECTS OFEXAMPLE-BASED MACHINE TRANSLATIONEi ich i ro  SUMITA*andHitoshi  HDAATR Interpreting Telephony Research LaboratoriesSanpeidani, Inuidani, Seika-choSouraku-gun, Kyoto 619-02, JAPANABSTRACTEBMT (Example-Based Machine Translation)is proposed.
EBMT retrieves imilar examples(pairs of  source phrases, sentences, ortexts and their translations) from a d~t.hase ofexamples, adapting the examples to translate anewinput.
EBMT has the following features: (1) It iseasily upgraded simply by inputting appropriateexamples to the database; (2) It assigns areliabilityfactcr to the translation result; (3) It is acoeleratedeffectively by both indexing and parallel computing;(4) It is robust because of best-match reasoning; ~d(5) It well utilizes translator expertise.
A prototypesystem has been implemented to deal with a difficulttranslation problem for conventional Rule-BasedMachine Translation (RBMT), i.e., translatingJapanese noun phrases of the form "N~ no N2" intoEnglish.
The system has achieved about a 78%success rate on average.
This paper explains the basicidea of EBMT, illustrates the experiment in detail,explains the broad applicability of EBMT to severaldifficult ranslation problems for RBMT anddiscusses the advantages of integrating EBMT withRBMT.1 INTRODUCTIONMachine Translation requires handcmt~ andcomplicated large-scale knowledge (Nirenburg 1987).Conventional machine translation systems userules as the knowledge.
This framework iscalled Rule-Based Machine Translation(RBMT).
It is difficult o scale up from a toyprogram to a practical system because of the problemof building such a lurge-scale rule-base.
It is alsodifficult o improve translation performance becausethe effect of adding anew rule is hard to anticipate,and because translation using a large-scule rule-basedsystem is time-consuming.
Moreover, it is difficultto make use of situational or domain-specificinformation for translation.their translations) has been implemented asthe knowledge (Nagao 1984; Sumita ndTsutsumi 1988; Sato and Nagao 1989; Sadler 1989a;Sumita et al 1990a, b).
The translation mechanismretrieves similar examples from the database,adapting the examples to Wanslate the new sourcetext.
This framework iscalled Example-BasedMachine Translation (EBMT).This paper focuses on ATR's linguisticdatabase ofspoken Japanese with Englishtranslations.
The corpus contains conversations aboutinternational conference r gistration (Ogura et al1989).
Results of this study indicate that EBMT is abreakthrough in MT technology.Our pilot EBMT system translates Japanesenoun phrases of the form '~1 xno N2" into Englishnoun phrases.
About a 78% success rate onaverage has been achieved in theexperiment, which i s considered tooutperform RBMT.
This rate cm be improved asdiscussed below.Section 2 explains the basic idea of EBMT.Section 3 discusses the broad applicability of EBMTand the advantages of integrating it with RBMT.Sections 4 and 5 give a rationale for section 3, i.e.,section 4 illustrates the experiment of translatingnoun phrases of the form "Nt no N2" in detail, andsection 5 studies other phenomena through actualdam from our corpus.
Section 6 concludes this paperwith detailed comparisons between RBMT andEBMT.2 BAS IC  IDEA OF  EBMT2.1 BAS IC  FLOWIn this section, the basic idea of EBMT,which is general and applicable to many phenomenadealt with by machine translation, is shown.In order to conquer these problems in machinetranslation, a database of examples (pairs ofsource phrases, sentences, or texts and* Currently with Kyoto UniversityFigure 1 shows the basic flow of EBMTusing translation of "kireru"\[cut/be sharp\].
Fromhere on, the literal English translations arebracketed.
(1) and (2) me examples (pairs ofJapanese sentences and their English185translations) in the database.Examples similar to the Japaneseinput sentence are retrieved in the followingmanner.
Syntactically, the input is similar toJapanese sentences (1) and (2).
However,semantically, "kachou" \[chief\] is far from "houchou"\[kitchen knife\].
But, "kachou" \[chief\] is semanticallysimilar to "kanojo" \[she\] in that both are people.
Inother words, the input is similar to example sentence(2).
By mimicking the similar example (2), wefinally get "The chief is sharp".Although it is possible to obtain the sameresult by a word selection rule using fme-tunedsemantic restriction, ote that ranslation here isobtained by retrieving similar examples to the input.?
Example Database(data for "kireru'\[cut / be sharp\])(1) houchou wa k l rsru -> The kitchen knife cuts.
(2) kanojo wa k i reru -> She Is sharp.?
Inputkachouwa k l reru o>??
Retrieval of similar examples(Syntax) Input = (1), (2)(Semantics) kachou/== houehoukachou ,= kanojo(Total) Input == (2)?
OUt0Ut -> The chief Is ~ h a r D,Figure I Mimicking Similar Examples2.2 DISTANCERetrieving similar examples to the input isdone by measuring the distance of the input toeach of examples.
The smaller adistance is, themore similar the example is to the input.
To definethe best distance metric is a problem of EBMT notyet completely solved.
However, one possibledefinition is shown in section 4.2.2.From similar examples retrieved, EBMTgenerates the most likely translation with areliability factor based on distance and frequency.
Ifthere is no similar example within the giventhreshold, EBMT tells the user that it cannottranslate the input.3 BROAD APPLICABILITY ANDINTEGRATION3.1 BROAD APPLICABILITYEBMT is applicable to many linguisticphenomena that are regarded as difficult o translate inconventional RBMT.
Some are well-known amongresearchers ofnatural language processing and othershave recently been given a great deal of attention.When one of the following conditions holdstrue for a linguistic phenomenon, RBMT is lesssuitable than EBMT.
(Ca) Translation rule formation isdifficult.
(Cb) The general rule cannot accuratelydescribe phenomena because itrepresents a special case, e.g., idioms.
(Cc) Translation cannot be made in acompositional way from target words(Nagao 1984; Nitta 1986; Sadler 1989b).This is a list (not exhaustive) of phenomenain J-E translation that are suitable for EBMT:?
optional cases with a case particle( "- de", "~ hi",...)?
subordinate conjunction ("- ba -", "~ nagara -","~ tara -",...,"- baai ~",...)?
noun phrases of the form '~1 no N2"?
sentences of the form "N~ wa N 2 da"?
sentences lacking the main verb (eg.
sentences ofthe form "~ o-negaishimasu")?
fragmental expressions Chai", "sou-desu","wakarimashita",...) (Furuse t al.
1990)?
modality represented bythe sentence endingC-tainodesuga", "~seteitadakimasu", ...)(Furuse t al.
1990)?
simple sentences (Sato and Nagao 1989)This paper discusses a detailed experiment for"N~ no N2" in section 4 and prospects for otherphenomena, "N1 wa N2 da" and "~ o-negaishimasu"in section 5.Similar phenomena in other languagepairs can be found.
For example, in Spanish toEnglish translation, the Spanish preposition "de",with its broad usage like Japanese "no", is alsoeffectively Iranslated by EBMT.
Likewise, in Germanto English translation, the German complex noun isalso effectively translated by EBMT.3.2 INTEGRATIONIt is not yet clear whether EBMT can orshould eal with the whole process of translation.
Weassume that here are many kinds of phenomena.Some are suitable for EBMT, while others aresuitable for RBMT.Integrating EBMT with RBMT i sexpected to be useful.
It would be moreacceptable for users if RBMT were first introduced asa base system, and then incrementally have itstranslation performance improved by attachingEBMT components.
This is in the line with theproposal in Nagao (1984).
Subsequently, weproposed a practical method of integration i186previous papers (Sumita et al 1990a, b).4 EBMT FOR "N  x no  Nz"4.1 THE PROBLEM"N~ no N2" is a common Japanese nounphrase form.
"no" in the "Nt no Nz" is a Japaneseadnominal particle.
There are other variants,including "deno", "karano", "madeno" and so on.Roughly speaking, Japanese noun phrases ofthe form "N~ no N2" correspond to English nounphrases of the form "N2 of N:" as shown in theexamples at the top of Figure 2.Japanese Englishyouka n o gogo the afternoon o f the 8thkaigi no mokuteki the object o f the conference.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.kaigi n o sankaryou the application fee for the conf.
?the application fee o fthe conf.kyoutodenokaigi theconf, in Kyoto.
'/the conf.
o f Kyotoisshukan no kyuka a week' s holiday?the holiday o f a weekmittsu no hoteru three hotels*hotels o fthreeFigure 2 Variations in Translation of "N1 no N2"However, "N2 of Nt" does not always providea natural translation as shown in the lower examplesin Figure 2.
Some translations are too broad inmeaning to interpret, others axe almostungrammatical.
For example, the fourth one, "theconference of Kyoto", could be misconstrued as "theconference about Kyoto", and the last one, "hotels ofthree", is not English.
Natural translations oftenrequire prepositions other than "of", or nopreposition at all.
In only about one-fifth of "N~ noN2" occurrences in our domain, "N2 of Nt" would bethe most appropriate English translation.
We cannotuse any particular preposition as an effecdve de.faultvalue.No rules for selecting the most appropriatetranslation for "N~ no N2" have yet been found.
Inother words, the condition (Ca) in section 3.1 holds.Selecting the translation for '~1~ no N2" is still animportant and complicated problem in J-Etranslation.In contrast with the preceding researchanalyzing "NI no N2" (Shimazu et al 1987; Hirai andKitahashi 1986), deep semantic analysis is avoidedbecause it is assumed that translations appropriatefor given domain can be obtained usingdomain-specific examples (pairs of source md targetexpressions).
EBMT has the advantage that it candirectly return a translation by adapting exampleswithout reasoning through a long chain of rules.4.2 IMPLEMENTATION4.2.1 OVERVIEWThe EBMT system consists of two databases:an example database and a thesaurus; and also threetranslation modules: analysis, example-based transfer,and generation (Figure 3).Examples (pairs of  source phrasesand their translations) are extracted from ATR'slinguistic database of spoken Japanese with Englishtranslations.
The corpus contains conversations aboutregistering for an international conference (Ogura1989).ExampleDatabase(1) Analysis I(2) Example-BasedTransferThesaurusI (3) Generation IFigure 3 System ConfigurationThe thesaurus is used in calculatingthe semantic distance between the contentwords in the input and those in theexamples.
It is composed of a hierarchical structurein accordance with the thesaurus ofeveryday Japanesewritten by Ohno and Hamanishi (1984).Analysiskyouto deno kaigiExample-Based Transferd Japanese English0.4 toukyou deno taizai the stay in Tokyo0.4 honkon deno taizai the stay in Hongkong0.4 toukyou deno go-taizai the stay in Tokyo1.0 oosaka no kaigi the conf.
in Osaka1.0 toukyou no kaigi the conf.
in TokyoGenerationthe conf.
in KyotoFigure 4 Translation ProcedureFigure 4 illustrates the translation procedurewith an actual sample.
First, morphological nalysisis performed for the input phrase,"kyouto\[Kyoto\]deno kaigi \[conference\]".
In this case, syntactical187analysis is not necessary.
Second, similar examplesare retrieved from the database.
The top five similarexamples are shown.
Note that he top threeexamples have the same distance and that hey are alltranslated with "in".
Third, using this rationale,EBMT generates "the conference inKyoto".4.2.2 D ISTANCE CALCULAT IONThe distance metric used when retrievingexamples i essential nd is explained hem in detail.we suppose that the input and examples (I, E)in the d~tAl~ase ~ r~ted  in the same datastructure, i.e., the list of words' syntactic andsemantic attribute values (refeaxed toas and I~, E~) foreach phrase.The attributes of the current arget, "Nt noN2" , 8~ as follows: 1) for the nouns "NI" and "N2":the lexical subcategory of the noun, the existence ofa prefix or suffix, and its semantic code in thethesaurus; 2) for the adnominal particle "no": thekinds of variants, "deno", "karano", "madeno" and soon.
Here, for simplicity, only the semantic code andthe kind of adnominal a=e considered.Distances ae calculated using the followingtwo expressions (Sumita et al 1990a, b):(1) d(I,E)=d(li,Ei) "w ii(2) wi=,~// ~.
( freq.
of t. p. when Ei=li ) 2t .p.The attribute distance, d(li, E.~ end the weightof attribute, w~ are explained in the followingsections.
Each Iranslation pattern (t.p.)
is abstractedfrom an example md is stored with the example inthe example d~mhase \[see Figure 6\].
(a) ATTRIBUTE DISTANCEFor the attribute of the adnominal particle"no", the distance is 0 or 1 depending on whether ornot they match exactly, for example,d("deno","deno") = 0 and d("deno", no") = 1.For semantic attributes, however, the distancevaries between 0 and 1.
Semantic distance d(0 <d < 1)is determined by the Most SpecificCommon Abstractlon(MSCA) (Kolodner andRiesbeck 1989) obtained from the thesaurusabstraction hierarchy.
When the thesaurus i  (n+l)layered, (k/n) is assigned to the concepts in the k-thlayer from the bottom.
For example, as shown withthe broken line in Figure 5, the MSCACkaigi ''\[conference\], "taizai" \[stay\]) is "koudou" \[actions\] andthe distance is2/3.
Of course, 0 is assigned when theMSCA is the bottom class, for instance,MSCACkyouto"\[Kyoto\], "toukyou" \[Tokyo\])="timei"\[placc\], or when nouns are identical (MSCA(N, N) for any N).Thesaurus Root\[actions\](1/3)ouralomingsgoings\]setsumeitions\](o)Ikaisetsu\[commen-tary\]/ / .
, .  "
, \\[ taizai I I hatchakuI \[stays\] II \[arrivals & \[meetingslJJ J Jdepartures', II :o), i l i  ikaigi taizai touchaku\[conference\] \[stay\] \[arrive\]Figure 5 Thesaurus(portion)(b) WEIGHT OF ATTRIBUTEThe weight of  the attribute is thedegree to which the attribute influencesthe selection of the translationpattern(t.p.).
We adopt the expression (2) usedby Stanfill and Waltz (1986) for memory-basedreasoning, to implement the intuition.t.p.
freq.B in A 12/27AB 4/27B from A 2/27BA 2/27BtoA 1/27(E l=timei)\[place/t.p.
freq.B in A 313(E2=deno)\[in/t.p.
freq.B 9/24AB 9/24B in A 2/24A's B 1124BonA 1/24(E3=soudan)\[meetings\]Figure 6 Weight of the i-th attribute188In Figure 6, all the examples whose E2 ="deno" aze translated with the same preposition,"in".
This implies that when El= "deno", E2 is anattribute which heavily influences the selection of thetranslation pattern.
In contrast to this, the translationpatterns of examples whose E1 = "timei"\[place\], =evaried.
This implies that when E1 -- "timei"\[place\],E~is an attribute which is less influential on theselection of the translation pattern.According to the expression (2), weights forattributes, E~, E2 and E3me as follows:W1=,~(12/27) 2+(4127 ) 2+...+(1/27)2 = 0.49W2=,,~(3/3) 2 = 1.0w3=,~(9/24 ) 2+(9124 ) 2+.
..+(1/24) 2 ,= 0.54(C) TOTAL DISTANCEThe distance between the input and the firstexample shown in Figure 4 is calculated using theweights in section 4.2.2 Co), attribute distances asexplained in section 4.2.2 (a) and expression (1) atthe beginning of section 4.2.2.d( "kyouto'\[Kyoto\] "deno'\[in\] "kaigi'\[ conference\],"toukyou'\[Tokyo\] "deno'\[in\] "taizai'\[stay\]),= d('kyouto','toukyou" )*0.49+d('deno",'deno')*1.0+d('kaigi", "taizai')*0.54= 0"0.49+0"1.0+2/3"0.54 = 0.44.3 EXPERIMENTSThe current number of words in the corpus isabout 300,000 and the number of examples i 2,550.The collection of examples from another domain isin progress.4.3.1 JACKKNIFE TESTIn ~ to roughly estimate translationperformance, a jackknife experiment was conducted.We partitioned the example database(2,550) in groupsof one hundred, then used one set as input(100) andtranslated them with the rest as an example database(2,450).
This was repeated 25 times.Figure 7 shows that the average successrate is 78%, the minimum 70% and themaximum 89% \[see section 4.3.4\].It is difficult to fairly compare this resultwith the success rate of the existing MT system.However, it is believed that current conventionalsystems can at best output the most commontranslation pattern, for example, "B of A", as thedefault.
In this case, the average success rate mayonly be about 20%.success(%) MAXIMUM(89%)10080 ~ ~ _  ,..60 AVERAGE(78%)MINIMUM(70%)0 I I1 11 21test numberFigure 7 Result of Jackknife Test40204.3.2 SUCCESS RATE PERNUMBER OF EXAMPLESFigure 8 shows the relationship between thesuccess rate and the number of examples.
Of thetwenty-five cases in the previous jackknife test, threeare shown: maximum, average, and minimum.
Thisgraph shows that, in general, the more exampleswe have, the better the quality \[see section4.3.4\].success(%) MAXIMUM80 t| / J~~~,~'~'~ ' ' - - / - -  .,, ~s  AVERAGE70 .
_ .
- .
.
- - - . '
' ' - - -501 11 21no.
of examples (x 100)Figure 8 Success Rate per No.
of Examples1894.3.3 SUCCESS RATE PERDISTANCEFigure 9 shows the relationship between thesuccess rate and the distance between the input andthe most similar examples retrieved.This graph shows that in general, thesmaller the distance, the better the quality.In other words, EBMT assigns the distance betweenthe input and the retrieved examples us a reliabilityfactor.SUCCESS0.9 r 1592/17900.80.70.60.50.40.30.20.1023137 100 / 169 ?
19/33?
= ?
35167951162 ?74/148 8 /247/143/56E ?
I I I I I0 0.2 0.4 0.6 0.8 1distanceFigure 9 Success Rate per Distance4.3.4 SUCCESSES AND FAILURESThe following represents successful results:(1) the noun phrase "kyouto-eki \[Kyoto-station\] noo-mise \[store\]" is wansta_!ed according to thetranslation pattern "B at A" while the similar nounphrase, "kyouto\[Kyoto\] no shiten \[branch\]" istranslated according to the translation pattern "13 inA"; (2) the noun phrase of the form "N~ no hou" istranslated according to the translation pattern "A", inother words, the second noun is omitted.We ~e now studying the results carefully ~dare striving to improve the success rate.
(a) About half of the failures are caused by a lack ofsimilar examples.
They are easily solved by addingappropriate examples.Co) The rest are caused by the existence of similarexamples: (1) equivalent but different examples areretrieved, for instance, those of the form, "B of A"and "AB" for "rolm-gatsu \[June\] no futsu-ka\[second\]".
This is one of the main reasons the graphs(Figure 7 and 8) show an up-and-down pattern.
Theycan be regarded as a correct translation or the distancecalculation may be changed to handle the problem;(2) Because the current distance calculation isinadequate, dissimilar examples are retrieved.5 PHENOMENA OTHER THAN"N 1 no Nz"This section studies the phenomena, "N1 waN2 da" and "- o-negaishimasu" with the same corpusused in the previous ection.5.1  "N x wa N~ da"A sentence of the form "N\] wa N2 da" iscalled a "da" sentence.
Here "N{' and '~2" ~e nouns,"wa" is a topical particle, and "da" is a kind of verbwhich, roughly speaking, is the English copula "be".The correspondences between "da" sentencesand the English equivalents are exemplified in Figure10.
Mainly, "N~ wa N2 da" corresvonds to '~  be Nz"like (a-l) - (a-4).However, sentences like (b) - (e) cannot betranslated according to the translation pattern ,N~ beN2".
In example (d), there is no Japanese counterpartof "payment should be made- by".
The Englishsentence has a modal, passive voice, the verb make,and its object, payment, while the Japanese sentencehas no such correspondences.
This translation cannotbe made in a compositional way from the targetwords which ale selected from a normal dictionary.
Itis difficult o formulate rules for the translation andto explain how the translation is made.
Theconditions (Ca) and (Co) in section 3.1 hold true.Conventional pproaches lead to theunderstanding of"da" sentences u ing contextual ndexwa-linguistic information.
However, manytranslations exist that are the result of humantranslators' understanding.
Translation can be madeby mimicking such similar examples.Example (e) is special, i.e., idiomatic.
Thecondition (Co) in section 3.1 holds.
(a)  NI be N=watashi\[I\]kochira\[this\]denwa-bango\[tel-no.\]sanka-hi\[fee\](b) N, cost  N=yokoushuu\[proc.\]N,jonson\[Johnson\]jim ukyoku\[secretariat\]06-951-0866106-951-0866\]85,000-en\[85,000 yen\]30,000-en\[30,000 yen\](c)  for N,, the fee is N=kigyou\[companies\] 85,000-en\[85,000 yen\](d) payment should be made by N=hiyou\[fee\] ginnkou-furikomi\[bank-transfer\](e)  the conference will end on N=saishuu-bi\[final day\] 10qatsu12nichi\[Oct. 12th\]Figure 1 0 Examples of "N1 wa N2da"The distribution of N\] and N2 in the examples190of our corpus vary for each case.
Attention should begiven to 2-tuples of nouns, (N1, N2).
N2s of (a-4), (13)and (c) are similar, i.e., both mean "prices".
HoweverN~s are not similar to each other.
Nls of (a-4) and (d)~e similar, i.e., both mean "fee".
However, the N2s~e not similar to each other.
Thus, EBMT isapplicable.5.2 "~ o -nega ish imasu"Figure 11 exemplifies the conespondencesbetween sentences of the form "~ o-negaishimasu"and the English equivalents.
(a) may I speak to N(b) please give me N(c) please pay by N(d) yes, please(e) thank YouFigure 11jim ukyoku\[secretariat\] oo-negaishlmasugo-ju usyo\[add ress\] o...genkin\[cash\] de...hal...voroshiku...Examples of "~ o-negaishimasu"Translations in examples (b) and (c) arepossible by finding substitutes in Japanese for giveme and pay by, respectively.
The conditions (Ca)and (Cc) in section 3.1 hold.
Usually, this kind ofsupplement is done by contextual nalysis.
However,the connection between the missing elements and thenoun in the examples i strong enough to reuse,because it is the product of a combination oftranslator expertise and domain specific restriction.Examples (a), (d) and (e) are idiomatic expressions.The condition (Cb) holds.
The distribution of thenoun and the particle in the examples of our corpusvaries for each case in the same way as in the "da"sentence.
EBMT is applicable.6 CONCLUDING REMARKSExample-Based Machine Translation (EBMT)has been proposed.
EBMT retrieves similar examples(pairs of source and target expressions), adaptingthem to translate a new source text.The feasibility of EBMT has been shown byimplementing a system which translates Japanesenoun phrases of the form '~1 no N2" into Englishnoun phrases.
The result of the experiment wasencouraging.
Bnaed applicability of EBMT wasshown by studying the d~m from the text corpus.
Theadvantages of integrating EBMT with RBMT werealso discussed.
The system has been written inCommon Lisp, and is running on a Genera 7.2Symbolics Lisp Machine at ATR.
(1) IMPROVEMENTThe more elaborate the RBMT becomes, theless expandable it is.
Considerably complex rulesconcerning semantics, context, and the real world, arerequired in machine translation.
This is the notoriousAI bottleneck: not only is it difficult o add a newrule to the database of rules that are mutuallydependent, but it is also difficult o build such a ruledatabase itself.
Moreover, computation using thishuge and complex rule database is so slow that itforces a developer to abandon efforts to improve thesystem.
RBMT is not easily upgraded.However, EBMT has no rules, and the use ofexamples i relatively localized.
Improvement iseffected simply by inputting appropriate examplesinto the database.
EBMT is easily upgraded, whichthe experiment in section 4.3.2 has shown: themore examples we have, the better thequality.
(2) REL IAB IL ITY  FACTOROne of the main reasons users dislike RBMTsystems i the so-called "poisoned cookie" problem.RBMT has no device to compute the reliability ofthe result.
In other words, users of RBMT cannottrust any RBMT translation, because itmay bewrong without any such indication from system.Consider the case where all translation processes havebeen completed successfully, et, the result isincorrect.In EBMT, a rel iabi l i ty factor isassigned to the translation result accordingto the distance between the input and thesimilar examples found \[see the experiment insection 4.3.3\].
In addition to this, retrieved examplesthat are similar to the input convince users that thetranslation isaccurate.
(3) TRANSLAT ION SPEEDRBMT translates slowly in general because itis really a large-scale rule-based system, whichconsists of analysis, transfer, and generation modulesusing syntactic rules, semantic restrictions, tructuraltransfer rules, word selections, generation rules, andso on.
For example, the Mu system has about 2,000rewriting and word selection rules for about 70,000lexical items (Nagao et al 1986).As recently pointed out (Furuse t al.
1990),conventional RBMT systems have been biasedtoward syntactic, semantic, and contextual nalysis,which consumes considerable computing time.However, such deep analysis is not always necessaryor useful for translation.In contrast with this, deep semantic analysisis avoided in EBMT because it is assumed thattranslations appropriate for given domaincan be obtained using domain-specificexamples (pairs of source and targetexpressions).
EBMT directly returns a translationwithout reasoning through a long chain of rules \[see191sections 2 and 4\].There is fear that retrieval from a large-scaleexample database will prove too slow.
However, itcan be accelerated effectively by bothindexing (Sumita nd Tsutsumi 1988) andparallel computing (Sumita nd Iida 1991).These processes multiply acceleration.
Consequently,the computation ofEBMT is acceptably efficient.
(4) ROBUSTNESSRBMT works on exact-match reasoning.
Itfails to translate when it has no knowledge thatmatches the input exactly.However, EBMT works on best-matchreasoning.
It intrinsically translates in a fail-safe way\[see sections 2 and 4\].
(5) TRANSLATORS EXPERTISEFormulating linguistic rules for RBMT is adifficult job and requires a linguistically trained staff.Moreover, linguistics does not deal with allphenomena occurring in real text (Nagao 1988).However, examples necessary for EBMT ~Eeeasy to obtain because a large number of texts andtheir translations are available.
These are realizationof translator expertise, which deals with all realphenomena.
Moreover, as electronic publishingincreases, more and more texts will bemachine-readable (Sadler 1989b).EBMT is intrinsically biased toward asublanguage: strictly speaking, toward an exampledatabase.
This is a good feature because itprovides away of automatically tuning itself to asublanguage.REFERENCESFuruse~ O., Sumita, E. and Iida, H. 1990 "A Method forRealizing Transfer-Driven Machine Translation",Reprint of W(~L 80-8, IPSJ, (in Japanese).Hirei, M. and Kitahashi, T. 1986, "A SemanticClassification of Noun Modifications in JapaneseSentences and Their Analysis", Reprint of WGNL58-1, IPSJ, (in Japanese).Kolodner, J. and Riesbeek, C. 1989 "Case-BasedReasoning", Tutorial Textbook of 11 th UCAI.Nagao, M. 1984 "A Framework of a MechanicalTranslation Between Japanese and English byAnalogy Principle", in A. Elithom and R.
Banerji(ed.
), Artificial and Human Intelligence,North-Holland, 173-180.Nagao, M. ,Tsujii, J. , Nakamura, J.
1986 "MachineTranslation from Japanese into English",Proceedings of the IFI~.F., 74, 7.Nagao, M.(chair) 1988 "Language Engineering : TheReal Bottleneck of Natural Language Processing",Proceedings of the 12th International Conference onComputational Linguistics.Nirenburg, S. 1987 Machine Translation, CambridgeUniversity Press, 350.Nitta, Y.
1986 'Idiosyncratic Gap: A Tough Problem toStructure-bound Machine Translation", Proceedingsof the 11th International Conference onComputational Linguistics, 107-111.Ogura, K., Hashimoto, K .
,  and Morimoto, T. 1989"Object-Oriented User Interface for LinguisticDatabase", Proceedings of Working Conference onData and Knowledge Base Integration, University ofKeele, England.Ohno, S. and Hamanishi, M. 1984 Ruigo-Shin-Jiten,Kadokawa, 93 2, (in Japanese).Sadler, V. 1989a ''Translating with a SimulatedBilingual Knowledge Bank(BKB)", BSO/Research.Sadler.
V. 1989b Working with Analogical Semantics,Foris Publications, 25 6.Sato, S. and Nagao, M. 1989 "Memory-BasedTranslation", Reprint of WGNL 70-9, IPSJ, (inJapanese).Sato, S. and Nagao, M. 1990 "Toward Memory-BasedTranslation", Proceedings of the 13th InternationalConference o n Computational Linguistics.Shimazu, A. , Naito, S. , and Nomura, H. 1987"Semantic Structure Analysis of Japanese NounPhrases with Adnominal Particles", Proceedings ofthe 25th Annual Meeting of the Association forComputational Linguistics, 123-130.Stanf'dl, C. and Waltz, D. 1986 'Toward Memory-BasedReasoning", CACM, 29-12, 1213-1228.Sumita, E. and Tsutsumi, Y.
1988 "A Translation AidSystem Using Flexible Text Retrieval Based onSyntax-Matching", Proceedings of The SecondInternational Conference on Theoretical andMethodological Issues in Machine Translation ofNaturalLanguages, CMU, Pittsburgh.Sumita, E., Iida, H. and Kohyama, H. 1990a'~l'ranslating with Examples: A New Approach toMachine Translation", Proceedings of The ThirdInternational Conference on Theoretical andMethodological Issues in Machine Translation ofNaturalLanguages, Texas, 203-212.Sumita, E. /ida, H. and Kohyama, H. 1990b"Example-based Approach in Machine Translation",Proceedings of lnfoJapan "90, Part 2: 65-72.Sumita, E. and Iida, H. 1991 "Acceleration ofExample-Based Machine Translation", (manuscript).192
