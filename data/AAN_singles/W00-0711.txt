In: Proceedings of CoNLL-2000 and LLL-2000, pages 61-66, Lisbon, Portugal, 2000.Modeling the Effect of Cross-Language Ambiguity onHuman Syntax AcquisitionWill iam Gregory  SakasDepartment  of Computer  ScienceHunter College and The Graduate CenterCity University of New YorkNew York, NY 10021sakasOhunter ,  cuny.
eduAbst rac tA computational framework is presented whichis used to model the process by which humanlanguage learners acquire the syntactic ompo-nent of their native language.
The focus is fea-sibility - -  is acquisition possible within a rea-sonable amount of time and/or with a reason-able amount of work?
The approach abstractsaway from specific linguistic descriptions in or-der to make a 'broad-stroke' prediction of anacquisition model's behavior by formalizing fac-tors that contribute to cross-linguistic ambigu-ity.
Discussion centers around an applicationto Fodor's Structural Trigger's Learner (STL)(1998) 1 and concludes with the proposal thatsuccessful computational modeling requires aparallel psycholinguistic investigation ofthe dis-tribution of ambiguity across the domain of hu-man languages.1 Principles and ParametersChomsky (1981) (and elsewhere) has proposedthat all natural languages hare the same in-nate universal principles (Universal Grammar- -  UG) and differ only with respect o the set-tings of a finite number of parameters.
The syn-tactic component of a grammar in the principlesand parameters (henceforth P&P) framework,is simply a collection of parameter values - -  onevalue per parameter.
(Standardly, two valuesare available per parameter.)
The set of human1The STL is an acquisition model in the principlesand parameters paradigm.
The results presented hereare not intended to forward an argument for or againstthe model, or for that matter, for or against the prin-ciples and parameters paradigm.
Rather, the resultsaxe presented to point out (the possibly not-so- earth-shattering observation) that the acquisition mechanismcan  be extremely sensitive to ambiguity.grammars i the set of all possible combinationsof parameter values (and lexicon).The P&P framework was motivated to a largedegree by psycholinguistic data demonstratingthe extreme fficiency of human language acqui-sition.
Children acquire the grammar of theirnative language at an early age - -  generally ac-cepted to be in the neighborhood of five yearsold.
In the P&P framework, even if the lin-guistic theory delineates over a billion possiblegrammars, a learner need only determine thecorrect 30 values that correspond to the gram-mar that generates the sentences of the tar-get language.
2 given that a successful syntac-tic theory must provide for an efficient acquisi-tion mechanism, and since, prima facie, param-eter values seem transparently learnable, it isnot surprising that parameters have been incor-porated into current generative syntactic the-ories.
However, the exact process of parame-ter setting has been studied only recently (e.g.Clark (1992), Gibson and Wexler (1994), Yang(1999), Briscoe (2000), among others) and al-though it has proved linguistically fruitful toconstruct parametric analyses, it turns out tobe surprisingly difficult to construct a workablemodel of parameter-value acquisition.1.1 Parametr i c  Ambigu i tyA sentence is parametrically ambiguous if it islicensed by two or more distinct combinationsof parameter values.
Ambiguity is a natural en-emy of efficient language acquisition.
The prob-lem is that, due to ambiguity, there does notexist a one-to-one correspondence b tween thelinear 'word-order' surface strings of the inputsample and the correct parameter values thatgenerate the target language.
Clearly, if ev-230 binary parameters entails approximately a billiongrammars (230 = 1,073,741,824.
)61ery sentence of the target language triggers oneand only one set of parameter values (i.e.
ev-ery sentence is completely unambiguous) andthe learner, upon encountering an input, candetermine what those values are, the parametersetting process is truly transparent.
Unfortu-nately, not all natural languages entences areso distinctively earmarked by their parametricsignatures.
However, if there exists some degreeof parametric unambiguity in a learner's inputsample, a learner can set parameters by: 1) de-coding the parametric signature of an input sen-tence, 2) determining if ambiguity exists, and 3)using the input to guide parameter setting onlyin the case that the sentence is parametricallyunambiguous.
The motto of such a learner is:Don't learn .from ambiguous input and learningefficiency can be measured by the number ofsentences the learner has to wait for usable, un-ambiguous inputs to occur in the input stream.
32 The  St ructura l  T r iggers  LearnerOne recent model of human syntax acquisi-tion, The Structural Triggers ?earner(ST?)
(Fodor, 1998), employs the humanparsing mechanism to determine if an input isparametrically ambiguous.
Parameter valuesare viewed as bits of tree structure (treelets).When the learner's current grammar is insuffi-cient to parse the current input sentence, thetreelets may be utilized during the parsing pro-cess in the same way as any natural languagegrammar would be applied; no unusual parsingactivity is necessary.
The treelets are adoptedas part of the learner's current grammar hy-pothesis when: 1) they are required for a suc-cessful parse of the current input sentence and2) the sentence is unambiguous.
The STL thuslearns only from fully unambiguous sentences.
43Of course, the extent o which such unambiguoussentences exist in the domain of human languages i anempirical issue.
This is an important open research ques-tion which is the focus of a recent research endeavor hereat CUNY.
Our approach involves tagging a large, cross-linguistic set of child-directed sentences, drawn from theCHILDES database, with each sentence's parametric sig-nature.
By cross-tabulating the shared parameter valuesagainst different languages, the study should shed somelight as to the shape of ambiguity in input samples typ-ically encountered by children.4This is actually the strategy employed by just oneof several different STL variants, some of which are de-signed to manage domains in which unambiguous sen-AA,xA Parameter valuetreelets?
.-.....~,,~.....,,,m~,~.
SentenceSente~cceCurrent grammarFigure 1: An example of how the STL acquiresnew parameter values.See Figure 1.3 The  Feas ib i l i ty  o f  the  STLThe number of input sentences consumed by theSTL before convergence on the target grammarcan be derived from a relatively straightforwardMarkov analysis.
Importantly, the formulationmost useful to analyze performance does not re-quire states which represent he grammars ofthe parameter space (contra Niyogi and Berwick(1996)).
Instead, each state of the system de-picts the number of parameters that have beenset, t, and  the state transitions represent theprobability that the STL  will adopt some num-ber of new parameter values, w, on the basis ofthe current state and whatever usable paramet-ric information is revealed by the current inputsentence.
See Figure 2.The  following factors (described in detail be-low) determine the transition probabilities:?
the number  of parameters that have beenset (t)?
the number of relevant parameters (r)?
the expression rate (e)?
the effective expression rate (e I)Not all parameters are relevant parameters.Irrelevant parameters control properties of phe-nomena not present in the target language, suchas clitic order in a language without clitics.
Fortences are rare or nonexistent.62Figffre 2: A transition diagram for the STL per-forming in a parameter space of three parame-ters.
Nodes represent he current number ofparameters that have been correctly set.
Arcsindicate a change in the number that are cor-rectly set.
In this diagram, after each input isconsumed, 0, 1 or 2 new parameters may be set.Once the learner enters tate 3, it has convergedon the target.our purposes, the number of relevant parame-ters, r, is the total number of parameters thatneed to be set in order to license all and onlythe sentences of the target language.Of the parameters relevant o the target lan-guage as a whole, only some will be relevant oany given sentence.
A sentence xpresses thoseparameters for which a specific value is requiredin order to build a parse tree, i.e.
those pa-rameters which are essential to the sentence'sstructural description.
For instance, if a sen-tence does not have a relative clause, it will notexpress parameters that concern only relativeclauses; if it is a declarative sentence, it won'texpress the properties peculiar to questions; andso on.
The expression rate, e, for a language,is the average number of parameters expressedby its input sentences.
Suppose that each sen-tence, on average, is ambiguous with respect oa of the parameters it expresses.
The effectiveexpression rate, e ~, is the mean proportion ofexpressed parameters that are expressed unam-biguously (i.e.
e' = (e - a)/e).
It will also beuseful to consider a' = (1 - e~).3.1 Der ivat ion  of  a TransitionProbability FunctionTo present he derivation of the probability thatthe system will change from an arbitrary stateSt to state St+w, (0 < w < e) it is useful to setambiguity aside for a moment.
In order to set alr parameters, the STL has to encounter enoughbatches of e parameter values, possibly overlap-ping with each other, to make up the full set ofr parameter values that have to be established.Let H(wlt, r,e) be the probability that an ar-bitrary input sentence xpresses w new (i.e.
asyet unset) parameters, out of the e parametersexpressed, given that the learner has already sett parameters (correctly), for a domain in whichthere are r total parameters that need to be set.This is a specification of the hypergeometricdistribution and is given in Equation 1.H(wlt,r,e)= (~t)(~-t~?)
(i) (:)Now, to deal with ambiguity, the effectiverate of expression, e t, is brought into play.
Re-call that e ~ is the proportion of expressed pa-rameters that are expressed unambiguously.
Itfollows that the probability that any single pa-rameter is expressed unambiguously is also e ~and the probability that all of the expressed,but as yet unset parameters are expressed un-ambiguously is e ~w.
That is, the probability thatan input is effectively unambiguous and henceusable for learning is equal to e ~w.f H(w\[t,r,e)  w, O<w<et H(O\[t'r'e)+ i_~l H(ilt'r'e)(1-e'i)'w=O(2)Equation (2) can be used to calculate the prob-ability of any possible transition of the Markovsystem that models STL performance.
Onemethod to determine the number of sentencesexpected to be consumed by the STL is to sumthe number of sentences consumed in each state.Let E(Si) represent the expected number of sen-tences that will be consumed in state Si.
E isgiven by the following recurrence relation: 5E(So) = 1/e'?E(Zn) = II(1-P(Sn----~Sn)) ~ P(Si-'-~Sn)(Si)i~r t~e(5)The expected total is simply:r - -1Etot=E(So)+ ~ E(Si) (4)i=ewhich is equal to the expected number to beconsumed before any parameters have been set5The functional E is derived from basic propertiesof Markov Chains.
See Taylor and Karlin (1994) for ageneral derivation.63e a~(%)  r =15 r =20 r =25 r =301 20 62 90 119 15040 83 120 159 20060 124 180 238 30080 249 360 477 5995 20 15 22 29 3640 34 46 59 7360 144 176 210 24580 3 ,300  3 ,466  3 ,666 3 ,89110 20 14 18 23 2840 174 187 203 22160 9 ,560  9 ,621 9 ,727  9 ,87880 9 ,765 ,731  9 ,766 ,375  9 ,768 ,376  9 ,772 ,74015 20 28 32 37 4140 2 ,127  2 ,136  2 ,153 2 ,18060 931 ,323  931 ,352  931 ,479  931 ,82280 .
.
.over  l0  b i l l i on  .
.
.20 20 87 91 9540 27,351 27 ,361 27 ,38360 90 ,949 ,470  90 ,949 ,504  90 ,949 ,72880 .
.
.
in  the  t r i l l i ons  .
.
.Table 1: Average number of inputs consumedby the waiting-STL before convergence.
Fixedrate of expression.
(= E(So)) plus the number expected to be con-sumed after the first successful learning event(at which point the learner will be in state Se)summed with the number of sentences expectedto be consumed in every other state up to thestate just before the target is attained (St- l ) .Etot can be tractably calculated using dynamicprogramming.3.2 Some ResultsTable 1 presents numerical results derived byfixing different values of r, e, and e ~.
In orderto make assessments of performance across dif-ferent situations in terms of increasing rates ofambiguity, a percentage measure of ambiguity,a ~, is employed which is directly derived fromer: a ~ = 1 - e ~, and is presented in Table 1 as apercent (the proportion is multiplied by 100).Notice that the number of parameters to beset (r) has relatively little effect on convergencetime.
What dominates learning speed is am-biguity and expression rates.
When a ~ and eare both high, the STL is consuming an unrea-sonable number of input sentences.
However,the problem is not intrinsic to the STL modelof acquisition.
Rather, the problem is due toa too rigid restriction present in the currentformulation of the input sample.
By relaxingthe restriction, the expected performance of theSTL improves dramatically.
But first, it is in-formative to discuss why the framework, as pre-sented so far, leads to the prediction that theSTL will consume an extremely large numberof sentences at rates of ambiguity and expres-sion approaching natural anguage.By far the greatest amount of damage in-flicted by ambiguity occurs at the very earlieststages of learning.
This is because before anylearning takes place, the STL must wait for theoccurrence of a sentence that is fully unambigu-ous.
Such sentences are bound to be extremelyrare if the expression rate and the degree of am-biguity is high.
For instance, a sentence with 20out of 20 parameters unambiguous will virtuallynever occur if parameters are ambiguous on av-erage 99% of the time (the probability would be(1/100)2?
).After learning gets underway, STL perfor-mance improves tremendously; the generallydamaging effect of ambiguity is mitigated.
Ev-ery successful learning event decreases the num-ber of parameters till to be set.
Hence, theexpression rate of unset parameters decreasesas learning proceeds.
And to be usable by theSTL, the only parameters that need to be ex-pressed unambiguously are those that have notyet been set.
For example, if 19 parametershave already been set and e = r = 20 as in theexample above, the probability of encounteringa usable sentence in the case that parametersare ambiguous on average 99% of the time andthe input sample consists of sentences express-ing 20 parameters, is only (1/100) 1 = 1/100.This can be derived by plugging into Equation(2): w = 1, t = 19, e ---- 20, and r = 20 which isequal to: H(1119, 20, 20)(1/100) 1 = (1)(1/100).Clearly~ the probability of seeing usable in-puts increases rapidly as the number of param-eters that are set increases.
All that is needed,therefore, is to get parameter setting started, sothat the learner can be quickly be pulled downinto more comfortable regions of parametric ex-pression.
Once parameter setting is underway,the STL is extremely efficient.3.3 Distr ibuted Expression RateSo far e has been conveniently taken to be fixedacross all sentences of the target language.
Inwhich case, when e = 10, the learner will haveto wait for a sentence with exactly 10 unam-biguously expressed parameters in order to getstarted on learning, and as discussed above, itcan be expected that this will be a very longwait.
However, if one takes the value of e to beuniformly distributed (rather than fixed) thenthe learner will encounter some sentences which64express fewer than 10 parameters, and whichare correspondingly more likely to be fully un-ambiguous and hence usable for learning.In fact, any distribution of e can be incorpo-rated into the framework presented so far.
LetDi(x) denote the probability distribution of ex-pression of the input sample.
That is, the prob-ability that an arbitrarily chosen sentence fromthe input sample I expresses x parameters.
Forexample, if Di imposes a uniform distribution,then DI(x) = 1/emax where every sentence x-presses at least 1 parameter and emax is themaximum number of parameters expressed byany sentence.
Given Di, a new transition prob-ability P'(St '+ St+w) = P'(wlt, r, emax, e') canbe formulated as:?mazP'(w\[t,r,e .
.
.
.
et)-- - ~ Di(i)P(wlt,r,i,e' ) (5)where P is defined in (2) above and emaz repre-sents the maximum number of parameters thata sentence may express instead of a fixed num-ber for all sentences.To see why Equation (5) is valid, considerthat to set w new parameters at least w mustbe expressed in the current input sentence.
Alsousable, are sentences that express more  param-eters (w + I, w + 2, w + 3,..., emax).
Thus,  theprobability of setting w new parameters  is sim-ply the sum of the probabilities that a sentenceexpressing a number  of parameters,  i, f rom wto emax, is encountered by the STL (= Di(i)),times the probability that the STL can set w ad-ditional parameters given that i are expressed.By replacing P with P' in Equation 3 and mod-ifying the derivation of the base case, 6 the to-tal expected number of sentences that will beconsumed by the STL given a distribution ofexpression Di(x) can be calculated.Table 2 presents numerical results derived byfixing r and a' and allowing e to vary uniformlyfrom 0 to emax.
As in Table 1, a percentagemeasure of ambiguity, a', is employed.The results displayed in the table indicatea striking decrease in the number of sentencesthat the that the STL can be expected to con-sume compared to those obtained with a fixedexpression rate in place.
As a rough compari-son, with the ambiguity rate (a') at 80%: when6E(So) = 1/(1 - -  P'(So --+ So))emaz  a ' (%)  r = 15 r = 20 r = 25 r = 301 20 124 180 238 30040 166 240 318 39960 249 360 477 59980 498 720 954 119820 28 40 53 6740 46 65 86 10760 89 124 161 19980 235 324 417 51110 20 17 24 32 4040 40 55 70 8660 102 137 173 20980 323 430 538 64815 2040608020 2040608015 21 27 3346 62 77 93134 176 219 262447 586 726 86820 26 3274 91 109223 275 327755 931 1108Table 2: Average number of inputs consumedby the STL before convergence.
Uniformly dis-tributed rate of expression.e varies uniformly from 0 to 10, the STL re-quires 430 sentences (from Table 2); when e isfixed at 5, the number of sentences required is3,466 (from Table 1).4 D iscuss ionAlthough presented as a feasibility analysis ofparameter-setting - -  specifically of STL perfor-mance, it should be clear that the relevant fac-tors e', e, r, etc.
can be applied to shape anabstract input domain for almost any learningstrategy.
This is important because questions ofa model's feasibility have proved difficult to an-swer in spaces of a linguistically plausible size.Recent attempts necessarily rely on severelysmall, highly circumscribed language domains(e.g.
Gibson and Wexler (1994), among others).These studies frequently involve the construc-tion of an idealized language sample which is(at best) an accurate subset of sentences thata child might hear.
A simulated learner is letloose on the input space and results consist ofeither the structure of the grammar(s) acquiredor the specific circumstances under which thelearner succeeds or fails to attain the target.Without question, this research agenda is valu-able and can bring to light interesting charac-teristics of the acquisition process.
(cf.
Gibsonand Wexler's (1994) argument for certain de-fault parameter values based on the potentialsuccess or failure of verb-second acquisition ina three-parameter domain.
And, for a differentperspective, Elman et al's (1996) discussions of65English part-of-speech and past-tense morphol-ogy acquisition in a connectionist framework.
)I stress that my point here is not to give afull accounting of STL performance.
Substan-tim work has been completed towards this end(Sakas (2000), Sakas and Fodor (In press.
)),as well as development of a similar frame-work to other models (See Sakas and Demner-Fushman (In prep.)
for an application to Gib-son and Wexler's Triggering Learning Algo-rithm).
Rather, I intend to put forth the conjec-ture that syntax acquisition is extremely sensi-rive to the distribution of ambiguity, and, giventhis extreme sensitivity, suggest that simulationstudies need to be conducted in conjunctionwith a broader analysis which abstracts awayfrom whatever linguistic particulars are neces-sary to bring about the sentences required tobuild the input sample that feeds the simulatedlearner.Ultimately, whether a particular acquisitionmodel is successful is an empirical issue and de-pends on the exact conditions under which themodel performs well and the extent to whichthose favorable conditions are in line with thefacts of human language.
Thus, I believe athree-fold approach to validate a computationalmodel of acquisition is warranted.
First, anabstract analysis (similar to the one presentedhere) should be constructed that can be usedto uncover a model's sweet spots - -  where theshape of ambiguity is favorable to learning per-formance.
Second, a computational psycholin-guistic study should be undertaken to see if themodel's weet spots are in line with the distri-bution of ambiguity in natural anguage.
Andfinally, a simulation should be carried out.Obviously, this a huge proposal requiringyears of person-hours and coordinated planningamong researchers with diverse skills.
But ifcomputational modeling is going to eventuallylay claim to a model which accurately mir-rors the human process of language acquisition,years of fine grinding are necessary.Acknowledgments .
This work was supportedin part by PSC-CUNY-30 Research Grant 61595-00-30.
The three-fold approach is at the root of aproject we have begun at The City University of NewYork.
Much thanks to my collaborators Janet DeanFodor and Virginia Teller for many useful discussionsand input, as well as to two anonymous reviewers fortheir helpful comments.Re ferencesE.J.
Briscoe.
2000.
Grammatical Acquisition:Inductive Bias and Coevolution of Languageand the Language Acquisition Device.
Lan-guage, 76(2).N.
Chomsky.
1981.
Lectures on Governmentand Binding.
Foris.
DordrechtR.
Clark.
1992.
The selection of syntacticknowledge.
Language Acquisition, 2(2):83-149.J.L.
Elman, E. Bates, M.A.
Johnson,A.
Karmiloff-Smith, D. Parisi, and K. Plun-kett.
Rethinking Innateness: A Connection-ist Perspective on Development.
MIT Press,Cambridge, MA.J.D.
Fodor.
1998.
Unambiguous triggers.
Lin-guistic Inquiry, 29(1):1-36.E.
Gibson and K. Wexler.
1994.
Triggers.
Lin-guistic Inquiry, 25(3):407-454.P.
Niyogi and R.C.
Berwick.
1996.
A languagelearning model for finite parameter spaces.Cognition, 61:161-193.W.G.
Sakas.
2000.
Ambiguity and the Compu-tational Feasibility of Syntax Acquisition.
Un-published Ph.D. dissertation, City Universityof New York.W.G.
Sakas and D. Demner-Fushman.
I  Prep.Simulating Parameter Setting Performance inDomains with a Large Number of Parameters:A Hybrid Approach.W.G.
Sakas and J.D.
Fodor.
In Press.
TheStructural Triggers Learner.
In StefanoBertolo, editor, Parametric Linguistics andLearnability: A Self-contained Tutorial forLinguists.
Cambridge University Press, Cam-bridge,UK.H.M.
Taylor and S. Karlin.
1994.
An Introduc-tion to Stochastic Modeling.
Academic Press,San Diego, CA.C.D.
Yang.
1999.
A selectionist theory of lan-guage acquisition.
In Proceedings of the 37thAnnual Meeting of the A CL.
Association forComputational Linguistics.66
