Proceedings of the Workshop on How Can Computational Linguistics Improve Information Retrieval?, pages 17?24,Sydney, July 2006. c?2006 Association for Computational LinguisticsExtracting Key Phrases to DisambiguatePersonal Name Queries in Web SearchDanushka Bollegala Yutaka Matsuo ?Graduate School of Information Science and TechnologyThe University of Tokyo7-3-1, Hongo, Bunkyo-ku, Tokyo, 113-8656, Japandanushka@mi.ci.i.u-tokyo.ac.jpy.matsuo@aist.go.jpishizuka@i.u-tokyo.ac.jpMitsuru IshizukaAbstractAssume that you are looking for informa-tion about a particular person.
A searchengine returns many pages for that per-son?s name.
Some of these pages maybe on other people with the same name.One method to reduce the ambiguity in thequery and filter out the irrelevant pages, isby adding a phrase that uniquely identi-fies the person we are interested in fromhis/her namesakes.
We propose an un-supervised algorithm that extracts suchphrases from the Web.
We represent eachdocument by a term-entity model and clus-ter the documents using a contextual sim-ilarity metric.
We evaluate the algorithmon a dataset of ambiguous names.
Ourmethod outperforms baselines, achievingover 80% accuracy and significantly re-duces the ambiguity in a web search task.1 IntroductionThe Internet has grown into a collection of bil-lions of web pages.
Web search engines are im-portant interfaces to this vast information.
Wesend simple text queries to search engines and re-trieve web pages.
However, due to the ambigu-ities in the queries, a search engine may returna lot of irrelevant pages.
In the case of personalname queries, we may receive web pages for otherpeople with the same name (namesakes).
For ex-ample, if we search Google 1 for Jim Clark, evenamong the top 100 results we find at least eightdifferent Jim Clarks.
The two popular namesakes;?National Institute of Advanced Industrial Science andTechnology1www.google.comJim Clark the Formula one world champion (46pages), and Jim Clark the founder of Netscape (26pages), cover the majority of the pages.
What ifwe are interested only in the Formula one worldchampion and want to filter out the pages for theother Jim Clarks?
One solution is to modify ourquery by including a phrase such as Formula oneor racing driver with the name, Jim Clark.This paper presents an automatic method to ex-tract such phrases from the Web.
We follow athree-stage approach.
In the first stage we rep-resent each document containing the ambiguousname by a term-entity model, as described in sec-tion 5.2.
We define a contextual similarity metricbased on snippets returned by a search engine, tocalculate the similarity between term-entity mod-els.
In the second stage, we cluster the documentsusing the similarity metric.
In the final stage, weselect key phrases from the clusters that uniquelyidentify each namesake.2 ApplicationsTwo tasks that can readily benefit from automat-ically extracted key phrases to disambiguate per-sonal names are query suggestion and social net-work extraction.
In query suggestion (Gauch andSmith, 1991), the search engine returns a set ofphrases to the user alongside with the search re-sults.
The user can then modify the original queryusing these phrases to narrow down the search.Query suggestion helps the users to easily navigatethrough the result set.
For personal name queries,the key phrases extracted by our algorithm can beused as suggestions to reduce the ambiguity andnarrow down the search on a particular namesake.Social networking services (SNSs) have beengiven much attention on the Web recently.
Asa kind of online applications, SNSs can be used17to register and share personal information amongfriends and communities.
There have been recentattempts to extract social networks using the infor-mation available on the Web 2(Mika, 2004; Mat-suo et al, 2006).
In both Matsuo?s (2006) andMika?s (2004) algorithms, each person is repre-sented by a node in the social network and thestrength of the relationship between two peopleis represented by the length of the edge betweenthe corresponding two nodes.
As a measure of thestrength of the relationship between two people Aand B, these algorithms use the number of hits ob-tained for the query A AND B.
However, this ap-proach fails when A or B has namesakes becausethe number of hits in these cases includes the hitsfor the namesakes.
To overcome this problem, wecould include phrases in the query that uniquelyidentify A and B from their namesakes.3 Related WorkPerson name disambiguation can be seen asa special case of word sense disambiguation(WSD) (Schutze, 1998; McCarthy et al, 2004)problem which has been studied extensively inNatural Language Understanding.
However, thereare several fundamental differences between WSDand person name disambiguation.
WSD typicallyconcentrates on disambiguating between 2-4 pos-sible meanings of the word, all of which are apriori known.
However, in person name disam-biguation in Web, the number of different name-sakes can be much larger and unknown.
From aresource point of view, WSD utilizes sense taggeddictionaries such as WordNet, whereas no dictio-nary can provide information regarding differentnamesakes for a particular name.The problem of person name disambiguationhas been addressed in the domain of research pa-per citations (Han et al, 2005), with various super-vised methods proposed for its solution.
However,citations have a fixed format compared to free texton the Web.
Fields such as co-authors, title, jour-nal name, conference name, year of publicationcan be easily extracted from a citation and providevital information to the disambiguation process.Research on multi-document person name res-olution (Bagga and Baldwin, 1998; Mann andYarowsky, 2003; Fleischman and Hovy, 2004) fo-cuses on the related problem of determining if2http://flink.sematicweb.org/.
The system won the 1stplace at the Semantic Web Challenge in ISWC2004.two instances with the same name and from dif-ferent documents refer to the same individual.Bagga and Baldwin (1998) first perform within-document coreference resolution to form coref-erence chains for each entity in each document.They then use the text surrounding each referencechain to create summaries about each entity ineach document.
These summaries are then con-verted to a bag of words feature vector and areclustered using standard vector space model of-ten employed in IR.
The use of simplistic bag ofwords clustering is an inherently limiting aspect oftheir methodology.
On the other hand, Mann andYarowsky (2003) proposes a richer document rep-resentation involving automatically extracted fea-tures.
However, their clustering technique can bebasically used only for separating two people withthe same name.
Fleischman and Hovy (2004) con-structs a maximum entropy classifier to learn dis-tances between documents that are then clustered.Their method requires a large training set.Pedersen et al (2005) propose an unsupervisedapproach to resolve name ambiguity by represent-ing the context of an ambiguous name using sec-ond order context vectors derived using singularvalue decomposition (SVD) on a co-occurrencematrix.
They agglomeratively cluster the vec-tors using cosine similarity.
They evaluate theirmethod only on a conflated dataset of pseudo-names, which begs the question of how well sucha technique would fair on a more real-world chal-lenge.
Li et al (2005) propose two approaches todisambiguate entities in a set of documents: a su-pervisedly trained pairwise classifier and an unsu-pervised generative model.
However, they do notevaluate the effectiveness of their method in Websearch.Bekkerman and McCallum (2005) present twounsupervised methods for finding web pages re-ferring to a particular person: one based onlink structure and another using Agglomera-tive/Conglomerative Double Clustering (A/CDC).Their scenario focuses on simultaneously disam-biguating an existing social network of people,who are closely related.
Therefore, their methodcannot be applied to disambiguate an individualwhose social network (for example, friends, col-leagues) is not known.
Guha and Grag (2004)present a re-ranking algorithm to disambiguatepeople.
The algorithm requires a user to select oneof the returned pages as a starting point.
Then,18Table 1: Data set for experimentsCollection No of namesakesperson-X 4Michael Jackson 3Jim Clark 8William Cohen 10through comparing the person descriptions, the al-gorithm re-ranks the entire search results in sucha way that pages referring to the same person de-scribed in the user-selected page are ranked higher.A user needs to browse the documents in order tofind which matches the user?s intended referent,which puts an extra burden on the user.None of the above mentioned works attempt toextract key phrases to disambiguate person namequeries, a contrasting feature in our work.4 Data SetWe select three ambiguous names (Micheal Jack-son, William Cohen and Jim Clark) that appear inprevious work in name resolution.
For each namewe query Google with the name and download top100 pages.
We manually classify each page ac-cording to the namesakes discussed in the page.We ignore pages which we could not decide thenamesake from the content.
We also remove pageswith images that do not contain any text.
No pageswere found where more than one namesakes of aname appear.
For automated pseudo-name evalua-tion purposes, we select four names (Bill Clinton,Bill Gates, Tom Cruise and Tiger Woods) for con-flation, who we presumed had one vastly predom-inant sense.
We download 100 pages from Googlefor each person.
We replace the name of the per-son by ?person-X?
in the collection, thereby intro-ducing ambiguity.
The structure of our dataset isshown in Table 1.5 Method5.1 Problem StatementGiven a collection of documents relevant to an am-biguous name, we assume that each document inthe collection contains exactly one namesake ofthe ambiguous name.
This is a fair assumptionconsidering the fact that although namesakes sharea common name, they specializes in differentfields and have different Web appearances.
More-over, the one-to-one association between docu-ments and people formed by this assumption, letus model the person name disambiguation prob-lem as a one of hard-clustering of documents.The outline of our method is as following;Given a set of documents representing a group ofpeople with the same name, we represent eachdocument in the collection using a Term-Entitymodel (section 5.2).
We define a contextual sim-ilarity metric (section 5.4) and then cluster (sec-tion 5.5) the term-entity models using the contex-tual similarity between them.
Each cluster is con-sidered to be representing a different namesake.Finally, key phrases that uniquely identify eachnamesake are selected from the clusters.
We per-form experiments at each step of our method toevaluate its performance.5.2 Term-Entity ModelThe first step toward disambiguating a personalname is to identify the discriminating features ofone person from another.
In this paper we proposeTerm-Entity models to represent a person in a doc-ument.Definition.
A term-entity model T (A), represent-ing a person A in a document D, is a booleanexpression of n literals a1, a2, .
.
.
, an.
Here, aboolean literal ai is a multi-word term or a namedentity extracted from the document D.For simplicity, we only consider boolean ex-pressions that combine the literals through ANDoperator.The reasons for using terms as well as namedentities in our model are two fold.
Firstly, there aremulti-word phrases such as secretary of state, rac-ing car driver which enable us to describe a personuniquely but not recognized by named entity tag-gers.
Secondly, automatic term extraction (Frantziand Ananiadou, 1999) can be done using statisticalmethods and does not require extensive linguisticresources such as named entity dictionaries, whichmay not be available for some domains.5.3 Creating Term-Entity ModelsWe extract terms and named entities from eachdocument to build the term-entity model for thatdocument.
For automatic multi-word term ex-traction, we use the C-value metric proposed byFrantzi et al (1999).
Firstly, the text from whichwe need to extract terms is tagged using a partof speech tagger.
Then a linguistic filter and astop words list constrain the word sequences that19020406080100120 President of the United StatesGeorge BushpresidentialgeorgepresidentnewsbiographygamesbushbushslibraryfathervicegovernmentpresidentsshallunitedstatesexecutiveFigure 1: Distribution of words in snippets for?George Bush?
and ?President of the UnitedStates?are allowed as genuine multi-word terms.
Thelinguistic filter contains a predefined set of pat-terns of nouns, adjectives and prepositions that arelikely to be terms.
The sequences of words that re-main after this initial filtering process (candidateterms) are evaluated for their termhood (likelinessof a candidate to be a term) using C-value.
C-value is built using statistical characteristics of thecandidate string, such as, total frequency of oc-currence of the candidate string in the document,the frequency of the candidate string as part ofother longer candidate strings, the number of theselonger candidate terms and the length of candidatestring (in number of words).
We select the candi-dates with higher C-values as terms (see (Frantziand Ananiadou, 1999) for more details on C-valuebased term extraction).To extract entities for the term-entity model, thedocuments were annotated by a named entity tag-ger 3.
We select personal names, organizationnames and location names to be included in theterm-entity model.5.4 Contextual SimilarityWe need to calculate the similarity between term-entity models derived from different documents,in order to decide whether they belong to thesame namesake or not.
WordNet 4 based similar-ity metrics have been widely used to compute thesemantic similarity between words in sense dis-3The named entity tagger was developed by the CognitiveComputation Group at UIUC.
http://L2R.cs.uiuc.edu/ cog-comp/eoh/ne.html4http://wordnet.princeton.edu/perl/webwn0306090120150President of the United StatesTiger WoodsstatesgeorgegolferbushwoodstourvicetigercheatsshallgovernmentpresidentspgagolfunitedpresidentreviewsexecutiveFigure 2: Distribution of words in snippets for?Tiger Woods?
and ?President of the UnitedStates?ambiguation tasks (Banerjee and Pedersen, 2002;McCarthy et al, 2004).
However, most of theterms and entities in our term-entity models areproper names or multi-word expressions which arenot listed in WordNet.Sahami et al (2005) proposed the use of snip-pets returned by a Web search engine to calculatethe semantic similarity between words.
A snippetis a brief text extracted from a document aroundthe query term.
Many search engines provide snip-pets alongside with the link to the original docu-ment.
Since snippets capture the immediate sur-rounding of the query term in the document, wecan consider a snippet as the context of a queryterm.
Using snippets is also efficient because wedo not need to download the source documents.To calculate the contextual similarity between twoterms (or entities), we first collect snippets foreach term (or entity) and pool the snippets intoa combined ?bag of words?.
Each collection ofsnippets is represented by a word vector, weightedby the normalized frequency (i.e., frequency of aword in the collection is divided by the total num-ber of words in the collection).
Then, the contex-tual similarity between two phrases is defined asthe inner product of their snippet-word vectors.Figures 1 and 2 show the distribution of mostfrequent words in snippets for the queries ?GeorgeBush?, ?Tiger Woods?
and ?President of theUnited States?.
In Figure 1 we observe the words?george?
and ?bush?
appear in snippets for thequery ?President of the United States?, whereasin Figure 2 none of the high frequent words ap-pears in snippets for both queries.
Contextual20similarity calculated as the inner product betweenword vectors is 0.2014 for ?George Bush?
and?President of the United States?, whereas thesame is 0.0691 for ?Tiger Woods?
and ?Presi-dent of the United States?.
We define the simi-larity sim(T (A), T (B)), between two term-entitymodels T (A) = {a1, .
.
.
, an} and T (B) ={b1, .
.
.
, bm} of documents A and B as follows,sim(T (A), T (B)) = 1nn?i=1max1?j?m|ai| ?
|bj |.
(1)Here, |ai| represents the vector that contains thefrequency of words that appear in the snippetsfor term/entity ai.
Contextual similarity betweenterms/entities ai and bj , is defined as the innerproduct |ai| ?
|bj |.
Without a loss of generality weassume n ?
m in formula 1.5.5 ClusteringWe use Group-average agglomerative clustering(GAAC) (Cutting et al, 1992), a hybrid of single-link and complete-link clustering, to group thedocuments that belong to a particular namesake.Initially, we assign a separate cluster for each ofthe documents in the collection.
Then, GAAC ineach iteration executes the merger that gives riseto the cluster ?
with the largest average correla-tion C(?)
where,C(?)
= 121|?|(|?| ?
1)Xu??Xv?
?sim(T (u), T (v)) (2)Here, |?| denotes the number of documents inthe merged cluster ?
; u and v are two documentsin ?
and sim(T (u), T (v)) is given by equation 1.Determining the total number of clusters is an im-portant issue that directly affects the accuracy ofdisambiguation.
We will discuss an automaticmethod to determine the number of clusters in sec-tion 6.3.5.6 Key phrases SelectionGAAC process yields a set of clusters representingeach of the different namesakes of the ambiguousname.
To select key phrases that uniquely iden-tify each namesake, we first pool all the terms andentities in all term-entity models in each cluster.For each cluster we select the most discrimina-tive terms/entities as the key phrases that uniquelyidentify the namesake represented by that clusterfrom the other namesakes.
We achieve this intwo steps.
In the first step, we reduce the num-ber of terms/entities in each cluster by removingterms/entities that also appear in other clusters.In the second step, we select the terms/entitiesin each cluster according to their relevance tothe ambiguous name.
We compute the con-textual similarity between the ambiguous nameand each term/entity and select the top rankingterms/entities from each cluster.6 Experiments and Results6.1 Evaluating Contextual SimilarityIn section 5.4, we defined the similarity betweendocuments (i.e., term-entity models created fromthe documents) using a web snippets based con-textual similarity (Formula 1).
However, how wellsuch a metric represents the similarity betweendocuments, remains unknown.
Therefore, to eval-uate the contextual similarity among documents,we group the documents in ?person-X?
datasetinto four classes (each class representing a differ-ent person) and use Formula 1 to compute within-class and cross-class similarity histograms, as il-lustrated in Figure 3.Ideally, within-class similarity distributionshould have a peak around 1 and cross-class sim-ilarity distribution around 0, whereas both his-tograms in Figure 3(a) and 3(b) have their peaksaround 0.2.
However, within-class similarity dis-tribution is heavily biased toward to the right ofthis peak and cross-class similarity distribution tothe left.
Moreover, there are no document pairswith more than 0.5 cross-class similarity.
The ex-perimental results guarantees the validity of thecontextual similarity metric.6.2 Evaluation MetricWe evaluate experimental results based on theconfusion matrix, where A[i.j] represents thenumber of documents of ?person i?
predicted as?person j?
in matrix A.
A[i, i] represents the num-ber of correctly predicted documents for ?personi?.
We define the disambiguation accuracy as thesum of diagonal elements divided by the sum ofall elements in the matrix.6.3 Cluster QualityEach cluster formed by the GAAC process is sup-posed to be representing a different namesake.Ideally, the number of clusters formed should beequal to the number of different namesakes for210300600900120015001.11.00.90.80.70.60.50.40.30.20.1(a) Within-class similarity distribution in?person-X?
dataset0100020003000400050001.11.00.90.80.70.60.50.40.30.20.1(b) Cross-class similarity distribution in?person-X?
datasetFigure 3: The histogram of within-class and cross-class similarity distributions in ?person-X?
dataset.
Xaxis represents the similarity value.
Y axis represents the number of document pairs from the same class(within-class) or from different classes (cross-class) that have the corresponding similarity value.the ambiguous name.
However, in reality it isimpossible to exactly know the number of name-sakes that appear on the Web for a particular name.Moreover, the distribution of pages among name-sakes is not even.
For example, in the ?Jim Clark?dataset 78% of documents belong to the two fa-mous namesakes (CEO Nestscape and Formulaone world champion).
The rest of the documentsare distributed among the other six namesakes.
Ifthese outliers get attached to the otherwise pureclusters, both disambiguation accuracy and keyphrase selection deteriorate.
Therefore, we moni-tor the quality of clustering and terminate furtheragglomeration when the cluster quality drops be-low a pre-set threshold.
Numerous metrics havebeen proposed for evaluating quality of cluster-ing (Kannan et al, 2000).
We use normalizedcuts (Shi and Malik, 2000) as a measure of cluster-quality.Let, V denote the set of documents for a name.Consider, A ?
V to be a cluster of documentstaken from V .
For two documents x,y in V ,sim(x, y) represents the contextual similarity be-tween the documents (Formula 1).
Then, the nor-malized cut Ncut(A) of cluster A is defined as,Ncut(A) =?x?A y?
(V?A) sim(x, y)?x?A y?V sim(x, y).
(3)For a set, {A1, .
.
.
, An} of non-overlapping nclusters Ai, we define the quality of clustering,AccuracyQuality00.20.40.60.811.20.8 0.85 0.9 0.95 1 1.05Figure 4: Accuracy Vs Cluster Quality for person-X data set.Quality({A1, .
.
.
, An}), as follows,Quality({A1, .
.
.
, An}) = 1nn?i=1Ncut(Ai).
(4)To explore the faithfulness of cluster qualityin approximating accuracy, we compare accuracy(calculated using human-annotated data) and clus-ter quality (automatically calculated using For-mula 4) for person-X data set.
Figure 4 showscluster quality in x-axis and accuracy in y-axis.We observe a high correlation (Pearson coefficientof 0.865) between these two measures, which en-ables us to guide the clustering process throughcluster quality.When cluster quality drops below a pre-defined22ThresholdAccuracy0.690.70.710.720.730.740.750.760.770.780.790.6 0.7 0.8 0.9 1Figure 5: Accuracy Vs Threshold value forperson-X data set.threshold, we terminate further clustering.
Weassign the remaining documents to the alreadyformed clusters based on the correlation (For-mula 2) between the document and the cluster.
Todetermine the threshold of cluster quality, we useperson-X collection as training data.
Figure 5 il-lustrates the variation of accuracy with threshold.We select threshold at 0.935 where accuracy max-imizes in Figure 5.
Threshold was fixed at 0.935for the rest of the experiments.6.4 Disambiguation AccuracyTable 2 summarizes the experimental results.
Thebaseline, majority sense , assigns all the doc-uments in a collection to the person that havemost documents in the collection.
Proposedmethod outperforms the baseline in all data sets.Moreover, the accuracy values for the proposedmethod in Table 2 are statistically significant (t-test: P(T?t)=0.0087, ?
= 0.05) compared to thebaseline.
To identify each cluster with a name-sake, we chose the person that has most num-ber of documents in the cluster.
?Found?
columnshows the number of correctly identified name-sakes as a fraction of total namesakes.
Althoughthe proposed method correctly identifies the pop-ular namesakes, it fails to identify the namesakeswho have just one or two documents in the collec-tion.6.5 Web Search TaskKey phrases extracted by the proposed method arelisted in Figure 6 (Due to space limitations, weshow only the top ranking key phrases for two col-lections).
To evaluate key phrases in disambiguat-Table 2: Disambiguation accuracy for each collec-tion.Collection Majority Proposed FoundSense Method Correctperson-X 0.3676 0.7794 4/4Michael Jackson 0.6470 0.9706 2/3Jim Clark 0.4407 0.7627 3/8William Cohen 0.7614 0.8068 3/10Michael JacksonJim Clarkfan clubtrialworld networksuperstarnew charity songneverland ranchbeer hunterultimate beer FAQchristmas beergreat beerpilsener beerbarvariaCLUSTER #1 CLUSTER #2CLUSTER #1 CLUSTER #2racing driverrallyscotsmandriving geniusscottish automobile racerbritish rally newsentrepreneurstorysilicon valleyCEOsilicon graphicsSGI/ NetscapeFigure 6: Top ranking key phrases in clusters forMichael Jackson and Jim Clark datasets.ing namesakes, we set up a web search experimentas follows.
We search for the ambiguous name andthe key phrase (for example, ?Jim Clark?
AND?racing driver?)
and classify the top 100 resultsaccording to their relevance to each namesake.
Re-sults of our experiment on Jim Clark dataset forthe top ranking key phrases are shown in Table 3.In Table 3 we classified Google search resultsinto three categories.
?person-1?
is the formulaone racing world champion, ?person -2?
is thefounder of Netscape and ?other?
category containsrest of the pages that we could not classify to pre-vious two groups 5.
We first searched Googlewithout adding any key phrases to the name.
In-cluding terms racing diver, rally and scotsman,Table 3: Effectiveness of key phrases in disam-biguating namesakes.Phrase person-1 person-2 others HitsNONE 41 26 33 1,080,000racing driver 81 1 18 22,500rally 42 0 58 82,200scotsman 67 0 33 16,500entrepreneur 1 74 25 28,000story 17 53 30 186,000silicon valley 0 81 19 46,8005some of these pages were on other namesakes and somewere not sufficiently detailed to properly classify23which were the top ranking terms for Jim Clarkthe formula one champion, yields no results for theother popular namesake.
Likewise, the key wordsentrepreneur and silicon valley yield results forthe founder of Netscape.
However, the key wordstory appears for both namesakes.
A close investi-gation revealed that, the keyword story is extractedfrom the title of the book ?The New New Thing:A Silicon Valley Story?, a book on the founder ofNetscape.7 ConclusionWe proposed and evaluated a key phrase extractionalgorithm to disambiguate people with the samename on the Web.
We represented each documentwith a term-entity model and used a contextualsimilarity metric to cluster the documents.
We alsoproposed a novel approach to determine the num-ber of namesakes.
Our experiments with pseudoand naturally ambiguous names show a statisti-cally significant improvement over the baselinemethod.
We evaluated the key phrases extractedby the algorithm in a web search task.
The websearch task reveals that including the key phrasesin the query considerably reduces ambiguity.
Infuture, we plan to extend the proposed methodto disambiguate other types of entities such aslocation names, product names and organizationnames.ReferencesA.
Bagga and B. Baldwin.
1998.
Entity-based cross-document coreferencing using the vector spacemodel.
In Proceedings of COLING, pages 79?85.Satanjeev Banerjee and Ted Pedersen.
2002.
Anadapted lesk algorithm for word sense disambigua-tion using word net.
In Proceedings of the third in-ternational conference on computational linguisticsand intelligent text processing, pages 136?145.Ron Bekkerman and Andrew McCallum.
2005.
Dis-ambiguating web appearances of people in a socialnetwork.
In Proceedings of the 14th internationalconference on World Wide Web, pages 463?470.Douglass R. Cutting, Jan O. Pedersen, David Karger,and John W. Tukey.
1992.
Scatter/gather: A cluster-based approach to browsing large document collec-tions.
In Proceedings SIGIR ?92, pages 318?329.M.B.
Fleischman and E. Hovy.
2004.
Multi-documentperson name resolution.
In Proceedings of 42nd An-nual Meeting of the Association for ComputationalLinguistics (ACL), Reference Resolution Workshop.K.T.
Frantzi and S. Ananiadou.
1999.
The c-value/nc-value domain independent method for multi-wordterm extraction.
Journal of Natural Language Pro-cessing, 6(3):145?179.S.
Gauch and J.
B. Smith.
1991.
Search improvementvia automatic query reformulation.
ACM Trans.
onInformation Systems, 9(3):249?280.R.
Guha and A. Garg.
2004.
Disambiguating people insearch.
In Stanford University.Hui Han, Hongyuan Zha, and C. Lee Giles.
2005.Name disambiguation in author citations using a k-way spectral clustering method.
In Proceedings ofthe International Conference on Digital Libraries.Ravi Kannan, Santosh Vempala, and Adrian Vetta.2000.
On clusterings: Good, bad, and spectral.
InProceedings of the 41st Annual Symposium on theFoundation of Computer Science, pages 367?380.Xin Li, Paul Morie, and Dan Roth.
2005.
Semanticintegration in text, from ambiguous names to identi-fiable entities.
AI Magazine, American Associationfor Artificial Intelligence, Spring:45?58.Gideon S. Mann and David Yarowsky.
2003.
Unsuper-vised personal name disambiguation.
In Proceed-ings of CoNLL-2003, pages 33?40.Y.
Matsuo, J. Mori, and M. Hamasaki.
2006.
Poly-phonet: An advanced social network extraction sys-tem.
In to appear in World Wide Web Conference(WWW).D.
McCarthy, R. Koeling, J. Weeds, and J. Carroll.2004.
Finding predominant word senses in untaggedtext.
In Proceedings of the 42nd Meeting of the As-sociation for Computational Linguistics (ACL?04),pages 279?286.P.
Mika.
2004.
Bootstrapping the foaf-web: and ex-periment in social networking network minning.
InProceedings of 1st Workshop on Friend of a Friend,Social Networking and the Semantic Web.Ted Pedersen, Amruta Purandare, and Anagha Kulka-rni.
2005.
Name discrimination by clustering sim-ilar contexts.
In Proceedings of the Sixth Interna-tional Conference on Intelligent Text Processing andComputational Linguistics.Mehran Sahami and Tim Heilman.
2005.
A web-basedkernel function for matching short text snippets.
InInternational Workshop located at the 22nd Inter-national Conference on Machine Learning (ICML2005).Hinrich Schutze.
1998.
Automatic word sense dis-crimination.
Computational Linguistics, 24(1):97?123.Jianbo Shi and Jitendra Malik.
2000.
Normalized cutsand image segmentation.
IEEE Transactions on Pat-tern Analysis and Machine Intelligence, 22(8):888?905.24
