Workshop on Computational Linguistics for Literature, pages 26?35,Montre?al, Canada, June 8, 2012. c?2012 Association for Computational LinguisticsUnsupervised Stylistic Segmentation of Poetrywith Change Curves and Extrinsic FeaturesJulian BrookeDept of Computer ScienceUniversity of Torontojbrooke@cs.toronto.eduAdam HammondDept of EnglishUniversity of Torontoadam.hammond@utoronto.caGraeme HirstDept of Computer ScienceUniversity of Torontogh@cs.toronto.eduAbstractThe identification of stylistic inconsistency is achallenging task relevant to a number of gen-res, including literature.
In this work, wecarry out stylistic segmentation of a well-knownpoem, The Waste Land by T.S.
Eliot, whichis traditionally analyzed in terms of numerousvoices which appear throughout the text.
Ourmethod, adapted from work in topic segmen-tation and plagiarism detection, predicts breaksbased on a curve of stylistic change which com-bines information from a diverse set of features,most notably co-occurrence in larger corpora viareduced-dimensionality vectors.
We show thatthis extrinsic information is more useful than(within-text) distributional features.
We achievewell above baseline performance on both artifi-cial mixed-style texts and The Waste Land itself.1 IntroductionMost work in automated stylistic analysis operatesat the level of a text, assuming that a text is stylis-tically homogeneous.
However, there are a numberof instances where that assumption is unwarranted.One example is documents collaboratively createdby multiple authors, in which contributors may, ei-ther inadvertently or deliberately (e.g.
Wikipediavandalism), create text which fails to form a stylis-tically coherent whole.
Similarly, stylistic incon-sistency might also arise when one of the ?contrib-utors?
is actually not one of the purported authorsof the work at all ?
that is, in cases of plagia-rism.
More-deliberate forms of stylistic dissonanceinclude satire, which may first follow and then floutthe stylistic norms of a genre, and much narrative lit-erature, in which the author may give the speech orthought patterns of a particular character their ownstyle distinct from that of the narrator.
In this paper,we address this last source of heterogeneity in thecontext of the well-known poem The Waste Land byT.S.
Eliot, which is often analyzed in terms of thedistinct voices that appear throughout the text.T.S.
Eliot (1888?1965), recipient of the 1948 No-bel Prize for Literature, is among the most importanttwentieth-century writers in the English language.Though he worked in a variety of forms ?
he wasa celebrated critic as well as a dramatist, receivinga Tony Award in 1950 ?
he is best remembered to-day for his poems, of which The Waste Land (1922)is among the most famous.
The poem deals withthemes of spiritual death and rebirth.
It is notablefor its disjunctive structure, its syncopated rhythms,its wide range of literary allusions, and its incorpo-ration of numerous other languages.
The poem is di-vided into five parts; in total it is 433 lines long, andcontains 3533 tokens, not including the headings.A prominent debate among scholars of The WasteLand concerns whether a single speaker?s voice pre-dominates in the poem (Bedient, 1986), or whetherthe poem should be regarded instead as dramaticor operatic in structure, composed of about twelvedifferent voices independent of a single speaker(Cooper, 1987).
Eliot himself, in his notes to TheWaste Land, supports the latter view by referring to?characters?
and ?personage[s]?
in the poem.One of the poem?s most distinctive voices is thatof the woman who speaks at the end of its secondsection:26I can?t help it, she said, pulling a long face,It?s them pills I took, to bring it off, she said[158?159]Her chatty tone and colloquial grammar and lexisdistinguish her voice from many others in the poem,such as the formal and traditionally poetic voice of anarrator that recurs many times in the poem:Above the antique mantel was displayedAs though a window gave upon the sylvan sceneThe change of Philomel[97?99]While the stylistic contrasts between these andother voices are apparent to many readers, Eliotdoes not explicitly mark the transitions betweenthem.
The goal of the present work is to investigatewhether computational stylistic analysis can identifythe transition between one voice and the next.Our unsupervised approach, informed by researchin topic segmentation (Hearst, 1994) and intrinsicplagiarism detection (Stamatatos, 2009), is basedon deriving a curve representing stylistic change,where the local maxima represent likely transitionpoints.
Notably, our curve represents an amalga-mation of different stylistic metrics, including thosethat incorporate external (extrinsic) knowledge, e.g.vector representations based on larger corpus co-occurrence, which we show to be extremely use-ful.
For development and initial testing we followother work on stylistic inconsistency by using arti-ficial (mixed) poems, but the our main evaluation ison The Waste Land itself.
We believe that even whenour segmentation disagrees with expert human judg-ment, it has the potential to inform future study ofthis literary work.2 Related workPoetry has been the subject of extensive computa-tional analysis since the early days of literary andlinguistic computing (e.g., Beatie 1967).
Most of theresearch concerned either authorship attribution oranalysis of metre, rhyme, and phonetic properties ofthe texts, but some work has studied the style, struc-ture, and content of poems with the aim of better un-derstanding their qualities as literary texts.
Amongresearch that, like the present paper, looks at varia-tion with a single text, Simonton (1990) found quan-titative changes in lexical diversity and semanticclasses of imagery across the components of Shake-speare?s sonnets, and demonstrated correlations be-tween some of these measures and judgments of the?aesthetic success?
of individual sonnets.
Duggan(1973) developed statistical measures of formulaicstyle to determine whether the eleventh-century epicpoem Chanson de Ronald manifests primarily anoral or a written style.
Also related to our work,although it concerned a novel rather than a poem,is that of McKenna and Antonia (2001), who usedprincipal component analysis of lexical frequencyto discriminate different voices (dialogue, interiormonologue, and narrative) and different narrativestyles in sections of Ulysses by James Joyce.More general work on identifying stylistic incon-sistency includes that of Graham et al (2005), whobuilt artificial examples of style shift by concate-nating Usenet postings by different authors.
Fea-ture sets for their neural network classifiers includedstandard textual features, frequencies of functionwords, punctuation and parts of speech, lexical en-tropy, and vocabulary richness.
Guthrie (2008) pre-sented some general methods for identifying stylis-tically anomalous segments using feature vector dis-tance, and tested the effectiveness of his unsuper-vised method with a number of possible stylisticvariations.
He used features such as simple textualmetrics (e.g.
word and sentence length), readabilitymeasures, obscure vocabulary features, frequencyrankings of function words (which were not foundto be useful), and context analysis features fromthe General Inquirer dictionary.
The most effectivemethod ranked each segment according to the city-block distance of its feature vector to the feature vec-tor of the textual complement (the union of all othersegments in the text).
Koppel et al (2011) used asemi-supervised method to identify segments fromtwo different books of the Bible artificially mixedinto a single text.
They first demonstrated that, inthis context, preferred synonym use is a key stylis-tic feature that can serve as high-precision boot-strap for building a supervised SVM classifier onmore general features (common words); they thenused this classifier to provide an initial predictionfor each verse and smooth the results over adjacentsegments.
The method crucially relied on propertiesof the King James Version translation of the text in27order to identify synonym preferences.The identification of stylistic inconsistency or het-erogeneity has received particular attention as acomponent of intrinsic plagiarism detection ?
thetask of ?identify[ing] potential plagiarism by analyz-ing a document with respect to undeclared changesin writing style?
(Stein et al, 2011).
A typical ap-proach is to move a sliding window over the textlooking for areas that are outliers with respect to thestyle of the rest of the text, or which differ markedlyfrom other regions in word or character-trigram fre-quencies (Oberreuter et al, 2011; Kestemont et al,2011).
In particular, Stamatatos (2009) used a win-dow that compares, using a special distance func-tion, a character trigram feature vector at varioussteps throughout the text, creating a style changefunction whose maxima indicate points of interest(potential plagarism).Topic segmentation is a similar problem that hasbeen quite well-explored.
A common thread in thiswork is the importance of lexical cohesion, thougha large number of competing models based on thisconcept have been proposed.
One popular unsu-pervised approach is to identify the points in thetext where a metric of lexical coherence is at a (lo-cal) minimum (Hearst, 1994; Galley et al, 2003).Malioutov and Barzilay (2006) also used a lexi-cal coherence metric, but applied a graphical modelwhere segmentations are graph cuts chosen to max-imize coherence of sentences within a segment, andminimize coherence among sentences in differentsegments.
Another class of approaches is basedon a generative model of text, for instance HMMs(Blei and Moreno, 2001) and Bayesian topic mod-eling (Utiyama and Isahara, 2001; Eisenstein andBarzilay, 2008); in such approaches, the goal is tochoose segment breaks that maximize the probabil-ity of generating the text, under the assumption thateach segment has a different language model.3 Stylistic change curvesMany popular text segmentation methods dependcrucially on a reliable textual unit (often a sentence)which can be reliably classified or compared to oth-ers.
But, for our purposes here, a sentence is bothtoo small a unit ?
our stylistic metrics will be moreaccurate over larger spans ?
and not small enough?
we do not want to limit our breaks to sentenceboundaries.
Generative models, which use a bag-of-words assumption, have a very different problem: intheir standard form, they can capture only lexical co-hesion, which is not the (primary) focus of stylisticanalysis.
In particular, we wish to segment using in-formation that goes beyond the distribution of wordsin the text being segmented.
The model for stylis-tic segmentation we propose here is related to theTextTiling technique of Hearst (1994) and the stylechange function of Stamatatos (2009), but our modelis generalized so that it applies to any numeric met-ric (feature) that is defined over a span; importantly,style change curves represent the change of a set ofvery diverse features.Our goal is to find the precise points in the textwhere a stylistic change (a voice switch) occurs.
Todo this, we calculate, for each token in the text, ameasure of stylistic change which corresponds tothe distance of feature vectors derived from a fixed-length span on either side of that point.
That is, if vi jrepresents a feature vector derived from the tokensbetween (inclusive) indices i and j, then the stylisticchange at point ci for a span (window) of size w is:ci = Dist(v(i?w)(i?1),vi(i+w?1))This function is not defined within w of the edge ofthe text, and we generally ignore the possibility ofbreaks within these (unreliable) spans.
Possible dis-tance metrics include cosine distance, euclidean dis-tance, and city-block distance.
In his study, Guthrie(2008) found best results with city-block distance,and that is what we will primarily use here.
The fea-ture vector can consist of any features that are de-fined over a span; one important step, however, is tonormalize each feature (here, to a mean of 0 and astandard deviation of 1), so that different scaling offeatures does not result in particular features havingan undue influence on the stylistic change metric.That is, if some feature is originally measured to befi in the span i to i+w?1, then its normalized ver-sion f ?i (included in vi(i+w?1)) is:f ?i =fi?
f?
fThe local maxima of c represent our best predic-tions for the stylistic breaks within a text.
However,28stylistic change curves are not well behaved; theymay contain numerous spurious local maxima if alocal maximum is defined simply as a higher valuebetween two lower ones.
We can narrow our def-inition, however, by requiring that the local max-imum be maximal within some window w?.
Thatis, our breakpoints are those points i where, for allpoints j in the span x?w?, x+w?, it is the case thatgi > g j.
As it happens, w?
= w/2 is a fairly goodchoice for our purposes, creating spans no smallerthan the smoothed window, though w?
can be low-ered to increase breaks, or increased to limit them.The absolute height of the curve at each local min-imum offers a secondary way of ranking (and elim-inating) potential breakpoints, if more precision isrequired; however, in our task here the breaks arefairly regular but often subtle, so focusing only onthe largest stylistic shifts is not necessarily desirable.4 FeaturesThe set of features we explore for this task fallsroughly into two categories: surface and extrinsic.The distinction is not entirely clear cut, but we wishto distinguish features that use the basic propertiesof the words or their PoS, which have traditionallybeen the focus of automated stylistic analysis, fromfeatures which rely heavily on external lexical infor-mation, for instance word sentiment and, in partic-ular, vector space representations, which are morenovel for this task.4.1 Surface FeaturesWord length A common textual statistic in reg-ister and readability studies.
Readability, in turn,has been used for plagiarism detection (Stein et al,2011), and related metrics were consistently amongthe best for Guthrie (2008).Syllable count Syllable count is reasonably goodpredictor of the difficulty of a vocabulary, and isused in some readability metrics.Punctuation frequency The presence or absenceof punctuation such as commas, colons, semicolonscan be very good indicator of style.
We also includeperiods, which offer a measure of sentence length.Line breaks Our only poetry-specific feature; wecount the number of times the end of a line appearsin the span.
More or fewer line breaks (that is, longeror shorter lines) can vary the rhythm of the text, andthus its overall feel.Parts of speech Lexical categories can indicate,for instance, the degree of nominalization, which isa key stylistic variable (Biber, 1988).
We collectstatistics for the four main lexical categories (noun,verb, adjective, adverb) as well as prepositions, de-terminers, and proper nouns.Pronouns We count the frequency of first-,second-, and third-person pronouns, which can in-dicate the interactiveness and narrative character ofa text (Biber, 1988).Verb tense Past tense is often preferred in narra-tives, whereas present tense can give a sense of im-mediacy.Type-token ratio A standard measure of lexicaldiversity.Lexical density Lexical density is the ratio of thecount of tokens of the four substantive parts ofspeech to the count of all tokens.Contextuality measure The contextuality mea-sure of Heylighen and Dewaele (2002) is based onPoS tags (e.g.
nouns decrease contextuality, whileverbs increase it), and has been used to distin-guish formality in collaboratively built encyclope-dias (Emigh and Herring, 2005).Dynamic In addition to the hand-picked featuresabove, we test dynamically including words andcharacter trigrams that are common in the text beinganalyzed, particularly those not evenly distributedthroughout the text (we exclude punctuation).
Tomeasure the latter, we define clumpiness as thesquare root of the index of dispersion or variance-to-mean ratio (Cox and Lewis, 1966) of the (text-length) normalized differences between successiveoccurrences of a feature, including (importantly) thedifference between the first index of the text and thefirst occurrence of the feature as well as the last oc-currence and the last index; the measure varies be-tween 0 and 1, with 0 indicating perfectly even dis-tribution.
We test with the top n features based onthe ranking of the product of the feature?s frequency29in the text (tf ) or product of the frequency and itsclumpiness (tf-cl); this is similar to a tf-idf weight.4.2 Extrinsic featuresFor those lexicons which include only lemmatizedforms, the words are lemmatized before their valuesare retrieved.Percent of words in Dale-Chall Word List A listof 3000 basic words that is used in the Dale-ChallReadability metric (Dale and Chall, 1995).Average unigram count in 1T Corpus Anothermetric of whether a word is commonly used.
We usethe unigram counts in the 1T 5-gram Corpus (Brantsand Franz, 2006).
Here and below, if a word is notincluded it is given a zero.Sentiment polarity The positive or negativestance of a span could be viewed as a stylistic vari-able.
We test two lexicons, a hand-built lexicon forthe SO-CAL sentiment analysis system which hasshown superior performance in lexicon-based sen-timent analysis (Taboada et al, 2011), and Senti-WordNet (SWN), a high-coverage automatic lexiconbuilt from WordNet (Baccianella et al, 2010).
Thepolarity of each word over the span is averaged.Sentiment extremity Both lexicons provide ameasure of the degree to which a word is positive ornegative.
Instead of summing the sentiment scores,we sum their absolute values, to get a measure ofhow extreme (subjective) the span is.Formality Average formality score, using a lex-icon of formality (Brooke et al, 2010) built usinglatent semantic analysis (LSA) (Landauer and Du-mais, 1997).Dynamic General Inquirer The General Inquirerdictionary (Stone et al, 1966), which was used forstylistic inconsistency detection by Guthrie (2008),includes 182 content analysis tags, many of whichare relevant to style; we remove the two polarity tagsalready part of the SO-CAL dictionary, and selectothers dynamically using our tf-cl metric.LSA vector features Brooke et al (2010) haveposited that, in highly diverse register/genre corpora,the lowest dimensions of word vectors derived us-ing LSA (or other dimensionality reduction tech-niques) often reflect stylistic concerns; they foundthat using the first 20 dimensions to build their for-mality lexicon provided the best results in a near-synonym evaluation.
Early work by Biber (1988)in the Brown Corpus using a related technique (fac-tor analysis) resulted in discovery of several identi-fiable dimensions of register.
Here, we investigateusing these LSA-derived vectors directly, with eachof the first 20 dimensions corresponding to a sepa-rate feature.
We test with vectors derived from theword-document matrix of the ICWSM 2009 blogdataset (Burton et al, 2009) which includes 1.3 bil-lion tokens, and also from the BNC (Burnard, 2000),which is 100 million tokens.
The length of the vectordepends greatly on the frequency of the word; sincethis is being accounted for elsewhere, we normalizeeach vector to the unit circle.5 Evaluation method5.1 MetricsTo evaluate our method we apply standard topicsegmentation metrics, comparing the segmentationboundaries to a gold standard reference.
The mea-sure Pk, proposed by Beeferman et al (1997), uses aprobe window equal to half the average length of asegment; the window slides over the text, and countsthe number of instances where a unit (in our case,a token) at one edge of the window was predictedto be in the same segment (according to the refer-ence) as a unit at the other edge, but in fact is not; orwas predicted not to be in the same segment, but infact is.
This count is normalized by the total numberof tests to get a score between 0 and 1, with 0 be-ing a perfect score (the lower, the better).
Pevznerand Hearst (2002) criticize this metric because itpenalizes false positives and false negatives differ-ently and sometimes fails to penalize false positivesaltogether; their metric, WindowDiff (WD), solvesthese problems by counting an error whenever thereis a difference between the number of segments inthe prediction as compared to the reference.
Recentwork in topic segmentation (Eisenstein and Barzi-lay, 2008) continues to use both metrics, so we alsopresent both here.During initial testing, we noted a fairly seriousshortcoming with both these metrics: all else be-ing equal, they will usually prefer a system which30predicts fewer breaks; in fact, a system that predictsno breaks at all can score under 0.3 (a very com-petitive result both here and in topic segmentation),if the variation of the true segment size is reason-ably high.
This is problematic because we do notwant to be trivially ?improving?
simply by movingtowards a model that is too cautious to guess any-thing at all.
We therefore use a third metric, whichwe call BD (break difference), which sums all thedistances, calculated as fractions of the entire text,between each true break and the nearest predictedbreak.
This metric is also flawed, because it can betrivially made 0 (the best score) by guessing a breakeverywhere.
However, the relative motion of the twokinds of metric provides insight into whether we aresimply moving along a precision/recall curve, or ac-tually improving overall segmentation.5.2 BaselinesWe compare our method to the following baselines:Random selection We randomly select bound-aries, using the same number of boundaries in thereference.
We use the average over 50 runs.Evenly spaced We put boundaries at equallyspaced points in the text, using the same number ofboundaries as the reference.Random feature We use our stylistic changecurve method with a single feature which is createdby assigning a uniform random value to each tokenand averaging across the span.
Again, we use theaverage score over 50 runs.6 Experiments6.1 Artificial poemsOur main interest is The Waste Land.
It is, however,prudent to develop our method, i.e.
conduct an initialinvestigation of our method, including parametersand features, using a separate corpus.
We do this bybuilding artificial mixed-style poems by combiningstylistically distinct poems from different authors, asothers have done with prose.6.1.1 SetupOur set of twelve poems used for this evaluation wasselected by one of the authors (an English literatureexpert) to reflect the stylistic range and influencesof poetry at the beginning of the twentieth century,and The Waste Land in particular.
The titles wereremoved, and each poem was tagged by an auto-matic PoS tagger (Schmid, 1995).
Koppel et al builttheir composite version of two books of the Bible bychoosing, at each step, a random span length (from auniform distribution) to include from one of the twobooks being mixed, and then a span from the other,until all the text in both books had been included.Our method is similar, except that we first randomlyselect six poems to include in the particular mixedtext, and at each step we randomly select one of po-ems, reselecting if the poem has been used up or theremaining length is below our lower bound.
For ourfirst experiment, we set a lower bound of 100 tokensand an upper bound of 200 tokens for each span; al-though this gives a higher average span length thanthat of The Waste Land, our first goal is to testwhether our method works in the (ideal) conditionwhere the feature vectors at the breakpoint gener-ally represent spans which are purely one poem oranother for a reasonably high w (100).
We create 50texts using this method.
In addition to testing eachindividual feature, we test several combinations offeatures (all features, all surface features, all extrin-sic features), and present the best results for greedyfeature removal, starting with all features (exclud-ing dynamic ones) and choosing features to removewhich minimize the sum of the three metrics.6.1.2 ResultsThe Feature Sets section of Table 1 gives the in-dividual feature results for segmentation of theartificially-combined poems.
Using any of the fea-tures alone is better than our baselines, though someof the metrics (in particular type-token ratio) areonly a slight improvement.
Line breaks are obvi-ously quite useful in the context of poetry (thoughthe WD score is high, suggesting a precision/recalltrade-off), but so are more typical stylistic featuressuch as the distribution of basic lexical categoriesand punctuation.
The unigram count and formal-ity score are otherwise the best two individual fea-tures.
The sentiment-based features did more mod-estly, though the extremeness of polarity was use-ful when paired with the coverage of SentiWord-Net.
Among the larger feature sets, the GI was theleast useful, though more effective than any of the31Table 1: Segmentation accuracy in artificial poemsConfiguration MetricsWD Pk BDBaselinesRandom breaks 0.532 0.465 0.465Even spread 0.498 0.490 0.238Random feature 0.507 0.494 0.212Feature setsWord length 0.418 0.405 0.185Syllable length 0.431 0.419 0.194Punctuation 0.412 0.401 0.183Line breaks 0.390 0.377 0.200Lexical category 0.414 0.402 0.177Pronouns 0.444 0.432 0.213Verb tense 0.444 0.433 0.202Lexical density 0.445 0.433 0.192Contextuality 0.462 0.450 0.202Type-Token ratio 0.494 0.481 0.204Dynamic (tf, n=50) 0.399 0.386 0.161Dynamic (tf-cl, 50) 0.385 0.373 0.168Dynamic (tf-cl, 500) 0.337 0.323 0.165Dynamic (tf-cl, 1000) 0.344 0.333 0.199Dale-Chall 0.483 0.471 0.202Count in 1T 0.424 0.414 0.193Polarity (SO-CAL) 0.466 0.487 0.209Polarity (SWN) 0.490 0.478 0.221Extremity (SO-CAL) 0.450 0.438 0.199Extremity (SWN) 0.426 0.415 0.182Formality 0.409 0.397 0.184All LSA (ICWSM) 0.319 0.307 0.134All LSA (BNC) 0.364 0.352 0.159GI (tf, n=5) 0.486 0.472 0.201GI (tf-cl, 5) 0.449 0.438 0.196GI (tf-cl, 50) 0.384 0.373 0.164GI (tf-cl, 100) 0.388 0.376 0.163CombinationsSurface 0.316 0.304 0.150Extrinsic 0.314 0.301 0.124All 0.285 0.274 0.128All w/o GI, dynamic 0.272 0.259 0.102All greedy (Best) 0.253 0.242 0.099Best, w=150 0.289 0.289 0.158Best, w=50 0.338 0.321 0.109Best, Diff=euclidean 0.258 0.247 0.102Best, Diff=cosine 0.274 0.263 0.145individual features, while dynamic word and char-acter trigrams did better, and the ICWSM LSA vec-tors better still; the difference in size between theICWSM and BNC is obviously key to the perfor-mance difference here.
In general using our tf-clmetric was better than tf alone.When we combine the different feature types, wesee that extrinsic features have a slight edge over thesurface features, but the two do complement eachother to some degree.
Although the GI and dynamicfeature sets do well individually, they do not com-bine well with other features in this unsupervisedsetting, and our best results do not include them.The greedy feature selector removed 4 LSA dimen-sions, type-token ratio, prepositions, second-personpronouns, adverbs, and verbs to get our best result.Our choice of w to be the largest fully-reliable size(100) seems to be a good one, as is our use of city-block distance rather than the alternatives.
Overall,the metrics we are using for evaluation suggest thatwe are roughly halfway to perfect segmentation.6.2 The Waste Land6.2.1 SetupIn order to evaluate our method on The Waste Land,we first created a gold standard voice switch seg-mentation.
Our gold standard represents an amal-gamation, by one of the authors, of several sourcesof information.
First, we enlisted a class of 140 un-dergraduates in an English literature course to seg-ment the poem into voices based on their own intu-itions, and we created a combined student versionbased on majority judgment.
Second, our Englishliterature expert listened to the 6 readings of thepoem included on The Waste Land app (Touch PressLLP, 2011), including two readings by T.S.
Eliot,and noted places where the reader?s voice seemedto change; these were combined to create a readerversion.
Finally, our expert amalgamated these twoversions and incorporated insights from independentliterary analysis to create a final gold standard.We created two versions of the poem for evalua-tion: for both versions, we removed everything butthe main body of the text (i.e.
the prologue, dedi-cation, title, and section titles), since these are notproduced by voices in the poem.
The ?full?
ver-sion contains all the other text (a total of 68 voice32switches), but our ?abridged?
version involves re-moving all segments (and the corresponding voiceswitches, when appropriate) which are 20 or fewertokens in length and/or which are in a languageother than English, which reduces the number ofvoice switches to 28 (the token count is 3179).
Thisversion allows us to focus on the segmentation forwhich our method has a reasonable chance of suc-ceeding and ignore the segmentation of non-Englishspans, which is relatively trivial but yet potentiallyconfounding.
We use w = 50 for the full version,since there are almost twice as many breaks as inthe abridged version (and our artificially generatedtexts).6.2.2 ResultsOur results for The Waste Land are presented in Ta-ble 2.
Notably, in this evaluation, we do not investi-gate the usefulness of individual features or attemptto fully optimize our solution using this text.
Ourgoal is to see if a general stylistic segmentation sys-tem, developed on artificial texts, can be applied suc-cessfully to the task of segmenting an actual stylis-tically diverse poem.
The answer is yes.
Althoughthe task is clearly more difficult, the results for thesystem are well above the baseline, particularly forthe abridged version.
One thing to note is that usingthe features greedily selected for the artificial sys-tem (instead of just all features) appears to hinder,rather than help; this suggests a supervised approachmight not be effective.
The GI is too unreliable tobe useful here, whereas the dynamic word and tri-gram features continue to do fairly well, but they donot improve the performance of the rest of the fea-tures combined.
Once again the LSA features seemto play a central role in this success.
We manuallycompared predicted with real switches and foundthat there were several instances (corresponding tovery clear voices switches in the text) which werenearly perfect.
Moreover, the model did tend to pre-dict more switches in sections with numerous realswitches, though these predictions were often fewerthan the gold standard and out of sync (because thesampling windows never consisted of a pure style).7 ConclusionIn this paper we have presented a system for auto-matically segmenting stylistically inconsistent textTable 2: Segmentation accuracy in The Waste LandConfiguration MetricsWD Pk BDFull textBaselinesRandom breaks 0.517 0.459 0.480Even spread 0.559 0.498 0.245Random feature 0.529 0.478 0.314System (w=50)Table 1 Best 0.458 0.401 0.264GI 0.508 0.462 0.339Dynamic 0.467 0.397 0.257LSA (ICWSM) 0.462 0.399 0.280All w/o GI 0.448 0.395 0.305All w/o dynamic, GI 0.456 0.394 0.228Abridged textBaselinesRandom breaks 0.524 0.478 0.448Even spread 0.573 0.549 0.266Random feature 0.525 0.505 0.298System (w=100)Table 1 Best 0.370 0.341 0.250GI 0.510 0.492 0.353Dynamic 0.415 0.393 0.274LSA (ICWSM) 0.411 0.390 0.272All w/o GI 0.379 0.354 0.241All w/o dynamic, GI 0.345 0.311 0.208and applied it to The Waste Land, a well-knownpoem in which stylistic variation, in the form of dif-ferent ?voices?, provides an interesting challenge toboth human and computer readers.
Our unsuper-vised model is based on a stylistic change curve de-rived from feature vectors.
Perhaps our most inter-esting result is the usefulness of low-dimension LSAvectors over surface features such as words and tri-gram characters as well as other extrinsic featuressuch as the GI dictionary.
In both The Waste Landand our development set of artificially combined po-ems, our method performs well above baseline.
Oursystem could probably benefit from the inclusion ofmachine learning, but our main interest going for-ward is the inclusion of additional features ?
in par-ticular, poetry-specific elements such as alliterationand other more complex lexicogrammatical features.33AcknowledgmentsThis work was financially supported by the Natu-ral Sciences and Engineering Research Council ofCanada.ReferencesStefano Baccianella, Andrea Esuli, and Fabrizio Sebas-tiani.
2010.
SentiWordNet 3.0: An enhanced lexicalresource for sentiment analysis and opinion mining.In Proceedings of the 7th conference on InternationalLanguage Resources and Evaluation (LREC?10), Val-letta, Malta, May.Bruce A. Beatie.
1967.
Computer study of medieval Ger-man poetry: A conference report.
Computers and theHumanities, 2(2):65?70.Calvin Bedient.
1986.
He Do the Police in DifferentVoices: The Waste Land and its protagonist.
Univer-sity of Chicago Press.Doug Beeferman, Adam Berger, and John Lafferty.1997.
Text segmentation using exponential models.
InIn Proceedings of the Second Conference on EmpiricalMethods in Natural Language Processing (EMNLP?97), pages 35?46.Douglas Biber.
1988.
Variation Across Speech and Writ-ing.
Cambridge University Press.David M. Blei and Pedro J. Moreno.
2001.
Topic seg-mentation with an aspect hidden Markov model.
InProceedings of the 24th annual international ACM SI-GIR conference on Research and Development in In-formation Retrieval, SIGIR ?01, pages 343?348.Thorsten Brants and Alex Franz.
2006.
Web 1T 5-gramCorpus Version 1.1.
Google Inc.Julian Brooke, Tong Wang, and Graeme Hirst.
2010.
Au-tomatic acquisition of lexical formality.
In Proceed-ings of the 23rd International Conference on Compu-tational Linguistics (COLING ?10).Lou Burnard.
2000.
User reference guide for BritishNational Corpus.
Technical report, Oxford University.Kevin Burton, Akshay Java, and Ian Soboroff.
2009.
TheICWSM 2009 Spinn3r Dataset.
In Proceedings of theThird Annual Conference on Weblogs and Social Me-dia (ICWSM 2009), San Jose, CA.John Xiros Cooper.
1987.
T.S.
Eliot and the politics ofvoice: The argument of The Waste Land.
UMI Re-search Press, Ann Arbor, Mich.David R. Cox and Peter A.W.
Lewis.
1966.
The Sta-tistical Analysis of Series of Events.
Monographs onStatistics and Applied Probability.
Chapman and Hall.Edgar Dale and Jeanne Chall.
1995.
Readability Re-visited: The New Dale-Chall Readability Formula.Brookline Books, Cambridge, MA.Joseph J. Duggan.
1973.
The Song of Roland: Formulaicstyle and poetic craft.
University of California Press.Jacob Eisenstein and Regina Barzilay.
2008.
Bayesianunsupervised topic segmentation.
In Proceedings ofthe Conference on Empirical Methods in Natural Lan-guage Processing (EMNLP ?08, EMNLP ?08, pages334?343.William Emigh and Susan C. Herring.
2005.
Collabo-rative authoring on the web: A genre analysis of on-line encyclopedias.
In Proceedings of the 38th AnnualHawaii International Conference on System Sciences(HICSS ?05), Washington, DC.Michel Galley, Kathleen McKeown, Eric Fosler-Lussier,and Hongyan Jing.
2003.
Discourse segmentation ofmulti-party conversation.
In Proceedings of the 41stAnnual Meeting of the Association for ComputationalLinguistics (ACL ?03), ACL ?03, pages 562?569.Neil Graham, Graeme Hirst, and Bhaskara Marthi.
2005.Segmenting documents by stylistic character.
NaturalLanguage Engineering, 11(4):397?415.David Guthrie.
2008.
Unsupervised Detection ofAnomalous Text.
Ph.D. thesis, University of Sheffield.Marti A. Hearst.
1994.
Multi-paragraph segmentationof expository text.
In Proceedings of the 32nd AnnualMeeting of the Association for Computational Linguis-tics (ACL ?94), ACL ?94, pages 9?16.Francis Heylighen and Jean-Marc Dewaele.
2002.
Vari-ation in the contextuality of language: An empiricalmeasure.
Foundations of Science, 7(3):293?340.Mike Kestemont, Kim Luyckx, and Walter Daelemans.2011.
Intrinsic plagiarism detection using charactertrigram distance scores.
In Proceedings of the PAN2011 Lab: Uncovering Plagiarism, Authorship, andSocial Software Misuse.Moshe Koppel, Navot Akiva, Idan Dershowitz, andNachum Dershowitz.
2011.
Unsupervised decompo-sition of a document into authorial components.
InProceedings of the 49th Annual Meeting of the Associ-ation for Computational Linguistics (ACL ?11).Thomas K. Landauer and Susan Dumais.
1997.
A so-lution to Plato?s problem: The latent semantic analysistheory of the acquisition, induction, and representationof knowledge.
Psychological Review, 104:211?240.Igor Malioutov and Regina Barzilay.
2006.
Minimumcut model for spoken lecture segmentation.
In Pro-ceedings of the 44th Annual Meeting of the Associa-tion for Computational Linguistics (ACL ?06), pages25?32.C.
W. F. McKenna and A. Antonia.
2001.
The statisticalanalysis of style: Reflections on form, meaning, andideology in the ?Nausicaa?
episode of Ulysses.
Liter-ary and Linguistic Computing, 16(4):353?373.34Gabriel Oberreuter, Gaston L?Huillier, Sebastia?n A.
R?
?os,and Juan D. Vela?squez.
2011.
Approaches for intrin-sic and external plagiarism detection.
In Proceedingsof the PAN 2011 Lab: Uncovering Plagiarism, Author-ship, and Social Software Misuse.Lev Pevzner and Marti A. Hearst.
2002.
A critique andimprovement of an evaluation metric for text segmen-tation.
Computational Linguistics, 28:19?36, March.Helmut Schmid.
1995.
Improvements in part-of-speechtagging with an application to German.
In Proceed-ings of the ACL SIGDAT Workshop, pages 47?50.Dean Keith Simonton.
1990.
Lexical choices and aes-thetic success: A computer content analysis of 154Shakespeare sonnets.
Computers and the Humanities,24(4):251?264.Efstathios Stamatatos.
2009.
Intrinsic plagiarism detec-tion using character n-gram profiles.
In Proceedingsof the SEPLN?09 Workshop on Uncovering Plagia-rism, Authorship and, Social Software Misuse (PAN-09), pages 38?46.
CEUR Workshop Proceedings, vol-ume 502.Benno Stein, Nedim Lipka, and Peter Prettenhofer.
2011.Intrinsic plagiarism analysis.
Language Resourcesand Evaluation, 45(1):63?82.Philip J.
Stone, Dexter C. Dunphy, Marshall S. Smith,and Daniel M. Ogilivie.
1966.
The General In-quirer: A Computer Approach to Content Analysis.MIT Press.Maite Taboada, Julian Brooke, Milan Tofiloski, KimberlyVoll, and Manifred Stede.
2011.
Lexicon-based meth-ods for sentiment analysis.
Computational Linguis-tics, 37(2):267?307.Touch Press LLP.
2011.
The Waste Landapp.
http://itunes.apple.com/ca/app/the-waste-land/id427434046?mt=8 .Masao Utiyama and Hitoshi Isahara.
2001.
A statisticalmodel for domain-independent text segmentation.
InProceedings of the 39th Annual Meeting of the Associ-ation for Computational Linguistics (ACL ?01), pages499?506.35
