Proceedings of the Workshop on BioNLP: Shared Task, pages 128?136,Boulder, Colorado, June 2009. c?2009 Association for Computational LinguisticsAnalyzing text in search of bio-molecular events:a high-precision machine learning frameworkSofie Van Landeghem1,2, Yvan Saeys1,2, Bernard De Baets3, Yves Van de Peer1,21.
Dept.
of Plant Systems Biology, VIB2.
Dept.
of Plant Biotechnology and Genetics, Ghent University3.
Dept.
of Applied Mathematics, Biometrics and Process Control, Ghent UniversityB-9000 Gent, Belgiumyves.vandepeer@psb.vib-ugent.beAbstractThe BioNLP?09 Shared Task on Event Ex-traction is a challenge which concerns the de-tection of bio-molecular events from text.
Inthis paper, we present a detailed account ofthe challenges encountered during the con-struction of a machine learning framework forparticipation in this task.
We have focusedour work mainly around the filtering of falsepositives, creating a high-precision extractionmethod.
We have tested techniques such asSVMs, feature selection and various filters fordata pre- and post-processing, and report onthe influence on performance for each of them.To detect negation and speculation in text,we describe a custom-made rule-based sys-tem which is simple in design, but effective inperformance.1 IntroductionBioNLP recently emerged from the combined exper-tise of molecular biology and computational linguis-tics.
At first, the community was mainly focusedon named entity recognition (NER) and simple rela-tion extraction, such as protein-protein interactions(Plake et al, 2005; Giuliano et al, 2006; Fundel etal., 2007; Saetre et al, 2008).
However, the futureof BioNLP lies in the ability to extract more com-plex events from text, in order to fully capture allavailable information (Altman et al, 2008).Two recent community-wide challenges, Biocre-ative I (Hirschman et al, 2005) and II (Krallinger etal., 2008) have shown their merits by providing com-mon benchmarking data and a meaningful compari-son of various techniques.
In contrast to the mono-lithic Biocreative tasks, the BioNLP?09 Shared Taskhas a more modular nature (Kim et al, 2009).
Itis not concerned with named entity recognition ornormalization, but focuses on the task of event ex-traction itself.This article is organized as follows: we first de-scribe the Shared Task in a little more detail.
Next,we present the methods used in our machine learn-ing framework, carefully discussing our choices indesign and their influence on performance.
We thenpresent the final results of our approach.
Finally, wedraw conclusions from our participation in this task,and suggest some future work for our own researchas well as on a community-wide level.2 BioNLP?09 Shared Task2.1 SubtasksThe BioNLP?09 Shared Task was divided into threesubtasks, of which only the first one was mandatory.We have participated in tasks 1 and 3, and will there-fore only briefly discuss task 2.
In accordance withthe provided gold entity annotation, we will refer toall genes and gene products as proteins.Task 1 represents the core of the challenge: de-tection and characterization of bio-molecular eventsfrom text.
There are 9 distinct event types.
Sixevents influence proteins directly, and we will referto them as ?Protein events?.
Five of them are unary:Localization, Gene expression, Transcription, Pro-tein catabolism and Phosphorylation.
The Bindingevent can be related to one protein (e.g.
protein-DNA binding), two proteins (e.g.
protein-protein in-128teraction) or more (e.g.
a complex).
On top of theseevent types, there are three Regulation events: Reg-ulation, Positive regulation and Negative regulation.Each of them can be unary or binary.
In the lattercase, an extra argument specifying the cause of theregulation is added.
Each argument of a Regulationevent can be either a protein or any other event.Participants in task 2 had to recognise extra ar-guments for the events from task 1.
For example,the cellular location should be added to a Localiza-tion event, and Site arguments had to be specifiedfor Phosphorylation, Binding and Regulation.Finally, task 3 was about detecting negation andspeculation in text.2.2 ExamplesSuppose we are dealing with this sentence:?MAD-3 masks the nuclear localization signalof p65 and inhibits p65 DNA binding.
?There are three proteins in this sentence:?
T1 : Protein : ?MAD-3??
T2 : Protein : ?p65?
(first occurrence)?
T3 : Protein : ?p65?
(second occurrence)There are also three triggers, which are defined bya contiguous stream of characters from the originaltext, and point to a specific event type:?
T27 : Negative regulation : ?masks??
T29 : Negative regulation : ?inhibits??
T30 : Binding : ?binding?In this example, we see there is one binding eventwhich involves trigger T30 and protein T3.
Further-more, this binding event is being influenced by pro-tein T1, using trigger T29 which implies a Negativeregulation event.
Similarly, T1 has a negative effecton protein T2, which is expressed by trigger T27.When participating in subtask 2, one should also findthe extra Site argument T28 for this last event:?
T28 : Entity : ?nuclear localization signal?Now look at the following example:?NF-kappa B p50 is not directly regulated byI kappa B.?This sentence expresses a Regulation event involv-ing the trigger ?regulated?
and protein ?p50?.
Partic-ipation in subtask 3 requires detecting the negationof this event.2.3 DatasetsBoth training and testing data consist of PubMed ab-stracts extracted from the GENIA corpus (Kim et al,2008).
All proteins are annotated and extra informa-tion is provided, such as analysis of sentence seg-mentation and tokenization, dependency graphs andphrase structure parses.The training data consists of 800 articles.
Thedevelopment data contains an additional 150 arti-cles with gold standard annotations.
During devel-opment (6 weeks), the system?s performance couldbe estimated with this dataset, using an online sub-mission system.
Participants had one week time toprovide predictions for the final test dataset of 260articles.3 MethodsOur machine learning framework is tailored towardsspecific properties of different events, but is still keptsufficiently general to deal with new event types.The nature of the event extraction task leads to un-balanced datasets, with much more negative exam-ples than positive ones.
This is due to the factthat proteins could be involved in all possible eventtypes, and each of the words in the text could be atrigger for an event.
Finding the right events thusseems like looking for a needle in a haystack, whichis why it is crucial to start with a good definitionof candidate instances.
This problem has motivatedus to try and filter out as many irrelevant negativeinstances as possible by introducing specific pre-processing methods and filters.
This reduces un-balancedness of the datasets and will lead to betterprecision as there will be less false positives (FPs).High-precision systems produce less noise and canbe considered to be more useful when a researcheris trying to extract reliable interaction networks fromtext.
There is a considerable degree of informationredundancy in the original PubMed articles, whichmakes up for low recall when using the system ina real-world application.
We have also tested a fewpost-processing techniques in order to remove FPsafter classification.Figure 1 shows a high-level overview of the dif-ferent modules in our framework.
More details aredescribed in the next sections.129PredictionsTraining dataTriggerdictionaries InstancecreationPost-processingmodulesTesting dataFeaturegeneration Classification(SVM)Resultssubtask 1Rule based systemfor Negation and SpeculationResultssubtask 31432Figure 1: High-level overview of the modules used in ourframework.3.1 ParsingFor sentence segmentation, we made use of the pro-vided tokenization files.
Analysis of part-of-speechtags and dependency graphs was done using theStanford parser (de Marneffe et al, 2006).3.2 Dictionaries of triggersFrom the training data, we automatically compileddictionaries of triggers for each event type, applyingthe Porter stemming algorithm (Porter, 1980) to eachtrigger.
This resulted in some entries in the dictio-naries which were of limited use, such as ?through?for Binding, or ?are?
for Localization.
Such wordsare too general or too vague, and lead to many neg-ative and irrelevant instances.
For this reason, wemanually cleaned the dictionaries, only keeping spe-cific triggers for each event type (e.g.
?interaction?for Binding and ?secretion?
for Localization).During development, we noticed a significantdifference between the triggers for unary Bind-ing events (e.g.
?homodimer?, ?binding site?)
andthose for Binding events with multiple arguments(e.g.
?heterodimer?, ?complex?).
This motivated ourchoice to create two separate dictionaries and classi-fiers, thus discarding irrelevant candidate instances.Such an example would be a candidate binary Bind-ing event with the trigger ?homodimer?, while ho-modimerization is clearly a unary event.
In therest of this article, we will refer to these two eventtypes as Single binding and Multiple binding events.The revision of the dictionaries resulted in a signifi-cant drop in the number of Binding instances in thetraining data, and improved the balancedness of thedatasets: from a total of 34 612 instances (of which2% positives) to 4708 Single binding instances (11%positives) and 3861 Multiple binding instances (5%positives).Following the same reasoning, Regulation wasalso divided into unary and binary events.
Further-more, we have carefully analysed the nature of Bi-nary regulation events, and noticed that a vast major-ity of these events had a protein in the ?cause?
slot.We decided to split up the dictionaries of Binary reg-ulations accordingly, differentiating between regu-lation events caused by proteins and those causedby other events.
This keeps the more general words(e.g.
?causes?)
out of the dictionaries of events reg-ulated by proteins (e.g.
?response?
), again resultingin better balance of the datasets.3.3 Instance creationIn a machine learning framework, a classifier triesto distinguish between positive instances (true bio-molecular events) and negative instances (candidateswhich should be discarded).
To run such a frame-work, one has to define candidate instances automat-ically by scanning the text.
The first step towardsinstance creation consists of looking up triggersin text, using the constructed dictionaries for eachevent type.
To this end, we have implemented a fastalgorithm using Radix trees1.
Next, candidate argu-ments have to be found.
Initially, we have selectedall (combinations of) proteins that were mentionedin the same sentence.
However, this may result ina lot of negative and irrelevant instances, mainly inlong sentences.
This is why we have implemented aNegative-instances (NI) filter, which checks whetherthe length of the sub-sentence spanned by a candi-date event does not exceed a certain value.
Figure 2shows the distribution of positive and negative Mul-tiple binding events, according to the length of therelevant sub-sentence.
It seems reasonable to onlykeep instances with a sub-sentence of less than 175characters, as this includes almost all positive exam-ples, while at the same time removing a significantamount of irrelevant negatives.1Java implementation by Tahseen Ur Rehman,http://code.google.com/p/radixtree/130Figure 2: Distribution of Multiple binding instances, ac-cording to the length of the sub-sentence (training data).Furthermore, for each instance, a minimal sub-graph of the dependency graph was extracted, con-taining the full trigger and all arguments.
The size ofthis subgraph was also used as a parameter for the NIfilter, as positive instances are usually expressed in asmaller subtree than negative examples.
In Figure 3we see how the subgraphs of positive Multiple bind-ing instances are never larger than 10 edges, whilenegative instances can contain up to 18 edges.
In thiscase, only keeping instances with subgraphs smallerthan 8 edges will discard many irrelevant negatives,while keeping most of the positive instances.The NI filter further reduces noise in the data andunbalancedness.
We now end up with 4070 Sin-gle binding instances (of which 13% positives) and2365 Multiple binding instances (8% positives).
Ta-ble 1 shows the final distribution of instances for allevent types.
Transcription, Localization and Mul-tiple binding have the lowest percentage of posi-tive instances, ranging between 7% and 8%, whilePhosphorylation has up to 48% positive instances.
Itshould be noted that the number of positive instancesin Table 1 is lower than the actual number of posi-tive examples in the training set, due to limitationsof our instance definition method.
However, a studyregarding maximal recall shows that we do not re-move too many true positives (TPs) (more details inSection 4.1).3.4 Feature generationFor feature generation, we base our method on therich feature set we previously used in our work onFigure 3: Distribution of Multiple binding instances, ac-cording to the size of the subgraph (training data).protein-protein interactions (Van Landeghem et al,2008).
The goal of that study was to extract bi-nary relations and only one path in the dependencygraph was analyzed for each instance.
In the presentwork however, we are processing larger and morecomplex subgraphs.
This is why we have excluded?edge walks?, i.e.
patterns of two consecutive edgesand their common vertex (e.g.
?nsubj VBZ prep?
).To compensate for the loss of information, we haveadded trigrams to the feature set.
These are threestemmed consecutive words from the sub-sentencespanning the event, e.g.
?by induc transcript?, whichis the stemmed variant of ?by inducing transcrip-tion?.
Other features include?
A BOW-approach by looking at all the wordswhich appear at a vertex of the subgraph.
Thisautomatically excludes uninformative wordssuch as prepositions.?
Lexical and syntactic information of triggers.?
Size of the subgraph.Event type # neg.
# pos.
% pos.inst.
inst.
inst.Localization 3415 249 7Single binding 3548 522 13Multiple binding 2180 185 8Gene expression 5356 1542 22Transcription 6930 489 7Protein catabolism 175 96 35Phosphorylation 163 153 48Table 1: Distribution of instances131Event type FeaturesLocalization 18 121Single binding 21 332Multiple binding 11 228Gene expression 31 332Transcription 30 306Protein catabolism 1 883Phosphorylation 2 185Table 2: Dimensionality of the datasets?
Length of the sub-sentence.?
Extra features for Regulation events, storingwhether the arguments are proteins or events,specifying the exact event type.?
Vertex walks which consist of two verticesand their connecting edge.
For these patterns,both lexical as well as syntactic information iskept.
When using lexical information, proteinnames and triggers were blinded in order to ex-tract more general patterns (e.g.
?trigger nsubjprotx?
which expresses that the given protein isthe subject of a trigger).
Blinding avoids over-fitting of the classifier.In the training phase, each instance generates dif-ferent patterns, and each pattern is stored as a nu-meric feature in the feature vector.
During testing,we count how many times each feature is found foreach instance.
This results in very sparse and high-dimensional datasets.
Table 2 shows the dimen-sionality of the datasets for all event types.
Proteincatabolism has the lowest dimensionality with 1883features, while Transcription and Gene expressionproduce over 30 000 features.3.5 ClassificationTo process our dataset, we had to find a classi-fier able to deal with thousands of instances, thou-sands of features, and an unbalancedness of up to93% negative instances.
We have used the Lib-SVM implementation as provided by WEKA2, as afew preliminary tests using different classifiers (suchas Random Forests) gave worse results.
We inte-grated an internal 5-fold cross-validation loop onthe training portion of the data to determine a use-ful C-parameter.
All other parameters were left un-2Available at http://www.cs.waikato.ac.nz/ml/weka/changed, including the type of kernel which is a ra-dial basis function by default.In combination with the LibSVM, we have triedapplying feature selection (FS).
At first sight, FS didnot seem to lead to gain in performance, although wewere not able to test this hypothesis more thoroughlydue to time limitations of the task.
Finally, we havealso tested the influence of assigning higher weightsto positive training instances, in order to make upfor the unbalanced nature of the data, but this hadalmost no effect on overall performance.3.6 Post-processingWe have implemented a few custom-made post-processing modules, designed to further reduce FPsand improve precision of our method.
We reporthere on their influence on performance.Overlapping triggers of different event typesPredictions for different event types were processedin parallel and merged afterwards.
This means thattwo triggers of different event types might overlap,based on the same words in the text.
However, aword in natural language can only have one mean-ing at a time.
When two such triggers lead to eventswith different event types, this means that some ofthese events should be FPs.
When testing on the de-velopment data, we found a few predictions wherethis problem occurred.
For example, the trigger ?ex-pression?
can lead to both a Transcription and a Geneexpression event, but not at the same time.
In such acase, we only select the prediction with the highestSVM score.
However, thanks to careful construc-tion of the dictionaries (Section 3.2), their mutualoverlap is rather small, and thus this post-processingmodule has almost no influence on performance.Events based on the same triggerOne trigger might be involved in different eventsfrom the same event type.
For example, the sentence?it induces expression of STAT5-regulated genes inCTLL-2, i.e.
beta-casein, and oncostatin M (OSM)?mentions two Gene expression events based on thetrigger ?expression?, one involving beta-casein, andone involving OSM.
For these two events, the sub-graphs will be very similar, resulting in similar fea-tures and SVM scores.
However, often a triggeronly leads to one true event, while all other candi-132dates from the same event type are false positives.We have carefully benchmarked this hypothesis, andfound that for Protein catabolism and Phosphoryla-tion, we could achieve better performance by onlykeeping the top-ranked prediction.
Up to 5% in F-score could be gained for these events.
This is due tothe fact that for these two event types, usually onlyone true event is linked to each trigger.3.7 NegationWe found that there are three major categories ofevent negation:1.
A negation construct is found in the close vicin-ity of the trigger (e.g.
?no?, ?failure to?).2.
A trigger already expresses negation by itself(e.g.
?non-expressing?, ?immobilization?).3.
A trigger in a certain sentence expresses bothpositive as negative events.
In this case, thepattern ?but not?
is often used (e.g.
?overexpres-sion of Vav, but not SLP-76, augments CD28-induced IL-2 promoter activity?
).We have created a custom-made rule-based systemto process these three categories.
The rules makeuse of small dictionaries collected from the train-ing data.
For rule 1, we checked whether a nega-tion word appears right in front of the trigger.
Toapply rule 2, we used a list of inherent negative trig-gers deduced from the training set.
For rule 3, wechecked whether we could find patterns such as ?butnot?
or ?whereas?, negating only the event involvingthe protein mentioned right after that pattern.3.8 SpeculationWe identified two major reasons why the descriptionof an event could be regarded as speculation insteadof a mere fact.
These categories are:1.
Uncertainty: the authors state the interactionsor events they are investigating, without know-ing the true results (yet).
This is often indicatedwith expressions such as ?we have examinedwhether (...)?.2.
Hypothesis: authors formulate a hypothesis totry and explain the results of an experiment.Specific speculation words such as ?might?
or?appear to?
often occur right before the trigger.Event type Maximal recallLocalization 84.91 %Binding 78.23 %Gene expression 91.57 %Transcription 90.24 %Protein catabolism 100 %Phosphorylation 95.74 %Regulation 46.15 %Positive regulation 39.71 %Negative regulation 43.88 %Negation 28.97 %Speculation 25.26 %Table 3: Maximal recall for the development dataSimilar to detecting negation, we compiled a list ofrelevant expressions from the training data and haveused this to implement a simple rule-based system.For rule 1, we checked the appearance of such an ex-pression in a range of 60 characters before the trig-ger and up to 60 characters after the trigger.
Rule 2was applied on a smaller range: only 20 charactersright before the trigger were scanned.4 ResultsOur final machine learning framework consists of allthe modules described in the previous section.
Tosummarize, these design choices were made: auto-matically compiled dictionaries which were cleanedmanually, usage of the NI filter, no weights on pos-itive instances, a LibSVM classifier and no featureselection.
We used both post-processing modules,but the second one only for Protein catabolism andPhosphorylation events.
The best SVM cut-offswere chosen by determining the best F-score on thedevelopment data for each classifier.4.1 Benchmarking on the development dataProtein eventsTo evaluate maximal recall of our instance extrac-tion method, we executed an evaluation using anall-true classifier.
As can be seen in Table 3, maxi-mal recall is quite high for almost all Protein events,meaning that dictionary coverage is good, our NI fil-ter does not remove too many TPs, and not too manyevents are expressed across sentences and thus notpicked up by our method.
Binding and Localizationare the only events with less than 90% recall.
Due to133Event type Recall Precision F-scoreLocalization 77.36 91.11 83.67Binding 45.16 37.21 40.80Gene expression 70.79 79.94 75.08Transcription 60.98 75.76 67.57Protein catabolism 80.95 89.47 85.00Phosphorylation 68.09 88.89 77.11Total 62.45 64.40 63.41Regulation 23.67 41.67 30.19Positive regulation 21.56 38.00 27.51Negative regulation 30.10 41.26 34.81Total 23.63 39.39 29.54Task 1 41.03 53.50 46.44Negation 15.89 45.95 23.61Speculation 20.00 26.87 22.93Total 17.82 33.65 23.30Task 3 38.77 52.24 44.51Table 4: Final performance of all events for the develop-ment datatime constraints, we were not able to test which ofour modules leads to false negative (FN) instances.For each event, we have determined the best clas-sifier cut-offs to achieve maximal F-score.
Resultsof the final performance for the predictions of Pro-tein events on the development data, can be seen inTable 4.
For most events, we achieve very high pre-cision, thanks to our careful definition of instancesin combination with the NI-filter.Looking at the F-measures, Transcription, Geneexpression and Phosphorylation all perform be-tween 67 and 77%, while Localization and Proteincatabolism have an F-score of more than 83%.
It be-comes clear that Binding is the most difficult eventtype, with a performance of 41% F. Unfortunately,this group of events contains 44% of all Proteinevents, greatly influencing total performance.
Aver-age performance of predicting Protein events resultsin 63.41% F.RegulationWhen evaluating the predictions of Regulationevents, one has to take into account that the perfor-mance greatly depends on the ability of our systemto predict Protein events.
Indeed, one FN Proteinevent can lead to multiple FN Regulation events, andthe same holds for FPs.
Furthermore, we do not tryto extract events across sentences, which may leadto more FNs.
To study maximal recall of the Regu-lation events, we have again applied an all-true clas-sifier.
Table 3 shows that the highest possible recallof the Regulation events is never above 50%, greatlylimiting the performance of our method.As regulation events can participate in new regu-lation events, one should run the regulation pipelinerepeatedly until no more new events are found.
Inour experiments, we have found that even the first re-cursive run did not lead to much better performance,and only a few more Regulation events were found.Final results are shown in Table 4.
With recallbeing rather low, between 21% and 30%, at leastwe achieve relatively good precision: around 40%for each of the three regulation types.
On average,the F-score is almost 30% for the regulation events,which is significantly lower than the performance ofProtein events.
On average, we obtain an F-score of46.44% on the development data for task 1.Negation and speculationThe performance of this subtask depends heavily onthe performance of subtask 1.
Again we have ap-plied an all-true classifier to determine maximal re-call (Table 3).
Less than 30% of the events necessaryfor task 3 can be found with our setup; all of theseFNs are due to FNs in task 1.Final results are shown in Table 4.
Performanceof around 23% F-score is achieved on the develop-ment data.
We take into consideration that accordingto the maximal recall study, only 29% of the neces-sary events for Negation were extracted by task 1.
Inthe final results, 16% of all the negation events werefound.
This means that our rule-based method byitself achieves about 55% recall for Negation.
Sim-ilarly, the system has a recall of 80% for Specula-tion when only considering events found in task 1.We conclude that our simple rule-based system per-forms reasonably well.4.2 Scoring and ranking on final test setFinally, our system was applied to the test data.Achieving a global F-score of 40.54% for subtask 1,we obtain a 5th place out of 24 participating teams.For subtask 3 of finding negation and speculation,we obtain a second place with a 37.80% F-score.Final results for each of the event types are shownin Table 5.
As on the development data, we see134Event type Recall Precision F-scoreLocalization 43.68 78.35 56.09Binding 38.04 38.60 38.32Gene expression 59.42 81.56 68.75Transcription 39.42 60.67 47.79Protein catabolism 64.29 60.00 62.07Phosphorylation 56.30 89.41 69.09Total 50.75 67.24 57.85Regulation 10.65 22.79 14.52Positive regulation 17.19 32.19 22.41Negative regulation 22.96 35.22 27.80Total 17.36 31.61 22.41Task 1 33.41 51.55 40.54Negation 10.57 45.10 17.13Speculation 8.65 15.79 11.18Total 9.66 24.85 13.91Task 3 30.55 49.57 37.80Table 5: Performance of all events for the final test setthat the Binding event performs worst, and the sametrend is found when analyzing results of other teams.In general however, we achieve a high precision:67% for Protein events, 52% on average on subtask1, and 50% on average on subtask 3.
Another trendwhich is confirmed by other teams, is the fact thatpredicting Protein events achieves much higher per-formance than the prediction of Regulation events.Compared to our results on the development data(Table 4), we notice a drop of performance for theProtein events of about 0.06F.
This loss is prop-agated to the Regulation events and to Negationand Speculation, each also performing about 0.06Fworse than on the development data.
We believethis drop in performance might be due to overfittingof the system during training.
It is difficult to findthe best SVM cut-offs to achieve maximal perfor-mance.
We have tuned these cut-offs on the devel-opment data, but they might not be ideal for the finaltest set.
For this reason, we believe that it might bemore representative to use evaluation schemes suchas the area under the receiver operating character-istics curve (AUC) measure (Hanley and McNeil,1982; Airola et al, 2008).5 Conclusions and future workWe have participated in the BioNLP?09 Shared Task,joining the rest of the community in the progressionof relation-based extraction towards the extractionof events from bio-molecular texts.
Out of the 24participants, we see quite some teams with a verygood performance, with the highest result achievingan F-score of nearly 52%.
We believe the commu-nity is off to a good start in this task, and we hopework in this field will continue afterwards.In our own study, we notice that the taskof extracting bio-molecular events leads to high-dimensional and unbalanced datasets.
We carefullydesigned our system in order to improve balance ofthe datasets and to avoid false positives.
For featuregeneration, we have made use of a modified bag-of-words approach, included trigrams extracted fromthe sentence, and derived patterns from dependencygraphs.
Our high-precision framework achieves afifth position out of 24 participating teams in sub-task 1, and second position out of six for subtask 3.In the future, we would like to investigate the useof feature selection to produce better models for theclassification task.
Another interesting topic wouldbe how to combine coreference resolution with de-pendency graphs in order to process events whichspan multiple sentences in text.For the community as a whole, we think the nextstep would be to work on full articles instead of mereabstracts.
Also, it might be interesting to investigatethe use of text-bound annotation which is not neces-sarily contiguous, such as is the case in the Bioinfercorpus (Pyysalo et al, 2007), to be able to fully cap-ture the semantics of a certain event.AcknowledgmentsSVL and YS would like to thank the Research Foun-dation Flanders (FWO) for funding their research.Furthermore, the authors would like to thank the or-ganizers of the BioNLP?09 Shared Task for offeringto the community a very valuable and well organizedtask about event extraction.
We believe careful eval-uation and discussion of the results will lead to asignificant step forward in this domain.135ReferencesA.
Airola, S. Pyysalo, J. Bjo?rne, T. Pahikkala, F. Gin-ter and T. Salakoski.
2008.
All-paths graph kernelfor protein-protein interaction extraction with evalua-tion of cross-corpus learning.
BMC Bioinformatics,9(Suppl 11):S2R.B.
Altman, C.M.
Bergman, J. Blake, C. Blaschke, A.Cohen, F. Gannon, L. Grivell, U. Hahn, W. Hersh, L.Hirschman, L.J.
Jensen, M. Krallinger, B. Mons, S.I.O?Donoghue, M.C.
Peitsch, D. Rebholz-Schuhmann,H.
Shatkay and A. Valencia.
2008.
Text mining forbiology - the way forward: opinions from leading sci-entists.
Genome Biology, 9(Suppl 2):S7B.
Boser, I. Guyon and V.N.
Vapnik.
1992.
A trainingalgorithm for optimal margin classifiers.
Proceedingsof the 5th annual workshop on Computational learningtheory (COLT), 144-152K.
Fundel, R. Ku?ffner and R. Zimmer.
2007.
RelEx?Relation extraction using dependency parse trees.Bioinformatics, 23(3):365-371C.
Giuliano, A. Lavelli and L. Romano 2006.
Exploitingshallow linguistic information for relation extractionfrom biomedical literature.
Proceedings of the 11thConference of the European Chapter of the Associa-tion for Computational Linguistics (EACL), 401-408J.
Hanley and B. J. McNeil.
1982.
The meaning anduse of the area under a receiver operating characteristic(roc) curve.
Radiology, 143(1):29-36L.
Hirschman, A. Yeh, C. Blaschke and A. Valencia.2005.
Overview of BioCreAtIvE: critical assessmentof information extraction for biology.
BMC Bioinfor-matics, 6(Suppl 1):S1J.-D. Kim, T. Ohta and J. Tsujii.
2008.
Corpus anno-tation for mining biomedical events from literature.BMC Bioinformatics, 19(Suppl 1):i180-i182J.-D. Kim, T. Ohta, S. Pyssalo, Y. Kano and J. Tsujii.2009.
Overview of BioNLP?09 Shared Task on EventExtraction, Proceedings of Natural Language Pro-cessing in Biomedicine (BioNLP) NAACL 2009 Work-shop, to appearM.
Krallinger, A. Morgan, L. Smith, F. Leitner, L.Tanabe, J. Wilbur, L. Hirschman and A. Valencia.2008.
Evaluation of text-mining systems for biology:overview of the Second BioCreative community chal-lenge.
Genome Biology, 9(Suppl 2):S1MC.
de Marneffe, B. MacCartney and C.D.
Manning.2006.
Generating typed dependency parses fromphrase structure parses.
Proceedings of the 5th In-ternational Conference on Language Resources andEvaluation (LREC), 449-454C.
Plake, J. Hakenberg and U. Leser.
2005.
Optimizingsyntax patterns for discovering protein-protein inter-actions.
Proceedings of the 2005 ACM symposium onApplied computing (SAC), 195-201M.F.
Porter.
1980.
An algorithm for suffix stripping.Program, 14(3), 130-137S.
Pyysalo, F. Ginter, J. Heimonen, J. Bjo?rne, J. Boberg,J.
Ja?rvinen and T. Salakoski.
2007.
BioInfer: A corpusfor information extraction in the biomedical domain.BMC Bioinformatics, 8(50)R. Saetre, K. Sagae and J. Tsujii.
2008.
Syntactic fea-tures for protein-protein interaction extraction.
Pro-ceedings of the 2nd International Symposium on Lan-guages in Biology and Medicine (LBM), 6.1-6.14S.
Van Landeghem, Y. Saeys, B.
De Baets and Y. Vande Peer.
2008.
Extracting protein-protein interactionsfrom text using rich feature vectors and feature selec-tion.
Proceedings of the Third International Sympo-sium on Semantic Mining in Biomedicine (SMBM), 77-84.136
