NAACL-HLT 2012 Workshop on Future directions and needs in the Spoken Dialog Community: Tools and Data, pages 49?52,Montre?al, Canada, June 7, 2012. c?2012 Association for Computational LinguisticsTowards Quality-Adaptive Spoken Dialogue ManagementStefan Ultes, Alexander Schmitt, Wolfgang MinkerDialogue Systems - Ulm UniversityAlbert-Einstein-Allee 4389081 Ulm, Germany{stefan.ultes,alexander.schmitt,wolfgang.minker}@uni-ulm.deAbstractInformation about the quality of a Spoken Di-alogue System (SDS) is usually used only forcomparing SDSs with each other or manuallyimproving the dialogue strategy.
This infor-mation, however, provides a means for inher-ently improving the dialogue performance byadapting the Dialogue Manager during the in-teraction accordingly.
For a quality metric tobe suitable, it must suffice certain conditions.Therefore, we address requirements for thequality metric and, additionally, present ap-proaches for quality-adaptive dialogue man-agement.1 IntroductionFor years, research has been focused on enablingSpoken Dialogue Systems (SDSs) to behave moreadaptively to the user?s expectations and needs.Mo?ller et al (2009) presented a taxonomy for qual-ity of human-machine interaction, i.e., Quality ofService (QoS) and Quality of Experience (QoE).
ForQoE, several aspects are identified.
They contributeto good user experience, e.g., interaction quality, us-ability and acceptability.
These aspects can be com-bined to the term User Satisfaction (US), describ-ing the degree by which the user is satisfied with thesystem?s performance.
The dialogue community hasbeen investigating this aspect for years.
Most promi-nently is the PARADISE framework by Walker et al(2000) which maps objective performance metricsof an SDS to subjective user ratings.Recent work mostly discusses how to evaluateSpoken Dialogue Systems.
However, the issue ofhow this information can be useful for improv-ing dialogue performance remains hardly addressed.Hence, we focus on exploring techniques for incor-porating dialogue quality information into the Dia-logue Manager (DM).
This is accompanied by theproblem of defining characteristics of a suitable dia-logue quality metric.In Section 2, we present related work both onmeasuring dialogue quality and on approaches forincorporating user state information into the DM.In Section 3, requirements for a quality metric arepresented along with a suitable example.
Section 4presents our ongoing and future work on incorpo-rating quality measures into dialogue strategies.
Fi-nally, Section 5 concludes this work.2 Related WorkIn recent years, several studies have been publishedon determining the qualitative performance of aSDS.
Engelbrecht et al (2009) predicted User Sat-isfaction on a five-point scale at any point within thedialogue using Hidden Markov Models (HMMs).Evaluation was based on labels the users appliedthemselves during a Wizard-of-Oz experiment.
Toguarantee for comparable conditions, the dialogueflow was controlled by predefined scenarios creat-ing transcripts with equal length for each scenario.Further work based on HMMs was presented byHigashinaka et al (2010).
The HMM was trained onUS rated at each exchange.
These exchange ratingswere derived from ratings for the whole dialogue.The authors compare their approach with HMMstrained on manually annotated exchanges achievinga better performance for the latter.49In order to predict US, Hara et al (2010) createdn-gram models from dialogue acts (DA).
Based ondialogues from real users interacting with a musicretrieval system, overall ratings for the whole dia-logue have been labeled on a five point scale afterthe interaction.
An accuracy (i.e., rate of correctlypredicted ratings) of 34% by a 3-gram model wasthe best performance which could be achieved.Dealing with true User Satisfaction, Schmitt et alpresented their work about statistical classificationmethods for automatic recognition of US (Schmittet al, 2011b).
The data was collected in a labstudy where the users themselves had to rate theconversation during the ongoing dialogue.
Labelswere applied on a scale from 1 to 5.
Perform-ing automatic classification using a Support VectorMachine (SVM), they achieved an Unweighted Av-erage Recall (UAR) of 49.2 (i.e., average rate ofcorrectly predicted ratings, compensated for unbal-anced data).An approach for affective dialogue modelingbased on Partially Observable Markov DecisionProcesses (POMDPs) was presented by Bui et al(2007).
Adding stress to the dialogue state enablesthe dialogue manager to adapt to the user.
To makebelief-update tractable, the authors introduced Dy-namic Decision Networks as means for reducingcomplexity.Pittermann et al (2007) presented another ap-proach for adaptive dialogue management.
The au-thors incorporated emotions by modeling the dia-logue in a semi-stochastic way.
Thus, an emotionaldialogue model was created as a combination of aprobabilistic emotional model and probabilistic dia-logue model defining the current dialogue state.3 Interaction Quality MetricIn order to enable the Dialogue Manager to bequality-adaptive, the quality metric must suffice cer-tain criteria.
In this Section, we identify the impor-tant issues and render the requirements for a suitablequality metric.3.1 General AspectsFor adapting the dialogue strategy to the quality ofthe dialogue, the quality metric is required to imple-ment certain characteristics.
We identify the follow-ing items:?
exchange-level quality measurement,?
automatically derivable features,?
domain-independent features,?
consistent labeling process,?
reproducible labels and?
unbiased labels.The performance of a Spoken Dialogue Systemmay be evaluated either on the dialogue level or onthe exchange level.
As dialogue management is per-formed after each system-user exchange, dynamicadaption of the dialogue strategy to the dialogueperformance requires exchange-level performancemeasures.
Therefor, Dialogue-level approaches areof no use.
Furthermore, previous presented meth-ods for exchange-level quality measuring could notachieve satisfying accuracy in predicting dialoguequality (Engelbrecht et al, 2009; Higashinaka et al,2010).Features serving as input variables for a classi-fication algorithm must be automatically derivablefrom the dialogue system modules.
This is impor-tant because other features, e.g., manually annotateddialogue acts (Higashinaka et al, 2010; Hara et al,2010), produce high costs and are also not availableimmediately during run-time in order to use them asadditional input to the Dialogue Manager.
Further-more, for creating a general quality metric, featureshave to be domain-independent, i.e., not dependingon the task domain of the dialogue system.Another important issue is the consistency of thelabels.
Labels applied by the users themselves aresubject to large fluctuations among the differentusers (Lindgaard and Dudek, 2003).
As this resultsin inconsistent labels, which do not suffice for creat-ing a generally valid quality model, ratings appliedby expert raters yield more consistent labels.
Theexperts are asked to estimate the user?s satisfactionfollowing previously established rating guidelines.Furthermore, expert labelers are also not prone to beinfluenced by certain aspects of the SDS, which arenot of interest in this context, e.g., the character ofthe synthesized voice.
Therefore, they create less bi-ased labels.503.2 Interaction QualityAs metric, which fulfills all previously addressedrequirements, we present the Interaction Quality(IQ) metric, see also (2011a).
Based on dialoguesfrom the ?Let?s Go Bus Information System?
of theCarnegie Mellon University in Pittsburgh (Raux etal., 2006), IQ is labeled on a five point scale.
Thelabels are (from best (5) to worst (1)) ?satisfied?,?slightly unsatisfied?, ?unsatisfied?, ?very unsatis-fied?
and ?extremely unsatisfied?.
They are appliedby expert raters following rating guidelines, whichhave been established to allow consistent and repro-ducible ratings.Additionally, domain-independent features usedfor IQ recognition have been derived from the di-alogue system modules automatically for each ex-change grouped on three levels: the exchange level,the dialogue level, and the window level.
As parame-ters like ASRCONFIDENCE or UTTERANCE can di-rectly be acquired from the dialogue modules theyconstitute the exchange level.
Based on this, counts,sums, means, and frequencies of exchange level pa-rameters from multiple exchanges are computed toconstitute the dialogue level (all exchanges up to thecurrent one) and the window level (the three previousexchanges).A corpus containing the labeled data has beenpublished recently (Schmitt et al, in press) contain-ing 200 calls annotated by three expert labelers, re-sulting in a total of 4,885 labeled exchanges.
Us-ing statistical classification of IQ based on SVMsachieves an Unweighted Average Recall of 0.58(Schmitt et al, 2011a).4 Quality-Adaptive Spoken DialogueManagementThe goal of our work is to enable Dialogue Man-agers to directly adapt to information about the qual-ity of the ongoing dialogue.
We present two differ-ent approaches that outline our ongoing and futurework.4.1 Dialogue Design-Patterns for QualityAdaptionRule-based Dialogue Managers are still state-of-the-art for commercial SDSs.
It is hardly arguable thatmaking the rules quality-dependent is a promisingway for dialogue improvement.
However, the num-ber of possibilities for adapting the dialogue strategyto the dialogue quality is high.
Based on the Speech-Cycle RPA Dialogue Manager, we are planning onidentifying common dialogue situations in order tocreate design-patterns.
These patterns can be ap-plied as a general means of dealing with situationsthat arise by introducing quality-adaptiveness to thedialogue.4.2 Statistical Quality-Adaptive DialogueManagementFor the incorporation of Interaction Quality into astatistical DM, two approaches have been found.First, based on work on factored Partially Observ-able Markov Decision Processes by Williams andYoung (2007) and similar to Bui et al (2006), wepresented our own approach for incorporating addi-tional user state information (Ultes et al, 2011).In the factored POMDP by Williams and Young(2007), the state of the underlying process is de-fined as s = (u, g, h).
To incorporate IQ, it isextended by adding the IQ-state siq, resulting ins = (u, g, h, siq).Following the concept of user acts, we further in-troduce IQ-acts iq that describe the current qual-ity predicted by the classification algorithm for thecurrent exchange.
Incorporating IQ acts into obser-vation o results in the two-dimensional observationspaceO = U ?
IQ,where U denotes the set of all user actions and IQthe set of all possible Interaction Quality values.Second, for training an optimal policy for ac-tion selection in POMDPs, a reward function hasto be defined.
Common reward functions are task-oriented and based on task success and dialoguelength.
As an example, a considerable positive re-ward is given for reaching the task goal, a consider-able negative reward for aborting the dialogue, and asmall negative reward for each exchange in order tokeep the dialogue short.
Interaction Quality scoresoffer an interesting and promising way of defining areward function, e.g., by rewarding improvements inIQ.
By that, strategies that try to keep the quality atan overall high can be trained allowing for a betteruser experience.515 ConclusionFor incorporating information about the dialoguequality into the Dialogue Manager, we identifiedcharacteristics of a quality metric defining neces-sary prerequisites for being used during dialoguemanagement.
Further, the Interaction Quality met-ric has been proposed as measure, which suffices allrequirements.
In addition, we presented concrete ap-proaches of incorporating IQ into the DM outliningour ongoing and future work.AcknowledgementsWe would like to thank Maxine Eskenazi, AlanBlack, Lori Levin, Rita Singh, Antoine Raux andBrian Langner from the Lets Go Lab at CarnegieMellon University, Pittsburgh, for providing the LetsGo Sample Corpus.
We would further like to thankRoberto Pieraccini and David Suendermann fromSpeechCycle, Inc., New York, for providing theSpeechCycle RPA Dialogue Manager.ReferencesT.
H. Bui, J. Zwiers, M. Poel, and A. Nijholt.
2006.
To-ward affective dialogue modeling using partially ob-servable markov decision processes.
In Proceedingsof workshop emotion and computing, 29th annual Ger-man conference on artificial intelligence.T.
H. Bui, M. Poel, A. Nijholt, and J. Zwiers.
2007.A tractable ddn-pomdp approach to affective dialoguemodeling for general probabilistic frame-based dia-logue systems.
In Proceedings of the 5th IJCAI Work-shop on Knowledge and Reasoning in Practical Dia-logue Systems, pages 34?37.Klaus-Peter Engelbrecht, Florian Go?dde, Felix Hartard,Hamed Ketabdar, and Sebastian Mo?ller.
2009.
Mod-eling user satisfaction with hidden markov model.
InSIGDIAL ?09: Proceedings of the SIGDIAL 2009 Con-ference, pages 170?177.
ACL.Sunao Hara, Norihide Kitaoka, and Kazuya Takeda.2010.
Estimation method of user satisfaction usingn-gram-based dialog history model for spoken dia-log system.
In Proceedings of the Seventh conferenceon International Language Resources and Evaluation(LREC?10), Valletta, Malta, may.
ELRA.Ryuichiro Higashinaka, Yasuhiro Minami, KohjiDohsaka, and Toyomi Meguro.
2010.
Modelinguser satisfaction transitions in dialogues from over-all ratings.
In Proceedings of the SIGDIAL 2010Conference, pages 18?27, Tokyo, Japan, September.Association for Computational Linguistics.Gitte Lindgaard and Cathy Dudek.
2003.
What is thisevasive beast we call user satisfaction?
Interactingwith Computers, 15(3):429?452.Sebastian Mo?ller, Klaus-Peter Engelbrecht, C. Ku?hnel,I.
Wechsung, and B. Weiss.
2009.
A taxonomy ofquality of service and quality of experience of multi-modal human-machine interaction.
In Quality of Mul-timedia Experience, 2009.
QoMEx 2009.
InternationalWorkshop on, pages 7?12, July.Johannes Pittermann, A. Pittermann, Hong Meng, andW.
Minker.
2007.
Towards an emotion-sensitivespoken dialogue system - classification and dialoguemodeling.
In Intelligent Environments, 2007.
IE 07.3rd IET International Conference on, pages 239 ?246,September.Antoine Raux, Dan Bohus, Brian Langner, Alan W.Black, and Maxine Eskenazi.
2006.
Doing researchon a deployed spoken dialogue system: One year oflets go!
experience.
In Proc.
of the International Con-ference on Speech and Language Processing (ICSLP),September.Alexander Schmitt, Benjamin Schatz, and WolfgangMinker.
2011a.
Modeling and predicting quality inspoken human-computer interaction.
In Proceedingsof the SIGDIAL 2011 Conference, Portland, Oregon,USA, June.
Association for Computational Linguis-tics.Alexander Schmitt, Benjamin Schatz, and WolfgangMinker.
2011b.
A statistical approach for estimat-ing user satisfaction in spoken human-machine inter-action.
In Proceedings of the IEEE Jordan Confer-ence on Applied Electrical Engineering and Comput-ing Technologies (AEECT), Amman, Jordan, Decem-ber.
IEEE.Alexander Schmitt, Stefan Ultes, and Wolfgang Minker.in-press.
A parameterized and annotated corpus of thecmu let?s go bus information system.
In InternationalConference on Language Resources and Evaluation(LREC).Stefan Ultes, Tobias Heinroth, Alexander Schmitt, andWolfgang Minker.
2011.
A theoretical framework fora user-centered spoken dialog manager.
In Proceed-ings of the Paralinguistic Information and its Integra-tion in Spoken Dialogue Systems Workshop, pages 241?
246.
Springer, September.Marilyn Walker, Candace Kamm, and Diane Litman.2000.
Towards developing general models of usabil-ity with paradise.
Nat.
Lang.
Eng., 6(3-4):363?377.Jason D. Williams and Steve J.
Young.
2007.
Par-tially observable markov decision processes for spo-ken dialog systems.
Computer Speech and Language,(21):393?422.52
