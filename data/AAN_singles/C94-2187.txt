A DISCOURSE GRAMMATICO-~TATISTICAL APPROACH TOPARTITIONINGTadashi Nomoto and Yoshihiko NittaAdwmced Research Laboratory, IIitachi Ltd.*cmMl :{nomoto ,  n i t ta}?har l ,  h i tach i ,  co .
j pAbst rac tThe paper presents a new approach to text segmen-tation - - .
which concerns dividing a text into coher-ent discourse units.
The approach builds on tile ttle-ory of discourse segment (Nomoto and Nitta, 1993),incorporating ideas from the research on informationretrieval (Salton, 1988).
A discourse segment has todo with a structure of Japanese discourse; it could bethought of as a linguistic unit delnarcated by wa, aJapanese topic particle, which may extend over sev-eral sentences.
The segmentation works with discoursesegments and makes use of coherence measure ba~scdon t f idf ,  a standard information retrieval measurement(Salton, 1988; IIearst, 1993).
Experi,nents have beendone with a Japanese newspaper corpus.
It has beenfound that  the present approach is quite sucecssfld inrecovering articles fronl tile unstructured corpus.I n t roduct ionIn this paper, we describe a method for discoveringcoherent texts from the unstructured corpus.
Themethod is both linguistically and statistically moti-vated.
It derives a linguistic motivation front the viewthat discourse consists of what we call discourse seg-ments, minimal coherent mtits of discourse (Nomotoand Nitta, 1993), while statistically it is guided by ideasfrom intbrmatiou retrieval (Salton, 1988).
Previousquantitative approaches to text segmentation (Hearst,1993; Kozima, 1993; Youmans, 1991) have paid littleattention to a statistically important structure that adiscourse might have and detined it away ~Ls a lump ofwords or sentences.
1'art of our concern here is withexplicating possible effects of a discourse segment onthe quantitative structuring of discourse.In what follows, we will describe some importantfeatures about discourse segment and see how it canbe incorporated into a statistical analysis of discourse.Also some comparison is made with other apl)roachessuch as (Youmans, 1991), followed by discussion on theresults of the present method.Theory of Discourse  SegmentThe theory o f  discourse seF, ment  (Nomoto a.nd Nitta,*2520 | ta toyama Sa i tama 350-03 Japantel.
+81-492-9C~6111 fax.
+81-492-9(~-60061993) carries with it a set of empirical hypotheses aboutstructure of Japanese discourse.
Among them is theclaim that Japanese discourse is constructed from a se-ries of linguistics units called discourse segment.
Thediscourse segment is thought of az a topic-commentstructure, where a topic corresponds to the subjectmatter and a comment a discussion about it.
In partic-ular, Japanese haz a special way of marking the topic:by sutllxing it with a postpositionM particle wa.
Thusin Japanese, a topic-comment s ructure takes the form:topic commentwhere "*" represents a word.
The comment part couldbecome quite long, extending over quite a few sentences(Mikami, 1960).
Now Japanese provides for a varietyof ways to mark off a topic-comment s ructure; the wa-marking is one such and a typographical device suchas a line- or a page-break is another.
For the presentdiscussiou, we take a discourse segment to bca  block ofsentences bounded by a text break and/or  a wa-markedelement.I T1 ~'1 5'2 " ' '  Sn \[ ~t~2 Sn-F1 Sn+2 ' "" Sn-Frn \],Discourse Segment Discourse Segmentwhere "T" denotes a boundary marker, "S" a sentence,and "\[" a segment gap.
For the semantics of a discoursesegment, Nomoto and Nitta (1993) observes an inter-esting tendency that zero (elliptical) anapl,ora occur-ring within the segment do not refer across the segmentboundary; that is, their references tend to be resolvedinternally 1 .Now we take a very simple view about tile gk)balstructure of discourse: syntactically, discourse is justl l le rc  and throughout  we use 01 for a sub jec t (NOMinat ive)zero; 02 for a ob jec t (ACCusat ive)  zero; TOP for a topic case;I )AT fox" a dat ive( indirect  object)  c~me; PASS for a passive mor-\].
)heIIle.I 'l'aro<,> -wa 01<i> rojin<i > -hi sekiTOP old man DAT seat-wo yuzutte -ageta node, 01<i> 02<j>ACC give help becauseorei -wo iwar et,~.
\[thank say PASS"l\]ecause ~\]hro gave the old man a favor of 9iv-im.l a seat, he thanked Taro.
"Note that all the instances of01 aa~d 02 have internal antecedents:Taro and rojin.114.5a chronological juxtaposit ion of contiguous, disjointblocks of sentences, each of which corresponds to adiscourse segment; semantically, discourse is a set ofanaphoric islauds set side by side.
Thus a discourseshould look like Figure 1, where G denotes a discoursesegment.
Fnrthermore, we do not intend the dis-DFigure 1: Discourse Structurecourse structure to be anything close to the ones thatrhetorical theories of discourse (tIovy, 1990; Mann andTholnpson, 1987; Itobbs, 1979) claim it to be, or inteu-lional structure (Grosz and Sidner, 1986) ; indeed wedo not assmne any functional relation, i.e.
causation,elaboration, extension, etc., among the segments thatconstitute a discourse structure.
The present theoryis not so much about the rhetoric or the function ofdiscourse as about the way anaphora re interpreted.It is quite possible that a set of discourse segmentsare not aggregated into a single discourse but may havediverse discourse groupings (Nomoto and Nitta, 1993).This happens when discourses are interleaved or em-bedded into some other.
An interleaving or an embed-ding of discourse is often invoked by changes in narra-tive mode such as direct/ indirect speech, quoting, orinterruption; which will cue the reader/hearer to sus-pend a current discourse flow, start another, or resumethe interrupted iscourse.A Quantitative Structuring ofDiscourseVector  Space  Mode lFormally, a discourse segment is represented as a lermvector of the form:Gi = (gil ,  gi2, gi3 .
.
.
.
,git)where a..qi represents a nominal occurrence in Gi.
Illthe information retrieval terms, what happens here isthat a discourse segment Gi is indexed with a set ofterms gll through tit; namely, we characterize Gi witha set of indices gi l , .
.
.
,tit.
A term vector can eitherbe binary, where each term in the w~'ctor takes 0 or 1,i.e., absence or presence, or weighted, where a term isassigned to a certain importance value.
In general, theweighted indexing is preferable to the biuary indexing,as the latter policy is known to have problems withprecision (Salton, 1988) 2.
The weighting policy thatwe will adopt is known as If.
idf It is an indicator ofterm importance Wij defined by:Nwij .= t fi j * log ~ j2 Precision measures the proportioi1 of correct items retrievedagainst he total number of retrieved items.where tf (term frequency) is the number of occurrencesof a term Tj in a document Di; df (document fre-quency) is tile number of documents in a collection ofN documents in which 7) occurs; and the importance,wljis given ~s the product of if and the inverse dffactor,or idf, log N/dfi.
With the tf idfpolicy, high-frequencyterms that are scattered evenly over the entire docu-ments collection are considered to be less importantthan those that are frequent but whose occurrencesare concentrated in particnlar documents 3.
Thus thetf.idfindexing favors rare words, which distinguish thedocuments more effectively than common words.With the indexing method in place, it is now pos-sible to define the cohercnce between two term vec-tors.
For term vectors X = (x l ,x~, .
.
.
,x~)  and Y =(Yt, Y2,.. .
,  Yt), let the coherence be defined by:t/=1C(X ,Y )  = t ti-.~1 4=1where w(xi) represents a t f idf  weight assigned to theterm xi.
The measure is known as Dice coefficienl 4.Exper imentsEarlier quantitative approaches to text partit ioning(Youmans, 1991; Kozima, 1993; Ilearst, 1993) workwith an arbitrary block of words or sentences to de-termine a strneture of discourse.
In contrast, we workwith a block of discourse segments.
It is straightfor-ward to apply the tf idf  to the analysis of discourse;one may just treat a block of discourse segments as adocument unit by itself and then define the term fre-quency (t J), the document fi'equency (dJ), and the sizeof docmnents collcction (N), accordingly.
Coherencewould then be determined by the number of terms seg-ment blocks share and tf.idf weights the terms carry.Thus one pair of blocks will become more cohesive thanauother if the pair share more of the terms that arc lo-cally frequent.The partit ioning proceeds in two steps.
We startwith the following:1.
Collect all the nominal occurrences found in acorpus  52.
Divide the collection into disjoint discourse seg-l r lents .3.
Compare all pairs of adjacent blocks of discoursesegments.3 Precision depends on the size of docmnents  coLLection; as thecollection get, s smal ler  in size, index ternls become less extensiveand more discr iminatory.
The  id\]factor could be dispensed withill such cases.4Other  measures  s tandard ly  avai lable in the in format ion re-n' ieval include inner product, cosine coefficient~ and Jaccard co-eJficient.5Thls is (tone by JUMAN, a Japanese morphological nalyzer(Mat.sumoto et al, 1993).1146OmnJooS EGM EN T SFigure 2: A coherence curve4, Assign a coherence/similarity wdue to each pair.Next, we examine the coherence curve for hills andvalleys and partit ion it manually.
Valleys (viz, low co-herence val,es) are likely to signal a potential breakin a discourse tlow, whereas hills (viz, high coherencevalues) would signal local coherency, l,'igure 2 showshow a coherence curve lnight appear.Coherence is measured at every segment with apaired comparison window moving along the sequenceof segments.
Or more precisely, given a segment djand a block size n, what we do is to compare a blockSl)am'ing dj-t~4-1 through dj and one sl)anning dj+lthrough dj+n-1.
The lneasnrement is l)lotted at thej th  position on the x-axis.
If either of |;lie comparisonyl,'igure 3: A Moving Paired Windowwindows is underiilled, no measurelnent will be made.The graph that the procedure gives out is smoothed(with an appropriate width) to bring out its globaltrends.
The length of a single segment, i.e., nouncounts, wtries from text to text, genre to gem:e, rang-ing from a few words (a junior high science book) tosomewhere around 60 words (a newspaper editorial).We performed experiments on a tbur-week col-lection of editorials fi'om Niho, Keizai Shimbun, a,lapanese conomics newsl)al)er, which contains the to-tal of 1111 sentences with some 10,000 nouns, and 556discourse segments.
The corpus was divided into seg-mental sets of nouns semi-autolnatically 6 (k)herencewas measured ff)r each adjacent pair of segments, using6The lll~lllllill part  consists in tumd-liltoering the corpus toel iminate non-topic mark ing htstemccs of tl~e particle wa, i.e.,those that  are suffixed to case particles uch a.s t0 (C;ONmNCTIVI.
;),de (LOcAT IVF , / IN ' r I '~UM~:NTAL) ,  he (F:ImnCrlONA\[,), kara (SOUltCP,),nl (DATIVE), ere., or to & part icular  form of verbM inflection(renyou-kei, i.e.
intinitive); thus wa is t reated as non-topicalunless it occm* as a postpo~itlon to the bare noun.the Dice coefficient.
It wa~s found that  the block size of10 segments yiehls good results.
Figure 4 shows a co-herence graph covering ahmlt a week's amount of thecorpus, The graph is smoothed with tile width of 5.We see that article boundaries (vertical ines) coincidefairly well with major minima on the graph: with onlyone miss at 65, which falls on a paragraph boundary.o?-=05'OA'03'O2 202 :00 20 40I6O 80 100 120 :140 160l:igure 4: A Dice AnalysisExperinmnts with w~rious block sizes suggest hatthe choice of block size relates in some way to the strucoture of discourse; an increasing block size would extracta more global or general structure of discourse.Youmans (1991) has suggested a information mea-surement b~sed on word frequency.
It is intended tomeasure the ehb and flow of 'new information' in dis?coarse.
The idea is simply to count the mmlber of newwords introduced ow'x a moving interval and 1)roducewhat he calls a vocabulary managemenl profile (VMI'),or lneasurements at intervals.
Now given a discourse1) = {wl , .
.
.
,w, ,} ,  tile k-th interval of the size A isdefined by lk :: {wt.,.
?., w,.}
where:k+A- I  i f k<n-Av n otherwiseMeasurements are made at interwfls I1 th rough/n - l .11{o71.60"140"\] 2o100"80"60"40-=0 ?
?
.1 " .
.
N"- '~" ",,,r'.u .
- . '
T ' , .
-  I , , - I - ,  u - , ,u  ' ,u200 400 600 800 1.0001200140016001800WORDS (TOKENS)Figure 5: A VMP AnalysisWhat  we like to  see is how the scheme ('Omlmres with11470D9-OD8-0D7-0D6-0D5-004-003- -0D2-0D1- -O', o ?
, , ?i a i g t i i n a a I o ii i !
!
i !l I l I I I Ii I I I I I I' : ; I : ii f f i  i !^  , -I i ; I iI I l l , , , o : i' \[ !
~ : : .
.a i | | i i J i a l t i |I I I " I '  ' I I I I I100 200, .
iII qJ300i I I I !l l : \[' ' I !a4OO| i| | i500Figure 6: The Dice on the Nikkei corpus' I600ours.
Figure 5 shows the results of a VMP analysis forthe same nominal collection as above.
The interval isset to 300 words, or the average length of a paired win-dow in the previous analysis.
The y-axis corresponds tothe number of new words (TYro,:) and the x-axis to aninterval position (TOK~;N).
As it tnrns out, the VMPfails to detect any significant pattern in the corpus.One of the problems with tim analysis has to do withits generality (Kozima, 1993); a text with many repeti-tions and/or limited vocabulary would yield a flattenedVMP, which, of course, does not tell us much about itsinner strncturings.
Indeed, this could be the case withFigure 5.
We suspect hat the VMP scheme fares bet-ter with a short-term coherency than with a long-termor global coherency.Eva luat ionl,'igure 6 demonstrates the results of the Dice analysison the nikkei collection of editorial articles.
What wesee here is a close correspondence between the Dicecurve an(l the global discourse structure.
Evaluationhere simply consists of finding major minima on thegraph and locating them on the list of those discoursesegments which comprise the corpus.
The procedure isperformed by hand.Correspondences valuation has been l)roblemati-cal, since it requires human judgments o~ how dis-course is structnred, whose reliability is yet to bedemonstrated.
It was decided here to use copiousboundary indicators such as an article or paragraphbreak for evalnating matches between the Dice analysisand the discourse.
For us, discourse structure reducesto just an orthographic structure 7.In the figure, article boundaries are marked bydashes.
7 out of 27 local minima are found to be in-correct, which puts the error rate at around 25%.
Weobtained similar results for the Jaccard and cosine co-efficient.
A possible improvement would include ad-justing the document frequency (d\]) factor for indexterms; the average df factor we had for the Nikkei cor-pns is around 1.6, which is so low as to be negligible s.;'Yet, there is some evidence that an orthographic structure isliuguislically significant (Fujisawa et al, 1993; Nunberg, 19(00).8IIowever, the averagc df factor would increase in proportionAnother interesting possibilty is to use an alternativeweighting policy, the weighled invet~se documenl fre-quency (Tokunaga and Iwayama, 1994).
A widfvalueof a term is its frequency within the docmnent dividedby its frequency throughout he entire document col-lection.
"\['he widf policy is reported to have a markedadvantage over the idffor the text categorization task.Reca l l  and Prec is ionAs with the document analysis, the effectiveness of textsegmentation appears to be dictated by recall/precisionparameters where:number of correct boundaries retrievedFecall =total number o/'correct boundariesnumber of correct boundaries retrievedprecision = total number of boundaries retrievedA boundary here is meant to be a minimum on thecoherence graph.
Precision is strongly affected by thesize of block or intervalg; a large-block segmentationyields less boundaries than a small-block segmentation.
(Table 1).
Experiments were made on the Nikkci cor-Block Size 5 10 15 20 25 30 35Boundm'ies 52 25 24 20 21 19 12Table h Block size (in word) and the nnmber of bound-aries retrieved.pus to examine the effccts of the block size parameteron text segmentation.
The corpus was divided intoequMly sized blocks, ranging from 5 to 35 words inlength.
The window size was kept to 10 blocks.
Shownin Figure 7 are the results given in terms of recall andprecision.
Also, a partitioning task with discourse seg-ments, whose length varies widely, is measured for re-call and precision, and the result is represented as G.Averaging recall and precision values for each size givesto the growth of corpus size.
It is likely, therefore, that with a11480.8-ftT-0.
(r0.~O4-0~(LI-00th'ecision+3530 15$+ +20 10 +25G+5+0.1 0.2 113 0.4Recall"1 I' J0.7 (1.8 0.9Figure 7: Rec.all and Precisionan ordering:35<25<20<5<30<15<10<( ; .O ranks highest, whose average value, 0.66, is higherthan any other.
~10' comes second (0.61) 1?
.
(It is aninteresting coincidence that the average length of dis=course segments is \[3.7 words.)
The results demon:strate in quantitative terms the significance of dis-eotlrse segments.It is worth t)ointing out that l, he method here ixrather selectlw'~ about a level of granularity it detects,namely, that of a news article.
It is possible, how-ever, to have a much smaller granularity; as shown inTable 1, decreasing the block size would give a segmen-tation of a smaller granularity.
Still, we chose not towork on fine-grained segmentations I)(;cause they lacka reliable evaluation metric ~ t.ConclusionIn this l)aper, we haw; described a method for I)art,ition -ing ,qll unstructured eorlms into coherent extual milts.We have adoptc~d the view that discourse consists ofcontiguous, non-overlal)ping discourse segments.
Wehave referred to a vector sl)acc model for a statisti-cal representation f discourse seglnellt.
(Joherence b(>tween segments is determined by the Dice coefficientwith the If .idf term weighting.We have demonstrated in quantitative terms thatthe me.thod here is quite suceessfld in discovering arti-cles fi'om the eort)us.
An interesting question yet to beanswered is how the corpus size affects the docul|mntlarger corpus, we might get, better esults.
?Blocks here a.re in|elided 1;o lll(}all minlntal l.extll;t| units intowhich a discourse is divided and fro' which c()hcrellCe is lttetmure(l.1?In general, recall is inversely proport.i(mate o precision; ahigh precision implies a low recall and vice versa.11 Passonneau nd l,itman (199.'
0 reports apsychological studyon the htlnt;I.n reliability of discourse segmental.ion.frequency and coherence measurements.
Another prob-lem h~s to do with relating the present discussion torhetorical analyses of discourse.ReferencesSl,inji Fujisawa, Shigeru Masuyama, and Shozo Naito.1993.
An Inspection oll Effect of DiscourseContraints pertaining to l,',llipsis Supplement inJapanese Sentences.
In Kouen-Ronbun-Shuu 3(conference papers 3).
Inforn\]ation Processing So-ciety of Japan.
In Japanese.Barbara Grosz and Candance Sidner.
1986.
Attention,Intentions and the.
Structure of 1)iseourse.
Compu-tational Linguistics, 12(3):175--204.Mart| A. Hearst.
1993.
TextTiling: A QuantitativeApproach to l)iseom'se Segmentation.
Sequoia 200093/24, University of California, Berkeley.Jerry R. l\[obbs.
1979.
Coherence and Corefernce.Cognitive 5'cicncc, 3(l):67-90.F, duard II.
l\[ovy.
1990.
I)arsimonious and ProfligateA1)proaches to the Question of l)iscourse StructureI(elations.
In 5th ACI, Workshop on Natural Lan-guage Geucralion, l)awson, Pemnsylwmia.llideki Kozima.
19!i)3.
3\'.xt Segmentation Based onSimilarity Between Words.
In Proceedings of the31st Annual Meelin.g of the ACL.W.
C. Mann and S. A. q'homI)son.
1987.
RhetoricalStructure Theory.
lu L. Polyani, editor, The S'truc-ture of Discourse.
Ablex Publishing Corp., Nor-wood, Nil.Yuji Matsmnoto, Sadao I(urohashi, Takehito Utsuro,Yutaka Tack|, and Makoto Nagao, 19.q3.
JapaneseMorphological Analysis System JUMAN Manual.Kyoto University.
In Japanese.Akira Mikami.
1960.
Zou wa tlana ga Nagai (7'heelephant has a long trunk.).
Kuroshio Shuppan,Tokyo.Tadashi Nomoto and Yoshihiko Nitta.
1993.
Resolv-ing Zero Anaphora in Japanese.
In A C\[, Proceed-ings of Sizlh European CoT@r'ence, pages 315 321,Utrecht, The Netherla,lds.
(~eotl'rey Nunberg.
1990.
The LinguiMics of Fuuctua-lion, volume 18 of (/SLI Leclure holes.
CSLI.ll.ebecca J. Passotmeau and Diane J. I,itman.
i\[9!
)3.Intention-based Segmentation: l luman Relial)ilityand (Jorrclation with Linguistic Cues.
In Proceed-ings of the 3lst Annual Meeting of the Associalionfor Computational Linguistics.
The Association forComputational I,inguistics.
Ohio State University,(~'olulnbus, Ohio, USA.1149Gerald Salton.
1988.
Automatic Text Processing: the"l~'ansfovmalion, Analysis, and Retl%val of Infor-mation by Compuler.
Addison-Wesley, Reading,MA.Takenobn Tokunaga nd Makoto Iwayama.
1994.
TextCategorization based on Weighted Inverse Docu-ment Frequency.
unpublished manuscript, submit-ted to ACM SIGIR 1994.Gilbert Youmans.
1991.
A New Tool for Dis-course Analysis:The Vocabulary Management Pro-file.
Language, 67:763-789.1750
