Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 491?500,Gothenburg, Sweden, April 26-30 2014.c?2014 Association for Computational LinguisticsA Latent Variable Model forDiscourse-aware Concept and Entity DisambiguationAngela Fahrni and Michael StrubeHeidelberg Institute for Theoretical Studies gGmbHSchloss-Wolfsbrunnenweg 3569118 Heidelberg, Germany(angela.fahrni|michael.strube)@h-its.orgAbstractThis paper takes a discourse-oriented per-spective for disambiguating common andproper noun mentions with respect toWikipedia.
Our novel approach mod-els the relationship between disambigua-tion and aspects of cohesion using MarkovLogic Networks with latent variables.Considering cohesive aspects consistentlyimproves the disambiguation results onvarious commonly used data sets.1 Introduction?I have to review a paper?, the super-visor moaned from the office.
?Pleasedon?t disturb me until I?m done with thereview.?
His student nodded, went tothe cafeteria, sat down in the sunshineand started to read yesterday?s paper.This text snippet illustrates two aspects that havebeen neglected by previous disambiguation ap-proaches.
(1) The interpretation of different men-tions, i.e.
common and proper nouns, is deter-mined by different notions of context: some men-tions depend more on a local sentence-level con-text (paper in read yesterday?s paper; the globalcontext is misleading), some more on a global one(review in I?m done with the review; the local con-text is not discriminative), some on both globaland local context (paper in review a paper).
(2)The context relevant to disambiguate a mentiondepends on how it is embedded into discourse andis not bound to the surface form of a mention (pa-per in the first sentence vs. paper in the last one).Starting from this observation, we argue that thecontext relevant to disambiguate a mention cor-relates with its cohesive scope, i.e.
the text spanwithin which a mention establishes cohesive re-lations.
Therefore, we propose to disambiguatementions differently depending on their cohesivescopes (Section 2).
We distinguish between threedifferent cohesive scopes of mentions and modelthem as latent variables using Markov Logic Net-works (Section 3).
The use of latent variables al-lows us to learn and predict the cohesive scopeand the disambiguation of a mention jointly.
Thiscomes with the advantage that the learning of thescope assignment does not need annotated data byitself but is guided by the annotations available forthe target prediction task, i.e.
the disambiguation.In this paper, we focus on concept and entitydisambiguation1with respect to an inventory de-rived from Wikipedia and compare (1) to a state-of-the-art approach that treats all mentions alikeand uses the same features for disambiguation,(2) to a pipeline-based approach, and (3) to otherstate-of-the-art approaches (Section 4).While early work disambiguated concepts us-ing the local context (Csomai and Mihalcea,2008), current research focuses on exploiting theglobal document context (Milne and Witten, 2008;Kulkarni et al., 2009; Ratinov et al., 2011; Fahrniand Strube, 2012; Cheng and Roth, 2013).
Al-though such global approaches try to balance be-tween local and global context, they treat all men-tions alike, i.e., they apply the same model and thesame weighting of local and global context fea-tures for disambiguating all mentions (Section 5).2 MotivationHalliday and Hasan (1976) define cohesion as ?re-lations of meaning that exist within the text, andthat define it as a text?
(p. 4).
A tie is one instanceof such a cohesive relation between two items.
Co-hesive ties occur on various linguistic levels, suchas on the entity level (e.g.
coreference and bridg-ing relations) or on the concept level (e.g.
lexical1In the following, we use concept to refer to concepts andwhat is usually called entities (e.g.
Ji et al.
(2011)).491chains).
In this paper, we focus on concept-levelcohesion and assume that each concept referred toby a mention can exhibit cohesive ties with con-cepts from other lexical units.
The cohesive scopeof a mention is the text span within which a con-cept referred to by a mention shows such cohesiveties.
We distinguish three broad categories of co-hesive scopes: (1) Mentions with local cohesivescope exhibit cohesive ties with lexical units inthe same sentence; (2) mentions with intermedi-ate cohesive scope show cohesive ties both withinthe sentence and beyond; (3) mentions with globalcohesive scope form cohesive ties with mentionsacross sentence boundaries.The notion of scope is a means to define the ap-propriate context to disambiguate a mention.
Amention of local scope does not exhibit relationswith lexical units outside its sentence.
Hence, theglobal context does not help to disambiguate it orcan even lead to the wrong disambiguation.
Fora mention with global scope, the global context iscrucial, while the local context is not discrimina-tive or even misleading.
For a mention with in-termediate scope both local and global context arerelevant.
Hence, while the scope influences the ap-propriate disambiguation context, the disambigua-tion of a mention influences its scope.
In the ex-ample (Section 1), paper in read yesterday?s pa-per refers to the concept NEWSPAPER.
Its scopeis local, as it lacks some cohesive ties with men-tions in other sentences.
If it had been disam-biguated to SCHOLARLY PAPER, its scope wouldbe global.
This reciprocal relationship betweendiscourse structure and meaning has also been dis-cussed by Asher and Lascarides (1995).
Theyuse rhetorical relations for structuring discoursewhile we rely on the notion of lexical cohesionand model scope assignment and disambiguationjointly.Our notion of scope is related to work onlexical chains (Morris and Hirst, 1991; Nelkenand Shieber, 2006; Mihalcea, 2006) and to workin content modeling, e.g.
Haghighi and Vander-wende (2009) distinguish content vocabulary anddocument-specific vocabulary.3 ApproachGiven a set of features for disambiguation, weaim to weight them differently depending on thescope.
To model the reciprocal relationship be-tween scope assignment and disambiguation, wepropose a latent variables based approach usingMarkov Logic Networks that allows us to learnthe parameters for the scope assignment and thedisambiguation tasks jointly and enables us to per-form joint inference.Our approach is joint as we assign the scope sand predict the concept c for a mention m simulta-neously.
As during learning training data is avail-able for the disambiguation task but not for thescope assignment task, we face a problem withlatent variables.
Latent variables represent miss-ing information in the input or a part of the out-put which is not relevant except for supporting theprediction of the target (Smith, 2011).
In our ap-proach, the different cohesive scopes are modeledby latent variables.
Each mention to be disam-biguated is assigned a scope s. All feature weightsare parametrized by scope s. The parameters forthe disambiguation and scope assignment tasks arelearned jointly and are guided by the annotationsavailable for the disambiguation task.Markov Logic Networks can be represented aslog-linear models, when grounded, and are there-fore straightforward to extend with latent variables(Smith, 2011; Poon and Domingos, 2008).
In ad-dition, global features can be conveniently inte-grated.3.1 Markov Logic NetworksMarkov Logic (ML) incorporates first-order logicand probabilities (Domingos and Lowd, 2009).A Markov Logic Network (MLN) is a first-orderknowledge base and consists of a set of pairs(Fi, wi), where Fiis a first-order formula andwi?
R is the weight of formula Fi.
It is a tem-plate for constructing a Markov Network.
ThisMarkov Network has a binary node for each pos-sible grounding for each predicate of the MLN.
Ifthe grounding of the predicate is true, the binarynode?s value is set to 1, otherwise to 0.
Further-more, it contains one feature2for each ground for-mula Fi.
If a ground formula is true, its feature?svalue is set to 1, otherwise to 0.
The feature?sweight is provided by wi.The probability distribution in the groundMarkov Network is given byP (X = x) =1Zexp(?iwini(x))2In this section feature is used differently than in the restof the paper.492where ni(x) is the number of true groundings ofFiin x.
The normalization factor Z is the partitionfunction.To perform MAP inference we use thebeast3which transforms the inference problem into anInteger Linear Program and solves it using cuttingplane inference (Riedel, 2008).3.1.1 Weight Learning with Latent VariablesSince no annotations are available for the scopedistinction, we face a latent variable learning prob-lem.
For learning weights in this situation we fol-low Poon and Domingos (2008).
We split our hid-den predicates into two parts: V are the ones forwhich the ground truth is known (concepts) andU are the ones for which there is no annotation(scopes).
Let O be the observed predicates.
Leto and v be the values of O and V in the train-ing data.
u denotes values assigned to U .
Weightlearning finds a w that maximizes the conditionallog-likelihoodLw(o, v) = logPw(V = v|O = o)= log?uPw(V = v, U = u|O = o),where the sum is over all possible values of U .Although Lw(o, v) is not convex, a local opti-mum can be found via gradient descent by itera-tively solvingwt+1= wt+ ?
?wLw(o, v),where the gradient?wLw(o, v) is given by?
?wiLw(o, v) = Ew[ni(o, v, U)]?
Ew[ni(o, V, U)].Ewdenotes the expectation according to Pwand ni(o, v, u) is the number of true ground-ings of formula Fiunder the assignment spec-ified by (o, v, u).
We use a voted perceptron(Lowd and Domingos, 2007) which approximatesthe expectations via computing the MAP solutionwith (o, v) fixed (Ew[ni(o, v, U)]) and (o) fixed(Ew[ni(o, V, U)]) respectively.3.1.2 Scope-aware Concept DisambiguationBoth the scope assignment and the disambiguationtask are performed jointly using Markov LogicNetworks.3http://code.google.com/p/thebeast.Table 1 shows the core of our proposed ap-proach in terms of predicates and first-order logicformulas.
We build upon our previous approachfor joint concept disambiguation and clustering(Fahrni and Strube, 2012).
For brevity, we onlydiscuss the scope-aware extension of the disam-biguation part.
The extension for clustering isdone analogously.The purpose of assigning a scope to eachmention m is to learn scope-specific weightsfor disambiguation to account for heteroge-nous scopes of mentions.
The learned weightsare parametrized by scopes.
We indicate thisparametrization of learned weights by w(s) (cf.Table 1, f8, f9).For each relation to predict, a hidden predicateis defined.
We are interested in predicting tworelations: a relation between a mention m and aconcept c (p1: hasConcept(m, c)) and a relationbetween a mention m and a scope s (p3: hasS-cope(m, s)).
To bridge between the disambigua-tion and the scope assignment task a third hid-den predicate relatesScopeToConcept(m, c, s) (p2)models a relation between a mention m, a conceptc and a scope s. This predicate together with For-mulas f4 ?
f7 garantuees that the scope assign-ment and the selection of a concept for a mentioninfluence each other and that the ground hiddenpredicates are in accordance.4Hard cardinalityconstraints (f1, f2, f3) enforce that each mentionm is assigned exactly one scope s and at most oneconcept c.The hidden predicates and formulas form thecore.
Features for the disambiguation and thescope assignment tasks are incorporated using lo-cal and global formulas with learned weights.
Thefeatures are described in Section 3.2.
Table 1 givesformula templates for both tasks (please note thatthese are templates not formulas (Section 3.2)):(1) a template for formulas that add informationfor scope assignment (f8) and (2) a template forformulas that add information for disambigua-tion (f9).
All formulas with scope-parametrizedweights that are relevant for the concept predictiontask are defined for the predicate relatesScopeTo-Concept.
This enables us to activate the relevant4We also run experiments with just two hidden predi-cates, i.e.
hasConcept(m, c) and hasScope(m, s).
All for-mulas with learned weight were then defined in the fol-lowing, less efficient way: ?m ?
M, c ?
C, s ?
S :featureDisambiguation(m, c, q) ?
hasConcept(m, c) ?hasScope(m, s).
q is a score (Table 1).493PredicatesHidden predicatesp1 hasConcept(m, c)p2 relatesScopeToConcept(m, c, s)p3 hasScope(m, s)Predicate template for disambiguation featuresp4 featureDisambiguation(m, c, q)Predicate template for scope assignment featuresp5 featureScope(m, q)FormulasHard cardinality constraintsf1 ?m ?M : |{c ?
C : hasConcept(m, c)}| ?
1f2 ?m ?M : |{c ?
C, s ?
S : relatesScopeToConcept(m, c, s)}| ?
1f3 ?m ?M : |{s ?
S : hasScope(m, s)}| = 1Hard constraintsf4 ?m ?M, c ?
C, s ?
S : relatesScopeToConcept(m, c, s)?
hasConcept(m, c)f5 ?m ?M, c ?
C, s ?
S : relatesScopeToConcept(m, c, s)?
hasScope(m, s)f6 ?m ?M, c ?
C, s ?
S : hasConcept(m, c) ?
hasScope(m, s)?
relatesScopeToConcept(m, c, s)f7 ?m ?M, c ?
C : hasConcept(m, c)?
(|{s ?
S : relatesScopeToConcept(m, c, s)}| = 1)Formula template with learned weights for scope assignmentf8 q ?
w(s) ?m ?M, s ?
S : featureScope(m, q)?
hasScope(m, s)Formula template with learned weights for disambiguationf9 q ?
w(s) ?m ?M, c ?
C, s ?
S : featureDisambiguation(m, c, q)?
relatesScopeToConcept(m, c, s)Table 1: Predicates and formulas used for scope distinction and disambiguation (m represents a mention,M sets of mentions, c a concept, C sets of concepts, s a scope, S sets of scopes, q scores, w weights andw(s) a weight which is parametrized by s).
The two template predicates and formulas are generalizedpatterns to integrate the features for the scope assignment and disambiguation task (Section 3.2).scope-specific weights w(s) which depend on thechosen scope s. The final weight for a formulacan also include a score q defined by the observedpredicate.3.2 FeaturesFor disambiguation and clustering we build uponour previous work (Fahrni and Strube, 2012).
Weuse the same features and formulas and adopt thelatter to learn scope-specific weights.
Given forexample the local context similarity feature (pred-icate hasContextSimilarity(m, c, q) where q is thesimilarity score) and the corresponding formula?m ?M, c ?
Cm: hasContextSimilarity(m, c, q)?
hasConcept(m, c)with weight (q ?
w) we adopt it in the followingway (cf.
Table 1, template f9):?m ?M, s ?
S, c ?
Cm:hasContextSimilarity(m, c, q)?
relatesScopeToConcept(m, c, s)with weight (q ?
w(s)).In order to distinguish between the three pro-posed scopes, we use the features described in Ta-ble 2.
The first column shows the predicate whichcan be used for template f8 in Table 1.4 ExperimentsWe compare our novel scope-aware approach toour previous scope-ignorant approach (Fahrni andStrube, 2012) ?
which has achieved good resultsin the English monolingual and Chinese and Span-ish cross-lingual entity linking tasks at TAC 2012and 2013 (Fahrni et al., 2014) ?
and a scope-awarepipeline-based approach using the same featuresand preprocessing to ensure a fair comparison.This allows us to identify the differences in theresults that are due to scope-awareness and differ-ences in the results that are due to different learn-ing strategies (joint vs. pipeline-based).
In addi-tion, we compare our joint scope-aware approachto state-of-the-art approaches using various datasets.4.1 DataTable 3 summarizes our test sets (ACE 2005, ACE2004, MSNBC and TAC 2011) and our train-ing and development sets derived from Wikipedia(WP Training, WP Dev).
For each data set we re-port the total number of annotated mentions, thenumber of mentions with a corresponding conceptin Wikipedia (non-NILs) and the number of NILs(i.e.
mentions that do not refer to a Wikipedia con-494Predicates DescriptionMention-based FeaturesidfHead(m, q) The more frequent a mention is, the more likely it is to exert a local scope.
This is inspired bywork on indexing for IR.
We use the idf score of the head of a mention according the EnglishGigaword Corpus (Parker et al., 2011).propernoun(m) Proper nouns are usually more prominent than common nouns and are more likely to have anintermediate or global scope than common nouns.singlewordNoun(m) Single word NPs are often less prominent than multi-word NPs and are more likely to be of localscope.abbrev(m) Abbreviations with a terminal dot such as Mr. or Ltd. tend to have a local scope as they are usuallylocal modifiers or specifications.Features Based on ModificationisPreModified(m) If a mention is pre-modified, it tends to be more prominent than unmodified mentions.
If a mentionis more prominent, it is more likely to have a larger scope.headOfRelClause(m) Mentions that are the head of a relative clause are usually more prominent and are more likely tohave an intermediate or global scope.Features Based on the Text StructureinSubjPosition(m) Mentions in theme position, which is in English often the subject, tend to pick up what has alreadybeen mentioned before (Dane?s, 1974).
Since this is not just the case on the reference-level, butalso on the concept-level, the mention in theme position tends to be related to other mentions inthe text and tends to have an intermediate or global scope.posInSentence(m, q) The earlier a mention appears in the sentence in English, the more thematic it is, and the morelikely it has an intermediate or global scope.focusingAdverb(m) Focusing adverbs in the text pattern <focusing adverb> <mention> ?
e.g.
?particularly Jack?
?indicate that the mention is thematic and therefore has larger scope.modifiesArgument(m) A premodifier of a verbal argument is usually more likely to be of local scope.passiveBy(m) A passive construction ?
e.g.
?the thief was catched by the police?
?
is a way to reduce theprominency of the agent (e.g.
police).
The agent tends to be of local scope.inConjunction(m) Conjunctions are often used for exemplifications.
Therefore mentions in conjunctions are oftenless prominent.inDepRelPP(m1,m2)inDepRelGen(m1,m2)In NPs with prepositional or genitive modifiers usually at most one part ?
either the modifying NPor the head ?
has intermediate or global scope.morphoTiesHead(m, q) The more frequent the head of a mention appears in the text ?
also as a derivation, e.g.
a verb,according to CatVar (Habash and Dorr, 2003) ?, the more prominent it is.positionInText(m, q) The earlier a mention appears in text, the more likely it is to exhibit global cohesive scope (cf.
thehard-to-be-beat lead baseline in summarization (Radev et al., 2003)).Table 2: Features for cohesive scope distinction.
m,m1,m2denote mentions, q a score.
The predicatesare plugged in the template formula f8 in Table 1.Data set No.
ofMen-tionsNon-NILsNILs Avg.Ambi-guityWP Training 56,372 53,097 3,275 2.31WP Dev 9,992 9,375 617 2.28ACE 2005 29,300 27,184 2,116 6.52ACE 2004 306 257 49 5.04TAC 2011 2,250 1,124 1,126 6.32MSNBC 756 629 127 5.29Table 3: Statistics for data sets.cept).
The average ambiguity of mentions is givenby our lexicon (see Section 4.2).Our system is exclusively trained on the internalhyperlinks in Wikipedia with the advantage that nomanual annotation effort is needed.
We use 500 ar-ticles for training and 100 articles for development(Fahrni and Strube, 2012).
Each internal hyper-link is considered as an annotated mention.
Thepointer to the Wikipedia article serves as the cor-rect concept for this mention and all other candi-date concepts we obtain from our lexicon as wrongconcepts for this mention.For the detailed analysis of our approach, weuse a version of the ACE 2005 corpus which con-tains Wikipedia link annotations (Bentivogli et al.,2010).
All ACE mentions, both common andproper nouns, are annotated with one or more linksto the English Wikipedia or as NILs.
If a men-tion is annotated with more than one link, we con-sider it as correctly disambiguated if one of the an-notated concepts has been chosen by our system.ACE 2005 consists of 597 texts from newswire re-ports, broadcast news, internet sources and tran-scribed audio data and contains more annotationsthan the other data sets we use for comparison.While ACE 2005 and ACE 2004 (Ratinov etal., 2011) fit our target scenario most (both com-mon and proper nouns are annotated), MSNBC(Cucerzan, 2007) and TAC 2011 (Ji et al., 2011)are only annotated for proper nouns.4954.2 PreprocessingThe training, development and testing data are allpreprocessed in the same way.
We perform POStagging, syntactic parsing and named entity recog-nition using the Stanford CoreNLP pipeline5.
Foridentifying mentions we extract all noun phrases(excluding discontinuous phrases and determin-ers) and look them up in our lexicon.
Our lex-icon and also all other information we obtainedfrom Wikipedia are extracted from the same En-glish Wikipedia dump.6The lexicon consists ofanchor texts, article titles and redirects.4.3 SettingsUpper bound: The upper bound shows the maxi-mum performance we can reach given our lexiconand preprocessing.
If the correct concept is amongthe candidate concepts of a mention, it is consid-ered as correct.First Concept: The first concept baseline is astrong baseline in disambiguation.
It chooses foreach mention its most frequent concept.Scope-ignorant (Disambig.
): Our previousMLN-based approach for concept disambiguation(Fahrni and Strube, 2012).Scope-ignorant (Disambig.
& Clust.
): Our pre-vious MLN-based approach for joint disambigua-tion and clustering of concepts (Fahrni and Strube,2012).Pipeline-based Scope-aware (Disambig.
): Wecompare our joint approach to a pipeline-basedone in which the assignment of the cohesive scopeis done before disambiguation.
The features forthe scope assignment and the disambiguation taskare exactly the same as in the joint setting andimplemented in Markov Logic.
The weights forthe scope assignment and disambiguation task arelearned in a cascaded way.
In contrast to thejoint approach, the hasScope(m, s) predicate is ob-served during disambiguation.Joint Scope-aware (Disambig.
): This is our ap-proach as described in Section 3 for concept dis-ambiguation.
As only local optimization is possi-ble, initialization is crucial.
We use the same ini-tialization strategy as for the cascaded approach.Joint Scope-aware (Disambig.
& Clust.
): This isour approach as described in Section 3 for disam-biguation and clustering of concepts.5http://nlp.stanford.edu/software/corenlp.shtml6We use the English Wikipedia dump from Jan. 4, 2012.4.4 Analysis of Scope-awareness onACE 2005In Table 4 we report precision (P), recall (R) andF-measure (F) for non-NILs and NILs for the ACE2005 data.
We also report overall accuracy (Acc)(aka micro-average) and calculate significance us-ing a paired t-test.Differences in the results can be exclusivelytraced back to differences in the modeling (scope-ignorant vs. scope-aware) and learning (pipeline-based vs. joint).
Learning scope-specific models(pipeline-based or joint) significantly improves theresult with p < 0.01 while using the same featuresfor disambiguation.
Scope-aware joint approachessignificantly outperform the other correspondingapproaches (pipeline-based and scope-ignorant)that use the same features for disambiguation (andclustering) with p < 0.01.
While the pipeline-based approach suffers from error propagation,the joint approach also benefits from the learn-ing strategy: learning weights for scope distinctioncan be guided by the training data available forthe disambiguation task.
Joint disambiguation andclustering of mentions improves the disambigua-tion results for both the scope-ignorant (Fahrni andStrube, 2012) and the scope-aware approach.As Table 4 indicates, the gain of the joint scope-aware approach with respect to non-NILs is sub-stantial in both precision and recall.
For NILsthe recall improves while the precision decreases.This leads to a slightly worse F-Measure for theNILs.
As NILs are much rarer than non-NILs inthe corpus, the overall accurracy for which we op-timize is significantly higher for the scope-awareapproaches.As no gold annotations for cohesive scopes areavailable, we present statistics on the distributionof induced scopes.
Table 5 shows the distribu-tion of the mentions across induced scopes.
Men-tions with local scope are more frequent than men-tions with intermediate scope followed by men-tions with global scope.
Table 5 compares theoverall accurracy of the scope-ignorant joint dis-ambiguation and clustering approach (Fahrni andStrube, 2012) with the accurracy of the corre-sponding joint scope-aware approach.
The jointscope-aware approach improves the disambigua-tion results for mentions of all three scopes.
Thebiggest gain (2.79) is achieved for mentions withinduced global scope.
The gain for mentions withlocal and intermediate scope is 1.27 and 0.3 re-496Non-NILs NILsP R F P R F AccUpper bound 94.8 91.8 93.3 71.3 100.0 83.3 92.4First Concept 68.6 70.0 69.3 55.3 40.3 46.6 67.9Scope-ignorant (Disambig.)
(Fahrni & Strube 2012) 77.3 76.0 76.6 44.7 54.2 49.0 74.4Scope-ignorant (Disambig.
& Clust.)
(Fahrni & Strube 2012) 76.8 76.9 76.9 50.2 50.0 50.1 74.9Pipeline-based Scope-aware (Disambig.)
80.1 75.8 77.9 37.3 63.4 47.0 74.9Joint Scope-aware (Disambig.)
80.1 76.6 78.3 39.2 61.5 47.9 75.5Joint Scope-aware (Disambig.
& Clust.)
80.3 77.1 78.6 40.8 62.1 49.3 76.0Table 4: Evaluation on ACE 2005 dataScope-ignorant Approach(Disambig.
& Clust.
)(Fahrni & Strube 2012) (Acc)Joint Scope-aware Approach(Disambig.
& Clust.)
(Acc)Scope Distribution (%)Global Scope 73.20 75.99 8.54Intermediate Scope 76.34 76.64 31.05Local Scope 75.57 76.84 60.40Total 75.61 76.71 100.00Table 5: Evaluation on ACE 2005 data across induced scopes.
The accurracy of the two comparedsystems is slightly higher than in Table 4 as we consider here only mentions that have been recognized byour mention identification strategy.
In the evaluation in Table 4 mentions that have not been recognizedare considered as wrong.spectively.
A comparison of the learned weightsfor the different scope-specific models shows thatfor mentions with local scope the local context hasrelatively more weight than for mentions with in-termediate scope.
For mentions with global scope,it is striking that candidiate concepts that are notrelated to the global context are relatively higherpunished than in the other two models.To obtain some insights on the behaviour of thejoint scope-aware approach, we investigate someexamples.
In a text on the 2004 US elections, themention Kerry in ?Kerry was the clear winner, butvictory was snatched from him?
is wrongly disam-biguated to KERRY GAA, a branch of the Gaelicfootball association, by the scope-ignorant ap-proach, because the local context strongly prefersan interpretation in the domain of sports.
Inthe joint scope-aware approach, Kerry is assignedglobal scope, and it is correctly disambiguatedto JOHN KERRY, an American politician, as theglobal relatedness overrules the local context inthis model.
In another text on U.S. troops inIraq, the scope-ignorant approach disambiguatessouth in ?Monday?s advances came one day af-ter British forces in the south made their deepestpush into Iraq?s second largest city?
to SOUTHERNUNITED STATES as concepts related to the USAare quite prominent in the text.
In the scope-awareapproach south is considered as being of localscope and is correctly disambiguated as SOUTH.In ?we happen to be at a very nice spot by thebeach where this is a chance for people to getaway from cnn coverage?
spot is disambiguatedas SPOT (SATELLITE) in the scope-ignorant ap-proach (misled by CNN), while it has been cor-rectly recognized as NIL by the scope-aware ap-proach in which it is considered as being of inter-mediate scope.
The remaining disambiguation er-rors can be traced back to (1) scope assignment er-rors and (2) disambiguation errors (e.g.
Palmisano(global scope) is disambiguated as SAMUEL J.PALMISANO, but the text refers to a different un-known Palmisano).4.5 Comparison to State-of-the-artApproachesCompared to the state-of-the-art for concept andentity disambiguation our approach performs fa-vorably (Table 6).
On ACE 2004 (Ratinov etal., 2011) ?
which contains annotations for com-mon and proper nouns and fits our target scenariomost ?
our scope-aware approach outperforms re-cent state-of-the-art approaches for concept andentity disambiguation, i.e.
Ratinov et al.
(2011)and Cheng and Roth (2013).
We also ran Rati-nov et al.
?s (2011) sytem on ACE 2005, but itseems that its mention recognition is not designedfor ACE 2005.We also evaluate our system on the task of en-tity linking, i.e.
the disambiguation of (selected)proper nouns (MSNBC and TAC 2011).
Oursystem fails to beat the best systems, but still497System ACE 2004 MSNBC TAC 2011BOC BOC Acc B3P B3R B3F1Ratinov et al.
2011; Cogcomp 77.3 74.9 78.7 75.7 76.5 76.1Cheng & Roth 2013 85.3 81.2 86.1 82.9 84.5 83.7Monahan et al.
2011 (Best System at TAC 2011) 86.1 84.4 84.7 84.6Scope-ignorant (Disambig.
& Clust.)
(Fahrni & Strube 2012) 83.4 76.5 84.8 82.5 83.0 82.8Joint Scope-aware (Disambig.
& Clust.)
86.3 79.0 85.5 83.6 82.7 83.1Table 6: Evaluation on various data sets using the respective standard evaluation metrics.
BOC standsfor Bag-of-Concepts.
We use the code of Ratinov et al.
(2011) to evaluate on ACE 2004 and MSNBC.For TAC 2011, we use the offical evaluation script and report the micro-average (Acc) and B3scores.Note that for TAC we use three additional disambiguation features ?
they measure the similarity of thearticle name to the context ?
both in the scope-ignorant and the scope-aware approach.achieves competitive performance without train-ing on TAC data.
On all data sets, the jointscope-aware approach consistently outperformsthe scope-ignorant approach ceteris paribus.5 Related WorkJoint approaches have been successful in the pastin NLP (e.g.
Meza-Ruiz and Riedel (2009)).
Theidea of augmenting a model with additional latentvariables to increase its expressiveness is known ashidden or latent variable learning (Smith, 2011)and is a promising research direction with success-ful applications in e.g.
syntactic parsing (Petrovet al., 2006), statistical machine translation (Blun-som et al., 2008) and sentiment analysis (Yesse-nalina et al., 2010; Trivedi and Eisenstein, 2013).For latent variable learning generative approaches(Petrov et al., 2006), large margin methods (Smith,2011) and conditional log-linear models have beenproposed.
We focus here on conditional log-linearmodels due to their flexibility and their previoussuccess for many tasks.
Blunsom et al.
(2008)for instance use latent variables in the context ofdiscriminative machine translation and model thederivation as a latent variable.
Chang et al.
(2010)is close to our approach, as their latent variable ap-proach also uses ILP.
Poon and Domingos (2008)also use latent variables with Markov Logic, al-though with a completely different aim, i.e.
for un-supervised coreference resolution.Most approaches that use Wikipedia as a re-source for disambiguation focus on named enti-ties (Bunescu and Pas?ca, 2006; Cucerzan, 2007;Dredze et al., 2010; Ji and Grishman, 2011;Hachey et al., 2013; Hoffart et al., 2011), whileonly a few disambiguate common and propernouns like us (Csomai and Mihalcea, 2008; Milneand Witten, 2008; Zhou et al., 2010; Ratinov et al.,2011; Cheng and Roth, 2013).
We build upon ourprevious Markov Logic based approach for jointconcept disambiguation and clustering (Fahrni andStrube, 2012).
In contrast to us, most approachesfor lexical disambiguation use either one modelfor all mentions (Milne and Witten, 2008; Rati-nov et al., 2011) or a separate model for each men-tion or concept which requires a lot of training data(e.g.
Bryl et al.
(2010)).
Only a few approaches tryto learn specific models for groups of mentions,although none of them is discourse-motivated asours: Mihalcea and Csomai (2005) learn a specificmodel for each POS, Ando (2006) uses alternatingstructure optimization to simultantanously learn anumber of WSD problems and Dhillon and Ungar(2009) improve feature selection for WSD by in-tegrating knowledge from similar words.6 ConclusionsIn this paper, we discuss the relationship betweencohesion and concept disambiguation and pro-pose a cohesive scope-aware disambiguation ap-proach.
We distinguish between three different co-hesive scopes (local, intermediate and global) andmodel the scope assignment and the disambigua-tion jointly using latent variables in the frameworkof MLN.
The joint scope-aware approach signifi-cantly improves over both a state-of-the-art and apipeline-based approach using the same featuresfor the disambiguation task.For future work, we are planning to investigatethe relation between discourse structure and co-hesive scope more deeply and to integrate scope-specific disambiguation features.AcknowledgmentsWe would like to thank Sebastian Martschat for hisvaluable comments.
This work has been partiallyfunded by the Klaus Tschira Foundation.498ReferencesRie Kubota Ando.
2006.
Applying alternating struc-ture optimization to word sense disambiguation.
InProceedings of the 10th Conference on Computa-tional Natural Language Learning, New York, N.Y.,USA, 8?9 June 2006, pages 77?84.Nicholas Asher and Alex Lascarides.
1995.
Lexicaldisambiguation in a discourse context.
Journal ofSemantics, 12(1):69?108.Luisa Bentivogli, Pamela Forner, Claudio Giu-liano, Alessandro Marchetti, Emanuele Pianta, andKateryna Tymoshenko.
2010.
Extending EnglishACE 2005 corpus annotation with ground-truth linksto Wikipedia.
In Proceedings of the 2nd Work-shop on The People?s Web: Colloboratively Con-structed Semantic Resources, Beijing, China, 28 Au-gust 2010, pages 19?27.Phil Blunsom, Trevor Cohn, and Miles Osborne.
2008.A discriminative latent variable model for statisti-cal machine translation.
In Proceedings of the 46thAnnual Meeting of the Association for Computa-tional Linguistics: Human Language Technologies,Columbus, Ohio, 15?20 June 2008, pages 200?208.Volha Bryl, Claudio Giuliano, Luciano Serafini, andKateryna Tymoshenko.
2010.
Supporting natu-ral language processing with background knowl-edge: Coreference resolution case.
In Proceedingsof the 9th International Semantic Web Conference,Revised Selected Papers, Part I, Shanghai, China, 7-11 November 2010, pages 80?95.Razvan Bunescu and Marius Pas?ca.
2006.
Using en-cyclopedic knowledge for named entity disambigua-tion.
In Proceedings of the 11th Conference of theEuropean Chapter of the Association for Compu-tational Linguistics, Trento, Italy, 3?7 April 2006,pages 9?16.Ming-Wei Chang, Vivek Srikumar, Dan Goldwasser,and Dan Roth.
2010.
Structured output learningwith indirect supervision.
In Proceedings of the27th International Conference on Machine Learn-ing, Haifa, Israel, 21?24 June 2010, pages 199?206.Xiao Cheng and Dan Roth.
2013.
Relational infer-ence for Wikification.
In Proceedings of the 2013Conference on Empirical Methods in Natural Lan-guage Processing, Seattle, Wash., 18?21 October2013, pages 1787?1796.Andras Csomai and Rada Mihalcea.
2008.
Linkingdocuments to encyclopedic knowledge.
IEEE Intel-ligent Systems, 23(5):34?41.Silviu Cucerzan.
2007.
Large-scale named entitydisambiguation based on Wikipedia data.
In Pro-ceedings of the 2007 Joint Conference on EmpiricalMethods in Natural Language Processing and Com-putational Language Learning, Prague, Czech Re-public, 28?30 June 2007, pages 708?716.Franti?sek Dane?s.
1974.
Functional sentence perspec-tive and the organization of the text.
In F. Dane?s,editor, Papers on Functional Sentence Perspective,pages 106?128.
Prague: Academia.Paramveer S. Dhillon and Lyle H. Ungar.
2009.
Trans-fer learning, feature selection and word sense dis-ambguation.
In Proceedings of the ACL-IJCNLP2009 Conference Short Papers, Singapore, 2?7 Au-gust 2009, pages 257?260.Pedro Domingos and Daniel Lowd.
2009.
MarkovLogic: An Interface Layer for Artificial Intelligence.Morgan Claypool Publishers.Mark Dredze, Paul McNamee, Delip Rao, Adam Ger-ber, and Tim Finin.
2010.
Entity disambigua-tion for knowledge base population.
In Proceed-ings of the 23rd International Conference on Com-putational Linguistics, Beijing, China, 23?27 Au-gust 2010, pages 277?285.Angela Fahrni and Michael Strube.
2012.
Jointlydisambiguating and clustering concepts and entitieswith Markov logic.
In Proceedings of the 24th Inter-national Conference on Computational Linguistics,Mumbai, India, 8?15 December 2012, pages 815?832.Angela Fahrni, Benjamin Heinzerling, Thierry G?ockel,and Michael Strube.
2014.
HITS?
monolin-gual and cross-lingual entity linking system at TAC2013.
In Proceedings of the Text Analysis Confer-ence, National Institute of Standards and Technol-ogy, Gaithersburg, Maryland, USA, 18?19 Novem-ber 2013.Nizar Habash and Bonnie Dorr.
2003.
A catego-rial variation database for English.
In Proceed-ings of the Human Language Technology Confer-ence of the North American Chapter of the Associ-ation for Computational Linguistics, Edmonton, Al-berta, Canada, 27 May ?1 June 2003, pages 17?23.Ben Hachey, Will Radford, Joel Nothman, MatthewHonnibal, and James R. Curran.
2013.
Evaluat-ing entity linking with Wikipedia.
Artificial Intel-ligence, 194:130?150.Aria Haghighi and Lucy Vanderwende.
2009.
Explor-ing content models for multi-document summariza-tion.
In Proceedings of Human Language Technolo-gies 2009: The Conference of the North AmericanChapter of the Association for Computational Lin-guistics, Boulder, Col., 31 May ?
5 June 2009, pages362?370.M.
A. K. Halliday and Ruqaiya Hasan.
1976.
Cohe-sion in English.
London, U.K.: Longman.Johannes Hoffart, Mohamed Amir Yosef, Ilaria Bor-dino, Hagen F?urstenau, Manfred Pinkal, Marc Span-iol, Bilyana Taneva, Stefan Thater, and GerhardWeikum.
2011.
Robust disambiguation of namedentities in text.
In Proceedings of the 2011 Con-ference on Empirical Methods in Natural Language499Processing, Edinburgh, Scotland, U.K., 27?29 July2011, pages 782?792.Heng Ji and Ralph Grishman.
2011.
Knowledge basepopulation: Successful approaches and challenges.In Proceedings of the 49th Annual Meeting of the As-sociation for Computational Linguistics, Portland,Oreg., 19?24 June 2011, pages 1148?1158.Heng Ji, Ralph Grishman, and Hoa Dang.
2011.Overview of the TAC 2011 knowledge base popula-tion track.
In Proceedings of the Text Analysis Con-ference, National Institute of Standards and Technol-ogy, Gaithersburg, Maryland, USA, 14?15 Novem-ber 2011.Sayali Kulkarni, Amit Singh, Ganesh Ramakrishnan,and Soumen Chakrabarti.
2009.
Collective anno-tation of Wikipedia entities in web text.
In Pro-ceedings of the 15th ACM SIGKDD Conferenceon Knowledge Discovery and Data Mining, Paris,France, 28 June ?
1 July 2009, pages 457?466.Daniel Lowd and Pedro Domingos.
2007.
Efficientweight learning for Markov logic networks.
InProceedings of the 11th European Conference onPrinciples and Practices of Knowledge Discoveryin Databases, Warsaw, Poland, 17?21 September2007, pages 200?211.Ivan Meza-Ruiz and Sebastian Riedel.
2009.
Jointlyidentifying predicates, arguments and senses usingMarkov logic.
In Proceedings of Human LanguageTechnologies 2009: The Conference of the NorthAmerican Chapter of the Association for Computa-tional Linguistics, Boulder, Col., 31 May ?
5 June2009, pages 155?163.Rada Mihalcea and Andras Csomai.
2005.
Sense-Learner: Word sense disambiguation for all words inunrestricted text.
In Proceedings of the InteractivePoster and Demonstrations Sessions at the 43rd An-nual Meeting of the Association for ComputationalLinguistics, Ann Arbor, Mich., 25?30 June 2005,pages 53?56.Rada Mihalcea.
2006.
Knowledge-based methodsfor WSD.
In E. Agirre and P.G.
Edmonds, editors,Word Sense Disambiguation: Algorithms and Appli-cations, pages 107?131.
Springer, Heidelberg, Ger-many.David Milne and Ian H. Witten.
2008.
Learning to linkwith Wikipedia.
In Proceedings of the ACM 17thConference on Information and Knowledge Man-agement (CIKM 2008), Napa Valley, Cal., USA, 26?30 October 2008, pages 1046?1055.Jane Morris and Graeme Hirst.
1991.
Lexical cohe-sion computed by thesaural relations as an indicatorof the structure of text.
Computational Linguistics,17(1):21?48.Rani Nelken and Stuart Shieber.
2006.
Lexical chain-ing and word-sense-disambiguation.
Technical Re-port TR-06-07, Computer Science Group, HarvardUniversity, Cambridge, Mass.Robert Parker, David Graff, Junbo Kong, Ke Chen, andKazuaki Maeda.
2011.
English Gigaword Fifth Edi-tion.
LDC2011T07.Slav Petrov, Leon Barrett, Romain Thibaux, and DanKlein.
2006.
Learning accurate, compact, andinterpretable tree annotation.
In Proceedings ofthe 21st International Conference on ComputationalLinguistics and 44th Annual Meeting of the Associ-ation for Computational Linguistics, Sydney, Aus-tralia, 17?21 July 2006, pages 433?440.Hoifung Poon and Pedro Domingos.
2008.
Joint unsu-pervised coreference resolution with Markov Logic.In Proceedings of the 2008 Conference on EmpiricalMethods in Natural Language Processing, Waikiki,Honolulu, Hawaii, 25?27 October 2008, pages 650?659.Dragomir R. Radev, Simone Teufel, Horacio Saggion,Wai Lam, John Blitzer, Hong Qi, Arda Celebi,Danyu Liu, and Elliott Drabek.
2003.
Evaluationchallenges in large-scale document summarization.In Proceedings of the 41st Annual Meeting of the As-sociation for Computational Linguistics, Sapporo,Japan, 7?12 July 2003, pages 375?382.Lev Ratinov, Dan Roth, Doug Downey, and Mike An-derson.
2011.
Local and global algorithms for dis-ambiguation to Wikipedia.
In Proceedings of the49th Annual Meeting of the Association for Com-putational Linguistics, Portland, Oreg., 19?24 June2011, pages 1375?1384.Sebastian Riedel.
2008.
Improving the accuracy andefficiency of MAP inference for Markov logic.
InProceedings of the 24th Conference on Uncertaintyin Artificial Intelligence, Helsinki, Finland, 9?12July 2008, pages 468?475.Noah A. Smith.
2011.
Linguistic Structure Prediction.Morgan & Claypool Publishers.Rakshit Trivedi and Jacob Eisenstein.
2013.
Dis-course connectors for latent subjectivity in sentimentanalysis.
In Proceedings of the 2013 Conference ofthe North American Chapter of the Association forComputational Linguistics: Human Language Tech-nologies, Atlanta, Georgia, 9?14 June 2013, pages808?813.Ainur Yessenalina, Yejin Choi, and Claire Cardie.2010.
Automatically generating annotator rationalesto improve sentiment classification.
In Proceedingsof the 48th Annual Meeting of the Association forComputational Linguistics, Uppsala, Sweden, 11?16 July 2010, pages 336?341.Yiping Zhou, Lan Nie, Omid Rouhani-Kalleh, FlavianVasile, and Scott Gaffney.
2010.
Resolving sur-face forms to Wikipedia topics.
In Proceedingsof the 23rd International Conference on Compu-tational Linguistics, Beijing, China, 23?27 August2010, pages 1335?1343.500
