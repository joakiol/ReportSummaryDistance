Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 967?975,Beijing, August 2010A Multiple-Domain Ontology BuilderSara SalemSamir AbdelRahmanAbstractThe interpretation of a multiple-domaintext corpus as a single ontology leads tomisconceptions.
This is because someconcepts may be syntactically equal;though, they are semantically lopsided indifferent domains.
Also, the occurrencesof a domain concept in a large multiple-domain corpus may not gauge correctlythe concept significance.
This papertackles the mentioned problems and pro-poses a novel ontology builder to extractseparate domain specific ontologies fromsuch a corpus.
The builder contribution isto sustain each domain specific conceptsand relations to get precise answers foruser questions.
We extend a single ontol-ogy builder named Text2Onto to applyour thought.
We fruitfully enhance it toanswer, more precisely, questions on asubset of AQUAINT corpus.1 IntroductionDomain ontology is a knowledge representationof the domain as a set of concepts and relations.Ontology notion always presents handy semanticsolutions for various hot research areas such asSemantic Web, Informational Retrieval, andQuestion Answering.Currently, automatic ontology builders pre-sume that the given corpus has a single domain.When used with a multiple-domain corpus, thesebuilders generate 1 large ontology for the wholecorpus.
Dramatically, this causes 2 domain mis-conception problems.
First, the ontology concep-tual model becomes imprecise for the commonconcepts in various domains having differentsemantics.
Second, the relevance weights as-signed to the concepts do not measure preciselytheir significance in specific domains.This paper presents a promising solution forthe mentioned problems.
The proposed solutionis an integrated 2-layer ontology builder.
Theontology layers are: 1) the conceptual layer,which has the key concepts and relations of eachseparate domain, and 2) the general layer, whichmaintains the general domain information re-garding related persons, organizations, locations,and dates.
Our proposed 2-layer ontology im-proves the extracted answers for single-domainand cross-domain questions.
We successfullyprove our thought against Text2Onto builder.Ontology extraction from a domain corpus hasbeen targeted by many researchers.
The core ex-traction approaches can be classified into 3 ap-proaches.
The first approach is to build the on-tology from scratch (Buitelaar et al, 2004; Ci-miano and V?lker, 2005).
The second approachis to extend a predefined general ontology, suchas WordNet, with possible application domainconcepts and relations (Navigli and Velardi,2004).
The last approach is to build ontology as acomposition of other predefined ontologies (Ci-miano et al, 2006).
Moreover, as an ontologybuilding design decision, the resultant ontologyis either a single layer ontology or a multi-layered ontology (Benslimane et al, 2000; Du-montier and Villanueva-Rosales, 2007).The paper is organized as follows: Section 2introduces some related systems; Section 3 ex-plains the misconceptions due to extracting asingle ontology from a multiple-domain corpus;Section 4 describes our proposed builder; Section5 illustrates our Question Answering system,which is used for the evaluation; Section 6 statesour evaluation results; and Section 7 draws ourconclusion and directions for the future work.Computer Science DepartmentFaculty of Computers and Information - Cairo University{s.salem,s.abdelrahman@fci-cu.edu.eg}9672 Related WorkThere are 3 main approaches for ontology build-ing, namely building from scratch, extending ageneral ontology, or building an ontology as acomposition of other predefined ontologies.Text2Onto (Cimiano and V?lker, 2005) ap-plies the first approach.
It is a framework forlearning ontologies automatically from textualdata.
It implements diverse linguistic and statis-tical techniques to extract domain concepts andrelations.
It combines results from different tech-niques, and it represents the extracted ontologyelements in a so called Probabilistic OntologyModel (POM), which assigns a confidence valuefor each learnt element.OntoLT (Buitelaar et al, 2004) is another ex-ample of building from scratch.
It is a Prot?g?1plug-in that extracts ontology from text by defin-ing a set of mapping rules.
The rules map certainlinguistic structures in an annotated text into on-tological elements.
The extracted elements arevalidated by the user before being inserted intothe ontology.OntoLearn (Navigli and Velardi, 2004) em-ploys the second approach.
It is a framework fortrimming and extending general purpose ontolo-gies, like WordNet, with specific domain termi-nologies and taxonomies.
It extracts domain ter-minologies, and it uses a relevance measure tokeep out non-relevant terms.
OntoLearn uses anovel technique, called SSI, to assign a domainspecific term to the correct sense in a generalontology.The third approach is proposed in (Cimiano etal., 2006).
It presents a system that integratesseveral heterogeneous semantic sources into 1ontology, which is used to extract answers foruser queries from various knowledge sources.As a design decision, the ontology may consistof a single layer or of multiple layers.
Bensli-mane et al (2000) apply the multiple-layer ap-proach for manually generating a set of interre-lated ontology layers; each layer models a spatialdomain specific function.
Also, Dumontier andVillanueva-Rosales (2007) suggest a 3-layer on-tology design.
The first layer (primitive layer)defines the basic domain concepts and relations.The second layer (complex layer) imposes morecomplex domain restrictions on the primitive1 http://protege.stanford.edu/layer.
The top layer (application layer) maintainsapplication specific restrictions.Our builder constructs a layered ontologyfrom scratch.
Its main distinguished features are:1) generating separate domain specific ontolo-gies from a multiple-domain corpus, 2) extract-ing general domain information, in addition tocore domain conceptual information, and 3) it isan automatic multi-layered ontology builder, un-like other automatic builders, which generatesingle layer ontologies.Our system can extend current builders, whichextract ontologies from textual data, allowingthem to handle a multiple-domain corpus.
Weselected Text2Onto because it is an automaticontology builder, and it implements a variety ofalgorithms to extract many types of ontologyelements.
We use a news corpus as a multiple-domain corpus since it contains documents fromdifferent domains like Politics, Sports, Arts, andFinance.3 Ontology MisconceptionsBuilding a single ontology for a given corpus is afamiliar method.
However, when dealing with amultiple-domain corpus, the builder usually suf-fers from the following 2 problems:First, the ontology conceptual model becomesimprecise in the definition of common conceptsthat are semantically lopsided in different do-mains.
For example, the concept "wall street" inthe Finance domain is defined as a financial in-stitution, and it is in the Arts domain defined as amovie.
It is inaccurate to define the concept with2 totally different meanings in 1 ontology.
It isalso incorrect to ignore a definition of them.When using Text2Onto for that example, it gene-rates only 1 definition for "wall street" as a sub-class-of "institution".Second, when weighing concepts in amultiple-domain corpus, the relevance weightsassigned to the concepts do not indicate thesignificance of each concept in a certain domain.As a result, some core domain specific conceptsmay have low weights with respect to the wholecorpus.
For example the concept "trading" has alow weight in a multiple-domain corpus;although, it is a main concept in the Financedomain (Section 6.2).
This gives wrongindication of the concept importance to the user.968Figure 1.
The Multiple-Domain Ontology Builder.
?Clustering?Clustered?corpus?Input CorpusConceptual?Layer?
?Generator?General?Layer?GeneratorThe?General?LayerThe?Conceptual?Layer?4 The Proposed Ontology BuilderOur builder aims to extract precise ontologies,which model possible knowledge in a multiple-domain corpus.
A domain corpus, mostly, notonly contains information about the core domainconcepts and their relations, but it also containsgeneral domain information such as dates ofevents and names of persons, locations, or organ-izations participating in the domain.
Existingontology builders either ignore this general in-formation or they provide a limited implementa-tion to extract it.4.1 System OverviewThe input to our builder (Figure 1) is a multiple-domain corpus.
The first step is the clusteringoperation, which divides the given corpus doc-uments into clusters that are different amongeach other with high internal similarity.
The nextstep is the conceptual layer generation.
In thisstep, we use Text2Onto to extract a separate on-tology for each domain.
Finally, the generallayer generator uses each domain corpus and theconceptual layer ontology to extract relationsamong the concepts and the Named Entities inthat domain.4.2 The Conceptual LayerThe first step in constructing the conceptual layeris the clustering operation.
We separate a mul-tiple-domain corpus into various domain specificcorpora such that the domain concepts areweighted based on their significance in that do-main; also, the common concepts in differentdomains are separated.
We favored a hierarchicalclustering technique over a flat clustering one.That was because the number of resulting clus-ters should be known as a parameter in the latter.However, the number of corpus domains mightbe unknown in our case.We employ the agglomerative hierarchicalclustering technique (Manning et al, 2008).
Thetechnique starts with each document as a single-ton cluster, and then it successively merges pairsof similar clusters until all clusters are mergedinto 1 cluster.
We use the vector space model(Manning et al, 2008) to represent each docu-ment as a vector of terms' weights.
The weight ofa term w in a document d is calculated using theTF-IDF measure (Equation 1).??????
?, ??
?
???
?, ??
?
?????????
?1?Where N is the corpus size, TF (w,d) is thenumber of occurrences of the term w in the doc-ument d, and DF (w) is the number of documentscontaining the term w.The similarity between 2 documents is calcu-lated using the Cosine Similarity measure (Equa-tion 2).???????
?1, ?2?
????1?
.
???2?||??
?1?|| ?
||??
?2?||?2?Where V(d) is the terms' weights vector for thedocument d, ||V(d)|| is the Euclidean length ofthe vector V(d), and the numerator is the dotproduct of the 2 vectors.The similarity between 2 clusters is calculatedusing the UPGMA measure (Steinbach et al,2000) (Equation 3).969Figure 2.
The General Relations Extraction.
?MixedTaggingInputTaggedTextRelationsExtractorPatternsRelations ???
?1?, 2??
??
???????
?1, ?2????????????????1??
?
?????2??
?3?We use the UPGMA measure to cluster a sub-set of DMOZ2 data (1322 documents, in 4 do-mains), and it performs F-Measure of 0.86.Steinbach et al (2000) describe how to calculateF-Measure for a hierarchy of clusters.The combination similarity is the similarity of2 merged clusters.
We use this measure to cut theclusters hierarchy into M clusters by groupingones having a minimum combination similarityof the threshold value ?3.
After clustering, we useText2Onto to generate an ontology for each clus-ter (domain).4.3 The General LayerText2Onto performs well in extracting ontologyelements such as concepts, sub-class-of relations,instance-of relations, and part-of relations.
Un-fortunately, it performs inaccurately in extractinggeneral domain information such as Named Enti-ties and numeric information.
There are 3 rea-sons for such misconception.
First, proper nounsare not extracted as concepts.
Second, numericdata is ignored.
Third, restricted patterns are ap-plied for the relations of Named Entities, thatinclude only verb relations like [(NP |PNP) verb(NP|PNP)] and instance-of relations like [NPsuch as PNP], [such NP as PNP], and [PNP(NP)].Because of the above reasons, we propose ahighly flexible pattern based relation extractor.In our system, a pattern is a sequence of tags inthe form of a regular expression.
The possibletags are the normal POS tags like NN, VB, JJ, INbesides the following 5 tags CONCEPT, PER-SON, LOCATION, ORGANIZATION, andDATE.
This criterion is called Mixed Tagging.Currently, dates are the only data containingnumbers extracted by our builder, but we caneasily extend it to handle more numeric data.The Mixed Tagging operation inputs are adocument and the related conceptual ontology(Figure 2).
The operation output is a mixedtagged document.
The tagged text is then pro-vided to the Relations Extractor to take out all2 http://www.dmoz.org/3 For clustering 600 AQUAINT documents, we use ?=0.55resulting in 7 Clusters (Secion 6.4).relations matching our current predefined pat-terns.
Example patterns are listed in Table 1; thefirst 2 patterns are verb relations, and the last 2are noun relations.The regular expression ([.
{1,12}]){0,5} isused to limit the maximum number of tokensbetween the subject, the object, and the relationto 5 tokens.
The expression [NN.?.?]
matchesany noun tag, and [VB.?]
matches any verb tag.After extracting the relations in all domaindocuments, the domain general ontology iscreated.
It imports the corresponding conceptualontology to model the relations among NamedEntities and concepts.
([PERSON]) ([.{1,12}]){0,5}([VB.?])+([.{1,12}]){0,5}([CONCEPT])([ORGANIZATION])([.{1,12}]){0,5}([DATE])([.{1,12}]){0,5}([VB.?])+([PERSON])([.{1,12}]){0,5}([NN.?.?])+([.{1,12}]){0,5}([DATE])([NN.?.?])+([.{1,12}]){0,5}([PERSON])([.
{1,12}]){0,5}([ORGANIZATION])Table 1.
Sample Relation Patterns.5 Question Answering SystemBased on (Brank et al, 2005), a generated ontol-ogy can be evaluated using 4 different ways: 1)by a human who assesses it based on specificcriteria, 2) by a comparison with the source data,3) by a comparison with a golden standard, or 4)by using the ontology in an application and mea-suring the application performance.
We chosethe last option because the manual human as-sessment and the comparison with the sourcedata are time consuming.
Also, there is no goldenstandard ontology for a multiple-domain corpus.Recently, researchers have studied the use ofontologies to extract answers to the userquestions.
AquaLog (Lopez et al, 2007) and970Figure 3.
The Question Answering System.Indexer?
Ontology?Index?QuestionQuestion??Parser?Question?Elements?Concepts?Finder?Concepts??URIs?Triples??Finder?Ontology??Triples?Triples??Weighting?Answer??Extraction?Weighted?Answer(s)?Our?Ontology???
?Weighted?Triples?PowerAqua (Lopez et al, 2009) are bothontology based Question Answering systems.PowerAqua extracts answers from variousontologies available on the web, unlikeAquaLog, which extracts answers from 1configurable ontology.5.1 System DescriptionWe implemented our simple Question Answer-ing system handling who, when, where, and whatquestions.
In the following, we describe thecomponents of the system (Figure 3).The Indexer: to make it easier for the systemto locate the question concepts, an index is gen-erated for our layered ontology.
All concepts indifferent ontologies containing a certain stem aregrouped in an index entry in the index file.
Theform of an index entry is as follows:Stem,(Concept URI)+The Question Parser: this component parsesthe user question, and it extracts 4 elements fromit.
First, the answer type; it can be PERSON,LOCATION, ORGANIZATION, DATE, orANY based on the question type such as who,where, when, or what.
Second, the answer re-striction; it is used to limit the answers of whatquestions.
For example, the answers for "whatsport ??"
question are restricted only to thesport types.
Third, the question target; it definesthe thing in which the question is interested.
Thefourth element is the relation; it contains themain verb(s) in the question.
As an example, theelements of the question "What sport does Jenni-fer Capriati play?"
are: the answer type (ANY),the restriction (sport), the question target (Jenni-fer Capriati), and the relation (play).For a compound (2-clause) question such as"What countries have Rhodes Scholars comefrom and has the Hale Bopp comet visible?
",each question clause is parsed as a separate ques-tion; finally, the answer extraction step intersectsthe answers of both clauses.The Concepts Finder: using the ontology in-dex, it locates concepts containing the stems ofthe question target and the restriction (if exists).The Triples Finder: it extracts the tripleswhich contain the question target concepts eitheras subjects or as objects.
If the question is a defi-nition question like "What is something?
", thetriple finder extracts only the sub-class-of triples.The Triples Weighting: the triples areweighted based on their similarity to the questionusing our similarity criterion (Equation 4):????
?, ??
??
????
?, ????????????
?
????
?4?Where Q and T are sets of the bag-of-wordsfor the question relation and the triple relationrespectively, Lin(a,b) is a measure for the seman-tic similarity between a and b based on WordNet(Lin, 1998), and L(x) is the number of elementsin the set x.The Answer Extraction: this component firstfilters out the triples mismatching the expectedanswer type.
Then, if there is no restriction ele-ment, it extracts the answer from the weightedtriples by considering the triple object if thequestion target is the subject, and vice versa.
Theextracted answer from a triple is assigned thesame triple weight.
If the question has a restric-tion element, the answer(s) will be limited to thesub concepts of the restriction element.
A weight(Equation 5) is assigned to each sub concept sbased on its similarity to the extracted triples asfollows:971????
??
????
?, ?????????
?5?Where R is the set of extracted triples, S and Tare the sets of bag-of-words for the sub conceptand the triple relation respectively, sim(S,T) iscalculated using Equation 4, and L(R) is thenumber of elements in R.For a compound question, the list of resultinganswers contains only the common answers ex-tracted for the 2 clauses.6 Evaluation and DiscussionIn our evaluation, we assess: 1) the enhancementof the concepts' weights in a specific domaincorpus, 2) the enhancement of modeling com-mon concepts in different domains with differentsemantics, and 3) the performance of our  Ques-tion Answering system.
The assessment is donethrough a comparison between our approach andText2Onto.In the development of our builder, we usedText2Onto 4 , Stanford Part-Of-Speech Tagger(POS Tagger)5, Stanford Named Entity Recog-nizer (NER)6, and Jena7.
In the Question Ans-wering system, we also used the Java WordNetSimilarity Library (JWSL)8; it implements theLin measure.6.1 Data SetOur evaluation is based on the AQUAINT9 cor-pus (Graff, 2002).
It is an English news corpuscontaining documents from the New York TimesNews Service, the Xinhua News Service, and theAssociated Press Worldstream News Service.The Question Answering track in TREC10 (TheText REtrieval Conference) provides a set ofquestions on AQUAINT corpus along with theiranswers.6.2 Concepts Weights EnhancementFor this experiment, we generated a corpus forthe 3 domains, namely Finance, Sports, and4 http://code.google.com/p/text2onto/5 http://nlp.stanford.edu/software/tagger.shtml6 http://nlp.stanford.edu/software/CRF-NER.shtml7 http://jena.sourceforge.net/8 http://grid.deis.unical.it/similarity/9http://www.ldc.upenn.edu/Catalog/CatalogEntry.jsp?catalogId=LDC2002T3110 http://trec.nist.gov/data/qa.htmlMovies, from AQUAINT documents, such thateach domain has equal number of documents.We measured the concepts' significance weightswhen using Text2Onto to generate a single on-tology for the whole corpus and when using ourbuilder to generate 3 different domains ontolo-gies.
We consider 3 measures implemented inText2Onto, namely the Relative Term Frequency(RTF), the Entropy, and the TF-IDF.The RTF for a concept w is the probability ofthe concept occurrence in the corpus (Equation6).????
???.
?
??
??????????
????.
????????
??????
???
??
?6?The entropy and the normalized entropy for aconcept w are calculated as follows (Equations 7and 8 respectively):??????
?
????
?
log ????
?7??????????
??????????
???
?
???
???
?8?In Section 4.2, we mention how to calculatethe TF-IDF value for a term w in a document d(Equation 1).
The TF-IDF weight and the norma-lized TF-IDF weight for a concept w in thewhole corpus are calculated as follows (Equa-tions 9 and 10 respectively):????????
??
??????
?, ???????9????????????
???????????
??????????????
?10?Where D is the set of documents containing w,N is the corpus size, and C is the set of all con-cepts in the corpus.Since the concept weight is proportional to itsoccurrences in the corpus with respect to the oth-er concepts, the fair distribution of the occur-rences leads to precise weight calculation.
In thespecific domain corpus, the distribution is morereasonable than in multiple-domain corpus.972Domain Concept Entropy TF-IDF RTFText2OntoOurBuilderText2OntoOurBuilderText2OntoOurBuilderFinance Stock 0.181 0.999 0.053 0.103 0.001 0.020Trading 0.155 0.670 0.044 0.139 0.001 0.010Shares 0.100 0.670 0.036 0.139 0.000 0.010Economy 0.100 0.670 0.026 0.051 0.000 0.010Sports Sport 0.822 0.974 0.344 0.379 0.012 0.019Baseball 0.321 0.389 0.147 0.190 0.003 0.006League 0.299 0.363 0.134 0.174 0.003 0.005Football 0.205 0.251 0.085 0.111 0.002 0.003Movies Actor 0.525 0.613 0.150 0.194 0.007 0.022Movie Industry 0.230 0.362 0.098 0.263 0.002 0.011Music 0.205 0.326 0.085 0.230 0.002 0.009Home Video 0.038 0.066 0.012 0.032 0.000 0.001Table 2.
Concepts Weights Comparison between Our Builder and Text2Onto.This fact can be verified easily from Table 2.The 3 measures give higher weights in the do-main specific ontologies than in a single ontolo-gy for the whole corpus.6.3 Modeling Common ConceptsTo study the enhancement in modeling commonconcepts having different meaning in differentdomains, we chose 5 concepts as samples (Table3).
For each concept, we selected documentsfrom AQUAINT and from the Wikipedia con-cerning the concepts in 2 different domains.In this experiment, the single ontology gener-ated by Text2Onto contains only 1 definition foreach concept namely wall_street is_a institution,marijuana is_a drug, bear is_a mammal, jaguaris_a cat, and world_war is_a war.
On the otherhand, our builder maintains both concept defini-tions in different ontologies.Concept Definition 1 Definition 2Wall Street A financialInstitutionA movieMarijuana A drug A songThe bear A Mammal A movieJaguar A big cat A carWorld War A war A museumTable 3.
Sample of Lopsided Concepts.6.4 Question Answering EnhancementThe experiment includes common concepts defi-nition questions, single-domain questions, andcross-domain questions.To illustrate the effect of the common con-cepts misconception problem solved by ourbuilder against Text2Onto, we generated 5 defi-nition questions for the 5 concepts in Table 3,like "what is wall street?
", "what is marijua-na?
"?etc.For the single-domain questions, we used asubset of AQUAINT corpus composed of 600documents clustered into 7 domains using com-bination similarity threshold value of 0.55.
Weselected 60 factoid questions from TREC 2004questions having their answers in these docu-ments.
Examples of single-domain questions are:?
Who discovered prions??
When was the IFC established?In addition to factoid questions, TREC 2004also includes list questions.
The answers of eachquestion are aggregated from multiple docu-ments.
We used these questions in generating 10cross-domain questions.
Each question combines2 of TREC list questions such that the 2 list ques-tions are in different domains.
Examples of thesequestions are:?
What cities have an Amtrak terminal andhave Crip gangs??
What countries are Burger King located inand have IFC financed projects?Evaluation Criteria: the accuracy (A) (Equa-tion 11) is used for evaluating single-domainquestions because each factoid question has only1 correct answer.973?
???.
???????
???????
????.
?????????
??
?11?The definition and cross-domain questionshave multiple correct answers.
The average Pre-cision (P), Recall (R), and F-Measure (F) (Equa-tions 12, 13, and 14 respectively) of all questionsare used for our evaluation.?
???.
???????
???????
????.
???????
?????????
???12??
???.
???????
???????
????.
???????
??????
???13??
?2 ?
?
?
??
?
?
?14?Table 4 shows that, in the definition questions,we achieve F-Measure of 1, while Text2Ontoachieves 0.5.
This is because our builder main-tains the 2 different definitions of each concept,unlike Text2Onto, which contains only one.QuestionsTypeOurOntologyText2OntoOntologyDefinitionQuestionsP=1.0R=1.0F=1.0P=0.5R=0.5F=0.5Single-Domain  A=68% A=0.05%Cross-DomainP=0.49R=0.59F=0.44P=0R=0F=0Table 4.
Question Answering Evaluation.In the single-domain questions, using our on-tology, we could answer 41 questions while us-ing Text2Onto ontology we could answer only 3questions ("what particle is a quark?
", "what areprions made of?
", and "What is the treatment ofcataract?").
The low coverage of Named Entitiesin Text2Onto hinders it from answering correctlyany question of types Who, When, and Where.This indicates the enhancement introduced by theproposed general layer for modeling accuratelymore domain information.
In the cross-domainquestions, we achieve F-Measure of 0.44.
Noneof the cross-domain questions are answered us-ing Text2Onto ontology due to the mentionedNamed Entity coverage problem.Although our results are better thanText2Onto, there is a room for more improve-ments.
There are 4 main sources for retrievingwrong or incomplete answers (Table 5).
Somerelations are not extracted because their elements(subject, relation, and object) are not nearenough from each other in the text, so none ofour patterns or Text2Onto patterns could matchthem.
This is the source of 65% of the errors.Missed Named Entities or wrongly tagged onescause 16% of the errors.
Some relations are notextracted because co-reference has not been han-dled yet.
That leads to 12% of the total errors.Finally, in the factoid questions, we consider theanswer with the highest weight to be the correctanswer; 7% of the answers are extracted but withlower weights.Error Type Error percentageNo matching pattern 65%NER Error 16%Co-Reference 12%Low answer weight 7%Table 5.
Answer Error Sources.Based on the mentioned experiments, ourbuilder outperforms Text2Onto in QuestionAnswering.
In addition, it can be used skillfullyto enhance other Natural Language Processingapplications such as Information Retrieval frommultiple-domain data.
Our initial results using220 queries on 600 AQUAINT documentsrecords 0.35 F-Measure against Lucene11, whichachieves 0.18.7 Conclusion and Future WorkThis paper presents the misconception problemswhen interpreting a multiple-domain corpus in asingle ontology.
A novel ontology builder is pre-sented handling these problems by generatingseparate domain ontologies describing core andgeneral domain information.Currently, we hand on improving our builderrelation extractor to answer more TREC ques-tions by automatically learning patterns from textand by handling co-reference.
Moreover, we areworking to enhance the performance of our In-formation Retrieval system.11 http://lucene.apache.org/974ReferencesBenslimane, D., E. Leclercq, M. Savonnet, M.-N.Terrasse, and K. Y?tongnon.
2000.
On the Defini-tion of Generic Multi-layered Ontologies for Ur-ban Applications.
In the International Journal ofComputers, Environment, and Urban Systems, vo-lume 24: 191-214.Brank, Janez, Marko Grobelnik, and Dunja Mladeni?.2005.
A Survey of Ontology Evaluation Tech-niques.
In the Proceedings of the 8th InternationalMulticonference on Information Society: 166-169.Buitelaar, Paul, Daniel Olejnik, and Michael Sintek.2004.
A Prot?g?
Plug-In for Ontology Extractionfrom Text Based on Linguistic Analysis.
In the Pro-ceedings of the 1st European Semantic Web Sym-posium: 31-44.Cimiano, Philipp, and Johanna V?lker.
2005.Text2Onto - A Framework for Ontology Learningand Data-driven Change Discovery.
In the Pro-ceedings of the 10th International Conference onApplications of Natural Language to InformationSystems: 227-238.Cimiano, Philipp, Peter Haase, York Sure, JohannaV?lker, and Yimin Wang.
2006.
Question Answer-ing on Top of the BT Digital Library.
In the Pro-ceedings of the 15th International Conference onWorld Wide Web: 861-862.Dumontier, Michel, and Natalia Villanueva-Rosales.2007.
Three-Layer OWL Ontology Design.
In theProceedings of the 2nd International Workshop onModular Ontologies.
CEUR Workshop Proceed-ings, volume 315.Graff, David.
2002.
The AQUAINT Corpus of EnglishNews Text.
Linguistic Data Consortium, Philadel-phia.Lin, Dekang.
1998.
An Information-Theoretic Defini-tion of Similarity.
In the Proceedings of the 15th In-ternational Conference on Machine Learning: 296-304.Lopez, Vanessa, Victoria Uren, Enrico Motta, andMichele Pasin.
2007.
AquaLog: An Ontology-driven Question Answering System for Organiza-tional Semantic Intranets.
In the Journal of WebSemantics, volume 5: 72-105.Lopez, Vanessa, Victoria Uren, Marta Sabou, andEnrico Motta.
2009.
Cross Ontology Query Ans-wering on the Semantic Web: An Initial Evalua-tion.
In the Proceedings of the 5th InternationalConference on Knowledge Capture: 17-24.Manning, Christopher D., Prabhakar Raghavan, andHinrich Sch?tze.
2008.
Introduction to InformationRetrieval.
Online edition.http://nlp.stanford.edu/IR-book/information-retrieval-book.html.
Cambridge University Press.Navigli, Roberto, and Paola Velardi.
2004.
LearningDomain Ontologies from Document Warehousesand Dedicated Web Sites.
In the Journal of Compu-tational Linguistics, volume 30: 151-179.Steinbach, Michael, George Karypis, and Vipin Ku-mar.
2000.
A Comparison of Document ClusteringTechniques.
Technical Report #00-034, Universityof Minnesota.975
