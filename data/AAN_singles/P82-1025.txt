P~FLECTIONS ON TWENTY YEARS OF THE ACLJonathan AllenResearch Laboratory of ElectronicsandDepartment of Electrical Engineering and Computer ScienceMassachusetts Institute of TechnologyCambridge, MA 02139I entered the field of computationallinguistics in 1967 and one of my earliestrecollections is of studying the Harvard SyntacticAnalyzer.
To this date, this parser is one of thebest documented programs and the extensivediscussions cover a wide range of English syntax.It is sobering to recall that this analyzer wasimplemented on an IBM 7090 computer using 32Kwords of memory with tape as its mass storagemedium.
A great deal of attention was focussedon means to deal with the main memory and massstorage limitations.
It is also interesting toreflect back on the decision made in the HarvardSyntactic Analyzer to use a large number of partsof speech, presumably, to aid the refinement ofthe analysis.
Unfortunately, this introduction ofsuch a large number of parts of speech(approximately 300) led to a large number ofunanticipated ambiguous parsings, rather thancutting down on the number of legitimateparsings as had been hoped for.
This analyzerfunctioned at a time when revelations about theamount of inherent ambiguity in English (and othernatural languages) was a relatively new thing andthe Harvard Analyzer produced all possibleparsings for a given sentence.
At that time, someeffort was focused on discovering a use for allthese different parsings and I can recall that onesuch application was the parsing of the GenevaNuclear Convention.
By displaying the largenumber of possible interpretations of thesentence, it was in fact possible to flush outpossible misinterpretations of the document andI believe that some editing was performed in orderto remove these ambiguities.In the late sixties, there was also asubstantial effort to attempt parsing in terms ofa transformational grammar.
Stan Petrick'sDoctoral Thesis dealt with this problem, usingunderlying logical forms very different from thosedescribed by Chomsky, and another effort at MitreCorporation, led by Don Walker, also built atransformational parser.
I think it is signifi-cant that this early effort at Mitre was one ofthe fJr=~ examples where linguists were directlyinvolved in computational applications.It is in=cresting that in the development ofsyntax, from the perspective of both linguists andcomputational linguists, there has been acontinuing need to develop formalisms thatprovided both insight, as well as coverage.
Ithink these two requirements can be seen both intransformational grammar and the ATN formalism.Thus, transformational grammar provided a simple,insightful base through the use of context-freegrammar and then provided for the difficulties ofthe syntax by adding on to this base the use oftransformations and of course, gaining turingmachine power in the process.
Similarly, ATNsprovided the simple base of a finite state machineand added to it turing machine power through theuse of actions on the arcs.
It seems to benecessary to provide some representational meansthat is relatively easy to think about as a baseand then contemplate how these simpler base formscan be modified to provide for the range of actualfacts of natural language.Moving to today's emphasis, we see increasedinterest in psychological reality.
An example ofthis work is'the thesis of M itch Marcus, whichattempts to deal with constraints imposed byhuman performance, as well as constraints of amore universal nature recently characterized bylinguists.
This model has been extended furtherby Bob Berwick to serve as the basis for alearning model.
Another recent trend that causesme to smile a little is the resurgence of interestin context free grammars.
I think back to Lyons'book on theoretical linguistics where context freegrammar is chastised as was the custom, due to itsinability to insightfully characterize subject-verb agreement, discontinuous constituents, andother things thought inappropriate for contextfree grammars.
The fact that a context freegrammar can always characterize any finite segmentof the language was not a popular notion in theearly days.
Now we find increasing concern withefficiency arguments, and also due to theincreasing emphasis in trying to find the simplestpossible grammatical formalism to describe thefacts of language, a vigorous effort to providecontext free systems that provide a great deal ofcoverage.
In the earlier days, the necessity ofintroducing additional non-terminals to deal withproblems such as subject-verb agreement was seenas a definite disadvantage, but today suchcriticisms are hard to find.
An additional trendthat is interesting to observe is the currentemphasis on ill-formed sentences which are nowrecognized as valid exemplars of the language andwith which we must deal in a variety ofcomputational applications.
Thus, there has beenattention focused on relaxation techniques and the104ability to parse limited phrases within discoursestructures that may be ill-formed.In the early days of the ACL, I believe thatcomputation was seen mainly as a tool used torepresent algorithms and provide for theirexecution.
Now there is a much different emphasison computation.
Computing is seen as a metaphor,and as an important means to model varioUslinguistic phenomena, as well as more broadlycognitive phenomena.
This is an important trend,and is due in part to the emphasis in cognitivescience on representational i?sues.
When we mustdeal representations explicitly, then the branchof knowledge that provides the most help iscomputer science, and this fact is becoming muchmore widely appreciated, even by those workerswho are not focused primarily on computing.
Thisis a healthy trend, I believe, but we need also tobe aware of the possibility of introducing biasesand constraints on our thinking dictated by ourcurrent understanding and view of computation.Since our view of computation is in turn condi-tioned very substantially by the actual computingtechnology that is present at any given time, itis well to be very cautious in attributing basicunderstanding of these representations.
Aparticular case in point is the emphasis, quitepopular today, on parallelism.
When we were usedto thinking of computation solely in terms ofsingle-sequence Von Neumann machines, thenparallelism did not enjoy a prominent place inour models.
Now that it is possible technologi-cally to implement a great deal of parallelism,one can even discern more of a move to breadthfirst rather than depth first analyses.
It seemsclear that we are still very much the children ofthe technology that surrounds us.I want to turn my attention now to adiscussion of the development of speech processingtechnology, in particular, text-to-speechconversion and speech recognition, during the lasttwenty years.
Speech has been studied over manydecades, but its secrets have been revealed at avery slow pace.
Despite the substantial in fusionof money into the study of speech recognition inthe seventies, there still seems to be a naturalgestation period for achieving new understandingof such complicated phenomena.
Nevertheless,during these last twenty years, a great deal ofuseful speech processing capability has beenachieved.
Not only has there been much achieve-ment, but these results have achieved greatprominence through their coupling with moderntechnology.
The outstanding example in speechsynthesis technology has been of course the TexasInstruments Speak and Spell which demonstrated forthe first time that acceptable use of syntheticspeech could be achieved for a very modest price.Currently, there are at least 20 differentintegrated circuits, either already fabricated orunder development, for speech synthesis.
So ahuge change has taken place.
It is possible todayto produce highly intelligible synthetic speechfrom text, using a variety of techniques incomputational linguistics, including morphologicalanalysis, letter-to-sound rules, lexical stress,syntactic parsing, and prosodic analysis.
Whilethis speech can be highly intelligible, it iscertainly not very natural yet.
This reflects inpart the fact we have been able to determinesufficient correlates for the percepts that wewant to convey, but that we have thus far beenunable to characterize the redundant interactionof a large variety of correlates that lead tointegrated percepts in natural speech.
Even suchsimple distinctions as the voiced/unvolcedcontrast are marked by more than a dozen differentcorrelates.
We simply don't know, even after allthese years, how these different correlates areinterrelated as a function of the local context.The current disposition would lead one to hopethat thls interaction is deterministic in nature,but I suppose there is still some segment of theresearch community that has no such hopes.
Whenthe redundant interplay of correlates is properlyunderstood, I believe this will herald a newimprovement in our understanding needed for highperformance speech recognition systems.
Neverthe-less, it is important to emphasize that duringthese twenty years, commercially acceptable text-to-speech systems have become viable, as well asmany other speech synthesis systems utilizingparametric storage or waveform coding techniquesof some sort.Speech recognition has undergone a lot ofchange during this period also.
The systems thatare available in the marketplace are still basedexclusively on template matching techniques,which probably have little or nothing to do withthe intrinsic nature of speech and language.
Thatis to say, they usa some form of informationally-reduced representation of the input speech wave-form and then contrive to match this representa-tion against a set of stored templates.
Varioustechniques have been introduced to improve theaccuracy of this matching procedure by allowingfor modifications of the input representation orthe stored templates.
For example, the use ofdynamic programming to facilitate matching hasbeen very popular, and for good reason, since itsuse has led to improvements in accuracy ofbetween 20 and 30 percent.
Nevertheless, Ibelieve that the use of dynamic programming willnot remain over the long pull and that morephonetically and linguistically based techniqueswill have to be used.
This prediction ispredicated, of course, on the need for a hugeamount of improved understanding of language inall of its various representations and I feel thatthere is need for an incredibly large amount ofnew data to be acquired before we can hope tomake substantial progress on these issues,Certainly an important contribution of computa-tional linguistics is the provision of instru-mental means to acquire data, In my view, thestudy of both speech synthesis and speechrecognition has been hampered over the years inlarge part due to the sheer lack of insufficientdata on which to base models and theories.
Whilewe would still like to have more computationalpower than we have, at present, we are able toprovide highly capable interactive researchenvironments for exploring new areas.
The factthat there is none too much of these computationalresources is supported by the fact that the speech105recognition group at IBM is, I believe, thelargest user of 370/168 time at Yorktown Heights.An interesting aspect of the study of speechrecognition is that there is still no agreementamong researchers as to the best approach.
Thus,we see techniques based on statistical decoding,those based on template matching using dynamicprogramming, and those that are much more phoneticand linguistic in nature.
I believe that thenotion, at one time prevalent during theseventies, that the speech waveform could often beignored in favor of constraints supplied bysyntax, semantics, or pragmatics is no longer heldand there is an increasing view that one shouldtry to extract as much information as possiblefrom the speech waveform.
Indeed, word boundaryeffects and manifestations at the phonetic levelof high level syntactic and semantic constraintsare being discovered continually as research inspeech production and perception continues.
Forall of our research into speech recognition, weare still a long ways away from approximatinghuman speech perception capability.
We reallyhave no idea as to how human listeners are able toadapt to a large variety of speakers and a largevariety of communication environments, we have noidea how humans manage to reject noise in thebackground, and very little understanding as tothe interplay of the various constraint domainsthat are active.
Within the last five years,however we have seen an increasing level ofcooperation between linguists, psycholinguistsand computational linguists on these matters andI believe that the depth of understanding inpsycholinguisties is now at a level where it canbe tentatively exploited by computationallinguists for models of speech perception.Over these twenty years, we have seencomputational linguistics grow from a relativelyesoteric academic discipline to a robustcon~ercial enterprise.
Certainly the need withinindustry for man-machlne interaction is verystrong and many computer companies are hiringcomputational linguists to provide for naturallanguage access to data bases, speech control ofinstruments, and audio announcements of all sorts.There is a need to get newly developed ideas intopractice, and as a result of that experience,provide feedback to the models that computationallinguists create.
There is a tension, I believe,between, on the one hand, the need to be farreaching in our research programs vs. the needfor short-term payoff in industrial practice.
Itis important that workers in the field seek toinfluence those that control resources to maintaina healthy balance between these two influences.For example, the relatively new interest instudying discourse structure is a difficult, butimportant area for long range research and itdeserves encouragement, despite the fact thatthere are large areas of ignorance and the needfor extended fundamental research.
One can hopehowever, that the demonstrated achi~vp~nt ofcomputational linguistics over the last twentyyears will provide a base upon which society willbe willing to continue to support us to furtherexplore the large unknowns in language competenceand behavior.106
