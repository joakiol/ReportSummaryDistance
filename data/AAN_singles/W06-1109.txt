Proceedings of the Workshop on Linguistic Distances, pages 63?72,Sydney, July 2006. c?2006 Association for Computational LinguisticsStudy of Some Distance Measures for Language and EncodingIdenticationAnil Kumar SinghLanguage Technologies Research CentreInternational Institute of Information TechnologyHyderabad, Indiaanil@research.iiit.netAbstractTo determine how close two languagemodels (e.g., n-grams models) are, wecan use several distance measures.
If wecan represent the models as distributions,then the similarity is basically the simi-larity of distributions.
And a number ofmeasures are based on information theo-retic approach.
In this paper we presentsome experiments on using such similar-ity measures for an old Natural LanguageProcessing (NLP) problem.
One of themeasures considered is perhaps a novelone, which we have called mutual crossentropy.
Other measures are either wellknown or based on well known measures,but the results obtained with them vis-a-vis one-another might help in gaining aninsight into how similarity measures workin practice.The first step in processing a text is toidentify the language and encoding of itscontents.
This is a practical problem sincefor many languages, there are no uni-versally followed text encoding standards.The method we have used in this paperfor language and encoding identificationuses pruned character n-grams, alone aswell augmented with word n-grams.
Thismethod seems to give results comparableto other methods.1 IntroductionMany kinds of models in NLP can be seen as dis-tributions of a variable.
For various NLP prob-lems, we need to calculate the similarity of suchmodels or distributions.
One common example ofthis is the n-grams model.
We might have sev-eral reference data sets and then we may want tofind out which of those matches most closely witha test data set.
The problem of language and en-coding identification can be represented in theseterms.
One of the most important questions thenis which similarity measure to use.
We can expectthat the performance obtained with the similaritymeasure will vary with the specific problem andthe kind of model used or some other problem spe-cific details.
Still, it will be useful to explore howthese measures relate to each other.The measures we are going to focus on in thispaper are all very simple ones and they all try tofind the similarity of two models or distributions ina (more or less) information theoretic way, exceptthe out of rank measure proposed by Cavnar andTrenkle (Cavnar and Trenkle, 1994).This work had started simply as an effort tobuild a language and encoding identification toolspecifically for South Asian languages.
During thecourse of this work, we experimented with varioussimilarity measures and some of the results we ob-tained were at least a bit surprising.
One of themeasures we used was something we have calledmutual cross entropy and its performance for thecurrent problem was better than other measures.Before the content of a Web page or of any kindof text can be processed for computation, its lan-guage and encoding has to be known.
In manycases this language-encoding is not known before-hand and has to be determined automatically.
Forlanguages like Hindi, there is no standard encod-ing followed by everyone.
There are many wellknown web sites using their own proprietary en-coding.
This is one of the biggest problems in ac-tually using the Web as a multilingual corpus andfor enabling a crawler to search the text in lan-63guages like Hindi.
This means that the content inthese languages, limited as it is, is invisible notjust to people (which could be just due to lack ofdisplay support or unavailability of fonts for a par-ticular encoding) but even to crawlers.The problem of language identification is sim-ilar to some other problems in different fieldsand the techniques used for one such problemhave been found to be effective for other prob-lems too.
Some of these problems are text cate-gorization (Cavnar and Trenkle, 1994), cryptanal-ysis (Beesley, 1988) and even species identifi-cation (Dunning, 1994) from genetic sequences.This means that if something works for one ofthese problems, it is likely to work for these otherproblems.It should be noted here that the identifica-tion problem here is that of identifying both lan-guage and encoding.
This is because (especiallyfor South Asian languages) the same encodingcan be used for more than one languages (ISCIIfor all Indian languages which use Brahmi-originscripts) and one language can have many encod-ings (ISCII, Unicode, ISFOC, typewriter, pho-netic, and many other proprietary encodings forHindi).In this paper we describe a method basedmainly on character n-grams for identifying thelanguage-encoding pair of a text.
The methodrequires some training text for each language-encoding, but this text need not have the same con-tent.
A few pages (2500-10000 words) of text in aparticular language-encoding is enough.
A prunedcharacter based n-grams model is created for eachlanguage-encoding.
A similar model is created forthe test data too and is compared to the trainingmodels.
The best match is found using a similar-ity measure.
A few (5-15) words of test data seemsto be enough for identification in most cases.The method has been evaluated using varioussimilarity measures and for different test sizes.
Wealso consider two cases, one in which the prunedcharacter n-grams model is used alone, and theother in which it is augmented with a word n-grammodel.2 Previous WorkLanguage identification was one of the first naturallanguage processing (NLP) problems for which astatistical approach was used.Ingle (Ingle, 1976) used a list of short wordsin various languages and matched the words in thetest data with this list.
Such methods based on listsof words or letters (unique strings) were meant forhuman translators and couldn?t be used directly forautomatic language identification.
They ignoredthe text encoding, since they assumed printed text.Even if adapted for automatic identification, theywere not very effective or scalable.However, the earliest approaches used for au-tomatic language identification were based on theabove idea and could be called ?translator ap-proaches?.
Newman (Newman, 1987), among oth-ers, used lists of letters, especially accented lettersfor various languages and identification was doneby matching the letters in the test data to theselists.Beesley?s (Beesley, 1988) automatic languageidentifier for online texts was based on mathemat-ical language models developed for breaking ci-phers.
These models basically had characteristicletter sequences and frequencies (?orthographicalfeatures?)
for each language, making them similarto n-grams models.
The insights on which they arebased, as Beesley points out, have been known atleast since the time of Ibn ad-Duraihim who livedin the 14th century.
Beesley?s method needed 6-64K of training data and 10-12 words of test data.
Ittreats language and encoding pair as one entity.Adams and Resnik (Adams and Resnik, 1997)describe a client-server system using Dunning?sn-grams based algorithm (Dunning, 1994) for avariety of tradeoffs available to NLP applicationslike between the labelling accuracy and the sizeand completeness of language models.
Their sys-tem dynamically adds language models.
The sys-tem uses other tools to identify the text encoding.They use 5-grams with add-k smoothing.
Trainingsize was 1-50 K and test size above 50 characters.Some pruning is done, like for frequencies up to 3.Some methods for language identification usetechniques similar to n-gram based text catego-rization (Cavnar and Trenkle, 1994) which calcu-lates and compares profiles of n-gram frequencies.This is the approach nearest to ours.
Such meth-ods differ in the way they calculate the likelihoodthat the test data matches with one of the profiles.Beesley?s method simply uses word-wise proba-bilities of ?digram?
sequences by multiplying theprobabilities of sequences in the test string.
Oth-ers use some distance measure between trainingand test profiles to find the best match.64Cavnar also mentions that top 300 or so n-gramsare almost always highly correlated with the lan-guage, while the lower ranked n-grams give morespecific indication about the text, namely the topic.The distance measure used by Cavnar is called?out-of-rank?
measure and it sums up the differ-ences in rankings of the n-grams found in the testdata as compared to the training data.
This isamong the measures we have tested.The language model used by Combrinck andBotha (Combrinck and Botha, 1994) is also basedon bigram or trigram frequencies (they call them?transition vectors?).
They select the most dis-tinctive transition vectors by using as measure theratio of the maximum percentage of occurrencesto the total percentage of occurrences of a transi-tion vector.
These distinctive vectors then form themodel.Dunning (Dunning, 1994) also used an n-gramsbased method where the model selected is the onewhich is most likely to have generated the teststring.
Giguet (Giguet, 1995b; Giguet, 1995a) re-lied upon grammatically correct words instead ofthe most common words.
He also used the knowl-edge about the alphabet and the word morphologyvia syllabation.
Giguet tried this method for tag-ging sentences in a document with the languagename, i.e., dealing with multilingual documents.Another method (Stephen, 1993) was based on?common words?
which are characteristic of eachlanguage.
This methods assumes unique wordsfor each language.
One major problem with thismethod was that the test string might not containany unique words.Cavnar?s method, combined with some heuris-tics, was used by Kikui (Kikui, 1996) to identifylanguages as well as encodings for a multilingualtext.
He relied on known mappings between lan-guages and encodings and treated East Asian lan-guages differently from West European languages.Kranig (Muthusamy et al, 1994) and (Simon,2005) have reviewed and evaluated some of thewell known language identification methods.
Mar-tins and Silva (Martins and Silva, 2005) describea method similar to Cavnar?s but which uses a dif-ferent similarity measure proposed by Jiang andConrath (Jiang and Conrath, 1997).
Some heuris-tics are also employed.Poutsma?s (Poutsma, 2001) method is based onMonte Carlo sampling of n-grams from the begin-ning of the document instead of building a com-plete model of the whole document.
Sibun andReynar (Sibun and Reynar, 1996) use mutual in-formation statistics or relative entropy, also calledKullback-Leibler distance for language identifica-tion.
Souter et al(Souter et al, 1994) comparedunique character string, common word and ?tri-graph?
based approaches and found the last to bethe best.Compression based approaches have also beenused for language identification.
One example ofsuch an approach is called Prediction by PartialMatching (PPM) proposed by Teahan (Teahan andHarper, 2001).
This approach uses cross entropyof the test data with a language model and predictsa character given the context.3 Pruned Character N-gramsLike in Cavnar?s method, we used pruned n-gramsmodels of the reference or training as well astest data.
For each language-encoding pair, sometraining data is provided.
A character based n-gram model is prepared from this data.
N-gramsof all orders are combined and ranked accordingto frequency.
A certain number of them (say 1000)with highest frequencies are retained and the restare dropped.
This gives us the pruned charac-ter n-grams model, which is used for language-encoding identification.As an attempt to increase the performance, wealso tried to augment the pruned character n-gramsmodel with a word n-gram model.4 Distance MeasuresSome of the measures we have experimented withhave already been mentioned in the section on pre-vious work.
The measures considered in this workrange from something as simple as log probabil-ity difference to the one based on Jiang and Con-rath (Jiang and Conrath, 1997) measure.Assuming that we have two models or distribu-tions P and Q over a variable X, the measures (sim)are defined as below (p and q being probabilitiesand r and s being ranks in models P and Q:1.
Log probability difference:sim =?x(log p(x) ?
log q(x)) (1)2.
Absolute log probability difference:sim =?x(abs(log p(x)) ?
abs(log q(x)))(2)653.
Cross entropy:sim =?x(p(x) ?
log q(x)) (3)4.
RE measure (based on relative entropy orKullback-Leibler distance ?
see note below):sim =?xp(x) log p(x)log q(x) (4)5.
JC measure (based on Jiang and Conrath?smeasure) (Jiang and Conrath, 1997):sim = A ?
B (5)where,A = 2 ?
?x(log p(x) + log q(x)) (6)and,B =?xlog p(x) +?xlog q(x) (7)6.
Out of rank measure (Cavnar and Trenkle,1994):sim =?xabs(r(x) ?
s(x)) (8)7.
MRE measure (based on mutual or symmet-ric relative entropy, the original definition ofKL-distance given by Kullback and Leibler):sim =?xp(x) log p(x)log q(x)+?xq(x) log q(x)log p(x)(9)8.
Mutual (or symmetric) cross entropy:sim =?x(p(x)?log q(x)+q(x)?log p(x))(10)As can be noticed, all these measures, in a way,seem to be information theoretic in nature.
How-ever, our focus in this work is more on the pre-senting empirical evidence rather than discussingmathematical foundation of these measures.
Thelatter will of course be interesting to look into.NOTE:We had initiallly experimented with relative en-tropy or KL-distance as defined below (instead ofthe RE measure mentioned above):sim =?xp(x) log p(x)q(x) (11)Another measure we tried was DL measure(based on Dekang Lin?s measure, on which the JCmeasure is based):sim = AB (12)where A and B are as given above.The results for the latter measure were not verygood (below 50% in all cases) and the RE mea-sure defined above performed better than relativeentropy.
These results have not been reported inthis paper.5 Mutual Cross EntropyCross entropy is a well known distance measureused for various problems.
Mutual cross entropycan be seen as bidirectional or symmetric cross en-tropy.
It is defined simply as the sum of the crossentropies of two distributions with each other.Our motivation for using ?mutual?
cross entropywas that many similarity measures like cross en-tropy and relative entropy measure how similarone distribution is to the other.
This will not neces-sary mean the same thing as measuring how sim-ilar two distributions are to each other.
Mutualinformation measures this bidirectional similarity,but it needs joint probabilities, which means thatit can only be applied to measure similarity ofterms within one distribution.
Relative entropy orKullback-Leibler measure is applicable, but as theresults show, it doesn?t work as well as expected.Note that some authors treat relative entropyand mutual information interchangeably.
They arevery similar in nature except that one is applicablefor one variable in two distributions and the otherfor two variables in one distribution.Our guess was that symmetric measures maygive better results as both the models give some in-formation about each other.
This seems to be sup-ported by the results for cross entropy, but (asym-metric) cross entropy and RE measures also gavegood results.6 The AlgorithmThe foundation of the algorithm for identifying thelanguage and encoding of a text or string has al-ready been explained earlier.
Here we give a sum-mary of the algorithm we have used.
The parame-ters for the algorithm and their values used in ourexperiments reported here have also been listed.These parameters allow the algorithm to be tuned66Table 1: DESCRIPTION OF DATA SETSNames Total CountLanguages Afrikaans (1), Assamese (1), Bengali (2), Bulgarian (1), Catalan (1)Czech (1), Danish (1), Dutch (1), English (1), Esperanto (1)Finnish (1), French (1), German (1), Gujarati (2), Hindi (8)Icelandic (1), Iloko (1), Iroquoian (1), Italian (1), Kannada (1)Khasi (1), Latin (1), Malayalam (1), Marathi (5), Modern Greek (1)Nahuatl (1), Norwegian (1), Oriya (2), Polish (1), Portugues (1)Punjabi (1), Romanian (1), Russian (1), Serbian (1), Spanish (1)Tagalog (1), Tamil (1), Telugu (1), Welsh (1) 39Encodings UTF8 (7), ISO-8859-1 (16), ISO-8859-2 (1), US-ASCII (4)Windows-1251 (2), Windows-1250 (1), ISCII (10), ISFOCB (1)ITrans (1), Shusha (1), Typewriter (1), WX (1), Gopika (1)Govinda (1), Manjusha (1), Saamanaa (1), Subak (1)Akruti Sarala (1), Webdunia (1) 19Counts in parenthesis represent the extra ambiguity for that language or encoding.For example, Hindi (8) means that 8 different encodings were tested for Hindi.Language-Encoding Pairs: 53Minimum training data size: 16035 characters (2495 words)Maximum training data size: 650292 characters (102377 words)Average training data size: 166198 characters (22643 words)Confusable Languages: Assamese/Bengali/Oriya, Dutch/Afrikaans, Norwegian/Danish,Spanish/Tagalog, Hindi/Marathi, Telugu/Kannada/Malayalam, Latin/FranchTable 2: NUMBER OF TEST SETSSize Number100 22083200 10819500 40911000 18672000 1524All test data 840or customized for best performance.
Perhaps theycan even be learned by using some approach as theEM algorithm.1.
Train the system by preparing characterbased and word based (optional) n-gramsfrom the training data.2.
Combine n-grams of all orders (Oc for char-acters and Ow for words).3.
Sort them by rank.4.
Prune by selecting only the top Nc charac-ter n-grams and Nw word n-grams for eachlanguage-encoding pair.5.
For the given test data or string, calculatethe character n-gram based score simc withevery model for which the system has beentrained.6.
Select the t most likely language-encodingpairs (training models) based on this charac-ter based n-gram score.7.
For each of the t best training models, calcu-late the score with the test model.
The scoreis calculated as:score = simc + a ?
simw (13)where c and w represent character based andword based n-grams, respectively.
And a isthe weight given to the word based n-grams.In our experiment, this weight was 1 for thecase when word n-grams were consideredand 0 when they were not.8.
Select the most likely language-encoding pairout of the t ambiguous pairs, based on thecombined score obtained from word andcharacter based models.67Table 3: PRECISION FOR VARIOUS MEASURES AND TEST SIZESPrecisionTest Size (characters) LPD ALPD CE RE CT JC MRE MCE100 CN 91.00 90.69 96.13 98.51 78.92 97.71 98.26 97.64CWN 94.31 94.15 97.50 75.54 81.63 98.35 94.16 98.38200 CN 94.46 94.37 97.72 99.35 91.24 99.05 99.24 99.05CWN 96.52 96.52 98.85 90.54 92.79 99.21 91.13 99.39500 CN 96.24 96.24 98.39 99.68 96.41 99.58 99.63 99.63CWN 98.19 97.80 99.46 94.65 96.82 99.63 98.78 99.851000 CN 97.18 96.81 98.81 99.78 97.73 99.89 99.73 99.95CWN 98.21 98.21 99.68 96.64 98.05 99.89 99.40 100.002000 CN 95.01 94.21 98.20 99.40 95.21 99.33 99.20 99.47CWN 96.74 97.14 99.47 94.01 95.81 99.40 96.67 99.60All available CN 82.50 88.57 98.33 99.88 94.76 99.88 99.76 100.00test data CWN 89.88 94.64 99.88 94.76 96.55 99.88 97.86 100.00CN: Character n-grams only, CWN: Character n-grams plus word n-gramsTo summarize, the parameters in the abovemethod are:1.
Character based n-gram models Pc and Qc2.
Word based n-gram models Pw and Qw3.
Orders Oc and Ow of n-grams models4.
Number of retained top n-grams Nc and Nw(pruning ranks for character based and wordbased n-grams, respectively)5.
Number t of character based models to bedisambiguated by word based models6.
Weight a of word based modelsParameters 3 to 6 can be used to tune the per-formace of the identification system.
The resultsreported in this paper used the following values ofthese parameters:1.
Oc = 42.
Ow = 33.
Nc = 10004.
Nw = 5005. t = 56. a = 1There is, of course, the type of similarity score,which can also be used to tune the performance.Since MCE gave the best overall performance inour experiments, we have selected it as the defaultscore type.7 ImplementationThe language and encoding tool has been imple-mented as a small API in Java.
This API uses an-other API to prepare pruned character and wordn-grams which was developed as part of anotherproject.
A graphical user interface (GUI) has alsobeen implemented for identifying the languagesand encodings of texts, files, or batches of files.The GUI also allows a user to easily train the toolfor a new language-encoding pair.
The tool will bemodified to work in client-server mode for docu-ments from the Internet.From implementation point of view, there aresome issues which can significantly affect the per-formance of the system:1.
Whether the data should be read as text or asa binary file.2.
The assumed encoding used for reading thetext, both for training and testing.
For ex-ample, if we read UTF8 data as ISO-8859-1,there will be errors.3.
Whether the tranining models should be readevery time they are needed or be kept inmemory.4.
If training models are stored (even if they areonly read at the beginning and then kept inmemory), as will have to be done for practicalapplications, how should they be stored: astext or in binary files?68To take care of these issues, we adopted the fol-lowing policy:1.
For preparing character based models, weread the data as binary files and the charac-ters are read as bytes and stored as numbers.For word based models, the data is read astext and the encoding is assumed to be UTF8.This can cause errors, but it seems to be thebest (easy) option as we don?t know the ac-tual encoding.
A slightly more difficult op-tion to implement would be to use charac-ter based models to guess the encoding andthen build word based models using that asthe assumed encoding.
The problem with thismethod will be that no programming environ-ment supports all possible encodings.
Notethat since we are reading the text as bytesrather than characters for preparing ?charac-ter based n-grams?, technically we should saythat we are using byte based n-grams mod-els, but since we have not tested on multi-byteencodings, a byte in our experiments was al-most always a character, except when the en-coding was UTF8 and the byte representedsome meta-data like the script code.
So, forpractical purposes, we can say that we are us-ing character based n-grams.2.
Since after pruning, the size of the models(character as well as word) is of the order of50K, we can afford to keep the training mod-els in memory rather than reading them everytime we have to identify the language and en-coding of some data.
This option is naturallyfaster.
However, for some applications wherelanguage and encoding identification is to bedone rarely or where there is a memory con-straint, the other option can be used.3.
It seems to be better to store the training mod-els in binary format since we don?t know theactual encoding and the assumed encodingfor storing may be wrong.
We tried bothoptions and the results were worse when westored the models as text.Our identification tool provides customizabilitywith respect to all the parameters mentioned in thisand the previous section.8 EvaluationEvaluation was performed for all the measureslisted earlier.
These are repeated here with a codefor easy reference in table-3.?
LPD: Log probability difference?
ALPD: Absolute log probability difference?
CE: Cross entropy?
RE: RE measure based on relative entropy?
JC: JC measure (based on Jiang and Con-rath?s measure)?
CT: Cavnar and Trenkle?s out of rank mea-sure?
MRE: MRE measure based on mutual (sym-metric) relative entropy?
MCE: Mutual (symmetric) cross entropyWe tested on six different sizes in terms of char-acters, namely 100, 200, 500, 1000, 2000, and allthe available test data (which was not equal forvarious language-encoding pairs).
The number oflanguage-encoding pairs was 53 and the minimumnumber of test data sets was 840 when we usedall available test data.
In other cases, the numberwas naturally larger as the test files were split infragments (see table-2).The languages considered ranged from Es-peranto and Modern Greek to Hindi and Telugu.For Indian languages, especially Hindi, several en-codings were tested.
Some of the pairs had UTF8as the encoding, but the information from UTF8byte format was not explicitly used for identifi-cation.
The number of languages tested was 39and number encodings was 19.
Total number oflanguage-encoding pairs was 53 (see table-1).The test and training data for about half ofthe pairs was collected from web pages (such asGutenberg).
For Indian languages, most (but notall) data was from what is known as the CIIL cor-pus.We didn?t test on various training data sizes.The size of the training data ranged from 2495 to102377 words, with more on the lower side thanon the higher.Note that we have considered the case whereboth the language and the encoding are unknown,not where one of them is known.
In the latter case,the performance can only improve.
Another pointworth mentioning is that the training data was notvery clean, i.e., it had noise (such as words or sen-tences from other languages).
Error details havebeen given in table-4.69Table 4: ERROR DETAILSLanguage-Encoding Identified AsAfrikaans::ISO-8859-1 Dutch::ISO-8859-1 (9)Assamese::ISCII Bengali::ISCII (6), Oriya::ISCII (113)Bengali::ISCII Hindi::ISCII (2), Oriya::ISCII (193)Bulgarian::Windows-1251 Marathi::ISCII (6)Catalan::ISO-8859-1 Latin::ISO-8859-1 (4)Danish::ISO-8859-1 Norwegian::ISO-8859-1 (7)Dutch::ISO-8859-1 Afrikaans::ISO-8859-1 (4)English::ASCII Icelandic::UTF8 (36)Esperanto::UTF8 Danish::ISO-8859-1 (5), Italian::ISO-8859-1 (1)French::ISO-8859-1 Catalan::ISO-8859-1 (6)German::ISO-8859-1 Dutch::ISO-8859-1 (4), Latin::ISO-8859-1 (3)Hindi::ISCII English::ASCII (14), Marathi::ISCII (20)Hindi::Isfocb Dutch::ISO-8859-1 (4), English::ASCII (6)Hindi::Phonetic-Shusha English::ASCII (14)Hindi::Typewriter English::ASCII (12)Hindi::UTF8 Marathi::UTF8 (82)Hindi::WX English::ASCII (8)Hindi::Webdunia French::ISO-8859-1 (2), Gujarati::Gopika (9)Icelandic::UTF8 Dutch::ISO-8859-1 (3), Latin::ISO-8859-1 (2)Iloko::ISO-8859-1 Tagalog::ISO-8859-1 (18)Iroquoian::ISO-8859-1 French::ISO-8859-1 (7)Italian::ISO-8859-1 Catalan::ISO-8859-1 (2)Kannada::ISCII Malayalam::ISCII (9)Latin::ISO-8859-1 Catalan::ISO-8859-1 (3), Dutch::ISO-8859-1 (85)French::ISO-8859-1 (28)Malayalam::ISCII Tamil::ISCII (3)Marathi::ISCII Hindi::ISCII (13)Marathi::Manjusha English::ASCII (1)Marathi::UTF8 Hindi::UTF8 (30)Nahuatl::ISO-8859-1 English::ASCII (2)Norwegian::ISO-8859-1 Danish::ISO-8859-1 (69)Oriya::ISCII Assamese::ISCII (5), Bengali::ISCII (70), Hindi::ISCII (7)Portugues::ISO-8859-1 Catalan::ISO-8859-1 (4)Punjabi::ISCII Assamese::ISCII (2), Hindi::ISCII (1)Romanian::US-ASCII Italian::ISO-8859-1 (2)Russian::Windows-1251 Portugues::ISO-8859-1 (12)Spanish::ISO-8859-1 Portugues::ISO-8859-1 (2), Tagalog::ISO-8859-1 (44)Tagalog::ISO-8859-1 English::ASCII (37), Khasi::US-ASCII (15)Telugu::ISCII Hindi::ISCII (15), Kannada::ISCII (21), Malayalam::ISCII (2)These error were for MCE, both with and without word models forall the test data sizes from 200 to all available data.
Most of theerrors were for smaller sizes, i.e., 100 and 200 characters.709 ResultsThe results are presented in table-3.
As can beseen almost the measures gave at least moderatelygood results.
The best results on the whole wereobtained with mutual cross entropy.
The JC mea-sure gave almost equally good results.
Even a sim-ple measure like log probability difference gavesurprisingly good results.It can also be observed from table-3 that the sizeof the test data is an important factor in perfor-mance.
More test data gives better results.
But thisdoes not always happen, which too is surprising.It means some other factors also come into play.One of these factors seem to whether the train-ing data for different models is of equal size ornot.
Another factor seems to be noise in the data.This seems to affect some measures more than theothers.
For example, LPD gave the worst perfor-mance when all the available test data was used.For smaller data sets, noise is likely to get isolatedin some data sets, and therefore is less likely toaffect the results.Using word n-grams to augment character n-grams improved the performance in most of thecases, but for measures like JC, RE, MRE andMCE, there wasn?t much scope for improvement.In fact, for smaller sizes (100 and 200 charac-ters), word models actually reduced the perfor-mance for these better measures.
This means ei-ther that word models are not very good for bettermeasures, or we have not used them in the bestpossible way, even though intuitively they seem tooffer scope for improvement when character basedmodels don?t perform perfectly.10 Issues and EnhancementsAlthough the method works very well even on lit-tle test and training data, there are still some is-sues and possible enhancements.
One major issueis that Web pages quite often contain text in morethan one language-encoding.
An ideal language-encoding identification tool should be able to markwhich parts of the page are in which language-encoding.Another possible enhancement is that in thecase of Web pages, we can also take into accountthe language and encoding specified in the Webpage (HTML).
Although it may not be correct fornon-standard encodings, it might still be useful fordifferentiating between very close encodings likeASCII and ISO-8859-1 which might seem identi-cal to our tool.If the text happens to be in Unicode, then itmight be possible to identify at least the encod-ing (the same encoding might be used for morethan one languages, e.g., Devanagari for Hindi,Sanskrit and Marathi) without using a statisticalmethod.
This might be used for validating the re-sult from the statistical method.Since every method, even the best one, hassome limitations, it is obvious that for practicalapplications we will have to combine several ap-proaches in such a way that as much of the avail-able information is used as possible and the var-ious approaches complement each other.
What isleft out by one approach should be taken care of bysome other approach.
There will be some issuesin combining various approaches like the order inwhich they have to used, their respective prioritiesand their interaction (one doesn?t nullify the gainsfrom another).It will be interesting to apply the same methodor its variations on text categorization or topicidentification and other related problems.
The dis-tance measures can also be tried for other prob-lems.11 ConclusionWe have presented the results about some dis-tance measures which can be applied to NLP prob-lems.
We also described a method for automati-cally identifying the language and encoding of atext using several measures including one called?mutual cross entropy?.
All these measures are ap-plied on character based pruned n-grams modelscreated from the training and the test data.
Thereis one such model for each of the known language-encoding pairs.
The character based models maybe augmented with word based models, which in-creases the performance for not so good measures,but doesn?t seem to have much effect for bettermeasures.
Our method gives good performance ona few words of test data and a few pages of trainingdata for each language-encoding pair.
Out of themeasures considered, mutual cross entropy gavethe best results, but RE, MRE and JC measuresalso performed almost equally well.12 AcknowledgementThe author wishes to thank Preeti Pradhan, Nan-dini Upasani and Anita Chaturvedi of Language71Technologies Research Centre, International Insti-tute of Information Technology, Hyderabad, Indiafor helping in preparing the data for some of thelanguage-encoding pairs.
The comments of re-viewers also helped in improving the paper.ReferencesGary Adams and Philip Resnik.
1997.
A languageidentification application built on the Java client-server platform.
In Jill Burstein and Claudia Lea-cock, editors, From Research to Commercial Appli-cations: Making NLP Work in Practice, pages 43?47.
Association for Computational Linguistics.K.
Beesley.
1988.
Language identifier: A computerprogram for automatic natural-language identifica-tion on on-line text.William B. Cavnar and John M. Trenkle.
1994.
N-gram-based text categorization.
In Proceedings ofSDAIR-94, 3rd Annual Symposium on DocumentAnalysis and Information Retrieval, pages 161?175,Las Vegas, US.H.
Combrinck and E. Botha.
1994.
Automatic lan-guage identification: Performance vs. complexity.In Proceedings of the Sixth Annual South AfricaWorkshop on Pattern Recognition.Ted Dunning.
1994.
Statistical identification of lan-guage.
Technical Report CRL MCCS-94-273, Com-puting Research Lab, New Mexico State University,March.E.
Giguet.
1995a.
Categorization according to lan-guage: A step toward combining linguistic knowl-edge and statistic learning.Emmanuel Giguet.
1995b.
Multilingual sentence cate-gorisation according to language.
In Proceedings ofthe European Chapter of the Association for Compu-tational Linguistics, SIGDAT Workshop, From Textto Tags: Issues in Multilingual Language Analysis,Dublin, Ireland.Norman C. Ingle.
1976.
A language identification ta-ble.
In The Incorporated Linguist, 15(4).Jay J. Jiang and David W. Conrath.
1997.
Semanticsimilarity based on corpus statistics and lexical tax-onomy.G.
Kikui.
1996.
Identifying the coding system andlanguage of on-line documents on the internet.
InCOLING, pages 652?657.Bruno Martins and Mario J. Silva.
2005.
Languageidentification in web pages.
In Proceedings of ACM-SAC-DE, the Document Engeneering Track of the20th ACM Symposium on Applied Computing.Y.
K. Muthusamy, E. Barnard, and R. A. Cole.
1994.Reviewing automatic language identification.
InIEEE Signal Processing Magazine.Patricia Newman.
1987.
Foreign language identifica-tion - first step in the translation process.
In Pro-ceedings of the 28th Annual Conference of the Amer-ican Translators Association., pages 509?516.Arjen Poutsma.
2001.
Applying monte carlo tech-niques to language identification.
In Proceedings ofCLIN.P.
Sibun and J. C. Reynar.
1996.
Language identifi-cation: Examining the issues.
In In Proceedings ofSDAIR-96, the 5th Symposium on Document Analy-sis and Information Retrieval., pages 125?135.Kranig Simon.
2005.
Evaluation of language identifi-cation methods.
In BA Thesis.
Universitt Tbingens.C.
Souter, G. Churcher, J. Hayes, J. Hughes, andS.
Johnson.
1994.
Natural language identificationusing corpus-based models.
In Hermes Journal ofLinguistics., pages 183?203.Johnson Stephen.
1993.
Solving the problem of lan-guage recognition.
In Technical Report.
School ofComputer Studies, University of Leeds.W.
J. Teahan and D. J. Harper.
2001.
Using compres-sion based language models for text categorization.In J. Callan, B. Croft and J. Lafferty (eds.
), Work-shop on Language Modeling and Information Re-trieval., pages 83?88.
ARDA, Carnegie Mellon Uni-versity.72
