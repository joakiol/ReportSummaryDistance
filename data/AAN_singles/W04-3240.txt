Learning to Classify Email into "Speech Acts"William W. Cohen1 Vitor R. Carvalho2 Tom M. Mitchell1,21Center for Automated Learning & DiscoveryCarnegie Mellon UniversityPittsburgh, PA 152132Language Technology InstituteCarnegie Mellon UniversityPittsburgh, PA 15213AbstractIt is often useful to classify email accord-ing to the intent of the sender (e.g., "pro-pose a meeting", "deliver information").We present experimental results in learn-ing to classify email in this fashion,where each class corresponds to a verb-noun pair taken from a predefined ontol-ogy describing typical ?email speechacts?.
We demonstrate that, althoughthis categorization problem is quite dif-ferent from ?topical?
text classification,certain categories of messages can none-theless be detected with high precision(above 80%) and reasonable recall (above50%) using existing text-classificationlearning methods.
This result suggeststhat useful task-tracking tools could beconstructed based on automatic classifi-cation into this taxonomy.1 IntroductionIn this paper we discuss using machine learn-ing methods to classify email according to theintent of the sender.
In particular, we classifyemails according to an ontology of verbs (e.g.,propose, commit, deliver) and nouns (e.g., infor-mation, meeting, task), which jointly describe the?email speech act?
intended by the email sender.A method for accurate classification of emailinto such categories would have many potentialbenefits.
For instance, it could be used to help anemail user track the status of ongoing joint activi-ties.
Delegation and coordination of joint tasks isa time-consuming and error-prone activity, andthe cost of errors is high: it is not uncommon thatcommitments are forgotten, deadlines are missed,and opportunities are wasted because of a failureto properly track, delegate, and prioritize sub-tasks.
The classification methods we considermethods which could be used to partially auto-mate this sort of activity tracking.
A hypotheticalexample of an email assistant that works alongthese lines is shown in Figure 1.Bill,Do you have any samplescheduling-related email wecould use as data?
-SteveAssistant announces:  ?newemail request, priorityunknown.
?Sure, I?ll put some togethershortly.
-BillAssistant:  ?should I add thisnew commitment to your to-do list?
?Fred, can you collect the msgsfrom the CSPACE corporatagged w/ the  ?meeting?noun, ASAP?
-BillAssistant:  notices outgoingrequest, may take action if noanswer is received promptly.Yes, I can get to that in thenext few days.
Is nextMonday ok?
-FredAssistant:  notices incomingcommitment.
?Should I sendFred a reminder on Monday?
?Figure 1 - Dialog with a hypothetical email assistantthat automatically detects email speech acts.
Dashedboxes indicate outgoing messages.
(Messages havebeen edited for space and anonymity.
)2 Related WorkOur research builds on earlier work defining il-locutionary points of speech acts (Searle, 1975),and relating such speech acts to email and work-flow tracking (Winograd, 1987, Flores & Lud-low, 1980, Weigant et al 2003).
Winogradsuggested that research explicating the speech-actbased ?language-action perspective?
on humancommunication could be used to build more use-ful tools for coordinating joint activities.
TheCoordinator (Winograd, 1987) was one such sys-tem, in which users augmented email messageswith additional annotations indicating intent.While such systems have been useful in lim-ited contexts, they have also been criticized ascumbersome: by forcing users to conform to aparticular formal system, they constrain commu-nication and make it less natural (Schoop, 2001);in short, users often prefer unstructured emailinteractions (Camino et al 1998).
We note thatthese difficulties are avoided if messages can beautomatically annotated by intent, rather thansoliciting a statement of intent from the user.Murakoshi et al (1999) proposed an email an-notation scheme broadly similar to ours, called a?deliberation tree?, and an algorithm for con-structing deliberation trees automatically, buttheir approach was not quantitatively evaluated.The approach is based on recognizing a set ofhand-coded linguistic ?clues?.
A limitation oftheir approach is that these hand-coded linguistic?clues?
are language-specific (and in fact limitedto Japanese text.
)Prior research on machine learning for textclassification has primarily considered classifica-tion of documents by topic (Lewis, 1992; Yang,1999), but also has addressed sentiment detection(Pang et al, 2002;  Weibe et al, 2001) and au-thorship attribution (e.g., Argamon et al 2003).There has been some previous use of machinelearning to classify email messages (Cohen 1996;Sahami et al, 1998; Rennie, 2000; Segal &Kephart, 2000).
However, to our knowledge,none of these systems has investigated learningmethods for assigning email speech acts.
Instead,email is generally classified into folders (i.e., ac-cording to topic) or according to whether or not itis ?spam?.
Learning systems have been previ-ously used to automatically detect acts inconversational speech (e.g.
Finke et al, 1998).3 An Ontology of Email ActsOur ontology of nouns and verbs covering someof the possible speech acts associated with emailsis summarized in Figure 2.
We assume that asingle email message may contain multiple acts,and that each act is described by a verb-noun pairdrawn from this ontology (e.g., "deliver data").The underlined nodes in the figure indicate thenouns and verbs for which we have trained clas-sifiers (as discussed in subsequent sections).To define the noun and verb ontology ofFigure 2, we first examined email from severalcorpora (including our own inboxes) to find regu-larities, and then performed a more detailedanalysis of one corpus.
The ontology was furtherrefined in the process of labeling the corpora de-scribed below.In refining this ontology, we adopted severalprinciples.
First, we believe that it is more impor-tant for the ontology to reflect observed linguisticbehavior than to reflect any abstract view of thespace of possible speech acts.
As a consequence,the taxonomy of verbs contains concepts that areatomic linguistically, but combine several illocu-tionary points.
(For example, the linguistic unit"let's do lunch" is both directive, as it requests thereceiver, and commissive, as it implicitly com-mits the sender.
In our taxonomy this is a single'propose' act.)
Also, acts which are abstractlypossible but not observed in our data are not rep-resented (for instance, declarations).NounActivity InformationMeetingLogisticsDataOpinion OngoingActivityData SingleEventMeeting OtherShort TermTaskOtherData Committee<Verb><Noun>VerbRemindProposeDeliverCommitRequestAmendRefuseGreetOther NegotiateInitiate ConcludeFigure 2 ?
TaxonomySecond, we believe that the taxonomy must re-flect common non-linguistic uses of email, suchas the use of email as a mechanism to deliverfiles.
We have grouped this with the linguisticallysimilar speech act of delivering information.The verbs in Figure 1 are defined as follows.A request asks (or orders) the recipient to per-form some activity.
A question is also considereda request (for delivery of information).A propose message proposes a joint activity,i.e., asks the recipient to perform some activityand commits the sender as well, provided the re-cipient agrees to the request.
A typical exampleis an email suggesting a joint meeting.An amend message amends an earlier proposal.Like a proposal, the message involves both acommitment and a request.
However, while aproposal is associated with a new task, anamendment is a suggested modification of analready-proposed task.A commit message commits the sender tosome future course of action, or confirms thesenders' intent to comply with some previouslydescribed course of action.A deliver message delivers something, e.g.,some information, a PowerPoint presentation,the URL of a website, the answer to a question, amessage sent "FYI?, or an opinion.The refuse, greet, and remind verbs occurredvery infrequently in our data, and hence we didnot attempt to learn classifiers for them (in thisinitial study).
The primary reason for restrictingourselves in this way was our expectation thathuman annotators would be slower and less reli-able if given a more complex taxonomy.The nouns in Figure 2 constitute possible ob-jects for the email speech act verbs.
The nounsfall into two broad categories.Information nouns are associated with emailspeech acts described by the verbs Deliver, Re-mind and Amend, in which the email explicitlycontains information.
We also associate informa-tion nouns with the verb Request, where theemail contains instead a description of the neededinformation (e.g., "Please send your birthdate.
"versus "My birthdate is ?".
The request act isactually for a 'deliver information' activity).
In-formation includes data believed to be fact aswell as opinions, and also attached data files.Activity nouns are generally associated withemail speech acts described by the verbs Pro-pose, Request, Commit, and Refuse.
Activitiesinclude meetings, as well as longer term activitiessuch as committee memberships.Notice every email speech act is itself an ac-tivity.
The <verb><noun> node in Figure 1 indi-cates that any email speech act can also serve asthe noun associated with some other emailspeech act.
For example, just as (deliver infor-mation) is a legitimate speech act, so is (commit(deliver information)).
Automatically construct-ing such nested speech acts is an interesting anddifficult topic; however, in the current paper weconsider only the problem of determining top-level the verb for such compositional speech acts.For instance, for a message containing a (commit(deliver information)) our goal would be toautomatically detect the commit verb but not theinner (deliver information) compound noun.4 Categorization Results4.1 CorporaAlthough email is ubiquitous, large and realis-tic email corpora are rarely available for researchpurposes.
The limited availability is largely dueto privacy issues: for instance, in most US aca-demic institutions, a users?
email can only be dis-tributed to researchers if all senders of the emailalso provided explicit written consent.The email corpora used in our experimentsconsist of four different email datasets collectedfrom working groups who signed agreements tomake their email accessible to researchers.
Thefirst three datasets, N01F3, N02F2, and N03F2are annotated subsets of a larger corpus, theCSpace email corpus, which contains approxi-mately 15,000 email messages collected from amanagement course at Carnegie Mellon Univer-sity.
In this course, 277 MBA students, organizedin approximately 50 teams of four to six mem-bers, ran simulated companies in different marketscenarios over a 14-week period (Kraut et al).N02F2, N01F3 and N03F2 are collections of allemail messages written by participants from threedifferent teams, and contain 351, 341 and 443different email messages respectively.The fourth dataset, the PW CALO corpus, wasgenerated during a four-day exercise conductedat SRI specifically to generate an email corpus.During this time a group of six people assumeddifferent work roles (e.g.
project leader, financemanager, researcher, administrative assistant, etc)and performed a number of group activities.There are 222 email messages in this corpus.These email corpora are all task-related, andassociated with a small working group, so it isnot surprising that they contain many instances ofthe email acts described above?for instance, theCSpace corpora contain an average of about 1.3email verbs per message.
Informal analysis ofother personal inboxes suggests that this sort ofemail is common for many university users.
Webelieve that negotiation of shared tasks is a cen-tral use of email in many work environments.All messages were preprocessed by removingquoted material, attachments, and non-subjectheader information.
This preprocessing was per-formed manually, but was limited to operationswhich can be reliably automated.
The most diffi-cult step is removal of quoted material, which weaddress elsewhere (Carvalho & Cohen, 2004).4.2 Inter-Annotator AgreementEach message may be annotated with severallabels, as it may contain several speech acts.
Toevaluate inter-annotator agreement, we double-labeled N03F2 for the verbs Deliver, Commit,Request, Amend, and Propose, and the noun,Meeting, and computed the kappa statistic (Car-letta, 1996) for each of these, defined asRRA?
?=1?where A is the empirical probability of agreementon a category, and R is the probability of agree-ment for two annotators that label documents atrandom (with the empirically observed frequencyof each label).
Hence kappa ranges from -1 to +1.The results in Table 1 show that agreement isgood, but not perfect.Email Act KappaMeeting 0.82Deliver 0.75Commit 0.72Request 0.81Amend 0.83Propose 0.72Table 1 - Inter-Annotator Agreement on N03F2.We also took doubly-annotated messageswhich had only a single verb label and con-structed the 5-class confusion matrix for the twoannotators shown in Table 2.
Note kappa valuesare somewhat higher for the shorter one-act mes-sages.Req Prop Amd Cmt Dlv kappaReq 55 0 0 0 0 0.97Prop 1 11 0 0 1 0.77Amd 0 1 15 0 0 0.87Cmt 1 3 1 24 4 0.78Dlv 1 0 2 3 135 0.91Table 2 - Inter-annotator agreement on documentswith only one category.4.3 Learnability of CategoriesRepresentation of documents.
To assess thetypes of message features that are most importantfor prediction, we adopted Support Vector Ma-chines (Joachims, 2001) as our baseline learningmethod, and a TFIDF-weighted bag-of-words asa baseline representation for messages.
We thenconducted a series of experiments with theN03F2 corpus only to explore the effect of dif-ferent representations.NF032 Cmt Dlv DirectiveBaseline SVM 25.0 49.8 75.2no tfidf  47.3 58.4 74.6+bigrams 46.1 66.1 76.0+times 43.6 60.1 73.2+POSTags 48.6 61.8 75.4+personPhrases 41.2 61.1 73.4NF02F2 and NF01F3 Cmt Dlv DirectiveBaseline SVM 10.1 56.3 66.1All ?useful?
features 42.0 64.0 73.3Table 3 ?
F1 for different feature sets.We noted that the most discriminating wordsfor most of these categories were common words,not the low-to-intermediate frequency words thatare most discriminative in topical classification.This suggested that the TFIDF weighting wasinappropriate, but that a bigram representationmight be more informative.
Experiments showedthat adding bigrams to an unweighted bag ofwords representation slightly improved perform-ance, especially on Deliver.
These results areshown in Table 4 on the rows marked ?no tfidf?and ?bigrams?.
(The TFIDF-weighted SVM isshown in the row marked ?baseline?, and the ma-jority classifier in the row marked ?default?
; allnumbers are F1 measures on 10-fold cross-validation.)
Examination of messages suggestedother possible improvements.
Since much nego-tiation involves timing, we ran a hand-coded ex-tractor for time and date expressions on the data,and extracted as features the number of time ex-pressions in a message, and the words that oc-curred near a time (for instance, one such featureis ?the word ?before?
appears near a time?
).These results appear in the row marked ?times?.Similarly, we ran a part of speech (POS) taggerand added features for words appearing near apronoun or proper noun (?personPhrases?
in thetable), and also added POS counts.To derive a final representation for each cate-gory, we pooled all features that improved per-formance over ?no tfidf?
for that category.
Wethen compared performance of these documentrepresentations to the original TFIDF bag ofwords baseline on the (unexamined) N02F2 andN01F3 corpora.
As Table 3 shows, substantialimprovement with respect to F1 and kappa wasobtained by adding these additional features overthe baseline representation.
This result contrastswith previous experiments with bigrams for topi-cal text classification (Scott & Matwin, 1999)and sentiment detection (Pang et al, 2002).
Thedifference is probably that in this task, more in-formative words are potentially ambiguous: forinstance, ?will you?
and ?I will?
are correlatedwith requests and commitments, respectively, butthe individual words in these bigrams are lesspredictive.Learning methods.
In another experiment,we fixed the document representation to be un-weighted word frequency counts and varied thelearning algorithm.
In these experiments, wepooled all the data from the four corpora, a totalof 9602 features in the 1357 messages, and sincethe nouns and verbs are not mutually exclusive,we formulated the task as a set of several binaryclassification problems, one for each verb.The following learners were used from theBased on the MinorThird toolkit (Cohen, 2004).VP is an implementation of the voted perceptronalgorithm (Freund & Schapire, 1999).
DT is asimple decision tree learning system, whichlearns trees of depth at most five, and choosessplits to maximize the function ( )00112?+?+ + WWWW  suggested by Schapire andSinger (1999) as an appropriate objective for?weak learners?.
AB is an implementation of theconfidence-rated boosting method described bySinger and Schapire (1999), used to boost the DTalgorithm 10 times.
SVM is a support vector ma-chine with a linear kernel (as used above).ActVP AB SVM  DTRequest(450/907)ErrorF10.250.580.220.650.230.640.200.69Proposal(140/1217)ErrorF10.110.190.120.260.120.440.100.13Delivery(873/484)ErrorF10.260.800.280.780.270.780.300.76Commit-ment(208/1149)ErrorF10.150.210.140.440.170.470.150.11Directive(605/752)ErrorF10.250.720.230.730.230.730.190.78Commis-sive(993/364)ErrorF10.230.840.230.840.240.830.220.85Meet(345/1012)ErrorF10.1870.5730.170.620.140.720.180.60Table 4 ?
Learning on the entire corpus.Table 4 reports the results on the most commonverbs, using 5-fold cross-validation to assess ac-curacy.
One surprise was that DT (which we hadintended merely as a base learner for AB) workssurprisingly well for several verbs, while AB sel-dom improves much over DT.
We conjecturethat the bias towards large-margin classifiers thatis followed by SVM, AB, and VP (and which hasbeen so successful in topic-oriented text classifi-cation) may be less appropriate for this task, per-haps because positive and negative classes arenot clearly separated (as suggested by substantialinter-annotator disagreement).Class:Commisive(Total: 1357 msgs)0.40.60.810 0.2 0.4 0.6 0.8 1RecallPrecision Voted PerceptronAdaBoostSVMDecision TreeFigure 3 - Precision/Recall for Commissive actFurther results are shown in Figure 3-5, whichprovide precision-recall curves for many of theseclasses.
The lowest recall level in these graphscorresponds to the precision of random guessing.These graphs indicate that high-precision predic-tions can be made for the top-level of the verbhierarchy, as well as verbs Request and Deliver,if one is willing to slightly reduce recall.Class:  Directive(Total: 1357 msgs)0.20.40.60.810 0.2 0.4 0.6 0.8 1RecallPrecision VotedPerceptronAdaBoostSVMDecisionTreeFigure 4 - Precision/Recall for Directive act00.20.40.60.810 0.2 0.4 0.6 0.8 1RecallPrecisionMeetDlvReqAdaBoost Learner(Total: 1357messages)Figure 5 - Precision/Recall of 3 different classesusing AdaBoostTransferability.
One important question in-volves the generality of these classifiers: to whatrange of corpora can they be accurately applied?Is it possible to train a single set of email-actclassifiers that work for many users, or is it nec-essary to train individual classifiers for eachuser?
To explore this issue we trained a DT clas-sifier for Directive emails on the NF01F3 corpus,and tested it on the NF02F2 corpus; trained thesame classifier on NF02F2 and tested it onNF01F3; and also performed a 5-fold cross-validation experiment within each corpus.
(NF02F2 and NF01F3 are for disjoint sets of us-ers, but are approximately the same size.)
Wethen performed the same experiment with VP forDeliver verbs and SVM for Commit verbs (ineach case picking the top-performing learner withrespect to F1).
The results are shown in Table 5.Test DataDT/Directive 1f3 2f2Train Data Error F1 Error F11f3 25.1 71.6 16.4 72.82f2 20.1 68.8 18.8 71.2VP/Deliver1f3 30.1 55.1 21.1 56.12f2 35.0 25.4 21.1 35.7SVM/Commit1f3 23.4 39.7 15.2 31.62f2 31.9 27.3 16.4 15.1Table 5 - Transferability of classifiersIf learned classifiers were highly specific to aparticular set of users, one would expect that thediagonal entries of these tables (the ones basedon cross-validation within a corpus) would ex-hibit much better performance than the off-diagonal entries.
In fact, no such pattern isshown.
For Directive verbs, performance is simi-lar across all table entries, and for Deliver andCommit, it seems to be somewhat better to trainon NF01F3 regardless of the test set.4.4 Future DirectionsNone of the algorithms or representations dis-cussed above take into account the context of anemail message, which intuitively is important indetecting implicit speech acts.
A plausible notionof context is simply the preceding message in anemail thread.Exploiting this context is non-trivial for sev-eral reasons.
Detecting threads is difficult; al-though email headers contain a ?reply-to?
field,users often use the ?reply?
mechanism to startwhat is intuitively a new thread.
Also, sinceemail is asynchronous, two or more users mayreply simultaneously to a message, leading to athread structure which is a tree, rather than a se-quence.
Finally, most sequential learning modelsassume a single category is assigned to each in-stance?e.g., (Ratnaparkhi, 1999)?whereas ourscheme allows multiple categories.Classification of emails according to our verb-noun ontology constitutes a special case of a gen-eral family of learning problems we might callfactored classification problems, as the classes(email speech acts) are factored into two features(verbs and nouns) which jointly determine thisclass.
A variety of real-world text classificationproblems can be naturally expressed as factoredproblems, and from a theoretical viewpoint, theadditional structure may allow construction ofnew, more effective algorithms.For example, the factored classes provide amore elaborate structure for generative probabil-istic models, such as those assumed by Na?veBayes.
For instance, in learning email acts, onemight assume words were drawn from a mixturedistribution with one mixture component pro-duces words conditioned on the verb class factor,and a second mixture component generates wordsconditioned on the noun (see Blei et al(2003) fora related mixture model).
Alternatively, modelsof the dependencies between the different factors(nouns and verbs) might also be used to improveclassification accuracy, for instance by buildinginto a classifier the knowledge that some nounsand verbs are incompatible.The fact that an email can contain multipleemail speech acts almost certainly makes learn-ing more difficult: in fact, disagreement betweenhuman annotators is generally higher for longermessages.
This problem could be addressed bymore detailed annotation: rather than annotatingeach message with all the acts it contains, humanannotators could label smaller message segments(say, sentences or paragraphs).
An alternative tomore detailed (and expensive) annotation wouldbe to use learning algorithms that implicitly seg-ment a message.
As an example, another mixturemodel formulation might be used, in which eachmixture component corresponds to a single verbcategory.5 Concluding RemarksWe have presented an ontology of ?emailspeech acts?
that is designed to capture some im-portant properties of a central use of email: nego-tiating and coordinating joint activities.
Unlikeprevious attempts to combine speech act theorywith email (Winograd, 1987; Flores and Ludlow,1980), we propose a system which passively ob-serves email and automatically classifies it byintention.
This reduces the burden on the users ofthe system, and avoids sacrificing the flexibilityand socially desirable aspects of informal, naturallanguage communication.This problem also raises a number of interest-ing research issues.
We showed that entity ex-traction and part of speech tagging improvesclassifier performance, but leave open the ques-tion of whether other types of linguistic analysiswould be useful.
Predicting speech acts requirescontext, which makes classification an inherentlysequential task, and the labels assigned to mes-sages have non-trivial structure; we also leaveopen the question of whether these properties canbe effectively exploited.Our experiments show that many categoriesof messages can be detected, with high precisionand moderate recall, using existing text-classification learning methods.
This suggeststhat useful task-tracking tools could be con-structed based on automatic classifiers?a poten-tially important practical application.ReferencesS.
Argamon, M. ?ari?
and S. S. Stein.
(2003).
Stylemining of electronic messages for multiple authorshipdiscrimination: first results.
Proceedings of the 9thACM SIGKDD, Washington, D.C.V.
Bellotti, N. Ducheneaut, M. Howard and I.
Smith.(2003).
Taking email to task: the design and evalua-tion of a task management centered email tool.
Pro-ceedings of the Conference on Human Factors inComputing Systems, Ft. Lauderdale, Florida.D.
Blei, T. Griffiths, M. Jordan, and J.
Tenenbaum.(2003).
Hierarchical topic models and the nested Chi-nese restaurant process.
Advances in Neural Informa-tion Processing Systems, 16, MIT Press.B.
M. Camino, A. E. Millewski, D. R. Millen and T.M.
Smith.
(1998).
Replying to email with structuredresponses.
International Journal of Human-ComputerStudies.
Vol.
48, Issue 6, pp 763 ?
776.J.
Carletta.
(1996).
Assessing Agreement on Classifi-cation Tasks: The Kappa Statistic.
ComputationalLinguistics, Vol.
22, No.
2, pp 249-254.V.
R. Carvalho & W. W. Cohen (2004).
Learning toExtract Signature and Reply Lines from Email.
Toappear in Proc.
of the 2004 Conference on Email andAnti-Spam.
Mountain View, California.W.
W. Cohen.
(1996).
Learning Rules that Classify E-Mail.
Proceedings of the 1996 AAAI Spring Sympo-sium on Machine Learning and Information Access,Palo Alto, California.W.
W. Cohen.
(2004).
Minorthird: Methods for Identi-fying Names and Ontological Relations in Text usingHeuristics for Inducing Regularities from Data,http://minorthird.sourceforge.net.M.
Finke, M. Lapata, A. Lavie, L. Levin, L. May-fieldTomokiyo, T. Polzin, K. Ries, A. Waibel and K.Zechner.
(1998).
CLARITY: Inferring DiscourseStructure from Speech.
In Applying Machine Learn-ing to Discourse Processing, AAAI'98.F.
Flores, and J.J. Ludlow.
(1980).
Doing and Speak-ing in the Office.
In: G. Fick, H. Spraque Jr.
(Eds.
).Decision Support Systems: Issues and Challenges,Pergamon Press, New York, pp.
95-118.Y.
Freund and R. Schapire.
(1999).
Large MarginClassification using the Perceptron Algorithm.
Ma-chine Learning 37(3), 277?296.T.
Joachims.
(2001).
A Statistical Learning Model ofText Classification with Support Vector Machines.Proc.
of the Conference on Research and Develop-ment in Information Retrieval (SIGIR), ACM, 2001.R.
E. Kraut, S. R. Fussell, F. J. Lerch, and J. A.Espinosa.
(under review).
Coordination in teams: evi-dence from a simulated management game.
To appearin the Journal of Organizational Behavior.D.
D. Lewis.
(1992).
Representation and Learning inInformation Retrieval.
PhD Thesis, No.
91-93, Com-puter Science Dept., Univ of Mass at AmherstA.
McCallum, D. Freitag and F. Pereira.
(2000).Maximum Entropy Markov Models for InformationExtraction and Segmentation.
Proc.
of the 17th Int?lConf.
on Machine Learning, Nashville, TN.B.
Pang, L. Lee and S. Vaithyanathan.
(2002).Thumbs up?
Sentiment Classification using MachineLearning Techniques.
Proc.
of the 2002 Conferenceon Empirical Methods in Natural Language Process-ing (EMNLP), pp 79-86.A.
E. Milewski and T. M. Smith.
(1997).
An Experi-mental System For Transactional Messaging.
Proc.
ofthe international ACM  SIGGROUP conference onSupporting group work: the integration challenge, pp.325-330.H.
Murakoshi, A. Shimazu and K. Ochimizu.
(1999).Construction of Deliberation Structure in EmailCommunication,.
Pacific Association for Computa-tional Linguistics, pp.
16-28, Waterloo, Canada.A.
Ratnaparkhi.
(1999).
Learning to Parse NaturalLanguage with Maximum Entropy Models.
MachineLearning, Vol.
34, pp.
151-175.J.
D. M. Rennie.
(2000).
Ifile: An Application of Ma-chine Learning to Mail Filtering.
Proc.
of the KDD-2000 Workshop on Text Mining, Boston, MA.M.
Sahami, S. Dumais, D. Heckerman and E.
Horvitz.(1998).
A Bayesian Approach to Filtering Junk E-Mail.
AAAI'98 Workshop on Learning for Text Cate-gorization.
Madison, WI.M.
Schoop.
(2001).
An introduction to the language-action perspective.
SIGGROUP Bulletin, Vol.
22, No.2, pp 3-8.S.
Scott and S. Matwin.
(1999).
Feature engineeringfor text classification.
Proc.
of 16th International Con-ference on Machine Learning, Bled, Slovenia.J.
R. Searle.
(1975).
A taxonomy of illocutionary acts.In K. Gunderson (Ed.
), Language, Mind and Knowl-edge, pp.
344-369.
Minneapolis: University of Min-nesota Press.R.
B. Segal and J. O. Kephart.
(2000).
Swiftfile: Anintelligent assistant for organizing e-mail.
In AAAI2000 Spring Symposium on Adaptive User Interfaces,Stanford, CA.Y.
Yang.
(1999).
An Evaluation of Statistical Ap-proaches to Text Categorization.
Information Re-trieval, Vol.
1, Numbers 1-2, pp 69-90.S.
Wermter and M. L?chel.
(1996).
Learning dialogact processing.
Proc.
of the International Conferenceon Computational Linguistics, Kopenhagen, Denmark.R.
E. Schapire and Y.
Singer.
(1998).
Improved boost-ing algorithms using confidence-rated predictions.
The11th Annual Conference on Computational LearningTheory, Madison, WI.H.
Wiegend, G. Goldkuhl, and A. de Moor.
(2003).Proc.
of the Eighth Annual Working Conference onLanguage-Action Perspective on CommunicationModelling (LAP 2003), Tilburg, The Netherlands.J.
Wiebe, R. Bruce, M. Bell, M. Martin and T.
Wilson.(2001).
A Corpus Study of Evaluative and SpeculativeLanguage.
Proceedings of the 2nd ACL SIGdialWorkshop on Discourse and Dialogue.
Aalborg,Denmark.T.
Winograd.
1987.
A Language/Action Perspectiveon the Design of Cooperative Work.
Human-Computer Interactions, 3:1, pp.
3-30.S.
Whittaker, Q. Jones, B. Nardi, M. Creech, L.Terveen, E. Isaacs and J. Hainsworth.
(in press).
Us-ing Personal Social Networks to Organize Communi-cation in a Social desktop.
To appear in Transactionson Human Computer Interaction.
