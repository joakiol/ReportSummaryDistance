2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 284?294,Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational LinguisticsCorrecting Comma Errors in Learner Essays, and Restoring Commas inNewswire TextRoss IsraelIndiana UniversityMemorial Hall 322Bloomington, IN 47405, USAraisrael@indiana.eduJoel TetreaultEducational Testing Service660 Rosedale RoadPrinceton, NJ 08541, USAjtetreault@ets.orgMartin ChodorowHunter College of CUNY695 Park AvenueNew York, NY 10065, USAmchodoro@hunter.cuny.eduAbstractWhile the field of grammatical error detectionhas progressed over the past few years, onearea of particular difficulty for both native andnon-native learners of English, comma place-ment, has been largely ignored.
We present asystem for comma error correction in Englishthat achieves an average of 89% precision and25% recall on two corpora of unedited studentessays.
This system also achieves state-of-the-art performance in the sister task of restor-ing commas in well-formed text.
For bothtasks, we show that the use of novel featureswhich encode long-distance information im-proves upon the more lexically-driven featuresused in prior work.1 IntroductionAutomatically detecting and correcting grammati-cal errors in learner language is a growing sub-fieldof Natural Language Processing.
As the field hasprogressed, we have seen research focusing on arange of grammatical phenomena including Englisharticles and prepositions (c.f.
Tetreault et al, 2010;De Felice and Pulman, 2008), particles in Koreanand Japanese (c.f.
Dickinson et al, 2011; Oyama,2010), and broad approaches that aim to find mul-tiple error types (c.f Rozovskaya et al, 2011; Ga-mon, 2011).
However, to the best of our knowledge,there has not been any research published specifi-cally on correcting erroneous comma usage in En-glish (though there have been efforts such as the MSWord grammar checker, and products like Gram-marly and White Smoke that include comma check-ing).There are a variety of reasons that motivate ourinterest in attempting to correct comma errors.
Firstof all, a review of error typologies in Leacock et al(2010) reveals that comma usage errors are thefourth most common error type among non-nativewriters in the Cambridge Learner Corpus (Nicholls,1999), which is composed of millions of words oftext from essays written by learners of English.
Theproblem of comma usage is not limited to non-native writers; six of the top twenty error types fornative writers involve misuse of commas (Connorsand Lunsford, 1988).
Given these apparent deficitsamong both non-native and native speakers, devel-oping a sound methodology for automatically iden-tifying comma errors will prove useful in both learn-ing and automatic assessment environments.A quick examination of English learner essays re-veals a variety of errors, with writers both overusingand underusing commas in certain contexts.
Con-sider examples (1) and (2):(1) erroneous: If you want to be a master youshould know your subject well.corrected: If you want to be a master , youshould know your subject well.
(2) erroneous: I suppose , that it is better to spe-cialize in one specific subject.corrected: I suppose that it is better to special-ize in one specific subject.In example (1), an introductory conditional phrasebegins the sentence, but the learner has not used theappropriate comma to separate the dependent clausefrom the independent clause.
The comma in thiscase helps the reader to see where one clause ends284and another begins.
In example (2), the comma aftersuppose is unnecessary in American English, and al-though this error is related more to style than to read-ability, most native writers would omit the commain this context, so it should be avoided by learners aswell.Another motivating factor for this work is the factthat sentence internal punctuation contributes to theoverall readability of a sentence (Hill and Murray,1998).
Proper comma placement can lead to fasterreading times and reduce the need to re-read en-tire sentences.
Commas also help remove or reduceproblems arising from difficult ambiguities; the gar-den path effect can be greatly reduced if commas arecorrectly inserted after introductory phrases and re-duced relative clauses.This paper makes the following contributions:?
We present the first published comma error cor-rection system for English, evaluated on essayswritten by both native and non-native speakersof English.?
The same system also achieves state-of-the-artperformance in the task of restoring commas inwell-edited text.?
We describe a novel annotation scheme that al-lows for robust mark up of comma errors anduse it to annotate two corpora of student essays.?
We show that distance and combination fea-tures can improve performance for both the er-ror correction and restoration tasks.The rest of this paper is organized as follows.In section 2, we review prior work.
Section 3 de-tails our typology of comma usage.
We discuss ourchoice of classifier and selection of features in sec-tion 4.
In section 5, we apply our system to the taskof comma restoration.
We describe our annotationscheme and error correction system and evaluationin sections 6 and 7.
Finally, we summarize and out-line plans for future research in section 8.2 Previous WorkThe only reported research that we are awareof which specifically deals with comma errors inlearner writing is reported in Hardt (2001) and Ale-gria et al (2006), two studies that deal with Dan-ish and Basque, respectively.
Hardt (2001) employsan error driven approach featuring the Brill tagger(Brill, 1993).
The Brill tagger works as it wouldfor the part-of-speech tagging task for which it wasdesigned, i.e.
it learns rules based on templates byiterating over a large corpus.
This work is also eval-uated on native text where all existing commas areconsidered correct, and additional ?erroneous?
com-mas are added randomly to a sub-corpus, so that thetagger can learn from the errors.
The system is testedon a distinct subset for the task of correcting exist-ing comma errors and achieves 91.4% precision and76.9% recall.Alegria et al (2006) compare implementationsof Naive Bayes, decision-tree, and support vectormachine (SVM) classifiers and utilize a feature setbased on word-forms, categories, and syntactic in-formation about each decision point.
While the sys-tem is designed as a possible means for correctingerrors, it is only evaluated on the task of restor-ing commas in well-formed text produced by nativewriters.
The system obtains good precision (96%)and recall (98.3%) for correctly not inserting com-mas, but performs less well at actually insertingcommas (69.6% precision, 48.6% recall).It is important to note that the results in both of theprojects are based on constructed errors in an other-wise native corpus which is free of any other con-textual errors that might be present in actual learnerdata.
Moreover, as we will show in section 6, er-rors of omission (failing to use needed commas) aremuch more common than errors of commission (in-serting commas inappropriately) in the English asa Foreign Language (EFL) data that we use.
Cru-cially, our error correction efforts described in sec-tion 7 must be able to account for noise and be ableto insert new commas as well as remove erroneousones, as we do evaluate on a set of English learneressays.Although we have not found any work publishedspecifically on correcting comma errors in English,for language learners or otherwise, there is a fairlylarge amount of work that focuses on the task ofcomma restoration.
Comma restoration refers toplacing commas in a sentence which is presentedwith no sentence internal punctuation.
This task is285mostly attempted in the larger context of AutomaticSpeech Recognition (ASR), since there are no ab-solute cues of where commas should be placed in astream of speech.
Many of these systems use featuresets that include prosodic elements that are clearlynot available for text based work (see e.g., Favreet al, 2009; Huang and Zweig, 2002; Moniz et al,2009).There are, however, a few punctuation restora-tion projects that have used well-formed text-onlydata.
Shieber and Tao (2003) explore restoring com-mas to the Wall Street Journal (WSJ) section ofthe Penn Treebank (PTB).
The authors augment aHMM trigram-based system with constituency parseinformation at each insertion point.
Using fullycorrect parses directly from the PTB, the authorsachieve an F-score of 74.8% and sentence accuracyof 57.9%1.
However, a shortcoming of this method-ology is that it dictates that all commas are missing,but these parses were generated with comma infor-mation present in the sentence and moreover hand-corrected by human annotators.
Using parses auto-matically generated with commas removed from thedata, they achieve an F-score of 70.1% and sentenceaccuracy of 54.9%.More recently, Gravano et al (2009), who workwith newswire text, including WSJ, pursue the taskof inserting all punctuation and correcting capital-ization in a string of text in a single pass, ratherthan just comma restoration, but do provide resultsbased solely on comma insertion.
The authors em-ploy an n-gram language model and experiment withn-grams from size n = 3 to n = 6, and with differenttraining data sizes.
The result relevant to our work istheir comma F-score on WSJ test data, which is justover 60% when using 5-grams and 55 billion train-ing tokens.
Baldwin and Joseph (2009) also restorepunctuation and capitalization to newswire texts, us-ing machine based learning with retagging.
Theirresults are difficult to compare with our work be-cause they use a different data set and do not focuson commas in their evaluation.Lu and Ng (2010) take an approach that inserts all1Sentence accuracy is a measure used by some in the fieldthat counts sentences with 100% correct comma decisions ascorrect, and any sentence where a comma is missing or mis-takenly placed as incorrect.
It is motivated by the idea that allcommas are essential to understanding a sentence.punctuation symbols into text.
They use transcribedEnglish and Chinese speech data and do not providespecific evaluation for commas, however one im-portant contribution of their research to our currenttask is the finding that Conditional Random Fields(CRFs) perform better at this task than Hidden EventLanguage Models, another algorithm that has beenused for restoration.
One reason for this could beCRFs?
better handling of long range dependenciesbecause they model the entire sequence, rather thanmaking a singular decision based on information ateach point in the sequence (Liu et al, 2005).
CRFsalso do not suffer from the label bias problem thataffects Maximum Entropy classifiers (Lafferty et al,2001).3 Comma UsageOne of the challenges present in this research is theambiguity as to what constitutes ?correct?
commausage in American English.
For one thing, notall commas contribute to grammaticality; some aremore tied to stylistic rules and preferences.
Whilethere are certainly rule-based decision points forcomma insertion (Doran, 1998), particularly in thecase of commas that set off significant chunks orphrases within sentences, there are also some com-mas that appear to be more prescriptive, as they haveless of an effect on sentence processing (such as inexample (2) in the introduction), and opposing us-age rules for the same contexts are attested in differ-ent style manuals.
A common example of opposingrules is the notorious serial or Oxford comma thatrefers to the final comma found in a series, whichis required by the Chicago Manual of Style (Univer-sity of Chicago, 1993), but is considered incorrectby the New York Times Manual of Style (Siegal andConnolly, 1999).As a starting point, we needed to know what kindsof commas are taught by English language teachers,as well as what style manuals recommend and/or re-quire.
However, creating a list of comma uses wasa non-trivial part of the process.
After consultingstyle manuals (University of Chicago, 1993; Siegaland Connolly, 1999; Strunk and White, 1999) andpopular ESL websites, we compiled a list of over 30rules for use of commas in English.
We took themost commonly mentioned rules and created a final286Rule ExampleElements in a List Paul put the kettle on, Don fetched the teapot, and I made tea.Initial Word/Phrase Hopefully, this car will last for a while.Dependent Clause After I brushed the cat, I lint-rollered my clothes.Independent Clause I have finished painting, but he is still sanding the doors.Parentheticals My father, a jaded and bitter man, ate the muffin.Quotations ?Why,?
I asked, ?do you always forget to do it?
?Adjectives She is a strong, healthy woman.Conjunctive Adverbs I would be happy, however, to volunteer for the Red Cross.Contrasting Elements He was merely ignorant, not stupid.Numbers 345,280,000Dates She met her husband on December 5, 2003.Geographical Names I lived in San Francisco, California, for 20 years.Titles Al Mooney, M.D., is a good doctorIntroducing Words You may be required to bring many items, e.g., spoons, pans, and flashlights.Other Catch-all rule for any other comma useTable 1: Common Comma Useslist of 15 usage rules (the 14 most common plus onemiscellaneous category) for our annotation scheme,which is discussed in section 6.
These rules aregiven in Table 1.
The 16 rules that were removedfrom the list occurred in only one source or weresimilar enough to other rules to be conflated.
It isworth noting here that while many of the commauses in this table might be best served by some sta-tistical methodology like the one we describe in sec-tion 4, one can envision fairly simple heuristic rulesto insert commas and find errors in numbers, dates,geographical names, titles, and introducing words.4 Classifier and FeaturesWe use CRFs2 as the basis for our system and treatthe task of comma insertion as a sequence label-ing task; each space between words is consideredby the classifier, and a comma is either inserted ornot.
The feature set incorporates features that haveproven useful in comma restoration and other errorcorrection tasks, as well as a handful of new featuresdevised for this specific task (combination and dis-tance features).
The full set of features used in ourfinal system is given in Figure 1 along with exam-ples of each feature for the sentence If the teachereasily gets mad , then the child will always fear go-ing to school and class.
The target insertion point isafter the word mad.2http://crfpp.sourceforge.net/Feature Example(s)Lexical and Syntactic Featuresunigram easily, gets, mad, then, thebigram easily gets, gets mad, mad then, ...trigram easily gets mad, gets mad then, ...pos uni RB, VBZ, JJ, RB, DTpos bi RB VBZ, VBZ JJ, JJ RB, ...pos tri RB VBZ JJ, VBZ JJ RB, ...combo easily+RB, gets+VBZ,mad+JJ, ...first combo If+RBDistance Featuresbos dist 5eos dist 10prevCC dist -nextCC dist 9Figure 1: CRF Features with examples for:If the teacher easily gets mad , then the child will alwaysfear going to school and class.4.1 Lexical and Syntactic FeaturesThe first six features in Figure 1 refer to simple uni-grams, bigrams, and trigrams of the words and POStags in a sliding 5 word window (target word, +/- 2words).
The lexical items help to encode any id-iosyncratic relationships between words and com-mas that might not be exploited through the exami-nation of more in-depth linguistic features.
For ex-ample, then is a special case of an adverb (RB) thatis often preceded by a comma, even if other adverbsare not, so POS tags might not capture this relation-287ship.
The lexical items also provide an approxima-tion of a language model or hidden event languagemodel approach, which has proven to be useful incomma restoration tasks (see e.g.
Lu and Ng, 2010).The POS features abstract away from the wordsand avoid the problem of data sparseness by allow-ing the classifier to focus on the categories of thewords, rather than the lexical items themselves.
Thecombination (combo) feature is a unigram of theword+pos for every word in the sliding window.
Itreinforces the relationship between the lexical itemsand their POS tags, further strengthening the evi-dence of entries like then RB.
All of these featureshave been used in previous grammatical error detec-tion tasks which target particle, article, and prepo-sition errors (c.f., Dickinson et al, 2011; Gamon,2010; Tetreault and Chodorow, 2008).The first combo feature keeps track of the firstcombination feature of the sentence so that it canbe referred to by the classifier throughout process-ing the entire sentence.
This feature is helpful whenan introductory phrase is longer than the classifier?sfive word window.
Figure 1 provides a good exam-ple of the utility of this feature, as If the teacher eas-ily gets mad is so long that by the time the windowhas moved to the target position of the space follow-ing mad, the first word and POS, If RB, which canoften indicate an introductory phrase, is beyond thescope of the sliding window.4.2 Distance FeaturesNext, we encode four distance features.
We keeptrack of the following distances: from the beginningof the sentence (bos dist), to the end of the sentence(eos dist), from the previous coordinating conjunc-tion (prevCC dist), and to the next coordinating con-junction (nextCC dist).
All of these distance fea-tures help the classifier by encoding measures forcomponents of the sentence that can affect the deci-sion to insert a comma.
These features are especiallyhelpful over long range dependencies, when the in-formation encoded by the feature is far outside thescope of the 5-word window the CRF uses.
The dis-tance to the beginning of the sentence helps to en-code introductory words and phrases, which makeup the bulk of the commas used in essays by learnersof English.
The distance to the end of the sentenceis less obviously useful, but it can let the classifierknow the likelihood of a phrase beginning or endingat a certain point in the sentence.
The distances toand from the nearest CC are useful because manycommas are collocated with coordinating conjunc-tions.
The distance features, as well as first combo,were designed specifically for the task of comma er-ror correction, and have not, as far as we know, beenutilized in previous research.5 Comma RestorationBefore applying our system to the task of error cor-rection, we tested its utility in restoring commas innewswire texts.
Specifically, we evaluate on section23 of the WSJ, training on sections 02-22.
Here,the task is straightforward: we remove all commasfrom the test data and performance is measured onthe system?s ability to put the commas back in theright places.
After stripping all commas from ourtest data, the text is tokenized and POS tagged usinga maximum entropy tagger (Ratnaparkhi, 1996) andevery token is considered by the classifier as eitherrequiring a following comma or not.
Out of 53,640tokens, 3062 should be followed by a comma.
Weprovide accuracy, precision, recall, F1-score, andsentence accuracy (S Acc.)
for these tests, alongwith results from Gravano et al (2009) and Shieberand Tao (2003) in Table 2.
The first system (LexSyn)includes only the lexical and syntactic features fromFigure 1; the second (LexSyn+Dist) includes all ofthe features.System Acc.
P R F S Acc.LexSyn 97.4 85.8 64.9 73.9 60.5LexSyn+Dist 97.5 85.8 66.3 74.8 61.4Shieber & Tao 97.0 79.7 62.6 70.1 54.9Gravano et al N.A.
57 67 ?61 N.A.Table 2: Comma Restoration System Results (%)As can be seen in Table 2, the full system(LexSyn+Dist) performs significantly better thanWSJ LexSyn (p < .02, two-tailed), achieving anF-score of 74.8 on WSJ.
This F-score outperformsShieber and Tao?s system, which was also tested onsection 23 of the WSJ, by about 4% and our sentenceaccuracy of 61.5% is about 7% higher than theirs.Our F-score is also about 13% higher than that ofGravano et al (2009), however, they evaluate on the288entire WSJ section of the Penn Treebank, so it is nottotally fair to compare results.6 AnnotationFor the comma restoration task, we needed only toobtain well-formed text and remove the commas toproduce a test set.
However, this is not so in the caseof error correction.
In order to test a system thatcorrects errors in learner essays, we need an anno-tated test corpus that tells us where the errors are.Although there are a handful of corpora that includepunctuation errors in their annotation scheme, suchas NUCLE (Dahlmeier and Ng, 2011) and HOO(Dale and Kilgarriff, 2010), there are none to ourknowledge that focus specifically on commas.
Thus,we designed and implemented our own annotationscheme on a set of essays to allow us the freedom toidentify the most important aspects of comma usagefor our work.Our annotation scheme allows the mark-up of anumber of aspects of comma usage.
First, eachcomma in a text is marked as rejected or acceptedby the annotator.
Additionally, any space betweenwords can be treated as an insertion point for a miss-ing comma.
The annotators also marked all acceptedand inserted commas as either required or optional.Finally, the annotation also includes the appropriateusage rule from the set in Table 1.3 In contrast, theNUCLE and HOO data sets do not have this gran-ularity of information (the annotation only indicateswhether a comma should be inserted or removed)and are not exhaustively annotated.After a one-hour training session on comma us-age rules, three native English speakers were given aset of ten learner essays comprising 3,665 tokens toannotate for comma errors.
To assess the difficultyof the annotation task, we calculated agreement andkappa.
Agreement is a simple measure of how oftenthe annotators agree, and kappa provides a more ro-bust measure of agreement since it takes chance intoaccount (Cohen, 1960).
Table 3 provides the resultsof these measurements.
As can be seen in the table,the agreement is quite high at either 97 or 98%, andkappa is a bit lower, ranging from 72 to 81%.
The3The full annotation manual is available athttp://www.cs.rochester.edu/?tetreaul/comma-manual.pdfagreement is likely so high due to the great numberof decision points where it is obvious to any nativewriter that no comma is needed.
To account for thisimbalance, we also provide an adjusted agreementin the final column of the table that excludes all de-cisions where both annotators agree that no commais necessary.Annotators Agreement Kappa Adj.
agr.1 & 2 97 74 611 & 3 98 72 612 & 3 98 81 76Table 3: Agreement over Annotation Training Set (%)After completing the training phase, we assignedone annotator the task of annotating our develop-ment and test data from two different corpora: es-says written by English as a foreign language learn-ers (EFL) and essays written by native speakers ofEnglish (Native).
For both data sets we selected 60essays for development and 60 essays for test.
Theannotation was carried out using an annotation tooldeveloped in-house that gives the annotator an easyto use interface and outputs standoff annotations inxml format.
(3) is an example of an annotated sen-tence from an EFL essay, where ?
??
marks a spanfor annotation.
(3) The new millenium , 1 the 21st century 2has dawned upon us 3 and this new centuryhas brought many positive advancements in ourdaily lives .1) Accept, required, parenthetical2) Insert, required, parenthetical3) Insert, required, independent clauseTable 4 provides the comma usage information forthe essays in both sets used in development and test-ing.
The table shows the total number of sentences,commas in the original text that were accepted bythe annotator, and errors (rejected and missing com-mas) for the 60 essays in each set.As can be seen in Table 4, the majority of exist-ing commas (columns Accept plus Rej) in the textswere accepted by the annotator; about 84% in theEFL development set, 87% in the EFL test set, 85%in the Native development set, and 88% in the Na-tive test set.
The important fact uncovered by thesenumbers is that most of the commas that learners do289Data Set SentCommasAcceptErrorsRej MissEFL Dev 717 474 49 233EFL Test 683 427 65 232Native Dev 970 506 86 363Native Test 839 377 50 314Table 4: Comma Usage Statisticsuse are correct.
However, there are a great numberof commas that the annotator inserted (over 80% ofall errors are missing commas) meaning that theselearners are more prone to underusing than overus-ing commas.
Another interesting fact that can begleaned from our annotation is that the top fivecomma uses, those listed in the first five rows of Ta-ble 1, account for more than 80% of all commas inthese essays.7 Error CorrectionWith a competitive comma restoration system inplace, we turn to the primary task of correcting er-rors in learner essays.
While the task remains simi-lar to comma restoration, error correction in studentwriting brings a new set of challenges, especiallywhen the writers are non-native.
Newswire texts aremost often well-formed, so the system should notexperience interference from other contextual errorsaround the missing commas.
Sentences taken fromlearner texts, though, often contain multiple errorsthat can make it difficult to focus on a single problemat a time.
Spelling errors, for example, can exacer-bate error correction efforts that use contextual lex-ical features because well-formed text that is oftenused for training data is usually free of such noise.In these experiments, we use the annotated es-says described in section 6 for evaluation and trainon 40,000 sentences taken from essays written byboth native and non-native high level college stu-dents.
All of the essays are run through automaticspelling correction to reduce the noise in the test setbefore being tagged with the same tagger used in thecomma restoration experiments.Because we approach comma error correction asessentially a comma restoration task, we can we uselargely the same system for error correction as wedid for comma restoration.
We still employ CRFsand label each space between words as requiringa comma or not, however, there is one significantchange to our methodology for this task.
Namely,we can leave the commas that were present in thetext as provided by the writer as we pre-processthe data for error correction, whereas they were re-moved in the comma restoration task.
For error cor-rection, the task is really comparing the system?s an-swer to the annotator?s and the learner?s, as opposedto simply inserting commas into raw text.
Leavingthe learners?
commas in the text does introduce someerrors to the POS tagging phase.
However, sinceover 85% of the existing commas in the developmentset were judged as acceptable by our annotator (cf.section 6) , the number of erroneous commas is notso great as to contaminate the system.
Removing allof the commas would introduce unnecessary errorsin the pre-processing phase.We also augment the system with three post-processing filters that we tuned on the developmentset.
One requires that the classifier be completelyconfident before a change is made to an existingcomma; crf++ will give 100% confidence to a singleclass in some cases.
This filter is based on the factthat 85% of the existing commas can be expectedto be correct.
A similar filter requires that the clas-sifier be at least 90% confident in a decision to in-sert a new comma.
The final filter, which overridesany other information provided by the system, doesnot allow commas to be inserted before the word be-cause.
These ensure high precision even though theymay reduce recall.Table 5 provides the accuracy, precision, recall,F-score, and number of errors in each set for tests onour 60 annotated EFL and Native essays, and the re-sult for the combined corpus.
The system performsquite well on the EFL test set, with scores of 94%precision, 31.7% recall, and 47.4% F-score for theLexSyn+Dist system.
The results for the Native setare a bit lower, with 84.9% precision, 20% recall,and 32.4% F-score for the LexSyn+Dist system.For both data sets, when the distance features areadded to the model, precision increases by 1%, andin the EFL set, recall also increases.
In keeping withpractices established within the field of grammaticalerror correction, the system has been optimized forhigh precision even at the cost of recall, to ensurethat feedback systems avoid confusing learners by290Data System Acc.
P R F nEFLLexSyn 98.2 92.9 30.9 46.5 297LexSyn+Dist 98.3 94.0 31.7 47.4 297NativeLexSyn 97.8 83.9 20.0 32.3 365LexSyn+Dist 97.8 84.9 20.0 32.4 365CombinedLexSyn 98.1 88.7 24.9 38.9 662LexSyn+Dist 98.1 89.8 25.2 39.4 662Table 5: Comma Error Correction Results (%)marking correct comma usage as erroneous.
Con-sidering performance over all of the test data, thesystem achieves over 89% precision and 25% recall,results which are comparable to those in other er-ror correction tasks.
For example, the prepositionerror detection system described in Tetreault andChodorow (2008) achieved 84% precision, 19% re-call for prepositions.It is worth noting that the results in Table 5 in-clude commas that the annotator had marked asoptional.
For these, whatever decision the systemmakes is scored as correct.
Since the grammatical-ity/readability of the sentence will not be affected bythe presence or absence of a comma in these cases,we feel this is the fairest assessment of the system.7.1 Error AnalysisIn order to get a sense of what kinds of construc-tions are difficult for our system, we randomly ex-tracted 50 sentences from the output that exhibitedat least one wrong comma decision made by the sys-tem.
The 50 sentences contained a combined total of62 system errors.
Among these cases, the most com-mon context where the system makes the wrong de-cision is in introductory words and phrases, which isnot surprising given the frequency with which com-mas occur in these environments in our developmentset (about 40% of all commas in the essays).
In (4),for example, the first word, Here, should be followedby a comma.
Since Here is not a common introduc-tory word in this type of sentence structure in thetraining data, this is a difficult case for the system tocorrect.
(4) Here we can get specific knowledge in the sci-ence that we like the most .The next most common misclassification involvescomma splices, i.e.
conjoining complete sentenceswith a comma rather than separating them with a fullstop.
In (5), for example, there should be a full stopbetween college and I, rather than a comma.
Thisresult is not surprising because the system is notyet equipped to deal with comma splices.
Commasplices are a different type of phenomenon becausecorrecting them requires removing the comma andinserting a full stop, essentially two separate stepsrather than the single reject/accept step that the sys-tem currently handles.
(5) I entered college, I could learn it and make aneffort to achieve my goal.The next most common context for system errorswas between clauses that are conjoined with a co-ordinating conjunction as in (6), where there shouldnot be a comma.
In (6), the second clause is actuallya dependent clause, so no comma should precedethe coordinating conjunction.
There are a numberof system errors dealing with commas between twoindependent clauses.
For example in (7), our annota-tor recommended a comma between things and but,however the system did not make the insertion.
Theproblem with these examples likely stems from thefact that the rule for comma usage in these contextsis not clearly stated, even in well-respected manu-als, and therefore likely not clearly understood, evenby high-level native writers.
For example, the NYTstyle manual (Siegal and Connolly, 1999) states that?Commas should be used in compound sentencesbefore conjunctions...
When the clauses are excep-tionally short, however, the comma may be omit-ted.?
Adding a feature that measures clause lengthmight help, but even then the classifier must rely ontraining data that may have considerable variationas to what length of clauses requires an interveningcomma.291(6) They wants to see their portfolio, and what kindof skill do they have for company.
(7) I have many things but the best is my parents.Another facet of the data that consistently chal-lenges the system is the existence of errors otherthan the commas in the sentences.
Consider the sen-tence in (8), where erroneous is the original textfrom the essay and corrected is a well-formed in-terpretation.
(8) erroneous: In the other hand , having justone specific subject , which represents a greatdownfall for many studentscorrected: On the other hand, knowing onlyone subject is a downfall for many students.The comma after subject is unnecessary, but so isthe word which.
In fact, which would normally sig-nify the beginning of a non-restrictive clause in thiscontext, which should be set off with a comma.
Itis no surprise then, that the system has trouble re-moving commas in these types of contexts.
At least11 of the 62 system mistakes that we examined havegrammatical errors in the immediate context of thecomma in question, which makes the classificationmore difficult.8 Summary and ConclusionWe presented a novel comma error correction sys-tem for English that achieves an average of 89% pre-cision and 25% recall on essays written by learn-ers of different levels and language backgrounds,including native English speakers.
The systemachieves state-of-the-art performance on the task ofcomma restoration, beating previous systems?
F-score and sentence accuracy by 4% and 7%, respec-tively.
We discovered that augmenting lexical fea-tures, which have been commonly used in previouswork, with the combination and distance featurescan improve F-score by as much as 1% in both theerror correction and comma restoration tasks.
Wealso developed and implemented a novel comma er-ror annotation scheme.Additionally, we are interested in the effect ofcorrect comma placement on other NLP processes.Jones (1994) and Briscoe and Carroll (1995) showthat adding punctuation to grammars that utilizepart-of-speech (POS) tags, rather than lexical items,adds more structure and reduces ambiguity as wellas the number of parses for each sentence.
Simi-larly, Doran (1998) and White and Rajkumar (2008)found that adding punctuation improved parsing re-sults in tree-adjoining grammar (TAG) and combi-natorial categorial grammar (CCG) parsing, respec-tively.
These studies all highlight the importance ofcorrectly inserted punctuation, especially commas,for parsing.
Given these results, we believe that byenhancing the quality of the text, comma error cor-rection will improve not only tagging and parsing,but also the ability of systems to correct many otherforms of grammatical errors, such as those involv-ing incorrect word order, number disagreement, andmisuse of prepositions, articles, and collocations.AcknowledgmentsWe would like to thank Melissa Lopez for help withannotating our corpora and Michael Flor for kindlydeveloping an annotation tool for our purposes.
Wealso thank Aoife Cahill, Robbie Kantor, MarkusDickinson, Michael Heilman, Nitin Madnani, andour anonymous reviewers for insightful commentsand discussion.ReferencesIn?aki Alegria, Bertol Arrieta, Arantza Diaz de Ilar-raza, Eli Izagirre, and Montse Maritxalar.
2006.Using machine learning techniques to build acomma checker for Basque.
In Proceedings of theCOLING/ACL main conference poster sessions.Timothy Baldwin and Manuel Paul Anil KumarJoseph.
2009.
Restoring punctuation and casingin English text.
In Australasian Conference onArtificial Intelligence?09.E.
Brill.
1993.
A Corpus-Based Approach to Lan-guage Learning.
Ph.D. thesis, The University ofPennsylvania, Philadelpha, PA.Ted Briscoe and John Carroll.
1995.
Developing andevaluating a probabilistic LR parser of part-of-speech and punctuation labels.
In Proceedings ofthe ACL/SIGPARSE 4th International Workshopon Parsing Technologies.Jacob Cohen.
1960.
A coefficient of agreement for292nominal scales.
Educational and PsychologicalMeasurement, 20(1):37?46.Robert J. Connors and Andrea A. Lunsford.
1988.Frequency of formal errors in current collegewriting, or Ma and Pa Kettle do research.
Col-lege Composition and Communication, 39(4).Daniel Dahlmeier and Hwee Tou Ng.
2011.
Gram-matical error correction with alternating structureoptimization.
In Proceedings of the 49th An-nual Meeting of the Association for Computa-tional Linguistics: Human Language Technolo-gies - Volume 1.
Association for ComputationalLinguistics.Robert Dale and Adam Kilgarriff.
2010.
Helpingour own: Text massaging for computational lin-guistics as a new shared task.
In InternationalConference on Natural Language Generation.Rachele De Felice and Stephen Pulman.
2008.
Aclassifier-based approach to preposition and de-terminer error correction in L2 English.
In Pro-ceedings of COLING-08.
Manchester.Markus Dickinson, Ross Israel, and Sun-Hee Lee.2011.
Developing methodology for Korean par-ticle error detection.
In Proceedings of the 6thWorkshop on Innovative Use of NLP for BuildingEducational Applications.
Portland, Oregon.Christine Doran.
1998.
Incorporating Punctuationinto the Sentence Grammar: A Lexicalized Tree-Adjoining Grammar Perspective.
Ph.D. thesis,University of Pennsylvania.Benoit Favre, Dilek Hakkani-Tur, and ElizabethShriberg.
2009.
Syntactically-informed modelsfor comma prediction.
In Proceedings of the2009 IEEE International Conference on Acous-tics, Speech and Signal Processing.Michael Gamon.
2010.
Using mostly native datato correct errors in learners?
writing: A meta-classifier approach.
In Human Language Tech-nologies: The 2010 Annual Conference of theNorth American Chapter of the Association forComputational Linguistics.Michael Gamon.
2011.
High-order sequence model-ing for language learner detection high-order se-quence modeling for language learner error de-tection.
In Proceedings of the 6th Workshop onInnovative Use of NLP for Building EducationalApplications.Agustin Gravano, Martin Jansche, and Michiel Bac-chiani.
2009.
Restoring punctuation and capi-talization in transcribed speech.
In Proceedingsof the 2009 IEEE International Conference onAcoustics, Speech and Signal Processing.Daniel Hardt.
2001.
Comma Checking in Danish.
InCorpus Linguistics.Robin L. Hill and Wayne S. Murray.
1998.
Commasand spaces: The point of punctuation.
In 11th An-nual CUNY Conference on Human Sentence Pro-cessing.Jing Huang and Geoffrey Zweig.
2002.
Maximumentropy model for punctuation annotation fromspeech.
In Proceedings of ICSLP 2002.Bernard E. M. Jones.
1994.
Exploring the role ofpunctuation in parsing natural text.
In Proceed-ings of the 15th conference on Computational lin-guistics - Volume 1.John D. Lafferty, Andrew McCallum, and FernandoC.
N. Pereira.
2001.
Conditional random fields:Probabilistic models for segmenting and labelingsequence data.
In Proceedings of the EighteenthInternational Conference on Machine Learning.Claudia Leacock, Martin Chodorow, Michael Ga-mon, and Joel R. Tetreault.
2010.
Auto-mated Grammatical Error Detection for Lan-guage Learners.
Synthesis Lectures on Hu-man Language Technologies.
Morgan & ClaypoolPublishers.Yang Liu, Elizabeth Shriberg, Andreas Stolcke, andMary Harper.
2005.
Comparing hmm, maximumentropy, and conditional random fields for disflu-ency detection.
In In Proceeedings of the Euro-pean Conference on Speech Communication andTechnology.Wei Lu and Hwee T. Ng.
2010.
Better punctua-tion prediction with dynamic conditional randomfields.
In Proceedings of the 2010 Conference onEmpirical Methods in Natural Language Process-ing.Helena Moniz, Fernando Batista, Hugo Meinedo,and Alberto Abad.
2009.
Prosodically-based au-tomatic segmentation and punctuation.
In Pro-293ceedings of the 5th International Conference onSpeech Prosody.Diane Nicholls.
1999.
The cambridge learner corpus- error coding and analysis for writing dictionariesand other books for english learners.
In SummerWorkshop on Learner Corpora.
Showa Woman?sUniversity.Hiromi Oyama.
2010.
Automatic error detectionmethod for japanese particles.
Polyglossia, 18.Adwait Ratnaparkhi.
1996.
A Maximum EntropyModel for Part-Of-Speech Tagging.
In Eric Brilland Kenneth Church, editors, Proceedings of theEmpirical Methods in Natural Language Process-ing.Alla Rozovskaya, Mark Sammons, Joshua Gioja,and Dan Roth.
2011.
University of Illinois sys-tem in HOO text correction shared task.
In Pro-ceedings of the Generation Challenges Sessionat the 13th European Workshop on Natural Lan-guage Generation, pages 263?266.
Associationfor Computational Linguistics, Nancy, France.Stuart M. Shieber and Xiaopeng Tao.
2003.
Commarestoration using constituency information.
InProceedings of the 2003 Human Language Tech-nology Conference and Conference of the NorthAmerican Chapter of the Association for Compu-tational Linguistics.Allan M. Siegal and William G. Connolly.
1999.
TheNew York Times Manual of Style and Usage : TheOfficial Style Guide Used by the Writers and Edi-tors of the World?s Most Authoritative Newspaper.Crown, rev sub edition.William Strunk and E. B.
White.
1999.
The Ele-ments of Style, Fourth Edition.
Longman, fourthedition.Joel Tetreault and Martin Chodorow.
2008.
The upsand downs of preposition error detection in ESLwriting.
In Proceedings of COLING-08.
Manch-ester.Joel Tetreault, Jennifer Foster, and MartinChodorow.
2010.
Using parse features forpreposition selection and error detection.
InProceedings of the ACL 2010 Conference ShortPapers.University of Chicago.
1993.
The Chicago Manualof Style.
University Of Chicago Press, Chicago,fourteenth edition.Michael White and Rajakrishnan Rajkumar.
2008.A more precise analysis of punctuation for broad-coverage surface realization with CCG.
In Pro-ceedings of the Workshop on Grammar Engineer-ing Across Frameworks.294
