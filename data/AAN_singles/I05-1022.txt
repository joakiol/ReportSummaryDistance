R. Dale et al (Eds.
): IJCNLP 2005, LNAI 3651, pp.
245 ?
256, 2005.?
Springer-Verlag Berlin Heidelberg 2005A Method of Recognizing Entity and RelationXinghua Fan1, 2 and Maosong Sun11State Key Laboratory of Intelligent Technology and Systems,Tsinghua University, Beijing 100084, Chinafanxh@tsinghua.org.cn, sms@mail.tsinghua.edu.cn2State Intellectual Property Office of P.R.
China, Beijing, 100088, ChinaAbstract.
The entity and relation recognition, i.e.
(1) assigning semantic classesto entities in a sentence, and (2) determining the relations held between entities,is an important task in areas such as information extraction.
Subtasks (1) and(2) are typically carried out sequentially, but this approach is problematic: theerrors made in subtask (1) are propagated to subtask (2) with an accumulativeeffect; and, the information available only in subtask (2) cannot be used in sub-task (1).
To address this problem, we propose a method that allows subtasks (1)and (2) to be associated more closely with each other.
The process is performedin three stages: firstly, employing two classifiers to do subtasks (1) and (2) in-dependently; secondly, recognizing an entity by taking all the entities and rela-tions into account, using a model called the Entity Relation Propagation Dia-gram; thirdly, recognizing a relation based on the results of the preceding stage.The experiments show that the proposed method can improve the entity and re-lation recognition in some degree.1   IntroductionThe entity and relation recognition, i.e.
assigning semantic classes (e.g., person, or-ganization and location) to entities in a sentence and determining the relations (e.g.,born-in and employee-of) that hold between entities, is an important task in areas suchas information extraction (IE) [1] [2] [3] [4], question answering (QA) [5] and storycomprehension [6].
In a QA system, many questions concern the specific entities insome relations.
For example, the question that ?Where was Poe born??
in TREC-9asks for the location entity in which Poe was born.
In a typical IE task in constructinga job database from unstructured texts, the system are required to extract many mean-ingful entities like titles and salary from the texts and to determine how these entitiesare associated with job positions.The task of recognizing entity and relation is usually treated as two separate sub-tasks carried out sequentially: (1) to recognize entities using an entity recognizer, and(2) to determine the relations held between them.
This approach has two shortcom-ings.
Firstly, the errors made in subtask (1) will be propagated to subtask (2) with anaccumulative effect, leading to a loss in performance of relation recognition.
Forexample, if ?Boston?
is mislabeled as a person, it will never have chance to be classi-fied as the location of Poe?s birthplace.
Secondly, the information available only in246 X.
Fan and M. Sunsubtask (2) cannot be used for subtask (1).
For example, if we feel difficult todetermine whether the entity X is a person or not, but we can determine that thereexists a relation born-in between X and China easily, it is obvious that we can claimthat X must be a person.To address the problems described above, this paper presents a novel approachwhich allows subtasks (1) and (2) to be linked more closely together.
The process isseparated into three stages.
Firstly, employing two classifiers to perform subtasks (1)and (2) independently.
Secondly, recognizing an entity by taking all the entities andrelations into account using a particularly designed model called the Entity RelationPropagation Diagram.
And, thirdly, recognizing a relation based on the results of thepreceding step.The rest of the paper is organized as follows.
Section 2 defines the problem of en-tity and relation recognition in a formal way.
Section 3 describes the proposed methodof recognizing entity and relation.
Section 4 gives the experimental results.
Section 5is the related work and comparison.
Section 6 is conclusions.2   The Problem of Entity and Relation RecognitionConceptually, the entities and relations in a sentence can be viewed, while takingaccount of the mutual dependencies among them, as a labeled graph in Fig.
1.Fig.
1.
Concept view of the entities and relations among themIn Fig.1, a node represents an entity and a link denotes the relation held betweentwo entities.
The arrowhead of a link represents the direction of the relation.
Eachentity or relation has several attributes, which are structured as a list of the node or theedge.
These attributes can be classified into two classes.
Some of them that are easy toacquire, such as words in an entity and parts of speech of words in a context, arecalled local attributes; the others that are difficult to acquire, such as semantic classesof phrases and relations among them, are called decision attributes.
The issue of entityand relation recognition is to determine a unique value for each decision attribute ofall entities and relations, by considering the local attributes of them.
To describe theproblem in a formal way, we first give some basic definitions as follows.A Method of Recognizing Entity and Relation 247Definition 1 (Entity).
An entity can be a single word or a set of consecutive wordswith a predefined boundary.
A sentence is a linked list, which consists of words andentities.
Entities in a sentence are denoted as E1, E2 ?
according to their order, withvalues ranging over a set of entity class CE.
For example, the sentence in Fig.
2 hasthree entities: E1= ?Dole?, E2= ?Elizabeth?
and E3= ?Salisbury, N.C.?.
Note that it isnot easy to determine the entity boundaries [7].
Here we assume that it has beensolved and its output serves as the input to our model.Fig.
2.
A sentence that have three entitiesDefinition 2 (Relation).
In this paper, we only consider the relation between twoentities.
An entity pair (Ei, Ej) represents a relation Rij from entity Ei and Ej, where Eiis the first argument and Ej is the second argument.
Relation Rij takes its value thatranges over a set of relation class CR.
Note that (Ei, Ej) is an ordered pair, and thereexist two relations Rij =(Ei, Ej) and Rji =(Ej, Ei) between entities Ei and Ej.Definition 3 (Class).
The class of an entity or relation is its decision attribute, whichis one of the predefined class set and is unknown before being recognized.
We denotethe sets of predefined entity class and relation class as CE and CR respectively.
CE hasone special element other-ent, which represents any unlisted entity class.
For algo-rithmic reasons, we suppose all elements in CE are mutually exclusive.
Similarly, CRalso has one special element other-rel, which represents that the two involved entitiesare irrelevant or their relation class is undefined.
For algorithmic reasons, we supposeall elements in CR are mutually exclusive.
In fact, because the class of an entity or arelation is only a label that we want to predict, if an entity or a relation have morethan one labels simultaneously, to satisfy the constraint that all elements in CE or CRare mutually exclusive, we can separate it into several cases and construct severalpredefined entity class sets and relation class sets.The classes of entities and relations in a sentence must satisfy some constraints.
Forexample, if the class of entity E1, which is the first argument of relation R12, is a loca-tion, then the class of relation R12 cannot be born-in because the class of the first ar-gument in relation R12 has to be a person.Definition 4 (Constraint).
A constraint is a 5-tuple ),,, R,( R21 ?????
.
The symbolsare defined as follows.
RCR ?
represents the class of relation R. E21 C, ???
repre-sents the classes of the first argument Ei and the second argument Ej in the relation Rrespectively.
]1,0[?R?
is a real number that represents a joint conditional probabilitydistribution }R|,Pr{ 21R ???
= .
]1,0[???
is a real number that represents a condi-tional probability distribution },|RPr{ 21 ????
= .
Note that R?
and ??
need not tobe specified manually and can be learned from an annotated training dataset easily.248 X.
Fan and M. SunDefinition 5 (Observation).
We denote the observations of an entity and a relation ina sentence as OE and OR respectively.
OE or OR represent all the ?known?
local attrib-utes of an entity or a relation, e.g., the spelling of a word, parts of speech, andsemantic related attributes acquired from external resources such as WordNet.
Theobservations OE and OR can be viewed as a random event, and 1}Pr{O}Pr{O RE ?=because OE and OR in a sentence are known.Based on the above definitions, the issue of entity and relation recognition can bedescribed in a formal way as follows.
Suppose in a sentence, the set of entity is {E1,E2 ?
En}, the set of relation is {R12, R21, R13, R31, ?, R1n, Rn1, ?, Rn-1,n, Rn,n-1}, thepredefined sets of entity class and relation class are CE ={e1, e2, ?
em} and CR ={ r1,r2, ?
rk} respectively, the observation of entity Ei is EiO , and the observation of rela-tion Rij is RijO .
n, m and k represent the number of entity, the number of the prede-fined entity class and the number of the predefined relation class respectively.
Theproblem is to search the most probable class assignment for each entity and eachrelation of interest, given the observations of all entities and relations.
In other words,the problem is to solve the following two equations, using two kinds of constraintknowledge ???
,R  and the interaction among entities and relations.}
O ,O ,,O ,O , ,O ,O ,O, ,O ,O|ePr{E max arge R 1-nn,R n1,-nRn1R1nR21R12EnE2E1did LLL==  (1)} O ,O ,,O ,O , ,O ,O ,O, ,O ,O|rPr{R max argr R 1-nn,R n1,-nRn1R1nR21R12EnE2E1dijd LLL==  (2)In (1), d =1, 2, ?, m, and in (2), d=1, 2, ?, k.3   The Proposed MethodBecause the class assignment of a single entity or relation depends not only on localattributes itself, but also on those of all other entities and relations, the equations (1)and equation (2) cannot be solved directly.
To simplify the problem, we present thefollowing method consisting of three stages.
Firstly, employ two classifiers to performentity recognition and relation recognition independently.
Their outputs are the condi-tional probability distributions Pr{E| OE} and Pr{R|OR}, given the correspondingobservations.
Secondly, recognize an entity by taking account of all entities and rela-tions, as computed in the previous step.
This is achieved by using the model EntityRelation Propagation Diagram (ERPD).
And, recognize a relation based on the resultsof the second step at last.In this paper, we concentrate on the processes at the second and the third stages, as-suming that the process at the first stage is solved and its output are given to us asinput.
At the second stage, the aim of introducing ERPD is to estimate the conditionalprobability distributionERPD}|Pr{E  given the constraint R?
in Definition 5 andthe sets {  }O|Pr{E Eii } and { }O|Pr{R R ijij } (i, j=1,?,n), as computed at the firststage.
For the readability, supposeERPD}|Pr{E is given, the entity recognitionequation (1) becomes the equation (3).A Method of Recognizing Entity and Relation 249??????=>==?
?RV     ERPD}|ePr{E max argRV     }O|ePr{E max argeddEddiii(3)where ?
is a threshold determined by the experiment.
RV?
[0, 1] is a real number,called the reliable value, representing the belief degree of the output of the entityrecognizer at the first stage.
Suppose the maximum value of the conditional probabil-ity distribution  }O|Pr{E E is Vm and the second value is Vs, RV is defined as:smsmVVVVRV+?=  (4)The reason of introducing RV is due to a fact that only for ambiguous entities, it iseffective by taking the classes of all entities in a sentence into account.
?ReliableValue?
measures whether an entity is ambiguous.At the third stage, the basic idea of recognizing a relation is to search the probablerelation given its observation, under a condition of satisfying the constraints imposedby the results of entity recognition at the second stage.
The relation recognition equa-tion (2) becomes the equation (5).RRkk W}O|rPr{R max argr ?==????
?=>=0}?,?|Pr{r if  00}?,?|Pr{r if   1W2121R(5)where 21,??
is the results of entity recognition at the second stage, },|Pr{r 21 ??
isconstraint knowledge ??
in Definition 4, and WR is the weight of the constraintknowledge.In the following sections, we present ERPD and two algorithms to estimate theconditional probability distributionERPD}|Pr{E .3.1   The Entity Relation Propagation DiagramTo represent the mutual dependencies among entities and relations, a model namedthe Entity Relation Propagation Diagram that can deal with cycles, similar to theCausality Diagram [8][9] for the complex system fault diagnosis, is developed forentity and relation recognition.The classes of any two entities are dependent on each other through the relationsbetween them, while taking account of the relations in between.
For example, theclass of entity Ei in Fig.
3 (a) depends on the classes of relations Rji between entitiesEi and Ej, and the classes of relations Rij and Rji depend on the classes of entitiesEi and Ej.
This means that we can predict the class of a target entity according to theclass of its neighboring entity, making use of the relations between them.
Wefurther introduce the relation reaction intensity to describe the prediction ability ofthis kind.250 X.
Fan and M. SunFig.
3.
Illustration of relation reactionDefinition 6 (Relation Reaction Intensity).
We denote the relation reaction intensityfrom entity Ei to entity Ej as Pij, which represents the ability that we guess the class ofEj if we know the class of its neighboring entity Ei and the relation Rij between them.The relation reaction intensity could be modeled using a condition probability distri-bution Pij=Pr {Ej |Ei}.The element klijp of Pij represents the conditional probability Pr {Ej=el |Ei=ek}:?==========N1t kitijljkitijkiljklij }ePr{E}rR| eE ,e}Pr{ErPr{R}eE|eEPr{paccording to Definition 5:}O|rPr{R}rPr{R Rijtijtij === , }O|ePr{E}ePr{E Eikiki ===Then, we have:}O|ePr{E}rR| eE ,e}Pr{EO|rPr{RN1tEikitijljkiRijtijklij ?=======p  (6)where Rt Cr ?
, N is the number of relations in relation class set.
In equation (6),}rR| eE ,ePr{E tijljki === represents the constraint knowledge R?
among entitiesand relations.
}O|rPr{R Rijtij =  and }O|ePr{E Eiki =  represent the outputs at thefirst stage.Definition 7 (Observation Reaction Intensity).
We denote the observation reactionintensity as the conditional probability distribution }O|Pr{E E  of an entity class,given the observation, which is the output at the first stage.The Entity Relation Propagation Diagram (ERPD).
is a directed diagram thatallows cycles.
As illustrated in Fig.
4, the symbols used in the ERPD are defined asfollows.
A circle node represents an event variable that can be any one from a set ofmutually exclusive events, which all together cover the whole sample space.
Here, anevent variable represents an entity, an event represents a predefined entity class, andthe whole sample space represents the set of predefined entity classes.
Box noderepresents a basic event which is one of the independent sources of the associatedevent variable.
Here, a basic event represents the observation of an entity.
Directedarc represents a linkage event variable that may or may not enable an input event tocause the corresponding output event.
The linkage event variable from an eventA Method of Recognizing Entity and Relation 251variable to another event variable represents the relation reaction intensity in Defini-tion 6.
And, the linkage event variable from a basic event to the corresponding eventvariable represents the observation reaction intensity in Definition 7.
All arcs pointingto a node are in a logical OR relationship.Fig.
4.
Illustration of the Entity Relation Propagation DiagramNow, we present two algorithms to compute the conditional probability distribu-tionERPD}|Pr{E , one is based on the entity relation propagation tree, and the otheris the directed iteration algorithm on ERPD.3.2   The Entity Relation Propagation TreeThe Entity Relation Propagation Tree (ERPT).
is a tree decomposed from anERPD, which represents the relation reaction propagation from all basic events toeach event variable logically.
Each event variable in the ERPD corresponds to anERPT.
For example, the ERPT of X1 in Fig.
4 is illustrated in Fig.
5.
The symbolsused in the ERPT are defined as follows.
The root of the tree, denoted as Circle, is anevent variable corresponding to the event variable in the ERPD.
A leaf of the tree,denoted as Box, is a basic event corresponding to the basic event in the ERPD.
Themiddle node of the tree, denoted as Diamond, is a logical OR gate variable, which ismade from an event variable that has been expanded in the ERPD, and, the label inDiamond corresponds to the label of the expanded event variable.
The directed arc ofthe tree corresponds to the linkage event variable in the ERPD.
All arcs pointing to anode are in a logical OR relationship.
The relation between the directed arc and thenode linked to it is in logical AND relationship.To decompose an ERPD into entity relation propagation trees, firstly we decom-pose the ERPD into mini node trees.
Each event variables in the ERPD corresponds toa mini node tree, in which the root of the mini tree is the event variable in concern atpresent, and the leaves are composed of all neighboring basic events and event vari-ables that are connected to the linkage event variables pointing to the top event vari-ables.
Secondly, expand a mini node tree into an entity relation propagation tree, i.e.,the neighboring event variables in the mini node tree are replaced with their corre-sponding mini trees.
During expanding a node event variable, when there are loops,Rule BreakLoop is applied to break down the loops.252 X.
Fan and M. SunFig.
5.
Illustration of the entity relation propagation treeRule BreakLoop.
An event variable cannot propagate the relation reaction to itself.Rule 1 is derived from a law commonsense - one can attest that he is sinless.
When sucha loop is encountered, the descendant event variable, which is same as the head eventvariable of the loop, is treated as a null event variable, together with its connected link-age event variable to be deleted.Compute the Conditional Probability Distribution in an ERPT.
After an ERPD isdecomposed into entity relation propagation trees, the conditional probability distribu-tion  ERPD}|Pr{E becomes  ERPT}|Pr{E .
When an event variable Xi has more thanone input, these inputs will be in logic OR relationship, as defined in the ERPD.
Sincethese inputs are independent, there exists such a case that one input causes Xi to be aninstance kiX  while another input causes Xi to be an instanceliX , this would be impos-sible because kiX  andliX  are exclusive.
In the real world, the mechanism, in whichiX  can response to more than one independent input properly, is very complicatedand may vary from one case to another.
To avoid this difficulty, a basic assumption isintroduced.Assumption.
When there is more than one input to Xi, each input will contribute apossibility to Xi.
For each input, its contribution to this possibility equals to the prob-ability that it causes Xi directly, as if the other inputs do not exist.
The final possibilitythat Xi occurs is the sum of the possibilities from all inputs.Suppose an event variable X has m inputs, and the probability distributions of alllinkage event variables, linked basic events or event variables are Pi and Pr {Xi} re-spectively, i=1,2?m.
Based on the above assumption, the formula for computing theprobability distribution of X can be derived as:)}Pr{X}Pr{XPNorm(}Pr{X}Pr{Xm1i ni1iin1?= ???????????=?????????
?MM  (7)A Method of Recognizing Entity and Relation 253where, Norm () is a function that normalizes the vector in {}, and n is the statenumber of X.So, the probability distribution  ERPT}|Pr{E of the variable X in the correspond-ing ERPT can be computed in the following steps.
Firstly, to find the middle nodesequence in the corresponding ERPT in the depth-first search; secondly, according tothe sequence, for each middle node, equation (7) is applied to compute its probabilitydistribution.
In this procedure, the previous results can be used for the lattercomputation.3.3   The Directed Iteration Algorithm on ERPDThe idea is to compute the probability distribution of the event variable on the ERPDdirectly, without decomposing the ERPD to some ERPTs.
The aim is to avoid thecomputational complexity of using ERPT.
This is achieved by adopting an iterationstrategy, which is the same as that used in the loopy belief network [10].The Directed Iteration Algorithm.
is as follows: Firstly, only take the basic event asinput, and initialize each event variable according to formula (7), i.e., assigning aninitialized probability distribution to each event variable.
Secondly, take the basicevent and the probability distributions of all neighboring nodes computed in the pre-vious step as input, and iterate to update the probability distributions of all nodes inERPD in parallel according to formula (7).
Thirdly, if none of the probability distribu-tion of all nodes in ERPD in successive iterations changes larger than a small thresh-old, the iteration is said to converge and then stops.4   ExperimentsDataset.
The dataset in our experiments is the same as the Roth?s dataset ?all?
[11],which consists of 245 sentences that have the relation kill, 179 sentences that have therelation born-in and 502 sentences that have no relations.
The predefined entityclasses are other-ent, person and location, and the predefined relation classes areother-rel, kill and born-in.
In fact, we use the results at the first stage in our method asthe input, which are provided by W. Yih.Experiment Design.
We compare five approaches in the experiments: Basic, Omnis-cient, ERPD, ERPD* and BN.
The Basic approach, which is a baseline, tests the per-formance of the two classifiers at the first stage, which are learned from their localattributes independently.
The Omniscient approach is similar to Basic, the only defer-ence is that the classes of entities are exposed to relation classifier and vice versa.Note that it is certainly impossible to know the true classes of an entity and a relationin advance.
The BN is the method based on the belief network, -- we follow the BNmethod according to the description in [11].
The ERPD is the proposed method basedon ERPT, and the ERPD* is the proposed method based on the directed iterationalgorithm.
The threshold of RV is 0.4.Results.
The experimental results are shown in Table 1.
It can be seen from the tablethat 1) it is very difficult to improve the entity recognition because BN and Omnis-cient almost do not improve the performance of Basic; 2) the proposed method can254 X.
Fan and M. Sunimprove the precision, which is thought of being more important than the recall forthe task of recognizing entity; 3) the relation recognition can be improved if we canimprove the entity recognition, as indicated by the comparisons of Basic, ERPD andOmniscient; 4) the proposed method can improve the relation recognition, and it per-formance is almost equal to that of BN; 5) the performance of ERPD and ERPD* isalmost equal, so the directly iteration algorithm is effective.Table 1.
Experimental results5   Related Work and ComparisonTargeting at the problems mentioned above, a method based on the belief network hasbeen presented in [11], in which two subtasks are carried out simultaneously.
Its pro-cedure is as follows: firstly, two classifiers are trained for recognizing entities andrelations independently and their outputs are treated as the conditional probabilitydistributions for each entity and relation, given the observed data; secondly, this in-formation together with the constraint knowledge among relations and entities arerepresented in a belief network [12] and are used to make global inferences for allentities and relations of interest.
This method is denoted BN in our experiments.Although BN can block the error propagation from the entity recognizer to the rela-tion classifier as well as improve the relation recognition, it cannot make use of theinformation, which is only available in relation recognition, to help entity recognition.Experiments show that BN cannot improve entity recognition.Comparing to BN, the proposed method in this paper can overcome the two short-comings of it.
Experiments show that it can not only improve the relation recognition,but also improve the precision of entity recognition.
Moreover, the model ERPDcould be more expressive enough than the belief network for the task of recognizingA Method of Recognizing Entity and Relation 255entity and relation.
It can represent the mutually dependences between entities andrelations by introducing relation reaction intensity, and can deal with a loop withoutthe limitation of directed acyclic diagram (DAG) in the belief network.
At the sametime, the proposed method can merge two kinds of constraint knowledge (i.e.???
and R  in Definition 4), but the method based on belief network can only use ??
.Finally, the proposed method has a high computation efficiency while using the di-rected iteration algorithm.6   ConclusionsThe subtasks of entity recognition and relation recognition are typically carried outsequentially.
This paper proposed an integrated approach that allows the two subtasksto be performed in a much closer way.
Experimental results show that this method canimprove the entity and relation recognition in some degree.In addition,  the Entity Relation Propagation Diagram (ERPD) is used to figure outthe dependencies among entities and relations.
It can also merge some constraintknowledge.
Regarding to ERPD, two algorithms are further designed, one is based onthe entity relation propagation tree, the other is the directed iteration algorithm onERPD.
The latter can be regarded as an approximation of the former with a highercomputational efficiency.AcknowledgementsWe would like to express our deepest gratitude to Roth D. and Yih W. for makingtheir dataset available for us.
The research is supported in part by the National 863Project of China under grant number 2001AA114210-03, the National Natural Sci-ence Foundation of China under grant number 60321002, and the Tsinghua-ALVISProject co-sponsored by the National Natural Science Foundation of China undergrant number 60520130299 and EU FP6.References1.
Chinchor, N. MUC-7 Information Extraction Task Definition.
In Proceeding of the Sev-enth Message Understanding Conference (MUC-7), Appendices, 1998.2.
Califf, M. and Mooney, R. Relational Learning of Pattern-match Rules for InformationExtraction.
In Proceedings of the Sixteenth National Conference on Artificial Intelligenceand Eleventh Conference on Innovative Applications of Artificial Intelligence, 328-334,Orlando, Florida, USA, AAAI Press, 1999.3.
Freitag, D. Machine Learning for Information Extraction in Informal Domains.
Machinelearning, 39(2/3): 169-202, 2000.4.
Roth, D. and Yih, W. Relational Learning via Prepositional Algorithms: An InformationExtraction Case Study.
In Proceedings of the Seventeenth International Joint Conference onArtificial Intelligence, 1257-1263, Seattle, Washington, USA, Morgan Kaufmann, 2001.5.
Voorhees, E. Overview of the Trec-9 Question Answering Track.
In The Ninth Text Re-trieval Conference (TREC-9), 71-80, 2000.256 X.
Fan and M. Sun6.
Hirschman, L., Light, M., Breck, E. and Burger, J.
Deep Read: A Reading ComprehensionSystem.
In Proceedings of the 37th Annual Meeting of Association for Computational Lin-guistics, 1999.7.
Abney, S.P.
Parsing by Chunks.
In S. P. Abney, R. C. Berwick, and C. Tenny, editors,Principle-based parsing: Computation and Psycholinguistics, 257-278.
Kluwer, Dordrecht,1991.8.
Xinghua Fan.
Causality Diagram Theory Research and Applying it to Fault Diagnosis ofComplexity System, Ph.D. Dissertation of Chongqing University, P.R.
China,  2002.9.
Xinghua Fan, Zhang Qin, Sun Maosong, Huang Xiyue.
Reasoning Algorithm in Multi-Valued Causality Diagram, Chinese Journal of Computers, 26(3), 310-322, 2003.10.
Murphy, K., Weiss, Y., and Jordan, M. Loopy Belief Propagation for Approximate Infer-ence: An empirical study.
In Proceeding of Uncertainty in AI, 467-475, 1999.11.
Roth, D. and Yih, W. Probability Reasoning for Entity & Relation Recognition.
In Pro-ceedings of 20th International Conference on Computational Linguistics (COLING-02),835-841, 2002.12.
Pearl, J. Probability Reasoning in Intelligence Systems.
Morgan Kaufmann, 1988.
