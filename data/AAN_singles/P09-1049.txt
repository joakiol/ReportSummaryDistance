Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 432?440,Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLPBilingual Co-Training for Monolingual Hyponymy-Relation AcquisitionJong-Hoon Oh, Kiyotaka Uchimoto, and Kentaro TorisawaLanguage Infrastructure Group, MASTAR Project,National Institute of Information and Communications Technology (NICT)3-5 Hikaridai Seika-cho, Soraku-gun, Kyoto 619-0289 Japan{rovellia,uchimoto,torisawa}@nict.go.jpAbstractThis paper proposes a novel frameworkcalled bilingual co-training for a large-scale, accurate acquisition method formonolingual semantic knowledge.
Inthis framework, we combine the indepen-dent processes of monolingual semantic-knowledge acquisition for two languagesusing bilingual resources to boost perfor-mance.
We apply this framework to large-scale hyponymy-relation acquisition fromWikipedia.
Experimental results showthat our approach improved the F-measureby 3.6?10.3%.
We also show that bilin-gual co-training enables us to build classi-fiers for two languages in tandem with thesame combined amount of data as requiredfor training a single classifier in isolationwhile achieving superior performance.1 MotivationAcquiring and accumulating semantic knowledgeare crucial steps for developing high-level NLPapplications such as question answering, althoughit remains difficult to acquire a large amount ofhighly accurate semantic knowledge.
This pa-per proposes a novel framework for a large-scale,accurate acquisition method for monolingual se-mantic knowledge, especially for semantic rela-tions between nominals such as hyponymy andmeronymy.
We call the framework bilingual co-training.The acquisition of semantic relations betweennominals can be seen as a classification task of se-mantic relations ?
to determine whether two nom-inals hold a particular semantic relation (Girju etal., 2007).
Supervised learning methods, whichhave often been applied to this classification task,have shown promising results.
In those methods,however, a large amount of training data is usuallyrequired to obtain high performance, and the highcosts of preparing training data have always beena bottleneck.Our research on bilingual co-training sprangfrom a very simple idea: perhaps training data in alanguage can be enlarged without much cost if wetranslate training data in another language and addthe translation to the training data in the originallanguage.
We also noticed that it may be possi-ble to further enlarge the training data by trans-lating the reliable part of the classification resultsin another language.
Since the learning settings(feature sets, feature values, training data, corpora,and so on) are usually different in two languages,the reliable part in one language may be over-lapped by an unreliable part in another language.Adding the translated part of the classification re-sults to the training data will improve the classifi-cation results in the unreliable part.
This processcan also be repeated by swapping the languages,as illustrated in Figure 1.
Actually, this is nothingother than a bilingual version of co-training (Blumand Mitchell, 1998).Language 1 Language 2IterationManually PreparedTraining Datafor Language 1Classifier ClassifierTraining TrainingEnlargedTraining Datafor Language 1EnlargedTraining Datafor Language 2Manually PreparedTraining Datafor Language 2ClassifierClassifierFurther EnlargedTraining Datafor Language 1Further EnlargedTraining Datafor Language 2Translatereliable parts ofclassificationresultsTrainingTraining TrainingTraining?..
?..Translatereliable parts ofclassificationresultsFigure 1: Concept of bilingual co-trainingLet us show an example in our current task:hyponymy-relation acquisition from Wikipedia.Our original approach for this task was super-432vised learning based on the approach proposed bySumida et al (2008), which was only applied forJapanese and achieved around 80% in F-measure.In their approach, a common substring in a hyper-nym and a hyponym is assumed to be one strongclue for recognizing that the two words constitutea hyponymy relation.
For example, recognizing aproper hyponymy relation between two Japanesewords, (kouso meaning enzyme) and (kasuibunkaikouso meaning hydrolase), isrelatively easy because they share a common suf-fix: kouso.
On the other hand, judging whethertheir English translations (enzyme and hydrolase)have a hyponymy relation is probably more dif-ficult since they do not share any substrings.
Aclassifier for Japanese will regard the hyponymyrelation as valid with high confidence, while aclassifier for English may not be so positive.
Inthis case, we can compensate for the weak part ofthe English classifier by adding the English trans-lation of the Japanese hyponymy relation, whichwas recognized with high confidence, to the En-glish training data.In addition, if we repeat this process by swap-ping English and Japanese, further improvementmay be possible.
Furthermore, the reliable partsthat are automatically produced by a classifier canbe larger than manually tailored training data.
Ifthis is the case, the effect of adding the transla-tion to the training data can be quite large, and thesame level of effect may not be achievable by areasonable amount of labor for preparing the train-ing data.
This is the whole idea.Through a series of experiments, this papershows that the above idea is valid at least for onetask: large-scale monolingual hyponymy-relationacquisition from English and Japanese Wikipedia.Experimental results showed that our methodbased on bilingual co-training improved the per-formance of monolingual hyponymy-relation ac-quisition about 3.6?10.3% in the F-measure.Bilingual co-training also enables us to build clas-sifiers for two languages in tandem with the samecombined amount of data as would be requiredfor training a single classifier in isolation whileachieving superior performance.People probably expect that a key factor in thesuccess of this bilingual co-training is how totranslate the training data.
We actually did transla-tion by a simple look-up procedure in the existingtranslation dictionaries without any machine trans-lation systems or disambiguation processes.
De-spite this simple approach, we obtained consistentimprovement in our task using various translationdictionaries.This paper is organized as follows.
Section 2presents bilingual co-training, and Section 3 pre-cisely describes our system.
Section 4 describesour experiments and presents results.
Section 5discusses related work.
Conclusions are drawnand future work is mentioned in Section 6.2 Bilingual Co-TrainingLet S and T be two different languages, and letCL be a set of class labels to be obtained as a re-sult of learning/classification.
To simplify the dis-cussion, we assume that a class label is binary; i.e.,the classification results are ?yes?
or ?no.?
Thus,CL = {yes, no}.
Also, we denote the set of allnonnegative real numbers by R+.Assume X = XS ?
XT is a set of instances inlanguages S and T to be classified.
In the con-text of a hyponymy-relation acquisition task, theinstances are pairs of nominals.
Then we assumethat classifier c assigns class label cl in CL andconfidence value r for assigning the label, i.e.,c(x) = (x, cl, r), where x ?
X , cl ?
CL, andr ?
R+.
Note that we used support vector ma-chines (SVMs) in our experiments and (the abso-lute value of) the distance between a sample andthe hyperplane determined by the SVMs was usedas confidence value r. The training data are de-noted by L ?
X?CL, and we denote the learningby function LEARN ; if classifier c is trained bytraining data L, then c = LEARN(L).
Particu-larly, we denote the training sets for S and T thatare manually prepared by LS and LT , respectively.Also, bilingual instance dictionary DBI is definedas the translation pairs of instances in XS and XT .Thus, DBI = {(s, t)} ?
XS ?
XT .
In the caseof hyponymy-relation acquisition in English andJapanese, (s, t) ?
DBI could be (s=(enzyme, hy-drolase), t=( (meaning enzyme), (meaning hydrolase))).Our bilingual co-training is given in Figure 2.
Inthe initial stage, c0S and c0T are learned with manu-ally labeled instances LS and LT (lines 2?5).
ThenciS and ciT are applied to classify instances in XSand XT (lines 6?7).
Denote CRiS as a set of theclassification results of ciS on instances XS that isnot in LiS and is registered in DBI .
Lines 10?18describe a way of selecting from CRiS newly la-4331: i = 02: L0S = LS ; L0T = LT3: repeat4: ciS := LEARN(LiS)5: ciT := LEARN(LiT )6: CRiS := {ciS(xS)|xS ?
XS ,?cl (xS , cl) /?
LiS , ?xT (xS , xT ) ?
DBI}7: CRiT := {ciT (xT )|xT ?
XT ,?cl (xT , cl) /?
LiT , ?xS (xS , xT ) ?
DBI}8: L(i+1)S := LiS9: L(i+1)T := LiT10: for each (xS , clS , rS) ?
TopN(CRiS) do11: for each xT such that (xS , xT ) ?
DBIand (xT , clT , rT ) ?
CRiT do12: if rS > ?
then13: if rT < ?
or clS = clT then14: L(i+1)T := L(i+1)T ?
{(xT , clS)}15: end if16: end if17: end for18: end for19: for each (xT , clT , rT ) ?
TopN(CRiT ) do20: for each xS such that (xS , xT ) ?
DBIand (xS , clS , rS) ?
CRiS do21: if rT > ?
then22: if rS < ?
or clS = clT then23: L(i+1)S := L(i+1)S ?
{(xS , clT )}24: end if25: end if26: end for27: end for28: i = i + 129: until a fixed number of iterations is reachedFigure 2: Pseudo-code of bilingual co-trainingbeled instances to be added to a new training setin T .
TopN(CRiS) is a set of ciS(x), whose rSis top-N highest in CRiS .
(In our experiments,N = 900.)
During the selection, ciS acts as ateacher and ciT as a student.
The teacher instructshis student in the class label of xT , which is actu-ally a translation of xS by bilingual instance dic-tionary DBI , through clS only if he can do it witha certain level of confidence, say rS > ?, andif one of two other condition meets (rT < ?
orclS = clT ).
clS = clT is a condition to avoidproblems, especially when the student also has acertain level of confidence in his opinion on a classlabel but disagrees with the teacher: rT > ?
andclS 6= clT .
In that case, the teacher does nothingand ignores the instance.
Condition rT < ?
en-ables the teacher to instruct his student in the classlabel of xT in spite of their disagreement in a classlabel.
If every condition is satisfied, (xT , clS) isadded to existing labeled instances L(i+1)T .
Theroles are reversed in lines 19?27 so that ciT be-comes a teacher and ciS a student.Similar to co-training (Blum and Mitchell,1998), one classifier seeks another?s opinion to se-lect new labeled instances.
One main differencebetween co-training and bilingual co-training isthe space of instances: co-training is based on dif-ferent features of the same instances, and bilin-gual co-training is based on different spaces of in-stances divided by languages.
Since some of theinstances in different spaces are connected by abilingual instance dictionary, they seem to be inthe same space.
Another big difference lies inthe role of the two classifiers.
The two classifiersin co-training work on the same task, but thosein bilingual co-training do the same type of taskrather than the same task.3 Acquisition of Hyponymy Relationsfrom WikipediaOur system, which acquires hyponymy relationsfrom Wikipedia based on bilingual co-training,is described in Figure 3.
The following threemain parts are described in this section: candidateextraction, hyponymy-relation classification, andbilingual instance dictionary construction.Classifier in E Classifier in JLabeledinstancesLabeledinstancesWikipediaArticles in EWikipediaArticles in JCandidatesin JCandidatesin EAcquisition oftranslation dictionaryBilingual Co-TrainingUnlabeledinstances in JUnlabeledinstances in EBilingual instance dictionaryNewly labeledinstances for ENewly labeledinstances for JTranslationdictionaryHyponymy-relationcandidate extractionHyponymy-relationcandidate extractionFigure 3: System architecture3.1 Candidate ExtractionWe follow Sumida et al (2008) to extracthyponymy-relation candidates from English andJapanese Wikipedia.
A layout structure is chosen434(a) Layout structureof article TIGERRangeSiberian tigerBengal tigerSubspeciesTaxonomyTigerMalayan tiger(b) Tree structure ofFigure 4(a)Figure 4: Wikipedia article and its layout structureas a source of hyponymy relations because it canprovide a huge amount of them (Sumida et al,2008; Sumida and Torisawa, 2008)1, and recog-nition of the layout structure is easy regardless oflanguages.
Every English and Japanese Wikipediaarticle was transformed into a tree structure likeFigure 4, where layout items title, (sub)sectionheadings, and list items in an article were usedas nodes in a tree structure.
Sumida et al (2008)found that some pairs consisting of a node and oneof its descendants constituted a proper hyponymyrelation (e.g., (TIGER, SIBERIAN TIGER)), andthis could be a knowledge source of hyponymyrelation acquisition.
A hyponymy-relation candi-date is then extracted from the tree structure by re-garding a node as a hypernym candidate and allits subordinate nodes as hyponym candidates ofthe hypernym candidate (e.g., (TIGER, TAXON-OMY) and (TIGER, SIBERIAN TIGER) from Fig-ure 4).
39 M English hyponymy-relation candi-dates and 10 M Japanese ones were extracted fromWikipedia.
These candidates are classified intoproper hyponymy relations and others by using theclassifiers described below.3.2 Hyponymy-Relation ClassificationWe use SVMs (Vapnik, 1995) as classifiers forthe classification of the hyponymy relations on thehyponymy-relation candidates.
Let hyper be a hy-pernym candidate, hypo be a hyper?s hyponymcandidate, and (hyper, hypo) be a hyponymy-relation candidate.
The lexical, structure-based,and infobox-based features of (hyper, hypo) in Ta-ble 1 are used for building English and Japaneseclassifiers.
Note that SF3?SF5and IF were not1Sumida et al (2008) reported that they obtained 171 K,420 K, and 1.48 M hyponymy relations from a definition sen-tence, a category system, and a layout structure in JapaneseWikipedia, respectively.used in Sumida et al (2008) but LF1?LF5andSF1?SF2are the same as their feature set.Let us provide an overview of the featuresets used in Sumida et al (2008).
See Sum-ida et al (2008) for more details.
Lexical fea-tures LF1?LF5are used to recognize the lexi-cal evidence encoded in hyper and hypo for hy-ponymy relations.
For example, (hyper,hypo) isoften a proper hyponymy relation if hyper andhypo share the same head morpheme or word.In LF1and LF2, such information is providedalong with the words/morphemes and the parts ofspeech of hyper and hypo, which can be multi-word/morpheme nouns.
TagChunk (Daume?
III etal., 2005) for English and MeCab (MeCab, 2008)for Japanese were used to provide the lexical fea-tures.
Several simple lexical patterns2 were alsoapplied to hyponymy-relation candidates.
For ex-ample, ?List of artists?
is converted into ?artists?by lexical pattern ?list of X.?
Hyponymy-relationcandidates whose hypernym candidate matchessuch a lexical pattern are likely to be valid (e.g.,(List of artists, Leonardo da Vinci)).
We use LF4for dealing with these cases.
If a typical or fre-quently used section heading in a Wikipedia arti-cle, such as ?History?
or ?References,?
is used asa hyponym candidate in a hyponymy-relation can-didate, the hyponymy-relation candidate is usuallynot a hyponymy relation.
LF5is used to recognizethese hyponymy-relation candidates.Structure-based features are related to thetree structure of Wikipedia articles from whichhyponymy-relation candidate (hyper,hypo) is ex-tracted.
SF1provides the distance between hyperand hypo in the tree structure.
SF2represents thetype of layout items from which hyper and hypoare originated.
These are the feature sets used inSumida et al (2008).We also added some new items to the abovefeature sets.
SF3represents the types of treenodes including root, leaf, and others.
For exam-ple, (hyper,hypo) is seldom a hyponymy relationif hyper is from a root node (or title) and hypois from a hyper?s child node (or section head-ings).
SF4and SF5represent the structural con-texts of hyper and hypo in a tree structure.
Theycan provide evidence related to similar hyponymy-relation candidates in the structural contexts.An infobox-based feature, IF , is based on a2We used the same Japanese lexical patterns in Sumida etal.
(2008) to build English lexical patterns with them.435Type Description ExampleLF1Morphemes/words hyper: tiger?, hypo: Siberian, hypo: tiger?LF2POS of morphemes/words hyper: NN?, hypo: NP, hypo: NN?LF3hyper and hypo, themselves hyper: Tiger, hypo: Siberian tigerLF4Used lexical patterns hyper: ?List of X?, hypo: ?Notable X?LF5Typical section headings hyper: History, hypo: ReferenceSF1Distance between hyper and hypo 3SF2Type of layout items hyper: title, hypo: bulleted listSF3Type of tree nodes hyper: root node, hypo: leaf nodeSF4LF1and LF3of hypo?s parent node LF3:SubspeciesSF5LF1and LF3of hyper?s child node LF3: TaxonomyIF Semantic properties of hyper and hypo hyper: (taxobox,species), hypo: (taxobox,name)Table 1: Feature type and its value.
?
in LF1and LF2represent the head morpheme/word and its POS.Except those in LF4and LF5, examples are derived from (TIGER, SIBERIAN TIGER) in Figure 4.Wikipedia infobox, a special kind of template, thatdescribes a tabular summary of an article subjectexpressed by attribute-value pairs.
An attributetype coupled with the infobox name to which itbelongs provides the semantic properties of itsvalue that enable us to easily understand whatthe attribute value means (Auer and Lehmann,2007; Wu and Weld, 2007).
For example, in-fobox template City Japan in Wikipedia articleKyoto contains several attribute-value pairs suchas ?Mayor=Daisaku Kadokawa?
as attribute=itsvalue.
What Daisaku Kadokawa, the attributevalue of mayor in the example, represents is hardto understand alone if we lack knowledge, butits attribute type, mayor, gives a clue?DaisakuKadokawa is a mayor related to Kyoto.
Thesesemantic properties enable us to discover seman-tic evidence for hyponymy relations.
We ex-tract triples (infobox name, attribute type, attributevalue) from the Wikipedia infoboxes and encodesuch information related to hyper and hypo in ourfeature set IF .33.3 Bilingual Instance DictionaryConstructionMultilingual versions of Wikipedia articles areconnected by cross-language links and usuallyhave titles that are bilinguals of each other (Erd-mann et al, 2008).
English and Japanese articlesconnected by a cross-language link are extractedfrom Wikipedia, and their titles are regarded astranslation pairs4.
The translation pairs between3We obtained 1.6 M object-attribute-value triples inJapanese and 5.9 M in English.4197 K translation pairs were extracted.English and Japanese terms are used for buildingbilingual instance dictionary DBI for hyponymy-relation acquisition, where DBI is composed oftranslation pairs between English and Japanesehyponymy-relation candidates5.4 ExperimentsWe used the MAY 2008 version of EnglishWikipedia and the JUNE 2008 version ofJapanese Wikipedia for our experiments.
24,000hyponymy-relation candidates, randomly selectedin both languages, were manually checked to buildtraining, development, and test sets6.
Around8,000 hyponymy relations were found in the man-ually checked data for both languages7.
20,000 ofthe manually checked data were used as a train-ing set for training the initial classifier.
The restwere equally divided into development and testsets.
The development set was used to select theoptimal parameters in bilingual co-training and thetest set was used to evaluate our system.We used TinySVM (TinySVM, 2002) with apolynomial kernel of degree 2 as a classifier.
Themaximum iteration number in the bilingual co-training was set as 100.
Two parameters, ?
andTopN , were selected through experiments on thedevelopment set.
?
= 1 and TopN=900 showed5We also used redirection links in English and JapaneseWikipedia for recognizing the variations of terms when webuilt a bilingual instance dictionary with Wikipedia cross-language links.6It took about two or three months to check them in eachlanguage.7Regarding a hyponymy relation as a positive sample andthe others as a negative sample for training SVMs, ?positivesample:negative sample?
was about 8,000:16,000=1:2436the best performance and were used as the optimalparameter in the following experiments.We conducted three experiments to show ef-fects of bilingual co-training, training data size,and bilingual instance dictionaries.
In the first twoexperiments, we experimented with a bilingual in-stance dictionary derived from Wikipedia cross-language links.
Comparison among systems basedon three different bilingual instance dictionaries isshown in the third experiment.Precision (P ), recall (R), and F1-measure (F1),as in Eq (1), were used as the evaluation measures,where Rel represents a set of manually checkedhyponymy relations and HRbyS represents a setof hyponymy-relation candidates classified as hy-ponymy relations by the system:P = |Rel ?
HRbyS|/|HRbyS| (1)R = |Rel ?
HRbyS|/|Rel|F1= 2 ?
(P ?
R)/(P + R)4.1 Effect of Bilingual Co-TrainingENGLISH JAPANESEP R F1P R F1SYT 78.5 63.8 70.4 75.0 77.4 76.1INIT 77.9 67.4 72.2 74.5 78.5 76.6TRAN 76.8 70.3 73.4 76.7 79.3 78.0BICO 78.0 83.7 80.7 78.3 85.2 81.6Table 2: Performance of different systems (%)Table 2 shows the comparison results of the foursystems.
SYT represents the Sumida et al (2008)system that we implemented and tested with thesame data as ours.
INIT is a system based on ini-tial classifier c0 in bilingual co-training.
We trans-lated training data in one language by using ourbilingual instance dictionary and added the trans-lation to the existing training data in the otherlanguage like bilingual co-training did.
The sizeof the English and Japanese training data reached20,729 and 20,486.
We trained initial classifier c0with the new training data.
TRAN is a systembased on the classifier.
BICO is a system basedon bilingual co-training.For Japanese, SYT showed worse performancethan that reported in Sumida et al (2008), proba-bly due to the difference in training data size (oursis 20,000 and Sumida et al (2008) was 29,900).The size of the test data was also different ?
oursis 2,000 and Sumida et al (2008) was 1,000.Comparison between INIT and SYT shows theeffect of SF3?SF5and IF , newly introducedfeature types, in hyponymy-relation classification.INIT consistently outperformed SYT, although thedifference was merely around 0.5?1.8% in F1.BICO showed significant performance im-provement (around 3.6?10.3% in F1) over SYT,INIT, and TRAN regardless of the language.
Com-parison between TRAN and BICO showed thatbilingual co-training is useful for enlarging thetraining data and that the performance gain bybilingual co-training cannot be achieved by sim-ply translating the existing training data.817977757360 55 50 45 40 35 30 25 20F 1Training Data (103)EnglishJapaneseFigure 5: F1curves based on the increase of train-ing data size during bilingual co-trainingFigure 5 shows F1curves based on the sizeof the training data including those manually tai-lored and automatically obtained through bilin-gual co-training.
The curve starts from 20,000 andends around 55,000 in Japanese and 62,000 in En-glish.
As the training data size increases, the F1curves tend to go upward in both languages.
Thisindicates that the two classifiers cooperate wellto boost their performance through bilingual co-training.We recognized 5.4 M English and 2.41 MJapanese hyponymy relations from the classifi-cation results of BICO on all hyponymy-relationcandidates in both languages.4.2 Effect of Training Data SizeWe performed two tests to investigate the effect ofthe training data size on bilingual co-training.
Thefirst test posed the following question: ?If we build2n training samples by hand and the building costis the same in both languages, which is better fromthe monolingual aspects: 2n monolingual trainingsamples or n bilingual training samples??
Table 3and Figure 6 show the results.437In INIT-E and INIT-J, a classifier in each lan-guage, which was trained with 2n monolingualtraining samples, did not learn through bilingualco-training.
In BICO-E and BICO-J, bilingual co-training was applied to the initial classifiers trainedwith n training samples in both languages.
Asshown in Table 3, BICO, with half the size of thetraining samples used in INIT, always performedbetter than INIT in both languages.
This indicatesthat bilingual co-training enables us to build clas-sifiers for two languages in tandem with the samecombined amount of data as required for traininga single classifier in isolation while achieving su-perior performance.81797775737169676520000 15000 10000 7500 5000 2500F 1Training Data SizeINIT-EINIT-JBICO-EBICO-JFigure 6: F1based on training data size:with/without bilingual co-trainingn2n nINIT-E INIT-J BICO-E BICO-J2500 67.3 72.3 70.5 73.05000 69.2 74.3 74.6 76.910000 72.2 76.6 76.9 78.6Table 3: F1based on training data size:with/without bilingual co-training (%)The second test asked: ?Can we always im-prove performance through bilingual co-trainingwith one strong and one weak classifier??
If theanswer is yes, then we can apply our frameworkto acquisition of hyponymy-relations in other lan-guages, i.e., German and French, without mucheffort for preparing a large amount of trainingdata, because our strong classifier in English orJapanese can boost the performance of a weakclassifier in other languages.To answer the question, we tested the perfor-mance of classifiers by using all training data(20,000) for a strong classifier and by changing thetraining data size of the other from 1,000 to 15,000({1,000, 5,000, 10,000, 15,000}) for a weak clas-sifier.INIT-E BICO-E INIT-J BICO-J1,000 72.2 79.6 64.0 72.75,000 72.2 79.6 73.1 75.310,000 72.2 79.8 74.3 79.015,000 72.2 80.4 77.0 80.1Table 4: F1based on training data size: when En-glish classifier is strong oneINIT-E BICO-E INIT-J BICO-J1,000 60.3 69.7 76.6 79.35,000 67.3 74.6 76.6 79.610,000 69.2 77.7 76.6 80.115,000 71.0 79.3 76.6 80.6Table 5: F1based on training data size: whenJapanese classifier is strong oneTables 4 and 5 show the results, where ?INIT?represents a system based on the initial classifierin each language and ?BICO?
represents a sys-tem based on bilingual co-training.
The resultswere encouraging because the classifiers showedbetter performance than their initial ones in everysetting.
In other words, a strong classifier alwaystaught a weak classifier well, and the strong onealso got help from the weak one, regardless of thesize of the training data with which the weaker onelearned.
The test showed that bilingual co-trainingcan work well if we have one strong classifier.4.3 Effect of Bilingual Instance DictionariesWe tested our method with different bilingual in-stance dictionaries to investigate their effect.
Webuilt bilingual instance dictionaries based on dif-ferent translation dictionaries whose translationentries came from different domains (i.e., gen-eral domain, technical domain, and Wikipedia)and had a different degree of translation ambigu-ity.
In Table 6, D1 and D2 correspond to sys-tems based on a bilingual instance dictionary de-rived from two handcrafted translation dictionar-ies, EDICT (Breen, 2008) (a general-domain dic-tionary) and ?The Japan Science and TechnologyAgency Dictionary,?
(a translation dictionary fortechnical terms) respectively.
D3, which is thesame as BICO in Table 2, is based on a bilingual438instance dictionary derived from Wikipedia.
EN-TRY represents the number of translation dictio-nary entries used for building a bilingual instancedictionary.
E2J (or J2E) represents the averagetranslation ambiguities of English (or Japanese)terms in the entries.
To show the effect of thesetranslation ambiguities, we used each dictionaryunder two different conditions, ?=5 and ALL.
?=5represents the condition where only translation en-tries with less than five translation ambiguities areused; ALL represents no restriction on translationambiguities.DIC F1DIC STATISTICSTYPE E J ENTRY E2J J2ED1 ?=5 76.5 78.4 588K 1.80 1.77D1 ALL 75.0 77.2 990K 7.17 2.52D2 ?=5 76.9 78.5 667K 1.89 1.55D2 ALL 77.0 77.9 750K 3.05 1.71D3 ?=5 80.7 81.6 197K 1.03 1.02D3 ALL 80.7 81.6 197K 1.03 1.02Table 6: Effect of different bilingual instance dic-tionariesThe results showed that D3 was the best andthat the performances of the others were sim-ilar to each other.
The differences in the F1scores between ?=5 and ALL were relatively smallwithin the same system triggered by translationambiguities.
The performance gap between D3and the other systems might explain the fact thatboth hyponymy-relation candidates and the trans-lation dictionary used in D3 were extracted fromthe same dataset (i.e., Wikipedia), and thus thebilingual instance dictionary built with the trans-lation dictionary in D3 had better coverage ofthe Wikipedia entries consisting of hyponymy-relation candidates than the other bilingual in-stance dictionaries.
Although D1 and D2 showedlower performance than D3, the experimental re-sults showed that bilingual co-training was alwayseffective no matter which dictionary was used(Note that F1of INIT in Table 2 was 72.2 in En-glish and 76.6 in Japanese.
)5 Related WorkLi and Li (2002) proposed bilingual bootstrappingfor word translation disambiguation.
Similar tobilingual co-training, classifiers for two languagescooperated in learning with bilingual resources inbilingual bootstrapping.
However, the two clas-sifiers in bilingual bootstrapping were for a bilin-gual task but did different tasks from the monolin-gual viewpoint.
A classifier in each language is forword sense disambiguation, where a class label (orword sense) is different based on the languages.On the contrary, classifiers in bilingual co-trainingcooperate in doing the same type of tasks.Bilingual resources have been used for mono-lingual tasks including verb classification andnoun phrase semantic interpolation (Merlo et al,2002; Girju, 2006).
However, unlike ours, their fo-cus was limited to bilingual features for one mono-lingual classifier based on supervised learning.Recently, there has been increased interest in se-mantic relation acquisition from corpora.
Someregarded Wikipedia as the corpora and appliedhand-crafted or machine-learned rules to acquiresemantic relations (Herbelot and Copestake, 2006;Kazama and Torisawa, 2007; Ruiz-casado et al,2005; Nastase and Strube, 2008; Sumida et al,2008; Suchanek et al, 2007).
Several researcherswho participated in SemEval-07 (Girju et al,2007) proposed methods for the classification ofsemantic relations between simple nominals inEnglish sentences.
However, the previous workseldom considered the bilingual aspect of seman-tic relations in the acquisition of monolingual se-mantic relations.6 ConclusionWe proposed a bilingual co-training approach andapplied it to hyponymy-relation acquisition fromWikipedia.
Experiments showed that bilingualco-training is effective for improving the perfor-mance of classifiers in both languages.
We fur-ther showed that bilingual co-training enables usto build classifiers for two languages in tandem,outperforming classifiers trained individually foreach language while requiring no more trainingdata in total than a single classifier trained in iso-lation.We showed that bilingual co-training is alsohelpful for boosting the performance of a weakclassifier in one language with the help of a strongclassifier in the other language without loweringthe performance of either classifier.
This indicatesthat the framework can reduce the cost of prepar-ing training data in new languages with the help ofour English and Japanese strong classifiers.
Ourfuture work focuses on this issue.439ReferencesSo?ren Auer and Jens Lehmann.
2007.
What haveInnsbruck and Leipzig in common?
Extracting se-mantics from wiki content.
In Proc.
of the 4thEuropean Semantic Web Conference (ESWC 2007),pages 503?517.
Springer.Avrim Blum and Tom Mitchell.
1998.
Combining la-beled and unlabeled data with co-training.
In COLT?98: Proceedings of the eleventh annual conferenceon Computational learning theory, pages 92?100.Jim Breen.
2008.
EDICT Japanese/English dictionaryfile, The Electronic Dictionary Research and Devel-opment Group, Monash University.Hal Daume?
III, John Langford, and Daniel Marcu.2005.
Search-based structured prediction as classi-fication.
In Proc.
of NIPS Workshop on Advances inStructured Learning for Text and Speech Processing,Whistler, Canada.Maike Erdmann, Kotaro Nakayama, Takahiro Hara,and Shojiro Nishio.
2008.
A bilingual dictionaryextracted from the Wikipedia link structure.
In Proc.of DASFAA, pages 686?689.Roxana Girju, Preslav Nakov, Vivi Nastase, Stan Sz-pakowicz, Peter Turney, and Deniz Yuret.
2007.Semeval-2007 task 04: Classification of semantic re-lations between nominals.
In Proc.
of the FourthInternational Workshop on Semantic Evaluations(SemEval-2007), pages 13?18.Roxana Girju.
2006.
Out-of-context noun phrase se-mantic interpretation with cross-linguistic evidence.In CIKM ?06: Proceedings of the 15th ACM inter-national conference on Information and knowledgemanagement, pages 268?276.Aurelie Herbelot and Ann Copestake.
2006.
Acquir-ing ontological relationships from Wikipedia usingRMRS.
In Proc.
of the ISWC 2006 Workshop onWeb Content Mining with Human Language Tech-nologies.Jun?ichi Kazama and Kentaro Torisawa.
2007.
Ex-ploiting Wikipedia as external knowledge for namedentity recognition.
In Proc.
of Joint Conference onEmpirical Methods in Natural Language Process-ing and Computational Natural Language Learning,pages 698?707.Cong Li and Hang Li.
2002.
Word translation disam-biguation using bilingual bootstrapping.
In Proc.
ofthe 40th Annual Meeting of the Association for Com-putational Linguistics, pages 343?351.MeCab.
2008.
MeCab: Yet another part-of-speechand morphological analyzer.
http://mecab.sourceforge.net/.Paola Merlo, Suzanne Stevenson, Vivian Tsang, andGianluca Allaria.
2002.
A multilingual paradigmfor automatic verb classification.
In Proc.
of the40th Annual Meeting of the Association for Compu-tational Linguistics, pages 207?214.Vivi Nastase and Michael Strube.
2008.
DecodingWikipedia categories for knowledge acquisition.
InProc.
of AAAI 08, pages 1219?1224.Maria Ruiz-casado, Enrique Alfonseca, and PabloCastells.
2005.
Automatic extraction of semanticrelationships for Wordnet by means of pattern learn-ing from Wikipedia.
In Proc.
of NLDB, pages 67?79.
Springer Verlag.Fabian M. Suchanek, Gjergji Kasneci, and GerhardWeikum.
2007.
Yago: A Core of Semantic Knowl-edge.
In Proc.
of the 16th international conferenceon World Wide Web, pages 697?706.Asuka Sumida and Kentaro Torisawa.
2008.
Hack-ing Wikipedia for hyponymy relation acquisition.
InProc.
of the Third International Joint Conferenceon Natural Language Processing (IJCNLP), pages883?888, January.Asuka Sumida, Naoki Yoshinaga, and Kentaro Tori-sawa.
2008.
Boosting precision and recall of hy-ponymy relation acquisition from hierarchical lay-outs in Wikipedia.
In Proceedings of the 6th In-ternational Conference on Language Resources andEvaluation.TinySVM.
2002. http://chasen.org/?taku/software/TinySVM.Vladimir N. Vapnik.
1995.
The nature of statisticallearning theory.
Springer-Verlag New York, Inc.,New York, NY, USA.Fei Wu and Daniel S. Weld.
2007.
Autonomously se-mantifying Wikipedia.
In CIKM ?07: Proceedingsof the sixteenth ACM conference on Conference oninformation and knowledge management, pages 41?50.440
