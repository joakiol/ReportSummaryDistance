Proceedings of the 14th European Workshop on Natural Language Generation, pages 125?135,Sofia, Bulgaria, August 8-9 2013. c?2013 Association for Computational LinguisticsDeconstructing Human Literature Reviews ?A Framework for  Multi-Document Summar izationKokil Jaidka, Chr istopher  S.G. Khoo, J in-Cheon NaDivision of Information StudiesWee Kim Wee School of Communication and InformationNanyang Technological University, Singapore[kokil, chriskhoo]@pmail.ntu.edu.sg, tjcna@ntu.edu.sgAbstractThis study is conducted in the area of multi-document summarization, and develops aliterature review framework based on adeconstruction of human-written literaturereview sections in information science researchpapers.
The first part of the study presents theresults of a multi-level discourse analysis toinvestigate their discourse and contentcharacteristics.
These findings wereincorporated into a framework for literaturereviews, focusing on their macro-leveldocument structure and the sentence-leveltemplates, as well as the informationsummarization strategies.
The second part ofthis study discusses insights from this analysis,and how the framework can be adapted toautomatic summaries resembling human writtenliterature reviews.
Summaries generated from apartial implementation are evaluated againsthuman written summaries and assessors?comments are discussed to formulaterecommendations for future work.1 IntroductionThis project proposes a framework for literaturereviews, which has applications in automaticsummarization of scientific papers.
A literaturereview is the traditional multi-document summaryof research papers which is constructed by aresearcher to survey previous findings and itsstructure follows certain linguistic rules.
Severalstudies have identified that literature reviews areused to achieve distinct rhetorical purposes (Hart,1998; Bourner, 1996; Boot & Beile, 2005; Jonsson,2006; Massey, 2006; Torraco, 2005; Hinchliffe,2003; Bruce, 1994), such as to:?
Compare and contrast previous research.?
Identify gaps in the literature?
Identify new research questions?
Define the proposed research contributions?
Build the justification for the current work?
Situate the work in the research literature?
Reinterpret and critique previous resultsThese rhetorical characteristics of literaturereviews make it a challenging research problem inautomatic multi-document summarization ?
notonly should the summarizer identify salientinformation, but it should also synthesize thesummary in a way that achieves certainargumentative purposes.
The problem ofsummarization in context was first identified bySparck Jones and Endres-Niggemeyer (1995) andsubsequently in Sparck Jones?
follow-up article(2007), wherein they questioned the usefulness ofstate-of-the-art summarization methods inaddressing users?
information needs.
As articulatedby Sparck Jones (2007) and echoed by Nenkovaand McKeown (2011), summarization needs to beviewed as a part of the larger discourse (academicwriting) it belongs to, tailored to the purpose(literature review) of summarization, the reader (inthis case, a researcher) and the genre beingsummarized (research papers).
Motivated by thisresearch gap, we outline the aims of our analyses:?
To identify how to emulate the purpose ofliterature reviews, we conducted adiscourse analysis to identify the macro-level structure and the sentence-levellinguistic expressions embedded inliterature review sections.?
To identify the relationship betweenresearch paper and literature review, weconducted an information analysis toidentify rules for selecting and125transforming information from researchpapers.The focus of the paper is to draw insights from theframework to propose strategies for automaticliterature review generation.
An automaticsummary fashioned as a literature review canfunction as a tool to help literature review writersby pointing out ways in which information in thesource papers can be compared and integrated.
Forinformation searchers, it can provide acustomisable overview of a set of retrieval resultsthat is more readable and more logical than a list ofsalient sentences.2 Previous WorkThis paper investigates the human summarizationprocess through an extensive discourse analysis.Human summarization is a process comprisingdocument exploration to investigate the documentmacrostructure, relevance assessment byconstructing a mental representation and summaryproduction by selecting and transforming text fromthe source(s) (Endres-Niggemeyer, Maier, andSigel, 1995).
The underlying principle is the theoryof human synthesis of information, by Van Dijkand Kintsch (1983).This study proposes a linguistically motivatedframework for summarization.
In previous work, asummarization framework developed by Marcu(2000) compressed information from general textsby identifying rhetorical relationships betweenclauses and sentences, and extracting sentencenuclei.
Shiyan, Khoo & Goh (2008) summarizedsocial science dissertation abstracts by referencinga social science taxonomy to identify importantinformation and a specially constructed knowledgebank to identify important inter-relationships.
Inearlier work, a summarization framework designedby Teufel and Moens (2002) identified 7 categoriesof scientific arguments and extracted single-document summaries from chemistry andcomputational linguistics papers (Teufel,Siddharthan & Batchelor, 2009) based on user?squeries.
However, it required large corpora ofmanually annotated papers to be applied to anyfield, and it generated only single-documentsummaries.Some other scientific summarization systemsaim to model information relationships accuratelywithout concerning themselves with summarystructure.
Centrifuser, a framework forsummarizing medical literature (Elhadad, Kan,Klavans and McKeown, 2005) produced a multi-document, query-focused indicative summaryhighlighting the similarities and differencesbetween source documents.
The topic tree for thefinal summary was constructed offline byclustering a large number of documents, thus itwas not suitable for real-time user queries.
In arelated recent approach, Hoang and Kan (2010)presented preliminary results from automaticallygenerating related work sections for a target paperby taking a hierarchical topic tree as an input;however, the requirement of a pre-conceived topictree limits the scalability of this system.
To sumup, these scientific summarization systems aretypically delimited by their scalability andgeneralizability for multiple documents anddomains.Newer approaches in scientific papersummarization rely on preselected informationcited in other papers to judge whether informationis influential or not, and generate a multi-documentsummary of a topic (Nanba, Kando & Okumura,2011) or a single document summary for a paperusing its relevant cited information (Qazvinian &Radev, 2008).
A system for generating literaturesurveys through citations was proposed byMohammad et al(2009) which applied superficialanalysis of research paper citation sentences tosuggest model sentences; the present studydescribes parallel efforts to refine a summarizationframework after extensive discourse analysis.
Weconsider providing not just a synopsis ofinformation, but also integrating the synopsis withthe contextual and rhetorical features which makea human written literature review a coherent,cohesive and useful reference.
Our study thusaddresses a different, and more challenging, set ofobjectives than the citation-based summarizers ofrecent work.3 Developing the Literature ReviewFrameworkFollowing the first research aim, we carried out ananalysis of the discourse structure of a sample of30 literature review sections in research papershaphazardly selected from the Journal of theAmerican Society for Information Science andTechnology between the years 2000-2008, 2 or 3126from each year.
On average, a literature reviewsection was 1146 words in length and it cited 17studies.
The texts were analyzed at 3 levels ofdetail:?
Macro-level document structure: to identifythe different sections of the literature, thetypes of information they contain and howthey are organized hierarchically.?
Sentence-level rhetorical structure: toidentify how sentences are framed accordingto the overall purpose of the literaturereview.?
Summarization strategies: to identify howinformation was selected and synthesizedfor the literature review.Preliminary findings of these discourse analyseshave been discussed in previous work by theauthors, notably, in a discussion of the features ofthe macro-structure of information scienceliterature reviews (Khoo, Na & Jaidka, 2011),rhetorical functions found in literature reviews(Jaidka, Khoo & Na, 2010) and associationsbetween sections in source papers and their citingsentences in literature reviews (Jaidka, Khoo &Na, 2013).
The current study applies the discoursecharacteristics thus identified to develop and test aliterature review framework for multi-documentsummarization.3.1 Designing Document Structure TemplatesAs noted in academic writing textbooks (Hart,1998), literature reviews are structured as ahierarchy of topics and each ?paragraph?
fulfillscertain functions.
To identify these macro-structures and their functions, we conducted thisdiscourse analysis, proceeding with the assumptionthat a literature is structured as a set of topicelements, with each topic having a set ofembedded study elements (i.e.
descriptions ofresearch studies relevant to the topic).
Anexploratory study was conducted to identify thestructures within these topics and their hierarchicalrelationships.
Two Research Assistants holdinggraduate degrees annotated every sentence withone or more of the following tags:?
title tag: to provide a statement of the topictheme or study objective?
description tag: to encapsulate the details ofthe topic or study?
meta-summary tag: to provide the writers?comments as an overview summary of theresearch in the field?
meta-critique tag: to contain the writers?critique or interpretation of cited studies,critical comparison of research orjustification for the current study?
current-study tag: to refers to and comparewith the current work being described in thepaper.?
method and result tags: to provide adescription of the research methods andresearch results reported in the cited papers.In this coding scheme, the meta-summary andmeta-critique tags provide the writers?
comments,citing one or more studies together.
The rest of theelements comprise descriptive text aboutindividual studies.
The average inter-coderreliability score (Cohen?s Kappa) obtained washigh at 0.76.
Disagreements between the coderswere resolved through discussion until a mutuallyagreeable solution was reached.
The analysisidentified different types of literature reviews aswell as different structures.
In our literature reviewframework, these findings suggested rules forgenerating different types of literature reviews:?
Integrative literature reviews shouldcomprise a large proportion of meta-summary and meta-critique elements.
Thisis because they discuss and critique ideasfrom a number of studies in a high-levelsummary.?
Descriptive literature reviews shouldreport the results of individual studies indetail, outlining their methodology andrecommendations.
This is because theywere found to comprise significantly morestudy elements.?
Integrative literature reviews should beorganized as a hierarchical structure withembedded topics.
Comparatively,descriptive literature reviews should beorganized as a flat structure, with manymore topic elements per text but lessembedded topics.
This is because127integrative literature reviews were found tocomprise an average of 2.5 embeddedtopics, and descriptive literature reviewshad an average of 1.4 embedded sub-topics.These rules have been applied in designing severalintegrative and descriptive literature reviewtemplates.
Fig 1 illustrates one of the templateintegrative literature reviews we designed.
Itcomprises a level 1 starting topic which acts as theoverall topic of the literature review.
The topic hasother sub-topic elements within it, each of whichbegins with a meta-summary element whichintroduces it, followed by study elements toillustrate it.
The topic elements determine thelogical organization of the literature review; meta-summary are incorporated into the structurebecause they provide research overviews andhighlight the similarities across related papers.
Thestudy elements highlight the unique features   forindividual papers.
These templates will beinstantiated in the automatic literature reviewgeneration process.Figure 1.
A template document structure in theliterature review framework3.2 Designing Sentence TemplatesPrevious studies of literature reviews (Bunton,2002; Kwan, 2006) have highlighted the broadrhetorical ?moves?
which organize the text, butnone have attempted to identify their linguisticstructure or specific functions.
In the clause-levelanalysis, we annotated linguistic expressionsframing research descriptions, defined as discoursemarkers by Hyland (2004).
Although discoursemarkers include generic logical connectives suchas ?so?, ?therefore?
and ?because?, we followedTeufel?s criteria (Teufel, 1999 pp.
76) to focus ononly those discourse markers which are used inscientific discourse to perform one of the functionslisted below:?
Describe a topic: Present a broad overviewof research (e.g., ?Previous researchfocused on?)
or its context (e.g., ?Researchin the area of?)?
Describe a study: Cite an author (e.g., ?Ina study by?)
or describe research processes(e.g., ?X identified?
?, ?Y has conductedan experiment to??)?
Compare studies: Highlight similarities ordifferences in research (e.g., ?Severalstudies have applied?).?
Provide additional information: Frameexamples or enumerate research studies(e.g., ?For example?, ?A list includes?
).It was found that a total of 110 expressions wereused in 1298 variations to frame different types ofinformation in different ways and achieve differentrhetorical functions.
We have applied thesefindings in the literature review framework todevelop sentence templates for text generation, andto formulate rules for selecting templates which aresignificantly associated with the type of literaturereview and discourse element to be populated:?
In integrative literature reviews: apply regularexpressions which describe research objectivesin the description elements.
In the meta-summary elements in integrative literaturereviews, apply expressions which ?state thecommon aims?.?
In descriptive literature reviews: chooseexpressions which ?state the research method?and ?state the common approaches?
in thedescription and meta-summary elements.Regular expressions are applied for text-to-textgeneration, serving as a means to extractinformation from source papers as well as to mapthem into appropriate sentence templates.
Thoseapplied to extract and instantiate research objectivesentences within topics, studies and comparisonsare illustrated in Table 1.3.3 Designing Information Selection andSummar ization StrategiesIn accordance with the second research aim, weconducted a content analysis to identify therelationship between the source papers and thefinal literature review.
Similar work describing textediting strategies has been done by Jing andSTUDY STUDY META-SUMMARYTOPICTOPICTOPIC META-SUMMARY128McKeown (1999); however, in this analysis weextend their objectives to additionally identify:?
The source sections of the paper fromwhere information was selected (i.e.,Abstract Introduction, Methodology,Results or Conclusion).?
The types of transformations used toconvert the source sentence to thereferencing sentence (i.e., copy-paste,paraphrase, or higher-level summary).?
Identifying the types of informationselected from the source papers (i.e.,objective, methodology, results and criticalsummary).?
Analysis of the reasons for preference ofone source sentence over another, despiteproviding similar information.
This wasinferred by comparing candidate sourcesentences against each other.The corpus for analysis was constructed byanalyzing the 20 literature reviews line-by-line andretaining all the sentences referencing previouswork, either explicitly (e.g., ?X and Y (1998)conducted experiments in transitive translation?)
orimplicitly by adding onto the details of a citedstudy (e.g., ?Studies have also focused on users'mental models of information seeking (X, 1989)?.A total of 349 references were collected fromthe twenty literature review sections.
Sentenceproviding definitions, or citing sources other thanresearch papers, were further discarded becausethey lay outside the scope of our analysis.
Thefindings, revealed that more than a quarter of allselected information was from the Abstract of thesource paper.
The information selected by thereviewer is copy-pasted more often in descriptiveas compared to integrative literature reviews.
Someof these findings have been applied to suggeststrategies for information selection andsummarization in the literature review framework:?
For research objective information:choose sentences from the Abstract andIntroduction of source papers; copy-pasteit into descriptive literature reviews, butparaphrase it in integrative literaturereviews.?
In descriptive literature reviews: providedetailed method information, copy-pastedfrom the Introduction or Method of sourcepapers.?
In integrative literature reviews: providedetailed result information, summarized ata higher level from the Results andConclusions.When more than one sentence provides the samefactual information, the sentence selection criterialisted in Table 2 should be followed to choose themore concise alternative.FunctionType of InformationRequiredRegular  Expression which map into Sentence TemplatesDescribea topicIntroduce a topic through itsresearch aspectsIntroduce a topic through itsliterature reviewIntroduce area of research(Researchers | Research) (have |has) (in | are concerned with |have addressed |proposed | observed | investigated | focused on)The (literature review | prior work) (covered | dealt with | lookedat | focused on )?research | studies | findings) in the (field | area | domain |context) ofDescribea studyState the study objectiveState the study motivationState the study hypothesis(the study | we | who) (conducted |explored | proposed | pursued| described | attempted to | represented | analyzed | examined |investigated |deals with | seeks to discover)(The | Their) underlying research (question | objective) (was |is)(They) (argue | opine | hold |debate | believe) thatComparestudiesState the common aim ofstudiesThe (common)?
(issue | motivation |aim | principle) (for |behind) (many | most| some| these| such | existing) studies(Many| Most |These | Some | Such | Existing | Various)?
(studies| work) have (explored | focused on)Table 1.
Regular expressions obtained from clause-level analysis129Type of Cr iter ia Order  of Pr ior ityLexical?
?This article/paper...??
?The aim/goal/objective is???
?We present/ describe...??
?Recent research into...??
Sentences with how/what/why questionsSyntactic?
Sentence having the main topic in its main clause?
The sentence with fewer clauses?
The sentence with no back-referencingSurface?
Sentence from the first paragraphs of a section?
The title of the source paper?
The sentence which is the shortestTable 2.
Criteria for selecting sentences4 EvaluationTo evaluate the framework, the objective was tocompare its ?human-ness?
represented by itsComprehensibility, Readability and Usefulnessagainst human-written literature reviews andmachine-generated sentence extracts.
For thispurpose, the framework was partially adapted in asummarization method focusing on comparingresearch objective information extracted fromAbstracts and Introduction sections, and presentinga topical overview resembling a three-levelliterature review.
The output generated is similar tothe summaries generated by Centrifuser (Elhadadet al 2005) ?
sentences are extracted to provide asynopsis of similarities and unique features ofstudies are highlighted for individual papers;however our prototype does so without rely onexternal domain knowledge.
The method wasimplemented in Java on the Eclipse IDE, and itcomprised three stages:?
Text pre-processing: to extract sentencesfrom the Abstract and Introduction of theinput source papers.
Here the text issegmented, tokenized, parsed, stop-wordsare filtered and n-grams of noun phrasesare created to represent concepts in thesource papers.?
Information selection and integration: toidentify similarities and differences acrossthe research objective sentences of sourcepapers.
It selects important concepts basedon the document frequency of lexicalconcept chains (Barzilay and McKeown,2005), and applies the research objectivesentence selection rules developed in theframework to select important informationfor summarization.?
Text presentation: to produce text that hasthe characteristics of the literature review.It applies the document structure describedin the framework, to organize the literaturereview, and sentence templates particularto research objective information inintegrative literature reviews (the oneslisted in Table 1).The resultant summaries resemble a human writtenliterature review because they are laid out as atopic tree and present a comparative overview ofsimilarities and unique features.
However, somegrammatical errors can be spotted, which wouldneed a post-processing module to remove.30 sets of information science source paperswere prepared by sampling topics from 30literature reviews from 2000-2008 issues ofJASIST, Journal of Documentation and Journal ofInformation Science and downloading the papersthey cited.
Only 3-10 source papers weredownloaded for every sampled topic; this was sothat the task could be manageable for theresearchers constructing the human summaries.
Anexcerpt system summary is provided in Table 3.For each input set of related research papers,three types of summaries were generated, eachwith a different kind of method ?
framework-basedstructure (by our method), sentence-extractionstructure (by the baseline, MEAD) and a human-written summary by a researcher:?
MEAD: The MEAD summarizationsystem (Radev, Jing, Stys, & Tam, 2004)was the baseline; it followed a sentence-130extraction approach to generate multi-document extracts of information(generally news articles).?
System: Our system based on theframework, and focusing on thesimilarities and differences betweenresearch objectives at the lexical andsyntactic level.?
Human: Five researchers from the Schoolof Humanities and Social Sciences of ouruniversity summarized the researchobjective sentences from set of sourcepapers in the context of a given (main)topic.This literature review presents research inrelevance published by Barry (1994), Harter (1992),Tang and Solomon (1998), Vakkari and Hakala(2000) and Wang and Soergel (1998).Studies by Barry (1994) and Tang et al(1998)focus on retrieval mechanism.Researchers in relevance have also consideredusers (Harter, 1992; Vakkari et al 2000; Wang etal., 1998).The study by Vakkari et al(2000) demonstrates thatit is productive to study relevance as a task andprocess-oriented user construct.Studies by Wang et al(1998) and Tang et al(1998)focus on dynamic models.The study by Tang et al(1998) is a step in theempirical exploration of the evolutionary nature ofrelevance judgments.Table 3: Excerpt from a system summaryIn the human summaries, the coders selected anaverage of 3 sub-topics and 8 unique sub-topics intheir summaries.
Human summaries also had thehighest compression rate of 18%, as compared to acompression rate of 25% by MEAD and ourSystem.
An inter-coder agreement was conductedover 10 summaries by taking the summaries doneby one of the post-graduate researchers asreference and comparing each pair of summaries,considering each of the ?similarities?
or?differences?
as a ?common?
or ?unique?
sub-topic.
Comparisons revealed that the codersusually had the same idea of what constituted animportant ?similarity?
or common sub-topic(percent agreement= 70%) though they often chosedifferent ?differences?
or unique sub-topics in theirsummaries (percent agreement= 56%).Content evaluation of the 30 sets of summariesby the ROUGE-1 metric (Lin & Hovy, 2003)revealed that system summaries had a higher butnot significantly different effectiveness or f-measure of 0.38 as compared to the baseline(0.33).
We developed our own version of ROUGEto measure information overlap by comparing theinformation concepts extracted from summaries.
Itwas different from the standard ROUGE-1 in threeways: it filtered out ?research stopwords?
such as?method?, ?experiment?
and ?study?, which didn?trepresent research information; it aggregatedwords which shared the same lemma; and it alsoconflated co-occurring adjacent words into thesame information concepts.
Consequently, weobtained real scores of effectiveness in terms ofhigher f-measure scores for both the system andthe baseline.
The system?s f-measure (0.57) was asignificant improvement over the baseline (0.50) atthe 0.01 level.
The results are provided in Table 4.For the quality evaluation, 90 questionnaireswere prepared from the 30 sets of summaries,using permutations of presentation orders toaccount for carry-over effects during assessment.To recruit assessors, a call for participation in theevaluation was broadcast over the internet, throughpostings in discussion boards, personal emails andlibrary sciences mailing lists.
The invitation wasalso personally extended to authors of otherpublications in JASIST, JDoc and JIS.
Theinvitation for participation was restricted to onlyLibrary and Information Science and ComputerScience researchers and PhD students who hadpassed their qualifying exam.
It was anticipatedthat such assessors would be more familiar withthe topics in the summary, and would be able tomake meaningful comments about the summariesand their characteristics, such as lack of evidentcomparisons and generalizations, or incorrectcomparisons and generalizations among unlikeinformation.
There were a total number of 35assessors with a mean research experience of 6years, who provided 67 responses, by filling out 1or 2 each, over a period of two months.
Theassessors were from reputable internationaluniversities in different countries.
The highestdegrees held by the assessors varied fromBachelors (for PhD students who had passed theirqualifying exam) to PhD.
They scored the131summaries on their Comprehensibility, Readabilityand Usefulness and also provided qualitativecomments to the following questions:?
What did you like about this summary??
What did you find confusing about thissummary??
How is this summary, a good/bad literaturereview?The quantitative results in Table 5 show that theSystem summary was significantly more readableand more useful than the baseline at the 0.05 level.The qualitative results (provided in Table 6) areequally interesting and show that researchers withdifferent number of years of research liked ordisliked different things about the Systemsummary.
Researchers with 0-4 years ofexperience did not have any specific preference ofone type of summary over another.
Researcherswith 5-8 years of experience were more consciousof grammatical errors and repetition mistakes inthe system summary.
Researchers with 9-12 yearsof experience ignored the grammatical errors inHuman summaries and System and insteadcriticized their lack of detail.
Researchers with 13years or experience or more were sensitive to theoverall ?context?
and ?flow?
of the summary.
Mostof the assessors were able to identify the maintopic and its related sub-topics; however, theyexperienced the System as being more disjointed,lacking ?focus?
as compared to the Humansummaries.
On the whole, researchers weresatisfied with the overview provided as well as thehierarchical organization.
It would be interesting tosee whether these findings and differences wouldbe replicated in a larger study.Measures System MEADRecall 0.70 0.63Precision 0.49 0.44F-measure 0.57 0.50Table 4.
Results from the content evaluation(N=30)MEAD System HumanComprehensibility 5.6 5.6 6.2Readability 4.9 5.3 5.6Usefulness 5.7 6.4 6.3Table 5.
Results from the quality evaluation(N=67)5 Conclusion and Future WorkThis study has analyzed how authors selectinformation, transform it and organize it in adefinite discourse structure as a literature review.Our findings identified two styles of literaturereviews ?
the integrative and descriptive literaturereviews, with different profiles of discourseelements and rhetorical expressions.
Integrativeliterature reviews present information from severalstudies in a condensed form as a critical summary,possibly complemented with a comparison,evaluation or comment on the research gap.
Thefocus is on highlighting relationships amongstconcepts or comparing studies against each other.Descriptive reviews present experimental detailabout previous studies, such as the approachfollowed, their results and evaluation.
The focus ison providing important details of previous studiesin a concise form.From these findings, we conjecture that authorsbegin a literature review with an overall strategy inmind.
They select and edit the information contentbased on the style of literature review.
They maychoose to write an integrative style of literaturereview to guide the reader along a critical surveyof previous research.
To support their argument,they paraphrase information selected from theAbstract and Conclusion sections, and integrateinformation from the Results sections into a high-level overview of important findings.
Accordingly,they choose the discourse structure and linguisticexpressions to frame their argument.Our framework has since been validated on alarger sample size of 90 articles selected from 3top journals in information science.
It isrecommended for application in a completeautomatic literature review generation system,wherein a user would be able to control the style ofliterature review, the level of detail and analysisrequired, as well as the structure of the layout andthe number of topics.
At the information selectionstage, it would be able to apply differentinformation selection and transformation strategiesto generate different parts of a literature review.
Atthe text generation stage, it would be able tointroduce a topic and describe its context and coreconcepts, describe a study and its objectives,methods and findings, delineate a research gap andidentify the common and different features amongstudies, and illustrate its argument with examples.132Year   0-4 Year   5-8 Year   9-12 Year   13+Comprehensibility- It gives a goodoverview on thetopic and points- I liked the structure.-  It summarizes theresearch and connectsthe authors to the topicby the use of "theseauthors.
"- It's not too short nortoo long.- Easy to read andunderstand.-  It is better reviewthan the othersbecause it tries to tiethe literature togetherin some fashion.- There seemed to be no reasonfor the ordering of thesentences about the differentresearch papers- Each individual statement inthe summary seems relevant(of some objective value) byitself, but all together lacksuniformity in subject.- However it does seem to getthe core issues.Readability- Continuity- Yet, thelinking ofsentence couldbe better.- Too manyrepetitions, butgives someinformation- This summary isneither readable norinformative.- The same studies arecited several times- It kept repeating allthe studies.- It felt very disjointed,maybe because of allthe small paragraphs.- Badly written, hardto read.- It flows well- Has some sentencesseemingly unrelatedto neighboringsentences- Generally easy to read.- There are a few mistakes ingrammar, which is distracting.- Very readable.- Like:  seems to have a bit offlow.Usefulness- This summaryseems quitegood- I feel I got anoverview overthe research inthe area.- The summarycovered a gooddeal of literature- The overviewis nice but stillreally flat.- This is the bestsummary of thesample.- Comprehensivelycovers the text- The summaryprovides informationabout groups of studiesresearching certaintopics- This summaryprovides an overviewof research in websearch with moreinformative details- Comparisonbetween studies ishelpful.- More info requiredabout study,including methods,findings.- It would be prettyuseful for lit review.- While comparisonsof different papersare well done, itwould also be usefulto have moredescription of eachstudy.- Should give an indication ofthese trends in order to helpthe reader contextualize theresearch field.- There is an attempt atrelating studies to each otherso that one gets an overview ofthe research area.Table 6.
Comments on System by assessors with different years of research experienceReferencesBarzilay, R., & McKeown, K. R. (2005).
Sentencefusion for multidocument news summarization.Computational Linguistics, 31(3), 297-328.Boote, D. N., & Beile, P. (2005).
Scholars beforeresearchers: On the centrality of the dissertationliterature review in research preparation.Educational researcher, 34(6), 3-15.Bourner, T. (1996).
The research process: four steps tosuccess.
Research methods: guidance forpostgraduates, Arnold, London, 7-11.133Bruce, C. S. (1994).
Research students' earlyexperiences of the dissertation literature review.Studies in Higher Education, 19(2), 217-229.Bunton, D. (2002) Generic moves in Ph.D Introductionchapters.
In J. Flowerdew (Ed.
), AcademicDiscourse.
London: Longman.Cooper, H. M. (1988).
The structure of knowledgesynthesis.
Knowledge in Society, 1, 104-126.Hoang, C., & Kan, M.Y.
2010.
Towards automatedrelated work summarization.
In Proceedings of the23rd International Conference on ComputationalLinguistics (COLING?10): Posters (pp.
427?435).DUC.
(2002).
The Document UnderstandingConference.
Retrieved Oct 2010, fromhttp://duc.nist.gov.Elhadad, N., Kan, M. Y., Klavans, J. L., & McKeown,K.
R. (2005).
Customization in a unified frameworkfor summarizing medical literature.ArtificialIntelligence in Medicine, 33(2), 179.Endres-Niggemeyer, B., Maier, E., & Sigel, A.
(1995).How to implement a naturalistic model ofabstracting: four core working steps of an expertabstractor.
Information Processing & Management,31(5), 631-674.Guo, Q., & Li, C. (2007, August).
The Research on theApplication of Text Clustering and NaturalLanguage Understanding in Automatic Abstracting.In Fuzzy Systems and Knowledge Discovery, 2007.FSKD 2007.
Fourth International Conference on(Vol.
4, pp.
92-96).
IEEE.Hart, C. (1998).
Doing a literature review.
London:Sage.Hinchliffe, L. (2003).
Having your say in a scholarlyway.
Research Strategies, 19, 163?
164.Hyland, K. (2004).
Disciplinary interactions:Metadiscourse in L2 postgraduate writing.
Journalof Second Language Writing, 13(2), 133-151.Jing, H., & McKeown, K. R. (1999).
The decompositionof human-written summary sentences.
InProceedings of the 22nd annual international ACMSIGIR conference on Research and development ininformation retrieval (pp.
129-136).
ACM.Jaidka, K., Khoo, C., and Na, J.-C. (2010).
ImitatingHuman Literature Review Writing: An Approach toMulti-Document Summarization.
In Proceedings ofthe International Conference on Asian DigitalLibraries (ICADL) (pp.
116-119).
Australia:Springer-Verlag.Jaidka, K., Khoo, C., & Na, J. C. (2013).
LiteratureReview Writing: How Information is Selected andTransformed.
Aslib Proceedings, 65(3), 303-325.Khoo, C., Na, J. C., & Jaidka, K. (2011).
Analysis of themacro-level discourse structure of literature reviews.Online Information Review, 35(2), 255-271.Kwan, B. S. (2006).
The schematic structure ofliterature reviews in doctoral theses of appliedlinguistics.
English for Specific Purposes, 25(1), 30-55.Lin, C. Y., & Hovy, E. (2003, May).
Automaticevaluation of summaries using n-gram co-occurrence statistics.
In Proceedings of the 2003Conference of the North American Chapter of theAssociation for Computational Linguistics onHuman Language Technology-Volume 1 (pp.
71-78).Association for Computational Linguistics.Marcu, D. (1997, July).
From discourse structures totext summaries.
In Proceedings of the ACL (Vol.
97,pp.
82-88).Nenkova, A., & McKeown, K. (2011).
Automaticsummarization.
Now Publishers Inc.Nanba, H., Kando, N., & Okumura, M. (2011).Classification of research papers using citation linksand citation types: Towards automatic review articlegeneration.
Advances in Classification ResearchOnline, 11(1), 117-134.Ou, S., Khoo, C. S. G., & Goh, D. H. (2008).
Designand development of a concept-based multi-documentsummarization system for research abstracts.Journal of information science, 34(3), 308-326.Radev, D. R., Jing, H., Sty?, M., & Tam, D. (2004).Centroid-based summarization of multipledocuments.
Information Processing & Management,40(6), 919-938.Saggion, H., & Lapalme, G. (2002).
Generatingindicative-informative summaries with sumum.Computational linguistics, 28(4), 497-526.Mohammad, S., Dorr, B., Egan, M., Ahmed, H.,Muthukrishan, P., Qazvinian, V., Radev, D., Zajic,D.
(2009).
Using citations to generate surveys ofscientific paradigms.
In Proceedings of HumanLanguage Technologies: The 2009 AnnualConference of the North American Chapter of theAssociation for Computational Linguistics (pp.
584?592).
Association for Computational Linguistics.Sparck Jones, K., & Endres-Niggemeyer, B.
(1995).Automatic summarizing.
Information Processing &Management, 31(5), 625-630.134Sparck Jones, K. (2007).
Automatic summarising: Thestate of the art.
Information Processing &Management, 43(6), 1449-1481.Teufel, S. (1999).
Argumentative zoning: Informationextraction from scientific text (Doctoral dissertation,University of Edinburgh).Teufel, S., & Moens, M. (2002).
Summarizing scientificarticles: experiments with relevance and rhetoricalstatus.
Computational linguistics, 28(4), 409-445.Teufel, S., Siddharthan, A., & Batchelor, C. (2009,August).
Towards discipline-independentargumentative zoning: Evidence from chemistry andcomputational linguistics.
In Proceedings of the2009 Conference on Empirical Methods in NaturalLanguage Processing: Volume 3 (pp.
1493-1502).Association for Computational Linguistics.Torraco, R. J.
(2005).
Writing integrative literaturereviews: Guidelines and examples.
Human ResourceDevelopment Review, 4(3), 356-367.Van Dijk, T. A., & Kintsch, W. (1983).
Strategies ofdiscourse comprehension.
New York: AcademicPress.135
