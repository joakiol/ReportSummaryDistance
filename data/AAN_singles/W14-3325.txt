Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 215?220,Baltimore, Maryland USA, June 26?27, 2014.c?2014 Association for Computational LinguisticsDCU-Lingo24 Participation in WMT 2014 Hindi-English Translation taskXiaofeng Wu, Rejwanul Haque*, Tsuyoshi OkitaPiyush Arora, Andy Way, Qun LiuCNGL, Centre for Global Intelligent ContentSchool of Computing, Dublin City UniversityDublin 9, Ireland{xf.wu,tokita,parora,away,qliu}@computing.dcu.ie*Lingo24, Edinburgh, UKrejwanul.haque@lingo24.comAbstractThis paper describes the DCU-Lingo24submission to WMT 2014 for the Hindi-English translation task.
We exploitmiscellaneous methods in our system,including: Context-Informed PB-SMT,OOV Word Conversion (OWC), Multi-Alignment Combination (MAC), Oper-ation Sequence Model (OSM), Stem-ming Align and Normal Phrase Extraction(SANPE), and Language Model Interpola-tion (LMI).
We also describe various pre-processing steps we tried for Hindi in thistask.1 IntroductionThis paper describes the DCU-Lingo24 submis-sion to WMT 2014 for the Hindi-English transla-tion task.All our experiments on WMT 2014 are builtupon the Moses phrase-based model (PB-SMT)(Koehn et al., 2007) and tuned with MERT(Och, 2003).
Starting from this baseline system,we exploit various methods including Context-Informed PB-SMT (CIPBSMT), zero-shot learn-ing (Palatucci et al., 2009) using neural network-based language modelling (Bengio et al., 2000;Mikolov et al., 2013) for OOV word conversion,various lexical reordering models (Axelrod et al.,2005; Galley and Manning, 2008), various Mul-tiple Alignment Combination (MAC) (Tu et al.,2012), Operation Sequence Model (OSM) (Dur-rani et al., 2011) and Language Model Interpola-tion(LMI).In the next section, the preprocessing steps areexplained.
In Section 3 a detailed explanation ofthe technique we exploit is provided.
Then in Sec-tion 4, we provide our experimental results and re-sultant discussion.2 Pre-processing StepsWe use all the training data provided for Hindi?English translation.
Following Bojar et al.
(2010),we apply a number of normalisation methods onthe Hindi corpus.
The HindEnCorp parallel cor-pus compiles several sources of parallel data.
Weobserve that the source-side (Hindi) of the TIDESdata source contains font-related noise, i.e.
manyHindi sentences are a mixture of two different en-codings: UTF-81and WX2notations.
We pre-pared a WX-to-UTF-8 font conversion script forHindi which converts all WX encoded charactersinto UTF-8, thus removing all WX encoding ap-pearing in the TIDES data.We also observe that a portion of the Englishtraining corpus contained the following bracket-like sequences of characters: -LRB-, -LSB-, -LCB-, -RRB-, -RSB-, and -RCB-.3For consis-tency, those character sequences in the trainingdata were replaced by the corresponding brackets.For English ?
both monolingual and the targetside of the bilingual data ?
we perform tokeniza-tion, normalization of punctuation, and truecasing.For parallel training data, we filter sentences pairscontaining more than 80 tokens on either side and1http://en.wikipedia.org/wiki/UTF-82http://en.wikipedia.org/wiki/WX_notation3The acronyms stand for (Left|Right)(Round|Square|Curly) Bracket.215sentence pairs with length difference larger than 3times.3 Techniques Deployed3.1 Combination of Various LexicalReordering Model (LRM)Clearly, Hindi and English have quite differentword orders, so we adopt three lexical reorderingmodels to address this problem.
They are word-based LRM and phrase-based LRM, which mainlyfocus on local reordering phenomena, and hierar-chical phrase-based LRM, which mainly focuseson longer distance reordering (Galley and Man-ning, 2008).3.2 Operation Sequence ModelThe Operation Sequence Model (OSM) of Dur-rani et al.
(2011) defines four translation opera-tions: Generate(X,Y), Continue Source Concept,Generate Source Only (X) and Generate Identical,as well as three reordering operations: Insert Gap,Jump Back(W) and Jump Forward.The probability of an operation sequence O =(o1o2?
?
?
oJ) is calculated as in (1):p(O) =J?j=1p(oj|oj?n+1?
?
?
oj?1) (1)where n indicates the number of previous opera-tions used.We employ a 9-order OSM in our framework.3.3 Language Model Interpolation (LMI)We build a large language model by including datafrom the English Gigaword fifth edition, the En-glish side of the UN corpus, the English side of the109French?English corpus and the English side ofthe Hindi?English parallel data provided by the or-ganisers.
We interpolate language models trainedusing each dataset, with the monolingual data pro-vided split into three parts (news 2007-2013, Eu-roparl (?)
and news commentary) and the weightstuned to minimize perplexity on the target side ofthe devset.The language models in our systems are trainedwith SRILM (Stolcke, 2002).
We train a 5-grammodel with Kneser-Ney discounting (Chen andGoodman, 1996).3.4 Context-informed PB-SMTHaque et al.
(2011) express a context-dependentphrase translation as a multi-class classificationproblem, where a source phrase with given addi-tional context information is classified into a dis-tribution over possible target phrases.
The size ofthis distribution needs to be limited, and wouldideally omit irrelevant target phrase translationsthat the standard PB-SMT (Koehn et al., 2003) ap-proach would normally include.
Following Haqueet al.
(2011), we derive a context-informed feature?hmblthat is expressed as the conditional probabil-ity of the target phrase e?kgiven the source phrase?fkand its context information (CI), as in (2):?hmbl= log P(e?k|?fk,CI(?fk)) (2)Here, CI may include any feature that can pro-vide useful information to disambiguate the givensource phrase.
In our experiment, we use CCG su-pertag (Steedman, 2000) as a contextual features.CCG supertag expresses the specific syntactic be-haviour of a word in terms of the arguments ittakes, and more generally the syntactic environ-ment in which it appears.We consider the CCG supertags of the contextwords, as well as of the focus phrase itself.
In ourmodel, the supertag of a multi-word focus phraseis the concatenation of the supertags of the wordscomposing that phrase.
We generate a windowof size 2l + 1 features (we set l:=2), includingthe concatenated complex supertag of the focusphrase.
Accordingly, the supertag-based contex-tual information (CIst) is described as in (3):CIst(?fk) = {st(fik?l), ..., st(fik?1), st(?fk),st(fjk+1), ..., st(fjk+l)}(3)For the Hindi-to-English translation task, we usepart-of-speech (PoS) tags4of the source phraseand the neighbouring words as the contextual fea-ture, owing to the fact that supertaggers are readilyavailable only for English.We use a memory-based machine learning(MBL) classifier (TRIBL: (Daelemans, 2005))5that is able to estimate P(e?k|?fk,CI(?fk)) bysimilarity-based reasoning over memorizednearest-neighbour examples of source?targetphrase translations.
Thus, we derive the feature?hmbldefined in Equation (2).
In addition to?hmbl,4In order to obtain PoS tags of Hindi words,we used the LTRC shallow parser for Hindi fromhttp://ltrc.iiit.ac.in/analyzer/hindi/shallow-parser-hin-4.0.fc8.tar.gz.5An implementation of TRIBL is freely available as partof the TiMBL software package, which can be downloadedfrom http://ilk.uvt.nl/timbl.216we derive a simple two-valued feature?hbest,defined in Equation (4):?hbest={1 if e?kmaximizes P(e?k|?fk,CI(?fk))u 0 otherwise(4)where?hbestis set to 1 when e?kis one of the tar-get phrases with highest probability according toP(e?k|?fk,CI(?fk)) for each source phrase?fk; oth-erwise?hbestis set to 0.000001.
We performed ex-periments by integrating these two features?hmbland?hbestdirectly into the log-linear model ofMoses.
Their weights are optimized using mini-mum error-rate training (MERT)(Och, 2003) on aheld-out development set for each of the experi-ments.3.5 Morphological SegmentationHaque et al.
(2012) applied a morphological suffixseparation process in a Bengali-to-English trans-lation task and showed that suffix separation sig-nificantly reduces data sparseness in the Bengalicorpus.
They also showed an SMT model trainedon the suffix-stripped training data significantlyoutperforms the state-of-the-art PB-SMT baseline.Like Bengali, Hindi is a morphologically very richand highly inflected Indian language.
As donepreviously for Bengali-to-English (Haque et al.,2012), we employ a suffix-stripping method forlemmatizing inflected Hindi words in the WMTHindi-to-English translation task.
Following Das-gupta and Ng (2006), we developed an unsu-pervised morphological segmentation method forHindi.
We also used a Hindi lightweight stem-mer (Ramanathan and Rao, 2003) in order to pre-pare a training corpus with only Hindi stems.
Weprepared Hindi-to-English SMT systems on theboth types of training data (i.e.
suffix-stripped andstemmed).63.6 Multi-Alignment Combination (MAC)Word alignment is a critical component of MTsystems.
Various methods for word alignmenthave been proposed, and different models can pro-duce signicantly different outputs.
For example,Tu et al.
(2012) demonstrates that the alignmentagreement between the two best-known alignmenttools, namely Giza++(Och and Ney, 2003) and6Suffixes were separated and completely removed fromthe training data.the Berkeley aligner7(Liang et al., 2006), is be-low 70%.
Taking into consideration the small sizeof the the corpus, in order to extract more ef-fective phrase tables, we concatenate three align-ments: Giza++ with grow-diag-final-and, Giza++with intersection, and that derived from the Berke-ley aligner.3.7 Stemming Alignment and Normal PhraseExtraction (SANPE)The rich morphology of Hindi will cause wordalignment sparsity, which results in poor align-ment quality.
Furthermore, word stemming onthe Hindi side usually results in too many Englishwords being aligned to one stemmed Hindi word,i.e.
we encounter the problem of phrase over-extraction.
Therefore, we conduct word alignmentwith the stemmed version of Hindi, and then atthe phrase extraction step, we replace the stemmedform with the original Hindi form.3.8 OOV Word Conversion MethodOur algorithm for OOV word conversion uses therecently developed zero-shot learning (Palatucciet al., 2009) using neural network language mod-elling (Bengio et al., 2000; Mikolov et al., 2013).The same technique is used in (Okita et al., 2014).This method requires neither parallel nor compa-rable corpora, but rather two monolingual corpora.In our context, we prepare two monolingual cor-pora on both sides, which are neither parallel norcomparable, and a small amount of already knowncorrespondences between words on the source andtarget sides (henceforth, we refer to this as the?dictionary?).
Then, we train both sides with theneural network language model, and use a contin-uous space representation to project words to eachother on the basis of a small amount of correspon-dences in the dictionary.
The following algorithmshows the steps involved:1.
Prepare the monolingual source and targetsentences.2.
Prepare the dictionary which consists of Uentries of source and target sentences com-prising non-stop-words.3.
Train the neural network language model onthe source side and obtain the real vectors ofX dimensions for each word.7http://code.google.com/p/berkeleyaligner/2174.
Train the neural network language model onthe target side and obtain the real vectors ofX dimensions for each word.5.
Using the real vectors obtained in the abovesteps, obtain the linear mapping between thedictionary items in two continuous spaces us-ing canonical component analysis (CCA).In our experiments we use U the same as the en-tries of Wiki corpus, which is provided amongWMT14 corpora, and X as 50.
The resulted pro-jection by this algorithm can be used as the OOVword conversion which projects from the sourcelanguage which among OOV words into the tar-get language.
The overall algorithm which usesthe projection which we build in the above step isshown in the following.1.
Collect unknown words in the translation out-puts.2.
Do Hindi named-entity recognition (NER) todetect noun phrases.3.
If they are noun phrases, do the projectionfrom each unknown word in the source sideinto the target words (We use the projectionprepared in the above steps).
If they are notnoun phrases, run the transliteration to con-vert each of them.We perform Hindi NER by training CRF++ (Kudoet al., 2004) using the Hindi named entity corpus,and use the Hindi shallow parser (Begum et al.,2008) for preprocessing of the inputs.4 Results and Discussion4.1 DataWe conduct our experiments on the standarddatasets released in the WMT14 shared translationtask.
We use HindEnCorp8(Bojar et al., 2014)parallel corpus for MT system building.
We alsoused the CommonCrawl Hindi monolingual cor-pus (Bojar et al., 2014) in order to build an addi-tional language model for Hindi.For the Hindi-to-English direction, we also em-ployed monolingual English data used in the othertranslation tasks for building the English languagemodel.8http://ufallab.ms.mff.cuni.cz/ bojar/hindencorp/4.2 Moses BaselineWe employ a standard Moses PB-SMT model asour baseline.
The Hindi side is preprocessed butunstemmed.
We use Giza++ to perform wordalignment, the phrase table is extracted via thegrow-diag-final-and heuristic and the max-phrase-length is set to 7.4.3 Automatic EvaluationExperiments BLEUMoses Baseline 8.7Context-Based 9.4Context-Based + CommonCrawl LM 11.4Table 1: BLEU scores of the English-to-Hindi MTSystems on the WMT test set.Experiments BLEUMoses Baseline 10.1Context-Based 10.1Suffix-Stripped 10.0OWC 11.2OSM 10.3Three LRMs 10.5MAC 10.7SANPE 10.6LMI 10.9LMI+SANPE+MAC+ThreeLRMs+OSM 11.7Table 2: BLEU scores of the Hindi-to-English MTSystems on the WMT test set.We prepared a number of MT systems for bothEnglish-to-Hindi and Hindi-to-English, and sub-mitted their runs in the WMT 2014 EvaluationMatrix.
The BLEU scores of the different English-to-Hindi MT systems (Moses Baseline, Context-Based (CCG) MT system, and Context-Based(CCG) MT system with an additional LM builton the CommonCrawl Hindi monolingual corpus(Bojar et al., 2014)) on the WMT 2014 English-to-Hindi test set are reported in Table 1.
As canbe seen from Table 1, Context-Based (CCG) MTsystem produces 0.7 BLEU points improvement(8.04% relative) over the Moses Baseline.
Whenwe add an additional large LM built on the Com-monCrawl data to the Context-Based (CCG) MTsystem, we achieved a 2 BLEU-point improve-ment (21.3% relative) (cf.
last row in Table 1) over218the Context-Based (CCG) MT system.9The BLEU scores of the different Hindi-to-English MT systems on the WMT 2014 Hindi-to-English test set are reported in Table 2.
Thefirst row of Table 2 shows the BLEU score forthe Baseline MT system.
We note that the per-formance of the Context-Based (PoS) MT systemobtains identical performance to the Moses base-line (10.1 BLEU points) on the WMT 2014 Hindi-to-English test set.We employed a source language (Hindi) nor-malisation technique, namely suffix separation,but unfortunately this did not bring about anyimprovement for the Hindi-to-English translationtask.
The improvement gained by individuallyemploying OSM, three lexical reordering mod-els, Multi-alignment Combination, Stem-align andnormal Phrase Extraction and Language Model In-terpolation can be seen in Table 2.
Our best sys-tem is achieved by combining OSM, Three LMR,MAC, SANPE and LMI, which results in a 1.6BLEU point improvement over the Baseline.5 AcknowledgmentsThis research is supported by the Science Foun-dation Ireland (Grant 12/CE/I2267) as part ofthe CNGL Centre for Global Intelligent Content(www.cngl.ie) at Dublin City University.ReferencesAmittai Axelrod, Ra Birch Mayne, Chris Callison-burch, Miles Osborne, and David Talbot.
2005.
Ed-inburgh system description for the 2005 iwslt speechtranslation evaluation.
In Proceedings of the Inter-national Workshop on Spoken Language Translation(IWSLT).Rafiya Begum, Samar Husain, Arun Dhwaj,Dipti Misra Sharma, Lakshmi Bai, and RajeevSangal.
2008.
Dependency annotation scheme forindian languages.
In Proceedings of The Third In-ternational Joint Conference on Natural LanguageProcessing (IJCNLP).Yoshua Bengio, Rejean Ducharme, and Pascal Vincent.2000.
A neural probabilistic language model.
InProceedings of Neural Information Systems.Ond Bojar, Pavel Stranak, and Daniel Zeman.
2010.Data issues in english-to-hindi machine translation.In LREC.9Please note that this is an unconstrained submission.Ondrej Bojar, V. Diatka, Rychly P., Pavel Stranak,A.
Tamchyna, and Daniel Zeman.
2014.
Hindi-english and hindi-only corpus for machine transla-tion.
In LREC.Stanley F. Chen and Joshua Goodman.
1996.
An em-pirical study of smoothing techniques for languagemodeling.
In Proceedings of the 34th Annual Meet-ing on Association for Computational Linguistics,ACL ?96, pages 310?318, Stroudsburg, PA, USA.Association for Computational Linguistics.Walter Daelemans.
2005.
Memory-based languageprocessing.
Cambridge University Press.Sajib Dasgupta and Vincent Ng.
2006.
Unsupervisedmorphological parsing of bengali.
Language Re-sources and Evaluation, 40(3-4):311?330.Nadir Durrani, Helmut Schmid, and Alexander Fraser.2011.
A joint sequence translation model with in-tegrated reordering.
In Proceedings of the 49th An-nual Meeting of the Association for ComputationalLinguistics: Human Language Technologies - Vol-ume 1, HLT ?11, pages 1045?1054, Stroudsburg, PA,USA.
Association for Computational Linguistics.Michel Galley and Christopher D. Manning.
2008.
Asimple and effective hierarchical phrase reorderingmodel.
In Proceedings of the 2008 Conference onEmpirical Methods in Natural Language Process-ing, pages 848?856, Honolulu, Hawaii, October.
As-sociation for Computational Linguistics.Rejwanul Haque, Sudip Kumar Naskar, Antal van denBosch, and Andy Way.
2011.
Integrating source-language context into phrase-based statistical ma-chine translation.
Machine translation, 25(3):239?285.Rejwanul Haque, Sergio Penkale, Jie Jiang, and AndyWay.
2012.
Source-side suffix stripping for bengali-to-english smt.
In Asian Language Processing(IALP), 2012 International Conference on, pages193?196.
IEEE.Philipp Koehn, Franz Josef Och, and Daniel Marcu.2003.
Statistical phrase-based translation.
InProceedings of the 2003 Conference of the NorthAmerican Chapter of the Association for Computa-tional Linguistics on Human Language Technology-Volume 1, pages 48?54.
Association for Computa-tional Linguistics.Philipp Koehn, Hieu Hoang, Alexandra Birch, ChrisCallison-Burch, Marcello Federico, Nicola Bertoldi,Brooke Cowan, Wade Shen, Christine Moran,Richard Zens, Chris Dyer, Ond?rej Bojar, AlexandraConstantin, and Evan Herbst.
2007.
Moses: Opensource toolkit for statistical machine translation.
InProceedings of the 45th Annual Meeting of the ACLon Interactive Poster and Demonstration Sessions,ACL ?07, pages 177?180, Stroudsburg, PA, USA.Association for Computational Linguistics.219Taku Kudo, Kaoru Yamamoto, and Yuji Matsumoto.2004.
Appliying conditional random fields tojapanese morphological analysis.
In Proceedings ofEMNLP.Percy Liang, Ben Taskar, and Dan Klein.
2006.
Align-ment by agreement.
In Proceedings of the mainconference on Human Language Technology Con-ference of the North American Chapter of the As-sociation of Computational Linguistics, pages 104?111.
Association for Computational Linguistics.Tomas Mikolov, Quoc V. Le, and Ilya Sutskever.
2013.Exploiting similarities among languages for ma-chine translation.
ArXiv.Franz Josef Och and Hermann Ney.
2003.
A sys-tematic comparison of various statistical alignmentmodels.
Computational linguistics, 29(1):19?51.Franz Josef Och.
2003.
Minimum error rate training instatistical machine translation.
In Proceedings of the41st Annual Meeting on Association for Computa-tional Linguistics - Volume 1, ACL ?03, pages 160?167, Stroudsburg, PA, USA.
Association for Com-putational Linguistics.Tsuyoshi Okita, Ali Hosseinzadeh Vahid, Andy Way,and Qun Liu.
2014.
Dcu terminology translationsystem for medical query subtask at wmt14.Mark Palatucci, Dean Pomerleau, Geoffrey Hinton,and Tom Mitchell.
2009.
Zero-shot learning withsemantic output codes.
In Neural Information Pro-cessing Systems (NIPS), December.Ananthakrishnan Ramanathan and Durgesh D Rao.2003.
A lightweight stemmer for hindi.
In the Pro-ceedings of EACL.Mark Steedman.
2000.
The syntactic process, vol-ume 35.
MIT Press.Andreas Stolcke.
2002.
Srilm ?
an extensible lan-guage modeling toolkit.
In Proceedings of the Inter-national Conference Spoken Language Processing,pages 901?904, Denver, CO.Zhaopeng Tu, Yang Liu, Yifan He, Josef van Genabith,Qun Liu, and Shouxun Lin.
2012.
Combining mul-tiple alignments to improve machine translation.
InCOLING (Posters), pages 1249?1260.220
