Text  F i l ter ing in B/IUC-3 and MUC-4David D. LewisCenter  for In fo rmat ion  and Language StudiesUnivers i ty  of Ch icagoChicago,  IL 60637Ie wis O~ ira.
uc hic ago.
eduandRichard M. TongAdvanced Decis ion Systems(a divis ion of Booz,  Al len & Hami l ton)1500 P lymouth  St reetMounta in  View, CA 94706r~ong~ads.comIntroduct ionOne of the changes from the Third (MUC-3) to the Fourth (MUC-4) Message Understanding Confer-ence was the emergence of text filtering as an explicit opic of discussion.
In this paper we examinetext filtering in MUC systems with three goals in mind.
First, we clarify the difference between twouses of the term ~text filtering ~ in the context of data extraction systems, and put these phenomenain the context of prior research on information retrieval (IR).Secondly, we discuss the use of text filtering components in MUC-3 and MUC-4 systems, andpresent a preliminary scheme for classifying data extraction systems in terms of the features overwhich they do text filtering.Finally, we examine the text filtering effectiveness ofMUC-3 and MUC-4 systems, and introducesome approaches to the evaluation of text filtering systems which n~y be of interest hemselves.
Twoquestions of crucial interest are whether sites improved their system level text filtering effectivenessfrom MUC-3 to MUC-4, and what the effectiveness of MUC systems would be on real world datastreams.
Because of changes in both test set and system design since MUC-3 we were not able toaddress the first question.
However, with respect o the second question, we present preliminaryevidence suggesting that the text filtering precision of MUC systems declines with the generality ofthe data stream they process, i.e.
the proportion of relevant documents.
The ramifications of thisfor future research and for operational systems are discussed.Text Fi ltering and Information Retr ievalThe term ~ez~ fi,i~ering (or relevance fil~emng) has been used in the context of data extraction systemsto refer to two rather different hings.
First, any data extraction system can be evaluated on thedegree to which it extracts data for all and only those documents which actually contain legitimatedata.
Implicitly or explicitly the system makes a decision, for each document, whether to extractsome data or to extract no data.
We can refer to the result of these decisions as aystem level ~ez~filtering.
Therefore, the effectiveness of system level text filtering is a property that can be measuredfor any data extraction system.Second, from an architectural standpoint, data extraction systems can include components hatmark documents or parts of documents as nonrelevant, so that later stages of processing are notapplied to them, or are applied differently to them.
We call these ~ez~ filtering components or ~eztfi~ ~ e rs.In both system level text filtering and component level text filtering, we have a computer programclassifying texts into two categories: relevant and nonrelevant.
Text filtering in data extraction51systems is therefore closely related to a number of text classification tasks that information retrievalresearchers have investigated.
It is useful to view these tasks as lying along a continuum accordingto the degree of care that goes into defining classes of documents, and the duration of interest inthose closes:1.
Tezt retrieoal: Computer selection of a subset of a document database to display in wholeor summary form to a user in response to a user request.
A user's division of documentsinto relevant and nonrelevant may be of relatively short-term interest, and may indeed changeduring a retrieval session.2.
Tezt routing, tezt fdter/n9, and SDI (selective dissemination of information): These termsrefer to a loose collection of text classification tasks such as managing personal electronic mail,distribution of information to the appropriebte person in an organization, and so on.
Categoriesare of longer term interest han in text retrieval, may be defined by someone other than theconsumer of information, and may be part of an organized scheme.3.
Tezt categorization: Classification of documents with respect to a set of one or more pre-existing categories.
The categories are usually of long term interest and part of an organizedscheme (hierarchical, disjoint, etc.
).Text filtering in data extraction systems falls toward the text categorization end of this contin-uum, since the two categories of interest (templates/no templates) axe of long term interest and axegiven considerable attention system builders.
One difference between MUC text filtering and mostIR categorization tasks is the complexity of the category definition.
A relevance judgment in MUC-3or MUC-4 required distinguishing between terrorist events and state sponsored violence on the onehand and very similar guerilla warfare vents on the other.
The exclusion of discredited, nonspecific,and nonrecent events further complicated the definition of relevance.
In contrast, the distinctionsmade in classical text categorization tasks, such as automated controlled vocabulary indexing, tendto be broader and involve fewer conditions.Another distinction is that the proportion of relevant documents in the MUC test sets is muchhigher than in typical text categorization applications.
The consequences of this will be discussedwhen we examine the system level text filtering results from MUC-3 and MUC-4.Approaches to Text Filtering in MUC System DesignMUC-3 and MUCo4 systems varied widely in the extent to which they made use of text filteringcomponents.
Some used none at all, others used several kinds.
In this section we summarize theuses of text filtering found in these systems.
We break down the uses of text filtering into threeclasses, according to whether the filtering takes \]place before, during or after parsing.
Note that weus the term "parsing" rather loosely here to refer not just to syntactic analysis, but to all substantivelinguistic analysis.
Thus our categories are:1.
Pre-parse: Filtering is performed before significant linguistic analysis.
Entire documents orparts of documents are flagged as nonrelevant based on the presence or absence of particularwords or simple patterns of words.2.
Intr~-parse: Some documents or parts of documents are classified as nonrelevant by examiningpartially produced syntactic or semantic representations before linguistic analysis is complete.3.
Post-parse: Filtering is based on examining the final output of linguistic analysis, or examiningactual templates that are otherwise ready to be output.
The decision made is usually to allowor disallow an entire document or an entire template, rather than a part of a document.
1INote, that we chose not make a further distinction between '~ost..analyais" and '~poat-templstes" filtering evanthough we have examldes of both in the MUC-3/MUCo4 systen~.
From a text filtering perspective, this distinctionis not so interesting since they are both examp|es of filt~-ing on complex semantic structures using domain specificheurktlca52Table 1: Summary of Text Filtering Functions for MUC3 and MUC4 SitesSystemADSBBNConQuestGEGE-CMUGTEHUGHESITPLSIMDCMITRENMSU/BrandeisNYUPARAMAXPRCSRASRIUMASSUMICHUNL/USLUSCPre-parseMUC-3/4yes/***no/nono/noyes/yes***/yesno/***no/noyes/***yes/yesno/yes***/no***/yesyes/yesyes/yesno/no***/yesyes/yesno/no***/yesyes/******/noIntra-parseMUC-3/4yes/yesno/nono/no***/nono/***no/***no/nono/no***/no***/yesno/no--/-no/no***/nono/noyes/yes***/no- / ******/noPost-parseMUC-3/4no/***no/yesyes/noyes/yes***/yesyes/***no/noyes/***yes/yesyes/yes***/no***/noyes/yesno/noyes/yes***/noyes/yesyes/yes***/yesno/******/noLEGEND:- -denotes  "not applicable"*** denotes "did not participate"The lines between these classes are not sharp.
Filtering rules which refer to semantic lasses ofwords, or which are based on complicated patterns defined over words, straddle the pre-parse andintra-parse cases.
Similarly, the point at which parsing is over and any filtering would be post-parseis not always clear.
Nevertheless, we found the distinctions useful, and summarize our analysis ofthe systems with respect to them in Table 1.
The entries in this table are primarily derived from thesystem descriptions provided by the individual sites in the MUC-3 proceedings in their presentationsat the MUC-4 workshop.
In some cases, clarification was provided by sites in response to requestsby the authors.Use of Text Fi ltering in MUC-3 and MUC-4 SystemsIn this section we briefly describe the text filtering processes performed by each of the sites partic-ipating in MUC-3 and/or MUC-4.
While every site that participated in both MUC-3 and MUC-4made some changes to their overall system these were mostly not in the text filtering functions.
Onthe other hand, some repeating sites fielded completely different systems in MUC-4.
2ADS: ADS' CODEX system used a pre-parse filter based on matching complex patterns of wordsand phrases at the sentence level.
Sentences that matched the patterns above some thresholdwere candidates for further processing.
Those that were below threshold were eliminated.
ADSparticipated in MUC-3 hut not in MUC-4.BBN: BBN's PLUM system performed intra-parse filtering in a "Discourse Module" where only~More detailed escriptions ofthe MUC-4 systems can be found elsewhere inthis volume.
Detailed escriptions ofthe MUC-3 systen~ can be found in "Proceeclings Third Meuage Understanding Conference (MUC-3)," San Diego,May 1991.
Morgan Kaufmann Publishers, San Mateo, CA.53terrorist incidents, or possible terrorist incidents, generate "discourse vent structures."
The versionof PLUM fielded at MUC-4 added a statistical classification algorithm for paragraph level filtering.This probabilistic filter was trained on paragraph level relevance judgments and flagged paragraphsas relevant or nonrelevant based on the presence of particular word stems.
Full NL analysis ofnonrelevant paragraphs was stin carried out, but events in those paragraphs were not allowed totrigger templates.
Thus this method mixed aspec!~s ofpre-parse and post-parse filtering.
(We classifyit as post-parse in Table 1.
)OEz GE's NLToolset performed pre-parse filtering of non-relevant sentences and parts of sentencesusing lexico-semantic patterns.
Post-parse filtering involved generating templates in their final formand then checking them against relevance conditions.GE-CMU:  The GE-CMU system fielded for MUC-4 was identical to the GE system except for thereplacement ofGE's TRUMP analyzer with a generalized LR parser.
Filtering functions are exactlythe same.
GE-CMU were new participants in MUC-4.GTE:  GTE's TIA system performed post-parse filtering on the semantic output of their seman-tic analyzer.
This "output translator" was only partially implemented, however.
GTE did notparticipate in MUC-4.HUGHES:  Hughes' approach made substantial use of text classification methods, but apparentlydid not use classifiers trained specifically on the distinction between relevant and nonrelevant texts.ITP :  ITP's system used a simple form of pre-parse filtering by ignoring sentences which did notcontain a terrorism word.
The system also performed post-parse filtering on the output of itslanguage analysis module.
The "cognitive model" produced by this module is an interpretation ofthe complete message text which was then examined for indicators of relevant MUC events.
ITPdid not participate in MUC-4.LSI: LSI's DBG system performed both pre-parse and post-parse filtering.
The pre-parse filteringwas done by selecting only those sentences that contained a keyword taken from either the "eventword list" or the "result word list."
Post-parse filtering was based on processing semantic inter-pretations which, if they contained relevance indicators, caused the generation of an entry on theinternal DBG-template-queue.
This queue was subjected to further processing to determine whichMUC templates hould be generated.MDC:  MDC's INLET system fielded for MUC.-3 used a preliminary form of post-parse filteringin the generation of templates.
For MUC-4, MDC fielded an improved system called TexUS thatperformed keyword-pattern-based, pre-parse filtering at the sentence l vel, as well as some post-parsefiltering based on event rejection while attempting to fill templates.MITRE:  Mitre's system performed no filtering as defined in this paper.
Mitre was a new participantat MUC-4.NMSU/Brande is :  The NMSU/Brandeis MucBruce system performed "two-pass" pre-parse filter-ing to determine both relevant exts and then relevant paragraphs with respect o specific events.These filters are statistical classifiers developed using the MUC training corpus.
Intra-parse filteringwas performed as part of the "robust parsing" process.
Sentence level parsing was terminated ifeither no appropriate action word was found or various template filling criteria were not met.
TheNMSU/Brandeis team were new participants at MUC-4.NYU:  NYU's PROTEUS system performed pre-parse filtering at the sentence level using simplekeyword pattern matching.
This filtering is done after lexlcal analysis to minimize the numberof patterns that need to be specified.
Post-parse filtering was performed on the "event frames"generated from the output of the linguistic analysis.
In particular, filters removed frames involvingonly rrfi\]itary targets and those involving events more than two months old.PARAMAX (MUC-3:  UNISYS) :  Par,max's CBAS system fielded at MUC-4 was a more el-54ficient implementation of the KBIRD rule base used in MUC-3.
The CBAS system performedpre-parse filtering by using statistical correlations between words occurring in the text and "conceptprofiles" for MUC related events.
Text was marked as relevant if the match was above a specificthreshold.
Subsequent processing keyed off the text marked as relevant.PRC:  PRC's PAKTUS system performed post-parse filtering by examining the output of the "dis-course analysis" module.
The filter looked for domain-specific patterns in the output and if nopatterns were matched within a sentence, then that sentence was ignored in generating the finaltemplate output.SRA:  SRA's SOLOMON system performed pre-parse filtering by rejecting paragraphs that did notcontain MUC-specific keywords.
SRA were new participants in MUC-4.SRI :  SRI's TACITUS system fielded for MUC-3 used both pre-parse and post-parse filtering.
Thepre-parse filter was a two-stage function.
First, it identified relevant sentences using a statisticaln-gram model trained on the MUC corpus.
This was foUowed by an "keyword antifilter" that flaggedas relevant sentences that had been filtered out by the previous tage, but which contained certainkeywords and were in close proximity to a sentence already marked as relevant.
The post-parsefilter operated on completed templates and removed those that did not satisfy certain coherence orrelevance criteria.
The completely new FASTUS system fielded for MUC-4 again used both pre-parseand post-parse filtering.
Pre-parse filtering used "trigger words" derived from patterns recognized by~the system as it constructed MUC templates.
A variety of optional post-parse filters were used, suchas screening out templates with too many targets, templates with civiIian targets, and templateswith stale dates.Univ .
o f  Mary land/ConQuest  (MUC-3 :  Synchronet ics) :  The system fielded for MUC-3 hada rudimentary post-parse filter that examined the semantic network output of the linguistic analysiscomponent and determined whether any of the actions detected were within the parameters of a"reportable action" as defined by the MUC task.
The ICOTAN system fielded for MUC-4 performedno text filtering as defined in this paper.Univ .
o f  Massachuset ts :  The University of Massachusetts system performed both intra-parseand post-parse filterlng.
The intra-parse filtering was achieved by screening input sentences fortriggers associated with "concept nodes".
If the input contained no triggers then no case frameswere instantiated and no further processing of that input was performed.
Post-parse filtering wasdone using a memory-based consolidation of case frames into event descriptions.
Event descriptionsthat did not meet MUC specific criteria were not used to generate filled templates.Univ .
o f  Mich igan:  The University of Michigan's LINK system performed pre-parse filtering atthe sentence level.
A sentence was filtered if it contains no "interesting" definitions.
A definitionwas interesting if it was one of the terrorist acts or related words.
Post-parse filtering was a two-stepprocess.
First, inappropriate slot fillers were removed, then entire templates were filtered if theycontained an inappropriate date, or if they did not contain enough fillers to enable a possible matchwith the answer key.
The University of Michigan were new participants at MUC-4.Univ .
o f  Nebraska  L inco ln /Un lv .
of  Southwestern  Louis iana: The UNL/USL system wasbased on statistical text categorization.
Templates were created for a document if it passed throughthe "optimal query" filter (trained to distinguish between relevant and nonrelevant documents) orif it produced sufficient activation of some incident type and other associated concepts.
UNL/USLdid not participate in MUC-4.Univ .
o f  Southern  Cal i fornia:  USC's SNAP systems performed no text filtering in the senseused in this paper.
USC were new participants in MUC-4.55Discuss ionThe most notable feature of Table 1 is that nearly all the systems include some filtering component.The relative rarity of intra-parse filtering is perhaps urprising, though without detailed descriptionsof the parsing process, we admittedly were often unable to decide whether filtering was taking place.In any case, filtering during linguistic analysis is often an implicit process--the fact that an analysisdid not produce a recognizable interpretation ofa text unit and, therefore, ignored that unit is kindof filtering, but is not the kind under discussion here.There was a tendency for certain constraints to be implemented as filters at particular points inthe analysis process, and by particular methods.
A. broad distinction between potentially relevanttext involving violent events, and nonrelevant text lacking such events, was often encoded as a pre-parse filter.
These pre-parse filters usually checked text units for words strongly associated with oneor more incident ypes, and often were trained by statistical methods.
This seems appropriate, sincestatistical classification methods using words as features are probably at their best in making suchbroad distinctions in subject matter.On the other hand, constraints uch those agaiust events which were too old, too vague, orwhich involved military targets typically were implemented as simple rules which examined eitherthe output of semantic interpretation or, more commonly, templates generated in otherwise finalform.
Again, this seems a reasonable strategy since the constraints on relevance here referred toproperties of data which should be observable in correctly generated templates.As well as exhibiting a range of text filtering strategies, MUC sites also exhibited a range ofmotivations for their use of text filtering.
One motivation was efficiency, particularly in MUC-3,where many sites were challenged by a test set size larger than that used in previous MessageUnderstanding Conferences, or in traditional NLP research.
The use of computationally cheapfiltering components to limit the amount of text analyzed by expensive natural anguage methodswas crucial to allowing several systems to process the test set.In other cases, filtering components were intended to improve effectiveness rather than, or inaddition to, efficiency.
Filtering out or flagging nonrelevant text early in processing reduced thechance of later stages of analysis extracting spurious data.
Alternately, plausible data was tentativelyextracted from parts of a document, but a later filtering stage could reject the as a whole if it didnot satisfy the relevance constraints.A third motivation for including filtering components was to save effort by system builders.Filtering is a simple classification process, which means that statistical and machine learning tech-niques can be used to induce filters from training data with relatively little human effort.
MUC-3 andMUC-4 participants used corpora tagged for relevance of messages, paragraphs, or even sentencesin training filters.
These labelled corpora have the advantage that they can be shared between sitesmore easily than actual code, and can support he investigation of a variety of filtering strategies.Finally, many filtering techniques include thresholds that can be varied to make a text moreor less apt to be filtered out, providing a degree of flexibility to the overall system.
Varying thethreshold on the text filter can be used directly to vary the recall and precision of system level textfiltering, as well as indirectly to vary recall and precision of data extraction.Unfortunately, it is difficult to tell how useful text filtering components were in achieving theabove goals.
As we write this article, we know of only two MUC-4 sites that have presented ataon the effect of running their system with different filtering variations.
BBN showed the ability totrade off data extraction recall and precision by varying the threshold on their paragraph level textfilter.
SRI investigated turning on or off four template-level post-parse filters.
Two were found toimprove both recall and precision (for overall data extraction), while two were found to producerecall/precision tradeoffs.We hope there will be more experiments of this kind, even (or especially!)
if they produceseemingly trivial results, such as performance being terrible without the filter, or performance beingunchanged without the filter.Evaluation of Text Filtering in MUC56Yes is I No isCorrect CorrectDecides Yes a + z : a + b + zJDecides No c d + y c + d + yFigure 1: Contingency table for binary decisions, including z + y = o optional/don't care decisions.The MUC workshops have borrowed several of their effectiveness measures from information re-trieval.
As used in evaluating data extraction, the measures had to be modified considerably, sothat the MUC measures recall, precision, and fallout are related to those in IR by analogy, notidentity \[CHL92\].However, when evaluating the degree to which MUC systems elect all and only relevant doc-uments from which to extract data, the measures recall, precision, and fallout have essentially thesame meaning they do in text retrieval or text categorization.
In all cases, we are evaluating theability of a system to make correct yes/no decisions, and the contingency table model from signaldetection is appropriate \[Swe69\].One di/ference from typical IR tasks is the presence in MUC of documents for which all templatesare optional, i.e.
messages for which either decision by a system (generate templates or don't generatetemplates) is treated as correct.
In Figure 1 we give a contingency table for binary decisions, adaptedto allow for z + y = o optional documents, as well as a+ c relevant documents and b+ d nonrelevantdocuments.
The total in the yes/yes cell is the number of relevant documents that were judgedrelevant by the system Ca), plus the number of optional documents that were judged relevant bythe system (z).
Similarly, the total in the no/no cell is the sum of the nonrelevant documents thatwere judged nonrelevant (d), and the optional documents that were judged nonrelevant (y).
Theeffectiveness measures for system level text filtering used in MUC-4 were:(1) recall = (o,?
z)/(o.+ c+ z)(2) precision = (a + o)/(o + b + ~)(3) fallout = b/(b + d + ~)When z and y are 0 (no optional documents) these reduce to the conventional measures used inIR.Another measure used in IR is 9enerali~, or the proportion of documents in the test set whichare relevant \[Rob69\].
It is not a measure of system effectiveness, but rather of a property of a testset.
In the traditional IR case, when there are no optional documents, generality is defined as:(4) generality ---- (a -I- c) l (o -I- b -I- c -I- d)In the presence of optional documents, we would again like a definition of generality which is ameasurement of the properties of the test set, not of any particular system.
(Thus we should notrefer to z or y in the formula, only o.)
Two alternatives suggest hemselves:(4 ~) generality = C a + c)/(a + b ~ c + d ~ o)(4") generality : (a + c + o)/(a + b + c + d + o)57I Yes is No is \[Correct CorrectDecides Yes (r + o)s n8 (r + n + 0)8DecidesNo r(l-8) (n.+o)(l-s) (r+n+o)(l-8)I,+o8Figure 2: Expected values of contingency table cell counts for a system which randomly guesses aproportion 8 of documents to be relevant.From a mathematical standpoint, the twb meane~ures "~eem equally reasonable.
We will use 4" inthis paper for reasons described below.We mentioned in the previous ection that the proportion of relevant documents i much higherin MUC than in most IR tasks.
This means that chance ffectiveness , i.e.
the effectiveness we wouldexpect a system to achieve by random guessing, is relatively high.
Along with the presence ofoptionaimessages, this means that intuitions about whether particular effectiveness values constitute goodperformance can be misleading.One aid to interpreting data is significance testing \[Chi92b\].
Another aid is to present forcomparison the effectiveness that would be achieved by a system that performed the task randomly.We can compute the expected values of the effectiveness measures for a randomly performing textfilter as follows.
Suppose there are r relevant documents, n nonre\]evant documents, and o optionaldocuments in the test set.
Assume a hypothetical system which will guess randomly with probabilitys that any particular document is relevant, and with probability 1- s will consider it nonrelevant.This gives us the contingency table shown in Figure 2.
We show in each cell the expected number ofdocuments which would fall in that cell, under the assumption that the system is given the benefitof the doubt for optional documents, as in MUC?From this table we can calculate the expected values for recall, precision, and fallout:(5) E(recail) ---- gS.t2~.P.-l- os(6) E(precision)----(7) E(fallout)= n, ~wr:uFor example, the TST3 test set had 65 relevant, 4 optional, and 31 nonrelevant documents.
Asystem which randomly treated 70% of documents as relevant would have an expected recall on TST3of 0.71, expected precision of .69, and expected i~dlout of .67.
A system that randomly treated 25%of documents as relevant would have expected recall of .26, expected precision of .69, and expectedfallout of .23.Note that the expected value of precislon for a randomly guessing system is constant with varyingvalues of 8.
In fact, it is simply generality, as defined in 4" above.
(Defining generality as in 4" letsus maintain the identity between generality and the expected value of precision, even when optionaldocuments are present.
)This points up a weakness in using precision to measure text filtering effectiveness.
When systemsare operating at chance or near chance levels, precision cannot distinguish between a tendency toblindly guess many documents as relevant and a tendency to blindly guess few documents as relevant.One consequence of this is that the best strategy for a system with poor text filtering capability, ifevaluated on recall and precision, is to guess that every document is relevant, maximizing recall andignoring precision.58Table 2: Recall, precision, and fallout for system level text filtering for MUC-3  and MUC-4  systems,plus a system which judges every document relevant.
Values are in hundredths, i.e.
79 means 0.79.MUC-3  TST2 5/91 MUC-4  TST3 5/92 MUC-4  TST4 5/92Site REC PRE FALL  REC PRE FALL  REC PRE FALLADS 91 77 51 *** *** *** *** *** ***BBN 100 69 85 83 87 23 83 74 31CONQUEST 37 78 15 32 81 14 48 74 19GE 95 81 38 91 87 28 96 84 22GE-CMU *** *** *** 87 89 22 87 87 15GTE 51 71 35 *** *** *** *** *** ***HUGHES 98 66 100 98 69 100 100 57 95ITP  65 74 37 *** *** *** *** *** ***LSI 58 83 17 91 75 64 89 63 61MDC 82 75 46 69 81 34 68 79 22MITRE *** *** *** 90 76 59 100 64 70NMSU *** *** *** 83 80 41 95 66 61NYU 95 72 74 82 86 27 91 85 20PARAMAX 60 90 10 88 77 56 96 68 56PRC 98 78 53 73 86 24 72 76 26SRA *** *** *** 71 85 23 91 75 35SRI 70 83 24 90 88 25 93 82 28UMASS 97 82 39 91 94 12 91 82 24UMICH *** *** *** 90 82 41 82 69 44UNL/USL  94 74 53 *** *** *** *** *** ***USC *** *** *** 45 70 39 69 70 31AlIRel I00 66 i00 I00 69 100 100 55 100Fallout, on the other hand, correctly distinguishes systems which treat many documents asrelevant from those that treat few documents as relevant, even if those decisions are nearly random.We will see other reasons to prefer fallout over precision as an effectiveness measure for text filtering.It is important to remember, of course, that MUC-3  and MUC-4  system designers were attempt-ing to achieve the best possible overall template filling effectiveness, not just the best text filteringeffectiveness.
The data extraction effectiveness of a system that randomly generated templates, notjust relevance decisions, would of course be much worse than that of any actual MUC system.Text Fi ltering Effectiveness in MUC-3  and MUC-4In this section we present data on the effectiveness of system level text filtering in MUC-3  and MUC-4 systems.
Table 2 presents the recall, precision, and fallout scores for text filtering for each MUC-3site on the TST2 testset, and for each MUC-4  site on the TST3 and TST4 testsets.
These scoreswere computed from the official MUC-3  and MUC-4  score reports.
Along with the system scores, wealso present a final row, AlIRe~ which indicates what scores a system would get if it simply treatedevery document as relevant.Table 3 similarly presents the values of the F-measure \[van79\] for all MUC-3  and MUC-4  sys-tems.
We compute the F-measure for system level text filtering the same way as for overall systemeffectiveness \[Chi92\], but use the text filtering recall and precision values as input, rather than thedata extraction recall and precision values:F= (~2 +1.0) x Px  R~2 ?
P?RFinally, Figures 3 to 8 plot recall vs. precision, and fallout vs. recall for each of the three data59Table 3: F-values for system level text filtering effectiveness of MUC-3 and MUC-4 sites, plus ahypothetical system (AIIRel) which judges every document relevant.
Values of F for 13 of 0.5, 1.0,and 2.0 are given.
Values are in hundredths, i.e.
79 means 0.79.MUC-3 TST2 5/91Site ~0.5  ~1.0  ~2.0ADS 79 83 88BBN 74 82 92CONQUEST 64 50 41GE 83 87 92GE-CMU *** *** ***GTE 66 59 54HUGHES 71 79 89ITP  72 69 67LSI 76 68 62MDC 76 78 80MITRE *** *** ***NMSU *** *** ***NYU 76 82 89PARAMAX 82 72 64PRC 81 87 93SRA *** *** ***SRI 80 76 72UMASS 85 89 94UMICH *** *** ***UNL/USL  77 83 89USC *** *** ***AIIRel 71 80 91MUC-4 TST3 5/92 MUC-4 TST4 5/92~o.5 ~1.0 ~2.0 ~0.5 ~1.0 ~2.086 85 84 76 78 8162 46 36 67 58 5288 89 90 86 90 9389 88 87 87 87 8773 81 90 62 73 8778 82 87 67 74 8278 75 71 77 73 7078  82 87 69 78 9081 81 82 70 78 8785 84 83 86 88 9079 82 I 86 72 80 8983 79 75 75 74 7382 77 73 78 82 8788 89 90 84 87 9193 92 92 84 86 8983 86 88 71 75 7963 55 48 70 69 6974 82 92 60 71 8660.8.6Precision.4.2I ITST2 Data ?TST2 Chance OI?
I0 I I I I0 .2 .4 .6 .8 1RecallFigure 3: System level text filtering: recall vs. precision of MUC-3 systems on TST2.Recall.8.6.4.2TST2 Data ?TST2 ChanceI O~ ' I I0 .2 .4 .6 .8 1FalloutFigure 4: System level text filtering: fallout vs. recall (ROC plot) of MUC-3 systems on TST2.61.8.6Precision.4.2I I I IeD ?
?~%iTST3 Data ?TST3 Chance ,@---0 I I\[ I I0 .2 .4 .6 .8 1RecallFigure 5: System level text filtering: recalJL vs. precision of MUC-4 systems on TST3.Recall.8.6.4.2I0000fl !TST3 Data ?TST3 Chance0 (~ I e I I0 .2 .4 .8 .8 1FalloutFigure 6: System level text filtering: fallout vs. recall (ROC plot) of MUC-4 systems on TST3.621 I I I I.8.6Precision.4.2@?
0 -0000?
asTST4 Data ?TST4 Chance O0 I I I I0 .2 .4 .6 .8RecallFigure 7: System level text filtering: recall vs. precision of MUC-4 systems on TST4.Recall.8.6.4.2I I Ioo  ?TST4 Data @TST4 ChanceI I I 0 (~ u0 .2 .4 .6 .8 1FalloutFigure 8: System level text filtering: fallout vs. recall (ROC plot) of MUC-4 systems on TST4.63sets.
Displays of fallout vs. recall are often called ROC (relative operating characteristic) plots andare widely used in signal detection and decision theory.
Along with the observed values for theMUC-3 and MUC-4 systems, we also plot the wLlues that would be produced by a system whichrandomly classified some fraction s of documents as relevant.
The graphs show how different valuesof 8 lead to different values of recall and fallout in such a system, while precision remains constant.AnalysisThe first thing that one notices about the system level text filtering results is the high level ofeffectiveness in absolute terms.
For instance, Table 3 shows that the text filtering F-measure scoreson TST3 range from .48 to .92 with a mean of .80 and a median of .82.
On the other hand, as theAI1Rel row shows, the F-measure for a system that simply guessed every story to be relevant wouldalso be .82.
The MUC-4 systems tack up somewhat better on TST4, but still not startlingly betterthan a system that did no filtering at all.We would like to know how much the text filtering ability of systems progressed from MUC-3 toMUC-4.
Unfortunately, both the testsets and the systems themselves changed between MUC-3 andMUC-4.
The TST2 and TST3 test sets were drawn from the same population of stories, but notrandomly, and we also do not have data on the w~riability between 100 message samples from thispopulation.
Thus, while 8 of the 11 sites who participated in both MUC-3 and MUC-4 increasedin their text filtering F-measure score (if we give ,equal weight to recall and precision, i.e.
j3 = 1.0),we do not know whether this was a result of improvements in system design, the change of test set,random fluctuation, or some combination of these.An issue of great interest is how robust MUC systems are to changes in the nature of their inputtext.
Here the comparsion i  effectiveness between TST3 and TST4 is informative, since exactlythe same system configurations were run on the two data sets.
TST4 was drawn from an earliertime slice of the same text stream as TST3 \[Sun92\], and so was a relatively mild test of portabilityto new texts.
Of the 17 MUC-4 systems, 12 declined in the text filtering F-measure (j3 = 1.0) and5 improved, from TST3 to TST4.
However, the mean change in F-measure across the 12 systemswhich declined was only 4.9. s This compares faw)rably with the decline of 11.0 in F-measure for astrategy of guessing every document to be relevant.However, going beyond the summary F-measure to the individual recall, precision, and fallouttext filtering scores reveals a different picture.
Examining Figures 5 and 7 shows that from TST3to TST4 there was a substantial decline in precision, coinciding with the decline in generality of thetestset or, equivalently, the expected precision of a randomly guessing system.
In contrast, falloutremains relatively stable between TST3 and TST4, as seen in Figures 6 and 8.We can quantify this by examining Table 2.
For the 17 MUC-4 systems, the mean decline inprecision from TST3 to TST4 was 7.9, while fallout increased by only 1.9.
Taking a trimmed mean\[Cha88\] by discarding the highest and lowest differences as outliers makes the distinction even moreapparent: precision drops by 8.0 but fallout increases by only 1.0.The ramifications of this are two-fold.
First, from the standpoint of choosing evaluation measuresfor system level text filtering, precision and, thereg~re, the F-measure, are handicapped by the strongcorrelation between precision and generality.
4 The correlation between precision and generality hasbeen pointed out in the IR literature \[Rob89\], but is more serious in the MUC testsets.
The generalityfor queries in a typical IR test collection is quite low.
For example, generality for the 64 queriesin the CACM-3204 test collection \[FNL88\] ranges from 0.000 to 0.016, putting a relatively lowfloor on precision.
This contrasts with generalities for TST2, TST3, and TST4 of .66, .69, and .55respectively, which constrain precision considerably.Secondly, and more important, the generality of real world text streams will be much lowerthan that of the MUC test sets.
The MUC data sets were a small subset of the original full textaThe mean change in F-measure for all 17 MUC-4 systems, including the tlve that increased in F-meuure thatincreased was a decrease ofonly 1.4.
However, that figure is heavily affected by two systems that did much better onTST4 than TST3, but nevertheless did poorly on both.4 It would, of course, be possible to deKue a single rneasure similar to the F-measure but based on recall and fallout,and this might be desirable.64databases of messages \[Sun91\].
First, only messages categorized as being about Latin America wereused.
Against that subset, a boolean query was used to find messages which contained a countryname of interest as well as one of a set of 24 words highly suggestive of terrorist incidents wereincluded in the MUC training and test sets.
This further restriction resulted in a subset about 15%of the size of the original Latin American set \[Sun92b\].The fact that only a small proportion of the database was used suggests that the raw textstream has a much lower generality than the MUC test sets.
If MUC systems have constant falloutor, equivalently, precision which drops in proportion to generality, then system level text filteringeffectiveness would be much lower on the original FBIS text stream than it was on the MUC subsets.Overall data extraction effectiveness is unlikely to be high if systems do poorly even at distinguishingfrom which documents to extract data.Is the assumption of constant fallout too pessimistic?
For one query-based text retrieval testcollection, Salton found that both precision and fallout decreased by factors of 2 to 3 when generalitywas decreased by a factor of 7 \[Sa172\].
Thus precision decreased at a slower rate than would be thecase if fallout was constant, but still at a substantial rate.Even these figures are probably overoptimistic for the MUC case.
Salton's experiment involvedexpanding the collection by a set of documents containing no relevant documents.
The transitionfor the MUC testset to the raw FBIS data stream would involve adding new relevant as well asnonrelevant documents, and those new relevant documents would be harder to detect than thecurrent ones (since they would not contain any of the 24 suggestive keywords, and would not havebeen classified by FBIS as being about Latin America).
In order to obtain the same level of recallas on the MUC testsets, still more of a sacrifice in preclsion/fallout would likely need to be made.ConclusionsIn this article we introduced a distinction between system level text filtering, a characteristic whichcan be measured for any data extraction system, and component level text filtering, a strategy whichis used by some data extraction systems.
While we discussed the architectural choices that MUC-3and MUC-4 systems have made with respect o component level text filtering, there was relativelylittle we could say about the ramifications of those choices, since few sites tested turning off orvarying the nature of their text filtering components.
We encourage more experiments of this sort.We also note that text filtering components, particularly those which operate on raw words or oncompleted templates rather than internal representations, are good candidates for interchangableparts in data extraction systems.We were able to draw somewhat stronger conclusions about system level text filtering in MUCsystems.
The MUC-3 and MUC-4 systems exhibited a high absolute level of effectiveness at systemlevel text filtering.
We pointed out, however, that on the MUC test sets, a level of effectivenessequal to that of some systems could be achieved by blindly guessing which documents were relevant.Of more consequence were the differences in text filtering effectiveness between test sets TST3and TST4.
While TST4 was chosen to provide a different ime slice than TST3, it was more notablefor having a lower generality (proportion of relevant documents) than TST3.
The fact that precisiondeclined on the average by 7.9 points from TST3 to TST4, while fallout increased by 1.0 points,raises the possibility that the effectiveness ofMUC systems may drop dramatically if they are appliedto real world text streams.We also noted that, due to the correlation between precision and generality, fallout is a moreuseful effectiveness measure for understanding system behavior than is precision, even if operationalrequirements may sometimes more conveniently be stated in terms of precision.Verifying whether this is the case will require testing of system level text filtering effectiveness onmuch larger databases, with a generality level similar to that of real world data streams.
Fortunately,this is practical, since a large number of documents can be judged for relevance in the time it takesto create a data extraction answer key for a single document.
Such an approach would also supportthe construction ofbetter data extraction test sets, since a wide range of types of relevant documentscould be sampled from, not just those which contain obvious keyword clues.65As a final note, we would like to stress the iimportance of measuring the degree of agreementamong human coders both for the judging of relevance and the filling out of answer key templates.All analyses of the MUC-3 and MUC-4 data must be tempered by the fact that we do not know thisdegree of agreement, which research in IR has shown might be only 60% or lower \[Cle91\].AcknowledgmentsWe thank DARPA for supporting the authors' travel to the MUC-4 workshop, and Beth Sundheimfor her aid with this paper and her splendid handling of the MUC-4 process.References\[ChaS8\] Chatfield, Christopher.
Problem Solving: A St~tistician's Guide.
Chapman and Hall, Lon-don, 1988.\[Chi92\] Chinchor, Nancy.
MUC-4 evaluation metrics.
In this volume.\[Chi92b\] Chinchor, Nancy.
The statistical significance of the MUC-4 results.
In this volume.\[CHL92\] Chinchor, Nancy; Hirschman, Lynette; and Lewis, David D. Evaluating message un-derstanding systems: An analysis of the third message understanding conference (MUC-3).Submitted to Computational I.inguistica, 1992.\[Cle91\] Cleverdon, Cyril W. The significance of the Cranfield tests of index languages.
In FourteenthInternational Conference on Research H Development in Information Retrieval, pages 3-12,1991.\[FNL88\] Fox, Edward A., Nunn, Gary L., and Lee, Whay C. Coefficients for combining conceptclasses in a collection.
In Eleventh Internal~ional Conference on Research H Development inInformation tLetrie~al, pages 291-307, 1988.\[Rob69\] Robertson, S. E. The parametric description of retrieval tests.
Part I: The basic parameters.Journal of Documentation, 25(1):1-27, 1969.\[Sal72\] Salton, G. The Ugenera\]ity" effect and the retrieval evaluation for large collections.
Journalof the American SocietTi for Inform~,tion Science, pages 11-22, January-February, 1972.\[Sun91\] Sundheim, Beth M. Overview of the Third Message Understanding Evaluation and Con-ference.
In Proceedings of the Third Message Understanding Evalltation and Conference, LosAltos, CA: Morgan Kaufmann, pages 3-16, May 1991.\[Sun92\] Sundheim, Beth M. Overview of the Fourth Message Understanding Evaluation and Con-ference.
In this volume.\[Sun92b\] Sundhelm, Beth M. Personal communication.
August 13, 1992.\[Swe69\] Swots, John A.
Effectiveness of information retrieval methods.
American Documentation,pages 72-89, January, 1969.\[van79\] van Rijsbergen, C. J.
Information Retrie.val.
Butterworths, London, second edition, 1979.
