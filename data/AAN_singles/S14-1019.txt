Proceedings of the Third Joint Conference on Lexical and Computational Semantics (*SEM 2014), pages 151?159,Dublin, Ireland, August 23-24 2014.Vagueness and Learning: A Type-Theoretic ApproachRaquel Fern?andezInstitute for Logic, Languageand ComputationUniversity of Amsterdamraquel.fernandez@uva.nlStaffan LarssonDepartment of Philosophy, Linguisticsand Theory of ScienceUniversity of Gothenburgsl@ling.gu.seAbstractWe present a formal account of the mean-ing of vague scalar adjectives such as ?tall?formulated in Type Theory with Records.Our approach makes precise how percep-tual information can be integrated intothe meaning representation of these pred-icates; how an agent evaluates whether anentity counts as tall; and how the proposedsemantics can be learned and dynamicallyupdated through experience.1 IntroductionTraditional semantic theories such as those de-scribed in Partee (1989) and Blackburn andBos (2005) offer precise accounts of the truth-conditional content of linguistic expressions, butdo not deal with the connection between meaning,perception and learning.
One can argue, however,that part of getting to know the meaning of lin-guistic expressions consists in learning to identifythe individuals or the situations that the expres-sions can describe.
For many concrete words andphrases, this identification relies on perceptual in-formation.
In this paper, we focus on characteris-ing the meaning of vague scalar adjectives suchas ?tall?, ?dark?, or ?heavy?.
We propose a for-mal account that brings together notions from tra-ditional formal semanticswith perceptual informa-tion, which allows us to specify how a logic-basedinterpretation function is determined and modifieddynamically by experience.The need to integrate language and percep-tion has been emphasised by researchers work-ing on the generation and resolution of referringThis work is licensed under a Creative Commons Attribution4.0 International Licence.
Page numbers and proceedingsfooter are added by the organisers.
Licence details: http://creativecommons.org/licenses/by/4.0/expressions (Kelleher et al., 2005; Reiter et al.,2005; Portet et al., 2009) and, perhaps even morestrongly, on the field of robotics, where ground-ing language on perceptual information is criticalto allow artificial agents to autonomously acquireand verify beliefs about the world (Siskind, 2001;Steels, 2003; Roy, 2005; Skocaj et al., 2010).Most of these approaches, however, do not buildon theories of formal semantics for natural lan-guage.
Here we choose to formalise our accountin a theoretical framework known as Type Theorywith Records (TTR), which has been shown to besuitable for formalising classic semantic aspectssuch as intensionality, quantification, and nega-tion (Cooper, 2005a; Cooper, 2010; Cooper andGinzburg, 2011) as well as less standard phenom-ena such as linguistic interaction (Ginzburg, 2012;Purver et al., 2014), perception and action (Dob-nik et al., 2013), and semantic coordination andlearning (Larsson, 2009).
In this paper we useTTR to put forward an account of the semantics ofvague scalar predicates like ?tall?
that makes pre-cise how perceptual information can be integratedinto their meaning representation; how an agentevaluates whether an entity counts as tall; and howthe proposed semantics for these expressions canbe learned and dynamically updated through lan-guage use.We start by giving a brief overview of TTR andexplaining how it can be used for classifying en-tities as being of particular types integrating per-ceptual information.
After that, in Section 3, wedescribe the main properties of vague scalar pred-icates.
Section 4 presents a probabilistic TTR for-malisation of the meaning of ?tall?, which capturesits context-dependence and its vague character.
InSection 5, we then offer an account of how thatmeaning representation is acquired and updatedwith experience.
Finally, in Section 6 we discussrelated work, before concluding in Section 7.1512 Meaning as Classification in TTRIn this section we give a brief and hence inevitablypartial introduction to Type Theory with Records.For more comprehensive introductions, we referthe reader to Cooper (2005b) and Cooper (2012).2.1 Type Theory with Records: Main NotionsAs in any type theory, the most central notion inTTR is that of a judgement that an object a isof type T , written as a : T .
In TTR judgementsare seen as fundamentally related to perception, inthe sense that perceiving inherently involves cate-gorising what we perceive.
Some common basictypes in TTR are Ind (the type of individuals) andR+(the type of positive real numbers).
All basictypes are members of a special type Type.
Giventypes T1and T2, we can create the function typeT1?
T2 whose domain are objects of type T1and whose range are objects of type T2.
Typescan also be constructed from predicates and ob-jects P (a1, .
.
.
, an).
Such types are called ptypesand correspond roughly to propositions in first or-der logic.
In TTR, propositions are types of proofs,where proofs can be a variety of things, from situ-ations to sensor readings (more on this below).Next, we introduce records and record types.These are structured objects made up of pairs ?l, v?of labels and values that are displayed in a matrix:(1) a.
A record type:???
?`1: T1`2: T2(`1).
.
.`n: Tn(`1, `2, .
.
.
, `n?1)????b.
A record: r =????
?`1= a1`2= a2.
.
.`n= an.
.
.????
?Record r in (1b) is of the record type in (1a) ifand only if a1: T1, a2: T2(a1), .
.
.
, and an:Tn(a1, a2, .
.
.
, an?1).
Note that the record maycontain more fields but would still be of type (1a)if the typing condition holds.
Records and recordtypes can be nested so that the value of a label isitself a record (or record type).
We can use pathswithin a record or record type to refer to specificbits of structure: for instance, we can use r.`2torefer to a2in (1b).As can be seen in (1a), the labels `1, .
.
.
`nin arecord type can be used elsewhere to refer to thevalues associated with them.
This is a commonway of constructing ptypes where the argumentsof a predicate are entities that have been intro-duced before in the record type.
A sample recordand record type are shown in (2).(2)?
?x = acman= prf(man(a))crun= prf(run(a))??:?
?x : Indcman: man(x)crun: run(x)?
?In (2), a is an entity of type individual and prf(P )is used as a placeholder for proofs of ptypes P .In the record type above, the ptypes man(x) andrun(x) constructed from predicates are dependenton x (introduced earlier in the record type).2.2 Perceptual MeaningLarsson (2013) proposes a system formalised inTTR where some perceptual aspects of meaningare represented using classifiers.
For example, themeaning of ?right?
(as in ?to the right of ?)
involvesa two-input perceptron classifier ?right(w, t, r),specified by a weight vector w and a thresholdt, which takes as input a context r including anobject x and a position-sensor reading srpos.
Thesensor reading consists of a vector containing tworeal numbers representing the space coordinates ofx.
The classifier classifies x as either being to theright on a plane or not.1(3) if r :[x : Indsrpos: RealVector], then?right(w, t, r) ={right(r.x) if (r.srpos?
w) > t?
right(r.x) otherwiseAs output we get a record type containing either aptype right(x) or its negation, ?
right(x).
Larsson(2013) proposes that readings from sensors maycount as proofs of such ptypes.
A classifier canbe used for judging x as being of a particular typeon the grounds of perceptual information.
A per-ceptual proof for right(x) would thus include theoutput from the position sensor that is directed to-wards x.
Here, this output would be the space co-ordinates of x.3 Vague Scalar PredicatesScalar predicates such as ?tall?, ?long?
and ?ex-pensive?, also called ?relative gradable adjectives?
(Kennedy, 2007), are interpreted with respect to a1We are here assuming that we have a definition of dotproduct for TTR vectors a:RealVectornand b:RealVectornsuch that a ?
b = ?ni=1aibi= a1b1+ a2b2+ .
.
.+ anbn.
Wealso implicitly assume that the weight vector and the sensorreading vector have the same dimensionality.152scale, i.e., a dimension such as height, length, orcost along which entities for which the relevant di-mension is applicable can be ordered.
This makesscalar predicates compatible with degree morphol-ogy, like comparative and superlative morphemes(?taller than?, ?the longest?)
and intensifier mor-phemes such as ?very?
or ?quite?.
In this pa-per, our focus is on the so-called positive form ofthese adjectives (e.g.
?tall?
as opposed to ?taller?or ?tallest?
).A property that distinguishes the positive formfrom the comparative and the superlative forms isits context-dependance.
To take a common exam-ple: If Sue?s height is 180cm, she may be appro-priately described as a tall woman, but probablynot as a tall basketball player.
Thus, what countsas tall can vary from context to context, with themost relevant contextual parameter being a com-parison class relative to which the adjective is in-terpreted (e.g., the set of women, the set of bas-ketball players, etc.).
In addition to being context-dependent, positive-form scalar predicates are alsovague, in the sense that they give rise to borderlinecases, i.e., entities for which it is unclear whetherthe predicate holds or not.Vagueness is certainly a property that affectsmost natural language expressions, not only scalaradjectives.
However, scalar adjectives have arelatively simple semantics (they are often uni-dimensional) and thus constitute a perfect case-study for investigating the properties and effects ofvagueness on language use.
Gradable adjectiveshave received a high amount of attention in theformal semantics literature.
It is common to dis-tinguish between two main approaches to their se-mantics: delineation-based and degree-based ap-proaches.
The delineation approach is associatedwith the work of Klein (1980), who proposes thatgradable adjectives denote partial functions de-pendent on a comparison class.
They partition thecomparison class into three disjoint sets: a positiveextension, a negative extension, and an extensiongap (entities for which the predicate is neither truenor false).
In contrast, degree-based approachesassume a measure function m mapping individu-als x to degrees on a particular scale (degrees ofheight, degrees of darkness, etc.)
and a standardof comparison or degree threshold ?
(again, de-pendent on a comparison class) such that x be-longs to the adjective?s denotation if m(x) > ?
(Kamp, 1975; Pinkal, 1979; Pinkal, 1995; Barker,2002; Kennedy and McNally, 2005; Kennedy,2007; Solt, 2011; Lassiter, 2011).We build on degree approaches but adopt aperception-based perspective and take a step fur-ther to formalise how the meaning of these pred-icates can be learned and constantly updatedthrough language use.4 A Perceptual Semantics for ?Tall?To exemplify our approach, we will use the scalarpredicate ?tall?
throughout.4.1 Context-sensitivityWe first focus on capturing the context-dependence of relative scalar predicates.
Forthis we define a type Tctxtas follows:(4) Tctxt=?
?c : Typex : ch : R+?
?The context (ctxt) of a scalar predicate like ?tall?is a record of the type in (4), which includes: atype c (typically a subtype of Ind) representing thecomparison class; an individual x within the com-parison class (the argument of tall); a perceivedmeasure on the relevant scale(s), in this case theperceived height h of x expressed as a positive realnumber.The context presupposes the acquisition of sen-sory input from the environment.
In particular, itassumes that an agent using such a representationis able to classify the entity in focus x as beingof type c and is able to use some height sensor toobtain an estimate of x?s height (the value of h isthe sensor reading).
We thus forgo the inclusion ofan abstract measure function in the representation.In an artificial agent, this may be accomplished byimage processing software for detecting and mea-suring objects in a digital image.Besides the ctxt, we also assume a standardthreshold of tallness ?tallof the type given in (5).
?tallis a function from a type specifying a com-parison class to a height value, which correspondsto a tallness threshold for that comparison class.
(In Section 5 we will discuss how such a thresholdmay be computed.
)(5) ?tall: Type?
R+The meaning of ?tall?
involves a classifier for tall-ness, ?tall, of the following type:(6) ?tall: (Type?
R+, Tctxt)?
Type153We define this classifier as a one-input perceptronthat compares the perceived height h of an indi-vidual x to the relevant threshold ?
determined bya comparison class c. Thus, if ?
: Type?
R+andr : Tctxt, then:?tall(?, r) ={tall(r.x) if r.h > ?
(r.c)?tall(r.x) otherwiseSimplifying somewhat, we can represent the mea-ning of ?tall?, tall, as a record specifying the typeof context (Tctxt) where an utterance of ?tall?
canbe made, the parameter of the tallness classifier(the threshold ?
), and a function f which is appliedto the context to produce the content of ?tall?.
(7)tall =??????????Tctxt=?
?c : Typex : ch : R+???
= ?tallf = ?r : Tctxt.
[sit = rsit-type =[ctall: ?tall(?, r)]]?????????
?The output of the function f is an Austinian propo-sition (Cooper, 2005b): a judgement that a situa-tion (sit, represented as a record r of type Tctxt),is of a particular type (specified in sit-type).
In thecase of tall, the context of utterance (which instan-tiates r) is judged to be of the type where there isan individual x which is either tall or not tall, ac-cording to the output of the classifier ?tall.
Thecontext of utterance in the sit field will include theheight-sensor reading, which means that the sen-sor reading is part of the proof of the sit-type indi-cating that x is tall (or not, as the case may be).Thus, to decide whether to refer to some indi-vidual x as tall or to evaluate someone else?s utter-ance describing x as tall, an agent applies the func-tion tall.f to the current situation, represented as arecord r : Tctxt.
As an example, let us consider asituation that includes the context in (8), resultingfrom observing John Smith as being 1.88 meterstall (assuming this is our scale of tallness):(8) ctxt =?
?c = Humanx = john smithh = 1.88?
?Let us assume that given the comparison classHuman, ?tall(Human) = 1.87.
In this case,tall.f(ctxt) will compute as shown in (9).
The re-sulting Austinian proposition corresponds to theagent?s judgement that the situation in sit is onewhere John Smith counts as tall.
(9) ?r : Tctxt.
[sit = rsit-type =[ctall: ?tall(?tall, r)]](?
?c = Humanx = john smithh = 1.88??)
=???
?sit =?
?c = Humanx = john smithh = 1.88?
?sit-type =[ctall: tall(john smith)]???
?4.2 VaguenessAccording to the above account, ?tall?
has aprecise interpretation: given a degree of heightand a comparison class, the threshold sharplydetermines whether tall applies or not.
Thereare several ways in which one can account forvagueness?amongst others, by introducing per-ceptual uncertainty (possibly inaccurate sensorreadings).
Here, in line with Lassiter (2011), weopt for substituting the precise threshold with anoisy, probabilistic threshold.
We consider thethreshold to be a normal random variable, whichcan be represented by the parameters of its Gaus-sian distribution, the mean ?
and the standard de-viation ?
(the noise width).2To incorporate this modification into our ap-proach, we update the tallness classifier ?tallwehad defined in (6) so that it now takes as parame-ters ?talland ?tall, both of them dependent on thecomparison class and hence of type Type?
R+.The output of the classifier is now a probabilityrather than a ptype such as tall(x) or?tall(x).
Be-fore indicating how this probability is computed,we give the type of the vague version of the clas-sifier in (10) and the vague representation of themeaning of ?tall?
in (11).
(10)?tall: (Type?R+, Type?R+, Tctxt)?
[0, 1](11)tall =??????????????Tctxt=?
?c : Typex : ch : R+???
= ?tall?
= ?tallf = ?r : Tctxt.?
?sit = rsit-type =[ctall: tall(r.x)]prob = ?tall(?, ?, r)???????????????
?2Which noise function may be the most appropriate is anempirical question we do not tackle in this paper.
Our choiceof Gaussian noise follows Schmidt et al.
(2009)?see Sec-tion 5.1.154The output of the function tall.f is now a prob-abilistic Austinian proposition (Cooper et al.,2014).
Like before, the proposition expresses ajudgement that a situation sit is of a particulartype.
But here the judgement is probabilistic?itencodes the belief of an agent concerning the like-lihood that sit is of a type where x counts as tall.Since we take the noisy threshold to be a normalrandom variable, given a particular ?
and ?, wecan calculate the probability that the height r.h ofindividual r.x counts as tall as follows:?tall(?, ?, r) =12[1 + erf(r.h?
?(r.c)?
(r.c)?2)]Here erf is the error function, defined as3erf(x) =2?pi?xt=0e?t2dtThe error function defines a sigmoid shape (seeFigure 1), in line with the upward monotonicityof ?tall?.
The output of ?tall(?, ?, r) correspondsto the probability that h will exceed the normalrandom threshold with mean ?
and deviation ?.Figure 1: Plot of the error function.Let us consider an example.
Assume that we have?tall(Human) = 1.87 and ?tall(Human) = 0.05(see Section 5.1 below for justification of the lattervalue).
Let?s also assume the same ctxt as abovein (8).
In this case, tall.f(ctxt) will compute as in(12), given that?tall(?tall, ?tall,?
?c=Humanx=john smithh=1.88??)
=12[1 + erf(1.88?
1.870.05?2)]= 0.5793For an explanation of this standard definition, see http://en.wikipedia.org/wiki/Error_function,which is the source of the graph in Figure 1.
(12) ?r : Tctxt.?
?sit = rsit-type =[ctall: tall(r.x)]prob = ?tall(?tall, ?tall, r)??(?
?c = Humanx = john smithh = 1.88??)
=?????
?sit =?
?c = Humanx = john smithh = 1.88?
?sit-type =[ctall: tall(john smith)]prob = 0.579?????
?This probability can now be used in further prob-abilistic reasoning, to decide whether to refer toan individual x as tall, or to evaluate someoneelse?s utterance describing x is tall.
For exam-ple, an agent may map different probabilities todifferent adjective qualifiers of tallness to yieldcompositional phrases such as ?sort of tall?, ?quitetall?, ?very tall?, ?extremely tall?, etc.
The mean-ings of these composed adjectival phrases couldspecify probability ranges trained independently.Compositionality for vague perceptual meanings,and the interaction between compositionality andlearning, is an exciting area for future research.45 Learning from Language UseIn this section we consider possibilities for com-puting the noisy threshold we have introducedin the previous section and discuss how such athreshold and the probabilistic judgements it givesrise to are updated with language use.5.1 Computing the Noisy ThresholdWe assume that agents keep track of judgementsmade by other agents.
More concretely, for avague scalar predicate like ?tall?, we assume thatan agent will have at its disposal a set of obser-vations consisting of entities of a particular typeT (a comparison class such as Human) that havebeen judged to be tall, together with their observedheights.
Judgements of tallness may vary acrossindividuals?indeed, such variation (both inter-and intra-individual) is a hallmark of vague pred-icates.
We use ?Ttallto refer to the set of heightsof those entities x : T that have been consideredtall by some individual.
From this agent-specificset of observations, which is constantly updated asthe agent is exposed to new judgements by otherindividuals, we want to compute a noisy threshold,4See Larsson (2013) for a sketch of compositionality forperceptual meaning.155which the agent uses to make her own judgementsof tallness, as specified in (11).Different functions can be used to compute ?talland ?tallfrom ?Ttall.
What constitutes an appro-priate function is an empirical matter and whatthe most suitable function is possibly varies acrosspredicates (what may apply to ?tall?
may not besuitable for ?dark?
or ?expensive?, for example).Hardly any work has been done on trying to iden-tify how the threshold is computed from experi-ence.
A notable exception, however, is the work ofSchmidt et al.
(2009), who collect judgements ofpeople asked to indicate which items are tall givendistributions of items of different heights.
Schmidtand colleagues then propose different probabilis-tic models to account for the data and comparetheir output to the human judgements.
They ex-plore two types of models: threshold-based mod-els and category-based or cluster models.
The bestperforming models within these two types performequally well and the study does not identify anyadvantages of one type over the other one.
Sincewe have chosen threshold models as our case-study, we focus our attention on those here.Each of the threshold models tested by Schmidtet al.
(2009) corresponds to a possible way of com-puting the mean ?tallof a noisy threshold from aset of observations.
The best performing thresholdmodel in their study is the relative height by rangemodel, where (in our notation):(13) relative height by range (RH-R): ?tall(T ) =max(?Ttall)?
k ?
(max(?Ttall)?min(?Ttall))Here max(?Ttall) and min(?Ttall) stand for themaximum and the minimum height, respectively,of the items that have been judged to be tallby some individual.
According to this thresholdmodel, any item within the top k% of the rangeof heights that have been judged to be tall countsas tall.
The model includes two parameters, k anda noise-width parameter that in our approach cor-responds to ?tall.
Schmidt et al.
(2009) reportthat the best fit of their data was obtained withk = 29% and ?tall= 0.05.5.2 Updating Vague MeaningsWe now want to specify how the vague meaningof ?tall?
is updated as an agent is exposed to newjudgements via language use.
Our setting so faroffers a straightforward solution to this: If a newentity x : T with height h is referred to as tall, theagent adds h to its set of observations ?Ttallandrecomputes ?tall(Human), for instance using RH-R as defined in (13).
If RH-H is used, ideally thevalue of k and ?tallshould be (re)estimated from?Ttall.
For the sake of simplicity, however, herewe will assume that these two parameters take thevalues experimentally validated by Schmidt et al.
(2009) and are kept constant.
An update to ?tallwill take place if it is the case that h > max(?Ttall)or h < min(?Ttall).
This in turn will trigger unupdate to the probability outputted by ?tall.As an example, let us assume that ourinitial set of observations is ?Humantall={1.87, 1.92, 1, 90, 1.75, 1.80} (recall this corre-sponds to the perceived heights of individualsthat have been described as tall by some agent).This means that max(?Humantall) = 1.92 andmin(?Humantall) = 1.75.
Hence, given (13):(14) ?tall(Human) =1.92?
0.29 ?
(1.92?
1.75) = 1.87Let?s assume we now make an observation wherea person of height 1.72 is judged to be tall.
Thiswill mean that the set of observations is now?Humantall= {1.87, 1.92, 1, 90, 1.75, 1.80, 1.72}and consequently min(?Humantall) = 1.72, whichyields an updated mean of the noisy threshold:(15) ?tall(Human) =1.92?
0.29 ?
(1.92?
1.72) = 1.862If we were to re-evaluate John Smith?s tallness inlight of this observation, we would get a new prob-ability 0.64 that he is tall (in contrast to the earlierprobability of 0.579 given in (12)).5.3 Possible ExtensionsThe set of observations ?Humantallcan be derivedfrom a set of Austinian propositions correspond-ing to instances where people have been judgedto be tall.
To update from an Austinian proposi-tion p we simply add p.sit.h to ?tallHumanand re-compute ?tall(p.c).
Note that we are here treatingthese Austinian propositions as non-probabilistic.This seems to make sense since an addressee doesnot have direct access to the probability associatedwith the judgement of the speaker.
If we were totake these probabilities into account (for instance,the use of a hedge in ?sort of tall?
may be usedto make inferences about such probabilities), andif those probabilities are not always 1, we wouldneed a different way of computing ?tallthan the156one specified so far.Somewhat related to the point above, note thatin our approach we treat all judgements equally,i.e., we do not distinguish between possible dif-ferent levels of trustworthiness amongst speakers.An agent who is told that an entity with height his tall adds that observation to its knowledge basewithout questioning the reliability of the speaker.This is clearly a simplification.
For instance, thereis developmental evidence showing that childrenare more sensitive to reliable speakers than to un-reliable ones during language acquisition (Scofieldand Behrend, 2008).6 Other ApproachesWithin the literature in formal semantics, Las-siter (2011) has put forward a proposal that ex-tends in interesting ways earlier work by Barker(2002) and shares some aspects with the accountwe have presented here.
Operating in a probabilis-tic version of classical possible-worlds semantics,Lassiter assumes a probability distribution over aset of possible worlds and a probability distribu-tion over a set of possible languages.
Each pos-sible language represents a precise interpretationof a predicate like ?tall?
: tall1= ?x.x?s height ?5?6?
; tall2= ?x.x?s height ?
5?7?
; and so forth.Lassiter thus treats ?metalinguistic belief?
(repre-senting an agent?s knowledge of the meaning ofwords) in terms of probability distributions overprecise languages.
Since each precise interpreta-tion of ?tall?
includes a given threshold, this canbe seen as defining a probability distribution overpossible thresholds, similarly to the noisy thresh-old we have used in our account.
Lassiter, how-ever, is not concerned with learning.Within the computational semantics literature,DeVault and Stone (2004) describe an imple-mented system in a drawing domain that is able tointerpret and execute instructions including vaguescalar predicates such as ?Make a small circle?.Their approach makes use of degree-based seman-tics, but does not take into account comparisonclasses.
This is possible in their drawing domainsince the kind of geometric figures it includes(squares, rectangles, circles) do not have intrinsicexpected properties (size, length, etc).
Their focusis on modelling how the threshold for a predicatesuch as ?small?
is updated during an interactionwith the system given the local discourse context.For instance, if the initial context just contains asquare, the size of that square is taken to be thestandard of comparison for the predicate ?small?.The user?s utterance ?Make a small circle?
is theninterpreted as asking for a circle of an arbitrarysize that is smaller than the square.In our characterisation of the context-sensitivityof vague gradable adjectives in Section 4.1, wehave focused on their dependence on general com-parison classes corresponding to types of entities(such as Human, Woman, etc) with expected prop-erties such as height.
Thus, in contrast to DeVaultand Stone (2004), who focus on the local contextof discourse, we have focused on what could becalled the global context (an agent?s experience re-garding types of entities and their expected prop-erties).
How these two types of context interactremains an open question, which we plan to ex-plore in our future work (see Kyburg and Morreau(2000), Kemp et al.
(2007), and Fern?andez (2009)for pointers in this direction).7 Conclusions and future workTraditional formal semantics theories postulate afixed, abstract interpretation function that medi-ates between natural language expressions and theworld, but fall short of specifying how this func-tion is determined or modified dynamically byexperience.
In this paper we have presented acharacterisation of the semantics of vague scalarpredicates such as ?tall?
that clarifies how theircontext-dependent meaning and their vague char-acter are connected with perceptual information,and we have also shown how this low-level per-ceptual information (here, real-valued readingsfrom a height sensor) connects to high level logicalsemantics (ptypes) in a probabilistic framework.In addition, we have put forward a proposal forexplaining how the meaning of vague scalar ad-jectives like ?tall?
is dynamically updated throughlanguage use.Tallness is a function of a single value (height),and is in this sense a uni-dimensional pred-icate.
Indeed, most linguistic approaches tovagueness focus on uni-dimensional predicatessuch as ?tall?.
However, many vague predicatesare multi-dimensional, including nouns for posi-tions (?above?
), shapes (?hexagonal?
), and colours(?green?
), amongst many others.
Together withcompositionality (mentioned at the end of Sec-tion 4.2), generalisation of the present account tomulti-dimensional vague predicates is an interest-ing area of future development.157AcknowledgementsThe first author acknowledges the support of theNetherlands Organisation for Scientific Research(NWO) and thanks the Centre for Language Tech-nology at the University of Gothenburg for gen-erously funding research visits that led to thework presented in this paper.
The second au-thor acknowledges the support of Vetenskapsr?adet,project 2009-1569, Semantic analysis of interac-tion and coordination in dialogue (SAICD); theDepartment of Philosophy, Linguistics, and The-ory of Science; and the Centre for Language Tech-nology at the University of Gothenburg.ReferencesChris Barker.
2002.
The dynamics of vagueness.
Lin-guistics & Philosophy, 25(1):1?36.Patrick Blackburn and Johan Bos.
2005.
Represen-tation and Inference for Natural Language: A FirstCourse in Computational Semantics.
CSLI Publica-tions.Robin Cooper and Jonathan Ginzburg.
2011.
Negationin dialogue.
In Proceedings of the 15th Workshop onthe Semantics and Pragmatics of Dialogue (SemDial2011), Los Angeles (USA).Robin Cooper, Simon Dobnik, Shalom Lappin, andStaffan Larsson.
2014.
A probabilistic rich typetheory for semantic interpretation.
In Proceedingsof the EACL Workshop on Type Theory and NaturalLanguage Semantics (TTNLS).Robin Cooper.
2005a.
Austinian truth, attitudes andtype theory.
Research on Language and Computa-tion, 3(4):333?362, December.Robin Cooper.
2005b.
Austinian truth, attitudes andtype theory.
Research on Language and Computa-tion, 3:333?362.Robin Cooper.
2010.
Generalized quantifiers and clar-ification content.
In Pawe?
?upkowski and MatthewPurver, editors, Aspects of Semantics and Pragmat-ics of Dialogue.
SemDial 2010, 14th Workshop onthe Semantics and Pragmatics of Dialogue, Pozna?n.Polish Society for Cognitive Science.Robin Cooper.
2012.
Type theory and semantics influx.
In Ruth Kempson, Nicholas Asher, and TimFernando, editors, Handbook of the Philosophy ofScience, volume 14: Philosophy of Linguistics.
El-sevier BV.
General editors: Dov M. Gabbay, PaulThagard and John Woods.David DeVault and Matthew Stone.
2004.
Interpret-ing vague utterances in context.
In Proceedings ofthe 20th International Conference on ComputationalLinguistics (COLING?04), pages 1247?1253.Simon Dobnik, Robin Cooper, and Staffan Larsson.2013.
Modelling language, action, and perceptionin type theory with records.
In Constraint Solvingand Language Processing, Lecture Notes in Com-puter Science, pages 70?91.
Springer.Raquel Fern?andez.
2009.
Salience and feature vari-ability in definite descriptions with positive-formvague adjectives.
In Workshop on the Productionof Referring Expressions: Bridging the gap betweencomputational and empirical approaches to refer-ence (CogSci?09).Jonathan Ginzburg.
2012.
The Interactive Stance.
Ox-ford University Press.Hans Kamp.
1975.
Two theories of adjectives.
InE.
Keenan, editor, Formal Semantics of Natural Lan-guage, pages 123?155.
Cambridge University Press.John Kelleher, Fintan Costello, and Josef van Genabith.2005.
Dynamically structuring, updating and inter-relating representations of visual and linguistic dis-course context.
Artificial Intelligence, 167(1):62?102.Charles Kemp, Amy Perfors, and Joshua B. Tenen-baum.
2007.
Learning overhypotheses with hier-archical bayesian models.
Developmental Science,10(3):307?321.Christopher Kennedy and Louise McNally.
2005.Scale structure, degree modification, and the seman-tics of gradable predicates.
Language, pages 345?381.Christopher Kennedy.
2007.
Vagueness and grammar:The semantics of relative and absolute gradable ad-jectives.
Linguistics and Philosophy, 30(1):1?45.Ewan Klein.
1980.
A semantics for positive andcomparative adjectives.
Linguistics and Philosophy,4:1?45.Alice Kyburg and Michael Morreau.
2000.
Fittingwords: Vague language in context.
Linguistics andPhilosophy, 23:577?597.Staffan Larsson.
2009.
Detecting and learning fromlexical innovation in dialogue: a ttr account.
InProceedings of the 5th International Conference onGenerative Approaches to the Lexicon.Staffan Larsson.
2013.
Formal semantics for percep-tual classification.
Journal of Logic and Computa-tion.Dan Lassiter.
2011.
Vagueness as probabilistic linguis-tic knowledge.
In R. Nowen, R. van Rooij, U. Sauer-land, and H. C. Schmitz, editors, Vagueness in Com-munication.
Springer.Barbara Partee.
1989.
Possible worlds in model-theoretic semantics: A linguistic perspective.
InS.
Allen, editor, Possible Worlds in Humanities, Artsand Sciences, pages 93?123.
Walter de Gruyter.158Manfred Pinkal.
1979.
Semantics from differentpoints of view.
In R. B?aurle, U. Egli, and A. vonStechow, editors, How to Refer with Vague Descrip-tions, pages 32?50.
Springer-Verlag.Manfred Pinkal.
1995.
Logic and lexicon: the seman-tics of the indefinite, volume 56 of Studies in Lin-guistics and Philosophy.
Springer.Franc?ois Portet, Ehud Reiter, Albert Gatt, Jim Hunter,Somayajulu Sripada, Yvonne Freer, and CindySykes.
2009.
Automatic generation of textual sum-maries from neonatal intensive care data.
ArtificialIntelligence, 173(7):789?816.Matthew Purver, Julian Hough, and Eleni Gre-goromichelaki.
2014.
Dialogue and compoundcontributions.
In A. Stent and S. Bangalore, ed-itors, Natural Language Generation in InteractiveSystems.
Cambridge University Press.Ehud Reiter, Somayajulu Sripada, Jim Hunter, Jin Yu,and Ian Davy.
2005.
Choosing words in computer-generated weather forecasts.
Artificial Intelligence,167(1):137?169.Deb Roy.
2005.
Semiotic schemas: A framework forgrounding language in action and perception.
Artifi-cial Intelligence, 167(1):170?205.L.A.
Schmidt, N.D. Goodman, D. Barner, and J.B.Tenenbaum.
2009.
How tall is tall?
composition-ality, statistics, and gradable adjectives.
In Proceed-ings of the 31st annual conference of the cognitivescience society.Jason Scofield and Douglas A Behrend.
2008.
Learn-ing words from reliable and unreliable speakers.Cognitive Development, 23(2):278?290.Jeffrey Mark Siskind.
2001.
Grounding the lexicalsemantics of verbs in visual perception using forcedynamics and event logic.
Journal of Artificial In-telligence Research, (15):31?90.Danijel Skocaj, M Janicek, Matej Kristan, Geert-Jan MKruijff, Ale?s Leonardis, Pierre Lison, Alen Vrecko,and Michael Zillich.
2010.
A basic cognitive sys-tem for interactive continuous learning of visualconcepts.
In Proceeding of the Workshop on Inter-active Communication for Autonomous IntelligentRobots, pages 30?36.Stephanie Solt.
2011.
Notes on the comparison class.In Vagueness in communication, pages 189?206.Springer.Luc Steels.
2003.
Evolving grounded communicationfor robots.
Trends in cognitive sciences, 7(7):308?312.159
