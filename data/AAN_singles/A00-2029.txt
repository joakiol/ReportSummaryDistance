Predicting Automatic Speech Recognition Performance UsingProsodic CuesDiane  J .
L i tman and Ju l ia  B .
H i rschberg  Marc  Swer tsAT&T Labs - -  Research IPO,  Center for User-System Interact ionF lo rham Park,  NJ 07932-0971 USA Eindhoven, The Nether lands{ diane,julia} @research.
att.com swerts@ipo.tue.nlAbst rac tIn spoken dialogue systems, it is important for asystem to know how likely a speech recognition hy-pothesis is to be correct, so it can reprompt forfresh input, or, in cases where many errors haveoccurred, change its interaction strategy or switchthe caller to a human attendant.
We have discov-ered prosodic features which more accurately predictwhen a recognition hypothesis contains a word errorthan the acoustic onfidence score thresholds tradi-tionally used in automatic speech recognition.
Wepresent analytic results indicating that there are sig-nificant prosodic differences between correctly andincorrectly recognized turns in the TOOT train in-formation corpus.
We then present machine learn-ing results howing how the use of prosodic featuresto automatically predict correct versus incorrectlyrecognized turns improves over the use of acousticconfidence scores alone.1 I n t roduct ionOne of the central tasks of the dialogue managerin most current spoken dialogue systems (SDSs) iserror handling.
The automatic speech recognition(ASR) component of such systems i prone to error,especially when the system has to operate in noisyconditions or when the domain of the system is large.Given that it is impossible to fully prevent ASR er-rors, it is important for a system to know how likelya speech recognition hypothesis to be correct, soit can take appropriate action, since users have con-siderable difficulty correcting incorrect informationthat is presented by the system as true (Krahmeret al, 1999).
Such action may include verifying theuser's input, reprompting for fresh input, or, in caseswhere many errors have occurred, changing the in-teraction strategy or switching the caller to a humanattendant (Smith, 1998; Litman et al, 1999; Langk-ilde et al, 1999).
Traditionally, the decision to re-ject a recognition hypothesis i based on acousticconfidence score thresholds, which provide a relia-bility measure on the hypothesis and are set in theapplication (Zeljkovic, 1996).
However, this processoften fails, as there is no simple one-to-one mappingbetween low confidence scores and incorrect recog-nitions, and the setting of a rejection threshold isa matter of trial and error (Bouwman et al, 1999).Also, some incorrect recognitions do not necessarilylead to misunderstandings at aconceptual level (e.g."a.m."
recognized as "in the morning").The current paper looks at prosody as one possiblepredictor of ASR performance.
ASR performanceis known to vary based upon speaking style (Wein-traub et al, 1996), speaker gender and age, na-tive versus non-native speaker status, and, in gen-eral, the deviation of new speech from the trainingdata.
Some of this variation is linked to prosody, asprosodic differences have been found to character-ize differences in speaking style (Blaauw, 1992) andidiosyncratic differences (Kraayeveld, 1997).
Sev-eral other studies (Wade et al, 1992; Oviatt et al,1996; Swerts and Ostendorf, 1997; Levow, 1998; Belland Gustafson, 1999) report that hyperarticulatedspeech, characterized by careful enunciation, slowedspeaking rate, and increase in pitch and loudness,often occurs when users in human-machine interac-tions try to correct system errors.
Still others haveshown that such speech also decreases recognitionperformance (Soltau and Waibel, 1998).
Prosodicfeatures have also been shown to be effective inranking recognition hypotheses, asa post-processingfilter to score ASR hypotheses (Hirschberg, 1991;Veilleux, 1994; Hirose, 1997).In this paper we present results of empirical stud-ies testing the hypothesis that prosodic features pro-vide an important clue to ASR performance.
Wefirst present results comparing prosodic analyses ofcorrectly and incorrectly recognized speaker turnsin TOOT, an experimental SDS for obtaining traininformation over the phone.
We then describe ma-chine learning experiments based on these resultsthat explore the predictive power of prosodic fea-tures alone and in combination with other automat-ically available information, including ASR confi-dence scores and recognized string.
Our results in-dicate that there are significant prosodic differencesbetween correctly and incorrectly recognized utter-ances.
These differences can in fact be used to pre-218dict whether an utterance has been misrecognized,with a high degree of accuracy.2 The  TOOT CorpusOur corpus consists of a set of dialogues betweenhumans and TOOT, an SDS for accessing trainschedules from the web via telephone, which wascollected to study both variations in SDS strat-egy and user-adapted interaction (Litman and Pan,1999).
TOOT is implemented on a platform com-bining ASR, text-to-speech, a phone interface, afinite-state dialogue manager, and application func-tions (Kamm et al, 1997).
The speech recognizer isa speaker-independent hidden Markov model systemwith context-dependent phone models for telephonespeech and constrained grammars for each dialoguestate.
Confidence scores for recognition were avail-able only at the turn, not the word, level (Zeljkovic,1996).
An example TOOT dialogue is shown in Fig-ure 1.Subjects performed four tasks with one of sev-eral versions of TOOT, that differed in terms of locusof initiative (system, user, or mixed), confirmationstrategy (explicit, implicit, or none), and whetherthese conditions could be changed by the user duringthe task.
Subjects were 39 students, 20 native speak-ers of standard American English and 19 non-nativespeakers; 16 subjects were female and 23 male.
Dia-logues were recorded and system and user behaviorlogged automatically.
The concept accuracy (CA) ofeach turn was manually labeled by one of the exper-imenters.
If the ASR output correctly captured allthe task-related information in the turn (e.g.
time,departure and arrival cities), the turn was given aCA score of 1 (a semantically correct recognition).Otherwise, the CA score reflected the percentage ofcorrectly recognized task information in the turn.The dialogues were also transcribed by hand andthese transcriptions automatically compared to theASR recognized string to produce a word error rate(WEPt) for each turn.
Note that a concept can becorrectly recognized even though all words are not,so the CA metric does not penalize for errors thatare unimportant to overall utterance interpretation.For the study described below, we examined 1994user turns from 152 dialogues in this corpus.
Thespeech recognizer was able to generate a recognizedstring and an associated acoustic confidence scoreper turn for 1975 of these turns.
1 1410 of these 1975turns had a CA score of 1 (for an overall conceptualaccuracy score of 71%) and 961 had a WER of 0 (foran overall transcription accuracy score of 49%, witha mean WER per turn of 47%).1For the remaining turns, ASR output "no speech" (andTOOT played a timeout message) or "garbage" (TOOT playeda rejection message).3 D is t ingu ish ing  Cor rect  f romIncor rect  Recogn i t ionsWe first looked for distinguishing prosodic charac-teristics of misrecognitions, defining misrecognitionsin two ways: a) as turns with WER>0; and b) asturns with CA<I.
As noted in Section 1, previousstudies have speculated that hyperarticulated speech(slower and louder speech which contains wider pitchexcursions) may be associated with recognition fail-ure.
So, we examined the following features for eachuser turn: 2?
maximum and mean fundamental frequencyvalues (F0 Max, F0 Mean)?
maximum and mean energy values (RMS Max,RMS Mean)?
total duration?
length of pause preceding the turn (Prior Pause)* speaking rate (Tempo)?
amount of silence within the turn (% Silence)F0 and I:LMS values, representing measures of pitchexcursion and loudness, were calculated from theoutput of Entropic Research Laboratory's pitchtracker, get_fO, with no post-correction.
Timing vari-ation was represented by four features.
Durationwithin and length of pause between turns was com-puted from the temporal labels associated with eachturn's beginning and end.
Speaking rate was ap-proximated in terms of syllables in the recognizedstring per second, while % Silence was defined as thepercentage of zero frames in the turn, i.e., roughlythe percentage of time within the turn that thespeaker was silent.
These features were chosen basedupon previous findings (see Section 1) and observa-tions from our data.To ensure that our results were speaker indepen-dent, we calculated mean values for each speaker'srecognized turns and their misrecognized turns forevery feature.
Then, for each feature, we createdvectors of speaker means for recognized and misrec-ognized turns and performed paired t-tests on thevectors.
For example, for the feature "F0 max",we calculated mean maxima for misrecognized turnsand for correctly recognized turns for each of ourthirty-nine speakers.
We then performed a pairedt-test on these thirty-nine pairs of means to de-rive speaker-independent results for differences in F0maxima between correct and incorrect recognitions.Tables 1 and 2 show results of these compar-isons when we calculate misrecognition i terms of2While the features were automatically computed, turnbeginnings and endings were hand segmented in dialogue-levelspeech files, as the turn-level files created by TOOT were notavailable.219Toot:User:Toot:User:Toot:Hi, this is AT&T Amtrak schedule system.
This is TOOT.
How may I help you?I want the trains from New York City to Washington DC on Monday at 9:30 in the evening.Do you want me to find the trains from New York City to Washington DC on Mondayapproximately at 9:30 in the evening now?Yes.I am going to get the train schedule for you ...Figure 1: Example Dialogue Excerpt with TOOT.Table 1: Comparison of Misrecognized (WER>0)vs.
Recognized Turns by Prosodic Feature AcrossSpeakers.
Fe tur0 I st tlMeanMisrecdRocd PII*F0 Max 7.83 30.31 Hz 0*F0 Mean 3.66 ~I.12 Hz 0*RMS Max 5.70 235.93 0RMS Mean -.57 -8.50 .57*Duration 10.30 2.20 sec 0*Prior Pause 5.55 .35 sec 0Tempo -.05 .15 sps .13*% Silence -5.15 -.06% 0*significant at a 95% confidence levelTable 2: Comparison of Misrecognized (CA<I)vs.
Recognized Turns by Prosodic Feature AcrossSpeakers.
Fe turo I st t  ?
nMisrecdl rlq ecd*F0 Max 5.60 29.64 Hz 0F0 Mean 1.70 2.10 Hz .10*RMS Max 2.86 173.87 .007RMS Mean -1.85 -27.75 .07*Duration 9.80 2.15 sec 0*Prior Pause 4.05 .38 sec 0*Tempo -4.21 -.58 sps 0% Silence -1.42 -.02% .16*significant at a 95% confidence level (p< .05)WER>0 and CA<l ,  respectively.
These results in-dicate that misrecognized turns do differ from cor-rectly recognized ones in terms of prosodic features,although the features on which they differ varyslightly, depending upon the way "misrecognition"is defined.
Whether defined by WER or CA, mis-recognized turns exhibit significantly higher F0 andRMS maxima, longer durations, and longer preced-ing pauses than correctly recognized speaker turns.For a traditional WER definition of misrecognition,misrecognitions are slightly higher in mean F0 andcontain a lower percentage of internal silence.
For aCA definition, on the other hand, tempo is a signif-icant factor, with misrecognitions spoken at a fasterrate than correct recognitions - - contrary to our hy-pothesis about the role of hyperarticulation in recog-nition error.While the comparisons in Tables 1 and 2 weremade on the means of raw values for all prosodic fea-tures, little difference is found when values are nor-malized by value of first or preceding turn, or by con-verting to z scores.
3 From this similarity between theperformance of raw and normalized values, it wouldseem to be relative differences in speakers' prosodicvalues, not deviation from some 'acceptable' range,that distinguishes recognition failures from success-ful recognitions.
A given speaker's turns that areThe only differences occur for CA defined misrecognition,where normalizing by first utterance results in significant dif-ferences in mean RMS, and normalizing by preceding turnresults in no significant differences in tempo.higher in pitch or loudness, or that are longer, orthat follow longer pauses, are less likely to be recog-nized correctly than that same speaker's turns thatare lower in pitch or loudness, shorter, and followshorter pauses - -  however correct recognition is de-fined.It is interesting to note that the features we foundto be significant indicators of failed recognitions (F0excursion, loudness, long prior pause, and longer du-ration) are all features previously associated withhyperarticulated speech.
Since prior research hassuggested that speakers may respond to failed recog-nition attempts by hyperarticulating, which itselfmay lead to more recognition failures, had we in factsimply identified a means of characterizing and iden-tifying hyperarticulated speech prosodically?Since we had independently labeled all speakerturns for evidence of hyperarticulation (two of theauthors labeled each turn as "not hyperarticulated","some hyperarticulation in the turn", and "hyperar-ticulated", following Wade et al (1992)), we wereable to test this possibility.
We excluded any turneither labeler had labeled as partially or fully hy-perarticulated, and again performed paired t-testson mean values of misrecognized versus recognizedturns for each speaker.
Results show that for bothWER-defined and CA-defined misrecognitions, notonly are the same features ignificant differentiatorswhen hyperarticulated turns are excluded from theanalysis, but in addition, tempo also is significantfor WER-defined misrecognition.
So, our findings220for the prosodic characteristics of recognized and ofmisrecognized turns hold even when perceptibly hy-perarticulated turns are excluded from the corpus.4 P red ic t ing  M is recogn i t ions  Us ingMach ine  Learn ingGiven the prosodic differences between misrecog-nized and correctly recognized utterances in ourcorpus, is it possible to predict accurately when aparticular utterance will be misrecognized or not?This section describes experiments using the ma-chine learning program RIPPER (Cohen, 1996) to au-tomatically induce prediction models, using prosodicas well as additional features.
Like many learningprograms, RIPPER takes as input the classes to belearned, a set of feature names and possible values,and training data specifying the class and featurevalues for each training example.
RIPPER outputsa classification model for predicting the class of fu-ture examples.
The model is learned using greedysearch guided by an information gain metric, and isexpressed as an ordered set of if-then rules.Our predicted classes correspond to correct recog-nition (T) or not (F).
As in Section 3, we examineboth WER-defined and CA-defined notions of cor-rect recognition, and represent each user turn as aset of features.
The features used in our learningexperiments include the raw prosodic features in Ta-bles 1 and 2 (which we will refer to as the feature set"Prosody"), the hyperarticulation score discussed inSection 3, and the following additional potential pre-dictors of misrecognition (described in Section 2):?
ASR grammar?
ASR confidence?
ASR string?
system adaptability?
dialogue strategy?
task number?
subject?
gender?
native speakerThe first three features are derived from the ASRprocess (the context-dependent grammar used torecognize the turn, the turn-level acoustic onfidencescore output by the recognizer, and the recognizedstring).
We included these features as a baselineagainst which to test new methods of predictingmisrecognitions, although, currently, we know of noASR system that includes recognized string in itsrejection calculations.
4 TOOT itself used only the4Note that, while the entire recognized string is providedto the learning algorithm, RIPPER rules test for the presenceof particular words in the string.first two features to calculate rejections and ask theuser to repeat the utterance, whenever the confi-dence score fell below a pre-defined grammar-specificthreshold.
The other features represent he exper-imental conditions under which the data was col-lected (whether users could adapt TOOT's dialoguestrategies, TOOT's initial initiative and confirmationstrategies, experimental task, speaker's name andcharacteristics).
We included these features to de-termine the extent o which particulars of task, sub-ject, or interaction influenced ASR success rates orour ability to predict them; previous work showedthat these factors impact TOOT's performance (Lit-man and Pan, 1999; Hirschberg et al, 1999).
Exceptfor the task, subject, gender, native language, andhyperarticulation scores, all of our features are au-tomatically available.Table 3 shows the relative performance of a num-ber of the feature sets we examined; results hereare for misrecognition defined in terms of WER.
5 Abaseline classifier for misrecognition, predicting thatASR is always wrong (the majority class of F), hasan error of 48.66%.
The best performing featureset includes only the raw prosodic and ASR featuresand reduces this error to an impressive 6.53% +/ -.63%.
Note that this performance is not improvedby adding manually labeled features or experimen-tal conditions: the feature set corresponding to ALLfeatures yielded the statistically equivalent 6.68%+/ -  0.63%.With respect o the performance of prosodic fea-tures, Table 3 shows that using them in conjunctionwith ASR features (error of 6.53%) significantly out-performs prosodic features alone (error of 12.76%),which, in turn, significantly outperforms any singleprosodic feature; duration, with an error of 17.42%,is the best such feature.
Although not shown inthe table, the unnormalized prosodic features ig-nificantly outperform the normalized versions by 7-13%.
Recall that prosodic features normalized byfirst task utterance, by previous utterance, or byz scores showed little performance difference in theanalyses performed in Section 3.
This difference mayindicate that there are indeed limits on the rangesin features uch as F0 and RMS maxima, durationand preceding pause within which recognition per-formance is optimal.
It seems reasonable that ex-treme deviation from characteristics of the acoustictraining material should in fact impact ASR perfor-mance, and our experiments may have uncovered, ifnot the critical variants, at least important acousticcorrelates of them.
However, it is difficult to com-SThe errors and standard errors (SE) result from 25-foldcross-validation the 1975 turns where ASR yielded a stringand confidence.
When two errors plus or minus twice the stan-dard error do not overlap, they are statistically significantlydifferent.221Table 3: Estimated Error for Predicting Misrecognized Turns (WER>0).Features Used Error \] SEProsody, ASR Confidence, ASR String, ASR Grammar 6.53% .63ALL 6.68% .63Prosody, ASR String 7.34% .75ASR Confidence, ASR String, ASR Grammar 9.01% .70Prosody, ASR Confidence, ASR Grammar 10.63% .88Prosody, ASR Confidence 10.99% .87Prosody 12.76% .79ASR String 15.24% 1.11Duration 17.42% .88ASR Confidence, ASR Grammar 17.77% .72ASR Confidence 22.23% 1.16ASR Grammar 26.28% .84Tempo 32.76% 1.03Hyperarticulation 35.24% 1.46% Silence 36.46% .79Prior Pause 36.61% .97F0 Max 38.73% .82RMS Max 42.23% .96F0 Mean 46.33% 1.10RMS Mean 48.35% 1.15II Majority Baseline J.
48.66%_%_\[___~pare our machine learning results with the statisti-cal analyses, since a) the statistical analyses lookedat only a single prosodic variable at a time, and b)data points for that analysis were means calculatedper speaker, while the learning algorithm operatedon all utterances, allowing for unequal contributionsby speaker.We now address the issue of what prosodic fea-tures are contributing to misrecognition identifica-tion, relative to the more traditional ASR tech-niques.
Do our prosodic features imply correlatewith information already in use by ASR systems(e.g., confidence score, grammar), or at least avail-able to them (e.g., recognized string)?
First, theerror using ASR confidence score alone (22.23%)is significantly worse than the error when prosodicfeatures are combined with ASR confidence scores(10.99%) - -  and is also significantly worse thanthe use of prosodic features alone (12.76%).
Simi-larly, the error using ASR confidence scores and theASR grammar (17.77%) is significantly worse thanprosodic features alone (12.76%).
Thus, prosodicfeatures, either alone or in conjunction with tradi-tional ASR features, significantly outperform thesetraditional features alone for predicting WER-basedmisrecognitions.Another interesting finding from our experimentsis the predictive power of information available tocurrent ASR systems but not made use of in calcu-lating rejection likelihoods, the identity of the recog-nized string.
This feature is in fact the best perform-ing single feature in predicting our data (15.24%).And, at a 95% confidence level, the error usingASR confidence scores, the recognized string, andgrammar (9.01%) matches the performance of ourbest performing feature set (6.53%).
It seems that,at least in our task and for our ASR system, theappearance of particular words in the recognizedstrings is an extremely useful cue to recognition ac-curacy.
So, even by making use of information cur-rently available from the traditional ASR process,ASR systems could improve their performance onidentifying rejections by a considerable margin.
Acaveat here is that this feature, like grammar state,is unlikely to generalize from task to task or recog-nizer to recognizer, but these findings suggest hatboth should be considered as a means of improvingrejection performance in stable systems.The classification model earned from the best per-forming feature set in Table 3 is shown in Figure 2.
6The first rule RIPPER finds with this feature set isthat if the user turn is less than .9 seconds and therecognized string contains the word "yes" (and possi-bly other words as well), with an acoustic onfidencescore > -2.6, then predict hat the turn will be cor-rectly recognized.7 Note that all of the prosodic fea-6Rules are presented in order of importance in classifyingdata.
When multiple rules are applicable, RIPPER uses thefirst rule.7The confidence scores observed in our data ranged froma high of -0.087662 to a low of-9.884418.222if (durationif (durationif (durationif (durationif (durationif (durationif (durationif (durationif (durationif (durationif (durationif (durationelse F< 0.897073) A (confidence > -2.62744 ) A (string contains 'yes') then T< 1.03872 ) A (confidence > -2.69775) A (string contains 'no') then T< 0.982051) A (confidence > -1.99705) A (tempo > 3.1147) then T< 0.813633) A (duration > 0.642652) A (confidence > -3.33945) A (F0 Mean > 176.794) then T< 1.30312) A (confidence > -3.37301) A (% silences ~_ 0.647059) then T0.610734) A (confidence > -3.37301) A (% silences > 0.521739) then T< 1.09537) A (string contains 'Baltimore') then T< 0.982051) A (string contains 'no') then T< 1.1803) A (confidence > -2.93085) A (grammar ---- date) then T< 1.09537) A (confidence > -2.30717) A (% silences > 0.356436) A (F0 Max > 249.225) then T< 0.868743) A (confidence > -4.14926 ) A (% silences > 0.51923) A (F0 Max > 205.296) then T< 1.18036) A (string contains 'Philadelphia') then TFigure 2: Ruleset for Predicting Correctly Recognized Turns (WER = 0) from Prosodic and ASR Features.tures except for RMS mean, max, and prior pauseappear in at least one rule, and that the featuresshown to be significant in our statistical analyses(Section 3) are not the same features as in the rules.But, as noted above, our data points in these twoexperiments differ.
It is useful to note though, thatwhile this ruleset contains all three ASR features,none of the experimental parameters was found tobe a useful predictor, suggesting that our results arenot specific to the particular conditions of and par-ticipants in the corpus collection, although they arespecific to the lexicon and grammars.Results of our learning experiments with mis-recognition defined in terms of CA rather than WERshow the overall role of the features which predictWER-defined misrecognition to be less successfulin predicting CA-defined error.
Table 4 shows therelative performance of the same feature sets dis-cussed above, with misrecognition ow defined interms of CA<I.
As with the WER experiments, thebest performing feature set makes use of prosodicand ASR-derived features.
However, the predictivepower of prosodic over ASR features decreases whenmisrecognition is defined in terms of CA - -  which isparticularly interesting since ASR confidence scoresare intended to predict WER rather than CA; the er-ror rate using ASR confidence scores alone (13.52%)is now significantly lower than the error obtainedusing prosody (18.18%).
However, prosodic featuresstill improve the predictive power of ASR confidencescores, to 11.34%, although this difference is not sig-nificant at a 95% confidence level.
And the errorrate of the three ASR features combined (11.70%) isreduced to the lowest error rate in our table whenprosodic features are added (10.43%); this error rateis (just) significantly different from the use of ASRconfidence scores alone.
Thus, for CA-defined mis-recognitions, our experiments have uncovered onlyminor improvements over traditional ASR rejectioncalculation procedures.5 D iscuss ionA statistical comparison of recognized versus mis-recognized utterances indicates that F0 excursion,loudness, longer prior pause, and longer durationare significant prosodic haracteristics of both WERand CA-defined failed recognition attempts.
Resultsfrom a set of machine learning experiments showthat prosodic differences can in fact be used to im-prove the prediction of misrecognitions with a highdegree of accuracy (12.76% error) for WER-basedmisrecognit ions-  and an even higher degree (6.53%error) when combined with information currentlyavailable from ASR systems.
The use of ASR confi-dence scores alone had a predicted WER of 22.23%,so the improvement over traditional methods is quiteconsiderable.
For CA-defined misrecognitions, theimprovement provided by prosodic features is con-siderably less.
One of our future research directionswill be to understand this difference.Another future direction will be to address the is-sue of just why  prosodic features provide such use-ful indicators of recognition failure.
Do the featuresthemselves make recognition difficult, or are theyinstead indirect correlates of other phenomena notcaptured in our study?
While the negative influenceof speaking rate variation on ASR has been reportedbefore (e.g.
(Ostendorf et al, 1996), it is tradition-ally assumed that ASR is impervious to differencesin F0 and RMS; yet, it is known that F0 and RMSvariations co-vary to some extent with spectral char-acteristics (e.g.
(Swerts and Veldhuis, 1997; Fant etal., 1995)), so that it is not unlikely that utteranceswith extreme values for these may differ criticallyfrom the training data.
Other prosodic features maybe more indirect indicators of errors.
Longer ut-terances may simply provide more chance for errorthan shorter ones, while speakers who pause longerbefore utterances and take more time making themmay also produce more disfluencies than others.We are currently replicating our experiment on anew domain with a new speech recognizer.
We areexamining the W99 corpus, which was collected in a223Table 4: Estimated Error for Predicting Misrecognized Turns (CA<l).Features Used \[ ErrorProsody, ASR Confidence~ ASR String, ASR Grammar 10.43% .63ALL 10.68% .71Prosody, ASR Confidence, ASR Grammar 11.24% .68Prosody, ASR Confidence 11.34% .64ASR Confidence, ASR String, ASR Grammar 11.70% .68ASR Confidence 13.52% .82ASR Confidence, ASR Grammar 13.52% .84ASR String 13.62% .83Prosody, ASR String 15.04% .84Prosody 18.18% .85Duration 18.38% .90ASR Grammar 22.73% .96Tempo 24.61% 1.28Hyperarticulation 25.27% 1.05F0 Mean 28.61% 1.19F0 Max 28.76% .90RMS Mean 28.86% 1.17% Silence 28.91% 1.23RMS Max 29.01% 1.16Prior Pause 29.22% 1.26Majority Baseline \[ 28.61%spoken dialogue system that supported registration,checking paper status, and information access for theIEEE Automatic Speech Recognition and Under-standing Workshop (ASRU99) (Rahim et al, 1999).This system employed the AT&T WATSON speechrecognition technology (Sharp et al, 1997).
Prelim-inary results indicate that our TOOT results do infact hold up across recognizers.
We also are extend-ing our TOOT corpus analysis to include prosodicanalyses of turns in which users become aware ofmisrecognitions and correct them.
In addition, weare exploring whether prosodic differences can helpexplain the "goat" phenomenon - -  the fact thatsome voices are recognized much more poorly thanothers (Doddington et al, 1998; Hirschberg et al,1999).
Our ultimate goal is to provide prosodically-based mechanisms for identifying and reacting toASR failures in SDS systems.AcknowledgementsWe would like to thank Jennifer Chu-Carroll, CandyKamm, participants in the AT&T "SLUG" seminarseries, and participants in the 1999 JHU SummerLanguage Engineering Workshop, for providing uswith useful comments on this research and on earlierversions of this paper.ReferencesLinda Bell and Joakim Gustafson.
1999.
Repe-tition and its phonetic realizations: Investigat-ing a Swedish database of spontaneous computer-directed speech.
In Proceedings of ICPhS-99, SanFrancisco.
International Congress of Phonetic Sci-ences.E.
Blaauw.
1992.
Phonetic differences between readand spontaneous speech.
In Proceedings of IC-SLP92, volume 1, pages 751-758, Banff.A.
G. Bouwman, J. Sturm, and L. Boves.
1999.Incorporating confidence measures in the dutchtrain timetable information system developed inthe ARISE project.
In Proc.
International Con-ference on Acoustics, Speech and Signal Process-ing, volume 1, pages 493-496, Phoenix.William Cohen.
1996.
Learning trees and rules withset-valued features.
In l$th Conference of theAmerican Association of Artificial Intelligence,AAAI.George Doddington, Walter Liggett, Alvin Martin,Mark Przybocki, and Douglas Reynolds.
1998.Sheep, goats, lambs and wolves: A statistical anal-ysis of speaker performance in the NIST 1998speaker ecognition evaluation.
In Proceedings ofICSLP-98.G.
Fant, J. Liljencrants, I. Karlsson, andM.
B?veg?rd.
1995.
Time and frequency do-main aspects of voice source modelling.
BRSpeechmaps 6975, ESPRIT.
Deliverable 27 WP1.3.Keikichi Hirose.
1997.
Disambiguating recogni-tion results by prosodic features.
In Computing224Prosody: Computational Models for ProcessingSpontaneous Speech, pages 327-342.
Springer.Julia Hirschberg, Diane Litman, and Marc Swerts.1999.
Prosodic cues to recognition errors.
In Pro-ceedings of the Automatic Speech Recognition andUnderstandin9 Workshop (ASRU'99).Julia Hirschberg.
1991.
Using text analysis to pre-dict intonational boundaries.
In Proceedings of theSecond European Conference on Speech Commu-nication and Technology, Genova.
ESCA.C.
Kamm, S. Narayanan, D. Dutton, and R. Rite-nour.
1997.
Evaluating spoken dialog systemsfor telecommunication services.
In 5th EuropeanConference on Speech Technology and Communi-cation, EUROSPEECH 97.Hans Kraayeveld.
1997.
Idiosyncrasy in prosody.Speaker and speaker group identification i  Dutchusing melodic and temporal information.
Ph.D.thesis, Nijmegen University.E.
Krahmer, M. Swerts, M. Theune, andM.
Weegels.
1999.
Error spotting in human-machine interactions.
In Proceedings ofE UR OSPEECH- 99.Irene Langkilde, Marilyn Walker, Jerry Wright,A1 Gorin, and Diane Litman.
1999.
Automaticprediction of problematic human-computer dia-logues in 'how may i help you?'.
In Proceedingsof the Automatic Speech Recognition and Under-standin 9 Workshop (ASRU'99).Gina-Anne Levow.
1998.
Characterizing and recog-nizing spoken corrections in human-computer dia-logue.
In Proceedings of the 36th Annual Meetingof the Association of Computational Linguistics,COLING/ACL 98, pages 736-742.Diane J. Litman and Shimei Pan.
1999.
Empiricallyevaluating an adaptable spoken dialogue system.In Proceedings of the 7th International Conferenceon User Modeling (UM).Diane J. Litman, Marilyn A. Walker, and Michael J.Kearns.
1999.
Automatic detection of poorspeech recognition at the dialogue level.
In Pro-ceedings of the 37th Annual Meeting of the As-sociation of Computational Linguistics , ACL99,pages 309-316.M.
Ostendorf, B. Byrne, M. Bacchiani, M. Finke,A.
Gunawardana, K. Ross, S. Roweis, E. Shriberg,D.
Talkin, A. Waibel, B. Wheatley, and T. Zep-penfeld.
1996.
Modeling systematic variationsin pronunciation via a language-dependent hid-den speaking mode.
Report on 1996 CLSP/JHUWorkshop on Innovative Techniques for Large Vo-cabulary Continuous Speech Recognition.S.
L. Oviatt, G. Levow, M. MacEarchern, andK.
Kuhn.
1996.
Modeling hyperarticulate speechduring human-computer error resolution.
In Pro-ceedings of ICSLP-96, pages 801-804, Philadel-phia.M.
Rahim, R. Pieracini, W. Eckert, E. Levin, G. DiFabbrizio, G. Riccardi, C. Lin, and C. Kamm.1999.
W99 - a spoken dialogue system for theasru'99 workshop.
In Proc.
ASRU'99.R.D.
Sharp, E. Bocchieri, C. Castillo,S.
Parthasarathy, C. Rath, M. Riley, andJ Rowland.
1997.
The watson speech recognitionengine.
In Proc.
ICASSP97, pages 4065-4068.Ronnie W. Smith.
1998.
An evaluation of strate-gies for selectively verifying utterance meaningsin spoken natural language dialog.
InternationalJournal of Human- Computer Studies, 48:627-647.Hagen Soltau and Alex Waibel.
1998.
On the in-fluence of hyperarticulated speech on recognitionperformance.
In Proceedings of ICSLP-98, Syd-ney.
International Conference on Spoken Lan-guage Processing.M.
Swerts and M. Ostendorf.
1997.
Prosodicand lexical indications of discourse structure inhuman-machine interactions.
Speech Communica-tion, 22:25-41.Marc Swerts and Raymond Veldhuis.
1997.
Interac-tions between intonation and glottal-pulse char-acteristics.
In A. Botinis, G. Kouroupetroglou,and G. Carayiannis, editors, Intonation: Theory,Models and Applications, pages 297-300, Athens.ESCA.Nanette Veilleux.
1994.
Computational Models ofthe Prosody/Syntax Mapping for Spoken LanguageSystems.
Ph.D. thesis, Boston University.E.
Wade, E. E. Shriberg, and P. J.
Price.
1992.
Userbehaviors affecting speech recognition.
In Pro-ceedings of ICSLP-92, volume 2, pages 995-998,Banff.M.
Weintraub, K. Taussig, K. Hunicke-Smith, andA.
Snodgrass.
1996.
Effect of speaking style onLVCSR performance.
In Proceedings of ICSLP-96, Philadelphia.
International Conference onSpoken Language Processing.Ilija Zeljkovic.
1996.
Decoding optimal state se-quences with smooth state likelihoods.
In Interna-tional Conference on Acoustics, Speech, and Sig-nal Processing, ICASSP 96, pages 129-132.225
