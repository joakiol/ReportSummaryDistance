Proceedings of the 3rd Workshop on Constraints and Language Processing (CSLP-06), pages 41?50,Sydney, July 2006. c?2006 Association for Computational LinguisticsCapturing Disjunction in Lexicalizationwith Extensible Dependency GrammarJorge Marques PelizzoniICMC - Univ.
de S?o Paulo - BrazilLangue & Dialogue - LORIA - FranceJorge.Pelizzoni@loria.frMaria das Gra?as Volpe NunesICMC - Univ.
de S?o Paulo - Brazilgracan@icmc.usp.brAbstractIn spite of its potential for bidirectionality,Extensible Dependency Grammar (XDG)has so far been used almost exclusivelyfor parsing.
This paper represents one ofthe first steps towards an XDG-based inte-grated generation architecture by tacklingwhat is arguably the most basic amonggeneration tasks: lexicalization.
Hereinwe present a constraint-based account ofdisjunction in lexicalization, i.e.
a wayto enable an XDG grammar to generateall paraphrases ?
along the lexicalizationaxis, of course ?
realizing a given in-put semantics.
Our model is (i) efficient,yielding strong propagation, (ii) modu-lar and (iii) favourable to synergy inas-much as it allows collaboration betweenmodules, notably semantics and syntax.We focus on constraints ensuring well-formedness and completeness and avoid-ing over-redundancy.1 IntroductionIn text generation the term lexicalization (Reiterand Dale, 2000) refers to deciding which among achoice of potentially applicable lexical items real-izing a given intended meaning are actually goingto take part in a generated utterance.
It can be re-garded as a general, necessary generation task ?especially if one agrees that the term task does notnecessarily imply pipelining ?
and remarkablypervasive at that.
For instance, even though therealization of such a phrase as ?a ballerina?
owesmuch to referring expression generation, a com-plementary task, it is still a matter of lexicaliza-tion whether to prioritize that specific phrase overall its possible legitimate alternates, e.g.
?a femaledancer?, ?a dancing woman?
or ?a dancing femaleperson?.
However, prior to the statement of prior-itizing criteria or selection preferences and ratheras the very substratum thereto, the ultimate matterof lexicalization is exactly alternation, choice ?in one word, disjunction.Given the combinatorial nature of languageand specifically the interchangeability of lexicalitems yielding hosts of possible valid solutions toone same instance lexicalization task, disjunctionmay well become a major source of (combinato-rial) complexity.
Our subject matter in this pa-per is solely disjunction in lexicalization as a ba-sis for more advanced lexicalization models, andour purpose is precisely to describe a constraint-based model that (i) captures the disjunctive po-tential of lexicalization, i.e.
allows the generationof all mutually paraphrasing solutions (accordingto a given language model) to any given lexical-ization task, (ii) ensures well-formedness, espe-cially ruling out over-redundancy (such as foundin ?
?a dancing female dancer/ballerina/woman?
)and syntactic anomalies (?
?a dancer woman?
), anddoes so (iii) modularly, in that not only are con-cerns neatly separated (e.g.
semantics vs. syntax),but also solutions are reusable, and future exten-sions, likely to be developed with no change tocurrent modules, (iv) efficiently, having an imple-mentation yielding strong propagation and thusprone to keep complexity at acceptable levels, and(v) synergicly, inasmuch as it promotes the inter-play between modules (namely syntax and seman-tics) and seems compatible with the concept of in-tegrated generation architectures (Reiter and Dale,2000), i.e.
those in which tasks are not executed inpipeline, but are rather interleaved so as to avoidfailed or suboptimal choices during search.We build upon the Extensible DependencyGrammar (XDG) (Debusmann et al, 2004b; De-busmann et al, 2004a; Debusmann et al, 2005)model and its CP implementation in Oz (Van Royand Haridi, 2004), namely the XDG Develop-ment Toolkit1 (XDK) (Debusmann et al, 2004c).1http://www.ps.uni-sb.de/~rade/xdg.41In fact, all those appealing goals of modularity,efficiency and synergy are innate to XDG andthe XDK, and our work can most correctly beregarded as the very first attempts at equippingXDG for generation and fulfilling its bidirectionalpromise.The paper proceeds as follows.
Section 2 pro-vides background information on XDG and theXDK.
Section 3 motivates our lexicalization dis-junction model and describes it both intuitivelyand formally, while Section 4 presents implemen-tation highlights, assuming familiarity with theXDK and focusing on the necessary additions andmodifications to it, as well as discussing perfor-mance.
Finally, in Section 5 we conclude and dis-cuss future work.2 Extensible Dependency GrammarAn informal overview of XDG?s core concepts isin order; for a formal description of XDG, how-ever, see (Debusmann and Smolka, 2006; Debus-mann et al, 2005).
Strictly speaking, XDG isnot a grammatical framework, but rather a descrip-tion language over finite labelled multigraphs thathappens to show very convenient properties forthe modeling of natural language, among which aremarkable reconciliation between monostratality,on one side, and modularity and extensibility, onthe other.Most of XDG?s strengths stem from its multi-dimensional metaphor (see Fig.
1), whereby an(holistic or multidimensional) XDG analysis con-sists of a set of concurrent, synchronized, comple-mentary, mutually constraining one-dimensionalanalyses, each of which is itself a graph sharingthe same set of nodes as the other analyses, buthaving its own type or dimension, i.e., its ownedge label and lexical feature types and its ownwell-formedness constraints.
In other words, each1D analysis has a nature and interpretation of itsown, associates each node with one respective in-stance of a data type of its own (lexical features)and establishes its own relations/edges betweennodes using labels and principles of its own.That might sound rather autistic at first, but the1D components of an XDG analysis interact infact.
It is exactly their sharing one same set ofnodes, whose sole intrinsic property is identity,that provides the substratum for interdimensionalcommunication, or rather, mutual constraining.htmlFigure 1: Three concurrent one-dimensionalanalyses.
It is the sharing of one same set ofnodes that co-relates and synchronizes them intoone holistic XDG analysis.ID1Mary2wants3to4laugh5.rootpartvinfsubjLP1Mary2wants3to4laugh5.nounffinfpartfinfinfrootrootpartfvinffsubjfDS1Mary2wants3to4laugh5.root delsubjdsubjdsubdPA1Mary2wants3to4laugh5.rootroot delarg1argearg1SC1Mary2wants3to4laugh5.root delasFigure 2: A 5D XDG analysis for ?Mary wants tolaugh.?
according to grammar Chorus.ul deployedwith the XDK42That is chiefly achieved by means of two devices,namely: multidimensional principles and lexicalsynchronization.Multidimensional principles.
Principles arereusable, usually parametric constraint predicatesused to define grammars and their dimensions.Those posing constraints between two or more 1Danalyses are said multidimensional.
For example,the XDK library provides a host of linking princi-ples, one of whose main applications is to regu-late the relationship between semantic argumentsand syntactic roles according to lexical specifica-tions.
The framework allows lexical entries to con-tain features of the type lab(D1) ?
{lab(D2)},i.e.
mappings from edge labels in dimension D1to sets of edge labels in D2.
Therefore, lexi-cal entries specifying {pat ?
{subj}} might becharacteristic of unaccusative verbs, while thosewith {agt ?
{subj} , pat ?
{obj}} would suita class of transitive ones.
Linking principles poseconstraints taking this kind of features into ac-count.Lexical synchronization.
The lexicon compo-nent in XDG is specified in two steps: first, eachdimension declares its own lexicon entry type;next, once all dimensions have been declared, lex-icon entries are provided, each specifying the val-ues for features on all dimensions.
Finally, at run-time it is required of well-formed analyses thatthere should be at least one valid assignment oflexicon entries to nodes such that all principles aresatisfied.
In other words, every node must be as-signed a lexicon entry that simultaneously satisfiesall principles on all dimensions, for which reasonthe lexicon is said to synchronize all 1D compo-nents of an XDG analysis.
Lexical synchroniza-tion is a major source of propagation.Figure 2 presents a sample 5D XDG analysisinvolving the most standard dimensions in XDGpractice and jargon, namely (i) PA, capturingpredicate argument structure; (ii) SC, captur-ing the scopes of quantifiers; (iii) DS, for deepsyntax, i.e.
syntactic structure modulo control andraising phenomena; (iv) ID, for immediate domi-nance in surface syntax (as opposed to DS); and(v) LP, for linear precedence, i.e.
a structuretightly related to ID working as a substratum forconstraints on the order of utterance of words.
Infact, among these dimensions LP is the only oneactually to involve a concept of order.
PA and DS,in turn, are the only ones not constrained to betrees, but directed acyclic graphs instead.
Furtherdetails on the meaning of all these dimensions, aswell as the interactions between them, would bebeyond the scope of this paper and have been dealtwith elsewhere.
From Section 3 on we shall focuson PA and, to a lesser extent, the dimension withwhich it interfaces directly: DS.Emulating deletion.
Figure 2 also illustrates therather widespread technique of deletion, there ap-plied to infinitival ?to?
on dimensions DS, PA,and SC.
As XDG is an eminently monostratal andthus non-transformational framework, ?deletion?herein refers to an emulation thereof.
Accordingto this technique, whenever a node has but one in-coming edge with a reserved label, say del, on di-mension D it is considered as virtually deleted onD.
In addition, one artificial root node is postu-lated from which emerge as many del edges as re-quired on all dimensions.
The trick also comes inhandy when tackling, for instance, multiword ex-pressions (Debusmann, 2004), which involve wor-thy syntactic nodes that conceptually have no se-mantic counterparts.3 Modelling Lexicalization Disjunctionin XDGGeneration input.
Having revised the basics ofXDG, it is worth mentioning that so far it hasbeen used mostly for parsing, in which case the in-put type is usually rather straightforward, namelytypewritten sentences or possibly other text units.Model creation is also very simple in parsing andconsists of (i) creating exactly one node for eachinput token, all nodes being instances of one sin-gle homogeneous feature structure type automat-ically inferred from the grammar definition, (ii)making each node select from all the lexical en-tries indexed by its respective token, (iii) posingconstraints automatically generated from the prin-ciples found in the grammar definition and (iv)deterministically assigning values to the order-related variables in nodes so as to reflect the actualorder of tokens in input.As concerns generation, things are not so clear,though.
For a start, take input, which usuallyvaries across applications and systems, not tomention the fact that representability and com-putability of meaning in general are open issues.Model creation should follow closely, as it is a di-rect function of input.
Notwithstanding, we can43to some extent and advantage tell what genera-tion input is not.
Under the hypothesis of anXDG-based generation system tackling lexicaliza-tion, input is not likely to contain some direct rep-resentation of fully specified PA analyses, muchthough this is usually regarded as a satisfactoryoutput for a parsing system (!).
What happenshere is that generating an input PA analysis wouldpresuppose lexicalization having already been car-ried out.
In other words, PA analyses accountingfor e.g.
?a ballerina?
and ?a dancing female hu-man being?
have absolutely nothing to do witheach other whereas what we wish is exactly tofeed input allowing both realizations.
Therefore,PA analyses are themselves part of generation out-put and are acceptable as parsing output inasmuchas ?de-lexicalization?
is considered a trivial task,which is not necessarily true, however.Although our system still lacks a comprehen-sive specification of input format and semantics,we have already established on the basis of theabove rationale that our original PA predicatesmust be decomposed into simpler, primitive predi-cates that expose their inter-relations.
For the pur-pose of the present discussion, we understand thatit suffices to specify that our input will contain flatfirst-order logic-like conjunctions such as?x (dance(x) ?
female(x) ?
human(x)) ,in order to characterize entities, even if the fi-nal accepted language is sure to have a stricterlogic component than first-order logic and mightinvolve crossings with yet other formalisms.
Pred-icates, fortunately, are not necessarily unary; and,for example, ?A ballerina tapped a lovely she-dog?might well be generated from the following input:?e, x, y?????
?dance(x) ?
female(x)?
?human(x) ?
event(e)??
past(e) ?
tap(e, x, y)?
?female(y) ?
dog(y)?lovely(y)??????.
(1)Deletion as the substance of disjunction.
Nat-urally, simply creating one node for each inputsemantic literal is not at all the idea behind ourmodel.
For example, if ?woman?
is to be ac-tually employed in a specific lexicalization task,then it should continue figuring as one single nodein XDG analyses as usual in spite of potentiallycovering a complex of literals.
In fact, XDG and,in specific, PA analyses should behave and resem-ble much the same as they used to.However, one remarkable difference of analysesin our generation model as compared to parsinglies in the role and scope of deletion, which in-deed constitutes the very substance of disjunctionnow.
By assigning all nodes but the root one ex-tra lexical entry synchronizing deletion on all di-mensions2, we build an unrestrained form of dis-junction whereby whole sets of nodes may as wellact as if not taking part in the solution.
Now itis possible to create nodes at will, even one foreach applicable lexical item, and rely on the factthat, many ill-formed outputs as the set of all so-lutions may contain, it still covers all correct para-phrases, i.e.
those in which all and only the rightnodes have been deleted.
For example, should onenode be created for each of ?ballerina?, ?woman?,?dancer?, ?dancing?, ?female?
and ?person?, allpossible combinations of these words, includingthe correct ones, are sure to be generated.Our design obviously needs further constrain-ing, yet the general picture should be visible bynow that we really intend to finish model creation?
or rather, start search ?
with (i) a bunch of per-fectly floating nodes in that not one edge is givenat this time, all of which are equally willing andoften going to be deleted, and (ii) a bunch of con-straints to rule out ill-formed output and providefor efficiency.
There are two main gaps in thissummary, namely:?
what these constraints are and?
how exactly nodes are to be created.This paper restricts itself to the first question.
Thesecond one involves issues beyond lexicalization,actually permeating all generation tasks, and iscurrently our research priority.
Consequently, inall our experiments most of model creation washandcrafted.In the name of clarity, we shall hereafter ab-stract over deletion, that is to say we shall in all re-spects adhere to the illusion of deletion, that nodesmay cease to exist.
In specific, whenever we referto the sisters, daughters and mothers of a node, wemean those not due to deletion.
In other words, allhappens as if deleted nodes had no relation what-soever to any other node.
This abstraction is ex-tremely helpful and is actually employed in ourimplementation, as shown in Section 4.2That is, specifying valencies such that an incoming deledge is required on all dimensions simultaneously.443.1 How Nodes RelateIn the following description, we shall mostly re-strict ourselves to what is novel in our model ascompared to current practice in XDG modelling.Therefore, we shall emphasize dimension PA andthe new constraints we had to introduce in orderto have only the desired PA analyses emerge.
Ex-cept for sparse remarks on dimension DS and itsrelationship with PA, which we shall also discussbriefly, we assume without further mention theconcurrence of other XDG dimensions, principlesand concepts (e.g.
lexical synchronization) in anyactual application of our model.Referents, arguments and so nodes meet.
Forthe most part, ruling out ill-formed output con-cerns posing constraints on acceptable edges, es-pecially when one takes into account that all wehave is some floating nodes to start with.
Let usfirst recall that dimension PA is all about predicatearguments, which are necessarily variables thanksto the flat nature of our input semantics.
Roughlyspeaking, each PA edge relates a predicate withone of its arguments and thus ?is about?
one sin-gle variable.
Therefore, our first concern must beto ensure that every PA edge should land on a nodethat ?is (also) about?
the same variable as the edgeitself.In order to provide for such an ?aboutness?agreement, so to speak, one must first provide for?aboutness?
itself.
Thus, we postulate that everynode should now have two new features, namely(i) hook, identifying the referent of the node,i.e.
the variable it is about, and (ii) holes, map-ping every PA edge label ` into the argument (avariable) every possible `-labelled outgoing edgeshould be about.
Normally these features shouldbe lexicalized.
The coincidence with Copestake etal.
?s terminology (Copestake et al, 2001) is notcasual; in fact, our formulation can be regarded asa decoupled fragment of theirs, since neither ourholes involves syntactic labels nor are scopal is-sues ever touched.
As usual in XDG, we leave itfor other modules such as mentioned in the previ-ous section to take charge of scope and the rela-tionship between semantic arguments and syntac-tic roles.
The role of these new features is depictedin Figure 3, in which an arrow does not mean anedge but the possibility of establishing edges.Completeness and compositionality.
Next weproceed to ensure completeness, i.e.
that every so-Figure 3: For every node v and on top of e.g.
va-lency constraints, features hook and holes furtherconstrain the set of nodes able to receive edgesfrom v for each specific edge label.lution should convey the whole intended seman-tic content.
To this end, nodes must have fea-tures holding semantic information, the most ba-sic of which is bsem, standing for base seman-tic content, or rather, the semantic contribution alexical entry may make on its own to the whole.For example, ?woman?
might be said to contribute?x.female(x)?human(x), while ?female?, only?x.
female(x).
Normally bsem should be lexi-calized.In addition, we postulate feature sem for hold-ing the actual semantic content of nodes, whichshould not be lexicalized, but rather calculatedby a principle imposing semantic composition-ality.
In our rather straightforward formulation,for every node v, sem(v) is but the conjunctionof bsem(v) and the sems of all its PA daughtersthus:sem(v)=bsem(v) ??
{sem(u) : v ?
?PA u} ,(2)where v ?`?D u denotes that node u is a daughter ofv on dimension D through an edge labelled ` (theabsence of the label just denotes that it does notmatter).Finally, completeness is imposed by means ofnode feature axiom, upon which holds the invari-antsem(v) ?
axiom(v) , (3)for every node v. The idea is to have axiom asa lexicalized feature and consistently assign it theneutralizing constant true for all lexical entriesbut those meant for the root node, in which casethe value should equal the intended semantic con-tent.Coreference classes, concentrators and revi-sions to dimensions PA and DS.
The unavoid-45able impediment to propagation is intrinsic choice,i.e.
that between things equivalent and that wewish to remain so.
That is exactly what we wouldlike to capture for lexicalization while attemptingto make the greatest amount of determinacy avail-able to minimize failure.
To this end, our strategyis to make PA analyses as flat as possible, withcoreferent nodes ?
i.e.
having the same ref-erent or hook ?
organizing in plexuses around,or rather, directly below hopefully one single nodeper plexus, thus said to be a concentrator.
Thisoffers advantages such as the following:1. the number of leaf nodes is maximized,whose sem features are determinate andequals their respective bsems;2. coreferent nodes tend to be either potentialsisters below a concentrator or deleted.
Thisallows most constraints to be stated in termsof direct relationships of mother-, daughter-or sisterhood.
Such proximity and concen-tration is rather opportune because we aredealing simply with potential relationships asnodes will usually be deleted.
In other words,our constraints aim mostly at ruling out un-desired relations rather than establishing cor-rect ones.
The latter must remain a matter ofchoice.It is in order to define which are the best candi-dates for concentrators.
Having different concen-trators in equivalent alternative realizations, suchas ?a ballerina?, ?a female dancer?
or ?a danc-ing woman?
(hypothetical concentrators are un-derlined), would be rather hampering, since thetask of assigning ?concentratingness?
would thenbe fatally bound to lexicalization disjunction it-self and not much determinacy could possibly bederived ahead of committing to this or that re-alization.
In face of that, the natural candidatemust be something that remains constant all along,namely the article.
Certainly, what specific arti-cle and, among others, whether to generate a defi-nite/anaphoric or indefinite/first-time referring ex-pression is also a matter of choice, but not pertain-ing to lexicalization.
For the sake of simplicity andscope, let us stick to the case of indefinite articles,keeping in mind that possible extensions to ourmodel to cope with (especially definite anaphoric)referring expression generation shall certainly re-quire some revisions.DSa dancing female person ?npdmodd moddrootPAa dancing female person ?apply apply applyrootFigure 4: new PA and DS analyses for ?a dancingfemale person?.
An asterisk stands for the rootnodeElecting articles for concentrators means thatthey now directly dominate their respective nounsand accompanying modifiers on dimension PA asshown in Figure 4 for ?a dancing female person?.One new edge label apply is postulated to connectconcentrators with their complements, the follow-ing invariants holding:1. for every node v, hook(v) =holes(v)(apply), i.e.
only coreferentnodes are linked by apply edges;2. every concentrator lexical entry provides avalency allowing any number of outgoingapply edges, though requiring at least one.Roughly speaking, the intuition behind this newPA design is that the occurrence of a lexical (asopposed to grammatical) word corresponds to theevaluation of a lambda expression, resulting ina fresh unary predicate built from the basesemof the word/node and the sems of its children.In turn, every apply edge denotes the applica-tion of one such predicate to the variable/referentof a concentrator.
In fact, even verbs might betreated analogously if Infl constituents were mod-elled, constituting the concentrators of verb baseforms.
Also useful is the intuition that PA abstractsover most morphosyntactic oppositions, such asthat between nouns and adjectives, which figureas equals there.
The subordination of the latterword class to the former becomes a strictly syntac-tic phenomenon or, in any case, other dimensions?affairs.Dimension DS is all about such oppositions,however, and should remain much the same ex-cept that the design is rather simplified if DS main-tains concentrator dominance.
As a result, arti-cles must stand as heads of noun ?
or rather, de-46Figure 5: Starting conditions with perfectly float-ing nodes in the lexicalization of ?a ballerina?
andits paraphrasesterminer ?
phrases, which is not an unheard-ofapproach, just unprecedented in XDG.
Naturally,standard syntactic structures should appear belowdeterminers, as exemplified in Figure 4.
Grantedthis, the flatness of PA and its relation to DS canstraightforwardly be accomplished by the applica-tion of XDK library principles Climbing, wherebyPA is constrained to be a flattening of DS, and Bar-riers, whereby concentrators are set as obstacles toclimbing by means of special lexical features.
Fig-ure 5 thus illustrates the starting conditions for thelexicalization of ?a ballerina?
and its paraphrases,including the bsems of nodes.
Notice that we havecreated distinct nodes for different parts of speechof one same word, ?female?.
The relevance of thismeasure shall be clarified along this section as wedevelop this example.Fighting over-redundancy.
We currently em-ploy two constraints to avoid over-redundancy.The first is complete in that its declarative seman-tics already sums up all we desire to express in thatmatter, while the other is redundant, incomplete,but supplied to improve propagation.The complete constraint is imposed betweenevery node and each of its potential daughters.Apart from overhead reasons, it might as well beimposed between every pair of nodes.
However,the set of potential daughters of a node v is bestapproximated by function dcands thus:dcands(v)=(?
{?x?
: x ?
ran(holes(v))})?
{v} ,where ?x?
denotes the coreference class of vari-able x; and ran(f), the range of function f .
It isworth noticing that in generation dcands is knownat model creation.Given a node u and a potential daughter v ?dcands(u), this constraint involves hypothesiz-ing what the actual semantic content of u wouldbe like if v were not among its daughters.Let hdsv(u) and hsemv(u) be respectively thehypothetical set of daughters of u counting v outand its ?actual?
semantic content in that case,which can be defined thus:hdsv(u) = {w : u ?
?PA w} ?
{v}andhsemv(u) = bsem(u)???
{sem(w) : w ?
hdsv(u)} .
(4)The constraint consists of ensuring that, if the ac-tual semantic content of the potential daughter vwould be subsumed by the hypothetical semanticcontent of u, then v can never be a daughter of u.In other words, each daughter of u must make adifference.
Formally, we have the following:(hsemv(u) ?
sem(v)) ?
?
(u ?
?PA v) (5)where the two implication symbols, ?
and ?have the same interpretation in this logic state-ment, but are nonetheless distinguished becausetheir implementations are radically different asshall be discussed in Section 4.
Constraint (5)is especially active after some choices have beenmade.
Suppose, in our ?a ballerina?
example,that ?dancing?
is the only word selected so farfor lexicalization.
Let u and v be respectivelythe nodes for ?a?
and ?dancing?.
In this case,the consequent in (5) is false and so must be theantecedent hsemv(u) ?
dance(x), which im-plies that hsemv(u) can never ?contain?
the literaldance(x).
From (4) and the fact that articles haveneutral base semantics ?
i.e.
bsem(u) = true ?it follows that all further daughters of u must notimply dance(x).
As that does not hold for ?bal-lerina?
and ?dancer?, these nodes are ruled out asdaughters of u and thus deleted for lack of moth-ers.
Conversely, if ?ballerina?
had been selected47in the first place, (5) would trivially detect the re-dundancy of all other words and analogously en-tail their deletion.In turn, the redundant constraint ensures that,for every pair of coreferent nodes u and v ?
?upvar(u)?, if the actual semantic content of vis subsumed by u, then they can never be sisters.Formally:(sem(u) ?
sem(v)) ?
(v /?
sistersPA(u)) .
(6)This constraint is remarkable for being activeeven in the absence of choice since it is establishedbetween potential sisters, which usually have theirsems sufficiently, if not completely, determined.Surprisingly enough, the main effect of (6) is onsyntax, by constraining alliances on DS.
As ournew version of the XDK?s Climbing principle isnow aware of sisterhood constraints, it will con-strain every node on PA to have as a mother onDS either its current PA mother or some node be-longing to one of its PA sister trees3.
In groundterms, when (6) detects that e.g.
?woman?
sub-sumes ?female (adj./n.)?
and constrains them notto be sisters on PA, the Climbing principle willrule out ?woman?
as a potential DS mother of?female (adj.)?.
It is worth mentioning that oncev /?
sistersD(u) is imposed, our sisterhood con-straints entail u /?
sistersD(v).Redundant compositionality constraints.
Al-though a complete statement of semantic compo-sitionality is given by Equation 2, we introducetwo redundant constraints to improve propagation.The first of them attempts to advance detection ofnodes whose semantic contribution is strictly re-quired even before the sem features of their moth-ers become sufficiently constrained.
It does soby means of an strategy analogous to that of (5),namely by hypothesizing, for every node v, whatthe total semantic content would be like if v weredeleted.
Let root, hdownv(u) and htotsemv berespectively the root node, the set of nodes directlyor indirectly below u counting v out, and the to-tal semantic content supposing v is deleted, whichcan be defined thus:hdownv(u) = downPA(u)?
{v}andhtotsemv =?
{sem(u) : u ?
hdownv(root)} .3If the subgraphs option is active, which is the case here.The constraint can be formally expressed thus:deleted(v) ?
(htotsemv ?
sem(v)) .
(7)Unfortunately, (7) is not of much use in our currentexample, better applying to cases where there are agreater number of alternative inner nodes.
For ex-ample, in the lexicalization of (1), this constraintwas immediately able to infer that ?lovely?
mustnot be deleted since it was the sole node contribut-ing lovely(y).The second redundant compositionality con-straint attempts to advance detection of nodes notcounting on enough potential sisters to fulfill theactual semantic content of their mothers.
To thisend, for every node v, the following constraint isimposed:?
{sem(u) : u ?
?PA v}=???
{bsem(u) : u ?
?PA v}??
{sem(u) : u ?
eqsisPA(v)}??
,(8)whereeqsisD(v) ={ ?, iff v is deleted on DsistersD(v) ?
{v} , else.
(9)which reads ?the actual semantic content of themothers of a node is equal to their base seman-tic content in conjunction with the actual se-mantic content of this node and its sisters?.
Itis worth noticing that, when v is deleted, both{u : u ?
?PA v} and eqsisPA(v) become empty sothat (8) still holds.
This constraint is especiallyinteresting because our new versions of principlesClimbing and Barriers, which hold between DSand PA, propagate sisters constraints in both di-rections.
In association with (6) and (8), theseprinciples promote an interesting interplay be-tween syntax and semantics.
Resuming our ex-ample, let v be node ?female (n.)?.
Before anyselection is performed, constraint (6) infers thatonly ?dancing?, ?person?
and ?dancer?
can be sis-ters to v on PA and thus (now due to Climbing)daughters to v on DS.
They cannot be mothersto v because its valency on DS and Climbing areenough to establish that, if v has any mother atall on DS, it is ?a?.
Again taking the DS valencyof v into account, it is possible to infer that, if vhas any daughter at all on DS, it is ?dancing?, i.e.the only adjective in the original set of candidate48daughters.
It is the new sisterhood-aware versionof Barriers that propagates this new piece of in-formation back to PA.
This principle now knowsthat the sisters of v on PA must come from ei-ther (i) the tree below v on DS, (ii) one of its DSsister trees or (iii) some DS tree whose root be-longs to eqsisDS(inter) for some node inter ap-pearing ?
on DS ?
between v and one of itsmothers on PA.
In our example, (ii) and (iii) areknown to be empty sets, while (i) is at most ?danc-ing?.
Consequently, ?dancing?
is the only poten-tial PA sister of v. Now (8) is finally able to con-tribute.
As ?a?
is the only possible DS motherof v and any article has empty basic semantics,one is entitled to equate ?
{sem(u) : u ?
?PA v} to?
{sem(u) : u ?
eqsisPA(v)}.
Even though it isnot known whether v will ever have mothers ordaughters, (8) knows that the left-hand side of theequation yields either the whole intended seman-tics or nothing, while the right-hand side yields ei-ther nothing or at most dance(x) ?
female(x) .Therefore, the only solution to the equation isnothing on both sides, implying that eqsisPA(v)is empty and thus v is deleted by definition (9).Such strong interplay is only possible becausewe have created distinct nodes for the differentparts of speech ?
or rather, the two different DSvalencies ?
of ?female?.
With somewhat morecomplicated, heavier constraints it would be pos-sible to have the same propagation for one sin-gle node selecting from different parts of speech.Notwithstanding, that does not seem worth the ef-fort because a model creation algorithm would beperfectly able to detect the diverging DS valencies,create as many nodes as needed and distribute theright lexical entries among them.4 Implementation and PerformanceRemarksThe ideas presented in Section 3 were fully im-plemented in a development branch of the XDK.As with the original XDK, all development isbased on the multiparadigm programming systemMozart4.The implementation closely follows the originalCP approach of the XDK and strongly reflects theconstraints we have presented after some ratherstandard transformations to CP, namely:?
variable identifiers in hooks and holes, aswell as all semantic input literals such as4http://www.mozart-oz.orghuman(x) and tap(e, x, y), are encoded asinteger values.
Features bsem/sem are im-plemented as set constants/variables of suchintegers;?
logic conjunction ?
is thus modelled by setunion ?.
Each ?big?
conjunction is re-duced to the form ?
{f(v) : v ?
V }, whereV is a set variable of integer-encoded nodeidentifiers, and modelled by a union selec-tion constraint ?
?f(1) f(2) ... f(M)?
[V ],where M is the maximum node identifier andwhich constrains its result ?
a set variable?
to be the union of f(v) for all v ?
V ;?
implications of the form x ?
y are imple-mented as y ?
x, while those of the formx ?
y as reify(x) ?
reify(y) , wherethe result of reify(x) is an integer-encodedboolean variable constrained to coincide withthe truth-value of expression x.Our branch of the XDK now counts on two newprinciples, namely (i) Delete, which requires theGraph principle, creates doubles for the node at-tributes introduced by the latter, providing the il-lusion of deletion, and introduces features for sis-terhood constraints; and (ii) Compsem, imposingall constraints described in Section 3.A few preliminary proof-of-concept experi-ments were carried out with input similar to (1)and linguistically and combinatorially analogousto our ?ballerina?
example.
In all of them, the sys-tem was able to generate all paraphrases with nofailed state (backtracking) in search, which meansthat propagation was maximal for all cases.
Al-though our design supports more complex linguis-tic constructs such as relative clauses and prepo-sition phrases and is expected to behave similarlyfor those cases, we have not made any such exper-iments so far.
This is so because we are currentlyprioritizing the issue of model creation and cover-age of other generation tasks.5 Conclusions and Future WorkIn this paper we have presented the results of thevery first steps towards the application of XDG toNatural Language Generation, hopefully in an in-tegrated architecture.
Our main contribution andfocus was a formulation of lexicalization disjunc-tion in XDG terms, preserving the good propertiesof modularity and extensibility while achieving49good propagation.
We also hope to have demon-strated how strong the interplay between linguis-tic dimensions can be in XDG.
As basic issuesas the very nature of input were discussed also asan evidence that there is still a long way to go.We are currently working on extending our designto cover other generation tasks than lexicalizationand perform model creation.AcknowledgementsWe would like to thank Claire Gardent and DenysDuchier, for all their invaluable insights and com-ments, and group Langue & Dialogue and Brazil-ian agencies CNPq and CAPES, for funding thisresearch project and travel to COLING/ACL.ReferencesCopestake, A.
A., Lascarides, A., and Flickinger, D.(2001).
An algebra for semantic construction inconstraint-based grammars.
In Meeting of the As-sociation for Computational Linguistics, pages 132?139.Debusmann, R. (2004).
Multiword expressions as de-pendency subgraphs.
In Proceedings of the ACL2004 Workshop on Multiword Expressions: Integrat-ing Processing, Barcelona/ESP.Debusmann, R., Duchier, D., Koller, A., Kuhlmann,M., Smolka, G., and Thater, S. (2004a).
A rela-tional syntax-semantics interface based on depen-dency grammar.
In Proceedings of the COLING2004 Conference, Geneva/SUI.Debusmann, R., Duchier, D., and Kruijff, G.-J.
M.(2004b).
Extensible dependency grammar: A newmethodology.
In Proceedings of the COLING2004 Workshop on Recent Advances in DependencyGrammar, Geneva/SUI.Debusmann, R., Duchier, D., and Niehren, J.
(2004c).The xdg grammar development kit.
In Proceedingsof the MOZ04 Conference, volume 3389 of Lec-ture Notes in Computer Science, pages 190?201,Charleroi/BEL.
Springer.Debusmann, R., Duchier, D., and Rossberg, A.
(2005).Modular Grammar Design with Typed ParametricPrinciples.
In Proceedings of FG-MOL 2005, Ed-inburgh/UK.Debusmann, R. and Smolka, G. (2006).
Multi-dimensional dependency grammar as multigraph de-scription.
In Proceedings of FLAIRS-19, MelbourneBeach/US.
AAAI.Reiter, E. and Dale, R. (2000).
Building Natural Lan-guage Generation Systems.
Cambridge UniversityPress.Van Roy, P. and Haridi, S. (2004).
Concepts, Tech-niques, and Models of Computer Programming.MIT Press.50
