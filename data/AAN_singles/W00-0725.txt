In: Proceedings of CoNLL-2000 and LLL -2000,  pages 123-125, Lisbon, Portugal, 2000.A Comparison of PCFG Models*J ose  Lu is  Verdd-Mas  and Jo rge  Ca lera -Rub io  and Rafae l  C.  Car rascoDepartament  de Llenguatges i Sistemes Inform?ticsUniversitat d'Alacant, E-03071 Alacant (Spain)(verdu, calera, carrasco)@dlsi.ua.esAbst rac tIn this paper, we compare three different ap-proaches to build a probabilistic ontext-freegrammar for natural language parsing from atree bank corpus: 1) a model that simply ex-tracts the rules contained in the corpus andcounts the number of occurrences of each rule2) a model that also stores information aboutthe parent node's category and, 3) a model thatestimates the probabilities according to a gen-eralized k-gram scheme with k -- 3.
The lastone allows for a faster parsing and decreases theperplexity of test samples.1 In t roduct ionRecent work (Johnson, 1998) has explored theperformance of parsers based on a probabilisticcontext-free grammar (PCFG) extracted from atraining corpus.
The results show that the typeof tree representation used in the corpus canhave a substantial effect in the estimated like-lihood of each sentence or parse tree.
Accord-ing to (Johnson, 1998), weaker independence as-sumptions --such as decreasing the number ofnodes or increasing the number of node labels--improve the efficiency of the parser.
The bestresults were obtained with parent-annotated la-bels where each node stores contextual informa-tion in the form of the category of the node'sparent.
This fact is in agreement with theobservation put forward by Charniak (Char-niak, 1996) that simple PCFGs, directly ob -tained from a corpus, largely overgeneralize.This property suggests that, in these models,a large probability mass is assigned to incorrect* Work partially supported by the Spanish CICYT un-der grant TIC97-0941.parses and, therefore, any procedure that con-centrates the probability on the correct parseswill increase the likelihood of the samples.In this spirit, we introduce a generalization ofthe classic k-gram models, widely used for stringprocessing (Brown et al, 1992; Ney et al, 1995),to the case of trees.
The PCFG obtained in thisway consists of rules that include informationabout the context where the rule is applied.The experiments were performed using theWall Street Journal (WSJ) corpus of the Uni-versity of Pennsylvania (Marcus et al, 1993)modified as described in (Charniak, 1996)and (Johnson, 1998).2 A genera l i zed  k -gram mode lRecall that k-gram models are stochastic mod-els for the generation of sequences l,s2,...based on conditional probabilities, that is:1. the probability P(s l s2 .
.
.
s t lM)  of a se-quence in the model M is computed as aproductpM(  Sl )pM(  S2\[Sl ) " " " pM(  St\[Sl S2 .
.
.
St-l),and2.
the dependence of the probabilities PMon previous history is assumed to be re-stricted to the immediate preceding con-text, in particular, the last k - 1 words:PM(St \ [S l  .
.
.
S t -1 )  ---- pM(St \ [S t -k+l  .
.
.
S t -1 ) .Note that in this kind of models, the probabilitythat the observation st is generated at time t iscomputed as a function of the subsequence oflength k - 1 that immediately precedes t (thisis called a state).
However, in the case of trees,it is not obvious what context should be taken into account.
Indeed, there is a natural preferencewhen processing strings (the usual left-to-right123VPlabel: VP sV NP PP NpvP~ ppVPDet N p NP Np PPDet NFigure 1: A sample parse tree of depth 3.order) but there are at least two standard waysof processing trees: ascending (or bottom-up)analysis and descending (or top-down) analysis.Ascending tree automata recognize a wider classof languages (Nivat and Podelski, 1!197; G~csegand Steinby, 1984) and, therefore, they allow forricher descriptions.Thus, our model will compute the expansionprobability for a given node as a function of thesubtree of depth k - 2 that the node generates 1,i.e., every state stores a subtree of depth k - 2.In the particular case k = 2, only the label of thenode is taken into account (this is analogous tothe standard bigram model for strings) and themodel coincides with the simple rule-countingapproach.
For instance, for the tree depicted inFig.
1, the following rules are obtained:VP ~ V NP PPNP ~ Det NPP ~ P NPHowever, in case k = 3, the expansion proba-bilities depend on the states that are defined bythe node label, the number of descendents thenode and the sequence of labels in the descen-dents (if any).
Therefore, for the same tree thefollowing rules are obtained in this case:VP(V NP PP) ~ V NP(Det N) PP(P NP)NP(Det N) --+ Det NPP(P NP) ~ P NP(Det N)where each state has the form X(Z1...Zm).This is equivalent to a relabeling of the parsetree before extracting the rules.Finally, in the parent annotated model (PA)described in (Johnson, 1998) the states depend1Note that in our notation a single node tree hasdepth 0.
This is in contrast to strings, where a singlesymbol has length 1.on both the node label and the node's parent--+ V NP VP ppVP---+ Det NP NP PP---+ Det NIt is obvious that the k = 3 and PA modelsincorporate contextual information that is notpresent in the case k = 2 and, then, a highernumber of rules for a fixed number of categoriesis possible.
In practice, due to the finite sizeof the training corpus, the number of rules isalways moderate.
However, as higher values ofk lead to an enormous number of possible rules,huge data sets would be necessary in order tohave a reliable estimate of the probabilities forvalues above k = 3.
A detailed mathematicaldescription of these type of models can be foundin (Rico-Juan et al, 2000)3 Exper imenta l  resu l t sThe following table shows some data obtainedwith the three different models and the WSJcorpus.
The second column contains the num-ber of rules in the grammar obtained from atraining subset of the corpus (24500 sentences,about the first half in the corpus) and the lastone contains the percentage of sentences in atest set (2000 sentences) that cannot be parsedby the grammar.Modelk=2k=3PAnumber of rules %unparsed sent.11377 064892 2418022 0.2As expected, the number of rules obtained in-creases as more information is conveyed by thenode label, although this increase is not ex-treme.
On the other hand, as the generaliza-tion power decreases, ome sentences in the testset become unparsable, that is, they cannot begenerated by the grammar.
The number of un-parsed sentences i  very small for the parent an-notated model but cannot be neglected for thek = 3 model.As we will use the perplexity of a test sampleS = {wl, ..., w\]s\] } as an indication of the qualityof the model,1 k~__S\]llog2p(WklM) PP = iS--- ~124, unparsable sentences would produce an infiniteperplexity.
Therefore, we studied the perplex-ity of the test set for a linear combination oftwo models Mi and Mj with p(wklMi -- Mj) =)~p(wklMi) + (1 - ~)p(wklMj).
The mixing pa-rameter ~ was chosen in order to minimize theperplexity.
Figure 2 shows that there is always11o105lOO9590?
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
..~.'?
'"+'"i05Figure 2: Test set perplexity as a function of themixture parameter A.
Upper line: k -- 2 and PA.Lower line: k = 2 and k = 3.a minimum perplexity for an intermediate valueof ),.
The best results were obtained with a mix-ture of the k-gram models for k = 2 and k = 3with a heavier component (73%) of the last one.The minimum perplexity PPm and the corre-sponding value of ~ obtained are shown in thefollowing table:Mixture model PPm Amk = 2 and PA 107.9 0.58k = 2 and k = 3 91.0 0.27It is also worth to remark that the model k = 3is the less ambiguous model and, then, parsingof sentences becomes much faster.4 Conc lus ionWe have investigated the applicability of aPCFG model based on the extension of k-grammodels described in (Rico-Juan et al, 2000).The perplexity of the test sample decreaseswhen a combination of models with k = 2 andk = 3 is used to predict string probabilities.
Weare at present checking that the behavior alsoholds for other quality measures as the precisionand recall of parses of sentences that expressstrong equivalence between the model and thedata.Re ferencesPeter F. Brown, Vincent J. Della Pietra, Peter V.deSouza, Jennifer C. Lai, and Robert L. Mercer.1992.
Class-based n-gram models of natural lan-guage.
Computational Linguistics, 18(4):467-479.Eugene Charniak.
1996.
Tree-bank grammars.
InProceedings of the Thirteenth National Confer-ence on Artificial Intelligence and the EighthInnovative Applications of Artificial IntelligenceConference, pages 1031-1036, Menlo Park.
AAAIPress/MIT Press.Ferenc Gdcseg and Magnus Steinby.
1984.
Tree Au-tomata.
Akad@miai Kiad6, Budapest.Mark Johnson.
1998.
PCFG models of linguistictree representations.
Computational Linguistics,24(4):613-632.Mitchell P. Marcus, Beatrice Santorini, and MaryAnn Marcinkiewicz.
1993.
Building a large anno-tated corpus of english: the penn treebank.
Com-putational Linguistics, 19:313-330.H.
Ney, U. Essen, and R. Kneser.
1995.
On the esti-mation of small probabilities by leaving-one-out.IEEE Trans.
on Pattern Analysis and MachineIntelligence, 17(12):1202-1212.Maurice Nivat and Andreas Podelski.
1997.
Min-imal ascending and descending tree automata.SIAM Journal on Computing, 26(1):39-58.Juan Ram6n Rico-Juan, Jorge Calera-Rubio, andRafael C. Carrasco.
2000.
Probabilistic k-testabletree language.
In ICGI-2000, to appear.125
