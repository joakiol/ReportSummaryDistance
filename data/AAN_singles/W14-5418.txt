Proceedings of the 25th International Conference on Computational Linguistics, pages 112?114,Dublin, Ireland, August 23-29 2014.Coloring Objects: Adjective-Noun Visual Semantic CompositionalityDat Tien Nguyen(1,2)Angeliki Lazaridou(2)Raffaella Bernardi(2)(1)EM LCT,(2)University of Trento/ Italyname.surname@unitn.itAbstractThis paper reports preliminary experiments aiming at verifying the conjecture that semantic com-positionality is a general process irrespective of the underlying modality.
In particular, we modelcompositionality of an attribute with an object in the visual modality as done in the case of an ad-jective with a noun in the linguistic modality.
Our experiments show that the concept topologiesin the two modalities share similarities, results that strengthen our conjecture.1 Language and VisionRecently, fields like computational linguistics and computer vision have converged to a common way ofcapturing and representing the linguistic and visual information of atomic concepts, through vector spacemodels.
At the same time, advances in computational semantics have lead to effective and linguisticallyinspired approaches of extending such methods from single concepts to arbitrary linguistic units (e.g.phrases), through means of vector-based semantic composition (Mitchell and Lapata, 2010).Compositionality is not to be considered only an important component from a linguistic perspective,but also from a cognitive perspective and there has been efforts to validate it as a general cognitiveprocess.
However, in computer vision so far compositionality has received limited attention.
Thus, inthis work, we study the phenomenon of visual compositionality and we complement limited previousliterature that has focused on event compositionality (St?ottinger et al., 2012) or general image struc-ture (Socher et al., 2011), by studying models of attribute-object semantic composition.In a nutshell, our work consists of learning vector representations of attribute-object (e.g., ?red car?,?cute dog?
etc.)
and objects (e.g., ?car?, ?dog?, ?truck?, ?cat?
etc.)
and by using those compute therepresentation of new objects having similar attributes (?red truck?, ?cute cat?
etc.).
This question hasboth theoretical and applied impact.
The possibility of developing a visual compositional model ofattribute-object, on the one hand, could shed light on the acquisition of such ability in humans; how welearn attribute representation and compose them with different objects is still an open question within thecognitive science community (Mintz and Gleitman, 2002).
On the other hand, computer vision systemscould become generative and be able to recognize unseen attribute-object combinations, a componentespecially useful for object recognition and image retrieval.2 Visual Compositional ModelAs our source of inspiration regarding the type of compositionality, we use the Lexical Functional model(LF) (Baroni and Zamparelli, 2010), under which adjectives, in linguistic compositionality, are repre-sented as linear functions (i.e., matrix of weights).
Concretely, each adjective function fWadjis inducedfrom corpus-observed vectors of adjective-noun phrases wi?
Wphraseand noun wj?
Wnoun, e.g.,?
(wred car, wcar), (wred flag, wflag), .
.
.
?, by solving the least-squares regression problem:arg minfWadj?Rd?d||Wphrase?
fWadjWnoun||This work is licensed under a Creative Commons Attribution 4.0 International Licence.
License details:http://creativecommons.org/licenses/by/4.0/112In this work, we propose to import the LF method in the visual modality, aiming at develop-ing a Visual Compositional Model.
Similarly to the case of linguistic compositionality, each at-tribute function fVattris induced from image-harvested vector representations of attribute-object vi?Vphraseand object vj?
Vobject, e.g.
for training the function fVredthe following data can be used?
(vred car, vcar), (vred flag, vflag), .
.
.
?.3 ExperimentsThe visual representations of attribute-objects and objects are created with the PHOW-color fea-tures (Bosch et al., 2007) and SIFT color-agnostic features (Lowe, 2004) respectively.
The linguisticrepresentations for the adjective-noun Wphraseand noun Wnounare built with the word2vec toolkit1using a corpus of 3 billion tokens.2Both visual and linguistic representations consist of 300 dimensions.In this work, we focus on attributes related to 10 colors (Russakovsky and Fei-Fei, 2012) for atotal number of 9699 images depicting 202 unique objects/nouns and 886 unique phrases (attribute-object/adjective-noun).
Our experiments are conducted with aggregated attribute-object representationsobtained by summing the visual vectors extracted from images representing the same attribute-object,The same pipeline is followed for the objects to obtain aggregated object vectors.This work aims at comparing the behavior of the semantically-driven compositionality process acrossthe two modalities.
For this reason, we report results on the intersection of Vphraseand Wphrase, aprocess that results in 266 attribute-object/adjective-noun items.
Furthermore, although the training datafor the two modalities are different, the size of the training data is identical, i.e., the fVattris trained usingthe remaining 620 attribute-object items, whereas for the fWadj, we randomly sample 620 adjective-nounitems from the language space.3.1 Analysis of Language and Visual Semantic SpacesThis experiment aims at assessing the degree to which language and vision share commonalities.
To thisend, we compute the cosine similarities between all possible combination of objects (resp., nouns) andperform a correlation analysis of the similarity of the corresponding pairs in the two lists resulting in 0.45Spearman correlation ?
e.g., we correlate the similarity between vcatand vdogwith that between wcatandwdog.
For instance, ?goat?
and ?sheep?
are highly similar in both spaces, whereas ?whale?
and ?bird?are similar only linguistically, whereas ?blackboard?
and ?chair?
are similar only visually.
The sameexperiment is performed between all possible combinations of attribute-object/adjective-noun items, e.g.we correlate the similarity between vwhite catand vblack dogwith that between wwhite catand wblack dog,resulting in 0.33 Spearman correlation (see Table 1).Overall, our results suggest that the topologies of the semantic spaces are similar in the two modalities.Furthermore, since this phenomenon is also apparent in the cases of attribute-object and adjective-nounpairs, this alludes to the possibility of transferring approaches of semantic compositionality from thelinguistic to the visual modality.High Visual Low VisualHigh Linguistic goat-sheep, jaguar- lion baboon-transporter, bird-whaleblack bag - brown bag, brown bear - yellow dog blue grass - blue van, gray whale - white deerLow Linguistic ball-horse, blackboard-chair baboon-sofa, backboard-pandared strawberry - white ball, white bear - yellow dog black bag - green bridge, green table - yellow stickTable 1: Similar and dissimilar concepts in the language and vision space.3.2 Semantically-driven composition for attribute-object representationsThe findings of the previous experiment suggest a high correlation between the visual attribute-attributerepresentations and the corpus-harvested adjective-noun representations.
An interesting question thatarises is whether we could approximate such visual representations of complex visual units, similarly to1https://code.google.com/p/word2vec/2http://wacky.sslmit.unibo.it, http://www.natcorp.ox.ac.uk113how is done in Computational Linguistics for approximating the text-based representations of adjective-noun phrases.
Thus, this experiment is designed in order to assess the validity of the semantically-drivencompositionality approach in the visual domain.
Results are reported in Table 2.
Since we expect thatthe quality of the aggregated vectors depends on the numbers of available images, we report results forsubsets of the original data set that differ on the number of images per phrase.By means of the LF composition method sketched in Section 2, we obtain the compositional represen-tations of attribute-object (Vcompphrase) and adjective-noun (Wcompphrase) items.
We then perform the correlationanalyses between the similarities obtained in the composed visual space Vcompphrasewith: 1) the equiva-lent image-harvested representations Vphrase, 2) the equivalent corpus-derived linguistic representationsWphrase, 3) the equivalent compositionally-derived linguistic representations Wcompphrase.Overall, the correlation between Vcompspaceand Vspacesuggests that the visual compositionality ofattribute-object can account, to some extend, for the visual semantics of the respective image, and itfurther improves with the number of images we consider for obtaining the aggregated vectors of the vi-sual phrases.
Finally, as expected, the correlations between Vcompspacealthough lower than the ones reportedin Section 3.1, i.e., 0.22 vs 0.32, are still non negligible.all phrases > 10 images > 20 images > 30 imagesVcompphrase- Vphrase0.24 0.40 0.53 0.58Vcompphrase- Wphrase0.10 0.22 0.19 0.23Vcompphrase- Wcompphrase0.04 0.05 0.18 0.10Table 2: Spearman correlations between the similarities in the Vcompphraseand other semantic spaces.4 ConclusionsIn this work, we have experimented with semantically-driven compositionality of attributes with objectsin the visual modality, by adopting an out-of-the-box composition method from the computational se-mantics literature.
Our preliminary results have shown that the visual representations of attribute-objectswhen obtained compositionally reflect properties similar not only to the ones found in representationsharvested automatically from images, but also from those extracted from text corpora.
These resultsshow that semantic compositionality might be a general process irrespective of the underlying modality.We have just scratched the surface on this topic and in the future we plan to experiment with a largervariety of attributes and use and design alternative visual compositional models.AcknowledgementsThe second and third author acknowledge ERC 2011 Starting Independent Research Grant n. 283554(COMPOSES).
We thank the 3 anonymous reviewers for their comments, Marco Baroni and Elia Brunifor their constant and useful feedback.References[Baroni and Zamparelli2010] Marco Baroni and Roberto Zamparelli.
2010.
Nouns are vectors, adjectives arematrices: Representing adjective-noun constructions in semantic space.
In Proceedings of EMNLP, 1183?1193.
[Bosch et al.2007] Anna Bosch, Andrew Zisserman, and Xavier Munoz.
2007.
Image classification using randomforests and ferns.
In Proceedings of ICCV, 1?8.
[Lowe2004] David G Lowe.
2004.
Distinctive image features from scale-invariant keypoints.
InternationalJournal of Computer Vision, 60:91?110.
[Mintz and Gleitman2002] Toben H. Mintz and Lila R. Gleitman.
2002.
Adjectives really do modify nouns: theincremental and restricted nature of early adjective acquisition.
Cognition, 84:267?293.
[Mitchell and Lapata2010] Jeff Mitchell and Mirella Lapata.
2010.
Composition in distributional models ofsemantics.
Cognitive Science, 34(8):1388?1429.
[Russakovsky and Fei-Fei2012] Olga Russakovsky and Li Fei-Fei.
2012.
Attribute learning in large-scale datasets.In Trends and Topics in Computer Vision, 1?14.
Springer.
[Socher et al.2011] Richard Socher, Cliff C Lin, Chris Manning, and Andrew Y Ng.
2011.
Parsing natural scenesand natural language with recursive neural networks.
In Proceedings of ICML, 129?136.
[St?ottinger et al.2012] J. St?ottinger, J.R.R.
Uijlings, A.K.
Pandey, N. Sebe, and F. Giunchiglia.
2012.
(unseen)event recognition via semantic compositionality.
In CVPR.114
