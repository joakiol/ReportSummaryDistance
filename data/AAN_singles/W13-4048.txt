Proceedings of the SIGDIAL 2013 Conference, pages 319?323,Metz, France, 22-24 August 2013. c?2013 Association for Computational LinguisticsInvestigating speaker gaze and pointing behaviourin human-computer interaction with the mint.tools collectionSpyros Kousidis Casey Kennington David SchlangenDialogue Systems Group / CITEC / SFB 673Bielefeld Universityspyros.kousidis@uni-bielefeld.deAbstractCan speaker gaze and speaker arm move-ments be used as a practical informa-tion source for naturalistic conversationalhuman?computer interfaces?
To investi-gate this question, we recorded (with eyetracking and motion capture) a corpus ofinteractions with a (wizarded) system.
Inthis paper, we describe the recording, anal-ysis infrastructure that we built for suchstudies, and analysis we performed onthese data.
We find that with some initialcalibration, a ?minimally invasive?, sta-tionary camera-based setting provides dataof sufficient quality to support interaction.1 IntroductionThe availability of sensors such as MicrosoftKinect and (almost) affordable eye trackers bringnew methods of naturalistic human-computer in-teraction within reach.
Studying the possibilitiesof such methods requires building infrastructurefor recording and analysing such data (Kousidis etal., 2012a).
We present such an infrastructure?the mint.tools collection (see also (Kousidis etal., 2012b))1?and present results of a study weperformed on whether speaker gaze and speakerarm movements can be turned into an informationsource for an interactive system.2 The mint.tools CollectionThe mint.tools collection comprises tools (andadaptations to existing tools) for recording andanalysis of multimodal data.
The recording archi-tecture (Figure 1) is highly modular: each infor-mation source (sensor) runs on its own dedicatedworkstation and transmits its data via the local areanetwork.
In the setup described in this paper, we1Available at http://dsg-bielefeld.de/mint/.kinect.srvfaceLab.srvMINT.toolsforacquisitionforanalysisinstantIOinstantplayerkinect.srvfaceLab.srv.xiomumodo.py /IPythonELAN.modMPI, ELANannotationtoolfame.rcfame.rpFraunhofer instant realityFigure 1: Overview of components of mint.tools;our contributions denoted by italics font.
Top mid-dle shows example lab setup; middle right showscorresponding VR scene, visualising motion cap-ture and tracking of head posture, eye and gazeperform motion capture via Microsoft Kinect andhead, eye and gaze tracking via SeeingmachinesFacelab 5.2 We have developed specialised plug-ins that connect these sensors to the central com-ponent in our architecture, Instantreality.3 Thisis a VR environment we use for monitoring therecording process by visualising a reconstructed3D scene in real-time.
A logging component si-multaneously streams the timestamped and inte-grated sensor data to disk, ensuring that all data aresynchronised.
The data format is a shallow XMLrepresentation of timed, typed events.The tracking equipment used in this setting iscamera-based, providing for a minimally invasivesetting, as subjects are not required to wear anyequipment or tracking markers.
In addition to thetracking sensors, video and audio are recorded us-2http://www.microsoft.com/en-us/kinectforwindows/, http://www.seeingmachines.com/product/facelab/, re-spectively3Built by IGD Fraunhofer, http://www.instantreality.org319ing one HD camera.
The AV channel is synchro-nised with the stream data from the sensors bymeans of a timecode in view of the camera.Representative of the high modularity and flexi-bility of the mint.tools architecture is the ease withwhich components can be added.
For the settingdescribed here, a GUI was created which connectsto the VR environment as an additional sensor,transmitting all of its state updates, which thenare synchronously logged together with all otherstream data from the trackers.
This allows us torecreate the full scene (subject behaviour and thestimuli they received) in the virtual reality envi-ronment, for later inspection (see below Figure 6).The analysis part of the mint.tools collectioncomprises a package for the Python programminglanguage (described below) and a version of theELAN annotation tool (Lausberg and Sloetjes,2009), which we modified to control the replay ofthe virtual reality scene; this makes it possible toview video, annotations and the 3D reconstructionat the same time and in synchronisation.Sensors are represented as nodes in a node-treewithin the 3D environment.
The values of datafields in these nodes are continuously updated asnew data is received from the network.
Usingmore than one sensor of the same type means sim-ply another instantiation of that node type withinthe tree.
In this way, our architecture facilitatestracking many people or complex setups wheremany sensors are required to cover an area.3 Procedure / The TAKE CorpusOur experiment is a Wizard-of-Oz scenario inwhich subjects (7 in total) were situated in front ofa 40?
screen displaying random Pentomino boards(Ferna?ndez et al 2007).
Each board configura-tion had exactly 15 Pentomino pieces of variouscolours and shapes, divided in four grids locatednear the four corners of the screen (see Figure 3below).
At the beginning of the session, a head andgaze model were created for the subject within theFaceLab software.
Next, the subjects were askedto point (with their arm stretched) at the four cor-ners and the center of the screen (with each hand),to calibrate to their pointing characteristics.In the main task, subjects were asked to(silently) choose a piece and instruct the ?system?to select it, using speech and/or pointing gestures.A wizard then selected the indicated piece, caus-ing it to be highlighted.
Upon approval by thesubject, the wizard registered the result and a newboard was created.
We denote the time-span fromthe creation of a board to the acknowledgementby the subject that the correct piece was selectedan episode.
The wizard had the option to not im-mediately highlight the indicated piece, in orderto elicit a more detailed description of the pieceor a pointing gesture.
What we were interestedin learning from these data was whether speakergaze and arm movements could be turned into sig-nals that can support a model of situated languageunderstanding.
We focus here on the signal pro-cessing and analysis that was required; the modelis described in (Kennington et al 2013).4 Analysis and ResultsWe perform the analyses described in this sec-tion using the analysis tools in the mint.tools col-lection, mumodo.py.
This is a python packagewe have developed that interfaces our recordedstream data with powerful, freely available, sci-entific computing tools written in the Python pro-gramming language.4 mumodo.py facilitates im-porting streamed data into user-friendly, easilymanageable structures such as dataframes (tableswith extended database functionality), or compati-ble formats such as Praat TextGrids (Boersma andWeenink, 2013) and ELAN tiers.
In addition, mu-modo.py can remote-control playback in ELANand Instant Reality for the purpose of data view-ing and annotation.4.1 GazeOur post-processing and analysis of the gaze datafocuses primarily on the detection of eye fixationsin order to determine the pentomino pieces that thesubjects look at while speaking.
This knowledgeis interesting from a reference resolution point ofview.
Although Koller et al2012) explored lis-tener gaze in that context, it is known that gaze pat-terns differ in interactions, depending on whetherone speaks or listens (Jokinen et al 2009).Facelab provides a mapping between a person?sgaze vector and the screen, which yields an in-tersection point in pixel coordinates.
However,due to limitations to the accuracy of the calibra-tion procedure and noise in the data, it is pos-4Especially IPython and Pandas, as collected for exam-ple in https://www.enthought.com/products/epd/.
Example of finished analyses using this packagecan be found at http://dsg-bielefeld.de/mint/mintgaze.html320sible that the gaze vector does not intersect themodel of the screen when the subject is looking atpieces near screen corners.
For this reason, we firstperform offline linear interpolation, artificially ex-tending the screen by 200 pixels in each direction,by means of linear regression of the x, y compo-nents of the gaze vector with the x, y pixel coordi-nates, respectively (R2 > 0.95 in all cases).
Fig-ure 2 shows the probability density function of in-tersection points before (left) and after this process(right), for one of the subjects.
We see on the rightplot that many intersection points fall outside theviewable screen area, denoted by the shaded rect-angle.Figure 2: Probability density function of gaze in-tersections on screen before (left) and after inter-polating for points 200 pixels around screen edges(right).
Shaded rectangle shows screen sizeIn order to detect the eye fixations, we use twocommon algorithms, namely the I-DT and ve-locity algorithms, as described in (Nystro?m andHolmqvist, 2010).
The I-DT algorithm requiresthe points to lie within a pre-defined ?dispersion?area (see Figure 3), while the velocity algorithmrequires the velocity to remain below a thresh-old.
In both algorithms, a minimum fixation timethreshold is also used, while a fixation centroid iscalculated as the midpoint of all points in a fixa-tion.
Increasing the minimum fixation time thresh-old and decreasing the dispersion area or velocity(depending on the algorithm) results in fewer fix-ations being detected.Figure 3: Fixation detection using the I-DT algo-rithm, circles show the dispersion radius thresholdGaze fixations can be combined with informa-tion on the pentomino board in order to determinewhich piece is being looked at.
To do this, we cal-culate the euclidean distance between each pieceand the fixation centroid, and assign the piece aprobability of being gazed at, which is inverselyproportional to its distance from the centroid.Figure 4 illustrates the gazing behaviour of thesubjects during 1051 episodes: After an initialrapid scan of the whole screen (typically beforethey start speaking), subjects fixate on the piecethey are going to describe (the ?gold piece?).
Thisis denoted by the rising number of fixations on thegold piece between seconds 5?10.
At the sametime, the average rank of the gold piece is higher(i.e.
closer to 1, hence lower in the plot).
Subse-quently, the average rank drops as subjects tend tocasually look around the screen for possible dis-tractors (i.e.
pieces that are identical or similar tothe gold piece).We conclude from this analysis that, especiallyaround the onset of the utterance, gaze can providea useful signal about intended referents.Figure 4: Average Rank and Counts over time (allepisodes)4.2 Pointing GesturesWe detect pointing gestures during which the armis stretched from Kinect data (3D coordinates of20 body joints) using two different methods.
Thefirst is based on the distance of the hand joint fromthe body (Sumi et al 2010).
We define the bodyas a plane, using the coordinates of the two shoul-ders, shoulder-center and head joints, and use athreshold beyond which a movement is considereda possible pointing gesture.The second detection method uses the idea that,while the arm is stretched, the vectors defined bythe hand and elbow, and hand and shoulder joints,respectively, should be parallel, i.e.
have a dotproduct close to 1 (vectors are first normalised).321Figure 5: detection of pointing thresholds by dis-tance of left(blue) or right(green) hand from bodyIn reality, the arm is never strictly a straight line,hence a threshold (0.95-0.98) is set, depending onthe subject.
The result of this process is an an-notation tier of pointing gestures (for each hand),similar to the one shown in Figure 5.
To makepointing gesture detection more robust, we onlyconsider gestures identified by both methods, i.e.the intersection of the two annotation tiers.Further, we want to map the pointing gestures tolocations on the screen.
Following a methodologysimilar to Pfeiffer (2010), we define two methodsof determing pointing direction: (a) the extensionof the arm, i.e.
the shoulder-hand vector, and (b)the hand-head vector, which represents the subjec-tive point-of-view (looking through the tip of one?sfinger).
Figure 6 shows both vectors: dependingon the subject and the target point, we have foundthat both of these vectors perform equally well, byconsidering the gaze intersection point (green doton screen) and assuming that subjects are lookingwhere they are pointing.Figure 6: Hand-to-head and hand-to-shoulderpointing vectorsIn order to map the pointing gestures to ac-tual locations on the screen, we use the calibra-tion points acquired at the beginning of the ses-sion, and plot their intersections to the screenplane, which we compute analytically, as we al-ready have a spatial model of both the vector inquestion (Kinect data) and the screen location (In-stantreality model).Based on the pointing gestures we have de-tected, we look at the pointing behaviour of par-ticipants as a function of the presence of distrac-tors.
This knowledge can be used in designingsystem responses in a multimodal interactive en-viroment or in training models to expect pointinggestures depending on the state of the scene.
Fig-ure 7 shows the result from 868 episodes (a subsetthat satisfies minor technical constraints).
Overall,the subjects pointed in 60% of all episodes.
Pieceson the board may share any of three properties:shape, colour, and location (being in the same cor-ner on the screen).
The left plot shows that sub-jects do not point more than normal when onlyone property is shared, regardless of how manysuch distractors are present, while they point in-creasingly more when pieces that share two or allthree properties exist.
The plot on the right showsthat subjects point more when the number of samecolour pieces increases (regardless of position andshape) and even more when identical pieces occuranywhere on the board.
Interestingly, shape by it-self does not appear to be considered a distractorby the subjects.Figure 7: Frequency of pointing gestures as afunction of the presence of distractors.
Dot sizedenotes the confidence of each point, based onsample size5 ConclusionsWe have presented a detailed account of analysisprocedures on multimodal data acquired from ex-periments in situated human-computer interaction.These analyses have been facilitated by mint.tools,our collection of software components for mul-timodal data acquisition, annotation and analysisand put to use in (Kennington et al 2013).
Wewill continue to further improve our approach formanageable and easily reproducible analysis.322ReferencesPaul Boersma and David Weenink.
2013.
Praat: do-ing phonetics by computer (version 5.3.48)[com-puter program].
retrieved may 1, 2013.Raquel Ferna?ndez, Andrea Corradini, DavidSchlangen, and Manfred Stede.
2007.
To-wards Reducing and Managing Uncertainty inSpoken Dialogue Systems.
In Proceedings of the7th International Workshop on ComputationalSemantics (IWCS?07), pages 1?3.Kristiina Jokinen, Masafumi Nishida, and Seiichi Ya-mamoto.
2009.
Eye-gaze experiments for conversa-tion monitoring.
In Proceedings of the 3rd Interna-tional Universal Communication Symposium, pages303?308.
ACM.Casey Kennington, Spyros Kousidis, and DavidSchlangen.
2013.
Interpreting situated dialogue ut-terances: an update model that uses speech, gaze,and gesture information.
In Proceedings of SIGdial2013.Alexander Koller, Maria Staudte, Konstantina Garoufi,and Matthew Crocker.
2012.
Enhancing referen-tial success by tracking hearer gaze.
In Proceed-ings of the 13th Annual Meeting of the Special Inter-est Group on Discourse and Dialogue, pages 30?39.Association for Computational Linguistics.Spyros Kousidis, Thies Pfeiffer, Zofia Malisz, PetraWagner, and David Schlangen.
2012a.
Evaluat-ing a minimally invasive laboratory architecture forrecording multimodal conversational data.
In Proc.of the Interdisciplinary Workshop on Feedback Be-haviours in Dialogue.Spyros Kousidis, Thies Pfeiffer, and David Schlangen.2012b.
Mint.tools: Tools and adaptors supportingacquisition, annotation and analysis of multimodalcorpora.
In to appear in Proc.
of Interspeech 2013.Hedda Lausberg and Han Sloetjes.
2009.
Coding ges-tural behavior with the neuroges-elan system.
Be-havior research methods, 41(3):841?849.Marcus Nystro?m and Kenneth Holmqvist.
2010.
Anadaptive algorithm for fixation, saccade, and glis-sade detection in eyetracking data.
Behavior re-search methods, 42(1):188?204.Thies Pfeiffer.
2010.
Understanding multimodal deixiswith gaze and gesture in conversational interfaces.Ph.D.
thesis, Bielefeld University, Technical Fac-ulty.Yasuyuki Sumi, Masaharu Yano, and Toyoaki Nishida.2010.
Analysis environment of conversational struc-ture with nonverbal multimodal data.
In Interna-tional Conference on Multimodal Interfaces and theWorkshop on Machine Learning for Multimodal In-teraction, page 44.
ACM.323
