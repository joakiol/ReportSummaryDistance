Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 292?295,Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational LinguisticsCityU-DAC: Disambiguating Sentiment-Ambiguous Adjectives withinContextBin LU and Benjamin K. TSOUDepartment of Chinese, Translation and Linguistics &Language Information Sciences Research CentreCity University of Hong Kong{lubin2010, rlbtsou}@gmail.comAbstractThis paper describes our systemparticipating in task 18 of SemEval-2010,i.e.
disambiguating Sentiment-Ambiguous Adjectives (SAAs).
Todisambiguating SAAs, we compare themachine learning-based and lexicon-based methods in our submissions: 1)Maximum entropy is used to trainclassifiers based on the annotatedChinese data from the NTCIR opinionanalysis tasks, and the clause-level andsentence-level classifiers are compared;2) For the lexicon-based method, we firstclassify the adjectives into two classes:intensifiers (i.e.
adjectives intensifyingthe intensity of context) and suppressors(i.e.
adjectives decreasing the intensity ofcontext), and then use the polarity ofcontext to get the SAAs?
contextualpolarity based on a sentiment lexicon.The results show that the performance ofmaximum entropy is not quite high dueto little training data; on the other hand,the lexicon-based method could improvethe precision by considering the polarityof context.1 IntroductionIn recent years, sentiment analysis, which minesopinions from information sources such as news,blogs, and product reviews, has drawn muchattention in the NLP field (Hatzivassiloglou andMcKeown, 1997; Pang et al, 2002; Turney,2002; Hu and Liu, 2004; Pang and Lee, 2008).
Ithas many applications such as social mediamonitoring, market research, and publicrelations.Some adjectives are neutral in sentimentpolarity out of context, but they could showpositive, neutral or negative meaning withinspecific context.
Such words can be calleddynamic sentiment-ambiguous adjectives(SAAs).
However, SAAs have not beenintentionally tackled in the researches ofsentiment analysis, and usually have beendiscarded or ignored by most previous work.
Wuet al, (2008) presents an approach of combiningcollocation information and SVM todisambiguate SAAs, in which the collocation-based method was first used to disambiguateadjectives within the context of collocation (i.e.
asub-sentence marked by comma), and then theSVM algorithm was explored for those instancesnot covered by the collocation-based method.According to their experiments, their supervisedalgorithm achieves encouraging performance.The task 18 at SemEval-2010 is intended tocreate a benchmark dataset for disambiguatingSAAs.
Given only 100 trial sentences, but notprovided with any official training data,participants are required to tackle this problemdata by unsupervised approaches or use theirown training data.
The task consists of 14 SAAs,which are all high-frequency words in MandarinChinese.
They are ?|big, ?|small, ?|many, ?|few, ?|high, ?|low, ?|thick, ?|thin, ?|deep,?|shallow, ?|heavy, ?|light, ?
?|huge, ??|grave.
This task deals with Chinese SAAs, butthe disambiguating techniques should belanguage-independent.
Please refer to (Wu andJin, 2010) for more descriptions of the task.In our participating system, the annotatedChinese data from the NTCIR opinion analysistasks is used as training data with the help of acombined sentiment lexicon.
A machinelearning-based method (namely maximumentropy) and the lexicon-based method arecompared in our submissions.
The results showthat the performance of maximum entropy is notquite high due to little training data; on the otherhand, the lexicon-based method could improve292the precision by considering the context ofSAAs.
In Section 2, we briefly describe datapreparation of sentiment lexicon and trainingdata.
Our approaches for disambiguating SAAsare given in Section 3.
The experiment andresults are presented in Section 4, followed by aconclusion in Section 5.2 Data Preparation2.1 Sentiment LexiconSeveral traditional Chinese resources of polarwords/phrases are collected, including NTUSentiment Dictionary1, The Lexicon of ChinesePositive Words (Shi and Zhu, 2006), The Lexiconof Chinese Negative Words (Yang and Zhu, 2006)0, and CityU?s sentiment-bearing word/phraselist (Lu et al 2008), which were manuallymarked in the political news data by trainedannotators (Benjamin and Lu, 2008).
Sentiment-bearing items marked with the SENTIMENT_KWtag (SKPI), including only positive and negativeitems but not neutral ones, were alsoautomatically extracted from the Chinese sampledata of NTCIR-6 OAPT (Seki et al, 2007).
Allthese polar item lexicons were combined, and thecombined polar item lexicon consists of 13,437positive items and 18,365 negative items, a totalof 31,802 items.2.2 Training DataThe training data is extracted from the Chinesesample and test data from the NTCIR opinionanalysis task, including NTCIR-6 (Seki et al,2007), NTCIR-7 (Seki et al, 2008) and NTCIR-8(Seki et al, 2010).
The NTCIR opinion analysistasks provide an opportunity to evaluate thetechniques used by different participants basedon a common evaluation framework in Chinese(simplified and traditional), Japanese andEnglish.For data from NTCIR-6 and NTCIR-7, threeannotators manually marked the polarity of eachopinionated sentence, and the lenient polarity isused here as the gold standard (please refer toSeki et al, 2008 for explanation of lenient andstrict standard).
For each opinionated sentencefrom NTCIR-8, only two annotators marked andthe strict polarity is used as the gold standard.The traditional Chinese sentences are transferredinto simplified Chinese.
In total, there are about12K opinionated sentences annotated withpolarity, out of which about 9K are marked as1 http://nlg18.csie.ntu.edu.tw:8080/opinion/index.htmlpositive or negative, and others neutral.
All the9K sentences plus the 100 sentences from thetrial data are used as the sentence-level trainingdata.Meanwhile, we also try to get the clause-leveltraining data since the context of collocationwithin sub-sentences are quite crucial fordisambiguating SAAs according to Wu et al(2008).
From the 9K positive/ negative sentencesabove, we automatically extract the clause foreach occurrence of SAAs.Note the polarity for a whole sentence is notnecessarily the same with that of the clausecontaining SAAs.
Consider the sentence ?
???
??
?
??
?
?
??
??
??
??
(In the current large circumstance of the world,China and Russia support each other).
Thepolarity of the whole sentence is positive, whilethe clause ??????????
(In the currentlarge circumstance of the world) containing aSAA ?
(large) is neutral, and the polarity lies inthe second part of the whole sentence, i.e.
????
(support each other).Thus, we manually checked the polarity ofclauses containing SAAs.
Due to time limitation,we only checked 465 clauses.
Plus the clausesextracted from 100 trial sentences, the finalclause-level training data consist of 565positive/negative clauses containing SAAs.3 Our Approach for DisambiguatingSAAsTo disambiguating SAAs, we use the maximumentropy algorithm and the sentiment lexicon-based method, and also combine them together.3.1 The Maximum Entropy-based MethodMaximum entropy classification (MaxEnt) is atechnique which has proven effective in anumber of natural language processingapplications (Berger et al, 1996).
Le Zhang?smaximum entropy tool2 is used for classification.The Chinese sentences are segmented intowords using a production segmentation system.Unigrams of words are used as basic features forclassification.
Bigrams are also tried, but doesnot show improvement, and thus are notdescribed in details here.3.2 The Lexicon-based MethodFor the lexicon-based method, we first classifythe 14 adjectives into two classes: intensifiers2 http:// homepages.inf.ed.ac.uk/lzhang10/maxent_toolkit.html293and suppressors.
Intensifiers refer to adjectivesintensifying the intensity of context, including ?|big, ?
|many, ?
|high, ?
|thick, ?
|deep, ?|heavy, ?
?|huge, ?
?|grave, while suppressorsrefer to adjectives decreasing the intensity ofcontext, including ?|small, ?|few, ?|low, ?|thin, ?|shallow, ?|light.Meanwhile, the collocation nouns are alsoclassified into two classes: positive and negative.Positive nouns include ?
?
|quality, ?
?|standard, ?
?
|level, ?
?
|benefit, ?
?|achievement, etc.
Negative nouns include ?
?|pressure, ??
|disparity, ??
|problem, ?
?|risk, ?
?|pollution etc.The hypothesis here is that intensifiers willreceive the polarity of their collocations whilesuppressors will get the opposite polarity of theircollocations.
For example, ??
|achievementcould be collocated with one of the followingintensifiers: ?|big, ?|many or ?|high, and theadjectives just receive the polarity of ?
?|achievement, which is positive.
Meanwhile, ?
?|pollution could be collocated with one of thefollowing suppressors: ?|small, ?|few, ?|low,and the adjectives just receive the oppositepolarity of?
?|pollution, which is also positive.Based on this hypothesis, we could get thepolarity of SAAs through theirs collocationnouns within the clauses containing SAAs.
Thecontext of SAAs is a sub-sentence marked bycomma.
The sentiment lexicon mentioned inSection 2.1 is used to find polarity of collocationnouns.3.3 Combining Maximum Entropy andLexiconTo combine the two methods above, the lexicon-based method is first used to disambiguate thesentiment of SAAs, and the context ofcollocation is a sub-sentence marked by comma.Then for those instances that are not covered bythe lexicon-based method, the maximum entropyalgorithm is explored.4 Experiment and ResultsThe dataset contains two parts: some sentenceswere extracted from Chinese Gigaword (LDCcorpus: LDC2005T14), and other sentences weregathered through the search engine like Google.Firstly, these sentences were automaticallysegmented and POS-tagged, and then theambiguous adjectives were manually annotatedwith the correct sentiment polarity within thesentence context.
Two annotators annotated thesentences double blindly, and the third annotatorchecks the annotation.
All the data of 2,917sentences is provided as the test set, andevaluation is performed in terms of microaccuracy and macro accuracy.We submitted 4 runs: run 1 is based on thesentence-level MaxEnt classifier; run 2 on theclause-level MaxEnt classifier; run 3 is got bycombining the lexicon-based method and thesentence-level MaxEnt classifier; and run 4 bycombining the lexicon-based method and theclause-level MaxEnt classifier.
The officialscores for the 4 runs are shown in Table 2.Table 2.
Results of 4 RunsRun Micro Acc.
(%) Macro Acc.
(%)1 61.98 67.892 62.63 60.853 71.55 75.544 72.47 69.80From Table 2, we can observe that:1) Compared the highest scores achieved byother teams, the performance of maximumentropy (run 1 and 2) is not quite high due tolittle training data;2) By integrating the lexicon-based methodand maximum entropy (run 3 and 4), we improvethe accuracy by considering the context of SAAs;3) The sentence-level maximum entropyclassifier shows better macro accuracy, andclause-level one better micro accuracy.In addition to the official scores, we alsoevaluate the performance of the lexicon-basedmethod alone.
The micro and macro accuracy arerespectively 0.847 and 0.835665, showing thatthe lexicon-based method is more accurate thanthe maximum entropy algorithm (run 1 and 2).But it only covers 1,436 (49%) of 2,917 testinstances.Because the data from the NTCIR opinionanalysis task is not specifically annotated for thistask, and the manually checked clauses are lessthan 600, the performance of our system is notquite high compared to the highest performanceachieved by other teams.5 ConclusionTo disambiguating SAAs, we compare machinelearning-based and lexicon-based methods in oursubmissions: 1) Maximum entropy is used totrain classifiers based on the annotated Chinesedata from the NTCIR opinion analysis tasks, andthe clause-level and sentence-level classifiers are294compared; 2) For the lexicon-based method, wefirst classify the adjectives into two classes:intensifiers (i.e.
adjectives intensifying theintensity of context) and suppressors (i.e.adjectives decreasing the intensity of context),and then use the polarity of context to get theSAAs?
contextual polarity.
The results show thatthe performance of maximum entropy is notquite high due to little training data; on the otherhand, the lexicon-based method could improvethe precision by considering the context ofSAAs.ReferencesAdam L. Berger, Stephen A. Della Pietra, and VincentJ.
Della Pietra.
1996.
A maximum entropyapproach to natural language processing.Computational Linguistics, 22(1):39-71.Vasileios Hatzivassiloglou and Kathleen McKeown.1997.
Predicting the Semantic Orientation ofAdjectives.
Proceedings of ACL-97.
174-181.Minqing Hu and Bing Liu.
2004.
Mining OpinionFeatures in Customer Reviews.
In Proceedings ofthe 19th National Conference on ArtificialIntelligence, pp.
755-760.Bin Lu, Benjamin K. Tsou and Oi Yee Kwong.
2008.Supervised Approaches and Ensemble Techniquesfor Chinese Opinion Analysis at NTCIR-7.
InProceedings of the Seventh NTCIR Workshop(NTCIR-7).
pp.
218-225.
Tokyo, Japan.Bo Pang and Lillian Lee.
2008.
Opinion mining andsentiment analysis, Foundations and Trends inInformation Retrieval, Now Publishers.Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.2002.
Thumbs up?
Sentiment classification usingmachine learning techniques.
In Proceedings ofEMNLP 2002, pp.79?86.Yohei Seki, David Kirk Evans, Lun-Wei Ku, Le Sun,Hsin-His Chen, Noriko Kando.
2007.
Overview ofOpinion Analysis Pilot Task at NTCIR-6.
Proc.
ofthe Seventh NTCIR Workshop.
Japan.
2007.6.Yohei Seki, David Kirk Evans, Lun-Wei Ku, Le Sun,Hsin-His Chen, Noriko Kando and Chin-Yew Lin.2008.
Overview of Multilingual Opinion AnalysisTask at NTCIR-7.
Proc.
of the Seventh NTCIRWorkshop.
Japan.
Dec. 2008.Yohei Seki, Lun-Wei Ku, Le Sun, Hsin-His Chen,Noriko Kando.
2010.
Overview of MultilingualOpinion Analysis Task at NTCIR-8.
Proc.
of theSeventh NTCIR Workshop.
Japan.
June, 2010.Jilin Shi and Yinggui Zhu.
2006.
The Lexicon ofChinese Positive Words (?????).
SichuanLexicon Press.Benjamin K. Tsou and Bin Lu.
2008.
A PoliticalNews Corpus in Chinese for Opinion Analysis.
InProceedings of the Second International Workshopon Evaluating Information Access (EVIA2008).
pp.6-7.
Tokyo, Japan.Peter D. Turney.
2002.
Thumbs up or thumbs down?Semantic orientation applied to unsupervisedclassification of reviews, In Proceedings of ACL-02, Philadelphia, Pennsylvania, 417-424.Yunfang Wu, Miao Wang, Peng Jin and Shiwen Yu.2008.
Disambiguate sentiment ambiguousadjectives.
In Proceedings of  IEEE InternationalConference on Natural Language Processing andKnowledge Engineering (NLP-KE?08).Yunfang Wu, and Peng Jin.
2010.
SemEval-2010Task 18: Disambiguate sentiment ambiguousadjectives.
In Proceedings of SemEval-2010.Ruifeng Xu, Kam-Fai Wong and Yunqing Xia.
2008.Coarse-Fine Opinion Mining - WIA in NTCIR-7MOAT Task.
In Proceedings of the Seventh NTCIRWorkshop (NTCIR-7).
Tokyo, Japan, Dec. 16-19.Ling Yang and Yinggui Zhu.
2006.
The Lexicon ofChinese Negative Words (?????).
SichuanLexicon Press.295
