A LOGICAL  VERSION OF  FUNCTIONAL GRAMMARWilliam C. RoundsUniversity of MichiganXerox PARCAlexis Manaster-RamerIBM T.J. Watson Research CenterWayne State UniversityI AbstractKay's functional-unification grammar notation \[5\] isa way of expressing rammars which relies on very fewprimitive notions.
The primary syntactic structure is thefeature structure, which can be visualised as a directedgraph with arcs labeled by attributes of a constituent, andthe primary structure-building operation is unification.In this paper we propose a mathematical formulation ofFUG, using logic to give a precise account of the stringsand the structures defined by any grammar written inthis notation.2 Int roduct ionOur basic approach to the problem of syntactic de-scription is to use logical formulas to put conditions orconstraints on ordering of constituents, ancestor and de-scendant relations, and feature attribute information insyntactic structures.
The present version of our logichas predicates specifically designed for these purposes.A grammar can be considered as just a logical formula,and the structures satisfying the formula are the syntacticstructures for the sentences of the language.
This notiongoes back to DCG's  \[0\], but our formulation is quite dif-ferent.
In particular, it builds on the logic of Kasper andRounds \[3\], a logic intended specifically to describe fea-ture structures.The formulation has several new aspects.
First, itintroduces the oriented feature structure as the primarysyntactic structure.
One can think of these structuresas parse trees superimposed on directed graphs, althoughthe general definition allows much more flexibility.
Infact, our notation does away with the parse tree alto-gether.A second aspect of the notation is its treatment ofword order.
Our logic allows small grammars to definefree-word order languages over large vocabularies in a waynot possible with standard ID/LP rules.
It is not clearwhether or not this treatment of word order was intendedby Kay, but the issue naturally arose during the processof making this model precise.
(Joshi \[1\] has adopted muchthe same conventions in tree adjunct grammar.
)A third aspect of our treatment is the use of fixed-point formulas to introduce recursion into grammars.
Thisidea is implicit in DCG's, and has been made explicit inthe logics CLFP  and ILFP \[9\].
We give a simple way ofexpressing the semantics of these formulas which corre-sponds closely to the usual notion of grammatical deriva-tions.
There is an interesting use of type ~ariables todescribe syntactic categories and/or constructions.We illustrate the power of the notation by sketchinghow the constructions of relational grammar \[7\] can beformulated in the logic.
To our knowledge, this is thefirst attempt to interpret the relational ideas in a fullymathematical framework.
Although relational networksthemselves have been precisely specified, there does notseem to be a precise statement of how relational deriva-tions take place.
We do not claim that our formalizationis the one intended by Postal and Perlmutter, but wedo claim that our notation shows clearly the relationshipof relational to transformational grammars on one hand,and to lexical-functional grammars on the other.Finally, we prove that the satisfiability problem for ourlogic is undecidable.
This should perhaps be an expectedresult, because the proof relies on simulating Turing ma-chine computations in a grammar, and follows the stan-dard undecidability arguments.
The satisfiability prob-lem is not quite the same problem as the aniversal recog-nition problem, however, and with mild conditions onderivations similar to those proposed for LFG \[2\], thelatter problem should become decidable.We must leave efficiency questions unexamined in thispaper.
The notation has not been implemented.
We viewthis notation as a temporary one, and anticipate thatmany revisions and extensions will be necessary if it is tobe implemented at all.
Of course, FUG itself could beconsidered as an implementation, but we have added theword order relations to our logic, which are not explicitin FUG.In this paper, which is not full because of space limi-tations, we will give definitions and examples in Section3; then will sketch the relational application in Section 4,and will conclude with the undecidability result and somefinal remarks.3 Def in i t ions and examples3.1 Or iented  f -structuresIn this section we will describe the syntactic structuresto which our logical formulas refer.
The next subsection89obi,e de~.,.
CFigure i: A typical DG.
Figure 2: An oriented f-structure for a4b4c 4.will give the logic itself.
Our intent is to represent notonly feature information, but also information about or-dering of constituents in a single structure.
We begin withthe unordered version, which is the simple DG (directedgraph) structure commonly used for non-disjunctive in-formation.
This is formalized as an acyclic finite automa-ton, in the manner of Kasper-Rounds \[3\].
Then we addtwo relations on nodes of the DG: ancestor and linearprecedence.
The key insight about these relations is thatthey are partial; nodes of the graph need not participatein either of the two relations.
Pure feature informationabout a constituent need not participate in any ordering.This allows us to model the "cset" and "pattern" infor-mation of FUG, while allowing structure sharing in theusual DG representation of features.We are basically interested in describing structureslike that shown in Figure i.A formalism appropriate for specifying such DG struc-tures is that of finite automata theory.
A labeled DG canbe regarded as a transition graph for a partially speci-fied deterministic finite automaton.
We will thus use theordinary 6 notation for the transition function of the au-tomaton.
Nodes of the graph correspond to states of theautomaton, and the notation 6(q, z) implies that startingat state(node) q a transition path actually exists in thegraph labeled by the sequence z, to the state 6(q, z).Let L be a set of arc labels, and A be a set of atomicfeature values.
An  ( A, L)- automaton is a tuple.4 = (Q,6,qo, r)where Q is a finite set of states, q0 is the initial state, L isthe set of labels above, 6 is a partial function from Q x L toQ, and r is a partial function from terminating states of Ato A.
(q is terminating if 6(q, l) is undefined for all l ?
L.)We require that ,4 be connected and acyclic.
The map rspecifies the atomic feature values at the final nodes of theDG.
(Some of these nodes can have unspecified values, tobe unified in later.
This is why r is only partial.)
Let F bethe set of terminating states of.A, and let PC.A) be the setof full paths of,4, namely the set {z ?
L* : 6(q0, z) ?
F}.Now we add the constituent ordering information tothe nodes of the transition graph.
Let Z be the termi-nal vocabulary (the set of all possible words, morphemes,etc.)
Now r can be a partial map from Q to E u A, withthe requirement that if r(q) ?
A, then q ?
F. Next,let a and < be binary relations on Q, the ancestor andprecedence relations.
We require a to be reflexive, an-tisymmetric and transitive; and the relation < must beirrefiexive and transitive.
There is no requirement thatany two nodes must be related by one or the other of theserelations.
There is, however, a compatibility constraintbetween the two relations:v(q, r, 8, t) ?
Q : (q < ~) ^  (q a s) ^ (~ a t) = s < t.Note: We have required that the precedence and dom-inance relations be transitive.
This is not a necessaryrequirement, and is only for elegance in stating condi-tions like the compatibility constraint.
A better formula-tion of precedence for computational purposes would bethe "immediate precedence" relation, which says that oneconstituent precedes another, with no constituents inter-vening.
There is no obstacle to having such a relation inthe logic directly.Example.
Consider the structure in Figure 2.
Thisgraph represents an oriented f-structure arising from aLFG-style grammar for the language {anb"c n I n > I}.In this example, there is an underlying CFG given bythe following productions:S - -  TCT- -  aTb labC - -cC lc .The arcs labeled with numbers (1,2,3) are analogousto arcs in the derivation tree of this grammar.
The rootnode is of "category" S, although we have not representedthis information in the structure.
The nodes at the endsof the arcs 1,2, and 3 are ordered left to right; in ourlogic this will be expressed by the formula I < 2 < 3.The other arcs, labeled by COUNT and #, are feature90arcs used to enforce the counting information required bythe language.
It is a little difficult in the graph repre-sentation to indicate the node ordering information andthe ancestor information, so this will wait until the nextsection.
Incidentally, no claim is made for the linguisticnaturalness of this example!3.2 A presentat ion  o f  the  log icWe will introduce the logic by continuing the exam-ple of the previous ection.
Consider Figure 2.
Particu-lar nodes of this structure will be referenced by the se-quences of arc labels necessary to reach them from theroot node.
These sequences will be called paths.
Thusthe path 12223 leads to an occurrence of the terminalsymbol b.
Then a formula of the form, say, 12 COUNT -22 COUNT would indicate that these paths lead to thesame node.
This is also how we specify linear precedence:the last b precedes the first c, and this could be indicatedby the formula 12223<22221.It should already be clear that our formulas will de-scribe oriented f-structures.
We have just illustrated twokinds of atomic formula in the logic.
Compound formulaswill be formed using A (and), and V (or).
Additionally,let I be an arc label.
Then an f-structure will satisfy a for-mula of the form I : ?, iff there is an/-transition from theroot node to the root of a substructure satisfying ~b.
Whatwe have not explained yet is how the recursive informa-tion implicit in the CFG is expressed in our logic.
To dothis, we introduce type variables as elementary formulasof the logic.
In the example, these are the "category"variables S, T, and C. The grammar is given as a systemof equations (more properly, equivalences), relating thesevariables.We can now present a logical formula which describesthe language of the previous ection.S whereS ::~C ::~VT ::-"Vl :TA2:CA( Icount - -  2count)A(1 <2) A~b12( l :cA2:CA(count  #---- 2count) A?1~)(i :CA(count ~ -- end) A ~I)(I :aA2:TA3:bA(count  #---- 2count)A (I < 2) A (2 < 3) A?1~z)( l :aA2:bA (count # : end) A (I < 2) A ~b12),where ?I~ is the formula (e a 1) A (e a 2), in which e isthe path of length 0 referring to the initial node of thef-structure, and where the other ~ formulas are similarlydefined.
(The ~b formulas give the required dominanceinformation.
)In this example, the set L - (1,2, 3, #,  count}, the setE - {a,b,c}, and the set A -- {end}.
Thus the atomicsymbol "end" does not appear as part of any derivedstring.
It is easy to see how the structure in Figure 2satisfies this formula.
The whole structure must satisfythe formula S, which is given recursively.
Thus the sub-structure at the end of the 1 arc from the root must satisfythe clause for T, and so forth.It should now be clearer why we consider our logic alogic for functional grammar.
Consider the FUG descrip-tion in Figure 3.According to \[5, page 149\], this descril~tion specifiessentences, verbs, or noun phrases.
Let us call such struc-tures "entities", and give a partial translation of this de-scription into our logic.
Create the type variables ENT,S, VERB,  and NP.
Consider the recursive formulaENT whereENT ::=S ::--S v NP  v VERBsubj : NP  A pred : VERBA(subj < pred)A((seomp : none) V (seomp : SA(pred <scomp)))Notice that the category names can be represented astype variables, and that the categories NP and VERBare free type variables.
Given an assignment of a set off-structures to these type variables, the type ENT willbecome well-specified.A few other points need to be made concerning thisexample.
First, our formula does not have any ancestorinformation in it, so the dominance relations implicit inKay's patterns axe not represented.
Second, our word or-der conventions are not the same as Kay's.
For example,in the pattern (subj pred...), it is required that the sub-ject be the very first constituent in the sentence, and thatnothing intervene between the subject and predicate.
Tomodel this we would need to add the "immediately eft of"predicate, because our < predicate is transitive, and doesnot require this property.
Next, Kay uses "CAT" arcs torepresent category information, and considers "NP" to bean atomic value.
It would be possible to do this in ourlogic as well, and this would perhaps not allow NPs to beunified with VERBs.
However, the type variables wouldstill be needed, because they are essential for specifyingrecursion.
Finally, FUG has other devices for special pur-poses.
One is the use of nonlocai paths, which are usedat inner levels of description to refer to features of the"root node" of a DG.
Our logic will not treat these, be-cause in combination with recursion, the description ofthe semantics is quite complicated.
The full version ofthe paper will have the complete semantics.9\]cat = Spattern = (subj pred.. .
)i:i: } I cat = VERB \] $corrlp -~.
none \] pattern = (.. .
scomp) \] ?
co~p = \[ ~at = S \] Jcat = N P \]cat = VERB \]Figure 3: Disjunctive specification i  FUG.3.3  The fo rmal i sm3.3.1 SyntaxWe summarize the formal syntax of our logic.
Wepostulate a set A of atomic feature names, a set L ofattribute labels, and a set E of terminal symbols (wordentries in a lexicon.)
The type variables come from aset TVAR = {X0,Xt .
.
.
.
}.
The following list gives thesyntactical constructions.
All but the last four items areatomic formulas.1.
N IL2.
TOP3.
X, in which X E TVAR4.
a, in which a E A5.
o', in which o" E E6.
z<v,  in which z and v E L"7. x c~ V, in which z and V E L"8.
\[zt .
.
.
.
.
x~\], in which each z~ E L=9 .
/ :$10.
@^g,11.
~v ,~12.
~b where  \[Xt ::= ~bt;... X,~ ::= ~,\]Items (1) and (2) are the identically true and falseformulas, respectively.
Item (8) is the way we officiallyrepresent path equations.
We could as well have usedequations like z = V, where ~ and V E L ' ,  but our deft-nition lets us assert the simultaneous equality of a finitenumber of paths without writing out all the pairwise pathequations.
Finally, the last item (12) is the way to expressrecursion.
It will be explained in the next subsection.Notice, however, that the keyword where  is part of thesyntax.3.3.2 SemanticsThe semantics is given with a standard Tarski defini-tion based on the inductive structure of wffs.
Formulaeare satisfied by pairs (.4,p), where ,4 is an oriented f-structure and p is a mapping from type variables to setsoff-structures, called an environment.
This is needed be-cause free type variables can occur in formulas.
Here arethe official clauses in the semantics:NIL  always;TOP never;x iff.4 e p(X);a iff 7"(q0) = a, where q0 is the initial state1.
(.4, p)2.
(.4,p)3.
(.4,p)4.
(.4, p)of ,4;5.
(A,p)6.
(.4, p)T. (.4,p)8.
(.4, p)~, where o" E ~-, iff r(q0) = o';v < w iff 6(q0, v) < 6(qo, w);v a w iff 6(qo, v) a ~(qo, w);\[=~ .
.
.
.
.
=.\]  iffVi, j  : 6(q0,zl) = ~(qo,xj);9.
(.4,p) ~ l : ~ iff (.4/l,p) ~ ~, where .4/1 is theautomaton .4 started at 6(qo, l);10.
(A, p) ~ ~ ^  ~ iff (A, p) ~ ~ and (A, p) ~ ~;11.
(.4,p) ~ ~ V ~b similarly;12.
(.4,p) ~ ~b where  \[Xt ::= Ot ; .
.
.X ,  ::= 0n\] ifffor some k, (.4, p(~)) ~ ~b, where p(k) is definedinductively as follows:?
p (? )
(xo  = 0;?
p(k+~)(Xd = {B I (~,p(~)) \[= ,~,},and where p(k)(X) = p(X) if X # Xi for any i.We need to explain the semantics of recursion.
Oursemantics has two presentations.
The above definition isshorter to state, hut it is not as intuitive as a syntactic,operational definition.
In fact, our notation~b where  \[Xt ::= ~bl .
.
.
.
.
Xn : : -  ~bn\]92is meant o suggest hat the Xs can be replaced by the Csin ?.
Of course, the Cs may contain free occurrences ofcertain X variables, so we need to do this same replace-ment process in the system of Cs beforehand.
It turnsout that the replacement process is the same as the pro-cess of carrying out grammatical derivations, but makingreplacements of nonterminal symbols all at once.With this idea in mind, we can turn to the definitionof replacement.
Here is another advantage of our logic -replacement is nothing more than substitution of formu-las for type variables.
Thus, if a formula 0 has distinctfree type variables in the set D = {Xt .
.
.
.
.
An}, andCt, .
.
- ,  ?,  are formulas, then the notationdenotes the simultaneous replacement of any free occur-rences of the Xj in 0 with the formula Cj, taking careto avoid variable clashes in the usual way (ordinarily thiswill not be a problem.
)Now consider the formula?
where \[Xt ::= Ct ; .
- .X ,  ::= ?,\].The semantics of this can be explained as follows.
LetD = {XI ..... X,~}, and for each k _> 0 define a set offormulas {?~k) \[ I _< i _< n}.
This is done inductively onk:~o) = ?,\[X *-- TOP : X E D\];?
(k+1) .- elk) i = ~' i \ [X : X e O\] .These formulas, which can be calculated iteratively, cor-respond to the derivation process.Next, we consider the formula ?.
In most grammars,?
will just be a "distinguished" type variable, say S. If(`4, p) is a pair consisting of an automaton and an envi-ronment, then we define(`4, p) ~ ?
where \[Xt ::= ?
i ; .
.
.X , t  ::= ?,\]iff for some k,(.4, p) ~ ?\[X, , -  elk): X, E D\].Example.
Consider the formula (derived from a reg-ular grammar)S whereT " '~(I :aA2  : S) V(I :hA2  :T) Vc(I :bA2  : S) V(I :aA2  : T) Vd.Then, using the above substitutions, and simplifying ac-cording to the laws of Kasper-Rounds, we have?
(s o)C,?~) = d;CH) = (1:aA2:c) V(1:bA2:d)Vc;?
(~) = (1:bA2:c) V(1:aA2:d)Vd;?
(2) = I:aA2:(I:aA2:c) V(I:bA2:d)Vc)V l:bA2:((l:bA2:c) V(l:aA2:d)Vd)VC.The f-structures defined by the successive formulas for Scorrespond in a natural way to the derivation trees of thegrammar underlying the example.Next, we need to relate the official semantics to thederivational semantics just explained.
This is done withthe help of the following lemmas.Lemma 1 (`4,p) ~ ?~) ~ (`4, p(k)) ~ ?i.Lemma 2 (`4,p) ~ 0\[Xj - -  ?./ : X./ E D\] i f f(`4,p')O, where p?
(Xi) = {B \] (B,p) ~ ?i}, if Xi E D, andotherwise is p(X).The proofs are omitted.Finally, we must explain the notion of the languagedefined by ?, where ?
is a logical formula.
Suppose forsimplicity that $ has no free type variables.
Then thenotion A ~ 0 makes sense, and we say that a stringw E L(~b) iff for some subsumpfion.minirnal f-structure,4, A ~ ?, and w is compatible with ,4.
The notionof subsumption is explained in \[8\].
Briefly, we have thefollowing definition.Let ,4 and B be two automata.
We say ,4 _ B (.4subsumes B; B extends `4) iff there is a homomorphisrnfrom `4 to B; that is, a map h : Q.4 - -  Qs such that (forall existing transitions)1. h(6.~(q, l)) = 6B(h(q), l);2. r(h(q)) = r(q) for all q such that r(q) E A;3. h(qoa) = qo~.It can be shown that subsurnption is a partial order onisomorphism classes of automata (without orderings), andthat for any formula 4} without recursion or ordering, thatthere are a finite number of subsumption-minimal au-tomata satisfying it.
We Consider as candidate structuresfor the language defined by a formula, only automatawhich are minimal in this sense.
The reason we do thisis to exclude f-structures which contain terminal symbolsnot mentioned in a formula.
For example, the formulaNIL  is satisfied by any f-structure, but only the mini-mal one, the one-node automaton, should be the principalstructure defined by this formula.By compatibility we mean the following.
In an f-structure `4, restrict the ordering < to the terminal sym-bois of,4.
This ordering need not be total; it may in factbe empty.
If there is an extension of this partial order onthe terminal nodes to a total order such that the labeling93symbols agree with the symbols labeling the positions ofw, then w is compatible with A.This is our new way of dealing with free word order.Suppose that no precedence relations are specified in aformula.
Then, minimal satisfying f-structures will havean empty < relation.
This implies that any permutationof the terminal symbols in such a structure will be al-lowed.
Many other ways of defining word order can alsobe expressed in this Logic, which enjoys an advantage overID/LP rules in this respect.4 Modeling Relational GrammarConsider the relational analyses in Figures 4 and 5.These analyses, taken from \[7\], have much in commonwith functional analyses and also with transsformationalones.
The present pair of networks illustrates a kind ofraising construction common in the relational literature.In Figure 4, there are arc labels P, I, and 2, representing"predicate", "subject", and "object" relations.
The "cl"indicates that this analysis is at the first linguistic stra-tum, roughly like a transformational cycle.
In Figure 5,we learn that at the second stratum, the predicate ("be-lieved") is the same as at stratum i, as is the subject.However, the object at level 2 is now "John", and thephrase "John killed the farmer" has become a "chSmeur"for level 2.The relational network is almost itself a feature struc-ture.
To make it one, we employ the trick of introducingan arc labeled with l, standing for "previous level".
Theconditions relating the two levels can easily be stated aspath equations, as in Figure 6.The dotted lines in Figure 6 indicate that the nodesthey connect are actually identical.
We can now indicateprecisely other information which might be specified ina relational grammar, such as the ordering informationI < P < 2.
This would apply to the "top level", whichfor Perlmutter and Postal would be the "final level", orsurface level.
A recursive specification would also becomepossible: thusSENT ::= CLAUSEA( I<P<2)CLAUSE ::= I :NOMAP:VERBA 2 : (CLAUSE V NOM)A (RA ISE  V PASS IVE  V .
.
.
)A I : CLAUSEl : 2 : CLAUSE A (equations in (6)) RAISE  ::=This is obviously an incomplete grammar, but we thinkit possible to use this notation to give a complete specifi-cation of an RG and, perhaps at some stage, a computa-tional test.5 UndecidabilityIn this section we show that the problem of sa(is/ia-bility - given a formula, decide if there is an f-structuresatisfying it - is undecidable.
We do this by building a for-mula which describes the computations of a given Turingmachine.
In fact, we show how to speak about the com-putations of an automaton with one stack (a pushdownautomaton.)
This is done for convenience; although thehalting problem for one-stack automata is decidable, itwill be clear from the construction that the computationof a two-stack machine could be simulated as well.
Thismodel is equivalent to a Turing machine - one stack rep-resents the tape contents to the left of the TM head, andthe other, the tape contents to the right.
We need notsimulate moves which read input, because we imagine theTM started with blank tape.
The halting problem forsuch machines is still undecidable.We make the following conventions about our PDA.Moves are of two kinds:?
q i  : push  b; go  to  q j  ;?
q i  : pop  s tack ;  i f  a go  to  q j  e l se  go  to  qk.The machine has a two-character stack alphabet {a, b}.
(In the push instruction, of course pushing "a" is allowed.
)If the machine attempts to pop an empty stack, it can-not continue.
There is one final state qf.
The machinehalts sucessfully in this and only this state.
We reducethe halting problem for this machine to the satisfiabilityproblem for our logic.Atoms: "none ..... bookkeeping markerfor telling whatis in the stackqO, q l  .
.
.
.
.
qn - - -  one  fo reach  s ta teLabels: a, b --- for describingstack contentss -- pointer to top of stacknext --- value of next statep --- pointer to previousstack configurationType var iab les :CONF -- structure representsa machine configurationINIT0 FINAL --confi~trationsat start and finishQO ..... QN: property of beingin one  o f  these  s ta tesThe simulation proceeds as in the relational grammarexample.
Each configuration of the stack corresponds toa level in an RG derivation.
Initially, the stack is empty.Thus we put94Figure 4: Network for The woman believed that John killed the farmer.b ~  p c a.fFigure 5: Network for The woman believed John to have killed the farmer.p = lp1 = l l2 = 121Chop = 12PCho 2 " 1 2 2Figure 6: Representing Figure 5 as an f-structure.95INIT ::= s : (b : none A a : none) A nerl; : q0.Then we describe standard configurations:C0//F ::= ISIT V (p : CONF A (QO V. .
.
V QN)).Next, we show how configurations are updated, de-pending on the move rules.
If q?
is push b; go to qj, thenwe writeQI : :=nex~:q jAp:next :q iAs :a :noneAsb=ps .The last clause tells us that the current stack contents,after finding a %"  on top, is the same as the previouscontents.
The %: none" clause guarantees that only a%"  is found on the DG representing the stack.
The sec-ond clause enforces a consistent state transition from theprevious configuration, and the first clause says what thenext state should be.If q?
ispop stack; if a go to qj else go to qk,then we write the following.QI ::= p : nex~ : qiA ( ( s=psaAnex~: :q jAp :s :b :none)V(s=psbAnext :qkAp:s :a :none) )For the last configuration, we putI~F ::---- C011F A p : nex~ : qf.We take QF as the "distinguished predicate" of ourscheme.It should be clear that this formula, which is a bigwhere-formula, is satisfiable if\[" the machine reaches stateqf.6 Conc lus ionIt would be desirable to use the notation providedby our logic to state substantive principles of particu-lax linguistic theories.
Consider, for example, Kashket'sparser for Warlpiri \[4\], which is based on GB theory.
Forlanguages like Warlpiri, we might be able to say thatlinear order is only explicitly represented at the mor-phemic level, and not at the phrase level.
This wouldtranslate into a constraint on the kinds of logical for-mulas we could use to describe such languages: the <relation could only be used as a relation between nodesof the MORPHEME type.
Given such a condition onformulas, it migh t then be possible to prove complexityresults which were more positive than a general undecid-ability theorem.
Similar remarks hold for theories likerelational grammar, in which many such constraints havebeen studied.
We hope that logical tools will provide away to classify these empirically motivated conditions.References\[1\] Joshi, A. , K. Vijay-Shanker, and D. Weir, The Con-vergence of Mildly Context-Sensitive Grammar For-malisms.
To appear in T. Wasow and P. Sells, ed.
"The Processing of Linguistic Structure", MIT Press.\[2\] Kaplan, R. and J. Bresnan, LFG: a Formal Sys-tem for Grammatical Representation, in Bresnan,ed.
The Mental Representation of Grammatical Re-lations, MIT Press, Cambridge, 1982, 173-281.\[3\] Kasper, R. and W. Rounds, A Logical Semantics forFeature Structures, Proceedings of e4th A CL AnnualMeeting, June 1986.\[4\] Kashket, M. Parsing a free word order language:Warlpiri.
Proc.
24th Ann.
Meeting of ACL, 1986,60-66.\[5\] Kay, M. Functional Grammar.
In Proceedings of theFifth Annual Meeting of the Berkeley Linguistics So-ciety, Berkeley Linguistics Society, Berkeley, Califor-nia, February 17-19, 1979.\[6\] Pereira, F.C.N., and D. Warren, Definite Clause Gram-mars for Language Analysis: A Survey of the Formal-ism and a Comparison with Augmented TransitionNetworks, Artificial Intelligence 13, (1980), 231-278.\[7\] Perlmutter, D. M., Relational Grammar, in Syntaxand Semantics, voi.
18: Current Approaches to Syn-taz, Academic Press, 1980.\[8\] Rounds, W. C. and R. Kasper.
A Complete Logi-cal Calculus for Record Structures Representing Lin-guistic Information.
IEEE Symposium on Logic inComputer Science, June, 1986.\[9\] Rounds, W., LFP: A Formalism for Linguistic De-scriptions and an Analysis of its Complexity, Com-putational Linguistics, to appear.96
