Proceedings of NAACL HLT 2009: Short Papers, pages 253?256,Boulder, Colorado, June 2009. c?2009 Association for Computational LinguisticsCombining Constituent ParsersVictoria FossumDept.
of Computer ScienceUniversity of MichiganAnn Arbor, MI 48104vfossum@umich.eduKevin KnightInformation Sciences InstituteUniversity of Southern CaliforniaMarina del Rey, CA 90292knight@isi.eduAbstractCombining the 1-best output of multipleparsers via parse selection or parse hybridiza-tion improves f-score over the best indi-vidual parser (Henderson and Brill, 1999;Sagae and Lavie, 2006).
We propose threeways to improve upon existing methods forparser combination.
First, we propose amethod of parse hybridization that recom-bines context-free productions instead of con-stituents, thereby preserving the structure ofthe output of the individual parsers to a greaterextent.
Second, we propose an efficient linear-time algorithm for computing expected f-scoreusing Minimum Bayes Risk parse selection.Third, we extend these parser combinationmethods from multiple 1-best outputs to mul-tiple n-best outputs.
We present results onWSJ section 23 and also on the English sideof a Chinese-English parallel corpus.1 IntroductionParse quality impacts the quality of downstream ap-plications such as syntax-based machine translation(Quirk and Corston-Oliver, 2006).
Combining theoutput of multiple parsers can boost the accuracyof such applications.
Parses can be combined intwo ways: parse selection (selecting the best parsefrom the output of the individual parsers) or parsehybridization (constructing the best parse by recom-bining sub-sentential components from the output ofthe individual parsers).1.1 Related Work(Henderson and Brill, 1999) perform parse selec-tion by maximizing the expected precision of theselected parse with respect to the set of parses be-ing combined.
(Henderson and Brill, 1999) and(Sagae and Lavie, 2006) propose methods for parsehybridization by recombining constituents.1.2 Our WorkIn this work, we propose three ways to improve uponexisting methods for parser combination.First, while constituent recombination (Hender-son and Brill, 1999; Sagae and Lavie, 2006) gives asignificant improvement in f-score, it tends to flattenthe structure of the individual parses.
To illustrate,Figures 1 and 2 contrast the output of the Charniakparser with the output of constituent recombinationon a sentence from WSJ section 24.
We recombinecontext-free productions instead of constituents, pro-ducing trees containing only context-free produc-tions that have been seen in the individual parsers?output (Figure 3).Second, the parse selection method of (Hender-son and Brill, 1999) selects the parse with maxi-mum expected precision; here, we present an effi-cient, linear-time algorithm for selecting the parsewith maximum expected f-score within the Mini-mum Bayes Risk (MBR) framework.Third, we extend these parser combination meth-ods from 1-best outputs to n-best outputs.
Wepresent results on WSJ section 23 and also on theEnglish side of a Chinese-English parallel corpus.253SBARINalthoughFRAGRBnotADJPRBasRBsharplyPPINasNPNPDTtheNNgainVPVBNreportedNPNNPFridayFigure 1: Output of Charniak ParserSBARINalthoughRBnotRBasRBsharplyINasNPDTtheNNgainVPVBNreportedNPNNPFridayFigure 2: Output of Constituent Recombination2 Parse SelectionIn the MBR framework, although the true referenceparse is unknown, we assume that the individualparsers?
output forms a reasonable distribution overpossible reference parses.
We compute the expectedf-score of each parse tree pi using this distribution:expected f(pi) =?pjf(pi, pj) ?
pr(pj)where f(pi, pj) is the f-score of parse pi withrespect to parse pj and pr(pj) is the prior prob-ability of parse pj .
We estimate pr(pj) as fol-lows: pr(pj) = pr(parserk) ?
pr(pj |parserk),where parserk is the parser generating pj .
Weset pr(parserk) according to the proportion of sen-tences in the development set for which the 1-bestoutput of parserk achieves the highest f-score ofany individual parser, breaking ties randomly.When n = 1, pr(pj |parserk) = 1 for all pj ;when n > 1 we must estimate pr(pj |parserk), thedistribution over parses in the n-best list output byany given parser.
We estimate this distribution us-ing the model score, or log probability, given byparserk to each entry pj in its n-best list:pr(pj |parserk) = e?
?scorej,k?nj?=1 e?
?scorej?,kSBARINalthoughSADVPADVPRBnotRBasADVPRBsharplyPPINasNPDTtheNNgainVPVBNreportedNPNNPFridayFigure 3: Output of Context-Free Production Recombi-nationParser wsj cedev test dev testBerkeley 88.6 89.3 82.9 83.5(Petrov and Klein, 2007)Bikel?Collins Model 2 87.0 88.2 81.2 80.6(Bikel, 2002)Charniak 90.6 91.4 84.7 84.1(Charniak and Johnson, 2005)Soricut?Collins Model 2 87.3 88.4 82.3 82.1(Soricut, 2004)Stanford 85.4 86.4 81.3 80.1(Klein and Manning, 2003)Table 1: F-Scores of 1-best Output of Individual ParsersWe tune ?
on a development set to maximize f-score,1 and select the parse pi with highest expectedf-score.Computing exact expected f-score requiresO(m2) operations per sentence, where m is thenumber of parses being combined.
We can computean approximate expected f-score in O(m) time.
Todo so, we compute expected precision for all parsesin O(m) time by associating with each uniqueconstituent ci a list of parses in which it occurs,plus the total probability qi of those parses.
Foreach parse p associated with ci, we increment theexpected precision of that parse by qi/size(p).
Thiscomputation yields the same result as the O(m2)algorithm.
We carry out a similar operation forexpected recall.
We then compute the harmonicmean of expected precision and expected recall,which closely approximates the true expectedf-score.1A low value of ?
creates a uniform distribution, while ahigh value concentrates probability mass on the 1-best entry inthe n-best list.
In practice, tuning ?
produces a higher f-scorethan setting ?
to the value that exactly reproduces the individualparser?s probability distribution.254Parse Selection: Minimum Bayes RiskSystem wsj-dev wsj-test ce-dev ce-testP R F P R F P R F P R Fbest individual 91.3 89.9 90.6 91.8 91.0 91.4 86.1 83.4 84.7 85.6 82.6 84.1parsern=1 91.7 90.5 91.1 92.5 91.8 92.0 87.1 84.6 85.8 86.7 83.7 85.2n=10 92.1 90.8 91.5 92.4 91.7 92.0 87.9 85.3 86.6 87.7 84.4 86.0n=25 92.1 90.9 91.5 92.4 91.7 92.0 88.0 85.4 86.7 87.4 84.2 85.7n=50 92.1 91.0 91.5 92.4 91.7 92.1 88.0 85.3 86.6 87.6 84.3 85.9Table 2: Precision, Recall, and F-score Results from Parse Selection3 Constituent Recombination(Henderson and Brill, 1999) convert each parse intoconstituents with syntactic labels and spans, andweight each constituent by summing pr(parserk)over all parsers k in whose output the constituentappears.
They include all constituents with weightabove a threshold t = m+12 , where m is the numberof input parses, in the combined parse.
(Sagae and Lavie, 2006) extend this method bytuning t on a development set to maximize f-score.2 They populate a chart with constituentswhose weight meets the threshold, and use a CKY-style parsing algorithm to find the heaviest tree,where the weight of a tree is the sum of its con-stituents?
weights.
Parsing is not constrained by agrammar; any context-free production is permitted.Thus, the combined parses may contain context-freeproductions not seen in the individual parsers?
out-puts.
While this failure to preserve the structure ofindividual parses does not affect f-score, it may hin-der downstream applications.To extend this method from 1-best to n-bestlists, we weight each constituent by summingpr(parserk)?pr(pj |parserk) over all parses pj gen-erated by parserk in which the constituent appears.4 Context-Free Production RecombinationTo ensure that all context-free productions in thecombined parses have been seen in the individualparsers?
outputs, we recombine context-free produc-tions rather than constituents.
We convert each parseinto context-free productions, labelling each con-stituent in the production with its span and syntac-tic category and weighting each production by sum-2A high threshold results in high precision, while a lowthreshold results in high recall.ming pr(parserk) ?
pr(pj |parserk) over all parsespj generated by parserk in which the production ap-pears.
We re-parse the sentence with these produc-tions, returning the heaviest tree (where the weightof a tree is the sum of its context-free productions?weights).
We optimize f-score by varying the trade-off between precision and recall using a derivationlength penalty, which we tune on a developmentset.35 ExperimentsTable 1 illustrates the 5 parsers used in our combi-nation experiments and the f-scores of their 1-bestoutput on our data sets.
We use the n-best outputof the Berkeley, Charniak, and Soricut parsers, andthe 1-best output of the Bikel and Stanford parsers.All parsers were trained on the standard WSJ train-ing sections.
We use two corpora: the WSJ (sec-tions 24 and 23 are the development and test sets, re-spectively) and English text from the LDC2007T02Chinese-English parallel corpus (the developmentand test sets contain 400 sentences each).6 Discussion & ConclusionResults are shown in Tables 2, 3, and 4.
On bothtest sets, constituent recombination achieves the bestf-score (1.0 points on WSJ test and 2.3 points onChinese-English test), followed by context-free pro-duction combination, then parse selection, thoughthe differences in f-score among the combinationmethods are not statistically significant.
Increasingthe n-best list size from 1 to 10 improves parse se-lection and context-free production recombination,3By subtracting higher(lower) values of this length penaltyfrom the weight of each production, we can encourage the com-bination method to favor trees with shorter(longer) derivationsand therefore higher precision(recall) at the constituent level.255Parse Hybridization: Constituent RecombinationSystem wsj-dev wsj-test ce-dev ce-testP R F P R F P R F P R Fbest individual 91.3 89.9 90.6 91.8 91.0 91.4 86.1 83.4 84.7 85.6 82.6 84.1parsern=1 92.5 90.3 91.4 93.0 91.6 92.3 89.2 84.6 86.8 89.1 83.6 86.2n=10 92.6 90.5 91.5 93.1 91.7 92.4 89.9 84.4 87.1 89.9 83.2 86.4n=25 92.6 90.5 91.5 93.2 91.7 92.4 89.9 84.4 87.0 89.7 83.4 86.4n=50 92.6 90.5 91.5 93.1 91.7 92.4 89.9 84.4 87.1 89.7 83.2 86.3Table 3: Precision, Recall, and F-score Results from Constituent RecombinationParse Hybridization: Context-Free Production RecombinationSystem wsj-dev wsj-test ce-dev ce-testP R F P R F P R F P R Fbest individual 91.3 89.9 90.6 91.8 91.0 91.4 86.1 83.4 84.7 85.6 82.6 84.1parsern=1 91.7 91.0 91.4 92.1 91.9 92.0 86.9 85.4 86.2 86.2 84.3 85.2n=10 92.1 90.9 91.5 92.5 91.8 92.2 87.8 85.1 86.4 86.2 84.3 86.1n=25 92.2 91.0 91.6 92.5 91.8 92.2 87.8 85.1 86.4 87.6 84.6 86.1n=50 92.1 90.8 91.4 92.4 91.7 92.1 87.6 84.9 86.2 87.7 84.6 86.1Table 4: Precision, Recall, and F-score Results from Context-Free Production Recombinationthough further increasing n does not, in general,help.4 Chinese-English test set f-score gets a biggerboost from combination than WSJ test set f-score,perhaps because the best individual parser?s baselinef-score is lower on the out-of-domain data.We have presented an algorithm for parse hy-bridization by recombining context-free produc-tions.
While constituent recombination results inthe highest f-score of the methods explored, context-free production recombination produces trees whichbetter preserve the syntactic structure of the indi-vidual parses.
We have also presented an efficientlinear-time algorithm for selecting the parse withmaximum expected f-score.AcknowledgmentsWe thank Steven Abney, John Henderson, andKenji Sagae for helpful discussions.
This researchwas supported by DARPA (contract HR0011-06-C-0022) and by NSF ITR (grant IIS-0428020).4These diminishing gains in f-score as n increases reflectthe diminishing gains in f-score of the oracle parse produced byeach individual parser as n increases.ReferencesDaniel M. Bikel.
2004.
Design of a Multi-lingual,Parallel-processing Statistical Parsing Engine.
In Pro-ceedings of HLT.Eugene Charniak and Mark Johnson.
2005.
Coarse-to-fine n-best parsing and MaxEnt discriminative rerank-ing.
In Proceedings of ACL.Michael Collins and Terry Koo.
2005.
DiscriminativeReranking for Natural Language Parsing.
Computa-tional Linguistics, 31(1):25-70.John C. Henderson and Eric Brill.
2000.
Exploiting Di-versity in Natural Language Processing: CombiningParsers.
In Proceedings of EMNLP.Dan Klein and Christopher D. Manning.
2003.
AccurateUnlexicalized Parsing.
In Proceedings of ACL.Slav Petrov and Dan Klein.
2007.
Improved Inferencefor Unlexicalized Parsing.
In Proceedings of HLT-NAACL.Chris Quirk and Simon Corston-Oliver.
2006.
The Im-pact of Parse Quality on Syntactically-Informed Statis-tical Machine Translation.
In Proceedings of EMNLP.Kenji Sagae and Alon Lavie.
2006.
Parser Combinationby Reparsing.
In Proceedings of HLT-NAACL.Radu Soricut.
2004.
A Reimplementation of Collins?Parsing Models.
Technical report, Information Sci-ences Institute, Department of Computer Science, Uni-versity of Southern California.256
