Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1536?1545,Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational LinguisticsAutomatic Labelling of Topic ModelsJey Han Lau,??
Karl Grieser,?
David Newman,??
and Timothy Baldwin???
NICTA Victoria Research Laboratory?
Dept of Computer Science and Software Engineering, University of Melbourne?
Dept of Computer Science, University of California Irvinejhlau@csse.unimelb.edu.au, kgrieser@csse.unimelb.edu.au, newman@uci.edu, tb@ldwin.netAbstractWe propose a method for automatically la-belling topics learned via LDA topic models.We generate our label candidate set from thetop-ranking topic terms, titles of Wikipedia ar-ticles containing the top-ranking topic terms,and sub-phrases extracted from the Wikipediaarticle titles.
We rank the label candidates us-ing a combination of association measures andlexical features, optionally fed into a super-vised ranking model.
Our method is shown toperform strongly over four independent sets oftopics, significantly better than a benchmarkmethod.1 IntroductionTopic modelling is an increasingly popular frame-work for simultaneously soft-clustering terms anddocuments into a fixed number of ?topics?, whichtake the form of a multinomial distribution overterms in the document collection (Blei et al,2003).
It has been demonstrated to be highly ef-fective in a wide range of tasks, including multi-document summarisation (Haghighi and Vander-wende, 2009), word sense discrimination (Brodyand Lapata, 2009), sentiment analysis (Titov andMcDonald, 2008), information retrieval (Wei andCroft, 2006) and image labelling (Feng and Lapata,2010).One standard way of interpreting a topic is to usethe marginal probabilities p(wi|tj) associated witheach term wi in a given topic tj to extract out the 10terms with highest marginal probability.
This resultsin term lists such as:1stock market investor fund trading invest-ment firm exchange companies share1Here and throughout the paper, we will represent a topic tjvia its ranking of top-10 topic terms, based on p(wi|tj).which are clearly associated with the domain ofstock market trading.
The aim of this research is toautomatically generate topic labels which explicitlyidentify the semantics of the topic, i.e.
which take usfrom a list of terms requiring interpretation to a sin-gle label, such as STOCK MARKET TRADING in theabove case.The approach proposed in this paper is to firstgenerate a topic label candidate set by: (1) sourc-ing topic label candidates from Wikipedia by query-ing with the top-N topic terms; (2) identifying thetop-ranked document titles; and (3) further post-processing the document titles to extract sub-strings.We translate each topic label into features extractedfrom Wikipedia, lexical association with the topicterms in Wikipedia documents, and also lexical fea-tures for the component terms.
This is used as thebasis of a support vector regression model, whichranks each topic label candidate.Our contributions in this work are: (1) the genera-tion of a novel evaluation framework and dataset fortopic label evaluation; (2) the proposal of a methodfor both generating and scoring topic label candi-dates; and (3) strong in- and cross-domain resultsacross four independent document collections andassociated topic models, demonstrating the abilityof our method to automatically label topics with re-markable success.2 Related WorkTopics are conventionally interpreted via their top-N terms, ranked based on the marginal probabilityp(wi|tj) in that topic (Blei et al, 2003; Griffiths andSteyvers, 2004).
This entails a significant cognitiveload in interpretation, prone to subjectivity.
Topicsare also sometimes presented with manual post-hoclabelling for ease of interpretation in research pub-lications (Wang and McCallum, 2006; Mei et al,15362006).
This has obvious disadvantages in terms ofsubjectivity, and lack of reproducibility/automation.The closest work to our method is that of Mei etal.
(2007), who proposed various unsupervised ap-proaches for automatically labelling topics, basedon: (1) generating label candidates by extracting ei-ther bigrams or noun chunks from the document col-lection; and (2) ranking the label candidates basedon KL divergence with a given topic.
Their proposedmethodology generates a generic list of label can-didates for all topics using only the document col-lection.
The best method uses bigrams exclusively,in the form of the top-1000 bigrams based on theStudent?s t-test.
We reimplement their method andpresent an empirical comparison in Section 5.3.In other work, Magatti et al (2009) proposed amethod for labelling topics induced by a hierarchi-cal topic model.
Their label candidate set is theGoogle Directory (gDir) hierarchy, and label selec-tion takes the form of ontological alignment withgDir.
The experiments presented in the paper arehighly preliminary, although the results certainlyshow promise.
However, the method is only applica-ble to a hierarchical topic model and crucially relieson a pre-existing ontology and the class labels con-tained therein.Pantel and Ravichandran (2004) addressed themore specific task of labelling a semantic classby applying Hearst-style lexico-semantic patternsto each member of that class.
When presentedwith semantically homogeneous, fine-grained near-synonym clusters, the method appears to work well.With topic modelling, however, the top-rankingtopic terms tended to be associated and not lexicallysimilar to one another.
It is thus highly questionablewhether their method could be applied to topic mod-els, but it would certainly be interesting to investi-gate whether our model could conversely be appliedto the labelling of sets of near-synonyms.In recent work, Lau et al (2010) proposed to ap-proach topic labelling via best term selection, i.e.selecting one of the top-10 topic terms to label theoverall topic.
While it is often possible to label top-ics with topic terms (as is the case with the stockmarket topic above), there are also often cases wheretopic terms are not appropriate as labels.
We reusea selection of the features proposed by Lau et al(2010), and return to discuss it in detail in Section 3.While not directly related to topic labelling,Chang et al (2009) were one of the first to proposehuman labelling of topic models, in the form of syn-thetic intruder word and topic detection tasks.
In theintruder word task, they include a term w with lowmarginal probability p(w|t) for topic t into the top-N topic terms, and evaluate how well both humansand their model are able to detect the intruder.The potential applications for automatic labellingof topics are many and varied.
In document col-lection visualisation, e.g., the topic model can beused as the basis for generating a two-dimensionalrepresentation of the document collection (Newmanet al, 2010a).
Regions where documents have ahigh marginal probability p(di|tj) of being associ-ated with a given topic can be explicitly labelledwith the learned label, rather than just presentedas an unlabelled region, or presented with a dense?term cloud?
from the original topic.
In topic model-based selectional preference learning (Ritter et al,2010; O` Se?aghdha, 2010), the learned topics canbe translated into semantic class labels (e.g.
DAYSOF THE WEEK), and argument positions for individ-ual predicates can be annotated with those labels forgreater interpretability/portability.
In dynamic topicmodels tracking the diachronic evolution of topicsin time-sequenced document collections (Blei andLafferty, 2006), labels can greatly enhance the inter-pretation of what topics are ?trending?
at any givenpoint in time.3 MethodologyThe task of automatic labelling of topics is a naturalprogression from the best topic term selection taskof Lau et al (2010).
In that work, the authors usea reranking framework to produce a ranking of thetop-10 topic terms based on how well each term ?
inisolation ?
represents a topic.
For example, in ourstock market investor fund trading ... topic example,the term trading could be considered as a more rep-resentative term of the overall semantics of the topicthan the top-ranked topic term stock.While the best term could be used as a topic la-bel, topics are commonly ideas or concepts that arebetter expressed with multiword terms (for exampleSTOCK MARKET TRADING), or terms that might notbe in the top-10 topic terms (for example, COLOURS1537would be a good label for a topic of the form redgreen blue cyan ...).In this paper, we propose a novel method for au-tomatic topic labelling that first generates topic labelcandidates using English Wikipedia, and then ranksthe candidates to select the best topic labels.3.1 Candidate GenerationGiven the size and diversity of English Wikipedia,we posit that the vast majority of (coherent) topicsor concepts are encapsulated in a Wikipedia article.By making this assumption, the difficult task of gen-erating potential topic labels is transposed to find-ing relevant Wikipedia articles, and using the title ofeach article as a topic label candidate.We first use the top-10 topic terms (based on themarginal probabilities from the original topic model)to query Wikipedia, using: (a) Wikipedia?s nativesearch API; and (b) a site-restricted Google search.The combined set of top-8 article titles returnedfrom the two search engines for each topic consti-tutes the initial set of primary candidates.Next we chunk parse the primary candidates us-ing the OpenNLP chunker,2 and extract out all nounchunks.
For each noun chunk, we generate all com-ponent n-grams (including the full chunk), out ofwhich we remove all n-grams which are not in them-selves article titles in English Wikipedia.
For exam-ple, if the Wikipedia document title were the singlenoun chunk United States Constitution, we wouldgenerate the bigrams United States and States Con-stitution, and prune the latter; we would also gen-erate the unigrams United, States and Constitution,all of which exist as Wikipedia articles and are pre-served.In this way, an average of 30?40 secondary labelsare produced for each topic based on noun chunk n-grams.
A good portion of these labels are commonlystopwords or unigrams that are only marginally re-lated to the topic (an artifact of the n-gram gener-ation process).
To remove these outlier labels, weuse the RACO lexical association method of Grieseret al (2011).RACO (Related Article Conceptual Overlap) usesWikipedia?s link structure and category membershipto identify the strength of relationship between arti-2http://opennlp.sourceforge.net/cles via their category overlap.
The set of categoriesrelated to an article is defined as the union of the cat-egory membership of all outlinks in that article.
Thecategory overlap of two articles (a and b) is the in-tersection of the related category sets of each article.The formal definition of this measure is as follows:|(?p?O(a)C(p)) ?
(?p?O(b)C(p))|where O(a) is the set of outlinks from article a, andC(p) is the set of categories of which article p is amember.
This is then normalised using Dice?s co-efficient to generate a similarity measure.
In the in-stance that a term maps onto multiple Wikipedia ar-ticles via a disambiguation page, we return the bestRACO score across article pairings for a given termpair.
The final score for each secondary label can-didate is calculated as the average RACO score witheach of the primary label candidates.
All secondarylabels with an average RACO score of 0.1 and aboveare added to the label candidate set.Finally, we add the top-5 topic terms to the set ofcandidates, based on the marginals from the origi-nal topic model.
Doing this ensures that there arealways label candidates for all topics (even if theWikipedia searches fail), and also allows the pos-sibility of labeling a topic using its own topic terms,which was demonstrated by Lau et al (2010) to be abaseline source of topic label candidates.3.2 Candidate RankingAfter obtaining the set of topic label candidates, thenext step is to rank the candidates to find the best la-bel for each topic.
We will first describe the featuresthat we use to represent label candidates.3.2.1 FeaturesA good label should be strongly associated withthe topic terms.
To learn the association of a labelcandidate with the topic terms, we use several lexicalassociation measures: pointwise mutual information(PMI), Student?s t-test, Dice?s coefficient, Pearson?s?2 test, and the log likelihood ratio (Pecina, 2009).We also include conditional probability and reverseconditional probability measures, based on the workof Lau et al (2010).
To calculate the associationmeasures, we parse the full collection of EnglishWikipedia articles using a sliding window of width153820, and obtain term frequencies for the label candi-dates and topic terms.
To measure the associationbetween a label candidate and a list of topic terms,we average the scores of the top-10 topic terms.In addition to the association measures, we in-clude two lexical properties of the candidate: the rawnumber of terms, and the relative number of terms inthe label candidate that are top-10 topic terms.We also include a search engine score for eachlabel candidate, which we generate by querying alocal copy of English Wikipedia with the top-10topic terms, using the Zettair search engine (basedon BM25 term similarity).3 For a given label candi-date, we return the average score for the Wikipediaarticle(s) associated with it.3.2.2 Unsupervised and Supervised RankingEach of the proposed features can be used as thebasis for an unsupervised model for label candidateselection, by ranking the label candidates for a giventopic and selecting the top-N .
Alternatively, theycan be combined in a supervised model, by trainingover topics where we have gold-standard labellingof the label candidates.
For the supervised method,we use a support vector regression (SVR) model(Joachims, 2006) over all of the features.4 DatasetsWe conducted topic labelling experiments usingdocument collections constructed from four distinctdomains/genres, to test the domain/genre indepen-dence of our method:BLOGS : 120,000 blog articles dated from Augustto October 2008 from the Spinn3r blog dataset4BOOKS : 1,000 English language books from theInternet Archive American Libraries collectionNEWS : 29,000 New York Times news articlesdated from July to September 1999, from theEnglish Gigaword corpusPUBMED : 77,000 PubMed biomedical abstractspublished in June 20103http://www.seg.rmit.edu.au/zettair/4http://www.icwsm.org/data/The BLOGS dataset contains blog posts that covera diverse range of subjects, from product reviewsto casual, conversational messages.
The BOOKStopics, coming from public-domain out-of-copyrightbooks (with publication dates spanning more thana century), relate to a wide range of topics includ-ing furniture, home decoration, religion and art,and have a more historic feel to them.
The NEWStopics reflect the types and range of subjects onemight expect in news articles such as health, finance,entertainment, and politics.
The PUBMED topicsfrequently contain domain-specific terms and aresharply differentiated from the topics for the othercorpora.
We are particularly interested in the perfor-mance of the method over PUBMED, as it is a highlyspecialised domain where we may expect lower cov-erage of appropriate topic labels within Wikipedia.We took a standard approach to topic modellingeach of the four document collections: we tokenised,lemmatised and stopped each document,5 and cre-ated a vocabulary of terms that occurred at leastten times.
From this processed data, we created abag-of-words representation of each document, andlearned topic models with T = 100 topics in eachcase.To focus our experiments on topics that were rela-tively more coherent and interpretable, we first usedthe method of Newman et al (2010b) to calculatethe average PMI-score for each topic, and filteredall topics that had an average PMI-score lower than0.4.
We additionally filtered any topics where lessthan 5 of the top-10 topic terms are default nomi-nal in Wikipedia.6 The filtering criteria resulted in45 topics for BLOGS, 38 topics for BOOKS, 60 top-ics for NEWS, and 85 topics for PUBMED.
Man-ual inspection of the discarded topics indicated thatthey were predominantly hard-to-label junk topics ormixed topics, with limited utility for document/termclustering.Applying our label candidate generation method-ology to these 228 topics produced approximately6000 labels ?
an average of 27 labels per topic.5OpenNLP is used for tokenization, Morpha for lemmatiza-tion (Minnen et al, 2001).6As determined by POS tagging English Wikipedia withOpenNLP, and calculating the coarse-grained POS priors (noun,verb, etc.)
for each term.1539Figure 1: A screenshot of the topic label evaluation task on Amazon Mechanical Turk.
This screen constitutes aHuman Intelligence Task (HIT); it contains a topic followed by 10 suggested topic labels, which are to be rated.
Notethat been would be the stopword label in this example.4.1 Topic Candidate LabellingTo evaluate our methods and train the supervisedmethod, we require gold-standard ratings for the la-bel candidates.
To this end, we used Amazon Me-chanical Turk to collect annotations for our labels.In our annotation task, each topic was presentedin the form of its top-10 terms, followed by 10 sug-gested labels for the topic.
This constitutes a HumanIntelligence Task (HIT); annotators are paid basedon the number of HITs they have completed.
Ascreenshot of a HIT seen by annotator is presentedin Figure 1.In each HIT, annotators were asked to rate the la-bels based on the following ordinal scale:3: Very good label; a perfect description of thetopic.2: Reasonable label, but does not completely cap-ture the topic.1: Label is semantically related to the topic, butwould not make a good topic label.0: Label is completely inappropriate, and unrelatedto the topic.To filter annotations from workers who did notperform the task properly or from spammers, we ap-1540Domain Topic Terms Label Candidate AverageRatingBLOGS china chinese olympics gold olympic team win beijing medal sport 2008 summer olympics 2.60BOOKS church arch wall building window gothic nave side vault tower gothic architecture 2.40NEWS israel peace barak israeli minister palestinian agreement prime leader palestinians israeli-palestinian conflict 2.63PUBMED cell response immune lymphocyte antigen cytokine t-cell induce receptor immunity immune system 2.36Table 1: A sample of topics and topic labels, along with the average rating for each label candidateplied a few heuristics to automatically detect theseworkers.
Additionally, we inserted a small num-ber of stopwords as label candidates in each HITand recorded workers who gave high ratings to thesestopwords.
Annotations from workers who failed topassed these tests are removed from the final set ofgold ratings.Each label candidate was rated in this way by atleast 10 annotators, and ratings from annotators whopassed the filter were combined by averaging them.A sample of topics, label candidates, and the averagerating is presented in Table 1.7Finally, we train the regression model over allthe described features, using the human rating-basedranking.5 ExperimentsIn this section we present our experimental resultsfor the topic labelling task, based on both the unsu-pervised and supervised methods, and the methodol-ogy of Mei et al (2007), which we denote MSZ forthe remainder of the paper.5.1 EvaluationWe use two basic measures to evaluate the perfor-mance of our predictions.
Top-1 average rating isthe average annotator rating given to the top-rankedsystem label, and has a maximum value of 3 (whereannotators unanimously rated all top-ranked systemlabels with a 3).
This is intended to give a sense ofthe absolute utility of the top-ranked candidates.The second measure is normalized discountedcumulative gain (nDCG: Jarvelin and Kekalainen(2002), Croft et al (2009)), computed for the top-1(nDCG-1), top-3 (nDCG-3) and top-5 ranked sys-tem labels (nDCG-5).
For a given ordered list of7The dataset is available for download fromhttp://www.csse.unimelb.edu.au/research/lt/resources/acl2011-topic/.scores, this measure is based on the difference be-tween the original order, and the order when the listis sorted by score.
That is, if items are ranked op-timally in descending order of score at position N ,nDCG-N is equal to 1. nDCG is a normalised score,and indicates how close the candidate label rankingis to the optimal ranking within the set of annotatedcandidates, noting that an nDCG-N score of 1 tellsus nothing about absolute values of the candidates.This second evaluation measure is thus intended toreflect the relative quality of the ranking, and com-plements the top-1 average rating.Note that conventional precision- and recall-basedevaluation is not appropriate for our task, as eachlabel candidate has a real-valued rating.As a baseline for the task, we use the unsuper-vised label candidate ranking method based on Pear-son?s ?2 test, as it was overwhelmingly found to bethe pick of the features for candidate ranking.5.2 Results for the Supervised MethodFor the supervised model, we present both in-domain results based on 10-fold cross-validation,and cross-domain results where we learn a modelfrom the ratings for the topic model from a givendomain, and apply it to a second domain.
In eachcase, we learn an SVR model over the full set of fea-tures described in Section 3.2.1.
In practical terms,in-domain results make the unreasonable assump-tion that we have labelled 90% of labels in orderto be able to label the remaining 10%, and cross-domain results are thus the more interesting datapoint in terms of the expected results when apply-ing our method to a novel topic model.
It is valuableto compare the two, however, to gauge the relativeimpact of domain on the results.We present the results for the supervised methodin Table 2, including the unsupervised baseline andan upper bound estimate for comparison purposes.The upper bound is calculated by ranking the candi-1541Test Domain Training Top-1 Average Rating nDCG-1 nDCG-3 nDCG-5All 1?
2?
Top5BLOGSBaseline (unsupervised) 1.84 1.87 1.75 1.74 0.75 0.77 0.79In-domain 1.98 1.94 1.95 1.77 0.81 0.82 0.83Cross-domain: BOOKS 1.88 1.92 1.90 1.77 0.77 0.81 0.83Cross-domain: NEWS 1.97 1.94 1.92 1.77 0.80 0.83 0.83Cross-domain: PUBMED 1.95 1.95 1.93 1.82 0.80 0.82 0.83Upper bound 2.45 2.26 2.29 2.18 1.00 1.00 1.00BOOKSBaseline (unsupervised) 1.75 1.76 1.70 1.72 0.77 0.77 0.79In-domain 1.91 1.90 1.83 1.74 0.84 0.81 0.83Cross-domain: BLOGS 1.82 1.88 1.79 1.71 0.79 0.81 0.82Cross-domain: NEWS 1.82 1.87 1.80 1.75 0.79 0.81 0.83Cross-domain: PUBMED 1.87 1.87 1.80 1.73 0.81 0.82 0.83Upper bound 2.29 2.17 2.15 2.04 1.00 1.00 1.00NEWSBaseline (unsupervised) 1.96 1.76 1.87 1.70 0.80 0.79 0.78In-domain 2.02 1.92 1.90 1.82 0.82 0.82 0.84Cross-domain: BLOGS 2.03 1.92 1.89 1.85 0.83 0.82 0.84Cross-domain: BOOKS 2.01 1.80 1.93 1.73 0.82 0.82 0.83Cross-domain: PUBMED 2.01 1.93 1.94 1.80 0.82 0.82 0.83Upper bound 2.45 2.31 2.33 2.12 1.00 1.00 1.00PUBMEDBaseline (unsupervised) 1.73 1.74 1.68 1.63 0.75 0.77 0.79In-domain 1.79 1.76 1.74 1.67 0.77 0.82 0.84Cross-domain: BLOGS 1.80 1.77 1.73 1.69 0.78 0.82 0.84Cross-domain: BOOKS 1.77 1.70 1.74 1.64 0.77 0.82 0.83Cross-domain: NEWS 1.79 1.76 1.73 1.65 0.77 0.82 0.84Upper bound 2.31 2.17 2.22 2.01 1.00 1.00 1.00Table 2: Supervised results for all domainsdates based on the annotated human ratings.
The up-per bound for top-1 average rating is thus the high-est average human rating of all label candidates fora given topic, while the upper bound for the nDCGmeasures will always be 1.In addition to results for the combined candidateset, we include results for each of the three candi-date subsets, namely the primary Wikipedia labels(?1??
), the secondary Wikipedia labels (?2??)
andthe top-5 topic terms (?Top5?
); the nDCG resultsare over the full candidate set only, as the numbersaren?t directly comparable over the different subsets(due to differences in the number of candidates andthe distribution of ratings).Comparing the in-domain and cross-domain re-sults, we observe that they are largely compara-ble, with the exception of BOOKS, where there isa noticeable drop in both top-1 average rating andnDGC-1 when we use cross-domain training.
Wesee an appreciable drop in scores when we trainBOOKS against BLOGS (or vice versa), which weanalyse as being due to incompatibility in documentcontent and structure between these two domains.Overall though, the results are very encouraging,and point to the plausibility of using labelled topicmodels from independent domains to learn the besttopic labels for a new domain.Returning to the question of the suitability of la-bel candidates for the highly specialised PUBMEDdocument collection, we first notice that the up-per bound top-1 average rating is comparable tothe other domains, indicating that our method hasbeen able to extract equivalent-quality label can-didates from Wikipedia.
The top-1 average rat-ings of the supervised method are lower than theother domains.
We hypothesise that the cause ofthe drop is that the lexical association measures aretrained over highly diverse Wikipedia data ratherthan biomedical-specific data, and predict that theresults would improve if we trained our features overPubMed.The results are uniformly better than the unsuper-vised baselines for all four corpora, although thereis quite a bit of room for improvement relative to theupper bound.
To better gauge the quality of theseresults, we carry out a direct comparison of our pro-posed method with the best-performing method ofMSZ in Section 5.3.1542Looking to the top-1 average score results over thedifferent candidate sets, we observe first that the up-per bound for the combined candidate set (?All?)
ishigher than the scores for the candidate subsets in allcases, underlining the complementarity of the differ-ent candidate sets.
We also observe that the top-5topic term candidate set is the lowest performer outof the three subsets across all four corpora, in termsof both upper bound and the results for the super-vised method.
This reinforces our comments aboutthe inferiority of the topic word selection method ofLau et al (2010) for topic labelling purposes.
ForNEWS and PUBMED, there is a noticeable differ-ence between the results of the supervised methodover the full candidate set and each of the candidatesubsets.
In contrast, for BOOKS and BLOGS, the re-sults for the primary candidate subset are at timesactually higher than those over the full candidate setin most cases (but not for the upper bound).
This isdue to the larger search space in the full candidateset, and the higher median quality of candidates inthe primary candidate set.5.3 Comparison with MSZThe best performing method out of the suite ofapproaches proposed by MSZ method exclusivelyuses bigrams extracted from the document collec-tion, ranked based on Student?s t-test.
The potentialdrawbacks to this approach are: all labels must bebigrams, there must be explicit token instances ofa given bigram in the document collection for it tobe considered as a label candidate, and furthermore,there must be enough token instances in the docu-ment collection for it to have a high t score.To better understand the performance differenceof our approach to that of MSZ, we perform directcomparison of our proposed method with the bench-mark method of MSZ.5.3.1 Candidate RankingFirst, we compare the candidate ranking method-ology of our method with that of MSZ, using thelabel candidates extracted by the MSZ method.We first extracted the top-2000 bigrams using theN -gram Statistics Package (Banerjee and Pedersen,2003).
We then ranked the bigrams for each topicusing the Student?s t-test.
We included the top-5 la-bels generated for each topic by the MSZ methodin our Mechanical Turk annotation task, and use theannotations to directly compare the two methods.To measure the performance of candidate rank-ing between our supervised method and MSZ?s, were-rank the top-5 labels extracted by MSZ usingour SVR methodology (in-domain) and compare thetop-1 average rating and nDCG scores.
Results areshown in Table 3.
We do not include results for theBOOKS domain because the text collection is muchlarger than the other domains, and the computationfor the MSZ relevance score ranking is intractabledue to the number of n-grams (a significant short-coming of the method).Looking at the results for the other domains, it isclear that our ranking system has the upper hand:it consistently outperforms MSZ over every evalu-ation metric.8 Comparing the top-1 average ratingresults back to those in Table 2, we observe thatfor all three domains, the results for MSZ are be-low those of the unsupervised baseline, and well be-low those of our supervised method.
The nDCG re-sults are more competitive, and the nDCG-3 resultsare actually higher than our original results in Ta-ble 2.
It is important to bear in mind, however, thatthe numbers are in each case relative to a different la-bel candidate set.
Additionally, the results in Table 3are based on only 5 candidates, with a relatively flatgold-standard rating distribution, making it easier toachieve higher nDCG-5 scores.5.3.2 Candidate GenerationThe method of MSZ makes the implicit assump-tion that good bigram labels are discoverable withinthe document collection.
In our method, on the otherhand, we (efficiently) access the much larger andvariable n-gram length set of English Wikipedia ar-ticle titles, in addition to the top-5 topic terms.
Tobetter understand the differences in label candidatesets, and the relative coverage of the full label can-didate set in each case, we conducted another surveywhere human users were asked to suggest one topiclabel for each topic presented.The survey consisted, once again, of presentingannotators with a topic, but in this case, we gavethem the open task of proposing the ideal label for8Based on a single ANOVA, the difference in results is sta-tistically significant at the 5% level for BLOGS, and 1% forNEWS and PUBMED.1543Test Domain Candidate Ranking Top-1 nDCG-1 nDCG-3 nDCG-5System Avg.
RatingBLOGSMSZ 1.26 0.65 0.76 0.87SVR 1.41 0.75 0.85 0.92Upper bound 1.87 1.00 1.00 1.00NEWSMSZ 1.37 0.73 0.81 0.90SVR 1.66 0.88 0.90 0.95Upper bound 1.86 1.00 1.00 1.00PUBMEDMSZ 1.53 0.77 0.85 0.93SVR 1.73 0.87 0.91 0.96Upper bound 1.98 1.00 1.00 1.00Table 3: Comparison of results for our proposed supervised ranking method (SVR) and that of MSZthe topic.
In this, we did not enforce any restrictionson the type or size of label (e.g.
the number of termsin the label).Of the manually-generated gold-standard labels,approximately 36% were contained in the originaldocument collection, but 60% were Wikipedia arti-cle titles.
This indicates that our method has greaterpotential to generate a label of the quality of the idealproposed by a human in a completely open-endedtask.6 DiscussionOn the subject of suitability of using Amazon Me-chanical Turk for natural language tasks, Snow et al(2008) demonstrated that the quality of annotationis comparable to that of expert annotators.
With thatsaid, the PUBMED topics are still a subject of inter-est, as these topics often contain biomedical termswhich could be difficult for the general populace toannotate.As the number of annotators per topic and thenumber of annotations per annotator vary, there isno immediate way to calculate the inter-annotatoragreement.
Instead, we calculated the MAE scorefor each candidate, which is an average of the ab-solute difference between an annotator?s rating andthe average rating of a candidate, summed across allcandidates to get the MAE score for a given corpus.The MAE scores for each corpus are shown in Ta-ble 4, noting that a smaller value indicates higheragreement.As the table shows, the agreement for thePUBMED domain is comparable with the otherdatasets.
BLOGS and NEWS have marginally betterCorpus MAEBLOGS 0.50BOOKS 0.56NEWS 0.52PUBMED 0.56Table 4: Average MAE score for label candidate ratingover each corpusagreement, almost certainly because of the greaterimmediacy of the topics, covering everyday areassuch as lifestyle and politics.
BOOKS topics are oc-casionally difficult to label due to the breadth of thedomain; e.g.
consider a topic containing terms ex-tracted from Shakespeare sonnets.7 ConclusionThis paper has presented the task of topic labelling,that is the generation and scoring of labels for agiven topic.
We generate a set of label candidatesfrom the top-ranking topic terms, titles of Wikipediaarticles containing the top-ranking topic terms, andalso a filtered set of sub-phrases extracted from theWikipedia article titles.
We rank the label candidatesusing a combination of association measures, lexicalfeatures and an Information Retrieval feature.
Ourmethod is shown to perform strongly over four inde-pendent sets of topics, and also significantly betterthan a competitor system.AcknowledgementsNICTA is funded by the Australian government as rep-resented by Department of Broadband, Communicationand Digital Economy, and the Australian Research Coun-cil through the ICT centre of Excellence programme.
DNhas also been supported by a grant from the Institute ofMuseum and Library Services, and a Google ResearchAward.1544ReferencesS.
Banerjee and T. Pedersen.
2003.
The design, im-plementation, and use of the Ngram Statistic Package.In Proceedings of the Fourth International Conferenceon Intelligent Text Processing and Computational Lin-guistics, pages 370?381, Mexico City, February.D.M.
Blei and J.D.
Lafferty.
2006.
Dynamic topic mod-els.
In ICML 2006.D.M.
Blei, A.Y.
Ng, and M.I.
Jordan.
2003.
LatentDirichlet alocation.
JMLR, 3:993?1022.S.
Brody and M. Lapata.
2009.
Bayesian word senseinduction.
In EACL 2009, pages 103?111.J.
Chang, J. Boyd-Graber, S. Gerrish, C. Wang, andD.
Blei.
2009.
Reading tea leaves: How humans in-terpret topic models.
In NIPS, pages 288?296.B.
Croft, D. Metzler, and T. Strohman.
2009.
SearchEngines: Information Retrieval in Practice.
AddisonWesley.Y.
Feng and M. Lapata.
2010.
Topic models for im-age annotation and text illustration.
In Proceedingsof Human Language Technologies: The 11th AnnualConference of the North American Chapter of the As-sociation for Computational Linguistics (NAACL HLT2010), pages 831?839, Los Angeles, USA, June.K.
Grieser, T. Baldwin, F. Bohnert, and L. Sonenberg.2011.
Using ontological and document similarity toestimate museum exhibit relatedness.
ACM Journalon Computing and Cultural Heritage, 3(3):1?20.T.
Griffiths and M. Steyvers.
2004.
Finding scientifictopics.
In PNAS, volume 101, pages 5228?5235.A.
Haghighi and L. Vanderwende.
2009.
Exploring con-tent models for multi-document summarization.
InHLT: NAACL 2009, pages 362?370.K.
Jarvelin and J. Kekalainen.
2002.
Cumulated gain-based evaluation of IR techniques.
ACM Transactionson Information Systems, 20(4).T.
Joachims.
2006.
Training linear svms in linear time.In Proceedings of the ACM Conference on KnowledgeDiscovery and Data Mining (KDD), pages 217?226,New York, NY, USA.
ACM.J.H.
Lau, D. Newman, S. Karimi, and T. Baldwin.
2010.Best topic word selection for topic labelling.
In Coling2010: Posters, pages 605?613, Beijing, China.D.
Magatti, S. Calegari, D. Ciucci, and F. Stella.
2009.Automatic labeling of topics.
In ISDA 2009, pages1227?1232, Pisa, Italy.Q.
Mei, C. Liu, H. Su, and C. Zhai.
2006.
A probabilisticapproach to spatiotemporal theme pattern mining onweblogs.
In WWW 2006, pages 533?542.Q.
Mei, X. Shen, and C. Zhai.
2007.
Automatic labelingof multinomial topic models.
In SIGKDD, pages 490?499.G.
Minnen, J. Carroll, and D. Pearce.
2001.
Appliedmorphological processing of English.
Journal of Nat-ural Language Processing, 7(3):207?223.D.
Newman, T. Baldwin, L. Cavedon, S. Karimi, D. Mar-tinez, and J. Zobel.
2010a.
Visualizing document col-lections and search results using topic mapping.
Jour-nal of Web Semantics, 8(2-3):169?175.D.
Newman, J.H.
Lau, K. Grieser, and T. Baldwin.2010b.
Automatic evaluation of topic coherence.
InProceedings of Human Language Technologies: The11th Annual Conference of the North American Chap-ter of the Association for Computational Linguistics(NAACL HLT 2010), pages 100?108, Los Angeles,USA, June.
Association for Computational Linguis-tics.D.
O` Se?aghdha.
2010.
Latent variable models of selec-tional preference.
In ACL 2010.P.
Pantel and D. Ravichandran.
2004.
Automaticallylabeling semantic classes.
In HLT/NAACL-04, pages321?328.P.
Pecina.
2009.
Lexical Association Measures: Collo-cation Extraction.
Ph.D. thesis, Charles University.A.
Ritter, Mausam, and O. Etzioni.
2010.
A la-tent Dirichlet alocation method for selectional pref-erences.
In ACL 2010.R.
Snow, B. O?Connor, D. Jurafsky, and A. Y. Ng.
2008.Cheap and fast?but is it good?
: evaluating non-expertannotations for natural language tasks.
In EMNLP?08: Proceedings of the Conference on EmpiricalMethods in Natural Language Processing, pages 254?263, Morristown, NJ, USA.I.
Titov and R. McDonald.
2008.
Modeling online re-views with multi-grain topic models.
In WWW ?08,pages 111?120.X.
Wang and A. McCallum.
2006.
Topics over time: Anon-Markov continuous-time model of topical trends.In KDD, pages 424?433.S.
Wei and W.B.
Croft.
2006.
LDA-based documentmodels for ad-hoc retrieval.
In SIGIR ?06, pages 178?185.1545
