Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on SemanticEvaluation (SemEval 2013), pages 580?584, Atlanta, Georgia, June 14-15, 2013. c?2013 Association for Computational LinguisticsEHU-ALM: Similarity-Feature Based Approach for Student ResponseAnalysisItziar Aldabe, Montse MaritxalarIXA NLP GroupUniversity of Basque Country (UPV-EHU)itziar.aldabe@ehu.esmontse.maritxalar@ehu.esOier Lopez de LacalleUniversity of EdinburghIKERBASQUE,Basque Foundation for Scienceoier.lopezdelacalle@gmail.comAbstractWe present a 5-way supervised system basedon syntactic-semantic similarity features.
Themodel deploys: Text overlap measures,WordNet-based lexical similarities, graph-based similarities, corpus-based similarities,syntactic structure overlap and predicate-argument overlap measures.
These measuresare applied to question, reference answer andstudent answer triplets.
We take into accountthe negation in the syntactic and predicate-argument overlap measures.
Our system usesthe domain-specific data as one dataset tobuild a robust system.
The results show thatour system is above the median and mean onall the evaluation scenarios of the SemEval-2013 task #7.1 IntroductionIn this paper we describe our participation with afeature-based supervised system to the SemEval-2013 task #7: The Joint Student Response Analy-sis and 8th Recognizing Textual Entailment Chal-lenge (Dzikovska et al 2013).
The goal of ourparticipation is to build a generic system that isrobust enough across domains and scenarios.
Adomain-specific system requires new training ex-amples when shifting to a new domain.
However,domain-specific data is difficult to obtain and creat-ing new resources is expensive.We seek robustness by mixing the instances fromBEETLE and SCIENTSBANK.
We show our strategyis suitable to build a generic system that performscompetitively on any domain in the 5-way task.The paper proceeds as follows.
Section 2 de-scribes the system presenting the learning featuresand the runs.
In Section 3 we show the optimiza-tion details, followed by the results (Section 4) anda preliminary error analysis (Section 5).2 System descriptionOur system aims for robustness using the domain-specific training data as one dataset.
Therefore,we do not differentiate between examples from thegiven domains (BEETLE and SCIENTSBANK) whentraining the system.
In contrast, our approach dintin-guishes between new questions (unseen answer vs.unseen question) as well as question types (how,what and why) by means of simple heuristics.The runs are organized according to different sys-tem designs.
Although all the runs use the same fea-ture set, we split the training set to build more spe-cialized classifiers.
Training examples are groupeddepending on: i) the answer is unseen; ii) the ques-tion is unseen; and iii) the question type (i.e.
what,how, why).
Each run defines a framework to explorethe different ways to approach the problem.
Whilethe first run is the simplest and is the most genericin nature, the third tries to split the task into simplerproblems and creates more specialized classifiers.2.1 Similarity learning featuresOur model is based on various text similarity fea-tures.
Almost all of the measures are computed be-tween question, reference answer and student an-swer triplets.
The measures based on syntactic struc-ture and predicate-argument overlaps are only ap-plied to the student and reference answer pairs.
In580total, we defined 30 features which can be groupedas follows:Text overlapmeasures The similarity of two textsis computed based on the number of overlappingwords.
We obtain the similarity of two texts basedon the F-Measure, the Dice Coefficient, The Cosine,and the Lesk measures.
For that, we use the imple-mentation available in the Text::Similarity package1.WordNet-based lexical similarities All the simi-larity metrics based on WordNet (Miller, 1995) fol-low the methodology proposed in (Mihalcea et al2006).
For each open-class word in one of the in-put texts, we obtain the maximun semantic similar-ity or relatedness value matching the same open-class words in the other input text.
The values ofeach matching are summed up and normalized bythe length of the two input texts as explained in(Mihalcea et al 2006).
We compute the measuresof Resnik, Lin, Jiang-Conrath, Leacock-Chodorow,Wu-Palmer, Banerjee-Pedersen, and Patwardhan-Pedersen provided in the WordNet::Similarity pack-age (Patwardhan et al 2003).Graph-based similarities The similarity of twotexts is based on a graph-based representation(Agirre and Soroa, 2009) of WordNet.
The methodis a two-step process: first the personalized PageR-ank over WordNet is computed for each text.
Thisproduces a probability distribution over WordNet.Then, the probability distributions are encoded asvectors and the cosine similarity between those vec-tors is calculated.Corpus-based similarities We compute twocorpus-based similarity measures: Latent SemanticAnalysis (Deerwester et al 1990) and LatentDirichlet Allocation (Blei et al 2003).
We estimate100 dimensions for LSA and 50 topics for LDA.Both models are obtained from a subset of the En-glish Wikipedia following the hierarchy of sciencecategories.
We started with a small set of categoriesand recovered the articles below the sub-hierarchy.We only went 3 levels down to avoid noisy articlesas the category system is rather flat.
The similarityof two texts is the cosine similarity between the1http://www.d.umn.edu/ tpederse/text-similarity.htmlresulting vectors associated with each text in thelatent space.Syntactic structure overlap The role of syntax isstudied by the use of graph subsumption based onthe approach proposed in (McCarthy et al 2008).The text is mapped into a graph with nodes rep-resenting words and links indicating syntactic de-pendencies between them.
The similarity of twotexts is computed based on the overlap of the syn-tactic structures.
Negation is handled explicitly inthe graph.Predicate-argument overlap The similarity oftwo texts is computed by analyzing the overlap ofthe predicates and their associated semantic argu-ments.
The system looks for verbal and nominalpredicates.
The similarity is also based on the ap-proach proposed in (McCarthy et al 2008).
Thegraph is represented with words as nodes and thesemantic role of arguments as links.
First, the ver-bal propositions and their arguments are automat-ically obtained (Bjo?rkelund et al 2009) as repre-sented in PropBank (Palmer et al 2005).
Second,a generalization of the predicates is obtained basedon VerbNet (Kipper, 2005) and NomBank (Meyerset al 2004).
Finally, the similarity of two textsis computed based on the overlap of the predicate-argument relations.2.2 Architecture of the runsGeneric Framework RUN1 This is the simplestframework for the assessment of student answers.The system relies on a single classifier, which hasbeen optimized on the unseen question scenario.The scenario is simulated by splitting the trainingset so that each question and its answers are in thesame fold.Unseen Framework RUN2 This framework relieson two classifiers.
The first is tuned on an unseenanswer scenario and the second is prepared for thequestion scenario (cf.
RUN1).
In order to build theunseen answer classifier, we split the training set sothat answers to the same question can occur in dif-ferent folders.
In test time, the instance is classifieddepending on whether it is an unseen answer or an581BEETLE SCIENTSBANK OVERALLUns-answ Uns-qst All Uns-answ Uns-qst Uns-dom All AllRUN1 0.499 (6) 0.352 (7) 0.404 0.396 (7) 0.283 (4) 0.345 (3) 0.348 0.406RUN2 0.526 (4) 0.352 (7) 0.413 0.418 (6) 0.283 (4) 0.345 (3) 0.350 0.414RUN3 0.502 (5) 0.370 (6) 0.415 0.424 (5) 0.260 (8) 0.337 (5) 0.340 0.403LOWEST 0.170 0.173 - 0.089 0.095 0.121 - -BEST 0.619 0.552 - 0.478 0.307 0.380 - -MEAN 0.435 0.343 - 0.341 0.240 0.267 - -MEDIAN 0.437 0.326 - 0.376 0.259 0.268 - -Table 1: 5-way results of the runs in F1 macro-average on BEETLE and SCIENTSBANK domains across differentscenarios.
Along with the runs, the LOWEST and the BEST system in each scenario are shown.
The MEAN andMEDIAN of the dataset are also presented.
Finally, the OVERALL results are showed summing up both domains.
Uns-answ refers to unseen answers scenario, Uns-qst stands for unseen question, Uns-dom unseen domain and All refersto the sum of all scenarios.
The run results are presented together with the ranked position in the task.unseen question2.Question-type Framework RUN3 The run con-sists of a set of question-type expert classifiers.
Wedivided the training set based on whether an instancereflected a what, how or why question.
We then par-titioned each question type into unseen answer andunseen question scenarios.
In total, the frameworkdeploys 6 classifiers, i.e.
a test instance is classifiedaccording to the question type and scenario.
We setheuristics to automatically distinguish the instancetype.3 Optimization on training setWe set a heuristic to create the training instances.For each student answer, if the matching referenceanswer is indicated in it, we create a triplet with thequestion, the student answer, and the matching ref-erence answer.
If there is no matching answer, thereference answer is randomly selected giving pref-erence to the best reference answers.Once we have a training set, we split it into dif-ferent ways to simulate the scenarios described inSection 2.2.
All the models are optimized using 10-fold cross-validation of the pertaining training set.For the classifiers in RUN1 and RUN2 we used 8910training instances.
For RUN3 the instances were di-vided as follows: 1235 instances for how questions,3089 for what questions and 4589 for why ques-tions.
In total, we obtained 8 models which weredistributed through the runs.2We treat unseen-domain instances as unseen-question in-stances.Our approach uses Support Vector Ma-chine (Chang and Lin, 2011) to build the classifiers.As the number of features is not high, we used thegaussian kernel in order to solve the non-linearproblem.
The main parameters of the kernel (?
andC) were tuned using grid search over the parameterin the cross-validation setting.
We focused onoptimizing the F1 macro average of the classifierin order to avoid a bias towards the major classes.Each of the 8 classifiers were tuned independently.The triplets of question, student answer and ref-erence answer of the test instances were always cre-ated selecting the first reference answer of the givenset of answers.4 ResultsA total of 8 teams participated in the 5-way task,submitting a total of 16 system runs (Dzikovska etal., 2013).
Table 1 shows the performance obtainedby our systems across domains and different scenar-ios.
Our three runs ranked differently based on theevaluation scenario: beetle-uns-answ (6,4,5 rank forRUN1, RUN2, RUN3, respectively); beetle-uns-qst(7,7,6); scientsbank-uns-answ (7,6,5); scientsbank-uns-qst (4,4,8) and scientsbank-uns-dom (3,3,5).
Wealso evaluated our runs on the entire domain (Allcolumns) and on the whole test set (OVERALL).The results show we built robust systems.
Despitebeing below the best system of each evaluation sce-nario, the results show that the runs are competitive.All our runs are above the median and outperformthe average results on each evaluation.
Overall, theresults attained in SCIENTSBANK are lower than in582BEETLE.
This might be due to the questions andanswers being longer in SCIENTSBANK, making itdifficult to obtain good patterns.As regards our runs, there is no significant overalldifference.
While RUN3 performs better in BEETLEunseen question and SCIENTSBANK unseen answer,in the rest of scenarios RUN2 outperforms the restof the runs.
As expected, RUN2 outperforms RUN1in the unseen answer scenario since the former hasa module specializing in unseen answers.
However,although RUN3 is an ensemble of six classifiers, it isnot the best run.
This is probably because the train-ing sets are not big enough.Unseen framework (RUN2)Prec Rec F1correct 0.552 0.677 0.608partially correct 0.324 0.323 0.323contradictory 0.239 0.121 0.160irrelevant 0.472 0.377 0.419non domain 0.415 0.849 0.557Macro average 0.400 0.469 0.414Micro average 0.443 0.464 0.446Table 2: results of the RUN2 system on a entire test set.Table 2 shows the detailed results of the RUN2system on the entire test set.
It is noticeable thelow results obtained on the contradictory class.
Thismight be because the defined features are not ableto model negation properly and do not deal withantonymy.
Surprisingly, the non domain class is notthe most problematic, even if the system was trainedon a low number of instances.5 Preliminary Error AnalysisWe conducted a preliminary error analysis and stud-ied some of the misclassified test instances to detectsome problematic issues and to define improvementsto our approach.Example 5.1 Sam and Jasmine were sitting on apark bench eating their lunches.
A mosquito landedon Sam?s arm and Sam began slapping at it.
Whenhe did that, he knocked Jasmine?s soda into her lap,causing her to jump up.
What was Sam?s response?R: Sam?s response was to slap the mosquito.S1: Sam?s response was to say sorryS2: To smack the bee.Some of the detected errors suggest that our useof syntax and lexical overlap is not sufficient to iden-tify the correct class.
Our system marks the studentanswer S1 from Example 5.13 as correct.
The ref-erence answer and the student answer share a greatnumber of words and the dependency trees are al-most identical, but not the meanings.
In addition, thequestion contains additional information that mayrequire other types of features to correctly classifythe instance.The predicate-argument overlap feature tries togeneralize the predicate information to find similar-ities between verbs with the same meaning.
How-ever, our system does not always work in a correctway.
The verb smack in the student answer S2 andthe verb slap in the reference answer mean the same.Our system classifies the answer incorrectly.
If welook at PropBank and VerbNet, we find that thereis not mapping between PropBank and VerbNet forthese particular verbs.Example 5.2 Why do you think the other terminalsare being held in a different electrical state than thatof the negative terminal?R: Terminals 4, 5 and 6 are not connected to thenegative battery terminalS1: They are connected to the positive battery ter-minalWe consider the negation as part of the syntac-tic and predicate-argument overlap measures.
How-ever, our system does not characterize the similar-ity between not connected to the negative and con-nected to the positive (Example 5.2).
This type ofexamples suggest that the system needs to model thenegation and antonyms with additional features.In the future, further error analysis will be car-ried out to design features to better model the prob-lem.
We also anticipate creating a specialized fea-ture space for each question type.AcknowledgmentsThis research was partially funded by the Ber2Tekproject (IE12-333), the SKaTeR project (TIN2012-38584-C06-02) and the NewsReader project (FP7-ICT-2011-8-316404).3R refers to the reference answer and S1 and S2 to studentanswers.583ReferencesEneko Agirre and Aitor Soroa.
2009.
Personalizingpagerank for word sense disambiguation.
In Proceed-ings of the 12th conference of the European chapter ofthe Association for Computational Linguistics (EACL-2009), Athens, Greece.Anders Bjo?rkelund, Love Hafdell, and Pierre Nugues.2009.
Multilingual semantic role labeling.
In Pro-ceedings of The Thirteenth Conference on Compu-tational Natural Language Learning (CoNLL-2009),pages 43?48.David M. Blei, Andrew Y. Ng, and Michael I. Jordan.2003.
Latent Dirichlet alcation.
Journal of MachineLearning Research, 3:993?1022.Chih-Chung Chang and Chih-Jen Lin.
2011.
Libsvm:A library for support vector machines.
ACM Trans.Intell.
Syst.
Technol., 2(3):27:1?27:27, May.Scott Deerwester, Susan Dumais, Goerge Furnas,Thomas Landauer, and Richard Harshman.
1990.
In-dexing by Latent Semantic Analysis.
Journal of theAmerican Society for Information Science, 41(6):391?407.Myroslava O. Dzikovska, Rodney Nielsen, Chris Brew,Claudia Leacock, Danilo Giampiccolo, Luisa Ben-tivogli, Peter Clark, Ido Dagan, and Hoa Trang Dang.2013.
Semeval-2013 task 7: The joint student re-sponse analysis and 8th recognizing textual entailmentchallenge.
In *SEM 2013: The First Joint Conferenceon Lexical and Computational Semantics, Atlanta,Georgia, USA, 13-14 June.
Association for Compu-tational Linguistics.Karin Kipper.
2005.
VerbNet: A broad-coverage, com-prehensive verb lexicon.
Ph.D. thesis, University ofPennsylvania.Philip M. McCarthy, Vasile Rus, Scott A. Crossley,Arthur C. Graesser, and Danielle S. McNamara.
2008.Assessing forward-, reverse-, and average-entailmentindices on natural language input from the intelligenttutoring system, iSTART.
In D. Wilson and G. Sut-cliffe, editors, Proceedings of the 21st InternationalFlorida Artificial Intelligence Research Society Con-ference, pages 201?206, Menlo Park, CA: The AAAIPress.Adam Meyers, Ruth Reeves, Catherine Macleod, RachelSzekely, Veronika Zielinska, Brian Young, and RalphGrishman.
2004.
The nombank project: An interimreport.
In A. Meyers, editor, HLT-NAACL 2004 Work-shop: Frontiers in Corpus Annotation, pages 24?31,Boston, Massachusetts, USA, May 2 - May 7.
Associ-ation for Computational Linguistics.Rada Mihalcea, Courtney Corley, and Carlo Strappar-ava.
2006.
Corpus-based and knowledge-based mea-sures of text semantic similarity.
In Proceedings theAmerican Association for Artificial Intelligence (AAAI2006), Boston.George A. Miller.
1995.
Wordnet: A lexical database forenglish.
Communications of the ACM, 38(11):39?41.Martha Palmer, Dan Gildea, and Paul Kingsbury.
2005.The proposition bank: A corpus annotated with se-mantic role.
Computational Linguistics, 31(1):71?106.Siddharth Patwardhan, Satanjeev Banerjee, and Ted Ped-ersen.
2003.
Using measures of semantic related-ness for word sense disambiguation.
In Proceedingsof the Fourth International Conference on IntelligentText Processing and Computational Linguistics.584
