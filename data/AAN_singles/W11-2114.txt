Proceedings of the 6th Workshop on Statistical Machine Translation, pages 130?134,Edinburgh, Scotland, UK, July 30?31, 2011. c?2011 Association for Computational LinguisticsMAISE: A Flexible, Configurable, Extensible Open Source Package forMass AI System EvaluationOmar F. ZaidanDept.
of Computer ScienceandThe Center for Language and Speech ProcessingJohns Hopkins UniversityBaltimore, MD 21218, USAozaidan@cs.jhu.eduAbstractThe past few years have seen an increasinginterest in using Amazon?s Mechanical Turkfor purposes of collecting data and perform-ing annotation tasks.
One such task is themass evaluation of system output in a varietyof tasks.
In this paper, we present MAISE,a package that allows researchers to evalu-ate the output of their AI system(s) using hu-man judgments collected via Amazon?s Me-chanical Turk, greatly streamlining the pro-cess.
MAISE is open source, easy to run, andplatform-independent.
The core of MAISE?scodebase was used for the manual evaluationof WMT10, and the completed package is be-ing used again in the current evaluation forWMT11.
In this paper, we describe the mainfeatures, functionality, and usage of MAISE,which is now available for download and use.1 IntroductionThe ability to evaluate system output is one of themost important aspects of system development.
Aproperly designed evaluation paradigm could helpresearchers test and illustrate the effectiveness, orlack thereof, of any changes made to their system.The use of an automatic metric, whether it is a sim-ple one such as classification accuracy, or a moretask-specific metric such as BLEU and TER for ma-chine translation, has become a standard part of anyevaluation of empricial methods.
There is also ex-tensive interest in exploring manual evaluation ofsystem outputs, and in making such a process fea-sible and efficient, time- and cost-wise.
Such humanfeedback would also be valuable because it wouldhelp identify systematic errors and guide future sys-tem development.Amazon?s Mechanical Turk (MTurk) is a virtualmarketplace that allows anyone to create and posttasks to be completed by human workers around theglobe.
Each instance of those tasks, called a HumanIntelligence Task (HIT) in MTurk lingo, typicallyrequires human understanding and perception thatmachines are yet to achieve, hence making MTurkan example of ?artificial artificial intelligence,?
asthe developers of MTurk aptly put it.
Arguably, themost attractive feature of MTurk is the low cost asso-ciated with completing HITs and the speed at whichthey are completed.Having discovered this venue, many researchersin the fields of artificial intelligence and machinelearning see MTurk as a valuable and effectivesource of annotations, labels, and data, namely thekind requiring human knowledge.One such kind of data is indeed human evalua-tion of system outputs.
For instance, if you constructseveral speech recognition systems, and would liketo know how well each of the systems performs,you could create HITs on MTurk that ?showcase?
thetranscriptions obtained by the different systems, andask annotators to indicate which systems are supe-rior and which ones are inferior.
The same can beapplied to a variety of tasks, such as machine trans-lation, object recognition, emotion detection, etc.The aim of the MAISE package is to stream-line the process of creating those evaluation tasksand uploading the relevant content to MTurk to bejudged, without having to familiarize and involveoneself with the mechanics, if you will, of Mechan-ical Turk.
This would allow you to spend moretime worrying about improving your system ratherthan dealing with file input and output and MTurk?ssometimes finicky interface.1302 OverviewMAISE is a collection of tools for Mass AI SystemEvaluation.
MAISE allows you to evaluate the out-put of different systems (and/or different variationsof a system) using the workforce of Amazon?s Me-chanical Turk (MTurk).
MAISE can be used to com-pare two simple variants of the same system, work-ing with a couple of variations of your task, or it canbe used to perform complete evaluation campaignsinvolving tens of systems and many variations.The core of MAISE?s codebase was written torun the manual component of WMT10?s evaluationcampaign.
In the manual evaluation, various MTsystems are directly compared to each other, by an-notators who indicate which systems produce betteroutputs (i.e.
better translations).
Starting in 2010,the evaluation moved from using a locally hostedweb server, and onto MTurk, taking advantage ofMTurk?s existing infrastructure, and making avail-able the option to collect data from a large pool ofannotators, if desired, rather than relying solely onrecruited volunteers.
That evaluation campaign in-volved around 170 submissions over eight differentlanguage pairs.
In 2011, the number increased to190 submissions over ten language pairs.We note here that although MAISE was writtenwith MT in mind, it can be used for other ML/AItasks as well.
Some of the supported features aremeant to make MT evaluation easier (e.g.
MAISE isaware of which language is being translated to andfrom), but those could simply be ignored for othertasks.
As long as the task has some concept of ?in-put?
and some concept of ?output?
(e.g.
a foreignsentence and a machine translation), then MAISE isappropriate.Given this paper?s venue of publication, the re-mainder of the paper assumes the task at hand is ma-chine translation.3 The Mechanics of MAISEThe components of MAISE have been designed tocompletely eliminate the need to write any dataprocessing code, and to minimize the need for theuser to perform any manual tasks on MTurk?s inter-face, since MAISE facilitates communication withMTurk.
Whenever MAISE needs to communicatewith MTurk, it will rely on MTurk?s Java SDK,which is already included in the MAISE release(allowed under the SDK?s license, Apache LicenseV2.0).Once you create your evaluation tasks and uploadthe necessary content to MTurk, workers will beginto complete the corresponding HITs.
On a regular(e.g.
daily) basis, you will tell MAISE to retrieve thenew judgments that workers provided since the lasttime MAISE checked.
The process continues untileither all your tasks are completed, or you decideyou have enough judgments.You can use MAISE with any evaluation setupyou like, as long as you design the user interfacefor it.
Currently, MAISE comes with existing sup-port for a particular evaluation setup that asks anno-tators to rank the outputs of different systems rela-tive to each other.
When we say ?existing support?we mean the user interface is included, and so is ananalysis tool that can make sense of the judgments.This way, you don?t need to do anything extra to ob-tain rankings of the systems.
You can read moreabout this evaluation setup in the overview papersof the Workshop on Statistical Machine Translation(WMT) for the past two years.3.1 Requirements and SetupMAISE is quite easy to use.
Beyond compilinga few Java programs, there is no need to installanything, modify environment variables, etc.
Fur-thermore, since it is Java-based, it is completelyplatform-independent.To use MAISE, you will need:?
Java 6?
Apache Ant?
A hosting location (where you place certainHTML files)?
An MTurk Requester accountYou will also need an active Internet connectionwhenever new tasks need to be uploaded to MTurk,and whenever judgments need to be collected fromMTurk.
The setup details are beyond the scopeof this paper, but are straightforward, and can befound in MAISE?s documentation, including guid-ance with all the MTurk-related administrative is-sues (e.g.
the last point in the above list).1313.2 Essential FilesMAISE will assume that the user has a certain set of?essential files?
that contain all the needed informa-tion to perform an evaluation.
These files are:1) The system outputs should be in plaintext format, one file per system.
Thefilenames should follow the patternPROJECT.xx-yy.sysname, wherePROJECT is any identifying string cho-sen by the user, xx is a short name for thesource language, and yy is a short name forthe the target language.2) The source files should be in plain textas well, one file per language pair.
Thesource filenames should follow the patternPROJECT.xx-yy.src, where PROJECTmatches the identifying string used in the sub-mission filenames.
(The contents of such a fileare in the xx language.
)3) The reference files, also one per language pair,with filenames PROJECT.xx-yy.ref.
(Thecontents of such a file are in the yy language.
)4) A specification file that contains values for var-ious parameters about the project (e.g.
the lo-cation of the above files).5) A batch details file that contains informationabout the desired number of MTurk tasks andtheir particular properties.As one could see, the user need only provide thebare minimum to get their evaluation started.
Moredetails about items (4) and (5) are provided in thedocumentation.
Essentially, they are easily readableand editable files, and all the user needs to do to cre-ate them is to fill out the provided templates.3.3 The Components of MAISEThere are three main steps necessary to perform anevaluation on MTurk: create the evaluation tasks,upload them to MTurk, and retrieve answers forthem.
Each of those three steps corresponds to asingle component in MAISE.3.3.1 The BatchCreatorThe first step is to create some input files forMTurk: the files that contain actual instantiations ofour tasks, with actual sentences.
This will be the firststep that requires you to make some real executivedecisions regarding your tasks.
Among other things,you will decide how many judgments to collect andwho to allow to give you those judgments.Each batch corresponds to a single task onMTurk.
Typically, each batch corresponds to a sin-gle language pair.
So, if you are performing afull evaluation campaign, you would be creating asmany batches as there are language pairs.
If you aremerely comparing several variants of the same sys-tem, say, for Arabic-English, you would probablyhave just one batch.That said, you may have more than one batch forthe same language pair, that nonetheless differ inother properties.
In fact, each batch has a numberof settings that need to be specified, including:1) what language pair does this batch involve?2) how many HITs does this batch include?3) how many times should each HIT be com-pleted?4) what is the reward per assignment?5) what are the qualifications necessary for an an-notator to be allowed to perform the task (e.g.location, approval rating)?Those settings are all specified in a single file,the abovementioned batch details file.
The userthem simply runs the BatchCreator component,which processes all this information and creates thenecessary files for each batch.3.3.2 The UploaderAfter the BatchCreator creates the differentfiles for the different batches, those files must beuploaded to MTurk in order to create the variousbatches.
There will be a single file, called the up-load info file, that contains the locations of the filesto be uploaded.
The upload info file is created au-tomatically, and all the user needs to do is pass itas a parameter to the next MAISE component, theUploader.132The Uploader communicates with MTurk viaa web connection.
Once it has completed execution,HITs for your tasks will start to appear on the MTurkwebsite, available for MTurk?s workers to view andcomplete them.3.3.3 The RetrieverAt this point, you would be waiting for Turkers tofind your task and start accepting HITs and complet-ing them.
You can retrieve those answers by usinganother MAISE component that communicates withMTurk called the Retriever.
It can be instructedto retrieve all answers for your HITs or only a subsetof them.
It retrieves all the answers for those HITs,and appends those answers to an answer log file.Note that the Retriever does not necessarilyapprove any of the newly submitted assignments.
Itcan be instructed to explicitly retrieve those answerswithout approving them, giving you the chance tofirst review them for quality.
Alternatively, it can beinstructed to approve the assignments as it retrievesthem, and also to reject certain assignments or cer-tain annotators that you have identified as being ofsub-par quality.
All this information is placed inplain text files, easy to create and maintain.When you use MAISE to perform an actual eval-uation on MTurk, you should run the Retrieverfairly regularly, perhaps once every day or two.Each time, review the retrieved results, and rerunthe Retriever in ?decision mode?
enabled, toaprove/reject the pending submissions.4 Analyzing the Results: An ExampleOnce the tasks have been completed, all the an-swers will have been written into an answers log file.The log file is in plain format, and contains exten-sive information about each HIT completed, includ-ing a worker ID, time required to complete, and, ofcourse, the answers themselves.
Naturally, analyz-ing the results of the evaluation depends on what thetask was, and what the interface you designed lookslike.
You can write your own code to read the logfile and make sense out of them.MAISE already comes equipped with an analy-sis tool for one particular task: the ranking task.
Inthis setup, the annotator evaluates system outputs byranking them from best to worst.
The rank labelsare interpreted as pairwise comparisons (e.g.
5 ranklabels correspond to(52)= 10 pairwise compar-isons), and each system is assigned a score reflect-ing how often it wins those pairwise comparisons.This is the setup used in the evaluation campaignsof WMT10 and WMT11.The analysis tool takes as input the answers logfile as is, and extracts from it all the rank labels.Each system?s score is computed, and the tool pro-duces a table for each language pair displaying theparticipating systems, in descending order of theirscores.
It also creates an additional head-to-head ta-ble, that summarizes for a specific pair of systemshow often each system outranked the other.
The out-put is created in HTML format, for easy viewing ina browser.Furthermore, the tool produces a detailed workerprofile table.
Each row in this table corresponds toone worker, identified by their Amazon worker ID,and includes certain measures that can help guideyou identify bad workers, who are either clickingrandomly, or perhaps simply not doing the task prop-erly.
Those measures include:?
Average time required per HIT: a suspi-ciously fast annotator might not be performingthe task diligently.?
The reference preference rate (RPR): how of-ten did the annotator correctly prefer an em-bedded reference translation; a low RPR almostcertainly indicates random clicking, with typi-cal good values at 0.97 and up.?
Prevalence of tied rank labels: an overly highpercentage of tied comparisons indicates anoverly ?conservative?
worker, hesitant to distin-guish between outputs.?
The annotator?s intra-annotator agreement:i.e.
the annotator?s consistency with them-selves, based on how often they repeated thesame judgment when comparing the same sys-tem pair.To appreciate the tool?s output, the reader is en-couraged to view the results of a real-life evaluationcampaign at http://bit.ly/jJYzkO.
Theseare results of analyzing 85,000+ rank labels in anevaluation campaign of 40+ MT systems over sixlanguage pairs.1335 Download and LicensingMAISE can be obtained from the author?s webpage:http://cs.jhu.edu/?ozaidan/maise/.The release includes MAISE?s source code, in-structions, documentation, and a tutorial.
MAISEis an open-source tool, licensed under the terms ofthe GNU Lesser General Public License (LGPL).Therefore, it is free for personal and scientific useby individuals and/or research groups.
It may notbe modified or redistributed, publicly or privately,unless the licensing terms are observed.
If in doubt,contact the author for clarification and/or an explicitpermission.
The distribution also includes theMTurk Java SDK v1.2.2, which is licensed underthe terms of the Apache License V2.0.AcknowledgmentsI developed MAISE while I was funded by DARPA?sGALE Program, and in part by the EuroMatrixPlusProject.
I would like to thank Chris Callison-Burch,Ondr?ej Bojar, and everybody who gave feedbackduring the WMT10 evaluation campaign.
Moreimportantly, much thanks goes to Josh Schroeder,the author of the previous implementation used pre-2010, who helped me navigate his code when I firststarted reimplementing the data processing compo-nents.ReferencesChris Callison-Burch, Philipp Koehn, Christof Monz,Kay Peterson, Mark Przybocki, and Omar Zaidan.2010.
Findings of the 2010 joint workshop on sta-tistical machine translation and metrics for machinetranslation.
In Proceedings of the Joint Fifth Workshopon Statistical Machine Translation and MetricsMATR,pages 17?53, Uppsala, Sweden, July.
Association forComputational Linguistics.134
