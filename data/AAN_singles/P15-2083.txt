Proceedings of the 53rd Annual Meeting of the Association for Computational Linguisticsand the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 506?510,Beijing, China, July 26-31, 2015.c?2015 Association for Computational LinguisticsExtended Topic Model for Word DependencyTong Wang1, Vish Viswanath2and Ping Chen11University of Massachusetts Boston, Boston, MA2Harvard School of Public Health, Boston, MAtongwang0001@gmail.com, Ping.Chen@umb.eduVish Viswanath@dfci.harvard.eduAbstractTopic Model such as Latent DirichletAllocation(LDA) makes assumption thattopic assignment of different words areconditionally independent.
In this paper,we propose a new model Extended GlobalTopic Random Field (EGTRF) to modelnon-linear dependencies between words.Specifically, we parse sentences into de-pendency trees and represent them as agraph, and assume the topic assignment ofa word is influenced by its adjacent wordsand distance-2 words.
Word similarity in-formation learned from large corpus is in-corporated to enhance word topic assign-ment.
Parameters are estimated efficientlyby variational inference and experimen-tal results on two datasets show EGTRFachieves lower perplexity and higher logpredictive probability.1 IntroductionProbabilistic topic model such as Latent Dirich-let Allocation(LDA) (Blei et al, 2003) has beenwidely used for discovering latent topics fromdocument collections by capturing words?
co-occuring relation.
However, the ?bag of words?assumption is employed in most existing topicmodels, it assumes the order of words can be ig-nored and topic assignment of each word is condi-tionally independent given the topic mixture of adocument.To relax the ?bag of words?
assumption, manyextended topic models have been proposed to ad-dress the limitation of conditional independence.Wallach (Wallach, 2006) explores a hierarchicalgenerative probabilistic model that incorporatesboth n-gram statistics and latent topic variables.Gruber (Gruber et al, 2007) models the topics ofwords in the document as a Markov chain, and as-sumes all words in the same sentence are morelikely to have the same topic.
Zhu (Zhu et al,2010) incorporates Markov dependency betweentopic assignments of neighboring words, and em-ploys a general structure of the GLM to definea conditional distribution of latent topic assign-ments over words.
Most of the models above arelimited to model linear topical dependencies be-tween words, word topical dependencies can alsobe modeled by a non-linear way.
In Syntactic topicmodels (Boyd-Graber et al, 2009), each word ofa sentence is generated by a distribution that com-bines document-specific topic weights and parse-tree-specific syntactic transitions.In Global Topic Random Field(GTRF)model (Li et al, 2014), sentences of a documentare parsed into dependency trees (Marneffe etal, 2008) (Manning et al, 2014) (Marneffe etal, 2006).
They show topics of semanticallyor syntactically dependent words achieve thehighest similarity and are able to provide moreuseful information in topic modeling, which isalso the basic assumption of our model.
Thenthey propose GTRF to model non-linear topicaldependencies, word topics are sampled basedon graph structure instead of ?bag of words?representation, the conditional independence ofword topic assignment is thus relaxed.However, GTRF assumes topic assignment of aword vertex depends on the topic mixture of thedocument and its neighboring word vertices, ig-noring the fact that word vertex can also be influ-enced by the distance-2 or further word vertices.In this paper, we extend GTRF model and presenta novel model Extended Global Topic RandomField (EGTRF) to exploit topical dependency be-tween words.
In EGTRF, the topic assignment of aword is assumed to depend on both distance-1 anddistance-2 word vertices.
An example of a simpledocument that has two sentences shows in Figure1.
The two sentences are parsed into dependencytrees respectively, and then merged into a graph.506standsLDA alloationlatent dirichlet(a) Sentence 1discoversIt topicslatentcorpus(b) Sentence 2standsLDA alloationlatenttopicsdiscoversit corpusdirichlet(c) DocumentExample document: LDA stands for latent dirichlet allocation.
It discoverslatent topcis from corpus.Example word vertex: allocationDistance-1 word vertics: {stands, latent, dirichlet}Distance-2 word vertics: {LDA, topics}Figure 1: Dependency tree exampleSome hidden dependency relations can also be ex-tracted by merging dependency trees.
For exam-ple, word ?allocation?
has a new distance-2 word?topics?
after merging.
Therefore, EGTRF canexploit more semantically or syntactically worddependencies.
Theoretically, we can also modelthe distance further than 2, however, it leads tomore complicated computation and small increaseof performance.Another advantage of EGTRF is it incorporatesword features.
The word vector representationsare very interesting because the learned vectorsexplicitly encode many linguistic regularities andpatterns (Mikolov et al, 2013).
We use the pre-trained model from Google News dataset(about100 billion words) using word2vec1tool to repre-sent each word as a 300-dimensional word vector,and apply normalized word similarity as a con-fidence score to indicate how possible two wordvertices share same topic.We organized the paper as below: EGTRF ispresented in Section 2, variational inference andparameter estimation are derived in Section 3, ex-periments on two datasets are showed in Section4, we conclude the paper in Section 5.2 Extended Global Topic Random FieldIn this section, we first present Extended GlobalRandom Field(EGRF) in section 2.1, then showhow to model topical dependencies using EGRFin section 2.2.
We incorporate word similarity in-formation into model in section 2.3.1https://code.google.com/p/word2vec/2.1 Extended Global Random FieldAfter representing document to undirected graphon previous section, we extend Global RandomField and give the definition of Extended GlobalRandom Field to model the graph as below:Given an undirected graph G, word vertex set isdenoted as W = {wi|i = 1, 2, ..n}, where wiis aword vertex, and n is the number of unique wordsin a document.
E1is distance-1 edge set, E1={(wi, wj)|?path between wi, wjthat length is 1}.E2is distance-2 edge set, E2={(wi, wj)|?path between wi, wjthat length is 2}.The state(topic assignment) of a word vertex w isgenerated from Z = {zi|i = 1, 2, ..., k}, k is thenumber of topics.P (G) = fG(g) =1| E1| + | E2|?w?Wf(zw)?(?(w?1,w?
?1)?E1f(1)(zw?1, zw?
?1) +?(w?2,w?
?2)?E2f(2)(zw?2, zw??2))(1)s.t.
1.f(z) > 0, f(1)(z?, z??)
> 0, f(2)(z?, z??)
> 02.?z?Zf(z) = 13.?z?,z???Zf(z?)f(z??
)f(1)(z?, z??)
= 14.?z?,z???Zf(z?)f(z??
)f(2)(z?, z??)
= 1In Equation (1), f(z) is the function defined onword vertex, which is a probability measure be-cause of the constraints 1 and 2. f(1)(z, z?)
andf(2)(z, z?)
are the function defined on edge set E1and E2.
f(1)and f(2)are not necessarily probabil-ity measure, however, summing over all possiblestates of the product of the edge and the linkedword pair should equal to 1, which are from con-straints 3 and 4.
So f(z?)f(z??
)f(1)(z?, z??)
andf(z?)f(z??
)f(2)(z?, z??)
are probability measure.
gis one sample of word topic assignments fromgraph G. If Equation (1) satisfies all the four con-straints, it is easy to verify P (G) is also a prob-ability measure since summing over all possiblesamples g equals to 1.We define the random field as in Equation (1)a Extended Global Random Field (EGRF).
AndEGRF does not have normalization factor, whichis much simplier than models with intractable nor-malizing factor.2.2 Topic Model Using EGRFWe define Extended Global Topic Random Fieldbased on EGRF.
EGTRF is a generative proba-507bilistic model, the basic idea is that documentsare represented as mixtures of topics, words aregenerated depending on the topic mixtures andgraph structure of current document.
The genera-tive process for word sequence of a document isdescribed as below:For each document d in corpus D:Transform document d into graph.Choose ?
?
Dir(?
).For each of the n words wnin d:Choose topic zn?
Pegrf(z | ?
),Choose word wn?Multi(?zn,wn).Given Dirichlet prior?, word distribution of topics?, topic mixture of document ?, topic assignmentsz and words w. We obtain the marginal distribu-tion of a document:p(w | ?, ?)
=?P (?
| ?
)?zPegrf(z | ?
)?nP (wn| zwn, ?)d?
(2)We can see the marginal distribution is similarto LDA except topic assignment of word is sam-pled by Extended Global Random Field insteadof Multinomial.
So the word topic assignment isno longer conditionally independent.
Accordingto EGRF described in section 2.1, we define theprobability of topic sequence z as below:Pegrf(z | ?)
=1| E1| + | E2|?w?Vf(zw)?(?(w?1,w?
?1)?E1f(1)(zw?1, zw?
?1) +?(w?2,w?
?2)?E2f(2)(zw?2, zw?
?2))(3)where f(zw) = Multi(zw|?)
(4)f(1)(zw?1, zw?
?1) = ?zw?1=zw?
?1?1+ ?zw?16=zw?
?1?2(5)f(2)(zw?2, zw?
?2) = ?zw?2=zw?
?2?3+ ?zw?26=zw??2?4(6)?
is an indicator function and equals 1 if thetopic assignments of two words on an edge aresame.
In order to model Equation (3) as an EGRF,it must satisfy all the four constraints in Equation(1).
Equation (4) defines word vertex as multino-mial distribution, and we assign ?1, ?2, ?3and ?4nonzero values, then it is clear to verify constraint1 and 2 are satisfied.
To satisfy the constraint 3and 4, combine with (5), (6), we get the relationbetween ?1and ?2, ?3and ?4.?
?2i?1+ (1??
?2i)?2= 1 i = 1, 2, ..|E1| (7)?
?2i?3+ (1??
?2i)?4= 1 i = 1, 2, ..|E2| (8)Lower ?2, ?4give higher reward to the edgethat connects two word vertices with same topic.If (7) and (8) hold true, Equation (3) is an EGRF.And we define the topic model based on EGRF asExtended Global Topic Random Field(EGTRF).If |E2| = 0, |E1| 6= 0, EGTRF is equivalent toGTRF.
If |E1| = 0, |E2| = 0, EGTRF is equiva-lent to LDA.2.3 Word Similarity InformationThe coherent edge is the edge that the two linkedwords have same topic.
In distance-i edge set,i= 1, 2.
ECiincludes all coherent edges, ENCicontains all non-coherent edges.
Then equation(3) can be represented as below:Pegrf(z | ?
)=1| E1| + | E2|?w?VMulti(zw| ?)?
(| EC1| ?1+ | ENC1| ?2+ | EC2| ?3+ | ENC2| ?4)=?w?VMulti(zw| ?
)(| E1| + | E2|)?T??
(| EC1| (1?
?2)+ | E1| ?2?T?+| EC2| (1?
?4)+ | E2| ?4?T?
)(9)From the second line to the third line of Equa-tion (9), we represent ?1, ?3as the function of?2, ?4based on (7) and (8).
The expectation ofthe number of edges in Ecican be computed as:E(| ECi|) =?(w1,w2)?Ei?Tw1?w2Sw1,w2(10)?
is the K dimensional variational multinomialparameters and can be thought as the posteriorprobability of a word given the topic assignment.Sw1,w2is the similarity measure between word w1and w2.As we discussed in section 1, word similarityinformation Sw1,w2works as a confidence score tomodel how likely two words on an edge have sametopic.
And we make assumption that two wordsare more likely to have same topic if they have ahigher similarity score.
To get the similarity scorebetween words, we use word2vec tool to learn theword representation of each word from pre-trainedmodel.
The word representations are computedusing neural networks, and the learned representa-tions explicitly encode many linguistic regularities508Figure 2: Experimental results on NIPS(left) and 20 news(right) dataand patterns from the corpus.
Normalized similar-ity between word vectors can be regarded as theconfidence score of how possible two words havesame topic.
In this way, knowledge from large cor-pus other than current document collections is in-corporated to guide topic modeling.3 Posterior Inference and ParameterEstimationWe derive Variational Inference for posterior in-ference.
The variational function q is same to theoriginal LDA paper (Blei et al, 2003).
All termsexcept P (z|?)
in likelihood function are also sameto LDA, Based on Equation (9), we obtain:Eq[logPegrf(z | ?
)]?Eq[log(?nMulti(zwn| ?))]+1?
?2?1Eq(| EC1|) +1?
?4?1Eq(| EC2|)+(| E1| ?2+ | E2| ?4?1?| E1| + | E2|?2)Eq(?T?
)+log ?1?
log ?2(11)We get the approximation in Equation (11)from Taylor series, where ?1and ?2are Taylorapproximation.
Eq(| ECi|) is obtained directlyfrom (10),Eq(?T?)
is from the property of Dirich-let distribution.
The updating rule of ?
and ?
aresame to LDA, ?
is updated using Newton methodsince we can not obtain the direct updating rule for?.
?
can be approximated as:?wn,i?
?i,vexp(?(?i)+1?
?2?1??(wn,wm)?E1?wm,iSm,n+1?
?4?1??
(wn,wp)?E2?wp,iSp,n) (12)EM algorithm is applied using above updatingrules.
At E-step, we estimate the best ?
and ?given current ?
and ?.
At M-step, we update new?
and ?
based on obtained ?
and ?.
We run suchiterations until convergence.4 ExperimentIn this section we study the empirical performanceof EGTRF on two datasets.
For each dataset, weremove very short documents, and compute a vo-cabulary by removing stop words, rare words, fre-quent words.
Eighty percent data are used fortraining, others for testing.?
20 News Groups: After processing, it con-tains 13706 documents with a vocabulary of5164 terms.?
NIPS data (Globerson et al, 2004): Span-ning from 2000 to 2005.
After processing,it contains 843 documents with a vocabularyof 6098 terms.We evaluate how well a model fits the data withheld-out perplexity (Blei et al, 2003) and predic-tive distribution (Hoffman et al, 2013).
Lowerperplexity, higher log predictive probability indi-cate better generalization performance.
We im-plement GTRF without adding self defined edgesfrom the original paper, and set ?2= 0.2 to givehigher reward to edges from E1that the two wordvertices have same topic.
We set ?4= 1.2 togive lower(even negative) reward to edges fromE2that the two word vertices have same topic inEGTRF, since the distance-1 words are expectedto have greater topical affects than distance-2509words.
Word is represented as vector from pre-trained Google News dataset, we use the word vec-tor learned from original corpus when the worddoes not exist in pre-trained Google News dataset.We choose 10, 20, 30, 50 topics for 20 newsdataset, 10, 15, 20, 25 topics for NIPS dataset.Figure 2 shows the experimental results of fourmodels: lda, gtrf, egtrf(EGTRF without wordsimilarity information), and egtrf+s(EGTRF withword similarity information) on two datasets.
Theresults show EGTRF outperforms LDA and GTRFin general, and EGTRF with word similarity infor-mation achieves best performance.We believe modeling distance-2 word verticescan exploit more semantically or syntacticallyword dependencies from document, and word sim-ilarity information obtained from large corpus canmake up the lack of sufficient information from theoriginal corpus.
Therefore, adding the influence ofdistance-2 word vertices and word similarity infor-mation can improve performance of topic model-ing.5 ConclusionIn this paper, we extended Global Topic RandomField(GTRF) and proposed a novel topic modelExtended Global Topic Random Field(EGTRF)which can model dependency relation betweenadjacent words and distance-2 words.
Wordtopics are drawed by Extended Global RandomField(EGRF) instead of Multinomial, the con-ditional independence of word topic assignmentis thus relaxed.
Word similarity informationlearned from large corpus is incorporated into themodel.
Experiments on two datasets show EGTRFachieves better performance than GTRF and LDA,which confirm our assumption that adding topicaldependency of distance-2 words and incorporatingword similarity information can improve modelperformance.ReferencesAmir Globerson, Gal Chechik, Fernando Pereira, Naf-tali Tishby Euclidean Embedding of Co-occurrenceData.
In Advances in neural information processingsystems.
pp.
497-504.
2004.Amit Gruber, Michal Rosen-Zvi and Yair Wei.
Hid-den Topic Markov Models.
In Proceedings of theEleventh International Conference on Artificial In-telligence and Statistic.
pp.
163-170.
2007.David Blei, Andrew Ng., and Michael Jordan.
La-tent Dirichlet Allocation.
The Journal of MachineLearning Research.
3:993-1022, 2003.Hanna M Wallach.
Topic modeling: Beyond bag-of-words.
In International Conference on MachineLearning.
pp.
977-984.
ACM, 2006.Jordan Boyd-Graber and David Blei.
Syntactic topicmodels.
In Neural Information Processing Systems.pp.
185-192.
2009.Jun Zhu and Eric P. Xing.
Conditional Topic RandomFields.
In Proceedings of the 27th InternationalConference on Machine Learning.
2010.Christopher D. Manning, Mihai Surdeanu, John Bauer,Jenny Finkel, Steven J. Bethard and David Mc-Closky.
The Stanford CoreNLP Natural LanguageProcessing Toolkit.
In Proceedings of 52nd AnnualMeeting of the Association for Computational Lin-guistics: System Demonstrations.
pp.
55-60.
2014.Marie-Catherine de Marneffe, Bill MacCartney, andChristopher D. Manning.
Generating typed depen-dency parses from phrase structure parses.
In Pro-ceedings of LREC.
Vol.
6, No.
2006, pp.
449-454.2006.Marie-Catherine de Marneffe, Christopher D. Man-ning.
The Stanford typed dependencies represen-tation.
In COLING 2008 Workshop on Cross-framework and Cross-domain Parser Evaluation.pp.
1-8.
2008.Matthew Hoffman, David Blei, Chong Wang, JohnPaisley Stochastic Variational Inference The Jour-nal of Machine Learning Research.
14(1), 1303-1347.
2013.Tomas Mikolov, Kai Chen, Greg Corrado, and JeffreyDean.
Efficient Estimation of Word Representationsin Vector Space.
In Proceedings of Workshop atICLR.
arXiv:1301.3781, 2013.Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Cor-rado, and Jeffrey Dean.
Distributed Representationsof Words and Phrases and their Compositionality.
InProceedings of NIPS.
pp.
3111-3119.
2013.Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig.Linguistic Regularities in Continuous Space WordRepresentations.
In Proceedings of NAACL HLT.pp.
746-751, 2013.Zhixing Li, Siqiang Wen, Juanzi Li, Peng Zhang andJie Tang.
On Modeling Non-linear Topical Depen-dencies.
In Proceedings of the 31th InternationalConference on Machine Learning.
pp.
458-466,2014.510
