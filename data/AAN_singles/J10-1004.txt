The Noisy Channel Model for UnsupervisedWord Sense DisambiguationDeniz Yuret?Koc?
UniversityMehmet Ali YatbazKoc?
UniversityWe introduce a generative probabilistic model, the noisy channel model, for unsupervised wordsense disambiguation.
In our model, each context C is modeled as a distinct channel throughwhich the speaker intends to transmit a particular meaning S using a possibly ambiguous wordW.
To reconstruct the intended meaning the hearer uses the distribution of possible meaningsin the given context P(S|C) and possible words that can express each meaning P(W|S).
Weassume P(W|S) is independent of the context and estimate it using WordNet sense frequencies.The main problem of unsupervised WSD is estimating context-dependent P(S|C) without accessto any sense-tagged text.
We show one way to solve this problem using a statistical languagemodel based on large amounts of untagged text.
Our model uses coarse-grained semantic classesfor S internally and we explore the effect of using different levels of granularity on WSD per-formance.
The system outputs fine-grained senses for evaluation, and its performance on noundisambiguation is better than most previously reported unsupervised systems and close to thebest supervised systems.1.
IntroductionWord sense disambiguation (WSD) is the task of identifying the correct sense of anambiguous word in a given context.
An accurate WSD system would benefit appli-cations such as machine translation and information retrieval.
The most successfulWSD systems to date are based on supervised learning and trained on sense-taggedcorpora.
In this article we present an unsupervised WSD algorithm that can leverageuntagged text and can perform at the level of the best supervised systems for the all-nouns disambiguation task.The main drawback of the supervised approach is the difficulty of acquiringconsiderable amounts of training data, also known as the knowledge acquisitionbottleneck.
Yarowsky and Florian (2002) report that each successive doubling of thetraining data for WSD only leads to a 3?4% error reduction within their experimentalrange.
Banko and Brill (2001) experiment with the problem of selection amongconfusable words and show that the learning curves do not converge even after?
Koc?
University, Department of Computer Engineering, 34450 Sar?yer, I?stanbul, Turkey.E-mail: dyuret@ku.edu.tr, myatbaz@ku.edu.tr.Submission received: 7 October 2008; revised submission received: 17 April 2009; accepted for publication:12 September 2009.?
2010 Association for Computational LinguisticsComputational Linguistics Volume 36, Number 1a billion words of training data.
They suggest unsupervised, semi-supervised, oractive learning to take advantage of large data sets when labeling is expensive.
Yuret(2004) observes that in a supervised naive Bayes WSD system trained on SemCor,approximately half of the test instances do not contain any of the contextual features(e.g., neighboring content words or local collocation patterns) observed in the trainingdata.
SemCor is the largest publicly available corpus of sense-tagged text, and has onlyabout a quarter million sense-tagged words.
In contrast, our unsupervised system usesthe Web1T data set (Brants and Franz 2006) for unlabeled examples, which containscounts from a 1012 word corpus derived from publicly-available Web pages.A note on the term ?unsupervised?
may be appropriate here.
In the WSD literature?unsupervised?
is typically used to describe systems that do not directly use sense-tagged corpora for training.
However, many of these unsupervised systems, includingours, use sense ordering or sense frequencies from WordNet (Fellbaum 1998) or otherdictionaries.
Thus it might be more appropriate to call them weakly supervised orsemi-supervised.
More specifically, context?sense pairs or context?word?sense triplesare not observed in the training data, but context-word frequencies (from untaggedtext) and word-sense frequencies (from dictionaries or other sources) are used in modelbuilding.
One of the main problems we explore in this study is the estimation of context-dependent sense probabilities when no context?sense pairs have been observed in thetraining data.The first contribution of this article is a probabilistic generative model for wordsense disambiguation that seamlessly integrates unlabeled text data into the modelbuilding process.
Our approach is based on the noisy channel model (Shannon 1948),which has been an essential ingredient in fields such as speech recognition and machinetranslation.
In this study we demonstrate that the noisy channel model can also be thekey component for unsupervised word sense disambiguation, provided we can solvethe context-dependent sense distribution problem.
In Section 2.1 we show one wayto estimate the context-dependent sense distribution without using any sense-taggeddata.
Section 2.2 outlines the complete unsupervised WSD algorithm using this model.We estimate the distribution of coarse-grained semantic classes rather than fine-grainedsenses.
The solution uses the two distributions for which we do have data: the distribu-tion of words used to express a given sense, and the distribution of words that appearin a given context.
The first can be estimated using WordNet sense frequencies, and thesecond can be estimated using an n-gram language model as described in Section 2.3.The second contribution of this article is an exploration of semantic classes at differ-ent levels of granularity for word sense disambiguation.
Using fine-grained senses formodel building is inefficient both computationally and from a learning perspective.
Thenoisy channel model can take advantage of the close distribution of similar senses if theyare grouped into semantic classes.
We take semantic classes to be groups of WordNetsynsets defined using the hypernym hierarchy.
In each experiment we designate anumber of synsets high in the WordNet hypernym hierarchy as ?head synsets?
anduse their descendants to partition the senses into separate semantic classes.
In Section 3we present performance bounds for such class-based WSD and describe our method ofexploring the different levels of granularity.In Section 4 we report on our actual experiments and compare our results withthe best supervised and unsupervised systems from SensEval-2 (Cotton et al 2001),SensEval-3 (Mihalcea and Edmonds 2004), and SemEval-2007 (Agirre, Ma`rquez, andWicentowski 2007).
Section 5 discusses these results and the idiosyncrasies of thedata sets, baselines, and evaluation metrics used.
Section 6 presents related work, andSection 7 summarizes our contributions.112Yuret and Yatbaz The Noisy Channel Model for Unsupervised WSD2.
The Noisy Channel Model for WSD2.1 ModelThe noisy channel model has been the foundation of standard models in speech recog-nition (Bahl, Jelinek, and Mercer 1983) and machine translation (Brown et al 1990).In this article we explore its application to WSD.
The noisy channel model can beused whenever a signal received does not uniquely identify the message being sent.Bayes?
Law is used to interpret the ambiguous signal and identify the most probableintended message.
In WSD, we model each context as a distinct channel where theintended message is a word sense (or semantic class) S, and the signal received is anambiguous wordW.
In this section we will describe how to model a given context C asa noisy channel, and in particular how to estimate the context-specific sense distributionwithout using any sense-tagged data.Equation (1) expresses the probability of a sense S of wordW in a given context C.This is the well-known Bayes?
formula with an extra P(.|C) in each term indicating thedependence on the context.P(S|W,C) =P(W|S,C)P(S|C)P(W|C)(1)To perform WSD we need to find the sense S that maximizes the probability P(S|W,C).This is equivalent to the maximization of the product P(W|S,C)P(S|C) because thedenominator P(W|C) does not depend on S. To perform the maximization, the twodistributions P(W|S,C) and P(S|C) need to be estimated for each context C.The main challenge is to estimate P(S|C), the distribution of word senses that canbe expressed in the given context.
In unsupervised WSD we do not have access to anysense-tagged data, thus we do not know what senses are likely to be expressed in anygiven context.
Therefore it is not possible to estimate P(S|C) directly.What we do have is the word frequencies for each sense P(W|S), and the wordfrequencies for the given context P(W|C).
We use the WordNet sense frequencies toestimate P(W|S) and a statistical language model to estimate P(W|C) as detailed inSection 2.3.
We make the independence assumption P(W|S,C) = P(W|S), that is, thedistribution of words used to express a particular sense is the same for all contexts.Finally, the relationship between the three distributions, P(S|C), P(W|S,C), and P(W|C)is given by the total probability theorem:P(W|C) =?SP(S|C)P(W|S,C) (2)We can solve for P(S|C) using linear algebra.
Let WS be a matrix, s and w two vectorssuch that:WSij = P(W = i|S = j)sj = P(S = j|C = k)wi = P(W = i|C = k) (3)113Computational Linguistics Volume 36, Number 1Using this new form, we can see that Equation (2) is equivalent to the linear equationw =WS?
s and s can be solved using a linear solver.
Typically WS is a tall matrix andthe system has no exact solutions.
We use the Moore?Penrose pseudoinverse WS+ tocompute an approximate solution:s =WS+ ?
w (4)Appendix A discusses possible scaling issues of this solution and offers alternativesolutions.
We use the pseudoinverse solution in all our experiments because it can becomputed fast and none of the alternatives we tried made a significant difference inWSD performance.2.2 AlgorithmSection 2.1 described how to apply the noisy channel model for WSD in a single context.In this section we present the steps we follow in our experiments to simultaneouslyapply the noisy channel model to all the contexts in a given word sense disambiguationtask.Algorithm 11.
Let W be the vocabulary.
In this study we took the vocabulary to be theapproximately 12,000 nouns in WordNet that have non-zero sensefrequencies.2.
Let S be the set of senses or semantic classes to be used.
In this study weused various partitions of noun synsets as semantic classes.3.
Let C be the set of contexts (nine-word windows for a 5-gram model)surrounding each target word in the given WSD task.4.
Compute the matrix WC where WCik = P(W = i|C = k).
Here i rangesover the vocabulary W and k ranges over the contexts C. This matrixconcatenates the (w) word distribution vectors from Equation (4) for eachcontext.
The entries of the matrix are computed using the n-gram languagemodel described in Section 2.3.
This is the most expensive step in thealgorithm (see Appendix B for a discussion of implementation efficiency).5.
Compute the matrix WS where WSij = P(W = i|S = j).
Here i ranges overthe vocabulary W and j ranges over the semantic classes S. The entries ofthe matrix are computed using the WordNet sense frequencies.6.
Compute the matrix SC =WS+ ?WC where SCjk = P(S = j|C = k).Here j ranges over the semantic classes S and k ranges over the contexts C.This step computes the pseudoinverse solution described in Section 2.1simultaneously for all the contexts, and the resulting SC matrix is aconcatenation of the (s) solution vectors from Equation (4) for each context.WS+ is the pseudoinverse of the matrix WS.7.
Compute the best semantic class for each WSD instance by usingargmaxSP(S|W,C) ?
P(W|S)P(S|C).
Here P(S|C) comes from the columnof the SC matrix that corresponds to the context of the WSD instance114Yuret and Yatbaz The Noisy Channel Model for Unsupervised WSDand P(W|S) comes from the row of the WS matrix that corresponds to theword to be disambiguated.8.
Compute the fine-grained answer for each WSD instance by taking themost frequent (lowest numbered) sense in the chosen semantic class.9.
Apply the one sense per discourse heuristic: If a word is found to havemultiple senses in a document, replace them with the majority answer.2.3 Estimation ProcedureIn Section 2.1, we showed how the unsupervised WSD problem expressed as a noisychannel model can be decomposed into the estimation of two distributions: P(W|S) andP(W|C).
In this section we detail our estimation procedure for these two distributions.To estimate P(W|S), the distribution of words that can be used to express a givenmeaning, we used the WordNet sense frequencies.1 We did not perform any smoothingfor the zero counts and used the maximum likelihood estimate: count(W,S)/count(S).As described in later sections, we also experimented with grouping similar WordNetsenses into semantic classes.
In this case S stands for the semantic class, and the countsfrom various senses of a word in the same semantic class are added together to estimateP(W|S).To estimate the distribution of words in a given context, P(W|C), we used a 5-gramlanguage model.
We define the context as the nine-word window centered on the targetword w1w2 .
.
.w9, whereW = w5.
The probability of a word in the given context can beexpressed as:P(W = w5) ?
P(w1 .
.
.w9) (5)= P(w1)P(w2|w1) .
.
.
P(w9|w1 .
.
.w8) (6)?
P(w5|w1 .
.
.w4)P(w6|w2 .
.
.w5)P(w7|w3 .
.
.w6) (7)P(w8|w4 .
.
.w7)P(w9|w5 .
.
.w8)Equation (5) indicates that P(W|C) is proportional to P(w1 .
.
.w9) because the otherwords in the context are fixed for a given WSD instance.
Equation (6) is the standarddecomposition of the probability of a word sequence into conditional probabilities.The first four terms do not include the target word w5, and have been dropped in Equa-tion (7).
We also truncate the remaining conditionals to four words reflecting the Markovassumption of the 5-gram model.
Finally, using an expression that is proportional toP(W|C) instead of P(W|C) itself will not change the WSD result because we are takingthe argmax in Equation (1).Each term on the right hand side of Equation (7) is estimated using a 5-gramlanguage model.
To get accurate domain-independent probability estimates we used theWeb 1T data set (Brants and Franz 2006), which contains the counts of word sequencesup to length five in a 1012 word corpus derived from publicly-accessible Web pages.Estimation of P(W|C) is the most computationally expensive step of the algorithm, andsome implementation details are given in Appendix B.1 The sense frequencies were obtained from the index.sense file included in the WordNet distribution.We had to correct the counts of three words (person, group, and location) whose WordNet countsunfortunately include the corresponding named entities and are thus inflated.115Computational Linguistics Volume 36, Number 1Figure 1Upper bound on fine-grained accuracy for a given number of semantic classes.3.
Semantic ClassesOur algorithm internally differentiates semantic classes rather than fine-grained senses.Using fine-grained senses in the noisy channel model would be computationally ex-pensive because the word?sense matrix needs to be inverted (see Equation [4]).
It isalso unclear whether using fine-grained senses for model building will lead to betterlearning performance: The similarity between the distributions of related senses isignored and the data becomes unnecessarily fragmented.Even though we use coarse-grained semantic classes for model building, we usefine-grained senses for evaluation.
During evaluation, the coarse-grained semanticclasses predicted by the model are mapped to fine-grained senses by picking the lowestnumbered WordNet sense in the chosen semantic class.2 This is necessary to perform ameaningful comparison with published results.We take semantic classes to be groups of WordNet synsets defined using the hyper-nym hierarchy (see Section 6 for alternative definitions).
Section 4 presents three WSDexperiments using different sets of semantic classes at different levels of granularity.In each experiment we designate a number of synsets high in the WordNet hypernymhierarchy as ?head synsets?
and use their descendants to form the separate semanticclasses.An arbitrary set of head synsets will not necessarily have mutually exclusive andcollectively exhaustive descendants.
To assign every synset to a unique semantic class,we impose an ordering on the semantic classes.
Each synset is assigned only to the firstsemantic class whose head it is a descendant of according to this ordering.
If there aresynsets that are not descendants of any of the heads, they are collected into a separatesemantic class created for that purpose.Using the coarse-grained semantic classes for prediction, Algorithm 1 will be unableto return the correct fine-grained sense when this is not the lowest numbered sense ina semantic class.
To quantify the restrictive effect of working with a small number ofsemantic classes, Figure 1 plots the number of semantic classes versus the best possible2 The sense numbers are ordered by the frequency of occurrence in WordNet.116Yuret and Yatbaz The Noisy Channel Model for Unsupervised WSDoracle accuracy for the nouns in the SemCor corpus.
To compute the oracle accuracy, weassume that the program can find the correct semantic class for each instance, but has topick the first sense in that class as the answer.
To construct a given number of semanticclasses, we used the following algorithm:Algorithm 21.
Initialize all synsets to be in a single ?default?
semantic class.2.
For each synset, compute the following score: the oracle accuracy achievedif that synset and all its descendants are split into a new semantic class.3.
Take the synset with the highest score and split that synset and itsdescendants into a new semantic class.4.
Repeat steps 2 and 3 until the desired number of semantic classes isachieved.The upper bound on fine-grained accuracy given a small number of semanticclasses is surprisingly high.
In particular, the best reported noun WSD accuracy (78%)is achievable if we could perfectly distinguish between five semantic classes.4.
Three ExperimentsWe ran three experiments with the noisy channel model using different sets of semanticclasses.
The first experiment uses the 25 WordNet semantic categories for nouns, thesecond experiment looks at what happens when we group all the senses to just twoor three semantic classes, and the final experiment optimizes the number of semanticclasses using one data set (which gives 135 classes) and reports the out-of-sample resultusing another data set.The noun instances from the last three SensEval/SemEval English all-words tasksare used for evaluation.
We focus on the disambiguation of nouns for several reasons.Nouns constitute the largest portion of content words (48% of the content words in theBrown corpus [Kucera and Francis 1967] are nouns).
For many tasks and applications(e.g., Web queries [Jansen, Spink, and Pfaff 2000]) nouns are the most frequently encoun-tered and important part of speech.
Finally, WordNet has a more complete coverageof noun semantic relations than other parts of speech, which is important for ourexperiments with semantic classes.As described in Section 2.2 we use the model to assign each ambiguous word to itsmost likely semantic class in all the experiments.
The lowest numbered sense in thatclass is taken as the fine-grained answer.
Finally we apply the one sense per discourseheuristic: If the same word has been assigned more than one sense within the samedocument, we take a majority vote and use sense numbers to break the ties.Table 1 gives some baselines for comparison.
The performance of the best super-vised and unsupervised systems on noun disambiguation for each data set are given.The first-sense baseline (FSB) is obtained by always picking the lowest numbered sensefor the word in the appropriate WordNet version.
We prefer the FSB baseline over thecommonly used most-frequent-sense baseline because the tie breaking is unambiguous.All the results reported are for fine-grained sense disambiguation.
The top three systemsgiven in the table for each task are all supervised systems; the result for the best117Computational Linguistics Volume 36, Number 1Table 1Baselines for the three SensEval English all-words tasks; the WordNet version used (WN);number of noun instances (Nouns); percentage accuracy of the first-sense baseline (FSB); the topthree supervised systems; and the best unsupervised system (Unsup).
The last row gives thetotal score of the best systems on the three tasks.Task WN Nouns FSB 1st 2nd 3rd Unsupsenseval2 1.7 1,067 71.9 78.0 74.5 70.0 61.8senseval3 1.7.1 892 71.0 72.0 71.2 71.0 62.6semeval07 2.1 159 64.2 68.6 66.7 66.7 63.5total 2,118 70.9 74.4 72.5 70.2 62.2unsupervised system is given in the last column.
The reported unsupervised systemsdo use the sense ordering and frequency information from WordNet.4.1 First Experiment: The 25 WordNet CategoriesIn previous work, descendants of 25 special WordNet synsets (known as the uniquebeginners) have been used as the coarse-grained semantic classes for nouns (Crestan,El-Be`ze, and De Loupy 2001; Kohomban and Lee 2005).
These unique beginners wereused to organize the nouns into 25 lexicographer files based on their semantic categoryduring WordNet development.
Figure 2 shows the synsets at the top of the nounhierarchy in WordNet.
The 25 unique beginners have been shaded, and the two graphicsshow how the hierarchy evolved between the two WordNet versions used in this study.Figure 2The top of the WordNet noun hypernym hierarchy for version 1.7 (left) and version 2.1 (right).The 25 WordNet noun categories are shaded.118Yuret and Yatbaz The Noisy Channel Model for Unsupervised WSDTable 2The performance of the noisy channel model with the 25 semantic classes based on WordNetlexicographer files.
The columns give the data set, the percentage of times the model picks thecorrect semantic class, maximum possible fine-grained score if the model had always picked thecorrect class, and the actual score.Data Set CorrClass MaxScore Scoresenseval2 85.1 90.3 77.7senseval3 78.0 88.7 70.1semeval07 75.5 86.2 64.8total 81.4 89.3 73.5We ran our initial experiments using these 25 WordNet categories as semanticclasses.
The distribution of words for each semantic class, P(W|S), is estimated basedon WordNet sense frequencies.
The distribution of words for each context, P(W|C), isestimated using a 5-gram model based on the Web 1T corpus.
The system first finds themost likely semantic class based on the noisy channel model, then picks the first sense inthat class.
Table 2 gives the results for the three data sets, which are significantly higherthan the previously reported unsupervised results.To illustrate which semantic classes are the most difficult to disambiguate, Table 3gives the confusion matrix for the Senseval2 data set.
We can see that frequently occur-ring concrete classes like person and body are disambiguated well.
The largest sourceof errors are the abstract classes like act, attribute, cognition, and communication.
These25 classes may not be the ideal candidates for word sense disambiguation.
Even thoughthey allow a sufficient degree of fine-grained distinction (Table 2 shows that we can getTable 3Confusion matrix for Senseval2 data with the 25 WordNet noun classes.
The rows are actualclasses, the columns are predicted classes.
Column names have been abbreviated to save space.The last two columns give the frequency of the class (F) and the accuracy of the class (A).ac an ar at bo co co ev fe fo gr lo mo ob pe ph po pr qu re sh st su ti F Aact 58 0 4 7 0 7 2 3 2 0 5 0 0 0 0 0 1 4 1 1 0 2 0 0 9.1 59.8animal 0 17 5 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2.1 77.3artifact 0 0 66 2 0 0 6 5 0 0 5 1 0 1 0 0 0 0 0 0 3 1 0 0 8.4 73.3attribute 3 0 0 19 0 3 0 0 0 0 0 1 0 1 2 0 2 1 0 0 1 3 0 0 3.4 52.8body 0 0 0 0 123 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 11.6 99.2cognition 6 0 1 2 0 82 5 1 0 0 0 2 1 1 1 0 1 0 5 1 0 5 0 0 10.7 71.9communicat 2 0 1 0 0 2 29 1 0 0 0 2 5 0 0 1 0 0 0 1 0 0 0 2 4.3 63.0event 0 0 0 0 0 0 0 19 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 2.0 90.5feeling 0 0 0 0 0 0 0 0 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0.4 100.food 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0.1 100.group 0 0 0 2 0 5 0 0 0 0 69 2 0 3 0 0 0 0 0 1 1 0 0 1 7.9 82.1location 0 0 0 1 0 0 0 0 0 0 0 22 0 0 0 0 0 0 0 0 0 0 0 0 2.2 95.7motive 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0.2 50.0object 2 0 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 1 0 0 0 0 1 0.7 14.3person 2 4 0 0 0 1 1 0 0 0 1 0 0 0 168 0 0 0 0 0 0 0 0 0 16.6 94.9phenomenon 1 0 0 1 0 1 0 2 0 0 0 0 0 0 0 3 0 0 0 3 0 0 0 0 1.0 27.3possession 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 4 0 0 0 0 0 0 0 0.4 100.process 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 0 0 12 0 0 0 1 0 0 1.4 80.0quantity 0 0 0 1 0 0 0 1 0 0 1 0 0 0 0 0 0 0 10 0 0 0 0 0 1.2 76.9relation 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0.3 66.7shape 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0.1 100.state 1 0 1 5 0 1 1 2 0 0 1 0 0 0 1 0 0 0 0 0 0 98 0 0 10.4 88.3substance 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 10 0 0.9 100.time 1 0 0 1 0 0 1 1 0 0 1 0 0 0 0 0 0 0 0 0 0 2 0 44 4.8 86.3119Computational Linguistics Volume 36, Number 1Table 4The performance of the noisy channel model with two to three semantic classes.
The columnsgive the data set, the head synsets, the percentage of times the model picks the correct semanticclass, maximum possible fine-grained score if the model had always picked the correct class, andthe actual score.Data Set Heads CorrClass MaxScore Scoresenseval2 entity/default 86.6 76.8 74.9senseval3 entity/default 94.2 75.8 71.2senseval3 object/entity/default 93.8 77.4 72.9semeval07 psychological-feature/default 91.2 74.8 68.685?90% if we could pick the right class every time), they seem too easy to confuse.
In thenext few experiments we will use these observations to design better sets of semanticclasses.4.2 Second Experiment: Distinguishing Mental and Physical ConceptsFigure 1 shows that the upper bound for fine-grained disambiguation is relatively higheven for a very small number of semantic classes.
In our next experiment we look athow well our approach can perform differentiating only two or three semantic classes.We use Algorithm 2 applied to the appropriate version of SemCor to pick the headsynsets used to define the semantic classes.
Figure 2 shows that the top level of thehypernym hierarchy has changed significantly between the WordNet versions.
Thus,different head synsets are chosen for different data sets.
However, the main distinctioncaptured by our semantic classes seems to be between mental and physical concepts.Table 4 gives the results.
The performance with a few semantic classes is comparable tothe top supervised algorithms in each of the three data sets.4.3 Third Experiment: Tuning the Number of ClassesIncreasing the number of semantic classes has two opposite effects on WSD perfor-mance.
The higher the number, the finer distinctions we can make, and the maximumpossible fine-grained accuracy goes up.
However, the more semantic classes we define,the more difficult it becomes to distinguish them from one another.
For an empiricalanalysis of the effect of semantic class granularity on the fine-grained WSD accuracy,we generated different sets of semantic classes using the following algorithm.Algorithm 31.
Sort all the synsets according to their ?subtree frequency?
: i.e., the totalfrequency of each synset?s descendants in the hypernym tree.2.
Take the desired number of synsets with the highest subtree frequency anduse them as head synsets, that is, split their descendants into separatesemantic classes.Figure 3 shows the fine-grained accuracy we achieved on the Senseval2 data setwith up to 600 semantic classes defined based on Algorithm 3.
Note the differences: (i)120Yuret and Yatbaz The Noisy Channel Model for Unsupervised WSDFigure 3The fine-grained accuracy on Senseval2 data set for a given number of semantic classes.Figure 1 gives the best possible oracle accuracy, Figure 3 gives the actual WSD accuracy;(ii) Algorithm 2 chooses the head synsets based on their oracle score, Algorithm 3chooses them based on their subtree frequency.As we suspected, the relationship is not simple or monotonic.
However, one canidentify distinct peaks at 3, 25, and 100?150 semantic classes.
One hypothesis is thatthese peaks correspond to ?natural classes?
at different levels of granularity.
Here aresome example semantic classes from each peak:3 classes entity, abstraction25 classes action, state, content, location, attribute, ...135 classes food, day, container, home, word, business, feeling, material, job, man, ...To test the out-of-sample effect of tuning the semantic classes based on the peaksof Figure 3, we used the SemEval-2007 data set as our test sample.
When the 135semantic classes from the highest peak are used for the disambiguation of the nounsin the SemEval-2007 data set, an accuracy of 69.8% was achieved.
This is higher thanthe accuracy of the best supervised system on this task (68.6%), although the differenceis not statistically significant.5.
DiscussionIn this section we will address several questions raised by the results of the experi-ments.
Why do we get different results from different data sets?
Are the best resultssignificantly different than the first-sense baseline?
Can we improve our results usingbetter semantic classes?5.1 Why Do We Get Different Results from Different Data Sets?Table 5 summarizes our results from the three experiments of Section 4.
There are somesignificant differences between the data sets.121Computational Linguistics Volume 36, Number 1Table 5Result summary for the three data sets.
The columns give the data set, the results of the threeexperiments, best reported result, the first-sense baseline, and the number of instances.Data set Exp1 Exp2 Exp3 Best FSB Instancessenseval2 77.7 74.9 - 78.0 71.9 1,067senseval3 70.1 72.9 - 72.0 71.0 892semeval07 64.8 68.6 69.8 68.6 64.2 159The SemEval-2007 data set appears to be significantly different from the other twowith its generally lower baseline and scores.
The difference in accuracy is probablydue to the difference in data preparation.
In the two Senseval data sets all contentwords were targeted for disambiguation.
In the SemEval-2007 data set only verbs andtheir noun arguments were selected, targeting only about 465 lemmas from about 3,500words of text.
For the Senseval-3 data set none of our results, or any published resultwe know of, is significantly above the baseline for noun disambiguation.
This may bedue to extra noise in the data?the inter-annotator agreement for nouns in this data setwas 74.9%.5.2 Are the Best Results Significantly Different Than the FSB?Among all the published results for these three data sets, our two results for theSenseval-2 data set and the top supervised result for the Senseval-2 data set are theonly ones statistically significantly above the FSB for noun disambiguation at the 95%confidence interval.
This is partly because of the lack of sufficient data.
For example, theSemEval-2007 data set has only 159 nouns; and a result of 71.8% would be needed todemonstrate a difference from the baseline of 64.2% at the 95% confidence interval.More importantly, however, statistical significance should not be confused with?significance?
in general.
A statistically significant difference may not be necessary orsufficient for a significant impact on an application.
Even a WSD system that is statis-tically indistinguishable from the baseline according to the ?total accuracy?
metric ismost probably providing significantly different answers compared to always guessingthe first sense.
There are metrics that can reveal these differences, such as ?balancederror rate?
(i.e., arithmetic average of the error rates for different senses) or ?accuracy indetecting the use of a non-dominant sense.
?Finally, the relatively high first-sense baseline (e.g., 71.0% for Senseval-3 nouns)combined with the relatively low inter-annotator agreement (e.g., 74.9% for Senseval-3 nouns) makes progress in the traditional WSD task difficult.
Annotators who areperfectly proficient in comprehending language nevertheless find it difficult to distin-guish between artificially-created dictionary senses.
If our long term goal is to modelhuman competence in language comprehension, it would make sense to focus on tasksat which humans are naturally competent.
Dictionary-independent tasks such as lexicalsubstitution or textual entailment may be the right steps in this direction.5.3 Can We Improve Our Results Using Better Semantic Classes?In order to get an upper bound for our approach, we searched for the best set of semanticclasses specific to each data set using the following greedy algorithm.122Yuret and Yatbaz The Noisy Channel Model for Unsupervised WSDTable 6The performance of the noisy channel model with the best set of semantic classes picked for eachdata set.
The columns give the data set, the number of classes, maximum possible score if themodel always picks the correct class, percentage of times it actually picks the correct class, andits fine-grained accuracy.Data Set NumClass MaxScore CorrClass Scoresenseval2 23 89.2 88.8 80.1senseval3 29 87.2 87.4 77.4semeval07 12 84.9 89.9 79.2Algorithm 41.
Initialize all synsets to be in a single ?default?
semantic class.2.
For each synset, compute the following score: the WSD accuracy achievedif that synset and all its descendants are split into a new semantic class.3.
Take the synset with the highest score and split that synset and itsdescendants into a new semantic class.4.
Repeat steps 2 and 3 until the WSD accuracy can no longer be improved.Algorithm 4 was run for each of the three data sets, which resulted in three differentsets of semantic classes.
The noisy channel model was applied with the best set ofsemantic classes for each data set.
Table 6 summarizes the results.
Note that these resultsare not predictive of out-of-sample accuracy because Algorithm 4 picks a specific set ofsemantic classes optimal for a given data set.
But the results do indicate that a betterset of semantic classes may lead to significantly better WSD accuracy.
In particulareach result in Table 6 is significantly higher than previously reported supervised orunsupervised results.How to construct a good set of semantic classes that balance specificity and identi-fiability is a topic of ongoing research.
See Kohomban and Lee (2007) for a supervisedsolution using feature-based clustering that tries to maintain feature?class coherence.Non-parametric Bayesian approaches such as Teh et al (2006) applied to context distri-butions could reveal latent senses in an unsupervised setting.6.
Related WorkFor a general overview of different approaches to WSD, see Navigli (2009) andStevenson (2003).
The Senseval and SemEval workshops (Cotton et al 2001; Mihalceaand Edmonds 2004; Agirre, Ma`rquez, and Wicentowski 2007) are good sources of recentwork, and have been used in this article to benchmark our results.Generative models based on the noisy channel framework have previously beenused for speech recognition (Bahl, Jelinek, and Mercer 1983), machine translation(Brown et al 1990), question answering (Echihabi and Marcu 2003), spelling correction(Brill and Moore 2000), and document compression (Daume III and Marcu 2002) amongothers.
To our knowledge our work is the first application of the noisy channel modelto unsupervised word sense disambiguation.123Computational Linguistics Volume 36, Number 1Using statistical language models based on large corpora for WSD has been ex-plored in Yuret (2007) and Hawker (2007).
For specific modeling techniques used in thisarticle see Yuret (2008); for a more general review of statistical language modeling seeChen and Goodman (1999), Rosenfeld (2000), and Goodman (2001).Grouping similar senses into semantic classes for WSD has been explored in previ-ous work.
Senses that are similar have been identified using WordNet relations (Peters,Peters, and Vossen 1998; Crestan, El-Be`ze, and De Loupy 2001; Kohomban and Lee2005), discourse domains (Magnini et al 2003), annotator disagreements (Chklovskiand Mihalcea 2003), and other lexical resources such as Roget (Yarowsky 1992), LDOCE(Dolan 1994), and ODE (Navigli 2006).Ciaramita and Altun (2006) build a supervised HMM tagger using ?supersenses,?essentially the 25 WordNet noun categories we have used in our first experiment inaddition to 15 verb categories similarly defined.
They report a supersense precision of67.60 for nouns and verbs of Senseval-3.
Table 2 gives our supersense score as 78% forSenseval-3 nouns.
However, the results are not directly comparable because they do notreport the noun and verb scores separately or calculate the corresponding fine-grainedscore to compare with other Senseval-3 results.Kohomban and Lee (2007) go beyond the WordNet categories based on lexicogra-pher files and experiment with clustering techniques to construct their semantic classes.Their classes are based on local features from sense-labeled data and optimize feature?class coherence rather than adhering to the WordNet hierarchy.
Their supervised systemachieves an accuracy of 74.7% on Senseval-2 nouns and 73.6% on Senseval-3 nouns.The systems mentioned so far are supervised WSD systems.
Agirre and Martinez(2004) explore the large-scale acquisition of sense-tagged examples from the Web andtrain supervised, minimally supervised (requiring sense bias information from hand-tagged corpora, similar to our system), and fully unsupervised WSD algorithms usingthis corpus.
They report good results on the Senseval-2 lexical sample data compared toother unsupervised systems.
Martinez, de Lacalle, and Agirre (2008) test a similar set ofsystems trained using automatically acquired corpora on Senseval-3 nouns.
Their mini-mally supervised system obtains 63.9% accuracy on polysemous nouns from Senseval-3(corresponding to 71.86% on all nouns).7.
ContributionsWe have introduced a new generative probabilistic model based on the noisy channelframework for unsupervised word sense disambiguation.
The main contribution of thismodel is the reduction of the word sense disambiguation problem to the estimation oftwo distributions: the distribution of words used to express a given sense, and the dis-tribution of words that appear in a given context.
In this framework, context similarityis determined by the distribution of words that can be placed in the given context.
Thisreplaces the ad hoc contextual feature design process by a statistical language model,allowing the advances in language modeling and the availability of large unlabeledcorpora to have a direct impact on WSD performance.We have provided a detailed analysis of using coarse-grained semantic classes forfine-grained WSD.
The noisy channel model is a good fit for class-based WSD, wherethe model decides on a coarse-grained semantic class instead of a fine-grained sense.The chosen semantic class is then mapped to a specific sense based on the WordNetordering during evaluation.
We show that the potential loss from using coarse-grainedclasses is limited, and state-of-the-art performance is possible using only a few semanticclasses.
We explore semantic classes at various levels of granularity and show that124Yuret and Yatbaz The Noisy Channel Model for Unsupervised WSDthe relationship between granularity and fine-grained accuracy is complex, thus morework is needed to determine an ideal set of semantic classes.In several experiments we compare the performance of our unsupervised WSDsystem with the best systems from previous Senseval and SemEval workshops.
Weconsistently outperform any previously reported unsupervised results and achievecomparable performance to the best supervised results.Appendix A: Solutions for P(S|C)To solve for P(S|C) using P(W|C) and P(W|S), we represent the first two as vectors: sj =P(S = j|C = k) and wi = P(W = i|C = k), and the last one as a matrix: WSij = P(W =i|S = j).
Our problem becomes finding a solution to the linear equation w =WS?
s.Using the Moore?Penrose pseudoinverse, WS+, we find a solution s =WS+ ?
w. Thissolution minimizes the distance |WS?
s?
w|.
There are two potential problems withthis pseudoinverse solution.
First, it may violate the non-negativity and normalizationconstraints of a probability distribution.
Second, a maximum likelihood estimate shouldminimize the cross entropy between WS?
s and w, not the Euclidean distance.
Weaddressed the normalization problem using a constrained linear solver and the cross-entropy problem using numerical optimization.
However, our experiments showed thedifference in WSD performance to be less than 1% in each case.
The pseudoinversesolution, s =WS+ ?
w, can be computed quickly and works well in practice, so thisis the solution that is used in all our experiments.Appendix B: Estimating P(W|C)Estimating P(W|C) for each context is expensive because the number of words that needto be considered is large.
The Web 1T data set contains 13.5 million unique words, andWordNet defines about 150,000 lemmas.
To make the computation feasible we neededto limit the set of words for which P(W|C) needs to be estimated.
We limited our set toWordNet lemmas with the same part of speech as the target word.
We further requiredthe word to have a non-zero count in WordNet sense frequencies.
The inflection andcapitalization of each word W was automatically matched to the target word.
As aresult, we estimated P(W|C) for about 10,000 words for each noun context and assumedthe other words had zero probability.
The n-grams required for all the contexts werelisted, and their counts were extracted from the Web 1T data set in one pass.
The P(W|C)was estimated for all the words and contexts based on these counts.
In the end, we onlyused the 100 most likely words in each context for efficiency, as the difference in resultsusing the whole distribution was not significant.
For more details on smoothing with alarge language model see Yuret (2008), although we did not see a significant differencein WSD performance based on the smoothing method used.AcknowledgmentsThis work was supported in part bythe Scientific and Technical ResearchCouncil of Turkey (TU?BI?TAK Project108E228).
We would like to thank PeterTurney, Rada Mihalcea, Diana McCarthy,and the four anonymous reviewersfor their helpful comments and suggestions.ReferencesAgirre, E. and D. Martinez.
2004.Unsupervised WSD based onautomatically retrieved examples:The importance of bias.
In Proceedingsof the Conference on Empirical Methodsin Natural Language Processing (EMNLP),pages 25?32, Barcelona.125Computational Linguistics Volume 36, Number 1Agirre, Eneko, Llu?
?s Ma`rquez, and RichardWicentowski, editors.
2007.
Proceedingsof the Fourth International Workshop onSemantic Evaluations (SemEval-2007),Prague.Bahl, Lalit R., Frederick Jelinek, and Robert L.Mercer.
1983.
A maximum likelihoodapproach to continuous speechrecognition.
IEEE Transactions on PatternAnalysis and Machine Intelligence,5(2):179?190.Banko, Michele and Eric Brill.
2001.Scaling to very very large corporafor natural language disambiguation.In Proceedings of 39th Annual Meeting ofthe Association for Computational Linguistics,pages 26?33, Toulouse, France, July.Association for Computational Linguistics.Brants, Thorsten and Alex Franz.
2006.
Web1T 5-gram version 1.
Linguistic DataConsortium, Philadelphia.
LDC2006T13.Brill, Eric and Robert C. Moore.
2000.
Animproved error model for noisy channelspelling correction.
In Proceedings of the 38thAnnual Meeting of the Association forComputational Linguistics, pages 286?293,Hong Kong.Brown, Peter F., John Cocke, Stephen A.Della Pietra, Vincent J. Della Pietra,Frederick Jelinek, John D. Lafferty,Robert L. Mercer, and Paul S. Roossin.1990.
A statistical approach to machinetranslation.
Computational Linguistics,16(2):79?85.Chen, S. F. and J. Goodman.
1999.
Anempirical study of smoothing techniquesfor language modeling.
ComputerSpeech and Language, 13(4):359?394.Chklovski, Timothy and Rada Mihalcea.
2003.Exploiting agreement and disagreementof human annotators for word sensedisambiguation.
In Proceedings of theConference on Recent Advances in NaturalLanguage Processing, pages 357?366,Borovetz.Ciaramita, Massimiliano andYasemin Altun.
2006.
Broad-coveragesense disambiguation and informationextraction with a supersense sequencetagger.
In Proceedings of the 2006Conference on Empirical Methods inNatural Language Processing,pages 594?602, Sydney.Cotton, Scott, Phil Edmonds,Adam Kilgarriff, and Martha Palmer,editors.
2001.
SENSEVAL-2: SecondInternational Workshop on Evaluating WordSense Disambiguation Systems, Toulouse,France.Crestan, E., M. El-Be`ze, and C. De Loupy.2001.
Improving WSD with multi-levelview of context monitored by similaritymeasure.
In Proceedings of SENSEVAL-2:Second International Workshop on EvaluatingWord Sense Disambiguation Systems,Toulouse.Daume III, Hal and Daniel Marcu.
2002.A noisy-channel model for documentcompression.
In Proceedings of 40thAnnual Meeting of the Association forComputational Linguistics, pages 449?456,Philadelphia, PA.Dolan, W. B.
1994.
Word sense ambiguation:clustering related senses.
In Proceedingsof the 15th conference on ComputationalLinguistics, pages 05?09, Kyoto.Echihabi, Abdessamad and Daniel Marcu.2003.
A noisy-channel approach toquestion answering.
In Proceedings of the41st Annual Meeting of the Association forComputational Linguistics, pages 16?23,Sapporo.Fellbaum, Christiane, editor.
1998.Wordnet:An Electronic Lexical Database.
MIT Press,Cambridge, MA.Goodman, Joshua.
2001.
A bit of progress inlanguage modeling.
Computer Speech andLanguage, 15:403?434.Hawker, Tobias.
2007.
Usyd: WSD and lexicalsubstitution using the Web1t corpus.
InProceedings of the Fourth InternationalWorkshop on Semantic Evaluations(SemEval-2007), pages 446?453,Prague.Jansen, B. J., A. Spink, and A. Pfaff.
2000.Linguistic aspects of Web queries.
InProceedings of the ASIS Annual Meeting,pages 169?176, Chicago, IL.Kohomban, Upali Sathyajith andWee Sun Lee.
2005.
Learning semanticclasses for word sense disambiguation.
InProceedings of the 43rd Annual Meeting of theAssociation for Computational Linguistics(ACL?05), pages 34?41, Ann Arbor, MI.Kohomban, Upali Sathyajith andWee Sun Lee.
2007.
Optimizing classifierperformance in word sense disambiguationby redefining word sense classes.
InProceedings of the International JointConference on Artificial Intelligence,pages 1635?1640, Hyderabad.Kucera, Henry and W. Nelson Francis.
1967.Computational Analysis of Present-DayAmerican English.
Brown University Press,Providence, RI.Magnini, B., C. Strapparava, G. Pezzulo, andA.
Gliozzo.
2003.
The role of domaininformation in word sense disambiguation.126Yuret and Yatbaz The Noisy Channel Model for Unsupervised WSDNatural Language Engineering,8(04):359?373.Martinez, D., O. Lopez de Lacalle, andE.
Agirre.
2008.
On the use of automaticallyacquired examples for all-nounsword sense disambiguation.
Journalof Artificial Intelligence Research, 33:79?107.Mihalcea, Rada and Phil Edmonds, editors.2004.
SENSEVAL-3: The Third InternationalWorkshop on the Evaluation of Systemsfor the Semantic Analysis of Text, Barcelona.Navigli, Roberto.
2006.
Meaningful clusteringof senses helps boost word sensedisambiguation performance.
InProceedings of the 21st InternationalConference on Computational Linguisticsand 44th Annual Meeting of theAssociation for Computational Linguistics,pages 105?112, Sydney.Navigli, Roberto.
2009.
Word sensedisambiguation: A survey.
ACMComputing Surveys, 41(2):1?69.Peters, W., I. Peters, and P. Vossen.
1998.Automatic sense clustering inEuroWordNet.
In Proceedings of theInternational Conference on LanguageResources and Evaluation, pages 409?416,Granada.Rosenfeld, Ronald.
2000.
Two decadesof statistical language modeling:Where do we go from here?
Proceedingsof the IEEE, 88:1270?1278.Shannon, Claude Elwood.
1948.
Amathematical theory of communication.The Bell System Technical Journal,27:379?423, 623?656.Stevenson, Mark.
2003.Word SenseDisambiguation: The Case for Combinationsof Knowledge Sources.
CSLI, Stanford, CA.Teh, Y. W., M. I. Jordan, M. J. Beal, andD.
M. Blei.
2006.
Hierarchical Dirichletprocesses.
Journal of the American StatisticalAssociation, 101(476):1566?1581.Yarowsky, David.
1992.
Word sensedisambiguation using statisticalmodels of Roget?s categories trainedon large corpora.
In Proceedings of the 15thInternational Conference on ComputationalLinguistics, pages 454?460, Nantes.Yarowsky, David and Radu Florian.
2002.Evaluating sense disambiguation acrossdiverse parameter spaces.Natural LanguageEngineering, 8(4):293?310.Yuret, Deniz.
2004.
Some experimentswith a Naive Bayes WSD system.
InSenseval-3: Third International Workshopon the Evaluation of Systems for theSemantic Analysis of Text, pages 265?268,Barcelona.Yuret, Deniz.
2007.
KU: Word sensedisambiguation by substitution.
InProceedings of the Fourth InternationalWorkshop on Semantic Evaluations(SemEval-2007), pages 207?214, Prague.Yuret, Deniz.
2008.
Smoothing a tera-wordlanguage model.
In Proceedings ofACL-08: HLT, Short Papers,pages 141?144, Columbus, OH.127
