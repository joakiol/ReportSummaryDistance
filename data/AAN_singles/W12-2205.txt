NAACL-HLT 2012 Workshop on Predicting and Improving Text Readability for target reader populations (PITR 2012)., pages 33?39,Montre?al, Canada, June 7, 2012. c?2012 Association for Computational LinguisticsBuilding Readability Lexicons with Unannotated CorporaJulian Brooke* Vivian Tsang?
David Jacob?
Fraser Shein*?
Graeme Hirst**Department of Computer ScienceUniversity of Toronto{jbrooke,gh}@cs.toronto.edu?Quillsoft Ltd.Toronto, Canada{vtsang, djacob, fshein}@quillsoft.caAbstractLexicons of word difficulty are useful for var-ious educational applications, including read-ability classification and text simplification.
Inthis work, we explore automatic creation ofthese lexicons using methods which go beyondsimple term frequency, but without relying onage-graded texts.
In particular, we derive infor-mation for each word type from the readabilityof the web documents they appear in and thewords they co-occur with, linearly combiningthese various features.
We show the efficacy ofthis approach by comparing our lexicon with anexisting coarse-grained, low-coverage resourceand a new crowdsourced annotation.1 IntroductionWith its goal of identifying documents appropriateto readers of various proficiencies, automatic anal-ysis of readability is typically approached as a text-level classification task.
Although at least one pop-ular readability metric (Dale and Chall, 1995) anda number of machine learning approaches to read-ability rely on lexical features (Si and Callan, 2001;Collins-Thompson and Callan, 2005; Heilman et al,2007; Petersen and Ostendorf, 2009; Tanaka-Ishii etal., 2010), the readability of individual lexical itemsis not addressed directly in these approaches.
Nev-ertheless, information about the difficulty of individ-ual lexical items, in addition to being useful for textreadability classification (Kidwell et al, 2009), canbe applied to other tasks, for instance lexical simpli-fication (Carroll et al, 1999; Burstein et al, 2007).Our interest is in providing students with educa-tional software that is sensitive to the difficulty ofparticular English expressions, providing proactivesupport for those which are likely to be outside areader?s vocabulary.
However, our existing lexicalresource is coarse-grained and lacks coverage.
Inthis paper, we explore the extent to which an auto-matic approach could be used to fill in the gaps ofour lexicon.
Prior approaches have generally de-pended on some kind of age-graded corpus (Kid-well et al, 2009; Li and Feng, 2011), but this kindof resource is unlikely to provide the coverage thatwe require; instead, our methods here are based onstatistics from a huge web corpus.
We show thatfrequency, an obvious proxy for difficulty, is onlythe first step; in fact we can derive key informationfrom the documents that words appear in and thewords that they appear with, information that can becombined to give high performance in identifyingrelative difficulty.
We compare our automated lexi-con against our existing resource as well as a crowd-sourced annotation.2 Related WorkSimple metrics form the basis of much readabilitywork: most involve linear combinations of wordlength, syllable count, and sentence length (Kincaidet al, 1975; Gunning, 1952), though the popularDale-Chall reading score (Dale and Chall, 1995) isbased on a list of 3000 ?easy?
words; a recent re-view suggests these metrics are fairly interchange-able (van Oosten et al, 2010).
In machine-learningclassification of texts by grade level, unigrams havebeen found to be reasonably effective for this task,outperforming readability metrics (Si and Callan,2001; Collins-Thompson and Callan, 2005).
Var-33ious other features have been explored, includingparse (Petersen and Ostendorf, 2009) and coherencefeatures (Feng et al, 2009), but the consensus seemsto be that lexical features are the most consistentlyuseful for automatic readability classification, evenwhen considering non-native readers (Heilman etal., 2007).In the field of readability, the work of Kidwell etal.
(2009) is perhaps closest to ours.
Like the above,their goal is text readability classification, but theyproceed by first deriving an age of acquisition foreach word based on its statistical distribution in age-annotated texts.
Also similar is the work of Li andFeng (2011), who are critical of raw frequency as anindicator and instead identify core vocabulary basedon the common use of words across different agegroups.
With respect to our goal of lowering relianceon fine-grained annotation, the work of Tanaka-Ishiiet al (2010) is also relevant; they create a readabilitysystem that requires only two general classes of text(easy and difficult), other texts are ranked relative tothese two classes using regression.Other lexical acquisition work has also informedour approach here.
For instance, our co-occurrencemethod is an adaption of a technique applied insentiment analysis (Turney and Littman, 2003),which has recently been shown to work for formal-ity (Brooke et al, 2010), a dimension of stylisticvariation that seems closely related to readability.Taboada et al (2011) validate their sentiment lex-icon using crowdsourced judgments of the relativepolarity of pairs of words, and in fact crowd sourcinghas been applied directly to the creation of emotionlexicons (Mohammad and Turney, 2010).3 ResourcesOur primary resource is an existing lexicon, pre-viously built under the supervision of the one ofauthors.
This resource, which we will refer toas the Difficulty lexicon, consists of 15,308 wordsand expressions classified into three difficulty cate-gories: beginner, intermediate, and advanced.
Be-ginner, which was intended to capture the vocabu-lary of early elementary school, is an amalgamationof various smaller sources, including the Dolch list(Dolch, 1948).
The intermediate words, which in-clude words learned in late elementary and middleTable 1: Examples from the Difficulty lexiconBeginnercoat, away, arrow, lizard, afternoon, rainy,carpet, earn, hear, chillIntermediatebale, campground, motto, intestine, survey,regularly, research, conflictAdvancedcontingency, scoff, characteristic, potent, myriad,detracted, illegitimate, overtureschool, were extracted from Internet-published textswritten by students at these grade levels, and then fil-tered manually.
The advanced words began as a listof common words that were in neither of the origi-nal two lists, but they have also been manually fil-tered; they are intended to reflect the vocabulary un-derstood by the average high school student.
Table1 contains some examples from each list.For our purposes here, we only use a subset of theDifficulty lexicon: we filtered out inflected forms,proper nouns, and words with non-alphabetic com-ponents (including multiword expressions) and thenrandomly selected 500 words from each level forour test set and 300 different words for our develop-ment/training set.
Rather than trying to duplicate ourarbitrary three-way distinction by manual or crowd-sourced means, we instead focused on the relativedifficulty of individual words: for each word in eachof the two sets, we randomly selected three compar-ison words, one from each of the difficulty levels,forming a set of 4500 test pairs (2700 for the de-velopment set): 1/3 of these pairs are words fromthe same difficulty level, 4/9 are from adjacent dif-ficulty levels, and the remaining 2/9 are at oppositeends of our difficulty spectrum.Our crowdsourced annotation was obtained usingCrowdflower, which is an interface built on top ofMechanical Turk.
For each word pair to be com-pared, we elicited 5 judgments from workers.
Ratherthan frame the question in terms of difficulty or read-ability, which we felt was too subjective, we insteadasked which of the two words the worker thoughthe or she learned first: the worker could choose ei-ther word, or answer ?about the same time?.
They34were instructed to choose the word they did know ifone of the two words was unknown, and ?same?
ifboth were unknown.
For our evaluation, we took themajority judgment as the gold standard; when therewas no majority judgment, then the words were con-sidered ?the same?.
To increase the likelihood thatour workers were native speakers of English, werequired that the responses come from the US orCanada.
Before running our main set, we ran sev-eral smaller test runs and manually inspected themfor quality; although there were outliers, the major-ity of the judgments seemed reasonable.Our corpus is the ICWSM Spinn3r 2009 dataset(Burton et al, 2009).
We chose this corpus becauseit was used by Brooke et al (2010) to derive a lexi-con of formality; they found that it was more effec-tive for these purposes than smaller mixed-registercorpora like the BNC.
The ICWSM 2009, collectedover several weeks in 2008, contains about 7.5 mil-lion blogs, or 1.3 billion tokens, including well overa million word types (more than 200,000 of whichwhich appear at least 10 times).
We use only thedocuments which have at least 100 tokens.
The cor-pus has been tagged using the TreeTagger (Schmid,1995).4 Automatic Lexicon CreationOur method for lexicon creation involves first ex-tracting a set of relevant numerical features for eachword type.
We can consider each feature as defin-ing a lexicon on its own, which can be evaluated us-ing our test set.
Our features can be roughly brokeninto three types: simple features, document readabil-ity features, and co-occurrence features.
The first ofthese types does not require much explanation: it in-cludes the length of the word, measured in terms ofletters and syllables (the latter is derived using a sim-ple but reasonably accurate vowel-consonant heuris-tic), and the log frequency count in our corpus.1The second feature type involves calculating sim-ple readability metrics for each document in our cor-pus, and then defining the relevant feature for theword type as the average value of the metric for allthe documents that the word appears in.
For exam-1Though it is irrelevant when evaluating the feature alone,the log frequency was noticeably better when combining fre-quency with other features.ple, if Dw is the set of documents where word typew appears and di is the ith word in a document d,then the document word length (DWL) for w can bedefined as follows:DWL(w) = |Dw|?1 ?d?Dw?|d|i=0 length(di)|d|Other features calculated in this way include: thedocument sentence length, that is the average tokenlength of sentences; the document type-token ratio2;and the document lexical density, the ratio of contentwords (nouns, verbs, adjectives, and adverbs) to allwords.The co-occurence features are inspired by thesemi-supervised polarity lexicon creation method ofTurney and Littman (2003).
The first step is to builda matrix consisting of each word type and the docu-ments it appears in; here, we use a binary representa-tion, since the frequency with which a word appearsin a particular document does not seem directly rel-evant to readability.
We also do not remove tradi-tional stopwords, since we believe that the use ofcertain common function words can in fact be goodindicators of text readability.
Once the matrix isbuilt, we apply latent semantic analysis (Landauerand Dumais, 1997); we omit the mathematical de-tails here, but the result is a dimensionality reduc-tion such that each word is represented as a vectorof some k dimensions.
Next, we select two sets ofseed words (P and N) which will represent the endsof the spectrum which we are interested in deriving.We derive a feature value V for each word by sum-ming the cosine similarity of the word vector withall the seeds:V (w) =?p?P cos(?(w,p))|P|?
?n?N cos(?
(w,n))|N|We further normalize this to a range of 1 to?1, centered around the core vocabulary word and.Here, we try three possible versions of P and N: thefirst, Formality, is the set of words used by Brookeet al (2010) in their study of formality, that is, a2We calculate this using only the first 100 words of the docu-ment, to avoid the well-documented influence of length on TTR.35set of slang and other markers of oral communica-tion as N, and a set of formal discourse markers andadverbs as P, with about 100 of each.
The second,Childish, is a set of 10 common ?childish?
concretewords (e.g.
mommy, puppy) as N, and a set of 10common abstract words (e.g.
concept, philosophy)as P. The third, Difficulty, consists of the 300 begin-ner words from our development set as N, and the300 advanced words from our development set as P.We tested several values of k for each of the seedsets (from 20 to 500); there was only small variationso here we just present our best results for each setas determined by testing in the development set.Our final lexicon is created by taking a linearcombination of the various features.
We can find anappropriate weighting of each term by taking themfrom a model built using our development set.
Wetest two versions of this: by default, we use a linearregression model where for training beginner wordsare tagged as 0, advanced words as 1, and intermedi-ate words as 0.5.
The second model is a binary SVMclassifier; the features of the model are the differ-ence between the respective features for each of thetwo words, and the classifier predicts whether thefirst or second word is more difficult.
Both modelswere built using WEKA (Witten and Frank, 2005),with default settings except for feature normaliza-tion, which must be disabled in the SVM to get use-ful weights for the linear combination which createsour lexicon.
In practice, we would further normalizeour lexicon; here, however, this normalization is notrelevant since our evaluation is based entirely on rel-ative judgments.
We also tested a range of other ma-chine learning algorithms available in WEKA (e.g.decision trees and MaxEnt) but the crossvalidatedaccuracy was similar to or slightly lower than usinga linear classifier.5 EvaluationAll results are based on comparing the relative dif-ficulty judgments made for the word pairs in ourtest set (or, more often, some subset) by the varioussources.
Since even the existing Difficulty lexicon isnot entirely reliable, we report agreement rather thanaccuracy.
Except for agreement of Crowdflowerworkers, agreement is the percentage of pairs wherethe sources agreed as compared to the total num-ber of pairs.
For agreement between Crowdflowerworkers, we follow Taboada et al (2011) in calcu-lating agreement across all possible pairings of eachworker for each pair.
Although we considered usinga more complex metric such as Kappa, we believethat simple pairwise agreement is in fact equally in-terpretable when the main interest is relative agree-ment of various methods; besides, Kappa is intendedfor use with individual annotators with particular bi-ases, an assumption which does not hold here.To evaluate the reliability of our human-annotatedresources, we look first at the agreement within theCrowdflower data, and between the Crowdflowerand our Difficulty lexicon, with particular attentionto within-class judgments.
We then compare thepredictions of various automatically extracted fea-tures and feature combinations with these humanjudgments; since most of these involve a continuousscale, we focus only on words which were judged tobe different.3 For the Difficulty lexicon (Diff.
), then in this comparison is 3000, while for the Crowd-flower (CF) judgments it is 4002.6 ResultsWe expect a certain amount of noise using crowd-sourced data, and indeed agreement among Crowd-flower workers was not extremely high, only 56.6%for a three-way choice; note, however, that in thesecircumstances a single worker disagreeing with therest will drop pairwise agreement in that judgementto 60%.4 Tellingly, average agreement was rela-tively high (72.5%) for words on the extremes of ourdifficulty spectrum, and low for words in the samedifficulty category (46.0%), which is what we wouldexpect.
As noted by Taboada et al (2011), whenfaced with a pairwise comparison task, workers tendto avoid the ?same?
option; instead, the proximity ofthe words on the underlying spectrum is reflected indisagreement.
When we compare the crowdsourcedjudgements directly to the Difficulty lexicon, base3A continuous scale will nearly always predict some differ-ence between two words.
An obvious approach would be to seta threshold within which two words will be judged the same,but the specific values depend greatly on the scale and for sim-plicity we do not address this problem here.4In 87.3% of cases, at least 3 workers agreed; in 56.2% ofcases, 4 workers agreed, and in 23.1% of cases all 5 workersagreed.36agreement is 63.1%.
This is much higher thanchance, but lower than we would like, consideringthese are two human-annotated sources.
However,it is clear that much of this disagreement is due to?same?
judgments, which are three times more com-mon in the Difficulty lexicon-based judgments thanin the Crowdflower judgments (even when disagree-ment is interpreted as a ?same?
judgment).
Pairwiseagreement of non-?same?
judgments for word pairswhich are in the same category in the Difficultly lex-icon is high enough (45.9%)5 for us to conclude thatthis is not random variation, strongly suggesting thatthere are important distinctions within our difficultycategories, i.e.
that it is not sufficiently fine-grained.If we disregard all words that are judged as same inone (or both) of the two sources, the agreement ofthe resulting word pairs is 91.0%, which is reason-ably high.Table 2 contains the agreement when feature val-ues or a linear combination of feature values are usedto predict the readability of the unequal pairs fromthe two manual sources.
First, we notice that theCrowdflower set is obviously more difficult, proba-bly because it contains more pairs with fairly subtle(though noticeable) distinctions.
Other clear differ-ences between the annotations: whereas for Crowd-flower frequency is the key indicator, this is not truefor our original annotation, which prefers the morecomplex features we have introduced here.
A fewfeatures did poorly in general: syllable count ap-pears too coarse-grained to be useful on its own,lexical density is only just better than chance, andtype-token ratio performs at or below chance.
Oth-erwise, many of the features within our major typesgive roughly the same performance individually.When we combine features, we find that simpleand document features combine to positive effect,but the co-occurrence features are redundant witheach other and, for the most part, the document fea-tures.
A major boost comes, however, from combin-ing either document or co-occurrence features withthe simple features; this is especially true for ourDifficulty lexicon annotation, where the gain is 7%to 8 percentage points.
It does not seem to mattervery much whether the weights of each feature aredetermined by pairwise classifier or by linear regres-5Random agreement here is 33.3%.Table 2: Agreement (%) of automated methods with man-ual resources on pairwise comparison task (Diff.
= Diffi-culty lexicon, CF = Crowdflower)FeaturesResourceDiff.
CFSimpleSyllable length 62.5 54.9Word length 68.8 62.4Term frequency 69.2 70.7DocumentAvg.
word length 74.5 66.8Avg.
sentence length 73.5 65.9Avg.
type-token ratio 47.0 50.0Avg.
lexical density 56.1 54.7Co-occurrenceFormality 74.7 66.5Childish 74.2 65.5Difficulty 75.7 66.1Linear CombinationsSimple 79.3 75.0Document 80.1 70.8Co-occurrence 76.0 67.0Document+Co-occurrence 80.4 70.2Simple+Document 87.5 79.1Simple+Co-occurrence 86.7 78.2All 87.6 79.5All (SVM) 87.1 79.2sion: this is interesting because it means we can traina model to create a readability spectrum with onlypairwise judgments.
Finally, we took all the 2500instances where our two annotations agreed that oneword was more difficult, and tested our best modelagainst only those pairs.
Results using this selec-tive test set were, unsurprisingly, higher than thoseof either of the annotations alone: 91.2%, which isroughly the same as the original agreement betweenthe two manual annotations.7 DiscussionWord difficulty is a vague concept, and we have ad-mittedly sidestepped a proper definition here: in-stead, we hope to establish a measure of reliabil-ity in judgments of ?lexical readability?
by lookingfor agreement across diverse sources of informa-tion.
Our comparison of our existing resources with37crowdsourced judgments suggests that some consis-tency is possible, but that granularity is, as we pre-dicted, a serious concern, one which ultimately un-dermines our validation to some degree.
An auto-matically derived lexicon, which can be fully con-tinuous or as coarse-grained as needed, seems likean ideal solution, though the much lower perfor-mance of the automatic lexicon in predicting themore fine-grained Crowdflower judgments indicatesthat automatically-derived features are limited intheir ability to deal with subtle differences.
How-ever, a visual inspection of the spectrum created bythe automatic methods suggests that, with a judi-cious choice of granularity, it should be sufficient forour needs.
In future work, we also intend to evalu-ate its use for readability classification, and perhapsexpand it to include multiword expressions and syn-tactic patterns.Our results clearly show the benefit of combin-ing multiple sources of information to build a modelof word difficulty.
Word frequency and word lengthare of course relevant, and the utility of the docu-ment context features is not surprising, since theyare merely a novel extension of existing proxiesfor readability.
The co-occurrence features werealso useful, though they seem fairly redundant andslightly inferior to document features; we posit thatthese features, in addition to capturing notions ofregister such as formality, may also offer seman-tic distinctions relevant to the acquisition process.For instance, children may have a large vocabularyin very concrete domains such as animals, includ-ing words (e.g.
lizard) that are not particularly fre-quent in adult corpora, while very common words inother domains (such as the legal domain) are com-pletely outside the range of their experience.
If welook at some of the examples which term frequencyalone does not predict, they seem to be very muchof this sort: dollhouse/emergence, skirt/industry,magic/system.
Unsupervised techniques for identi-fying semantic variation, such as LSA, can capturethese sorts of distinctions.
However, our results indi-cate that simply looking at the readability of the textsthat these sort of words appear in (i.e.
our documentfeatures) is mostly sufficient, and less than 10% ofthe pairs which are correctly ordered by these twofeature sets are different.
In any case, an age-gradedcorpus is definitely not required.There are a few other benefits of using word co-occurrence that we would like to touch on, thoughwe leave a full exploration for future work.
First, ifwe consider readability in other languages, each lan-guage may have different properties which renderproxies such as word length much less useful (e.g.ideographic languages like Chinese or agglutinativelanguages like Turkish).
However, word (or lemma)co-occurrence, like frequency, is essentially a uni-versal feature across languages, and thus can be di-rectly extended to any language.
Second, if we con-sider how we would extend difficulty-lexicon cre-ation to the context of adult second-language learn-ers, it might be enough to adjust our seed terms toreflect the differences in the language exposure ofthis population, i.e.
we would expect difficulty in ac-quiring colloquialisms that are typically learned inchildhood but are not part of the core vocabulary ofthe adult language.8 ConclusionIn this paper, we have presented an automaticmethod for the derivation of a readability lexicon re-lying only on an unannotated word corpus.
Our re-sults show that although term frequency is a key fea-ture, there are other, more complex features whichprovide competitive results on their own as well ascombining with term frequency to improve agree-ment with manual resources that reflect word diffi-culty or age of acquisition.
By comparing our man-ual lexicon with a new crowdsourced annotation, wealso provide a validation of the resource, while atthe same time highlighting a known issue, the lackof fine-grainedness.
Our manual lexicon provides asolution for this problem, albeit at the cost of somereliability.
Although our immediate interest is nottext readability classification, the information de-rived could be applied fairly directly to this task, andmight be particularly useful in the case when anno-tated texts are not avaliable.AcknowledgmentsThis work was financially supported by the Natu-ral Sciences and Engineering Research Council ofCanada.38ReferencesJulian Brooke, Tong Wang, and Graeme Hirst.
2010.
Au-tomatic acquisition of lexical formality.
In Proceed-ings of the 23rd International Conference on Compu-tational Linguistics (COLING ?10).Jill Burstein, Jane Shore, John Sabatini, Yong-Won Lee,and Matthew Ventura.
2007.
The automated textadaptation tool.
In Proceedings of the Annual Confer-ence of the North American Chapter of the Associationfor Computational Linguistics (NAACL ?07), SoftwareDemonstrations, pages 3?4.Kevin Burton, Akshay Java, and Ian Soboroff.
2009.
TheICWSM 2009 Spinn3r Dataset.
In Proceedings of theThird Annual Conference on Weblogs and Social Me-dia (ICWSM 2009), San Jose, CA.John Carroll, Guido Minnen, Darren Pearce, YvonneCanning, Siobhan Devlin, and John Tait.
1999.
Sim-plifying English text for language impaired readers.In Proceedings of the 9th Conference of the EuropeanChapter of the Association for Computational Linguis-tics (EACL?99), pages 269?270.Kevyn Collins-Thompson and Jamie Callan.
2005.Predicting reading difficulty with statistical languagemodels.
Journal of the American Society for Informa-tion Science Technology, 56(13):1448?1462.Edgar Dale and Jeanne Chall.
1995.
Readability Re-visited: The New Dale-Chall Readability Formula.Brookline Books, Cambridge, MA.Edward William Dolch.
1948.
Problems in Reading.The Garrard Press.Lijun Feng, Noe?mie Elhadad, and Matt Huenerfauth.2009.
Cognitively motivated features for readabilityassessment.
In Proceedings of the 12th Conference ofthe European Chapter of the Association for Compu-tational Linguistics (EACL ?09), pages 229?237.Robert Gunning.
1952.
The Technique of Clear Writing.McGraw-Hill.Michael J. Heilman, Kevyn Collins, and Jamie Callan.2007.
Combining lexical and grammatical features toimprove readability measures for first and second lan-guage texts.
In Proceedings of the Conference of theNorth American Chapter of Association for Computa-tional Linguistics (NAACL-HLT ?07).Paul Kidwell, Guy Lebanon, and Kevyn Collins-Thompson.
2009.
Statistical estimation of wordacquisition with application to readability predic-tion.
In Proceedings of the 2009 Conference onEmpirical Methods in Natural Language Processing(EMNLP?09), pages 900?909.J.
Peter Kincaid, Robert.
P. Fishburne Jr., Richard L.Rogers, and Brad.
S. Chissom.
1975.
Derivation ofnew readability formulas for Navy enlisted personnel.Research Branch Report 8-75, Millington, TN: NavalTechnical Training, U. S. Naval Air Station, Memphis,TN.Thomas K. Landauer and Susan Dumais.
1997.
A so-lution to Plato?s problem: The latent semantic analysistheory of the acquisition, induction, and representationof knowledge.
Psychological Review, 104:211?240.Hanhong Li and Alex C. Feng.
2011.
Age taggingand word frequency for learners?
dictionaries.
In Har-ald Baayan John Newman and Sally Rice, editors,Corpus-based Studies in Language Use, LanguageLearning, and Language Documentation.
Rodopi.Saif Mohammad and Peter Turney.
2010.
Emotionsevoked by common words and phrases: Using Me-chanical Turk to create an emotion lexicon.
In Pro-ceedings of the NAACL HLT 2010 Workshop on Com-putational Approaches to Analysis and Generation ofEmotion in Text, pages 26?34, Los Angeles.Sarah E. Petersen and Mari Ostendorf.
2009.
A machinelearning approach to reading level assessment.
Com-puter Speech and Language, 23(1):89?106.Helmut Schmid.
1995.
Improvements in part-of-speechtagging with an application to German.
In Proceed-ings of the ACL SIGDAT Workshop, pages 47?50.Luo Si and Jamie Callan.
2001.
A statistical modelfor scientific readability.
In Proceedings of the TenthInternational Conference on Information and Knowl-edge Management (CIKM ?01), pages 574?576.Maite Taboada, Julian Brooke, Milan Tofiloski, KimberlyVoll, and Manifred Stede.
2011.
Lexicon-based meth-ods for sentiment analysis.
Computational Linguis-tics, 37(2):267?307.Kumiko Tanaka-Ishii, Satoshi Tezuka, and Hiroshi Ter-ada.
2010.
Sorting texts by readability.
Computa-tional Linguistics, 36(2):203?227.Peter Turney and Michael Littman.
2003.
Measuringpraise and criticism: Inference of semantic orientationfrom association.
ACM Transactions on InformationSystems, 21:315?346.Philip van Oosten, Dries Tanghe, and Veronique Hoste.2010.
Towards an improved methodology for auto-mated readability prediction.
In Proceedings of the 7thInternational Conference on Language Resources andEvaluation (LREC ?10).Ian H. Witten and Eibe Frank.
2005.
Data Mining: Prac-tical Machine Learning Tools and Techniques.
Mor-gan Kaufmann, San Francisco.39
