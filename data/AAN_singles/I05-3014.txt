The Robustness of Domain Lexico-Taxonomy:Expanding Domain Lexicon with CiLinChu-Ren HuangInstitute of Linguistics,Academia Sinica, Taipeichuren@sinica.edu.twXiang-Bing LiInstitute of Information Science,Academia Sinica, Taipeidreamer@hp.iis.sinica.edu.twJia-Fei HongInstitute of Linguistics,Academia Sinica, Taipeijiafei@gate.sinica.edu.twAbstract.This paper deals with the robustexpansion of Domain Lexico-Taxonomy (DLT).
DLT is a domaintaxonomy enriched with domain lexica.DLT was proposed as an infrastructurefor crossing domain barriers (Huang etal.
2004).
The DLT proposal is basedon the observation that domain lexicacontain entries that are also part of ageneral lexicon.
Hence, when entries ofa general lexicon are marked with theirassociated domain attributes, thisinformation can have two importantapplications.
First, the DLT will serveas seeds for domain lexica.
Second, theDLT offers the most reliable evidencefor deciding the domain of a new textsince these lexical clues belong to thegeneral lexicon and do occur reliably inall texts.
Hence general lexiconlemmas are extracted to populatedomain lexica, which are situated indomain taxonomy.
Based on thisprevious work, we show in this paperthat the original DLT can be furtherexpanded when a new languageresource is introduced.
We appliedCiLin, a Chinese thesaurus, and addedmore than 1000 new entries for DLTand show with evaluation that the DLTapproach is robust since the size andnumber of domain lexica increasedeffectively.1.
IntroductionDomain-based language processing has aninherent research dilemma when theconstruction of domain lexicons is involved.The standard approach of building domainlexicon from domain corpora requires a veryhigh threshold of existing domain resources andknowledge.
Since only well-documenteddomains can provide enough quality corpora, itis likely these fields already have good manuallyconstructed domain lexica.
Hence this approachis can only deal with domains where onlymarginal benefit can be achieved, while itcannot deal with domains where it can makemost contribution since there is not enoughresources to work with.It was observed that the type of domainlanguage processing that has the widestapplication and best potentials are cross-domainand multi-domain in nature.
For instance, atypical web-search is a search for specificdomain information from the www as an archiveof mixed and heterogeneous domains.
Thecontribution will be immediate and salient to beable to acquire resources and information for anew domain that is not well documented yet.A new approach towards domain languageprocessing by constructing an infrastructure formulti-domain language processing called theDomain Lexico-Taxonomy (DLT) was proposedin Huang et al (2004).
In the DLT approach,domain lexica are semi-automatically acquiredto populate domain taxonomy.
This lexicallypopulated domain taxonomy serves twopurposes: as the basis of stylo-statisticalprediction of the domain of a new text, and asthe core seed of complete domain lexica.
For thefirst purpose, the DLT approach relies cruciallyon the ability to effectively identify words thatare good indicators of specific domains.
For thesecond purpose, the DLT needs to be robustenough to allow incremental expansion whennew content resources are integrated.
In thisstudy, we integrate CiLin, a Chinese thesaurus,to show that the DLT architecture is indeedrobust.1032.
Related WorkTypical studies on domain lexica focuses onassigning texts to specific classes, hence theyuse a limited taxonomy augmented with a smallset of features (e.g.
Avancini et al 2003,Sebastiani 2002, and Yand and Pederson 1997).However, specialized lemmas cannot be usefulin multi-domain processing.
To achieve domainversatility in processing, it is necessary toidentify lemmas with wider distributions and yetis associated with particular domain(s).
Wefollow the DLT architecture (Huang et al 2004),which was shown to be effective in predictingthe domain of documents extracted from theweb.
We aim to elaborate that framework byproposing a domain lexica can be incrementallyexpanded with knowledge from a new resource.3.
Domain TaxonomyA domain taxonomy containing 549 nodes wasmanually constructed.
The main sources ofdomain classification are from Chinese LibraryClassification system, Encyclopedia Britannicaand the Global View English-Chinese dictionary.Two important criteria were chosen: that thetaxonomy is bilingual and that it is maintainedlocally.
First, the bilingual taxonomy is essentialfor future cross-lingual processing but alsoallows us to access relevant resources in bothlanguages.
Second, since our emphasis was noton the correctness of a dogmatic taxonomy buton the flexibility that allows monotonicextensions, it is essential to be able to monitorany changes in the taxonomy.There are four layers in the constructeddomain taxonomy.
Fourteen (14) domains are inthe upper layer, including Humanities, SocialScience, Formal Science, Natural Science,Medical Science, Engineering Science,Agriculture and Industry, Fine Arts, Recreation,Proper Name, Genre/Strata, Etymology, CountryName, Country People.
The Second layer has147 domains.
The third layer has 279 domains.Lastly the fourth layer has only 109 domainssince not all branches need to be expanded atthis level.
In sum, there are 549 possible domaintags when the hierarchy is ignored.
The domaintaxonomy is available online at the Sinica BOWwebsite (http://BOW.sinica.edu.tw/, Huang andChang 2004).4.
Detection of Domain Lexicon in DLTThe challenge in integrating heterogeneouslanguage resources for domain information isthat conceptual classification varies from oneresource to another and hence cannot be directlyharvested.
We propose to utilize the inheritancerelations of these resources, instead of theirhierarchy.
In other words, lexical (and henceconceptual) identity is established first,following by expanding this matching withlogical inheritance but without branching out onthe conceptual hierarchy.DLT establish the correspondencesbetween the taxonomic nodes of domains andthe linguistic resources of sub-lexica.
Note thata lexical knowledgebase, in a Wordnet fashion,also contains hierarchical relations.
The domaintaxonomy can be enriched by taking thehierarchical information internal to the lexica.
Ifthese resources directly encodes the ?is-a?relation by hyponymy, we assume that both thenode (lexicons) and their hyponym node(lexicons) belong to that domain.
Using thesimple supposition, we can observe the domainknowledge with various resources, andstrengthens the domain lexica for domaintaxonomy.
The process of populating DLT isshown in Fig.
1.Figure.
1.
Populating DLT from Linguistic Resource1045.
Experiment5.1.
The Original Study with BilingualWordNetThe original DLT work was based on bilingualWordnet (Huang et al 2004).
This is because ofthe Wordnet lexical knowledgebase is highlyenriched with lexical semantic relationinformation.
In addition, the bilingual Wordnetadds an unparallel dimension of knowledgecoverage.
The bilingual Wordnet used is SinicaBOW (The Academia Sinica BilingualOntological WordNet, Huang and Chang(2004)).
Sinica BOW is bilingual lexicalknowledgebase connecting WordNet and SUMOand mapping both between English and Chinese.The study reported in Huang et al (2004) alsocontains a small domain identificationexperiment to show the application of DLT.5.1.1 Description of WordNet and SinicaBOWWordNet is inspired by current psycholinguisticand computational theories of human lexicalmemory (Fellbaum (1998), Miller et al (1993)).English nouns, verbs, adjectives, and adverbsare organized into synonym sets, eachrepresenting one underlying lexicalized concept.Different semantic relations link the synonymsets (synsets).
The version of WordNet thatSinica BOW implemented is version 1.6, withnearly 100,000 synsets.In Sinica BOW, ach English synset wasgiven up to 3 most appropriate Chinesetranslation equivalents.
And in cases where thetranslation pairs are not synonyms, theirsemantic relations are marked (Huang et al2003).
The bilingual WordNet is further linkedto the SUMO ontology.
We use the semanticrelations in bilingual resource to expand andpredict domain classification when it cannot bejudged directly from a lexical lemma.5.1.2 Experiment and Result with WordNet463 of the 549 nodes in the domain taxonomywere successfully mapped to a WordNet synsetthrough an identical lemma.
452 or 463mappings were manually confirmed to becorrect, a precision score of over 97%.
Thesedomains were expanded to cover a total of11,918 synsets corresponding to 15,160 Chineselemmas.
Note that both English and Chinesecorrespondences are used since our resources(WordNet and domain taxonomy) are bothbilingual.Due mostly to hyponymy expansion, eachlemma is mapped to 1.38 domains in average.While each lemma is assigned to no more than 8domains, with the majority (6,464) assigned toonly one.
These mapped lemmas populate a setof domain lexica.
The number of entries in thesedomain lexica ranges from 1 to 3762.
Theaverage size of these domain lexica is 32.8lemmas.
Only 41 domains lexical contain 33 ormore lemmas.
Since we cannot know theeffective of the lexicon of a domain a priori, wetake those whose size are above average as theeffective domain lexica.These domain lexica and their sizes areshown in Table 1.5.1.3 Evaluation: precision of domain lexicaIt is impossible to formally evaluate the recallrate of this domain lexica study since we do notknow the total number of entries to be recalled.However, it is possible to evaluate the precisionrate of the constructed domain lexica.
First, theprecision of all recalled lemmas is tested.Among the mapped lemmas, 8696 (out of15,160) lemmas are assigned to multipledomains, while 6,464 are assigned to singledomain.
The single domain mappings werespot-checked to be correct.
On the other hand,the precision of all 8,696 multi-domain lemmasare carefully evaluated.
Among these lemmas,only 4.81% (418) proves to be wrong; and anoverwhelming majority of 95.19% turns out tobe correct (8278).Second, a more meaningful test is toevaluate how well the domain lexica are defined.Five effective domain lexica with over 100entries were randomly chosen for evaluation:Insect (515 entries), Natural Science (262entries), Sports (180 entries), Dance (124 entries)and Religious Music (48 entries).The manually checked precision of thesedomain lexica is listed below the Table 2:105Domain Domain Domain DomainVertebrates??
?3676 ?
Food ?2968 ?
Bird ??
1059 Fish ??
729Language ??699Recreation????
548 Insect ??
515Natural Science????
262Country??
250 contest??
207 music??
192 Indian???188Sports!??
180 commerce??
144 Business ??
144 Dance??
124Heraldic design????
120Medical Science????
85 Medicine ??
76Pathologicalmedicine ????
76Clinical medicine????
76Mathematics??
69Humanities??
?64 ?Social Science????
62physics???
56 Religion??
52Religious Music????
48Plastic art????
45Pure mathematics???
44Anthropology???
42Earth science????
39 drawing??
4:Norse Mythology????
39 Philosophy ??
37Telecommunication????
35 theater??
34Fine Arts??
33Table 1.
Domain lexica containing 33 or more lemmasDomain Label # of entries Precision (%)Insect 515 99.03Natural Science 262 69.85Sports 180 86.11Dance 124 100.00Religious Music 48 93.75Table 2.
Size and Precision of selected domain lexicaTable 2 shows an overall precision of over 95%,while no other lexica has precision lower than86%, natural science is lowest at just below 70%.This is because ?Natural Science?
is a higherlevel domain and hence open to more noises inthe detection process.
This study clearly showedthat the WordNet helped to effectively buildcore domain lexica.We take the domain ?Dance?
as anexample to explain the process.
First, we map?Dance?
to the Wordnet synset?
?dance?, andwe look for the hyponym synsets.
Table3 will beshown the expanding lexica of one of hyponymsynsets.
These lexical entries are associated withdomain ?Dance?
and populate the domainlexicon.Level synset1 social_dancing2 folk_dancing, folk_dance3 country-dance, country_dancing,4          square_dance, square_dancing5             quadrille5.2 For CiLin5.2.1 Description of CiLinCiLin, a short name for Tongyici CiLin, is aChinese thesaurus published in 1984 (Mei et al1984).
The terms in CiLin are organized in aconceptual hierarchy, with near-synonym termsforming a set.
There are five levels in thetaxonomy structure of CiLin.
The CiLin termsbetween Level1 to Level4 are taxonomycategories.
Level1 is the upper class, and itincludes 12 categories, like as people, object,time and space, abstract etc.
Level2 has 106categories.
Level3 has 3,948 categories.
Level4has 4,014 categories.
There are 64,157 terms inLevel5 since all branches need to be expandedat this level.
These terms are classified to 12,193sets by the meaning.
The average number ofterms in each set is 5.34.
Fig.
2 shows thestructure of CiLin.Table 3.
The expanding hyponym synsets of ?dance?106Figure.
2.
The structure of CiLin5.2.2.
Experiment and Result with CiLinFirst, we map the 549 domains to CiLin?staxonomy.
Unlike the previous study, onlyChinese terms were available on CiLin.
Theresult is given in Table 4.# ofentries# ofdomains entries/domainsLeve1 1 146 1 146Level 2 1,587 3 529Level 4 1,222 32 38.19Table 4.
Number of expanding entries andmapping domainsManual checking showed that mappings toLevel 1 and Level 2 are both imprecise andsmall in number.
Hence we take Level 4 as thelexical anchor for enriching domain lexica.1,222 lexical items are expanded from 32domains, and these domain lexica and their sizesare shown in Table 5.Domain DomainInsect(??)
-- 146 Sewing(??)
-- 25Country(??)
-- 128 Movie(??)
-- 25Theater(??)
-- 116 Game(??)
-- 25Painting(??)
-- 88 Photography(??)
-- 21Capital(??)
-- 54 Payment(??)
-- 20Cookery(??)
-- 52 Printing(20 -- (??Dance(??)
-- 52 Literature(??)
-- 18Law(??)
-- 50 Investment(??)
-- 14Education(??)
-- 47 Swimming(??)
-- 12Martial_art(??)
-- 45 Broadcasting(??)
-- 11Religion(??)
-- 39 Ranching_and_animal_husbandry(??)
-- 10Architecture(??)
-- 38 Textile_industry??
10Carving(?37 -- (?
Boating(??)
-- 8Language(??)
-- 37 Trade(?7 -- (?Table 5.
Domain lexicaWhen all mappings are evaluated, 873(71.44%)of them are correct, and 349 (28.56%) incorrect.Five effective domain lexica are evaluated, asshown below in Table 6:Domain Label # of entries Precision (%)Insect 146 58.9Country 128 55.47Theater 116 80.17Painting 88 80.68Dance 52 80.77Table 6.
Size and Precision of selected domain lexicaCompared with the work reported in (Huang etal.
2004), both the number of lemma (1,222 vs.15,160) and precision (71.44% vs. nearly 95%)are lower.
This result is expected since CiLinhas a simple taxonomy without the rich lexicalinformation of a Wordnet.
The crucial factshown, however, is that DLT can beincrementally enhanced with the new mappings.Of the 873 correct domain lexica entries, 79.5%(694) are new entries that were not identifiedpreviously.
Even more impressive is theeffectiveness of increase in lexica sizes forapplicable domains, as shown below in Table 7.107domain WN/old CiLin/new increase domain WN/old CiLin/new!increase!??
34 80 0.7018??
12 15 0.5556??
515 65 0.1121??
22 15 0.4054??
17 61 0.7821??
28 14 0.3333??
250 44 0.1497??
192 12 0.0588??
124 34 0.2152??
20 11 0.3548??
7 33 0.8250??
23 10 0.3030??
26 33 0.5593??
15 9 0.3750??
14 32 0.6957??
5 8 0.6154??
2 29 0.9355??
9 7 0.4375??
2 27 0.9310??
2 7 0.7778??
22 24 0.5217??
0 5 1.0000??
26 23 0.4694??
16 5 0.2381??
699 22 0.0305??
27 4 0.1290??
52 21 0.2877??
4 4 0.5000??
55 19 0.2568??
6 2 0.2500??
21 17 0.4474??
1 2 0.6667Table 7.
Increase in Domain Lexicon Size after CiLin IntegrationTable 7 shows that, even though adding CiLinonly helped 32 domain lexica, 14 of them havetheir lexicon size more than doubled.
One ofthem, ranching and animal husbandry is a newdomain lexicon where no mapping was possiblewith WordNet.
In other words, adding the CiLinresource substantially enhanced effectivedomain coverage of DLT.6.
ConclusionIn this paper, test the robustness of the DLTarchitecture.
We show both the coverage and thesizes of the domain lexica on DLT can beeffectively expanded by integrating a newlanguage resource.
The robustness is convincinggiven that the coverage and quality of the newresource is actually not as good as the originalreference resources.
In other words, we showedthe open architecture of DLT facilitatesintegration of new domain information withoutimposing any high threshold on the format andquality of new resources.
We also verify partialresults of previous work since 205 lemmamappings were repeated.
For future work, weplan to continue to populate DLT, as well as toexplore other possibilities for putting DLT toactual applications.ReferencesChu-Ren Huang, Elanna I. J. Tseng, Dylan B. S. Tsai,Brian Murphy.
Cross-lingual Portability ofSemantic relations: Bootstrapping ChineseWordNet with English WordNet Relations.Languages and Linguistics.
4.3.
(2003)509-532Chu-Ren, Huang and Ru-Yng Chang.
Sinica BOW(Bilingual Ontological Wordnet): Integration ofBilingual WordNet and SUMO?.
Presented at the4th International Conference on LanguageResources and Evaluation (LREC2004).
Lisbon.Portugal.
26-28 May (2004)Chu-Ren Huang, Xiang-Bing Li, Jia-Fei Hong.
"Domain Lexico-Taxonomy:An ApproachTowards Multi-domain Language Processing",Asian Symposium on Natural LanguageProcessing to Overcome Language Barriers, TheFirst International Joint Conference on NaturalLanguage Processing (IJCNLP-04).
Sanya City,Hainan Island, China.
22-24 March (2004)F.
Sebastiani., ?Machine learning in automated textcategorization?.
ACM Computing Surveys, 34(1)(2002)1-47Fellbaum C.. WordNet: An Electronic LexicalDatabase.
Cambridge: MIT Press (1998)G. A. Miller, R. Beckwith, C. Fellbaum, D. Grossand K. Miller.
?Introduction to WordNet: AnOn-line Lexical Database,?
In Proceedings of thefifteenth International Joint Conference on108Artificial Intelligence.
Chamb?ry, France.
28August- 3 September (1993)Henri Avancini, Alberto Lavelli, Bernardo Magnini,Fabrizio Sebastiani, Roberto Zanoli.
ExpandingDomain-Specific Lexicons by TermCategorization.
Proceedings of the 2003 ACMsymposium on Applied computing.
Melbourne,Florida, USA.
9-12 March (2003)Jia-ju Mei, Yi-Ming Zheng, Yun- Qi Gao and Hung-Xiang Yin.
TongYiCi CiLin.
Shanghai: theCOMMERCIAL Press (1984)Y. Yang and J. O. Pedersen.
A comparative study onfeature selection in text categorization.
In D. H.Fisher, editor, Proceedings of ICML-97, 14thInternational Conference on Machine Learning,pages 412 420.
San Francisco: Morgan Kaufmann(1997)109
