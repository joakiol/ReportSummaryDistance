Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon?s Mechanical Turk, pages 184?187,Los Angeles, California, June 2010. c?2010 Association for Computational LinguisticsTools for Collecting Speech Corpora via Mechanical-TurkIan Lane1,2, Alex Waibel1,21Language Technologies InstituteCarnegie Mellon UniversityPittsburgh, PA 15213, USA{ianlane,ahw}@cs.cmu.eduMatthias Eck2, Kay Rottmann22Mobile Technologies LLCPittsburgh, PA, USAmatthias.eck@jibbigo.comkay.rottmann@jibbigo.comAbstractTo rapidly port speech applications tonew languages one of the most difficulttasks is the initial collection of sufficientspeech corpora.
State-of-the-art automaticspeech recognition systems are typicaltrained on hundreds of hours of speechdata.
While pre-existing corpora do existfor major languages, a sufficient amountof quality speech data is not available formost world languages.
While previousworks have focused on the collection oftranslations and the transcription of audiovia Mechanical-Turk mechanisms, in thispaper we introduce two tools which ena-ble the collection of speech data remotely.We then compare the quality of audio col-lected from paid part-time staff and unsu-pervised volunteers, and determine thatbasic user training is critical to obtain us-able data.1 IntroductionIn order to port a spoken language application to anew language, first an automatic speech recogni-tion (ASR) system must be developed.
For manylanguages pre-existing corpora do not exist andthus speech data must be collected before devel-opment can begin.
The collection of speech corpo-ra is an expensive undertaking and obtaining thisdata rapidly, for example in response to a disaster,cannot be done using the typical methodology inwhich corpora are collected in controlled environ-ments.To build an ASR system for a new language, twosets of data are required; first, a text corpus con-sisting of written transcriptions of utterances usersare likely to speak to the system, this is used totrain the language model (LM) applied duringASR; and second, a corpora of recordings ofspeech, which are used to train an acoustic model(AM).
Text corpora for a new language can becreated by manually translating a pre-existing cor-pus (or a sub-set of that corpus) into the new lan-guage and crowd-sourcing methodologies can beused to rapidly perform this task.
Rapidly creatingcorpora of speech data, however, is not trivial.Generally speech corpora are collected in con-trolled environments where speakers are super-vised by experts to ensure the equipment is setupcorrectly and recordings are performed adequately.However, for most languages performing this taskon-site, where developers are located, is impractic-al as there may not be a local community of speak-ers of the required language.
An alternative is toperform the data collection remotely, allowingspeakers to record speech on their own PCs or mo-bile devices in their home country or whereverthey are located.
While previous works have fo-cused on the generation of translations (Razavian,2009) and transcribing of audio (Marge, 2010) viaMechanical-Turk, in this paper we focus on thecollection of speech corpora using a Mechanical-Turk type framework.Previous works (Voxforge), (Gruenstein, 2009),(Schultz, 2007) have developed solutions for col-lecting speech data remotely via web-based inter-faces.
A web-based system for the collection ofopen-source speech corpora has been developed bythe group at www.voxforge.org.
Speech recordingsare collected for ten major European languages andspeakers can either record audio directly on thewebsite or they can call in on a dedicated phoneline.
In (Gruenstein, 2009) spontaneous speech(US English) was collected via a web-based mem-ory game.
In this system speech prompts were notprovided, but rather a voice-based memory gamewas used to gather and partially annotate184Figure 1: Screenshots from Speech Collection iPhone Appspontaneous speech.
In comparison to the aboveworks which focus on the collection of data formajor languages, the SPICE project (Schultz,2007) provides a set of web-based tools to enabledevelopers to create voice-based applications forless-common languages.
In addition to tools fordefining the phonetic units of a language and creat-ing pronunciation dictionaries, this system alsoincludes tools to create prompts and collect speechdata from volunteers over the web.In this paper, we describe two tools we have de-veloped to collect speech corpora remotely.
Thefirst, a Mobile smart-phone based system whichallows speakers to record prompted speech directlyon their phones and second, a web-based systemwhich allows recordings to be collected remotelyon PCs.
We compare the quality of audio collectedfrom paid part-time staff and unsupervised volun-teers and determine that basic user training andautomatic feedback mechanisms are required toobtain usable data.2 Collection of Speech on Mobile DevicesToday?s smart-phones are able to record qualityaudio onboard and generally have the ability toconnect to the internet via a fast wifi-connection.This makes them an ideal platform for collectingspeech data in the field.
Speech data can be col-lected by a user at any time in any location, and thedata can be uploaded at a later time when a wire-less connection is available.
At Mobile Technolo-gies we have developed an iPhone application toperform this task.The collection procedure consists of three steps.First, on start-up a small amount of personal in-formation, namely, gender and age, are requestedfrom the user.
They then select the language forwhich they intend to provide speech data.
The mo-bile-device ID, personal information and languageselected is used as an identifier for individualspeakers.
Next, collection of speech data is per-formed.
Collection is performed offline, enablingdata to be collected in the field where there maynot be a persistent internet connection.
A prompt israndomly selected from an onboard database ofsentences and is presented to the user, who readsthe sentence aloud holding down a push-to-talkbutton while speaking.
During the speech collec-tion stage, the system automatically proceeds to thefollowing prompt when the current recording iscomplete.
The user however has the ability to goback to previous recordings, listen to it and re-speak the sentence if any issues are found.
Finally,the speech data is uploaded using a wireless collec-tion.
Data is uploaded one utterance at a time to anFTP server.
Uploading each utterance individuallyallows the user to halt the upload and continue it ata later time if required.185Figure 2: Java applet for Web-based recording3 Collection via Web-based RecordingOne of the most popular websites for crowd-sourcing is Amazon Mechanical Turk (AMT).?Requesters?
post Human Intelligence Tasks(HITs) to this website and ?Workers?
browse theHITs, perform tasks and get paid a predefinedamount after submitting their work.
It has beenreported that over 100,000 workers from 100 coun-tries are using AMT (Pontin, 2007).AMT allows two general types of HITs.
A Ques-tion Form HIT is based on a provided XML tem-plate and only allows certain elements in the HIT.However, it is possible to integrate an externalJAVA applet within a Question Form HIT whichallows for some flexibility.
Questions can also behosted on an external website which increases flex-ibility for the HIT developer while remainingtightly integrated in the AMT environment.For collection of audio data Amazon does not offerany integrated tools.
We thus designed and imple-mented a Java applet for web based speech collec-tion.
The Java applet can easily be incorporated inthe AMT Question-Form mechanism and couldalso be used as part of an External-Question HIT.Currently the Java applet provides the same basicfunctionality as outlined for the iPhone application.The applet sequentially shows a number ofprompts to record.
The user can skip a sentence,playback a recording to check the quality and alsoredo the recording for the current sentence (seescreenshot in Figure 2).After the user is finished, the recorded sentencesare uploaded to a web-server using an HTTP Postrequest.
An important difference is the necessity tobe online during the speech recordings.4 Evaluation of Recorded AudioOne issue when collecting speech data remotely isthe quality of the resulting audio.
When collectionTable 1: Details of Evaluated CorporaTable 2: Annotations used to label poor qualityrecordingsis performed in a controlled environment, the de-veloper can ensure that the recording equipment issetup correctly, background noise is kept to a min-imum and the speaker is adequately trained to usethe recording equipment.
However, the same is notguaranteed when collecting speech remotely viamechanical-turk frameworks.When recording prompted speech there are threetypes of issues that result in unsuitable data:?
Garbage Audio: recordings that are emp-ty, clipped, have insufficient power, or areincorrectly segmented.?
Low quality recordings: low Signal-to-Noise recordings due to poor equipment orlarge background noise?
Speaker errors: Misspeaking of prompts,both accidental and maliciousTo verify the quality of audio recorded in unsuper-vised environments we compared two sets ofspeech data.
First, in an earlier data collection taskwe collected 445 prompted utterances from 10 US-English speakers.
This data collection was per-formed in a quiet office environment with technic-al supervision.
Speakers were paid a fee for theirtime.
As a comparison a similar collection of Hai-tian Creole was performed.
In this case data wascollected on a volunteer basis and supervision waslimited.
Details of the collected data are shown inTable 1.Paid EmployeesLanguage EnglishNumber of Speakers 10Utterances Evaluated 445VolunteersLanguage Haitian CreoleNumber of Speakers 3Utterances Evaluated 1671 Recorded utterance is empty2 Utterance is not segmented correctly3 Recording is clipped4 Recording contains audible echo5 Recording contains audible noise186Figure 3: Percentage of recorded utterances de-termined to be inadequate for acoustic modeltraining.
Annotations limited to five issueslisted in Table 1.To determine the frequency of the quality issueslisted above, we manually verified the two sets ofcollected speech.
The recording of each utterancewas listened to and if the audio file was determinedto be of low quality it was annotated with one ofthe tags listed in Table 2.
The percentage of utter-ances labeled with each annotation is shown for theEnglish and volunteer Haitian Creole cases in Fig-ure 3.Around 10% of the English recordings were foundto have issues.
Clipping occurred in approximately5% and a distinct echo was present in the record-ings for one speaker.
For the Haitian Creole casethe yield of useable audio was significantly lowerthan that obtained for English.
For all three speak-ers clipping was more prevalent and the level ofbackground noise was higher.
We discovered thatdue to lack of training, one of the volunteers hadsignificant issues with the push-to-talk interface inour system.
This led to many empty or incorrectlysegmented recordings.
In both cases, prompts weregenerally spoken accurately and technical prob-lems caused poor quality recordings.We believe the large difference in the yield of highquality recordings, 90% for English compared to65% for Haitian Creole case, is directly due to thelack of training speakers received and the volun-teer nature of the Haitian Creole task.
By incorpo-rating a basic tutorial when users first start ourtools and an explicit feedback mechanism whichautomatically detects quality issues and promptsusers to correct them we expect the yield of highquality recordings to increase significantly.
In thenear future we plan to use the tools to collect datafrom large communities of remote users.5 Conclusions and Future WorkIn this work, we have described two applicationsthat allow speech corpora to be collected remotely,either directly on Mobile smart-phones or on a PCvia a web-based interface.
We also investigated thequality of recordings made by unsupervised volun-teers and found that although prompts were gener-ally read accurately, lack of training led to asignificantly lower yield of high quality record-ings.In the near future we plan to use the tools to collectdata from large communities of remote users.
Wewill also investigate the user of tutorials and feed-back to improve the yield of high quality data.AcknowledgementsWe would like to thank the Haitian volunteers whogave their time to help with this data collection.ReferencesN.
S. Razavian, S Vogel, "The Web as a Platformto Build Machine Translation Resources",IWIC2009M.
Marge, S. Banerjee and A. Rudnicky, "Usingthe Amazon Mechanical Turk for Transcriptionof Spoken Language", IEEE-ICASSP, 2010Voxforge, www.voxforge.orgA.
Gruenstein, I. McGraw, and A. Sutherland, "Aself-transcribing speech corpus: collecting con-tinuous speech with an online educationalgame," Submitted to the Speech and LanguageTechnology in Education (SLaTE) Workshop,2009.T.
Schultz, et.
al, "SPICE: Web-based Tools forRapid Language Adaptation in SpeechProcessing Systems", In the Proceedings ofINTERSPEECH, Antwerp, Belgium, 2007.J.
Pontin, ?Artificial Intelligence, With Help Fromthe Humans?, The New York Times, 25 March2007187
