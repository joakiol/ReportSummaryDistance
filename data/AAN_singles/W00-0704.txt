In: Proceedings of CoNLL-2000 and LLL-2000, pages 19-24, Lisbon, Portugal, 2000.The Role of Algorithm Bias vs Information Source inLearning Algorithms for Morphosyntactic DisambiguationGuy De Pauw and Wal ter  Dae lemansCNTS - Language Technology GroupUIA - University of AntwerpUniversiteitsplein 1, 2610 Antwerpen, Belgium{depauw, daelem}~uia.ua, ac.beAbst rac tMorphosyntactic Disambiguation (Part ofSpeech tagging) is a useful benchmark problemfor system comparison because it is typicalfor a large class of Natural Language Process-ing (NLP) problems that can be defined asdisambiguation in local context.
This paperadds to the literature on the systematic andobjective evaluation of different methods toautomatically earn this type of disambiguationproblem.
We systematically compare twoinductive learning approaches to tagging: MX-POST (based on maximum entropy modeling)and MBT (based on memory-based learning).We investigate the effect of different sourcesof information on accuracy when comparingthe two approaches under the same conditions.Results indicate that earlier observed differ-ences in accuracy can be attributed largely todifferences in information sources used, ratherthan to algorithm bias.1 Compar ing  TaggersMorphosyntactic Disambiguation (Part ofSpeech tagging) is concerned with assigningmorpho-syntactic categories (tags) to words ina sentence, typically by employing a complexinteraction of contextual and lexical clues totrigger the correct disambiguation.
As a con-textual clue, we might for instance assume thatit is unlikely that a verb will follow an article.As a lexical (morphological) clue, we mightassign a word like better the tag comparative ifwe notice that its suffix is er.POS tagging is a useful first step in text anal-ysis, but also a prototypical benchmark task forthe type of disambiguation problems which isparamount in natural anguage processing: as-signing one of a set of possible labels to a linguis-tic object given different information sources de-rived from the linguistic context.
Techniquesworking well in the area of POS tagging mayalso work well in a large range of other NLPproblems such as word sense disambiguationand discourse segmentation, when reliable an-notated corpora providing good predictive in-formation sources for these problems becomeavailable.F inding the information sources relevant forsolving a particular task, and  optimally inte-grating them with an inductive mode l  in a dis-ambiguator  has been the basic idea of most  ofthe recent empirical research on this type ofNLP  prob lems and  part of speech tagging \] inparticular.It is unfortunate, however, that this line of re-search most  often refrains f rom investigating therole of each component  proper, so that it is notalways clear whether  differences in accuracy aredue to inherent bias in the learning algorithmsused, or to different sources of information usedby the algorithms.This paper  expands  on an empirical compar-ison (van Halteren et al, 1998) in which TRI-GRAM tagging, BR ILL  tagging, MAXIMUM EN-TROPY and MEMORY BASED tagging were com-pared on the LOB corpus.
We will provide amore  detailed and  systematic compar ison be-tween MAXIMUM ENTROPY MODEL ING (aatna-parkhi, 1996) and  MEMORY BASED LEARNING(Dae lemans  et al, 1996) for morpho-syntact icd isambiguat ion and  we investigate whether  ear-lier observed differences in tagging accuracy canbe attributed to algorithm bias, informationsource issues or both.1See van Halteren (ed.)
(1999) for a comprehensiveoverview of work on morphosyntactic disambiguation,including empirical approaches.19After a brief introduction of the 2 algorithmsused in the comparison (Section 2), we willoutline the experimental setup in Section 3.Next we compare both algorithms on respec-tively typical MSW-features (Section 4) and typ-ical MXPOSW-features (Section 5), followed bya brief error analysis and some concluding re-marks.2 A lgor i thms and  Implementat ionIn this Section, we provide a short descriptionof the two learning methods we used and theirassociated implementations.2.1 Memory-Based LearningMemory-Based Learning is based on the as-sumption that new problems are solved bydirect reference to stored experiences of pre-viously solved problems, instead of by refer-ence to rules or other knowledge structuresextracted from those experiences (Stanfill andWaltz, 1986).
A memory-based (case-based)approach to tagging has been investigated inCardie (1994) and Daelemans et al (1996).Implementat ionFor our experiments we have used TIMBL 2(Daelemans et al, 1999a).
TIMBL includes anumber of algorithmic variants and parameters.The base model (ISl) defines the distance be-tween a test item and each memory item asthe number of features for which they have adifferent value.
Information gain can be intro-duced ( IB i - IG)  to weigh the cost of a featurevalue mismatch.
The heuristic approximationof computationally expensive pure MBL vari-ants, (IGTREE), creates an oblivious decisiontree with features as tests, ordered accordingto information gain of features.
The number ofnearest neighbors that are taken into accountfor extrapolation, can be determined with theparameter K.For typical symbolic (nominal) features, val-ues are not ordered.
In the previous variants,mismatches between values are all interpretedas equally important, regardless of how similar(in terms of classification behavior) the valuesare.
We adopted the modified value differencemetric (MVDM) to assign a different distance be-tween each pair of values of the same feature.2TIMBL is available from: http://ilk.kub.nl/For more references and information aboutthese algorithms we refer to Daelemans et al(1999a).2.2 Max imum EntropyIn this classification-based approach, diversesources of information are combined in an expo-nential statistical model that computes weights(parameters) for all features by iteratively max-imizing the likelihood of the training data.
Thebinary features act as constraints for the model.The general idea of maximum entropy model-ing is to construct a model that meets theseconstraints but is otherwise as uniform as pos-sible.
A good introduction to the paradigm ofmaximum entropy can be found in Berger et al(1996).MXPOST (Ratnaparkhi, 1996) applied maxi-mum Entropy learning to the tagging problem.The binary features of the statistical model aredefined on the linguistic context of the wordto be disambiguated (two positions to the left,two positions to the right) given the tag ofthe word.
Information sources used include thewords themselves, the tag of the previous words,and for unknown words: prefix letters, suffixletters, and information about whether a wordcontains a number, an upcase character, or ahyphen.
These are the primitive informationsources which are combined uring feature gen-eration.In tagging an unseen sentence, a beam searchis used to find the sequence of tags with thehighest probability, using binary features ex-tracted from the context to predict the mostprobable tags for each word.Implementat ionFor our experiments, we used MACCENT, animplementation f maximum entropy modelingthat allows symbolic features as input.
3 Thepackage takes care of the translation of sym-bolic values to binary feature vectors, and im-plements the iterative scaling approach to findthe probabilistic model.
The only parametersthat are available in the current version are themaximum number of iterations and a value fre-quency threshold which is set to 2 by default(values occurring only once are not taken intoaccount).aDetails on how to obtain MACCENT can be found on:http://www.cs.kuleuven.ac.be/-ldh/203 Exper imenta l  SetupWe have set up the experiments in such a waythat neither tagger is given an unfair advantageover the other.
The output of the actual taggers(MBT and MXPOST) is not suitable to study theproper effect of the relevant issues of informa-tion source and algorithmic parameterisation,since different information sources are used foreach tagger.
Therefore the taggers need to beemulated using symbolic learners and a prepro-cessing front-end to translate the corpus datainto feature value vectors.The tagging experiments were performed onthe LOB-corpus (Johansson et al 1986).
Thecorpus was divided into 3 partitions: an 80%training partition, consisting of 931.062 words,and two 10% partitions: the VALIDATION SET(114.479 words) and the TEST SET (115.101words) on which the learning algorithms wereevaluated.The comparison was done in both direc-tions: we compared both systems using infor-mation sources as described in Daelemans et al(1996) as well as those described in Ratnaparkhi(1996).Corpus  P reprocess ingSince the implementations of both learning al-gorithms take propositional data as their input(feature-value pairs), it is necessary to translatethe corpora into this format first.
This can bedone in a fairly straightforward manner, as is il-lustrated in Tables 1 and 2 for the sentence Shelooked him up and down.word dShe *looked PP3Ahim VBDup PP30and RPdown CCRPf aPP3A VBD-VBNVBD-VBN PP30PP30 RP-INRP-IN CCCC RPRP SPERSPER *valuePP3AVBDPP30RPCCRPSPERTable 1: Contextual featuresThe disambiguation of known words is usu-ally based on contextual features.
A word isconsidered to be known when it has an ambigu-ous tag (henceforth ambitag) attr ibuted to it inthe LEXICON, which is compiled in the same wayas for the MBT-tagger (Daelemans et al, 1996).A lexicon entry like telephone for example car-ries the ambitag NN-VB, meaning that it wasobserved in the training data as a noun or averb and that it has more often been observedas a noun (frequency being expressed by order).Surrounding context for the focus word (fi aredisambiguated tags (a 0 on the left-hand side andambiguous tags (a) on the right-hand side.In order to avoid the unrealistic situation thatall disambiguated tags assigned to the left con-text of the target word are correct, we simulateda realistic situation by tagging the validationand test set with a trained memory-based ormaximum entropy tagger (trained on the train-ing set), and using the tags predicted by thistagger as left context tags.word p s s s c hShe S S h e T Flooked 1 k e d F Fhim h h i m F Fup u * u p F Fand a a n d F Fdown d o w n F F?
* F FTable 2: Morphological featuresUnknown words need more specific word-forminformation to trigger the correct disambigua-tion.
Prefix-letters (p), suffix-letters (s), the oc-currence of a hyphen (h) or a capital (c) are allconsidered to be relevant features for the dis-ambiguation of unknown words.4 Us ing  MBT- type  featuresThis section describes tagging experiments forboth algorithms using features as described inDaelemans et al (1996).
A large number ofexperiments were done to find the most suitablefeature selection for each algorithm, the mostrelevant results of which axe presented here.Va l idat ion  PhaseIn the validation phase, both learning algo-rithms iteratively exhaust different feature com-binations on the VALIDATION SET, as well asleaxner-specific parameterisations.
For each al-gorithm, we try all feature combinations thathardware restrictions allow: we confined our-selves to a context of maximum 6 surrounding21Known Words % f df fa dfa ddfaa dddfaaaTIMBL IGTREE 92.5 95.1 95.9 97.2 97.2 97.2TIMBL IB1 92.5 95.1 95.9 97.2 97.4 97.3TIMBL IBi K----5 92.5 95.1 95.6 93.8 96.4 97.0TIMBL IB1 K----10 92.5 95.1 95.6 93.4 93.7 96.1TIMBL MVDM 92.5 95.1 95.9 97.4 97.4 97.2TIMBL MVDM K=5 92.5 95.1 95.2 97.5 97.5 97.4TIMBL MVDM K----10 92.5 95.1 94.9 97.5 97.5 97.3MACCENT 92.5 94.5 95.8 97.5 97.6 97.4Unknown Words % ddaap ddaas ddaaps ddaapss ddaapsscn ddaapsshcnTIMBL IGTREE 42.1 65.9 65.2 65.8 68.6 70.0TIMBL ml 53.8 63.7 66.3 68.3 68.8 70.7TIMBL IB1 K----5 54.2 61.6 66.7 71.4 72.5 774.3TIMBL IB1 K----10 49.5 55.3 64.2 68.4 70.3 72.7TIMBL MVDM 58.1 72.0 70.9 75.1 71.0 73.3TIMBL MVDM K----5 61.2 72.0 75.6 79.7 75.5 77.6TIMBL MVDM K----10 61.7 72.7 76.0 '79.7 77.1 77.9MACCENT 61.8 67.0 74.8 '78.6 75.3 77.0Table 3: Validation Phase Resultstags or less, since we already noticed perfor-mance degradation for both systems when us-ing a context of more than 5 surrounding tags.For unknown words, we have to discern between2 different tuning phases.
First, we find theoptimal contextual feature set, next the opti-mal morphological features, presupposing bothtypes of features operate independently.We investigate seven of the variations ofMemory-Based Learning available in TIMBL (seeDaelemans et al (1999b) for details) and oneinstantiation of maccent, since the current ver-sion does not implement many variations.A summary of the most relevant results ofthe validation phase can be found in Table 3.The result of the absolute optimal feature setfor each algorithm is indicated in bold.
Forsome contexts, we observe a big difference be-tween IGTREE and IBi-IG and IB1-MVDM.
Forunknown words, the abstraction made by themWREE-algorithm seems to be quite harmfulcompared to the true lazy learning of the othervariants (see Daelemans et al (1999b) for a pos-sible explanation for this type of behaviour).Of all algorithms, Maximum Entropy has thehighest agging accuracy for known words, out-performing TIMBL-algorithms however by onlya very small margin.
The overall optimal con-text for the algorithms turned out to be dfa andddfaa respectively, while enlarging the contexton either side of the focus word resulted in alower tagging accuracy.Overall, we noticed a tendency for TIMBL toperform better when the information source israther limited (i.e.
when few features are used),while MACCENT seems more robust when deal-ing with a more elaborate feature space.Test PhaseThe Test Phase of the experiment consists ofrunning the optimised subalgorithm paired withthe optimal feature set on the test set.
TIMBL,augmented with the Modified Value DifferenceMetric and k set to 5, was used to disambiguateknown words with a dfa feature value, unknownwords with the features ddaapss.
MACCENTused the same features for unknown words, butused more elaborate features (ddfaa) to disam-biguate known words.
The results of the opti-mised algorithms on the test set can be foundin Table 4.TIMBL MACCENTKnown Words 97.6 97.7Unknown Words 77.3 78.2Total 97.2 97.2Sentence 62.7 63.5Table 4: Test results with MBT featuresOverall tagging accuracy is similar for bothalgorithms, indicating that for the overall tag-ging problem, the careful selection of optimalinformation sources in a validation phase, hasa bigger influence on accuracy than inherentproperties or bias of the two learning algorithms22Algorithm Accuracy (%) on test setIGTREE K----1 94.3T IMBLMVDMK:5  92.8Maccent 94.3Maccent Beam(n=5) 94.3Table 5: Test results with MXPOST featurestested.Beam SearchNote that MACCENT does not include the beamsearch over N highest probability tag sequencecandidates at sentence level, which is part ofthe MXPOST tagger (but not part of maximumentropy-based learning proper; it could be com-bined with MBL as well).
To make sure thatthis omission does not affect maximum entropylearning adversely for this task, we implementedthe beam search, and compared the results withthe condition in which the most probable tagis used, for different beam sizes and differentamounts of training data.
The differences inaccuracy were statistically not significant (beamsearch even turned out to be significantly worsefor small training sets).
The beam search veryrarely changes the probability order suggestedby MACCENT, and when it does, the number oftimes the suggested change is correct is aboutequal to the number of times the change iswrong.
This is in contrast with the results ofRatnaparkhi (1996), and will be investigatedfurther in future research.5 Us ing  MXPOST-type featuresIn order to complete the systematic ompari-son, we compared maximum entropy (again us-ing the MACCENT implementation) with MBLwhen using the features uggested in (Ratna-parkhi, 1996).
Due to the computational ex-pense of the iterative scaling method that is in-herent o maximum entropy learning, it was nottractable to incorporate an extensive validationphase for feature selection or algorithmic vari-ant selection.
We simply took the features ug-gested in that paper, and 2 different settings forour MBL implementation, IGTREE and MVDMK----5, the latter being the optimal algorithm forthe previous experiments.
The results on thetest set are shown in Table 5.Beam searchNotice that again, the sentence level beamsearch does not add significantly to accuracy.Also note that the results report in Table 5 dif-fer significantly from those reported for MXPOSTin (van Halteren et al, 1998).
The difference intagging accuracy is most likely due to the prob-lematic translation of MXPOST'S binary featuresto nominal features.
This involves creating in-stances with a fixed number of features (not justthe active features for the instance as is thecase in MXPOST),  resulting in a bigger, lessmanageable instance space.
When IGTREE com-presses the elaborate instance space, we conse-quently notice a significant improvement over aMVDM approach.6 Error  Ana lys i sThe following table contains ome more detailedinformation about the distribution of the er-rors 4:Known UnknownBoth wrong - same tag 1384 335Both Wrong - different ag 117 130Only MACCENT Wrong 1008 181Only TIMBL Wrong 1103 193In 87% of the cases where both algorithms arewrong, they assign the same tag to a word.
Thisindicates that about 55% of the errors can eitherbe attributed to a general shortcoming presentin both algorithms or to an inadequate informa-tion source.
We can also state that 97.8% of thetime, the two algorithms agree on which tag toassign to a word (even though they both agreeon the wrong tag 1.7% of the time).We also observed the same (erroneous) tag-ging behavior in both algorithms for lower-frequency tags, the interchanging of noun tagsand adjective tags, past tense tags and past par-ticiple tags and the like.Another issue is the information value of theambitag.
We have observed several cases wherethe correct ag was not in the distribution spec-ified by the ambitag, which has substantial in-formation value.
In our test set, this is thecase for 1235 words (not considering unknownwords).
553 times, neither algorithm finds thecorrect ag.
Differences can be observed in the4The error analysis described in this Section, is basedon the first set of experiments in which MBT-featureswere used to disambiguate he test set.23way the algorithms deal with the informationvalue of the ambitag, with Maximum Entropyexhibiting a more conservative approach withrespect to the distribution suggested by the am-bitag, more reluctant to break free from the am-bitag.
It only finds the correct part-of-speechtag 507 times, whereas TiMBL performs betterat 594 correct ags.
There is a downside to this:sometimes the correct ag is featured in the am-bitag, but the algorithm breaks free from theambitag nevertheless.
This happens to TiMBL267 times, and 288 times to MACCENT.In any case, the construction of the ambitagseems to be a problematic ssue that needs to beresolved, since its problematic nature accountsfor almost 40% of all tagging errors.
This isespecially a problem for MBT as it relies on am-bitags in its representation.7 Concluding RemarksA systematic omparison between two state-of-the-art agging systems (maximum entropyand memory-based learning) was presented.
Bycarefully controlling the information sourcesavailable to the learning algorithms when usedas a tagger generator, we were able to show that,although there certainly are differences betweenthe inherent bias of the algorithms, these differ-ences account for less variability in tagging ac-curacy than suggested in previous comparisons(e.g.
van Halteren et al (1998)).Even though overall tagging accuracy of bothlearning algorithms turns out to be very similar,differences can be observed in terms of accuracyon known and unknown words separately, butalso in the differences in the (erroneous) taggingbehaviour the two learning algorithms exhibit.Furthermore, evidence can be found thatgiven the same information source, differentlearning algorithms, and also different instan-tiations of the same learning algorithm, yieldsmall, but significant differences in tagging ac-curacy.
This may be in line with theoreticalwork by Roth (1998);Roth (1999) in which bothmaximum entropy modeling and memory-basedlearning (among other learning algorithms) areshown to search for a decision surface which is alinear function in the feature space.
The resultsput forward in this paper support the claimthat, although the linear separator found canbe different for different learning algorithms, thefeature space used is more important.We also showed that which informationsources, algorithmic parameters, and even algo-rithm variants are optimal depends on a com-plex interaction of learning algorithm, task, anddata set, and should accordingly be decidedupon by cross-validation.ReferencesAdam Berger, Stephen Della Pietra, and VincentDella Pietra.
1996.
Maximum Entropy Approachto Natural Language Processing.
ComputationalLinguistics, 22(1).C.
Caxdie.
1994.
Domain Specific Knowledge Acqui-sition for Conceptual Sentence Analysis.
Ph.D.thesis, University of Massachusets, Amherst, MA.W.
Daelemans, J. Zavrel, P. Berck, and S. Gillis.1996.
MBT: A memory-based part of speech tag-ger generator.
In E. Ejerhed and I. Dagan, ed-itors, Proc.
of Fourth Workshop on Very LargeCorpora, pages 14-27.
ACL SIGDAT.W.
Daelemans, A.
Van den Bosch, and J. Zavrel.1999a.
Forgetting exceptions i harmful in lan-guage learning.
Machine Learning, Special issueon Natural Language Learning, 34:11-41.W.
Daelemans, J. Zavrel, K. Van der Sloot, andA.
Van den Bosch.
1999b.
TiMBL: Tilburg Mem-ory Based Learner, version 2.0, reference manual.Technical Report ILK-9901, ILK, Tilburg Univer-sity.A.
Ratnaparkhi.
1996.
A maximum entropy part-of-speech tagger.
In Proc.
of the Conference on Em-pirical Methods in Natural Language Processing,May 1Y-18, 1996, University of Pennsylvania.Dan Roth.
1998.
Learning to resolve natural an-guage ambiguities: A unified approach.
In Pro-ceedings of the 15th National Conference on Ar-tificial Intelligence (AAAI-98) and of the lOthConference on Innovative Applications of Artifi-cial Intelligence (IAAI-98), pages 806-813, MenloPark, July 26-30.
AAAI Press.Dan Roth.
1999.
Learning in natural language.
InProceedings of the 16th Joint Conference on Arti-ficial Intelligence.C.
Stanfill and D. Waltz.
1986.
Toward memory-based reasoning.
Communications of the ACM,29(12):1213-1228, December.H.
van Halteren, J. Zavrel, and W. Daelemans.1998.
Improving data-driven wordclass tagging bysystem combination.
In Proceedings of the 36thAnnual Meeting of the Association for Compu-tational Linguistics, Montr'eal, Quebec, Canada,pages 491-497, Montreal, Canada, August 10-14.H.
van Halteren (ed.).
1999.
Syntactic Word-class Tagging.
Kluwer Academic Publishers, Dor-drecht, The Netherlands.24
