Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 120?127,Sydney, July 2006. c?2006 Association for Computational LinguisticsTechniques to incorporate the benefits of a Hierarchy in a modified hiddenMarkov modelLin-Yi ChouUniversity of WaikatoHamiltonNew Zealandlc55@cs.waikato.ac.nzAbstractThis paper explores techniques to take ad-vantage of the fundamental difference instructure between hidden Markov models(HMM) and hierarchical hidden Markovmodels (HHMM).
The HHMM structureallows repeated parts of the model to bemerged together.
A merged model takesadvantage of the recurring patterns withinthe hierarchy, and the clusters that exist insome sequences of observations, in orderto increase the extraction accuracy.
Thispaper also presents a new technique for re-constructing grammar rules automatically.This work builds on the idea of combininga phrase extraction method with HHMMto expose patterns within English text.
Thereconstruction is then used to simplify thecomplex structure of an HHMMThe models discussed here are evaluatedby applying them to natural language tasksbased on CoNLL-20041 and a sub-corpusof the Lancaster Treebank2.Keywords: information extraction, natu-ral language, hidden Markov models.1 IntroductionHidden Markov models (HMMs) were introducedin the late 1960s, and are widely used as a prob-abilistic tool for modeling sequences of obser-vations (Rabiner and Juang, 1986).
They haveproven to be capable of assigning semantic la-bels to tokens over a wide variety of input types.1The 2004 Conference on Computational Natural Lan-guage Learning, http://cnts.uia.ac.be/conll20042Lancaster/IBM Treebank,http://www.ilc.cnr.it/EAGLES96/synlex/node23.htmlThis is useful for text-related tasks that involvesome uncertainty, including part-of-speech tag-ging (Brill, 1995), text segmentation (Borkar etal., 2001), named entity recognition (Bikel et al,1999) and information extraction tasks (McCal-lum et al, 1999).
However, most natural languageprocessing tasks are dependent on discovering ahierarchical structure hidden within the source in-formation.
An example would be predicting se-mantic roles from English sentences.
HMMs areless capable of reliably modeling these tasks.
Incontrast hierarchical hidden Markov models (HH-MMs) are better at capturing the underlying hier-archy structure.
While there are several difficultiesinherent in extracting information from the pat-terns hidden within natural language information,by discovering the hierarchical structure more ac-curate models can be built.HHMMs were first proposed by Fine (1998)to resolve the complex multi-scale structures thatpervade natural language, such as speech (Rabinerand Juang, 1986), handwriting (Nag et al, 1986),and text.
Skounakis (2003) described the HHMMas multiple ?levels?
of HMM states, where lowerlevels represents each individual output symbol,and upper levels represents the combinations oflower level sequences.Any HHMM can be converted to a HMM bycreating a state for every possible observation,a process called ?flattening?.
Flattening is per-formed to simplify the model to a linear sequenceof Markov states, thus decreasing processing time.But as a result of this process the model no longercontains any hierarchical structure.
To reduce themodels complexity while maintaining some hier-archical structure, our algorithm uses a ?partialflattening?
process.In recent years, artificial intelligence re-120searchers have made strenuous efforts to re-produce the human interpretation of language,whereby patterns in grammar can be recognisedand simplified automatically.
Brill (1995) de-scribes a simple rule-based approach for learningby rewriting the bracketing rule?a method forpresenting the structure of natural language text?for linguistic knowledge.
Similarly, Krotov (1999)puts forward a method for eliminating redundantgrammar rules by applying a compaction algo-rithm.
This work draws upon the lessons learnedfrom these sources by automatically detecting sit-uations in which the grammar structure can be re-constructed.
This is done by applying the phraseextraction method introduced by Pantel (2001) torewrite the bracketing rule by calculating the de-pendency of each possible phrase.
The outcomeof this restructuring is to reduce the complexity ofthe hierarchical structure and reduce the numberof levels in the hierarchy.This paper considers the tasks of identifyingthe syntactic structure of text chunking and gram-mar parsing with previously annotated text doc-uments.
It analyses the use of HHMMs?bothbefore and after the application of improvementtechniques?for these tasks, then compares the re-sults with HMMs.
This paper is organised as fol-lows: Section 2 describes the method for trainingHHMMs.
Section 3 describes the flattening pro-cess for reducing the depth of hierarchical struc-ture for HHMMs.
Section 4 discusses the use ofHHMMs for the text chunking task and the gram-mar parser.
The evaluation results of the HMM,the plain HHMM and the merged and partially flat-tened HHMM are presented in Section 5.
Finally,Section 6 discusses the results.2 Hierarchical Hidden Markov ModelA HHMM is a structured multi-level stochasticprocess, and can be visualised as a tree structuredHMM (see Figure 1(b)).
There are two types ofstates:?
Production state: a leaf node of the treestructure, which contains only observations(represented in Figure 1(b) as the empty cir-cle ?).?
Internal state: contains several productionstates or other internal states (represented inFigure 1(b) as a circle with a cross inside ?
).The output of a HHMM is generated by a pro-cess of traversing some sequence of states withinthe model.
At each internal state, the automa-tion traverses down the tree, possibly through fur-ther internal states, until it encounters a productionstate where an observation is contained.
Thus, as itcontinues through the tree, the process generates asequence of observations.
The process ends whena final state is entered.
The difference between astandard HMM and a hierarchical HMM is that in-dividual states in the hierarchical model can tra-verse to a sequence of production states, whereaseach state in the standard model corresponds is aproduction state that contains a single observation.2.1 MergingAA(a)A A(b)Figure 1: Example of a HHMMFigure 1(a) and Figure 1(b) illustrate the processof reconstructing a HMM as a HHMM.
Figure 1(a)shows a HMM with 11 states.
The two dashedboxes (A) indicate regions of the model that havea repeated structure.
These regions are further-more independent of the other states in the model.Figure 1(b) models the same structure as a hier-archical HMM, where each repeated structure isnow grouped under an internal state.
This HHMMuses a two level hierarchical structure to exposemore information about the transitions and proba-bilities within the internal states.
These states, asdiscussed earlier, produce no observation of theirown.
Instead, that is left to the child productionstates within them.
Figure 1(b) shows that eachinternal state contains four production states.In some cases, different internal states of aHHMM correspond to exactly the same structurein the output sequence.
This is modelled by mak-ing them share the same sub-models.
Using aHHMM allows for the merging of repeated partsof the structure, which results in fewer states need-ing to be identified?one of the three fundamen-tal problems of HMM construction (Rabiner and121Juang, 1986).2.2 Sub-model CalculationEstimating the parameters for multi-level HH-MMs is a complicated process.
This section de-scribes a probability estimation method for inter-nal states, which transforms each internal stateinto three production states.
Each internal state Siin the HHMM is transformed by resolving eachchild production state Si,j , into one of three trans-formed states, Si ?
{s(i)in , s(i)stay, s(i)out}.
The trans-formation requires re-calculating the new observa-tional and transition probabilities for each of thesetransformed states.
Figure 2 shows the internalstates of S2 have been transformed into s(2)in , s(2)stay,s(2)stay and s(2)out.outS Sin stay stayS SS SS 1 S 3(2) (2) (2) (2)S S2,1 2,2 2,3 2,4Figure 2: Example of a transformed HHMM withthe internal state S2.The procedure to transform internal states is:I) calculate the transformed observation (O?)
foreach internal state; II) apply the forward algorithmto estimate the state probabilities (b?)
for the threetransformed states; III) reform the transition ma-trix by including estimated values for additionaltransformed internal states (A?).I.
Calculate the observation probabilities O?
:Every observation in each internal state Si isre-calculated by summing up all the observa-tion probabilities in each production state Sjas:O?i,t =Ni?j=1Oj,t, (1)where time t corresponds to a position in thesequence, O is an observation sequence overt, Oj,t is the observation probability for stateSj at time t, and Ni represents the number ofproduction states for internal state Si.II.
Apply forward algorithm to estimate thetransform observation value b?
: The trans-formed observation values are simplified to{b?
(i)in,t, b?
(i)stay,t, b?
(i)out,t}, which are then given asthe observation values for the three produc-tions states (s(i)in , s(i)stay , s(i)out).
The observa-tional probability of entering state Si at timet, i.e.
production state s(i)in , is given by:b?
(i)in,t = maxj=1..Ni[pij ?
O?j,t], (2)where pij represents the transition probabil-ities of entering child state Sj .
The secondprobability of staying in state Si at time t, i.e.production state, s(i)stay , is given by:b?
(i)stay,t = maxj=1..Ni[Aj?
?,j ?
O?j,t], (3)j?
= arg maxj=1..Ni[Aj?
?,j ?
O?j,t],where j??
is the state corresponding to j?
cal-culated at previous time t?1, and Aj?
?,j repre-sents the transition probability from state Sj?
?to state to Sj .
The third probability of exitingstate Si at time t, i.e.
production state, s(i)out,is given by:b?
(i)out,t = maxj=1..Ni[Aj?
?,j ?
O?j,t ?
?j], (4)where ?j is the transition probabilities forleaving the state Sj .III.
Reform transition probability A?
(i): Eachinternal state Si reforms a new 3 ?
3 transi-tion probability matrix A?, which records thetransition status for the transform matrix.
Theformula for the estimated cells in A?
are:A?
(i)in,stay =Ni?j=1pij (5)A?
(i)in,out =Ni?j=1pij2 (6)A?
(i)stay,stay =Ni,Ni?k=1,j=1Ak,j (7)A?
(i)stay,out =Ni?j=1?j (8)where Ni is the number of child states forstate Si, A?
(i)in,stay is estimated by summing122up all entry state probabilities for state Si,A?
(i)in,out is estimated from the observation that50% of sequences transit from state s(i)in di-rectly to state s(i)out, A?
(i)stay,stay is the sum ofall the internal transition probabilities withinstate Si, and A?
(i)stay,out is the sum of all exitstate probabilities.
The rest of the probabili-ties for transition matrix A?
are set to zero toprevent illegal transitions.Each internal state is implemented by a bottom-up algorithm using the values from equations (1)-(8), where lower levels of the hierarchy tree arecalculated first to provide information for upperlevel states.
Once all the internal states have beencalculated, the system then need only to use thetop-level of the hierarchy tree to estimate the prob-ability sequences.
This means the model will nowbecome a linear HMM for the final Viterbi searchprocess (Viterbi, 1967).3 Partial flatteningPartial flattening is a process for reducing thedepth of hierarchical structure trees.
The processinvolves moving sub-trees from one node to an-other.
This section presents an interesting auto-matic partial flattening process that makes use ofthe term extractor method (Pantel and Lin, 2001).The method discovers ways of more tightly cou-pling observation sequences within sub-modelsthus eliminating rules within the HHMM.
This re-sults in more accurate model.
This process in-volves calculating dependency values to measurethe dependency between the elements in the statesequence (or observation sequence).This method uses mutual information and log-likelihood, which Dunning (1993) used to calcu-late the dependency value between words.
Wherethere is a higher dependency value between wordsthey are more likely to be treat as a term.
The pro-cess involves collecting bigram frequencies froma large dataset, and identifying the possible twoword candidates as terms.
The first measurementused is mutual information, which is calculated us-ing the formula:mi(x, y) = P (x, y)P (x)P (y) (9)where x and y are words adjacent to each other inthe training corpus, C(x, y) to be the frequency ofthe two words, and ?
represents all the words inentire training corpus.
The log-likelihood ratio ofx and y is defined as:logL(x, y) = ll(k1n1, k1, n1) + ll(k2n2, k2, n2)?ll( k1 + k2n1 + n2, k1, n1)?ll( k1 + k2n1 + n2, k2, n2) (10)where k1 = C(x, y), n1 = C(x, ?
), k2 =C(?x, y), n2 = C(?x, ?)
andll(p, k, n) = k log(p) + (n?
k) log(1?
p) (11)The system computes dependency values betweenstates (tree nodes) or observations (tree leaves) inthe tree in the same way.
The mutual informa-tion and log-likelihood values are highest whenthe words are adjacent to each other throughoutthe entire corpus.
By using these two values,the method is more robust against low frequencyevents.Figure 3 is a tree representation of the HHMM,the figure illustrates the flattening process for thesentence:(S (N?
A AT1 graphical JJ zoo NN1 (P ?of IO (N ( strange JJ and CC peculiar JJ ) at-tractors NN2 )))).where only the part-of-speech tags and grammarinformation are considered.
The left hand side ofthe figure shows the original structure of the sen-tence, and the right hand side shows the trans-formed structure.
The model?s hierarchy is re-duced by one level, where the state P ?
has becomea sub-state of state S instead of N?.
The processis likely to be useful when state P ?
is highly de-pendent on state N?.The flattening process can be applied to themodel based on two types of sequence depen-dancy; observation dependancy and state depen-dancy.?
Observation dependency : The observationdependency value is based upon the observa-tion sequence, which in Figure 3 would bethe sequence of part-of-speech tags {AT1 JJNN1 IO JJ CC JJ NN2}.
Given observationsNN1 and IO?s as terms with a high depen-dency value, the model then re-construct thesub-tree at IO parent state P ?
moving it to thesame level as state N?, where the states of P ?and N?
now share the same parent, state S.123AT1 JJ NN1SNSNIOIOP*N* P*AT1 JJ NN1N*NN2JJ CC JJJJ CC JJ NN2Figure 3: Partial flattening process for state N?
and P ?.?
State dependency : The state dependencyvalue is based upon the state sequence, whichin Figure 3 would be {N?, P ?, N}.
The flat-tening process occurs when the current statehas a high dependency value with the previ-ous state, say N?
and P ?.term dependency valueNN1 IO 570.55IO JJ 570.55JJ CC 570.55CC JJ 570.55JJ NN2 295.24AT1 JJ 294.25JJ NN1 294.25Table 1: Observation dependency values of part-of-speech tagsThis paper determines the high dependency val-ues by selecting the top n values from a list ofall possible terms ranked by either observation orstate dependency values, where n is a parameterthat can be configured by the user for better per-formance.
Table 1 shows the observation depen-dency values of terms for part-of-speech tags forFigure 3.
The term NN1 IO has a higher depen-dency value than JJ NN1, therefore state P ?
isjoined as a sub-tree of state S. States P ?
and Nremain unchanged since state P ?
has already beenmoved up a level of the tree.
After the flatteningprocess, the state P ?
no longer belongs to the childstate of state N?, and is instead joined as the sub-tree to state S as shown in Figure 3.4 Application4.1 Text ChunkingText chunking involves producing non-overlapping segments of low-level noun groups.The system uses the clause information to con-struct the hierarchical structure of text chunks,where the clauses represent the phrases withinthe sentence.
The clauses can be embedded inother clauses but cannot overlap one another.Furthermore each clause contains one or moretext chunks.Consider a sentence from a CoNLL-2004 cor-pus:(S (NP He PRP) (VP reckons VBZ) (S (NPthe DT current JJ account NN deficit NN)(VP will MD narrow VB) (PP to TO) (NPonly RB # # 1.8 CD billion D) (PP in IN)(NP September NNP)) (O .
.
))where the part-of-speech tag associated with eachword is attached with an underscore, the clause in-formation is identified by the S symbol and thechunk information is identified by the rest of thesymbols NP (noun phrase), VP (verb phrase), PP(prepositional phrase) and O (null complemen-tizer).
The brackets are in Penn Treebank II style3.The sentence can be re-expressed just as its part-of-speech tags thusly: {PRP VBZ DT JJ NN NNMD VB TO RB # CD D IN NNP}, where onlythe part-of-speech tags and grammar informationare to be considered for the extraction tasks.
Thisis done so the system can minimise the computa-tion cost inherent in learning a large number of un-required observation symbols.
Such an approach3The Penn Treebank Project,http://www.cis.upenn.edu/?
treebank/home.html124also maximises the efficiency of trained data bylearning the pattern that is hidden within words(syntax) rather than the words themselves (seman-tics).Figure 4 represents an example of the tree repre-sentation of an HHMM for the text chunking task.This example involves a hierarchy with a depth ofthree.
Note that state NP appears in two differ-ent levels of the hierarchy.
In order to build anHHMM, the sentence shown above must be re-structured as:(S (NP PRP) (VP VBZ) (S (NP DT JJ NN NN)(VP MD VB) (PP TO) (NP RB # CD D) (PP IN)(NP NNP)) (O .
))where the model makes no use of the word infor-mation contained in the sentence.4.2 Grammar ParsingCreation of a parse tree involves describing lan-guage grammar in a tree representation, whereeach path of the tree represents a grammar rule.Consider a sentence from the Lancaster Tree-bank4:(S (N A AT1 graphical JJ zoo NN1 (P of IO(N ( strange JJ and CC peculiar JJ) attrac-tors NN2))))where the part-of-speech tag associated with eachword is attached with an underscore, and the syn-tactic tag for each phrase occurs immediately afterthe opening square-bracket.
In order to build theJJNAT1 JJ NN1 PIO NNN2N_dCCJJFigure 5: Parse tree for the HHMM4Lancaster/IBM Treebank,http://www.ilc.cnr.it/EAGLES96/synlex/node23.htmlmodels from the parse tree, the system takes thepart-of-speech tags as the observation sequences,and learns the structure of the model using the in-formation expressed by the syntactic tags.
Duringconstruction, phrases, such as the noun phrase ?
(strange JJ and CC peculiar JJ )?, are groupedunder a dummy state (N d).
Figure 5 illustratesthe model in the tree representation with the struc-ture of the model based on the previous sentencefrom Lancaster Treebank.5 EvaluationThe first evaluation presents preliminary evi-dence that the merged hierarchical hidden MarkovModel (MHHMM) is able to produce more accu-rate results either a plain HHMM or a HMM dur-ing the text chunking task.
The results suggestthat the partial flattening process is capable of im-proving model accuracy when the input data con-tains complex hierarchical structures.
The evalua-tion involves analysing the results over two sets ofdata.
The first is a selection of data from CoNLL-2004 and contains 8936 sentences.
The seconddataset is part of the Lancaster Treebank corpusand contains 1473 sentences.
Each sentence con-tains hand-labeled syntactic roles for natural lan-guage text.A.200A.400A.600A.800A.1000A.1200A.14000.860.880.900.920.94B.200B.400B.600B.800B.1000B.1200B.1400FC.200C.400C.600C.800C.1000C.1200C.1400FFigure 6: The graph of micro-average F -measureagainst the number of training sentences duringtext chunking (A: MHHMM, B: HHMM and C:HMM)The first finding is that the size of training datadramatically affects the prediction accuracy.
Amodel with an insufficient number of observations125NILSVPNPPRP VBZ NPJJ NN NNVPMDDT TOPP NPRB # CD DPPIN .OFigure 4: HHMM for syntax rolestypically has poor accuracy.
In the text chunk-ing task the number of observation symbol relieson the number of part-of-speech tags contained intraining data.
Figure 6 plots the relationship ofmicro-average F -measure for three types of mod-els (A: MHHMM, B: HHMM and C: HMM) on10-fold cross validation with the number of train-ing sentences ranging from 200 to 1400.
The re-sult shows that the MHHMM has the better per-formance in accuracy over both the HHMM andHMM, although the difference is less marked forthe latter.50 100150200020406080number of sentencessecondsA: HHMMB: HHMM?treeC: HMMFigure 7: The average processing time for textchunkingFigure 7 represents the average processing timefor testing (in seconds) for the 10-fold cross vali-dation.
The test were carried out on a dual P4-Dcomputer running at 3GHz and with 1Gb RAM.The results indicate that the MHHMM gains ef-ficiency, in terms of computation cost, by merg-ing repeated sub-models, resulting in fewer statesin the model.
In contrast the HMM has lowerefficiency as it is required to identify every sin-gle path, leading to more states within the modeland higher computation cost.
The extra costs ofconstructing a HHMM, which will have the samenumber of production states as the HMM, make itthe least efficient.The second evaluation presents preliminary ev-idence that the partially flattened hierarchical hid-den Markov model (PFHHMM) can assign propo-sitions to language texts (grammar parsing) at leastas accurately as the HMM.
This is assignment is atask that HHMMs are generally not well suited to.Table 2 shows the F1-measures of identified se-mantic roles for each different model on the Lan-caster Treebank data set.
The models used in thisevaluation were trained with observation data fromthe Lancaster Treebank training set.
The trainingset and testing set are sub-divided from the corpusin proportions of 23 and13 .
The PFHHMMs had ex-tra training conditions as follows: PFHHMM obs2000 made use of the partial flattening process,with the high dependency parameter determinedby considering the highest 2000 dependency val-ues from observation sequences from the corpus.PFHHMM state 150 again uses partial flattening,however this time the highest 150 dependency val-ues from state sequences were utilized in discover-ing the high dependency threshold.
The n valuesof 2000 and 150 were determined to be the optimalvalues when applied to the training set.The results show that applying the partial flat-tening process to a model using observation se-quences to determine high dependency values re-duces the complexity of the model?s hierarchy andconsequently improves the model?s accuracy.
Thestate dependency method is shown to be less favor-able for this particular task, but the micro-averageresult is still comparable with the HMM?s perfor-mance.
The results also show no significant re-126State Count HMM HHMM PFHHMM PFHHMMobs state2000 150N 16387 0.874 0.811 0.882 0.874NULL 4670 0.794 0.035 0.744 0.743V 4134 0.768 0.755 0.804 0.791P 2099 0.944 0.936 0.928 0.926Fa 180 0.525 0.814 0.687 0.457Micro-Average 0.793 0.701 0.809 0.792Table 2: F1-measure of top 5 states during grammar parsingset.lationship between the occurance count of a stateagainst the various models prediction accuracy.6 Discussion and Future WorkDue to the hierarchical structure of a HHMM, themodel has the advantage of being able to reuseinformation for repeated sub-models.
Thus theHHMM can perform more accurately and requiresless computational time than the HMM in certainsituations.The merging and flattening techniques havebeen shown to be effective and could be appliedto many kinds of data with hierarchical structures.The methods are especially appealing where themodel involves complex structure or there is ashortage of training data.
Furthermore, they ad-dress an important issue when dealing with smalldatasets: by using the hierarchical model to un-cover less obvious structures, the model is ableto increase model performance even over morelimited source materials.
The experimental re-sults have shown the potential of the merging andpartial flattening techniques in building hierarchi-cal models and providing better handling of stateswith less observation counts.
Further research inboth experimental and theoretical aspects of thiswork is planned, specifically in the area of recon-structing hierarchies where recursive formationsare present and formal analysis and testing of tech-niques.ReferencesD.
M. Bikel, R. Schwartz and R. M. Weischedel.
1999.An Algorithm that Learns What?s in a Name.
Ma-chine Learning, 34:211?231.V.
R. Borkar, K. Deshmukh and S. Sarawagi.
2001.Automatic Segmentation of Text into StructuredRecords.
Proceedings of SIGMOD.E.
Brill.
1995.
Transformation-based error-drivenlearning and natural language processing: a casestudy in part-of-speech tagging.
Computational Lin-guistics, 21(4):543?565.T.
Dunning.
1993.
Accurate methods for the statisticsof surprise and coincidence.
Computational Lin-guistics, 19(1):61?74.S.
Fine , Y.
Singer and N. Tishby.
1998.
The Hierar-chical Hidden Markov Model: Analysis and Appli-cations.
Machine Learning, 32:41?62.A.
Krotov, M Heple, R. Gaizauskas and Y. Wilks.1999.
Compacting the Penn Treebank Grammar.Proceedings of COLING-98 (Montreal), pages 699-703.A.
McCallum, K. Nigam, J. Rennie and K. Sey-more.
1999.
Building Domain-Specific Search En-gines with Machine Learning Techniques.
In AAAI-99 Spring Symposium on Intelligent Agents in Cy-berspace.R.
Nag, K. H. Wong, and F. Fallside.
1986.
ScriptRecognition Using Hidden Markov Models.
Proc.of ICASSP 86, pp.
2071-1074, Toyko.P.
Pantel and D. Lin.
2001.
A Statistical Corpus-Based Term Extractor.
In Stroulia, E. and Matwin,S.
(Eds.)
AI 2001.
Lecture Notes in Artificial Intel-ligence, pp.
36-46.
Springer-Verlag.L.
R. Rabiner and B. H. Juang.
1986.
An Introductionto Hidden Markov Models.
IEEE Acoustics Speechand Signal Processing ASSP Magazine, ASSP-3(1):4?16, January.M.
Skounakis, M. Craven and S. Ray.
2003.
Hi-erarchical Hidden Markov Models for InformationExtraction.
In Proceedings of the 18th Interna-tional Joint Conference on Artificial Intelligence,Acapulco, Mexico, Morgan Kaufmann.A.
J. Viterbi.
1967.
Error bounds for convolutionalcodes and an asymtotically optimum decoding algo-rithm.
IEEE Transactions on Information Theory,IT-13:260?267.127
