TextGraphs-2: Graph-Based Algorithms for Natural Language Processing, pages 25?32,Rochester, April 2007 c?2007 Association for Computational LinguisticsTimestamped Graphs: Evolutionary Models of Text forMulti-document SummarizationZiheng Lin, Min-Yen KanSchool of ComputingNational University of SingaporeSingapore 177543{linzihen, kanmy}@comp.nus.edu.sgAbstractCurrent graph-based approaches to auto-matic text summarization, such as Le-xRank and TextRank, assume a staticgraph which does not model how the in-put texts emerge.
A suitable evolutionarytext graph model may impart a better un-derstanding of the texts and improve thesummarization process.
We propose atimestamped graph (TSG) model that ismotivated by human writing and readingprocesses, and show how text units in thismodel emerge over time.
In our model,the graphs used by LexRank and Tex-tRank are specific instances of our time-stamped graph with particular parametersettings.
We apply timestamped graphs onthe standard DUC multi-document textsummarization task and achieve compara-ble results to the state of the art.1 IntroductionGraph-based ranking algorithms such asKleinberg?s HITS (Kleinberg, 1999) or Google?sPageRank (Brin and Page, 1998) have been suc-cessfully applied in citation network analysis andranking of webpages.
These algorithms essentiallydecide the weights of graph nodes based on globaltopological information.
Recently, a number ofgraph-based approaches have been suggested forNLP applications.
Erkan and Radev (2004) intro-duced LexRank for multi-document text summari-zation.
Mihalcea and Tarau (2004) introducedTextRank for keyword and sentence extractions.Both LexRank and TextRank assume a fully con-nected, undirected graph, with text units as nodesand similarity as edges.
After graph construction,both algorithms use a random walk on the graph toredistribute the node weights.Many graph-based algorithms feature an evolu-tionary model, in which the graph changes overtimesteps.
An example is a citation network whoseedges point backward in time: papers (usually)only reference older published works.
Referencesin old papers are static and are not updated.
Simplemodels of Web growth are exemples of this: theymodel the chronological evolution of the Web inwhich a new webpage must be linked by an incom-ing edge in order to be publicly accessible and mayembed links to existing webpages.
These modelsdiffer in that they allow links in previously gener-ated webpages to be updated or rewired.
However,existing graph models for summarization ?LexRank and TextRank ?
assume a static graph,and do not model how the input texts evolve.
Thecentral hypothesis of this paper is that modelingthe evolution of input texts may improve the sub-sequent summarization process.
Such a model maybe based on human writing/reading process andshould show how just composed/consumed units oftext relate to previous ones.
By applying thismodel over a series of timesteps, we obtain a rep-resentation of how information flows in the con-struction of the document set and leverage this toconstruct automatic summaries.We first introduce and formalize our timestam-ped graph model in next section.
In particular, ourformalization subsumes previous works: we showin Section 3 that the graphs used by LexRank andTextRank are specific instances of our timestam-ped graph.
In Section 4, we discuss how the result-ing graphs are applied to automatic multi-document text summarization: by counting nodein-degree or applying a random walk algorithm tosmooth the information flow.
We apply these mod-els to create an extractive summarization program25and apply it to the standard Document Understand-ing Conference (DUC) datasets.
We discuss theresulting performance in Section 5.2 Timestamped GraphWe believe that a proper evolutionary graph modelof text should capture the writing and readingprocesses of humans.
Although such human proc-esses vary widely, when we limit ourselves to ex-pository text, we find that both skilled writers andreaders often follow conventional rhetorical styles(Endres-Niggemeyer, 1998; Liddy, 1991).
In thiswork, we explore how a simple model of evolutionaffects graph construction and subsequent summa-rization.
In this paper, our work is only exploratoryand not meant to realistically model human proc-esses and we believe that deep understanding andinference of rhetorical styles (Mann and Thompson,1988) will improve the fidelity of our model.
Nev-ertheless, a simple model is a good starting point.We make two simple assumptions:1: Writers write articles from the first sentenceto the last;2: Readers read articles from the first sentenceto the last.The assumptions suggest that we add sentencesinto the graph in chronological order: we add thefirst sentence, followed by the second sentence,and so forth, until the last sentence is added.These assumptions are suitable in modeling thegrowth of individual documents.
However whendealing with multi-document input (common inDUC), our assumptions do not lead to a straight-forward model as to which sentences should ap-pear in the graph before others.
One simple way isto treat multi-document problems simply as multi-ple instances of the single document problem,which evolve in parallel.
Thus, in multi-documentgraphs, we add a sentence from each document inthe input set into the graph at each timestep.
Ourmodel introduces a skew variable to model this andother possible variations, which is detailed later.The pseudocode in Figure 1 summarizes howwe build a timestamped graph for multi-documentinput set.
Informally, we build the graph itera-tively, introducing new sentence(s) as node(s) inthe graph at each timestep.
Next, all sentences inthe graph pick other previously unconnected onesto draw a directed edge to.
This process continuesuntil all sentences are placed into the graph.Figure 2 shows this graph building process inmid-growth, where documents are arranged in col-umns, with dx represents the xth document and syrepresents the yth sentence of each document.
Thebottom shows the nth sentences of all m documentsbeing added simultaneously to the graph.
Each newnode can either connect to a node in the existinggraph or one of the other m-1 new nodes.
Eachexisting node can connect to another existing nodeor to one of the m newly-introduced nodes.
Notethat this model differs from the citation networksin such that new outgoing edges are introduced toold nodes, and differs from previous models forWeb growth as it does not require new nodes tohave incoming edges.Figure 2: Snapshot of a timestamped graph.Figure 3 shows an example of the graph buildingprocess over three timesteps, starting from anempty graph.
Assume that we have three docu-ments and each document has three sentences.
Letdxsy indicate the yth sentence in the xth document.At timestep 1, sentences d1s1, d2s1 and d3s1 ares1s2s3.....snd1  d2  d3  ????
dmexistinggraphFigure 1: Pseudocode for a specific instance of atimestamped graph algorithmInput:  M, a cluster of m documents relating to acommon event;Let: i = index to sentences, initially 1;G = the timestamped graph, initially empty.Step 1:  Add the ith sentence of all documents into G.Step 2:  Let each existing sentence in G choose andconnect to one other existing sentence in G.The chosen sentence must be sentence whichhas not been previously chosen by this sentencein previous iterations.Step 3:  if there are no new sentences to add, break;else i++, goto Step 1.Output:  G, a timestamped graph.m newsentences26added to the graph.
Three edges are introduced tothe graph, in which the edges are chosen by somestrategy; perhaps by choosing the candidate sen-tence by its maximum cosine similarity with thesentence under consideration.
Let us say that thisprocess connects d1s1?d3s1, d2s1?d3s1 andd3s1?d2s1.
At timestep 2, sentences d1s2, d2s2 andd3s2 are added to the graph and six new edges areintroduced to the graph.
At timestep 3, sentencesd1s3, d2s3 and d3s3 are added to the graph, and ninenew edges are introduced.
(a) Timestep 1           (b) Timestep 2            (c) Timestep 3Figure 3: An example of the growth of atimestamped graph.The above illustration is just one instance of atimestamped graph with specific parameter settings.We generalize and formalize the timestampedgraph algorithm as follows:Definition: A timestamped graph algorithmtsg(M) is a 9-tuple (d, e, u, f, ?, t, i, s, ?)
that speci-fies a resulting algorithm that takes as input the setof texts M and outputs a graph G, where:d  specifies the direction of the edges, d?
{f, b, u};e  is the number of edges to add for each vertexin G at each timestep, e??
+;u  is 0 or 1, where 0 and 1 specifies unweighted andweighted edges, respectively;f  is the inter-document factor, 0 ?
f ?
1;?
is a vertex selection function ?
(u, G) that takesin a vertex u and G, and chooses a vertex v?G;t  is the type of text units, t?
{word, phrase,sentence, paragraph, document};i  is the node increment factor, i??
+;s  is the skew degree, s ?
-1 and s??
, where -1represent free skew and 0 no skew;?
is a document segmentation function ?(?
).In the TSG model, the first set of parameters d,e, u, f deal with the properties of edges; ?, t, i, sdeal with properties of nodes; finally, ?
is a func-tion that modifies input texts.
We now discuss thefirst eight parameters; the relevance of ?
will beexpanded upon later in the paper.2.1 Edge SettingsWe can specify the direction of information flowby setting different d values.
When a node v1chooses another node v2 to connect to, we set d to fto represent a forward (outgoing) edge.
We saythat v1 propagates some of its information into v2.When letting a node v1 choose another node v2 toconnect to v1 itself, we set d to b to represent abackward (incoming) edge, and we say that v1 re-ceives some information from v2.
Similarly, d = uspecifies undirected edges in which informationpropagates in both directions.
The larger amount ofinformation a node receives from other nodes, thehigher the importance of this node.Our toy example in Figure 3 has small dimen-sions: three sentences for each of three documents.Experimental document clusters often have muchlarger dimensions.
In DUC, clusters routinely con-tain over 25 documents, and the average length fordocuments can be as large as 50 sentences.
In suchcases, if we introduce one edge for each node ateach timestep, the resulting graph is loosely con-nected.
We let e be the number of outgoing edgesfor each sentence in the graph at each timestep.
Tointroduce more edges into the graph, we increase e.We can also incorporate unweighted orweighted edges into the graph by specifying thevalue of u. Unweighted edges are good when rank-ing algorithms based on in-degree of nodes areused.
However, unlike links between webpages,edges between text units often have weights to in-dicate connection strength.
In these cases, un-weighted edges lose information and a weightedrepresentation may be better, such as in caseswhere PageRank-like algorithms are used for rank-ing.Edges can represent information flow from onenode to another.
We may prefer intra-documentedges over inter-document edges, to model the in-tuition that information flows within the samedocument more likely than across documents.
Thuswe introduce an inter-document factor f, where 0 ?f ?
1.
When this feature is smaller than 1, we re-place the weight w for inter-document edges by fw.272.2 Node SettingsIn Figure 1 Step 2, every existing node has achance to choose another existing node to connectto.
Which node to choose is decided by the selec-tion strategy ?.
One strategy is to choose the nodewith the highest similarity.
There are many similar-ity functions to use, including token-based Jaccardsimilarity, cosine similarity, or more complexmodels such as concept links (Ye et al, 2005).t controls the type of text unit that representsnodes.
Depending on the application, text units canbe words, phrases, sentences, paragraphs or evendocuments.
In the task of automatic text summari-zation, systems are conveniently assessed by let-ting text units be sentences.i controls the number of sentences entering thegraph at every iteration.
Certain models, such asLexRank, introduce all of the input sentences inone time step (i.e., i = Lmax, where Lmax is themaximum length of the input documents), com-pleting the construction of G in one step.
However,to model time evolution, i needs to be set to a valuesmaller than this.Most relevant to our study is the skew parame-ter s. Up to now, the TSG models discussed allassume that authors start writing all documents inthe input set at the same time.
It is reflected byadding the first sentences of all documents simul-taneously.
However in reality, some documents areauthored later than others, giving updates or report-ing changes to events reported earlier.
In DUCdocument clusters, news articles are typically takenfrom two or three different newswire sources.
Theyreport on a common event and thus follow a story-line.
A news article usually gives summary aboutwhat have been reported in early articles, and givesupdates or changes on the same event.To model this, we arrange the documents in ac-cordance with the publishing time of the docu-ments.
The earliest document is assigned tocolumn 1, the second earliest document to column2, and so forth, until the latest document is as-signed to the last column.
The graph constructionprocess is the same as before, except that we delayadding the first sentences of later documents until aproper iteration, governed by s. With s = 1, we de-lay the addition of the first sentence of column 2until the second timestep, and delay the addition ofthe first sentence of column 3 until the thirdtimestep.
The resulting timestamped graph isskewed by 1 timestep (Figure 4 (a)).
We can in-crease the skew degree s if the time intervals be-tween publishing time of documents are large.Figure 4 (b) shows a timestamped graph skewed by2 timesteps.
We can also skew a graph freely bysetting s to -1.
When we start to add the first sen-tence dis1 of a document di, we check whether thereare existing sentences in the graph that want toconnect to dis1 (i.e., that ?
(?,G) = dis1).
If there is,we add dis1 to the graph; else we delay the additionand reassess again in next timestep.
The result is afreely skewed graph (Figure 4 (c)).
In Figure 4 (c),we start adding the first sentences of documents d2to d4 at timesteps 2, 5 and 7, respectively.
Attimestep 1, d1s1 is added into the graph.
Attimestep 2, an existing node (d1s1 in this case)wants to connect to d2s1, so d2s1 is added.
d3s1 isadded at timestep 5 as no existing node wants toconnect to d3s1 until timestep 5.
Similarly, d4s1 isadded until some nodes choose to connect to it attimestep 7.
Notice that we hide edges in Figure 4for clarity.
(a) Skewed by 1         (b) Skewed by 2      (c) Freely skewedFigure 4: Skewing the graphs.
Edges are hidden for clarity.For each graph, the leftmost column is the earliest document.Documents are then chronologically ordered, with the right-most one being the latest.3 Comparison and Properties of TSGThe TSG representation generalizes many pos-sible specific algorithm configurations.
As such, itis natural that previous works can be cast as spe-cific instances of a TSG.
For example, we can suc-cinctly represent the algorithm used in the runningexample in Section 2 as the tuple (f, 1, 0, 1, max-cosine-based, sentence, 1, 0, null).
LexRank andTextRank can also be cast as TSGs: (u, N, 1, 1,cosine-based, sentence, Lmax, 0, null) and (u, L, 1, 1,modified-co-occurrence-based, sentence, L, 0,28null).
As LexRank is applied in multi-documentsummarizations, e is set to the total number of sen-tences in the cluster, N, and i is set to the maxi-mum document length in the cluster, Lmax.TextRank is applied in single-document summari-zation, so both its e and i are set to the length of theinput document, L. This compact notation empha-sizes the salient differences between these two al-gorithm variants: namely that, e, ?
and i.Despite all of these possible variations, alltimestamped graphs have two important features,regardless of their specific parameter settings.
First,nodes that were added early have more chosenedges than nodes added later, as visible in Figure 3(c).
If forward edges (d = f) represent informationflow from one node to another, we can say thatmore information is flowing from these earlynodes to the rest of the graph.
The intuition for thisis that, during the writing process of articles, earlysentences have a greater influence to the develop-ment of the articles?
ideas; similarly, during thereading process, sentences that appear early con-tribute more to the understanding of the articles.The fact that early nodes stay in the graph for alonger time leads to the second feature: early nodesmay attract more edges from other nodes, as theyhave larger chance to be chosen and connected byother nodes.
This is also intuitive for forwardedges (d = f): during the writing process, later sen-tences refer back to early sentences more oftenthan vice versa; and during the reading process,readers tend to re-read early sentences when theyare not able to understand the current sentence.4 Random WalkOnce a timestamped graph is built, we want tocompute an importance score for each node.
Thesescores are then used to determine which nodes(sentences) are the most important to extract sum-maries from.
The graph G shows how informationflows from node to node, but we have yet to let theinformation actually flow.
One method to do this isto use the in-degree of each node as the score.However, most graph algorithms now use an itera-tive method that allows the weights of the nodesredistribute until stability is reached.
One methodfor this is by applying a random walk, used in Pag-eRank (Brin and Page, 1998).
In PageRank theWeb is treated as a graph of webpages connectedby links.
It assumes users start from a randomwebpage, moving from page to page by followingthe links.
Each user follows the links at randomuntil he gets ?bored?
and jumps to a random web-page.
The probability of a user visiting a webpageis then proportional to its PageRank score.
PageR-ank can be iteratively computed by:??
?+=)()()(1)1()(uInvvPRvOutNuPR ??
(1)where N is the total number of nodes in the graph,In(u) is the set of nodes that point to u, and Out(u)is the set of nodes that node u points to.
?
is adamping factor that can be set between 0 and 1,which has the role of integrating into the model theprobability of jumping from a given node to an-other random node in the graph.
In the context ofweb surfing, a user either clicks on a link on thecurrent page at random with probability 1 - ?, oropens a completely new random page with prob-ability ?.Equation 1 does not take into consideration theweights of edges, as the original PageRank defini-tion assumes hyperlinks are unweighted.
Thus wecan use Equation 1 to rank nodes for an un-weighted timestamped graph.
To integrate edgeweights into the graph, we modify Eq.
1, yielding:?
???
?+=)()()()1()(uInvvOutxvxvu vPRwwNuPR ??
(2)where Wvu represents the weight of the edge point-ing from v to u.As we may have a query for each documentcluster, we also wish to take queries into consid-eration in ranking the nodes.
Haveliwala (2003)introduces a topic-sensitive PageRank computation.Equations 1 and 2 assume a random walker jumpsfrom the current node to a random node with prob-ability ?.
The key to creating topic-sensitive Pag-eRank is that we can bias the computation byrestricting the user to jump only to a random nodewhich has non-zero similarity with the query.
Ot-terbacher et al (2005) gives an equation for topic-sensitive and weighted PageRank as:?
??
???
?+=)()()()1(),(),()(uInvvOutxvxvuSyvPRwwQysimQusimuPR ??
(3)29where S is the set of all nodes in the graph, andsim(u, Q) is the similarity score between node uand the query Q.5 Experiments and ResultsWe have generalized and formalized evolutionarytimestamped graph model.
We want to apply it onautomatic text summarization to confirm that theseevolutionary models help in extracting importantsentences.
However, the parameter space is toolarge to test all possible TSG algorithms.
We con-duct experiments to focus on the following re-search questions that relating to 3 TSG parameters- e, u and s, and the topic-sensitivity of PageRank.Q1: Do different e values affect the summariza-tion process?Q2: How do topic-sensitivity and edge weight-ing perform in running PageRank?Q3: How does skewing the graph affect infor-mation flow in the graph?The datasets we use are DUC 2005 and 2006.These datasets both consist of 50 document clus-ters.
Each cluster consists of 25 news articleswhich are taken from two or three different news-wire sources and are relating to a common event,and a query which contains a topic for the clusterand a sequence of statements or questions.
Thefirst three experiments are run on DUC 2006, andthe last experiment is run on DUC 2005.In the first experiment, we analyze how e, thenumber of chosen edges for each node at eachtimestep, affects the performance, with other pa-rameters fixed.
Specifically the TSG algorithm weuse is the tuple (f, e, 1, 1, max-cosine-based, sen-tence, 1, 0, null), where e is being tested for differ-ent values.
The node selection function max-cosine-based takes in a sentence  s and the currentgraph G, computes the TFIDF-based cosine simi-larities between s and other sentences in G, andconnects s to e sentence(s) that has(have) the high-est cosine score(s) and is(are) not yet chosen by sin previous iterations.
We run topic-sensitive Pag-eRank with damping factor ?
set to 0.5 on thegraphs.
Figures 5 (a)-(b) shows the ROUGE-1 andROUGE-2 scores with e set to 1, 2, 3, 4, 5, 6, 7, 10,15, 20 and N, where N is the total number of sen-tences in the cluster.
We succinctly representLexRank graphs by the tuple (u, N, 1, 1, cosine-based, sentence, Lmax, 0, null) in Section 3; it canalso be represented by a slightly different tuple (f,N, 1, 1, max-cosine-based, sentence, 1, 0, null).
Itdiffers from the first representation in that we itera-tively add 1 sentence for each document in eachtimestep and let al nodes in the current graph con-nect to every other node in the graph.
In this ex-periment, when e is set to N, the timestampedgraph is equivalent to a LexRank graph.
We do notuse any reranker in this experiment.NNFigure 5: (a) ROUGE-1 and (b) ROUGE-2 scores fortimestamped graphs with different e settings.
N is the totalnumber of sentences in the cluster.The results allow us to make several observa-tions.
First, when e = 2, the system gives the bestperformance, with ROUGE-1 score 0.37728 andROUGE-2 score 0.07692.
Some values of e givebetter scores than LexRank graph configuration, inwhich e = N. Second, the system gives very badperformance when e = 1.
This is because when e isset to 1, the graph is too loosely connected and isnot suitable to apply random walk on it.
Third, thesystem gives similar performance when e is set30greater than 10.
The reason for this is that thehigher values of e make the graph converge to afully connected graph so that the performancestarts to converge and display less variability.We run a second experiment to analyze howtopic-sensitivity and edge weighting affect the sys-tem performance.
We use concept links (Ye et al,2005) as the similarity function and a MMRreranker to remove redundancy.
Table 1 shows theresults.
We observe that both topic-sensitive Pag-eRank and weighted edges perform better than ge-neric PageRank on unweighted timestampedgraphs.
When topic-sensitivity and edge weightingare both set to true, the system gives the best per-formance.Topic-sensitiveWeightededgesROUGE-1 ROUGE-2No No 0.39358 0.07690Yes No 0.39443 0.07838No Yes 0.39823 0.08072Yes Yes 0.39845 0.08282Table 1: ROUGE-1 and ROUGE-2 scores for different com-binations of topic-sensitivity and edge weighting(u) settings.To evaluate how skew degree s affects summa-rization performance, we use the parameter settingfrom the first experiment, with e fixed to 1.
Spe-cifically, we use the tuple (f, 1, 1, 1, concept-link-based, sentence, 1, s, null), with s set to 0, 1 and 2.Table 2 gives the evaluation results.
We observethat s = 1 gives the best ROUGE-1 and ROUGE-2scores.
Compared to the system without skewing (s= 0), s = 2 gives slightly better ROUGE-1 scorebut worse ROUGE-2 score.
The reason for this isthat s = 2 introduces a delay interval that is toolarge.
We expect that a freely skewed graph (s =-1) will give more reasonable delay intervals.Skew degree ROUGE-1 ROUGE-20 0.36982 0.075801 0.37268 0.076822 0.36998 0.07489Table 2: ROUGE-1 and ROUGE-2 scores fordifferent skew degrees.We tune the system using different combina-tions of parameters, and the TSG algorithm withtuple (f, 1, 1, 1, concept-link-based, sentence, 1, 0,null) gives the best scores.
We run this TSG algo-rithm with topic-sensitive PageRank and MMRreranker on DUC 2005 dataset.
The results showthat our system ranks third in both ROUGE-2 andROUGE-SU4 scores.Rank System ROUGE-2 System ROUGE-SU41 15 0.0725 15 0.13162 17 0.0717 17 0.12973 TSG 0.0712 TSG 0.12854 10 0.0698 8 0.12795 8 0.0696 4 0.1277Table 3: top ROUGE-2 and ROUGE-SU4scores in DUC 2005.
TSG is our system.6 DiscussionA closer inspection of the experimental clustersreveals one problem.
Clusters that consist ofdocuments that are of similar lengths tend to per-form better than those that contain extremely longdocuments.
The reason is that a very long docu-ment introduces too many edges into the graph.Ideally we want to have documents with similarlengths in a cluster.
One solution to this is that wesplit long documents into shorter documents withappropriate lengths.
We introduce the last parame-ter in the formal definition of timestamped graphs,?, which is a document segmentation function ?(?).?
(M) takes in as input a set of documents M, ap-plies segmentation on long documents to split theminto shorter documents, and output a set of docu-ments with similar lengths, M?.
Slightly better re-sults are achieved when a segmentation function isapplied.
One shortcoming of applying ?(?)
is thatwhen a document is split into two shorter ones, theearly sentences of the second half now come be-fore the later sentences of the first half, and thismay introduce inconsistencies in our representation:early sentences of the second half contribute moreinto later sentences of the first half than the viceversa.7 Related WorksDorogovtsev and Mendes (2001) suggest schemesof the growth of citation networks and the Web,which are similar to the construction process oftimestamped graphs.Erkan and Radev (2004) proposed LexRank todefine sentence importance based on graph-basedcentrality ranking of sentences.
They construct asimilarity graph where the cosine similarity of eachpair of sentences is computed.
They introducethree different methods for computing centrality in31similarity graphs.
Degree centrality is defined asthe in-degree of vertices after removing edgeswhich have cosine similarity below a pre-definedthreshold.
LexRank with threshold is the secondmethod that applies random walk on an un-weighted similarity graph after removing edgesbelow a pre-defined threshold.
Continuous Le-xRank is the last method that applies random walkon a fully connected, weighted similarity graph.LexRank has been applied on multi-document textsummarization task in DUC 2004, and topic-sensitive LexRank has been applied on the sametask in DUC 2006.Mihalcea and Tarau (2004) independently pro-posed another similar graph-based random walkmodel, TextRank.
TextRank is applied on keywordextraction and single-document summarization.Mihalcea, Tarau and Figa (2004) later applied Pag-eRank to word sense disambiguation.8 ConclusionWe have proposed a timestamped graph modelwhich is motivated by human writing and readingprocesses.
We believe that a suitable evolutionarytext graph which changes over timesteps captureshow information propagates in the text graph.
Ex-perimental results on the multi-document textsummarization task of DUC 2006 showed thatwhen e is set to 2 with other parameters fixed, orwhen s is set to 1 with other parameters fixed, thegraph gives the best performance.
It also showedthat topic-sensitive PageRank and weighted edgesimprove summarization process.
This work alsounifies representations of graph-based summariza-tion, including LexRank and TextRank, modelingthese prior works as specific instances of time-stamped graphs.We are currently looking further on skewedtimestamped graphs.
Particularly we want to lookat how a freely skewed graph propagates informa-tion.
We are also analyzing in-degree distributionof timestamped graphs.AcknowledgmentsThe authors would like to thank Prof. Wee Sun Leefor his very helpful comments on random walk andthe construction process of timestamped graphs,and thank Xinyi Yin (Yin, 2007) for his help inspearheading the development of this work.
Wealso would like to thank the reviewers for theirhelpful suggestions in directing the future of thiswork.ReferencesJon M. Kleinberg.
1999.
Authoritative sources in a hy-perlinked environment.
In Proceedings of ACM-SIAM Symposium on Discrete Algorithms, 1999.Sergey Brin and Lawrence Page.
1998.
The anatomy ofa large-scale hypertextual Web search engine.
Com-puter Networks and ISDN Systems, 30(1-7).G?nes Erkan and Dragomir R. Radev.
2004.
LexRank:Graph-based centrality as salience in text summari-zation.
Journal of Artificial Intelligence Research,(22).Rada Mihalcea and Paul Tarau.
2004.
TextRank: Bring-ing order into texts.
In Proceedings of EMNLP 2004.Rada Mihalcea, Paul Tarau, and Elizabeth Figa.
2004.PageRank on semantic networks, with application toword sense disambiguation.
In Proceedings ofCOLING 2004.S.N.
Dorogovtsev and J.F.F.
Mendes.
2001.
Evolutionof networks.
Submitted to Advances in Physics on6th March 2001.Shiren Ye, Long Qiu, Tat-Seng Chua, and Min-YenKan.
2005.
NUS at DUC 2005: Understanding docu-ments via concepts links.
In Proceedings of DUC2005.Xinyi Yin, 2007.
Random walk and web informationprocessing for mobile devices.
PhD Thesis.Taher H. Haveliwala.
2003.
Topic-sensitive pagerank: Acontext-sensitive ranking algorithm for web search.IEEE Transactions on Knowledge and Data Engi-neeringJahna Otterbacher, G?nes Erkan and Dragomir R.Radev.
2005.
Using Random Walks for Question-focused Sentence Retrieval.
In Proceedings ofHLT/EMNLP 2005.Brigitte Endres-Niggemeyer.
1998.
Summarizing infor-mation.
Springer New York.Elizabeth D. Liddy.
1991.
The discourse-level structureof empirical abstracts: an exploratory study.
Infor-mation Processing and Management 27(1):55-81.William C. Mann and Sandra A. Thompson.
1988.
Rhe-torical structure theory: Towards a functional theoryof text organization.
Text 8(3): 243-281.32
