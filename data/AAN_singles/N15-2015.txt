Proceedings of NAACL-HLT 2015 Student Research Workshop (SRW), pages 110?117,Denver, Colorado, June 1, 2015.c?2015 Association for Computational LinguisticsImproving the Translation of Discourse Markers for Chinese into EnglishDavid SteeleDepartment Of Computer ScienceThe University of SheffieldSheffield, UKdbsteele1@sheffield.ac.ukAbstractDiscourse markers (DMs) are ubiquitous co-hesive devices used to connect what is saidor written.
However, across languages thereis divergence in their usage, placement, andfrequency, which is considered to be a majorproblem for machine translation (MT).
Thispaper presents an overview of a proposed the-sis, exploring the difficulties around DMs inMT, with a focus on Chinese and English.The thesis will examine two main areas: mod-elling cohesive devices within sentences andmodelling discourse relations (DRs) acrosssentences.
Initial experiments have shownpromising results for building a predictionmodel that uses linguistically inspired featuresto help improve word alignments with respectto the implicit use of cohesive devices, whichin turn leads to improved hierarchical phrase-based MT.1 IntroductionStatistical Machine Translation (SMT) has, in re-cent years, seen substantial improvements, yet ap-proaches are not able to achieve high quality trans-lations in many cases.
The problem is especiallyprominent with complex composite sentences anddistant language pairs, largely due to computa-tional complexity.
Rather than considering largerdiscourse segments as a whole, current SMT ap-proaches focus on the translation of single sentencesindependently, with clauses and short phrases beingtreated in isolation.
DMs are seen as a vital con-textual link between discourse segments and couldbe used to guide translations in order to improveaccuracy.
However, they are often translated intothe target language in ways that differ from howthey are used in the source language (Hardmeier,2012a; Meyer and Popescu-Belis, 2012).
DMscan also signal numerous DRs and current SMTapproaches do not adequately recognise or distin-guish between them during the translation process(Hajlaoni and Popescu-Belis, 2013).
Recent devel-opments in SMT potentially allow the modellingof wider discourse information, even across sen-tences (Hardmeier, 2012b), but currently most exist-ing models appear to focus on producing well trans-lated localised sentence fragments, largely ignoringthe wider global cohesion.Five distinct cohesive devices have been identified(Halliday and Hasan, 1976), but for this thesis thepertinent devices that will be examined are conjunc-tion (DMs) and (endophoric) reference.
Conjunc-tion is pertinent as it encompasses DMs, whilst ref-erence includes pronouns (amongst other elements),which are often connected with the use of DMs (e.g.
?Because John ..., therefore he ...?
).The initial focus is on the importance of DMswithin sentences, with special attention given to im-plicit markers (common in Chinese) and a numberof related word alignment issues.
However, the finalthesis will cover two main areas:?
Modelling cohesive devices within sentences?
Modelling discourse relations across sentencesand wider discourse segments.This paper is organized as follows.
In Section 2a survey of related work is conducted.
Section 3110outlines the initial motivation and research includ-ing a preliminary corpus analysis.
It covers exam-ples that highlight various problems with the transla-tion of (implicit) DMs, leading to an initial intuition.Section 4 looks at experiments and word alignmentissues following a deeper corpus analysis and dis-cusses how the intuition led towards developing themethodology used to study and improve word align-ments.
It also includes the results of the experimentsthat show positive gains in BLEU.
Section 5 pro-vides an outline of the future work that needs to becarried out.
Finally, Section 6 is the conclusion.2 Literature ReviewThis section is a brief overview of some of the per-tinent important work that has gone into improvingSMT with respect to cohesion.
Specifically the focusis on the areas of: identifying and annotating DMs,working with lexical and grammatical cohesion, andtranslating implicit DRs.2.1 Identifying and Annotating Chinese DMsA study on translating English discourse connectives(DCs) (Hajlaoni and Popescu-Belis, 2013) showedthat some of them in English can be ambiguous, sig-nalling a variety of discourse relations.
However,other studies have shown that sense labels can beincluded in corpora and that MT systems can takeadvantage of such labels to learn better translations(Pitler and Nenkova, 2009; Meyer and Popescu-Belis, 2012).
For example, The Penn DiscourseTreebank project (PDTB) adds annotation relatedto structure and discourse semantics with a focuson DRs and can be used to guide the extraction ofDR inferences.
The Chinese Discourse Treebank(CDTB) adds an extra layer to the annotation inthe PDTB (Xue, 2005) focussing on DCs as wellas structural and anaphoric relations and follows thelexically grounded approach of the PDTB.The studies also highlight how anaphoric relationscan be difficult to capture as they often have one dis-course adverbial linked with a local argument, leav-ing the other argument to be established from else-where in the discourse.
Pronouns, for example, areoften used to link back to some discourse entity thathas already been introduced.
This essentially sug-gests that arguments identified in anaphoric relationsEnglish Chinese DCalthough(1)/but(2)(1)???????(2)??????because(1)/therefore(2)(1)???????(2)??if(1)/then(2)(1)???????
(2)?Table 1: Examples of Interchangeable DMs.can cover a long distance and Xue (2005) argues thatone of the biggest challenges for discourse annota-tion is establishing the distance of the text span andhow to decide on what discourse unit should be in-cluded or excluded from the argument.There are also some additional challenges suchas variants or substitutions of DCs.
Table 1 (Xue,2005) shows a range of DCs that can be used inter-changeably.
The numbers indicate that any markerfrom (1) can be paired with any marker from (2) toform a compound sentence with the same meaning.2.2 Lexical and Grammatical CohesionPrevious work has attempted to address lexical andgrammatical cohesion in SMT (Gong et al, 2011;Xiao et al, 2011; Wong and Kit, 2012; Xiong et al,2013b) although their results are still relatively lim-ited (Xiong et al, 2013a).
Lexical cohesion is deter-mined by identifying lexical items forming links be-tween sentences in text (also lexical chains).
A num-ber of models have been proposed in order to try andcapture document-wide lexical cohesion and whenimplemented they showed significant improvementsover the baseline (Xiong et al, 2013a).Lexical chain information (Morris and Hirst,1991) can be used to capture lexical cohesion in textand it is already successfully used in a range of fieldssuch as information retrieval and the summarisationof documents (Xiong et al, 2013b).
The work ofXiong et al (2013b) introduces two lexical chainmodels to incorporate lexical cohesion into docu-ment wide SMT and experiments show that, com-pared to the baseline, implementing these modelssubstantially improves translation quality.
Unfor-tunately with limited grammatical cohesion, prop-agated by DMs, translations can be difficult to un-derstand, especially if there is no context provided111by local discourse segments.To achieve improved grammatical cohesion Tu etal.
(2014) propose creating a model that generatestransitional expressions through using complex sen-tence structure based translation rules alongside agenerative transfer model, which is then incorpo-rated into a hierarchical phrase-based system.
Thetest results show significant improvements leadingto smoother and more cohesive translations.
Oneof the key reasons for this is through reserving co-hesive information during the training process byconverting source sentences into ?tagged flattenedcomplex sentence structures?
(Tu et al, 2014) andthen performing word alignments using the trans-lation rules.
It is argued that connecting complexsentence structures with transitional expressions issimilar to the human translation process (Tu et al,2014) and therefore improvements have been madeshowing the effectiveness of preserving cohesion in-formation.2.3 Translation of Implicit Discourse RelationsIt is often assumed that the discourse informationcaptured by the lexical chains is mainly explicit.However, these relations can also be implicitly sig-nalled in text, especially for languages such asChinese where implicitation is used in abundance(Yung, 2014).
Yung (2014) explores DM annotationschemes such as the CDTB (2.1) and observes thatexplicit relations are identified with an accuracy ofup to 94%, whereas with implicit relations this candrop as low as 20% (Yung, 2014).
To overcome this,Yung proposes implementing a discourse-relationaware SMT system, that can serve as a basis for pro-ducing a discourse-structure-aware, document-levelMT system.
The proposed system will use DC an-notated parallel corpora, that enables the integrationof discourse knowledge.
Yung argues that in Chi-nese a segment separated by punctuation is consid-ered to be an elementary discourse unit (EDU) andthat a running Chinese sentence can contain manysuch segments.
However, the sentence would stillbe translated into one single English sentence, sepa-rated by ungrammatical commas and with a distinctlack of connectives.
The connectives are usually ex-plicitly required for the English to make sense, butcan remain implicit in the Chinese (Yung, 2014).However, this work is still in the early stages.3 MotivationThis section outlines the initial research, includinga preliminary corpus analysis, examining difficul-ties with automatically translating DMs across dis-tant languages such as Chinese and English.
It drawsattention to deficiencies caused from under-utilisingdiscourse information and examines divergences inthe usage of DMs.
The final part of this section out-lines the intuition garnered from the given examplesand highlights the approach to be undertaken.For the corpus analysis, research, and experi-ments three main parallel corpora are used:?
Basic Travel Expression Corpus (BTEC): Pri-marily made up of short simple phrases that oc-cur in travel conversations.
It contains 44, 016sentences in each language with over 250, 000Chinese characters and over 300, 000 Englishwords (Takezawa et al, 2012).?
Foreign Broadcast Information Service (FBIS)corpus: This uses a variety of news stories andradio podcasts in Chinese.
It contains 302, 996parallel sentences with 215 million Chinesecharacters and over 237 million English words.?
Ted Talks corpus (TED): Made up of approvedtranslations of the live Ted Talks presenta-tions1.
It contains over 300, 000 Chinese char-acters and over 2 million English words from156, 805 sentences (Cettolo et al, 2012) .Chinese uses a rich array of DMs including:simple conjunctions, composite conjunctions, andzero connectives where the meaning or contextis strongly inferred across clauses with sentenceshaving natural, allowable omissions, which cancause problems for current SMT approaches.
Herea few examples2are outlined:Ex (1)??????????
?he because ill, not come class.Because he was sick, he didn?t come to class3.He is ill, absent.
(Bing)1http://www.ted.com2These examples (Steele and Specia, 2014) are presentedas: Chinese sentence / literal translation / reference translation /automated translation - using either Google or Bing.3(Ross and Sheng, 2006)112Ex (2)???????????
?you because this (be) eat what medicine?Have you been taking anything for this?
(BTEC)What are you eating because of this medicine?
(Google)Both examples show ?because?
(??)
being usedin different ways and in each case the automatedtranslations fall short.
In Ex1 the dropped (implied)pronoun in the second clause could be the problem,whilst in Ex2 significant reordering is needed as?because?
should be linked to ?this?
(??)
- thetopic - rather than ?medicine?
(?).
The ?this?
(??)
refers to an ?ailment?, which is hard to capturefrom a single sentence.
Information preserved froma larger discourse segment may have provided moreclues, but as is, the sentence appears somewhatexophoric and the meaning cannot necessarily begleaned from the text alone.Ex (3)????????????
?as soon as have space we then give you make phone.We?ll call you as soon as there is an opening.
(BTEC)A space that we have to give you a call.
(Google)In Ex3 the characters ???
and ???
are work-ing together as coordinating markers in the form:...?VPa ?
VPb.
However, individually thesecharacters have significantly different meanings,with ???
meaning ?a?
or ?one?
amongst manythings.
Yet, in the given sentence using the ???
and???
constuct ???
has a meaning akin to ?as soonas?
or ?once?, while ???
implies a ?then?
relation,both of which can be difficult to capture.
Figure14shows an example where word alignment failedto map the ?as soon as ... then?
structure to ...?...?...
.
That is, columns 7, 8, 9, which represent ?assoon as?
in the English have no alignment pointswhatsoever.
Yet, in this case, all three items shouldbe aligned to the single element ???
which is onrow 1 on the Chinese side.
Additionally, the word?returns?
(column 11), which is currently alignedto ???
(row 1) should in fact be aligned to ????
(return/come back) in row 2.
This misalignment4The boxes with a ?#?
inside are the alignment points andeach coloured block (large or small) is a minimal-biphrase.Figure 1: A visualisation of word alignments for thegiven parallel sentence, showing a non-alignment of ?assoon as?.could be a direct side-effect of having no alignmentfor ?as soon as?
in the first place.
Consequently, theknock-on effect of poor word alignment, especiallyaround markers - as in this case, will lead to theoverall generation of poorer translation rules.Ex (4)?????,???????
?he because ill, so he not come class.Because he was sick, he didn?t come to class.He is ill, so he did not come to class.
(Bing)Ex4 is a modified version of Ex2, with an extra?so?(??)
and ?he?
(?)
manually inserted in thesecond clause of the Chinese sentence.
Grammat-ically these extra characters are not required for theChinese to make sense, but are still correct.
How-ever, the interesting point is that the extra informa-tion (namely ?so?
and ?he?)
has enabled the systemto produce a much better final translation.From the given examples it appears that both im-plicitation and the use of specific DM structures cancause problems when generating automated transla-tions.
The highlighted issues suggest that makingmarkers (and possibly, by extension, pronouns) ex-plicit, due to linguistic clues, more information be-comes available, which can support the extraction ofword alignments.
Although making implicit mark-113ers explicit can seem unnatural and even unneces-sary for human readers, it does follow that if theword alignment process is made easier by this ex-plicitation it will lead to better translation rules andultimately better translation quality.4 Experiments and Word AlignmentsThis section examines the current ongoing researchand experiments that aim to measure the extent ofthe difficulties caused by DMs.
In particular the fo-cus is on automated word alignments and problemsaround implicit and misaligned DMs.
The workdiscussed in Section 3 highlighted the importanceof improving word alignments, and especially howmissing alignments around markers can lead to thegeneration of poorer rules.Before progressing onto the experiments an initialbaseline system was produced according to detailedcriteria (Chiang, 2007; Saluja et al, 2014).
The ini-tial system was created using the ZH-EN data fromthe BTE parallel corpus (Paul, 2009) (Section 3).Fast-Align is used to generate the word alignmentsand the CDEC decoder (Dyer et al, 2010) is usedfor rule extraction and decoding.
The baseline andsubsequent systems discussed here are hierarchicalphrase-based systems for Chinese to English trans-lation.Once the alignments were obtained the next stepin the methodology was to examine the misalign-ment information to determine the occurrence of im-plicit markers.
A variance list was created5thatcould be used to cross-reference discourse markerswith appropriate substitutable words (as per Table1).
Each DM was then examined in turn (automati-cally) to look at what it had been aligned to.
Whenthe explicit English marker was aligned correctly,according to the variance list, then no change wasmade.
If the marker was aligned to an unsuitableword, then an artificial marker was placed into theChinese in the nearest free space to that word.
Fi-nally if the marker was not aligned at all then an arti-ficial marker was inserted into the nearest free space5The variance list is initially created by filtering good align-ments and bad alignments by hand and using both on-line andoff-line (bi-lingual) dictionaries/resources.DM BTEC FBIS TEDif 25.70% 40.75% 23.35%then 21.00% 50.85 % 40.47%because 23.95% 32.80% 16.48%but 29.40% 39.90% 27.08%Table 2: Misalignment information for the 3 corpora.System DEV TSTBTEC-Dawn (baseline) 34.39 35.02BTEC-Dawn (if) 34.60 35.03BTEC-Dawn (then) 34.69 35.04BTEC-Dawn (but) 34.51 35.21BTEC-Dawn (because) 34.41 35.02BTEC-Dawn (all) 34.53 35.46Table 3: BLEU Scores for the Experimental Systemsby number6.
A percentage of misalignments7acrossall occurrences of individual markers was also cal-culated.Table 2 shows the misalignment percentages forthe four given DMs across the three corpora.
Theaverage sentence length in the BTE Corpus is eightunits, in the FBIS corpus it is 30 units, and in theTED corpus it is 29 units.
The scores show that thereis a wide variance in the misalignments across thecorpora, with FBIS consistently having the highesterror rate, but in all cases the percentage is fairlysignificant.Initially tokens were inserted for single markersat a time, but then finally with tokens for all markersinserted simultaneously.
Table 3 shows the BLEUscores for all the experiments.
The first few exper-iments showed improvements over the baseline ofup to +0.30, whereas the final one showed improve-ments of up to +0.44, which is significant.After running the experiments the visualisation ofa number of word alignments (as per Figures 1,2,3)were examined and a single example of a ?then?
sen-tence was chosen at random.
Figure 2 shows theword alignments for a sentence from the baselinesystem, and Figure 3 shows the word alignments for6The inserts are made according to a simple algorithm, andinspired by the examples in Section 3.7A non-alignment is not necessarily a bad alignment.
Forexample: ????
= ?positive and negative?, with no ?and?
in theChinese.
In this case a non-alignment for ?and?
is acceptable.114Figure 2: Visualisation of word alignments showing noalignment for ?then?
in column 3.the same sentence, but with an artificial marker au-tomatically inserted for the unaligned ?then?.The differences between the word alignments inthe figures are subtle, but positive.
For example, inFigure 3 more of the question to the left of ?then?
iscaptured correctly.
Moreover, to the right of ?then?,?over?
has now been aligned quite well to ????
(over there) and ?to?
has been aligned to ????
(please - go to).
Perhaps most significantly though isthe mish-mash of alignments to ?washstand?
in Fig-ure 2 has now been replaced by a very good align-ment to ?????
(washbasin/washstand) showingan overall smoother alignment.
These preliminaryfindings indicate that there is plenty of scope for fur-ther positive investigation and experimentation.5 Ongoing WorkThis section outlines the two main research areas(Section 1) that will be tackled in order to feed intothe final thesis.
Having addressed the limitations ofcurrent SMT approaches, the focus has moved on tolooking at cohesive devices at the sentential level,but ultimately the overall aim is to better model DRsacross wider discourse segments.5.1 Modelling Cohesive Devices WithinSentencesEven at the sentence level there exists a local con-text, which produces dependencies between certainFigure 3: Visualisation of word alignments showing theartificial marker ?<then>?
and a smoother overall align-ment.words.
The cohesion information within the sen-tence can hold vital clues for tasks such as pronounresolution, and so it is important to try to capture it.Simply looking at the analysis in Section 4 pro-vides insight into which other avenues should be ex-plored for this part, including:?
Expanding the number of DMs being explored,including complex markers (e.g.
as soon as).?
Improving the variance list to capture morevariant translations of marker words.
It is alsoimportant here to include automated filteringfor difficult DMs (e.g.
cases where ?and?
or ?so?are not being used as specific markers can per-haps make them more difficult to align).
Mak-ing significant use of parts of speech taggingand annotated texts could be useful.?
Develop better insertion algorithms to producean improved range of insertion options, and re-duce damage to existing word alignments.?
Looking at using alternative/additional evalua-tion metrics and tools to either replace or com-plement BLEU.
This could produce more tar-geted evaluation that is better at picking up onindividual linguistic components such as DMsand pronouns.115However, the final aim is to work towards a true pre-diction model using parallel data as a source of an-notation.
Creating such a model can be hard mono-lingually, whereas a bilingual corpus can be used asa source of additional implicit annotation or indeeda source of additional signals for discourse relations.The prediction model should make the word align-ment task easier (through either guiding the processor adding constraints), which in turn will generatebetter translation rules and ultimately should im-prove MT.5.2 Modelling Discourse Relations AcrossSentencesThis part will be an extension of the tasks in Section5.1.
The premise is that if the discourse informationor local context within a sentence can be capturedthen it could be applied to wider discourse segmentsand possibly the whole document.
Some inroadsinto this task have been trialled through using lex-ical chaining (Xiong et al, 2013b).
However, morerecently tools are being developed enabling docu-ment wide access to the text, which should providescope for examining the links between larger dis-course units - especially sentences and paragraphs.6 ConclusionsThe findings in Section 3 highlighted that implicitcohesive information can cause significant problemsfor MT and that by adding extra information trans-lations can be made smoother.
Section 4 extendedthis idea and outlined the experiments and method-ology used to capture some effects of automaticallyinserting artificial tokens for implicit or misalignedDMs.
It showed largely positive results, with somegood improvements to the word alignments, indicat-ing that there is scope for further investigation andexperimentation.
Finally, section 5 highlighted thetwo main research areas that will guide the thesis,outlining a number of ways in which the currentmethodology and approach could be developed.The ultimate aim is to use bilingual data as asource of additional clues for a prediction model ofChinese implicit markers, which can, for instance,guide and improve the word alignment process lead-ing to the generation of better rules and smoothertranslations.ReferencesMauro Cettolo, Christian Girardi, and Marcello Federico.2012 Web Inventory of Transcribed and TranslatedTalks.
In: EAMT, pages 261-268.
Trento, Italy.David Chiang.
2007.
Hierarchical phrase-based trans-lation.
Computational Linguistics, 33(2):201?228.Chris Dyer, Adam Lopez, Juri Ganitkevitch, JohnathanWeese, Ferhan Ture, Phil Blunsom, Hendra Setiawan,Vladimir Eidelman, and Philip Resnik.
2010.
CDEC:A decoder, Alignment, and Learning Framework forFinite-state and Context-free Translation Models.
InProceedings of ACL.Zhengxian Gong, Min Zhang, and Guodong Zhou.2011.
Cache-based Document-level Statistical Ma-chine Translation.
In 2011 Conference on EmpiricalMethods in Natural Language Processing, pages 909-919.
Edinburgh, Scotland, UKNajeh Hajlaoui and Andre Popescu-Belis.
2013 Trans-lating English Discourse Connectives into Arabic:a Corpus-based analysis and an Evaluation Metric.In: CAASL4 Workshop at AMTA (Fourth Workshopon Computational Approaches to Arabic Script-basedLanguages), San Diego, CA, pages 1-8.M.A.K Halliday and Ruqaiya Hasan.
1976.
Cohesion inEnglish (English Language Series Longmen, LondonChristian Hardmeier.
2012.
Discourse in StatisticalMachine Translation: A Survey and a Case StudyElanders Sverige, Sweden.Christian Hardmeier, Sara Stymne, Jorg Tiedemann, andJoakim Nivre.
2012 Docent: A Document-Level De-coder for Phrase-Based Statistical Machine Transla-tion.
In: 51st Annual Meeting of the ACL.
Sofia,Bulgaria, pages 193-198.Christian Hardmeier.
2014 Discourse in Statistical Ma-chine Translation.
Elanders Sverige, Sweden.Thomas Meyer and Andrei Popescu-Belis.
2012.
Us-ing sense-labelled discourse connectives for statisticalmachine translation.
In: EACL Joint Workshop onExploiting Synergies between IR and MT, and HybridApproaches to MT (ESIRMTHyTra), pages 129-138.Avignon, France.Jane Morris and Graeme Hirst.
March 1991 Lexical Co-hesion Computed by Thesaural Relations as an Indica-tor of the Structure of Text.
Computational Linguistics,17(1):Pages 21-48.Joseph Olive, Caitlin Christianson, and John McCary (ed-itors).
2011, Handbook of Natural Language Pro-cessing and Machine Translation: DARPA Global Au-tonomous Language Exploitation.
Springer Scienceand Business Media, New York.Michael Paul.
2009.
Overview of the IWSLT 2009 evalu-ation campaign.
In Proceedings of IWSLT.116Emily Pitler and Ani Nenkova.
2009.
Using Syntax toDisambiguate Explicit Discourse Connectives in Text.In: ACL-IJCNLP 2009 (47th Annual Meeting of theACL and 4th International Joint Conference on NLPof the AFNLP), Short Papers, pages 13-16, Singapore.Claudia Ross and Jing-heng Sheng Ma.
2006.
Mod-ern Mandarin Chinese Grammar: A Practical Guide.Routledge, London.Avneesh Saluja, Chris Dyer, and Shay B. Cohen.
2014Latent-Variable Synchronous CFGs for HierarchicalTranslation.
In: Empirical methods in Natural lan-guage processing (EMNLP), pages 1953-1964 Doha,Qatar.David Steele and Lucia Specia.
2014.
Divergences in theUsage of Discourse Markers in English and MandarinChinese.
In: Text, Speech and Dialogue (17th Interna-tional Conference TSD), pages 189-200, Brno, CzechRepublic.Toshiyuki Takezawa, Eiichiro Sumita, Fumiaki Sug-aya, Hirofumi Yamamoto, and Seiichi Yamamoto.2002 Toward a Broad-coverage Bilingual Corpus forSpeech Translation of Travel Conversations in the RealWorld.
In: LREC , pages 147-152.
Las Palmas, Spain.Mei Tu, Yu Zhou and Chengqing Zong.
2014.
Enhanc-ing Grammatical Cohesion: Generating TransitionalExpressions for SMT.
In: 52nd annual meeting of theACL, June 23-25, Baltimore, USA.Billy T.M.
Wong and Chunyu Kit.
2012.
Extending Ma-chine Translation Evaluation Metrics with Lexical Co-hesion to Document Level.
In: 2012 Joint Conferenceon Empirical Methods in Natural Language Processingand Computational Natural Language Learning, pages1060-1068.
Jeju Island, Korea.Tong Xiao, Jingbo Zhu, Shujie Yao, and Hao Zhang.September 2011.
Document-level Consistency Verifi-cation in Machine Translation.
In 2011 MT summitXIII, pages 131-138.
Xiamen, China:Deyi Xiong., Guosheng Ben, Min Zhang, Yajuan Lu, andQun Liu.
August 2013.
Modelling Lexical Cohesionfor Document-level Machine Translation.
In: Twenty-Third International Joint Conference on Artificial In-telligence (IJCAI-13) Beijing, China.Deyi Xiong, Yang Ding, Min Zhang, and Chew LimTan.
2013 Lexical Chain Based Cohesion Modelsfor Document-Level Statistical Machine Translation.In: 2013 Conference on Empirical Methods in NaturalLanguage Processing, pages: 1563-1573.Jinxi Xu and Roger Bock.
2011.
Combination of Al-ternative Word Segmentations for Chinese MachineTranslation.
DARPA Global Autonomous LanguageExploitation.
Springer Science and Business Media,New York.Nianwen Xue.
2005.
Annotating Discourse Connectivesin the Chinese Treebank.
In: ACL Workshop on Fron-tiers in Corpus Annotation 2: Pie in the Sky.Frances Yung.
2014.
Towards a Discourse Relation-aware Approach for Chinese-English Machine Trans-lation.
In: ACL Student Research Workshop, pages18-25.
Baltimore, Maryland USA.117
