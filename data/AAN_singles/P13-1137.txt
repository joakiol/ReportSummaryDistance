Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1395?1405,Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational LinguisticsDomain-Independent Abstract Generationfor Focused Meeting SummarizationLu WangDepartment of Computer ScienceCornell UniversityIthaca, NY 14853luwang@cs.cornell.eduClaire CardieDepartment of Computer ScienceCornell UniversityIthaca, NY 14853cardie@cs.cornell.eduAbstractWe address the challenge of generating natu-ral language abstractive summaries for spokenmeetings in a domain-independent fashion.We apply Multiple-Sequence Alignment to in-duce abstract generation templates that can beused for different domains.
An Overgenerate-and-Rank strategy is utilized to produce andrank candidate abstracts.
Experiments us-ing in-domain and out-of-domain training ondisparate corpora show that our system uni-formly outperforms state-of-the-art supervisedextract-based approaches.
In addition, humanjudges rate our system summaries significantlyhigher than compared systems in fluency andoverall quality.1 IntroductionMeetings are a common way to collaborate,share information and exchange opinions.
Con-sequently, automatically generated meeting sum-maries could be of great value to people and busi-nesses alike by providing quick access to the es-sential content of past meetings.
Focused meet-ing summaries have been proposed as particularlyuseful; in contrast to summaries of a meeting asa whole, they refer to summaries of a specific as-pect of a meeting, such as the DECISIONS reached,PROBLEMS discussed, PROGRESS made or AC-TION ITEMS that emerged (Carenini et al, 2011).Our goal is to provide an automatic summariza-tion system that can generate abstract-style fo-cused meeting summaries to help users digest thevast amount of meeting content in an easy manner.Existing meeting summarization systems re-main largely extractive: their summaries are com-prised exclusively of patchworks of utterances se-lected directly from the meetings to be summa-rized (Riedhammer et al, 2010; Bui et al, 2009;Xie et al, 2008).
Although relatively easy to con-struct, extractive approaches fall short of produc-ing concise and readable summaries, largely dueC: Looking at what we?ve got, we we want an LCD dis-play with a spinning wheel.B: You have to have some push-buttons, don?t you?C: Just spinning and not scrolling, I would say.B: I think the spinning wheel is definitely very now.A: but since LCDs seems to be uh a definite yes,C: We?re having push-buttons on the outsideC: and then on the inside an LCD with spinning wheel,Decision Abstract (Summary):The remote will have push buttons outside, and an LCDand spinning wheel inside.A: and um I?m not sure about the buttons being in theshape of fruit though.D: Maybe make it like fruity colours or something.C: The power button could be like a big apple or some-thing.D: Um like I?m just thinking bright colours.Problem Abstract (Summary):How to incorporate a fruit and vegetable theme into theremote.Figure 1: Clips from the AMI meeting corpus (Mc-cowan et al, 2005).
A, B, C and D refer to distinctspeakers.
Also shown is the gold-standard (manual)abstract (summary) for the decision and the problem.to the noisy, fragmented, ungrammatical and un-structured text of meeting transcripts (Murray etal., 2010b; Liu and Liu, 2009).In contrast, human-written meeting summariesare typically in the form of abstracts ?
distilla-tions of the original conversation written in newlanguage.
A user study from Murray et al (2010b)showed that people demonstrate a strong prefer-ence for abstractive summaries over extracts whenthe text to be summarized is conversational.
Con-sider, for example, the two types of focused sum-mary along with their associated dialogue snippetsin Figure 1.
We can see that extracts are likely toinclude unnecessary and noisy information fromthe meeting transcripts.
On the contrary, the man-ually composed summaries (abstracts) are morecompact and readable, and are written in a dis-tinctly non-conversational style.1395To address the limitations of extract-based sum-maries, we propose a complete and fully automaticdomain-independent abstract generation frame-work for focused meeting summarization.
Fol-lowing existing language generation research (An-geli et al, 2010; Konstas and Lapata, 2012), wefirst perform content selection: given the dia-logue acts relevant to one element of the meet-ing (e.g.
a single decision or problem), we traina classifier to identify summary-worthy phrases.Next, we develop an ?overgenerate-and-rank?strategy (Walker et al, 2001; Heilman and Smith,2010) for surface realization, which generates andranks candidate sentences for the abstract.
Af-ter redundancy reduction, the full meeting abstractcan thus comprise the focused summary for eachmeeting element.
As described in subsequent sec-tions, the generation framework allows us to iden-tify and reformulate the important information forthe focused summary.
Our contributions are as fol-lows:?
To the best of our knowledge, our system isthe first fully automatic system to generatenatural language abstracts for spoken meet-ings.?
We present a novel template extraction al-gorithm, based on Multiple Sequence Align-ment (MSA) (Durbin et al, 1998), to inducedomain-independent templates that guide ab-stract generation.
MSA is commonly usedin bioinformatics to identify equivalent frag-ments of DNAs (Durbin et al, 1998) andhas also been employed for learning para-phrases (Barzilay and Lee, 2003).?
Although our framework requires labeledtraining data for each type of focused sum-mary (decisions, problems, etc.
), we alsomake initial tries for domain adaptation sothat our summarization method does not needhuman-written abstracts for each new meet-ing domain (e.g.
faculty meetings, theatergroup meetings, project group meetings).We instantiate the abstract generation frame-work on two corpora from disparate domains?
the AMI Meeting Corpus (Mccowan et al,2005) and ICSI Meeting Corpus (Janin et al,2003) ?
and produce systems to generate fo-cused summaries with regard to four types ofmeeting elements: DECISIONs, PROBLEMs, AC-TION ITEMSs, and PROGRESS.
Automatic eval-uation (using ROUGE (Lin and Hovy, 2003) andBLEU (Papineni et al, 2002)) against manuallygenerated focused summaries shows that our sum-marizers uniformly and statistically significantlyoutperform two baseline systems as well as astate-of-the-art supervised extraction-based sys-tem.
Human evaluation also indicates that theabstractive summaries produced by our systemsare more linguistically appealing than those ofthe utterance-level extraction-based system, pre-ferring them over summaries from the extraction-based system of comparable semantic correctness(62.3% vs. 37.7%).Finally, we examine the generality of our modelacross domains for two types of focused summa-rization ?
decisions and problems ?
by train-ing the summarizer on out-of-domain data (i.e.
theAMI corpus for use on the ICSI meeting data,and vice versa).
The resulting systems yield re-sults comparable to those from the same systemtrained on in-domain data, and statistically signif-icantly outperform supervised extractive summa-rization approaches trained on in-domain data.2 Related WorkMost research on spoken dialogue summariza-tion attempts to generate summaries for full dia-logues (Carenini et al, 2011).
Only recently hasthe task of focused summarization been studied.Supervised methods are investigated to identifykey phrases or utterances for inclusion in the de-cision summary (Ferna?ndez et al, 2008; Bui etal., 2009).
Based on Ferna?ndez et al (2008), arelation representation is proposed by Wang andCardie (2012) to form structured summaries; weadopt this representation here for content selec-tion.Our research is also in line with generating ab-stractive summaries for conversations.
Extrac-tive approaches (Murray et al, 2005; Xie et al,2008; Galley, 2006) have been investigated exten-sively in conversation summarization.
Murray etal.
(2010a) present an abstraction system consist-ing of interpretation and transformation steps.
Ut-terances are mapped to a simple conversation on-tology in the interpretation step according to theirtype, such as a decision or problem.
Then an in-teger linear programming approach is employedto select the utterances that cover more entities as1396Dialogue Acts:C: Looking at what we've got,we we want [an LCD displaywith a spinning wheel].B: You have to have somepush-buttons, don't you?C: Just spinning and notscrolling , I would say .B: I think the spinning wheel isdefinitely very now.A: but since LCDs seems to beuh a definite yes,C: We're having push-buttons[on the outside]C: and then on the inside anLCD with spinning wheel,Relation Instances:<want, an LCD display with a spinningwheel><an LCD display, with a spinningwheel><have, some push-buttons><having, push-buttons on the outside><push-buttons, on the outside><an LCD, with spinning wheel>?
(other possibilities)<want, an LCD display with a spinning wheel>?
The team will want an LCD display with aspinning wheel.?
The team with work with an LCD displaywith a spinning wheel.?
The group decide to use an LCD display witha spinning wheel.?
(other possibilities)<push-buttons, on the outside>?
Push-buttons are going to be on the outside.?
Push-buttons on the outside will be used.?
There will be push-buttons on the outside.?
(other possibilities)One-BestAbstract:The group decide touse an LCD displaywith a spinningwheel.One-BestAbstract:There will be push-buttons on theoutside.Final Summary:The group decide touse an LCD display witha spinning wheel.There will be push-buttons on the outside.Learned Templates?
(all possible abstracts per relationinstance)RelationExtractionContent SelectionTemplateFillingStatisticalRankingSurface Realization?
(one-best abstractper relation instance)Post-SelectionFigure 2: The abstract generation framework.
It takes as input a cluster of meeting-item-specific dialogue acts,from which one focused summary is constructed.
Sample relation instances are denoted in bold (The indicatorsare further italicized and the arguments are in [brackets]).
Summary-worthy relation instances are identified bycontent selection module (see Section 4) and then filled into the learned templates individually.
A statistical rankersubsequently selects one best abstract per relation instance (see Section 5.2).
The post-selection component reducesthe redundancy and outputs the final summary (see Section 5.3).determined by an external ontology.
Liu and Liu(2009) apply sentence compression on extractedsummary utterances.
Though some of the unnec-essary words are dropped, the resulting compres-sions can still be ungrammatical and unstructured.This work is also broadly related to ex-pert system-based language generation (Reiterand Dale, 2000) and concept-to-text generationtasks (Angeli et al, 2010; Konstas and Lapata,2012), where the generation process is decom-posed into content selection (or text planning) andsurface realization.
For instance, Angeli et al(2010) learn from structured database records andparallel textual descriptions.
They generate textsbased on a series of decisions made to select therecords, fields, and proper templates for render-ing.
Those techniques that are tailored to specificdomains (e.g.
weather forecasts or sportcastings)cannot be directly applied to the conversationaldata, as their input is well-structured and the tem-plates learned are domain-specific.3 FrameworkOur domain-independent abstract generationframework produces a summarizer that gener-ates a grammatical abstract from a cluster ofmeeting-element-related dialogue acts (DAs) ?all utterances associated with a single decision,problem, action item or progress step of interest.Note that identifying these DA clusters is a diffi-cult task in itself (Bui et al, 2009).
Accordingly,our experiments evaluate two conditions ?
onein which we assume that they are perfectly iden-tified, and one in which we identify the clustersautomatically.The summarizer consists of two major compo-nents and is depicted in Figure 2.
Given the DAcluster to be summarized, the Content Selectionmodule identifies a set of summary-worthy rela-tion instances represented as indicator-argumentpairs (i.e.
these constitute a finer-grained represen-tation than DAs).
The Surface Realization compo-nent then generates a short summary in three steps.In the first step, each relation instance is filled intotemplates with disparate structures that are learnedautomatically from the training set (Template Fill-ing).
A statistical ranker then selects one best ab-stract per relation instance (Statistical Ranking).Finally, selected abstracts are processed for redun-dancy removal in Post-Selection.
Detailed descrip-tions for each individual step are provided in Sec-tions 4 and 5.4 Content SelectionPhrase-based content selection approaches havebeen shown to support better meeting sum-maries (Ferna?ndez et al, 2008).
Therefore, wechose a content selection representation of a finergranularity than an utterance: we identify relationinstances that can both effectively detect the cru-cial content and incorporate enough syntactic in-formation to facilitate the downstream surface re-alization.More specifically, our relation instances arebased on information extraction methods thatidentify a lexical indicator (or trigger) that evokesa relation of interest and then employ syntac-tic information, often in conjunction with se-mantic constraints, to find the argument con-stituent(or target phrase) to be extracted.
Rela-1397tion instances, then, are represented by indicator-argument pairs (Chen et al, 2011).
For example,in the DA cluster of Figure 2, ?want, an LCD dis-play with a spinning wheel?
and ?push-buttons, onthe outside?
are two relation instances.Relation Instance Extraction We adopt andextend the syntactic constraints from Wang andCardie (2012) to identify all relation instances inthe input utterances; the summary-worthy oneswill be selected by a discriminative classifier.Constituent and dependency parses are obtainedby the Stanford parser (Klein and Manning, 2003).Both the indicator and argument take the form ofconstituents in the parse tree.
We restrict the el-igible indicator to be a noun or verb; the eligi-ble arguments is a noun phrase (NP), prepositionalphrase (PP) or adjectival phrase (ADJP).
A validindicator-argument pair should have at least onecontent word and satisfy one of the following con-straints:?
When the indicator is a noun, the argumenthas to be a modifier or complement of the in-dicator.?
When the indicator is a verb, the argumenthas to be the subject or the object if it is anNP, or a modifier or complement of the indi-cator if it is a PP/ADJP.We view relation extraction as a binary classifi-cation problem rather than a clustering task (Chenet al, 2011).
All relation instances can be cate-gorized as summary-worthy or not, but only thesummary-worthy ones are used for abstract gen-eration.
A discriminative classifier is trained forthis purpose based on Support Vector Machines(SVMs) (Joachims, 1998) with an RBF kernel.For training data construction, we consider a re-lation instance to be a positive example if it sharesany content word with its corresponding abstracts,and a negative example otherwise.
The featuresused are shown in Table 1.5 Surface RealizationIn this section, we describe surface realization,which renders the relation instances into naturallanguage abstracts.
This process begins with tem-plate extraction (Section 5.1).
Once the templatesare learned, the relation instances from Section 4are filled into the templates to generate an abstract(see Section 5.2).
Redundancy handling is dis-cussed in Section 5.3.Basic Featuresnumber of words/content wordsportion of content words/stopwordsnumber of content words in indicator/argumentnumber of content words that are also in previous DAindicator/argument only contains stopword?number of new nounsContent Featureshas capitalized word?has proper noun?TF/IDF/TFIDF min/max/averageDiscourse Featuresmain speaker or not?is in an adjacency pair (AP)?is in the source/target of the AP?number of source/target DA in the APis the target of the AP a positive/negative/neutral response?is the source of the AP a question?Syntax Featuresindicator/argument constituent tagdependency relation of indicator and argumentTable 1: Features for content selection.
Most areadapted from previous work (Galley, 2006; Xie et al,2008; Wang and Cardie, 2012).
Every basic or con-tent feature is concatenated with the constituent tags ofindicator and argument to compose a new one.
Mainspeakers include the most talkative speaker (who hassaid the most words) and other speakers whose wordcount is more than 20% of the most talkative one (Xieet al, 2008).
Adjacency pair (AP) (Galley, 2006) isan important conversational analysis concept; each APconsists of a source utterance and a target utterance pro-duced by different speakers.5.1 Template ExtractionSentence Clustering.
Template extraction startswith clustering the sentences that constitute themanually generated abstracts in the training dataaccording to their lexical and structural similarity.From each cluster, multiple-sequence alignmenttechniques are employed to capture the recurringpatterns.Intuitively, desirable templates are those thatcan be applied in different domains to generatethe same type of focused summary (e.g.
decisionor problem summaries).
We do not want sen-tences to be clustered only because they describethe same domain-specific details (e.g.
they are allabout ?data collection?
), which will lead to frag-mented templates that are not reusable for new do-mains.
We therefore replace all appearances ofdates, numbers, and proper names with generic la-bels.
We also replace words that appear in boththe abstract and supporting dialogue acts by a la-bel indicating its phrase type.
For any noun phrasewith its head word abstracted, the whole phrase isalso replaced with ?NP?.1398start TheyThe group were not sure whether to VP NPuseNP should include endhow much would cost to make1) The group were not sure whether to [include]VP [a recharger for the remote]NP .
2) The group were not sure whether to use [plastic and rubber or titanium for the case]NP .
3) The group were not sure whether [the remote control]NP should include [functions for controlling video]NP .
4) They were not sure how much [a recharger]NP would cost to make .
?
(Kther abstracts)1) The group were not sure whether to VP NP .
2) The group were not sure whether to use NP .
3) The group were not sure whether NP should include NP .
4) They were not sure how much NP would cost to make .Generic Label Replacement + ClusteringTemplate Examples:  Fine T1: The group were not sure whether to SLOTVP NP .
(1, 2)  Fine T2: The group were not sure whether NP SLOTVP SLOTVP NP .
(3)  Fine T3: SLOTNP were not sure SLOTWHADJP SLOTWHADJP NP SLOTVP SLOTVP SLOTVP SLOTVP SLOTVP .
(4)  Coarse T1: SLOTNP SLOTNP were not sure SLOTSBAR SLOTVP SLOTVP SLOTNP .
(1, 2)  Coarse T2: SLOTNP SLOTNP were not sure SLOTSBAR SLOTNP SLOTVP SLOTVP SLOTNP .
(3)  Coarse T3: SLOTNP were not sure SLOTWHADJP SLOTWHADJP SLOTNP SLOTVP SLOTVP SLOTVP SLOTVP .
(4)Template InductionMSAFigure 3: Example of template extraction by Multiple-Sequence Alignment for problem abstracts from AMI.Backbone nodes shared by at least 50% sentences areshaded.
The grammatical errors exist in the originalabstracts.Following Barzilay and Lee (2003), we ap-proach the sentence clustering task by hierarchicalcomplete-link clustering with a similarity metricbased on word n-gram overlap (n = 1, 2, 3).
Clus-ters with fewer than three abstracts are removed1.Learning the Templates via MSA.
For learn-ing the structural patterns among the abstracts,Multiple-Sequence Alignment (MSA) is first com-puted for each cluster.
MSA takes as input multi-ple sentences and one scoring function to measurethe similarity between any two words.
For inser-tions or deletions, a gap cost is also added.
MSAcan thus find the best way to align the sequenceswith insertions or deletions in accordance with thescorer.
However, computing an optimal MSA isNP-complete (Wang and Jiang, 1994), thus weimplement an approximate algorithm (Needlemanand Wunsch, 1970) that iteratively aligns two se-quences each time and treats the resulting align-ment as a new sequence2.
Figure 3 demonstratesan MSA computed from a sample cluster of ab-1Clustering stops when the similarity between any pair-wise clusters is below 5.
This is applied to every type of sum-marization.
We tune the parameter on a small held-out devel-opment set by manually evaluating the induced templates.
Nosignificant change is observed within a small range.2We adopt the scoring function for MSA from Barzilayand Lee (2003), where aligning two identical words scores1, inserting a gap scores ?0.01, and aligning two differentwords scores ?0.5.stracts.
The MSA is represented in the form ofword lattice, from which we can detect the struc-tural similarities shared by the sentences.To transform the resulting MSAs into templates,we need to decide whether a word in the sentenceshould be retained to comprise the template or ab-stracted.
The backbone nodes in an MSA are iden-tified as the ones shared by more than 50%3 of thecluster?s sentences (shaded in gray in Figure 3).We then create a FINE template for each sentenceby abstracting the non-backbone words, i.e.
re-placing each of those words with a generic token(last step in Figure 3).
We also create a COARSEtemplate that only preserves the nodes shared byall of the cluster?s sentences.
By using the op-erations above, domain-independent patterns arethus identified and domain-specific details are re-moved.Note that we do not explicitly evaluate the qual-ity of the learned templates, which would requirea significant amount of manual evaluation.
In-stead, they are evaluated extrinsically.
We encodethe templates as features (Angeli et al, 2010) thatcould be selected or ignored in the succeeding ab-stract ranking model.5.2 Template FillingAn Overgenerate-and-Rank Approach.
Sincefilling the relation instances into templates of dis-tinct structures may result in abstracts of vary-ing quality, we rank the abstracts based on thefeatures of the template, the transformation con-ducted, and the generated abstract.
This is realizedby the Overgenerate-and-Rank strategy (Walker etal., 2001; Heilman and Smith, 2010).
It takes asinput a set of relation instances (from the samecluster) R = {?indi, argi?
}Ni=1 that are producedby content selection component, a set of templatesT = {tj}Mj=1 that are represented as parsing trees,a transformation function F (described below),and a statistical ranker S for ranking the generatedabstracts, for which we defer description later inthis Section.For each ?indi, argi?, the overgenerate-and-rank approach fills it into each template in T byapplying F to generate all possible abstracts.
Thenthe ranker S selects the best abstract absi.
Post-selection is conducted on the abstracts {absi}Ni=1to form the final summary.3See Barzilay and Lee (2003) for a detailed discussionabout the choice of 50% according to pigeonhole principle.1399The transformation function F models theconstituent-level transformations of relation in-stances and their mappings to the parse trees oftemplates.
With the intuition that people will reusethe relation instances from the transcripts albeitnot necessarily in their original form to write theabstracts, we consider three major types of map-ping operations for the indicator or argument in thesource pair, namely, Full-Constituent Mapping,Sub-Constituent Mapping, and Removal.
Full-Constituent Mapping denotes that a source con-stituent is mapped directly to a target constituentof the template parse tree with the same tag.
Sub-Constituent Mapping encodes more complex andflexible transformations in that a sub-constituentof the source is mapped to a target constituentwith the same tag.
This operation applies whenthe source has a tag of PP or ADJP, in which caseits sub-constituent, if any, with a tag of NP, VP orADJP can be mapped to the target constituent withthe same tag.
For instance, an argument ?with aspinning wheel?
(PP) can be mapped to an NP in atemplate because it has a sub-constituent ?a spin-ning wheel?
(NP).
Removal means a source is notmapped to any constituent in the template.Formally, F is defined as:F (?indsrc, argsrc?, t) ={?indtrank , argtrank , indtark , argtark ?
}Kk=1where ?indsrc, argsrc?
?
R is a relation in-stance (source pair); t ?
T is a template; indtrankand argtrank is the transformed pair of indsrc andargsrc; indtark and argtark are constituents in t, andthey compose one target pair for ?indsrc, argsrc?.We require that indsrc and argsrc are not removedat the same time.
Moreover, for valid indtark andargtark , the words subsumed by them should be allabstracted in the template, and they do not overlapin the parse tree.To obtain the realized abstract, we traverse theparse tree of the filled template in pre-order.
Thewords subsumed by the leaf nodes are thus col-lected sequentially.Learning a Statistical Ranker.
We utilize a dis-criminative ranker based on Support Vector Re-gression (SVR) (Smola and Scho?lkopf, 2004) torank the generated abstracts.
Given the train-ing data that includes clusters of gold-standardsummary-worthy relation instances, associated ab-stracts they support, and the parallel templates foreach abstract, training samples for the ranker areBasic Featuresnumber of words in indsrc/argsrcnumber of new nouns in indsrc/argsrcindtrank /argtrank only has stopword?number of new nouns in indtrank /argtrankStructure Featuresconstituent tag of indsrc/argsrcconstituent tag of indsrc with constituent tag of indtarconstituent tag of argsrc with constituent tag of argtartransformation of indsrc/argsrc combined with constituent tagdependency relation of indsrc and argsrcdependency relation of indtar and argtarabove 2 features have same value?Template Featurestemplate type (fine/coarse)realized template (e.g.
?the group decided to?
)number of words in templatethe template has verb?Realization Featuresrealization has verb?realization starts with verb?realization has adjacent verbs/NPs?indsrc precedes/succeeds argsrc?indtar precedes/succeeds argtar?above 2 features have same value?Language Model Featureslog pLM (first word in indtrank |previous 1/2 words)log pLM (realization)log pLM (first word in argtrank |previous 1/2 words)log pLM (realization)/lengthlog pLM (next word | last 1/2 words in indtrank )log pLM (next word | last 1/2 words in argtrank )Table 2: Features for abstracts ranking.
The languagemodel features are based on a 5-gram language modeltrained on Gigaword (Graff, 2003) by SRILM (Stolcke,2002).constructed according to the transformation func-tion F mentioned above.
Each sample is repre-sented as:(?indsrc, argsrc?, ?indtrank , argtrank , indtark , argtark ?, t, a)where ?indsrc, argsrc?
is the source pair,?indtrank , argtrank ?
is the transformed pair,?indtark , argtark ?
is the target pair in template t,and a is the abstract parallel to t.We first find ?indtar,absk , argtar,absk ?, whichis the corresponding constituent pair of?indtark , argtark ?
in a.
Then we identifythe summary-worthy words subsumed by?indtrank , argtrank ?
that also appear in a.
If thosewords are all subsumed by ?indtar,absk , argtar,absk ?,then it is considered to be a positive sample, anda negative sample otherwise.
Table 2 displays thefeatures used in abstract ranking.5.3 Post-Selection: Redundancy Handling.Post-selection aims to maximize the informationcoverage and minimize the redundancy of thesummary.
Given the generated abstracts A =1400Input : relation instances R = {?indi, argi?
}Ni=1,generated abstracts A = {absi}Ni=1, objectivefunction f , cost function COutput: final abstract GG?
?
(empty set);U ?
A;while U 6= ?
doabs?
arg maxabsi?U f(A,G?absi)?f(A,G)C(absi) ;if f(A,G ?
abs)?
f(A,G) ?
0 thenG?
G ?
abs;endU ?
U \ abs;endAlgorithm 1: Greedy algorithm for post-selection to generate the final summary.
{absi}Ni=1, we use a greedy algorithm (Lin andBilmes, 2010) to select a subsetA?, whereA?
?
A,to form the final summary.
We define wij asthe unigram similarity between abstracts absi andabsj , C(absi) as the number of words in absi.
Weemploy the following objective function:f(A,G) =?absi?A\G?absj?G wi,j , G ?
AAlgorithm 1 sequentially finds an abstract withthe greatest ratio of objective function gain tolength, and add it to the summary if the gain isnon-negative.6 Experimental SetupCorpora.
Two disparate corpora are used forevaluation.
The AMI meeting corpus (Mccowanet al, 2005) contains 139 scenario-driven meet-ings, where groups of four people participate ina series of four meetings for a fictitious project ofdesigning remote control.
The ICSI meeting cor-pus (Janin et al, 2003) consists of 75 naturally oc-curring meetings, each of them has 4 to 10 par-ticipants.
Compared to the fabricated topics inAMI, the conversations in ICSI tend to be special-ized and technical, e.g.
discussion about speechand language technology.
We use 57 meetings inICSI and 139 meetings in AMI that include a short(usually one-sentence), manually constructed ab-stract summarizing each important output for ev-ery meeting.
Decision and problem summaries areannotated for both corpora.
AMI has extra ac-tion item summaries, and ICSI has progress sum-maries.
The set of dialogue acts that support eachabstract are annotated as such.System Inputs.
We consider two system inputsettings.
In the True Clusterings setting, weuse the annotations to create perfect partitions ofthe DAs for input to the system; in the SystemFigure 4: Content selection evaluation by usingROUGE-SU4 (multiplied by 100).
SVM-DA andSVM-TOKEN denotes for supervised extract-basedmethods with SVMs on utterance- and token-level.Summaries for decision, problem, action item, andprogress are generated and evaluated for AMI and ICSI(with names in parentheses).
X-axis shows the numberof meetings used for training.Clusterings setting, we employ a hierarchical ag-glomerative clustering algorithm used for this taskin (Wang and Cardie, 2011).
DAs are grouped ac-cording to a classifier trained beforehand.Baselines and Comparisons.
We compare oursystem with (1) two unsupervised baselines, (2)two supervised extractive approaches, and (3) anoracle derived from the gold standard abstracts.Baselines.
As in Riedhammer et al (2010), theLONGEST DA in each cluster is selected as thesummary.
The second baseline picks the clus-ter prototype (i.e.
the DA with the largest TF-IDF similarity with the cluster centroid) as thesummary according to Wang and Cardie (2011).Although it is possible that important content isspread over multiple DAs, both baselines allowus to determine summary quality when summariesare restricted to a single utterance.Supervised Learning.
We also compare ourapproach to two supervised extractive sum-marization methods ?
Support Vector Ma-chines (Joachims, 1998) trained with the same fea-1401tures as our system (see Table 1) to identify the im-portant DAs (no syntax features) (Xie et al, 2008;Sandu et al, 2010) or tokens (Ferna?ndez et al,2008) to include into the summary4.Oracle.
We compute an oracle consisting of thewords from the DA cluster that also appear in theassociated abstract to reflect the gap between thebest possible extracts and the human abstracts.7 ResultsContent Selection Evaluation.
We first employROUGE (Lin and Hovy, 2003) to evaluate thecontent selection component with respect to thehuman written abstracts.
ROUGE computes thengram overlapping between the system summarieswith the reference summaries, and has been usedfor both text and speech summarization (Dang,2005; Xie et al, 2008).
We report ROUGE-2 (R-2) and ROUGE-SU4 (R-SU4) that are shown tocorrelate with human evaluation reasonably well.In AMI, four meetings of different functions arecarried out in each group5.
35 meetings for ?con-ceptual design?
are randomly selected for testing.For ICSI, we reserve 12 meetings for testing.The R-SU4 scores for each system are displayedin Figure 4 and show that our system uniformlyoutperforms the baselines and supervised systems.The learning curve of our system is relatively flat,which means not many training meetings are re-quired to reach a usable performance level.Note that the ROUGE scores are relative lowwhen the reference summaries are human ab-stracts, even for evaluation among abstracts pro-duced by different annotators (Dang, 2005).
Theintrinsic difference of styles between dialogue andhuman abstract further lowers the scores.
But thetrend is still respected among the systems.Abstract Generation Evaluation.
To evaluatethe full abstract generation system, the BLEUscore (Papineni et al, 2002) (the precision of uni-grams and bigrams with a brevity penalty) is com-puted with human abstracts as reference.
BLEUhas a fairly good agreement with human judge-ment and has been used to evaluate a variety oflanguage generation systems (Angeli et al, 2010;Konstas and Lapata, 2012).4We use SVMlight (Joachims, 1999) with RBF kernel bydefault parameters for SVM-based classifiers and regressor.5The four types of meetings in AMI are: project kick-off(35 meetings), functional design (35 meetings), conceptualdesign (35 meetings), and detailed design (34 meetings).Figure 5: Full abstract generation system evaluationby using BLEU (multiplied by 100).
SVM-DA de-notes for supervised extractive methods with SVMs onutterance-level.We are not aware of any existing work gen-erating abstractive summaries for conversations.Therefore, we compare our full system againsta supervised utterance-level extractive methodbased on SVMs along with the baselines.
TheBLEU scores in Figure 5 show that our system im-proves the scores consistently over the baselinesand the SVM-based approach.Domain Adaptation Evaluation.
We furtherexamine our system in domain adaptation sce-narios for decision and problem summarization,where we train the system on AMI for use on ICSI,and vice versa.
Table 3 indicates that, with bothtrue clusterings and system clusterings, our sys-tem trained on out-of-domain data achieves com-parable performance with the same system trainedon in-domain data.
In most experiments, it alsosignificantly outperforms the baselines and theextract-based approaches (p < 0.05).Human Evaluation.
We randomly select 15 de-cision and 15 problem DA clusters (true cluster-ings).
We evaluate fluency (is the text gram-matical?)
and semantic correctness (does thesummary convey the gist of the DAs in the clus-ter?)
for OUR SYSTEM trained on IN-domain data1402System (True Clusterings) AMI Decision ICSI Decision AMI Problem ICSI ProblemR-2 R-SU4 BLEU R-2 R-SU4 BLEU R-2 R-SU4 BLEU R-2 R-SU4 BLEUCENTROID DA 1.3 3.0 7.7 1.8 3.5 3.8 1.0 2.7 4.2 1.0 2.3 2.8LONGEST DA 1.6 3.3 7.0 2.8 4.7 6.5 1.0 3.0 3.6 1.2 3.4 4.6SVM-DA (IN) 3.4 4.7 9.7 3.4 4.5 5.7 1.4 2.4 5.0 1.6 3.4 3.4SVM-DA (OUT) 2.7 4.2 6.6 3.1 4.2 4.6 1.4 2.2 2.5 1.3 3.0 4.6OUR SYSTEM (IN) 4.5 6.2 11.6 4.9 7.1 10.0 3.1 4.8 7.2 4.0 5.9 6.0OUR SYSTEM (OUT) 4.6 6.1 10.3 4.8 6.4 7.8 3.5 4.7 6.2 3.0 5.5 5.3ORACLE 7.5 12.0 22.8 9.9 14.9 20.2 6.6 11.3 18.9 6.4 12.6 13.0System (System Clusterings) AMI Decision ICSI Decision AMI Problem ICSI ProblemR-2 R-SU4 BLEU R-2 R-SU4 BLEU R-2 R-SU4 BLEU R-2 R-SU4 BLEUCENTROID DA 1.4 3.3 3.8 1.4 2.1 2.0 0.8 2.8 2.9 0.9 2.3 1.8LONGEST DA 1.4 3.3 5.7 1.7 3.4 5.5 0.8 3.2 4.1 0.9 3.4 4.4SVM-DA (IN) 2.6 4.6 10.5 3.5 6.5 7.1 1.8 3.7 4.9 1.8 4.0 4.6SVM-DA (OUT) 3.4 5.8 10.3 2.7 4.8 6.3 2.1 3.8 4.3 1.5 3.8 3.5OUR SYSTEM (IN) 3.5 5.4 11.7 4.4 7.4 9.1 3.3 4.6 9.5 2.3 4.2 7.4OUR SYSTEM (OUT) 3.9 6.4 11.4 4.1 5.1 8.4 3.6 5.6 8.9 1.8 4.0 6.8ORACLE 6.4 12.0 15.1 8.2 15.2 17.6 6.5 13.0 20.9 5.5 11.9 15.5Table 3: Domain adaptation evaluation.
Systems trained on out-of-domain data are denoted with ?
(OUT)?, oth-erwise with ?(IN)?.
ROUGE and BLEU scores are multiplied by 100.
Our systems that statistically significantlyoutperform all the other approaches (except ORACLE) are in bold (p < 0.05, paired t-test).
The numbers in italicsshow the significant improvement over the baselines by our systems.System Fluency Semantic LengthMean S.D.
Mean S.D.OUR SYSTEM (IN) 3.67 0.85 3.27 1.03 23.65OUR SYSTEM (OUT) 3.58 0.90 3.25 1.16 24.17SVM-DA (IN) 3.36 0.84 3.44 1.26 38.83Table 4: Human evaluation results of Fluency and Se-mantic correctness for the generated abstracts.
The rat-ings are on 1 (worst) to 5 (best) scale.
The averageLength of the abstracts for each system is also listed.and OUT-of-domain data, and for the utterance-level extraction system (SVM-DA) trained on in-domain data.
Each cluster of DAs along with threerandomly ordered summaries are presented to thejudges.
Five native speaking Ph.D. students (noneare authors) performed the task.We carry out an one-way Analysis of Variancewhich shows significant differences in score as afunction of system (p < 0.05, paired t-test).
Re-sults in Table 4 demonstrate that our system sum-maries are significantly more compact and fluentthan the extract-based method (p < 0.05) whilesemantic correctness is comparable.The judges also rank the three summaries interms of the overall quality in content, concise-ness and grammaticality.
An inter-rater agreementof Fleiss?s ?
= 0.45 (moderate agreement (Landisand Koch, 1977)) was computed.
Judges selectedour system as the best system in 62.3% scenarios(IN-DOMAIN: 35.6%, OUT-OF-DOMAIN: 26.7%).Sample summaries are exhibited in Figure 6.8 ConclusionWe presented a domain-independent abstract gen-eration framework for focused meeting summa-rization.
Experimental results on two disparatemeeting corpora show that our system can uni-Decision Summary:Human: The remote will have push buttons outside, andan LCD and spinning wheel inside.Our System (In): The group decide to use an LCD dis-play with a spinning wheel.
There will be push-buttons onthe outside.Our System (Out): LCD display is going to be with aspinning wheel.
It is necessary having push-buttons onthe outside.SVM-DA: Looking at what we?ve got, we we want anLCD display with a spinning wheel.
Just spinning and notscrolling, I would say.
I think the spinning wheel is defi-nitely very now.
We?re having push-buttons on the outsideProblem Summary:Human: How to incorporate a fruit and vegetable themeinto the remote.Our System (In): Whether to include the shape of fruit.The team had to thinking bright colors.Our System (Out): It is unclear that the buttons being inthe shape of fruit.SVM-DA: and um Im not sure about the buttons being inthe shape of fruit though.Figure 6: Sample decision and problem sum-maries generated by various systems for examplesin Figure 1.formly outperform the state-of-the-art supervisedextraction-based systems in both automatic andmanual evaluation.
Our system also exhibits anability to train on out-of-domain data to generateabstracts for a new target domain.9 AcknowledgmentsThis work was supported in part by National Sci-ence Foundation Grant IIS-0968450 and a giftfrom Boeing.
We thank Moontae Lee, Myle Ott,Yiye Ruan, Chenhao Tan, and the ACL reviewersfor valuable suggestions and advice on various as-pects of this work.1403ReferencesGabor Angeli, Percy Liang, and Dan Klein.
2010.
Asimple domain-independent probabilistic approachto generation.
In Proceedings of the 2010 Confer-ence on Empirical Methods in Natural LanguageProcessing, EMNLP ?10, pages 502?512, Strouds-burg, PA, USA.
Association for Computational Lin-guistics.Regina Barzilay and Lillian Lee.
2003.
Learn-ing to paraphrase: an unsupervised approach usingmultiple-sequence alignment.
In Proceedings of the2003 Conference of the North American Chapterof the Association for Computational Linguistics onHuman Language Technology - Volume 1, NAACL?03, pages 16?23, Stroudsburg, PA, USA.
Associa-tion for Computational Linguistics.Trung H. Bui, Matthew Frampton, John Dowding, andStanley Peters.
2009.
Extracting decisions frommulti-party dialogue using directed graphical mod-els and semantic similarity.
In Proceedings of theSIGDIAL 2009 Conference: The 10th Annual Meet-ing of the Special Interest Group on Discourse andDialogue, SIGDIAL ?09, pages 235?243, Strouds-burg, PA, USA.
Association for Computational Lin-guistics.Giuseppe Carenini, Gabriel Murray, and Raymond Ng.2011.
Methods for Mining and Summarizing TextConversations.
Morgan & Claypool Publishers.Harr Chen, Edward Benson, Tahira Naseem, andRegina Barzilay.
2011.
In-domain relation discov-ery with meta-constraints via posterior regulariza-tion.
In Proceedings of the 49th Annual Meeting ofthe Association for Computational Linguistics: Hu-man Language Technologies - Volume 1, HLT ?11,pages 530?540, Stroudsburg, PA, USA.
Associationfor Computational Linguistics.Hoa T. Dang.
2005.
Overview of DUC 2005.
In Doc-ument Understanding Conference.Richard Durbin, Sean R. Eddy, Anders Krogh, andGraeme Mitchison.
1998.
Biological SequenceAnalysis: Probabilistic Models of Proteins and Nu-cleic Acids.
Cambridge University Press, July.Raquel Ferna?ndez, Matthew Frampton, John Dowding,Anish Adukuzhiyil, Patrick Ehlen, and Stanley Pe-ters.
2008.
Identifying relevant phrases to sum-marize decisions in spoken meetings.
In INTER-SPEECH, pages 78?81.Michel Galley.
2006.
A skip-chain conditional randomfield for ranking meeting utterances by importance.In Proceedings of the 2006 Conference on EmpiricalMethods in Natural Language Processing, EMNLP?06, pages 364?372, Stroudsburg, PA, USA.
Associ-ation for Computational Linguistics.David Graff.
2003.
English Gigaword.Michael Heilman and Noah A. Smith.
2010.
Goodquestion!
statistical ranking for question generation.In Human Language Technologies: The 2010 An-nual Conference of the North American Chapter ofthe Association for Computational Linguistics, HLT?10, pages 609?617, Stroudsburg, PA, USA.
Associ-ation for Computational Linguistics.A.
Janin, D. Baron, J. Edwards, D. Ellis, D. Gelbart,N.
Morgan, B. Peskin, T. Pfau, E. Shriberg, A. Stol-cke, and C. Wooters.
2003.
The icsi meeting corpus.volume 1, pages I?364?I?367 vol.1.Thorsten Joachims.
1998.
Text categorization withsuport vector machines: Learning with many rele-vant features.
In Proceedings of the 10th EuropeanConference onMachine Learning, ECML ?98, pages137?142, London, UK, UK.
Springer-Verlag.Thorsten Joachims.
1999.
Advances in kernel meth-ods.
chapter Making large-scale support vector ma-chine learning practical, pages 169?184.
MIT Press,Cambridge, MA, USA.Dan Klein and Christopher D. Manning.
2003.
Ac-curate unlexicalized parsing.
In Proceedings of the41st Annual Meeting on Association for Computa-tional Linguistics - Volume 1, ACL ?03, pages 423?430, Stroudsburg, PA, USA.
Association for Com-putational Linguistics.Ioannis Konstas and Mirella Lapata.
2012.
Concept-to-text generation via discriminative reranking.
InProceedings of the 50th Annual Meeting of the Asso-ciation for Computational Linguistics: Long Papers- Volume 1, ACL ?12, pages 369?378, Stroudsburg,PA, USA.
Association for Computational Linguis-tics.J R Landis and G G Koch.
1977.
The measurement ofobserver agreement for categorical data.
Biometrics,33(1):159?174.Hui Lin and Jeff Bilmes.
2010.
Multi-document sum-marization via budgeted maximization of submod-ular functions.
In Human Language Technologies:The 2010 Annual Conference of the North AmericanChapter of the Association for Computational Lin-guistics, HLT ?10, pages 912?920, Stroudsburg, PA,USA.
Association for Computational Linguistics.Chin-Yew Lin and Eduard Hovy.
2003.
Auto-matic evaluation of summaries using n-gram co-occurrence statistics.
In Proceedings of the 2003Conference of the North American Chapter of theAssociation for Computational Linguistics on Hu-man Language Technology - Volume 1, pages 71?78.Fei Liu and Yang Liu.
2009.
From extractive to ab-stractive meeting summaries: can it be done by sen-tence compression?
In Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, ACLShort?09, pages 261?264, Stroudsburg, PA, USA.
Associ-ation for Computational Linguistics.1404I.
Mccowan, G. Lathoud, M. Lincoln, A. Lisowska,W.
Post, D. Reidsma, and P. Wellner.
2005.
The amimeeting corpus.
In In: Proceedings Measuring Be-havior 2005, 5th International Conference on Meth-ods and Techniques in Behavioral Research.
L.P.J.J.Noldus, F. Grieco, L.W.S.
Loijens and P.H.
Zimmer-man (Eds.
), Wageningen: Noldus Information Tech-nology.Gabriel Murray, Steve Renals, and Jean Carletta.
2005.Extractive summarization of meeting recordings.
InINTERSPEECH, pages 593?596.Gabriel Murray, Giuseppe Carenini, and Raymond Ng.2010a.
Interpretation and transformation for ab-stracting conversations.
In Human Language Tech-nologies: The 2010 Annual Conference of the NorthAmerican Chapter of the Association for Com-putational Linguistics, HLT ?10, pages 894?902,Stroudsburg, PA, USA.
Association for Computa-tional Linguistics.Gabriel Murray, Giuseppe Carenini, and Raymond T.Ng.
2010b.
Generating and validating abstracts ofmeeting conversations: a user study.
In INLG.S.
B. Needleman and C. D. Wunsch.
1970.
A generalmethod applicable to the search for similarities inthe amino acid sequence of two proteins.
Journal ofmolecular biology, 48(3):443?453, March.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
Bleu: a method for automatic eval-uation of machine translation.
In Proceedings ofthe 40th Annual Meeting on Association for Com-putational Linguistics, ACL ?02, pages 311?318,Stroudsburg, PA, USA.
Association for Computa-tional Linguistics.Ehud Reiter and Robert Dale.
2000.
Building naturallanguage generation systems.
Cambridge Univer-sity Press, New York, NY, USA.Korbinian Riedhammer, Benoit Favre, and DilekHakkani-Tu?r.
2010.
Long story short - global unsu-pervised models for keyphrase based meeting sum-marization.
Speech Commun., 52(10):801?815, Oc-tober.Oana Sandu, Giuseppe Carenini, Gabriel Murray, andRaymond Ng.
2010.
Domain adaptation to sum-marize human conversations.
In Proceedings of the2010 Workshop on Domain Adaptation for NaturalLanguage Processing, DANLP 2010, pages 16?22,Stroudsburg, PA, USA.
Association for Computa-tional Linguistics.Alex J. Smola and Bernhard Scho?lkopf.
2004.
A tu-torial on support vector regression.
Statistics andComputing, 14(3):199?222, August.Andreas Stolcke.
2002.
SRILM ?
an extensible lan-guage modeling toolkit.
In Proceedings of ICSLP,volume 2, pages 901?904, Denver, USA.Marilyn A. Walker, Owen Rambow, and Monica Ro-gati.
2001.
Spot: a trainable sentence planner.In Proceedings of the second meeting of the NorthAmerican Chapter of the Association for Com-putational Linguistics on Language technologies,NAACL ?01, pages 1?8, Stroudsburg, PA, USA.
As-sociation for Computational Linguistics.Lu Wang and Claire Cardie.
2011.
Summarizing de-cisions in spoken meetings.
In Proceedings of theWorkshop on Automatic Summarization for DifferentGenres, Media, and Languages, WASDGML ?11,pages 16?24, Stroudsburg, PA, USA.
Associationfor Computational Linguistics.Lu Wang and Claire Cardie.
2012.
Focused meet-ing summarization via unsupervised relation extrac-tion.
In Proceedings of the 13th Annual Meeting ofthe Special Interest Group on Discourse and Dia-logue, SIGDIAL ?12, pages 304?313, Stroudsburg,PA, USA.
Association for Computational Linguis-tics.Lusheng Wang and Tao Jiang.
1994.
On the complex-ity of multiple sequence alignment.
Journal of Com-putational Biology, 1(4):337?348.Shasha Xie, Yang Liu, and Hui Lin.
2008.
Evaluatingthe effectiveness of features and sampling in extrac-tive meeting summarization.
In in Proc.
of IEEESpoken Language Technology (SLT.1405
