F IN ITE-STATE APPROXIMATIONOF PHRASE STRUCTURE GRAMMARSFernando C. N. PereiraAT&T Bell Laboratories600 Mountain Ave.Murray Hill, NJ 07974Rebecca N. WrightDept.
of Computer Science, Yale UniversityPO Box 2158 Yale StationNew Haven, CT 06520AbstractPhrase-structure grammars are an effective rep-resentation for important syntactic and semanticaspects of natural anguages, but are computa-tionally too demanding for use as language mod-els in real-time speech recognition.
An algorithmis described that computes finite-state approxi-mations for context-free grammars and equivalentaugmented phrase-structure grammar formalisms.The approximation is exact for certain context-free grammars generating regular languages, in-cluding all left-linear and right-linear context-freegrammars.
The algorithm has been used to con-struct finite-state language models for limited-domain speech recognition tasks.1 Mot ivat ionGrammars for spoken language systems are sub-ject to the conflicting requirements of languagemodeling for recognition and of language analysisfor sentence interpretation.
Current recognitionalgorithms can most directly use finite-state ac-ceptor (FSA) language models.
However, thesemodels are inadequate for language interpreta-tion, since they cannot express the relevant syntac-tic and semantic regularities.
Augmented phrasestructure grammar (APSG) formalisms, such asunification-based grammars (Shieber, 1985a), canexpress many of those regularities, but they arecomputationally ess suitable for language mod-eling, because of the inherent cost of computingstate transitions in APSG parsers.The above problems might be circumvented byusing separate grammars for language modelingand language interpretation.
Ideally, the recog-nition grammar should not reject sentences ac-ceptable by the interpretation grammar and itshould contain as much as reasonable of the con-straints built into the interpretation grammar.However, if the two grammars are built indepen-dently, those goals are difficult to maintain.
Forthis reason, we have developed a method for con-structing automatically a finite-state approxima-tion for an APSG.
Since the approximation servesas language model for a speech-recognition front-end to the real parser, we require it to be soundin the sense that the it accepts all strings in thelanguage defined by the APSG.
Without qualifica-tion, the term "approximation" will always meanhere "sound approximation.
"If no further constraints were placed on thecloseness of the approximation, the trivial al-gorithm that assigns to any APSG over alpha-bet E the regular language E* would do, but ofcourse this language model is useless.
One pos-sible criterion for "goodness" of approximationarises from the observation that many interest-ing phrase-structure grammars have substantialparts that accept regular languages.
That doesnot mean that the grammar rules are in the stan-dard forms for defining regular languages (left-linear or right-linear), because syntactic and se-mantic onsiderations often require that strings ina regular set be assigned structural descriptionsnot definable by left- or right-linear rules.
A use-ful criterion is thus that if a grammar generatesa regular language, the approximation algorithmyields an acceptor for that regular language.
Inother words, one would like the algorithm to be ex-act for APSGs yielding regular languages.
1 Whilewe have not proved that in general our methodsatisfies the above exactness criterion, we show inSection 3.2 that the method is exact for left-linearand right-linear grammars, two important classesof context-free grammars generating regular lan-guages.1 At first sight, this requirement may be seen as conflict-ing with the undecidability ofdetermining whether a CFGgenerates a regular language (Harrison, 1978).
However,note that the algorithm just produces an approximation,but cannot say whether the approximation is exact.2462 The AlgorithmOur approximation method applies to anycontext-free grammar (CFG), or any unification-based grammar (Shieber, 1985a) that can be fullyexpanded into a context-free grammar.
2 The re-sulting FSA accepts all the sentences acceptedby the input grammar, and possibly some non-sentences as well.The current implementation accepts as inputa form of unification grammar in which featurescan take only atomic values drawn from a speci-fied finite set.
Such grammars can only generatecontext-free languages, since an equivalent CFGcan be obtained by instantiating features in rulesin all possible ways.The heart of our approximation method is analgorithm to convert he LR(0) characteristic ma-chine .Ad(G) (Aho and Ullman, 1977; Backhouse,1979) of a CFG G into an FSA for a superset ofthe language L(G) defined by G. The characteris-tic machine for a CFG G is an FSA for the viableprefixes of G, which are just the possible stacksbuilt by the standard shift-reduce recognizer forG when recognizing strings in L(G).This is not the place to review the character-istic machine construction in detail.
However, toexplain the approximation algorithm we will needto recall the main aspects of the construction.
Thestates of .~4(G) are sets of dotted rules A ---* a .
\[3where A ---, a/~ is some rule of G. .A4(G) is thedeterminization by the standard subset construc-tion (Aho and Ullman, 1977) of the FSA definedas follows:?
The initial state is the dotted rule f f  ---, -Swhere S is the start symbol of G and S' is anew auxiliary start symbol.?
The final state is S' --~ S..?
The other states are all the possible dottedrules of G.?
There is a transition labeled X, where X is aterminal or nonterminal symbol, from dottedrule A -+ a .
X~ to A --+ c~X.//.?
There is an e-transition from A --~ a ?
B/~ toB --~ "7, where B is a nonterminal symboland B -+ 7 a rule in G.2Unification-based grammars not in this class wouldhave to be weakened first, using techniques akin to those ofSato and Tamaki (1984), Shieber (1985b) and Haas (1989).I S' -> .
S S -> .
Ab A ->.
A a A->.1Is'->s.\]'Aqk~ SA'>A'.ba Ja~\ [A .>Aa.
jFigure 1: Characteristic Machine for G1.A~(G) can be seen as the finite state control fora nondeterministic shift-reduce pushdown recog-nizer TO(G) for G. A state transition labeled by aterminal symbol z from state s to state s' licensesa shift move, pushing onto the stack of the recog-nizer the pair (s, z).
Arrival at a state containinga completed dotted rule A --~ a. licenses a reduc-tion move.
This pops from the stack as many pairsas the symbols in a, checking that the symbols inthe pairs match the corresponding elements of a,and then takes the transition out of the last statepopped s labeled by A, pushing (s, A) onto thestack.
(Full definitions of those concepts are givenin Section 3.
)The basic ingredient of our approximation algo-rithm is the f lattening of a shift-reduce recognizerfor a grammar G into an FSA by eliminating thestack and turning reduce moves into e-transitions.It will be seen below that flattening 7~(G) directlyleads to poor approximations in many interestingcases.
Instead, .bq(G) must first be unfolded intoa larger machine whose states carry informationabout the possible stacks of g (G) .
The quality ofthe approximation is crucially influenced by howmuch stack information is encoded in the states ofthe unfolded machine: too little leads to coarse ap-proximations, while too much leads to redundantautomata needing very expensive optimization.The algorithm is best understood with a simpleexample.
Consider the left-linear grammar G1S---.
AbA---* Aa JeAJ(G1) is shown on Figure 1.
Unfolding is not re-quired for this simple example, so the approximat-ing FSA is obtained from .Ad(G1) by the flatten-ing method outlined above.
The reducing states inAJ(G1), those containing completed otted rules,are states 0, 3 and 4.
For instance, the reductionat state 4 would lead to a transition on nonter-247Figure 2: Flattened FSA0aFigure 3: Minimal Acceptorminal A, to state 2, from the state that activatedthe rule being reduced.
Thus the correspondinge-transition goes from state 4 to state 2.
Addingall the transitions that arise in this way we ob-tain the FSA in Figure 2.
From this point on, thearcs labeled with nonterminals can be deleted, andafter simplification we obtain the deterministic fi-nite automaton (DFA) in Figure 3, which is theminimal DFA for L(G1).If flattening were always applied to the LR(0)characteristic machine as in the example above,even simple grammars defining regular languagesmight be inexactly approximated by the algo-rithm.
The reason for this is that in general thereduction at a given reducing state in the char-acteristic machine transfers to different states de-pending on context.
In other words, the reducingstate might be reached by different routes whichuse the result of the reduction in different ways.Consider for example the grammar G2S ~ aXa \] bXbX -'* cwhich accepts just the two strings aca and bcb.Flattening J~4(G2) will produce an FSA that willalso accept acb and bca, an undesirable outcome.The reason for this is that the e-transitions leav-ing the reducing state containing X ~ c. do notdistinguish between the different ways of reach-ing that state, which are encoded in the stack ofOne way of solving the above problem is to un-fold each state of the characteristic machine intoa set of states corresponding to different stacks atthat state, and flattening the corresponding recog-nizer rather than the original one.
However, theset of possible stacks at a state is in general infi-nite.
Therefore, it is necessary to do the unfoldingnot with respect o stacks, but with respect o afinite partition of the set of stacks possible at thestate, induced by an appropriate equivalence r la-tion.
The relation we use currently makes twostacks equivalent if they can be made identicalby collapsing loops, that is, removing portions ofstack pushed between two arrivals at the samestate in the finite-state control of the shift-reducerecognizer.
The purpose of collapsing loops is to~forget" stack segments that may be arbitrarilyrepeated, s Each equivalence class is uniquely de-fined by the shortest stack in the class, and theclasses can be constructed without having to con-sider all the (infinitely) many possible stacks.3 Formal PropertiesIn this section, we will show here that the approx-imation method described informally in the pre-vious section is sound for arbitrary CFGs and isexact for left-linear and right-linear CFGs.In what follows, G is a fixed CFG with termi-nal vocabulary ~, nonterminal vocabulary N, andstart symbol S; V = ~ U N.3.1 SoundnessLet J~4 be the characteristic machine for G, withstate set Q, start state so, set of final states F,and transition function ~ : S x V --* S. As usual,transition functions uch as 6 are extended frominput symbols to input strings by defining 6(s, e) --s and 6is , a/~) = 5(6(s, a),/~).
The shift-reducerecognizer 7~ associated to A4 has the same states,start state and final states.
Its configurations aretriples Is, a, w) of a state, a stack and an inputstring.
The stack is a sequence of pairs / s, X) of astate and a symbol.
The transitions of the shift-reduce recognizer are given as follows:Shift: is, a, zw) t- (s', a/s, z), w) if 6(s, z) = s'Reduce:  is, err, w) ~- /5( s ' ,  A), cr/s', A/, w) if ei-ther (1) A --~ ?
is a completed otted rule3Since possible stacks can be shown to form a regularlanguage, loop collapsing has a direct connection to thepumping lemma for regular languages.248in s, s"  = s and r is empty, or (2) AX1.
.
.Xn .
is a completed dotted rule in s,T = is1,  X l )  .
.
.
( sn ,Xn)  and s" = 81.The initial configurations of ~ are (so, e, w} forsome input string w, and the final configurationsare ( s, (so, S), e) for some state s E F.  A deriva-tion of a string w is a sequence of configura-tions c0 , .
.
.
, cm such that  c0 = (s0,e,w), c,~ =( s, (so, S), e) for some final state s, and ei-1 l- cifor l< i<n.Let s be a state.
We define the set Stacks(s) tocontain every sequence (s0 ,X0) .
.
.
(sk,Xk) suchthat si = 6(s i - l ,X i -1 ) , l  < i < k and s =6(st,  Xk).
In addition, Stacks(s0) contains theempty  sequence .
By construction, it is clear thatif ( s, a, w) is reachable from an initial configura-tion in ~,  then o- E Stacks(s).A stack congruence on 7?
is a family of equiv-alence relations _=o on Stacks(s) for each states E 8 such that  if o- = ,  a '  and/f(s ,  X)  = d theno-(s,X} =,,  , r (s ,X) .
A stack congruence ---- par-titions each set Stacks(s) into equivalence classes\[<r\]?
of the stacks in Stacks(s) equivalent o o- un-der --_,.Each stack congruence - on ~ induces a cor-responding unfolded recognizer 7~-.
The states ofthe unfolded recognizer axe pairs i s, M , ) ,  notatedmore concisely as \[~\]?, of a state and stack equiv-alence class at that state.
The initial state is \[e\],o,and the final states are all \[o-\]?
with s E F ando- E Stacks(s).
The transit ion function 6- of theunfolded recognizer is defined byt-(\[o-\]', x )  = \[o-is, x) \ ]  '( ' 'x)That  this is well-defined follows immediately fromthe definition of stack congruence.The  definitions of dotted rules in states, config-urations, shift and reduce transitions given abovecarry over immediately to unfolded recognizers.Also, the characteristic recognizer can also be seenas an unfolded recognizer for the trivial coarsestcongruence.Unfolding a characteristic recognizer does notchange the language accepted:P ropos i t ion  1 Let G be a CFG, 7~ its charac-teristic recognizer with transition function ~, and= a stack congruence on T?.
Then the unfoldedrecognizer ~=_ and 7~ are equivalent recognizers.Proof :  We show first that any string w acceptedby T?--- is accepted by 7~.
Let do, .
.
.
,dm be aderivation of w in ~=.
Each di has the formdi = ( \ [P / \ ] " ,  o'i ,  ul), and can be mapped to an T?configuration di = (sl, 8i, ul), where ?
= E and((s, C), X) = 8i  s, X) .
It  is straightforward to ver-ify that  do , .
.
.
,  d,, is a derivation of w in ~.Conversely, let w E L(G),  and c0 , .
.
.
,em bea derivation of w in 7~, with ci = isl,o-i, ui).We define el = (\[~ri\] s~, hi, ui), where ~ = e ando-is, x )  = aito-\]', x ) .If ci-1 P ci is a shift move, then ui-1 = zui  and6(s i - l ,  z)  = si.
Therefore,6-@,_ , \ ] " - ' ,~)  = \[o-~-,(s~-,,~)\]~("- '")= \[o-,\]',Furthermore,~ = o-~- l (S , -  1, ~) = ~, -1  (\[o-,- 1 \ ] " - ' ,  ~)Thus we have~',-x = ( \ [o - l -d" - ' ,a i -x , *u , )~, = @d",e~- l (P~-d"- ' , * ) ,~'~)with 6_=(\[o-i-1\]"-', z) = \[o-i\]".
Thus, by definitionof shift move, 6i-1 I- 6i in 7?_--.Assume now that  ei-1 I- ci is a reduce move in~.
Then ui = ui-1 and we have a state s in 7~,a symbol  A E N,  a stack o- and a sequence r ofstate-symbol pairs such thatsi = 6(s,A)o-i-1 = o"1"o-, = o-(s,a)and either(a) A --* ?
is in s i - t ,  s = si-1 and r = e, or(b) A ---, X I .
.
.Xn .
is in si-1 , r =(ql, Xd .
.
.
(q., X . )
and s = ql-Let ~ = \[o-\]*.
Then6=(~,A) = \[o-(s,A)p0,A)= \[o-d"We now define a pair sequence ~ to play thesame role in 7~- as r does in ~.
In case (a)above, ~ = e. Otherwise, let rl = e and ri =r i - l (q i - l ,X i -1 )  for 2 < i ( n, and define ~ by= (\[d q', x l ) .
.
.
@h i  q', xi)  ?
?
?
( \ [~ .p - ,  x .
)ThenO'i-- 1 --~- 0"7"= o- (q1 ,X1) .
.
.
(q .
-x ,x .
-x )249Thusx.)
-- ?
r (q~,X ,} .
.
.
(q i -hX i - l )xd-- .
x.
)== a(\[d',A)= a(#,A)~i = (~f=(&A),a(~,A),ui)which by construction of e immediately entailsthat ~_ 1 ~- Ci is a reduce move in ~=.
flFor any unfolded state p, let Pop(p) be the setof states reachable from p by a reduce transition.More precisely, Pop(p) contains any state pl suchthat there is a completed otted rule A --* (~.
inp and a state pll such that 6-(p I~, ~) - p and6 - ( f * ,A )  -- f .
Then the flattening ~r= of~-  isa nondeterministic FSA with the same state set,start state and final states as ~-  and nondeter-ministic transition function @= defined as follows:?
I f  6=(p,z) - pt for some z E E, then f E?
I f  p~ E Pop(p) then f E ~b=(p, ~).Let co , .
.
.
,  cm be a derivation of string w in ~,and put ei -- (q~,~q, wl), and p~ = \[~\]~'.
Byconstruction, if ci_~ F ci is a shift move on z(wi-x -- zw~), then 6=(pi - l ,Z)  = Pi, and thusp~ ~ ~-(p~_~, z).
Alternatively, assume the transi-tion is a reduce move associated to the completeddotted rule A --* a..  We consider first the casea ~ ~.
Put a -- X1 .
.
.
X~.
By definition of reducemove, there is a sequence of states r l , .
.
.
,  r~ anda stack # such that o'i-x = ?
(r~, X1)... (rn, Xn),qi -- #(r~,A), 5(r~,A) = qi, and 5(rj,X1) - ri+~for 1 ~ j < n. By  definition of stack congruence,we will then have=where rx = ?
and rj = ( r~,X , ) .
.
.
( r~-x ,X~- , )  forj > 1.
Furthermore, again by definition of stackcongruence we have 6=(\[cr\] r*,A) = Pi.
Therefore,Pi 6 Pop(pi_l) and thus pi e ~_--(pi-x,?).
A sim-ilar but simpler argument allows us to reach thesame conclusion for the case a = e. Finally, thedefinition of final state for g= and ~r__ makes Pma final state.
Therefore the sequence P0, .
- .
,Pmis an accepting path for w in ~r_.
We have thusprovedPropos i t ion  2 For any CFG G and stack con-gruence =_ on the canonical LR(0) shift-reduce rec-ognizer 7~(G) of G, L(G) C_ L(~r-(G)), where~r-(G) is the flattening of ofT~(G)--.Finally, we should show that the stack collaps-ing equivalence described informally earlier is in-deed a stack congruence.
A stack r is a loop if' / "  - "  (81, X1)... (sk, Xk) and 6(sk, Xt )  = sz.
Astack ~ collapses to a stack ~' if cr = pry, cr ~ = pvand r is a loop.
Two stacks are equivalent if theycan be collapsed to the same stack.
This equiv-alence relation is closed under suffixing, thereforeit is a stack congruence.3 .2  ExactnessWhile it is difficult to decide what should be meantby a "good" approximation, we observed earlierthat a desirable feature of an approximation algo-r ithm would be that it be exact for a wide class ofCFGs generating regular languages.
We show inthis section that our algorithm is exact both forleft-linear and for right-linear context-free gram-mars, which as is well-known generate regular lan-guages.The proofs that follow rely on the following ba-sic definitions and facts about the LR(0) construc-tion.
Each LR(0) state s is the closure of a set ofa certain set of dotted rules, its core.
The closure\[R\] of a set R of dotted rules is the smallest setof dotted rules containing R that contains B --~ "7whenever it contains A --~ a ?
Bfl and B ---* 7 isin G. The core of the initial state so contains justthe dotted rule f f  ~ .S.
For any other state s,there is a state 8 ~ and a symbol X such that 8 isthe closure of the set core consisting of all dottedrules A ~ aX.
/~ where A --* a .
X/~ belongs to s'.3 .3  Le f t -L inear  GrammarsIn this section, we assume that the CFG G is left-linear, that is, each rule in G is of the form AB/~ or A --+/~, where A, B E N and/3 E ~*.P ropos i t ion  3 Let G be a left-linear CFG, andlet gz be the FSA produced by the approximationalgorithm from G. Then L(G) = L(3r).Proof :  By Proposition 2, L(G) C. L(.~').
Thus weneed only show L(~)  C_ L(G).The proof hinges on the observation that eachstate s of At(G) can be identified with a stringE V* such that every dotted rule in s is of thefo rmA ~ ~.a  for some A E N and c~ E V*.250Clearly, this is true for so = \[S' --* .S\], with ~0 = e.The core k of any other state s will by constructioncontain only dotted rules of the form A ~ a .with a ~ e. Since G is left linear, /3 must bea terminal string, ensuring that s = \[h\].
There-fore, every dotted rule A --* a .
f in s must resultfrom dotted rule A ~ .aft in so by the sequenceof transitions determined by a (since ?tq(G) is de-terministic).
This means that if A ~ a .
f andA' --* a ' .
fl' are in s, it must be the case thata - a ~.
In the remainder of this proof, let ~ = swhenever a = ~.To go from the characteristic machine .M(G) tothe FSA ~', the algorithm first unfolds Ad(G) us-ing the stack congruence relation, and then flat-tens the unfolded machine by replacing reducemoves with e-transitions.
However, the above ar-gument shows that the only stack possible at astate s is the one corresponding to the transitionsgiven by $, and thus there is a single stack con-gruence state at each state.
Therefore, .A4(G)will only be flattened, not unfolded.
Hence thetransition function ?
for the resulting flattenedautomaton ~" is defined as follows, where a EN~* U \]~*,a E ~, and A E N:(a) ?
(~,a)  = {~}(b) ?
(5, e) = {.4 I A --, a e G}The start state of ~" is ~.
The only final state is S.We will establish the connection between Y~derivations and G derivations.
We claim that ifthere is a path from ~ to S labeled by w then ei-ther there is a rule A --* a such that w = xy andS :~ Ay  =~ azy ,  or a = S and w = e. The claimis proved by induction on Iw\[.For the base case, suppose.
\[w I = 0 and there is apath from & to .~ labeled by w. Then w = e, andeither a - S, or there is a path of e-transitionsfrom ~ to S. In the latter case, S =~ A =~ e forsome A E N and rule A --~ e, and thus the claimholds.Now, assume that the claim is true for all Iwl <k, and suppose there is a path from & to ,~ labeledw I, for some \[wl\[ = k. Then w I - aw for some ter-minal a and Iw\[ < k, and there is a path from ~-~to S labeled by w. By the induction hypothesis,S =~.
Ay  =~ aaz 'y ,  where A --.
* aaz  ~ is a rule andz ly  - w (since aa y?
S).
Letting z -- ax I, we havethe desired result.If w E L(~),  then there is a path from ~ tolabeled by w. Thus, by claim just proved, S =~Ay ::~ :cy, where A ~ ?
is a rule and w = ~y(since e # S).
Therefore, S =~ w, so w ~ L(G),  asdesired.3 .4  R ight -L inear  GrammarsA CFG G is right linear if each rule in G is of theform A --~ fB  or A --* /3, where A, B E N andPropos i t ion  4 Let G be a right-linear CFG and9 e be the unfolded, f lattened automaton producedby the approximation algorithm on input G. ThenL(G)  = L(Yz).Proof :  As before, we need only show L(~') CL(G).Let ~ be the shift-reduce recognizer for G. Thekey fact to notice is that, because G is right-linear,no shift transition may follow a reduce transition.Therefore, no terminal transition in 3 c may followan e-transition, and after any e-transition, thereis a sequence of G-transitions leading to the finalstate \[$' --* S.\].
Hence ~" has the following kinds ofstates: the start state, the final state, states withterminal transitions entering or leaving them (wecall these reading states), states with e-transitionsentering and leaving them (prefinal states), andstates with terminal transitions entering them ande-transitions leaving them (cr0ssover states).
Anyaccepting path through ~" will consist of a se-quence of a start state, reading states, a crossoverstate, prefinal states, and a final state.
The excep-tion to this is a path accepting the empty string,which has a start state, possibly some prefinalstates, and a final state.The above argument also shows that unfoldingdoes not change the set of strings accepted by ~,because any reduction in 7~= (or e-transition injc), is guaranteed to be part of a path of reductions(e-transitions) leading to a final state of 7~_- (~).Suppose now that w = w: .
.
.
wn is accepted by~'.
Then there is a path from the start state Sothrough reading states s l , .
.
.
,  s,,-1, to crossoverstate sn, followed by e-transitions to the finalstate.
We claim that if there there is a path fromsl to sn labeled wi+l .
.
.wn ,  then there is a dot-ted rule A ---* x ?
yB  in si such B :~ z and yz =w~+1.. .wn,  where A E N ,B  E NU~*,y ,z  ~ ~*,and one of the following holds:(a) z is a nonempty suffix of wt .
.
.
wi,(b) z = e, A" =~ A, A'  --* z ' .
A"  is a dotted rulein sl, and z t is a nonempty suffix ofT1 .
.
.wi,or(c) z=e,  s i=s0 ,  andS=~A.We prove the claim by induction on n - i. Forthe base case, suppose there is an empty path from251Sn to s , .
Because sn  is the crossover state, theremust be some dotted rule A ~ x. in sn .
Lettingy = z = B = e, we get that A ---* z .
yB  is a dottedrule of s ,  and B = z.
The dotted rule A --', z .
yBmust have either been added to 8n by closure orby shifts.
I f  it arose from a shift, z must be anonempty suffix of wl .
.
.wn.
If  the dotted rulearose by closure, z = e, and there is some dottedrule A ~ --~ z t ?
A" such that A" =~ A and ~l is anonempty suffix of Wl .
.
.
wn.Now suppose that the claim holds for paths fromsi to sn, and look at a path labeled w i .
.
.wnfrom si-1 to sn.
By the induction hypothesis,A ~ z ?
yB  is a dotted rule of st, where B =~ z,uz  = w i+ l .
.
.wn ,  and (since st ~ s0), either z is anonempty suffix of wl .
.
.
wi or z = e, A ~ - .
z ~.
A"is a dotted rule of si, A" :~ A, and z ~ is anonempty suffix of wl  .
.
.
w l .In the former case, when z is a nonempty suffixof wl  .
.
.
w l ,  then z = w j  .
.
.
w i  for some 1 < j <i.
Then A ---, w j  .
.
.w l  ?
yB  is a dotted rule ofsl, and thus A ---* w j  .
.
.w i -1  ?
w iyB  is a dottedrule o fs i _ l .
I f j  < i -  1, then wj .
.
.w i _ l  is anonempty suffix of w l .
.
.w i -1 ,  and we are done.Otherwise, wj  .
.
.w i -1  = e, and so  A --* .w iyB  is adotted rule ofs i -1 .
Let y~ = w iy .
Then A ~ .yJBis a dotted rule of si-1, which must have beenadded by closure.
Hence there are nonterminalsA I and A" such that A" :~ A and A I ~ z I ?
A"is a dotted rule of s t - l ,  where z ~ is a nonemptysUtTLX of Wl .. ?
wi -  1.In the latter case, there must be a dotted ruleA ~ ~ w j  .
.
.w i -1  ?
w iA"  in si-1.
The rest of theconditions are exactly as in the previous case.Thus, if w - w l .
.
.wn  is accepted by ~c, thenthere is a path from so to sn labeled by wl .
.
.
w,.Hence, by the claim just proved, A ~ z .
yB  isa dotted rule of sn, and B :~ z, where yz  -"wl .
.
.wa  -- w. Because the st in the claim isso, and all the dotted rules of si can have nothingbefore the dot, and z must be the empty string.Therefore, the only possible case is case 3.
Thus,S :~ A ---, yz  = w, and hence w E L (G) .
Theproof that the empty string is accepted by ~" onlyif it is in L(G)  is similar to the proof of the claim.D4 A Complete ExampleThe appendix shows an APSG for a small frag-ment of English, written in the notation acceptedby the current version of our grammar compiler.The categories and features used in the grammarare described in Tables 1 and 2 (categories withoutfeatures are omitted).
Features enforce person-number agreement, personal pronoun case, and alimited verb subcategorization scheme.Grammar compilation has three phrases: (i)construction of an equivalent CFG, (ii) approxi-mation, and (iii) determinization and minimiza-tion of the resulting FSA.
The equivalent CFG isderived by finding all full instantiations of the ini-tial APSG rules that are actually reachable in aderivation from the grammar's  start symbol.
Inthe current implementation, the construction ofthe equivalent CFG is is done by a Prolog pro-gram, while the approximator, determinizer andminimizer are written in C.For the example grammar, the equivalent CFGhas 78 nonterminals and 157 rules, the unfoldedand flattened FSA 2615 states and 4096 transi-tions, and the determinized and minimized finalDFA 16 states and 97 transitions.
The runtimefor the whole process is 4.91 seconds on a SunSparcStation 1.Substantially larger grammars, with thousandsof instantiated rules, have been developed for aspeech-to-speech translation project.
Compilationtimes vary widely, but very long compilations ap-pear to be caused by a combinatorial explosion inthe unfolding of right recursions that will be dis-cussed further in the next section.5 Informal AnalysisIn addition to the cases of left-linear and right-linear grammars discussed in Section 3, our algo-r ithm is exact in a variety of interesting cases, in-cluding the examples of Church and Patil (1982),which illustrate how typical attachment ambigu-ities arise as structural ambiguities on regularstring sets.The algorithm is also exact for some self-embedding rammars 4 of regular languages, suchasS --+ aS  l Sb  l cdefining the regular language a*eb*.A more interesting example is the following sim-plified grammar for the structure of English noun4 A grammar isself-embedding if and only if licenses thederivation X ~ c~X~ for nonempty c~ and/3.
A languageis regular if and only if it can be described by some non-self-embedding grammar.252Figure 4: Acceptor for Noun Phrasesphrases:NP -+ Det Nom \[ PNDet -+ Art \] NP'sNom -+ N I Nom PP J Adj NomPP --* P NPThe symbols Art, N, PN and P correspond to theparts of speech article, noun, proper noun andpreposition.
From this grammar, the algorithmderives the DFA in Figure 4.As an example of inexact approximation, con-sider the the self-embedding CFGS -+ aSb I ~for the nonregular language a'~b'~,n > O. Thisgrammar is mapped by the algorithm into an FSAaccepting ~ I a+b+.
The effect of the algorithm isthus to "forget" the pairing between a's and b'smediated by the stack of the grammar's charac-teristic recognizer.Our algorithm has very poor worst-case perfor-mance.
First, the expansion of an APSG into aCFG, not described here, can lead to an exponen-tial blow-up in the number of nonterminals andrules.
Second, the subset calculation implicit inthe LR(0) construction can make the number ofstates in the characteristic machine xponentialon the number of CF rules.
Finally, unfolding canyield another exponential b ow-up in the numberof states.However, in the practical examples we have con-sidered, the first and the last problems appear tobe the most serious.The rule instantiation problem may be allevi-ated by avoiding full instantiation of unificationgrammar rules with respect o "don't care" fea-tures, that is, features that are not constrained bythe rule.The unfolding problem is particularly serious ingrammars with subgrammars of the formS -+ X IS  I ""  J X, ,S J Y (I)It is easy to see that the number of unfolded statesin the subgrammar is exponential in n. This kindof situation often arises indirectly in the expan-sion of an APSG when some features in the right-hand side of a rule are unconstrained and thuslead to many different instantiated rules.
In fact,from the proof of Proposition 4 it follows immedi-ately that unfolding is unnecessary for right-lineargrammars.
Ultimately, by dividing the gram-mar into non-mutually recursive (strongly con-nected) components and only unfolding center-embedded components, this particular problemcould he avoided, s In the meanwhile, the prob-lem can be circumvented by left factoring (1) asfollows:S -+ ZS\ [Yz -+x ,  I...IX.6 Related Work and Conclu-sionsOur work can be seen as an algorithmic realizationof suggestions ofChurch and Patil (1980; 1982) onalgebraic simplifications of CFGs of regular lan-guages.
Other work on finite state approximationsof phrase structure grammars has typically re-lied on arbitrary depth cutoffs in rule application.While this is reasonable for psycholinguistic mod-eling of performance r strictions on center embed-ding (Pulman, 1986), it does not seem appropriatefor speech recognition where the approximatingFSA is intended to work as a filter and not re-ject inputs acceptable by the given grammar.
Forinstance, depth cutoffs in the method escribed byBlack (1989) lead to approximating FSAs whoselanguage is neither a subset nor a superset of thelanguage of the given phrase-structure grammar.In contrast, our method will produce an exact FSAfor many interesting grammars generating regularlanguages, uch as those arising from systematicattachment ambiguities (Church and Patil, 1982).It important to note, however, that even when theresult FSA accepts the same language, the origi-nal grammar is still necessary because interpreta-SWe have already implemented a version of the algo-rithm that splits the grammar into strongly connected com-ponents, approximates and minimizes separately each com-ponent and combines the results, but the main purpose ofthis version is to reduce approximation a d determinizationcosts for some grmmmars.253tion algorithms are generally expressed in terms ofphrase structures described by that grammar,  notin terms of the states of the FSA.Although the algorithm described here hasmostly been adequate for its intended applica-tion - -  grammars ufficiently complex not to beapproximated within reasonable time and spacebounds usually yield automata  that are far toobig for our current real-time speech recognitionhardware - -  it would be eventually of interest tohandle right-recursion i  a less profligate way.
In amore theoretical vein, it would also be interestingto characterize more tightly the class of exactlyapproximable grammars.
Finally, and most spec-ulatively, one would like to develop useful notionsof degree of approximation of  a language by a reg-ular language.
Formal-language-theoretic notionssuch as the rational index (Boason et al, 1981)or probabilistic ones (Soule, 1974) might be prof-itably investigated for this purpose.AcknowledgmentsWe thank Mark Liberman for suggesting that welook into finite-state approximations and PedroMoreno, David Roe, and Richard Sproat for try-ing out several prototypes of the implementationand supplying test grammars.ReferencesAlfred V. Aho and Jeffrey D. Ullman.
1977.
Princi.pies of Compiler Design.
Addison-Wesley, Reading,Massachusetts.Roland C. Backhouse.
1979.
Syntaz o\] ProgrammingLanguages--Theorll and Practice.
Series in Com-puter Science.
Prentice-Hall, Englewood Cliffs, NewJersey.Alan W. Black.
1989.
Finite state machines from fea-ture grammars.
In Masaru Tomita, editor, Inter.national Workshop on Parsing Technologies, pages277-285, Pittsburgh, Pennsylvania.
Carnegie Mel-lon University.Luc Boason, Bruno Courcelle, and Maurice Nivat.1981.
The rational index: a complexity measure forlanguages.
SIAM Journal o\] Computing, 10(2):284-296.Kenneth W. Church and Ramesh Patil.
1982.
Copingwith syntactic ambiguity or how to put the blockin the box on the table.
Computational Linguistics,8(3--4):139-149.Kenneth W. Church.
1980.
On memory \]imitations in?
natural anguage processing.
Master's thesis, M.I.T.Published as Report MIT/LCS/TR-245.Andrew Haas.
1989.
A parsing algorithm forunification grammar.
Computational Linguistics,15(4):219-232.Michael A. Harrison.
1978.
Introduction to FormalLanguage Theor~l.
Addison-Wesley, Reading, Mas-sachussets.Steven G. Pulman.
1986.
Grammars, parsers, andmemory limitations.
Language and Cognitive Pro-cesses, 1(3):197-225.Taisuke Sato and Hisao Tamaki.
1984.
Enumerationof success patterns in logic programs.
TheoreticalComputer Science, 34:227-240.Stuart M. Shieber.
1985a.
An Introduction toUnification-Based Approaches to Grammar.
Num-ber 4 in CSLI Lecture Notes.
Center for the Studyof Language and Information, Stanford, California.Distributed by Chicago University Press.Stuart M. Shieber.
1985b.
Using restriction to ex-tend parsing algorithms for complex-feature-basedformalisms.
In ~3rd Annual Meeting of the Asso-ciation \]or Computational Linguistics, pages 145-152, Chicago, Illinois.
Association for Computa-tionai Linguistics, Morristown, New Jersey.Stephen Soule.
1974.
Entropies of probabilistic gram-mars.
In\]ormation and Control, 25:57-74.Appendix APSG Formalismand ExampleNonterminal symbols (syntactic ategories) may havefeatures that specify variants of the category (eg.
sin-gular or plural noun phrases, intransitive or transitiveverbs).
A category cat with feature constraints i writ-tencat# \[ca, ?
?
?, em3.Feature constraints for feature f have one of theforms.f  = ,, (2)\] = c (3).f = (c~ .
.
.
.
.
c . )
(4)where v is a variable name (which must be capitalized)and c, c l , .
.
.
,  c,  are feature values.All occurrences of a variable v in a rule stand forthe same unspecified value.
A constraint with form (2)specifies a feature as having that value.
A constraintof form (3) specifies an actual value for a feature, anda constraint of form (4) specifies that a feature mayhave any value from the specified set of values.
Thesymbol "!"
appearing as the value of a feature in theright-hand side of a rule indicates that that featuremust have the same value as the feature of the samename of the category in the left-hand side of the rule.This notation, as well as variables, can be used to en-force feature agreement between categories in a rule,?254Symbol Category Featuress sentencenpvpargsdetnpronVnoun phraseverb phraseverb argumentsdeterminernounpronounverbn (number), p(person)n, p,  c (case)n, p, t (verb type)tnnn, p, Cn, p, tTable 1: Categories of Example GrammarFeaturen' (number)p (person)c (case)t (verb type)Valuess (singular), p (plural)!
(first), 2 (second), 3 (third)s (subject), o (nonsubject)i (intransitive), t (transitive), d(ditransitive)Table 2: Features of  Example Grammarfor instance, number agreement between Subject andverb.It is convenient to declare the features and possiblevalues of categories with category declarations appear-ing before the grammar ules.
Category declarationshave the formcat CatS\[  /1 = (V l l  .
.
.
.
,V2kl),.
.
o ,fm = (vml .
.
.
.
,Vmk,) \ ] .giving all the possible values of all the features for thecategory.The declarations tar t  cat.declares cat as the start symbol of the grammar.In  the grammar ules, the symbol " ' "  prefixes ter-minal symbols, commas are used for sequencing and\[" for alternation.s ta r t  s.cat sg\[n=Cs,p),p=(1,2,3)\].cat npg\[n=(s,p) ,p=(1,2,3) ,c=(s,o)\].cat vpg\[n=(s,p) , l>=(1,2,3),type=(i,t ,d)\].cat argsg\[type=(i .
t ,d) \ ] .cat detg\[n=(s,p)\].cat ng\[n=(s,p)\].cat prong\[n=(s,p),p=(1,2,3),c=(s,o)\].cat vg\[n-(s ,p) ,p=(1,2,3) , type=(i , t ,d) \ ] .s => npg\[n=!
,pffi!
,c=s\],  vpg\[n=!
,p=!\].npg\[p=3\] => detg\ [n=!\ ] ,  adjs ,  ng\[n=!\] .n l~\ [n=s ,p -3 \ ]  -> pn.np => prong In= !, p= !, c= !
\ ] .prong \[n=s,p-1, c=s\]  => ' i .prong \[p=2\] => ' you.prong\[n=s,p=3,c=s\]  => 'he I 'she.prong\[n-s ,p-3\]  => ' i t .prong\[nffip,l~l,c-s\] => 'vs .prong\[n=p,p=3,c=s\] > ' they.prong\[n=s,p- l ,c=o\]  => 'me.prong\[n=s,p=3,c=o\]  => 'him \[prong\[n=p,p=1,c=o\] > 'us .prong\[n=p,p-3,c=o\] => 'them.
'her.vp => vg\[n=!
,p=!
, type=: \ ] ,  argsg\ [ type=!\ ] .adjs -> ~.adjs => adj,  adjs .args#\[type=i\]  => \ [ \ ] .args#\[type=t\]  => npg\[c=o\].argsg\[type-d\]  =>npg\[c=o\],  ' to ,  npg\[cfo\ ] .pn => ' ton  I 'd ick  \[ 'har ry .det => 'soaeJ ' the .det#\[n=s\] =>'every  \[ 'a ,det#\[n-p\]  => 'a l l  \[ 'most.n#\[n=s\] => ' ch i ld  \[ 'cake.n#\[n~p\] => ' ch i ld ren  I 'cakes.adj .
-> 'n ice  J ' sgeet .v#\[n=s, l~3,type=i\]  > ' s leeps .v#\[nffip,type=i\] > ' s leep .v#\[n=s, l~,(1,2),type=/\]  => 's leep.v#\[n-s ,p-3 , type=t\ ]  -> 'eats .v#\[n~p,type-t \ ]  => 'eat .v#\[n=s,p- (1 ,2) , type=t\ ]  ffi> 'eat .v#\[n=s,pffi3,type=d\] > 'g ives .v#\[nffip,type-d\] =>'g ive.v#\[n=s,p=(1,2),type=d\] => 'g ive .255
