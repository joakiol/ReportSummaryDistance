RECENT ADVANCES IN JANUS:A SPEECH TRANSLATION SYSTEMM.Woszczyna, N.Coccaro, A.Eisele, A.Lavie, A.McNair, ZPolzin, l.Rogina,C.PRose, T.Sloboda, M.Tomita, J.Tsutsumi, N.Aoki-Waibel, A.Waibel, W. WardCarnegie Mel lon Univers i tyUniversity o f  Kar lsruheABSTRACTWe present recent advances from our efforts in increasing cover-age, robustness, generality and speed of JANUS, CMU's speech-to-speech translation system.
JANUS is a speaker-independent systemtranslating spoken utterances in English and also in German intoone of German, English or Japanese.
The system has been designedaround the task of conference r gistration (CR).
It has initially beenbuilt based on a speech database of i 2 read dialogs, encompassing avocabulary ofaround 500 words.
We have since been expanding thesystem along several dimensions toimprove speed, robustness andcoverage and to move toward spontaneous input.Speech ia Source r~n~,u~eN-best SearchLR-Parser \] I NN-Parser \] l Semantic Pra.1.
~TRODUCTIONIn this paper we describe recent improvements of JANUS,a speech to speech translation system.
Improvements havebeen made mainly along the following dimensions: 1.)
bet-ter context-dependent modeling improves performance in thespeech recognition module, 2.)
improved language models,smoothing, and word equivalence classes improve coverageand robustness of the sentence that the system accepts, 3.
)an improved N-best search reduces run-time from severalminutes to now real time, 4.)
trigram and parser escoringimproves election of suitable hypotheses from the N-bestlist for subsequent translation.
On the machine translationside, 5.)
a cleaner interlinguawas designed and syntactic anddomain-specific analysis were separated for greater reusabil-ity of components and greater quality of translation, 6.)
asemantic parser was developed to achieve semantic analysis,should more careful analysis fail.The JANUS \[1, 2\] framework as it is presented here alsoallows us to experiment with components ofa speech transla-tion system, in an effort o achieve both robustness and high-quality translation.
In the following we describe these ffortsand system components that have been developed to date.
Atpresent, JANUS consists conceptually out of three major com-ponents: speech recognition, machine translation and speechsynthesis.
Since we have not made any significant attempts atimproving performance on the synthesis end (DEC-talk andsynthesizers produced by NEC and AEG-Daimler are usedfor English, Japanese and Gerrnan output, respectively), ourdiscussion will focus on the recognition and translation parts.211Generate I.J.Synthesize J.,1Figure 1: Overview of theSystem2.
RECOGNIT ION ENGINEOur recognition engine uses several techniques tooptimize theoverall system performance.
Speech input is preprocessedinto time frames of spectral coefficients.
Acoustic modelsare trained to give a score for each phoneme, representingthe phoneme probability at the given frame.
These scoresare used by an N-best search algorithm to produce a list ofsentencchypothcses.
Ba ed on thislist, more computationallyexpensive language models are then applied to achieve furtherimprovement ofrecognition accuracy.2.1.
Acoust ic  mode l ingFor acoustic modeling, several alternative algorithms are be-ing evaluated including TDNN, MS-TDNN, MLP and LVQ\[6, 5\].
In the main JANUS system, an LVQ algorithm withcontext-dependent phonemes i now used for speaker inde-pendent recognition.
For each phoneme, there is a contextindependent set of prototypical vectors.
The output scoresfor each phoneme segment are computed from the euclidiandistance using context dependent segment weights.Error rates using context dependent phonemes are lower bya factor 2 to 3 for English (1.5 to 2 for German) than usingcontext independent phonemes.
Results are shown in table 1.English Germanlanguage model PP WA PP WAnoneword-pairsbigramssmoothed bigramsafter esorting400.0 58.228.9 83A16.2 92.618.1 91.598.8425.0 63.020.8 89.118.3 93.728.90 84.7Table 1: Word Accuracy for First HypothesisThe performance on the RM-task at comparable perplexitiesis significantly better than for the CR-task, suggesting that heCR-task is somewhat more difficult.2.2.
SearchThe search module of the recognizer builds a sorted list ofsentence hypotheses.
Speed and memory requirements couldbe dramatically improved: Though the amount of hypothesescomputed for each utterance was increased from 6 to 100hypotheses, the time required for their computation could bereduced from typically 3 minutes to 3 seconds.This was achieved by implementing the word dependent N-best algorithm\[3\] as backward pass in the forward backwardalgorithm\[4\]: First a fast firstbest only search is performed,saving the scores at each possible word ending.
In a secondpass, this information is used for aggressive pruning to re-duce the search effort for the N-best search.
Further speedupwas achieved by dynamically adapting the beam width to keepnumber of active states constant, and by carefully avoiding theevaluation of states in large inactive regions of words.
Impor-tant for total system performance is the fact that the firstbesthypothesis can already be analyzed by theMT modules whilethe N-best list is computed.All language models (word-pairs, bigrarns or smoothed bi-grams, and trigrams for resorting) are now trained on morethan 1000 CR-sentences, using word class specific equiva-lence classes (digits, names, towns, languages etc.)2.3.
ResortingThe resulting N-best list is resorted using trigrams to furtherimprove results.
Resorting improves the word accuracy forthe best scoring hypothesis (created using smoothed bigrams)from 91.5% to 98%, and the average rank of the correct hy-pothesis within the list from 5.7 to 1.1;Much longer N-best lists have been used for experiments (500-1000).
However it is very unlikely that a rescoring algorithmmoves a hypothesis from the very bottom of such a long listto the 1st position.
For practical application, a number of 100hypotheses was found to be best.3.
THE MACHINE TRANSLAT ION (MT)ENGINEThe MT-component that we have previously used has nowbeen replaced by a new module that can run several alternateprocessing strategies in parallel.
To translate spoken lan-guage from one language to another, the analysis of spokensentences, that suffer from ill-formed input and recognitionerrors is most certainly the hardest part.
Based on the listof N-best hypotheses delivered by the recognition engine,we can now attempt to select and analyze the most plausiblesentence hypothesis n view of producing and accurate andmeaningful translation.
Two goals are central in this attempt:high fMelity and accurate translation wherever possible, androbustness or graceful degradation, should attempts for highfidelity translation fail in face of ill-formed or misrecognizedinput.
At present, three parallel modules attempt to addressthese goals: 1) an LR-parser based syntactic approach, 2)a semantic pattern based approach and 3) a connectionistapproach.
The most useful analysis from these modules ismapped onto a common Interlingua,  language independent,but domain-specific representation f meaning.
The analysisstage attempts to derive a high precision analysis first, usinga strict syntax and domain specific semantics.
Connection-ist and/or semantic parsers are currently applied as back-up,if the higher precision analysis falls.
The Interlingua ensuresthat alternate modules can be applied in a modular fashion andthat different output languages can be added without redesignof the analysis tage.3.1.
Generalized LR Parser'the first step of the translation process is syntactic parsingwith the Generalized LR Parser/Compiler \[16\].
The General-ized LR parsing algorithm is an extension of LR parsing withthe special device called "Graph-Structured Stack" \[14\], and itcan handle arbitrary context-free grammars while most of theLR efficiency is preserved.
A grammar with about 455 rulesfor general colloquial English is written in a Pseudo Unifica-tion formalism \[15\], that is similar to Unification Grammarand LFG formalisms.
Figure2 shows the result of syntacticparsing of the sentence "Hello is this the conference office".Robust GLR Parsing: Modifications have been made tomake the Generalized LR Parser more robust against ill-formed input sentences \[18\].
In case the standard parsingprocedure fails to parse an input sentence, the parser nonde-terministically skips some word(s) in the sentence, and returnsthe parse with fewest skipped words.
In this mode, the parserwill return some parse(s) with any input sentence, unless nopart of the sentence can be recognized at all.212(HELLO IS S~IS THE COmrER~CE OFFICE $);++++ GLR Parser ru~ninR to produce ~g l i sh  structure +++?
(I) amblgu l t ies  fou~ ~d took 1.164878 seconds of r~ l  time(((PREV-SEMT~CES ((COUNTER 1) (MOOD *OPEN\]R}(RCOT *HELLO)))(?Ot~lm 2)(NCOD * INTEI~f3G~TIVE )(SUBJECt ((AGR *3-SING) (ROOT *THIS)(CASE ('OR* "NON *O~)) ) )( ~I~.14 *FINITE)(PREDICATE( ( lET ( (ROOT *'mE) (I~EF *D~)  ) ) (AGR *3-SI~IG)(AND4 * - )(A-AN *A)(ROOT *C(WER~:E-OFF ICE)  ) )(AGR *3-SING)(St~CAT *SU~J-PRED)(ROOT *COFQt,A )(TE~C.SE *~, .
.~r)  ) )Figure 2: Example F-StructureIn the example in figure 3, the input sentence "Hello is thisis this the office for the AI conference which will be heldsoon" is parsed as "Hello is this the office for the conference"by skipping 8 words.
Because the analysis gramrnar or theinterligua does not handle the relative clause "which will beheld soon", 8 is the fewest possible words to skip to obtaina grammatical sentence which can be represented in the in-terligua.
In the Generalized LR parsing, an extra procedure isapplied every time a word is shifted onto the Graph StructuredStack.
A heuristic similar to beam search makes the algorithmcomputationally tractable.When the standard GLR parser fails on all of the 20 bestsentence candidates, this robust GLR parser is applied to thebest sentence candidate.3.2.
The In ter l inguaThis result, called "syntactic f-structure", is then fed into amapper to produce an Interlingua representation.
For themapper, we use a software tool called Transformation Kit\[17\].
A mapping rammar with about 300 rules is written forthe Conference Registration domain of English.Figure 4 is an example of Interlingua representation producedfrom the sentence "Hello is this the conference office".
In theexample, "Hello" is represented asspeech-act *ACKNOWL-EDGEMENT, and the rest as speech-act *IDENTFY-OTHER.Input s~nt~ce  ,(hello is this is thls the AI confeDe~ce office which wlll be held soon $1)Parse of input ~t lnce  )(HELLO IS THIS Tree tCONFER~4CE OFFICE $)Words sh ipped  ~ ((IS 2) (TH~S 3) (AI 7) (WHICH I0) (WILL II)(BE 12) (HELD 13) (SCON 14) )Figure 3: Example for robust parsing( (PP.L~'-UTTI~.ANCES ( (SPE~:~-I-ACT *AC IG~OWILE I~T)(TI)O~ * PIR.&S~CT )l PAR'I~t((DETINITE *) (I~J1~E\];t *SG)IAND(-){TirPE *C~'Ei%I~CCE ){CX~NCI~PT *o~ICE)  ) )(SPEECR-ACT *ID~TIFY-OII4ER) )(VALUE *HELbO) I )Figure 4: Example: Interlingua OutputThe JANUS interlingua is tailored to dialog translation.
Eachutterance isrepresented asone or more speech acts.
A speechact can be thought of as what effect the speaker is intendinga particular utterance to have on the listener.
Our interlinguacurrently has eleven speech acts such as request direction, in-form, and command.
For purposes of this task, each sentenceutterance corresponds to exactly one speech act.
So the firsttask in the mapping process is to match each sentence with itscorresponding speech act.
In the current system, this is doneon a sentence by sentence basis.
Rules in the mapping ram-mar look for cues in the syntactic f-structure such as mood,combinations of auxilliary verbs, and person of the subjectand object where it applies.
In the future we plan to use moreinformation from context in determining which speech act toassign to each sentence.Once the speech act is determined, the rule for a particularspeech act is fired.
Each speech act has a top level semanticslot where the semantic representation fora particular instanceof the speech act is stored during translation.
This semanticstructure is represented as a hierarchical concept list whichresembles the argument s ructure of the sentence.
Each speechact rule contains information about where in the syntacticstructure to look for constituents ofill thematic roles such asagent, recipient, and patient in the semantic structure.
Specificlexical rules map nouns and verbs onto concepts.
In additionto the top level semantic slot, there are slots where informationabout one and mood are stored.
Each speech act rule containsinformation about what to look for in the syntactic structure inorder to know how to fill this slot.
For instance the auxiliaryverb which is used in a command etermines how imperativethe command is.
For example, 'You must register for theconference within a week' is much more imperative than 'Youshould register for the conference within a week'.
The secondexample leaves ome room for negotiation where the first doesnot.3.3.
The GeneratorThe generation of target language from an Interlingua repre-sentation involves two steps.
Figure 5 shows sample tracesof C~'man and Japanese, from the Interlingua in figure 4.First, with the same Transformation Kit used in the analysisphase, Interlingua representation is mapped into syntactic f-213structure of the target language.
;*+ TranJiKLt rules being applied to pr~k/~ G stl~/ctux~ +e\[ (PR~V-S~T~I~ \[ (VALUE ItALI.,OI (ROOT t, IT,.RAt,) ) )(RCOI' SEIN) (CAT V) (PERSON 31(SUEJ~';'((CAT N) (CA.g N) (DIST +) (;X3C +) (PEPS I  3)(Nt~ER SGI (ROOT D-PRONOt~) ) )(NtR, mER SG) (FORM FIN) (MOD I\]'.~) (T~SE PRF~)(I,10OD D, rI'ElaROG )(PPJm( (DET ( (CCAS t,l) (G ' I~ I~ nEU)(/,,',.n, IB~.~ SG)(CAT DET)(ROOT DE~) ) )(CLASS ~} (~4BE~ SG) (PERSON 3) (CAT H)(CO~OU~((CAT N) (PL -C I~ PL3I(u -cLAss  SG0)(ROOT KC~ER.I~Z)  )(RO(71' S I~TARL%T) (PL-CI.~,~ PL:~) (~-CLAg.g SG3)(G~,~UER IS'U) (CAS N) (ANIJ4 - ) ) ) );+?
GenKll: ru les  be ing  epp l ied  tO prodtlce Genpan text  ++"HALLO , iLST DORT DAS KO~ERE~ZSEKRETARIAT ??
;*+ TransRit rules being applied to produce J structure ++( ( P~V-Oq'~q~ANC~g( (FOR-R~4OVE-DESU *ZD~TIFY-OTHER) (VALUE MO~HXMOSHI)(ROOT *LITERAL) ) )(vrYPE M~ISRI )(SUFF (*MULTIPLE' ~A DEStI))(PRED ((ROOT GAKXAIJIMUKYOIqJ) (CAT NI( DEF L~ITE +)(Rcxn" CO~LA) );++ Ge~lKit rules be ing  app l ied  tO produce Japanelle text  ++"M~HD~C~4I GAX\]~I JIMUKYOKU DESt~"Figure 5: Output language F-structureThere are about 300 rules in the generation mapping rammarfor German, and 230 rules for Japanese.
The f-structure is thenfed into sentence generation software called "GENK1T" \[17\]to produce a sentence in the target language.
A grammar forGENK1T is written in the same formalism as the GeneralizedLR Parser: phrase structure rules augmented with pseudounification equations.
The GENKIT grammar for generalcolloquial German has about 90 rules, and Japanese about 60rules.
Software called MORPHEis also used for motphlogicalgeneration for German.3.4.
Semantic Pattern Based Pars ingA human-human translation task is even harder than human-machine communication, in that the dialog structure inhuman-human communication is more complicated and therange of topics is usually less restricted.
These factors pointto the requirement for robust strategies in speech translationsystems.Our robust semantic parser combines frame based semanticswith semantic phrase grammars.
We use a frame based parsersimilar to the DYPAR parser used by Carbonell, et al to pro-cess ill-formed text,\[9\] and the MINDS system previously de-veloped at CMU.\[10\] Semantic information is represented ina set of frames.
Each frame contains aset of slots representingpieces of information.
In order to fill the slots in the frames,we use semantic fragment grammars.
Each slot type is rep-resented by a separate Recursive Transition Network, whichspecifies all ways of saying the meaning represented by theslot.
The grammar is a semantic grammar, non-terminals aresemantic concepts instead of parts of speech.
The grammar isalso written so that information carrying fragments ( emanticfragments) can stand alone (be recognized by a net) as well asbeing embedded in a sentence.
Fragments which do not forma grammatical English sentence are still parsed by the system.Here there is not one large network representing all sentencelevel patterns, but many small nets representing informationcarrying chunks.
Networks can "call" other networks, therebysignificantly reducing the overall size of the system.
Thesenetworks are used to perform pattern matches against inputword strings.
This general approach as been described inearlier papers.
\[7, 8\]The operation of the parser can be viewed as "phrase spot-ting".
A beam of possible interpretations are pursued simul-taneously.
An interpretation is a frame with some of its slotsfilled.
The RTNs perform pattern matches against he inputstring.
When a phrase is recognized, it attempts to extendall current interpretations.
That is, it is assigned to slots inactive interpretations that it can fill.
Phrases assigned to slotsin the same interpretation are not allowed to overlap.
In caseof overlap, multiple interpretations are produced.
When twointerpretations for the same frame end with the same phrase,the lower scoring one is pruned.
This amounts to dynamicprogramming on series of phrases.
The score for an interpre-tation is the number of input words that it accounts for.
At theend of the utterance, the best scoring interpretation is picked.Our strategy is to apply grammatical constraints atthe phraselevel and to associate phrases in frames.
Phrases representword strings that can fill slots in frames.
The slots representinformation which, taken together, the frame is able to act on.We also use semantic rather than lexical grammars.
Seman-tics provide more constraint than parts of speech and mustultimately be delt with in order to take actions.
We believethat this approach offers a good compromise of constraintand robustness for the phenomena of spontaneous speech.Restarts and repeats are most often between phases, so in-dividual phrases can still be recognized correctly.
PoorlyconsVucted grammar often consists of well-formed phrases,and is often semantically well-formed.
It is only syntacticallyincorrect.The parsing grammar was designed so that each frame hasexactly one corresponding speech act.
Each top level slotcorresponds to some thematic role or other major semanticconcept such as action.
Subnets correspond to more specificsemantic classes of constituents.
In this way, theinterpretationreturned by the parser can be easily mapped onto the inter-lingua and missing information can be filled by meaningfuldefault values with minimal effort.214Once an utterance isparsed in this way, it must hen be mappedonto the interlingua discussed earlier in this paper.
The map-ping grammar contains rules for each slot and subnet in theparsing ramar which correspond to either concepts or speechacts in the interlingua.
These rules specify the relationshipbetween a subnet and the subnets it calls which will be repre-sented in the interlingua structure it will produce.
Each rulepotentially contains four parts.
It need not contain all of them.The first part contains a default interlingua structure for theconcept represented by a particular nile.
If all else fails, thisdefault representation will be returned.
The next part con-talns a skeletal interlingua representation for that rule.
Thisis used in cases where a net calls multiple subnets which fillparticular slots within the structure corresponding tothe rule.A third part is used if the slot is filled by a terminal string ofwords.
This part of the rule contains a context which can beplaced around that string of words so that it can be attemptedto be parsed and mapped by the LR system.
It also containsin formaiton about where in the structure returned from the LRsystem to find the constituent corresponding tothis rule.
Thefinal part contains rules for where in the skeletal structure toplace interlingua structures returned from the subnets calledby this net.3.5.
Conneet ion is t  Pars ingThe connectionist parsing system PARSEC \[12\] is used as afall-back module if the symbolic high precision one fails to an-alyze the input.
The important aspect of the PARSEC systemis that it learns to parse sentences from a corpus of trainingexamples.
A connectionist approach to parse spontaneousspeech offers the following advantages:1.
Because PARSEC learns and generalizes from the exam-pies given in the training set no explicit grammar ruleshave to be specified by hand.
In particular, this is of im-portance when the system has to cope with spontaneousutterances which frequently are "corrupted" with disflu-encies, restarts, repairs or ungrammatical constructions.To specify symbolic grammars capturing these phenom-ena has been proven to be very difficult.
On the other sidethere is a "build-in" robustness against these phenomenain a connectionist ystem.2.
The connectionist parsing process is able to combinesymbolic information (e.g.
syntactic features of words)with non-symbolic information (e.g.
statistical likeli-hood of sentence types).
Moreover, the system can eas-ily integrate different knowledge sources.
For example,instead of just training on the symbolic input string wetrained PARSEC on both the symbolic input string andthe pitch contour.
After training was completed the sys-tem was able to use the additional information to deter-mine the sentence mood in cases where syntactic lueswere not sufficient.
We think of extending the idea ofintegrating prosodic information into the parsing pro-cess in order to increase the performance of the systemwhen it is confronted with corrupted input.
We hope thatprosodic information will help to indicate restarts andrepairs.The current PARSEC system comprises six hierarchically or-dered (back-propagation) connectionist modules.
Each mod-ule is responsible for a specific task.
For example, there aretwo modules which determine phrase and clause boundaries.Other modules are responsible for assigning to phrases orclauses labels which indicate their function and/or relation-ship to other constituents.
The top module determines themood of the sentence.Recent Extensions: We applied a slightly modified PAR-SEC system to the domain of air travel information (ATIS).We could show that the system was able to analyze utterancelike "show me flights from boston to denver on us air" andthat the system's output representation could be mapped to aSemantic Query Language (SQL).
In order to do this we in-cluded semantic information (represented asbinary features)in the lexicon.
By doing the same for the CR-task we hope toincrease the overall parsing performance.We have also changed PARSEC to handle syntactic structuresof arbitrary depth (both left and right branching) \[13\].the main idea of the modified PARSEC system is to make itauto recursive, i.e.
in a recursion step n it will take its outputof the previous tep n-1 as its input.
This offers the followingadvantages:i.
Increased Expressive Power: The enhanced expressivepower allows a much more natural mapping of linguisticintuitions to the specification of the training set.2.
Ease of learning: Learning difficulties can be reduced.Because PARSEC is now allowed to make more abstrac-tion steps each individual step can be smaller and, hence,is easier to learn.3.
Compatibility: Because PARSEC is now capable ofproducing arbitrary tree structures as its output it can bemore easily used as a submodule in NLP-systems (e.g.the JANUS system).
For example, it is conceivable toproduce as the parsing output f-structures which then canbe mapped irectly to the generation component \[11\].4.
SYSTEM INTEGRATIONThe system accepts continuous speech speaker-independentlyin either input language, and produces ynthetic speech outputin near real-time.
Our system can be linked to different lan-guage versions of the system or corresponding partner systems215via ethernet or via telephone modem lines.
This possibilityhas recently been tested between sites in the US, Japan andGermany to illustrate the possibility of international telephonespeech translation.The minimal equipment for this system is a Gradient Deskiab14 A/D-converter, an HP 9000/730 (64 Meg RAM) worksta-tion for each input laguage, and a DECtalk speech synthesizer.Included in the processing are A/D conversion, signal pro-cessing, continuous speech recognition, language analysis andparsing (both syntactic and semantic) into a language inde-pendent interlingua, text generation from that interlingua, andspeech synthesis.The amount of time needed for the processing of an utterance,depends on its length and acoustic quality, but also on theperplexity of the language model, on whether or not the firsthypothesis is parsable and on the grammatical complexityand ambiguity of the sentence.
While it can take the parserseveral seconds to process a long list of hypotheses for acomplex utterance with many relative clauses (extremely rarein spoken language), the time consumed for parsing is usuallynegligible (0.1 second).For our current system, we have eliminated considerableamounts ofcornmunication delays by introducing socket com-munication between pipelined parts of the system.
Thus thesearch can start before the preprocessing program is done,and the parser starts working on the first hypothesis while theN-best list is computed.5.
CONCLUSIONIn this paper, we have discussed recent extensions to theJANUS system a speaker independent multi-lingual speech-to-speech translation system under development a CarnegieMellon and Karlsruhe University.
The components includean speech recognition using an N-best sentence search, toderive alternate hypotheses for later processing during thetranslation.
The MT component attempts to produce a high-accuracy translation using precise syntactic and semantic anal-ysis.
Should this analysis fail due to ill-formed input or mis-recognitions, a connectionist parser, PARSEC, and a seman-tic parser produce alternative minimalist analyses, to at leastestablish the basic meaning of an input utterance.
Human-to-human dialogs appear to generate a larger and more variedbreadth of expression than human-machine dialogs.
Furtherresearch is in progress to quantify this observation and toincrease robustness and coverage of the system in this envi-ronment.References1.
A. Waibel, A. Jain, A. McNair, H. Saito, A. Hauptmann, and J.Tebelskis, JANUS: A Speech-to-Speech Translation System Us-ing Connectionist and Symbolic Processing Strategies, volume2, pp 793-796.
ICASSP 1991.2.
L. Osterholtz, A. McNair, I. Rogina, H. Saito, T. Sloboda, J.Tebelskis, A. Waibel, and M. Woszczyna.
Testing Generality inJANUS: A Multi-Lingual Speech to Speech Translation System,volume 1, pp 209-212.ICASSP 1992.3.
Austin S., Schwartz R. A Comparison of Several ApproximateAlgorithms for Finding N-best Hypotheses, ICASSP 1991, vol-ume 1, pp 701-704.4.
Schwartz R., Austin S. The Forward-Backward Search Algo-rithm, ICASSP 1990, volume I, pp 81-84.5.
O. Schmidbauer and J. Tebelskis.
An LVQ based ReferenceModel for Speaker-Adaptive Speech Recognition.
ICASSP1992, volume 1, pages 441-444.6.
J. Tebelskis and A. Waibel.
Performance through consistency:MS-TDNNs for large vocabulary continuous peech recog-nition, Advances in Neural Information Processing Systems,Morgan Kaufmann.7.
W.Ward, Understanding Spontaneous Speech, DARPA Speechand Natural Language Workshop 1989, pp 137-141.8.
W. Ward, The CMU Air Travel Information Service: Under-standing Spontaneous Speech, DARPA Speech and NaturalLanguage Workshop 1990.9.
J.G.
Carbonell and PJ.
Hayes, Recovery Strategies for Pars-ing Extragrammatical L nguage, Carnegie-Mellon UniversityComputer Science Technical Report 1984, (CMU-CS-84-107)10.
S.R.
Young, A.G. Hauptmann, W.H.
Ward, E.T.
Smith, andP.
Werner, High Level Knowledge Sources in Usable SpeechRecognition Systems, in Communications of the ACM 1989,Volume 32, Number 2, pp 183-19411.
F. D.
Bu?, A learnable connectionist parser that outputs f-structures (working title), PhD-Thesis proposal, University ofKarlsruhe, in preparation.12.
AJ.
Jain, A. Waibel, D. Touretzky, PARSEC: A StructuredConnectionist Parsing System for Spoken Language, ICASSP1992, volume 1, pp 205-208.13.
T.S.
Polzin, Pronoun Resolution.
Interaction of Syntactic andSemantic Information in Connectionist Parsing, Master The-sis, Carnegie Mellon University, Department of Philosophy,Computational Linguistics, in preparation.14.
Tomita, M.
(ed.
), GeneralizedLR Parsing, Kluwer AcademicPublishers, Boston MA, 1991.15.
Tomita, M., The GeneralizedLR Parser/Compilerin 13th In-ternational Conference on Computational Linguistics (COL-ING90), Helsinki, 199016.
Tomita, M., Mitamura, T., Musha, H. and Kee, M.; The Gener-alized LR Parser/Compiler VersionS.l : User~ Guide, Techni-cal Memo, Center for Machine Translation, Carnegie MellonUniversity, CMU-CMT-88-MEMO, 1988.17.
Tomita, M. and Nyberg, E.; The Generation Kit and The Trans-formation Kit: User's Guide Technical Memo, Center for Ma-chine Translation, Carnegie Mellon University, CMU-CMT-88-MEMO, 198818.
Lavie, A and Tomita, M.; An Efficient Word-Skipping ParsingAlgorithm for Context-Free Grammars ubmitted to 3rd In-ternational Workshop on Parsing Technologies (IWPT93) Bel-guim, 1993.216
