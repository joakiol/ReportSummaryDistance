Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 937?944Manchester, August 2008Source Language Markers in EUROPARL TranslationsHans van HalterenRadboud University NijmegenDepartment of Linguistics / CLSTP.O.
Box 9103, NL-6500HD NijmegenThe Netherlandshvh@let.ru.nlAbstractThis paper shows that it is very oftenpossible to identify the source languageof medium-length speeches in the EU-ROPARL corpus on the basis of fre-quency counts of word n-grams (87.2%-96.7% accuracy depending on classifica-tion method).
The paper also examines indetail which positive markers are mostpowerful and identifies a number of lin-guistic aspects as well as culture- anddomain-related ones.11 IntroductionThe EUROPARL Corpus (Koehn, 2005) is oneof the most important resources for translationresearch.
It is used extensively, mostly but cer-tainly not exclusively in statistical machine trans-lation.
In much of that research, the relation be-tween a source language SL and target languageTL is investigated on the basis of aligned sen-tences of SL and TL without considering whetherSL is indeed the actual source language.
In thispaper we question the lack of attention for thetrue SL, because we expect significant differ-ences between texts written originally in TL,texts translated from SL to TL and texts trans-lated from another language into TL, even if allthree types have been produced by native speak-ers of TL.2 At least with respect to the differ-ences between original and translated texts, our?
2008.
Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license(http://creativecommons.org/licenses/by-nc-sa/3.0/).
Somerights reserved.2In principle, the EU translation service always lets transla-tors translate into their mother tongue.
Given that we willhere be working only with major EU languages, it isunlikely that the principle was violated for our selectedtexts.expectations are supported by the literature (foran overview, see e.g.
Baroni and Bernardini,2006).We embarked on a data-driven investigation,using several text classification techniques todetermine if the differences are salient enough todistinguish between different source languagesfor various texts and target languages.
In thispaper, we first describe the data and methods weused (Sections 2 and 3).
Then we present theclassification results (Section 4), after which wegive a description of some types of markers wewere able to identify (Section 5).
Finally, in Sec-tion 6, we present our conclusions and plans forfuture research.2 Experimental DataThe experimental data was taken from the EU-ROPARL Corpus, but had to be preprocessed tosome degree to be suitable for our experiments.First of all, the annotation of the corpus includesa LANGUAGE attribute, indicating the originallanguage of the text.
However, it is often absentin one or more versions of the text.
If we exam-ine the speeches of at least 100 words in thewhole corpus, we find that 11% of them lack theattribute in all available versions and for 5% theattribute has different values in different ver-sions.
We linked the attribute across the variousversions and excluded all speeches showing in-consistent values.We decided to focus on the six most commonlanguages in the corpus: English (EN), German(DE), French (FR), Dutch (NL), Spanish (ES)and Italian (IT).
For each of these languages as asource language, we aimed for 1000 speeches(henceforth: texts) which were present as originaland as translations into each of the other fivelanguages.
Furthermore, we decided to focus onthe medium range as for length, avoiding veryshort and very long texts, as these were likely to937be different in nature.3  Our aim of 1000 textsfrom each source language led us to settle ontexts between 380 and 2500 words.Finally, as we wanted the classification sys-tems to focus on language use rather than con-tents, we transformed the tokens in the texts,separately for each target language.
Only tokensthat occurred in at least 10% of the texts re-mained intact.
Other tokens were mapped to amarker <X>.3 MethodsWe classified all texts in our experimental setwith several text classification methods (see Sec-tions 3.1 to 3.3), using 10-fold cross-validation,each time with 80% of the texts used to trainmodels for each source language, 10% of thetexts used to tune parameters and to base thresh-olds on and 10% of the texts to test classificationaccuracy.
For all methods, the same train/tune/test splits were used and some dozens ofpromising parameter settings were tried, afterwhich the best one for each specific run was se-lected on the basis of tune set results.Each method was provided with the same textfeatures, viz.
n-gram counts.
From each selectedtext (in its mapped version), we extracted countsfor all uni-, bi- and trigrams of tokens occurringin at least 10% of the texts (i.e.
those not trans-formed to <X>) and allowing <X> markers to in-tervene between any two tokens in the n-gram.For example, the sequence "I join with RoyPerry in" would give rise to the n-gram<3>_I_<X>_with_<X>_<X>_in, with the <3> in-dicating that there are three real tokens in the n-gram, and possibly any number of intervening<X> markers.3.1 Marker-Based ClassificationOur first classification method attempts to clas-sify texts on the basis of individual markerswhich are by themselves a strong indication thatthe text originated in a certain source language.All n-grams which occurred more often in thetraining data with a specific source language SLthan with all other source languages taken to-gether were deemed to be markers for SL.When classifying test data, each marker for anSL observed in the test text leads to an increaseof the score for that SL for that text.
The exact3E.g., short texts tend to be interruptions and statementssuch as the opening of the session, where long texts includepresentations of written reports.calculation of the increase depends on some pa-rameter settings.
It always involves both themarker's precision (how often its presence indeedcoincides with the specific SL) and its recall(how many of the specific SL texts contain themarker), but with a weighting favouring preci-sion over recall by a factor of 10 to 100.
Preci-sion and recall can be based on either raw orsmoothed counts.
The calculation may also in-volve the frequency of the marker in the test text.In this way, the frequency of a marker is nottaken into account for determining whether it is amarker and what value it is given, but may betaken into account when classifying test texts.3.2 Linguistic ProfilingThe second classification system was LinguisticProfiling, which was previously shown to be use-ful for language verification (van Halteren andOostdijk, 2004), a task which is similar to thecurrent task.
Roughly speaking, it classifies onthe basis of noticeable over- and underuse ofspecific n-grams.
As the marker-based classifica-tion only used overuse and the necessary degreeof overuse is lower for Linguistic Profiling, thelatter pays attention to many more features andshould be able to attain a better classificationrate.
However, if we want to determine whichfeatures are most powerful, interpreting theworkings of Linguistic Profiling will be moredifficult than for the marker-based approach.All n-gram counts were normalized to countsper 1000 words.
Furthermore, in order to reducethe number of counts, so that the system couldcope with the resulting vectors, we included onlyn-grams which occurred in at least 10 texts.
Thisled to vectors with about 90,000 counts for eachtarget language.3.3 Support Vector MethodsFinally, we employed Support Vector Machines,viz.
LIBSVM (Chang and Lin, 2001).
We of-fered the same vectors we used for LinguisticProfiling to both standard Support Vector Classi-fication (SVC; RBF kernel, various settings for Cand ?)
and Support Vector Regression (?-SVR;RBF kernel, various settings for C, ?
and ?
).The Support Vector methods use all availableinformation rather than focusing on over- andunderuse.
They should therefore attain the bestclassification results.
However, extracting infor-mation about salient features from the results willbe virtually impossible.938MB LP SVC SVRSL vs TL: TL 84.1 91.5 92.6 95.2SL vs TL: SL 75.3 88.4 87.6 91.6Combination 87.2 94.7 90.1 96.9SL vs SL: SL 74.2 85.8 85.4 89.7Combination 81.1 91.8 85.4 94.3Table 1: Average accuracies for two-way SL vsSL choices for various classification methods.The top part represents cases where one of theSL is the TL, the bottom where neither SL is TL.3.4 Score ComparabilityFor all methods except SVC, the ranges of testtext scores vary greatly as train sets and parame-ter settings are changed.
As we wanted to com-bine scores from classifications based on modelsfor various SL, however, we needed the scores tobe comparable.
In order to make them so, wecompared the score for each test text with thescores for all tune texts.4 All lower scoring posi-tive examples provided a increase of the final(comparable) score and all higher scoring nega-tive examples a decrease.
For fine tuning, therelative position of the test text's score betweenthe next higher and next lower scoring tune textswas also taken into consideration.54 Classification ResultsWe first used our classification techniques tochoose between every possible pair of sourcelanguages for each text (Section 4.1).
Then wecombined the various two-way decisions into asingle six-way decision (Section 4.2).4.1 Two-Way DecisionsFor each choice between two possible sourcelanguages, there are two classification models(one for each of the SL) that can make the choiceindividually, but we can also combine the twoopinions by choosing the SL whose classificationmodel claims the text with a higher score.6The quality of these decisions, for those caseswhere  the  actual  SL  is  present  in  the  pair,  is4We deliberately chose a non-parametric technique here.However, preliminary additional experiments show that aparametric alternative may actually provide slightly betterresults.5This technique did not work for SVC, as SVC only scoresa text with 1 or -1.
However, for SVC the technique wasalso not needed as the scores were already comparable.
Wedid add a very small random number to each score to re-solve ties where necessary.6Remember that we made all classification scores compara-ble.MB LP SVC SVRES   64.7 82.9 81.1 87.4DE   62.0 81.6 80.8 87.5EN   60.4 80.6 79.3 86.8FR   58.7 77.0 77.7 85.7NL   58.4 76.7 75.2 83.1IT   52.8 73.6 60.7 81.5ALL TL   87.2 90.6 91.5 96.7Table 2: Average accuracies for six-way SLchoices for various classification methods.
Therows with a TL indication represent the resultswhen the text is only available in that TL.
Thebottom row shows the combined result, using allsix versions of the text.shown in Table 1.
We distinguish two types ofchoices.
The top of the table shows the results forthe cases in which one of the two SL is the targetlanguage, with SL equal to TL indicating that thetext is an original TL text rather than a transla-tion.
For the cases represented in the table, i.e.cases where one of the two SL is the actual one,the task measured here is in fact translation rec-ognition.
As the table shows, the accuracy forthis task is higher when modeling original TLtext than when modeling translations, meaningthat it is easier to spot (violations against) regu-larities in the target language than it is to spotregularities in the translations from a specificsource language.
Combining the two modelsyields even better classification, except for SVC,where combination cannot be used properly be-cause SVC only produces scores of 1 and -1.
Thebottom of the table shows the results for thecases in which both SL are different from thetarget language.
Here the choices appear to bemore difficult on average, but the classificationquality is still impressive.4.2 Six-Way DecisionsOnce we had classification scores for the choicebetween all fifteen possible pairs of sourcelanguages (actually the combination scores), wecould use these to choose a single SL fromamong the set of six possible SL.
For the currentpaper, we did this by simply adding allclassification scores in favour of each specific SLand then choosing the SL with the highest total.The addition can either be done over all two-waychoices referring to a specific target language, orover all choices for all six possible targetlanguages.The quality of these decisions is shown in Ta-ble  2.
For  the  individual  target languages,  the939Table 3: Confusion table for SVR classificationof each of the 6000 selected speeches on thebasis of all six language versions of each speech.Rows are actual languages; columns arelanguages assigned by the classifier.accuracies are not all that high, but this was to beexpected.
Interestingly,  the relative difficulty ofthe choice for the various TL is largely inde-pendent of the classification method used, withtexts in Spanish being most easily classifiableand texts in Italian least easily.
The relative per-formance of the classification methods is againas expected, except that SVC is slightly behindLinguistic Profiling for the individual languages.For the best performing classification method,Support Vector Regression, we also show theconfusion table for SL assignment (Table 3).
Ingeneral, there are more confusions within theGermanic and Romance language families thanbetween them.
However, English seems to takean intermediate position between the two fami-lies.
On the other hand, the English model ap-pears to be somewhat greedier than the others,which confuses the analysis.
Also, given that theLANGUAGE attribute does not seem to be as-signed perfectly, it would be advisable to exam-ine the misassigned texts, so as to check if theyare indeed misassigned or merely mislabeled.5 A Look at Source Language Mark-ersNow we have shown that word n-grams providea solid basis for source language identification,we proceed to an examination of the n-grams thatare the most useful in this identification.
Unfor-tunately, as stated above, Linguistic Profiling andthe Support Vector methods are not very amena-ble to extracting information about the most sali-ent features.
Therefore, for now, we will have tofall   back  on   the   marker-based   classificationwhere it is trivial to identify the source languagemarkers which characterize their source lan-guages most strongly.
Admittedly, the marker-based  classification had  the lowest performancein the classification task, but still it is goodenough for an examination to make sense.Table 4: Numbers of source language markersfor various source languages SL in translation toEnglish.To optimize understandability for all readers,we focus on English as the target language in thisexamination.
We distinguish between  two kindsof marker strength.
Obviously, there is thestrength in the SL classification described above.For a marker to be strong in this sense, it has tooccur more often with the source language inquestion than with all other source languagestaken together (Section 5.1).
If, however, we en-vision an application where we know that a spe-cific SL is being translated to a specific TL andwe want to give feedback to the translator thatthe translation contains strong influences fromthe SL, then strength can also be taken to be thedegree to which the marker occurs more in textstranslated from SL than in original TL texts (Sec-tion 5.2).
For both kinds of marker strength, wecan identify specific types of markers, relatedeither to linguistic or culture- and domain-relatedaspects of texts and their translations (Section 5.3)5.1 Statistics for SL vs All OthersWe took all n-grams from the English versions ofthe texts in our experimental data and countedthe number of texts they were contained in foreach of the six source languages.
We then calcu-lated their strength for each source language SLby dividing the observed number of SL texts,plus one, by the number of non-SL texts, againplus one.
The classification described in Section3.1 in principle used all markers with strengthgreater than one.If the full set of texts were used as trainingmaterial, we would find the numbers of markersas shown in Table 4.
The majority of n-gramsoccurs only once (Column 2) and necessarilywith a single specific SL so that they can betaken to be markers.
Note, however, that thesemarkers did not play a role in the classification inthe ten-fold cross-validation, since they werealways either only in the training set, only in thetune set or only in the test set.
The third columnrepresents n-grams which also occur only withEN DE FR NL ES ITEN 980 1 12 2 1 4DE 16 961 6 9 3 5FR 15 3 969 2 7 4NL 16 10 12 956 4 2ES 14 5 13 2 960 6IT 8 2 9 0 8 973Occurringonly once;With SLOccurringmore often;Only with SLOccurringalso withnon-SLDE 137256 6691 9459FR 122386 4916 7102NL 120071 5594 7653ES 119899 5740 8495IT 129251 5900 8538940Table 5: Strongest n-grams occurring only with aspecific SLone specific SL.
These markers are in principlethe most useful because of their precision.
How-ever, their number of occurrences turns out to begenerally very low,  with a maximum of 23 and amean of 2.1, so that their recall is low to verylow.
Finally, there are the n-grams which do oc-cur  with  various  source  languages,   but  mostoften with one specific SL.
They are representedby the last column.Probably more insightful than the general sta-tistics are the actual markers themselves, hererepresented by the strongest ones for each sourcelanguage,  still  with  English  as  the  target  lan-Table 6: Strongest  n-grams occurring also withother languagesguage.
The lists we find are always headed by n-grams which occur only with a specific SL (Ta-ble 5).
Then follow the n-grams  which can alsobe found with other languages.
We show thestrongest ones of this type separately as Table 6.As already stated above, most of the strongermarkers occur in only few SL texts.
The big ex-ception seems to be Dutch, which has severalmarkers (notably <3>_._<X>_to_say with com-peting languages and <3>_like_to_<X>_offexclusively) which show up quite often.
Only<2>_framework_conditions for German and thecertain_number cluster for French occur over20 times too, but they are not exclusive.
We alsosee various clusters, where longer strings are rep-resented by several n-grams, such as "a certainnumber of"  and  "with  no regard for"  forSL n-gram textsDE <3>_is_,_though 10<2>_Commission_here <3>_,_again_and<3>_are_right_<X>_you <3>_If_,_though<3>_me_say_this <3>_the_Commission_here<3>_the_<X>_Council_Presidency7<3>_._What_that <2>_What_that<3>_reason_we_must <3>_must_at_last<2>_more_able <3>_go_without_saying<3>_not_,_though <2>_need_<X>_-<2>_taken_here <3>_believe_that_here<3>_is_needed_here<3>_gentlemen_,_<X>_<X>_is6FR <3>_common_to_the <3>_quality_of_her 6<3>_with_no_regard <3>_no_regard_for<2>_no_regard <3>_conditions_of_<X>_of<2>_into_<X>_this <2>_on_<X>_services<2>_particular_about <2>_various_policies<3>_all_those_, <3>_of_cooperation_is<3>_the_United_<X>_<X>_Commissioner<3>_,_<X>_society_,5NL <3>_like_to_<X>_off 23<3>_to_<X>_off_with 16<3>_On_a_final 14<2>_Commissioner_whether<3>_the_Commissioner_whether10<3>_and_such_like <2>_such_like 8<3>_past_<X>_of_years <3>_,_it_<X>_as<3>_are_in_order <3>_too_<X>_for_words<3>_to_<X>_off_by7ES <3>_going_to_support 11<3>_amendments_presented_by <3>_end_here_,<3>_Community_system_for<3>_the_people_responsible8<3>_citizens_._And <3>_going_to_debate 7<3>_the_Community_<X>_sector<2>_Community_<X>_sector<2>_than_<X>_<X>_<X>_million<3>_million_<X>_year_.<3>_President_,_without <3>_Let_us_see<3>_adopt_measures_to<3>_move_<X>_with_the<3>_And_the_Commission<3>_people_responsible_for6IT <3>_least_in_that 9<2>_task_before <3>_or_,_<X>_still 7<3>_change_the_current <3>_is_the_Europe<3>_feel_that_Parliament <3>_task_before_us<3>_of_my_Commission <2>_with_<X>_<X>_'<3>_the_<X>_available_.
<2>_:_<X>_which<3>_security_and_peace6SL n-gram SLtextsother sourcelanguagetextsDE <3>_means_is_that 11 1 IT<2>_framework_conditions 22 2 EN, 1 FR<3>_in_future_be 14 1 EN, 1 IT<3>_,_that_being 13 2 NL<2>_action_here 8 1 FR<3>_So_let_me 8 1 FR<3>_to_at_last 8 1 FRFR <3>_why_I_shall 8 1 NL<2>_certain_number 25 1 DE, 2 IT,2 NL<3>_certain_number_of 25 1 DE, 2 IT,2 NL<3>_a_certain_number 24 1 DE, 2 IT,2 NL<3>_thank_our_rapporteur 7 1 NL<3>_We_now_know 6 1 NL<3>_provide_itself_with 6 1 ESNL <3>_._<X>_to_say 61 4 EN, 2 DE,1 FR<3>_that_is_concerned 10 1 DE<3>_we_as_Parliament 9 1 IT<3>_group_,_it 9 1 ES<3>_is_every_reason 9 1 ES<3>_deal_of_support 8 1 DE<3>_think_that_that 12 1 ES, 1 FRES <3>_._<X>_this_context 9 1 IT<3>_I_<X>_this_to 9 1 EN<2>_people_responsible 8 1 NL<3>_going_to_deal 8 1 NL<3>_President_,_<X>_allow 8 1 IT<3>_legislation_in_force 8 1 FR<3>_report_,_since 8 1 FRIT <2>_the_now 10 1 FR<3>_other_,_there 10 1 DE<3>_the_individual_States 10 1 DE<3>_,_<X>_<X>_,_ladies 10 1 DE<2>_my_Commission 8 1 FR<3>_due_regard_for 12 1 EN, 1 NL<2>_quite_aware 16 3 FR941Table 7: Strongest individual tokens markingtranslations into EnglishFrench and "the people responsible for"for Spanish.5.2 Markers for Translation vs OriginalIf we want to call a translator's attention to atranslation which deviates from the general lan-guage use in the target language, the markeridentifying the deviation need not be exclusive toa single source language.
Such markers are muchmore common than source language specificmarkers.
They also include large numbers of sin-gle tokens (i.e.
unigrams; Table 7), something wedid not find in Section 5.1.Most remarkable are the top two, ladies andgentlemen.
Apparently, speakers from all overEurope address the whole house when openingtheir speech, but those speaking English onlyaddress the President (i.e.
the chairperson).
Therest are mostly words providing discourse func-tions, either by themselves, such as thereforeand though, or in larger combinations, such ashand in "on the one hand" and opinion in "inmy opinion".
There are only a few words withactual content, such as guarantee, freedom andOffice, although the latter may also well be partof a parliament term.As for the longer n-grams, we will not presentthe full list of strongest markers, but instead afiltered selection (Table 8).
The reason for this isthat there is quite a lot of repetition in the fulllist.
The strongest eleven, and fifteen more of thestrongest fifty, are (parts of) combinations ofvocatives, such as "Commissioner, ladies andgentlemen", which we already addressed above.As  vocative   use  in  the   European  ParliamentTable 8: Selection from the strongest n-gramsmarking translations into Englishcould well be a separate study in itself, and isprobably not something we need to bother trans-lators with, we leave out all vocatives.5.3 Types of MarkersThe markers shown in the previous sections areof a rather varied nature.
Some of them have lin-guistic explanations, but there are also quite afew which are more culture- and domain-related.The best example in the latter category is thealready mentioned use of vocatives.
Althoughthere are clear links to at least one source lan-guage, English, this is not something that iscaused by translation.
Another example of seem-ingly typical parliamentary behaviour are thephrases "like to finish off" and "On a fi-nal note", responsible for the three strongestDutch markers in Table 5.
The Dutch (or Flem-ish) parliamentarians announce in some way thatthey are nearing the end of their speech.
How-ever, if we examine the original Dutch text, weobserve a much more varied phrasing.
We findthe literal counterpart ("ik wil afsluiten"),but also "ter afsluiting" ("to close") and"dan nog iets" ("then another thing").
Ap-parently, one or more of the Dutch to Englishtranslators have developed their own favouritephrases to cover this general situation.Another domain-specific type of marker canbe found in content words (here mostly com-pounds) referring to parliamentary matters.
Herewe turn to German for some examples.
In Tabletoken EN DE FR NL ES IT<1>_ladies 11 574 362 197 273 378<1>_gentlemen 14 584 383 205 304 388<1>_And 69 154 160 164 307 128<1>_above 58 151 119 115 145 198<1>_guarantee 53 97 110 87 164 151<1>_favour 75 157 197 173 164 122<1>_everyone 55 144 141 126 57 93<1>_namely 54 158 94 184 54 60<1>_opinion 140 164 244 272 250 311<1>_mention 65 109 120 86 127 121<1>_although 95 122 110 116 233 219<1>_therefore 296 396 488 477 580 525<1>_regard 174 212 292 254 408 286<1>_shall 108 178 240 144 166 169<1>_though 80 215 131 116 82 117<1>_everything 69 152 126 98 97 98<1>_various 107 168 188 154 186 175<1>_hand 103 160 183 178 145 171<1>_freedom 72 118 119 88 105 157<1>_Office 73 158 110 77 142 102token EN DE ES FR IT NL<3>_of_the_Group 0 27 41 19 16 34<2>_end_by 0 2 42 18 25 7<3>_,_by_means 0 13 40 21 11 8<3>_s_<X>_(_<X>_<X>_) 0 35 13 8 11 21<3>_)_and_European 0 29 14 8 11 21<3>_,_for_we 0 25 0 7 26 20<3>_at_last_, 0 9 3 5 53 5<3>_countries_,_which 0 13 16 24 10 9<2>_we_therefore 0 10 19 8 22 10<3>_the_Europe_of 0 5 12 18 30 3<3>_order_to_guarantee 0 9 26 14 6 11<3>_and_above_all 1 35 29 26 28 13<3>_out_that_, 0 11 22 12 12 8<2>_therefore_believe 1 13 67 21 19 8<3>_think_that_this 1 14 22 29 15 46<3>_think_that_, 0 9 15 14 10 11<3>_like_to_<X>_this 0 5 15 4 7 27<3>_of_third_countries 0 4 17 15 12 9<2>_here_too 0 27 1 4 8 15<3>_of_all_like 0 4 12 14 2 22<3>_,_though_, 3 128 4 17 14 53<3>_too_,_we 0 21 0 6 11 15<3>_I_shall_not 0 3 10 21 7 12<3>_State_or_Government 0 15 5 10 16 69425 we find <3>_the_<X>_Council_Presidency.One might think that only Germans are interestedin who runs the council at any given time, but infact this is an idiosyncratic alternative translationas "Council Presidency" is generally justcalled Presidency.
Another example is<2>_framework_conditions in Table 6, with 22German SL occurrences, 2 English and 1 French.When examining the source text, we find Rah-menbedingungen, elsewhere translated (probablybetter) as "basic conditions".
From these ex-amples it would seem that some work may stillbe needed on harmonizing terminology.7 A lackof (knowledge of) central terms leads in one caseto a translation with some unfortunate connota-tions and in another case to an acceptable butdeviant translation.
In both cases the use of asingle term would most certainly also improveinformation retrieval on the parliament proceed-ings.Related to the domain, but much more culturespecific is the variation in the way the speakersorganize their argumentation.
The example ofspeakers of Dutch announcing the last part oftheir speech has already been mentioned.
An-other thing speakers of Dutch seem to do is thatthey exaggerate their viewpoint, both positivelyand negatively.
The positive exaggeration is visi-ble in the word natuurlijk (naturally, obvi-ously, ?
), which is found in almost a third(326) of the originally Dutch speeches.
One ofthe translations chosen for this word is "need-less to say", thus giving rise to the extremelystrong marker for Dutch in Table 6.
The negativeexaggeration is present in cases where a situationis called insane: waanzinnig, "te gek voorwoorden" ("too crazy for words") or "te gekom los te lopen" ("too crazy to walkaround freely").
A possible translation tem-plate here is "too crazy/absurd/ridiculousfor words", explaining another marker forDutch in Table 5.
A further obviously discourse-related marker is therefore.
As Table 7 shows,it is found from 30% (DE) to 100% (ES) more intranslations than in original English text.
Thismay mean that speakers of the other languagesplace more causal relations in their arguments,but it can also be that therefore just happens tobe a favourite translation option among the trans-lators.
Again, this could be a research topic by7Since EUROPARL only contains speeches some years inthe past, the situation may well have been remedied in themeantime.itself and a thorough investigation is beyond thescope of this paper.Obviously, each source language has somewords or phrases in its vocabulary which makethemselves felt in the translations.
Apart from theones already mentioned, the strongest examplehere is the French marker "a certain numberof".
The original turns out to be "un certainnombre", elsewhere more English-like translatedas "some of".There must also be instances of influencesfrom languages' syntax, but these are muchharder to find.
It is possible that the overuse ofshall, especially for French, is linked to verbswhich are morphologically marked for futuretense.
Also the overuse of And at the beginning ofsentences may be linked to splitting source lan-guage long sentences into two English sentencesor merely to more extensive use of a coordinat-ing connective in the other languages.
For boththese words, there are also far too many occur-rences to examine at this time.6 Comparison to Other WorkThe most similar investigation we are aware of isthat by Baroni and Bernardini (2006).
Theyworked on a corpus of Italian geopolitical journalarticles and used SVMs to distinguish translatedand original Italian text on the basis of mostly n-gram features representing both types of text.They did not attempt to identify the source lan-guage.
Their task corresponds to the translationrecognition task presented in the top half of Ta-ble 1 and their method is comparable to the com-bination of the two models for original texts andtranslations.
They report an accuracy of 86.7%.If we examine the texts with TL equal to Italian,we find combination scores of 85.4% (MB),86.0% (SVC), 93.2% (LP) and 96.3% (SVR).Although their work is different in the choice ofdomain (geopolitical journal articles) and they donot distinguish translated texts as to source lan-guage, the results for SVM classifiers are compa-rable.They also mention that in earlier research(Baroni and Bernardini, 2003), they found that"bigrams most characteristic of translated textare sequences of function words", for both theItalian corpus already mentioned and a corpus ofEU reports written in and translated into English.The 2003 paper itself, however, reports that "amore thorough investigation of the EU data ?failed to reveal systematic differences betweentranslated and original documents".
We must943therefore conclude that their observation mustrefer to the Italian texts.
Still, our tables do showquite a few function word bigrams, but of coursewe have contributed to any such predominanceby blanking out most content words.Borin and Pr?tz (2001) examined translationsfrom Swedish into English, using news articles.They examine over- and underuse of POS n-grams.
They manage to explain some of theirobservations, but not the overuse in the transla-tions of adverbs, infinitives, pronouns and sen-tence-initial prepositions.
We have not examinedPOS classes, but only specific words.
However,we do see various adverbs in prominent positions,especially in Table 7, which indeed shows over-use.
The sentence-initial prepositions might alsobe partly explained by (often lexicalized) adver-bial prepositional phrases.7 ConclusionsWe have shown that classification on the basis ofword n-grams markers is able to identify thesource language of medium-length EuropeanParliament speeches.
Depending on the classifi-cation method used, the actual source languagecan be identified for 87.2% to 96.7% of the textswhen all six target language versions of the textcan be accessed.
If only a single version is avail-able, classification is considerably worse andonly Support Vector Regression consistentlyshows relatively high scores, with accuracies of81.5% for the Italian rendering to 87.4% for theSpanish one.We also examined the strongest markers.
Wefound that they are rather varied in nature andrepresent a wide range of information sources.Vocabulary, discourse structure and probablysyntax of the source language all contribute.Contrasts between source and target languagescan be seen to have an influence too, both purelylinguistically and through the behaviour of thetranslators.
But also the behaviour patterns of theparliamentarians of the various countries have aclear influence.
Some of these influences areharmless or even attractive.
Others should befollowed up on, e.g.
it would be good to attempta harmonization of terminology throughout thevarious translation services, so that informationretrieval on the parliamentary proceedings can beimproved.As for further research, it is vital to first inves-tigate how exactly the European Parliament pro-ceedings have been translated in the past, arebeing translated in the present and will be trans-lated in the future.
It may well be that some ofthe effects we are finding are limited to individ-ual translators, or caused by their use of (ma-chine) translation tools.
Once it is clear that weare measuring what we think we are measuring,namely general trends in speaker and translatorbehaviour, we need to automate the retrieval oftarget language and source language phrases(maybe using statistical machine translationmethodology), and possibly also to address se-mantically related clusters.
Only then can wereally investigate if observations like "Dutchspeakers exaggerate more often" are valid or arejust false impressions from looking at the datathrough too small a window.Once all this is in place, we will have themeans for a whole range of activities, e.g.
tostudy parliamentary behaviour, to study thetranslation process, to determine if source lan-guage should be taken more into account in EU-ROPARL translation models and potentiallyeven to give useful advise to the EU translationservices.ReferencesMarco Baroni and Silvia Bernardini.
2003.
A Prelimi-nary Analysis of Collocational Differences inMonolingual Comparable Corpora.
Proc.
CorpusLinguistics 2003, Lancaster, UK.Marco Baroni and Silvia Bernardini.2006.
A NewApproach to the Study of Translationese: Machine-Learning the Difference between Original andTranslated Text.
Literary and Linguistic Comput-ing, 21(3): 259-274.Lars Borin and Klas Pr?tz.
2001.
Through a glassdarkly: Part of speech distribution in original andtranslated text.
Computational linguistics in theNetherlands 2000.
Edited by Walter Daelemans,Khalil Sima'an, Jorn Veenstra, Jakub Zavrel.
Am-sterdam: Rodopi.
2001.
30-44Chih-Chung Chang and Chih-Jen Lin, LIBSVM: alibrary for support vector machines, 2001.
Avail-able at http://www.csie.ntu.edu.tw/~cjlin/libsvmHans van Halteren and Nelleke Oostdijk.
2004.
Lin-guistic Profiling of Texts for the Purpose of Lan-guage Verification.
COLING 2004, Geneva: 966-972.Philipp Koehn.
2005.
Europarl: A Parallel Corpus forStatistical Machine Translation.
MT Summit X,Phuket, Thailand: 79-86.944
