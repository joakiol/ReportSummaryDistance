Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 305?312Manchester, August 2008Homotopy-based Semi-Supervised Hidden Markov Modelsfor Sequence Labeling?Gholamreza Haffari and Anoop SarkarSchool of Computing ScienceSimon Fraser UniversityBurnaby, BC, Canada{ghaffar1,anoop}@cs.sfu.caAbstractThis paper explores the use of the homo-topy method for training a semi-supervisedHidden Markov Model (HMM) used forsequence labeling.
We provide a novelpolynomial-time algorithm to trace the lo-cal maximum of the likelihood functionfor HMMs from full weight on the la-beled data to full weight on the unla-beled data.
We present an experimentalanalysis of different techniques for choos-ing the best balance between labeled andunlabeled data based on the characteris-tics observed along this path.
Further-more, experimental results on the field seg-mentation task in information extractionshow that the Homotopy-based methodsignificantly outperforms EM-based semi-supervised learning, and provides a moreaccurate alternative to the use of held-outdata to pick the best balance for combin-ing labeled and unlabeled data.1 IntroductionIn semi-supervised learning, given a sample con-taining both labeled data L and unlabeled dataU , the maximum likelihood estimator ?mle maxi-mizes:L(?)
:=?
(x,y)?Llog P (x,y|?
)+?x?Ulog P (x|?
)(1)where y is a structured output label, e.g.
a se-quence of tags in the part-of-speech tagging task,or parse trees in the statistical parsing task.
Whenthe number of labeled instances is very small com-pared to the unlabeled instances, i.e.
|L| ?
|U |,?
We would like to thank Shihao Ji and the anonymousreviewers for their comments.
This research was supported inpart by NSERC, Canada.?
c?
2008.
Licensed under the Creative CommonsAttribution-Noncommercial-Share Alike 3.0 Unported li-cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).Some rights reserved.the likelihood of labeled data is dominated by thatof unlabeled data, and the valuable information inthe labeled data is almost completely ignored.Several studies in the natural language process-ing (NLP) literature have shown that as the size ofunlabeled data increases, the performance of themodel with ?mle may deteriorate, most notablyin (Merialdo, 1993; Nigam et al, 2000).
One strat-egy commonly used to alleviate this problem is toexplicitly weigh the contribution of labeled and un-labeled data in (1) by ?
?
[0, 1].
This new parame-ter controls the influence of unlabeled data but isestimated either by (a) an ad-hoc setting, wherelabeled data is given more weight than unlabeleddata, or (b) by using the EM algorithm or (c) byusing a held-out set.
But each of these alternativesis problematic: the ad-hoc strategy does not workwell in general; the EM algorithm ignores the la-beled data almost entirely; and using held-out datainvolves finding a good step size for the search,but small changes in ?
may cause drastic changesin the estimated parameters and the performanceof the resulting model.
Moreover, if labeled data isscarce, which is usually the case, using a held-outset wastes a valuable resource1 .In this paper, we use continuation techniques(Corduneanu and Jaakkola, 2002) for determining?
for structured prediction tasks involving HMMs,and more broadly, the product of multinomials(PoM) model.
We provide a polynomial-time al-gorithm for HMMs to trace the local maxima ofthe likelihood function from full weight on the la-beled data to full weight on the unlabeled data.
Indoing so, we introduce dynamic programming al-gorithms for HMMs that enable the efficient com-putation over unlabeled data of the covariance be-tween pairs of state transition counts and pairsof state-state and state-observation counts.
Wepresent a detailed experimental analysis of differ-ent techniques for choosing the best balance be-1Apart from these reasons, we also provide an experimen-tal comparision between the homotopy based approach, theEM algorithm, and the use of a held out set.305tween labeled and unlabeled data based on thecharacteristics observed along this path.
Further-more, experimental results on the field segmen-tation task in information extraction show thatthe Homotopy-based method significantly outper-forms EM-based semi-supervised learning, andprovides a more accurate alternative to the use ofheld-out data to pick the best balance for combin-ing labeled and unlabeled data.
We argue this ap-proach is a best bet method which is robust to dif-ferent settings and types of labeled and unlabeleddata combinations.2 Homotopy ContinuationA continuation method embeds a given hard rootfinding problem G(?)
= 0 into a family of prob-lems H(?(?
), ?)
= 0 parameterized by ?
suchthat H(?
(1), 1) = 0 is the original given problem,and H(?
(0), 0) = 0 is an easy problem F (?)
= 0(Richter and DeCarlo, 1983).
We start from a solu-tion ?0for F (?)
= 0, and deform it to a solution?1for G(?)
= 0 while keeping track of the so-lutions of the intermediate problems2.
A simpledeformation or homotopy function is:H(?, ?)
= (1?
?
)F (?)
+ ?G(?)
(2)There are many ways to define a homotopy map,but it is not trivial to always guarantee the exis-tence of a path of solutions for the intermediateproblems.
Fortunately for the homotopy map wewill consider in this paper, the path of solutionswhich starts from ?
= 0 to ?
= 1 exists and isunique.In order to find the path numerically, we seeka curve ?(?)
which satisfies H(?(?
), ?)
= 0.This is found by differentiating with respect to ?and solving the resulting differential equation.
Tohandle singularities along the path and to be ableto follow the path beyond them, we introduce anew variable s (which in our case is the unit pathlength) and solve the following differential equa-tion for (?
(s), ?
(s)):?H(?, ?)?
?d?ds+?H(?, ?)?
?d?ds= 0 (3)subject to ||(d?ds,d?ds)||2= 1 and the initial con-dition (?
(0), ?
(0)) = (?0, 0).
We use the Euler2This deformation gives us a solution path (?(?
), ?
)in Rd+1 for ?
?
[0, 1], where each component of the d-dimensional solution vector ?(?)
= (?1(?
), .., ?d(?))
is afunction of ?.method (see Algorithm 1) to solve (3) but higherorder methods such as Runge-Kutta of order 2 or 3can also be used.3 Homotopy-based Parameter EstimationOne way to control the contribution of the labeledand unlabeled data is to parameterize the log like-lihood function as L?(?)
defined by1?
?|L|?
(x,y)?Llog P (x, y|?)
+?|U |?x?Ulog P (x|?
)How do we choose the best ??
An operator calledEM?is used with the property that its fixed points(locally) maximize L?(?).
Starting from a fixedpoint of EM?when ?
is zero3, the path of fixedpoint of this operator is followed for ?
> 0 bycontinuation techniques.
Finally the best value for?
is chosen based on the characteristics observedalong the path.
One option is to choose an allo-cation value where the first critical4 point occurshad we followed the path based on ?, i.e.
withoutintroducing s (see Sec.
2).
Beyond the first criti-cal point, the fixed points may not have their rootsin the starting point which has all the informa-tion from labeled data (Corduneanu and Jaakkola,2002).
Alternatively, an allocation may be cho-sen which corresponds to the model that gives themaximum entropy for label distributions of unla-beled instances (Ji et al, 2007).
In our experi-ments, we compare all of these methods for de-termining the choice of ?.3.1 Product of Multinomials ModelProduct of Multinomials (PoM) model is an im-portant class of probabilistic models especially forNLP which includes HMMs and PCFGs amongothers (Collins, 2005).
In the PoM model, theprobability of a pair (x,y) isP (x,y|?)
=M?m=1????m?m(?)Count(x,y,?)
(4)where Count(x,y, ?)
shows how many times anoutcome ?
?
?mhas been seen in the input-outputpair (x,y), and M is the total number of multino-mials.
A multinomial distribution parameterized3In general, EM0can have multiple local maxima, but inour case, EM0has only one global maximum, found analyti-cally using relative frequency estimation.4A critical point is where a discontinuity or bifurcation oc-curs.
In our setting, almost all of the critical points correspondto discontinuities (Corduneanu, 2002).306by ?mis put on each discrete space ?mwhere theprobability of an outcome ?
is denoted by ?m(?
).So for each space ?m, we have????m?m(?)
=1.Consider an HMM with K states.
There arethree types of parameters: (i) initial state probabili-ties P (s) which is a multinomial over states ?0(s),(ii) state transition probabilities P (s?|s) which areK multinomials over states ?s(s?)
, and (iii) emis-sion probabilities P (a|s) which are K multinomi-als over observation alphabet ?s+K(a).
To com-pute the probability of a pair (x,y), normally wego through the sequence and multiply the proba-bility of the seen state-state and state-observationevents:P (x,y|?)
= ?0(y0)?y1+K(x1)|y|Yt=2?yt?1(yt)?yt+K(xt)which is in the form of (4) if it is written in termsof the multinomials involved.3.2 EM?Operator for the PoM ModelUsually EM is used to maximize L(?)
and esti-mate the model parameters in the situation wheresome parts of the training data are hidden.
EM hasan intuitive description for the PoM model: start-ing from an arbitrary value for parameters, itera-tively update the probability mass of each eventproportional to its count in labeled data plus its ex-pected count in the unlabeled data, until conver-gence.By changing the EM?s update rule, we get analgorithm for maximizing L?(?):??m(?)
=1?
?|L|?
(x,y)?LCount(x,y, ?)
+?|U |?x?U?y?YxCount(x,y, ?
)P (y|x,?old) (5)where ?
?mis the unnormalized parameter vector,i.e.
?m(?)
=??m(?)P???m??m(?).
The expected countscan be computed efficiently based on the forward-backward recurrence for HMMs (Rabiner, 1989)and inside-outside recurrence for PCFGs (Lari andYoung, 1990).
The right hand side of (5) is an op-erator we call EM?which transforms the old pa-rameter values to their new (unnormalized) values.EM0and EM1correspond respectively to purelysupervised and unsupervised parameter estimationsettings, and:EM?(?)
= (1?
?)EM0(?)
+ ?EM1(?)
(6)3.3 Homotopy for the PoM ModelThe iterative maximization algorithm, described inthe previous section, proceeds until it reaches afixed point EM?(?)
=?
?, where based on (6):(1?
?)
(???
EM0(?
))| {z }F (?)+?
(???
EM1(?
))| {z }G(?
)= 0 (7)The above condition governs the (local) maximaof EM?.
Comparing to (2) we can see that (7) canbe viewed as a homotopy map.We can generalize (7) by replacing (1?
?)
witha function g1(?)
and ?
with g2(?)5.
This corre-sponds to other ways of balancing labeled and un-labeled data log-likelihoods in (1).
Moreover, wemay partition the parameter set and use the homo-topy method to just estimate the parameters in onepartition while keeping the rest of parameters fixed(to inject some domain knowledge to the estima-tion procedure), or repeat it through partitions.
Wewill see this in Sec.
5.2 where the transition matrixof an HMM is frozen and the emission probabili-ties are learned with the continuation method.Algorithm 1 describes how to use continuationtechniques used for homotopy maps in order totrace the path of fixed points for the EM?oper-ator.
The algorithm uses the Euler method to solvethe following differential equation governing thefixed points of EM?:[????EM1(?)?
I EM1(?)?
EM0][d??d?
]= 0For PoM models???EM1(?)
can be written com-pactly as follows6:1|U |?x?UCOVP (y|x,?
)[Count(x,y)]?H (8)where COVP (y|x,?
)[Count(x,y)] is the con-ditional covariance matrix of all featuresCount(x,y, ?)
given an unlabeled instancex.
We denote the entry corresponding to events ?1and ?2of this matrix by COVP (y|x,?
)(?1, ?2); His a block diagonal matrix built from H?iwhereH?i= (?
?i(?1), ..,?
?i(?|?i|)) ?
I?1|?i|?|?i|????i??i(?
)5However the following two conditions must be satisfied:(i) the deformation map is reduced to ( ???EM0(?))
at ?
=0 and ( ???EM1(?))
at ?
= 1, and (ii) the path of solutionsexists for Eqn.
(2).6A full derivation is provided in (Haffari and Sarkar, 2008)307Algorithm 1 Homotopy Continuation for EM?1: Input: Labeled data set L2: Input: Unlabeled data set U3: Input: Step size ?4: Initialize [ ??
?]
= [EM00] based on L5: ?old ?
[0 1]6: repeat7: Compute ???EM1(?)
and EM1(?)
basedon unlabeled data U8: Compute ?
= [d??
d?]
as the kernel of[????EM1(?)?
I EM1(?)?
EM0]9: if ?
?
?old < 0 then10: ?
?
?
?11: end if12: [ ??
?]?
[ ??
?]
+ ?
?||?||213: ?old ?
?14: until ?
?
1Computing the covariance matrix in (8) is achallenging problem because it consists of sum-ming quantities over all possible structures Yxas-sociated with each unlabeled instance x, which isexponential in the size of the input for HMMs.4 Efficient Computation of the Covari-ance MatrixThe entry COVP (y|x,?
)(?1, ?2) of the features co-variance matrix isE[Count(x,y, ?1)Count(x,y, ?2)]?E[Count(x,y, ?1)]E[Count(x,y, ?2)]where the expectations are taken under P (y|x,?
).To efficiently calculate the covariance, we needto be able to efficiently compute the expectations.The linear count expectations can be computed ef-ficiently by the forward-backward recurrence forHMMs.
However, we have to design new algo-rithms for quadratic count expectations which willbe done in the rest of this section.We add a special begin symbol to the se-quences and replace the initial probabilities withP (s|begin).
Based on the terminology used in (4),the outcomes belong to two categories: ?
= (s, s?
)where state s?
follows state s, and ?
= (s, a)where symbol a is emitted from state s. De-fine the feature function f?
(x,y, t) to be 1 if theoutcome ?
happens at time step t, and 0 other-wise.
Based on the fact that Count(x,y, ?)
=?|x|t=1f?
(x,y, t), we haveE[Count(x,y, ?1)Count(x,y, ?2)] =?t1?t2?y?Yxf?1(x,y, t1)f?2(x,y, t2)P (y|x,?
)which is the summation of |x|2 different expecta-tions.
Fixing two positions t1and t2, each expec-tation is the probability (over all possible labels) ofobserving ?1and ?2at these two positions respec-tively, which can be efficiently computed using thefollowing data structure.
Prepare an auxiliary tableZx containing P (x[i+1,j], si, sj), for every pair ofstates siand sjfor all positions i, j (i ?
j):Zxi,j(si, sj) =Xsi+1,..,sj?1j?1Yk=iP (sk+1|sk)P (xk+1|sk+1)Let matrix Mxk= [Mxk(s, s?)]
where Mxk(s, s?)
=P (s?|s)P (xk|s?
); then Zxi,j=?j?1k=iMxk.
Forwardand backward probabilities can also be computedfrom Zx, so building this table helps to computeboth linear and quadratic count expectations.With this table, computing the quadratic countsis straightforward.
When both events are of typestate-observation, i.e.
?
= (s, a) and ??
= (s?, a?
),their expected quadratic count can be computed as?t1?t2?xt1,a?xt2,a?
[?kP (k|begin)Zx1,t1(k, s).Zxt1,t2(s, s?).
?kZxt2,n(s?, k)P (end|k)]where ?xt,ais 1 if xtis equal to a and 0 otherwise.Likewise we can compute the expected quadraticcounts for other combination of events: (i) bothare of type state-state, (ii) one is of type state-stateand the other state-observation.There are L(L+1)2tables needed for a sequenceof length L, and the time complexity of buildingeach of them is O(K3) where K is the number ofstates in the HMM.
When computing the covari-ance matrix, the observations are fixed and thereis no need to consider all possible combinationsof observations and states.
The most expensivepart of the computation is the situation where thetwo events are of type state-state which amountsto O(L2K4) matrix updates.
Noting that a singleentry needs O(K) for its updating, the time com-plexity of computing expected quadratic counts fora single sequence is O(L2K5).
The space neededto store the auxiliary tables is O(L2K2) and thespace needed for covariance matrix is O((K2 +NK)2) where N is the alphabet size.5 Experimental ResultsIn the field segmentation task, a document is con-sidered to be a sequence of fields.
The goal is308[EDITOR A. Elmagarmid, editor.]
[TITLE Transaction Models for Advanced Database Applications] [PUBLISHER Morgan-Kaufmann,] [DATE 1992.
]Figure 1: A field segmentation example for Citations dataset.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 10.120.130.140.150.160.170.18?Error (per position)EM?2freezError on Citation Test (300L5000U)Viterbi DecodingSMS Decoding?MLE(a)0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 10.120.130.140.150.160.170.180.190.20.21?Error (per position)EM?freezError on Citation Test (300L5000U)Viterbi DecodingSMS Decoding?MLE(b)Figure 2: EM?error rates while increasing the allocation from 0 to 1 by the step size 0.025.to segment the document into fields, and to labeleach field.
In our experiments we use the bibli-ographic citation dataset described in (Peng andMcCallum, 2004) (see Fig.
1 for an example ofthe input and expected label output for this task).This dataset has 500 annotated citations with 13fields; 5000 unannotated citations were added toit later by (Grenager et al, 2005).
The annotateddata is split into a 300-document training set, a100-document development (dev) set, and a 100-document test set7.We use a first order HMM with the size of hid-den states equal to the number of fields (equal to13).
We freeze the transition probabilities to whathas been observed in the labeled data and onlylearn the emission probabilities.
The transitionprobabilities are kept frozen due to the nature ofthis task in which the transition information canbe learned with very little labeled data, e.g.
firststart with ?author?
then move to ?title?
and so on.However, the challenging aspect of this dataset isto find the segment spans for each field, which de-pends on learning the emission probabilities, basedon the fixed transition probabilities.At test time, we use both Viterbi (most probablesequence of states) decoding and sequence of mostprobable states decoding methods, and abbreviatethem by Viterbi and SMS respectively.
We reportresults in terms of precision, recall and F-measurefor finding the citation fields, as well as accuracycalculated per position, i.e.
the ratio of the wordslabeled correctly for sequences to all of the words.The segment-based precision and recall scores are,7From http://www.stanford.edu/grenager/data/unsupie.tgzof course, lower than the accuracy computed onthe per-token basis.
However, both these numbersneed to be taken into account in order to under-stand performance in the field segmentation task.Each input word sequence in this task is very long(with an average length of 36.7) but the number offields to be recovered is a small number compar-atively (on average there are 5.4 field segments ina sentence where the average length of a segmentis 6.8).
Even a few one-word mistakes in findingthe full segment span leads to a drastic fall in pre-cision and recall.
The situation is quite differentfrom part-of-speech tagging, or even noun-phrasechunking using sequence learning methods.
Thus,for this task both the per-token accuracy as well asthe segment precision and recall are equally impor-tant in gauging performance.Smoothing to remove zero components in thestarting point is crucial otherwise these features donot generalize well and yet we know that they havebeen observed in the unlabeled data.
We use a sim-ple add-?
smoothing, where ?
is .2 for transitiontable entries and .05 for the emission table entries.In all experiments, we deal with unknown words intest data by replacing words seen less than 5 timesin training by the unknown word token.5.1 Problems with MLEMLE chooses to set ?
= |U ||L|+|U |which almostignores labeled data information and puts all theweight on the unlabeled data8.
To see this empir-ically, we show the per position error rates at dif-8One anonymous reviewer suggests using ?
= |L||L|+|U|but the ?best bet?
for different tasks that we mention in theIntroduction may not necessarily be a small ?
value.309ferent source allocation for HMMs trained on 300labeled and 5000 unlabeled sequences for the Ci-tation dataset in Fig.
2(a).
For each allocation wehave run EM?algorithm, initialized to smoothedcounts from labeled data, until convergence.
Asthe plots show, initially the error decreases as ?
in-creases; however, it starts to increase after ?
passesa certain value.
MLE has higher error rates com-pared to complete data estimate, and its perfor-mance is far from the best way of combining la-beled and unlabeled data.In Fig.
2(b), we have done similar experimentwith the difference that for each value of ?, thestarting point of the EM?is the final solutionfound in the previous value of ?.
As seen in theplot, the intermediate local optima have better per-formance compared to the previous experiment,but still the imbalance between labeled and unla-beled data negatively affects the quality of the so-lutions compared to the purely supervised solution.The likelihood surface is non-convex and hasmany local maxima.
Here EM performs hill climb-ing on the likelihood surface, and arguably the re-sulting (locally optimal) model may not reflect thequality of the globally optimal MLE.
But we con-jecture that even the MLE model(s) which globallymaximize the likelihood may suffer from the prob-lem of the size imbalance between labeled and un-labeled data, since what matters is the influenceof unlabeled data on the likelihood.
(Chang et.al., 2007) also report on using hard-EM on thesedatasets9 in which the performance degrades com-pared to the purely supervised model.5.2 Choosing ?
in Homotopy-based HMMWe analyze different criteria in picking the bestvalue of ?
based on inspection of the continuationpath.
The following criteria are considered:?
monotone: The first iteration in which themonotonicity of the path is changed, or equiva-lently the first iteration in which the determinantof ????EM1(?
)?I in Algorithm 1 becomes zero(Corduneanu and Jaakkola, 2002).?
minEig: Instead of looking into the determinantof the above matrix, consider its minimum eigen-value.
Across all iterations, choose the one forwhich this minimum eigenvalue is the lowest.
?maxEnt: Choose the iteration whose model putsthe maximum entropy on the labeling distributionfor unlabeled data (Ji et al, 2007).9In Hard-EM, the probability mass is fully assigned to themost probable label, instead of all possible labels.The second criterion is new, and experimentallyhas shown a good performance; it indicates theamount of singularity of a matrix.100 150 200 250 300 350 400 450 50000.10.20.30.40.50.60.70.80.9# Unlabeled Sequences?Best Selected AllocationsmontonmaxEntminEigEMFigure 3: ?
values picked by different methods.The size of the labeled data is fixed to 100, andresults are averaged over 4 runs.
The ?
valuespicked by MaxEnt method for 500 unlabeled ex-amples was .008.We fix 100 labeled sequences and vary the num-ber of unlabeled sequences from 100 to 500 by astep of 50.
All of the experiments are repeatedfour times with different randomly chosen unla-beled datasets, and the results are the average overfour runs.
The chosen allocations based on the de-scribed criteria are plotted in Figure 3, and theirassociated performance measures can be seen inFigure 4.Figure 3 shows that as the unlabeled data setgrows, the reliance of ?minEig?
and ?monotone?methods on unlabeled data decreases whereas inEM it increases.
The ?minEig?
method is moreconservative than ?monotone?
in that it usuallychooses smaller ?
values.
The plots in Fig-ure 4 show that homotopy-based HMM alwaysoutperforms EM-based HMM.
Moreover, ?max-Ent?
method outperforms other ways of picking?.
However, as the size of the unlabeled data in-creases, the three methods tend to have similar per-formances.5.3 Homotopy v.s.
other methodsIn the second set of experiments, we comparethe performance of the homotopy based methodagainst the competitive methods for picking thevalue of ?.We use all of the labeled sequences (size is 300)and vary the number of unlabeled sequences from300 to 1000 by the step size of 100.
For the firstcompetitive method, 100 labeled sequences are putin a held out set and used to select the best value310100 200 300 400 5000.20.25F?measure# Unlabeled Sequences100 200 300 400 5000.80.820.84Total accuracy# Unlabeled Sequences100 200 300 400 5000.10.150.20.250.3F?measure100 200 300 400 5000.80.820.84Total accuracymonotone maxEnt minEig EMFigure 4: The comparison of different techniquesfor choosing the best allocation based on datasetswith 100 labeled sequences and varying number ofunlabeled sequences.
Each figure shows the av-erage over 4 runs.
F-measure is calculated basedon the segments, and total accuracy is calculatedbased on tokens in individual positions.
The twoplots in the top represent Viterbi decoding, and thetwo plots in the bottom represent SMS decoding.of ?
based on brute-force search using a fixed stepsize; afterwards, this value is used to train HMM(based on 200 remaining labeled sequences andunlabeled data).
The second competitive method,which we call ?Oracle?, is similar to the previousmethod except we use the test set as the held out setand all of the 300 labeled sequences as the train-ing set.
In a sense, the resulting model is the bestwe can expect from cross validation based on theknowledge of true labels for the test set.
Despitethe name ?Oracle?, in this setting the ?
value is se-lected based on the log-likelihood criterion, so it ispossible that the ?Oracle?
method is outperformedby another method in terms of precision/recall/f-score.
Finally, EM is considered as the third base-line.The results are summarized in Table 1.
Whendecoding based on SMS, the homotopy-basedHMM outperforms the ?Held-out?
method for allof performance measures, and generally behavesbetter than the ?Oracle?
method.
When decodingbased on Viterbi, the accuracy of the homotopy-based HMM is better than ?Held-out?
and is inthe same range as the ?Oracle?
; the three meth-ods have roughly the same f-score.
The ?
valuefound by Homotopy gives a small weight to unla-beled data, and so it might seem that it is ignoringthe unlabeled data.
This is not the case, even witha small weight the unlabeled data has an impact,as can be seen in the comparison with the purelySupervised baseline in Table 1 where the Homo-topy method outperforms the Supervised baselineby more than 3.5 points of f-score with SMS-decoding.
Homotopy-based HMM with SMS-decoding outperforms all of the other methods.We noticed that accuracy was better for 700 un-labeled examples in this dataset, and so we includethose results as well in Table 1.
We observed somenoise in unlabeled sequences; so as the size of theunlabeled data set grows, this noise increases aswell.
In addition to finding the right balance be-tween labeled and unlabeled data, this is anotherfactor in semi-supervised learning.
For each par-ticular unlabeled dataset size (we experimented us-ing 300 to 1000 unlabeled data with a step size of100) the Homotopy method outperforms the otheralternatives.6 Related Previous WorkHomotopy based parameter estimation was orig-inally proposed in (Corduneanu and Jaakkola,2002) for Na?
?ve Bayes models and mixture ofGaussians, and (Ji et al, 2007) used it for HMM-based sequence classification which means that aninput sequence x is classified into a class labely ?
{1, .
.
.
, k} (the class label is not structured,i.e.
not a sequence of tags).
The classification isdone using a collection of k HMMs by computingPr(x, y | ?y) which sums over all states in eachHMM ?y for input x.
The algorithms in (Ji et al,2007) could be adapted to the task of sequence la-beling, but we argue that our algorithms provide astraightforward and direct solution.There have been some studies using the Cita-tion dataset, but it is not easy to directly comparetheir results due to differences in preprocessing,the amount of the previous knowledge and richfeatures used by the models, and the training datawhich were used.
(Chang et.
al., 2007) used a firstorder HMM in order to investigate injecting priordomain knowledge to self-training style bootstrap-ping by encoding human knowledge into declara-tive constraints.
(Grenager et al, 2005) used a firstorder HMM which has a diagonal transition matrixand a specialized boundary model.
In both works,the number of randomly selected labeled and un-labeled training data is varied, which makes a di-311size of ?
Viterbi decoding SMS decodingunlab data p, r, f-score accuracy p, r, f-score accuracyHomotopy 700 .004 .292, .290, .290 87.1% .321, .332, .326 89%1000 .004 .292, .291, .291 87.9% .296, .298, .296 88.6%Held-out 700 .220 .311, .291, .297 87.1% .295, .288, .289 87.2%1000 .320 .300, .276, .283 86.9% .308, .281, .287 87.2%Oracle 700 .150 .284, .293, .287 87.8% .295, .313, .303 88%1000 .200 .285, .294, .289 87.9% .277, .292, .284 88.7%EM 700 .700 .213, .211, .211 84.8% .213, .220, .216 85.2%1000 .770 .199, .198, .198 83.7% .187, .198, .192 83.6%Supervised 0 0 .281, .278, .279 87% .298, .280, .288 88.4%Table 1: Results using entire labeled data with segment precision/recall/f-score and token based accuracy.rect numerical comparison impossible.
(Peng andMcCallum, 2004) used only labeled data to trainconditional random fields and HMMs with secondorder state transitions where they allow observa-tion in each position to depend on the current stateas well as observation of the previous position.7 ConclusionIn many NLP tasks, the addition of unlabeled datato labeled data can decrease the performance onthat task.
This is often because the unlabeled datacan overwhelm the information obtained from thelabeled data.
In this paper, we have described amethodology and provided efficient algorithms foran approach that attempts to ensure that unlabeleddata does not hurt performance.
The experimen-tal results show that homotopy-based training per-forms better than other commonly used compet-itive methods.
We plan to explore faster waysfor computing the (approximate) covariance ma-trix, e.g., label sequences can be sampled fromP (y|x,?)
and an approximation of the covari-ance matrix can be computed based on these sam-ples.
Also, it is possible to compute the covariancematrix in polynomial-time for labels which havericher interdependencies such as those generatedby a context free grammars (Haffari and Sarkar,2008).
Finally, in Algorithm 1 we used a fixedstep size; the number of iterations in the homo-topy path following can be reduced greatly withadaptive step size methods (Allgower and Georg,1993).ReferencesE.
L. Allgower, K. Georg 1993.
Continuation and PathFollowing, Acta Numerica, 2:1-64.M.
Chang and L. Ratinov and D. Roth.
2007.
GuidingSemi-Supervision with Constraint-Driven Learning,ACL 2007.M.
Collins 2005.
Notes on the EM Algorithm, NLPcourse notes, MIT.A.
Corduneanu.
2002.
Stable Mixing of Complete andIncomplete Information, Masters Thesis, MIT.A.
Corduneanu and T. Jaakkola.
2002.
ContinuationMethods for Mixing Heterogeneous Sources, UAI2002.T.
Grenager, D. Klein, and C. Manning.
2005.
Unsu-pervised Learning of Field Segmentation Models forInformation Extraction, ACL 2005.G.
Haffari and A. Sarkar.
2008.
A ContinuationMethod for Semi-supervised Learning in Productof Multinomials Models, Technical Report.
SimonFraser University.
School of Computing Science.K.
Lari, and S. Young.
1990.
The estimation ofstochastic context-free grammars using the inside-outside algorithm, Computer Speech and Language(4).S.
Ji, L. Watson and L. Carin.
2007.
Semi-SupervisedLearning of Hidden Markov Models via a HomotopyMethod, manuscript.B.
Merialdo.
1993.
Tagging English text with a proba-bilistic model, Computational LinguisticsK.
Nigam, A. McCallum, S. Thrun and T. Mitchell.2000.
Text Classification from Labeled and Unla-beled Documents using EM, Machine Learning, 39.p.
103-134.F.
Peng and A. McCallum.
2004.
Accurate InformationExtraction from Research Papers using ConditionalRandom Fields, HLT-NAACL 2004.L.
Rabiner.
1989.
A Tutorial on Hidden Markov Mod-els and Selected Applications in Speech Recogni-tion, Proc.
of the IEEE, 77(2).S.
Richter, and R. DeCarlo.
1983.
Continuation meth-ods: Theory and applications, IEEE Trans.
on Auto-matic Control, Vol 26, issue 6.312
