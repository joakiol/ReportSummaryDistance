Proceedings of the 4th Biennial International Workshop on Balto-Slavic Natural Language Processing, pages 24?33,Sofia, Bulgaria, 8-9 August 2013. c?2010 Association for Computational LinguisticsFrequently Asked Questions Retrieval for CroatianBased on Semantic Textual SimilarityMladen Karan?
Lovro ?mak?
Jan ?najder?
?University of Zagreb, Faculty of Electrical Engineering and ComputingUnska 3, 10000 Zagreb, Croatia?Studio Artlan, Andrije ?tangera 18, 51410 Opatija, Croatia{mladen.karan,jan.snajder}@fer.hr lovro.zmak@studioartlan.hrAbstractFrequently asked questions (FAQ) are anefficient way of communicating domain-specific information to the users.
Unlikegeneral purpose retrieval engines, FAQ re-trieval engines have to address the lexi-cal gap between the query and the usu-ally short answer.
In this paper we de-scribe the design and evaluation of a FAQretrieval engine for Croatian.
We framethe task as a binary classification prob-lem, and train a model to classify eachFAQ as either relevant or not relevant fora given query.
We use a variety of se-mantic textual similarity features, includ-ing term overlap and vector space features.We train and evaluate on a FAQ test col-lection built specifically for this purpose.Our best-performing model reaches 0.47of mean reciprocal rank, i.e., on averageranks the relevant answer among the toptwo returned answers.1 IntroductionThe amount of information available online isgrowing at an exponential rate.
It is becoming in-creasingly difficult to navigate the vast amountsof data and isolate relevant pieces of informa-tion.
Thus, providing efficient information accessfor clients can be essential for many businesses.Frequently asked questions (FAQ) databases are apopular way to present domain-specific informa-tion in the form of expert answers to users ques-tions.
Each FAQ consists of a question and ananswer, possibly complemented with additionalmetadata (e.g., keywords).
A FAQ retrieval engineprovides an interface to a FAQ database.
Given auser query in natural language as input, it retrievesa ranked list of FAQs relevant to the query.FAQ retrieval can be considered half way be-tween traditional document retrieval and questionanswering (QA).
Unlike in full-blown QA, in FAQretrieval the questions and the answers are alreadyextracted.
On the other hand, unlike in documentretrieval, FAQ queries are typically questions andthe answers are typically much shorter than doc-uments.
While FAQ retrieval can be approachedusing simple keyword matching, the performanceof such systems will be severely limited due to thelexical gap ?
a lack of overlap between the wordsthat appear in a query and words from a FAQ pair.As noted by Sneiders (1999), there are two causesfor this.
Firstly, the FAQ database creators in gen-eral do not know the user questions in advance.Instead, they must guess what the likely questionswould be.
Thus, it is very common that users?
in-formation needs are not fully covered by the pro-vided questions.
Secondly, both FAQs and userqueries are generally very short texts, which di-minishes the chances of a keyword match.In this paper we describe the design and theevaluation of a FAQ retrieval engine for Croat-ian.
To address the lexical gap problem, we takea supervised learning approach and train a modelthat predicts the relevance of a FAQ given a query.Motivated by the recent work on semantic textualsimilarity (Agirre et al 2012), we use as modelfeatures a series of similarity measures based onword overlap and semantic vector space similar-ity.
We train and evaluate the model on a FAQdataset from a telecommunication domain.
On thisdataset, our best performing model achieves 0.47of mean reciprocal rank, i.e., on average ranks therelevant FAQ among the top two results.In summary, the contribution of this paper istwofold.
Firstly, we propose and evaluate aFAQ retrieval model based on supervised machinelearning.
To the best of our knowledge, no previ-24ous work exists that addresses IR for Croatian ina supervised setting.
Secondly, we build a freelyavailable FAQ test collection with relevance judg-ments.
To the best of our knowledge, this is thefirst IR test collection for Croatian.The rest of the paper is organized as follows.In the next section we give an overview of relatedwork.
In Section 3 we describe the FAQ test col-lection, while in Section 4 we describe the retrievalmodel.
Experimental evaluation is given in Sec-tion 5.
Section 6 concludes the paper and outlinesfuture work.2 Related WorkMost prior work on FAQ retrieval has focused onthe problem of lexical gap, and various approacheshave been proposed for bridging it.
Early work,such as Sneiders (1999), propose to manually en-rich the FAQ databases with additional meta datasuch as the required, optional, and forbidden key-words and keyphrases.
This effectively reducesFAQ retrieval to simple keyword matching, how-ever in this case it is the manually assigned meta-data that bridges the lexical gap and provides thelook and feel of semantic search.For anything but a small-sized FAQ database,manual creation of metadata is tedious and costintensive, and in addition requires expert knowl-edge.
An alternative is to rely on general lin-guistic resources.
FAQ finder (Burke et al 1997)uses syntax analysis to identify phrases, and thenperforms matching using shallow lexical semanticknowledge from WordNet (Miller, 1995).
Yet an-other way to bridge the lexical gap is smoothingvia clustering, proposed by Kim and Seo (2006).First, query logs are expanded with word defini-tions from a machine readable dictionary.
Subse-quently, query logs are clustered, and query simi-larity is computed against the clusters, instead ofagainst the individual FAQs.
As an alternative toclustering, query expansion is often used to per-form lexical smoothing (Voorhees, 1994; Navigliand Velardi, 2003).In some domains a FAQ engine additionallymust deal with typing errors and noisy user-generated content.
An example is the FAQ re-trieval for SMS messages, described by Kothari etal.
(2009) and Contractor et al(2010).Although low lexical overlap is identified asthe primary problem in FAQ retrieval, sometimesit is the high lexical overlap that also presents aproblem.
This is particularly true for large FAQdatabases in which a non-relevant document can?accidentally?
have a high lexical overlap witha query.
Moreo et al(2012) address the prob-lem of false positives using case based reason-ing.
Rather than considering only the words, theyuse phrases (?differentiator expressions?)
that dis-criminate well between FAQs.The approaches described so far are essentiallyunsupervised.
A number of supervised FAQ re-trieval methods have been described in the litera-ture.
To bridge the lexical gap, Xue et al(2008)use machine translation models to ?translate?
theuser query into a FAQ.
Their system is trained onvery large FAQ knowledge bases, such as Yahooanswers.
Soricut and Brill (2004) describe anotherlarge-scale FAQ retrieval system, which uses lan-guage and transformation models.
A good gen-eral overview of supervised approaches to rankingtasks is the work by Liu (2009).Our system falls into the category of supervisedmethods.
In contrast to the above-described ap-proaches, we use a supervised model with wordoverlap and semantic similarity features.
Takinginto account that FAQs are short texts, we use fea-tures that have been recently proposed for deter-mining the semantic similarity between pairs ofsentences (?aric?
et al 2012).
Because we trainour model to output a relevance score for eachdocument, our approach is essentially a pointwiselearning-to-rank approach (Qin et al 2008).3 Croatian FAQ test collectionThe standard procedure for IR evaluation requiresa test collection consisting of documents, queries,and relevance judgments.
We additionally requirean annotated dataset to train the model.
As therecurrently exists no standard IR test collection forCroatian, we decided to build a FAQ test collec-tion from scratch.
We use this collection for bothmodel training and retrieval evaluation.To obtain a FAQ test collection, we crawled theweb FAQ of Vip,1 a Croatian mobile phone opera-tor.
For each FAQ, we retrieved both the questionand the answer.
In the Vip FAQ database ques-tions are categorized into several broad categories(e.g., by type of service).
For each FAQ, we alsoextract the category name assigned to it.
We ob-tained a total of 1344 FAQs.
After removing the1http://www.vipnet.hr/pitanja-i-odgovori/ (accessed Sep 2009)25Query FAQ question FAQ answerKako se spaja na internet?
(How to con-nect to the internet?
)?to mi je potrebno da bih spojio rac?u-nalo i koristio se internetom?
(What doI need to connect my computer and usethe internet)Morate spojiti rac?unalo sa Homeboxured?ajem LAN kabelom.
.
.
(You mustconnect your computer to the Homeboxdevice using a LAN cable .
.
.
)Putujem izvan Hrvatske i ?elim koristitisvoj Vip mobilni ured?aj.
Koliko c?e meto ko?tati?
(I am traveling abroad andwant to use my Vip mobile device.
Howmuch will this cost?
)Koja je mre?a najpovoljnija za razgov-ore, a koja za slanje SMS i MMSporuka u roamingu?
(Which network isthe best for conversations, and whichone for SMS and MMS messages inroaming?
)Cijene za odlazne pozive u inozemstvusu najpovoljnije u mre?ama Vodafonepartnera.
.
.
(Outgoing calls cost less onnetworks of Vodafone partners .
.
.
)Kako pogledati e-mail preko mobitela?
(How to check e-mail using a mobilephone?
)Koja je cijena kori?tenja BlackBerryOffice usluge?
(What is the price of us-ing the BlackBerry Office service?).
.
.
business e-mail usluga urac?unata je ucijenu.
.
.
(.
.
.
business e-mail is includedin the price .
.
.
)Table 1: Examples of relevant answers to queries from the datasetduplicates, 1222 unique FAQ pairs remain.Next, we asked ten annotators to create at leasttwelve queries each.
They were instructed to in-vent queries that they think would be asked by realusers of Vip services.
To ensure that the queriesare as original as possible, the annotators were notshown the original FAQ database.
Following Lyti-nen and Tomuro (2002), after creating the queries,the annotators were instructed to rephrase them.We asked the annotators to make between threeand ten paraphrases of each query.
The paraphrasestrategies suggested were the following: (1) turn aquery into a multi-sentence query, (2) change thestructure (syntax) of the query, (3) substitute somewords with synonyms, while leaving the structureintact, (4) turn the query into a declarative sen-tence, and (5) any combination of the above.
Theimportance of not changing the underlying mean-ing of a query was particularly stressed.The next step was to obtain the binary relevancejudgments for each query.
Annotating relevancefor the complete FAQ database is not feasible, asthe total number of query-FAQ pairs is too large.On the other hand, not considering some of theFAQs would make it impossible to estimate re-call.
A feasible alternative is the standard poolingmethod predominantly used in IR evaluation cam-paigns (Voorhees, 2002).
In the pooling method,the top-k ranked results of each evaluated systemare combined into a single list, which is then an-notated for relevance judgments.
For a sufficientlylarge k, the recall estimate will be close to realrecall, as the documents that are not in the poolare likely to be non-relevant.
We simulate this set-ting using several standard retrieval models: key-word search, phrase search, tf-idf, and languagemodeling.
The number of combined results perquery is between 50 and 150.
To reduce the an-notators?
bias towards top-ranked examples, theretrieved results were presented in random order.For each query, the annotators gave binary judg-ments (?relevant?
or ?not relevant?)
to each FAQfrom the pooled list; FAQs not in the pool are as-sumed to be not relevant.
Although the appropri-ateness of binary relevance has been questioned(e.g., by Kek?l?inen (2005)), it is still commonlyused for FAQ and QA collections (Wu et al 2006;Voorhees and Tice, 2000).
Table 1 shows exam-ples of queries and relevant FAQs.The above procedure yields a set of pairs(Qr, Frel ), where Qr is a set of query paraphrasesand Frel is the set of relevant FAQs for any queryparaphrase from Qr.
The total number of suchpairs is 117.
From this set we generate a set ofpairs (q, Frel ), where q ?
Qr is a single query.The total number of such pairs is 419, of which327 have at least one answer (Frel 6= ?
), while92 are not answered (Frel = ?).
In this workwe focus on optimizing the performance on an-swered queries and leave the detection and han-dling of unanswered queries for future work.
Theaverage number of relevant FAQs for a query is1.26, while on average each FAQ is relevant for1.44 queries.
Test collection statistics is shown inTable 2.
We make the test collection freely avail-able for research purposes.2For further processing, we lemmatized thequery and FAQ texts using the morphological lex-icon from ?najder et al(2008).
We removed thestopwords using a list of 179 Croatian stopwords.2Available under CC BY-SA-NC license fromhttp://takelab.fer.hr/faqir26Word counts FormMin Max Avg Quest.
Decl.Queries 1 25 8 372 47FAQ questions 4 63 7 287 4FAQ answers 1 218 30 ?
?Table 2: FAQ test collection statisticsWe retained the stopwords that constitute a part ofa service name (e.g., the pronoun ?me?
(?me?)
in?Nazovi me?
(?Call me?
)).4 Retrieval modelThe task of the retrieval model is to rank the FAQsby relevance to a given query.
In an ideal case,the relevant FAQs will be ranked above the non-relevant ones.
The retrieval model we propose isa confidence-rated classifier trained on binary rel-evance judgments, which uses as features the se-mantic textual similarity between the query andthe FAQ.
For a given a query-FAQ pair, the clas-sifier outputs whether the FAQ is relevant (posi-tive) or irrelevant (negative) for the query.
Moreprecisely, the classifier outputs a confidence score,which can be interpreted as the degree of rele-vance.
Given a single query as input, we run theclassifier on all query-FAQ pairs to obtain the con-fidence scores for all FAQs from the database.
Wethen use these confidence scores to produce the fi-nal FAQ ranking.The training set consists of pairs (q, f) from thetest collection, where q ?
Qr is a query fromthe set of paraphrase queries and f ?
Frel is aFAQ from the set of relevant FAQs for this query(cf.
Section 3).
Each (q, f) pair represents a posi-tive training instance.
To create a negative traininginstance, we randomly select a (q, f) pair from theset of positive instances and substitute the relevantFAQ f with a randomly chosen non-relevant FAQf ?.
As generating all possible negative instanceswould give a very imbalanced dataset, we chose togenerate only 2N negative instances, where N isthe number of positive instances.
Because |Frel|varies depending on query q, number of instancesN per query also varies; on average, N is 329.To train the classifier, we compute a feature vec-tor for each (q, f) instance.
The features measurethe semantic textual similarity between q and f .More precisely, the features measure (1) the sim-ilarity between query q and the question from fand (2) the similarity between query q and the an-swer from f .
Considering both FAQ question andanswer has proven to be beneficial (Tomuro andLytinen, 2004).
Additionally, ngram overlap fea-tures are computed between the query and FAQcategory name.As the classification model, we use the SupportVector Machine (SVM) with radial basis kernel.We use the LIBSVM implementation from Changand Lin (2011).4.1 Term overlap featuresWe expect that FAQ relevance to be positively cor-related with lexical overlap between FAQ text andthe user query.
We use several lexical overlapfeatures.
Similar features have been proposed byMichel et al(2011) for paraphrase classificationand by ?aric?
et al(2012) for semantic textual sim-ilarity.Ngram overlap (NGO).
Let T1 and T2 be thesets of consecutive ngrams (e.g., bigrams) in thefirst and the second text, respectively.
NGO is de-fined asngo(T1, T2) = 2?
(|T1||T1 ?
T2|+|T2||T1 ?
T2|)?1(1)NGO measures the degree to which the first textcovers the second and vice versa.
The two scoresare combined via a harmonic mean.
We computeNGO for unigrams and bigrams.IC weighted word overlap (ICNGO).
NGOgives equal importance to all words.
In practice,we expect some words to be more informative thanothers.
The informativeness of a word can be mea-sured by its information content (Resnik, 1995),defined asic(w) = ln?w?
?C freq(w?
)freq(w)(2)where C is the set of words from the corpus andfreq(w) is the frequency of word w in the corpus.We use the HRWAC corpus from Ljube?ic?
and Er-javec (2011) to obtain the word counts.Let S1 and S2 be the sets of words occurringin the first and second text, respectively.
The IC-weighted word coverage of the second text by thefirst text is given bywwc(S1, S2) =?w?S1?S2 ic(w)?w??S2ic(w?
)(3)We compute the ICNGO feature as the harmonicmean of wwc(S1, S2) and wwc(S2, S1).274.2 Vector space featuresTf-idf similarity (TFIDF).
The tf-idf (term fre-quency/inverse document frequency) similarity oftwo texts is computed as the cosine similarity oftheir tf-idf weighted bag-of-words vectors.
The tf-idf weights are computed on the FAQ test collec-tion.
Here we treat each FAQ (without distinctionbetween question, answer, and category parts) as asingle document.LSA semantic similarity (LSA).
Latent seman-tic analysis (LSA), first introduced by Deerwesteret al(1990), has been shown to be very effectivefor computing word and document similarity.
Tobuild the LSA model, we proceed along the linesof Karan et al(2012).
We build the model fromCroatian web corpus HrWaC from Ljube?ic?
andErjavec (2011).
For lemmatization, we use themorphological lexicon from ?najder et al(2008).Prior to the SVD, we weight the matrix elementswith their tf-idf values.
Preliminary experimentsshowed that system performance remained satis-factory when reducing the vector space to only 25dimensions, but further reduction caused deterio-ration.
We use 25 dimensions in all experiments.LSA represents the meaning of a w by a vectorv(w).
Motivated by work on distributional seman-tic compositionality (Mitchell and Lapata, 2008),we compute the semantic representation of text Tas the semantic composition (defined as vector ad-dition) of the individual words constituting T :v(T ) =?w?Tv(w) (4)We compute the similarity between texts T1 andT2 as the cosine between v(T1) and v(T2).IC weighted LSA similarity (ICLSA).
In theLSA similarity feature all words occurring in a textare considered to be equally important when con-structing the compositional vector, ignoring thefact that some words are more informative thanothers.
To acknowledge this, we use informationcontent weights defined by (2) and compute the ICweighted compositional vector of a text T asc(T ) =?wi?Tic(wi)v(wi) (5)Aligned lemma overlap (ALO).
This featuremeasures the similarity of two texts by semanti-cally aligning their words in a greedy fashion.
Tocompare texts T1 and T2, first all pairwise sim-ilarities between words from T1 and words fromT2 are computed.
Then, the most similar pair isselected and removed from the list.
The procedureis repeated until all words are aligned.
The alignedpairs are weighted by the larger information con-tent of the two words:sim(w1, w2) = (6)max(ic(w1), ic(w2))?
ssim(w1, w2)where ssim(w1, w2) is the semantic similarity ofwords w1 and w2 computed as the cosine similar-ity of their LSA vectors, and ic is the informationcontent given by (2).
The overall similarity be-tween two texts is defined as the sum of weightedpair similarities, normalized by the length of thelonger text:alo(T1, T2) =?
(w1,w2)?P sim(w1, w2)max(length(T1), length(T2))(7)where P is the set of aligned lemma pairs.
A sim-ilar measure is proposed by Lavie and Denkowski(2009) for machine translation evaluation, and hasbeen found out to work well for semantic textualsimilarity (?aric?
et al 2012).4.3 Question type classification (QC)Related work on QA (Lytinen and Tomuro, 2002)shows that the accuracy of QA systems can be im-proved by question type classification.
The intu-ition behind this is that different types of ques-tions demand different types of answers.
Conse-quently, information about the type of answer re-quired should be beneficial as a feature.To explore this line of improvement, we traina simple question classifier on a dataset fromLombarovic?
et al(2011).
The dataset consistsof 1300 questions in Croatian, classified into sixclasses: numeric, entity, human, description, lo-cation, and abbreviation.
Following Lombarovic?et al(2011), we use document frequency to selectthe most frequent 300 words and 600 bigrams touse as features.
An SVM trained on this datasetachieves 80.16% accuracy in a five-fold cross-validation.
This is slightly worse than the best re-sult from Lombarovic?
et al(2011), however weuse a smaller set of lexical features.
We use thequestion type classifier to compute two features:the question type of the query and the questiontype of FAQ question.28Feature RM1 RM2 RM3 RM4 RM5NGO + + + + +ICNGO + + + + +TFIDF ?
+ + + +LSA ?
?
+ + +ICLSA ?
?
+ + +ALO ?
?
+ + +QED ?
?
?
+ +QC ?
?
?
?
+Table 4: Features used by our models4.4 Query expansion dictionary (QED)Our error analysis revealed that some false nega-tives could easily be eliminated by expanding thequery with similar/related words.
To this end, weconstructed a small, domain-specific query expan-sion dictionary.
We aimed to (1) mitigate minorspelling variances, (2) make the high similarity ofsome some cross-POS or domain-specific wordsexplicit, and (3) introduce a rudimentary ?worldknowledge?
useful for the domain at hand.
The fi-nal dictionary contains 53 entries; Table 3 showssome examples.5 Evaluation5.1 Experimental setupBecause our retrieval model is supervised, weevaluate it using five-fold cross-validation on theFAQ test collection.
In each fold we train our sys-tem on the training data as described in Section4, and evaluate the retrieval performance on thequeries from the test set.
While each (q, Frel) oc-curs in the test set exactly once, the same FAQ mayoccur in both the train and test set.
Note that thisdoes not pose a problem because the query part ofthe pair will differ (due to paraphrasing).To gain a better understanding of which featurescontribute the most to retrieval performance, wecreated several models.
The models use increas-ingly complex feature sets; an overview is givenin Table 4.
We leave exhaustive feature analysisand selection for future work.As a baseline to compare against, we use a stan-dard tf-idf weighted retrieval model.
This modelranks the FAQs by the cosine similarity of tf-idfweighted vectors representing the query and theFAQ.
When computing the vector of the FAQ pair,the question, answer, and category name are con-catenated into a single text unit.Model P R F1RM1 14.1 68.5 23.1RM2 25.8 75.1 37.8RM3 24.4 75.4 36.3RM4 25.7 77.7 38.2RM5 25.3 76.8 37.2Table 5: Classification results5.2 ResultsRelevance classification performance.
Recallthat we use a binary classifier as a retrieval model.The performance of this classifier directly deter-mines the performance of the retrieval system as awhole.
It is therefore interesting to evaluate clas-sifier performance separately.
To generate the testset, in each of the five folds we sample from thetest set the query-FAQ instances using the proce-dure described in Section 4 (N positive and 2Nnegative instance).Precision, recall, and F1-score for each modelare shown in Table 5.
Model RM4 outperformsthe other considered models.
Model RM5, whichadditionally uses question type classification, per-forms worse than RM4, suggesting that the ac-curacy of question type classification is not suf-ficiently high.
Our analysis of the test collectionrevealed that this can be attributed to a domainmismatch: the questions (mobile phone opera-tor FAQ) are considerably different than those onwhich the question classifier was trained (factoidgeneral questions).
Moreover, some of the queriesand questions in our FAQ test collection are notquestions at all (cf.
Table 2); e.g., ?Popravak mo-bitela.?
(?Mobile phone repair.?).
Consequently,it is not surprising that question classification fea-tures do not improve the performance.Retrieval performance.
Retrieval results of thefive considered models are given in Table 6.
Wereport the standard IR evaluation measures: meanreciprocal rank (MRR), average precision (AP),and R-precision (RP).
The best performance wasobtained with RM4 model, which uses all featuresexcept the question type.
The best MRR resultof 0.479 (with standard deviation over five foldsof ?0.04) indicates that, on average, model RM4ranks the relevant answer among top two results.Performance of other models expectedly in-crease with the complexity of features used.
How-ever, RM5 is again an exception, performingworse than RM4 despite using additional question29Query word Expansion words Remarkface facebook A lexical mismatch that would often occurogranic?iti (to limit) ogranic?enje (limit) Cross POS similarity important in the domain explicitcijena (price) tro?ak (cost), ko?tati (to cost) Synonyms very often used in the domaininozemstvo (abroad) roaming (roaming) Introduces world knowledgeADSL internet Related words often used in the domainTable 3: Examples from query expansions dictionaryModel MRR MAP RPBaseline 0.341 21.77 15.28RM1 0.326 20.21 17.6RM2 0.423 28.78 24.37RM3 0.432 29.09 24.90RM4 0.479 33.42 28.74RM5 0.475 32.37 27.30Table 6: Retrieval resultstype features, for the reasons elaborated above.Expectedly, classification performance andretrieval performance are positively correlated(cf.
Tables 5 and 6).
A noteworthy case is RM4,which improves the F1-score by only 5% overRM3, yet improves IR measures by more than10%.
This suggest that, in addition to improvingthe classifier decisions, the QED boosts the confi-dence scores of already correct decisions.A caveat to the above analysis is the fact thatthe query expansion dictionary was constructedbase on the cross-validation result.
While onlya small amount of errors were corrected with thedictionary, this still makes models RM4 and RM5slightly biased to the given dataset.
An objectiveestimate of maximum performance on unseen datais probably somewhere between RM3 and RM4.5.3 Error analysisBy manual inspection of false positive and falsenegative errors, we have identified several char-acteristic cases that account for the majority ofhighly ranked irrelevant documents.Lexical interference.
While a query does havea significant lexical similarity with relevant FAQpairs, it also has (often accidental) lexical simi-larity with irrelevant FAQs.
Because the classifierappears to prefer lexical overlap, such irrelevantFAQs interfere with results by taking over some ofthe top ranked positions from relevant pairs.Lexical gap.
Some queries ask a very similarquestion to an existing FAQ from the database, butparaphrase it in such a way that almost no lexicaloverlap remains.
Even though the effect of this ispartly mitigated by our semantic vector space fea-tures, in extreme cases the relevant FAQs will beranked rather low.Semantic gap.
Taken to the extreme, a para-phrase can change a query to the extent that itnot only introduces a lexical gap, but also a se-mantic gap, whose bridging would require logi-cal inference and world knowledge.
An exam-ple of such query is ?Postoji li moguc?nost ko-ri?tenja Vip kartice u Australiji??
(?Is it possi-ble to use Vip sim card in Australia??).
The asso-ciated FAQ question is ?Kako mogu saznati pos-toji li GPRS/EDGE ili UMTS/HSDPA roaming uzemlji u koju putujem??
(?How can I find out ifthere is GPRS/EDGE or UMTS/SPA roaming inthe country to which I am going??
).Word matching errors.
In some cases wordswhich should match do not.
This is most oftenthe case when one of the words is missing fromthe morphological lexicon, and thus not lemma-tized.
A case in point is the word ?Facebook?, orits colloquial Croatian variants ?fejs?
and ?face?,along with their inflected forms.
Handling this isespecially important because a significant numberof FAQs from our dataset contain such words.
Anobvious solution would be to complement lemma-tization with stemming.5.4 Cutoff strategiesOur model outputs a list of all FAQs from thedatabase, ranked by relevance to the input query.As low-ranked FAQs are mostly not relevant, pre-senting the whole ranked list puts an unnecessaryburden on the user.
We therefore explored somestrategies for limiting the number of results.First N (FN).
This simply returns the N bestranked documents.Measure threshold criterion (MTC).
We definea threshold on FAQ relevance score, and re-30Figure 1: Recall vs. average number of documentsretrieved (for various cutoff strategies)turn only the FAQs for which the classifierconfidence is above a specified threshold.Cumulative threshold criterion (CTC).
We de-fine a threshold for cumulative relevancescore.
The top-ranked FAQs for which thesum of classifier confidences is below thethreshold are returned.Relative threshold criterion (RTC).
Returns allFAQs whose relevance is within the givenpercentage of the top-ranked FAQ relevance.A good cutoff strategy should on average re-turn a smaller number of documents, while still re-taining high recall.
To reflect this requirement wemeasure the recall vs. average number of retrieveddocuments (Fig.
1).
While there is no substantialdifference between the four strategies, MTC andRTC perform similarly and slightly better than FNand CTC.
As the number of documents increases,the differences between the different cutoff strate-gies diminish.5.5 Performance and scalabilityWe have implemented the FAQ engine using in-house code in Java.
The only external library usedis the Java version of LIBSVM.
Regarding systemperformance, the main bottleneck is in generatingthe features.
Since all features depend on the userquery, they cannot be precomputed.
Computation-ally most intensive feature is ALO (cf.
Section4.2), which requires computing a large number ofvector cosines.The response time of our FAQ engine is accept-able ?
on our 1222 FAQs test collection, the re-sults are retrieved within one second.
However, toretrieve the results, the engine must generate fea-tures and apply a classifier to every FAQ from thedatabase.
This makes the response time linearlydependent on the number of FAQs.
For largerdatabases, a preprocessing step to narrow downthe scope of the search would be required.
To thisend, we could use a standard keyword-based re-trieval engine, optimized for high recall.
Unfortu-nately, improving efficiency by precomputing thefeatures is impossible because it would require thequery to be known in advance.6 Conclusion and PerspectivesWe have described a FAQ retrieval engine forCroatian.
The engine uses a supervised retrievalmodel trained on a FAQ test collection with bi-nary relevance judgments.
To bridge the notoriouslexical gap problem, we have employed a series offeatures based on semantic textual similarity be-tween the query and the FAQ.
We have built a FAQtest collection on which we have trained and evalu-ated the model.
On this test collection, our modelachieves a very good performance with an MRRscore of 0.47.We discussed a number of open problems.
Er-ror analysis suggests that our models prefer thelexical overlap features.
Consequently, most er-rors are caused by deceivingly high or low wordoverlap.
One way to address the former is to con-sider not only words themselves, but also syntacticstructures.
A simple way to do this is to use POSpatterns to detect similar syntactic structures.
Amore sophisticated version could make use of de-pendency relations obtained by syntactic parsing.We have demonstrated that even a small,domain-specific query expansion dictionary canprovide a considerable performance boost.
An-other venue of research could consider the auto-matic methods for constructing a domain-specificquery expansion dictionary.
As noted by a re-viewer, one possibility would be to mine querylogs collected over a longer period of time, as em-ployed in web search (Cui et al 2002) and alsoFAQ retrieval (Kim and Seo, 2006).From a practical perspective, future work shallfocus on scaling up the system to large FAQdatabases and multi-user environments.31AcknowledgmentsThis work has been supported by the Ministry ofScience, Education and Sports, Republic of Croa-tia under the Grant 036-1300646-1986.
We thankthe reviewers for their constructive comments.ReferencesEneko Agirre, Mona Diab, Daniel Cer, and AitorGonzalez-Agirre.
2012.
SemEval-2012 task 6: Apilot on semantic textual similarity.
In Proceedingsof the First Joint Conference on Lexical and Compu-tational Semantics, pages 385?393.
Association forComputational Linguistics.Robin D. Burke, Kristian J. Hammond, Vladimir Ku-lyukin, Steven L. Lytinen, Noriko Tomuro, and ScottSchoenberg.
1997.
Question answering from fre-quently asked question files: experiences with theFAQ Finder system.
AI magazine, 18(2):57.Chih-Chung Chang and Chih-Jen Lin.
2011.
LIB-SVM: A library for support vector machines.
ACMTransactions on Intelligent Systems and Technol-ogy, 2:27:1?27:27.
Software available at http://www.csie.ntu.edu.tw/~cjlin/libsvm.Danish Contractor, Govind Kothari, Tanveer A.Faruquie, L. Venkata Subramaniam, and SumitNegi.
2010.
Handling noisy queries in cross lan-guage FAQ retrieval.
In Proceedings of the EMNLP2010, pages 87?96.
Association for ComputationalLinguistics.Hang Cui, Ji-Rong Wen, Jian-Yun Nie, and Wei-YingMa.
2002.
Probabilistic query expansion usingquery logs.
In Proceedings of the 11th interna-tional conference on World Wide Web, pages 325?332.
ACM.Scott Deerwester, Susan T. Dumais, George W. Furnas,Thomas K. Landauer, and Richard Harshman.
1990.Indexing by latent semantic analysis.
Journal of theAmerican society for information science, 41(6).Mladen Karan, Jan ?najder, and Bojana Dalbelo Ba?ic?.2012.
Distributional semantics approach to detect-ing synonyms in Croatian language.
In InformationSociety 2012 - Eighth Language Technologies Con-ference, pages 111?116.Jaana Kek?l?inen.
2005.
Binary and graded rele-vance in IR evaluations?comparison of the effectson ranking of IR systems.
Information processing &management, 41(5):1019?1033.Harksoo Kim and Jungyun Seo.
2006.
High-performance FAQ retrieval using an automatic clus-tering method of query logs.
Information processing& management, 42(3):650?661.Govind Kothari, Sumit Negi, Tanveer A. Faruquie,Venkatesan T. Chakaravarthy, and L. Venkata Sub-ramaniam.
2009.
SMS based interface for FAQ re-trieval.
In Proceedings of the Joint Conference ofthe 47th Annual Meeting of the ACL and the 4th In-ternational Joint Conference on Natural LanguageProcessing of the AFNLP, volume 2, pages 852?860.Alon Lavie and Michael J. Denkowski.
2009.
The ME-TEOR metric for automatic evaluation of machinetranslation.
Machine translation, 23(2-3):105?115.Tie-Yan Liu.
2009.
Learning to rank for informationretrieval.
Foundations and Trends in InformationRetrieval, 3(3):225?331.Nikola Ljube?ic?
and Toma?
Erjavec.
2011.
HrWaCand SlWaC: compiling web corpora for Croatian andSlovene.
In Text, Speech and Dialogue, pages 395?402.
Springer.Tomislav Lombarovic?, Jan ?najder, and Bojana Dal-belo Ba?ic?.
2011.
Question classification for aCroatian QA system.
In Text, Speech and Dialogue,pages 403?410.
Springer.Steven Lytinen and Noriko Tomuro.
2002.
The useof question types to match questions in FAQ Finder.In AAAI Spring Symposium on Mining Answers fromTexts and Knowledge Bases, pages 46?53.Jean-Baptiste Michel, Yuan Kui Shen, Aviva P. Aiden,Adrian Veres, Matthew K. Gray, et al2011.
Quan-titative analysis of culture using millions of digitizedbooks.
Science, 331(6014):176.George A. Miller.
1995.
WordNet: a lexicaldatabase for English.
Communications of the ACM,38(11):39?41.Jeff Mitchell and Mirella Lapata.
2008.
Vector-basedmodels of semantic composition.
Proceedings ofACL-08: HLT, pages 236?244.Alejandro Moreo, Maria Navarro, Juan L. Castro, andJose M. Zurita.
2012.
A high-performance FAQ re-trieval method using minimal differentiator expres-sions.
Knowledge-Based Systems.Roberto Navigli and Paola Velardi.
2003.
An anal-ysis of ontology-based query expansion strategies.In Proceedings of the 14th European Conferenceon Machine Learning, Workshop on Adaptive TextExtraction and Mining, Cavtat-Dubrovnik, Croatia,pages 42?49.Tao Qin, Tie-Yan Liu, Jun Xu, and Hang Li.
2008.How to make letor more useful and reliable.
In Pro-ceedings of the ACM Special Interest Group on In-formation Retrieval 2008 Workshop on Learning toRank for Information Retrieval, pages 52?58.Philip Resnik.
1995.
Using information content toevaluate semantic similarity in a taxonomy.
In eprintarXiv: cmp-lg/9511007, volume 1, page 11007.32Frane ?aric?, Goran Glava?, Mladen Karan, Jan ?na-jder, and Bojana Dalbelo Ba?ic?.
2012.
TakeLab:systems for measuring semantic text similarity.
InProceedings of the First Joint Conference on Lexi-cal and Computational Semantics, pages 441?448.Association for Computational Linguistics.Jan ?najder, Bojana Dalbelo Ba?ic?, and Marko Tadic?.2008.
Automatic acquisition of inflectional lexicafor morphological normalisation.
Information Pro-cessing & Management, 44(5).Eriks Sneiders.
1999.
Automated FAQ answering:continued experience with shallow language under-standing.
In Question Answering Systems.
Papersfrom the 1999 AAAI Fall Symposium, pages 97?107.Radu Soricut and Eric Brill.
2004.
Automatic questionanswering: beyond the factoid.
In Proceedings ofHLT-NAACL, volume 5764.Noriko Tomuro and Steven Lytinen.
2004.
Retrievalmodels and Q and A learning with FAQ files.
NewDirections in Question Answering, pages 183?194.Ellen M. Voorhees and Dawn M. Tice.
2000.
Buildinga question answering test collection.
In Proceedingsof the 23rd annual international ACM SIGIR confer-ence on Research and development in informationretrieval, pages 200?207.
ACM.Ellen M. Voorhees.
1994.
Query expansion usinglexical-semantic relations.
In SIGIR?94, pages 61?69.
Springer.Ellen M. Voorhees.
2002.
The philosophy of infor-mation retrieval evaluation.
In Evaluation of cross-language information retrieval systems, pages 355?370.
Springer.Chung-Hsien Wu, Jui-Feng Yeh, and Yu-Sheng Lai.2006.
Semantic segment extraction and matchingfor internet FAQ retrieval.
Knowledge and Data En-gineering, IEEE Transactions on, 18(7):930?940.Xiaobing Xue, Jiwoon Jeon, and W Bruce Croft.
2008.Retrieval models for question and answer archives.In Proceedings of the 31st annual internationalACM SIGIR conference on Research and develop-ment in information retrieval, pages 475?482.
ACM.33
