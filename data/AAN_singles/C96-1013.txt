Concept  c luster ing and knowledge integrat ion from achi ldren's dict ionaryCaro l ine  Bar r i ;~re  an( I  I ,?ed  l ' opowichSchool  of (k )nqmt ing  S(:iencc, Si \]n(m l,'rascr U l l ivcrs i tyl~urnaby,  l~C, ( ;n.nada, V5A 1S6ba.rric're,l)Ol)OWi(;ll(@(:s.stTu.
(;aAbst ractKnowledge structures called Conce l ) t(?lustering Knowledge (\]raphs (CCKGs)are introduced along with a process fortheir construction from a machine read-able dictionary.
C(3K(\]s contain multi-ple concepts interrelated through multi-l)le semantic relations together forminga semantic duster represented by a con-.ceptual graph.
'1'he knowledge acquisi-tion is performed on a children's first dic-tionary.
The concepts inw)lved are gen-eral and typical of a daily l id conw'a'sa-lion.
A collection of conceptual clusterstogether can lbrm the basis of a lexi-cal knowledge base, where each C'(,'l((.~contains a limited nnmber of highly con-nected words giving usefid informationabout a particular domain or situation.it IntroductionWhen constructing a l,exieal Knowledge Ilase(1,KB) useful for Natural l,anguage Processing,the source of information from which knowledgeis acquired and the structuring of this informa-tion within the LKB are two key issues.
Ma-chine Readable Dictionaries (MIH)s) are a goodsour(:e of lexical information and have been shownto be al)plical)le to the task of I,KII COllStruction(l)ola.n ct al., 1993; Calzolari, t992; Copestake,\[990; Wilks et al, 1989; Byrd et al, 1987).
Oftenthough, a localist approaeh is adopted wherebythe words are kept in alphabetical order with somerepresentation of their definitions in the form ofa template or feature structure.
F, flbrt in find-lug cormections between words is seen in work onautomatic extraction of sem~mtic relations DoraMRI)s (Ahlswede and Evens, 1988; Alshawi, 1989;Montemagrfi and Vandorwende, 19!32).
Addition-ally, effort in finding words that are close seman-tically is seen by the current interest in statisti-cal techniques for word clustering, looking at (-o-occurrences of words in text corpora or dictionar-ies (Church and IIanks, 1989; Wilks et al, 1989;Brown et al, 11992; l'ereira et al, 11995).Inspired by research in the.
areas of semanticrelations, semantic distance, concept clustering,and using ( ,once  I tual (Ji a l hs (Sowa, 1984) as ourknowledge representation, we introduce (;oncept(?lustering I{nowledge Graphs (CCKGs).
Each(JCKG will start as a Conceptual Graph represen-tation of a trigger word and will expaud followinga search algorit, hm to incorporate related wordsand ibrm a C'oncept Cn,s(,er.
The concept chls-tcr in itself is interesting for tasks such as worddisambiguation, but the C(~K(\] will give more tothat cluster.
It will give the relations between thewords, making the graph in some aspects similarto a script (Schank and Abelson, 11975).
llowever,a CCK(I is generated automaticMly and does notrely on prin,itives but on an unlimited number ofconcel ) ts  , showing objects, persons, and actionsinteracting with each other.
This interaction willbe set, within a lmrtieular domain, and the trig-ger word should be a key word of the domain torepresent.
11' that process would be done for thewhole dictionary, we would obtain an l,l( II dividedinto multiple clusters of words, each representedby a CCK(\].
Then during text processing fin: ex-ample, a portion of text could be analyzed usingthe appropriate CCK(\] to lind implicit relationsand hell) understanding the text.Our source of knowledge is the Americ~m iter-itage First I)ictionary t which contains 1800 en-tries aml is designed for children of age six to eight.lit is made for yom~g l)eople learning the structureand the basic w)cabulary of their language.
Incomparison, an adult's dictiouary is more of a reference tool which assumes knowledge of a largebasic vocabulary, while a learner's dictionary as-sumes at limited vocabulary but still some verysophisticated concepts.
Using a children's dictio-nary allows us to restrict our vocabulary, but stillwork on general knowledge about day to day (:Oil--cel)tS and actions.In the folk)wing sections, we first present thel Copyright @1994 by \[Ioughton Miftlin Company.Reproduced by permission h'om TIlE AMERICANItERITAGI'; FIRST DIC'I?IONAIlY.55transformation steps from the definitions into con-ceptual graphs, then we elaborate on the integra-tion process, and finally, we close with a discus-sion.2 Transforming definitionsOur definitions may contain up to three generaltypes of information, as shown in the examples inFigure 1.?
description: This contains genus/differentiainformation.
Such information is frequentlyused for noun taxonomy construction (Byrd etal., 1987; Klavans et al, 1990; Barri~re andPopowich, To appear August 1996).?
general knowledge or usage: This gives in-formation useflfl in daily life, like how to use anobject, what it is made of, what it looks llke, etc.?
specific example: This presents a typical situ-ation using the word defined and it involves spe-cific persons and actions.Cereal is a kind of food.
\[description\]Many cereals are made from corn, wheat, or rice.
\[usage\]Most people eat cereal with milk in a bowl.
\[usage\]Asia is what is left after something burns.
\[usage\]It is a soft gray powder.
\[description\]Ray watched his father clean the ashes out of the fireplace.\[example\]Figure 1: Example of definitionsThe information given by the description andgeneral knowledge will be used to perform theknowledge integration proposed in section 3.
Thespecific examples are excluded as they tend to in-volve specific concepts not always deeply relatedto the word defined.Our processing of the definitions results in theconstruction of a special type of conceptual graphwhich we call a temporary graph.
The set of rela-tions used in temporary graphs come from threesources.
Table 1 shows some examples for eachtype.1.
the set of closed class words, ex: of, to, in, and;2. relations extracted via defining formulas ex: part-of, made-of, instrument; defining formulas cor-respond to phrasal patterns that occur oftenthrough the dictionary suggesting particular se-mantic relations (ix.
A is a part of B) (Ahlswedeand Evens, 1988; Dolan et al, 1993).3. the relations that are extracted from the syntac-tic structure of a sentence, ex: subject, object,goal, attribute, modifier.As some relations are defined using the closedclass words, and many of those words are ambigu-ous, the resulting graph will itself be ambiguous.This is the main reason for calling our graphstemporary as we assume a conceptual graph, theultimate goal of our translation process, shouldcontain a restricted set of well-defined and non-ambiguous emantic relations.
For example, bycan be a relation of manner (by chewing), time(by noon) or place (by the door).
By keepingthe preposition itself within the temporary graph,we delay the ambiguity resolution process untilwe have gathered more information and we evenhopefully avoid the decision process as the ambi-guity might later be resolved by the integrationprocess itself.1.
closed class words temporary I~raphnp:np\[A\],prep\[Bl,np\[C \] \[A\]->(B)->\[C\]apple on the table \[apple\]->(on)->\[table\]2. defining formulasAis used to BAis a part of BA is a place where B3.
syntactic patterns:np\[A\],vp\[B\]John eatsvp:vp\[A\],inf_vp\[B\]eat to growtemporary graph\[B\]- >(instrument )- > \[A\]\[A\]-> (part-of)-> \[B\]\[Bl->(loc)->\[A\]temporary graph\[B\]-> (agent)-> \[h\]\[eat\]- > (agent)- >\[John\]\[A\]-> (goal)-> \[B\]\[e at\]- > ( goal)- > \[grow\]Table 1: Examples of relations found in sentencesand their corresponding temporary graphs3 Knowledge integrationThis section describes how given a trigger word,we perform a series of forward and backwardsearches in the dictionary to build a CCKG con-taining useful information pertaining to the trig-ger word and to closely related words.
The pri-mary building blocks for the CCKG are the tem-porary graphs built from the dictionary definitionsof those words using our transformation processmentioned in the previous section.
Those tem-porary graphs express similar or related ideas indifferent ways and with different levels of detail.As we will try to put all this information togetherinto one large graph, we must first find what in-formation the various temporary graphs have incommon and then join them around this commonknowledge.To help us build this CCKG and perform ourintegration process, we assume two main knowl-edge structures are available, a concept hierarchyand a relation hierarchy, and we assume the exis-tance of some graph operations.
The concept hi-erarchy concentrates on nouns and verbs as theyaccount for three quarters of the dictionary def-initions.
It has been constructed automaticallyaccording to the techniques described in (Barri~reand Popowich, To appear August 1996).
The re-lation hierarchy was constructed manually.
A richhierarchical structure between the set of relationsis essential to the graph matching operations weuse for the integration phase.As we are using the conceptual graph formalismto represent our definitions, we can use the graph66matching operations defined in (Sowa, 1984).
Thet, wo operations we will need are the maximal com-mon subgraph algorithm and the maximal join al-gorithm.3.1.
Max imal  common subgraphThe maximal common subgraph between twographs consists of finding a subgraph of tile firstgraph that is isomorphic to a subgraph of the see-ond graph.
In our case, we cannot often expect tofind two graphs that contain an identical subgral)hwith the exact same relations and concepts.
Ideascart be expressed in many ways and we thereforeneed a more relaxed matching schema.
We de-scribe a few elements of this "relaxation" processand illustrate them by an example in Figure 2.
(1) John makes a nice drawing on a piece of paper with the pen.\[make\]- >(sub)-  >\[ John\]- >(obj) -  >\[drawing\]- >(nit ) -  >\[nice\]- >(on)- >\[piece\]- >(or)- >\[paper\]->(with) -  >\[pen\](2) John uses the big crayon to  draw rapidly on the paper.\[(haw\]- >(sub)- >\[ John l->(on)->\[paper\]- >(inst .
.
.
.
.
.
.
.
t)- > \[crayon\]- >(manner ) -  >\[rapidly\]MAXIMAl ,  COMMON SUBGRAPn:\[make(draw)\]-  >(sub)-  >\[ John\]- >(obj ) -  >\[drawing\]- >(on)->\[piece\]- >(of)->\[paper\]- >( inst rument) -  >\[label-11~ methodl~elation subsumptionPredictable meaning Mdftnelat ion t r~nsitivitygraphl  \] graph2pell cr~yollwith instrumentdrawing dr~wpiece of paper  p~perMAXIMAL JOIN:\[make(draw)\]-  >(sub)-  >\[ John\]- >(obj) -  > \[drawing\]- >(art ) -  >\[nice\]- >(o. )
-  > \[piece\]- >(of)- >\[paperl->( inst  .
.
.
.
.
.
t)->\[l~b?l-1\]->( .
.
.
.
.
.
.
.
.
)->\[rapidly\]Figure 2: Example of "relaxed" maximal commonsui)graph and maximal join algorithmsSemant ic  d is tance  between concepts .
Inthe maximal common subgraph algorithm pro-posed by (Sow% :1984), two concepts (C1,CY)could be matched if one snbsumed the other inthe concept hierarchy.
We can relax that criteriato match two concepts when a third concept Cwhich subsumes C1 and C2 has a high enough de-gree of informativeness (Resnik, 1995).
The con-cept hierarchy can be useful in many cases, but itis generated from the dictionary and might not becomplete nough to find all similar concepts.In the example of Figure 2, when using tile con-cept hierarchy to establish the similarity betweenpen and crayon, we find that; one is a subclassof lool and the other of wax, both then are sub-stoned by the general concept something.
We havereached the root of the noun tree in the concept hi-erarchy and this would give a similarity of 0 basedon the informativeness notion.We extend the subsumption notion to thegraphs.
Iustead of finding a concept that sub-sulnes two concepts, we will try finding a commonsubgraph that subsumes the graph representationof both concepts.
In our example, pen and crayonhave a common subgraph \[write\]->(inst)->~.
Thenotion of semantic distance can be seen as the in-formativeness of the subsuming graph.
The re-suiting maximal comlnon snbgraph as shown inFigure 2 contains the concept label-1.
This labelis associated to a covert category ~s presented in(Barri~re and Popowich, To appear August 1996).We carl update tile concept hierarchy and add thislabel-1 as a subclass of something and a superclassof pen and crayon.
It expresses a concept of "writ-ing instrument".tRe la t ion  subsmnpt ion .
Since we have a re-lation hierarchy in addition to our concept hier-archy, we can similarly use subsumption to matchtwo relations.
In i,'igure 2, with is subsumed by in-strument, and by lnapping them, we disantbiguatewilh from corresponding to another semantic rela-tion, such as possession or accompaniment.
Thisis a case where an arnbiguons preposition left inthe temporary graph is resolved by the integrationprocess.P red ic tab le  mean ing  shift .
A set of lexicalimplication rules were developed by (Ostler andAtkins, 1992) for relating word senses.
Based onthem, we are developing a set of graph match-ing rules.
Figure 2 exemplifies one of theln wheretwo graphs containing the same word (or morpho-logically related), here draw and drawing, used asdifferent parts of speech can be related.Re la t ion  t rans i t i v i ty .
Some relations, likepart-of, in, from can be transitive.
For example,we can map a graph that contains a concept A ina certain relation to concept B onto another graphwhere concept A is in the same relation with a partor a piece of B as exemplified in Figure 2.
Tran-sitivity in relations is in itself a challenging areaof study (Cruse, 1986) and we have only begun toexplore it.3.2 Max imal  jo inThe basic operation for the integration of tempo-rary graphs is the maximal join operation where aunion of two graphs is formed around their max-imal common subgraph using the most specificconcepts of each.
We just saw how to relax themaximal common subgraph operation and we willperform the join around that "relaxed" subgraph.Figure 2 shows the result of the maximal join.The join operation allows us to bring new con-ccpts into a graph by finding relations with ex-57isting concepts, as well as bringing new relationsbetween existing concepts.3.3 Integrat ion processGiven the concept hierarchy, relation hierarchyand graph matching operations, we now describethe two major steps required to integrate all thetemporary graphs into a CCKG.TR IGGER.
PHASE.
Start with a centralword, a keyword for the subject of interest thatbecomes the trigger word.
The temporary graphbuilt from the trigger word forms the initialCCKG.
To expand its meaning, we want to lookat the important concepts involved and use theirrespective temporary graphs to extend our initialgraph.
We deem words in the definition to be im-portant if they have a large semantic weight.2.
'he semantic weight of a word or its informa-tiveness can be related to its frequency (l~esnik,1995).
Itere, we calculate the number of occur-rence of each word within the definitions of nounsand verbs in our dictionary.
The most frequentword "a" occurs 2600 times among a total of 38000word occurrences.
Only 1% of the words occurmore than 130 times, 5% occur more than 30 timesbut over 60% occur less than 5 times.Ordering the dictionary words in terms of de-creasing number of occurrences, the top 10% ofthese words account for 75% of word occurrences.For our current investigation, we propose thisas the division between semantically significantwords, and semantically insignificant ones.
So aword from the dictionary is deemed to be seman-tically significant if it occurs less than 17 times.Note that constraining the number of semanti-cally significant words is important in limiting theexploration process tbr constructing the conceptcluster, as we shall soon see.T r igger  fo rward :  Find the semantically signif-icant words fi'om the CCKG, and join theirrespective temporary graphs to the initialCCKG.T r igger  backward :  Find all the words in thedictionary that use the trigger word in theirdefinition and join their respective temporarygraphs to the CCKG.Instead of a single trigger word, we now havea cluster of words that are related through theCCKG.
Those words ,form the concept cluster.EXPANSION PHASE.
We try finding wordsin the dictionary containing many concepts iden-tical to the ones already present in the CCKG butperhaps interacting through different relations al-lowing us to create additional links within the setof concepts present in the CCKG.
Our goal is tocreate a more interconnected graph rather thansprouting from a particular concept.
For this rea-son, we establish a graph matching threshold todecide whether we will join a new graph to theCCKG being built.
We set this threshold empir-ically: the maximal common subgraph betweenthe CCKG and the new temporary graph mustcontain at least three concepts connected throughtwo relations.Expans ion  fo rward :  For each semanticallysignificant word in the CCKG, not alreadypart of the concept cluster, find the maxi-mal common subgraph between its temporarygraph and the CCKG.
If matching surpassesthe graph matching threshold, perform inte-gration (maximal join operation) and add theword in the concept cluster.
Continue for-ward until no changes are made.Expans ion  backward :  Find words in the dic-tionary whose definitions contain the seman-tically significant words from the conceptcluster.
For each possible new word, findthe maximal common subgraph between itstemporary graph and the CCKG.
Again, ifmatching is over the graph matching thresh-old, perform integration and add the wordto the concept cluster.
Continue until nochanges are made.We can set a limit to the number of steps in theexpansion phase to ensure its termination.
Ilow-ever in practice, M'ter two or three steps forwardor backward, the maximal common subgraphs be-tween the new graphs and CCKG do not exceedthe graph matching threshold and thus are notadded to the cluster, terminating the expansion.3.4 Example  of  in tegrat ionFigure 3 shows the starting point of an integra-tion process with the trigger word (TW) lelter, itsdefinition, its temporary graph (TG), the conceptcluster (CC) containing only the trigger word,and the CCKG being the same as the temporarygraph.
Then we show the trigger forward phase.The number of occurences (NOte) of each wordpresent in the definition of letter is given.
Us-ing the criteria described in the previous section,only the word message is a semantically significantword (SSW).
We then see the definition of mes-sage, the new concept cluster and the resultingCCKG.The trigger backward phase, would incorporatethe temporary graphs for address, mail, post officeand stamp.
The expansion forward phase wouldfurther add the temporary graphs for the seman-tically significant words: {send, package} dur-ing the first step and then would terminate withthe second step as no more semantically signifi-cant words not yet explored have a maximal com-mon subgraph with the CCKG that exceeds thegraph matching threshold.
The expansion back-ward would finally add the temporary graphs forcard and note, again terminating after two steps.63The resulting cluster is: {letter, message, ad-dress, mail, post office, stamp, send, package,card, note}.
The resulting CCKG shows the in-teraction between those concepts which smnma-rizes general knowledge about lnow we use thoseconcepts together in a da.ily conversation: we goto the post office to mail letters, or packages; wewrite letters, notes and cards to send to peoI)lethrough the mail, etc.
Ilaving such clusters andsuch knowledge of the relationship between wordsas part of our lexical knowledge base can be useflflto understand or even generate a text containingthe concepts involved in the cluster.S'I 'Al l . '
I ' ING POIN 'F :TW:  le t te rDef :  A le t te r  is a message  you wr i te  on paper ,TG:  same as CCKGCO:  {letter}CCKG:  \[write\]- > (obj)- > \[message(let t r)\]- > (sub j)-  > \[per .
.
.
.
:you\]- > (on)- > \[>q,e,'lT l l .
IGG EtI~ I!
'O I tWA I/.D :NOtes :  you:280,  paper :42 ,  wr i te:31,message:7SSWs:  messagel_)ef: A ln ( ' .
ssage is a g roup  of words  that  is sentl ' l 'Olll ()lie person  to ;~.
I lothel ' .Many people  send nmssages  through the  mail .
( ;C :  { letter ,  message}CCKG:\ [word :group(message( le t te r ) ) \ ]<@~bj) <-\[write\]- > (sub)- > \[person:you\]- > (o,)- > \[l,~per\]<-(obj)<-\[ ..... 11->(.~.bj)->\[pe~" ............ y\]- > ( f ro l / l ) -  > \[pe .
.
.
.
.
.
.
.
.
.
.
\]- > (to)-  > \[I .
.
.
.
.
.
.
.
.
.
.
.
.
.
ther\]-> (through)->\[mail IFignre 3: iDigger forward from letter.4 Discuss ion'l'lu:ough this paper, we showed the multiple stepsleading us to tile building of Concept ClusteringKnowledge Graphs (CCKGs).
Those knowledgestructm:es arc built within the Lexical Knowl-edge Base (LKB), integrating lnultiple parts of theI,Kt~ around a particular concept o form a clus.-ter and express the multiple relations among thewords in that cluster.
The CCKGs could be eitherpermanent or temporary structures depending onthe.
applicatkm using the LKB.
For example, for atext understanding tusk, we can build before handthe CCKGs corresponding to one or multiple key-words from the text.
Once built, the CCKGs willhelp us in our comprehension a d disambiguationof the text.By using the American lh;ritage First l)ictio-nary a~s our source of lexical information, we wereable to restrict our vocabulary to result ill aproject of reasonable size, dealing with generalknowledge about (lay to day concepts and actions.The ideas explored using this dictionary can be ex-tended to other dictionaries as well, but the taskmight becorne more complex as the defilfitions inadult's dictionaries are not as clear and usage ori-ented.
In fact, an LKB lmilt fl'om a children'sdictionary could be seen as a starting point fromwhich we could extend our acquisition of knowl-edge using text corpora or other dictionaries.
Cer-tainly, if we euvisage applications trying to under-stand children's tories or help in child education,a corpora of texts for children would be a goodsource of information to extend our LKB.The graph operations (maximM commou sub-graph and maximal join) defined on conceptualgraphs, anti adapted here, play an important rolein our integration process toward a final CCKG.Graph matching was also suggested as an alterna-tiw; to taxonomic search when trying to establishsemantic similarity between concepts.
As well, byputting a threshohl on the graph matching pro-cess, we were able to limit the expansion of ourclustering, as we can decide and justify the incor-poration of a new concept into a particular cluster.Many aspects of the concept clustering andknowledge integration processes have already beenimplemented and it will soon be possible to testthe techniques on different rigger words using dif-ferent thresholds to see how they effect the qualityof the clusters.
(~lustering is often seen as a statistical opera-tion that puts together words "somehow" related.ltere, we give a meaning to their clustering, wetint\[ and show the connections between concepts,and by doing so, we build more than a cluster oFwords.
We build a knowledge graph where theconcepts interact with each other giving impe ltaut implicit information that will be useful forNatural Language Processing tusks.5 Acknowledgmentsi\['his research was supported by the Institute forRobotics and Intelligent Systems.
The autlnorswould like to thank the anonymous referees fortheir comments and suggestions, and Petr Kubonfor his many comments on the paper.Re ferencesT.
Ahlswede and M. Evens.
1988.
Generating arelational lexicon from a machine-readable dic-tionary.
International JowrnM of Lexicography,1l(3):214 237.II.
A\]shawi.
1989.
Analysing tile dictionary def-inil.ions.
In 1~.
Boguraev and T. llriscoe, ed-itors, Compulalional Lexicography for NaturalLanguage Processing, chapter 7, pages 153-170.Long,nan (\]reap IlK l,imited.C.
Barri6re and F. Popowich.
To apl)ear, August1996.
Building a noun taxonomy from a chil-69dren's dictionary.
In Proceedings of Euralex'96,GSteborg, Sweden.P.
Brown, V.J.
Della Pietra, P.V.
deSouza, J.C.Lai, and I{.L.
Mercer.
1992.
Class-based n-grain models of natural language.
Computa-tional Linguistics, 18(4):467-480.R.J.
Byrd, N. Calzolari, M. Chodorow, J. Kla-vans, M. Neff, and O. Rizk.
1987.
Tools andmethods for computational lexieology.
Compu-tational Linguistics, 13(3-4):219-240.N.
Calzolari.
1992.
Acquiring and representing se-mantic information i a lexical knowledge base.In J. Pustejovsky and S. Bergler, editors, Lex-ical Semantics and Knowledge Representation: First SIGLEX Workshop, chapter 16, pages235-244.
Springer-Verlag.K.
Church and P. Hanks.
1989.
Word associa-tion norms, mutual information and lexicogra-phy.
In Proceedings of the 27lh Annual meetingof the Association for Computational Linguis-tics, pages 76-83, Vancouver, BC.A.A.
Copestake.
1990.
An approach to buildingthe hierarchical element of a lexical knowledgebase from a machine readable dictionary, inProceedings of the Workshop on Inheritance inNatural Language Processing, 7'ilburg.D.A.
Cruse.
1986.
Lexical Semantics.
CambridgeUniversity Press.W.
Dolan, L. Vanderwende, and S. D. Richard-son.
1993.
Automatically deriving structuredknowledge bases from on-line dictionaries.
InThe First Conference of the Pacific Associa-tion for Computational Linguistics, pages 5-14,IIarbour Center, Campus of SFU, Vancouver,April.J.
Klavans, M. S. Chodorow, and N. Wacholder.1990.
From dictionary to knowledge base viataxonomy.
In P~vceedings of the 6th AnnualConference of the UW Centre for the New OED:Electronic Text Research, pages 110-132.S.
Montemagni and L. Vanderwende.
1992.
Struc-tural patterns vs. string patterns for extract-ing semantic information from dictionaries.
InProc.
of the 14 o~ COLING, pages 546-552,Nantes, France.N.
Ostler and B.T.S.
Atkins.
1992.
Predictablemeaning shift: Some linguistic properties oflexical implication rules.
In J. Pustejovskyand S. Bergler, editors, Lexical Semantics andKnowledge Representation : First S\[GLEXWorkshop, chapter 7, pages 87-100.
Springer-Verlag.17.
Pereira, N. Tishby, and L. Lee.
1995.
Distri-butional clustering of english words.
In Proc.
ofthe 33 th A CL, Cambridge,MA.P.
Resnik.
1995.
Using information content oevaluate semantic similarity in a taxonomy.
InProc.
of the 14 th IJCAL volume 1, pages 448-453, Montreal, Canada.R.
Schank and FL.
Abelson.
1975.
Scripts, plansand knowledge.
In Advance papers 4th Intl.Joint Conf.
Artificial Intelligence.J.
Sowa.
1984.
Conceptual Structures in Mindand Machines.
Addison-Wesley.Y.
Wilks, D. Fass, G-M Guo, J. McDonald,T.
Plate, and B. Slator.
1989.
A tractable ma-chine dictionary as a resource for computationalsemantics.
In Bran Boguraev and Ted Briseoe,editors, Computational Lexicography for Natu-ral Language Processing, chapter 9, pages 193-231.
Longman Group UK Limited.70
