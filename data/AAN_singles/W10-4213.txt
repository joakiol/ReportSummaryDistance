Complex Lexico-Syntactic Reformulation of Sentences using TypedDependency RepresentationsAdvaith SiddharthanDepartment of Computing ScienceUniversity of Aberdeenadvaith@abdn.ac.ukAbstractWe present a framework for reformulat-ing sentences by applying transfer ruleson a typed dependency representation.
Wespecify a list of operations that the frame-work needs to support and argue thattyped dependency structures are currentlythe most suitable formalism for complexlexico-syntactic paraphrasing.
We demon-strate our approach by reformulating sen-tences expressing the discourse relation ofcausation using four lexico-syntactic dis-course markers ?
?cause?
as a verb andas a noun, ?because?
as a conjunction and?because of?
as a preposition.1 IntroductionThere are many reasons why a writer might wantto choose one formulation of a discourse relationover another; for example, maintaining thread ofdiscourse, avoiding shifts in focus and issues ofsalience and end weight.
There are also reasons touse different formulations for different audiences;for example, to account for differences in readingskills and domain knowledge.
In recent work, Sid-dharthan and Katsos (2010) demonstrated throughpsycholinguistic experiments that domain expertsand lay readers show significant differences inwhich formulations of causation they find accept-able.
They further showed that the most appropri-ate formulation depends both on the domain ex-pertise of the user and the propositional contentof the sentence, and that these preferences canbe learnt in a supervised machine learning frame-work.
That work, as does much of the relatedcomprehension and literacy literature, used man-ually reformulated sentences.
In this paper, wepresent an approach to automate such complex re-formulation.
We consider the four lexico-syntacticdiscourse markers for causation studied by Sid-dharthan and Katsos (2010); consider 1a.?d.
be-low (from their corpus, but simplified to aid pre-sentation):(1) a.
An incendiary device caused the explosion.[A-CAUSE-B]b.
The explosion occurred because of an incen-diary device.
[B-BECAUSEOF-A]c. The explosion occurred because there was anincendiary device.
[B-BECAUSE-A]d. The cause of the explosion was an incendiarydevice.
[CAUSEOF-B-A]These differ in terms of the lexico-syntactic prop-erties of the discourse marker (shown in boldfont).
Indeed the discourse markers here are verbs,prepositions, conjunctions and nouns.
As a conse-quence, the propositional content is expressed ei-ther as a clause or a noun phrase (?The explosionoccurred?
vs ?the explosion?, etc.).
Additionally,the order of presentation of propositional contentcan be varied to give four more lexico-syntacticparaphrases:(1) e. The explosion was caused by an incendiarydevice.
[B-CAUSEBY-A]f. Because of an incendiary device, the explo-sion occurred.
[BECAUSEOF-A-B]g. Because there was an incendiary device, theexplosion occurred.
[BECAUSE-A-B]h. An incendiary device was the cause of the ex-plosion.
[A-CAUSEOF-B]It is clear that some formulations of a givenpropositional content can be more felicitous thanothers; for example, 1e.
seems preferable to 1g.However, for different propositional content, otherformulations might be more felicitous.
While dis-course level choices based on information order-ing play a role in choosing a formulation, Sid-dharthan and Katsos (2010) demonstrate that somede-contextualised information orderings within asentence are deemed unacceptable by some cate-gories of readers.
This has implications for textregeneration tasks that try to reformulate texts fordifferent audiences; for instance, simplifying lan-guage for low reading ages or summarising tech-nical writing for lay readers.
In short, considera-tions of discourse coherence should not introducesentence-level unacceptability in regenerated text.We focus on causal relations for many reasons.For the purpose of this paper, our main reason isthat the 8 formulations selected are different in-formation orderings of 4 different lexico-syntacticconstructs.
Thus, we explore a broad range of con-structions and are confident that the framework wedevelop covers the range of operations required fortext regeneration in general.
Of less relevance tothis paper, but equally important to our broad goalsof reformulating technical writing for lay readers,causal relations are pervasive in science writingand are integral to how humans conceptualise theworld.
We have a particular interest in scientificwriting ?
reformulating such texts for lay audi-ences is a highly relevant task today and manynews agencies perform this service; e.g., ReutersHealth summarises medical literature for lay audi-ences and BBC online has a Science/Nature sec-tion that reports on science.
These services relyeither on press releases by scientists and universi-ties or on specialist scientific reporters, thus lim-iting coverage of a growing volume of scientificliterature in a digital economy.In Section 2, we relate our research to the exist-ing linguistic and computational literature.
Thenin Section 3, we compare three different linguisticrepresentations with respect to their suitability forlexico-syntactic reformulation.
We found typeddependency structures to be the most promisingand present an evaluation in Section 4.2 Related Work2.1 Discourse Connectives and ComprehensionPrevious work has shown that when texts havebeen manually rewritten to make the languagemore accessible (L?Allier, 1980), or to make thecontent more transparent (Beck et al, 1991), stu-dents?
reading comprehension shows significantimprovements.
An example of a revision choicethat might be applied differentially depending onthe literacy skills of the reader involves connec-tives such as because.
Connectives that permitpre-posed adverbial clauses have been found tobe difficult for third to fifth grade readers, evenwhen the order of mention coincides with thecausal (and temporal) order (Anderson and Davi-son, 1988); this experimental result is consistentwith the observed order of emergence of connec-tives in children?s narratives (Levy, 2003).Thus the b) version of the following examplewould be preferred for children who can graspcausation, but who have not yet become comfort-able with alternative clause orders (example fromAnderson and Davison (1988), p. 35):(2) a.
Because Mexico allowed slavery, many Amer-icans and their slaves moved to Mexico duringthat time.b.
Many Americans and their slaves moved toMexico during that time, because Mexico al-lowed slavery.Such studies show that comprehension can beimproved by reformulating text for readers withlow reading skills (Linderholm et al, 2000; Becket al, 1991) and for readers with low levels of do-main expertise (Noordman and Vonk, 1992).
Fur-ther, specific information orderings were found tobe facilitatory by Anderson and Davison (1988).All these studies suggest that the automatic lexico-syntactic reformulation of causation can benefitvarious categories of readers.2.2 Connectives and Text (Re)GenerationMuch of the work regarding (re)generation of textbased on discourse connectives aims to simplifytext in certain ways, to make it more accessibleto particular classes of readers.
The PSET project(Carroll et al, 1998) considered simplifying newsreports for aphasics.
The PSET project focusedmainly on lexical simplification (replacing diffi-cult words with easier ones), but there has beenwork on syntactic simplification and, in particu-lar, the way syntactic rewrites interact with dis-course structure and text cohesion (Siddharthan,2006).
These were restricted to string substitutionand sentence splitting based on pattern matchingover chunked text.
Our work aims to extend thesestrands of research by allowing for more sophis-ticated insertion, deletion and substitution oper-ations that can involve substantial reorganisationand modification of content within a sentence.Elsewhere, there has been interest in paraphras-ing, including the replacement of words (espe-cially verbs) with their dictionary definitions (Kajiet al, 2002) and the replacement of idiomatic orotherwise troublesome expressions with simplerones.
The emphasis has been on automaticallylearning paraphrases from comparable or alignedcorpora (Barzilay and Lee, 2003; Ibrahim et al,2003).
The text simplification and paraphrasingliterature does not address paraphrasing that re-quires syntactic alterations such as those in Exam-ple 1 or the question of appropriateness of differ-ent formulations of a discourse relation.Some natural language generation systems in-corporate results from psycholinguistic studies tomake principled choices between alternative for-mulations.
For example, SkillSum (Williams andReiter, 2008) and ICONOCLAST (Power et al,2003) are two contemporary generation systemsthat allow for specifying aspects of style such aschoice of discourse marker, clause order, repeti-tion and sentence and paragraph lengths in theform of constraints that can be optimised.
How-ever, to date, these systems do not consider syn-tactic reformulations of the type we are interestedin.
Our research is directly relevant to such gen-eration systems as it can help such systems makedecisions in a principled manner.Williams et al (2003) examined the impact ofdiscourse level choices on readability in the do-main of reporting the results of literacy assessmenttests, using the results of the test to control boththe content and the realisation of the generated re-port.
Our research aims to facilitate the transfer ofsuch user-driven generation research to text regen-eration areas.2.3 Sentence CompressionSentence compression is a research area that aimsto shorten sentences for the purpose of summaris-ing the main content.
There are similarities be-tween our interest in reformulation and existingwork in sentence compression.
Sentence com-pression has usually been addressed in a gener-ative framework, where transformation rules arelearnt from parsed corpora of sentences alignedwith manually compressed versions.
The com-pression rules learnt are therefore tree-tree trans-formations (Knight and Marcu, 2000; Galley andMcKeown, 2007; Riezler et al, 2003) of some va-riety.
These approaches focus on deletion oper-ations, mostly performed low down in the parsetree to remove modifiers.
Further they make as-sumptions about isomorphism between the alignedtree, which means they cannot be readily appliedto more complex reformulation operations suchas insertion and reordering that are essential toperform reformulations such as those in Example1.
Cohn and Lapata (2009) provide an approachbased on Synchronous Tree Substitution Grammar(STSG) that in principle can handle the range ofreformulation operations.
However, given their fo-cus on sentence compression, they restricted them-selves to local transformations near the bottom ofthe parse tree.
In this paper, we explore whetherthis framework could prove useful to more in-volved reformulation tasks.
Our experience (seeSection 3.2) suggests that parse trees are the wrongrepresentation for learning complex transforma-tion rules and that dependency structures are moresuited for complex lexico-syntactic reformulation.3 Regeneration using Transfer RulesWe experimented with three representations ?phrasal parse trees, typed dependencies and Min-imal Recursion Semantics (MRS).
In this section,we first describe our data, and then report our ex-perience with performing text reformulation usingthese representations.3.1 DataWe use the corpus described in Siddharthan andKatsos (2010).
This corpus contains examples ofcomplex lexico-syntactic reformulations such asthose in Example 1a?f; each example consists of8 formulations, 7 of which are manual reformu-lations.
The corpus contains 144 such examplesfrom three genres, giving 1152 sentences in to-tal.
The manual reformulation is formulaic andExample 1 is indicative of the process.
To makea clause out of a noun phrase, either the copula orthe verb ?occur?
is introduced, based on a subjec-tive judgement of whether this is an event or a con-tinuous phenomenon.
Conversely, to create a nounphrase from a clause, a possessive and gerund areused; for example (from Siddharthan and Katsos(2010)):(3) a. Irwin had triumphed because he was so gooda man.b.
The cause of Irwin?s having triumphed was hisbeing so good a man.The corpus contains equal numbers of sentencesfrom three different genres: PubMed Abstracts1(technical writing from the Biomedical domain),and articles from the British National Corpus2tagged as World News or Natural Science (popularscience writing in the mainstream media).3.2 Reformulation using Phrasal Parse TreesAs described above, we have access to a cor-pus that contains aligned sentences for each pairof types (a type is a combination of a discoursemarker and an information order; thus we have 8types).
In principle it should be easy to learn trans-fer rules between parse trees of aligned sentences.Figure 1 shows parse trees ( using the RASP parser(Briscoe et al, 2006)) for the active and the pas-sive voice with ?cause?
as a verb.
A transfer ruleis derived by aligning nodes between two parsetrees so that the rule only contains the differencesin structure between the trees.
In the represen-tation in Figure 1, the variable ?
?X0[NP] maps1PubMed URL: http://www.ncbi.nlm.nih.gov/pubmed/2The British National Corpus, version 3 (BNC XML Edi-tion).
2007. http://www.natcorp.ox.ac.ukThe explosion was caused by an incendiary device.
(S(NP (AT The) (NN1 explosion))(VP (VBDZ be+ed)(VP (VVN cause+ed)(PP (II by)(NP (AT1 an) (JJ incendiary) (NN1 device))))))An incendiary device caused the explosion.
(S(NP (AT1 An) (JJ incendiary) (NN1 device))(VP (VVD cause+ed)(NP (AT the) (NN1 explosion))))Derived Rule:(S(?
?X0[NP])(VP (VBZ be+s)(VP(VVN cause+ed) (PP(II by+) (??X1[NP])))))?(S(?
?X1[NP])(VP (VVZ cause+s) (?
?X0[NP])))Figure 1: Example of a transfer rule derived fromtwo parse trees.onto any node (subtree) with label NP.
RASP per-forms a morphological analysis of words (shownas lemma+suffix in the figure).
Thus such rulescan be used to account for changes in morphology,as in example 3a.?b.
above.In practise however, the parse tree representa-tion is too dependent on the grammar rules em-ployed by the parser.
For instance, the parse treefor the sentence:The explosion was presumed to be caused by anincendiary device.
(S(NP (AT The) (NN1 explosion))(VP (VBDZ be+ed)(VP (VVN presume+ed)(VP (TO to)(VP (VB0 be)(VP (VVN cause+ed)(PP (II by) (NP (AT1 an)(JJ incendiary) (NN1 device)))))))))looks very different and does not match the rulein figure 1.
With longer sentences, further prob-lems arise when similar strings are parsed differ-ently in the two aligned sentences (for example,different PP attachment) ?
these lead to very com-plicated rules, often with more than 20 variables.We split our data into development/training (96 in-stances of passive to active) and test sets (48 in-stances of passive).
Using the top parse for eachsentence, we derived 92 rules, including the oneshown in Figure 1.
However, coverage of theserules over the test corpus was poor (less than 10%recall).
By learning rules using the top 20 parsesfor each sentence rather than just the top parse,we could improve coverage to around 70%, butthis involved the acquisition of over 4000 differ-ent rules ?
just to change voice.
The situation waseven worse for reformulations that change syntac-tic categories, such as ?because?
to ?cause?, andwe obtained more than 20,000 rules that still gaveus a coverage of only around 15% for the test set.We concluded that this was not a sensible rep-resentation for general text reformulation.
In otherwords, while substitution grammars for parse treeshave been shown to be useful for sentence com-pression tasks (e.g., Cohn and Lapata (2009)), theyare less useful for more complex lexico-syntacticreformulation tasks.3.3 Reformulation using MRSAnother option is to use a bi-directional grammarand perform the transforms at a semantic level.
Wenow briefly discuss the use of Minimal RecursionSemantics (MRS) as a representation for transferrules.
Consider a very short example for ease ofillustration:Tom ate because of his hunger.This can be analysed by a deep grammar to givea compositional semantic representation whichcaptures the information that is available from thesyntax and inflectional morphology.
We show thissentence below in the Minimal Recursion Seman-tics (MRS) (Copestake et al, 2005) representation,as produced by the English Resource Grammar(ERG3) (Flickinger, 2000), but considerably sim-plified for ease of exposition and to save space:named(x5,Tom), _eat_v_1(e2,x5),_because_of(e2,x11), poss(x11,x16),pron(x16), _hunger_n(x11)The main part of the MRS structure is alist of elementary predications (EPs), whichmay have predicates derived from lexemes (e.g.,_eat_v_1; these are indicated by the leadingunderscore) or supplied by the grammar (e.g.,poss).
The ERG treats because of as a multiwordexpression and assigns it a semantics comparableto a preposition.
Paraphrase rules map between se-mantic representations; for our application, a pos-sible rule is the following:_because_of(e,x), P(e,y) <->_cause_v_1(e10,x,y,l1), l1:P(e,y)Here ?P?
is to be understood as a general predi-cate.
The left hand side of the rule will match thepreposition-like ?because of?
relation when it hasan event as an argument, where the event is the3Available at http://www.delph-in.net.characteristic event of an underspecified verbal EP.The right hand side indicates that the ?because of?can be substituted by a verbal relation correspond-ing to cause, with the verbal EP being a scopalargument.
This rule matches the MRS above andmaps it to the following (with P=_eat_v_1):named(x5,Tom), l1:_eat_v_1(e2,x5),_cause_v_1(e10,x11,x5,l1), poss(x11,x16),pron(x16), _hunger_n(x11), x5 aeq x16This can be input to the realiser, giving:His hunger caused Tom to eat.Writing transfer rules is intuitive and easy inMRS.
Further, the use of a bi-directional grammarfor generation ensures that the generated sentenceis grammatical.
An infrastructure of writing para-phrase rules exists in this framework and semantictransfer has also been explored for machine trans-lation (e.g., Copestake et al (1995)).The problem we encountered, however, is thatbidirectional grammars such as the ERG fail toparse ill-formed input and will also fail to anal-yse some well-formed input because of limitationsin coverage of unusual constructions.
Althoughthe DELPH-IN parsing technology allows for un-known words, missing lexical items can also causeparse failure and even more problems for gener-ation.
The ERG gives an acceptable parse ?outof the box?
for only around 50-60% of sentencesfrom scientific papers.
Further, the generator canget slow and memory intensive for long sentencesand many of our sentences are around 30 wordslong.
Much of this processing effort during gen-eration is redundant as the input sentence can beused to narrow down generation choices, but as ofnow, the infrastructure does not exist to supportthis.
Thus, while using a bi-directional grammarand semantic transfer might indeed be the most in-tuitive approach to complex lexico-syntactic refor-mulation, it is not quite feasible yet.3.4 Reformulation using Typed DependenciesHaving had mixed success with transformingphrasal parse trees and semantic representations,we turned our attention to typed dependency struc-tures.
We used the RASP toolkit (Briscoe et al,2006) for finding grammatical relations (GRs) be-tween words in the text.
GRs are triplets con-sisting of a relation-type and arguments and alsoencode morphology (stem + suffix), word posi-tion (after colon) and part-of-speech (after under-score); GRs produced for the sentence:The explosion was caused by an incendiary device.are:(|ncsubj| |cause+ed:4 VVN| |explosion:2 NN1| )(|aux| |cause+ed:4 VVN| |be+ed:3 VBDZ|)(|passive| |cause+ed:4 VVN|)(|iobj| |cause+ed:4 VVN| |by:5 II|)(|dobj| |by:5 II| |device:8 NN1|)(|det| |device:8 NN1| |an:6 AT1|)(|ncmod| |device:8 NN1| |incendiary:7 JJ|)(|det| |explosion:2 NN1| |the:1 AT|)This representation shares aspects of phrasalparse trees and MRS.
Note that the sets of de-pendencies (such as those above) represent a tree.4While phrase structure trees such as those in Sec-tion 3.2 represent the nesting of constituents withthe actual words at the leaf nodes, dependencytrees have words at every node:cause+ed:4XXXXXexplosion:2the:1be+ed:3 by:5device:8bb""an:6 incendiary:7To generate from a dependency tree, we needto know the order in which to process nodes - ingeneral tree traversal will be ?inorder?
; i.e, leftsubtrees will be processed before the root andright subtrees after.
These are generation deci-sions that would usually be guided by the typeof dependency and statistical preferences for wordand phrase order.
However, we can simply use theword positions (1?8) from the original sentence.While typed dependencies share characteris-tics with parse trees, the flat structure repre-sents dependencies between words, and we canwrite transformation rules for this representationin fairly compact form.
For instance, a transfor-mation rule to convert the above to active voicewould require five deletions and two insertions:1.
Match and Delete:(a) (|passive| |?
?X0|)(b) (|iobj| |?
?X0| |?
?X1(by II)|)(c) (|dobj| |?
?X1| |?
?X2|)(d) (|ncsubj| |?
?X0| |?
?X3| )(e) (|aux| |?
?X0| |??X4|)2.
Insert:(a) (|ncsubj| |?
?X0| |?
?X2| )(b) (|dobj| |?
?X0| |?
?X3|)4In fact, the GR scheme is only ?almost?
acyclic.
Thereare a small number of (predictable) relations that introducecycles; for instance, dependencies between the head of a rel-ative clause and the verb in the relative clause are representedas both a clausal modifier relation (cmod head verb) and anobject relation (obj verb head).
To resolve this, we use a fixedset of rules to remove these cycles from the dependency graphand ensure a tree structure.Thus far, the rule looks very similar to ruleswritten for MRS: one list of predicates is replacedby another.
Applying this transformation to theGR set above creates a new dependency tree:cause+ed:4PPPPdevice:8bb""an:6 incendiary:7explosion:2the:1However, unlike the case with MRS, where astatistical generator decides issues of morphologyand ordering, we have to specify the consequencesof the rule application for generation.
Note thatwe can no longer rely on the original word or-der to determine the order in which to traversethe tree for generation.
Thus our transformationrules, in addition to Deletion and Insertion oper-ations, also need to provide rules for tree traver-sal order.
These only need to be provided fornodes where the transform has reordered subtrees(??
?X0?, which instantiates to ?cause+ed:4?
in thetrees).
Our rule would thus include:3.
Traversal Order Specifications:(a) Node ?
?X0: [?
?X2, ?
?X0, ?
?X3]This states that for node ?
?X0, the traversal or-der should be subtree ?
?X2 followed by currentnode ?
?X0 followed by subtree ??X3.
Using thisspecification would allow us to traverse the treeusing the original word order for nodes with noorder specification, and the specified order wherea specification exist.
In the above instance, thiswould lead us to generate:An incendiary device caused the explosion.Our transfer rule is still incomplete and thereis one further issue that needs to be addressed ?operations to be performed on nodes rather thanrelations.
There are two node-level operations thatmight be required for sentence reformulation:1.
Lexical substitution: In our example above,we still need to ensure number agreement for theverb ?cause?
(??X0).
By changing voice, ?
?X0now has to agree with ?
?X2 rather than ??X3.
Fur-ther the tense of ?
?X0 was encoded in the auxiliaryverb ?
?X4 that has been deleted from the GRs.
Wethus need the transfer rule to encode the lexicalsubstitution required for node ??X0:4.
Lexical substitution:(a) Node ?
?X0: IF (?
?X4 is Present Tense) THEN {IF (?
?X2 is Plural) THEN {SET ??X0:SUFFIX=??}
ELSE {SET ?
?X0:SUFFIX =?s?}
}Other lexical substitutions are easier to spec-ify; for instance to reformulate ?John ran be-cause David shouted.?
as ?David?s shoutingcaused John to run?, the following lexical substi-tution rule is required for node ?
?Xn representing?shout?
that replaces its suffix ?ed?
with ?ing?
:Lexical substitution: Node ?
?Xn: Suffix=?ing?2.
Node deletion: This is an operation that re-moves a node from the tree.
Any subtrees aremoved to the parent node.
If a root node is deleted,one of the children adopts the rest.
By default, theright-most child takes the rest as dependents, butwe allow the rule to specify the new parent.
Inthe above example, we want to remove the nodes??X1(?by?)
and ?
?X4 (?was?)
(note that deletinga relation does not necessarily remove a node ?there might be other nodes connected to ?
?X1 or??X4).
We would like to move these to the node?
?X0 (?cause?):5.
Node Deletion:(a) Node ?
?X1: Target=?
?X0(b) Node ?
?X4: Target=?
?X0Node deletion is easily implemented usingsearch and replace on sets of GRs.
It is centralto reformulations that alter syntactic categories ofdiscourse markers; for instance, to reformulate?The cause of X is Y?
as ?Y causes X?, we needto delete the verb ?is?
and move its dependents tothe new verb ?causes?.To summarise, we propose a framework forlexico-syntactic reformulation based on typed de-pendency structures and have discussed the formof a transformation.
We now specify the structureof transfer rules and tree nodes more formally.Specification for Transfer RulesOur proposal is based on applying transfer rules tolists of grammatical relations (GRs).
Our transferrules take the form of five lists:1.
CONTEXT: Transform only proceeds if this list of GRscan be unified with the input GRs.2.
DELETE: List of GRs to delete from input.3.
INSERT: List of GRs to insert into input.4.
ORDERING: List of nodes with subtree order specified5.
NODE-OPERATIONS: List of lexical substitutionsand deletion operations on nodes.For the reformulations in this paper, the CON-TEXT and DELETE lists are one and the same, butone can imagine reformulation tasks where extracontext needs to be specified to determine whetherreformulation is appropriate.
The first three listscorrespond to the CONTEXT, INPUT and OUT-PUT lists used to specify transform in the MRSframework.
However, because we do not use a for-mal grammar for generation, we need two furtherlists that capture changes in morphology or con-stituent ordering.
The list ORDERING is used totraverse the dependency tree constructed from thetransformed GRs.
Again, the lexical substitutionlists are prescriptions for generation.
We restrictour lexical substitutions to change of suffix andpart of speech (for instance, ?X is a frequent causeof Y?
to ?X frequently causes Y?
), but in generalthis can be an arbitrary string substitution (for in-stance, ?X and Y are two causes of Z?
to ?X andY both cause Z?
).In this paper, we have tried to do away with agenerator altogether by encoding generation deci-sions within the transfer rule.
A case can be made,particularly for the issue of agreement, for suchissues to be handled by a generator.
This wouldmake the transfer rules simpler to write, and easierto learn automatically in a supervised setting.Specification for Dependency TreeApplying a transfer rule specified above results ina new set of GRs.
To generate a sentence, we needto create a dependency tree from these GRs.
Asdescribed earlier, a dependency tree needs to betraversed ?inorder?
to generate a sentence.
Thismeans that at each node, the order in which tovisit the daughters and the current node needs tobe specified.
To enable this, we propose that eachnode in the tree have the following features:1.
VALUE: stem, suffix and part-of-speech of the word;2.
PARENT: parent node;3.
CHILDREN: list of daughters;4.
ORDER: list specifying order in which to visit childrenand current node.The parent node is required for DELETE oper-ations and to find the root of the tree (node withno parent).
Further, if there is more than one nodewith no parent, the GRs do not form a tree andgeneration will result in multiple fragments.The dependency tree is constructed using thefollowing algorithm:1.
For each word in the list of GRs:(a) Create a Node and instantiate the VALUE field.2.
For each GR (relation word1 word2):(a) If GR is one that introduces a cycle, remove itfrom list, else add the node created for word2 tothe CHILDREN list of node for word1 and setPARENT of word 2 to word1.3.
After Step 2, the tree is created.
Now for each Node:(a) If an ORDERING specification is introduced forthis node by the transformation rule, copy thatlist to the ORDER field, else add the daughternodes to the ORDER list in increasing order ofword position.The reformulated sentence is generated bytraversing the tree ?inorder?, outputting the wordat each node visited (the stem, suffix and part-of-speech tag are fed to the RASP morphologicalgenerator, which returns the correct word).4 Evaluating Transformation RulesIn this paper we have proposed a framework forcomplex lexico-syntactic reformulations.
We wantto evaluate our framework for (a) how easy it is towrite transformation rules, (b) how many are re-quired for intuitive lexico-syntactic reformulationsand (c) how robust the transformation is to parsingerrors.
With this intended purpose, we evaluatehand-written transformation rules that have beendeveloped looking at one third of the corpus (48sentences) and tested on the remaining two thirds(96 sentences).
We report results using:?
Recall: The proportion of sentences in the test set forwhich a transform was performed; i.e., (a) the DELETEpattern matched the input GRs and (b) there was ex-actly one root node in the transformed GRs resulting inexactly one sentence being output?
Precision: The proportion of transformed sentencethat were accurate; i.e., grammatical with (a) cor-rect verb agreement and inflexion and (b) modi-fiers/complements appearing in acceptable orders.Note that we are merely evaluating the frame-work and not evaluating the utility of these trans-formations for text simplification ?
that would re-quire an evaluation using test subjects drawn fromour intended users.
Table 1 provides some exam-ples of accurate and inaccurate transformations.The rule for converting passives to actives de-scribed in Section 3.4 already achieves a recallof 42% and precision of 83%.
Writing 6 addi-tional rules to handle reduced relative clauses (1a-b, Table 1) etc., we could boost recall to 71% withprecision dropping marginally to 82%.
We hand-crafted rules to implement three other reformula-tions.
These were selected based on results fromthe Siddharthan and Katsos (2010) study that sug-gested:1. cause as a noun (either information ordering), passivevoice, ?because of?
and ?because a, b?
formulations(versions b,d,e,f,g and h in Example 1, Section 1) aredispreferred by lay readers.
Moreover, these are com-mon constructs in scientific writing.2.
cause as a verb in active voice and ?b because a?
arethe most preferred formulations for lay readers.Accurate Transformations1a.
Apart from occasional problems of ensemble caused by the complex rhythms of the outer movements, the orchestra gavean animated and committed reading of the work.
[B-CAUSEBY-A?A-CAUSE-B]b.
Apart from occasional problems of ensemble the complex rhythms of the outer movements caused, the orchestra gave ananimated and committed reading of the work.2a.
Because of transvection, the expression of a gene can be sensitive to the proximity of a homolog.[BEC-OF-A-B?A-CAUSE-B]b.
Transvection can cause the expression of a gene to be sensitive to the proximity of a homolog.3a.
Because each myosin is expressed in Drosophila indirect flight muscle, in the absence of other myosin isoforms, thisallows for muscle mechanical and whole organism locomotion assays.
[BEC-A-B?B-BEC-A]b.
In the absence of other myosin isoforms, this allows for muscle mechanical and whole organism locomotion assaysbecause each myosin is expressed in Drosophila indirect flight muscle.4a.
Almost certainly, however, the underlying cause of the war was the problem of Aquitaine.
[CAUSEOF-B-A?A-CAUSE-B]b.
Almost certainly, however, the underlying problem of Aquitaine caused the war.Inaccurate Transformation5a.
Moreover, main road traffic has scarcely been slowed and concern should be caused by the rising number of cyclistcasualties.
[B-CAUSEBY-A?A-CAUSE-B]b.
Moreover, the rising number of cyclist casualties should cause main road traffic has scarcely been slowed and concern.6a.
Because of the risk of injury and the need to kill prey quickly, predators usually predate animals smaller than themselves.
[BEC-OF-A-B?A-CAUSE-B]b The risk of injury and the need cause kill to prey quickly predators usually predate animals smaller than themselves.Table 1: Examples of automatic reformulations (version a. is the original and b. the reformulation).Handcrafted rules n P R FB-CAUSEBY-A?
A-CAUSE-B 7 .82 (1.00) .71 (.75) .76 (.86)BEC-OF-A-B?
A-CAUSE-B 9 .75 (.92) .70 (1.00) .72 (.97)BEC-A-B?
B-BEC-A 8 .85 (.92) .83 (.87) .84 (.89)CAUSEOF?
A-CAUSE-B 6 .97 (.90) .78 (1.00) .86 (.95)Table 2: Number of Rules (n), Precision, Recalland F-Measure for lexico-syntactic reformulationusing hand-crafted rules over GRs.
Numbers inbrackets are over the subset of the corpus that con-tains only the original sentences from PubMed andthe BNC.We summarise our results in Table 2.
Most ofthe sentences in the corpus are manual reformu-lations and some of them are quite stilted.
Thenumbers in brackets show performance over thesmaller set of original sentences from PubMed andthe BNC.
These are more indicative of how therules will perform on real data.
Our results sug-gests that the framework we propose is adequatefor a range of lexico-syntactic reformulations anda fairly small number of rules is required to cap-ture a reformulation.Loss of recall was usually from parsing error(either misparses, in which case our rules don?tmatch the GRs, or partial parses, where a full treecan?t be formed because of missing GRs).Loss of precision was a more worrying issue asit often resulted in badly corrupted output.
Thiswas usually the result of either bad parser deci-sions regarding attachment or scope or just mis-parsing (e.g., wide scoping of ?and?
in 5a-b andparsing ?prey?
as a verb in 6a-b, Table 1).
It mightbe possible to trade-off recall for improved preci-sion by identifying sentences where ambiguity isa problem (by looking at multiple parses).5 Conclusions and Future WorkIn this paper we have reported our experience withusing different linguistic formalisms as represen-tations for applying transform rules to generatecomplex lexico-syntactic reformulations of sen-tences expressing the discourse relation of causa-tion.
We find typed dependency structures to bethe most suited for this task and report that hand-crafted transformation rules generalise well to sen-tences in an unseen test corpus.
We believe thatthe framework we have described is adequate for arange of regeneration tasks focused on text simpli-fication.
While in this paper we focus on the dis-course relation of causation, other discourse rela-tions commonly used in scientific writing can alsobe realised using markers with different lexico-syntactic properties; for instance, contrast can beexpressed using markers such as ?while?, ?un-like?, ?but?, ?compared to?, ?in contrast to?
and?the difference between?.
Our rules for voice con-version and information reordering for subordina-tion are already general enough to be applied tonon-causal constructs.
We also plan to use ourframework to explore sentence simplification andsentence shortening applications.We would in the future like to learn transfor-mations rules automatically from a corpus.
Hand-crafting can get tedious as there are 17 types ofgrammatical relations to take into account in theRASP scheme.
Preliminary work by us in this re-gard suggests that augmenting a few hand-craftedrules with around a hundred automatically learntrules can increase recall substantially.
However,our learning framework as yet does not allow nodetransformations, and more work is required here.AcknowledgementsThis work was supported by the Economic andSocial Research Council (Grant Number RES-000-22-3272).
We would also like to thank DanFlickinger and Ann Copestake for many discus-sions on the topic of paraphrase and for help withusing the ERG.ReferencesR.C.
Anderson and A. Davison.
1988.
Conceptualand empirical bases of readibility formulas.
In AliceDavison and G. M. Green, editors, Linguistic Com-plexity and Text Comprehension: Readability IssuesReconsidered.
Lawrence Erlbaum Associates, Hills-dale, NJ.R.
Barzilay and L. Lee.
2003.
Learning to paraphrase:An unsupervised approach using multiple-sequencealignment.
In HLT-NAACL 2003: Main Proceed-ings, pages 16?23.I.L.
Beck, M.G.
McKeown, G.M.
Sinatra, and J.A.Loxterman.
1991.
Revising social studies textfrom a text-processing perspective: Evidence of im-proved comprehensibility.
Reading Research Quar-terly, pages 251?276.T.
Briscoe, J. Carroll, and R. Watson.
2006.
The sec-ond release of the RASP system.
In Proceedings ofthe COLING/ACL, volume 6.J.
Carroll, G. Minnen, Y. Canning, S. Devlin, and J.Tait.
1998.
Practical simplification of English news-paper text to assist aphasic readers.
In Proceedingsof AAAI98 Workshop on Integrating Artificial Intelli-gence and Assistive Technology, pages 7?10, Madi-son, Wisconsin.T.
Cohn and M. Lapata.
2009.
Sentence compres-sion as tree transduction.
Journal of Artificial In-telligence Research, 34(1):637?674.A.
Copestake, D. Flickinger, R. Malouf, S. Riehemann,and I.
Sag.
1995.
Translation using minimal recur-sion semantics.
In Proceedings of the Sixth Interna-tional Conference on Theoretical and Methodologi-cal Issues in Machine Translation, pages 15?32.A.
Copestake, D. Flickinger, I.
Sag, and C. Pollard.2005.
Minimal recursion semantics: An intro-duction.
Research in Language and Computation,3:281?332.D.
Flickinger.
2000.
On building a more efficientgrammar by exploiting types.
Natural LanguageEngineering, 6(1):15?28.M.
Galley and K. McKeown.
2007.
LexicalizedMarkov grammars for sentence compression.
InHLT-NAACL 2007: Main Proceedings, pages 180?187, Rochester, New York, April.
Association forComputational Linguistics.A.
Ibrahim, B. Katz, and J. Lin.
2003.
Extracting para-phrases from aligned corpora.
In Proceedings of TheSecond International Workshop on Paraphrasing.N.
Kaji, D. Kawahara, S. Kurohash, and S. Sato.
2002.Verb paraphrase based on case frame alignment.
InProceedings of the 40th Annual Meeting of the As-sociation for Computational Linguistics (ACL?02),pages 215?222, Philadelphia, USA.K.
Knight and D. Marcu.
2000.
Statistics-based sum-marization ?
step one: Sentence compression.
InProceeding of The American Association for Arti-ficial Intelligence Conference (AAAI-2000), pages703?710.J.J.
L?Allier.
1980.
An evaluation study of a computer-based lesson that adjusts reading level by monitor-ing on task reader characteristics.
Ph.D. thesis,University of Minnesota, Minneapolis, MN.E.T.
Levy.
2003.
The roots of coherence in discourse.Human Development, pages 169?88.T.
Linderholm, M.G.
Everson, P. van den Broek,M.
Mischinski, A. Crittenden, and J. Samuels.
2000.Effects of Causal Text Revisions on More-and Less-Skilled Readers?
Comprehension of Easy and Dif-ficult Texts.
Cognition and Instruction, 18(4):525?556.L.
G. M. Noordman and W. Vonk.
1992.
Reader?sknowledge and the control of inferences in reading.Language and Cognitive Processes, 7:373?391.R.
Power, D. Scott, and N. Bouayad-Agha.
2003.
Gen-erating texts with style.
Proceedings of the 4 thInter-national Conference on Intelligent Texts Processingand Computational Linguistics.S.
Riezler, T.H.
King, R. Crouch, and A. Zaenen.
2003.Statistical sentence condensation using ambiguitypacking and stochastic disambiguation methods forlexical-functional grammar.
In HLT-NAACL 2003:Main Proceedings, Edmonton, Canada.A.
Siddharthan and N. Katsos.
2010.
Reformulat-ing discourse connectives for non-expert readers.In Proceedings of the 11th Annual Conference ofthe North American Chapter of the Association forComputational Linguistics (NAACL-HLT 2010), LosAngeles, CA.A.
Siddharthan.
2006.
Syntactic simplification andtext cohesion.
Research on Language and Compu-tation, 4(1):77?109.S.
Williams and E. Reiter.
2008.
Generating basicskills reports for low-skilled readers.
Natural Lan-guage Engineering, 14(04):495?525.S.
Williams, E. Reiter, and L. Osman.
2003.
Ex-periments with discourse-level choices and read-ability.
In Proceedings of the European NaturalLanguage Generation Workshop (ENLG), EACL?03,pages 127?134, Budapest, Hungary.
