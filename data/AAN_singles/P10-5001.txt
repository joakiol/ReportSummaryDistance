Tutorial Abstracts of ACL 2010, page 1,Uppsala, Sweden, 11 July 2010. c?2010 Association for Computational LinguisticsWide-coverage NLP with Linguistically Expressive GrammarsJulia HockenmaierDepartment of Computer Science,University of Illinoisjuliahmr@illinois.eduYusuke MiyaoNational Institute of Informaticsyusuke@nii.ac.jpJosef van GenabithCentre for Next Generation Localisation,School of Computing,Dublin City Universityjosef@computing.dcu.ie1 IntroductionIn recent years, there has been a lot of researchon wide-coverage statistical natural languageprocessing with linguistically expressive gram-mars such as Combinatory Categorial Grammars(CCG), Head-driven Phrase-Structure Grammars(HPSG), Lexical-Functional Grammars (LFG)and Tree-Adjoining Grammars (TAG).
But al-though many young researchers in natural lan-guage processing are very well trained in machinelearning and statistical methods, they often lackthe necessary background to understand the lin-guistic motivation behind these formalisms.
Fur-thermore, in many linguistics departments, syntaxis still taught from a purely Chomskian perspec-tive.
Additionally, research on these formalismsoften takes place within tightly-knit, formalism-specific subcommunities.
It is therefore often dif-ficult for outsiders as well as experts to grasp thecommonalities of and differences between theseformalisms.2 Content OverviewThis tutorial overviews basic ideas of TAG/CCG/LFG/HPSG, and provides attendees with acomparison of these formalisms from a linguis-tic and computational point of view.
We startfrom stating the motivation behind using these ex-pressive grammar formalisms for NLP, contrast-ing them with shallow formalisms like context-free grammars.
We introduce a common set ofexamples illustrating various linguistic construc-tions that elude context-free grammars, and reusethem when introducing each formalism: boundedand unbounded non-local dependencies that arisethrough extraction and coordination, scrambling,mappings to meaning representations, etc.
In thesecond half of the tutorial, we explain two keytechnologies for wide-coverage NLP with thesegrammar formalisms: grammar acquisition andparsing models.
Finally, we show NLP applica-tions where these expressive grammar formalismsprovide additional benefits.3 Tutorial Outline1.
Introduction: Why expressive grammars2.
Introduction to TAG3.
Introduction to CCG4.
Introduction to LFG5.
Introduction to HPSG6.
Inducing expressive grammars from corpora7.
Wide-coverage parsing with expressivegrammars8.
Applications9.
SummaryReferencesAoife Cahill, Michael Burke, Ruth O?Donovan, StefanRiezler, Josef van Genabith and Andy Way.
2008.Wide-Coverage Deep Statistical Parsing using Au-tomatic Dependency Structure Annotation.
Compu-tational Linguistics, 34(1).
pp.81-124, MIT Press.Yusuke Miyao and Jun?ichi Tsujii.
2008.
Feature For-est Models for Probabilistic HPSG Parsing.
Compu-tational Linguistics, 34(1).
pp.35-80, MIT Press.Julia Hockenmaier and Mark Steedman.
2007.
CCG-bank: A Corpus of CCG Derivations and Depen-dency Structures Extracted from the Penn Treebank.Computational Linguistics, 33(3).
pp.355-396, MITPress.1
