Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and ComputationalNatural Language Learning, pp.
524?532, Prague, June 2007. c?2007 Association for Computational LinguisticsA Systematic Comparison of Training Criteriafor Statistical Machine TranslationRichard Zens and Sas?a Hasan and Hermann NeyHuman Language Technology and Pattern RecognitionLehrstuhl fu?r Informatik 6 ?
Computer Science DepartmentRWTH Aachen University, D-52056 Aachen, Germany{zens,hasan,ney}@cs.rwth-aachen.deAbstractWe address the problem of training the freeparameters of a statistical machine transla-tion system.
We show significant improve-ments over a state-of-the-art minimum er-ror rate training baseline on a large Chinese-English translation task.
We present noveltraining criteria based on maximum likeli-hood estimation and expected loss compu-tation.
Additionally, we compare the maxi-mum a-posteriori decision rule and the min-imum Bayes risk decision rule.
We showthat, not only from a theoretical point ofview but also in terms of translation qual-ity, the minimum Bayes risk decision rule ispreferable.1 IntroductionOnce we specified the Bayes decision rule for statis-tical machine translation, we have to address threeproblems (Ney, 2001):?
the search problem, i.e.
how to find the besttranslation candidate among all possible targetlanguage sentences;?
the modeling problem, i.e.
how to structurethe dependencies of source and target languagesentences;?
the training problem, i.e.
how to estimate thefree parameters of the models from the trainingdata.Here, the main focus is on the training problem.
Wewill compare a variety of training criteria for statisti-cal machine translation.
In particular, we are consid-ering criteria for the log-linear parameters or modelscaling factors.
We will introduce new training cri-teria based on maximum likelihood estimation andexpected loss computation.
We will show that someachieve significantly better results than the standardminimum error rate training of (Och, 2003).Additionally, we will compare two decision rules,the common maximum a-posteriori (MAP) deci-sion rule and the minimum Bayes risk (MBR) de-cision rule (Kumar and Byrne, 2004).
We will showthat the minimum Bayes risk decision rule resultsin better translation quality than the maximum a-posteriori decision rule for several training criteria.The remaining part of this paper is structuredas follows: first, we will describe related work inSec.
2.
Then, we will briefly review the baselinesystem, Bayes decision rule for statistical machinetranslation and automatic evaluation metrics for ma-chine translation in Sec.
3 and Sec.
4, respectively.The novel training criteria are described in Sec.
5and Sec.
6.
Experimental results are reported inSec.
7 and conclusions are given in Sec.
8.2 Related WorkThe most common modeling approach in statisticalmachine translation is to use a log-linear combina-tion of several sub-models (Och and Ney, 2002).
In(Och and Ney, 2002), the log-linear weights weretuned to maximize the mutual information criterion(MMI).
The current state-of-the-art is to optimizethese parameters with respect to the final evaluationcriterion; this is the so-called minimum error ratetraining (Och, 2003).Minimum Bayes risk decoding for machine trans-524lation was introduced in (Kumar and Byrne, 2004).It was shown that MBR outperforms MAP decodingfor different evaluation criteria.
Further experimentsusing MBR for Bleu were performed in (Venugopalet al, 2005; Ehling et al, 2007).
Here, we willpresent additional evidence that MBR decoding ispreferable over MAP decoding.Tillmann and Zhang (2006) describe a percep-tron style algorithm for training millions of features.Here, we focus on the comparison of different train-ing criteria.Shen et al (2004) compared different algorithmsfor tuning the log-linear weights in a rerankingframework and achieved results comparable to thestandard minimum error rate training.An annealed minimum risk approach is presentedin (Smith and Eisner, 2006) which outperforms bothmaximum likelihood and minimum error rate train-ing.
The parameters are estimated iteratively usingan annealing technique that minimizes the risk of anexpected-BLEU approximation, which is similar tothe one presented in this paper.3 Baseline SystemIn statistical machine translation, we are given asource language sentence fJ1 = f1 .
.
.
fj .
.
.
fJ ,which is to be translated into a target language sen-tence eI1 = e1 .
.
.
ei .
.
.
eI .
Statistical decision the-ory tells us that among all possible target languagesentences, we should choose the sentence whichminimizes the expected loss, also called Bayes risk:e?I?1 = argminI,eI1{?I?,e?I?1Pr(e?I?1 |fJ1 ) ?
L(eI1, e?I?1 )}Here, L(eI1, e?I?1 ) denotes the loss function underconsideration.
It measures the loss (or errors) of acandidate translation eI1 assuming the correct trans-lation is e?I?1 .
In the following, we will call this de-cision rule the MBR rule (Kumar and Byrne, 2004).This decision rule is optimal in the sense that anyother decision rule will result (on average) in at leastas many errors as the MBR rule.
Despite this, mostSMT systems do not use theMBR decision rule.
Themost common approach is to use the maximum a-posteriori (MAP) decision rule.
Thus, we select thehypothesis which maximizes the posterior probabil-ity Pr(eI1|fJ1 ):e?I?1 = argmaxI,eI1{Pr(eI1|fJ1 )}This is equivalent to the MBR decision rule undera 0-1 loss function:L0?1(eI1, e?I?1 ) ={0 if eI1 = e?I?11 elseHence, the MAP decision rule is optimal for thesentence or string error rate.
It is not necessarilyoptimal for other evaluation metrics such as the Bleuscore.
One reason for the popularity of the MAPdecision rule might be that, compared to the MBRrule, its computation is simpler.The posterior probability Pr(eI1|fJ1 ) is modeleddirectly using a log-linear combination of severalmodels (Och and Ney, 2002):p?M1 (eI1|fJ1 ) =exp(?Mm=1 ?mhm(eI1, fJ1 ))?I?,e?I?1exp(?Mm=1 ?mhm(e?I?1 , fJ1 ))(1)This approach is a generalization of the source-channel approach (Brown et al, 1990).
It has theadvantage that additional models h(?)
can be easilyintegrated into the overall system.The denominator represents a normalization fac-tor that depends only on the source sentence fJ1 .Therefore, we can omit it in case of the MAP de-cision rule during the search process and obtain:e?I?1 = argmaxI,eI1{M?m=1?mhm(eI1, fJ1 )}Note that the denominator affects the results of theMBR decision rule and, thus, cannot be omitted inthat case.We use a state-of-the-art phrase-based translationsystem similar to (Koehn, 2004; Mauser et al, 2006)including the following models: an n-gram lan-guage model, a phrase translation model and a word-based lexicon model.
The latter two models are usedfor both directions: p(f |e) and p(e|f).
Additionally,we use a word penalty, phrase penalty and a distor-tion penalty.525In the following, we will discuss the so-calledtraining problem (Ney, 2001): how do we train thefree parameters ?M1 of the model?
The currentstate-of-the-art is to use minimum error rate train-ing (MERT) as described in (Och, 2003).
The freeparameters are tuned to directly optimize the evalu-ation criterion.Except for the MERT, the training criteria thatwe will consider are additive at the sentence-level.Thus, the training problem for a development setwith S sentences can be formalized as:?
?M1 = argmax?M1S?s=1F (?M1 , (eI1, fJ1 )s) (2)Here, F (?, ?)
denotes the training criterion that wewould like to maximize and (eI1, fJ1 )s denotes a sen-tence pair in the development set.
The optimizationis done using the Downhill Simplex algorithm fromthe Numerical Recipes book (Press et al, 2002).This is a general purpose optimization procedurewith the advantage that it does not require the deriva-tive information.
Before we will describe the detailsof the different training criteria in Sec.
5 and 6, wewill discuss evaluation metrics in the following sec-tion.4 Evaluation MetricsThe automatic evaluation of machine translation iscurrently an active research area.
There exists avariety of different metrics, e.g., word error rate,position-independent word error rate, BLEU score(Papineni et al, 2002), NIST score (Doddington,2002), METEOR (Banerjee and Lavie, 2005), GTM(Turian et al, 2003).
Each of them has advantagesand shortcomings.A popular metric for evaluating machine trans-lation quality is the Bleu score (Papineni et al,2002).
It has certain shortcomings for compar-ing different machine translation systems, especiallyif comparing conceptually different systems, e.g.phrase-based versus rule-based systems, as shownin (Callison-Burch et al, 2006).
On the other hand,Callison-Burch concluded that the Bleu score is re-liable for comparing variants of the same machinetranslation system.
As this is exactly what we willneed in our experiments and as Bleu is currently themost popular metric, we have chosen it as our pri-mary evaluation metric.
Nevertheless, most of themethods we will present can be easily adapted toother automatic evaluation metrics.In the following, we will briefly review the com-putation of the Bleu score as some of the trainingcriteria are motivated by this.
The Bleu score is acombination of the geometric mean of n-gram pre-cisions and a brevity penalty for too short translationhypotheses.
The Bleu score for a translation hypoth-esis eI1 and a reference translation e?I?1 is computed as:Bleu(eI1, e?I?1) = BP(I, I?)
?4?n=1Precn(eI1, e?I?1)1/4withBP(I, I?)
={1 if I ?
I?exp (1 ?
I/I?)
if I < I?Precn(eI1, e?I?1) =?wn1min{C(wn1 |eI1), C(wn1 |e?I?1)}?wn1C(wn1 |eI1)(3)Here, C(wn1 |eI1) denotes the number of occur-rences of an n-gram wn1 in a sentence eI1.
The de-nominators of the n-gram precisions evaluate to thenumber of n-grams in the hypothesis, i.e.
I ?n+1.The n-gram counts for the Bleu score computa-tion are usually collected over a whole document.For our purposes, a sentence-level computation ispreferable.
A problem with the sentence-level Bleuscore is that the score is zero if not at least one four-gram matches.
As we would like to avoid this prob-lem, we use the smoothed sentence-level Bleu scoreas suggested in (Lin and Och, 2004).
Thus, we in-crease the nominator and denominator of Precn(?, ?
)by one for n > 1.
Note that we will use thesentence-level Bleu score only during training.
Theevaluation on the development and test sets will becarried out using the standard Bleu score, i.e.
at thecorpus level.
As the MERT baseline does not requirethe use of the sentence-level Bleu score, we use thestandard Bleu score for training the baseline system.In the following, we will describe several crite-ria for training the log-linear parameters ?M1 of ourmodel.
For notational convenience, we assume thatthere is just one reference translation.
Nevertheless,the methods can be easily adapted to the case of mul-tiple references.5265 Maximum Likelihood5.1 Sentence-Level ComputationA popular approach for training parameters is max-imum likelihood estimation (MLE).
Here, the goalis to maximize the joint likelihood of the parametersand the training data.
For log-linear models, this re-sults in a nice optimization criterion which is con-vex and has a single optimum.
It is equivalent to themaximum mutual information (MMI) criterion.
Weobtain the following training criterion:FML?S(?M1 , (eI1, fJ1 )) = log p?M1 (eI1|fJ1 )A problem that we often face in practice is thatthe correct translation might not be among the can-didates that our MT system produces.
Therefore,(Och and Ney, 2002; Och, 2003) defined the trans-lation candidate with the minimum word-error rateas pseudo reference translation.
This has some biastowards minimizing the word-error rate.
Here, wewill use the translation candidate with the maximumBleu score as pseudo reference to bias the systemtowards the Bleu score.
However, as pointed out in(Och, 2003), there is no reason to believe that the re-sulting parameters are optimal with respect to trans-lation quality measured with the Bleu score.The goal of this sentence-level criterion is to dis-criminate the single correct translation against all theother ?incorrect?
translations.
This is problematicas, even for human experts, it is very hard to definea single best translation of a sentence.
Furthermore,the alternative target language sentences are not allequally bad translations.
Some of them might bevery close to the correct translation or even equiva-lent whereas other sentences may have a completelydifferent meaning.
The sentence-level MLE crite-rion does not distinguish these cases and is thereforea rather harsh training criterion.5.2 N -gram Level ComputationAs an alternative to the sentence-level MLE, weperformed experiments with an n-gram level MLE.Here, we limit the order of the n-grams and assumeconditional independence among the n-gram prob-abilities.
We define the log-likelihood (LLH) of atarget language sentence eI1 given a source languagesentence fJ1 as:FML?N (?M1 , (eI1, fJ1 )) =N?n=1?wn1?eI1log p?M1 (wn1 |fJ1 )Here, we use the n-gram posterior probabilityp?M1 (wn1 |fJ1 ) as defined in (Zens and Ney, 2006).The n-gram posterior distribution is smoothed usinga uniform distribution over all possible n-grams.p?M1 (wn1 |fJ1 ) = ?
?N?M1 (wn1 , fJ1 )?w?n1N?M1 (w?n1 , fJ1 )+ (1 ?
?)
?1V nHere, V denotes the vocabulary size of the tar-get language; thus, V n is the number of possi-ble n-grams in the target language.
We defineN?M1 (wn1 , fJ1 ) as in (Zens and Ney, 2006):N?M1 (wn1 , fJ1 ) =?I,eI1I?n+1?i=1p?M1 (eI1|fJ1 )??
(ei+n?1i , wn1 )(4)The sum over the target language sentences is lim-ited to an N -best list, i.e.
the N best translationcandidates according to the baseline model.
In thisequation, we use the Kronecker function ?
(?, ?
), i.e.the term ?
(ei+n?1i , wn1 ) evaluates to one if and onlyif the n-gram wn1 occurs in the target sentence eI1starting at position i.An advantage of the n-gram level computationof the likelihood is that we do not have to definepseudo-references as for the sentence-level MLE.We can easily compute the likelihood for the humanreference translation.
Furthermore, this criterion hasthe desirable property that it takes partial correctnessinto account, i.e.
it is not as harsh as the sentence-level criterion.6 Expected Bleu ScoreAccording to statistical decision theory, one shouldmaximize the expected gain (or equivalently mini-mize the expected loss).
For machine translation,this means that we should optimize the expectedBleu score, or any other preferred evaluation metric.5276.1 Sentence-Level ComputationThe expected Bleu score for a given source sentencefJ1 and a reference translation e?I?1 is defined as:E[Bleu|e?I?1, fJ1 ] =?eI1Pr(eI1|fJ1 ) ?
Bleu(eI1, e?I?1)Here, Pr(eI1|fJ1 ) denotes the true probability dis-tribution over the possible translations eI1 of thegiven source sentence fJ1 .
As this probability dis-tribution is unknown, we approximate it using thelog-linear translation model p?M1 (eI1|fJ1 ) from Eq.
1.Furthermore, the computation of the expected Bleuscore involves a sum over all possible translationseI1.
This sum is approximated using an N -best list,i.e.
the N best translation hypotheses of the MT sys-tem.
Thus, the training criterion for the sentence-level expected Bleu computation is:FEB?S(?M1 , (e?I?1, fJ1 )) =?eI1p?M1 (eI1|fJ1 )?Bleu(eI1, e?I?1)An advantage of the sentence-level computation isthat it is straightforward to plug in alternative eval-uation metrics instead of the Bleu score.
Note thatthe minimum error rate training (Och, 2003) usesonly the target sentence with the maximum posteriorprobability whereas, here, the whole probability dis-tribution is taken into account.6.2 N -gram Level ComputationIn this section, we describe a more fine grained com-putation of the expected Bleu score by exploiting itsparticular structure.
Hence, this derivation is spe-cific for the Bleu score but should be easily adapt-able to other n-gram based metrics.
We can rewritethe expected Bleu score as:E[Bleu|e?I?1, fJ1 ] = E[BP|I?
, fJ1 ]?4?n=1E[Precn|e?I?1, fJ1 ]1/4We assumed conditional independence betweenthe brevity penalty BP and the n-gram precisionsPrecn.
Note that although these independence as-sumptions do not hold, the resulting parametersmight work well for translation.
In fact, we willshow that this criterion is among the best perform-ing ones in Sec.
7.
This type of independence as-sumption is typical within the naive Bayes classifierframework.
The resulting training criterion that wewill use in Eq.
2 is then:FEB?N (?M1 , (e?I?1, fJ1 )) = E?M1 [BP|I?
, fJ1 ]?4?n=1E?M1 [Precn|e?I?1, fJ1 ]1/4We still have to define the estimators for the ex-pected brevity penalty as well as the expected n-gram precision:E?M1 [BP|I?
, fJ1 ] =?IBP(I, I?)
?
p?M1 (I|fJ1 )E?M1 [Precn|e?I?1, fJ1 ] = (5)?wn1p?M1 (wn1 |fJ1 )?cmin{c, C(wn1 |e?I?1)} ?
p?M1 (c|wn1 , fJ1 )?wn1p?M1 (wn1 |fJ1 )?cc ?
p?M1 (c|wn1 , fJ1 )Here, we use the sentence length posterior proba-bility p?M1 (I|fJ1 ) as defined in (Zens and Ney, 2006)and the n-gram posterior probability p?M1 (wn1 |fJ1 ) asdescribed in Sec.
5.2.
Additionally, we predict thenumber of occurrences c of an n-gram.
This infor-mation is necessary for the so-called clipping in theBleu score computation, i.e.
the min operator in thenominator of formulae Eq.
3 and Eq.
5.
The denom-inator of Eq.
5 is the expected number of n-grams inthe target sentence, whereas the nominator denotesthe expected number of correct n-grams.To predict the number of occurrences within atranslation hypothesis, we use relative frequenciessmoothed with a Poisson distribution.
The mean ofthe Poisson distribution ?
(wn1 , fJ1 , ?M1 ) is chosen tobe the mean of the unsmoothed distribution.p?M1 (c|wn1 , fJ1 ) = ?
?N?M1 (c, wn1 , fJ1 )N?M1 (wn1 , fJ1 )+ (1 ?
?)
??
(wn1 , fJ1 , ?M1 )c ?
e?cc!528Table 1: Chinese-English TC-Star task: corpusstatistics.Chinese EnglishTrain Sentence pairs 8.3MRunning words 197M 238MVocabulary size 224K 389KDev Sentences 1 019 2 038Running words 26K 51KEval 2006 Sentences 1 232 2 464Running words 30K 62K2007 Sentences 917 1 834Running words 21K 45Kwith?
(wn1 , fJ1 , ?M1 ) =?cc ?N?M1 (c, wn1 , fJ1 )N?M1 (wn1 , fJ1 )Note that in case the mean ?
(wn1 , fJ1 , ?M1 ) is zero,we do not need the distribution p?M1 (c|wn1 , fJ1 ).
Thesmoothing parameters ?
and ?
are both set to 0.9.7 Experimental Results7.1 Task DescriptionWe perform translation experiments on the Chinese-English TC-Star task.
This is a broadcast newsspeech translation task used within the EuropeanUnion project TC-Star1.
The bilingual trainingdata consists of virtually all publicly available LDCChinese-English corpora.
The 6-gram languagemodel was trained on the English part of the bilin-gual training data and additional monolingual En-glish parts from the GigaWord corpus.
We use themodified Kneser-Ney discounting as implementedin the SRILM toolkit (Stolcke, 2002).Annual public evaluations are carried out for thistask within the TC-Star project.
We will report re-sults on manual transcriptions, i.e.
the so-called ver-batim condition, of the official evaluation test sets ofthe years 2006 and 2007.
There are two referencetranslations available for the development and testsets.
The corpus statistics are shown in Table 1.7.2 Translation ResultsIn Table 2, we present the translation resultsfor different training criteria for the development1http://www.tc-star.orgset and the two blind test sets.
The reportedcase-sensitive Bleu scores are computed usingthe mteval-v11b.pl2 tool using two referencetranslations, i.e.
BLEUr2n4c.
Note that already thebaseline system (MERT-Bleu) would have achievedthe first rank in the official TC-Star evaluation 2006;the best Bleu score in that evaluation was 16.1%.The MBR hypotheses were generated using thealgorithm described in (Ehling et al, 2007) on a10 000-best list.On the development data, the MERT-Bleuachieves the highest Bleu score.
This seems reason-able as it is the objective of this training criterion.The maximum likelihood (MLE) criteria performsomewhat worse under MAP decoding.
Interest-ingly, the MBR decoding can compensate this toa large extent: all criteria achieve a Bleu score ofabout 18.9% on the development set.
The bene-fits of MBR decoding become even more evidenton the two test sets.
Here, the MAP results for thesentence-level MLE criterion are rather poor com-pared to the MERT-Bleu.
Nevertheless, using MBRdecoding results in very similar Bleu scores for mostof the criteria on these two test sets.
We can there-fore support the claim of (Smith and Eisner, 2006)that MBR tends to have better generalization capa-bilities.The n-gram level MLE criterion seems to performbetter than the sentence-level MLE criterion, espe-cially on the test sets.
The reasons might be thatthere is no need for the use of pseudo referencesas described in Sec.
5 and that partial correctnessis taken into account.The best results are achieved using the expectedBleu score criteria described in Sec.
6.
Here, the sen-tence level and n-gram level variants achieve moreor less the same results.
The overall improvementon the Eval?06 set is about 1.0% Bleu absolute forMAP decoding and 0.9% for MBR decoding.
Onthe Eval?07 set, the improvements are even larger,about 1.8% Bleu absolute for MAP and 1.1% Bleufor MBR.
All these improvements are statisticallysignificant at the 99% level using a pairwise signifi-cance test3.Given that currently the most popular approach isto use MERT-Bleu MAP decoding, the overall im-2http://www.nist.gov/speech/tests/mt/resources/scoring.htm3The tool for computing the significance test was kindly pro-vided by the National Research Council Canada.529Table 2: Translation results: Bleu scores [%] for the Chinese-English TC-Star task for various trainingcriteria (MERT: minimum error rate training; MLE: maximum likelihood estimation; E[Bleu]: expectedBleu score) and the maximum a-posteriori (MAP) as well as the minimum Bayes risk (MBR) decision rule.Development Eval?06 Eval?07Decision Rule MAP MBR MAP MBR MAP MBRTraining Criterion MERT-Bleu (baseline) 19.5 19.4 16.7 17.2 22.2 23.0MLE sentence-level 17.8 18.9 14.8 17.1 18.9 22.7n-gram level 18.6 18.8 17.0 17.8 22.8 23.5E[Bleu] sentence-level 19.1 18.9 17.5 18.1 23.5 24.1n-gram level 18.6 18.8 17.7 17.6 24.0 24.0provement is about 1.4% absolute for the Eval?06set and 1.9% absolute on the Eval?07 set.Note that the MBR decision rule almost alwaysoutperforms theMAP decision rule.
In the rare caseswhere the MAP decision rule yields better results,the difference in terms of Bleu score are small andnot statistically significant.We also investigated the effect of the maximumn-gram order for the n-gram level maximum like-lihood estimation (MLE).
The results are shown inFigure 1.
We observe an increase of the Bleu scorewith increasing maximum n-gram order for the de-velopment corpus.
On the evaluation sets, however,the maximum is achieved if the maximum n-gramorder is limited to four.
This seems intuitive as theBleu score uses n-grams up to length four.
However,one should be careful here: the differences are rathersmall, so it might be just statistical noise.Some translation examples from the Eval?07 testset are shown in Table 3 for different training criteriaunder the maximum a-posteriori decision rule.8 ConclusionsWe have presented a systematic comparison of sev-eral criteria for training the log-linear parameters ofa statistical machine translation system.
Addition-ally, we have compared the maximum a-posterioriwith the minimum Bayes risk decision rule.We can conclude that the expected Bleu scoreis not only a theoretically sound training criterion,but also achieves the best results in terms of Bleuscore.
The improvement over a state-of-the-artMERT baseline is 1.3% Bleu absolute for the MAPdecision rule and 1.1% Bleu absolute for the MBRdecision rule for the large Chinese-English TC-Starspeech translation task.1 2 3 4 5 6 7 8 9max.
n-gram order141618202224Bleu[%]DevEval'06Eval'07Figure 1: Effect of the maximum n-gram order onthe Bleu score for the n-gram level maximum like-lihood estimation under the maximum a-posterioridecision rule.We presented two methods for computing the ex-pected Bleu score: a sentence-level and an n-gramlevel approach.
Both yield similar results.
We thinkthat the n-gram level computation has certain ad-vantages: The n-gram posterior probabilities couldbe computed from a word graph which would resultin more reliable estimates.
Whether this pays offin terms of translation quality is left open for futurework.Another interesting result of our experiments isthat the MBR decision rule seems to be less affectedby sub-optimal parameter settings.Although it is well-known that the MBR decisionrule is more appropriate than the MAP decision rule,the latter is more popular in the SMT community(and many other areas of natural language process-ing).
Our results show that it can be beneficial to530Table 3: Translation examples from the Eval?07 test set for different training criteria and the maximum a-posteriori decision rule.
(MERT: minimum error rate training, MLE-S: sentence-level maximum likelihoodestimation, E[Bleu]: sentence-level expected Bleu)Criterion TranslationReference 1 Saving Private Ryan ranks the third on the box office revenue list which is also a movie that ispossible to win an 1999 Oscar award2 Saving Private Ryan ranked third in the box office income is likely to compete in the nineteenninety-nine Oscar AwardsMERT-Bleu Saving private Ryan in box office income is possible ranked third in 1999 Oscar a filmMLE-S Saving private Ryan box office revenue ranked third is possible in 1999 Oscar a filmE[Bleu]-S Saving private Ryan ranked third in the box office income is also likely to run for the 1999Academy Awards a filmReference 1 The following problem is whether people in countries like China and Japan and other countrieswill choose Euros rather than US dollars in international business activities in the future2 The next question is whether China or Japan or other countries will choose to use Euros insteadof US dollars when they conduct international business in the futureMERT-Bleu The next question is in China or Japan international business activities in the future they will notuse the Euro dollarMLE-S The next question was either in China or Japan international business activities in the future theywill adopt the Euro instead of the dollarE[Bleu]-S The next question was in China or Japan in the international business activities in the future theywill adopt the Euro instead of the US dollarReference 1 The Chairman of the European Commission Jacques Santer pointed out in this September that thefinancial crisis that happened in Russia has not affected people?s confidence in adopting the Euro2 European Commission President Jacques Santer pointed out in September this year thatRussia?s financial crisis did not shake people?s confidence for planning the use of the EuroMERT-Bleu President of the European Commission Jacques Santer on September this year that the Russianfinancial crisis has not shaken people ?s confidence in the introduction of the EuroMLE-S President of the European Commission Jacques Santer September that the Russian financial crisishas not affected people ?s confidence in the introduction of the EuroE[Bleu]-S President of the European Commission Jacques Santer pointed out that Russia ?s financial crisislast September has not shaken people ?s confidence in the introduction of the EuroReference 1 After many years of friction between Dutch and French speaking Belgians all of them now hopeto emphasize their European identities2 After years of friction between Belgium?s Dutch-speaking and French-speaking people they nowall wish to emphasize their European identityMERT-Bleu Belgium?s Dutch-speaking and French-speaking after many years of civil strife emphasized thatthey now hope that EuropeansMLE-S Belgium?s Dutch-speaking and francophone after years of civil strife that they now hope thatEuropeansE[Bleu]-S Belgium?s Dutch-speaking and French-speaking after many years of civil strife it is now wantto emphasize their European identity531use the MBR decision rule.
On the other hand, thecomputation of the MBR hypotheses is more timeconsuming.
Therefore, it would be desirable to havea more efficient algorithm for computing the MBRhypotheses.AcknowledgmentsThis material is partly based upon work supportedby the Defense Advanced Research Projects Agency(DARPA) under Contract No.
HR0011-06-C-0023,and was partly funded by the European Union un-der the integrated project TC-STAR (Technologyand Corpora for Speech to Speech Translation, IST-2002-FP6-506738, http://www.tc-star.org).ReferencesSatanjeev Banerjee and Alon Lavie.
2005.
METEOR: An au-tomatic metric for MT evaluation with improved correlationwith human judgments.
In Proc.
Workshop on Intrinsic andExtrinsic Evaluation Measures for MT and/or Summariza-tion at the 43th Annual Meeting of the Association of Com-putational Linguistics (ACL), pages 65?72, Ann Arbor, MI,June.Peter F. Brown, John Cocke, Stephen A. Della Pietra, Vincent J.Della Pietra, Frederick Jelinek, John D. Lafferty, Robert L.Mercer, and Paul S. Roossin.
1990.
A statistical approach tomachine translation.
Computational Linguistics, 16(2):79?85, June.Chris Callison-Burch, Miles Osborne, and Philipp Koehn.2006.
Re-evaluating the role of BLEU in machine trans-lation research.
In Proc.
11th Conf.
of the Europ.
Chapterof the Assoc.
for Computational Linguistics (EACL), pages249?256, Trento, Italy, April.George Doddington.
2002.
Automatic evaluation of machinetranslation quality using n-gram co-occurrence statistics.
InProc.
ARPA Workshop on Human Language Technology.Nicola Ehling, Richard Zens, and Hermann Ney.
2007.
Mini-mum Bayes risk decoding for BLEU.
In Proc.
45th AnnualMeeting of the Assoc.
for Computational Linguistics (ACL):Poster Session, Prague, Czech Republic, June.Philipp Koehn.
2004.
Pharaoh: a beam search decoder forphrase-based statistical machine translation models.
In Proc.6th Conf.
of the Assoc.
for Machine Translation in the Amer-icas (AMTA), pages 115?124, Washington DC, Septem-ber/October.Shankar Kumar and William Byrne.
2004.
Minimum Bayes-risk decoding for statistical machine translation.
In Proc.Human Language Technology Conf.
/ North American Chap-ter of the Assoc.
for Computational Linguistics Annual Meet-ing (HLT-NAACL), pages 169?176, Boston, MA, May.Chin-Yew Lin and Franz Josef Och.
2004.
Orange: a methodfor evaluating automatic evaluation metrics for machinetranslation.
In Proc.
COLING ?04: The 20th Int.
Conf.on Computational Linguistics, pages 501?507, Geneva,Switzerland, August.Arne Mauser, Richard Zens, Evgeny Matusov, Sas?a Hasan,and Hermann Ney.
2006.
The RWTH statistical machinetranslation system for the IWSLT 2006 evaluation.
In Proc.Int.
Workshop on Spoken Language Translation (IWSLT),pages 103?110, Kyoto, Japan, November.Hermann Ney.
2001.
Stochastic modelling: from patternclassification to language translation.
In Proc.
39th AnnualMeeting of the Assoc.
for Computational Linguistics (ACL):Workshop on Data-Driven Machine Translation, pages 1?5,Morristown, NJ, July.Franz Josef Och and Hermann Ney.
2002.
Discriminative train-ing and maximum entropy models for statistical machinetranslation.
In Proc.
40th Annual Meeting of the Assoc.
forComputational Linguistics (ACL), pages 295?302, Philadel-phia, PA, July.Franz Josef Och.
2003.
Minimum error rate training in statisti-cal machine translation.
In Proc.
41st Annual Meeting of theAssoc.
for Computational Linguistics (ACL), pages 160?167,Sapporo, Japan, July.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-JingZhu.
2002.
Bleu: a method for automatic evaluation of ma-chine translation.
In Proc.
40th Annual Meeting of the As-soc.
for Computational Linguistics (ACL), pages 311?318,Philadelphia, PA, July.William H. Press, Saul A. Teukolsky, William T. Vetterling, andBrian P. Flannery.
2002.
Numerical Recipes in C++.
Cam-bridge University Press, Cambridge, UK.Libin Shen, Anoop Sarkar, and Franz Josef Och.
2004.
Dis-criminative reranking for machine translation.
In Proc.
Hu-man Language Technology Conf.
/ North American Chapterof the Assoc.
for Computational Linguistics Annual Meeting(HLT-NAACL), pages 177?184, Boston, MA, May.David A. Smith and Jason Eisner.
2006.
Minimum risk anneal-ing for training log-linear models.
In Proc.
21st Int.
Conf.on Computational Linguistics and 44th Annual Meeting ofthe Assoc.
for Computational Linguistics (COLING/ACL):Poster Session, pages 787?794, Sydney, Australia, July.Andreas Stolcke.
2002.
SRILM ?
an extensible language mod-eling toolkit.
In Proc.
Int.
Conf.
on Speech and LanguageProcessing (ICSLP), volume 2, pages 901?904, Denver, CO,September.Christoph Tillmann and Tong Zhang.
2006.
A discriminativeglobal training algorithm for statistical MT.
In Proc.
21stInt.
Conf.
on Computational Linguistics and 44th AnnualMeeting of the Assoc.
for Computational Linguistics (COL-ING/ACL), pages 721?728, Sydney, Australia, July.Joseph P. Turian, Luke Shen, and I. Dan Melamed.
2003.
Eval-uation of machine translation and its evaluation.
TechnicalReport Proteus technical report 03-005, Computer ScienceDepartment, New York University.Ashish Venugopal, Andreas Zollmann, and Alex Waibel.
2005.Training and evaluating error minimization rules for statis-tical machine translation.
In Proc.
43rd Annual Meeting ofthe Assoc.
for Computational Linguistics (ACL): Workshopon Building and Using Parallel Texts: Data-Driven MachineTranslation and Beyond, pages 208?215, Ann Arbor, MI,June.Richard Zens and Hermann Ney.
2006.
N -gram posterior prob-abilities for statistical machine translation.
In Proc.
HumanLanguage Technology Conf.
/ North American Chapter of theAssoc.
for Computational Linguistics Annual Meeting (HLT-NAACL): Proc.
Workshop on Statistical Machine Transla-tion, pages 72?77, New York City, NY, June.532
