A Concept-based Adaptive Approach to Word SenseDisambiguationJen Nan Chen Jason S. ChangDepartment of Computer Science Department ofComputer ScienceNational Tsing Hua University National Tsing Hua UniversityHsinchu 30043, Taiwan Hsinchu 30043, Taiwanjnchen@mcu.edu.tw jschang@cs.nthu.edu.twAbstractWord sense disambiguation forunrestricted text is one of the most difficulttasks in the fields of computationallinguistics.
The crux of the problem is todiscover a model that relates the intendedsense of a word with its context.
Thispaper describes a general framework foradaptive conceptual word sensedisambiguation.
Central to this WSDframework is the sense division andsemantic relations based on topicalanalysis of dictionary sense definitions.The process begins with an initialdisambiguation step using an MRD-derived knowledge base.
An adaptationstep follows to combine the initialknowledge base with knowledge gleanedfrom the partial disambiguated text.
Oncethe knowledge base is adjusted to suit thetext at hand, it is then applied to the textagain to finalize the disambiguation result.Definitions and example sentences fromLDOCE are employed as training materialsfor WSD, while passages from the Browncorpus and Wall Street Journal are used fortesting.
We report on several experimentsillustrating effectiveness of the adaptiveapproach.1 IntroductionWord sense disambiguation for unrestricted textis one of the most difficult tasks in the fields ofcomputational linguistics.
The crux of theproblem is to discover a model that relates theintended sense of a word with its context.
Itseems to be very difficult, if not impossible, tostatistically acquire enough word-basedknowledge about a language necessary tobuild arobust system capable of automaticallydisambiguating senses in unrestricted text.
Forsuch a system to be effective, a great deal ofbalanced materials must be assembled in orderto cover many idiosyncratic aspects of thelanguage.
There exist three issues in alexicalized statistical word sense disambiguation(WSD) model - data sparseness, lack of a levelof abstraction, and static learning strategy.First, word-based models have a plethora ofparameters that are difficult to estimate reliablyeven with a very large corpus.
Under-trainedmodels lead to low precision.
Second, word-based models lack a degree of abstraction that iscrucial for a broad coverage system.
Third, astatic WSD model is unlikely to be robust andportable, since it is very difficult to make asingle static model relevant to a wide variety ofunrestricted texts.
Recent WSD systems havebeen developed using word-based model forspecific limited domain to disambiguate s nsesappearing in usually easy context (Leacock,Towell, and Voorlees 1996) with a lot of typicalsalient words.
For unrestricted text, however,the context tends to be very diverse and difficultto capture with a lexicalized model, therefore acorpus-trained system is unlikely to port to newdomains and run off the shelf.Generality and adaptiveness are thereforekey to a robust and portable WSD system.
Aconcept-based model for WSD requires lessparameter and has an element of generality builtin (Liddy and Paik 1993).
Conceptual classesmake it possible to generalize from word-specific context in order to disambiguate a wordsense appearing in a particularly unfamiliarcontext in term of word recurrences.
Anadaptive system armed with an initial lexical andconceptual knowledge base extracted frommachine-readable dictionaries (MRDs), has twostrong advantages over static lexicalized modelstrained using a corpus.
First, the initial237knowledge is rich and unbiased such that asubstantial portion of text can be disambiguatedprecisely.
Second, based on the result of initialdisambiguated text.
Subsequently, theknowledge base is adjusted to suit the text athand.
The adjusted knowledge base is thenMachine Readable Dictionary \]Machine Readable ThesaurusInitialized Knowledge BaseWord Sense Lexical nd Conceptual Context.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.bank-GEO river lake land ...GEOMOTIONbank-MONEY money account bill ...MONEY COMMERCE ... ~ed Text ~ ~ f Partially Tagged Text "l /~ f  12" ~n?~te~Is~:: =fff~nb~hbeaknf~" " t  ~ 21 Iootedst ud~ "-- ' I. investig,ot~:na, f~lY/c?h.
,cck.~u~CR2ME .- ~ //?
I 3. adeer near the river bank.,.
| ~ /  " I 3. a deer/ANIMAL near the river banldGEO ... Ii \[, 4 Ab..~,olc J i ( " 4.
A bank~ ,o,e )/, ~ Adapted Knowledge BaseIWord_Sen:: Lexical nd Conceptual ContextNI~ bank-GEO river lake land deer near... IGEO MOTION- -~ I~L  ... \[ NI bank-MONEY money account b~gat ion  check fraud i \MONEY COMMERCE CRIME... - -  j \)WSD Result \]f l  .... investigation of bank/MONEY check fraud/CRiME... ~ / / /| 2 .... looted/CRIME stores and robbed/CRIME banks/MONEY ....
I /| 3 .... a deer/ANIMAL near the river bank/GEO ..
I ./ 4.
A bank/GEO vole/ANIMAL / ,er"- kFigure I General framework for WSD using MRD.disambiguation, an adaptation step is taken tomake the knowledge base more relevant o thetask at hand, leading to broader and moreprecise WSD.Figure 1 lays out the general framework foran adaptive conceptual WSD approach, underwhich this research is being carried out.
Thelearning process described here begins with astep of knowledge acquisition from MRDs.With the acquired knowledge, the system readsthe input text and starts the step of initialdisambiguation.
Adaptive step follows tocombine the initial knowledge base withknowledge gleaned from the partiallyapplied to the text again to finalize thedisambiguation result.
For instance, Figure 1shows the initial contextual representation (CR)extracted from the Longrnan Dictionary ofContemporary English (Protor 1978, LDOCE)for the GEO-bank sense contained both lexicaland conceptual information: {land, river,lake, ...} u {GEO, MOTION .... }.
The initialCR is informative nough to disambiguate apassage containing a deer near the river bank inthe input text.
The initial disambiguation stepproduces ense tagging of deer~ANIMAL andbank~GEOGRAPHY, but certain instances ofbank are left untagged for lack of relevant WSD238knowledge.
For instance, the GEO-bank sensein the context of vole is unresolved since there isno information linking ANIMAL context toGEOGRAPHY sense of bank.
The adaptationstep adds deer and ANIMAL to the contextualrepresentation for GEO-bank.
The enrichedCR therefore contains information capable ofdisambiguating the instance of bank in thecontext of vole to produce final disambiguationresult.2 Acquiring Conceptual Knowledgefrom MRDIn this section we apply a so-called TopSensealgorithm (Chen and Chang 1998) to acquire CRfor MRD senses.
The current implementationof TopSense uses the topical information inLongman Lexicon of Contemporary English(McArthur 1992, LLOCE) to represent WSDknowledge for LDOCE senses.
In thefollowing subsections we describe how that isdone.2.1 Contextual Representation fromMRDsDictionary is a text whose subject matter is alanguage.
The purpose of dictionary is toprovide definitions of word senses, and in theprocess it supply knowledge not just about thelanguage, but the world (Wilks et al 1990).
Agood-sized dictionary usually has a largevocabulary and good coverage of word sensesuseful for WSD.
However, short MRDdefinitions and examples per se lack a level ofabstraction to function effectively as acontextual representation of word sense.
Onthe other hand, the thesaurus organizes wordsenses into a fixed set of coarse semanticcategories and thus could potentially be usefulas the basis of a conceptual CR of word sense.To get the best of both worlds of dictionary andthesaurus, we propose to link an MRD sense tothesaurus categories to produce conceptualrepresentation of its context.
Content wordsextracted irectly from the definition sentence ofa word sense can be put to use as the word-levelcontextual representation f that particular wordsense.One way of producing such conceptual CRis to link MRD senses to their relevant hesaurussenses and categories.
These links furnish theMRD senses with information necessary forbuilding a conceptual CR.
We will describeone such approach under which each MRDsense is linked to a relevant thesaurus enseaccording to its defining words.
The linkedthesaurus sense, unlike the isolated MDR sense,falls within a certain semantic category.Consequently, we can establish relationsbetween defining words and semantic ategorythat eventually lead to conceptual CR.With the word lists in a thesaurus categorycast as a document representing a certain subjectmatter or topic, the task of constructingconceptual representation f context for a certainMRD sense bears a striking resemblance to thedocument retrieval task in information retrieval(IR) research.
Relatively well-established IRtechniques of weighting terms and rankingdocuments are applied to build a list of topicsthat are most relevant o the definition of eachMRD sense.
This list of ranked topics, for aparticular word sense, forms a vectorizedconceptual representation f context in the spaceof all possible topics.2.2 Illustrative ExampleOne example is given in this subsection toillustrate how TopSense works.Example 1.
Conceptual representation of anLDOCE senseerane.l.n.1, a machine for lifting and movingheavy objects by means of a verystrong rope or wire fastened to amovable arm (JIB).For the most relevant topics to fine-grainedsense, we get the following ranked list Hd(EQUIPMENT), Ha (MATERIALS), Ma(MOVING).Furthermore, the definition and examplesof a particular sense on the surface level seldomare information sufficient o represent context ofthe sense.
For instance, the words machine, lift,move, heavy, object, strong, rope, wire, fasten,movable, arm, jib in the definition of the sense,crane.l.n.1, are hardly enough contextualinformation to resolve a crane.l.n.1 instance inthe Brown corpus shown below:Unsinkable slowed and stopped, hundreds ofbrilliant white flares swayed eerily down from239the black, the air raid sirens ashore rose in akeening shriek, the anti-aircraft guns coughedand chattered- and above it all motors roaredand the bombs came whispering and wailingand crashing down among the ships at anchor atBad.
They had come from airports in theBalkans, these hundred-odd Junkers 88's.They had winged over the Adriatic, they hadtaken Bari by complete surprise and now theywere battering her, attacking with deadly skill.They had ruined the radar warning system withtheir window, they had made themselvesinvisible above their flares.
And they also hadthe lights of the city, the port wall lanterns, anda shore crane's potlight to guide on.However, with a level of abstraction madepossible by using a thesaurus, it is not difficultto build a conceptual CR of word sense, which isintuitively more effective for WSD.
Forinstance, based on LLOCE topics, theconceptual CR (EQUIPMENT, MATERIALS,MOVING) derived from the definition ofcrane.l.n.1, is general enough to characterizemany salient words appearing in the context ofthe crane.l.n.1 instance, including motor(EQUIPMENT), lantern (EQUIPMENT), andflare (EQUIPMENT, MATERIALS).3 The Adapt ive WSD Algor i thmWe sum up the above descriptions and outlinethe procedure for the algorithm in this section.In what follows an adaptive disambiguationalgorithm based on class-based approach will bedescribed.
Next, we give an illustrativeexample to show how the proposed algorithmworks for unrestricted text.3.1 The a lgor i thmThe proposed algorithm starts with the step ofinitial disambiguation using the contextualrepresentation CR(W, S) derived from the MRDfor the sense S of the head entry W. A step ofadaptation followed to produce a knowledgebase from the partially disambiguated text.Finally, the undisambiguated part isdisambiguated according to the newly acquiredknowledge base.
The following algorithmgives a formal and detailed description ofadaptive WSD.Algorithm AdaptSenseStep I: Preprocess the context and produce alist of lemmatized content wordsCON(W) in a polysemous word W'scontext.Step 2: For each sense S of W, compute thesimilarity between the contextrepresentation CR(W, S) and topicalcontext CON(W).Sim (CR(W, S), CON(W))E (w,., + w, ) whereteME w,,+ E w,'tGCR(W.S) " t~CON(W)M = CR(W, S') N CON(W),Wt, s = weight of a contextual word twith sense S in CR(W, S),1W t = weight oft  in CON(W) = .\[\]~.lX, = distance from t to W in number ofwords.Step 3: For each word W, choose a relevantsense Sw if passes a preset hreshold thenconstruct triples T={(W, S, CON(W))}.Step4: Compute a new set of contextualrepresentation CR(W,S) = { u \[ ueCON(W)and (W, S, CON(W))e T }Step S: Infer remaining less relevant sense forW in CON3.2 Illustrative ExampleConsider the following passage from the Browncorpus:... Of cattle in a pasture without hrowin' 'emtogether for the purpose was called a "pasturecount".
The counters rode through the pasturecountin' each bunch of grazin' cattle, and driftedit back so that it didn't get mixed with theuncounted cattle ahead.
This method of countin'was usually done at the request, and in thepresence, of a representative of the bank thatheld the papers against the herd.
The notes andmortgages were spoken of as "cattle paper".A "book count" was the sellin' of cattle by thebooks, commonly resorted to in the early days,sometimes much to the profit of the seller.
Thisled to the famous ayin' in the Northwest of the"books won't freeze".
This became a commonbyword durin' the ...In our experiment, we observed that holdand paper are related to both MONEY andROAD sense in the initial knowledge base.240Thus, this instance of bank is leftunresolved in the initial disambiguationstep.
The adaptation step discovers thatboth hold and paper co-occur with someMONEY-bank instances in the partiallydisambiguated text.
Therefore, thesystem is able to correctly resolve this bankinstance to MONEY sense.4 Exper iments  and Discussions4.1 Exper imentIn our experiment, we use the materials of textwindows of 50 words to the left and 50 words tothe right of thirteen polysemous words in theBrown corpus and a sample of Wall StreetJournal articles.
All instances of these thirteenwords are first disambiguated by two humanjudges.
For these thirteen words underinvestigation, only nominal senses areconsidered.
The experimental results show thatthe adaptive algorithm disambiguated correctly71% and 77% of these test cases in the Browncorpus and the WSJ sample.
Table 1 providesfurther details.
However, there are still roomfor improvement in the area of precision.Evidence have shown that by exploiting theconstraint of so-called "one sense perdiscourse," (Gale, Church and Yarowsky 1992b)and the strategy of bootstrapping (Yarowsky1995), it is possible to boost coverage, whilemaintaining about he same level of precision.4.2 Discuss ionsAlthough it is often difficult to compare studieson different ext domain, genre and experimentalsetup, the approach presented here seems tocompare favorably with the experimental resultsreported in previous WSD research.
Luk (1995)experiments with the same words we use exceptthe word bank and reports that there are totally616 instances of these words in the Browncorpus, (slightly less than the 749 instances wehave experimented on).
The author eports that60% of instances are resolved correctly usingthe definition-based concept co-occurrence(DBCC) approach.
Leacock et al (1996)report that precision rate of 76% fordisambiguating the word line in a sample ofWSJ articles.One of the limiting factors of this approachis the quality of sense definition in the MRD.Short and vague definitions tend to lead toinclusion of inappropriate topics in thecontextual representation.
Using inferior CR, itis not possible to produce enough and precisesamples in the initial step for subsequentadaptation.Table l(a) Disambiguation results for thirteenambiguous words in Brown corpus.Word # ofsensesbank 8bass 2bow 5:cone 2duty 2!galley 3l interest 4issue 4\]mole 2sentence 2slug 5star 6taste 3TotalPrecision# ofinstances9716121475434614143284651846Without Withadaptation adaptation# of correct68 7116 163 314 1467 694 4213 22867 882 230 304 628 2936 36552 59665.2% 70.5%Table l(b) Disambiguation resultsambiguous words inJournal articles.Word # ofsensesbank 8bass 2bow 5one 2duty 2galley 3interest 4issue 4mole 2sentence 2slug 5star 6taste 3TotalPrecisionfor thirteenWall Street# ofinstances370252212601276903Without Withadaptation adaptation# of correct350 3532 219 22123 127181 17711 123 23 3692 69876.6% 77.3%241The experiment and evaluation shows thatadaptation is most effective when a high-frequency word with topically contrasting sensesis involved.
For low-frequency senses uch asEARTH, ROW, and ROAD senses of bank, theapproach does not seem to be very effective.For instance the following passage containing aninstance of bank has the ROW sense but ouralgorithm fails to disambiguate it....
They slept- Mynheer with a marvelouslyhigh-pitched snoring, the damn seahorse ivoryteeth watching him from a bedside table.
Inthe ballroom below, the dark had given way tomoonlight coming in through the bank offrench windows, it was a delayed moon, butnow the sky had cleared of scudding blackand the stars sugared the silver-gray sky.Martha Schuyler, old, slow, careful of foot,came down the great staircase, dressed in herbest lace-drawn black silk, her jeweled shoebuckles held forward.Non-topical sense like ROW-bank canappeared in many situations, thus are verydifficult to captured using a topical contextualrepresentation.
Local contextual representationmight be more effective.Infrequent and non-topical senses areproblematic due to data sparseness.
However,that is not specific to the adaptive approach, allother approaches in the literature suffer the samepredicament.
Even with a static knowledgeacquired from a very large corpus, these senseswere disambiguated at a considerably ower rate.S Related approachesIn this section, we review recent WSD literaturefrom the prospective of types of contextualknowledge and different representationalschemes.5.1 Topical  vs. Local  Representat ion ofContext5.1.1 Topical ContextWith topical representation of context, thecontext of a given sense is reviewed as a bag ofwords without structure.
Gale, Church andYarowsky (1992a) experiment on acquiringtopical context from substantial bilingualtraining corpus and report good results.5.1.2 Local ContextLocal context includes the structuredinformation on word order, distance, andsyntactic feature.
For instance, the localcontent of a line from does not suggest the samesense for the word line as a line for does.Brown et al (1990) use the trigram model as away of resolving sense ambiguity for lexicalselection in statistical machine translation.This model makes the assumption that only theprevious two words have any effect on thetranslation, thus word sense, of the next word.The model attacks the problem of lexicalambiguity and produces satisfactory results,under some strong assumption.
A majorproblem with trigram model is that of longdistance dependency.
Dagan and Itai (1994)indicate that two languages are more informativethan one; an English corpus is very helpful indisambiguating polysemous words in Hebrewtext.
Local context in the form of lexicalrelations are identified in a very large corpus.Brown, et al (1991) describe a statisticalalgorithm for partitioning word senses into twogroups.
The authors use mutual information tofind a contextual feature that most reliablyindicates which of the senses of the Frenchambiguous word is used.
The authors report a20% improvement in the performance of amachine translation system when the words arefirst disambiguated this way.5.2 Static vs. Adaptive StrategyOf the recent WSD systems proposed in theliterature, almost all have the property that theknowledge is fixed when the system completesthe training phase.
That means the acquiredknowledge never expands during the course ofdisambiguation.
Gale, et al (1992a) report hatif one had obtained a set of training materialswith errors no more than twenty to thirty percent,one could iterate training materials election justonce or twice and have training sets that had lessthan ten percent errors.
The adaptive approachis somehow similar to their idea of incrementallearning and to the bootstrap approach proposedby Yarowsky (1995).
However, bothapproaches are still considered static modelswhich are changed only in the training phase.2426 ConclusionsWe have described a new adaptive approach toword sense disambiguation.
Under thislearning strategy, first contextual representationfor each word sense is built from the sensedefinition in MRD and represented as aweighted-vector of concepts represented aswordlists in a thesaurus.
Then the knowledge baseis applied to the text for WSD in an adaptivefashion to improve on disambiguation precision.We have demonstrated that this approach as thepotential of outperforming established staticapproaches.
This performance is achieveddespite the fact no lengthy training time or avery large corpus is required.
It is evident hatthe WSD algorithms proposed herein are simple,take up little time and space, and mostimportantly, require no human intervention i allphases of WSD.
Sense tagging of trainingmaterial, knowledge acquisition from trainingdata, and disambiguation all are doneautomatically.AcknowledgementsThis work is partially supported by ROC NSCgrants 84-2213-E-007-023 and NSC 85-2213-E-007-042.
We are grateful to Betty Teng andNora Liu from Longman Asia Limited for thepermission to use their lexicographical resourcesfor research purpose.
Finally, we would like tothank the anonymous reviewers for manyconstructive and insightful suggestions.ReferencesBrown, P. F., S. A. Della Pietra, V. J. Della Pietra,and R. L. Mercer (1991).
Word-sensedisambiguation using statistical methods.
InProceedings of the 29th Annual Meeting of theAssociation for Computational Linguistics, pp 264-270.Chen, J. N. and J. S. Chang (1998).
Topicalclustering of MRD senses based on informationretrieval techniques.
Special Issue on Word SenseDisambiguation, Computational Linguistics, 24(1),pp 61-95.Dagan, I. and A. Itai (1994).
Word SenseDisambiguation Using a second languagemonolingual corpus.
Computational Linguistics,20(4), pp 563-596.Gale, W. A., K. W. Church, and D. Yarowsky(1992a).
Using bilingual materials to develop wordsense disambiguation methods.
In Proceedings ofthe 4th International Conference on Theoreticaland Methodological Issues in Machine Translation,pp 101-112.Gale, W. A., K. W. Church and D. Yarowsky(1992b).
One sense per discourse.
In Proceedingsof the Speech and Natural Language Workshop, pp233-237.Leacock, C., G. Towell, and E. M. Voorlees (1996).Towards building contextual representations ofword senses using statistical models.
In B.Boguraev and J. Pustejovsky, editor, CorpusProcessing for Lexical Acquisition.
MIT Press,Cambridge, MA.Liddy, E. D. and W. Paik (1993).
Document filteringusing semantic information from a machinereadable dictionary.
In Proceedings of theWorkshop on Very Large Corpora, pp 20-29.Luk, A. K. (1995).
Statistical sense disambiguationwith relatively small corpora using dictionarydefinitions.
In Proceedings of the 33rd AnnualMeeting of the Association for ComputationalLinguistics, pp 181-188.McArthur, T. (1992).
Longman lexicon ofcontemporary English.
Longman Group (Far East)Ltd., Hong Kong.Proctor, P.
(ed.)
(1978).
Longman dictionary ofcontemporary English.
Harlow: Longrnan Group.Wilks, Y.
A., D. C. Fass, C. M. Guo, J. E. McDonald,T.
Plate, and B. M. Slator (1990).
Providingtractable dictionary tools.
Machine Translation, 5,pp 99-154.Yarowsky, D. (1995).
Unsupervised word sensedisambiguation rivaling supervised methods.
InProceedings of the 33rd Annual Meeting of theAssociation for Computational Linguistics, pp 189-196.243
