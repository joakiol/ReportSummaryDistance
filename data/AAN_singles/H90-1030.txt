Management and Evaluation of InteractiveDialog in the Air Travel DomainLewis M. Norton, Deborah A. Dahl, Donald P. McKay~Lynette Hirschman~ Marcia C. Linebarger~ David Magerman~and Catherine N. BallIntroductionThis paper presents the Unisys Spoken Language Sys-tem, as applied to the Air Travel Planning (ATIS)domain.
1 This domain provides a rich source of inter-active dialog, and has been chosen as a common appli-cation task for the development and evaluation of spo-ken language understanding systems.
The Unisys ap-proach to developing a spoken language system combinesSUMMIT (the MIT speech recognition system \[6\]), PUN-DIT (the Unisys language understanding system \[3\]) andan Ingres database of air travel information for elevencities and nine airports (the ATIS database).
Accessto the database is mediated via a general knowledge-base/database interface (the Intelligent Database Server\[4\]).
To date, we have concentrated on the language un-derstanding and database interface components.A Dialog Manager integrates the overall user-systeminteraction.
The Dialog Manager accepts user requestsin the form of strings of words and calls PUNDIT to in-terpret the input; it then calls the database indirectly,via the IDS database interface.
An important functionof the Dialog Manager is to maintain a record of thediscourse context, so that the system can successfullyprocess connected ialog.We first describe our architecture in more detail, thengive a short discussion of dialog management, a topic wefeel will be crucial to successful systems interacting withusers via natural anguage.
We conclude with the pre-sentation and analysis of results from the ATIS commonevaluation task and from data gathered at Unisys usingour system.Unisys DefenSe SystemsCenter for Advanced Information TechnologyPO Box 517Paoli, PA 19301of that input.
If no interpretation is forthcoming, theuser is notified; otherwise the interpretation is passed toa module called QTIP (Query Translation and InterfaceProgram), which attempts to create a database querycorresponding to the request.
QTIP does not produceSQL code directly, instead, communication with Ingresis done via an Intelligent Database Server \[4\], developedon another DARPA contract, which we describe in thenext section.In te l l igent  Database  ServerThe ATIS Intelligent Database Server (see Figure 2) con-sists of the Intelligent Database Interface (IDI) and aserver supporting the interaction beteen QTIP and a rela-tional database.
The IDI provides a logic-based languagefor queries and transparent support for the actual inter-action with an Ingres DBMS.
The server supports theinteraction with a logic-based query generator (for PUN-DIT, qTIP; for the MIT system, TINA \[5\]).
It providesinput/output conversion between Ingres and Prolog orLisp (the languages of choice for language understandingsystems), commands for selecting databases, informativeDialog ~PUNDITArchitectureSystem Leve l  Arch i tec tureOur system architecture (see Figure 1) is based on a Dia-log Manager, and is taken from a previous application fornavigating around Cambridge, Massachusetts \[1\].
Themajor difference is that the module providing answersfor direction assistance was an expert system, while hereit is a database.
The Dialog Manager, upon receiving aninput from the user, calls PUNDIT for an  interpretationZThis work was supported by DARPA contract N000014-89-C0171, administered by the Office of Naval Research.USERINGRESATI$ DBFigure 1: Overall System Architecture141ID I  Server  Arch i tec turePUNDI"(QT IP )ID IL  QuerySOL QueryPro log  Tup lesSta?llstiEcsili!ii~i!i i~iiiiiilill i ii i!i!ii!i!
!
i!
i i i i i i \[ i i i i i i i i i !
i i i i i i!
i i i i i i!!
!
:i !:!
i i:!:i:i:i:i:i:i:i:i:i:i:i:i:i:i:i:i:!
:i:i:i:i:i:i:i:o Manages  conneot lon  to IngresOAG databaseo Trans la tes  ID IL  quer ies  to SOLo Executes  SOl_  query  on  IngremOAG databaseo Conver ts  Ingres  tab le  outputto l ists of  l istso Opt iona l ly  ool leots  s tat is t icso Opt iona l ly  caohes  resu l tsSQL QueryIngres  Tup les, IFigure 2: Intelligent Database Server ArchitectureINGRESOAGDBstatistics and generation of comparator output.The Intelligent Database Interface is a portable, cache-based interface providing transparent, efficient access toone or more databases on one or more remote databasemanagement systems (DBMS) which support SQL.
TheIDI was designed to be compatible with a logic-basedknowledge representation scheme and has been used toimplement a query server supporting the ATIS databaseaccess for PUNDIT.
The query language of the IDI isthe Intelligent Database Interface Language (IDIL) and isbased on a restricted subset of function-free Horn clauseswhere the head of a clause represents the target list (i.e.,the form of the result relation) and the body is a con-junction of literals which denote database relations oroperations on the relations and/or their attributes (e.g.,negation, aggregation, and arithmetic operations).The Intelligent Database Server supports QTIP's in-teraction with the ATIS database, accepting IDIL queries,using the IDI to translate and execute the queries andreturning results as straightforward lists of lists.
TheIDI Server supports QTIP'S interaction with a relationaldatabase in several specific ways; the IDI Server?
accepts an IDIL query as input and returns Prologtuples or Lisp tuples, the translated SQL query andstatistics;?
translates IDIL queries to SQL and executes them onthe database;?
manages connections transparently to the Ingresdatabase;?
converts Ingres tuple output to Prolog or Lisp tu-ples, and?
produces cAs evaluation output.The IDI architecture also contains a cache; the IDIcurrently caches results of database queries.
Our cachingconcept also includes the notion of advice provided to thecache.
While we have not used it in this application todate, we believe that there are many useful heuristics inthe travel planning domain that can lead to optimizedDB retrieval strategies, using the cache for query gen-eralizations, pre-fetching and replacement.
We plan tocollect statistics on ATIS transactions which could thenbe used to define an effective advice language and strat-egy.Overview of the Dialog ManagerThe Dialog Manager oversees the interaction of the sys-tem with the user.
It also oversees the communicationbetween the language understanding subsystem and thedatabase, via QTIP.
QTIP reports back to the DialogManager, returning both the IDIL query and the zero ormore tuples which it received from the IDS (or a diag-nostic message containing information explaining why itdidn't make a database call).
The Dialog Manager thendoes two things: it presents the answer to the user, usinginformation from the IDIL query to format the tuples inthe answer and to generate column headings for them;and it retains the answer in a data structure representingthe discourse context.
The latter action is what makes itpossible for our system to handle certain types of refer-ence by the user to material in answers.
The basic ideaof having the Dialog Manager store responses was de-veloped for the direction-assistance application.
In thattask, the expert system responded in English sentences,and the responses were processed by PUNDIT and theirinterpretations stored in the discourse context.
For ATIS,the DB answers are kept, and mechanisms for referenc-ing data in that form have been added to PUNDIT, toenable it to interpret subsequent inputs in the contextof both previous user queries and previous system re-sponses.
This is illustrated in the next section.142Dialog ManagementReference ResolutionPUNDIT has for some time had the ability to  resolveanaphoric references of various kinds, including pronom-inal reference ("that one") and definite noun anaphors("the Delta flight", "those flights").
This capability wasused in the direction-assistance application, and it ispresent also for ATIS.
We have made no major exten-sions to this capability for ATIS.DeixisPresenting tabular responses encourages the user to re-fer to  entries in those responses.
This introduces theneed for a different kind of contextual reference.
For in-stance, the code "LH" is used in the ATIS DB both fora (relatively rare) fare class and for Lufthansa GermanAirlines.
If a user asks what "LH" means, the coopera-tive response is not to give both interpretations, but togive the interpretation corresponding to its use in a pre-vious response, which will be in an "airline" column ora "fare class" column, rarely both.
A more interestingexample involves expressions like "Delta flight 123".
Inthe general case, that flight has several legs, say fromCityl to City2 to City3.
If a user has asked for flightsfrom Cityl to  City2, and then asks for fares for one ofthem, it is not cooperative to respond with fares not onlyfrom Cityl to City2, but also from City2 to City3 andfrom Cityl to City3, just because the fight in questiongoes on to  City3.
It is quite clear from the context thatthe user wants only fares from Cityl to  City2; specifi-cally, Delta flight 123 will have been found in a previousanswer tuple for just the Cityl to City2 leg.
Both ofthe previous examples involve examination of entries inprevious responses.
Our system has been extended tohandle the "Delta flight 123" example as a demonstra-tion of this kind of capability; we plan to  add the abilityto  handle other such contextual references in the nearfuture.The mechanisms involved in handling such contextualreferences can be illustrated by how the "Delta flight123" example proceeds.
When the table with flightsfrom Cityl to  City2 is returned, a discourse entity repre-senting it  is added to  the discourse context, but withoutrepresenting any individual flights as discourse entities.A semantics for the table is also provided, which cor-responds to something like "table of Delta flights fromCityl to City2".
When "Delta flight 123" is subsequentlyreferred to, PUNDIT tries to find a flight in the contextwith that flight number and airline (we can also handlejust "flight 123"), by searching the tables in the discoursecontext whose semantics are consistent with those of theflight referred to.DiagnosesThe Dialog Manager is also responsible for enhancingthe cooperativeness of the system.
It  has a primitivegeneration capability (based on sentence templates andkeywords) to provide English diagnoses of failures.
Forinstance, if a user asks for flights leaving after "eighto'clock", QTIP doesn't know if the DB query should spec-ify 8 a.m. or 8 p.m.
The optimum action is to request theuser to resolve the ambiguity.
Our system at least tellsthe user it couldn't decide between a.m. or p.m. An-other example involves queries "just outside" or "on thefringe of '  the database.
The DB contains informationon ground transportation between cities and the airportsserving them, but not on ground transportation betweentwo cities or between two airports.
If a query requestinginformation of the latter kind is properly understood, itis preferable for the system to tell the user as specificallyas possible why the query cannot be answered, ratherthan to go through the motions of making a call to  thedatabase and returning a LLtable" with no tuples.
Oursystem issues a message stating that ground transporta-tion is between an airport and a city, not between twocities or airports.EvaluationCommon EvaluationThe first experiment using our system that we reporton is the common evaluation task.
Most of the discoursefeatures we discussed in the previous section do not comeinto play for this task, because it involves testing the sys-tem on requests which are entirely self-contained, withinthe bounds of the domain, and unambiguous-so-called"class A" sentences.
Thus there is no need for resolu-tion of anaphoric expressions, reference to previous an-swer tables, or diagnoses of out-of-domain requests forthe June 1990 common evaluation.
Furthermore, the an-swers for evaluation of sentences in context were not uni-formly available for the June evaluation.
For these rea-sons, we will not report on an evaluation of sentences incontext a t  this time.
In the following section, we presentour results for the common ATIS evaluation task, alongwith an analysis of the data.The common task data consisted of 90 queries, ofwhich our system obtained correct ("True") answers for48, or 53%.
Of the remainder, 10 resulted in DB callswhich obtained inappropriate ("False") information, and32 resulted in no DB call at all ("NA").
We consider in-correct answers to  be even worse than no answers, andthe greater than 10% "false alarm" rate experienced onthis task to be beyond the acceptable rate for such errors.The 42 queries that were not successfully processedcan be further subclassified as follows.
5 contained itemsthat were not in our lexicon.
9 either did not parse,or did not obtain a usable parse.
10 either obtainedno semantic/pragmatic analysis, or an incomplete one.QTIP, though given a complete interpretation of the in-put, could not create a call to  the DB for 12 more, andcreated an incorrect call 6 times.
Our results are summa-rized in Table 1; there we show the error source for theincorrect ("False") answers and the unanswered (LLNA")questions separately.
From this table we note that PUN-DIT performed quite well.
QTIP was directly responsiblePunditQTIPOutcomelexiconparsingsemanticsQTIP-no callTrue False NA1 493 712QTIP-cal l  48 6Totals \ [4s \ [  x0 \[~2Table 1: Common Evaluation Task Resultsfor nearly half (18 of 42) of the cases where an inputquery was not processed correctly.Even among the queries for which our system obtainedthe correct answer, there were 5 cases where the inputwas not processed entirely correctly.
These cases canbe subdivided into two groups, those where the unpro-cessed material was irrelevant o the handling of the re-quest, and those where the unprocessed material couldwell have resulted in an error.
An example of the formeris the query Under the category ground transportationwhat is transport A ?
Our system ignored the redundant"under the category ground transportation".
An exam-ple of the latter type is the query What is ~he fare onflight eleven forty nine from continental airlines?
Oursystem failed to process "from continental irlines", butsince no other airline has a flight number 1149 in theATIS DB, the correct answer was obtained anyway.Speaker Success Ratebdbfbmbpbw50%71%26%46%87%Table 2: Success Rate by SpeakerIt is interesting how much variance there is betweenspeakers.
There were 5 different speakers in the commontask data, and our system's uccess rate for them rangedfrom 26% to 87%, as shown in Table 2.Common Eva luat ion  Ana lys i sSuccessful handling of only slightly more than half of theinput queries, and class A queries at that, indicates thatthis system is a long way from being an operational sys-tem.
However~ these data were gathered with maximalco-operativeness (and therefore permissiveness).
Fromour experiences with the direction-assistance applica-tion, we suspect hat that the ATIS method of gatheringthe input data decreased the success rate of our (andanybody else's) system, since the Wizard coped withnearly all inputs, giving a user no reason to change modeof expression.
By contrast, if user bm had been usingour system, his or her lack of success in getting answersmight well have led to exploration with alternative waysof phrasing queries, with the result that a larger percent-age of inputs would have been processed correctly overthe entire session.
In the next section, however, we willsee that the" diagnoses, etc.
from our system are not yetgood enough to enable such an adaptation to take placefor all users.The bottom line is that our system has not yetachieved a satisfactory level of performance, and it isnot hard to understand why.
First and foremost, it isan incomplete system, in the middle of its development,and the results of the common task are simply a measureof our progress o far, and in no sense a measure of thelevel of achievement that our system will attain whenfully developed.
In fact, in the few weeks before thetest, we confined our attention to a subset of about 550class A queries from development data available to us,and had achieved a success rate of 65% on those.
So wewere pleasantly surprised to succeed on as many as 53%of new, previously unseen utterances.
We believe thatthis is evidence that our development work is indeed ofgeneral applicability for this domain, as opposed to con-sisting of a collection of ad hoc tricks to make specificinputs get through the system.On the other hand, why can our system correctly pro-cess only 65% of the training input?
Why could wenot have achieved a greater success rate by now?
Wesuspect that the answer involves the wide range of ex-pressions different people use to make essentially thesame requests in this domain.
Indeed, in a later sec-tion we quantify this observation, comparing vocabularygrowth and grammar growth in this domain with thatin the direction-assistance domain.
To return to a pointtouched on earlier, it may be significant that the data inthe ATIS domain was collected using a Wizard arrange-ment which bypassed not only the speech recognitioncomponent but ALSO the automated natural languageunderstanding component; such was not the case for thedirection-assistance experiment.Our widely different rates of success for the differentspeakers in the common task data supports the observa-tion that there are a large number of different ways toask essentially the same questions.
And if this is reallythe case, it means that a natural anguage understandingsystem will have to be trained on much larger volumes ofdata for the ATI$ domain.
In the direction-assistance do-main, we reported having to train on 1000 sentences to asuccess rate of over 80% before our system could achieve70% new sentence coverage.
It is an open question howmany more than 1000 sentences will be necessary for theATIS domain; currently we have worked with less than1000 sentences and have achieved (with comparable f-fort) only a 65% coverage on the class A subset of thetraining corpus (and about 50% coverage of the entirecorpus).
It would be informative to train to an 80%coverage and reassess coverage on test data.Individual examples of successes and failures of oursystem on the common task data seem not to be of suf-ficient general interest o report in this paper, given the144E?.F.g:o030I * ATISDirection-assistancei i i t i i i0 200 400 600 800 1000 1200Sentences  in CorpusFigure 3: Incremental growth of grammar in ATIS and direction-assistance domainscurrent limits of our system.
We do, however, feel thateven these experiments with this partially developed sys-tem point to a need to work in the ATIS domain at a tasklevel as well as at a sentence level.
So, even with the defi-ciencies of our system in its present state of development,we have begun experiments along those lines, which wediscuss in the next section.Un isys  Data  and  Eva luat ionIn order to explore issues in evaluation particularlyfrom the user's perspective, we designed a data collec-tion/evaluation task using the system as a tool to collectdata from users.
Seven subjects were asked to use theUnisys ATIS system to solve travel planning scenarios.They were given the same instructions as the ATIS sub-jects at TI, the same scenarios, and the same follow upquestionaire.
In addition, in order to measure user sat-isfaction, after the session was over, the subjects werealso asked to score each response from the system on azero to five scale of satisfactoriness.
A total of 206 typedinputs were collected, 2 of which 38% were processed cor-rectly.
The mean user satisfaction was 2.4 on the 0 to5 scale.
Although we had planned to collect other data,such as time to complete task, very few of the subjectsactually completed the task.
This was because of theincomplete development of the system and the difficultyof the scenarios.
Consequently we were unable to collectthis data.One question which we wished to address was whatfactors affect user satisfaction i  a spoken language sys-tem.
For example, we were interested in how coverageaffects user satisfaction.
Coverage is clearly the most im-portant component of user satisfaction, although it doesnot completely determine it.
In comparing user satis-faction on the queries that were handled to those which2 This data is available from the authors upon request.145were not handled, we found a mean rating of 4.8 (on a0-5 scale) for the queries that were handled and a meanrating of .98 for the queries that were not handled.
Someinputs which were not handled received a relatively highscore (4) from the users because the error messages wereperceived to be useful.
For example, the query FromOakland to Boston, what is the fare ?.
was answered withthe error message Sorry, could you rephrase that ?.
whichindicates that it wasn't parsed, but it nevertheless gota rating of 4 from the user.
On the other hand, some-times a query which was completely understood got arelatively low rating because the user didn't like howthe information was presented.
For example, the queryHow much does a flight from San Francisco cost?
wasanswered correctly, but received a score of 3 because thefares presented were not associated with specific flights.Aneedotally, we noted that response time, which isindependent ofcoverage, is also an important componentof user satisfaction.
Nearly all the Unisys subjects saidthat the system was too slow, and 28/53 or 53% of theTI subjects also said that the system they were usingwas too slow.
~ This data lead us to believe that theremay be important trade-offs in coverage and informativeerror messages vs. speed that can lead to increased usersatisfaction and usability of the system.Sys tem Growth  as a Funct ion  o f  T ra in ingDataOne of our most interesting findings was our ability toquantify the lack of convergence ofthe ATIS data, both interms of grammar rules and in terms of lexicon.
Startingwith the direction-assistance application, we developedtechniques for quantifying the growth of the system as~This data was collected from the TI debriefing questionaires.We thank Charles Hemphill of TI for making these questionairesavailable to us .i i i I i i0 200 400 600 800 1000 1200Sentences in CorpusFigure 4: Incremental growth of lexicon in ATIS and direction-assistance domainsa function of training data.
We recorded the rate ofgrowth in terms of grammar ules and lexical items asa measure of convergence for both ATIS and direction-assistance (\[1\],\[2\]) versions of PUNDIT.
Our expectationis that the rate of growth should level off as more andmore training is seen.
To the extent that it does not,significant gaps in coverage can be expected.
Figure 3shows the incremental growth of the grammar for bothdomains and Figure 4 shows the incremental growth ofthe lexicon.
It is interesting to note that after 600 sen-tences from the direction-assistance domain the rate ofgrowth in both grammar and vocabulary is quite slow,indicating that this amount of training data is enoughto provide a good sample of the kinds of constructionsused in the domain.
In contrast, we do not see any level-ing off in ATIS growth after 600 sentences.
From this wecan conclude that a larger set of data will be requiredto provide a good sample of the constructions neededfor ATIS.
It is important for future evaluations to de-velop some better methods for estimating the amountof training data needed for a given application.
Sincethe vocabulary growth curve is similar to the grammargrowth curve in both applications it may be that sim-ple measurement of vocabulary convergence would serveas a crude measure of amount of training data needed.We are just beginning to assemble some data points interms of training data for multiple applications.
Thedirection-assistance vs. ATIS  applications illustrate thattwo seemingly similar kinds of applications can have verydifferent characteristics, perhaps reflecting how the ac-tual data collection was carried out.
As we look at morespoken language applications, our ability to make rea-sonable estimates on training data should improve sig-nificantly.References\[1\] Catherine N. Ball, Deborah Dahl, Lewis M. Nor-ton, Lynette Hirschman, Carl Weir, and MarciaLinebarger.
Answers and questions: Processing mes-sages and queries.
In Proceedings of the DARPASpeech and Natural Language Workshop, Cape Cod,MA, October 1989.\[2\] Deborah A. Dahl, Lynette Hirschman, Lewis M. Nor-ton, Marcia C. Linebarger, David Magerman, andCatherine N. Ball.
Training and evaluation of spo-ken language understanding system.
In Proceedingsof the Darpa Speech and Language Workshop, HiddenValley, PA, June 1990.\[3\] Lynette Hirschman, Martha Palmer, John Dowd-ing, Deborah Dahl, Marcia Linebarger, Rebecca Pas-sonneau, Francois-Michel Lang, Catherine Ball, anCarl Weir.
The PUNDIT  natural-language pr, ~system.
In AI Systems in Government C~Computer Society of the IEEE, March 198\[4\] Don McKay, Tim Finin, and Anthony O'?intelligent database interface.
In Proceed7 th National Conference on Artificial In~,.1990.\[5\] Stephanie Seneff.
Tina: a probabilistic syntacticparser for speech understanding systems.
In Proceed-ings of the First DARPA Speech and Natural Lan-guage Workshop, Philadelphia, PA, February 1989.\[6\] Victor Zue, James Glass, Michael Phillips, andStephanie Seneff.
The MIT SUMMIT  speech recogni-tion system: A progress report.
In Proceedings of theFirst DARPA Speech and NaturaI Language Work-shop, Philadelphia, PA, February 1989.146
