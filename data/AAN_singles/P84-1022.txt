A PARSING ARCHITECTURE BASED ON DISTRIBUTED MEMORY MACHINESJon M. SlackDepartment of PsychologyOpen UniversityMilton Keynes MK7 6AAENGLANDABSTRACTThe paper begins by defining a class ofdistributed memory machines which have usefulproperties as retrieval and filtering devices.These memory mechanisms store large numbers ofassociations on a single composite vector.
Theyprovide a natural format for encoding thesyntactic and semantic constraints associatedwith linguistic elements.
A computationalarchitecture for parsing natural language isproposed which utillses the retrieval andassociative features of these devices.
Theparsing mechanism is based on the principles ofLexlcal Functional Grammar and the paperdemonstrates how these principles can be derivedfrom the properties of the memory mechanisms.I INTRODUCTIONRecently, interest has focussed oncomputational architectures employing massivelyparallel processing lip2\].
Some of thesesystems have used a distributed form ofknowledge representation \[3\].
This type ofrepresentation encodes an item of knowledge interms of the relationships among a collection ofelementary processing units, and suchassemblages can encode large numbers of items.Representational similarity and the ability togeneralize are the principal features of suchmemory systems.
The next section defines adistributed memory machine which incorporatessome of the computational advantages ofdistributed representations within a traditionalyon Neumann architecture.
The rest of the paperexplores the properties of such machines as thebasis for natural language parsing.II DISTRIBUTED MEMORY MACHINESDistributed memory machines (DMM) can berepresented formally by the septupleDMM=(V,X,Y,Q,qo,p,A) , whereV is a finite set denoting the total vocabulary;X is a finite set of inputs, and XGV;Y is a finite set of acceptable outputs and Y~V;Q is a set of internal states;q0 is a distinguished initial state;~.QxX-->Q, the retrieval function;A:Q-->Qxy, the output function.Further, where Y" denotes the set of all finiteconcatenations of the elements of the set Y,Q~Y', and therefore QgV'.
This statementrepresents the notion that internal states ofDMMs can encode multiple outputs or hypotheses.The vocabulary, V, can be represented by thespace I k, where I is some interval range definedwithin a chosen number system, N; IoN.
Theelements of X, Y and Q are encoded as k-elementvectors, referred to as memory vectozs.A.
Holographic Associative MemoryOne form of DMM is the holographic associativememory \[4,5,6\] which encodes large numbers ofassociations on a single composite vector.Items of information are encoded as k-elementzero-centred vectors over an interval such as\[-I,+I\]; <X>=(...x.t,x0,x~t,...).
Two items, <A>and <B> (angular brackets denote memoryvectors), are associated in memory through theoperation of convolution.
This method ofassociation formation is fundamental to theconcept of holographic memory and the resultingassociative trace is denoted <A>*<B>.
Theoperation of convolution is define by theequation (<A>*<B>~=.~AIB~.
i and has thefollowing propertles*\[7\]:Commutative: <A>*<B> = <B>*<A>,Associative: <A>*(<B>*<C>) = (<A>*<B>)*<C>.Further, where a delta vector, denoted ~,  isdefined as a vector that has values of zero onall features except the central feature, whichhas a value of one, then <A>* ~ffi <A>.
Moreover,<A>*0 ffi 0, where 0 is a zero vector in which allfeature values are zero.
Convolving an itemwlth an attenuated delta vector (i.e., a vectorwith values of zero on all features except thecentral one, which has a value between 0 and i)produces the original item with a strength thatis equal to the value of the central feature ofthe attenuated delta vector.The initial state, qo, encodes all theassociations stored in the machine.
In thismodel, associative traces are concatenated (+)through the operations of vector addition andnormalization to produce a single vector.Overlapping associative items produce composite92vectors which represent both the range of itemsstored and the central tendency of the thoseitems.
This form of prototype generation is abasic property of distributed memories.The retrieval function,@ , is simulated by theoperation of correlation.
If the state, q~,encodes the association <A>*<B>, then presentingsay <A> as an input, or retrieval key, producesa new state, q{~, which encodes the item <B>', anoisy version of <B>, under the operation ofcorrelation.
This operation is defined by theequation (<A>#<B>~=~A%Bm,%and has thefollowing properties: % An item correlated withitself, autocorrelation, produces anapproximation to a delta vector.
If two similarmemory vectors are correlated, the centralfeature of the resulting vector will be equal totheir similarity, or dot product, producing anattenuated delta vector.
If the two items arecompletely independent, correlation produces azero vector.The re la t ion  between convo lu t ion  andcor re la t ion  i s  g iven by<A>~(<A>*<B>) = (<A>~<A>)*<B> +(<A>~<B>)*<A> + noise ...(I)where the noise component results from some ofthe less significant cross products.
Assumingthat <A> and <B> are unrelated, Equation (I)becomes:<AMI(<A>*<B>) = ~*<B> + 0*<A> + noise- <B> + 0 + noiseExtending these results to a composite trace,suppose that q encodes two associated pairs offour unrelated items forming the vector (<A>*<B>+ <C>*<D>).
When <A> is given as the retrievalcue, the reconstruction can be characterized asfollows:<A>~(<A>*<B> + <C>*<D>)= (<A>~t<A>)*<B> + (<A>~<B>)*<A> + noise+ (<A>~<C>)*<D> + (<A>@<D>)*<C> + noise= ~ *<B>+0*<A>+noise+O*<D>+O*<C>+noise- <B> + noise + noiseWhen the additional unrelated items are added tothe memory trace their affect on retrieval is toadd noise to the reconstructed item <B>, whichwas associated with the retrieval cue.
In asituation in which the encoded items are relatedto each other, the composite trace causes all ofthe related items to contribute to thereconstructed pattern, in addition to producingnoise.
The amount of noise added to a retrieveditem is a function of both the amount ofinformation held on the composite memory vectorand the size of the vector.I I I  BUILDING NATURAL LANCUACZ PARSERSA.
Case-Frame ParsingThe computational properties of distributedmemory machines (DMM) make them naturalmechanisms for case-frame parsing.
Consider aDMM which encodes case-frame structures of thefollowing form:<Pred>*(<Cl>*<Pl> + <C2>*<P2> + ...+ <Cn>*<Pn>)where <Pred> i s  the vector  represent ing  thepred icate  assoc ia ted  with the verb of an inputc lause ;  <C1> to <Cn> are  the case vectors  suchas <agent>, <instrument>, etc., and <PI> to <Pn>are vectors representing prototype conceptswhich can fill the associated cases.
Thesestructures can be made more complex by includingtagging vectors which indicate such features asob l igatory  case ,  as shown in the case-framevector for the predicate BREAK:(<agent>*<anlobJ+natforce> + <obJect>*<physobJ>*<obllg> + <instrument>*<physobJ>)In this example, the object case has a prototypecovering the category of physical objects, andis tagged as obligatory.The initial state of the DMM, qo, encodes theconcatenation of the set of case-frame vectorsstored by the parser.
The system receives twotypes of inputs, noun concept vectorsrepresenting noun phrases, and predicate vectorsrepresenting the verb components.
If the systemis in state qo only a predicate vector inputproduces a significant new state representingthe case-frame structure associated with it.Once in this state, noun vector inputs identifythe case slots they can potentially fill asillustrated in the following example:In pars ing  the sentence  Fred broke the windowwi th  e s tone ,  the input  vector  encodin  E brokew i l l  re t r ieve  the case - f rame s t ruc ture  fo r  breakg iven  above.
The input  of <Fred> now g ives<Fred>~q<agent>*<Pa>+<obJ>*<Po>+<instr>*<Pi>) "<Fred>g<agent>*<Pa>+<Fred>~<Pa>*<agent> + ... -0*<Pa>+ee*<agent> O*<Po>+e@*<obJ> +O*<Pi>+e%*<instr> :e~agent> + e~obJ> + es<instr>where ej is a measure of the similarity betweenthe vectors, and underlying concepts, <Fred> andthe case prototype <Pj>.
In this example,<Fred> would be identified as the agent becausee 0 and e~ would be low relative to ee.
Thevector is "cleaned-up" by a threshold functionwhich is a component of the output function,)%.This process is repeated for the other nounconcepts in the sentence, linking <window> and<stone> with the object and instrument cases,respect ive ly .
However, the parser  requ i resadd i t iona l  machinery  to handle  the la rge  set  ofsentences  in  which the case ass ignment  i sambiguous us ing  semant ic  knowledge a lone .B.
Encodin~ Syntactic KnowledgeUnambiguous case ass ignment  can only  beach ieved through the in tegrat ion  of syntact i cand semant ic  p rocess ing .
Moreover, an adequateparser  should generate  an encoding of thegrammatical relations between sentential elementsin  add i t ion  to a semant ic  representat ion .
The res tof  the paper demonst ra tes  how the proper t ies  ofDMMs can be combined with the ideas  embodied inthe theory  of Lextca l - funct iona l  CTammar (LFG) \[8\]in a parser which builds both types of relationalstructure.93In LFG the mapping between grammatical andsemantic relations is represented directly inthe semantic form of the lexlcal entries forverbs.
For example, the lexlcal entry for theverb hands is given byhands: V, #participle) = NONE#tense) = PRESENT(tsubJ hum) = SO~pred) = HAND\[@subJ)#obj2)@obJ)\]where the arguments of the predicate HAND areordered such that they map directly onto thearguments of the semantic predicate-argumentstructure.
The order and value of the argumentsin a lexical entry are transformed by lexlcalrules, such as the passive, to produce newlexical entries, e.g., HAND\[#byobJ)~subJ)(~oobJ)\].The direct mapping between lexical predicates andcase-frame structures is encoded on the case-frameDMM by augmenting the vectors as follows:Hands:- <HAND>*(<agent>*<Pa>*<subJ> +<obJect>*<Po>*<obJ2>+<goal>*<Pg>*<obJ>)When the SUBJ component has been identifiedthrough syntactic processing the resultingassociation vector, for example <subJ>*<John>for the sentence John handed Mary the book, willretrieve <agent> on input to the CF-DMM,according to the principles specified above.The multiple lexical entries produced by lexicalrules have corresponding multiple case-framevectors which are tagged by the appropriategrammatical vector.
The CF-DMM encodes multiplecase-frame entries for verbs, and the grammaticalvector tags, such as <PASSIVE>, generated by thesyntactic component, are input to the CF-DMM toretrieve the appropriate case-frame for the verb.The grammatical relations Between thesententlal elements are represented in the formof functional structure (f-structures) as inLFG.
These structures correspond to embeddedlists of attrlbute-value pairs, and because ofthe Uniqueness criterion which governs theirformat they are efficiently encoded as memoryvectors.
As an example, the grammaticalrelations for the sentence John handed Mary abook are encoded in the f-structure below:SUBJ NUMRED 'JOPAST'HAND\[( SUBJ)( OSJ2)( OBJ)\]TENSEPREDOBJ \[~UM MARY 3SG RED "OBJ2 \ [~C ASG K~"\[,PRED "BOOThe lists of grammatical functions and featuresare encoded as single vectors under the +operator, and the embedded structure ispreserved by the associative operator, *.
Thef-structure is encoded by the vector(<SUBJ>*(<NUM>*<SG>+<PRED>*<JOHN>) + <TENSE>*<PAST> + <PRED>*(<HAND>*(<#SUBJ>*<TOBJ2>*<TOBJ>)) + <OBJ>*(<NUM>*<SG>+<PRED>*<MARY>)+<OBJ2>*(<SPEC>*<A>+<NUM>*<SG>+<PRED>*<BOOK>))This compatibility between f-structures andmemory vectors is the basis for an efficientprocedure for deriving f-structures from inputstrings.
In LFG f-structures are generated inthree steps.
First, a context-free grammar(CFG) is used to derive an input string'sconstituent structure (C-structure).
The grammaris augmented so that it generates a phrasestructure tree which includes statements aboutthe properties of the string's f-structure.
Inthe next step, this structure is condensed toderive a series of equations, called the functionaldescription of the string.
Finally, the f-structureis derived from the f-description.
The propertiesof DMMs enable a simple procedure to be writtenwhich derives f-structures from augmented phrasestructure trees, obviating the need for anf-descrlptlon.
Consider the tree in figure 1generated for our example sentence:~SUBJ) - & St&~ENSE)-PAST \@FRED) =HAND\[ ..\] ~ \(I'NUM)- SO \[ ~OBJ)-~,~PRED)=JOHN) I (~qUM) =SG~PRED)=MARY #OBJ2)=&/ \[ b John handed Mary a kFigure I. Augmented Phrase Structure TreeThe f-structure, encoded as a memory vector, canbe derived from this tree by the followingprocedure.
First, all the grammaticalfunctions, features and semantic forms must beencoded as vectors.
The~-var iables,  f,-f#,have no values at this point; they are derivedby the procedure.
All the vectors dominated bya node are concatenated to produce a singlevector at that node.
The symbol '=" isinterpreted as the association operator ,*.Applying this interpretation to the tree fromthe bottom up produces a memory vector for thevalue of f!
which encodes the f-structure forthe string, as given above.
Accordingly, f~takes the value (<TNUM>*<SG>+<TPRED>*<JOHN>);applying the rule specified at the node, (f, SUBJ)=f~gives <tSUBJ>*(<tNUM>*<SG>+<TPRED>*<JOHN>) as acomponent of f,.
The other components of fl arederived in the same way.
The front-end CFG canbe veiwed as generating the control structurefor the derivation of a memory vector whichrepresents the input string's f-structure.94The properties of memory vectors also enablethe procedure to automatically determine theconsistency Df the structure.
For example, inderiving the value of f& the concatenationoperator merges the (%NUM)~SG features for A andbook to form a single component of the f~vector,(<SPEC>*<A>+<NUM>*<SG>+<PRED>*<MARY>).
.owever,if the two features had not matched, producingthe vector component <NU}~*(<SG>+<PL>) forexample, the vectors encoding the incompatiblefeature values are set such that theirconcatenation produces a special control vectorwhich signals the mismatch.C.
A Parsing ArchitectureThe ideas outlined above are combined in thedesign of a tentative parsing architecture shownin figure 2.
The diamonds denote DMMs, and therFigure 2.
Parsing Architectureellipse denotes a form of DMM functioning as aworking memory for encoding temporary f-structures.As elements of the input string enter thelexicon their associated entries are retrieved.The syntactic category of the element is passedonto the CFG, and the lexical schemata {e.g.,~PRED)='JOHN'}, encoded as memory vectors, arepassed to the f-structure working memory.
Thelexical entry associated with the verb is passedto the case-frame memory to retrieve theappropriate set of structures.
The partialresults of the CFG control the formation ofmemory vectors in the f-structure memory, asindicated by the broad arrow.
The CFG alsogenerates grammatical vectors as inputs forcase-frame memory to select the appropriatestructure from the multiple encodings associatedwith each verb.
The partial f-structureencoding can then be used as input to thecase-frame memory to assign the semantic formsof grammatical functions to case slots.
Whenthe end of the string is reached both thecase-frame instantiation and the f-structureshould be complete.IV CONCLUSIONSThis paper attempts to demonstrate the value ofdistributed memory machines as components of aparsing system which generates both semantic andgrammatical relational structures.
The ideaspresented are similar to those being developedwithin the connectlonist paradigm \[I\].
Small,and his colleagues \[9\], have proposed a parsingmodel based directly on connectionist principles.The computational architecture consists of a largenumber of appropriately connected computing unitscommunicating through weighted levels of excitationand inhibition.
The ideas presented here differfrom those embodied in the connectionist parserin that they emphasise distributed informationstorage and retrieval, rather than distributedparallel processing.
Retrieval and filteringare achieved through simple computable functionsoperating on k-element arrays, in contrast tothe complex interactions of the independentunits in connectlonist models.
In figure 2,although the network of machines requiresheterarchical control, the architecture can beconsidered to be at the lower end of the familyof parallel processing machines \[i0\].V BEF~e~wCES\[I\] Feldman, J.A.
and Ballard, D.H. Connection-ist models and their properties.
CognitiveScience, 1982, 6, 205-254.\[2\] Hinton, G.E.
and Anderson, J.A.
(Eds)Parallel Models of Associative Memory.Hillsdale, NJ: Lawrence Erlhat~Q Associates,1981.\[3\] Hinton, G.E.
Shape representation in parallelsystems.
In Proceedinss of the SeventhInternational Joint Conference on ArtificialIn te l l i~ence,  Vol.
2, Vancouver BC, Canada,August, 1981.\[4\] Longuet-Higgins, H.C., Willshaw, D.J., andBunemann, O.P.
Theories of associative recall.~uarterly Reviews of Biophysics, 1970, 3,223-244.\[5\] Murdock, B.B.
A theory for the storage andretrieval of item and associative information.Psychological Review, 1982, 89, 609-627.\[6\] Kohonen, T. Associative memory~ system-theoretical approach.
Berlin: Springer-Verlag, 1977.\[7\] Borsellino, A., and Poggio, T. Convolutionand Correlation algebras.
Kybernetik,1973, 13, 113-122.\[8\] Kaplan, R., and Bresnan, J. Lexical-FunctionalGrammar: A formal system for grammaticalrepresentation.
In J. Bresnan (ed.
), TheMental 9~presentation of Grammatical Relations.Cambridge, Mass.
:MIT Press, 1982.\[9\] Small, S.L., Cottre11, G.W., and Shastri, L.Toward connectlonlst parsing.
In Proceedingsof the National Conference on ArtificialIntelligence, Pittsburgh, P~nsylvanla, 1982.\[10\] Fahlman, S,E., Hinton, G.E., and Sejnowski, T.Massively para l le l  arch i tectures  for AI: NETL,THISTLE, and BOLTZMANNmachines.
In Proceed-ings of  the National Conference on Ar t i f i c ia lIntelli~enc~e, Washington D.C., I~3o95
