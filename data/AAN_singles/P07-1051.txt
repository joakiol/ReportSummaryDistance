Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 400?407,Prague, Czech Republic, June 2007. c?2007 Association for Computational LinguisticsIs the End of Supervised Parsing in Sight?Rens BodSchool of Computer ScienceUniversity of St Andrews, ILLC, University of Amsterdamrb@cs.st-and.ac.ukAbstractHow far can we get with unsupervisedparsing if we make our training corpusseveral orders of magnitude larger than hashitherto be attempted?
We present a newalgorithm for unsupervised parsing usingan all-subtrees model, termed U-DOP*,which parses directly with packed forestsof all binary trees.
We train both on Penn?sWSJ data and on the (much larger) NANCcorpus, showing that U-DOP* outperformsa treebank-PCFG on the standard WSJ testset.
While U-DOP* performs worse thanstate-of-the-art supervised parsers on hand-annotated sentences, we show that themodel outperforms supervised parserswhen evaluated as a language model insyntax-based machine translation onEuroparl.
We argue that supervised parsersmiss the fluidity between constituents andnon-constituents and that in the field ofsyntax-based language modeling the end ofsupervised parsing has come in sight.1    IntroductionA major challenge in natural language parsing isthe unsupervised induction of syntactic structure.While most parsing methods are currentlysupervised or semi-supervised (McClosky et al2006; Henderson 2004; Steedman et al 2003), theydepend on hand-annotated data which are difficultto come by and which exist only for a fewlanguages.
Unsupervised parsing methods arebecoming increasingly important since theyoperate with raw, unlabeled data of whichunlimited quantities are available.There has been a resurgence of interest inunsupervised parsing during the last few years.Where van Zaanen (2000) and Clark (2001)induced unlabeled phrase structure for smalldomains like the ATIS, obtaining around 40%unlabeled f-score, Klein and Manning (2002)report 71.1% f-score on Penn WSJ part-of-speechstrings ?
10 words (WSJ10) using a constituent-context model called CCM.
Klein and Manning(2004) further show that a hybrid approach whichcombines constituency and dependency models,yields 77.6% f-score on WSJ10.While Klein and Manning?s approach maybe described as an ?all-substrings?
approach tounsupervised parsing, an even richer modelconsists of an ?all-subtrees?
approach tounsupervised parsing, called U-DOP (Bod 2006).U-DOP initially assigns all unlabeled binary treesto a training set, efficiently stored in a packedforest, and next trains subtrees thereof on a held-out corpus, either by taking their relativefrequencies, or by iteratively training the subtreeparameters using the EM algorithm (referred to as?UML-DOP?).
The main advantage of an all-subtrees approach seems to be the direct inclusionof discontiguous context that is not captured by(linear) substrings.
Discontiguous context isimportant not only for learning structuraldependencies but also for learning a variety of non-contiguous constructions such as nearest ?
to?
ortake ?
by surprise.
Bod (2006) reports 82.9%unlabeled f-score on the same WSJ10 as used byKlein and Manning (2002, 2004).
Unfortunately,his experiments heavily depend on a priorisampling of subtrees, and the model becomeshighly inefficient if larger corpora are used orlonger sentences are included.In this paper we will also test analternative model for unsupervised all-subtrees400parsing, termed U-DOP*, which is based on theDOP* estimator by Zollmann and Sima?an (2005),and which computes the shortest derivations forsentences from a held-out corpus using all subtreesfrom all trees from an extraction corpus.
While wedo not achieve as high an f-score as the UML-DOPmodel in Bod (2006), we will show that U-DOP*can operate without subtree sampling, and that themodel can be trained on corpora that are twoorders of magnitude larger than in Bod (2006).
Wewill extend our experiments to 4 million sentencesfrom the NANC corpus (Graff 1995), showing thatan f-score of 70.7% can be obtained on thestandard Penn WSJ test set by means ofunsupervised parsing.
Moreover, U-DOP* can bedirectly put to use in bootstrapping structures forconcrete applications such as syntax-basedmachine translation and speech recognition.
Weshow that U-DOP* outperforms the supervisedDOP model if tested on the German-EnglishEuroparl corpus in a syntax-based MT system.In the following, we first explain theDOP* estimator and discuss how it can beextended to unsupervised parsing.
In section 3, wediscuss how a PCFG reduction for supervised DOPcan be applied to packed parse forests.
In section 4,we will go into an experimental evaluation of U-DOP* on annotated corpora, while in section 5 wewill evaluate U-DOP* on unlabeled corpora in anMT application.2     From DOP* to U-DOP*DOP* is a modification of the DOP model in Bod(1998) that results in a statistically consistentestimator and in an efficient training procedure(Zollmann and Sima?an 2005).
DOP* uses the all-subtrees idea from DOP: given a treebank, take allsubtrees, regardless of size, to form a stochastictree-substitution grammar (STSG).
Since a parsetree of a sentence may be generated by several(leftmost) derivations, the probability of a tree isthe sum of the probabilities of the derivationsproducing that tree.
The probability of a derivationis the product of the subtree probabilities.
Theoriginal DOP model in Bod (1998) takes theoccurrence frequencies of the subtrees in the treesnormalized by their root frequencies as subtreeparameters.
While efficient algorithms have beendeveloped for this DOP model by converting it intoa PCFG reduction (Goodman 2003), DOP?sestimator was shown to be inconsistent by Johnson(2002).
That is, even with unlimited training data,DOP's estimator is not guaranteed to converge tothe correct distribution.Zollmann and Sima?an (2005) developed astatistically consistent estimator for DOP which isbased on the assumption that maximizing the jointprobability of the parses in a treebank can beapproximated by maximizing the joint probabilityof their shortest derivations (i.e.
the derivationsconsisting of the fewest subtrees).
This assumptionis in consonance with the principle of simplicity,but there are also empirical reasons for the shortestderivation assumption: in Bod (2003) and Hearneand Way (2006), it is shown that DOP models thatselect the preferred parse of a test sentence usingthe shortest derivation criterion perform very well.On the basis of this shortest-derivationassumption, Zollmann and Sima?an come up with amodel that uses held-out estimation: the trainingcorpus is randomly split into two parts proportionalto a fixed ratio: an extraction corpus EC and aheld-out corpus HC.
Applied to DOP, held-outestimation would mean to extract fragments fromthe trees in EC and to assign their weights suchthat the likelihood of HC is maximized.
If wecombine their estimation method with Goodman?sreduction of DOP, Zollman and Sima?an?sprocedure operates as follows:(1) Divide a treebank into an EC and HC(2) Convert the subtrees from EC into a PCFGreduction(3) Compute the shortest derivations for thesentences in HC (by simply assigning eachsubtree equal weight and applying Viterbi1-best)(4) From those shortest derivations, extract thesubtrees and their relative frequencies inHC to form an STSGZollmann and Sima?an show that the resultingestimator is consistent.
But equally important is thefact that this new DOP* model does not sufferfrom a decrease in parse accuracy if larger subtreesare included, whereas the original DOP modelneeds to be redressed by a correction factor tomaintain this property (Bod 2003).
Moreover,DOP*?s estimation procedure is very efficient,while the EM training procedure for UML-DOP401proposed in Bod (2006) is particularly timeconsuming and can only operate by randomlysampling trees.Given the advantages of DOP*, we  willgeneralize this model in the current paper tounsupervised parsing.
We will use the same all-subtrees methodology as in Bod (2006), but nowby applying the efficient and consistent DOP*-based estimator.
The resulting model, which wewill call U-DOP*, roughly operates as follows:(1) Divide a corpus into an EC and HC(2) Assign all unlabeled binary trees to thesentences in EC, and store them in ashared parse forest(3) Convert the subtrees from the parse forestsinto a compact PCFG reduction (see nextsection)(4) Compute the shortest derivations for thesentences in HC (as in DOP*)(5) From those shortest derivations, extract thesubtrees and their relative frequencies inHC to form an STSG(6) Use the STSG to compute the mostprobable parse trees for new test data bymeans of Viterbi n-best (see next section)We will use this U-DOP* model to investigate ourmain research question: how far can we get withunsupervised parsing if we make our trainingcorpus several orders of magnitude larger thanhas hitherto be attempted?3  Converting shared parse forests intoPCFG reductionsThe main computational problem is how to dealwith the immense number of subtrees in U-DOP*.There exists already an efficient supervisedalgorithm that parses a sentence by means of allsubtrees from a treebank.
This algorithm wasextensively described in Goodman (2003) andconverts a DOP-based STSG into a compact PCFGreduction that generates eight rules for each nodein the treebank.
The reduction is based on thefollowing idea: every node in every treebank tree isassigned a unique number which is called itsaddress.
The notation A@k denotes the node ataddress k where A is the nonterminal labeling thatnode.
A new nonterminal is created for each nodein the training data.
This nonterminal is called Ak.Let aj represent the number of subtrees headed bythe node A@j, and let a represent the number ofsubtrees headed by nodes with nonterminal A, thatis a = ?j aj.
Then there is a PCFG with thefollowing property: for every subtree in thetraining corpus headed by A, the grammar willgenerate an isomorphic subderivation.
Forexample, for a node (A@j (B@k, C@l)), thefollowing eight PCFG rules in figure 1 aregenerated, where the number following a rule is itsweight.Aj ?
BC       (1/aj) A ?
BC        (1/a)Aj ?
BkC      (bk/aj) A ?
BkC      (bk/a)Aj ?
BCl      (cl/aj) A ?
BCl         (cl/a)Aj ?
BkCl     (bkcl/aj) A ?
BkCl       (bkcl/a)Figure 1.
PCFG reduction of supervised DOPBy simple induction it can be shown that thisconstruction produces PCFG derivationsisomorphic to DOP derivations (Goodman 2003:130-133).
The PCFG reduction is linear in thenumber of nodes in the corpus.While Goodman?s reduction method wasdeveloped for supervised DOP where each trainingsentence is annotated with exactly one tree, themethod can be generalized to a corpus where eachsentence is annotated with all possible binary trees(labeled with the generalized category X), as longas we represent these trees by a shared parse forest.A shared parse forest can be obtained by addingpointers from each node in the chart (or tabulardiagram) to the nodes that caused it to be placed inthe chart.
Such a forest can be represented in cubicspace and time (see Billot and Lang 1989).
Then,instead of assigning a unique address to each nodein each tree, as done by the PCFG reduction forsupervised DOP, we now assign a unique addressto each node in each parse forest for each sentence.However, the same node may be part of more thanone tree.
A shared parse forest is an AND-ORgraph where AND-nodes correspond to the usualparse tree nodes, while OR-nodes correspond todistinct subtrees occurring in the same context.
Thetotal number of nodes is cubic in sentence length n.This means that there are O(n3) many nodes thatreceive a unique address as described above, towhich next our PCFG reduction is applied.
This isa huge reduction compared to Bod (2006) where402the number of subtrees of all trees increases withthe Catalan number, and only ad hoc samplingcould make the method work.Since U-DOP* computes the shortestderivations (in the training phase) by combiningsubtrees from unlabeled binary trees, the PCFGreduction in figure 1 can be represented as infigure 2, where X refers to the generalized categorywhile B and C either refer to part-of-speechcategories or are equivalent to X.
The equalweights follow from the fact that the shortestderivation is equivalent to the most probablederivation if all subtrees are assigned equalprobability (see Bod 2000; Goodman 2003).Xj ?
BC        1  X ?
BC        0.5Xj ?
BkC      1  X ?
BkC       0.5Xj ?
BCl       1  X ?
BCl         0.5Xj ?
BkCl      1  X ?
BkCl       0.5Figure 2.
PCFG reduction for U-DOP*Once we have parsed HC with the shortestderivations by the PCFG reduction in figure 2, weextract the subtrees from HC to form an STSG.The number of subtrees in the shortest derivationsis linear in the number of nodes (see Zollmann andSima?an 2005, theorem 5.2).
This means that U-DOP* results in an STSG which is much moresuccinct than previous DOP-based STSGs.Moreover, as in Bod (1998, 2000), we use anextension of Good-Turing to smooth the subtreesand to deal with ?unknown?
subtrees.Note that the direct conversion of parseforests into a PCFG reduction also allows us toefficiently implement the maximum likelihoodextension of U-DOP known as UML-DOP (Bod2006).
This can be accomplished by training thePCFG reduction on the held-out corpus HC bymeans of the expectation-maximization algorithm,where the weights in figure 1 are taken as initialparameters.
Both U-DOP*?s and UML-DOP?sestimators are known to be statistically consistent.But while U-DOP*?s training phase merelyconsists of the computation of the shortestderivations and the extraction of subtrees, UML-DOP involves iterative training of the parameters.Once we have extracted the STSG, wecompute the most probable parse for newsentences by Viterbi n-best, summing up theprobabilities of derivations resulting in the sametree (the exact computation of the most probableparse is NP hard ?
see Sima?an 1996).
We haveincorporated the technique by Huang and Chiang(2005) into our implementation which allows forefficient Viterbi n-best parsing.4    Evaluation on hand-annotated corporaTo evaluate U-DOP* against UML-DOP and otherunsupervised parsing models, we started out withthree corpora that are also used in Klein andManning (2002, 2004) and Bod (2006): Penn?sWSJ10 which contains 7422 sentences ?
10 wordsafter removing empty elements and punctuation,the German NEGRA10 corpus and the ChineseTreebank CTB10 both containing 2200+ sentences?
10 words after removing punctuation.
As withmost other unsupervised parsing models, we trainand test on p-o-s strings rather than on wordstrings.
The extension to word strings isstraightforward as there exist highly accurateunsupervised part-of-speech taggers (e.g.
Sch?tze1995) which can be directly combined withunsupervised parsers, but for the moment we willstick to p-o-s strings (we will come back to wordstrings in section 5).
Each corpus was divided into10 training/test set splits of 90%/10% (n-foldtesting), and each training set was randomlydivided into two equal parts, that serve as EC andHC and vice versa.
We used the same evaluationmetrics for unlabeled precision (UP) and unlabeledrecall (UR) as in Klein and Manning (2002, 2004).The two metrics of UP and UR are combined bythe unlabeled f-score F1 = 2*UP*UR/(UP+UR).All trees in the test set were binarized beforehand,in the same way as in Bod (2006).For UML-DOP the decrease in cross-entropy became negligible after maximally 18iterations.
The training for U-DOP* consisted inthe computation of the shortest derivations for theHC from which the subtrees and their relativefrequencies were extracted.
We used the techniquein Bod (1998, 2000) to include ?unknown?subtrees.
Table 1 shows the f-scores for U-DOP*and UML-DOP against the f-scores for U-DOPreported in Bod (2006), the CCM model in Kleinand Manning (2002), the DMV dependency modelin Klein and Manning (2004) and their combinedmodel DMV+CCM.403Model English(WSJ10)German(NEGRA10)Chinese(CTB10)CCM 71.9 61.6 45.0DMV 52.1 49.5 46.7DMV+CCM 77.6 63.9 43.3U-DOP 78.5 65.4 46.6U-DOP* 77.9 63.8 42.8UML-DOP 79.4 65.2 45.0Table 1.
F-scores of U-DOP* and UML-DOPcompared to other models on the same data.It should be kept in mind that an exact comparisoncan only be made between U-DOP* and UML-DOP in table 1, since these two models were testedon 90%/10% splits, while the other models wereapplied to the full WSJ10, NEGRA10 and CTB10corpora.
Table 1 shows that U-DOP* performsworse than UML-DOP in all cases, although thedifferences are small and was statisticallysignificant only for WSJ10 using paired t-testing.As explained above, the main advantage ofU-DOP* over UML-DOP is that it works with amore succinct grammar extracted from the shortestderivations of HC.
Table 2 shows the size of thegrammar (number of rules or subtrees) of the twomodels for resp.
Penn WSJ10, the entire Penn WSJand the first 2 million sentences from the NANC(North American News Text) corpus whichcontains a total of approximately 24 millionsentences from different news sources.Model Size ofSTSGfor WSJ10Size ofSTSGfor PennWSJSize of STSGfor 2,000KNANCU-DOP* 2.2 x 104 9.8 x 105 7.2 x 106UML-DOP 1.5 x 106 8.1 x 107 5.8 x 109Table 2.
Grammar size of U-DOP* and UML-DOPfor WSJ10 (7,7K sentences), WSJ (50K sentences)and the first 2,000K sentences from NANC.Note that while U-DOP* is about 2 orders ofmagnitudes smaller than UML-DOP for theWSJ10, it is almost 3 orders of magnitudes smallerfor the first 2 million sentences of the NANCcorpus.
Thus even if U-DOP* does not give thehighest f-score in table 1, it is more apt to betrained on larger data sets.
In fact, a well-knownadvantage of unsupervised methods oversupervised methods is the availability of almostunlimited amounts of text.
Table 2 indicates thatU-DOP*?s grammar is still of manageable sizeeven for text corpora that are (almost) two ordersof magnitude larger than Penn?s WSJ.
The NANCcorpus contains approximately 2 million WSJsentences that do not overlap with Penn?s WSJ andhas been previously used by McClosky et al(2006) in improving a supervised parser by self-training.
In our experiments below we will start bymixing subsets from the NANC?s WSJ data withPenn?s WSJ data.
Next, we will do the same with 2million sentences from the LA Times in the NANCcorpus, and finally we will mix all data together forinducing a U-DOP* model.
From Penn?s WSJ, weonly use sections 2 to 21 for training (just as insupervised parsing) and section 23 (?100 words)for testing, so as to compare our unsupervisedresults with some binarized supervised parsers.The NANC data was first split intosentences by means of a simple discriminitivemodel.
It was next p-o-s tagged with the the TnTtagger (Brants 2000) which was trained on thePenn Treebank such that the same tag set was used.Next, we added subsets of increasing size from theNANC p-o-s strings to the 40,000 Penn WSJ p-o-sstrings.
Each time the resulting corpus was splitinto two halfs and the shortest derivations werecomputed for one half by using the PCFG-reduction from the other half and vice versa.
Theresulting trees were used for extracting an STSGwhich in turn was used to parse section 23 ofPenn?s WSJ.
Table 3 shows the results.# sentences added  f-score byadding WSJdataf-score byadding LATimes data0 (baseline) 62.2 62.2100k 64.7 63.0250k 66.2 63.8500k 67.9 64.11,000k 68.5 64.62,000k 69.0 64.9Table 3.
Results of U-DOP* on section 23 fromPenn?s WSJ by adding sentences from NANC?sWSJ and NANC?s LA Times404Table 3 indicates that there is a monotonousincrease in f-score on the WSJ test set if NANCtext is added to our training data in both cases,independent of whether the sentences come fromthe WSJ domain or the LA Times domain.Although the effect of adding LA Times data isweaker than adding WSJ data, it is noteworthy thatthe unsupervised induction of trees from the LATimes domain still improves the f-score even if thetest data are from a different domain.We also investigated the effect of addingthe LA Times data to the total mix of Penn?s WSJand NANC?s WSJ.
Table 4 shows the results ofthis experiment, where the baseline of 0 sentencesthus starts with the 2,040k sentences from thecombined Penn-NANC WSJ data.Sentences addedfrom LA Times toPenn-NANC WSJf-score byadding LATimes data0 69.0100k 69.4250k 69.9500k 70.21,000k 70.42,000k 70.7Table 4.
Results of U-DOP* on section 23 fromPenn?s WSJ by mixing sentences from thecombined Penn-NANC WSJ with additions fromNANC?s LA Times.As seen in table 4, the f-score continues to increaseeven when adding LA Times data to the largecombined set of Penn-NANC WSJ sentences.
Thehighest f-score is obtained by adding 2,000ksentences, resulting in a total training set of 4,040ksentences.
We believe that our result is quitepromising for the future of unsupervised parsing.In putting our best f-score in table 4 intoperspective, it should be kept in mind that the goldstandard trees from Penn-WSJ section 23 werebinarized.
It is well known that such a binarizationhas a negative effect on the f-score.
Bod (2006)reports that an unbinarized treebank grammarachieves an average 72.3% f-score on WSJsentences ?
40 words, while the binarized versionachieves only 64.6% f-score.
To compare U-DOP*?s results against some supervised parsers,we additionally evaluated a PCFG treebankgrammar and the supervised DOP* parser usingthe same test set.
For these supervised parsers, weemployed the standard training set, i.e.
Penn?s WSJsections 2-21, but only by taking the p-o-s stringsas we did for our unsupervised U-DOP* model.Table 5 shows the results of this comparison.Parser f-scoreU-DOP* 70.7Binarized treebank PCFG 63.5Binarized DOP* 80.3Table 5.
Comparison between the (best version of)U-DOP*, the supervised treebank PCFG and thesupervised DOP* for section 23 of Penn?s WSJAs seen in table 5, U-DOP* outperforms thebinarized treebank PCFG on the WSJ test set.While a similar result was obtained in Bod (2006),the absolute difference between unsupervisedparsing and the treebank grammar was extremelysmall in Bod (2006): 1.8%, while the difference intable 5 is 7.2%, corresponding to 19.7% errorreduction.
Our f-score remains behind thesupervised version of DOP* but the gap getsnarrower as more training data is being added toU-DOP*.5   Evaluation on unlabeled corpora in apractical applicationOur experiments so far have shown that despite theaddition of large amounts of unlabeled trainingdata, U-DOP* is still outperformed by thesupervised DOP* model when tested on hand-annotated corpora like the Penn Treebank.
Yet it iswell known that any evaluation on hand-annotatedcorpora unreasonably favors supervised parsers.There is thus a quest for designing an evaluationscheme that is independent of annotations.
Oneway to go would be to compare supervised andunsupervised parsers as a syntax-based languagemodel in a practical application such as machinetranslation (MT) or speech recognition.In Bod (2007), we compared U-DOP* andDOP* in a syntax-based MT system known asData-Oriented Translation or DOT (Poutsma 2000;Groves et al 2004).
The DOT model starts with abilingual treebank where each tree pair constitutesan example translation and where translationallyequivalent constituents are linked.
Similar to DOP,405the DOT model uses all linked subtree pairs fromthe bilingual treebank to form an STSG of linkedsubtrees, which are used to compute the mostprobable translation of a target sentence given asource sentence (see Hearne and Way 2006).What we did in Bod (2007) is to let bothDOP* and U-DOP* compute the best trees directlyfor the word strings in the German-EnglishEuroparl corpus (Koehn 2005), which containsabout 750,000 sentence pairs.
Differently from U-DOP*, DOP* needed to be trained on annotateddata, for which we used respectively the Negra andthe Penn treebank.
Of course, it is well-known thata supervised parser?s f-score decreases if it istransferred to another domain: for example, the(non-binarized) WSJ-trained DOP model in Bod(2003) decreases from around 91% to 85.5% f-score if tested on the Brown corpus.
Yet, this scoreis still considerably higher than the accuracyobtained by the unsupervised U-DOP model,which achieves 67.6% unlabeled f-score on Brownsentences.
Our main question of interest is in howfar this difference in accuracy on hand-annotatedcorpora carries over when tested in the context of aconcrete application like MT.
This is not a trivialquestion, since U-DOP* learns ?constituents?
forword sequences such as Ich m?chte (?I would liketo?)
and There are (Bod 2007), which are usuallyhand-annotated as non-constituents.
While U-DOP* is punished for this ?incorrect?
prediction ifevaluated on the Penn Treebank, it may berewarded for this prediction if evaluated in thecontext of machine translation using the Bleu score(Papineni et al 2002).
Thus similar to Chiang(2005), U-DOP can discover non-syntacticphrases, or simply ?phrases?, which are typicallyneglected by linguistically syntax-based MTsystems.
At the same time, U-DOP* can also learndiscontiguous constituents that are neglected byphrase-based MT systems (Koehn et al 2003).In our experiments, we used both U-DOP*and DOP* to predict the best trees for the German-English Europarl corpus.
Next, we assigned linksbetween each two nodes in the respective trees foreach sentence pair.
For a 2,000 sentence test setfrom a different part of the Europarl corpus wecomputed the most probable target sentence (usingViterbi n best).
The Bleu score was used tomeasure translation accuracy, calculated by theNIST script with its default settings.
As a baselinewe compared our results with the publiclyavailable phrase-based system Pharaoh (Koehn etal.
2003), using the default feature set.
Table 6shows for each system the Bleu score together witha description of the productive units.
?U-DOT?refers to ?Unsupervised DOT?
based on U-DOP*,while DOT is based on DOP*.System Productive Units Bleu-scoreU-DOT / U-DOP* Constituents and Phrases 0.280DOT / DOP* Constituents only 0.221Pharaoh Phrases only 0.251Table 6.
Comparing U-DOP* and DOP* in syntax-based MT on the German-English Europarl corpusagainst the Pharaoh system.The table shows that the unsupervised U-DOTmodel outperforms the supervised DOT modelwith 0.059.
Using Zhang?s significance tester(Zhang et al 2004), it turns out that this differenceis statistically significant (p < 0.001).
Also thedifference between U-DOT and the baselinePharaoh is statistically significant (p < 0.008).Thus even if supervised parsers like DOP*outperform unsupervised parsers like U-DOP* onhand-parsed data with >10%, the same supervisedparser is outperformed by the unsupervised parserif tested in an MT application.
Evidently, U-DOP?scapacity to capture both constituents and phrasespays off in a concrete application and shows theshortcomings of models that only allow for eitherconstituents (such as linguistically syntax-basedMT) or phrases (such as phrase-based MT).
In Bod(2007) we also show that U-DOT obtains virtuallythe same Bleu score as Pharaoh after eliminatingsubtrees with discontiguous yields.6    Conclusion: future of supervised parsingIn this paper we have shown that the accuracy ofunsupervised parsing under U-DOP* continues togrow when enlarging the training set withadditional data.
However, except for the simpletreebank PCFG, U-DOP* scores worse thansupervised parsers if evaluated on hand-annotateddata.
At the same time U-DOP* significantlyoutperforms the supervised DOP* if evaluated in apractical application like MT.
We argued that thiscan be explained by the fact that U-DOP learns406both constituents and (non-syntactic) phrases whilesupervised parsers learn constituents only.What should we learn from these results?We believe that parsing, when separated from atask-based application, is mainly an academicexercise.
If we only want to mimick a treebank orimplement a linguistically motivated grammar,then supervised, grammar-based parsers arepreferred to unsupervised parsers.
But if we wantto improve a practical application with a syntax-based language model, then an unsupervised parserlike U-DOP* might be superior.The problem with most supervised (andsemi-supervised) parsers is their rigid notion ofconstituent which excludes ?constituents?
like theGerman Ich m?chte or the French Il y a.
Instead, ithas become increasingly clear that the notion ofconstituent is a fluid which may sometimes be inagreement with traditional syntax, but which mayjust as well be in opposition to it.
Any sequence ofwords can be a unit of combination, including non-contiguous word sequences like closest X to Y. Aparser which does not allow for this fluidity maybe of limited use as a language model.
Sincesupervised parsers seem to stick to categoricalnotions of constituent, we believe that in the fieldof syntax-based language models the end ofsupervised parsing has come in sight.AcknowledgementsThanks to Willem Zuidema and three anonymousreviewers for useful comments and suggestions onthe future of supervised parsing.ReferencesBillot, S. and B. Lang, 1989.
The Structure of SharedForests in Ambiguous Parsing.
In ACL 1989.Bod, R. 1998.
Beyond Grammar: An Experience-BasedTheory of Language, CSLI Publications.Bod, R. Parsing with the Shortest Derivation.
InCOLING 2000, Saarbruecken.Bod, R. 2003.
An efficient implementation of a newDOP model.
In EACL 2003, Budapest.Bod, R. 2006.
An All-Subtrees Approach toUnsupervised Parsing.
In ACL-COLING 2006,Sydney.Bod, R. 2007.
Unsupervised Syntax-Based MachineTranslation.
Submitted for publication.Brants, T. 2000.
TnT - A Statistical Part-of-SpeechTagger.
In ANLP 2000.Chiang, D. 2005.
A Hierarchical Phrase-Based Modelfor Statistical Machine Translation.
In ACL 2005,Ann Arbor.Clark, A.
2001.
Unsupervised induction of stochasticcontext-free grammars using distributional clustering.In CONLL 2001.Goodman, J.
2003.
Efficient algorithms for the DOPmodel.
In R. Bod, R. Scha and K. Sima'an (eds.
).Data-Oriented Parsing, CSLI Publications.Graff, D. 1995.
North American News Text Corpus.Linguistic Data Consortium.
LDC95T21.Groves, D., M. Hearne and A.
Way, 2004.
Robust Sub-Sentential Alignment of Phrase-Structure Trees.
InCOLING 2004, Geneva.Hearne, M and A.
Way, 2006.
DisambiguationStrategies for Data-Oriented Translation.
Proceedingsof the 11th Conference of the European Associationfor Machine Translation, Oslo.Henderson, J.
2004.
Discriminative training of a neuralnetwork statistical parser.
In ACL 2004, Barcelona.Huang, L. and D. Chiang 2005.
Better k-best parsing.
InIWPT 2005, Vancouver.Johnson, M. 2002.
The DOP estimation method isbiased and inconsistent.
Computational Linguistics28, 71-76.Klein, D. and C. Manning 2002.
A general constituent-context model for improved grammar induction.
InACL 2002, Philadelphia.Klein, D. and C. Manning 2004.
Corpus-basedinduction of syntactic structure: models ofdependency and constituency.
ACL 2004, Barcelona.Koehn, P., Och, F. J., and Marcu, D. 2003.
Statisticalphrase based translation.
In HLT-NAACL 2003.Koehn, P. 2005.
Europarl: a parallel corpus forstatistical machine translation.
In MT Summit 2005.McClosky, D., E. Charniak and M. Johnson 2006.Effective self-training for parsing.
In HLT-NAACL2006, New York.Poutsma, A.
2000.
Data-Oriented Translation.
InCOLING 2000, Saarbruecken.Sch?tze, H. 1995.
Distributional part-of-speech tagging.In ACL 1995, Dublin.Sima'an, K. 1996.
Computational complexity ofprobabilistic disambiguation by means of treegrammars.
In COLING 1996, Copenhagen.Steedman, M. M. Osborne, A. Sarkar, S. Clark, R. Hwa,J.
Hockenmaier, P. Ruhlen, S. Baker, and J. Crim.2003.
Bootstrapping statistical parsers from smalldatasets.
In EACL 2003, Budapest.van Zaanen, M. 2000.
ABL: Alignment-BasedLearning.
In COLING 2000, Saarbr?cken.Zhang, Y., S. Vogel and A. Waibel, 2004.
InterpretingBLEU/NIST scores: How much improvement do weneed to have a better system?
Proceedings of theFourth International Conference on LanguageResources and Evaluation (LREC).Zollmann, A. and K. Sima'an 2005.
A consistent andefficient estimator for data-oriented parsing.
Journalof Automata, Languages and Combinatorics, Vol.
10(2005) Number 2/3, 367-388.407
