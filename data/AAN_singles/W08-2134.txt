CoNLL 2008: Proceedings of the 12th Conference on Computational Natural Language Learning, pages 238?242Manchester, August 2008A Cascaded Syntactic and Semantic Dependency Parsing SystemWanxiang Che, Zhenghua Li, Yuxuan Hu, Yongqiang Li, Bing Qin, Ting Liu, Sheng LiInformation Retrieval LabSchool of Computer Science and TechnologyHarbin Institute of Technology, China, 150001{car, lzh, yxhu, yqli, qinb, tliu, ls}@ir.hit.edu.cnAbstractWe describe our CoNLL 2008 Shared Tasksystem in this paper.
The system includestwo cascaded components: a syntactic anda semantic dependency parsers.
A first-order projective MSTParser is used as oursyntactic dependency parser.
In order toovercome the shortcoming of the MST-Parser, that it cannot model more global in-formation, we add a relabeling stage afterthe parsing to distinguish some confusablelabels, such as ADV, TMP, and LOC.
Be-sides adding a predicate identification anda classification stages, our semantic de-pendency parsing simplifies the traditionalfour stages semantic role labeling into two:a maximum entropy based argument clas-sification and an ILP-based post inference.Finally, we gain the overall labeled macroF1 = 82.66, which ranked the second posi-tion in the closed challenge.1 System ArchitectureOur CoNLL 2008 Shared Task (Surdeanu et al,2008) participating system includes two cascadedcomponents: a syntactic and a semantic depen-dency parsers.
They are described in Section 2and 3 respectively.
Their experimental results areshown in Section 4.
Section 5 gives our conclusionand future work.2 Syntactic Dependency ParsingMSTParser (McDonald, 2006) is selected as ourbasic syntactic dependency parser.
It views thec?
2008.
Licensed under the Creative CommonsAttribution-Noncommercial-Share Alike 3.0 Unported li-cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).Some rights reserved.syntactic dependency parsing as a problem offinding maximum spanning trees (MST) in di-rected graphs.
MSTParser provides the state-of-the-art performance for both projective and non-projective tree banks.2.1 FeaturesThe score of each labeled arc is computed throughthe Eq.
(1) in MSTParser.score(h, c, l) = w ?
f(h, c, l) (1)where node h represents the head node of the arc,while node c is the dependent node (or child node).l denotes the label of the arc.There are three major differences between ourfeature set and McDonald (2006)?s:1) We use the lemma as a generalization featureof a word, while McDonald (2006) use the word?sprefix.2) We add two new features: ?bet-pos-h-same-num?
and ?bet-pos-c-same-num?.
They representthe number of nodes which locate between node hand node c and whose POS tags are the same withh and c respectively.3) We use more back-off features than McDon-ald (2006) by completely enumerating all of thepossible combinatorial features.2.2 RelabelingBy observing the current results of MSTParser onthe development data, we find that the performanceof some labels are far below average, such as ADV,TMP, LOC.
We think the main reason lies in thatMSTParser only uses local features restricted to asingle arc (as shown in Eq.
(1)) and fails to usemore global information.
Consider two sentences:?I read books in the room.?
and ?I read books inthe afternoon.?.
It is hard to correctly label the arc238Deprel Total Mislabeled asNMOD 8,922 NAME [0.4], DEP [0.4], LOC [0.1],AMOD [0.1]OBJ 1,728 TMP [0.5], ADV [0.4], OPRD[0.3]ADV 1,256 TMP [2.9], LOC [2.3], MNR [1.8],DIR [1.5]NAME 1,138 NMOD [2.2]VC 953 PRD [0.9]DEP 772 NMOD [4.0]TMP 755 ADV [9.9], LOC [6.5]LOC 556 ADV [12.6], NMOD [7.9], TMP [5.9]AMOD 536 ADV [2.2]PRD 509 VC [4.7]APPO 444 NMOD [2.5]OPRD 373 OBJ [4.6]DIR 119 ADV [18.5]MNR 109 ADV [28.4]Table 1: Error Analysis of Each Labelbetween ?read?
and ?in?
unless we know the objectof ?in?.We count the errors of each label, and show thetop ones in Table 1.
?Total?
refers to the total num-ber of the corresponding label in the developmentdata.
The column of ?Mislabeled as?
lists the la-bels that an arc may be mislabeled as.
The numberin brackets shows the percentage of mislabeling.As shown in the table, some labels are often con-fusable with each other, such as ADV, LOC andTMP.2.3 Relabeling using Maximum EntropyClassifierWe constructed two confusable label set whichhave a higher mutual mislabeling proportion:(NMOD, LOC, ADV, TMP, MNR, DIR) and (OBJ,OPRD).
A maximum entropy classifier is used torelabel them.Features are shown in Table 2.
The first columnlists local features, which contains information ofthe head node h and the dependent node c of an arc.
?+ dir dist?
means that conjoining existing featureswith arc direction and distance composes new fea-tures.
The second column lists features using theinformation of node c?s children.
?word c c?
rep-resents form or lemma of one child of the nodec.
?dir c?
and ?dist c?
represents the direction anddistance of the arc which links node c to its child.The back-off technique is also used on these fea-tures.Local features (+ dir dist) Global features (+ dir c dist c)word h word c word h word c word c cTable 2: Relabeling Feature Set (+ dir dist)3 Semantic Dependency Parsing3.1 ArchitectureThe whole procedure is divided into four separatestages: Predicate Identification, Predicate Classifi-cation, Semantic Role Classification, and Post In-ference.During the Predicate Identification stage we ex-amine each word in a sentence to discover targetpredicates, including both noun predicates (fromNomBank) and verb predicates (from PropBank).In the Predicate Classification stage, each predi-cate is assigned a certain sense number.
For eachpredicate, the probabilities of a word in the sen-tence to be each semantic role are predicted in theSemantic Role Classification stage.
Maximum en-tropy model is selected as our classifiers in thesestages.
Finally an ILP (Integer Linear Program-ming) based method is adopted for post infer-ence (Punyakanok et al, 2004).3.2 Predicate IdentificationThe predicate identification is treated as a binaryclassification problem.
Each word in a sentence ispredicted to be a predicate or not to be.
A set offeatures are extracted for each word, and an opti-mized subset of them are adopted in our final sys-tem.
The following is a full list of the features:DEPREL (a1): Type of relation to the parent.WORD (a21), POS (a22), LEMMA (a23),HEAD (a31), HEAD POS (a32), HEAD LEMMA(a33): The forms, POS tags and lemmas of a wordand it?s headword (parent) .FIRST WORD (a41), FIRST POS (a42),FIRST LEMMA (a43), LAST WORD (a51),LAST POS (a52), LAST LEMMA (a53): Acorresponding ?constituent?
for a word consistsof all descendants of it.
The forms, POS tags andlemmas of both the first and the last words in theconstituent are extracted.POS PAT (a6): A ?POS pattern?
is produced forthe corresponding constituent as follows: a POSbag is produced with the POS tags of the wordsin the constituent except for the first and the lastones, duplicated tags removed and the original or-der ignored.
Then we have the POS PAT feature239by combining the POS tag of the first word, thebag and the POS tag of the last word.CHD POS (a71), CHD POS NDUP (a72),CHD REL (a73), CHD REL NDUP (a74): ThePOS tags of the child words are joined to-gether to form feature CHD POS.
With adja-cently duplicated tags reduced to one, featureCHD POS NDUP is produced.
Similarly we canget CHD REL and CHD REL NDUP too, withthe relation types substituted for the POS tags.SIB REL (a81), SIB REL NDUP (a82),SIB POS (a83), SIB POS NDUP (a84): Siblingwords (including the target word itself) and thecorresponding dependency relations (or POS tags)are considered as well.
Four features are formedsimilarly to those of child words.VERB VOICE (a9): Verbs are examined forvoices: if the headword lemma is either ?be?
or?get?, or else the relation type is ?APPO?, then theverb is considered passive, otherwise active.Also we used some ?combined?
features whichare combinations of single features.
The final op-timized feature set is (a1, a21, a22, a31, a32, a41,a42, a51, a52, a6, a72, a73, a74, a81, a82, a83,a1+a21, a21+a31, a21+a6, a21+a74, a73+a81,a81+a83).3.3 Predicate ClassificationAfter predicate identification is done, the resultingpredicates are processed for sense classification.
Aclassifier is trained for each predicate that has mul-tiple senses on the training data (There are totally962 multi-sense predicates on the training corpus,taking up 14% of all) In additional to those fea-tures described in the predicate identification sec-tion, some new ones relating to the predicate wordare introduced:BAG OF WORD (b11), BAG OF WORD O(b12): All words in a sentence joined, namely?Bag of Words?.
And an ?ordered?
version is in-troduced where each word is prefixed with a letter?L?, ?R?
or ?T?
indicating it?s to the left or right ofthe predicate or is the predicate itself.BAG OF POS O (b21), BAG OF POS N(b22): The POS tags prefixed with ?L?, ?R?
or?T?
indicating the word position joined together,namely ?Bag of POS (Ordered)?.
With theprefixed letter changed to a number indicatingthe distance to the predicate (negative for beingleft to the predicate and positive for right), an-other feature is formed, namely ?Bag of POS(Numbered)?.WIND5 BIGRAM (b3): 5 closest words fromboth left and right plus the predicate itself, in total11 words form a ?window?, within which bigramsare enumerated.The final optimized feature set for the task ofpredicate classification is (a1, a21, a23, a71, a72,a73, a74, a81, a82, a83, a84, a9, b11, b12, b22, b3,a71+a9).3.4 Semantic Role ClassificationIn our system, the identification and classifica-tion of semantic roles are achieved in a singlestage (Liu et al, 2005) through one single classi-fier (actually two, one for noun predicates, and theother for verb predicates).
Each word in a sentenceis given probabilities to be each semantic role (in-cluding none of the these roles) for a predicate.Features introduced in addition to those of the pre-vious subsections are the following:POS PATH (c11), REL PATH (c12): The ?POSPath?
feature consists of POS tags of the wordsalong the path from a word to the predicate.
Otherthan ?Up?
and ?Down?, the ?Left?
and ?Right?
di-rection of the path is added.
Similarly, the ?Re-lation Path?
feature consists of the relation typesalong the same path.UP PATH (c21), UP REL PATH (c22): ?Up-stream paths?
are parts of the above paths that stopat the common ancestor of a word and the predi-cate.PATH LEN (c3): Length of the pathsPOSITION (c4): The relative position of a wordto the predicate: Left or Right.PRED FAMILYSHIP (c5): ?Familyship rela-tion?
between a word and the predicate, being oneof ?self?, ?child?, ?descendant?, ?parent?, ?ances-tor?, ?sibling?, and ?not-relative?.PRED SENSE (c6): The lemma plus sensenumber of the predicateAs for the task of semantic role classification,the features of the predicate word in addition tothose of the word under consideration can alsobe used; we mark features of the predicate withan extra ?p?.
For example, the head word ofthe current word is represented as a31, and thehead word of the predicate is represented as pa31.So, with no doubt for the representation, our fi-nal optimized feature set for the task of seman-tic role classification is (a1, a23, a33, a43, a53,a6, c11, c12, c21, c3, c4, c6, pa23, pa71, pa73,240pa83, a1+a23+a33, a21+c5, a23+c12, a33+c12,a33+c22, a6+a33, a73+c5, c11+c12, pa71+pa73).3.5 ILP-based Post InferenceThe final semantic role labeling result is gener-ated through an ILP (Integer Linear Programming)based post inference method.
An ILP problem isformulated with respect to the probability given bythe above stage.
The final labeling is formed at thesame time when the problem is solved.Let W be the set of words in the sentence, andR be the set of semantic role labels.
A virtual label?NULL?
is also added to R, representing ?none ofthe roles is assigned?.For each word w ?
W and semantic role labelr ?
R we create a binary variable vwr?
(0, 1),whose value indicates whether or not the word wis labeled as label r. pwrdenotes the possibil-ity of word w to be labeled as role r. Obviously,when objective function f =?w,rlog(pwr?
vwr)is maximized, we can read the optimal labeling fora predicate from the assignments to the variablesvwr.
There are three constrains used in our system:C1: Each relation should be and only be la-beled with one label (including the virtual label?NULL?
), i.e.
:?rvwr= 1C2: Roles with a small probability should neverbe labeled (except for the virtual role ?NULL?
).The threshold we use in our system is 0.3, whichis optimized from the development data.
i.e.
:vwr= 0, if pwr< 0.3 and r 6= ?NULL?C3: Statistics shows that the most roles (ex-cept for the virtual role ?NULL?)
usually appearonly once for a predicate, except for some rare ex-ception.
So we impose a no-duplicated-roles con-straint with an exception list, which is constructedaccording to the times of semantic roles?
duplica-tion for each single predicate (different senses of apredicate are considered different) and the ratio ofduplication to non-duplication.?rvwr?
1,if < p, r > /?
{< p, r > |p ?
P, r ?
R;dprcpr?dpr> 0.3 ?
dpr> 10}(2)where P is the set of predicates; cprdenotes thecount of words in the training corpus, which arePredicate Type Predicate LabelNoun president.01 A3Verb match.01 A1Verb tie.01 A1Verb link.01 A1Verb rate.01 A0Verb rate.01 A2Verb attach.01 A1Verb connect.01 A1Verb fit.01 A1Noun trader.01 SUTable 3: No-duplicated-roles constraint exceptionlist (obtained by Eq.
(2))labeled as r ?
R for predicate p ?
P ; while dprdenotes something similar to cpr, but what takeninto account are only those words labeled with r,and there are more than one roles within the sen-tence for the same predicate.
Table 3 lists the com-plete exception set, which has a size of only 10.4 ExperimentsThe original MSTParser1is implemented in Java.We were confronted with memory shortage whentrying to train a model with the entire CoNLL 2008training data with 4GB memory.
Therefore, werewrote it with C++ which can manage the mem-ory more exactly.
Since the time was limited, weonly rewrote the projective part without consider-ing second-order parsing technique.Our maximum entropy classifier is implementedwith Maximum Entropy Modeling Toolkit2.
Theclassifier parameters: gaussian prior and iterations,are tuned with the development data for differentstages respectively.lp solve 5.53is chosen as our ILP problemsolver during the post inference stage.The training time of the syntactic and the se-mantic parsers are 22 and 5 hours respectively, onall training data, with 2.0GHz Xeon CPU and 4Gmemory.
While the prediction can be done within10 and 5 minutes on the development data.4.1 Syntactic Dependency ParsingThe experiments on development data show thatrelabeling process is helpful, which improves the1http://sourceforge.net/projects/mstparser2http://homepages.inf.ed.ac.uk/s0450736/maxenttoolkit.html3http://sourceforge.net/projects/lpsolve241Precision (%) Recall (%) F1Pred Identification 91.61 91.36 91.48Pred Classification 86.61 86.37 86.49Table 4: The performance of predicate identifica-tion and classificationPrecision (%) Recall (%) F1Simple 81.02 76.00 78.43ILP-based 82.53 75.26 78.73Table 5: Comparison between different post infer-ence strategiesLAS performance from 85.41% to 85.94%.
The fi-nal syntactic dependency parsing performances onthe WSJ and the Brown test data are 87.51% and80.73% respectively.4.2 Semantic Dependency ParsingThe semantic dependency parsing component isbased on the last syntactic dependency parsingcomponent.
All stages of the system are trainedwith the closed training corpus, while predictedagainst the output of the syntactic parsing.Performance for predicate identification andclassification is given in Table 4, wherein the clas-sification is done on top of the identification.Semantic role classification and the post infer-ence are done on top of the result of predicate iden-tification and classification.
The final performanceis presented in Table 5.
A simple post inferencestrategy is given for comparison, where the mostpossible label (including the virtual label ?NULL?
)is select except for those duplicated non-virtual la-bels with lower probabilities (lower than 0.5).
OurILP-based method produces a gain of 0.30 with re-spect to the F1 score.The final semantic dependency parsing perfor-mance on the development and the test (WSJ andBrown) data are shown in Table 6.Precision (%) Recall (%) F1Development 82.53 75.26 78.73Test (WSJ) 82.67 77.50 80.00Test (Brown) 64.38 68.50 66.37Table 6: Semantic dependency parsing perfor-mances4.3 Overall PerformanceThe overall macro scores of our syntactic and se-mantic dependency parsing system are 82.38%,83.78% and 73.57% on the development and twotest (WSJ and Brown) data respectively, which isranked the second position in the closed challenge.5 Conclusion and Future WorkWe present our CoNLL 2008 Shared Task systemwhich is composed of two cascaded components:a syntactic and a semantic dependency parsers,which are built with some state-of-the-art methods.Through a fine tuning features and parameters, thefinal system achieves promising results.
In orderto improve the performance further, we will studyhow to make use of more resources and tools (openchallenge) and how to do joint learning betweensyntactic and semantic parsing.AcknowledgmentsThe authors would like to thank the reviewers fortheir helpful comments.
This work was supportedby National Natural Science Foundation of China(NSFC) via grant 60675034, 60575042, and the?863?
National High-Tech Research and Develop-ment of China via grant 2006AA01Z145.ReferencesLiu, Ting, Wanxiang Che, Sheng Li, Yuxuan Hu, andHuaijun Liu.
2005.
Semantic role labeling systemusing maximum entropy classifier.
In Proceedingsof CoNLL-2005, June.McDonald, Ryan.
2006.
Discriminative Learning andSpanning Tree Algorithms for Dependency Parsing.Ph.D.
thesis, University of Pennsylvania.Punyakanok, Vasin, Dan Roth, Wen-tau Yih, and DavZimak.
2004.
Semantic role labeling via integerlinear programming inference.
In Proceedings ofColing-2004, pages 1346?1352.Surdeanu, Mihai, Richard Johansson, Adam Meyers,Llu?
?s M`arquez, and Joakim Nivre.
2008.
TheCoNLL-2008 shared task on joint parsing of syntac-tic and semantic dependencies.
In Proceedings ofthe 12th Conference on Computational Natural Lan-guage Learning (CoNLL-2008).242
