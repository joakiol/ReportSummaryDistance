Proceedings of the Workshop on Multiword Expressions: Identifying and Exploiting Underlying Properties, pages 20?27,Sydney, July 2006. c?2006 Association for Computational LinguisticsUsing Information about Multi-word Expressionsfor the Word-Alignment TaskSriram Venkatapathy1Language Technologies Research Center,Indian Institute ofInformation Technology,Hyderabad, India.sriramv@linc.cis.upenn.eduAravind K. JoshiDepartment of Computer andInformation Science and Institute forResearch in Cognitive Science,University of Pennsylvania, PA, USA.joshi@linc.cis.upenn.eduAbstractIt is well known that multi-word expres-sions are problematic in natural languageprocessing.
In previous literature, it hasbeen suggested that information abouttheir degree of compositionality can behelpful in various applications but it hasnot been proven empirically.
In this pa-per, we propose a framework in whichinformation about the multi-word expres-sions can be used in the word-alignmenttask.
We have shown that even simplefeatures like point-wise mutual informa-tion are useful for word-alignment task inEnglish-Hindi parallel corpora.
The align-ment error rate which we achieve (AER =0.5040) is significantly better (about 10%decrease in AER) than the alignment errorrates of the state-of-art models (Och andNey, 2003) (Best AER = 0.5518) on theEnglish-Hindi dataset.1 IntroductionIn this paper, we show that measures representingcompositionality of multi-word expressions canbe useful for tasks such as Machine Translation,word-alignment to be specific here.
We use an on-line learning framework called MIRA (McDon-ald et al, 2005; Crammer and Singer, 2003) fortraining a discriminative model for the word align-ment task (Taskar et al, 2005; Moore, 2005).
Thediscriminative model makes use of features whichrepresent the compositionality of multi-word ex-pressions.1At present visiting Institute for Research in CognitiveScience, University of Pennsylvania, PA, USA.Multi-word expressions (MWEs) are thosewhose structure and meaning cannot be derivedfrom their component words, as they occur inde-pendently.
Examples include conjunctions suchas ?as well as?
(meaning ?including?
), idioms like?kick the bucket?
(meaning ?die?)
phrasal verbssuch as ?find out?
(meaning ?search?)
and com-pounds like ?village community?.
They can be de-fined roughly as idiosyncratic interpretations thatcross word boundaries (Sag et al, 2002).A large number of MWEs have standardsyntactic structure but are semantically non-compositional.
Here, we consider the class of verbbased expressions (verb is the head of the phrase),which occur very frequently.
This class of verbbased multi-word expressions include verbal id-ioms, support-verb constructions, among others.The example ?take place?
is a MWE but ?take agift?
is not.In the past, various measures have been sug-gested for measuring the compositionality ofmulti-word expressions.
Some of these are mu-tual information (Church and Hanks, 1989), dis-tributed frequency (Tapanainen et al, 1998) andLatent Semantic Analysis (LSA) model (Baldwinet al, 2003).
Even though, these measures havebeen shown to represent compositionality quitewell, compositionality itself has not been shown tobe useful in any application yet.
In this paper, weexplore this possibility of using the informationabout compositionality of MWEs (verb based) forthe word alignment task.
In this preliminary work,we use simple measures (such as point-wise mu-tual information) to measure compositionality.The paper is organized as follows.
In section 2,we discuss the word-alignment task with respectto the class of multi-word expressions of interestin this paper.
In section 3, we show empirically,20the behavior of verb based expressions in a paral-lel corpus (English-Hindi in our case).
We thendiscuss our alignment algorithm in section 4.
Insection 5, we describe the features which we haveused in our training model.
Section 6 discusses thetraining algorithm and in section 7, the results ofour discriminative model for the word alignmenttask.
Related work and conclusion follow in sec-tion 8 and 9 respectively.2 Task: Word alignment of verbs andtheir dependentsThe task is to align the verbs and their dependents(arguments and adjuncts) in the source languagesentence (English) with words in the target lan-guage sentence (Hindi).
The dependents of theverbs in the source sentence are represented bytheir head words.
Figure 1. shows an exampleof the type of multi-word expressions which weconsider for alignment.subjobjprep_inevent placetookThe cyclingPhiladelphia(The cycling event took place in Philadelphia)Figure 1: Example of MWEs we considerIn the above example, the goal will the to alignthe words ?took?, ?event?, ?place?
and ?Philadel-phia?
with corresponding word(s) in the target lan-guage sentence (which is not parsed) using a dis-criminative approach.
The advantage in using thediscriminative approach for alignment is that it letsyou use various compositionality based featureswhich are crucial towards aligning these expres-sions.
Figure 2. shows the appropriate alignmentof the expression in Figure 1. with the words in thetarget language.
The pair (take place), in English,a verb and one of its dependents is aligned with asingle verbal unit in Hindi.It is essential to obtain the syntactic roles for de-pendents in the source language sentence as theyare required for computing the compositionalityvalue between the dependents and their verbs.
ThePhiladelphia   mein   saikling     kii    pratiyogitaa   huiPhiladelphiaThe cyclingtookplaceeventprep_inobjsubjFigure 2: Alignment of Verb based expressionsyntactic roles on the source side are obtained byapplying simple rules to the output of a depen-dency parser.
The dependency parser which weused in our experiments is a stochastic TAG baseddependency parser (Shen, 2006).
A sentencecould have one or more verbs.
We would liketo align all the expressions represented by thoseverbs with words in the target language.3 Behavior of MWEs in parallel corporaIn this section, we will briefly discuss the com-plexity of the alignment problem based on theverb based MWE?s.
From the word aligned sen-tence pairs, we compute the fraction of times asource sentence verb and its dependent are alignedtogether with the same word in the target lan-guage sentence.
We count the number of times asource sentence verb and its dependent are alignedtogether with the same word in the target lan-guage sentence, and divide it by the total num-ber of dependents.
The total size of our wordaligned corpus is 400 sentence pairs which in-cludes both training and test sentences.
The totalnumber of dependents present in these sentencesare 2209.
Total number of verb dependent pairswhich aligned with same word in target languageare 193.
Hence, the percentage of such occur-rences is 9%, which is a significant number.4 Alignment algorithmIn this section, we describe the algorithm for align-ing verbs and their dependents in the source lan-guage sentence with the words in the target lan-guage.
Let V be the number of verbs and A be thenumber of dependents.
Let the number of words in21the target language be N. If we explore all the waysin which the V + A words in the source sentenceare aligned with words in the target language be-fore choosing the best alignment, the total numberof possibilites are NV+A.
This is computationallyvery expensive.
Hence, we use a Beam-search al-gorithm to obtain the K-best alignments.Our algorithm has three main steps.1.
Populate the Beam : Use the local features(which largely capture the co-occurence in-formation between the source word and thetarget word) to determine the K-best align-ments of verbs and their dependents withwords in the target language.2.
Re-order the Beam: Re-order the abovealignments using more complex features(which include the global features and thecompositionality based feature(s)).3.
Post-processing : Extend the alignment(s) ofthe verb(s) (on the source side) to includewords which can be part of the verbal uniton the target side.For a source sentence, let the verbs and depen-dents be denoted by sij.
Here i is the index ofthe verb (1 <= i <= V ).
The variable j isthe index of the dependents (0 <= j <= A)except when j = 0 which is used to representthe verb itself.
Let the source sentences be de-noted as S = fsijg and the target sentences byT = ftng.
The alignment from a source sen-tence S to target sentence T is defined as the map-ping a = faijnj aijn (sij!
tn);8i; jg.
Abeam is used to store a set of K-best alignmentsbetween a source sentence and the target sentence.It is represented using the symbol B where Bk(0 <= k <= K) is used to refer to a particularalignment configuration.4.1 Populate the BeamThe task in this step is to obtain the K-best can-didate alignments using local features.
The localfeatures mainly contain the coccurence informa-tion between a source and a target word and are in-dependent of other alignment links or words in thesentences.
Let the local feature vector be denotedas fL(sij; tk).
The score of a particular alignmentlink is computed by taking the dot product of theweight vector W with the local feature vector (ofwords connected by the alignment link).
Hence,the local score will besoreL(sij; tk) = W:fL(sij; tk)The total score of an alignment configuration iscomputed by adding the scores of individual linksin the alignment configuration.
Hence, the align-ment score will besoreLa(a; S; T ) =XsoreL(sij; tk)8sij2 S & sij!
tk2 aWe propose an algorithm of order O((V +A)Nlog(N) + K) to compute the K-best align-ment configurations.
First, the local scores of eachverb and its dependents are computed for eachword in the target sentence and stored in a lo-cal beam denoted by bij.
The local beams cor-responding to all the verbs and dependents arethen sorted.
This operation has the complexity(V + A) N log(N).The goal now is to pick the K-best configura-tions of alignment links.
A single slot in the localbeam corresponds to one alignment link.
We de-fine a boundary which partitions each local beaminto two sets of slots.
The slots above the bound-ary represent the slots which have been exploredby the algorithm while slots below the boundaryhave still to be explored.
The figure 3. shows theboundary which cuts across the local beams.Bb (i,j)BeamAlignmentBoundaryLocal BeamsFigure 3: BoundaryWe keep on modifying the boundary untill allthe K slots in the Alignment Beam are filled withthe K-best configurations.
At the beginning of thealgorithm, the boundary is a straight line passingthrough the top of all the local beams.
The top slotof the alignment beam at the beginning represents22the combination of alignment links with the bestlocal scores.The next slot bij[p?
(from the set of unexploredslots) to be included in the boundary is the slotwhich has the least difference in score from thescore of the slot at the top of its local beam.
Thatis, we pick the slot bij[p?
such that sore(bij[p?) sore(bij[1?)
is the least among all the unexploredslots (or alignment links).
Trivially, bij[p  1?
wasalready a part of the boundary.When the slot bij[p?
is included in the boundary,various configurations, which now contain bij[p?,are added to the alignment beam.
The new con-figurations are the same as the ones which previ-ously contained bij[p   1?
but with the replace-ment of bij[p  1?
by bij[p?.
The above procedureensures that the the alignment configurations areK-best and are sorted according to the scores ob-tained using local features.4.2 Re-order the beamWe now use global features to re-order the beam.The global features look at the properties of the en-tire alignment configuration instead of alignmentlinks locally.The global score is defined as the dot product ofthe weight vector and the global feature vector.soreG(a) = W:fG(a)The overall score is calculated by adding the localscore and the global score.sore(a) = soreLa(a) + soreG(a)The beam is now sorted based on the overallscores of each alignment.
The alignment config-uration at the top of the beam is the best possiblealignment between source sentence and the targetsentence.4.3 Post-processingThe first two steps in our alignment algorithmcompute alignments such that one verb or depen-dent in the source language side is aligned withonly one word in the target side.
But, in the caseof compound verbs in Hindi, the verb in English isaligned to all the words which represent the com-pound verb in Hindi.
For example, in Figure 3, theverb ?lost?
is aligned to both ?khoo?
and ?dii?.Our alignment algorithm would have aligned?lost?
only to ?khoo?.
Hence, we look at the win-dow of words after the word which is aligned tomainee    Shyam   ki    kitaaba  khoo   diiShyam?sbookIlostFigure 4: Case of compound verb in Hindithe source verb and check if any of them is a verbwhich has not been aligned with any word in thesource sentence.
If this condition is satisfied, wealign the source verb to these words too.5 ParametersAs the number of training examples (294 sen-tences) is small, we choose to use very representa-tive features.
Some of the features which we usedin this experiment are as follows,5.1 Local features (FL)The local features which we consider are mainlyco-occurence features.
These features estimate thelikelihood of a source word aligning to a targetword based on the co-occurence information ob-tained from a large sentence aligned corpora1.1.
DiceWords: Dice Coefficient of the sourceword and the target wordDCoe (sij; tk) =2  Count(sij; tk)Count(sij) + Count(tk)where Count(sij; tk) is the number of timesthe word tkwas present in the translation ofsentences containing the word sijin the par-allel corpus.2.
DiceRoots: Dice Coefficient of the lemma-tized forms of the source and target words.It is important to consider this feature be-cause the English-Hindi parallel corpus is notlarge and co-occurence information can belearnt effectively only after we lemmatize thewords.3.
Dict: Whether there exists a dictionary entryfrom the source word sijto the target word150K sentence pairs originally collected as part of TIDESMT project and later refined at IIIT-Hyderabad, India.23tk.
For English-Hindi, we used a dictionaryavailable at IIIT - Hyderabad, India.4.
Null: Whether the source word sijis alignedto nothing in the target language.5.2 Global featuresThe following are the four global features whichwe have considered, AvgDist: The average distance between thewords in the target language sentence whichare aligned to the verbs in the source lan-guage sentence .
AvgDist is then normalizedby dividing itself by the number of words inthe target language sentence.
If the averagedistance is small, it means that the verbs inthe source language sentence are aligned withwords in the target language sentence whichare located at relatively close distances, rela-tive to the length of the target language sen-tence.This feature expresses the distribution ofpredicates in the target language. Overlap: This feature stores the count ofpairs of verbs in the source language sentencewhich align with the same word in the targetlanguage sentence.
Overlap is normalized bydividing itself by the total pairs of verbs.This feature is used to discourage overlapsamong the words which are alignments ofverbs in the source language sentence. MergePos: This feature can be considered asa compositionality based feature.
The partof speech tag of a dependent is essential todetermine the likelihood of the dependent toalign with the same word in the target lan-guage sentence as the word to which its verbis aligned.This binary feature is active when the align-ment links of a dependent and its verbmerge.
For example, in Figure 5., the feature?merge RP?
will be active (that is, merge RP= 1). MergeMI: This is a compositionality basedfeature which associates point-wise mutualinformation (apart from the POS informa-tion) with the cases where the dependentswhich have the same alignment in the targetHe/N away/RPran/Vvaha     bhaaga     gayaaFigure 5: Example of MergePos featurelanguage as their verbs.
This features whichnotes the the compositionality value (repre-sented by point-wise mutual information inour experiments) is active if the alignmentlinks of dependent and its verb merge.The mutual information (MI) is classifiedinto three groups depending on its absolutevalue.
If the absolute value of mutual infor-mation rounded to nearest integer is in therange 0-2, it is considered LOW.
If the valueis in the range 3-5, it is considered MEDIUMand if it is above 5, it is considered HIGH.The feature ?merge RP HIGH?
is active inthe example shown in figure 6.He/N away/RPran/Vvaha     bhaaga     gayaaMI = HIGHFigure 6: Example of MergeMI feature6 Online large margin trainingFor parameter optimization, we have used an on-line large margin algorithm called MIRA (Mc-Donald et al, 2005) (Crammer and Singer, 2003).We describe the training algorithm that we usedvery briefly.
Our training set is a set of English-Hindi word aligned parallel corpus.
We get theverb based expressions in English by running a de-pendency parser (Shen, 2006).
Let the number ofsentence pairs in the training data be m. We have24fSq; Tq; a^qg for training where q <= m is the in-dex number of the sentence pair fSq; Tqg in thetraining set and a^qis the gold alignment for thepair fSq; Tqg.
Let W be the weight vector whichhas to be learnt, Wibe the weight vector after theend of ith update.
To avoid over-fitting, W is ob-tained by averaging over all the weight vectors Wi.A generic large margin algorithm is defined fol-lows for the training instances fSq; Tq; a^qg,1.
Initialize W0, W , i2.
for p:1 to NIterations3.
for q:1 to m4.
Get K-Best predictionsq= fa1; a2:::akgfor the training example (Sq; Tq; a^q) usingthe current model W i and applying step1 and 2 of section 4.
Compute W i+1 byupdating W i based on (Sq; Tq; a^q;q).5. i = i + 16.
W = W + W i+17.
W = WNIterationsmThe goal of MIRA is to minimize the change inWi such that the score of the gold alignment a^ ex-ceeds the score of each of the predictions in  by amargin which is equal to the number of mistakes inthe predictions when compared to gold alignment.While computing the number of mistakes, the mis-takes due to the mis-alignment of head verb couldbe given greater weight, thus prompting the opti-mization algorithm to give greater importance toverb related mistakes and thereby improving over-all performance.Step 4 in the algorithm mentioned above canbe substituted by the following optimizationproblem,minimize k(W i+1  W i)ks.t.
8k, sore(a^q; Sq; Tq)   sore(aq;k; Sq; Tq)>= Mistakes(ak; a^q; Sq; Tq)The above optimization problem is converted tothe Dual form using one Lagrangian multiplier foreach constraint.
In the Dual form, the Lagrangianmultipliers are solved using Hildreth?s algorithm.Here, prediction of  is similar to the predictionof K   best classes in a multi-class classificationproblem.
Ideally, we need to consider all the possi-ble classes and assign margin constraints based onevery class.
But, here the number of such classesis exponential and thus we restrict ourselves to theK   best classes.7 Results on word-alignment task7.1 DatasetWe have divided the 400 word aligned sentencepairs into a training set consisting of 294 sen-tence pairs and a test set consisting of 106 sentencepairs.
The source sentences are all dependencyparsed (Shen, 2006) and only the verb and its de-pendents are considered for both training and test-ing our algorithm.
Our training algorithm requiresthat the each of the source words is aligned to onlyone or zero target words.
For this, we use simpleheuristics to convert the training data to the appro-priate format.
For the words aligned to a sourceverb, the first verb is chosen as the gold alignment.For the words aligned to any dependent which isnot a verb, the last content word is chosen as thealignment link.
For test data, we do not make anymodifications and the final output from our align-ment algorithm is compared with the original testdata.7.2 Experiments with GizaWe evaluated our discriminative approach by com-paring it with the state-of-art Giza++ alignments(Och and Ney, 2003).
The metric that we haveused to do the comparison is the Alignment ErrorRate (AER).
The results shown below also containPrecision, Recall and F-measure.Giza was trained using an English-Hindialigned corpus of 50000 sentence pairs.
In Table1., we report the results of the GIZA++ alignmentsrun from both the directions (English to Hindi andHindi to English).
We also show the results of theintersected model.
See Table 1. for the results ofthe GIZA++ alignments.Prec.
Recall F-meas.
AEREng!
Hin 0.45 0.38 0.41 0.5874Hin!
Eng 0.46 0.27 0.34 0.6584Intersected 0.82 0.19 0.31 0.6892Table 1: Results of GIZA++ - Original datasetWe then lemmatize the words in both the sourceand target sides of the parallel corpora and thenrun Giza++ again.
As the English-Hindi dataset25of 50000 sentence pairs is relatively small, we ex-pect lemmatizing to improve the results.
Table 2.shows the results.
As we hoped, the results afterlemmatizing the word forms are better than thosewithout.Prec.
Recall F-meas.
AEREng!
Hin 0.52 0.40 0.45 0.5518Hin!
Eng 0.53 0.30 0.38 0.6185Intersected 0.82 0.23 0.36 0.6446Table 2: Results of GIZA++ - lemmatized set7.3 Experiments with our modelWe trained our model using the training set of 294word aligned sentence pairs.
For training the pa-rameters, we used a beam size of 3 and number ofiterations equal to 3.
Table 3. shows the resultswhen we used only the basic local features (Dice-Words, DiceRoots, Dict and Null) to train and testour model.Prec.
Recall F-meas.
AERLocal Feats.
0.47 0.38 0.42 0.5798Table 3: Results using the basic featuresWhen we add the the global features (AvgDist,Overlap), we obtain the AER shown in Table 4.Prec.
Recall F-meas.
AER+ AvgD., Ove.
0.49 0.39 0.43 0.5689Table 4: Results using the features - AvgDist,OverlapNow, we add the transition probabilities ob-tained from the experiments with Giza++ as fea-tures in our model.
Table 5. contains the results.The compositionality related features are nowadded to our discriminative model to see if there isany improvement in performance.
Table 6. showsthe results by adding one feature at a time.We observe that there is an improvement in theAER by using the compositionality based features,thus showing that compositionality based featuresaid in the word-alignment task in a significant way(AER = 0.5045).8 Related workVarious measures have been proposed in the pastto measure the compositionality of multi-word ex-Prec.
Recall F-meas.
AER+ Giza++ prob.
0.54 0.44 0.49 0.5155Table 5: Results using the Giza++ probabilitiesPrec.
Recall F-meas.
AER+ MergePos 0.54 0.45 0.49 0.5101+ MergeMI 0.55 0.45 0.50 0.5045Table 6: Results using the compositionality basedfeaturespressions of various types.
Some of them are Fre-quency, Point-wise mutual information (Churchand Hanks, 1989), Distributed frequency of object(Tapanainen et al, 1998), Distributed frequencyof object using verb information (Venkatapathyand Joshi, 2005), Similarity of object in verb-object pair using the LSA model (Baldwin et al,2003), (Venkatapathy and Joshi, 2005) and Lex-ical and Syntactic fixedness (Fazly and Steven-son, 2006).
These features have largely been eval-uated by the correlation of the compositionalityvalue predicted by these measures with the goldstandard value suggested by human judges.
It hasbeen shown that the correlation of these measuresis higher than simple baseline measures suggest-ing that these measures represent compositionalityquite well.
But, the compositionality as such hasnot been used in any specific application yet.In this paper, we have suggested a frameworkfor using the compositionality of multi-word ex-pressions for the word alignment task.
State-of-artsystems for doing word alignment use generativemodels like GIZA++ (Och and Ney, 2003; Brownet al, 1993).
Discriminative models have beentried recently for word-alignment (Taskar et al,2005; Moore, 2005) as these models give the abil-ity to harness variety of complex features whichcannot be provided in the generative models.
Inour work, we have used the compositionality ofmulti-word expressions to predict how they alignwith the words in the target language sentence.For parameter optimization for the word-alignment task, Taskar, Simon and Klein (Taskaret al, 2005) used a large margin approach by fac-toring the structure level constraints to constraintsat the level of an alignment link.
We cannot dosuch a factorization because the scores of align-ment links in our case are not computed in a com-pletely isolated manner.
We use an online largemargin approach called MIRA (McDonald et al,262005; Crammer and Singer, 2003) which fits wellwith our framework.
MIRA has previously beenused by McDonald, Pereira, Ribarov and Hajic(McDonald et al, 2005) for learning the param-eter values in the task of dependency parsing.It should be noted that previous word-alignmentexperiments such as Taskar, Simon and Klein(Taskar et al, 2005) have been done with verylarge datasets and there is little word-order vari-ation in the languages involved.
Our dataset issmall at present and there is substantial word ordervariation between the source and target languages.9 Conclusion and future workIn this paper, we have proposed a discriminativeapproach for using the compositionality informa-tion about verb-based multi-word expressions forthe word-alignment task.
For training our model,use used an online large margin algorithm (Mc-Donald et al, 2005).
For predicting the alignmentgiven a model, we proposed a K-Best beam searchalgorithm to make our prediction algorithm com-putationally feasible.We have investigated the usefulness of simplefeatures such as point-wise mutual information forthe word-alignment task in English-Hindi bilin-gual corpus.
We have show that by adding thecompositionality based features to our model, weobtain an decrease in AER from 0.5155 to 0.5045.Our overall results are better than those obtainedusing the GIZA++ models (Och and Ney, 2003).In future, we will experiment with more ad-vanced compositionality based features.
But, thiswould require a larger dataset for training and weare working towards buidling such a large dataset.Also, we would like to conduct similar exper-iments on other language pairs (e.g.
English-French) and compare the results with the state-of-art results reported for those languages.ReferencesTimothy Baldwin, Colin Bannard, Takaaki Tanaka, andDominic Widdows.
2003.
An empirical modelof multiword expression decomposability.
In Di-ana McCarthy Francis Bond, Anna Korhonen andAline Villavicencio, editors, Proceedings of the ACL2003 Workshop on Multiword Expressions: Analy-sis, Acquisition and Treatment, pages 89?96.P.
Brown, S. A. Pietra, V. J. Della, Pietra, and R. L.Mercer.
1993.
The mathmatics of stastistical ma-chine translation.
In Computational Linguistics.Kenneth Church and Patrick Hanks.
1989.
Word as-sociation norms, mutual information, and lexicog-raphy.
In Proceedings of the 27th.
Annual Meet-ing of the Association for Computational Linguis-tics, 1990.Koby Crammer and Yoram Singer.
2003.
Ultraconser-vative online algorithms for multiclass problems.
InJournal of Machine Learning Research.Afsaneh Fazly and Suzanne Stevenson.
2006.
Auto-matically constructing a lexicon of verb phrase id-iomatic combinations.
In Proceedings of EuropeanChapter of Association of Computational Linguis-tics.
Trento, Italy, April.Ryan McDonald, Fernando Pereira, Kiril Ribarov, andJan Hajic.
2005.
Non-projective dependency pars-ing using spanning tree algorithms.
In Proceed-ings of Human Language Technology Conferenceand Conference on Empirical Methods in NaturalLanguage Processing, pages 523?530, Vancouver,British Columbia, Canada, October.
Association ofComputational Linguistics.Robert C. Moore.
2005.
A discriminative frame-work for bilingual word alignment.
In Proceedingsof Human Language Technology Conference andConference on Empirical Methods in Natural Lan-guage Processing, pages 81?88, Vancouver, BritishColumbia, Canada, October.
Association of Compu-tational Linguistics.F.
Och and H. Ney.
2003.
A systematic comparisoinof various statistical alignment models.
In Compu-tational Linguistics.Ivan A.
Sag, Timothy Baldwin, Francis Bond, AnnCopestake, and Dan Flickinger.
2002.
Multi-wordexpressions: a pain in the neck for nlp.
In Proceed-ings of CICLing , 2002.Libin Shen.
2006.
Statistical LTAG Parsing.
Ph.D.thesis.Pasi Tapanainen, Jussi Piitulaine, and Timo Jarvinen.1998.
Idiomatic object usage and support verbs.
In36th Annual Meeting of the Association for Compu-tational Linguistics.Ben Taskar, Locoste-Julien Simon, and Klein Dan.2005.
A discriminative machine learning approachto word alignment.
In Proceedings of Human Lan-guage Technology Conference and Conference onEmpirical Methods in Natural Language Process-ing, pages 73?80, Vancouver, British Columbia,Canada, October.
Association of ComputationalLinguistics.Sriram Venkatapathy and Aravind Joshi.
2005.
Mea-suring the relative compositionality of verb-noun (v-n) collocations by integrating features.
In Proceed-ings of Human Language Technology Conferenceand Conference on Empirical Methods in NaturalLanguage Processing, pages 899?906.
Associationof Computational Linguistics, Vancouver, BritishColumbia, Canada, October.27
